{
  "id": "http://arxiv.org/abs/1702.02295v2",
  "title": "Guided Optical Flow Learning",
  "authors": [
    "Yi Zhu",
    "Zhenzhong Lan",
    "Shawn Newsam",
    "Alexander G. Hauptmann"
  ],
  "abstract": "We study the unsupervised learning of CNNs for optical flow estimation using\nproxy ground truth data. Supervised CNNs, due to their immense learning\ncapacity, have shown superior performance on a range of computer vision\nproblems including optical flow prediction. They however require the ground\ntruth flow which is usually not accessible except on limited synthetic data.\nWithout the guidance of ground truth optical flow, unsupervised CNNs often\nperform worse as they are naturally ill-conditioned. We therefore propose a\nnovel framework in which proxy ground truth data generated from classical\napproaches is used to guide the CNN learning. The models are further refined in\nan unsupervised fashion using an image reconstruction loss. Our guided learning\napproach is competitive with or superior to state-of-the-art approaches on\nthree standard benchmark datasets yet is completely unsupervised and can run in\nreal time.",
  "text": "Guided Optical Flow Learning\nYi Zhu1\nZhenzhong Lan2\nShawn Newsam1\nAlexander G. Hauptmann2\n1University of California, Merced\n2Carnegie Mellon University\n{yzhu25,snewsam}@ucmerced.edu\n{lanzhzh,alex}@cs.cmu.edu\nAbstract\nWe study the unsupervised learning of CNNs for opti-\ncal ﬂow estimation using proxy ground truth data. Super-\nvised CNNs, due to their immense learning capacity, have\nshown superior performance on a range of computer vision\nproblems including optical ﬂow prediction. They however\nrequire the ground truth ﬂow which is usually not accessi-\nble except on limited synthetic data. Without the guidance\nof ground truth optical ﬂow, unsupervised CNNs often per-\nform worse as they are naturally ill-conditioned. We there-\nfore propose a novel framework in which proxy ground truth\ndata generated from classical approaches is used to guide\nthe CNN learning. The models are further reﬁned in an un-\nsupervised fashion using an image reconstruction loss. Our\nguided learning approach is competitive with or superior to\nstate-of-the-art approaches on three standard benchmarks\nyet is completely unsupervised and can run in real time.\n1. Introduction\nOptical ﬂow contains valuable information for general\nimage sequence analysis due to its capability to represent\nmotion. It is widely used in vision tasks such as human\naction recognition [18, 22, 21], semantic segmentation [8],\nvideo frame prediction [15], video object tracking etc.\nClassical approaches for estimating optical ﬂow are of-\nten based on a variational model and solved as an energy\nminimization process [11, 4, 5]. They remain top perform-\ners on a number of evaluation benchmarks; however, most\nof them are too slow to be used in real time applications.\nDue to the great success of Convolutional Neural Network\n(CNN), several works [7, 16] have proposed using CNNs to\nestimate the motion between image pairs and have achieved\npromising results. Although they are much more efﬁcient\nthan classical approaches, these methods require supervi-\nsion and cannot apply to real world data where the ground\ntruth is not easily accessible.\nThus, some recent works\n[1, 20, 23] have investigated unsupervised learning through\nnovel loss functions but they often perform worse than su-\npervised ones.\n(ii) Unsupervised Reconstruction Loss\n(i) Average End-point Error\nFigure 1. An overview of our proposed guided learning frame-\nwork. ⊕denotes computing the per-pixel endpoint error with re-\nspect to the proxy ground truth ﬂow. ▷◁represents the inverse\nwarping and unsupervised reconstruction loss with respect to the\ninput image pairs.\nTo improve the accuracy of unsupervised CNNs for opti-\ncal ﬂow estimation, we propose to use the results of classi-\ncal methods as guidance for our unsupervised learning pro-\ncess. We refer to this as novel guided optical ﬂow learn-\ning as shown in Fig. 1. Speciﬁcally, there are two stages.\n(i) We generate proxy ground truth ﬂow using classical ap-\nproaches, and then train a supervised CNN with them. (ii)\nWe ﬁne tune the learned models by minimizing an image re-\nconstruction loss. By training the CNNs using proxy ground\ntruth, we hope to provide a good initialization point for sub-\nsequent network learning. By ﬁne tuning the models on tar-\nget datasets, we hope to overcome the risk that CNN might\nhave learned the failure cases of the classical approaches.\nThe entire learning framework is thus unsupervised.\nOur contributions are two-fold. First, we demonstrate\nthat supervised CNNs can learn to estimate optical ﬂow well\neven when only guided using noisy proxy ground truth data\ngenerated from classical methods. Second, we show that\nﬁne tuning the learned models for target datasets by mini-\nmizing a reconstruction loss further improves performance.\nOur proposed guided learning is completely unsupervised\nand achieves competitive or superior performance to state-\nof-the-art, real time approaches on standard benchmarks.\n2. Method\nGiven an adjacent frame pair I1 and I2, our goal is to\nlearn a model that can estimate the per-pixel motion ﬁeld\n(U, V ) between the two images accurately and efﬁciently.\narXiv:1702.02295v2  [cs.CV]  1 Jul 2017\nU and V are the horizontal and vertical displacements, re-\nspectively.\nWe describe our proxy ground truth guided\nframework in Section 2.1, and the unsupervised ﬁne tuning\nstrategy in Section 2.2.\n2.1. Proxy Ground Truth Guidance\nCurrent approaches to the supervised training of CNNs\nfor estimating optical ﬂow use synthetic ground truth\ndatasets. These synthetic motions/scenes are quite differ-\nent from real ones which limits the generalizability of the\nlearned models. And, even constructing synthetic dataset\nrequires a lot of manual effort [6]. The current largest syn-\nthetic datasets with dense ground truth optical ﬂow, Flying\nChairs [7] and FlyingThings3D [16], consist of only 22k\nimage pairs which is not ideal for deep learning especially\nfor such an ill-conditioned problem as motion estimation.\nIn order for CNN-based optical ﬂow estimation to reach its\nfull potential, a learning framework is needed that can scale\nthe size of the training data. Unsupervised learning is one\nideal way to achieve this scaling because it does not require\nground truth ﬂow.\nClassical approaches to optical ﬂow estimation are un-\nsupervised in that there is no learning process involved\n[11, 4, 5, 2, 12]. They only require the image pairs as in-\nput, with some extra assumptions (like image brightness\nconstancy, gradient constancy, smoothness) and informa-\ntion (like motion boundaries, dense image matching). These\nnon-CNN based classical methods currently achieve the\nbest performance on standard benchmarks and are thus con-\nsidered the state-of-the-art. Inspired by their good perfor-\nmance, we conjecture that these approaches can be used to\ngenerate proxy ground truth data for training CNN-based\noptical ﬂow estimators.\nIn this work, we choose FlowFields [2] as our classical\noptical ﬂow estimator. To our knowledge, it is one of the\nmost accurate ﬂow estimators among the published work.\nWe hope that by using FlowFields to generate proxy ground\ntruth, we can learn to estimate motion between image pairs\nas effectively as using the true ground truth.\nFor fair comparison, we use the “FlowNet Simple” net-\nwork as descried in [7] as our supervised CNN architecture.\nThis allows us to compare our guided learning approach to\nusing the true ground truth, particularly with respect to how\nwell the learned models generalize to other datasets. We\nuse endpoint error (EPE) as our guided loss since it is the\nstandard error measure for optical ﬂow evaluation\nLepe = 1\nN\nX p\n(U −U ′)2 + (V −V ′)2,\n(1)\nwhere N denotes the total number of pixels in I1. U and V\nare the proxy ground truth ﬂow ﬁelds while U ′ and V ′ are\nthe ﬂow estimates from the CNN.\n2.2. Unsupervised Fine Tuning\nAs stated in Section 1, a potential drawback to using\nclassical approaches to create training data is that the qual-\nity of this data will necessarily be limited by the accuracy\nof the estimator. If a classical approach fails to detect cer-\ntain motion patterns, a network trained on the proxy ground\ntruth is also likely to miss these patterns. This leads us to\nask if there is other unsupervised guidance that can improve\nthe network training?\nThe unsupervised approach of [20] treats optical ﬂow es-\ntimation as an image reconstruction problem based on the\nintuition that if the estimated ﬂow and the next frame can\nbe used to reconstruct the current frame then the network\nhas learned useful representations of the underlying mo-\ntions. During training, the loss is computed as the pho-\ntometric error between the true current frame I1 and the\ninverse-warped next frame I′\n1\nLreconst = 1\nN\nN\nX\ni,j\nρ(I1(i, j) −I′\n1(i, j)),\n(2)\nwhere I′\n1(i, j) = I2(i + Ui,j, j + Vi,j). The inverse warp\nis performed using a spatial transformer module [13] inside\nthe CNN. We use a robust convex error function, the gen-\neralized Charbonnier penalty ρ(x) = (x2 + ϵ2)α, to reduce\nthe inﬂuence of outliers. This reconstruction loss is similar\nto the brightness constancy objective in classical variational\nformulations but is quite different from the EPE loss in the\nproxy ground truth guided learning. We thus propose ﬁne\ntuning our model using this reconstruction loss as an addi-\ntional unsupervised guide.\nDuring ﬁne tuning, the total energy we aim to minimize\nis a simple weighted sum of the EPE loss and the image\nreconstruction loss\nL(U, V ; I1, I2) = Lepe + λ · Lreconst,\n(3)\nwhere λ controls the level of reconstruction guidance. Note\nthat we could add additional unsupervised guides like a\ngradient constancy assumption or an edge-aware weighted\nsmoothness loss [10] to further ﬁne tune our models.\nAn overview of our guided learning framework with both\nthe proxy ground truth guidance and the unsupervised ﬁne\ntuning is illustrated in Fig. 1.\n3. Experiments\n3.1. Datasets\nFlying Chairs [7] is a synthetic dataset designed specif-\nically for training CNNs to estimate optical ﬂow. It is cre-\nated by applying afﬁne transformations to real images and\nsynthetically rendered chairs. The dataset contains 22,872\nimage pairs: 22,232 training and 640 test samples according\nto the standard evaluation split.\nMPI Sintel [6] is also a synthetic dataset derived from a\nshort open source animated 3D movie.\nThere are 1,628\nframes, 1,064 for training and 564 for testing. It is the most\nwidely adopted benchmark to compare optical ﬂow estima-\ntors. In this work, we only report performance on its ﬁnal\npass because it contains sufﬁciently realistic scenes includ-\ning natural image degradations.\nKITTI Optical Flow 2012 [9] is a real world dataset col-\nlected from a driving platform. It consists of 194 training\nimage pairs and 195 test pairs with sparse ground truth ﬂow.\nWe report the average EPE in total for the test set.\nWe consider guided learning with and without ﬁne tun-\ning. In the no ﬁne tuning regime, the model is trained using\nthe proxy ground truth produced using a classical estimator.\nIn the ﬁne tuning regime, the model is ﬁrst trained using\nthe proxy ground truth and then ﬁne tuned using both the\nproxy ground truth and the reconstruction guide. The Sintel\nand KITTI datasets are too small to produce enough proxy\nground truth to train our model from scratch so the models\nevaluated on these datasets are ﬁrst pretrained on the Chairs\ndataset. These models are then either applied to the Sin-\ntel and KITTI datasets without ﬁne tuning or are ﬁne tuned\nusing the target dataset (proxy ground truth).\n3.2. Implementation\nAs shown in Fig. 1, our architecture consists of con-\ntractive and expanding parts. In the no ﬁne tuning learning\nregime, we calculate the per-pixel EPE loss for each expan-\nsion. There are 5 expansions resulting in 5 losses. We use\nthe same loss weights as in [7]. The models are trained us-\ning Adam optimization with the default parameter values\nβ1 = 0.9 and β2 = 0.999. The initial learning rate is set\nto 10−4 and divided by half every 100k iterations after the\nﬁrst 300k. We end our training at 600k iterations.\nIn the ﬁne tuning learning regime, we calculate both the\nEPE and reconstruction loss for each expansion. Thus there\nare a total of 10 losses. The generalized Charbonnier pa-\nrameter α is set to 0.25 in the reconstruction loss. λ is 0.1.\nWe use the default Adam optimization with a ﬁxed learning\nrate of 10−6 and training is stopped at 10k iterations.\nWe apply the same intensive data augmentation as in\n[7] to prevent over-ﬁtting in both learning regimes. The\nproxy ground truth is computed using the FlowFields binary\nkindly provided by authors in [2].\n3.3. Results and Discussion\nWe have three observations given the results in Table 1.\nObservation 1: We can use proxy ground truth generated\nby state-of-the-art classical ﬂow estimators to train CNNs\nfor optical ﬂow prediction. A model trained using the Flow-\nFields proxy ground truth achieves an average EPE of 3.34\non Chairs which is comparable to the 2.71 achieved by the\nmodel trained using the true ground truth. Note that the\nMethod\nChairs\nSintel\nKITTI\nFlowFields [2]\n2.45\n5.81\n3.5\nFlowNetS (Ground Truth) [7]\n2.71\n8.43\n9.1\nUnsupFlowNet [20]\n5.30\n11.19\n11.3\nFlowNetS (FlowFields)\n3.34\n8.05\n9.7\nFlowNetS (FlowFields) + Unsup\n3.01\n7.96\n9.5\nTable 1. Results reported using average EPE, lower is better. Bot-\ntom section shows our guided learning results, the models are\ntrained using the FlowFields proxy ground truth. The last row\nincludes ﬁne tuning.\nproxy ground truth is still quite noisy with an average EPE\nof 2.45 away from the true ground truth.\nThe model trained using the FlowFields proxy ground\ntruth (EPE 3.34) performs worse than the FlowFields esti-\nmator (EPE 2.45), which is expected. This is because Flow-\nFields adopts a hierarchical approach which is non-local in\nthe image space. It also uses dense correspondence to cap-\nture image details. Thus, FlowFields itself can output crisp\nmotion boundaries and accurate ﬂow. However, unlike the\nCNN model, it cannot run in real time.\nObservation 2: Sometime, training using proxy ground\ntruth can generalize better than training using the true\nground truth. The model trained using the Chairs proxy\nground truth (computed with FlowFields) performs better\n(EPE 8.05) on Sintel than the model trained using the Chairs\ntrue ground truth (EPE 8.43). We make similar observations\nfor KITTI1. This improved generalization might result from\nover-ﬁtting when training with the true ground truth since\nthe three datasets are quite different with respect to object\nand motion types. The proxy is noisier which could serve\nas a form of data augmentation for unseen motion types.\nIn addition, we experiment on directly training a Sin-\ntel model from scratch without using the pretrained Chairs\nmodel. We use the same implementation details. The per-\nformance is about one and half pixel worse in terms of EPE\nthan using the pretrained model.\nTherefore, pretraining\nCNNs on a large dataset (with either true or proxy ground\ntruth data) is important for optical ﬂow estimation.\nObservation 3: Our proposed ﬁne tuning regime improves\nperformance on all three datasets. Fine tuning results in an\naverage EPE decrease from 3.34 to 3.01 for Chairs, 8.05 to\n7.96 for Sintel, and 9.7 to 9.5 for KITTI. Note that an aver-\nage EPE of 3.01 for Chairs is very close to performance of\nthe supervised model FlowNetS (EPE 2.71). This demon-\nstrates that image reconstruction loss is effective as an ad-\nditional unsupervised guide for motion learning. It can act\nlike ﬁne tuning without requiring ground truth ﬂow of the\ntarget dataset.\nWe also investigate training a network from scratch using\na joint training regime. That is, using both Lepe and Lreconst,\nnot only using Lreconst in the ﬁne tuning stage. The per-\n1Note that FlowNetS’s performance on KITTI (EPE 9.1) is ﬁne tuned.\nImages\nGround Truth\nUnsupFlowNet\nFlowNetS\nOurs\nFigure 2. Visual examples of predicted optical ﬂow from different methods. Top two are from Sintel, and bottom two from KITTI.\nformance was worse on all three benchmarks. The reason\nmight be that pretraining using just the proxy ground truth\nprevents the model from becoming trapped in local minima.\nIt thus can provide a good initialization for further network\nlearning. A joint training regime using both losses may hurt\nthe network’s convergence in the beginning.\nHowever, we expect unsupervised learning to bring more\ncomplementarity. Image reconstruction loss may not be the\nmost appropriate guidance for learning optical ﬂow predic-\ntion. We will explore how to best incorporate additional\nunsupervised objectives in future work.\n3.4. Comparison to State-of-the-Art\nWe compare our proposed method to recent state-of-the-\nart approaches. We only consider approaches that are fast\nbecause optical ﬂow is often used in time sensitive applica-\ntions. We evaluated all CNN-based approaches on a work-\nstation with Intel Core I7 with 4.00GHz and a Nvidia Ti-\ntan X GPU. For classical approaches, we just use their re-\nported runtime. As shown in Table 2, our method performs\nthe best for Sintel even though it does not require the true\nground truth for training. For Chairs, we achieve on par\nperformance with [7]. For KITTI, we perform inferior to\n[19]. This is likely because the ﬂow in KITTI is caused\npurely by the motion of the car so the segmentation into lay-\ners performed in [19] helps in capturing motion boundaries.\nOur approach outperforms the state-of-the-art unsupervised\napproaches of [1, 20] by a large margin, thus demonstrat-\ning the effectiveness of our proposed guided learning us-\ning proxy ground truth and image reconstruction. Visual\ncomparison of Sintel and KITTI results are shown in Fig.\n2. We can see that UnsupFlowNet [20] is able to produce\nreasonable ﬂow ﬁelds estimation, but quite noisy. And it\ndoesn’t perform well in highly saturated and very dark re-\ngions. Our results are much more detailed and smoothed\ndue to the proxy guidance and unsupervised ﬁne tuning.\nMethod\nChairs\nSintel\nKITTI\nRuntime\nEPPM [3]\n−\n8.38\n9.2\n0.25\nPCA-Flow [19]\n−\n8.65\n6.2\n0.19∗\nDIS-Fast [14]\n−\n10.13\n14.4\n0.02∗\nFlowNetS [7]\n2.71\n8.43\n9.1\n0.06\nUnsupFlowNet [20]\n5.30\n11.19\n11.3\n0.06\nUSCNN [1]\n−\n8.88\n−\n−\nOurs\n3.01\n7.96\n9.5\n0.06\nTable 2. State-of-the-art comparison, runtime is reported in sec-\nonds per frame. Top: Classical approaches. Middle: CNN-based\napproaches. Bottom: Ours. ∗indicates the algorithm is evaluated\nusing CPU, while the rest are on GPU.\n4. Conclusion\nWe propose a guided optical ﬂow learning framework\nwhich is unsupervised and results in an estimator that can\nrun in real time. We show that proxy ground truth data pro-\nduced using state-of-the-art classical estimators can be used\nto train CNNs. This allows the training sets to scale which\nis important for deep learning. We also show that training\nusing proxy ground truth can result in better generalization\nthan training using the true ground truth. And, ﬁnally, we\nalso show that an unsupervised image reconstruction loss\ncan provide further learning guidance.\nMore broadly, we introduce a paradigm which can be in-\ntegrated into future state-of-the-art motion estimation net-\nworks [17] to improve performance. In future work, we\nplan to experiment with large-scale video corpora to learn\nnon-rigid real world motion patterns rather than just learn-\ning limited motions found in synthetic datasets.\nAcknowledgements This work was funded in part by a Na-\ntional Science Foundation CAREER grant, #IIS-1150115.\nWe gratefully acknowledge NVIDIA Corporation through\nthe donation of the Titan X GPU used in this work.\nReferences\n[1] A. Ahmadi and I. Patras. Unsupervised Convolutional Neu-\nral Networks for Motion Estimation. In ICIP, 2016.\n[2] C. Bailer, B. Taetz, and D. Stricker. Flow Fields: Dense Cor-\nrespondence Fields for Highly Accurate Large Displacement\nOptical Flow Estimation. In ICCV, 2015.\n[3] L. Bao, Q. Yang, and H. Jin. Fast Edge-Preserving Patch-\nMatch for Large Displacement Optical Flow.\nIn CVPR,\n2014.\n[4] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert.\nHigh\nAccuracy Optical Flow Estimation Based on a Theory for\nWarping. In ECCV, 2004.\n[5] T. Brox and J. Malik. Large Displacement Optical Flow: De-\nscriptor Matching in Variational Motion Estimation. PAMI,\n33:500–513, March 2011.\n[6] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A Nat-\nuralistic Open Source Movie for Optical Flow Evaluation. In\nECCV, 2012.\n[7] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazrbas,\nV. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. FlowNet:\nLearning Optical Flow with Convolutional Networks.\nIn\nICCV, 2015.\n[8] K. Fragkiadaki, P. Arbelez, P. Felsen, and J. Malik. Learning\nto Segment Moving Objects in Videos. In CVPR, 2015.\n[9] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for Au-\ntonomous Driving? The KITTI Vision Benchmark Suite. In\nCVPR, 2012.\n[10] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised\nMonocular Depth Estimation with Left-Right Consistency.\narXiv preprint arXiv:1609.03677, 2016.\n[11] B. K. Horn and B. G. Schunck. Determining Optical Flow.\nArtiﬁcial Intelligence, 17:185–203, 1981.\n[12] Y. Hu, R. Song, and Y. Li. Efﬁcient Coarse-to-Fine Patch-\nMatch for Large Displacement Optical Flow.\nIn CVPR,\n2016.\n[13] M.\nJaderberg,\nK.\nSimonyan,\nA.\nZisserman,\nand\nK. Kavukcuoglu.\nSpatial Transformer Network.\nIn\nNIPS, 2015.\n[14] T. Kroeger, R. Timofte, D. Dai, and L. V. Gool. Fast Optical\nFlow using Dense Inverse Searchn. In ECCV, 2016.\n[15] M. Mathieu, C. Couprie, and Y. LeCun. Deep Multi-Scale\nVideo Prediction beyond Mean Square Error. In ICLR, 2016.\n[16] N. Mayer, E. Ilg, P. Husser, P. Fischer, D. Cremers, A. Doso-\nvitskiy, and T. Brox. A Large Dataset to Train Convolutional\nNetworks for Disparity, Optical Flow, and Scene Flow Esti-\nmation. In CVPR, 2016.\n[17] A. Ranjan and M. J. Black. Optical Flow Estimation using a\nSpatial Pyramid Network. arXiv preprint arXiv:1611.00850,\n2016.\n[18] K. Simonyan and A. Zisserman. Two-Stream Convolutional\nNetworks for Action Recognition in Videos. In NIPS, 2014.\n[19] J. Wulff and M. Black.\nEfﬁcient Sparse-to-Dense Opti-\ncal Flow Estimation using a Learned Basis and Layers. In\nCVPR, 2015.\n[20] J. J. Yu, A. W. Harley, and K. G. Derpanis. Back to Basics:\nUnsupervised Learning of Optical Flow via Brightness Con-\nstancy and Motion Smoothness. In ECCVW, 2016.\n[21] Y. Zhu, Z. Lan, S. Newsam, and A. G. Hauptmann. Hid-\nden Two-Stream Convolutional Networks for Action Recog-\nnition. arXiv preprint arXiv:1704.00389, 2017.\n[22] Y. Zhu and S. Newsam. Depth2Action: Exploring Embed-\nded Depth for Large-Scale Action Recognition. In ECCV\nWorkshop, 2016.\n[23] Y. Zhu and S. Newsam. DenseNet for Dense Flow. In ICIP,\n2017.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-02-08",
  "updated": "2017-07-01"
}