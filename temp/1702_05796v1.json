{
  "id": "http://arxiv.org/abs/1702.05796v1",
  "title": "Collaborative Deep Reinforcement Learning",
  "authors": [
    "Kaixiang Lin",
    "Shu Wang",
    "Jiayu Zhou"
  ],
  "abstract": "Besides independent learning, human learning process is highly improved by\nsummarizing what has been learned, communicating it with peers, and\nsubsequently fusing knowledge from different sources to assist the current\nlearning goal. This collaborative learning procedure ensures that the knowledge\nis shared, continuously refined, and concluded from different perspectives to\nconstruct a more profound understanding. The idea of knowledge transfer has led\nto many advances in machine learning and data mining, but significant\nchallenges remain, especially when it comes to reinforcement learning,\nheterogeneous model structures, and different learning tasks. Motivated by\nhuman collaborative learning, in this paper we propose a collaborative deep\nreinforcement learning (CDRL) framework that performs adaptive knowledge\ntransfer among heterogeneous learning agents. Specifically, the proposed CDRL\nconducts a novel deep knowledge distillation method to address the\nheterogeneity among different learning tasks with a deep alignment network.\nFurthermore, we present an efficient collaborative Asynchronous Advantage\nActor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into\nthe online training of agents, and demonstrate the effectiveness of the CDRL\nframework using extensive empirical evaluation on OpenAI gym.",
  "text": "Collaborative Deep Reinforcement Learning\nKaixiang Lin\nComputer Science and Engineering\nMichigan State University\n428 S Shaw Ln.\nEast Lansing, MI 48824\nlinkaixi@msu.edu\nShu Wang\nComputer Science\nRutgers University\n57 US Highway 1\nNew Brunswick, NJ 088901\nsw498@cs.rutgers.edu\nJiayu Zhou\nComputer Science and Engineering\nMichigan State University\n428 S Shaw Ln.\nEast Lansing, MI 48824\njiayuz@msu.edu\nABSTRACT\nBesides independent learning, human learning process is highly\nimproved by summarizing what has been learned, communicating\nit with peers, and subsequently fusing knowledge from diﬀerent\nsources to assist the current learning goal. Tis collaborative learn-\ning procedure ensures that the knowledge is shared, continuously\nreﬁned, and concluded from diﬀerent perspectives to construct a\nmore profound understanding. Te idea of knowledge transfer has\nled to many advances in machine learning and data mining, but\nsigniﬁcant challenges remain, especially when it comes to rein-\nforcement learning, heterogeneous model structures, and diﬀerent\nlearning tasks. Motivated by human collaborative learning, in\nthis paper we propose a collaborative deep reinforcement learn-\ning (CDRL) framework that performs adaptive knowledge transfer\namong heterogeneous learning agents. Speciﬁcally, the proposed\nCDRL conducts a novel deep knowledge distillation method to ad-\ndress the heterogeneity among diﬀerent learning tasks with a deep\nalignment network. Furthermore, we present an eﬃcient collabo-\nrative Asynchronous Advantage Actor-Critic (cA3C) algorithm to\nincorporate deep knowledge distillation into the online training of\nagents, and demonstrate the eﬀectiveness of the CDRL framework\nusing extensive empirical evaluation on OpenAI gym.\nCCS CONCEPTS\n•Computing methodologies →Machine learning; Reinforce-\nment learning; Transfer learning;\nKEYWORDS\nKnowledge distillation; Transfer learning; Deep reinforcement\nlearning\nACM Reference format:\nKaixiang Lin, Shu Wang, and Jiayu Zhou. 1997. Collaborative Deep Rein-\nforcement Learning. In Proceedings of ACM Woodstock conference, El Paso,\nTexas USA, July 1997 (WOODSTOCK’97), 9 pages.\nDOI: 10.475/123 4\n1\nINTRODUCTION\nIt is the development of cognitive abilities including learning, re-\nmembering, communicating that enables human to conduct social\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nWOODSTOCK’97, El Paso, Texas USA\n© 2016 Copyright held by the owner/author(s). 123-4567-24-567/08/06...$15.00\nDOI: 10.475/123 4\nAgent\nAgent\nAgent\nAgent\nHomogeneous\nInteraction\nAgent\nAgent\nAgent\nAgent\nHomogeneous\nInteraction\nHeterogeneous Interaction\nEnvironment\nAgent\nAgent model\nDeep\nknowledge\ndistilling\nInteract with\nheterogeneous \nagents\nLearn from \nenvironment\nInteract\nwith\nenvironment\nLearn from \ndistilled \nknowledge\nTarget (student)\nAgent\nAgent\nAgent\n...\nSources (teachers)\nFigure 1: Te illustration of Collaborative Deep Reinforce-\nment Learning Framework.\ncooperation, which is the key to the rise of humankind. As a social\nanimal, the ability to collaborate awoke the cognitive revolution\nand reveals the prosperous history of human [14]. In disciplines of\ncognitive science, education and psychology, collaborative learning,\na situation in which a group of people learn to achieve a set of tasks\ntogether, has been advocated throughout previous studies [9]. It is\nintuitive to illustrate the concept of collaborative learning in the\nexample of group study. A group of students are studying together\nto master some challenging course materials. As each student may\nunderstand the materials from a distinctive perspective, eﬀective\ncommunication would greatly help the entire group achieve a bet-\nter understanding than those from independent study, and could\nsigniﬁcantly improve the eﬃciency and eﬀectiveness of learning\nprocess, as well [12].\nOn the other hand, the study of human learning has largely ad-\nvanced the design of machine learning and data mining algorithms,\nespecially in reinforcement learning and transfer learning. Te\nrecent success of deep reinforcement learning (DRL) has atracted\nincreasing atention from the community, as DRL can discover very\ncompetitive strategies by having learning agents interacting with\na given environment and using rewards from the environment as\nthe supervision (e.g., [16, 18, 20, 28]). Even though most of current\nresearch on DRL has focused on learning from games, it possesses\ngreat transformative power to impact many industries with data\nmining and machine learning techniques such as clinical decision\nsupport [32], marketing [2], ﬁnance [1], visual navigation [37], and\nautonomous driving [8]. Although there are many existing eﬀorts\ntowards eﬀective algorithms for DRL [19, 21], the computational\ncost still imposes signiﬁcant challenges as training DRL for even a\narXiv:1702.05796v1  [cs.LG]  19 Feb 2017\nWOODSTOCK’97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\nsimple game such as Pong [5] remains very expensive. Te under-\nlying reasons for the obstacle of eﬃcient training mainly lie in two\naspects: First, the supervision (rewards) from the environment is\nvery sparse and implicit during training. It may take an agent hun-\ndreds or even thousands actions to get a single reward, and which\nactions that actually lead to this reward are ambiguous. Besides the\ninsuﬃcient supervision, training deep neural network itself takes\nlots of computational resources.\nDue to the aforementioned diﬃculties, performing knowledge\ntransfer from other related tasks or well-trained deep models to facil-\nitate training has drawn lots of atention in the community [16, 24–\n26, 31]. Existing transfer learning can be categorized into two\nclasses according to the means that knowledge is transferred: data\ntransfer [15, 24, 26] and model transfer [10, 24, 34, 35]. Model trans-\nfer methods implement knowledge transfer from introducing in-\nductive bias during the learning, and has been extensively studied\nin both transfer learning/multi-task learning (MTL) community\nand deep learning community. For example, in the regularized\nMTL models such as [11, 36], tasks with the same feature space are\nrelated through some structured regularization. Another example\nis the multi-task deep neural network, where diﬀerent tasks share\nparts of the network structures [35]. One obvious disadvantage\nof model transfer is the lack of ﬂexibility: usually the feasibility\nof inductive transfer has largely restricted the model structure of\nlearning task, which makes it not practical in DRL because for\ndiﬀerent tasks the optimal model structures may be radically diﬀer-\nent. On the other hand, the recently developed data transfer (also\nknown as knowledge distillation or mimic learning) [15, 24, 26]\nembeds the source model knowledge into data points. Ten they\nare used as knowledge bridge to train target models, which can\nhave diﬀerent structures as compared to the source model [6, 15].\nBecause of the structural ﬂexibility, the data transfer is especially\nsuitable to deal with structure variant models.\nTere are two situations that transfer learning methods are es-\nsential in DRL:\nCertiﬁcated heterogeneous transfer. Training a DRL agent is\ncomputational expensive. If we have a well-trained model, it will be\nbeneﬁcial to assist the learning of other tasks by transferring knowl-\nedge from this model. Terefore we consider following research\nquestion: Given one certiﬁcated task (i.e. the model is well-designed,\nextensively trained and performs very well), how can we maximize\nthe information that can be used in the training of other related\ntasks? Some model transfer approaches directly use the weights\nfrom the trained model to initialize the new task [24], which can\nonly be done when the model structures are the same. Tus, this\nstrict requirement has largely limited its general applicability on\nDRL. On the other hand, the initialization may not work well if the\ntasks are signiﬁcantly diﬀerent from each other in nature [24]. Tis\nchallenge could be partially solved by generating an intermediate\ndataset (logits) from the existing model to help learning the new\ntask. However, new problems would arise when we are transfer-\nring knowledge between heterogeneous tasks. Not only the action\nspaces are diﬀerent in dimension, the intrinsic action probability\ndistributions and semantic meanings of two tasks could diﬀer a\nlot. Speciﬁcally, one action in Pong may refer to move the pad-\ndle upwards while the same action index in Riverraid [5] would\ncorrespond to ﬁre. Terefore, the distilled dataset generated from\nthe trained source task cannot be directly used to train the hetero-\ngeneous target task. In this scenario, the ﬁrst key challenge we\nidentiﬁed in this work is that how to conduct data transfer among\nheterogeneous tasks so that we can maximally utilize the informa-\ntion from a certiﬁcated model while still maintain the ﬂexibility of\nmodel design for new tasks. During the transfer, the transferred\nknowledge from other tasks may contradict to the knowledge that\nagents learned from its environment. One recently work [25] use an\natention network selective eliminate transfer if the contradiction\npresents, which is not suitable in this seting since we are given a\ncertiﬁcated task to transfer. Hence, the second challenge is how to\nresolve the conﬂict and perform a meaningful transfer.\nLack of expertise. A more general desired but also more chal-\nlenging scenario is that DRL agents are trained for multiple het-\nerogeneous tasks without any pre-trained models available. One\nfeasible way to conduct transfer under this scenario is that agents\nof multiple tasks share part of their network parameters [26, 35].\nHowever, an inevitable drawback is, multiple models lose their\ntask-speciﬁc designs since the shared part needs to be the same.\nAnother solution is to learn a domain invariant feature space shared\nby all tasks [3]. However, some task-speciﬁc information is ofen\nlost while converting the original state to a new feature subspace.\nIn this case, an intriguing questions is that: can we design a frame-\nwork that fully utilizes the original environment information and\nmeanwhile leverages the knowledge transferred from other tasks?\nTis paper investigates the aforementioned problems system-\natically and proposes a novel Collaborative Deep Reinforcement\nLearning (CDRL) framework (illustrated in Figure 1) to resolve\nthem. Our major contribution is threefold:\n• First, in order to transfer knowledge among heterogeneous\ntasks while remaining the task-speciﬁc design of model struc-\nture, a novel deep knowledge distillation is proposed to address\nthe heterogeneity among tasks, with the utilization of deep\nalignment network designed for the domain adaptation.\n• Second, in order to incorporate the transferred knowledge from\nheterogeneous tasks into the online training of current learning\nagents, similar to human collaborative learning, an eﬃcient\ncollaborative asynchronously advantage actor-critic learning\n(cA3C) algorithm is developed under the CDRL framework. In\ncA3C, the target agents are able to learn from environments\nand its peers simultaneously, which also ensure the information\nfrom original environment is suﬃciently utilized. Further, the\nknowledge conﬂict among diﬀerent tasks is resolved by adding\nan extra distillation layer to the policy network under CDRL\nframework, as well.\n• Last but not least we present extensive empirical studies on\nOpenAI gym to evaluate the proposed CDRL framework and\ndemonstrate its eﬀectiveness by achieving more than 10% per-\nformance improvement compared to the current state-of-the-\nart.\nNotations: In this paper, we use teacher network/source task\ndenotes the network/task contained the knowledge to be transferred\nto others. Similarly, the student network/target task is referred\nto those tasks utilizing the knowledge transferred from others to\nfacilitate its own training. Te expert network denotes the network\nthat has already reached a relative high averaged reward in its own\nCollaborative Deep Reinforcement Learning\nWOODSTOCK’97, July 1997, El Paso, Texas USA\nenvironment. In DRL, an agent is represented by a policy network\nand a value network that share a set of parameters. Homogeneous\nagents denotes agents that perform and learn under independent\ncopies of same environment. Heterogeneous agents refer to those\nagents that are trained in diﬀerent environments.\n2\nRELATED WORK\nMulti-agent learning. One closely related area to our work is\nmulti-agent reinforcement learning. A multi-agent system includes\na set of agents interacting in one environment. Meanwhile they\ncould potentially interact with each other [7, 13, 17, 30]. In collabo-\nrative multi-agent reinforcement learning, agents work together to\nmaximize a shared reward measurement [13, 17]. Tere is a clear\ndistinction between the proposed CDRL framework and multi-agent\nreinforcement learning. In CDRL, each agent interacts with its own\nenvironment copy and the goal is to maximize the reward of the\ntarget agents. Te formal deﬁnition of the proposed framework is\ngiven in Section 4.1.\nTransfer learning. Another relevant research topic is domain\nadaption in the ﬁeld of transfer learning [23, 29, 33]. Te authors\nin [29] proposed a two-stage domain adaptation framework that\nconsiders the diﬀerences among marginal probability distributions\nof domains, as well as conditional probability distributions of tasks.\nTe method ﬁrst re-weights the data from the source domain using\nMaximum Mean Discrepancy and then re-weights the predictive\nfunction in the source domain to reduce te diﬀerence on conditional\nprobabilities. In [33], the marginal distributions of the source and\nthe target domain are aligned by training a network, which maps\ninputs into a domain invariant representation. Also, knowledge\ndistillation was directly utilized to align the source and target class\ndistribution. One clear limitation here is that the source domain\nand the target domain are required to have the same dimensionality\n(i.e. number of classes) with same semantics meanings, which is\nnot the case in our deep knowledge distillation.\nIn [3], an invariant feature space is learned to transfer skills\nbetween two agents. However, projecting the state into a feature\nspace would lose information contained in the original state. Tere\nis a trade-oﬀbetween learning the common feature space and\npreserving the maximum information from the original state. In\nour work, we use data generated by intermediate outputs in the\nknowledge transfer instead of a shared space. Our approach thus\nretains complete information from the environment and ensures\nhigh quality transfer. Te recently proposed A2T approach [25]\ncan avoid negative transfer among diﬀerent tasks. However, it\nis possible that some negative transfer cases may because of the\ninappropriate design of transfer algorithms. In our work, we show\nthat we can perform successful transfer among tasks that seemingly\ncause negative transfer.\nKnowledge transfer in deep learning. Since the training of\neach agent in an environment can be considered as a learning task,\nand the knowledge transfer among multiple tasks belongs to the\nstudy of multi-task learning. Te multi-task deep neural network\n(MTDNN) [35] transfers knowledge among tasks by sharing pa-\nrameters of several low-level layers. Since the low-level layers can\nbe considered to perform representation learning, the MTDNN is\nlearning a shared representation for inputs, which is then used\nby high-level layers in the network. Diﬀerent learning tasks are\nrelated to each other via this shared feature representation. In the\nproposed CDRL, we do not use the share representation due to\nthe inevitable information loss when we project the inputs into a\nshared representation. We instead perform explicitly knowledge\ntransfer among tasks by distilling knowledge that are independent\nof model structures. In [15], the authors proposed to compress\ncumbersome models (teachers) to more simple models (students),\nwhere the simple models are trained by a dataset (knowledge) dis-\ntilled from the teachers. However, this approach cannot handle the\ntransfer among heterogeneous tasks, which is one key challenge\nwe addressed in this paper.\nKnowledge transfer in deep reinforcement learning. Knowl-\nedge transfer is also studied in deep reinforcement learning. [19]\nproposed multi-threaded asynchronous variants of several most\nadvanced deep reinforcement learning methods including Sarsa,\nQ-learning, Q-learning and advantage actor-critic. Among all those\nmethods, asynchronous advantage actor-critic (A3C) achieves the\nbest performance. Instead of using experience replay as in previous\nwork, A3C stabilizes the training procedure by training diﬀerent\nagents in parallel using diﬀerent exploration strategies. Tis was\nshown to converge much faster than previous methods and use less\ncomputational resources. We show in Section 4.1 that the A3C is\nsubsumed to the proposed CDRL as a special case. In [24], a single\nmulti-task policy network is trained by utilizing a set of expert\nDeep Q-Network (DQN) of source games. At this stage, the goal is\nto obtain a policy network that can play source games as close to\nexperts as possible. Te second step is to transfer the knowledge\nfrom source tasks to a new but related target task. Te knowledge\nis transferred by using the DQN in last step as the initialization of\nthe DQN for the new task. As such, the training time of the new\ntask can be signiﬁcantly reduced. Diﬀerent from their approach,\nthe proposed transfer strategy is not to directly mimic experts’\nactions or initialize by a pre-trained model. In [26], knowledge\ndistillation was adopted to train a multi-task model that outper-\nforms single task models of some tasks. Te experts for all tasks are\nﬁrstly acquired by single task learning. Te intermediate outputs\nfrom each expert are then distilled to a similar multi-task network\nwith an extra controller layer to coordinate diﬀerent action sets.\nOne clear limitation is that major components of the model are\nexactly the same for diﬀerent tasks, which may lead to degraded\nperformance on some tasks. In our work, transfer can happen even\nwhen there are no experts available. Also, our method allow each\ntask to have their own model structures. Furthermore, even the\nmodel structures are the same for multiple tasks, the tasks are not\ntrained to improve the performance of other tasks (i.e. it does not\nmimic experts from other tasks directly). Terefore our model can\nfocus on maximizing its own reward, instead of being distracted by\nothers.\n3\nBACKGROUND\n3.1\nReinforcement Learning\nIn this work, we consider the standard reinforcement learning\nseting where each agent interacts with it’s own environment\nover a number of discrete time steps. Given the current state\nst ∈S at step t, agent дi selects an action at ∈A according\nWOODSTOCK’97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\nto its policy π(at |st ), and receives a reward rt+1 from the envi-\nronment. Te goal of the agent is to choose an action at at step t\nthat maximize the sum of future rewards {rt } in a decaying man-\nner: Rt = Í∞\ni=0 γ irt+i, where scalar γ ∈(0, 1] is a discount rate.\nBased on the policy π of this agent, we can further deﬁne a state\nvalue function V (st ) = E[Rt |s = st ], which estimates the expected\ndiscounted return starting from state st , taking actions following\npolicy π until the game ends. Te goal in reinforcement learning\nalgorithm is to maximize the expected return. Since we are mainly\ndiscussing one speciﬁc agent’s design and behavior throughout the\npaper, we leave out the notation of the agent index for conciseness.\n3.2\nAsynchronous Advantage actor-critic\nalgorithm (A3C)\nTe asynchronous advantage actor-critic (A3C) algorithm [19]\nlaunches multiple agents in parallel and asynchronously updates\na global shared target policy network π(a|s,θp) as well as a value\nnetwork V (s,θv). parametrized by θp and θv, respectively. Each\nagent interacts with the environment, independently. At each step\nt the agent takes an action based on the probability distribution\ngenerated by policy network. Afer playing a n-step rollout or\nreaching the terminal state, the rewards are used to compute the\nadvantage with the output of value function. Te updates of policy\nnetwork is conducted by applying the gradient:\n∇θp log π(at |st ;θp)A(st,at ;θv),\nwhere the advantage function A(st,at ;θv) is given by:\nÕT −t−1\ni=0\nγ irt+i + γT −tV (sT ;θv) −V (st ;θv).\nTerm T represents the step number for the last step of this rollout,\nit is either the max number of rollout steps or the number of steps\nfrom t to the terminal state. Te update of value network is to\nminimize the squared diﬀerence between the environment rewards\nand value function outputs, i.e.,\nmin\nθv\n(\nÕT −t−1\ni=0\nγ irt+i + γT −tV (sT ;θv) −V (st ;θv))2.\nTe policy network and the value network share the same layers\nexcept for the last output layer. An entropy regularization of policy\nπ is added to improve exploration, as well.\n3.3\nKnowledge distillation\nKnowledge distillation [15] is a transfer learning approach that\ndistills the knowledge from a teacher network to a student network\nusing a temperature parameterized ”sof targets” (i.e. a probabil-\nity distribution over a set of classes). It has been shown that it\ncan accelerate the training with less data since the gradient from\n”sof targets” contains much more information than the gradient\nobtained from ”hard targets” (e.g. 0, 1 supervision).\nTo be more speciﬁc, logits vector z ∈Rd for d actions can be\nconverted to a probability distribution h ∈(0, 1)d by a sofmax\nfunction, raised with temperature τ:\nh(i) = sofmax(z/τ)i =\nexp(z(i)/τ)\nÍ\nj exp(z(j)/τ),\n(1)\nwhere h(i) and z(i) denotes the i-th entry of h and z, respectively.\nTen the knowledge distillation can be completed by optimize\nthe following Kullback-Leibler divergence (KL) with temperature\nτ [15, 26].\nLKL(D,θ β\np ) =\nÕ\nt=1\nsofmax(zα\nt /τ) ln sofmax(zα\nt /τ)\nsofmax(zβ\nt )\n(2)\nwhere zα\nt is the logits vector from teacher network (notation\nα represents teacher) at step t, while zβ\nt is the logits vector from\nstudent network (notation β represents student) of this step. θ β\np\ndenotes the parameters of the student policy network. D is a set of\nlogits from teacher network.\n4\nCOLLABORATIVE DEEP REINFORCEMENT\nLEARNING FRAMEWORK\nIn this section, we introduce the proposed collaborative deep rein-\nforcement learning (CDRL) framework. Under this framework, a\ncollaborative Asynchronous Advantage Actor-Critic (cA3C) algo-\nrithm is proposed to conﬁrm the eﬀectiveness of the collaborative\napproach. Before we introduce our method in details, one underly-\ning assumption we used is as follows:\nAssumption 1. If there is a universe that contains all the tasks\nE = {e1,e2, ...,e∞} and ki represents the corresponding knowledge\nto master each task ei, then ∀i, j,ki ∩kj , ∅.\nTis is a formal description of our common sense that any pair\nof tasks are not absolutely isolated from each other, which has\nbeen implicitly used as a fundamental assumption by most prior\ntransfer learning studies [11, 24, 26].Terefore, we focus on mining\nthe shared knowledge across multiple tasks instead of providing\nstrategy selecting tasks that share knowledge as much as possible,\nwhich remains to be unsolved and may lead to our future work. Te\ngoal here is to utilize the existing knowledge as well as possible. For\nexample, we may only have a well-trained expert on playing Pong\ngame, and we want to utilize its expertise to help us perform beter\non other games. Tis is one of the situations that can be solved by\nour collaborative deep reinforcement learning framework.\n4.1\nCollaborative deep reinforcement learning\nIn deep reinforcement learning, since the training of agents are\ncomputational expensive, the well-trained agents should be further\nutilized as source agents (agents where we transferred knowledge\nfrom) to facilitate the training of target agents (agents that are\nprovided with the extra knowledge from source). In order to in-\ncorporate this type of collaboration to the training of DRL agents,\nwe formally deﬁne the collaborative deep reinforcement learning\n(CDRL) framework as follows:\nDeﬁnition 4.1. Givenm independent environments {ε1,ε2, ...,εm}\nofm tasks {e1,e2, ...,em} , the correspondingm agents {д1,д2, ...,дm}\nare collaboratively trained in parallel to maximize the rewards (mas-\nter each task) with respect to target agents.\n• Environments. Tere is no restriction on the environments: Te\nm environments can be totally diﬀerent or with some duplica-\ntions.\nCollaborative Deep Reinforcement Learning\nWOODSTOCK’97, July 1997, El Paso, Texas USA\nlogits 𝒛𝜶\nAligned logits 𝟊𝛉𝐰(𝐳𝛂)\nDistillation logits 𝒛𝜷′\nTraining\nStudent network\nTeacher network\nDeep\nalignment\nnetwork\nDistillation loss\nDistillation logits𝒛𝜷′\nParameter 𝜽𝒑\n𝜷′\nParameter 𝜽𝒑\n𝜷\nFully connected \npolicy layer\nFully connected \ndistillation layer\nSoftmax\nLSTM Network\nConvolutional \nNeural Network\nstate\nAction\nProbability\ndistribution\nPolicy logits\n𝒛𝜷\n(a) Distillation procedure\n(b) Student network structure.\nFigure 2: Deep knowledge distillation. In (a), the teacher’s output logits zα is mapped through a deep alignment network and\nthe aligned logits Fθ ω (zα ) is used as the supervision to train the student. . In (b), the extra fully connected layer for distillation\nis added for learning knowledge from teacher. For simplicity’s sake, time step t is omitted here.\n• In parallel. Each environment εi only interacts with the one\ncorresponding agent дi, i.e., the action aj\nt from agent дj at step\nt has no inﬂuence on the state si\nt+1 in εi, ∀i , j.\n• Collaboratively. Te training procedure of agent дi consists of in-\nteracting with environment εi and interacting with other agents\nas well. Te agent дi is not necessary to be at same level as\n”collaborative” deﬁned in cognitive science [9]. E.g., д1 can be\nan expert for task e1 (environment ε1) while he is helping agent\nд2 which is a student agent in task e2.\n• Target agents. Te goal of CDRL can be set as maximizing the\nrewards that agent дi obtains in environment εi with the help\nof interacting with other agents, similar to inductive transfer\nlearning where дi is the target agent for target task and others\nare source tasks. Te knowledge is transfered from source to\ntarget дi by interaction. When we set the goal to maximize\nthe rewards of multiple agents jointly, it is similar to multi-task\nlearning where all tasks are source tasks and target tasks at the\nsame time.\nNotice that our deﬁnition is very diﬀerent from the previously\ndeﬁned collaborative multiagent Markow Decision Process (col-\nlaborative multiagent MDP) [13, 17] where a set of agents select a\nglobal joint action to maximize the sum of their individual rewards\nand the environment is transited to a new state based on that joint\naction. First, MDP is not a requirement in CDRL framework. Sec-\nond, in CDRL, each agent has its own copy of environment and\nmaximizes its own cumulative rewards. Te goal of collaboration\nis to improve the performance of collaborative agents, compared\nwith isolated ones, which is diﬀerent from maximizing the sum\nof global rewards in collaborative multiagent MDP. Tird, CDRL\nfocuses on how agents collaborate among heterogeneous environ-\nments, instead of how joint action aﬀects the rewards. In CDRL,\ndiﬀerent agents are acting in parallel, the actions taken by other\nagents won’t directly inﬂuence current agent’s rewards. While in\ncollaborative multiagent MDP, the agents must coordinate their\naction choices since the rewards will be directly aﬀected by the\naction choices of other agents.\nFurthermore, CDRL includes diﬀerent types of interaction, which\nmakes this a general framework. For example, the current state-of-\nthe-art is A3C [19] can be categorized as one homogeneous CDRL\nmethod with advantage actor-critic interaction. Speciﬁcally, multi-\nple agents in A3C are trained in parallel with the same environment.\nAll agents ﬁrst synchronize parameters from a global network, and\nthen update the global network with their individual gradients. Tis\nprocedure can be seen as each agent maintains its own model (a\ndiﬀerent version of global network) and interacts with other agents\nby sending and receiving gradients.\nIn this paper, we propose a novel interaction method named\ndeep knowledge distillation under the CDRL framework. It is worth\nnoting that the interaction in A3C only deals with the homogeneous\ntasks, i.e. all agents have the same environment and the same model\nstructure so that their gradients can be accumulated and interacted.\nBy deep knowledge distillation, the interaction can be conducted\namong heterogeneous tasks.\n4.2\nDeep knowledge distillation\nAs we introduced before, knowledge distillation [15] is trying to\ntrain a student network that can behave similarly to the teacher\nnetwork by utilizing the logits from the teacher as supervision.\nHowever, transferring the knowledge among heterogeneous tasks\nfaces several diﬃculties. First, the action spaces of diﬀerent tasks\nmay have diﬀerent dimensions. Second, even if the dimensionality\nof action space is same among tasks, the action probability distribu-\ntions for diﬀerent tasks could vary a lot, as we illustrated in Figure 5\n(a) and (b). Tus, the action paterns represented by the logits of\ndiﬀerent policy networks are usually diﬀerent from task to task.\nIf we directly force a student network to mimic the action patern\nof a teacher network for a diﬀerent task, it could be trained in a\nwrong direction, and ﬁnally ends up with worse performance than\nisolated training. In fact, this suspect has been empirically veriﬁed\nin our experiments.\nBased on the above observation, we propose deep knowledge\ndistillation to transfer knowledge between heterogeneous tasks.\nAs illustrated in Figure 2 (a), the approach for deep knowledge\ndistillation is straightforward. We use a deep alignment network to\nmap the logits of the teacher network from a heterogeneous source\ntask eα (environment εα ), then the logits is used as our supervision\nto update the student network of target task eβ (environment εβ).\nTis procedure is performed by minimizing following objective\nfunction over student policy network parameters θ β\np\n′\n:\nLKL(D,θ β\np\n′\n,τ) =\nÕ\nt\nlKL(Fθ ω (zα\nt ), zβ\nt\n′\n,τ),\n(3)\nwhere\nlKL(Fθ ω (zα\nt ), zβ\nt\n′\n,τ) = sofmax(Fθ ω (zα\nt )/τ) ln sofmax(Fθ ω (zα\nt )/τ)\nsofmax(zβ\nt\n′\n)\n.\nHere θω denotes the parameters of the deep alignment network,\nwhich transfers the logits zα\nt from the teacher policy network for\nWOODSTOCK’97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\nknowledge distillation by function Fθ ω (zα\nt ) at step t. As we show in\nFigure 2 (b), θ β\np is the student policy network parameters (including\nparameters of CNN, LSTM and policy layer) for task eβ, while θ β\np\n′\ndenotes student network parameters of CNN, LSTM and distillation\nlayer. It is clear that the distillation logits zβ\nt\n′ from the student\nnetwork does not determine the action probability distribution\ndirectly, which is established by the policy logits zβ\nt , as illustrated\nin Figure 2 (b). We add another fully connected distillation layer\nto deal with the mismatch of action space dimensionality and the\ncontradiction of the transferred knowledge from source domain\nand the learned knowledge from target domain. Te input to both\nof the teacher and the student network is the state of environment\nεβ of target task eβ . It means that we want to transfer the expertise\nfrom an expert of task eα towards the current state. Symbol D\nis a set of logits from the teacher network in one batch and τ is\nthe temperature same as described in Eq (1). In a trivial case that\nthe teacher network and the student network are trained for same\ntask (eα equals eβ), then the deep alignment network Fθ ω would\nreduce to an identity mapping, and the problem is also reduced to a\nsingle task policy distillation, which has been proved to be eﬀective\nin [26]. Before we can apply the deep knowledge distillation, we\nneed to ﬁrst train a good deep alignment network. In this work, we\nprovide two types of training protocols for diﬀerent situations:\nOﬀline training: Tis protocol ﬁrst trains two teacher networks\nin both environment εα and εβ . Ten we use the logits of both two\nteacher networks to train a deep alignment network Fθ ω . Afer\nacquiring a pre-trained Fθ ω , we train a student network of task eβ\nfrom scratch, in the meanwhile the teacher network of task eα and\nFθ ω are used for deep knowledge distillation.\nOnline training: Suppose we only have a teacher network of\ntask eα , and we want to use the knowledge from task eα to train\nthe student network for task eβ to get higher performance from\nscratch. Te pipeline of this method is that, we ﬁrstly train the\nstudent network by interacting with the environment εβ for a\ncertain amount of steps T1, and then start to train the alignment\nnetwork Fθ ω , using the logits from the teacher network and the\nstudent network. Aferwards, at step T2, we start performing deep\nknowledge distillation. ObviouslyT2 is larger thanT1, and the value\nof them are task-speciﬁc, which is decided empirically in this work.\nTe oﬄine training could be useful if we have already had a rea-\nsonably good model for task eβ , while we want to further improve\nthe performance using the knowledge from task eα . Te online\ntraining method is used when we need to learn the student network\nfrom scratch. Both types of training protocol can be extended to\nmultiple heterogeneous tasks.\n4.3\nCollaborative Asynchronous Advantage\nActor-Critic\nIn this section, we introduce the proposed collaborative asynchro-\nnous advantage actor-critic (cA3C) algorithm. As we described in\nsection 4.1, the agents are running in parallel. Each agent goes\nthrough the same training procedure as described in Algorithm 1.\nAs it shows, the training of agent д1 can be separated into two parts:\nTe ﬁrst part is to interact with the environment, get the reward\nand compute the gradients to minimize the value loss and policy\nloss based on Generalized Advantage Estimation (GAE) [27]. Te\nsecond part is to interact with source agent д2 so that the logits\ndistilled from agent д2 can be transferred by the deep alignment\nnetwork and used as supervision to bias the training of agent д1.\nTo be more concrete, the pseudo code in Algorithm 1 is an en-\nvolved version of A3C based on online training of deep knowledge\ndistillation. At T-th iteration, the agent interacts with the environ-\nment for tmax steps or until the terminal state is reached (Line 6 to\nLine 15). Ten the updating of value network and policy network is\nconducted by GAE. Tis variation of A3C is ﬁrstly implemented in\nOpenAI universe starter agent [22]. Since the main asynchronous\nframework is the same as A3C, we still use the A3C to denote this\nalgorithm although the updating is the not the same as advantage\nactor-critic algorithm used in original A3C paper [19].\nTe online training of deep knowledge distillation is mainly com-\npleted from Line 25 to Line 32 in Algorithm 1. Te training of the\ndeep alignment network starts from T1 steps (Line 25 - 28). Afer\nT1 steps, the student network is able to generate a representative\naction probability distribution, and we have suitable supervision\nto train the deep alignment network as well, parameterized by θω.\nAferT2 steps, θω will gradually converge to a local optimal, and we\nstart the deep knowledge distillation. As illustrated in Figure 2 (b),\nwe use symbol θ β\np\n′\nto represent the parameters of CNN, LSTM and\nthe fully connected distillation layer, since we don’t want the logits\nfrom heterogeneous directly aﬀect the action patern of target task.\nTo simplify the discussion, the above algorithm is described based\non interacting with a single agent from a heterogeneous task. In\nalgorithm 1, logits zα\nt can be acquired from multiple teacher net-\nworks of diﬀerent tasks, each task will train its own deep alignment\nnetwork θω and distill the aligned logits to the student network.\nAs we described in previous section 4.1, there are two types\nof interactions in this algorithm: 1). GAE interaction uses the\ngradients shared by all homogeneous agents. 2) Distillation interac-\ntion is the deep knowledge distillation from teacher network. Te\nGAE interaction is performed only among homogeneous tasks. By\nsynchronizing the parameters from a global student network in Al-\ngorithm 1 (line 3), the current agent receives the GAE updates from\nall the other agents who interactes with the same environment. In\nline 21 and 22, the current agent sends his gradients to the global\nstudent network, which will be synchronized with other homoge-\nneous agents. Te distillation interaction is then conducted in line\n31, where we have the aligned logits Fθ ω (zα\nt ) and the distillation\nlogits zβ\nt\n′\nto compute the gradients for minimizing the distillation\nloss. Te gradients of distillation are also sent to the global student\nnetwork. Te role of global student network can be regarded as a\nparameter server that helps sending interactions among the homo-\ngeneous agents. From a diﬀerent angle, each homogeneous agent\nmaintains an instinct version of global student network. Terefore,\nboth two types of interactions aﬀect all homogeneous agents, which\nmeans that the distillation interactions from agent д2 and agent д1\nwould aﬀect all homogeneous agents of agent д11.\n1Code is publicly available at htps://github.com/illidanlab/cdrl\nCollaborative Deep Reinforcement Learning\nWOODSTOCK’97, July 1997, El Paso, Texas USA\nAlgorithm 1 online cA3C\nRequire: Global shared parameter vectors Θp and Θv and global shared\ncounter T = 0; Agent-speciﬁc parameter vectors Θ′p and Θ′v , GAE [27]\nparameters γ and λ. Time step to start training deep alignment network\nand deep knowledge distillation T1, T2.\n1: while T < Tmax do\n2:\nReset gradients: dθp = 0 and dθv = 0\n3:\nSynchronize agent-speciﬁc parameters θ ′p = θp and θ ′v = θv\n4:\ntstart = t, Get state st\n5:\nReceive reward rt and new state st+1\n6:\nrepeat\n7:\nPerform at according to policy\n8:\nReceive reward rt and new state st+1\n9:\nCompute value of state vt = V (st ; θ ′v )\n10:\nif T ≥T1 then\n11:\nCompute the logits zα\nt from teacher network.\n12:\nCompute the policy logits zβ\nt and distillation logits zβ\nt\n′ from\nstudent network.\n13:\nend if\n14:\nt = t + 1, T = T + 1\n15:\nuntil terminal st or t −tstart >= tmax\n16:\nR = vt =\n(\n0\nfor terminal st\nV (st, θ ′v )\nfor non-terminal st\n17:\nfor i ∈{t −1, ..., tstart } do\n18:\nδi = ri + γvi+1 −vi\n19:\nA = δi + (γ λ)A\n20:\nR = ri + γ R\n21:\ndθp ←dθp + ∇log π(ai |si; θ ′)A\n22:\ndθv ←dθv + ∂(R −vi)2/∂θ ′v\n23:\nend for\n24:\nPerform asynchronous update of θp using dθp and of θv using dθv .\n25:\nif T ≥T1 then\n26:\n// Training deep alignment network.\n27:\nminθ ω Í\nt lKL(zβ\nt , zα\nt , τ ), lKL is deﬁned in Eq (3).\n28:\nend if\n29:\nif T ≥T2 then\n30:\n// online deep knowledge distillation.\n31:\nminθ β\np\n′ Í\nt lKL(Fθ ω (zα\nt ), zβ\nt\n′)\n32:\nend if\n33: end while\n5\nEXPERIMENTS\n5.1\nTraining and Evaluation\nIn this work, training and evaluation are conducted in OpenAI\nGym [5], a toolkit that includes a collection of benchmark problems\nsuch as classic Atari games using Arcade Learning Environment\n(ALE) [4], classic control games, etc. Same as the standard RL\nseting, an agent is stimulated in an environment, taking an action\nand receiving rewards and observations at each time step. Te\ntraining of the agent is divided into episodes, and the goal is to\nmaximize the expectation of the total reward per episode or to\nreach higher performance using as few episodes as possible.\n5.2\nCertiﬁcated Homogeneous transfer\nIn this subsection, we verify the eﬀectiveness of knowledge distil-\nlation as a type of interaction in collaborative deep reinforcement\nlearning for homogeneous tasks. Tis is also to verify the eﬀec-\ntiveness of the simplest case for deep knowledge distillation. Al-\nthough the eﬀectiveness of policy distillation in deep reinforcement\nlearning has been veriﬁed in [26] based on DQN, there is no prior\nstudies on asynchronous online distillation. Terefore, our ﬁrst\nexperiment is to demonstrate that the knowledge distilled from a\ncertiﬁcated task can be used to train a decent student network for\na homogeneous task. Otherwise, the even more challenging task of\ntransferring among heterogeneous sources may not work. We note\nthat in this case, the Assumption 1 is fully satisﬁed given k1 = k2,\nwhere k1 and k2 are the knowledge needed to master task e1 and\ne2, respectively. In this experiment, we conduct experiments in a\ngym environment named Pong. It is a classic Atari game that an\nagent controls a paddle to bounce a ball pass another player agent.\nTe maximum reward that each episode can reach is 21.\nFirst, we train a teacher network that learns from its own en-\nvironment by asynchronously performing GAE updates. We then\ntrain a student network using only online knowledge distillation\nfrom the teacher network. For fair comparisons, we use 8 agents\nfor all environments in the experiments. Speciﬁcally, both the stu-\ndent and the teacher are training in Pong with 8 agents. Te 8\nagents of the teacher network are trained using the A3C algorithm\n(equivalent to CDRL with GAE updates in one task). Te 8 agents of\nstudent network are trained using normal policy distillation, which\nuses the logits generated from the teacher network as supervision\nto train the policy network directly. From the results in Figure 3\n(a) we see that the student network can achieve a very competi-\ntive performance that is is almost same as the state-of-arts, using\nonline knowledge distillation from a homogeneous task. It also\nsuggests that the teacher doesn’t necessarily need to be an expert,\nbefore it can guide the training of a student in the homogeneous\ncase. Before 2 million steps, the teacher itself is still learning from\nthe environment, while the knowledge distilled from teacher can\nalready be used to train a reasonable student network. Moreover,\nwe see that the hybrid of two types of interactions in CDRL has\na positive eﬀect on the training, instead of causing performance\ndeterioration.\nIn the second experiment, the student network is learning from\nboth the online knowledge distillation and the GAE updates from\nthe environment. We ﬁnd that the convergence is much faster than\nthe state-of-art, as shown in Figure 3 (b). In this experiment, the\nknowledge is distilled from the teacher to student in the ﬁrst one\nmillion steps and the distillation is stopped afer that. We note\nthat in homogeneous CDRL, knowledge distillation is used directly\nwith policy logits other than distillation logits. Te knowledge\ntransfer seting in this experiment is not a practical one because\nwe already have a well-trained model of Pong, but it shows that\nwhen knowledge is correctly transferred, the combination of online\nknowledge distillation and the GAE updates is an eﬀective training\nprocedure.\n5.3\nCertiﬁcated Heterogeneous Transfer\nIn this subsection, we design experiments to illustrate the eﬀec-\ntiveness of CDRL in certiﬁcated heterogeneous transfer, with the\nproposed deep knowledge distillation. Given a certiﬁcated task\nPong, we want to utilize the existing expertise and apply it to\nfacilitate the training of a new task Bowling. In the following\nexperiments, we do not tune any model-speciﬁc parameters such\nWOODSTOCK’97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\n(a) online KD only\n(b) online KD with GAE\nFigure 3: Performance of online homogeneous knowledge\ndistillation. Te results show that the combination of knowl-\nedge distillation and GAE is an eﬀective training strategy for\nhomogeneous tasks.\n(a) KD with policy layer\n(b) KD with distillation layer\nFigure 4:\nPerformance of online knowledge distillation\nfrom a heterogeneous task. (a) distillation from a Pong ex-\npert using the policy layer to train a Bowling student (KD-\npolicy). (b) distillation from a Pong expert to a Bowling\nstudent using an extra distillation layer (KD-distill).\n(a) Pong\n(b) Bowling\n(c) aligned Pong\nFigure 5: Te action probability distributions of a Pong ex-\npert, a Bowling expert and an aligned Pong expert.\nas number of layers, size of ﬁlter or network structure for Bowling.\nWe ﬁrst directly perform transfer learning from Pong to Bowling\nby knowledge distillation. Since the two tasks has diﬀerent action\npaterns and action probability distributions, directly knowledge\ndistillation with a policy layer is not successful, as shown in Fig-\nure 4 (a). In fact, the knowledge distilled from Pong contradicts to\nthe knowledge learned from Bowling, which leads to the much\nworse performance than the baseline. We show in Figure 5 (a) and\n(b) that the action distributions of Pong and Bowling are very\ndiﬀerent. To resolve this, we distill the knowledge through an\nextra distillation layer as illustrated in Figure 2 (b). As such, the\nknowledge distilled from the certiﬁcated heterogeneous task can\nbe successfully transferred to the student network with improved\nperformance afer the learning is complete. However, this leads\nto a much slower convergence than the baseline as shown in Fig-\nure 4 (b), because that it takes time to learn a good distillation layer\nto align the knowledge distilled from Pong to the current learning\ntask. An interesting question is that, is it possible to have both\nimproved performance and faster convergence?\nDeep knowledge distillation – Oﬀline training. To handle the\nheterogeneity between Pong and Bowling, we ﬁrst verify the ef-\nfectiveness of deep knowledge distillation with an oﬄine training\nprocedure. Te oﬄine training is split into two stages. In the ﬁrst\nstage, we train a deep alignment network with four fully connected\nlayers using the Relu activation function. Te training data are\nlogits generated from an expert Pong network and Bowling net-\nwork. Te rewards of the networks at convergence are 20 and 60\nrespectively. In stage 2, with the Pong teacher network and trained\ndeep alignment network, we train a Bowling student network\nfrom scratch. Te student network is trained with both GAE inter-\nactions with its environment, and the distillation interactions from\nthe teacher network and the deep alignment network. Te results\nin Figure 6 (a) show that deep knowledge distillation can transfer\nknowledge from Pong to Bowling both eﬃciently and eﬀectively.\nDeep knowledge distillation – Online training. A more practi-\ncal seting of CDRL is the online training, where we simultaneously\ntrain deep alignment network and conduct the online deep knowl-\nedge distillation. We use two online training strategies: 1) Te\ntraining of deep alignment network starts afer 4 million steps,\nwhen the student Bowling network can perform reasonably well,\nand the knowledge distillation starts afer 6 million steps. 2) Te\ntraining of deep alignment network starts afer 0.1 million steps,\nand the knowledge distillation starts afer 1 million steps. Results\nare shown in Figure 6 (b) and (c) respectively. Te results show\nthat both strategies reach higher performance than the baseline.\nMoreover, the results suggest that we do not have to wait until\nthe student network reaches a reasonable performance before we\nstart to train the deep alignment network. Tis is because the deep\nalignment network is train to align two distributions of Pong and\nBowling, instead of transferring the actual knowledge. Recall that\nthe action probability distribution of Pong and Bowling are quite\ndiﬀerent as shown in Figure 5 (a) and (b). Afer we projecting the\nlogits of Pong using the deep alignment network, the distribution\nis very similar to Bowling, as shown in Figure 5 (c).\n5.4\nCollaborative Deep Reinforcement\nLearning\nIn previous experiments, we assume that there is a well-trained\nPong expert, and we transfer knowledge from the Pong expert\nto the Bowling student via deep knowledge distillation. A more\nchallenging setings that both of Bowling and Pong are trained\nfrom scratch. In this experiment, we we show that the CDRL frame-\nwork can still be eﬀective in this seting. In this experiment, we\ntrain a Bowling network and a Pong network from scratch using\nthe proposed cA3C algorithm. Te Pong agents are trained with\nGAE interactions only, and the target Bowling receive supervision\nfrom both GAE interactions and distilled knowledge from Pong via\na deep alignment network. We start to train the deep alignment\nnetwork afer 3 million steps, and perform deep knowledge distilla-\ntion afer 4 million steps, where the Pong agents are still updating\nfrom the environment. We note that in this seting, the teacher\nnetwork is constantly being updated, as knowledge is distilled from\nthe teacher until 15 million steps. Results in Figure 6 (d) show that\nthe proposed cA3C is able to converge to a higher performance than\nthe current state-of-art. Te reward of last one hundred episodes\nCollaborative Deep Reinforcement Learning\nWOODSTOCK’97, July 1997, El Paso, Texas USA\n(a) Oﬄine\n(b) Online Strategy 1\n(c) Online Strategy 2\n(d) Collaborative\nFigure 6: Performance of oﬀline, online deep knowledge distillation, and collaborative learning. Results averaged over 3 runs.\nof A3C is 61.48 ± 1.48, while cA3C achieves 68.35 ± 1.32, with a\nsigniﬁcant reward improvement of 11.2%.\n6\nCONCLUSION\nIn conclusion, we propose a collaborative deep reinforcement learn-\ning framework that can address the knowledge transfer among\nheterogeneous tasks. Under this framework, we propose deep\nknowledge distillation to adaptively align the domain of diﬀerent\ntasks with the utilization of deep alignement network. Further-\nmore, we develeop an eﬃcient cA3C algorithm and demonstrate\nits eﬀectiveness by extensive evaluation on OpenAI gym.\nACKNOWLEDGMENTS\nTis research is supported in part by the Oﬃce of Naval Research\n(ONR) under grant number N00014-14-1-0631 and National Science\nFoundation under Grant IIS-1565596, IIS-1615597.\nREFERENCES\n[1] Naoki Abe, Prem Melville, Cezar Pendus, Chandan K Reddy, David L Jensen,\nVince P Tomas, James J Bennet, Gary F Anderson, Brent R Cooley, Melissa\nKowalczyk, and others. 2010. Optimizing debt collections using constrained\nreinforcement learning. In SIGKDD. ACM, 75–84.\n[2] Naoki Abe, Naval Verma, Chid Apte, and Robert Schroko. 2004. Cross channel\noptimized marketing by reinforcement learning. In SIGKDD. ACM, 767–772.\n[3] YuXuan Liu Pieter AbbeelﬁﬁSergey Levine Abhishek Guptaﬁ, Coline Devinﬁ.\n2017. Learning Invariant Feature Spaces to Transfer Skills with Reinforcement\nLearning. In Under review as a conference paper at ICLR 2017.\n[4] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. Te\nArcade Learning Environment: An evaluation platform for general agents. J.\nArtif. Intell. Res.(JAIR) 47 (2013), 253–279.\n[5] Greg Brockman, Vicki Cheung, Ludwig Petersson, Jonas Schneider, John Schul-\nman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI gym. arXiv preprint\narXiv:1606.01540 (2016).\n[6] Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model\ncompression. In SIGKDD. ACM, 535–541.\n[7] Lucian Bus¸oniu, Robert Babuˇska, and Bart De Schuter. 2010. Multi-agent re-\ninforcement learning: An overview. In Innovations in multi-agent systems and\napplications-1. Springer, 183–221.\n[8] Marc Carreras, Junku Yuh, Joan Batlle, and Pere Ridao. 2005. A behavior-based\nscheme using reinforcement learning for autonomous underwater vehicles. IEEE\nJournal of Oceanic Engineering 30, 2 (2005), 416–427.\n[9] Pierre Dillenbourg. 1999. Collaborative Learning: Cognitive and Computational\nApproaches. Advances in Learning and Instruction Series. ERIC.\n[10] A Evgeniou and Massimiliano Pontil. 2007. Multi-task feature learning. NIPS 19\n(2007), 41.\n[11] Teodoros Evgeniou and Massimiliano Pontil. 2004. Regularized multi–task\nlearning. In SIGKDD. ACM, 109–117.\n[12] Anuradha A Gokhale. 1995. Collaborative learning enhances critical thinking.\n(1995).\n[13] Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. 2002. Coordinated rein-\nforcement learning. In ICML, Vol. 2. 227–234.\n[14] Yuval N. Harari. 2015. Sapiens: a brief history of humankind.\n[15] Geoﬀrey Hinton, Oriol Vinyals, and JeﬀDean. 2015. Distilling the knowledge in\na neural network. arXiv preprint arXiv:1503.02531 (2015).\n[16] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z\nLeibo, David Silver, and Koray Kavukcuoglu. 2016. Reinforcement learning with\nunsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397 (2016).\n[17] Jelle R Kok and Nikos Vlassis. 2006. Collaborative multiagent reinforcement\nlearning by payoﬀpropagation. JMLR 7, Sep (2006), 1789–1828.\n[18] Guillaume Lample and Devendra Singh Chaplot. 2016. Playing FPS games with\ndeep reinforcement learning. arXiv preprint arXiv:1609.05521 (2016).\n[19] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-\nothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016.\nAsynchronous methods for deep reinforcement learning.\narXiv preprint\narXiv:1602.01783 (2016).\n[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, and others. 2015. Human-level control through deep reinforcement\nlearning. Nature 518, 7540 (2015), 529–533.\n[21] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon,\nAlessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles\nBeatie, Stig Petersen, and others. 2015. Massively parallel methods for deep\nreinforcement learning. arXiv preprint arXiv:1507.04296 (2015).\n[22] OpenAI. 2017.\nOpenAI universe-starter-agent.\nhtps://github.com/openai/\nuniverse-starter-agent. (2017). Accessed: 2017-0201.\n[23] Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. TKDE 22,\n10 (2010), 1345–1359.\n[24] Emilio Parisoto, Jimmy Lei Ba, and Ruslan Salakhutdinov. 2015.\nActor-\nmimic: Deep multitask and transfer reinforcement learning. arXiv preprint\narXiv:1511.06342 (2015).\n[25] Janarthanan Rajendran, Aravind Lakshminarayanan, Mitesh M Khapra, Balara-\nman Ravindran, and others. 2015. A2T: Atend, Adapt and Transfer: Atentive\nDeep Architecture for Adaptive Transfer from multiple sources. arXiv preprint\narXiv:1510.02879 (2015).\n[26] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume\nDesjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray\nKavukcuoglu, and Raia Hadsell. 2015.\nPolicy distillation.\narXiv preprint\narXiv:1511.06295 (2015).\n[27] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.\n2015. High-dimensional continuous control using generalized advantage estima-\ntion. arXiv preprint arXiv:1506.02438 (2015).\n[28] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schritwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, and others. 2016. Mastering the game of Go with deep neural\nnetworks and tree search. Nature 529, 7587 (2016), 484–489.\n[29] Qian Sun, Rita Chatopadhyay, Sethuraman Panchanathan, and Jieping Ye. 2011.\nA two-stage weighting framework for multi-source domain adaptation. In NIPS.\n505–513.\n[30] Ming Tan. 1993. Multi-agent reinforcement learning: Independent vs. cooperative\nagents. In ICML. 330–337.\n[31] Mathew E Taylor and Peter Stone. 2009. Transfer learning for reinforcement\nlearning domains: A survey. JMLR 10, Jul (2009), 1633–1685.\n[32] Devinder Tapa, In-Sung Jung, and Gi-Nam Wang. 2005. Agent based decision\nsupport system using reinforcement learning under emergency circumstances.\nIn International Conference on Natural Computation. Springer, 888–892.\n[33] Eric Tzeng, Judy Hoﬀman, Trevor Darrell, and Kate Saenko. 2015. Simultaneous\ndeep transfer across domains and tasks. In ICCV. 4068–4076.\n[34] Yu Zhang and Dit-Yan Yeung. 2012. A convex formulation for learning task\nrelationships in multi-task learning. arXiv preprint arXiv:1203.3536 (2012).\n[35] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. 2014. Facial\nlandmark detection by deep multi-task learning. In ECCV. Springer, 94–108.\n[36] Jiayu Zhou, Jianhui Chen, and Jieping Ye. 2011. MALSAR: Multi-task learning\nvia structural regularization. Arizona State University (2011).\n[37] Yuke Zhu, Roozbeh Motaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei,\nand Ali Farhadi. 2016. Target-driven visual navigation in indoor scenes using\ndeep reinforcement learning. arXiv preprint arXiv:1609.05143 (2016).\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-02-19",
  "updated": "2017-02-19"
}