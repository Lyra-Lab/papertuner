{
  "id": "http://arxiv.org/abs/1702.05796v1",
  "title": "Collaborative Deep Reinforcement Learning",
  "authors": [
    "Kaixiang Lin",
    "Shu Wang",
    "Jiayu Zhou"
  ],
  "abstract": "Besides independent learning, human learning process is highly improved by\nsummarizing what has been learned, communicating it with peers, and\nsubsequently fusing knowledge from different sources to assist the current\nlearning goal. This collaborative learning procedure ensures that the knowledge\nis shared, continuously refined, and concluded from different perspectives to\nconstruct a more profound understanding. The idea of knowledge transfer has led\nto many advances in machine learning and data mining, but significant\nchallenges remain, especially when it comes to reinforcement learning,\nheterogeneous model structures, and different learning tasks. Motivated by\nhuman collaborative learning, in this paper we propose a collaborative deep\nreinforcement learning (CDRL) framework that performs adaptive knowledge\ntransfer among heterogeneous learning agents. Specifically, the proposed CDRL\nconducts a novel deep knowledge distillation method to address the\nheterogeneity among different learning tasks with a deep alignment network.\nFurthermore, we present an efficient collaborative Asynchronous Advantage\nActor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into\nthe online training of agents, and demonstrate the effectiveness of the CDRL\nframework using extensive empirical evaluation on OpenAI gym.",
  "text": "Collaborative Deep Reinforcement Learning\nKaixiang Lin\nComputer Science and Engineering\nMichigan State University\n428 S Shaw Ln.\nEast Lansing, MI 48824\nlinkaixi@msu.edu\nShu Wang\nComputer Science\nRutgers University\n57 US Highway 1\nNew Brunswick, NJ 088901\nsw498@cs.rutgers.edu\nJiayu Zhou\nComputer Science and Engineering\nMichigan State University\n428 S Shaw Ln.\nEast Lansing, MI 48824\njiayuz@msu.edu\nABSTRACT\nBesides independent learning, human learning process is highly\nimproved by summarizing what has been learned, communicating\nit with peers, and subsequently fusing knowledge from diÔ¨Äerent\nsources to assist the current learning goal. Tis collaborative learn-\ning procedure ensures that the knowledge is shared, continuously\nreÔ¨Åned, and concluded from diÔ¨Äerent perspectives to construct a\nmore profound understanding. Te idea of knowledge transfer has\nled to many advances in machine learning and data mining, but\nsigniÔ¨Åcant challenges remain, especially when it comes to rein-\nforcement learning, heterogeneous model structures, and diÔ¨Äerent\nlearning tasks. Motivated by human collaborative learning, in\nthis paper we propose a collaborative deep reinforcement learn-\ning (CDRL) framework that performs adaptive knowledge transfer\namong heterogeneous learning agents. SpeciÔ¨Åcally, the proposed\nCDRL conducts a novel deep knowledge distillation method to ad-\ndress the heterogeneity among diÔ¨Äerent learning tasks with a deep\nalignment network. Furthermore, we present an eÔ¨Écient collabo-\nrative Asynchronous Advantage Actor-Critic (cA3C) algorithm to\nincorporate deep knowledge distillation into the online training of\nagents, and demonstrate the eÔ¨Äectiveness of the CDRL framework\nusing extensive empirical evaluation on OpenAI gym.\nCCS CONCEPTS\n‚Ä¢Computing methodologies ‚ÜíMachine learning; Reinforce-\nment learning; Transfer learning;\nKEYWORDS\nKnowledge distillation; Transfer learning; Deep reinforcement\nlearning\nACM Reference format:\nKaixiang Lin, Shu Wang, and Jiayu Zhou. 1997. Collaborative Deep Rein-\nforcement Learning. In Proceedings of ACM Woodstock conference, El Paso,\nTexas USA, July 1997 (WOODSTOCK‚Äô97), 9 pages.\nDOI: 10.475/123 4\n1\nINTRODUCTION\nIt is the development of cognitive abilities including learning, re-\nmembering, communicating that enables human to conduct social\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation\non the Ô¨Årst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nWOODSTOCK‚Äô97, El Paso, Texas USA\n¬© 2016 Copyright held by the owner/author(s). 123-4567-24-567/08/06...$15.00\nDOI: 10.475/123 4\nAgent\nAgent\nAgent\nAgent\nHomogeneous\nInteraction\nAgent\nAgent\nAgent\nAgent\nHomogeneous\nInteraction\nHeterogeneous Interaction\nEnvironment\nAgent\nAgent model\nDeep\nknowledge\ndistilling\nInteract with\nheterogeneous \nagents\nLearn from \nenvironment\nInteract\nwith\nenvironment\nLearn from \ndistilled \nknowledge\nTarget (student)\nAgent\nAgent\nAgent\n...\nSources (teachers)\nFigure 1: Te illustration of Collaborative Deep Reinforce-\nment Learning Framework.\ncooperation, which is the key to the rise of humankind. As a social\nanimal, the ability to collaborate awoke the cognitive revolution\nand reveals the prosperous history of human [14]. In disciplines of\ncognitive science, education and psychology, collaborative learning,\na situation in which a group of people learn to achieve a set of tasks\ntogether, has been advocated throughout previous studies [9]. It is\nintuitive to illustrate the concept of collaborative learning in the\nexample of group study. A group of students are studying together\nto master some challenging course materials. As each student may\nunderstand the materials from a distinctive perspective, eÔ¨Äective\ncommunication would greatly help the entire group achieve a bet-\nter understanding than those from independent study, and could\nsigniÔ¨Åcantly improve the eÔ¨Éciency and eÔ¨Äectiveness of learning\nprocess, as well [12].\nOn the other hand, the study of human learning has largely ad-\nvanced the design of machine learning and data mining algorithms,\nespecially in reinforcement learning and transfer learning. Te\nrecent success of deep reinforcement learning (DRL) has atracted\nincreasing atention from the community, as DRL can discover very\ncompetitive strategies by having learning agents interacting with\na given environment and using rewards from the environment as\nthe supervision (e.g., [16, 18, 20, 28]). Even though most of current\nresearch on DRL has focused on learning from games, it possesses\ngreat transformative power to impact many industries with data\nmining and machine learning techniques such as clinical decision\nsupport [32], marketing [2], Ô¨Ånance [1], visual navigation [37], and\nautonomous driving [8]. Although there are many existing eÔ¨Äorts\ntowards eÔ¨Äective algorithms for DRL [19, 21], the computational\ncost still imposes signiÔ¨Åcant challenges as training DRL for even a\narXiv:1702.05796v1  [cs.LG]  19 Feb 2017\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\nsimple game such as Pong [5] remains very expensive. Te under-\nlying reasons for the obstacle of eÔ¨Écient training mainly lie in two\naspects: First, the supervision (rewards) from the environment is\nvery sparse and implicit during training. It may take an agent hun-\ndreds or even thousands actions to get a single reward, and which\nactions that actually lead to this reward are ambiguous. Besides the\ninsuÔ¨Écient supervision, training deep neural network itself takes\nlots of computational resources.\nDue to the aforementioned diÔ¨Éculties, performing knowledge\ntransfer from other related tasks or well-trained deep models to facil-\nitate training has drawn lots of atention in the community [16, 24‚Äì\n26, 31]. Existing transfer learning can be categorized into two\nclasses according to the means that knowledge is transferred: data\ntransfer [15, 24, 26] and model transfer [10, 24, 34, 35]. Model trans-\nfer methods implement knowledge transfer from introducing in-\nductive bias during the learning, and has been extensively studied\nin both transfer learning/multi-task learning (MTL) community\nand deep learning community. For example, in the regularized\nMTL models such as [11, 36], tasks with the same feature space are\nrelated through some structured regularization. Another example\nis the multi-task deep neural network, where diÔ¨Äerent tasks share\nparts of the network structures [35]. One obvious disadvantage\nof model transfer is the lack of Ô¨Çexibility: usually the feasibility\nof inductive transfer has largely restricted the model structure of\nlearning task, which makes it not practical in DRL because for\ndiÔ¨Äerent tasks the optimal model structures may be radically diÔ¨Äer-\nent. On the other hand, the recently developed data transfer (also\nknown as knowledge distillation or mimic learning) [15, 24, 26]\nembeds the source model knowledge into data points. Ten they\nare used as knowledge bridge to train target models, which can\nhave diÔ¨Äerent structures as compared to the source model [6, 15].\nBecause of the structural Ô¨Çexibility, the data transfer is especially\nsuitable to deal with structure variant models.\nTere are two situations that transfer learning methods are es-\nsential in DRL:\nCertiÔ¨Åcated heterogeneous transfer. Training a DRL agent is\ncomputational expensive. If we have a well-trained model, it will be\nbeneÔ¨Åcial to assist the learning of other tasks by transferring knowl-\nedge from this model. Terefore we consider following research\nquestion: Given one certiÔ¨Åcated task (i.e. the model is well-designed,\nextensively trained and performs very well), how can we maximize\nthe information that can be used in the training of other related\ntasks? Some model transfer approaches directly use the weights\nfrom the trained model to initialize the new task [24], which can\nonly be done when the model structures are the same. Tus, this\nstrict requirement has largely limited its general applicability on\nDRL. On the other hand, the initialization may not work well if the\ntasks are signiÔ¨Åcantly diÔ¨Äerent from each other in nature [24]. Tis\nchallenge could be partially solved by generating an intermediate\ndataset (logits) from the existing model to help learning the new\ntask. However, new problems would arise when we are transfer-\nring knowledge between heterogeneous tasks. Not only the action\nspaces are diÔ¨Äerent in dimension, the intrinsic action probability\ndistributions and semantic meanings of two tasks could diÔ¨Äer a\nlot. SpeciÔ¨Åcally, one action in Pong may refer to move the pad-\ndle upwards while the same action index in Riverraid [5] would\ncorrespond to Ô¨Åre. Terefore, the distilled dataset generated from\nthe trained source task cannot be directly used to train the hetero-\ngeneous target task. In this scenario, the Ô¨Årst key challenge we\nidentiÔ¨Åed in this work is that how to conduct data transfer among\nheterogeneous tasks so that we can maximally utilize the informa-\ntion from a certiÔ¨Åcated model while still maintain the Ô¨Çexibility of\nmodel design for new tasks. During the transfer, the transferred\nknowledge from other tasks may contradict to the knowledge that\nagents learned from its environment. One recently work [25] use an\natention network selective eliminate transfer if the contradiction\npresents, which is not suitable in this seting since we are given a\ncertiÔ¨Åcated task to transfer. Hence, the second challenge is how to\nresolve the conÔ¨Çict and perform a meaningful transfer.\nLack of expertise. A more general desired but also more chal-\nlenging scenario is that DRL agents are trained for multiple het-\nerogeneous tasks without any pre-trained models available. One\nfeasible way to conduct transfer under this scenario is that agents\nof multiple tasks share part of their network parameters [26, 35].\nHowever, an inevitable drawback is, multiple models lose their\ntask-speciÔ¨Åc designs since the shared part needs to be the same.\nAnother solution is to learn a domain invariant feature space shared\nby all tasks [3]. However, some task-speciÔ¨Åc information is ofen\nlost while converting the original state to a new feature subspace.\nIn this case, an intriguing questions is that: can we design a frame-\nwork that fully utilizes the original environment information and\nmeanwhile leverages the knowledge transferred from other tasks?\nTis paper investigates the aforementioned problems system-\natically and proposes a novel Collaborative Deep Reinforcement\nLearning (CDRL) framework (illustrated in Figure 1) to resolve\nthem. Our major contribution is threefold:\n‚Ä¢ First, in order to transfer knowledge among heterogeneous\ntasks while remaining the task-speciÔ¨Åc design of model struc-\nture, a novel deep knowledge distillation is proposed to address\nthe heterogeneity among tasks, with the utilization of deep\nalignment network designed for the domain adaptation.\n‚Ä¢ Second, in order to incorporate the transferred knowledge from\nheterogeneous tasks into the online training of current learning\nagents, similar to human collaborative learning, an eÔ¨Écient\ncollaborative asynchronously advantage actor-critic learning\n(cA3C) algorithm is developed under the CDRL framework. In\ncA3C, the target agents are able to learn from environments\nand its peers simultaneously, which also ensure the information\nfrom original environment is suÔ¨Éciently utilized. Further, the\nknowledge conÔ¨Çict among diÔ¨Äerent tasks is resolved by adding\nan extra distillation layer to the policy network under CDRL\nframework, as well.\n‚Ä¢ Last but not least we present extensive empirical studies on\nOpenAI gym to evaluate the proposed CDRL framework and\ndemonstrate its eÔ¨Äectiveness by achieving more than 10% per-\nformance improvement compared to the current state-of-the-\nart.\nNotations: In this paper, we use teacher network/source task\ndenotes the network/task contained the knowledge to be transferred\nto others. Similarly, the student network/target task is referred\nto those tasks utilizing the knowledge transferred from others to\nfacilitate its own training. Te expert network denotes the network\nthat has already reached a relative high averaged reward in its own\nCollaborative Deep Reinforcement Learning\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\nenvironment. In DRL, an agent is represented by a policy network\nand a value network that share a set of parameters. Homogeneous\nagents denotes agents that perform and learn under independent\ncopies of same environment. Heterogeneous agents refer to those\nagents that are trained in diÔ¨Äerent environments.\n2\nRELATED WORK\nMulti-agent learning. One closely related area to our work is\nmulti-agent reinforcement learning. A multi-agent system includes\na set of agents interacting in one environment. Meanwhile they\ncould potentially interact with each other [7, 13, 17, 30]. In collabo-\nrative multi-agent reinforcement learning, agents work together to\nmaximize a shared reward measurement [13, 17]. Tere is a clear\ndistinction between the proposed CDRL framework and multi-agent\nreinforcement learning. In CDRL, each agent interacts with its own\nenvironment copy and the goal is to maximize the reward of the\ntarget agents. Te formal deÔ¨Ånition of the proposed framework is\ngiven in Section 4.1.\nTransfer learning. Another relevant research topic is domain\nadaption in the Ô¨Åeld of transfer learning [23, 29, 33]. Te authors\nin [29] proposed a two-stage domain adaptation framework that\nconsiders the diÔ¨Äerences among marginal probability distributions\nof domains, as well as conditional probability distributions of tasks.\nTe method Ô¨Årst re-weights the data from the source domain using\nMaximum Mean Discrepancy and then re-weights the predictive\nfunction in the source domain to reduce te diÔ¨Äerence on conditional\nprobabilities. In [33], the marginal distributions of the source and\nthe target domain are aligned by training a network, which maps\ninputs into a domain invariant representation. Also, knowledge\ndistillation was directly utilized to align the source and target class\ndistribution. One clear limitation here is that the source domain\nand the target domain are required to have the same dimensionality\n(i.e. number of classes) with same semantics meanings, which is\nnot the case in our deep knowledge distillation.\nIn [3], an invariant feature space is learned to transfer skills\nbetween two agents. However, projecting the state into a feature\nspace would lose information contained in the original state. Tere\nis a trade-oÔ¨Äbetween learning the common feature space and\npreserving the maximum information from the original state. In\nour work, we use data generated by intermediate outputs in the\nknowledge transfer instead of a shared space. Our approach thus\nretains complete information from the environment and ensures\nhigh quality transfer. Te recently proposed A2T approach [25]\ncan avoid negative transfer among diÔ¨Äerent tasks. However, it\nis possible that some negative transfer cases may because of the\ninappropriate design of transfer algorithms. In our work, we show\nthat we can perform successful transfer among tasks that seemingly\ncause negative transfer.\nKnowledge transfer in deep learning. Since the training of\neach agent in an environment can be considered as a learning task,\nand the knowledge transfer among multiple tasks belongs to the\nstudy of multi-task learning. Te multi-task deep neural network\n(MTDNN) [35] transfers knowledge among tasks by sharing pa-\nrameters of several low-level layers. Since the low-level layers can\nbe considered to perform representation learning, the MTDNN is\nlearning a shared representation for inputs, which is then used\nby high-level layers in the network. DiÔ¨Äerent learning tasks are\nrelated to each other via this shared feature representation. In the\nproposed CDRL, we do not use the share representation due to\nthe inevitable information loss when we project the inputs into a\nshared representation. We instead perform explicitly knowledge\ntransfer among tasks by distilling knowledge that are independent\nof model structures. In [15], the authors proposed to compress\ncumbersome models (teachers) to more simple models (students),\nwhere the simple models are trained by a dataset (knowledge) dis-\ntilled from the teachers. However, this approach cannot handle the\ntransfer among heterogeneous tasks, which is one key challenge\nwe addressed in this paper.\nKnowledge transfer in deep reinforcement learning. Knowl-\nedge transfer is also studied in deep reinforcement learning. [19]\nproposed multi-threaded asynchronous variants of several most\nadvanced deep reinforcement learning methods including Sarsa,\nQ-learning, Q-learning and advantage actor-critic. Among all those\nmethods, asynchronous advantage actor-critic (A3C) achieves the\nbest performance. Instead of using experience replay as in previous\nwork, A3C stabilizes the training procedure by training diÔ¨Äerent\nagents in parallel using diÔ¨Äerent exploration strategies. Tis was\nshown to converge much faster than previous methods and use less\ncomputational resources. We show in Section 4.1 that the A3C is\nsubsumed to the proposed CDRL as a special case. In [24], a single\nmulti-task policy network is trained by utilizing a set of expert\nDeep Q-Network (DQN) of source games. At this stage, the goal is\nto obtain a policy network that can play source games as close to\nexperts as possible. Te second step is to transfer the knowledge\nfrom source tasks to a new but related target task. Te knowledge\nis transferred by using the DQN in last step as the initialization of\nthe DQN for the new task. As such, the training time of the new\ntask can be signiÔ¨Åcantly reduced. DiÔ¨Äerent from their approach,\nthe proposed transfer strategy is not to directly mimic experts‚Äô\nactions or initialize by a pre-trained model. In [26], knowledge\ndistillation was adopted to train a multi-task model that outper-\nforms single task models of some tasks. Te experts for all tasks are\nÔ¨Årstly acquired by single task learning. Te intermediate outputs\nfrom each expert are then distilled to a similar multi-task network\nwith an extra controller layer to coordinate diÔ¨Äerent action sets.\nOne clear limitation is that major components of the model are\nexactly the same for diÔ¨Äerent tasks, which may lead to degraded\nperformance on some tasks. In our work, transfer can happen even\nwhen there are no experts available. Also, our method allow each\ntask to have their own model structures. Furthermore, even the\nmodel structures are the same for multiple tasks, the tasks are not\ntrained to improve the performance of other tasks (i.e. it does not\nmimic experts from other tasks directly). Terefore our model can\nfocus on maximizing its own reward, instead of being distracted by\nothers.\n3\nBACKGROUND\n3.1\nReinforcement Learning\nIn this work, we consider the standard reinforcement learning\nseting where each agent interacts with it‚Äôs own environment\nover a number of discrete time steps. Given the current state\nst ‚ààS at step t, agent –¥i selects an action at ‚ààA according\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\nto its policy œÄ(at |st ), and receives a reward rt+1 from the envi-\nronment. Te goal of the agent is to choose an action at at step t\nthat maximize the sum of future rewards {rt } in a decaying man-\nner: Rt = √ç‚àû\ni=0 Œ≥ irt+i, where scalar Œ≥ ‚àà(0, 1] is a discount rate.\nBased on the policy œÄ of this agent, we can further deÔ¨Åne a state\nvalue function V (st ) = E[Rt |s = st ], which estimates the expected\ndiscounted return starting from state st , taking actions following\npolicy œÄ until the game ends. Te goal in reinforcement learning\nalgorithm is to maximize the expected return. Since we are mainly\ndiscussing one speciÔ¨Åc agent‚Äôs design and behavior throughout the\npaper, we leave out the notation of the agent index for conciseness.\n3.2\nAsynchronous Advantage actor-critic\nalgorithm (A3C)\nTe asynchronous advantage actor-critic (A3C) algorithm [19]\nlaunches multiple agents in parallel and asynchronously updates\na global shared target policy network œÄ(a|s,Œ∏p) as well as a value\nnetwork V (s,Œ∏v). parametrized by Œ∏p and Œ∏v, respectively. Each\nagent interacts with the environment, independently. At each step\nt the agent takes an action based on the probability distribution\ngenerated by policy network. Afer playing a n-step rollout or\nreaching the terminal state, the rewards are used to compute the\nadvantage with the output of value function. Te updates of policy\nnetwork is conducted by applying the gradient:\n‚àáŒ∏p log œÄ(at |st ;Œ∏p)A(st,at ;Œ∏v),\nwhere the advantage function A(st,at ;Œ∏v) is given by:\n√ïT ‚àít‚àí1\ni=0\nŒ≥ irt+i + Œ≥T ‚àítV (sT ;Œ∏v) ‚àíV (st ;Œ∏v).\nTerm T represents the step number for the last step of this rollout,\nit is either the max number of rollout steps or the number of steps\nfrom t to the terminal state. Te update of value network is to\nminimize the squared diÔ¨Äerence between the environment rewards\nand value function outputs, i.e.,\nmin\nŒ∏v\n(\n√ïT ‚àít‚àí1\ni=0\nŒ≥ irt+i + Œ≥T ‚àítV (sT ;Œ∏v) ‚àíV (st ;Œ∏v))2.\nTe policy network and the value network share the same layers\nexcept for the last output layer. An entropy regularization of policy\nœÄ is added to improve exploration, as well.\n3.3\nKnowledge distillation\nKnowledge distillation [15] is a transfer learning approach that\ndistills the knowledge from a teacher network to a student network\nusing a temperature parameterized ‚Äùsof targets‚Äù (i.e. a probabil-\nity distribution over a set of classes). It has been shown that it\ncan accelerate the training with less data since the gradient from\n‚Äùsof targets‚Äù contains much more information than the gradient\nobtained from ‚Äùhard targets‚Äù (e.g. 0, 1 supervision).\nTo be more speciÔ¨Åc, logits vector z ‚ààRd for d actions can be\nconverted to a probability distribution h ‚àà(0, 1)d by a sofmax\nfunction, raised with temperature œÑ:\nh(i) = sofmax(z/œÑ)i =\nexp(z(i)/œÑ)\n√ç\nj exp(z(j)/œÑ),\n(1)\nwhere h(i) and z(i) denotes the i-th entry of h and z, respectively.\nTen the knowledge distillation can be completed by optimize\nthe following Kullback-Leibler divergence (KL) with temperature\nœÑ [15, 26].\nLKL(D,Œ∏ Œ≤\np ) =\n√ï\nt=1\nsofmax(zŒ±\nt /œÑ) ln sofmax(zŒ±\nt /œÑ)\nsofmax(zŒ≤\nt )\n(2)\nwhere zŒ±\nt is the logits vector from teacher network (notation\nŒ± represents teacher) at step t, while zŒ≤\nt is the logits vector from\nstudent network (notation Œ≤ represents student) of this step. Œ∏ Œ≤\np\ndenotes the parameters of the student policy network. D is a set of\nlogits from teacher network.\n4\nCOLLABORATIVE DEEP REINFORCEMENT\nLEARNING FRAMEWORK\nIn this section, we introduce the proposed collaborative deep rein-\nforcement learning (CDRL) framework. Under this framework, a\ncollaborative Asynchronous Advantage Actor-Critic (cA3C) algo-\nrithm is proposed to conÔ¨Årm the eÔ¨Äectiveness of the collaborative\napproach. Before we introduce our method in details, one underly-\ning assumption we used is as follows:\nAssumption 1. If there is a universe that contains all the tasks\nE = {e1,e2, ...,e‚àû} and ki represents the corresponding knowledge\nto master each task ei, then ‚àÄi, j,ki ‚à©kj , ‚àÖ.\nTis is a formal description of our common sense that any pair\nof tasks are not absolutely isolated from each other, which has\nbeen implicitly used as a fundamental assumption by most prior\ntransfer learning studies [11, 24, 26].Terefore, we focus on mining\nthe shared knowledge across multiple tasks instead of providing\nstrategy selecting tasks that share knowledge as much as possible,\nwhich remains to be unsolved and may lead to our future work. Te\ngoal here is to utilize the existing knowledge as well as possible. For\nexample, we may only have a well-trained expert on playing Pong\ngame, and we want to utilize its expertise to help us perform beter\non other games. Tis is one of the situations that can be solved by\nour collaborative deep reinforcement learning framework.\n4.1\nCollaborative deep reinforcement learning\nIn deep reinforcement learning, since the training of agents are\ncomputational expensive, the well-trained agents should be further\nutilized as source agents (agents where we transferred knowledge\nfrom) to facilitate the training of target agents (agents that are\nprovided with the extra knowledge from source). In order to in-\ncorporate this type of collaboration to the training of DRL agents,\nwe formally deÔ¨Åne the collaborative deep reinforcement learning\n(CDRL) framework as follows:\nDeÔ¨Ånition 4.1. Givenm independent environments {Œµ1,Œµ2, ...,Œµm}\nofm tasks {e1,e2, ...,em} , the correspondingm agents {–¥1,–¥2, ...,–¥m}\nare collaboratively trained in parallel to maximize the rewards (mas-\nter each task) with respect to target agents.\n‚Ä¢ Environments. Tere is no restriction on the environments: Te\nm environments can be totally diÔ¨Äerent or with some duplica-\ntions.\nCollaborative Deep Reinforcement Learning\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\nlogits ùíõùú∂\nAligned logits ùüäùõâùê∞(ùê≥ùõÇ)\nDistillation logits ùíõùú∑‚Ä≤\nTraining\nStudent network\nTeacher network\nDeep\nalignment\nnetwork\nDistillation loss\nDistillation logitsùíõùú∑‚Ä≤\nParameter ùúΩùíë\nùú∑‚Ä≤\nParameter ùúΩùíë\nùú∑\nFully connected \npolicy layer\nFully connected \ndistillation layer\nSoftmax\nLSTM Network\nConvolutional \nNeural Network\nstate\nAction\nProbability\ndistribution\nPolicy logits\nùíõùú∑\n(a) Distillation procedure\n(b) Student network structure.\nFigure 2: Deep knowledge distillation. In (a), the teacher‚Äôs output logits zŒ± is mapped through a deep alignment network and\nthe aligned logits FŒ∏ œâ (zŒ± ) is used as the supervision to train the student. . In (b), the extra fully connected layer for distillation\nis added for learning knowledge from teacher. For simplicity‚Äôs sake, time step t is omitted here.\n‚Ä¢ In parallel. Each environment Œµi only interacts with the one\ncorresponding agent –¥i, i.e., the action aj\nt from agent –¥j at step\nt has no inÔ¨Çuence on the state si\nt+1 in Œµi, ‚àÄi , j.\n‚Ä¢ Collaboratively. Te training procedure of agent –¥i consists of in-\nteracting with environment Œµi and interacting with other agents\nas well. Te agent –¥i is not necessary to be at same level as\n‚Äùcollaborative‚Äù deÔ¨Åned in cognitive science [9]. E.g., –¥1 can be\nan expert for task e1 (environment Œµ1) while he is helping agent\n–¥2 which is a student agent in task e2.\n‚Ä¢ Target agents. Te goal of CDRL can be set as maximizing the\nrewards that agent –¥i obtains in environment Œµi with the help\nof interacting with other agents, similar to inductive transfer\nlearning where –¥i is the target agent for target task and others\nare source tasks. Te knowledge is transfered from source to\ntarget –¥i by interaction. When we set the goal to maximize\nthe rewards of multiple agents jointly, it is similar to multi-task\nlearning where all tasks are source tasks and target tasks at the\nsame time.\nNotice that our deÔ¨Ånition is very diÔ¨Äerent from the previously\ndeÔ¨Åned collaborative multiagent Markow Decision Process (col-\nlaborative multiagent MDP) [13, 17] where a set of agents select a\nglobal joint action to maximize the sum of their individual rewards\nand the environment is transited to a new state based on that joint\naction. First, MDP is not a requirement in CDRL framework. Sec-\nond, in CDRL, each agent has its own copy of environment and\nmaximizes its own cumulative rewards. Te goal of collaboration\nis to improve the performance of collaborative agents, compared\nwith isolated ones, which is diÔ¨Äerent from maximizing the sum\nof global rewards in collaborative multiagent MDP. Tird, CDRL\nfocuses on how agents collaborate among heterogeneous environ-\nments, instead of how joint action aÔ¨Äects the rewards. In CDRL,\ndiÔ¨Äerent agents are acting in parallel, the actions taken by other\nagents won‚Äôt directly inÔ¨Çuence current agent‚Äôs rewards. While in\ncollaborative multiagent MDP, the agents must coordinate their\naction choices since the rewards will be directly aÔ¨Äected by the\naction choices of other agents.\nFurthermore, CDRL includes diÔ¨Äerent types of interaction, which\nmakes this a general framework. For example, the current state-of-\nthe-art is A3C [19] can be categorized as one homogeneous CDRL\nmethod with advantage actor-critic interaction. SpeciÔ¨Åcally, multi-\nple agents in A3C are trained in parallel with the same environment.\nAll agents Ô¨Årst synchronize parameters from a global network, and\nthen update the global network with their individual gradients. Tis\nprocedure can be seen as each agent maintains its own model (a\ndiÔ¨Äerent version of global network) and interacts with other agents\nby sending and receiving gradients.\nIn this paper, we propose a novel interaction method named\ndeep knowledge distillation under the CDRL framework. It is worth\nnoting that the interaction in A3C only deals with the homogeneous\ntasks, i.e. all agents have the same environment and the same model\nstructure so that their gradients can be accumulated and interacted.\nBy deep knowledge distillation, the interaction can be conducted\namong heterogeneous tasks.\n4.2\nDeep knowledge distillation\nAs we introduced before, knowledge distillation [15] is trying to\ntrain a student network that can behave similarly to the teacher\nnetwork by utilizing the logits from the teacher as supervision.\nHowever, transferring the knowledge among heterogeneous tasks\nfaces several diÔ¨Éculties. First, the action spaces of diÔ¨Äerent tasks\nmay have diÔ¨Äerent dimensions. Second, even if the dimensionality\nof action space is same among tasks, the action probability distribu-\ntions for diÔ¨Äerent tasks could vary a lot, as we illustrated in Figure 5\n(a) and (b). Tus, the action paterns represented by the logits of\ndiÔ¨Äerent policy networks are usually diÔ¨Äerent from task to task.\nIf we directly force a student network to mimic the action patern\nof a teacher network for a diÔ¨Äerent task, it could be trained in a\nwrong direction, and Ô¨Ånally ends up with worse performance than\nisolated training. In fact, this suspect has been empirically veriÔ¨Åed\nin our experiments.\nBased on the above observation, we propose deep knowledge\ndistillation to transfer knowledge between heterogeneous tasks.\nAs illustrated in Figure 2 (a), the approach for deep knowledge\ndistillation is straightforward. We use a deep alignment network to\nmap the logits of the teacher network from a heterogeneous source\ntask eŒ± (environment ŒµŒ± ), then the logits is used as our supervision\nto update the student network of target task eŒ≤ (environment ŒµŒ≤).\nTis procedure is performed by minimizing following objective\nfunction over student policy network parameters Œ∏ Œ≤\np\n‚Ä≤\n:\nLKL(D,Œ∏ Œ≤\np\n‚Ä≤\n,œÑ) =\n√ï\nt\nlKL(FŒ∏ œâ (zŒ±\nt ), zŒ≤\nt\n‚Ä≤\n,œÑ),\n(3)\nwhere\nlKL(FŒ∏ œâ (zŒ±\nt ), zŒ≤\nt\n‚Ä≤\n,œÑ) = sofmax(FŒ∏ œâ (zŒ±\nt )/œÑ) ln sofmax(FŒ∏ œâ (zŒ±\nt )/œÑ)\nsofmax(zŒ≤\nt\n‚Ä≤\n)\n.\nHere Œ∏œâ denotes the parameters of the deep alignment network,\nwhich transfers the logits zŒ±\nt from the teacher policy network for\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\nknowledge distillation by function FŒ∏ œâ (zŒ±\nt ) at step t. As we show in\nFigure 2 (b), Œ∏ Œ≤\np is the student policy network parameters (including\nparameters of CNN, LSTM and policy layer) for task eŒ≤, while Œ∏ Œ≤\np\n‚Ä≤\ndenotes student network parameters of CNN, LSTM and distillation\nlayer. It is clear that the distillation logits zŒ≤\nt\n‚Ä≤ from the student\nnetwork does not determine the action probability distribution\ndirectly, which is established by the policy logits zŒ≤\nt , as illustrated\nin Figure 2 (b). We add another fully connected distillation layer\nto deal with the mismatch of action space dimensionality and the\ncontradiction of the transferred knowledge from source domain\nand the learned knowledge from target domain. Te input to both\nof the teacher and the student network is the state of environment\nŒµŒ≤ of target task eŒ≤ . It means that we want to transfer the expertise\nfrom an expert of task eŒ± towards the current state. Symbol D\nis a set of logits from the teacher network in one batch and œÑ is\nthe temperature same as described in Eq (1). In a trivial case that\nthe teacher network and the student network are trained for same\ntask (eŒ± equals eŒ≤), then the deep alignment network FŒ∏ œâ would\nreduce to an identity mapping, and the problem is also reduced to a\nsingle task policy distillation, which has been proved to be eÔ¨Äective\nin [26]. Before we can apply the deep knowledge distillation, we\nneed to Ô¨Årst train a good deep alignment network. In this work, we\nprovide two types of training protocols for diÔ¨Äerent situations:\nOÔ¨Äline training: Tis protocol Ô¨Årst trains two teacher networks\nin both environment ŒµŒ± and ŒµŒ≤ . Ten we use the logits of both two\nteacher networks to train a deep alignment network FŒ∏ œâ . Afer\nacquiring a pre-trained FŒ∏ œâ , we train a student network of task eŒ≤\nfrom scratch, in the meanwhile the teacher network of task eŒ± and\nFŒ∏ œâ are used for deep knowledge distillation.\nOnline training: Suppose we only have a teacher network of\ntask eŒ± , and we want to use the knowledge from task eŒ± to train\nthe student network for task eŒ≤ to get higher performance from\nscratch. Te pipeline of this method is that, we Ô¨Årstly train the\nstudent network by interacting with the environment ŒµŒ≤ for a\ncertain amount of steps T1, and then start to train the alignment\nnetwork FŒ∏ œâ , using the logits from the teacher network and the\nstudent network. Aferwards, at step T2, we start performing deep\nknowledge distillation. ObviouslyT2 is larger thanT1, and the value\nof them are task-speciÔ¨Åc, which is decided empirically in this work.\nTe oÔ¨Ñine training could be useful if we have already had a rea-\nsonably good model for task eŒ≤ , while we want to further improve\nthe performance using the knowledge from task eŒ± . Te online\ntraining method is used when we need to learn the student network\nfrom scratch. Both types of training protocol can be extended to\nmultiple heterogeneous tasks.\n4.3\nCollaborative Asynchronous Advantage\nActor-Critic\nIn this section, we introduce the proposed collaborative asynchro-\nnous advantage actor-critic (cA3C) algorithm. As we described in\nsection 4.1, the agents are running in parallel. Each agent goes\nthrough the same training procedure as described in Algorithm 1.\nAs it shows, the training of agent –¥1 can be separated into two parts:\nTe Ô¨Årst part is to interact with the environment, get the reward\nand compute the gradients to minimize the value loss and policy\nloss based on Generalized Advantage Estimation (GAE) [27]. Te\nsecond part is to interact with source agent –¥2 so that the logits\ndistilled from agent –¥2 can be transferred by the deep alignment\nnetwork and used as supervision to bias the training of agent –¥1.\nTo be more concrete, the pseudo code in Algorithm 1 is an en-\nvolved version of A3C based on online training of deep knowledge\ndistillation. At T-th iteration, the agent interacts with the environ-\nment for tmax steps or until the terminal state is reached (Line 6 to\nLine 15). Ten the updating of value network and policy network is\nconducted by GAE. Tis variation of A3C is Ô¨Årstly implemented in\nOpenAI universe starter agent [22]. Since the main asynchronous\nframework is the same as A3C, we still use the A3C to denote this\nalgorithm although the updating is the not the same as advantage\nactor-critic algorithm used in original A3C paper [19].\nTe online training of deep knowledge distillation is mainly com-\npleted from Line 25 to Line 32 in Algorithm 1. Te training of the\ndeep alignment network starts from T1 steps (Line 25 - 28). Afer\nT1 steps, the student network is able to generate a representative\naction probability distribution, and we have suitable supervision\nto train the deep alignment network as well, parameterized by Œ∏œâ.\nAferT2 steps, Œ∏œâ will gradually converge to a local optimal, and we\nstart the deep knowledge distillation. As illustrated in Figure 2 (b),\nwe use symbol Œ∏ Œ≤\np\n‚Ä≤\nto represent the parameters of CNN, LSTM and\nthe fully connected distillation layer, since we don‚Äôt want the logits\nfrom heterogeneous directly aÔ¨Äect the action patern of target task.\nTo simplify the discussion, the above algorithm is described based\non interacting with a single agent from a heterogeneous task. In\nalgorithm 1, logits zŒ±\nt can be acquired from multiple teacher net-\nworks of diÔ¨Äerent tasks, each task will train its own deep alignment\nnetwork Œ∏œâ and distill the aligned logits to the student network.\nAs we described in previous section 4.1, there are two types\nof interactions in this algorithm: 1). GAE interaction uses the\ngradients shared by all homogeneous agents. 2) Distillation interac-\ntion is the deep knowledge distillation from teacher network. Te\nGAE interaction is performed only among homogeneous tasks. By\nsynchronizing the parameters from a global student network in Al-\ngorithm 1 (line 3), the current agent receives the GAE updates from\nall the other agents who interactes with the same environment. In\nline 21 and 22, the current agent sends his gradients to the global\nstudent network, which will be synchronized with other homoge-\nneous agents. Te distillation interaction is then conducted in line\n31, where we have the aligned logits FŒ∏ œâ (zŒ±\nt ) and the distillation\nlogits zŒ≤\nt\n‚Ä≤\nto compute the gradients for minimizing the distillation\nloss. Te gradients of distillation are also sent to the global student\nnetwork. Te role of global student network can be regarded as a\nparameter server that helps sending interactions among the homo-\ngeneous agents. From a diÔ¨Äerent angle, each homogeneous agent\nmaintains an instinct version of global student network. Terefore,\nboth two types of interactions aÔ¨Äect all homogeneous agents, which\nmeans that the distillation interactions from agent –¥2 and agent –¥1\nwould aÔ¨Äect all homogeneous agents of agent –¥11.\n1Code is publicly available at htps://github.com/illidanlab/cdrl\nCollaborative Deep Reinforcement Learning\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\nAlgorithm 1 online cA3C\nRequire: Global shared parameter vectors Œòp and Œòv and global shared\ncounter T = 0; Agent-speciÔ¨Åc parameter vectors Œò‚Ä≤p and Œò‚Ä≤v , GAE [27]\nparameters Œ≥ and Œª. Time step to start training deep alignment network\nand deep knowledge distillation T1, T2.\n1: while T < Tmax do\n2:\nReset gradients: dŒ∏p = 0 and dŒ∏v = 0\n3:\nSynchronize agent-speciÔ¨Åc parameters Œ∏ ‚Ä≤p = Œ∏p and Œ∏ ‚Ä≤v = Œ∏v\n4:\ntstart = t, Get state st\n5:\nReceive reward rt and new state st+1\n6:\nrepeat\n7:\nPerform at according to policy\n8:\nReceive reward rt and new state st+1\n9:\nCompute value of state vt = V (st ; Œ∏ ‚Ä≤v )\n10:\nif T ‚â•T1 then\n11:\nCompute the logits zŒ±\nt from teacher network.\n12:\nCompute the policy logits zŒ≤\nt and distillation logits zŒ≤\nt\n‚Ä≤ from\nstudent network.\n13:\nend if\n14:\nt = t + 1, T = T + 1\n15:\nuntil terminal st or t ‚àítstart >= tmax\n16:\nR = vt =\n(\n0\nfor terminal st\nV (st, Œ∏ ‚Ä≤v )\nfor non-terminal st\n17:\nfor i ‚àà{t ‚àí1, ..., tstart } do\n18:\nŒ¥i = ri + Œ≥vi+1 ‚àívi\n19:\nA = Œ¥i + (Œ≥ Œª)A\n20:\nR = ri + Œ≥ R\n21:\ndŒ∏p ‚ÜêdŒ∏p + ‚àálog œÄ(ai |si; Œ∏ ‚Ä≤)A\n22:\ndŒ∏v ‚ÜêdŒ∏v + ‚àÇ(R ‚àívi)2/‚àÇŒ∏ ‚Ä≤v\n23:\nend for\n24:\nPerform asynchronous update of Œ∏p using dŒ∏p and of Œ∏v using dŒ∏v .\n25:\nif T ‚â•T1 then\n26:\n// Training deep alignment network.\n27:\nminŒ∏ œâ √ç\nt lKL(zŒ≤\nt , zŒ±\nt , œÑ ), lKL is deÔ¨Åned in Eq (3).\n28:\nend if\n29:\nif T ‚â•T2 then\n30:\n// online deep knowledge distillation.\n31:\nminŒ∏ Œ≤\np\n‚Ä≤ √ç\nt lKL(FŒ∏ œâ (zŒ±\nt ), zŒ≤\nt\n‚Ä≤)\n32:\nend if\n33: end while\n5\nEXPERIMENTS\n5.1\nTraining and Evaluation\nIn this work, training and evaluation are conducted in OpenAI\nGym [5], a toolkit that includes a collection of benchmark problems\nsuch as classic Atari games using Arcade Learning Environment\n(ALE) [4], classic control games, etc. Same as the standard RL\nseting, an agent is stimulated in an environment, taking an action\nand receiving rewards and observations at each time step. Te\ntraining of the agent is divided into episodes, and the goal is to\nmaximize the expectation of the total reward per episode or to\nreach higher performance using as few episodes as possible.\n5.2\nCertiÔ¨Åcated Homogeneous transfer\nIn this subsection, we verify the eÔ¨Äectiveness of knowledge distil-\nlation as a type of interaction in collaborative deep reinforcement\nlearning for homogeneous tasks. Tis is also to verify the eÔ¨Äec-\ntiveness of the simplest case for deep knowledge distillation. Al-\nthough the eÔ¨Äectiveness of policy distillation in deep reinforcement\nlearning has been veriÔ¨Åed in [26] based on DQN, there is no prior\nstudies on asynchronous online distillation. Terefore, our Ô¨Årst\nexperiment is to demonstrate that the knowledge distilled from a\ncertiÔ¨Åcated task can be used to train a decent student network for\na homogeneous task. Otherwise, the even more challenging task of\ntransferring among heterogeneous sources may not work. We note\nthat in this case, the Assumption 1 is fully satisÔ¨Åed given k1 = k2,\nwhere k1 and k2 are the knowledge needed to master task e1 and\ne2, respectively. In this experiment, we conduct experiments in a\ngym environment named Pong. It is a classic Atari game that an\nagent controls a paddle to bounce a ball pass another player agent.\nTe maximum reward that each episode can reach is 21.\nFirst, we train a teacher network that learns from its own en-\nvironment by asynchronously performing GAE updates. We then\ntrain a student network using only online knowledge distillation\nfrom the teacher network. For fair comparisons, we use 8 agents\nfor all environments in the experiments. SpeciÔ¨Åcally, both the stu-\ndent and the teacher are training in Pong with 8 agents. Te 8\nagents of the teacher network are trained using the A3C algorithm\n(equivalent to CDRL with GAE updates in one task). Te 8 agents of\nstudent network are trained using normal policy distillation, which\nuses the logits generated from the teacher network as supervision\nto train the policy network directly. From the results in Figure 3\n(a) we see that the student network can achieve a very competi-\ntive performance that is is almost same as the state-of-arts, using\nonline knowledge distillation from a homogeneous task. It also\nsuggests that the teacher doesn‚Äôt necessarily need to be an expert,\nbefore it can guide the training of a student in the homogeneous\ncase. Before 2 million steps, the teacher itself is still learning from\nthe environment, while the knowledge distilled from teacher can\nalready be used to train a reasonable student network. Moreover,\nwe see that the hybrid of two types of interactions in CDRL has\na positive eÔ¨Äect on the training, instead of causing performance\ndeterioration.\nIn the second experiment, the student network is learning from\nboth the online knowledge distillation and the GAE updates from\nthe environment. We Ô¨Ånd that the convergence is much faster than\nthe state-of-art, as shown in Figure 3 (b). In this experiment, the\nknowledge is distilled from the teacher to student in the Ô¨Årst one\nmillion steps and the distillation is stopped afer that. We note\nthat in homogeneous CDRL, knowledge distillation is used directly\nwith policy logits other than distillation logits. Te knowledge\ntransfer seting in this experiment is not a practical one because\nwe already have a well-trained model of Pong, but it shows that\nwhen knowledge is correctly transferred, the combination of online\nknowledge distillation and the GAE updates is an eÔ¨Äective training\nprocedure.\n5.3\nCertiÔ¨Åcated Heterogeneous Transfer\nIn this subsection, we design experiments to illustrate the eÔ¨Äec-\ntiveness of CDRL in certiÔ¨Åcated heterogeneous transfer, with the\nproposed deep knowledge distillation. Given a certiÔ¨Åcated task\nPong, we want to utilize the existing expertise and apply it to\nfacilitate the training of a new task Bowling. In the following\nexperiments, we do not tune any model-speciÔ¨Åc parameters such\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\nKaixiang Lin, Shu Wang, and Jiayu Zhou\n(a) online KD only\n(b) online KD with GAE\nFigure 3: Performance of online homogeneous knowledge\ndistillation. Te results show that the combination of knowl-\nedge distillation and GAE is an eÔ¨Äective training strategy for\nhomogeneous tasks.\n(a) KD with policy layer\n(b) KD with distillation layer\nFigure 4:\nPerformance of online knowledge distillation\nfrom a heterogeneous task. (a) distillation from a Pong ex-\npert using the policy layer to train a Bowling student (KD-\npolicy). (b) distillation from a Pong expert to a Bowling\nstudent using an extra distillation layer (KD-distill).\n(a) Pong\n(b) Bowling\n(c) aligned Pong\nFigure 5: Te action probability distributions of a Pong ex-\npert, a Bowling expert and an aligned Pong expert.\nas number of layers, size of Ô¨Ålter or network structure for Bowling.\nWe Ô¨Årst directly perform transfer learning from Pong to Bowling\nby knowledge distillation. Since the two tasks has diÔ¨Äerent action\npaterns and action probability distributions, directly knowledge\ndistillation with a policy layer is not successful, as shown in Fig-\nure 4 (a). In fact, the knowledge distilled from Pong contradicts to\nthe knowledge learned from Bowling, which leads to the much\nworse performance than the baseline. We show in Figure 5 (a) and\n(b) that the action distributions of Pong and Bowling are very\ndiÔ¨Äerent. To resolve this, we distill the knowledge through an\nextra distillation layer as illustrated in Figure 2 (b). As such, the\nknowledge distilled from the certiÔ¨Åcated heterogeneous task can\nbe successfully transferred to the student network with improved\nperformance afer the learning is complete. However, this leads\nto a much slower convergence than the baseline as shown in Fig-\nure 4 (b), because that it takes time to learn a good distillation layer\nto align the knowledge distilled from Pong to the current learning\ntask. An interesting question is that, is it possible to have both\nimproved performance and faster convergence?\nDeep knowledge distillation ‚Äì OÔ¨Äline training. To handle the\nheterogeneity between Pong and Bowling, we Ô¨Årst verify the ef-\nfectiveness of deep knowledge distillation with an oÔ¨Ñine training\nprocedure. Te oÔ¨Ñine training is split into two stages. In the Ô¨Årst\nstage, we train a deep alignment network with four fully connected\nlayers using the Relu activation function. Te training data are\nlogits generated from an expert Pong network and Bowling net-\nwork. Te rewards of the networks at convergence are 20 and 60\nrespectively. In stage 2, with the Pong teacher network and trained\ndeep alignment network, we train a Bowling student network\nfrom scratch. Te student network is trained with both GAE inter-\nactions with its environment, and the distillation interactions from\nthe teacher network and the deep alignment network. Te results\nin Figure 6 (a) show that deep knowledge distillation can transfer\nknowledge from Pong to Bowling both eÔ¨Éciently and eÔ¨Äectively.\nDeep knowledge distillation ‚Äì Online training. A more practi-\ncal seting of CDRL is the online training, where we simultaneously\ntrain deep alignment network and conduct the online deep knowl-\nedge distillation. We use two online training strategies: 1) Te\ntraining of deep alignment network starts afer 4 million steps,\nwhen the student Bowling network can perform reasonably well,\nand the knowledge distillation starts afer 6 million steps. 2) Te\ntraining of deep alignment network starts afer 0.1 million steps,\nand the knowledge distillation starts afer 1 million steps. Results\nare shown in Figure 6 (b) and (c) respectively. Te results show\nthat both strategies reach higher performance than the baseline.\nMoreover, the results suggest that we do not have to wait until\nthe student network reaches a reasonable performance before we\nstart to train the deep alignment network. Tis is because the deep\nalignment network is train to align two distributions of Pong and\nBowling, instead of transferring the actual knowledge. Recall that\nthe action probability distribution of Pong and Bowling are quite\ndiÔ¨Äerent as shown in Figure 5 (a) and (b). Afer we projecting the\nlogits of Pong using the deep alignment network, the distribution\nis very similar to Bowling, as shown in Figure 5 (c).\n5.4\nCollaborative Deep Reinforcement\nLearning\nIn previous experiments, we assume that there is a well-trained\nPong expert, and we transfer knowledge from the Pong expert\nto the Bowling student via deep knowledge distillation. A more\nchallenging setings that both of Bowling and Pong are trained\nfrom scratch. In this experiment, we we show that the CDRL frame-\nwork can still be eÔ¨Äective in this seting. In this experiment, we\ntrain a Bowling network and a Pong network from scratch using\nthe proposed cA3C algorithm. Te Pong agents are trained with\nGAE interactions only, and the target Bowling receive supervision\nfrom both GAE interactions and distilled knowledge from Pong via\na deep alignment network. We start to train the deep alignment\nnetwork afer 3 million steps, and perform deep knowledge distilla-\ntion afer 4 million steps, where the Pong agents are still updating\nfrom the environment. We note that in this seting, the teacher\nnetwork is constantly being updated, as knowledge is distilled from\nthe teacher until 15 million steps. Results in Figure 6 (d) show that\nthe proposed cA3C is able to converge to a higher performance than\nthe current state-of-art. Te reward of last one hundred episodes\nCollaborative Deep Reinforcement Learning\nWOODSTOCK‚Äô97, July 1997, El Paso, Texas USA\n(a) OÔ¨Ñine\n(b) Online Strategy 1\n(c) Online Strategy 2\n(d) Collaborative\nFigure 6: Performance of oÔ¨Äline, online deep knowledge distillation, and collaborative learning. Results averaged over 3 runs.\nof A3C is 61.48 ¬± 1.48, while cA3C achieves 68.35 ¬± 1.32, with a\nsigniÔ¨Åcant reward improvement of 11.2%.\n6\nCONCLUSION\nIn conclusion, we propose a collaborative deep reinforcement learn-\ning framework that can address the knowledge transfer among\nheterogeneous tasks. Under this framework, we propose deep\nknowledge distillation to adaptively align the domain of diÔ¨Äerent\ntasks with the utilization of deep alignement network. Further-\nmore, we develeop an eÔ¨Écient cA3C algorithm and demonstrate\nits eÔ¨Äectiveness by extensive evaluation on OpenAI gym.\nACKNOWLEDGMENTS\nTis research is supported in part by the OÔ¨Éce of Naval Research\n(ONR) under grant number N00014-14-1-0631 and National Science\nFoundation under Grant IIS-1565596, IIS-1615597.\nREFERENCES\n[1] Naoki Abe, Prem Melville, Cezar Pendus, Chandan K Reddy, David L Jensen,\nVince P Tomas, James J Bennet, Gary F Anderson, Brent R Cooley, Melissa\nKowalczyk, and others. 2010. Optimizing debt collections using constrained\nreinforcement learning. In SIGKDD. ACM, 75‚Äì84.\n[2] Naoki Abe, Naval Verma, Chid Apte, and Robert Schroko. 2004. Cross channel\noptimized marketing by reinforcement learning. In SIGKDD. ACM, 767‚Äì772.\n[3] YuXuan Liu Pieter AbbeelÔ¨ÅÔ¨ÅSergey Levine Abhishek GuptaÔ¨Å, Coline DevinÔ¨Å.\n2017. Learning Invariant Feature Spaces to Transfer Skills with Reinforcement\nLearning. In Under review as a conference paper at ICLR 2017.\n[4] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. Te\nArcade Learning Environment: An evaluation platform for general agents. J.\nArtif. Intell. Res.(JAIR) 47 (2013), 253‚Äì279.\n[5] Greg Brockman, Vicki Cheung, Ludwig Petersson, Jonas Schneider, John Schul-\nman, Jie Tang, and Wojciech Zaremba. 2016. OpenAI gym. arXiv preprint\narXiv:1606.01540 (2016).\n[6] Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model\ncompression. In SIGKDD. ACM, 535‚Äì541.\n[7] Lucian Bus¬∏oniu, Robert BabuÀáska, and Bart De Schuter. 2010. Multi-agent re-\ninforcement learning: An overview. In Innovations in multi-agent systems and\napplications-1. Springer, 183‚Äì221.\n[8] Marc Carreras, Junku Yuh, Joan Batlle, and Pere Ridao. 2005. A behavior-based\nscheme using reinforcement learning for autonomous underwater vehicles. IEEE\nJournal of Oceanic Engineering 30, 2 (2005), 416‚Äì427.\n[9] Pierre Dillenbourg. 1999. Collaborative Learning: Cognitive and Computational\nApproaches. Advances in Learning and Instruction Series. ERIC.\n[10] A Evgeniou and Massimiliano Pontil. 2007. Multi-task feature learning. NIPS 19\n(2007), 41.\n[11] Teodoros Evgeniou and Massimiliano Pontil. 2004. Regularized multi‚Äìtask\nlearning. In SIGKDD. ACM, 109‚Äì117.\n[12] Anuradha A Gokhale. 1995. Collaborative learning enhances critical thinking.\n(1995).\n[13] Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. 2002. Coordinated rein-\nforcement learning. In ICML, Vol. 2. 227‚Äì234.\n[14] Yuval N. Harari. 2015. Sapiens: a brief history of humankind.\n[15] GeoÔ¨Ärey Hinton, Oriol Vinyals, and JeÔ¨ÄDean. 2015. Distilling the knowledge in\na neural network. arXiv preprint arXiv:1503.02531 (2015).\n[16] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z\nLeibo, David Silver, and Koray Kavukcuoglu. 2016. Reinforcement learning with\nunsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397 (2016).\n[17] Jelle R Kok and Nikos Vlassis. 2006. Collaborative multiagent reinforcement\nlearning by payoÔ¨Äpropagation. JMLR 7, Sep (2006), 1789‚Äì1828.\n[18] Guillaume Lample and Devendra Singh Chaplot. 2016. Playing FPS games with\ndeep reinforcement learning. arXiv preprint arXiv:1609.05521 (2016).\n[19] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-\nothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016.\nAsynchronous methods for deep reinforcement learning.\narXiv preprint\narXiv:1602.01783 (2016).\n[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, and others. 2015. Human-level control through deep reinforcement\nlearning. Nature 518, 7540 (2015), 529‚Äì533.\n[21] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon,\nAlessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles\nBeatie, Stig Petersen, and others. 2015. Massively parallel methods for deep\nreinforcement learning. arXiv preprint arXiv:1507.04296 (2015).\n[22] OpenAI. 2017.\nOpenAI universe-starter-agent.\nhtps://github.com/openai/\nuniverse-starter-agent. (2017). Accessed: 2017-0201.\n[23] Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. TKDE 22,\n10 (2010), 1345‚Äì1359.\n[24] Emilio Parisoto, Jimmy Lei Ba, and Ruslan Salakhutdinov. 2015.\nActor-\nmimic: Deep multitask and transfer reinforcement learning. arXiv preprint\narXiv:1511.06342 (2015).\n[25] Janarthanan Rajendran, Aravind Lakshminarayanan, Mitesh M Khapra, Balara-\nman Ravindran, and others. 2015. A2T: Atend, Adapt and Transfer: Atentive\nDeep Architecture for Adaptive Transfer from multiple sources. arXiv preprint\narXiv:1510.02879 (2015).\n[26] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume\nDesjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray\nKavukcuoglu, and Raia Hadsell. 2015.\nPolicy distillation.\narXiv preprint\narXiv:1511.06295 (2015).\n[27] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.\n2015. High-dimensional continuous control using generalized advantage estima-\ntion. arXiv preprint arXiv:1506.02438 (2015).\n[28] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schritwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, and others. 2016. Mastering the game of Go with deep neural\nnetworks and tree search. Nature 529, 7587 (2016), 484‚Äì489.\n[29] Qian Sun, Rita Chatopadhyay, Sethuraman Panchanathan, and Jieping Ye. 2011.\nA two-stage weighting framework for multi-source domain adaptation. In NIPS.\n505‚Äì513.\n[30] Ming Tan. 1993. Multi-agent reinforcement learning: Independent vs. cooperative\nagents. In ICML. 330‚Äì337.\n[31] Mathew E Taylor and Peter Stone. 2009. Transfer learning for reinforcement\nlearning domains: A survey. JMLR 10, Jul (2009), 1633‚Äì1685.\n[32] Devinder Tapa, In-Sung Jung, and Gi-Nam Wang. 2005. Agent based decision\nsupport system using reinforcement learning under emergency circumstances.\nIn International Conference on Natural Computation. Springer, 888‚Äì892.\n[33] Eric Tzeng, Judy HoÔ¨Äman, Trevor Darrell, and Kate Saenko. 2015. Simultaneous\ndeep transfer across domains and tasks. In ICCV. 4068‚Äì4076.\n[34] Yu Zhang and Dit-Yan Yeung. 2012. A convex formulation for learning task\nrelationships in multi-task learning. arXiv preprint arXiv:1203.3536 (2012).\n[35] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. 2014. Facial\nlandmark detection by deep multi-task learning. In ECCV. Springer, 94‚Äì108.\n[36] Jiayu Zhou, Jianhui Chen, and Jieping Ye. 2011. MALSAR: Multi-task learning\nvia structural regularization. Arizona State University (2011).\n[37] Yuke Zhu, Roozbeh Motaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei,\nand Ali Farhadi. 2016. Target-driven visual navigation in indoor scenes using\ndeep reinforcement learning. arXiv preprint arXiv:1609.05143 (2016).\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-02-19",
  "updated": "2017-02-19"
}