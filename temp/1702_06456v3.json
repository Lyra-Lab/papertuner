{
  "id": "http://arxiv.org/abs/1702.06456v3",
  "title": "Online Representation Learning with Single and Multi-layer Hebbian Networks for Image Classification",
  "authors": [
    "Yanis Bahroun",
    "Andrea Soltoggio"
  ],
  "abstract": "Unsupervised learning permits the development of algorithms that are able to\nadapt to a variety of different data sets using the same underlying rules\nthanks to the autonomous discovery of discriminating features during training.\nRecently, a new class of Hebbian-like and local unsupervised learning rules for\nneural networks have been developed that minimise a similarity matching\ncost-function. These have been shown to perform sparse representation learning.\nThis study tests the effectiveness of one such learning rule for learning\nfeatures from images. The rule implemented is derived from a nonnegative\nclassical multidimensional scaling cost-function, and is applied to both single\nand multi-layer architectures. The features learned by the algorithm are then\nused as input to an SVM to test their effectiveness in classification on the\nestablished CIFAR-10 image dataset. The algorithm performs well in comparison\nto other unsupervised learning algorithms and multi-layer networks, thus\nsuggesting its validity in the design of a new class of compact, online\nlearning networks.",
  "text": "arXiv:1702.06456v3  [cs.NE]  29 Jan 2018\nOnline Representation Learning with Single and\nMulti-layer Hebbian Networks for Image Classiﬁcation\nYanis Bahroun and Andrea Soltoggio\nLoughborough University, Computer Science Department,\nLE11 3TU, Leicestershire, United Kingdom\n{y.bahroun,a.soltoggio}@lboro.ac.uk\nAbstract. Unsupervised learning permits the development of algorithms that are\nable to adapt to a variety of different data sets using the same underlying rules\nthanks to the autonomous discovery of discriminating features during training.\nRecently, a new class of Hebbian-like and local unsupervised learning rules for\nneural networks have been developed that minimise a similarity matching cost-\nfunction. These have been shown to perform sparse representation learning. This\nstudy tests the effectiveness of one such learning rule for learning features from\nimages. The rule implemented is derived from a nonnegative classical multidi-\nmensional scaling cost-function, and is applied to both single and multi-layer\narchitectures. The features learned by the algorithm are then used as input to a\nSVM to test their effectiveness in classiﬁcation on the established CIFAR-10 im-\nage dataset. The algorithm performs well in comparison to other unsupervised\nlearning algorithms and multi-layer networks, thus suggesting its validity in the\ndesign of a new class of compact, online learning networks.\nKeywords: Classiﬁcation; Competitive learning; Feature learning; Hebbian learn-\ning; Online algorithm; Neural networks; Sparse coding; Unsupervised learning.\n1\nIntroduction\nBiological synaptic plasticity is hypothesized to be one of the main phenomena respon-\nsible for human learning and memory. One mechanism of synaptic plasticity is inspired\nby the Hebbian learning principle which states that connections between two units, e.g.,\nneurons, are strengthened when they are simultaneously activated. In artiﬁcial neural\nnetworks, implementations of Hebbian plasticity are known to learn recurring patterns\nof activations. The use of extensions of this rule, such as Oja’s rule [8] or the Gener-\nalized Hebbian rule, also called Sanger’s rule [14], have permitted the development of\nalgorithms that have proved particularly efﬁcient at tasks such as online dimensional-\nity reduction. Two important properties of brain-inspired models, namely competitive\nlearning [13] and sparse coding [9] can be performed using Hebbian and anti-Hebbian\nlearning rules. Such properties can be achieved with inhibitory connections, which ex-\ntend the capabilities of such learning rules beyond simple extraction of the principal\ncomponent of input data. The continuous and local update dynamics of Hebbian learn-\ning also make it suitable for learning from a continuous stream of data. Such an algo-\nrithm can take one image at a time with memory requirements that are independent of\nthe number of samples.\n2\nOnline Representation Learning with Hebbian Networks\nThis study employs Hebbian/anti-Hebbian learning rules derived from a similarity\nmatching cost-function [11] and applies it to perform online unsupervised learning of\nfeatures from multiple image datasets. The rule proposed in [11] is applied here for the\nﬁrst time to online features learning for image classiﬁcation with single and multi-layer\narchitectures. The quality of the features is assessed visually and by performing classi-\nﬁcation with a linear classiﬁer working on the learned features. The simulations show\nthat a simple single-layer Hebbian network can outperform more complex models such\nas Sparse Autoencoders (SAE) and Restricted Boltzmann machines (RBM) for image\nclassiﬁcations tasks [2]. When applied to multi-layer architectures, the rule learns addi-\ntional features. This study is the ﬁrst of its kind to perform multi-layer sparse dictionary\nlearning based on the similarity matching principle developed in [11] and to apply it to\nimage classiﬁcation.\n2\nHebbian/anti-Hebbian Network Derived From a Similarity\nMatching Cost-Function\nThe rule implemented by the Hebbian/anti-Hebbian network used in this work derives\nfrom an adaptation of Classical MultiDimensional Scaling (CMDS). CMDS is a popular\nembedding technique [3]. Unlike most dimensionality reduction techniques, e.g. PCA,\nthe CMDS uses as input the matrix of similarity between inputs to generate a set of\nembedding coordinates. The advantage of MDS is that any kind of distance or similarity\nmatrix can be analyzed. However, in its simplest form, CMDS produces dense features\nmaps which are often unsuitable when considered for image classiﬁcation. Therefore an\nadaptation of the CMDS introduced recently in [11] is used to overcome this weakness.\nThe model implemented is a nonnegative classical multidimensional scaling that has\nthree properties: it takes a similarity matrix as input, it produces sparse codes, and can\nbe implemented using a new biologically plausible Hebbian model. The Hebbian/anti-\nHebbian rule introduced in [11] is given as follows: for a set of inputs xt ∈Rn for\nt ∈{1, . . . , T }, the concatenation of the inputs deﬁnes an input matrix X ∈Rn×T .\nThe output matrix Y of encodings is an element of Rm×T that corresponds to a sparse\novercomplete representation of the input if m > n, or to a low-dimensional embedding\nof the input if m < n. The objective function proposed by [11] is:\nY ∗= arg min\nY ≥0\n∥X′X −Y ′Y ∥2\nF\n(1)\nwhere F is the Frobenius norm and X′X is the Gram matrix of the inputs which cor-\nresponds to the similarity matrix. Solving Eq.1 directly requires storing Y ∈Rm×T\n+\nwhich increases with time T making online learning difﬁcult. Thus instead an online\nlearning version of Eq.1 is expressed as:\n(yT )∗= arg min\nyT ≥0\n∥X′X −Y ′Y ∥2\nF\n.\n(2)\nThe components of the solution of Eq.2, found in [11] using coordinate descent, are :\n(yT\ni )∗= max\n\u0012\nW T\ni xT −M T\ni (yT )∗, 0\n\u0013\n∀i ∈{1, . . ., m},\n(3)\nOnline Representation Learning with Hebbian Networks\n3\nwhere\nW T\nij =\nT −1\nP\nt=1\nyt\nixt\nj\nT −1\nP\nt=1\n(yt\ni)2\n;\nM T\nij =\nT −1\nP\nt=1\nyt\niyt\nj\nT −1\nP\nt=1\n(yt\ni)2\n1i̸=j\n.\n(4)\nW T and M T can be found using the recursive formulations:\nW T\nij = W T −1\nij\n+\n\u0012\nyT −1\ni\n(xT −1\nj\n−W T −1\nij\nyT −1\ni\n)\n\u001e\nˆY T\ni\n\u0013\n(5)\nM T\nij̸=i = M T −1\nij\n+\n\u0012\nyT −1\ni\n(yT −1\nj\n−M T −1\nij\nyT −1\ni\n)\n\u001e\nˆY T\ni\n\u0013\n(6)\nˆY T\ni\n= ˆY T −1\ni\n+ (yT −1\ni\n)2\n.\n(7)\nW T (green arrows) and M T (blue arrows) can be interpreted respectively as feed-\nforward synaptic connections between the input and the hidden layer and lateral synap-\ntic inhibitory connections within the hidden layer. The weight matrices are of ﬁxed sizes\nand updated sequentially, which makes the model suitable for online learning. The ar-\nchitecture of the Hebbian/anti-Hebbian network is represented in Figure 1.\nFeed-forward\nconnections W T\nLateral synaptic\nconnections M T\nxT\n1\nxT\n2\nxT\n3\nyT\n1\nyT\n2\nyT\n3\nyT\n4\nHidden\nlayer\nInput\nlayer\nOutput\nlayer\nFig. 1: Hebbian/anti-Hebbian network with lateral connections derived from Eq.2\n3\nA Model to Learn Features From Images\nIn the new model presented in this study, the input data vectors (x1, . . . , xT ) are com-\nposed of patches taken randomly from a training dataset of images. For every new input\nxt presented, the model ﬁrst computes a sparse post-synaptic activity yt. Second, the\nsynaptic weights are modiﬁed based on local Hebbian/anti-Hebbian learning rules re-\nquiring only the current pre- post-synaptic neuronal activities. The model can be seen as\na sparse encoding followed by a recursive updating scheme, which are both well suited\nto solve large-scale online problems.\nA multi-class SVM classiﬁes the pictures using output vectors obtained by a sim-\nple pooling of the feature vectors, Y ∗, obtained for the input images from the trained\nnetwork. In particular, given an input image, each neuron in the output layer produces\na new image, called a feature map, which is pooled in quadrants [2] to form 4 terms of\nthe input vector for the SVM.\n4\nOnline Representation Learning with Hebbian Networks\n3.1\nMulti-layer Hebbian/anti-Hebbian Neural Network\nIn the proposed approach, layers of Hebbian/anti-Hebbian network are stacked sim-\nilarly to the Convolutional DBN [4], and Hierarchical K-means. In the multi-layer\nHebbian/anti-Hebbian network, both the weights of the ﬁrst layer and second layer are\ncontinuously updated. Unlike other CNNs, the non-linearity used in each layer is not\nonly due to the positivity constraint, but to the combination of a rectiﬁed linear unit ac-\ntivation function and of interneuronal competition. This model combines the powerful\narchitecture of convolutional neural networks using ReLU activation with interneuronal\ncompetition, while all synaptic weights are updated using online local learning rules. In\nbetween layers, a 2 × 2 average pooling is used to downsample the feature maps.\n3.2\nOvercompleteness of the Representation and Multi-resolution\nAs part of the evaluation of the new model, it is important to assess its performance with\ndifferent sizes (m) of the hidden layers. If the number of neurons exceeds the size of\nthe input (m > n), the representation is called overcomplete. Overcompleteness may be\nbeneﬁcial, but requires increased computation, particularly for deep networks in which\nthe number of neurons has to grow exponentially in order to keep this property. One\nmotivation for overcompleteness is that it may allow more ﬂexibility in matching the\noutput structure with the input. However, not all learning algorithms can learn and take\nadvantage of overcomplete representations. The behaviour of the algorithm is analysed\nin the transition between undercomplete (m < n) and overcomplete (m > n) represen-\ntations.\nAlthough the model might beneﬁt from a large number of neurons, from a practical\nperspective an increase in the number of neurons is a challenge for such models due\nto the number of operations required in the coordinate descent. In order to limit the\ncomputational cost of training a large network while still beneﬁting from overcomplete\nrepresentations, this study proposes to train simultaneously three single-layer neural\nnetworks, each of them having different receptive ﬁeld sizes (4 × 4, 6 × 6, and 8 × 8\npixels). Thus, a variation of the model tested here is composed of three different net-\nworks. This architecture of parallel networks with different receptive ﬁeld sizes requires\nless computational time and memory than a model with only one receptive ﬁeld size and\nthe same total number of neurons, because the synaptic weights only connect neurons\nwithin each neural network. This model will be called multi-resolution in the following.\n3.3\nParameters and Preprocessing\nThe architecture used here has the following tunable parameters: the receptive ﬁeld size\n(n) of the neurons and the number of neurons (m). These parameters are standard to\nCNNs but their inﬂuence on this online feed-forward model needs to be investigated.\nFor computer vision models, understanding the inﬂuence of input preprocessing is\nof critical importance for both biological plausibility and practical applicability. Recent\nﬁndings [1], conﬁrm partial decorrelation of the input signal in the retinal ganglion\ncells. The inﬂuence of input decorrelation by applying whitening will be investigated.\nOnline Representation Learning with Hebbian Networks\n5\n4\nResults\nThe effectiveness of the algorithm is assessed by measuring the performance on an\nimage classiﬁcation task. We acknowledge that classiﬁcation accuracy is at best an im-\nplicit measure evaluating the performance of representation learning algorithms, but\nprovides a standardised way of comparing them. In the following, single and multi-\nlayer Hebbian/anti-Hebbian neural networks combined with the standard multi-class\nSVM are trained on the CIFAR-10 dataset [5].\n4.1\nEvaluation of the Single-layer Model\nA ﬁrst experiment tested the performance of the model with and without whitening of\nthe input data. Although there exist Hebbian networks that can perform online whiten-\ning [10], an ofﬂine technique based on singular value decomposition [2] is applied in\nthese experiments. Figure 2a and 2b show the features learned by the network from\nraw input and whitened input respectively. The features learned from raw data (Fig.2a)\nare neither sharp nor localised ﬁlters and just slightly capture edges. With whitened\ndata (Fig.2b), the features are sharp, localised, and resemble Gabor ﬁlters, which are\nobserved in the primary visual cortex [9].\nFig. 2: Sample of features learned from raw (2a) and whitened input (2b). Classiﬁcation\naccuracy with raw (2c) and whitened input (2d).\n(a) Features learned from raw data\n(b) Features learned from whitened data\n(c) Accuracy using raw data\nReceptive Field Size\n3\n5\n7\n9\nAccuracy in %\n64\n66\n68\n70\n72\n74\n400 Neurons\n500 Neurons\n600 Neurons\n800 Neurons\n(d) Accuracy using whitened data\nReceptive Field Size\n3\n5\n7\n9\nAccuracy in %\n64\n66\n68\n70\n72\n74\n400 Neurons\n500 Neurons\n600 Neurons\n800 Neurons\nIn a second set of experiments, the performance of the network was tested for vary-\ning receptive ﬁeld sizes (Fig.2c-2d) and varying network sizes (400, 500, 600, and 800\nneurons). The results show that the performance peaks at a receptive ﬁeld size of 7\npixels and then begins to decline. This property is common to most unsupervised learn-\ning algorithms [2], showing the difﬁculty of learning spatially extended features. Fig-\n6\nOnline Representation Learning with Hebbian Networks\nures 2c and 2d also show that for every conﬁguration, the performance of the algorithm\nis largely and uniformly improved when whitening is applied to the input.\n4.2\nComparison to State-of-the-art Performances and Online Training\nVarious unsupervised learning algorithms have been tested on the CIFAR-10 dataset.\nSpherical K-means, in particular, proved in [2] to outperform autoencoders and re-\nstricted Boltzmann machines, providing a very simple and efﬁcient solution for dic-\ntionary learning for image classiﬁcation. Thus, spherical K-means is used here as a\nbenchmark to evaluate the performance of the single-layer network. As with other un-\nsupervised learning algorithms, increasing the number of output neurons to reach over-\ncompleteness also improved classiﬁcation performance (Fig.3a). Although the single-\nlayer neural network has a higher degree of sparsity than the K-means proposed in [2]\n(results not shown here), they appear to have the same performance in their optimal\nconﬁgurations (Fig.3a).\nThe classiﬁcation accuracy of the network during training is shown in Fig.3b. The\ngraph (Fig.3b) suggests that the features learned by the network over time help the\nsystem improve the classiﬁcation accuracy. This is signiﬁcant because it demonstrates\nfor the ﬁrst time the effectiveness of features learned with a Hebb-like cost-function\nminimisation. It is not obvious a priori that the online optimisation of a cost-function\nfor sparse similarity matching (Eq.2) produces features suitable for image classiﬁcation.\nFig. 3: (a) Proposed model vs K-means, (b) Classiﬁcation accuracy\n(a) Optimal setup vs K-means\nNumber of Neurons\n0\n200\n400\n600\n800\nAccuracy in %\n40\n50\n60\n70\n80\nHebbian Network\nK-Means\n(b) Online training\nNumber of input streamed in 10k\n0\n5\n10\n15\n20\nAccuracy in %\n20\n30\n40\n50\n60\n70\n80\nHebbian Network\nAs shown in Table 1, the multi-resolution network outperforms the single resolution\nnetwork and K-means algorithm [2], reaching 80.42% accuracy on the CIFAR-10. The\nmulti-resolution model shows better performance, while requiring less computation and\nmemory than the single resolution model. It also outperforms the single layer NOMP\n[6], sparse TIRBM [15], CKN-GM and CKN-PM [7], which are more complex models.\nIt was outperformed only by combined models or models with three layers or more.\nOnline Representation Learning with Hebbian Networks\n7\nAlgorithm\nAccuracy\nSingle-Layer, Single Resolution (4k neurons)\n79.58 %\nSingle-Layer, Multi-Resolution (3×1.6k neurons)\n80.42 %\nSingle-layer K-means [2] (4k neurons)\n79.60 %\nMulti-layer K-means [2] (3 Layers, >4k neurons)\n82.60 %\nSparse RBM\n72.40 %\nConvolutional DBN [4]\n78.90 %\nSparse TIRBM [15] (4k neurons)\n80.10%\nTIOMP-1/T [15] (combined transformations, 4k neurons)\n82.20 %\nSingle Layer NOMP [6] ( 5k neurons)\n78.00 %\nMulti-Layer NOMP [6] (3 Layers, >4k neurons)\n82.90 %\nMulti-Layer CKN-GM [7]\n74.84 %\nMulti-Layer CKN-PM [7]\n78.30 %\nMulti-Layer CKN-CO [7] (combining CKN-GM & CKN-PM)\n82.18 %\nTable 1: Comparison of the single-layer network with unsupervised learning algorithms\non CIFAR-10.\n4.3\nEvaluation of the Multi-layer Model\nA single resolution, double-layer neural network with different numbers of neurons in\neach layer was trained similarly to the single-layer network in the previous section. In\nTable 2, φ1 and φ2 correspond respectively to the features learned by the ﬁrst and second\nlayer. The results show that φ2 alone are less discriminative than φ1 as indicated in Fig.\n3a. However, when combined (φ1 + φ2) the model achieves better performance than\neach layer considered separately. Nevertheless, the preliminary results indicate that the\nsizes of the two layers unevenly affect the performance of the network. A future test may\ninvestigate if a multi-layer architecture can outperform the largest shallow networks.\n#Neurons Layer 2\n50\n100\n200\n400\n800\n100 Neurons Layer 1\nφ2\n54.9% 59.7% 64.7% 68.7% 71.45%\nφ1+φ2 67.2% 68.1% 69.9% 72.4% 73.81%\n200 Neurons Layer 1\nφ2\n55.8% 60.6% 65.3% 70.3% 72.7%\nφ1+φ2 69.9% 70.8% 71.9% 73.7% 75.1%\nTable 2: Classiﬁcation accuracy for a two-layer network.\n5\nConclusion\nThis work proposes a multi-layer neural network exploiting Hebbian/anti-Hebbian rules\nto learn features for image classiﬁcation. The network is trained on the CIFAR-10 im-\nage dataset prior to feeding a linear classiﬁer. The model successfully learns online\nmore discriminative representations of the data when the number of neurons and the\nnumber of layers increase. The overcompleteness of the representation is critical for\nlearning relevant features. The results show that a minimum unsupervised learning time\nis needed to optimise the network leading to better classiﬁcation accuracy. Finally, one\n8\nOnline Representation Learning with Hebbian Networks\nkey factor in improving image classiﬁcation is the appropriate choice of the receptive\nﬁeld size used for training the network.\nSuch ﬁndings prove that neural networks can be trained to solve problems as com-\nplex as sparse dictionary learning with Hebbian learning rules, delivering competitive\naccuracy compared to other encoder, including deep neural networks. This makes deep\nHebbian networks attractive for building large-scale image classiﬁcation systems. The\ncompetitive performances on the CIFAR-10 suggests that this model can offer an alter-\nnative to batch trained neural networks. Ultimately, thanks to its bio-inspired architec-\nture and learning rules, it also stands as a good candidate for memristive devices [12].\nMoreover, if a decaying factor is added to the proposed model that might result in an\nalgorithm that can deal with complex datasets with temporal variations of the distribu-\ntions.\nReferences\n1. Abbasi-Asl, R., Pehlevan, C., Yu, B., Chklovskii, D.B.: Do retinal ganglion cells project\nnatural scenes to their principal subspace and whiten them? arXiv preprint arXiv:1612.03483\n(2016)\n2. Coates, A., Lee, H., Ng, A.Y.: An analysis of single-layer networks in unsupervised feature\nlearning. In: AISTATS 2011. vol. 1001 (2011)\n3. Cox, T.F., Cox, M.A.: Multidimensional scaling. CRC press (2000)\n4. Krizhevsky, A., Hinton, G.: Convolutional deep belief networks on cifar-10. Unpublished\nmanuscript 40 (2010)\n5. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images (2009)\n6. Lin, T.h., Kung, H.: Stable and efﬁcient representation learning with nonnegativity con-\nstraints. In: Proceedings of the 31st International Conference on Machine Learning (ICML-\n14). pp. 1323–1331 (2014)\n7. Mairal, J., Koniusz, P., Harchaoui, Z., Schmid, C.: Convolutional kernel networks. In: Ad-\nvances in Neural Information Processing Systems. pp. 2627–2635 (2014)\n8. Oja, E.: Neural networks, principal components, and subspaces. International journal of neu-\nral systems 1(01), 61–68 (1989)\n9. Olshausen, B.A., et al.: Emergence of simple-cell receptive ﬁeld properties by learning a\nsparse code for natural images. Nature 381(6583), 607–609 (1996)\n10. Pehlevan, C., Chklovskii, D.: A normative theory of adaptive dimensionality reduction in\nneural networks. In: Advances in Neural Information Processing Systems. pp. 2269–2277\n(2015)\n11. Pehlevan, C., Chklovskii, D.B.: A Hebbian/anti-Hebbian network derived from online non-\nnegative matrix factorization can cluster and discover sparse features. In: 2014 48th Asilomar\nConference on Signals, Systems and Computers. pp. 769–775. IEEE (2014)\n12. Poikonen, J.H., Laiho, M.: Online linear subspace learning in an analog array computing\narchitecture. CNNA 2016 (2016)\n13. Rumelhart, D.E., Zipser, D.: Feature discovery by competitive learning. Cognitive science\n9(1), 75–112 (1985)\n14. Sanger, T.D.: Optimal unsupervised learning in a single-layer linear feedforward neural net-\nwork. Neural networks 2(6), 459–473 (1989)\n15. Sohn, K., Lee, H.: Learning invariant representations with local transformations. In: Proceed-\nings of the 29th International Conference on Machine Learning (ICML-12). pp. 1311–1318\n(2012)\n",
  "categories": [
    "cs.NE",
    "cs.CV",
    "I.5.1"
  ],
  "published": "2017-02-21",
  "updated": "2018-01-29"
}