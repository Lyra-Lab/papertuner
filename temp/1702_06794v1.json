{
  "id": "http://arxiv.org/abs/1702.06794v1",
  "title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing",
  "authors": [
    "Minh Le",
    "Antske Fokkens"
  ],
  "abstract": "Error propagation is a common problem in NLP. Reinforcement learning explores\nerroneous states during training and can therefore be more robust when mistakes\nare made early in a process. In this paper, we apply reinforcement learning to\ngreedy dependency parsing which is known to suffer from error propagation.\nReinforcement learning improves accuracy of both labeled and unlabeled\ndependencies of the Stanford Neural Dependency Parser, a high performance\ngreedy parser, while maintaining its efficiency. We investigate the portion of\nerrors which are the result of error propagation and confirm that reinforcement\nlearning reduces the occurrence of error propagation.",
  "text": "Tackling Error Propagation through Reinforcement Learning:\nA Case of Greedy Dependency Parsing\nMinh Lê\nCLTL\nVrije Universiteit Amsterdam\nAmsterdam, The Netherlands\nm.n.le@vu.nl\nAntske Fokkens\nCLTL\nVrije Universiteit Amsterdam\nAmsterdam, The Netherlands\nantske.fokkens@vu.nl\nAbstract\nError propagation is a common problem\nin NLP. Reinforcement learning explores\nerroneous states during training and can\ntherefore be more robust when mistakes\nare made early in a process. In this paper,\nwe apply reinforcement learning to greedy\ndependency parsing which is known to\nsuffer from error propagation. Reinforce-\nment learning improves accuracy of both\nlabeled and unlabeled dependencies of\nthe Stanford Neural Dependency Parser,\na high performance greedy parser, while\nmaintaining its efﬁciency. We investigate\nthe portion of errors which are the result\nof error propagation and conﬁrm that rein-\nforcement learning reduces the occurrence\nof error propagation.\n1\nIntroduction\nError propagation is a common problem for many\nNLP tasks (Song et al., 2012; Quirk and Corston-\nOliver, 2006; Han et al., 2013; Gildea and Palmer,\n2002; Yang and Cardie, 2013). It can occur when\nNLP tools applied early on in a pipeline make\nmistakes that have negative impact on higher-level\ntasks further down the pipeline. It can also occur\nwithin the application of a speciﬁc task, when se-\nquential decisions are taken and errors made early\nin the process affect decisions made later on.\nWhen reinforcement learning is applied, a sys-\ntem actively tries out different sequences of ac-\ntions. Most of these sequences will contain some\nerrors. We hypothesize that a system trained in this\nmanner will be more robust and less susceptible to\nerror propagation.\nWe test our hypothesis by applying reinforce-\nment learning to greedy transition-based parsers\n(Yamada and Matsumoto, 2003; Nivre, 2004),\nwhich have been popular because of superior efﬁ-\nciency and accuracy nearing state-of-the-art. They\nare also known to suffer from error propagation.\nBecause they work by carrying out a sequence of\nactions without reconsideration, an erroneous ac-\ntion can exert a negative effect on all subsequent\ndecisions. By rendering correct parses unreach-\nable or promoting incorrect features, the ﬁrst error\ninduces the second error and so on. McDonald\nand Nivre (2007) argue that the observed negative\ncorrelation between parsing accuracy and sentence\nlength indicates error propagation is at work.\nWe compare reinforcement learning to super-\nvised learning on Chen and Manning (2014)’s\nparser. This high performance parser is available\nas open source. It does not make use of alterna-\ntive strategies for tackling error propagation and\nthus provides a clean experimental setup to test\nour hypothesis. Reinforcement learning increased\nboth unlabeled and labeled accuracy on the Penn\nTreeBank and German part of SPMRL (Seddah\net al., 2014). This outcome shows that reinforce-\nment learning has a positive effect, but does not yet\nprove that this is indeed the result of reduced er-\nror propagation. We therefore designed an exper-\niment which identiﬁed which errors are the result\nof error propagation. We found that around 50%\nof avoided errors were cases of error propagation\nin our best arc-standard system. Considering that\n27% of the original errors were caused by error\npropagation, this result conﬁrms our hypothesis.\nThis paper provides the following contributions:\n1. We introduce Approximate Policy Gradient\n(APG), a new algorithm that is suited for de-\npendency parsing and other structured pre-\ndiction problems.\n2. We show that this algorithm improves the ac-\ncuracy of a high-performance greedy parser.\narXiv:1702.06794v1  [cs.CL]  22 Feb 2017\n3. We design an experiment for analyzing error\npropagation in parsing.\n4. We conﬁrm our hypothesis that reinforce-\nment learning reduces error propagation.\nTo our knowledge, this paper is the ﬁrst to ex-\nperimentally show that reinforcement learning can\nreduce error propagation in NLP.\nThe rest of this paper is structured as follows.\nWe discuss related work in Section 2. This is fol-\nlowed by a description of the parsers used in our\nexperiments in Section 3. Section 4 outlines our\nexperimental setup and presents our results. The\nerror propagation experiment and its outcome are\ndescribed in Section 5. Finally, we conclude and\ndiscuss future research in Section 6.\n2\nRelated Work\nIn this section, we address related work on depen-\ndency parsing, including alternative approaches\nfor reducing error propagation, and reinforcement\nlearning.\n2.1\nDependency Parsing\nWe use Chen and Manning (2014)’s parser as a\nbasis for our experiments. Their parser is open-\nsource and has served as a reference point for\nmany recent publications (Dyer et al., 2015; Weiss\net al., 2015; Alberti et al., 2015; Honnibal and\nJohnson, 2015, among others). They provide an\nefﬁcient neural network that learns dense vec-\ntor representations of words, PoS-tags and depen-\ndency labels. This small set of features makes their\nparser signiﬁcantly more efﬁcient than other popu-\nlar parsers, such as the Malt (Nivre et al., 2007) or\nMST (McDonald et al., 2005) parser while obtain-\ning higher accuracy. They acknowledge the error\npropagation problem of greedy parsers, but leave\naddressing this through (e.g.) beam search for fu-\nture work.\nDyer et al. (2015) introduce an approach that\nuses Long Short-Term Memory (LSTM). Their\nparser still works incrementally and the number of\nrequired operations grows linearly with the length\nof the sentence, but it uses the complete buffer,\nstack and history of parsing decisions, giving the\nmodel access to global information. Weiss et al.\n(2015) introduce several improvements on Chen\nand Manning (2014)’s parser. Most importantly,\nthey put a globally-trained perceptron layer in-\nstead of a softmax output layer. Their model uses\nsmaller embeddings, rectiﬁed linear instead of cu-\nbic activation function, and two hidden layers in-\nstead of one.\nThey furthermore apply an aver-\naged stochastic gradient descent (ASGD) learn-\ning scheme. In addition, they apply beam search\nand increase training data by using unlabeled data\nthrough the tri-training approach introduced by Li\net al. (2014), which leads to further improvements.\nKiperwasser and Goldberg (2016) introduce a\nnew way to represent features using a bidirectional\nLSTM and improve the results of a greedy parser.\nAndor et al. (2016) present a mathematical proof\nthat globally normalized models are more expres-\nsive than locally normalized counterparts and pro-\npose to use global normalization with beam search\nat both training and testing.\nOur approach differs from all of the work men-\ntioned above, in that it manages to improve results\nof Chen and Manning (2014) without changing the\narchitecture of the model nor the input represen-\ntation. The only substantial difference lies in the\nway the model is trained. In this respect, our re-\nsearch is most similar to training approaches us-\ning dynamic oracles (Goldberg and Nivre, 2012).\nTraditional static oracles can generate only one se-\nquence of actions per sentence. A dynamic ora-\ncle gives all trajectories leading to the best pos-\nsible result from every valid parse conﬁguration.\nThey can therefore be used to generate more train-\ning sequences including those containing errors.\nA drawback of this approach is that dynamic or-\nacles have to be developed speciﬁcally for indi-\nvidual transition systems (e.g. arc-standard, arc-\neager). Therefore, a large number of dynamic or-\nacles have been developed in recent years (Gold-\nberg and Nivre, 2012; Goldberg and Nivre, 2013;\nGoldberg et al., 2014; Gomez-Rodriguez et al.,\n2014; Björkelund and Nivre, 2015). In contrast,\nthe reinforcement learning approach proposed in\nthis paper is more general and can be applied to a\nvariety of systems.\nZhang and Chan (2009) present the only study\nwe are aware of that also uses reinforcement learn-\ning for dependency parsing. They compare their\nresults to Nivre et al. (2006b) using the same fea-\ntures, but they also change the model and apply\nbeam search. It is thus unclear to what extend their\nimprovements are due to reinforcement learning.\nEven though most approaches mentioned above\nimprove the results reported by Chen and Man-\nning (2014) and even more impressive results on\ndependency parsing have been achieved since (no-\ntably, Andor et al. (2016)), Chen and Manning’s\nparser provides a better baseline for our purposes.\nWe aim at investigating the inﬂuence of reinforce-\nment learning on error propagation and want to\ntest this in a clean environment, where reinforce-\nment learning does not interfere with other meth-\nods that address the same problem.\n2.2\nReinforcement Learning\nReinforcement learning has been applied to sev-\neral NLP tasks with success, e.g. agenda-based\nparsing (Jiang et al., 2012), semantic parsing (Be-\nrant and Liang, 2015) and simultaneous machine\ntranslation (Grissom II et al., 2014). To our knowl-\nedge, however, none of these studies investigated\nthe inﬂuence of reinforcement learning on error\npropagation.\nLearning to Search (L2S) is probably the most\nprominent line of research that applies reinforce-\nment learning (more precisely, imitation learn-\ning) to NLP. Various algorithms, e.g. SEARN\n(Daumé III et al., 2009) and DAgger (Ross et\nal., 2011), have been developed sharing common\nhigh-level steps: a roll-in policy is executed to\ngenerate training states from which a roll-out pol-\nicy is used to estimate the loss of certain actions.\nThe concrete instantiation differs from one algo-\nrithm to another with choices including a referent\npolicy (static or dynamic oracle), learned policy,\nor a mixture of the two. Early work in L2S fo-\ncused on reducing reinforcement learning into bi-\nnary classiﬁcation (Daumé III et al., 2009), but\nnewer systems favored regressors for efﬁciency\n(Chang et al., 2015, Supplementary material, Sec-\ntion B). Our algorithm APG is simpler than L2S in\nthat it uses only one policy (pre-trained with stan-\ndard supervised learning) and applies the existing\nclassiﬁer directly without reduction (the only re-\nquirement is that it is probabilistic). Nevertheless,\nour results demonstrate its effectiveness.\nAPG belongs to the family of policy gradient al-\ngorithms (Sutton et al., 1999), i.e. it maximizes the\nexpected reward directly by following its gradient\nw.r.t. the parameters. The advantage of using a\npolicy gradient algorithm in NLP is that gradient-\nbased optimization is already widely used. REIN-\nFORCE (Williams, 1992; Ranzato et al., 2016) is\na widely-used policy gradient algorithm but it is\nalso well-known for suffering from high variance\n(Sutton et al., 1999).\nWe directly compare our approach to REIN-\nFORCE, whereas we leave a direct comparison\nto L2S for future work. Our experiments show\nthat our algorithm results in lower variance and\nachieves better performance than REINFORCE.\nRecent work addresses the approximation of re-\ninforcement learning gradient in the context of\nmachine translation.\nShen et al. (2016)’s algo-\nrithm is roughly equivalent to the combination\nof an oracle and random sampling.\nTheir ap-\nproach differs from ours, because it does not retain\nmemory across iteration as in our best-performing\nmodel (see Section 3.4).\n2.3\nReinforcement and error propagation\nAs mentioned above, previous work that applied\nreinforcement learning to NLP has, to our knowl-\nedge, not shown that it improved results by reduc-\ning error propagation.\nWork on identifying the impact of error prop-\nagation in parsing is rare, Ng and Curran (2015)\nbeing a notable exception. They provide a detailed\nerror analysis for parsing and classify which kind\nof parsing errors are involved with error propa-\ngation. There are four main differences between\ntheir approaches and ours. First, Ng and Curran\ncorrect arcs in the tree and our algorithm corrects\ndecisions of the parsing algorithm. Second, our\napproach distinguishes between cases where one\nerroneous action deterministically leads to multi-\nple erroneous arcs and cases where an erroneous\naction leads to conditions that indirectly result in\nfurther errors (see Section 5.1 for a detailed expla-\nnation). Third, Ng and Curran’s algorithm corrects\nall erroneous arcs that are the same type of pars-\ning error and point out that they cannot examine\nthe interaction between multiple errors of the same\ntype in a sentence. Our algorithm corrects errors\nincrementally and therefore avoids this issue. Fi-\nnally, the classiﬁcation and analysis presented in\nNg and Curran (2015) are more extensive and de-\ntailed than ours. Our algorithm can, however, eas-\nily be extended to perform similar analysis. Over-\nall, Ng and Curran’s approach for error analysis\nand ours are complementary. Combining them and\napplying them to various systems would form an\ninteresting direction for future work.\n3\nA Reinforced Greedy Parser\nThis section describes the systems used in our ex-\nperiments. We ﬁrst describe the arc-standard al-\nStep\nTransition\nStack\nBuffer\nArcs\n0\n<ROOT>\nwaves hit ... Big Board\n∅\n1\nSHIFT\n<ROOT> waves\nhit stocks ... Big Board\n∅\n2\nSHIFT\n<ROOT> waves hit\nstocks themselves ... Big Board\n∅\n3\nLEFTnsubj\n<ROOT> hit\nstocks themselves ... Big Board\nA1 = { hit\nnsubj\n−−−→waves}\n4\nSHIFT\n<ROOT> hit stocks\nthemselves on the Big Board\nA1\n5\nSHIFT\n<ROOT> hit stocks themselves\non the Big Board\nA1\n6\nRIGHTdep\n<ROOT> hit stocks\non the Big Board\nA2 = A1∪\n{ stock\ndep\n−−→themselves}\n7\nRIGHTdobj\n<ROOT> hit\non the Big Board\nA3 = A2∪{ hit\ndobj\n−−−→stock}\nTable 1: Parsing oracle walk-through\ngorithm, because familiarity with it helps to un-\nderstand our error propagation analysis. Next, we\nbrieﬂy point out the main differences between the\narc-standard algorithm and the alternative algo-\nrithms we experimented with (arc-eager and swap-\nstandard). We then outline the traditional and our\nnovel machine learning approaches. The features\nwe used are identical to those described in Chen\nand Manning (2014). We are not aware of research\nidentifying the best feature for a neural parser with\narc-eager or swap-standard so we use the same\nfeatures for all transition systems.\n3.1\nTransition-Based Dependency Parsing\nIn an arc-standard system (Nivre, 2004), a parsing\nconﬁguration consists of a triple ⟨Σ, β, A⟩, where\nΣ is a stack, β is a buffer containing the remain-\ning input tokens and A are the dependency arcs\nthat are created during parsing process. At initi-\nation, the stack contains only the root symbol (Σ\n= [ROOT]), the buffer contains the tokens of the\nsentence (β = [w1, ..., wn]) and the set of arcs is\nempty (A = ∅).\nThe arc-standard system supports three transi-\ntions. When σ1 is the top element and σ2 the sec-\nond element on the stack, and β1 the ﬁrst element\nof the buffer:1\nLEFTl adds an arc σ1\nl−→σ2 to A and removes σ2\nfrom the stack.\nRIGHTl adds an arc σ2\nl−→σ1 to A and removes\nσ1 from the stack.\nSHIFT moves β1 to the stack.\nWhen the buffer is empty, the stack contains\nonly the root symbol and A contains a parse tree,\nthe conﬁguration is completed. For a sentence of\n1Naturally, the transitions LEFTl and RIGHTl can only\ntake place if the stack contains at least two elements and\nSHIFT can only occur when there is at least one element on\nthe buffer.\n<ROOT> waves\nhit\nstocks\nthemselves on the Big Board\nFigure 1: Correct dependencies for a simpliﬁed\nexample from Penn TreeBank\nNw tokens, a full parse takes 2Nw + 1 transitions\nto complete (including the initiation).\nFigure 1\nprovides the gold parse tree for a (simpliﬁed) ex-\nample from the Penn Treebank. The steps taken\nto create the dependencies between the sentence’s\nhead word hit and its subject and direct object are\nprovided in Table 1.\nTo demonstrate that reinforcement learning can\ntrain different systems, we also carried out ex-\nperiments with arc-eager (Nivre, 2003) and swap-\nstandard (Nivre, 2009). Arc-eager is designed for\nincremental parsing and included in the popular\nMaltParser (Nivre et al., 2006a). Swap-standard is\na simple and effective solution to unprojective de-\npendency trees. Because arc-eager does not guar-\nantee complete parse trees, we used a variation\nthat employs an action called UNSHIFT to re-\nsume processing of tokens that would otherwise\nnot be attached to a head (Nivre and Fernández-\nGonzález, 2014).\n3.2\nTraining with a Static Oracle\nIn transition-based dependency parsing, it is com-\nmon to convert a dependency treebank D ∋(x, y)\ninto a collection of input features s ∈S and cor-\nresponding gold-standard actions a ∈A for train-\ning, using a static oracle O. In Chen and Man-\nning (2014), a neural network works as a function\nmapping input features to probabilities of actions:\nfNN : S × A →[0, 1]. The neural network is\ntrained to minimize negative log-likelihood loss\non the converted treebank:\nL =\nX\n(x,y)∈D\nX\n(s,a)∈O(x,y)\n−log fNN(s, a; θ)\n(1)\n3.3\nReinforcement Learning\nFollowing Maes et al. (2009), we view transition-\nbased dependency parsing as a deterministic\nMarkov Decision Process. The problem is sum-\nmarized by a tuple ⟨S, A, T , r⟩where S is the set\nof all possible states, A contains all possible ac-\ntions, T is a mapping S ×A →S called transition\nfunction and r : S × A →R is a reward function.\nA state corresponds to a conﬁguration and is\nsummarized into input features. Possible actions\nare deﬁned for each transition system described in\nSection 3.1. We keep the training approach simple\nby using only one reward r(¯y) at the end of each\nparse.\nGiven this framework,\na stochastic policy\nguides our parser by mapping each state to a prob-\nabilistic distribution of actions. During training,\nwe use function fNN described in Section 3.2 as a\nstochastic policy. At test time, actions are chosen\nin a greedy fashion following existing literature.\nWe aim at ﬁnding the policy that maximizes the\nexpected reward (or, equivalently, minimizes the\nexpected loss) on the training dataset:\nmaximize η =\nX\n(x,y)∈D\nX\na1:m∼f\nr(¯y)\nm\nY\ni=1\nfNN(si, ai; θ)\n(2)\nwhere a1:m is a sequence of actions obtained by\nfollowing policy fNN until termination and s1:m\nare corresponding states (with sm+1 being the ter-\nmination state).\n3.4\nApproximate Policy Gradient\nGradient ascent can be used to maximize the ex-\npected reward in Equation 2. The gradient of ex-\npected reward w.r.t. parameters is (note that dz =\nzd(log z)):\n∂η\n∂θ =\nX\n(x,y)∈D\nX\na1:m∼fNN\nr(¯y)\nm\nY\ni=1\nfNN(si, ai)\nm\nX\ni=1\n∂\n∂θ log fNN(si, ai; θ)\n(3)\nBecause of the exponential number of possible\ntrajectories, calculating the gradient exactly is not\npossible. We propose to replace it by an approxi-\nmation (hence the name Approximate Policy Gra-\ndient) by summing over a small subset U of trajec-\ntories. Following common practice, we also use a\nbaseline b(y) that only depends on the correct de-\npendency tree. The parameter is then updated by\nfollowing the approximate gradient:\n∆θ ∝\nX\n(x,y)∈D\nX\na1:m∈U\n(r(¯y) −b(y))\nm\nY\ni=1\nfNN(si, ai)\nm\nX\ni=1\n∂\n∂θ log fNN(si, ai; θ)\n(4)\nInstead of sampling one trajectory at a time as in\nREINFORCE, Equation 4 has the advantage that\nsampling over multiple trajectories could lead to\nmore stable training and higher performance. To\nachieve that goal, the choice of U is critical. We\nempirically evaluate three strategies:\nRL-ORACLE: only includes the oracle transition\nsequence.\nRL-RANDOM: randomly samples k distinct tra-\njectories at each iteration.\nEvery action is\nsampled according to fNN, i.e. preferring tra-\njectories for which the current policy assigns\nhigher probability.\nRL-MEMORY: samples randomly as the previ-\nous method but retains k trajectories with\nhighest rewards across iterations in a sepa-\nrate memory. Trajectories are “forgotten” (re-\nmoved) randomly with probability ρ before\neach iteration.2\nIntuitively, trajectories that are more likely and\nproduce higher rewards are better training exam-\nples. It follows from Equation 3 that they also\nbear bigger weight on the true gradient. This is the\nrationale behind RL-RANDOM and RL-ORACLE.\nFor a suboptimal parser, however, these objec-\ntives sometimes work against each other.\nRL-\nMEMORY was designed to ﬁnd the right balance\nbetween them. It is furthermore important that the\nparser is pretrained to ensure good samples. Algo-\nrithm 1 illustrates the procedure of training a de-\npendency parser using the proposed algorithms.\n2We assign a random number (drawn uniformly from\n[0, 1]) to each trajectory in memory and remove those as-\nsigned a number less than ρ.\nMemorySeqs ←∅;\nforeach training batch b do\nforeach sentence s ∈b do\nOracleSeq ←Oracle(s);\nSystemSeqs ←(sample k parsing\ntransition sequences for s);\nif RL-Oracle then\nComputeGradients(OracleSeq);\nelse if RL-Random then\nComputeGradients(SystemSeqs);\nelse if RL-Memory then\nm ←MemorySeqs[s];\nforeach q ∈m do\nif RandomNumber() < ρ then\nRemove q from m;\nend\nend\nforeach q ∈SystemSeqs do\nif |m| < k then\nInsert q into m;\nelse\np ←(sequence with\nsmallest reward in m);\nif reward(q) > reward(p)\nthen\nReplace p by q in m;\nend\nend\nComputeGradients(m);\nend\nPerform one gradient descent step;\nend\nAlgorithm 1: Training a dependency parser with\napproximate policy gradient.\n4\nReinforcement Learning Experiments\nWe ﬁrst train a parser using a supervised learning\nprocedure and then improve its performance using\nAPG. We empirically tested that training a second\ntime with supervised learning has little to no effect\non performance.\n4.1\nExperimental Setup\nWe use PENN Treebank 3 with standard split\n(training, development and test set) for our exper-\niments with arg-standard and arg-eager. Because\nthe swap-standard parser is mainly suited for non-\nprojective structures, which are rare in the PENN\nTreebank, we evaluate this parser on the German\nArc-\nArc-\nSwap-\nstandard\neager\nstandard\nUAS\nLAS\nUAS\nLAS\nUAS\nLAS\nSL\n91.3\n89.4\n88.3\n85.8\n84.3\n81.3\nRE\n91.9\n90.2\n89.7\n87.2\n87.5\n84.4\nRL-O\n91.8\n90.2\n88.9\n86.5\n86.8\n83.9\nRL-R\n92.2\n90.6\n89.4\n87.0\n87.5\n84.5\nRL-M\n92.2\n90.6\n89.8\n87.4\n87.6\n84.6\nTable 2: Comparing training methods on PENN\nTreebank (arc-standard and arc-eager) and Ger-\nman part of SPMRL-2014 (swap-standard).\nsection of the SPMRL dataset. For PENN Tree-\nbank, we follow Chen and Manning’s preprocess-\ning steps. We also use their pretrained model3 for\narc-standard and train our own models in similar\nsettings for other transition systems.\nFor reinforcement learning, we use AdaGrad for\noptimization. We do not use dropout because we\nobserved that it destablized the training process.\nThe reward r(¯y) is the number of correct labeled\narcs (i.e. LAS multiplied by number of tokens).4\nThe baseline is ﬁxed to half the number of tokens\n(corresponding to a 0.5 LAS score).\nAs train-\ning takes a lot of time, we tried only few values\nof hyperparameters on the development set and\npicked k = 8 and ρ = 0.01. 1,000 updates were\nperformed (except for REINFORCE which was\ntrained for 8,000 updates) with each training batch\ncontains 512 randomly selected sentences.\nThe\nStanford dependency scorer5 was used for evalu-\nation.\n4.2\nEffectiveness of Reinforcement Learning\nTable 2 displays the performance of different ap-\nproaches to training dependency parsers.\nAl-\nthough we used Chen and Manning (2014)’s pre-\ntrained model and Stanford open-source software,\nthe results of our baseline are slightly worse than\nwhat is reported in their paper. This could be due\nto minor differences in settings and does not affect\nour conclusions.\nAcross transition systems and two languages,\nAPG outperforms supervised learning, verifying\nour hypothesis.\nMoreover, it is not simply be-\ncause the learners are exposed to more examples\nthan their supervised counterparts. RL-ORACLE\n3We use PTB_Stanford_params.txt.gz down-\nloaded from http://nlp.stanford.edu/software/\nnndep.shtml on December 30th, 2015.\n4Punctuation is not taken into account, following Chen\nand Manning (2014).\n5Downloaded from http://nlp.stanford.edu/\nsoftware/lex-parser.shtml.\nis trained on exactly the same examples as the\nstandard supervised learning system (SL), yet it\nis consistently superior.\nThis can only be ex-\nplained by the superiority of the reinforcement\nlearning objective function compared to negative\nlog-likelihood.\nThe results support our hypothesis that APG is\nbetter than REINFORCE (abbreviated as RE in\nTable 2) as RL-MEMORY always outperforms the\nclassical algorithm and the other two heuristics do\nin two out of three cases. The usefulness of train-\ning examples that contain errors is evident through\nthe better performance of RL-RANDOM and RL-\nMEMORY in comparison to RL-ORACLE.\nTable 3 shows the importance of samples for\nRL-RANDOM. The algorithm hurts performance\nwhen only one sample is used whereas training\nwith two or more samples improves the results.\nThe difference cannot be explained by the total\nnumber of observed samples because one-sample\ntraining is still worse after 8,000 iterations com-\npared to a sample size of 8 after 1,000 itera-\ntions. The beneﬁt of added samples is twofold: in-\ncreased performance and decreased variance. Be-\ncause these beneﬁts saturate quickly, we did not\ntest sample sizes beyond 32.\nDev\nTest\nTest std.\nUAS\nLAS\nUAS\nLAS\nUAS\nLAS\nSL\n91.5\n89.6\n91.3\n89.4\n-\n-\nRE\n92.1∗\n90.4∗\n91.9∗\n90.2∗\n0.04\n0.05\n1\n91.2∗\n89.1∗\n91.0∗\n88.9∗\n0.12\n0.15\n2\n91.8∗\n90.0∗\n91.6∗\n89.9∗\n0.09\n0.09\n4\n92.2∗\n90.5∗\n92.0∗\n90.4∗\n0.09\n0.08\n8\n92.4∗\n90.8∗\n92.2∗\n90.6∗\n0.03\n0.05\n16\n92.4\n90.8\n92.2\n90.6\n-\n-\n32\n92.4\n90.8\n92.3\n90.6\n-\n-\nTable 3: Parsing accuracy of RL-RANDOM (arc-\nstandard) with different sample sizes compared to\nsupervised learning (SL) and REINFORCE (RE).\n∗: signiﬁcantly different from SL with p < 0.001\n5\nError Propagation Experiment\nWe hypothesized that reinforcement learning\navoids error propagation. In this section, we de-\nscribe our algorithm and the experiment that iden-\ntiﬁes error propagation in the arc-standard parsers.\n5.1\nError Propagation\nSection 3.1 explained that a transition-based\nparser goes through the sentence incrementally\nand must select a transition from [SHIFT, LEFTl,\n<ROOT> waves\nhit\nstocks\nthemselves on the Big Board\n<ROOT> waves\nhit\nstocks\nthemselves on the Big Board\n(A)\n(B)\n<ROOT> waves\nhit\nstocks\nthemselves on the Big Board\n(C)\nFigure 2: Three dependency graphs: gold (A), arc\nerrors caused by one decision error (B) and arc er-\nrors caused by multiple decision errors (C).\nRIGHTl] at each step. We use the term arc error\nto refer to an erroneous arc in the resulting tree.\nThe term decision error refers to a transition that\nleads to a loss in parsing accuracy. Decision er-\nrors in the parsing process lead to one or more arc\nerrors in the resulting tree. There are two ways\nin which a single decision error may lead to mul-\ntiple arc errors. First, the decision can determin-\nistically lead to more than one arc error, because\n(e.g.) an erroneously formed arc also blocks other\ncorrect arcs. Second, an erroneous parse decision\nchanges some of the features that the model uses\nfor future decisions and these changes can lead to\nfurther (decision) errors down the road.\nWe illustrate both cases using two incorrect\nderivations presented in Figure 2.\nThe original\ngold tree is repeated in (A). The dependency graph\nin Figure 2 (B) contains three erroneous depen-\ndency arcs (indicated by dashed arrows). The ﬁrst\nerror must have occurred when the parser executed\nRIGHTamod creating the arc Big →Board. After\nthis error, it is impossible to create the correct re-\nlations on →Board and Board →the. The wrong\narcs Big →the and on →Big are thus all the result\nof a single decision error.\nFigure 2 (C) represents the dependency graph\nthat is actually produced by our parser.6 It contains\ntwo erroneous arcs: hit →themselves and them-\nselves →on. Table 4 provides a possible sequence\nof steps that led to this derivation, starting from\nthe moment stocks was added to the stack (Step\n4). The ﬁrst error is introduced in Step 5’, where\nhit combines with stocks before stocks has picked\nup its dependent themselves. At that point, them-\nselves can no longer be combined with the right\nhead. The proposition on, on the other hand, can\n6The example is a fragment of a more complex sentence\nconsisting of 33 tokens. The parser does provide the correct\noutput when is analyzes this sequence in isolation.\nStep\nTransition\nStack\nBuffer\nArcs\n4\nSHIFT\n<ROOT> hit stocks\nthemselves on the Big Board\nA1\n5’\nRIGHTdobj\n<ROOT> hit\nthemselves on the Big Board\nA2 = A1∪\n{hit\ndobj\n−−−→stock}\n6’\nSHIFT\n<ROOT> hit themselves\non the Big Board\nA2\n7’\nSHIFT\n<ROOT> hit themselves on\nthe Big Board\nA2\n...\n10’\nSHIFT\n<ROOT> hit themselves on the Big Board\nA2\n11’\nLEFTnn\n<ROOT> hit themselves on the Board\nA3 = A2∪\n{Board\nnn\n−→Big}\n12’\nLEFTdet\n<ROOT> hit themselves on Board\nA4 = A3∪\n{Board\ndet\n−−→the}\n13’\nRIGHTpobj\n<ROOT> hit themselves on\nA5 = A4∪\n{on\npobj\n−−−→Board}\n14’\nRIGHTdep\n<ROOT> hit themselves\nA6 = A5∪\n{themselves\ndep\n−−→on}\nTable 4: Possible parsing walk-through with error\nstill be combined with the correct head. This error\nis introduced in Step 7’, where the parser moves\non to the stack rather than creating an arc from hit\nto themselves.7 There are thus two decision er-\nrors that lead to the arc errors in Figure 2 (C). The\nsecond decision error can, however, be caused in-\ndirectly by the ﬁrst error. If a decision error causes\nadditional decision errors later in the parsing pro-\ncess, we talk of error propagation. This cannot be\nknown just by looking at the derivation.\n5.2\nExamining the impact of decision errors\nWe examine the impact of individual decision er-\nrors on the overall parse results in our test set by\ncombining a dynamic oracle and a recursive func-\ntion. We use a dynamic oracle based on Goldberg\net al. (2014) which gives us the overall loss at any\npoint during the derivation. The loss is equal to\nthe minimal number of arc errors that will have\nbeen made once the parse is complete. We can\nthus deduce how many arc errors are deterministi-\ncally caused by a given decision error.\nThe propagation of decision errors cannot be\ndetermined by simply examining the increase in\nloss during the parsing process. We use a recur-\nsive function to identify whether a particular parse\nsuffered from this. While parsing the sentence, we\nregister which decisions lead to an increase in loss.\nWe then recursively reparse the sentence correct-\ning one additional decision error during each run\nuntil the parser produces the gold. If each erro-\nneous decision has to be corrected in order to ar-\nrive at the gold, we assume the decision errors are\n7Note that technically, on can still become a dependent\nof hit, but this can only happen if on becomes the head of\nthemselves which would also be an error.\nSL\nRL-O\nRL-R\nRL-M\nTotal Loss\n7069\n6227\n6042\n6144\nDec. Errors\n5177\n4410\n4345\n4476\nErr. Prop.\n1399\n1124\n992\n1035\nNew errors\n411\n432\n403\n400\nLoss/error\n1.37\n1.41\n1.39\n1.37\nErr. Prop. (%)\n27.0\n25.5\n22.8\n23.1\nTable 5: Overview of average impact of decision\nerrors\nindependent of each other. If, on the other hand,\nthe correction of a speciﬁc decision also ﬁxes other\ndecisions down the road, the original parse suffers\nfrom error propagation.\nThe results are presented in Table 5. Total Loss\nindicates the number of arc errors in the corpus,\nDec. Errors the number of decision errors and Err.\nProp. the number of decision errors that were the\nresult of error propagation. This number was ob-\ntained by comparing the number of decision er-\nrors in the original parse to the number of decision\nerrors that needed to be ﬁxed to obtain the gold\nparse. If less errors had to be ﬁxed than originally\npresent, we counted the difference as error prop-\nagation. Note that ﬁxing errors sometimes leads\nto new decision errors during the derivation. We\nalso counted the cases where more decision errors\nneeded to be ﬁxed than were originally present and\nreport them in Table 5.8\n8We ran an alternative analysis where we counted all cases\nwhere ﬁxing one decision error in the derivation reduced the\noverall number of decision errors in the parse by more than\none. Under this alternative analysis, similar reductions in the\nproportion of error propagation were observed for reinforce-\nment learning.\nOn average, decision errors deterministically\nlead to more than one arc error in the resulting\nparse tree.\nThis remains stable across systems\n(around 1.4 arc errors per decision error).\nWe\nfurthermore observe that the proportion of deci-\nsion errors that are the result of error propagation\nhas indeed reduced for all reinforcement learn-\ning models. Among the errors avoided by APG,\n35.9% were propagated errors for RL-ORACLE,\n48.9% for RL-RANDOM, and 51.9% for RL-\nMEMORY. These percentages are all higher than\nthe proportion of propagated errors occurring in\nthe corpus parsed by SL (27%). This outcome\nconﬁrms our hypothesis that reinforcement learn-\ning is indeed more robust for making decisions in\nimperfect environments and therefore reduces er-\nror propagation.\n6\nConclusion\nThis paper introduced Approximate Policy Gra-\ndient (APG), an efﬁcient reinforcement learning\nalgorithm for NLP, and applied it to a high-\nperformance greedy dependency parser. We hy-\npothesized that reinforcement learning would be\nmore robust against error propagation and would\nhence improve parsing accuracy.\nTo verify our hypothesis, we ran experiments\napplying APG to three transition systems and two\nlanguages. We furthermore introduced an exper-\niment to investigate which portion of errors were\nthe result of error propagation and compared the\noutput of standard supervised machine learning to\nreinforcement learning. Our results showed that:\n(a) reinforcement learning indeed improved pars-\ning accuracy and (b) propagated errors were over-\nrepresented in the set of avoided errors, conﬁrming\nour hypothesis.\nTo our knowledge, this paper is the ﬁrst to show\nexperimentally that reinforcement learning can re-\nduce error propagation in an NLP task. This re-\nsult was obtained by a straight-forward implemen-\ntation of reinforcement learning. Furthermore, we\nonly applied reinforcement learning in the training\nphase, leaving the original efﬁciency of the model\nintact. Overall, we see the outcome of our exper-\niments as an important ﬁrst step in exploring the\npossibilities of reinforcement learning for tackling\nerror propagation.\nRecent research on parsing has seen impressive\nimprovement during the last year achieving UAS\naround 94% (Andor et al., 2016). This improve-\nment is partially due to other approaches that, at\nleast in theory, address error propagation, such as\nbeam search. Both the reinforcement learning al-\ngorithm and the error propagation study we devel-\noped can be applied to other parsing approaches.\nThere are two (related) main questions to be ad-\ndressed in future work in the domain of parsing.\nThe ﬁrst addresses whether our method is comple-\nmentary to alternative approaches and could also\nimprove the current state-of-the-art. The second\nquestion would address the impact of various ap-\nproaches on error propagation and the kind of er-\nrors they manage to avoid (following Ng and Cur-\nran (2015)).\nAPG is general enough for other structured pre-\ndiction problems. We therefore plan to investigate\nwhether we can apply our approach to other NLP\ntasks such as coreference resolution or semantic\nrole labeling and investigate if it can also reduce\nerror propagation for these tasks.\nThe source code of all experiments is pub-\nlicly available at https://bitbucket.org/\ncltl/redep-java.\nAcknowledgments\nThe research for this paper was supported by the\nNetherlands Organisation for Scientiﬁc Research\n(NWO) via the Spinoza-prize Vossen projects (SPI\n30-673, 2014-2019) and the VENI project Read-\ning between the lines (VENI 275-89-029).\nEx-\nperiments were carried out on the Dutch national\ne-infrastructure with the support of SURF Co-\noperative.\nWe would like to thank our friends\nand colleagues Piek Vossen, Roser Morante, Tom-\nmaso Caselli, Emiel van Miltenburg, and Ngoc Do\nfor many useful comments and discussions. We\nwould like to extend our thanks the anonymous\nreviewers for their feedback which helped improv-\ning this paper. All remaining errors are our own.\nReferences\n[Alberti et al.2015] Chris Alberti, David Weiss, Greg\nCoppola, and Slav Petrov.\n2015.\nImproved\nTransition-Based Parsing and Tagging with Neu-\nral Networks. In EMNLP 2015, pages 1354–1359.\nACL.\n[Andor et al.2016] Daniel Andor, Chris Alberti, David\nWeiss, Aliaksei Severyn, Alessandro Presta, Kuz-\nman Ganchev, Slav Petrov, and Michael Collins.\n2016. Globally Normalized Transition-Based Neu-\nral Networks. arXiv.org, cs.CL.\n[Berant and Liang2015] Jonathan Berant and Percy\nLiang. 2015. Imitation Learning of Agenda-based\nSemantic Parsers. TACL, 3:545–558.\n[Björkelund and Nivre2015] Anders\nBjörkelund\nand\nJoakim Nivre. 2015. Non-Deterministic Oracles for\nUnrestricted Non-Projective Transition-Based De-\npendency Parsing.\nIn IWPT 2015, pages 76–86.\nACL.\n[Chang et al.2015] Kai-Wei Chang, Akshay Krishna-\nmurthy, Alekh Agarwal, Hal Daumé III, and John\nLangford. 2015. Learning to search better than your\nteacher. In ICML 2015.\n[Chen and Manning2014] Danqi Chen and Christopher\nManning. 2014. A Fast and Accurate Dependency\nParser using Neural Networks.\nIn EMNLP 2014,\npages 740–750. ACL.\n[Daumé III et al.2009] Hal Daumé III, John Langford,\nand Daniel Marcu. 2009. Search-based Structured\nPrediction. Machine Learning, 75(3):297–325, 6.\n[Dyer et al.2015] Chris\nDyer,\nMiguel\nBallesteros,\nWang Ling, Austin Matthews, and Noah A Smith.\n2015.\nTransition-Based Dependency Parsing with\nStack Long Short-Term Memory.\nIn ACL 2015,\npages 334–343.\n[Gildea and Palmer2002] Daniel Gildea and Martha\nPalmer. 2002. The Necessity of Parsing for Pred-\nicate Argument Recognition. In ACL 2002, pages\n239–246. ACL.\n[Goldberg and Nivre2012] Yoav Goldberg and Joakim\nNivre. 2012. A Dynamic Oracle for Arc-Eager De-\npendency Parsing. In COLING 2012, pages 959–\n976.\n[Goldberg and Nivre2013] Yoav Goldberg and Joakim\nNivre. 2013. Training Deterministic Parsers with\nNon-Deterministic Oracles.\nIn TACL 2013, vol-\nume 1, pages 403–414.\n[Goldberg et al.2014] Yoav Goldberg, Francesco Sarto-\nrio, and Giorgio Satta.\n2014.\nA tabular method\nfor dynamic oracles in transition-based parsing. In\nTACL 2014, volume 2, pages 119–130.\n[Gomez-Rodriguez et al.2014] Carlos\nGomez-\nRodriguez,\nFrancesco\nSartorio,\nand\nGiorgio\nSatta. 2014. A Polynomial-Time Dynamic Oracle\nfor Non-Projective Dependency Parsing. In EMNLP\n2014, pages 917–927. ACL.\n[Grissom II et al.2014] Alvin C. Grissom II, Jordan\nBoyd-Graber,\nHe He,\nJohn Morgan,\nand Hal\nDaume III. 2014. Don’t Until the Final Verb Wait:\nReinforcement Learning for Simultaneous Machine\nTranslation. In EMNLP 2014, pages 1342–1352.\n[Han et al.2013] Dan Han, Pascual Martínez-Gómez,\nYusuke Miyao, Katsuhito Sudoh, and Masaaki\nNagata.\n2013.\nEffects of parsing errors on\npre-reordering performance for Chinese-to-Japanese\nSMT. PACLIC 27, pages 267–276.\n[Honnibal and Johnson2015] Matthew\nHonnibal\nand\nMark Johnson. 2015. An Improved Non-monotonic\nTransition System for Dependency Parsing.\nIn\nEMNLP 2015, pages 1373–1378. ACL.\n[Jiang et al.2012] Jiarong Jiang, Adam Teichert, Hal\nDaumé III, and Jason Eisner. 2012. Learned Priori-\ntization for Trading Off Accuracy and Speed. ICML\nworkshop on Inferning: Interactions between Infer-\nence and Learning, (0964681):1–9.\n[Kiperwasser and Goldberg2016] Eliyahu Kiperwasser\nand Yoav Goldberg.\n2016.\nSimple and Accu-\nrate Dependency Parsing Using Bidirectional LSTM\nFeature Representations. CoRR, abs/1603.0.\n[Li et al.2014] Zhenghua Li, Min Zhang, and Wenliang\nChen. 2014. Ambiguity-aware Ensemble Training\nfor Semi-supervised Dependency Parsing. In ACL\n2014, pages 457–467.\n[Maes et al.2009] Francis Maes, Ludovic Denoyer, and\nPatrick Gallinari.\n2009.\nStructured prediction\nwith reinforcement learning.\nMachine Learning,\n(77):271–301.\n[McDonald and Nivre2007] Ryan\nMcDonald\nand\nJoakim Nivre.\n2007.\nCharacterizing the Errors\nof Data-Driven Dependency Parsing Models.\nIn\nEMNLP-CoNLL 2007.\n[McDonald et al.2005] Ryan\nMcDonald,\nFernando\nPereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-\nprojective dependency parsing using spanning tree\nalgorithms. In HLT-EMNLP 2005, pages 523–530.\nAssociation for Computational Linguistics.\n[Ng and Curran2015] Dominick Ng and James R Cur-\nran. 2015. Identifying Cascading Errors using Con-\nstraints in Dependency Parsing.\nIn ACL-IJCNLP,\npages 1148–1158, Beijing. ACL.\n[Nivre and Fernández-González2014] Joakim\nNivre\nand Daniel Fernández-González. 2014. Arc-eager\nParsing with the Tree Constraint.\nComputational\nLinguistics, 40(2):259–267, 6.\n[Nivre et al.2006a] Joakim Nivre, Johan Hall, and Jens\nNilsson. 2006a. MaltParser: A data-driven parser-\ngenerator for dependency parsing. In LREC 2006,\nvolume 6, pages 2216–2219.\n[Nivre et al.2006b] Joakim Nivre, Johan Hall, Jens\nNilsson, Gül¸sen Eryi˘git, and Svetoslav Marinov.\n2006b. Labeled pseudo-projective dependency pars-\ning with support vector machines. In CoNLL 2006,\npages 221–225. ACL.\n[Nivre et al.2007] Joakim Nivre, Johan Hall, Jens Nils-\nson, Atanas Chanev, Eryiˇgit Gül¸sen, Sandra Kübler,\nSvetoslav Marinov, and Erwin Marsi. 2007. Malt-\nParser: A language-independent system for data-\ndriven dependency parsing. Natural Language En-\ngineering, 13(02):95–135.\n[Nivre2003] Joakim Nivre. 2003. An Efﬁcient Algo-\nrithm for Projective Dependency Parsing. In IWPT\n2003, pages 149–160.\n[Nivre2004] Joakim Nivre.\n2004.\nIncrementality in\nDeterministic Dependency Parsing. In Proceedings\nof the Workshop on Incremental Parsing: Bringing\nEngineering and Cognition Together.\n[Nivre2009] Joakim Nivre. 2009. Non-projective De-\npendency Parsing in Expected Linear Time. In ACL-\nIJCNLP 2009, pages 351–359, Stroudsburg, PA,\nUSA. ACL.\n[Quirk and Corston-Oliver2006] Chris Quirk and Si-\nmon Corston-Oliver.\n2006.\nThe impact of parse\nquality on syntactically-informed statistical machine\ntranslation. In EMNLP 2006, pages 62–69, Sydney,\nAustralia. ACL.\n[Ranzato et al.2016] Marc’Aurelio\nRanzato,\nSumit\nChopra, Michael Auli, and Wojciech Zaremba.\n2016.\nSequence Level Training with Recurrent\nNeural Networks. ICLR, pages 1–15.\n[Ross et al.2011] Stephane Ross, Geoffrey J Gordon,\nand J Andrew Bagnell. 2011. A Reduction of Im-\nitation Learning and Structured Prediction to No-\nRegret Online Learning. AISTATS, 15:627–635.\n[Seddah et al.2014] Djamé Seddah, Sandra Kübler, and\nReut Tsarfaty. 2014. Introducing the SPMRL 2014\nShared Task on Parsing Morphologically-Rich Lan-\nguages. In Proceedings of the First Joint Workshop\non Statistical Parsing of Morphologically Rich Lan-\nguages and Syntactic Analysis of Non-Canonical\nLanguages, pages 103–109.\n[Shen et al.2016] Shiqi Shen, Yong Cheng, Zhongjun\nHe, Wei He, Hua Wu, Maosong Sun, and Yang Liu.\n2016. Minimum Risk Training for Neural Machine\nTranslation. In ACL 2016, pages 1683–1692, Berlin,\nGermany. ACL.\n[Song et al.2012] Hyun-Je Song, Jeong-Woo Son, Tae-\nGil Noh, Seong-Bae Park, and Sang-Jo Lee. 2012.\nA Cost Sensitive Part-of-Speech Tagging: Differen-\ntiating Serious Errors from Minor Errors. In ACL\n2012, pages 1025–1034. ACL.\n[Sutton et al.1999] Richard\nS.\nSutton,\nDavid\nMcallester, Satinder Singh, and Yishay Mansour.\n1999. Policy Gradient Methods for Reinforcement\nLearning with Function Approximation.\nIn NIPS\n1999, pages 1057–1063.\n[Weiss et al.2015] David Weiss, Chris Alberti, Michael\nCollins, and Slav Petrov. 2015. Structured Train-\ning for Neural Network Transition-Based Parsing.\nIn ACL-IJCNLP 2015, pages 323–333. ACL.\n[Williams1992] Ronald J. Williams.\n1992.\nSimple\nstatistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine Learning,\n8(3-4):229–256.\n[Yamada and Matsumoto2003] Hiroyasu Yamada and\nYuji Matsumoto.\n2003.\nStatistical Dependency\nAnalysis with Support Vector Machines.\nIn Pro-\nceedings of IWPT, pages 195–206.\n[Yang and Cardie2013] Bishan Yang and Claire Cardie.\n2013. Joint Inference for Fine-grained Opinion Ex-\ntraction. In ACL 2013, pages 1640–1649. ACL.\n[Zhang and Chan2009] Lidan Zhang and Kwok Ping\nChan.\n2009.\nDependency Parsing with Energy-\nbased Reinforcement Learning.\nIn IWPT 2009,\npages 234–237. ACL.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-02-22",
  "updated": "2017-02-22"
}