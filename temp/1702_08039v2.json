{
  "id": "http://arxiv.org/abs/1702.08039v2",
  "title": "Criticality & Deep Learning I: Generally Weighted Nets",
  "authors": [
    "Dan Oprisa",
    "Peter Toth"
  ],
  "abstract": "Motivated by the idea that criticality and universality of phase transitions\nmight play a crucial role in achieving and sustaining learning and intelligent\nbehaviour in biological and artificial networks, we analyse a theoretical and a\npragmatic experimental set up for critical phenomena in deep learning. On the\ntheoretical side, we use results from statistical physics to carry out critical\npoint calculations in feed-forward/fully connected networks, while on the\nexperimental side we set out to find traces of criticality in deep neural\nnetworks. This is our first step in a series of upcoming investigations to map\nout the relationship between criticality and learning in deep networks.",
  "text": "Criticality & Deep Learning I: Generally Weighted Nets\nDan Oprisa\ndan.oprisa@critical.ai\nPeter Toth\npeter.toth@critical.ai\nCriticalAI\nhttp://www.critical.ai\nAbstract\nMotivated by the idea that criticality and\nuniversality of phase transitions might play a\ncrucial role in achieving and sustaining learn-\ning and intelligent behaviour in biological and\nartiﬁcial networks, we analyse a theoretical\nand a pragmatic experimental set up for crit-\nical phenomena in deep learning.\nOn the\ntheoretical side, we use results from statis-\ntical physics to carry out critical point calcu-\nlations in feed-forward/fully connected net-\nworks, while on the experimental side we set\nout to ﬁnd traces of criticality in deep neural\nnetworks. This is our ﬁrst step in a series of\nupcoming investigations to map out the re-\nlationship between criticality and learning in\ndeep networks.\n1\nIntroduction\nVarious systems in nature display patterns, forms, at-\ntractors and recurrent behavior, which are not caused\nby a law per se; the ubiquity of such systems and\nsimilar statistical properties of their exhibit order has\nlead to the term ”universality”, since such phenomena\nshow up in cosmology, the fur of animals [1], chemical\nand physical systems [2], landscapes, biological prey-\npredator systems and endless many others [3]. Fur-\nthermore, because of universality, it turns out that\nthe most simplistic mathematical models exhibit the\nsame statistical properties when their parameters are\ntuned correctly. As such it suﬃces to study N-particle\nsystems with simple, ”atomistic” components and in-\nteractions since they already exhibit many non-trivial\nemergent properties in the large N limit. Certain ”or-\nder” parameters change behavior in a non-classical\nfashion, for speciﬁc noise levels. Using the rich and\ndeep knowledge gained in statistical physics about\nthose systems, we map the mathematical properties\nand learn about novel behaviors in deep learning set\nups. Speciﬁcally we look at a collection of N units on\na lattice with various pair interactions; when the units\nare binary spins with values (±1), the model is known\nas a Curie-Weiss model. From a physical point of view,\nthis is one of the basic, analytically solvable models,\nwhich still possesses the rich emergent properties of\ncritical phenomena. However, given its general math-\nematical structure, the model has already been used\nto explain population dynamics in biology [4], opin-\nion formation in society [5], machine learning [6, 7, 8]\nand many others [9, 10].\nAll those systems, with a\nrich and diverse origination, posses almost identical\nbehavior at criticality. In the latter case of machine\nlearning, the Curie-Weiss model encodes information\nabout fully connected and feed-forward architectures\nto ﬁrst order. Similar work was done in [11, 12], where\ninsights from Ising models and fully connected layers\nare drawn and applied to net architectures; in [13] a\nnatural link between the energy function and an au-\ntoencoder is established. We will address the gener-\nalisation of fully connected system and understand its\nproperties, before moving to the deep learning network\nand applying there the same techniques and intuition.\nThe article is organised as follows: section 2 gives a\nshort introduction of critical systems and appropriate\nexamples from physics; in section 3 we map a concrete,\nnon-linear, feed forward net to its physical counterpart\nand discuss other architectures as well; then we turn\nto investigating the practical question whether we can\nspot traces of criticality in current deep learning nets\nin 4. Finally we summarise our ﬁndings in 5 and hint\nat future directions for the rich map between statistical\nsystems and deep learning.\n2\nBrief summary of critical\nphenomena\nCritical phenomena were ﬁrst thoroughly explained\nand analysed in the ﬁeld of statistical mechanics, al-\nthough they were observed in various other systems,\nbut lacking a theoretical understanding. The study of\ncriticality belongs to statistical physics and is an in-\ncredibly rich and wide ﬁeld, hence we can only brieﬂy\nsummarise some few results of interest for the present\narticle; deﬁnitely a much more comprehensive cover-\nage can be found, see e.g.[14, 15, 16, 17]. In a nut-\nshell, the subject is concerned with the behavior of\narXiv:1702.08039v2  [cs.AI]  31 May 2017\nCriticality & Deep Learning I: Generally Weighted Nets\n2\nsystems in the neighbourhood of their critical points,\n[18]. One thus looks at systems composed of (families\nof) many, identical particles, trying to derive prop-\nerties for macroscopic parameters, such as density or\npolarisation from the microscopic properties and inter-\nactions of the particles; statistical mechanics can hence\nbe understood as a bridge between macroscopic phe-\nnomenology (e.g. thermodynamics) and microscopic\ndynamics (e.g. molecular or quantum-mechanical in-\nteracting collections of particles). In a nutshell, criti-\ncality is achieved when macroscopic parameters show\nanomalous, divergent behavior at a phase transition.\nDepending on the system at hand, the parameters\nmight be magnetisation, polarisation, correlation, den-\nsity, etc. Speciﬁcally it is the correlation function of\nthe ”components” which then displays divergent be-\nhavior, and signals strong coordinated group behavior\nover a wide range of magnitudes.\nUsually it is the\nnoise (temperature) which at certain values will in-\nduce the phase transition accompanied by the critical\nanomalous behavior.\nGiven its relevance in physics\nand also its mathematical analogy to our deep learn-\ning networks, we will brieﬂy review here the Curie-\nWeiss model with non-constant coupling and examine\nits behavior at criticality.\n2.1\nCurie-Weiss model\nA simplistic, fully solvable model for a magnet is the\nCurie-Weiss model (CW), [19]. It possesses many in-\nteresting features, exhibits critical behavior and cor-\nrectly predicts some of the experimental ﬁndings. As\nits mathematics is later on used in our deep learning\nsetup, we will brieﬂy present main properties and so-\nlutions for the sake of self-consistency.\nThe Hamiltonian of the CW model is given by\nH = −J\n2N\nN\nX\nij\nsisj −b\nN\nX\ni\nsi\n(1)\nHere the si are a collection of interacting ”particles”, in\nour physical case, spins, that interact with each other\nvia the coupling J; they take values (±1) and inter-\nact pairwise with each other, at long distances; the\ninclusion of a factor of\n1\nN multiplying the quadratic\nspin term makes this long-range interaction tractable\nin the large N limit. Furthermore, there is a directed\nexternal magnetic ﬁeld which couples to every spin via\nb. Since the coupling between spins is a constant and\nsince every spin interacts with every other spin (except\nself-interactions, which is accounted by a factor of 1\n2)\nthe Hamiltonian can be rewritten to\nH = −J\n2N\n\u0012 N\nX\ni\nsi\n\u00132\n−b\nN\nX\ni\nsi\n(2)\nWith β = 1/kT being the inverse temperature the par-\ntition function can be formulated\nZ =\nX\nsi∈{±1}\ne−βH(s)\n(3)\n=\nX\nsi∈{±1}\nexp β\n\u0014 J\n2N\n\u0012 PN\ni si\n\u00132\n+ b PN\ni si\n\u0015\n(4)\nwhich can be fully solved, [19], summing over each of\nthe 2N states; given an explicit partition Z, the free\nenergy can be computed via\nF = −kT ln Z\n(5)\nOnce we have F various macroscopic values of inter-\nest can be inferred such as the magnetisation of the\nsystem, aka ﬁrst derivative of F wrt. b. This is a so\ncalled ”order parameter”, which carries various other\ndenominations, such as polarisation, density, opinion\nimbalance, etc. depending on the system at hand. It\nbasically measures how arranged or homogeneous the\nsystem is under the inﬂuence of the outside ﬁeld which\ncouples to the spins via b. A full treatment and deriva-\ntion of the model including all its critical behavior can\nbe found in [20], from where we get the equation of\nstate for the magnetisation\nm = b tanh\n\u0000K\nb m + b\nT\n\u0001\n(6)\nwith K = ( J\nT )1/2. The analysis of this equation for\nvarious temperatures T and couplings J, b reveals a\nphase transition at the critical temperature Tc = J.\nIntroducing the dimensionless parameter t = (T/Tc−1)\nand expanding (6) in small couplings the famous power\nlaw dependence on temperature for the magnetisation\nemerges:\nm ≃\n√\n3(K −1)1/2\nK3/2\n∼|t|1/2\n(7)\nHere we recognise one of the very typical power laws\nwhich are ubiquitous to critical systems. The quantity\nwe are most interested in though is the second deriva-\ntive of the free energy F wrt.\nb, which is basically\nthe 2-point correlation function of the spins si. Again,\nexpanding the second derivative of the free energy in\nsmall couplings and looking in the neighbourhood of\nthe critical temperature Tc yields\n⟨si, sj⟩∼b2\nTc\n|t|−1\n(8)\nCriticality & Deep Learning I: Generally Weighted Nets\n3\nagain displaying power law behavior with a power co-\neﬃcient γ = 1. The innocent looking equation 8 has\nactually tremendous consequences, as it implies that\ncorrelation does not simply restrict to nearest neigh-\nbours but goes over very long distances only slowly\ndecaying; further, because of the power law behavior,\nthere will be self-similar, fractal patterns in the sys-\ntem: islands of equal magnetisation will form within\nother islands and so on, all the way through all scales.\nAlso, the correlation diverges at the criticality point\nTc. We will carry out the explicit calculations for our\ncase of interest - non-constant matrix couplings - later\none, in section 3.1.\n2.2\nCriticality in real-world networks\nTwo of the main motivations why we look for criti-\ncality and exploit on it in artiﬁcial networks, are the\nuniversal arising of this phenomenon as well as vari-\nous hints of its occurrence in biological [4] and neural\nsystems [25, 22]; once systems get ”sizable” enough,\ngaining complexity, critical behavior emerges, which\nalso applies to man-made nets [23]. Various measures\ncan be formulated to detect criticality, and they all\nshow power law distribution behavior. In the world\nwide web, e.g. the number of links to a source, and\nthe number of links away from a source, both exhibit\npower law distribution\nP(k) ∼k−γ\n(9)\nfor some power coeﬃcient γ ̸= 0.\nSimilar behavior\ncan be uncovered in various other networks, if sizable\nenough, such as citation behavior of scientiﬁc articles,\nsocial networks, etc. A simple, generic metric to de-\ntect criticality in networks is the degree distribution,\ndeﬁned as the number of (weighted) links connecting\nto one node.\nFurther, also the correlation between nodes is non-\ntrivial, such that nodes with similar degree have higher\nprobability of being connected than nodes of diﬀerent\ndegree [23], chapter VII. We will follow a similar path\nas proposed above and grow an experimental network\nwith new nodes having the simplest preferential and\ndirected attachment towards existing nodes, as a func-\ntion of their degree:\nΠ(k) ∼kα\n(10)\nHere, Π(k) denotes the probability that some node will\ngrow a link to another node of degree k. Hence, ev-\nery new node, will prefer nodes with higher degrees,\nleading to the overall power distribution observed in\nthe real world systems. Additional metrics we look at\nare single neuron activity as well as layer activity and\npattern behavior; more details on that in section 4.\n3\nCriticality in deep learning nets\n3.1\nFrom feed-forward to fully connected\narchitecture\nWe will focus now on a feed-forward network, with two\nlayers, ai and bj connected via a weight matrix wij; In\norder to probe our system for criticality, we write down\nits Hamiltonian\nH = −1\n2N\nN\nX\nij\nwijaibj −h\nN\nX\ni\nbi\n(11)\nwhich has been ﬁrst formulated in the seminal paper\n[9]. Here, the values of the a and b are {0, 1}. Further,\nby absorbing the biases bi in the usual way we can\nassume our weight matrix has the form:\nW =\n\n\n\n\n2Nh\n0\n· · ·\n0\n2Nh\nw11\n· · ·\nw1n\n...\n...\n...\n...\n−2Nh\nwn1\n· · ·\nwnn\n\n\n\n\n(12)\nwhile the Vi read (1, V1, · · · , VN).\nThis Hamiltonian describes a two layer net contain-\ning rectiﬁed linear units (ReLU) in the b-layer with a\ncommon bias term h. The weight matrix wij sums the\nbinary inputs coming from the ai and those are fed\ninto bi; depending whether the ReLU threshold has\nbeen reached, ai is activated, hence the binary values\nallowed for both, inputs and b-layer.\nFurther, we show in appendix A, that the partition\nfunction is up to a constant the same for the units\ntaking values in {±1} or {0, 1}. By redeﬁning N +1 →\nN We can then formulate the partition function as\nZ =\nX\na,b∈{±1}\ne−β\n2N\nP\nij Wijaibj\n(13)\nwhere β is the inverse temperature 1/T. This is the par-\ntition function of a bipartite graph with non-constant\nconnection matrix w.\nHowever, it turns out, that the partition function of\nthe fully connected layer is the highest contribution\n(1st order) of our feed forward network (see appendix\nB), hence further simplifying the expression to\nZ =\nX\nsi∈{±1}\ne−β\n2N\nP\nij Wijsisj\n(14)\nWe will now proceed and compute the free energy F,\ndeﬁned as F = −T ln Z, using the procedure presented\nin [10]. From the free energy we then ﬁnd all quantities\nof interest, especially the 2-point correlation function\nof the neurons.\nCriticality & Deep Learning I: Generally Weighted Nets\n4\n3.2\nFully connected architecture with\nnon-constant weights\nIn order to solve the CW model analytically, one has to\nperform the sum over spins, which is hindered by the\nquadratic term sisj. The standard way to overcome\nthis problem is the gaussian linearisation trick which\nreplaces quadratic term by its square root - linear in si\nand one additional continuous variable - the ”mean”\nﬁeld, which is being integrated over entire R:\nea2 =\n1\n√\n2π\nZ ∞\n−∞\ndxe−x2/2+\n√\n2ax\n(15)\nwhich\nin\nphysics,\nis\nknown\nas\nthe\nHubbard-\nStratonovich transform.\nUnfortunately our coupling is not scalar, and hence we\nwill linearise the sum term by term to keep track of all\nthe weight matrix entries. First we will insert N iden-\ntities via the Dirac delta function into our Hamiltonian\nas used in (14):\nH(s) = −1\n2N\nN\nX\nij\nsiWijsj\n(16)\n= −1\n2N\nY\nk\nZ ∞\n−∞\ndVkδ(sk −Vk)\nN\nX\nij\nViWijVj\n=\nY\nk\nZ ∞\n−∞\ndVkδ(sk −Vk)H(V )\nWith the deﬁnition of the delta function δ(x) =\n1\n2πi\nR i∞\n−i∞dyexy the partition function (14) reads now\nZ(s) =\nY\nk\nZ ∞\n−∞\ndVkδ(sk −Vk)\nX\nsi∈{±1}\ne−βH(V )\n(17)\n∼\nY\nk\nZ ∞\n−∞\ndVk\nZ i∞\n−i∞\ndUk\nX\nsi∈{±1}\neUk(sk−Vk)e−βH(V )\n=\nY\nk\nZ ∞\n−∞\ndVk\nZ i∞\n−i∞\ndUke−UkVk+ln(cosh Uk)e−βH(V )\nAs already stated, we could perform the sum over the\nbinary units si, since they show up linearly in the expo-\nnential after the change of variables via delta identity1;\nwe eﬀectively converted the sum over binary values\n{±1} into integrals over R, leading to\n1In general we’re not interested in numerical multiplica-\ntive constants, as later on, when logging the partition and\ncomputing the free energy, those terms will be simple ad-\nditive constants without any contribution after diﬀerenti-\nating the expression\nZ = c\nY\ni\nZ ∞\n−∞\ndVi\nZ i∞\n−i∞\ndUi e−Hg(V,U,T )\n(18)\nwith a generalised Hamiltonian\nHg = −β\n2N\nX\nij\nWijViVj +\nX\ni\n\u0002\nUiVi −ln (cosh Ui)\n\u0003\n= −β\n2N\nP\nij wijViVj −βh P\ni Vi\n+\nX\ni\n\u0002\nUiVi −ln (cosh Ui)\n\u0003\n(19)\nUltimately we are interested in the free energy per\nunit, which contains the partition function, via\nF = lim\nN→∞(−T ln Z)/N\n(20)\nFrom F we can now obtain all quantities of interest\nvia derivatives, in our case with respect to h.\nThe\npartition function Z still contains a product of double\nintegrals, which can be solved via the saddle point ap-\nproximation; we recall here the one-dimensional case\nZ ∞\n−∞\ndxe−f(x) ≈\n\u00002π\nf ′′(x0)\n\u00011/2e−f(x0)\n(21)\nwhere x0 is the stationary value of f and f ′′(x0) is in\nour case the Hessian evaluated at the stationary point:\nHg\nViVj = −β\nN wij\n(22)\nHg\nUiUj = −δij(1 −tanh2 Ui)\nHg\nViUj = δij\n(23)\nwhile Hg is given in (19).\nThe expression 18 can now be computed by applying\nsimultaneously the saddle point conditions for both\nintegrals. The stationarity conditions2 for Vi and Ui\ngive\n∂Hg\n∂Vi\n= −β\nN\nX\nj\nWijVj + Ui = 0\n(24)\n= −β(P\nj wijVj/N + h) + Ui = 0\n∂Hg\n∂Ui\n= Vi −tanh Ui = 0\n2We keep in mind that we enlarged W to contain h as\nwell, hence the explicit equations are h dependent\nCriticality & Deep Learning I: Generally Weighted Nets\n5\nwhich combined deliver the self consistency mean ﬁeld\nequation of the fully connected layer (29).\nFurther,\ndenoting Hg\n0 the the Hamiltonian satisfying the sta-\ntionarity conditions, it reads\nHg\n0 = β\n2N\nP\nij wijViVj\n(25)\n−\nX\ni\nln cosh β(P\nj wijVj/N + h)\nEquation (25) already displays manifestly the consis-\ntency equation for the mean ﬁeld, as taking the ﬁrst\nderivative wrt. Vi leaves exactly the consistency equa-\ntion over per its construction;\nNow we can rewrite the free energy (20) as\nF = lim\nN→∞\nT\nN\n\u0002\nHg\n0 + ln det Hg\nhh\n\u0003\n∼lim\nN→∞\n(26)\n\u0002\n1\n2N 2\nX\nij\nwijViVj + 1\nN 2\nX\nij\nln[wij(1 −V 2\ni ) −1]\n−T\nN\nX\ni\nln cosh β(P\nj wijVj/N + h)\n\u0003\nWe need to address now the large N limit; obviously\nthe second term coming from the determinant clearly\nvanishes in the large-N limit, as the logarithm is slowly\nincreasing, while we divide through N 2; the ﬁrst term\n- a double sum over Vi is of order N 2 and hence a well\ndeﬁned average in the limit; the last term - ln cosh,\nwhen expanded, is again linear in the sum3, and hence\na well deﬁned average after dividing through N, hence\nwe’re left with the free energy\nF = T\nN Hg\n0\n(27)\n=\n1\n2N 2\nX\nij\nwijViVj\n−T\nN\nX\ni\nln cosh β(P\nj wijVj/N + h)\n\u0003\nWe’re at the point now, where all quantities of interest\ncan be derived from the free energy F; the order pa-\nrameter (aka magnetisation when dealing with spins)\nper unit is deﬁned as\nm ≡dF\ndh = ∂F\n∂h\n\f\f\f\f\nV st\n+\n\u0013\n\u0013\n\u00137\n0\n∂F\n∂Vi\n∂Vi\n∂h\n\f\f\f\f\nV st\n(28)\n3The interior sum over j is an average, hence well de-\nﬁned in the limit; after expansion, we’re left with the outer\nsum (over i), which is again a well deﬁned average when\ndivided by N\nThe second term on the right vanishes identically, as\nwe recognize it being evaluated at the stationarity con-\ndition V st for the Hamiltonian. The contribution of\nthe ﬁrst term is:\nX\ni\nwikVk/N = 1\nN\nX\ni\ntanh β(P\nk wikVk/N + h)wik\n⇕\n(29)\nVi = tanh β(P\nk wikVk/N + h)\nwhich is (the weighted sum version of) the iconic self-\nconsistency mean ﬁeld equation of the CW magnet (6).\nThe critical point, Pc is located where the correlation\nfunction diverges for h →0; the 2-point correlation\nfunction (aka susceptibility when dealing with spins)\nis the second derivative of F, i.e. the derivative of (29)\nwrt. h:\nPc ≡d2F\ndh2 = dm\ndh\n(30)\n⇕\n∂Vi\n∂h = β(1 −V 2\ni )(1 +\nX\nk\nwik\n∂Vk\n∂h /N)\nwhere we used the original equation (29) for taking\nthe derivatives. It is worth contemplating ﬁrst equa-\ntions (29) and (30). They both capture the essence\nof the criticality of our system, including it’s power\nlaw behavior. When the weight matrix reduces to a\nscalar coupling, both equations reduce to the classical\nCW system and display the behavior shown in (7) and\n(8). Furthermore, eq. (30) encodes all the information\nneeded for ﬁnding the critical point of matrix system\nat hand; we recall that all V s (and their derivatives)\nare already implicitly ”solved” in terms of h and wij\nvia the stationarity equation (29) and hence the Vi are\njust place holders for functions of w and h; we’re thus\nleft with a non-linear system of ﬁrst order diﬀeren-\ntial equations in N variables, which will produce poles\nfor speciﬁc values of the couplings and temperature at\ncriticality.\nCriticality & Deep Learning I: Generally Weighted Nets\n6\n1.14\n1.16\n1.18\n1.20\n1.22\n1.24\n1.26\n1.28\n1.30\nlog weight degree\n100\n101\n102\nlog counts\nweight distribution\nFigure 1: Feed-forward net: Layer 3 weight distribu-\ntion\n4\nExperimental results\nAfter investigating criticality through the partition\nfunction in our theoretical setup, now we turn to a\npractical question: do current deep learning networks\nexhibit critical behaviour, or put it diﬀerently, can we\nspot traces of critical phenomena in them? Instead of\ndirectly attacking the partition function of real world\ndeep neural nets, we start with the practical observa-\ntion, that systems at around criticality show oﬀpower\nlaw distributions in certain internal attributes.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog activation rank\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog counts\nactivation pattern frequency\nFigure 2: Feed-forward net: Log-log plot of layer acti-\nvation pattern frequencies by rank\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog activation rank\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nlog counts\nactivation pattern frequency\nFigure 3: Autoencoder: Log-log plot of layer activa-\ntion pattern frequencies by rank\nConcretely for networks [23, 24] we look for traces\nof power laws in weight distributions, layer activation\npattern frequencies, single node activation frequencies\nand average layer activations.\nIn the following we\nwill present experimental results for multilayer feed-\nforward networks, convolutional neural nets and au-\ntoencoders.\nFor all networks we ran experiments on the CIFAR-\n10 dataset, training each models for 200 epochs using\nReLU activations and Adam Optimizer without gra-\ndient clipping and run inferences for 100 epochs. The\nfeed forward network had 3 layers with 500, 400 and\n200 nodes, the CNN had 3 convolutional layers fol-\nlowed by 3 fully connected layers and the autoencoder\nhad one layer with 500 nodes.\nFor weight distributions we looked at sums of abso-\nlute values of the outgoing weights at each node, as a\nweighted order of the node. In ﬁg. 1 we have a log-log\nplot of counts versus the node order as deﬁned above,\nand detect no linear behavior.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog activation rank\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog counts\nactivation pattern frequency\nFigure 4: CNN: Log-log plot of layer activation pattern\nfrequencies by rank\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog activation rank\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog counts\nactivation pattern frequency\nFigure 5: CNN: Log-log plot of layer activation pattern\nfrequencies by rank\nFor layer activation patterns we counted the frequency\nof each layer activations through the inference epochs.\nFigures 2 and 3 are log-log plots of layer activation\nfrequencies versus their respective counts for the feed-\nforward layer the autoencoder.\nAs we see, the hid-\nden layer activation pattern frequencies of the Autoen-\ncoder resembles a truncated straight line, indicating\nthat learning hidden features in unsupervised manner\ncan give rise for scale free, power law phenomena in\naccordance with the ﬁndings of [24], but no other ar-\nchitectures show traces of any power law.\nFor single node activation frequencies we counted the\nfrequency of each node activations through the infer-\nence epochs.\nCriticality & Deep Learning I: Generally Weighted Nets\n7\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog activation rank\n4.4\n4.6\n4.8\n5.0\n5.2\n5.4\n5.6\n5.8\n6.0\nlog counts\nsingle node activation frequency\nFigure 6: Feed-forward net: Log-log plot of single node\nactivation frequencies by rank\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nlog activation rank\n0\n1\n2\n3\n4\n5\n6\nlog counts\nsingle node activation frequency\nFigure 7: CNN: Log-log plot of single node activation\nfrequencies by rank\nFigures 6 and 7 depict the behavior of feed-forward\nand CN network. The ﬂat, nearly horizontal line in the\nlatter architecture is again a sign of missing exponent\nwhatsoever.\nAs a last measure we employed the sum of activa-\ntions deﬁned as the average activations on each layer\nthroughout the inference epochs.\nSpontaneous and detectable criticality did not arise in\nclassical architectures so the next step will be to cre-\nate and experiment with systems that have induced\ncriticality and learning rules that take into account\ncriticality. Our ﬁrst approach was to grow a fully con-\nnected net using the preferential attachment algorithm\nto induce at least some power law in node weights, and\nuse the fully connected net as a hidden to hidden mod-\nule. We further experimented with diﬀerent solutions,\nregarding input and read out of activations from this\nhidden to hidden module, without changing the power\nlaw distribution. (This would simulate a system lo-\ncated at a critical state, with power law weight distri-\nbution). Our ﬁndings so far show that learning in these\nsystems is very unstable without any advancement in\nlearning and inference. The fundamental missing part\nis how to naturally induce a critical state in a network,\nwhich is equipped with learning rules that inherently\ntake into account the critical state. For that we need\nnew architectures and new learning rules, derived from\nthe critical point equations (30).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nlog avg activations\n101\n102\n103\n104\n105\nlog count\naverage activations\nFigure 8: Feed-forward net: Average layer activation\ndistribution\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nlog avg activations\n101\n102\n103\n104\n105\nlog count\naverage activations\nFigure 9: CNN: Average layer activation distribution\n5\nSummary and outlook\nSummary: In this article we make our ﬁrst steps in\ninvestigating the relationship between criticality and\ndeep learning networks.\nAfter a short introduction\nof criticality in statistical physics and real world net-\nworks we started with the theoretical setup of a fully\nconnected layer. We used continuous mean ﬁeld ap-\nproximation techniques to tackle the partition func-\ntion of the system ending up with a system of diﬀer-\nential equations that determine the critical behaviour\nof the system.\nThese equations can be the starting\npoint for a possible network architecture with induced\ncriticality and learning rules exploiting criticality. Af-\nter that we presented results of experiments aiming to\nﬁnd traces of power law distributions in current deep\nlearning networks such as multilayer feed-forward nets,\nconvolutional networks and autoencoders. The results\n- except for the autoencoder - were aﬃrmative in the\nnegative sense, setting up as next the necessity to cre-\nate networks with induced criticality and learning rules\nthat exploit the critical state.\nOutlook: Obviously the fully connected layer, which\ncan be solved analytically on the theoretical side is\nof limited importance, as it translates into a rather\nsimplistic architecture; more realistic, widely used set-\nups, e.g. convolutional or recurrent nets, do very well\ncontain the feed-forward mechanism, but are strongly\ndeviating and hence only partially mapped to our theo-\nretical treatment; it would deﬁnitely be essential to ad-\ndress theoretically the convolution mechanism of deep\nCriticality & Deep Learning I: Generally Weighted Nets\n8\nnets and establish a link between the theoretical and\nexperimental side; also inducing criticality into the net\nvia eq. (30) could prove beneﬁcial and might very well\naﬀect learning behavior and ﬂow on the surface on the\nloss function.\nAppendix\nA\nDiﬀerent unit values\nWe here show that the partition function with Hamil-\ntonian\nH{0,1} =\nX\nij\naiwijaj + hiai\n(31)\nwho’s units are taking values in {0, 1} has the same\nqualities as encoded in the partition function with\nHamiltonian H{±1}, who’s units take values in {±1}.\nWe rewrite the Hamiltonian in (31) with units taking\nvalues in {±1} (using Einstein’s summation conven-\ntion over double indices) :\nH = 1\n4(1 −ui)wij(1 −uj) + 1\n2hi(1 −ui)\n(32)\nwhere the ui and vi take values in {±1}. Carrying now\nthe multiplications in (32) yields\nH = 1\n4\nX\nij\nwij + 1\n4uiwijuj\n−2\n4uiwij1j −1\n2hiui + 1\n2hi\n(33)\n= 1\n2(c + uiwijuj + h′\niui)\nwith h′\ni = −wij1j −hi. Hence when computing the\npartition Z with (32) we obtain\nZ =\nX\nu∈{±1}\neH = ec\nX\na∈{0,1}\neaiwijaj+h′\niai\n(34)\nwhere the right hand side is the original Hamiltonian\nwith a shifted coupling h′. The additional constant\nc factors out completely and hence when taking the\nlogarithm and the second derivative it won’t change\nthe outcome. Also we note that the second derivative\nwrt. h′ is ∂h′h′ = ∂hh.\nB\nFirst order contribution\nWe consider here the Hamiltonian of the bi-partite\ngraph connected via weight matrix w (with Einstein\nsummation convention):\nHb = uiwijvj\n(35)\nwith the free energy\nFb = −ln\nX\nu,v∈{±1}\nexp(uiwijvj)\n(36)\nWithout any loss of generality we set the temperature\nT = 1, and we won’t keep track of it. Carrying the\npartial sum over vi yields\nFb = −ln\nX\nu\nY\nj\n\u0002\nexp(uiwij) + exp(−uiwij)\n\u0003\n(37)\n= −ln\nX\nu\nY\nj\n\u0002\n2 cosh(uiwij)\n\u0003\nThe sum over the vi is understood as a collection of\n2N terms, each corresponding to a unique combination\nof 0’s and 1’s in the vector of length N representing\nthat speciﬁc state of the spins; however, the sum can\nbe conveniently written as a product of N binary sum-\nmands, where each contains exactly the two possible\nstates of the ith spin - this is where the product over\nj comes from in upper formula.\nExpanding now to\nlowest order in w we obtain\nFb ∼−ln\nX\nu\nY\nj\n(1 + (uiwij)2\n2\n)\n(38)\n∼−ln\nX\nu\nexp\nX\nj\n(uiwij)2\n2\n= −ln\nX\nu\neH(ui)\nwhere H(ui) is the Hamiltonian of the fully connected\ngraph, deﬁned as (Einstein summation convention)\nH(ui) =\nX\nj\n(uiwij)2\n2\n(39)\n= 1\n2\nX\nik\n(P\nj wijwjk)\n|\n{z\n}\nw′\nik\nuiuk\n= 1\n2\nX\nik\nuiw′\nikuk\nA few notes are in place regarding eq. (39): the matrix\nw′\nik is now symmetric by construction and hence medi-\nates between equally sized (actually identical) layers;\nfurther, all higher terms of the cosh function are even,\nhence all contributions are higher order, symmetric in-\nteractions of the layer ui with itself.\nCriticality & Deep Learning I: Generally Weighted Nets\n9\nReferences\n[1] Per Bak, ”How Nature Works: the science of self-\norganized criticality”, Copernicus Springer-Verlag,\nNew York, 1996\n[2] Lesne Annick, Lagues Michel, ”Scale Invariance,\nFrom Phase Transitions to Turbulence”, Springer,\n2012\n[3] Gunnar\nPruessner,\n”Self-organised\ncriticality”,\nCambridge University Press, 2012\n[4] Thierry Mora, William Bialek, ”Are biological\nsystems poised at criticality?”, arXiv:1012.2242\n[q-bio.QM]\n[5] Thomas M. Cover, Joy A. Thomas, ”Elements of\nInformation Theory”, Wiley Series, 2006\n[6] Kunihiko Fukushima,\n”Neocognitron:\nA Self-\norganizing Neural Network Model for a Mechanism\nof Pattern Recognition Unaﬀected by Shift in Po-\nsition”\nwww.cs.princeton.edu/courses/archive/\nspr08/cos598B/Readings/Fukushima1980.pdf\n[7] Adriano Barra,Giuseppe Genovese,\nPeter Sol-\nlich, Daniele Tantari, ”Phase transitions in Re-\nstricted Boltzmann Machines with generic priors”,\narXiv:1612.03132 [cond-mat.dis-nn]\n[8] Adriano Barra, Alberto Bernacchia, Enrica San-\ntucci, Pierluigi Contucci, ”On the equivalence\nof Hopﬁeld Networks and Boltzmann Machines”,\narXiv:1105.2790 [cond-mat.dis-nn]\n[9] J. J. Hopﬁeld, ”Neural Networks and physical sys-\ntems with emergent collective computational abili-\nties,” Proceedings of the National Academy of Sci-\nence, USA, 79 (1982) 2554-2558.\n[10] Carsten Peterson, James R . Anderson, ”A Mean\nField Theory Learning Algorithm for Neural Net-\nworks”, Complex Systems 1 (1987) 995- 1019\n[11] Anna\nChoromanska,\nMikael\nHenaﬀ,\nMichael\nMathieu,\nGerard\nBen\nArous,\nYann\nLeCun,\n”The\nLoss\nSurfaces\nof\nMultilayer\nNetworks”,\narXiv:1412.0233 [cs.LG]\n[12] Gasper Tkacik, Elad Schneidman, Michael J.\nBerry II, William Bialek, ”Spin glass models for\na network of real neurons”, arXiv:0912.5409\n[q-bio.NC]\n[13] Hanna Kamyshanska, Roland Memisevic, ”The\nPotential Energy of an Autoencoder”\nhttps://www.iro.umontreal.ca/~memisevr/\npubs/AEenergy.pdf\n[14] Kim Christensen, Nicholas R. Moloney, ”Com-\nplexity and Criticality (Advanced Physics Texts)”,\nImperial College Press, 2005\n[15] Philip Ball, ”One rule of life: Are we poised on\nthe border of order?”, New Scientist, April 2014\n[16] Uwe C. Tauber, ”Renormalization Group: Appli-\ncations in Statistical Physics”, Nuclear Physics B,\n(2011) 128\n[17] Stuart A. Kauﬀman, ”The Origins of Order: Self-\nOrganization and Selection in Evolution”, Oxford\nUniversity Press, 1993\n[18] H.E. Stanley, ”Scaling,universality and renormal-\nization:three pillars of modern critical phenom-\nena”,\nhttp://journals.aps.org/rmp/abstract/10.\n1103/RevModPhys.71.S358\n[19] Silvio\nSalinas,\n”Introduction\nto\nstatistical\nphysics”, Springer, 2001\n[20] Martin Kochmanski, Tadeusz Paszkiewicz, Sla-\nwomir Wolski, ”Curie-Weiss magnet:\na simple\nmodel of phase transition”, arXiv:1301.2141 [cond-\nmat.stat-mech]\n[21] Dante R. Chialvo, ”Critical brain dynamics at\nlarge scale”, arXiv:1210.3632 [q-bio.NC]\n[22] Elad Schneidman, Michael J. Berry II, Ronen\nSegev, William Bialek, ”Weak pairwise correla-\ntions imply strongly correlated network states\nin a neural population”, arXiv:q-bio/0512013\n[q-bio.NC]\n[23] Reka\nAlbert,\nAlbert-Laszlo\nBarabasi,\n”Statistical\nmechanics\nof\ncomplex\nnet-\nworks”,\narXiv:cond-mat/0106096\n[cond-mat.stat-mech]\n[24] David\nJ.\nSchwab,\nIlya\nNemenman,\nPankaj\nMehta, ”Zipfs law and criticality in multivariate\ndata without ﬁne-tuning”, arXiv:1310.0448v3\n[q-bio.NC]\n[25] Dante R. Chialvo, ”Critical brain dynamics at\nlarge scale”, arXiv:1210.3632 [q-bio.NC]\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2017-02-26",
  "updated": "2017-05-31"
}