{
  "id": "http://arxiv.org/abs/1702.08798v1",
  "title": "Unsupervised Triplet Hashing for Fast Image Retrieval",
  "authors": [
    "Shanshan Huang",
    "Yichao Xiong",
    "Ya Zhang",
    "Jia Wang"
  ],
  "abstract": "Hashing has played a pivotal role in large-scale image retrieval. With the\ndevelopment of Convolutional Neural Network (CNN), hashing learning has shown\ngreat promise. But existing methods are mostly tuned for classification, which\nare not optimized for retrieval tasks, especially for instance-level retrieval.\nIn this study, we propose a novel hashing method for large-scale image\nretrieval. Considering the difficulty in obtaining labeled datasets for image\nretrieval task in large scale, we propose a novel CNN-based unsupervised\nhashing method, namely Unsupervised Triplet Hashing (UTH). The unsupervised\nhashing network is designed under the following three principles: 1) more\ndiscriminative representations for image retrieval; 2) minimum quantization\nloss between the original real-valued feature descriptors and the learned hash\ncodes; 3) maximum information entropy for the learned hash codes. Extensive\nexperiments on CIFAR-10, MNIST and In-shop datasets have shown that UTH\noutperforms several state-of-the-art unsupervised hashing methods in terms of\nretrieval accuracy.",
  "text": "arXiv:1702.08798v1  [cs.CV]  28 Feb 2017\nUNSUPERVISED TRIPLET HASHING FOR FAST IMAGE RETRIEVAL\nShanshan Huang, Yichao Xiong, Ya Zhang and Jia Wang\nShanghai Jiao Tong University, Shanghai, China\nABSTRACT\nHashing has played a pivotal role in large-scale image re-\ntrieval. With the development of Convolutional Neural Net-\nwork (CNN), hashing learning has shown great promise. But\nexisting methods are mostly tuned for classiﬁcation, which\nare not optimized for retrieval tasks, especially for instance-\nlevel retrieval.\nIn this study, we propose a novel hash-\ning method for large-scale image retrieval. Considering the\ndifﬁculty in obtaining labeled datasets for image retrieval\ntask in large scale, we propose a novel CNN-based unsuper-\nvised hashing method, namely Unsupervised Triplet Hashing\n(UTH). The unsupervised hashing network is designed under\nthe following three principles: 1) more discriminative repre-\nsentations for image retrieval; 2) minimum quantization loss\nbetween the original real-valued feature descriptors and the\nlearned hash codes; 3) maximum information entropy for the\nlearned hash codes. Extensive experiments on CIFAR-10,\nMNIST and In-shop datasets have shown that UTH outper-\nforms several state-of-the-art unsupervised hashing methods\nin terms of retrieval accuracy.\nIndex Terms— CNN, unsupervised hashing, triplet loss,\nfast image retrieval.\n1. INTRODUCTION\nWith the explosive growth of multimedia contents, how to\nspeed up image retrieval draws much attention in computer\nvision. Hashing, which uses mapping functions to transform\na high-dimensional feature vector into a compact and expres-\nsive binary codes [1, 2, 3], has shown signiﬁcant success for\nfast image retrieval. In recent years, with the rapid develop-\nment of Convolutional Neural Network (CNN), several CNN-\nbased hashing methods [4, 5, 6, 7, 8, 9, 10] have been pro-\nposed and demonstrated promising results. In particular, un-\nsupervised hashing learning has recently received increasing\nattention because it does not require labeled training data thus\nmaking the methods widely applicable. The earliest studies\nuse stacked Restricted Boltzmann Machines (RBMs) to en-\ncode binary codes [8, 9] for unsupervised hashing. However,\nRBMs are complex and require pre-training, which are not\nefﬁcient for practical applications. More recently, data aug-\nmentation is leveraged to reinforce the representation ability\nof the model [10], which achieves the state-of-the-art results\nrotation\nanchor\nrotation\nrandom\ntriplet_loss\nquantization_loss\nentropy_loss\nhashing layer\n16 Layer VGGNet\nFig. 1: The proposed UTH method. We add a hashing layer\nto generate a compact feature vector. The input to our ar-\nchitecture is an image triplet consisting of an anchor image, a\nrotated image and a random image. These image triplets share\nparameters in our architecture. At the top of our architecture,\nwe use three criterions to learn efﬁcient codes: 1) more dis-\ncriminative representations for image retrieval; 2) minimum\nquantization loss between the original real-valued feature de-\nscriptors and the learned hash codes; 3) maximum informa-\ntion entropy for the learned hash codes.\nso far. They augment the training data with different rota-\ntions of the reference images, and attempt to minimize the\ndistances between the binary codes of the reference images\nand that of their rotations. However, optimizing with rotation\ninvariance between images and their rotations only provides\npositive data for the learning, and cannot guarantee the model\nto generate discriminative binary codes for different images.\nIn this paper, we propose a novel unsupervised hashing\nmethod, namely Unsupervised Triplet Hashing (UTH), based\non CNN with triplet loss to ensure the discriminability of the\nhash codes. The novelties of this paper are 1) we replace\nthe rotation invariance loss in DeepBit [10], one of the state-\nof-the arts deep hashing method, with a triplet loss; 2) We\nconstruct triplets from unlabeled data in optimizing the triplet\nloss. The triplet loss enforces balanced training samples while\nthe rotation invariance loss fails to consider negative training\nsamples. The three key components of UTH are illustrated in\nFig. 1. First of all, we design a triplet loss to learn more dis-\ncriminative representations for fast image retrieval, which en-\nforces a margin between the distances of rotated images and\nrandom images to the original images. This allows the ro-\ntated images for each image to live on a manifold, while still\nenforcing the distances and thus discriminability to other im-\nages. Then we add two constraints to guarantee the retrieval\nperformance of the learned hash codes, a minimum quantiza-\ntion loss between the original real-valued feature descriptors\nand the learned hash codes to maintain the high retrieval ac-\ncuracy and a maximum information entropy loss to reinforce\nthe representation ability of learned hash codes. We evalu-\nate the proposed UTH architecture on three benchmarks, and\nshow that it outperforms several state-of-the-art unsupervised\nhashing methods in terms of retrieval accuracy.\n2. RELATED WORKS\nHashing Method. According to whether the semantic infor-\nmation is used, learning-based hashing method can be divided\ninto three categories: supervised hashing [4, 5, 6, 11, 12],\nsemi-supervised hashing [7] and unsupervised hashing [1, 2,\n3, 8, 9, 10].\nFor supervised hashing, Lin et al. [4] employ a hidden\nlayer to learn binary hash codes by using a “Softmax” layer\non the top of the model. The learned features which are op-\ntimized for image classiﬁcation are directly applied to image\nretrieval. Two-step learning method is adopted to learn bi-\nnary hash codes. For example, Xia et al. [5] adopt a learn-\ning method which learns binary codes for all the training data\nin the ﬁrst step and learns hash functions on the basis of the\nlearned codes in the second step. Lin et al. [6] introduce a\nhashing scheme based on stacked RBMs and Siamese net-\nwork, in which stacked RBMs are aimed to learn the initial\nparameters of the network and then the parameters are ﬁne-\ntuned through a Siamese network. Nguyen et al. [11] use a\ntriplet loss function to minimize the Hamming distance be-\ntween the neighbor pairs while preserving the relative simi-\nlarity of non-neighbor pairs with a relaxed empirical penalty.\nLai et al. [12] present a divide-and-encode module to divide\nthe intermediate image features into multiple branches, each\nencoded into one hash bit, then use a triplet loss to ﬁne-tune\nthe network.\nFor semi-supervised hashing, Wang et al. [7] present\na semi-supervised hashing (SSH) framework to learn hash\ncodes by minimizing empirical error on the labeled data and\nmaximizing variance and independence of hash codes over\nthe labeled and unlabeled data.\nFor unsupervised hashing, most of the previous unsu-\npervised methods [1, 2, 3] make use of hand-crafted image\nfeatures and are not end-to-end. Har-Peled et al. [1] pro-\npose Local Sensitive Hashing (LSH), which uses random pro-\njections to construct hash functions, making samples within\nshort Hamming distance in hash space be near in their source\nspace. Gong et al. [2] propose the popular Iterative Quan-\ntization (ITQ), which ﬁrst performs PCA and then learns a\nrotation to minimize the quantization error of mapping the\ntransformed data to the vertices of a zero-centered binary hy-\npercube. As the deep learning develops, many unsupervised\nhashing methods [8, 9, 10] based on deep learning are pro-\nposed. Salakhutdinov, et al. [8] propose semantic hashing\n(SH), which uses RBMs as an auto-encoder network to gen-\nerate efﬁcient binary codes. Lin et al. [10] propose DeepBit to\nlearn a set of nonlinear mapping functions by inserting a latent\nlayer into the previous model and construct pair-wise training\ndata by combining the original images with its rotated images,\nwhich outperforms state-of-the-art unsupervised schemes.\nDeep Learning. Recently, deep learning has achieved ex-\nplosive success in pattern recognition including image classi-\nﬁcation, segmentation and learning-based hashing for fast im-\nage retrieval. Guo et al. [13] propose a straightforward CNN-\nbased hashing method, they quantize the activations of a fully\nconnected layer with threshold 0 and take the binary result as\nhash codes. Liong et al. [14] present a framework to learn bi-\nnary codes by seeking multiple hierarchical non-linear trans-\nformations, so that the nonlinear relationship of samples can\nbe well exploited. Xia et al. [5] present a framework to auto-\nmatically learn a good image representation tailored to hash-\ning as well as a set of hash functions. Yao et al. [15] propose\na co-training hashing network by jointly learning projections\nfrom image representations to hash codes and classiﬁcation.\n3. THE PROPOSED APPROACH\nGenerally, the proposed UTH architecture contains three ma-\njor components: 1) learning more discriminative represen-\ntations for image retrieval via a triplet loss; 2) minimizing\nquantization loss between the original real-valued feature de-\nscriptors and the learned hash codes to maintain the high re-\ntrieval performances; 3) maximizing information entropy for\nthe learned hash codes to carry as much information as possi-\nble. The whole architecture is shown in Fig. 1. Let LT denote\nthe triplet loss function, LQ denote the quantization loss func-\ntion and LE denote the entropy loss function. We deﬁne an\noverall loss function:\nL = αLT + βLQ + γLE,\n(1)\nwhere α, β and γ are the parameters for each object. These\nloss functions will be explained in the following chapters.\n3.1. Unsupervised Triplet Loss\nTo ensure the discriminability of the hash codes, we propose\nan unsupervised triplet neural network. A triplet training set\nis constructed by from the unlabeled data in the following\nmeans. For each image in the unlabeled set, a rotation of the\nimage, a randomly selected image from the dataset, and itself\nform a triplet. It is safe to assume that the distance between\nthe rotation of the image to the image is smaller than that of\nthe randomly selected image to the image.\nLet (p, p+, p−) denote a triplet example. F() denotes the\nhashing function we have learned. Speciﬁcally, F(p) is the\nfeature of the anchor image, F(p+) and F(p−) are the fea-\ntures of the rotated image and the random image respectively.\nThe triplet loss function is written as\nLT\n= max{0, m + DE(F(p), F(p+)) −DE(F(p), F(p−))}\n= max{0, m + ∥F(p) −F(p+)∥2\n2 −∥F(p) −F(p−)∥2\n2},\n(2)\nwhere DE denotes the Euclidean distance between two ob-\njects and we use L2-norm to calculate the distance, and m\ndenotes the margin we select in our method.\nIn preparing training dataset, we rotate each image p in the\ntraining set by some ﬁxed degree to form a p+, and randomly\nselect an image except itself to form a p−, which constructs a\ntriplet (p, p+, p−).\n3.2. Quantization Loss\nIn order to learn multiple nonlinear hashing functions, we add\nan activation layer followed by the hashing layer. In our study,\nReLU is chosen as the activation function because it prevents\nfrom gradient disappearance in our training process. A binary\nhash code is generated by quantizing the output feature. The\nquantization rule is shown as\nb =\n(\n1,\nif F(p) > threshold,\n0,\notherwise.\n(3)\nIn our experiments, we set the threshold as 0.5 and add\na constraint to narrow the gap between the retrieval perfor-\nmances before and after quantizing the image features. The\nminimum quantization loss, i.e., LQ, is deﬁned as\nLQ =\nN\nX\nn=1\nM\nX\nm=1\n∥F(p) −b∥2\n=\n\n\n\n\n\n\n\nN\nP\nn=1\nM\nP\nm=1\n∥F(p) −1∥2,\nif F(p) > 0.5,\nN\nP\nn=1\nM\nP\nm=1\n∥F(p)∥2,\notherwise,\n(4)\nwhere N is the number of training data, M is the length of\nhash codes.\nThe loss function (4) pushes the real value of each dimen-\nsion to either 0 or 1, thus the retrieval performance by using\nquantized image features, i.e. hash codes, is approximate to\nthe performance by using real-value image features.\n3.3. Entropy Loss\nAccording to information theory, the highest entropy is\nreached when information distributes evenly among each bit\nin the code. Thus a higher entropy means that the code car-\nries more information. Inspired by this theory and DeepBit,\nwe add a constraint to impel each bit in our output binary\ncodes to be evenly distributed. Hence, the maximum entropy\nloss is formulated as\nLE =\nM\nX\nm=1\n(µm −0.5)2, µm = 1\nN\nN\nX\nn=1\nbn(m).\n(5)\nSubstituting equations (2), (4), and (5) into equation (1),\nwe can obtain the overall loss function.\n4. EXPERIMENTS\n4.1. Experimental Setting\nTo test the generalizability of different hashing methods,\nwe choose three public datasets of different characteristics\nto evaluation the methods under comparison, i.e., CIFAR-\n10 [16], MNIST1 and In-shop [17]. The basic statistic of the\ndatasets is shown in Table 1. The CIFAR-10 dataset is chosen\nfor a direct comparison with DeepBit. MNIST, a dataset quite\ndifferent from ImageNet which is used for the pre-trained\nmodel, is selected to test the generalizability of different hash-\ning methods. Finally, the In-shop dataset is chosen to test dif-\nferent hashing methods for instance-level retrieval.\nWe compare the proposed UTH method with the\nstate-of-the-art unsupervised hashing methods: KMH [18],\nSphH [19], SpeH [3], PCAH [7], LSH [1], PCA-ITQ [2],\nDH [14], DeepBit [10]. Similar to previous studies, we eval-\nuate the performance of the hashing methods with the fol-\nlowing two widely-adopted metrics: mean average precision\n(mAP) at top 1,000 and Recall-Precision curve.\nWe implement UTH using the open source Caffe [20]\nand update parameters by Stochastic Gradient Descent. Be-\nsides, we use VGGNet [21] in UTH for pre-training and add\na latent layer (named hashing layer) following by a ReLU\nactivation layer.\nWe adopt the idea of using weight shar-\ning network for model ﬁne-tuning to learn a more general-\nized network. When generating p+, we rotate each image by\n−10, −5, 5, 10 degrees respectively. In training process, we\nset α = β = γ = 1, and follow the same setting as DeepBit.\nWe update the parameters of the network by minimizing the\nquantization error and entropy error using the original train-\ning data at ﬁrst, then we ﬁne-tune the network by adding a\ntriplet loss using the triplet examples we have constructed.\nWe set the output hashing layer as 16 bits, 32 bits and 64\nbits respectively for CIFAR-10 and MNIST datasets. Con-\nsidering the complexity of In-shop dataset, we set the output\nhashing layer as 64 bits, 128 bits and 256 bits. In compari-\nson with the other unsupervised hashing methods, the results\nof the compared methods are from [10, 14] for CIFAR-10 and\nMNIST datasets, and the results of the traditional methods are\nobtained by representing each image as a 512-D GIST feature\nfor In-shop dataset.\n1http://yann.lecun.com/exdb/mnist/.\nTable 1: The basic statistic information of the selected datasets in our experiments.\nDataset\nImage Number\nCategory\nTraining Number\nTesting Number\nCIFAR-10 [16]\n60,000\n10\n50,000\n10,000\nMNIST 1\n70,000\n10\n60,000\n10,000\nIn-shop [17]\n52,712\n7,982\n38,494\n14,218\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\nPrecision\nKMH\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDH\nDeepBit\nUTH\n(a) 16 bits\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nPrecision\nKMH\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDH\nDeepBit\nUTH\n(b) 32 bits\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nPrecision\nKMH\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDH\nDeepBit\nUTH\n(c) 64 bits\nFig. 2: Recall-Precision curves on the CIFAR-10 dataset for different unsupervised hashing methods with respect to 16, 32 and\n64 bits, respectively.\nTable 2: Mean Average Precision (mAP, %) at top 1,000 of\ndifferent unsupervised hashing methods on CIFAR-10 dataset\nwith respect to different hash codes.\nMethod\n16-bit\n32-bit\n64-bit\nKMH [18]\n13.59\n13.93\n14.46\nSphH [19]\n13.98\n14.58\n15.38\nSpeH [3]\n12.55\n12.42\n12.56\nPCAH [7]\n12.91\n12.60\n12.10\nLSH [1]\n12.55\n13.76\n15.07\nPCA-ITQ [2]\n15.67\n16.20\n16.64\nDH [14]\n16.17\n16.62\n16.69\nDeepBit [10]\n19.43\n24.86\n27.73\nUTH\n28.66\n30.66\n32.41\n4.2. Experiments Results\nFollowing DeepBit, we randomly sample 1000 images as the\nquery data, and use the remaining images as the gallery set\nfor each dataset. Table 2, Table 3 and Table 4 show the mAP\nresults at top 1,000 of different unsupervised hashing meth-\nods. The Recall-Precision curves are presented Fig. 2, Fig. 3\nand Fig. 4.\nFor all the three datasets, the proposed UTH\noutperforms the other unsupervised hashing methods under\ncomparison. The consistently better performance of UTH has\ndemonstrated that the proposed UTH learns more discrimina-\ntive hash codes for fast image retrieval.\nTable 3: Mean Average Precision (mAP, %) at top 1,000 of\ndifferent unsupervised hashing methods on MNIST dataset\nwith respect to different hash codes.\nMethod\n16-bit\n32-bit\n64-bit\nKMH [18]\n32.12\n33.29\n35.78\nSphH [19]\n25.81\n30.77\n34.75\nSpeH [3]\n26.64\n25.72\n24.10\nPCAH [7]\n27.33\n24.85\n21.47\nLSH [1]\n20.88\n25.83\n31.71\nPCA-ITQ [2]\n41.18\n43.82\n45.37\nDH [14]\n43.14\n44.97\n46.74\nDeepBit [10]\n28.18\n32.02\n44.53\nUTH\n43.15\n46.58\n49.88\nFrom the results of mAP, UTH improves the retrieval ac-\ncuracy with respect to 16-bit, 32- bit and 64-bit hash codes\nby 9.23%, 5.80%, 4.68% on CIFAR-10 dataset, 14.97%,\n14.56%, 5.33% on MNIST dataset and 3.87%, 2.40%, 0.61%\non In-shop respectively by comparing with DeepBit [10].\nThat means the proposed UTH achieves even better perfor-\nmance on more compact hash codes compared with DeepBit.\nThe signiﬁcant improvement of UTH over DeepBit lies\npartly in that DeepBit optimizes the rotation invariance be-\ntween images and their rotations, which is to provide rota-\ntion invariant descriptors for images. As a result, it may not\nguarantee the model to generate discriminative binary codes\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nKMH\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDH\nDeepBit\nUTH\n(a) 16 bits\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nKMH\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDH\nDeepBit\nUTH\n(b) 32 bits\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nKMH\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDH\nDeepBit\nUTH\n(c) 64 bits\nFig. 3: Recall-Precision curves on the MNIST dataset for different unsupervised hashing methods with respect to 16, 32 and 64\nbits, respectively.\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0\n0.002\n0.004\n0.006\n0.008\n0.01\n0.012\n0.014\n0.016\n0.018\n0.02\nPrecision\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDeepBit\nUTH\n(a) 64 bits\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0\n0.002\n0.004\n0.006\n0.008\n0.01\n0.012\n0.014\n0.016\n0.018\n0.02\nPrecision\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDeepBit\nUTH\n(b) 128 bits\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\n0\n0.002\n0.004\n0.006\n0.008\n0.01\n0.012\n0.014\n0.016\n0.018\n0.02\nPrecision\nSphH\nSpeH\nPCAH\nLSH\nITQ\nDeepBit\nUTH\n(c) 256 bits\nFig. 4: Recall-Precision curves on the In-shop dataset for different unsupervised hashing methods with respect to 64, 128 and\n256 bits, respectively.\nfor different images. With UTH, we attempt to maximize the\ndiscriminability of the hash codes while still keeping the ro-\ntation invariant features using a triplet network. UTH learns\nweights of all layers in order to consider the co-adaption be-\ntween neighboring layers in CNNs which has been proved\nimportant in [22], while DeepBit freezes the parameters of\nlayers lower than the hashing layer when training, which is\nanother reason why UTH is superior to DeepBit.\nThe improvement of UTH over DH [24] lies in that our\nmethod utilizes the 16 layers VGGNet as the initialized net-\nwork and ﬁne tune the network by three loss components. DH\ntakes only three layer hierarchical neural networks to learn\nhash codes. The improvement over the other hashing meth-\nods [18, 19, 3, 7, 1, 2] lies in that our proposed architecture is\nbased on a deep CNN, which is an end-to-end network, which\nhas been proved to have advantage over hashing methods us-\ning hand crafted image features.\n5. CONCLUSION\nIn this paper, we present a novel unsupervised hashing\nmethod based on convolutional neural network (CNN) called\nTable 4: Mean Average Precision (mAP, %) at top 20 of dif-\nferent unsupervised hashing methods on In-shop dataset with\nrespect to different hash codes.\nMethod\n64-bit\n128-bit\n256-bit\nSphH [19]\n9.03\n15.52\n17.94\nSpeH [3]\n8.77\n12.38\n17.14\nPCAH [7]\n6.60\n10.32\n14.73\nLSH [1]\n8.34\n13.51\n15.39\nPCA-ITQ [2]\n9.77\n16.74\n21.29\nDeepBit [10]\n6.53\n14.70\n22.65\nUTH\n10.40\n17.10\n23.26\nunsupervised triplet hashing (UTH). The UTH is designed\nwith a triplet network structure to simultaneously achieve the\nfollowing three objectives: 1) discriminative representations\nfor fast image retrieval; 2) accurate binary feature descriptors;\n3) maximizing the information of the learned hash codes. Ex-\ntensive experiment evaluations based on CIFAR-10, MNIST\nand In-shop datasets have showed the promise of the pro-\nposed UTH method for fast image retrieval.\n6. REFERENCES\n[1] Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani,\n“Approximate nearest neighbor: Towards removing the\ncurse of dimensionality.,” Theory of computing, vol. 8,\nno. 1, pp. 321–350, 2012.\n[2] Yunchao Gong and Svetlana Lazebnik, “Iterative quan-\ntization:\nA procrustean approach to learning binary\ncodes,”\nin Computer Vision and Pattern Recognition\n(CVPR), 2011 IEEE Conference on. IEEE, 2011, pp.\n817–824.\n[3] Yair Weiss, Antonio Torralba, and Rob Fergus, “Spec-\ntral hashing,” in Advances in neural information pro-\ncessing systems, 2009, pp. 1753–1760.\n[4] Kevin Lin, Huei-Fang Yang, Jen-Hao Hsiao, and Chu-\nSong Chen, “Deep learning of binary hash codes for fast\nimage retrieval,” in Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition Work-\nshops, 2015, pp. 27–35.\n[5] Rongkai Xia, Yan Pan, Hanjiang Lai, Cong Liu, and\nShuicheng Yan, “Supervised hashing for image retrieval\nvia image representation learning.,”\nin AAAI, 2014,\nvol. 1, p. 2.\n[6] Jie Lin, Olivier Morere, Vijay Chandrasekhar, Antoine\nVeillard, and Hanlin Goh,\n“Deephash: Getting regu-\nlarization, depth and ﬁne-tuning right,” arXiv preprint\narXiv:1501.04711, 2015.\n[7] Jun Wang, Sanjiv Kumar, and Shih-Fu Chang, “Semi-\nsupervised hashing for large-scale search,” IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,\nvol. 34, no. 12, pp. 2393–2406, 2012.\n[8] Ruslan Salakhutdinov and Geoffrey Hinton, “Semantic\nhashing,” International Journal of Approximate Reason-\ning, vol. 50, no. 7, pp. 969–978, 2009.\n[9] Jie Lin, Olivier Morere, Julie Petta, Vijay Chan-\ndrasekhar, and Antoine Veillard,\n“Tiny descriptors\nfor image retrieval with unsupervised triplet hashing,”\narXiv preprint arXiv:1511.03055, 2015.\n[10] Kevin Lin, Jiwen Lu, Chu-Song Chen, and Jie Zhou,\n“Learning compact binary descriptors with unsuper-\nvised deep neural networks,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, 2016, pp. 1183–1192.\n[11] Viet-Anh Nguyen and Minh N Do,\n“Deep learning\nbased supervised hashing for efﬁcient image retrieval,”\nin Multimedia and Expo (ICME), 2016 IEEE Interna-\ntional Conference on. IEEE, 2016, pp. 1–6.\n[12] Hanjiang Lai, Yan Pan, Ye Liu, and Shuicheng Yan, “Si-\nmultaneous feature learning and hash coding with deep\nneural networks,”\nin Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition,\n2015, pp. 3270–3278.\n[13] Jinma Guo and Jianmin Li, “Cnn based hashing for im-\nage retrieval,” arXiv preprint arXiv:1509.01354, 2015.\n[14] Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre\nMoulin, and Jie Zhou, “Deep hashing for compact bi-\nnary codes learning,” in Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition,\n2015, pp. 2475–2483.\n[15] Ting Yao, Fuchen Long, Tao Mei, and Yong Rui, “Deep\nsemantic-preserving and ranking-based hashing for im-\nage retrieval,” CVPR, 2015.\n[16] Alex Krizhevsky and Geoffrey Hinton, “Learning mul-\ntiple layers of features from tiny images,” 2009.\n[17] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and\nXiaoou Tang, “Deepfashion: Powering robust clothes\nrecognition and retrieval with rich annotations,” in Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 1096–1104.\n[18] Kaiming He, Fang Wen, and Jian Sun,\n“K-means\nhashing: An afﬁnity-preserving quantization method for\nlearning binary compact codes,” in Proceedings of the\nIEEE conference on computer vision and pattern recog-\nnition, 2013, pp. 2938–2945.\n[19] Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu\nChang, and Sung-Eui Yoon, “Spherical hashing,” in\nComputer Vision and Pattern Recognition (CVPR), 2012\nIEEE Conference on. IEEE, 2012, pp. 2957–2964.\n[20] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey\nKarayev, Jonathan Long, Ross Girshick, Sergio Guadar-\nrama, and Trevor Darrell, “Caffe: Convolutional archi-\ntecture for fast feature embedding,” in Proceedings of\nthe 22nd ACM international conference on Multimedia.\nACM, 2014, pp. 675–678.\n[21] Karen Simonyan and Andrew Zisserman, “Very deep\nconvolutional networks for large-scale image recogni-\ntion,” arXiv preprint arXiv:1409.1556, 2014.\n[22] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson, “How transferable are features in deep neural\nnetworks?,” in Advances in neural information process-\ning systems, 2014, pp. 3320–3328.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-02-28",
  "updated": "2017-02-28"
}