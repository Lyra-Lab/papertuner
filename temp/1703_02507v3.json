{
  "id": "http://arxiv.org/abs/1703.02507v3",
  "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features",
  "authors": [
    "Matteo Pagliardini",
    "Prakhar Gupta",
    "Martin Jaggi"
  ],
  "abstract": "The recent tremendous success of unsupervised word embeddings in a multitude\nof applications raises the obvious question if similar methods could be derived\nto improve embeddings (i.e. semantic representations) of word sequences as\nwell. We present a simple but efficient unsupervised objective to train\ndistributed representations of sentences. Our method outperforms the\nstate-of-the-art unsupervised models on most benchmark tasks, highlighting the\nrobustness of the produced general-purpose sentence embeddings.",
  "text": "Unsupervised Learning of Sentence Embeddings\nusing Compositional n-Gram Features\nMatteo Pagliardini*\nIprova SA, Switzerland\nmpagliardini@iprova.com\nPrakhar Gupta*\nEPFL, Switzerland\nprakhar.gupta@epfl.ch\nMartin Jaggi\nEPFL, Switzerland\nmartin.jaggi@epfl.ch\nAbstract\nThe recent tremendous success of unsuper-\nvised word embeddings in a multitude of ap-\nplications raises the obvious question if simi-\nlar methods could be derived to improve em-\nbeddings (i.e.\nsemantic representations) of\nword sequences as well. We present a sim-\nple but efﬁcient unsupervised objective to train\ndistributed representations of sentences. Our\nmethod outperforms the state-of-the-art unsu-\npervised models on most benchmark tasks,\nhighlighting the robustness of the produced\ngeneral-purpose sentence embeddings.\n1\nIntroduction\nImproving unsupervised learning is of key impor-\ntance for advancing machine learning methods, as\nto unlock access to almost unlimited amounts of\ndata to be used as training resources.\nThe ma-\njority of recent success stories of deep learning\ndoes not fall into this category but instead relied\non supervised training (in particular in the vision\ndomain). A very notable exception comes from\nthe text and natural language processing domain,\nin the form of semantic word embeddings trained\nunsupervised (Mikolov et al., 2013b,a; Penning-\nton et al., 2014). Within only a few years from\ntheir invention, such word representations – which\nare based on a simple matrix factorization model\nas we formalize below – are now routinely trained\non very large amounts of raw text data, and have\nbecome ubiquitous building blocks of a majority\nof current state-of-the-art NLP applications.\nWhile very useful semantic representations are\navailable for words, it remains challenging to pro-\nduce and learn such semantic embeddings for\nlonger pieces of text, such as sentences, para-\ngraphs or entire documents. Even more so, it re-\n* indicates equal contribution\nmains a key goal to learn such general-purpose\nrepresentations in an unsupervised way.\nCurrently, two contrary research trends have\nemerged in text representation learning: On one\nhand, a strong trend in deep-learning for NLP\nleads towards increasingly powerful and com-\nplex models, such as recurrent neural networks\n(RNNs), LSTMs, attention models and even Neu-\nral Turing Machine architectures.\nWhile ex-\ntremely strong in expressiveness, the increased\nmodel complexity makes such models much\nslower to train on larger datasets. On the other end\nof the spectrum, simpler “shallow” models such\nas matrix factorizations (or bilinear models) can\nbeneﬁt from training on much larger sets of data,\nwhich can be a key advantage, especially in the\nunsupervised setting.\nSurprisingly, for constructing sentence embed-\ndings, naively using averaged word vectors was\nshown to outperform LSTMs (see Wieting et al.\n(2016b) for plain averaging, and Arora et al.\n(2017) for weighted averaging).\nThis example\nshows potential in exploiting the trade-off be-\ntween model complexity and ability to process\nhuge amounts of text using scalable algorithms,\ntowards the simpler side. In view of this trade-\noff, our work here further advances unsupervised\nlearning of sentence embeddings. Our proposed\nmodel can be seen as an extension of the C-BOW\n(Mikolov et al., 2013b,a) training objective to train\nsentence instead of word embeddings. We demon-\nstrate that the empirical performance of our re-\nsulting general-purpose sentence embeddings very\nsigniﬁcantly exceeds the state of the art, while\nkeeping the model simplicity as well as training\nand inference complexity exactly as low as in aver-\naging methods (Wieting et al., 2016b; Arora et al.,\n2017), thereby also putting the work by (Arora\net al., 2017) in perspective.\narXiv:1703.02507v3  [cs.CL]  28 Dec 2018\nContributions. The main contributions in this\nwork can be summarized as follows:\n• Model.\nWe propose Sent2Vec1, a sim-\nple unsupervised model allowing to com-\npose sentence embeddings using word vec-\ntors along with n-gram embeddings, simulta-\nneously training composition and the embed-\nding vectors themselves.\n• Efﬁciency & Scalability. The computational\ncomplexity of our embeddings is only O(1)\nvector operations per word processed, both\nduring training and inference of the sentence\nembeddings. This strongly contrasts all neu-\nral network based approaches, and allows our\nmodel to learn from extremely large datasets,\nin a streaming fashion, which is a crucial ad-\nvantage in the unsupervised setting. Fast in-\nference is a key beneﬁt in downstream tasks\nand industry applications.\n• Performance.\nOur method shows signiﬁ-\ncant performance improvements compared to\nthe current state-of-the-art unsupervised and\neven semi-supervised models. The resulting\ngeneral-purpose embeddings show strong ro-\nbustness when transferred to a wide range of\nprediction benchmarks.\n2\nModel\nOur model is inspired by simple matrix factor\nmodels (bilinear models) such as recently very\nsuccessfully used in unsupervised learning of\nword embeddings (Mikolov et al., 2013b,a; Pen-\nnington et al., 2014; Bojanowski et al., 2017)\nas well as supervised of sentence classiﬁcation\n(Joulin et al., 2017). More precisely, these models\ncan all be formalized as an optimization problem\nof the form\nmin\nU,V\nX\nS∈C\nfS(UV ιS)\n(1)\nfor two parameter matrices U ∈Rk×h and V ∈\nRh×|V|, where V denotes the vocabulary. Here,\nthe columns of the matrix V represent the learnt\nsource word vectors whereas those of U represent\nthe target word vectors. For a given sentence S,\n1 All our code and pre-trained models are publicly avail-\nable for download at http://github.com/epfml/\nsent2vec\nwhich can be of arbitrary length, the indicator vec-\ntor ιS ∈{0, 1}|V| is a binary vector encoding S\n(bag of words encoding).\nFixed-length context windows S running over\nthe corpus are used in word embedding methods\nas in C-BOW (Mikolov et al., 2013b,a) and GloVe\n(Pennington et al., 2014). Here we have k = |V|\nand each cost function fS : Rk →R only de-\npends on a single row of its input, describing the\nobserved target word for the given ﬁxed-length\ncontext S. In contrast, for sentence embeddings\nwhich are the focus of our paper here, S will\nbe entire sentences or documents (therefore vari-\nable length). This property is shared with the su-\npervised FastText classiﬁer (Joulin et al., 2017),\nwhich however uses soft-max with k ≪|V| being\nthe number of class labels.\n2.1\nProposed Unsupervised Model\nWe propose a new unsupervised model, Sent2Vec,\nfor learning universal sentence embeddings. Con-\nceptually, the model can be interpreted as a natu-\nral extension of the word-contexts from C-BOW\n(Mikolov et al., 2013b,a) to a larger sentence con-\ntext, with the sentence words being speciﬁcally\noptimized towards additive combination over the\nsentence, by means of the unsupervised objective\nfunction.\nFormally, we learn a source (or context) embed-\nding vw and target embedding uw for each word w\nin the vocabulary, with embedding dimension h\nand k = |V| as in (1). The sentence embedding\nis deﬁned as the average of the source word em-\nbeddings of its constituent words, as in (2). We\naugment this model furthermore by also learning\nsource embeddings for not only unigrams but also\nn-grams present in each sentence, and averaging\nthe n-gram embeddings along with the words, i.e.,\nthe sentence embedding vS for S is modeled as\nvS :=\n1\n|R(S)|V ιR(S) =\n1\n|R(S)|\nX\nw∈R(S)\nvw\n(2)\nwhere R(S) is the list of n-grams (including un-\nigrams) present in sentence S. In order to pre-\ndict a missing word from the context, our objective\nmodels the softmax output approximated by neg-\native sampling following (Mikolov et al., 2013b).\nFor the large number of output classes |V| to be\npredicted, negative sampling is known to signiﬁ-\ncantly improve training efﬁciency, see also (Gold-\nberg and Levy, 2014). Given the binary logistic\nloss function ℓ: x 7→log (1 + e−x) coupled with\nnegative sampling, our unsupervised training ob-\njective is formulated as follows:\nmin\nU,V\nX\nS∈C\nX\nwt∈S\n\u0012\nℓ\n\u0000u⊤\nwtvS\\{wt}\n\u0001\n+\nX\nw′∈Nwt\nℓ\n\u0000−u⊤\nw′vS\\{wt}\n\u0001\u0013\nwhere S corresponds to the current sentence and\nNwt is the set of words sampled negatively for\nthe word wt ∈S. The negatives are sampled2\nfollowing a multinomial distribution where each\nword w is associated with a probability qn(w) :=\n√fw\n\u000e \u0000 P\nwi∈V\np\nfwi\n\u0001\n, where fw is the normal-\nized frequency of w in the corpus.\nTo select the possible target unigrams (posi-\ntives), we use subsampling as in (Joulin et al.,\n2017; Bojanowski et al., 2017), each word w be-\ning discarded with probability 1 −qp(w) where\nqp(w) := min\n\b\n1,\np\nt/fw + t/fw\n\t\n. Where t is\nthe subsampling hyper-parameter.\nSubsampling\nprevents very frequent words of having too much\ninﬂuence in the learning as they would introduce\nstrong biases in the prediction task. With positives\nsubsampling and respecting the negative sampling\ndistribution, the precise training objective function\nbecomes\nmin\nU,V\nX\nS∈C\nX\nwt∈S\n\u0012\nqp(wt)ℓ\n\u0000u⊤\nwtvS\\{wt}\n\u0001\n(3)\n+ |Nwt|\nX\nw′∈V\nqn(w′)ℓ\n\u0000−u⊤\nw′vS\\{wt}\n\u0001\u0013\n2.2\nComputational Efﬁciency\nIn contrast to more complex neural network based\nmodels, one of the core advantages of the pro-\nposed technique is the low computational cost for\nboth inference and training. Given a sentence S\nand a trained model, computing the sentence rep-\nresentation vS only requires |S| · h ﬂoating point\noperations (or |R(S)| · h to be precise for the n-\ngram case, see (2)), where h is the embedding di-\nmension. The same holds for the cost of training\nwith SGD on the objective (3), per sentence seen\nin the training corpus. Due to the simplicity of the\n2To efﬁciently sample negatives, a pre-processing table\nis constructed, containing the words corresponding to the\nsquare root of their corpora frequency. Then, the negatives\nNwt are sampled uniformly at random from the negatives ta-\nble except the target wt itself, following (Joulin et al., 2017;\nBojanowski et al., 2017).\nmodel, parallel training is straight-forward using\nparallelized or distributed SGD.\nAlso, in order to store higher-order n-grams efﬁ-\nciently, we use the standard hashing trick, see e.g.\n(Weinberger et al., 2009), with the same hashing\nfunction as used in FastText (Joulin et al., 2017;\nBojanowski et al., 2017).\n2.3\nComparison to C-BOW\nC-BOW (Mikolov et al., 2013b,a) aims to predict\na chosen target word given its ﬁxed-size context\nwindow, the context being deﬁned by the average\nof the vectors associated with the words at a dis-\ntance less than the window size hyper-parameter\nws.\nIf our system, when restricted to unigram\nfeatures, can be seen as an extension of C-BOW\nwhere the context window includes the entire sen-\ntence, in practice there are few important differ-\nences as C-BOW uses important tricks to facilitate\nthe learning of word embeddings. C-BOW ﬁrst\nuses frequent word subsampling on the sentences,\ndeciding to discard each token w with probability\nqp(w) or alike (small variations exist across imple-\nmentations). Subsampling prevents the generation\nof n-grams features, and deprives the sentence of\nan important part of its syntactical features. It also\nshortens the distance between subsampled words,\nimplicitly increasing the span of the context win-\ndow. A second trick consists of using dynamic\ncontext windows: for each subsampled word w,\nthe size of its associated context window is sam-\npled uniformly between 1 and ws. Using dynamic\ncontext windows is equivalent to weighing by the\ndistance from the focus word w divided by the\nwindow size (Levy et al., 2015). This makes the\nprediction task local, and go against our objective\nof creating sentence embeddings as we want to\nlearn how to compose all n-gram features present\nin a sentence.\nIn the results section, we report\na signiﬁcant improvement of our method over C-\nBOW.\n2.4\nModel Training\nThree different datasets have been used to train\nour models: the Toronto book corpus3, Wikipedia\nsentences and tweets. The Wikipedia and Toronto\nbooks sentences have been tokenized using the\nStanford NLP library (Manning et al., 2014),\nwhile for tweets we used the NLTK tweets tok-\nenizer (Bird et al., 2009). For training, we select a\n3http://www.cs.toronto.edu/˜mbweb/\nsentence randomly from the dataset and then pro-\nceed to select all the possible target unigrams us-\ning subsampling.\nWe update the weights using\nSGD with a linearly decaying learning rate.\nAlso, to prevent overﬁtting, for each sentence\nwe use dropout on its list of n-grams R(S) \\\n{U(S)}, where U(S) is the set of all unigrams\ncontained in sentence S.\nAfter empirically try-\ning multiple dropout schemes, we ﬁnd that drop-\nping K n-grams (n > 1) for each sentence is\ngiving superior results compared to dropping each\ntoken with some ﬁxed probability. This dropout\nmechanism would negatively impact shorter sen-\ntences.\nThe regularization can be pushed fur-\nther by applying L1 regularization to the word\nvectors. Encouraging sparsity in the embedding\nvectors is particularly beneﬁcial for high dimen-\nsion h. The additional soft thresholding in every\nSGD step adds negligible computational cost. See\nalso Appendix B. We train two models on each\ndataset, one with unigrams only and one with un-\nigrams and bigrams. All training parameters for\nthe models are provided in Table 5 in the supple-\nmentary material. Our C++ implementation builds\nupon the FastText library (Joulin et al., 2017; Bo-\njanowski et al., 2017). We will make our code and\npre-trained models available open-source.\n3\nRelated Work\nWe discuss existing models which have been pro-\nposed to construct sentence embeddings. While\nthere is a large body of works in this direction –\nseveral among these using e.g. labelled datasets of\nparaphrase pairs to obtain sentence embeddings in\na supervised manner (Wieting et al., 2016a,b; Con-\nneau et al., 2017) to learn sentence embeddings –\nwe here focus on unsupervised, task-independent\nmodels. While some methods require ordered raw\ntext i.e., a coherent corpus where the next sentence\nis a logical continuation of the previous sentence,\nothers rely only on raw text i.e., an unordered col-\nlection of sentences. Finally, we also discuss alter-\nnative models built from structured data sources.\n3.1\nUnsupervised Models Independent of\nSentence Ordering\nThe ParagraphVector DBOW model (Le and\nMikolov, 2014) is a log-linear model which is\ntrained to learn sentence as well as word embed-\ndings and then use a softmax distribution to predict\nwords contained in the sentence given the sentence\nvector representation.\nThey also propose a dif-\nferent model ParagraphVector DM where they\nuse n-grams of consecutive words along with the\nsentence vector representation to predict the next\nword.\n(Lev et al., 2015) also presented an early ap-\nproach to obtain compositional embeddings from\nword vectors. They use different compositional\ntechniques including static averaging or Fisher\nvectors of a multivariate Gaussian to obtain sen-\ntence embeddings from word2vec models.\nHill et al. (2016a) propose a Sequential (De-\nnoising) Autoencoder, S(D)AE. This model ﬁrst\nintroduces noise in the input data: Firstly each\nword is deleted with probability p0, then for each\nnon-overlapping bigram, words are swapped with\nprobability px. The model then uses an LSTM-\nbased architecture to retrieve the original sentence\nfrom the corrupted version. The model can then\nbe used to encode new sentences into vector rep-\nresentations. In the case of p0 = px = 0, the\nmodel simply becomes a Sequential Autoencoder.\nHill et al. (2016a) also propose a variant (S(D)AE\n+ embs.) in which the words are represented by\nﬁxed pre-trained word vector embeddings.\nArora et al. (2017) propose a model in which\nsentences are represented as a weighted average\nof ﬁxed (pre-trained) word vectors, followed by\npost-processing step of subtracting the principal\ncomponent. Using the generative model of (Arora\net al., 2016), words are generated conditioned on\na sentence “discourse” vector cs:\nPr[w | cs] = αfw + (1 −α)exp(˜c⊤\ns vw)\nZ˜cs\n,\nwhere Z˜cs\n:=\nP\nw∈V exp(˜c⊤\ns vw) and ˜cs\n:=\nβc0 + (1 −β)cs and α, β are scalars. c0 is the\ncommon discourse vector, representing a shared\ncomponent among all discourses, mainly related\nto syntax. It allows the model to better generate\nsyntactical features. The αfw term is here to en-\nable the model to generate some frequent words\neven if their matching with the discourse vector ˜cs\nis low.\nTherefore, this model tries to generate sentences\nas a mixture of three type of words: words match-\ning the sentence discourse vector cs, syntacti-\ncal words matching c0, and words with high fw.\n(Arora et al., 2017) demonstrated that for this\nmodel, the MLE of ˜cs can be approximated by\nP\nw∈S\na\nfw+avw, where a is a scalar. The sentence\ndiscourse vector can hence be obtained by sub-\ntracting c0 estimated by the ﬁrst principal com-\nponent of ˜cs’s on a set of sentences.\nIn other\nwords, the sentence embeddings are obtained by\na weighted average of the word vectors strip-\nping away the syntax by subtracting the com-\nmon discourse vector and down-weighting fre-\nquent tokens.\nThey generate sentence embed-\ndings from diverse pre-trained word embeddings\namong which are unsupervised word embeddings\nsuch as GloVe (Pennington et al., 2014) as well\nas supervised word embeddings such as paragram-\nSL999 (PSL) (Wieting et al., 2015) trained on the\nParaphrase Database (Ganitkevitch et al., 2013).\nIn a very different line of work, C-PHRASE\n(Pham et al., 2015) relies on additional informa-\ntion from the syntactic parse tree of each sentence,\nwhich is incorporated into the C-BOW training\nobjective.\nHuang and Anandkumar (2016) show that sin-\ngle layer CNNs can be modeled using a tensor\ndecomposition approach.\nWhile building on an\nunsupervised objective, the employed dictionary\nlearning step for obtaining phrase templates is\ntask-speciﬁc (for each use-case), not resulting in\ngeneral-purpose embeddings.\n3.2\nUnsupervised Models Depending on\nSentence Ordering\nThe SkipThought model (Kiros et al., 2015) com-\nbines sentence level models with recurrent neu-\nral networks.\nGiven a sentence Si from an or-\ndered corpus, the model is trained to predict Si−1\nand Si+1.\nFastSent (Hill et al., 2016a) is a sentence-\nlevel log-linear bag-of-words model.\nLike\nSkipThought, it uses adjacent sentences as the pre-\ndiction target and is trained in an unsupervised\nfashion. Using word sequences allows the model\nto improve over the earlier work of paragraph2vec\n(Le and Mikolov, 2014). (Hill et al., 2016a) aug-\nment FastSent further by training it to predict the\nconstituent words of the sentence as well. This\nmodel is named FastSent + AE in our compar-\nisons.\nCompared to our approach, Siamese C-BOW\n(Kenter et al., 2016) shares the idea of learning to\naverage word embeddings over a sentence. How-\never, it relies on a Siamese neural network archi-\ntecture to predict surrounding sentences, contrast-\ning our simpler unsupervised objective.\nNote that on the character sequence level in-\nstead of word sequences, FastText (Bojanowski\net al., 2017) uses the same conceptual model to ob-\ntain better word embeddings. This is most similar\nto our proposed model, with two key differences:\nFirstly, we predict from source word sequences to\ntarget words, as opposed to character sequences to\ntarget words, and secondly, our model is averaging\nthe source embeddings instead of summing them.\n3.3\nModels requiring structured data\nDictRep (Hill et al., 2016b) is trained to map dic-\ntionary deﬁnitions of the words to the pre-trained\nword embeddings of these words. They use two\ndifferent architectures, namely BOW and RNN\n(LSTM) with the choice of learning the input word\nembeddings or using them pre-trained. A similar\narchitecture is used by the CaptionRep variant,\nbut here the task is the mapping of given image\ncaptions to a pre-trained vector representation of\nthese images.\n4\nEvaluation Tasks\nWe use a standard set of supervised as well as un-\nsupervised benchmark tasks from the literature to\nevaluate our trained models, following (Hill et al.,\n2016a). The breadth of tasks allows to fairly mea-\nsure generalization to a wide area of different do-\nmains, testing the general-purpose quality (univer-\nsality) of all competing sentence embeddings. For\ndownstream supervised evaluations, sentence em-\nbeddings are combined with logistic regression to\npredict target labels. In the unsupervised evalua-\ntion for sentence similarity, correlation of the co-\nsine similarity between two embeddings is com-\npared to human annotators.\nDownstream Supervised Evaluation.\nSen-\ntence embeddings are evaluated for various su-\npervised classiﬁcation tasks as follows.\nWe\nevaluate paraphrase identiﬁcation (MSRP) (Dolan\net al., 2004), classiﬁcation of movie review sen-\ntiment (MR) (Pang and Lee, 2005), product re-\nviews (CR) (Hu and Liu, 2004), subjectivity clas-\nsiﬁcation (SUBJ) (Pang and Lee, 2004), opinion\npolarity (MPQA) (Wiebe et al., 2005) and ques-\ntion type classiﬁcation (TREC) (Voorhees, 2002).\nTo classify, we use the code provided by (Kiros\net al., 2015) in the same manner as in (Hill et al.,\n2016a). For the MSRP dataset, containing pairs of\nsentences (S1, S2) with associated paraphrase la-\nbel, we generate feature vectors by concatenating\ntheir Sent2Vec representations |vS1 −vS2| with\nthe component-wise product vS1 ⊙vS2. The pre-\ndeﬁned training split is used to tune the L2 penalty\nparameter using cross-validation and the accuracy\nand F1 scores are computed on the test set. For\nthe remaining 5 datasets, Sent2Vec embeddings\nare inferred from input sentences and directly fed\nto a logistic regression classiﬁer. Accuracy scores\nare obtained using 10-fold cross-validation for the\nMR, CR, SUBJ and MPQA datasets. For those\ndatasets nested cross-validation is used to tune the\nL2 penalty.\nFor the TREC dataset, as for the\nMRSP dataset, the L2 penalty is tuned on the pre-\ndeﬁned train split using 10-fold cross-validation,\nand the accuracy is computed on the test set.\nUnsupervised Similarity Evaluation. We per-\nform unsupervised evaluation of the learnt sen-\ntence embeddings using the sentence cosine sim-\nilarity, on the STS 2014 (Agirre et al., 2014)\nand SICK 2014 (Marelli et al., 2014) datasets.\nThese similarity scores are compared to the gold-\nstandard human judgements using Pearson’s r\n(Pearson, 1895) and Spearman’s ρ (Spearman,\n1904) correlation scores. The SICK dataset con-\nsists of about 10,000 sentence pairs along with\nrelatedness scores of the pairs.\nThe STS 2014\ndataset contains 3,770 pairs, divided into six dif-\nferent categories on the basis of the origin of sen-\ntences/phrases, namely Twitter, headlines, news,\nforum, WordNet and images.\n5\nResults and Discussion\nIn Tables 1 and 2, we compare our results with\nthose obtained by (Hill et al., 2016a) on different\nmodels. Table 3 in the last column shows the dra-\nmatic improvement in training time of our mod-\nels (and other C-BOW-inspired models) in con-\ntrast to neural network based models.\nAll our\nSent2Vec models are trained on a machine with\n2x Intel Xeon E5−2680v3, 12 cores @2.5GHz.\nAlong with the models discussed in Section 3, this\nalso includes the sentence embedding baselines\nobtained by simple averaging of word embeddings\nover the sentence, in both the C-BOW and skip-\ngram variants. TF-IDF BOW is a representation\nconsisting of the counts of the 200,000 most com-\nmon feature-words, weighed by their TF-IDF fre-\nquencies. To ensure coherence, we only include\nunsupervised models in the main paper. Perfor-\nmance of supervised and semi-supervised models\non these evaluations can be observed in Tables 6\nand 7 in the supplementary material.\nDownstream Supervised Evaluation Results.\nOn running supervised evaluations and observing\nthe results in Table 1, we ﬁnd that on an aver-\nage our models are second only to SkipThought\nvectors.\nAlso, both our models achieve state\nof the art results on the CR task.\nWe also ob-\nserve that on half of the supervised tasks, our\nunigrams + bigram model is the best model af-\nter SkipThought. Our models are weaker on the\nMSRP task (which consists of the identiﬁcation of\nlabelled paraphrases) compared to state-of-the-art\nmethods. However, we observe that the models\nwhich perform very strongly on this task end up\nfaring very poorly on the other tasks, indicating a\nlack of generalizability.\nOn rest of the tasks, our models perform ex-\ntremely well. The SkipThought model is able to\noutperform our models on most of the tasks as it is\ntrained to predict the previous and next sentences\nand a lot of tasks are able to make use of this con-\ntextual information missing in our Sent2Vec mod-\nels. For example, the TREC task is a poor measure\nof how one predicts the content of the sentence\n(the question) but a good measure of how the next\nsentence in the sequence (the answer) is predicted.\nUnsupervised Similarity Evaluation Results.\nIn Table 2, we see that our Sent2Vec models\nare state-of-the-art on the majority of tasks when\ncomparing to all the unsupervised models trained\non the Toronto corpus, and clearly achieve the\nbest averaged performance. Our Sent2Vec mod-\nels also on average outperform or are at par with\nthe C-PHRASE model, despite signiﬁcantly lag-\nging behind on the STS 2014 WordNet and News\nsubtasks.\nThis observation can be attributed to\nthe fact that a big chunk of the data that the C-\nPHRASE model is trained on comes from English\nWikipedia, helping it to perform well on datasets\ninvolving deﬁnition and news items.\nAlso, C-\nPHRASE uses data three times the size of the\nToronto book corpus. Interestingly, our model out-\nperforms C-PHRASE when trained on Wikipedia,\nas shown in Table 3, despite the fact that we use\nno parse tree information.\nOfﬁcial STS 2017 benchmark. In the ofﬁcial\nresults of the most recent edition of the STS 2017\nbenchmark (Cer et al., 2017), our model also sig-\nniﬁcantly outperforms C-PHRASE, and in fact de-\nlivers the best unsupervised baseline method.\n4For the Siamese C-BOW model trained on the Toronto\nData\nModel\nMSRP\n(Acc / F1)\nMR\nCR\nSUBJ\nMPQA\nTREC\nAverage\nUnordered Sentences:\n(Toronto Books;\n70 million sentences,\n0.9 Billion Words)\nSAE\n74.3 / 81.7\n62.6\n68.0\n86.1\n76.8\n80.2\n74.7\nSAE + embs.\n70.6 / 77.9\n73.2\n75.3\n89.8\n86.2\n80.4\n79.3\nSDAE\n76.4 / 83.4\n67.6\n74.0\n89.3\n81.3\n77.7\n78.3\nSDAE + embs.\n73.7 / 80.7\n74.6\n78.0\n90.8\n86.9\n78.4\n80.4\nParagraphVec DBOW\n72.9 / 81.1\n60.2\n66.9\n76.3\n70.7\n59.4\n67.7\nParagraphVec DM\n73.6 / 81.9\n61.5\n68.6\n76.4\n78.1\n55.8\n69.0\nSkipgram\n69.3 / 77.2\n73.6\n77.3\n89.2\n85.0\n82.2\n78.5\nC-BOW\n67.6 / 76.1\n73.6\n77.3\n89.1\n85.0\n82.2\n79.1\nUnigram TFIDF\n73.6 / 81.7\n73.7\n79.2\n90.3\n82.4\n85.0\n80.7\nSent2Vec uni.\n72.2 / 80.3\n75.1\n80.2\n90.6\n86.3\n83.8\n81.4\nSent2Vec uni. + bi.\n72.5 / 80.8\n75.8\n80.3\n91.2\n85.9\n86.4\n82.0\nOrdered Sentences:\nToronto Books\nSkipThought\n73.0 / 82.0\n76.5\n80.1\n93.6\n87.1\n92.2\n83.8\nFastSent\n72.2 / 80.3\n70.8\n78.4\n88.7\n80.6\n76.8\n77.9\nFastSent+AE\n71.2 / 79.1\n71.8\n76.7\n88.8\n81.5\n80.4\n78.4\n2.8 Billion words\nC-PHRASE\n72.2 / 79.6\n75.7\n78.8\n91.1\n86.2\n78.8\n80.5\nTable 1: Comparison of the performance of different models on different supervised evaluation tasks. An underline indicates\nthe best performance for the dataset. Top 3 performances in each data category are shown in bold. The average is calculated as\nthe average of accuracy for each category (For MSRP, we take the accuracy). )\nSTS 2014\nSICK 2014\nModel\nNews\nForum\nWordNet\nTwitter\nImages\nHeadlines\nTest + Train\nAverage\nSAE\n.17/.16\n.12/.12\n.30/.23\n.28/.22\n.49/.46\n.13/.11\n.32/.31\n.26/.23\nSAE + embs.\n.52/.54\n.22/.23\n.60/.55\n.60/.60\n.64/.64\n.41/.41\n.47/.49\n.50/.49\nSDAE\n.07/.04\n.11/.13\n.33/.24\n.44/.42\n.44/.38\n.36/.36\n.46/.46\n.31/.29\nSDAE + embs.\n.51/.54\n.29/.29\n.56/.50\n.57/.58\n.59/.59\n.43/.44\n.46/.46\n.49/.49\nParagraphVec DBOW\n.31/.34\n.32/.32\n.53/.50\n.43/.46\n.46/.44\n.39/.41\n.42/.46\n.41/.42\nParagraphVec DM\n.42/.46\n.33/.34\n.51/.48\n.54/.57\n.32/.30\n.46/.47\n.44/.40\n.43/.43\nSkipgram\n.56/.59\n.42/.42\n.73/.70\n.71/.74\n.65/.67\n.55/.58\n.60/.69\n.60/.63\nC-BOW\n.57/.61\n.43/.44\n.72/.69\n.71/.75\n.71/.73\n.55/.59\n.60/.69\n.60/.65\nUnigram TF-IDF\n.48/.48\n.40/.38\n.60/.59\n.63/.65\n.72/.74\n.49/.49\n.52/.58\n.55/.56\nSent2Vec uni.\n.62/.67\n.49/.49\n.75/.72\n.70/.75\n.78/.82\n.61/.63\n.61/.70\n.65/.68\nSent2Vec uni. + bi.\n.62/.67\n.51/.51\n.71/.68\n.70/.75\n.75/.79\n.59/.62\n.62/.70\n.65/.67\nSkipThought\n.44/.45\n.14/.15\n.39/.34\n.42/.43\n.55/.60\n.43/.44\n.57/.60\n.42/.43\nFastSent\n.58/.59\n.41/.36\n.74/.70\n.63/.66\n.74/.78\n.57/.59\n.61/.72\n.61/.63\nFastSent+AE\n.56/.59\n.41/.40\n.69/.64\n.70/.74\n.63/.65\n.58/.60\n.60/.65\n.60/.61\nSiamese C-BOW4\n.58/.59\n.42/.41\n.66/.61\n.71/.73\n.65/.65\n.63/.64\n−\n−\nC-PHRASE\n.69/.71\n.43/.41\n.76/.73\n.60/.65\n.75/.79\n.60/.65\n.60/.72\n.63/.67\nTable 2: Unsupervised Evaluation Tasks: Comparison of the performance of different models on Spearman/Pearson corre-\nlation measures. An underline indicates the best performance for the dataset. Top 3 performances in each data category are\nshown in bold. The average is calculated as the average of entries for each correlation measure.\nMacro Average. To summarize our contribu-\ntions on both supervised and unsupervised tasks,\nin Table 3 we present the results in terms of the\nmacro average over the averages of both super-\nvised and unsupervised tasks along with the train-\ning times of the models5. For unsupervised tasks,\naverages are taken over both Spearman and Pear-\nson scores. The comparison includes the best per-\nforming unsupervised and semi-supervised meth-\nods described in Section 3. For models trained\non the Toronto books dataset, we report a 3.8 %\npoints improvement over the state of the art. Con-\nsidering all supervised, semi-supervised methods\nand all datasets compared in (Hill et al., 2016a),\ncorpus, supervised evaluation as well as similarity evaluation\nresults on the SICK 2014 dataset are unavailable.\n5time taken to train C-PHRASE models is unavailable\nwe report a 2.2 % points improvement.\nWe also see a noticeable improvement in ac-\ncuracy as we use larger datasets like Twitter and\nWikipedia. We furthermore see that the Sent2Vec\nmodels are faster to train when compared to meth-\nods like SkipThought and DictRep, owing to the\nSGD optimizer allowing a high degree of paral-\nlelizability.\nWe can clearly see Sent2Vec outperforming\nother unsupervised and even semi-supervised\nmethods. This can be attributed to the superior\ngeneralizability of our model across supervised\nand unsupervised tasks.\nComparison with Arora et al. (2017). We also\ncompare our work with Arora et al. (2017) who\nalso use additive compositionality to obtain sen-\ntence embeddings.\nHowever, in contrast to our\nType\nTraining corpus\nMethod\nSupervised\naverage\nUnsupervised\naverage\nMacro\naverage\nTraining time\n(in hours)\nunsupervised\ntwitter (19.7B words)\nSent2Vec uni. + bi.\n83.5\n68.3\n75.9\n6.5*\nunsupervised\ntwitter (19.7B words)\nSent2Vec uni.\n82.2\n69.0\n75.6\n3*\nunsupervised\nWikipedia (1.7B words)\nSent2Vec uni. + bi.\n83.3\n66.2\n74.8\n2*\nunsupervised\nWikipedia (1.7B words)\nSent2Vec uni.\n82.4\n66.3\n74.3\n3.5*\nunsupervised\nToronto books (0.9B words)\nSent2Vec books uni.\n81.4\n66.7\n74.0\n1*\nunsupervised\nToronto books (0.9B words)\nSent2Vec books uni. + bi.\n82.0\n65.9\n74.0\n1.2*\nsemi-supervised\nstructured dictionary dataset\nDictRep BOW + emb\n80.5\n66.9\n73.7\n24**\nunsupervised\n2.8B words + parse info.\nC-PHRASE\n80.5\n64.9\n72.7\n−\nunsupervised\nToronto books (0.9B words)\nC-BOW\n79.1\n62.8\n70.2\n2\nunsupervised\nToronto books (0.9B words)\nFastSent\n77.9\n62.0\n70.0\n2\nunsupervised\nToronto books (0.9B words)\nSkipThought\n83.8\n42.5\n63.1\n336**\nTable 3: Best unsupervised and semi-supervised methods ranked by macro average along with their training times. ** indicates\ntrained on GPU. * indicates trained on a single node using 30 threads. Training times for non-Sent2Vec models are due to Hill\net al. (2016a). For CPU based competing methods, we were able to reproduce all published timings (+-10%) using our same\nhardware as for training Sent2Vec.\nDataset\nUnsupervised\nGloVe (840B words)\n+ WR\nSemi-supervised\nPSL + WR\nSent2Vec Unigrams\n(19.7B words)\nTweets Model\nSent2Vec Unigrams + Bigrams\n(19.7B words)\nTweets Model\nSTS 2014\n0.685\n0.735\n0.710\n0.701\nSICK 2014\n0.722\n0.729\n0.710\n0.715\nSupervised average\n0.815\n0.807\n0.822\n0.835\nTable 4: Comparison of the performance of the unsupervised and semi-supervised sentence embeddings by (Arora et al., 2017)\nwith our models. Unsupervised comparisons are in terms of Pearson’s correlation, while comparisons on supervised tasks are\nstating the average described in Table 1.\nmodel, they use ﬁxed, pre-trained word embed-\ndings to build a weighted average of these em-\nbeddings using unigram probabilities. While we\ncouldn’t ﬁnd pre-trained state of the art word em-\nbeddings trained on the Toronto books corpus, we\nevaluated their method using GloVe embeddings\nobtained from the larger Common Crawl Corpus6,\nwhich is 42 times larger than our twitter corpus,\ngreatly favoring their method over ours.\nIn Table 4, we report an experimental compar-\nison to their model on unsupervised tasks.\nIn\nthe table, the sufﬁx W indicates that their down-\nweighting scheme has been used, while the suf-\nﬁx R indicates the removal of the ﬁrst princi-\npal component.\nThey report values of a\n∈\n[10−4, 10−3] as giving the best results and used\na = 10−3 for all their experiments. We observe\nthat our results are competitive with the embed-\ndings of Arora et al. (2017) for purely unsuper-\nvised methods.\nIt is important to note that the\nscores obtained from supervised task-speciﬁc PSL\nembeddings trained for the purpose of semantic\nsimilarity outperform our method on both SICK\nand average STS 2014, which is expected as our\nmodel is trained purely unsupervised.\nIn order to facilitate a more detailed compari-\nson, we also evaluated the unsupervised Glove +\nWR embeddings on downstream supervised tasks\n6http://www.cs.toronto.edu/˜mbweb/\nand compared them to our twitter models. To use\nArora et al. (2017)’s method in a supervised setup,\nwe precomputed and stored the common discourse\nvector c0 using 2 million random Wikipedia sen-\ntences.\nOn an average, our models outperform\ntheir unsupervised models by a signiﬁcant margin,\nthis despite the fact that they used GloVe embed-\ndings trained on larger corpora than ours (42 times\nlarger). Our models also outperform their semi-\nsupervised PSL + WR model. This indicates our\nmodel learns a more precise weighing scheme than\nthe static one proposed by Arora et al. (2017).\nFigure 1: Left ﬁgure: the proﬁle of the word vector L2-\nnorms as a function of log(fw) for each vocabulary word w,\nas learnt by our unigram model trained on Toronto books.\nRight ﬁgure: down-weighting scheme proposed by Arora\net al. (2017): weight(w) =\na\na+fw .\nThe effect of datasets and n-grams. Despite\nbeing trained on three very different datasets, all\nof our models generalize well to sometimes very\nspeciﬁc domains. Models trained on Toronto Cor-\npus are the state-of-the-art on the STS 2014 im-\nages dataset even beating the supervised Caption-\nRep model trained on images. We also see that\naddition of bigrams to our models doesn’t help\nmuch when it comes to unsupervised evaluations\nbut gives a signiﬁcant boost-up in accuracy on\nsupervised tasks. We attribute this phenomenon\nto the ability of bigrams models to capture some\nnon-compositional features missed by unigrams\nmodels. Having a single representation for “not\ngood” or “very bad” can boost the supervised\nmodel’s ability to infer relevant features for the\ncorresponding classiﬁer. For semantic similarity\ntasks however, the relative uniqueness of bigrams\nresults in pushing sentence representations further\napart, which can explain the average drop of scores\nfor bigrams models on those tasks.\nOn learning the importance and the direction\nof the word vectors. Our model – by learning\nhow to generate and compose word vectors – has\nto learn both the direction of the word embeddings\nas well as their norm. Considering the norms of\nthe used word vectors as by our averaging over the\nsentence, we observe an interesting distribution of\nthe “importance” of each word. In Figure 1 we\nshow the proﬁle of the L2-norm as a function of\nlog(fw) for each w ∈V, and compare it to the\nstatic down-weighting mechanism of Arora et al.\n(2017). We can observe that our model is learn-\ning to down-weight frequent tokens by itself. It\nis also down-weighting rare tokens and the norm\nproﬁle seems to roughly follow Luhn’s hypothesis\n(Luhn, 1958), a well known information retrieval\nparadigm, stating that mid-rank terms are the most\nsigniﬁcant to discriminate content.\n6\nConclusion\nIn this paper, we introduce a novel, computa-\ntionally efﬁcient, unsupervised, C-BOW-inspired\nmethod to train and infer sentence embeddings.\nOn supervised evaluations, our method, on an av-\nerage, achieves better performance than all other\nunsupervised competitors with the exception of\nSkipThought.\nHowever, SkipThought vectors\nshow a very poor performance on sentence simi-\nlarity tasks while our model is state-of-the-art for\nthese evaluations on average. Also, our model is\ngeneralizable, extremely fast to train, simple to un-\nderstand and easily interpretable, showing the rel-\nevance of simple and well-grounded representa-\ntion models in contrast to the models using deep\narchitectures. Future work could focus on aug-\nmenting the model to exploit data with ordered\nsentences. Furthermore, we would like to investi-\ngate the model’s ability to use pre-trained embed-\ndings for downstream transfer learning tasks.\nAcknowledgments\nWe are indebted to Piotr Bojanowski and Armand\nJoulin for helpful discussions. This project was\nsupported by a Google Faculty Research Award.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014.\nSemeval-2014 task 10:\nMultilin-\ngual semantic textual similarity. In Proceedings of\nthe 8th international workshop on semantic evalua-\ntion (SemEval 2014). Association for Computational\nLinguistics Dublin, Ireland, pages 81–91.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu\nMa, and Andrej Risteski. 2016.\nA Latent Vari-\nable Model Approach to PMI-based Word Embed-\ndings. In Transactions of the Association for Com-\nputational Linguistics. pages 385–399.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In International Conference on Learning\nRepresentations (ICLR).\nSteven Bird, Ewan Klein, and Edward Loper. 2009.\nNatural language processing with Python: analyz-\ning text with the natural language toolkit. ” O’Reilly\nMedia, Inc.”.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching Word Vectors with\nSubword Information. Transactions of the Associa-\ntion for Computational Linguistics 5:135–146.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemEval-2017\nTask 1:\nSemantic Textual Similarity Multilin-\ngual and Cross-lingual Focused Evaluation.\nIn\nSemEval-2017 - Proceedings of the 11th Interna-\ntional Workshop on Semantic Evaluations. Asso-\nciation for Computational Linguistics, Vancouver,\nCanada, pages 1–14.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\nBarrault, and Antoine Bordes. 2017.\nSupervised\nlearning of universal sentence representations from\nnatural language inference data.\narXiv preprint\narXiv:1705.02364 .\nBill Dolan, Chris Quirk, and Chris Brockett. 2004.\nUnsupervised construction of large paraphrase cor-\npora: Exploiting massively parallel news sources. In\nProceedings of the 20th international conference on\nComputational Linguistics. Association for Compu-\ntational Linguistics, page 350.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013.\nPpdb:\nThe paraphrase\ndatabase. In HLT-NAACL. pages 758–764.\nYoav Goldberg and Omer Levy. 2014. word2vec Ex-\nplained: deriving Mikolov et al.’s negative-sampling\nword-embedding method. arXiv .\nFelix Hill, Kyunghyun Cho, and Anna Korhonen.\n2016a.\nLearning Distributed Representations of\nSentences from Unlabelled Data. In Proceedings of\nNAACL-HLT.\nFelix Hill, KyungHyun Cho, Anna Korhonen, and\nYoshua Bengio. 2016b.\nLearning to understand\nphrases by embedding the dictionary. TACL 4:17–\n30.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining. ACM, pages 168–\n177.\nFurong Huang and Animashree Anandkumar. 2016.\nUnsupervised Learning of Word-Sequence Repre-\nsentations from Scratch via Convolutional Tensor\nDecomposition. arXiv .\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of Tricks for Efﬁcient\nText Classiﬁcation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics, Short Papers. Valen-\ncia, Spain, pages 427–431.\nTom Kenter, Alexey Borisov, and Maarten de Rijke.\n2016. Siamese CBOW: Optimizing Word Embed-\ndings for Sentence Representations. In ACL - Pro-\nceedings of the 54th Annual Meeting of the Asso-\nciation for Computational Linguistics. Berlin, Ger-\nmany, pages 941–951.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-Thought Vectors. In\nNIPS 2015 - Advances in Neural Information Pro-\ncessing Systems 28. pages 3294–3302.\nQuoc V Le and Tomas Mikolov. 2014.\nDistributed\nRepresentations of Sentences and Documents.\nIn\nICML 2014 - Proceedings of the 31st International\nConference on Machine Learning. volume 14, pages\n1188–1196.\nGuy Lev, Benjamin Klein, and Lior Wolf. 2015. In de-\nfense of word embedding for generic text representa-\ntion. In International Conference on Applications of\nNatural Language to Information Systems. Springer,\npages 35–50.\nOmer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-\nproving distributional similarity with lessons learned\nfrom word embeddings. Transactions of the Associ-\nation for Computational Linguistics 3:211–225.\nHans Peter Luhn. 1958. The automatic creation of lit-\nerature abstracts. IBM Journal of research and de-\nvelopment 2(2):159–165.\nChristopher D Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014.\nThe stanford corenlp natural lan-\nguage processing toolkit. In ACL (System Demon-\nstrations). pages 55–60.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. 2014. A sick cure for the evaluation of com-\npositional distributional semantic models. In LREC.\npages 216–223.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a.\nEfﬁcient estimation of word\nrepresentations in vector space.\narXiv preprint\narXiv:1301.3781 .\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed Representa-\ntions of Words and Phrases and their Compositional-\nity. In NIPS - Advances in Neural Information Pro-\ncessing Systems 26. pages 3111–3119.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings of\nthe 42nd annual meeting on Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics, page 271.\nBo Pang and Lillian Lee. 2005.\nSeeing stars: Ex-\nploiting class relationships for sentiment categoriza-\ntion with respect to rating scales. In Proceedings of\nthe 43rd annual meeting on association for compu-\ntational linguistics. Association for Computational\nLinguistics, pages 115–124.\nKarl Pearson. 1895. Note on regression and inheritance\nin the case of two parents. Proceedings of the Royal\nSociety of London 58:240–242.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014.\nGlove: Global vectors for word\nrepresentation. In EMNLP. volume 14, pages 1532–\n1543.\nNT Pham, G Kruszewski, A Lazaridou, and M Baroni.\n2015. Jointly optimizing word representations for\nlexical and sentential tasks with the c-phrase model.\nACL/IJCNLP .\nR Tyrrell Rockafellar. 1976. Monotone operators and\nthe proximal point algorithm. SIAM journal on con-\ntrol and optimization 14(5):877–898.\nCharles Spearman. 1904. The proof and measurement\nof association between two things. The American\njournal of psychology 15(1):72–101.\nEllen M Voorhees. 2002. Overview of the trec 2001\nquestion answering track. In NIST special publica-\ntion. pages 42–51.\nKilian Weinberger, Anirban Dasgupta, John Langford,\nAlex Smola, and Josh Attenberg. 2009.\nFeature\nhashing for large scale multitask learning. In Pro-\nceedings of the 26th Annual International Confer-\nence on Machine Learning. ACM, pages 1113–\n1120.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\n2005. Annotating expressions of opinions and emo-\ntions in language. Language resources and evalua-\ntion 39(2):165–210.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016a.\nCharagram: Embedding Words\nand Sentences via Character n-grams. In EMNLP\n- Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing. Asso-\nciation for Computational Linguistics, Stroudsburg,\nPA, USA, pages 1504–1515.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016b.\nTowards universal paraphrastic\nsentence embeddings. In International Conference\non Learning Representations (ICLR).\nJohn Wieting, Mohit Bansal, Kevin Gimpel, Karen\nLivescu, and Dan Roth. 2015.\nFrom paraphrase\ndatabase to compositional paraphrase model and\nback. In TACL - Transactions of the Association for\nComputational Linguistics.\nSupplementary Material\nA\nParameters for training models\nModel\nEmbedding\nDimensions\nMinimum\nword count\nMinimum\nTarget word\nCount\nInitial\nLear ning\nRate\nEpochs\nSubsampling\nhyper-parameter\nBigrams\nDropped\nper sentence\nNumber of\nnegatives\nsampled\nBook corpus\nSent2Vec\nunigrams\n700\n5\n8\n0.2\n13\n1 × 10−5\n-\n10\nBook corpus\nSent2Vec\nunigrams + bigrams\n700\n5\n5\n0.2\n12\n5 × 10−6\n7\n10\nWiki Sent2Vec\nunigrams\n600\n8\n20\n0.2\n9\n1 × 10−5\n-\n10\nWiki Sent2Vec\nunigrams + bigrams\n700\n8\n20\n0.2\n9\n5 × 10−6\n4\n10\nTwitter Sent2Vec\nunigrams\n700\n20\n20\n0.2\n3\n1 × 10−6\n-\n10\nTwitter Sent2Vec\nunigrams + bigrams\n700\n20\n20\n0.2\n3\n1 × 10−6\n3\n10\nTable 5: Training parameters for the Sent2Vec models\nB\nL1 regularization of models\nOptionally, our model can be additionally improved by adding an L1 regularizer term in the objective\nfunction, leading to slightly better generalization performance. Additionally, encouraging sparsity in the\nembedding vectors is beneﬁcial for memory reasons, allowing higher embedding dimensions h.\nWe propose to apply L1 regularization individually to each word (and n-gram) vector (both source and\ntarget vectors). Formally, the training objective function (3) then becomes\nmin\nU,V\nX\nS∈C\nX\nwt∈S\nqp(wt)\n\u0012\u0010\nℓ\n\u0000u⊤\nwtvS\\{wt}\n\u0001\n+ τ(∥uwt∥1 + ∥vS\\{wt}∥1)\n\u0011\n+\n(4)\n|Nwt|\nX\nw′∈V\nqn(w′)\n\u0010\nℓ\n\u0000−u⊤\nw′vS\\{wt}\n\u0001\n+ τ(∥uw′∥1)\n\u0011\u0013\nwhere τ is the regularization parameter.\nNow, in order to minimize a function of the form f(z) + g(z) where g(z) is not differentiable over the\ndomain, we can use the basic proximal-gradient scheme. In this iterative method, after doing a gradient\ndescent step on f(z) with learning rate α, we update z as\nzn+1 = proxα,g(zn+ 1\n2 )\n(5)\nwhere proxα,g(x) = arg miny{g(y)+ 1\n2α∥y−x∥2\n2} is called the proximal function (Rockafellar, 1976)\nof g with α being the proximal parameter and zn+ 1\n2 is the value of z after a gradient (or SGD) step on zn.\nIn our case, g(z) = ∥z∥1 and the corresponding proximal operator is given by\nproxα,g(x) = sign(x) ⊙max(|xn| −α, 0)\n(6)\nwhere ⊙corresponds to element-wise product.\nSimilar to the proximal-gradient scheme, in our case we can optionally use the thresholding operator\non the updated word and n-gram vectors after an SGD step. The soft thresholding parameter used for\nthis update is\nτ·lr′\n|R(S\\{wt})| and τ · lr′ for the source and target vectors respectively where lr′ is the current\nlearning rate, τ is the L1 regularization parameter and S is the sentence on which SGD is being run.\nWe observe that L1 regularization using the proximal step gives our models a small boost in perfor-\nmance. Also, applying the thresholding operator takes only |R(S \\ {wt})| · h ﬂoating point operations\nfor the updating the word vectors corresponding to the sentence and (|N| + 1) · h for updating the target\nas well as the negative word vectors, where |N| is the number of negatives sampled and h is the em-\nbedding dimension. Thus, performing L1 regularization using soft-thresholding operator comes with a\nsmall computational overhead.\nWe set τ to be 0.0005 for both the Wikipedia and the Toronto Book Corpus unigrams + bigrams\nmodels.\nC\nPerformance comparison with Sent2Vec models trained on different corpora\nData\nModel\nMSRP (Acc / F1)\nMR\nCR\nSUBJ\nMPQA\nTREC\nAverage\nUnordered Sentences:\n(Toronto Books)\nSent2Vec uni.\n72.2 / 80.3\n75.1\n80.2\n90.6\n86.3\n83.8\n81.4\nSent2Vec uni. + bi.\n72.5 / 80.8\n75.8\n80.3\n91.2\n85.9\n86.4\n82.0\nSent2Vec uni. + bi. L1-reg\n71.6 / 80.1\n76.1\n80.9\n91.1\n86.1\n86.8\n82.1\nUnordered sentences: Wikipedia\n(69 million sentences; 1.7 B words)\nSent2Vec uni.\n71.8 / 80.2\n77.3\n80.3\n92.0\n87.4\n85.4\n82.4\nSent2Vec uni. + bi.\n72.4 / 80.8\n77.9\n80.9\n92.6\n86.9\n89.2\n83.3\nSent2Vec uni. + bi. L1-reg\n73.6 / 81.5\n78.1\n81.5\n92.8\n87.2\n87.4\n83.4\nUnordered sentences: Twitter\n(1.2 billion sentences; 19.7 B words)\nSent2Vec uni.\n71.5 / 80.0\n77.1\n81.3\n90.8\n87.3\n85.4\n82.2\nSent2Vec uni. + bi.\n72.4 / 80.6\n78.0\n82.1\n91.8\n86.7\n89.8\n83.5\nOther structured\nData Sources\nCaptionRep BOW\n73.6 / 81.9\n61.9\n69.3\n77.4\n70.8\n72.2\n70.9\nCaptionRep RNN\n72.6 / 81.1\n55.0\n64.9\n64.9\n71.0\n62.4\n65.1\nDictRep BOW\n73.7 / 81.6\n71.3\n75.6\n86.6\n82.5\n73.8\n77.3\nDictRep BOW+embs\n68.4 / 76.8\n76.7\n78.7\n90.7\n87.2\n81.0\n80.5\nDictRep RNN\n73.2 / 81.6\n67.8\n72.7\n81.4\n82.5\n75.8\n75.6\nDictRep RNN+embs.\n66.8 / 76.0\n72.5\n73.5\n85.6\n85.7\n72.0\n76.0\nTable 6:\nComparison of the performance of different Sent2Vec models with different semi-\nsupervised/supervised models on different downstream supervised evaluation tasks. An underline\nindicates the best performance for the dataset and Sent2Vec model performances are bold if they per-\nform as well or better than all other non-Sent2Vec models, including those presented in Table 1.\nSTS 2014\nSICK 2014\nAverage\nModel\nNews\nForum\nWordNet\nTwitter\nImages\nHeadlines\nTest + Train\nSent2Vec book corpus uni.\n.62/.67\n.49/.49\n.75/.72.\n.70/.75\n.78/.82\n.61/.63\n.61/.70\n.65/.68\nSent2Vec book corpus uni. + bi.\n.62/.67\n.51/.51\n.71/.68\n.70/.75\n.75/.79\n.59/.62\n.62/.70\n.65/.67\nSent2Vec book corpus uni. + bi. L1-reg\n.62/.68\n.51/.52\n.72/.70\n.69/.75\n.76/.81\n.60/.63\n.62/.71\n.66/.68\nSent2Vec wiki uni.\n.66/.71\n.47/.47\n.70/.68\n.68/.72\n.76/.79\n.63/.67\n.64/.71\n.65/.68\nSent2Vec wiki uni. + bi.\n.68/.74\n.50/.50\n.66/.64\n.67/.72\n.75/.79\n.62/.67\n.63/.71\n.65/.68\nSent2Vec wiki uni. + bi. L1-reg\n.69/.75\n.52/.52\n.72/.69\n.67/.72\n.76/.80\n.61/.66\n.63/.72\n.66/.69\nSent2Vec twitter uni.\n.67/.74\n.52/.53\n.75/.72\n.72/.78\n.77/.81\n.64/.68\n.62/.71\n.67/.71\nSent2Vec twitter uni. + bi.\n.68/.74\n.54/.54\n.72/.69\n.70/.77\n.76/.79\n.62/.67\n.63/.72\n.66/.70\nCaptionRep BOW\n.26/.26\n.29/.22\n.50/.35\n.37/.31\n.78/.81\n.39/.36\n.45/.44\n.54/.62\nCaptionRep RNN\n.05/.05\n.13/.09\n.40/.33\n.36/.30\n.76/.82\n.30/.28\n.36/.35\n.51/.59\nDictRep BOW\n.62/.67\n.42/.40\n.81/.81\n.62/.66\n.66/.68\n.53/.58\n.61/.63\n.58/.66\nDictRep BOW + embs.\n.65/.72\n.49/.47\n.85/.86\n.67/.72\n.71/.74\n.57/.61\n.61/.70\n.62/.70\nDictRep RNN\n.40/.46\n.26/.23\n.78/.78\n.42/.42\n.56/.56\n.38/.40\n.47/.49\n.49/.55\nDictRep RNN + embs.\n.51/.60\n.29/.27\n.80/.81\n.44/.47\n.65/.70\n.42/.46\n.52/.56\n.49/.59\nTable 7: Unsupervised Evaluation: Comparison of the performance of different Sent2Vec models with\nsemi-supervised/supervised models on Spearman/Pearson correlation measures. An underline indicates\nthe best performance for the dataset and Sent2Vec model performances are bold if they perform as well\nor better than all other non-Sent2Vec models, including those presented in Table 2.\nD\nDataset Description\nSTS 2014\nSICK 2014\nWikipedia\nDataset\nTwitter\nDataset\nBook Corpus\nDataset\nSentence Length\nNews\nForum\nWordNet\nTwitter\nImages\nHeadlines\nTest + Train\nAverage\n17.23\n10.12\n8.85\n11.64\n10.17\n7.82\n9.67\n25.25\n16.31\n13.32\nStandard Deviation\n8.66\n3.30\n3.10\n5.28\n2.77\n2.21\n3.75\n12.56\n7.22\n8.94\nTable 8: Average sentence lengths for the datasets used in the comparison.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.IR",
    "I.2.7"
  ],
  "published": "2017-03-07",
  "updated": "2018-12-28"
}