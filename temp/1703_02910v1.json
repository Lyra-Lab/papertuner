{
  "id": "http://arxiv.org/abs/1703.02910v1",
  "title": "Deep Bayesian Active Learning with Image Data",
  "authors": [
    "Yarin Gal",
    "Riashat Islam",
    "Zoubin Ghahramani"
  ],
  "abstract": "Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task).",
  "text": "Deep Bayesian Active Learning with Image Data\nYarin Gal 1 2 Riashat Islam 1 Zoubin Ghahramani 1\nAbstract\nEven though active learning forms an important\npillar of machine learning, deep learning tools\nare not prevalent within it. Deep learning poses\nseveral difﬁculties when used in an active learn-\ning setting. First, active learning (AL) methods\ngenerally rely on being able to learn and update\nmodels from small amounts of data. Recent ad-\nvances in deep learning, on the other hand, are no-\ntorious for their dependence on large amounts of\ndata. Second, many AL acquisition functions rely\non model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this\npaper we combine recent advances in Bayesian\ndeep learning into the active learning framework\nin a practical way. We develop an active learn-\ning framework for high dimensional data, a task\nwhich has been extremely challenging so far, with\nvery sparse existing literature. Taking advantage\nof specialised models such as Bayesian convolu-\ntional neural networks, we demonstrate our active\nlearning techniques with image data, obtaining a\nsigniﬁcant improvement on existing active learn-\ning approaches. We demonstrate this on both the\nMNIST dataset, as well as for skin cancer diagno-\nsis from lesion images (ISIC2016 task).\n1. Introduction\nA big challenge in many machine learning applications\nis obtaining labelled data. This can be a long, laborious,\nand costly process, often making the deployment of ML\nsystems uneconomical. A framework where a system could\nlearn from small amounts of data, and choose by itself what\ndata it would like the user to label, would make machine\nlearning much more widely applicable. Such frameworks\nfor learning are referred to as active learning (Cohn et al.,\n1996) (also known as “experiment design” in the statistics\nliterature), and have been used successfully in ﬁelds such as\nmedical diagnosis, microbiology, and manufacturing (Tong,\n2001). In active learning, a model is trained on a small\n1University of Cambridge, UK 2The Alan Turing Institute, UK.\nCorrespondence to: Yarin Gal <yg279@cam.ac.uk>.\namount of data (the initial training set), and an acquisition\nfunction (often based on the model’s uncertainty) decides\nwhich data points to ask an external oracle for a label. The\nacquisition function selects one or more points from a pool\nof unlabelled data points, with the pool points lying outside\nof the training set. An oracle (often a human expert) labels\nthe selected data points, these are added to the training set,\nand a new model is trained on the updated training set. This\nprocess is then repeated, with the training set increasing\nin size over time. The advantage of such systems is that\nthey often result in dramatic reductions in the amount of\nlabelling required to train an ML system (and therefore cost\nand time).\nEven though existing techniques for active learning have\nproven themselves useful in a variety of tasks, a major re-\nmaining challenge in active learning is its lack of scalability\nto high-dimensional data (Tong, 2001). This data appears of-\nten in image form, with a physician classifying MRI scans to\ndiagnose Alzheimer’s for example (Marcus et al., 2010), or\nan expert clinician diagnosing skin cancer from dermoscopic\nlesion images. To perform active learning, a model has to\nbe able to learn from small amounts of data and represent\nits uncertainty over unseen data. This severely restricts the\nclass of models that can be used within the active learning\nframework. As a result most approaches to active learning\nhave focused on low dimensional problems (Tong, 2001;\nHernandez-Lobato & Adams, 2015), with only a handful\nof exceptions (Zhu et al., 2003; Holub et al., 2008; Joshi\net al., 2009) relying on kernel or graph-based approaches to\nhandle high-dimensional data.\nIn recent years, with the increased availability of data in\nsome domains, attention within the machine learning com-\nmunity has shifted from small data problems to big data\nproblems (Sundermeyer et al., 2012; Krizhevsky et al.,\n2012; Kalchbrenner & Blunsom, 2013; Sutskever et al.,\n2014). And with the increased interest in big data problems,\nnew tools were developed and existing tools were reﬁned\nfor handling high dimensional data within such regimes.\nDeep learning, and convolutional neural networks (CNNs)\n(Rumelhart et al., 1985; LeCun et al., 1989) in particular, are\nan example of such tools. Originally developed in 1989 to\nparse handwritten zip codes, these tools have ﬂourished and\nwere adapted to a point where a CNN is able to beat a hu-\nman on object recognition tasks (given enough training data)\narXiv:1703.02910v1  [cs.LG]  8 Mar 2017\nDeep Bayesian Active Learning with Image Data\n(He et al., 2015). New techniques such as dropout (Hinton\net al., 2012; Srivastava et al., 2014) are used extensively to\nregularise these huge models, which often contain millions\nof parameters (Jozefowicz et al., 2016). But even though ac-\ntive learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learn-\ning poses several difﬁculties when used in an active learning\nsetting. First, we have to be able to handle small amounts of\ndata. Recent advances in deep learning, on the other hand,\nare notorious for their dependence on large amounts of data\n(Krizhevsky et al., 2012). Second, many AL acquisition\nfunctions rely on model uncertainty. But in deep learning\nwe rarely represent such model uncertainty.\nRelying on Bayesian approaches to deep learning, in this\npaper we combine recent advances in Bayesian deep learn-\ning into the active learning framework in a practical way.\nWe develop an active learning framework for high dimen-\nsional data, a task which has been extremely challenging\nso far with very sparse existing literature from the past 15\nyears (Zhu et al., 2003; Li & Guo, 2013; Holub et al., 2008;\nJoshi et al., 2009). Taking advantage of specialised models\nsuch as Bayesian convolutional neural networks (BCNNs)\n(Gal & Ghahramani, 2016a;b), we demonstrate our active\nlearning techniques with image data. Using a small model,\nour system is able to achieve 5% test error on MNIST with\nonly 295 labelled images without relying on unlabelled data\n(in comparison, 835 labelled images are needed to achieve\n5% test error using random sampling – requiring an expert\nto label more than twice as many images to achieve the\nsame accuracy), and achieves 1.64% test error with 1000\nlabelled images. This is in comparison to 2.40% test er-\nror of DGN (Kingma et al., 2014) or 1.53% test error of\nthe Ladder Network Γ-model (Rasmus et al., 2015), both\nsemi-supervised learning techniques which additionally use\nthe entire unlabelled training set. Finally, we study a real-\nworld application by diagnosing melanoma (skin cancer)\nfrom a small number of lesion images by ﬁne-tuning the\nVGG16 convolutional neural network (Simonyan & Zisser-\nman, 2015) on the ISIC 2016 dataset (Gutman et al., 2016).\n2. Related Research\nPast attempts at active learning of image data have concen-\ntrated on kernel based methods. Using ideas from previous\nresearch in active learning of low dimensional data (Tong,\n2001), Joshi et al. (2009) used “margin-based uncertainty”\nand extracted probabilistic outputs from support vector ma-\nchines (SVM) (Cortes & Vapnik, 1995). They used linear,\npolynomial, and Radial Basis Function (RBF) kernels on\nthe raw images, picking the kernel that gave best classiﬁca-\ntion accuracy. Analogously to SVM approaches, Li & Guo\n(2013) used Gaussian processes (GPs) with RBF kernels\nto get model uncertainty. However Li & Guo (2013) fed\nlow dimensional features (such as SIFT features) to their\nRBF kernel. Lastly, making use of unlabelled data as well,\nZhu et al. (2003) acquire points using a Gaussian random\nﬁeld model, evaluating an RBF kernel over raw images. We\ncompare to this last technique and explain it in more detail\nbelow.\nOther related work includes semi-supervised learning of im-\nage data (Weston et al., 2012; Kingma et al., 2014; Rasmus\net al., 2015). In semi-supervised learning a model is given a\nﬁxed set of labelled data, and a ﬁxed set of unlabelled data.\nThe model can use the unlabelled data to learn about the\ndistribution of the inputs, in the hopes that this information\nwill aid in learning from the small labelled set as well. Al-\nthough the learning paradigm is fairly different from active\nlearning, this research forms the closest modern literature\nto active learning of image data. We will compare to these\ntechniques below as well, in section 5.4.\n3. Bayesian Convolutional Neural Networks\nIn this paper we concentrate on high dimensional image\ndata, and need a model able to represent prediction uncer-\ntainty on such data. Existing approaches such as (Zhu et al.,\n2003; Li & Guo, 2013; Joshi et al., 2009) rely on kernel\nmethods, and feed image pairs through linear, polynomial,\nand RBF kernels to capture image similarity as an input to\nan SVM for example. In contrast, we rely on specialised\nmodels for image data, and in particular on convolutional\nneural networks (CNNs) (Rumelhart et al., 1985; LeCun\net al., 1989). Unlike the kernels above, which cannot cap-\nture spatial information in the input image, CNNs are de-\nsigned to use this spatial information, and have been used\nsuccessfully to achieve state-of-the-art results (Krizhevsky\net al., 2012). To perform active learning with image data\nwe make use of the Bayesian equivalent of CNNs, proposed\nin (Gal & Ghahramani, 2016a)1. These Bayesian CNNs are\nCNNs with prior probability distributions placed over a set\nof model parameters ω = {W1, ..., WL}:\nω ∼p(ω),\nwith for example a standard Gaussian prior p(ω). We further\ndeﬁne a likelihood model\np(y = c|x, ω) = softmax(f ω(x))\nfor the case of classiﬁcation, or a Gaussian likelihood for\nthe case of regression, with f ω(x) model output (with pa-\nrameters ω).\nTo perform approximate inference in the Bayesian CNN\nmodel we make use of stochastic regularisation techniques\nsuch as dropout (Hinton et al., 2012; Srivastava et al., 2014),\noriginally used to regularise these models. As shown in\n(Gal & Ghahramani, 2016b; Gal, 2016) dropout and various\n1As far as we are aware, there are no other tools in current\nliterature that offer model uncertainty in specialised models for\nimage data, which perform as well as CNNs.\nDeep Bayesian Active Learning with Image Data\nother stochastic regularisation techniques can be used to\nperform practical approximate inference in complex deep\nmodels. Inference is done by training a model with dropout\nbefore every weight layer, and by performing dropout at\ntest time as well to sample from the approximate posterior\n(stochastic forward passes, referred to as MC dropout).\nMore formally, this approach is equivalent to performing\napproximate variational inference where we ﬁnd a distri-\nbution q∗\nθ(ω) in a tractable family which minimises the\nKullback-Leibler (KL) divergence to the true model poste-\nrior p(ω|Dtrain) given a training set Dtrain. Dropout can be\ninterpreted as a variational Bayesian approximation, where\nthe approximating distribution is a mixture of two Gaussians\nwith small variances and the mean of one of the Gaussians\nis ﬁxed at zero. The uncertainty in the weights induces pre-\ndiction uncertainty by marginalising over the approximate\nposterior using Monte Carlo integration:\np(y = c|x, Dtrain) =\nZ\np(y = c|x, ω)p(ω|Dtrain)dω\n≈\nZ\np(y = c|x, ω)q∗\nθ(ω)dω\n≈1\nT\nT\nX\nt=1\np(y = c|x, bωt)\nwith bωt ∼q∗\nθ(ω), where qθ(ω) is the Dropout distribution\n(Gal, 2016).\nBayesian CNNs work well with small amounts of data (Gal\n& Ghahramani, 2016a), and possess uncertainty information\nthat can be used with existing acquisition functions (Gal,\n2016). Such acquisition functions for the case of classiﬁca-\ntion are discussed next.\n4. Acquisition Functions and their\nApproximations\nGiven a model M, pool data Dpool, and inputs x ∈Dpool,\nan acquisition function a(x, M) is a function of x that the\nAL system uses to decide where to query next:\nx∗= argmaxx∈Dpoola(x, M).\nWe next explore various acquisition functions appropriate\nfor our image data setting, and develop tractable approxi-\nmations for us to use with our Bayesian CNNs. In tasks\ninvolving regression we often use the predictive variance or\na quantity derived from this for our acquisition function (al-\nthough we still need to be careful to query from informative\nareas rather than querying noise). For example, we might\nlook for images with high predictive variance and choose\nthose to ask an expert to label – in the hope that these will\ndecrease model uncertainty. However, many tasks involving\nimage data are often phrased as classiﬁcation problems. For\nclassiﬁcation, several acquisition functions are available:\n1. Choose pool points that maximise the predictive en-\ntropy (Max Entropy, (Shannon, 1948))\nH[y|x, Dtrain] :=\n−\nX\nc\np(y = c|x, Dtrain) log p(y = c|x, Dtrain).\n2. Choose pool points that are expected to maximise the\ninformation gained about the model parameters, i.e.\nmaximise the mutual information between predictions\nand model posterior (BALD, (Houlsby et al., 2011))\nI[y, ω|x, Dtrain] = H[y|x, Dtrain]−Ep(ω|Dtrain)\n\u0002\nH[y|x, ω]\n\u0003\nwith ω the model parameters (here H[y|x, ω] is the\nentropy of y given model weights ω). Points that max-\nimise this acquisition function are points on which the\nmodel is uncertain on average, but there exist model\nparameters that produce disagreeing predictions with\nhigh certainty. This is equivalent to points with high\nvariance in the input to the softmax layer (the logits)\n– thus each stochastic forward pass through the model\nwould have the highest probability assigned to a differ-\nent class.\n3. Maximise the Variation Ratios (Freeman, 1965)\nvariation-ratio[x] := 1 −max\ny\np(y|x, Dtrain)\nLike Max Entropy, Variation Ratios measures lack of\nconﬁdence.\n4. Maximise mean standard deviation (Mean STD)\n(Kampffmeyer et al., 2016; Kendall et al., 2015)\nσc =\nq\nEq(ω)[p(y = c|x, ω)2] −Eq(ω)[p(y = c|x, ω)]2\nσ(x) = 1\nC\nX\nc\nσc\naveraged over all c classes x can take. Compared to the\nabove acquisition functions, this is more of an ad-hoc\ntechnique used in recent literature.\n5. Random acquisition (baseline): a(x) = unif() with\nunif() a function returning a draw from a uniform dis-\ntribution over the interval [0, 1]. Using this acquisition\nfunction is equivalent to choosing a point uniformly at\nrandom from the pool.\nThese acquisition functions and their properties are dis-\ncussed in more detail in (Gal, 2016, pp. 48–52).\nWe can approximate each of these acquisition functions\nusing our approximate distribution q∗\nθ(ω). For BALD, for\nexample, we can write the acquisition function as follows:\nI[y, ω|x, Dtrain] := H[y|x, Dtrain] −Ep(ω|Dtrain)\n\u0002\nH[y|x, ω]\n\u0003\n= −\nX\nc\np(y = c|x, Dtrain) log p(y = c|x, Dtrain)\n+ Ep(ω|Dtrain)\n\u0014 X\nc\np(y = c|x, ω) log p(y = c|x, ω)\n\u0015\n,\nDeep Bayesian Active Learning with Image Data\nwith c the possible classes y can take. I[y, ω|x, Dtrain] can\nbe approximated in our setting using the identity p(y =\nc|x, Dtrain) =\nR\np(y = c|x, ω)p(ω|Dtrain)dω:\nI[y, ω|x, Dtrain] =\n−\nX\nc\nZ\np(y = c|x, ω)p(ω|Dtrain)dω\n· log\nZ\np(y = c|x, ω)p(ω|Dtrain)dω\n+ Ep(ω|Dtrain)\n\u0014 X\nc\np(y = c|x, ω) log p(y = c|x, ω)\n\u0015\n.\nSwapping the posterior p(ω|Dtrain) with our approximate\nposterior q∗\nθ(ω), and through MC sampling, we then have:\n≈−\nX\nc\nZ\np(y = c|x, ω)q∗\nθ(ω)dω\n· log\nZ\np(y = c|x, ω)q∗\nθ(ω)dω\n+ Eq∗\nθ(ω)\n\u0014 X\nc\np(y = c|x, ω) log p(y = c|x, ω)\n\u0015\n≈−\nX\nc\n\u0012 1\nT\nX\nt\nbpt\nc\n\u0013\nlog\n\u0012 1\nT\nX\nt\nbpt\nc\n\u0013\n+ 1\nT\nX\nc,t\nbpt\nc log bpt\nc =: bI[y, ω|x, Dtrain]\ndeﬁning our approximation, with bpt\nc the probability of input\nx with model parameters bωt ∼q∗\nθ(ω) to take class c:\nbpt = [bpt\n1, ..., bpt\nC] = softmax(f bωt(x)).\nWe then have\nbI[y, ω|x, Dtrain] −−−−→\nT →∞H[y|x, q∗\nθ] −Eq∗\nθ(ω)\n\u0002\nH[y|x, ω]\n\u0003\n≈I[y, ω|x, Dtrain],\nresulting in a computationally tractable estimator approxi-\nmating the BALD acquisition function. The other acquisi-\ntion functions can be approximated similarly.\nIn the next section we will experiment with these acquisi-\ntion functions and assess them empirically. These will be\ncompared to the baseline acquisition function which uni-\nformly acquires new data points from the pool set at random,\nand to various other techniques for active learning of image\ndata and semi-supervised learning. This is followed by a\nreal-world case study using cancer diagnosis.\n5. Active Learning with Bayesian\nConvolutional Neural Networks\nWe study the proposed technique for active learning of im-\nage data. We compare the various acquisition functions\nrelying on Bayesian CNN uncertainty with a simple image\nclassiﬁcation benchmark. We then study the importance of\nmodel uncertainty by evaluating the same acquisition func-\ntions with a deterministic CNN. This is followed by a com-\nparison to a current technique for active learning with image\ndata, which relies on SVMs. We follow with a comparison to\nthe closest modern models to our active learning with image\ndata – semi-supervised techniques with image data. These\nsemi-supervised techniques have access to much more data\n(the unlabelled data) than our active learning models, yet\nwe still perform in comparable terms to them. Finally, we\ndemonstrate the proposed methodology with a real world\napplication of skin cancer diagnosis from a small number of\nlesion images, relying on ﬁne-tuning of a large CNN model.\n5.1. Comparison of various acquisition functions\nWe next study all acquisition functions above with our\nBayesian CNN trained on the MNIST dataset (LeCun\n& Cortes, 1998).\nAll acquisition functions are as-\nsessed with the same model structure: convolution-relu-\nconvolution-relu-max pooling-dropout-dense-relu-dropout-\ndense-softmax, with 32 convolution kernels, 4x4 kernel size,\n2x2 pooling, dense layer with 128 units, and dropout proba-\nbilities 0.25 and 0.5 (following the example Keras MNIST\nCNN implementation (fchollet, 2015)).\nAll models are trained on the MNIST dataset with a (random\nbut balanced) initial training set of 20 data points, and a\nvalidation set of 100 points on which we optimise the weight\ndecay (this is a realistic validation set size, in comparison\nto the standard validation set size of 5K used in similar\napplications such as semi-supervised learning on MNIST).\nWe further use the standard test set of 10K points, and the\nrest of the points are used as a pool set. The test error of\neach model and each acquisition function was assessed after\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nBALD\nVar Ratios\nMax Entropy\nMean STD\nRandom\nFigure 1. MNIST test accuracy as a function of number of ac-\nquired images from the pool set (up to 1000 images, using valida-\ntion set size 100, and averaged over 3 repetitions). Four acquisition\nfunctions (BALD, Variation Ratios, Max Entropy, and Mean STD)\nare evaluated and compared to a Random acquisition function.\nDeep Bayesian Active Learning with Image Data\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nBALD\nDeterministic BALD\n(a) BALD\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nVar Ratios\nDeterministic Var Ratios\n(b) Var Ratios\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nMax Entropy\nDeterministic Max Entropy\n(c) Max Entropy\nFigure 2. Test accuracy as a function of number of acquired images for various acquisition functions, using both a Bayesian CNN (red)\nand a deterministic CNN (blue).\neach acquisition, using the dropout approximation at test\ntime. To decide what data points to acquire though we used\nMC dropout following the derivations above. We repeated\nthe acquisition process 100 times, each time acquiring the\n10 points that maximised the acquisition function over the\npool set. Each experiment was repeated three times and\nthe results averaged (the standard deviation for the three\nrepetitions is shown below)2.\nWe compared the acquisition functions BALD, Variation\nRatios, Max Entropy, Mean STD, and the baseline Random.\nWe found Random and Mean STD to under-perform com-\npared to BALD, Variation Ratios, and Max Entropy (ﬁgure\n1). The Variation Ratios acquisition function seems to obtain\nslightly better accuracy faster than BALD and Max Entropy.\nIt is interesting that Mean STD seems to perform similarly\nto Random – which samples points at random from the pool\nset.\nLastly, in table 1 we give the number of acquisition steps\nneeded to get to test errors of 5% and 10%. As can be seen,\nBALD, Variation Ratios, and Max Entropy attain a small\ntest error with much fewer acquisitions than Mean STD and\nRandom. This table demonstrates the importance of data\nefﬁciency – an expert using the Variation Ratios model for\nexample would have to label less than half the number of\nimages she would have had to label had she acquired new\nimages at random.\n% error BALD Var Ratios Max Ent Mean STD Random\n10%\n145\n120\n165\n230\n255\n5%\n335\n295\n355\n695\n835\nTable 1. Number of acquired images to get to model error of % on\nMNIST.\n2The\ncode\nfor\nthese\nexperiments\nis\navailable\nat\nhttp://mlg.eng.cam.ac.uk/yarin/publications.\nhtml#Gal2016Active.\n5.2. Importance of model uncertainty\nWe assess the importance of model uncertainty in our\nBayesian CNN by evaluating three of the acquisition func-\ntions (BALD, Variation Ratios, and Max Entropy) with a\ndeterministic CNN. Much like the Bayesian CNN, the de-\nterministic CNN produces a probability vector which can\nbe used with the acquisition functions of §4 (formally, by\nsetting q∗\nθ(ω) = δ(ω −θ) to be a point mass at the location\nof the model parameters θ). Such deterministic models can\ncapture aleatoric uncertainty – the noise in the data – but\ncannot capture epistemic uncertainty – the uncertainty over\nthe parameters of the CNN, which we try to minimise dur-\ning active learning. The models in this experiment still use\ndropout, but for regularisation only (i.e. we do not perform\nMC dropout at test time).\nA comparison of the Bayesian models to the deterministic\nmodels for the BALD, Variation Ratios, and Max Entropy\nacquisition functions is given in ﬁg. 2. The Bayesian mod-\nels, propagating uncertainty throughout the model, attain\nhigher accuracy early on, and converge to a higher accuracy\noverall. This demonstrates that the uncertainty propagated\nthroughout the Bayesian models has a signiﬁcant effect on\nthe models’ measure of their conﬁdence.\n5.3. Comparison to current active learning techniques\nwith image data\nWe next compare to a method in the sparse existing literature\nof active learning with image data, concentrating on (Zhu\net al., 2003) which relies on a kernel method and further\nleverages the unlabelled images (which will be discussed in\nmore detail in the next section). Zhu et al. (2003) evaluate\nan RBF kernel over the raw images to get a similarity graph\nwhich can be used to share information about the unlabelled\ndata. Active learning is then performed by greedily selecting\nunlabelled images to be labelled, such that an estimate to\nthe expected classiﬁcation error is minimised. This will be\nreferred to as MBR.\nMBR was formulated for the binary classiﬁcation case,\nDeep Bayesian Active Learning with Image Data\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n94\n95\n96\n97\n98\n99\n100\nBALD\nVar Ratios\nMax Entropy\nMBR\nRandom\nFigure 3. MNIST test accuracy (two digit classiﬁcation) as a function of number acquired images, compared to a current\ntechnique for active learning of image data: MBR (Zhu et al., 2003).\nhence we compared MBR to the acquisition functions\nBALD, Variation Ratios, Max Entropy, and Random on\na binary classiﬁcation task (two digits from the MNIST\ndataset). Classiﬁcation accuracy is shown in ﬁg. 3. Note\nthat even a random acquisition function, when coupled with\na CNN (a specialised model for image data) outperforms\nMBR which relies on an RBF kernel. We further experi-\nmented with a CNN version for MBR where we replaced\nthe RBF kernel with a CNN. It is interesting to note that this\ndid not give improved results.\n5.4. Comparison to semi-supervised learning\nWe continue with a comparison to the closest models\n(in modern literature) to our active learning with image\ndata: semi-supervised learning with image data. In semi-\nsupervised learning a model is given a ﬁxed set of labelled\ndata, and a ﬁxed set of unlabelled data. The model can use\nthe unlabelled dataset to learn about the distribution of the\ninputs, in the hopes that this information will aid in learning\nthe mapping to the outputs as well. Several semi-supervised\nmodels for image data have been suggested in recent years\n(Weston et al., 2012; Kingma et al., 2014; Rasmus et al.,\n2015), models which have set benchmarks on MNIST given\na small number of labelled images (1000 random images).\nThese models make further use of a (very) large unlabelled\nset of 49K images, and a large validation set of 5K-10K\nlabelled images to tune model hyper-parameters and model\nstructure (Rasmus et al., 2015). These models have access\nto much more data than our active learning models, but we\nstill compare to them as they are the most relevant models\nin the ﬁeld given the constraint of small amounts of labelled\ndata.\nTest error for our active learning models with various ac-\nquisition functions (after the acquisition of 1000 training\npoints), as well as the semi-supervised models, is given in\ntable 2. In this experiment, to be comparable to the other\ntechniques, we use a validation set of 5K points. Our model\nattains similar performance to that of the semi-supervised\nmodels (although note that we use a fairly small model\ncompared to (Rasmus et al., 2015) for example). Rasmus\net al. (2015)’s ladder network (full) attains error 0.84% with\n1000 labelled images and 59,000 unlabelled images. How-\never, (Rasmus et al., 2015)’s Γ-model architecture is more\ndirectly comparable to ours. The Γ-model attains 1.53%\nerror, compared to 1.64% error of our Var Ratio acquisition\nfunction which relies on no additional unlabelled data.\nTechnique\nTest error\nSemi-supervised:\nSemi-sup. Embedding (Weston et al., 2012)\n5.73%\nTransductive SVM (Weston et al., 2012)\n5.38%\nMTC (Rifai et al., 2011)\n3.64%\nPseudo-label (Lee, 2013)\n3.46%\nAtlasRBF (Pitelis et al., 2014)\n3.68%\nDGN (Kingma et al., 2014)\n2.40%\nVirtual Adversarial (Miyato et al., 2015)\n1.32%\nLadder Network (Γ-model) (Rasmus et al., 2015) 1.53%\nLadder Network (full) (Rasmus et al., 2015)\n0.84%\nActive learning with\nvarious acquisitions:\nRandom\n4.66%\nBALD\n1.80%\nMax Entropy\n1.74%\nVar Ratios\n1.64%\nTable 2. Test error on MNIST with 1000 labelled training sam-\nples, compared to semi-supervised techniques. Active learning\nhas access to only the 1000 acquired images. Semi-supervised fur-\nther has access to the remaining images with no labels. Following\nexisting research we use a large validation set of size 5000.\nDeep Bayesian Active Learning with Image Data\nFigure 4. Skin cancer (melanoma) example lesions from the ISIC 2016 melanoma diagnosis dataset. The two lesions on the left are benign\n(non-cancerous), while the two lesions on the right are malignant (cancerous).\n5.5. Cancer diagnosis from lesion image data\nWe ﬁnish by assessing the proposed technique with a real\nworld test case. We experiment with melanoma (skin can-\ncer) diagnosis from dermoscopic lesion images. In this task\nwe are given image data of skin segments, of both malig-\nnant (cancerous) as well as benign (non-cancerous) lesions.\nOur task is to classify the images as malignant or benign\n(an example is shown in ﬁg. 4). The data used is the ISIC\nArchive (Gutman et al., 2016). This dataset was collected in\norder to provide a “large public repository of expertly anno-\ntated high quality skin images” to provide clinical support\nin the identiﬁcation of skin cancer, and to develop algo-\nrithms for skin cancer diagnosis. Speciﬁcally, we use the\ntraining data of the “ISBI 2016: Skin Lesion Analysis To-\nwards Melanoma Detection – Part 3B: Segmented Lesion\nClassiﬁcation” task. The data contains 900 dermoscopic\nlesion images in JPEG format with EXIF tags removed.\nMalignancy diagnosis for these lesions was obtained from\nexpert consensus and pathology report information. The\ndata contains lesion segmentation as well, which we did not\nuse.\nFor our model we replicate the model of (Agarwal et al.,\n2016). This model achieved second place in the “Part 3B:\nSegmented Lesion Classiﬁcation” task, with its code open-\nsourced. The model relies on data augmentation of the\npositive examples (ﬂipping the lesions vertically and hori-\nzontally), and ﬁne-tunes the VGG16 CNN model (Simonyan\n& Zisserman, 2015) (i.e. optimises a pre-trained model with\na small learning rate). The VGG16 model was pre-trained\non ImageNet (Deng et al., 2009). The top layer of the\nmodel (1000 logits) was removed and replaced with a 2\ndimensional output (for our classiﬁcation task of malig-\nnant/benign). Preceding the last layer are two fully con-\nnected layers of size 4096, each one followed by a dropout\nlayer with dropout probability 0.5. This architecture seems\nto provide good uncertainty estimates as observed before\n(Kendall et al., 2015; Gal & Ghahramani, 2016a).\nThe data is unbalanced, containing 727 negative (benign)\nexamples, and 173 positive (malignant) examples (20% pos-\nitive examples). Since the data is so small, to assess model\nperformance reliably we have to take a large balanced test\nset. We randomly partition the data, and set aside 100 neg-\native and 100 positive examples. All our experiments are\nperformed on two different random splits – since even a test\nset size of 200 gives very different accuracy with different\nrandom splits. Note that on each such random split we\nrepeat our experiments three times and average the results\nwith respect to the ﬁxed test set.\nWe experiment with active learning by following the fol-\nlowing procedure. We begin by creating an initial training\nset of 80 negative examples and 20 positive examples from\nour training data, as well as a pool set from the remaining\ndata. With each experiment repetition (out of the three ex-\nperiment repetitions w.r.t. the ﬁxed test split) the pool is\nshufﬂed anew. The positive examples in the current training\nset are augmented following the original training procedure,\nand a model is trained on the augmented training set for\n100 epochs until convergence. We use batch size 8 and\nweight decay set by (1 −p)l2/N, where N is the number\nof training points, p = 0.5 is the dropout probability, and\nthe length-scale squared l2 is set to 0.5. An acquisition\nfunction is then used to select the 100 most informative\nimages from the pool set. These points are removed from\nthe pool set and added to the (non-augmented) training set,\nwhere we use the original expert-provided labels for these\npoints. The process is repeated until all pool points have\nbeen exhausted, where at each acquisition step we reset the\nmodel to its original pre-trained weights (as we also did\nin the previous section experiments). This reset is done in\norder to avoid local optima, and to avoid confusing model\nperformance improvement with an improvement resulting\nfrom simply using longer (cumulative) optimisation time.\nAfter each acquisition the test performance of the model\nis logged using MC dropout with 20 samples. We further\nkeep track of the number of positive examples acquired\nafter each acquisition. Model performance is assessed using\narea-under-the-curve (AUC) as this seems to be the most\ninformative of all metrics used by Gutman et al. (2016). We\nexperimented with the average precision metric suggested\nby Gutman et al. (2016) as well, but managed to get results\nimproving over the competition winner by simply predicting\nall points as “benign”. This might be because of the data\nimbalance. AUC on the other hand takes into account all\npossible decision-thresholds possible to classify a malignant\nimage.\nWe assessed two acquisition functions: a uniform baseline,\nDeep Bayesian Active Learning with Image Data\n0\n1\n2\n3\n4\nAcquisition steps\n0.60\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\nAUC\nBALD\nuniform\n(a) AUC as a function of acquisition step, ﬁrst test split\n0\n1\n2\n3\n4\nAcquisition steps\n20\n30\n40\n50\n60\n70\n# positive examples acquired\nBALD\nuniform\n(b) # of positive examples as a function of acquisition\nstep, ﬁrst test split\n0\n1\n2\n3\n4\nAcquisition steps\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\nAUC\nBALD\nuniform\n(c) AUC as a function of acquisition step, second test\nsplit\n0\n1\n2\n3\n4\nAcquisition steps\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n# positive examples acquired\nBALD\nuniform\n(d) # of positive examples as a function of acquisition\nstep, second test split\nFigure 5. AUC (left) as well as the number of acquired positive examples (right) for both the BALD acquisition function as well as\nuniform acquisition function, on ISIC 2016 melanoma diagnosis dataset. Two random test splits are assessed (top and bottom), and on\neach test set the experiment was repeated three times with different random seeds (shown mean with standard error).\nand BALD. Even though Variation Ratios performs well on\nMNIST above, the function fails with the melanoma data\nsince most malignant images are given only a slight higher\nprobability of being malignant compared to the probability\nof benign images of being malignant. As a result all pool\npoints are given identical Variation Ratios acquisition value.\nExperiment results are given in ﬁg. 5, where results are\nreported on both test splits (top and bottom), and where\nwith each split the experiment is repeated three times and\nperformance results are averaged on that ﬁxed split. For\neach test split we report mean with standard error. AUC is\nreported for each split (left), and number of acquired posi-\ntive examples is reported as well (right) for each acquisition\nstep. BALD achieves better AUC faster than uniform, and\nacquires more positive examples at each acquisition step\nthan uniform (i.e. BALD ﬁnds positive examples as infor-\nmative and adds these to the training set, whereas uniform\nsimply selects positive examples from the pool set based on\ntheir frequency).\nNote how AUC range varies wildly between the two differ-\nent test splits, but how AUC is similar for both acquisition\nfunctions on each ﬁxed test set before the initial acquisition\n(when both uniform and BALD models are trained on the\nsame initial training set). This demonstrates the difﬁculties\nwith handling of small data: each test split gives radically\ndifferent results, and in this case even though each acqui-\nsition function experiment has a relatively small standard\nerror, averaging the AUC of the acquisition functions over\nthe different test splits would artiﬁcially increase the stan-\ndard error. Lastly, it is interesting to experiment with a\nmodel trained over the entire pool set, i.e. with the settings\nof the second place winner in the ISIC2016 task. For the\nﬁrst test split this model attains AUC 0.71 ± 0.003, whereas\nwith the second test split it attains AUC 0.75 ± 0.01. For\nboth test splits this AUC is worse than BALD’s converged\nAUC after 4 acquisition steps. This might be because BALD\navoided selecting noisy points – near-by images for which\nthere exist multiple noisy labels of different classes. Such\npoints have large aleatoric uncertainty – uncertainty which\ncannot be explained away – rather than large epistemic un-\ncertainty – the uncertainty which BALD captures in order\nto explain it away, i.e. reduce it.\n6. Future Research\nWe presented a new approach for active learning of im-\nage data, relying on recent advances at the intersection of\nDeep Bayesian Active Learning with Image Data\nBayesian modelling and deep learning, and demonstrated\na real-world application in medical diagnosis. We assessed\nthe performance of the techniques by resetting the models\nafter each acquisition, and training them again to conver-\ngence. This was done to isolate the effects of our acquisition\nfunctions, which came at a cost of prolonged training times\n(20 hours for each melanoma experiment for example). We\nshowed that even with this long running time, our technique\nstill reduces required expert labels, thus reduces costs for\nsuch a system. This running time can be reduced further by\nnot resetting the system – with the potential price of falling\ninto local optima. We leave this problem for future research.\nReferences\nAgarwal,\nMohit,\nDamaraju,\nNandita,\nand\nChaieb,\nSahbi.\nDl8803.\nhttps://github.com/\nNanditaDamaraju/DL8803, 2016.\nCohn, David A, Ghahramani, Zoubin, and Jordan, Michael I.\nActive learning with statistical models. Journal of artiﬁ-\ncial intelligence research, 1996.\nCortes, Corinna and Vapnik, Vladimir. Support-vector net-\nworks. Machine learning, 20(3):273–297, 1995.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,\nand Fei-Fei, Li. Imagenet: A large-scale hierarchical\nimage database. In Computer Vision and Pattern Recog-\nnition, 2009. CVPR 2009. IEEE Conference on, pp. 248–\n255. IEEE, 2009.\nfchollet. Keras. https://github.com/fchollet/\nkeras, 2015.\nFreeman, Linton G. Elementary applied statistics, 1965.\nGal, Yarin. Uncertainty in Deep Learning. PhD thesis,\nUniversity of Cambridge, 2016.\nGal, Yarin and Ghahramani, Zoubin. Bayesian convolu-\ntional neural networks with Bernoulli approximate varia-\ntional inference. ICLR workshop track, 2016a.\nGal, Yarin and Ghahramani, Zoubin. Dropout as a Bayesian\napproximation: Representing model uncertainty in deep\nlearning. ICML, 2016b.\nGutman, David, Codella, Noel CF, Celebi, Emre, Helba,\nBrian, Marchetti, Michael, Mishra, Nabin, and Halpern,\nAllan.\nSkin lesion analysis toward melanoma detec-\ntion: A challenge at the international symposium on\nbiomedical imaging (ISBI) 2016, hosted by the interna-\ntional skin imaging collaboration (ISIC). arXiv preprint\narXiv:1605.01397, 2016.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Delving deep into rectiﬁers: Surpassing human-\nlevel performance on imagenet classiﬁcation. In Proceed-\nings of the IEEE International Conference on Computer\nVision, pp. 1026–1034, 2015.\nHernandez-Lobato, Jose Miguel and Adams, Ryan. Proba-\nbilistic backpropagation for scalable learning of Bayesian\nneural networks. In Proceedings of The 32nd Interna-\ntional Conference on Machine Learning, pp. 1861–1869,\n2015.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan R. Improving\nneural networks by preventing co-adaptation of feature\ndetectors. arXiv preprint arXiv:1207.0580, 2012.\nHolub, Alex, Perona, Pietro, and Burl, Michael C. Entropy-\nbased active learning for object recognition. In Com-\nputer Vision and Pattern Recognition Workshops, 2008.\nCVPRW’08. IEEE Computer Society Conference on, pp.\n1–8. IEEE, 2008.\nHoulsby, Neil, Husz´ar, Ferenc, Ghahramani, Zoubin, and\nLengyel, M´at´e. Bayesian active learning for classiﬁcation\nand preference learning. arXiv preprint arXiv:1112.5745,\n2011.\nJoshi, Ajay J, Porikli, Fatih, and Papanikolopoulos, Niko-\nlaos. Multi-class active learning for image classiﬁcation.\nIn Computer Vision and Pattern Recognition, 2009. CVPR\n2009. IEEE Conference on, pp. 2372–2379. IEEE, 2009.\nJozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer,\nNoam, and Wu, Yonghui. Exploring the limits of lan-\nguage modeling. arXiv preprint arXiv:1602.02410, 2016.\nKalchbrenner, Nal and Blunsom, Phil. Recurrent continuous\ntranslation models. In EMNLP, 2013.\nKampffmeyer, Michael, Salberg, Arnt-Borre, and Jenssen,\nRobert.\nSemantic segmentation of small objects and\nmodeling of uncertainty in urban remote sensing images\nusing deep convolutional neural networks. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR) Workshops, June 2016.\nKendall, Alex, Badrinarayanan, Vijay, and Cipolla, Roberto.\nBayesian segnet: Model uncertainty in deep convolu-\ntional encoder-decoder architectures for scene understand-\ning. arXiv preprint arXiv:1511.02680, 2015.\nKingma,\nDiederik\nP,\nMohamed,\nShakir,\nRezende,\nDanilo Jimenez, and Welling, Max. Semi-supervised\nlearning with deep generative models. In Advances in\nNeural Information Processing Systems, pp. 3581–3589,\n2014.\nDeep Bayesian Active Learning with Image Data\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems, pp. 1097–1105, 2012.\nLeCun, Yann and Cortes, Corinna. The MNIST database of\nhandwritten digits, 1998.\nLeCun, Yann, Boser, Bernhard, Denker, John S, Henderson,\nDonnie, Howard, Richard E, Hubbard, Wayne, and Jackel,\nLawrence D. Backpropagation applied to handwritten zip\ncode recognition. Neural Computation, 1(4):541–551,\n1989.\nLee, Dong-Hyun. Pseudo-label: The simple and efﬁcient\nsemi-supervised learning method for deep neural net-\nworks. In Workshop on Challenges in Representation\nLearning, 2013.\nLi, Xin and Guo, Yuhong. Adaptive active learning for\nimage classiﬁcation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pp.\n859–866, 2013.\nMarcus, Daniel S, Fotenos, Anthony F, Csernansky, John G,\nMorris, John C, and Buckner, Randy L. Open access\nseries of imaging studies: longitudinal mri data in nonde-\nmented and demented older adults. Journal of cognitive\nneuroscience, 22(12):2677–2684, 2010.\nMiyato, Takeru, Maeda, Shin-ichi, Koyama, Masanori,\nNakae, Ken, and Ishii, Shin.\nDistributional smooth-\ning by virtual adversarial examples.\narXiv preprint\narXiv:1507.00677, 2015.\nPitelis, Nikolaos, Russell, Chris, and Agapito, Lourdes.\nSemi-supervised learning using an unsupervised atlas.\nIn Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases, pp. 565–580.\nSpringer, 2014.\nRasmus, Antti, Berglund, Mathias, Honkala, Mikko,\nValpola, Harri, and Raiko, Tapani. Semi-supervised learn-\ning with ladder networks. In Advances in Neural Infor-\nmation Processing Systems, pp. 3546–3554, 2015.\nRifai, Salah, Dauphin, Yann N, Vincent, Pascal, Bengio,\nYoshua, and Muller, Xavier. The manifold tangent clas-\nsiﬁer. In Advances in Neural Information Processing\nSystems, pp. 2294–2302, 2011.\nRumelhart, David E, Hinton, Geoffrey E, and Williams,\nRonald J. Learning internal representations by error prop-\nagation. Technical report, DTIC Document, 1985.\nShannon, Claude Elwood. A mathematical theory of com-\nmunication. Bell System Technical Journal, 27(3):379–\n423, 1948.\nSimonyan, K. and Zisserman, A. Very deep convolutional\nnetworks for large-scale image recognition. In Interna-\ntional Conference on Learning Representations, 2015.\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\nA simple way to prevent neural networks from overﬁtting.\nJMLR, 2014.\nSundermeyer, Martin, Schl¨uter, Ralf, and Ney, Hermann.\nLSTM neural networks for language modeling. In IN-\nTERSPEECH, 2012.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence\nto sequence learning with neural networks. In NIPS,\n2014.\nTong, Simon. Active Learning: Theory and Applications.\nPhD thesis, 2001. AAI3028187.\nWeston, Jason, Ratle, Fr´ed´eric, Mobahi, Hossein, and Col-\nlobert, Ronan. Deep learning via semi-supervised em-\nbedding. In Neural Networks: Tricks of the Trade, pp.\n639–655. Springer, 2012.\nZhu, X, Lafferty, J, and Ghahramani, Z. Combining active\nlearning and semi-supervised learning using Gaussian\nﬁelds and harmonic functions. In Proceedings of the\nICML-2003 Workshop on The Continuum from Labeled\nto Unlabeled Data, pp. 58–65. ICML, 2003.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2017-03-08",
  "updated": "2017-03-08"
}