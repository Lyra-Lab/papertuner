{
  "id": "http://arxiv.org/abs/1703.10090v1",
  "title": "A Short Review of Ethical Challenges in Clinical Natural Language Processing",
  "authors": [
    "Simon Šuster",
    "Stéphan Tulkens",
    "Walter Daelemans"
  ],
  "abstract": "Clinical NLP has an immense potential in contributing to how clinical\npractice will be revolutionized by the advent of large scale processing of\nclinical records. However, this potential has remained largely untapped due to\nslow progress primarily caused by strict data access policies for researchers.\nIn this paper, we discuss the concern for privacy and the measures it entails.\nWe also suggest sources of less sensitive data. Finally, we draw attention to\nbiases that can compromise the validity of empirical research and lead to\nsocially harmful applications.",
  "text": "arXiv:1703.10090v1  [cs.CL]  29 Mar 2017\nA Short Review of Ethical Challenges in\nClinical Natural Language Processing\nSimon ˇSuster1,2, St´ephan Tulkens1 and Walter Daelemans1\n1CLiPS, University of Antwerp\n2Antwerp University Hospital\n{simon.suster,stephan.tulkens,walter.daelemans}@uantwerpen.be\nAbstract\nClinical NLP has an immense potential in\ncontributing to how clinical practice will\nbe revolutionized by the advent of large\nscale processing of clinical records. How-\never, this potential has remained largely\nuntapped due to slow progress primarily\ncaused by strict data access policies for re-\nsearchers.\nIn this paper, we discuss the\nconcern for privacy and the measures it\nentails. We also suggest sources of less\nsensitive data. Finally, we draw attention\nto biases that can compromise the validity\nof empirical research and lead to socially\nharmful applications.\n1\nIntroduction\nThe use of notes written by healthcare providers\nin\nthe\nclinical\nsettings\nhas\nlong\nbeen\nrec-\nognized\nto\nbe\na\nsource\nof\nvaluable\ninfor-\nmation for clinical practice and medical re-\nsearch.\nAccess to large quantities of clinical\nreports may help in identifying causes of dis-\neases, establishing diagnoses, detecting side ef-\nfects of beneﬁcial treatments, and monitoring\nclinical outcomes (Agus, 2016; Goldacre, 2014;\nMurdoch and Detsky, 2013).\nThe goal of clini-\ncal natural language processing (NLP) is to de-\nvelop and apply computational methods for lin-\nguistic analysis and extraction of knowledge from\nfree text reports (Demner-Fushman et al., 2009;\nHripcsak et al., 1995; Meystre et al., 2008).\nBut\nwhile the beneﬁts of clinical NLP and data min-\ning have been universally acknowledged, progress\nin the development of clinical NLP techniques has\nbeen slow. Several contributing factors have been\nidentiﬁed, most notably difﬁcult access to data,\nlimited collaboration between researchers from\ndifferent groups, and little sharing of implemen-\ntations and trained models (Chapman et al., 2011).\nFor comparison, in biomedical NLP, where the\nworking data consist of biomedical research litera-\nture, these conditions have been present to a much\nlesser degree, and the progress has been more\nrapid (Cohen and Demner-Fushman, 2014).\nThe\nmain contributing factor to this situation has been\nthe sensitive nature of data, whose processing may\nin certain situations put patient’s privacy at risk.\nThe ethics discussion is gaining momentum in\ngeneral NLP (Hovy and Spruit, 2016). We aim in\nthis paper to gather the ethical challenges that are\nespecially relevant for clinical NLP, and to stim-\nulate discussion about those in the broader NLP\ncommunity. Although enhancing privacy through\nrestricted data access has been the norm, we do\nnot only discuss the right to privacy, but also draw\nattention to the social impact and biases emanat-\ning from clinical notes and their processing. The\nchallenges we describe here are in large part not\nunique to clinical NLP, and are applicable to gen-\neral data science as well.\n2\nSensitivity of data and privacy\nBecause of legal and institutional concerns arising\nfrom the sensitivity of clinical data, it is difﬁcult\nfor the NLP community to gain access to relevant\ndata (Barzilay, 2016; Friedman et al., 2013). This\nis especially true for the researchers not connected\nwith a healthcare organization. Corpora with trans-\nparent access policies that are within reach of\nNLP researchers exist, but are few.\nAn often\nused corpus is MIMICII(I) (Johnson et al., 2016;\nSaeed et al., 2011). Despite its large size (cover-\ning over 58,000 hospital admissions), it is only\nrepresentative of patients from a particular clini-\ncal domain (the intensive care in this case) and ge-\nographic location (a single hospital in the United\nStates). Assuming that such a speciﬁc sample is\nrepresentative of a larger population is an exam-\nple of sampling bias (we discuss further sources\nof bias in section 3). Increasing the size of a sam-\nple without recognizing that this sample is atypical\nfor the general population (e.g. not all patients are\ncritical care patients) could also increase sampling\nbias (Kaplan et al., 2014).1 We need more large\ncorpora for various medical specialties, narrative\ntypes, as well as languages and geographic areas.\nRelated to difﬁcult access to raw clinical data\nis the lack of available annotated datasets for\nmodel training and benchmarking.\nThe reality\nis that annotation projects do take place, but are\ntypically constrained to a single healthcare or-\nganization.\nTherefore, much of the effort put\ninto annotation is lost afterwards due to impos-\nsibility of sharing with the larger research com-\nmunity (Chapman et al., 2011;\nFan et al., 2011).\nAgain, exceptions are either few—e.g. THYME\n(Styler IV et al., 2014), a corpus annotated with\ntemporal information—or consist of small datasets\nresulting from shared tasks like the i2b2 and\nShARe/CLEF. In addition, stringent access poli-\ncies hamper reproduction efforts, impede scien-\ntiﬁc oversight and limit collaboration, not only be-\ntween institutions but also more broadly between\nthe clinical and NLP communities.\nThere are known cases of datasets that had\nbeen used in published research (including re-\nproduction) in its full form,\nlike MiPACQ2,\nBlulab, EMC Dutch Clinical Corpus and 2010\ni2b2/VA (Albright et al., 2013; Kim et al., 2015;\nAfzal et al., 2014; Uzuner et al., 2011), but were\nlater trimmed down or made unavailable, likely\ndue to legal issues. Even if these datasets were still\navailable in full, their small size is still a concern,\nand the comments above regarding sampling bias\ncertainly apply. For example, a named entity rec-\nognizer trained on 2010 i2b2/VA data, which con-\nsists of 841 annotated patient records from three\ndifferent specialty areas, will due to its size only\ncontain a small portion of possible named entities.\nSimilarly, in linking clinical concepts to an ontol-\nogy, where the number of output classes is larger\n(Pradhan et al., 2013), the small amount of train-\ning data is a major obstacle to deployment of sys-\n1Sampling bias could also be called selection bias; it is\nnot inherent to the individual documents, but stems from the\nway these are arranged into a single corpus.\n2The access to the MiPACQ corpus will be re-enabled in\nthe future within the Health NLP Center for distributing lin-\nguistic annotations of clinical texts (Guergana Savova, per-\nsonal communication).\ntems suitable for general use.\n2.1\nProtecting the individual\nClinical notes contain detailed information about\npatient-clinician encounters in which patients con-\nﬁde not only their health complaints, but also their\nlifestyle choices and possibly stigmatizing condi-\ntions. This conﬁdential relationship is legally pro-\ntected in US by the HIPAA privacy rule in the case\nof individuals’ medical data. In EU, the conditions\nfor scientiﬁc usage of health data are set out in the\nGeneral Data Protection Regulation (GDPR). San-\nitization of sensitive data categories and individu-\nals’ informed consent are in the forefront of those\nlegislative acts and bear immediate consequences\nfor the NLP research.\nThe GDPR lists general principles relating to\nprocessing of personal data, including that pro-\ncessing must be lawful (e.g. by means of con-\nsent), fair and transparent; it must be done for ex-\nplicit and legitimate purposes; and the data should\nbe kept limited to what is necessary and as long\nas necessary.\nThis is known as data minimiza-\ntion, and it includes sanitization.\nThe scientiﬁc\nusage of health data concerns “special categories\nof personal data”.\nTheir processing is only al-\nlowed when the data subject gives explicit consent,\nor the personal data is made public by the data\nsubject. Scientiﬁc usage is deﬁned broadly and\nincludes technological development, fundamental\nand applied research, as well as privately funded\nresearch.\nSanitization\nSanitization techniques are often\nseen as the minimum requirement for protect-\ning individuals’ privacy when collecting data\n(Berman, 2002; Velupillai et al., 2015). The goal\nis to apply a procedure that produces a new ver-\nsion of the dataset that looks like the original for\nthe purposes of data analysis, but which maintains\nthe privacy of those in the dataset to a certain\ndegree, depending on the technique. Documents\ncan be sanitized by replacing, removing or oth-\nerwise manipulating the sensitive mentions such\nas names and geographic locations.\nA distinc-\ntion is normally drawn between anonymization,\npseudonymization and de-identiﬁcation. We refer\nthe reader to Polonetsky et al. (2016) for an excel-\nlent overview of these procedures.\nAlthough it is a necessary ﬁrst step in protect-\ning the privacy of patients, sanitization has been\ncriticized for several reasons.\nFirst, it affects\nthe integrity of the data, and as a consequence,\ntheir utility (Duquenoy et al., 2008).\nSecond,\nalthough sanitization in principle promotes data\naccess and sharing, it may often not be sufﬁ-\ncient to eliminate the need for consent.\nThis\nis largely due to the well-known fact that orig-\ninal sensitive data can be re-identiﬁed through\ndeductive\ndisclosure\n(Amblard et al., 2014;\nDe Mazancourt et al., 2015;\nHardt et al., 2016;\nMalin et al., 2013; Tene, 2011).3 Finally, sanitiza-\ntion focuses on protecting the individual, whereas\nethical harms are still possible on the group level\n(O’Doherty et al., 2016; Taylor et al., 2017).\nIn-\nstead of working towards increasingly restrictive\nsanitization and access measures, another course\nof action could be to work towards heightening\nthe perception of scientiﬁc work, emphasizing pro-\nfessionalism and existence of punitive measures\nfor\nillegal\nactions\n(Fairﬁeld and Shtein, 2014;\nMittelstadt and Floridi, 2016).\nConsent\nClinical NLP typically requires a large\namount of clinical records describing cases of pa-\ntients with a particular condition. Although obtain-\ning consent is a necessary ﬁrst step, obtaining ex-\nplicit informed consent from each patient can also\ncompromise the research in several ways. First,\nobtaining consent is time consuming by itself, and\nit results in ﬁnancial and bureaucratic burdens. It\ncan also be infeasible due to practical reasons such\nas a patient’s death. Next, it can introduce bias as\nthose willing to grant consent represent a skewed\npopulation (Nyr´en et al., 2014). Finally, it can be\ndifﬁcult to satisfy the informedness criterion: In-\nformation about the experiment sometimes can not\nbe communicated in an unambiguous way, or ex-\nperiments happen at speed that makes enacting in-\nformed consent extremely hard (Bird et al., 2016).\nThe alternative might be a default opt-in pol-\nicy with a right to withdraw (opt-out). Here, con-\nsent can be presumed either in a broad manner—\nallowing unspeciﬁed future research,\nsubject\nto ethical restrictions—or\na tiered manner—\nallowing certain areas of research but not others\n(Mittelstadt and Floridi, 2016; Terry, 2012). Since\nthe information about the intended use is no longer\nuniquely tied to each research case but is more\ngeneral, this could facilitate the reuse of datasets\n3Additionaly, it may be due to organizational skepticism\nabout the effectiveness of sanitization techniques, although\nit has been shown that automated de-identiﬁcation systems\nfor English perform on par with manual de-identiﬁcation\n(Deleger et al., 2013).\nby several research teams, without the need to\nask for consent each time. The success of imple-\nmenting this approach in practice is likely to de-\npend on public trust and awareness about possible\nrisks and opportunities. We also believe that a dis-\ntinction between academic research and commer-\ncial use of clinical data should be implemented,\nas the public is more willing to allow research\nthan commercial exploitation (Lawrence, 2016;\nvan Staa et al., 2016).\nYet another possibility is open consent, in which\nindividuals make their data publicly available. Ini-\ntiatives like Personal Genome Project may have\nan exemplary role, however, they can only provide\nlimited data and they represent a biased population\nsample (Mittelstadt and Floridi, 2016).\nSecure access\nSince withholding data from re-\nsearchers would be a dubious way of ensuring con-\nﬁdentiality (Berman, 2002), the research has long\nbeen active on secure access and storage of sensi-\ntive clinical data, and the balance between the de-\ngree of privacy loss and the degree of utility. This\nis a broad topic that is outside the scope of this ar-\nticle. The interested reader can ﬁnd the relevant\ninformation in Dwork and Pottenger (2013), Ma-\nlin et al. (2013) and Rindﬂeisch (1997).\nPromotion of knowledge and application of\nbest-of-class approaches to health data is seen\nas one of the ethical duties of researchers\n(Duquenoy et al., 2008; Lawrence, 2016). But for\nthis to be put in practice, ways need to be guar-\nanteed (e.g. with government help) to provide re-\nsearchers with access to the relevant data.\nRe-\nsearchers can also go to the data rather than have\nthe data sent to them.\nIt is an open question\nthough whether medical institutions—especially\nthose with less developed research departments—\ncan provide the infrastructure (e.g. enough CPU\nand GPU power) needed in statistical NLP. Also,\ngranting access to one healthcare organization at\na time does not satisfy interoperability (cross-\norganizational data sharing and research), which\ncan reduce bias by allowing for more complete in-\nput data. Interoperability is crucial for epidemiol-\nogy and rare disease research, where data from one\ninstitution can not yield sufﬁcient statistical power\n(Kaplan et al., 2014).\nAre there less sensitive data?\nOne criterion\nwhich may have inﬂuence on data accessibility is\nwhether the data is about living subjects or not.\nThe HIPAA privacy rule under certain conditions\nallows disclosure of personal health information of\ndeceased persons, without the need to seek IRB\nagreement and without the need for sanitization\n(Huser and Cimino, 2014). It is not entirely clear\nthough how often this possibility has been used in\nclinical NLP research or broader.\nNext, the work on surrogate data has re-\ncently seen a surge in activity. Increasingly more\nhealth-related texts are produced in social media\n(Abbasi et al., 2014), and patient-generated data\nare available online. Admittedly, these may not\nresemble the clinical discourse, yet they bear to\nthe same individuals whose health is documented\nin the clinical reports. Indeed, linking individu-\nals’ health information from online resources to\ntheir health records to improve documentation is\nan active line of research (Padrez et al., 2015). Al-\nthough it is generally easier to obtain access to\nsocial media data, the use of social media still\nrequires similar ethical considerations as in the\nclinical domain.\nSee for example the inﬂuen-\ntial study on emotional contagion in Facebook\nposts by Kramer et al. (2014), which has been\ncriticized for not properly gaining prior consent\nfrom the users who were involved in the study\n(Schroeder, 2014).\nAnother way of reducing sensitivity of data and\nimproving chances for IRB approval is to work\non derived data. Data that can not be used to\nreconstruct the original text (and when sanitized,\ncan not directly re-identify the individual) include\ntext fragments, various statistics and trained mod-\nels. Working on randomized subsets of clinical\nnotes may also improve the chances of obtaining\nthe data. When we only have access to trained\nmodels from disparate sources, we can reﬁne them\nthrough ensembling and creation of silver standard\ncorpora, cf. Rebholz-Schuhmann et al. (2011).\nFinally, clinical NLP is also possible on veteri-\nnary texts.\nRecords of companion animals are\nperhaps less likely to involve legal issues, while\nstill amounting to a large pool of data.\nAs an\nexample, around 40M clinical documents from\ndifferent veterinary clinics in UK and Australia\nare stored centrally in the VetCompass repository.\nFirst NLP steps in this direction were described in\nthe invited talk at the Clinical NLP 2016 workshop\n(Baldwin, 2016).\n3\nSocial impact and biases\nUnlocking knowledge from free text in the health\ndomain has a tremendous societal value.\nHow-\never, discrimination can occur when individuals\nor groups receive unfair treatment as a result of\nautomated processing, which might be a result of\nbiases in the data that were used to train mod-\nels. The question is therefore what the most im-\nportant biases are and how to overcome them,\nnot only out of ethical but also legal responsi-\nbility.\nRelated to the question of bias is so-\ncalled algorithm transparency (Goodman, 2016;\nKamarinou et al., 2016), as this right to explana-\ntion requires that inﬂuences of bias in training data\nare charted. In addition to sampling bias, which\nwe introduced in section 2, we discuss in this sec-\ntion further sources of bias. Unlike sampling bias,\nwhich is a corpus-level bias, these biases here are\nalready present in documents, and therefore hard\nto account for by introducing larger corpora.\nData quality\nTexts produced in the clinical set-\ntings do not always tell a complete or accurate pa-\ntient story (e.g. due to time constraints or due to\npatient treatment in different hospitals), yet impor-\ntant decisions can be based on them.4 As language\nis situated, a lot of information may be implicit,\nsuch as the circumstances in which treatment de-\ncisions are made (Hersh et al., 2013). If we fail\nto detect a medical concept during automated pro-\ncessing, this can not necessarily be a sign of nega-\ntive evidence.5 Work on identifying and imputing\nmissing values holds promise for reducing incom-\npleteness, see Lipton et al. (2016) for an example\nin sequential modeling applied to diagnosis classi-\nﬁcation.\nReporting bias\nClinical texts may include bias\ncoming from both patient’s and clinician’s report-\ning. Clinicians apply their subjective judgments\nto what is important during the encounter with pa-\ntients. In other words, there is separation between,\non the one side, what is observed by the clinician\nand communicated by the patient, and on the other,\n4A way to increase data completeness and reduce selec-\ntion bias is the use of nationwide patient registries, as known\nfor example in Scandinavian countries (Schmidt et al., 2015).\n5We can take timing-related “censoring” effects as an ex-\nample. In event detection, events prior to the start of an obser-\nvation may be missed or are uncertain, which means that the\nﬁrst appearance of a diagnosis in the clinical record may not\ncoincide with the occurrence of the disease. Similarly, key\nevents after the end of the observation may be missing (e.g.\ndeath, when it occurred in another institution).\nwhat is noted down. Cases of more serious illness\nmay be more accurately documented as a result of\nclinician’s bias (increased attention) and patient’s\nrecall bias. On the other hand, the cases of stig-\nmatized diseases may include suppressed informa-\ntion. In the case of trafﬁc injuries, documentation\nmay even be distorted to avoid legal consequences\n(Indrayan, 2013).\nWe need to be aware that clinical notes may re-\nﬂect health disparities. These can originate from\nprejudices held by healthcare practitioners which\nmay impact patients’ perceptions; they can also\noriginate from communication difﬁculties in the\ncase of ethnic differences (Zestcott et al., 2016).\nFinally, societal norms can play a role. Brady et al.\n(2016) ﬁnd that obesity is often not documented\nequally well for both sexes in weight-addressing\nclinics. Young males are less likely to be recog-\nnized as obese, possibly due to societal norms see-\ning them as “stocky” as opposed to obese. Unless\nwe are aware of such bias, we may draw premature\nconclusions about the impact of our results.\nIt is clear that during processing of clinical texts,\nwe should strive to avoid reinforcing the biases. It\nis difﬁcult to give a solution on how to actually\nreduce the reporting bias after the fact. One pos-\nsibility might be to model it. If we see clinical re-\nports as noisy annotations for the patient story in\nwhich information is left-out or altered, we could\ntry to decouple the bias from the reports. Inspira-\ntion could be drawn, for example, from the work\non decoupling reporting bias from annotations in\nvisual concept recognition (Misra et al., 2016).\nObservational bias\nAlthough variance in health\noutcome is affected by social, environmental and\nbehavioral factors, these are rarely noted in clini-\ncal reports (Kaplan et al., 2014). The bias of miss-\ning explanatory factors because they can not be\nidentiﬁed within the given experimental setting is\nalso known as the streetlight effect.\nIn certain\ncases, we could obtain important prior knowledge\n(e.g. demographic characteristics) from data other\nthan clinical notes.\nDual use\nWe have already mentioned linking\npersonal health information from online texts to\nclinical records as a motivation for exploring sur-\nrogate data sources. However, this and many other\napplications also have potential to be applied in\nboth beneﬁcial and harmful ways. It is easy to\nimagine how sensitive information from clinical\nnotes can be revealed about an individual who\nis present in social media with a known identity.\nMore general examples of dual use are when the\nNLP tools are used to analyze clinical notes with\na goal of determining individuals’ insurability and\nemployability.\n4\nConclusion\nIn this paper, we reviewed some challenges that\nwe believe are central to the work in clinical NLP.\nDifﬁcult access to data due to privacy concerns has\nbeen an obstacle to progress in the ﬁeld. We have\ndiscussed how the protection of privacy through\nsanitization measures and the requirement for in-\nformed consent may affect the work in this do-\nmain. Perhaps, it is time to rethink the right to pri-\nvacy in health in the light of recent work in ethics\nof big data, especially its uneasy relationship to\nthe right to science, i.e. being able to beneﬁt\nfrom science and participate in it (Tasioulas, 2016;\nVerbeek, 2014). We also touched upon possible\nsources of bias that can have an effect on the ap-\nplication of NLP in the health domain, and which\ncan ultimately lead to unfair or harmful treatment.\nAcknowledgments\nWe would like to thank Madhumita and the anony-\nmous reviewers for useful comments. Part of this\nresearch was carried out in the framework of the\nAccumulate IWT SBO project, funded by the gov-\nernment agency for Innovation by Science and\nTechnology (IWT).\nReferences\n[Abbasi et al.2014] Ahmed Abbasi, Donald Adjeroh,\nMark Dredze, Michael J. Paul, Fatemeh Mariam\nZahedi, Huimin Zhao, Nitin Walia, Hemant Jain,\nPatrick Sanvanson, Reza Shaker, et al. 2014. Social\nmedia analytics for smart health. IEEE Intelligent\nSystems, 29(2):60–80.\n[Afzal et al.2014] Zubair Afzal, Ewoud Pons, Ning\nKang, Miriam C.J.M. Sturkenboom, Martijn J.\nSchuemie, and Jan A. Kors. 2014. ContextD: an al-\ngorithm to identify contextual properties of medical\nterms in a Dutch clinical corpus. BMC Bioinformat-\nics, 15(1):373.\n[Agus2016] David B. Agus. 2016. Give Up Your Data\nto Cure Disease, The New York Times, February 6.\nhttps://goo.gl/0REG0n.\n[Albright et al.2013] D.\nAlbright,\nA.\nLanfranchi,\nA. Fredriksen, W. F. Styler, C. Warner, J. D. Hwang,\nJ. D. Choi, D. Dligach, R. D. Nielsen, J. Martin,\nW. Ward, M. Palmer, and G. K. Savova.\n2013.\nTowards comprehensive syntactic and semantic\nannotations of the clinical narrative.\nJournal of\nthe American Medical Informatics Association,\n20(5):922–930.\n[Amblard et al.2014] Maxime Amblard, Kar¨en Fort,\nMichel Musiol, and Manuel Rebuschi.\n2014.\nL’impossibilit´e de l’anonymat dans le cadre de\nl’analyse du discours.\nIn Journ´ee ATALA ´ethique\net TAL.\n[Baldwin2016] Timothy Baldwin.\n2016.\nVet-\nCompass:\nClinical Natural Language Processing\nfor Animal Health. Clinical NLP 2016 keynote.\nhttps://goo.gl/ScGFa2.\n[Barzilay2016] Regina Barzilay.\n2016.\nHow NLP\ncan help cure cancer?\nNAACL’16 keynote.\nhttps://goo.gl/hi5nrq.\n[Berman2002] Jules J. Berman.\n2002.\nConﬁdential-\nity issues for medical data miners. Artiﬁcial Intel-\nligence in Medicine, 26(1):25–36.\n[Bird et al.2016] Sarah Bird,\nSolon Barocas,\nKate\nCrawford, Fernando Diaz, and Hanna Wallach.\n2016. Exploring or Exploiting? Social and Ethical\nImplications of Autonomous Experimentation in AI.\nIn Workshop on Fairness, Accountability, and Trans-\nparency in Machine Learning.\n[Brady et al.2016] Cassandra\nC.\nBrady,\nVidhu\nV.\nThaker, Todd Lingren, Jessica G. Woo, Stephanie S.\nKennebeck,\nBahram\nNamjou-Khales,\nAshton\nRoach, Jonathan P. Bickel, Nandan Patibandla,\nGuergana K. Savova, et al. 2016. Suboptimal Clini-\ncal Documentation in Young Children with Severe\nObesity at Tertiary Care Centers.\nInternational\nJournal of Pediatrics, 2016.\n[Chapman et al.2011] Wendy W. Chapman, Prakash M.\nNadkarni,\nLynette\nHirschman,\nLeonard\nW.\nD’Avolio, Guergana K. Savova, and ¨Ozlem Uzuner.\n2011. Overcoming barriers to NLP for clinical text:\nthe role of shared tasks and the need for additional\ncreative solutions. Journal of the American Medical\nInformatics Association, 18(5):540–543.\n[Cohen and Demner-Fushman2014] Kevin\nBretonnel\nCohen and Dina Demner-Fushman. 2014. Biomed-\nical natural language processing. John Benjamins\nPublishing Company.\n[De Mazancourt et al.2015] Hugues\nDe\nMazancourt,\nAlain Couillault, Gilles Adda, and Ga¨elle Recourc´e.\n2015. Faire du TAL sur des donn´ees personnelles :\nun oxymore ? In TALN 2015.\n[Deleger et al.2013] Louise Deleger, Katalin Molnar,\nGuergana Savova, Fei Xia, Todd Lingren, Qi Li,\nKeith Marsolo, Anil Jegga, Megan Kaiser, Laura\nStoutenborough, and Imre Solti.\n2013.\nLarge-\nscale evaluation of automated clinical note de-\nidentiﬁcation and its impact on information extrac-\ntion. Journal of the American Medical Informatics\nAssociation, 20(1):84.\n[Demner-Fushman et al.2009] Dina Demner-Fushman,\nWendy W. Chapman, and Clement J. McDonald.\n2009. What can natural language processing do for\nclinical decision support?\nJournal of Biomedical\nInformatics, 42(5):760–772.\n[Duquenoy et al.2008] Penny\nDuquenoy,\nCarlisle\nGeorge, and Anthony Solomonides.\n2008.\nCon-\nsidering\nsomething\nELSE:\nEthical,\nlegal\nand\nsocio-economic factors in medical imaging and\nmedical informatics.\nComputer Methods and\nPrograms in Biomedicine, 92(3):227–237.\n[Dwork and Pottenger2013] Cynthia Dwork and Re-\nbecca Pottenger. 2013. Toward practicing privacy.\nJournal of the American Medical Informatics Asso-\nciation, 20(1):102–108.\n[Fairﬁeld and Shtein2014] Joshua Fairﬁeld and Hannah\nShtein. 2014. Big data, big problems: Emerging\nissues in the ethics of data science and journalism.\nJournal of Mass Media Ethics, 29(1):38–51.\n[Fan et al.2011] Jung-wei\nFan,\nRashmi\nPrasad,\nRomme M. Yabut, Richard M. Loomis, Daniel S.\nZisook, John E. Mattison, and Yang Huang. 2011.\nPart-of-speech tagging for clinical text:\nwall or\nbridge between institutions.\nIn AMIA Annual\nSymposium Proceedings.\n[Friedman et al.2013] Carol\nFriedman,\nThomas\nC.\nRindﬂesch, and Milton Corn.\n2013.\nNatural\nlanguage processing: state of the art and prospects\nfor signiﬁcant progress, a workshop sponsored\nby the National Library of Medicine.\nJournal of\nBiomedical Informatics, 46(5):765–773.\n[Goldacre2014] Ben Goldacre. 2014. The NHS plan to\nshare our medical data can save lives but must be\ndone right. https://goo.gl/MH2eC0.\n[Goodman2016] Bryce W. Goodman. 2016. A Step To-\nwards Accountable Algorithms?: Algorithmic Dis-\ncrimination and the European Union General Data\nProtection. In NIPS Symposium on Machine Learn-\ning and the Law.\n[Hardt et al.2016] Moritz Hardt, Eric Price, and Nati\nSrebro. 2016. Equality of opportunity in supervised\nlearning. In NIPS.\n[Hersh et al.2013] William R. Hersh, Mark G. Weiner,\nPeter J. Embi, Judith R. Logan, Philip R.O. Payne,\nElmer V. Bernstam, Harold P. Lehmann, George\nHripcsak, Timothy H. Hartzog, James J. Cimino,\nand Joel H. Saltz. 2013. Caveats for the use of oper-\national electronic health record data in comparative\neffectiveness research. Medical care, 51(8 0 3).\n[Hovy and Spruit2016] Dirk Hovy and Shannon L.\nSpruit.\n2016.\nThe social impact of natural lan-\nguage processing. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 591–\n598, Berlin, Germany, August. Association for Com-\nputational Linguistics.\n[Hripcsak et al.1995] George Hripcsak, Carol Fried-\nman, Philip O. Alderson, William DuMouchel,\nStephen B. Johnson, and Paul D. Clayton. 1995. Un-\nlocking clinical data from narrative reports: a study\nof natural language processing. Annals of internal\nmedicine, 122(9):681–688.\n[Huser and Cimino2014] Vojtech Huser and James J.\nCimino.\n2014.\nDon’t take your EHR to heaven,\ndonate it to science: legal and research policies for\nEHR post mortem. Journal of the American Medical\nInformatics Association, 21(1):8–12.\n[Indrayan2013] Abhaya\nIndrayan.\n2013.\nVarieties\nof\nbias\nto\nguard\nagainst.\nhttps://goo.gl/SqnuZY.\n[Johnson et al.2016] Alistair EW Johnson, Tom J. Pol-\nlard, Lu Shen, Li-wei H. Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter\nSzolovits, Leo Anthony Celi, and Roger G. Mark.\n2016. MIMIC-III, a freely accessible critical care\ndatabase. Scientiﬁc data, 3.\n[Kamarinou et al.2016] Dimitra Kamarinou,\nChristo-\npher Millard, and Jatinder Singh. 2016. Machine\nLearning with Personal Data. Queen Mary School\nof Law Legal Studies Research Paper, (247/2016).\n[Kaplan et al.2014] Robert M. Kaplan, David A. Cham-\nbers, and Russell E. Glasgow.\n2014.\nBig data\nand large sample size: a cautionary note on the po-\ntential for bias. Clinical and translational science,\n7(4):342–346.\n[Kim et al.2015] Youngjun Kim,\nEllen Riloff,\nand\nJohn F. Hurdle. 2015. A study of concept extrac-\ntion across different types of clinical notes. In AMIA\nAnnual Symposium Proceedings, volume 2015, page\n737. American Medical Informatics Association.\n[Kramer et al.2014] Adam D.I. Kramer, Jamie E. Guil-\nlory, and Jeffrey T. Hancock.\n2014.\nExperimen-\ntal evidence of massive-scale emotional contagion\nthrough social networks.\nProceedings of the Na-\ntional Academy of Sciences, 111(24):8788–8790.\n[Lawrence2016] Neil\nLawrence.\n2016.\nData\nAnalysis,\nNHS\nand\nIndustrial\nPartners.\nhttps://goo.gl/rRIcu5.\n[Lipton et al.2016] Zachary C. Lipton, David Kale, and\nRandall Wetzel. 2016. Modeling Missing Data in\nClinical Time Series with RNNs.\narXiv preprint\narXiv:1606.04130.\n[Malin et al.2013] Bradley A. Malin, Khaled El Emam,\nand Christine M. O’Keefe.\n2013.\nBiomedical\ndata privacy: problems, perspectives, and recent ad-\nvances. Journal of the American Medical Informat-\nics Association, 20(1):2–6.\n[Meystre et al.2008] St´ephane M. Meystre, Guergana K.\nSavova, Karin C. Kipper-Schuler, John F. Hurdle,\net al.\n2008.\nExtracting information from textual\ndocuments in the electronic health record: a review\nof recent research. Yearbook of Medical Informatics,\n35:128–44.\n[Misra et al.2016] Ishan Misra, C. Lawrence Zitnick,\nMargaret Mitchell, and Ross B. Girshick.\n2016.\nSeeing through the Human Reporting Bias: Visual\nClassiﬁers from Noisy Human-Centric Labels. In\nThe IEEE Conference on Computer Vision and Pat-\ntern Recognition.\n[Mittelstadt and Floridi2016] Brent Daniel Mittelstadt\nand Luciano Floridi. 2016. The ethics of big data:\ncurrent and foreseeable issues in biomedical con-\ntexts.\nScience and engineering ethics, 22(2):303–\n341.\n[Murdoch and Detsky2013] Travis B. Murdoch and Al-\nlan S. Detsky. 2013. The inevitable application of\nbig data to health care. JAMA, 309(13):1351–1352.\n[Nyr´en et al.2014] Olof Nyr´en, Magnus Stenbeck, and\nHenrik Gr¨onberg. 2014. The European Parliament\nproposal for the new EU General Data Protection\nRegulation may severely restrict European epidemi-\nological research. European Journal of Epidemiol-\nogy, 29(4):227–230.\n[O’Doherty et al.2016] Kieran C. O’Doherty, Emily\nChristoﬁdes, Jeffery Yen, Heidi Beate Bentzen,\nWylie Burke, Nina Hallowell, Barbara A. Koenig,\nand Donald J. Willison.\n2016.\nIf you build it,\nthey will come: unintended future uses of organised\nhealth data collections. BMC Medical Ethics, 17(1).\n[Padrez et al.2015] Kevin\nA.\nPadrez,\nLyle\nUngar,\nHansen Andrew Schwartz, Robert J. Smith, Shawn-\ndra Hill, Tadas Antanavicius, Dana M. Brown,\nPatrick Crutchley, David A. Asch, and Raina M.\nMerchant. 2015. Linking social media and medical\nrecord data: a study of adults presenting to an aca-\ndemic, urban emergency department. BMJ quality\n& safety.\n[Polonetsky et al.2016] Jules Polonetsky, Omer Tene,\nand Kelsey Finch.\n2016.\nShades of Gray:\nSeeing the Full Spectrum of Practical Data De-\nidentiﬁcation. Santa Clara Law Review, 56(3).\n[Pradhan et al.2013] Sameer Pradhan, Noemie Elhadad,\nBrett R. South, David Martinez, Amy Vogel, Hanna\nSuominen, Wendy W. Chapman, and Guergana\nSavova. 2013. Task 1: Share/clef ehealth evaluation\nlab. In Online Working Notes of CLEF.\n[Rebholz-Schuhmann et al.2011] Dietrich\nRebholz-\nSchuhmann, Antonio Jimeno Yepes, Chen Li, et al.\n2011.\nAssessment of NER solutions against the\nﬁrst and second CALBC Silver Standard Corpus.\nJournal of Biomedical Semantics, 2(5):S11.\n[Rindﬂeisch1997] Thomas C. Rindﬂeisch. 1997. Pri-\nvacy, information technology, and health care. Com-\nmunications of the ACM, 40(8):92–100.\n[Saeed et al.2011] Mohammed Saeed, Mauricio Villar-\nroel, Andrew T. Reisner, Gari Clifford, Li-Wei\nLehman, George Moody, Thomas Heldt, Tin H.\nKyaw, Benjamin Moody, and Roger G. Mark. 2011.\nMultiparameter Intelligent Monitoring in Intensive\nCare II (MIMIC-II): a public-access intensive care\nunit database. Critical care medicine, 39(5):952.\n[Schmidt et al.2015] Morten Schmidt, S.A. Schmidt,\nJakob Lynge Sandegaard, Vera Ehrenstein, Lars Ped-\nersen, and Henrik Toft Sørensen. 2015. The danish\nnational patient registry: a review of content, data\nquality, and research potential. Clinical Epidemiol-\nogy, 7(449):e490.\n[Schroeder2014] Ralph Schroeder. 2014. Big data and\nthe brave new world of social media research. Big\nData & Society, 1(2):2053951714563194.\n[Styler IV et al.2014] William\nStyler\nIV,\nSteven\nBethard, Sean Finan, Martha Palmer, Sameer Prad-\nhan, Piet de Groen, Brad Erickson, Timothy Miller,\nChen Lin, Guergana Savova, and James Pustejovsky.\n2014. Temporal annotation in the clinical domain.\nTransactions of the Association for Computational\nLinguistics, 2:143–154.\n[Tasioulas2016] John\nTasioulas.\n2016.\nVan\nHasselt\nLecture\n2016:\nBig\nData,\nHuman\nRights and the Ethics of Scientiﬁc Research.\nhttps://goo.gl/QREHUN.\n[Taylor et al.2017] Linnet Taylor, Luciano Floridi, and\nBart van der Sloot.\n2017.\nGroup Privacy: New\nChallenges of Data Technologies. Springer Interna-\ntional Publishing.\n[Tene2011] Omer Tene. 2011. Privacy: The new gen-\nerations. International Data Privacy Law, 1(1):15–\n27.\n[Terry2012] Nicolas P. Terry.\n2012.\nProtecting pa-\ntient privacy in the age of big data. University of\nMissouri-Kansas City Law Review, 81(2).\n[Uzuner et al.2011] ¨Ozlem Uzuner, Brett R. South,\nShuying Shen, and Scott L. DuVall. 2011. 2010\ni2b2/va challenge on concepts, assertions, and rela-\ntions in clinical text. Journal of the American Medi-\ncal Informatics Association, 18(5):552–556.\n[van Staa et al.2016] Tjeerd-Pieter\nvan\nStaa,\nBen\nGoldacre, Iain Buchan, and Liam Smeeth.\n2016.\nBig health data: the need to earn public trust. BMJ,\n354:i3636.\n[Velupillai et al.2015] Sumithra Velupillai, D. Mowery,\nBrett R. South, Maria Kvist, and Hercules Dalianis.\n2015. Recent advances in clinical natural language\nprocessing in support of semantic analysis.\nYear-\nbook of Medical Informatics, 10:183–193.\n[Verbeek2014] Peter-Paul Verbeek.\n2014.\nOp de\nvleugels van Icarus. Lemniscaat.\n[Zestcott et al.2016] Colin A. Zestcott, Irene V. Blair,\nand Jeff Stone. 2016. Examining the presence, con-\nsequences, and reduction of implicit bias in health\ncare: A narrative review. Group Processes & Inter-\ngroup Relations.\n",
  "categories": [
    "cs.CL",
    "cs.CY"
  ],
  "published": "2017-03-29",
  "updated": "2017-03-29"
}