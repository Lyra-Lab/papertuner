{
  "id": "http://arxiv.org/abs/1703.10901v1",
  "title": "Unsupervised learning from video to detect foreground objects in single images",
  "authors": [
    "Ioana Croitoru",
    "Simion-Vlad Bogolin",
    "Marius Leordeanu"
  ],
  "abstract": "Unsupervised learning from visual data is one of the most difficult\nchallenges in computer vision, being a fundamental task for understanding how\nvisual recognition works. From a practical point of view, learning from\nunsupervised visual input has an immense practical value, as very large\nquantities of unlabeled videos can be collected at low cost. In this paper, we\naddress the task of unsupervised learning to detect and segment foreground\nobjects in single images. We achieve our goal by training a student pathway,\nconsisting of a deep neural network. It learns to predict from a single input\nimage (a video frame) the output for that particular frame, of a teacher\npathway that performs unsupervised object discovery in video. Our approach is\ndifferent from the published literature that performs unsupervised discovery in\nvideos or in collections of images at test time. We move the unsupervised\ndiscovery phase during the training stage, while at test time we apply the\nstandard feed-forward processing along the student pathway. This has a dual\nbenefit: firstly, it allows in principle unlimited possibilities of learning\nand generalization during training, while remaining very fast at testing.\nSecondly, the student not only becomes able to detect in single images\nsignificantly better than its unsupervised video discovery teacher, but it also\nachieves state of the art results on two important current benchmarks, YouTube\nObjects and Object Discovery datasets. Moreover, at test time, our system is at\nleast two orders of magnitude faster than other previous methods.",
  "text": "Unsupervised learning from video\nto detect foreground objects in single images\nIoana Croitoru1\nSimion-Vlad Bogolin1\nMarius Leordeanu1,2\nioana.croi@gmail.com\nvladbogolin@gmail.com\nmarius.leordeanu@imar.ro\n1Institute of Mathematics of the Romanian Academy\n2University ”Politehnica” of Bucharest\n21 Calea Grivitei, Bucharest, Romania\n313 Splaiul Independentei, Bucharest, Romania\nAbstract\nUnsupervised learning from visual data is one of the\nmost difﬁcult challenges in computer vision, being a fun-\ndamental task for understanding how visual recognition\nworks. From a practical point of view, learning from un-\nsupervised visual input has an immense practical value, as\nvery large quantities of unlabeled videos can be collected at\nlow cost. In this paper, we address the task of unsupervised\nlearning to detect and segment foreground objects in single\nimages. We achieve our goal by training a student pathway,\nconsisting of a deep neural network. It learns to predict\nfrom a single input image (a video frame) the output for\nthat particular frame, of a teacher pathway that performs\nunsupervised object discovery in video. Our approach is\ndifferent from the published literature that performs unsu-\npervised discovery in videos or in collections of images at\ntest time. We move the unsupervised discovery phase during\nthe training stage, while at test time we apply the standard\nfeed-forward processing along the student pathway. This\nhas a dual beneﬁt: ﬁrstly, it allows in principle unlimited\npossibilities of learning and generalization during training,\nwhile remaining very fast at testing. Secondly, the student\nnot only becomes able to detect in single images signiﬁ-\ncantly better than its unsupervised video discovery teacher,\nbut it also achieves state of the art results on two important\ncurrent benchmarks, YouTube Objects and Object Discov-\nery datasets. Moreover, at test time, our system is at least\ntwo orders of magnitude faster than other previous methods.\n1. Introduction\nThe problem of unsupervised learning is one of the most\ndifﬁcult and intriguing in computer vision and machine\nlearning today. In a very general sense, many researchers\nbelieve that unsupervised learning from video could help\ndecode many hard questions regarding the nature of intel-\nligence and learning. Since unlabeled videos are easy to\ncollect at a very low cost, solving this task would bring a\ngreat practical value in many vision and robotics applica-\ntions. There are several papers addressing this difﬁcult task,\nbut the current methods are still far from fully solving the\nchallenge. Many recent unsupervised methods in vision fol-\nlow two main directions: one is to learn powerful features\nin a completely unsupervised manner and then use them in\na classic supervised learning scheme in combination with\ndifferent classiﬁers, such as SVMs or CNNs [30, 24, 22].\nThe second, more classical line of research, is to discover\ncommon patterns in unlabeled data, at test time, using dif-\nferent clustering, feature matching or other data mining ap-\nproaches [11, 7, 39]. In the ﬁrst case the unsupervised learn-\ning task is limited to the intermediate level of feature learn-\ning, while in the second, its performance depends on the\nspeciﬁc structure of the image collection given at test time.\nThe task of object discovery and unsupervised learning\nin video is related to co-segmentation [13, 18, 35, 14, 20, 42,\n36] and weakly supervised localization [9, 25, 38]. Earlier\nmethods are based on local feature matching and detection\nof their co-occurrences patterns [40, 39, 21, 27, 23], while\nrecent approaches [15, 32] discover object tubes by linking\ncandidate detections between frames with or without reﬁn-\ning their location. Traditionally, the task of unsupervised\nlearning from image sequences, formulated as an optimiza-\ntion problem for either feature matching, conditional ran-\ndom ﬁelds or data clustering is inherently expensive due\nto the combinatorial nature of the problem. That is why\nour approach, in which we learn to detect in a fast, feed-\nforward manner from an unsupervised object discoverer in\nvideo (while having virtually unlimited training data), has\ncertain advantages that might open new possibilities in the\nquest for solving the unsupervised learning problem in the\nreal world.\nOur system is presented in Figure 1. We have an un-\nsupervised training stage, in which a student deep neural\nnetwork (Figure 2) learns frame by frame from an unsu-\n1\narXiv:1703.10901v1  [cs.CV]  31 Mar 2017\npervised teacher, which performs object segmentation dis-\ncovery in videos, to produce similar object masks in single\nimages. The teacher method takes advantage of the consis-\ntency in appearance, shape and motion manifested by ob-\njects in video. In this way, it discovers objects in the video\nand produces a foreground segmentation for each individual\nframe. Then, the student network tries to imitate for each\nframe the output of the teacher, while having as input only\na single image - the current frame. The teacher pathway\nis much simpler in structure, but it has access to informa-\ntion over time. In contrast, the student is much deeper in\nstructure, but has access only to one image. Thus, the infor-\nmation discovered by the teacher in time is captured by the\nstudent in depth, over neural layers of abstraction. In exper-\niments, we show a very encouraging fact: the student easily\nlearns to outperform its teacher and discovers by itself gen-\neral knowledge about the shape and appearance properties\nof objects, well beyond the abilities of the teacher. Thus, the\nstudent produces signiﬁcantly better object masks, which\ngenerally have a good form, do not have holes and display\nsmooth contours, while having an appearance that is often\nin contrast to the background scene.\nSince there are available methods for video discovery\nwith good performance, the training task becomes imme-\ndiately feasible. In this work we chose the VideoPCA al-\ngorithm introduced as part of the system in [40] because it\nis very fast (50-100 fps), uses very simple features (pixel\ncolors) and it is completely unsupervised - with no usage\nof supervised pre-trained features. That method exploits\nthe stability in appearance and location of objects, which is\ncommon in video shots. While the object masks discovered\nare far from being perfect and are often noisy, the student\ndeep network manages to generalize and overcome some of\nthese limitations. We propose a ten layer deep neural net-\nwork for the student pathway (Figure 2). It takes as input\nthe original RGB, HSV and image spatial derivatives chan-\nnels. It outputs a low resolution soft segmentation mask of\nthe main objects present in a given image.\nMain contributions:\nOur main contributions are:\n1) Our approach, to our best knowledge, is the ﬁrst one\nthat learns to detect and segment foreground objects in im-\nages in a completely unsupervised fashion, with no pre-\ntrained features needed or manual labeling, while requiring\nonly a single image at test time.\n2) We propose a novel architecture for unsupervised\nlearning in video, consisting of two processing pathways,\nwith complementary functions and properties.\nThe ﬁrst\npathway discovers foreground objects in videos in an un-\nsupervised manner and has access to all the video frames.\nIt acts as a teacher. The second ”student” pathway, which\nis a deep convolutional net, learns to predict the teacher’s\noutput for each frame while having access only to a single\nFigure 1. Our dual student-teacher system used for unsupervised\nlearning to detect foreground objects in images. It combines two\nprocessing pathways: the teacher, on the right hand side, discovers\nin an unsupervised fashion foreground objects in video and outputs\nsoft masks for each frame. The resulting soft masks, are then ﬁl-\ntered and only good segmentations are kept, based on a simple and\neffective unsupervised quality metric. The set of selected segmen-\ntations is then augmented in a relatively simple manner, automati-\ncally. The resulting ﬁnal set of pairs - input image (a video frame)\nand soft mask (the mask for that particular frame which acts as an\nunsupervised label) - are used for training the student CNN path-\nway. Note that the student, after being fully trained, outperforms\nthe teacher.\ninput image. An important fact, shown in our experiments,\nis that the student learns to outperform its teacher, despite\nbeing limited to a single image input. Once trained using\nour dual-pathway system, the student achieves state of the\nart results on two important datasets.\n2. Our approach and intuition\nThere are several observations that motivate the ap-\nproach we take for addressing the unsupervised learning\ntask. First, we notice that unsupervised learning methods\nare generally more effective when considering video input,\nin which objects satisfy spatio-temporal consistency, with\nsmooth variations in shape, appearance and location over\ntime.\nFor that matter it is usually harder to learn about\nobjects from collections of images that are independently\ntaken. This motivates the inclusion of the video discov-\nery pathway, for which there are available several published\nmethods that could be used. We should also keep in mind\nthat the video discovery module should not use pre-trained\nfeatures on manually labeled ground truth, if we want to de-\nvelop a fully unsupervised method. On the contrary, the vi-\nsual cues and features used by the teacher should be as sim-\nple as possible, such as individual pixel colors. These are\nprecisely the features used by the teacher pathway of our\nchoice, the VideoPCA algorithm introduced in [40]. That\nmethod has the added quality of being very fast and reason-\nably accurate.\nSecond, if we want the student pathway to learn general\nprinciples about objects in images, we need to limit its ac-\ncess to a single input image. Otherwise, if given the entire\nvideo as input, a powerful deep network would easily overﬁt\nwhen trained to predict the teacher’s output.\nThe most important question that needs to be answered\nis whether the student can outperform its teacher. If this is\nindeed the case, then the student, processing a single image,\nhas an important quality, besides the speed advantage over\nthe teacher (which needs to process an entire video). Once\nthe student progresses beyond the capabilities of its teacher,\nwe could indeed envision the potential practical advantages\nof unsupervised learning - especially when there is so much\nunlabeled video data available. Therefore, we ﬁrst have to\nmake sure that the student receives only the best quality in-\nput possible from the teacher. For that we add an extra mod-\nule for unsupervised soft masks selection. It is based on a\nsimple and intuitive measure of quality, explained in detail\nlater, which does a good job at ordering masks with respect\nto their true quality.\nThen, we also need to make sure that the student sees\nas much training data as possible. For that, we design an\nautomatic data augmentation module, which creates extra\ntraining data by randomly scaling and shifting the masks\nprovided by the teacher after the mask selection procedure.\nHaving this in mind, one of the important ﬁndings in\nour experiments is that in all our tests the student indeed\noutperforms its teacher.\nMoreover, it achieves state of\nthe art results on two important and different benchmarks.\nWe believe that the success of this unsupervised learning\nparadigm is due to the fact that the student is forced to cap-\nture from appearance only (as it is limited to a single image)\nvisual properties and cues that are good predictors for the\npresence of objects.\n3. System architecture\nWe now present in more detail the architecture of our\nsystem, module by module, as it is presented in Figure 1.\n3.1. Teacher path: unsupervised discovery in video\nThere are several methods already available for discov-\nering objects and salient regions in images and videos [4, 6,\n10, 12, 8, 3], with reasonably good performance. More re-\ncent methods for foreground objects discovery such as [26,\nFigure 2. The ”student” deep convolutional network architecture\nthat processes single images. It is trained to predict the unsuper-\nvised labels given by the teacher pathway, frame by frame. We\nobserved that by adding at the last level the original input and mid-\nlevel features (skip connections) and resizing them appropriately,\nthe performance increases.\n40] are both relatively fast and accurate, with testing time\nabove 4 seconds per frame. However, this time is still long\nand prohibitive for training a deep neural net (the student\npathway) that requires millions of images to train. For that\nreason we chose the VideoPCA algorithm proposed in [40],\nwith code available online, which is an important part of\nthat framework. It has lower accuracy than the full system\nin [40], but it is much faster, running at 50 −100 fps. At\nthis speed we can produce one million unsupervised soft\nsegmentations in a reasonable time of about 5-6 hours.\nVideoPCA models the background in video frames with\nPrincipal Component Analysis. It ﬁnds initial foreground\nregions as parts of the frames that are not reconstructed well\nwith the PCA model. Foreground objects are smaller than\nthe background and have more complex movements, which\nmake the foreground less likely to be captured well by the\nﬁrst principal components. The initial soft masks are used\nto learn color models of foreground and background, which\nare then improved by independent pixel-wise classiﬁcation\nbased on color. For more details the reader is invited to\nconsult [40].\n3.2. Student path: single-image segmentation\nThe student processing pathway (Figure 1) consists of\na deep convolutional network, with ten layers (seven con-\nvolutional, two pooling and one fully connected layer) and\nskip connections as shown in Figure 2. Skip connections\nhave proved to provide a boost in the network’s perfor-\nmance [31, 28]. We also observed a slight improvement\nin our case (≈%1). The net takes as input a 128 × 128\ncolor image (along with its hue, saturation, derivatives w.r.t.\nx and y) and produces a 32 × 32 soft segmentation of the\nmain objects present in the image. While it does not iden-\ntify the particular object classes, it learns from the unsu-\npervised soft-masks provided by the teacher to detect and\nsoftly segment the main foreground objects present, regard-\nless of their particular category, one frame at a time. Thus,\nas shown in experiments, it is also able to detect and seg-\nment classes it has never seen before.\nWe treat foreground object segmentation as a regression\nproblem, where the soft mask given by the unsupervised\nvideo segmentation system acts as the desired output. Let\nI be the input RGB image (a video frame) and Y be the\ncorresponding 0-255 valued soft segmentation given by the\nunsupervised teacher pathway for that particular frame. The\ngoal of our network is to predict a soft segmentation mask\nˆY of width W = 32 and height H = 32, that approximates\nas good as possible the mask Y. In other words, for each\npixel in the output image, we predict a 0-255 value, so that\nthe total difference between Y and ˆY is minimized. So,\ngiven a set of N training examples, let I(n) be the input\nimage (a video frame), ˆY(n) be the predicted output mask\nfor I(n), Y(n) the soft segmentation mask (corresponding\nto I(n)) and w the network parameters. Y(n) is produced\nby the video discoverer by processing the whole video that\nI(n) belongs to. Then, our loss is:\nL(w) = 1\nN\nN\nX\nn=1\nW ×H\nX\np=1\n(Y(n)\np\n−ˆY(n)\np (w, I(n)))\n2\n(1)\nwhere Y(n)\np\nand ˆY(n)\np\ndenotes the p-th pixel from Y(n),\nrespectively ˆY(n). We observed that in our tests, the L2 loss\nperformed better than the cross-entropy loss.\nWe train our network using the Tensorﬂow [1] frame-\nwork with the Adam optimizer [19]. All our models are\ntrained end-to-end using a ﬁxed learning rate of 0.001. The\ntraining time for a given model is about 3 days on a Nvidia\nGeForce GTX 1080 GPU.\nPost-processing:\nAs we stated before, our CNN outputs\na 32 × 32 soft mask. In order to be able to fairly compare\nourselves against other methods, we have two different post\nprocessing steps: 1) bounding box ﬁtting and 2) segmen-\ntation reﬁnement. For ﬁtting a box around our soft mask,\nFigure 3. Purity of soft masks vs degree of selection. The more\nselective we are, the purity of the training frames as compared to\nthe ground truth bounding boxes improves. Note that our selection\nmethod is not perfect and some low quality segmentations have\nhigh scores or we remove some good segmentations.\nwe ﬁrst up-sample the 32 × 32 output mask to the origi-\nnal size of the image, then threshold the output, determine\nthe connected components, ﬁlter out the small ones and ﬁ-\nnally ﬁt a tight box around each of the remaining compo-\nnents.\nHowever, if we are interested in obtaining a ﬁne\nobject segmentation, we use the OpenCV implementation\nof the GrabCut [34] method to reﬁne our soft mask, up-\nsampled to the original size.\n3.3. Unsupervised soft masks selection\nThe performance of the single-image processing path-\nway is inﬂuenced by the quality of the soft masks provided\nas labels by the video discovery path.\nThe cleaner and\nsharper masks provided by the teacher, the more chances\nthe student has to actually learn to segment well the objects\nin images. VideoPCA used by the video processing path\nusually has good results if the object present in the video\nstands out against the background scene, in terms of motion\nand appearance. However, if the object is occluded at some\npoint in the video, or if it does not move w.r.t the scene or\nif it has a very similar appearance to its background, the\nresulting soft masks might be poor. We used a simple mea-\nsure of masks quality based on the following observation:\nwhen masks are close to the ground truth, the mean of their\nnonzero values is usually high - which means that when the\ndiscoverer is generally conﬁdent about a certain mask it is\nmore likely to be closer to the true segmentation. The mean\nvalue of non-zero pixels in the soft mask is then used as a\nscore indicator for each segmented frame.\nNext we sort all soft masks in the entire training dataset\n(e.g.\nVID [37], YTO [29]) in descending order of their\nmean score and keep only the top k percent. In this way,\nwe obtain a very simple but completely unsupervised se-\nlection method. In Figure 3 we present the dependency of\nsegmentation performance w.r.t ground truth object boxes\n(used only for evaluation) vs. the percentile k of masks\nkept after the automatic selection. In other words, the fewer\nframes we select the more likely it is that they are correctly\nsegmented. This procedure is not perfect, however, so we\nsometimes remove good segmentations during this masks\nselection step. Even though we can expect to improve the\nquality of the unsupervised masks by drastically pruning\nthem, the fewer we are left with, the less training data we\nhave, which increases the chance of overﬁtting. Therefore\nthere is a price to pay. We make up for the losses in training\ndata by augmenting the set of training masks (explained in\nSection 3.4) and by bringing in unlabeled videos from other\ndatasets. Thus, the more selective we are about what masks\nto accept for training, the more videos we need to collect\nand process with the teacher pathway, in order to improve\ngeneralization.\n3.4. Data augmentation\nAnother drawback of VideoPCA is that it can only detect\nthe main object if it is close to the center of the image. The\nassumption that the foreground is close to the center is often\ntrue and indeed helps that method to produce soft masks\nwith a relatively high precision, but it fails when the object\nis not in the center, therefore its recall is relatively low. Our\ndata augmentation procedure also addresses this limitation.\nThis module can be concisely described as follows: scale\nthe input image and the corresponding soft mask given by\nthe video discovery framework at a higher resolution (160×\n160) and randomly crop 128 × 128 patches from the scaled\nversion. Finally, we down-scale each soft mask to 32 ×\n32. This would produce slightly larger objects at locations\nthat cover the whole image area, not just the center. As\nour experiments show, the student net is able to see objects\nat different locations in the image, unlike its raw teacher,\nwhich is strongly biased towards the image center. Data\nselection, along with data augmentation of the training set\nsigniﬁcantly improves unsupervised learning, as shown in\nthe experiments section (Section 4).\n4. Experimental analysis\nThe experiments we conducted aim to highlight vari-\nous aspects about the performance of our method. Firstly,\nwe compare the quality of the segmentations obtained by\nthe feed-forward CNN against its teacher, VideoPCA (Sec-\ntion 4.1). Secondly, we tested that adding extra unlabeled\nvideos improves performance (Section 4.2).\nFinally, we\ncompare the performance of our unsupervised system to\nstate of the art approaches in the ﬁeld for object discov-\nery in video, testing on the YouTube Objects Dataset [29]\nbenchmark and object discovery in images, testing on the\nObject Discovery in Internet images [35] benchmark. (Sec-\ntion 4.3).\nMethod\nF1 measure\nDataset\nVideoPCA [40]\n41.83\n-\nBaseline\n51.17\nVID\nBaseline\n51.9\nVID + YTO\nReﬁned\n52.51\nVID\nData selection 5%\n53.20\nVID\nData selection 10%\n53.82\nVID\nData selection 30%\n53.67\nVID\nData selection 10%\n54.53\nVID + YTO\nTable 1. Results on the VID dataset [37]. The ”dataset” column\nrefers to the datasets used for training the student network. Our\nbaseline model is represented by a classic CNN having only the\nRGB image as input and no skip-connections. The reﬁned model\nis our ﬁnal student CNN model as presented in Figure 2. The data\nselection entries refer to the percentage of kept soft masks after ap-\nplying our selection method. All soft masks selection experiments\nwere conducted using the reﬁned model. We want to highlight that\nthe overall system performance improves with the amount of se-\nlectivity. This shows that a simple quality measure used for soft\nmask selection can improve the performance of the CNN image-\nbased pathway and that the data augmentation module makes up\nfor the frames lost during the selection process.\n4.1. Unsupervised learning from ImageNet\nIt is a well known fact that the performance of a convolu-\ntional network strongly depends on the amount of data used\nfor training. Because of this, we chose to use as our primary\ntraining dataset the ImageNet Object Detection from Video\n(VID) dataset [37]. VID is one of the largest video datasets\npublicly available, being fully annotated with ground truth\nbounding boxes. The large set of annotations available al-\nlowed us to have a thorough evaluation of our unsupervised\nsystem. The dataset consists of about 4000 videos, having a\ntotal of about 1.2M frames. The videos contain objects that\nbelong to 30 different classes. Each frame could have zero,\none or multiple objects annotated. The benchmark chal-\nlenge associated with this dataset focuses on the supervised\nobject detection and recognition problem, which is differ-\nent from the problem that we tackle here. Our system is not\ntrained to identify different object categories. On the VID\ndataset we evaluated the student CNN against its teacher\npathway. We measure performance of soft-masks by maxi-\nmum F-measure computed w.r.t ground truth bounding box,\nby considering pixels inside the bounding box as true posi-\ntives and those outside as true negatives. This simple metric\nallows us to use the soft masks directly, without any post-\nprocessing steps.\nWe tested our unsupervised system on the validation split\nof the VID dataset. As it can be seen from Table 1 our sys-\ntem outperforms its teacher (VideoPCA) by a very signif-\nicant margin. Also, in Figure 4 we present some qualita-\ntive results on this dataset as compared to VideoPCA. We\nFigure 4. Visual results on the VID dataset [37] compared to the teacher method.\nA: current frame, B: soft mask produced by\nVideoPCA [40] for the current frame, after processing the entire video, C: thresholded soft mask produced by our network, D: segmentation\nmask produced after reﬁning the soft output of our network with GrabCut [34], E: bounding box obtained from the soft segmentation mask;\nF: ground truth bounding box.\ncan see that the masks produced by VideoPCA are of lower\nquality, having holes, non-smooth boundaries and strange\nshapes that are far from the idea of ”objectness” [2]. In\ncontrast, the student learns general shape and appearance\ncharacteristics of objects in images reminding of the group-\ning principles governing the basis of visual perception as\nstudied by the Gestalt psychologists [33]. Note that object\nmasks produced by the student are simpler, with very few\nholes, have nicer and smoother shapes and capture well the\nﬁgure-ground contrast and organization. Another interest-\ning observation is that the network is able to detect multi-\nple objects, a feature that is less commonly achieved by the\nteacher.\n4.2. Adding more data\nWe also tested how adding more unlabeled data affects\nthe overall performance of our system.\nTherefore, we\nadded the Youtube Objects(YTO) dataset to the existing\nVID dataset. The YTO dataset is a weakly annotated dataset\nthat consists of about 2500 videos, having a total of about\n720K frames, divided into 10 classes. Adding more unla-\nbeled videos (from YouTube Objects dataset, without an-\nnotations) to the unsupervised training set clearly improves\nperformance as reported in Tables 3, 1 and 4. The capacity\nof our system to improve its performance in the presence of\nunlabeled data, without degradation or catastrophic forget-\nting is mainly due to the robustness of the teacher pathway\ncombined with data selection and augmentation, in conjunc-\ntion with the tendency of the single-image CNN net to im-\nprove over its teacher.\nAs it comes to the soft mask selection, our experiments\nshow that we obtain the best overall results by using the\ntop 10% soft masks with data augmentation. Because of\nthis, all other experiments are conducted using this setup\nfor each dataset.\n4.3. Comparisons with other methods\nSingle image discovery methods\nNext, we compare our\nunsupervised system with state of the art methods designed\nfor the task of object discovery in collections of images,\nthat might contain one or a few main object categories of\ninterest. A representative current benchmark in this sense\nis the Object Discovery in Internet images dataset. This set\ncontains Internet images and it is annotated with high detail\nsegmentation masks. In order to enable comparison with\nprevious methods, we use the 100 images subsets.\nThe methods evaluated on this dataset, in the literature,\naim to either discover the bounding box of the main object\nin given image, or its ﬁne segmentation mask. We eval-\nuate our system on both. Different from other methods,\nwe do not need a collection of images during testing, since\neach image is processed independently by our system, at\ntest time. Therefore, our performance is not affected by the\nstructure of the image collection or the number of classes of\ninterest being present in the collection.\nFor evaluating the detection of bounding boxes the\nFigure 5. Visual results on the Object Discovery dataset. A: input image, B: segmentation obtained by [14], C: segmentation obtained\nby [35], D: thresholded soft mask produced by our network, E: segmentation mask produced after reﬁning the soft output of our network\nwith GrabCut [34], F: ground truth segmentation.\nMethod\nAirplane\nCar\nHorse\nAvg\n[18]\n21.95\n0.00\n16.13\n12.69\n[13]\n32.93\n66.29\n54.84\n51.35\n[14]\n57.32\n64.04\n52.69\n58.02\n[35]\n74.39\n87.64\n63.44\n75.16\n[41]\n71.95\n93.26\n64.52\n76.58\n[7]\n82.93\n94.38\n75.27\n84.19\n[7](mixed-class)\n81.71\n94.38\n70.97\n82.35\nOursVID\n93.90\n93.26\n70.97\n86.04\nOursVID+YTO\n87.80\n95.51\n74.19\n85.83\nTable 2. Results on the Object Discovery in Internet images [35]\ndataset (CorLoc metric). OursVID represents our network trained\nusing the VID dataset (with 10% selection), while OursVID+YTO\nrepresents our network trained on VID and YTO datasets (with\n10% selection).\nmost used metric is CorLoc deﬁned as the percentage\nof images correctly localized according to the PASCAL\ncriterion: Bp∩BGT\nBp∪BGT ≥0.5, where BP is the predicted bound-\ning box and BGT is the ground truth bounding box. In Ta-\nble 2 we present the performance of our method as com-\npared to other unsupervised object discovery methods in\nterms of CorLoc on the Object Discovery dataset. We com-\npare our predicted box against the tight box ﬁtted around\nthe ground-truth segmentation as done in [7, 41]. Our sys-\ntem can be considered in the mixed class category: it does\nAirplane\nCar\nHorse\nP\nJ\nP\nJ\nP\nJ\n[18]\n80.20\n7.90\n68.85\n0.04\n75.12\n6.43\n[13]\n49.25\n15.36\n58.70\n37.15\n63.84\n30.16\n[14]\n47.48\n11.72\n59.20\n35.15\n64.22\n29.53\n[35]\n88.04\n55.81\n85.38\n64.42\n82.81\n51.65\n[5]\n90.25\n40.33\n87.65\n64.86\n86.16\n33.39\nOurs1\n90.92\n62.76\n85.15\n66.39\n87.11\n54.59\nOurs2\n91.41\n61.37\n86.59\n70.52\n87.07\n55.09\nTable 3. Results on the Object Discovery in Internet images [35]\ndataset (P, J metric). Ours1 represents our network trained using\nthe VID dataset (with 10% selection), while Ours2 represents our\nnetwork trained on VID and YTO datasets (with 10% selection).\nWe observe that Ours2 has better results with mean P of 88.36\nand mean J of 62.33 compared to Ours1 (mean P: 87.73, mean J:\n61.25).\nnot depend on the structure of the image collection. It treats\neach image independently. The performance of the other\nalgorithms degrades as the number of main categories in-\ncreases in the collection (some are not even tested by their\nauthors on the mixed-class case).\nWe obtain state of the art results on all classes (in the\nmixed class case), improving by a signiﬁcant margin over\nthe method of [7] in the mixed class case. When the method\nin [7] is allowed to see a collection of images that are lim-\nFigure 6. Qualitative results on the Object Discovery in Internet images [35] dataset. For each example we show the input RGB image\n(ﬁrst and third row) and immediately below (second and fourth row) our reﬁned segmentation result obtained by applying GrabCut on the\nsoft segmentation mask predicted by our network. Note that our method produces good quality segmentation results, even in cases with\ncluttered background.\nMethod\nAero\nBird\nBoat\nCar\nCat\nCow\nDog\nHorse\nMbike\nTrain\nAvg\nTime\nVersion\n[29]\n51.7\n17.5\n34.4\n34.7\n22.3\n17.9\n13.5\n26.7\n41.2\n25.0\n28.5\nN/A\nv1 [29]\n[26]\n65.4\n67.3\n38.9\n65.2\n46.3\n40.2\n65.3\n48.4\n39.0\n25.0\n50.1\n4s\n[16]\n64.3\n63.2\n73.3\n68.9\n44.4\n62.5\n71.4\n52.3\n78.6\n23.1\n60.2\nN/A\nOursVID\n69.8\n59.7\n65.4\n57.0\n50.0\n71.7\n73.3\n46.7\n32.4\n34.9\n56.1\n0.04s\nOursVID+YTO\n77.0\n67.5\n77.2\n68.4\n54.5\n68.3\n72.0\n56.7\n44.1\n34.9\n61.6\n0.04s\nOursVID+YTO\n75.7\n56.0\n52.7\n57.3\n46.9\n57.0\n48.9\n44.0\n27.2\n56.2\n52.2\n0.04s\nv2.2 [17]\nTable 4. Results on Youtube Objects dataset [29]. OursVID represents our network trained using the VID dataset (with 10% selection),\nwhile OursVID+YTO represents our network trained on VID and YTO datasets (with 10% selection). Note that our system has a signiﬁcantly\nlower test time than [26] which we estimate that is the fastest method.\nited to a single majority class, its performance improves and\noutperforms ours on one class. However, the comparison is\nnot truly appropriate since our method has no other infor-\nmation necessary besides the input image, at test time.\nWe also tested our system on the task of ﬁne foreground\nobject segmentation and compared to the best performers in\nthe literature on the Object Discovery dataset in Table 3. For\nreﬁning our soft masks we apply the GrabCut method, as it\nis available in OpenCV. We evaluate based on the same P, J\nevaluation metric as described by Rubinstein et al. [35] - the\nhigher P and J, the better. P refers to the per pixel precision,\nwhile J is the Jaccard similarity (the intersection over union\nof the result and ground truth segmentations). In Figure 6\nand 5 we present some qualitative samples from each class,\nwhile in Figure 7, 8, 9 we present the qualitative results on\nall the images that we tested.\nVideo discovery methods\nWe also performed compar-\nisons with methods speciﬁcally designed for object discov-\nery in video.\nFor this, we choose the YouTube Objects\nDataset and compared to the best performers on this dataset\nin the literature (Table 4). Evaluations are conducted on\nboth versions of YouTube Objects dataset, YTOv1 [29] and\nYTOv2.2 [17]. On YTOv1 we follow the same experimen-\ntal setup as [16, 29], by running experiments only on the\ntraining videos. It is important to stress out again, the fact\nthat, while the methods presented here for comparison have\naccess to whole video shots, ours only needs a single image\nat test time. Despite this limitation, our method outperforms\nthe others on 8 out of 10 classes and has the best overall av-\nerage performance. It is also important to note that our CNN\nfeed-forward net processes each image in 0.04 sec, being at\nleast one to two orders of magnitude faster than all other\nmethods (see Table 4). It is also important to note that in\nall our comparisons, while our system is faster at test time,\nit takes much longer during its unsupervised training phase\nand requires large quantities of unsupervised training data.\nIn Table 5 we report additional experiments, on all an-\nnotated frames from YouTube Objects in order to compare\nwith the full system of [40], where VideoPCA was intro-\nduced. We also report comparisons with VideoPCA alone\non the same train+test split. For VideoPCA we also ﬁtted a\nFigure 7. Qualitative results on the 100 airplane image subset from Object Discovery in Internet images [35] dataset.\nFigure 8. Qualitative results on the 100 car image subset from Object Discovery in Internet images [35] dataset.\nFigure 9. Qualitative results on the 100 horse image subset from Object Discovery in Internet images [35] dataset.\nMethod\nAero\nBird\nBoat\nCar\nCat\nCow\nDog\nHorse\nMbike\nTrain\nAvg\nWhole method [40]\n38.3\n62.5\n51.1\n54.9\n64.3\n52.9\n44.3\n43.8\n41.9\n45.8\n49.9\nTeacher(VideoPCA) [40] + tight box\n69.6\n55.8\n64.9\n50.5\n44.8\n43.2\n48.6\n37.4\n17.9\n22.0\n45.5\nOurs\n71.9\n61.5\n75.4\n70.3\n53.0\n59.3\n70.6\n56.0\n37.9\n39.0\n59.5\nTable 5. Results on the whole (training + testing) YouTube Objects dataset [29]. Our system outperforms both VideoPCA (used in the\nteacher pathway) and the full method from [40] by very signiﬁcant margins (about 10% and 14%, respectively).\ntight bounding box. For [40] we report the results presented\nin their paper.\n5. Conclusions and Future Work\nWe have shown in extensive experiments that it is pos-\nsible to use a relatively simple method for unsupervised\nobject discovery in video to train a powerful deep neural\nnetwork for detection and segmentation of objects in sin-\ngle images. The result is interesting and encouraging and\nshows how a system could learn in a completely unsuper-\nvised fashion, general visual characteristics that predict well\nthe presence and shape of objects in images. The network\nessentially discovers appearance object features from single\nimages, at different levels of abstraction, that are strongly\ncorrelated with the spatiotemporal consistency of objects in\nvideo.\nThe student network, during the unsupervised training\nphase is also able to signiﬁcantly outperform its teacher, by\nlearning such general ”objectness” characteristics that are\nwell beyond the capabilities of its teacher. These character-\nistics include good form, closure, smooth contours, as well\nas contrast with its background. What the simpler teacher\ndiscovers over time, the deep, complex student is able to\nlearn across several layers of image features at different lev-\nels of abstraction. Thus, our unsupervised learning model,\ntested in extensive experiments, brings a valuable contri-\nbution to the unsupervised learning problem in vision re-\nsearch.\nAcknowledgements:\nThis work was supported by UE-\nFISCDI, under project PN-III-P4-ID-ERC-2016-0007.\nReferences\n[1] M. Abadi et al. Tensorﬂow: Large-scale machine learning\non heterogeneous systems, 2015. Software available from\ntensorﬂow.org, 2015. 4\n[2] B. Alexe, T. Deselaers, and V. Ferrari. What is an object? In\nIEEE CVPR, 2010. 6\n[3] O. Barnich and M. Van Droogenbroeck. Vibe: A universal\nbackground subtraction algorithm for video sequences. Im-\nage Processing, 20(6), 2011. 3\n[4] A. Borji, D. Sihite, and L. Itti. Salient object detection: A\nbenchmark. In ECCV, 2012. 3\n[5] X. Chen et al. Enriching visual knowledge bases via object\ndiscovery and segmentation. In CVPR, 2014. 7\n[6] M. Cheng et al. Global contrast based salient region detec-\ntion. PAMI, 37(3), 2015. 3\n[7] M. Cho et al. Unsupervised object discovery and localiza-\ntion in the wild: Part-based matching with bottom-up region\nproposals. In CVPR, 2015. 1, 7\n[8] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati. Detect-\ning moving objects, ghosts, and shadows in video streams.\nPAMI, 25(10), 2003. 3\n[9] T. Deselaers, B. Alexe, and V. Ferrari. Weakly supervised\nlocalization and learning with generic knowledge.\nIJCV,\n100(3), 2012. 1\n[10] X. Hou and L. Zhang. Saliency detection: A spectral residual\napproach. In CVPR, 2007. 3\n[11] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering:\na review. ACM computing surveys (CSUR), 31(3):264–323,\n1999. 1\n[12] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.\nSalient object detection: A discriminative regional feature\nintegration approach. In CVPR, 2013. 3\n[13] A. Joulin, F. Bach, and J. Ponce. Discriminative clustering\nfor image co-segmentation. In CVPR, 2010. 1, 7\n[14] A. Joulin, F. Bach, and J. Ponce. Multi-class cosegmentation.\nIn CVPR, 2012. 1, 7\n[15] A. Joulin, K. Tang, and L. Fei-Fei. Efﬁcient image and video\nco-localization with frank-wolfe algorithm. In ECCV 2014.\n2014. 1\n[16] Y. Jun Koh, W.-D. Jang, and C.-S. Kim. Pod: Discovering\nprimary objects in videos based on evolutionary reﬁnement\nof object recurrence, background, and primary object mod-\nels. In CVPR, 2016. 8\n[17] V. Kalogeiton, V. Ferrari, and C. Schmid. Analysing domain\nshift factors between videos and images for object detection.\nPAMI, 38(11), 2016. 8\n[18] G. Kim, E. Xing, L. Fei-Fei, and T. Kanade. Distributed\ncosegmentation via submodular optimization on anisotropic\ndiffusion. In ICCV, 2011. 1, 7\n[19] D. Kingma and J. Ba. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980, 2014. 4\n[20] D. Kuettel, M. Guillaumin, and V. Ferrari.\nSegmentation\npropagation in imagenet. In ECCV, 2012. 1\n[21] M. Leordeanu, R. Collins, and M. Hebert.\nUnsupervised\nlearning of object features from video sequences. In CVPR,\n2005. 1\n[22] D. Li, W.-C. Hung, J.-B. Huang, S. Wang, N. Ahuja, and\nM.-H. Yang. Unsupervised visual representation learning by\ngraph-based consistent constraints. In ECCV, 2016. 1\n[23] D. Liu and T. Chen. A topic-motion model for unsupervised\nvideo object discovery. In CVPR, 2007. 1\n[24] I. Misra et al. Shufﬂe and learn: unsupervised learning using\ntemporal order veriﬁcation. In ECCV, 2016. 1\n[25] M. Nguyen, L. Torresani, F. D. la Torre, and C. Rother.\nWeakly supervised discriminative localization and classiﬁ-\ncation: a joint learning process. In CVPR, 2009. 1\n[26] A. Papazoglou and V. Ferrari. Fast object segmentation in\nunconstrained video.\nIn Proceedings of the IEEE ICCV,\npages 1777–1784, 2013. 3, 8\n[27] D. Parikh and T. Chen. Unsupervised identiﬁcation of mul-\ntiple objects of interest from multiple images: discover. In\nAsian Conference on Computer Vision, 2007. 1\n[28] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll´ar. Learn-\ning to reﬁne object segments. In ECCV. Springer, 2016. 4\n[29] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Fer-\nrari. Learning object class detectors from weakly annotated\nvideo. In CVPR, pages 3282–3289. IEEE, 2012. 4, 5, 8, 12\n[30] F. Radenovi´c, G. Tolias, and O. Chum. Cnn image retrieval\nlearns from bow: Unsupervised ﬁne-tuning with hard exam-\nples. In ECCV. Springer, 2016. 1\n[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made\neasier by linear transformations in perceptrons. In AISTATS,\nvolume 22, pages 924–932, 2012. 4\n[32] M. Rochan and Y. Wang. Efﬁcient object localization and\nsegmentation in weakly labeled videos. In Advances in Vi-\nsual Computing, pages 172–181. Springer, 2014. 1\n[33] I. Rock and S. Palmer. Gestalt psychology. Sci Am, 263:84–\n90, 1990. 6\n[34] C. Rother, V. Kolmogorov, and A. Blake. Grabcut: Interac-\ntive foreground extraction using iterated graph cuts. In ACM\ntransactions on graphics (TOG), volume 23, pages 309–314.\nACM, 2004. 4, 6, 7\n[35] M. Rubinstein, A. Joulin, J. Kopf, and C. Liu. Unsupervised\njoint object discovery and segmentation in internet images.\nIn CVPR, 2013. 1, 5, 7, 8, 9, 10, 11\n[36] J. Rubio, J. Serrat, and A. L´opez. Video co-segmentation. In\nACCV, 2012. 1\n[37] O. Russakovsky et al. Imagenet large scale visual recognition\nchallenge. International Journal of Computer Vision, 115(3),\n2015. 4, 5, 6\n[38] P. Siva, C. Russell, T. Xiang, and L. Agapito. Looking be-\nyond the image: Unsupervised learning for object saliency\nand detection. In CVPR, 2013. 1\n[39] J. Sivic, B. Russell, A. Efros, A. Zisserman, and W. Freeman.\nDiscovering objects and their location in images. In ICCV,\n2005. 1\n[40] O. Stretcu and M. Leordeanu. Multiple frames matching for\nobject discovery in video. In BMVC, 2015. 1, 2, 3, 5, 6, 8,\n12\n[41] K. Tang, A. Joulin, L.-J. Li, and L. Fei-Fei. Co-localization\nin real-world images. In CVPR, 2014. 7\n[42] S. Vicente, C. Rother, and V. Kolmogorov. Object coseg-\nmentation. In CVPR, 2011. 1\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-03-31",
  "updated": "2017-03-31"
}