{
  "id": "http://arxiv.org/abs/1704.03952v4",
  "title": "Virtual to Real Reinforcement Learning for Autonomous Driving",
  "authors": [
    "Xinlei Pan",
    "Yurong You",
    "Ziyan Wang",
    "Cewu Lu"
  ],
  "abstract": "Reinforcement learning is considered as a promising direction for driving\npolicy learning. However, training autonomous driving vehicle with\nreinforcement learning in real environment involves non-affordable\ntrial-and-error. It is more desirable to first train in a virtual environment\nand then transfer to the real environment. In this paper, we propose a novel\nrealistic translation network to make model trained in virtual environment be\nworkable in real world. The proposed network can convert non-realistic virtual\nimage input into a realistic one with similar scene structure. Given realistic\nframes as input, driving policy trained by reinforcement learning can nicely\nadapt to real world driving. Experiments show that our proposed virtual to real\n(VR) reinforcement learning (RL) works pretty well. To our knowledge, this is\nthe first successful case of driving policy trained by reinforcement learning\nthat can adapt to real world driving data.",
  "text": "PAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n1\nVirtual to Real Reinforcement Learning for\nAutonomous Driving\nXinlei Pan∗1\nxinleipan@berkeley.edu\nYurong You∗2\nyurongyou@sjtu.edu.cn\nZiyan Wang3\nzy-wang13@mails.tsinghua.edu.cn\nCewu Lu2\nlu-cw@cs.sjtu.edu.cn\n∗Indicates equal contribution.\n1 University of California, Berkeley\nBerkeley, CA, USA\n2 Shanghai Jiao Tong University\nShanghai, China\n3 Tsinghua University\nBeijing, China\nAbstract\nReinforcement learning is considered as a promising direction for driving policy\nlearning. However, training autonomous driving vehicle with reinforcement learning in\nreal environment involves non-affordable trial-and-error. It is more desirable to ﬁrst train\nin a virtual environment and then transfer to the real environment. In this paper, we pro-\npose a novel realistic translation network to make model trained in virtual environment\nbe workable in real world. The proposed network can convert non-realistic virtual image\ninput into a realistic one with similar scene structure. Given realistic frames as input,\ndriving policy trained by reinforcement learning can nicely adapt to real world driving.\nExperiments show that our proposed virtual to real (VR) reinforcement learning (RL)\nworks pretty well. To our knowledge, this is the ﬁrst successful case of driving policy\ntrained by reinforcement learning that can adapt to real world driving data.\nAutonomous driving aims to make a vehicle sense its environment and navigate without\nhuman input. To achieve this goal, the most important task is to learn the driving policy\nthat automatically outputs control signals for steering wheel, throttle, brake, etc., based on\nobserved surroundings. The straight-forward idea is end-to-end supervised learning [3, 4],\nwhich trains a neural network model mapping visual input directly to action output, and the\ntraining data is labeled image-action pairs. However, supervised approach usually requires\nlarge amount of data to train a model [31] that can generalize to different environments. Ob-\ntaining such amount of data is time consuming and requires signiﬁcant human involvement.\nBy contrast, reinforcement learning learns by a trial-and-error fashion, and does not require\nexplicit supervision from human. Recently, reinforcement learning has been considered as a\npromising technique to learn driving policy due to its expertise in action planing [15, 23, 25].\nHowever, reinforcement learning requires agents to interact with environments, and un-\ndesirable driving actions would happen. Training autonomous driving cars in real world\nwill cause damages to vehicles and the surroundings. Therefore, most of current research\nin autonomous driving with reinforcement learning focus on simulations [15, 18, 25] rather\nthan training in real world. While an agent trained with reinforcement learning achieves\nc⃝2017. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:1704.03952v4  [cs.AI]  26 Sep 2017\n2\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\nFigure 1: Framework for virtual to real reinforcement learning for autonomous driving. Vir-\ntual images rendered by a simulator (environment) are ﬁrst segmented to scene parsing repre-\nsentation and then translated to synthetic realistic images by the proposed image translation\nnetwork (VISRI). Agent observes synthetic realistic images and takes actions. Environment\nwill give reward to the agent. Since the agent is trained using realistic images that are visually\nsimilar to real world scenes, it can nicely adapt to real world driving.\nnear human-level driving performance in virtual world [18], it may not be applicable to real\nworld driving environment, since the visual appearance of virtual simulation environment is\ndifferent from that of real world driving scene.\nWhile virtual driving scenes have a different visual appearance compared with real driv-\ning scenes, they share similar scene parsing structure. For example, virtual and real driving\nscenes may all have roads, trees, buildings, etc., though the textures may be signiﬁcantly\ndifferent. Therefore, it is reasonable that by translating virtual images to their realistic coun-\nterparts, we can obtain a simulation environment that looks very similar to the real world in\nterms of both scene parsing structure and object appearance. Recently, generative adversar-\nial network (GAN) [9] has drawn a lot of attention in image generation. The work by [11]\nproposed an image-to-image translation network that can translate images from one domain\nto another using paired data from both domains. However, it is very hard to ﬁnd paired\nvirtual-real world images for driving, making it difﬁcult to apply this method to our case of\ntranslating virtual driving images to realistic ones.\nIn this paper, we propose a realistic translation network to help train self-driving car\nentirely in virtual world that can adapt to real world driving environment. Our proposed\nframework (shown in Figure 1) converts virtual images rendered by the simulator to a re-\nalistic one and train the reinforcement learning agent with the synthesized realistic images.\nThough virtual and realistic images have a different visual appearance, they share a com-\nmon scene parsing representation (segmentation map of roads, vehicles etc.). Therefore, we\ncan translate virtual images to realistic images by using scene parsing representation as the\ninterim. This insight is similar to natural language translation, where semantic meaning is\nthe interim between different languages. Speciﬁcally, our realistic translation network in-\ncludes two modules. The ﬁrst one is a virtual-to-parsing or virtual-to-segmentation module\nthat produces a scene parsing representation of input virtual image. The second one is a\nparsing-to-real network that translates scene parsing representations into realistic images.\nWith realistic translation network, reinforcement learning model learnt on the realistic driv-\ning data can nicely apply to real world driving.\nTo demonstrate the effectiveness of our method, we trained our reinforcement learning\nmodel by using the realistic translation network to ﬁlter virtual images to synthetic realistic\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n3\nimages and feed these realistic images as state inputs. We further compared with supervised\nlearning and other reinforcement learning approaches that use domain randomization [22].\nOur experiments illustrate that a reinforcement learning model trained with translated re-\nalistic images has better performance than reinforcement learning model trained with only\nvirtual input and virtual to real reinforcement learning with domain randomization.\n1\nRelated Work\nSupervised Learning for Autonomous Driving. Supervised learning methods are obvi-\nously straightforward ways to train autonomous vehicles. ALVINN [19] provides an early\nexample of using neural network for autonomous driving. Their model is simple and di-\nrect, which maps image inputs to action predictions with a shallow network. Powered by\ndeep learning especially a convolutional neural network, NVIDIA [3] recently provides an\nattempt to leverage driving video data for simple lane following task. Another work by [4]\nlearns a mapping between input images to a number of key perception indicators, which are\nclosely related to the affordance of a driving state. However, the learned affordance must\nbe associated with actions through hand-engineered rules. These supervised methods work\nrelatively well in simple tasks such as lane-following and driving on a highway. On the other\nhand, imitation learning can also be regarded as supervised learning approach [32], where\nthe agent observes the demonstrations performed by some experts and learns to imitate the\nactions of the experts. However, an intrinsic shortcoming of imitation learning is that it has\nthe covariate shift problem [20] and can not generalize very well to scenes never experienced\nbefore.\nReinforcement Learning for Autonomous Driving. Reinforcement learning has been\napplied to a wide variety of robotics related tasks, such as computer games [17], robot lo-\ncomotion [8, 12], and autonomous driving [1, 25]. One of the challenges in practical real-\nworld applications of reinforcement learning is the high-dimensionality of state space as\nwell as the non-trivial large action range. Developing an optimal policy over such high-\ncomplexity space is time consuming. Recent work in deep reinforcement learning has made\ngreat progress in learning in a high dimensional space with the power of deep neural net-\nworks [13, 15, 17, 18, 24]. However, both deep Q-learning method [17] and policy gradient\nmethod [15] require the agent to interact with the environment to get reward and feedback.\nHowever, it is unrealistic to train autonomous vehicle with reinforcement learning in a real\nworld environment since the car may hurt its surroundings once it takes a wrong action.\nReinforcement Learning in the Wild. Performing reinforcement learning with a car\ndriving simulator and transferring learned models to the real environment could enable faster,\nlower-cost training, and it is much safer than training with a real car. However, real-world\ndriving challenge usually spans a diverse range, and it is often signiﬁcantly different from the\ntraining environment in a car driving simulator in terms of their visual appearance. Models\ntrained purely on virtual data do not generalize well to real images [6, 27]. Recent progress\nof transfer and domain adaptation learning in robotics provides examples of simulation-to-\nreal reinforcement training [10, 21, 26]. These models either ﬁrst train a model in virtual\nenvironment and then ﬁne-tune in the real environment [21], or learn an alignment between\nvirtual images and real images by ﬁnding representations that are shared between the two\ndomains [27], or use randomized rendered virtual environments to train and then test in real\nenvironment [22, 26]. The work of [21] proposes to use progressive network to transfer\nnetwork weights from model trained on virtual data to the real environment and then ﬁne-\n4\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\ntune the model in a real setting. The training time in real environment has been greatly\nreduced by ﬁrst training in a virtual environment. However, it is still necessary to train\nthe agent in the real environment, thus it does not solve the critical problem of avoiding\nrisky trial-and-error in real world. Methods that try to learn an alignment between virtual\nimages and real images could fail to generalize to more complex scenarios, especially when\nit is hard to ﬁnd a good alignment between virtual images and real images. As a more\nrecent work, [22] proposed a new framework for training a reinforcement learning agent\nwith only a virtual environment. Their work proved the possibility of performing collision-\nfree ﬂight in real world with training in 3D CAD model simulator. However, as mentioned in\nthe conclusion of their paper [22], the manual engineering work to design suitable training\nenvironments is nontrivial, and it is more reasonable to attain better results by combining\nsimulated training with some real data, though it is unclear from their paper how to combine\nreal data with simulated training.\nImage Synthesis and Image Translation. Image translation aims to predict image in\nsome speciﬁc modality, given an image from another modality. This can be treated as a\ngeneric method as it predicts pixels from pixels. Recently, the community has made sig-\nniﬁcant progress in generative approaches, mostly based on generative adversarial networks\n[9]. To name a few, the work of [29] explored the use of VAE-GAN [14] in generating 3D\nvoxel models, and the work of [28] proposed a cascade GAN to generate natural images by\nstructure and style. More recently, the work of [11] developed a general and simple frame-\nwork for image-to-image translation which can handle various pixel level generative tasks\nlike semantic segmentation, colorization, rendering edge maps, etc.\nScene Parsing. One part of our network is the semantic image segmentation network.\nThere are already many great works in the ﬁeld of semantic image segmentation. Many of\nthem are based on deep convolutional neural network or fully convolutional neural network\n[16]. In this paper, we use the SegNet for image segmentation, the structure of the network\nis revealed in [2], which is composed of two main parts. The ﬁrst part is an encoder, which\nconsists of Convolutional, Batch Normalization, ReLU and max pooling layers. The second\npart is a decoder, which replaces the pooling layers with upsampling layers.\n2\nReinforcement Learning in the Wild\nWe aim to successfully apply a driving model trained entirely in virtual environment to real-\nworld driving challenges. One of the major gaps is that what the agent observes are frames\nrendered by a simulator, which are different from real world frames in terms of their appear-\nance. Therefore, we proposed a realistic translation network to convert virtual frames to\nrealistic ones. Inspired by the work of image-to-image translation network [11], our network\nincludes two modules, namely virtual-to-parsing and parsing-to-realistic network. The ﬁrst\none maps virtual frame to scene parsing image. The second one translates scene parsing to\nrealistic frame with similar scene structure as the input virtual frame. These two modules\ngenerate realistic frames that maintain the scene parsing structure of input virtual frames.\nThe architecture of realistic translation network is illustrated on Figure 1. Finally, we train\na self-driving agent using reinforcement learning method on realistic frames obtained by re-\nalistic translation network. The approach we adopt is developed by [18], where they use the\nasynchronous actor-critic reinforcement learning algorithm to train a self-driving vehicle in\nthe car racing simulator TORCS [30]. In this section, we will ﬁrst present proposed real-\nistic translation network and then discuss how to train driving agent under a reinforcement\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n5\nFigure 2: Example image segmentation for both virtual world images (Left 1 and Left 2) and\nreal world images (Right 1 and Right 2).\nlearning framework.\n2.1\nRealistic Translation Network\nAs there is no paired virtual and real world image, a direct mapping from virtual world\nimage to real world image using [11] would be awkward. However, as these two types of\nimages both express driving scene, we can translate them by using scene parsing representa-\ntion. Inspired by [11], our realistic translation network is composed of two image translation\nnetworks, where the ﬁrst image translation network translates virtual images to their seg-\nmentations, and the second image translation network translates segmented images to their\nreal world counterparts.\nThe image-to-image translation network proposed by [11] is basically a conditional\nGAN. The difference between traditional GANs and conditional GANs is that GANs learn a\nmapping from random noise vector z to output image s : G : z →s, while conditional GANs\ntake in both an image x and a noise vector z, and generate another image s : G : {x,z} →s,\nwhere s is usually in a different domain compared with x (For example, translate images to\ntheir segmentations).\nThe objective of a conditional GAN can be expressed as,\nLcGAN(G,D) =Ex,s∼pdata(x,s)[logD(x,s)]\n+Ex∼pdata(x),z∼pz(z)[log(1−D(x,G(x,z)))],\n(1)\n6\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\nwhere G is the generator that tries to minimize this objective and D is the adversar-\nial discriminator that acts against G to maximize this objective.\nIn other words, G∗=\nargminG maxD LcGAN(G,D). In order to suppress blurring, a L1 loss regularization term\nis added, which can be expressed as,\nLL1(G) = Ex,s∼pdata(x,s),z∼pz(z)[∥s−G(x,z)∥1].\n(2)\nTherefore, the overall objective for the image-to-image translation network is,\nG∗= argmin\nG max\nD LcGAN(G,D)+λLL1(G),\n(3)\nwhere λ is the weight of regularization.\nOur network consists of two image-to-image translation networks, both networks use\nthe same loss function as equation 3. The ﬁrst network translates virtual images x to their\nsegmentations s : G1 : {x,z1} →s, and the second network translates segmented images s\ninto their realistic counterparts y : G2 : {s,z2} →y, where z1,z2 are noise terms to avoid\ndeterministic outputs. As for GAN neural network structures, we use the same generator and\ndiscriminator architectures as used in [11].\n2.2\nReinforcement Learning for Training a Self-Driving Vehicle\nWe use a conventional RL solver Asynchronous Advantage Actor-Critic (A3C)[18] to train\nthe self driving vehicle, which has performed well on various machine learning tasks. A3C\nalgorithm is a fundamental Actor-Critic algorithm that combines several classic reinforce-\nment learning algorithms with the idea of asynchronous parallel threads. Multiple threads\nrun at the same time with unrelated copies of the environment, generating their own se-\nquences of training samples. Those actors-learners proceed as though they are exploring\ndifferent parts of the unknown space. For one thread, parameters are synchronized before an\niteration of learning and updated after ﬁnishing it. The details of A3C algorithm implemen-\ntation can be found in [18].\nIn order to encourage the agent to drive faster and avoid collisions, we deﬁne the reward\nfunction as\nrt =\n\u001a\n(vt ·cosα −dist(t)\ncenter)·β\nno collision,\nγ\ncollision,\n(4)\nwhere vt is the speed (in m/s) of the agent at time step t, α is the angle (in rad) between\nthe agent’s speed and the tangent line of the track, and dist(t)\ncenter is the distance between the\ncenter of the agent and the middle of the track. β,γ are constants and are determined at the\nbeginning of training. We take β = 0.006,γ = −0.025 in our training.\n3\nExperiments\nWe performed two sets of experiments to compare the performance of our method and other\nreinforcement learning methods as well as supervised learning methods. The ﬁrst sets of\nexperiments involves virtual to real reinforcement learning on real world driving data. The\nsecond sets of experiments involves transfer learning in different virtual driving environ-\nments. The virtual simulator used in our experiments is TORCS[30].\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n7\n480\n640\nStride of 4\n8\n8\n3\n32\n119\n159\n32\n58\n78\n4\n4\nStride of 2\n3\n3\nStride of 1\nf(a|s)\n29\n39\n32\nFully Connected Layer\nAction Probability\nFigure 3: Reinforcement learning network architecture. The network is an end-to-end net-\nwork mapping state representations to action probability outputs.\nFigure 4: Examples of Virtual to Real Image Translation. Odd columns are virtual images\ncaptured from TORCS. Even columns are synthetic real world images corresponding to vir-\ntual images on the left.\n3.1\nVirtual to Real RL on Real World Driving Data\nIn this experiment, we trained our proposed reinforcement learning model with realistic\ntranslation network. We ﬁrst trained the virtual to real image translation network, and then\nused the trained network to ﬁlter virtual images in simulator to realistic images. These real-\nistic images were then feed into A3C to train a driving policy. Finally, the trained policy was\ntested on a real world driving data to evaluate its steering angle prediction accuracy.\nFor comparison, we also trained a supervised learning model to predict steering angles\nfor every test driving video frame. The model is a deep neural network that has the same\narchitecture design as the policy network in our reinforcement learning model. The input of\nthe network is a sequence of four consecutive frames, the output of the network is the ac-\ntion probability vector, and elements in the vector represent the probability of going straight,\nturning left and turning right. The training data for the supervised learning model is differ-\nent from the testing data that is used to evaluate model performance. In addition, another\nbaseline reinforcement learning model (B-RL) is also trained. The only difference between\nB-RL and our method is that the virtual world images were directly taken by the agent as\nstate inputs. This baseline RL is also tested on the same real world driving data.\nDataset. The real world driving video data are from [5], which is collected in a sunny\nday with detailed steering angle annotations per frame. There are in total around 45k im-\nages in this dataset, of which 15k were selected for training the supervised learning model,\nand another 15k were selected and held out for testing. To train our realistic translation net-\n8\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\nFigure 5: Transfer learning between different environments. Oracle was trained in Cgtrack2\nand tested in Cgtrack2, so its performance is the best. Our model works better than the\ndomain randomization RL method. Domain randomization method requires training in mul-\ntiple virtual environments, which imposes signiﬁcant manual engineering work.\nwork, we collected virtual images and their segmentations from the Aalborg environment\nin TORCS. A total of 1673 images were collected which covers the entire driving cycle of\nAalborg environment.\nScene Segmentation. We used the image semantic segmentation network design of [2]\nand their trained segmentation network on the CityScape image segmentation dataset [7] to\nsegment 45k real world driving images from [5]. The network was trained on the CityScape\ndataset with 11 classes and was trained with 30000 iterations.\nImage Translation Network Training. We trained both virtual-to-parsing and parsing-\nto-real network using the collected virtual-segmentation image pairs and segmentation-real\nimage pairs. The translation networks are of a encoder-decoder fashion as shown in ﬁgure 1.\nIn the image translation network, we used U-Net architecture with skip connection to connect\ntwo separate layers from encoder and decoder respectively, which have the same output\nfeature map shape. The input size of the generator is 256×256. Each convolutional layer has\na kernel size of 4×4 and striding size of 2. LeakyReLU is applied after every convolutional\nlayer with a slope of 0.2 and ReLU is applied after every deconvolutional layer. In addition,\nbatch normalization layer is applied after every convolutional and deconvolutional layer.\nThe ﬁnal output of the encoder is connected with a convolutional layer which yields output\nof shape 3 × 256 × 256 followed by Tanh. We used all 1673 virtual-segmentation image\npairs to train a virtual to segmentation network. As there are redundancies in the 45k real\nimages, we selected 1762 images and their segmentations from the 45k images to train a\nparsing-to-real image translation network. To train the image translation models, we used\nthe Adam optimizer with an initial learning rate of 0.0002, momentum of 0.5, batchsize of\n16, and 200 iterations until convergence.\nReinforcement Training. The RL network structure used in our training is similar to\nthat of [18] where the actor network is a 4-layer convolutional network (shown in ﬁgure 3)\nwith ReLU activation functions in-between. The network takes in 4 consecutive RGB frames\nas state input and output 9 discrete actions which corresponds to “go straight with accelera-\ntion”, “go left with acceleration”, “go right with acceleration”, “go straight and brake”, “go\nleft and brake”, “go right and brake”, “go straight”, “go left”, and “go right”. We trained\nthe reinforcement learning agent with 12 asynchronous threads, and with the RMSProp op-\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n9\ntimizer at an initial learning rate of 0.01, γ = 0.9, and ε = 0.1.\nEvaluation. The real world driving dataset [5] provides the steering angle annotations\nper frame. However, the actions performed in the TORCS virtual environment only contain\n\"going left\", \"going right\", and \"going straight\" or their combinations with \"acceleration\" or\n\"brake\". Therefore, we deﬁne a label mapping strategy to translate steering angle labels to\naction labels in the virtual simulator. We relate steering angle in (−10,10) to action \"going\nstraight\" (since small steering angle is not able to result in a distinct turning in a short time),\nsteering angle less than −10 to action \"going left\" and steering angle more than 10 to action\n\"going right\". By comparing output actions generated from our method with ground truth,\nwe can obtain the accuracy of driving action prediction.\n3.2\nTransfer Learning in Virtual Driving Environments\nWe further performed another sets of experiments and obtained results of transfer learning\nbetween different virtual driving environments. In this experiments, we trained three rein-\nforcement learning agents. The ﬁrst agent was trained with standard A3C in the Cg−track2\nenvironment in TORCS, and evaluated its performance frequently in the same environments.\nIt is reasonable to expect the performance of this agent to be the best, so we call it \"Oracle\".\nThe second agent was trained with our proposed reinforcement learning method with real-\nistic translation network. However, it was trained in E-track1 environment in TORCS and\nthen evaluated in Cg-track2. It is necessary to note that the visual appearance of E-track1\nis different from that of Cg-track2. The third agent was trained with domain randomization\nmethod similar to that of [22], where the agent was trained with 10 different virtual environ-\nments and evaluated in Cg-track2. For training with our methods, we obtain 15k segmented\nimages for both E-track1 and Cg-track2 to train virtual-to-parsing and parsing-to-real im-\nage translation networks. The image translation training details and reinforcement learning\ndetails are the same as that of section 3.1.\n4\nResults\nImage Segmentation Results. We used image segmentation model trained on the cityscape\n[7] dataset to segment both virtual and real images. Examples are shown in ﬁgure 2. As\nshown in the ﬁgure, although the original virtual image and real image look quite different,\ntheir scene parsing results are very similar. Therefore, it is reasonable to use scene parsing\nas the interim to connect virtual images and real images.\nQualitative Result of Realistic Translation Network. Figure 4 shows some repre-\nsentative results of our image translation network. The odd columns are virtual images in\nTORCS, and the even columns are translated realistic images. The images in the virtual envi-\nronment appears to be darker than the translated images, as the real images used to train the\ntranslation network is captured in a sunny day. Therefore, our model succeed to synthesize\nrealistic images of similar appearance with the original ground truth real images.\nReinforcement Training Results. The results for virtual to real reinforcement learning\non real world driving data are shown in table 1. Results show that our proposed method\nhas a better overall performance than the baseline method (B-RL), where the reinforcement\ntraining agent is trained in a virtual environment without seeing any real data. The supervised\nmethod (SV) has the best overall performance, however, was trained with large amounts of\nsupervised labeled data.\n10\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\nTable 1: Action prediction accuracy for the three methods.\nAccuracy\nOurs\nB-RL\nSV\nDataset in [5]\n43.40%\n28.33%\n53.60%\nThe result for transfer learning in different virtual environments is shown in ﬁgure 5.\nObviously, standard A3C (Oracle) trained and tested in the same environment gets the best\nperformance. However, our model performs better than the domain randomization method,\nwhich requires training in multiple environments to generalize. As mentioned in [22], do-\nmain randomization requires lots of engineering work to make it generalize. Our model suc-\nceeds by observing translated images from E-track1 to Cg-track2, which means the model\nalready gets training in an environment that looks very similar to the test environment (Cg-\ntrack2), thus the performance is improved.\n5\nConclusion\nWe proved through experiments that by using synthetic real images as training data in re-\ninforcement learning, the agent generalizess better in a real environment than pure training\nwith virtual data or training with domain randomization. The next step would be to design a\nbetter image-to-image translation network and a better reinforcement learning framework to\nsurpass the performance of supervised learning.\nThanks to the bridge of scene parsing, virtual images can be translated into realistic\nimages which maintain their scene structure. The learnt reinforcement learning model on\nrealistic frames can be easily applied to real-world environment. We also notice that the\ntranslation results of a segmentation map are not unique. For example, segmentation map\nindicates a car, but it does not assign which color of that car should be. Therefore, one of our\nfuture work is to make parsing-to-realistic network output various possible appearances (e.g.\ncolor, texture). In this way, bias in reinforcement learning training would be largely reduced.\nWe provide the ﬁrst example of training a self-driving vehicle using reinforcement learn-\ning algorithm by interacting with a synthesized real environment with our proposed image-\nto-segmentation -to-image framework. We show that by using our method for RL training,\nit is possible to obtain a self driving vehicle that can be placed in the real world.\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n11\nReferences\n[1] Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y Ng. An application of\nreinforcement learning to aerobatic helicopter ﬂight. Advances in neural information\nprocessing systems, 19:1, 2007.\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\nSegnet: A deep con-\nvolutional encoder-decoder architecture for image segmentation.\narXiv preprint\narXiv:1511.00561, 2015.\n[3] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat\nFlepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai\nZhang, Xin Zhang, Jake Zhao, and Karol Zieba. End to end learning for self-driving\ncars.\nCoRR, abs/1604.07316, 2016.\nURL http://arxiv.org/abs/1604.\n07316.\n[4] Chenyi Chen, Ari Seff, Alain L. Kornhauser, and Jianxiong Xiao. Deepdriving: Learn-\ning affordance for direct perception in autonomous driving. CoRR, abs/1505.00256,\n2015. URL http://arxiv.org/abs/1505.00256.\n[5] Sully Chen.\nAutopilot-tensorﬂow,\n2016.\nURL https://github.com/\nSullyChen/Autopilot-TensorFlow.\n[6] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua\nTobin, Pieter Abbeel, and Wojciech Zaremba. Transfer from simulation to real world\nthrough learning deep inverse dynamics model.\narXiv preprint arXiv:1610.03518,\n2016.\n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,\nRodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding. CoRR, abs/1604.01685, 2016. URL\nhttp://arxiv.org/abs/1604.01685.\n[8] Gen Endo, Jun Morimoto, Takamitsu Matsubara, Jun Nakanishi, and Gordon Cheng.\nLearning cpg-based biped locomotion with a policy gradient method: Application to\na humanoid robot. The International Journal of Robotics Research, 27(2):213–228,\n2008.\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial nets.\nIn Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Wein-\nberger, editors, Advances in Neural Information Processing Systems 27, pages 2672–\n2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/\n5423-generative-adversarial-nets.pdf.\n[10] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learn-\ning invariant feature spaces to transfer skills with reinforcement learning.\narXiv\npreprint arXiv:1703.02949, 2017.\n[11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image trans-\nlation with conditional adversarial networks.\nCoRR, abs/1611.07004, 2016.\nURL\nhttp://arxiv.org/abs/1611.07004.\n12\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n[12] Nate Kohl and Peter Stone. Policy gradient reinforcement learning for fast quadrupedal\nlocomotion. In Robotics and Automation, 2004. Proceedings. ICRA’04. 2004 IEEE\nInternational Conference on, volume 3, pages 2619–2624. IEEE, 2004.\n[13] Jan Koutník, Giuseppe Cuccu, Jürgen Schmidhuber, and Faustino Gomez. Evolving\nlarge-scale neural networks for vision-based reinforcement learning. In Proceedings\nof the 15th annual conference on Genetic and evolutionary computation, pages 1061–\n1068. ACM, 2013.\n[14] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, and Ole Winther. Autoencoding\nbeyond pixels using a learned similarity metric. CoRR, abs/1512.09300, 2015. URL\nhttp://arxiv.org/abs/1512.09300.\n[15] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\nYuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforce-\nment learning. arXiv preprint arXiv:1509.02971, 2015.\n[16] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks\nfor semantic segmentation. In The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2015.\n[17] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Os-\ntrovski, et al. Human-level control through deep reinforcement learning. Nature, 518\n(7540):529–533, 2015.\n[18] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P.\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous meth-\nods for deep reinforcement learning.\nCoRR, abs/1602.01783, 2016.\nURL http:\n//arxiv.org/abs/1602.01783.\n[19] Dean A Pomerleau. Alvinn, an autonomous land vehicle in a neural network. Technical\nreport, Carnegie Mellon University, Computer Science Department, 1989.\n[20] Stéphane Ross and Drew Bagnell. Efﬁcient reductions for imitation learning. In AIS-\nTATS, volume 3, pages 3–5, 2010.\n[21] Andrei A. Rusu, Matej Vecerik, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, and\nRaia Hadsell. Sim-to-real robot learning from pixels with progressive nets. CoRR,\nabs/1610.04286, 2016. URL http://arxiv.org/abs/1610.04286.\n[22] Fereshteh Sadeghi and Sergey Levine. (cad)$ˆ2$rl: Real single-image ﬂight without a\nsingle real image. CoRR, abs/1611.04201, 2016. URL http://arxiv.org/abs/\n1611.04201.\n[23] Ahmad El Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani.\nEnd-to-end deep reinforcement learning for lane keeping assist.\narXiv preprint\narXiv:1612.04340, 2016.\n[24] John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz.\nTrust region policy optimization. In ICML, pages 1889–1897, 2015.\nPAN,YOU,WANG,LU: VIRTUAL TO REAL REINFORCEMENT LEARNING\n13\n[25] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, re-\ninforcement learning for autonomous driving. CoRR, abs/1610.03295, 2016. URL\nhttp://arxiv.org/abs/1610.03295.\n[26] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter\nAbbeel. Domain randomization for transferring deep neural networks from simulation\nto the real world. arXiv preprint arXiv:1703.06907, 2017.\n[27] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel, Sergey Levine,\nKate Saenko, and Trevor Darrell. Adapting deep visuomotor representations with weak\npairwise constraints. In Workshop on the Algorithmic Foundations of Robotics (WAFR),\n2016.\n[28] Xiaolong Wang and Abhinav Gupta. Generative image modeling using style and struc-\nture adversarial networks. In ECCV, 2016.\n[29] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T Freeman, and Joshua B Tenen-\nbaum.\nLearning a probabilistic latent space of object shapes via 3d generative-\nadversarial modeling. In Advances in Neural Information Processing Systems, pages\n82–90, 2016.\n[30] Bernhard Wymann, Eric Espié, Christophe Guionneau, Christos Dimitrakakis, Rémi\nCoulom, and Andrew Sumner. Torcs, the open racing car simulator. Software available\nat http://torcs. sourceforge. net, 2000.\n[31] Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell. End-to-end learning of driving\nmodels from large-scale video datasets. arXiv preprint arXiv:1612.01079, 2016.\n[32] Jiakai Zhang and Kyunghyun Cho. Query-efﬁcient imitation learning for end-to-end\nautonomous driving. arXiv preprint arXiv:1605.06450, 2016.\n",
  "categories": [
    "cs.AI",
    "cs.CV"
  ],
  "published": "2017-04-13",
  "updated": "2017-09-26"
}