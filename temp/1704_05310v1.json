{
  "id": "http://arxiv.org/abs/1704.05310v1",
  "title": "Unsupervised Learning by Predicting Noise",
  "authors": [
    "Piotr Bojanowski",
    "Armand Joulin"
  ],
  "abstract": "Convolutional neural networks provide visual features that perform remarkably\nwell in many computer vision applications. However, training these networks\nrequires significant amounts of supervision. This paper introduces a generic\nframework to train deep networks, end-to-end, with no supervision. We propose\nto fix a set of target representations, called Noise As Targets (NAT), and to\nconstrain the deep features to align to them. This domain agnostic approach\navoids the standard unsupervised learning issues of trivial solutions and\ncollapsing of features. Thanks to a stochastic batch reassignment strategy and\na separable square loss function, it scales to millions of images. The proposed\napproach produces representations that perform on par with state-of-the-art\nunsupervised methods on ImageNet and Pascal VOC.",
  "text": "Unsupervised Learning by Predicting Noise\nPiotr Bojanowski 1 Armand Joulin 1\nAbstract\nConvolutional neural networks provide visual\nfeatures that perform remarkably well in many\ncomputer vision applications.\nHowever, train-\ning these networks requires signiﬁcant amounts\nof supervision. This paper introduces a generic\nframework to train deep networks, end-to-end,\nwith no supervision.\nWe propose to ﬁx a set\nof target representations, called Noise As Targets\n(NAT), and to constrain the deep features to align\nto them. This domain agnostic approach avoids\nthe standard unsupervised learning issues of triv-\nial solutions and collapsing of features. Thanks\nto a stochastic batch reassignment strategy and a\nseparable square loss function, it scales to mil-\nlions of images.\nThe proposed approach pro-\nduces representations that perform on par with\nstate-of-the-art unsupervised methods on Ima-\ngeNet and PASCAL VOC.\n1. Introduction\nIn recent years, convolutional neural networks, or con-\nvnets (Fukushima, 1980; LeCun et al., 1989) have pushed\nthe limits of computer vision (Krizhevsky et al., 2012; He\net al., 2016), leading to important progress in a variety of\ntasks, like object detection (Girshick, 2015) or image seg-\nmentation (Pinheiro et al., 2015). Key to this success is\ntheir ability to produce features that easily transfer to new\ndomains when trained on massive databases of labeled im-\nages (Razavian et al., 2014; Oquab et al., 2014) or weakly-\nsupervised data (Joulin et al., 2016). However, human an-\nnotations may introduce unforeseen bias that could limit\nthe potential of learned features to capture subtle informa-\ntion hidden in a vast collection of images.\nSeveral strategies exist to learn deep convolutional features\nwith no annotation (Donahue et al., 2016).\nThey either\ntry to capture a signal from the source as a form of self-\nsupervision (Doersch et al., 2015; Wang & Gupta, 2015) or\nlearn the underlying distribution of images (Vincent et al.,\n1Facebook AI Research. Correspondence to: Piotr Bojanowski\n<bojanowski@fb.com>.\n2010; Goodfellow et al., 2014). While some of these ap-\nproaches obtain promising performance in transfer learn-\ning (Donahue et al., 2016; Wang & Gupta, 2015), they do\nnot explicitly aim to learn discriminative features. Some at-\ntempts were made with retrieval based approaches (Doso-\nvitskiy et al., 2014) and clustering (Yang et al., 2016; Liao\net al., 2016), but they are hard to scale and have only been\ntested on small datasets. Unfortunately, as in the supervised\ncase, a lot of data is required to learn good representations.\nIn this work, we propose a novel discriminative framework\ndesigned to learn deep architectures on massive amounts\nof data. Our approach is general, but we focus on con-\nvnets since they require millions of images to produce good\nfeatures. Similar to self-organizing maps (Kohonen, 1982;\nMartinetz & Schulten, 1991), we map deep features to\na set of predeﬁned representations in a low dimensional\nspace. As opposed to these approaches, we aim to learn\nthe features in a end-to-end fashion, which traditionally\nsuffers from a feature collapsing problem. Our approach\ndeals with this issue by ﬁxing the target representations\nand aligning them to our features. These representations\nare sampled from a uninformative distribution and we use\nthis Noise As Targets (NAT). Our approach also shares\nsome similarities with standard clustering approches like\nk-means (Lloyd, 1982) or discriminative clustering (Bach\n& Harchaoui, 2007).\nIn addition, we propose an online algorithm able to scale\nto massive image databases like ImageNet (Deng et al.,\n2009). Importantly, our approach is barely less efﬁcient\nto train than standard supervised approaches and can re-\nuse any optimization procedure designed for them. This\nis achieved by using a quadratic loss as in (Tygert et al.,\n2017) and a fast approximation of the Hungarian algo-\nrithm. We show the potential of our approach by training\nend-to-end on ImageNet a standard architecture, namely\nAlexNet (Krizhevsky et al., 2012) with no supervision.\nWe test the quality of our features on several image classi-\nﬁcation problems, following the setting of Donahue et al.\n(2016). We are on par with state-of-the-art unsupervised\nand self-supervised learning approaches while being much\nsimpler to train and to scale.\nThe paper is organized as follows: after a brief review of\nthe related work in Section 2, we present our approach in\narXiv:1704.05310v1  [stat.ML]  18 Apr 2017\nUnsupervised Learning by Predicting Noise\nSection 3. We then validate our solution with several ex-\nperiments and comparisons with standard unsupervised and\nself-supervised approaches in Section 4.\n2. Related work\nSeveral approaches have been recently proposed to tackle\nthe problem of deep unsupervised learning (Coates & Ng,\n2012; Mairal et al., 2014; Dosovitskiy et al., 2014). Some\nof them are based on a clustering loss (Xie et al., 2016;\nYang et al., 2016; Liao et al., 2016), but they are not tested\nat a scale comparable to that of supervised convnet train-\ning. Coates & Ng (2012) uses k-means to pre-train con-\nvnets, by learning each layer sequentially in a bottom-up\nfashion. In our work, we train the convnet end-to-end with\na loss that shares similarities with k-means. Closer to our\nwork, Dosovitskiy et al. (2014) proposes to train convnets\nby solving a retrieval problem. They assign a class per im-\nage and its transformation. In contrast to our work, this\napproach can hardly scale to more than a few hundred of\nthousands of images, and requires a custom-tailored archi-\ntecture while we use a standard AlexNet.\nAnother traditional approach for learning visual representa-\ntions in an unsupervised manner is to deﬁne a parametrized\nmapping between a predeﬁned random variable and a set\nof images. Traditional examples of this approach are varia-\ntional autoencoders (Kingma & Welling, 2013), generative\nadversarial networks (Goodfellow et al., 2014), and to a\nlesser extent, noisy autoencoders (Vincent et al., 2010). In\nour work, we are doing the opposite; that is, we map images\nto a predeﬁned random variable. This allows us to re-use\nstandard convolutional networks and greatly simpliﬁes the\ntraining.\nGenerative adversarial networks.\nAmong those ap-\nproaches, generative adversarial networks (GANs) (Good-\nfellow et al., 2014; Denton et al., 2015; Donahue et al.,\n2016) share another similarity with our approach, namely\nthey are explicitly minimizing a discriminative loss to learn\ntheir features. While these models cannot learn an inverse\nmapping, Donahue et al. (2016) recently proposed to add\nan encoder to extract visual features from GANs.\nLike\nours, their encoder can be any standard convolutional net-\nwork. However, their loss aims at differentiating real and\ngenerated images, while we are aiming directly at differ-\nentiating between images. This makes our approach much\nsimpler and faster to train, since we do not need to learn the\ngenerator nor the discriminator.\nSelf-supervision.\nRecently, a lot of work has explored\nself-supervison: leveraging supervision contained in the\ninput signal (Doersch et al., 2015; Noroozi & Favaro,\n2016; Pathak et al., 2016).\nIn the same vein as\nword2vec (Mikolov et al., 2013),\nDoersch et al. (2015)\nshow that spatial context is a strong signal to learn visual\nfeatures. Noroozi & Favaro (2016) have further extended\nthis work. Others have shown that temporal coherence in\nvideos also provides a signal that can be used to learn pow-\nerful visual features (Agrawal et al., 2015; Jayaraman &\nGrauman, 2015; Wang & Gupta, 2015). In particular, Wang\n& Gupta (2015) show that such features provide promis-\ning performance on ImageNet. In contrast to our work,\nthese approaches are domain dependent since they require\nexplicit derivation of weak supervision directly from the\ninput.\nAutoencoders.\nMany have also used autoencoders with\na reconstruction loss (Bengio et al., 2007; Ranzato et al.,\n2007; Masci et al., 2011). The idea is to encode and de-\ncode an image, while minimizing the loss between the de-\ncoded and original images. Once trained, the encoder pro-\nduces image features and the decoder can be used to gen-\nerate images from codes. The decoder is often a fully con-\nnected network (Ranzato et al., 2007) or a deconvolutional\nnetwork (Masci et al., 2011; Zhao et al., 2016) but can\nbe more sophisticated, like a PixelCNN network (van den\nOord et al., 2016).\nSelf-organizing map.\nThis family of unsupervised meth-\nods aims at learning a low dimensional representation of\nthe data that preserves certain topological properties (Ko-\nhonen, 1982; Vesanto & Alhoniemi, 2000). In particular,\nNeural Gas (Martinetz & Schulten, 1991) aligns feature\nvectors to the input data. Each input datum is then assigned\nto one of these vectors in a winner-takes-all manner. These\nfeature vectors are in spirit similar to our target representa-\ntions and we use a similar assignment strategy. In contrast\nto our work, the target vectors are not ﬁxed and aligned to\nthe input vectors. Since we primarly aim at learning the\ninput features, we do the opposite.\nDiscriminative clustering.\nMany methods have been\nproposed to use discriminative losses for clustering (Xu\net al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010;\nJoulin & Bach, 2012). In particular, Bach & Harchaoui\n(2007) shows that the ridge regression loss could be use\nto learn discriminative clusters. It has been successfully\napplied to several computer vision applications, like ob-\nject discovery (Joulin et al., 2010; Tang et al., 2014) or\nvideo/text alignment (Bojanowski et al., 2013; 2014; Ra-\nmanathan et al., 2014). In this work, we show that a similar\nframework can be designed for neural networks. As op-\nposed to Xu et al. (2004), we address the empty assignment\nproblems by restricting the set of possible reassignments to\npermutations rather than using global linear constrains the\nassignments. Our assignments can be updated online, al-\nlowing our approach to scale to very large datasets.\nUnsupervised Learning by Predicting Noise\nTarget space\nFeatures\nAssignment\nImages\ncj\nP\nf(X)\nCNN\nFigure 1. Our approach takes a set of images, computes their deep\nfeatures with a convolutional network and matches them to a set of\npredeﬁned targets from a low dimensional space. The parameters\nof the network are learned by aligning the features to the targets.\n3. Method\nIn this section, we present our model and discuss its re-\nlations with several clustering approaches including k-\nmeans. Figure 1 shows an overview of our approach. We\nalso show that it can be trained on massive datasets using\nan online procedure. Finally, we provide all the implemen-\ntation details.\n3.1. Unsupervised learning\nWe are interested in learning visual features with no su-\npervision.\nThese features are produced by applying a\nparametrized mapping fθ to the images. In the presence\nof supervision, the parameters θ are learned by minimiz-\ning a loss function between the features produced by this\nmapping and some given targets, e.g., labels. In absence of\nsupervision, there is no clear target representations and we\nthus need to learn them as well. More precisely, given a\nset of n images xi, we jointly learn the parameters θ of the\nmapping fθ, and some target vectors yi:\nmin\nθ\n1\nn\nn\nX\ni=1\nmin\nyi∈Rd ℓ(fθ(xi), yi),\n(1)\nwhere d is the dimension of target vectors. In the rest of\nthe paper, we use matrix notations, i.e., we denote by Y the\nmatrix whose rows are the target representations yi, and by\nX the matrix whose rows are the images xi. With a slight\nabuse of notation, we denote by fθ(X) the n × d matrix of\nfeatures whose rows are obtained by applying the function\nfθ to each image independently.\nChoosing the loss function.\nIn the supervised setting, a\npopular choice for the loss ℓis the softmax function. How-\never, computing this loss is linear in the number of targets,\nmaking it impractical for large output spaces (Goodman,\n2001). While there are workarounds to scale these losses to\nlarge output spaces, Tygert et al. (2017) has recently shown\nthat using a squared ℓ2 distance works well in many su-\npervised settings, as long as the ﬁnal activations are unit\nnormalized. This loss only requires access to a single tar-\nget per sample, making its computation independent of the\nnumber of targets. This leads to the following problem:\nmin\nθ\nmin\nY ∈Rn×d\n1\n2n∥fθ(X) −Y ∥2\nF ,\n(2)\nwhere we still denote by fθ(X) the unit normalized fea-\ntures.\nUsing ﬁxed target representations.\nDirectly solving the\nproblem deﬁned in Eq. (2) would lead to a representation\ncollapsing problem: all the images would be assigned to\nthe same representation (Xu et al., 2004). We avoid this\nissue by ﬁxing a set of k predeﬁned target representations\nand matching them to the visual features. More precisely,\nthe matrix Y is deﬁned as the product of a matrix C con-\ntaining these k representations and an assignment matrix P\nin {0, 1}n×k, i.e.,\nY = PC.\n(3)\nNote that we can assume that k is greater than n with\nno loss of generality (by duplicating representations oth-\nerwise). Each image is assigned to a different target and\neach target can only be assigned once. This leads to a set\nP of constraints for the assignment matrices:\nP = {P ∈{0, 1}n×k | P1k ≤1n, P ⊤1n = 1k}.\n(4)\nThis formulation forces the visual features to be diversiﬁed,\navoiding the collapsing issue at the cost of ﬁxing the target\nrepresentations. Predeﬁning these targets is an issue if their\nnumber k is small, which is why we are interested in the\ncase where k is at least as large as the number n of images.\nChoosing the target representations.\nUntil now, we\nhave not discussed the set of target representations stored\nin C. A simple choice for the targets would be to take\nk elements of the canonical basis of Rd.\nIf d is larger\nthan n, this formulation would be similar to the framework\nof Dosovitskiy et al. (2014), and is impractical for large\nn. On the other hand, if d is smaller than n, this formula-\ntion is equivalent to the discriminative clustering approach\nof Bach & Harchaoui (2007). Choosing such targets makes\nvery strong assumptions on the nature of the underlying\nproblem. Indeed, it assumes that each image belongs to a\nunique class and that all classes are orthogonal. While this\nassumption might be true for some classiﬁcation datasets, it\nUnsupervised Learning by Predicting Noise\ndoes not generalize to huge image collections nor capture\nsubtle similarities between images belonging to different\nclasses.\nSince our features are unit normalized, another natural\nchoice is to uniformly sample target vectors on the ℓ2 unit\nsphere. Note that the dimension d will then directly inﬂu-\nence the level of correlation between representations, i.e.,\nthe correlation is inversely proportional to the square root\nof d. Using this Noise As Targets (NAT), Eq. (2) is now\nequivalent to:\nmax\nθ\nmax\nP ∈P Tr\n\u0000PCfθ(X)⊤\u0001\n.\n(5)\nThis problem can be interpreted as mapping deep features\nto a uniform distribution over a manifold, namely the d-\ndimension ℓ2 sphere. Using k predeﬁned representations is\na discrete approximation of this manifold that justiﬁes the\nrestriction of the mapping matrices to the set P of 1-to-1\nassignment matrices. In some sense, we are optimizing a\ncrude approximation of the earth mover’s distance between\nthe distribution of deep features and a given target distribu-\ntion (Rubner et al., 1998).\nRelation to clustering approaches.\nUsing the same no-\ntations as in Eq. (5), several clustering approaches share\nsimilarities with our method. In the linear case, spherical\nk-means minimizes the same loss function w.r.t. P and C,\ni.e.,\nmax\nC\nmax\nP ∈Q tr\n\u0000PCXT \u0001\n.\nA key difference is the set Q of assignment matrices:\nQ = {P ∈{0, 1}n×k | P1k = 1n}.\nThis set only guarantees that each data point is assigned\nto a single target representation. Once we jointly learn the\nfeatures and the assignment, this set does not prevent the\ncollapsing of the data points to a single target representa-\ntion.\nAnother similar clustering approach is Diffrac (Bach &\nHarchaoui, 2007). Their loss is equivalent to ours in the\ncase of unit normalized features. Their set R of assignment\nmatrices, however, is different:\nR = {P ∈{0, 1}n×k | P ⊤1n ≥c1k},\nwhere c > 0 is some ﬁxed parameter. While restricting\nthe assignment matrices to this set prevents the collapsing\nissue, it introduces global constraints that are not suited\nfor online optimization. This makes their approach hard\nto scale to large datasets.\n3.2. Optimization\nIn this section, we describe how to efﬁciently optimize the\ncost function described in Eq. (5). In particular, we explore\nAlgorithm 1 Stochastic optimization of Eq. (5).\nRequire: T batches of images, λ0 > 0\nfor t = {1, . . . , T} do\nObtain batch b and representations r\nCompute fθ(Xb)\nCompute P ∗by minimizing Eq. (2) w.r.t. P\nCompute ∇θL(θ) from Eq. (2) with P ∗\nUpdate θ ←θ −λt∇θL(θ)\nend for\napproximated updates of the assignment matrix that are\ncompatible with online optimization schemes, like stochas-\ntic gradient descent (SGD).\nUpdating the assignment matrix P.\nDirectly solving\nfor the optimal assignment requires to evaluate the dis-\ntances between all the n features and the k representations.\nIn order to efﬁciently solve this problem, we ﬁrst reduce\nthe number k of representations to n. This limits the set P\nto the set of permutation matrices, i.e.,\nP = {P ∈{0, 1}n×n | P1n = 1n, P ⊤1n = 1n}.\n(6)\nRestricting the problem deﬁned in Eq. (5) to this set, the\nlinear assignment problem in P can be solved exactly with\nthe Hungarian algorithm (Kuhn, 1955), but at the pro-\nhibitive cost of O(n3).\nInstead, we perform stochastic updates of the matrix. Given\na batch of samples, we optimize the assignment matrix P\non its restriction to this batch. Given a subset B of b dis-\ntinct images, we only update the b × b square sub matrix\nPB obtained by restricting P to these b images and their\ncorresponding targets. In other words, each image can only\nbe re-assigned to a target that was previously assigned to\nanother image in the batch. This procedure has a complex-\nity of O(b3) per batch, leading to an overall complexity of\nO(nb2), which is linear in the number of data points. We\nperform this update before updating the parameters θ of our\nfeatures, in an on-line manner. Note that this simple proce-\ndure would not have been possible if k > n; we would have\nhad to also consider the k −n unassigned representations.\nStochastic gradient descent.\nApart from the update of\nthe assignment matrix P, we use the same optimization\nscheme as standard supervised approaches, i.e., SGD with\nbatch normalization (Ioffe & Szegedy, 2015). As noted\nby Tygert et al. (2017), batch normalization plays a cru-\ncial role when optimizing the l2 loss, as it avoids exploding\ngradients. For each batch b of images, we ﬁrst perform a\nforward pass to compute the distance between the images\nand the corresponding subset of target representations r.\nThe Hungarian algorithm is then used on these distances to\nobtain the optimal reassignments within the batch. Once\nUnsupervised Learning by Predicting Noise\nSoftmax\nSquare loss\nImageNet\n59.2\n58.4\nTable 1. Comparison between the softmax and the square loss for\nsupervised object classiﬁcation on ImageNet. The architecture\nis an AlexNet. The features are unit normalized for the square\nloss (Tygert et al., 2017). We report the accuracy on the validation\nset.\nthe assignments are updated, we use the chain rule in order\nto compute the gradients of all our parameters. Our opti-\nmization algorithm is summarized in Algorithm 1.\n3.3. Implementation details\nOur experiments solely focus on learning visual features\nwith convnets. All the details required to train these archi-\ntectures with our approach are described below. Most of\nthem are standard tricks used in the usual supervised set-\nting.\nDeep features.\nTo ensure a fair empirical comparison\nwith previous work, we follow Wang & Gupta (2015) and\nuse an AlexNet architecture. We train it end to end using\nour unsupervised loss function. We subsequently test the\nquality of the learned visual feature by re-training a classi-\nﬁer on top. During transfer learning, we consider the output\nof the last convolutional layer as our features as in Raza-\nvian et al. (2014). We use the same multi-layer perceptron\n(MLP) as in Krizhevsky et al. (2012) for the classiﬁer.\nPre-processing.\nWe\nobserve\nin\npractice\nthat\npre-\nprocessing the images greatly helps the quality of our\nlearned features. As in Ranzato et al. (2007), we use im-\nage gradients instead of the images to avoid trivial solu-\ntions like clustering according to colors. Using this pre-\nprocessing is not surprising since most hand-made features\nlike SIFT or HoG are based on image gradients (Lowe,\n1999; Dalal & Triggs, 2005).\nIn addition to this pre-\nprocessing, we also perform all the standard image trans-\nformations that are commonly applied in the supervised\nsetting (Krizhevsky et al., 2012), such as random cropping\nand ﬂipping of images.\nOptimization details.\nWe project the output of the net-\nwork on the ℓ2 sphere as in Tygert et al. (2017). The net-\nwork is trained with SGD with a batch size of 256. Dur-\ning the ﬁrst t0 batches, we use a constant step size. Af-\nter t0 batches, we use a linear decay of the step size, i.e.,\nlt =\nl0\n1+γ[t−t0]+ . Unless mentioned otherwise, we permute\nthe assignments within batches every 3 epochs. For the\ntransfer learning experiments, we follow the guideline de-\nscribed in Donahue et al. (2016).\n4. Experiments\nWe perform several experiments to validate different design\nchoices in NAT. We then evaluate the quality of our fea-\ntures by comparing them to state-of-the-art unsupervised\napproaches on several auxiliary supervised tasks, namely\nobject classiﬁcation on ImageNet and object classiﬁcation\nand detection of PASCAL VOC 2007 (Everingham et al.,\n2010).\nTransfering the features.\nIn order to measure the quality\nof our features, we measure their performance on transfer\nlearning. We freeze the parameters of all the convolutional\nlayers and overwrite the parameters of the MLP classiﬁer\nwith random Gaussian weights. We precisely follow the\ntraining and testing procedure that is speciﬁc to each of the\ndatasets following Donahue et al. (2016).\nDatasets and baselines.\nWe use the training set of Im-\nageNet to learn our convolutional network (Deng et al.,\n2009). This dataset is composed of 1, 281, 167 images that\nbelong to 1, 000 object categories. For the transfer learn-\ning experiments, we also consider PASCAL VOC 2007. In\naddition to fully supervised approaches (Krizhevsky et al.,\n2012), we compare our method to several unsupervised\napproaches, i.e., autoencoder, GAN and BiGAN as re-\nported in Donahue et al. (2016). We also compare to self-\nsupervised approaches, i.e., Agrawal et al. (2015); Doersch\net al. (2015); Pathak et al. (2016); Wang & Gupta (2015)\nand Zhang et al. (2016). Finally we compare to state-of-\nthe-art hand-made features, i.e., SIFT with Fisher Vectors\n(SIFT+FV) (S´anchez et al., 2013). They reduce the Fisher\nVectors to a 4, 096 dimensional vector with PCA, and apply\nan 8, 192 unit 3-layer MLP on top.\n4.1. Detailed analysis\nIn this section, we validate some of our design choices,\nlike the loss function, representations and the inﬂuences of\nsome parameters on the quality of our features. All the ex-\nperiments are run on ImageNet.\nSoftmax versus square loss.\nTable 1 compares the per-\nformance of an AlexNet trained with a softmax and a\nsquare loss. We report the accuracy on the validation set.\nThe square loss requires the features to be unit normal-\nized to avoid exploding gradients. As previously observed\nby Tygert et al. (2017), the performances are similar, hence\nvalidating our choice of loss function.\nEffect of image preprocessing.\nIn supervised classi-\nﬁcation, image pre-processing is not frequently used,\nand transformations that remove information are usually\navoided. In the unsupervised case, however, we observe\nthat is it is preferable to work with simpler inputs as\nUnsupervised Learning by Predicting Noise\nclean\nhigh-pass\nsobel\nacc@1\n59.7\n58.5\n57.4\nTable 2. Performance of supervised models with various image\npre-processings applied. We train an AlexNet on ImageNet, and\nreport classiﬁcation accuracy.\nit avoids learning trivial features.\nIn particular, we ob-\nserve that using grayscale image gradients greatly helps our\nmethod, as mentioned in Sec. 3. In order to verify that\nthis preprocessing does not destroy crucial information, we\npropose to evaluate its effect on supervised classiﬁcation.\nWe also compare with high-pass ﬁltering. Table 2 shows\nthe impact of this preprocessing methods on the accuracy\nof an AlexNet on the validation set of ImageNet. None\nof these pre-processings degrade the perform signiﬁcantly,\nmeaning that the information related to gradients are suf-\nﬁcient for object classiﬁcation. This experiment conﬁrms\nthat such pre-processing does not lead to a signiﬁcant drop\nin the upper bound performance for our model.\nContinuous versus discrete representations.\nWe com-\npare our choice for the target vectors to those commonly\nused for clustering, i.e., elements of the canonical basis of\na k dimensional space. Such discrete representation make a\nstrong assumption on the underlying structure of the prob-\nlem, that it can be linearly separated in k different classes.\nThis assumption holds for ImageNet giving a fair advan-\ntage to this discrete representation. We test this representa-\ntion with k in {103, 104, 105}, which is a range well-suited\nfor the 1, 000 classes of ImageNet. The matrix C contains\nn/k replications of k elements of the canonical basis. This\nassumes that the clusters are balanced, which is veriﬁed on\nImageNet.\nWe compare these cluster-like representations to our con-\ntinuous target vectors on the transfer task on ImageNet. Us-\ning discrete targets achieves an accuracy of 19%, which is\nsigniﬁcantly worse that our best performance, i.e., 33.5%.\nA possible explanation is that binary vectors induce sharp\ndiscontinuous distances between representations. Such dis-\ntances are hard to optimize over and may result in early\nconvergence to poorer local minima.\nEvolution of the features.\nIn this experiment, we are in-\nterested in understanding how the quality of our features\nevolves with the optimization of our cost function. Dur-\ning the unsupervised training, we freeze the network every\n20 epochs and learn a MLP classiﬁer on top. We report\nthe accuracy on the validation set of ImageNet. Figure 2\nshows the evolution of the performance on this transfer task\nas we optimize for our unsupervised approach. The train-\ning performance improves monotonically with the epochs\n50\n100\n150\n200\nepoch of unsupervised training\n20\n30\n40\n50\naccuracy\ntransfer train acc\ntransfer test acc\n2\n3\n4\n5\npermutation period\n20\n30\n40\n50\naccuracy\nFigure 2. On the left, we measure the accuracy on ImageNet after\ntraining the features with different permutation rates There is a\nclear trade-off with an optimum at permutations performed every\n3 epochs. On the right, we measure the accuracy on ImageNet\nafter training the features with our unsupervised approach as a\nfunction of the number of epochs. The performance improves\nwith longer unsupervised training.\nof the unsupervised training. This suggests that optimizing\nour objective function correlates with learning transferable\nfeatures, i.e., our features do not destroy useful class-level\ninformation. On the other hand, the test accuracy seems\nto saturate after a hundred epochs. This suggests that the\nMLP is overﬁtting rapidly on pre-trained features.\nEffect of permutations.\nAssigning images to their target\nrepresentations is a crucial feature of our approach. In this\nexperiment, we are interested in understanding how fre-\nquently we should update this assignment. Indeed, updat-\ning the assignment, even partially, is relatively costly and\nmay not be required to achieve good performance. Figure 2\nshows the transfer accuracies on ImageNet as a function of\nthe frequency of these updates. The model is quite robust\nto choice of frequency, with a test accuracy always above\n30%. Interestingly, the accuracy actually degrades slightly\nwith high frequency. A possible explanation is that the net-\nwork overﬁts rapidly to its own output, leading to relatively\nworse features. In practice, we observe that updating the\nassignment matrix every 3 epochs offers a good trade-off\nbetween performance and accuracy.\nVisualizing the ﬁlters.\nFigure 4 shows a comparison be-\ntween the ﬁrst convolutional layer of an AlexNet trained\nwith and without supervision. Both take grayscale gradient\nimages as input. The visualization are obtained by com-\nposing the Sobel ﬁltering with the ﬁlters of the ﬁrst layer\nof the AlexNet. Unsupervised ﬁlters are slightly less sharp\nthan their supervised counterpart, but still maintain edge\nand orientation information.\nNearest neighbor queries.\nOur loss optimizes a distance\nbetween features and ﬁxed vectors. This means that look-\ning at the distance between features should provide some\ninformation about the type of structure that our model cap-\nUnsupervised Learning by Predicting Noise\nFigure 3. Images and their 3 nearest neighbors in ImageNet according to our model using an ℓ2 distance. The query images are shown on\nthe top row, and the nearest neighbors are sorted from the closer to the further. Our features seem to capture global distinctive structures.\nFigure 4. Filters form the ﬁrst layer of an AlexNet trained on Im-\nageNet with supervision (left) or with NAT (right). The ﬁlters\nare in grayscale, since we use grayscale gradient images as input.\nThis visualization shows the composition of the gradients with the\nﬁrst layer.\ntures. Given a query image x, we compute its feature fθ(x)\nand search for its nearest neighbors according to the ℓ2 dis-\ntance. Figure 3 shows images and their nearest neighbors.\nThe features capture relatively complex structures in im-\nages.\nObjects with distinctive structures, like trunks or\nfruits, are well captured by our approach. However, this\ninformation is not always related to true labels. For exam-\nple, the image of bird over the sea is matched to images\ncapturing information about the sea or the sky rather than\nthe bird.\n4.2. Comparison with the state of the art\nWe report results on the transfer task both on ImageNet and\nPASCAL VOC 2007. In both cases, the model is trained on\nImageNet.\nImageNet classiﬁcation.\nIn this experiment, we evaluate\nthe quality of our features for the object classiﬁcation task\nof ImageNet. Note that in this setup, we build the unsuper-\nvised features on images that correspond to predeﬁned im-\nage categories. Even though we do not have access to cat-\negory labels, the data itself is biased towards these classes.\nIn order to evaluate the features, we freeze the layers up\nto the last convolutional layer and train the classiﬁer with\nsupervision. This experimental setting follows Noroozi &\nFavaro (2016).\nWe compare our model with several self-supervised ap-\nproaches (Wang & Gupta, 2015; Doersch et al., 2015;\nZhang et al., 2016) and an unsupervised approach,\ni.e., Donahue et al. (2016). Note that self-supervised ap-\nproaches use losses speciﬁcally designed for visual fea-\ntures. Like BiGANs (Donahue et al., 2016), NAT does not\nmake any assumption about the domain but of the struc-\nture of its features. Table 3 compares NAT with these ap-\nproaches.\nAmong unsupervised approaches, NAT compares favor-\nably to BiGAN (Donahue et al., 2016). Interestingly, the\nperformance of NAT are slightly better than self-supervised\nUnsupervised Learning by Predicting Noise\nMethod\nAcc@1\nRandom (Noroozi & Favaro, 2016)\n12.0\nSIFT+FV (S´anchez et al., 2013)\n55.6\nWang & Gupta (2015)\n29.8\nDoersch et al. (2015)\n30.4\nZhang et al. (2016)\n35.2\n1Noroozi & Favaro (2016)\n38.1\nBiGAN (Donahue et al., 2016)\n32.2\nNAT\n36.0\nTable 3. Comparison of the proposed approach to state-of-the-art\nunsupervised feature learning on ImageNet. A full multi-layer\nperceptron is retrained on top of the features. We compare to sev-\neral self-supervised approaches and an unsupervised approach,\ni.e., BiGAN (Donahue et al., 2016).\n1Noroozi & Favaro (2016)\nuses a signiﬁcantly larger amount of features than the original\nAlexNet. We report classiﬁcation accuracy.\nmethods, even though we do not explicitly use domain-\nspeciﬁc clues in images or videos to guide the learning.\nWhile all the models provide performance in the 30 −36%\nrange, it is not clear if they all learn the same features. Fi-\nnally, all the unsupervised deep features are outperformed\nby hand-made features, in particular Fisher Vectors with\nSIFT descriptors. This baseline uses a slightly bigger MLP\nfor the classiﬁer and its performance can be improved by\n2.2% by bagging 8 of these models. This difference of 20%\nin accuracy shows that unsupervised deep features are still\nquite far from the state-of-the-arts among all unsupervised\nfeatures.\nTransferring to PASCAL VOC 2007.\nWe carry out a\nsecond transfer experiment on the PASCAL VOC dataset,\non the classiﬁcation and detection tasks.\nThe model is\ntrained on ImageNet. Depending on the task, we ﬁnetune\nall layers in the network, or solely the classiﬁer, follow-\ning Donahue et al. (2016). In all experiments, the parame-\nters of the convolutional layers are initialized with the ones\nobtained with our unsupervised approach.\nThe parame-\nters of the classiﬁcation layers are initialized with gaussian\nweights. We get rid of batch normalization layers and use\na data-dependent rescaling of the parameters (Kr¨ahenb¨uhl\net al., 2015). Table 4 shows the comparison between our\nmodel and other unsupervised approaches. The results for\nother methods are taken from Donahue et al. (2016) except\nfor Zhang et al. (2016).\nAs with the ImageNet classiﬁcation task, our performance\nis on par with self-supervised approaches, for both de-\ntection and classiﬁcation.\nAmong purely unsupervised\napproaches, we outperform standard approaches like au-\ntoencoders or GANs by a large margin. Our model also\nClassiﬁcation\nDetection\nTrained layers\nfc6-8\nall\nall\nImageNet labels\n78.9\n79.9\n56.8\nAgrawal et al. (2015)\n31.0\n54.2\n43.9\nPathak et al. (2016)\n34.6\n56.5\n44.5\nWang & Gupta (2015)\n55.6\n63.1\n47.4\nDoersch et al. (2015)\n55.1\n65.3\n51.1\nZhang et al. (2016)\n61.5\n65.6\n46.9\nAutoencoder\n16.0\n53.8\n41.9\nGAN\n40.5\n56.4\n-\nBiGAN (Donahue et al., 2016)\n52.3\n60.1\n46.9\nNAT\n56.7\n65.3\n49.4\nTable 4. Comparison of the proposed approach to state-of-the-art\nunsupervised feature learning on VOC 2007 Classiﬁcation and de-\ntection. We either ﬁx the features after conv5 or we ﬁne-tune the\nwhole model. We compare to several self-supervised and an un-\nsupervised approaches. The GAN and autoencoder baselines are\nfrom Donahue et al. (2016). We report mean average prevision as\ncustomary on PASCAL VOC.\nperforms slightly better than the best performing BiGAN\nmodel (Donahue et al., 2016). These experiments conﬁrm\nour ﬁndings from the ImageNet experiments. Despite its\nsimplicity, NAT learns feature that are as good as those ob-\ntained with more sophisticated and data-speciﬁc models.\n5. Conclusion\nThis paper presents a simple unsupervised framework to\nlearn discriminative features. By aligning the output of a\nneural network to low-dimensional noise, we obtain fea-\ntures on par with state-of-the-art unsupervised learning ap-\nproaches. Our approach explicitly aims at learning discrim-\ninative features, while most unsupervised approaches target\nsurrogate problems, like image denoising or image genera-\ntion. As opposed to self-supervised approaches, we make\nvery few assumptions about the input space. This makes\nour appproach very simple and fast to train. Interestingly, it\nalso shares some similarities with traditional clustering ap-\nproaches as well as retrieval methods. While we show the\npotential of our approach on visual data, it will be interest-\ning to try other domains. Finally, this work only considers\nsimple noise distributions and alignment methods. A pos-\nsible direction of research is to explore target distributions\nand alignments that are more informative. This also would\nstrengthen the relation between NAT and methods based on\ndistribution matching like the earth mover distance.\nAcknowledgement.\nWe greatly thank Herv´e J´egou for\nhis help throughout the development of this project. We\nalso thank Allan Jabri, Edouard Grave, Iasonas Kokkinos,\nUnsupervised Learning by Predicting Noise\nL´eon Bottou, Matthijs Douze and the rest of FAIR for their\nsupport and helpful discussion. Finally, we thank Richard\nZhang, Jeff Donahue and Florent Perronnin for their help.\nReferences\nAgrawal, P., Carreira, J., and Malik, J. Learning to see by\nmoving. In ICCV, 2015.\nBach, F. and Harchaoui, Z. Diffrac: a discriminative and\nﬂexible framework for clustering. In NIPS, 2007.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.\nGreedy layer-wise training of deep networks. In NIPS,\n2007.\nBojanowski, P., Bach, F., Laptev, I., Ponce, J., Schmid, C.,\nand Sivic, J. Finding actors and actions in movies. In\nICCV, 2013.\nBojanowski, P., Lajugie, R., Bach, F., Laptev, I., Ponce,\nJ., Schmid, C., and Sivic, J. Weakly supervised action\nlabeling in videos under ordering constraints. In ECCV,\n2014.\nCoates, A. and Ng, A.\nLearning feature representations\nwith k-means. In Neural Networks: Tricks of the Trade.\nSpringer, 2012.\nDalal, N. and Triggs, B. Histograms of oriented gradients\nfor human detection. In CVPR, 2005.\nDeng, J., Dong, W., Socher, R., Li, L. J., Li, K., and\nFei-Fei, L. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\nDenton, E. L., Chintala, S., and Fergus, R. Deep generative\nimage models using a laplacian pyramid of adversarial\nnetworks. In NIPS, 2015.\nDoersch, C., Gupta, A., and Efros, A. Unsupervised visual\nrepresentation learning by context prediction. In CVPR,\n2015.\nDonahue, J., Kr¨ahenb¨uhl, P., and Darrell, T.\nAdversar-\nial feature learning. arXiv preprint arXiv:1605.09782,\n2016.\nDosovitskiy, A., Springenberg, J., Riedmiller, M., and\nBrox, T. Discriminative unsupervised feature learning\nwith convolutional neural networks. In NIPS, 2014.\nEveringham, M., Van Gool, L., Williams, C. K. I., Winn,\nJ., and Zisserman, A. The PASCAL visual object classes\n(VOC) challenge. IJCV, 2010.\nFukushima, K.\nNeocognitron: A self-organizing neural\nnetwork model for a mechanism of pattern recognition\nunaffected by shift in position. Biological Cybernetics,\n36:193–202, 1980.\nGirshick, R. Fast r-cnn. In CVPR, 2015.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. Generative adversarial nets. In NIPS, 2014.\nGoodman, J. Classes for fast maximum entropy training.\nIn ICASSP, 2001.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In CVPR, 2016.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerat-\ning deep network training by reducing internal covariate\nshift. arXiv preprint arXiv:1502.03167, 2015.\nJayaraman, D. and Grauman, K. Learning image represen-\ntations tied to ego-motion. In ICCV, 2015.\nJoulin, A. and Bach, F. A convex relaxation for weakly\nsupervised classiﬁers. In ICML, 2012.\nJoulin, A., Bach, F., and Ponce, J. Discriminative clustering\nfor image co-segmentation. In CVPR, 2010.\nJoulin, A., van der Maaten, L., Jabri, A., and Vasilache, N.\nLearning visual features from large weakly supervised\ndata. In ECCV, 2016.\nKingma, D. and Welling, M.\nAuto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nKohonen, T. Self-organized formation of topologically cor-\nrect feature maps. Biological cybernetics, 1982.\nKr¨ahenb¨uhl,\nPhilipp,\nDoersch,\nCarl,\nDonahue,\nJeff,\nand Darrell, Trevor.\nData-dependent initializations\nof convolutional neural networks.\narXiv preprint\narXiv:1511.06856, 2015.\nKrause, A., Perona, P., and Gomes, R. G. Discriminative\nclustering by regularized information maximization. In\nNIPS, 2010.\nKrizhevsky, A., Sutskever, I., and Hinton, G.\nImagenet\nclassiﬁcation with deep convolutional neural networks.\nIn NIPS, 2012.\nKuhn, H. W. The hungarian method for the assignment\nproblem. Naval research logistics quarterly, 2(1-2):83–\n97, 1955.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D.,\nHoward, R. E., Hubbard, W., and Jackel, L.D. Handwrit-\nten digit recognition with a back-propagation network.\nIn NIPS, 1989.\nLiao, R., Schwing, A., Zemel, R., and Urtasun, R. Learning\ndeep parsimonious representations. In NIPS, 2016.\nUnsupervised Learning by Predicting Noise\nLloyd, S. Least squares quantization in pcm. Transactions\non information theory, 28(2):129–137, 1982.\nLowe, D. Object recognition from local scale-invariant fea-\ntures. In ICCV, 1999.\nMairal, J., Koniusz, P., Harchaoui, Z., and Schmid, C. Con-\nvolutional kernel networks. In NIPS, 2014.\nMartinetz, T. and Schulten, K.\nA” neural-gas” network\nlearns topologies. 1991.\nMasci, J., Meier, U., Cires¸an, D., and Schmidhuber,\nJ. Stacked convolutional auto-encoders for hierarchical\nfeature extraction. In ICANN, 2011.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient\nestimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781, 2013.\nNoroozi, M. and Favaro, P. Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles. arXiv preprint\narXiv:1603.09246, 2016.\nOquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning\nand transferring mid-level image representations using\nconvolutional neural networks. In CVPR, 2014.\nPathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., and\nEfros, A. Context encoders: Feature learning by inpaint-\ning. In CVPR, 2016.\nPinheiro, P. O., Collobert, R., and Dollar, P. Learning to\nsegment object candidates. In NIPS, 2015.\nRamanathan, V., Joulin, A., Liang, P., and Fei-Fei, L. Link-\ning people in videos with their names using coreference\nresolution. In ECCV, 2014.\nRanzato, M. A., Huang, F. J., Boureau, Y. L., and LeCun,\nY. Unsupervised learning of invariant feature hierarchies\nwith applications to object recognition. In CVPR, 2007.\nRazavian, A. Sharif, Azizpour, H., Sullivan, J., and Carls-\nson, S. CNN features off-the-shelf: an astounding base-\nline for recognition. In arXiv 1403.6382, 2014.\nRubner, Y., Tomasi, C., and Guibas, L. J. A metric for\ndistributions with applications to image databases.\nIn\nICCV, 1998.\nS´anchez, J., Perronnin, F., Mensink, T., and Verbeek, J.\nImage classiﬁcation with the ﬁsher vector: Theory and\npractice. IJCV, 105(3):222–245, 2013.\nTang, K., Joulin, A., Li, L.-J., and Fei-Fei, L.\nCo-\nlocalization in real-world images. In CVPR, 2014.\nTygert, M., Chintala, S., Szlam, A., Tian, Y., and Zaremba,\nW. Scale-invariant learning and convolutional networks.\nApplied and Computational Harmonic Analysis, 42(1):\n154–166, 2017.\nvan den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals,\nO., and Graves, A. Conditional image generation with\npixelcnn decoders. In NIPS, 2016.\nVesanto, J. and Alhoniemi, E.\nClustering of the self-\norganizing map. Transactions on neural networks, 2000.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Man-\nzagol, P.-A. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local de-\nnoising criterion. JMLR, 11(Dec):3371–3408, 2010.\nWang, X. and Gupta, A. Unsupervised learning of visual\nrepresentations using videos. In ICCV, 2015.\nXie, J., Girshick, R., and Farhadi, A. Unsupervised deep\nembedding for clustering analysis. In ICML, 2016.\nXu, L., Neufeld, J., Larson, B., and Schuurmans, D. Maxi-\nmum margin clustering. In NIPS, 2004.\nYang, J., Parikh, D., and Batra, D. Joint unsupervised learn-\ning of deep representations and image clusters. In CVPR,\n2016.\nZhang, R., Isola, P., and Efros, A. Colorful image coloriza-\ntion. In ECCV, 2016.\nZhao, J., Mathieu, M., Goroshin, R., and LeCun, Y.\nStacked What-Where Auto-encoders.\nIn Workshop at\nICLR, 2016.\n",
  "categories": [
    "stat.ML",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2017-04-18",
  "updated": "2017-04-18"
}