{
  "id": "http://arxiv.org/abs/1704.05674v1",
  "title": "Unsupervised object segmentation in video by efficient selection of highly probable positive features",
  "authors": [
    "Emanuela Haller",
    "Marius Leordeanu"
  ],
  "abstract": "We address an essential problem in computer vision, that of unsupervised\nobject segmentation in video, where a main object of interest in a video\nsequence should be automatically separated from its background. An efficient\nsolution to this task would enable large-scale video interpretation at a high\nsemantic level in the absence of the costly manually labeled ground truth. We\npropose an efficient unsupervised method for generating foreground object\nsoft-segmentation masks based on automatic selection and learning from highly\nprobable positive features. We show that such features can be selected\nefficiently by taking into consideration the spatio-temporal, appearance and\nmotion consistency of the object during the whole observed sequence. We also\nemphasize the role of the contrasting properties between the foreground object\nand its background. Our model is created in two stages: we start from pixel\nlevel analysis, on top of which we add a regression model trained on a\ndescriptor that considers information over groups of pixels and is both\ndiscriminative and invariant to many changes that the object undergoes\nthroughout the video. We also present theoretical properties of our\nunsupervised learning method, that under some mild constraints is guaranteed to\nlearn a correct discriminative classifier even in the unsupervised case. Our\nmethod achieves competitive and even state of the art results on the\nchallenging Youtube-Objects and SegTrack datasets, while being at least one\norder of magnitude faster than the competition. We believe that the competitive\nperformance of our method in practice, along with its theoretical properties,\nconstitute an important step towards solving unsupervised discovery in video.",
  "text": "Unsupervised object segmentation in video by\nefﬁcient selection of highly probable positive features\nEmanuela Haller1,2\nhaller.emanuela@gmail.com\nMarius Leordeanu1,2\nmarius.leordeanu@imar.ro\n1University Politehnica of Bucharest\n313 Splaiul Independentei, Bucharest, Romania\n2Institute of Mathematics of the Romanian Academy\n21 Calea Grivitei, Bucharest, Romania\nAbstract\nWe address an essential problem in computer vision, that\nof unsupervised object segmentation in video, where a main\nobject of interest in a video sequence should be automati-\ncally separated from its background. An efﬁcient solution\nto this task would enable large-scale video interpretation at\na high semantic level in the absence of the costly manually\nlabeled ground truth. We propose an efﬁcient unsupervised\nmethod for generating foreground object soft-segmentation\nmasks based on automatic selection and learning from\nhighly probable positive features. We show that such fea-\ntures can be selected efﬁciently by taking into consideration\nthe spatio-temporal, appearance and motion consistency of\nthe object during the whole observed sequence. We also em-\nphasize the role of the contrasting properties between the\nforeground object and its background. Our model is cre-\nated in two stages: we start from pixel level analysis, on\ntop of which we add a regression model trained on a de-\nscriptor that considers information over groups of pixels\nand is both discriminative and invariant to many changes\nthat the object undergoes throughout the video. We also\npresent theoretical properties of our unsupervised learning\nmethod, that under some mild constraints is guaranteed to\nlearn a correct discriminative classiﬁer even in the unsu-\npervised case. Our method achieves competitive and even\nstate of the art results on the challenging Youtube-Objects\nand SegTrack datasets, while being at least one order of\nmagnitude faster than the competition. We believe that the\ncompetitive performance of our method in practice, along\nwith its theoretical properties, constitute an important step\ntowards solving unsupervised discovery in video.\n1. Introduction\nUnsupervised learning in video is a very challenging un-\nsolved task in computer vision. Many researchers believe\nthat solving this problem could shed new light on our un-\nderstanding of intelligence and learning from a scientiﬁc\nperspective. It could also have a signiﬁcant impact in many\nreal-world computer and robotics applications. The task is\ndrawing an increasing attention in the ﬁeld due in part to\nthe recent successes of deep neural network models in im-\nage recognition and to relatively low cost of collecting large\ndatasets of unlabeled videos.\nThere are several different published approaches for un-\nsupervised learning and discovery of the main object of in-\nterest in video [20, 12, 17, 16], but most have a high com-\nputational cost.\nIn general, algorithms for unsupervised\nmining and clustering are expected to be computationally\nexpensive due to the inherent combinatorial nature of the\nproblem [7].\nIn this paper we address the computational cost chal-\nlenge and propose a method that is both accurate and fast.\nWe achieve our goal based on a key insight: we focus on\nselecting and learning from features that are highly corre-\nlated with the presence of the object of interest and can be\nrapidly selected and computed. Note: in this paper, when\nreferring to highly probable positive features, we use ”fea-\nture” to indicate a feature vector sample, not a feature type.\nWhile we do not require these features to cover all instances\nand parts of the object of interest (we could expect low re-\ncall), we show that is is possible to ﬁnd, in the unsupervised\ncase, positive features with high precision (a large number\nof those selected are indeed true positives). Then we prove\ntheoretically that we can reliably train an object classiﬁer\nusing sets of positive and negative samples, both selected\nin an unsupervised way, as long as the set of features con-\nsidered to be positive has high precision, regardless of the\nrecall, if certain conditions are met (and they are often met\nin practice). We present an algorithm that can effectively\nand rapidly achieve this task in practice, in an unsupervised\nway, with state-of-the art results in difﬁcult experiments,\nwhile being at least 10x faster than its competition. The\nproposed method outputs both the soft-segmentation of the\n1\narXiv:1704.05674v1  [cs.CV]  19 Apr 2017\nFigure 1. Qualitative results of our method, which provides the soft-segmentation of the main object of interest and its bounding box.\nmain object of interest as well as its bounding box. Two\nexamples are shown in Figure 1.\nWhile we do not make any assumption about the type of\nobject present in the video, we do expect the sequence to\ncontain a single main foreground object. The key insights\nthat led to our formulation and algorithm are the following:\n1) First, the foreground and background are complementary\nand in contrast to each other - they have different sizes, ap-\npearance and movements. We observed that the more we\ncan take advantage of these contrasting properties the bet-\nter the results, in practice. While the background occupies\nmost of the image, the foreground is usually small and has\ndistinct color and movement patterns - it stands out against\nits background scene.\n2) The second main idea of our approach is that we should\nuse this foreground-background complementarity in order\nto automatically select, with high precision, foreground fea-\ntures, even if the expected recall is low. Then, we could reli-\nably use those samples as positives, and the rest as negative,\nto train a classiﬁer for detecting the main object of interest.\nWe present this formally in Sec 2.2.\nThese insights lead to our two main contributions in this\npaper: ﬁrst, we show theoretically that by selecting features\nthat are positive with highly probability, a robust classiﬁer\nfor foreground regions can be learned. Second, we present\nan efﬁcient method based on this insight, which in prac-\ntice outperforms its competition on many different object\nclasses, while being 10x faster.\nRelated work on object discovery in video: The task of\nobject discovery in video has been tackled for many years,\nwith early approaches being based on local features match-\ning [20, 12]. Current literature offers a wide range of so-\nlutions, with varying degrees of supervision, going from\nfully unsupervised methods [17, 16] to partially supervised\nones [10, 25, 24, 11, 21] - which start from region, object\nor segmentation proposals estimated by systems trained in\na supervised manner [1, 4, 3]. Some methods also require\nuser input for the ﬁrst frame of the video [8]. Most object\ndiscovery approaches that produce a ﬁne shape segmenta-\ntion of the object also make use of off-the-shelf shape seg-\nmentation methods [19, 5, 14, 2, 15].\n2. Approach\nOur method receives as input a video sequence, in which\nthere is a main object of interest, and it outputs its soft-\nsegmentation masks and associated bounding boxes. The\nproposed approach has, as starting point, a processing stage\nbased on principal component analysis of the video frames,\nwhich provides an initial soft-segmentation of the object\n- similar to the recent VideoPCA algorithm introduced as\npart of the object discovery approach of [21]. This soft-\nsegmentation usually has high precision but may have low\nrecall. Starting from this initial stage that classiﬁes pix-\nels independently based only on their individual color, next\nwe learn a higher level descriptor that considers groups of\npixel colors and is able to capture higher order statistics\nabout the object properties, such as different color patterns\nand textures. During the last stage we combine the soft-\nsegmentation based on appearance, with foreground cues\ncomputed from the contrasting motion of the main object\nvs. its scene. The resulting method is accurate and fast\n(≈3 fps in Matlab). Below, we summarize the steps of our\napproach (also see Figure 2).\n• Step 1: select highly probable foreground pixels based\non the differences between the original frames and the\nframes projected on their subspace with principal com-\nponent analysis (2.1).\n• Step 2: estimate empirical color distributions for fore-\nground and background from the pixel masks com-\nputed at Step 1. Use these distributions to estimate the\nprobability of foreground for each pixel independently\nbased on its color (2.1.1).\n• Step 3: improve the soft-segmentation from Step 2, by\nprojection on the subspace of soft-segmentations (2.3).\n• Step 4: re-estimate empirical color distributions for\nforeground and background from the pixel masks up-\ndated at Step 3. Use these distributions to estimate the\nprobability of foreground for each pixel independently\nbased on its color (2.1.1).\n• Step 5: learn a discriminative classiﬁer of foreground\nregions with regularized least squares regression on the\nsoft segmentation real output ∈[0, 1]. Use a feature\nvector that considers groups of colors that co-occur in\nlarger patches. Run classiﬁer at each pixel location in\nthe video and produce improved per frame foreground\nsoft-segmentation (2.4).\n• Step 6: combine soft-segmentation using appearance\n(Step 5) with foreground motion cues efﬁciently com-\nputed by modeling the background motion. Obtain the\nﬁnal soft-segmentation (2.5).\n• Step 7: Optional: reﬁne segmentation using Grab-\nCut\n[19], by considering as potential foreground\nand background samples the pixels given by the soft-\nsegmentation from Step 6. (2.5)\nFigure 2. Algorithm overview. a) original image b) ﬁrst pixel-level\nappearance model, based on initial object cues (Step 1 & Step 2)\nc) reﬁned pixel-level appearance model, built from the projection\nof soft-segmentation (Step 3 & Step 4) d) patch-level appearance\nmodel (Step 5) e) motion estimation mask (part of Step 6) f) ﬁnal\nsoft-segmentation mask (Step 6).\nWe reiterate: our algorithm has at its core two main\nideas.\nThe ﬁrst is that the object and the background\nhave contrasting properties in terms of size, appearance and\nmovement. This insight leads to the ability to select reliably\na few regions in the video that are highly likely to belong to\nthe object. The following, second idea, which brings certain\nformal guarantees, is that if we are able to select in an unsu-\npervised manner, even a small portion of the foreground ob-\nject but with high precision then, under some reasonable as-\nsumptions, we could train a robust foreground-background\nclassiﬁer that can be used for the automatic discovery of the\nobject. The pseudo-code of our approach is presented in\nAlgorithm 1. In Table 2 we introduce quantitative results of\nthe different stages of our Algorithm, along with the asso-\nciated execution times. In Table 1 we present the improve-\nments in precision, recall and f-measure between the differ-\nent steps of our algorithm. Note that the arrows go from the\nprecision and recall of the samples considered to be posi-\ntive during training, to the precision and recall of the pixels\nclassiﬁed during testing. The signiﬁcant improvement in\nf-measure is explained by our theoretical results (stated in\nProposition 1), which shows that under certain conditions,\na reliable classiﬁer will be learned even if the recall of the\ncorrupted positive samples is low, as long as the precision\nis relatively high.\nAlgorithm 1 Video object segmentation\n1: get input frames Fi\n2: PCA(A1) => V1 eigenvectors; A1(i, :) = Fi(:)\n3: R1 = ¯A1 + (A1 −¯A1) ∗V1 ∗VT\n1 - reconstruction\n4: P1\ni = d(A1(i, :), R1(i, :))\n5: P1\ni = P1\ni ⊗Gσ1\n6: P1\ni => pixel-level appearance model => S1\ni\n7: PCA(A2) => V2 eigenvectors; A2(i, :) = S1\ni(:)\n8: R2 = ¯A2 + (A2 −¯A2) ∗V2 ∗VT\n2 - reconstruction\n9: P2\ni = R2\ni ⊗Gσ2\n10: P2\ni => pixel-level appearance model => S2\ni\n11: D - data matrix containing patch-level descriptors\n12: s patch labels extracted from S2\ni\n13: select k features from D => Ds\n14: w = (λI + Ds\nT Ds)\n−1Ds\nT s\n15: evaluate => patch-level appearance model => S3\ni\n16: for each frame i do\n17:\ncompute Ix, Iy and It\n18:\nbuild motion matrix Dm\n19:\nwm = (Dm\nT Dm)\n−1Dm\nT It\n20:\ncompute motion model Mi\n21:\nMi = Mi ⊗Gσ\ni\n22:\ncombine S3\ni and Mi => S4\ni\n23: end for\n2.1. Select highly probable object regions\nWe estimate the initial foreground regions by Princi-\npal Component Analysis, an approach similar to the recent\nmethod for soft foreground segmentation, VideoPCA [21].\nOther approaches for soft foreground discovery could have\nbeen applied here, such as [26, 6, 9], but we have found the\ndirection using PCA to be both fast and reliable and to ﬁt\nperfectly with the later stages of our method. At this step\nwe ﬁrst project the frames on their subspace using PCA and\ncompute reconstruction error images as differences between\nthe original frames and their PCA reconstructed counter\nparts. If principal components are ui, i ∈[0 . . . nu] (we\nused nu = 3) and frame f projected on the subspace is\nfr ≈f0 + Pnu\ni=1((f −f0)⊤ui)ui, then we compute the er-\nror image fdiff = |f −fr|. High value pixels in the er-\nror image are more likely to belong to foreground. If we\nfurther smooth these regions with a large enough Gaussian\nand multiply the resulting smoothed difference with another\nlarge centered Gaussian (which favors objects in the center\nof the image), we obtain soft foreground masks that have\nhigh precision (most pixels on these masks indeed belong\nto true foreground), even though they often have low recall\n(only a small fraction of all object pixels are selected). As\ndiscussed, high precision and low recall is all need at this\nstage (see Table 1)\n2.1.1\nInitial soft-segmentation\nConsidering the small fraction of the object regions ob-\ntained at the previous step, the initial whole object soft\nsegmentation is computed by capturing foreground and\nbackground color distributions, followed by an independent\npixel-wise classiﬁcation. Let p(c|fg) and p(c|bg) be the\ntrue foreground (fg) and background (bg) probabilities for\na given color c. Using Bayes’ formula with equal priors,\nwe compute the probability of foreground for a given pixel,\nwith an associated color c, as p(fg|c) =\np(c|fg)\np(c|fg)+p(c|bg).\nThe foreground color likelihood is computed as p(c|fg) =\nn(c,fg)\nn(c) , where n(c, fg) is the number of considered fore-\nground pixels having color c and n(c) is the total number\nof pixels having color c. The background color likelihood\nis computed in a similar manner. Note that when comput-\ning the color likelihoods, we take into consideration infor-\nmation gathered from the whole movie, obtaining a robust\nmodel. The initial soft segmentation produced here is not\noptimal but it is computed fast (50 −100 fps) and of suf-\nﬁcient quality to insure the good performance of the sub-\nsequent stages. The ﬁrst two steps of the method follow\nthe algorithm VideoPCA ﬁrst proposed in [21]. In the next\nSec. 2.2 we present and prove our main theoretical result\n(Proposition 1), which explains in large part why our ap-\nproach is able to produce accurate object segmentations in\nan unsupervised way.\n2.2. Learning with HPP features\nIn Proposition 1 we show that a classiﬁer trained on cor-\nrupted sets of positive and negative samples, can learn the\nright thing as if true positives and negatives were used for\ntraining, if the following condition is met: the set of cor-\nrupted positives should contain positive samples in a pro-\nportion that is greater than the overall proportion of true\npositives in the whole training set. Let us start with the ex-\nample in Figure 3, where we have selected a set of samples\nS, inside the box, as being positive. The set S has high\nprecision (most samples are indeed positive), but low recall\n(most true positives are wrongly labeled). Next we show\nthat the sets S and ¬S could be used reliably (as deﬁned in\nProposition 1, below) to train a binary classiﬁer.\nLet p(E+) and p(E−) be the true distributions of posi-\ntive and negative elements, and p(x|S) and p(x|¬S) be the\nprobabilities of observing a sample inside and outside the\nconsidered positive set S and negative set ¬S, respectively.\nProposition 1 (learning from highly probable pos-\nitive (HPP) features):\nConsidering the following hy-\nFigure 3. Learning with HPP feature vectors. Essentially, Proposi-\ntion 1 shows that we could learn a reliable discriminative classiﬁer\nfrom a small set of corrupted positive samples, with the rest being\nconsidered negative, if the corrupted positive set contains mostly\ngood features such that the ratio of true positives in the corrupted\npositive set is greater than the overall ratio of true positives. This\nassumption can often be met in practice and efﬁciently used for\nunsupervised learning.\npotheses H1\n:\np(E+)\n<\nq\n<\np(E−),\nH2\n:\np(E+|S) > q > p(E−|S), where q ∈(0, 1), and H3 :\np(x|E+) and p(x|E−) are independent of S, then, for any\nsample x we have: p(x|S) > p(x|¬S) <=> p(x|E+) >\np(x|E−). In other words, a classiﬁer that classiﬁes pixels\nbased on their likelihoods w.r.t to S and ¬S will take the\nsame decision as if it was trained on the true positives and\nnegatives, and we refer to it as a reliable classiﬁer.\nProof: We express p(E−) as (p(E−)−p(E−|S)·p(S))\n(1−p(S))\n(Eq\n1), using the hypothesis and the sum rule of probabilities.\nConsidering (Eq 1), hypothesis H1, H2, and the fact that\np(S) > 0, we obtain that p(E−|¬S) > q (Eq 2). In a\nsimilar fashion, p(E+|¬S) < q (Eq 3). The previously\ninferred relations (Eq 2 and Eq 3) generate p(E−|¬S) >\nq > p(E+|¬S) (Eq 4), which along with hypothesis H2\nhelp as conclude that p(E+|S)\n>\np(E+|¬S) (Eq 5).\nAlso, from H3, we infer that p(x|E+, S) = p(x|E+)\nand p(x|E−, S) = p(x|E−) (Eq 6). Using the sum rule\nand hypothesis H3, we obtain that p(x|S) = p(E+|S) ·\n(p(x|E+)−p(x|E−))+p(x|E−) (Eq 7). In a similar way, it\nresults that p(x|¬S) = p(E+|¬S)·(p(x|E+)−p(x|E−))+\np(x|E−) (Eq 8).\np(x|S) > p(x|¬S) => p(x|E+) > p(x|E−): using the\nhypothesis and previously inferred results (Eq 5, 7 and 8) it\nresults that p(x|E+) > p(x|E−).\np(x|E+) > p(x|E−) => p(x|S) > p(x|¬S): from the\nhypothesis we can infer that p(x|E+) −p(x|E−) > 0, and\nusing (Eq 5) we obtain p(x|S) > p(x|¬S). □\n2.3. Object proposals reﬁnement\nThe result presented above provides a basis for both\nstages of our method, the one that classiﬁes pixels inde-\npendently based on their colors, and the second in which\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nprecision\n66 →70\n62 →60\n64 →74\nrecall\n17 →51\n45 →60\n58 →68\nf-measure\n27 →59\n53 →60\n61 →72\nTable 1. Evolution of precision, recall and f-measure of the feature\nsamples considered as positives (foreground) at different stages\nof our method. We start with a corrupted set of positive samples\nwith high precision and low recall, and improve both precision\nand recall through the stages of our method. Thus the soft masks\nbecome more and more accurate from one stage to the next.\nwe consider higher order color statistics among groups of\npixels that belong to different image patches. First, we im-\nprove the soft segmentations obtained so far, by projecting\nthem on their PCA subspace. Instead of looking at the dif-\nferences between original input and its PCA reconstruction,\nwe now simply replace the soft segmentations with the PCA\nprojected versions (using 8 principal components), thus re-\nducing the amount of noise that might be leftover from the\nprevious steps. The pseudocode of this step can also be seen\nin Algorithm 1.\n2.4. Considering color co-occurrences\nThe foreground masks obtained so far were computed by\ntreating each pixel independently, which results in masks\nthat are not always correct, as ﬁrst-order statistics, such\nas colors of individual pixels, cannot capture more global\ncharacteristics about object texture and shape. At this step\nwe move to the next level of abstraction by considering\ngroups of colors present in local patches, which are sufﬁ-\nciently large to capture object texture and local shape. We\ndeﬁne a patch descriptor based on local color occurrences,\nas an indicator vector dW over a given patch window W,\nsuch that dW (c) = 1 if color c is present in window W\nand 0 otherwise (Figure 4). Colors are indexed according\nto their values in HSV space, where channels H, S and V\nare discretized in ranges [1, 15], [1, 11] and [1, 7], generat-\ning a total of 1155 possible colors. The descriptor does not\ntake in consideration the exact spatial location of a given\ncolor in the patch, nor its frequency. It only accounts for\nthe presence of c in the patch. This leads to invariance to\nmost rigid or non-rigid transformations, while preserving\nthe local appearance characteristics of the object. Then, we\ntake a classiﬁcation approach and learn a classiﬁer (using\nregularized least squares regression, due to its considerable\nspeed and efﬁciency) to separate between highly probable\npositive (HPP) descriptors and the rest, collected from the\nwhole video according to the soft masks computed at the\nprevious step. The classiﬁer is generally robust to changes\nin viewpoint, scale, illumination, and other noises, while\nremaining discriminative (Figure 2).\nFigure 4. Initial patch descriptors encoding color occurrences (n\nnumber of considered colors).\nUnsupervised descriptor learning: Not all 1155 colors are\nrelevant for our classiﬁcation problem. Most object textures\nare composed of only a few important colors that distin-\nguishes them against the background scene. Effectively re-\nducing the number of colors in the descriptor and selecting\nonly the relevant ones can improve both speed and perfor-\nmance. We use the efﬁcient selection algorithm presented\nin [13]. The method proceeds as follows. Let n be the total\nnumber of colors and k < n the number of relevant colors\nwe want to select. The idea is to identify the group of k\ncolors with the largest amount of covariance - they will be\nthe ones most likely to select well the foreground versus the\nbackground (see [13] for details). Now consider C the co-\nvariance matrix of the colors forming the rows in the data\nmatrix D. The task is to solve the following optimization\nproblem:\nw∗= argmax\nw\nwT Cw\ns.t.\nn\nX\ni=1\nwi = 1, wi ∈[0, 1\nk ]\n(1)\nThe non-zero elements of w∗correspond to the col-\nors we need to select for creating our descriptor used by\nthe classiﬁer (based on regularized least squares regression\nmodel), so we deﬁne a binary mask ws ∈Rnx1 over the\ncolors (that is, the descriptor vector) as follows:\nws(i) =\n(\n1\nif w∗(i) > 0\n0\notherwise\n(2)\nThe problem above is NP-hard, but a good approximation\ncan be efﬁciently found by the method presented in [13],\nbased on a convergent series of integer projections on the\nspace of valid solutions.\nNext we deﬁne Ds ∈Rmx(1+k) to be the data matrix,\nwith a training sample per row, after applying the selection\nmask to the descriptor; m is the number of training sam-\nples and k is the number of colors selected to form the de-\nscriptor; we add a constant column of 1’s for the bias term.\nThen the weights w ∈R(1+k)x1 of the regularized regres-\nsion model are learned very fast, in closed-form:\nw = (λI + Ds\nT Ds)\n−1Ds\nT s\n(3)\nwhere I is the identity matrix, λ is the regularization term\nand s is the vector of soft-segmentation masks values (es-\ntimated at the previous step) corresponding to the samples\nchosen for training of the descriptor. The optimal number\nof selected colors is a relatively small fraction of the total\nnumber, as expected. Besides the slight increase in perfor-\nmance, the real gain is in the signiﬁcant decrease in compu-\ntation time (see Figure 5). Then, the ﬁnal appearance based\nsoft-segmentation masks are generated by evaluating the re-\ngression model for each pixel.\nFigure 5. Features selection - optimization and sensitivity analysis.\n2.5. Combining appearance and motion\nThe foreground and background have complementary\nproperties at many levels, not just that of appearance. Here\nwe consider that the object of interest must distinguish itself\nfrom the rest of the scene in terms of its motion pattern. A\nforeground object that does not move in the image, relative\nto its background, cannot be discovered using information\nfrom the current video alone. We take advantage of this idea\nby the following efﬁcient approach.\nLet It be the temporal derivative of the image as a func-\ntion of time, estimated as difference between subsequent\nframes It+1 −It. Also let Ix and Iy be the partial deriva-\ntives in the image w.r.t x and y. Consider Dm to be the\nmotion data matrix, with one row per pixel p in the current\nframe corresponding to [Ix, Iy, xIx, xIy, xIy, yIy] at loca-\ntions estimated as background by the foreground segmen-\ntation estimated so far. Given such a matrix at time t we\nlinearly regress It on Dm. The solution would be a least\nsquare estimate of an afﬁne motion model for the back-\nground using ﬁrst order Taylor expansion of the image w.r.t\ntime: wm = (Dm\nT Dm)\n−1Dm\nT It. Here wm contains the\nsix parameters deﬁning the afﬁne motion (including trans-\nlation) in 2D.\nThen, we consider deviations from this model as po-\ntential good candidates for the presence of the foreground\nobject, which is expected to move differently than the\nbackground scene.\nThe idea is based on an approxima-\ntion, of course, but it is very fast to compute and can be\nStep 2\nStep 4\nStep 5\nStep 6\nf-meas. (SegTrack)\n59.0\n60.0\n72.0\n74.6\nf-meas. (YTO)\n53.6\n54.5\n58.8\n63.4\nRuntime\n0.05\n0.03\n0.25\n0.02\nTable 2. Performance analysis and execution time for all stages of\nour method.\nreliably combined with the appearance soft-segmentation\nmask. Thus we evaluate the model in each location p and\ncompute errors |Dm(p)wm−It(p)|. We normalize the error\nimage and map it to [0, 1]. This produces a soft mask (us-\ning motion only) of locations that do not obey the motion\nmodel - they are usually correlated with object locations.\nThis map is then smoothed with a Gaussian (with σ propor-\ntional to the distribution on x and y of the estimated object\nregion).\nAt this point we have a soft object segmentation com-\nputed from appearance alone, and a second one computed,\nindependently based on motion cues. We then multiply the\nsoft results of the two independent pathways and obtain the\nﬁnal segmentation.\nOptional: reﬁnement of video object segmentation\nOp-\ntionally we can further reﬁne the soft-mask by applying an\noff-the-shelf segmentation algorithm, such as GrabCut [19]\nand feeding it our soft foreground segmentation. Note: in\nour experiments we used GrabCut only for evaluation on\nSegTrack, where we were interested in the ﬁne details of the\nobjects shape. All other experiments are performed without\nthis step.\n3. Experimental analysis\nOur experiments were performed on two datasets:\nYouTube-Objects dataset and SegTrack v2 dataset. We ﬁrst\nintroduce some qualitative results of our method, on the\nconsidered datasets (Figure 6). Note that for the ﬁnal evalu-\nation on the YouTube-Objects dataset, we also extract ob-\nject bounding boxes, that are computed using the distri-\nbution of the pixels with high probability of being part of\nthe foreground. Both position and size of the boxes are\ncomputed using a mean shift approach. For the ﬁnal eval-\nuation on the SegTrack dataset, we have reﬁned the soft-\nsegmentation masks, using the GrabCut algorithm [19]. In\nTabel 2 we present evaluation results for different stages of\nour algorithm, along with the execution time, per stage. The\nf-measure is increased with each stage of our algorithm.\n3.1. YouTube-Objects dataset\nDataset. The YouTube-Objects dataset [18] contains a\nlarge number of videos ﬁlmed in the wild, collected from\nYouTube. It contains challenging, unconstrained sequences\nof ten object categories (aeroplane, bird, boat, car, cat, cow,\nFigure 6. Qualitative results on YouTube-Objects dataset and SegTrack dataset.\ndog, horse, motorbike, train). The sequences are considered\nto be challenging as they are completely unconstrained, dis-\nplaying objects performing rapid movements, with difﬁcult\ndynamic backgrounds, illumination changes, camera mo-\ntion, scale and viewpoint changes and even editing effects,\nlike ﬂying logos or joining of different shots. The ground\ntruth is provided for a small number of frames, and contains\nbounding boxes for the object instances. Usually, a frame\ncontains only one primary object of the considered class,\nbut there are some frames containing multiple instances of\nthe same class of objects. Two versions of the dataset were\nreleased, the ﬁrst (YouTube-Objects v1.0) containing 1407\nannotated objects from a total of 570 000 frames, while\nthe second (YouTube-Objects v2.2) contains 6975 anno-\ntated objects from 720 000 frames.\nMetric.\nFor the evaluation on the YouTube-Objects\ndataset we have adopted the CorLoc metric, computing the\npercentage of correctly localized object bounding-boxes.\nWe evaluate the correctness of a provided box using the\nPASCAL-criterion (intersection over union ≥0.5).\nResults. We compare our method against [10, 25, 18,\n21, 17]. We considered their results as originally reported\nin the corresponding papers. The comparison is presented\nin Table 3. From our knowledge, the other methods were\nevaluated on YouTube-Objects v1.0, on the training sam-\nples (the only exception would be [21], where they have\nconsidered the full v1.0 dataset). Considering this, and the\ndifferences between the two versions, regarding the num-\nber of annotations, we have reported our performances on\nboth versions, in order to provide a fair comparison and also\nto report the results on the latest version, YouTube-Objects\nv2.2 (not considered for comparison). We report results of\nthe evaluation on v1.0 by only considering the training sam-\nples, for a fair comparison with other methods. Our method,\nwhich is unsupervised, is compared against both supervised\nand unsupervised methods. In the table, we have marked\nstate-of-the-art results for unsupervised methods (bold), and\noverall state-of-the-art results (underlined). We also men-\ntion the execution time for the considered methods, in order\nto prove that our method is one order of magnitude faster\nthan others (see section 3.3 for details).\nThe performances of our method are competitive, obtain-\ning state-of-the-art results for 3 classes, against both super-\nvised and unsupervised methods. If we report only to the\nunsupervised methods, we obtain state-of-the-art results for\n7 classes. On average, our method performs better than all\nthe others, and also in terms of execution time (also see sec-\ntion 3.3). The fact that, on average, our algorithm outper-\nforms other methods proves that it generalizes better for dif-\nferent classes of objects and different types of videos. Our\nsolution performs poorly on the ”horse” class, as many se-\nquences contain multiple horses, and our method is not able\nto correctly separate the instances. Another class with low\nperformances is the ”cow” class, where we deal with same\nproblems as in the case of ”horse” class, and in addition,\nthe objects are usually still, being hard to segment in our\nsystem.\n3.2. SegTrack v2 dataset\nDataset.\nThe SegTrack dataset was originally intro-\nduced by\n[22], for evaluating tracking algorithms. Fur-\nther, it was adapted for the task of video object segmen-\ntation [16]. We work with the second version of the dataset\n(SegTrack v2), which contains 14 videos (≈1000 frames),\nwith pixel level ground truth annotations for the foreground\nobject, in every frame. It contains challenging sequences\nwith complex backgrounds, objects with similar color pat-\nterns to the background, a wide variety of object sizes, cam-\nera motion and complex object deformations. There are ob-\njects from multiple categories: bird, cheetah, human, worm,\nmonkey, dog, frog and parachute. There are 8 videos with\nannotations for the primary object and 6 videos with 2 or\nmultiple objects.\nMetric. For the evaluation on the SegTrack we have\nMethod\nSupervised?\n[10]\nY\n[25]\nY\n[18]\nN\n[21]\nN\n[17]\nN\nOurs\nv1.0\nN\nOurs\nv2.2\nN\naeroplane\n64.3\n75.8\n51.7\n38.3\n65.4\n76.3\n76.3\nbird\n63.2\n60.8\n17.5\n62.5\n67.3\n71.4\n68.5\nboat\n73.3\n43.7\n34.4\n51.1\n38.9\n65.0\n54.5\ncar\n68.9\n71.1\n34.7\n54.9\n65.2\n58.9\n50.4\ncat\n44.4\n46.5\n22.3\n64.3\n46.3\n68.0\n59.8\ncow\n62.5\n54.6\n17.9\n52.9\n40.2\n55.9\n42.4\ndog\n71.4\n55.5\n13.5\n44.3\n65.3\n70.6\n53.5\nhorse\n52.3\n54.9\n48.4\n43.8\n48.4\n33.3\n30.0\nmotorbike\n78.6\n42.4\n39.0\n41.9\n39.0\n69.7\n53.5\ntrain\n23.1\n35.8\n25.0\n45.8\n25.0\n42.4\n60.7\nAvg\n60.2\n54.1\n30.4\n49.9\n50.1\n61.1\n54.9\ntime\nsec/frame\nN/A\nN/A\nN/A\n6.9\n4\n0.35\nTable 3. The CorLoc scores of our method and 5 other state-of-\nthe-art methods, on the YouTube-Objects dataset (note that result\nfor v2.2 of the dataset are not considered for comparison).\nadopted the average intersection over union metric.\nWe\nspecify that for the purpose of this evaluation, we use Grab-\nCut for reﬁnement of the soft-segmentation masks.\nResults. We compare our method against [11, 24, 23,\n17, 16]. We considered their results as originally reported\nby\n[23].\nThe comparison is presented in Table 4.\nAs\nfor the YouTube-Objects dataset, we compare our method\nagainst both supervised and unsupervised methods, and in\nthe table, we have marked state-of-the-art results for unsu-\npervised methods (bold), and overall state-of-the-art results\n(underlined). The execution times are also introduced, to\nhighlight that our method outperforms other approaches in\nterms of speed (see section 3.3).\nThe performances of our method are competitive, while\nbeing an unsupervised method, that beneﬁts of no tuning\ndepending on the testing database. Also, we prove that our\nmethod is one order of magnitude faster than the previous\nstate-of-the-art, in terms of speed, [17] (for details see sec-\ntion 3.3).\nMethod\nSupervised?\n[11]\nY\n[24]\nY\n[23]\nY\n[17]\nN\n[16]\nN\nOurs\nN\nbird of paradise\n92\n-\n95\n66\n94\n93\nbirdfall\n49\n71\n70\n59\n63\n58\nfrog\n75\n74\n83\n77\n72\n58\ngirl\n88\n82\n91\n73\n89\n69\nmonkey\n79\n62\n90\n65\n85\n69\nparachute\n96\n94\n92\n91\n93\n94\nsoldier\n67\n60\n85\n69\n84\n60\nworm\n84\n60\n80\n74\n83\n84\nAvg\n79\n72\n86\n72\n83\n73\ntime\nsec/frame\n>120\n>120\nN/A\n4\n242\n0.35\nTable 4. The average IoU scores of our method and 5 other state-\nof-the-art methods, on the SegTrack v2 dataset.\n3.3. Computation time\nOne of the main advantages of our method is the reduced\ncomputational time. Note that all per pixel classiﬁcations\ncan be efﬁciently implemented by linear ﬁltering routines,\nas all our classiﬁers are linear. It takes only 0.35 sec/frame\nfor generating the soft segmentation masks (initial object\ncues: 0.05 sec/frame, object proposals reﬁnement: 0.03\nsec/frame, patch-based regression model: 0.25 sec/frame,\nmotion estimation: 0.02 sec/frame (Tabel 2)). The method\nwas implemented in Matlab, with no special optimizations.\nAll timing measurements were performed using a computer\nwith an Intel core i7 2.60GHz CPU. The method of Papa-\nzoglou et al.\n[17] report a time of 3.5 sec/frame for the\ninitial optical ﬂow computation, on top of which they run\ntheir method, which requires 0.5 sec/frame, leading to a to-\ntal time of 4 sec/frame. The method introduced in [21] has\na total of 6.9 sec/frame. For other methods, like the one\nintroduced in [24, 11], it takes up to 120 sec/frame only\nfor generating the initial object proposals using the method\nof\n[3].\nThere is no available information regarding the\ncomputational time of other considered methods, but due to\ntheir complexity we expect them to be orders of magnitude\nslower than our method.\n4. Conclusions\nWe presented an efﬁcient fully unsupervised method for\nobject discovery in video that is both fast and accurate -\nit achieves state of the art results on a challenging bench-\nmark for bounding box object discovery and very compet-\nitive performance on an unsupervised segmentation video\ndataset. At the same time, our method is fast, being at least\nan order of magnitude faster than competition. We achieve\nan excellent combination of speed and performance by ex-\nploiting the contrasting properties between objects and their\nscenes, in terms of appearance and motion, which makes it\npossible to select positive feature samples with a very high\nprecision. We show theoretically that high precision is sufﬁ-\ncient for reliable unsupervised learning (since positives are\ngenerally less frequent than negatives), which we perform\nboth at the level of single pixels and at the higher level\nof groups of pixels to capture higher order statistics about\nobjects appearance, such as local texture and shape. The\nspeed and state of the art accuracy of our algorithm, com-\nbined with theoretical guarantees that hold in practice under\nmild conditions, make our approach unique and valuable in\nthe quest for solving the unsupervised learning problem in\nvideo.\nAcknowledgements:\nThis work was supported by UE-\nFISCDI, under project PN-III-P4-ID-ERC-2016-0007.\nReferences\n[1] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the ob-\njectness of image windows. IEEE transactions on pattern\nanalysis and machine intelligence, 34(11):2189–2202, 2012.\n[2] J. Carreira and C. Sminchisescu. Cpmc: Automatic object\nsegmentation using constrained parametric min-cuts. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n34(7):1312–1328, 2012.\n[3] I. Endres and D. Hoiem. Category independent object pro-\nposals. Computer Vision–ECCV 2010, pages 575–588, 2010.\n[4] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part-\nbased models. IEEE transactions on pattern analysis and\nmachine intelligence, 32(9):1627–1645, 2010.\n[5] B. Fulkerson, A. Vedaldi, and S. Soatto. Class segmentation\nand object localization with superpixel neighborhoods. In\nComputer Vision, 2009 IEEE 12th International Conference\non, pages 670–677. IEEE, 2009.\n[6] X. Hou and L. Zhang. Saliency detection: A spectral resid-\nual approach.\nIn Computer Vision and Pattern Recogni-\ntion, 2007. CVPR’07. IEEE Conference on, pages 1–8. IEEE,\n2007.\n[7] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering:\na review. ACM computing surveys (CSUR), 31(3):264–323,\n1999.\n[8] S. D. Jain and K. Grauman.\nSupervoxel-consistent fore-\nground propagation in video. In European Conference on\nComputer Vision, pages 656–671. Springer, 2014.\n[9] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.\nSalient object detection: A discriminative regional feature\nintegration approach. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2083–\n2090, 2013.\n[10] Y. Jun Koh, W.-D. Jang, and C.-S. Kim. Pod: Discovering\nprimary objects in videos based on evolutionary reﬁnement\nof object recurrence, background, and primary object mod-\nels. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 1068–1076, 2016.\n[11] Y. J. Lee, J. Kim, and K. Grauman. Key-segments for video\nobject segmentation. In Computer Vision (ICCV), 2011 IEEE\nInternational Conference on, pages 1995–2002. IEEE, 2011.\n[12] M. Leordeanu, R. Collins, and M. Hebert.\nUnsupervised\nlearning of object features from video sequences. In IEEE\nCOMPUTER SOCIETY CONFERENCE ON COMPUTER\nVISION AND PATTERN RECOGNITION, volume 1, page\n1142. IEEE Computer Society; 1999, 2005.\n[13] M. Leordeanu, A. Radu, S. Baluja, and R. Sukthankar.\nLabeling the features not the samples:\nEfﬁcient video\nclassiﬁcation with minimal supervision.\narXiv preprint\narXiv:1512.00517, 2015.\n[14] A. Levinshtein, A. Stere, K. N. Kutulakos, D. J. Fleet, S. J.\nDickinson, and K. Siddiqi. Turbopixels: Fast superpixels us-\ning geometric ﬂows. IEEE transactions on pattern analysis\nand machine intelligence, 31(12):2290–2297, 2009.\n[15] F. Li, J. Carreira, G. Lebanon, and C. Sminchisescu. Com-\nposite statistical inference for semantic segmentation.\nIn\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3302–3309, 2013.\n[16] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg. Video\nsegmentation by tracking many ﬁgure-ground segments. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, pages 2192–2199, 2013.\n[17] A. Papazoglou and V. Ferrari. Fast object segmentation in\nunconstrained video. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pages 1777–1784,\n2013.\n[18] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Fer-\nrari. Learning object class detectors from weakly annotated\nvideo. In Computer Vision and Pattern Recognition (CVPR),\n2012 IEEE Conference on, pages 3282–3289. IEEE, 2012.\n[19] C. Rother, V. Kolmogorov, and A. Blake. Grabcut: Interac-\ntive foreground extraction using iterated graph cuts. In ACM\ntransactions on graphics (TOG), volume 23, pages 309–314.\nACM, 2004.\n[20] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T.\nFreeman. Discovering objects and their location in images.\nIn Computer Vision, 2005. ICCV 2005. Tenth IEEE Interna-\ntional Conference on, volume 1, pages 370–377. IEEE, 2005.\n[21] O. Stretcu and M. Leordeanu. Multiple frames matching for\nobject discovery in video. In BMVC, pages 186–1, 2015.\n[22] D. Tsai, M. Flagg, A. Nakazawa, and J. M. Rehg. Motion\ncoherent tracking using multi-label mrf optimization. Inter-\nnational journal of computer vision, 100(2):190–202, 2012.\n[23] L. Wang, G. Hua, R. Sukthankar, J. Xue, Z. Niu, and\nN. Zheng. Video object discovery and co-segmentation with\nextremely weak supervision. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2016.\n[24] D. Zhang, O. Javed, and M. Shah. Video object segmentation\nthrough spatially accurate and temporally dense extraction\nof primary object regions. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n628–635, 2013.\n[25] Y. Zhang, X. Chen, J. Li, C. Wang, and C. Xia. Semantic\nobject segmentation via detection in weakly labeled video.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3641–3649, 2015.\n[26] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object\nproposals from edges. In European Conference on Computer\nVision, pages 391–405. Springer, 2014.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-04-19",
  "updated": "2017-04-19"
}