{
  "id": "http://arxiv.org/abs/1705.01734v2",
  "title": "Attributes2Classname: A discriminative model for attribute-based unsupervised zero-shot learning",
  "authors": [
    "Berkan Demirel",
    "Ramazan Gokberk Cinbis",
    "Nazli Ikizler-Cinbis"
  ],
  "abstract": "We propose a novel approach for unsupervised zero-shot learning (ZSL) of\nclasses based on their names. Most existing unsupervised ZSL methods aim to\nlearn a model for directly comparing image features and class names. However,\nthis proves to be a difficult task due to dominance of non-visual semantics in\nunderlying vector-space embeddings of class names. To address this issue, we\ndiscriminatively learn a word representation such that the similarities between\nclass and combination of attribute names fall in line with the visual\nsimilarity. Contrary to the traditional zero-shot learning approaches that are\nbuilt upon attribute presence, our approach bypasses the laborious\nattribute-class relation annotations for unseen classes. In addition, our\nproposed approach renders text-only training possible, hence, the training can\nbe augmented without the need to collect additional image data. The\nexperimental results show that our method yields state-of-the-art results for\nunsupervised ZSL in three benchmark datasets.",
  "text": "Attributes2Classname: A discriminative model for attribute-based\nunsupervised zero-shot learning\nBerkan Demirel1,3, Ramazan Gokberk Cinbis2, Nazli Ikizler-Cinbis3\n1HAVELSAN Inc., 2Bilkent University, 3Hacettepe University\nbdemirel@havelsan.com.tr, gcinbis@cs.bilkent.edu.tr, nazli@cs.hacettepe.edu.tr\nAbstract\nWe propose a novel approach for unsupervised zero-shot\nlearning (ZSL) of classes based on their names. Most ex-\nisting unsupervised ZSL methods aim to learn a model for\ndirectly comparing image features and class names. How-\never, this proves to be a difﬁcult task due to dominance\nof non-visual semantics in underlying vector-space embed-\ndings of class names. To address this issue, we discrimina-\ntively learn a word representation such that the similarities\nbetween class and combination of attribute names fall in\nline with the visual similarity. Contrary to the traditional\nzero-shot learning approaches that are built upon attribute\npresence, our approach bypasses the laborious attribute-\nclass relation annotations for unseen classes. In addition,\nour proposed approach renders text-only training possible,\nhence, the training can be augmented without the need to\ncollect additional image data.\nThe experimental results\nshow that our method yields state-of-the-art results for un-\nsupervised ZSL in three benchmark datasets.\n1. Introduction\nZero-shot learning (ZSL) enables identiﬁcation of\nclasses that are not seen before by means of transferring\nknowledge from seen classes to unseen classes. This knowl-\nedge transfer is usually done via utilizing prior informa-\ntion from various auxiliary sources, such as attributes (e.g.\n[20, 12, 27, 5, 35, 6, 4]), class hierarchies (e.g. [27]), vector-\nspace embeddings of class names (e.g. [35, 4, 6]) and tex-\ntual descriptions of classes (e.g. [22, 10]). Among these,\nattributes stand out as an excellent source of prior informa-\ntion: (i) thanks to their visual distinctiveness, it is possi-\nble to build highly accurate visual recognition models of at-\ntributes; (ii) being linguistically descriptive, attributes can\nnaturally be used to encode classes in terms of their vi-\nsual appearances, functional affordances or other human-\nHead\nStripe\nLong Leg\nTail\nZero-shot classes with \nunknown attributes\nGorilla\nZebra\nCow\nPanda\nDiscriminative \nword embeddings\nFigure 1: We propose a zero-shot recognition model based\non attribute and class names. Unlike most other attribute-\nbased methods, our approach avoids the laborious attribute-\nclass relations at test time, by discriminatively learning\na word-embedding space for predicting the unseen class\nname, based on combinations of attribute names.\nunderstandable aspects.\nAlmost all attribute-based ZSL works, however, have\nan important disadvantage: attribute-class relations need\nto be precisely annotated not only for the seen (training)\nclasses, but also for the unseen (zero-shot) classes (e.g.\n[12, 20, 27, 5]).\nThis usually involves collecting ﬁne-\ngrained information about attributes and classes, which is\na time-consuming and error-prone task limiting the scala-\nbility of the approaches to a great extent.\nSeveral recent studies explore other sources of prior in-\nformation to alleviate the need of collecting annotations\nat test time.\nThese approaches rely on readily available\nsources like word embeddings and/or semantic class hier-\narchies, hence, do not require dedicated annotation efforts.\nWe simply refer to these as unsupervised ZSL. Such ap-\nproaches, however, exclude attributes at the cost of exhibit-\ning a lower recognition performance [4].\nTowards combining the practical merit of unsupervised\nZSL with the recognition power of attribute-based meth-\n1\narXiv:1705.01734v2  [cs.CV]  5 Aug 2017\nods, we propose an attribute-based unsupervised ZSL ap-\nproach. The main idea is to discriminatively learn a vector-\nspace representation of words in which the combination of\nattributes relating to a class and the corresponding class\nname are mapped to nearby points.\nIn this manner, the\nmodel would map distinctive attributes in images to a se-\nmantic word vector space, using which we can predict un-\nseen classes solely based on their names. This idea is illus-\ntrated in Figure 1.\nOur use of vector space word embeddings differs sig-\nniﬁcantly from the way they are used in existing unsuper-\nvised ZSL methods: existing approaches (e.g. [35, 4]) aim\nto build a comparison function directly between image fea-\ntures and class names. However, learning such a compari-\nson function is difﬁcult since word embeddings are likely to\nbe dominated by non-visual semantics, due to lack of visual\ndescriptions in the large-scale text corpora that is used in\nthe estimation of the embedding vectors. Therefore, the re-\nsulting zero-shot models also tend to be dominated by non-\nvisual cues, which can degrade the zero-shot recognition ac-\ncuracy. To address this issue, we propose to use the names\nof visual attributes as an intermediate layer that connects the\nimage features and the class names in an unsupervised way\nfor the unseen classes.\nAn additional interesting aspect of our approach is the\ncapability of text-only training. Given pre-trained attribute\nmodels, the proposed ZSL model can be trained based on\ntextual attribute-class associations, without the need for ex-\nplicit image data even for training classes. This gives an ex-\ntreme ﬂexibility for scalability: the training set can be easily\nextended by enumerating class-attribute relationships, with-\nout the need for collecting accompanying image data. The\nresulting ZSL model can then be used for recognition of\nzero-shot classes for which no prior attribute information or\nvisual training example is available.\nWe provide an extensive experimental evaluation on two\nZSL object recognition and one ZSL action recognition\nbenchmark datasets.\nThe results indicate that the pro-\nposed method yields state-of-the-art unsupervised zero-shot\nrecognition performance both for object and cross-domain\naction recognition. Our unsupervised ZSL model also pro-\nvides competitive performance compared to the state-of-\nthe-art supervised ZSL methods. In addition, we experi-\nmentally demonstrate the success of our approach in the\ncase of text-only training. Finally, the qualitative results\nsuggest that the non-linear transformation of the proposed\napproach improves visual semantics of word embeddings,\nwhich can facilitate further research.\nTo sum up, our main contributions are as follows: (i)\nwe propose a novel method for discriminatively learning a\nword vector space representation for relating class and at-\ntribute combinations purely based on their names. (ii) We\nshow that the learned non-linear transformation improves\nthe visual semantics of word vectors.\n(iii) Our method\nachieves the state-of-the-art performance among unsuper-\nvised ZSL approaches and (iv) we show that by augmenting\nthe training dataset by additional class names and their at-\ntribute predicate matrices but no visual examples, a boost in\nperformance can be achieved.\n2. Related work\nInitial attempts towards zero-shot classiﬁcation were su-\npervised, in the sense that they require explicit attribute an-\nnotations of the test classes (e.g. [21, 20, 5, 27, 9, 16, 29,\n36, 38, 39]). Lampert et al. [21, 20] are among the ﬁrst\nto use attributes in this setting. They propose direct (DAP)\nand indirect attribute prediction (IAP) where attribute and\nclass relations are provided explicitly. Al-Halah et al. [5]\nintroduce hierarchy and apply attribute label propagation\non object classes, to utilize attributes at different abstrac-\ntion levels. Rohrbach et al. [27] propose a similar hierar-\nchical method, but they use only class taxonomies. Deng et\nal. [9] introduce Hierarchy and Exclusion (HEX) graphs as\na standalone layer to be used on top of any-feedforward ar-\nchitecture for classiﬁcation. Jayaraman and Grauman [16]\npropose a random forest approach to handle error tenden-\ncies of attributes. Romera et al. [29] develop two linear lay-\nered network to handle relations between classes, attributes\nand features. Zhang and Saligrama [36] propose a method\nto use semantic similarity embedding where target classes\nare represented with histograms of the source classes.\nAn important limitation of the aforementioned methods\nis their dependency on the attribute signatures of the test\nclasses.\nTo apply these approaches to additional unseen\nclasses, the attribute signatures of those new classes need to\nbe provided explicitly. Our method alleviates this need by\nlearning a word representation that allows zero-shot clas-\nsiﬁcation by comparing class names and attribute combi-\nnations, with no explicit prior information about attribute\nrelations of unseen classes.\nRecently, unsupervised ZSL methods are gaining more\nattention, due to their increased scalability.\nInstead of\nusing class-attribute relations at test time, various auxil-\niary sources of side information, such as textual informa-\ntion [22, 10] or word embeddings [3, 4, 25, 14, 6, 8] are\nexplored in such methods. Ba et al. [22] propose to com-\nbine MLP and CNN networks handling text based informa-\ntion acquired from Wikipedia articles and visual informa-\ntion of images, respectively. Another interesting direction\nis explored by Elhoseiny et al. [10], where the classiﬁers\nare built directly on textual corpus that is accompanied with\nimages.\nDistributional word representations, or word embed-\ndings, [23, 24, 26] are becoming increasingly popular\n[3, 4, 25, 14], due to the powerful vector-space represen-\n2\ntations where the distances can be meaningfully utilized.\nAkata et al. [3] propose attribute label embedding (ALE)\nmethod that uses textual data as side information in the\nWSABIE [34] formulation. Akata et al. [4] improve ALE\nby using embedding vectors that were obtained from large-\nscale text corpora. Frome et al. [14] propose a similar model\nwhere a pre-trained CNN model is ﬁne-tuned in an end-to-\nend way to relate images with semantic class embeddings.\nNorouzi et al. [25] proposes to use convex combinations\nof ﬁxed class name embeddings, weighted by class pos-\nterior probabilities given by a pre-trained CNN model, to\nmap images to the class name embedding space. In the re-\ncent approach of Akata et al. [2] language representations\nare utilized jointly with the stronger supervision given by\nvisual part annotations. Xian et al. [35] use multiple vi-\nsual embedding spaces to encode different visual character-\nistics of object classes. Jain et al. [15] and Kordumova et\nal. [18] leverage pre-trained object classiﬁers, and, action-\nobject similarities given by class embeddings to assign ac-\ntion labels to unseen videos.\nThe work closest to ours is Al-Halah et al. [6], which\nproposes an approach for using visual attributes in the unsu-\npervised ZSL setting. In their approach, a model is learned\nto predict whether an individual attribute is related to a class\nname or not. For this purpose, they learn a separate bilinear\ncompatibility function for each group of attributes, where\nsimilar attributes are grouped together to improve the per-\nformance. For unsupervised ZSL, this approach ﬁrst esti-\nmates the association of attributes with the test class, and\nthen employs an attribute-based ZSL method using the esti-\nmated class-attribute relations. Our approach differs in two\nmajor ways. First, instead of comparing classes with indi-\nvidual attribute names, we model the relationship between\nclass names and combinations of attribute names. Second,\nas opposed to handling class-attribute relation estimation\nand zero-shot classiﬁcation as two separate problems, we\ndiscriminatively train our attribute based ZSL model in an\nend-to-end manner.\n3. Method\nIn this section, we present the details of our approach.\nFirst, we explain our zero-shot learning model. Then, we\ndescribe how to train our ZSL model using discriminative\nimage-based training and predicate-based training formu-\nlations. Finally, we brieﬂy discuss our text-only training\nstrategy for incorporating additional classes during training.\n3.1. Zero-shot learning model\nWe deﬁne our ZSL model compatibility function\nf(x, y) : X × Y →R that measures the relevance of label\ny ∈Y for a given image x ∈X. Using this function, a\ntest image x can be classiﬁed simply by choosing the class\nmaximizing the compatibility score: arg maxy f(x, y).\nIn order to enable zero-shot learning of classes based on\nclass names only, we assume that an initial d0-dimensional\nvector space embedding ϕy ∈Rd0 is available for each\nclass y. These initial class name embeddings are obtained\nusing general purpose corpora, due to lack of a large-scale\ntext corpus dedicated for visual descriptions of objects. The\nrepresentations obtained by the class embeddings, hence,\nare typically dominated by non-visual semantics. For in-\nstance, according to the GloVe vectors, the similarity be-\ntween wolf and bear (both wild animals) is higher that the\nsimilarity between wolf and dog, though the latter pair is\nvisually much more similar to each other.\nThese observations suggest that learning a compatibil-\nity function directly between the image features and class\nembeddings may not be easy due to non-visual components\nof word embeddings. To address this issue, we propose to\nleverage attributes, which are appealing for the dual repre-\nsentation they provide: each attribute corresponds to (i) a\nvisual cue in the image domain, and, (ii) a named entity in\nthe language domain, whose similarity with class names can\nbe estimated using word embeddings. We deﬁne a function\nΦ(x) : X →Rd for embedding each image based on the\nattribute combination associated with it:\nΦ(x) =\n1\nP\na p(a|x)\nX\na\np(a|x)T(ϕa)\n(1)\nwhere p(a|x) is the posterior probability of attribute a1,\ngiven by a pre-trained binary attribute classiﬁer, ϕa is the\ninitial embedding vector of attribute a, and T : Rd0 →Rd\nis the transformation that we aim to learn. Similarly, we\ndeﬁne our class embedding function φ(y) : Y →Rd as\nthe transformation of the initial class name embeddings ϕy:\nφ(y) = T(ϕy).\nThe purpose of the function T is to transform the ini-\ntial word embeddings of attributes and classes such that\neach image, and its corresponding class are represented by\nnearby points in the d-dimensional vector embedding space.\nConsequently, we can deﬁne f(x, y) as a similarity measure\nbetween the image and class embeddings. In our approach,\nwe opt for the cosine-similarity:\nf(x, y) =\nΦ(x)Tφ(y)\n∥Φ(x)∥∥φ(y)∥\n(2)\nWe emphasize that our approach requires only the name of\nan unseen class at test time, as the compatibility function\nrelies solely on the learned attribute and class name embed-\ndings, rather than attribute-class relations.\nFigure 2 illustrates our zero-shot classiﬁcation approach.\nGiven an image, we ﬁrst apply the attribute predictors and\ncompute a weighted average of the attribute name embed-\ndings.\nThe class assignment is done by comparing the\n1The normalization in the denominator aims to make the embeddings\ncomparable across images with varying number of observed attributes.\n3\nDiscriminative attribute \nname embedding\nᶰ(antelope)\nᶰ(zebra)\nᶰ(rat)\np(а1|ᬕ)\n...\np(аN|ᬕ)\nAttribute Classification\nᶰ(beaver)\nᵔ(ᬕ)\nᬕ\nTest image\nDiscriminative class \nname embedding\nᶰ(tail)\nᶰ(long leg)\nᶰ(head)\nᶰ(stripe)\nFigure 2: Illustration of our unsupervised zero-shot recog-\nnition model. Prediction depends on the similarity between\ndiscriminatively learned representations of attribute combi-\nnations and class names. (Best viewed in color.)\nresulting embedding of attribute combination with that of\neach (unseen) class name. The image is then assigned to\nthe class with the highest cosine similarity.\nAs deﬁned above, the embeddings of attribute combi-\nnations and class names are functions of the shared trans-\nformation T(ϕ).2 In our experiments, we deﬁne T(ϕ) as\na two-layer feed-forward neural network. In the following\nsections, we describe techniques for discriminatively learn-\ning this transformation network.\n3.2. Image-based training (IBT)\nIn image-based training, we assume that there exists a\nsupervised training set S of N examples. Each example\nforms an image and class label pair. By deﬁnition, no ex-\nample in S belongs to one of the zero-shot test classes. Our\ngoal is to discriminatively learn the function f(x, y) such\nthat for each training example i, the compatibility score of\nthe correct class y = yi is higher than any other class yj,\nby a margin of ∆(yi, yj). More formally, the training con-\nstraint for the i-th training example is given by\nf(xi, yi) ≥f(xi, yj) + ∆(yi, yj),\n∀yj ̸= yi\n(3)\nThe margin function ∆indicates a non-negative pairwise\ndiscrepancy value for each pair of the training classes.\nAs explained in the previous section, f(x, y) is a func-\ntion of the transformation network T(ϕ). Let θ be the vector\nof all parameters in the transformation network. Inspired\nfrom the structural SVMs [33, 28], we formalize our ap-\n2In principle, one can separately deﬁne a T(ϕ) for attribute names,\nand, another one for class names. We have explored this empirically, but\ndid not observe a consistent improvement. Therefore, for the sake of sim-\nplicity, we use a shared transformation network in our experiments.\nproach as a constrained optimization problem:\nminθ,ξ λ||θ|| + PN\ni=1\nP\nyj̸=yi ξij\nf(xi, yi) ≥f(xi, yj) + ∆(yi, yj) −ξij\n∀yj ̸= yi, ∀i\n(4)\nwhere ξ is a vector of slack variables for soft-penalizing\nunsatisﬁed similarity constraints, and λ is the regulariza-\ntion weight.\nTo avoid optimization over non-linear con-\nstraints, we can equivalently express this problem as an un-\nconstrained optimization problem:\nminθ λ∥θ∥2\n2+\nPN\ni=1\nP\nyj̸=yi max (0, f(xi, yj) −f(xi, yi) + ∆(yi, yj))\n(5)\nUsing this formulation, the transformation T(ϕ) is learned\nin an discriminative and end-to-end manner, by ensuring\nthat the correct class score is higher than the incorrect ones,\nfor each image.\nWe empirically observe that cross-validating the num-\nber of iterations provides an effective regularization strat-\negy, therefore, we ﬁx λ = 0. We use average Hamming\ndistance between the attribute indicator vectors, which de-\nnote the list of attributes associated with each class, to com-\npute ∆values. This is the only point where we utilize the\nclass-attribute predicate matrix in our image-based training\napproach. In the absence of a predicate matrix, other types\nof ∆functions, like word embedding similarities, may be\nexplored, which we leave for future work. Other imple-\nmentation details are provided in Section 4.\n3.3. Predicate-based training (PBT)\nIn this section, we propose an alternative training ap-\nproach, which we call predicate-based training. In this ap-\nproach, the goal is to learn the ZSL model solely based\non the predicate matrix, which denotes the class-attribute\nrelations. While image-based training is deﬁned in terms\nof image-class similarities, we formulate predicate-based\ntraining in terms of class-class similarities, without directly\nusing any visual examples during training.\nThe predicate matrix consists of per-class indicator vec-\ntors, where each element is one if the corresponding at-\ntribute is associated with the class, and zero, otherwise. We\ndenote the indicator vector for class y by πy. Then, similar\nto image embedding function Φ(x), we deﬁne a predicate-\nembedding function Ψ(π):\nΨ(π) =\n1\nP\na π(a)\nX\na\nπ(a)T(ϕa).\n(6)\nThis embedding function is obtained by replacing posterior\nprobabilities in Eq. (1) by binary attribute-class relations.\nThen, we deﬁne a new compatibility function g(π, y), as\nthe cosine similarity between the vector Ψ(π) and vector\n4\nφ(y). This function is basically similar to Eq. (2), where the\nimage embedding Φ(x) is replaced by the attribute indicator\nembedding Ψ(π).\nFinally, we deﬁne the learning problem as optimizing the\nfunction g(x, y) such that for each class, the compatibility\nscore for its ideal set of attributes πy is higher than the at-\ntribute combination πy′ of another class y′, by a margin of\n∆(y, y′). This constraint aims to ensure that the similar-\nity between the name embedding of a set of attributes and\nthe embedding of a class name reliably indicates the visual\nsimilarity indicated by the predicate matrix.\nThis deﬁnition leads us to an unconstrained optimization\nproblem analogous to Eq. (5):\nminθ λ∥θ∥2\n2+\nPK\ny=1\nP\ny′̸=yi max (0, g(πy′, yi) −g(πyi, yi) + ∆(yi, y′))\n(7)\nwhere K indicates the number of training classes in the\npredicate matrix. As in image-based training, we deﬁne\n∆(y, y′) as the average Hamming distance between πy and\nπy′, and use λ = 0.\nFigure 3 illustrates the predicate-based training ap-\nproach. As shown in this ﬁgure, the main idea is to project\nthe ϕ word representations into a new space, where the sim-\nilarity between a class and an attribute combination in terms\nof their name vectors is indicative of their visual similar-\nity. At test time, we use the learned transformation net-\nwork in zero-shot classiﬁcation via the compatibility func-\ntion f(x, y) in Eq. (2). This compatibility function uses\nonly attribute classiﬁer outputs and the transformed word\nembeddings.\n3.4. Text-only training\nPredicate-based training, as explained in the previous\nsection, is completely based on a class-attribute predicate\nmatrix for the training classes, and training images are used\nonly for pre-training attribute classiﬁers that will be used at\ntest time. In contrast, image-based training, directly learns\nthe ZSL model based on attribute classiﬁcation probabilities\nin training images, therefore in principle, we expect image-\nbased training to perform better. This is, in fact, veriﬁed\nin our experimental results: while predicate-based training\nshows competitive accuracy, we obtain our state-of-the-art\nresults using image-based training.\nDespite the relatively lower performance of predicate-\nbased training, it has one interesting property: we can ex-\npand the training set by simply adding textual information\nfor additional novel classes into the predicate matrix. This\nallows improving the ZSL model by using classes with no\nvisual examples. We call incorporation of additional train-\ning classes in this manner as text-based training. In Sec-\ntion 4, we empirically show that it is possible to improve\nthe predicate-based training using text-based training.\nᶨ\nᶨ\nᶨ\nᶰ\nᶰ\nᶰ\nᶰ\nᶰ\nᶰ\nᶰ\nᵰ\nᶨ\nFigure 3: Illustration of our predicate-based training ap-\nproach, which uses only the predicate matrix of class and\nattribute relations as the source of supervision. The goal is\nto represent class and attribute combinations, based on their\nnames, in a space where each class is closest to its ideal\nattribute combination.\n4. Experiments\nTo evaluate the effectiveness of the proposed approach,\nwe consider two different ZSL applications: zero-shot ob-\nject classiﬁcation and zero-shot action recognition.\n4.1. Zero Shot Object Classiﬁcation\nIn this part, we explain our zero-shot object classiﬁcation\nexperiments on two common datasets namely AwA [20],\naPaY [13]. AwA dataset [20] contains 30,475 images of\n50 different animal classes. 85 per-class attribute labels are\nprovided in the dataset. In the predeﬁned split for zero-shot\nlearning, 40 animal classes are marked for training and 10\nclasses for testing. aPaY dataset [13] is formed of images\nobtained from two different sources. aPascal (aP) part of\nthis dataset is obtained from PASCAL VOC 2008 [11]. This\npart contains 12,695 images of 20 different classes. The\nsecond part, aYahoo (aY), is collected using Yahoo search\nengine and contains 2,644 images of 12 object classes com-\npletely different from aPascal classes. Images are anno-\ntated with 64 binary per-image attribute labels. In zero-\nshot learning settings on this dataset, aPascal part is used\nfor training and aYahoo part is used for testing. We follow\nthe same experimental setup as in [5] and only use training\nsplit of aPascal part to learn attribute classiﬁers.\nAttribute Classiﬁers. We use CNN-M2K features [5] to\nencode images and train attribute classiﬁers. We resize each\nimage to 256x256 and then subtract the mean image. Data\n5\nTable 1: Zero-shot classiﬁcation performance of proposed\npredicate-based (PBT) and image-based (IBT) methods on\nAwA and aPaY datasets. We report normalized accuracy.\nMethod\nAwA\naPaY\nBaseline\n10.2\n16.0\nPBT\n60.7\n29.4\nIBT\n69.9\n38.2\naugmentation is applied via using ﬁve different crops and\ntheir ﬂipped versions. Outputs of fc7 layer are used, result-\ning in 2,048 dimensional feature vectors. Following [13],\nwe obtain the attribute classiﬁers by training ℓ2-regularized\nsquared-hinge-loss linear SVMs.\nParameter selection is\ndone using 10-fold cross validation over the training set and\nPlatt scaling is applied to map the attribute prediction scores\nto posterior probabilities. For image-based training, cross-\nvalidation outputs are used as the classiﬁcation scores in\ntraining images.\nWord Embeddings. For each class and attribute name, we\ngenerate a 300-dimensional word embedding vector using\nGloVe [26] based on Common Crawl Data3. These word\nvectors are publicly available4. For those names that consist\nof multiple words, we use the average of the word vectors.\nWord Representation Learning. We deﬁne the transfor-\nmation function as a two layer feed-forward network. We\nuse 2-fold cross-validation over the training set to select\nnumber of hidden units and number of iterations. tanh func-\ntion is used as the activation function in the ﬁrst hidden layer\nand sigmoid function is used in the second hidden layer.\nAdam [17] is used for stochastic optimization, and learn-\ning rate value is set to 1e-4. Implementation is done using\nTensorFlow [1].5\nResults. In our experiments, we ﬁrst evaluate the perfor-\nmance of attribute classiﬁers, since this is likely to have\na signiﬁcant inﬂuence on zero-shot classiﬁcation. The at-\ntribute classiﬁers yield 80.56% mean AUC on the AwA\ndataset, 84.91% mean AUC on the aPaY dataset. These re-\nsults suggest that our attribute classiﬁers are relatively accu-\nrate, if not perfect. Further improvements in attribute classi-\nﬁcation are likely to have a positive impact on the ﬁnal ZSL\nperformance.\nTable 1 presents the experimental results for our ap-\nproach. In this table, baseline represents the case where\nthe transformation T(ϕ) is deﬁned as an identity mapping.\nPBT (predicate-based training) represents our proposed ap-\nproach that learns a transformation using the attribute predi-\n3 commoncrawl.org/the-data/\n4 nlp.stanford.edu/projects/glove/\n5 github.com/berkandemirel/attributes2classname\npers.cat\nhippo.\nleopard\nh.whale\nseal\nchimp.\ng.panda\nrat\npig\nraccoon\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPBT\nIBT\nFigure 4: Class-wise prediction accuracies on AwA Dataset.\ncate matrix, whereas IBT (image-based training) represents\nlearning transformation using training images. The results\nin Table 1 shows the importance and success of our learn-\ning formulations, compared to the baseline. In addition, we\nobserve that image-based training outperforms predicate-\nbased training on average, which is in accordance with our\nexpectations. Class-wise accuracy comparison of PBT and\nIBT methods is given in Figure 4. We observe that some\nof the classes respond particularly well to the image-based\ntraining.\nTable 2 presents a comparison of our results against\na number of supervised and unsupervised ZSL methods.\nIn this table, the supervision corresponds to the informa-\ntion needed during test time for zero-shot learning: the su-\npervised methods require additional data about the unseen\nclasses such as attribute-class predicate matrices, whereas\nunsupervised methods do not require any explicit inputs\nabout test classes. Hence, supervised methods have a very\nmajor advantage in this comparison, as they employ exter-\nnal attribute signatures of test classes. In contrast, unsuper-\nvised methods carry out zero-shot classiﬁcation among the\ntest classes without using data additional to the training set.\nFinally, we note that, we exclude ZSL methods that oper-\nate on low-level visual image features, as their results are\nnot directly comparable. Instead, for the sake of fair com-\nparison, we only compare to those methods that use similar\nconvolutional neural network based image representations.\nFrom Table 2 we see that on AwA and aPaY datasets,\nour unsupervised ZSL method yields state-of-the-art classi-\nﬁcation performance compared to other unsupervised ZSL\nmethods. In addition, our method performs on par with\nsome of the supervised ZSL methods.\n4.2. Zero Shot Action Recognition\nFor zero-shot action recognition, we evaluate our ap-\nproach on UCF-Sports Action Recognition Dataset [30].\nThe dataset is formed of videos from various sport actions\nwhich are featured from television channels such as the\nBBC and ESPN, and contains a total of 150 videos of 10\ndifferent sport action classes.\n6\nTable 2: Comparison to state-of-the-art ZSL methods (un-\nsupervised and supervised).\nTest supervision\nMethod\nAwA\naPaY\nunsupervised\nDeViSE[14]\n44.5\n25.5\nConSE[25]\n46.1\n22.0\nText2Visual[10, 7]\n55.3\n30.2\nSynC[8]\n57.5\n-\nALE[4]\n58.8\n33.3\nLatEm[35]\n62.9\n-\nCAAP[6]\n67.5\n37.0\nOur method\n69.9\n38.2\nsupervised\nDAP[20]\n54.0\n28.5\nENS[27]\n57.4\n31.7\nHAT[5]\n63.1\n38.3\nALE-attr[4]\n66.7\n-\nSSE-INT[36]\n71.5\n44.2\nSSE-ReLU[36]\n76.3\n46.2\nSynC-attr[8]\n76.3\n-\nSDL[38]\n79.1\n50.4\nJFA[37]\n81.0\n52.0\nWord Embeddings.\nFollowing [15], we utilize 500-\ndimensional word embedding vectors generated with\nthe skip-gram model of word2vec [23] learned over\nYFCC100M [32] dataset.\nYFCC100M dataset contains\nmetadata tags of about 100M Flickr images and the word\nvectors obtained from YFCC100M are publicly available6.\nObject Classiﬁers. Since there is no explicit deﬁnition of\nattributes for actions, the object cues can be leveraged in-\nstead of attributes, as suggested by [15]. To this end, we\nobtain predicate matrices from the textual data by measur-\ning the cosine similarity between actions and object clas-\nsiﬁcation scores. We operate on the object classiﬁcation\nresponses made available by [15]6. These are obtained by\nAlexNet[19], where every 10th frame is sampled for each\nvideo and each sampled frame is represented with the total\nof 15,293 ImageNet object categories. Average pooling is\napplied afterwards, so that each video is represented with\n15,293 dimensional vectors. To have a fair comparison, we\nalso apply the sparsiﬁcation step of [15] using the same pa-\nrameters. This sparsiﬁcation is done for eliminating noisy\nobject classiﬁcation responses.\nWord Representation Learning. Model learning settings\nare the same with those of ZSL object classiﬁcation exper-\niments, with the exception that only image-based loss is\nused, because predicate matrices are not available during\ntraining. Since we do not have any training data for target\ndatasets, we train our transformation function with a differ-\nent dataset (i.e. UCF-101 [31]). To avoid any overlap be-\n6 staff.fnwi.uva.nl/m.jain/projects/Objects2action.html\nTable 3: Zero-shot action recognition accuracies.\nMethod\nUCF-Sport\nDAP[20]\n11.7\nobjects2action[15]\n26.4\nOur method\n28.3\nTable 4: Zero-shot learning using external training class\nnames and their predicate matrices. These EXT classes con-\nsist of class names outside AwA dataset and do not include\nimage data. The method is trained only on class names and\ntheir predicate matrices. We report normalized accuracy.\nMethod\nTrain Classes\nAccuracy\nPBT\nEXT\n44.0\nPBT\nAWA\n60.7\nPBT\nAWA+EXT\n63.0\ntween datasets, we exclude the common action classes from\nthe training set for an accurate zero-shot setting. Some of\nsuch common classes that are excluded from training are\nDiving and Horse Riding.\nResults.\nWe\ncompare\nour\napproach\nwith\nOb-\njects2Action [15] and DAP [20] methods. The normalized\naccuracy results are shown in Table 3. From these results\nwe see that our approach for relating action names and\nobject cues in the transformed word vector space yields\npromising results in UCF-Sport dataset.\nThese results\nshow that our embedding transformation function carries\nsubstantial semantic information not only between training\nand test sets, but also across datasets.\n4.3. Training on Textual Data\nAs stated before, one of the interesting aspects of our\nformulation is the ability to train over only textual data (i.e.\nnames of attributes, objects and classes), without having any\nvisual examples of training classes. In this case, using our\nmodel, we can use the pre-trained attribute classiﬁers, to-\ngether with the learned semantic word vector representation\nand predict the class of a newly seen example.\nTo demonstrate the effect, we select 20 classes outside\nthe AwA dataset from Wikipedia Animal List7, and build\nan attribute-class predicate matrix. We then learn the cor-\nresponding semantic vector space using only these classes\nthat have no image data. The results are shown in Table 4.\nNote that, here, we only train the PBT model, because IBT\nis based on image data. Training our model using only addi-\ntional textual class names and their corresponding attribute\npredicate matrices gives an impressive accuracy of 44.0%.\nMoreover, when we augment the AwA train set with these\n7 en.wikipedia.org/wiki/List_of_animal_names\n7\nK. Whale\nB. Whale\nElephant\nWalrus\nB. Whale\nWalrus\nP. Bear\nB. Whale\nDolphin\nWalrus\nMole\nWeasel\nS. Cat\nB. Whale\nSquirrel\nBeaver\nMouse\nMouse\nHamster\nBat\nWolf\nP. Bear\nG. Bear\nFox\nG. Bear\nShepherd\nFox\nFox\nBobcat\nShepherd\nFigure 5: Top-3 most similar classes for some example classes from the AwA dataset. The similarities of the class word\nvectors are measured by cosine similarity. The images shown depict class representatives. From left-to-right, the columns\nshow the query class (ﬁrst column), and the most similar classes according to raw word embeddings (second column), those\nusing the transformation learned by PBT (third column), and those using the transformation learned by IBT (fourth column),\nrespectively.\nadditional class names and their predicate matrix, the ac-\ncuracy improves from 60.7% to 63.0%. These results sug-\ngest that the performance of the proposed model can be im-\nproved by just enumerating additional class names and their\ncorresponding attribute lists, without necessarily collecting\nadditional image data.\n4.4. Visual Similarities of Word Vectors\nOne of the favorable aspects of our method is that it can\nlead to visually more consistent word embeddings of visual\nentities. To demonstrate this, Figure 5 shows the similari-\nties across the classes according to the original and trans-\nformed word embeddings in the AwA dataset. In the ﬁrst\nrow, we see that while one of the most similar classes to the\nkiller whale is elephant using the original embeddings, this\nchanges to the dolphin class after using the transformation\nlearned by IBT. We observe similar improvements for other\nclasses, such as mole (second row) and wolf (third row), for\nwhich the word embeddings transformed by PBT or IBT\ntraining lead to visually more sensible word similarities.\n4.5. Randomly Sampled Vectors\nTo quantify the importance of initial word embeddings,\nwe evaluate our approach on the AwA dataset by using vec-\ntors sampled from a uniform distribution, instead of pre-\ntrained GloVe vectors. In this case, PBT yields 28.6%, and\nIBT yields 13.6% top-1 classiﬁcation accuracy, which are\nsigniﬁcantly lower than our actual results (PBT 69.9% and\nIBT 60.7%). This observation highlights the importance of\nleveraging prior knowledge derived from unsupervised text\ncorpora through pre-trained word embeddings.\n5. Conclusion\nAn important limitation of the existing attribute-based\nmethods for zero-shot learning is their dependency on the\nattribute signatures of the unseen classes. To eliminate this\ndependency, in this work, we leverage attributes as an in-\ntermediate representation, in an unsupervised way for the\nunseen classes. To this end, we learn a discriminative word\nrepresentation such that the similarities between class and\nattribute names follow the visual similarity, and use this\nlearned representation to transfer knowledge from seen to\nunseen classes. Our proposed zero-shot learning method is\neasily scalable to work with any unseen class without re-\nquiring manually deﬁned attribute-class annotations or any\ntype of auxiliary data.\nExperimental results on several benchmark datasets\ndemonstrate the efﬁciency of our approach, establishing the\nstate-of-the-art among the unsupervised zero-shot learning\nmethods. The qualitative results show that the non-linear\ntransformation using the proposed approach improves dis-\ntributed word vectors in terms of visual semantics. In ad-\ndition, we show that by adding just text-based class names\nand their attribute signatures, the training set can be easily\nextended, which can further boost the performance.\nReferences\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.\nTensorﬂow: Large-scale machine learning on heterogeneous\nsystems, 2015. 6\n[2] Z. Akata, M. Malinowski, M. Fritz, and B. Schiele. Multi-\ncue zero-shot learning with strong supervision.\nIn Proc.\n8\nIEEE Conf. Comput. Vis. Pattern Recog., pages 59–68, 2016.\n3\n[3] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-\nembedding for attribute-based classiﬁcation. In Proc. IEEE\nConf. Comput. Vis. Pattern Recog., pages 819–826, 2013. 2\n[4] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Eval-\nuation of output embeddings for ﬁne-grained image classi-\nﬁcation. In Proc. IEEE Conf. Comput. Vis. Pattern Recog.,\npages 2927–2936, 2015. 1, 2, 3, 7\n[5] Z. Al-Halah and R. Stiefelhagen. How to transfer? zero-shot\nobject recognition via hierarchical transfer of semantic at-\ntributes. In IEEE Winter Conf. on Applications of Computer\nVision, pages 837–843. IEEE, 2015. 1, 2, 5, 7\n[6] Z. Al-Halah, M. Tapaswi, and R. Stiefelhagen. Recovering\nthe missing link: Predicting class-attribute associations for\nunsupervised zero-shot learning. In Proc. IEEE Conf. Com-\nput. Vis. Pattern Recog., pages 5975–5984, 2016. 1, 2, 3,\n7\n[7] L. Bo and C. Sminchisescu.\nTwin gaussian processes for\nstructured prediction.\nInt. J. on Computer Vision, 87(1-\n2):28–52, 2010. 7\n[8] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthe-\nsized classiﬁers for zero-shot learning. In Proc. IEEE Conf.\nComput. Vis. Pattern Recog., pages 5327–5336, 2016. 2, 7\n[9] J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy, S. Bengio,\nY. Li, H. Neven, and H. Adam. Large-scale object classiﬁca-\ntion using label relation graphs. In Proc. European Conf. on\nComputer Vision, pages 48–64. Springer, 2014. 2\n[10] M. Elhoseiny, B. Saleh, and A. Elgammal.\nWrite a clas-\nsiﬁer: Zero-shot learning using purely textual descriptions.\nIn Proc. IEEE Int. Conf. on Computer Vision, pages 2584–\n2591, 2013. 1, 2, 7\n[11] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\nA. Zisserman. The PASCAL Visual Object Classes Chal-\nlenge 2008 Results. 5\n[12] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing\nobjects by their attributes. In Proc. IEEE Conf. Comput. Vis.\nPattern Recog., 2009. 1\n[13] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing\nobjects by their attributes. In Proc. IEEE Conf. Comput. Vis.\nPattern Recog., pages 1778–1785. IEEE, 2009. 5, 6\n[14] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,\nT. Mikolov, et al. Devise: A deep visual-semantic embed-\nding model. In Proc. Adv. Neural Inf. Process. Syst., pages\n2121–2129, 2013. 2, 3, 7\n[15] M. Jain, J. C. van Gemert, T. Mensink, and C. G. Snoek.\nObjects2action: Classifying and localizing actions without\nany video example. In Proc. IEEE Int. Conf. on Computer\nVision, pages 4588–4596, 2015. 3, 7\n[16] D. Jayaraman and K. Grauman. Zero-shot recognition with\nunreliable attributes. In Proc. Adv. Neural Inf. Process. Syst.,\npages 3464–3472, 2014. 2\n[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-\nmization. In Proc. Int. Conf. Learn. Represent., 2015. 6\n[18] S. Kordumova, T. Mensink, and C. G. Snoek. Pooling objects\nfor recognizing scenes without examples. In Proc. ACM Int.\nConf. Multimedia Retrieval, pages 143–150. ACM, 2016. 3\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-\nsiﬁcation with deep convolutional neural networks. In Proc.\nAdv. Neural Inf. Process. Syst., pages 1097–1105, 2012. 7\n[20] C. Lampert, H. Nickisch, and S. Harmeling.\nAttribute-\nbased classiﬁcation for zero-shot visual object categoriza-\ntion. IEEE Trans. Pattern Anal. Mach. Intell., 36(3):453–\n465, March 2014. 1, 2, 5, 7\n[21] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to\ndetect unseen object classes by between-class attribute trans-\nfer. In Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages\n951–958. IEEE, 2009. 2\n[22] J. Lei Ba, K. Swersky, S. Fidler, et al. Predicting deep zero-\nshot convolutional neural networks using textual descrip-\ntions. In Proc. IEEE Int. Conf. on Computer Vision, pages\n4247–4255, 2015. 1, 2\n[23] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. Distributed representations of words and phrases\nand their compositionality. In Proc. Adv. Neural Inf. Pro-\ncess. Syst., pages 3111–3119, 2013. 2, 7\n[24] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities\nin continuous space word representations. In HLT-NAACL,\npages 746–751, 2013. 2\n[25] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,\nA. Frome, G. S. Corrado, and J. Dean. Zero-shot learning\nby convex combination of semantic embeddings. In Proc.\nInt. Conf. Learn. Represent., 2014. 2, 3, 7\n[26] J. Pennington, R. Socher, and C. D. Manning. Glove: Global\nvectors for word representation.\nProc. of the Empiricial\nMethods in Natural Language Processing, 12:1532–1543,\n2014. 2, 6\n[27] M. Rohrbach, M. Stark, and B. Schiele. Evaluating knowl-\nedge transfer and zero-shot learning in a large-scale setting.\nIn Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages\n1641–1648, 2011. 1, 2, 7\n[28] B. T. C. G. D. Roller. Max-margin markov networks. Proc.\nAdv. Neural Inf. Process. Syst., 16:25, 2004. 4\n[29] B. Romera-Paredes and P. Torr. An embarrassingly simple\napproach to zero-shot learning. In Proc. Int. Conf. Mach.\nLearn., pages 2152–2161, 2015. 2\n[30] K. Soomro and A. R. Zamir. Action recognition in realistic\nsports videos. In Computer Vision in Sports, pages 181–208.\nSpringer, 2014. 6\n[31] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset\nof 101 human action classes from videos in the wild. Tech-\nnical Report CRCV-TR-12-01, University of Central Florida,\nNovember 2012. 7\n[32] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,\nD. Poland, D. Borth, and L.-J. Li. The new data and new\nchallenges in multimedia research. Communications of the\nACM, 59(2):64–73, 2016. 7\n[33] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun.\nLarge margin methods for structured and interdependent out-\nput variables.\nJ. Mach. Learn. Res., 6(Sep):1453–1484,\n2005. 4\n[34] J. Weston, S. Bengio, and N. Usunier. Large scale image\nannotation: learning to rank with joint word-image embed-\ndings. Machine learning, 81(1):21–35, 2010. 3\n9\n[35] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and\nB. Schiele. Latent embeddings for zero-shot classiﬁcation.\nIn Proc. IEEE Conf. Comput. Vis. Pattern Recog., pages 69–\n77, 2016. 1, 2, 3, 7\n[36] Z. Zhang and V. Saligrama. Zero-shot learning via semantic\nsimilarity embedding. In Proc. IEEE Int. Conf. on Computer\nVision, pages 4166–4174, 2015. 2, 7\n[37] Z. Zhang and V. Saligrama. Learning joint feature adaptation\nfor zero-shot recognition. arXiv preprint arXiv:1611.07593,\n2016. 7\n[38] Z. Zhang and V. Saligrama. Zero-shot learning via joint la-\ntent similarity embedding. In Proc. IEEE Conf. Comput. Vis.\nPattern Recog., pages 6034–6042, 2016. 2, 7\n[39] Z. Zhang and V. Saligrama. Zero-shot recognition via struc-\ntured prediction. In Proc. European Conf. on Computer Vi-\nsion, pages 533–548. Springer, 2016. 2\n10\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-05-04",
  "updated": "2017-08-05"
}