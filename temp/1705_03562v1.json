{
  "id": "http://arxiv.org/abs/1705.03562v1",
  "title": "Deep Episodic Value Iteration for Model-based Meta-Reinforcement Learning",
  "authors": [
    "Steven Stenberg Hansen"
  ],
  "abstract": "We present a new deep meta reinforcement learner, which we call Deep Episodic\nValue Iteration (DEVI). DEVI uses a deep neural network to learn a similarity\nmetric for a non-parametric model-based reinforcement learning algorithm. Our\nmodel is trained end-to-end via back-propagation. Despite being trained using\nthe model-free Q-learning objective, we show that DEVI's model-based internal\nstructure provides `one-shot' transfer to changes in reward and transition\nstructure, even for tasks with very high-dimensional state spaces.",
  "text": "Deep Episodic Value Iteration\nfor Model-based Meta-Reinforcement Learning\nSteven S. Hansen\nDepartment of Psychology\nStanford University\nStanford, CA 94305\nsshansen@stanford.edu\nAbstract\nWe present a new deep meta reinforcement learner, which we call Deep Episodic\nValue Iteration (DEVI). DEVI uses a deep neural network to learn a similarity\nmetric for a non-parametric model-based reinforcement learning algorithm. Our\nmodel is trained end-to-end via back-propagation. Despite being trained using\nthe model-free Q-learning objective, we show that DEVI’s model-based internal\nstructure provides ‘one-shot’ transfer to changes in reward and transition structure,\neven for tasks with very high-dimensional state spaces.\n1\nIntroduction\nThe deep reinforcement learning paradigm popularized by [1] has been uniquely capable of obtaining\ngood asymptotic performance in complex high-dimensional domains. By using highly non-linear\nfunction-approximators (e.g. deep neural network), traditional model-free reinforcement learning\nalgorithms can solve highly non-linear control problems. As this work has matured, interest has\nshifted from asymptotic performance to sample complexity and task transfer. It is well known that\nmodel-free methods provide poor transfer, as the knowledge of the transition and reward functions\nare \"baked in\" to the representation of the value function.\nMeta-learning has begun to appear as one possible solution the issues of sample complexity that deep\nlearning models suffer from. By treating ‘solve this task from few examples’ as an instance of a meta-\ntask, slow gradient based learning is only applied across tasks, with within-task learning delegated to\nthe forward pass of the network. While traditional networks architectures could do this in principle,\nmuch of the current success comes from networks designed to parametrize a speciﬁc nonparametric\nalgorithm [2]. However, most previous attempts have focused on supervised learning, especially\nfew-shot classiﬁcation. Recently, there have been a few attempts at meta reinforcement learning, but\nnone utilize the non-parametric internal structures that have proven vital in the classiﬁcation context\n[15,16].\n2\nBackground\nWhile reinforcement learning as a ﬁeld has been around for decades, with a rich variety of algorithmic\napproaches, most of the work utilizing non-linear function approximation (e.g. deep neural networks)\nhad focused on the model-free algorithm Q-learning. In this section we review this algorithm\nalongside value iteration and kernel-based reinforcement learning, the model-based reinforcement\nlearning algorithms that forms the foundation of our approach.\narXiv:1705.03562v1  [stat.ML]  9 May 2017\n2.1\nQ-learning\nConsider the discrete action Markov decision process (MDP) M : {S, A, R, T, γ}, where S is\nthe set of all possible states (possibly continuous), A is the ﬁnite set of discrete actions, R is the\nreward function mapping state-action-next-state tuples to a scalar immediate reward, and T is the\nstate transition function, mapping state-action-next-state tuples to the probability of that transitions\noccurrence: T(s, a, s′) = p(s′|s, a), and γ is discount factor weighing immediate versus future\nrewards. We wish to ﬁnd the policy, π(s, a) = p(a|s), that maximizes the expected cumulative\ndiscounted reward:\nV (s) =\n∞\nX\nt=0\nX\na∈A\nγtπ(st, a)T(st, a, st+1)R(st, a)\n(1)\nThe Bellman equation gives the relationship between the value of a state and the value of its successor:\nV (s) = maxa\nX\ns′\nT(s, a, s′)(R(s, a) + γV (s′))\n(2)\nNotice that this can be rewritten for state-action values and expressed as an expectation:\nQ(s, a) = E[R(s, a), +γmaxa′Q(s′, a′)]\n(3)\nBecause we have eliminated the explicit usage of T, this second equation can be evaluated without a\ndynamics model, simply by taking actions and observing their consequences (i.e sampling from the\nunderlying state and reward transition functions). The Bellman equation holds only for the optimal\nvalue function. We can thus view the difference between LHS and RHS as the error in the relationship\nbetween the value estimates at time t and t + 1. Since we can sample directly from the true reward\nfunction (i.e. by taking actions in the real environment and observing the outcomes), it makes sense\nto treat the RHS as the target value for the LHS. After making this value symmetric, we can use it as\na loss function for training a parametric function approximator such as a deep neural network:\nloss = E[(Q(s, a) −(r(s, a) + γmaxa′Q(s′, a′)))2]\n(4)\nOptimizing this temporal difference loss using stochastic gradient descent is non-trivial, as iid\nsampling from a MDP is non-trivial, and there is a dependency between the current estimated and\ntarget values. Both problems are addressed in [1], using random samples from a ‘replay buffer’ of\npast transition samples {s, a, r, s′} to address iid concerns, and a slow changing ‘target network’ to\nbreak the dependence between estimated and target values.\n2.2\nMatching Networks\nDespite their focus on classiﬁcation, the matching networks of Vinyals et al [2] are the closest prior\nwork to our approach. As shown in ﬁgure 1, matching networks use kernel-based averaging to\nestimate label probabilities, and learn the latent-space over which the kernel is applied end-to-end.\nRelating this approach to the reinforcement learning problem, one could imagine using a kernel to\nrelate a novel state to states with known values (i.e. kernel weighted regression). Blundell et al\ndo this with a nearest neighbor kernel and empirical returns as the value estimates of previously\nencountered states [4]. While the embedding space used by the kernel was based on either random\nprojections or learnt in an unsupervised fashion, the kernel could theoretically be learned in an\nend-to-end manner based on a loss between predicted and actual returns. Episodic Neural Control\n[14] recently made something akin to this extension, and will be explored in greater detail in the\ndiscussion section. However, both works were constrained to deterministic MDPs with frequently\nrevisited states, limiting its overall applicability.\n2.3\nKernel-based Reinforcement Learning\nKernel-based reinforcement learning (KBRL) is a non-parametric reinforcement learning algorithm\nwith several advantageous properties [5]. It is a model-based reinforcement learning algorithm, but\n2\nFigure 1: The matching network architecture (reproduced with permission from [2]). A neural-\nnetwork (fθ) is used to embed an query image into a latent space, alongside several other images with\nknown labels (gθ). A similarity vector is then produced by passing the query latent vector through a\ncosine kernel with each known latent vector. This vector is then normalized and multiplied by the\nvector of known labels to produce an estimate of the query image’s label. All of these operations\nare differentiable, so the supervised loss can be back-propagated to the parameters of the embedding\nnetwork.\nrelies on stored empirical transitions rather than learning a parametric approximation. This reliance\non empirical state and reward transitions results in low sample complexity and excellent convergence\nresults. 1.\nThe key insight of this algorithm is that a set of stored transitions DA = {S, R, S′} can be paired\nwith a similarity kernel κ(x0, x1) to represent each resultant state (s′ ∈S′) as a distribution over\nthe origin states (S). This new MDP can approximate the real one arbitrarily closely (as the stored\ntransitions go to inﬁnity and the kernel bandwidth goes to 0), but has the advantage of being closed\nunder states whose transitions have been sampled from the environment.\nTo see why this is the case, note that, empirically, we know what the state transitions are for the\nstored origin states:\n∀s ∈S :\n\u001aTa(s, s′) = 1/ P{s, ·, ·} ∈Da\n{s, ·, s′} ∈Da\nTa(s, s′) = 0\notherwise\n(5)\nLikewise we know the reward transitions for these states:\n∀s ∈S : rs,a = E[Ra(s)]\n(6)\nSince we are dealing with a continuous space, the state resulting from one of these transitions is\nunlikely to have its own stored transition (i.e. ∃s′ ∈S′ : s′ /∈S). Using the similarity kernel, we can\nreplace this unknown resultant state with a distribution over the origin states. This makes the state\ntransitions from S →S instead of S →S′, meaning that all transitions only involve states for which\nwe have experienced empirically.\nSince there are a ﬁnite number of states (equal to the number of stored transitions), and these states\nhave known transitions, we can perform value iteration to obtain value estimates for all resultant\n1Convergence to a global optima assuming that underlying MDP dynamics are Lipschitz continuous, and the\nkernel is appropriately shrunk as a function of data. While this result doesn’t hold when a non-linear function\napproximator is used to adapt the kernel, it seems to converge empirically with proper hyper-parameter tuning.\n3\nstates S′ (the values of the initial states S aren’t needed, as the bellman equation only evaluates states\nafter a transition). We can obtain an approximate version of the bellman equation by using the kernel\nto compare all successor states to all initial states in the transition set:\n∀x ∈S′ : V (x) ←maxa\nX\ns∈Sa\nκ(x, s)[Ra(s) + γV (s′\na)]\n(7)\n3\nModel\nThe KBRL algorithm is great in theory, but suffers from the so-called ‘curse of dimensionality’ –\nas the dimensionality of the state space grows, the number of exemplars needed to cover the space\ngrows exponentially. By taking the KBRL algorithm and making the similarity kernel act on the\noutput of a deep neural network, we can force the data to live in a lower dimensional space. While\nrandom projections could accomplish the same thing, an embedding function optimized on overall\nperformance should be able to do a much better job, by focusing on performance relevant aspects of\nthe observed states.\nAlgorithm 1: Deep Episodic Value Iteration\nκ(ˆx, xi) = ecosine(ˆx,xi)/ Pk\nj=1 ecosine(ˆx,xj)\nFunction Q (s, a, E, limit; θ)\n(S, R, S′) := E\nZ := Encoder(S; θ)\nZ′ := Encoder(S′; θ)\nz := Encoder(s; θ)\nfor a := 1, A do\nΘa := κ(Z′, Za)\nend\nV := 0\nfor i := 1, limit do\nV := maxaΘa[Ra + γVa]\nend\nreturn κ(z, Za)[R + γV ]\nInitialize replay memory D to capacity N\nInitialize action-value function Q with random weights\nfor episode := 1, M do\nfor t := 1, T do\nfor a := 1, A do\nSample random set of transitions Ea := (sak, rak, sak+1) from D(·, a, ·, ·)\nend\nWith probability ϵ select a random action at\notherwise select at := maxaQ(st, a, E, T; θ)\nExecute action at and observe reward rt and observation st+1\nStore transition in (st, at, rt, st+1) in D\nSample random minibatch of transitions (sj, aj, rj, sj+1) from D\nloss := 0\nfor i := 1, T do\nif terminal or i = 1 then\nyj := rj\nelse\nyj := rj + γmaxa′Q(sj+1, a′, E, i −1; θ)\nend\nlossj := lossj + (yj −Q(sj, aj, E, i; θ))2\nend\nPerform a gradient descent step on loss\nend\nend\n4\nThe resulting model, Deep Episodic Value Iteration (DEVI) is KBRL applied to a state representation\nlearned through Q-learning. This representation is the output of a deep neural network, with weights\ntied such that query states are encoded using the same function as states in the episodic store. To\nensure that, during learning, this parametric encoder doesn’t rely upon any particular states being\nthe episodic store, a random subsample of the episodic store is used for any given training step. In\norder to keep GPU memory usage relatively constant during training, the size of the subsampled set\nof experiences is held constant, though there is no fundamental objection to using a growing set as\nper KBRL.\nThe size of the computation graph grows linearly with the planning horizon, so DEVI uses a ﬁxed\nnumber of value iteration steps rather than iterating to convergence (though again, this is a pragmatic\nrather than fundamental deviation from KBRL). For the simple tasks considered here, it was trivial to\ndetermine the maximum number of value iteration sweeps required to compute the value function,\nbut adaptive computation techniques, such as those designed of recurrent neural networks [7], should\nbe able to automate this process on more demanding environments.\nDEVI is trained using Q-learning (modiﬁed to account for the ﬁxed planning horizon), and all\nof the improvements made since the inﬂuential DQN paper (e.g. Double DQN [8]) [1] could\nalso be incorporated. However, one unique problem that DEVI faces is the long gradient path\ncorresponding to the value iteration steps. The structure of the computational graph means that the\npaths corresponding to shorter term predictions (e.g. the ﬁrst value iteration step simply predicts the\nmaximum immediate reward) are the last to receive gradient information. It is possible to eliminate\nthis problem by using the values after each step of value iteration (rather than just the after the ﬁnal\nstep) to calculate the predicted value of the current state-action pair. Thus, if K steps of value iteration\nare performed, the model has K different predictions for the value of Q(s,a), corresponding to the K\ndifferent planning horizons. Each of these is amenable to Q-learning (using target values derived\nfrom the immediate reward and the values from the previous planning step), and these K losses are\nsimple averaged to calculate the net loss.\n4\nInitial Experiments\nThe primary objective of this work is to demonstrate that ‘one-shot learning’ is possible on reinforce-\nment learning problems using deep neural networks. 2 As there has been no prior work demonstrating\nthis phenomenon, we’ve designed a toy task domain as a proof of concept – it is as minimalistic as\npossible while still allowing for a variety of tasks requiring non-linear state representations.\n4.1\nOmniglot Graph World\nThe Omniglot dataset consists of 28 by 28 pixel grey-scale images depicting characters from real and\nﬁctional languages. Often referred to as the ‘transpose of MNIST’, there are 1600 different classes of\ncharacter, but only 20 examples per class. Due to this limited sample size, it has quickly become the\nstandard bearer for ‘one-shot’ learning [19]. However, the format of the dataset is only conducive to\n‘one-shot’ learning for classiﬁcation. Here we extend the dataset for use as a proof-of-concept for\n‘one-shot’ reinforcement learning.\nThe Omniglot Graph World is a task domain consisting of all MDPs with a small ﬁnite number of\nstates, each randomly assigned to a character class from the Omniglot dataset. Rather than observe\nthe states directly, the agent only has access to a sample from the corresponding Omniglot class.\nWhile trivially easy to solve in isolation, the performance criterion of interest is the number of\nsamples required to learn a new task in this domain, given prior exposure to some other tasks. This\ndomain is interesting in that all of the tasks share the same similarity structure (i.e. observations\ncoming from the same class are bisimilar), while differing greatly in their state and reward transition\nstructure.\nThe number of tasks contained within the Omniglot Graph World is huge and vary widely in difﬁculty\ndue to the possibility of degenerate transition structures (e.g. all states equally rewarding). So for the\n2Our deﬁnition of one-shot learning is the ability to solve a novel task without parameter updates (i.e. changes\nto the weights of the deep network). The episodic store must still be populated with transitions from the new\ntask, but this requires orders of magnitude fewer samples than any parametric adaptation.\n5\nFigure 2: Three sub-domains within the Omniglot graph world. The nodes represent the states and the\nedges representing the connectivity resulting from the possible actions. The agent observes images\nof characters from the Omniglot dataset rather than the states themselves, with this mapping being\nrandomly drawn for each new task. Additionally, the relative reward locations and action semantics\nare permuted. A) The Ring task distribution consists of a few states (10 in the experiments) and a\nsingle positive terminal reward. B) The Hard Ring task distribution consists of more states (15 in\nthe experiments), large terminal negative reward and a small non-terminal reward. C) The Tree task\ndistribution has states that form a full binary tree (depth 5 in the experiments), with all leaf nodes\nexcept one giving negative terminal rewards.\npurposes of thorough evaluation, we’ve constrained the space of MDPs to variations on 3 prototypes\n(as shown in Figure 2), each having 2 actions and unique transition structures.\n4.2\nImplementation Details\nAnalogously to work on ‘one-shot’ classiﬁcation, we trained DEVI in an interleaved manner across\ntasks randomly sampled from our task domain. Rather than interleave within mini-batches, tasks\nwere switched out after every step, and 1000 ‘burn in’ steps were taken to ﬁll in a small replay buffer\nfrom which to sample a minibatch of 100 tuples for Q-learning. 3\nDQN was trained for 1000 minibatches on both the initial and transfer tasks. A replay buffer was\nused with a capacity of 100000, and was ﬁlled prior to the start of learning to avoid tainting our\nsample complexity analysis.\nTo focus the evaluation on between task transfer rather than within task exploration, we decided\nto train all models with a uniform random behavior policy. This ensures that the data provided to\nthe different algorithms is equivalent, and performance differences aren’t due to more structured\nexploration. 4 We also modiﬁed DEVI’s episodic store such that it always contains 5 samples from\neach state. Randomly sampling from the replay buffer (as described in Algorithm 1) works similarly,\nbut this method allows for useful comparison with the work on ‘one-shot’ learning for classiﬁcation.\nThe same basic deep neural network architecture is used for both algorithms, taken directly from the\nMatching Networks paper [2]. It consists of four 3 × 3 convolutional layers, each with 64 ﬁlters,\ninterleaved with 2 × 2 max-pooling. For DQN, this is followed by a densely connected layer (100\nunits) with ReLU activations. This is used to encode the latent space in DEVI, and to represent the\nvalue function in DQN. However, since the output dimensionality of DQN is tied to the number of\nactions, a ﬁnal linear layer is added.\nThe ADAM optimizer was used for both algorithms with a batch-size of 100. Default hyper-\nparameters were used, though a preliminary grid search showed that these didn’t signiﬁcantly impact\nperformance across a wide range of values. Likewise, extensions to DQN were considered (e.g.\nDouble DQN), but none yielded qualitatively different results.\n3Initial experiments showed that longer gaps between interleaving didn’t alter performance, so episode level\ninterleaving should be used to avoid intractability on larger tasks.\n4while intelligent exploration isn’t a free byproduct of our approach, it is easily obtained by exploiting the\npseudo-tabular nature of the learned model. An extension of R-MAX, whereby neighbors within a ﬁxed distance\nsubstitute for visitation counts, has been shown by Li et al [9] to be quite effective.\n6\nFigure 3: results on the Omniglot Graph task domain. 5 instances of DQN and DEVI were initially\ntrained using random seeds. The average performance over time for this initial phase is shown in\nblack for DQN. DEVI was trained over a distribution of tasks, so its performance curve is not shown.\nEach of the 5 models were then used as the initial weights for transfer learning. 5 transfer attempts\nwere performed for each model and averaged together, with the average across models in bold. In the\ncase of DEVI, these transferred weights were frozen to emphasize the non-parametric nature of their\ngeneralization.\n4.3\nResults\nAs Figure 3 shows, DEVI was able to instantly generalize to new tasks, even though none the\ncharacters representing the underlying states had ever been seen before (they were drawn from a held\nout set of characters). In addition, both test task distributions had qualitatively different state and\nreward transition structures from that of the training task distribution. As expected, DQN exhibited\nvery little transfer, needing samples on the same order of magnitude as networks initialized with\nrandom weights.\nThe differences between these two models give evidence to the claim that while both are trained\nusing Q-learning, a model-free reinforcement learning algorithm, DEVI’s non-parametric internal\nstructure allows for learning representations that work across a wide array of tasks. More precisely,\nthis structure paired with interleaved learning forces DEVI to learn an appropriate latent space for\nkernel-based planning.\n5\nDiscussion\nThese initial experiments demonstrate the possibility of meta model-based reinforcement learning,\nbut it remains unclear how well this approach will scale to realistic domains. There have recently\nbeen several papers proposing model-free meta reinforcment learning methods [14,15,16], and it is\nan interesting open question how DEVI’s model-based generalization compares to fast model-free\nadaptation.\n7\n5.1\nRelated Work\nThe most similar among this recent work on model-free meta reinforcement learning is Neural\nEpisodic Control [14]. Interestingly, it isn’t presented as a meta learning algorithm, but rather as just\nsample-efﬁcient learner. Despite this, NEC can be seen as the offspring of Matching Networks, as it\nuses the same kernel-based averaging, but to weight values instead of labels. It shares with DEVI the\nidea of training the encoding network using Q-learning, but differs in only storing the latent space in\nthe episodic store, meaning that the encoder is only updated for the queried state, not the states it\nis being compared to. Instead, the latent vectors are themselves treated like parameters once stored\nand modiﬁed via back-propagation. Additionally, similarity was approximated by only comparing\na query against its k approximate nearest neighbors. These two tricks allow NEC to scale to very\ncomplex tasks such as the Atari 2600 environment.\nDespite the similar titles, ‘Value-Iteration Networks’ [11] and this work are related, but quite distinct.\nIn that work, the authors assumed a underlying 2D state-space with local transitions. This allowed\nthem to perform efﬁcient value iteration by noticing that for this speciﬁc topology, the value iteration\nalgorithm can be performed by 2D convolution. Additionally, this approach made the reward and\nvalue function internal (end-to-end training was performed from the resulting policy), foregoing any\nattempt to learn a veridical reward function. The work presented here shares neither of these qualities,\nthough it is an interesting open-question as to which approach is more likely to scale to complex\ndomains.\n5.2\nFuture Directions\nOne pragmatic concern is that this architecture scales poorly as the size of the episodic store grows.\nIn the cognitive science literature, it is well known that ﬁxed episodic stores can be augmented by\ntraditional neural networks to incorporate knowledge from the distant past [13], though this has yet to\nbe empirically validated on a large scale model. A more immediate (and complementary) solution\nwould be to sparsify the similarity matrices by zeroing out all but the nearest k neighbors for each\nrow. While this could theoretically make gradient propagation more problematic, recent related work\non Neural Episodic Control and sparsiﬁed Neural Turing Machines suggests that this approach can\nbe quite effective empirically [14,10].\nDue to DEVI being both similarity-based and model-based, it is worth emphasizing that even without\ninterleaved learning of multiple tasks, signiﬁcant generalization should be possible. So long as the\noptimal Q-values of the initial task don’t alias state-action pairs that are important to a transfer task,\nthen ‘one shot’ transfer should be possible to the extent the two tasks share similarity structure.\nFor example, changing reward structures in a ﬁxed environment would guarantee that only value\naliasing would prevent transfer. Work on the so-called Successor representation ﬂeshes out a similar\nargument, though its architecture imposes the harsher limitation of immediate reward aliasing (which\nis signiﬁcant for the sparely rewarded environments common to the deep reinforcement learning\nliterature), and it has additional constraints owing to its on-policy nature [17,18].\nRecently, researchers have noted that humans’ ability to react to changes in task structure far surpasses\nthat of deep reinforcement learning approaches such as DQN [12]. Speciﬁcally, these authors note\nthat even extreme alterations to reward structure (e.g. lose an Atari game instead of winning), are\nreadily solvable by humans but not deep learning systems. As previously mentioned, our model is\nunique in being able to rapidly adjust to changes in reward structure. While there are several technical\nhurdles to overcome, we believe that DEVI will be able to tackle this challenge within the immediate\nfuture.\nReferences\n[1] Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540\n(2015): 529-533.\n[2] Vinyals, O., Blundell, C., Lillicrap, T., & Wierstra, D. (2016). Matching networks for one shot learning. In\nAdvances in Neural Information Processing Systems (pp. 3630-3638).\n[3] Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., & Lillicrap, T. (2016). Meta-learning with memory-\naugmented neural networks. In Proceedings of The 33rd International Conference on Machine Learning (pp.\n1842-1850).\n8\n[4] Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman, A., Leibo, J. Z., ... & Hassabis, D. (2016). Model-free\nepisodic control. arXiv preprint arXiv:1606.04460.\n[5] Ormoneit, D., & Sen, ´S. (2002). Kernel-based reinforcement learning. Machine learning, 49(2-3), 161-178.\nChicago\n[6] Dayan, P. (1993). Improving generalization for temporal difference learning: The successor representation.\nNeural Computation, 5(4), 613-624. Chicago\n[7] Graves, A. (2016).\nAdaptive computation time for recurrent neural networks.\narXiv preprint\narXiv:1603.08983.\n[8] Van Hasselt, H., Guez, A., & Silver, D. (2016, March). Deep Reinforcement Learning with Double\nQ-Learning. In AAAI (pp. 2094-2100).\n[9] Li, L., Littman, M. L., & Mansley, C. R. (2009, May). Online exploration in least-squares policy iteration. In\nProceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems-Volume 2 (pp.\n733-739). International Foundation for Autonomous Agents and Multiagent Systems.\n[10] Rae, J., Hunt, J. J., Danihelka, I., Harley, T., Senior, A. W., Wayne, G., ... & Lillicrap, T. (2016). Scaling\nMemory-Augmented Neural Networks with Sparse Reads and Writes. In Advances In Neural Information\nProcessing Systems (pp. 3621-3629).\n[11] Tamar, A., Wu, Y., Thomas, G., Levine, S., & Abbeel, P. (2016). Value iteration networks. In Advances in\nNeural Information Processing Systems (pp. 2154-2162).\n[12] Lake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2016). Building machines that learn and\nthink like people. arXiv preprint arXiv:1604.00289.\n[13] Kumaran, D., Hassabis, D., & McClelland, J. L. (2016). What learning systems do intelligent agents need?\ncomplementary learning systems theory updated. Trends in Cognitive Sciences, 20(7), 512-534.\n[14] Pritzel, A., Uria, B., Srinivasan, S., Puigdomènech, A., Vinyals, O., Hassabis, D., ... & Blundell, C. (2017).\nNeural Episodic Control. arXiv preprint arXiv:1703.01988.\n[15] Wang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z., Munos, R., ... & Botvinick, M. (2016).\nLearning to reinforcement learn. arXiv preprint arXiv:1611.05763.\n[16] Finn, C., Abbeel, P., & Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep\nNetworks. arXiv preprint arXiv:1703.03400.\n[17] Dayan, P. (1993). Improving generalization for temporal difference learning: The successor representa-\ntion.Neural Computation, 5(4), 613-624.\n[18] Kulkarni, T. D., Saeedi, A., Gautam, S., & Gershman, S. J. (2016). Deep successor reinforcement learning.\narXiv preprint arXiv:1606.02396.\n[19] Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through\nprobabilistic program induction. Science, 350(6266), 1332-1338.\n9\n",
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2017-05-09",
  "updated": "2017-05-09"
}