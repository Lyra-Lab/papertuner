{
  "id": "http://arxiv.org/abs/1705.04804v2",
  "title": "Automatically Redundant Features Removal for Unsupervised Feature Selection via Sparse Feature Graph",
  "authors": [
    "Shuchu Han",
    "Hao Huang",
    "Hong Qin"
  ],
  "abstract": "The redundant features existing in high dimensional datasets always affect\nthe performance of learning and mining algorithms. How to detect and remove\nthem is an important research topic in machine learning and data mining\nresearch. In this paper, we propose a graph based approach to find and remove\nthose redundant features automatically for high dimensional data. Based on the\nsparse learning based unsupervised feature selection framework, Sparse Feature\nGraph (SFG) is introduced not only to model the redundancy between two\nfeatures, but also to disclose the group redundancy between two groups of\nfeatures. With SFG, we can divide the whole features into different groups, and\nimprove the intrinsic structure of data by removing detected redundant\nfeatures. With accurate data structure, quality indicator vectors can be\nobtained to improve the learning performance of existing unsupervised feature\nselection algorithms such as multi-cluster feature selection (MCFS). Our\nexperimental results on benchmark datasets show that the proposed SFG and\nfeature redundancy remove algorithm can improve the performance of unsupervised\nfeature selection algorithms consistently.",
  "text": "arXiv:1705.04804v2  [cs.LG]  30 Jun 2017\nAbstract\nThe redundant features existing in high dimensional datasets always\naﬀect the performance of learning and mining algorithms.\nHow to de-\ntect and remove them is an important research topic in machine learning\nand data mining research. In this paper, we propose a graph based ap-\nproach to ﬁnd and remove those redundant features automatically for high\ndimensional data. Based on sparse learning based unsupervised feature\nselection framework, Sparse Feature Graph (SFG) is introduced not only\nto model the redundancy between two features, but also to disclose the\ngroup redundancy between two groups of features. With SFG, we can\ndivide the whole features into diﬀerent groups, and improve the intrinsic\nstructure of data by removing detected redundant features. With accurate\ndata structure, quality indicator vectors can be obtained to improve the\nlearning performance of existing unsupervised feature selection algorithms\nsuch as multi-cluster feature selection (MCFS). Our experimental results\non benchmark datasets show that the proposed SFG and feature redun-\ndancy remove algorithm can improve the performance of unsupervised\nfeature selection algorithms consistently.\n1\nAutomatically Redundant Features Removal for\nUnsupervised Feature Selection via Sparse\nFeature Graph\nShuchu Han\nshuchu.han@gmail.com\nDepartment of Computer Science,\nStony Brook University,\nStony Brook, NY 11794, United States\nHao Huang\nhaohuanghw@gmail.com\nMachine Learning Laboratory,\nGeneral Electric Global Research,\nSan Ramon, CA 94853, United States\nHong Qin\nqin@cs.stonybrook.edu\nDepartment of Computer Science,\nStony Brook University,\nStony Brook, NY 11794, United States\nJuly 4, 2017\nFor unsupervised feature selection algorithms, the structure of data is used\nto generate indication vectors for selecting informative features.\nThe struc-\nture of data could be local manifold structure [8] [9], global structure [14] [28],\ndiscriminative information [24] [12] and etc. To model the structure of data,\nmethods like Gaussian similarity graph, or k-nearest neighbor similarity graph\nare very popular in machine learning research. All these similarity graphs are\nbuilt based on the pairwise distance like Euclidean distance (L2 norm) or Man-\nhattan distance (L1 norm) deﬁned between two data samples (vectors). As we\ncan see, the pairwise distance is crucial to the quality of indication vectors, and\nthe success of unsupervised feature selection depends on the accuracy of these\nindication vectors.\nWhen the dimensional size of data becomes high, or say, for high dimen-\nsional datasets, we will meet the curse of high dimensionality issue [2]. That\nmeans the diﬀerentiating ability of pairwise distance will degraded rapidly when\nthe dimension of data goes higher, and the nearest neighbor indexing will give\n2\ninaccurate results [23] [1]. As a result, the description of data structure by using\nsimilarity graphs will be not precise and even wrong. This create an embarrass-\ning chicken-and-egg problem [5] for unsupervised feature selection algorithms:\n“the success of feature selection depends on the quality of indication vectors\nwhich are related to the structure of data. But the purpose of feature selection\nis to giving more accurate data structure.”\nMost existing unsupervised feature selection algorithms use all original fea-\ntures [5] to build the similarity graph. As a result, the obtained data structure\ninformation will not as accurate as the intrinsic one it should be. To remedy\nthis problem, dimensionality reduction techniques are required. For example,\nPrincipal Component Analysis (PCA) and Random Projection (RP) are popu-\nlar methods in machine learning research. However, most of them will project\nthe data matrix into another (lower dimensional) space with the constraint to\napproximate the original pairwise similarities. As a result, we lose the physical\nmeaning or original features and the meaning of projected features are unknown.\nIn this study, we proposed a graph-based approach to reduce the data dimen-\nsion by removing redundant features. Without lose of generality, we categorize\nfeatures into three groups [4]: relevant feature,irrelevant feature and redundant\nfeature. A feature fi is relevant or irrelevant based on it’s correlation with in-\ndication vectors (or target vectors named in other articles) Y = {yi, i ∈[1, k]}.\nFor supervised feature selection algorithms [18] [20] [17], these indication vectors\nusually relate to class labels. For unsupervised scenario [6] [3], as we mentioned\nearly, they follow the structure of data. Redundant features are features that\nhighly correlated to other features, and have no contribution or trivial contri-\nbution to the target learning task. The formal deﬁnition of redundant feature\nis by [26] based on the Markov blanket given by [10].\nBased on the philosophy of sparse learning based MCFS algorithm, a feature\ncould be redundant to another single feature, or to a subset of features. In\nthis work, we propose a graph based approach to identify these two kind of\nredundancy at the same time. The ﬁrst step is to build a Sparse Feature Graph\n(SFG) at feature side based on sparse representation concept from subspace\nclustering theory [7]. Secondly, we review the quality of sparse representation\nof each single feature vector and ﬁltered out those failed ones. In the last, we\ndeﬁned Local Compressible Subgraphs (LCS) to represent those local feature\ngroups that are very redundant. Moreover, a greedy local search algorithm is\ndesigned to discover all those LCSs. Once we have all LCSs, we pick the feature\nwhich has the highest node in-degree as the representative feature and treat all\nother as redundant features. With this approach, we obtain a new data matrix\nwith reduced size and alleviate the curse of dimensional issues.\nTo be speciﬁc, the contribution of our study can be highlighted as:\n• We propose sparse feature graph to model the feature redundancy existing\nin high dimensional datasets. The sparse feature graph inherits the philos-\nophy of sparse learning based unsupervised feature selection framework.\nThe sparse feature graph not only records the redundancy between two\nfeatures but also show the redundancy between one feature and a subset\n3\nof features.\n• We propose local compressible subgraph to represent redundant feature\ngroups. And also design a local greedy search algorithm to ﬁnd all those\nsubgraphs.\n• We reduce the dimensionality of input data and alleviate the curse of di-\nmensional issue through redundant features removal. With a more accu-\nrate data structure, the chicken-and-egg problem for unsupervised feature\nselection algorithms are remedied in certain level. One elegant part of our\nproposed approach is to reduce the data dimension without any pairwise\ndistance calculation.\n• Abundant experiments and analysis over twelve high dimensional datasets\nfrom three diﬀerent domains are also presented in this study. The experi-\nment results show that our method can obtain better data structure with\nreduced size of dimensionality, and proof the eﬀectiveness of our proposed\napproach.\nThe rest of paper is organized as follows.\nThe ﬁrst section describe the\nmath notation used in our work.\nThe Section 2 introduces the background\n, motivation and preliminaries of our problem.\nIn Section 3, we deﬁne the\nproblem we are going to solve. In Section 4, we present our proposed sparse\nfeature graph algorithm and discuss the sparse representation error problem.\nWe also introduce the local compressible subgraph and related algorithm. The\nexperiment results are reported in Section 5, and a brieﬂy reviewing of related\nworks is given in Section 6. Finally, we conclude our study in last Section 7.\n1\nMath Notation\nThroughout this paper, matrices are written as boldface capital letters and vec-\ntors are represented as boldface lowercase letters. Let the data matrix be repre-\nsented as X ∈Rn×d, while each row is a sample (or instance), and each column\nmeans a feature. If we view the data matrix X = [x1, x2, · · ·, xn]T , xi ∈Rd×1\nfrom feature side, it can be seen as F = XT = [f1, f2, · · ·, fd], fi ∈Rn×1(1 ≤\ni ≤d).\n2\nBackground and Preliminaries\n2.1\nUnsupervised Feature Selection\nIn unsupervised feature selection framework, we don’t have label information\nto determine the feature relevance.\nInstead, the data similarity or manifold\nstructure constructed from the whole feature space are used as criteria to select\nfeatures.\nAmong all those algorithms of unsupervised feature selection, the\nmost famous one is MCFS. The MCFS algorithm is a sparse learning based\n4\nFigure 1: The framework of sparse learning based unsupervised feature selection.\nunsupervised feature selection method which can be illustrated as ﬁgure 1. The\ncore idea of MCFS is to use the eigenvectors of graph Lapalcian over similarity\ngraph as indication vectors. And then ﬁnd set of features that can approximate\nthese eigenvectors through sparse linear regression. Let us assume the input\ndata has number K clusters that is known beforehand (or an estimated K\nvalue by the expert’s domain knowledge). The top K non-trivial eigenvectors,\nY = [y1, · · · , yk], form the spectral embedding Y of the data. Each row of Y\nis the new coordinate in the embedding space. To select the relevant features,\nMCFS solves K sparse linear regression problems between F and Y as:\nmin\nαi ∥yi −F αi∥2 + β∥αi∥1,\n(1)\nwhere αi is a n-dimensional vector and it contains the combination coeﬃcients\nfor diﬀerent features fi in approximating yi. Once all coeﬃcients αi are col-\nlected, features will be ranked by the absolute value of these coeﬃcients and top\nfeatures are selected. This can be show by a weighted directed bipartite graph\nas following:\nFigure 2: Sparse learning bipartite graph for MCFS.\n2.2\nAdaptive Structure Learning for High Dimensional\nData\nAs we can seen, the MCFS uses whole features to model the structure of data.\nThat means the similarity graph such as Gaussian similarity graph is built from\n5\nFigure 3: Unsupervised Feature Selection with Adaptive Structure Learning.\nall features. This is problematic when the dimension of data vector goes higher.\nTo be speciﬁc, the pairwise distance between any two data vectors becomes\nalmost the same, and as a consequence of that, the obtained structural informa-\ntion of data is not accuracy. This observation is the motivation of unsupervised\nFeature Selection with Adaptive Structure Learning (FSASL) algorithm which\nis proposed by Du et al. [5]. The idea of FSASL is to repeat MCFS iteratively\nwith updating selected feature sets. It can be illustrated as following: FASAL is\nan iterative algorithms which keeps pruning irrelevant and noisy features to ob-\ntain better manifold structure while improved structural info can help to search\nbetter relevant features. FASAL shows better performance in normalized mu-\ntual information and accuracy than MCFS generally. However, it’s very time\nconsuming since it is an iterative algorithm includes many eigen-decompositions.\n2.3\nRedundant Features\nFor high dimensional data X ∈Rn×d, it exists information redundancy among\nfeatures since d ≪n. Those redundant features can not provide further per-\nformance improvement for ongoing learning task. Instead, they impair the eﬃ-\nciency of learning algorithm to ﬁnd intrinsic data structure.\nIn this section, we describe our deﬁnition of feature redundancy.\nUnlike\nthe feature redundancy deﬁned bt Markov blanket [26] which is popular in\nexisting research works, our deﬁnition of feature redundancy is based on the\nlinear correlation between two vectors (the “vector” we used here could be a\nfeature vector or a linear combination of several feature vectors.) To measure\nthe redundancy between two vectors fi and fj, squared cosine similarity[22] is\nused:\nRij = cos2(fi, fj).\n(2)\nBy the math deﬁnition of cosine similarity, it is straightforward to know that\na higher value of Ri,j means high redundancy existing between fi and fj. For\nexample, feature vector fi and its duplication fi will have Rii value equals to\none. And two orthogonal feature vectors will have redundancy value equals to\nzero.\n6\n3\nProblem Statement\nIn this work, our goal is to detect those redundant features existing in high\ndimensional data and obtain a more accurate intrinsic data structure. To be\nspeciﬁc:\nProblem 1 Given a high dimensional data represented in the form of feature\nmatrix X, how to remove those redundant features f(·) ∈XT for unsupervised\nfeature selection algorithms such as MCFS?\nTechnically, the MCFS algorithm does not involve redundant features. How-\never, the performance of MCFS depends on the quality of indication vectors\nwhich are used to select features via sparse learning. And those indication vec-\ntors are highly related to the intrinsic structure of data which is described by the\nselected features and given distance metric. For example, the MCFS algorithm\nuses all features and Gaussian similarity to represent the intrinsic structure.\nThis is the discussed ‘chicken-and-egg” problem [5] between structure charac-\nterization and feature selection. The redundant and noise features will lead to\nan inaccurate estimation of data structure. As a result, it’s very demanding to\nremove those redundant (and noise) features before the calculation of indication\nvectors.\n4\nAlgorithm\nIn this section, we present our graph-based algorithm to detect and remove\nredundant features existing in high dimensional data. First, the sparse feature\ngraph that modeling the redundancy among feature vectors will be introduced.\nSecondly, the sparse representation error will be discussed. In the last, the local\ncompressible subgraph is proposed to extract redundant feature groups.\n4.1\nSparse Feature Graph (SFG)\nThe most popular way to model the redundancy among feature vectors is cor-\nrelation such as Pearson Correlation Coeﬃcient (PCC). The correlation value\nis deﬁned over two feature vectors, and it’s a pairwise measurement. However,\nthere also exiting redundancy between one feature vector and a set of feature\nvectors according to the philosophy of MCFS algorithm. In this section, we\npresent SFG, which model the redundancy not only between two feature vec-\ntors but also one feature vector and a set of feature vectors.\nThe basic idea of sparse feature graph is to looking for a sparse linear rep-\nresentation for each feature vector while using all other feature vectors as dic-\ntionary. For each feature vector fi in features set F = [f1, f2, · · ·, fd], SFG\nsolves the following optimization problem:\nmin\nα∈Rd−1 ∥fi −Φiαi∥2\n2,\ns.t. ∥αi∥0 < L,\n(3)\n7\nwhere Φi = [f1, f2, · · ·, fi−1, fi+1, · · ·, fd] is the dictionary of fi and each\ncolumn of Φi is a selected feature from data matrix X. L is a constraint to\nlimit the number of nonzero coeﬃcients.\nIn SFG, we set it to the number\nof features d. The αi is the coeﬃcient of each atom of dictionary Φi. This\ncoeﬃcient vector not only decides the edge link to fi but also indicates the\nweight of that connection. The resulted SFG is a weighted directed graph and\nmay have multiple components.\nFigure 4: Sparse feature graph and its relation with indication vectors. The\nlevel 1 features are direct sparse representation of those calculated indication\nvectors. The level 2 features only have representation relationship with level 1\nfeatures but not with indication vectors.\nTo solve the optimization problem 3, we use Orthogonal Matching Pursuit\n(OMP) solver [25] here since the number of features in our datasets is larger\nthan 1,000. We modify the stop criterion of OMP by checking the value change\nof residual instead of residual itself or the maximum number of supports. The\nreason is that we want the number of supports (or say, the number of edge con-\nnections) to follow the raw data property. Real world datasets are always noisy\nand messy. It’s highly possible that several feature vectors may fail to ﬁnd a cor-\nrect sparse linear representation through OMP. If we set residual or maximum\nof supports as criteria, we can not diﬀerentiate the successful representations\nand the failed ones.\nThe OMP solver and SFG algorithm can be described as following.\n4.2\nSparse Representation Error\nIn our modiﬁed OMP algorithm 1, we set a new stop criterion of searching sparse\nrepresentation solution for each feature vector fi. Instead of keep searching until\narriving a minimization error, we stop running while the solver could not reduce\nthe length of residual vector anymore. To be speciﬁc, the 2-norm of residual\nvector is monitored and the solver will stop once the change of this value small\nthan a user speciﬁed threshold.\n8\nAlgorithm 1: Orthogonal Matching Pursuit (OMP)\nInput\n: Φ = [f1, f2, · · ·, fi−1, fi+1, · · ·, fd] ∈Rn×(d−1), fi ∈Rn, ǫ.\nOutput: Coeﬃcient αi.\nInitialize residual diﬀerence threshold r0 = 1.0, residual q0 = fi, support\nset Γ0 = ∅, k = 1 ;\nwhile k ≤d −1 and |rk −rk−1| > ǫ do\nSearch the atom which most reduces the objective:\nj∗= arg min\nj∈ΓC\nn\nmin\nα ∥fi −ΦΓ∪{j}α∥2\n2\no\n;\nUpdate the active set:\nΓk = Γk−1 ∪{j∗};\nUpdate the residual (orthogonal projection):\nqk = (I −ΦΓk(ΦT\nΓkΦΓk)−1ΦT\nΓk)fi;\nUpdate the coeﬃcients:\nαΓk = (ΦT\nΓkΦΓk)−1ΦT\nΓkfi;\nrk = ∥qk∥2\n2;\nk ←k + 1;\nend\nAlgorithm 2: Sparse Feature Graph\nInput\n: Data matrix F = [f1, f2, · · ·, fd] ∈Rn×d;\nOutput: Adjacent matrix W of Graph G ∈Rd×d;\nNormalize each feature vector fi with ∥fi∥2\n2 = 1;\nfor i = 1, · · · , d do\nCompute αi from OMP(F−i,fi) using algorithm 1;\nend\nSet adjacent matrix Wij = αi(j) if i > j, Wij = αi(j −1), if i < j and\nWij = 0 if i == j;\n9\nFigure 5: Illustration of sparse representation error. SFG is a weighted directed\ngraph.\nThe reason we use this new stop criterion is that several feature vectors\nmay not ﬁnd correct sparse representation in current dataset, and the ordinary\nOMP solver will return a meaningless sparse representation when the maximum\niteration threshold arrived. Since the goal of SFG is not to ﬁnd a correct sparse\nrepresentation for every feature vectors, we utilize the new stop criterion and add\na ﬁlter process in our algorithm to identify those failed sparse representation.\nTo identify those failed sparse representation, we check the angle between\nthe original vector and the linear combination of its sparse representation. In\nthe language of SFG, we check the angle between a node (a feature vector) and\nthe weighted combination of its one-ring neighbor. Only the neighbors of out\nedges will be considered. This can be illustrated by following ﬁgure 5. As the\nexample in Figure 5, node fi has seven one-ring neighbors. But only\nbmf1, bmf2, f3, f5, f6 are its sparse representation and f4 and f7 are not. Then\nthe sparse representation error ζ is calculated by:\nf ∗\ni = w1f1 + w2f2 + w3f3 + w5f5 + w6f6,\nζ = arccos(fi, f ∗\ni ).\nOnce we have the SFG, we calculate the sparse representation errors for all\nnodes. A sparse representation is treated as fail if the angle ζ less than a user\nspeciﬁed value. We will ﬁlter out these node which has failed representation by\nremoving its out-edges.\n4.3\nLocal Compressible Subgraph\nWe group high correlated features through local compressible subgraphs. The\nSFG G is a weighted directed graph. With this graph, we need to ﬁnd all feature\nsubsets that has very high redundancy. To archive this goal, we propose a local\nsearch algorithm with seed nodes to group those highly correlated features into\nmany subgraphs which are named as local compressible subgraphs in this study.\nOur local search algorithm involves two steps, the ﬁrst step is to sort all nodes\nby the in-degree.\nBy the deﬁnition of SFG, the node with higher in-degree\nmeans it appears more frequently in other nodes’ sparse representation. The\n10\nsecond step is a local bread-ﬁrst search approach which ﬁnds all nodes that has\nhigher weight connections (in and out) to the growing subgraph. The detail\nsubgraph searching algorithm can be described by: In Alg. 3, function label(n)\nAlgorithm 3: Local Compressible Subgraphs.\nInput\n: Weighted directed graph G = (V, E), edge weight threshold θ;\nOutput: Local compressible subgraphs C .\nTag all nodes with initial label 0;\nSort the nodes by its in-degree decreasingly;\ncurrent label = 1;\nfor n = 1 : |V | do\nif label(n) ! = 0 then\ncontinue;\nend\nset label of node n to current label;\nBFS(n, θ, current label);\ncurrent label + = 1;\nend\n/* current label now has the maximum value of labels.\n*/\nfor i = 1 : current label do\nExtract subgraph ci which all nodes have label i;\nif |ci| > 1 then\nadd ci to C;\nend\nend\ncheck the current label of node n, and BFS(n, θ, current label) function runs a\nlocal Breadth-First search for subgraph that has edge weight large than θ.\n4.4\nRedundant Feature Removal\nThe last step of our algorithm is to remove the redundant features. For each\nlocal compressible subgraph we found, we pick up the node which has the highest\nin-degree as the representative node of that local compressible subgraph. So\nthe number of ﬁnal feature vectors equals to the number of local compressible\nsubgraphs.\n5\nExperiments\nIn this section, we present experimental results to demonstrate the eﬀectiveness\nof our proposed algorithm. We ﬁrst evaluate the spectral clustering performance\nbefore and after applying our algorithms. Secondly, we show the performance\nof MCFS with or without our algorithm. In the last, the properties of generated\nsparse graphs and sensitivity of parameters are discussed.\n11\n5.1\nExperiment Setup\nDatasets.\nWe select twelve real-world high dimensional datasets [11] from\nthree diﬀerent domains: Image, Text and Biomedical. The detail of each dataset\nis listed in Table 1. The datasets have sample size diﬀerent from 96 to 8293 and\nfeature size ranging from 1,024 to 18,933. Also, the datasets have class labels\nfrom 2 to 64. The purpose of this selection is to let the evaluation results be\nmore general by applying datasets with various characteristics.\nName\n#Sample\n#Feature\n#Class\nType\nORL\n400\n1024\n40\nImage\nYale\n165\n1024\n15\nImage\nPIE10P\n210\n2420\n10\nImage\nORL10P\n100\n10304\n10\nImage\nBASEHOCK\n1993\n4862\n2\nText\nRELATHE\n1427\n4322\n2\nText\nPCMAC\n1943\n3289\n2\nText\nReuters\n8293\n18933\n65\nText\nlymphoma\n96\n4026\n9\nBiomedical\nLUNG\n203\n3312\n5\nBiomedical\nCarcinom\n174\n9182\n11\nBiomedical\nCLL-SUB-111\n111\n11340\n3\nBiomedical\nTable 1: High dimensional datasets.\nNormalization.\nThe features of each dataset are normalized to have unit\nlength, which means ∥fi∥2 = 1 for all datasets.\nEvaluation Metric.\nOur proposed algorithm is under the framework of un-\nsupervised learning. Without loss of generality, the cluster structure of data\nis used for evaluation. To be speciﬁc, we measure the spectral clustering per-\nformance with Normalized Mutual Information (NMI) and Accuracy (ACC).\nNMI value ranges from 0.0 to 1.0, with higher value means better clustering\nperformance. ACC is another metric to evaluate the clustering performance\nby measuring the fraction of its clustering result that are correct. Similar to\nNMI, its values range from 0 to 1 and higher value indicates better algorithm\nperformance.\nSuppose A is the clustering result and B is the known sample label vector.\nLet p(a) and p(b) denote the marginal probability mass function of A and B,\nand let p(a, b) be the joint probability mass function of A and B.\nSuppose\nH(A), H(B) and H(A, B) denote the entropy of p(a), p(b) and p(a, b) respec-\ntively. Then the normalized mutual information NMI is deﬁned as:\nNMI(A, B) = H(A) + H(B) −H(A, B)\nmax(H(A), H(B))\n(4)\n12\nAssume A is the clustering result label vector, and B is the known ground\ntruth label vector, ACC is deﬁned as:\nACC =\nN\nP\ni=1\nδ(B(i), Map(A,B)(i))\nN\n(5)\nwhere N denotes the length of label vector, δ(a, b) equals to 1 if only if a and b\nare equal. MapA,B is the best mapping function that permutes A to match B.\n5.2\nEﬀectiveness of Redundant Features Removal\nOur proposed algorithm removes many features to reduce the dimension size of\nall data vectors. As a consequence, the pairwise Euclidean distance is changed\nand the cluster structure will be aﬀected. To measure the eﬀectiveness of our\nproposed algorithm, we check the spectral clustering performance before and\nafter redundant feature removal. If the NMI and ACC values are not changed to\nmuch and stay in the same level, the experiment results show that our proposed\nalgorithm is correct and eﬀective.\nThe spectral clustering algorithm we used in our experiments is the Ng-\nJordan-Weiss (NJW) algorithm [16]. The Gaussian similarity graph is applied\nhere as the input and parameter σ is set to the mean value of pairwise Euclidean\ndistance among all vectors.\nOur proposed LCS algorithm includes a parameter θ which is the threshold\nof redundancy.\nIt decides the number of redundant features implicitly, and\naﬀects the cluster structure of data consequently. In our experiment design, we\ntest diﬀerent θ values ranging from 90% to 10% with step size equal to 10%:\nθ = [0.9, 0.8, 0.7, · · · , 0.1].\nWe present our experiment results for image datasets, text datasets, and\nbiological datasets in Figure 6, Figure 7 and Figure 8 respectively. For each\ndataset, we show the NMI, ACC performance with diﬀerent θ and comparing\nwith original spectral clustering performance by using all features. From the\nexperimental results, we can read that: Even when θ is reduced to 30%,\nthe NMI and ACC values are staying in same level as original data.\nWhen θ equals to 30%, it means the edges of SFG that with weights (absolute\nvalue) in the highest 70% value range are removed. (It does not mean that 70%\nof top weights edges are removed). This observation validate the correctness of\nour proposed algorithm.\n5.3\nPerformance of MCFS\nOur proposed algorithm is targeting for unsupervised feature selection. And the\nquality of indication vectors (or the spectral clustering performance based on\neigenvectors) is an important factor evaluate the eﬀectiveness of our proposed\nalgorithm. In this section, we evaluate the MCFS performance over the redun-\ndant feature removed data, and comparing with the raw data that without any\nfeature removal.\n13\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nORL\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nYale\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nPIE10P\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nORL10P\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nORL\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nYale\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nPIE10P\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nORL10P\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n0\n200\n400\n600\n800\n1000\n1200\nORL\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n0\n200\n400\n600\n800\n1000\n1200\nYale\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n0\n500\n1000\n1500\n2000\n2500\nPIE10P\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n2000\n4000\n6000\n8000\n10000\n12000\nORL10P\nFigure 6: Spectral clustering performance of Image datasets with diﬀerent pa-\nrameter θ. Top row: NMI; Middle row: ACC; Bottom row: number of features,\nthe red dash line means the size of raw dataset.\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nBasehock\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI x 0.001\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\nRelathe\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI x 0.01\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nPCMAC\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nReuters\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nBasehock\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nRelathe\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nPCMAC\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nReuters\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n3800\n4000\n4200\n4400\n4600\n4800\n5000\nBasehock\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n3800\n3900\n4000\n4100\n4200\n4300\n4400\nRelathe\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n3000\n3050\n3100\n3150\n3200\n3250\n3300\nPCMAC\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n×104\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\nReuters\nFigure 7: Spectral clustering performance of Text datasets with diﬀerent pa-\nrameter θ. Top row: NMI; Middle row: ACC; Bottom row: number of features,\nthe red dash line means the size of raw dataset.\n14\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nlymphoma\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nLUNG\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nCarcinom\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNMI\n0\n0.2\n0.4\n0.6\n0.8\n1\nCLL-SUB-111\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nlymphoma\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nLUNG\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nCarcinom\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nACC\n0\n0.2\n0.4\n0.6\n0.8\n1\nCLL-SUB-111\nDifferent θ\nAll features\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n0\n1000\n2000\n3000\n4000\n5000\nlymphoma\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nLUNG\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n0\n2000\n4000\n6000\n8000\n10000\nCarcinom\nθ\n90%\n70%\n50%\n30%\n10%\nNumber of features\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n11000\n12000\nCLL-SUB-111\nFigure 8: Spectral clustering performance of Biomedical datasets with diﬀerent\nparameter θ.\nTop row: NMI; Middle row: ACC; Bottom row: number of\nfeatures, the red dash line means the size of raw dataset.\nThe spectral clustering performance is measured for diﬀerent input data from\noriginal whole feature data to processed ones by our proposed algorithm with\ndiﬀerent θ. We report the experiment results over image datasets and biological\ndatasets in this section. For text datasets, the feature vectors of them are very\nsparse, and our eigen decomposition process are always failed and we only can\ncollect partial results. For fair evaluation, we omit the experiment results of text\ndatasets in this work. The result of MCFS performance shows from Table 2 to\nTable 17.\nFor each dataset, we set the number of selected features ranging from [5, 10, 15, · · · , 60],\nwhich has 11 diﬀerent sizes in total. The parameter θ is conﬁgured from 0.9 to\n0.1 with stepsize equals to 0.1.\nWe report the experimental results in tables (from Table 2 to Table 17). For\neach table, the ﬁrst row means the number of features that used as input of\nMCFS. The ﬁrst column is the number of selected features by MCFS algorithm.\nThe baseline is in the second column, which is the testing result of MCFS\nalgorithm with raw data.\nThe hyphens in the tables means the number of\nselected features is larger than the feature size of input data, which means\ninvalid test. To show the eﬀectiveness of our algorithm, we also mark those\nNMI and ACC scores that larger or equals to baseline in bold text.\n5.4\nSparse Representation Errors\nWith the design of our modiﬁed OMP solvers, there will be failed/wrong sparse\nrepresentations existing in generated sparse feature graph.\nThe meaning of\nthese edge connections and edge weights are invalid. And they should be re-\n15\n#f\n1024\n913\n620\n535\n469\n327\n160\n104\n58\n33\n10\n0.63\n0.51\n0.60\n0.56\n0.53\n0.62\n0.61\n0.65 0.60 0.62\n15\n0.66\n0.56\n0.63\n0.60\n0.58\n0.67\n0.62\n0.60\n0.63 0.58\n20\n0.67\n0.59\n0.65\n0.64\n0.59\n0.64\n0.63\n0.61\n0.64 0.56\n25\n0.67\n0.59\n0.66\n0.64\n0.63\n0.65\n0.66\n0.64\n0.65 0.58\n30\n0.68\n0.63\n0.66\n0.65\n0.66\n0.67\n0.65\n0.67\n0.65 0.59\n35\n0.69\n0.64\n0.70\n0.66\n0.65\n0.67\n0.67\n0.68\n0.65\n-\n40\n0.70\n0.67\n0.71\n0.68\n0.67\n0.68\n0.70 0.70 0.66\n-\n45\n0.70\n0.69\n0.70\n0.69\n0.66\n0.69\n0.70\n0.69\n0.65\n-\n50\n0.73\n0.71\n0.72\n0.68\n0.66\n0.70\n0.72\n0.69\n0.66\n-\n55\n0.71\n0.74\n0.70\n0.68\n0.67\n0.71\n0.71\n0.71\n0.66\n-\n60\n0.71\n0.74\n0.71 0.72 0.71\n0.69\n0.72\n0.71\n-\n-\nTable 2: NMI results of “ORL” dataset\n#f\n1024\n913\n620\n535\n469\n327\n160\n104\n58\n33\n10\n0.38\n0.28\n0.36\n0.31\n0.28 0.39 0.39 0.46 0.39 0.41\n15\n0.45\n0.33\n0.41\n0.40\n0.34\n0.43\n0.40\n0.38\n0.42\n0.36\n20\n0.47\n0.34\n0.43\n0.43\n0.35\n0.43\n0.41\n0.39\n0.43\n0.32\n25\n0.48\n0.35\n0.45\n0.44\n0.37\n0.42\n0.47\n0.41\n0.45\n0.34\n30\n0.47\n0.40\n0.42\n0.42\n0.43\n0.47\n0.43\n0.45\n0.42\n0.35\n35\n0.49\n0.41\n0.48\n0.46\n0.44\n0.44\n0.47\n0.47\n0.42\n-\n40\n0.51\n0.46\n0.53\n0.48\n0.46\n0.45\n0.48\n0.51\n0.43\n-\n45\n0.49\n0.47\n0.51 0.51 0.44\n0.48\n0.49 0.49\n0.43\n-\n50\n0.55\n0.51\n0.52\n0.47\n0.47\n0.50\n0.52\n0.48\n0.46\n-\n55\n0.53\n0.53\n0.51\n0.46\n0.45\n0.48\n0.50\n0.53\n0.46\n-\n60\n0.51\n0.55 0.52 0.54 0.51\n0.47\n0.54\n0.51\n-\n-\nTable\n3:\nACC\nresults\nof\n“ORL”\ndataset.\n#f\n1024\n1023\n964\n654\n525\n427\n271\n152\n83\n34\n10\n0.48\n0.43\n0.43\n0.45\n0.42\n0.46\n0.45\n0.46\n0.47\n0.44\n15\n0.49\n0.47\n0.46\n0.51 0.49\n0.48\n0.45\n0.47\n0.50 0.43\n20\n0.49\n0.48\n0.46\n0.55\n0.48\n0.51\n0.47\n0.47\n0.51 0.41\n25\n0.51\n0.49\n0.49\n0.52 0.52 0.52\n0.45\n0.49\n0.54 0.41\n30\n0.51\n0.51\n0.49\n0.54\n0.50\n0.51 0.51\n0.49\n0.50 0.39\n35\n0.53\n0.49\n0.50\n0.54 0.53\n0.52\n0.52\n0.48\n0.50\n-\n40\n0.49\n0.50 0.51 0.53 0.58 0.55 0.55\n0.48\n0.51\n-\n45\n0.48\n0.51 0.51 0.56 0.59 0.57 0.52 0.52 0.49\n-\n50\n0.52\n0.50\n0.47\n0.53 0.59 0.53 0.53 0.56\n0.49\n-\n55\n0.54\n0.51\n0.52\n0.55\n0.50\n0.51\n0.51\n0.51\n0.49\n-\n60\n0.54\n0.49\n0.51\n0.49\n0.54\n0.50\n0.51\n0.46\n0.52\n-\nTable 4: NMI results of “Yale” dataset\n#f\n1024\n1023\n964\n654\n525\n427\n271\n152\n83\n34\n10\n0.39\n0.36\n0.37\n0.36\n0.33\n0.38\n0.41 0.40 0.41 0.36\n15\n0.43\n0.41\n0.42\n0.44\n0.41\n0.41\n0.39\n0.41\n0.46 0.39\n20\n0.44\n0.42\n0.41\n0.48 0.44 0.44\n0.43\n0.42\n0.44 0.35\n25\n0.45\n0.45\n0.44\n0.46 0.47 0.45\n0.41\n0.43\n0.49 0.33\n30\n0.48\n0.44\n0.42\n0.47\n0.47\n0.45\n0.45\n0.40\n0.47\n0.33\n35\n0.48\n0.48\n0.44\n0.50\n0.47\n0.46\n0.47\n0.41\n0.44\n-\n40\n0.42\n0.44 0.45 0.50 0.55 0.48 0.53\n0.41\n0.44\n-\n45\n0.41\n0.48 0.46 0.51 0.53 0.54 0.49 0.47 0.42\n-\n50\n0.46\n0.41\n0.42\n0.48 0.56 0.50 0.46 0.52\n0.41\n-\n55\n0.48\n0.44\n0.48 0.48\n0.43\n0.45\n0.49\n0.47\n0.42\n-\n60\n0.50\n0.42\n0.44\n0.40\n0.50\n0.41\n0.46\n0.42\n0.43\n-\nTable 5: ACC results of “Yale” dataset.\n#f\n2420\n2409\n1871\n793\n698\n662\n654\n630\n566\n324\n10\n0.44\n0.48 0.55 0.53 0.58 0.56 0.54 0.61 0.50\n0.38\n15\n0.44\n0.61 0.57 0.50 0.58 0.58 0.55 0.59 0.53\n0.39\n20\n0.43\n0.56 0.61 0.59 0.60 0.56 0.62 0.59 0.56\n0.41\n25\n0.52\n0.61 0.61 0.64 0.61 0.60 0.58 0.58 0.54 0.43\n30\n0.53\n0.61 0.62 0.57 0.62 0.62 0.60 0.53 0.63\n0.41\n35\n0.59\n0.60 0.59 0.60 0.63 0.61 0.60 0.62 0.64\n0.43\n40\n0.53\n0.60 0.58 0.57 0.66 0.62 0.59 0.62 0.69\n0.42\n45\n0.55\n0.61 0.61 0.62 0.60 0.64\n0.60\n0.64 0.65\n0.43\n50\n0.56\n0.63 0.62 0.68 0.64 0.62 0.58 0.63 0.66\n0.37\n55\n0.61\n0.60\n0.62 0.69 0.62\n0.60\n0.57\n0.65\n0.58\n0.39\n60\n0.55\n0.64 0.63 0.64 0.60 0.63\n0.54\n0.63\n0.51\n0.39\nTable 6:\nNMI results of “PIE10P”\ndataset\n#f\n2420\n2409\n1871\n793\n698\n662\n654\n630\n566\n324\n10\n0.39\n0.45 0.48 0.50 0.56 0.50 0.53 0.59 0.46\n0.39\n15\n0.39\n0.58 0.51 0.49 0.51 0.55 0.56 0.60 0.50 0.41\n20\n0.36\n0.51 0.53 0.53 0.55 0.56 0.60 0.54 0.50 0.38\n25\n0.45\n0.59 0.53 0.60 0.54 0.59 0.60 0.56 0.52\n0.40\n30\n0.50\n0.58 0.56 0.58 0.59 0.60 0.59 0.49 0.60\n0.40\n35\n0.48\n0.57 0.51 0.59 0.61 0.53 0.54 0.62 0.61 0.37\n40\n0.42\n0.52 0.53 0.56 0.63 0.59 0.53 0.60 0.64\n0.38\n45\n0.44\n0.52 0.52 0.58 0.51 0.63 0.54 0.62 0.60 0.41\n50\n0.44\n0.61 0.52 0.64 0.60 0.59 0.55 0.62 0.61\n0.37\n55\n0.46\n0.54 0.53 0.67 0.58 0.57 0.57 0.63 0.54\n0.37\n60\n0.49\n0.60 0.61 0.61 0.57 0.61 0.51 0.61 0.46\n0.35\nTable 7:\nACC results of “PIE10P”\ndataset.\n#f\n10304\n10302\n8503\n3803\n3408\n3244\n3030\n2822\n2638\n2175\n10\n0.65\n0.78\n0.77 0.76 0.77 0.80 0.74 0.72 0.75 0.73\n15\n0.72\n0.82\n0.79 0.78 0.81 0.83 0.79 0.81 0.75 0.79\n20\n0.76\n0.81\n0.74\n0.78 0.84 0.83 0.81 0.76 0.80 0.78\n25\n0.79\n0.84\n0.74\n0.73\n0.82 0.86 0.88 0.83 0.86 0.81\n30\n0.75\n0.77\n0.82\n0.74\n0.88 0.82 0.83 0.83 0.86 0.86\n35\n0.81\n0.81\n0.80\n0.83 0.85 0.83\n0.80\n0.82 0.85 0.85\n40\n0.83\n0.88\n0.84 0.84 0.90 0.86\n0.81\n0.93 0.84 0.87\n45\n0.84\n0.93\n0.83\n0.85 0.91 0.86\n0.83\n0.88 0.84 0.86\n50\n0.78\n0.88\n0.88 0.87 0.89 0.86 0.82 0.90 0.84 0.83\n55\n0.84\n0.89\n0.86 0.89 0.91 0.89 0.88 0.86 0.84 0.86\n60\n0.85\n0.88\n0.86\n0.84\n0.85 0.91 0.85 0.88 0.86 0.85\nTable 8:\nNMI results of “ORL10P”\ndataset\n#f\n10304\n10302 8503\n3803\n3408\n3244\n3030\n2822\n2638\n2175\n10\n0.66\n0.74\n0.81 0.75 0.75 0.69 0.72 0.70 0.69 0.67\n15\n0.69\n0.85\n0.76 0.78 0.78 0.86 0.80 0.75 0.73 0.75\n20\n0.77\n0.84\n0.74\n0.76\n0.80 0.80 0.78\n0.69\n0.75\n0.74\n25\n0.71\n0.79\n0.68\n0.74 0.78 0.86 0.84 0.82 0.82 0.74\n30\n0.71\n0.71\n0.77\n0.68\n0.86 0.77 0.81 0.77 0.82 0.81\n35\n0.74\n0.74\n0.74 0.76 0.81 0.77\n0.73\n0.76 0.82 0.78\n40\n0.80\n0.85\n0.74\n0.77\n0.87 0.80\n0.75\n0.89 0.80 0.83\n45\n0.82\n0.89\n0.73\n0.81\n0.88\n0.78\n0.77\n0.86\n0.80\n0.79\n50\n0.73\n0.80\n0.80 0.74 0.86 0.79 0.74 0.88 0.81 0.77\n55\n0.79\n0.85\n0.82 0.86 0.89 0.87 0.80 0.82 0.81 0.79\n60\n0.82\n0.84\n0.77\n0.75\n0.82 0.89\n0.77\n0.84 0.82 0.82\nTable 9:\nACC results of “ORL10P”\ndataset.\nmoved from the SFG since wrong connections will deteriorate the accuracy of\nfeature redundancy relationship. To validate the sparse representation, we check\nthe angle between original feature vector and the linear weighted summation re-\nsulted vector (or recover signal from sparse coding point of view) from its sparse\nrepresentation. If the angle lower than a threshold, we remove all out-edges from\nthe generated sparse feature graph. To specify the threshold, we learn it from\nthe empirical results of our selected twelve datasets. The distribution (or his-\ntogram) result of angle values is presented in ﬁgure 9.\n16\n#f\n4026\n4009\n3978\n3899\n3737\n3456\n2671\n1203\n334\n136\n10\n0.51\n0.59 0.58 0.52\n0.50\n0.50\n0.51\n0.50\n0.50\n0.49\n15\n0.55\n0.60 0.62 0.56 0.58 0.58 0.58 0.56\n0.47\n0.52\n20\n0.60\n0.61 0.60\n0.57\n0.62 0.62 0.64\n0.58\n0.58\n0.60\n25\n0.63\n0.59\n0.64\n0.60\n0.63\n0.58\n0.66\n0.57\n0.56\n0.53\n30\n0.59\n0.61 0.62 0.60 0.62 0.64 0.65 0.60 0.60 0.59\n35\n0.61\n0.66 0.62\n0.60\n0.65 0.62 0.61 0.62\n0.56\n0.53\n40\n0.64\n0.60\n0.66\n0.63\n0.61\n0.63\n0.66\n0.61\n0.58\n0.55\n45\n0.58\n0.63 0.62 0.62 0.58 0.61 0.63 0.64 0.60\n0.57\n50\n0.65\n0.60\n0.61\n0.61\n0.56\n0.63\n0.61\n0.63\n0.58\n0.54\n55\n0.63\n0.60\n0.61\n0.62\n0.60\n0.60\n0.63\n0.60\n0.58\n0.58\n60\n0.60\n0.60 0.63 0.61 0.63\n0.59\n0.65\n0.59\n0.57\n0.57\nTable 10: NMI results of “Lymphoma”\ndataset\n#f\n4026\n4009\n3978\n3899\n3737\n3456\n2671\n1203\n334\n136\n10\n0.50\n0.57 0.56 0.53\n0.49\n0.51 0.51\n0.48\n0.50 0.50\n15\n0.53\n0.62 0.59 0.58 0.56 0.59 0.58 0.55\n0.50\n0.53\n20\n0.59\n0.56\n0.55\n0.56\n0.56\n0.59 0.59\n0.54\n0.55\n0.59\n25\n0.60\n0.57\n0.62\n0.56\n0.62\n0.58\n0.64\n0.56\n0.52\n0.50\n30\n0.56\n0.60 0.58 0.58 0.59 0.61 0.65 0.59 0.57\n0.55\n35\n0.55\n0.62 0.59 0.58 0.61 0.60 0.57 0.59 0.55\n0.53\n40\n0.66\n0.57\n0.61\n0.61\n0.61\n0.59\n0.60\n0.58\n0.59\n0.54\n45\n0.54\n0.60 0.60 0.58 0.55 0.60 0.62 0.59 0.56 0.54\n50\n0.65\n0.62\n0.58\n0.64\n0.52\n0.59\n0.56\n0.59\n0.53\n0.53\n55\n0.57\n0.60 0.65 0.60\n0.54\n0.57 0.65 0.59\n0.54\n0.59\n60\n0.56\n0.58 0.64 0.58 0.61 0.57 0.67 0.56\n0.53\n0.57\nTable 11: ACC results of “Lymphoma”\ndataset.\n#f\n3312\n3311\n3309\n3236\n1844\n559\n384\n344\n305\n183\n10\n0.42\n0.42 0.43 0.49 0.52 0.53 0.43 0.46 0.43 0.25\n15\n0.54\n0.54\n0.53\n0.51\n0.51\n0.51\n0.45\n0.52\n0.38\n0.21\n20\n0.51\n0.51 0.52 0.53\n0.41\n0.49\n0.36\n0.52\n0.38\n0.20\n25\n0.51\n0.51 0.53\n0.48\n0.42\n0.52\n0.40\n0.48\n0.35\n0.26\n30\n0.47\n0.48 0.52 0.49\n0.41\n0.37\n0.49 0.48\n0.41\n0.24\n35\n0.46\n0.38\n0.46 0.48\n0.39\n0.52 0.49\n0.38\n0.35\n0.27\n40\n0.49\n0.49 0.50\n0.46\n0.43\n0.40\n0.38\n0.35\n0.40\n0.29\n45\n0.36\n0.42\n0.33\n0.47 0.40\n0.33\n0.38\n0.35\n0.35\n0.31\n50\n0.45\n0.45 0.47 0.49 0.52\n0.32\n0.40\n0.36\n0.35\n0.31\n55\n0.44\n0.44 0.44 0.49 0.51\n0.33\n0.49\n0.31\n0.30\n0.31\n60\n0.47\n0.46\n0.45\n0.51 0.49\n0.33\n0.39\n0.32\n0.31\n0.35\nTable 12:\nNMI results of “LUNG”\ndataset\n#f\n3312\n3311\n3309\n3236\n1844\n559\n384\n344\n305\n183\n10\n0.71\n0.72 0.73 0.77 0.77 0.75 0.68 0.65 0.66 0.56\n15\n0.81\n0.81\n0.79\n0.72\n0.73\n0.72\n0.67 0.65 0.58 0.48\n20\n0.71\n0.73 0.74 0.72\n0.69\n0.69\n0.61 0.60 0.58 0.39\n25\n0.71\n0.71 0.74\n0.67\n0.69\n0.68\n0.59 0.61 0.56 0.49\n30\n0.66\n0.66 0.67 0.71 0.68\n0.56\n0.59 0.59 0.61 0.43\n35\n0.64\n0.60\n0.63\n0.68 0.66\n0.60\n0.58 0.56 0.53 0.49\n40\n0.65\n0.65 0.66 0.65\n0.64\n0.57\n0.54 0.54 0.56 0.46\n45\n0.60\n0.63\n0.57\n0.65 0.61\n0.52\n0.54 0.52 0.52 0.49\n50\n0.65\n0.65\n0.63\n0.65 0.65\n0.48\n0.57 0.53 0.53 0.52\n55\n0.61\n0.61\n0.59\n0.65 0.62\n0.48\n0.59 0.48 0.49 0.49\n60\n0.64\n0.63\n0.63\n0.64\n0.62\n0.51\n0.55 0.49 0.48 0.51\nTable 13:\nACC results of “LUNG”\ndataset.\n#f\n9182\n9180\n9179\n9150\n7736\n3072\n697\n449\n360\n144\n10\n0.70\n0.70 0.70\n0.69\n0.67\n0.64\n0.66\n0.65\n0.66\n0.47\n15\n0.71\n0.70\n0.73 0.73 0.74\n0.66\n0.67\n0.70\n0.66\n0.52\n20\n0.77\n0.78 0.77\n0.72\n0.75\n0.72\n0.73\n0.71\n0.73\n0.54\n25\n0.74\n0.77 0.77 0.75 0.74\n0.71\n0.79 0.75 0.74 0.53\n30\n0.69\n0.71 0.72 0.70 0.74 0.75 0.77 0.79 0.73 0.54\n35\n0.77\n0.76\n0.76\n0.76\n0.74\n0.77 0.78 0.78 0.78 0.60\n40\n0.75\n0.74\n0.76 0.77\n0.74\n0.79 0.76 0.78 0.75 0.59\n45\n0.77\n0.76\n0.74\n0.78\n0.74\n0.82 0.78 0.80 0.79 0.57\n50\n0.79\n0.76\n0.75\n0.75\n0.79\n0.76\n0.79 0.84 0.83 0.58\n55\n0.75\n0.76 0.76\n0.74\n0.75 0.79 0.79 0.83 0.83 0.59\n60\n0.74\n0.72\n0.76\n0.73\n0.76 0.82 0.84 0.82 0.78 0.62\nTable 14: NMI results of “Carcinom”\ndataset\n#f\n9182\n9180\n9179\n9150\n7736\n3072\n697\n449\n360\n144\n10\n0.63\n0.66\n0.62\n0.61\n0.67\n0.60\n0.60\n0.59\n0.64 0.48\n15\n0.67\n0.57\n0.70\n0.66\n0.68\n0.63\n0.57\n0.67\n0.64\n0.53\n20\n0.70\n0.68\n0.74\n0.66\n0.71 0.71\n0.64\n0.73 0.74 0.56\n25\n0.70\n0.72 0.75\n0.69\n0.75\n0.64\n0.75 0.72 0.76 0.51\n30\n0.61\n0.64 0.70 0.69 0.67 0.71 0.74 0.76 0.71 0.52\n35\n0.76\n0.74\n0.74\n0.74\n0.70\n0.75\n0.70\n0.76 0.77 0.57\n40\n0.72\n0.72 0.73 0.75\n0.69\n0.76\n0.66\n0.78\n0.71\n0.56\n45\n0.75\n0.74\n0.70\n0.75\n0.74\n0.79\n0.72\n0.79 0.76 0.55\n50\n0.74\n0.74\n0.70\n0.72\n0.74\n0.66\n0.74 0.83 0.79 0.56\n55\n0.73\n0.74 0.74\n0.72\n0.71\n0.72\n0.72\n0.82 0.80 0.56\n60\n0.70\n0.61\n0.71\n0.66\n0.72 0.75 0.82 0.80 0.77 0.55\nTable 15: ACC results of “Carcinom”\ndataset.\n#f\n11340\n11335 11301 10573\n8238\n7053\n6697\n6533\n6180\n4396\n10\n0.16\n0.16\n0.15\n0.26\n0.18 0.22 0.20 0.20 0.20 0.21\n15\n0.14\n0.14\n0.15\n0.26\n0.18 0.28\n0.09\n0.24\n0.07\n0.06\n20\n0.16\n0.16\n0.15\n0.08\n0.14\n0.21\n0.04\n0.31 0.16\n0.11\n25\n0.14\n0.14\n0.15\n0.09\n0.08\n0.22 0.23\n0.10\n0.09\n0.11\n30\n0.13\n0.13\n0.13\n0.08\n0.07\n0.18\n0.03\n0.14\n0.10\n0.11\n35\n0.17\n0.17\n0.13\n0.03\n0.07\n0.12\n0.10\n0.01\n0.08\n0.10\n40\n0.14\n0.14\n0.14\n0.07\n0.08\n0.13\n0.12\n0.05\n0.14\n0.09\n45\n0.09\n0.09\n0.18\n0.08\n0.11 0.10 0.13\n0.07\n0.12 0.09\n50\n0.15\n0.14\n0.15\n0.08\n0.11\n0.11\n0.12\n0.12\n0.13\n0.09\n55\n0.15\n0.15\n0.14\n0.21\n0.08\n0.13\n0.13\n0.12\n0.13\n0.07\n60\n0.10\n0.10\n0.14\n0.15\n0.08\n0.10 0.12 0.12 0.14\n0.07\nTable 16: NMI results of “CLL-SUB-\n111” dataset\n#f\n11340\n11335 11301 10573 8238\n7053\n6697\n6533\n6180\n4396\n10\n0.51\n0.51\n0.50\n0.54\n0.59 0.57 0.58 0.55 0.51\n0.50\n15\n0.51\n0.51\n0.50\n0.57\n0.55 0.62\n0.47\n0.59\n0.45\n0.43\n20\n0.50\n0.50\n0.48\n0.46\n0.50 0.54\n0.40\n0.59 0.54 0.50\n25\n0.48\n0.48\n0.51\n0.44\n0.46\n0.54 0.57 0.50\n0.46\n0.50\n30\n0.49\n0.49\n0.49\n0.44\n0.44\n0.53\n0.42\n0.51\n0.48\n0.48\n35\n0.51\n0.51\n0.49\n0.42\n0.44\n0.49\n0.49\n0.41\n0.44\n0.48\n40\n0.51\n0.51\n0.50\n0.43\n0.45\n0.50\n0.49\n0.43\n0.48\n0.47\n45\n0.46\n0.45\n0.52\n0.44\n0.46 0.47 0.51\n0.45\n0.47 0.47\n50\n0.51\n0.50\n0.51\n0.45\n0.46\n0.50\n0.49\n0.49\n0.49\n0.48\n55\n0.49\n0.49\n0.50\n0.54\n0.46\n0.50 0.50 0.49 0.49\n0.45\n60\n0.49\n0.49\n0.50\n0.53\n0.43\n0.48\n0.49 0.49 0.50\n0.44\nTable 17: ACC results of “CLL-SUB-\n111” dataset.\n6\nRelated Works\nRemove redundant features is an important step for feature selection algorithms.\nPrestigious works include\n[26] which gives a formal deﬁnition of redundant\nfeatures. Peng et al. [17] propose a greedy algorithm (named as mRMR) to\nselect features with minimum redundancy and maximum dependency. Zhao et\nal. [27] develop an eﬃcient spectral feature selection algorithm to minimize the\n17\nAngle\n0\n20\n40\n60\n80\n100\n0\n100\n200\n300\n400\n500\n600\norl\nAngle\n0\n20\n40\n60\n80\n100\n0\n100\n200\n300\n400\n500\n600\n700\n800\nyale\nAngle\n0\n20\n40\n60\n80\n100\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\npie10p\nAngle\n0\n20\n40\n60\n80\n100\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\norl10p\nAngle\n0\n20\n40\n60\n80\n100\n0\n200\n400\n600\n800\n1000\nbasehock\nAngle\n0\n20\n40\n60\n80\n100\n0\n200\n400\n600\n800\n1000\nrelathe\nAngle\n0\n20\n40\n60\n80\n100\n0\n100\n200\n300\n400\n500\n600\n700\npcmac\nAngle\n0\n20\n40\n60\n80\n100\n0\n500\n1000\n1500\n2000\n2500\n3000\nreuters\nAngle\n0\n20\n40\n60\n80\n100\n0\n500\n1000\n1500\n2000\n2500\n3000\nlymphoma\nAngle\n0\n20\n40\n60\n80\n100\n0\n500\n1000\n1500\n2000\n2500\nlung\nAngle\n0\n20\n40\n60\n80\n100\n0\n1000\n2000\n3000\n4000\n5000\ncarcinom\nAngle\n0\n20\n40\n60\n80\n100\n0\n1000\n2000\n3000\n4000\n5000\ncll-sub-111\nFigure 9: The distribution of angle between original feature vector and its sparse\nrepresentation.\nredundancy within the selected feature subset through L2,1 norm. Recently, re-\nsearchers pay attention to unsupervised feature selection with global minimized\nredundancy [22] [21]. Several graph based approaches are proposed in [15], [19].\nThe most closed research work to us is [13] which build a sparse graph at feature\nside and ranking features by approximation errors.\n7\nConclusion\nIn this study, we propose sparse feature graph to model both one-to-one feature\nredundancy and one-to-many features redundancy. By separate whole features\ninto diﬀerent redundancy feature group through local compressible subgraphs,\nwe reduce the dimensionality of data by only select one representative feature\nfrom each group. One advantage of our algorithm is that it does not need to\ncalculate the pairwise distance which is always not accurate for high dimensional\ndatasets. The experiment results shows that our algorithm is an eﬀective way to\nobtain accurate data structure information which is demanding for unsupervised\nfeature selection algorithms.\nReferences\n[1] Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the\nsurprising behavior of distance metrics in high dimensional space. In Inter-\nnational Conference on Database Theory, pages 420–434. Springer, 2001.\n18\n[2] Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft.\nWhen is nearest neighbor meaningful?\nIn International conference on\ndatabase theory, pages 217–235. Springer, 1999.\n[3] Deng Cai, Chiyuan Zhang, and Xiaofei He. Unsupervised feature selection\nfor multi-cluster data. In Proceedings of the 16th ACM SIGKDD interna-\ntional conference on Knowledge discovery and data mining, pages 333–342.\nACM, 2010.\n[4] Manoranjan Dash and Huan Liu. Feature selection for classiﬁcation. Intel-\nligent data analysis, 1(3):131–156, 1997.\n[5] Liang Du and Yi-Dong Shen. Unsupervised feature selection with adaptive\nstructure learning.\nIn Proceedings of the 21th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data Mining, pages 209–\n218. ACM, 2015.\n[6] Jennifer G Dy and Carla E Brodley. Feature selection for unsupervised\nlearning. The Journal of Machine Learning Research, 5:845–889, 2004.\n[7] Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm,\ntheory, and applications. Pattern Analysis and Machine Intelligence, IEEE\nTransactions on, 35(11):2765–2781, 2013.\n[8] Xiaofei He, Ming Ji, Chiyuan Zhang, and Hujun Bao. A variance minimiza-\ntion criterion to feature selection using laplacian regularization. Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, 33(10):2013–\n2025, 2011.\n[9] Chenping Hou, Feiping Nie, Xuelong Li, Dongyun Yi, and Yi Wu. Joint\nembedding learning and sparse regression: A framework for unsupervised\nfeature selection. Cybernetics, IEEE Transactions on, 44(6):793–804, 2014.\n[10] D KOLLER. Toward optimal feature selection. In Proc. 13th International\nConference on Machine Learning, pages 284–292. Morgan Kaufmann, 1996.\n[11] J. Li, K. Cheng, S. Wang, F. Morstatter, R. Trevino, J. Tang, and H. Liu.\nFeature selection: A data perspective. 2016.\n[12] Zechao Li, Yi Yang, Jing Liu, Xiaofang Zhou, and Hanqing Lu. Unsu-\npervised feature selection using nonnegative spectral analysis. In AAAI,\n2012.\n[13] Mingxia Liu, Dan Sun, and Daoqiang Zhang. Sparsity score: A new ﬁlter\nfeature selection method based on graph. In Pattern Recognition (ICPR),\n2012 21st International Conference on, pages 959–962. IEEE, 2012.\n[14] Xinwang Liu, Lei Wang, Jian Zhang, Jianping Yin, and Huan Liu. Global\nand local structure preservation for feature selection. Neural Networks and\nLearning Systems, IEEE Transactions on, 25(6):1083–1095, 2014.\n19\n[15] Julien Mairal and Bin Yu. Supervised feature selection in graphs with path\ncoding penalties and network ﬂows.\nThe Journal of Machine Learning\nResearch, 14(1):2449–2485, 2013.\n[16] Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering:\nAnalysis and an algorithm. In NIPS, volume 14, pages 849–856, 2001.\n[17] Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on\nmutual information criteria of max-dependency, max-relevance, and min-\nredundancy. Pattern Analysis and Machine Intelligence, IEEE Transac-\ntions on, 27(8):1226–1238, 2005.\n[18] Marko Robnik-ˇSikonja and Igor Kononenko.\nTheoretical and empirical\nanalysis of relieﬀand rrelieﬀ. Machine learning, 53(1-2):23–69, 2003.\n[19] Qinbao Song, Jingjie Ni, and Guangtao Wang.\nA fast clustering-based\nfeature subset selection algorithm for high-dimensional data. Knowledge\nand Data Engineering, IEEE Transactions on, 25(1):1–14, 2013.\n[20] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal\nof the Royal Statistical Society. Series B (Methodological), pages 267–288,\n1996.\n[21] De Wang, Feiping Nie, and Heng Huang. Feature selection via global redun-\ndancy minimization. Knowledge and Data Engineering, IEEE Transactions\non, 27(10):2743–2755, 2015.\n[22] Xuerui Wang, Andrew McCallum, and Xing Wei. Feature selection with\nintegrated relevance and redundancy optimization. In Data Mining, 2015.\nICDM 2015. Fifteenth IEEE International Conference on, pages 697–702.\nIEEE, 2015.\n[23] Roger Weber, Hans-J¨org Schek, and Stephen Blott. A quantitative analysis\nand performance study for similarity-search methods in high-dimensional\nspaces. In VLDB, volume 98, pages 194–205, 1998.\n[24] Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou. l2, 1-\nnorm regularized discriminative feature selection for unsupervised learning.\nIn IJCAI Proceedings-International Joint Conference on Artiﬁcial Intelli-\ngence, volume 22, page 1589. Citeseer, 2011.\n[25] Chong You, Daniel Robinson, and Ren´e Vidal. Scalable sparse subspace\nclustering by orthogonal matching pursuit.\nIn Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 3918–3927,\n2016.\n[26] Lei Yu and Huan Liu. Eﬃcient feature selection via analysis of relevance\nand redundancy. The Journal of Machine Learning Research, 5:1205–1224,\n2004.\n20\n[27] Zheng Zhao, Lei Wang, Huan Liu, et al. Eﬃcient spectral feature selection\nwith minimum redundancy. In AAAI, 2010.\n[28] Zheng Zhao, Lei Wang, Huan Liu, and Jieping Ye. On similarity preserving\nfeature selection. Knowledge and Data Engineering, IEEE Transactions on,\n25(3):619–632, 2013.\n21\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-05-13",
  "updated": "2017-06-30"
}