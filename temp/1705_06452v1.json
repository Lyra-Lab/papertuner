{
  "id": "http://arxiv.org/abs/1705.06452v1",
  "title": "Delving into adversarial attacks on deep policies",
  "authors": [
    "Jernej Kos",
    "Dawn Song"
  ],
  "abstract": "Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples.",
  "text": "Workshop track - ICLR 2017\nDELVING INTO ADVERSARIAL ATTACKS\nON DEEP\nPOLICIES\nJernej Kos\nNational University of Singapore\nDawn Song\nUniversity of California, Berkeley\nABSTRACT\nAdversarial examples have been shown to exist for a variety of deep learning ar-\nchitectures. Deep reinforcement learning has shown promising results on training\nagent policies directly on raw inputs such as image pixels. In this paper we present\na novel study into adversarial attacks on deep reinforcement learning polices. We\ncompare the effectiveness of the attacks using adversarial examples vs. random\nnoise. We present a novel method for reducing the number of times adversarial ex-\namples need to be injected for a successful attack, based on the value function. We\nfurther explore how re-training on random noise and FGSM perturbations affects\nthe resilience against adversarial examples.\n1\nINTRODUCTION\nAdversarial examples have been shown to exist for a variety of deep learning architectures. They\nare small perturbations of the original inputs, often barely visible to a human observer, but carefully\ncrafted to misguide the neural network into producing incorrect outputs. Seminal work by Szegedy\net al. (2013) and Goodfellow et al. (2014), as well as much recent work, has shown that adversarial\nexamples are abundant and ﬁnding them is easy. Deep neural networks have been used in deep\nreinforcement learning (DRL) with promising results on training policies directly on raw inputs\nsuch as image pixels. One of the most successful algorithms for training deep policies is A3C (Mnih\net al., 2016), which enables asynchronous updates of policy weights, leading to an efﬁcient parallel\nimplementation. As the policies may drive various autonomous agents such as self-driving cars in\nthe real world, adversarial attacks may be of even greater importance.\nOur paper is among the ﬁrst to investigate adversarial examples on DRL policies, showing that these\ndeep policies are easily fooled by adversarial attacks with very small adversarial perturbations. In\naddition, in this paper, we examine three new dimensions about adversarial attacks on DRL policies\nthat no other work has not addressed before. First, we compare adversarial examples to random\nnoise and show that the former are an order of magnitude more effective for attacking DRL policies.\nAnother important dimension with DRL systems is time. If the attacker needs to inject adversarial\nperturbations less frequently, then the attack is easier to perform. To this end, we explore using\nthe policy’s value function as a guide for when to inject perturbations. Our experiments show that\nwith guided injection, the attacker can succeed with injecting perturbations in only a fraction of the\nframes, and is more effective than injecting perturbations with similar frequency but without the\nguidance. Our results show that adversarial attacks can be much more complex in the reinforcement\nlearning setting than other settings previously studied such as image classiﬁcation.\nThe third dimension is policy resilience through re-training. We present preliminary results showing\nthat the agents are able to become more resilient to fast-gradient sign method (FGSM) attack under\nre-training with both random noise and FGSM perturbations, while re-training with FGSM pertur-\nbations may be more effective than re-training with random noise. The re-trained agent may still be\nvulnerable to other attack methods such as optimization-based attacks, however, these other attack\nmethods are much slower to perform, often rendering the attacks extremely slow especially for the\nagent setting.\nConcurrently and independently from our work (submission to the same ICLR workshop), Huang\net al. (2017) also presented a study into adversarial attacks on DRL policies, showing when an\nattacker injects small adversarial perturbations into every frame, the learned agent will fail.\n1\narXiv:1705.06452v1  [stat.ML]  18 May 2017\nWorkshop track - ICLR 2017\nDue to space limit, we focus on agents trained on the Atari Pong task using the A3C algorithm\nand FGSM adversarial perturbations. Our work is a ﬁrst step towards better understanding of the\nchallenges and limitations of DRL under adversarial inputs.\n2\nSTUDY OBJECTIVES\nAttack Effectiveness of Adversarial Examples vs. Random Noise\nWe study how injecting ran-\ndom noise into the environment compares to injecting FGSM adversarial perturbations.\nUsing the Value Function to Guide Adversarial Perturbation Injection\nWe want to see if re-\nducing the frequency of adversarial perturbation injection can still generate an effective attack. We\nstudy three different methods: a) we only inject an adversarial perturbation every N frames and the\nintermediate frames are without any perturbation, b) we only recompute an adversarial perturbation\nevery N frames and inject the last computed perturbation in the intermediate frames; and c) we use\nthe value function, computed over the original input, in order to estimate when to inject the adver-\nsarial perturbation for it to be most effective, and only inject the adversarial perturbation when this\nestimate is above a certain threshold.\nEffectiveness of Re-training with Adversarial Examples and Random Noise\nWe study whether\nthe agents can be re-trained on an environment with injected random noise or adversarial perturba-\ntions in order to make them more resilient against further adversarial perturbations. Additionally, we\nstudy whether this obtained resilience transfers to environments with different magnitudes and dif-\nferent types of perturbations (e.g., is an agent trained on random noise any more resilient to FGSM\nadversarial perturbations).\n3\nEXPERIMENTAL EVALUATION\nTo perform our experiments, we use a TensorFlow implementation of the A3C (Mnih et al., 2016)\nalgorithm. We evaluate the method on the Atari Pong task, where the initial input image pixels are\ncropped and scaled to 42x42. Finally, luminosity is computed from RGB values, giving us frame\ndimensions of 42x42x1.\nTo generate adversarial perturbations, we use the fast gradient sign method (FGSM) initially devel-\noped by Goodfellow et al. (2014). FGSM requires a loss function J(θ, x, y) in order to compute its\ngradient ∇x. We use the cross-entropy loss between y (a vector of logits, representing weights for\neach action, produced by the policy) and the one-hot encoding of arg max y. This means that the\nattack attempts to generate an input which moves the policy output away from the optimal action.\nIn all our experiments, the agent is ﬁrst trained on a baseline (non-noisy) environment until it\nachieves an optimal reward for a number of episodes (baseline agent). Then, for generating the\nFGSM perturbations we set an appropriate ϵ and compute ϵ sgn ∇xJ(θ, x, y). For generating ran-\ndom noise, we sample from a uniform distribution Unif(0, β), where we set β based on the required\nintensity.\nAttack Effectiveness of Adversarial Examples vs. Random Noise\nThe baseline agent is eval-\nuated on a modiﬁed version of the environment, where either random noise or FGSM perturbation\nis injected on every frame. Figure 3 in Appendix shows the difference in attack effectiveness be-\ntween random noise and FGSM perturbations. While low levels of random noise (β ≤0.02) do\nnot impact the agent’s performance much, using random noise of greater magnitude (β ≥0.05)\nseverely degrades performance. FGSM adversarial perturbations are orders of magnitude more ef-\nfective than random noise for successful attacks, succeeding on attacking the baseline agent at much\nlower perturbation levels.\nUsing the Value Function to Guide Adversarial Perturbation Injection\nFirst, we explore how\nthe frequency of injecting adversarial perturbation affects attack success. In this experiment, we\neither inject FGSM perturbations only every tenth frame and use original frames in-between, or re-\ncompute perturbations every tenth frame and use the last computed perturbation in-between. All\n2\nWorkshop track - ICLR 2017\n0\n1000000\n2000000\n3000000\n4000000\n5000000\n6000000\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nAdversarial\nEvaluation\nFGSM (0.005), Skip 10\n0\n1000000\n2000000\n3000000\n4000000\n5000000\n6000000\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nAdversarial\nEvaluation\nFGSM (0.005), Skip 10 (reuse)\nFigure 1: Attack effectiveness when FGSM perturbations are only injected every 10th frame (left)\nand when the perturbations are only recomputed every 10th frame, but reused in the intermediate\nframes (right).\nexperiments were performed with ϵ set to 0.001. Our results hwo that only injecting FGSM pertur-\nbations on every tenth frame does not seem to be a particularly effective attack (Figure 1, left). On\nthe other hand, recomputing perturbations every tenth frame and reusing the previous perturbation\nin intermediate frames is equally effective as the original attack (Figure 1, right).\nWe also develop an attack method (VF) where we inject adversarial perturbations only when the\nvalue function, computed over the original frame, is above a certain threshold (in this experiment\nwe set the threshold to 1.4). The reasoning behind this is that we only want to disrupt the agent in\ncrucial moments, when it is close to achieving a reward. Figure 2 shows the effectiveness of this\nmethod, demonstrating that the VF attack method is very effective while only injecting adversarial\nperturbations in a fraction of the frames. We can compare the VF method against blindly injecting\nperturbations on every tenth frame (Figure 1, left). Even though both methods inject perturbations a\nsimilar number of times on average during one episode (120 for the VF method and 125 for the blind\nmethod), the VF method shows to be much more effective. This demonstrates that an attacker can\nuse the value function to conduct a more efﬁcient attack than the traditional attack method where\nthe adversarial perturbation is injected in every frame (as in (Huang et al., 2017)). This also shows\nthat adversarial attacks can be much more complex in the reinforcement learning setting than other\nsetting previously studied such as image classiﬁcation.\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nBaseline\n0\n5\n10\n15\n20\nValue Function\nRewards\n0\n1000000\n2000000\n3000000\n4000000\n5000000\n6000000\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nAdversarial\nEvaluation\nFGSM (0.005), VF Skip\nFigure 2: Policy’s value function approximation during one baseline episode (left). Effectiveness of\ninjecting FGSM perturbations only in frames where the value function is above a threshold (right).\nEffectiveness of Re-training with Adversarial Examples and Random Noise\nFinally, we also\nexplore if the agents can be re-trained to improve resilience to both random noise and FGSM adver-\nsarial perturbations. We additionally explore if this resilience then transfers to different magnitudes\nand types of perturbations. During these experiments, after the initial training in non-noisy envi-\nronment, the agent is ﬁrst allowed to re-train while we inject random noise or FGSM perturbations\non each frame. After the agent achieves good performance, it is then frozen and evaluated in a new\nnoisy environment, either with random noise or FGSM perturbations.\nFigure 4 in Appendix shows that in this setting, the baseline agent can be resilient against certain\nlevels of FGSM perturbations after re-training on a noisy environment for a number of episodes, with\nsufﬁcient level of random noise or FGSM perturbations added during re-training. More interestingly,\nour experiment shows that the re-trained agent is also resilient against FGSM perturbation of much\ngreater (or smaller) magnitude than the magnitude of the FGSM perturbations were used during\nre-training. We also visualize the actions predicted by the policy in image space (see Figure 5).\n3\nWorkshop track - ICLR 2017\nREFERENCES\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572, 2014.\nSandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks\non neural network policies. arXiv preprint arXiv:1702.02284, 2017.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. In International Conference on Machine Learning, 2016.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\nA\nAPPENDIX\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1e7\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRandom Noise\nEvaluation\nRandom Noise Evaluation (0.02)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1e7\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRandom Noise\nEvaluation\nRandom Noise Evaluation (0.05)\n0\n1000000\n2000000\n3000000\n4000000\n5000000\n6000000\n7000000\n8000000\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nAdversarial\nEvaluation\nFGSM Evaluation (0.001)\n0\n1000000\n2000000\n3000000\n4000000\n5000000\n6000000\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nAdversarial\nEvaluation\nFGSM Evaluation (0.005)\nFigure 3: Attack effectiveness of random noise with β values 0.02 and 0.05 (top) vs. attack effec-\ntiveness of FGSM adversarial perturbations with ϵ values 0.001 and 0.005 (bottom).\nA.1\nVISUALIZING THE POLICY NETWORK ACTION BOUNDARY\nWe further study how the action boundary looks like for the policy network and how re-training\naffects it. To this end, we prepare a visualization of predicted actions in image space (Figure 5). We\ngenerate the plot by deﬁning two normalized vectors, d1 and d2, spanning the input image space.\nThe one shown on the x-axis points in the direction of the generated adversarial perturbation (d1),\nwhile the other shown on the y-axis points in a randomly chosen orthogonal direction (d2). The\npoints in the plane represent actions predicted by the policy network for input x+ud1 +vd2, where\nx is the original image (a single frame). Since A3C is stochastic, we sample the predictions from\nthe policy network 7 times for each input and show the most common action. Each discrete action\n(action 0 to 5) is represented by its own color, shown in the ﬁgure on the far right. Values on the\naxes are the values of variables u and v.\nThe visualization shows that the decision space is fragmented. Small perturbations in the input can\ncause the optimal action chosen by the policy network to change drastically. Re-training under a\nnoisy environment (both random noise and adversarial FGSM perturbations) does not seem to make\n4\nWorkshop track - ICLR 2017\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1e7\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRe-training with\nrandom noise\nAdversarial\nEvaluation\nFGSM (0.001) After Random Noise Re-training (0.05)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n1e7\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRe-training with\nrandom noise\nAdversarial\nEvaluation\nFGSM (0.005) After Random Noise Re-training (0.1)\n0.0\n0.5\n1.0\n1.5\n2.0\n1e7\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRe-training with\nrandom noise\nAdversarial\nEvaluation\nFGSM (0.01) After Random Noise Re-training (0.1)\n0\n1000000\n2000000\n3000000\n4000000\n5000000\n6000000\n7000000\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRe-training\nwith FGSM\nAdversarial\nEvaluation\nFGSM (0.005) After FGSM Re-training (0.001)\n0.0\n0.5\n1.0\n1.5\n2.0\n1e7\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRe-training\nwith FGSM\nAdversarial\nEvaluation\nFGSM (0.01) After FGSM Re-training (0.005)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1e7\n30\n20\n10\n0\n10\n20\n30\nTraining on\nnon-noisy environment\nRe-training\nwith FGSM\nAdversarial\nEvaluation\nFGSM (0.001) After FGSM Re-training (0.005)\nFigure 4: Agent re-training experiments. Initially the agent was trained on a non-noisy environment.\nTop: After ﬁrst re-training with random noise (with β values 0.05 and 0.1). Bottom: After ﬁrst re-\ntraining with FGSM perturbations (with ϵ values 0.001 and 0.005). After re-training, the agent was\nevaluated on FGSM perturbations (with ϵ values 0.001, 0.005 and 0.01).\n0.075\n0.050\n0.025 0.000\n0.025\n0.050\n0.075\n0.08\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.075\n0.050\n0.025 0.000\n0.025\n0.050\n0.075\n0.08\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.075\n0.050\n0.025 0.000\n0.025\n0.050\n0.075\n0.08\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0\n1\n2\n3\n4\n5\nFigure 5: Visualization of actions in image space for a single frame. The x-axis is in the direction\nof the generated adversarial example (FGSM, ϵ = 0.001) for the target network. The y-axis is in\na random orthogonal direction. Each point is the result of sampling the policy network 7 times\ngiven the image at that point as input, and shows the action most commonly output by the network\n(different actions have different colors). Blue “x” marks the position of the original frame, while\nthe blue square marks the position of the adversarial example. Color bar on the far right shows the\nmapping of colors to discrete actions. Left: Baseline network without any re-training (action for the\noriginal input is action 5). Middle: Network with re-training on random noise (β = 0.1, action for\nthe original input is action 5). Right: Network with re-training on FGSM perturbations (ϵ = 0.005,\naction for the original input is action 0).\nthe decision boundary more smooth and the space seems to become even more fragmented (Figure 5\nmiddle and right).\nWe also adjust the visualization for action semantics. The reasoning behind this is that even though\nthe action space contains 6 valid actions, the actions are actually duplicated (e.g., multiple actions\nactually have the exact same effect on the environment). We manually checked the effect of each\naction on the environment and mapped the actions accordingly. The three actions are: noop (do\nnothing), move the paddle up and move the paddle down. Figure 6 shows that even when we adjust\nfor duplication, the space remains fragmented.\n5\nWorkshop track - ICLR 2017\n0.075\n0.050\n0.025 0.000\n0.025\n0.050\n0.075\n0.08\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.075\n0.050\n0.025 0.000\n0.025\n0.050\n0.075\n0.08\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.075\n0.050\n0.025 0.000\n0.025\n0.050\n0.075\n0.08\n0.06\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\nNoop\nMove Up\nMove Down\nFigure 6: Visualization of actions, adjusted for action semantics, in image space for a single frame.\nThe x-axis is in the direction of the generated adversarial example (FGSM, ϵ = 0.001) for the target\nnetwork. The y-axis is in a random orthogonal direction. Each point is the result of sampling the\npolicy network 7 times given the image at that point as input, and shows the action most commonly\noutput by the network (different actions have different colors, mapped based on action semantics).\nBlue “x” marks the position of the original frame, while the blue square marks the position of\nthe adversarial example. Color bar below shows the mapping of colors to discrete actions. Left:\nBaseline network without any re-training (action for the original input is “move down”). Middle:\nNetwork with re-training on random noise (β = 0.1, action for the original input is “move down”).\nRight: Network with re-training on FGSM perturbations (ϵ = 0.005, action for the original input is\n“noop”).\n6\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2017-05-18",
  "updated": "2017-05-18"
}