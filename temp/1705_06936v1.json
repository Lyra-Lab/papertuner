{
  "id": "http://arxiv.org/abs/1705.06936v1",
  "title": "Atari games and Intel processors",
  "authors": [
    "Robert Adamski",
    "Tomasz Grel",
    "Maciej Klimek",
    "Henryk Michalewski"
  ],
  "abstract": "The asynchronous nature of the state-of-the-art reinforcement learning\nalgorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes\nthem exceptionally suitable for CPU computations. However, given the fact that\ndeep reinforcement learning often deals with interpreting visual information, a\nlarge part of the train and inference time is spent performing convolutions. In\nthis work we present our results on learning strategies in Atari games using a\nConvolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0\nmachine learning framework. We also analyze effects of asynchronous\ncomputations on the convergence of reinforcement learning algorithms.",
  "text": "Atari games and Intel processors\nRobert Adamski, Tomasz Grel, Maciej Klimek and Henryk Michalewski\nIntel, deepsense.io, University of Warsaw\nRobert.Adamski@intel.com, T.Grel@deepsense.io, M.Klimek@deepsense.io,\nH.Michalewski@mimuw.edu.pl\nAbstract. The asynchronous nature of the state-of-the-art reinforce-\nment learning algorithms such as the Asynchronous Advantage Actor-\nCritic algorithm, makes them exceptionally suitable for CPU computa-\ntions. However, given the fact that deep reinforcement learning often\ndeals with interpreting visual information, a large part of the train and\ninference time is spent performing convolutions.\nIn this work we present our results on learning strategies in Atari games\nusing a Convolutional Neural Network, the Math Kernel Library and\nTensorFlow 0.11rc0 machine learning framework. We also analyze ef-\nfects of asynchronous computations on the convergence of reinforcement\nlearning algorithms.\nKeywords: reinforcement learning, deep learning, Atari games, asynchronous\ncomputations\n1\nIntroduction\nIn this work we approach the problem of learning strategies in Atari games\nfrom the hardware architecture perspective. We use a variation of the statistical\nmodel developed in [13,14]. Using the provided code1 our experiments are easy\nto re-create and we encourage the reader to draw his own conclusions about how\nCPUs perform in the context of Atari games. Following [7,13,14] we treat Atari\ngames as a key benchmark problem for modern reinforcement learning.\nWe use a statistical model consisting of approximately one million ﬂoating\npoint numbers which are iteratively updated using a gradient descent algorithm\ndescribed in [12].\nAt ﬁrst glance a training of such model appears as a relatively straightforward\ntask: a screen from the simulator is fed into the statistical model which decides\nwhich button must be pressed; over an episode of a game we estimate how the\nagent performs and calculate the loss accordingly and update the model so that\nthe loss is reduced.\nIn practice ﬁlling details of the above scenario is quite challenging. In this\nwork we accept a number of technical solutions presented in [13]. Our work also\n1 https://github.com/deepsense-io/BA3C-CPU\narXiv:1705.06936v1  [cs.DC]  19 May 2017\n2\nfollows closely research done in [5], where a batch version of [13] is analyzed. We\ndescribe our algorithmic decisions in considerable detail in Section 2.2.\nWe obtained state-of-the-art results in all tested games (see Section 6) and\nin the process of obtaining them we detected certain interesting issues described\nin Sections 2.3, 6.2 related to batch sizes, learning rates and the asynchronous\nlearning algorithm we use in this paper. The issues are illustrated by Figures 5\nand 6. Apparently our algorithm relies on timely emptying of queues. If queues\nare growing, then updates are delayed and learning performance degenerates\nup to the point where the trained agent goes back to an essentially a random\nbehavior. This in turn implies certain “preferred” sizes of batches as illustrated\nby Figure 8. Those batch sizes in turn imply “preferred” learning rates also visible\nin Figure 8.\nOur contribution can be considered as a snapshot of the CPU performance\nin the domain of reinforcement learning illustrating engineering opportunities\nand obstacles one can encounter relying solely on a CPU hardware. We also\ncontributed an integration of Google’s machine learning framework TensorFlow\n0.11rc0 with Intel’s Math Kernel Library (MKL). Details of the integration are\ndescribed in Section 5 and benchmarks comparing behavior of the out-of-the-\nbox TensorFlow 0.11rc0 with our modiﬁed version are included in Section 5.3.\nSection 3 contains a description of our hardware. Let us underline that we relied\non a completely standard Intel servers. Section 4 contains a brief characteristic\nof the MKL library and its currently available deep learning functionalities.\nThe learning times on our hardware described in Section 3 are very compet-\nitive (see Figure 7) and in a future work we are planning to bring it down to\nminutes using suﬃciently large CPU clusters.\n1.1\nRelated tools\nThis work would be impossible without a number of custom machine learning\nand reinforcement learning engineering tools. Our work is based on\n– OpenAI Gym [7], an open source machine learning platform allowing a very\neasy access to a rich library of games, including Atari games,\n– Google’s TensorFlow 0.11rc0, an open source machine learning framework\n[4] allowing for streamlined integration of various neural networks primitives\n(layers) implemented elsewhere,\n– Tensorpack, an open source library [23] implementing a very eﬃcient rein-\nforcement learning algorithm,\n– Intel’s Math Kernel Library 2017 (MKL) [19], a freely available library which\nimplemented neural networks primitives (layers) and overall speeds up ma-\ntrix and in particular deep learning computations on Intel’s processors.\n1.2\nRelated work\nRelation to [13]. Decisions in which we follow [13]. One of the key decisions is\nto run many independent agents in separate environments in an asynchronous\n3\nway. In the training process in every environment we play an episode of 2000\nsteps (the number may be smaller if the agent dies). An input to the statistical\nmodel consists of 4 subsequent screens in the RGB format. An output of the\nstatistical model is one of 18 possible moves of the controller. Over each episode\nthe agent generates certain reward. The reward allows us to estimate how good\nwere decisions made for every screen appearing during the episode. At every step\nan impact of the reward on decisions made earlier in the episode is discounted\nby a factor γ (0 < γ ≤1).\nHaving computed rewards for a given episode we can update weights of the\nmodel according to rewards — this is done through gradient updates which\nare applied directly to the statistical model weights. The updates are scaled\nby a learning rate λ. Authors of [13] reported good CPU performance and this\nencouraged the experiment described in this paper.\nDecisions left to readers of [13]. The key missing detail are all technical decisions\nrelated to communication between processes.\nRelation to [5] and [18]. Since the publication of [14] a signiﬁcant number of new\nresults was obtained in the domain of Atari games, however to the best of our\nknowledge only the works [5] and [18] were focused on the hardware performance.\nIn [5] authors modify the approach from [13] so it ﬁts better into the GPU multi-\ncore infrastructure. In this work we show that a similar modiﬁcation can be also\nquite helpful for the CPU performance. This work can be considered a CPU\nvariant of [5]. In [18] a signiﬁcant speedup of the A3C algorithm was obtained\nusing large CPU clusters. However, it is unclear if the method scales beyond the\ngame of Pong. Also the announcement [18] does not contain neither technical\ndetails or implementation.\nRelation to [22]. The fork of TensorFlow announced in [22] will oﬀer a much\ndeeper integration of TensorFlow and Intel’s Math Kernel Library (MKL). In\nparticular it should resolve the dimensionality issue mentioned in Section 5.4.\nHowever, at the moment of writing of this paper we had to do the integration\nof these tools on our own, because the fork mentioned in [22] was not ready for\nour experiments.\nOther references. The work [14] approaches the problem of learning a strategy\nin Atari games through approximation of the Q-function, that is implicitly it\nlearns a synthesized values of every move of a player in a given situation on the\nscreen. We did not consider this method, because of overall weaker results and\nmuch longer training times comparing to the asynchronous methods in [13].\nThe DeepBench [8], the FALCON Library [3] and the study [1] compare a\nperformance of CPU and GPU on neural network primitives (single convolutional\nand dense layers) as well as on a supervised classiﬁcation problem. Our article\ncan be considered a reinforcement learning variant of these works.\nA recently published work [20] shows a very promising CPU-only results for\nagent training tasks. The learning algorithm proposed in [20] is a novel approach\nwith yet untested stability properties. Our work focuses on a more established\n4\nfamily of algorithms with better understood theoretical properties and applica-\nbility tested on a broader class of domains.\nFor a broad introduction to reinforcement learning we refer the reader to\n[21]. For a historical background on Atari games we refer to [14].\n2\nThe Batch Asynchronous Advantage Actor Critic\nAlgorithm (BA3C)\nThe Advantage Actor Critic algorithm (A2C) is a reinforcement learning algo-\nrithm combining positive aspects of both policy-based and value function based\napproaches to reinforcement learning. The results reported recently by Mnih et\nal. in [13] provide strong arguments for using its asynchronous version (A3C).\nAfter testing several implementations of this algorithm we found that a high\nquality open source implementation of this algorithm is provided in the Tensor-\nPack (TP) framework [23]. However, the diﬀerences between this variant, which\nresembles an algorithm introduced in [5], and the one described originally in\n[13] are signiﬁcant enough to justify a new name. Therefore we will refer to this\nimplementation as the Batch Asynchronous Advantage Actor Critic (BA3C) al-\ngorithm.2\n2.1\nAsynchronous reinforcement learning algorithms\nAsynchronous reinforcement learning procedures are designed to use multiple\nconcurrent environments to speed up the training process. This leaves an issue\nhow the model or models are stored and synchronized between environments. We\ndiscuss some possible options in 2.2, including description of our own decisions.\nApart from obvious speedups resulting from utilizing concurrency, this ap-\nproach has also some statistical consequences. Usually in one environment the\nsubsequent states are highly correlated. This can have some adverse eﬀects on\nthe training process. However, when using multiple environments simultaneously,\nthe states in each environment are likely to be signiﬁcantly diﬀerent, thus decor-\nrelating the training points and enabling the algorithm to converge even faster.\n2.2\nBA3C – details of the implementation\nThe batch variant of the A3C algorithm was designed to better utilize mas-\nsively parallel hardware by batching data points. Multiple environments are still\nused, but there’s only one instance of the model. This forces the extensive use of\nthreading and message queues to decouple the part of the algorithm that gener-\nates the data from the one responsible for updates of the model. In a simple case\nof only one environment the BA3C algorithm consists of the steps described in\nalgorithm 1.\n2 In [5] is proposed a diﬀerent name GA3C derived from “hybrid CPU/GPU imple-\nmentation of the A3C algorithm”. This seems a bit inconvenient, because it suggests\na particular link between the batch algorithm and the GPU hardware; in this work\nwe obtain good results for a similar algorithm running only on CPU.\n5\nAlgorithm 1 Basic synchronous Reinforcement Learning scheme\n1: Randomly initialize the model.\n2: Initialize the environment.\n3: repeat\n4:\nPlay n episodes by using the current model to choose optimal actions.\n5:\nMemorize obtained states and rewards.\n6:\nUse the generated data points to train and update the model.\n7: until results are satisfactory.\nWhen using multiple environments one can follow a similar approach - each\nenvironment could simply use the global model to predict the optimal action\ngiven its current state. Let us notice that the model always performs prediction\non just a single data point from a single environment (i.e.: a single state vector\nof the environment). Obviously, this is far from optimal in terms of processing\nspeed. Also accessing the shared model from diﬀerent environments will quickly\nbecome a bottleneck. The two most popular approaches for solving this problem\nare:\n– Maintaining several local copies of the model (one for each environment)\nand synchronizing them with a global model. This approach is used and\nextensively described in [13,16,17] and we refer to it as A3C.\n– Using a single model and batching the predictions from multiple environ-\nments together (the “batch” variant, BA3C). This is much more suitable for\nuse on massively parallel hardware [5].\nThe batch variant requires using the following queues for storing data:\nTraining queue – stores the data points generated by the environments; the\ndata points are used in training. See Figure 1.\nFig. 1. Activities performed by the training thread. Please note that popping the data\nfrom the training queue may involve waiting until the queue has enough elements in it.\n6\nPrediction requests queue – stores the prediction requests made by the en-\nvironments; the predictions are made according to the current weights stored\nin the model. See Figure 2.\nPrediction results queue – stores the results of the predictions made by the\nmodel; the predictions are later used by the environments for choosing ac-\ntions. See Figure 3.\nFig. 2. Main loop of the prediction thread, which is responsible for evaluating the state\nof the environment and choosing the best action based on the current policy model.\nFig. 3. Main loop of a single environment thread. Usually multiple environment threads\nwill be working in parallel in order to generate the training data faster.\n7\nHyperparameters In Table 1 we list the most important hyperparameters of\nthe algorithm is presented.\nTable 1. Description of the hyperparameters of the algorithm.\nparameter\ndefault value description\nlearning rate\n0.001\nstep size for the optimization algo-\nrithm\nbatch size\n128\nnumber of training examples in a\ntraining batch\nframe history\n4\nthe number of consecutive frames to\ntake into consideration while evalu-\nating the current state of the game\nlocal time max\n5\nnumber of consecutive data points\nto memorize before concluding the\nepisode with a reward estimate\nbased on the output of the value\nnetwork\nimage size\n(84,84)\nthe size to which to rescale the orig-\ninal input into. This is done mainly\nbecause working on the original im-\nages is very expensive.\ngamma\n0.99\nthe discount factor\nConvNet architecture We made rather minor changes to the original Ten-\nsorPack ConvNet. The main focus of the changes was to better utilize the MKL\nconvolution primitives to enhance the performance. The architecture is presented\nin the diagram below.\nFig. 4. The structure of the Convolutional Neural Network used for processing the\ninput images\n8\n2.3\nEﬀects of asynchronism on convergence\nTraining and prediction part of the above described algorithm work in separate\nthreads and there’s a possibility that one of those parts will work faster than the\nother (in terms of data points processed per unit time). This is rarely an issue\nwhen the training thread is faster – in this case it’ll simply ﬁnd out that the\ntraining queue is empty and wait for a batch of training data to be generated.\nThis is ineﬃcient since the hardware is not fully utilized when the train thread\nis waiting for data, but it should not impact the correctness of the algorithm.\nA much more interesting case arises when data points are generated faster\nthan can be consumed by the training thread. If we’re using default ﬁrst-in-ﬁrst-\nout training queue and this queue is not empty, then there’s some delay between\nthe batch of data being generated by the prediction thread and it being used for\ntraining. It turns out that if this delay is large enough it will have detrimental\neﬀect on the convergence of the algorithm.\nWhen there’s a signiﬁcant delay between the generation of a batch and train-\ning on it, the training will be performed using a data point generated by an\nolder model. That is because when the batch of data was “waiting” in the train-\ning queue, other batches were used for training and the model was updated.\nThe number of such updates is equal to the size of the queue at the time when\nthis batch was generated. Therefore the updates are performed using out-of-date\ntraining data which may have little to do with the current policy maintained by\nthe current model.\nOf course when this delay is small and the learning rate is moderate the\ncurrent policy is almost equal to the “old” one used for generating the training\nbatch and the training process will converge. In other cases one should have\nmeans of constraining the delay to force correct behavior.\nThe solution is to restrict the size of the training queue. This way, when the\ntraining thread is generating too many training batches it will at some point\nreach the full capacity of the queue and will be forced to wait until some batch\nis popped. Usually the the size of the training queue is set to ensure that the\ntraining can take place smoothly. What we found out, however, is that setting\nthe queue capacity to extremely small values (i.e., less than ﬁve), has little if\nany impact on the overall training speed.\nImpact of delay on convergence – experiments This section describes a\nseries of experiments we’ve carried out in order to establish how big a delay in\nthe pipeline has to be to negatively impact the convergence. The setup involved\ninserting a ﬁxed size ﬁrst-in-ﬁrst-out buﬀer between the prediction and training\nparts of the algorithm. This buﬀer’s task was to ensure a predeﬁned delay in the\nalgorithm was present. With this modiﬁcation we were able to conduct a series\nof experiments for diﬀerent sizes of this buﬀer (delays). The results are shown\nbelow.\n9\n5\n0\n5\n10\n15\n20\n25\ndelay [batches]\n20\n0\n20\n40\n60\n80\n100\n120\n140\n160\nbest evaluation score\nEffect of delay on mean score in Atari Breakout (BA3C)\nFig. 5. Best evaluation results for experiments with diﬀerent artiﬁcial delays introduced\ninto the pipeline. For this experiment the default batch size of 128 was used. It seems\nthat even very small delays have a negative impact, while a delay of more than 10\nbatches (i.e.: 10 · 128 = 1280 data points when using the default batch size of 128) is\nenough to totally prevent the algorithm from convergence.\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\ntraining step [103]\n0\n20\n40\n60\n80\n100\n120\n140\nmean score\nMean scores for Atari Breakout for different delays\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\ndelay\nFig. 6. Mean scores for Atari breakout for diﬀerent delays. The plot shows the course of\nlearning for the artiﬁcial delays in the pipeline varying between 0 and 23, the brighter\nthe line, the more delay was introduced. It is visible that a delay greater than 10 can\nprevent the algorithm from successful convergence.\nBased on our results presented in the ﬁgures 5 and 6 we can conclude that\neven small delays have signiﬁcant impact on the results and delays of more than\n10\n10 batches (1280 data points) eﬀectively prevented the BA3C from converging.\nTherefore when designing an asynchronous RL algorithm it might be a good\nidea to try to streamline the pipeline as much as possible by making the queues\nas small as possible. This should not have signiﬁcant eﬀects on processing speed\nand can signiﬁcantly improve obtained results.\n3\nSpeciﬁcation of involved hardware\n3.1\nIntel Xeon® (Broadwell)\nWe used Intel Xeon® E5 2600 v4 processors to perform benchmarks tests of\nconvolutions. Xeon Broadwell is based on processor microarchitecture known\nas a “tick” [15] – a die shrink of an existing architecture, rather than a new\narchitecture. In that sense, Broadwell is basically a Haswell made on Intel’s\n14nm second generation tri-gate transistor process with few improvements to the\nmicro-architecture. Important changes are: up to 22 cores per CPU; support for\nDDR4 memory up to 2400 MHz; faster ﬂoating point instruction performance;\nimproved performance on large data sets. Results reported here are obtained on\na system with two Intel Xeon® Processor E5 2689 (3.10 GHz, 10 core) with 128\nGB of DDR4 2400MHz RAM, Intel Compute Module S2600TP and Intel Server\nChassis H2312XXLR2. The system was running Ubuntu 16.04 LTS operating\nsystem. The code was compiled with GCC 5.4.0 and linked against the Intel\nMKL 2017 library (build date 20160802).\n3.2\nIntel Xeon® (Haswell)\nIntel Xeon® E5 2600 v3 Processor, was used as base for series of experiments to\ntest hyperparameters of our algorithm. Haswell brings, along with new microar-\nchitecture, important features like AVX2. We used the Prometheus cluster with\na peak performance of 2.4 PFlops located at the Academic Computer Center\nCyfronet AGH as our testbed platform. Prometheus consists of more than 2,200\nservers, accompanied by 279 TB RAM in total, and by two storage ﬁle systems\nof 10 PB total capacity and 180 GB/s access speed. Experiments were performed\nin single-node mode, each node consisting of two Intel Xeon® E5-2680v3 pro-\ncessors with 24 cores at 2.5GHz with 128GB of RAM, with peak performance of\n1.07 TFlops.\nXeon Haswell CPU allows eﬀective computations of CNN algorithms, and\nconvolutions in particular, by taking advantage of SIMD (single instruction,\nmultiple data) instructions via vectorization and of multiple compute cores via\nthreading. Vectorization is extremely important as these processors operate on\nvectors of data up to 256 bits long (8 single-precision numbers) and can perform\nup to two multiply and add (Fused Multiply Add, or FMA) operations per\ncycle. Processors support Intel Advanced Vector Extensions 2.0 (AVX2) vector-\ninstruction sets which provide: (1) 256-bit ﬂoating-point arithmetic primitives,\n(2) Enhancements for ﬂexible SIMD data movements. These architecture-speciﬁc\n11\nadvantages have been implemented in the Math Kernel Library (MKL) and\nused in deep learning framework Caﬀe [9], [2] resulting in improved convolutions\nperformance.\n4\nThe MKL library\nThe Intel Math Kernel Library (Intel MKL) 2017 introduces a set of Deep Neural\nNetworks (DNN) [19] primitives for DNN applications optimized for the Intel ar-\nchitecture. The primitives implement forward and backward passes for the follow-\ning operations: (1) Convolution: direct batched convolution, (2) Inner product,\n(3) Pooling: maximum, minimum, and average, (4) Normalization: local response\nnormalization across channels and batch normalization, (5) Activation: recti-\nﬁed linear neuron activation (ReLU), (6) Data manipulation: multi-dimensional\ntransposition (conversion), split, concatenation, sum, and scale. Intel MKL DNN\nprimitives implement a plain C application programming interface (API) that\ncan be used in the existing C/C++ DNN frameworks, as well as in custom DNN\napplications.\n5\nChanges in TensorFlow 0.11rc0\n5.1\nMotivation\nPreliminary benchmarks showed that the vast majority of computation time dur-\ning training is spent performing convolutions. On CPU the single most expensive\noperation was the backward pass with respect to the convolution’s kernels, es-\npecially in the ﬁrst layers working on the largest inputs. Therefore signiﬁcant\nincreases in performance had to be achieved by optimizing the convolution op-\neration.\nWe considered the following approaches to this problem:\nTuning the current implementation of convolutions – TensorFlow (TF)\nuses the Eigen [10] library as a backend for performing matrix operations\non CPU. Therefore this approach would require performing changes in the\ncode of this library. The matrix multiplication procedures used inside Eigen\nhave multiple hyperparameters that determine the way in which the work\nis divided between the threads. Also, some rather strong assumptions about\nthe conﬁguration of the machine (e.g., its cache size) are made. This certainly\nleaves space for improvements, especially when optimizing for a very speciﬁc\nuse-case and hardware.\nProviding alternative implementation of convolutions – The MKL library\nprovides deep neural network operations optimized for the Intel architec-\ntures. Some tests of convolutions on a comparable hardware had already\nbeen performed by Baidu [8] and showed promising results. This also had\nthe added beneﬁt of leaving the original implementation unchanged thus\nmaking it possible for the user to decide which implementation (the default\nor the optimized one) to use.\n12\nWe decided to employ the second approach that involved using the MKL\nconvolution. A similar decision was taken also in the development of the Intel-\nfocused fork of TensorFlow [22].\n5.2\nImplementation\nTensorFlow provides a well documented mechanism for adding user-deﬁned op-\nerations in C++, which makes it possible to load additional operations as shared\nobjects. However, maintaining a build for a separate binary would make it harder\nto use some internal TF’s utilities and sharing code with the original convolution\noperation. Therefore we decided to fork the entire framework and provide the\nadditional operations.\nAnother TF’s feature called ’labels’ made it very simple to provide several\ndiﬀerent implementations of the same operation in C++ and choose between them\nfrom the python layer by specifying a ’label map’. This proved especially help-\nful while testing and benchmarking our implementation since we could quickly\ncompare it to the original implementation.\nThe implementation consisted of linking against the MKL library and provid-\ning the three additional operations: (1) MKL convolution forward pass, (2) MKL\nconvolution backpropagation w.r.t. the input feature map, (3) MKL convolution\nbackpropagation w.r.t. the kernels.\nThe code of these operations formed a glue layer between the TF’s and\nMKL’s programming interfaces. The computations were performed inside highly\noptimized MKL primitives.\n5.3\nBenchmark results\nTable 2. Forward convolution times [ms]. Notice that the MKL TF times are con-\nsistently smaller than the standard TF times. Data layout conversion times are not\nincluded in these measurements.\ninput size\nkernel size MKL TF\nTF\nPhi\nXeon Phi\nXeon\n128,84,84,16 16,32,5,5\n10.03 23.61 90.11 99.74\n128,40,40,32 32,32,5,5\n4.58\n8.76\n43.83 33.61\n128,18,18,32 32,64,5,5\n1.61\n2.71\n17.20 10.22\n128,7,7,64\n64,64,3,3\n0.88\n0.38\n3.50\n0.79\nMultiple benchmarks were conducted in order to assess the performance of our\nimplementation. They are focused on a speciﬁc 4-layer ConvNet architecture\nused for processing the Atari input images. The results are shown below.\n13\nTables 2, 3 and 4 show the benchmark results for the TensorFlow modi-\nﬁed to use MKL and standard TensorFlow. Measurements consist of the times\nof performing convolutions with speciﬁc parameters (input and ﬁlter sizes) for\nXeon® and Xeon Phi® CPUs. The same convolution parameters were used in\nthe convolutional network used in the atari games experiments.\nThe results show that the MKL convolutions can be substantially faster than\nthe ones implemented in TensorFlow. For some operations a speed-up of more\nthan 10 times was achieved. The results agree with the ones reported in [8]. It\nis also worth noticing that most of the time is spent in the ﬁrst layer which is\nresponsible for processing the largest images.\nTable 3. Backward data convolution times [ms]. TensorFlow times for the ﬁrst layer\nare not listed since computing the gradient w.r.t the input of the model is unnecessary.\ninput size\nkernel size MKL TF\nTF\nPhi\nXeon Phi\nXeon\n128,84,84,16 16,32,5,5\nN/A N/A N/A\nN/A\n128,40,40,32 32,32,5,5\n11.17 16.99 468.82 112.77\n128,18,18,32 32,64,5,5\n4.38\n4.55\n50.09\n9.74\n128,7,7,64\n64,64,3,3\n2.14\n0.77\n4.41\n1.22\nTable 4. Backward ﬁlter convolution times [ms]. Please note very long time spent in\nthe ﬁrst layer by the standard TensorFlow convolution. It was possible to reduce it\nmore than 10 times by using our implementation\ninput size\nkernel size MKL TF TF\nPhi Xeon Phi\nXeon\n128,84,84,16 16,32,5,5\n8.97 29.63 1,236.98 368.18\n128,40,40,32 32,32,5,5\n6.33 19.55 343.73\n114.72\n128,18,18,32 32,64,5,5\n2.52 6.07\n36.74\n28.82\n128,7,7,64\n64,64,3,3\n2.31 3.18\n7.38\n5.57\n5.4\nPossible improvements\nThe data layout can have a tremendous impact on performance of low-level array\noperations. In turn, eﬃciency of these operations is critical for performance of\nhigher-level machine learning algorithms.\n14\nTensorFlow and MKL have radically diﬀerent philosophies of storing visual\ndata. TensorFlow uses mostly its default “NHWC” format, in which pixels with\nthe same spatial location but diﬀerent channel indices are placed close to each\nother in memory. Some operations also provide the “NCHW” format widely\nused by other deep learning frameworks such as Caﬀe [11]. On the other hand\nMKL does not have a predeﬁned default format, rather it is designed to easily\nconnect MKL layers to one another. In particular, the same operation can require\ndiﬀerent data layouts depending on the sizes of its input (e.g. the number of\ninput channels). This is supposed to ensure that the number of intermediate\n“conversions” or “transpositions” in the pipeline is minimal, while at the same\ntime letting each operation use its preferred data layout.\nIt is important to note that our implementation provided an alternative\n“MKL” implementation only for the convolution. We did not provide similar\nalternatives for max pooling, ReLU etc. This forced us to repeatedly convert the\ndata between the TF’s NHWC format and the formats required by the MKL\nconvolution. Obviously this is not an optimal approach, however, implementing\nit optimally would most probably require signiﬁcant changes in the very heart\nof the framework – its compiler. This task was beyond the scope of the project,\nbut it’s certainly feasible and with enough eﬀort our implementation’s perfor-\nmance could be even further improved. The times necessary to perform data\nconversions are provided in the Table 5.\nTable 5. Data layout conversion times [ms].\ninput size\nkernel size Forward\nBWD Filter BWD data\nPhi\nXeon Phi\nXeon Phi\nXeon\n128,84,84,16 16,32,5,5\n37.44 12.20 14.55 11.70 N/A N/A\n128,40,40,32 32,32,5,5\n3.30\n2.92\n5.32\n4.34\n6.18 4.14\n128,18,18,32 32,64,5,5\n2.31\n0.58\n4.32\n0.62\n5.89 0.68\n128,7,7,64\n64,64,3,3\n1.96\n0.15\n11.48 0.56\n2.59 0.24\n6\nResults\n6.1\nGame scores and overall training time\nBy using the custom convolution primitives from the MKL library it was possible\nto increase the training speed by a factor of 3.8 (from 151.04 examples/s to\n517.12 examples/s). This made it possible to train well performing agents in\nunder 24 hours. As a result, novel concepts and improvements to the algorithm\ncan now be tested more quickly, possibly leading to further advances in the ﬁeld\nof reinforcement learning. The increase in speed was achieved without hurting\n15\nthe results obtained by the agents trained. Example training curves for 3 diﬀerent\ngames are presented in the Figure 7.\n 0\n 50\n 100\n 150\n 200\n 250\n 300\n 350\n 400\n 450\n 0\n 5\n 10\n 15\n 20\n 25\nscore\ntime [h]\nBreakout\n-25\n-20\n-15\n-10\n-5\n 0\n 5\n 10\n 15\n 20\n 0\n 5\n 10\n 15\n 20\n 25\nscore\ntime [h]\nPong\n 100\n 200\n 300\n 400\n 500\n 600\n 700\n 800\n 0\n 5\n 10\n 15\n 20\n 25\nscore\ntime [h]\nSpace Invaders\nFig. 7. Mean score for 50 consecutive games vs training time for the best model ob-\ntained for atari Breakout, Pong and Space Invaders.\n6.2\nBatch size and learning rate tuning\nUsing the previously described pipeline optimized for better CPU performance\nwe conducted a series of experiments designed to determine the optimal batch\nsize and learning rate hyperparameters. The experiments were performed using\nthe random search method [6]. For each hyperparameter its value was drawn\nfrom a loguniform distribution deﬁned on a range [10−4, 10−2] for learning rate\nand [21, 210] for batch size. Overall, over 200 experiments were conducted in this\nmanner for 5 diﬀerent games. The results are presented in the ﬁgures 8,9 below.\nIt appears that for the 5 games tested one could choose a combination of learning\nrate and batch size that would work reasonably well for all of them. However,\nthe optimal settings for speciﬁc games seem to diverge.\nAs one could expect when using large batch sizes, better results were ob-\ntained with greater learning rate’s. This is most probably caused by the stabi-\nlizing eﬀects of bigger batch sizes on the mean gradient vector used for training.\nFor smaller batch sizes using the same learning rate would cause instabilities,\nimpeding the training process.\nOverall, batch size of around 32 a learning rate of the order of 10−4 seems\nto have been a general good choice for the games tested. The detailed listing of\nthe best results obtained for each game is presented in the Table 6.\n16\nTable 6. Mean scores and hyperparameters obtained for the best models for each game\ngame\nlearning rate batch size score mean score max\nBreakout-v0\n0.00087\n22\n390.28\n654\nPong-v0\n0.00017\n19\n16.64\n21\nRiverraid-v0\n0.00024\n87\n10,018.40\n11570\nSeaquest-v0\n0.00160\n162\n1,823.41\n1840\nSpaceInvaders-v0\n0.00032\n14\n764.70\n2000\n101\n102\n103\nbatch size\n10-3\nlearning rate\nRiverraid\nSeaquest\nPong\nBreakout\nSpaceInvaders\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFig. 8. Overall results of the random search for all the games tested. The brighter the\ncolor the better the result for a given game. Color value 1 means the best score for the\ngame, color value 0 means the worst result for the given game.\n17\n101\n102\n103\n10-3\nlearning rate\nRiverraid\n101\n102\n103\n10-3\nSeaquest\n101\n102\n103\n10-3\nPong\n101\n102\n103\nbatch size\n10-3\nlearning rate\nBreakout\n101\n102\n103\nbatch size\n10-3\nSpaceInvaders\nFig. 9. Results of random search for each game separately. Brighter colors mean better\nresults.\n7\nConclusions and further work\nPreliminary results contained in this work can be considered as a next step in\nreducing the gap between CPU and GPU performance in deep learning appli-\ncations. As shown in this paper, in the area of reinforcement learning and in\nthe context of asynchronous algorithms, CPU-only algorithms already achieve a\nvery competitive performance.\nAs the most interesting future research direction we perceive extending re-\nsults of [18] and tuning of performance of asynchronous reinforcement learning\nalgorithms on large computer clusters with the idea of bringing the training time\ndown from hours to minutes.\nConstructing a compelling experiment for the Xeon Phi® platform also seems\nto be an interesting challenge. Our current approach would require a signiﬁcant\nmodiﬁcation because of much slower single core performance of Xeon Phi®.\nHowever, preliminary results on the Pong game are quite promising with a state-\nof-the-art results obtained in 12 hours on a single Xeon Phi® server.\n18\nReferences\n1. Intel Xeon Phi delivers competitive performance for deep learning—and get-\nting\nbetter\nfast\n(Dec\n2016),\nhttps://software.intel.com/en-us/articles/\nintel-xeon-phi-delivers-competitive-performance-for-deep-learning-\nand-getting-better-fast\n2. Caﬀe\noptimized\nfor\nIntel\narchitecture:\nApplying\nmodern\ncode\ntechniques\n(Feb\n2017),\nhttps://software.intel.com/en-us/articles/caffe-optimized-\nfor-intel-architecture-applying-modern-code-techniques\n3. FALCON Library: Fast Image Convolution in Neural Networks on Intel architec-\nture (Feb 2017), https://colfaxresearch.com/falcon-library/\n4. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\nJ., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\nViégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\nhttp://tensorflow.org/, software available from tensorﬂow.org\n5. Babaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., Kautz, J.: GA3C: gpu-\nbased A3C for deep reinforcement learning. CoRR abs/1611.06256 (2016), http:\n//arxiv.org/abs/1611.06256\n6. Bergstra,\nJ.,\nBengio,\nY.:\nRandom\nsearch\nfor\nhyper-parameter\noptimiza-\ntion. J. Mach. Learn. Res. 13(1), 281–305 (Feb 2012), http://dl.acm.org/\ncitation.cfm?id=2503308.2188395\n7. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,\nZaremba, W.: Openai gym. CoRR abs/1606.01540 (2016), http://arxiv.org/abs/\n1606.01540\n8. Duan, Y., Chen, X., Houthooft, R., Schulman, J., Abbeel, P.: Benchmarking deep\nreinforcement learning for continuous control. CoRR abs/1604.06778 (2016), http:\n//arxiv.org/abs/1604.06778\n9. Dubey, P.: Myth busted: General purpose CPUs can’t tackle deep neural network\ntraining (Jun 2016), https://itpeernetwork.intel.com/myth-busted-general-\npurpose-cpus-cant-tackle-deep-neural-network-training/\n10. Guennebaud, G., Jacob, B., et al.: Eigen v3. http://eigen.tuxfamily.org (2010)\n11. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.B., Guadar-\nrama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.\nCoRR abs/1408.5093 (2014), http://arxiv.org/abs/1408.5093\n12. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR\nabs/1412.6980 (2014), http://arxiv.org/abs/1412.6980\n13. Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T.P., Harley, T., Silver, D.,\nKavukcuoglu, K.: Asynchronous methods for deep reinforcement learning. CoRR\nabs/1602.01783 (2016), http://arxiv.org/abs/1602.01783\n14. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G.,\nGraves, A., Riedmiller, M.A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C.,\nSadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis,\nD.: Human-level control through deep reinforcement learning. Nature 518(7540),\n529–533 (2015), http://dx.doi.org/10.1038/nature14236\n15. Mulnix,\nD.:\nIntel\nxeon\nprocessor\ne5-2600\nv4\nproduct\nfamily\ntechni-\ncal overview (Jan 2017), https://software.intel.com/en-us/articles/intel-\nxeon-processor-e5-2600-v4-product-family-technical-overview\n19\n16. Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., Maria, A.D., Pan-\nneershelvam, V., Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V.,\nKavukcuoglu, K., Silver, D.: Massively parallel methods for deep reinforcement\nlearning. CoRR abs/1507.04296 (2015), http://arxiv.org/abs/1507.04296\n17. Niu, F., Recht, B., Re, C., Wright, S.J.: HOGWILD!: A Lock-Free Approach to\nParallelizing Stochastic Gradient Descent. ArXiv e-prints (Jun 2011)\n18. Mark\nO’Connor:\nDeep\nLearning\nEpisode\n4:\nSupercomputer\nvs\nPong\nII\n(Oct 2016), https://www.allinea.com/blog/201610/deep-learning-episode-4-\nsupercomputer-vs-pong-ii\n19. Pirogov,\nV.:\nIntroducing\nDNN\nprimitives\nin\nIntel\nMath\nKernel\nLibrary\n(Mar 2017), https://software.intel.com/en-us/articles/introducing-dnn-\nprimitives-in-intelr-mkl\n20. Salimans, T., Ho, J., Chen, X., Sutskever, I.: Evolution strategies as a scalable alter-\nnative to reinforcement learning (Mar 2017), https://arxiv.org/abs/1703.03864\n21. Sutton, R.S., Barto, A.G.: Reinforcement learning - an introduction. Adaptive\ncomputation and machine learning, MIT Press (1998), http://www.worldcat.org/\noclc/37293240\n22. Ould-ahmed vall, E.: Optimizing Tensorﬂow on Intel architecture for AI ap-\nplications (Mar 2017), https://itpeernetwork.intel.com/tensorflow-intel-\narchitecture-ai/\n23. Wu, Y.: Tensorpack. https://github.com/ppwwyyxx/tensorpack (2016)\n",
  "categories": [
    "cs.DC",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2017-05-19",
  "updated": "2017-05-19"
}