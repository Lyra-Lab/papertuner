{
  "id": "http://arxiv.org/abs/1705.07366v1",
  "title": "Forward Thinking: Building Deep Random Forests",
  "authors": [
    "Kevin Miller",
    "Chris Hettinger",
    "Jeffrey Humpherys",
    "Tyler Jarvis",
    "David Kartchner"
  ],
  "abstract": "The success of deep neural networks has inspired many to wonder whether other\nlearners could benefit from deep, layered architectures. We present a general\nframework called forward thinking for deep learning that generalizes the\narchitectural flexibility and sophistication of deep neural networks while also\nallowing for (i) different types of learning functions in the network, other\nthan neurons, and (ii) the ability to adaptively deepen the network as needed\nto improve results. This is done by training one layer at a time, and once a\nlayer is trained, the input data are mapped forward through the layer to create\na new learning problem. The process is then repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new dataset, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. In the case where the neurons of deep neural nets are\nreplaced with decision trees, we call the result a Forward Thinking Deep Random\nForest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the\nMNIST dataset. We also provide a general mathematical formulation that allows\nfor other types of deep learning problems to be considered.",
  "text": "Forward Thinking: Building Deep Random Forests\nKevin Miller, Chris Hettinger, Jeffrey Humpherys, Tyler Jarvis, and David Kartchner\nDepartment of Mathematics\nBrigham Young University\nProvo, Utah 84602\nmillerk5@byu.edu, hettinger@math.byu.edu, jeffh@math.byu.edu,\njarvis@math.byu.edu, david.kartchner@math.byu.edu\nAbstract\nThe success of deep neural networks has inspired many to wonder whether other\nlearners could beneﬁt from deep, layered architectures. We present a general frame-\nwork called forward thinking for deep learning that generalizes the architectural\nﬂexibility and sophistication of deep neural networks while also allowing for (i)\ndifferent types of learning functions in the network, other than neurons, and (ii)\nthe ability to adaptively deepen the network as needed to improve results. This is\ndone by training one layer at a time, and once a layer is trained, the input data are\nmapped forward through the layer to create a new learning problem. The process\nis then repeated, transforming the data through multiple layers, one at a time,\nrendering a new dataset, which is expected to be better behaved, and on which a\nﬁnal output layer can achieve good performance. In the case where the neurons\nof deep neural nets are replaced with decision trees, we call the result a Forward\nThinking Deep Random Forest (FTDRF). We demonstrate a proof of concept by\napplying FTDRF on the MNIST dataset. We also provide a general mathematical\nformulation, called Forward Thinking that allows for other types of deep learning\nproblems to be considered.\n1\nIntroduction\nClassiﬁcation and regression trees are a fast and popular class of methods for supervised learning.\nFor example, random forests [2], extreme gradient boosted trees [3], and conditional trees [7] have\nconsistently performed well in several benchmarking studies where various methods compete against\neach other [17, 18, 12].\nIn recent years, however, deep neural networks (DNNs) have become a dominant force in several areas\nof supervised learning, most notably in image, speech, and natural language recognition problems,\nwhere deep learning methods are also consistently beating humans [9, 16]. Although the use of\nmultiple layers of neurons in a “deep” architecture has been well-known for many years [11], it wasn’t\nuntil the discovery of feasible means of training via backpropagation that neural networks became\nsuccessful. However, DNNs still suffer from a variety of problems. In particular, it is extremely\nexpensive computationally to use backpropagation to train multiple layers of nonlinear activation\nfunctions [10]. This not only requires lengthy training, but also uses large quantities of memory,\nmaking the training of medium-to-large networks infeasible on a single CPU. Moreover, DNNs are\nhighly prone to overﬁtting and thus require both large amounts of training data and careful use of\nregularization to generalize effectively. Indeed, the computational resources required to fully train\na DNN are in many cases orders of magnitude more than other machine learning methods such as\ndecision tree methods which perform almost as well on many tasks and even better on other tasks\n[12]. In other words, a lot of work is required to get at best only slightly better performance using\nDNNs.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1705.07366v1  [stat.ML]  20 May 2017\nIn spite of these drawbacks, DNNs have out-performed simpler structures in a number of machine\nlearning tasks, with authors citing the use of “deep” architectures as a necessary element of their\nsuccess [5]. By stacking dozens of layers of weak learners (neurons), DNNs can capture the intricate\nrelationships necessary to effectively solve a wide variety of problems. Accordingly, we propose a\ngeneralization of the DNN architecture where neurons are replaced by other classiﬁers. In this paper,\nwe consider networks where each layer is a type of random forest, with neurons composed of the\nindividual decision trees and show how such networks can be quickly trained layer-by-layer instead\nof paying the high computational cost of training a DNN all at once.\nRandom forests [2, 14] use ensembling of bootstrapped decision trees as weak classiﬁers, reporting\nthe average or maximum value across all of the trees’ outputs for classiﬁcation probabilities and\nregression values. In [13], Liu notes that variety in individual weak learners is essential to the success\nof ensemble learning. Accordingly, we use a combination of random decision trees and extra random\ntrees [4] in each layer to increase variety and thus improve performance. We create both the random\ndecision trees and extra random trees using the implementations provided in Scikit-Learn [15].\nIt is important to note that Zhou and Feng [22] very recently posted a related idea called gcForest,\nwhere the layers of the deep architecture are comprised of multiple random forests. In their network,\nthe connections to subsequent layers are the outputs of random forests, whereas in our paper the\noutputs of the individual decision trees are passed to subsequent layers of decision trees. In other\nwords, they pass the results of the random forest through to the next layer (4 full random forests, each\nconsisting of 1, 000 decision trees), whereas we pass the results of 2, 000 individual decision trees\nforward. We get comparable results with less trees, but a higher memory requirement given that we\nare mapping more data to the next layer. In particular with the MNIST dataset, they have an accuracy\nof 98.96% and we get an essentially equivalent accuracy of 98.98%.\nIn both our work and Zhou and Feng’s work, these decision tree networks can be trained efﬁciently\nwithout the use of backpropagation. Each layer remains static once trained, and so the training data\ncan be pushed through to train the next layer. Hence, the training time for a multi-layer forest should\nbe much faster than training time for a traditional DNN architecture. We note that in a companion\npaper [6] , that we can also train a DNN in a similar fashion, without the use of backpropagation,\nthus also speeding up the training process.\nThe results of both gcForest and our study are convincing, and we believe that both papers conﬁrm\nthe validity of exploring deep architectures with decision trees and random forests.\nIn Section 2, we give a careful description of the general architecture for a forward thinking deep\nnetwork. In Section 3 we describe how the general theory is applied in the speciﬁc case of a\nForwarding Thinking Deep Random Forest (FTDRF). Details relating to the processing of data and\nthe experimental results are in the subsequent sections.\n2\nMathematical description of forward thinking\nThe main idea of forward thinking is that neurons can be generalized to any type of learner and then,\nonce trained, the input data are mapped forward through the layer to create a new learning problem.\nThe process is then repeated, transforming the data through multiple layers, one at a time, rendering a\nnew data set, which is expected to be better behaved, and on which a ﬁnal output layer can achieve\ngood performance.\nThe input layer\nThe data D(0) = {(x(0)\ni , yi)}N\ni=1 ⊂X (0) × Y are given as the set of input values x(0)\ni\nfrom a set X (0)\nand their corresponding outputs yi in a set Y.\nIn many learning problems, X (0) ⊂Rd, which means that there are d real-valued features. If the\ninputs are images, we can stack them as large vectors where each pixel is a component. In some deep\nlearning problems, each input is a stack of images. For example, color images can be represented as\nthree separate monochromatic images, or three separate channels of the image.\nFor binary classiﬁcation problems, the output space can be taken to be Y = {−1, 1}. For multi-class\nproblems we often set Y = {1, 2, . . . , K}.\n2\nThe ﬁrst hidden layer\nLet C(1) = {C(1)\n1 , C(1)\n2 , . . . , C(1)\nm1} be a set of m1 learning functions, C(1)\ni\n: X (0) →Z(1)\ni\n, for\nsome codomain Z(1)\ni\nwith parameters θ(1)\ni\n. This layer of learning functions (or learners) can be\nregression, classiﬁcation, or kernel functions and can be thought of as deﬁning new features. Let\nX (1) = Z(1)\n1\n× Z(1)\n2\n× · · · × Z(1)\nm1 and transform the inputs {x(0)\ni }N\ni=1 ⊂X (0) to X (1) according to\nthe map\nx(1)\ni\n= (C(1)\n1 (x(0)\ni ), C(1)\n2 (x(0)\ni ), . . . , C(1)\nm (x(0)\ni )) ⊂X (1),\ni = 1, . . . , N.\nThis gives a new data set D(1) = {(x(1)\ni , yi)}N\ni=1 ⊂X (1) × Y.\nIn many learning problems Z(1) = [−1, 1], in which case the new domain X (1) is a hypercube\n[−1, 1]m1. It is also common for Z(1) = [0, ∞), in which case X (1) is the m1-dimensional orthant\n[0, ∞)m1.\nThe goal is to choose C(1) to make the new dataset “more separable,” or better-behaved, than\nthe previous dataset. As we repeat this process iteratively, the data should become increasingly\nbetter-behaved so that in the ﬁnal layer, a single learner can ﬁnish the job.\nAdditional hidden layers\nLet C(ℓ) = {C(ℓ)\n1 , C(ℓ)\n2 , . . . , C(ℓ)\nmℓ} be a set (layer) of mℓlearning functions C(ℓ)\ni\n: X (ℓ−1) →Z(ℓ).\nThis layer is again trained on the data D(ℓ−1) = {(x(ℓ−1)\ni\n, yi)}. This would usually be done in the\nsame manner as the previous layer, but it need not be the same; for example, if the new layer consists\nof different kinds of learners, then the training method for the new layer might also need to differ.\nAs with the ﬁrst layer, the inputs {x(ℓ−1)\ni\n}N\ni=1 ⊂X (ℓ−1) = Z(ℓ−1)\n1\n× Z(ℓ−1)\n2\n× · · · × Z(ℓ−1)\nmℓ−1 are\ntransformed to a new domain {x(ℓ)\ni }N\ni=1 ⊂X (ℓ) = Z(ℓ)\n1\n× Z(ℓ)\n2\n× · · · × Z(ℓ)\nmℓaccording to the map\nx(ℓ)\ni\n= (C(ℓ)\n1 (x(ℓ−1)\ni\n), C(ℓ)\n2 (x(ℓ−1)\ni\n), . . . , C(ℓ)\nmℓ(x(ℓ−1)\ni\n)),\ni = 1, . . . , N.\nThis gives a new dataset D(ℓ) = {(x(ℓ)\ni , yi)}N\ni=1 ⊂X (ℓ) × Y, and the process is repeated.\nFinal layer\nAfter passing the data through the last hidden layer, we train the ﬁnal layer, which consists of a single\nlearning function CF : X (n) →Y on the data set D(n) = {(x(n)\ni\n, yi)}N\ni=1 ⊂X (n) × Y to determine\nthe outputs, where CF (x(n)\ni\n) is expected to be close to yi for each i.\nRemark 1 While in this paper we have applied the multi-layer architecture of neural networks to\ndecision trees in random forests, we note that this can be generalized to other types of classiﬁers.\nWhere the decision trees in our architecture are analogous to the neurons in a DNN, other classiﬁers\nsuch as SVMs, gradient boosted trees, etc., should be able to be substituted for neurons in a similar\nfashion.\n3\nForward thinking deep random forest architecture\nIn this section we describe the method of construction for layers of the Forwarding Thinking Deep\nRandom Forest (FTDRF) architecture. We note the similarities to the routine termed CascadeForest\nin [22] and address these in this section.\n3.1\nMultilayer random forests\nUsing the notation of the previous section, we have training data D(0) = {x(0)\ni , yi}N\ni=1 (inputs and\nlabels), where x(0)\ni\nare feature vectors and yi ∈{1, 2, . . . , K} are the corresponding labels. An\nFTDRF consists of multiple layers C(1), . . . , C(n) of classiﬁers, where each layer C(ℓ) consists of a\nforest, comprised of a blend of random and extra random trees.\n3\nThe output of each individual tree is a vector of class probabilities, as determined by the distribution\nof classes present in the leaf node into which the sample is sorted. Speciﬁcally, given any decision\ntree, each leaf of the tree is assigned a vector of class probabilities, p = (p1, . . . , pK), corresponding\nto the proportion of training data assigned by the tree to the leaf in each class.\nEach layer C(ℓ) is trained on the data D(ℓ−1) = {x(ℓ−1)\ni\n, yi}, and x(ℓ)\ni\nis the result of pushing the\ninput x(ℓ−1)\ni\nthrough that layer. Speciﬁcally, for each input x(ℓ−1)\ni\n, the output of tree j in layer ℓis\na probability vector p(ℓ)\nj (x(ℓ−1)\ni\n) = (p(ℓ)\nj,1(x(ℓ−1)\ni\n), . . . , p(ℓ)\nj,K(x(ℓ−1)\ni\n)). And these are concatenated\ntogether at each layer, so that for each input x(ℓ−1)\ni\n, the output of layer C(ℓ) is an mℓ-tuple of\nprobability vectors, where mℓis the number of trees in layer ℓ.\nAll such outputs for all trees in the layer are concatenated together to be the output of the layer for the\ngiven sample. This is done for all of the training data, hence transforming the data to be of dimension\nK × mℓ, where K is the number of classes for the training dataset and mℓis the number of trees in\nthe current layer.\nThe outputs of each layer become the inputs to the next, until the data have been mapped through\nthe ﬁnal layer C(n). The ﬁnal class prediction is made by averaging all the class probability output\nvectors from the mn decision trees in C(n), and predicting the class with the highest probability. One\ncould, of course, use any classiﬁer to ﬁnd an optimal combination of the weights for the ﬁnal layer,\nbut we do not explore this possibility in this paper.\nFigure 1: Forward Thinking Deep Random Forest (FTDRF)\n3.2\ngcForest comparison\nUnlike Zhou and Feng’s architecture for gcForest, our deep architecture of decision trees only requires\nthe previous layer’s output. In [22], each layer passes both the class probabilities predicted by the\nrandom forests (not the individual decision trees) and the original data to each subsequent layer.\nOur model, on the other hand, passes only the output of the previous layer of individual decision\ntrees to the next layer, to reduce the spatial complexity of network training and testing. Moreover,\nFTDRF seems to need fewer trees in each layer. For example, in our FTDRF described in Section\n5 we obtained results comparable to [22] on MNIST, but we use only 2, 000 decision trees in each\nlayer, whereas [22] uses 4 random forests of 1, 000 trees each (or 4, 000 trees per layer). Another\ndistinction is that our ﬁnal routine uses information gain entropy to calculate node splits, whereas\ngcForest implements gini impurity. We also ran some tests with gini impurity to determine node\nsplits, but found that entropy usually performed better.\n3.3\nHalf-half random forest layers\nAs is standard in random forests, a node split in a given decision tree is determined from a random\nsubset containing\n√\nd features of the input data passed to the layer. In a given layer, the collection of\ndecision trees representing the layer contains both random decision trees, as well as extra random\n4\ntrees to introduce more variety into the layer. This is similar to the layers of [22], where of the 4\nrandom forests in a given layer, 2 of them are completely random forests [13], closely related to extra\nrandom forests. An extra random forest increases tree randomization by choosing a random splitting\nvalue for each of the\n√\nd features subset to determine the node split. In our scheme, we randomly\nassign trees to be of this type based on a Bernoulli draw of p = 0.5.\n3.4\nAdding layers\nAn advantage of forward thinking is that the total number of layers is determined by the data, rather\nthan by a human designer. In the case of FTDRF, the choice of whether to create a new layer or\nterminate is determined by a cross validation scheme. After each layer is constructed, we evaluate the\naccuracy on a holdout sample consisting of 20% of the training data to determine the relative gain\nproduced by the last added layer. If the layer meaningfully increased the validation accuracy (i.e.,\nthe relative gain is above a chosen threshold), then we proceed and add another layer to the FTDRF.\nOnce the relative gain of a new layer falls below the threshold, we stop adding new layers and obtain\npredictions via the ﬁnal layer of our network. For our network, we chose a relative gain threshold of\n1%. Some results are shown in Table 1, below.\nWe note that the trees in the layers of our speciﬁc implementation here were not created using boosting\n(e.g. XGBoost [3]), but we expect that doing so could be beneﬁcial and possibly lead to increased\naccuracy.\n4\nPreprocessing of image data\nThe decision tree structure of FTDRF requires sufﬁcient training data to avoid overﬁtting in the\nﬁrst few layers. The state-of-the-art algorithms for dealing with image data in classiﬁcation use\npreprocessing and transforming techniques, such as convolutions. Accordingly, we experimented\nwith two of these techniques for FTDRF: a single-pixel “wiggle” and multi-grained scanning (MGS).\n4.1\nSingle-pixel wiggle\nFor the data set used here, we augmented the training data by a single pixel “wiggle” technique.\nThat is, for each training image in the MNIST training set, we include copies of the images shifted\naround in 4 diagonal directions (up-left, up-right, down-left, and down-right) by one pixel, see\nFigure 2. This data augmentation yields the results seen in Table 1. A further way to augment the\nfeature representation of the images is presented in the following Section 4.2, via a routine called\nMulti-Grained Scanning [22].\nFigure 2: Single pixel wiggle visualization\n4.2\nMulti-grained scanning (MGS)\nIn [22], a scheme similar to convolution is proposed, termed Multi-Grained Scanning (MGS), which\nwe implemented for the FTDRF architecture. We use the exact same process that Zhou and Feng do\nin their MGS scheme [22] so as to be able to compare the results of our architecture in the subsequent\nnetwork structure FTDRF. We view this MGS process as a preprocessing transformation akin to the\nconvolutions of convolutional neural networks (CNNs), with the beneﬁts and strengths that such\ntransformations provide.\n5\nFigure 3: Multi-grained scanning (MGS) routine, window size = 14\nIn MGS, windows of a set number of sizes are obtained inside the training set images (for the MNIST\ndataset window sizes are 7 × 7, 9 × 9, and 14 × 14).\nFor a given window size, the corresponding windows contained inside of all training images are\nused as a training set to construct a random forest and an extra random forest whose outputs are the\nclass probabilities. Unlike our routine for the building of the FTDRF layers that output the class\nprobabilities determined by each individual decision tree in the layer, this scheme outputs the class\nprobabilities determined by the whole random forest. Hence, for a given window size, the output of\nthe random forest for each image window is a vector of the class probabilities. For all samples fed\nthrough these random forests, the outputs of all image windows are concatenated together to produce\na feature vector representing classiﬁcation probabilities of each of the windows (see Figure 3).\nWith the 3 window sizes speciﬁed, the outputs of each of the 2 random forests for the respective\nwindow sizes are all concatenated together. This feature vector is the new representation of each given\nsample fed through the MGS process. With this transformation of the training data (and subsequently\nthe testing data), we train the FTDRF layers as previously described in Section 3.\n5\nFTDRF results on MNIST\nWe present results for an FTDRF on the MNIST handwriting digit recognition dataset, where each\nsample is a (28 × 28) black and white image of isolated digits, written by different people. The\ndataset is split into a training set with 60, 000 (see note below) samples and testing set of 10, 000\nsamples.\n5.1\nResults with single-pixel wiggle\nFor each training image, we created 4 more (28 × 28) images via the single-pixel wiggling technique\nto augment the size of the training data. The layers of FTDRF contained 2, 000 decision trees\n(∼1, 000 random decision trees and ∼1, 000 extra random trees). Layers were grown until the the\nrelative gain was less than 1%, totaling 3 layers. Node splits were determined by calculating the\ninformation gain entropy. We cite our results and the results of Zhou and Feng [22] to compare, as\ntheir architecture is most relevant to ours. We note, however, that Zhou and Feng do not augment\ndata in this test. The results are:\nModel\n# Trees\nAccuracy\ngcForest\n4000\n97.85%\nFTDRF\n2000\n97.58%\nTable 1: MNIST results without MGS\n5.2\nResults with MGS\nTable 2 presents the results of our architecture compared to Zhou’s gcForest [22], including the\nMGS preprocessing routine. Note then that we do not augment the dataset with the single pixel\naugmentation as we did previously. In this test, window sizes of 7, 9, and 14 were used for the MGS\n6\nstep, creating a total of 6 random forests (3 random forests and 3 extra random forests) to transform\nthe data for the FTDRF training. Then, training data was passed through to the FTDRF step, where\nlayers consisted of 2, 000 decision trees (∼1, 000 random decision trees and ∼1, 000 extra random\ntrees) but in this step, only 2 layers were necessary to achieve the desired relative validation error\nthreshold. The results are:\nModel\n# Trees\nAccuracy\ngcForest\n4000\n98.96%\nFTDRF\n2000\n98.98%\nFTDRF\n500\n98.89%\nTable 2: MNIST results with MGS\n6\nRelated work\nAs we have mentioned, the work of Zhou and Feng [22] is similar to our work, and their preprocessing\ntechnique of MGS was adapted for our use in testing. Our FTDRF primarily differs from Zhou and\nFeng’s gcForest in that gcForest passes the outputs of whole random forests concatenated onto the\noriginal data to each subsequent layer, whereas we pass only the outputs of the individual trees to\nsubsequent layers. The gcForest algorithm was very successful in a variety of classiﬁcation settings,\nincluding image and sequential data (in which MGS is applied), along with other non-sequential data\n(in which MGS is not applied).\nAnother related idea is that of “stacking” classiﬁers [20, 21]. In the context of [20], an ensemble\nof classiﬁers is trained and then further improvements are made by adding a classiﬁer or ensemble\nof classiﬁers to interpret the best way to combine the outputs of the original ensemble’s classiﬁers.\nIn the perspective of our deep architecture and its building process, the idea of stacking therefore\ncould be compared to a 2-layer architecture, with a relatively small second layer. Our idea proposes\nto continue the process, with larger layers stacked similarly to a DNN.\nThe connections between random forest and DNN structure were explored in [1, 8, 19]. These papers\nassert that random forest construction bears similarity to DNN construction and that random forests\ncan thus be transformed into neural networks and vice versa. More speciﬁcally, the mathematical\ndependencies between DNN nodes have been shown to be similar to the dependencies between\ndecision tree leaf nodes in random forests. While these methods draw connections between the\nconstruction of decision trees in random forests and DNNs, our work is fundamentally different in the\nidea of ensembling decision trees together in layers resembling a DNN architecture. As explained in\nSection 3, our architecture represents mapping data through different hypercubes in hopes of iterating\ntowards more easily classiﬁed data.\nReproducibility\nAll python code used to produce our results is available in our github repository at\nhttps://github.com/tkchris93/ForwardThinking.\nAcknowledgments\nThis work was supported in part by the National Science Foundation, Grant Number 1323785 and\nthe Defense Threat Reduction Agency, Grant Number HDRTA1-15-0049.\nReferences\n[1] Gerard Biau, Erwan Scornet, and Johannes Welbl. Neural random forests.\n[2] Leo Breiman. Random forests. Machine Learning, pages 45:5–32, 2001.\n[3] Tianqi Chen and Carlos Guestrin.\nXgboost: A scalable tree boosting system.\nCoRR,\nabs/1603.02754, 2016.\n[4] Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine\nLearning, 63(1):3–42, 2006.\n7\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. CoRR, abs/1512.03385, 2015.\n[6] Chris Hettinger, Tanner Christensen, Ben Ehlert, Jeffrey Humpherys, Tyler Jarvis, and Sean\nWade. Forward thinking: Building and training neural networks one layer at a time. 2017.\nPreprint.\n[7] Torsten Hothorn, Kurt Hornik, and Achim Zeileis. Unbiased recursive partitioning: A condi-\ntional inference framework. Journal of Computational and Graphical Statistics, 15(3):651–674,\n2006.\n[8] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel Rota Bulò . Deep neural\ndecision forests. June 2016.\n[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,\neditors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran\nAssociates, Inc., 2012.\n[10] Yann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Muller. Efﬁicient backprop. In\nNeural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop,\npages 9–50, London, UK, UK, 1998. Springer-Verlag.\n[11] Yann Lecun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied\nto document recognition. In Proceedings of the IEEE, pages 2278–2324, 1998.\n[12] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The mnist database of handwritten\ndigits. http://yann.lecun.com/exdb/mnist/. Accessed: 2017-05-19.\n[13] Fei Tony Liu, Kai Ming Ting, Yang Yu, and Zhi-Hua Zhou. Spectrum of variable-random trees.\nJ. Artif. Int. Res., 32(1):355–384, May 2008.\n[14] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.\n[15] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine\nLearning Research, 12:2825–2830, 2011.\n[16] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1631–1642, Seattle, Washington, USA, October 2013. Association\nfor Computational Linguistics.\n[17] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer:\nBenchmarking machine learning algorithms for trafﬁc sign recognition. Neural Networks,\n32:323–332, 2012.\n[18] Alexander Statnikov, Lily Wang, and Constantin F. Aliferis. A comprehensive comparison of\nrandom forests and support vector machines for microarray-based cancer classiﬁcation. BMC\nBioinformatics, 9(1):319, 2008.\n[19] Christian Wolf. Random forests v. deep learning. 2016.\n[20] David H. Wolpert. Stacked generalization. Neural Networks, 5:241–259, 1992.\n[21] Zhi-Hua Zhou. Ensemble Methods: Foundations and Algorithms. Chapman and Hall/CRC, 1st\nedition, 2012.\n[22] Zhi-Hua Zhou and Ji Feng. Deep forest: Towards an alternative to deep neural networks. CoRR,\nabs/1702.08835, 2017.\n8\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2017-05-20",
  "updated": "2017-05-20"
}