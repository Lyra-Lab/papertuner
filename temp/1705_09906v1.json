{
  "id": "http://arxiv.org/abs/1705.09906v1",
  "title": "Listen, Interact and Talk: Learning to Speak via Interaction",
  "authors": [
    "Haichao Zhang",
    "Haonan Yu",
    "Wei Xu"
  ],
  "abstract": "One of the long-term goals of artificial intelligence is to build an agent\nthat can communicate intelligently with human in natural language. Most\nexisting work on natural language learning relies heavily on training over a\npre-collected dataset with annotated labels, leading to an agent that\nessentially captures the statistics of the fixed external training data. As the\ntraining data is essentially a static snapshot representation of the knowledge\nfrom the annotator, the agent trained this way is limited in adaptiveness and\ngeneralization of its behavior. Moreover, this is very different from the\nlanguage learning process of humans, where language is acquired during\ncommunication by taking speaking action and learning from the consequences of\nspeaking action in an interactive manner. This paper presents an interactive\nsetting for grounded natural language learning, where an agent learns natural\nlanguage by interacting with a teacher and learning from feedback, thus\nlearning and improving language skills while taking part in the conversation.\nTo achieve this goal, we propose a model which incorporates both imitation and\nreinforcement by leveraging jointly sentence and reward feedbacks from the\nteacher. Experiments are conducted to validate the effectiveness of the\nproposed approach.",
  "text": "Listen, Interact and Talk: Learning to Speak via\nInteraction\nHaichao Zhang, Haonan Yu, and Wei Xu\nBaidu Research - Institue of Deep Learning\nSunnyvale, CA 94089\n{zhanghaichao,haonanyu,xuwei06}@baidu.com\nAbstract\nOne of the long-term goals of artiﬁcial intelligence is to build an agent that can\ncommunicate intelligently with human in natural language. Most existing work\non natural language learning relies heavily on training over a pre-collected dataset\nwith annotated labels, leading to an agent that essentially captures the statistics of\nthe ﬁxed external training data. As the training data is essentially a static snapshot\nrepresentation of the knowledge from the annotator, the agent trained this way\nis limited in adaptiveness and generalization of its behavior. Moreover, this is\nvery different from the language learning process of humans, where language is\nacquired during communication by taking speaking action and learning from the\nconsequences of speaking action in an interactive manner. This paper presents\nan interactive setting for grounded natural language learning, where an agent\nlearns natural language by interacting with a teacher and learning from feedback,\nthus learning and improving language skills while taking part in the conversation.\nTo achieve this goal, we propose a model which incorporates both imitation and\nreinforcement by leveraging jointly sentence and reward feedbacks from the teacher.\nExperiments are conducted to validate the effectiveness of the proposed approach.\n1\nIntroduction\nNatural language is the one of the most natural form of communication for human, and therefore\nit is of great value for an intelligent agent to be able to leverage natural language as the channel to\ncommunicate with human as well. Recent progress on natural language learning mainly relies on\nsupervised training with large scale training data, which typically requires a huge amount of human\nlabor for annotating. While promising performance has been achieved in many speciﬁc applications\nregardless of the labeling effort, this is very different from how humans learn. Humans act upon\nthe world and learn from the consequences of their actions [Skinner, 1957]. For mechanical actions\nsuch as movement, the consequences mainly follow geometrical and mechanical principles, while for\nlanguage, humans act by speaking and the consequence is typically response in the form of verbal\nand other behavioral feedbacks (e.g., nodding) from conversation partners. These feedbacks typically\ncontain informative signal on how to improve the language skills in subsequent conversions and\nplay an important role in human’s language acquisition process [Petursdottir and Mellor, 2016, Kuhl,\n2004, Weston, 2016].\nThe language acquisition process of a baby is both impressive as a manifestation of human intelligence\nand inspiring for designing novel settings and algorithms for computational language learning. For\nexample, baby interacts with people and learn through mimicking and feedbacks [Kuhl, 2004, Skinner,\n1957]. For learning to speak, baby initially performs verbal action by mimicking his conversational\nparter (e.g. parent) and masters the skill of generating a word (sentence). He could also possibly pick\nup the association of a word with a visual image when his parents saying “this is apple” while pointing\nto an apple or an image of it. Later, one can ask the baby question like “what is this” while pointing\narXiv:1705.09906v1  [cs.CL]  28 May 2017\n——–question-answer-feedback——–\nTeacher: what is on the north\nLearner: on . cabbage yes east\nTeacher: on the north is avocado [−]\n——–statement-repeat-feedback——–\nTeacher: on the west is orange\nLearner: on the west is apple\nTeacher: no orange is on the west [−]\n——-learner-statement-feedback——–\nTeacher: .\nLearner: cucumber is on the east\nTeacher: cucumber is on the east [+]\n(a) Training\n——compositional-generalization——\nTeacher: what is on the east\nLearner: avocado is on the east\nTeacher: yes on the east is avocado\nTeacher: where is avocado\nLearner: avocado is on the east\nTeacher: yes avocado is on the east\n———-knowledge-transferring———\nTeacher: what is on the south\nLearner: on the south is orange\nTeacher: yes orange is on the south\n(b) Testing\nFigure 1: Interactive language learning example. (a) During training, teacher interacts in natural\nlanguage with learner about objects. The interactions are in the form of (1) question-answer-feedback,\n(2) statement-repeat-feedback, and (3) statement from learner and then feedback from teacher. Certain\nforms of interactions may be excluded for certain set of object-direction combinations or objects\n(referred to as inactive combinations/objects) during training. For example, the combination of\n{avocado, east} does not appear in question-answer sessions; the object orange never appears in\nquestion-answer sessions but only in statement-repeat sessions. Teacher provides both sentence\nfeedback as well as reward signal (denoted as [+] and [−] in the ﬁgure). (b) During testing, teacher\ncan ask question about objects around, including questions involving inactive combinations/objects\nthat have never been asked before, e.g., questions about the combination of {avocado, east} and\nquestions about orange. This testing setup involves compositional generalization and knowledge\ntransferring settings and is used for evaluating the proposed approach (c.f. Section 4).\nto an object, and provides the correct answer if the baby doesn’t respond or responds incorrectly,\nwhich is typical in the initial stage. One can also provide at the same time a verbal conﬁrmation\n(e.g. “yes/no”) with a nodding/smile/kiss/hug when he answers correctly as a form of encouragement\nfeedback. From a baby’s perspective, the way to learn the language is by making verbal utterances to\nparent and adjusting his verbal behavior according to the corrections/conﬁrmation/encouragement\nfrom parent.\nThis example illustrates that the language learning process is inherently interactive, a property which\nis potentially difﬁcult to be captured by a static dataset as used in the conventional supervised learning\nsetting. Inspired by baby’s language learning process, we present a novel interactive setting for\ngrounded natural language learning, where the teacher and the learner can interact with each other\nin natural languages as shown in Figure 1. In this setting, there is no direct supervisions to guide\nthe behavior of the learner as in the supervised learning setting. Instead, the learner has to act in\norder to learn, i.e., engaging in the conversation with currently acquired speaking skills to obtain\nfeedbacks from the dialogue partner, which provide learning signals for further improvement on the\nconversation skills.\nTo leverage the feedbacks for learning, it is tempting to mimic the teacher directly (e.g., using a\nlanguage model). While this is a viable approach for learning how to speak, the agent trained by\npure imitation is not necessarily able to converse adaptively within context due to the negligence\nof the reinforcement signal. An example is that it is hard to make a successful conversation with a\nwell-trained parrot, which is only good at mimicking. The reason is that the learner is mimicking\nfrom a third person perspective [Stadie et al., 2017], mimicking the teacher who is conversing with it,\nthus certain words in the sentences from the teacher such as “yes/no” and “you/I” might need to be\nremoved/adapted due to the change of perspective from teacher to learner. This cannot be achieved\nwith imitation only. On the other hand, it is also challenging to generate appropriate conversational\nactions using purely the reinforcement signal without imitation. The fundamental reason is the\ninability of speaking, thus the probability of generating a sensible sentence by randomly uttering is\nlow, let alone that of a proper one. This is exempliﬁed by the fact that babies don’t fully develop their\nlanguage capabilities without the ability to hear, which is one of the most important channels for\nlanguage-related imitation.\nIn this paper, we propose a joint imitation and reinforcement approach for interactive language\nlearning. The proposed approach leverages both verbal and encouragement feedbacks from the\nteacher for joint learning, thus overcoming the difﬁculties encountered with either only imitation or\nreinforcement. The contributions of this paper can be therefore summarized as the following:\n• We present a novel human-like interaction-based grounded language learning setting where lan-\nguage is learned by interacting with the environment (teacher) in natural language.\n• We present a grounded natural language learning approach under the interactive setting by leverag-\ning feedbacks from the teacher during interaction through joint imitation and reinforcement.\n2\nTo the best of our knowledge, this is the ﬁrst work on using imitation and reinforcement jointly for\ngrounded natural language learning in an interactive setting.\nThe remainder of the paper is structured as follows. In Section 2, we make a brief review of related\nwork on natural language learning. Section 3 introduces the formulation of the interaction-based\nnatural language learning problem, followed with detailed explanation of the proposed approach.\nExperiments are carried out in Section 4 to show the language learning ability of the proposed\napproach in the interactive setting. Finally, we conclude the paper in Section 5.\n2\nRelated Work\nDeep network based language learning has received great success recently and has been applied\nin different applications, for example, machine translation [Sutskever et al., 2014], image caption-\ning/visual question answering [Mao et al., 2015, Vinyals et al., 2015, Antol et al., 2015] and dialogue\nresponse generation [Vinyals and Le, 2015, Wen et al., 2015]. For training, a large amount of training\ndata containing source-target pairs is needed, typically requiring a signiﬁcant amount of efforts to\ncollect. This setting essentially captures the statistics of the training data and does not respect the\ninteractive nature of language learning thus is very different from how humans learn.\nWhile conventional language model is trained in a supervised way, there are some recent works\nusing reinforcement learning for training. These works mainly target at the problem of tuning the\nperformance of a language model pre-trained in a supervised way according to a speciﬁc reward\nfunction which is either directly the evaluation metric such as standard BLEU core [Ranzato et al.,\n2016, Bahdanau et al., 2017], manually designed function [Li et al., 2016] or metric learned in an\nadversarial setting [Yu et al., 2017, Li et al., 2017b], which is non-differentiable, leading to the usage\nof reinforcement learning. Different from them, our main focus is on the possibility of language\nlearning in an interactive setting and required model designs, rather than optimizing a particular\nmodel output towards a speciﬁc evaluation metric.\nThere are some recent works on learning to communicate [Foerster et al., 2016, Sukhbaatar et al.,\n2016] and the emergence of language [Lazaridou et al., 2017, Mordatch and Abbeel, 2017]. The\nemerged language need to be interpreted via post-processing [Mordatch and Abbeel, 2017]. Dif-\nferently, we aim to achieve natural language learning from both perspectives of understanding and\ngeneration (i.e., speaking), thus the speaking action of the agent is readily understandable without any\npost-processing. There are also works on dialogue learning using a guesser/responser setting where\nthe guesser tries to achieve the ﬁnal goal (e.g., classiﬁcation/localization) by collecting additional\ninformation through asking questions to the responser [Strub et al., 2017, Das et al., 2017]. These\nworks try to optimize the question to be asked in order to help the guesser to achieve the ﬁnal guessing\ngoal. Thus the focus is very different from our goal of language learning through interactions with\nteacher.\nOur work is also related to reinforcement learning based control with natural language action\nspace [He et al., 2016] in the sense that our model also outputs action in natural language space.\nWe also shares similar motivation with [Weston, 2016, Li et al., 2017a], where language learning\nthrough textual dialogue has been explored. However, in these works [He et al., 2016, Weston, 2016,\nLi et al., 2017a] a set of candidate sequences is provided and the action required is selecting one\nfrom the candidate set, thus is essentially a discrete control problem. In contrast, our model achieves\nsentence generation through control in a continuous space, with a potentially inﬁnite sized action\nspace consisting of all possible sequences.\n3\nInteraction-based Language Learning\nWe will introduce the proposed interaction-based natural language learning approach in this section.\nThe goal is to design a learning agent1 that can learn to converse by interacting with the teacher,\nwhich can be either a virtual teacher or a human (c.f. Figure 1∼2). At time step t, according to\na visual image v, teacher generates a sentence wt which can be a question (e.g., “what is on the\neast”, “where is apple”), a statement (e.g., “banana is on the north”), or an empty sentence (denoted\nas “.”). The learner takes teacher’s sentence wt and the visual content v, and produces a sentence\nresponse at to the teacher. The teacher will then provide feedbacks to the learner according to\nits response in the form of both sentence wt+1 and reward rt+1. The sentence wt+1 represents\nverbal feedback from teacher (e.g., “yes on the east is cherry”, “no apple is on the east”) and rt+1\nmodels the non-verbal conﬁrmative feedback such as nodding/smile/kiss/hug, which also appears\n1We use the term agent interchangeably with learner according to context in the paper.\n3\nnaturally during interaction. The problem is therefore to design a model that can learn grounded\nnatural language from teacher’s sentences and reward feedbacks. While it might looks promising to\nformulate the problem as supervised training by learning from the subset of sentences from teacher\nwith only positive rewards, this approach won’t work because of the difﬁculties due to the changed of\nperspective [Stadie et al., 2017] as mentioned earlier. Our formulation of the problem as well as the\ndetails of the proposed approach are presented in the sequel.\n3.1\nProblem Formulation\nA response from the agent can be modeled as a sample from a probability distribution over the\npossible output sequences. Speciﬁcally, for one episode, given the visual input v and textual input\nw1:t from teacher upto time step t, the response at from the agent can be generated by sampling\nfrom a policy distribution pR\nθ (·) of the speaking action:\nat ∼pR\nθ (a|w1:t, v).\n(1)\nThe agent interacts with teacher by outputting the utterance at and receives the feedbacks from\nteacher at time step t + 1 as F = {wt+1, rt+1}. wt+1 is in the form of a sentence which represents\na verbal conﬁrmation/correction in accordance with wt and at, with preﬁxes (yes/no) added with a\nprobability of half (c.f. Figure 1∼2). Reward rt+1 is a scalar-valued feedback with positive value\nas encouragement while negative value as discouragement according to the correctness of the agent\nutterance at. The task of interaction-based language learning can be stated as learning by conversing\nwith teacher and improving from teacher’s feedbacks F. Mathematically, we formulate the problem\nas the minimization of a cost function as follows:\nLθ = LI\nθ + LR\nθ = ES\nh\n−P\nt log pI\nθ(wt+1|w1:t, v)\ni\n|\n{z\n}\nImitation\n+ EpR\nθ\nh\n−P\nt[γ]t · rt+1i\n|\n{z\n}\nReinforce\n,\n(2)\nwhere ES(·) is the expectation over all the sentence sequences S generated from teacher, rt+1 is the\nimmediate reward received at time step t + 1 after taking speaking action following policy pR\nθ (·) at\ntime step t and γ is the reward discount factor. [γ]t is used to denote the exponentiation over γ to\ndifferentiate it with superscript indexing. As for both components, the training signal is obtained\nvia interaction with the teacher, we termed this task as interaction-based language learning. For the\nimitation part, it essentially learns from teacher’s verbal response wt+1, which can only be obtained\nas a consequence of its speaking action. For the reinforce part, it learns from teacher’s reward signal\nrt+1, which is also obtained after taking the speaking action and received at the next time step.\nThe proposed interactive language learning formulation integrates two components which can fully\nleverage the feedbacks appearing naturally during conversational interaction:\n• Imitation plays the role of learning a grounded language model by observing teacher’s behaviors\nduring conversion with the learner itself. This enables the learner to have the basic ability to speak\nwithin context. The training data here are only the sentences from teacher, without any explicit\nlabeling of ground-truth and is a mixture of expected correct response and others. The way of\ntraining is by predicting the future. More speciﬁcally, the model is predicting the next future word\nat word level and predicting the next sentence at sentence level. Another important point is that it\nis in effect third person imitation [Stadie et al., 2017], as the learner is imitating the teacher who is\nconversing with it, rather than another expert student who is conversing with teacher.\n• Reinforce2 leverages the conﬁrmative feedbacks from the teacher for learning to converse properly\nby adjusting the action policy distribution. It enables the learner to use the acquired speaking\nability and adapt it according to feedbacks. Here we have the learning signal in the form of reward.\nThis is analogous to baby’s language learning process, who uses the acquired language skills by\ntrial and error with parents and improves according to the encouragement feedbacks.\nNote that while imitation and reinforce are represented as two separate components in Eqn.(2), they\nare tied via parameter sharing in order to fully leverage both forms of training signals. This form of\njoint learning is crucial for achieving successful language learning, compared with approaches with\nonly imitation or reinforce which are less effective, as veriﬁed by experiments in Section 4.\n2Reinforce denotes the module that learns from the reinforcement/encouragement signal throughout the\npaper and should be differentiated with the REINFORCE algorithm in the literature [Sutton and Barto, 1998].\n4\nwt\nwt+1\nat\nv\nv\nv\nht−1\nlast\nht\nlast\nt 7−→\nt + 1 7−→\nht\n0\nht+1\n0\nEncoding-RNN\nAction-RNN\n6\n?\nshare parameter\n<bos>\nwhere\nis\napple\nwhere\nis\napple\n<eos>\n<bos>\nno\nwest\nno\nwest\n<eos>\n<bos>\napple\nis\non\nthe\nsouth\n<eos>\ncontroller f\nkt\nvisual\nencoder\nvisual\nencoder\nvisual\nencoder\n(a)\nspatial convolution\nspatial summation\nHadamard product\nmix aggregation\nVatt\nCNN\natt.\nﬁlter\nmask\naggregated\nfeature\nv\nht\n0\n(b) visual encoder\nf\nht\nlast\nkt\nskip connect.\n(c) controller\nFigure 2: Network structure. (a) Illustration of the network structure with sample inputs. (b) Visual\nencoder network Vatt(·). Visual image is encoded by a CNN and spatially aggregated to a vector\nwith an attention map. The attention map is obtained by convolving the feature map from CNN with\na spatial ﬁlter generated from the initial state ht\n0. A binary mask generated from ht\n0 is applied to the\nspatially aggregated vector to produce the ﬁnal visual feature vector. At time step t, the encoding-\nRNN takes teacher’s sentence (“where is apple”) and the visual feature vector from the visual encoder\nVatt(·) as inputs. The last state of the encoding-RNN ht\nlast is passed through a controller f(·) to the\naction-RNN for response generation. Parameters are shared between encoding-RNN and action-RNN.\nDuring training, the RNN is trained by predicting next words and next sentences. (c) Controller\nnetwork with a residue control module followed by a Gaussian Policy module (c.f. Sec. 3.2.2).\n3.2\nApproach\nA hierarchical Recurrent Neural Network is used for capturing the sequential structure both across\nsentences and within a sentence [Yu et al., 2016, Serban et al., 2016], as shown in Figure 2(a).\nAt time-step t, an encoding-RNN encodes the input sentence wt from teacher as well as history\ninformation into a state vector ht\nlast, which is passed through an action controller f(·) to produce a\ncontrol vector kt as input to the action-RNN for generating the response at to the teacher’s sentence.\nTeacher will generate feedback F = {wt+1, rt+1} according to both wt and at. In addition to being\nused as input to action controller, the state vector is also passed to the next time step and used as the\ninitial state of the encoding-RNN in the next step (i.e., ht+1\n0\n≜ht\nlast) for learning from wt+1, thus\nforming another level of recurrence at the scale of time steps.\n3.2.1\nImitation with Hierarchical-RNN-based Language Modeling\nThe teacher’s way of speaking provides a source for the learner to mimic. One way to learn from\nthis source of information is by predictive imitation. Speciﬁcally, for a particular episode, we can\nrepresent the probability of the next sentence wt+1 conditioned on the previous sentences w1:t and\ncurrent image v as\npI\nθ(wt+1|w1:t, v) = pI\nθ(wt+1|ht\nlast, v) = Q\ni pI\nθ(wt+1\ni\n|wt+1\n1:i−1, ht\nlast, v),\n(3)\nwhere ht\nlast is the last state of RNN at time step t as the summarization of w1:t (c.f. Figure 2), and i\nindexes words within a sentence. It is natural to model the probability of the i-th word in the t+1-th\nsentence with an RNN as well, where the sentences up to t and words up to i within the t+1-th sentence\nwe conditioned upon is captured by a ﬁxed-length hidden state vector as ht+1\ni\n= RNN(ht+1\ni−1, wt+1\ni\n),\nthus\npI\nθ(wt+1\ni\n|wt+1\n1:i−1, ht\nlast, v) = softmax(Whht+1\ni\n+ WvVatt(v, ht+1\n0\n) + b),\n(4)\nwhere Wh, Wv and b denote the transformation weight and bias parameters respectively. Vatt(·)\ndenotes the visual encoding network with spatial attention incorporated as shown in Figure 2(b).\nVatt(·) takes the initial RNN state ht\n0 and visual image v as input. The visual image is ﬁrst encoded\nby a CNN to obtain the visual feature map (red cube in Figure 2(b)). The visual feature map is\nappended with another set of maps with learnable parameters for encoding the directional information\n(blue cube in Figure 2(b)). This set of feature maps is spatially aggregated to a vector with an attention\n5\nmap, which is obtained by convolving the feature map with a spatial ﬁlter generated from the initial\nRNN state. An attention mask for emphasizing visual or directional features is generated from ht\n0\nand is applied to the spatially aggregated vector to produce the ﬁnal feature vector. The initial state of\nthe encoding-RNN is the last state of the previous RNN, i.e., ht+1\n0\n= ht\nlast and h0\n0 = 0.\nThe language model trained this way will have the basic ability of producing a sentence conditioned\non the input. Therefore, when connecting an encoding-RNN with action-RNN directly, i.e., inputing\nthe last state vector from encoding-RNN into action-RNN as the initial state, the learner will have\nthe ability to generate a sentence by mimicking the way teacher speaks, due to parameter sharing.\nHowever, this basic ability of speaking is not enough for the learner to converse properly with teacher,\nwhich requires the incorporation of reinforcement signals as detailed in the following section.\n3.2.2\nLearning via Reinforcement for Sequence Actions\nAn agent generates an action according to pR\nθ (a|w1:t, v). As sentences w1:t can be summarized\nas the last RNN state ht\nlast, the action policy distribution can be represented as pR\nθ (a|ht\nlast, v). To\nleverage the language skill that is simultaneously learned from imitation, we can generate the sentence\nusing a language model shared with imitation, but with a modulated conditional signal via a controller\nnetwork f(·) as follows (c.f. Figure 2(a, c))\npR\nθ (at|ht\nlast, v) = pI\nθ(at ≜wt+1|f(ht\nlast), v).\n(5)\nThe reason for incorporating a controller f(·) for modulation is that the basic language model only\noffers the learner the ability to generate a sentence, but not necessarily the ability to respond correctly,\nor to answer a question from teacher properly. Without any additional module, the agent’s behaviors\nwould be the same as those from teacher because of parameter sharing, thus agent cannot learn to\nspeak correctly in an adaptive manner by leveraging the feedbacks from teacher.\nController f(·) is a composite function with two components: (1) a residue structured network for\ntransforming the encoding vector ht\nlast in order to modify the behavior; (2) a Gaussian policy module\nfor generating a control vector from a Gaussian distribution conditioned on the transformed encoding\nvector from the residue control network as a form of exploration. In practice, we also incorporate a\ngradient-stopping layer between the controller and its input, to encapsulate all the modulation ability\nwithin the controller.\nResidue Control. The action controller should have the property that it can pass the input vector\nto the next module unmodiﬁed when desirable while can modify the content of the input vector\notherwise. Therefore, a residue structured network is one design satisfying this requirement, with a\ncontent modifying vector added to the original input vector (i.e., skip connection) as follows\nc = τ(h) + h,\n(6)\nwhere τ(·) is a content transformation net and c is the generated control vector. The reason for\nincluding a skip connection is that it offers the ability to leverage the language model simultaneously\nlearned via imitation for generating sensible sentences and the transformation net τ(·) includes\nlearnable parameters for adjusting the behaviors via interactions with the environment and feedbacks\nfrom teacher. We implement τ(·) as two fully-connected layers with ReLU activation.\nGaussian Policy. Gaussian policy net models the output vector as a Gaussian distribution conditioned\non the input vector. It takes the generated control vector c as input and produces a vector k which is\nused as the initial state of the action-RNN. The Gaussian policy is modeled as follows:\npR\nθ (k|c) = N(c, ΓT Γ), Γ = diag[γ(c)].\n(7)\nThe incorporation of Gaussian policy introduces stochastic unit into the network, thus back-\npropagation cannot be applied directly. We therefore use policy gradient algorithm for optimiza-\ntion [Sutton and Barto, 1998]. where γ(·) is a sub-network for estimating the standard derivation\nvector and is implemented using a fully-connected layer with ReLU activation.3 The vector k gen-\nerated from the controller is then used as the initial state of action-RNN and the sentence output\nis generated using beam search (c.f. Figure 2(a)). For the reward rt+1 in Eqn.(2), we introduce\na baseline for reducing variance as rt+1 −Vυ(v), where Vυ(·) represents the value network with\nparameter vector υ and is estimated by adding to LR an additional value network cost LV as follows\nLV = EpR\nθ\n\u0000rt+1 + λVυ−(vt+1) −Vυ(vt)\n\u00012,\n(8)\nwhere υ denotes the set of parameters in the value network and Vυ−(·) denotes the target version of\nthe value network, whose parameter vector υ−is periodically copied from the training version [Mnih\net al., 2013].\n3In practice, we add a small value (0.01) to γ(c) as a constrain of the minimum standard deviation.\n6\n3.3\nTraining\nTraining involves optimizing the stochastic policy by using the teacher’s feedback F as a training\nsignal, obtaining a set of optimized parameters by considering jointly imitation and reinforcement as\nshown in Eqn.(2). Stochastic gradient descend is used for training the network. For LI from imitation\nmodule, we have its gradient as:\n∇θLI\nθ = −ES[∇θ\nP\nt log pI\nθ(wt+1|w1:t, v)].\n(9)\nUsing policy gradient theorem [Sutton and Barto, 1998], we have the following gradient for the\nreinforce module:\n∇θLR\nθ = −EpR\nθ\n\u0002\n[∇θ log pR\nθ (kt|ct) + ∇υVυ(v)] · δ\n\u0003\n,\n(10)\nwhere δ is the td-error deﬁned as δ = rt+1 + γVυ−(v) −Vυ(v). The algorithm is implemented with\ndeep learning platform PaddlePaddle4. We train the network with Adagrad [Duchi et al., 2011] with\na batch size of 16 and a learning rate of 1 × 10−5. Discount factor γ = 0.99. Experience replay is\nused in practice [Mnih et al., 2013].\n4\nExperimental Results\nWe evaluate the performance of the proposed approach under several different settings to demon-\nstrate its ability of interactive language learning. For training efﬁciency, we construct a simulated\nenvironment for language learning as shown in Figure 1. We consider the case with four different\nobjects around the learner in each direction (S, N, E, W), which are randomly sampled from a set of\nobjects for each session. Within this environment, a teacher interacts with the agent about objects\naround in three different forms: (1) asking a question as “what is on the south”, “where is apple” and\nthe agent answers the question; (2) describing objects around as “apple is on the east” and agents\nrepeat the statement; (3) saying nothing (“.”) then agent describes objects around and gets a feedback\nfrom teacher. The agent receives a positive reward (r=+1) if it behaves correctly (generates a correct\nanswer to a question from teacher or produces a correct statement if teacher says nothing) and a\nnegative reward (r=−1) otherwise. Reward is used to represent teacher’s non-verbal feedback such\nas nodding as a form of encouragement. Besides reward feedback, teacher also provides a verbal\nfeedback including the expected answer in the form of “X is on the east” or “on the east is X” and\nwith preﬁx (“yes/no”) added with a probability of half. The speaking action from the agent is correct\nif it outputs a sentence matches exactly with the expected answer in one of the above forms. There is\na possibility for the learner to generate a new correct sentence that beyond teacher’s knowledge. This\nis not handled in our current work due to the usage of a scripted teacher.\nLanguage Learning Evaluation. We ﬁrst validate the basic language learning ability of the proposed\napproach under the interactive language learning setting. In this setting, the teacher ﬁrst generates\na sentence for the learner, then the learner will respond, and the teacher will provide feedback in\nterms of sentence and reward. We compare the proposed approach with two baseline approaches:\n(1) Reinforce which uses directly reinforcement for learning from teacher’s reward feedback [Sutton\nand Barto, 1998]; (2) Imitation which learns by mimicking teacher’s behavior [Sutskever et al.,\n2014]. Experimental results are shown in Figure 3. It is interesting to note that learning directly\nfrom reward feedback only (Reinforce) does not lead to successful language acquisition. This is\nmainly because of the low possibility of generating a sensible sentence by random exploration, and\nthe even lower possibility of generating the correct sentence, thus the received reward can stay at\n−1. On the other hand, the Imitation approach performs better than Reinforce, due to the speaking\nability it gained through mimicking. The proposed approach achieves reward higher than both\ncompared approaches, due to the effectiveness of the joint formulation, which can fully leverage the\nfeedback signals appeared naturally during conversion for learning. This indicates the effectiveness\nof the proposed approach for language learning under the interactive setting. Similar behaviors\nhave been observed during testing. We further visualize some examples as shown in Figure 4 along\nwith the generated attention maps. As can be observed from the results, the proposed approach can\nsuccessfully generate correct attention maps for both what and where questions. When teacher says\nnothing (“.”), the agent can generate a statement describing an object around correctly.\n4https://github.com/PaddlePaddle/Paddle\n7\n(a)\n(b) Quantative Resutls\nTable 1: Testing Results with Mixed Conﬁg.\nSettings\nReinforce\nImitation\nProposed\nCompositional-gen.\n0.0%\n83.7%\n98.9%\nKnowledge-transfer\n0.0%\n81.6%\n97.5%\nTable 2: Testing Results wtih Held-out Conﬁg.\nSettings\nReinforce\nImitation\nProposed\nCompositional-gen.\n0.0%\n75.1%\n98.3%\nKnowledge-transfer\n0.0%\n70.4%\n89.0%\nFigure 3: Evaluation results. (a) Evolution of reward during training. (b) Comparison of the pro-\nposed approach with Reinforce and Imitation approaches across different settings and conﬁgurations.\nMixed conﬁg denotes the conﬁguration involving interactions with all objects. Held-out conﬁg denotes\nthe conﬁguration involving interactions with only the objects that are inactive during training.\nT: what is on the north\nL: on the north is apple\nT: yes apple is on the north [+]\natt. map\n(a)\nT: what is on the east\nL: on the east is avocado\nT: avocado is on the east [+]\natt. map\n(b)\nT: where is strawberry\nL: strawberry is on the west\nT: yes strawberry is on the west [+]\natt. map\n(c)\nT: .\nL: on the east is cucumber\nT: yes on the east is cucumber [+]\natt. map\n(d)\nFigure 4: Example results. (a-b) what questions. (c) where question. (d) teacher says nothing (“.”)\nand the agent is expected to produce a statement. For each example, we show the visual image, the\nconversion dialogues between teacher and learner, as well as the attention map (att. map) generated\nfrom the learner when producing the response to teacher (overlaid on top-right). The attention map is\nrendered as a heat map, with red color indicating large value while blue indicating small value. Grid\nlines are overlaid on top of the attention map to for visualization purpose. The position of the learner\nis marked with a cross in the attention map (T/L: teacher/learner, [+/−]: positive/negative rewards).\nZero-shot Dialogue. An intelligent agent is expected to have an ability to generalize. While this\nis not the main focus on the paper, we use it as a way to assess the language learning ability of\nan approach. Experiments are done in following two settings. (1) Compositional generalization:\nthe learner interacts with the teacher about objects around during training, but does not have any\ninteraction with certain objects (referred to as inactive objects) at particular locations, while in testing\nthe teacher can ask questions about an object regardless of its location. It is expected that a good\nlearner should be able to generalize the concepts it learned about both objects and locations as well as\nthe acquired conversation skills and can interact successfully in natural language with teacher about\nnovel {object, location} combinations that it never experienced before. (2) Knowledge transferring:\nteacher asks learner questions about the objects around. For certain objects, the teacher only provides\ndescriptions without asking questions during training, while in testing, the teacher can ask questions\nabout any object present in the scene. The learner is expected to be able to transfer the knowledge\nlearned from teacher’s description to generate an answer to teacher’s question about these objects.\nExperiments are carried out under these two settings for two conﬁgurations (mixed and held-out)\nand experimental results are summarized in Table 1 and Table 2 respectively. Mixed conﬁguration\ndenotes the case in which a mixture of interactions with all objects regardless of whether they are\nactive or inactive during training. Held-out conﬁguration denotes the case involving interactions with\nonly the objects that are inactive during training. The results shows that the Reinforce approach\nperforms poorly under both settings due to the lack of basic language-related abilities as mentioned\nin the previous section. The Imitation approach performs better than Reinforce mainly due to its\nlanguage speaking ability through mimicking. Note that the held-out conﬁguration is a subset of the\nmixed-conﬁguration involving only novel objects/combinations, thus is more difﬁcult than the mixed\ncase. It is interesting to note that the proposed approach maintains a consistent behavior under the\nmore difﬁcult held-out conﬁguration and outperforms the other two approaches under both settings,\ndemonstrating its effectiveness in interactive language learning.\n8\n5\nConclusion\nWe present an interactive setting for grounded natural language learning and propose an approach\nthat achieves effective interactive natural language learning by fully leveraging the feedbacks that\narise naturally during interaction through joint imitation and reinforcement. Experimental results\nshow that the proposed approach provides an effective way for natural language learning in the\ninteractive setting and enjoys desirable generalization and transferring abilities under several different\nscenarios. As for future work, we would like to explore the direction of explicit modeling of learned\nknowledge [Yang, 2003] and fast learning about new concepts [Andrychowicz et al., 2016]. Another\ninteresting direction is to connect the language learning task presented in this paper with other\nheterogeneous tasks such as navigation.\nAcknowledgements\nWe thank Xiaochen Lian, Zhuoyuan Chen, Yi Yang and Qing Sun for their discussions and comments.\nReferences\nM. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning\nto learn by gradient descent by gradient descent. In NIPS, 2016.\nS. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. VQA: Visual Question\nAnswering. In ICCV, 2015.\nD. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. C. Courville, and Y. Bengio. An actor-critic\nalgorithm for sequence prediction. In ICLR, 2017.\nA. Das, S. Kottur, , J. M. Moura, S. Lee, and D. Batra. Learning cooperative visual dialog agents with deep\nreinforcement learning. CoRR, abs/1703.06585, 2017.\nJ. C. Duchi, E. Hazan, and Y. Singer.\nAdaptive subgradient methods for online learning and stochastic\noptimization. Journal of Machine Learning Research, 12:2121–2159, 2011.\nJ. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson. Learning to communicate with deep multi-agent\nreinforcement learning. In NIPS, 2016.\nJ. He, J. Chen, X. He, J. Gao, L. Li, L. Deng, and M. Ostendorf. Deep reinforcement learning with a natural\nlanguage action space. In ACL, 2016.\nP. K. Kuhl. Early language acquisition: cracking the speech code. Nat Rev Neurosci, 5(2):831–843, 2004.\nA. Lazaridou, A. Peysakhovich, and M. Baroni. Multi-agent cooperation and the emergence of (natural) language.\nIn ICLR, 2017.\nJ. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao. Deep reinforcement learning for dialogue\ngeneration. In EMNLP, 2016.\nJ. Li, A. H. Miller, S. Chopra, M. Ranzato, and J. Weston. Learning through dialogue interactions. In ICLR,\n2017a.\nJ. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky. Adversarial learning for neural dialogue generation. CoRR,\nabs/1701.06547, 2017b.\nJ. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille. Deep captioning with multimodal recurrent neural\nnetworks (m-RNN). ICLR, 2015.\nV. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing Atari\nwith deep reinforcement learning. In NIPS Deep Learning Workshop. 2013.\nI. Mordatch and P. Abbeel. Emergence of grounded compositional language in multi-agent populations. CoRR,\nabs/1703.04908, 2017.\nA. I. Petursdottir and J. R. Mellor. Reinforcement contingencies in language acquisition. Policy Insights from\nthe Behavioral and Brain Sciences, 4(1):25–32, 2016.\nM. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. In\nICLR, 2016.\nI. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville, and J. Pineau. Building end-to-end dialogue systems using\ngenerative hierarchical neural network models. In AAAI, 2016.\nB. F. Skinner. Verbal Behavior. Copley Publishing Group, 1957.\nB. C. Stadie, P. Abbeel, and I. Sutskever. Third-person imitation learning. In ICLR, 2017.\nF. Strub, H. de Vries, J. Mary, B. Piot, A. C. Courville, and O. Pietquin. End-to-end optimization of goal-driven\nand visually grounded dialogue systems. In IJCAI, 2017.\nS. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropagation. In NIPS,\n2016.\nI. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.\nO. Vinyals and Q. V. Le. A neural conversational model. CoRR, abs/1506.05869, 2015.\n9\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR,\n2015.\nT. Wen, M. Gasic, N. Mrksic, P. Su, D. Vandyke, and S. J. Young. Semantically conditioned LSTM-based\nnatural language generation for spoken dialogue systems. In EMNLP, 2015.\nJ. Weston. Dialog-based language learning. In NIPS, 2016.\nC. D. Yang. Knowledge and Learning in Natural Language. Oxford University Press UK, 2003.\nH. Yu, J. Wang, Z. Huang, Y. Yang, and W. Xu. Video paragraph captioning using hierarchical recurrent neural\nnetworks. In CVPR, 2016.\nL. Yu, W. Zhang, J. Wang, and Y. Yu. SeqGAN: Sequence generative adversarial nets with policy gradient. In\nAAAI, 2017.\n10\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-05-28",
  "updated": "2017-05-28"
}