{
  "id": "http://arxiv.org/abs/1705.10432v1",
  "title": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning",
  "authors": [
    "Hamid Mirzaei",
    "Tony Givargis"
  ],
  "abstract": "Recent advances in combining deep learning and Reinforcement Learning have\nshown a promising path for designing new control agents that can learn optimal\npolicies for challenging control tasks. These new methods address the main\nlimitations of conventional Reinforcement Learning methods such as customized\nfeature engineering and small action/state space dimension requirements. In\nthis paper, we leverage one of the state-of-the-art Reinforcement Learning\nmethods, known as Trust Region Policy Optimization, to tackle intersection\nmanagement for autonomous vehicles. We show that using this method, we can\nperform fine-grained acceleration control of autonomous vehicles in a grid\nstreet plan to achieve a global design objective.",
  "text": "Fine-grained acceleration control for autonomous\nintersection management using deep reinforcement\nlearning\nHamid Mirzaei\nDept. of Computer Science\nUniversity of California, Irvine\nmirzaeib@uci.edu\nTony Givargis\nDept. of Computer Science\nUniversity of California, Irvine\ngivargis@uci.edu\nAbstract—Recent advances in combining deep learning and\nReinforcement Learning have shown a promising path for\ndesigning new control agents that can learn optimal policies for\nchallenging control tasks. These new methods address the main\nlimitations of conventional Reinforcement Learning methods such\nas customized feature engineering and small action/state space\ndimension requirements. In this paper, we leverage one of the\nstate-of-the-art Reinforcement Learning methods, known as Trust\nRegion Policy Optimization, to tackle intersection management\nfor autonomous vehicles. We show that using this method, we can\nperform ﬁne-grained acceleration control of autonomous vehicles\nin a grid street plan to achieve a global design objective.\nI. INTRODUCTION\nPrevious works on autonomous intersection management\n(AIM) in urban areas have mostly focused on intersection\narbitration as a shared resource among a large number\nof autonomous vehicles. In these works [1][2], high-level\ncontrol of the vehicles is implemented such that the vehicles\nare self-contained agents that only communicate with the\nintersection management agent to reserve space-time slots in\nthe intersection. This means that low-level vehicle navigation\nwhich involves acceleration and speed control is performed\nby each individual vehicle independent of other vehicles and\nintersection agents. This approach is appropriate for minor\narterial roads where a large number of vehicles utilize the\nmain roads at similar speeds while the adjacent intersections\nare far away.\nIn scenarios involving local roads, where the majority of the\nintersections are managed by stop signs, the ﬂow of trafﬁc is\nmore efﬁciently managed using a ﬁne-grained vehicle control\nmethodology. For example, when two vehicles are crossing\nthe intersection of two different roads at the same time, one\nvehicle can decelerate slightly to avoid collision with the other\none or it can take another path to avoid confronting the other\nvehicle completely. Therefore, the nature of the AIM problem\nis a combination of route planning and real-time acceleration\ncontrol of the vehicles. In this paper, we propose a novel AIM\nformulation which is the combination of route planning and\nﬁne-grained acceleration control. The main objective of the\ncontrol task is to minimize travel time of the vehicles while\navoiding collisions between them and other obstacles. In this\ncontext, since the movement of a vehicle is dependent on the\nother vehicles in the same vicinity, the motion data of all\nvehicles is needed in order to solve the AIM problem.\nTo explain the proposed AIM scheme, let us deﬁne a “zone”\nas a rectangular area consisting of a number of intersections\nand segments of local roads. An agent for each zone collects\nthe motion data and generates the acceleration commands\nfor all autonomous vehicles within the zone’s boundary. All\nthe data collection and control command generation should\nbe done in real-time. This centralized approach cannot be\nscaled to a whole city, regardless of the algorithm used, due\nto the large number of vehicles moving in a city which\nrequires enormous computational load and leads to other\ninfeasible requirements such as low-latency communication\ninfrastructure. Fortunately, the spatial independence (i.e., the\nfact that navigation of the vehicles in one zone is independent\nof the vehicles in another zone that is far enough away) makes\nAIM an inherently local problem. Therefore, we can assign an\nagent for each local zone in a cellular scheme.\nThe cellular solution nevertheless leads to other difﬁculties\nthat should be considered for a successful design of the AIM\nsystem. One issue is the dynamic nature of the transportation\nproblem. Vehicles can enter or leave a zone controlled by\nan agent or they might change their planned destinations\nfrom time to time. To cope with these issues, the receding\nhorizon control method can be employed where the agent\nrepeatedly recalculates the acceleration command over a\nmoving time horizon to take into account the mentioned\nchanges. Additionally, two vehicles that are moving toward\nthe same point on the boundary of two adjacent zones\nsimultaneously might collide because the presence of each\nvehicle is not considered by the agent of the adjacent\nzone. This problem can be solved by adequate overlap\nbetween adjacent zones. Furthermore, any planned trip for a\nvehicle typically crosses multiple zones. Hence, a higher level\nplanning problem should be solved ﬁrst that determines the\nentry and exit locations of a vehicle in a zone.\nIn this paper we focus on the subproblem of acceleration\ncontrol of the vehicles moving in a zone to minimize the\ntotal travel time. We use a deep reinforcement learning\n(RL) approach to tackle the ﬁne-grained acceleration control\narXiv:1705.10432v1  [cs.AI]  30 May 2017\nproblem since conventional control methods are not applicable\nbecause of the non-convex collision avoidance constraints\n[3]. Furthermore, if we want to incorporate more elements\ninto the problem, such as obstacles or reward/penalty terms\nfor gas usage, passenger comfort, etc., the explicit modeling\nbecomes intractable and an optimal control law derivation will\nbe computationally unattainable.\nRL methods can address the above mentioned limitations\ncaused by the explicit modeling requirement and conventional\ncontrol method limitations. The main advantage of RL is that\nmost of the RL algorithms are “model-free” or at most need\na simulation model of the physical system which is easier\nto develop than an explicit model. Moreover, the agent can\nlearn optimal policies just by interacting with the environment\nor executing the simulation model. However, conventional\nRL techniques are only applicable in small-scale problem\nsettings and require careful design of approximation functions.\nEmerging Deep RL methods [4] that leverage the deep neural\nnetworks to automatically extract features seem like promising\nsolutions to shortcomings of the classical RL methods.\nThe main contributions of this paper are: (1) Deﬁnition\nand formulation of the AIM problem for local road settings\nwhere vehicles arecoordinated by ﬁne-grained acceleration\ncommands. (2) Employing TRPO proposed in [5] to solve\nthe formulated AIM problem. (3) Incorporating collision\navoidance constraint in the deﬁnition of RL environment as\na safety mechanism.\nII. RELATED WORK\nAdvances in autonomous vehicles in recent years have\nrevealed a portrait of a near future in which all vehicles\nwill be driven by artiﬁcially intelligent agents. This emerging\ntechnology calls for an intelligent transportation system\nby redesigning the current transportation system which is\nintended to be used by human drivers. One of the interesting\ntopics that arises in intelligent transportation systems is AIM.\nDresner et al. have proposed a multi-agent AIM system in\nwhich vehicles communicate with intersection management\nagents to reserve a dedicated spatio-temporal trajectory at the\nintersection [2].\nIn [6], authors have proposed a self-organizing control\nframework in which a cooperative multi-agent control scheme\nis employed in addition to each vehicle’s autonomy. The\nauthors have proposed a priority-level system to determine\nthe right-of-way through intersections based on vehicles’\ncharacteristics or intersection constraints.\nZohdy et al. presented an approach in which the Cooperative\nAdaptive Cruise Control (CACC) systems are leveraged to\nminimize delays and prevent clashes [7]. In this approach,\nthe intersection controller communicates with the vehicles\nto recommend the optimal speed proﬁle based on the\nvehicle’s characteristics, motion data, weather conditions and\nintersection properties. Additionally, an optimization problem\nis solved to minimize the total difference of actual arrival times\nat the Intersection and the optimum times subject to conﬂict-\nfree temporal constraints.\nEnvironment\nAgent\nAction\nState\nReward\nFig. 1: Agent-Environment interaction model in RL\nA decentralized optimal control formulation is proposed in\n[8] in which the acceleration/deceleration of the vehicles are\nminimized subject to collision avoidance constraints.\nMakarem et al. introduced the notion of ﬂuent coordination\nwhere smoother trajectories of the vehicles are achieved\nthrough a navigation function to coordinate the autonomous\nvehicles along predeﬁned paths with expected arrival time at\nintersections to avoid collisions.\nIn all the aforementioned works, the AIM problem is\nformulated for only one intersection and no global minimum\ntravel time objective is considered directly. Hausknecht\net al. extended the approach proposed in [2] to multi-\nintersection settings via dynamic trafﬁc assignment and\ndynamic lane reversal [1]. Their problem formulation is based\non intersection arbitration which is well suited to main roads\nwith a heavy load of trafﬁc.\nIn this paper, for the ﬁrst time, we introduce ﬁne-grained\nacceleration control for AIM. In contrast to previous works,\nOur proposed AIM scheme is applicable to local road\nintersections. We also propose an RL-based solution using\nTrust Region Policy Optimization to tackle the deﬁned AIM\nproblem.\nIII. REINFORCEMENT LEARNING\nIn this section, we brieﬂy review RL and introduce the\nnotations used in the rest of the paper. In Fig. 1, the agent-\nenvironment model of RL is shown. The “agent” interacts with\nthe “environment” by applying “actions” that inﬂuence the\nenvironment state at the future time steps and observes the\nstate and “reward” in the next time step resulting from the\naction taken. The “return” is deﬁned as the sum of all the\nrewards from the current step to the end of current “episode”:\nGt =\nT\nX\ni=t\nri\n(1)\nwhere ri are future rewards and T is the total number of\nsteps in the episode. An “episode” is deﬁned as a sequence of\nagent-environment interactions. In the last step of an episode\nthe control task is “ﬁnished.” Episode termination is deﬁned\nspeciﬁcally for the control task of the application.\nFor example, in the cart-pole balancing task, the agent is the\ncontroller, the environment is the cart-pole physical system,\nthe action is the force command applied to the cart, and the\nreward can be deﬁned as r = 1 as long as the pole is nearly\nin an upright position and a large negative number when the\npole falls. The system states are cart position, cart speed, pole\nangle and pole angular speed. The agent task is to maximize\nthe return Gt, which is equivalent to prevent pole from falling\nfor the longest possible time duration.\nIn RL, a control policy is deﬁned as a mapping of the system\nstate space to the actions:\na = π(s)\n(2)\nwhere a is the action, s is the state and π is the policy. An\noptimal policy is one that maximizes the return for all the\nstates, i.e.:\nvπ∗(s) ≥vπ(s),\nfor all s, π\n(3)\nwhere v is the return function deﬁned as the return achievable\nfrom state s by following policy π. Equation (3) means that\nthe expected return under optimal policy π∗is equal to or\ngreater than any other policy for all the system states.\nThe concepts mentioned above to introduce RL are all\napplicable to deterministic cases, but generally we should be\nable to deal with inherent system uncertainty, measurement\nnoise, or both. Therefore, we model the system as a Markov\nDecision Process (MDP) assuming that the environment has\nthe Markov property [9]. However, contrary to most of the\ncontrol design methods, many RL algorithms do not require\nthe models to be known beforehand. The elimination of the\nrequirement to model the system under control is a major\nstrength of RL.\nA system has the Markov property if at a certain time\ninstant, t, the system history can be captured in a set of\nstate variables. Therefore, the next state of the system has\na distribution which is only conditioned on the current state\nand the taken action at the current time, i.e.:\nst+1 ∼P(st+1|st, at)\n(4)\nThe Markov property holds for many cyber-physical system\napplication domains and therefore MDP and RL can be applied\nas the control algorithm. We can also deﬁne the stochastic\npolicy which is a generalized version of (2) as a probability\ndistribution of actions conditioned on the current state, i.e.:\nat ∼π(at|st)\n(5)\nThe expected return function which is the expected value of\n‘return’ deﬁned in (1) can be written as:\nvπ(st) =\nE\naτ ∼π,τ≥t\n\" ∞\nX\ni=0\nγirt+i\n#\n(6)\nThis equation is deﬁned for inﬁnite episodes and the constant\n0 < γ < 1 is introduced to ensure that the deﬁned expected\nreturn is always a ﬁnite value, assuming the returns are\nbounded.\nAnother important concept in RL is the action-value\nfunction, Qπ(s, a) deﬁned as the expected return (value) if\naction at is taken at time t under policy π:\nQπ(st, at) =\nE\naτ ∼π,τ>t\n\" ∞\nX\ni=0\nγirt+i\n#\n(7)\nThere are two main categories of methods to ﬁnd the\noptimal policy. In the ﬁrst category, Qπ(s, a) is parameterized\nas Qθ\nπ(s, a) and the optimal action-value parameter vector θ\nis estimated in an iterative process. The optimal policy can\nbe deﬁned implicitly from Qπ(s, a). For example, the greedy\npolicy is the one that maximizes Qπ(s, a) in each step:\nat = arg max\na\n{Qπ(s, a)}\n(8)\nIn the second category, which is called policy optimization and\nhas been successfully applied to large-scale and continuous\ncontrol systems [4], the policy is parameterized directly as\nπθ(at|st) and the parameter vector of the optimal policy θ is\nestimated. The Trust Region Policy Method (TRPO) [5] is an\nexample of the second category of methods that guarantees\nmonotonic policy improvement and is designed to be scalable\nto large-scale settings. In each iteration of TRPO, a number\nof MDP trajectories are simulated (or actually experienced by\nthe agent) and θ is updated to improve the policy. A high level\ndescription of TRPO is shown in algorithm 1.\nIV. PROBLEM STATEMENT\nThere is a set of vehicles in a grid street plan area consisting\nof a certain number of intersections. For simplicity, we assume\nthat all the initial vehicle positions and the desired destinations\nare located at the intersections. There is a control agent for\nthe entire area. The agent’s task is to calculate the acceleration\ncommand for the vehicles in real-time (see Fig. 2). We assume\nthat there are no still or moving obstacles other than vehicles’\nor street boundaries.\nThe input to the agent is the real-time state of the vehicles\nwhich consists of their positions and speeds. We are assuming\nthat vehicles are point masses and their angular dynamics\nare ignored. However, to take the collision avoidance in the\nproblem formulation, we deﬁne a safe radius for each vehicle\nand no objects (vehicles or street boundaries) should be closer\nthan the safe radius to the vehicle.\nAlgorithm 1: High-Level description of Trust Region Optimization\nData: S\n▷Actual system or Simulation model\nπθ\n▷Parameterized Policy\nResult: θ∗\n▷Optimal parameters\n1 repeat\n2\nUse S to generate trajectories of the system using\ncurrent πθ;\n3\nPerform one iteration of policy optimization using\nMonte Carlo method to get θnew ;\n4\nθ ←θnew\n5 until no more improvements;\n6 return θ\nFig. 2: Intersection Management Problem. The goal of the problem is\nto navigate the vehicles from the sources to destinations in minimum\ntime with no collisions.\nThe objective is to drive all the vehicles to their respective\ndestinations in a way that the total travel time is minimized.\nFurthermore, no collision should occur between any two\nvehicles or a vehicle and the street boundaries.\nTo minimize the total travel time, a positive reward is\nassigned to the terminal state in which all the vehicles\napproximately reach the destinations within some tolerance. A\ndiscount factor γ strictly less than one is used. Therefore, the\nagent should try to reach the terminal state as fast as possible\nto maximize the discounted return. However, by using only\nthis reward, too many random walk trajectories are needed\nto discover the terminal state. Therefore, a negative reward is\ndeﬁned for each state, proportional to the total distance of the\nvehicles to their destinations as a hint of how far the terminal\nstate is. This negative reward is not in contradiction with the\nmain goal which is to minimize total travel time.\nTo avoid collisions, two different approaches can be\nconsidered: we can add large negative rewards for the collision\nstates or we can incorporate a collision avoidance mechanism\ninto the environment model. Our experiments show that\nthe ﬁrst approach makes the agent too conservative about\nmoving the vehicles to minimize the probability of collisions.\nThis might lead to extremely slow learning which makes it\ninfeasible. Furthermore, collisions are inevitable even with\nlarge negative rewards which limits the effectiveness of learned\npolicies in practice.\nFor the above mentioned reasons, the second approach is\nemployed, i.e. the safety mechanism that is used in practice is\nincluded in the environment deﬁnition. The safety mechanism\nis activated whenever two vehicles are too close to each other\nor a vehicle is too close to the street boundary. In these cases,\nthe vehicle built-in collision avoidance system will control the\nvehicle’s acceleration and the acceleration commands from\nthe RL agent are ignored as long as the distance is near\nthe allowed safe radius of the vehicle. In the agent learning\nprocess these cases are simulated in a way that the vehicles\ncome to a full stop when they are closer than the safe radius\nto another vehicle or boundary. By applying this heuristic\nin the simulation model, the agent should avoid any “near\ncollision” situations explained above because the deceleration\nand acceleration cycles take a lot of time and will decrease\nthe expected return.\nBased on the problem statement explained above, we can\ndescribe the RL formulation in the rest of the subsection. The\nstate is deﬁned as the following vector:\nst =\n\u0010\nx1\nt, y1\nt , v1\nxt, v1\nyt, . . . , xn\nt , yn\nt , vn\nx t, vn\ny t\n\u0011⊺\n(9)\nwhere (xi\nt, yi\nt) and (vi\nxt, vi\nyt) are the position and speed of\nvehicle i at time t. The action vector is deﬁned as:\nat =\n\u0010\na1\nxt, a1\nyt, . . . , an\nxt, an\ny t\n\u0011⊺\n(10)\nwhere (ai\nxt, ai\nyt) is the acceleration command of vehicle i at\ntime t. The reward function is deﬁned as:\nr(s) =\n(\n1\nif\n∥(xi −di\nx, yi −di\nx)⊺∥< η (1 ≤i ≤n)\n−α Pn\ni=1∥(xi −di\nx, yi −di\nx)⊺∥\notherwise\n(11)\nwhere (di\nx, di\ny) is the destination coordinates of vehicle i, η is\nthe distance tolerance and α is a positive constant.\nAssuming no collision occurs, the state transition equations\nfor the environment are deﬁned as follows:\nxi\nt+1 = satx,x(xi\nt + hvx\ni\nt)\nyi\nt+1 = saty,y(yi\nt + hvy\ni\nt)\nvx\ni\nt+1 = satvm,vm(vx\ni\nt + hax\ni\nt)\nvy\ni\nt+1 = satvm,vm(vy\ni\nt + hay\ni\nt)\n(12)\nwhere h is the sampling time, (x, x, y, y) deﬁnes area limits,\nvm is the maximum speed and satw,w(.) is the saturation\nfunction deﬁned as:\nsatw,w(x) =\n\n\n\n\n\nw\nx ≤w\nw\nx ≥w\nx\notherwise.\n(13)\nTo model the collisions, we should check certain conditions\nand set the speed to zero. A more detailed description of\ncollision modeling is presented in Algorithm 2.\nA. Solving the AIM problem using TRPO\nThe simulation model can be implemented based on the RL\nformulation described in Section IV. To use TRPO, we need\na parameterized stochastic policy, πθ(at|st), in addition to the\nsimulation model. The policy should specify the probability\ndistribution for each element of the action vector deﬁned in\n(10) as a function of the current state st.\nWe have used the sequential deep neural network (DNN)\npolicy representation as described in [5]. The input layer\nreceives the state containing the position and speed of the\nvehicles (deﬁned in (9)). There are a number of hidden layers,\neach followed by tanh activation functions [10]. Finally, the\noutput layer generates the mean of a gaussian distribution for\neach element of the action vector.\nTo execute the optimal policy learned by TRPO in each\nsampling time, the agent calculates the forward-pass of DNN\nusing the current state. Next, assuming that all the action\nelements have the same variance, the agent samples from the\naction gaussian distributions and applies the sampled actions\nto the environment as the vehicle acceleration commands.\nV. EVALUATION\nA. Baseline Method\nTo the best of our knowledge there is no other solution\nproposed for the ﬁne-grained acceleration AIM problem\nintroduced in this paper. Therefore, we use conventional\noptimization methods to study how close the proposed solution\nis to the optimal solution. Furthermore, we will see that the\nconventional optimization is able to solve the AIM problem\nonly for very small-sized problems. This conﬁrms that the\nproposed RL-based solution is a promising alternative to the\nconventional methods.\nTheoretically, the best solution to the problem deﬁned\nin section IV can be obtained if we reformulate it as a\nconventional optimization problem. The following equations\nand inequalities describe the AIM optimization problem:\nat\n∗= arg max\nat\nT −1\nX\nt=0\nn\nX\ni=1\n∥(xi\nt −di\nx, yi\nt −di\nx)⊺∥\n(14)\ns. t.\nx ≤xi\nt ≤x\n(1 ≤i ≤n)\n(15)\ny ≤yi\nt ≤y\n(1 ≤i ≤n)\n(16)\nvm ≤vx\ni\nt ≤vm\n(1 ≤i ≤n)\n(17)\nvm ≤vy\ni\nt ≤vm\n(1 ≤i ≤n)\n(18)\nAlgorithm 2: State Transition Function\nData: st\n▷State at time t\nat\n▷Action at time t\nResult: st+1\n▷State at time t + 1\n1 axi\nt ←satam,am(axi\nt) ;\n2 ayi\nt ←satam,am(ayi\nt) ;\n3 st+1 ←updated state using (12) ;\n4 vc1 ←ﬁnd all the vehicles colliding with street\nboundaries ;\n5 speed elements of vc1in st+1 ←0 ;\n6 location elements of vc1in st+1 ←closest point on the\nstreet boundary with the margin of ϵ ;\n7 vc2 ←ﬁnd all the vehicles colliding with some other\nvehicle ;\n8 speed elements of vc2in st+1 ←0 ;\n9 location elements of vc2in st+1 ←pushed back location\nwith the distance of 2× safe radius to the collided\nvehicle;\n10 return st+1\n(a)\n(b)\nFig. 3: Initial setup of each episode. Small circles are the sources and\nbig circles are the destinations. (a) small example (b) large example\nam ≤ax\ni\nt ≤am\n(1 ≤i ≤n)\n(19)\nam ≤ay\ni\nt ≤am\n(1 ≤i ≤n)\n(20)\n−⌊N/2⌋≤ri\nt ≤⌊N/2⌋\n(1 ≤i ≤n, ri\nt ∈Z)\n(21)\n−⌊M/2⌋≤ci\nt ≤⌊M/2⌋\n(1 ≤i ≤n, ci\nt ∈Z)\n(22)\nxi\n0 = si\nx, yi\n0 = si\ny\n(1 ≤i ≤n)\n(23)\nxi\nT −1 = di\nx, yi\nT −1 = di\ny\n(1 ≤i ≤n)\n(24)\nvx\ni\n0 = 0, vy\ni\n0 = 0\n(1 ≤i ≤n)\n(25)\nxi\nt+1 = xi\nt + vx\ni\nt.h\n(1 ≤i ≤n)\n(26)\nyi\nt+1 = yi\nt + vy\ni\nt.h\n(1 ≤i ≤n)\n(27)\nvx\ni\nt+1 = vx\ni\nt + ax\ni\nt.h\n(1 ≤i ≤n)\n(28)\nvy\ni\nt+1 = vy\ni\nt + ay\ni\nt.h\n(1 ≤i ≤n)\n(29)\n(xi\nt −xj\nt)2 + (yi\nt −yj\nt )2 ≥(2R)2\n(1 ≤i < j ≤n)\n(30)\n|xi\nt −ci\nt.bw| ≤( l\n2 −R) or\n|yi\nt −ri\nt.bh| ≤( l\n2 −R) (1 ≤i ≤n)\n(31)\nwhere ri\nt and ci\nt are the row number and column number\nof vehicle at time t, respectively, assuming the zone is a\nperfect rectangular grid; N and M are the number of rows and\ncolumns, respectively; bw and bh are block width and block\nheight; l is the street width; R is the vehicle clearance radius;\nT is number of sampling times; and (si\nx, si\ny) is the source\ncoordinates of vehicle i.\nIn the above mentioned problem setting, (15) to (20) are\nthe physical limit constraints. (23) to (25) describe the initial\nand ﬁnal conditions. (26) to (29) are dynamic constraints. (30)\nis the vehicle-to-vehicle collision avoidance constraint and\nﬁnally (31) is the vehicle-to-boundaries collision avoidance\nconstraint.\nFig. 4: Learnt policy by (left)RL agent and (right)the baseline method for the small example\nFig. 5: Learnt policy by the AIM agent at different iterations of the training. left: begging of the training, middle: just after the fast learning\nphase in Fig. 7. right: end of the training.\nThe basic problem with the above formulation is that\nconstraint (30) leads to a non-convex function and convex\noptimization algorithms cannot solve this problem. Therefore,\na Mixed-Integer Nonlinear Programming (MINLP) algorithm\nshould be used to solve this problem. Our experiments\nshow that even a small-sized problem with two vehicles and\n2×2 grid cannot be solved with an MINLP algorithm, i.e.\nAOA[11]. To overcome this issue, we should reformulate the\noptimization problem using 1-norm and introduce new integer\nvariables for the distance between vehicles using the ideas\nproposed in [12].\nTo achieve the best convergence and execution time by\nusing a Mixed-integer Quadratic Programming (MIQP), the\ncost function and all constraints should be linear or quadratic.\nFurthermore, the “or” logic in (31) should be implemented\nusing integer variables. The full MIQP problem can be written\nas the following equations and inequalities:\nat\n∗= arg max\nat\nT −1\nX\nt=0\nn\nX\ni=1\n(xi\nt −di\nx)2 + (yi\nt −di\nx)2 (32)\ns. t.\n(15) to (29)\nbx\ni\nt, by\ni\nt ∈{0, 1}\n(1 ≤i ≤n)\n(33)\nbx\ni\nt + by\ni\nt ≥1\n(1 ≤i ≤n)\n(34)\ncx\ni,j\nt ,cy\ni,j\nt , dx\ni,j\nt , dy\ni,j\nt\n∈{0, 1}\n(1 ≤i < j ≤n)\n(35)\ncx\ni,j\nt +cy\ni,j\nt\n+ dx\ni,j\nt\n+ dy\ni,j\nt\n≥1\n(1 ≤i < j ≤n)\n(36)\nxi\nt −xj\nt ≥2Rcx\ni,j\nt\n−M(1 −cx\ni,j\nt )\n(1 ≤i < j ≤n)\n(37)\nxi\nt −xj\nt ≤−2Rdx\ni,j\nt\n+ M(1 −dx\ni,j\nt )\n(1 ≤i < j ≤n)\n(38)\nyi\nt −yj\nt ≥2Rcy\ni,j\nt\n−M(1 −cy\ni,j\nt )\n(1 ≤i < j ≤n)\n(39)\nyi\nt −yj\nt ≤−2Rdy\ni,j\nt\n+ M(1 −dy\ni,j\nt )\n(1 ≤i < j ≤n)\n(40)\nxi\nt −ci\ntbw ≤( l\n2 −R)bx\ni\nt + M(1 −bx\ni\nt)\n(1 ≤i ≤n) (41)\nxi\nt −ci\ntbw ≥−( l\n2 −R)bx\ni\nt −M(1 −bx\ni\nt)\n(1 ≤i ≤n)\n(42)\nyi\nt −ci\ntbw ≤( l\n2 −R)by\ni\nt + M(1 −by\ni\nt)\n(1 ≤i ≤n) (43)\nyi\nt −ci\ntbw ≥−( l\n2 −R)by\ni\nt −M(1 −by\ni\nt)\n(1 ≤i ≤n)\n(44)\nwhere M is a large positive number.\n(37) to (40) represent the vehicle-to-vehicle collision\navoidance constraint using 1-norm:\n∥(xi\nt, yi\nt)⊺−(xj\nt, yj\nt )⊺∥1 ≥2R\n(45)\nfor any two distinct vehicles i and j. This constraint is\nequivalent to the following:\n|xi\nt −xj\nt| ≥2R or |yi\nt −yj\nt | ≥2R\n∀t, (1 ≤i < j ≤n)\n(46)\nThe absolute value function displayed in (46) should be\nreplaced by logical “or” of two linear conditions to avoid\nnonlinearity. Therefore we have the following four constraints\nrepresented by (37) to (40):\nxi\nt −xj\nt ≥2R or xi\nt −xj\nt ≤−2R or\nyi\nt −yj\nt ≥2R or yi\nt −yj\nt ≤−2R\n∀t, (1 ≤i < j ≤n)\n(47)\n(35) implements the “or” logic required in (47).\n(41) to (44) describe the vehicle-to-boundaries collision\navoidance constraint:\n|xi\nt −ci\ntbw| ≤( l\n2 −R)bx\ni\nt or\n|yi\nt −ri\ntbw| ≤( l\n2 −R)by\ni\nt\n∀t, (1 ≤i ≤n)\n(48)\nwhich is equivalent to:\n(xi\nt −ci\ntbw ≤( l\n2 −R)bx\ni\nt and xi\nt −ci\ntbw ≥−( l\n2 −R)bx\ni\nt) or\n(yi\nt −ri\ntbw ≤( l\n2 −R)by\ni\nt and yi\nt −ri\ntbw ≥−( l\n2 −R)by\ni\nt)\n∀t, (1 ≤i ≤n)\n(49)\nThe “or” logic in this constraint is realized in (34).\nWe will show in the next subsection that the explained\nconventional optimization formulation is not feasible except\nfor very small-sized problems. Another limitation that makes\nthe conventional method impractical is that this formulation\nworks only for a perfect rectangular grid. However, the\nproposed RL method in this paper can be extended to arbitrary\nstreet layouts.\nB. Simulation results\nThe implementation of the TRPO in rllab library [4] is used\nto simulate the RL formulation of the AIM problem described\nin Section IV. For this purpose, the AIM state transition and\nreward calculation are implemented as an OpenAI Gym [13]\nenvironment.\nThe neural network used to approximate the policy is an\nMLP which consists of three hidden layers. Each hidden layer\nhas 100 nodes (Fig. 6). Table I lists the parameters for the\nsimulation. To speed up simulation, normalized units are used\nfor the physical properties of the environment instead of real-\nworld quantities.\nFig. 3 shows the small and large grid plans used for the\nsimulation. The small and large circles represent the source\nand destination locations, respectively. The vehicles are placed\nat the intersections randomly at the beginning of each episode.\nTABLE I: Parameter value settings for the experiment\nParameter\nValue\ndiscount factor(γ)\n0.999\ndistance reward penalty factor(α)\n0.1\ndistance tolerance(η)\n0.05\nmaximum speed(vm)\n0.8\nmaximum acceleration(am)\n30\nsampling time(h)\n10 (ms)\nmaximum episode length\n200\nvehicle safe radius\n0.02\nInput\nLayer\nMLP \nLayers\nOutput\nLayer\nSampling\nPosition and Speed data\nAcceleration Commands\n4x7 units\n100 units\n100 units\n100 units\n2x7 units\nFig. 6: Neural network used in the simulations.\nTABLE II: Total travel time obtained by baseline method and\nproposed method\nBaseline Method\nProposed Method\nSmall example\n1.79 (s)\n2.43 (s)\nLarge Example\nno solution\n11.2 (s)\nThe destinations are also chosen randomly. When the simulator\nis reset, the same set of source and destination locations are\nused.\nThe small grid can be solved both by the baseline method\nand by the proposed RL method. However, the large grid\ncan only be solved by the RL method because the MIQP\nalgorithm could not ﬁnd a feasible solution (which is not\noptimal necessarily) and was stopped after around 68 hours\nand using 21 GB of system memory. On the other hand, the\nRL method can solve the problem using 560 MB of system\nmemory and 101 MB of GPU memory.\nTable II and Fig. 4 show the comparison of proposed RL\nand baseline method results. In Table II the total travel time\nis provided for both methods and Fig. 4 shows the vehicles’\ntrajectories by running the navigation policy obtained by both\nsolutions for the small examples.\nThe learning curve of the RL agent which is the expected\nreturn vs the training epoch number is shown in Fig. 7 for the\nlarge grid example. This ﬁgure shows that the learning rate is\nhigher at the beginning which corresponds to the stage where\nin the agent is learning the very basics of driving and avoiding\ncollisions, but improving the policy towards the optimal policy\ntakes considerably more time. The increase in learning occurs\nafter two epochs when the agent discovers the policy that\nsuccessfully drives all the vehicles to the destination and the\npositive terminal reward is gained. Moreover, the trajectories\nof vehicles are depicted in Fig. 5 at three stages of the learning\nprocess, i.e. at the early stage, at epoch 2 where the learning\ncurve slope suddenly decreases, and the end of the training.\nThe total number of “near collision” incidents discussed in\nSection IV is shown in Fig. 8. Fig. 9 shows the total travel\ntime as a function of training iteration.\nVI. CONCLUSION\nIn this paper, we have shown that Deep RL can be a\npromising solution for the problem of intelligent intersection\nmanagement in local road settings where the number of\nvehicles is limited and ﬁne-grained acceleration control and\n0\n2\n4\n6\n8\n10\n12\n14\n16\nEpoch Number\n90000\n80000\n70000\n60000\n50000\n40000\n30000\n20000\n10000\n0\nDiscounted Return\nFig. 7: Learning curve of the AIM agent for large grid example.\nThe discounted return is always a negative value because the return\nis the accumulated negative distance rewards and there is only one\npositive reward in the terminal state in which all the vehicles are at\nthe destinations.\nFig. 8: Number of near collision incidents vs training iteration\nnumber.\nmotion planning can lead to a more efﬁcient navigation of\nthe autonomous vehicles. We proposed an RL environment\ndeﬁnition in which collisions are avoided using a safety\nmechanism. Using this method instead of large penalties for\ncollision in the reward function, the agent can learn the optimal\npolicy faster and the learned policy can be used in practice\nwhere the safety mechanism is actually implemented. The\nexperiments show that the conventional optimization methods\nare not able to solve the problem with the sizes that are\nsolvable by the proposed method.\nSimilar to the learning process of human beings, the main\nbeneﬁt of the RL approach is that an explicit mathematical\nmodeling of the system is not required and, more importantly,\nthe challenging task of control design for a complex system is\neliminated. However, since the automotive systems demand\na high safety requirement, training of the RL agent using\na simulation model is inevitable in most cases. However,\ndeveloping a simulation model for a system is considerably\nsimpler task compared to explicit modeling especially for\nsystems with uncertainty.\nWhile the work at hand is a promising ﬁrst step towards\nusing RL in autonomous intersection management, a number\nof potential improvements can be mentioned that might be\ninteresting to address in future work. First, the possibility\nof developing pre-trained DNNs similar to the works in\nFig. 9: Total travel time of all the vehicles vs training iteration\nnumber.\nother mainstream deep learning domains that reduce learning\ntime can be a studied in future. Furthermore, a more\nadvanced rewards system that includes gas usage penalties is\nanother track to developing a practical intelligent intersection\nmanagement algorithm.\nVII. ACKNOWLEDGEMENT\nWe would like to thank Nvidia for their generous hardware\ndonation. This work was supported in part by the National\nScience Foundation under NSF grant number 1563652.\nREFERENCES\n[1] M. Hausknecht, T.-C. Au, and P. Stone, “Autonomous intersection\nmanagement:\nMulti-intersection\noptimization,”\nin\n2011\nIEEE/RSJ\nInternational Conference on Intelligent Robots and Systems.\nIEEE,\n2011, pp. 4581–4586.\n[2] K. Dresner and P. Stone, “A multiagent approach to autonomous\nintersection management,” Journal of artiﬁcial intelligence research,\nvol. 31, pp. 591–656, 2008.\n[3] C. Frese and J. Beyerer, “A comparison of motion planning algorithms\nfor cooperative collision avoidance of multiple cognitive automobiles,”\nin Intelligent Vehicles Symposium (IV), 2011 IEEE.\nIEEE, 2011, pp.\n1156–1162.\n[4] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel,\n“Benchmarking deep reinforcement learning for continuous control,”\narXiv preprint arXiv:1604.06778, 2016.\n[5] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust\nregion policy optimization,” CoRR, abs/1502.05477, 2015.\n[6] M. N. Mladenovi´c and M. M. Abbas, “Self-organizing control\nframework\nfor\ndriverless\nvehicles,”\nin\n16th\nInternational\nIEEE\nConference on Intelligent Transportation Systems (ITSC 2013).\nIEEE,\n2013, pp. 2076–2081.\n[7] I. H. Zohdy, R. K. Kamalanathsharma, and H. Rakha, “Intersection\nmanagement for autonomous vehicles using icacc,” in 2012 15th\nInternational IEEE Conference on Intelligent Transportation Systems.\nIEEE, 2012, pp. 1109–1114.\n[8] A. A. Malikopoulos, C. G. Cassandras, and Y. J. Zhang, “A decentralized\noptimal control framework for connected and automated vehicles at\nurban intersections,” arXiv preprint arXiv:1602.03786, 2016.\n[9] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press Cambridge, 1998, vol. 1, no. 1.\n[10] B.\nKarlik\nand\nA.\nV.\nOlgac,\n“Performance\nanalysis\nof\nvarious\nactivation functions in generalized mlp architectures of neural networks,”\nInternational Journal of Artiﬁcial Intelligence and Expert Systems,\nvol. 1, no. 4, pp. 111–122, 2011.\n[11] M. Hunting, “The aimms outer approximation algorithm for minlp,”\nParagon Decision Technology, Haarlem, 2011.\n[12] T. Schouwenaars, B. De Moor, E. Feron, and J. How, “Mixed integer\nprogramming for multi-vehicle path planning,” in Control Conference\n(ECC), 2001 European.\nIEEE, 2001, pp. 2603–2608.\n[13] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “Openai gym,” 2016.\n",
  "categories": [
    "cs.AI",
    "cs.RO",
    "cs.SY"
  ],
  "published": "2017-05-30",
  "updated": "2017-05-30"
}