{
  "id": "http://arxiv.org/abs/1705.11023v1",
  "title": "Criticality & Deep Learning II: Momentum Renormalisation Group",
  "authors": [
    "Dan Oprisa",
    "Peter Toth"
  ],
  "abstract": "Guided by critical systems found in nature we develop a novel mechanism\nconsisting of inhomogeneous polynomial regularisation via which we can induce\nscale invariance in deep learning systems. Technically, we map our deep\nlearning (DL) setup to a genuine field theory, on which we act with the\nRenormalisation Group (RG) in momentum space and produce the flow equations of\nthe couplings; those are translated to constraints and consequently interpreted\nas \"critical regularisation\" conditions in the optimiser; the resulting\nequations hence prove to be sufficient conditions for - and serve as an elegant\nand simple mechanism to induce scale invariance in any deep learning setup.",
  "text": "Criticality & Deep Learning II: Momentum Renormalisation Group\nDan Oprisa\ndan.oprisa@critical.ai\nPeter Toth\npeter.toth@critical.ai\nCriticalAI\nhttp://www.critical.ai\nAbstract\nGuided by critical systems found in nature\nwe develop a novel mechanism consisting of\ninhomogeneous polynomial regularisation via\nwhich we can induce scale invariance in deep\nlearning systems.\nTechnically, we map our\ndeep learning (DL) setup to a genuine ﬁeld\ntheory, on which we act with the Renormali-\nsation Group (RG) in momentum space and\nproduce the ﬂow equations of the couplings;\nthose are translated to constraints and con-\nsequently interpreted as ”critical regularisa-\ntion” conditions in the optimiser; the result-\ning equations hence prove to be suﬃcient con-\nditions for - and serve as an elegant and sim-\nple mechanism to induce scale invariance in\nany deep learning setup.\n1\nIntroduction\nThe ubiquity of self similarity stemming from univer-\nsal scale invariant behavior displayed by virtually all\nsystems in various disciplines serves as motivation of\nthe current research; starting from biological systems\n[1, 2], including the brain [3], physical systems [4]\nand even on large cosmological scales [5] self similarity\nis encountered. There are various underlying mecha-\nnisms producing the emergent scale invariance, some\nof which rely on tunable parameters [6, 7] and some\nof which are self-organised [8]. Hence the conjecture is\nnear, that self similarity, scale invariance, power law\ndistribution, criticality are all just facets, emergent\npatterns of underlying symmetries at heart of com-\nplex systems. In the following we will also treat those\ninterchangeably as they are just aspects of a deeper\nunderlying structure.\nThe brain on the other side, is arguably one of the most\ncomplex systems known to us, displaying architectural\nand functional level power law patterns.\nGiven the\nuniversality of power law behavior and the biological\nﬁndings about the brain, it seems almost necessary\nto consider those emergent laws as a necessity for in-\ntelligence and hence incorporate them (or the under-\nlying generating mechanism) into the respective DL\nsystems.\nIn order to do so, we make use of a very powerful\ntool, Wilson’s Renormalisation Group (RG) approach\n[9, 10], carried out in momentum space; the frame-\nwork was developed in the 70’s on ﬁeld theories deal-\ning eﬀectively with systems exhibiting scale invariant\nbehavior.\nThe subject of criticality in DL systems has been vastly\naddressed, see e.g. [11]. The connection with the RG\nwas proposed in [12], and implemented via block spin\nrenormalisation e.g. in [13]. To our knowledge this\nis the ﬁrst attempt to act with RG on the theory in\nmomentum space.\nThe article is organised as follows: in section 2.1 we\npresent a high-view, intuitive summary of the RG\nconcept, dealing with the transition between diﬀerent\nscales and emergent properties of the system; in sec-\ntion 2.2 the connection between the DL system and\na genuine ﬁeld theory is made; here we map the fully\nconnected graph to an eﬀective theory of ﬁelds encoded\nin the Hamiltonian density; in the subsequent section\nwe formulate the RG in momentum space and act with\nthe group on the eﬀective ﬁeld theory; this causes the\nHamiltonian to ”ﬂow”, tracing a path in the coupling\nspace, along which the couplings themselves change;\nthe latter change in the couplings is encoded in general\ndiﬀerential equations as presented in section 2.4, which\nwill then be translated into constraining conditions for\nthe connection weights of the system at hand in section\n2.5. A simple measure for criticality, the 2-point cor-\nrelation function is presented in section 2.6, which we\ncan compute exactly for the Gaussian system; it then\nserves as a tool to probe the DL architecture at crit-\nicality. After addressing the whole theoretical setup,\nwe implement the criticality constraints in section 3.\nWe conclude this article in section 4 and hint at future\nwork.\narXiv:1705.11023v1  [cond-mat.stat-mech]  31 May 2017\nCriticality & Deep Learning II: Momentum Renormalisation Group\n2\n2\nThe Renormalisation Group\ntechnique\n2.1\nAn RG primer\nThe Renormalization Group (RG) technique has its\norigin around problems dealing with scaling in eﬀective\nﬁeld theories; as such, it is universally useful whenever\nthe problem at hand shows scale invariance. In our\nparticular case, the ﬂuctuations (and with them the\ncorrelation) of a ﬁeld φ are self-similar at various scales\nwhen the system is located at a special locus in the\nspace of coupling parameters - called criticality. Hence\nwe can make use of the self-similarity of the system and\nimplement a renormalisation scheme, the Wilson RG\n[9, 10], which will produce equations and from them\nconsistency constraints on couplings of the system.\nPictorially, the problem at hand and its solution can\nbe understood through an analogy to dynamical equa-\ntions and their fractal behavior [14]; given a real (it-\nerative) map Mµ : R →R, depending on some one-\ndimensional parameter µ ∈R, we contemplate its im-\nage in R.\nBy carefully tuning the parameter µ, we\ncan navigate between trivially converging solutions,\nmulti-modal oscillations and self-similar behavior. At\nthe critical value µ∗, the map being scale invariant,\nits image will resemble some fractal, in our case some\none-dimensional fractal curve; at a given scale, we can\nidentify a (small) recurrent pattern, of given size b; this\nis the fractal motive of the image, repeating itself at\ndiﬀerent scales. Zooming out from our starting point,\nwe will change scale and also change resolution (by\nrescaling all lengths with b) at the new scale, in order\nto be able to compare present picture and recover the\npattern we had previously discovered;\nTo achieve that, conceptually the steps to be imple-\nmented are as follows:\n(1) assume system displays scale invariance\nthis is ultimately what we want to achieve by care-\nfully choosing our couplings\n(2) probe system (Partition function) at slightly dif-\nferent scale\nzoom out by a factor of b to search for the ”pat-\ntern” at new scale\n(3) impose structural equality of Hamiltonian, cf. (1)\n(4) absorb changes and renormalisation of ﬁelds into\ncouplings\nchange resolution at new scale to ensure compa-\nrability to starting point\n(5) solve for the ﬁxed points of the mapping, which\ndetermines criticality\nWe regard our system as a scale-dependent eﬀective\naction functional - the partition function, encoded in\nFigure 1: Block-spin momentum space analogy, repro-\nduced from K. Huang, Statistical Mechanics\nthe functional integral over the Hamiltonian H; the\nlatter will depend on ﬁelds and couplings r, g, u, · · · .\nDuring renormalisation, the RG will act on H(r, g, u)\nas\nRbH(r, g, u) = H′(r′, g′, u′)\n(1)\nwhere Rb denotes the renormalisation operator and b is\nthe scaling parameter. Eq. (1) eﬀectively describes the\ntrace in the space of the couplings which is generated\nby the ﬂowing Hamiltonian. Step (5) above singles out\nthe special point in the couplings’ space where\nH∗= RbH∗\n(2)\nholds, eﬀectively meaning the system is invariant un-\nder scale change.\nMoving now to practice, the program described above\nis implement the following way:\nCoarse grain\nAs depicted in ﬁg.\n1 we represent our system as a\ncollection of units in the 2d place interacting with\neach other via couplings; in the conﬁguration space\nwe group units together into adjacent blocks of say 2d\nCriticality & Deep Learning II: Momentum Renormalisation Group\n3\nunits (2 per each site of the block) and consider their\nproperties as a stand-alone unit:\n¯φ(x) = 1\nbd\nZ\nblock\nddyφ(y)\n(3)\nas depicted on the left in ﬁg. 1. This step will reduce\ndegrees of freedom from N to N ′ = N/4, thus bd =\nN\nN′ = 4.\nIn the Wilsonian picture (which is usually the one em-\nployed in practical computations), all calculations are\nperformed in momentum space; the calculations and\nthe pictures are completely dual, however slightly more\ninvolved in the momentum picture; the relabeling of\nunits into groups then corresponds to integrating out\nhighest wave numbers q>; the high wave number obvi-\nously produce the highest resolution in the system and\nhence correspond to the smallest parts of the system -\nthe very units; integrating out those wave numbers will\nbe performed within the shell between Λ/b ≤q ≤Λ,\nthe latter being the natural cutoﬀof our eﬀective ﬁeld\ntheory, see ﬁg. 2\nRescale\nIn the newly regrouped picture we restore the original\nresolution by blowing up all lengths to the original\nscale\nx′ = x\nb\n(4)\nor in momentum space\nq′ = bq\n(5)\nThis is the equivalent of step (2) above, where we ef-\nfectively ”zoom out” by adjusting lengths to be com-\nparable with original scale;\nRenormalise\nAfter rescaling, the newly produced Hamiltonian is re-\nquired to match in structure the starting one; all scal-\ning factors from ﬁelds will be absorbed into couplings\nwhich eﬀectively causes them to shift, or ”ﬂow”. Ulti-\nmately this produces equations for them, whose ﬁxed\npoint solution will single out the fully scale invariant\nHamiltonian.\n2.2\nEﬀective ﬁeld theory\nThe Wilson RG works in the framework of eﬀective\nﬁeld theories; in this chapter we will map our feed for-\nward network setup to an eﬀective ﬁeld theory. Start-\ning point is the mean ﬁeld consistency equation for a\nReLU network as it has been computed in [17]:\nFigure 2:\nDiﬀerent functional paths of the Hamil-\ntonian with shell of integration, between cutoﬀand\nrenormalised momentum\nVi = tanh β(P\nk wikVk/N + h)\n(6)\nHere Vk are mean ﬁeld variables, which are polyno-\nmials in wik and h, once the consistency relations are\nsolved; eq. (6) stems from the stationarity condition\nimposed on the Hamiltonian\nHV = β\n2N\nP\nij wijViVj\n(7)\n−\nX\ni\nln cosh β(P\nj wijVj/N + h)\nwhich is summed over all its states with the full par-\ntition function\nZ = c\nN\nY\nk=1\nZ\ndVk e−HV (w,h)\n(8)\nThe mean ﬁeld equation will blow up at criticality, for\nh →0 and speciﬁc values wik, while the temperature\napproaches T →Tc. Given the non-linear diﬀerential\nnature of the coupled consistency equations and the\nmany limits which need to be taken, a solution in this\ncase is rather cumbersome to obtain;\nThe RG technique however is designed to probe the\nsystem exactly at criticality, operating on a genuine\nﬁeld Hamiltonian. As shown in detail in appendix A\nwe lift our eﬀective variables to a genuine ﬁeld theory,\nby eﬀectively promoting variables to ﬁelds (densities):\nCriticality & Deep Learning II: Momentum Renormalisation Group\n4\nVk →φk(t)\n(9)\nZ\ndVk →\nZ\nDφk(t)\nHV →\nZ\nddxHφ(w, h)\nThe ﬁeld functions depend on d-dimensional spacial\ncoordinates x; the integral of the partition function\nthen morphs into a functional integral\nZ = c\nN\nY\nk=1\nZ\nDφk e−Hφ(w,h)\n(10)\nwith the eﬀective Hamiltonian\nHφ =\nZ\nddx β\n2N\nX\nkl\n[wklφkφl + δkl(∇xφk)(∇xφl)]\n−\nX\nk\nln cosh β(P\nl wklφl/N + h) ]\n(11)\n2.3\nThe action of RG\nAs explained in section 2.1 the RG transformation Rb\nwill map the Hamiltonian (11) structurally onto itself\nwhile scaling the parameters, hence tracing out the\nﬂow of the Hamiltonian in space spanned by the cou-\npling constants.\nObviously we have strong non-linearities in our Hamil-\ntonian, which need to be treated perturbatively; tak-\ning only the leading contributions from the ln cosh-\nterm, we obtain the Gaussian model (in vectorial,\ncoordinate-free notation):\nHφ =\nZ\nddx [ 1\n2r · φ · φ + 1\n2g · φx · φx −u · φ ] (12)\nHere we have deﬁned\nr ≡rkl = β\nN (wkl −β\nN w2\nkl),\n(13)\ng ≡gkl = β\nN δkl,\nu ≡uk = β2\nN h P\nl wkl,\nφx ≡∇xφk\nand dropped the constant term h2. Just for the sake\nof clarity, we have deﬁned the bold constants r, g, u\ncoordinate free; they are understood as (bi-)linear op-\nerators o ≡o( , ) which take in vectors (in our case\nφ) and produce a scalar. Obviously from eq. (11) we\nknow we have a collection of N ﬁelds φi which interact\nvia non-constant weights wij. We will use this opera-\ntor, coordinate free language during our derivation for\nthe RG equations, and only adopt coordinate notation,\nonce we go to the component level.\nThe Gaussian model will be solved via expanding the\nfunctions φ(x) wrt.\na suitable base such that the\nHamiltonian (12) will be diagonalised; as explained\nin appendix B, the base turns out to be exp(ikx), i.e.\nthe Fourier basis.\nIntroducing the Fourier transformed ﬁelds\nφ(x) =\nZ\nddq\n(2π)d φ(q)eiqx\n(14)\nφ(q) =\nZ\nddx φ(t)e−iqx\nand moving into momentum space we obtain (see eq.\n54)\nHφ =\nZ\nddq\n(2π)d\n1\n2(r + gq2)φ(q) · φ(−q) −u · φ(q = 0)\n(15)\nWe proceed now with the main three steps of the RG\nprocess as explained in section 2.1\nCoarse grain\nWe choose a coarse graining resolution b via which we\ndeﬁne the UV momentum region to be integrated out,\nas Λ/b < |k| < Λ, and we separate the ﬁelds into\nhigh/low momentum regions\nm(q) =\n\u001am<(q), 0 < |q < Λ/b\nm>(q), Λ/b < |q < Λ/b\n(16)\nWith that, partition function takes the form\nZ =\nZ\nDm<(q)\nZ\nDm>(q) e−βH[m<,m>]\n(17)\nThe low/high frequency ﬁelds decouple nicely in the\nHamiltonian in (17) and hence the high-frequency part\ncan be integrated out to:\nZ = Z>\nZ\nDm<(q)\n(18)\nexp\n\"\n−\nZ Λ/b\n0\nddq\n(2π)d\nr + gq2\n2\nm<(q)2 + um<(0)\n#\nCriticality & Deep Learning II: Momentum Renormalisation Group\n5\nwhile Z> = exp[−L\nR Λ\nΛ/b\ndq\n(2π)d ln(r + gq2)], where L is\na numerical constant related to the volume of integra-\ntion.\nRescale\nThe integral for Z<, representing the bulk of the\nmodes, is now almost identical to the original one in\nthe partition function Z, except for the upper limit of\nintegration; by rescaling q →q′ = bq we restore the\noriginal cutoﬀΛ; however this will result in rescaling\nall quantities dependent on q;\nRenormalise\nThis is the ﬁnal step in the program, which renor-\nmalises the ﬁelds,\naka the order parameter via\nm(x′) →m′(x′) = m<(x′)/z; from a pictorial point\nof view this will restore the resolution such that we can\ncompare quantities from this scale with the quantities\nbefore scaling;\nThe partition reads now\nZ = Z>\nZ\nDm′(q′)e−βH′[m′(q′)]\n(19)\nwith the term in the exponential βH′ given by\nexp\n\"\n−\nZ Λ\n0\ndq′\n(2π)d b−dz2 r + gb−2q′2\n2\nm′(q′)2 + zum′\n#\n(20)\nAt a glance we recognize that the singular point (r, u =\n{0, 0}) is already a solution for the stationarity of the\ncouplings; hence, in order to make the system scale\ninvariant for this speciﬁc case, we use the degree of\nfreedom of our renormalisation to keep g = g′, which\nimplies z = b1+d/2.\n2.4\nFlow of the coupling constants\nBy having fully determined the renormalisation free-\ndom we obtain now the recursion relations for the cou-\nplings\nr′ = b2 r\n(21)\nu′ = b(d+2)/2 u\nAssuming our ﬁrst coarse graining step small, i.e. b ≈\n1, we linearise and expand to ﬁrst order\nbn = (1 + dτ)n ≈1 + ndτ\n(22)\nand we obtain running equations of couplings for our\nsystem\ndr\ndτ = 2r\n(23)\ndu\ndτ = d + 2\n2\nu\ndg\ndτ = 0\nHowever, we remember the original coupling constants\nwij, h relate to r, g, u via eq. (13). Combining (23),\n(13) and going coordinate free, we ﬁnally obtain the\nfamous running equations of the couplings\nd\ndτ (w −β\nN ww) = 2(w −β\nN ww)\n(24a)\nd\ndτ hw1v = d + 2\n2\nhw1v\n(24b)\nwhere we have introduced the linear operator\nw1v = (P\nl wkl)\n(25)\nwhich is the contraction of wik with the one vector,\n1v = (1, . . . , 1), and hence eﬀectively summing over\nthe contracted index.\nWe carry out the diﬀerentiation, denote dw/dτ = wτ\nand solve for wτ in (24a), after which we solve for hτ\nin (24b), and simplify to\nwτ = 2w(1 −wβ/N) [1 −2wβ/N]−1\n(26)\nhτ = d + 2\n2\nh −h(w1v)−1wτ1v\nThe exponent −1 denotes the inverse of the linear op-\nerator, deﬁned s.t. O−1O = OO−1 = 1N×N.\nThe analysis of (26) is the topic of our next section.\n2.5\nConstraining equations\nAs explained in section 2.1 we search for the point in\nparameter space, where couplings do not ”run” any-\nmore with the scaling, which mathematically trans-\nlates into their derivatives (wrt. scaling parameter τ)\nbeing zero\n∂τ\n\u0012\nw\nh\n\u0013\n!= 0\n(27)\nJust a glance at eq.\n(26) reveals already some ﬁrst\nsolutions. We will classify now all solutions in terms\nCriticality & Deep Learning II: Momentum Renormalisation Group\n6\nof their physical meaning and single out the critical\npoint.\nThe equation for wτ in (26) does not depend on h,\nand hence can be solved on its own. Since we require\nwτ = 0, the solution of the second equation for hτ in\n(26) also requires h to be zero. We are thus left with\nclassifying the solutions which lead to wτ = 0, and\nhence following cases:\n• w = 0\n• w(1 −wβ/N) = 0\n• (1 −wβ/N) = 0\n• (1 −wβ/N)[1 −2wβ/N]−1 = 0\n• w(1 −wβ/N)[1 −2wβ/N]−1 = 0\nThe case [1 −2wβ/N]−1 = 0 cannot happen, as 0\ncannot be the inverse matrix. Also we understand that\nin the cases involving products contain strict non-zero\nterms, only the product itself is zero.\nTrivial solution, T →∞\nFirst case represents the trivial solution (w, h) =\n(0, 0); this basically means T →∞, and zero corre-\nlation due to total disorder.\nTrivial solution, T →0\nThe second bullet, can be written as (wN −w2/T) =\n0. If we assume w to be of order 1/N, then w2 is of or-\nder 1/N 2 and the temperature has to cancel that term,\neﬀectively leading to T = 1/N 2 →0 in the large N\nlimit. Here we deal with perfect correlation, all units\nparallel, either up or down.\n(We neglect the idem-\npotent case w = w2, since then w is either unity or\nsingular.)\nCritical CW system, w = c1\nThird bullet implies\nN1\nβ\n= w\n(28)\n⇕\nw\nN = const = T1 ≡Tc1\nEq. (28) resembles the constant coupling case, wik =\nJ of a classical fully connected system which reaches\ncriticality at a temperature Tc = J when h = 0, as\ndiscussed e.g. in [20]\nCritical, non-constant coupling\nThe fourth case reads (1 −wβ/N)[1 −2wβ/N]−1 =\n0. We can expand the inverse term into its Neumann\nseries\n[1 −2wβ/N]−1 =\nX\nk\n(2βw/n)k\n(29)\nup to quadratic order and then obtain\n(1 −wβ/N)[1 −2wβ/N]−1\n(30)\n≈(1 −wβ/N)[1 + 2wβ/N + 4w2(β/N)2]\n=(1 + wβ/N + 2w2(β/N)2)\n2.6\nCorrelation function and scale invariance\nAs shown in Appendix C we are able to fully solve our\nlinear model and hence compute the 2-point correla-\ntion function for two nodes k and l and its power law\nbehavior turns out to be\nCkl ∼\n1\n|x|d−2\n(31)\nEq.\n(31) shows the divergent (log) behavior of the\nfunction at criticality, i.e. the system exhibits scale\ninvariance for the right choice of couplings and noise\n(temperature): if we can constrain the weight matrix\nwij to obey the criticality conditions, then our sys-\ntem displays scale invariance through the power-law\nshaped correlation function.\nThis measure is a very handy tool to probe our real\ndeep learning setup for long-range correlations, once\nwe impose the ﬁxed-points constraints (30). We can\nsample the node activations during the prediction\nepochs and hence register their activity and decide\nwhat kind of law they obey. As it turns out, once we\nimpose the critical regularisation on our deep learning\narchitecture, the node activation patterns will obey\nstrong linear behavior on the log-log scale and hence\ndisplay a power law behavior supporting the scale in-\nvariance.\n3\nExperimental results\nWe now move on to implementing the constraints\nfound in section 2.5; as it turns out, a straightfor-\nward way of imposing those constraints on the sys-\ntem is modifying the loss function with an extra term\ncontaining the constraint; those constraints will hence\ntranslate into regularization terms, resembling elastic\n(L1, L2) regularization (and higher) given the linear\nand quadratic appearance of w;\nCriticality & Deep Learning II: Momentum Renormalisation Group\n7\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nlog activation rank\n3.2\n3.3\n3.4\n3.5\n3.6\n3.7\n3.8\nlog counts\nsingle node activation frequency\nFigure 3: Activation ranks for Feed Forward net-\nwork with no regularisation\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nlog activation rank\n0\n1\n2\n3\n4\nlog counts\nactivation pattern frequency\nFigure 4: Activation ranks for Feed Forward net-\nwork with critical regularisation\nBefore tackling the constraining equations for w we\nhave to address the fact, that w describes a fully con-\nnected system, which, to ﬁrst order, resembles the\nweight matrix between two layers of a feed forward\narchitecture, cf. appendix B of [17]:\n\n\nv11\n· · ·\nv1n\n...\n...\n...\nvm1\n· · ·\nvmn\n\n\n|\n{z\n}\nvm×n\n\u001a v × v⊺≡\nX\nk\nvikvkj\n|\n{z\n}\nwm×m\n(32)\nOn the left we have a bipartite graph representing a 2-\nlayer feed forward system with m respectively n units\nconnected via the weight matrix v; this is equivalent\nto ﬁrst order to a fully connected layer of m units,\nwhile the connection matrix w is a function of the\nfeed forward weight matrix as depicted in eq. (32).\nStarting from (30) we switch to coordinate language\nand obtain\nδi,jβwij\nN\n+\nX\nk\nwikwkj\n2β2\nN 2 = −δi,j\n(33)\nThose are component-wise constraints on the weight\nmatrix wij which, when satisﬁed, will induce criticality\nand hence scale invariance in our system.\n3.1\nCritical regularization\nAs computed in section 2.5 we have two cases of inter-\nest where scale invariance will be induced:\n• constant w, i.e. (1 = wβ/N)\n• (1 + wβ/N + 2w2(β/N)2)\nWhile the ﬁrst equation addresses the constant weight\nmatrix, i.e.\na multiple of unity, the second equa-\ntion implements a non-trivial solution of criticality and\nhence it will be our case of study.\nFor our experiments we used the CIFAR- 10 dataset\nfor all investigated models; furthermore, our architec-\nture relies on ReLU/eLU activations while for the op-\ntimisation we use the Adam Optimizer without gradi-\nent clipping. We implemented a feed forward network\nwith 4 layers, of 600, 400, 200 and 100 nodes respec-\ntively. Our focus was mainly inducing scale invariance\nand exactly capturing the regime of its emergence;\nLayer activation\nIn ﬁgure 3 we depict the activation ranks of a nor-\nmal feed forward architecture without regularisation;\nfor the layer activation patterns we counted the fre-\nquency of each layer’s activation through the inference\nepochs and then we sorted those by rank; the ﬁgure\nthen depicts the log counts versus the logged ranks.\nNext to it, we have implemented the critical regular-\nization, in ﬁgure 4. We obtain a strong deviation from\nthe non-regularized system: where on the left the sys-\ntem is almost linear and then abruptly falls oﬀtowards\nhigher ranks, with critical regularization the activation\nis nearly linear and stays that way until the very end\nof the distribution; also the slope of the distribution is\nvery steep, hence once more distinguishing it from the\n”normal” case; this strong linear behavior, implying a\npower law distribution is the prime indicator for scale\ninvariance, as discussed in section 2.6.\nAverage node activation\nAnother measure we employed in detecting deviating\nbehavior in critically regularized systems is the average\nactivation of the nodes during the prediction epochs.\nGiven a layer, we averaged over the activations of all\nunits in that layer for one prediction epoch, after which\nwe ranked the log averages by their log counts - the\nCriticality & Deep Learning II: Momentum Renormalisation Group\n8\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nlog avg activations\n101\n102\n103\n104\n105\nlog count\naverage activations\nFigure 5: Ranks of average layer activations for Feed\nForward network with no regularisation\n0.175\n0.150\n0.125\n0.100\n0.075\n0.050\n0.025\n0.000\nlog avg activations\n101\n102\n103\n104\nlog count\naverage activations\nFigure 6: Ranks of average layer activations for Feed\nForward network with critical regularisation\nresults are visible in ﬁgure 5 and 6.\nThe top graph depicts the log-log distribution of 4-\nlayered feed forward net with no regularization, con-\ntrasting to the graph below it, where the distribution\ncomes from same architecture but with critical regu-\nlarization employed. The linear behavior is strongly\nvisible, over four orders of magnitude in the count of\nthe ranks; hence another criterion supporting the scale\ninvariance of the architecture tuned rightly via regu-\nlarization.\nWeighted degree distribution\nA last measure we used to test the validity of our mech-\nanism is the weighted node degree, as suggested in\n[22]. Here we sum all the values of the weights going\nout from a node; this is a weighted sum of the out-\ngoing connections, as zero weights do not contribute\nand ﬁnite weights make a contribution weighted with\nunity. To every node in a layer we will hence attach\nthe real value of its weighted degree; once again, we\nlog-count the occurrences and plot against the logged\ndegree, as depicted in picture 7 and 8. The green graph\ndepicts the degree distribution of the four-layer archi-\ntecture without any regularization, while below it we\nhave the same architecture subjected to critical reg-\nularization. The diﬀerence is quite dramatic, as the\ndegree in the critical case exhibit a drastic bi-modal\ndistribution, roughly around 1 and some other frac-\ntional value. Once again, we interpret this bi-modal\ndistribution as the results of the inhomogeneous poly-\nnomial regularization employed.\n3.2\nApplicability of our results\nWe conclude the experimental section stressing our ap-\nproximations and shortcomings while arriving at the\ntheoretical and experimental results depicted. All our\ncalculations so far have been performed in a system\nwhere the units take on values in {±1}; this was\ndue to the analytic behavior of the results and hence\nthe tractability of calculations.\nThe domain of the\nfeed-forward ReLU network though, is contained in\n[0, +∞]; the translation from one domain into the\nother leaves the structure of the Hamiltonian unaltered\nand has as eﬀect re-deﬁned couplings; given the pre-\n1.10\n1.15\n1.20\n1.25\n1.30\n1.35\nlog weight degree\n100\n101\nlog counts\nweight distribution\nFigure 7: Log distribution of weighted degree of nodes\nper layer without regularisation\n2.0\n1.5\n1.0\n0.5\n0.0\nlog weight degree\n100\n101\nlog counts\nweight distribution\nFigure 8: Log distribution of weighted degree of nodes\nper layer with critical regularisation\nCriticality & Deep Learning II: Momentum Renormalisation Group\n9\nserved structure of the Hamiltonian, the polynomial\nnature of the constraints will stay conserved, possibly\nwith corrections in the coeﬃcients; we regard thus the\nresults of the RG transformation as a powerful hint\ntowards non-homogeneous polynomial regularisation,\nwhich we have implemented above.\n4\nSummary and outlook\nSummary: By mapping a classical deep learning archi-\ntecture to an eﬀective theory of ﬁeld (densities) we are\nable to employ the powerful tool of momentum space\nrenormalisation for scale-free systems in the realm of\ndeep learning networks. Carrying out the renormal-\nisation steps in momentum space we induce the ﬂow\nof the coupling constants, while keeping the Hamilto-\nnian structure unchanged; the ﬂow of the constants are\na set of non-linear diﬀerential equations which, when\nsolved, employ strong conditions on the couplings and\nhence on the parameters of the deep learning system.\nThe constraints are further translated into regularisa-\ntion conditions, which take form of a non-homogeneous\npolynomial in the weight matrix. We then implement\nthis critical regularisation and induce typical behavior\nof the net as observed in scale-invariant systems. In\nour experiments we use various metrics to measure the\ndegree of scale invariance and detect clearly its pres-\nence.\nOutlook: Despite the concreteness of the multy-layer\nfeed forward network, we still lack accuracy in our\nmapping and neglect many details in our mapping,\nsuch as the values of the units and the multi-layer na-\nture of the architecture. It would be of tremendous\nimportance to address a full architecture, including its\nnon-linearities in an analytical way. Ideally, the self-\nsimilarity of the network would be ported into deeply\nmanifest group symmetries of the analytic counter-\npart. This however, remains to be studied in future\nwork.\nAppendix\nA\nFrom variables to ﬁelds\nIn this section we will depict the steps in order to lift\nour classical fully connected layer system to an eﬀec-\ntive ﬁled theory.\nGiven a functional L(φ(x)) depending on (products) of\nthe ﬁeld φ(x) the functional (path) integral is deﬁned\nas a formal inﬁnite product of integrals over all the x:\nZ\nDφ(x)L(φ) =\nY\nx\nZ\ndφ(x)L(φ(x))\n(34)\nwhich in practice means discretising x into l supports\nx →xk, k ∈[−l, . . . , +l], and then taking the limit\nY\nx\nZ\ndφ(x) = lim\nl→∞\nY\nk∈[−l,...,+l]\nZ\ndφ(xk)\n(35)\nFurther, we will generalize the Gaussian integral\nZ ∞\n−∞\ndx\n√\n2π e−(1/2)ax2+bx =\n1\n√aeb2/(2a)\n(36)\nto its functional version.\nIntroducing the coordi-\nnate free notation ⟨a, b⟩= P\nk akbk and ⟨a, wb⟩=\nP\nkl akwklbk for the linear inner product, the general-\nization of the Gaussian integral to the functional case\nreads\nZ\nDφ exp[−1\n2⟨φ, Kφ⟩+ ⟨η, φ⟩]\n(37)\n=\n1\n√\ndet K\nexp 1\n2⟨η, K−1η⟩\nWe are now able, using these tools, to lift our system\nto an eﬀective ﬁeld theory; as explained in [17], our\nHamiltonian and associated partition function read in\ncoordinate free notation\nZ =\nX\ns∈{±1}\ne−1\n2⟨s, ws⟩−⟨h, s⟩\n(38)\nwith s = sk being the N units, while w = wkl being\nthe fully connecting weight matrix. We insert now the\nrelation (37) for the quadratic part exp[−1\n2⟨s, ws⟩] of\nthe partition function in (38) and obtain:\nZ = P\ns∈{±1}\n1\n√\ndet w\n(39)\nZ\nDφ exp[−1\n2⟨φ, w−1φ⟩+ ⟨φ, s⟩]e⟨h,s⟩\n=\n1\n√\ndet w\nZ\nDφ exp[−1\n2⟨φ, w−1φ⟩]P\ns∈{±1}e⟨h+φ,s⟩\n=c\nY\nk\nZ\nDφke−H(φk,w,h)\nwhile the Hamiltonian reads now\nH(φ, w, h) =\n(40)\n1\n2⟨φ, w−1φ⟩−ln P\ns∈{±1}⟨h + φ, s⟩\nCriticality & Deep Learning II: Momentum Renormalisation Group\n10\nWe have eﬀectively restricted the sum over s ∈{±1}\nin eq. (39) over the linear term only, by introducing\nthe eﬀective ﬁeld φ. Hence we can now calculate the\npartition sum in eq. (40) and after one more transfor-\nmation φk →P\ni wikφi, while neglecting the constant\nN ln 2 will bring us to the Hamiltonian:\nH(φ, w, h) =\n(41)\n1\n2\nX\nkl\nwklφkφl −\nX\nk\nln cosh(\nX\ni\nwikφi + φk)\nLast transformation will also produce a Jacobian equal\nto det w multiplying the partition function.\nWe eﬀectively traded the quadratic binary sum for ad-\nditional ﬁelds φi, while the remaining linear binary\nsum can be analytically computed;\nIn analogy to free energy per unit, we introduce the\nHamiltonian density (per space) and hence think of the\nﬁelds φk as density of the order parameter, which also\ndisplay ﬂuctuations (beyond the microsopic/atomic\nscale); the Hamiltonian density then reads:\nH(φ, w, h) =\nZ\ndx 1\n2\nX\nkl\n[wklφkφl + δkl(∂xφk)(∂xφl)]\n−\nX\nk\nln cosh(P\nl wklφl + h)\n(42)\nThe φk are now genuine eﬀective ﬁeld functions, de-\npending on spacial coordinates x. The additional term\ncontaining the spacial derivative takes into account\nthat φi ≡φi(x), hence ﬁelds being dynamic and hence\nable to ﬂuctuate, on larger scales than the next neigh-\nbour distance;\nB\nFourier transformed ﬁeld theory\nEq. (42) encodes all information of interest describing\nthe system, which can be expressed in terms of corre-\nlation functions of various degree (i.e. the coordinated\nﬁring of n random units through the architecture, as a\nfunction of their ”distance”, which is their index sep-\naration);\nThe term ln cosh has to be treated perturbatively any-\nway, hence we will ignore it for now; the ﬁrst part of\n(42) is the ”free” Hamiltonian, which can be fully di-\nagonalised and solved, while the non-linear part can\nbe expandede and treated as a correction term;\nIntegrating by parts the derivative term, we obtain a\nquadratic form\nH(φ, w, h) = −\nZ\nddx ddy\nX\nkl\nφkMkl(x, y)φl\n(43)\nwith the operator M deﬁned as\nMkl(x, y) ≡1\n2δ(x −y)(wkl −δkl∇2\nx)\n(44)\nBy partial integration we picked up a crucial minus\nsign in front of the Laplace operator, which will prove\nvery important in the solution of the system. Eq. (43)\ncan be fully diagonalised and hence solved once we ﬁnd\na suitable basis ψq, s.t. M acts linearly on it\nMψq = λqψq\n(45)\nWe then expand our ﬁelds in the eigenvectors\nφ =\nX\nq\nφqψq\n(46)\nwith φq given by the relation\nφq =\nZ\ndxψ∗\nq(x)φ(x)\n(47)\nThe form of M dictates the choice for the eigenvectors\nψq = exp(iqx)\n(48)\nInserting (48) into (45) we obtain for the eigenvalues\nλq = (δklq2 + wkl)/2\n(49)\nThe explicit expansion of the ﬁelds reads now\nφ(x) =\nZ\n|q|<Λ\nddq\n(2π)d φ(q)eiqx\n(50)\nwith φ(q) given by\nφ(q) =\nZ\nddx e−iqxφ(x)\n(51)\nand the Hamiltonian\nH(φ, w, h) =\n(52)\nZ\n|q|<Λ\nddq\n(2π)d\n1\n2\nX\nkl\n(δklq2 + wkl)φk(q)φl(−q)\nwhere we have used the identity\nR\ndxeiqxeipx\n=\n(2π)dδ(q −p), which is the normality condition of the\nbasis (48).\nThis is the diagonalised free Hamiltonian.\nCriticality & Deep Learning II: Momentum Renormalisation Group\n11\nGiven the eﬀective nature of our theory, we have in-\ntroduced a natural UV-cutoﬀΛ in order to account for\nthe ﬁnite validity of the Hamiltonian; in the partition\nfunction, also the integration measure will naturally\nchange from paths in conﬁguration space\nZ\nDφ(x) →\nZ\nDφ(q)\n(53)\nto paths over momenta once we transition to Fourier\nspace.\nGoing now back to the original, full Hamiltonian\nand expanding the ln cosh-term to ﬁrst order, group-\ning linear and quadratic terms together and going\ncoordinate-free we ﬁnally obtain\nHφ =\nZ\nddq\n(2π)d\n1\n2(r + gq2)φ(q) · φ(−q) −u · φ(q = 0)\n(54)\nwith\nr ≡rkl = (wkl −P\ni wkiwil),\n(55)\ng ≡gkl = δkl,\nu ≡uk = h P\nl wkl\nObviously in the base (48) the derivative term pro-\nduces only a multiplicative momentum factor.\nThe\npartition function based on the Gaussian Hamiltonian\nin momentum space reads:\nZ = c\nZ\nDφ(q)\n(56)\ne\n−β\nR\nddq\n(2π)d 1\n2(r + gq2)φ(q) · φ(−q) −u · φ(q = 0)\nThe functional integral Dφ(q) is understood to be an\ninﬁnite product Dφ(q) = Q\nq\nR\ndφ(q) over the momen-\ntum q, while each φ(qk) is ﬁxed at a speciﬁc location\nqk.\nThe constant c multiplying the partition func-\ntion contains the determinant and further numerical\nconstants which only appear additive in the free en-\nergy F = −kT ln Z and hence do not contribute any-\nthing neither to derivatives nor to normalised quanti-\nties, such as the correlation function.\nC\nSolution of the Gaussian model\nThe functional integral (56) is a Gaussian type of inte-\ngral and hence, luckily, can be fully solved; we arrived\nat it while lifting the theory to an eﬀective ﬁeld theory\nvia (37); solving thus the Gaussian is simply reversing\nthis very equation:\nZ =\nZ\nDφe−β[ 1\n2⟨φ, Kφ⟩−⟨u, φ⟩]\n(57)\n=\n1\n√\ndet K\nexp 1\n2⟨u, K−1u⟩\n(58)\nwhere we have identiﬁed the operator K = (r + gq2)\nand introduced the inner product notation ⟨a, b⟩=\nR\nddq\n(2π)d a(q)·b(−q). The partition function Z ≡Z(u)\nin (57) is also called the generating functional, for the\nobvious reason that we can generate from it n-point\ncorrelation functions; those are the average correla-\ntion functions for n random units, as a function of\ntheir separation. Generally speaking, the average of\nan operator is given by\n⟨O⟩\ndef\n=\n1\nZ(0)\nZ\nDφO e−β 1\n2⟨φ, Kφ⟩≡⟨O⟩0\n(59)\nHere ⟨O⟩0 denotes the average of operator O being\ntaken wrt. Z(0) ≡Z(u = 0)\nSince we are interested mostly in the 2-point function,\nwe will compute it here as:\nCkl ≡⟨φkφl⟩0 =\n(60)\n1\nZ(0)\nZ\nDφφkφl e−β 1\n2⟨φ, Kφ⟩=\n1\nZ(0)\nZ\nDφ\nδ2\nδukδul\ne−β[ 1\n2⟨φ, Kφ⟩−⟨u, φ⟩]\n\f\f\f\f\nu=0\n=\nδ2\nδukδul\nln Z(u)\n\f\f\f\f\nu=0\n(61)\nhence this justiﬁes the name ”generating functional”\nfor Z(u).\nWe can apply now (61) on (58) to yield the explicit\ncorrelation function between two units\nCkl = ⟨φkφl⟩0 = β\nZ\nddq\n(2π)d\ne−iqx\nr + gq2\n(62)\nWe recall the deﬁnition of r, g given in (55) and hence\nwe recognize K−1 as a matrix inverse.\nIn order to get an impression of the form and especially\nof the asymptotic behavior of the correlation function\n(62) we can rewrite it and proceed as follows:\nZ\nddq\n(2π)d\ne−iqx\nr + gq2 =\nZ\nddq\n(2π)dg\ne−iqx\nrg−1 + q2\n(63)\nCriticality & Deep Learning II: Momentum Renormalisation Group\n12\nThe right side of (63) is just the inverse Fourrier trans-\nform of the Lorenz function, and hence we obtain\nCkl ∼e−x√\ng/r\n(64)\nOur main goal though, is to reach a state of self-\nsimilarity, when the system displays scale-invariance;\nthis is the whole scope of the RG procedure, resulting\nin the equations (21).\nIn this case, r →0 and the\ncorrelation function (62) simpliﬁes to\nCkl = ⟨φkφl⟩0 = β\nZ\nddq\n(2π)d\ne−iqx\ngq2\n∼\n1\n|x|d−2\n(65)\nFor our case of interest when d = 2, the integral di-\nverges as ln |x|, hence the long range correlation.\nReferences\n[1] Stuart A. Kauﬀman, ”The Origins of Order: Self-\nOrganization and Selection in Evolution”, Oxford\nUniversity Press, 1993\n[2] Thierry Mora, William Bialek, Are biological sys-\ntems poised at criticality?, arXiv:1012.2242 [q-\nbio.QM]\n[3] Dante\nR.\nChialvo,\nPablo\nBalenzuela,\nDaniel\nFraiman, ”The brain: What is critical about it?”,\narXiv:0804.0032 [cond-mat.dis-nn]\n[4] Michael E. Fisher, ”Renormalization group theory:\nIts basis and formulation in statistical physics”,\nRev. Mod. Phys. 70, 653, April 1998, https://\ndoi.org/10.1103/RevModPhys.70.653\n[5] Hoang K. Nguyen, ”Scale invariance in cosmology\nand physics”, arXiv:1111.5529 [physics.gen-ph]\n[6] H.E. Stanley, ”Scaling,universality and renormal-\nization:three pillars of modern critical phenom-\nena”,\nhttp://journals.aps.org/rmp/abstract/10.\n1103/RevModPhys.71.S358\n[7] Lin-Yuan Chen, Nigel Goldenfeld, and Y. Oono,\n”Renormalization group and singular perturba-\ntions: Multiple scales, boundary layers, and reduc-\ntive perturbation theory”, Phys. Rev. E 54, 376, 1\nJuly 1996\n[8] Per Bak, How Nature Works: the science of self-\norganized criticality, Copernicus Springer-Verlag,\nNew York, 1996\n[9] Kenneth G. Wilson, ”Renormalization Group and\nCritical Phenomena. I. Renormalization Group\nand the KadanoﬀScaling Picture” Phys. Rev. B\n4, 3174 - November 1971\n[10] Kenneth G. Wilson, ”Renormalization Group and\nCritical Phenomena. II. Phase-Space Cell Analy-\nsis of Critical Behavior”, Phys. Rev. B 4, 3184 -\nNovember 1971\n[11] Adriano Barra,Giuseppe Genovese, Peter Sol-\nlich, Daniele Tantari, Phase transitions in Re-\nstricted Boltzmann Machines with generic priors,\narXiv:1612.03132 [cond-mat.dis-nn]\n[12] Pankaj Mehta, David J. Schwab, ”An exact\nmapping between the Variational Renormalization\nGroup and Deep Learning”, arXiv:1410.3831\n[stat.ML]\n[13] Gasper Tkacik, Elad Schneidman, Michael J\nBerry II, William Bialek, ”Ising models for net-\nworks of real neurons”, arXiv:q-bio/0611072\n[q-bio.NC]\n[14] O.B. Isaeva, S.P. Kuznetsov, ”Approximate De-\nscription of the Mandelbrot Set. Thermodynamic\nAnalogy”, arXiv:nlin/0504063 [nlin.CD]\n[15] W. D. McComb, ”Renormalization methods, a\nguide for beginners” Oxford university press, 2004\n[16] O.Kogan, J. Rogers, M. Cross, G. Refael, ”Renor-\nmalization Group Approach to Oscillator Synchro-\nnization” arXiv:0810.3075 [nlin.PS]\n[17] Dan\nOprisa,\nPeter\nToth,\n”Criticality\nand\nDeep Learning, Part I: Theory vs. Empirics”,\narXiv:1702.08039 [cs.AI]\n[18] Uwe C. Tauber, ”Renormalization Group: Appli-\ncations in Statistical Physics”, Nuclear Physics B,\n(2011) 128\n[19] P. C. Hohenberg,\nA. P. Krekhov,\n”An in-\ntroduction to the Ginzburg-Landau theory of\nphase transitions and nonequilibrium patterns”\narXiv:1410.7285 [cond-mat.stat-mech]\n[20] Martin Kochmanski, Tadeusz Paszkiewicz, Sla-\nwomir Wolski, ”Curie-Weiss magnet:\na simple\nmodel of phase transition”, arXiv:1301.2141 [cond-\nmat.stat-mech]\n[21] Kerson Huang, ”Statistical Mechanics”, Wiley,\n2nd edition, 1987\n[22] Reka Albert, Albert-Laszlo Barabasi, Statisti-\ncal mechanics of complex net- works, arXiv:cond-\nmat/0106096 [cond-mat.stat-mech]\n",
  "categories": [
    "cond-mat.stat-mech",
    "cs.LG"
  ],
  "published": "2017-05-31",
  "updated": "2017-05-31"
}