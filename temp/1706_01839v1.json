{
  "id": "http://arxiv.org/abs/1706.01839v1",
  "title": "Assessing the Linguistic Productivity of Unsupervised Deep Neural Networks",
  "authors": [
    "Lawrence Phillips",
    "Nathan Hodas"
  ],
  "abstract": "Increasingly, cognitive scientists have demonstrated interest in applying\ntools from deep learning. One use for deep learning is in language acquisition\nwhere it is useful to know if a linguistic phenomenon can be learned through\ndomain-general means. To assess whether unsupervised deep learning is\nappropriate, we first pose a smaller question: Can unsupervised neural networks\napply linguistic rules productively, using them in novel situations? We draw\nfrom the literature on determiner/noun productivity by training an\nunsupervised, autoencoder network measuring its ability to combine nouns with\ndeterminers. Our simple autoencoder creates combinations it has not previously\nencountered and produces a degree of overlap matching adults. While this\npreliminary work does not provide conclusive evidence for productivity, it\nwarrants further investigation with more complex models. Further, this work\nhelps lay the foundations for future collaboration between the deep learning\nand cognitive science communities.",
  "text": "Assessing the Linguistic Productivity of Unsupervised Deep Neural Networks\nLawrence Phillips (Lawrence.Phillips@pnnl.gov)\nPaciﬁc Northwest National Laboratory\nNathan Hodas (Nathan.Hodas@pnnl.gov)\nPaciﬁc Northwest National Laboratory\nAbstract\nIncreasingly, cognitive scientists have demonstrated interest in\napplying tools from deep learning. One use for deep learning is\nin language acquisition where it is useful to know if a linguistic\nphenomenon can be learned through domain-general means.\nTo assess whether unsupervised deep learning is appropriate,\nwe ﬁrst pose a smaller question: Can unsupervised neural net-\nworks apply linguistic rules productively, using them in novel\nsituations? We draw from the literature on determiner/noun\nproductivity by training an unsupervised, autoencoder network\nmeasuring its ability to combine nouns with determiners. Our\nsimple autoencoder creates combinations it has not previously\nencountered and produces a degree of overlap matching adults.\nWhile this preliminary work does not provide conclusive evi-\ndence for productivity, it warrants further investigation with\nmore complex models. Further, this work helps lay the foun-\ndations for future collaboration between the deep learning and\ncognitive science communities.\nKeywords: Deep Learning; Language Acquisition; Linguistic\nProductivity; Unsupervised Learning; Determiners\nIntroduction\nComputational modeling has long played a signiﬁcant role\nwithin cognitive science, allowing researchers to explore\nthe implications of cognitive theories and to discover what\nproperties are necessary to account for particular phenom-\nena (J. L. McClelland, 2009). Over time, a variety of mod-\neling traditions have seen their usage rise and fall. While the\n1980s saw the rise in popularity of connectionism (Thomas &\nMcClelland, 2008), more recently symbolic Bayesian mod-\nels have risen to prominence (Chater & Oaksford, 2008; Lee,\n2011). While the goals of cognitive modelers have largely\nremained the same, increases in computational power and ar-\nchitectures have played a role in these shifts (J. L. McClel-\nland, 2009). Following this pattern, recent advances in the\narea of deep learning (DL) have led to a rise in interest from\nthe cognitive science community as demonstrated by a num-\nber of recent workshops dedicated to DL (Saxe, 2014; J. Mc-\nClelland, Hansen, & Saxe, 2016; J. McClelland, Frank, &\nMirman, 2016).\nAs with any modeling technique, DL can be thought of\nas a tool which is best suited to answering particular types\nof questions.\nOne such question is that of learnability,\nwhether an output behavior could ever be learned from the\ntypes of input given to a learner.\nThese types of ques-\ntions play an integral role in the ﬁeld of language acquisi-\ntion where researchers have argued over whether particular\naspects of language could ever be learned by a child with-\nout the use of innate, language-speciﬁc mechanisms (Smith,\n1999; C. D. Yang, 2004; Chater & Christiansen, 2010; Pearl,\n2014). The success of a domain general learner does not nec-\nessarily imply that human learners acquire the phenomenon\nin a similar fashion, but it does open the possibility that we\nneed not posit innate, domain-speciﬁc knowledge.\nThe crux of these learning problems typically lies in mak-\ning a particular generalization which goes beyond the input\ndata. One major type of generalization that DL models would\nneed to capture is known as linguistic productivity. A gram-\nmatical rule is considered productive when it can be applied\nin novel situations. For example, as a speaker of English you\nmay never have encountered the phrase a gavagai before, but\nyou now know that gavagai must be a noun and can therefore\ncombine with other determiners to produce a phrase such as\nthe gavagai. Before DL might be applied to larger questions\nwithin language acquisition, the issue of productivity must\nﬁrst be addressed. If DL models are not capable of produc-\ntivity, then they cannot possibly serve to model the cognitive\nprocess of language acquisition. On the other hand, if DL\nmodels demonstrate basic linguistic productivity, we must ex-\nplore what aspects of the models allow for this productivity.\nThe Special Case of Determiners\nFor decades, debate has raged regarding the status of produc-\ntive rules among children acquiring their native language. On\nthe one hand, some have argued that children seem hardwired\nto apply rules productively and demonstrate this in their ear-\nliest speech (Valian, Solt, & Stewart, 2009; C. Yang, 2011).\nOn the other, researchers have argued that productivity ap-\npears to be learned, with children’s early speech either lack-\ning productivity entirely or increasing with age (Pine & Mar-\ntindale, 1996; Pine, Freudenthal, Krajewski, & Gobet, 2013;\nMeylan, Frank, Roy, & Levy, 2017). Of particular interest\nto this debate has been the special case of English determin-\ners. In question is whether or not English-learning children\nhave acquired the speciﬁc linguistic rule which allows them\nto create a noun phrase (NP) from a determiner (DET) and\nnoun (N) or if they have simply memorized the combinations\nthat they have previously encountered. This linguistic rule,\nNP →DET N, is productive in two senses. First, it can be\napplied to novel nouns, e.g. a gavagai. Second, consider the\ndeterminers a and the. If a singular noun can combine with\none of these determiners, it may also combine with the other,\ne.g. the wug.\nThis type of rule seems to be acquired quite early in acqui-\nsition, making it appropriate to questions of early productiv-\nity, and provides an easy benchmark for a DL model. Yet\nanswering such a simple question ﬁrst requires addressing\narXiv:1706.01839v1  [cs.CL]  6 Jun 2017\nhow one might measure productivity. Most attempts to mea-\nsure productivity have relied on what is known as an overlap\nscore, intuitively what percentage of nouns occur with both a\nand the (C. Yang, 2011). This simple measure has been the\nsource of some controversy. C. Yang (2011) argues that early\nattempts failed to take into account the way in which word\nfrequencies affect the chance for a word to “overlap”. Be-\ncause word frequency follows a Zipﬁan distribution, with a\nlong tail of many infrequent words, many nouns are unlikely\nto ever appear with both determiners. He proposes a method\nto calculate an expected level of overlap which takes into ac-\ncount these facts. Alternatively, Meylan et al. (2017) propose\na Bayesian measure of productivity which they claim takes\ninto account the fact that certain nouns tend to prefer one de-\nterminer over another. For instance, while one is more likely\nto hear a bath than the phrase the bath, the opposite is true of\nthe noun bathroom which shows a preference for the deter-\nminer the (Meylan et al., 2017).\nThe literature is quite mixed regarding whether or not chil-\ndren show early productivity. Differences in pre-processing\nhave lead researchers to draw opposite conclusions from sim-\nilar data, making interpretation quite difﬁcult (C. Yang, 2011;\nPine et al., 2013). Indeed, most corpora involving individual\nchildren are small enough that Meylan et al. (2017) argue it is\nimpossible to make a statistically signiﬁcant claim as to child\nproductivity. For analyzing whether or not text generated by\na DL model is productive or not, we thankfully do not need\nto fully address the problem of inferring child productivity.\nIdeally, the model would demonstrate a similar level of over-\nlap to the data it was exposed to. We make use of the overlap\nstatistic from Yang because it is more easily comparable to\nother works and has been better studied than the more recent\nBayesian metric of Meylan et al. (2017).\nDeep Learning for Language Acquisition\nDeep learning, or deep neural networks, are an extension of\ntraditional artiﬁcial neural networks (ANN) used in connec-\ntionist architectures. A “shallow” ANN is one that posits a\nsingle hidden layer of neurons between the input and out-\nput layers. Deep networks incorporate multiple hidden lay-\ners allowing these networks in practice to learn more com-\nplex functions. The model parameters can be trained through\nthe use of the backpropogation algorithm. The addition of\nmultiple hidden layers opens up quite a number of possible\narchitectures, not all of which are necessarily applicable to\nproblems in cognitive science or language acquisition more\nspeciﬁcally.\nWhile the most common neural networks are discrimina-\ntive, i.e. categorizing data into speciﬁc classes, a variety of\ntechniques have been proposed to allow for truly generative\nneural networks. These generative networks are able to take\nin input data and generate complex outputs such as images or\ntext which makes them ideal for modeling human behavior.\nWe focus on one generative architecture in particular known\nas a deep autoencoder (AE) (Hinton & Salakhutdinov, 2006).\nWhile AEs have been used for a variety of input data types,\nmost prominently images, we describe their use here primar-\nily for text. The ﬁrst half, the encoder, takes in sentences\nand transforms them into a condensed representation. This\ncondensed representation is small enough that the neural net-\nwork cannot simply memorize each sentence and instead is\nforced to encode only the aspects of the sentence it believes\nto be most important. The second half, the decoder, learns to\ntake this condensed representation and transform it back into\nthe original sentence. Backpropogation is used to train model\nweights to reduce the loss between the original input and the\nreconstructed output. Although backpropagation is more typ-\nically applied to supervised learning problems, the process is\nin fact unsupervised because the model is only given input\nexamples and is given no external feedback.\nAEs have been shown to successfully capture text repre-\nsentations in areas such as paragraph generation (Li, Luong,\n& Jurafsky, 2015), part-of-speech induction (Vishnubhotla,\nFernandez, & Ramabhadran, 2010), bilingual word represen-\ntations (Chandar et al., 2014), and sentiment analysis (Socher,\nPennington, Huang, Ng, & Manning, 2011), but have not\nbeen applied to modeling language acquisition. While any\nnumber of DL architectures could be used to model language\nacquisition, the differences between ANNs and actual neu-\nrons in the brain make any algorithmic claims difﬁcult. In-\nstead, DL models might be used to address computational-\nlevel questions, for instance regarding whether or not a piece\nof knowledge is learnable from the data encountered by chil-\ndren. Before this can be done, however, it remains to be seen\nwhether DL models are even capable of creating productive\nrepresentations. If they cannot, then they do not represent\nuseful models of language acquisition. This work attempts to\naddress this not by creating a model of how children acquire\nlanguage, but by using methods from the psychological liter-\nature on productivity to assess the capability of DL to learn\nproductive rules.\nMethods\nCorpora\nTo train our neural network, we make use of child-directed\nspeech taken from multiple American-English corpora in\nthe CHILDES database (MacWhinney, 2000).\nIn particu-\nlar, we make use of the CDS utterances in the Bloom 1970,\nBrent, Brown, Kuczaj, Providence, Sachs, and Suppes cor-\npora (Bloom, 1970; Brent & Siskind, 2001; Brown, 1973;\nKuczaj, 1977; Demuth & McCullough, 2009; Sachs, 1983;\nSuppes, 1974). The combined corpora contain almost 1 mil-\nlion utterances and span a wide age range, including speech\ndirected to children as young as 6 months and as old as 5\nyears. Relevant information about the used corpora can be\nfound in Table 1.\nBecause we are interested in seeing what the AE can learn\nfrom data similar to that encountered by children, we train\nthe model only on child-directed utterances. These can be\nproduced by any adult in the dataset, including parents and\nresearchers. Although a comparison with child-produced text\nEmbedding Layer (30 dim)\nGRU Layer (20 dim)\nLatent Representation (20 dim)\nGRU Layer (20 dim)\nDense Softmax Layer\nX1\nX2\nXT\ny1\nyT-1\nyT\nFigure 1: Visual representation of the autoencoder model.\nholds great interest, it is not clear whether child-produced\nspeech is rich enough to support robust language learning on\nits own. It therefore provides a poor basis upon which to train\nthe AE.\nText from the various corpora is processed as a single docu-\nment. Child-directed utterances are cleaned from the raw ﬁles\nusing the CHILDESCorpusReader function of the Python\nNatural Language Toolkit (NLTK). Utterances from all non-\nchildren speakers are included and not limited just to the pri-\nmary caregiver. Each utterance is split into words according\nto the available CHILDES transcription and then made low-\nercase. The model represents only the most frequent 3000\nwords, while the remainder are represented as a single out-\nof-vocabulary (OOV) token. This step is taken both to re-\nduce computational complexity but also to mimic the fact that\nyoung children are unlikely to store detailed representations\nof all vocabulary items encountered. Because the neural net-\nworks require each input to be of the same length, sentences\nare padded to a maximum length of 10 words. Sentences that\nare longer than this are truncated, while short sentences are\nprepended with a special PAD token.\nCorpora\nAge Range\nN. Utterances\nBloom 1970\n1;9 - 3;2\n62,756\nBrent\n0;6 - 1;0\n142,639\nBrown\n1;6 - 5;1\n176,856\nKuczaj\n2;4 - 4;1\n57,719\nProvidence\n1;0 - 3;0\n394,800\nSachs\n1;1 - 5;1\n28,200\nSuppes\n1;11 - 3;3\n67,614\nOverall\n0;6 - 5;1\n930,584\nTable 1: Descriptive statistics of CHILDES corpora. Ages\nare given in (year;month) format and indicate the age of the\nchild during corpus collection.\nNeural Network Architecture\nOur autoencoder model was implemented using Keras and\nTensorﬂow.\nThe words in each sentence are input to the\nmodel as a one-hot vector, a vector of 0s with a single 1 whose\nplacement indicates the presence of a particular word. This is\nan inefﬁcient representation because it assumes all words are\nequally similar, e.g. that dog is equally similar to dogs as\nit is to truck. To deal with this, the model passes the one-\nhot vector to an embedding layer. Neural word embeddings,\nas popularized by the word2vec algorithm (Mikolov, Chen,\nCorrado, & Dean, 2013), are a way to represent words in\na low-dimensional space without requiring outside supervi-\nsion. Words are placed within the space such that words that\nare predictive of neighboring words are placed closer to one\nanother. Because our training data is relatively small, we keep\nthe embedding dimensionality low, at only 30. Standard em-\nbeddings trained on much larger NLP corpora tend to use 100\nor 200 dimensions.\nOnce each word has been transformed into a 30-\ndimensional embedding vector, the sequence of words is\npassed into a gated-recurrent unit (GRU) layer (Cho et al.,\n2014). The GRU is a type of recurrent (RNN) layer which\nwe choose because it can be more easily trained. RNN lay-\ners read in their inputs sequentially and make use of hidden\n“memory” units that pass information about previous inputs\nto later inputs, making them ideal for sequence tasks such as\nlanguage. As such, the model creates a representation of the\nsentence which it passes from word to word. The ﬁnal repre-\nsentation is the output of the encoder, a latent representation\nof the full sentence.\nThis 20-dimensional latent vector serves as the input to the\ndecoder unit. The ﬁrst layer of the decoder is a GRU layer of\nthe same shape as in the encoder. For each timestep, we feed\ninto the GRU the latent vector, similar to the model proposed\nin Cho et al. (2014). Rather than producing a single output,\nas in the encoder, the decoder’s GRU layer outputs a vector at\neach timestep. Each of these vectors is fed into a shared dense\nsoftmax layer which produces a probability distribution over\nvocabulary items. The model then outputs the most likely\nword for each timestep.\nThe model loss is calculated based on the model’s ability to\nreconstruct the original sentence through categorical crossen-\ntropy. Model weights are trained using the Adam optimzer\nover 10 epochs. During each epoch the model sees the entire\ntraining corpus, updating its weights after seeing a batch of 64\nutterances. While this process does not reﬂect that used by a\nchild learner, it is a necessary component of training the neu-\nral network on such a small amount of data. If the network\nhad access to the full set of speech that a child encounters\nsuch a measure likely would not be necessary. Future work\nmight also investigate whether optimizing the dimensionality\nof the network might lead to better text generation with higher\nlevels of productivity.\nBaseline Models\nBecause the AE is learning to reproduce its input data, one\nmight wonder whether similar results might be achieved by a\nsimpler, distributional model. To assess this, we also mea-\nsure the performance of an n-gram language model.\nWe\ntrain bigram and trigram language models using the modi-\nﬁed Kneser-Ney smoothing (Heaﬁeld, Pouzyrevsky, Clark,\n& Koehn, 2013) implemented in the KenLM model toolkit\nto estimate the distributional statistics of the training corpus.\nSentences are generated from the n-gram language model by\npicking a seed word and then sampling a new word from the\nset of possible n-grams. The smoothing process allows for\nthe model to generate previously unseen n-grams. Sampling\nof new words continues for each utterance until the end-of-\nsentence token is generated or a maximum of 10 tokens is\nreached (the same maximum size as for the AE).\nSince the AE is able to generate sentences from a latent\nrepresentation, it would be inappropriate to generate n-gram\nsentences from random seed words. Instead, for every sen-\ntence in the test set we begin the n-gram model with the ﬁrst\nword of the utterance. While this allows the model to always\ngenerate its ﬁrst token correctly, this does not directly impact\nour measure of productivity as it relies on combinations of\ntokens.\nProductivity Measures\nWe measure the productivity of our autoencoders through the\noverlap score described in C. Yang (2011). Words both in\nthe child-directed corpus and the autoencoder-generated out-\nput are tagged using the default part-of-speech tagger from\nNLTK. The empirical overlap scores are simply calculated\nas a percentage of unique nouns that appear immediately af-\nter both the determiners a and the.\nThe expected overlap\nscore is calculated based off of three numbers from the cor-\npus under consideration, the number of unique nouns N, the\nnumber of unique determiners D, and the total number of\nnoun/determiner pairs S. The expected overlap is deﬁned as\nin Equation 1:\nO(N,D,S) = 1\nN\nN\n∑\nr=1\nO(r,N,D,S)\n(1)\nwhere O(r,N,D,S) is the expected overlap of the noun at\nfrequency rank r:\nO(r,N,D,S) = 1+(D−1)(1−pr)S −\nD\n∑\ni=1\n[(dipr +1−pr)S]\n(2)\ndi represents the probability of encountering determiner i,\nfor which we use the relative frequencies of a and the cal-\nculated from the training corpus (39.3% and 60.7%, respec-\ntively). The probability pr represents the probability assigned\nto a particular word rank.\nThe Zipﬁan distribution takes\na shape parameter, a which C. Yang (2011) set equal to 1\nand which we optimize over the training corpus using least\nsquares estimation and set at 1.06:\npr =\n1/ra\n∑N\nn=1( 1\nna )\n(3)\nIt should be noted that Zipﬁan distributions are not perfect\nmodels of word frequencies (Piantadosi, 2014), but assigning\nempirically-motivated values to the determiner probabilities\nand Zipﬁan parameter a represents an improvement upon the\noriginal measure.\nResults\nWe analyze our overlap measures for the adult-generated (i.e.\nchild-directed) as well as the autoencoder and n-gram model-\ngenerated text and present these results in Figure 2. We ana-\nlyze overlap scores across 10 training epochs with three lev-\nels of dropout, 10%, 20%, and 30%. Dropout is typically\nincluded in neural models to encourage the model to better\ngeneralize. We hypothesized that a certain level of dropout\nwould encourage the model to generate novel combinations\nof words that might lead to higher overlap scores. We ﬁnd\nthat with only two training epochs the AEs have already be-\ngun to near their maximum overlap performance. The 30%\ndropout AE achieves the highest level of performance, match-\ning the empirical overlap score of the original corpus. The\n10% and 20% dropout models perform somewhat worse sug-\ngesting that high levels of dropout may be necessary for good\ntext generation.\nIn Table 2, we present the results for the ﬁnal epoch of\nthe AE models as well as for the adult-generated and n-\ngram generated text. We note that the expected overlap mea-\nsure consistently overestimates the productivity of all learn-\ners, including the adult-generated text.\nIt is unclear why\nthis should be the case, but could be a result of capping the\nmodel vocabularies, resulting in lower N values. In particu-\nlar, the autoencoders tend to produce a relatively limited set\nof nouns. Looking at empirical overlap measures, the worst-\nperforming models are the bigram and trigram models with\noverlap scores below 30%. The AEs fair much better all pro-\nducing overlap scores over 50%. The 30% dropout AE is\nactually able to match the overlap score of the original adult-\ngenerated corpus (59.4% vs. 59.3%).\nLooking at the number of unique nouns following a de-\nterminer (N) and the total number of determiner-noun pairs\n(S), it becomes clear there are large differences between the\nn-gram and AE models. The n-gram models tend to pro-\nduce very few determiner-noun pairs (low S) but are likely to\nchoose from any of the nouns in the corpus, leading to high\nN. This fact accounts for the low overlap scores that they\nachieve. In contrast, the AEs follow a pattern which mir-\nrors the adult corpus with few unique nouns but a large num-\nber of noun-determiner pairs. In all cases, however, the AEs\nproduce both fewer unique nouns and fewer noun-determiner\npairs than the original corpus.\nOne possible problem for calculating the expected over-\nlaps comes from the difﬁculty of part-of-speech tagging text\ngenerated by the neural network. Whereas adult-generated\nspeech follows set patterns that machine taggers are built to\nrecognize, the neural network does not necessarily generate\nwell-formed language. Examples of AE-generated text can\nbe found in Table 3. In some cases, the tagger treats items\nthat occur after a determiner as a noun regardless of its typ-\nical usage. For example, in the generated sentence let put\nFigure 2: Empirical overlap scores. Adult-generated speech\nis marked by the solid black line while autoencoder-generated\nspeech is marked by the dashed colored lines. Results are\npresented for three levels of dropout, 10%, 20%, and 30%.\nThe x-axis represents the training epoch of the model.\nN\nS\nExp. Over.\nEmp. Over.\nAdult\n1,390\n34,138\n77.5%\n59.3%\nAE 10%\n861\n29,497\n88.4%\n53.3%\nAE 20%\n870\n28,817\n87.6%\n53.4%\nAE 30%\n816\n31,181\n90.8%\n59.4%\nBigram\n1,780\n5,177\n17.6%\n28.6%\nTrigram\n2,506\n4,595\n11.2%\n22.1%\nTable 2: Expected and empirical overlap scores for adult-\nand autoencoder-generated language with varying levels of\ndropout. Expected overlap scores were calculated as in Yang\n(2011). Empirical overlap was calculated as the percent of\nunique nouns that appeared immediately following both a and\nthe.\nput the over over here, the phrase the over is tagged as a\nDET+N pair. These type of errors are further evidenced by\nthe fact that the trigram language model produces a larger set\nof words tagged as nouns than the original adult-generated\ncorpus (2,506 vs. 1,390).\nAnother explanation for the difference between expected\nand empirical overlaps may come from deviation from a true\nZipﬁan distribution of word frequencies. If word frequencies\nare Zipﬁan, we should expect a perfect correlation between\nlog ranks and log counts. C. Yang (2011) report a correla-\ntion of 0.97, while our larger corpus deviates from this with\nr2 = 0.86. Although we attempt to take this into account by\nﬁtting the Zipﬁan distribution’s shape parameter, this diver-\ngence clearly indicates that further work is needed.\nThe success of the AE model in generating productive\ntext serves as a conﬁrmation that unsupervised neural models\nmight be used in future work to investigate other cognitive\nphenomena. This work does not directly address the ques-\ntion of how infants might learn to produce productive speech,\nit does represent one possible approach. AEs can, for in-\nstance, be thought of as information compression algorithms\nwhich learn to represent high-dimensional data into a low-\ndimensional latent space (Hinton & Salakhutdinov, 2006). If\nthe brain likewise attempts to ﬁnd efﬁcient representations of\nthe stimuli it encounters then it may prove fruitful to investi-\ngate how these representations compare to one another.\nAdult\nAutoencoder\nfalling down\ndown down\nyou’re playing with\nyou’re playing with\nyour bus\nthe head\nwhy did OOV say what’s\nwhat what you say say\nwrong with these apples\nsay with the dada\nTable 3: Example adult and AE-generated language. The AE-\ngenerated text is from the ﬁnal epoch of the AE with 20%\ndropout. In bold is a DET+N combination that does not ap-\npear in the AEs input.\nConclusion\nWhile there is great interest regarding the inclusion of deep\nlearning methods into cognitive modeling, a number of ma-\njor hurdles remain.\nFor the area of language acquisition,\ndeep learning is poised to help answer questions regarding\nthe learnability of complex linguistic phenomena without ac-\ncess to innate, linguistic knowledge. Yet it remains unclear\nwhether unsupervised versions of deep learning models are\ncapable of capturing even simple linguistic phenomena. In\nthis preliminary study, we ﬁnd that a simple autoencoder with\nsufﬁcient levels of dropout is able to mirror the productivity\nof its training data, although it is unclear whether this proves\nproductivity in and of itself.\nFuture work will need to investigate whether more com-\nplex models might be able to generate text with higher pro-\nductivity as well as further investigating how particular model\nchoices impact performance. It would also be worthwhile to\ncompare AEs against simpler models such as a basic LSTM\nlanguage model. While additional work needs to be done to\nmotivate the use of deep learning models as representations of\nhow children might learn, this preliminary work shows how\none might combine techniques from deep learning and devel-\nopmental psychology.\nAcknowledgments\nThe authors thank the reviewers for their thoughtful com-\nments and Lisa Pearl for initial discussion regarding produc-\ntivity.\nReferences\nBloom, L. (1970). Language development: Form and func-\ntion in emerging grammars. MIT Press.\nBrent, M., & Siskind, J. (2001). The role of exposure to iso-\nlated words in early vocabulary development. Cognition,\n81, 31–44.\nBrown, R. (1973). A ﬁrst language: The early stages. Har-\nvard University Press.\nChandar, S., Lauly, S., Larochelle, H., Khapra, M., Ravin-\ndran, B., Raykar, V. C., & Saha, A. (2014). An autoen-\ncoder approach to learning bilingual word representations.\nIn Advances in\nNeural Information Processing Systems\n(pp. 1853–1861).\nChater, N., & Christiansen, M. H. (2010). Language acqui-\nsition meets language evolution. Cognitive Science, 34(7),\n1131–1157.\nChater, N., & Oaksford, M. (2008). The probabilistic mind:\nProspects for bayesian cognitive science. Oxford Univer-\nsity Press.\nCho,\nK.,\nVan Merri¨enboer,\nB.,\nGulcehre,\nC.,\nBah-\ndanau, D., Bougares, F., Schwenk, H., & Bengio, Y.\n(2014). Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint\narXiv:1406.1078.\nDemuth, K., & McCullough, E.\n(2009).\nThe prosodic\n(re)organization of children’s early english articles. Jour-\nnal of Child Language, 36, 173–200.\nHeaﬁeld, K., Pouzyrevsky, I., Clark, J. H., & Koehn, P.\n(2013). Scalable modiﬁed kneser-ney language model es-\ntimation. In Proceedings of the Association of Computa-\ntional Linguistics conference (pp. 690–696).\nHinton, G. E., & Salakhutdinov, R. R. (2006). Reducing\nthe dimensionality of data with neural networks. Science,\n313(5786), 504–507.\nKuczaj, S. (1977). The acquisition of regular and irregular\npast tense forms. Journal of Verbal Learning and Verbal\nBehavior, 16, 589–600.\nLee, M. D.\n(2011).\nHow cognitive modeling can beneﬁt\nfrom hierarchical bayesian models. Journal of Mathemati-\ncal Psychology, 55(1), 1–7.\nLi, J., Luong, M.-T., & Jurafsky, D. (2015). A hierarchical\nneural autoencoder for paragraphs and documents. arXiv\npreprint arXiv:1506.01057.\nMacWhinney, B.\n(2000).\nThe CHILDES project:\nThe\ndatabase (Vol. 2). Psychology Press.\nMcClelland, J., Frank, S., & Mirman, D. (Eds.). (2016). Con-\ntemporary neural network models: Machine learning, arti-\nﬁcial intelligence, and cognition.\nMcClelland, J., Hansen, S., & Saxe, A. (Eds.). (2016). Tuto-\nrial workshop on contemporary deep neural network mod-\nels.\nMcClelland, J. L. (2009). The place of modeling in cognitive\nscience. Topics in Cognitive Science, 1(1), 11–38.\nMeylan, S. C., Frank, M. C., Roy, B. C., & Levy, R.\n(2017).\nThe emergence of an abstract grammatical cat-\negory in children’s early speech. Psychological Science,\n0956797616677753.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Ef-\nﬁcient estimation of word representations in vector space.\narXiv preprint arXiv:1301.3781.\nPearl, L. (2014). Evaluating learning-strategy components:\nBeing fair (commentary on Ambridge, Pine, and Lieven).\nLanguage, 90(3), e107–e114.\nPiantadosi, S. (2014). Zipf’s word frequency law in natural\nlanguage: A critical review and future directions. Psycho-\nnomic Bulletin & Review, 21(5), 1112-1130.\nPine, J. M., Freudenthal, D., Krajewski, G., & Gobet, F.\n(2013). Do young children have adult-like syntactic cat-\negories? Zipf’s law and the case of the determiner. Cogni-\ntion, 127(3), 345–360.\nPine, J. M., & Martindale, H. (1996). Syntactic categories in\nthe speech of young children: The case of the determiner.\nJournal of Child Language, 23(02), 369–395.\nSachs, J. (1983). Talking about the there and then: The emer-\ngence of displaced reference in parent-child discourse. In\nK. Nelson (Ed.), Children’s language (Vol. 4). Lawrence\nErlbaum Associates.\nSaxe, A. (Ed.). (2014). Workshop on deep learning and the\nbrain.\nSmith, L. B. (1999). Do infants possess innate knowledge\nstructures? The con side. Developmental Science, 2(2),\n133–144.\nSocher, R., Pennington, J., Huang, E. H., Ng, A. Y., & Man-\nning, C. D.\n(2011).\nSemi-supervised recursive autoen-\ncoders for predicting sentiment distributions. In Proceed-\nings of the conference on empirical methods in natural lan-\nguage processing (pp. 151–161).\nSuppes, P. (1974). The semantics of children’s language.\nAmerican Psychologist, 29, 103–114.\nThomas, M. S., & McClelland, J. L. (2008). Connectionist\nmodels of cognition.\nCambridge handbook of computa-\ntional cognitive modelling, 23–58.\nValian, V., Solt, S., & Stewart, J. (2009). Abstract categories\nor limited-scope formulae? The case of children’s deter-\nminers. Journal of Child Language, 36(04), 743–778.\nVishnubhotla, S., Fernandez, R., & Ramabhadran, B. (2010).\nAn autoencoder neural-network based low-dimensionality\napproach to excitation modeling for HMM-based text-to-\nspeech.\nIn IEEE International Conference on Acoustics\nSpeech and Signal Processing (pp. 4614–4617).\nYang, C.\n(2011).\nA statistical test for grammar.\nIn Pro-\nceedings of the 2nd workshop on Cognitive Modeling and\nComputational Linguistics (pp. 30–38).\nYang, C. D. (2004). Universal grammar, statistics or both?\nTrends in Cognitive Sciences, 8(10), 451–456.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-06-06",
  "updated": "2017-06-06"
}