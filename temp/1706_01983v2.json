{
  "id": "http://arxiv.org/abs/1706.01983v2",
  "title": "Deep Learning: Generalization Requires Deep Compositional Feature Space Design",
  "authors": [
    "Mrinal Haloi"
  ],
  "abstract": "Generalization error defines the discriminability and the representation\npower of a deep model. In this work, we claim that feature space design using\ndeep compositional function plays a significant role in generalization along\nwith explicit and implicit regularizations. Our claims are being established\nwith several image classification experiments. We show that the information\nloss due to convolution and max pooling can be marginalized with the\ncompositional design, improving generalization performance. Also, we will show\nthat learning rate decay acts as an implicit regularizer in deep model\ntraining.",
  "text": "Deep Learning: Generalization Requires Deep\nCompositional Feature Space Design\nMrinal Haloi\nIndian Institute of Technology, Guwahati\nh.mrinal@iitg.ernet.in\nAbstract—Generalization error deﬁnes the discriminability\nand the representation power of a deep model. In this work,\nwe claim that feature space design using deep compositional\nfunction plays a signiﬁcant role in generalization along with\nexplicit and implicit regularizations. Our claims are being\nestablished with several image classiﬁcation experiments. We\nshow that the information loss due to convolution and max\npooling can be marginalized with the compositional design,\nimproving generalization performance. Also, we will show that\nlearning rate decay acts as an implicit regularizer in deep model\ntraining.\nKeywords: Generalization, Deep Compositional Design, Con-\nvolutional Network, Deep Learning\nI. INTRODUCTION\nDeep learning massive success in almost every ﬁelds rep-\nresents its ability to solve complex problems. The trade-off\nbetween model complexity and accuracy is an important area\nof deep learning research. Very complex model with millions\nof parameters [8], [9] proved to the state of the art solution\nfor many vision and natural language problems. A common\nway to measure the performance or generalizability of a\ndeep learning model is to test it on a well discriminative\nvalidation/test set representing the variation of samples of the\ncorresponding problem. Learning very complex model is a\nmatter of the requirements of high computing power and huge\ndataset. So it’s important to understand the optimal complexity\nrequirement for a problem to reduce the burden of computing\npower. In a recent work by Zhang et al. [6], it has been proved\nthat a simple 2 layers neural network with 2n + d parameters\ncan represent any function for n samples in d dimensions. It\nis interesting to see that a simple multilayer perceptron with\nReLU activation can ﬁt a dataset of random labels with zero\ntraining accuracy, but poor generalizability. The problem with\nmere data memorizing is to be blamed for poor performance on\na test set. Preventing the network in memorizing data samples\nin inefﬁcient random high dimensional space is important\nmodel design paradigm. But at the same time, it’s acceptable\nfor a model to memorize the data in an efﬁcient hyperspace\nrepresenting original data distribution. Learning the original\ndata distribution solves the poor validation set performance\nof a model. Designing an optimal network with a minimum\nnumber of parameters will reduce computation costs and\nimprove performance.\nHow do we design model that best ﬁt the original data distri-\nbution? In this work, we will present the importance of deep\ncompositional feature space design with an optimal number\nof parameters. We will prove the rate of feature space size\nreduction matters irrespective of network parameters. Also, we\nwill deﬁne an optimal strategy to relate a number of parameters\nrequirements for a particular feature space representation.\nA. Contribution\nFeature space representability: There exists an optimal\nnumber of nonlinear transformations to represent a particular\nsize features space without losing discriminative information.\nConvolution is a linear operation projecting data from one\nspace to another, using a nonlinear activation ﬁnal output of\nconvolution transformation becomes non-linear. While trans-\nforming features from one space to another space there’s loss\nof information. The loss of information can be understood\nusing singular value decomposition. We have trained multiples\nnetwork with the same number of parameters but a different\nrate of feature space reduction on the CIFAR10 dataset to\nprove our points. Following observations have strong impact\non the learning and the generalization performance:\n• The rate of reductions of the feature space size with\nrespect to a number of convolution operations.\n• onvolution vs max pooling for feature space reduction.\nAn optimal number of model parameters: There exists\nan optimal number of model parameters for a model to\nachieve high generalization accuracy. The number of optimal\nparameters depend on the compositional design and the feature\nspace reduction rate.\nImplicit Regularizer: Learning rate decay policy acts as an\nimplicit regularizer boosting the model performance and faster\nlearning.\nNote: here with the term feature space size, we talk about\nwidth and height of a feature map, not the number of feature\nmaps.\nB. Related Work\nZhang et al.(2017)[6] studied the representational power of\na network with respect to the training sample size; shows that a\ndeep model can memorize any dataset with random labels, but\nit doesn’t imply generalization. Their ﬁnding also stated that\nexplicit regularization alone can’t prevent poor generalization\nperformance. Barlett (1998)[1] showed that VC dimension is\nnot relevant to measure generalization performance for neural\nnetworks, rather the L1 norm of the network weights is more\nprevalent measure. They deﬁned a fat-shattering dimension\narXiv:1706.01983v2  [cs.LG]  8 Jul 2017\nfor error estimation that depends on parameter magnitude.\nMaass (1995)[5] has established the VC dimension bounds\nfor the neural network with various activation functions for\ngeneralization analysis. Krogh et al. (1992)[4] showed that\nweight decay suppresses any irrelevant weights vector compo-\nnent also reduce noises, hence lowering generalization errors.\nHardt et al. (2016)[7] introduced a generalization error upper\nbound for a model trained with stochastic gradient descent\nfor convex and non-convex optimization problems. Neyshabur\net al. (2015)[3] shows that apart from adding L2 weight\ndecay or implicit regularization, increasing the network size\nimproves generalization performance for a model learned with\nstochastic gradient descent. They also asserted that with very\nhigh number of hidden units (> samplesize) a weight decay\nregularized network is considered as a convex neural net\nfor optimization. On the effectiveness of deep networks vs\nshallow networks Mhaskar et al. (2016)[10], showed that VC\ndimension and fat-shattering dimension are smaller for deep\nnetworks than shallow networks. They argued the beneﬁts\nof compositional function design for scalability and shift\ninvariance in image and text data. All these above works didn’t\ndiscuss the prominent effect of the loss of information while\ndata is projected from one space to another. We will show\nhere that the information loss effect the generalization error\nfor deep or shallow networks.\nII. FEATURE SPACE DESIGN IN DEEP NEURAL NETWORKS\nA. Information loss\nThe standard convolution operation used in the deep net-\nwork is linear. When feature space is projected from one space\nto another using convolution/pooling operation there’s always\na loss of information. The information loss depends on the\nprojected space dimension and capacity. Loss of information\ncan be understood from singular value decomposition (SVD).\nIf Fi denotes the input for ith convolutional layer\nF j\ni F jT\ni\n= WEW T\n(1)\nF j\ni,proj = W T\nd F j\ni\n(2)\nF j\ni is the jth input map, the equation (1) refer to a special\ncase where the input matrix F j\ni F jT\ni\nfor SVD is Hermitian and\npositive deﬁnite. When data is projected from n dimension\nto low-rank approximation d < n dimension, there’s loss of\ninformation. 2-D convolution for a single feature map with\na single kernel can be interpreted as projecting data from\none space to another. Signiﬁcant information is lost when\nconvolution stride > 1. Another way to calculate the retrived\ninformation after convolution is using the mutual information\nbetween two signals. For two independent signals X and Y,\nmutual information I(X, Y) = 0 and maximum if X ≈Y (fully\ncorrelated). For X and Y we deﬁne information loss as follows:\ninformationloss ∝\n1\nI(Y ; X)\n(3)\nMutual information between the original data and projected\ndata (convolution/pooling) is given as follows\nI(F j\ni,proj; F j\ni ) = H(F j\ni ) −H(F j\ni |F j\ni,proj)\n(4)\nFor N −D normal random vectors X ∼N(µX, CX) and Y ∼\nN(µY , CY ) mutual information can be calculated as follows:\nI(X; Y ) = H(X) −H(X|Y )\nI(X; Y ) = 1\n2log(πe)Nlog(|CX||CY |\n|C|\n)\nC =\n\u0014 CX\nCXY\nCY X\nCY\n\u0015\n(5)\nConvolution of a 2−D image with n×n kernel is equivalent\nto the same with 1 × n and n × 1 kernels; 2 1-D convolution.\nWhere C∗are covariance matrices for respective variables.\nSince we are using batch normalization also input whitening,\nit’s safe to assume input/output of convolution as normal.\nApart from that convolutional kernel also initialized as normal\nvariables. If the convolution input X is sampled from a normal\ndistribution X ∼N(µX, CX) and the kernel K ∼N(µK, CK),\nthen the ouput will also be a normal with distribution Y\n∼N(µX + µK, CX + CK). Using eq 3 and eq 5 we can\ncalculate information loss for convolution/pooling.\nB. Compositional Design of Convolutional Layer\nA single convolution layer followed by max pooling results\nin high information loss. Increasing the number of convolution\nkernel for a layer doesn’t necessarily solve the problem.\nBefore reducing the feature space size it’s important to project\nthe feature into high dimensional hyperspaces using multiple\nconvolution operations. It’s important to use non-linearity\nand batch normalization [13] for each convolution operation\nto achieve highly uncorrelated hyperspace projection. The\nstacking design best resembles the representation power of the\ncompositional function. Also, the VC dimension of composi-\ntional design is smaller than that of shallow design [10]. With\nthe composition of multiples convolution operation, receptive\nﬁeld grows in polynomial order. Compositional design inspired\nfrom the visual cortex increasing receptive ﬁelds for higher\nvisual areas. It has been established [12] that visual cortex\nreceptive ﬁelds are larger for a simple scene and smaller for a\ncomplex scene. Compositional design has smaller and larger\nreceptive ﬁelds those capture information related to simple and\ncomplex objects leading to decrease in information loss.\nfshallow(input) = conv−> bn−> relu(input)\nreceptiveField = filtersize\n(6)\nH = conv−> bn−> relu\nfcomposition(input) = H(H(H(H(input))))\nreceptiveField = 1 + 4(filtersize −1)\n(7)\nThe above equations ((6), (7)) only valid for convolution stride\n1.\nC. Convolution vs Max Pooling\nClaim 1.1 Strided convolution replaces max/avg pooling\nwith better generalization performance.\nFeature space reduction using max pooling is a very crude pro-\njection into another hyper space. Max pooling operation leads\n    Conv\n  BN\n  \nReLU/PReLU\nN x\nConv block input\nConv block output\nconv-block\nmax-pool\nconv-block\nconv-block\nconv-n-a\nconv stride 2\nconv-block\nconv-block\nconv-n-a\nHigh information loss\nLow information loss\nFig. 1. Left: A convolutional block design with compositional convolutional\noperations (conv-n-a means convolution followed by normalization and non-\nlinearity). Middle: design with max-pool for downsampling. Right: convolu-\ntion for downsampling\nto lossy non-linear transformation. The translational invariance\nwhich is one of the advantages of max pooling operation\ncan be easily well represented using afﬁne transformation,\nachieved with strided convolution. Information loss in strided\nconvolution is lower than hard non-linear max pooling. Fig 1\nillustrates model design with strided convolution and max\npooling.\nTable II proves our claims.\nD. Rate of Reduction\nClaim 1.2 Minimum one convolutional operation needed\nbefore reducing the feature space size of the model.\nTo minimize loss of information, projecting the data into\nmutliples non-linear hyper space is required for improved\ngeneralization. Minimum one convolution for the ﬁrst layer of\nthe model without stride and maximum 4 convolution opera-\ntions without residual connection is preferable for intermediate\nlayers; with residual connection, > 4 convolutional operations\ncan be added. It is also noteworthy that residual connection\nonly facilitate the training of deeper model.\nIII. GENERALIZATION REQUIRES FEATURE SPACE\nANALYSIS\nA. VC dimension and Fat Shattering dimension\nA function f : [a, b] is considered as Lipschitz function if\nit satisﬁes the following condition for a smallest constant c:\n|f(x) −f(x\n′)| ≤c|x −x\n′|\n∀x, x\n′ ∈[a, b]\n(8)\nFor deep networks non-linearity like ReLU is lipschitz func-\ntion for x ∈[0, ∞] and sigmoid for x ∈Rn\nVC dimension for a neural network class H with l layers,\ninputs X ⊂Rn and ReLU activation function is given as\nfollows [5]\nlimx→∞relu(x) ̸= limx→−∞relu(x)\nx ∈R\nrelu′(x) ̸= 0\nV Cdim(Net) = O(wllogw + wl2)\n(9)\nSigniﬁcance of VC dimension analysis for deep convolutional\nneural network training is marginal. The theretical O(n2)\ncomplexty for n number of total parameters of convolutinal\nmodel is rather very high upper bound for consideration in\ngeneralization analysis. It has been established that implicit\nand explicit regularization improves generalization. Fat Shat-\ntering dimension of a neural network class G with l layers\nand inputs X ⊂Rn is given as follows [2] for some constant\nc and λ > 0; number of points λ-shattered by G\nfatG(λ) = O(B2(cA)l(l+1)\nλ2(l−1)\n)\nX = x ∈Rn; ∥x∥∞≤B\n∥w∥1 ≤A\n(10)\nFat-shattering dimension is better bound than VC dimension\nfor learning algorithms, as it suggets that minimizing the\nvalues of networks parameters is important for better gener-\nalization. The values of model parameters can be minimized\nusing L1/L2 regularizer; achieved adding a extra penality term\nto the cost function using respective norms.\nE(W)L1 = E(W) + ∥W∥1\nE(W)L2 = E(W) + ∥W∥2\nE(W)L1+L2 = E(W) + ∥W∥1 + ∥W∥2\n(11)\nB. Implicit and explicit regularization\nMost widely used and effective explicit regularizers are\nData Augmentation, Data Balancing, dropout, l1 regularizer,\nl2 regularizer.\nData Augmentation: Due to increasing in capacity fo the\ndeep network and scarcity of enough discriminant labeled\ndata, it’s useful to generate deformed version fo original\ntraining examples using afﬁne transformation such as rotation,\ntranslation, shearing, mirroring and random cropping. Apart\nfrom that color space transformation such as RGB to Lab\nor HSV, also cropping and resizing proved to be effective in\nreducing overﬁtting.\nData Balancing: For a dataset with biased sample classes,\nlearning tends to overﬁt the class with high bias, results in poor\ngeneralization performance. Sample balancing methods such\nas uniform sampling and stratiﬁed sampling resample from\ndata for balanced mini batches as per the given probability\ndistribution of the classes.\nDropout: Dropping layer activation randomly realizes en-\nsemble of many functions for that layer, it helps reducing\noverﬁtting.\nL1 & L2 regularizer: L1 regularizer encourages sparsity by\nminimizing the L1 norm of the model weights. L2 regularizer\npenalizes model complexity, leads to small weights. The added\ncombination of L1 & L2 regularizations encourages sparsity\nwith small weights. The effectiveness of each regularization\ndepends on the application; in general L2 regularization works\nperfectly ﬁne.\nNormalization: Batch Normalization (BN) [13] is one of\nthe de facto implicit regularization for faster and better gener-\nalization learning of feedforward deep convolutional model.\nBN normalizes the layer inputs to a zero mean and unit\nvariance distribution. A model with BN can put an end to the\nbias term necessity. In the case of recurrent neural network\nlayer normalization [14] proved to be efﬁcient than batch\nnormalization. Another useful implicit regularizer is Local\nResponse Normalization (LRN). LRN [16] is inspired from the\nlateral inhibition of an excited neuron. Unbounded activations\nare normalized using the values of the local window. For appli-\ncations such as person re-identiﬁcation [15] LRN outperforms\nBN.\nAn experimental analysis of the effectiveness of the batch\nnorm and dropout can be observed from Table III\nC. Feature Space Analysis\nClaim 1.3: The rate of reduction of feature space size with\nrespect to the number of convolutional layers plays important\nrole in generalization.\nA detailed analysis is given in Section II on the impact\nof feature space design for features representation without\nlosing signiﬁcance information. Efﬁcient feature space design\nimproves generalization performance by a fair margin. Table I\nproves our claim.\nD. Learning Rate Decay\nClaim 1.4: Learning rate decay policy acts as an implicit\nregularizer for deep model learning.\nLearning rate decay policy plays a major role in generalized\nparameter learning with faster convergence. As the learning\nprogress, exploration in local neighborhood becomes more\nimportant to do away with oscillation and ill-conditioning. The\nTaylor series approxmation of the cost function f(x):\nf(x) ≈f(x0) + (x −x0)T g + 1\n2(x −x0)T H(x −x0)\nf(x −ϵg) ≈f(x0) −ϵgT g + 1\n2ϵ2gT Hg\n(12)\nwhere g and H are the gradient and the hessian of the cost\nfunction. When the values of H are large cost increases, this\neffect is known as ill conditioning and a common probelm with\ndeep learning training. The learning rate ϵ decay alleviate the\neffect of ill conditioning leading to low cost space exploration.\nIn practice, polynomial decay works very well in comparison\nto step decay or inverse decay methods. Step decay needs more\nsupervision, better not to use.\nFrom Table IV we can see that polynomial decay performs\nmuch better than step decay.\nE. Optimal Number of Parameters\nIn the deep model impact of VC dimension is marginal. The\nfat shattering dimension plays important role in regularization.\nIn determining the optimal numbers of parameters feature\nspace design comes into play. Inefﬁcient shallow model or\nextra deep model may underﬁt/overﬁt the data resulting in poor\ngeneralization performance. As per the discussion in Section\nTABLE I\nRESULTS OF 3 MAIN DESIGNS\nModel\n#params(K)\ntest accuracy (%)\ndesign 1\n20173\n89.4\ndesign 2\n20173\n86.8\ndesign 3\n20025\n87.9\nTABLE II\nCONVOLUTION VS MAX POOLING\nModel\n#params (K)\ntest accuracy (%)\ndesign 1 conv\n20948\n91.7\ndesign 1 (max pooling)\n20173\n89.4\nTABLE III\nRESULTS OF EXPLICIT REGULARIZATION\nModel\ndropout\nbatch norm\ntest accuracy (%)\ndesign 1 conv\nyes\nyes\n91.7\ndesign 1 conv\nyes\nno\n88.2\ndesign 1 conv\nno\nyes\n90.1\nTABLE IV\nRESULTS OF LEARNING RATE DECAY POLICY\nModel\npolicy\ntest accuracy (%)\ndesign 1 conv\npolynomial\n91.7\ndesign 1 conv\nstep\n90.1\nTABLE V\nRESULTS OF RATE OF REDUCTION\nModel\nﬁrst layer stride\ntest accuracy (%)\ndesign 1 conv\nno\n91.7\ndesign 1 conv stride\nyes\n89.4\nTABLE VI\nDEPTH\nModel\n#params (K)\ntest accuracy (%)\ndesign 1 conv\n20948\n91.7\ndesign 4\n21573\n89.3\nII, feature space design plays a major role in powerfull rep-\nresentation on uncorellated hyperspaces reducing information\nloss.\nClaim 1.5: The optimal numbers of parameters, is the\nnumber of parameters of a optimal model designed using\nfeature space analysis.\nTable VI and I gives an experimental validation of this claim.\nIV. EXPERIMENT SETUP\nPlatform Details: All our experiments were carried out on a\nLinux server with 128GB RAM, Xeon E5-4667 v4 processor,\nand two Nvidia K80 GPUs.\nTABLE VII\nNETWORK DESIGN\nblock\ndesign 1\ndesign 1 conv\ndesign 2\ndesign 3\ndesign 4\ninput\n28 x 28 x 3\n28 x 28 x 3\n28 x 28 x 3\n28 x 28 x 3\n28 x 28 x 3\nblock1\n1 x conv 3x3, 1, 64\n1 x conv3x3, 1, 64\n1 x conv3x3, 1, 64\nconv3x3, 64\n2 x conv3x3, 1, 64\nblock2\nmax pool\n1 x conv3x3, 2, 64\nmax pool\nmax pool\n1 x conv3x3, 2, 64\nblock2 1\n-\n-\n-\n-\n1 x conv1x1, 2, 128\nblock3\n2 x conv 3x3, 1, 128\n2 x conv3x3, 1, 128\n1 x conv3x3, 1, 128\n1 x conv3x3, 1, 128\n3 x conv3x3, 1, 128\nblock3 1\n-\n-\nmax pool\n-\nblock2 1 + block3\nblock3 2\n-\n-\n1 x conv3x3, 1, 128\n-\n3 x conv3x3, 1, 128\nblock4\nmax pool\n1 x conv3x3, 2, 128\nmax pool\nmax pool\n1 x conv3x3, 2, 128\nblock5\n4 x conv 3x3, 1, 256\n4 x conv3x3, 1, 256\n4 x conv3x3, 1, 256\n4 x conv3x3, 1, 256\n4 x conv3x3, 1, 256\nblock6\nmax pool\n1 x conv3x3, 2, 256\nmax pool\nmax pool\n1 x conv3x3, 2, 256\nblock7\n1 x conv 1x1, 1, 4096\n1 x conv1x1, 1, 4096\n1 x conv1x1, 1, 4096\n1 x conv1x1, 1, 4096\n1 x conv1x1, 1, 4096\nblock7 1\ndropout\ndropout\ndropout\ndropout\ndropout\nblock8\n1 x conv 1x1, 1, 4096\n1 x conv1x1, 1, 4096\n1 x conv1x1, 1, 4096\n1 x conv1x1, 1, 4096\n1 x conv1x1, 1, 4096\nblock8 1\ndropout\ndropout\ndropout\ndropout\ndropout\nblock9\n1 x conv 1x1, 1, 10\n1 x conv1x1, 1, 10\n1 x conv1x1, 1, 10\n1 x conv1x1, 1, 10\n1 x conv1x1, 1, 10\nDataset: To validate our claims we have used image clas-\nsiﬁcation CIFAR10 [17] dataset. It has 10 object classes and\ndivided into two splits for training and validation. The training\nsplit has 50000 and the validation split has 10000 images. The\nsize of each image is 32 × 32 × 3, RGB color channels.\nPreprocessing: For training a randomly cropped patch of\nsize 28 × 28 × 3 is used. Each patch is ﬂipped left/right and\nup/down based on coin ﬂipping results. Apart from that, we\nadjust the image color by scaling its values into [0, 1] range\nand changing its hue, contrast, and saturation. Each image\n(training/validation) is standardized by subtracting its mean\nand dividing its standard deviation.\nFor evaluation, the central crop of each image is selected and\nresized using bilinear interpolation.\nFramework: We have used TEFLA [18], a python frame-\nwork developed on the top of TENSORFLOW [19], for all\nexperiments described in this work.\nModel: Table VII details model design for different exper-\niments. Conventions are followed as (repeat\n×\nconv3 ×\n3, , stride, num kernels); where repeat is the number of\nconvolution for composition design, stride is the stride for\nconvolution and num kernels is the number of kernel for\neach convolution layer. Each convolutional layer of a model\nis followed by a batch normalization and a non-linearity (relu\nfor ur experiments) layer for all designs experimented in this\nwork.\nA. Results Analysis\nTable I shows performance of each model on CIFAR10\ndataset validation/test set. For design 1 and design 2 with\nthe same number of parameters, generalization performance\nvaries signiﬁcantly, asserting the importance of feature space\nsize importance and minimization of information loss.\nTable II shows the importance of strided convolution for\nfeature space size reduction than max pooling. Signiﬁcance\nperformance gain is observed while using convolution instead\nof max pooling; implying the information loss for max pooling\nis higher than strided convolution. For design 1 conv if we use\nstrided convolution for the ﬁrst layer instead of the second\nthere’s a signiﬁcant drop of generalization performance even\nthough number of parameters remain same, Table V.\nTable III shows the importance of dropout and batch nor-\nmalization for generalization. Effect of batch normalization is\nhigher than the dropout.\nTable IV proves our claim that learning rate decay policy\nalso acts as implicit regularizer improving generalization per-\nformance. polynomial decay is very robust and requires min-\nimal supervision, yielding better generalization performance.\nPerformance doesn’t always depend on more depth, an\noptimal design performs better than a deeper design, from\nTable VI we can see the experimental results of two design.\nFrom this, we can conclude that there exists an optimal number\nof parameters for generalization.\nV. CONCLUSION\nIn this work, a detailed analysis of deep model generaliza-\ntion performance trade-off is presented. We showed that the\ncompositional feature space design with implicit and explicit\nregularizations play important role in achieving better perfor-\nmance. In terms of model complexity traditional measure, VC\ndimension doesn’t give much information, but fat-shattering\ndimension analysis has an indirect effect on generalization.\nFrom our experiment, we showed that the optimal model\nsatisﬁes compositional design criteria and have the optimal\nnumber of parameters. We wrap up this work with the claim\nthat combination of compositional feature space design with\nexplicit and implicit generalization and efﬁcient optimization\nalgorithms give the best-generalized performance for any\ndataset.\nREFERENCES\n[1] Peter L Bartlett. The Sample Complexity of Pattern Classiﬁcation with\nNeural Networks - The Size of the Weights is More Important than the\nSize of the Network. IEEE Trans. Information Theory, 1998.\n[2] Bartlett, P. L. For valid generalization, the size of the weights is more\nimportant than the size of the network. Advances in neural information\nprocessing systems, 134-140, 1997\n[3] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the\nreal inductive bias: On the role of implicit regularization in deep learning.\nCoRR, abs/1412.6614, 2014.\n[4] Krogh, A. and Hertz, J. A. A simple weight decay can improve general-\nization. In Proc. NIPS, pp. 950957, 1992.\n[5] Maass, W. Vapnik-Chervonenkis dimension of neural nets. The handbook\nof brain theory and neural networks, pp. 1000-1003, 1995.\n[6] Zhang, Chiyuan, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.\nUnderstanding deep learning requires rethinking generalization. arXiv\npreprint arXiv:1611.03530, 2016.\n[7] Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize\nbetter: Stability of stochastic gradient descent. In ICML, 2016.\n[8] Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A. Inception-v4, inception-\nresnet and the impact of residual connections on learning. arXiv preprint\narXiv:1602.07261, 2016.\n[9] He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image\nrecognition. Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. 2016.\n[10] Mhaskar, H., Liao, Q., Poggio, T. Learning functions: When is deep\nbetter than shallow. arXiv preprint arXiv:1603.00988, 2016.\n[11] Ioffe, S., Szegedy, C. Batch normalization: Accelerating deep net-\nwork training by reducing internal covariate shift. arXiv preprint\narXiv:1502.03167, 2015.\n[12] Trappenberg, T. P., Rolls, E. T., Stringer, S. M. Effective size of\nreceptive ﬁelds of inferior temporal visual cortex neurons in natural\nscenes. Advances in neural information processing systems, 1, 293-300,\n2002.\n[13] Ioffe, S., Szegedy, C. Batch normalization: Accelerating deep net-\nwork training by reducing internal covariate shift. arXiv preprint\narXiv:1502.03167, 2015.\n[14] Ba, J. L., Kiros, J. R., Hinton, G. E. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[15] Varior, R. R., Haloi, M., Wang, G. Gated siamese convolutional neural\nnetwork architecture for human re-identiﬁcation. In European Conference\non Computer Vision (pp. 791-808). Springer International Publishing,\n2016.\n[16] Krizhevsky, A., Sutskever, I., Hinton, G. E. Imagenet classiﬁcation with\ndeep convolutional neural networks. In Advances in neural information\nprocessing systems (pp. 1097-1105), 2012.\n[17] Krizhevsky, A., Hinton, G. Learning multiple layers of features from tiny\nimages. Technical report, Department of Computer Science, University of\nToronto, 2009\n[18] https://github.com/n3011/teﬂa\n[19] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C.,\n... Ghemawat, S. (2016). Tensorﬂow: Large-scale machine learning on\nheterogeneous distributed systems. arXiv preprint arXiv:1603.04467.\nVI. APPENDIX\nA. Effective receptive ﬁeld\njout = jin ∗s\nrout = rin + (k −1) ∗jin\n(13)\nwhere j is the distance between two adjacent feature maps;\ns is the convolution stride, k is the kernel size and r is the\nreceptive ﬁelds size.\nB. Learning rate decay policy\nCommonly used learning rate decay policies are given\nbelow:\nfixed :λ = c\n(14)\nexponential :λiter = λ0 ∗γiter\n(15)\nstep :λiter = λ0 ∗γ\niter\nstep\n(16)\ninverse :λiter = λ0 ∗(1 + γ ∗iter)−c\n(17)\npoly :λiter = λ0 ∗(1 −\niter\nmax iter)c\n(18)\nsigmoid :λiter = λ0 ∗\n1\n1 + exp(−γ+(iter−step))\n(19)\nwhere c and γ are two constants; λ0 is the initial learning\nrate, λiter is the learning for iter (current iteration). max iter\nis the maximum number of iteration for learning and step is\nthe step for changing learning rate for step policy.\nC. Mutual information for normal random variables\nFor a normal random variables X ∼N(µ, C), entropy of X\nis calculated as follows\nH(X) = −Xloga(X)\n= −\nZ\np(x)logap(x)dx\n=\nZ\np(x)[1\n2loga(2π)n|C| + 1\n2(x −µ)T C−1(x −µ)logae]dx\n= 1\n2loga(2π)n|C| + 1\n2logaeE[(x −µ)T C−1(x −µ)]\n= 1\n2loga(2π)n|C| + 1\n2nlogae\n= 1\n2loga(2πe)n|C|\nFor another normal random variables Y ∼(Nµ, CY ); the\nmutual information between X and Y can be calculated as\nfollows:\nI(X; Y ) = H(X) −H(X|Y )\n= H(X) + H(Y ) −H(X, Y )\n= 1\n2loga(2πe)n|C| + 1\n2loga(2πe)n|CY | −1\n2loga(2πe)n|CXY |\n= 1\n2loga(2πe)n |C||CY |\n|CXY |\n",
  "categories": [
    "cs.LG",
    "stat.ML",
    "68T45"
  ],
  "published": "2017-06-06",
  "updated": "2017-07-08"
}