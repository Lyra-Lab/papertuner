{
  "id": "http://arxiv.org/abs/1706.03235v3",
  "title": "ACCNet: Actor-Coordinator-Critic Net for \"Learning-to-Communicate\" with Deep Multi-agent Reinforcement Learning",
  "authors": [
    "Hangyu Mao",
    "Zhibo Gong",
    "Yan Ni",
    "Zhen Xiao"
  ],
  "abstract": "Communication is a critical factor for the big multi-agent world to stay\norganized and productive. Typically, most previous multi-agent\n\"learning-to-communicate\" studies try to predefine the communication protocols\nor use technologies such as tabular reinforcement learning and evolutionary\nalgorithm, which can not generalize to changing environment or large collection\nof agents.\n  In this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) framework\nfor solving \"learning-to-communicate\" problem. The ACCNet naturally combines\nthe powerful actor-critic reinforcement learning technology with deep learning\ntechnology. It can efficiently learn the communication protocols even from\nscratch under partially observable environment. We demonstrate that the ACCNet\ncan achieve better results than several baselines under both continuous and\ndiscrete action space environments. We also analyse the learned protocols and\ndiscuss some design considerations.",
  "text": "ACCNet: Actor-Coordinator-Critic Net for\n‚ÄúLearning-to-Communicate‚Äù\nwith Deep Multi-agent Reinforcement Learning\nHangyu Mao1, Zhibo Gong2‚àó, Yan Ni1‚àóand Zhen Xiao1\n1 Peking University\n2 Huawei Technologies Co., Ltd.\n‚àó\nOctober 31, 2017\nAbstract\nCommunication is a critical factor for the big multi-agent world to stay orga-\nnized and productive. Typically, most previous multi-agent ‚Äúlearning-to-communicate‚Äù\nstudies try to predeÔ¨Åne the communication protocols or use technologies such as\ntabular reinforcement learning and evolutionary algorithm, which cannot general-\nize to the changing environment or large collection of agents directly.\nIn this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) frame-\nwork for solving multi-agent ‚Äúlearning-to-communicate‚Äù problem. The ACCNet\nnaturally combines the powerful actor-critic reinforcement learning technology\nwith deep learning technology. It can learn the communication protocols even from\nscratch under partially observable environments. We demonstrate that the ACCNet\ncan achieve better results than several baselines under both continuous and discrete\naction space environments. We also analyse the learned protocols and discuss some\ndesign considerations.\nIntroduction\nCommunication is an important factor for the big multi-agent world to stay organized\nand productive. For applications where individual agent has limited capability, it is\nparticularly critical for multiple agents to learn communication protocols to work in a\ncollaborative way, for example: data routing [1], congestion detection [2] and air trafÔ¨Åc\nmanagement [3].\nHowever, most previous multi-agent ‚Äúlearning-to-communicate‚Äù studies try to pre-\ndeÔ¨Åne the communication protocols or use technologies such as tabular reinforcement\n‚àóThese authors contribute equally to this study.\n1\narXiv:1706.03235v3  [cs.AI]  29 Oct 2017\nlearning (RL) and evolutionary algorithm, which cannot generalize to the changing en-\nvironment or large collection of agents directly. We argue that this Ô¨Åeld requires more\nin-depth studies with new technologies.\nRecently, we researchers have seen the success of Deep MARL, i.e., the combina-\ntion of deep learning (DL) and multi-agent reinforcement learning (MARL), in many\napplications, such as self-play Go [5], two-player Pong [6] and multi-player StarCraft\n[7]. However, those work either assume full observability of the environment or lack\ncommunication among multiple agents.\nNaturally, in this paper, we ask and try to answer a question: can we learn multi-\nagent communication protocols even from scratch under partially observable distributed\nenvironments with the help of Deep MARL?\nWe consider the setting where multiple distributed agents are fully cooperative with\nthe same goal to maximize the shared discounted sum of rewards R in a partially ob-\nservable environment. Full cooperation means that all agents receive the same R inde-\npendent of their contributions. Partially observable environments mean that no agent\ncan observe the underlying Markov states and they must learn effective communication\nprotocols. In fact, the problem setting can be exactly modelled as Dec-POMDP-Com\n[8, 9], which is an extension of Dec-POMDP [10, 11] when considering communica-\ntion. The novelty is that the communication bandwidth is limited.\nThe limited communication bandwidth is a common setting for recent ‚Äúlearning-\nto-communicate‚Äù studies [20, 12, 23, 4]. Traditional cooperative agents can share sen-\nsations, learned policies or even training episodes [19, 20], which is not suitable for\nreal-world applications because communication itself takes up much bandwidth. In our\nopinion, limited communication bandwidth has two meanings. On the one hand, the\nmessage at a speciÔ¨Åc timestep can be transported using a few packets so that it will\nnot take too much bandwidth. On the other hand, only valuable message is necessary\nto further reduce the bandwidth requirement. That is to say, message only comes from\ntime to time, and the intermittent time is task-speciÔ¨Åc. To achieve the former limited\nbandwidth, we suggest to use deep neural networks to compress the message so that\nboth the message dimension and the packets needed for transporting the message can\nbe controlled. And for the latter, we will introduce corresponding methods based on\nGating mechanism and Token mechanism in another paper because of the space limi-\ntation.\nTo this end, we propose an Actor-Coordinator-Critic Net (ACCNet) framework,\nwhich combines the powerful actor-critic RL technology with DL technology. The AC-\nCNet has two paradigms. The Ô¨Årst one is AC-CNet, which learns the communication\nprotocols among actors with the help of coordinator and keeps critics being indepen-\ndent. However, the actors of AC-CNet inevitably need communication even during\nexecution, which is impractical under some special situations [12]. The second one is\nA-CCNet, which learns the communication protocols among critics with the help of\ncoordinator and keeps actors being independent. As actors are independent, they can\ncooperate with each other even without communication after A-CCNet is trained well.\nNote that, actor and critic are not two different agents but two services in one agent.\nWe explore the proposed ACCNet under different partially observable environ-\nments. Experiments show that: (1) both AC-CNet and A-CCNet can achieve good re-\nsults for simple multi-agent environments; (2) for complex environments, A-CCNet\n2\nhas a better generalization ability and performs almost like the ideal fully observable\nmodels. To the best of our knowledge, this is the Ô¨Årst work to investigate multi-agent\n‚Äúlearning-to-communicate‚Äù problem based on deep actor-critic RL architecture under\npartially observable environment1.\nThe rest of this paper starts from a brief review of actor-critic RL algorithms and the\nreleted work. We then present the ACCNet, followed by experiments and conclusion.\nBackground\nReinforcement learning (RL) [13] is a machine learning approach to solve sequential\ndecision making problem. At each timestep t, the agent observes a state st and takes an\naction at, and then receives a feedback reward rt from the environment and observes\na new state st+1. The goal of RL is to learn a policy œÄ(a|s), i.e., a mapping from\nstate to action, which can maximize the expected discount cumulative future reward\nE[R] = E[PT\nt=0 Œ≥trt].\nModel-free RL algorithms can be divided into three groups [14, 15]. (1) Actor-only\nmethods directly learn the parameterized policy œÄ(a|s; Œ∏). They can generate continu-\nous action but suffer from high variance in the estimation of policy gradient. (2) Critic-\nonly methods use low variance temporal difference learning to estimate the Q-value\nQ(s, a; w) = E[R; s, a]. The policy can be derived using greedy action selection, i.e.,\nœÄ(a|s) = a‚àó= arg maxa Q(s, a; w). They are usually used for discrete action as Ô¨Ånd-\ning a‚àóis computationally intensive in continuous action space. (3) Actor-critic methods\njointly learn œÄ(a|s; Œ∏) and Q(s, a; w). They preserve the advantages of both actor-only\nand critic-only methods.\nActor\nùùÖ(ùíÇ|ùíî; ùúΩ)\nEnvironment\nCritic\nùëΩùíî; ùíò\nŒ¥=r+V(s‚Äô)- V(s)\nState s\nAction a\nReward r\nTD-error Œ¥\nFigure 1: The schematic overview of actor-critic algorithms. The dashed lines indicate\nthat the critic is responsible for updating the actor and itself.\nThe schematic structure of actor-critic methods is shown in Figure 1. Two functions\nreinforce each other: correct actor œÄ(a|s; Œ∏) gives high rewarding trajectory (s, a, r, s‚Ä≤),\nwhich updates critic V (s; w) or Q(s, a; w) towards the right direction; correct critic\nV (s; w) or Q(s, a; w) picks out the good action for actor œÄ(a|s; Œ∏) to reinforce. This\nmutual reinforcement behavior helps actor-critic methods avoid bad local minima and\n1One similar work [33] from OpenAI is released at the same time. Another concurrent work [34] from\nOxford also uses a similar idea. They do not explicitly address the ‚Äúlearning-to-communicate‚Äù problem, but\nwe afÔ¨Årm each other‚Äôs methods and results mutually. A comparison between ACCNet and all those related\nstudies are shown in Table 1.\n3\nconverge faster, in particular for on-policy methods that follow the very recent policy\nto sample trajectory during training [7]. SpeciÔ¨Åcally, if actor uses stochastic policy for\naction selection, the actor and critic are updated based on the following TD-error and\nStochastic Policy Gradient Theorem [13]:\nŒ¥t\n=\nrt + Œ≥V (st+1; w) ‚àíV (st; w)\n(1)\nŒ∏t+1\n=\nŒ∏t + Œ± ‚àóŒ¥t ‚àó‚ñΩŒ∏logœÄ(at|st; Œ∏)\n(2)\nIf actor uses deterministic policy for action selection, they are updated based on the\nfollowing TD-error and Deterministic Policy Gradient Theorem [16]:\nŒ¥t\n=\nrt + Œ≥Q(st+1, at+1; w) ‚àíQ(st, at; w)\n(3)\nŒ∏t+1\n=\nŒ∏t + Œ± ‚àó‚ñΩaQ(st, at; w) ‚àó‚ñΩŒ∏œÄ(at|st; Œ∏)\n(4)\nAs ACCNet is based on actor-critic methods, the following articles are strongly recom-\nmended to read: [35], [13], [36], [16] and [18].\nDeep RL (DRL) uses deep neural networks to approximate œÄ(a|s; Œ∏), Q(s, a; w)\nand/or the environment.\nRelated Work\nHow to learn communication protocols efÔ¨Åciently is critical to the success of multi-\nagent systems. Most previous work predeÔ¨Åne the communication protocols [19, 20] and\nsome others use technologies such as tabular RL [21] or evolutionary algorithm [22],\nwhich cannot generalize to the changing environment and large collection of agents\ndirectly as [13, 4] point out.\nRecently, the end-to-end differentiable communication channel embedded in deep\nneural network has been proven useful for learning communication protocols. Gener-\nally, the protocols can be optimized simultaneously while the network is optimized. Our\nwork is an instance of this method, and the most relevant studies include the CommNet\n[23], DIAL [4] and BiCNet [7].\nCommNet is a single network designed for all agents. The input is the concatenation\nof current states from all agents. The communication channels are embedded between\nnetwork layers. Each agent sends its hidden state as communication message to the\ncurrent layer channel. The averaged message from other agents then is sent to the next\nlayer of a speciÔ¨Åc agent. However, single network with a communication channel at\neach layer is not easy to scale up.\nDIAL trains a single network for each individual agent. At each timestep, the agent\noutputs its message as the input of other agents for the next timestep. To learn the\ncommunication protocols, it also pushes gradients from one agent to another through\nthe communication channels. However, the message is delayed for one timestep and\nthe environment will be non-stationary in multi-agent situation.\nBoth CommNet and DIAL are based on DQN [24] for discrete action. BiCNet is\nbased on actor-critic methods for continuous action. It uses bi-directional recurrent\nneural networks as the communication channels. This approach allows single agent to\nmaintain its own internal state and share information with other collaborators at the\n4\nsame time. However, it assumes that agents can know the global Markov states of the\nenvironment, which is no so realistic except for some game environments.\nOther relevant excellent studies include but not limited to [37, 38, 39]. Those re-\nsearchers have veriÔ¨Åed the possibility of learning communication protocols among\nagents. Nevertheless, we aim at providing a general framework to ease the learning\nof communication protocols among agents.\nActor-Coordinator-Critic Net Framework\nIn this section, we present two paradigms of ACCNet framework for learning commu-\nnication protocols based on actor-critic models.\nC\nCoordinator\ncommunication\nchannel\nOther agents\nBP2\nNo\nupdate\nUpdate\nœÄ-net\nBP1\nUpdate\nV-net\nV\nœÄ\nActor\nCritic\naction\nQ(s,a)\nGlobal shared reward\nBP1\nBP2\nNo\nupdate\nUpdate\nœÄ-net\nUpdate\nV-net\nIntegrated state\nM\nV\nœÄ\nActor\nCritic\naction\nQ(s,a)\nreward\nLocal state\nOther agents\nUpdate\nC-net\nUpdate\nM-net\nLocal state\nLocal\nMessage\nchannel\nOther agents\nM\nœÄ\nActor\naction\nQ2(s,a)\nGlobal shared reward\nBP2\nNo\nupdate\nUpdate\nœÄ-net\nBP1\nUpdate\nV-net\nLocal state\nOther agents\nC\nCoordinator\ncommunication\nchannel\nUpdate\nC-net\nUpdate\nM-net\nLocal\nMessage\nchannel\nV2\nCritic2\nV1\nCritic1\nOther agents\nQ1(s,a)\nOther agents\n(a) Actor-Critic model\n(b) AC-CNet\n(c) A-CCNet\nFigure 2: The proposed ACCNet.\nAC-CNet\nThe most straightforward approach is to build a communication channel between actors\nand keep critics being independent. As shown in Figure 2(b), a coordinator communi-\ncation channel is used for coordinating the actors to generate coordinated actions, so\nwe call this paradigm AC-CNet. SpeciÔ¨Åcally, each agent encodes its local state into\na local message and sends it to the coordinator, which further generates the global\ncommunication signal for this agent considering messages from all other agents. As\nthe global signal is an encoding of all local messages, we expect that it can catch the\nglobal information of the system. The integrated state is the concatenation of local state\nand global signal, which will be fed as input into the actor-critic model. Then the whole\nAC-CNet is trained as the original actor-critic model.\nHowever, the AC-CNet inevitably needs communication between actor and coordi-\nnator to get the global information even during execution, which is impractical under\nsome special situations [12, 45].\n5\nA-CCNet\nCan those agents generate actions as if they have shared the global knowledge even\nwithout communication during training? What about during execution? The answer\nmay be NO at the Ô¨Årst glance and we also think so. Fortunately, machine learning has a\nfascinating property that we can do prediction after one model is trained and the auxil-\niary data on which the model is trained need no longer to be kept. We ask ourselves that\ncan we move the communication among actors into critics so that actors can indepen-\ndently take actions according to their speciÔ¨Åc states during execution and the auxiliary\ncritics during training need no longer to be kept. In fact, it is possible for actor-critic\nmethods. However, both actor-only and critic-only methods are unsuitable for this task\nbecause the training and execution mechanisms of these methods are exactly the same.\nAs shown in Figure 2(c), a coordinator communication channel is used for coor-\ndinating the critics to generate better estimated Q-values, so we call this paradigm\nA-CCNet. SpeciÔ¨Åcally, the actor in A-CCNet is the same as the actor in the original\nactor-critic model shown in Figure 2(a), but the critics should communicate with each\nother through coordinator before they can generate the estimated Q-values. Compared\nto AC-CNet where communication occurs among actors and the communication signal\ncan only encode local state, A-CCNet put communication among critics where both\nstate and action can be encoded into the communication signal. So we expect that A-\nCCNet can generate better policies, which has been conÔ¨Årmed by the experiments.\nBesides, there are two designs for the critic. Critic1 uses the global signal to gen-\nerate Q-values directly, while critic2 combines global signal and local message to gen-\nerate Q-values. For both of the two designs, actors can generate their actions indepen-\ndently without communication during execution.\nFormal Formulation of ACCNet\nFor AC-CNet, as critics are independent, we can update each agent based on Equation\n(1-4) just like updating single actor-critic agent. One key difference is that we need to\npush the gradients of actors into the coordinator communication channels so that the\ncommunication protocols can also be optimized simultaneously.\nFor A-CCNet, as critics communicate with each other, the critic network of the i-th\nagent is now V i(si, sg; wi) or Qi(si, ai, sg; wi), where sg=f(s1, ..., sN, a1, ..., aN) is\nthe global communication signal2. We can then extend Equation (1-4) into multi-agent\n2Generally speaking, f is a injective function. Besides, using V i(si, sg; wi) and Qi(si, ai, sg; wi) for\ndiscrete and continuous action separately is natural. However, for discrete action, sg is a function of only the\nstates (s1, ..., sN); without knowing the actions (a1, ..., aN), Equation (11) can no longer be true.\n6\nformulations:\nŒ¥i\nt\n=\nrt + Œ≥V i(si\nt+1, sg\nt+1; wi) ‚àíV i(si\nt, sg\nt ; wi)\n(5)\nŒ∏i\nt+1\n=\nŒ∏i\nt + Œ± ‚àóŒ¥i\nt ‚àó‚ñΩŒ∏ilogœÄi(ai\nt|si\nt; Œ∏i)\n(6)\nyi\nt\n=\nrt + Œ≥Qi(si\nt+1, ai\nt+1, sg\nt+1; wi)\n(7)\nŒ¥i\nt\n=\nyi\nt ‚àíQi(si\nt, ai\nt, sg\nt ; wi)\n(8)\n‚ñΩŒ∏i\nt\n=\n‚ñΩaiQi(si\nt, ai\nt, sg\nt ; wi) ‚àó‚ñΩŒ∏iœÄi(ai\nt|si\nt; Œ∏i)\n(9)\nŒ∏i\nt+1\n=\nŒ∏i\nt + Œ± ‚àó‚ñΩŒ∏i\nt\n(10)\nOur primary insight about ACCNet (especially A-CCNet) is that once each agent\nknows the states and actions from other agents, the environment could be treated sta-\ntionary regardless of the changing policies. More formally, Equation (11) always keeps\ntrue for any agent indexed by i with any changing policies œÄi Ã∏= œÄ\n‚Ä≤i [33]:\nP(si\nt+1|si\nt; Env) =P(si\nt+1|si\nt; sg\nt , œÄ1, ..., œÄN)\n=P(si\nt+1|si\nt; sg\nt )\n=P(si\nt+1|si\nt; sg\nt , œÄ\n‚Ä≤1, ..., œÄ\n‚Ä≤N)\n(11)\nSome Comparisons\nBefore the comparison, we Ô¨Årst introduce the two concurrent studies mentioned in\nFootnote 1, i.e., COMA [34] from Oxford and MADDPG [33] from OpenAI.\nCOMA, MADDPG and A-CCNet share a similar idea: accelerating training with\nthe help of critics and executing in real environment only based on actors. But the re-\nsearch purposes are different. COMA aims at solving the credit assignment problem\nin multi-agent cooperative environments. MADDPG wants to investigate both cooper-\nation and competition among agents. The proposed ACCNet tries to provide a general\nframework to ease the learning of communication protocols among agents even from\nscratch. SpeciÔ¨Åcally, COMA is based on Stochastic Policy Gradient Theorem [13] and\nREINFORCE [35] algorithm. It uses a counterfactual baseline and a centralised critic\nto address multi-agent credit assignment problem. However, they only do experiments\nfor discrete action space environments and assume that the critic can get the entire\ngame screen. MADDPG extends DDPG [16, 18] into multi-agent environments. The\nauthors verify that this method is suitable for both cooperative and competitive tasks.\nHowever, their experiments are limited to continuous action space environments.\nAs COMA and MADDPG do not address the ‚Äúlearning-to-communicate‚Äù problem\nexplicitly, both of them use the states and actions of all other agents directly, without\nconsidering the communication cost. Nevertheless, we afÔ¨Årm each other‚Äôs methods and\nresults mutually.\nNow, we are ready to give a brief comparison as shown in Table 1. As we can see,\nACCNet has a better adaptability for different situations.\n7\nTable 1: Comparisons between ACCNet and related work. M1, M2, M3, M4 and M5\nstand for CommNet, DIAL, BiCNet, MADDPG and COMA separately.\nM1 M2 M3 M4 M5\nACCNet\nfully cooperative\nY\nY\nY\nY\nY\nY\ndiscrete action\nY\nY\nY\nN\nY\nY\ncontinuous action\nN\nN\nY\nY\nN\nY\nparti. observable\nY\nY\nN\nY\nN\nY\ndistri. agents\nN\nY\nN\nY\nN\nY\nlimited bandwith\nN\nY\nN\nN\nN\nY\nindep. execution\nN\nN\nN\nY\nY\nY\nM4 can also deal with competitive tasks very well.\nM5 can address the credit assignment problem very well.\nExperiments\nIn this section, we test the proposed ACCNet under both continuous and discrete ac-\ntion space environments. Those environments are partial observable with multiple dis-\ntributed and fully cooperative agents.\nContinuous Action Space Environment\nProblem DeÔ¨Ånition. For continuous action space environment, we focus on the Net-\nwork Routing Domain problem modiÔ¨Åed from [25]. Currently, the Internet is made up\nof many ISP networks. In each ISP network, as shown in Figure 3, there are several\nedge routers. Two edge routers are combined as ingress-egress router pair (IE-pair).\nThe i-th IE-pair has a input Ô¨Çow demand Fi and K available paths that can be used\nto deliver the Ô¨Çow from ingress-router to egress-router. Each path P k\ni is made up of\nseveral links and each link can belong to several paths. The l-th link Ll has a Ô¨Çow\ntransmission capacity Cl and a link utilization ratio Ul. As we know, high link utiliza-\ntion ratio is bad for dealing with burst trafÔ¨Åc, so we want to Ô¨Ånd a good trafÔ¨Åc splitting\npolicy jointly for all IE-pairs across their available paths to minimize the maximum\nlink utilization ratio in the network.\nI1\nI3\nE1\nE3\nI2\nE2\nA\nB\nC\nD\nF\nE\nL1\nL2\nL3\nL3\nL2\nL1\nFigure 3: TwoIE and ThreeIE topologies for network Ô¨Çow control studies [26, 27]. Link\nL1/L2/L3 are bottleneck links in both topologies.\n8\nI1\nI2\nI3\nI4\nI5\nE1\nE2\nE3\nE4\nE5\nL1\nL2\nL3\nL4\nL5\nL6\nL7\nL8\nL9\nFigure 4: FiveIE network topology for scalability test. Link L1‚àºL9 are bottleneck\nlinks.\nSetting. We design the following RL elements.\nState. Current trafÔ¨Åc demand and static network topology information are available.\nWe also encode the estimated link utilization ratio into the state. SpeciÔ¨Åcally, the local\nstate is s = [Fi, U l\ni, max(0, 1 ‚àíU l\ni), max(0, U l\ni ‚àí1)].\nAction. The ingress-router should generate a splitting ratio yk\ni with a constraint\nP\nk yk\ni = 1 for current trafÔ¨Åc demand Fi. So the softmax activation is chosen as the\nÔ¨Ånal layer of actor network. This design is natural for the continuous action with sum-\nto-one constraint.\nReward. As we want to minimize the maximum link utilization ratio, we set the\nreward signal to r = 1 ‚àímax(Ul).\nBaselines. As with CommNet and BiCNet, we also use the following baselines.\nIndependent controller (IND): each agent learns its own actor-critic network with-\nout any communication.\nFully-connected controller (FC): all agents are controlled by a big fully-connected\nactor-critic network to learn the trafÔ¨Åc splitting policy. The communication channel is\nembedded in the network without any bandwidth limitation.\nIND model is the worst situation and FC model can be seen as the ideal situation.\nBesides, as mentioned before, we design two kinds of critics for A-CCNet: all critics\nshare the same Q(s,a), or each critic separately learns its own Q(s,a). So we have the\nfollowing models: IND, FC-sep, FC-sha, AC-CNet, A-CCNet-sep and A-CCNet-sha.\nExperiment Results. In this environment, we care about convergence ratio (CR)\nof all independent experiments and maximum link utilization ratio (MLUi) of the i-th\nbottleneck link after convergence. All results are shown in Table 2 and 3. Due to space\nlimitation, we put the results of ThreeIE in the supplementary material.\nAs we can see, all models have high CR and low MLUi for simple TwoIE topology.\nBut A-CCNet has a better performance than AC-CNet and IND. It even has a similar\nperformance with the ideal fully observable FC model. For complex FiveIE topology,\nthe performances of AC-CNet and IND drop severely, while A-CCNet can still keep its\nability of performing almost like the ideal FC model. The reason may be that A-CCNet\nhas more global information than other models (except for FC model): A-CCNet put\ncommunication among critics where both local state and action can be encoded into the\ncommunication signal while the communication signal of AC-CNet can only encode\nlocal state and IND does not exchange information at all. In this case, more information\nmeans that the environment could be seen stationary as illustrated by Equation (11).\nCommunication Message Analysis. We show the state-message-action changing\nof one convergent experiment in Figure 5. As the value of state become large (for\n9\nTable 2: The CR and MLUi of TwoIE topology. Results are averaged over 30 indepen-\ndent experiments.\nCR\nMLU1\nMLU2\nMLU3\nIND\n0.655\n0.713\n0.724\n0.716\nFC-sep\n0.967\n0.707\n0.704\n0.709\nFC-sha\n0.967\n0.710\n0.702\n0.715\nAC-CNet\n0.433\n0.712\n0.713\n0.733\nA-CCNet-sep\n0.9\n0.708\n0.698\n0.714\nA-CCNet-sha\n0.9\n0.734\n0.707\n0.718\nTable 3: The CR and MLUi of FiveIE topology. Results are averaged over 30 indepen-\ndent experiments.\nCR\nMLU2 MLU4 MLU6 MLU8\nIND\n0.1\n0.817\n0.879\n0.891\n0.828\nFC-sep\n0.8\n0.818\n0.8\n0.797\n0.822\nFC-sha\n0.767\n0.817\n0.767\n0.836\n0.835\nAC-CNet\n0.0\n-\n-\n-\n-\nA-CCNet-sep 0.567\n0.751\n0.809\n0.800\n0.799\nA-CCNet-sha 0.467\n0.799\n0.810\n0.810\n0.805\nexample, more packets should be transmitted), agent1 will emit large message value\nwhile agent2 usually emits small message value. For action value, if agent1 splits more\ntrafÔ¨Åc to L1, agent2 will split more trafÔ¨Åc to L2 because L2 is now underused. Besides,\nagent1 has a wider range of state value, so the message value and action value generated\nby agent1 are also wider than agent2. Those sophisticated and coordinated behaviors\nare critical for MARL systems to stay organized.\n0.2\n0.4\n0.6\n0.8\n1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\nagent1\nagent2\nThe first\ncomponent\nThe sencond\ncomponent\nFigure 5: The state-message-action changing of one convergent experiment on topol-\nogy TwoIE with model AC-CNet. Only the 2D PCA projections of the original data are\nshown as done in CommNet.\n10\nDiscrete Action Space Environment\nProblem DeÔ¨Ånition. We consider the TrafÔ¨Åc Junction problem modiÔ¨Åed from [28, 23].\nAs shown in Figure 6, four cars are deriving on the 4-way junction road. New car will\nbe generated if one car reaches its destination at the edge of the grid. The simulation\nwill be classiÔ¨Åed as a failure if location overlaps have occurred in 40 timesteps. Our\ntarget is to learn a car driving policy so that we can get low failure rate (FR).\nNew car\narrivals\n3 possible\nroutes\nCar exiting\nFigure 6: The environment of trafÔ¨Åc junction task.\nSetting. We use the same RL elements as in CommNet.\nState. All cars can only know its location and driving direction. They cannot see\nother cars. So we represent the local state as a one-hot vector set {location, direction}.\nAction. A car has two possible actions: gassing itself by one cell on its route or\nbraking to stay at its current location.\nReward. A collision incurs a reward rcoll=-10.0, and each car gets reward of rœÑ\ntime=-\n0.01œÑ at each timestep to discourage a trafÔ¨Åc jam, where œÑ is the total timesteps since\nthe car arrived. So the total reward at time t is: r(t) = Ctrcoll +PNt\ni=1 rœÑi\ntime, where Ct\nis the number of location overlaps at time t, and N t is the number of cars. This setting\nis the same as CommNet.\nExperiment Results. Table 4 shows the results of this task. After training the mod-\nels 300 episodes as CommNet, the proposed A-CCNet can get lower FR than CommNet\nand other baselines. When the training episode increases to 600, A-CCNet can further\nget a lower FR and a higher CR, while other models cannot get the same results.\nCommunication Message Analysis. We Ô¨Ånd a special car driving policy where\nthe left car0 and the right car2 always brake to make space for the above car1 and the\nbelow car3. We illustrate the emitted messages by different cars under this policy in\nFigure 7. As we can see, messages for braking and gassing are naturally separated. For\nthe same type (no matter braking or gassing) of messages, they can also be separated\nby different cars so that the ACCNet can distinguish them. Besides, gassing message\nis more diverse than braking message. The reason may be that braking positions are a\nfew (near the junction) while each position of the grid road needs a different gassing\nmessage.\n11\nTable 4: Averaged results of 30000 experiments on trafÔ¨Åc junction task. The results\nof CommNet and Discrete-CN are directly cited from [23]. Please note that the two\nenvironments have some nuances.\n300 episodes\n600 episodes\nCR\nFR (%)\nCR\nFR (%)\nIND\n0.57\n11.18\n0.6\n18.35\nFC-sep\n0.8\n12.76\n0.8\n11.95\nFC-sha\n0.73\n12.69\n0.76\n10.05\nAC-CNet\n0.47\n12.04\n0.73\n14.25\nA-CCNet-sep\n0.6\n10.66\n0.73\n10.48\nA-CCNet-sha\n0.53\n7.88\n0.73\n4.96\nCommNet (CN)\n-\n10.0\n-\n-\nDiscrete-CN\n-\n100.0\n-\n-\nMessage for\nBraking car0\nMessage for\ngassing car3\nMessage for\ngassing car1\nThe first\ncomponent\nThe second\ncomponent\nMessage for\nBraking car2\nFigure 7: The emitted messages by different cars of one special learned car deriving\npolicy. Only the 2D PCA projections of the original messages are shown.\nDesign Discussion of ACCNet\nAs we know, some design choices are very important for the success of DRL in real-\nworld applications. For example, experience replay, frame skipping, target network,\nreward clipping, asynchronous training, auxiliary task and even methods of DL such\nas batch normalization, attention mechanism and skip connection are widely adopted\n[24, 29, 30, 31, 18, 32, 23]. In this section, we brieÔ¨Çy present a few design choices\nused by ACCNet, hoping that other researchers can conÔ¨Årm their usefulness for new\nenvironments. Note that all those design choices need to be further studied.\n(1) The embedded communication channel. We suggest to use deep neural net-\nworks to encode the communication message so that the Ô¨Ånal message dimension is\ncontrolled to be independent of the dimension of the original information. And most\nimportantly, as the communication channel is embedded in deep neural networks, the\ncommunication protocols can be learned even from scratch in an end-to-end differen-\ntiable way while the network is optimized.\n12\n(2) The concurrent experience replay (CER). Experience replay is beneÔ¨Åcial for\nsingle-agent RL. Except for making the collection process of training data more efÔ¨Å-\ncient, it can also break the correlation among sequential training data to accelerate the\nconvergence of models. However, as [4] point out, it is necessary to disable experience\nreplay for MARL due to the non-concurrent property of local experiences when sam-\npled independently for each agent. We propose CER to address this problem. Generally\nspeaking, CER samples the concurrent experiences collected at the same timestep for\nall agents as they are trained concurrently. In fact, [40] use the same replay method and\nname it CER. We refer the readers to this paper for further details.\n(3) The current episode experience replay (CEER). Traditional experience replay\nmethods uniformly sample a batch of experiences from replay buffer as training ex-\namples for model updating. [29] introduce prioritized experience replay based on the\nmagnitude of TD-error to accelerate learning. The proposed CEER can be seen as a\ntime-prioritized replay method. CEER keeps all experiences of current episode in a\ntemporary buffer and combines them with experiences from the main replay buffer\nas training examples at the end of each episode. Our preliminary experiments show\nthe effectiveness of this method. Detailed analyses can be found in the supplementary\nmaterial.\n(4) Disabled experience replay for discrete action space environments. We Ô¨Ånd that\nthe training of discrete action space environments is non-stable, no matter which replay\nmethod is used. Footnote 2 and [4, 41] explain this phenomenon in some extent, but\nfurther research is needed.\n(5) Full-information activation function for sensitive continuous action. Ideally (for\ncontinuous action environments), the policy œÄ(a|s) is a one-to-one function mapping\nbetween state s and optimal action a‚àó. If we use a neural network œÄ(a|s; Œ∏) to approx-\nimate œÄ(a|s) to meet the one-to-one mapping requirement, we should not throw away\nany (useful) information in state s at any layer of the network œÄ(a|s; Œ∏). Otherwise,\nsimilar states may be encoded into identical hidden vector and further be mapped to\nthe same optimal action a‚àó. So we suggest to use sigmoid, elu, etc. rather than relu as\nactivation functions for sensitive continuous action space applications. Similarly, we\nsuggest to use relu for discrete action space applications because action is Ô¨Ånite and\nsimilar states often correspond to the same optimal action a‚àóin those applications. Our\npreliminary experiments show that relu based models will generate an averagely op-\ntimal action but not the exactly optimal action. Further analyses can be found in the\nsupplementary material.\n(6) Centralized coordinator. Is there any single point failure? As ACCNet is fully\ndistributed, any agent or special designed agents can act as the coordinator. In addition,\nthe A-CCNet does not need the coordinator during execution, and centralized training\nis a common setting for MARL systems [4, 34, 33].\nConclusion\nThe proposed ACCNet, born with the combined abilities of deep models and actor-\ncritic reinforce models, is a general framework to learn communication protocols from\nscratch for fully cooperative, partially observable MARL problems, no matter the ac-\n13\ntion space is continuous or discrete. Specially, the A-CCNet, one concrete implementa-\ntion of ACCNet, can make the training of MARL systems more stationary than previous\nmethods as supported by both mathematical Equation (11) and experimental results of\nvarious environments. Another attractive advantage of A-CCNet is that it does not need\ncommunication during execution while still keeps a good generalization ability.\nFor the future work, we will put our efforts on the following important and chal-\nlenging problems. (1) How to make the training of discrete action space MARL sys-\ntems more stationary. Special experience replay method may be a powerful tool for\nthis problem. (2) How to make the communication signals more sparse. In this paper,\nwe use deep neural networks to compress the original communication messages. This\nmethod can achieve ‚Äúspatial-sparsity‚Äù, i.e., the dimension of communication signals is\nlimited and most values are around zero. Another one is ‚Äútime-sparsity‚Äù, i.e., the com-\nmunication signals only come intermittently. Although those concepts are borrowed\nfrom sparse autoencoder [42, 43, 44], they are very useful in the real-world distributed\nMARL systems. We Ô¨Ånd that gating mechanism and token mechanism are very use-\nful for achieving ‚Äútime-sparsity‚Äù. We will introduce our methods more formally in the\nfuture.\nAcknowledgments\nThe authors would like to thank Xiangyu Liu, Weichen Ke, Chao Ma, Quanbin Wang,\nYiping Song and the anonymous reviewers for their insightful comments. This work\nwas supported by the National Natural Science Foundation of China under Grant No.61572044.\nThe contact author is Zhen Xiao.\nReferences\n[1] Vicisano L, Crowcroft J, Rizzo L. TCP-like congestion control for layered\nmulticast data transfer[C]//INFOCOM‚Äô98. Seventeenth Annual Joint Conference\nof the IEEE Computer and Communications Societies. Proceedings. IEEE. IEEE,\n1998, 3: 996-1003.\n[2] Wan C Y, Eisenman S B, Campbell A T. CODA: congestion detection and\navoidance in sensor networks[C]//Proceedings of the 1st international conference\non Embedded networked sensor systems. ACM, 2003: 266-279.\n[3] Agogino A K, Tumer K. A multiagent approach to managing air trafÔ¨Åc Ô¨Çow[J].\nAutonomous Agents and Multi-Agent Systems, 2012, 24(1): 1-25.\n[4] Foerster J, Assael Y M, de Freitas N, et al. Learning to communicate with\ndeep multi-agent reinforcement learning[C]//Advances in Neural Information\nProcessing Systems. 2016: 2137-2145.\n14\n[5] Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep\nneural networks and tree search[J]. Nature, 2016, 529(7587): 484-489.\n[6] Tampuu A, Matiisen T, Kodelja D, et al. Multiagent cooperation and competition\nwith deep reinforcement learning[J]. PloS one, 2017, 12(4): e0172395.\n[7] Peng P, Yuan Q, Wen Y, et al. Multiagent Bidirectionally-Coordinated Nets for\nLearning to Play StarCraft Combat Games[J]. arXiv preprint arXiv:1703.10069,\n2017.\n[8] Goldman C V, Zilberstein S. Optimizing information exchange in cooperative\nmulti-agent systems[C]// 2003:137-144.\n[9] Goldman C V, Zilberstein S. Decentralized control of cooperative systems:\ncategorization and complexity analysis[M]. AI Access Foundation, 2004.\n[10] Oliehoek F A. Decentralized POMDPs[M]// Reinforcement Learning. Springer\nBerlin Heidelberg, 2012:471-503.\n[11] Oliehoek F A, Amato C. A concise introduction to decentralized POMDPs[M].\nSpringer International Publishing, 2016.\n[12] Chen Y F, Liu M, Everett M, et al. Decentralized Non-communicating Multia-\ngent Collision Avoidance with Deep Reinforcement Learning[J]. arXiv preprint\narXiv:1609.07845, 2016.\n[13] Sutton R S, Barto A G. Introduction to reinforcement learning[M]. Cambridge:\nMIT Press, 1998.\n[14] Konda V R, Tsitsiklis J N. Onactor-critic algorithms[J]. SIAM journal on Control\nand Optimization, 2003, 42(4): 1143-1166.\n[15] Grondman I, Busoniu L, Lopes G A D, et al. A survey of actor-critic reinforce-\nment learning: Standard and natural policy gradients[J]. IEEE Transactions on\nSystems, Man, and Cybernetics, Part C (Applications and Reviews), 2012, 42(6):\n1291-1307.\n[16] Lever G. Deterministic policy gradient algorithms[J]. 2014.\n15\n[17] Grondman I, Busoniu L, Lopes G A D, et al. A survey of actor-critic reinforce-\nment learning: Standard and natural policy gradients[J]. IEEE Transactions on\nSystems, Man, and Cybernetics, Part C (Applications and Reviews), 2012, 42(6):\n1291-1307.\n[18] Lillicrap T P, Hunt J J, Pritzel A, et al. Continuous control with deep reinforce-\nment learning[J]. arXiv preprint arXiv:1509.02971, 2015.\n[19] Tan M. Multi-agent reinforcement learning: Independent vs. cooperative\nagents[C]//Proceedings of the tenth international conference on machine learn-\ning. 1993: 330-337.\n[20] Coordinating multi-agent reinforcement learning with limited communication\n[21] Learning of communication codes in multi-agent reinforcement learning problem\n[22] Giles\nC\nL,\nJim\nK\nC.\nLearning\ncommunication\nfor\nmulti-agent\nsys-\ntems[C]//Workshop on Radical Agent Concepts. Springer Berlin Heidelberg,\n2002: 377-390.\n[23] Sukhbaatar S, Fergus R. Learning multiagent communication with backpropaga-\ntion[C]//Advances in Neural Information Processing Systems. 2016: 2244-2252.\n[24] Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep\nreinforcement learning[J]. Nature, 2015, 518(7540): 529-533.\n[25] Analysing Congestion Problems in Multi-agent Reinforcement Learning\n[26] Elwalid A, Jin C, Low S, et al. MATE: MPLS adaptive trafÔ¨Åc engineer-\ning[C]//INFOCOM 2001. Twentieth Annual Joint Conference of the IEEE\nComputer and Communications Societies. Proceedings. IEEE. IEEE, 2001, 3:\n1300-1309.\n[27] Kandula S, Katabi D, Davie B, et al. Walking the tightrope: Responsive yet stable\ntrafÔ¨Åc engineering[C]//ACM SIGCOMM Computer Communication Review.\nACM, 2005, 35(4): 253-264.\n16\n[28] Sukhbaatar S, Szlam A, Synnaeve G, et al. Mazebase: A sandbox for learning\nfrom games[J]. arXiv preprint arXiv:1511.07401, 2015.\n[29] Schaul T, Quan J, Antonoglou I, et al. Prioritized experience replay[J]. arXiv\npreprint arXiv:1511.05952, 2015.\n[30] Mnih V, Badia A P, Mirza M, et al. Asynchronous methods for deep rein-\nforcement learning[C]//International Conference on Machine Learning. 2016:\n1928-1937.\n[31] Jaderberg M, Mnih V, Czarnecki W M, et al. Reinforcement learning with\nunsupervised auxiliary tasks[J]. arXiv preprint arXiv:1611.05397, 2016.\n[32] Sorokin I, Seleznev A, Pavlov M, et al. Deep attention recurrent Q-network[J].\narXiv preprint arXiv:1512.01693, 2015.\n[33] Lowe, Ryan, et al. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive\nEnvironments. arXiv preprint arXiv:1706.02275 (2017).\n[34] Foerster, Jakob, et al. Counterfactual Multi-Agent Policy Gradients. arXiv\npreprint arXiv:1705.08926 (2017).\n[35] Williams, Ronald J. Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine learning 8.3-4 (1992): 229-256.\n[36] Konda, Vijay R., and John N. Tsitsiklis. Actor-critic algorithms. Advances in\nneural information processing systems. 2000.\n[37] Mordatch, Igor, and Pieter Abbeel. Emergence of Grounded Compositional\nLanguage in Multi-Agent Populations. arXiv preprint arXiv:1703.04908 (2017).\n[38] Das, Abhishek, et al. Learning Cooperative Visual Dialog Agents with Deep\nReinforcement Learning. arXiv preprint arXiv:1703.06585 (2017).\n[39] Havrylov, Serhii, and Ivan Titov. Emergence of Language with Multi-agent\nGames: Learning to Communicate with Sequences of Symbols. arXiv preprint\narXiv:1705.11192 (2017).\n17\n[40] OmidshaÔ¨Åei, Shayegan, et al. Deep decentralized multi-task multi-agent re-\ninforcement learning under partial observability. International Conference on\nMachine Learning. 2017.\n[41] Foerster, Jakob, et al. Stabilising experience replay for deep multi-agent rein-\nforcement learning. arXiv preprint arXiv:1702.08887 (2017).\n[42] Lee, Honglak, Chaitanya Ekanadham, and Andrew Y. Ng. Sparse deep belief net\nmodel for visual area V2. Advances in neural information processing systems.\n2008.\n[43] Makhzani, Alireza, and Brendan Frey. K-sparse autoencoders. arXiv preprint\narXiv:1312.5663 (2013).\n[44] Makhzani, Alireza, and Brendan J. Frey. Winner-take-all autoencoders. Advances\nin Neural Information Processing Systems. 2015.\n[45] Dobbe R, Fridovich-Keil D, Tomlin C. Fully Decentralized Policies for\nMulti-Agent Systems: An Information Theoretic Approach[J]. arXiv preprint\narXiv:1707.06334, 2017.\n18\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2017-06-10",
  "updated": "2017-10-29"
}