{
  "id": "http://arxiv.org/abs/1706.05749v1",
  "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement Learning",
  "authors": [
    "Nick Erickson",
    "Qi Zhao"
  ],
  "abstract": "This paper introduces Dex, a reinforcement learning environment toolkit\nspecialized for training and evaluation of continual learning methods as well\nas general reinforcement learning problems. We also present the novel continual\nlearning method of incremental learning, where a challenging environment is\nsolved using optimal weight initialization learned from first solving a similar\neasier environment. We show that incremental learning can produce vastly\nsuperior results than standard methods by providing a strong baseline method\nacross ten Dex environments. We finally develop a saliency method for\nqualitative analysis of reinforcement learning, which shows the impact\nincremental learning has on network attention.",
  "text": "Dex: Incremental Learning for Complex\nEnvironments in Deep Reinforcement Learning\nNick Erickson, Qi Zhao\nDepartment of Science and Engineering\nUniversity of Minnesota, Twin Cities\nMinneapolis, MN 55455\neric3068@umn.edu, qzhao@cs.umn.edu\nAbstract\nThis paper introduces Dex, a reinforcement learning environment toolkit special-\nized for training and evaluation of continual learning methods as well as general\nreinforcement learning problems. We also present the novel continual learning\nmethod of incremental learning, where a challenging environment is solved using\noptimal weight initialization learned from ﬁrst solving a similar easier environment.\nWe show that incremental learning can produce vastly superior results than standard\nmethods by providing a strong baseline method across ten Dex environments. We\nﬁnally develop a saliency method for qualitative analysis of reinforcement learning,\nwhich shows the impact incremental learning has on network attention.\n1\nIntroduction\nComplex environments such as Go, Starcraft, and many modern video-games present profound\nchallenges in deep reinforcement learning that have yet to be solved. They often require long, precise\nsequences of actions and domain knowledge in order to obtain reward, and have yet to be learned from\nrandom weight initialization. Solutions to these problems would mark a signiﬁcant breakthrough on\nthe path to artiﬁcial general intelligence.\nRecent works in reinforcement learning have shown that environments such as Atari games [2]\ncan be learned from pixel input to superhuman expertise [9]. The agents start with randomly\ninitialized weights, and learn largely from trial and error, relying on a reward signal to indicate\nperformance. Despite these successes, complex games, including those where rewards are sparse such\nas Montezuma’s Revenge, have been notoriously difﬁcult to learn. While methods such as intrinsic\nmotivation [3] have been used to partially overcome these challenges, we suspect this becomes\nintractable as complexity increases. Additionally, as environments become more complex, they will\nbecome more expensive to simulate. This poses a signiﬁcant problem, since many Atari games\nalready require upwards of 100 million steps using state-of-the-art algorithms, representing days of\ntraining on a single machine.\nThus, it appears likely that complex environments will become too costly to learn from randomly\ninitialized weights, due both to the increased simulation cost as well as the inherent difﬁculty of the\ntask. Therefore, some form of prior information must be given to the agent. This can be seen with\nAlphaGo [18], where the agent never learned to play the game without ﬁrst using supervised learning\non human games. While supervised learning certainly has been shown to aid reinforcement learning,\nit is very costly to obtain sufﬁcient samples and requires the environment to be a task humans can\nplay with reasonable skill, and is therefore impractical for a wide variety of important reinforcement\nlearning problems.\nIn this paper we introduce Dex, the ﬁrst continual reinforcement learning toolkit for training and\nevaluating continual learning methods. We present and demonstrate a novel continual learning method\narXiv:1706.05749v1  [stat.ML]  19 Jun 2017\nwe call incremental learning to solve complex environments. In incremental learning, environments\nare framed as a task to be learned by an agent. This task can be split into a series of subtasks that are\nsolved simultaneously.\nSimilar to how natural language processing and object detection are subtasks of neural image\ncaption generation [23], reinforcement learning environments also have subtasks relevant to a given\nenvironment. These subtasks often include player detection, player control, obstacle detection,\nenemy detection, and player-object interaction, to name a few. These subtasks are common to\nmany environments, but they are often sufﬁciently different in function and representation that\nreinforcement learning algorithms fail to generalize them across environments, such as in Atari.\nThese critical subtasks are what expert humans utilize to quickly learn in new environments that share\nsubtasks with previously learned environments, and are a reason for humans superior data efﬁciency\nin learning complex tasks.\nIn the case of deliberately similar environments, we can construct the subtasks such that they are\nsimilar in function and representation that an agent trained on the ﬁrst environment can accelerate\nlearning on the second environment due to its preconstructed subtask representations, thus partially\navoiding the more complex environment’s increased simulation cost and inherent learning difﬁculty.\n2\nRelated work\nTransfer learning [13] is the method of utilizing data from one domain to enhance learning of another\ndomain. While sharing signiﬁcant similarities to continual learning, transfer learning is applicable\nacross all machine learning domains, rather than being conﬁned to reinforcement learning. For\nexample, it has had signiﬁcant use with using networks trained on ImageNet [5] to accelerate or\nenhance learning and classiﬁcation accuracy by ﬁnetuning in a variety of vision tasks [12].\nWhile the concept of continual learning, where an agent learns from a variety of experiences to\nenhance future learning, has been deﬁned for some time [15], it has remained largely untapped by\nrecent powerful algorithms that are best ﬁt to beneﬁt from its effects. Recent work has been done\nwith Progressive Neural Networks [17], where transfer learning was used to apply positive transfer\nin a variety of reinforcement learning domains. However, our method differs in that it does not\nadd additional parameters for each environment or use lateral connections to features that result in\nincreased memory space and training time.\nRecent work related to subtask utilization comes from Kirkpatrick et al. [7], which shows that\nexpertise can be maintained on multiple environments that have not been experienced for a long time\nthrough elastic weight consolidation (EWC). Viable weights were found that simultaneously achieve\nexpertise in a variety of Atari games. Incremental learning similarly trains on multiple environments,\nbut with the goal of achieving enhanced expertise in a single environment, rather than expertise in all\nenvironments. We leave it to future work to overcome this limitation.\n3\nDex\nDex is a novel deep reinforcement learning toolkit for training and evaluating agents on continual\nlearning methods. Dex acts as a wrapper to the game Open Hexagon [16], sending screen pixel\ninformation, reward information and performing actions via an OpenAI Gym like API [4]. Dex\ncontains hundreds of levels, each acting as their own environment. These environments are collected\ninto groups of similar environments for the task of continual learning. Dex environments vary greatly\nin difﬁculty, ranging from very simple levels where agents achieve superhuman performance in less\nthat 4 minutes, to levels we consider far more complex than any previously learned environments.\nRefer to videos available at github.com/innixma/dex of the Dex environments shown in Figure 1,\nas screenshots do not capture the environment complexity.\nOpen Hexagon is a game involving navigating a triangle around a center ﬁgure to avoid incoming\nrandomly generated walls. Screenshots of various environments from the game are shown in Figure\n1. The game progresses regardless of player action, and thus the player must react to the environment\nin real-time. If the player contacts a wall the game is over. At each point in the game, a player has\nonly three choices for actions: move left, right, or stay put. It is a game of survival, with the score\nand thus total reward being the survival time in seconds. Open Hexagon contains hundreds of levels\n2\ndrastically ranging in difﬁculty, yet they each contain many similar core subtasks. This makes Open\nHexagon an ideal platform for testing continual learning methods.\n(a) Distortion\n(b) Lanes\n(c) Reversal\n(d) System\n(e) Desync\n(f) Stutter\n(g) Arithmetic\n(h) Stretch\n(i) Open\n(j) Overcharge\n(k) Enneagon\n(l) Blink\n(m) Tunnel Town\n(n) Roulette\n(o) Echnoderm\n(p) Transmission\n(q) Triptych\n(r) Duality\n(s) World Battle\n(t) Universe\n(u) Apeirogon\n(v) Euclidean\n(w) Pi\n(x) Golden Ratio\nFigure 1: Dex environments. The small triangle is the player that must be rotated around the center to\navoid incoming walls. Many of these environments incorporate various distortion effects that are not\nevident in screenshots. Reversal periodically ﬂips game controls, and some environments even add\nadditional actions, such as in Arithmetic, which requires the agent to correctly solve various math\nequations during the level through the numpad.\nThe Dex toolkit along with its source code is available at github.com/innixma/dex.\n4\nIncremental learning\nThe novel continual learning method of incremental learning is deﬁned as follows.\nIn the formal case, an agent must learn from a series of n environments E, each with identical legal\ngame actions A = {1, ..., K}. Note that any series of environments can be made to have the same\naction space by considering A to be the superset of all possible actions in each game, with new\nactions performing identically to no action, assuming no action is within the action space.\nEach environment Ei has a corresponding cost per step ci > 0 and a step count si ≥0, indicating the\nnumber of steps taken in that environment. Typically, more complex environments will have a higher\ncost per step. A resource maximum M is given, which indicates the total amount of data that can be\ngathered, shown in the following inequality:\nM ≥\nn\nX\ni=1\ncisi\nThe problem is to maximize the mean total reward R of the agent in the goal environment, En, while\nmaintaining the above inequality. Steps can be taken in any order from the n environments, and there\n3\nis no assumed correlation between the environments beyond their data and action dimensions. While\nit may appear an optimal solution to only examine and gather data from En, as has been done for\nvirtually all reinforcement learning algorithms in the past, this is not always the case. For example, if\nEn−1 and En are highly correlated, and cn−1 << cn, then training with En−1 may be superior due\nto its lesser cost. Additionally, an environment En−1 may contain important aspects of En, while\navoiding state spaces and rewards that are not useful for training, potentially allowing training on\nEn−1 to be optimal, even if cn−1 > cn.\nBy taking environments E to represent all real environments with their respective costs, the solution\nto this problem corresponds to the globally optimal sequence of training steps to achieve maximum\nperformance with a ﬁnite amount of computational resources for a given algorithm. It therefore\nnecessarily contains the solution of achieving artiﬁcial general intelligence with minimal resources.\nUnfortunately, this has several drawbacks. Most importantly, the selection of the optimal environments\nis not obvious, and their order even less so.\nWhile this formal deﬁnition is useful to deﬁne incremental learning, the following simpliﬁed version\nwill be what this paper focuses on. In the simpliﬁed case, all variables are the same, except that steps\nmust be taken in environments sequentially, without going back to previous environments. Thus,\nonce a step has been taken in Ei, no steps may be taken in future environments Ej where j < i.\nFurthermore, it is assumed that the environments are correlated, where environment Ei−1 contains a\nsubset of subtasks in environment Ei, with environment Ei being typically harder than environment\nEi−1.\nThe intuition behind this simpliﬁed case is that simple environments are both cheap to simulate\nand easy to learn, and that the features and strategies learned from the simple environment could\ntransfer to a more difﬁcult correlated environment. This process can be done repeatedly, producing\na compounding acceleration of learning as additional useful incremental environments are added.\nThus, the general process of incremental environment selection is to use easier subsets of the goal\nenvironment. Furthermore, this means that every environment Ei in a simpliﬁed incremental learning\nproblem can be seen as the goal environment in an easier problem containing E1:i.\n5\nBaseline learning algorithm and model architecture\nTo analyse the effectiveness of incremental learning, our agents learned environments from Dex. For\ntraining agents, we use Asynchronous Advantage Actor-Critic (A3C) framework introduced in [10],\ncoupled with the network architecture described below.\nConvNet implementation details. All Dex experiments were learned by a network with the follow-\ning architecture. The network takes as input a series of 2 consecutive 42 × 42 grayscale images\nof a game state. It is then processed through a series of 3 convolutional layers, of stride 1 × 1 and\nsize 5 × 5, 4 × 4, and 3 × 3 with ﬁlter counts of 32, 64, and 64 respectively. Between each pair of\nconvolutional layers is a 2 dimensional maxpooling layer, of size 2 × 2. The intermediate output is\nthen ﬂattened and processed by a dense layer of 512 nodes. The output layers are identical to those\nspeciﬁed for A3C. All convolutional layers and maxpooling layers are zero-padded. This results\nin an network with approximately 4,000,000 parameters. All intermediate layers are followed by\nrectiﬁed linear units (ReLU) [11], and Adam is used for the optimizer, with a learning rate of 10−3.\nLearning rate is not changed over the course of training as opposed to Mnih et al. [10], but instead\nstays constant. A gamma of 0.99 is used, along with n-step reward [14] with n = 4 , which is used to\nspeed up value propagation in the network, at a cost of minor instability in the learning. Training\nis done with batches of 128 samples. The architecture was created using TensorFlow 1.1.0 [1], and\nKeras 2.0.3.\nSince Dex runs in real time, a slightly altered method of learning was utilized to avoid inconsistent time\nbetween steps. Our implemented A3C was modiﬁed to work in an ofﬂine manner, with experience\nreplay as done in Deep-Q Networks [8]. This is a naive approximation to the more complex ACER\nalgorithm [22]. While this does destabilize the algorithm, which bases its computations on data being\nrepresentative of the network in its current state, the agents are still able to learn the environments\nand signiﬁcantly outperform Double Deep-Q Networks [20], thus serving as a reasonable baseline.\n4\n6\nExperiments\nSo far, we have performed experiments on ten different environments in Dex for incremental learning.\nThese ten environments are split into two incremental learning sets of three and seven environments.\nThe ﬁrst set will be referred to as a, and deals with increasingly complex patterns. The second set will\nbe referred to as b, and deals with increasingly complex task representation. The results show that\nincremental learning can have a signiﬁcant positive impact on learning speed and task performance,\nbut can also lead to bad initializations when trained on overly simplistic environments.\nCode to reproduce the experiments in this paper will be released at a future date.\n6.1\nSetup\nIn the following experiments, we naively assume that achieving near optimal performance in Ei with\nminimal resources requires as a prerequisite learning Ei−1 with minimal resources. This proceeds\ndownward to the base case of E1, which can be considered a trivially solvable environment from\nrandom weight initialization. This assumption is used to simplify training. Furthermore, due to the\nexploratory nature of the baseline experiments, the costs per step are ignored, as we seek only to show\nthat positive feature transfer is occurring, rather than optimizing the feature transfer itself, which we\nleave for future work.\nAll environments are learned with identical architecture, algorithm and hyperparameters. We act and\ngather data on the environments 50 times per second. We use an ϵ-greedy policy with ϵ = 0.05, and a\nreplay memory size of 40,000. Replay memory is initialized by a random agent. For all experiments,\nagents are trained with 75 batches after each episode. Total reward is equal to the number of seconds\nsurvived in the environment each episode. Mean reward is calculated from an hour of testing weights\nwithout training. Each episode scales in difﬁculty indeﬁnitely the longer the agent survives.\n6.2\nModels\nWe evaluate four different types of models in this experiment. The ﬁrst is a random agent for\ncomparison. The second is the baseline, which is the standard reinforcement learning training method.\nTo establish the baseline, each environment Ei is trained on from random initialization for one hour,\nequivalent to roughly 150,000 training steps. The weights which achieve the maximum total reward\nin a single episode are selected as the output weights from the training, called wi.\nA third model, which we shall call initial, is the initial weights to the incremental learning method\nbefore continued training. Thus, for environment Ei this model uses weights wi−1. This is used to\nmeasure the correlation between the two environments. We would expect uncorrelated environments\nto result in near random agent reward for initial.\nTo establish the incremental learning agents, for each environment Ei we take the weights wi−1\noutputted by the baseline, using them as the initial weights to training on Ei for an additional hour.\nThe weights outputted by this method we call w′\ni.\n6.3\nResults and Observations\nThe results can be found in Table 1 and Table 2.\nAs can be seen in Table 1, incremental learning provided superior or roughly equivalent results\non every environment in set b, with some environments such as b3 and b4 experiencing substantial\nincreases in maximum reward, with the incrementally learned b4 achieving nearly triple the baseline\nin both maximum and mean reward. However, on the harder environments, there was not signiﬁcant\nimprovement seen. This is likely because the earlier environments were not sufﬁciently learned along\nwith the fact that the tasks were generally too difﬁcult to learn in one hour of training, indicated by\nthe near random performance of b6 and b7 on both the baseline and incremental methods.\nThe other set, results shown in Table 2 for set a, show that incremental learning was harmful in the\ncase of a2. This is likely due to the difference in the wall patterns of a1 and a2. In a1, a single wall\nis on the screen at a time, requiring a simple avoidance. In a2, up to four separate walls occupy\nthe screen at once, requiring a more complex method involving future planning and understanding\nof which walls are more important in a given state. We suspect that learning a1 leads the agent to\n5\nTable 1: Set b rewards\nMax\nE\nRandom\nInitial\nBaseline\nIncremental\nb1\n31.43\n—\n425.92\n—\nb2\n8.23\n120.77\n259.80\n280.59\nb3\n7.73\n58.30\n132.31\n221.96\nb4\n8.27\n35.10\n35.66\n105.48\nb5\n8.79\n19.76\n52.81\n41.57\nb6\n9.33\n11.05\n10.79\n13.11\nb7\n9.97\n7.13\n9.59\n14.54\nMean\nE\nRandom\nInitial\nBaseline\nIncremental\nb1\n9.10\n—\n169.54\n—\nb2\n2.23\n36.05\n92.24\n85.52\nb3\n2.43\n8.22\n41.17\n66.32\nb4\n2.17\n5.30\n8.11\n26.75\nb5\n1.88\n4.17\n12.23\n11.61\nb6\n2.17\n2.89\n2.15\n2.25\nb7\n2.30\n2.34\n2.08\n2.58\nThis table shows the max and mean reward for an agent on the given environment E with a given\ntraining method, as described in the experimental setup. Here we observe that incremental learning\nprovided superior results to the baseline in nearly all environments, particularly b3 and b4.\nTable 2: Set a rewards\nMax\nE\nRandom\nInitial\nBaseline\nIncremental\na1\n40.73\n—\n771.67\n—\na2\n19.65\n53.37\n445.85\n86.57\na3\n10.85\n15.69\n49.50\n59.10\nMean\nE\nRandom\nInitial\nBaseline\nIncremental\na1\n8.93\n—\n717.93\n—\na2\n7.46\n18.34\n87.06\n18.31\na3\n6.01\n7.32\n9.81\n13.52\nThis table shows the max and mean reward for an agent on the given environment E with a given\ntraining method, as described in the experimental setup. Here we observe that incremental learning\nprovided inferior results to the baseline in the a2 environment, likely due to overﬁtting.\n6\novertrain on a variety of weights, hindering future learning. This indicates that certain environments\nmay be too simple to include in incremental learning, as a1 can be learned to a reward of over 700 in\nless than four minutes.\nAdditionally, the initial model shows that the environments are correlated, with generally far superior\nperformance than random, despite never training on the environment explicitly. In the case of b4, it is\nnearly equal to the baseline in the maximum metric, indicating signiﬁcant correlation. This is likely a\nreason for the greatly enhanced performance of incremental learning over the baseline in b4.\nNote that these experimental results are strictly a simple baseline for both Dex and incremental\nlearning, and suffer from instability due to the short timeline of training and lack of repeated\nexperiments. This means that agents do not necessarily consistently improve throughout training, but\nrather may quickly improve to maximum performance followed by decreased performance for the\nremainder of training. We leave more comprehensive experiments to future work.\n7\nVisualization\nTo qualitatively analyze the effects of incremental learning on a networks weights, we develop a\nsaliency visualization method based on Simonyan et al. [19] for reinforcement learning. Heatmaps are\ngenerated with a given networks weights and an input image. The method for gathering the heatmaps\nis identical to Simonyan et al. [19], and thus equations and derivations shall not be repeated in this\npaper. The most likely action the network will take at a given frame is used for the action to minimize\nthrough the gradient. This is a difference from the supervised learning case, where the ground truth is\nused. In reinforcement learning, the ground truth is not known, and thus must be inferred, as done in\nWang et al. [21]. Results of the visualization on the trained weights of environment sets a and b can\nbe seen in Figure 2 and Figure 3.\nFigure 2: Saliency mappings of a state from environment a3, with weights from set a. The ﬁrst row\nconsists of the trained baseline weights w1 to w3. The second row consists of the incrementally\nlearned weights w′\n2 to w′\n3.\nFigure 3: Saliency mappings of a state from environment b3, with weights from set b. The ﬁrst row\nconsists of the trained baseline weights w1 to w7. The second row consists of the incrementally\nlearned weights w′\n2 to w′\n7.\n7\nAs can be expected, saliency of a well performing trained agent will focus on the player location,\nbeing very sensitive to changes in a small region surrounding the player. This intuition is conﬁrmed\nin the ﬁrst row of Figure 3. The agents trained on the easier environments, such as b1, b2, and b3,\nfocus attention on the player and nearby threats. Interestingly, as the trained environment becomes\nmore complex, the agent appears less developed, and increasingly focuses on irrelevant locations in\nthe state, most prominent in the agent trained on b7, which explicitly avoids attention on the player\nand nearby threats, such as the walls. This indicates that the agent did not learn its environment\nsufﬁciently in the time it was given, due to the complexity of its training environment. This is further\nconﬁrmed through the experimental results of b6 and b7, which were near random.\nAdditionally, the saliency mappings show the correlation between the environments, as indicated by\nthe similarity in saliency in w1 and w2 to w3 in both Figure 2 and Figure 3, despite never explicitly\nbeing trained on the environment that the state in the visualization is from.\nInterestingly, the saliency mappings of the incrementally learned agents more closely resemble\nthe mappings of the weights it was initialized to than the weights its environment learned without\nincremental learning. This suggests that a signiﬁcant amount of the initialized weights are retained\nafter incrementally learning.\nFurther visualizations are included as a video of realtime saliency of an agent episode at\ngithub.com/innixma/dex.\n8\nFuture work\nWe hope to expand the analysis done in this paper by investigating incremental learning chains\ninvolving more than two environments. This will more effectively explore the effects of incremental\nlearning in complex problems. We also wish to extend the training time of experiments to allow for\nmore complex environments such as those shown in Figure 1 to be learned, as well as to compare our\nmethod to Progressive Neural Networks [17]. Incremental learning could be expanded to function as\na feature extractor in reinforcement learning. This would allow a more contained action space for\nincremental learning tasks of varying domains, and is similar to the work done in Rusu et al. [17].\nAdditionally, training could be done separately on multiple environments and then combined to\nlearn an environment that shares subproblems with all the previous environments. This would be a\nnatural merging of incremental learning and elastic weight consolidation [7]. It may also provide a\nsynergistic effect to algorithms such as UNREAL [6] which rely on auxiliary tasks. These tasks could\nact to maximize network utilization across multiple environments, potentially leading the network to\nbetter generalizations through incremental learning.\nFinally, environments such as Montezuma’s Revenge and Labyrinth [10] which require exploration\nwith sparse rewards are a natural expansion to the application of incremental learning. Developing\nincremental exploration environments could result in far superior performance on exploration tasks.\n9\nConclusion\nThe ability to learn and transfer knowledge across domains is essential to the advancement of agents\nthat can solve complex tasks. This paper introduced the continual learning toolkit Dex for training and\nevaluation of continual learning methods. We proposed the method of incremental learning for deep\nreinforcement learning, and demonstrated its ability to accelerate learning and produce drastically\nsuperior results to standard training methods in multiple Dex environments, supporting the notion\nof avoiding randomly initialized weights and instead using continual learning techniques to solve\ncomplex tasks.\nSource code for both the training methods used in this paper as well as the Dex toolkit source code\ncan be found at github.com/innixma/dex.\nAcknowledgments\nWe thank Vittorio Romeo for designing Open Hexagon.\n8\nReferences\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, I. J. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\nR. Józefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. G.\nMurray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. A. Tucker,\nV. Vanhoucke, V. Vasudevan, F. B. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,\nY. Yu, and X. Zheng. Tensorﬂow: Large-scale machine learning on heterogeneous distributed\nsystems. CoRR, abs/1603.04467, 2016.\n[2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An\nevaluation platform for general agents. CoRR, abs/1207.4708, 2012.\n[3] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying\ncount-based exploration and intrinsic motivation. CoRR, abs/1606.01868, 2016.\n[4] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba.\nOpenai gym. CoRR, abs/1606.01540, 2016.\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\nimage database.\nIn Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE\nConference on, pages 248–255. IEEE, 2009.\n[6] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu.\nReinforcement learning with unsupervised auxiliary tasks. CoRR, abs/1611.05397, 2016.\n[7] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,\nJ. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and\nR. Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National\nAcademy of Sciences, 114(13):3521–3526, 2017. doi: 10.1073/pnas.1611835114.\n[8] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Ried-\nmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.\n[9] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep rein-\nforcement learning. Nature, 518(7540):529–533, 2015.\n[10] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver,\nand K. Kavukcuoglu.\nAsynchronous methods for deep reinforcement learning.\nCoRR,\nabs/1602.01783, 2016.\n[11] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\nJ. Fürnkranz and T. Joachims, editors, Proceedings of the 27th International Conference on\nMachine Learning (ICML-10), pages 807–814. Omnipress, 2010.\n[12] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image\nrepresentations using convolutional neural networks. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), June 2014.\n[13] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and\ndata engineering, 22(10):1345–1359, 2010.\n[14] J. Peng and R. J. Williams. Incremental multi-step q-learning. Machine Learning, 22(1):\n283–290, 1996. ISSN 1573-0565. doi: 10.1023/A:1018076709321.\n[15] M. B. Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):77–104,\n1997.\n[16] V. Romeo. Open hexagon, 2012.\n[17] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu,\nR. Pascanu, and R. Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016.\n9\n[18] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch-\nbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.\nMastering the game of Go with deep neural networks and tree search. Nature, 529(7587):\n484–489, jan 2016. ISSN 0028-0836. doi: 10.1038/nature16961.\n[19] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising\nimage classiﬁcation models and saliency maps. CoRR, abs/1312.6034, 2013.\n[20] H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning.\nCoRR, abs/1509.06461, 2015.\n[21] Z. Wang, N. de Freitas, and M. Lanctot. Dueling network architectures for deep reinforcement\nlearning. CoRR, abs/1511.06581, 2015.\n[22] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. Sample\nefﬁcient actor-critic with experience replay. CoRR, abs/1611.01224, 2016.\n[23] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Ben-\ngio. Show, attend and tell: Neural image caption generation with visual attention. CoRR,\nabs/1502.03044, 2015.\n10\n",
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2017-06-19",
  "updated": "2017-06-19"
}