{
  "id": "http://arxiv.org/abs/1706.06302v1",
  "title": "Deep Learning in (and of) Agent-Based Models: A Prospectus",
  "authors": [
    "Sander van der Hoog"
  ],
  "abstract": "A very timely issue for economic agent-based models (ABMs) is their empirical\nestimation. This paper describes a line of research that could resolve the\nissue by using machine learning techniques, using multi-layer artificial neural\nnetworks (ANNs), or so called Deep Nets. The seminal contribution by Hinton et\nal. (2006) introduced a fast and efficient training algorithm called Deep\nLearning, and there have been major breakthroughs in machine learning ever\nsince. Economics has not yet benefited from these developments, and therefore\nwe believe that now is the right time to apply Deep Learning and multi-layered\nneural networks to agent-based models in economics.",
  "text": "arXiv:1706.06302v1  [q-fin.EC]  20 Jun 2017\nDeep Learning in (and of) Agent-Based Models:\nA Prospectus⋆\nSander van der Hooga\nJune 21, 2017\nAbstract\nA very timely issue for economic agent-based models (ABMs) is their empirical estima-\ntion. This paper describes a line of research that could resolve the issue by using machine\nlearning techniques, using multi-layer artiﬁcial neural networks (ANNs), or so called Deep\nNets. The seminal contribution by Hinton et al. (2006) introduced a fast and eﬃcient train-\ning algorithm called Deep Learning, and there have been major breakthroughs in machine\nlearning ever since. Economics has not yet beneﬁted from these developments, and there-\nfore we believe that now is the right time to apply Deep Learning and multi-layered neural\nnetworks to agent-based models in economics.\nKey words: Deep Learning, Agent-Based Models, Estimation, Meta-modelling.\n⋆This paper has beneﬁted from discussions with Spyros Kousides, Nan Su, Herbert Dawid and Blake LeBaron.\nAny remaining errors or omissions are the sole responsibility of the author. Financial support from the Horizon\n2020 ISIGrowth Project (Innovation-fuelled, Sustainable, Inclusive Growth), under grant no. 649186, is gratefully\nacknowledged.\naComputational Economics Group, Department of Business Administration and Economics, Chair for Eco-\nnomic Theory and Computation Economics, Bielefeld University, Universit¨atsstrasse 25, 33615 Bielefeld, Germany.\nEmail: svdhoog@wiwi.uni-bielefeld.de.\n1\n1\nIntroduction\nAgent-Based Models (ABMs) are becoming a powerful new paradigm for describing complex\nsocio-economic systems. A very timely issue for such models is their empirical estimation. The\nresearch programme described in this paper will use machine learning techniques to approach\nthe problem, using multi-layer artiﬁcial neural networks (ANNs), such as Deep Belief Networks\nand Restricted Boltzmann Machines. The seminal contribution by Hinton et al. (2006)\nintroduced a fast and eﬃcient training algorithm called Deep Learning, and there have been\nmajor breakthroughs in machine learning ever since. Economics has not yet beneﬁted from\nthese developments, and therefore we believe that now is the right time to apply Deep\nLearning and multi-layer neural nets to agent-based models in economics.\nEconomic Science is undergoing its own form of ”climate change” in economic theory as new\nsubﬁelds such as behavioural, experimental, computational, and complexity economics are\ngaining in support. Complexity economics brings in new tools and techniques that were\noriginally developed in physics and computer science, such as the theory of networks and new\nstatistical techniques for the study of many-body dynamics.\nThe agenda of this paper is to brieﬂy sketch the current state-of-the-art in Artiﬁcial\nIntelligence (AI) and Machine Learning (ML), and apply them to economic decision-making\nproblems. We outline a research programme that encompasses co-evolutionary learning,\nlearning from experience, and artiﬁcial neural networks (ANN), and connect these to\nagent-based modelling in economics and policy-making.\nWe begin by tracing back the common heritage of complexity economics and evolutionary\neconomics. A rich body of work by evolutionary economists deals with decision-making by real\neconomic agents, in an attempt to capture their routines by automatizing their\ndecision-making processes in computer algorithms. This starts with Herbert Simon’s ”A\nBehavioral Model of Rational Choice” (Simon, 1955), it continues with Cyert and March’s ”A\nBehavioral Theory of the Firm” (Cyert and March, 1963), and culminates in Simon’s seminal\nwork on ”The Sciences of the Artiﬁcial” (Simon, 1969).\nAround the same time, a computer science conference on ”The Mechanization of Thought\nProcesses” was held at the National Physical Laboratory (NPL) in 1958\n(National Physical Laboratory, 1959). The conference proceedings contain many of today’s hot\ntopics in Machine Learning: automatic pattern recognition, automatic speech recognition, and\nautomatic language translation. At the time, the developments in AI, machine learning and\ntheories of human decision-making were strongly intertwined.\nAfter describing this common heritage, we take stock of the current state-of-the-art in Machine\nLearning and extrapolate into the not too distant future.\nFirstly, we propose to emulate artiﬁcial agent behaviour by so called surrogate modelling,\nwhich could be thought of as a Doppelg¨anger approach, in which one agent is observing\nanother agent’s behavioural pattern and their performance. It then tries to imitate that agent,\nand eventually replace it in the simulation. In addition, the concept of an ANN-Policy-Agent\nis introduced who learns from observations of successful policy actions through reinforcement\nlearning mechanisms.\nSecondly, we propose to use ANNs as computational emulators of entire ABMs. The ANN\nfunctions as a computational approximation of the non-linear, multivariate time series\ngenerated by the ABM. It is a meta-modelling approach using statistical machine learning\ntechniques. There are various advantages to having such an emulator. It allows for a\ncomputationally tractable solution to the issue of parameter sensitivity analysis, robustness\nanalysis, and could also be used for empirical validation and estimation. This is particularly\n2\nappealing for large-scale ABMs that are computationally costly to simulate.\nThe goal is to develop new computational methods to improve the applicability of\nmacroeconomic ABMs to economic policy analysis. When successful, we would have drastically\nreduced the complexity and computational load of simulating ABMs, and come up with new\nmethods to model economic agents behaviour. Linking the time series forecasting capabilities\nof the Deep Learning algorithm to ABMs also allows us to envision the possibility of docking\nexperiments between diﬀerent ABMs: the time series output from one ABM can be fed into\nthe Deep Learning algorithm, resulting in an artiﬁcial neural network. This artiﬁcial neural\nnetwork can then be used as an agent inside another, larger-scale ABM. This notion leads to a\nhierarchical modelling scheme, in which ABMs of ABMs would become feasible.\nEach agent in the larger ABM can have an internal mental model of the world it inhabits, and\nthose mental models can diﬀer to any degree. On the longer term, this approach would allow\nthe inclusion of computational cognitive models into economic ABMs, allowing the agents to\nbe fully aware of their environment, and to consider the social embedding of their interactions.\n1.1\nRelated literature\nIn many cases, we do not know the correct equations of the economic model, and we might\nonly know the behaviour of the artiﬁcial economic agents approximately through observations\nof the empirical behaviour of their real-world counterparts (e.g., through direct observation of\nmarket participants, or through laboratory experiments). Therefore, we do not have access to\nthe mathematical description of the economic system, and have to resort to computational\nmodelling. Once we have constructed a computational model that satisﬁes certain\nrequirements (e.g., stock- ﬂow consistency of accounting relationships or dynamic completeness\nof behavioural repertoires) we usually ﬁnd that the model is realistic enough to reproduce\nseveral empirical stylized facts of macrovariables, such as GDP growth rates, inﬂation rates,\nand unemployment rates, but all too often it is computationally heavy.\nThere are currently several research eﬀorts under way to construct agent-based macroeconomic\nmodels (Dawid et al., 2014; Dosi et al., 2014; Grazzini and Delli Gatti, 2013). These models\naim to compete with standard Dynamic Stochastic General Equilibrium (DSGE) models that\nare currently in use by ECOFIN and most Central Banks around the world. Using such\nmodels for policy analysis requires that they are calibrated and estimated on empirical data.\nFor this, we need new methods and techniques to estimate such policy-oriented ABMs.\nCurrent large-scale agent-based simulation models (e.g., Dawid et al., 2014) require large\ncomputing systems, such as multi-processor servers, HPCs, or grids of GPUs, in order to run\nsuﬃciently many simulations. This not only involves running large numbers of simulations for\nproducing results for publications, but also to perform rigorous robustness testing, parameter\nsensitivity analyses, and general veriﬁcation and validation (V&V) procedures to ensure the\ncorrectness and validity of the computer simulations (cf. Sargent, 2011; Kleijnen, 1995).\nThe issue of computational intractability is ubiquitous. It has been around for a long time in\nphysics and climate science, where research using many-particle systems and large-scale\nclimate models is constantly pushing the frontier of what is feasible from a computational\npoint of view.\nIn this paper we describe how to tackle the problem by taking advantage of machine learning\ntechniques, in particular recent developments in artiﬁcial neural networks, such as Deep\nLearning, Deep Belief Networks, Recursive Networks and Restricted Boltzmann Machines.\nThe scientiﬁc relevance and innovativeness of this line of research is that it tries to solve the\ngeneric problem of computational tractability of computer simulation models (and more\n3\nspeciﬁcally, of agent-based economic models), not by resorting to technological solutions (e.g.,\nparallel computing, or GPU grids), but by using machine learning algorithms in order to\nreduce the computer simulation to a lighter form, by emulating the models using artiﬁcial\nneural networks, and by then adopting that simulation model to obtain results.\nIn order for agent-based models to be useful for economic policy analysis so called ”what-if\nscenarios” are used to test counter-factuals in would-be worlds. It is therefore necessary to use\nmodels with a suﬃciently high resolution in terms of the behavioural and institutional details.\nThe target models that we consider in this paper are large-scale agent-based models where\n’large-scale’ means on the order of millions of agents. High-resolution may refer to the\nresolution of time-scales, geographical scales, decision-making scales (number of options to\nconsider), or other dimensionality of the agents’ characteristics.\nThe advantage of such large-scale, high-resolution, high-ﬁdelity agent-based models is that\nthey can be used as virtual laboratories, or as laboratory ”in silico” (Tesfatsion and Judd,\n2006). The model can be used for testing various economic policies (Dawid and Fagiolo, 2008;\nDawid and Neugart, 2011; Fagiolo and Roventini, 2012b,a), that may not be feasible to test in\nthe real world (e.g., due to ethical objections). Examples include: What happens when the\nbiggest banks go bankrupt? Or: What happens when a Euro member leaves the Euro?\nObviously, these are not things we want to simply test in the real world, considering the\ndetrimental social consequences and ethical objections. The disadvantage is that such\nlarge-scale ABMs are quite heavy from a computational perspective. It is easy to generate\noverwhelming amounts of data, and reach the boundaries of what is commonly accepted to be\ncomputationally tractable, in terms of simulation time, number of processors used, and data\nstorage requirements.\nIf we want to apply such models to perform policy analyses, we have to test the robustness of\nthe model, i.e., to test whether the empirical stylized facts are still reproduced for many\nparameter settings. This involves performing a global parameter sensitivity analysis and a\nrobustness analysis against small changes in the economic mechanisms, or with respect to\nchanges in the individual behavioural repertoires of the agents. This usually requires a large\nnumber of simulations (on the order of thousands), in order to obtain a large enough sampling\nof the phase space, and to be able to ascertain whether the model is sensitive, stable, robust,\nor fragile.\nIn the social sciences where computer simulation models are being actively pursued (e.g.,\neconomics, sociology, econophysics) there are many discussions surrounding the empirical\nestimation and validation of these types of models (e.g., Werker and Brenner, 2004;\nBrenner and Werker, 2006; Fagiolo et al., 2007; Grazzini et al., 2012; Grazzini and Richiardi,\n2013; Grazzini et al., 2013; Yildizoglu and Salle, 2012; Barde, 2015; Lamperti, 2015). However,\nuntil now, no clear consensus has appeared how to resolve the the empirical validation problem.\nIn econometric applications, some advances have been made on the estimation of ABMs.\nNoteworthy are two approaches, one using non-parametric bootstrap methods (Boswijk et al.,\n2007) and the other using estimation of a master equation derived from the Focker-Planck\nequation (Alfarano et al., 2005; Aoki and Yoshikawa, 2007; Di Guilmi et al., 2008).\nCurrently, multiple projects are under way to construct agent-based macroeconomic models:\nthe Eurace@Unibi model (Dawid et al., 2014), the Crisis Project (Grazzini and Delli Gatti,\n2013), and the ”Keynes meeting Schumpeter” models (K+S models, Dosi et al., 2010, 2013,\n2014). These models consider it a feature, not a vice, to model the agents and their\nbehavioural repertoires in great detail, by taking care that all the behavioural assumptions are\nsupported by empirical evidence.\n4\n2\nApplying machine learning methods to economic problems\nA primary motivation for applying machine learning techniques to economic decision making\nproblems is the work by Herbert Simon on bounded rationality and satisﬁcing in\n”Administrative Behavior” and ”Sciences of the Artiﬁcial” (Simon, 1947, 1955, 1959, 1969).\nSimon being also one of the founders of modern-day Artiﬁcial Intelligence (AI), it seems only\nappropriate that in applying artiﬁcial neural networks to economic problems, we rely on\nvarious aspects of Simon’s path-breaking work.\nThe ﬁrst aspect we adopt is goal-oriented, adaptive behaviour. In a perfect world agents are\nnot required to spent time on planning and learning. They already have all the relevant\ninformation available, and are able to compute with full accuracy the outcome of their actions.\nHowever, a substantial amount of time of real decision makers is being spent on planning and\nlearning about new information. Time constraints are important for making decisions, hence\nsatisﬁcing with threshold aspiration levels rather then optimizing would be the preferred\nmethodology.\nOpen, complex systems make it essential to behave in a ﬂexible, adaptive manner, rather than\nusing rigid, predetermined rules that prescribe an exact course of action for every contingency.\nThis naturally leads to the use of heuristics, routines, and rules of thumb. Satisfying aspiration\nlevels instead of optimizing appears to be more appropriate as a model of man, as in the adage\n’Better to be approximately right, rather than exactly wrong.’ Such an approach would lead to\ndecision makers who realize they are fallible, and in order to achieve their goals they must do\nthe best they can given the circumstances. They would aim for robust decision making\nroutines, rather than precise prescriptions.\nSuch considerations point into the direction that human decision makers are not always able to\nmake perfect decisions, due to various limitations in their decision making capabilities: (i)\nImperfect information gathering, or incomplete observation of outcomes. (ii) Limitations in\nstorage capacity or faulty interpretation of those observations (imperfect recall, bad\ndocumentation of results). (iii) Limits in processing abilities. (iv) Imperfections in foreseeing\nthe exact consequences of their actions. Even when acting in perfect isolation or when they act\nin the belief that they have precise control over their actions, unintended consequences of\ndeliberate, decisive human action may result from a noisy environment. All such imperfections\nin gathering, storing and processing of information and in foreseeing events are a fact of life for\nthe human decision maker.\nA second motivation for applying machine learning techniques to economic problems is the\nseminal book ”A Behavioral Theory of the Firm” by Cyert and March (1963). This book\ndescribes many operating procedures related to real ﬁrm decision making. Besides an emphasis\non organizational processes and decision making routines, a further aim of the theory was to\nlink empirical data to the models by considering the results of case studies of real ﬁrms.\nA clear assessment of the impact of A Behavioral Theory of the Firm and its methodological\nstance was given by Argote and Greve (2007, p.339):\n”The general methodological point was that theory should model organizational\nprocesses, and should be generated through systematic observation of processes in\nactual organizations. One component of this point is that organizational theory\nshould not oversimplify. Although parsimony is needed in theory building,\nparsimony that throws out basic insights – like replacing a process model with a\nmaximization assumption – can be harmful.”\nIn the context of agent-based economic models, this idea has been developed further into the\n5\nManagement Science Approach (Dawid and Harting, 2012; Dawid et al., 2014). In this\napproach the economic agents are assumed to use decision making routines that are\nempirically-grounded in the Management Science literature. The underlying assumption is\nthat managers of a ﬁrm apply the methods and techniques that they have been taught whilst\ndoing their MBA at Management School. This method can for example be applied to model\nthe pricing behaviour of ﬁrms, the inventory management problem, the interest rate setting for\nloans by bank managers, or the hiring and ﬁring practices of a Human Resource Management\ndepartment.\nIn our approach, we use a combination of the artiﬁcial intelligence methods proposed by Simon\n(learning appropriate heuristics in order to satisfy certain goals), and the empirically-grounded\nbehavioural rules as proposed by Cyert and March (actual organizational processes).\nAnother exciting ﬁeld of research is to include a rich cognitive structure into the agents’\nbehavioral repertoires. The decision making routines, although adaptive, are often still too\nrigid from a cognitive science point of view. A lack of meta-rules to update the behavioral\nrules is often seen as a serious drawback of these models, especially when it comes to\naddressing the Lucas Critique which states that economic policy has to take into account the\nadaptive behavioral response by the agents that are subject to the policy.\nThis perceived lack of cognitive modelling in the behavioral routines of economic agents can be\nalleviated if we would allow each agent in the ABM to have an internal ”mental model” of the\nworld it inhabits, and those mental models can diﬀer to any degree. On the longer term, this\napproach would allow the inclusion of computational cognitive models into economic\nagent-based models, allowing the agents to be fully aware of their environment, and possibly\nalso to consider the social embedding of their interactions.\n2.1\nMachine learning for time series forecasting in Economics\nApplications of ANNs to time series forecasting problems in economics include: ﬁnancial\nmarket forecasting (Trippi and Turban, 1993; Azoﬀ, 1994; Refenes, 1995; Gately, 1996),\nforeign exchange rates (Weigend et al., 1992; Refenes, 1993; Kuan and Liu, 1995), load\ndemand forecasts on electricity markets (Bacha and Meyer, 1992; Srinivasan et al., 1994),\ncommodity prices (Kohzadi et al., 1996), and macroeconomic indices (Maasoumi et al., 1994)\nA review of applications of ANNs in the ﬁeld of Management Science and Operations Research\nis given by Wilson and Sharda (1992) and Sharda (1994). The M-competition\n(Makridakis et al., 1982) provides a widely cited data base for comparing the forecasting\nperformance of ANNs in comparison to traditional statistical methods. The data for the\nM-competition are mostly from business, economics, and ﬁnance, see Kang (1991); Sharda\n(1994); Tang and Fishwick (1993) for examples. Another comparison is provided by the Santa\nFe forecasting competition (Weigend and Gershenfeld, 1993) which includes very long time\nseries coming from various ﬁelds.\n2.2\nThe usefulness of ANNs to study complexity science\nAccording to Gorr (1994), ANNs are very appropriate in the following situations: (i) large\ndata sets; (ii) problems with nonlinear structure; (iii) multivariate time series forecasting\nproblems. Important issues that can be addressed include:\n(1) How do ANNs model the autocorrelated time series data and produce better results than\ntraditional linear and non-linear statistical methods?\n6\nAccording to Bengio et al. (2013), sequential statistical data (a.k.a. time series) suﬀer\nfrom the ”diﬃculty of learning long-term dependencies”. If the past is coded linearly\n(regardless of how many observations in the past) then the eﬀect of the past of the\nprevious step is diminishing exponentially. If the past is modelled non-linearly, then the\nnon-linearity is ”composed many times”, leading to a highly non-linear relationship of\npast to present. According to the paper cited, recurrent neural networks (RNN) are\nbetter in modelling such relationships. However, RNNs suﬀer from the problem of\n”diminishing gradients” when using back-propagation for training the weights with\nStochastic Gradient Descent. In such cases Hessian-Free (HF) optimization methods or\nMomentum Methods such as Nesterov’s Accelerated Gradient (NAG) seem more\npromising (see Section 3, Theme 3 for more details).\nThe paper suggest a number of optimizations for RNNs. We believe one of the most\nrelevant for our problem is that of ﬁltered, low-pass ﬁlter inputs. These are nodes with\nself-weights close to 1, (similar to exponential ﬁlters) that allow non-linearities to persist\nlonger and not disappear at the next step. This is coupled with non-linearities modeled\ne.g. as out = max(0, in) rather than a sigmoid or tanh function. There is justiﬁcation for\nthe approaches and some promising results (although this is by no means a solved\nproblem) in that the output of the error function is ”rough” and requires some form of\ncontrol for local cliﬀs that lead to local minima. All of the methods proposed in the\nliterature are gradient-tracking in one way or the other, and are conservative about\nsudden changes. Hessian-free optimization (Martens and Sutskever, 2011) and the\nPhD-thesis by Sutskever (2013) show the applicability of such methods in the\nmultivariate time series domain.\n(2) Given a speciﬁc forecasting problem, how do we systematically build an appropriate\nnetwork that is best suited for the problem?\nWe follow current best-practices as outlined above. We can start from the simplest RNN\nrepresentation, and try state-of-the-art approaches. The design of good initializations of\nthe networks is a good point of entry. If we have domain knowledge about the units that\noperate in the system and their qualities, we can estimate the relative size of each input\nnode and the long term eﬀect that actions should have. We then use state-of-the-art\nparameter estimation techniques, as described in Bengio et al. (2013), for example, in\norder to ﬁx the weights on the input nodes.\n(3) What is the best training method or algorithm for forecasting problems, particularly\ntime series forecasting problems?\nThis is discussed extensively in Sutskever (2013), noting that optimized Stochastic\nGradient Descent (SGD) may be adequate, if one considers proper initialization of the\nnetwork. Momentum methods are another option. As described above, we could start\nwith the simplest method ﬁrst (SGD), or consider the best practice for problems similar\nto ours. We should keep in mind, however, that we may have various problems that are\ndiﬀerent in structure. The doppelganger ANN described in Theme 1 (micro-emulation) is\nnot an RNN, but is rather an actuator based on the inputs. It can have memory of its\nown actions, but it is still distinctively diﬀerent from an RNN that models a time series.\nHence, we should ﬁnd diﬀerent best practices for each of our sub-tasks. In the theme\ndescriptions in Section 3 we make initial propositions for each of the theme descriptions.\n(4) How should we go about designing the sampling scheme, and the pre- and\n7\npost-processing of the data? What are the eﬀects of these factors on the predictive\nperformance of ANNs?\nOne of the advantages of ANNs is that they alleviate the need for feature engineering\nwhich is the art and science of traditional machine learning. Instead, any real number\ngoes through a squashing function (logistic or tanh), resulting in a number between 0\nand 1 (or -1 and 1). In case of categorical values, one can have a ’softmax layer’, that\nassigns a probability distribution over the states. Alternatively, one can have ”ON/OFF”\nnodes with binary values. The ﬁne art then becomes how to design the structure of the\nnetwork itself: how many layers, and how many nodes per layer.\nAll these questions are addressed in more detail below.\n3\nApplying Deep Learning algorithms to economic ABMs\nIn order to use macroeconomic agent-based models for policy, we need to reduce the\ncomplexity of the ABM simulation to a less complex, more computationally tractable system.\nIn other words, surrogate models or meta-modelling approaches need to be developed, that\nallow us to approximate or ’emulate’ the multi-dimensional nonlinear dynamics of the original\nsystem. The entire process of ﬁnding such an approximate ’emulator’ for an ABM consists of a\nfour-step procedure.\nFirst, we construct an ABM and generate synthetic time series data. Then, a multi-layered,\ndeep neural network is designed and trained on the synthetic data. Third, the trained neural\nnetwork should be empirically validated using real-world data. And fourth, we apply the\ntrained, empirically validated deep neural network to economic policy analysis.\nAccording to these four steps, the question how to construct eﬃcient emulators of ABMs could\nbe structured along four broad research themes:\nTheme 1: Micro-emulation of the behaviour of individual agents, creating so called\n”doppelg¨anger” of each agent.\nTheme 2: Macro-emulation of an entire ABM simulation, using the multivariate time\nseries data.\nTheme 3: Reduction of the complexity to design ANNs, setting the number of input\nnodes, number of hidden layers, and number of nodes in each hidden layer.\nTheme 4: Reinforcement learning in economic policy design, generating an ANN-policy\nagent that can be used for policy analysis.\nIn the end, the emulation of agent-based models using artiﬁcial neural networks\n(ANN-emulation) allows us to perform speciﬁc types of analysis that appear quite complicated\nwith large-scale ABMs.1 For example, the empirical validation of the ANN-emulator could be\nan eﬃcient, indirect method to calibrate the parameters of the ABM itself. Also, a global\nparameter sensitivity analysis could be performed using the ANN-emulator instead of the\nbulky ABM. And ﬁnally, after a careful internal validation procedure to check that the ANN\n1But not impossible in principle. For example, the global sensitivity analysis of a large-scale ABM such as the\nEurace@Unibi Model was already performed using HPC clusters. Also, empirical validation is currently being\ndone for medium-sized ABMs, and given the exponential increase in computing power is expected to yield results\nin the coming years.\n8\ncan in fact emulate the ABM to a suﬃcient degree of accuracy, the ANN-emulator could also\nbe used as a policy analysis tool.\nIn the following sections we describe each of these themes in more detail.\n3.1\nTheme 1: Micro-emulation of the behaviour of economic agents\nThis is a local approach, in the sense of modelling the behaviour of each of the rule-based\nagents by ANNs. A neural network is trained to predict the actions of a particular agent in the\nmodel, i.e. the ANN acts as a Doppelganger of that agent. Due to the multitude of instances\nand agent types, each with their own set of instructions and constraints, and because of the\ndynamically changing environment of the ABM, such networks can help us model the\nbehaviour of our agents and reduce the complexity of the model at the local level. The ANNs\nmay need extensive training, but are cheap when they run. In the end, the original agents may\nbe replaced by their Doppelganger, or we may run the hybrid model with both types of agents.\nVarious decision making problems in standard macroeconomics models are formulated as\noptimization problems. This theme is dedicated to show that this could be dealt with equally\nwell using machine learning methods. In each example below, we replace a standard\noptimization problem with a heuristic.\n1. The ﬁrm’s consumer demand estimation problem.\nIn the current model, demand is estimated by two methods, one is backward-looking, the other\nis forward-looking. In the backward-looking method, the ﬁrm only relies on past observations\nand uses a simple OLS regression on the previous months’ sales revenues to estimate the future\ndemand for its product. It estimates the parameters of a linear, quadratic, or cubic ﬁt for the\nperceived demand function that the ﬁrm beliefs to be facing (for more details, see Harting,\n2014, Ch.1, pp 13-14).\nIn the forward-looking method, the ﬁrm uses a market research routine that is commonly used\nin practice, namely to hold simulated purchase surveys among its consumers\n(Nagle and Hogan, 2006). Once a year, the ﬁrm considers a sample of households to present\nthem with a set of products at diﬀering prices. The survey contains questions regarding\nconsumers’ preferences and price sensitivities, and asks them how they would react c.q. how\nthey would alter their consumption pattern when the ﬁrm would change its price (assuming\nthe prices of all its competitors stay the same). In this way, the ﬁrm tries to gauge the overall\nprice sensitivity of consumers, and to estimate its future market share. (see Harting, 2014, Ch.\n3, pp. 79-81 for more details on the market research method, and references therein).\nThe ﬁrm’s consumer demand estimation problem using artiﬁcial neural networks.\nThe idea of replacing the linear, quadratic, or cubic ﬁtting of the data by a neural network is\nrelatively straightforward. The ANN-ﬁrm would try to estimate the local slope of its demand\nfunction by way of an ANN, and adjust its arc weights while the simulation is ongoing. Since\nneural networks are a data-driven approach, there is no need to assume any particular\nstatistical model. Also it is not necessary to rely on linear statistical methods such as OLS,\nsince ANNs are non-linear, non-parameteric time series methods.\n2. The consumers’ demand problem.\nIn the current model, the consumers’ decision to buy a product from a speciﬁc ﬁrm is derived\nfrom a discrete choice model using a multinomial logit function. The selection probability to\nselect a ﬁrm is an increasing function of the consumer’s utility for that ﬁrm’s product. The\nutility value is decreasing in price, so a ﬁrm with a higher price will have a lower selection\nprobability, but it is bounded away from zero. By replacing the logit function with a neural\n9\nnetwork formulation of the same problem, the ANN-consumer can learn how to best achieve its\ntarget value. In this way, the consumers’ choice problem is redeﬁned as goal-oriented\nbehaviour, rather than as a stochastic model of choice.\n3. The banks’ internal risk model.\nBanks’ decision making process involves the problem of setting interest rates for individual\nloans to the private sector. In order to make a proﬁt, banks should assess their debtors’ credit\nworthiness, and the likelihood that the borrower will not repay the loan in the near future (i.e.,\nthe probability that they will default on the credit). This includes an evaluation of the\nprobability of default of the debtor ﬁrm, but also the default on individual loans. In order to\nmake such an assessment, the banks either use an internal risk model, or rely on information\nprovided by external rating agencies, or both. Whichever method is being used, they have to\nassess the various types of risk associated to their assets, including market risk, credit risk, and\nsystemic risk (Duﬃe and Singleton, 2003). Market risk refers to changes in the value of the\nassets on the balance sheet of the bank. Typically, these are ﬂuctuating due to the\nmark-to-market asset valuation and the volatility of prices on the ﬁnancial asset markets.\nCredit risk refers to the risk the bank is facing due to the changing values of assets on the\nbalance sheets of its debtors.\nIn the current agent-based macroeconomic model (Eurace@Unibi), the bank uses a highly\nsimpliﬁed model to determine the probability of default of the ﬁrms to which they have\noutstanding loans, or of new ﬁrms that make credit requests. The essential aspect of the model\nis that the bank’s assessment of the ﬁrm’s probability of default is based on balance-sheet data\nof the ﬁrm, and derived from the ﬁrm’s equity and ﬁnancial ratios such as the\ndebt-equity-ratio, an indicator of ﬁnancial fragility.\nSuch a ”structural-model” approach may or may not be in accordance with the actual\nbehavior of real banks, which would be a matter of empirical study that is beyond the scope of\nour current research project. But in fact, many alternative models for evaluating credit default\nrisk exist, as illustrated by the rich overview given by Duﬃe and Singleton (2003).\nOne such an alternative approach is the ”ﬁrst-passage model” (Duﬃe and Singleton, 2003, pp.\n53), which uses empirical time series collected over a certain time window, to determine the\nactual default probabilities for a population of ﬁrms that have similar risk proﬁles. Such a\ntime series approach diﬀers substantially from the more theoretical ”reduced-form”\napproaches, but it would ﬁt quite nicely with the neural network approach.\nThe artiﬁcial neural network approach to model the banks’ decision making problem will thus\nprovide us with a nonlinear, nonparametric, multivariate time series forecasting method. The\nbank can be modelled as a goal-oriented entity, that tries to set interest rates based on its\nforecasted default probabilities, which are derived from time series that are being generated\nonline, i.e. during an ongoing simulation. In the end, this could yield an agent-based model of\nthe credit market in which the credit risk models proposed in Duﬃe and Singleton (2003) have\nbeen internalized into our agents’ behavioural repertoires.\nThis line of research can distinguish between ”oﬄine training” and ”online learning”. Oﬄine\ntraining makes use of time series data being generated by an agent-based model that is\n”detached” from the agent. One can think of this as an outside-observer-approach, where the\nagent is not part of the simulation environment, but can observe the actions and outcomes of\nother agents. This is similar to how children learn how to behave appropriately in a social\nenvironment, before they are accepted as full members of that environment.\nOnline learning, on the other hand, occurs while the agent is itself part of the simulation\nenvironment, and is observing the time series being generated online. If multiple agents are\nsimultaneously using online learning in this sense, we can speak of co-evolutionary learning\n10\nby a population of heterogeneous, artiﬁcial neural network agents.\nThe main aim of this particular research theme is to focus on the appropriate network\nstructure to emulate the multivariate time series data being generated by the target system (in\nthis case, the particular agents to emulate).\nThe ﬁnal goals of Theme 1 are to obtain: (i) a model of ﬁrm behaviour replaced by an ANN\nfor the ﬁrm’s demand estimation routine; (ii) a model of consumer behaviour replaced by an\nANN for the consumers’ choice problem; (ii) a model of bank behaviour replaced by an ANN\nfor the bank’s internal risk evaluation and risk-assessment problem.\n3.2\nTheme 2: Macro-emulation of an entire ABM simulation model\nDue to the recent breakthrough of Deep Learning techniques for multi-layered networks to\nmodel non-linearities, it becomes possible to emulate an entire ABM simulation by an ANN\ngenerating time-series. Contrary to the local approach in Theme 1, this is a global approach.\nA neural network is trained to predict the probabilistic structure of the macro-level variables\nof the model.\nThis is useful for robustness and parameter sensitivity analysis, since it allows a much larger\nexploration of the parameter space. A second advantage is that by training the ANN on many\ncounter-factual scenarios, it is expected to perform better than an ANN that has been trained\njust on the empirical, historic data, since this is just a single realization of the empirical data\ngenerating process.\nIn our aim to make the problem of tuning the parameters of an ABM more tractable, we try to\nemulate the input/output function of the entire ABM by an (ultimately) less complex and\nmore tractable Deep Neural Network. Our starting point is that, in an ABM, a multitude of\nautonomous agents react to changes in their (economic) environment and, in doing so, alter\nthat very environment. In a Deep Neural Net, a multitude of nodes at diﬀerent layers can\nencode diﬀerent information structures and decision processes, such that the network as a\nwhole can serve speciﬁc functions. Bringing the two together, we aim to train a neural network\nthat produces the same output (in terms of time series of macro-economic variables) as the\nABM.\nThis problem is similar to multi-variate time series forecasting, with the diﬀerence that instead\nof estimating the future values based on the past values, the input to the ABM are the actions\nof the agents in the model. A recurrent neural network (RNN) is the type of network that is\nthe obvious choice for such a task. A key part of the design is a feedback property, namely that\nthe output of the model (the values of the measured macro-economic variables) are fed back to\nthe input. A second key part is to split the network input layer to represent the ‘present’, and\nthe ‘past’. This is how the RNN design captures ‘history’: at each step t the network receives\ninputs at time t, but also of time t −1, and possibly further time lags.\nIn terms of integrating the decision processes of the individual agents, a ﬁrst approximation\ncould be a multi-layered structure in which the nodes of the ﬁrst input layer are entire ANNs\nthat model each individual agent in the agent-based model. Of course, this is not expected to\nbe any more tractable than the ABM itself. However, it is expected that the ANN will be able\nto emulate the ABM with a much smaller number of agents, as the multi-layer structure allows\nthe ANN to model increasingly more complex functions of the modelled economy without the\nneed to fully emulate it. Instead of representing the individual agents one by one, the ANN is\na representation of the entire ABM at increasing ”layers of abstraction”. Most importantly,\nthis theme will be informed by other themes, e.g. the modelling in Theme 1 for the individual\nagent ANNs can inform the initial design of the macro-emulation ANN.\n11\nTraining data for the macro-emulation ABM will be provided by already collected (synthetic)\ndata from ABM simulations, as well as from new simulations with agents that are themselves\nANNs, rather than the current ﬁxed behavioral routines. The big advantage of training the\nANN on simulated data, in addition to the abundance of such data, is that such a network will\nnot just learn to forecast a speciﬁc realization of some ABM emulation, but it will learn the\nmore general underlying data generating mechanism that is common to all such simulations\nwhich are seen during the training phase, and, ideally, also for new previously unseen\nsimulations. This provides for an out-of-sample validation stage by using a subset of the\nsynthetic data that was previously unseen by the ANN, and can be used to test the\nperformance of the macro-emulation Deep Neural Network.\nThe main aim of this particular research theme is to focus on the appropriate network\nstructure to emulate the multivariate time series generated by the macro-ABM as a whole. A\nsecond aim is to investigate what is the most appropriate learning/optimization technique for\nthis problem.\nThe ﬁnal goal of Theme 2 is to obtain a deep layered ANN that is trained on data generated\nby an ABM, and that can be usefully applied for empirical validation, and for policy analysis.\n3.3\nTheme 3: Reduction of the complexity to design ANNs\nThe design and training of deep ANNs is a complex task. To guide the design of the ANN in\nterms of the number of nodes and hidden layers, and in order to improve the eﬃciency of the\nDeep Learning algorithm, the complexity of the ANN must be reduced.\nThe problem of training deep neural networks is an unconstrained global minimization\nproblem, i.e. to ﬁnd the arc weights such that the training error of the ANN is minimized (the\ntraining error is the diﬀerence between the ANNs performance on the training set and on the\ntest set).\nThis problem is NP-hard, so the computational costs will increase exponentially with problem\nsize (given by the number of input nodes and the number of hidden layers). Therefore smart\nheuristics are needed to approximate the global minimum. Many such heuristics have been\ndeveloped, but most of these assume that the objective function (the loss function or error\nfunction) is diﬀerentiable in its arguments. Hence the algorithms make use of the gradient and\nthe Hessian of the objective function. Example methods include Gradient Descent (GD),\nStochastic Gradient Descent (SGD), Momentum methods and Nesterov’s Accelerated Gradient\n(see Sutskever, 2013 for an overview, and references therein).\nFor convex objective functions, to ﬁnd the global minimum the gradient methods are globally\nconverging, i.e. they will always ﬁnd the global minimum, but they will just take longer to\nconverge for worse initializations of the parameters. However, for deep and recurrent networks,\nthe initialization does matter since the objective function of such networks cannot be assumed\nto be convex. Hence, it is important to design good initializations for the algorithms.\nThe greedy unsupervised pre-training algorithm of Hinton et al. (2006) and\nHinton and Salakhutdinov (2006) is a good starting point since it greedily trains the\nparameters of each layer sequentially. Such greedy layerwise pre-training is then followed by a\n”ﬁne-tuning” algorithm such as the standard Stochastic Gradient Descent method.\nAnother method is the Hessian-Free (HF) Optimization (Martens and Sutskever, 2010, 2012)\nthat is able to train very deep feed-forward networks even without such a pre-training step.\nHF is a second-order method and therefore rather slow, but it is very powerful. It is the\npreferred method of optimization if there is no idea about good initializations of the network.\nThe most recent innovations in this ﬁeld, described by Sutskever (2013, Ch. 7), are able to\n12\ntrain very deep neural networks (up to 17 hidden layers) by using aggressive Momentum\nMethods. Such methods use gradient information to update parameters in a direction that is\nmore eﬀective then steepest descent by accumulating speed in directions that consistently\nreduce the objective function. The most promising method of this type is Nesterov’s\nAccelerated Gradient (NAG) method, which is a ﬁrst-order optimization algorithm that has\nbetter convergence properties than Gradient Descent. It has two parameters: a learning rate ε\nand a momentum constant µ, where (1 −µ) can be thought of as the friction of the error\nsurface. High values of µ implies the algorithm retains gradient information and leads to fast\nconvergence, while low values imply high friction and a loss of gradient information, leading to\nslower convergence to the global minimum.\nUsing NAG with very aggressive momentum values (µ close to 0.99) leads to excellent results\nfor problems that previously were deemed unsolvable, such as data sets exhibiting very long\ntime-dependencies (50-200 time steps).\nThe main aim of this research theme is to focus on Hessian-Free Optimization and Momentum\nMethods, and possibly adapt these methods to our speciﬁc problems. A second aim is to\noptimize the choice of network parameters: the number of input nodes, the number of hidden\nlayers, and how many nodes in each hidden layer.\nThe ﬁnal goals from Theme 3 are to design good initializations of the network parameters for\nthe Deep Learning algorithms, and to develop insights to inform the optimization methods and\nthe Deep Learning algorithms.\n3.4\nTheme 4: Reinforcement learning in economic policy design\nThe ﬁnal theme is to apply a surrogate, or meta-modelling approach to policy decision-making.\nA government or central bank agent may be given certain goals (e.g., maintaining a stable\nprice level, or a low unemployment rate, or macroﬁnancial stability) rather than using\nhand-crafted rules to serve those goals (such as a Taylor rule for monetary policy). Using\nreinforcement learning techniques, an agent starts with little knowledge of the world, but given\na reward function, the agent learns to perform better over time, during a simulation run. The\nABM allows us to evolve successful policies not only by using empirical data, but also by\nlearning from online-generated streaming data. The idea is to have a neural network policy\nagent (ANN-policy-agent), and this is again a local approach.\nThe objective is to develop a model with an endogenous policy-maker, the ANN-policy-agent,\nwho evolves its decision-making routines endogenously. This agent should adapt its policy in\nresponse to the behavioural changes of the other agents in the model.\nSimilar to Theme 1, we can again distinguish between ”oﬄine training” and ”online learning”.\nOﬄine training of the ANN-policy-agent.\nWe train the ANN-policy-agent using pre-generated, historical data from the original ABM\nsimulation. Its input are (rule-based) policy decisions made during that simulation and time\nseries of economic variables, and we use unsupervised layer-by-layer training. Thus, the\nANN-policy-agent learns (unsupervised) the outcome of policy decisions under speciﬁc\ncircumstances, and it is possible to re-enforce this training with data from multiple ABM\nsimulations by using a Monte Carlo approach. After this unsupervised pre-training, we\nperform an additional supervised training phase, in which we reward policy decisions that have\ndesired outcomes. The trained ANN-policy-agent is then used in ABM simulations as the\npolicy-making authority. Depending on the properties and coverage of the training data, this\ntype of ANN-policy-agent is expected to fare well in the test simulations.\n13\nOnline learning by the ANN-policy-agent.\nIn this setting, the ANN-policy-agent learns from online streaming data. It has to train its\nweights while taking actual policy actions during a running ABM economy. This situation is\nbad from an AI point of view, since training normally takes a long time and occurs in isolation\nof the actual environment. Therefore the analogy to our setup is a ANN-policy-agent that has\nnot been trained before, or has inappropriate weights for the situation. It has to adapt as best\nit can, similar to a learning child. However, this setting seems more close to what actual\nreal-world policy-makers are facing, especially in times of a changing economic environment.\nIn times of crisis, policy-makers have to adjust quickly to changing circumstances, possibly\nmaking choices that appear suboptimal, but satisfying certain target levels.\nThe main aim of this theme is to focus on which reward functions and what network structures\nare most appropriate for the ANN-policy-agent. A second aim is to design the endogenous\npolicy setting behaviour for the ANN-policy-agent: which behavioral heuristics are used, what\nmeta-rules adapt these heuristics, and what are the parameters for the ANN.\nThe ﬁnal goal of Theme 4 is to develop a model with a ANN-policy-agent that can set policy\nendogenously, and is able to adjust its policy response to the behavioral changes of the other\nagents in the model economy.\n4\nConclusion\nThe purpose of this paper is to sketch a line of research in which artiﬁcial neural networks\n(ANNs) are used as computational approximations or as emulators of the nonlinear,\nmultivariate time series dynamics of a pre-existing agent-based model (ABM). In other words,\nit is a meta-modelling approach using statistical machine learning techniques. There are\nvarious advantages to having such an emulator. For instance, it allows for a computationally\ntractable solution to the issue of parameter sensitivity analysis, robustness analysis, and could\nalso be used for empirical validation and estimation.\nThe overall goal is to develop new methods and techniques to improve the applicability of\nmacroeconomic ABMs to economic policy analysis. For the practical implementation of this\ngoal, we need to make advances in two domains:\n1. Deep Learning: developing new machine learning techniques to represent ABMs by\nANNs (Themes 1-2).\n2. Complexity Reduction: developing new complexity-reduction techniques to guide the\ndesign of ANNs (Theme 3).\nThe work to be done consists of the following broad research themes:\nTheme 1: Micro-emulation of the behaviour of agents. A neural network is trained to predict the\nactions of a particular agent in the model, i.e. the ANN acts as a Doppelganger of that\nagent.\nTheme 2: Macro-emulation of an entire ABM simulation. This is a global approach. A neural\nnetwork is trained to predict the probabilistic structure on the macro-level, of variables\nin the ABM model, based on the initialization parameters. ANNs have proved to be very\nsuccessful for multivariate time series forecasting. They are much more ﬂexible than\ntraditional statistical methods since they are nonlinear, nonparametric time series\napproximation techniques.\n14\nTheme 3: Reduction of complexity. The design of the structure of the neural network in terms of\nnumbers of input- and output nodes and the number of hidden layers is a complicated\nproblem. In order to improve the eﬃciency of the Deep Learning algorithm, the\ncomplexity of the ANN must therefore be reduced. This can be done by modelling it in\nterms of a Hamiltonian system, and proceed with describing the time-evolution of the\nHamiltonian.\nTheme 4: Reinforcement learning in policy design. A government or central bank agent may be\ngiven certain goals (such as a stable price level, low unemployment rates, or\nmacroﬁnancial stability), rather than hand-crafted rules. Using reinforcement learning\ntechniques, an agent starts with little knowledge of the world, but given a reward\nfunction that models those goals, the agent learns to perform better over time.\nThis may lead to more ﬂexible policies and more adaptive behaviour on the part of the\npolicy agent, as it allows for a more ﬂexible, discretionary policy setting behavior, rather\nthan using a ﬁxed, rule-based policy. As the policy agent learns how to set policies\noptimally, it must adapt to the behavioural changes of the other agents, who might\nchange their behaviour in response to the policy. Hence, this policy-feedback-loop\naddresses in a very natural way the Lucas Critique.\nIn summary, themes 1 through 4 not only help us to design a strategy how to emulate and\nestimate agent-based models using artiﬁcial neural networks, but it may also contribute to the\nburgeoning literature on learning in macroeconomics and optimal policy design. Hence, the\nresearch programme connects both micro- and macroeconomics, and joins both estimation and\nemulation in machine learning.\n4.1\nVision and outlook for the future\nWhen successful, we could apply the new methods to a plethoria of problems. We would have\ndrastically reduced the complexity and computational load of simulating agent-based models,\nand come up with new methods to model economic agents’ behaviour. Furthermore, linking\nthe time series forecasting capabilities of the Deep Learning algorithm to agent-based models\nalso allows us to envision the possibility of docking experiments between diﬀerent ABMs: the\ntime series output from one ABM can be fed into the Deep Learning algorithm, resulting in an\nartiﬁcial neural network. This artiﬁcial neural network can then be used as an agent inside\nanother, larger-scale ABM. This notion leads to a hierarchical modelling scheme, in which\nABMs of ABMs would become feasible. Each agent in the larger ABM can have an internal\n”mental model” of the world it inhabits, and those mental models can diﬀer to any degree. On\nthe longer term, this approach would allow the inclusion of computational cognitive models\ninto economic agent-based models, allowing the agents to be fully aware of their environment,\nand to consider the social embedding of their interactions.\nReferences\nAlfarano, S., Lux, T., and Wagner, F. (2005). Estimation of Agent-Based Models: The Case of\nan Asymmetric Herding Model. Computational Economics, 26(1):19–49.\nAoki, M. and Yoshikawa, H. (2007). Reconstructing Macroeconomics. Cambridge Books.\nCambridge University Press.\n15\nArgote, L. and Greve, H. R. (2007). A Behavioral Theory of the Firm – 40 years and counting:\nIntroduction and impact. Organization Science, 18(3):337–349.\nAzoﬀ, E. (1994). Neural Network Time Series Forecasting of Financial Markets. John Wiley\nand Sons, Chichester.\nBacha, H. and Meyer, W. (1992). A neural network architecture for load forecasting. In\nProceedings of the IEEE International Joint Conference on Neural Networks, volume 2,\npages 442–447.\nBarde, S. (2015). A Practical, Universal, Information Criterion over Nth Order Markov\nProcesses. Studies in Economics 1504, School of Economics, University of Kent.\nBengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2013). Advances in optimizing\nrecurrent networks. In IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 8624–8628.\nBoswijk, H. P., Hommes, C. H., and Manzan, S. (2007). Behavioral heterogeneity in stock\nprices. Journal of Economic Dynamics & Control, 31(6):1938–1970.\nBrenner, T. and Werker, C. (2006). A Practical Guide to Inference in Simulation Models.\nPapers on Economics and Evolution 2006-02, Philipps University Marburg, Department of\nGeography.\nCyert, R. M. and March, J. G. (1963). A Behavioral Theory of the Firm. Prentice Hall,\nEnglewood Cliﬀs.\nDawid, H. and Fagiolo, G. (2008). Agent-based models for economic policy design:\nIntroduction to the special issue. Journal of Economic Behavior & Organization, 67(2):351 –\n354. Special issue on Agent-based models for economic policy design.\nDawid, H., Gemkow, S., Harting, P., van der Hoog, S., and Neugart, M. (2014). Agent-Based\nMacroeconomic Modeling and Policy Analysis: The Eurace@Unibi Model. In Chen, S.-H.\nand Kaboudan, M., editors, Handbook on Computational Economics and Finance. Oxford\nUniversity Press.\nDawid, H. and Harting, P. (2012). Capturing ﬁrm behavior in agent-based models of industry\nevolution and macroeconomic dynamics. In B¨unsdorf, G., editor, Evolution, Organization\nand Economic Behavior, chapter 6. Edward Elgar.\nDawid, H. and Neugart, M. (2011). Agent-based models for economic policy design. Eastern\nEconomic Journal, 37.\nDi Guilmi, C., Gallegati, M., and Landini, S. (2008). Modeling Maximum Entropy and\nMean-Field Interaction in Macroeconomics. Economics Discussion Papers 2008-36, Kiel\nInstitute for the World Economy.\nDosi, G., Fagiolo, G., Napoletano, M., and Roventini, A. (2013). Income distribution, credit\nand ﬁscal policies in an agent-based Keynesian model. Journal of Economic Dynamics &\nControl, 37:1598–1625.\nDosi, G., Fagiolo, G., and Roventini, A. (2010). Schumpeter meeting Keynes: A policy-friendly\nmodel of endogenous growth and business cycles. Journal of Economic Dynamics & Control,\n34:1748–1767.\n16\nDosi, G., Napoletano, M., Roventini, A., and Treibich, T. (2014). Micro and Macro Policies in\nKeynes+Schumpeter Evolutionary Models. LEM Papers Series 2014/21, Laboratory of\nEconomics and Management (LEM), Sant’Anna School of Advanced Studies, Pisa, Italy.\nDuﬃe, D. and Singleton, K. J. (2003). Credit Risk: Pricing, Measurement, and Management.\nPrinceton University Press, Princeton.\nFagiolo, G., Moneta, A., and Windrum, P. (2007). A Critical Guide to Empirical Validation of\nAgent-Based Models in Economics: Methodologies, Procedures, and Open Problems.\nComputational Economics, 30(3):195–226.\nFagiolo, G. and Roventini, A. (2012a). Macroeconomic policy in DSGE and agent-based\nmodels. Revue de l’OFCE, 124:67–116.\nFagiolo, G. and Roventini, A. (2012b). On the scientiﬁc status of economic policy: a tale of\nalternative paradigms. The Knowledge Engineering Review, 27:163–185.\nGately, E. (1996). Neural Networks for Financial Forecasting. John Wiley, New York.\nGorr, W. L. (1994). Editorial: Research prospective on neural network forecasting.\nInternational Journal of Forecasting, 10(1):1–4.\nGrazzini, J. and Delli Gatti, D. (2013). Paper on the development of MABM Mark II: The\ninput-output network in the CRISIS Macro Agent-Based Model. CRISIS Project Deliverable\nD3.3, Universit Cattolica del Sacro Cuore, Milano.\nGrazzini, J., Richiardi, M., and Sella, L. (2012). Indirect estimation of agent-based models: An\napplication to a simple diﬀusion model. LABORatorio R. Revelli Working Papers Series 118,\nLABORatorio R. Revelli, Centre for Employment Studies.\nGrazzini, J. and Richiardi, M. G. (2013). Consistent Estimation of Agent-Based Models by\nSimulated Minimum Distance. LABORatorio R. Revelli Working Papers Series 130,\nLABORatorio R. Revelli, Centre for Employment Studies.\nGrazzini, J., Richiardi, M. G., and Sella, L. (2013). Analysis of Agent-based Models.\nLABORatorio R. Revelli Working Papers Series 135, LABORatorio R. Revelli, Centre for\nEmployment Studies.\nHarting, P. (2014). Policy design in the presence of technological change – an agent-based\napproach. Ph.D. Thesis, University of Bielefeld.\nHinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for deep belief\nnets. Neural Computing, 18(7):1527–1554.\nHinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786):504–507.\nKang, S. (1991). An Investigation of the Use of Feedforward Neural Networks for Forecasting.\nPhD thesis, Kent State University.\nKleijnen, J. P. C. (1995). Veriﬁcation and validation of simulation models. European Journal\nof Operational Research, 82(1):145–162.\n17\nKohzadi, N., Boyd, M., Kermanshahi, B., and Kaastra, I. (1996). A comparison of artiﬁcial\nneural network and time series models for forecasting commodity prices. Neurocomputing,\n10:169–181.\nKuan, C.-M. and Liu, T. (1995). Forecasting exchange rates using feedforward and recurrent\nneural networks. Journal of Applied Econometrics, 10(4):347–64.\nLamperti, F. (2015). An Information Theoretic Criterion for Empirical Validation of Time\nSeries Models. LEM Papers Series 2015/02, Laboratory of Economics and Management\n(LEM), Sant’Anna School of Advanced Studies, Pisa, Italy.\nMaasoumi, E., Khotanzad, A., and Abaye, A. (1994). Artiﬁcial neural networks for some\nmacroeconomic series: A ﬁrst report. Econometric Reviews, 13:105–122.\nMakridakis, S., Anderson, A., Carbone, R., Fildes, R., Hibdon, M., and Lewandowski, R.\n(1982). The accuracy of extrapolation (time series) methods: Results of a forecasting\ncompetition. Journal of Forecasting, 1:111–153.\nMartens, J. and Sutskever, I. (2010). Parallelizable sampling of markov random ﬁelds.\nArtiﬁcial Intelligence and Statistics, pages 517–524.\nMartens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-Free\noptimization. In Proceedings of the 28th International Conference on Machine Learning\n(ICML).\nMartens, J. and Sutskever, I. (2012). Training deep and recurrent networks with Hessian-Free\noptimization. In Montavon, G., Orr, G. B., and M¨uller, K.-R., editors, Neural Networks:\nTricks of the Trade, volume 7700 of Lecture Notes in Computer Science, pages 479–535.\nSpringer Berlin Heidelberg.\nNagle, T. and Hogan, J. (2006). The Strategy and Tactics of Pricing: A Guide to Growing\nMore Proﬁtably. Pearson Prentice Hall, New Jersey.\nNational Physical Laboratory, editor (1959). Mechanisation of Thought Processes, Proceedings\nof a Symposium held at the National Physical Laboratory on the 24th, 25th, 26th and 27th\nNovember 1958. Her Majesty’s Stationary Oﬃce, 1959.\nRefenes, A. (1993). Constructive learning and its application to currency exchange rate\nforecasting, chapter 39, pages 777–806. Probus Publishing Company, Chicago.\nRefenes, A. (1995). Neural Networks in the Capital Markets. John Wiley, Chichester.\nSargent, R. G. (2011). Veriﬁcation and validation of simulation models. In Proceedings of the\nWinter Simulation Conference, WSC ’11, pages 183 – 198. Winter Simulation Conference.\nSharda, R. (1994). Neural networks for the ms/or analyst: An application bibliography.\nInterfaces, 24:116–130.\nSimon, H. A. (1955). A Behavioral Model of Rational Choice. Quarterly Journal of\nEconomics, 69(1):99–118.\nSimon, H. A. (1959). Theories of Decision-Making in Economics and Behavioral Science.\nAmerican Economic Review, 49:253–283.\n18\nSimon, H. A. (1996 [1969]). The Sciences of the Artiﬁcial. The MIT Press, Cambridge, MA.\n(3rd ed.).\nSimon, H. A. (1997 [1947]). Administrative Behavior. The Free Press, New York, NY. (4th\ned.).\nSrinivasan, D., Liew, A., and Chang, C. (1994). A neural network short-term load forecaster.\nElectric Power Systems Research, 28:227–234.\nSutskever, I. (2013). Training Recurrent Neural Networks. PhD thesis, Department of\nComputer Science, University of Toronto.\nTang, Z. and Fishwick, P. (1993). Feedforward neural nets as models for time series\nforecasting. ORSA Journal on Computing, 5:374–385.\nTesfatsion, L. and Judd, K. E. (2006). Handbook of Computational Economics II: Agent-Based\nComputational Economics. North-Holland.\nTrippi, R. and Turban, E. (1993). Neural Networks in Finance and Investment: Using\nArtiﬁcial Intelligence to Improve Real-world Performance. Probus, Chicago.\nWeigend, A. and Gershenfeld, N. (1993). Time Series Prediction: Forecasting the Future and\nUnderstanding the Past. Addison-Wesley, Reading, MA.\nWeigend, A., Huberman, B., and Rumelhart, D. (1992). Predicting sunspots and exchange\nrates with connectionist networks, pages 395–432. Addison-Wesley, Redwood City, CA.\nWerker, C. and Brenner, T. (2004). Empirical Calibration of Simulation Models. Papers on\nEconomics and Evolution 2004-10, Philipps University Marburg, Department of Geography.\nWilson, R. and Sharda, R. (1992). Neural networks. OR/MS Today, pages 36–42.\nYildizoglu, M. and Salle, I. (2012). Eﬃcient Sampling and Metamodeling for Computational\nEconomic Models. Cahiers du GREThA 2012-18, Groupe de Recherche en Economie\nTh´eorique et Appliqu´ee.\n19\n",
  "categories": [
    "q-fin.EC"
  ],
  "published": "2017-06-20",
  "updated": "2017-06-20"
}