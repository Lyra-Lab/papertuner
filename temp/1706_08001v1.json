{
  "id": "http://arxiv.org/abs/1706.08001v1",
  "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of learning relational order via reinforcement learning procedure?",
  "authors": [
    "Zizhuang Wang"
  ],
  "abstract": "In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.",
  "text": "\nAbstract— Recent works on recurrent neural network\nand deep learning architecture have shown the power of\ndeep\nlearning\nin\nmodeling\ntime\ndependent\ninput\nsequences.\nSpecific\nlearning\nstructure\nsuch\nas\nHigher-order\nboltzmann\nmachine,\ngradient-based\nlearning manifold, and Recurrent “grammar cell” reveal\ntheir ability to learn feature transformation between\nrelated input maps, and perform well in time-related\nlearning & prediction tasks in higher order cases. In this\narticle,\nwe\nextend\nthe\nconventional\nconvolutional-Restricted-Boltzmann-Machine\nto\nlearn\nhighly abstract features among abitrary number of time\nrelated\ninput\nmaps\nby\nconstructing\na\nlayer\nof\nmultiplicative units, which capture the relations among\ninputs. In some cases, we only care about how one map\ntransforms into another, so the multiplicative unit takes\nfeatures from only this two maps. In other cases, however,\nmore than two maps are strongly related, so it is\nreasonable to make\nmultiplicative unit learn relations\namong more input maps, in other words, to find the\noptimal relational-order(number of related input maps\nthat the multiplicative unit extracts features from) of\neach unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning\nmethod whose optimality is proven to train the network.\nKeywords: Artificial Neural Network;\nConvolutional-Restricted-Boltzmann-Machine;\nReinforcement learning;\nDeep learning;\nTemporal-related; Relational-order\nⅠ. INTRODUCTION\nUnsupervised learning combined with deep architecture\nhas unveiled part of the mystery of artificial intelligence.\nSuch learning techniques have boarder applications in areas\nlike visual recognition, natural language processing, audio\ndetection,\nand\ncognitive\nanalysis.\nWith\ngrowing\ncomputational capabilities, deep learning framework like\nconvolutional deep belief network [1] can bring more\ncontribution to cognitive science. Recently, researchers begin\nto take the next step, trying to develop model that can handle\nManuscript received October 16, 2016\nZizhuang Wang is with Xiaogan Senior High School, China\n(email: 1012125144@qq.com ; web: http://kingofspace0wzz.github.io/ )\ntime dependent learning tasks. Traditionally, recurrent neural\nnetwork(RNN) has shown its efficiency in time-dependent\nrecognition problems. For instance, RNN is widely used as an\nelegant framework to manipulate audio model [2], which is\nbased\non\ntime-related\ninputs.\nRecent\nresearches\nalso\ncombined RNN with the power of convolutional restricted\nboltzmann machine(CRBM), such as gated autocoder and\nfactorized CRBM [3][4].\nLater, more attention has been driven to a special form\nof time-based model, in which a restricted boltzmann\nmachine is built to learn feature transformations that describe\nrelations between time-related input maps. In the model,\nwhich is similar to conventional RBM [5][6], a hidden stack\nof layers is constructed to describe conditional probabilistic\ndistributions over inputs. The difference from traditional\nmodel is that the hidden layers take into account of several\ninput maps at different time moment. That is , the hidden\nlayers are able to extract features from a combination of\nseveral time-related maps. By doing so, we can extract\nfeatures that are explained by hidden layers to represent\ncorrelation between inputs maps, in other words, to describe a\nmatrix transformation from one map to another. Previous\nworks [7][8][9][10] noted that multiplicative interaction is an\neffective way to correlate input maps. We use this method in\nour model to combine related inputs. With the learned hidden\nfeatures, or transformation, our machine can predict later\ninputs based on conditional distributions that are learned and\ncarried by the hidden layers of RBM. Also, with the power of\nhigh-order temporal dependencies that is describe by [10], we\ncan learn features that are even more abstract. In orther words,\nwe can learn the features of transformations of input maps.\nThis can be achieved with more hidden layers to be\nconstructed and through learning efficiency brought by deep\nlearning architecture.\nThere are limitations in higher order temporal model.\nSince conventional multiplicative interaction only takes into\naccount of\ntwo related maps, it lacks the ability to correlate\nmore input maps (input maps sequence for instance) and\ntherefore can only learn features from two related inputs.\nTheoretically, we can learn temporal\ndependence among inputs no matter how far they are through\nhigh-order training process. Thus, combining only two input\nmaps\nthrough\nmultiplicative\ninteraction\nseems\nto\nbe\nachievable and efficient. In practice, however, this will cause\nthe number of layers and parameters that are needed to be\nlearned to explode. Moreover, in some cases in which we\nmay have a large amount of similar or strongly correlated\nTemporal-related\nconvolutional-Restricted-Boltzmann-Machine capable of\nlearning relational order via reinforcement learning\nprocedure\nZizhuang Wang\ninput maps sequence, it is wise to combine them all together\nand to use only one stack of hidden layers to describe their\ncorrelation, saving a lot of space for parameters and time for\nlearning. To do this, we define the term “relational order” as\nthe number of maps that one stack of hidden layers learn\nfeatures from, in other words, the number of maps that we\ncorrelate with each other using multiplicative interactions.\nFinally, we developed a reinforcement-learning [11] based\nmethod to learn the relational order at each time by\nminimizing reconstruction error. We then proved that it\nsatisfies the sub-problem structure of dynamic programming\n[12][13][14].\nTherefore,\nby\nfinding\nthe\noptimized\nreconstruction error at each group of related input maps, we\nget the globally optimal solution of the entire input sequence.\nⅡ. RELATED WORK\nTraditional temporal-dependent RBM is widely used to\nmodel language processing, audio recognition and other\ntime-evolutional learning tasks . In [15][16], deep belief\nnetwork has revealed advantages over Gaussian mixture\nmodels for automatic speech recognition. Recent works [17]\nin image recognition show that a convolutional network\n(CNN) can dramatically enhance the power of artificial\nneural network. Basically, a CNN architecture includes\nhidden layers that are gained from convoluting high\ndimensional inputs with\nkernels, and a pooling layer to\nexclude noise in order to gain highly abstract features. By\nconstructing a convolutional layer, CNN takes the advantage\nof local invariance and parameter sharing [18]. In image\nclassification tasks, since the objects that need to be\nrecognized from the image input may show up at different\nlocations and angles, it is crucial to equip the model with the\ntranslational invariance property. By sharing parameters, on\nthe other hand, we reduce parametric redundancy and\ntherefore save a lot of space. Thus, instead of defining\nindependent hidden units learning features in different parts\nof the input space, we make hidden units share the same\ncombination of weights to extract features that may come up\nfrom different locations. These advantages make CNN\nextremely efficient. Therefore in this article, we developed a\nmodel based on convolutional deep neural network that takes\ntemporal dependency into account. Such model combines the\npower of deep convolution and the capability of time-based\nmodel that can handle temporal related inputs. We showed its\nefficiency in tasks of learning matrix transformations of\nrelated inputs.\nⅢ. MODEL\nRecently, convolutional Restricted Boltzmann Machine\nhas been used to extract features from high dimensional and\nhighly abstract dataset [1]. In case of image processing, we\nextract features in the image by convoluting it with a 2d\nkernels and then construct a 2d layer of RBM [1]. After\nupdating the kernels using CD-k algorithm, we are able to\nperform the task of image recognition efficiently by sending\nthe outputs to a classical discriministic layer, or to generate\nan image map sampling from the Joint distribution P(X,Z),\nwhere the latent variable Z is described by hidden layers of\nC-RBM [5][6].\nConventional recurrent network(RNN) has been proven to be\na useful model that can handle time dependent learning tasks\nsuch as recognition and prediction for future outcomes based\non previous inputs. In order to combine the power of deep\nlearning and the ability to extract features from time sequence,\nresearchers have developed structures that collaborate RBM\nwith RNN. In [3], for example, stacks of hidden units have\nbeen used to model time evolution features, and then a hidden\npooling layer computes the sum of these hidden units by a\nsigmoid\nfunction,\nand\nfinally\nupdate\nthe\nweights\nby\nminimizing the distance function between the real visible\ninputs and the generative visible variables sampled from\nconditional distribution of P(v|h). Such model provides the\nbasic\narchitecture\nfor\naudio\nrecognition\nand\nother\ntime-evolved learning tasks.\nHowever, unlike the model of deep audio recognition\nframework, in which the input is a 1d time-related sequence\nand the machine constructs a distribution over hidden layer to\ncapture time dependent features of sequenced audio signals,\nthe 2d C-RBM lacks the ability to grab relations between\ntime related sequence of images. Therefore In this article, we\nuse a different framework, which combines the structure of\nhigh\ndimensional\nconvolutional-neural-network\nwith\nrestricted boltzmann machine by constructing only one stack\nof hidden variables. The idea is, instead of building stacks of\nhidden layers, that the model constructs multiplicative units\nrelating the present input with the previous ones. In this\nmodel, hidden variables are viewed as a layer that portrays\ncorrelation among observations at different time t. For\nprediction, the hidden layer generates future maps based on\nthe\nlearned\nprobabilistic\ndistribution.\nIn\nmany\ncases\n(Bi-Temporal-related model, Section 4), each multiplicative\nunit takes account of two input maps. One major drawback of\nthis framework, therefore, is that the model only takes\ncorrelated features among observations of specified length or\ntime range. However, in natural brain system, cognitive\nprocesses are affected by wider range of inputs taken from\nsensors. Therefore, in order to make machine capable of\nlearning optimal range of input maps that the multiplicative\nunit takes features from, a reinforcement learning model is\nspecified over time range to help the machine take different\nlength of inputs sequences by solving the optimal value of the\nreward function defined by the model.\nⅣ. BI-TEMPORAL-RELATED MODEL\nIn Bi-related feature learning, machine extracts features\nfrom two related input maps. In order to combine the pair of\ntwo maps, a multiplicative unit is constructed to take the\nmatrix multiplication of these maps as an input, and then is\nconnected to a stack of hidden layers through k different\nkernels. Each hidden layer is constructed by sampling from a\nprobabilistic distribution gained by convoluting the input\nwith k kernels separately. The parameters of the model\ninclude k kernels and the biases associated with the hidden\nlayers and the multiplicative unit respectively.\nTo describe the probabilistic distribution learned by\nmodel more explicitly, we redefine the traditional energy\nfunction of RBM [5][6] as\n）\n（1\n)\n(\n)\n,\n,\n(\n2\n1\n,\n,\n1\n,1\n2\n1\n2\n1\n\n\n\n\n\n\n\n\n\n\nk\nk\nk\nk\nN\nj\ni\nN\ns\nr\nk\nij\ns\nj\nr\ni\nk\nrs\nv\nbv\nh\nc\nh\nv\nv\nW\nh\nv\nv\nE\nh\nw\nwhere k is the index for hidden layers, n is the index for input\nmaps, W represents the kernel matrix, and Nh NW denotes the\ndimensions( rows and cols) of the hidden layers and of kernel\nmatrix respectively. In [5][6], the probabilistic model RBM\nbased on energy above is defined by\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh\nh\nv\nE\nh\nv\nE\nv\nh\nv\nE\nh\nv\nE\nv\nh\nv\nE\nh\nv\nE\nh\nv\nE\ne\ne\nv\nP\nh\nv\nP\nv\nh\nP\ne\ne\nZ\ne\nZ\ne\nh\nP\nh\nv\nP\nh\nv\nP\nZ\ne\nh\nv\nP\n)\n,\n(\n)\n,\n(\n)\n,\n(\n)\n,\n(\n)\n,\n(\n)\n,\n(\n)\n,\n(\n)\n(\n)\n,\n(\n)\n|\n(\n,\n/\n/\n)\n(\n)\n,\n(\n)\n|\n(\n,\n)\n,\n(\nThen, we can derive the form of conditional distribution of\nhidden layers over the combined input maps as\n)\n))\n(\n((\n)\n,\n|\n1\n(\n2\n1\n2\n1\nk\nij\nk\nk\nij\nc\nv\nv\nW\nsigmoid\nv\nv\nh\nP\n\n\n\n\nwhich defines the distribution of the ijth element of the kth\nhidden layer conditioned on two input maps. To see this, we\nlet hxyz denote the xyth element of the zth hidden layer, and\nh(-1) denote the other elements. If we define\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nz\nk\ny\nx\nj\ni\nk\nij\nk\nz\nk\nN\ny\nx\nj\ni\nN\ns\nr\nk\nij\ns\nj\nr\ni\nk\nrs\nz\nN\ns\nr\ns\ny\nr\nx\nk\nrs\nz\nxy\nv\nbv\nh\nc\nh\nv\nv\nW\nh\nv\nB\nc\nv\nv\nW\nh\nA\nh\nw\nw\n2\n1\n,\n,\n,\n,\n,\n1\n,1\n2\n1\n,\n1\n,1\n2\n1\n)\n(\n))\n1\n(\n,\n(\n,\n)\n(\n)\n(\n, then it is easy to show that\n)\n))\n(\n*\n((\n)\n(\n)\n|1\n(\n2\n1\n0\n)\n(\n))\n1\n(\n,\n(\n1\n)\n(\n))\n1\n(\n,\n(\n))\n1\n(\n,\n(\n)\n(\nz\nxy\nz\nz\nxy\nh\nA\nh\nv\nB\nh\nA\nh\nv\nB\nh\nv\nB\nh\nA\nz\nxy\nc\nv\nv\nW\nsigmoid\nA\nsigmoid\ne\ne\ne\ne\ne\ne\nv\nh\nP\nz\nxy\nz\nxy\nz\nxy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwhere we used notation “*” to represent valid convolution, in\nwhich the last term can be viewed as a convolutional kernel.\nWe can also write it in matrix form if we refine “sigmoid” as\nan operation on each matrix’s element as\n)\n)\n(\n(\n)\n,\n|\n1\n(\n2\n1\n2\n1\nk\nk\nk\nc\nv\nv\nW\nsigmoid\nv\nv\nh\nP\n\n\n\n\nSimilarly, the conditional distribution of the multiplicative\nunit is\n\n\n\n\nk\nk\nk\nb\nh\nW\nsigmoid\nh\nv\nv\nP\n)\n*\n(\n)\n|\n1\n(\n2\n2\n1\nAfter concating it with the original one, we get the\nfull generative multiplicative-unit. We denote the or\nginal and the generative multiplicative unit by\nO\nO,\n. The\nn, by approximating the generative multiplicative unit as\n2\n1\n2\n1\nv\nv\nv\nv\nO\n\n\n, we can use least squares to find the ap\nproximated version of each generative input maps.\n)\n2\n(\n)\n,\n(\n2\n1\nT\nT\nT O\nv\nlstsq\nv \n)\n3\n(\n)\n,\n( 1\n2\nO\nv\nlstsq\nv \nwhere\nwe\nhave\nused\n“lstsq()” as the notation of least square method that solves the\nequation\n)\n,\n(\nB\nA\nlstsq\nx\nB\nAx\n\n\nthe\nbar\nabove\nO\nmeans\nthe\ngenerative\nversion\nof\nmultiplicative unit, and “T” represents matrix transpose.\nIdeally, we want to learn kernel and bias by maximizing\nthe log likelihood in gradient ascent fashion given the\nconditional distributions above [5][6]. The gradient of the log\nlikelihood for a CRBM based on energy model is\n)\n4\n(\n)\n,\n(\n)\n,\n(\n)\n,\n(\n)\n|\n(\n)\nln(\n)\nln(\n)\n|\n(\n(\n,\n,\n)\n,\n(\n)\n,\n(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nh\nv\nh\nh\nv\nh\nv\nE\nh\nh\nv\nE\nh\nv\nE\nh\nv\nP\nh\nv\nE\nv\nh\nP\ne\ne\nv\nP\nLn\n\n\n\n\n\n\nUnfo\nrtunately, computing this gradient involves an exponential\nnumber of terms. However, Hinton in [6] described a novel\nway,\nnamely\nthe\nso\ncalled\ncontrastive divergence\nto\napproximate\nthe\ngradient\nof\n(4),\nand\nthen\nuse\nthis\napproximated gradient to update parameters via gradient\nascent procedure. In contrastive divergence, for every\niteration of updates, the multiplicative unit is sampled over\nhidden layers by m times. Take the kth kernel matrix for\nexample, its gradient can be approximated as\n)\n5\n(\n)\n,\n|\n1\n(\n)\n(\n)\n,\n|\n1\n(\n)\n(\n)\n|\n(\n(\n)\n(\n2\n)\n(\n1\n)\n(\n2\n)\n(\n1\n)\n0\n(\n2\n)\n0\n(\n1\n)\n0\n(\n2\n)\n0\n(\n1\nm\nm\nk\nm\nm\nk\nk\nv\nv\nh\nP\nv\nv\nv\nv\nh\nP\nv\nv\nW\nv\nW\nP\nLn\n\n\n\n\n\n\n\n\nwhere the upper index over input layers means the original\ninput maps and those sampled by m times respectively. The\nm-step CD procedure does converge as shown by Hinton [6].\nSimilarly, the gradients associated with bias can be written as\n)\n6\n(\n)\n|\n(\n(\n)\n,\n|\n1\n(\n)\n,\n|\n1\n(\n)\n|\n(\n(\n)\n(\n2\n)\n(\n1\n)\n0\n(\n2\n)\n0\n(\n1\n)\n(\n2\n)\n(\n1\n)\n0\n(\n2\n)\n0\n(\n1\nm\nm\nm\nm\nk\nk\nk\nv\nv\nv\nv\nb\nv\nP\nLn\nv\nv\nh\nP\nv\nv\nh\nP\nc\nv\nP\nLn\n\n\n\n\n\n\n\n\n\n\n\n\nTo see that the gradients we get have the matrix form we want.\nWe denote the dimension of multiplicative unit, kernel, and\nhidden layers by NV, NW, NH. By the definition of\nconvolution, NH=NV-NW+1. According to (5), the gradient of\nthe kernel is gained by take the difference between two\nconvolution, in which the hidden layer acts as a kernel.\nTherefore, by definition, the dimension of the gradient is\nNg=NV-(NV-NW+1)+1=NW. Similarly, it is easy to show that\nthe dimension of bias’s gradients are equal to that of bias.\nThus, we verified that the gradients have the correct form.\nPseudocode of m-step constrastive divergence for\ntraining bi-related C-RBM is provided in algorithm 1.\nAlgorithm 1: Bi-temporal-related CRBM using m-step\ncontrastive divergence\nInitialize learning rate η = 0.2,\nFor t=0, 1, 2, 3 ...,T\nfor i = 0, 1, 2, 3, ..., m\nSet multiplicative unit O(i) := Vt(i)×Vt+1(i)\nFor k = 0, 1, 2, 3, ..., K\nHk = Bernoulli(P(hk=1|O))\nIf (i==0)\nGradient(0)k = O(i) * P( hk(t)=1|O(i) )\nIf (i==m)\nGradient(m)k=O(i) * P( hk(t)=1|O(i) )\nEnd for\nVt(i) = P( Vt(i)|h(t) )\nVt+1(i) = P( Vt+1(i)|h(t) )\nEnd for\nFor k = 0, 1, 2, 3, ..., k\nWk=Wk + η(Gradient(0)k - Gradient(m)k)\nCk = ck + ( P(hk=1|O(0) - P(hk=1|O(m)) )\nEnd for\nbt= bt + Vt(0) - Vt(m)\nBt+1 = bt+1 + Vt+1(0) - Vt+1(m)\nEnd for\nEnd for\nIn the algorithm, index k represents time, and the second\nindex i represents the step of CD. Figure.1 below shows the\ngeneral structure of Bi-related model.\nI. HELPFUL HINTS\nⅤ. Tri-Temporal-Related Model\nAs we have discussed, bi-related model is sufficient to\nhandle temporal learning task with the advantage brought by\ndeep CRBM. However, in practice we need to extend\nbi-multiplicative unit to learn correlation from more than two\nmaps. The reason for making that extension is that in many\nimage recognition problems, more that two maps are\ncorrelated. Therefore, constructing hidden layers to extract\nrelation\nbetween\nonly\ntwo\nmaps\nis\ninefficient\nand\nspace-consuming. Also, in other cases where the rate at\nwhich the frames are sent into the unit might be too fast for\nthe model to manipulate. For example, in cases where we\nmay want to predict the objects’ locations in a video based on\nfeatures captured by temporal CRBM from inputs that are\ngained by cutting video input into pieces of frames, each of\nwhich represents a input map, the model does not have\nenough time to train network completely on two-related input\nmaps before next pair of related maps shows up. Therefore,\nwe have no options by to keep the rate of frames down. In\nother words, we need to slow down the rate at which video\ninput is cut by frames into the model to give our machine\nmore time to learn features from any pair of maps. However,\nthis is extremely time-consuming.\nIn this section, we introduce a more general structure to\naddress these problems. Recall that we have defined the term\nrelational-order as the number of input maps that the hidden\nlayers capture features from, we want machine to break the\nconstrain of extracting features from limited number of maps\nby combining more related maps together, boosting up\nlearning efficiency and reducing the unnecessary waste of\nlearning space for parameters and hidden layers that are\nconstructed during the training process. More specifically, in\na typical tri-relational model, each multiplicative units takes\naccount of three subsequent maps instead of two, and a stack\nof hidden layers being constructed to learn correlation among\nthese three maps. Since the energy function and the\nconditional distribution in bi-related model do not hold for\ntr-related model, we need to slightly change the form of (1),\n(2), (3) to\n)\n10\n(\n)\n,\n(\n)\n9\n(\n)\n)\n,\n(\n,\n(\n)\n8\n(\n)\n,\n)\n((\n7\n)\n(\n)\n,\n,\n,\n(\n2\n1\n3\n3\n1\n2\n3\n2\n1\n3\n2\n1\n,\n,\n1\n,1\n3\n2\n1\n3\n2\n1\nO\nv\nv\nlstsq\nv\nO\nv\nlstsq\nv\nlstsq\nv\nO\nv\nv\nlstsq\nv\nv\nv\nbv\nh\nc\nh\nv\nv\nv\nW\nh\nv\nv\nv\nE\nT\nT\nT\nT\nT\nT\nk\nk\nk\nk\nN\nj\ni\nN\ns\nr\nk\nij\ns\nj\nr\ni\nk\nrs\nh\nw\n\n\n\n\n\n\n\n\n\n\n\n\n\n）\n（\nThen we are able to train the model by Algorithm 1 with\nm-step constrastive divergence, in which for each step, input\nmaps are sampled over conditional distributions (8) - (10).\nFigure.2 shows the structural difference between Bi-related\nand Tri-related model.\nⅥ.\nHigher-Temporal-Related Model based on\nReinforcement learning procedure\nWe\nthen\nextend\ntr-temporal-related\nCRBM\nto\nhigher-temporal-related CRBM with relational-order higher\nthan three. Assume we are now using N-relational-order\nCRBM, we write conditional distributions of each input maps\nas\n)\n15\n(\n)\n,\n(\n)\n14\n(\n)\n)\n,\n)\n((\n,\n(\n)\n13\n(\n)\n)\n,\n)\n((\n,\n(\n)\n12\n(\n)\n,\n)\n((\n11\n)\n(\n)\n,\n,\n,\n,\n(\n1\n2\n1\n5\n4\n2\n1\n3\n4\n3\n1\n2\n3\n2\n1\n3\n2\n1\n,\n,\n1\n,1\n3\n2\n1\n3\n2\n1\nO\nv\nv\nv\nlstsq\nv\nO\nv\nv\nv\nlstsq\nv\nv\nlstsq\nv\nO\nv\nv\nv\nlstsq\nv\nlstsq\nv\nO\nv\nv\nv\nlstsq\nv\nv\nv\nv\nbv\nh\nc\nh\nv\nv\nv\nv\nW\nh\nv\nv\nv\nv\nE\nN\nN\nT\nT\nT\nN\nT\nT\nT\nN\nT\nT\nT\nN\nk\nN\nn\nN\nk\nk\nk\nN\nj\ni\nN\ns\nr\nk\nij\ns\nj\nr\ni\nN\nk\nrs\nN\nh\nw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n）\n（\nThen we use algorithm 1 to train the model by constructing a\nmultiplicative unit O that learns features from N maps.\nFigure.1: Bi-temporal-related model\nFigure.2: Bi-related model vs Tri-related model\nAlgorithm 2: Higher order temporal CRBM\nInitialize learning rate η = 0.2,\nFor t = 1, 1+N, 1+2N, ...\nfor i = 0, 1, 2, 3, ..., m\nSet multiplicative unit O(i) := Vt(i)×Vt+1(i)...×Vt+N\n(i)\nFor k = 0, 1, 2, 3, ..., K\nHk (t)= Bernoulli( P( hk(t)=1|O ) )\nIf (i==0)\nGradient(0)k=O(i) P( hk(t)=1|O(i) )\nIf (i==m)\nGradient(m)k=O(i)*P (hk(t)=1|O(i))\nEnd for\nVt(i) = P( Vt(i)|h(t) )\nVt+1(i) = P( Vt+1(i)|h(t) )\n...\nVN(i) = P( VN(i)|h(t) )\nThrough equation (12) -\n(15)\nEnd for\nFor k = 0, 1, 2, 3, ..., k\nWk = Wk + η(Gradient(0)k - Gradient(m)k)\nCk = ck + (P(hk(t)=1|O(0) - P(hk(t)=1|O(m))\nEnd for\nbt = bt + Vt(0) - Vt(m)\nbt+1 = bt+1 + Vt+1(0) - Vt+1(m)\n...\nbN = bN + VN(0) - VN(m)\nIn the algorithm, index k represents time, and the second\nindex i represents the step of CD.\nAs we have discussed in Section 3, one major problem\nfaced by specific relational-order CRBM is the lack of\ngenerality. Once the relational-order is determined, the model\nis constrained to learn features from specific number of maps\nduring training process. However, in many cases, sequences\nof input maps and their correlations are not the same. For\nexample, in some input sequences, the number of strongly\nrelated maps might be three, but it may change to six later on.\nTherefore, it is reasonable to make relational order evolve\nthrough\ntime.\nTo\ndo\nthis,\nwe\nfirst\nproved\nthat\nthe\nsub-sequence of a given sequence of input maps satisfies the\nsub-problem structure in dynamic programming. Then we\ndevelop a reinforcement learning procedure to learn the\nrelational-order of each multiplicative unit by minimizing the\nreconstruction error of the whole sequence of inputs\nrecursively through dynamic programming method that\nminimizes the reconstruction error of each sub-sequence\ninput maps. Finally, we proved that the optimality is hold by\nthis method.\nWe first denote a Markov decision process as (S, A, r),\nwhere “S” is the state-space, “A” is the action-space, and “r”\nrepresents the reward function [11][13]. r(s, a) is defined as\nthe reward returned by taking action from state s. The task is\nto learn a policy \nthat maps from the current state to an\naction. One obvious approach to determine the overall value\nof a policy is to evaluate the cumulative reward through that\npolicy\nover\ntime.\nMore\nformally,\ncumulative\nreward\nfollowing a policy \nfrom a given state s is defined as,\n\n\n\n\n\n\n\n0\n1\n1\n0,\n)\n(\ni\ni\ni\nt\nc\nr\nc\ns\nV \n, where c is a constant\nthat determines the relative value versus immediate reward.\nMore specifically, the importance of reward received at i time\nstep is decreased exponentially by a factor ci. Based on\ndefinitions above, the best policy we want the model to learn\nis the policy that gives the biggest cumulative reward. That is,\n)\n16\n(\n)\n(\nmax\narg\n*\nts\nV \n\n\n\n.\nWith that, the model can take the best action following the\noptimal policy defined above. Alternatively, we can base on\ncumulative reward function to choose “a” among actions as\nwell. In that case, we redefine the best policy as\n)\n17\n(\n)}\n,\n(\n{\nmax\narg\n)}\n'\n(\n)\n,\n(\n{\nmax\narg\n*\n*\na\ns\nQ\ns\ncV\na\ns\nr\na\na\n\n\n\n\nwhere s’ denotes the state after taking action a, and function\nQ is defined as the largest cumulative reward after taking\naction a at state s. By writing the relation between reward\nfunction and Q function more explicitly as\n)'\n,\n(\nmax\n)\n(\n'\n*\na\ns\nQ\ns\nV\na\n\nit is easy to see that Q can be defined as\n)\n18\n(\n)'\n,'\n(\nmax\n)\n,\n(\n)\n,\n(\n'\na\ns\nQ\nc\na\ns\nr\na\ns\nQ\na\n\n\nThis recursive form of Q value function provides a basis for\nthe algorithm that iteratively approximates value of Q\nefficiently [11]. [11][13] shows that the Q value gained by\nrecursive method does converge to the optimal value.\nTo apply reinforcement-learning to our model, we view\neach relational-order as a state in “S”. “A” contains three\nactions, “+1” ,“0”, and “-1”(Here we assume that the\nrelational-order can only change by one each time for\nsimplicity). The goal is to minimize the average\nreconstruction error,\n))\n(\n*\n/\n2\n1\n)\n(\n2\n)\n(\n)\n0\n(\n,\nN\nf\nN\nv\nv\nN\nJ\nm\nn\nn\nN\nn\nj\ni\n（\n\n\n,\nwhere the upper index (0) represents the original map, and (m)\nrepresent\nthe\none\nreconstructed\nby\nm\ntimes\nthrough\nconstrastive divergence. The function of dominator N is to\nnormalize reconstruction error so J of each relational-order\nhas the same scale. Since we’d like to make model choose\nhigher-order so that it can model a sequence of inputs with\nfewer number of multiplicative units and thereby reduce the\nnumber of parameters, we multiply N with an monotone\nincreasing function f with respect to relational-order N. When\nN increase, f(N) will also increase, and therefore if\nreconstruction error of each input maps do not change by\nmuch, the J value would decrease. We than define the reward\nfunction as\nJ\nR\n\n\n, so maximizing the reward is the same\nas minimizing the reconstruction error. In general procedure,\nwe initialize all Q value to one, and choose the first relational\norder according to a gaussian distribution. After taking an\naction based on a probabilistic distribution defined as\n\n\n'\n)'\n,\n(\n)\n,\n(\nexp\n)\n|\n(\na\na\ns\nQ\na\ns\nQ\ns\na\nP\n, update the current Q(s,a) by (18),\nwhere a’ denotes the action taken at the next state s’. Note\nthat if the current state is two, then the model can only take\nthe action “+1” or “0”. Along this procedure, the model stores\na table of Q value representing the reward at each\nrelational-order state. Based on this table, the model can\nchoose\nthe\nrelational-order\nitself\nthat\nhas\nthe\nleast\nreconstruction error at each training step by taking actions\naccording to the distribution P(a|s). Also, we see that this\nbrings the advantage of short-term memory, since the\npreviously accumulative rewards added to Q will influence\nthe state of the current training step. This also confirms the\nprevious assumption that the relational-orders of nearby\nmultiplicative units are also correlated. If the relational-order\nof current unit is 4, then that of the next unit would probably\nbe around 4, instead of changing to 30 suddenly.\nReinforcement--learning\nprocedure\ncanhelp\neach\nmultiplicative unit find its optimal relation-order along which\nminimizes the reconstruction error of the input maps. But,\nwould\nminimizing\nthe\nreconstruction\nerror\nof\neach\nsub-sequences minimizes the reconstruction error of the\nwhole sequence of input maps? We now show it indeed is.\nAssume that we have a sequence of input maps with length T.\nAnd we construct N multiplicative units, each of which has\nits relational-order n1,n2,,, nN. JT is defined as\nnN\nn\nn\nT\nJ\nJ\nJ\nJ\n\n\n\n\n\n2\n1\n, which is the reconstruction error of the whole sequence.\nAccording to dynamic programming, we have\n）\n（19\n)\nmin\nmin(\n)\nmin(\n1\nnN\nT\nT\nJ\nJ\nJ\n\n\n\n.\nWe now prove it is sufficient by giving\n)\nmin\nmin(\nmin\n1\n1\n1\n2\n1\nnN\nT\nnN\nT\nnN\nT\nnN\nn\nn\nT\nJ\nJ\nJ\nJ\nJ\nJ\nJ\nJ\nJ\nJ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, (19) does give the optimal reconstruction error.\nThen, using (19), we see that\n)\n20\n(\nmin\nmin\nmin\nmin\n)\nmin(\n)\nmin\nmin(\n)\nmin(\n2\n1\n)\n1\n(\n2\n1\nnN\nn\nn\nnN\nN\nn\nT\nnN\nT\nT\nJ\nJ\nJ\nJ\nJ\nJ\nJ\nJ\nJ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis confirms the statement before that the optimal of J of the\nwhole sequence can be gained by finding the optimal J of\neach sub-sequence. This is the core of dynamic programming,\nand in artificial intelligence it is called reinforcement\nlearning, which is what we have introduced earlier. Thus, we\nuse our\nreinforcement-learning\nprocedure to\ntrain\nthe\nhigh-temporal-related model at each learning epoch.\nBelow is the pseudocode\nAlgorithm 3: Higher-related model\nInitialize all Q(s,a) and r(s,a) to one\nInitialize all weights and bias by gaussian distribution with\nmean 0 and variance 1.\nSet initial relational order s to arbitrary number that is greater\nthan 1.\nFor epoch e = 0, 1, ,,, E\nFor t = 0, 1, 2, ...,T\nRun algorithm 2 with relational-order n=S(t)\nChoose “a” according to P(a|S(t))\nIf “a” is “-1” & S > 2\nUpdate state, S(t+1) := S(t)-1\nIf “a” is “0”\nUpdate state, S(t+1) := S(t)\nIf “a” is “+1”\nUpdate state, S(t+1) := S(t)+1\nRun algorithm 2 with relational-order N= S(e)\nSet r(S(t), a) = -JN (set the reward at state S taking action\na to the minus of reconstruction error)\nUpdate the value of Q(S(t), a) by equation (18)\nEnd for\nEnd for\nⅦ. Experiments\nThis section focus on experiments that demonstrate the\nperformance of temporal-related CRBM in practice. We will\nuse\nthe\nfamous\nMINIST-dataset(http://yann.lecun.com/exdb/mnist/).\nAt\neach training process, we give the model a sequence of digit\ninput maps. And than we apply TD-CRBM to learn the\nmatrix transformations among the maps. Finally, we use the\nmatrix transformations we learned to produce generative\nmaps of each digit inputs and then measure the reconstruction\nerror to show model performance.\nA. Performance of Bi-Temporal-Related model in\nMINIST-dataset\nWe\nfirst\ntest\nmodel’s\nability\nto\ncapture\nsimple\ntransformation such as shifts and rotation of a single digit.\nWe construct a bi-temporal model, which consists of a\nmultiplicative unit with relational-order 2, each of which\nconnects to a stack of K hidden layers. We then give the\nmodel 10 sequences of input maps of length 100, each of\nwhich consists of 100 digit maps of different angles towards\nthe axis. We train the model following algorithm 1 and store\neach stack of hidden layers as feature maps that describe the\ncorrelation between each pair of digit inputs. After each\ntraining epoch is finished, we calculate the reconstruction\nerror between the generative digit inputs and the original ones.\nWe\nsee\nthat\nas\nthe\ntraining\nepoch\nincreases,\nthe\nreconstruction error drops out exponentially, which confirms\nthe efficiency and ability of bi-related model. Figure.3 shows\nthe performance of Bi-related model in the task of learning\ntransformation between pair of MINIST-digit maps.\nB. Higher-related with Q-learning VS Bi- & Tri-related\nAs our discussion in previous sections, higher-related\nmodel is more practical and can reduces the amount of\nparameters that must be learned. In this section, we\nimplemented an experiment that shows different performance\nof higher, tri, and bi-temporal-related models.\nAgain, we give each model 10 sequences of maps, each\nof which consists of 100 digit maps of the same number digit.\nWe constraint on the amplitude of the transformation\nbetween pair of inputs. More specifically, the rotation from\nthe last map do not exceed 20 degrees, and the shift of the\ncentroid do not exceed 2. We then use algorithm 1 to train\nbi-related model, algorithm 2 with relational-order 3 to train\ntri-related model, and algorithm 3 with initial order 5 to train\nhigher-related model .\nFigure.3 and .4 summarizes the results of the experiment,\nfrom which we can evaluate the performance of three\ndifferent models.\nModel \\\nEpoch\n0\n400\n800\n1000\n1200\n1400\nBi\n99\n99\n99\n99\n99\n99\nTri\n50\n50\n50\n50\n50\n50\nHigher\n25\n14\n13\n12\n16\n14\nWe now throw away the constraint on transformation to see\nhow higher-related model behaves.\nModel \\\nEpoch\n0\n400\n800\n1000\n1200\n1400\nBi\n99\n99\n99\n99\n99\n99\nTri\n50\n50\n50\n50\n50\n50\nHigher\n25\n68\n76\n81\n88\n91\nC. Results and conclusion\nWe first analyze bi-related model. We see from Figure.3\nand Figure.4 that performance of bi-related model are the\nsame in each case. This is because 2rd relational-order\nmultiplicative unit is the most basic one. And the feature map\nlearned by each unit can well describe the relationship\nbetween pair of input maps, no matter how much one map\ntransform into another.\nIn tri-related model, however, we see even though it has\nroughly the same training epoch to that of bi-related, it does\nnot perform well when there is no constraint on the\ntransformation\nbetween\neach\npair\nof\nmaps.\nAnd\nthe\nreconstruction error is still above “170” even after 1400\ntraining epochs. This is true because when there is no\nlimitation on transformation, the least number of highly\nrelated maps is only two. Each multiplicative unit can only\nlearn features from two maps with one transformation. When\na\n3rd\nrelational-order\nunit\nis\nconstructed\nto\nlearn\ntransformation from three maps with two far different\ntransformation, the error goes up. For example, consider a\nsequence of three maps, the first map rotates 90 degrees\nclockwise into the second map, and the second rotates 120\ndegrees counterclockwise into map three. In this case,\nbi-related model constructs two multi-units, each of which\nlearns the feature map of one rotation. The Tri-related model,\nhowever, will not perform well because one multi-unit can\nnot learn two different transformations at the same learning\nepoch.\nWe\nsee\nthat\nhigher-order\nmodel\nwith\nreinforcement--learning procedure is more flexible. It can\nchoose the optimal relational-order for each multi-units to\ngive the least reconstruction error. In Figure.3, when each\nmap\nis\nhighly\nrelated,\nhigher-order\nmodel\nconstructs\nmulti-units with relational-order higher than 5, thereby\nreducing the number of parameters dramatically. Then in\nFigure 4, when there is no constraint on transformation, we\nsee that even though its error during early steps of training is\nFigure.3: Performance of the three models\nwhen nearby inputs are highly correlated\nFigure.4: Performance of the three model when\nthere is no constraint on transformation\nfar higher that those of other two models, high-order model\ncan adjust the relational-order to the optimal that gives the\nleast reconstruction error. After 400 training epochs, it\noutperforms tri-related model. After 1400 training epochs, it\nreduce its reconstruction error to roughly the same scale to\nthat of bi-related model.\nⅧ. Conclusion: Limitation & Further research\nIn this article, we introduced a CRBM structure that\nmodels temporal-dependent input sequences by constructing\nmultiplicative units and hidden layers to learn features of\ncorrelation among related input maps. We then combined this\nmodel with reinforcement--learning procedure. By doing so,\nwe made our model capable of learning the optimal\nrelational-order for each multiplicative units to give the least\namount of errors. From the experiment, we see that the model\nwith reinforcement--learning procedure is more flexible on\nchoosing\nthe\nrelational-order.\nAnd\nit\nreconstructs\nthe\ngenerative maps with the same accuracy to that of bi-related\nmodel in the long-term.\nHowever, reinforcement--learning strategy increases\nthe complexity of computation and the amount of training\ntime dramatically. We see that it takes about 1400 training\nepochs for the model to reduce its reconstruction error down\nto 100. Its performance only catches up with that of bi-related\nmodel in the long-term. Therefore, further researches to\nreduce the training period are needed.\nREFERENCES\n(Periodical style)\n[1]\nLee, H. Convolutional deep belief networks for scalable\nunsupervised learning of hierarchical\nrepresentations[J].International Conference on Machine\nLearning, 2009, (June):609-616\n[2]\nLee, H, Pham, P. T., Yan, L., & Ng, A. Y.. Unsupervised\nfeature learning for audio classification using convolutional\ndeep belief networks. [J].Advances in Neural Information\nProcessing Systems, 2009, 22:1096-1104\n[3]\nShi, Y, Zhang, W. Q., Liu, J., & Johnson, M. T..Rnn\nlanguage model with word clustering and class-based output\nlayer[J].Eurasip Journal on Audio Speech & Music\nProcessing, 2013, (1):1-7\n[4]\nTaylor, G，. Factored Conditional Restricted Boltzmann\nMachines for Modeling Motion Style.\n(pp.1025-1032).[J].International Conference on Machine\nLearning, 2009, :1025-1032\n[5]\nCLP Chen，CY Zhang，L Chen，M Gan.Fuzzy\nRestricted Boltzmann Machine for the Enhancement of Deep\nLearning[J].IEEE Transactions on Fuzzy Systems, 2015,\n1(1):99-99\n[6]\nFischer, A，& Igel, C..Training restricted boltzmann\nmachines: an introduction[J].Pattern Recognition, 2014,\n47(1):25-39\n[7]\nSejnowski, Terrence J. Higher-order boltzmann\nmachines[J].Aip Conference 151 on Neural Networks for\nComputing, 1987, 151(1):398-403\n[8]\nMemisevic, R, & Hinton, G. E. .Learning to represent\nspatial transformations with factored higher-order boltzmann\nmachines [J].Neural Computation, 1989, 22(6):1473-1492\n[9]\nMemisevic, R.Learning to relate images[J].Pattern\nAnalysis & Machine Intelligence IEEE Transactions, 2013,\n35(8):1829-1846\n[10]\nMichalski, V, Memisevic, R., & Konda, K..Modeling\ndeep temporal dependencies with recurrent \"grammar\ncells[J].Advances in Neural Information Processing Systems,\n2014, 3:1925-1933\n[11]\nSutton, R，S., & Barto, A. G. .Reinforcement\nlearning : an introduction[J].Neural Networks IEEE\nTransactions, 2013, 9(5):1054-1054\n[12]\nHoward, R. Dynamic programming[J].Management\nScience, 1966, 12(5):317-348\n[13]\nPuterman, M.Markov decison processes: discrete\nstochastic dynamic programming[J].Technometrics, 1994,\n37(3):353-353\n[14]\nBertsekas, D.Dynamic Programming and Optimal\nControl[J].Athena Scientific, 2000, :-\n[15]\nSainath, T，N., Kingsbury, B., Ramabhadran, B., &\nFousek, P.Making Deep Belief Networks effective for large\nvocabulary continuous speech recognition[J].Automatic\nSpeech Recognition and Understanding, 2011, :30-35\n[16]\nLing, Z，H., Deng, L., & Yu, D. Modeling spectral\nenvelopes using restricted boltzmann machines and deep\nbelief networks for statistical parametric speech\nsynthesis[J].IEEE Transactions on Audio Speech &\nLanguage Processing, 2013, 21(10):2129-2139\n[17]\nChumerin, N. Convolutional neural network.[J].,\n2015,\n[18]\nAlpaydin, B. Introduction to machine learning[J].,\n2010, : p169\nZizhuang Wang is a Chinese high school student\nentering his 12th grade in year of 2016 . After finishing\nall school courses with a perfect GPA and won\nmultiple school scientific competitions at 11th grade,\nhe\nstarted\nto\nfocus\non\nresearches\nof\nartificial\nintelligence and mathematics. Wang’s major research\ninterest lies in machine learning, neural networks,\nbayesian inference, applied mathematics, and quantum\ncomputation. (email: 1012125144@qq.com ;)\nWeb: http://kingofspace0wzz.github.io/\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2017-06-24",
  "updated": "2017-06-24"
}