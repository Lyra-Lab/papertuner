{
  "id": "http://arxiv.org/abs/1706.08984v1",
  "title": "Unsupervised Learning via Total Correlation Explanation",
  "authors": [
    "Greg Ver Steeg"
  ],
  "abstract": "Learning by children and animals occurs effortlessly and largely without\nobvious supervision. Successes in automating supervised learning have not\ntranslated to the more ambiguous realm of unsupervised learning where goals and\nlabels are not provided. Barlow (1961) suggested that the signal that brains\nleverage for unsupervised learning is dependence, or redundancy, in the sensory\nenvironment. Dependence can be characterized using the information-theoretic\nmultivariate mutual information measure called total correlation. The principle\nof Total Cor-relation Ex-planation (CorEx) is to learn representations of data\nthat \"explain\" as much dependence in the data as possible. We review some\nmanifestations of this principle along with successes in unsupervised learning\nproblems across diverse domains including human behavior, biology, and\nlanguage.",
  "text": "Unsupervised Learning via Total Correlation Explanation\nGreg Ver Steeg\nUniversity of Southern California\nInformation Sciences Institute\ngregv@isi.edu\nAbstract\nLearning by children and animals occurs effortlessly\nand largely without obvious supervision. Successes\nin automating supervised learning have not trans-\nlated to the more ambiguous realm of unsupervised\nlearning where goals and labels are not provided.\nBarlow (1961) suggested that the signal that brains\nleverage for unsupervised learning is dependence,\nor redundancy, in the sensory environment. Depen-\ndence can be characterized using the information-\ntheoretic multivariate mutual information measure\ncalled total correlation. The principle of Total Cor-\nrelation Ex-planation (CorEx) is to learn represen-\ntations of data that “explain” as much dependence\nin the data as possible. We review some manifesta-\ntions of this principle along with successes in unsu-\npervised learning problems across diverse domains\nincluding human behavior, biology, and language.\n1\nIntroduction\nThe brain is an information-processing chunk of meat with\nsome amazing properties. Linsker, inventor of the InfoMax\nprinciple, made a statement thirty years ago that still rings true\ntoday, with some caveats.\nA young animal or child perceives and identiﬁes fea-\ntures in its environment in an apparently effortless\nway. No presently known algorithms even approach\nthis ﬂexible, general-purpose perceptual capability.\nDiscovering the principles that may underlie percep-\ntual processing is important both for neuroscience\nand for the development of synthetic perceptual sys-\ntems. [Linsker, 1988]\nPerception problems like visual object recognition that were\nonce the exclusive domain of humans are now routinely car-\nried out by computers. Surprisingly, this success did not come\nfrom profound understanding of the information processing\nprinciples in the brain, but more from brute force scaling\nof deep supervised optimization algorithms trained on large,\nlabeled image datasets [Bengio et al., 2013]. On the other\nhand, supervised deep learners are not as ﬂexible or general\npurpose as Linsker envisioned because they rely heavily on\nthe availability and quality of the training labels. Moreover,\nthe resulting representations can be brittle and hard to inter-\npret [Szegedy et al., 2014]. Reducing the reliance on labels\nwould greatly broaden the scope of these methods.\nUnfortunately, successes in supervised learning have not\ntranslated to unsupervised learning [Bengio et al., 2012]. The\ninherent ambiguity and open-ended nature of the unsupervised\nproblem makes it difﬁcult to brute force a solution [Minsky,\n1961]. Learning principles that are general and ﬂexible enough\nto apply across diverse domains, like human cognition, are\nneeded. Barlow (1961) posited that the signal that the brain\nleverages for effective learning in the absence of direct supervi-\nsion are redundancies or dependencies observed in the sensory\nenvironment. Investigating how the brain uses this redundancy\nhas motivated several inﬂuential ideas [Barlow et al., 1989;\nSimoncelli and Olshausen, 2001].\nThis paper describes an information-theoretic approach\nto formalizing Barlow’s idea called Total Cor-relation Ex-\nplanation (CorEx). CorEx constructs a hierarchy of latent\nfactors that progressively explain more dependencies in the\nobservations as measured by multivariate information, also\ncalled total correlation. “Explanation” here is meant in the\nstatistical sense that conditioned on the latent factors, Y , the\nobserved random variables X1, . . . , Xn will be statistically\nindependent. While some learning approaches will learn to\nmemorize even random, independent noise, from the CorEx\nperspective a lack of relationships in the data implies that there\nis nothing to learn. We will review the CorEx principle in the\ncontext of related information-theoretic methods then demon-\nstrate its power and versatility on a wide range of unsupervised\nlearning problems from human behavior to biology.\n2\nInformation Principles for Learning\nClaude Shannon launched the ﬁeld of information theory with\na seminal paper in 1948. Consider a random variable, X,\nthat can take values like x with probability p(X = x) (a\ncoin ﬂip, for example). Shannon deﬁned the entropy of this\nrandom variable as H(X) ≡\n\n−log p(x)\n\u000b\nwhere brackets\nindicate expectation values over random variables. Shannon\nshowed that this is the unique measure (up to scaling) that\nsatisﬁes reasonable axioms and bounds the best rate of lossless\ncompression. For a deeper intuition, see [DeDeo, 2015].\nGiven two random variables, X1 and X2, the mutual infor-\nmation is just the difference between the sum of individual\nentropies and the entropy of the variables considered jointly\narXiv:1706.08984v1  [stat.ML]  27 Jun 2017\nas a single system,\nI(X1; X2) ≡H(X1) + H(X2) −H(X1, X2).\nShannon demonstrated that this quantity bounds the number\nof messages that can be reliably sent over a noisy channel.\nThe deﬁnition of mutual information between two parties\ncan be generalized easily to n parties. We refer to this gen-\neralization as TC for total correlation following its historical\nintroduction [Watanabe, 1960].\nTC(X1, . . . , Xn) ≡\nn\nX\ni=1\nH(Xi) −H(X1, . . . , Xn)\n= DKL\n \np(x1, . . . , xn)∥\nn\nY\ni=1\np(xi)\n!\n.\nIn the following, we will often shorten X ≡X1, . . . , Xn. We\nalso wrote TC in terms of KL-divergence, DKL. TC(X) = 0\nif and only if all the Xi are independent. We can deﬁne\nconditional TC as TC(X|Y ) = DKL(p(x|y)∥Q\ni p(xi|y))\nand this quantity will be zero if and only if Xi are independent\nconditioned on Y .\nAlthough Shannon gave a general yet precise deﬁnition of\ninformation, he warned that using everyday words for termi-\nnology can be misleading [Shannon, 1956]. If we consider\nbuilding a representation of observations, X, as Y = f(X), it\nseems quite intuitive to maximize the “information” that Y has\nabout X by maximizing mutual information, I(X; Y ). Indeed,\nthis is the substance of the InfoMax principle [Linsker, 1988;\nBell and Sejnowski, 1995]. But the information is maximized\nif we simply memorize the data. Typically, InfoMax is in-\nvoked to maximize mutual information under some simplicity\nconstraints. Still, it seems counter-intuitive that adding more\nresources should degrade learning performance. This phe-\nnomenon has been observed using InfoMax for clustering:\nmore data leads to worse clustering [Ver Steeg et al., 2014].\nAn opposite point of view is taken in the information bot-\ntleneck [Tishby et al., 2000]. In that case, I(X; Y ) is actually\nminimized under the intuition that we should compress X as\nmuch as possible. A trade-off is formulated that we should\ncompress Y while maintaining information about some labels,\nZ. While this approach is natural, it requires labeled data.\nUnsupervised approaches have also been motivated from\nthe point of view that the brain has to compress information.\nIn particular, Barlow points out that in a compressed represen-\ntation neurons should ﬁre independently [Barlow et al., 1989]\n(since correlations signal redundancy). In this spirit, indepen-\ndent component analysis (ICA) seeks to minimize TC(Y ),\nwhere Y is a representation of the inputs, X [Hyv¨arinen and\nOja, 2000]. Other work suggests that efﬁcient coding of infor-\nmation in the brain should additionally require that the ﬁring\nof individual neurons be sparse [Simoncelli and Olshausen,\n2001]. While ICA has led to many useful results, the CorEx\nprinciple suggests a modiﬁcation with several beneﬁts.\nThese learning principles are summarized in Table 1. While\nthis list may look old-fashioned to some readers, these ideas\nare regularly invoked in modern papers on deep learning [Vin-\ncent et al., 2008; Dinh et al., 2014; Tishby and Zaslavsky,\n2015; Kolchinsky et al., 2017; Achille and Soatto, 2016;\nTable 1: Comparison of information principles for learning.\nMethod\nObjective\nInfoMax\nmaxY I(Y ; X)\nInfo bottleneck\nminY I(Y ; X) −βI(Y ; Z)\nICA\nminY TC(Y )\nCommon information\nminY TC(X|Y )\nCorEx (1 layer)\nminY TC(X|Y ) + TC(Y )\n(1 layer, alternate form)\nminY\nP\nj I(Yj; X) −P\ni I(Xi; Y )\nChen et al., 2016b]. For brevity, other notable ideas have\nbeen omitted in this discussion including learning based\non “coarse-graining” observations [Goerg and Shalizi, 2012;\nWolpert et al., 2014], Jaynes’ maximum entropy princi-\nple [Jaynes, 2003], integrated information theory [Oizumi et\nal., 2014], and common information [Ver Steeg et al., 2017].\n2.1\nTotal Correlation Explanation\nOne drawback of InfoMax, ICA, and others is that they are\nintrinsically shallow. For example, a transformation into inde-\npendent components using ICA does not provide intermediate\nrepresentations. Even if intermediate steps are used to get the\nindependent components, there is no reason for these terms\nto be meaningful since they are not represented in the objec-\ntive. It would be nice to have a hierarchy of abstraction where\nlower layers capture local relationships and higher layers re-\nﬂect more global relationships.\nOne Layer\nWe will begin with a shallow formulation of\nCorEx, to see how it compares to other learning principles,\nthen we will show that it has a natural hierarchical exten-\nsion. Assume that each factor, Yj, (also called neurons or\nhidden units) is a function of the data, X, Yj = fj(X). More\ngenerally, it could be drawn from a probabilistic function,\nYj ∼p(Yj|X). The Y ’s are generated according to a graph-\nical model like the left side of Fig. 1 (with r = 1 for now).\nUnder what conditions can we interpret these Y ’s as latent\nfactors that generate the data? In other words, when can we\nﬂip the arrows to get the graphical model on the right where\nthe Y ’s generate the dependence in X? The graphical model\non the right side of Fig. 1 is equivalent to a set of conditional\nindependence relationships that can be summarized by saying\nTC(X|Y ) + TC(Y ) = 0 [Ver Steeg and Galstyan, 2017]. In\nother words, each layer explains the correlations in the layer\nbelow or is independent. This is one way to write the CorEx\nobjective (for one layer).\nmin\np(yj|x) TC(X|Y ) + TC(Y )\nThe objective is non-negative and the global minimum occurs\nat zero, in which case we can ﬂip the arrows to interpret Y ’s as\ngenerating the dependence in X. Looking at Table 1, we see\nthat this optimization has an ICA term plus another term that\ndemands that Y ’s make the X’s conditionally independent.\nWith some manipulation, it can be seen that this optimization\nis also equivalent to the following optimization [Ver Steeg and\nGalstyan, 2015].\nmin\np(yj|x)\nX\nj\nI(Yj; X) −\nX\ni\nI(Xi; Y )\nX1\nX2\nX...\nXn\nY 1\nm1\nY 1\n1\nY 1\n...\nY r\n1\nY r\nmr\nk=r\nk=1\nk=0\nX1\nX2\nX...\nXn\nY 1\nm1\nY 1\n1\nY 1\n...\nY r\n1\nY r\nmr\n?\n=)\n...\n...\nFigure 1: A hierarchical representation where each layer is a proba-\nbilistic function of the layer below it. We deduce a condition under\nwhich we can ﬂip the arrows and interpret the constructed factors, Y ,\nas generating the dependence in the data.\nAgain looking at Table 1, this alternate form shows the similar-\nity to the information bottleneck. Instead of requiring labels,\nZ, we simply compress X into each latent factor Yj while\nmaintaining relevance about each of our observed variables,\nXi. From this point of view, we can view CorEx as a special\nunsupervised version of the information bottleneck.\nMultiple Layers\nNow we extend to the hierarchical case.\nWe start by building a hierarchy with r layers as on the left\nside of Fig. 1. For simplicity we deﬁne the variables at layer\nk as Y k and we deﬁne Y 0 ≡X and Y r+1 = 0 (a constant).\nNow let each layer explain dependence in the layer below.\nmin\np(yk\nj |x)\nr\nX\nk=0\nTC(Y k|Y k+1)\n(1)\nAgain, this quantity is non-negative and has a global minimum\nat zero. At the global minimum, we can ﬂip the arrows and\ninterpret the latent factors as a generative model for the depen-\ndence in X. We can again re-write the objective as a sum of\nbottleneck-like optimizations at each layer of the hierarchy.\nmax\np(yk\nj |x)\nr\nX\nk=0\n\nX\ni\nI(Y k\ni ; Y k+1) −\nX\nj\nI(Y k+1\nj\n; Y k)\n\n\n(2)\nAn advantage of writing the objective in this second form is\nthat it can be directly plugged into a useful inequality [Ver\nSteeg and Galstyan, 2015].\nTC(X) ≥\nr\nX\nk=0\n\nX\ni\nI(Y k\ni ; Y k+1) −\nX\nj\nI(Y k+1\nj\n; Y k)\n\n\n(3)\nTC(X) is the amount of dependence observed in the data. For\nhigh-dimensional systems, this is hard to estimate. However,\nthe bound in Eq. 3 allows us to solve a hierarchy of opti-\nmization problems giving progressively tighter lower bounds\non the dependence in X. Because each layer directly con-\ntributes to the lower bound on TC(X), we can quantify the\nvalue of depth and stop adding layers to the hierarchy when\nthe lower bound stops increasing. Local optima of this non-\nconvex optimization can be found using algorithms with low\ncomputational and sample complexity.\nCorEx hierarchically decomposes multivariate information\nin X in terms of contributions from latent factors at each\nlayer of a hierarchy. This decomposition can be viewed as a\ngeneralization of the hierarchical decomposition introduced\nby Watanabe [Watanabe, 1960].\nIncremental CorEx\nJust as PCA has an incremental ver-\nsion where one component is extracted at a time, the in-\ncremental version of the CorEx principle is the informa-\ntion sieve [Ver Steeg and Galstyan, 2016]. For the incre-\nmental version, we consider the special case where Y is\none-dimensional so that TC(Y ) = 0 and the objective in\nEq. 1 reduces to minY TC(X|Y ).\nAfter learning one Y\nthat optimizes the objective, we transform the data (a kind\nof information-theoretic orthogonalization) so that we can\nlearn another factor that extracts more dependence.\nThe\nsieve optimization is a dual formulation of the optimiza-\ntion deﬁning the Wyner common information [Wyner, 1975;\nOp’t Veld and Gastpar, 2016] and can be viewed as a decom-\nposition of common information [Ver Steeg et al., 2017].\n2.2\nImplementations of the CorEx Principle\nWe brieﬂy review implementations of the CorEx principle,\nincluding their applicability and limitations. Code is available\nat http://github.com/gregversteeg.\n• CorEx [Ver Steeg and Galstyan, 2014]: The original\nimplementation is restricted to discrete variables and tree\nstructured latent factors. The functionality is subsumed\nby other versions.\n• bio_corex [Ver Steeg and Galstyan, 2015; Pepke and\nVer Steeg, 2017]: The most ﬂexible version includes aug-\nmentations that were designed for challenges in the biol-\nogy domain, though there is nothing speciﬁc to biology\nin the implementation. This version handles discrete and\ncontinuous variables, overlapping latent factor structure,\nand missing data. Although CorEx has intrinsically low\nsample complexity, some biology data is severely under-\nsampled. This version implements a Bayesian smoothing\nof the marginal parameter estimates that reduces the ap-\npearance of spurious correlations [Pepke and Ver Steeg,\n2017]. This version runs quickly for problems with up to\nthousands of variables.\n• discrete_sieve [Ver Steeg and Galstyan, 2016]:\nThis is the ﬁrst implementation of the information sieve\nfor discrete variables. In the discrete case, the informa-\ntion orthogonalization required by the sieve is extremely\nchallenging. The approach is impractical for most real\nworld problems.\n• LinearSieve [Ver Steeg et al., 2017]: The linear ver-\nsion of the information sieve (for continuous variables) is\nfast and practical for ﬁnding the top components explain-\ning the most correlation in data. Like other incremental\nmethods, repeated application eventually introduces nu-\nmerical instability.\n• LinearCorEx [Ver Steeg and Galstyan, 2017]: The\nfastest, non-sparse version of CorEx assumes latent fac-\ntors are linear functions of the inputs. Linear CorEx is\nvery effective for covariance estimation and subspace\nclustering in high-dimensional, under-sampled data.\n• corex_topic [Reing et al., 2016; Gallagher et al.,\n2016]: This implementation exploits sparsity for major\nspeed-ups, easily handling hundreds of thousands of vari-\nables depending on the sparsity of the data. While this\nversion was developed for topic modeling using binary\nbag of words data, it can be applied to any binary data.\nThis implementation includes a semi-supervised option.\n3\nApplications\nThe most successful applications of the CorEx principle in-\nvolve high-dimensional data in complex domains that are dif-\nﬁcult to model a priori. In practice, the learned hierarchical\nrepresentation is used in several ways. First, the structure\ninduces a hierarchical clustering of input variables. Second,\nthe latent factors at each layer constitute a reduced dimension-\nality representation of the input data. Third, individual factors\noften disentangle true latent factors of variation in the data.\nFinally, like PCA, we can rank factors according to their value.\nWhile PCA ﬁnds components that explain the most variance,\nin CorEx we ﬁnd factors that explain the most dependence.\nSocial Science\nLatent factor models like the “Big 5” person-\nality factors are popular in social science because of their sim-\nplicity and interpretability. Given data from an online survey\nwith the Big 5 questions, CorEx was able to perfectly recon-\nstruct the Big 5 factors while standard methods failed [Ver\nSteeg and Galstyan, 2014]. CorEx outperforms traditional\nfactor methods when the number of variables is larger than the\nnumber of samples [Ver Steeg et al., 2017], as is increasingly\ncommon in social science experiments.\nGene Expression\nIf DNA is the software for our biology,\nthen gene expression tells us which code is running. New\nsequencing methods allow us to read expression levels for\nthousands of genes but, unfortunately, only a tiny fraction\nof these processes are understood. Applying CorEx to gene\nexpression reveals a rich array of strong biological signals.\nGene clusters discovered by CorEx exhibit four times more\nenrichment with respect to known biology in gene ontology\ndatabases than standard approaches [Pepke and Ver Steeg,\n2017]. The CorEx hierarchy also seems to accurately reﬂect\nbiological organization. For instance, two low-level clusters,\nimmune cell activation and inﬂammatory response, are com-\nbined in a higher-level group related to immune signaling.\nNeuroscience\nNext generation imaging technology gives\ndetailed views of individual brains but even the largest studies\ncan only afford to image a relatively small number of brains.\nSmall sample sizes with high-dimensional data has led to low\nstatistical power in most studies and a crisis of replication\nin the ﬁeld [Button et al., 2013]. Exploratory data analysis\nusing CorEx can help identify only the strongest, most robust\ndependencies in the data, even with small sample sizes. Early\nresults have identiﬁed well-known relationships along with\nnovel, biologically plausible candidate effects that increase\npredictive power [Madsen et al., 2015; Madsen et al., 2016;\nZavaliangos-Petropulu et al., 2017]. Moreover, CorEx out-\nperforms ICA for disentangling spatial modes in imaging\ndata [Ver Steeg et al., 2017].\nLanguage Applied to bag of words vectors of text data,\nCorEx representations can be interpreted as hierarchical\ntopic models [Ver Steeg and Galstyan, 2014; Hodas et al.,\n2015]. Latent tree based methods like CorEx outperform\nLDA [Chen et al., 2016a] on several measures of topic qual-\nity. A semi-supervised version further increases the inter-\npretability of topics by allowing us to “anchor” some latent\nfactors to designated words of interest [Reing et al., 2016;\nGallagher et al., 2016].\nFinance\nThe stock market exhibits a high degree of depen-\ndence and measuring this dependence is important for quanti-\nfying risk. Applied to monthly returns on the S&P 500, CorEx\ndiscovers a hierarchy of latent factors related to industry sec-\ntors [Ver Steeg and Galstyan, 2015]. Using a linear version\nof CorEx allows us to estimate covariance matrices and this\napproach outperforms state-of-the-art methods like GLASSO\nfor under-sampled, high-dimensional stock market data [Ver\nSteeg and Galstyan, 2017].\n4\nConclusion\nLearning useful representations of observations without su-\npervision across diverse domains is a challenging, unsolved\nproblem. While successes in supervised learning have not\nimmediately translated to comparable results for unsuper-\nvised learning, the engineering achievements that have driven\nthose successes now allow us to deﬁne ﬂexible, power-\nful architectures and quickly and easily optimize them un-\nder a wide variety objectives [TensorFlow, 2015]. Taking\ninformation-theoretic learning principles and applying them\nwithin these powerful frameworks has already generated many\ncompelling results [Chen et al., 2016b; Kolchinsky et al., 2017;\nAchille and Soatto, 2016]. Applying the CorEx principle in\nthe same context is a logical step for future work.\nUnlike similar learning principles, CorEx naturally decom-\nposes information in a hierarchical way. This hierarchical\nstructure has numerical advantages since the objective is less\nreliant on back-propagation for training. Credit for predicting\na correct label does not have to be assigned to intermediate\nrepresentations as in supervised learning [Minsky, 1961], in-\nstead each latent factor in the representation has a quantiﬁable\ninformation value within the hierarchical decomposition. Be-\nsides the technical advantages, many applications show that\nindividual latent factors learned via CorEx reﬂect diverse and\nmeaningful structure in real-world datasets. We hope that\nongoing CorEx developments continue to accelerate the dis-\ncovery of knowledge through unsupervised learning.\nReferences\n[Achille and Soatto, 2016] Alessandro Achille and Stefano Soatto.\nInformation dropout: Learning optimal representations through\nnoisy computation. ArXiv e-prints, 2016.\n[Barlow et al., 1989] Horace B Barlow, Tej P Kaushal, and Graeme J\nMitchison. Finding minimum entropy codes. Neural Computation,\n1(3):412–423, 1989.\n[Bell and Sejnowski, 1995] Anthony J Bell and Terrence J Se-\njnowski. An information-maximization approach to blind sep-\naration and blind deconvolution. Neural computation, 7(6):1129–\n1159, 1995.\n[Bengio et al., 2012] Yoshua Bengio, Aaron C Courville, and Pascal\nVincent. Unsupervised feature learning and deep learning: A\nreview and new perspectives. CoRR, abs/1206.5538, 1, 2012.\n[Bengio et al., 2013] Yoshua Bengio, Aaron Courville, and Pascal\nVincent. Representation learning: A review and new perspec-\ntives. Pattern Analysis and Machine Intelligence, 35(8):1798–\n1828, 2013.\n[Button et al., 2013] Katherine S Button, John PA Ioannidis, Claire\nMokrysz, Brian A Nosek, Jonathan Flint, Emma SJ Robinson,\nand Marcus R Munaf`o. Power failure: why small sample size\nundermines the reliability of neuroscience. Nature Reviews Neu-\nroscience, 14(5):365–376, 2013.\n[Chen et al., 2016a] Peixian Chen, Nevin L Zhang, Leonard KM\nPoon, and Zhourong Chen. Progressive em for latent tree models\nand hierarchical topic detection. In Thirtieth AAAI Conference on\nArtiﬁcial Intelligence, 2016.\n[Chen et al., 2016b] Xi Chen, Yan Duan, Rein Houthooft, John\nSchulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Inter-\npretable representation learning by information maximizing gen-\nerative adversarial nets. In Advances In Neural Information Pro-\ncessing Systems 29 (NIPS), pages 2172–2180, 2016.\n[DeDeo, 2015] Simon DeDeo. Information theory for intelligent\npeople, 2015.\n[Dinh et al., 2014] Laurent Dinh, David Krueger, and Yoshua Ben-\ngio. Nice: Non-linear independent components estimation. arXiv\npreprint arXiv:1410.8516, 2014.\n[Gallagher et al., 2016] R.J. Gallagher, K. Reing, D. Kale, and\nG. Ver Steeg.\nAnchored correlation explanation:\nTopic\nmodeling with minimal domain knowledge.\narXiv preprint\narXiv:1611.10277, 2016.\n[Goerg and Shalizi, 2012] G.M. Goerg and C.R. Shalizi. Licors:\nLight cone reconstruction of states for non-parametric forecasting\nof spatio-temporal systems. arXiv:1206.2398, 2012.\n[Hodas et al., 2015] N. Hodas, G. Ver Steeg, J. Harrison, S. Chikk-\nagoudar, E. Bell, and C. Corley. Disentangling the lexicons of\ndisaster response in twitter. In The 3rd International Workshop on\nSocial Web for Disaster Management (SWDM’15), 2015.\n[Hyv¨arinen and Oja, 2000] A. Hyv¨arinen and E. Oja. Independent\ncomponent analysis: algorithms and applications. Neural net-\nworks, 13(4):411–430, 2000.\n[Jaynes, 2003] Edwin T Jaynes. Probability theory: the logic of\nscience. Cambridge university press, 2003.\n[Kolchinsky et al., 2017] Artemy Kolchinsky, Brendan D Tracey,\nand David H Wolpert. Nonlinear information bottleneck. arXiv\npreprint arXiv:1705.02436, 2017.\n[Linsker, 1988] Ralph Linsker. Self-organization in a perceptual\nnetwork. Computer, 21(3):105–117, 1988.\n[Madsen et al., 2015] Sarah K. Madsen, Greg Ver Steeg, Adam\nMezher, Neda Jahanshad, Talia M. Nir, Xue Hua, Boris A. Gut-\nman, Aram Galstyan, and Paul M. Thompson.\nInformation-\ntheoretic characterization of blood panel predictors for brain at-\nrophy and cognitive decline in the elderly. IEEE International\nSymposium on Biomedical Imaging, 2015.\n[Madsen et al., 2016] Sarah K. Madsen, Greg Ver Steeg, Madelaine\nDaianu, Adam Mezher, Neda Jahanshad, Talia M. Nir, Xue Hua,\nBoris A. Gutman, Aram Galstyan, and Paul M. Thompson. Rel-\native value of diverse brain mri and blood-based measures for\npredicting cognitive decline in the elderly. SPIE Medical Imaging,\n2016.\n[Minsky, 1961] Marvin Minsky. Steps toward artiﬁcial intelligence.\nProceedings of the IRE, 49(1):8–30, 1961.\n[Oizumi et al., 2014] Masafumi Oizumi, Larissa Albantakis, and\nGiulio Tononi. From the phenomenology to the mechanisms of\nconsciousness: integrated information theory 3.0. 2014.\n[Op’t Veld and Gastpar, 2016] Giel Op’t Veld and Michael C Gast-\npar. Caching gaussians: Minimizing total correlation on the gray–\nwyner network. In 50th Annual Conference on Information Sys-\ntems and Sciences (CISS), 2016.\n[Pepke and Ver Steeg, 2017] Shirley Pepke and Greg Ver Steeg.\nComprehensive discovery of subsample gene expression com-\nponents by information explanation: therapeutic implications in\ncancer. BMC medical genomics, 10(1):12, 2017.\n[Reing et al., 2016] Kyle Reing, David C Kale, Greg Ver Steeg, and\nAram Galstyan. Toward interpretable topic discovery via anchored\ncorrelation explanation. In Proceedings of the 2016 ICML Work-\nshop on Human Interpretability in Machine Learning (WHI 2016),\nnumber arXiv:1606.07043, 2016.\n[Shannon, 1948] C.E. Shannon. A mathematical theory of commu-\nnication. The Bell System Technical Journal, 27:379–423, 1948.\n[Shannon, 1956] Claude Shannon. The bandwagon (edtl.). IRE\nTransactions on Information Theory, 1(2):3, 1956.\n[Simoncelli and Olshausen, 2001] Eero Simoncelli and Bruno Ol-\nshausen. Natural image statistics and neural representation. Annu.\nRev. Neurosci., 24, 2001.\n[Szegedy et al., 2014] C. Szegedy, W. Zaremba, I. Sutskever,\nJ. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing\nproperties of neural networks. In ICLR, 2014.\n[TensorFlow, 2015] TensorFlow. Large-scale machine learning on\nheterogeneous systems, 2015. Software available from tensor-\nﬂow.org.\n[Tishby and Zaslavsky, 2015] Naftali Tishby and Noga Zaslavsky.\nDeep learning and the information bottleneck principle. In Infor-\nmation Theory Workshop (ITW), 2015 IEEE. IEEE, 2015.\n[Tishby et al., 2000] Naftali Tishby, Fernando C Pereira, and\nWilliam\nBialek.\nThe\ninformation\nbottleneck\nmethod.\narXiv:physics/0004057, 2000.\n[Ver Steeg and Galstyan, 2014] G. Ver Steeg and A. Galstyan. Dis-\ncovering structure in high-dimensional data through correlation\nexplanation. In Advances in Neural Information Processing Sys-\ntems (NIPS), 2014.\n[Ver Steeg and Galstyan, 2015] Greg Ver Steeg and Aram Galstyan.\nMaximally informative hierarchical representations of high-\ndimensional data. In Proceedings of the Sixteenth International\nConference on Artiﬁcial Intelligence and Statistics (AISTATS),\n2015. http://arxiv.org/abs/1410.7404.\n[Ver Steeg and Galstyan, 2016] Greg Ver Steeg and Aram Galstyan.\nThe information sieve. In International Conference on Machine\nLearning (ICML), 2016.\n[Ver Steeg and Galstyan, 2017] Greg Ver Steeg and Aram Galstyan.\nLow complexity gaussian latent factor models and a blessing of\ndimensionality. arXiv:1706.03353 [stat.ML], 2017.\n[Ver Steeg et al., 2014] Greg Ver Steeg, Aram Galstyan, Fei Sha,\nand Simon DeDeo. Demystifying information-theoretic clustering.\nIn International Conference on Machine Learning, 2014.\n[Ver Steeg et al., 2017] Greg Ver Steeg, Shuyang Gao, Kyle Reing,\nand Aram Galstyan. Sifting common information from many\nvariables. IJCAI, 2017.\n[Vincent et al., 2008] Pascal Vincent, Hugo Larochelle, Yoshua Ben-\ngio, and Pierre-Antoine Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In Proceedings of\nthe 25th international conference on Machine learning, pages\n1096–1103. ACM, 2008.\n[Watanabe, 1960] Satosi Watanabe. Information theoretical anal-\nysis of multivariate correlation. IBM Journal of research and\ndevelopment, 4(1):66–82, 1960.\n[Wolpert et al., 2014] David H Wolpert, Joshua A Grochow, Eric\nLibby, and Simon DeDeo. A framework for optimal high-level de-\nscriptions in science and engineering—preliminary report. arXiv\npreprint arXiv:1409.7403, 2014.\n[Wyner, 1975] Aaron D Wyner. The common information of two\ndependent random variables. Information Theory, IEEE Transac-\ntions on, 21(2):163–179, 1975.\n[Zavaliangos-Petropulu et al., 2017] Artemis\nZavaliangos-\nPetropulu, Emily L Dennis, Greg Ver Steeg, Talin Babikian,\nRichard Mink, Christopher Babbitt, Jeffrey Johnson, Christo-\npher C Giza, Robert F Asarnow, and Paul M Thompson. Variable\nclustering reveals associations between subcortical brain volume\nand cognitive changes in pediatric traumatic brain injury. In 12th\nInternational Symposium on Medical Information Processing and\nAnalysis. International Society for Optics and Photonics, 2017.\n",
  "categories": [
    "stat.ML"
  ],
  "published": "2017-06-27",
  "updated": "2017-06-27"
}