{
  "id": "http://arxiv.org/abs/1706.09529v1",
  "title": "Learning to Learn: Meta-Critic Networks for Sample Efficient Learning",
  "authors": [
    "Flood Sung",
    "Li Zhang",
    "Tao Xiang",
    "Timothy Hospedales",
    "Yongxin Yang"
  ],
  "abstract": "We propose a novel and flexible approach to meta-learning for\nlearning-to-learn from only a few examples. Our framework is motivated by\nactor-critic reinforcement learning, but can be applied to both reinforcement\nand supervised learning. The key idea is to learn a meta-critic: an\naction-value function neural network that learns to criticise any actor trying\nto solve any specified task. For supervised learning, this corresponds to the\nnovel idea of a trainable task-parametrised loss generator. This meta-critic\napproach provides a route to knowledge transfer that can flexibly deal with\nfew-shot and semi-supervised conditions for both reinforcement and supervised\nlearning. Promising results are shown on both reinforcement and supervised\nlearning problems.",
  "text": "Learning to Learn: Meta-Critic Networks\nfor Sample Eﬃcient Learning\nFlood Sung\nIndependent Researcher\nfloodsung@gmail.com\nLi Zhang\nQueen Mary University of London\ndavid.lizhang@qmul.ac.uk\nTao Xiang\nQueen Mary University of London\nt.xiang@qmul.ac.uk\nTimothy Hospedales\nThe University of Edinburgh\nt.hospedales@ed.ac.uk\nYongxin Yang\nYang’s Accounting Consultancy Ltd\nyongxin@yang.ac\nAbstract\nWe propose a novel and ﬂexible approach to meta-learning for learning-to-learn\nfrom only a few examples. Our framework is motivated by actor-critic reinforcement\nlearning, but can be applied to both reinforcement and supervised learning. The key\nidea is to learn a meta-critic: an action-value function neural network that learns to\ncriticise any actor trying to solve any speciﬁed task.\nFor supervised learning, this\ncorresponds to the novel idea of a trainable task-parametrised loss generator. This\nmeta-critic approach provides a route to knowledge transfer that can ﬂexibly deal\nwith few-shot and semi-supervised conditions for both reinforcement and supervised\nlearning. Promising results are shown on both reinforcement and supervised learning\nproblems.\n1\nIntroduction\nLearning eﬀectively from few examples is a key capability of humans that machine learning\nhas yet to replicate, despite a long history of research [35, 25, 30] and explosive recent\ninterest [9, 27, 22, 4]. Like humans, we would like our AI systems to be able to learn new\nskills quickly: after only a few supervisions (supervised learning), or only a few interactions\nwith the environment (reinforcement learning). To this end, the learning-to-learn vision is\nof learners that address multiple learning problems, and eventually understand something\nabout how problems relate, so that problem-independent information can be extracted and\napplied to help learn new tasks more quickly and accurately.\nMotivated by this meta-learnning vision – and under various alternative names, including\nlife-long learning, learning to learn, etc – the community has studied various categories of\napproaches, including learning shared transferrable priors or weight initializations [9, 26],\nmeta-models that generate the parameters of few-shot models [37, 14], and learning eﬀective\ntransferrable optimizers [25, 22].\nIn this paper we propose a novel approach to meta-learning that is instead based on the\ncompletely diﬀerent perspective of learning a global ‘meta-critic’ network that can be used\nto train multiple ‘actor’ networks to solve speciﬁc problems.\nThis approach is inspired\nby actor-critic networks in reinforcement learning (RL), where an eﬀective strategy is to\njointly train a pair of networks for any given problem such that the actor learns to solve\nthe problem, and the critic learns how to eﬀectively supervise the actor [3, 13]. In RL our\n1\narXiv:1706.09529v1  [cs.LG]  29 Jun 2017\nshared meta-critic provides the transferrable knowledge that allows actors to be trained with\nonly a few trials on a new problem. We show that in supervised learning (SL) the same\nstrategy corresponds to the idea of using a trainable and task-parameterised loss function,\nin contrast to typical ﬁxed losses such as mean square error or cross-entropy.\nConventional critic networks [3, 13] are conditioned on a given task and actor that they\nare trained to criticise. To achieve our vision, we explicitly condition the meta-critic on an\nactor and a task, so that at any moment it knows what student (actor) it is training, what\nproblem (task) the actor should be trained to solve. To achieve this explicit conditioning, we\nintroduce the idea of a task-actor encoder network. This encoder reads a description of the\ncurrent task and state of the actor learner on that task. Based on this it produces a task-\nactor embedding that the critic uses as input to decide how to criticise the current actor on\nthe current task. For input to the encoder, we use an architecture agnostic featurisation of\nactor and task: the N-shots of inputs, labels and predictions in the case of SL; and N-trials\nof states, actions, and rewards in the case of RL. Fig. 1 provides a schematic illustration\nof the framework. For notational simplicity we will use RL terminology and refer to the\ntask-speciﬁc networks as actors and supervisory networks as critics for both RL and SL\nproblems.\nOur problem statement is to take a set of tasks T = {τi} (each deﬁned by feature vectors\nand labels in the case of SL, or POMDPs and reward functions in the case of RL) plus a\nnew target task τ ∗with few examples (SL) or allowed environmental interactions (RL) and\neﬃciently learn a predictor/policy (SL/RL) for the target task τ ∗by leveraging the back-\nground tasks T . In contrast to other meta-learning studies [9, 27] addressing this problem,\nwe explore an alternative solution of knowledge transfer by learning a task-independent\nmeta-critic from the source tasks that can be directly applied to training an actor for a\ntarget task.\nTo understand why the meta-critic approach is eﬀective in RL, consider that if the meta-\ncritic can correctly criticise a new task based on the provided task-encoding, then from\nthe perspective of the new task’s actor, it beneﬁts from a pre-trained critic which increases\nlearning speed. Moreover, as the meta-critic is actor-conditional, it never gets ‘out of date’\nand needing to ‘catch-up’ with its actor, as can happen during actor critic co-evolution in\nconventional actor-critic architectures. The approach has a number of further beneﬁts: It\ncan address both SL and RL with a single framework; and both continuous and discrete\noutputs (classiﬁcation/regression and discrete/continuous action RL). Unlike other studies,\nit does not make assumptions about base learner architecture like recurrent [28] or Siamese\n[18], or ﬁx the number of shots used [4]. The ability to assume the existence of a good\npre-trained critic for any target task also means that the actor can beneﬁt from training\non unlabelled data (i.e., exploiting semi-supervised learning, or environmental interactions\nwithout access to a reward function). In each case the actor can beneﬁt from the critic’s\nsupervision of what it should do in those unlabelled states.\nFinally, with a suﬃciently\npowerful task-encoder and meta-critic, it also means that the framework is robust to diverse\nrelatedness among the tasks – a situation which existing approaches that search for a single\nshared prior [9] are not robust to.\n2\nRelated Work\nFew-shot learning\nThere is extensive work on meta-learning to enable few-shot super-\nvised [37, 27, 18, 20, 4] and reinforcement [8, 34, 39, 30] learning. Only a few studies provide\nframeworks that do not require speciﬁc model architectures and that address both settings.\nThe most related to ours in terms of architecture agnostic few-shot learning in both SL\nand RL is [9]. However, the methodologies are completely diﬀerent: [9] aims to achieve\nfew-shot learning by learning a shared initialisation from which any individual speciﬁc task\ncan easily be ﬁne-tuned. As we will show later, this approach is weak to diversity among\nthe tasks. When learning neural networks with SGD, such a single shared initialisation\n2\nActor 1\nActor 2\nActor i\nTask /\nEnv. 1\nTask /\nEnv. 2\nTask /\nEnv. i\ns,r\na\nMeta-Critic\nMeta-\nValue Net\n(MVN)\nTask-Actor \nEncoder\n(TAEN)\n(s,a)\nQ(s,a,z)\n(s,a,r)t-1, \n…,\n(s,a,r)t-K\nz\nPolicy-Gradient\nTraining\nTask-Actor\nEmbedding\nFigure 1: Schematic illustration of our meta-critic based approach. Meta-training: actors and\nmeta-critic are trained. Meta-testing: Meta-critic is ﬁxed and used to train actor for a novel task.\nroughly corresponds to a shared prior representing the ‘average’ source task. The weights\nrepresenting the average of previous tasks can be particularly bad when the distribution of\ntasks is multi-modal. Our meta-critic approach is more robust to such task diversity.\nTask-parametrised Networks\nOur meta-critic is task-agnostic due to being parametrised\nby the task (and actor) to criticise. This is related to a number of topics. Parametrised\nTasks: In learning for control, the contextual policy or parametrised task setting considers\nactors that are parametrised by some metadata describing the goal they should achieve (e.g.,\nthe goal state of a mobile agent or robot arm to reach) [7, 19, 29]. In our case it is the\ncritic that is task-parametrised. And rather than parameterising in terms of a high-level\ngoal, it is parameterised by encoding the experiences of the actor. This approach simulta-\nneously encodes both the task (by the rewards) and the actor (via the actions chosen in\neach state), so the critic can learn to criticise any actor for any task. Weight Synthesis\nNetworks: Another group of approaches focus on designing a meta-network that generates\nthe weights of a task-speciﬁc network based on some encoding of that task [37, 14, 21, 4], so\nthat the task-agnostic knowledge is encoded in the weights of the meta-network. In our case\nthe task-agnostic knowledge is stored in the task-encoder and critic networks, and we use\nthe task-encoding to drive the supervision of the target network via the critic, rather than\nsynthesise its weights. Our ‘learning to supervise’ rather than this ‘learning to synthesise’\napproach has several advantages: it allows application to RL, which is not non-trivial for\nexisting weight-synthesis methods oriented at SL; it allows learning from unlabelled data /\nunrewarded interactions with the environment [10]; and it does not constrain the meta and\ntarget networks to have matching architectures.\nDistillation\nOur approach is somewhat related to knowledge distillation [6, 15] – the\nsupervision of a student network by a teacher network. The meta-critic shares distillation’s\nfavourable ability to use unlabelled data to train the student actor. In contrast to regular\ndistillation where teacher and student are both single-task actors, our teacher is a meta-\ncritic shared across all tasks. Related applications of distillation include actor-mimic [26]\nwhere multiple expert actor networks teach a single student actor how to solve multiple\ntasks. This is the opposite of our single teacher critic teaching multiple actor networks to\nsolve diﬀerent tasks (and thus providing knowledge transfer).\nMulti-Actor Methods\nMultiple actors with a centralised critic has been used in multi-\nagent learning context for decentralised training of centralised policies [11].\nWhile A3C\n[24] uses multiple actors to generate diverse uncorrelated samples of the environment to\nimprove learning speed. These studies address actors learning to solve the same task inde-\npendently or cooperatively, rather than our actors addressing distinct tasks, and meta-critic\nthat generalises across multiple tasks.\n3\n3\nMethodology\nWe ﬁrst introduce our methodology from a continuous reinforcement learning perspective,\nand later explain the discrete action (Section 3.1) and supervised learning (Section 3.2)\nvariants.\nBackground\nIn the RL problem, at each time t, an agent receives a state st, based on\nwhich it takes an action at, after that the agent will observe a new state st+1 and reward\nrt, according to the dynamics of the environment. We focus on model-free reinforcement\nlearning, so the agent does assume a model of the transition: (st, at) →st+1, nor reward:\n(st, at, st+1) →rt.\nOur proposed method is based on the actor-critic architecture [3, 13], and we use neu-\nral networks as function approximators for both actor (policy network) and critic (value\nnetwork).\nIn continuous-action RL, the actor produces the actual action and is trained by maximising\nthe output of critic (i.e., deterministic policy [32, 23]). The critic is the state-action value\nfunction (Q-function [38]) and is trained to estimate the future return by minimising the\ntemporal diﬀerence error1.\nThe policy network (actor) is parametrised by θ and it takes as input the state st, then\noutputs an action at, i.e., at = Pθ(st).\nA value network is parametrised by φ and it\ntakes as input the state st and action at, then outputs an expected discounted reward, i.e.,\nQPθ\nφ (st, at) = rt + γQPθ\nφ (st+1, at+1). Where the notation QPθ indicates the critic is trained\nto model the return of policy Pθ.\nThe objective of the value network is to predict expected discounted reward as accurately as\npossible, and the objective of the policy network is to maximise the discounted future reward,\nassumed to be the return estimated by the value network.\nTherefore the optimisation\nprocedure is to alternatively update policy network and value network:\nθ ←argmax\nθ\nQPθ\nφ (st, at)\n(1)\nφ ←argmin\nφ\n\u0000QPθ\nφ (st, at) −rt −γQPθ\nφ (st+1, at+1)\n\u00012\n(2)\nwhere at = Pθ(st) and at+1 = Pθ(st+1).\nA Meta-Critic and Task-Actor Encoder for Multiple Tasks\nOur vision is of a\nsingle meta-critic that can criticise any actor, performing any task. This requires two gen-\neralisations (task and actor conditioning) compared to conventional critic networks that\ncriticise a speciﬁc actor for a speciﬁc task. In order for the value network to issue correct\ntask- and actor-speciﬁc supervision, we need an additional input beyond the conventional\nstate-action pair. We encode the task and the actor via a task-actor embedding zt. To\ngenerate zt, we propose to use a task-actor encoder network (TAEN) Cω parametrised\nby ω. The task network inputs a featurisation of the actor and the task that it is sup-\nposed to be solving and produces the task embedding zt. There are various possible fea-\nturisations (e.g., the actor’s weights), but we take a simple featurisation that makes no\nassumption on the form of the actor. Speciﬁcally, we deﬁne the task-actor encoder as a\nrecurrent neural network (RNN) that inputs a learning trace as a sequence of k triplets\nLt\nt−k = [(st−k, at−k, rt−k), (st−k+1, at−k+1, rt−k+1), . . . , (st−1, at−1, rt−1)] and outputs the\ntask-actor encoding zt = Cω(Lt\nt−k). To understand this encoding, observe that by including\nstate-action pairs the encoder sees a featurisation of the actor it is to criticise (choice of\naction depends on actor parameters). Meanwhile by observing the rewards obtained, it sees\na featurisation of the task the actor is solving. The meta-critic is thus composed of the\nmeta-value network (MVN) and task-action encoder network (TAEN), and shared across all\n1Note that we choose deterministic policy and Q-function here for illustration purpose only, and we do\nnot make assumptions on how actor and critic are designed except they are both realised by neural networks.\nE.g., some alternative choices: actor can produce a Gaussian distribution over actions (i.e., Gaussian policy)\nand be trained by actor-critic policy gradient [33] with either Q-function or advantage function [17, 24, 31].\n4\ntasks and actors. Assuming M training tasks, the update rules for every individual actor i\nand meta-critic are:\nθ(i) ←argmax\nθ(i)\nQφ(s(i)\nt , a(i)\nt , z(i)\nt )\n∀i ∈[1, 2, . . . , M]\n(3)\nφ, ω ←argmin\nφ,ω\nPM\ni=1\n\u0000Qφ(s(i)\nt , a(i)\nt , z(i)\nt ) −r(i)\nt\n−γQφ(s(i)\nt+1, a(i)\nt+1, z(i)\nt+1)\n\u00012\n(4)\nwhere z(i)\nt\n= Cω(L(i)t\nt−k) and a(i)\nt\n= Pθi(si\nt), and we drop the superscript Pθ of Q to indicate\nthat Q is now parametrised by Pθ rather than trained to ﬁt a speciﬁc Pθ. When optimising\nθ, the gradient due to zt is ignored, i.e., we see zt as a constant rather than a function\ninvolving θ [2].\n3.1\nDiscrete-action RL\nFor discrete-action RL, we choose to use actor-critic policy gradient. The policy network Pθ\noutputs a categorical distribution ot over available actions. Actions are then sampled from\nthe produced distribution at ∼Pθ(st) , where at is the one-hot encoding vector indicating\nthe chosen action. The value network is optimised as in Eq. 2, and the policy network is\noptimised by\nθ ←argmin\nθ\nℓ(ot, at)QPθ\nφ (st, at),\n(5)\nwhere ot = Pθ(st) and ℓ(·, ·) is the cross entropy loss function. The extension from a single\nRL task to multiple ones is similar to the case of continuous-action RL (Eq. 3 and Eq. 4)\nexcept that the policy network update (Eq. 3) is modiﬁed according to Eq. 5, leading to\nθ(i) ←argmin\nθ\nℓ(o(i)\nt , a(i)\nt )Qφ(s(i)\nt , a(i)\nt , z(i)\nt )\n∀i ∈[1, 2, . . . , M]\n(6)\n3.2\nSupervised Learning\nAn Actor-Critic Reframing of Supervised Learning\nTo apply our framework to su-\npervised learning, we imagine a one-step only game, where the actor (function approximator\nto learn) takes as input a feature x, and outputs the estimation of target variable ˆy. Then\nthe environment returns the reward calculated from the negative loss value, i.e., r = −ℓ(ˆy, y)\nand the game terminates. Diﬀerently to conventional supervised learning, in this case we\nassume the ground truth y is not exposed to the agent directly, and the agent has no access\nto the form of loss function. This makes it more like a zero-order (black-box) optimisation\nproblem rather than conventional supervised learning problem.\nThe objective of critic is now to predict the current reward since the game has one step\nonly.\nθ ←argmax\nθ\nQφ(x, ˆy)\n(7)\nφ ←argmin\nφ\n\u0000Qφ(x, ˆy) −r)2\n(8)\nwhere ˆy = Pθ(x) and r = −ℓ(ˆy, y). We emphasise that though the reward is calculated by\n−ℓ(ˆy, y), this calculation is hidden in the environment. Thus we can see the correspondence\nto the previous RL problem: (i) state s →x, (ii) action a →ˆy, and (iii) reward r →−ℓ(ˆy, y).\nMeta-Critic-based Supervised Learning\nWith the above actor-critic interpretation,\nextending single-task SL problem to multiple tasks is similar to the case of continuous-action\nRL (Eq. 3 and Eq. 4) except the actor-critic updates are changed according to Eq. 7 and\nEq. 8:\nθ(i) ←argmax\nθ(i)\nQφ(x(i), ˆy(i), z(i))\n∀i ∈[1, 2, . . . , M]\n(9)\nφ, ω ←argmin\nφ,ω\nPM\ni=1\n\u0000Qφ(x(i), ˆy(i), z(i)) −r(i)\nt\n\u00012\n(10)\n5\nHere we can see that the function approximator (actor) is learning to maximize the negative\nsupervised learning loss, as estimated by the meta-critic, rather than minimise a ﬁxed loss\nfunction as in regular SL. Meanwhile the meta-critic learns to simulate the actual supervised\nlearning loss of each problem i = 1 . . . M. For a single-task, this indirection just makes\nlearning more ineﬃcient. But crucially for multiple tasks, it means the loss generator (meta-\ncritic) beneﬁts from cross-task knowledge sharing. And because the tasks are parametrised,\nit means the loss generator can immediately supervise a new task ‘out of the box’ based on\nits parametrisation, without needing much data.\nTask Parametrisation\nAs we are focused on rapid few-shot learning, we describe tasks by\nway of few-shot examples similarly to [4, 37]. Other parameterisations such as word vectors\nused in zero-shot learning are also possible [21, 12]. Speciﬁcally, we stick with an RNN-\nbased task-encoder C, that encodes the few-shot examples into a ﬁxed length embedding\ndescribing the task. Analogous to our [s, a, r] learning trace-based task description in RL,\nwe would use the mirror [x, ˆy, −ℓ(y, ˆy)] for SL. However, we ﬁnd that [x, y, 0] works slightly\nbetter in practice. I.e., we assume, for the purpose of constructing the input for TAEN,\nthat the actor is able to produce the perfect estimation.\n3.2.1\nSeamless Integration with Semi-Supervised Learning\nIn addition to few-shot training data of [x, y, l(y, ˆy)] or [s, a, r] tuples, it is often the case\nthat we have access to a larger set of unlabelled data: [x] in in the case of SL, or [s, a]\ntuples in the case of RL. This arises when examples are plentiful but costly to annotate\nin SL, or when environmental interactions are cheap but the reward function is costly to\nprovide in RL [10]. Crucially, a unique advantage of our approach is that the supervisory\nsignal generated by the meta-critic does not need the ground truth y after training. Similar\nto other meta-learning experimental designs, we assume a meta-training stage on a set of\nbackground tasks, and then a meta-testing stage where we should learn a novel task with\nfew trials. In meta-learning stage both meta-critic and actors are trained. In meta-testing\nour meta-critic is ﬁxed and only the new task’s actor is trained. The few examples of the\nmeta-testing task are used to tell the meta-critic how to supervise via the TAEN. But the\nactor’s training can include unlabelled data from the testing task and receive supervision\nabout those from the meta-critic instead of actual (unavailable) environmental rewards (RL)\nor instance labels (SL).\n4\nExperiments\n4.1\nImplementation and Details\nApplying meta-learning with a focus on few-shot learning proceeds in three stages: (i) Meta-\ntraining on multiple source tasks; (ii) Meta-testing to learn a new task with few training\nsamples (or environmental interactions); (iii) Final-Testing the out-of-sample performance\non the new task.\nIn meta-training (i) we train multiple actors for each task along with the meta-critic to\nsupervise the actors. This process is summarised in Algorithm 1. Here the scheduling of\nalternating actor-critic updates is determined by the task mini-batch size. Two extremes\nare: (i) when the mini-batch size is 1, the critic (both MVN and TAEN) is updated after\nany actor is updated; (ii) when the mini-batch size is M, the critic is updated after all actors\nare updated. In meta-testing (ii) we train an actor for a new held out task. The parameters\nof the meta-critic and task-encoder are ﬁxed. The few-shot data for the new-task provides\nthe task-actor description via the TAEN, and (possibly along with unlabelled data) provides\nthe input to the MVN, from which the meta-critic provides a supervisory signal for training\nthe new actor on the new task. The process is summarised in Algorithm 2.\nFor all experiments, we use the following neural networks architecture: (i) actor is an MLP\nwith 2 hidden layers (ReLU), and the number of neurons for both hidden layers is 40. (ii)\n6\nTask-action encoder network (TAEN) is a one-layer LSTM [16] with 30 cell units, and the\nlast hidden state is connected with a dense layer to 3 output neurons.\n(iii) Meta-value\nnetwork (MVN) is an MLP with 2 hidden layers (ReLU), and the number of neurons for\nboth hidden layers is 80. The input for the MVN is of the length = state size+action size+\nTAEN output size(3).\nAlgorithm 1: Meta-Learning Stage\nInput: Task generator T\nOutput: Trained task and value net\n1 Init: task and value net;\n2 for episode = 1 to max episode do\n3\nGenerate M tasks from T ;\n4\nInit M policy nets (actors);\n5\nfor step = 1 to max steps do\n6\nSample mini-batch of tasks;\n7\nforeach task in mini-batch do\n8\nSample training data from task;\n9\nTrain task-speciﬁc actor;\n10\nend\n11\nTrain value network;\n12\nTrain task network;\n13\nend\n14 end\nAlgorithm 2: Meta-Testing Stage\nInput: An unseen task\nInput: Trained task and value nets\nOutput: Trained policy network\n1 Init: one policy network (actor);\n2 for step = 1 to max step do\n3\nSample train data from task;\n4\nTrain actor;\n5 end\nBaselines\nIn most experiments, we compare four methods: Standard: For a new task\ntrain an actor directly from scratch, as per standard SL/RL (no stage (i)). All+FT: Train\na single actor on all the source tasks together (stage (i)) before ﬁne-tuning it (with a new\noutput layer) on the target2. Model-Agnostic Meta Learning (MAML): State of the\nart meta-learner for learning a transferable source model [9]. Meta-Critic: Our Meta-critic\nnetwork as described above.\n4.2\nSupervised Learning: Regression\nInspired by [9], we synthesise regression problems based on sinusoidal a sin(x+b) and linear\ncx+d functions. Within these function classes, diﬀerent tasks can be synthesised by choosing\ndiﬀerent a, b, c, and d values.\nThe ranges of these variables are: a ∈[1, 5], b ∈[0, π],\nc ∈[−3, 3], and d ∈[−3, 3]. The range of x is in [−5, 5]. We experiment with two conditions:\nthe ﬁrst involving sine functions only (as in [9]), and the second one has a mixture of sine and\nlinear functions (half-half) in both meta-training and testing stages. For meta-training (i),\nwe synthesise 10, 000 tasks with 30, 000 samples per task. Meta-learning thus means learning\nabout the function-class of sinusoids and lines. If successful, then for a meta-testing task, a\nmeta-learner should be able to estimate the line/sinusoid parameters from a few examples,\nand immediately be able to make a good prediction.\n2The eﬃcacy of this baseline will depend on whether the loosely termed ‘tasks’ are really diﬀering tasks\n(with diﬀerent reward functions), or contexts/domains (diﬀerent word models) [34].\n7\nSinusoid only\nSinusoid and Linear Mixture\nNum. of Samples\n4\n6\n8\n4\n6\n8\nStandard\n13.42 (20.21)\n6.17 (11.26)\n3.55 (7.17)\n8.18 (16.83)\n3.82 (9.93)\n1.90 (6.91)\nAll+FT\n6.41 (5.02)\n6.10 (4.73)\n5.82 (4.42)\n20.61 (26.23)\n19.90 (25.46)\n19.80 (25.59)\nMAML [9]\n0.87 (0.98)\n0.55 (0.75)\n0.53 (0.60)\n8.99 (11.56)\n3.68 (4.69)\n2.79 (3.13)\nMeta-Critic\n0.42 (0.61)\n0.15 (0.37)\n0.09 (0.11)\n7.91 (18.08)\n2.65 (11.18)\n0.86 (3.00)\nMeta-Critic-SL\n0.45 (0.81)\n0.14 (0.36)\n0.11 (1.06)\n7.69 (13.93)\n3.06 (10.22)\n1.26 (5.98)\nTable 1: Final-testing performance for diﬀerent regression meta learning strategies (MSE).\n-5\n0\n5\n-6\n-4\n-2\n0\n2\n4Random Sin Task (Sinusoid Only)\n-5\n0\n5\n-6\n-4\n-2\n0\n2\n4\nRandom Sin Task (Mixture)\nGround Truth Function\nFew-Shot Samples\nStandard\nMeta-Critic\nMAML\n-5\n0\n5\n-10\n-5\n0\n5\n10\n15 Random Linear Task (Mixture)\nFigure 2: Qualitative comparison of meta-learning strategies on few-shot learning regression tasks\ndrawn from a function pool of sin only (Sinusoid), versus a mixture of sin and linear (Mixture).\nFor meta-testing each task has only a few-shots K ∈{4, 6, 8} pairs of (x, y) for training. We\ngenerate 100 new testing tasks in total, and for each generate 100 testing samples on which\nto evaluate the ﬁnal-testing MSE. We repeat every adapting round (corresponding to a\ntask) 10 times with diﬀerent (x, y) pairs, so one testing task has 10 MSE values on the same\nout-of-sample set. The mean and standard deviation of all tasks’ MSEs are summarised in\nTable 1. For our method we also explore semi-supervised learning using unlabelled x. In\nreal-world problems these would correspond to unlabelled instances, but for this synthetic\nproblem, we simply uniformly sample x ∼[−5, 5]. We can see that our meta-critic performs\ncomparably or better than alternatives. Here we also evaluate Meta-Critic-SL: our method\nwhere the meta-testing actor is pre-trained with standard supervised learning before training\nby the meta-critic. The similar performance of these two variants shows that via the shared\nmeta-critic, a good performing actor can be obtained by learning from scratch, without\nrequiring any pre-training.\nFor qualitative illustration, we randomly pick tasks for K = 4 shot learning from sinusoid\nonly condition and line+sinusoid mixture condition, as shown in Fig. 2. We see that all\nmodels ﬁt the K = 4 shot meta-testing data. In the sin-only condition (Fig. 2(a)) MAML\nis not much worse than meta-critic in the out-of-sample areas. However, in the mixture\ncondition (Fig. 2(b)), MAML is much worse than meta-critic. The reason is that a single\nglobally shared prior/initialisation parameter implicitly assumes that tasks are similar, and\ntheir distribution is uni-modal. In the mixture case where tasks are diversely distributed\nand with varying relatedness, then this assumption is too strong and performance is poor.\nWe increased the number of parameters for the actor network in MAML so that there was\nample capacity for learning a complex multi-modal model, but it didn’t help. In contrast,\nour meta-critic approach is ﬂexible enough to model this multi-modal task-distribution\nand learns to supervise the actor appropriately.\nThese qualitative results are reﬂected\nquantitatively in Table 1, where we see that MAML and Meta-Critic perform comparably\n– and better than Standard/All-Fine-Tune – in the Sin only condition, but Meta-Critic is\nclearly better in the mixture condition. This is because the TAEN successfully learns to\nembed task category information in its task description z. Taking the zs for all the tasks in\nthe mixture condition, we ﬁnd that a simple classiﬁer (SVM with RBF kernel) can obtain\n85% accuracy in predicting the task category (linear vs sinusoid).\n8\n2−arm\n4−arm\n6−arm\nNum. of Pulls\n5\n10\n15\n10\n15\n20\n15\n20\n25\nRandom\n0.5\n0.25\n0.17\nStandard\n0.69 (0.17)\n0.69 (0.17)\n0.69 (0.17)\n0.41 (0.11)\n0.43 (0.12)\n0.43 (0.12)\n0.29 (0.07)\n0.29 (0.08)\n0.29 (0.07)\nAll+FT\n0.43 (0.30)\n0.43 (0.30)\n0.43 (0.30)\n0.31 (0.22)\n0.31 (0.22)\n0.31 (0.22)\n0.13 (0.13)\n0.13 (0.13)\n0.13 (0.13)\nMAML [9]\n0.61 (0.25)\n0.63 (0.24)\n0.65 (0.25)\n0.35 (0.18)\n0.38 (0.22)\n0.38 (0.19)\n0.26 (0.14)\n0.27 (0.15)\n0.27 (0.15)\nMeta-Critic\n0.70 (0.18)\n0.73 (0.17)\n0.74 (0.17)\n0.44 (0.14)\n0.47 (0.16)\n0.48 (0.16)\n0.30 (0.10)\n0.31 (0.11)\n0.32 (0.11)\nBest\n0.75\n0.52\n0.41\nTable 2:\nDiﬀerent meta-learning strategies for dependent multi-arm bandit.\nReward in ﬁnal-\ntesting.\n4.3\nReinforcement Learning: Dependant Multi-arm Bandit\nFor a ﬁrst RL experiment, we work with multi-arm bandits (MAB). In this experiment,\nthe arm rewards are dependent: the probability of getting a reward is drawn a Dirichlet\ndistribution parametrised by α1 = α2 = · · · = 1, so the sum of reward probabilities over all\narms is one. Dependent MAB means that a trial on one arm also tells us about the other\narms. However, it is not encoded in the model, i.e., the agent is unaware of the distribution,\nor that the arms are dependent. It is expected to ﬁnd this out for more sample-eﬃcient\nlearning. Each task is a diﬀerent arm reward conﬁguration: multiple samples drawn from\nthe same Dirichlet distribution. We experiment with 2−, 4−, and 6−arm bandits. For\nmeta-training (i), we generated 1000 tasks (bandits) with ample (30, 000) samples, and\nfor meta-testing (ii) we generate 100 new bandits (corresponding to diﬀerent arm-reward\ndistributions) and only a few trials are allowed per task. We repeat experiments 10 times\ndue to the randomness of getting a reward by pulling an arm.\nThe results are shown in Table 2, quantiﬁed by average reward which is calculated by the\ndot product of its softmax output and the bandit’s conﬁguration (probability of getting a\nreward by pulling each arm). The Upper bound is calculated by always pulling the arm\nwith largest probability of getting reward, which is 0.75 for 2-arm, 0.52 for 4-arm, and 0.41\nfor 6-arm. The random choice lower bound is always equal to\n1\nNum. of Arms. Our meta-critic\nstrategy achieves higher reward than competitors for any given number of trials.\n4.4\nReinforcement Learning: Cartpole Control\nFinally we try a classic RL control problem – cartpole. The agent should learn to balance\na pole by moving a cart left or right to keep it stable [5]. The state st is 4-dimensional\n(θ, x, ˙θ, ˙x) where θ is the pole angle, x the cart position, and ( ˙θ, ˙x) are their respective\nvelocities. For meta-training, we generate 1000 tasks by sampling pole lengths in the range\n[0.5, 5]. For each task, we train for 150, 000 interactions (actor moves left or right given\nthe state). For meta-testing, the agent is allowed to play 100 full games (episodes), while\nthe game terminates when the pole falls or 200 interactions have been reached. After each\ngame terminates, we update the model parameter using the experience collected, and start\nan oﬄine testing: let the agent play 10 games (episodes), and record the average reward\n(this experience will not be used for any training purpose). We carry out the experiment on\n100 new tasks, and repeat it 10 times for each task. In this experiment, we exclude All+FT\nas it performed much worse than the others and include the PG-ELLA [1] method (lifelong\npolicy gradient-based reinforcement learning via a low-rank policy factorisation), which we\nextended to work with a neural network policy as per [39].\nThe results in Fig. 3(left) show that the meta-critic has a much higher learning rate (im-\nprovement per-training episode), and quickly learns to outperform the others. It’s starting\npoint is slightly lower, since it starts from a randomly initialised actor policy unlike MAML\nand PG-ELLA. Quantifying a successful meta-test as succeeding on 10 corresponding ﬁnal\ntests (a successful episode is in turn deﬁned in the standard way of balancing the pole for\n> 195 iterations [5]), Fig. 3(right) shows that Meta-Critic’s success rate of 26% is much\nhigher than alternatives. Finally, Fig. 3(middle) shows a t-SNE [36] plot of the TAEN em-\nbeddings z coloured by the pole length of the task. The learned embedding clearly reﬂects\n9\nMeta-Critic\nStandard\nMAML\nPG-ELLA\n0\n5\n10\n15\n20\n25\n30\n% of successful meta-tests\nLearning Succeses\nFigure 3: Cartpole results. Left: Average reward per episode when learning a new task. Middle: t-\nSNE plot of cartpole task embeddings. Each point is a task and colour indicates pole length. Right:\nPercentage of successful meta-tests (learned to consistently balancing a pole of a new length).\na progression of pole length. This is noteworthy as the agent is not directly exposed to the\npole length. It has learned this task manifold based only on learning traces.\n5\nConclusion\nWe have presented a very ﬂexible meta-learning method for few-shot learning that applies to\nboth reinforcement and supervised learning settings, and can seamlessly exploit unlabelled\ndata. Promising results are obtained on a variety of problems. In future work we would\nlike to evaluate our method to more challenging problems in supervised learning and RL for\ncontrol in terms of diﬃculty and input/output dimensionality; and extend it to the continual\nlearning setting.\nReferences\n[1] H. B. Ammar, E. Eaton, P. Ruvolo, and M. Taylor. Online multi-task learning for\npolicy gradient methods. In ICML, 2014.\n[2] M. Andrychowicz, M. Denil, S. G. Colmenarejo, M. W. Hoﬀman, D. Pfau, T. Schaul,\nand N. de Freitas. Learning to learn by gradient descent by gradient descent. In NIPS,\n2016.\n[3] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that\ncan solve diﬃcult learning control problems. IEEE Transactions on Systems, Man, and\nCybernetics, 13(5):834–846, 1983.\n[4] L. Bertinetto, J. F. Henriques, J. Valmadre, P. H. S. Torr, and A. Vedaldi. Learning\nfeed-forward one-shot learners. In NIPS, 2016.\n[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and\nW. Zaremba. Openai gym, 2016.\n[6] C. Bucila, R. Caruana, and A. Niculescu-Mizil. Model compression. In KDD, 2006.\n[7] B. da Silva, G. Konidaris, and A. Barto. Learning parameterized skills. In ICML, 2012.\n[8] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL2: Fast\nreinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016.\n[9] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation\nof deep networks. In ICML, 2017.\n10\n[10] C. Finn, T. Yu, J. Fu, P. Abbeel, and S. Levine. Generalizing skills with semi-supervised\nreinforcement learning. In ICLR, 2017.\n[11] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual\nMulti-Agent Policy Gradients. ArXiv e-prints, May 2017.\n[12] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov.\nDevise: A deep visual-semantic embedding model. In NIPS, 2013.\n[13] I. Grondman, L. Busoniu, G. Lopes, and R. Babuska. A survey of actor-critic reinforce-\nment learning: Standard and natural policy gradients. Systems, Man, and Cybernetics,\nPart C: Applications and Reviews, IEEE Transactions on, 42(6):1291–1307, 2012.\n[14] D. Ha, A. Dai, and Q. Le. Hypernetworks. In ICLR, 2017.\n[15] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In\nNIPS 2014 Deep Learning Workshop, 2014.\n[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9\n(8):1735–1780, Nov. 1997.\n[17] L. C. B. III. Advantage updating. Technical Report WL–TR-93-1146, Wright-Patterson\nAir Force Base Ohio: Wright Laboratory, 1993.\n[18] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese neural networks for one-shot image\nrecognition. In ICML, 2015.\n[19] A. G. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-eﬃcient general-\nization of robot skills with contextual policy search. In AAAI, 2013.\n[20] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[21] H. Larochelle, D. Erhan, and Y. Bengio. Zero-data learning of new tasks. In AAAI,\n2008.\n[22] K. Li and J. Malik. Learning to optimize. In ICLR, 2017.\n[23] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and\nD. Wierstra. Continuous control with deep reinforcement learning. In ICLR, 2016.\n[24] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and\nK. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML,\n2016.\n[25] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In\nIJCNN, 1992.\n[26] E. Parisotto, J. Ba, and R. Salakhutdinov. Actor-mimic: Deep multitask and transfer\nreinforcement learning. In ICLR, 2016.\n[27] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In ICLR,\n2017.\n[28] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning\nwith memory-augmented neural networks. In ICML, 2016.\n[29] T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators.\nIn ICML, 2015.\n11\n[30] J. Schmidhuber, J. Zhao, and M. Wiering. Shifting inductive bias with success-story\nalgorithm, adaptive levin search, and incremental self-improvement. Machine Learning,\n28(1):105–130, 1997.\n[31] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional\ncontinuous control using generalized advantage estimation. In ICLR, 2016.\n[32] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A. Riedmiller. Deter-\nministic policy gradient algorithms. In ICML, 2014.\n[33] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for\nreinforcement learning with function approximation. In NIPS, 1999.\n[34] M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A\nsurvey. JMLR, 10:1633–1685, Dec. 2009.\n[35] S. Thrun and L. Pratt, editors.\nLearning to Learn.\nKluwer Academic Publishers,\nNorwell, MA, USA, 1998. ISBN 0-7923-8047-9.\n[36] L. van der Maaten and G. E. Hinton. Visualizing high-dimensional data using t-sne.\nJMLR, 9:2579–2605, 2008.\n[37] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching\nnetworks for one shot learning. In NIPS, 2016.\n[38] C. Watkins and P. Dayan. Q-learning. Machine Learning, 8(3-4):279–292, 1992.\n[39] C. Zhao, T. Hospedales, F. Stulp, and O. Sigaud. Tensor based knowledge transfer\nacross skill categories for robot control. In IJCAI, 2017.\n12\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-06-29",
  "updated": "2017-06-29"
}