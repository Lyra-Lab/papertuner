{
  "id": "http://arxiv.org/abs/1707.00061v1",
  "title": "Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English",
  "authors": [
    "Su Lin Blodgett",
    "Brendan O'Connor"
  ],
  "abstract": "We highlight an important frontier in algorithmic fairness: disparity in the\nquality of natural language processing algorithms when applied to language from\nauthors of different social groups. For example, current systems sometimes\nanalyze the language of females and minorities more poorly than they do of\nwhites and males. We conduct an empirical analysis of racial disparity in\nlanguage identification for tweets written in African-American English, and\ndiscuss implications of disparity in NLP.",
  "text": "Racial Disparity in Natural Language Processing:\nA Case Study of Social Media African-American English\nSu Lin Blodget\nUniversity of Massachusets Amherst\nAmherst, MA\nblodget@cs.umass.edu\nBrendan O’Connor\nUniversity of Massachusets Amherst\nAmherst, MA\nbrenocon@cs.umass.edu\nABSTRACT\nWe highlight an important frontier in algorithmic fairness: dispar-\nity in the quality of natural language processing algorithms when\napplied to language from authors of diﬀerent social groups. For\nexample, current systems sometimes analyze the language of fe-\nmales and minorities more poorly than they do of whites and males.\nWe conduct an empirical analysis of racial disparity in language\nidentiﬁcation for tweets writen in African-American English, and\ndiscuss implications of disparity in NLP.\n1\nINTRODUCTION: DISPARITY IN NLP\nAs machine learned algorithms govern more and more real-world\noutcomes, how to make them fair—and what that should mean—is\nof increasing concern. One strand of research, heavily represented\nat the FAT-ML series of workshops,1 considers scenarios where\na learning algorithm must make decisions about people, such as\napproving prospective applicants for employment, or deciding who\nshould be the targets of police actions [5], and seeks to develop\nlearners or algorithms whose decisions have only small diﬀerences\nin behavior between persons from diﬀerent groups [4] or that satisfy\nother notions of fairness (e.g. [12, 13]).\nAnother recent strand of research has examined a complemen-\ntary aspect of bias and fairness: disparate accuracy in language anal-\nysis. Linguistic production is a critically important form of human\nbehavior, and a major class of artiﬁcial intelligence algorithms—\nnatural language processing, or language technologies—may or\nmay not fairly analyze language produced by diﬀerent types of\nauthors [7]. For example, Tatman [20] ﬁnds that YouTube autocap-\ntioning has a higher word error rate for female speakers than for\nmale speakers in videos. Tis has implications for downstream uses\nof language technology:\n• Viewing: users who rely on autocaptioning have a harder\ntime understanding what women are saying in videos, rel-\native to what men are saying.\n• Access: search systems are necessary for people to access\ninformation online, and for videos they may depend on\nindexing text recognized from the audio. Tatman’s results\n[20] imply that such a search system will fail to ﬁnd infor-\nmation produced by female speakers more ofen than for\nmale speakers.\nTis bias aﬀects interests of the speakers—it is more diﬃcult for\ntheir voices to be communicated to the world—as well as other\nusers, who are deprived of information or opinions from females,\nor more generally, any social group whose language experiences\nlower accuracy of analysis by language technologies.\n1htp://www.fatml.org/\nGender and dialect are well-known confounds in speech recogni-\ntion, since they can implicate pitch, timbre, and the pronunciation\nof words (the phonetic level of language); domain adaptation is\nalways a challenge and research continues on how to apply do-\nmain transfer to speech recognizers across dialects [15]. And more\nbroadly, decades of research in the ﬁeld of sociolinguistics has doc-\numented an extensive array of both social factors that aﬀect how\npeople produce language (e.g. community, geography, ethnicity),\nand how speciﬁcally language is aﬀected (e.g. the lexicon, syntax,\nsemantics). We might expect a minority teenager in school as well\nas a white middle-aged sofware engineer to both speak English,\nbut they may exhibit variation in their pronunciation, word choice,\nslang, or even syntactic structures. Dialect communities ofen align\nwith geographic and sociological factors, as language variation\nemerges within distinct social networks, or is aﬃrmed as a marker\nof social identity.\nDialects pose a challenge to fairness in NLP, because they en-\ntail language variation that is correlated to social factors, and we\nbelieve there needs to be greater awareness of dialects among tech-\nnologists using and building language technologies. In the rest of\nthis paper, we focus on the dialect of African-American English\nas used on Twiter, which previous work [3, 9, 11] has established\nis very prevalent and sometimes quite diﬀerent than mainstream\nAmerican English. We analyze an African-American English Twit-\nter corpus (from Blodget et al. [3], described in §3), and analyze\nracial disparity in language identiﬁcation, a crucial ﬁrst step in any\nNLP application. Our previous work found that oﬀ-the-shelf tools\ndisplay racial disparity—they tend to erroneously classify messages\nfrom African-Americans as non-English more ofen than those from\nwhites. We extend this analysis from 200 to 20,000 tweets, ﬁnding\nthat the disparity persists when controlling for message length (§4),\nand evaluate the racial disparity for several black-box commercial\nservices. We conclude with a brief discussion (§5).\n2\nAFRICAN-AMERICAN ENGLISH AND\nSOCIAL MEDIA\nWe focus on language in social media, which is ofen informal and\nconversational. Social media NLP tools may be used for, say, senti-\nment analysis applications, which seek to measure opinions from\nonline communities. But current NLP tools are typically trained\non traditional writen sources, which are quite diﬀerent from so-\ncial media language, and even more so from dialectal social media\nlanguage. Not only does this imply social media NLP may be of\nlower accuracy, but since language can vary across social groups,\nany such measurements may be biased—incorrectly representing\nideas and opinions from people who use non-standard language.\narXiv:1707.00061v1  [cs.CY]  30 Jun 2017\nSpeciﬁcally, we investigate dialectal language in publicly avail-\nable Twiter data, focusing on African-American English (AAE), a\ndialect of American English spoken by millions of people across the\nUnited States [6, 14, 18]. AAE is a linguistic variety with deﬁned\nsyntactic-semantic, phonological, and lexical features, which have\nbeen the subject of a rich body of sociolinguistic literature. In addi-\ntion to the linguistic characterization, reference to its speakers and\ntheir geographical location or speech communities is important,\nespecially in light of the historical development of the dialect. Not\nall African-Americans speak AAE, and not all speakers of AAE are\nAfrican-American; nevertheless, speakers of this variety have close\nties with speciﬁc communities of African-Americans [6].\nTe phenomenon of “BlackTwiter” has been noted anecdotally;\nindeed, African-American and Hispanic minorities were markedly\nover-represented in the early years of the Twiter service (as well\nas younger people) relative to their representation in the American\ngeneral population.2 It is easy to ﬁnd examples of non-Standard\nAmerican English (SAE) language use, such as:\n(1) he woke af smart af educated af daddy af coconut oil af\nGOALS AF & shares food af\n(2) Bored af den my phone ﬁnna die‼!\nTe ﬁrst example has low punctuation usage (there is an uterance\nboundary afer every “af”), but more importantly, it displays a key\nsyntactic feature of the AAE dialect, a null copula: “he woke” would\nbe writen, in Standard American English, as “he is woke” (meaning,\npolitically aware). “af” is an online-speciﬁc term meaning “as f—.”\nTe second example displays two more traditional AAE features:\n“den” is a spelling of “then” which follows a common phonological\ntransform in AAE (initial “th” changing to a “d” sound: “dat,” “dis,”\netc. are also common), and the word “ﬁnna” is an auxiliary verb,\nshort for “ﬁxing to,” which indicates an immediate future tense (“my\nphone is going to die very soon”); it is part of AAE’s rich verbal\nauxiliary system capable of encoding diﬀerent temporal semantics\nthan mainstream English [6].\n3\nDEMOGRAPHIC MIXED MEMBERSHIP\nMODEL FOR SOCIAL MEDIA\nIn order to test racial disparity in social media NLP, [3] collects a\nlarge-scale AAE corpus from Twiter, inferring sof demographic\nlabels with a mixed-membership probabilistic model; we use this\nsame corpus and method, brieﬂy repeating the earlier description of\nthe method. Tis approach to identifying AAE-like text makes use\nof the connection between speakers of AAE and African-American\nneighborhoods; we harvest a set of messages from Twiter, cross-\nreferenced against U.S. Census demographics, and then analyze\nwords against demographics with a mixed-membership probabilis-\ntic model. Te data is a sample of millions of publicly posted geo-\nlocated Twiter messages (from the Decahose/Gardenhose stream\n[17]), most of which are sent on mobile phones, by authors in the\nU.S. in 2013.\nFor each message, we look up the U.S. Census blockgroup geo-\ngraphic area that the message was sent in, and use race and ethnicity\ninformation for each blockgroup from the Census’ 2013 American\nCommunity Survey, deﬁning four covariates: percentages of the\n2htp://www.pewinternet.org/fact-sheet/social-media/\npopulation that are non-Hispanic whites, non-Hispanic blacks, His-\npanics (of any race), and (non-Hispanic) Asians. Finally, for each\nuser u, we average the demographic values of all their messages in\nour dataset into a length-four vector π(census)\nu\n.\nGiven this set of messages and author-associated demograph-\nics, we infer statistical associations between language and demo-\ngraphics with a mixed membership probabilistic model. It directly\nassociates each of the demographic variables with a topic; i.e. a\nunigram language model over the vocabulary. Te model assumes\nan author’s mixture over the topics tends to be similar to their\nCensus-associated demographic weights, and that every message\nhas its own topic distribution. Tis allows for a single author to use\ndiﬀerent types of language in diﬀerent messages, accommodating\nmultidialectal authors. Te message-level topic probabilities θm\nare drawn from an asymmetric Dirichlet centered on π(census)\nu\n,\nwhose scalar concentration parameter α controls whether authors’\nlanguage is very similar to the demographic prior, or can have some\ndeviation. A token t’s latent topic zt is drawn from θm, and the\nword itself is drawn from ϕzt , the language model for the topic.\nTus, the model learns demographically-aligned language models\nfor each demographic category. Our previous work [3] veriﬁes that\nits African-American language model learns linguistic atributes\nknown in the sociolinguistics literature to be characteristic of AAE,\nin line with other work that has also veriﬁed the correspondence of\ngeographical AA prevalence to AAE linguistic features on Twiter\n[10, 19].\nTis publicly available corpus contains 59.2 million tweets. We\nﬁlter its messages to ones strongly associated with demographic\ngroups; for example, for each message we infer the posterior pro-\nportion of its tokens that came from the African-American language\nmodel, which can be high either due to demographic prior, or from\na message that uses many words exclusive to the AA language\nmodel (topic); these proportions are available in the released cor-\npus. When we ﬁlter to messages with AA proportion greater than\n0.8, this results in AAE-like text. We call these AA-aligned messages\nand we also select a set of white-aligned messages in the same way.3\n4\nBIAS IN NLP TOOLS\n4.1\nLanguage identiﬁcation\nLanguage identiﬁcation, the task of classifying the major world\nlanguage in which a message is writen, is a crucial ﬁrst step in al-\nmost any web or social media text processing pipeline. For example,\nin order to analyze the opinions of U.S. Twiter users, one might\nthrow away all non-English messages before running an English\nsentiment analyzer. (Some of the coauthors of this paper have done\nthis as a simple expedient step in the past.)\nA variety of methods for language identiﬁcation exist [8]; so-\ncial media language identiﬁcation is particularly challenging since\nmessages are short and also use non-standard language [1]. In\n3While Blodget et al. veriﬁed that the AA-aligned tweets contain well-known features\nof AAE, we hesitate to call these “AAE” and “SAE” corpora, since technically speaking\nthey are simply demographically correlated language models. Te Census refers to the\ncategories as “Black or African-American” and “White” (codes B03002E4 and B03002E3\nin ACS 2013). And, while Hispanic- and Asian-associated language models of Blodget\net al.’s model are also of interest, we focus our analysis here on the African-American\nand White language models.\n2\nfact, a popular language identiﬁcation system, langid.py [16], clas-\nsiﬁes both example messages in §2 as Danish with more than 99.9%\nconﬁdence.\nWe take the perspective that since AAE is a dialect of American\nEnglish, it ought to be classiﬁed as English for the task of major\nworld language identiﬁcation. We hypothesize that if a language\nidentiﬁcation tool is trained on standard English data, it may exhibit\ndisparate performance on AA- versus white-aligned tweets. In\nparticular, we wish to assess the racial disparity accuracy diﬀerence:\np(correct | Wh) −p(correct | AA)\n(1)\nFrom manual inspection of a sample of hundreds of messages, it\nappears that nearly all white-aligned and AA-aligned tweets are\nactually English, so accuracy is the same as proportion of English\npredictions by the classiﬁer. A disparity of 0 indicates a language\nidentiﬁer that is fair across these classes. (An alternative measure is\nthe ratio of accuracies, corresponding to Feldman et al.’s disparate\nimpact measure [4].)\n4.2\nExperiments\nWe conduct an evaluation of four diﬀerent oﬀ-the-shelf language\nidentiﬁers, which are popular and straightforward for engineers to\nuse when building applications:\n• langid.py (sofware): One of the most popular open source\nlanguage identiﬁcation tools, langid.py was originally trained\non over 97 languages and evaluated on both traditional\ncorpora and Twiter messages [16].\n• IBM Watson (API): Te Watson Developer Cloud’s Lan-\nguage Translator service supports language identiﬁcation\nof 62 languages.4\n• Microsof Azure (API): Microsof Azure’s Cognitive Ser-\nvices supports language identiﬁcation of 120 languages.5\n• Twitter (metadata): Te output of Twiter’s in-house\nidentiﬁer, whose predictions are included in a tweet’s meta-\ndata (from 2013, the time of data collection), which Twiter\nintends to “help developers more easily work with targeted\nsubsets of Tweet collections.”6\n• Google (API, excluded): We atempted to test Google’s\nlanguage detection service,7 but it returned a server error\nfor every message we gave it to classify.\nWe queried the remote API systems in May 2017.\nFrom manual inspection, we observed that longer tweets are sig-\nniﬁcantly more likely to be correctly classiﬁed, which is a potential\nconfound for a race disparity analysis, since the length distribution\nis diﬀerent for each demographic group. To minimize this eﬀect\nin our comparisons, we group messages into four bins (shown in\nTable 1) according to the number of words in the message. For each\nbin, we sampled 2,500 AA-aligned tweets and 2,500 white-aligned\ntweets, yielding a total of 20,000 messages across the two categories\n4htps://www.ibm.com/watson/developercloud/doc/language-translator/index.html\n5htps://docs.microsof.com/en-us/azure/cognitive-services/text-analytics/\noverview#language-detection\n6htps://blog.twiter.com/developer/en us/a/2013/introducing-new-metadata-for-tweets.\nhtml\n7htps://cloud.google.com/translate/docs/detecting-language\nand four bins.8 We limited pre-processing of the messages to ﬁxing\nof HTML escape characters and removal of URLs, keeping “noisy”\nfeatures of social media text such as @-mentions, emojis, and hash-\ntags. We then calculated, for each bin in each category, the number\nof messages predicted to be in English by each classiﬁer. Accuracy\nresults are shown in Table 1.9\nAs predicted, classiﬁer accuracy does increase as message lengths\nincrease; classiﬁer accuracy is generally excellent for all messages\ncontaining at least 10 tokens. Tis result agrees with previous work\nﬁnding short texts to be challenging to classify (e.g. [2]), since there\nare fewer features (e.g. character n-grams) to give evidence for the\nlanguage used.10\nHowever, the classiﬁer results display a disparity in performance\namong messages of similar length; for all but one length bin under\none classiﬁer, accuracy on the white-aligned sample is higher than\non the AA-aligned sample. Te disparity in performance between\nAA- and white-aligned messages is greatest when messages are\nshort; the gaps in performance for extremely short messages ranges\nacross classiﬁers from 6.6% to 19.7%. Tis gap in performance is\nparticularly critical as 41.7% of all AA-aligned messages in the\ncorpus as a whole have 5 or fewer tokens.11\n5\nDISCUSSION\nAre these disparities substantively signiﬁcant? It is easy to see\nhow statistical bias could arise in downstream applications. For\nexample, consider an analyst trying to look at major opinions about\na product or political ﬁgure, with a sentiment analysis system that\nonly gathers opinions from messages classiﬁed as English by Twiter.\nFor messages length 5 or less, opinions from African-American\nspeakers will be shown to be 1−54.0/73.7 = 27% less frequent than\nthey really are, relative to white opinions. Fortunately, the accuracy\ndisparities are ofen only a few percentage points; nevertheless, it\nis important for practitioners to keep potential biases like these in\nmind.\nOne way forward to create less disparate NLP systems will be\nto use domain adaptation and other methods to extend algorithms\nto work on diﬀerent distributions of data; for example, our demo-\ngraphic model’s predictions can be used to improve a language\nidentiﬁer, since the demographic language model’s posteriors accu-\nrately identify some cases of dialectal English [3]. In the context\nof speech recognition, Lehr et al. [15] pursue a joint modeling ap-\nproach, learning pronunciation model parameters for AAE and SAE\nsimultaneously.\nOne important issue may be the limitation of perspective of\ntechnologists versus users. In striking contrast to Twiter’s (histor-\nically) minority-heavy demographics, major U.S. tech companies\nare notorious for their low representation of African-Americans\nand Hispanics; for example, Facebook and Google report only 1%\n8Due to a data processing error, there are 5 duplicates (19,995 unique tweets); we\nreport on all 20,000 messages for simplicity.\n9We have made the 20,000 messages publicly available at: htp://slanglab.cs.umass.\nedu/TwiterAAE/\n10A reviewer asked if length is used as a feature; we know that the open-source\nlangid.py system does not (explicitly) use it.\n11For most (system,length) combinations, the accuracy diﬀerence is signiﬁcant under\na two-sided t-test (p < .01) except for two rows (t ≤5, langid.py, p = .03) and\n(10 < t ≤15, Twiter, p = 0.5). Accuracy rate standard errors range from 0.04% to\n0.9% (≈\np\nacc(1 −acc)/2500).\n3\nAA Acc.\nWH Acc.\nDiﬀ.\nlangid.py\nt ≤5\n68.0\n70.8\n2.8\n5 < t ≤10\n84.6\n91.6\n7.0\n10 < t ≤15\n93.0\n98.0\n5.0\nt > 15\n96.2\n99.8\n3.6\nIBM Watson\nt ≤5\n62.8\n77.9\n15.1\n5 < t ≤10\n91.9\n95.7\n3.8\n10 < t ≤15\n96.4\n99.0\n2.6\nt > 15\n98.0\n99.6\n1.6\nMicrosof Azure\nt ≤5\n87.6\n94.2\n6.6\n5 < t ≤10\n98.5\n99.6\n1.1\n10 < t ≤15\n99.6\n99.9\n0.3\nt > 15\n99.5\n99.9\n0.4\nTwiter\nt ≤5\n54.0\n73.7\n19.7\n5 < t ≤10\n87.5\n91.5\n4.0\n10 < t ≤15\n95.7\n96.0\n0.3\nt > 15\n98.5\n95.1\n-3.0\nTable 1: Percent of the 2,500 tweets in each bin classiﬁed as English by each classiﬁer; Diﬀ. is the diﬀerence (disparity on an\nabsolute scale) between the classiﬁer accuracy on the AA-aligned and white-aligned samples. t is the message length for the\nbin.\nof their tech employees are African-American,12 as opposed to\n13.3% in the overall U.S. population,13 and the population of com-\nputer science researchers in the U.S. has similarly low minority\nrepresentation. It is of course one example of the ever-present\nchallenge of sofware designers understanding how users use their\nsofware; in the context of language processing algorithms, such\nunderstanding must be grounded in an understanding of dialects\nand sociolinguistics.\nREFERENCES\n[1] Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang.\n2013. How Noisy Social Media Text, How Diﬀrnt Social Media Sources?. In\nInternational Joint Conference on Natural Language Processing. 356–364.\n[2] Timothy Baldwin and Marco Lui. 2010. Language identiﬁcation: Te long and the\nshort of the mater. In Human Language Technologies: Te 2010 Annual Conference\nof the North American Chapter of the Association for Computational Linguistics.\nAssociation for Computational Linguistics, 229–237.\n[3] Su Lin Blodget, Lisa Green, and Brendan O’Connor. 2016. Demographic Dialectal\nVariation in Social Media: A Case Study of African-American English. Proceedings\nof EMNLP (2016).\n[4] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and\nSuresh Venkatasubramanian. 2015. Certifying and removing disparate impact.\nIn Proceedings of the 21th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining. ACM, 259–268.\n[5] Sharad Goel, Maya Perelman, Ravi Shroﬀ, and David Alan Sklansky. 2017. Com-\nbating police discrimination in the age of big data. New Criminal Law Review:\nIn International and Interdisciplinary Journal 20, 2 (2017), 181–232.\n[6] Lisa J. Green. 2002. African American English: A Linguistic Introduction. Cam-\nbridge University Press.\n[7] Dirk Hovy and L. Shannon Spruit. 2016. Te Social Impact of Natural Language\nProcessing. In Proceedings of ACL.\n[8] Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy Nicholson, and Andrew\nMacKinlay. 2006. Reconsidering Language Identiﬁcation for Writen Language\nResources. In Proceedings of the Fifh International Conference on Language Re-\nsources and Evaluation (LREC’06). European Language Resources Association\n(ELRA). htp://aclweb.org/anthology/L06-1274\n[9] Taylor Jones. 2015. Toward a Description of African American Vernacular\nEnglish Dialect Regions Using “Black Twiter”. American Speech 90, 4 (2015),\n403–440.\n12htps://newsroom.f.com/news/2016/07/facebook-diversity-update-positive-hiring-trends-show-progress/\nhtps://www.google.com/diversity/\n13htps://www.census.gov/quickfacts/table/RHI225215/00\n[10] Anna Jørgensen, Dirk Hovy, and Anders Søgaard. 2016. Learning a POS tagger\nfor AAVE-like language. In Proceedings of NAACL. Association for Computational\nLinguistics.\n[11] Anna Katrine Jørgensen, Dirk Hovy, and Anders Søgaard. 2015. Challenges of\nstudying and processing dialects in social media. In Proceedings of the Workshop\non Noisy User-generated Text. 9–18.\n[12] Mathew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth.\n2016. Rawlsian fairness for machine learning. arXiv preprint arXiv:1610.09559\n(2016).\n[13] Mathew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. 2016.\nFairness in Learning: Classic and Contextual Bandits. In Advances in Neural In-\nformation Processing Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,\nand R. Garnet (Eds.). Curran Associates, Inc., 325–333. htp://papers.nips.cc/\npaper/6355-fairness-in-learning-classic-and-contextual-bandits.pdf\n[14] William Labov. 1972. Language in the inner city: Studies in the Black English\nvernacular. Vol. 3. University of Pennsylvania Press.\n[15] Maider Lehr, Kyle Gorman, and Izhak Shafran. 2014. Discriminative pronuncia-\ntion modeling for dialectal speech recognition. In Proc. Interspeech.\n[16] M. Lui and T. Baldwin. 2012. langid. py: An Oﬀ-the-shelf Language Identiﬁ-\ncation Tool. In Proceedings of the 50th Annual Meeting of the Association for\nComputational Linguistics (ACL 2012), Demo Session, Jeju, Republic of Korea.\nhtp://www.aclweb.org/anthology-new/P/P12/P12-3005.pdf\n[17] Fred Morstater, Jrgen Pfeﬀer, Huan Liu, and Kathleen Carley. 2013. Is the\nSample Good Enough? Comparing Data from Twiter’s Streaming API with\nTwiter’s Firehose. In International AAAI Conference on Weblogs and Social Media.\nhtp://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/view/6071\n[18] John Russell Rickford. 1999. African American vernacular English: Features,\nevolution, educational implications. Wiley-Blackwell.\n[19] Ian Stewart. 2014. Now We Stronger than Ever: African-American English\nSyntax in Twiter. In Proceedings of the Student Research Workshop at the 14th\nConference of the European Chapter of the Association for Computational Linguis-\ntics. Association for Computational Linguistics, Gothenburg, Sweden, 31–37.\nhtp://www.aclweb.org/anthology/E14-3004\n[20] Rachael Tatman. 2017. Gender and Dialect Bias in YouTube’s Automatic Captions.\nIn Proceedings of the First ACL Workshop on Ethics in Natural Language Processing.\nAssociation for Computational Linguistics, Valencia, Spain, 53–59. htp://www.\naclweb.org/anthology/W/W17/W17-1606\n4\n",
  "categories": [
    "cs.CY",
    "cs.CL"
  ],
  "published": "2017-06-30",
  "updated": "2017-06-30"
}