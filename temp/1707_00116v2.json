{
  "id": "http://arxiv.org/abs/1707.00116v2",
  "title": "Image Companding and Inverse Halftoning using Deep Convolutional Neural Networks",
  "authors": [
    "Xianxu Hou",
    "Guoping Qiu"
  ],
  "abstract": "In this paper, we introduce deep learning technology to tackle two\ntraditional low-level image processing problems, companding and inverse\nhalftoning. We make two main contributions. First, to the best knowledge of the\nauthors, this is the first work that has successfully developed deep learning\nbased solutions to these two traditional low-level image processing problems.\nThis not only introduces new methods to tackle well-known image processing\nproblems but also demonstrates the power of deep learning in solving\ntraditional signal processing problems. Second, we have developed an effective\ndeep learning algorithm based on insights into the properties of visual quality\nof images and the internal representation properties of a deep convolutional\nneural network (CNN). We train a deep CNN as a nonlinear transformation\nfunction to map a low bit depth image to higher bit depth or from a halftone\nimage to a continuous tone image. We also employ another pretrained deep CNN as\na feature extractor to derive visually important features to construct the\nobjective function for the training of the mapping CNN. We present experimental\nresults to demonstrate the effectiveness of the new deep learning based\nsolutions.",
  "text": "1\nImage Companding and Inverse Halftoning using\nDeep Convolutional Neural Networks\nXianxu Hou and Guoping Qiu\nAbstract‚ÄîThis paper presents a deep learning technology for\ntackling two traditional low-level image processing problems,\ncompanding and inverse halftoning. This paper makes two main\ncontributions. First, to the best knowledge of the authors, this\nis the Ô¨Årst work that has successfully developed deep learning\nbased solutions to these two traditional low-level image processing\nproblems. As well as introducing new methods to solving well-\nknown image processing problems, this paper also contributes to\na growing literature that demonstrates the power of deep learning\nin solving traditional signal processing problems. Second, build-\ning on insights into the properties of visual quality of images\nand the internal representation properties of a deep convolutional\nneural network (CNN) and inspired by recent success of deep\nlearning in other image processing applications, this paper has\ndeveloped an effective deep learning method that trains a deep\nCNN as a nonlinear transformation function to map a lower\nbit depth image to higher bit depth or from a halftone image\nto a continuous tone image, and at the same time employs\nanother pretrained deep CNN as a feature extractor to derive\nvisually important features to construct the objective function for\nthe training of the transformation CNN. Extensive experimental\nresults are presented to show that the new deep learning based\nsolution signiÔ¨Åcantly outperforms previous methods and achieves\nnew state-of-the-art results.\nIndex Terms‚ÄîImage Companding, Inverse Halftoning, CNNs,\nPerceptual Loss.\nI. INTRODUCTION\nC\nOMPANDING is a process of compression and then\nexpanding, allowing signals with a higher dynamic range\nto be transmitted with a lower dynamic range by reducing the\nnumber of bits. This technique is widely used in telecom-\nmunication and signal processing such as audio processing.\nFor image processing, companding could be regarded as an\nencoding and decoding framework. The encoding or quantized\ncompression process, while fairly simple and efÔ¨Åcient, could\nalso produce a lot of undesirable artifacts, such as blocking\nartifacts, contouring and ringing effects (see Fig. 1). These\ndegraded artifacts become more obvious with lower bit quan-\ntization.\nInverse halftoning, another similar image processing prob-\nlem considered in this paper, is the inversed process for\nhalftoning. Halftone images are binary images served as ana-\nlog representation and widely used in digital image printing,\ntrying to convey the illusion of having a higher number of bit\nX. Hou is with the School of Computer Science, University of Nottingham\nNingbo China and a visiting student in the College of Information Engineer-\ning, Shenzhen University, China. email: xianxu.hou@nottingham.edu.cn\nG. Qiu is with the College of Information Engineering, Shenzhen Uni-\nversity, China and with the School of Computer Science, the Univer-\nsity of Nottingham, United Kingdom. email: qiu@szu.edu.cn and guop-\ning.qiu@nottingham.ac.uk\nlevels (continuous-tone) to maintain the overall structure of\nthe original images. As a result, distortions will be introduced\nto the halftone images due to a considerable amount of\ninformation being discarded. Inverse halftoning, on the other\nhand, addresses the problem of recovering a continuous-tone\nimage from the corresponding halftoned version. This inversed\nprocess is needed since typical image processing techniques\nsuch as compression and scaling can be successfully applied to\ncontinuous-tone images but very difÔ¨Åcult to halftone images.\nHowever, these two problems are ill-posed considering that\nthere could be an inÔ¨Ånite number of possible solutions. They\nare essentially one-to-many mappings and the input image\ncould be transformed into an arbitrary number of plausible\noutputs even if the compression and halftone methods are\nknown in advance. Solving both problems requires to Ô¨Ånd a\nway to estimate and add more information into the images that\ndo not exist. There are no well-deÔ¨Åned mathematic functions\nor guidelines to describe the mappings to produce high-quality\nimages.\nIn this paper, we take advantage of the recent development\nin machine learning, in particular deep convolutional neural\nnetworks (CNNs), which have become the state-of-the-art\nworkforce for most computer vision tasks [1], [2]. Unlike\nprevious human-engineered methods [3], [4], [5], [6], we\nformulate the two image processing problems, i.e., compand-\ning and inverse halftoning, from the perspective of machine\nlearning. We train deep convolutional neural networks as non-\nlinear mapping functions in a supervised manner to expand\nimages from a lower bit depth to a higher bit depth to reduce\nartifacts in image companding and to produce continuous-tone\nimages in inverse halftoning. Moreover, we also investigate\nthe effect to construct loss functions based on different level\nconvolutional layers, which have shown different properties\nwhen applying an inverting processing to reconstruct the\nencoded images [7].\nOur core contributions in this work are two folds. Firstly,\nto the best knowledge of the authors, this is the Ô¨Årst work\nthat has successfully developed deep learning based solutions\nto these two traditional image processing problems. This not\nonly introduces new methods to tackle well-known image\nprocessing problems but also contributes to the literature that\ndemonstrates the power of deep learning in solving traditional\nsignal processing problems. Secondly, building on insights\ninto the properties of visual quality of images and the hidden\nrepresentation properties of deep CNNs, and also inspired by\nrecent works that use deep CNNs in other image processing\napplications [40], [42], [43], we take full advantage of the\nconvolutional neural networks both in the nonlinear mapping\narXiv:1707.00116v2  [cs.CV]  21 Jul 2017\n2\nùìõconv2_1\nùìõconv3_1\nùìõconv4_1\nPretrained convolutional neural network  \nùìõconv1_1\nTarget\nHigher-¬≠‚Äêbit ¬†outputs\nLower-¬≠‚Äêbit ¬†inputs\nTransformation ¬†network\nInput\nT(x)\nùìõconv5_1\nFig. 1. Method overview. A transformation convolutional neural network (CNN) to expand lower-bit images to higher-bit ones. A pretrained deep CNN for\nconstructing perceptual loss to train the transformation network.\nfunctions and in the neural networks loss functions for low-\nlevel image processing problems. We not only use a deep CNN\nas a nonlinear transformation function to map a low bit depth\nimage to a higher bit depth image or from a halftone image to\na continuous tone image, but also employ another pre-trained\ndeep CNN as a feature extractor or convolutional spatial Ô¨Ålter\nto derive visually important features to construct the objective\nfunction for the training of the transformation neural network.\nThrough these two low-level image processing case studies,\nwe demonstrate that a properly trained deep CNN can capture\nthe spatial correlations of pixels in a local region and other\nvisually important information, which can be used to help a\ndeep CNN to infer the ‚Äúcorrect‚Äù values of pixels and their\nneighbors. Our work further demonstrates that halftone images\nand heavily compressed low bit depth images, even though\nshowing visually annoying artifacts, they have preserved the\noverall structures of the images which are sufÔ¨Åcient to enable\ndeep neural networks to recover the original signals to a high\ndegree of Ô¨Ådelity.\nII. RELATED WORKS\nA. Image Companding\nCompanding, a combination of the words compressing\nand expanding, is a signal processing technique to allow\nsignals with a large dynamic range transmitted in a smaller\ndynamic range format. This technique is widely used in digital\ntelephony systems. Image companding [8], [9], [3] is designed\nto squeeze higher-bit images to lower bit ones, based on which\nto reproduce outputs with higher bits. Multi-scale subband\narchitecture [3] successfully compressed high dynamic range\n(HDR) images to displayable low dynamic range (LDR) ones.\nThey also demonstrated that the compression process can\nbe inverted by following the similar scheme as the previous\ncompression. As a result, low dynamic range images can be\nexpanded to approximate the original higher-bit ones with\nminimal degradation.\nB. Halftoning and Inverse Halftoning\nThe typical digital halftoning process is considered as a\ntechnique of converting a continuous-tone grayscale image\nwith 255 color levels (8 bits) into a binary black-and-white\nimage with only 0 and 1 two color levels (1 bit). These binary\nimages could be reproduced to ‚Äúcontinuous-tone‚Äù images\nfor humans based on an optical illusion that tiny dots are\nblended into smooth tones by human eyes at a macroscopic\nlevel. In this work, we focus on the most popular halftoning\ntechnique known as error diffusion, in which the residual\nquantization error of a pixel is distributed to neighboring\npixels. FloydSteinberg dithering is commonly used by image\nmanipulation software to achieve error diffused halftoning\nbased on a simple kernel. The reversed processing known as\ninverse halftoning is to reconstruct the continuous-tone images\nfrom halftones. Many approaches to addressing this problem\nhave been proposed in the literature, including non-linear Ô¨Ålter-\ning [5], vector quantization [10], projection onto convex sets\n[11], MAP projection [12], wavelets-based [13], anisotropic\ndiffusion [4], Bayesian-based [14], a method by combining\nlow-pass Ô¨Åltering and super-resolution [15], Look-up table\n[16], sparse representation [17], local learned dictionaries [18]\nand coupled dictionary training [19].\nC. Deep Learning for Image Transformation\nIn this work, we seek to formulate the image companding\nand inverse halftoning as image transformation problems and\nemploy deep convolutional neural networks as non-linear func-\ntions to map input images to output images for different pur-\nposes. Recent deep CNNs have become a common workhorse\nbehind a wide variety of image transformation problems.\nThese problems can be formulated as per-pixel classiÔ¨Åcation\nor regression by deÔ¨Åning low level loss. Semantic segmenta-\ntion methods [20], [21], [22] use fully convolutional neural\nnetworks trained by per-pixel classiÔ¨Åcation loss to predict\ndense scene labels. End-to-end automatic image colorization\ntechniques [23], [24] try to colorize grayscale image based on\nlow level losses. Other works for depth [25], [26] and edge\ndetection [27] are also similar to transform input images to\nmeaningful output images through deep convolutional neural\nnetworks, which are trained with per-pixel classiÔ¨Åcation or re-\ngression loss. However the per-pixel measurement essentially\ntreats the output images as ‚Äúunstructured‚Äù in a sense that each\npixel is independent with all other pixels for a given image.\n3\n256 x 1 x 1\n256 x 2 x 2\n256 x 4 x 4\n256 x 8 x 8\n256 x 16 x 16\n128 x 32 x 32\n64 x 64 x 64\n512 x 2 x 2\n512 x 4 x 4\n512 x 8 x 8\n512 x 16 x 16\n256 x 32 x 32\n128 x 64 x 64\n32 x 128 x 128\n3 x 256 x 256\n3 x 256 x 256\n64 x 128 x 128\nconv ¬†+ ¬†bn ¬†+ ¬†LeakyReLU\ndeconv ¬†+ ¬†bn ¬†+ ¬†ReLU\nFig. 2. The architecture of the transformation networks. The ‚ÄúU-Net‚Äù network is an encoder-decoder with skip connections between the encoder and decoder.\nThe dash-line arrows indicate the features from the encoding layers are directly copied to the decoding layers and form half of the corresponding layers‚Äô\nfeatures.\nConsidering the shortcoming of per-pixel loss, other ‚Äústruc-\ntured‚Äù measurements have been proposed such as structural\nsimilarity index measure (SSIM) [28] and conditional random\nÔ¨Åelds (CRF) [29], which take context into account. These\nkinds of ‚Äústructured‚Äù loss have been successfully applied to\ndifferent image transformation problems. However these mea-\nsurements are human-crafted, the community has successfully\ndeveloped structure loss directly learned from images. Gener-\native adversarial networks (GANs) [30] are able to generate\nhigh-quality images based on adversarial training. Many works\nhave tried to apply GANs in conditional settings such as\ndiscrete labels [31], texts [32] and of course images. Image-\nconditioned GANs involve style transfer [33], inpainting [34],\nframe prediction [35]. In addition, image-to-image translation\nframework [36] based on adversarial loss can effectively\nsynthesize photos under different circumstances.\nAnother way to improve per-pixel loss is to generate images\nby optimizing a perceptual loss which is based on high level\nfeatures extracted from pretrained deep convolutional neural\nnetworks. By optimizing individual deep features [37] and\nmaximizing classiÔ¨Åcation score [38], images can be generated\nfor a better understanding of hidden representations of trained\nCNNs. By inverting convolutional features [39], the colors\nand the rough contours of an image can be reconstructed\nfrom activations in pretrained CNNs. In addition, artistic style\ntransfer [40] can be achieved by jointly optimizing the content\nand style reconstruction loss based on deep features extracted\nfrom pretrained CNNs. A similar method is also used for\ntexture synthesis [41]. Similar strategies are also explored to\nachieve real-time style transfer and super-resolution [42]. Deep\nfeature consistent variational autoencoder [43] is proposed to\ngenerate sharp face images and manipulate facial attributes by\nminimizing the difference between the deep features of the\noutput images and target images.\nIII. METHOD\nIn this work, we propose to use deep convolutional neural\nnetworks with skip connections as non-linear mapping func-\ntions to expand images from a lower bit depth to a higher bit\ndepth. The objective of generating the higher bit depth version\nof the image is to ensure that this image is visually pleasing\nand to capture the essential and visual important properties\nof the original version of the image. Instead of using per-\npixel losses, i.e. measuring pixel-wise difference between the\noutput image and its target (the original) image, we measure\nthe difference between the output image and target image\nbased on the high level features extracted from pretrained\ndeep convolutional neural networks. The key insight is that\nthe pretrained networks have already encoded perceptually\nuseful information we desired, such as the spatial relationship\nbetween pixels nearby. Our system is diagrammatically illus-\ntrated in Fig. 1, which consists of two parts: an autoencoder\ntransformation neural network T(x) to achieve end-to-end\nmapping from an input image to an output image, and a\npretrained neural network Œ¶(x) to deÔ¨Åne the loss function.\nA. Network Architecture\nOur non-linear mappings are deep convolutional neural net-\nworks, which have been demonstrated to have state-of-the-art\nperformances in many computer vision tasks. Successful net-\nwork architecture like AlexNet [1], VGGNet [2] and ResNet\n[44] are designed for high level tasks like image classiÔ¨Åcation\nto output a single label, and they cannot be directly applied\nto image processing problems. Instead, previous works have\nemployed an encoder-decoder architecture [43], [42] to Ô¨Årstly\nencode the input images through several convolutional layers\nuntil a bottleneck layer, followed by a reversed decoding\nprocess to produce the output images. Such encoder-decoder\narchitecture forces all the information to pass through the\nnetworks layer by layer. Thus the Ô¨Ånal generated images\nare produced by higher layers‚Äô features. However for image\nprocessing, the output images can retain a great deal of lower\nlayers‚Äô information of the input images, and it would be better\nto incorporate lower layers features in the decoding process.\nBased on the architecture guidelines of previous work on\n4\nimage segmentation [45], image-to-image translation [36] and\nDCGAN [46], we add skip connections to construct a ‚ÄúU-\nNet‚Äù network to fuse lower layers and higher layers features\nand employ fully convolutions for image transformation.\nThe details of our model are shown in Fig. 2, we Ô¨Årst encode\nthe input image to lower dimension vector by a series of stride\nconvolutions, which consists of 4 x 4 convolution kernels and\n2 x 2 stride in order to achieve its own downsampling. We also\nuse a similar approach for decoding to allow the network to\nlearn its own upsampling by using deconvolutions [20]. Spatial\nbatch normalization [47] is added to stabilize the deep network\ntraining after each convolutional layer except the input layer of\nthe encoder and the last output layer of decoder as suggested in\n[46]. Additionally leaky rectiÔ¨Åed activation (LeakyReLU) and\nReLU are served as non-linear activation functions for encoder\nand decoder respectively. Finally we directly concatenate all\nthe encoding activations to the corresponding decoding layers\nto construct a symmetric ‚ÄúU-Net‚Äù structure [45] to fuse the\nfeatures from both low layers and high layers.\nB. Perceptual Loss\nIt is well known that per-pixel loss for regression and\nclassiÔ¨Åcation is problematic and could produce blurry outputs\nor other visual artifacts. This is because each pixel is regarded\nas an individual object for optimization, resulting in average\noutputs to some degree. A better strategy is to construct\nthe loss by incorporating the spatial correlation information.\nRather than encouraging matching each individual pixels of\ninput and output images, we follow previous works [43], [42],\n[40] to measure the difference between two images at various\ndeep feature levels based on pretrained deep convolutional\nneural networks. We seek to capture the input images‚Äô spatial\ncorrelations by means of convolution operations in the deep\nCNNs.\nWe denote the loss function as L(ÀÜy, y) to measure the\nperceptual difference between two images. As illustrated in\nFig. 1, both the output image ÀÜy = T(x) generated by the\ntransformation network and the corresponding target image y\nare fed into a pretrained deep CNN Œ¶ for feature extraction.\nWe use Œ¶i(y) to represent the hidden representations of image\ny at ith convolutional layer. Œ¶i(x) is a 3D array of shape\n[Ci, Wi, Hi], where Ci is the number of Ô¨Ålters, Wi and Hi\nare the width and height of the given feature map of the ith\nconvolutional layer. The Ô¨Ånal perceptual loss of two images\nat ith layer is the Euclidean distance of the corresponding 3D\narrays as following:\nLi(ÀÜy, y) =\n1\nCiWiHi\nCi\nX\nc=1\nWi\nX\nw=1\nHi\nX\nh=1\n(Œ¶i(ÀÜy)c,w,h ‚àíŒ¶i(y)c,w,h)2\n(1)\nIn fact, above loss still follows the per-pixel manner if we\ntreat the hidden features which are 3D arrays as ‚Äúimages‚Äù\nwith more than 3 color channels. However this kind of loss\nhas already incorporated the spatial correlation information\nbecause the ‚Äúpixels‚Äù in these images are the combinations of\nthe original pixels through convolution operations.\nC. Training Details\nOur implementation uses open source machine learning\nframework Torch [48] and a Nvidia Tesla K40 GPU to\nspeed up training. The pretrained 19-layer VGGNet [2] is\nchosen as the loss network for deep feature extraction which\nis Ô¨Åxed during the training. In addition, due to the similar\nconvolutional architecture, the loss network can be seamlessly\nstacked to our ‚ÄúU-Net‚Äù neural network to achieve end-to-end\ntraining. The training images are of the shape 256√ó256 and we\ntrain our model with a batch size of 16 for 30,000 iterations.\nAdam optimizer [49] is used for stochastic optimization with\na learning rate of 0.0002. For the LeakyReLU in the encoder,\nthe slope of the leak is set to 0.2 in all layers. Additionally\nwe experiment with conv1 1, conv2 1, conv3 1, conv4 1 and\nconv5 1 layers in VGGNet to construct perceptual loss for\ncomparison.\nIV. EXPERIMENTAL RESULTS\nIn our experiments, we use Microsoft COCO dataset [50]\nwhich is a large-scale database containing more than 300,000\nimages as our training images. We resize the training images\nto 256√ó256 as our inputs to train our models. We perform\nexperiments on two image processing problems: image com-\npanding and inverse halftoning.\nA. Image Companding\nOne essential part of image companding is to expand lower\nbit images to higher bit outputs. This technique has been\ninvestigated in the context of high dynamic range (HDR)\nimaging [3], Ô¨Årstly compressing the range of an HDR image\ninto an LDR image, at which point the process is then reversed\nto retrieve the original HDR image.\nSince it is impossible to display a true HDR image with\nmore than 8 bits, we use 8 bit images as our highest bit\ndepth images in the experiments. The 8 bit images are reduced\nby different depths as the lower bit depth images, and then\nexpanded back to 8 bits. Take 4 bit images for example, they\ncan only have 16 different levels for each color channel while\nthere are 256 different levels for 8 bit images. The default\napproach [3] for converting 8 bit images to 4 bit images is\nto divide by 16 to quantize the color level from 256 to 16,\nwhich will be then scaled up to Ô¨Åll the full range of the display.\nMathematically we can use the formula below to easily convert\n8 bit images to different lower bit outputs. This operation\ncan be applied to both grayscale images and color images\nby processing each channel separately.\nIlow = ‚åäIhigh\n2(h‚àíl) ‚åã2(h‚àíl)\n(2)\nwhere the Ilow and Ihigh are the pixel intensity of converted\nlower and higher bit depth images respectively, l and h are the\nbit depth for lower and higher bit depth images.\nWe Ô¨Årst preprocess the training images to different lower-bit\nones as input data, and use the original images as higher-bit\ntargets we want to retrieve. After training, the validation split\nof Microsoft COCO is used for testing. We Ô¨Årst compare the\nresults of different lower-bit input images, and then evaluate\n5\nOriginal\n2 Bit\nExpanded\nFig. 3. Results on color images from Microsoft COCO validation split for blocking and contour artifacts reduction. A pair of compressed 2 bit images and\nthe corresponding expanded ones are shown together. Additionally an enlarged sub-image of each image is given at the bottom for better comparison.\nExpanded\n4 Bit\nExpanded\n2 Bit\nOriginal\nFig. 4. Results on grayscale images from Microsoft COCO validation split for blocking and contour artifacts reduction. A pair of compressed 2 bit and 4\nbit images and the corresponding expanded ones are shown together. Additionally an enlarged sub-image of each image is given at the bottom for better\ncomparison.\n6\n0\n50\n100\n150\n200\n250\nPixel Value\n0\n50000\n100000\n150000\n200000\n250000\nNo. of Pixels\n0\n50\n100\n150\n200\n250\nPixel Value\n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\nNo. of Pixels\n0\n50\n100\n150\n200\n250\nPixel Value\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\nNo. of Pixels\n0\n50\n100\n150\n200\n250\nPixel Value\n0\n5000\n10000\n15000\n20000\n25000\nNo. of Pixels\n0\n50\n100\n150\n200\n250\nPixel Value\n0\n5000\n10000\n15000\n20000\n25000\nNo. of Pixels\n2 Bit image\n4 Bit image\n2 Bit companding image\nOriginal 8 bit image\nOriginal image\n4 Bit companding image\nFig. 5. Intensity histogram of different compressed and expanded images.\nTABLE I\nTHE AVERAGE COMPANDING RESULTS OF PSNR(DB) AND SSIM FOR 100\nCOLOR AND GRAYSCALE IMAGES RANDOMLY SELECTED FROM\nMICROSOFT COCO VALIDATION SPLIT. THE EXPANDED RESULTS WERE\nBASED ON A PERCEPTUAL LOSS CONSTRUCTED USING CONV1 1 LAYER.\nInput Bit-depth\nPSNR\nSSIM\nCompressed\nExpanded\nCompressed\nExpanded\nBit 1\nColor\n11.73\n18.67\n0.40\n0.55\nGrayscale\n11.58\n18.81\n0.35\n0.49\nBit 2\nColor\n17.37\n25.65\n0.67\n0.81\nGrayscale\n17.29\n25.84\n0.61\n0.74\nBit 3\nColor\n23.13\n30.79\n0.85\n0.90\nGrayscale\n23.16\n31.33\n0.78\n0.87\nBit 4\nColor\n29.03\n34.52\n0.94\n0.95\nGrayscale\n29.19\n36.69\n0.90\n0.94\nBit 5\nColor\n34.85\n37.59\n0.98\n0.97\nGrayscale\n35.08\n40.24\n0.96\n0.97\nhow the perceptual loss constructed from different convolu-\ntional layers affects the expanding quality.\n1) Different Bit Depths: We have separately trained models\nfor different lower-bit input images for comparison and use the\nconv1 1 layer of VGGNet to construct the perceptual loss for\nall the models.\nQualitative Results. Fig. 3 and Fig. 4 show the qualitative\nresults for a variety of color and grayscale images taken from\nMicrosoft COCO 2014 validation split. We can see that the\nlinearly quantized lower-bit images display severe blocking\nand contouring artifacts. The compression process ampliÔ¨Åes\nlow amplitudes and high frequencies which dominate the\nquantization artifacts because we try to show a lower dynamic\nrange image on a higher dynamic range displayable device.\nFor instance, our device is appropriate for the original 8 bit\ntargets with 256 color levels. We could drop the bit depths of\nthe original images by 5 bits and linearly quantize them to 3 bit\nimages with only 8 color levels. Since the compressed images\ncontain 5 fewer bits, they should be theoretically displayed\non 1/32 dynamic range device. It is obvious that this kind of\nlossy compression introduces visible artifacts in pixel blocks\nand at block boundaries.\nWe also show the corresponding expanded images (Fig.\n3 and Fig. 4) retrieved from our models. The blocking and\ncontouring artifacts are effectively reduced to show smooth\nappearance in the expanded outputs. For example in the\nairplane image in Fig. 4, the compressed images show obvious\ncontouring artifacts in the sky while the expanded images\nhave homogeneous gradually changing colors. And this can be\nfurther validated from the distribution of intensity histograms.\nFig. 5 shows the intensity histograms for the compressed 2\nand 4 bit airplanes and the expanded ones in Fig. 4. It is\nclear that our methods are able to infer the ‚Äúcorrect‚Äù values\nfor a single pixel based on its neighbors, and convey a more\nattractive impression with rich and saturated colors.\nQuantitative Results. In order to have a comprehensive\nquantitative evaluation for our models, we report peak signal-\nto-noise ratio (PSNR) and structural similarity index mea-\nsure (SSIM) [28] for quality assessment. PSNR is per-pixel\nbased measurement deÔ¨Åned via the mean squared error (MSE)\nwhile SSIM index is known as perceptual-aware method\nfor measuring the similarity between two images. For both\nmeasurements, a higher value indicates better quality. Table\nI summarizes the average PSNR (dB) and SSIM values of\n100 images selected from COCO validation split. Similar to\nqualitative results, the higher bit images have higher PSNR and\nSSIM values, indicating better image quality. Additionally, the\nexpanded images produced by our method have signiÔ¨Åcantly\nhigher PSNR and SSIM values compared to the corresponding\ncompressed ones. It is clear that our method can effectively\nimprove the image quality especially for lower bit depth\nimages.\n2) Perceptual Loss at Different Convolutional Layers: Due\nto the multi-layer architecture of deep convolutional neural\nnetworks, the perceptual loss can be deÔ¨Åned by different\nconvolutional layers. Therefore, we conduct experiments to\ninvestigate the performance for different perceptual losses. In\nall the experiment we use 3 bit depths (8 color levels) as input\nimages to train our deep networks for both color and grayscale\nimages.\nQualitative Results. As shown in Fig. 6, all the expanded\noutputs can effectively reduce the blocking and contouring\nartifacts and reveal continuous-tone results in general. How-\never, the reconstruction based on perceptual loss of higher-\nlevel layers could introduce new artifacts such as grid patterns\nas shown in images for conv3 1 and conv5 1 layers. We\nobserved similar phenomenons for input images of different\nbit-depths. One explanation to this is that, the higher layers\nwill cover a larger area in the input image, and the areas\ncovered by conv3 1 and conv5 1 layers are too large to\nconstruct a natural looking image. That is, spatial correlations\nacross a large area of the image do not capture natural\nappearances of an image. Expanding based on perceptual loss\nof deep features of conv1 1 layer or lower layers does not have\n7\nOriginal\n3 Bit\nconv1_1\nconv3_1\nconv5_1\nFig. 6. Results on color images for blocking and contour artifacts reduction. The compressed images are Ô¨Åxed to 3 bits with 8 color levels for each channel.\nThe conv1 1, conv3 1, conv5 1 are the expanded results produced by the models trained with perceptual loss constructed by corresponding convolutional\nlayers. Additionally an enlarged sub-image of each image is given at the bottom for better comparison.\nthis kind of artifacts. This could be also validated by previous\nwork [7] that tries to compute an approximate inverse image\nfrom its deep features. It shows that the Ô¨Årst few layers in a\npretrained CNN are essentially an invertible code of the image\nand maintain a photographically faithful representations, and\nthe higher level features are corresponding to a more coarse\nspace area of the encoded image.\nQuantitative Results. Table II shows the average PSNR\nand SSIM values for 100 COCO testing images based on per-\nceptual losses constructed with different convolutional layers.\nOn the one hand, both the PSNR and SSIM of our expanded\nimages are much higher than those of the compressed lower\nbit images, and the compressed images can be signiÔ¨Åcantly\nimproved by our method. On the other hand, the expanded\nimages based on perceptual losses of lower level layers have\nhigher PSNR and SSIM values. This is because new artifacts\nlike grid pattern will be introduced (Fig. 6) although the\nblocking artifacts can be reduced.\nB. Inverse halftoning\nAnother similar image processing problem we are interested\nin is inverse halftoning. This task is to generate a continuous-\ntone image from halftoned binary images. This problem is\nalso inherently ill-posed since there could exist multiple\ncontinuous-tone images corresponding to the halftoned ones.\nIn our experiments we try to use deep feature based perceptual\nloss to allow the inversed halftones perceptually similar to the\nTABLE II\nTHE AVERAGE COMPANDING RESULTS OF PSNR(DB) AND SSIM FOR 100\nCOLOR AND GRAYSCALE TESTING IMAGES. THE COMPRESSED INPUT\nIMAGES ARE 3 BITS, AND THE EXPANDED RESULTS BASED ON\nPERCEPTUAL LOSS CONSTRUCTED WITH DIFFERENT CONVOLUTIONAL\nLAYERS ARE SHOWN.\nPerceptual\nLoss Layer\nPSNR\nSSIM\nCompressed\nExpanded\nCompressed\nExpanded\nConv1\nColor\n23.13\n32.64\n0.85\n0.93\nGrayscale\n23.16\n32.57\n0.78\n0.91\nConv2\nColor\n23.13\n30.00\n0.85\n0.88\nGrayscale\n23.16\n30.74\n0.78\n0.85\nConv3\nColor\n23.13\n28.17\n0.85\n0.87\nGrayscale\n23.16\n29.60\n0.78\n0.81\nConv4\nColor\n23.13\n25.74\n0.85\n0.84\nGrayscale\n23.16\n29.54\n0.78\n0.82\nConv5\nColor\n23.13\n25.43\n0.85\n0.87\nGrayscale\n23.16\n27.50\n0.78\n0.77\ngiven targets. We experiment with both color and grayscale\nimages by using the same approach and employ error diffusion\nbased FloydSteinberg dithering for halftoning.\nQualitative Results. We test our models on random samples\nof images from Microsoft COCO validation split. The inverse\nhalftoning results are shown in Fig. 7. We can see that\nthe inversed outputs produced by our method are visually\nsimilar to the original images. All the outputs can show much\nsmoother textures and produce sharper edges. For instance,\nsharp kite line and smooth sky can be reconstructed in the kite\nimage. When comparing with the inversed outputs produced by\nusing perceptual loss of different level layers, the outputs from\n8\nOriginal\nHalftone\nconv1_1\nconv5_1\nFig. 7. Inverse halftoning results on images from Microsoft COCO validation split. The conv1 1, conv5 1 are the results produced by the models trained by\nthe perceptual losses of corresponding convolutional layers. Additionally an enlarged sub-image of each image is given at the bottom for better comparison.\nlower-level layer is visually better than those from higher-\nlevel layer. Like image companding, grid pattern artifacts\ncan be introduced when using higher-level layer to construct\nperceptual loss.\nIn addition, we also compare our method on two widely\nused grayscale images Lenna and Peppers with other algo-\nrithms. Fig. 8 shows comparative grayscale results against pre-\nvious Fastiht2 [4] and Wavelet-based WInHD [13] algorithms.\nWe also report the PSNR / SSIM measurement for each image.\nIt is clear that our learning-based method can achieve state-of-\nthe-art results and produce sharp edges and Ô¨Åne details, such\nas the hat in the Lenna image. Our deep models can effectively\nTABLE III\nTHE AVERAGE INVERSED HALFTONING RESULTS OF PSNR(DB) AND\nSSIM FOR 100 COLOR AND GRAYSCALE IMAGES SELECTED FROM\nMICROSOFT COCO VALIDATION SPLIT.\nPerceptual\nLoss Layer\nPSNR\nSSIM\nHalftone\nCNN Inverse\nHalftone\nCNN Inverse\nConv1\nColor\n8.08\n31.43\n0.20\n0.91\nGrayscale\n7.92\n31.36\n0.14\n0.90\nConv2\nColor\n8.08\n20.98\n0.20\n0.59\nGrayscale\n7.92\n23.98\n0.14\n0.67\nConv3\nColor\n8.08\n24.05\n0.20\n0.73\nGrayscale\n7.92\n27.44\n0.14\n0.74\nConv4\nColor\n8.08\n26.48\n0.20\n0.85\nGrayscale\n7.92\n27.82\n0.14\n0.76\nConv5\nColor\n8.08\n25.47\n0.20\n0.84\nGrayscale\n7.92\n26.48\n0.14\n0.69\n9\nPSNR / SSIM\n6.71 / 0.40 \n31.37 / 0.85 \n31.77 / 0.86\n33.85 / 0.90\nOriginal\nHalftone\nFastiht2\nWInHD\nCNN Inverse\nPSNR / SSIM\n6.93 / 0.05 \n31.46 / 0.83 \n31.05 / 0.84\n33.22 / 0.87\nOriginal\nHalftone\nFastiht2\nWInHD\nCNN Inverse\nFig. 8. A comparison of inverse halftoning results on grayscale Lena and Peppers images by different methods. We compare our CNN Inverse method with\nthose of Fastiht2 [4] and Wavelet-based WInHD [13]. We report PSNR / SSIM for each example.\nand correctly learn the relevant spatial correlation and semantic\nbetween different pixels and infer the ‚Äúbest‚Äù values for a single\npixel based on its neighbors. Moreover, our method can be\nnaturally adapted to color images and produce high-quality\ncontinuous-tone color images from corresponding halftones.\nFig. 9 shows the resulting images for the Koala and Cactus\nimage, which include Ô¨Åne textures and structures. We compare\nour results (CNN Inverse) with those of two recent methods\nGLDP [17] and LLDO [18]. We can see that our method can\nprovide better resulting images with well expressed fur and\nbark in the Koala image, and distinct boundaries of the Ô¨Åne\nsand and sharpened edges of splines in the Cactus image.\nQuantitative Results. We use PSNR and SSIM as quality\nmetrics to quantitatively evaluate our inverse halftoning re-\nsults. Table III shows the average PSNR and SSIM values\nfor 100 COCO testing images constructed from different\nconvolutional layers. It is clear that based on these image\nevaluation metrics, our method can improve the images by\na large margin for both color and grayscale images. In our\nexperiment, the best results are produced by the model trained\nwith conv1 1 layer. When using perceptual loss based on\nhigher layers gives rise to a slight grid pattern artifacts visible\nunder magniÔ¨Åcation, which harms the PSNR and SSIM.\nMoreover, we conduct experiments to compare with several\nprevious methods. We use 6 images Koala, Cactus, Bear,\nBarbara, Shop and Peppers, the same as [18] for testing.\nTable IV shows the PSNR and SSIM results for conventional\nmethods based on MAP estimation [12], ALF [4], LPA-ICI\n[51] and recent GLDP [17] and LLDO [18]. We can see that\nour algorithm (CNN Inverse) can achieve new state-of-the-\nart results and signiÔ¨Åcantly outperform previous methods for\ninverse halftoning.\n10\nPSNR / SSIM\n8.17 / 0.25\n24.58 / 0.78\n25.01 / 0.80\n27.63 / 0.89\nCNN Inverse\nLLDO\nGLDP\nHalftone\nOriginal\nPSNR / SSIM\n6.83 / 0.23\n25.40 / 0.81\n25.55 / 0.82\n27.69 / 0.92\nCNN Inverse\nLLDO\nGLDP\nHalftone\nOriginal\nFig. 9. A comparison of inverse halftoning results on color Koala and Cactus images by different methods. We compare our CNN Inverse method with those\nof GLDP [17] and LLDO [18]. We report PSNR / SSIM for each example.\nTABLE IV\nPSNR (DB) AND SSIM COMPARISON OF DIFFERENT INVERSE HALFTONING METHODS FOR COLOR IMAGES: MAP [12], ALF [4], LPA-ICI [51], GLDP\n[17], LLDO [18] AND OUR CNN INVERSE.\nImage\nALF\nMAP\nLPA-ICI\nGLDP\nLLDO\nCNN Inverse\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR\nSSIM\nPSNR\nSSIM\nKoala\n22.36\n0.66\n23.33\n0.74\n24.17\n0.76\n24.58\n0.78\n25.01\n0.80\n27.63\n0.89\nCactus\n22.99\n0.64\n23.95\n0.77\n25.04\n0.79\n25.40\n0.81\n25.55\n0.82\n27.69\n0.92\nBear\n21.82\n0.62\n22.63\n0.72\n23.14\n0.72\n23.66\n0.77\n24.17\n0.78\n26.35\n0.89\nBarbara\n25.41\n0.71\n26.24\n0.78\n27.88\n0.83\n27.12\n0.80\n28.48\n0.85\n31.79\n0.92\nShop\n22.14\n0.64\n22.46\n0.69\n24.12\n0.77\n23.86\n0.75\n24.61\n0.80\n27.27\n0.89\nPeppers\n30.92\n0.87\n28.25\n0.77\n30.70\n0.87\n30.92\n0.87\n31.07\n0.87\n31.44\n0.89\nV. DISCUSSION\nImage companding and inverse halftoning are two similar\nimage processing problems in the sense that they attempt to\nuse a lower bit depth image to represent a higher bit depth\nversion of the same image. The naive bit depth compression\nin image companding is directly applying image quantization\ntechnique. It can retain the overall structure and color contrast,\nhowever blocking and contouring artifacts will be introduced\nthat make the compressed images look unnatural with visually\nannoying artifacts. Halftone images try to simulate continuous-\ntone imagery through the use of dots with only two color\nlevels per channel. The reproduction of halftones for humans\nrelies on an optical illusion that tiny halftone dots could be\nblended into smooth tones by human eyes. In order to expand\n11\nthe compressed images and inverse the halftones, traditional\nmethods usually need to design expanding and inverse oper-\nators manually. For example, the halftone technique such as\nthe speciÔ¨Åc dithering algorithms should be given in advance in\norder to design an inverse operator. In this paper, we show that\na learning based method can formulate the two problems in\nthe same framework and a perceptual loss based on pretrained\ndeep networks can be used to guide the training. This paper\ndemonstrates that deep convolutional neural networks can not\nonly be applied to high-level vision problems like image\nclassiÔ¨Åcation, but also to traditional low-level vision problems.\nAlthough we can use popular metrics like PSNR and SSIM to\nquantitatively measure the image quality, it is worth pointing\nout that the assessment of image quality is still a challenging\nproblem. PSNR and SSIM could correlate poorly with human\nassessment of visual quality and further works are needed for\nperceptually better image measurement.\nVI. CONCLUSION\nIn this paper, we propose to train deep convolutional\nneural networks with a perceptual loss for two low-level\nimage processing problems: image companding and inverse\nhalftoning. Our method is very effective in dealing with\ncompressed blocking and contouring artifacts for compand-\ning and reproduces state-of-the-art continuous-tone outputs\nfrom binary halftone images. In addition, we systematically\ninvestigated how the perceptual loss constructed with different\nconvolutional layers of the pretrained deep network affects the\ngenerated image quality.\nREFERENCES\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation\nwith deep convolutional neural networks,‚Äù in Advances in neural infor-\nmation processing systems, 2012, pp. 1097‚Äì1105.\n[2] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for\nlarge-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556, 2014.\n[3] Y. Li, L. Sharan, and E. H. Adelson, ‚ÄúCompressing and companding high\ndynamic range images with subband architectures,‚Äù in ACM transactions\non graphics (TOG), vol. 24, no. 3.\nACM, 2005, pp. 836‚Äì844.\n[4] T. D. Kite, N. Damera-Venkata, B. L. Evans, and A. C. Bovik, ‚ÄúA fast,\nhigh-quality inverse halftoning algorithm for error diffused halftones,‚Äù\nIEEE Transactions on Image Processing, vol. 9, no. 9, pp. 1583‚Äì1592,\n2000.\n[5] M.-Y. Shen and C.-C. J. Kuo, ‚ÄúA robust nonlinear Ô¨Åltering approach\nto inverse halftoning,‚Äù Journal of Visual Communication and Image\nRepresentation, vol. 12, no. 1, pp. 84‚Äì95, 2001.\n[6] G. R. Easley, V. M. Patel, and D. M. Healy Jr, ‚ÄúInverse halftoning using\na shearlet representation,‚Äù in SPIE Optical Engineering+ Applications.\nInternational Society for Optics and Photonics, 2009, pp. 74 460C‚Äì\n74 460C.\n[7] A. Mahendran and A. Vedaldi, ‚ÄúUnderstanding deep image represen-\ntations by inverting them,‚Äù in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2015, pp. 5188‚Äì5196.\n[8] B. Yang, M. Schmucker, W. Funk, C. Busch, and S. Sun, ‚ÄúInteger dct-\nbased reversible watermarking for images using companding technique,‚Äù\nin Electronic Imaging 2004.\nInternational Society for Optics and\nPhotonics, 2004, pp. 405‚Äì415.\n[9] S. Bhooshan, V. Kumar, and H. Solan, ‚Äú2d t-law: a novel approach\nfor image companding,‚Äù in Proceedings of the 4th WSEAS international\nconference on Circuits, systems, signal and telecommunications. World\nScientiÔ¨Åc and Engineering Academy and Society (WSEAS), 2010, pp.\n19‚Äì22.\n[10] M. Y. Ting and E. A. Riskin, ‚ÄúError-diffused image compression using a\nbinary-to-gray-scale decoder and predictive pruned tree-structured vector\nquantization,‚Äù IEEE Transactions on Image Processing, vol. 3, no. 6, pp.\n854‚Äì858, 1994.\n[11] G. B. Unal and A. E. C¬∏ etin, ‚ÄúRestoration of error-diffused images using\nprojection onto convex sets,‚Äù IEEE Transactions on Image Processing,\nvol. 10, no. 12, pp. 1836‚Äì1841, 2001.\n[12] R. L. Stevenson, ‚ÄúInverse halftoning via map estimation,‚Äù IEEE Trans-\nactions on Image Processing, vol. 6, no. 4, pp. 574‚Äì583, 1997.\n[13] R. Neelamani, R. D. Nowak, and R. G. Baraniuk, ‚ÄúWinhd: Wavelet-\nbased inverse halftoning via deconvolution,‚Äù IEEE Transactions on\nImage Processing, 2002.\n[14] Y.-F. Liu, J.-M. Guo, and J.-D. Lee, ‚ÄúInverse halftoning based on the\nbayesian theorem,‚Äù IEEE Transactions on Image Processing, vol. 20,\nno. 4, pp. 1077‚Äì1084, 2011.\n[15] Y. Minami, S.-i. Azuma, and T. Sugie, ‚ÄúInverse halftoning using super-\nresolution image processing,‚Äù IEEJ Transactions on Electrical and\nElectronic Engineering, vol. 7, no. 2, pp. 208‚Äì213, 2012.\n[16] M. Mese and P. P. Vaidyanathan, ‚ÄúLook-up table (lut) method for inverse\nhalftoning,‚Äù IEEE Transactions on Image Processing, vol. 10, no. 10,\npp. 1566‚Äì1578, 2001.\n[17] C.-H. Son, ‚ÄúInverse halftoning based on sparse representation,‚Äù Optics\nletters, vol. 37, no. 12, pp. 2352‚Äì2354, 2012.\n[18] C.-H. Son and H. Choo, ‚ÄúLocal learned dictionaries optimized to\nedge orientation for inverse halftoning,‚Äù IEEE Transactions on Image\nProcessing, vol. 23, no. 6, pp. 2542‚Äì2556, 2014.\n[19] P. G. Freitas, M. C. Farias, and A. P. Ara¬¥ujo, ‚ÄúEnhancing inverse\nhalftoning via coupled dictionary training,‚Äù Signal Processing: Image\nCommunication, vol. 49, pp. 1‚Äì8, 2016.\n[20] J. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional networks\nfor semantic segmentation,‚Äù in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2015, pp. 3431‚Äì3440.\n[21] D. Eigen and R. Fergus, ‚ÄúPredicting depth, surface normals and se-\nmantic labels with a common multi-scale convolutional architecture,‚Äù in\nProceedings of the IEEE International Conference on Computer Vision,\n2015, pp. 2650‚Äì2658.\n[22] H. Noh, S. Hong, and B. Han, ‚ÄúLearning deconvolution network\nfor semantic segmentation,‚Äù in Proceedings of the IEEE International\nConference on Computer Vision, 2015, pp. 1520‚Äì1528.\n[23] S. Iizuka, E. Simo-Serra, and H. Ishikawa, ‚ÄúLet there be color!: joint\nend-to-end learning of global and local image priors for automatic image\ncolorization with simultaneous classiÔ¨Åcation,‚Äù ACM Transactions on\nGraphics (TOG), vol. 35, no. 4, p. 110, 2016.\n[24] G. Larsson, M. Maire, and G. Shakhnarovich, ‚ÄúLearning representations\nfor automatic colorization,‚Äù in European Conference on Computer\nVision.\nSpringer, 2016, pp. 577‚Äì593.\n[25] D. Eigen, C. Puhrsch, and R. Fergus, ‚ÄúDepth map prediction from a\nsingle image using a multi-scale deep network,‚Äù in Advances in neural\ninformation processing systems, 2014, pp. 2366‚Äì2374.\n[26] F. Liu, C. Shen, and G. Lin, ‚ÄúDeep convolutional neural Ô¨Åelds for depth\nestimation from a single image,‚Äù in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2015, pp. 5162‚Äì5170.\n[27] S. Xie and Z. Tu, ‚ÄúHolistically-nested edge detection,‚Äù in Proceedings\nof the IEEE International Conference on Computer Vision, 2015, pp.\n1395‚Äì1403.\n[28] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage\nquality assessment: from error visibility to structural similarity,‚Äù IEEE\ntransactions on image processing, vol. 13, no. 4, pp. 600‚Äì612, 2004.\n[29] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n‚ÄúSemantic image segmentation with deep convolutional nets and fully\nconnected crfs,‚Äù arXiv preprint arXiv:1412.7062, 2014.\n[30] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets,‚Äù in\nAdvances in neural information processing systems, 2014, pp. 2672‚Äì\n2680.\n[31] M. Mirza and S. Osindero, ‚ÄúConditional generative adversarial nets,‚Äù\narXiv preprint arXiv:1411.1784, 2014.\n[32] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,\n‚ÄúGenerative adversarial text to image synthesis,‚Äù in Proceedings of The\n33rd International Conference on Machine Learning, vol. 3, 2016.\n[33] C. Li and M. Wand, ‚ÄúPrecomputed real-time texture synthesis with\nmarkovian generative adversarial networks,‚Äù in European Conference\non Computer Vision.\nSpringer, 2016, pp. 702‚Äì716.\n[34] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros,\n‚ÄúContext encoders: Feature learning by inpainting,‚Äù in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2016,\npp. 2536‚Äì2544.\n[35] M. Mathieu, C. Couprie, and Y. LeCun, ‚ÄúDeep multi-scale video\nprediction beyond mean square error,‚Äù arXiv preprint arXiv:1511.05440,\n2015.\n12\n[36] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, ‚ÄúImage-to-image\ntranslation with conditional adversarial networks,‚Äù arXiv preprint\narXiv:1611.07004, 2016.\n[37] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, ‚ÄúUnder-\nstanding neural networks through deep visualization,‚Äù arXiv preprint\narXiv:1506.06579, 2015.\n[38] K. Simonyan, A. Vedaldi, and A. Zisserman, ‚ÄúDeep inside convolutional\nnetworks: Visualising image classiÔ¨Åcation models and saliency maps,‚Äù\narXiv preprint arXiv:1312.6034, 2013.\n[39] A. Dosovitskiy and T. Brox, ‚ÄúInverting visual representations with\nconvolutional networks,‚Äù in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 4829‚Äì4837.\n[40] L. A. Gatys, A. S. Ecker, and M. Bethge, ‚ÄúA neural algorithm of artistic\nstyle,‚Äù arXiv preprint arXiv:1508.06576, 2015.\n[41] L. Gatys, A. S. Ecker, and M. Bethge, ‚ÄúTexture synthesis using convolu-\ntional neural networks,‚Äù in Advances in Neural Information Processing\nSystems, 2015, pp. 262‚Äì270.\n[42] J. Johnson, A. Alahi, and L. Fei-Fei, ‚ÄúPerceptual losses for real-\ntime style transfer and super-resolution,‚Äù in European Conference on\nComputer Vision.\nSpringer, 2016, pp. 694‚Äì711.\n[43] X. Hou, L. Shen, K. Sun, and G. Qiu, ‚ÄúDeep feature consistent\nvariational autoencoder,‚Äù in Applications of Computer Vision (WACV),\n2017 IEEE Winter Conference on.\nIEEE, 2017, pp. 1133‚Äì1141.\n[44] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 770‚Äì778.\n[45] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convolutional net-\nworks for biomedical image segmentation,‚Äù in International Conference\non Medical Image Computing and Computer-Assisted Intervention.\nSpringer, 2015, pp. 234‚Äì241.\n[46] A. Radford, L. Metz, and S. Chintala, ‚ÄúUnsupervised representation\nlearning with deep convolutional generative adversarial networks,‚Äù arXiv\npreprint arXiv:1511.06434, 2015.\n[47] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,‚Äù arXiv preprint\narXiv:1502.03167, 2015.\n[48] R. Collobert, K. Kavukcuoglu, and C. Farabet, ‚ÄúTorch7: A matlab-like\nenvironment for machine learning,‚Äù in BigLearn, NIPS Workshop, no.\nEPFL-CONF-192376, 2011.\n[49] D. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù\narXiv preprint arXiv:1412.6980, 2014.\n[50] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in\ncontext,‚Äù in European Conference on Computer Vision. Springer, 2014,\npp. 740‚Äì755.\n[51] A. Foi, V. Katkovnik, K. Egiazarian, and J. Astola, ‚ÄúInverse halftoning\nbased on the anisotropic lpa-ici deconvolution,‚Äù in Proceedings of Int.\nTICSP Workshop Spectral Meth. Multirate Signal Process, 2004.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-07-01",
  "updated": "2017-07-21"
}