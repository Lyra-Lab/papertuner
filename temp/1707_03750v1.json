{
  "id": "http://arxiv.org/abs/1707.03750v1",
  "title": "DeepProf: Performance Analysis for Deep Learning Applications via Mining GPU Execution Patterns",
  "authors": [
    "Jiazhen Gu",
    "Huan Liu",
    "Yangfan Zhou",
    "Xin Wang"
  ],
  "abstract": "Deep learning applications are computation-intensive and often employ GPU as\nthe underlying computing devices. Deep learning frameworks provide powerful\nprogramming interfaces, but the gap between source codes and practical GPU\noperations make it difficult to analyze the performance of deep learning\napplications. In this paper, through examing the features of GPU traces and\ndeep learning applications, we use the suffix tree structure to extract the\nrepeated patten in GPU traces. Performance analysis graphs can be generated\nfrom the preprocessed GPU traces. We further present \\texttt{DeepProf}, a novel\ntool to automatically process GPU traces and generate performance analysis\nreports for deep learning applications. Empirical study verifies the\neffectiveness of \\texttt{DeepProf} in performance analysis and diagnosis. We\nalso find out some interesting properties of Tensorflow, which can be used to\nguide the deep learning system setup.",
  "text": "DeepProf: Performance Analysis for Deep Learning\nApplications via Mining GPU Execution Patterns\nJiazhen Gu∗, Huan liu∗, Yangfan Zou∗and Xin Wang∗\n∗School of Computer Science\nFudan University, Shanghai, China\nEmail:{16210240029, 1521024081, zyf, xinw}@fudan.edu.cn\nAbstract—Deep\nlearning\napplications\nare\ncomputation-\nintensive and often employ GPU as the underlying computing\ndevices. Deep learning frameworks provide powerful program-\nming interfaces, but the gap between source codes and practical\nGPU operations make it difﬁcult to analyze the performance of\ndeep learning applications. In this paper, through examing the\nfeatures of GPU traces and deep learning applications, we use the\nsufﬁx tree structure to extract the repeated patten in GPU traces.\nPerformance analysis graphs can be generated from the prepro-\ncessed GPU traces. We further present DeepProf, a novel tool\nto automatically process GPU traces and generate performance\nanalysis reports for deep learning applications. Empirical study\nveriﬁes the effectiveness of DeepProf in performance analysis\nand diagnosis. We also ﬁnd out some interesting properties of\nTensorﬂow, which can be used to guide the deep learning system\nsetup.\nI. INTRODUCTION\nRecently machine learning techniques, especially deep\nlearning [1], have been proven effective to many sophisti-\ncated applications like audio recognition [2], natural language\nprocessing [3] and computer vision [4]. Many handy soft-\nware frameworks, e.g., Tensorﬂow [5], have been proposed\nto improve the development efﬁciency of deep learning al-\ngorithms. With such tools, deep learning applications can\nbe implemented with quite simple and concise codes. For\nexample, with Tensorﬂow, a neural network to perform high-\nperformance image recognition task can be realized in less\nthan 100 lines of codes. Short as they are, such programs still\nrequire a long execution time. They are computation-intensive\nin nature, which are generally designed to process tremendous\ninput data, and may incur lots of iterative runs. Bad design\ncan result in intolerable execution time, which may even lead\nto their failed application in real-world scenarios. Therefore,\nexecution efﬁciency is a critical concern of such programs.\nHowever, analyzing the performance of deep learning ap-\nplications is difﬁcult. A key reason is that concise codes do\nnot indicate the simplicity of the underlying computation.\nActually, to get the best computational performance, deep\nlearning applications usually employ NVIDIA GPUs (Graph-\nics Processing Unit) as underlying computing devices [6].\nDeep learning frameworks, like Tensorﬂow, provide simple\nupper-layer APIs (Application Program Interface) that wrap a\nsequence of complicated underlying GPU operations. When\nexecuting a deep learning application, the source codes will\nbe optimized and transformed into concurrent GPU operations\n[7]. This may lead to a great gap between the codes written\nby developers and the operations actually executed. Therefore,\nthe underlying execution details of applications are concealed\nfrom developers as well as potential performance issues, as a\nprice for the convenience of programming.\nSuch a complex framework realization, along with the\nmassive data transfer between host and GPUs, can be too\ncomplicated to comprehend even for an experienced machine\nlearning developer. Moreover, current performance analysis\ntools are generally designed for applications running on CPU\nand cannot be used for GPU-based deep learning applications\nbecause the execution mechanism of GPU is different from\nthat of CPU [6]. For example, the calls to CPU executions\nare blocked while computing operations are asynchronous on\nGPU, thus, CPU proﬁling cannot reﬂect the real execution\nprocedure of GPU operations, not to mention performance\nanalysis. On the other hand, Nvidia provides tools for GPU\ntraces proﬁling [8], but the results can hardly be understood\nby deep learning developers, who are typically unfamiliar with\nGPU operations. Therefore, how to bridge the execution gap\nbetween the program codes and the underlying execution, and\nto analyze the performance of deep learning applications is\nyet to be well-addressed.\nIn this paper, we ﬁrst analyze the features of Tensorﬂow\nprograms and the corresponding GPU traces. Based on the\nfeatures, we summarize some execution rules of Tensorﬂow\napplications. We then use the rules to bridge the execution gap\nthrough recognizing repetition patterns in GPU traces. Since\nthe size of GPU traces is generally large, we leverage sufﬁx-\ntree structure [9] to partition the unordered GPU operations\nin O(n) space. By doing this, we simplify the tedious GPU\ntraces into the granularity of iteration, since deep learning\napplications are typically iterative programs. Based on the\nsecond generated GPU proﬁles, we generate performance\nanalysis graphs to help ﬁnding potential performance issues.\nWe implement DeepProf (Deep learning Proﬁler) to au-\ntomatically process GPU traces and generate performance\nanalysis of deep learning applications. Through DeepProf,\nwe further summarize several execution properties of Tensor-\nﬂow applications. In particular, we proﬁle different Tensorﬂow\napplications under different GPU devices with DeepProf.\nThe results reveal some potential GPU usage properties of\nTensorﬂow applications, which can be used as guidance for\nsystem setup.\narXiv:1707.03750v1  [cs.SE]  12 Jul 2017\nThe rest of this paper is organized as follows. We introduce\nsome preliminary knowledge on machine learning and deep\nlearning frameworks in Section II, using Tensorﬂow as a\ntypical example. Section III illustrates a motivating example.\nIn Section IV, we analyze the execution details of Tensorﬂow\napplications and elaborate how to extract patterns from GPU\ntraces. We also describe the implementation of DeepProf in\nthis section. Case studies are discussed in Section V. Section\nVI presents related work. Finally, we conclude the paper in\nSection VII.\nII. BACKGROUND KNOWLEDGE\nIn this section, we describe some preliminary knowledge\non machine learning applications. First, we introduce the main\nprocedure of machine learning applications. Then we illustrate\nhow to use Tensorﬂow, a typical deep learning framework, to\nexecute the program on GPUs.\nA. Machine Learning Applications\nMachine learning is a ﬁeld that focuses on giving ”comput-\ners the ability to learn without being explicitly programmed.”\n[10] Machine learning applications generally construct algo-\nrithms that can learn from and make predictions on data [11].\nThrough building a model from sample inputs, the algorithms\nare able to overcome the strictly static program instructions\nand make data-driven predictions or decisions [12]. Machine\nlearning models are deﬁned by many model parameters and to\nget the right values for all the parameters, iterative algorithms\nare essential. The iterative aspect of machine learning makes\nthe models able to independently adapt when exposed to new\ndata [12]. Consequently, a machine learning application must\nhave at least one loop to processing input data and update\nmodels, which we call the training step.\nDeep learning is a class of machine learning algorithms\nwhich learn multiple levels of representation and abstraction\nthat help to make sense of data [1]. Since deep learning\napplications are designed to process large amount of data, the\ndata is partitioned into batches and execute batch by batch in\nthe training step. Thus iteration also plays an important role\nin deep learning applications.\nB. Tensorﬂow Applications\nA powerful framework is essential to developing deep\nlearning applications. Tensorﬂow is one of the most popular\ndeep learning frameworks [7]. With Tensorﬂow, developers\nand researchers can focus on designing deep learning algo-\nrithms, with no regards for the complex underlying operations.\nTo achieve this, Tensorﬂow leverages data ﬂow graphs to\nrepresent the computation procedure, as well as other deep\nlearning frameworks [13]. A dataﬂow graph is consisted of\nnodes and edges. Nodes in the graph represent mathematical\noperators, while edges represent data arrays communicated be-\ntween nodes. To execute a deﬁned dataﬂow graph, Tensorﬂow\nﬁrst generates GPU computing operations according to the\noptimized dataﬂow graph, and then deploys the underlying\nprogram to GPU.\nIn general, Tensorﬂow applications can be abstracted into\ntwo parts, graph construction and iterative training. Ten-\nsorﬂow provides developers with powerful APIs to create\ndataﬂow graphs. In graph construction part, developers con-\nstruct the dataﬂow graph using Tensorﬂow APIs. The graph\nconsists not only essential computing operations of the train-\ning step, but also calculations of intermediate results or per-\nformance measurements. In iterative training part, developers\nhave to create a tensorﬂow session, which contains the whole\npredeﬁned dataﬂow graph. In order to execute the graph,\ncalling session.run method in a loop is required. The session\ninitializes the whole graph at the ﬁrst time session.run is\ncalled. Every session.run call takes a list of parameters that\nassign the execution part in the dataﬂow graph. The session\nand dataﬂow graph mechanism allow developers to develop\ndeep learning applications without any underlying details.\nC. CUDA\nAs mentioned in previous sections, deep learning applica-\ntions are actually executed on NVIDIA GPUs to get the most\ncomputation resources. The underlying computation platform\nused by most deep learning frameworks is CUDA [14]. CUDA\nis a parallel computing platform provided by NVIDIA. Deep\nlearning applications usually employ GPUs to accelerate ex-\necution time because the applications generally involve huge\namount of matrix multiplications and other operations. These\noperations can be massively parallelized and thus sped up on\nGPUs. With CUDA, developers are capable of deploy their\nprograms on NVIDIA GPUs for high performance computing.\nHowever, GPU based applications are much more complicated\nthan normal CPU-based ones. To run programs on a GPU,\ndevelopers have to deal with additional data management,(e.g.,\ndata copy from host to GPUs), as well as concurrency control.\nTo realize a simple matrix multiplication requires more than\n100 lines of C++ codes, not to consider sending massive data\nand executing complicated computations.\nTensorﬂow, like most other deep learning frameworks, hides\ncomplex underlying CUDA realization from developers. Every\nTensorﬂow operators in the dataﬂow graph corresponds to one\nor more CUDA operations including computing and memory\nmanagement. With Tensorﬂow, developers can focus on the\ndesign of deep learning algorithms and leave the intricate\nCUDA operations to the powerful framework. In next Section,\nwe illustrate the urgency and difﬁculty of deep learning\nperformance analyzers by a motivating example.\nIII. A MOTIVATING EXAMPLE\nThe dataﬂow graph mechanism of Tensorﬂow inevitably\nresults in complicated underlying executions. Consequently,\neven simple codes may potentially contain subtle performance\nissues, which are particularly hard to be detected by current\nproﬁling tools. We demonstrate this by a simple example in\nFigure 1.\nThe program in Figure 1 intends to build a gradient descent\nmodel for recognizing images. The program creates the model\nthrough line 2 to line 6. After constructing the dataﬂow graph,\n1   #Create the model\n2   ...\n3   loss = ...\n4   train = tf.train.GradientDescentOptimizer\n5   \n(0.01).minimize(loss)\n6   init = tf.initialize_all_variables()\n7   #Create Session\n8   with tf.Session() as sess:\n9   \nsess.run(init)\n10   #Train\n11   \nfor _ in range(1000000):\n12   \nsess.run(train)\n13   \ndbl_loss = loss * 2.0 #potential issue\n14   \nsess.run(dbl_loss)\nFig. 1: An example of tensorﬂow program\na session is required to launch the graph. In line 8, the program\ncreates a Tensorﬂow session instance sess and initializes all\nvariables of the graph in line 9. Finally, the training step of this\nprogram contains the deﬁned train and dbl loss operations, by\nsending the operations as parameters to the sess.run call. Since\nall the practical computations and underlying data copy are\nmanaged by the tensorﬂow session, we successfully implement\nthe core algorithm within 20 lines of codes.\nHowever, this simple program contains a subtle performance\nissue. In line 13, the overloaded operator “*” implicitly calls\ntf.convert to tensor function which leads to adding a new\nnode to the graph. Since the sess need to initialize the whole\ngraph before computing, the program has to add a node\nand re-initialize the graph in every loop, which becomes the\nbottleneck of the performance. Without adding new nodes to\nthe graph, the sess only need to initialize the graph when the\nﬁrst time sess.run is called. In the above example, the sess\nhas to initialize the graph at every loop step, which causes\nmassive overhead to the execution time.\nThe performance issue is a typical memory leak problem\ncaused by graph growth. The subtle operators that add nodes\nto the graph in training loop will result graph growth issues.\nIt is however difﬁcult to be unveiled. First, the issue is not\neasy to identify because the execution time depends on several\nfactors. Underlying GPU performance and data transfer rate\ncan affect the execution time as well. It is hard to tell whether\nthe running time is abnormal or not because many developers\ncannot estimate the expected execution time precisely. Even if\nthe performance issue is noticed, current tools cannot provide\nenough implications to help locate the problematic codes.\nNvprof, provided by NVIDIA, is the ofﬁcial proﬁling tool\nto collect CUDA-related activities [15]. Although nvprof is\ncapable of listing all CUDA operations during the execution,\nthe results are difﬁcult for deep learning developers to under-\nstand. The GPU trace collected by nvprof only contains GPU\noperations which have little relation to source codes of the\nprogram and make CPU proﬁling useless. Furthermore, the\nGPU traces generated by nvprof can be quite large (58MB in\nour simple example). The human effort to analyzing the results\nmanually is unaffordable for any individual or company.\nAnother tool provided by Tensorﬂow is timeline. The time-\nTABLE I: Attributes of GPU trace\nAttribute\nMeaning\nStart\nTime from the program starts to the operation is called\nDuration\nExecution time of the operation\nSize\nData copy size\nThroughput\nThroughput of data copy operation\nDevice\nDevice name the operation executed on\nStream\nStream number the operation belongs to\nName\nOperation name\nline tool is capable of proﬁling the Tensorﬂow operators in\none sess.run call and generate a .json ﬁle. However, the graph\ngrowth in the example happens before session.run and hence\nthe results are useless. Moreover, a simple application call\nsess.run for thousands of times so that this tool will create\nthousands of json ﬁles. As a result, developers can’t observe\nthe overall performance of applications but the disconnected\ntraining step.\nTherefore, lack of appropriate solutions to analyzing GPU\ntraces makes it difﬁcult to detect performance issues. In the\nnext Section, through detailed analyzing Tensorﬂow applica-\ntions and GPU traces, we propose DeepProf, a novel tool\nthat helps developers to analyze deep learning application\nperformance and to ﬁnd potential problems.\nIV. GPU TRACE ANALYSIS AND PROCESSING\nTo measure the application performance precisely, we ex-\npect to extract helpful information from the tedious and\nunreadable GPU traces. We ﬁrst analyze the GPU traces in\ndetail and extract the ordered GPU computing operations of\nTensorﬂow applications. Leveraging the iterative property of\ndeep learning algorithms, we use the sufﬁx tree structure and\nan approximate matching algorithm to partition all comput-\ning operations to different parts. Every part refers to the\nunderlying execution caused by one iteration step in deep\nlearning algorithms. Based on the generated GPU proﬁle\nwith ﬁne granularity, we deﬁne some metrics to measure\nthe execution performance intuitively. We further implement\nDeepProf to automatically analyze the performance for deep\nlearning applications. Although our approach can adapt to\nmulti-loop scenarios, for simplicity, we only discuss the one-\nloop condition in analysis part. We show how to deal with\nmulti-loop applications in the end of this section.\nA. GPU Trace Analysis\nTable 1 shows the attributes with their meanings of GPU\ntraces collected through nvprof. We introduce the concepts\nof CUDA operation and CUDA stream ﬁrst. We classify the\noperations in GPU traces according to name and stream\nattributes and then summarize some important rules about the\nunderlying execution of Tensorﬂow applications.\nKernel<>\nMemcpyDtoH\nSerial\nMemcpyHtoD\nK1\nDH1\nMemcpyHtoD\nK2\nK3\nK4\nDH2\nDH3\nDH4\nStreams\nexecute K1 with all processors\nGPU\ntime\ntime\nt\nFig. 2: An example on GPU concurrency\n1) CUDA Operations: According to the name attribute,\nwe notice that there are two main kinds of operations in\nGPU traces, kernels and memcpy (memory copy) operations. A\nkernel is a function deﬁned by CUDA api and can be executed\non GPU. The kernels can be deﬁned by either CUDA or\nTensorﬂow. The memcpy operations can be further divided into\nthree types according to the source and destination of the data\ncopy: memcpyHtoD, memcpyDtoH and memcpyDtoD. The ’H’\nmeans host while ’D’ means GPU device. In the GPU trace\nof deep learning applications, we can directly distinguish two\nkinds of CUDA operations deﬁned above, according to the\nthroughput attribute. Speciﬁcally, computing operations do\nnot have throughput attribute while memcpy operations do.\n2) CUDA Stream: CUDA platform use stream mechanism\nto achieve concurrency. A stream contains a sequence of\noperations that execute in issue-order on GPU [14]. The\noperations in different streams may run concurrently and be\ninterleaved. Note that although the operations in one stream are\nexecuted in order, one operation can utilize many processors\nand be executed in parallel on GPU. Figure 2 shows a simple\nconcurrency example on GPU. After copying data to the GPU,\nthe Kernel and MemcpyDtoH operations are executed by\n4 streams and get 1.33x performance improvement than serial\nexecution. Note that in t moment, only K1 is executed on\nGPU and thus this operation can utilize all GPU processors\nfor computing.\nThrough the above analysis, we can reveal some underlying\nfeatures of Tensorﬂow applications. As stated before, the\noperations in the same stream are executed in their issue-\norder. Through extracting all operations that have the same\nstream value and sorting them on the start value, we get\nthe ordered operations of different streams during the execu-\ntion. We continue to use the program in Section III as an\nexample. After applying the extraction and sorting operations\nto the GPU trace, we summarize the distribution of different\noperations in each stream in ﬁgure 3. We notice that only\nstream 13 contains computing operations while stream 14 and\n15 only consists of memcpyHtoD operations and memcpyDtoH\noperations respectively. The rest stream (stream 7) has only\n......\nMemcpyDtoD\n......\nCUDA Kernels\n......\nTF Kernels\n......\nStream 13\n......\nMemcpyHtoD\nMemcpyHtoD\n......\nMemcpyHtoD\nMemcpyHtoD\n......\nStream 14\n......\nMemcpyDtoH\nMemcpyDtoH\n......\nMemcpyDtoH\nMemcpyDtoH\n......\nStream 15\nMemset\nMemcpyHtoD\nStream 7\nFig. 3: An example of streams used by Tensorﬂow\ntwo entries, one memset (memory set) and one memcpyHtoD.\nBased on above observation, we classify underlying streams\nused by TensorFlow applications into three kinds. The speciﬁc\ndeﬁnitions are as follows:\n1. Main stream: Streams contain Tensorﬂow deﬁned kernels,\nlike stream 13 in the example, are called main streams\n2. Copy stream: Streams only contain memcpy operations,\nlike stream 14 and 15, are called copy streams.\n3. Assist stream: All streams that do not belong to main\nstream and copy stream are called assist streams, like stream\n7.\nWith above deﬁnitions on CUDA operations and streams,\nwe further analyze the GPU traces of different Tensorﬂow ap-\nplications, and summarize two general rules of the underlying\nTensorﬂow realization as follows:\nRule 1: Tensorﬂow only creates one main stream [7], which\nincludes all kernels deﬁned by Tensorﬂow. Besides kernels,\nthis stream may contain memcpyDtoD operations as well. This\ndesign allows the operations in main stream to utilize all\nprocessers and memory spcace in GPU if necessary.\nRule 2: Two copy streams are created by Tensorﬂow to\ncopy data between host and GPU. One stream deals with\nmemcpyHtoD operations while the other copys data from GPU\nto host.\nRule 3: Assist streams are used to handle trivial GPU\noperations. The number of assist streams is depended on the\nrealization of applications, but Tensorﬂow creates at least one\nassist stream, just like stream 7, to initialize GPU memory.\nAccording to Rule 1 and stream mechanism of CUDA, we\ncan conclude that the GPU computing procedure of Tensorﬂow\nis sequential. Based on this conclusion, we leverage the\niteration property of deep learning algorithms to partition the\nGPU traces.\nB. Sufﬁx Tree Based Pattern Search\nLet’s review the structure of Tensorﬂow applications. Any\ndeep learning algorithm requires a training step, which refers\nto a subgraph of the deﬁned dataﬂow graph in Tensorﬂow.\nIn iterative training part, a session is created to execute\nthe subgraph iteratively. We deﬁne the basic iteration as\none session.run on the subgraph of training step. Once the\ndataﬂow graph is deﬁned, the GPU operations caused by the\nbasic iteration is determined. According to Rule 1, all the\ncomputing operations of Tensorﬂow are issued by one stream\nand thus have a strict execution order. As a result, executing\nbasic iteration will lead to the same underlying computing\noperations. Since applications execute the basic iteration for\nmany times, there must be a repeated pattern in the collected\nGPU traces. Note that the repeated pattern may not occur\nin every iteration because applications are likely to calculate\nsome statistics periodically, which leads to the underlying\noperations different from those of basic iteration. Under this\ncircumstance, applications actually execute the basic iteration\nwith the operators performing statistic calculation. The GPU\ntrace of such an iteration should be an insertion of some\nGPU operations to the basic iteration trace. Now we get the\nfollowing criterion:\nCriterion 1: The GPU trace consists of the basic iteration,\nwhich is interleaved by GPU operations for other purposes,\nsuch as statistic calculation.\nBased on Criterion 1, we can extract the basic iteration\ncaused operations through mining frequent patterns in GPU\ntraces. We use the sufﬁx tree structure to preprocess the\noriginal GPU traces.\n1) Sufﬁx Tree: Sufﬁx trees allow efﬁcient query processing\non text data, which was ﬁrst introduced in [9]. Every internal\nnode in a sufﬁx tree represents a substring while every leaf\nnode represents a sufﬁx of the original string. Figure 4 shows a\nsufﬁx tree of the string “banana$”. The “$” symbol represents\nthe string terminator. The example tree consists of 4 internal\nnodes and 7 leaves. Note that each node in the sufﬁx tree\nrepresents a substring of the input string. More precisely,\na leaf node corresponds to a sufﬁx while a branch node\ncorresponds to a preﬁx of the sufﬁx, i.e., a substring. For\nexample, the internal node “5” in Figure 4 represents the\nsubstring ”ana”, and the leaf node “9” represents the sufﬁx\n“anana”. Furthermore every node and its descendants forms a\nsubtree in the sufﬁx tree. The number of leaves in a subtree\ndetermined by a node is the times that the corresponding\nsubstring repeats in the input string. For example, node “5” has\n2 leaves in its subtree so “Ana” repeats 2 times in “banana”.\n2) Why Sufﬁx Tree: As mentioned in section III, the GPU\ntrace of a deep learning application is often extremely large.\nBoth the iteration times and the complexity of algorithm\ncan cause a great amount of underlying computations in\nGPU trace. Thus we cannot afford a high space complexity\nalgorithm to discover frequent patterns. Constructing a sufﬁx\ntree is quite efﬁcient. The space and time complexity of\nconstructing a sufﬁx tree is O(n), thus we choose sufﬁx tree\nstructure to deal with the massive original traces.\nNext we introduce the details of pattern mining in GPU\ntraces. For ease of reference, we summarize the symbols we\nuse in Table 2.\nWe preprocess the original data and treat all operation\nnames in the main stream as a long string S. S is a two-\ndimension array and every element of S is a GPU operation\nname. We then construct a sufﬁx tree ST of S. Each substring\nrepeats c times in S is represented by an internal node that\nhas c leaves as its descendants in ST. Moreover, the length l\n0\n2\n3\n1\n5\n6\n7\n10\n9\n8\nbanana$\n$\nna\na\nna\nna$\n$\n$\nna$\n$\n4\nFig. 4: The sufﬁx tree of string “banana$”.\nTABLE II: Symbol Table\nSymbol\nMeaning\nS\nThe long string constructed by all operation names\nin a main stream\nST\nA sufﬁx tree\nlstr\nThe length of the string str\ncstr\nThe repetition times of the string str in S\nlp\nThe length of the pattern\ncp\nThe repetition times of the pattern\ni\nThe total number of iterations in a application\nk0\nThe maximum unmatched string length\nof the repeated substrings should be appropriate because short\npatterns cannot form an iteration step while long patterns may\ncontain too many operations. If we know the right cp and lp\nvalues, we can scan all nodes in ST and select the frequent\nsubstring in O(n) time.\n3) Parameter discussion: The value of cp and lp are related\nto the number of basic iteration executed. If the application\nexecutes exactly the same instructions in every iteration, cp\nis equal to i, which is the total number of iterations. If\nsome iterative steps execute additional instructions, according\nto Criterion 1, the number of total operations increases.\nConsidering there are some initial operations, that do not\nbelong to any iteration, the average GPU operations per\niteration must be larger than lp. Thus far we get the following\ntwo criterions for cp and lp:\nCriterion 2: (i −ε) < cp ≤i, where ε is the threshold we\ndeﬁned.\nCriterion 3: lp < lS\ni , where lS is the total length of string S\nand i is the number of iterations in the application. lS\ni means\nthe number of GPU operations per iteration.\nConsequently, we can scan all nodes in ST and select\nfrequent substrings that satisfy Criterion2 and 3. We only\nselect the longest substring P that repeats at least i −ε\ntimes, and is shorter than lS\ni , where ε is a priori threshold.\nIf no internal node satisﬁes the ﬁlter conditions, the program\ndoubles the value of ε and re-search the sufﬁx tree. In the\nAlgorithm 1 Performance Metrics Table\nInput: substring P, string S\nOutput: all matching positions in array res\n1: res ←∅\n2: while i ≤length(S) do\n3:\nk ←0, i0 ←i\n4:\nfor j ←1 · · · length(P) do\n5:\nwhile S[i0] ̸= P[j] do\n6:\nk ←k + 1, i0 ←i0 + 1\n7:\nif k > k0 then\n8:\ngoto label\n9:\nend if\n10:\nend while\n11:\nend for\n12: label:\n13:\nif k ≤k0 then\n14:\nappend(res, pair(i, i0)); i ←i0 + 1\n15:\nelse\n16:\ni ←i + 1\n17:\nend if\n18: end while\n19: return res\nworst scenario, the total time complexity of constructing ST\nand select the frequent substring is O(mn), where m is the\nre-search times for an eligible node in the tree.\nFinally, we search for all patterns that approximately match\nP in O(mn) time, where m is the length of P and n is\nthe length of S, as shown in Algorithm 1. The parameter\nk0 represents the maximum unmatched string length. In other\nwords, the approximate patterns can have at most k0 more\noperations than substring P. The algorithm checks all possible\npositions that may have a approximate match and return the\nstart and end position of the matching substrings.\nC. Performance Metrics Analysis\nThrough the sufﬁx-tree-based approximate match algo-\nrithms, we successfully divide the ruleless GPU trace into\ndifferent parts. Based on the partitioned trace, we further\ncalculate a serious of metrics to measure the execution perfor-\nmance of deep learning applications.\nWe deﬁne the iteration interval as the leisure time during\ntwo adjacent iterations. In other words, iteration interval is the\ntime difference between the start of one iteration and the end\nof its previous one. The latter iteration cannot start until GPU\nreceives the input data and CPU ﬁnishes the other instructions\nin the loop of source codes, as the state graph shown in\nﬁgure 5. Note that although data copy state is independent\nto GPU and CPU operations, the GPU iteration has to start\nafter the corresponding data copy operations. Therefore, the\nsize of iteration interval represents the execution time of CPU\ninstructions and data copy operations.\nWe further deﬁne the interval overlap as the ratio of data\ncopy time to compute interval. We only consider the time\nof memcpyHtoD operations during each iteration interval,\nTABLE III: Metrics Table\nMetrics\nMeaning\navg interval\nAverage of iteration interval\nmax interval\nThe maximum value of all iteration interval\navg overlap\nAverage value of interval overlap\navg operation\nAverage interval time of two GPU operations\navg size\nAverage data copy size from host to GPU per iteration\nbecause copying data from GPU to host can be executed\nconcurrently and has little effect on the start time of the\nnext iteration. A high interval overlap implies that the data\ntransfer operations become the bottleneck. We also calculate\nthe average value of iteration interval and interval overlap,\nas well as some intuitional metrics. Detailed meanings of\ndifferent metrics are shown in Table 3.\nCPU\nInstruction\ns\nGPU\nIteration\nData\nCopy\nFig. 5: State graph of iteration\nD. DeepProf realization\nBased on the above discussion, we implement DeepProf,\na novel tool to automatically mine patterns in original GPU\ntraces and generate performance proﬁles for deep learning ap-\nplications. The processing framework adopted in DeepProf\nis shown in ﬁgure 6. There are four main components of\nDeepProf. In preprocessing part, DeepProf ﬁrst extract all\nGPU operations belong to the main stream and then combine\nthese operations into a long string S. In the pattern mining part,\nDeepProf construct the sufﬁx tree of S, and mine the fre-\nquent pattern P according to the number of iterations, which\ncan be fetched through analyzing the source codes. The start\nand end positions of all approximately matched patterns of P\nare generated in the approximate match part. According to the\nresults from approximate match part, DeepProf partitions\nthe GPU traces and calculates the performance metrics in the\nmetrics generation part. The results generated by DeepProf\ncontain a summary of average performance metrics and a ﬁle\ncontains detailed metrics of every iteration. The results can be\nexplained intuitively by the state graph shown in ﬁgure 6.\n1) Adapt to multi-loop: The above DeepProf framework\ncan adapt to more complicated applications with multiple\nloops, which means GPU traces can have several patterns.\nIn the sufﬁx tree, the multi-loop is property reﬂected in\nseveral frequent substrings. The repetition times and length\nof each substring still satisfy Criterion2 and 3. Knowing the\nnumber of loops and corresponding iterative steps, we just\nneed to search the sufﬁx tree with different parameters. All\nthe parameters required to mine the frequent patterns can be\nobtained from the source codes, so DeepProf is capable of\ndealing with multi-loop applications.\nNext, we use DeepProf to analyze and diagnose the\nperformance of Tensorﬂow applications. The results show the\neffectiveness of DeepProf, and reveal some hidden features\nof Tensorﬂow.\nV. EMPIRICAL STUDY\nIn this section, we evaluate the effectiveness of DeepProf\nin two aspects, performance analysis and diagnosis. We ﬁrst\nanalyze the performance of computation-intensive Tensorﬂow\napplications as well as IO-intensive ones on different GPUs\nby DeepProf. The results reveal a few execution features of\nTensorﬂow, which can be used as guidance for system setup.\nThe analysis results generated by DeepProf can also be used\nto diagnose performance issues. We demonstrate how to detect\nthe potential performance issues according to the abnormal\nmetrics using two cases. The ﬁrst issue is shown in Section\nIII, which is caused by graph growth. The second issue resides\nin data transfer procedure where oversize data copy becomes\nthe bottleneck. We conduct the experiment on a Ubuntu 16.04\nserver with i7 6700K CPU and NVIDIA 1080Ti graphics card.\nA. Performance Analysis\nTensorﬂow\napplications\ncan\nbe\nclassiﬁed\ninto\ntwo\ntypes, computation-intensive and IO-intensive. Computation-\nintensive applications employ a complex model to improve\nthe prediction accuracy. In this kind of application, every\ntraining step requires a long execution time and the data\ntransfer operations has little effect on performance. On the\nother hand, data-intensive applications are designed to process\na large amount of input data, while large data transfer between\nhost and GPU is common in this kind of applications. In\ngeneral, developers intend to allocate the whole GPU memory\nto one application. In other words, one GPU only executes\nPreprocessing\nPattern\nMining\nString\nApproximate\nMatch\nPattern\nMetrics\nCalculation\nGPU\ntrace\nParameters\nResults\nFig. 6: Overview of DeepProf framework\n0\n20\n40\n60\n80\n100\nmemory percentage (%)\n0\n10\n20\n30\n40\n50\n60\ntime (s)\ncomputation intensive\nI/O intensive\n(a) Execution time with different memory size\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nmemory percentage (%)\n0\n20\n40\n60\n80\n100\nutilization (%)\ncomputation intensive\nI/O intensive\n(b) GPU utilization with different memory size\nFig. 7: Single execution result\none application at a time. So our question is whether running\nseveral applications concurrently on one GPU will cause great\nperformance loss. To address the problem, we ﬁrst execute\none application with a limited GPU memory size and use\nDeepProf to analyze the performance.\nWe test both computation-intensive and IO-intensive ap-\nplications with a limited GPU memory percentage, and we\nmeasure the how the allocated memory size affect execution\ntime and GPU utilization. The results are surprising, as shown\nin Figure 7. Allocating too little GPU memory to applications\nleads to ’out of memory’ error, so there is no time and\nutilization with little memory percentage in Figure 7. The\nexecution time and GPU utilization of both applications stay\nalmost the same, regardless of how much GPU memory is\nused. The results imply that once the application get enough\nmemory to ﬁnish the execution, the performance cannot be\nimproved with more GPU memory.\nWe then execute two applications concurrently on one GPU\nand ﬁnd if the two applications will inﬂuence each other. To\ncontrol variables, we test the computation-intensive application\nwith different memory size when data-intensive application\nis executed with a ﬁxed memory size, and vice versa. We\ntest the applications with 25% and 50% memory occupied. So\nthe corresponding memory percentage left is up to 75% and\n50%, as shown in Figure 8. Comparing to running on the GPU\nsingly, the execution time of both applications increases, which\n0\n20\n40\n60\n80\n100\nmemory percentage (%)\n0\n50\n100\n150\n200\ntime (s)\nno memory occupied\n25% memory occupied\n50% memory occupied\n(a) Result of IO-intensive application\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nmemory percentage (%)\n0\n10\n20\n30\n40\n50\n60\ntime (s)\nno memory occupied\n25% memory occupied\n50% memory occupied\n(b) Result of computation-intensive application\nFig. 8: Concurrent execution result\nproves that the executing several applications on one GPU\nwill cause performance loss. Noted that the execution time of\none application still stay the same, regardless of the memory\nsize allocated to the other application. Even though there is\navailable GPU memory space, the applications may sustain\nperformance loss. Through the results shown in Figure 7 and\nFigure 8, we draw the conclusion that although Tensorﬂow\napplications may occupy all GPU memory if not limited,\nthe memory size actually has little inﬂuence on execution\nperformance.\nBased the above analysis, we ﬁnd that although Tensorﬂow\napplications intend to use as much GPU as they can, high\nmemory usage does not imply high performance.\nB. Performance Diagnosis\nDeepProf can also be used for simple performance diag-\nnosis. We show two typical performance issues that can be\ndetected using DeepProf.\nThe ﬁrst case is the graph growth issues. As mentioned\nin Section III, adding new nodes to the dataﬂow graph leads\nto graph re-initialization. In fact, the session has to initialize\nthe graph before execution. Without graph change, the session\nonly needs to initialize the graph once in the ﬁrst iteration\nCPU\nInstruction\ns\nGPU\nIteration\nData\nCopy\navg size:\n504.7 KB\navg interval:\n0.0783 ms\navg overlay:\n0.0471 %\navg operation:\n0.0013 ms\nmax interval:\n0.1082 ms\n(a) Case 1 results\nCPU\nInstruction\ns\nGPU\nIteration\nData\nCopy\navg size:\n2603.7 KB\navg interval:\n0.0240 ms\navg overlay:\n13.963 %\navg operation:\n0.0003 ms\nmax interval:\n0.0421 ms\n(b) Case 2 results\nFig. 9: Results of DeepProf for performance diagnosis\nand keeps the initialized graph for later invocation. The graph\ngrowth causes the session to initialize the graph in every\niteration and produce a huge overhead. DeepProf can help\ndevelopers to diagnose such performance issues. Figure 9 (a)\nshows the execution state graph of the example program in\nFigure 3, along with the metrics calculated by DeepProf.\nWe can ﬁnd that the avg interval is much larger than avg\noperation, implying the performance bottleneck exists outside\nthe iteration. In other words, no GPU computing operation\nsomehow blocks the underlying execution. As the state graph\nshows, both slow data copy and CPU instructions may cause\nsuch block. In Figure 9 (a), the avg overlap metric, represent-\ning the average ratio of data copy time during the iteration\ninterval, is small. Small avg overlap implies the block is\nnot caused by data copy operations since data copy only\ncontributes little to the interval time. Therefore, it is very\nlikely that the CPU instructions are the origin of the issue and\ndevelopers can check the training loop part of source codes.\nDeepProf successfully detects the graph growth issues and\nhelp locating the slow code in this case.\nThe second case is the oversize data copy. Data transfer is\na well-known bottleneck in deep learning ﬁeld. Deep learning\napplications are generally designed to process tremendous data\nbut the data transfer efﬁciency between host and GPU can’t\nmatch the great computing power of GPU. Although data\ntransfer efﬁciency is an inherent problem, performance issues\ncan be unexpectedly caused by inappropriate parameters. Deep\nlearning applications usually process the data in batch, thus\nimproper batch size may cause unnecessary overheads. Figure\n9 (b) shows the execution state graph of an application with an\noverlarge batch size. In the state graph, we can see avg interval\nis large, and so does avg overlap. This phenomenon implies\nthat the GPU iteration is blocked by data copy operations,\nsince 12 percent of interval time is spent on waiting data\ncopy operations. A smaller batch size in the application can\nimprove the execution performance. On the other hand, some\ndeep learning algorithms require certain amount of data for\nprocessing and the batch size cannot be smaller. In this case,\ndevelopers should decide on the tradeoff between performance\nand effectiveness. DeepProf diagnose the data copy bottle-\nneck in this case.\nThe two cases above are all based on real Tensorﬂow\napplications and inexperienced developers often make such\nmistakes. DeepProf can help these inexperienced developers\nto rapidly diagnose the application performance.\nC. Discussions\nThrough the empirical study, DeepProf shows the power\nof analyzing the performance of deep learning applications.\nDeepProf is also capable of detecting the common perfor-\nmance bottleneck and help developers to identify mistakes.\nThe space complexity of DeepProf is O(n), which means\nDeepProf can deal with large GPU traces. Moreover, the\nresults generated by DeepProf are based on the number\nof iterations of the application and GPU traces collected by\nnvprof, thus the design ideas of DeepProf can be applied to\nmost GPU-based deep learning frameworks.\nHowever, there are also some defects in DeepProf. The\nperformance metrics generated by DeepProf are straightfor-\nward and how to ﬁnd more subtle metrics with the partitioned\nGPU trace is worth studying. In addition, DeepProf focuses\non analyzing the performance of applications executed on sin-\ngle GPU device. Analyzing the performance of deep learning\napplications under multi-GPU circumstance remains an open\nquestion.\nVI. RELATED WORK\nA. Proﬁling\nThe usefulness of proﬁling has long been recognized.\nProﬁling is a kind of dynamic program analysis which is\nwidely used in functional fault detection [16], [17], [18],\nand non-functional fault detection [19], [20], [21], [22].\nJiang et al. utilized execution proﬁlers that possibly contain\nfaults to simplify programs and scale down the complexity\nof programs for in-house testing [23]. AppInsight, provided\nby Ravindranath et al., instruments mobile-app binaries to\nidentify the critical path in user transactions automatically\n[24]. Coppa et al. proposed an approach to measure how\nthe size of input effects performance, and used it to ﬁnd out\nperformance faults by analyzing proﬁles [25]. Chilimbi et al.\nprovided HOLMES, a tool to proﬁle the selected parts of the\napplication, and then rank and identify the critical paths which\ncan predict the failures through path proﬁles [18]. Han et al.\nproposed StackMine, which extracts effective subsequences of\nfunction calls by a costly-pattern mining algorithm, to help\nthe performance debugging. Shen et al. proposed GA-Prof,\nwhich used search-based input-sensitive application proﬁling\nfor automating performance bottlenecks detection [26]. These\nwork identify critical paths and help identify performance\nproblems through proﬁling. DeepProf takes the core idea\nof proﬁling analysis and focus on handling GPU traces.\nSeveral work focus on deriving operational proﬁles with\nclustering algorithms [27]. Gittens et al. increased the proﬁling\napplicability by an extended operational proﬁle model to ad-\ndress the heterogeneity of software. Nagappan et al. proposed\na sufﬁx-array based algorithm to parse the execution logs and\ngenerate operational proﬁles [28]. However, GPU traces are\nmuch more complicated than logs since one upper API call\nmay cause several GPU operations. Furthermore, DeepProf\nprovides execution summaries from GPU traces which can\nhelp developers to analyze application performance.\nB. Detecting and ﬁxing performance problems\nDetecting and ﬁxing performance problems were shown to\nbe challenging [29]. Several techniques have been proposed\nto identify performance problems such as slow code [21],\n[30], [31], [32], [33] and increasing execution time [25],\n[26], [34]. Grechanik et al. proposed FOREPOST, a feedback-\ndirected black-box approach for detecting performance prob-\nlems and identifying bottlenecks [35]. Liu et al. designed\nan innovative system, AutoAnalyzer, to identify existence\nof performance bottlenecks by clustering algorithms and to\nlocate the bottlenecks using searching algorithm [36]. Song\nand Lu investigated design points in statistical debugging\nfor problem diagnosis [37]. Chis pinpointed memory issues\nthrough detecting memory anti-patterns from memory catalogs\n[38]. Nistor et al. designed Toddler, which detecting per-\nformance bugs through similar memory-access patterns [39].\nChen et al. proposed a framework to detect object-relational\nmapping performance anti-patterns automatically [40]. Other\nwork focused on concurrency performance problems [41],\nperformance testing [36] and latent performance bugs [42].\nDeepProf is designed for analyzing performance of deep\nlearning applications through mining patterns in GPU traces,\nand such scenario is not covered by previous work. To the best\nof our knowledge, we are the ﬁrst to analyze the performance\nof GPU-based deep learning applications.\nVII. CONCLUSION\nDeep learning applications are computation-intensive in\nnature and may incur incredibly lengthy execution time. Al-\nthough powerful frameworks, like Tensorﬂow, make it con-\nvenient to develop deep learning applications, the complex\nunderlying GPU operations make it difﬁcult to measure the\nperformance of applications. In this paper, through analyz-\ning the execution procedure of Tensorﬂow applications in\ndetail, we propose a novel analysis tool, DeepProf, to\nautomatically mine the patterns from GPU traces and analyze\napplication performance. We further verify the effectiveness of\nDeepProf through the empirical study on performance anal-\nysis and diagnosis. We also conclude the underlying execution\nfeatures of Tensorﬂow, which can be used as guidance for the\ndeep learning system setup. To the best of out knowledge, we\nare the ﬁrst to analyze the performance of GPU-based deep\nlearning applications. Although DeepProf is designed for\nTensorﬂow applications, the preprocessing procedure of GPU\ntraces can adapt to all GPU-based deep learning applications.\nFinally, in our future work, we are interested in extending\nDeepProf to analyze the performance of application using\nmulti-GPU and to achieve bug detection by taking CPU\nproﬁling into consideration.\nREFERENCES\n[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep neural\nnetworks for acoustic modeling in speech recognition: The shared views\nof four research groups,” IEEE Signal Processing Magazine, vol. 29,\nno. 6, pp. 82–97, 2012.\n[3] R. Collobert and J. Weston, “A uniﬁed architecture for natural language\nprocessing: Deep neural networks with multitask learning,” in Proc. of\nthe 25th ACM International Conference on Machine Learning (ICML).,\n2008, pp. 160–167.\n[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural infor-\nmation processing systems, 2012, pp. 1097–1105.\n[5] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for\nlarge-scale machine learning,” in Proc. of the 12th USENIX Symposium\non Operating Systems Design and Implementation (OSDI)., 2016.\n[6] “Nvidia gpus,” http://developer.nvidia.com/deep-learning/.\n[7] “Tensorﬂow,” https://www.tensorﬂow.org/.\n[8] “Gpu proﬁler,” http://docs.nvidia.com/cuda/proﬁler-users-guide/index.\nhtml/.\n[9] P. Weiner, “Linear pattern matching algorithms,” in Proc. of the IEEE\nConference Record of 14th Annual Symposium on Switching and Au-\ntomata Theory (SWAT)., 1973, pp. 1–11.\n[10] E. Alpaydin, Introduction to machine learning.\nMIT press, 2014.\n[11] R. Kohavi and F. Provost, “Glossary of terms,” Machine Learning,\nvol. 30, no. 2-3, pp. 271–274, 1998.\n[12] C. M. Bishop, “Pattern recognition,” Machine Learning, vol. 128, pp.\n1–58, 2006.\n[13] “Theano,” http://deeplearning.net/software/theano/index.html.\n[14] “Cuda toolkit,” https://developer.nvidia.com/cuda-toolkit/.\n[15] “nvprof tool,” http://docs.nvidia.com/cuda/proﬁler-users-guide.\n[16] M. Pradel and T. R. Gross, “Leveraging test generation and speciﬁcation\nmining for automated bug detection without false positives,” in Proc.\nof the 34th IEEE International Conference on Software Engineering\n(ICSE)., 2012, pp. 288–298.\n[17] S. Zhang and M. D. Ernst, “Automated diagnosis of software conﬁgu-\nration errors,” in Proc. of the 35th IEEE International Conference on\nSoftware Engineering (ICSE)., 2013, pp. 312–321.\n[18] T. M. Chilimbi, B. Liblit, K. Mehra, A. V. Nori, and K. Vaswani,\n“Holmes: Effective statistical debugging via efﬁcient path proﬁling,” in\nProc. of the 31st IEEE International Conference on Software Engineer-\ning (ICSE)., 2009, pp. 34–44.\n[19] D. Yan, G. Xu, and A. Rountev, “Uncovering performance problems in\njava applications with reference propagation proﬁling,” in Proc. of the\n34th IEEE International Conference on Software Engineering (ICSE).,\n2012, pp. 134–144.\n[20] G. Xu and A. Rountev, “Precise memory leak detection for java software\nusing container proﬁling,” in Proc. of the 30th ACM/IEEE International\nConference On Software Engineering (ICSE)., 2008, pp. 151–160.\n[21] S. Han, Y. Dang, S. Ge, D. Zhang, and T. Xie, “Performance debugging\nin the large via mining millions of stack traces,” in Proc. of the 34th\nIEEE International Conference on Software Engineering (ICSE)., 2012,\npp. 145–155.\n[22] B. Chen, Y. Liu, and W. Le, “Generating performance distributions via\nprobabilistic symbolic execution,” in Proceedings of the 38th Interna-\ntional Conference on Software Engineering.\nACM, 2016, pp. 49–60.\n[23] L. Jiang and Z. Su, “Proﬁle-guided program simpliﬁcation for effective\ntesting and analysis,” in Proc. of the 16th ACM International Symposium\non Foundations of software engineering (SIGSOFT)., 2008, pp. 48–58.\n[24] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan, I. Obermiller, and\nS. Shayandeh, “Appinsight: Mobile app performance monitoring in the\nwild.” in Proc. of the 12th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI)., vol. 12, 2012, pp. 107–120.\n[25] E. Coppa, C. Demetrescu, and I. Finocchi, “Input-sensitive proﬁling,”\nACM SIGPLAN Notices, vol. 47, no. 6, pp. 89–98, 2012.\n[26] D. Shen, Q. Luo, D. Poshyvanyk, and M. Grechanik, “Automating per-\nformance bottleneck detection using search-based application proﬁling,”\nin Proc. of the ACM International Symposium on Software Testing and\nAnalysis (ISSTA), 2015, pp. 270–281.\n[27] J. D. Musa, “Operational proﬁles in software-reliability engineering,”\nIEEE software, vol. 10, no. 2, pp. 14–32, 1993.\n[28] M. Nagappan, K. Wu, and M. A. Vouk, “Efﬁciently extracting opera-\ntional proﬁles from execution logs using sufﬁx arrays,” in Proc. of the\n20th IEEE International Symposium on Software Reliability Engineering\n(ISSRE)., 2009, pp. 41–50.\n[29] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu, “Understanding\nand detecting real-world performance bugs,” ACM SIGPLAN Notices,\nvol. 47, no. 6, pp. 77–88, 2012.\n[30] X. Xiao, S. Han, D. Zhang, and T. Xie, “Context-sensitive delta inference\nfor identifying workload-dependent performance bottlenecks,” in Proc.\nof the ACM International Symposium on Software Testing and Analysis\n(ISSTA)., 2013, pp. 90–100.\n[31] X. Yu, S. Han, D. Zhang, and T. Xie, “Comprehending performance from\nreal-world execution traces: A device-driver case,” in ACM SIGPLAN\nNotices, vol. 49, no. 4, 2014, pp. 193–206.\n[32] C. Sauvanaud, K. Lazri, M. Kaˆaniche, and K. Kanoun, “Anomaly\ndetection and root cause localization in virtual network functions,” in\nProc. of the 27th IEEE International Symposium on Software Reliability\nEngineering (ISSRE)., 2016, pp. 196–206.\n[33] W. Wen, T. Yu, and J. H. Hayes, “Colua: Automatically predicting\nconﬁguration bug reports and extracting conﬁguration options,” in Proc.\nof the 27th IEEE International Symposium on Software Reliability\nEngineering (ISSRE)., 2016, pp. 150–161.\n[34] A. Nistor, P.-C. Chang, C. Radoi, and S. Lu, “Caramel: detecting and\nﬁxing performance problems that have non-intrusive ﬁxes,” in Proc.\nof the 37th IEEE International Conference on Software Engineering\n(ICSE)., 2015, pp. 902–912.\n[35] M. Grechanik, C. Fu, and Q. Xie, “Automatically ﬁnding performance\nproblems with feedback-directed learning software testing,” in Proc.\nof the 34th IEEE International Conference on Software Engineering\n(ICSE)., 2012, pp. 156–166.\n[36] X. Liu, J. Zhan, K. Zhan, W. Shi, L. Yuan, D. Meng, and L. Wang,\n“Automatic performance debugging of spmd-style parallel programs,”\nJournal of Parallel and Distributed Computing, vol. 71, no. 7, pp. 925–\n937, 2011.\n[37] L. Song and S. Lu, “Statistical debugging for real-world performance\nproblems,” ACM SIGPLAN Notices, vol. 49, no. 10, pp. 561–578, 2014.\n[38] A. E. Chis, “Automatic detection of memory anti-patterns,” in Com-\npanion to the 23rd ACM SIGPLAN conference on Object-oriented\nprogramming systems languages and applications, 2008, pp. 925–926.\n[39] A. Nistor, L. Song, D. Marinov, and S. Lu, “Toddler: Detecting perfor-\nmance problems via similar memory-access patterns,” in Proc. of the\n35th IEEE International Conference on Software Engineering (ICSE).,\n2013, pp. 562–571.\n[40] T.-H. Chen, W. Shang, Z. M. Jiang, A. E. Hassan, M. Nasser, and\nP. Flora, “Detecting performance anti-patterns for applications developed\nusing object-relational mapping,” in Proc. of the 36th ACM International\nConference on Software Engineering (ICSE)., 2014, pp. 1001–1012.\n[41] J. Oh, C. J. Hughes, G. Venkataramani, and M. Prvulovic, “Lime: A\nframework for debugging load imbalance in multi-threaded execution,”\nin Proc. of the 33rd ACM International Conference on Software Engi-\nneering (ICSE)., 2011, pp. 201–210.\n[42] C. Killian, K. Nagaraj, S. Pervez, R. Braud, J. W. Anderson, and\nR. Jhala, “Finding latent performance bugs in systems implementations,”\nin Proc. of the 18th ACM International Symposium on Foundations of\nSoftware Engineering (SIGSOFT)., 2010, pp. 17–26.\n",
  "categories": [
    "cs.SE"
  ],
  "published": "2017-07-12",
  "updated": "2017-07-12"
}