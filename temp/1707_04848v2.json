{
  "id": "http://arxiv.org/abs/1707.04848v2",
  "title": "Do Neural Nets Learn Statistical Laws behind Natural Language?",
  "authors": [
    "Shuntaro Takahashi",
    "Kumiko Tanaka-Ishii"
  ],
  "abstract": "The performance of deep learning in natural language processing has been\nspectacular, but the reasons for this success remain unclear because of the\ninherent complexity of deep learning. This paper provides empirical evidence of\nits effectiveness and of a limitation of neural networks for language\nengineering. Precisely, we demonstrate that a neural language model based on\nlong short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law,\ntwo representative statistical properties underlying natural language. We\ndiscuss the quality of reproducibility and the emergence of Zipf's law and\nHeaps' law as training progresses. We also point out that the neural language\nmodel has a limitation in reproducing long-range correlation, another\nstatistical property of natural language. This understanding could provide a\ndirection for improving the architectures of neural networks.",
  "text": "Do Neural Nets Learn Statistical Laws behind Natural Language?\nShuntaro Takahashi1†, Kumiko Tanaka-Ishii2‡*\n1 The University of Tokyo, Graduate School of Frontier Sciences, Chiba 277-8563, Japan.\n2 The University of Tokyo, Research Center for Advanced Science and Technology, Tokyo 153-8904,\nJapan.\n* kumiko@cl.rcast.u-tokyo.ac.jp\nAbstract\nThe performance of deep learning in natural language processing has been spectacular, but the reasons\nfor this success remain unclear because of the inherent complexity of deep learning.\nThis paper\nprovides empirical evidence of its eﬀectiveness and of a limitation of neural networks for language\nengineering. Precisely, we demonstrate that a neural language model based on long short-term memory\n(LSTM) eﬀectively reproduces Zipf’s law and Heaps’ law, two representative statistical properties\nunderlying natural language. We discuss the quality of reproducibility and the emergence of Zipf’s\nlaw and Heaps’ law as training progresses. We also point out that the neural language model has\na limitation in reproducing long-range correlation, another statistical property of natural language.\nThis understanding could provide a direction for improving the architectures of neural networks.\n1\nIntroduction\nDeep learning has performed spectacularly in various natural language processing tasks such as ma-\nchine translation (Wu et al. 2016), text summarization (Rush et al. 2015), dialogue systems (Serban\net al. 2015), and question answering (Tan et al. 2015). A fundamental question that we ask, however,\nis why deep learning is such an eﬀective approach for natural language processing. In contrast to the\nprogress made in applying deep learning, our understanding of the reasons for its eﬀectiveness remains\nlimited because of its inherent complexity.\nOne approach to tackling this problem is mathematical analysis of the potential of neural networks\n(Montufar et al. 2014; Cohen and Shashua 2016; Cohen et al. 2016; Bianchini and Scarselli 2014; Poole\net al. 2016; Lin and Tegmark 2016b; Schwab and Mehta 2016). Here, we take a diﬀerent empirical\napproach based on the statistical properties of text generated by neural networks.\nPrecisely, we\ncompare the statistical properties of pseudo-text generated by a neural language model with those of\nthe real text with which the model is trained.\nWe have found that two well acknowledged statistical laws of natural language—Zipf’s law (Zipf\n1965) and Heaps’ law (Heaps 1978)(Herdan 1964)(Guiraud 1954)—almost hold for the pseudo-text\ngenerated by a neural language model. This ﬁnding is notable because previous language models, such\nas Markov models, cannot reproduce such properties, and mathematical models, which are designed to\nreproduce statistical laws (Pitman 2006)(Simon 1955), are also limited in their purpose. As compared\nwith those models, neural language models are far more advanced in satisfying the statistical laws.\nWe ﬁnd a shortcoming of neural language models, however, in that the generated pseudo-text has\na limitation with respect to satisfying a third statistical property, the long-range correlation. The\nanalyses described in this paper contribute to our understanding of the performance of neural networks\nand provide guidance as to how we can improve models.\n1\narXiv:1707.04848v2  [cs.CL]  28 Nov 2017\n2\nNeural language models generate text following Zipf’s law and\nHeaps’ law\n2.1\nNeural language model\nWe constructed a neural language model that learns from a corpus and generates a pseudo-text, and\nthen investigated whether the model produced any statistical laws of language. The language model\nestimates the probability of the next element of the sequence, wi+1, given its past sequence or a subset\nas context:\nP(wi+1|wi\ni−k),\n(1)\nwhere k is the context length, and wj\ni is the subsequence of text between the ith and jth elements.\nBengio et al.(Bengio et al. 2003) ﬁrst proposed the concept of a neural language model, and this concept\nhas been explored mainly with recurrent neural networks (RNNs) (Krause et al. 2016)(Chelba et al.\n2017)(Sundermeyer et al. 2012). We construct a language model at the character level, which we\ndenote as a stacked long short-term memory (LSTM) (Hochreiter and Schmidhuber 1997) model.\nThis model consists of three LSTM layers with 256 units each and a softmax output layer. We treat\nthis stacked LSTM model as a representative of neural language models.\nIn all experiments in this article, the model was trained to minimize the cross-entropy by using an\nAdam optimizer with the proposed hyper-parameters (Kingma and Ba 2014). The context length k was\nset to 128. To avoid sample biases and hence increase the generalization performance, the dataset was\nshuﬄed during the training procedure: i.e. every one learning scan of the training data is conducted\nin a diﬀerent shuﬄed order. This is a standard conﬁguration with respect to previous research on\nneural language models (Krause et al. 2016)(Chelba et al. 2017)(Sundermeyer et al. 2012)(Lin and\nTegmark 2016a).\nIn the normal scheme of deep learning research, the model learns from all the samples of the\ntraining dataset once during an epoch. In this work, however, we redeﬁned the scheme so that the\nmodel learns from 1% of the training dataset during every epoch. That is, the model learns from\nall the samples every 100 epochs. We adopted this deﬁnition because the evolutions of Zipf’s law\nand Heaps’ law are so fast that their corresponding behaviors are clearly present after the model has\nlearned from all the samples once. Although we discuss this topic in Figure 5, we emphasize here that\neither this redeﬁnition or some other approach was necessary to observe the model’s development with\nrespect to Zipf’s law and Heaps’ law.\nGeneration of a pseudo-text begins with 128 characters in succession as context, where the 128-\ncharacter sequence exists in the original text. One character to follow the context is chosen randomly\naccording to the probability distribution of the neural model’s output. The context is then shifted\nahead by one character to include the latest character.\nThis procedure is repeated to produce a\npseudo-text of 2 million characters, except in Fig.2, 20 million. The following is an example of a\ngenerated pseudo-text: “and you gracious inherites and what saist i should agge the guest.”\nWe chose a character-level language model because word-level models have the critical problem of\nbeing unable to introduce new words during generation: by deﬁnition, they do not generate new words\nunless special architectures are added. A word-level model typically processes all words with rarity\nabove a certain threshold by transforming each into a singular symbol “unk”. With such a model, there\nis a deﬁnite vocabulary size limit, thus destroying the tail of the rank-frequency distribution. Zipf’s\nlaw and Heaps’ law therefore cannot be reproduced with such a model. There have been discussions\nand proposals regarding this “unk” problem (Gulcehre et al. 2016) (Luong and Manning 2016), but\nthere is no de facto standard approach, and the problem is not straightforward to solve. Therefore,\nwe chose a character-level language model.\nNote that the English datasets, consisting of the Complete Works of Shakespeare and The Wall\nStreet Journal (WSJ), were preprocessed according to (Krause et al. 2016) by making all alphabetical\n2\nFigure 1: The rank-frequency distribution and vocabulary growth of the Complete Works of Shake-\nspeare (left) and the corresponding pseudo-text generated by the stacked LSTM model (right). All\naxes in this and subsequent ﬁgures in this paper are in logarithmic scale, and the plots were gener-\nated using logarithmic bins. The model learned from 4,121,423 characters in the Complete Works\nof Shakespeare, which was preprocessed as described in the main text. The colored sets of the plots\nof the ﬁgures in the ﬁrst row show the rank-frequency distributions of 1,2,3,4,5 grams. The ﬁgures\nin the second row show the vocabulary growth in red. The exponents ξ and ζ (deﬁned in formulas\nFormula 2 and Formula 3, respectively) were estimated by linear regression from in log-log scale. For\nall graphs, the corresponding estimated exponents are indicated in the caption, and the black solid\nline in each vocabulary growth ﬁgure shows the ﬁtted line. The dashed line indicates a reference with\nan exponent of 1.0. The same applies to all other rank-frequency distribution and vocabulary growth\nplots in this paper.\ncharacters lower case and removing all non-alphabetical characters except spaces. Consecutive spaces\nwere also reduced to one space.\n2.2\nZipf’s law and Heaps’ law for pseudo-texts generated by neural language\nmodels\nZipf’s law and Heaps’ law are two representative statistical properties of natural language. Zipf’s law\nstates that, given word rank u and frequency F(u) for a word of rank u, the following proportionality\nholds:\nF(u) ∝u−ξ.\n(2)\n3\nThis exponent ξ is approximately 1.0, according to Zipf, for individual word occurrence (uni-grams),\nbut, as will be shown, a power law with smaller ξ values holds for longer word sequences (i.e., n-grams,\nincluding 2-grams, 3-grams, and so on).\nHeaps’ law, another statistical law of natural language, underlies the growth rate of vocabulary\nsize (the number of types) with respect to text length (the number of tokens). Given vocabulary size\nV (m) for a text of length m, Heaps’ law indicates that\nV (m) ∝mζ.\n(3)\nThe power law underlying vocabulary growth was reported even before Heaps’ paper (Heaps 1978),\nas in (Herdan 1964)(Guiraud 1954), but in this paper we refer to the law as Heaps’ law. Zipf’s law\nand Heaps law are known to have a theoretical relationship, as discussed in (BaezaYates and Navarro\n2000)(van Leijenhorst and van der Weide 2005)(Lu et al. 2010).\nThe upper-left graph in Figure 1 shows the rank-frequency distribution of the Complete Works of\nShakespeare, consisting of 4,121,423 characters, for n-grams ranging from uni-grams to 5-grams. As\nZipf stated, the uni-gram distribution approximately follows a power law with an exponent of 1.0. The\nhigher n-gram distributions also follow power laws but with smaller exponents. Note that intersection\nof the uni-gram and 2-gram distributions in the tail is typically observed for natural language. The\nlower-left graph in Figure 1 shows the vocabulary growth of the Complete Works of Shakespeare. The\nred points show the vocabulary size V (m) for every text length m, and the exponent ζ was estimated\nas 0.773, as shown by the black ﬁtting line. This exponent is larger than that reported in previous\nworks, and this was due to the preprocessing, as previously mentioned.\nThe graphs on the right side of Figure 1 show the corresponding rank-frequency distribution and\nvocabulary growth of the pseudo-text generated by the stacked LSTM. The rank-frequency distribu-\ntion is almost identical to that of the Complete Works of Shakespeare for uni-grams and 2-grams,\nreproducing the original shape of the distribution. The distributions for longer n-grams are also well\nreproduced. As for the vocabulary growth, the language model introduces new words according to a\npower law with a slightly larger exponent than that of the original text. This suggests a limitation on\nthe recognition of words and the organization of n-gram sequences. These results indicate that the\nstacked LSTM can reproduce an n-gram structure closely resembling the original structure.\nThe potential of the stacked LSTM is still apparent even when we change the kind of text. Figure 2\nand Figure 3 show results obtained using The Wall Street Journal (from the Penn Tree Bank Dataset)\nand a Chinese literary text, Hong Lou Meng by X. C. Xueqin, respectively, and the corresponding\npseudo-texts generated by the stacked LSTM. The WSJ text of 4, 780, 916 characters was subjected to\nthe same preprocessing as for the Complete Works of Shakespeare. To deal with the large vocabulary\nsize of the Chinese characters, the model was trained at the byte level (Sennrich et al. 2016) for Hong\nLou Meng, resulting in a text of 2,932,451 bytes. To measure the rank-frequency distribution and\nvocabulary growth at the word level, the model had to learn not only the sequence of bytes but also\nthe splits between them.\nThe observations made for the Complete Works of Shakespeare apply also to Figure 2 and Figure 3.\nWe observe power laws for both the rank-frequency distributions and the vocabulary growth. The\nstacked LSTM replicates the power-law behaviors well, reproducing approximately the same shapes\nfor smaller n-grams. The intersection of the uni-gram and 2-gram rank-frequency distributions is\nreproduced as well. As for the vocabulary growth, the reproduced exponents were a little larger than\nthe original values, as seen for the case of Shakespeare.\nFigure 2 also highlights the high capacity of the stacked LSTM in learning with long n-grams. The\ntop right graph demonstrates that the stacked LSTM could repeat the same expression of 8-grams and\n16-grams obeying Zipf’s law. In the Complete Works of Shakespeare, written by a single author, long\nrepeated n-grams hardly occur, but the WSJ dataset contains many of these. For the WSJ data, the\nrank-frequency distributions of 8- and 16-grams do not obviously follow power laws, mainly because\n4\nFigure 2: The rank-frequency distribution and vocabulary growth of The Wall Street Journal (left)\nand the corresponding pseudo-text generated by the stacked LSTM model (right). The model learned\nfrom 4,780,916 characters. The length of the pseudo-text is 20 million characters. The rank-frequency\ndistributions are shown for 1,2,3,4,5,8,16-grams. The preprocessing procedure was the same as for the\nComplete Works of Shakespeare.\nof repetition of the same expressions. With such a corpus, the stacked LSTM can also reproduce the\npower-law behavior of the rank-frequency distribution of long n-grams.\nThese results indicate that a neural language model can learn the statistical laws behind natural\nlanguage, and that the stacked LSTM is especially capable of reproducing both patterns of n-grams\nand the properties of vocabulary growth.\nWe also tested language models with diﬀerent architectures. S1Fig shows results with diﬀerent\nneural architectures for pseudo-texts generated for the Complete Works of Shakespeare: a convolu-\ntional neural net (CNN), simple RNN, single-layer LSTM, and stacked LSTM. The stacked LSTM\nmodel was explained in §2.1, while the details of the other models are given in the caption of S1Fig.\nThe two bottom right graphs for the stacked LSTM are identical to the two righthand graphs in Fig-\nure 1. Overall, all the models using an RNN reproduce power law behavior, but a closer look reveals\ngreater capacity with the stacked LSTM. With the CNN (upper left), on the other hand, the shape\nof the rank-frequency distribution is quite diﬀerent, and the exponent of the vocabulary growth is too\nlarge. The simple RNN (upper right) shows weaker capacity in reproducing longer n-grams, and the\nexponent is still too large. Finally, the single-layer LSTM (bottom left) is less capable of learning the\nlongest n-gram of 5-grams as compared with the stacked LSTM (bottom right).\n5\nFigure 3: The rank-frequency distribution and vocabulary growth of the Chinese literary work Hong\nLou Meng, consisting of 2, 932, 451 bytes (left), and the pseudo-text generated by the stacked LSTM\n(right). The text was processed at the byte level with word borders.\nFigure 4: Rank-frequency distribution (left) and vocabulary growth (right) for a text of 20 million\ncharacters generated by the stacked LSTM without learning.\n6\n+1\nFigure 5: Training cross entropy as a function of the number of epochs (upper left); Zipf’s exponent\nξ (middle left) and Heaps’ exponent ζ (lower left) of pseudo-texts generated at diﬀerent epochs; and\nthe rank-frequency distributions of the pseudo-texts at various epochs (right) for the Complete Works\nof Shakespeare. The left-hand graphs are in logarithmic scale for the x-axes and linear scale for the\ny-axes. The ﬁtting lines for Zipf’s exponent ξ and Heaps’ exponent ζ were estimated from a linear\nregression with a semi-log scale.\n3\nThe Emergence of Zipf’s Law and Heaps’ Law\nThe stacked LSTM acquires the behaviors of Zipf’s law and Heaps’ law as learning progresses. It starts\nlearning obviously at the level of a monkey typing. Figure 4 shows the rank-frequency distribution\nand vocabulary growth of a texts generated by the stacked LSTM without training. Each case from\nuni-grams to 3-grams roughly forms a power-law kind of step function. The vocabulary growth follows\na power law with exponent ζ ≈1 because monkey typing consistently generates “new words.”\nAs shown by Figure 4, monkey-typed texts can theoretically produce power-law-like behaviors\nin the rank-frequency distribution and vocabulary growth. (Miller 1957) demonstrates how monkey\ntyping generates a power-law rank-frequency distribution. Following the explanation in (Mitzenmacher\n2003), we brieﬂy summarize the rationale as follows. Consider a monkey that randomly types any\nof n characters and the space bar. Since a space separates words, let its probability be q, and then\neach of the other characters is hit uniformly with a probability of (1 −q)/n. Given that the number\nof words of length c is nc, and that longer words are less likely to occur, then the rank frequency of\na word of length c is between S(c) + 1 and S(c + 1), where S(c) = Pc\ni=1 ni. Since S(c) = nc−1\nn−1 , the\nrank rc of a word of length c grows exponentially with respect to c; i.e., rc ≈nc. Given that the\n7\nprobability of occurrence of a word of length c is q(1−q\nn )c, by replacing c with the rank, we obtain the\nrank-probability distribution as\nP(rc) = q(1 −q\nn\n)log rc = q(rc)log(1−q)−1,\n(4)\nwhere the log is taken with base n. This result shows that the probability distribution follows a power\nlaw with respect to the rank. The LSTM models therefore start learning by innately possessing a\npower-law feature for the rank-frequency distribution and vocabulary growth. The learning process\nthus smooths the step-like function into a more continuous distribution; moreover, it decreases the\nexponent for vocabulary growth. (Bell et al. 1990) reports empirically that when the probabilities\nof each character are diﬀerent, the rank-frequency distribution becomes smoother. While learning\nprogresses, the exponent ζ is lowered by learning patterns within texts.\nFigure 5 illustrates the training progress of the language model for the Complete Works of Shake-\nspeare. The upper-left graph shows the cross entropy of the model at diﬀerent training epochs. The\ntraining successfully decreases the cross entropy and reaches a convergent state.\nThe middle and lower left graphs in Figure 5 show the Zipf’s exponent ξ and Heaps’ exponent\nζ of the pseudo-texts generated at diﬀerent epochs. At the very beginning of training, the Zipf’s\nexponents ξ tend to be smaller than that of the original dataset. They generally increase and become\nequivalent to the values of the original datasets for short n-grams or remain at smaller values for long\nn-grams. As the model minimizes the cross entropy, the Heaps’ exponent ζ generally decreases, with\nsome ﬂuctuation, by learning words. It roughly stops decreasing, however, at around 102 to103 epochs.\nThe fact that the exponents of Heaps’ law cannot reach the value of the original text indicates some\nlimitation in learning.\nThe right-hand side of Figure 5 shows the rank-frequency distributions of the pseudo-texts gen-\nerated at diﬀerent epochs.\nThe stacked LSTM model reproduces the power-law behavior well for\nuni-grams and 2-grams, and partially for 3-grams, with just a single epoch (upper right). Such be-\nhavior for 4-grams appears in epoch 2 (middle left), and the intersection of the uni-gram and 3-gram\npower laws appears in epoch 7 (middle right). Power-law behavior for 5-grams emerges in epoch 51\n(bottom left), and no further qualitative change is observed afterwards (bottom right).\nAs training progresses, the stacked LSTM ﬁrst learns short patterns (uni-grams and 2-grams)\nand then gradually acquires longer patterns (3- to 5-grams). It also learns vocabulary as training\nprogresses, which lowers the exponent of Heaps’ law. There are no tipping points at which the neural\nnets drastically change their behavior, and the two power laws are both acquired at a fairly early stage\nof learning.\n4\nNeural language models are limited in reproducing long-range\ncorrelation\nNatural language has structural features other than n-grams that underlie the arrangement of words. A\nrepresentative of such features is grammar, which has been described in various ways in the linguistics\ndomain.\nThe structure underlying the arrangement of words has been reported to be scale-free,\nglobally ranging across sentences and at the whole-text level. One quantiﬁcation methodology for\nsuch global structure is long-range correlation.\nLong-range correlation describes a property by which two subsequences within a sequence remain\nsimilar even with a long distance between them. Typically, such sequences have a power-law rela-\ntionship between the distance and the similarity. This statistical property is observed for various\nsequences in complex systems. Various studies (Ebeling and P¨oschel 1994; Ebeling and Neiman 1995;\nMontemurro and Pury 2002; Kosmidis et al. 2012; Altmann et al. 2009: 2012; Montemurro 2014;\nTanaka-Ishii and Bunde 2016) report that natural language has long-range correlation as well.\n8\nFigure 6: Mutual information for a Wikipedia source (left), the Complete Works of Shakespeare (mid-\ndle), and The Wall Street Journal (right). The red and blue points represent the mutual information\nof the datasets and the generated pseudo-texts, respectively. Following (Lin and Tegmark 2016a), the\nWikipedia source was preprocessed to exclude rare symbols.\n4.1\nThe power decay of mutual information is unlikely to hold for natural language\ntext\nMeasurement of long-range correlation is not a simple problem, as we will see, and various methods\nhave been proposed. (Lin and Tegmark 2016a) proposes applying mutual information to measure\nlong-range dependence between symbols. The mutual information at a distance s is deﬁned as\nIs(X, Y ) =\nX\nX=a,Y =b\nP(X, Y ) log\nP(X, Y )\nP(X)P(Y )\n(5)\nwhere X and Y are random variables of elements in each of two subsequences at distance s.\n(Lin and Tegmark 2016a) proves mathematically how a sequence generated with their simple re-\ncursive grammar model results in power decay of the mutual information. They also provide empirical\nevidence that a Wikipedia source from the enwik8 dataset exhibits power decay of the mutual infor-\nmation, and that a pseudo-text generated from Wikipedia also exhibits power decay when measured\nat the character level. The left graph in Fig. 6 shows our reproduction of the mutual information\nfor the Wikipedia source used in (Lin and Tegmark 2016a). For each of the graphs in the ﬁgure, the\nhorizontal axis indicates the distance s, and the vertical axis indicates the mutual information. The\nred and blue points represent the results for the real texts and the pseudo-texts, respectively. By using\nthe data in (Lin and Tegmark 2016a), we could reproduce their results: for the Wikipedia source, the\nmutual information exhibits power decay, and this statistical property was also learned by the stacked\nLSTM.\nWe doubt, however, that the power decay of the mutual information is being properly observed for\na natural language text when measured with the method proposed in (Lin and Tegmark 2016a). The\nmiddle and right graphs in Fig. 6 show the results for the Complete Works of Shakespeare and The\nWall Street Journal, which are more standard natural language datasets. The mutual information\nexhibits an exponential decay showing short-range correlation similar to what they reported as the\nbehavior of Markov models, and it almost reaches a plateau just after a 10-character distance for\nboth datasets. This plateau represents a state in which the probabilistic distributions of a and b pairs\nbecome almost the same because of the low frequency problem. Following the statistical properties\nof the datasets, the stacked LSTM replicates this exponential decay well. (Lin and Tegmark 2016a)\nalso examines a natural language corpus, corpatext. Unfortunately, corpatext is poorly organized, as\nit contains a huge number of repeats of long n-grams, chains of numbers, many meta-characters, and\n9\nsuccessive spaces. Our measurement of mutual information with corpatext (which was preprocessed\nto delete meta-characters and successive spaces) gave an exponential decay up to 10 characters. This\nobservation is similar to the results for the Complete Works of Shakespeare and WSJ. The mutual\ninformation slowly decayed after a length of 10, however, which could lead to misinterpretation of the\npower decay. (Lin and Tegmark 2016a) does not clearly mention any preprocessing or measurement\nof the mutual information of a pseudo-text for corpatext. We cannot reach a solid conclusion with\nsuch an unorganized corpus.\nThere are two reasons for this diﬀerence in results between the Wikipedia source and the Shake-\nspeare/WSJ: the kind of data, and the quantiﬁcation method. Regarding the kind of data, we must\nemphasize that (Lin and Tegmark 2016a) does not use any standard natural language text for veriﬁ-\ncation. Instead, they use the wiki source, including all Wikipedia annotations. Therefore, Wikipedia\nis strongly grammatical, that they consider for their mathematical proof.\nAs for the problem of the quantiﬁcation method, as seen from the plateau appearing in the results\nfor the Shakespeare and WSJ datasets, the mutual information in its basic form is highly susceptible\nto the low frequency problem. Therefore, (Lin and Tegmark 2016a) veriﬁes data with a small alphabet\nsize (including DNA sequences). When the alphabet is increased to the size of the Chinese character\nset, the mutual information reaches the plateau almost immediately; when using words, the plateau\nis reached in only two or three points.\nStill, (Lin and Tegmark 2016a) provides a crucial understanding of neural nets: the stacked LSTM\n(or other LSTM-like models) can replicate the power decay of the mutual information, if it exists in\nthe original data. Whether such strong long-range correlation exists, however, depends on the data\ntype. Given all other reports, as will be mentioned shortly, long-range correlation does exist in natural\nlanguage texts. The problem of how to quantify it is non-trivial, however, and the mutual information,\nas proposed, is not always a good measurement for natural language.\n4.2\nNeural language models cannot reproduce the power decay of the autocorre-\nlation function in natural language\nQuantiﬁcation of long-range correlation has been studied in the statistical physics domain and has been\neﬀective in analyzing extreme events in natural phenomena and ﬁnancial markets (Corral 1994: 2005;\nBunde et al. 2005; Santhanam and Kantz 2005; Blender et al. 2015; Turcotte 1997; Yamasaki et al.\n2007; Bogachev et al. 2007). The long-range correlation in this application is also a scale-free property\ntaking a power-law form. Long-range correlation is explained here as a “clustering phenomenon” of\nrare events. This could have some relation to an underlying grammar-like structure in a sequence, but\nthe measure might quantify a phenomenon diﬀerent from that captured by the mutual information.\nThe application of long-range correlation to natural language is controversial, because all proposed\nmethods are for numerical data, whereas natural language has a diﬀerent nature. Various reports show\nhow natural language is indeed long-range correlated (Ebeling and P¨oschel 1994; Ebeling and Neiman\n1995; Montemurro and Pury 2002; Altmann et al. 2009: 2012; Montemurro 2014). We apply the most\nrecent method (Tanaka-Ishii and Bunde 2016) to investigate whether the pseudo-text generated by a\nstacked LSTM retains long-range correlation.\nThis method is based on the autocorrelation function applied to a sequence R = r1, r2, . . . , rN with\nmean µ and standard deviation σ:\nC(s) =\n1\n(N −s)σ2\nN−s\nX\ni=1\n(ri −µ)(ri+s −µ),\n(6)\nwith C(0) = 1.0 by deﬁnition. Note that this function C(s) measures the similarity between two\nsubsequences that are distance s apart. If this autocorrelation function exhibits a power decay as\n10\nFigure 7:\nLong-range correlation measured with the autocorrelation function by the method of\n(Tanaka-Ishii and Bunde 2016) for the original and generated texts of the Complete Works of Shake-\nspeare (left) and The Wall Street Journal (right). The upper and lower graphs are in log-log and\nsemi-log scale, respectively. The ﬁtting line for The Wall Street Journal was estimated from the data\npoints where s < 100.\nfollows,\nC(s) = C(1)s−γ, s > 0,\n(7)\nthen the sequence R is long-range correlated. The functional range of C(s) is between −1.0 and 1.0.\nIf there is no correlation, C(s) is almost zero; if the sequences are well correlated positively, C(s) takes\na positive value. The method used here is based on the intervals between rare words, and we use the\nrarest 1/16th of words among all words appearing in a text, following (Tanaka-Ishii and Bunde 2016),\nin deﬁning the interval sequence R.\nFigure 7 shows the autocorrelation functions of the Complete Works of Shakespeare (left) and The\nWall Street Journal (right) at the word level. The upper graphs show log-log plots, whereas the lower\ngraphs show semi-log plots for the same sets of results. The red and blue points represent the original\ndataset and the pseudo-text, respectively. For the original Complete Works of Shakespeare, the results\nexhibit a clear, slow power decay up to s = 103. This behavior is similar to that of other literary\ntexts reported in (Tanaka-Ishii and Bunde 2016). In contrast, the autocorrelation function of the\npseudo-text takes values around 0 for any s. The Wall Street Journal results show similar behavior.\nThe power decay is faster than that of the Complete Works of Shakespeare and other single-author\nliterary texts, but the autocorrelation function still takes positive values and exhibits power decay up\nto about s = 102.\nIn summary, this analysis provides qualitative evidence regarding a shortcoming of the stacked\n11\nLSTM: it has a limitation with respect to reproducing long-range correlation, as quantiﬁed using a\nmethod proposed in the statistical physics domain.\nTo further clarify the stacked LSTM’s performance and limitation, we conducted three experiments\nfrom diﬀerent perspectives. First, S2Fig shows the encoding rate decay of the original text (WSJ)\nwith diﬀerent types of shuﬄing and the pseudo-text.\nThe encoding rate is a measure of a text’s\npredictability, and we provide more explanation in the caption of S2Fig. The results show that the\npseudo-text generated by the stacked LSTM model is more predictable than the word-shuﬄed WSJ,\nand less predictable than both the original and document-shuﬄed WSJ.\nSecond, S3Fig shows the mutual information and autocorrelation function of the original text\n(Shakespeare) and pseudo-texts generated by diﬀerent neural architectures. The mutual information\ndecays rapidly up to around s = 10, and the models except for the CNN model reproduce the behavior\nof the mutual information of the original dataset well. On the other hand, the power decay exhibited\nby the original dataset was never reproduced by any model that we tested.\nThird, S4Fig shows the autocorrelation function for the French novel, Les Mis´erables, and the\ntext translated into English by a neural machine translation system. We obtained the translated text\nfrom the Google Cloud Translation API(https://cloud.google.com/translate/). The translated\ntext maintains the power decay of the autocorrelation function observed for the original text. This\ncan be explained from the properties of machine translation. Machine translation is not expected\nto radically change the order of corresponding words between sentences. Therefore, as long as the\ntranslation system has the capacity to output rare words in the original text, the autocorrelation\nshould be preserved..\nBecause long-range correlation is a global, scale-free property of a text, one reason for the limitation\nof the stacked LSTM could lie in the context length of k = 128 at the character level. Considering\nthe availability of computational resources, however, this setting was a maximum limit, as the number\nof layers to be computed substantially increases with the context length.\nMoreover, it has been\nempirically reported that an LSTM architecture cannot retain past information beyond a length of\n100 (Hochreiter and Schmidhuber 1997).\nOne possible future approach is to test new neural models with enhanced long-memory features,\nsuch as a CNN application (van den Oord et al. 2016) (Kalchbrenner et al. 2016) or the hierarchical\nstructure of an RNN (Hihi and Bengio 1995)(Mehri et al. 2016). Overall, the behavior of pseudo-\ntexts with respect to the statistical laws of natural language partly reveals both the eﬀectiveness and\nlimitations of neural networks, which tend to remain black boxes because of their complexity. Analysis\nusing statistical laws could provide a direction towards improving the architectures of neural networks.\n5\nConclusion\nTo understand the eﬀectiveness and limitations of deep learning for natural language processing, we\nempirically analyzed the capacity of neural language models in terms of the statistical laws of natural\nlanguage. This paper considered three statistical laws of natural language: Zipf’s law, the power law\nunderlying the rank-frequency distribution; Heaps’ law, the power-law increase in vocabulary size with\nrespect to text size; and long-range correlation, which captures the self-similarity underlying natural\nlanguage sequences.\nThe analysis revealed that neural language models satisfy Zipf’s law, not only for uni-grams, but\nalso for longer n-grams.\nTo the best of our knowledge, this is the ﬁrst language model that can\nreproduce a statistical law at such a level. The language models also satisfy Heaps’ law: they generate\ntext with power-law vocabulary growth. The exponent remained higher than for the original texts,\nhowever, which showed both the limitation of detecting words and the self-organization of linguistic\nsequences.\n12\nFinally, a stacked LSTM showed a limitation with respect to capturing the long-range correlation\nof natural language. Investigation of a previous work(Lin and Tegmark 2016a) revealed that if the\noriginal learning text has a strong grammatical structure, then a stacked LSTM has the potential to\nreproduce it. A standard natural language text, however, does not have such a feature. The long-\nrange correlation quantiﬁed with another methodology for the original texts was not reproduced by\nthe stacked LSTM.\nOur analysis suggests a direction for improving language models, which has always been the central\nproblem in handling natural language on machines. The current neural language models are unable to\nhandle the global structures underlying texts. Because the Zipf’s law behavior with long n-grams was\nreproduced well by the stacked LSTM, this neural language model has high capacity for recognizing and\nreproducing local patterns or phrases in the original text. The model could not, however, reproduce\nthe long-range correlation measured by the autocorrelation function for intervals between rare words.\nThis long-range correlation cannot be reduced to local patterns but rather is a representation of global\nstructure in natural language. This irreproducibility demonstrates the limitation of neural language\nmodels and is a challenge for language modeling with neural networks.\nOur future work thus includes exploring conditions to reproduce the long-range correlation in text\nwith language models, including both stochastic and neural language models.\nReferences\nAltmann, E.G. , Pierrehumbert, J.B. , and Motter, E.A. (2009). Beyond word frequency: Bursts,\nlulls, and scaling in the temporal distributions of words. PLOS one.\nAltmann, E.G. , Cristadoro, G. , and Esposti, M.D. (2012). On the origin of long-range correlations\nin texts. In Proceedings of the National Academy of Sciences, volume 109, pages 11582–11587.\nBaezaYates, Ricardo and Navarro, Gonzalo (2000). Block addressing indices for approximate text\nretrieval. Journal of the American Society for Information Science, 51(1), 6982.\nBell, T.C. , Cleary, J.G. , and Witten, H. (1990). Text Compression. Prentice-Hall.\nBengio, Y. , Ducharme, R. , Vincent, P. , and Jauvin, C. (2003). A neural probabilistic language\nmodel. The Journal of Machine Learning Research, 3, 1137–1155.\nBianchini, Monica and Scarselli, Franco (2014). On the complexity of neural network classiﬁers: A\ncomparison between shallow and deep architectures. IEEE transactions on neural networks and\nlearning systems, 25(8), 1553–1565.\nBlender, R. , Raible, C. , and Lunkeit, F. (2015). Non-exponential return time distributions for vortic-\nity extremes explained by fractional poisson processes. Quarterly Journal of the Royal Meteorology\nSociety, 141, 249–257.\nBogachev, M.I. , Eichner, J.F. , and Bunde, A. (2007). Eﬀect of nonlinear correlations on the statistics\nof return intervals in multifractal data sets. Physical Review Letters, 99(240601).\nBunde, A. , Eichner, J. , Havlin, S. , and Kantelhardt, J.W. (2005). Long-term memory: A natural\nmechanism for the clustering of extreme events and anomalous residual times in climate records.\nPhysical Review Letters, 94(048701).\nChelba, Cipran , Norouzi, Mohammad , and Bengio, Samy (2017). N-gram language modeling using\nrecurrent neural network estimation. arXiv preprint, abs/1703.10724.\n13\nCohen, Nadav and Shashua, Amnon (2016). Convolutional rectiﬁer networks as generalized tensor\ndecompositions. In Proceedings of the 33th International Conference on Machine Learning, pages\n955–963.\nCohen, Nadav , Sharir, Or , and Shashua, Amnon (2016). On the expressive power of deep learning: A\ntensor analysis. In Proceedings of the 29th Annual Conference on Learning Theory, pages 698–728.\nCorral, A. (1994).\nLong-term clustering, scaling, and universality in the temporal occurrences of\nearthquakes. Physical Review Letters, 92(108501).\nCorral, A. (2005).\nRenomalization-group transformations and correlations of seismicity.\nPhysical\nReview Letters, 95(028501).\nEbeling, W. and Neiman, A. (1995). Long-range correlations between letters and sentences in texts.\nPhysica A, 215, 233–241.\nEbeling, W. and P¨oschel, T. (1994). Entropy and long-range correlations in literary english. Euro-\nphysics Letters, 26, 241–246.\nGuiraud, H. (1954). Les Charact`eres Statistique du Vocabulaire. Universitaires de France Press.\nGulcehre, C. , Ahn, S. , Nallapati, R. , Hou, B. , and Bengio, Y. (2016). Pointing the unknown words.\npages 140–149.\nHeaps, H. S. (1978). Information Retrieval: Computational and Theoretical Aspects, Academic Press.\nHerdan, G. (1964). Quantitative Linguistics. Butterworths.\nHihi, Salah and Bengio, Yoshua (1995). Hierarchical recurrent neural networks for long-term depen-\ndencies. In Advances in Neural Information Processing Systems 8, pages 493–499.\nHilberg, W. (1990).\nDer bekannte Grenzwert der redundanzfreien Information in Texten — eine\nFehlinterpretation der Shannonschen Experimente? Frequenz, 44, 243–248.\nHochreiter, S and Schmidhuber, J (1997).\nLong short-term memory.\nNeural computation, 9(8),\n17351780.\nKalchbrenner, Nam , Espeholt, Lasse , Simonyan, Karen , van den Oord, Aaron , Graves, Alex\n, and Kavukcuoglu, Koray (2016).\nNeural machine translation in linear time.\narXiv preprint,\nabs/1610.10099.\nKingma, D and Ba, J (2014).\nAdam:\nA method for stochastic optimization.\narXiv preprint,\nabs/1412.6980.\nKosmidis, K. , Kalampokis, A. , and Argyrakis, K. (2012). Language time series analysis. Physica A,\n370, 808–816.\nKrause, B. , Lu, L. , Murray, I. , and Renals, S. (2016). Multiplicative lstm for sequence modeling.\narXiv preprint, abs/1609.07959.\nLin, H and Tegmark, M (2016a). Critical behavior from deep dynamics: A hidden dimension in natural\nlanguage. arXiv preprint, abs/1606.06737.\nLin, Henry and Tegmark, Max (2016b). Why does deep and cheap learning work so well?\narXiv\npreprint, abs/1608.08225.\n14\nLu, Linyuan , Zhang, Zi-Ke , and Zhou, Tao (2010). Zipf’s law leads to heaps’ law: Analyzing their\nrelation in ﬁnite-size systems. arXiv preprint.\nLuong, Minh-Thang and Manning, Christopher (2016). Achieving open vocabulary neural machine\ntranslation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, page 10541063.\nMehri, Soroush , Kumar, Kundan , Gulrajani, Ishaan , Kumar, Rithesh , Jain, Shubham , Sotelo, Jose\n, Courville, Aaron , and Bengio, Yoshua (2016). Samplernn: An unconditional end-to-end neural\naudio generation model. arXiv preprint, abs/1612.07837.\nMiller, G.A. (1957). Some eﬀects of intermittent silence. American Journal of Psychology, 70, 311–314.\nMitzenmacher, M. (2003). A brief history of generative models for power law and lognormal distribu-\ntions. Internet Mathematics, 1(2), 226–251.\nMontemurro, M.A. (2014). Quantifying the information in the long-range order of words: Semantic\nstructures and universal linguistic constraints. Cortex, 55, 5–16.\nMontemurro, M. and Pury, P.A. (2002). Long-range fractal correlations in literary corpora. Fractals,\n10, 451–461.\nMontufar, Guido F. , Pascanu, Razvan , Cho, Kyunghyun , and Bengio, Yoshua (2014).\nOn the\nnumber of linear regions of deep neural networks. In Advances in Neural Information Processing\nSystems 27, pages 2924–2932.\nPitman, J. (2006). Combinatorial Stochastic Processes. Springer.\nPoole, Ben , Lahiri, Subhaneil , Raghu, Maithreyi , Sohl-Dickstein, Jascha , and Ganguli, Surya\n(2016). Exponential expressivity in deep neural networks through transient chaos. In Advances in\nNeural Information Processing Systems 29, pages 3360–3368.\nRush, Alexander M. , Chopra, Sumit , and Weston, Jason (2015).\nA neural attention model for\nabstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 379–389.\nSanthanam, M. and Kantz, H. (2005). Long-range correlations and rare events in boundary layer wind\nﬁelds. Physica A, 345, 713–721.\nSchwab, D. J. and Mehta, P. (2016). Comment on “Why does deep and cheap learning work so well?”.\narXiv preprint, 1abs/1609.03541.\nSennrich, R. , Haddow, B. , and Birch, A. (2016). Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics, pages 1715–1725.\nSerban, Iulian , Sordoni, Alessandro , Bengio, Yoshua , Courville, Aaron , and Pineau, Joelle (2015).\nBuilding end-to-end dialogue systems using generative hierarchical neural network models. In Pro-\nceedings of the 30th AAAI Conference on Artiﬁcial Intelligence.\nSimon, H.A. (1955). On a class of skew distribution functions. Biometrika, 42(3/4), 425–440.\nSundermeyer, Martin , Schlter, Ralf , and Herman, Ney (2012). Lstm neural networks for language\nmodeling. In 13th Annual Conference of the International Speech Communication Association, pages\n194–197.\n15\nTakahira,\nRyosuke\n,\nTanaka-Ishii,\nKumiko\n,\nand\nLukasz,\nDebowski\n(2016).\nLarge\nscale\nveriﬁcation\nof\nentropy\nof\nnatural\nlangauge.\nEntropy.\nOnline\nJournal.\nHTML\nVersion:\nhttp://www.mdpi.com/1099-4300/18/10/364/html,\nPDF\nVersion:\nhttp://www.mdpi.com/1099-4300/18/10/364/pdf.\nTan, Ming , dos Santos, Cicero , Xiang, Bing , and Zhou, Bowen (2015). Lstm-based deep learning\nmodels for non-factoid answer selection. arXiv preprint, abs/1511.04108.\nTanaka-Ishii, K. and Bunde, A. (2016).\nLong-range memory in literary texts: On the universal\nclustering of the rare words. PLOS One, 11(11), e0164658.\nTurcotte, D.L. (1997). Fractals and Chaos in Geology and Geophysics. Cambridge University Press.\nvan den Oord, A , Dieleman, S , and Zen, H (2016). Wavenet: A generative model for raw audio.\narXiv preprint, abs/1609.03499.\nvan Leijenhorst, D.C. and van der Weide, Th.P. (2005). A formal derivation of heaps law. Information\nSciences, 170(2-4), 263272.\nWu, Y , Schuster, M , Chen, Z , Le, Q , Norouzi, M , Macherey, W , Krikun, M , Cao, Y , Gao, Q\n, Macherey, K , and et al. (2016). Googles neural machine translation system: Bridging the gap\nbetween human and machine translation. arXiv preprint, abs/1609.08144.\nYamasaki, K. , Muchnik, L. , Havlin, S. , Bunde, A. , and Stanley, H.E. (2007). Scaling and memory\nin volatility return intervals in ﬁnancial markets. Proceedings of the National Acaddemy of Sciences,\n102, 9424–9428.\nZipf, G.K. (1965). Human behavior and the principle of least eﬀort: An introduction to human ecology.\nHafner.\n16\nWe thank JST PRESTO for ﬁnancial support.\nMoreover, we thank Ryosuke Takahira of the\nTanaka-Ishii Group for his help in creating Fig. 6.\n17\nSupporting Information\nS1Fig. Comparison of the rank-frequency distribution and vocabulary growth of diﬀerent models for\nthe Complete Works of Shakespeare. Each pair of graphs consists of the rank-frequency distribution\n(upper graph) and the vocabulary growth (lower graph). The models had the following speciﬁcations.\nCNN: 8 layers of one-dimensional convolution with 256 ﬁlters having a width of 7 without padding and\nglobal max pooling after the last convolutional layer. The activation function was rectiﬁed linear-unit,\nand batch normalization was applied before activation in every convolutional layer. Simple-RNN: 1\nlayer of RNN with 512 units and an output softmax layer. Single-layer LSTM: 1 layer of LSTM with\n512 units and an output softmax layer. Stacked-LSTM: as described in Section 2.\n18\nS2Fig. Encoding rate decay and ﬁtting functions for the WSJ with various shuﬄing methods and the\ncorresponding pseudo-text generated by the stacked LSTM. Let Xn\n1 be a text of length n characters,\nand let R(Xn\n1 ) be its size in bits after compression. Then the code length per unit, i.e., the encoding\nrate, is deﬁned as r(n) = R(Xn\n1 )/n. The more predictable the text is, the smaller r(n) becomes;\ntherefore, r(n) is smaller for longer n, exhibiting decay. The ﬁtting function here is a power ansatz\nfunction, f(n) = Anβ−1+h, proposed by (Hilberg 1990), and the compressor was PPMd, using the 7zip\napplication (refer to (Takahira et al. 2016) for details). In addition to the original text, the WSJ was\nshuﬄed at the character, word, and document levels. The decay of the pseudo-text is situated between\nthe decays of the word- and document-shuﬄed versions, indicating clearly that its predictability is\nsituated between the two and suggesting that the pseudo-text has lower predictability as compared to\nthe original and document-shuﬄed WSJ.\n19\nS3Fig. Comparison of the mutual information and autocorrelation function for pseudo-texts generated\nby diﬀerent models on the Complete Works of Shakespeare. Each pair of graphs represents the mutual\ninformation (upper) and the autocorrelation function (lower). The results were obtained with a CNN\n(upper left), simple RNN (upper right), single-layer LSTM (lower left), and stacked LSTM (bottom\nright, the same graphs from Figure 1). For the speciﬁcations of every model, see the caption of S1Fig.\n20\nS4Fig. Autocorrelation functions of the original text of Les Mis´erables (V. Hugo, 621,641 words)\nin French and its corresponding text translated into English by the Google Cloud Translation API\nhttps://cloud.google.com/translate/, which is based on neural machine translation. Because of\nthe API’s requirements, the original text was split into 5000 characters to obtain the translated text.\nDespite the results given in §4.2, the translated text exhibits long-range correlation as measured by the\nautocorrelation function. This result does not contradict our observation in §4.2, because translation\ndoes not radically change the order of words and the translation system has the capacity to output\nrare words.\n21\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-07-16",
  "updated": "2017-11-28"
}