{
  "id": "http://arxiv.org/abs/1707.07217v1",
  "title": "Deep Learning in Robotics: A Review of Recent Research",
  "authors": [
    "Harry A. Pierson",
    "Michael S. Gashler"
  ],
  "abstract": "Advances in deep learning over the last decade have led to a flurry of\nresearch in the application of deep artificial neural networks to robotic\nsystems, with at least thirty papers published on the subject between 2014 and\nthe present. This review discusses the applications, benefits, and limitations\nof deep learning vis-\\`a-vis physical robotic systems, using contemporary\nresearch as exemplars. It is intended to communicate recent advances to the\nwider robotics community and inspire additional interest in and application of\ndeep learning in robotics.",
  "text": "Deep Learning in Robotics: A Review of Recent Research\nHarry A. Pierson (corresponding author)\nDepartment of Industrial Engineering, University of Arkansas, Fayetteville, AR, USA\n4207 Bell Engineering Center\n1 University of Arkansas\nFayetteville, AR  72701\nhapierso@uark.edu\n+1 (479) 575-6034\nMichael S. Gashler\nDepartment of Computer Science and Computer Engineering, University of Arkansas, \nFayetteville, AR, USA\n504 J. B. Hunt Building\n1 University of Arkansas\nFayetteville, AR  72701\nmgashler@uark.edu\n1\nDeep Learning in Robotics: A Review of Recent Research\nAdvances in deep learning over the last decade have led to a flurry of research in \nthe application of deep artificial neural networks to robotic systems, with at least \nthirty papers published on the subject between 2014 and the present.  This review\ndiscusses the applications, benefits, and limitations of deep learning vis-à-vis \nphysical robotic systems, using contemporary research as exemplars.  It is \nintended to communicate recent advances to the wider robotics community and \ninspire additional interest in and application of deep learning in robotics.\nKeywords: deep neural networks; artificial intelligence; human-robot interaction \n1.  Introduction\nDeep learning is the science of training large artificial neural networks.  Deep neural \nnetworks (DNNs) can have hundreds of millions of parameters [1, 2], allowing them to \nmodel complex functions such as nonlinear dynamics.  They form compact \nrepresentations of state from raw, high-dimensional, multimodal sensor data commonly \nfound in robotic systems [3], and unlike many machine learning methods, they do not \nrequire a human expert to hand-engineer feature vectors from sensor data at design time.\nDNNs can, however, present particular challenges in physical robotic systems, where \ngenerating training data is generally expensive, and sub-optimal performance in training\nposes a danger in some applications.  Yet, despite such challenges, roboticists are \nfinding creative alternatives, such as leveraging training data via digital manipulation, \nautomating training, and employing multiple DNNs to improve performance and reduce\ntraining time.\nApplying deep learning to robotics is an active research area, with at least thirty \npapers published on the subject from 2014 through the time of this writing.  This review\npresents a summary of this recent research with particular emphasis on the benefits and \nchallenges vis-à-vis robotics.  A primer on deep learning is followed by a discussion of \n2\nhow common DNN structures are used in robotics and in examples from the recent \nliterature.  Practical considerations for roboticists wishing to use DNNs are also \nprovided.  Finally, limitations of and strategies that mitigate these as well as future \ntrends are discussed.\n2.  Deep learning\n2.1 A brief history of deep learning\nThe basic principles of linear regression were used by Gauss and Legendre [4], and \nmany of those same principles still cover what researchers in deep learning study.  \nHowever, several important advances have slowly transformed regression into what we \nnow call deep learning.  First, the addition of an activation function enabled regression \nmethods to fit to nonlinear functions.  It also introduced some biological similarity with \nbrain cells [5].\nNext, nonlinear models were stacked in “layers” to create powerful models, \ncalled multi-layer perceptrons.  In the 1960s a few researchers independently figured \nout how to differentiate multi-layer perceptrons [6], and by the 1980s, it evolved into a \npopular method for training them, called backpropagation [7, 8].  It was soon proven \nthat multi-layer perceptrons were universal function approximators [9], meaning they \ncould fit to any data, no matter how complex, with arbitrary precision, using a finite \nnumber of regression units.  In many ways, backpropagation marked the beginning of \nthe deep learning revolution; however, researchers still mostly limited their neural \nnetworks to a few layers because of the problem of vanishing gradients [10, 11].  \nDeeper neural networks took exponentially longer to train.\nNeural networks were successfully applied for robotics control as early as the \n1980s [12].  It was quickly recognized that nonlinear regression provided the \n3\nfunctionality that was needed for operating dynamical systems in continuous spaces [13,\n14], and closely related fuzzy systems seemed well suited for nominal logical control \ndecisions [15].  Even as early as 1989, Pomerleau’s ALVINN [16] famously \ndemonstrated that neural networks were effective for helping vehicles to stay in their \nlanes.  However, neural networks were still generally too slow to digest entire images, \nor perform the complex tasks necessary for many robotics applications.\nIn the 2000s, researchers began using graphical processing units (GPUs) to \nparallelize implementations of artificial neural networks [17].  The largest bottleneck in \ntraining neural networks is a matrix-vector multiplication step, which can be \nparallelized using GPUs.  In 2006, Hinton presented a training method that he \ndemonstrated to be effective with a many-layered neural network [18].  The near-\nsimultaneous emergence of these technologies triggered the flurry of research interest \nthat is now propelling deep learning forward at an unprecedented rate [19].\nAs hardware improved, and as neural networks began to become more practical, \nthey were increasingly found to be effective with real robotics applications.  In 2004 \nRNNPB showed that neural networks could self-organize high-level control schema that\ngeneralized effectively with several robotics test problems [20].  In 2008, \nneuroscientists made advances in recognizing how animals achieved locomotion, and \nwere able to extend this knowledge all the way to neural networks for experimental \ncontrol of robots [21].  In 2011, TNLDR demonstrated that deep neural nets could \neffectively model both state and dynamics from strictly unsupervised training with raw \nimages of a simulated robot [22].  Another relevant work is Pomerleau’s 2012 book \nsurveying applications for neural networks in perception for robot guidance [23].\nIn hindsight, we see that chess was considered in the early years of artificial \nintelligence to be representative of human intelligence over machines [24].  After \n4\nmachines beat world-class chess players [25], a new emblematic task was needed to \nrepresent the superior capabilities of human intelligence.  Visual recognition was largely\naccepted to be something easy for humans but difficult for machines [26].  But now, \nwith the emergence of deep learning, humans will not be able to claim that as an \nadvantage for much longer.  Deep learning has surged ahead of well-established image \nrecognition techniques [27] and has begun to dominate the benchmarks in handwriting \nrecognition [28], video recognition [29], small-image identification [30], detection in \nbiomedical imaging [31-33], and many others.  It has even achieved super-human \naccuracy in several image recognition contests [27, 34, 35].  Perhaps agility or dexterity \nwill be a forthcoming achievement where machines will begin to demonstrate human-\nlike proficiency.  If so, it appears that deep neural networks may be the learning model \nthat enables it.\n2.2 Common DNN structures\nThe idea of using machine learning in controlling robots requires humans to be willing \nto relinquish a degree of control.  This can seem counterintuitive at first, but the benefit \nfor doing so is that the system can then begin to learn on its own.  This makes the \nsystem capable of adapting, and therefore has potential to ultimately make better use of \nthe direction that comes from humans.\nDNNs are well suited for use with robots because they are flexible, and can be \nused in structures that other machine learning models cannot support.  Figure 1 \ndiagrams four common structures for using DNNs with robots.  \nStructure A (in Figure 1) shows a DNN for regressing arbitrary functions.  It is \ntypically trained by presenting a large collection of example training pairs:\n{ <x1, y1>,<x2, y2>, … , <xn,yn> }.  An optimization method is applied to minimize the\n5\nprediction loss.  For regression problems, loss is typically measured with sum-squared \nerror,  \n, and for classification problems it is often measured with cross-\nentropy,  \n, particularly when a softmax layer is used for the output layer \nof the neural network [36].  Traditionally, the most popular optimization method for \nneural networks is stochastic gradient descent [37], but improved methods such as \nRMSProp [38] and Adam [39] have recently garnered widespread usage.  Some other \nconsiderations for training them effectively are given in Section 2.4.  After training is \nfinished, novel vectors may be fed in as x to compute corresponding predictions for y.\nStructure B is called an autoencoder [40].  It is one common model for \nfacilitating “unsupervised learning.”  It requires two DNNs, called an “encoder” and a \n“decoder.”  In this configuration, only x needs to be supplied by the user.  s is a \n“latent” or internal encoding that the DNN generates.  For example, x might represent \nimages observed by a robot’s camera, containing thousands or even millions of values.  \nThe encoder might use convolutional layers, which are known to be effective for \ndigesting images [35, 41, 42].  s might be a small vector, perhaps only tens of values.  \nBy learning to reduce x to s, the autoencoder essentially creates its own internal \nencoding of “state.”  It will not necessarily use an encoding that has meaning for \nhumans, but it will be sufficient for the DNN to approximately reconstruct x.  How are \nautoencoders useful in robotics?  Sometimes, the robot designer may not know exactly \nwhat values are needed by the robot.  Autoencoders enable the system to figure that out \nautonomously.  This becomes especially useful when a hybrid of supervised and \nunsupervised learning is used.  For example, the user can impose certain values in s \n6\n(perhaps, positional coordinates or joint angles) and the DNNs will learn to work with \nthose values, using the other free elements in s for its own encoding purposes.  \nAutoencoders may also be used to initialize some parts of Structure C [22].  Generative \nmodels are closely related to autoencoders.  They utilize just the decoder portion of the \nmodel to predict observations from an internal representation of state.\nStructure C is a type of “recurrent neural network,” which is designed to model \ndynamic systems, including robots.  It is often trained with an approach called \n“backpropagation through time” [43, 44].  Many advances, such as “long short-term \nmemory units,” have made recurrent neural networks much stronger [27, 45].  In this \nconfiguration, u represents a control signal.  u may also contain recent observations.  s\nis an internal representation of future state, and x is a vector of anticipated future \nobservations.  The transition function approximates how the control signal will affect \nstate over time.  Just as with autoencoders, the representation of state can be entirely \nlatent, or partially imposed by the user.  (If it were entirely imposed, the model would \nbe prevented from learning.)  If x includes an estimate of the utility of state s, then this \nconfiguration is used in “model-based reinforcement learning” [46].\nStructure D learns a control policy.  It can facilitate “model-free” reinforcement \nlearning.  It uses a DNN to evaluate the utility or quality, q, of potential control vectors.\ns is a representation of state, and u is a control vector.  (Gradient methods can find the \nvalues for u that maximize q.  In cases with discrete control vectors, u may be omitted \nfrom the input-end and q augmented to contain an evaluation of each control vector.)  \nConfigurations like this are used when an objective task is known for the robot to \nperform, but the user does not know exactly how to achieve it.  By rewarding the robot \nfor accomplishing the task, it can be trained to learn how to prioritize its own choices \n7\nfor actions.  As one prominent example, reinforcement learning was used to teach a \nmachine to play a wide range of Atari video games [47].\nFigure 1.  Diagram of some common structures for using neural networks with robots. \nA: Function approximating models are trained to approximate the mappings represented\nin a training set of pair-wise examples. B: Autoencoders can reduce complex or high-\ndimensional observations to a simple feature representation, often extracting intrinsic \ninformation from images. C: Recurrent models specialize in dynamics and temporal \npredictions. D: Policy models trained with reinforcement learning seek to plan the best \ndecisions to make under possible future conditions.\n2.3 Convolutional layers\nEach of the various types of deep learning models are made by stacking multiple layers \nof regression models.  Within these models, different types of layers have evolved for \nvarious purposes.  One type of layer that warrants particular mention is convolutional \nlayers [48].  Unlike traditional fully connected layers, convolutional layers use the same\nweights to operate all across the input space.  This significantly reduces the total \nnumber of weights in the neural network, which is especially important with images that\ntypically have hundreds of thousands to millions of pixels that must be processed.  \n8\nProcessing such images with fully connected layers would require more than (100K)2 to \n(1M)2 weights connecting each layer, which would be completely impractical.  \nConvolutional layers were inspired by cortical neurons in the visual cortex, which \nrespond only to stimuli with a receptive field.  Since convolution approximates this \nbehavior, convolutional layers may be expected to excel at image processing tasks.\nThe pioneering works in neural networks with convolutional layers (CNNs) \napplied them to the task of image recognition [48, 49].  Many subsequent efforts built \non these, but widespread interest in convolutional layers surged around 2012, when \nKrizhevsky used them to dominate in the ImageNet image recognition competition [41],\nand they were able to achieve super-human recognition on other notable image \nrecognition benchmarks and competitions [27, 34, 35].  A flurry of research quickly \nfollowed seeking to establish deeper models with improved image processing \ncapabilities [50, 51].\nNow, CNNs have become well established as a highly effective deep learning \nmodel for a diversity of image-based applications.  These applications include semantic \nimage segmentation [52], object localization within images [53], scaling up images with\nsuper resolution [54], facial recognition [55, 56], scene recognition [57], and human \ngesture recognition [58].  Images are not the only type of signal for which CNNs excel.  \nTheir capabilities are also effective with any type of signal that exhibits spatio-temporal \nproximity, such as speech recognition [59], and speech and audio synthesis [60].  \nNaturally, they have also started to dominate in signal processing domains used heavily \nin robotics, such as pedestrian detection using LIDAR [61] and micro-Doppler \nsignatures [62], and depth-map estimation [63].  Recent works are even starting to \ncombine signals from multiple modalities and combine them together for unified \nrecognition and understanding [64].\n9\n2.4 High level trajectory of deep learning with robotics\nUltimately, the underlying philosophy that prevails in the deep learning community is \nthat every part of a complex system can be made to “learn.”  Thus, the real power of \ndeep learning does not come from using just one of the structures described in the \nprevious section as a component in a robotics system, but in connecting parts of all of \nthese structures together to form a full system that learns throughout.  This is where the \n“deep” in deep learning begins to make its impact – when each part of a system is \ncapable of learning, the system as a whole can adapt in sophisticated ways.\nNeuroscientists are even starting to recognize that many of the patterns evolving \nwithin the deep learning community and throughout artificial intelligence are starting to \nmirror some of those that have previously evolved in the brain [65, 66].  Doya identified\nthat supervised learning methods (Structures A and C) mirror the function of the \ncerebellum, unsupervised methods (Structure B) learn in a manner comparable to that of\nthe cerebral cortex, and reinforcement learning is analogous with the basal ganglia [67]. \nThus, the current trajectory of advancement strongly suggests that control of robots is \nleading toward full cognitive architectures that divide coordination tasks in a manner \nincreasingly analogous with the brain [68-70].\n3.  Deep learning in robotics\nThe robotics community has identified numerous goal for robotics in the next 5 to 20 \nyears.  These include, but certainly are not limited to, human-like walking and running, \nteaching by demonstration, mobile navigation in pedestrian environments, collaborative \nautomation, automated bin/shelf picking, automated combat recovery, and automated \naircraft inspection and maintenance, and robotic disaster mitigation and recovery [71-\n75].  This paper identifies seven general challenges for robotics that are critical for \n10\nreaching these goals and for which DNN technology has high potential for impact:  \n \nChallenge 1:  Learning complex, high-dimensional, and novel dynamics.  Analytic \nderivation of complex dynamics requires human experts, is time consuming, and poses a\ntrade-off between state dimensionality and tractability.  Making such models robust to \nuncertainty is difficult, and full state information is often unknown.  Systems that can \nquickly and autonomously adapt to novel dynamics are needed to solve problems such \nas grasping new objects, traveling over surfaces with unknown or uncertain properties, \nmanaging interactions between a new tool and/or environment, or adapting to \ndegradation and/or failure of robot subsystems.  Also needed are methods to accomplish\nthis for systems that possess hundreds (or even thousands) of degrees of freedom, \nexhibit high levels of uncertainty, and for which only partial state information is \navailable.\nChallenge 2:  Learning control policies in dynamic environments.  As with dynamics, \ncontrol systems that accommodate high degrees of freedom for applications such as \nmulti-arm mobile manipulators, anthropomorphic hands, and swarm robotics are \nneeded.  Such systems will be called upon to function reliably and safely in \nenvironments with high uncertainty and limited state information.\nChallenge 3:  Advanced manipulation.  Despite advances achieved over 3 decades of \nactive research, robust and general solutions for tasks such as grasping deformable \nand/or complex geometries, using tools, and actuating systems in the environment (turn \na valve handle, open a door, and so forth) remain elusive – especially in novel \nsituations.  This challenge includes kinematic, kinetic, and grasp planning inherent in \ntasks such as these.\n11\nChallenge 4:  Advanced object recognition.  DNNs have already proven to be highly \nadept at recognizing and classifying objects [27, 34, 35].  Advanced application \nexamples include recognizing deformable objects and estimating their state and pose for\ngrasping, semantic task and path specification (e.g., go around the table, to the car, and \nopen the trunk), and recognizing the properties of objects and surfaces such as sharp \nobjects that could pose a danger to human collaborators or wet/slippery floors.\nChallenge 5:  Interpreting and anticipating human actions.  This challenge is critical if \nrobots are to work with or amongst people in applications such as collaborative robotics\nfor manufacturing, eldercare, autonomous vehicles operating on public thoroughfares, \nor navigating pedestrian environments.  It will enable teaching by demonstration, which \nwill in turn facilitate task specification by individuals without expertise in robotics or \nprogramming.  This challenge may also be extended to perceiving human needs and \nanticipating when robotic intervention is appropriate.\nChallenge 6:  Sensor fusion & dimensionality reduction.  The proliferation of low-cost \nsensing technologies has been a boon for robotics, providing a plethora of potentially \nrich, high-dimensional, and multimodal data.  This challenge refers to methods for \nconstructing meaningful and useful representations of state from such data.\nChallenge 7:  High-level task planning.  Robots will need to reliably execute high-level \ncommands that fuse the previous six challenges to achieve a new level of utility, \nespecially if they are to benefit the general public.  For example, the command “get the \nmilk” must autonomously generate the lower-level tasks of navigating to/from the \nrefrigerator, opening/closing the door, identifying the proper container (milk containers \nmay take many forms), and securely grasping the container.\nLoosely speaking, these challenges form a sort of “basis set” for the goals \nmentioned above.  For example, human-like walking and running will rely heavily on \n12\nChallenges 1 (learning dynamics) and 2 (learning control policies), while teaching by \ndemonstration will require advances in Challenges 4 (object recognition), 5 (interpreting\nhuman action), and 6 (sensor fusion).\nTable 1 categorizes recent robotics research that utilizes DNN technology \naccording to these challenges, as well as the DNN structures discussed in the previous \nsection.  From this several observations are made: First is that Structure A is clearly the \nmost popular DNN architecture in the recent robotics literature.  This is likely explained\nby its intuitive nature, essentially learning to approximate the same function presented \nto it in the form of training samples.  It also requires the least amount of domain \nknowledge in DNNs to implement.  Robotics challenges, however, are not limited to the\nsort of classification and/or regression problems to which this structure is best suited.  \nAdditional focus on applying Structures B, C, and D to robotics problems may very \nwell catalyse significant advancement in many of the identified challenges.  One of the \npurposes of this paper is to emphasize the potential of the other structures to the robotics\ncommunity.  \nSomewhat related is the fact that some cells in Table 1 are empty.  In the \nauthors’ opinion, this is due to a lack of research focus rather than any inherent \nincompatibilities between challenges and structures.  In particular, the ability of \nStructure B to learn compact representations of state would be particularly useful for \nestimating the pose, state, and properties of objects (Challenge 4) and the state of human\ncollaborators (Challenge 5).\nTable 1.  An overview of how DNN structures are used in the recent literature to address\nthe seven challenges.\nDNN Structure\nA\nB\nC\nD\nChallenge 1\n  (Dynamics)\n[76, 78, 81, 85,\n115, 127]\n [87, 112, 113,\n115\n[115, 122]\n[125, 126]\nChallenge 2\n[85,115,]\n[112]\n[122]\n[125,128]\n13\n  (Control)\nChallenge 3\n  (Manipulation)\n[79, 82-85, 123]\n[112]\n[123]\n[128]\nChallenge 4\n  (Object rec.)\n[79-81, 88, 123]\n[123]\n[128]\nChallenge 5\n  (Human actions)\n[77, 79, 123, 127]\n[123]\nChallenge 6\n  (Sensor fusion)\n[77, 81, 83, 84,\n86, 88]\n[87, 116, 117]\n[114]\n[116, 117]\nChallenge 7\n  (high-level planning)\n[128]\nTable 1 also indicates limited application of DNNs to high-level task planning \n(Challenge 7).  One of the barriers to the application of DNNs is quantifying the quality \nof such decisions.  Standard benchmarks for decision quality are needed.  Once this is \naddressed, DNNs may very well be able to be the tool that allows roboticists to make \nprogress on this very significant challenge.\nThe balance of this section is categorized by DNN structure and is organized as \nfollows: 1) a discussion of the structure’s role in robotics, 2) examples from the recent \nliterature of how the structure is being applied in robotics, and 3) practical \nrecommendations for applying the structure in robotics.\n3.1 Classifiers and discriminative models (Structure A) in robotics\n3.1.1 The role of Structure A in robotics\nStructure A involves using a deep learning model to approximate a function from \nsample input-output pairs.  This may be the most general-purpose deep learning \nstructure, since there are many different functions in robotics that researchers and \npractitioners may want to approximate from sample observations.  Some examples \ninclude mapping from actions to corresponding changes in state, mapping from changes\nin state to the actions that would cause it, or mapping from forces to motions.  Whereas \nin some cases physical equations for these functions may already be known, there are \nmany other cases where the environment is just too complex for these equations to yield\n14\nacceptable accuracy.  In such situations, learning to approximate the function from \nsample observations may yield significantly better accuracy.\nThe functions that are approximated need not be continuous.  Function \napproximating models also excel at classification tasks, such as determining what type \nof object lies before the robot, which grasping approach or general planning strategy is \nbest suited for current conditions, or what is the state of a certain complex object with \nwhich the robot is interacting.\nThe next section reviews some of the many applications for classifiers, \nregression models, and discriminative models that have appeared in the recent literature \nwith robotics.\n3.1.2 Examples in recent research\nPunjani and Abbeel [76] used a function approximating deep learning architecture with \nrectifiers to model the highly coupled dynamics of a radio-controlled helicopter, which \nis a challenging analytic derivation and difficult system identification problem.  \nTraining data was obtained as a human expert flew the helicopter through various \naerobatic maneuvers, and the DNN outperformed three state-of-the-art methods for \nobtaining helicopter dynamics by about 60%.\nNeverova et al. [77] modeled how the time between a driver’s head movement \nand the occurrence of a maneuver varies with vehicle speed.  The resulting system made\npredictions every 0.8 seconds based on the preceding 5 seconds of data and anticipated \nmaneuvers about 3.5 seconds before they occurred, with 90.5% accuracy.\nA great many works have used function approximating models in the domains of\n(1) detection and perception, (2) grasping and object manipulation, and (3) scene \n15\nunderstanding and sensor fusion.  The following three subsections describe recent works\nin each of these domains.\nDetection and Perception.  DNNs have surged ahead of other models in the \ndomains of detection and perception.  They are especially attractive models because \nthey are capable of operating directly on high-dimensional input data instead of \nrequiring feature vectors that are hand-engineered at design time by experts in machine \nlearning and the particular application [1].  This reduces dependence on human experts, \nand the additional training time may be partially offset by reducing initial engineering \neffort.\nMariolis, Peleka, and Kargakos [78] studied object and pose recognition for \ngarments hanging from a single point, as if picked by a robotic gripper.  Training \noccurred on pants, shirts, and towels with various size, shape, and material properties, \nboth flat and hanging from various grasp points.  On a test set of six objects different \nfrom those used in training, the authors achieved 100% recognition and were able to \npredict grasp point on the garment with a mean error of 5.3 cm.  These results were \nmore accurate and faster than support vector machines.  Yang, Li, and Fermüller [79] \ntrained a DNN to recognize 48 common kitchen objects and classify human grasps on \nthem from 88 YouTube cooking videos.  Notably, the videos were not created with \ntraining robots in mind, exhibiting significant variation in background and scenery.  \nPower and precision grasps, and subclasses of each were classified.  The system \nachieved 79% object recognition accuracy and 91% grasp classification accuracy.  Chen\net al. [80] identified the existence and pose of doors with a convolutional neural \nnetwork and passed this information to a navigation algorithm for a mobile robot.  They \nsuggest that navigating by such visual information can be superior to map-building \nmethods in dynamic environments.  Gao, Hendricks, Kuchenbecker, and Darrell [81] \n16\nintegrated both vison- and contact-based perception to classify objects with haptic \nadjectives (smooth, compliant, etc.).  If a robot could predict such information before or \nquickly after making contact with the objects, it can take appropriate actions such as \nadjusting its grip on fragile objects or avoiding slippery surfaces.  As with the other \nresearch studies in this paper, their method did not require manual design of feature \nvectors from domain-specific knowledge.\nGrasping and object manipulation.  Yu, Weng, Liang, and Xie [82] used a deep \nconvolutional neural network to recognize five known objects resting on a flat surface \nand categorize their orientation into discretized categories.  The study focused on \nrecognition and pose estimation, so grasp planning was limited to positioning a parallel \ngripper at the object’s center and aligned with the estimated angle.  Grasping success \nrates exceeded 90%.  Lenz, Lee, and Saxena [83] used deep learning to detect optimal \ngrasps for objects from RGB-D (color + depth) images.  The network evaluates \nnumerous potential grasps for the object and identifies the one with the highest potential\nfor success without any prior knowledge of object geometry.  Trials on Baxter and PR2 \nrobots resulted in successful grasps 84% and 89% of the time, respectively, compared to\n31% for a state-of-the art reference algorithm.  Rather than evaluating a set of potential \ngrasps, Redmon and Angelova [84] trained a convolutional neural network to detect an \nacceptable grasp directly from RGB-D data in one pass.  They achieved 88% accuracy \nand claim real-time performance, arriving at a solution in under 100 milliseconds.\nLevine, Pastor, Krizhevsky, and Quillen [85] trained a convolutional neural \nnetwork to evaluate the potential of a particular robot motion for successfully grasping \ncommon office objects from image data, and used a second network to provide \ncontinuous feedback through the grasping process.  Inspired by hand-eye coordination \nin humans, the system was robust to object movement and uncertainty in gripper \n17\nmechanics.  While this research may seem similar to visual servoing at first glance, it \ndiffers in that no hand-designed feature vectors were required for perception, and \ntransfer functions for closed-loop control were not modeled analytically.\nScene understanding and sensor fusion.  Extracting meaning from video or still \nscenes is another application where deep learning has made impressive progress.  \nNeverova, Wolf, Taylor, and Nebout [77] report on their first-place winning entry in the \n2014 ChaLearn Looking at People Challenge (http://gesture.chalearn.org), which \nchallenges entrants to recognize 20 different Italian conversational gestures from 13,858\nseparate RGB-D videos of different people performing those gestures.  Ouyang and \nWang [86] simultaneously addressed four independent aspects of pedestrian detection \nwith a single DNN: feature extraction, articulation and motion handling, occlusion \nhandling, and classification.  They argue that their unified system avoids suboptimal \ninteractions between these usually separate systems.  The integrated network was \ntrained on over 60,000 samples from two publically available datasets.  Compared to 18 \nother approaches in the published literature, the authors’ system outperforms on both \ndata sets by as much as 9%.  Wu, Yildirim, Lim, Freeman, and Tenenbaum [87] \nattempted to predict the physical outcome of dynamic scenes by vision alone, based on \nthe premise that humans can often predict the outcome of a dynamic scene from visual \ninformation – for example, a block sliding down a ramp and impacting another block.  \nThey use both simulations from a physics engine and physical trials for training.\nDeep learning has also been found to be effective at handling multimodal data \ngenerated in robotic sensing applications.  Previously mentioned examples include \nintegrating vision and haptic sensor data [81] and incorporating both depth data and \nimage information from RGB-D camera data [77, 83].  Additionally, Schmitz et al. [88] \nstudied tactile object recognition with a TWENDY-ONE multi-finger hand, which \n18\nprovides a multimodal set of 312 values from distributed skin sensors, fingertip forces, \njoint torques, actuator currents, and joint angles.  The system was trained on a set of \ntwenty objects – some deliberately similar and some vastly different – handed to the \nrobot in various poses.  The investigators achieved an 88% recognition rate, as \ncompared to the 68% using other methods in the literature.\n3.1.3 Practical recommendations for working with Structure A\nDue to their large numbers of meta-parameters, DNNs have developed somewhat of a \nreputation for being difficult for non-experts to use effectively.  However, these \nparameters also provide significant flexibility, which is a major factor in their overall \nsuccess.  Therefore, training DNNs requires the user to develop at least a basic level of \nfamiliarity with several concepts.  This section summarizes some of the most important \nconcepts involved in training function approximating DNNs.  In particular, applying \nthese techniques will help to address Challenge 4 (advanced object recognition), and to \na lesser extent all of the other challenges as well.\nAlthough recent trends lean toward deeper and bigger models, a simple neural \nnetwork with just one hidden layer and a standard sigmoid-shaped activation function \nwill train much faster, and will provide a useful baseline to give meaning to any \nimprovements from the use of deeper models.  When deeper models are used, Leaky \nRectifiers tend to promote faster training by diminishing the effects of the vanishing \ngradient problem [41, 89], and improve accuracy through having a simpler monotonic \nderivative [91, 91].\nSince models with more weights have more flexibility to overfit to the training \ndata, regularization is important role for training the best model.  Elastic net combines \nthe well-established L1 and L2 regularization methods to promote robustness against \n19\nweight saturation and also promote sparsity in the weights [92].  Newer regularization \nmethods, including drop-out [93] and drop-connect [94] have achieved even better \nempirical results.  Several regularization methods also exist specifically to improve \nrobustness with autoencoders [95, 96].\nSpecial-purpose layers can also make a significant difference with DNNs.  It is a\ncommon practice to alternate between convolutional and max pooling layers.  The \npooling layers reduce the overall number of weights in the network and also enable the \nmodel to learn to recognize objects independent of where they occur in the visual field \n[97].  Batch normalization layers can yield significant improvements in the rate of \nconvergence by keeping the gradient in a range where it will affect the weights of all \nneurons [98].  And, residual layers can enable much deeper, and consequently more \nflexible, models to be trained [99].  \nTo make effective use of deep learning models, it is important to train on one or \nmore General Purpose Graphical Processing Units (GPGPUs) [17].  Many other ways of\nparallelizing deep neural networks have been attempted, but none of them yet yield the \nperformance gains of GPGPUs [27].  Since DNNs require the use of so many \nspecialized techniques, leveraging an existing toolkit that provides ready-made \nimplementations is an imperative.  Fortunately, the deep learning community has been \nvery helpful in releasing open source implementations of new developments, so many \nwell-refined open source deep learning toolkits are now available:\nTensorflow has recently surged in popularity [100].  Theano is a Python-based \nplatform that provides General Purpose Graphical Processing Unit (GPGPU)-\nparallelization for deep learning [101].  Several popular toolkits build on top of Theano, \nincluding as Lasagne and Pylearn2 [102].  Keras is a wrapper around Tensorflow and \nTheano that seeks to simplify the interfaces for deep learning [103].  Torch offers a \n20\nMatlab-like environment written in Lua for deep learning, with particular emphasis on \nconvolutional neural networks [104].  In C++, Caffe is one of the more popular toolkits \nfor high-performance convolutional neural networks [105].  It also provides Python \nbindings.  Other C++ toolkits with GPU support are available [106, 107].  Some other \ntoolkits with deep learning support include GroundHog, Theanets [100], Kaldi [109], \nand CURRENNT [110].  Kustikova gives a survey of many deep learning toolkits for \nimage recognition [111].  Toolkits that employ other parallelization methods, besides \nGPGPUs, include Hadoop, Mahout, Spark, DeepLearning4j, and Scala.\n3.2 Generative and Unsupervised models (Structure B) in robotics\n3.2.1 The role of Structure B in robotics\nOne of the characteristic capabilities that make humans so proficient at operating in the \nreal world is their ability to understand what they perceive.  A similar capability is \noffered in autoencoders, a type of deep learning model that both encodes observations \ninto an internal representation, then decodes it back to the original observation.  These \nmodels digest high-dimensional data and produce compact, low-dimensional internal \nrepresentations that succinctly describe the meaning in the original observations [3].  \nThus, auto-encoders are used primarily in cases where high-dimensional observations \nare available, but the user wants a low-dimensional representation of state.\nGenerative models are closely related.  They utilize only the decoding portion of\nan autoencoder, and are useful for predicting observations.  Inference methods may be \nused with generative models to estimate internal representations of state without \nrequiring an encoder to be trained at all.  In many ways, generative models may be \nconsidered to be the opposite of classifiers, or discriminative models, because they map \n21\nfrom a succinct representation to a full high-dimensional set of values similar to those \nthat might typically be observed.\n3.2.2 Examples in recent research\nFinn et al. [112] trained a deep spatial auto-encoder on visual features to extract \nmeaning from the observations and ultimately to achieve visuomotor control.  The \nautoencoder learned how robot actions affected the configuration of objects in the work \nenvelope, and this model was used in a closed-loop controller.  Tasks included pushing a\nblock, spooning material into a bowl, scooping with a spatula, and hanging a loop of \nrope on a hook.\nWu et al. [87] used a generative model to anticipate outcomes from physics \nsimulations.  Watter, Springenberg, Bodecker, and Reidmiller [113] also applied a \ngenerative model to model the nonlinear dynamics of simple physical systems and \ncontrol them.  Noda, Arie, Suga, and Ogata [114] developed a novel deep learning \nsolution involving both unsupervised methods and recurrent models for integrating \nmulti-modal sensorimotor data, including RGB images, sound data, and joint angles.\nAnother example of Structure B was demonstrated by Polydoros, Nalpantidis, \nand Kruger in modeling the inverse dynamics of a manipulator [115].  The network was \ntrained using state variables recorded while operating under standard closed-loop \ncontrol.  A “fading memory” feature allowed the DNN to adapt as dynamics changed \nwith payload and mechanical wear.  Analytic dynamic models have difficulty coping \nwith such changes, and are difficult to derive for highly compliant serial-elastic \nmanipulators such as the new class of collaborative robots.  The authors report that their\nsystem outperforms the state-of-the-art in real-time learning evaluations and converges \nquickly, even with noisy sensor data.\n22\nGünther, Pilarski, Helfrich, Shen, and Diepold [116, 117] designed a DNN to \nautomatically create meaningful feature vectors.  The network was able to extract low-\ndimensional features from high-dimensional camera images of welds in a laser welding \nprocess.  These features were subsequently used with other machine learning and \ncontrol strategies to close the loop on the welding process.\n3.2.3 Practical recommendations for working with Structure B\nAutoencoders and other unsupervised DNN techniques are particularly well suited for \naddressing challenges pertaining to high-dimensional observations (1 and 6).  They both\nreduce dimensionality and extract meaningful representations of state, which is the first \nstep in effective sensor fusion.\nConvolutional layers are well known to be effective for digesting images.  Since \nimages are common with robots, autoencoders that use convolutional layers in their \nencoding portion tend to be especially effective for estimating state from images [118].  \nFor the decoding portion of the autoencoder, convolutional layers offer little advantage. \nA somewhat less-known technique involves training the decoder to predict only a single\npixel and parameterizing the decoder to enable the user to specify which pixel it should \npredict [119].  This approach has many analogies with convolution, and experimentally \nseems to lead to much faster training times.\nRegularization is particularly important for achieving good results with \nautoencoders, and specialized regularization methods have been designed particularly \nfor autoencoders [120].  However, some experiments have shown that instead of heavily\nregularizing the encoder, it may even work better to entirely omit the encoder, and just \nuse a standalone decoder [119].  In this configuration, the internal representation of state\nis inferred in a latent manner by using gradient descent until the internal representation \n23\nconverges with the decoder.  Nonlinear dimensionality reduction methods have also \nbeen shown to be effective for pretraining such latent representations [119].\n3.3 Recurrent models (Structure C) in robotics\n3.3.1 The role of Structure C in robotics\nRecurrent models excel at learning to anticipate complex dynamics.  The recurrent \nconnections in such models give them a form of “memory” that they can use to \nremember the current state.  This knowledge of state enables them to model the effects \nof time in a changing environment.\n3.3.2 Examples in recent research\nJain et al. [121] trained a recurrent architecture to predict traffic maneuvers in a human-\ndriven automobile in an effort to improve current collision avoidance systems which \noften do not intervene in time to avoid an accident.  Multimodal data inputs included \nvideo of the driver, video of the road in front of the car, dynamic state of the vehicle, \nGPS coordinates, and street maps of the area around the car.  \nSeveral researchers have used recurrent networks to deduce system dynamics \ndirectly from full observations.  Lenz, Knepper, and Saxena [122] modeled robotic food\ncutting with a knife.  This includes difficult-to-model effects such as friction, \ndeformation, and hysteresis.  Food-knife surface contact changes through the cut, and so\ndo the material properties of the food, as when passing between the peel and the center \nof a fruit.  Data obtained while operating under fixed-trajectory stiffness control was \nused to train the DNN on the system dynamics, and the resulting model was used to \nimplement a model predictive control algorithm (a variation of Structure D).  Their \nsystem outperformed fixed-trajectory stiffness control, increasing mean cutting rate \nfrom 1.5 cm/s to 5.1 cm/s.\n24\nHwang et al. [123] demonstrated gesture recognition with a recurrent model, and\ncoordinated it with attention switching, object perception, and grasping.  The robot \nfocused on a human collaborator, who gestured to one of two objects.  The robot then \nswitched its focus to the indicated object, recognized the object, and found an \nacceptable grasp.  Their system achieved a successful grasp 85% of the time when \nsimulated on an iCub humanoid robot.\n3.3.3 Practical recommendations for working with Structure C\nRecurrent models are well suited for addressing challenges pertaining to the \ncomplexities of temporal effects (Challenges 1, 5, and 7).  This section describes some \nrecommendations for working effectively with recurrent models.\nUnfortunately, recurrent models have a somewhat negative reputation for being \ndifficult to train.  One of the biggest problems is that training them with gradient \nmethods requires unfolding through time, which effectively makes them behave as \nnetworks that are much deeper than they already are.  Given so much depth, the training\ngradients tend to become vanishingly small [10, 11].  This problem was largely solved \nby the error-carousel idea in LSTM networks [124], so it would be helpful to become \nfamiliar with that solution before attempting to work with recurrent models.\nWhen observations are very high dimensional, such as occurs when digital \nimages are used with robots, a much simpler solution becomes possible.  This solution \nis to simply infer the intrinsic state from the images.  If the state can be inferred \naccurately, then the recurrent essentially goes away, making it possible to train the \nstructure with example pairs presented in arbitrary order, just like Structure A [22].  \nEven if the internal state can only be inferred with a small degree of accuracy, this still \n25\nprovides useful pre-training, which may significantly reduce the necessary training time \nwith a recurrent model [119].\n3.4 Policy learning models (Structure D) in robotics\n3.4.1 The role of Structure D in robotics\nLearning a near optimal (or at least a reasonably acceptable) control policy is often the \nprimary objective in combining machine learning with robotics.  The canonical model \nfor using deep neural networks for learning a control policy is deep Q-learning [47].  It \nuses a DNN to model a table of Q-values, which are trained to converge to a \nrepresentation of the values for performing each possible action in any state.  Although \nStructure D is quite similar to Structure A in terms of the model itself, they are trained \nin significantly different ways.  Instead of minimizing prediction error against a training\nset of samples, deep Q-networks seek to maximize long-term reward.  This is done \nthrough seeking a balance between exploration and exploitation that ultimately leads to \nan effective policy model.\nUltimately, reinforcement learning models are useful for learning to operate \ndynamic systems from partial state information, and controllers based on deep \nreinforcement learning can be very computationally efficient at runtime [125].  They \nautomatically infer priorities based on rewards that are obtained during training.  In \ntheory, they provide a complete control policy learning system, but they do suffer from \nextremely slow training times.  Consequently, many of the works in the next section \ncombine them with other approaches in order to seek greater levels of control accuracy \nand training speed.\n26\n3.4.2 Examples in recent research\nZhang, Kahn, and Levine [125] learned a control policy to implement a model \npredictive control guided search for autonomous aerial vehicles.  Reducing \ncomputational load in mobile robotics translates into power savings that increase range \nand/or improve performance.  Without the need for full state information, fewer onboard\nsensors are required, further reducing power consumption, cost, and weight.\nDeep reinforcement learning has also been used to control dynamic systems \nfrom video, without direct access to state information.  Lillicrap, Hunt, and Pritzel [126]\ntrained a deep reinforcement learner based on pixel data over 20 simulated dynamic \nsystems and developed a motion planning system that performed as well or better than \nalgorithms that take advantage of the full state of the dynamic system.\nFinn, Levine, and Abbeel [127] used video of human experts performing various\ntasks to train a DNN to learn nonlinear cost functions (with Structure A).  Once these \ncost functions were learned, they could be used to train a reinforcement learner \n(Structure D) for motion planning.  They demonstrated the ability to complete tasks that\ninvolved complex 2nd-order dynamics and hard-to-model interactions between a \nmanipulator and various objects, including 2D navigation, reaching, peg insertion, \nplacing a dish, and pouring.  \nVisuomotor control requires an even closer integration between object \nperception and grasping, mapping image data directly to actuator control signals.  \nLevine, Finn, Darrell, and Abbeel [128] used reinforcement learning (Structure D) to \nshow that this can be superior to separate systems for perception and control.  Test \napplications included shape sorting, screwing a cap onto a bottle, fitting a hammer claw \nto a nail, and placing a coat hanger on a rack.  The resulting system could perform the \ntasks reliably, even with moderate visual distractors.\n27\nAs mentioned earlier, Günther, Pilarski, Helfrich, Shen, and Diepold [116, 117] \ncombined autoencoders with reinforcement learning models to control a laser welding \nsystem from camera images.\n3.4.3 Practical recommendations for working with Structure D\nPolicy learning models are ultimately the solution to addressing Challenges 2 (learning \ncontrol policies in dynamic environments) and 7 (high-level task planning).  Perhaps, \nthe biggest difficulty when working with reinforcement learning models, however, is the\nhuge amount of computation time necessary to train them.  Although such models are \nhighly efficient after training, they tend to require significantly more training pattern \npresentations before they converge to represent reliable control policies.  Taking care to \nfind an efficient GPU-optimized implementation, therefore, can make a big difference.  \nAnother important technique is to train in simulation before attempting to train with an \nactual robot.  This reduces wear on physical equipment, as well reduces training time.  \nEven if only a crude simulation is available, a model that has been pre-trained on a \nsimilar challenge will converge much more quickly to fit the real challenge than one \nthat was trained from scratch.\nSince robots often operate in a space with continuous actions, traditional Q-\nlearning is not directly applicable.  Actor-critic models, however, address this problem \nnicely.  They regress actions in conjunction with the continuous Q-table, and lead to a \nfinal model that directly computes the best action given the current observation, which \nis well suited for robotics applications [129].\nAnother important consideration is the exploration policy.  The traditional \nepsilon-greedy exploration policy leads to much higher computational training \n28\nrequirements than modern approaches [130, 131].  It is, therefore, advantageous to train \nStructure D in an approach that intelligently explores novel states.\n4.  Current Shortcomings of DNNs for Robotics\nFor all of its benefits, deep learning does pose some drawbacks.  Perhaps most \nsignificant is the volume of training data required, which is particularly problematic in \nrobotics because generating training data on physical systems can be expensive and time\nconsuming.  For instance, Levine et al. [85] used 14 robots to collect over 800,000 grasp\nattempts over a period of 2 months.  Jain et al. [121] trained their traffic maneuver \nprediction system on 1180 miles of high- and low-speed driving with 10 different \ndrivers.  Punjani and Abbeel [76] required repeated demonstrations of helicopter \naerobatic maneuvers by a human expert.  Neverova et al. [77] had access to over 13,000\nvideos of conversations, and Ouyang and Wang [86] had access to 60,000 samples for \npedestrian detection.  Pinto and Gupta [132] needed 700 hours of robot time to generate \na data set of 50,000 grasps for the training of a convolutional neural network.\nDespite this, the literature does contain clever approaches to mitigating this \ndisadvantage.  One approach entails using simulation to generate virtual training data.  \nFor example, Mariolis et al. 78] pre-trained their garment pose recognition networks on \na large synthetic data set created in simulation using 3D graphics software.  Kappler, \nBohg, and Schaal [133] generated a database of over 300,000 grasps on over 700 \nobjects in simulation, generating physics-based grasp quality metrics for each and using \nthis to classify grasp stability automatically.  They validated via human classification of \ngrasps and concluded that the computer- and human-generated labeling had good \ncorrelation.  Another strategy is leveraging training data through digital manipulation.  \nNeverova et al. [77] faced the challenge that speed of conversational gestures varies \n29\nsignificantly among different people.  They varied video playback speed to simulate this\ntemporal variance, expanding their training set without the need to acquire additional \nsamples.  Still other researchers utilizing reinforcement learning, such as Polydoros et \nal. [115] and Zhang et al. [125], automated training using alternative control systems \nduring the learning phase.\nTraining time is another challenge associated with the sheer size of DNNs.  \nTypical models involve up to millions of parameters and can take days to train on \nparallel hardware, which is practical only for frequently repeated tasks that provide \nadequate payback on training time invested.  One way to reduce training time is \ndistributing a task among multiple, smaller DNNs.  Mariolis et al. [78] trained two \nDNNs: One performed object classification, and its result was passed to a second \nnetwork for pose recognition.  This multi-step approach sped both training and \nclassification at runtime.  Lenz et al. [122] employed a two-stage network design for \ngrasp detection.  The first DNN had relatively few parameters.  Sacrificing accuracy for \nspeed, it eliminated highly unlikely grasps.  The second stage had more parameters, \nmaking it more accurate, but was relatively quick since it did not need to consider \nunlikely grasps.  They found the combination to be robust and computationally efficient.\nIt should be noted, however, that this strategy represents a tradeoff with other \nresearchers’ suggestions that integrating multiple functions within a single network \nresults in better performance [86, 128].\nThe work of Zhang et al. [125] highlights two additional challenges.  First, \nunsupervised learning is not practical for robotic systems where a single failure is \ncatastrophic, as in aerial vehicles.  Second, providing the necessary computational \nresources for deep learning in a system that is sensitive to weight, power consumption, \nand cost is often not practical.  The authors trained their aerial systems using a ground-\n30\nbased control system communicating wirelessly with the vehicle.  This made training \nsafe and automatic, and allowed them to use off-board computing resources for training.\n5.  Conclusion\nDeep learning has shown promise in significant sensing, cognition, and action \nproblems, and even the potential to combine these normally separate functions into a \nsingle system.  DNNs can operate on raw sensor data and deduce key features in that \ndata without human assistance, potentially greatly reducing up-front engineering time.  \nThey are also adept at fusing high-dimensional, multimodal data.  Improvement with \nexperience has been demonstrated, facilitating adaptation in the dynamic, unstructured \nenvironments in which robots operate.\nSome remaining barriers to the adoption of deep learning in robotics include the \nnecessity for large training data and long training times.  Generating training data on \nphysical systems can be relatively time consuming and expensive.  One promising trend\nis crowdsourcing training data via cloud robotics [134].  It is not even necessary that \nthis data be from other robots, as shown by Yang’s use of general-purpose cooking \nvideos for object and grasp recognition [79].  Regarding training time, local parallel \nprocessing [17] and increases in raw processing speed have led to significant \nimprovements.  Distributed computing offers the potential to direct more computing \nresources to a given problem [88] but can be limited by communication speeds [2].  \nThere may also be algorithmic ways of making the training process more efficient yet to\nbe discovered.  For example, deep learning researchers are actively working on \ndirecting the network’s attention to the most relevant subspaces within the data and \napplying biologically inspired, sparse DNNs with fewer synaptic connections to train \n[27].\n31\nUltimately, the trends are moving toward greater levels of cognition, and some \nresearchers even believe that deep learning may achieve human-level abilities in the \nnear future [1, 134].  However, deep learning still has many obstacles to overcome \nbefore achieving such an ambitious objective.  Currently, cognitive training datasets do \nnot even exist [134].  Although DNNs excel at 2D image recognition, they are known to\nbe highly susceptible to adversarial samples [135], and they still struggle to model 3D \nspatial layouts with object invariance [65].  Currently, DNNs appear to be powerful \ntools for practitioners in robotics, but only time will tell whether they can really deliver \nthe capabilities that are needed for dexterous adaptation in general environments.\nReferences\n[1] LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436-444.\n[2] Jordan MI, Mitchell TM. Machine learning: arends, perspectives, and prospects. \nScience. 2015;349(6245):255-260.\n[3] Böhmer W, Springenberg JT, Boedecker J, et al. Autonomous learning of state \nrepresentations for control: an emerging field aims to autonomously learn state \nrepresentations for reinforcement learning agents from their real-world sensor \nobservations. KI-Künstliche Intelligenz. 2015;29(4):353-362.\n[4] Stigler SM. Gauss and the invention of least squares. Ann of Statistics. \n1981;9(3):465-474.\n[5] Haykin S. Neural networks: a comprehensive foundation. 2nd ed. Upper Saddle \nRiver, New Jersey: Prentice Hall; 2004.\n[6] Bryson AE, Denham WF, Dreyfus SE. Optimal programming problems with \ninequality constraints. AIAA Journal. 1963;1(11):2544-2550.\n[7] Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-\npropagating errors. Nature. 1986;323:533-536.\n[8] Werbos P. Beyond regression: new tools for prediction and analysis in the behavioral\nsciences. [Ph.D. dissertation]. Dept. Statistics, Harvard Univ.; 1974.\n[9] Cybenko G. Approximation by superpositions of a sigmoidal function. Math of \nControl, Signals and Sys. 1989;2(4):303-314.\n[10] Hochreiter S. Untersuchungen zu dynamischen neuronalen netzen. [Master's \nthesis]. Institut Fur Informatik, Technische Universitat; 1991.\n32\n[11] Hochreiter S. The vanishing gradient problem during learning recurrent neural nets \nand problem solutions. Int. J. of Uncertainty, Fuzziness and Knowledge-Based \nSyst. 1998;6(2).\n[12] Miyamoto H, Kawato M, Setoyama T, & Suzuki R. Feedback-error-learning neural \nnetwork for trajectory control of a robotic manipulator. Neural Networks \n1998;1(3):251-265.\n[13] Lewis FW, Jagannathan S, & Yesildirak A. (1998). Neural network control of robot \nmanipulators and non-linear systems. CRC Press.\n[14] Miller WT, Werbos PJ, & Sutton RS. (1995). Neural networks for control. MIT \nPress.\n[15] Lin CT., & Lee CSG. Neural-network-based fuzzy logic control and decision \nsystem. IEEE Transactions on Computers. 1991;40(12):1320-1336.\n[16] Pomerleau DA. (1989). ALVINN, an autonomous land vehicle in a neural network \n(No. AIP-77). Carnegie Mellon University, Computer Science Department.\n[17] Oh K, Jung K. GPU implementation of neural networks. Pattern Recognition. \n2004;37(6):1311-1314.\n[18] Hinton GE, Osindero S, Teh Y. A fast learning algorithm for deep belief nets. \nNeural Computation. 2006;18(7):1527-1554.\n[19] Dean J, Corrado G, Monga R, et al. Large scale distributed deep networks. \nAdvances in Neural Information Process. Syst. 25; 2012.\n[20] Tani J, Ito M, & Sugita Y. Self-organization of distributedly represented multiple \nbehavior schemata in a mirror system: reviews of robot experiments using \nRNNPB. Neural Networks. 2004;17(8):1273-1289.\n[21] Ijspeert AJ. Central pattern generators for locomotion control in animals and \nrobots: a review. Neural Networks. 2008;21(4):642-653.\n[22] Gashler M, Martinez T. Temporal nonlinear dimensionality reduction. Neural \nNetworks (IJCNN), 2011 International Joint Conference on; 2011. p. 1959-1966.\n[23] Pomerleau DA (2012). Neural network perception for mobile robot guidance (Vol. \n239). Springer Science & Business Media.\n [24] Thrun S. Learning to play the game of chess. Advances in Neural Inform. Process. \nSyst.: Proc. of the 1994 Conf.\n[25] Campbell M, Hoane AJ, Hsu F. Deep blue. Artificial Intelligence. 2002;134(1):57-\n83.\n[26] Pinto N, Cox DD, DiCarlo JJ. Why is real-world visual object recognition hard? \nPLoS Computational Biology. 2008;4(1).\n[27] Schmidhuber J. Deep learning in neural networks: an overview. Neural Networks. \n2015;6(1)85-117.\n33\n[28] Graves A, Liwicki M, Fernández S, et al. A novel connectionist system for \nunconstrained handwriting recognition. Pattern Anal and Machine Intell., IEEE \ntrans. on. 2009;31(5):855-868.\n[29] Yang M, Ji S, Xu W, et al. Detecting human actions in surveillance videos. TREC \nVideo Retrieval Evaluation Workshop; 2009.\n[30] Lin M, Chen Q, Yan S. Network in network. 2013. Available: \nhttps://arxiv.org/abs/1312.4400\n[31] Ciresan D, Giusti A, Gambardella LM, et al. Deep neural networks segment \nneuronal membranes in electron microscopy images. Advances in Neural \nInformation Processing Sys 25; 2012.\n[32] Roux L, Racoceanu D, Lomenie N, et al. Mitosis detection in breast cancer \nhistological images an ICPR 2012 contest. J Pathol Inform. 2013;4(8).\n[33] Cireşan DC, Giusti A, Gambardella LM, et al. Mitosis detection in breast cancer \nhistology images with deep neural networks. In: K. Mori, I. Sakuma, Y. Sato, C. \nBarillot and N. Navab, editors. Medical Image Computing and Computer-\nAssisted Intervention–MICCAI 2013. Springer; 2013.\n[34] Cireşan D, Meier U, Masci J, et al. A committee of neural networks for traffic sign \nclassification. Neural Networks (IJCNN), 2011 Int. Joint Conf. on; 2011. p. \n1918-1921.\n[35] Ciresan D, Meier U, Schmidhuber J. Multi-column deep neural networks for image\nclassification. Computer Vision and Pattern Recognition (CVPR), 2012 IEEE \nConference on; 2012. p. 3642-3649.\n[36] Dunne RA, & Campbell NA. On the pairing of the softmax activation and cross-\nentropy penalty functions and the derivation of the softmax activation function. \nIn Proc. 8th Aust. Conf. on the Neural Networks, Melbourne 1997;181 (Vol. \n185).\n [37] Wilson DR, Martinez TR. The general inefficiency of batch training for gradient \ndescent learning. Neural Networks. 2003;16(10):1429-1451.\n[38] Tieleman T, Hinton G. (2012). Lecture 6.5-rmsprop: divide the gradient by a \nrunning average of its recent magnitude. COURSERA: Neural Networks for \nMachine Learning, 4(2).\n[39] Kingma D, Ba J. (2014). Adam: a method for stochastic optimization. arXiv \npreprint arXiv:1412.6980.\n[40] Vincent P, Larochelle H, Lajoie I, et al. Stacked denoising dutoencoders: learning \nuseful representations in a deep network with a local denoising criterion. J Mach\nLearning Research. 2010;11:3371-3408.\n34\n[41] Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep \nconvolutional neural networks. Advances in Neural Information Processing \nSystems 25; 2012.\n[42] LeCun Y, Bengio Y. Convolutional networks for images, speech, and time series. In\nM. Arbib, editor. The Handbook of Brain Theory and Neural Networks. 2nd \nedition. Cambridge, MA: MIT Press; 2003.\n[43] Werbos PJ. Backpropagation through time: what it does and how to do it. Proc. \nIEEE. 1990;78(10):1550-1560.\n[44] Sjöberg J, Zhang Q, Ljung L, et al. Nonlinear black-box modeling in system \nidentification: a unified overview. Automatica. 1995;31(12):1691-1724.\n[45] Hochreiter S, Schmidhuber J. Long short-term memory. Neural Computation. \n1997;9(8):1735-1780.\n[46] Atkeson CG, Santamaria JC. A comparison of direct and model-based \nreinforcement learning. Robotics and Automation, IEEE Int. Conf. on; \nAlbuquerque, NM. 1997. p. 3557-3564.\n[47] Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep \nreinforcement learning. Nature. 2015;518:529-533.\n[48] LeCun Y, Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, Jackel LD. \nBackpropagation applied to handwritten zip code recognition. Neural \nComputation. 1989;1(4), 541-551.\n[49] LeCun Y, Bottou L, Bengio Y, Haffner P.. Gradient-based learning applied to \ndocument recognition. Proceedings of the IEEE. 1998;86(11):2278-2324.\n[50] Simonyan K, Zisserman A. (2014). Very deep convolutional networks for large-\nscale image recognition. arXiv preprint arXiv:1409.1556.\n[51] Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D., ... Rabinovich A. \n(2015). Going deeper with convolutions. In Proc of the IEEE Conference on \nComputer Vision and Pattern Recognition (pp. 1-9).\n[52] Chen LC, Papandreou G, Kokkinos I, Murphy K, & Yuille AL. (2014). Semantic \nimage segmentation with deep convolutional nets and fully connected crfs. \narXiv preprint arXiv:1412.7062.\n[53] Sermanet P, Eigen D, Zhang X, Mathieu M, Fergus R, LeCun Y. (2013). Overfeat: \nintegrated recognition, localization and detection using convolutional networks. \narXiv preprint arXiv:1312.6229.\n[54] Dong C, Loy CC, He K, Tang X. (2014, September). Learning a deep \nconvolutional network for image super-resolution. In European Conf on \nComputer Vision (pp. 184-199). Springer International Publishing.\n35\n[55] Sun Y, Wang X, Tang X. (2013). Deep convolutional network cascade for facial \npoint detection. In Proc of the IEEE Conf on Computer Vision and Pattern \nRecognition (pp. 3476-3483).\n[56] Taigman Y, Yang M, Ranzato MA, Wolf L. (2014). Deepface: closing the gap to \nhuman-level performance in face verification. In Proc of the IEEE Conf on \nComputer Vision and Pattern Recognition (pp. 1701-1708).\n[57] Zhou B, Lapedriza A, Xiao J, Torralba A, Oliva A. (2014). Learning deep features \nfor scene recognition using places database. In Advances in Neural Information \nProcessing Sys (pp. 487-495).\n[58] Ji S, Xu W, Yang M, Yu K.. 3D convolutional neural networks for human action \nrecognition. IEEE Transactions on Pattern Analysis and Machine Intelligence. \n2013;35(1):221-231.\n[59] Graves A, Mohamed AR, Hinton G. (2013, May). Speech recognition with deep \nrecurrent neural networks. In Acoustics, Speech and Signal Proc (ICASSP), \n2013 IEEE Int Conf on (pp. 6645-6649). IEEE.\n[60] Bakshi BR, Stephanopoulos G. Wave‐net: a multiresolution, hierarchical neural \nnetwork with localized learning. AIChE Journal. 1993;39(1):57-81.\n[61] Arel I, Rose DC, Karnowski TP. Deep machine learning - a new frontier in artificial\nintelligence research [research frontier]. IEEE Computational Intelligence \nMagazine. 2010;5(4);13-18.\n[62] Kim Y, Moon T. . Human detection and activity classification based on micro-\ndoppler signatures using deep convolutional neural networks. IEEE Geoscience \nand Remote Sensing Letters. 2016;13(1):8-12.\n[63] Liu F, Shen C, Lin G, Reid I.. Learning depth from single monocular images using \ndeep convolutional neural fields. IEEE Transactions on Pattern Analysis and \nMachine Intelligence 2016;38(10):2024-2039.\n [64] Eitel A, Springenberg JT, Spinello L, Riedmiller M, Burgard W. (2015, \nSeptember). Multimodal deep learning for robust rgb-d object recognition. In \nIntelligent Robots and Sys (IROS), 2015 IEEE/RSJ International Conference on \n(pp. 681-687). IEEE.\n[65] Bohannon J. Helping robots see the big picture. Science. 2014;346(6206):186-187.\n[66] Sweller J. (2008, September). Evolutionary bases of human cognitive architecture: \nimplications for computing education. In Proc of the Fourth Int Workshop on \nComputing Education Research (pp. 1-2). ACM.\n [67] Doya K. What are the computations of the cerebellum, the basal ganglia and the \ncerebral cortex? Neural Networks, 1999;12(7):961-974.\n[68] Langley P, Laird JE, Rogers S. Cognitive architectures: research issues and \nchallenges. Cognitive Sys Research 2009;10(2):141-160.\n36\n[69] Sun R. (2006). Cognition and multi-agent interaction: from cognitive modeling to \nsocial simulation. Cambridge University Press.\n[70] Duch W, Oentaryo RJ, Pasquier M. (2008, June). Cognitive Architectures: Where \ndo we go from here? In AGI (Vol. 171, pp. 122-136).\n[71] A roadmap for US robotics: from internet to robotics, 2016 edition.  \n[72] World Technology Evaluation Center, Inc. International Assessment of Research \nand Development in Robotics. Baltimore, MD, USA; 2006.\n[73] FY2009-2034 Unmanned systems integrated roadmap. Washington, DC: \nDepartment of Defence (US); 2009.\n[74] Material Handling Institute. Material handling and logistics U.S. roadmap 2.0.  \n2017.\n[75] DARPA Robotics Challenge [Internet]. [cited 2017 May 20]. Available from:  \nhttp://www.darpa.mil/program/darpa-robotics-challenge\n[76] Punjani AP, Abbeel P. Deep learning helicopter dynamics models. Robotics and \nAutomation (ICRA), 2015 IEEE International Conference on; 2015. p. 3223-\n3230.\n[77] Neverova N, Wolf C, Taylor GW, et al. Multi-scale deep learning for gesture \ndetection and localization. Computer Vision-ECCV 2014 Workshops; 2014. p. \n474-490.\n[78] Mariolis I, Peleka G, Kargakos A, et al. Pose and category recognition of highly \ndeformable objects using deep learning. Advanced Robotics (ICAR), 2015 \nInternational Conference on; Istanbul. 2015. p. 655-662.\n[79] Yang Y, Li Y, Fermüller C, et al. Robot learning manipulation action plans by \nwatching unconstrained videos from the world wide web. 29th AAAI Conference\non Artificial Intelligence (AAAI-15); Austin, TX. 2015.\n[80] Chen W, Qu T, Zhou Y, et al. Door recognition and deep learning algorithm for \nvisual based robot navigation. Robotics and Biomimetics (ROBIO), 2014 IEEE \nInternational Conference on; Bali. 2014. p. 1793-1798.\n[81] Gao Y, Hendricks LA, Kuchenbecker KJ, et al. Deep learning for tactile \nunderstanding from visual and haptic data. 2015. Available: \nhttp://arxiv.org/abs/1511.06065\n[82] Yu J, Weng K, Liang G, et al. A vision-based robotic grasping system using deep \nlearning for 3D object recognition and pose estimation. Robotics and \nBiomimetics (ROBIO), 2013 IEEE International Conference on; Shenzhen. \n2013. p. 1175-1180.\n[83] Lenz I, Lee H, Saxena A. Deep learning for detecting robotic grasps. Int J Robotics \nres. 2015;34(4-5):705-724.\n37\n[84] Redmon J, Angelova A. Real-time grasp detection using convolutional neural \nnetworks. 2015 IEEE International Conference on Robotics and Automation \n(ICRA); Seattle, WA. 2015. p. 1316-1322.\n[85] Levine S, Pastor P, Krizhevsky A, et al. Learning hand-eye coordination for robotic\ngrasping with deep learning and large-scale data collection. 2016. Available: \nhttp://arxiv.org/abs/1603.02199\n[86] Ouyang W, Wang X. Joint deep learning for pedestrian detection. Computer Vision,\n2013 IEEE Int. Conf. on; Sidney, VIC. 2013. p. 2056-2063.\n[87] Wu J, Yildirim I, Lim JJ, et al. Galileo: Perceiving physical object properties by \nintegrating a physics engine with deep learning. Advances in Neural Information\nProcessing Systems 28; 2015.\n[88] Schmitz A, Bansho Y, Noda K, et al. Tactile object recognition using deep learning \nand dropout. Humanoid Robots, 2014 14th IEEE-RAS Int. Conf. on; 2014. p. \n1044-1050.\n[89] Zheng H, Yang Z, Liu , Liang J, Li Y. (2015, July). Improving deep neural \nnetworks using softplus units. In Neural Networks (IJCNN), 2015 Int Joint Conf\non (pp. 1-4). IEEE.\n[90] Xu B, Wang N, Chen T, Li M. (2015). Empirical evaluation of rectified activations \nin convolutional network. arXiv preprint arXiv:1505.00853.\n[91] Gashler MS, Ashmore SC. (2016). Modeling time series data with deep Fourier \nneural networks. Neurocomputing, 188, 3-11.\n[92] Zou H, Hastie T. Regularization and variable selection via the elastic net. J Royal \nStatistical Society: Series B (Statistical Methodology), 2005;67(2):301-320.\n[93] Srivastava N, Hinton GE, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a \nsimple way to prevent neural networks from overfitting. Journal of Machine \nLearning Research, 2014;15(1):1929-1958.\n[94] Wan L, Zeiler M, Zhang S, Cun YL, Fergus R. (2013). Regularization of neural \nnetworks using dropconnect. In Proc of the 30th Int Conf on Machine Learning \n(ICML-13) (pp. 1058-1066).\n[95] Rifai S, Vincent P, Muller X, Glorot X, Bengio Y. (2011). Contractive auto-\nencoders: explicit invariance during feature extraction. In Proc of the 28th Int \nConf on Machine Learning (ICML-11) (pp. 833-840).\n[96] Li Z, Fan Y, Liu W. The effect of whitening transformation on pooling operations \nin convolutional autoencoders. EURASIP J on Advances in Signal Proc, 2015;\n(1):37.\n[97] Jarrett K, Kavukcuoglu K, LeCun Y. (2009, September). What is the best multi-\nstage architecture for object recognition? In Computer Vision, 2009 IEEE 12th \nInt Conf on (pp. 2146-2153). IEEE.\n38\n[98] Ioffe S, Szegedy C. (2015). Batch normalization: accelerating deep network \ntraining by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.\n[99] He K, Zhang X, Ren S, Sun J. (2016). Deep residual learning for image \nrecognition. In Proc of the IEEE Conf on Computer Vision and Pattern \nRecognition (pp. 770-778).\n[100] Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, ... Ghemawat S. \n(2016). Tensorflow: large-scale machine learning on heterogeneous distributed \nsystems. arXiv preprint arXiv:1603.04467.\n[101] Bergstra J, Bastien F, Breuleux O, Lamblin P, Pascanu R, Delalleau O, ... Bengio \nY. (2011). Theano: deep learning on gpus with python. In NIPS 2011, \nBigLearning Workshop, Granada, Spain (Vol. 3).\n[102] Goodfellow IJ, Warde-Farley D, Lamblin P, Dumoulin V, Mirza M, Pascanu R,... \nBengio,Y. (2013). Pylearn2: a machine learning research library. arXiv preprint \narXiv:1308.4214.\n[103] Chollet F. (2015). Keras: deep learning library for theano and tensorflow. URL: \nhttps://keras. io/k.\n[104] Collobert R, Kavukcuoglu K, Farabet C. (2011). Torch7: a matlab-like \nenvironment for machine learning. In BigLearn, NIPS Workshop (No. EPFL-\nCONF-192376).\n[105] Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R, ... Darrell T. \n(2014, November). Caffe: convolutional architecture for fast feature embedding. \nIn Proc 22nd ACM Int Conf on Multimedia (pp. 675-678). ACM.\n[106] Attardi G. (2015, June). Deepnl: a deep learning nlp pipeline. In Proc of NAACL-\nHLT (pp. 109-115).\n[107] Gashler M. Waffles: a machine learning toolkit. J Machine Learning Res, \n2011;12(Jul):2383-2387.\n[108] Johnson L. Theanets documentation. Url: \nhttp://theanets.readthedocs.org/en/stable/\n[109] Povey D, Ghoshal A, Boulianne G, Burget L, Glembek O, Goel N,... Silovsky J. \n(2011). The Kaldi speech recognition toolkit. In IEEE 2011 workshop on \nautomatic speech recognition and understanding (No. EPFL-CONF-192584). \nIEEE Signal Processing Society.\n[110] Weninger F, Bergmann J, Schuller BW. Introducing CURRENNT: the munich \nopen-source CUDA recurrent neural network toolkit. J Machine Learning \nResearch, 2015;16(3):547-551.\n [111] Kustikova VD, Druzhkov PN. (2014). A survey of deep learning methods and \nsoftware for image classification and object detection. OGRW2014, 5.\n39\n[112] Finn C, Tan XY, Duan Y, et al. Deep spatial autoencoders for visuomotor learning.\n2015. Available: http://arxiv.org/abs/1509.06113\n[113] Watter M, Springenberg J, Boedecker J, et al. Embed to control: a locally linear \nlatent dynamics model for control from raw images. Advances in Neural \nInformation Proc Sys 28; 2015.\n[114] Noda K, Arie H, Suga Y, et al. Multimodal integration learning of robot behavior \nusing deep neural networks. Robotics and Autonomous Sys. 2014;62(6):721-\n736.\n[115] Polydoros AS, Nalpantidis L, Kruger V. Real-time deep learning of robotic \nmanipulator inverse dynamics. Intelligent Robots and Systems (IROS), 2015 \nIEEE/RSJ International Conference on; 2015. p. 3442-3448.\n[116] Günther J, Pilarski PM, Helfrich G, et al. Intelligent laser welding through \nrepresentation, prediction, and control learning: an architecture with deep neural \nnetworks and reinforcement learning. Mechatronics. 2015;34:1-11.\n[117] Günther J, Pilarski PM, Helfrich G, et al. First steps towards an intelligent laser \nwelding architecture using deep neural networks and reinforcement learning. \nProcedia Technology. 2014;15:474-483.\n[118] Masci J, Meier U, Cireşan D, Schmidhuber J. (2011). Stacked convolutional auto-\nencoders for hierarchical feature extraction. Artificial Neural Networks and \nMachine Learning–ICANN 2011, 52-59.\n[119] Ashmore SC, Gashler MS. (2016, December). Practical techniques for using \nneural networks to estimate state from images. In Machine Learning and \nApplications (ICMLA), 2016 15th IEEE Int Conf on (pp. 916-919). IEEE.\n[120] Rifai S, Vincent P, Muller X, Glorot X, Bengio Y. (2011). Contractive auto-\nencoders: explicit invariance during feature extraction. In Proc of the 28th Int \nConf on Machine Learning (ICML-11) (pp. 833-840).\n[121] Jain A, Koppula HS, Soh S, et al. Brain4Cars: car that knows before you do via \nsensory-fusion deep learning architecture. 2016. Available: \nhttp://arxiv.org/abs/1601.00740\n[122] Lenz I, Knepper R, Saxena A. Deepmpc: learning deep latent features for model \npredictive control. Robotics: Science and Systems XI; Rome, Italy. 2015.\n[123] Hwang J, Jung M, Madapana N, et al. Achieving \"synergy\" in cognitive behavior \nof humanoids via deep learning of dynamic visuo-motor-attentional \ncoordination. Humanoid Robots (Humanoids), 2015 IEEE-RAS 15th \nInternational Conference on; Seoul. 2015. p. 817-824.\n[124] Hochreiter S, Schmidhuber J. Long short-term memory. Neural Computation, \n1997;9(8):1735-1780.\n40\n [125] Zhang T, Kahn G, Levine S, et al. Learning deep control policies for autonomous \naerial vehicles with MPC-guided policy search. 2015. Available: \nhttp://arxiv.org/abs/1509.06791\n[126] Lillicrap TP, Hunt JJ, Pritzel A, et al. Continuous control with deep reinforcement \nlearning. 2015. Available: http://arxiv.org/abs/1509.02971\n[127] Finn C, Levine S, Abbeel P. Guided cost learning: deep inverse optimal control \nvia policy optimization. 2016. Available: http://arxiv.org/abs/1603.00448\n[128] Levine S, Finn C, Darrell T, et al. End-to-end training of deep visuomotor \npolicies. J Mach Learning Research. 2016;17:1-40.\n[129] Rosenstein M, Barto A.(2004). J. 4 supervised actor-critic reinforcement learning. \nHandbook of Learning and Approximate dynamic programming, 2, 359.\n [130] Houthooft R, Chen X, Duan Y, Schulman J, De Turck F, Abbeel P. (2016). VIME:\nvariational information maximizing exploration. In Advances in Neural \nInformation Processing Systems (pp. 1109-1117).\n[131] Osband I, Blundell C, Pritzel A, Van Roy B. (2016). Deep exploration via \nbootstrapped DQN. In Advances in Neural Information Processing Systems (pp. \n4026-4034).\n[132] Pinto L, Gupta A. Supersizing self-supervision: learning to grasp from 50k tries \nand 700 robot hours. 2015. Available: http://arxiv.org/abs/1509.06825\n[133] Kappler D, Bohg J, Schaal S. Leveraging big data for grasp planning. 2015 IEEE \nInternational Conference on Robotics and Automation (ICRA); Seattle, WA. \n2015. p. 4304-4311.\n[134] Pratt GA. Is a cambrian explosion coming for robotics? Journal of Economic \nPerspectives. 2015;29(3):51-60.\n[135] Szegedy C, Zaremba W, Sutskever I, et al. Intriguing properties of neural \nnetworks. 2013. Available: http://arxiv.org/abs/1312.6199\n41\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2017-07-22",
  "updated": "2017-07-22"
}