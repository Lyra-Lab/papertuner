{
  "id": "http://arxiv.org/abs/1707.07700v1",
  "title": "A Deep Investigation of Deep IR Models",
  "authors": [
    "Liang Pang",
    "Yanyan Lan",
    "Jiafeng Guo",
    "Jun Xu",
    "Xueqi Cheng"
  ],
  "abstract": "The effective of information retrieval (IR) systems have become more\nimportant than ever. Deep IR models have gained increasing attention for its\nability to automatically learning features from raw text; thus, many deep IR\nmodels have been proposed recently. However, the learning process of these deep\nIR models resemble a black box. Therefore, it is necessary to identify the\ndifference between automatically learned features by deep IR models and\nhand-crafted features used in traditional learning to rank approaches.\nFurthermore, it is valuable to investigate the differences between these deep\nIR models. This paper aims to conduct a deep investigation on deep IR models.\nSpecifically, we conduct an extensive empirical study on two different\ndatasets, including Robust and LETOR4.0. We first compared the automatically\nlearned features and hand-crafted features on the respects of query term\ncoverage, document length, embeddings and robustness. It reveals a number of\ndisadvantages compared with hand-crafted features. Therefore, we establish\nguidelines for improving existing deep IR models. Furthermore, we compare two\ndifferent categories of deep IR models, i.e. representation-focused models and\ninteraction-focused models. It is shown that two types of deep IR models focus\non different categories of words, including topic-related words and\nquery-related words.",
  "text": "A Deep Investigation of Deep IR Models\nLiang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Xueqi Cheng\nCAS Key Lab of Network Data Science and Technology, Institute of Computing Technology,\nChinese Academy of Sciences\nBeijing, China\npangliang@sofware.ict.ac.cn,{lanyanyan,guojiafeng,junxu,cxq}@ict.ac.cn\nABSTRACT\nTe eﬀective of information retrieval (IR) systems have become\nmore important than ever. Deep IR models have gained increasing\natention for its ability to automatically learning features from raw\ntext; thus, many deep IR models have been proposed recently. How-\never, the learning process of these deep IR models resemble a black\nbox. Terefore, it is necessary to identify the diﬀerence between\nautomatically learned features by deep IR models and hand-crafed\nfeatures used in traditional learning to rank approaches. Further-\nmore, it is valuable to investigate the diﬀerences between these\ndeep IR models. Tis paper aims to conduct a deep investigation\non deep IR models. Speciﬁcally, we conduct an extensive empirical\nstudy on two diﬀerent datasets, including Robust and LETOR4.0.\nWe ﬁrst compared the automatically learned features and hand-\ncrafed features on the respects of query term coverage, document\nlength, embeddings and robustness. It reveals a number of dis-\nadvantages compared with hand-crafed features. Terefore, we\nestablish guidelines for improving existing deep IR models. Fur-\nthermore, we compare two diﬀerent categories of deep IR models,\ni.e. representation-focused models and interaction-focused models.\nIt is shown that two types of deep IR models focus on diﬀerent cat-\negories of words, including topic-related words and query-related\nwords.\nCCS CONCEPTS\n•Information systems →Retrieval models and ranking;\nKEYWORDS\nDeep Learning; Ranking; Text Matching; Information Retrieval\n1\nINTRODUCTION\nRelevance ranking is the core problem in information retrieval (IR)\nsystem, which is to determine the relevance score for a document\nwith respect to a particular query. Traditional approaches to tackle\nthis problem include heuristic retrieval models and learning to\nrank approach. Heuristic retrieval models, such as TF-IDF [12] and\nBM25 [11], propose to incorporate human knowledge on relevance\ninto the design of ranking function. Modern learning to rank ap-\nproach currently turns to apply machine learning techniques to\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR’17), August 7–11, 2017,\nShinjuku, Tokyo, Japan\n© 2017 Copyright held by the owner/author(s). 123-4567-24-567/08/06.\nDOI: 10.475/123 4\nthe ranking function, which combines diﬀerent kinds of human\nknowledge (relevance features such as BM25 and PageRank) and\ntherefore has achieved great improvements on the ranking per-\nformances [6]. However, a successful learning to rank algorithm\nusually relies on eﬀective hand-crafed features for the learning\nprocess. Te feature engineering work is usually time consuming,\nincomplete and over-speciﬁed, which largely hinder the further\ndevelopment of this approach [3].\nDeep IR models have gained increasing atention for its ability\nto automatically learning features from raw text of query and doc-\nument. Terefore, many deep IR models have been proposed to\nsolve relevance ranking problem only considering the query and\ndocument textual data. As [3] has mentioned, deep IR models can\nbe categorized into two branches, namely representation-focused\nmodels and interaction-focused models, depending on the diﬀerent\nstructures. However, deep IR models are the end-to-end system,\nthe learning process of the deep IR models are still resemble a black\nbox. Tus, it is curious to us that the diﬀerent between automati-\ncally learned features by deep IR models and hand-crafed features\nused in traditional learning to rank approaches, and the diﬀerences\nbetween these deep IR models.\nIn this paper, we conduct a deep investigation on deep IR models\nunder two aspects. Firstly, we compare the automatically learned\nfeatures with hand-crafed features. Hand-crafed features, includ-\ning TF-IDF, BM25 and other traditional retrieval models, are com-\nbined human knowledge and proven to follow some heuristic re-\ntrieval constrains [2]. Incorporate with these constrains, we check\nthe properties of deep IR models, for example query term coverage,\ndocument length and embedding aﬀect the performance of deep\nIR models. Additionally, We pick out bad cases from test dataset to\nconduct error analysis. For each group of bad cases, we establish\nguidelines for improving existing deep IR models. We also show\nthat the robustness of automatically features are stronger than hand-\ncrafed features. Secondly, we compare the diﬀerences between\ntwo kinds of deep IR models. By visualizing the pooling words\nof these deep IR models, it is shown that representation-focused\nmodels focus on topic-related words and interaction-focused mod-\nels focus on query-related words. A synthetic experiment further\nprove these properties.\nTe rest of the paper is organized as follows. In Section 2, we\nintroduce two types of existing deep IR models. In Section 3, we\nintroduce the datasets for performance evaluation. Section 4 com-\npares the diﬀerences between the automatically learned features\nand the hand-crafed features and Section 5 compares the diﬀer-\nences between two types of deep IR models. Section 6 concludes\nthe paper.\narXiv:1707.07700v1  [cs.IR]  24 Jul 2017\nFigure 1: Representation-Focused Models\n2\nEXISTING DEEP IR MODEL\nTe core problem of information retrieval is to determine the rele-\nvance score for a document with respect to a particular query, which\ncan be formalized as the follows as indicated in [4] and [8]. Given a\nqueryQ = {q1, · · · ,qm} and a document D = {w1, · · · ,wn}, where\nqi and wj stand for the i-th and j-th words in the query and docu-\nment respectively, the degree of relevance is usually measured as a\nscore produced by a scoring function based on the representations\nof the query and document:\nmatch(Q, D) = F(Φ(Q), Φ(D)),\n(1)\nwhere Φ is a function to map query/document to a vector, and F is\na scoring function for modeling the interactions between them.\nDeep IR models propose to automatically learn relevance fea-\ntures from raw text data, i.e. Q and D. Considering diﬀerent struc-\ntures, existing deep models can be categorized into two kinds:\nrepresentation-focused models and interaction-focused models. Te\nrepresentation-focused models propose to focus on the learning\nparameters of function Φ, while interaction-focused models put\nmore eﬀorts on learning parameters of function F.\n2.1\nRepresentation-Focused Models\nRepresentation-focused models try to build a good representation\nfor query/document with a deep neural network, and then con-\nduct matching between two abstract representation vector. In this\napproach, Φ is a relatively complex representation mapping func-\ntion from text to vector, while F is a relatively simple matching\nfunction. For example, in DSSM [5], Φ is a feed forward neural\nnetwork with leter trigram representation as the input, while F is\nthe cosine similarity function. In CDSSM [13], Φ is a convolutional\nneural network (CNN) with leter trigram representation as the\ninput, while F is the cosine similarity function. In ARC-I [4], Φ\nis a CNN with word embeddings as the input, while F is a multi-\nlayer perceptron (MLP). Without loss of generality, all the model\narchitectures of representation-focused models can be viewed as a\nSiamese (symmetric) architecture over the text inputs, as shown in\nFigure 1.\n2.2\nInteraction-Focused Models\nInteraction-focused models ﬁrst build the local interactions between\nquery and document, based on basic representations, and then use\nFigure 2: Interaction-Foused Models\ndeep neural networks to learn the complex interaction paterns for\nrelevance. In this approach, Φ is usually a simple mapping function\nto map query and document to a sequence of words or word vectors,\nwhile F is a complex deep model with many learnable parameters.\nTypically, function F can be represented as the compound function\nof H, G and M, i.e. F = H ◦G ◦M, and the scoring function can be\nwriten as the following form:\nmatch(Q, D) = H ◦G ◦M(Φ(Q), Φ(D)),\n(2)\nwhere M is a function used to obtain the local interactions between\nthe representation of Q and D, H is a deep neural network to obtain\nthe abstract interaction paterns, and H is an aggregation function\nto obtain the relevance score based on the interaction paterns.\nFor example, in DeepMatch [7], Φ is an identical function which\nmaintains the representation of query/document as a sequences\nof words, M is a basic interaction function to output parallel texts,\ni.e. the set of interacting pairs of words from Q and D, G is a feed\nforward neural network constructed by a topic model over the\nparallel texts, and H is a logistic regression unit to summarize\nthe decision to obtain the ﬁnal relevance score. In ARC-II [4], Φ\nis a mapping function to map query/document to a sequence of\nword embeddings, M is the 1-D convolution operation over each\npatch of words from Q and D, G is a CNN, and H is an MLP. In\nMatchPyramid [8], Φ maps query/document to a sequence of word\nvectors, M is the similarity function between each word pair from Q\nand D to output a word-level interaction matrix, G is a CNN, and H\nis an MLP. In Match-SRNN, Φ is the same as that in MatchPyramid,\nM is a tensor operation to incorporate high dimensional word level\ninteractions, G is a 2D-GRU, and H is an MLP. Without loss of\ngenerality, all the interaction focused models can be viewed as a\ndeep architecture over the local interaction matrix, as shown in\nFigure 2.\n3\nDATASET\nIn this section, we introduce two datasets used for model analysis,\nnamely Robust Dataset, LETOR 4.0 Dataset. Te statistics of these\ndatasets are shown in Table 1. We can see that these datasets repre-\nsent diﬀerent sizes and genres of heterogeneous text collections.\n3.1\nRobust Dataset\nRobust data is a small news dataset. Its topics are collected from\nTREC Robust Track 2004. We made use of the title of each TREC\nTable 1: Statistics on Robust and LETOR 4.0 .\nDataset\n#Qery\n#Doc\n#Relevance\nRobust\n250\n12240\n12881\nLETOR 4.0\n1501\n57899\n61480\ntopic in our experiments. Te retrieval experiments on this dataset\nare implemented using the Galago Search Engine1. During indexing\nand retrieval, both documents and query words are white-space\ntokenized, lower-cased, and stemmed using the Krovetz stemmer.\nStopword removal is performed on query words during retrieval\nusing the INQUERY stop list.\n3.2\nLETOR 4.0 Dataset\nLETOR4.0 dataset [9] is a benchmark data for evaluating learning to\nrank methods. Tis dataset is sampled from the .GOV2 corpus using\nthe TREC 2007 Million Qery track queries. Tis dataset contains\ntwo subsets, i.e. MQ2007 and MQ2008. In this paper, we use MQ2007\nfor evaluation because it is much larger than MQ2008. MQ2007\ncontains 1692 queries and 65,323 documents, which is much larger\nthan Robust. Each query and document pair in this dataset is\nrepresented as a vector using 46 diﬀerent features. Te separation\nof training, validation and testing set are set to default. Te reason\nto choose LETOR4.0 beyond Robust lie in that: 1) the LETOR4.0\ndata is relatively large (especially for the query number), therefore\nit is more appropriate for training a deep learning model; 2) the\nfeatures have already been extracted, therefore it is convenient to\nconduct comparisons with learning to rank baselines.\n4\nCOMPARISONS WITH HAND-CRAFTED\nFEATURES\nLearning to rank approaches with hand-crafed features have achieved\na great success in information retrieval. In these hand-crafed fea-\ntures, BM25 and language model are the strong baselines in informa-\ntion retrieval. It is mainly because these traditional models satisﬁed\nseveral heuristic retrieval constrains proposed by Fang et al. [2].\nTese constrains reveal importance properties in information re-\ntrieval. Tey are 1) Term Frequency Constraints (TFC1 / TCF2); 2)\nTerm Discrimination Constraint (TDC); 3) Length Normalization\nConstraints (LNC1 / LNC2); 4) TF-LENGTH Constraint (TF-LNC).\nEmpirical results show that when a constraint is not satisﬁed, it\nofen indicates non-optimality of the method.\nIn this section, our aim is to ﬁnd out the diﬀerences between\ndeep IR models and hand-crafed features. Firstly, bad cases are\ncategorized to identify the weakness of deep IR models. Ten we\nmake use of heuristic retrieval constrains to explain the disadvan-\ntages of the deep IR models, such as query term coverage problem,\ndocument length problem and embedding semantic abuse problem.\nLastly, we point out that the advantage of the deep IR models is the\nrobustness of the automatically learnt features.\n1htp://www.lemurproject.org/galago.php\nTable 2: Performance comparison of deep IR models and\nhand-crafed features on Robust, LETOR 4.0.\nRobust\nModel\nNDCG@1\nNDCG@10\nP@1\nP@10\nMAP\nBM25-Title\n0.563\n0.445\n0.563\n0.402\n0.255\nLM.JM-Title\n0.560\n0.443\n0.560\n0.400\n0.253\nArc-I\n0.124\n0.138\n0.124\n0.132\n0.050\nMatchPyramid\n0.364\n0.242\n0.364\n0.240\n0.164\nLETOR 4.0\nModel\nNDCG@1\nNDCG@10\nP@1\nP@10\nMAP\nBM25-Title\n0.358\n0.414\n0.427\n0.366\n0.450\nLM.JM-Title\n0.300\n0.374\n0.359\n0.329\n0.421\nArc-I\n0.310\n0.386\n0.376\n0.364\n0.417\nMatchPyramid\n0.362\n0.409\n0.428\n0.371\n0.434\n4.1\nPerformance Comparison\nBefore we conduct error analysis, we ﬁrst overview the performance\nof two kinds of deep IR models and two strong hand-crafed features,\nnamely BM25 and language model, shown in Table 2.\nTe experimental result shows that deep IR models perform\nworse than the hand-crafed features, especially compared with\nBM25. Furthermore, we ﬁnd that the performance gap between\ndeep IR models and hand-crafed features is large in Robust dataset\ncompared with LETOR 4.0 dataset. Te main reason lays to the small\nsize of the dataset, which go against the data-driven mechanism\nin deep learning. We reduce the performance gap by increasing\nthe size of the dataset in LETOR 4.0, and we believe that the larger\ndataset will result in beter performance, even beter than the hand-\ncrafed features.\nApart from the aﬀection of dataset size, other heuristic problems\nare found by conducting error analysis. We will demonstrate these\nheuristic problem in the next section.\n4.2\nError Analysis\n4.2.1\nQery term coverage problem. With analyzing number\nof bad cases of deep IR model results, we ﬁnd that in the most of\nthe cases, deep IR models are hard to satisfy the Term Discrimina-\ntion Constraint (TDC) [2]. TDC ensures that given a ﬁxed num-\nber of occurrences of query terms, we favor a document that has\nmore occurrences of discriminative terms. However, for both Arc-I\nand MatchPyramid, we never intentionally designed a network\nstructure to distinguish two same query terms matching from two\ndiscriminative query terms matching. Although, it is possible for\nthese two models to learn from data, the limitation is the size of\nthe dataset, which make it hard for deep IR models to learn TDC\nwithout any prior information.\nWe pick out one example, shown in Figure 3, to demonstrate this\nissue. Here the query is “tooth fairy museum”. In the lef part of\nFigure 3, the document only contains query terms “museum” and\n“fairy”, where term “fairy” occurs more than 10 times. While in\nthe right part of Figure 3, the document contains all three query\nterms, but the total number of query terms is only 6 times. With\nthe constrain of TDC, a good IR model prefer to rank right side\ndocument higher than lef side. However, the deep IR models make\nthe opposite decision.\nSuggestions: In order to recover query term coverage prob-\nlem we propose two suggestions for representation-focused model\nand interaction-focused model respectively. 1) For representation-\nfocused models, an atention mechanism turns to be a useful strat-\negy to distinguish diﬀerent query terms. 2) For interaction-focused\nmodels, pooling across each query term rows in the interaction\nmatrix is helpful for considering diﬀerent query terms individually.\n4.2.2\nDocument length problem. Another type of errors relates\nto the general preprocessing of document length limitation before\nfeed it into the deep IR models. With the limitation of memory and\ntime, documents tailor to a maximum length. It always make sense\nin paraphrase identiﬁcation tasks and question answering tasks,\nwhich have the similar text length. But in information retrieval\ntasks, documents have variance length, ranging from 10 to 10,000\nwords. Directly cut oﬀthe exceeded text leads to information\nloss and violates the Term Frequency Constrain (TF1). TF1 claims\nthat replacing one of the non-query term word to a query term\nincreases the relevance score. Terefore for a cut-oﬀed document,\nif the replacement occurs in the exceeded part, the relevance score\nkeeps the same in deep IR models.\nFor example, a document contains 5000 words in Figure 4, and\nthe maximum length of our model set to 500 words. Tus we\nﬁnd that in the view of deep IR models, only few query terms\n“withdrawal” occur in the top 500 words of the document. While, as\nthe ﬁgure shown, the query terms “methadone” and “baby” occur\nin the exceeded part of the document.\nA more precision statistic of the last query term match position\nin a document is shown in Figure 5. Te red line represents the 500\nwords threshold, thus about 40% of document loss the query term\ninformation because of the length limitation of the document. So\nthis special preprocess aﬀect 40% of the documents, which reﬂecting\non the worse model performance.\nSuggestions: For this situation, inspired by passage retrieval\napproaches, document can be split into several short passages. Ten\ndeep IR models apply on each ¡query, passage¿ pairs. Finally, the\nrelevance score is the aggregation of each ¡query, passage¿ local\nrelevance scores.\n4.2.3\nEmbedding semantic abuse problem. Diﬀerent from using\none hot word representation in the hand-craf features such as\nBM25 and language model, recently most deep IR models adopt pre-\ntrained word embeddings as the word representation, except the\nleter-trigrams used in DSSM and CDSSM. Te advantage of adopt-\ning word embedding as the word representation is to investigate\nsemantic matching information into the model. However, on the\nﬂip side, semantic matching brings too much noise matching sig-\nnals, which covers up the exact matching signals and dominates the\nﬁnal matching score. Similar to the heuristic constrains proposed\nin [2] which only consider exact matching signals, we append one\nconstrain by considering semantic matching signals, called Term\nSemantic Frequency Constrain (TSFC).\nTSFC: Let q = {w} be a query with only one term w. Assume\n|d1| = |d2| and s(w,d1) = s(w,d2). If c(w,d1) > c(w,d2), then\nf (d1,q) > f (d2,q).\nTis constrain assumes that two documents have the same length,\nand the sum of the semantic matching signals (including exact\nmatching signals) are equivalent, s(w,d1) = s(w,d2). Te larger\nnumber of the exact matching signals c(w,d), the higher relevance\nscore f (d,q).\nTake an instance in LETOR 4.0 Dataset as an example shown in\nFigure 6. Te given query term “noradrenaline” has a high similarity\nwith so many medical related words, such as “epinephrine”, “cate-\ncholamine” and “metabolism”. Tus the sum of matching signals in\nthe lower document is higher than that in the upper document, even\nthe lower document dose not has any exact matching signal with\nquery term “noradrenaline”. As the experiment shows that deep\nIR models prefer the lower document, for the sake of the higher\ndensity matching signals.\nSuggestions: As TSFC shown that we need enlarge the gap\nbetween semantic matching signals and exact matching signals.\nAs the Figure 7 shows that for interaction-focused models, we can\ndeﬁne a proper similarity function between words, so that the exact\nmatching signals are larger than all semantic matching signals, such\nas the similarity functions show in Figure 7(b) and Figure 7(c).\n4.2.4\nFeature Robustness. Te robustness of the features can\nbe interpreted as that when some of the features are missing, how\nmuch it aﬀects the model performance. Te features we take into\nconsideration are the 46 dimensional features provided in LETOR\n4.0 dataset and the last layer 20 dimensional outputs in the Arc-I\nand MatchPyramid. Ten we use a linear model to ﬁt these 3 sets of\nfeatures to the ﬁnal relevance labels. In this way, the learnt weights\nreﬂect the importance of the features.\nIn order to visualize the robustness diﬀerent between hand-\ncrafed features, Arc-I and MatchPyramid, we conduct experiments\non the ﬁrst fold of LETOR 4.0 dataset. Afer suﬃcient model train-\ning, we ﬁrstly sort features by its importance (their corresponding\nweight). Ten remove features one-by-one following the order\nof the feature importance at each time. Finally, evaluate the per-\nformance of the rest of the features. Te Figure 8 illustrates the\nprocedure of the feature removing. Figure 8(a) shows the result of\nhand-crafed features, as we can see that when we remove the ﬁrst\ntwo features the performance drops a lot to about 0.36. Figure 8(b-c)\nshows the results of Arc-I and MatchPyramid, as we can see that\neven half of the feature have been removed, the model performance\naﬀects a litle. Tat is to say, deep IR models automatically learnt\nfeatures turn to be more robustness than the hand-crafed features.\n5\nCOMPARISONS AMONG DEEP IR MODELS\nIn this section, we investigate the diﬀerences between representation-\nfocused models and interaction-focused models based on informa-\ntion retrieval task. Deep IR models lay into these two categories,\nsuch as DSSM, CDSSM and Arc-I belong to representation-focused\nmodels, Arc-II and MatchPyramid belong to interaction-focused\nmodels. We choose two classical models from each category, Arc-I\nand MatchPyramid. In order to explorer their intrinsic diﬀerences,\nwe conduct our analysis on Robust dataset, LETOR 4.0 dataset and\na simulated dataset.\n… library museum legend christmas fairy legend christmas\nfairy word fairy come latin word fata meaning fate means\nfairy cousin classical fates believe control fate destiny hu-\nman race hope fairy associate christmas good … good men\ngermany christmas fairy legend tell … pater small feet\ncoming hall room open door fairy clad sparkle robe dance\nlaugh singing splendid … ernestine queen fairy came return\nlost gold ring love sight count oto soon ask fairy queen\nbride … live happily years count oto fairy wife decided\nhunt forest near castle count grew impatient …\n… dr samuel harris national museum dentistry baltimore\nmade possible generous support colgate palmolive children\nmuseum virginia select nine children science museum host\nbranch bristle … toothbrush ancient babylonia st century\ntooth sticks made wild … proper space inspire visitor make\nhealthy snack choice computer driven essential toothbrush\ntooth fairy tech savvy visitor option create virtual tooth-\nbrush learn evolutionary time periods emphasize dimension\ntimeline visitor take time innovate design handcrafe tooth-\nbrush bench view vintage dental product poster ﬁnd …\nFigure 3: Two documents related to query “tooth fairy museum”, the right document cover one more query term than the lef\ndocument, while the lef document rank high in the deep IR models.\nFigure 4: An example document shows that the exceed part of a long document has much query term matching information.\nFigure 5: Te distribution of last query term matching posi-\ntion.\nFigure 6: An example illustrates the embedding seman-\ntic matching problem.\nTe ﬁrst document contains ex-\nact matching signals, while the second document contains\nmuch high semantic matching signals.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nSimilarity Score\n0\n100\n200\n300\n400\n500\n600\n700\nCount\nIdentity Word\n(a) Dot Product\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity Score\n0\n100\n200\n300\n400\n500\n600\n700\nCount\nIdentity Word\n(b) Cosine\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity Score\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\nCount\nIdentity Word\n(c) Gaussian Kernel\nFigure 7: Choose one word from the vocabulary and mea-\nsure the similarity between other words, we draw the his-\ntogram of three type of similarity functions: dot product,\ncosine and gaussian kernel. Te arrow point the similarity\nbetween two identity word (the word we choose).\nFigure 8: Lef: 46 dimensional hand-crafed features; Middle: Arc-I last layer 20 dimensional features; Top: MatchPyramid\nlast layer 20 dimensional features.\nTable 3: Performance comparison of diﬀerent deep IR mod-\nels on Robust, LETOR 4.0.\nRobust\nModel\nNDCG@1\nNDCG@10\nP@1\nP@10\nMAP\nDSSM\n0.122\n0.137\n0.122\n0.135\n0.048\nCDSSM\n0.118\n0.134\n0.118\n0.130\n0.042\nArc-I\n0.124\n0.138\n0.124\n0.132\n0.050\nArc-II\n0.140\n0.148\n0.140\n0.156\n0.054\nMatchPyramid\n0.364\n0.242\n0.364\n0.240\n0.164\nLETOR 4.0\nModel\nNDCG@1\nNDCG@10\nP@1\nP@10\nMAP\nDSSM\n0.290\n0.371\n0.345\n0.352\n0.409\nCDSSM\n0.288\n0.325\n0.333\n0.291\n0.364\nArc-I\n0.310\n0.386\n0.376\n0.364\n0.417\nArc-II\n0.317\n0.390\n0.379\n0.366\n0.421\nMatchPyramid\n0.362\n0.409\n0.428\n0.371\n0.434\n5.1\nPerformance Comparison\nTe performance comparison results illustrate in Table. 3. Afer\ncomparing diﬀerent models on diﬀerent datasets, we can conclude\nas follow: 1) interaction-focused models are performed beter than\nrepresentation-focused models; 2) the gap between representation-\nfocused models and interaction-focused models on dataset Robust\nis larger than the gap on dataset LETOR 4.0.\nIt motives us to explore the diﬀerences between representation-\nfocused models and interaction-focused models.\n5.2\nProperty Analysis\n5.2.1\nText Representations. Text representations is a major task\nfor representation-focused models, since text representation is used\nto compress most valuable and distinguishable information into\none vector. In paraphrase identiﬁcation task, two sentences are\nsymmetric and have similar length. However, in information re-\ntrieval, query and document are totally diﬀerent objects. Qery is\nabstract and almost every terms in it reﬂect a perspective of search\nintent. Tus, text representation for a query need to keep all the\nquery terms information in one vector. On contrast, document is\nelaborate and relevant document just follow two hypotheses in the\nliterature [11]. Te Verbosity Hypothesis assumes that a long docu-\nment is like a short document, covering a similar scope but with\nmore words; while the Scope Hypothesis assumes a long document\nFigure 9: Te Arc-I pooling words have relation to LDA topic\nwords.\nconsists of a number of unrelated short documents concatenated\ntogether. Tus, text representation for a document only need to\ncontain the important part of a document.\nIn order to analyze the information encoded in text representa-\ntion, we visualize the words at the max pooling position in Arc-I\nmodel.\nAs an example, in Figure 9, the query terms are “sante fe new\nmexico”, which are listed in the ﬁrst text box. Te words selected\nby max pooling layer based on convolution output feature maps,\nare listed in second text box, such as “national” “part”. It is evident\nto see, all of the pooling words are not the words appear in query\nterms. Te reason of the above observation is that the document and\nquery text representations are extracted independently, thus the\nrelation of these two representations are weak. For document, most\ninformative words are extracted to composite the representation,\nwhich have the same purpose with the topic models. Tus we\nassume that the pooling words are related to the topic words in the\ntopic models.\nTo further check this assumption, we conduct comparison with\ntopic model. Without loss of generality, we choose the state-of-the-\nart, LDA model [1] to generate topic words. LDA model is trained on\nthe whole corpus, with 50 topics and default parameters in package\ngensim [10]. Ten we collect top 50 words in each topic, totally\n853 words (some words lay in multiple topics). Meanwhile, we\ncollect all pooling words using the Arc-I model, and sort decreasing\nby the words frequency. Te Figure 10 shows the distribution of\nthe word overlap ratio between top pooling words and the top 50\nLDA topic words. From the results as we can see, 80% of top 500\npooling words come from the top 50 LDA topic words, for example\nin Figure 9, “national” and “park” come from the topic 48 and “oﬃce”\nFigure 10: Te overlap ratio of top pooling words in Arc-I\nwith LDA topics top50 words and query words.\n… crest trail name act two national scenic trail … national\nhistoric trail recognize prominent past route exploration\nmigration\nmilitary\naction\nhistoric\ntrail\ngenerally\nconsist\nremnant site trail segment necessarily continuous … governing\nregulation national national system general authority act stat\namend national national recreation … title part code federal\nregulation department agriculture national\nenvironmental\npolicy act national trail system act stat amend policy rural\ndevelopment ... lakewood colorado contact steve http www fs\nfed department interior national national service contact salt\nnational city branch long distance trail office respect pony …\nnational park service long distance trail office south state …\nplace exact trail location determined individual trail club …\n… trail system bent old fort located sante fe national historic\ntrail statue pony express rider located pony express national\nhistoric trail national …\nsuperintendent department interior\nnational national service contact sante fe branch long\ndistance trail office respect sante fe national historic trail\ndepartment interior national national service long distance trail\noffice po box sante fe new mexico contact national\nrecreation trail iv location national system resource national trail\nsystem … individual trail club detailed information trail portion\nthree designate trail located colorado continental divide national\nscenic trail follow continental divide wyoming border new\nmexico\nborder\ncontinental\ndivide\ntrail\nspectacular\nbackcountry travel length rocky mountain mexico canada …\nQuery:\nSante Fe New Mexico\nARC-I\nMatchPyramid\nFigure 11: Te diﬀerent pooling words, highlighted using\nbold font, in the model of Arc-I and MatchPyramid.\ncome from the topic 36. Additionally, we also measure the overlap\nratio between top pooling words and query words (green line in\nFigure 10). Contrast with LDA topic words, the overlap ratio of\nquery words is relative small, for example about 30% of top 500\npooling words come from the query terms.\nWith above observation, we can conclude that representation-\nfocused models, eg. Arc-I, generate topic related text representation\nas the topic model do.\n5.2.2\nInteraction Representations. Interaction-focused models\naim to extract interaction representations on the top of interac-\ntion matrix, which is constructed using word-by-word similarity.\nIn paraphrase identiﬁcation, interaction representation can be in-\nterpreted as a hierarchical matching signal composition process,\nas described in [8]. Firstly, word level matching signals can be\ncomposited to phrase level matching signals, for example n-grams\nmatching and n-terms matching. Ten, conduct several composi-\ntion steps to achieve sentence level matching signal. However, it\ndiﬀers a lot when we apply MatchPyramid in information retrieval\ntask. Qery is too short to be treated as a sentence, for instance,\nquery “tooth fairy museum” only can be treated as a phrase or\nquery “guatemala” only has one word. So that hierarchical match-\ning signal composition reduces to word/phrase matching signals\naggregation.\nIn order to understand what interaction-focused models have\nlearnt, we analyze the MatchPyramid model from the pooling words.\nDiﬀerent from the Arc-I model, given a query document pair, al-\nmost all pooling words of MatchPyramid come from the query\nterms. Figure 11 shows the diﬀerent words pooling by Arc-I and\nMatchPyramid. In MatchPyramid, we use cosine similarity function\nto evaluate words similarity, thus identical words in query and doc-\nument achieve the highest similarity value as 1. Ten the following\nconvolutional and pooling operations have a high probability to\npool out these words.\n5.2.3\nSynthetic Analysis. From the above discussion, we realize\nthat presentation-focused models and interaction-focused models\ncapture diﬀerent kinds of informations in IR task. In order to dis-\ntinguish the performance of text representations and interaction\nrepresentations, we introduce an additional synthetic dataset to\nverify the ability of two kinds of deep IR models.\nTe synthetic dataset is constructed using two well-designed\nground truth.\n(1) Topic Match Te relevance document contain a speciﬁc\ntopic, where each topic expresses as a sequential of words.\nFor example, word sequence “neural network” represent a\ntopic, and any document contains this word sequence lays\ninto this topic, marked as relevance.\n(2) Density Match Te relevance degree of a document is\nproportional to the density of query terms it contained.\nFor example, the document contain 100 query terms is\nmore relevant compare to the one contain 10 query terms.\nTe whole dataset contains 10,000 queries. In each query, words\nare randomly sampled from a 2000 words vocabulary. Ten, for\neach query we construct one relevant document and four irrelevant\ndocuments. In each document, words are randomly sampled from\nthe same vocabulary with query. Te length of query is ranging\nfrom 2 to 8, while the length of the document is ranging from 300\nto 700.\nArc-I and MatchPyramid are evaluated on these two dataset\nrespectively, and the results are shown in Figure 12. As we can\nsee, the performance of representation-focused model Arc-I and\ninteraction-focused model MatchPyramid is quite the opposite. Te\nrepresentation-focused model have ability to learn topic informa-\ntion, while very litle information about query terms density. On\nRandom\nArc-I\nMatchPyramid\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.46\n0.71\n0.46\n0.46\n0.47\n0.98\nMAP\nTopic Match\nDense Match\nFigure 12: Performance comparison of Random, Arc-I and\nMatchPyramid on synthetic data.\nthe contrast, interaction-focused model is good at learning query\nterms density information, but almost no topic information covered.\n6\nCONCLUSIONS AND FUTURE WORK\nAs a conclusion, deep IR models still perform worse than hand-\ncrafed features. Te possible reason is that under a limit size of\ndataset, hand-crafed features, such as BM25, obey the heuristic re-\ntrieval constrains, while deep IR models ignored. Apart from using\nlarger dataset, we establish guidelines to explicit using heuristic\nretrieval constrains, in order to further improvement of deep IR\nmodels.\nOn comparing representation-focused models and interaction-\nfocused models, we can conclude that, 1) representation-focused\nmodels focus on learning good text representation, which encode\nthe topic related words in the ﬁnal representation; 2) interaction-\nfocused models focus on learning good interaction representation,\nwhich good at collecting the density of matching signals. Tese\ninteresting ﬁndings pave a way to beter understand the diﬀerent\ndeep IR models and demonstrate that for diﬀerent applications we\nneed to choose diﬀerent deep IR models.\nREFERENCES\n[1] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet alloca-\ntion. Journal of machine Learning research 3, Jan (2003), 993–1022.\n[2] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information\nretrieval heuristics. In Proceedings of the 27th annual international ACM SIGIR\nconference on Research and development in information retrieval. ACM, 49–56.\n[3] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Crof. 2016. A deep relevance\nmatching model for ad-hoc retrieval. In CIKM. ACM, 55–64.\n[4] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional\nneural network architectures for matching natural language sentences. In NIPS.\n2042–2050.\n[5] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\nHeck. 2013. Learning deep structured semantic models for web search using\nclickthrough data. In CIKM. ACM, 2333–2338.\n[6] Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and\nTrends in Information Retrieval 3, 3 (2009), 225–331.\n[7] Zhengdong Lu and Hang Li. 2013. A deep architecture for matching short texts.\nIn Advances in Neural Information Processing Systems. 1367–1375.\n[8] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng.\n2016. Text matching as image recognition. In AAAI. AAAI Press, 2793–2799.\n[9] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A benchmark collection\nfor research on learning to rank for information retrieval. Information Retrieval\n13, 4 (2010), 346–374.\n[10] Radim ˇReh˚uˇrek and Petr Sojka. 2010. Sofware Framework for Topic Modelling\nwith Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges\nfor NLP Frameworks. ELRA, Valleta, Malta, 45–50. htp://is.muni.cz/publication/\n884893/en.\n[11] Stephen E Robertson and Steve Walker. 1994. Some simple eﬀective approxi-\nmations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR.\nSpringer-Verlag New York, Inc., 232–241.\n[12] Gerard Salton and Michael J McGill. 1986. Introduction to modern information\nretrieval. (1986).\n[13] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr´egoire Mesnil. 2014.\nLearning semantic representations using convolutional neural networks for\nweb search. In WWW. International WWW Conferences Steering Commitee,\n373–374.\n",
  "categories": [
    "cs.IR"
  ],
  "published": "2017-07-24",
  "updated": "2017-07-24"
}