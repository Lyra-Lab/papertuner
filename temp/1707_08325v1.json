{
  "id": "http://arxiv.org/abs/1707.08325v1",
  "title": "Asymmetric Deep Supervised Hashing",
  "authors": [
    "Qing-Yuan Jiang",
    "Wu-Jun Li"
  ],
  "abstract": "Hashing has been widely used for large-scale approximate nearest neighbor\nsearch because of its storage and search efficiency. Recent work has found that\ndeep supervised hashing can significantly outperform non-deep supervised\nhashing in many applications. However, most existing deep supervised hashing\nmethods adopt a symmetric strategy to learn one deep hash function for both\nquery points and database (retrieval) points. The training of these symmetric\ndeep supervised hashing methods is typically time-consuming, which makes them\nhard to effectively utilize the supervised information for cases with\nlarge-scale database. In this paper, we propose a novel deep supervised hashing\nmethod, called asymmetric deep supervised hashing (ADSH), for large-scale\nnearest neighbor search. ADSH treats the query points and database points in an\nasymmetric way. More specifically, ADSH learns a deep hash function only for\nquery points, while the hash codes for database points are directly learned.\nThe training of ADSH is much more efficient than that of traditional symmetric\ndeep supervised hashing methods. Experiments show that ADSH can achieve\nstate-of-the-art performance in real applications.",
  "text": "Asymmetric Deep Supervised Hashing\nQing-Yuan Jiang, Wu-Jun Li\nNational Key Laboratory for Novel Software Technology\nCollaborative Innovation Center of Novel Software Technology and Industrialization\nDepartment of Computer Science and Technology, Nanjing University, P. R. China\njiangqy@lamda.nju.edu.cn, liwujun@nju.edu.cn\nAbstract\nHashing has been widely used for large-scale approximate nearest neighbor search\nbecause of its storage and search efﬁciency. Recent work has found that deep\nsupervised hashing can signiﬁcantly outperform non-deep supervised hashing in\nmany applications. However, most existing deep supervised hashing methods\nadopt a symmetric strategy to learn one deep hash function for both query points\nand database (retrieval) points. The training of these symmetric deep supervised\nhashing methods is typically time-consuming, which makes them hard to effectively\nutilize the supervised information for cases with large-scale database. In this paper,\nwe propose a novel deep supervised hashing method, called asymmetric deep\nsupervised hashing (ADSH), for large-scale nearest neighbor search. ADSH treats\nthe query points and database points in an asymmetric way. More speciﬁcally,\nADSH learns a deep hash function only for query points, while the hash codes for\ndatabase points are directly learned. The training of ADSH is much more efﬁcient\nthan that of traditional symmetric deep supervised hashing methods. Experiments\nshow that ADSH can achieve state-of-the-art performance in real applications.\n1\nIntroduction\nApproximate nearest neighbor (ANN) search [1, 25, 2] has attracted much attention from machine\nlearning community, with a lot of applications in information retrieval, computer vision and so on.\nAs a widely used technique for ANN search, hashing [29, 12, 27, 21, 8, 22, 19, 17, 25, 2, 3, 23] aims\nto encode the data points into compact binary hash codes. Due to its storage and search efﬁciency,\nhashing has attracted more and more attention for large-scale ANN search.\nAs the pioneering work, locality sensitive hashing (LSH) [6, 1] tries to utilize random projections as\nhash functions. LSH-like methods are always called data-independent methods, because the random\nprojections are typically independent of training data. On the contrary, data-dependent methods [17],\nwhich are also called learning to hash (L2H) methods, aim to learn the hash functions from training\ndata. Data-dependent methods usually achieve more promising performance than data-independent\nmethods with shorter binary codes. Hence, data-dependent methods have become more popular than\ndata-independent methods in recent years. Based on whether supervised information is used or not,\ndata-dependent methods can be further divided into two categories [17, 14]: unsupervised hashing\nand supervised hashing. Unsupervised hashing does not use supervised information for hash function\nlearning. On the contrary, supervised hashing tries to learn the hash function by utilizing supervised\ninformation. In recent years, supervised hashing has attracted more and more attention because it can\nachieve better accuracy than unsupervised hashing [18, 20, 14].\nMost traditional supervised hashing methods are non-deep methods which cannot perform feature\nlearning from scratch. Representative non-deep supervised hashing methods includes asymmetric\nhashing with two variants Lin:Lin and Lin:V [20], fast supervised hashing (FastH) [15], supervised\ndiscrete hashing (SDH) [24] and column-sampling based discrete supervised hashing (COSDISH) [9].\narXiv:1707.08325v1  [cs.LG]  26 Jul 2017\nRecently, deep supervised hashing, which adopts deep learning [11] to perform feature learning\nfor hashing, has been proposed by researchers. Representative deep supervised hashing methods\ninclude convolutional neural networks based hashing (CNNH) [30], deep pairwise supervised hash-\ning (DPSH) [14], deep hashing network (DHN) [33] and deep supervised hashing (DSH) [16]. By\nintegrating feature learning and hash-code learning into the same end-to-end architecture, deep\nsupervised hashing can signiﬁcantly outperform non-deep supervised hashing in many applications.\nMost existing deep supervised hashing methods, including those mentioned above, adopt a symmetric\nstrategy to learn one hash function for both query points and database points. The training of these\nsymmetric deep supervised hashing methods is typically time-consuming. For example, the storage\nand computational cost for these hashing methods with pairwise supervised information is O(n2)\nwhere n is the number of database points. The training cost for methods with triplet supervised\ninformation [32] is even higher. To make the training practicable, most existing deep supervised\nhashing methods have to sample only a small subset from the whole database to construct a training\nset for hash function learning, and many points in database may be discarded during training. Hence,\nit is hard for these symmetric deep supervised hashing methods to effectively utilize the supervised\ninformation for cases with large-scale database, which makes the search performance unsatisfactory.\nIn this paper, we propose a novel deep supervised hashing method, called asymmetric deep supervised\nhashing (ADSH), for large-scale nearest neighbor search. The main contributions of ADSH are\noutlined as follows: (1). ADSH treats the query points and database points in an asymmetric way.\nMore speciﬁcally, ADSH learns a deep hash function only for query points, while the binary hash\ncodes for database points are directly learned by adopting a bit by bit method. To the best of our\nknowledge, ADSH is the ﬁrst deep supervised hashing method which treats query points and database\npoints in an asymmetric way. (2). The training of ADSH is much more efﬁcient than that of traditional\nsymmetric deep supervised hashing methods. Hence, the whole set of database points can be used for\ntraining even if the database is large. (3). ADSH can directly learn the binary hash codes for database\npoints, which will be empirically proved to be more accurate than the strategies adopted by traditional\nsymmetric deep supervised hashing methods which use the learned hash function to generate hash\ncodes for database points. (4). Experiments on two large-scale datasets show that ADSH can achieve\nstate-of-the-art performance in real applications.\n2\nNotation and Problem Deﬁnition\n2.1\nNotation\nBoldface lowercase letters like b denote vectors, and boldface uppercase letters like B denote matrices.\nB∗j denotes the jth column of B. Bij denotes the (i, j)th element of matrix B. Furthermore, ∥B∥F\nand BT are used to denote the Frobenius norm and the transpose of matrix B, respectively. The\nsymbol ⊙is used to denote the Hadamard product. We use I(·) to denote an indicator function, i.e.,\nI(true) = 1 and I(false) = 0.\n2.2\nProblem Deﬁnition\nFor supervised hashing methods, supervised information can be point-wise labels [24], pairwise\nlabels [9, 14, 16] or triplet labels [28, 32]. In this paper, we only focus on pairwise-label based\nsupervised hashing which is a common application scenario.\nAssume that we have m query data points which are denoted as X = {xi}m\ni=1 and n database points\nwhich are denoted as Y = {yj}n\nj=1. Furthermore, pairwise supervised information, denoted as\nS ∈{−1, +1}m×n, is also available for supervised hashing. If Sij = 1, it means that point xi and\npoint yj are similar. Otherwise, xi and yj are dissimilar. The goal of supervised hashing is to learn\nbinary hash codes for both query points and database points from X, Y and S, and the hash codes\ntry to preserve the similarity between query points and database points. More speciﬁcally, if we\nuse U = {ui}m\ni=1 ∈{−1, +1}m×c and V = {vj}n\nj=1 ∈{−1, +1}n×c to denote the learned binary\nhash codes for query points and database points respectively, the Hamming distance distH(ui, vj)\nshould be as small as possible if Sij = 1 and vice versa. Here, c denotes the binary code length.\nMoreover, we should also learn a hash function h(xq) ∈{−1, +1}c so that we can generate binary\ncode for any unseen query point xq.\n2\n01011100\n11010101\n10011110\n00010100\n01110100\n…...\nQuery image\nDatabase code\nQuery code\nSimilarity information\nV\nAsymmetric \npairwise loss\n... ...\nFeature Learning Part\nLoss Function Part\nCNN-F model\nU\n(x;\n)\nh\n\nFigure 1: Model architecture of ADSH.\nPlease note that in many cases, we are only given a set of database points Y = {yj}n\nj=1 and the\npairwise supervised information between them. We can also learn the hash codes and hash function\nby sampling a subset or the whole set of Y as the query set for training. In these cases, X ⊆Y.\n3\nAsymmetric Deep Supervised Hashing\nIn this section, we introduce our asymmetric deep supervised hashing (ADSH) in detail, including\nmodel formulation and learning algorithm.\n3.1\nModel Formulation\nFigure 1 shows the model architecture of ADSH, which contains two important components: feature\nlearning part and loss function part. The feature learning part tries to learn a deep neural network\nwhich can extract appropriate feature representation for binary hash code learning. The loss function\npart aims to learn binary hash codes which preserve the supervised information (similarity) between\nquery points and database points. ADSH integrates these two components into the same end-to-end\nframework. During training procedure, each part can give feedback to the other part.\nPlease note that the feature learning is only performed for query points but not for database points.\nFurthermore, based on the deep neural network for feature learning, ADSH adopts a deep hash\nfunction to generate hash codes for query points, but the binary hash codes for database points are\ndirectly learned. Hence, ADSH treats the query points and database points in an asymmetric way.\nThis asymmetric property of ADSH is different from traditional deep supervised hashing methods.\nTraditional deep supervised hashing methods adopt the same deep neural network to perform feature\nlearning for both query points and database points, and then use the same deep hash function to\ngenerate binary codes for both query points and database points.\n3.1.1\nFeature Learning Part\nIn this paper, we adopt a convolutional neural network (CNN) model from [4], i.e., CNN-F model,\nto perform feature learning. This CNN-F model has also been adopted in DPSH [14] for feature\nlearning. The CNN-F model contains ﬁve convolutional layers and three fully-connected layers, the\ndetails of which can be found at [4, 14]. In ADSH, the last layer of CNN-F model is replaced by a\nfully-connected layer which can project the output of the ﬁrst seven layers into Rc space. Please note\nthat the framework of ADSH is general enough to adopt other deep neural networks to replace the\nCNN-F model for feature learning. Here, we just adopt CNN-F for illustration.\n3.1.2\nLoss Function Part\nTo learn the hash codes which can preserve the similarity between query points and database points,\none common way is to minimize the L2 loss between the supervised information (similarity) and\n3\ninner product of query-database binary code pairs. This can be formulated as follows:\nmin\nU,V J(U, V) =\nm\nX\ni=1\nn\nX\nj=1\n\u0000uT\ni vj −cSij\n\u00012\n(1)\ns.t.\nU ∈{−1, +1}m×c, V ∈{−1, +1}n×c, ui = h(xi) , ∀i ∈{1, 2, . . . , m}.\nHowever, it is difﬁcult to learn h(xi) due to the discrete output. We can set h(xi) = sign(F(xi; Θ)),\nwhere F(xi; Θ) ∈Rc. Then, the problem in (1) is transformed to the following problem:\nmin\nΘ,V J(Θ, V) =\nm\nX\ni=1\nn\nX\nj=1\n\u0002\nsign(F(xi; Θ))T vj −cSij\n\u00032,\ns.t.\nV ∈{−1, +1}n×c.\n(2)\nIn (2), we set F(xi; Θ) to be the output of the CNN-F model in the feature learning part, and Θ is the\nparameter of the CNN-F model. Through this way, we seamlessly integrate the feature learning part\nand the loss function part into the same framework.\nThere still exists a problem for the formulation in (2), which is that we cannot back-propagate the\ngradient to Θ due to the sign(F(xi; Θ)) function. Hence, in ADSH we adopt the following objective\nfunction:\nmin\nΘ,V J(Θ,V) =\nm\nX\ni=1\nn\nX\nj=1\n\u0002\ntanh(F(xi; Θ))T vj −cSij\n\u00032,\ns.t.\nV ∈{−1, +1}n×c\n(3)\nwhere we use tanh(·) to approximate the sign(·) function.\nIn practice, we might be given only a set of database points Y = {yj}n\nj=1 without query points. In\nthis case, we can randomly sample m data points from database to construct the query set. More\nspeciﬁcally, we set X = YΩwhere YΩdenotes the database points indexed by Ω. Here, we use\nΓ = {1, 2, . . . , n} to denote the indices of all the database points and Ω= {i1, i2, . . . , im} ⊆Γ\nto denote the indices of the sampled query points. Accordingly, we set S = SΩ, where S ∈\n{−1, +1}n×n denotes the supervised information (similarity) between pairs of all database points\nand SΩ∈{−1, +1}m×n denotes the sub-matrix formed by the rows of S indexed by Ω. Then, we\ncan rewrite J(Θ, V) as: J(Θ, V) = P\ni∈Ω\nP\nj∈Γ\n\u0002\ntanh(F(yi; Θ))T vj −cSij\n\u00032.\nBecause Ω⊆Γ, there are two representations for yi, ∀i ∈Ω. One is the binary hash code vi in\ndatabase, and the other is the query representation tanh(F(yi; Θ)). We add an extra constraint to\nkeep vi and tanh(F(yi; Θ)) as close as possible, ∀i ∈Ω. This is intuitively reasonable, because\ntanh(F(yi; Θ)) is the approximation of the binary code of yi. Then we get the ﬁnal formulation of\nADSH with only database points Y for training:\nmin\nΘ,V J(Θ, V) =\nX\ni∈Ω\nX\nj∈Γ\n\u0002\ntanh(F(yi; Θ))T vj −cSij\n\u00032 + γ\nX\ni∈Ω\n[vi −tanh(F(yi; Θ))]2\ns.t.\nV ∈{−1, +1}n×c\n(4)\nwhere γ is a hyper-parameter.\nIn real applications, if we are given both Y and X, we use the problem in (3) for training ADSH. If\nwe are only given Y, we use the problem in (4) for training ADSH. After training ADSH, we can get\nthe binary hash codes for database points, and a deep hash function for query points. We can use the\ntrained deep hash function to generate the binary hash codes for any query points including newly\ncoming query points which are not seen during training. One simple way to generate binary codes for\nquery points is to set uq = h(xq) = sign(F(xq; Θ)).\nFrom (3) and (4), we can ﬁnd that ADSH treats query points and database points in an asymmetric\nway. More speciﬁcally, the feature learning is only performed for query points but not for database\npoints. Furthermore, ADSH adopts a deep hash function to generate hash codes for query points, but\nthe binary hash codes for database points are directly learned. This is different from traditional deep\nsupervised hashing methods which adopt the same deep hash function to generate binary hash codes\nfor both query points and database points. Because m ≪n in general, ADSH can learn the deep\nneural networks efﬁciently, and is much faster than traditional symmetric deep supervised hashing\nmethods. This will be veriﬁed in our experiments.\n4\n3.2\nLearning Algorithm\nHere, we only present the learning algorithm for problem (4), which can be easily adapted for\nproblem (3). We design an alternating optimization strategy to learn the parameters Θ and V in\nproblem (4). More speciﬁcally, in each iteration we learn one parameter with the other ﬁxed, and this\nprocess will be repeated for many iterations.\n3.2.1\nLearn Θ with V ﬁxed\nWhen V is ﬁxed, we use back-propagation (BP) algorithm to update the neural network parameter Θ.\nSpeciﬁcally, we sample a mini-batch of the query points, then update the parameter Θ based on the\nsampled data. For the sake of simplicity, we deﬁne zi = F(yi; Θ) and eui = tanh(F(yi; Θ)). Then\nwe can compute the gradient of zi as follows:\n∂J\n∂zi\n=2\nn X\nj∈Γ\n\u0002\n(euT\ni vj −cSij)vj\n\u0003\n+ 2γ(eui −vi)\no\n⊙(1 −eu2\ni )\n(5)\nThen we can use chain rule to compute ∂J\n∂Θ based on ∂J\n∂zi , and the BP algorithm is used to update Θ.\n3.2.2\nLearn V with Θ ﬁxed\nWhen Θ is ﬁxed, we rewrite the problem (4) in matrix form:\nmin\nV\nJ(V) = ∥eUVT −cS∥2\nF + γ∥VΩ−eU∥2\nF\ns.t.\nV ∈{−1, +1}n×c,\n(6)\nwhere eU = [eui1, eui2, . . . , euim]T ∈[−1, +1]m×c, VΩdenotes the binary codes for the database\npoints indexed by Ω, i.e., VΩ= [vi1, vi2, . . . , vim]T .\nWe deﬁne ¯U = {¯uj}n\nj=1, where ¯uj is deﬁned as: ¯uj = I(j ∈Ω) · euj + I(j /∈Ω) · 0. Then we can\nrewrite the problem (6) as follows:\nmin\nV\nJ(V) =∥V eUT ∥2\nF −2tr\n\u0000V[c eUT S + γ ¯UT ]\n\u0001\n+ const = ∥V eUT ∥2\nF + tr(VQT ) + const\ns.t.\nV ∈{−1, +1}n×c\n(7)\nwhere Q = −2cST eU −2γ ¯U, const is a constant independent of V.\nThen, we update V bit by bit. That is to say, each time we update one column of V with other\ncolumns ﬁxed. Let V∗k denote the kth column of V and bVk denote the matrix of V excluding V∗k.\nLet Q∗k denote the kth column of Q and bQk denote the matrix of Q excluding Q∗k. Let eU∗k denote\nthe kth column of eU and bUk denote the matrix of eU excluding eU∗k. To optimize V∗k, we can get\nthe objective function: J(V∗k) = ∥V eUT ∥2\nF +tr(VQT )+const = tr\n\u0000V∗k[2 eUT\n∗k bUk bVT\nk +QT\n∗k]\n\u0001\n+\nconst. Then, we need to solve the following problem:\nmin\nV∗k J(V∗k) = tr(V∗k[2 eUT\n∗k bUk bVT\nk + QT\n∗k]\n\u0001\n+ const\ns.t.\nV∗k ∈{−1, +1}n.\n(8)\nThen, we can get the optimal solution of problem (8) as follows:\nV∗k = −sign(2 bVk bUT\nk eU∗k + Q∗k),\n(9)\nwhich can be used to update V∗k.\nWe summarize the whole learning algorithm for ADSH in Algorithm 1. Here, we repeat the learning\nfor several times, and each time we can sample a query set indexed by Ω.\n3.3\nOut-of-Sample Extension\nAfter training ADSH, the learned deep neural network can be applied for generating binary codes\nfor query points including unseen query points during training. More speciﬁcally, we can use the\nequation: uq = h(xq; Θ) = sign(F(xq; Θ)), to generate binary code for xq.\n5\nAlgorithm 1 The learning algorithm for ADSH\nInput: Y = {yi}n\ni=1: n data points. S ∈{−1, 1}n×n: similarity matrix. c: code length.\nOutput: V: binary hash codes for database points. Θ: neural network parameter.\nInitialization: initialize Θ, V, mini-batch size M and iteration number Tout, Tin.\nfor w = 1 →Tout do\nRandomly generate index set Ωfrom Γ. Set S = SΩ, X = YΩbased on Ω.\nfor t = 1 →Tin do\nfor s = 1, 2, . . . , m/M do\nRandomly sample M data points from X = YΩto construct a mini-batch.\nCalculate zi and eui for each data point yi in the mini-batch by forward propagation.\nCalculate the gradient according to (5).\nUpdate the parameter Θ by using back propagation.\nend for\nfor k = 1 →c do\nUpdate V∗k according to update rule in (9).\nend for\nend for\nend for\n3.4\nComplexity Analysis\nThe total computational complexity for training ADSH is O(ToutTin[(n + 2)mc + (m + 1)nc2 +\n(c(n+m)−m)c]). In practice, Tout, Tin, m and c will be much less than n. Hence, the computational\ncost of ADSH is O(n). For traditional symmetric deep supervised hashing methods, if all database\npoints are used for training, the computational cost is at least O(n2). Furthermore, the training for\ndeep neural network is typically time-consuming. For traditional symmetric deep supervised hashing\nmethods, they need to scan n points in an epoch of the neural network training. On the contrary,\nonly m points are scanned in an epoch of the neural network training for ADSH. Typically, m ≪n.\nHence, ADSH is much faster than traditional symmetric deep supervised hashing methods.\nTo make the training practicable, most existing symmetric deep supervised hashing methods have to\nsample only a small subset from the whole database to construct a training set for deep hash function\nlearning, and many points in database may be discarded during training. On the contrary, ASDH is\nmuch more efﬁcient to utilize more database points for training.\n4\nExperiment\nWe carry out experiments to evaluate our ADSH and baselines which are implemented with the deep\nlearning toolbox MatConvNet [26] on a NVIDIA K40 GPU server.\n4.1\nDatasets\nWe evaluate ADSH on two widely used datasets: CIFAR-10 [10] and NUS-WIDE [5].\nThe CIFAR-10 dataset is a single-label dataset which contains 60,000 32 × 32 color images. Each\nimage belongs to one of the ten classes. Two images will be treated as a ground-truth neighbor (similar\npair) if they share one common label.\nThe NUS-WIDE dataset is a multi-label dataset which consists of 269,648 web images associated with\ntags. Following the setting of DPSH [14], we only select 195,834 images that belong to the 21 most\nfrequent concepts. For NUS-WIDE, two images will be deﬁned as a ground-truth neighbor (similar\npair) if they share at least one common label.\n4.2\nBaselines and Evaluation Protocol\nTo evaluate our ADSH, ten start-of-the-art methods, including ITQ [7], Lin:Lin [20], Lin:V [20],\nLFH [31], FastH [15], SDH [24], COSDISH [9], DSH [16], DHN [33] and DPSH [14], are selected\nas baselines for comparison.\n6\nTable 1: MAP comparison to baselines. The best accuracy is shown in boldface.\nMethod\nCIFAR-10\nNUS-WIDE\n12 bits\n24 bits\n32 bits\n48 bits\n12 bits\n24 bits\n32 bits\n48 bits\nITQ\n0.2611\n0.2734\n0.2865\n0.2957\n0.6785\n0.7020\n0.7104\n0.7204\nLin:Lin\n0.6173\n0.6329\n0.6166\n0.5918\n0.6464\n0.6446\n0.6278\n0.6295\nFastH\n0.6111\n0.6713\n0.6874\n0.7065\n0.7506\n0.7785\n0.7853\n0.7993\nLFH\n0.4342\n0.5988\n0.6551\n0.6745\n0.7362\n0.7760\n0.7896\n0.8020\nSDH\n0.4866\n0.6328\n0.6457\n0.6539\n0.7550\n0.7723\n0.7727\n0.7830\nCOSDISH\n0.6048\n0.6684\n0.6871\n0.7068\n0.7418\n0.7738\n0.7895\n0.7986\nDSH\n0.6370\n0.7493\n0.7803\n0.8086\n0.7650\n0.7778\n0.7822\n0.7844\nDHN\n0.6794\n0.7201\n0.7309\n0.7408\n0.7598\n0.7894\n0.7960\n0.8021\nDPSH\n0.6863\n0.7276\n0.7406\n0.7520\n0.7895\n0.8083\n0.8138\n0.8216\nADSH\n0.8466\n0.9062\n0.9175\n0.9263\n0.8570\n0.8939\n0.9008\n0.9074\nFor non-deep hashing methods, we utilize 4,096-dim deep features which are extracted by the pre-\ntrained CNN-F model on ImageNet dataset for fair comparison. For FastH, LFH and COSDISH, we\nuse boosted decision tree for out-of-sample extension by following the setting of FastH. For DSH\nand DHN, although the authors provide source code, for fair comparison we carefully re-implement\ntheir methods on MatConvNet to remove effect on training time caused by different platforms. For\ndeep hashing methods, we resize all images to 224 × 224 and use the raw pixels as the inputs\nfor all datasets. In addition, some deep hashing methods adopt other neural networks for feature\nlearning. We ﬁnd that the deep baselines with CNN-F network can outperform the counterparts with\nthe original networks (refer to Appendix C in the supplementary material). For fair comparison,\nwe adopt the same deep neural networks for all deep hashing methods, i.e., the CNN-F network.\nWe initialize CNN-F with the pre-trained model on ImageNet. Following the suggestions of the\nauthors, we set the mini-batch size to be 128 and tune the learning rate among [10−6, 10−2]. For\nADSH method, we set γ = 200, Tout = 50 by using a validation strategy. Furthermore, we set\nTin = 3, 5, |Ω| = 1000, 2000 for CIFAR-10 and NUS-WIDE, respectively 1. To avoid effect caused\nby class-imbalance problem between positive and negative similarity information, inspired by [13],\nwe empirically set the weight of the element -1 in S as the ratio between the number of element 1 and\nthe number of element -1 in S.\nFor CIFAR-10 dataset, we randomly select 1,000 images for validation set and 1,000 images for test\nset, with the remaining images as database points. For NUS-WIDE dataset, we randomly choose\n2,100 images as validation set and 2,100 images as test set, with the rest of the images as database\npoints. Because the deep hashing baselines are very time-consuming for training, similar to existing\nworks like [14] we randomly sample 5,000 (500 per class) and 10,500 images from database for\ntraining all baselines except Lin:V for CIFAR-10 and NUS-WIDE, respectively. The necessity of\nrandom sampling for training set will also be empirically veriﬁed in Section 4.4.\nWe report Mean Average Precision (MAP), Top-k precision curve to evaluate ADSH and baselines.\nFor NUS-WIDE dataset, the MAP results are calculated based on the top-5000 returned samples. We\nalso compare the training time between different deep hashing methods. Furthermore, we also report\nthe precision-recall curve and case study, which are moved to the supplementary material due to the\nspace limitation. All experiments are run ﬁve times, and the average values are reported.\n4.3\nAccuracy\nThe MAP results are presented in Table 1. We can ﬁnd that in most cases the supervised methods can\noutperform the unsupervised methods, and the deep methods can outperform the non-deep methods.\nFurthermore, we can ﬁnd that ADSH can outperform all the other baselines, including deep hashing\nbaselines, non-deep supervised hashing baselines and unsupervised hashing baselines.\nSome baselines, including Lin:Lin, LFH, SDH, COSDISH, can also be adapted to learn binary hash\ncodes for database directly due to their training efﬁciency. We also carry out experiments to evaluate\nthe adapted counterparts of these methods which can learn binary codes for database directly, and\ndenote the counterparts of these methods as Lin:V, LFH-D, SDH-D, COSDISH-D, respectively. It\nalso means that Lin:V, LFH-D, SDH-D, COSDISH-D adopt all database points for training. We report\n1We report the effect of hyper-parameters γ and |Ω| in Appendix D of the supplementary materials.\n7\nTable 2: MAP comparison to baselines which can directly learn the binary hash codes for database\npoints. The best accuracy is shown in boldface.\nMethod\nCIFAR-10\nNUS-WIDE\n12 bits\n24 bits\n32 bits\n48 bits\n12 bits\n24 bits\n32 bits\n48 bits\nLin:V\n0.8197\n0.8011\n0.8055\n0.8021\n0.7762\n0.7756\n0.7806\n0.7820\nLFH-D\n0.4743\n0.6384\n0.7598\n0.8142\n0.7954\n0.8393\n0.8584\n0.8721\nSDH-D\n0.7115\n0.8297\n0.8314\n0.8336\n0.8478\n0.8683\n0.8647\n0.8670\nCOSDISH-D\n0.8431\n0.8666\n0.8738\n0.8724\n0.8031\n0.8448\n0.8581\n0.8670\nADSH\n0.8466\n0.9062\n0.9175\n0.9263\n0.8570\n0.8939\n0.9008\n0.9074\n12\n24\n32\n48\nBinary code length\n0.45\n0.55\n0.65\n0.75\n0.85\n0.95\nTop-2000 precision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(a) CIFAR-10\n12\n24\n32\n48\nBinary code length\n0.7\n0.75\n0.8\n0.85\n0.9\nTop-2000 precision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(b) NUS-WIDE\nFigure 2: Top-2000 precision on two datasets.\n5K\n8K\n10K\n20K\n58K\n#Training size\n01\n5\n10\n15\n20\nTraining time (in hour)\nDSH\nDHN\nDPSH\nADSH\n(a) CIFAR-10\n12345\n10\n15\n20\n25\ntime (in hour)\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nMAP\nADSH\nDSH\nDSH-D\nDPSH\nDPSH-D\nDHN\nDHN-D\n(b) NUS-WIDE\nFigure 3: Training time on two datasets.\nthe corresponding MAP results in Table 2. We can ﬁnd that Lin:V, LFH-D, SDH-D, COSDISH-D\ncan outperform Lin:Lin, LFH, SDH, COSDISH, respectively. This means that directly learning the\nbinary hash codes for database points is more accurate than the strategies which use the learned hash\nfunction to generate hash codes for database points. We can also ﬁnd that ADSH can outperform all\nthe other baselines.\nWe also report top-2000 precision in Figure 2 on two datasets. Once again, we can ﬁnd that ADSH\ncan signiﬁcantly outperform other baselines in all cases especially for large code length.\n4.4\nTime Complexity\nWe compare the training time of ADSH to that of other deep supervised hashing baselines on CIFAR-\n10 by changing the number of training points. The results are shown in Figure 3 (a). We can ﬁnd\nthat ADSH is much faster than other deep hashing methods in all cases. We can also ﬁnd that as\nthe number of training points increases, the computation cost for traditional deep hashing baselines\nincreases dramatically. On the contrary, the computation cost for ADSH increases slowly as the size\nof training set increases. For example, we can ﬁnd that ADSH with 58,000 training points is still\nmuch faster than all the other deep baselines with 5,000 training points. Hence, we can ﬁnd that\nADSH can achieve higher accuracy with much faster training speed.\nFurthermore, we compare ADSH to deep hashing baselines by adopting the whole database as the\ntraining set on NUS-WIDE with 12 bits. The results are shown in Figure 3 (b). Here, DSH, DHN and\nDPSH denote the deep hashing baselines with 10,500 sampled points for training. DSH-D, DHN-D\nand DPSH-D denote the counterparts of the corresponding deep hashing baselines which adopt the\nwhole database for training. We can ﬁnd that to achieve similar accuracy (MAP), DSH, DHN and\nDPSH need much less time than their counterparts with whole database for training. Moreover, if the\nwhole database is used for training, it need more than 10 hours for most baselines to converge for the\ncase of 12 bits. For longer code with more bits, the time cost will be even higher. Hence, we have to\nsample a subset for training. From Figure 3 (b), we can also ﬁnd that to achieve similar accuracy, our\nADSH is much faster than all the baselines, either with sampled training points or with the whole\ndatabase. In addition, ADSH can achieve a higher accuracy than all baselines with much less time.\n5\nConclusion\nIn this paper, we propose a novel deep supervised hashing method, called ADSH, for large-scale\nnearest neighbor search. To the best of our knowledge, this is the ﬁrst work to adopt an asymmetric\nstrategy for deep supervised hashing. Experiments show that ADSH can achieve the state-of-the-art\nperformance in real applications.\n8\nReferences\n[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high\ndimensions. Commun. ACM, 51(1):117–122, 2008.\n[2] A. Andoni, P. Indyk, T. Laarhoven, I. P. Razenshteyn, and L. Schmidt. Practical and optimal LSH for\nangular distance. In NIPS, 2015.\n[3] M. Á. Carreira-Perpiñán and R. Raziperchikolaei. Hashing with binary autoencoders. In CVPR, 2015.\n[4] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep\ninto convolutional nets. In BMVC, 2014.\n[5] T. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng. NUS-WIDE: a real-world web image database\nfrom national university of singapore. In CIVR, 2009.\n[6] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable\ndistributions. In SCG, 2004.\n[7] Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. In\nCVPR, 2011.\n[8] A. Gordo and F. Perronnin. Asymmetric distances for binary embeddings. In CVPR, 2011.\n[9] W. Kang, W. Li, and Z. Zhou. Column sampling based discrete supervised hashing. In AAAI, 2016.\n[10] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of\nToronto, 2009.\n[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In NIPS, 2012.\n[12] B. Kulis and T. Darrell. Learning to hash with binary reconstructive embeddings. In NIPS, 2009.\n[13] C. Leng, J. Cheng, J. Wu, X. Zhang, and H. Lu. Supervised hashing with soft constraints. In CIKM, 2014.\n[14] W. Li, S. Wang, and W. Kang. Feature learning based deep supervised hashing with pairwise labels. In\nIJCAI, 2016.\n[15] G. Lin, C. Shen, Q. Shi, A. van den Hengel, and D. Suter. Fast supervised hashing with decision trees for\nhigh-dimensional data. In CVPR, 2014.\n[16] H. Liu, R. Wang, S. Shan, and X. Chen. Deep supervised hashing for fast image retrieval. In CVPR, 2016.\n[17] W. Liu, C. Mu, S. Kumar, and S. Chang. Discrete graph hashing. In NIPS, 2014.\n[18] W. Liu, J. Wang, R. Ji, Y. Jiang, and S. Chang. Supervised hashing with kernels. In CVPR, 2012.\n[19] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing with graphs. In ICML, 2011.\n[20] B. Neyshabur, N. Srebro, R. Salakhutdinov, Y. Makarychev, and P. Yadollahpour. The power of asymmetry\nin binary hashing. In NIPS, 2013.\n[21] M. Norouzi and D. J. Fleet. Minimal loss hashing for compact binary codes. In ICML, 2011.\n[22] M. Norouzi, D. J. Fleet, and R. Salakhutdinov. Hamming distance metric learning. In NIPS, 2012.\n[23] R. Raziperchikolaei and M. Á. Carreira-Perpiñán. Optimizing afﬁnity-based binary hashing using auxiliary\ncoordinates. In NIPS, 2016.\n[24] F. Shen, C. Shen, W. Liu, and H. T. Shen. Supervised discrete hashing. In CVPR, 2015.\n[25] A. Shrivastava and P. Li. Densifying one permutation hashing via rotation for fast near neighbor search. In\nICML, 2014.\n[26] A. Vedaldi and K. Lenc. Matconvnet: Convolutional neural networks for MATLAB. In MM, 2015.\n[27] J. Wang, S. Kumar, and S. Chang. Sequential projection learning for hashing with compact codes. In\nICML, 2010.\n[28] J. Wang, W. Liu, A. X. Sun, and Y. Jiang. Learning hash codes with listwise supervision. In ICCV, 2013.\n[29] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In NIPS, 2008.\n[30] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan. Supervised hashing for image retrieval via image representation\nlearning. In AAAI, 2014.\n[31] P. Zhang, W. Zhang, W. Li, and M. Guo. Supervised hashing with latent factor models. In SIGIR, 2014.\n[32] F. Zhao, Y. Huang, L. Wang, and T. Tan. Deep semantic ranking based hashing for multi-label image\nretrieval. In CVPR, 2015.\n[33] H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing network for efﬁcient similarity retrieval. In AAAI,\n2016.\n9\nA\nPrecision-Recall Curve\nTypically, there are two retrieval protocols for hashing-base search [17]. One is called Hamming\nranking, and the other is called hash lookup. The MAP metric and top-k precision measure the\naccuracy of Hamming ranking. For the hash lookup protocol, precision-recall curve is usually adopted\nfor measuring the accuracy.\nWe report precision-recall curve on two datasets in Figure 4. Each marked point in the curve\ncorresponds to a Hamming radius to the binary hash code of the query. We can ﬁnd that in most cases\nADSH can achieve the best performance on both datasets.\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(a) 12 bits @CIFAR-10\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(b) 24 bits @CIFAR-10\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(c) 32 bits @CIFAR-10\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(d) 48 bits @CIFAR-10\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(e) 12 bits @NUS-WIDE\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(f) 24 bits @NUS-WIDE\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(g) 32 bits @NUS-WIDE\n0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nLin:V\nLFH-D\nSDH-D\nCOSDISH-D\nDSH\nDHN\nDPSH\nADSH\n(h) 48 bits @NUS-WIDE\nFigure 4: Precision recall curve on two dataset.\nB\nThe Effectiveness of CNN Network Structure\nSince we change the network structure of DHN and DSH for fair comparison, we compare the\nretrieval accuracy on CIFAR-10 dataset with different network structures. The results are reported in\nTable 3.\nWe can see that the methods with CNN-F can achieve higher accuracy than the methods with the\noriginal network structures. For fair comparison, we utilize CNN-F as the network structure for all\ndeep hashing methods.\nC\nSensitity to the Hyper-parameters\nFigure 5 presents the effect of the hyper-parameter γ and the number of query points (m or |Ω|) for\nADSH on CIFAR-10, with binary code length being 24 bits and 48 bits. From Figure 5 (a), we can\nsee that ADSH is not sensitive to γ in a large range with 10−2 < γ < 103.\nFigure 5 (b) presents the MAP results for different number of sampled query points (m) for ADSH on\nCIFAR-10. We can ﬁnd that better retrieval accuracy can be achieved with larger number of sampled\nquery points. Because larger number of sampled query points will result in higher computation cost,\nin our experiments we select a suitable number to get a tradeoff between retrieval accuracy and\ncomputation cost. By choosing m = 1000, our ADSH can signiﬁcantly outperform all other deep\nsupervised hashing baselines in terms of both accuracy and efﬁciency.\nD\nCase Study\nWe randomly sample some test data points and show their top-10 returned results from retrieval set\nfor each test point as a case study on CIFAR-10. For ADSH and the baselines, we use the same test\npoints to show the results.\n10\nTable 3: MAP on CIFAR-10 dataset.\nMethod\nNetwork\n12 bits\n24 bits\n32 bits\n48 bits\nDHN\nAlexNet\n0.5512\n0.6091\n0.6114\n0.6433\nDHN\nCNN-F\n0.5978\n0.6399\n0.6471\n0.6870\nDSH\nProposed\n0.6157\n0.6512\n0.6607\n0.6755\nDSH\nCNN-F\n0.6036\n0.6829\n0.7245\n0.7657\n10-2\n10-1\n1\n101\n102\n103\n.\n0.85\n0.87\n0.89\n0.9\n0.91\n0.93\n0.95\nMAP\n24 bits\n48 bits\n(a) γ\n100\n500\n8001000\n1500\n2000\n#Query Samples\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\nMAP\n24 bits\n48 bits\n(b) #query points\nFigure 5: Hyper-parameters on CIFAR-10 datasets.\nFigure 6 shows the returned results with 12 bits. We use a red rectangle to indicate that the returned\nimage is not the ground-truth neighbor of the corresponding test image. By comparing ADSH to\nthree best baselines, we can see that ADSH can signiﬁcantly outperform other baselines.\n(a) ADSH\n(b) LFH-D\n(c) SDH-D\n(d) COSDISH-D\nFigure 6: Case study with 12 bits on CIFAR-10 dataset. For each sub-ﬁgure, the ﬁrst column denotes\nthe test images. The following 10 columns denote the top-10 returned samples. A red rectangle is\nused to indicate that the returned image is not the ground-truth neighbor of the corresponding test\nimage.\n11\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2017-07-26",
  "updated": "2017-07-26"
}