{
  "id": "http://arxiv.org/abs/1707.09798v1",
  "title": "Unsupervised Visual Attribute Transfer with Reconfigurable Generative Adversarial Networks",
  "authors": [
    "Taeksoo Kim",
    "Byoungjip Kim",
    "Moonsu Cha",
    "Jiwon Kim"
  ],
  "abstract": "Learning to transfer visual attributes requires supervision dataset.\nCorresponding images with varying attribute values with the same identity are\nrequired for learning the transfer function. This largely limits their\napplications, because capturing them is often a difficult task. To address the\nissue, we propose an unsupervised method to learn to transfer visual attribute.\nThe proposed method can learn the transfer function without any corresponding\nimages. Inspecting visualization results from various unsupervised attribute\ntransfer tasks, we verify the effectiveness of the proposed method.",
  "text": "Unsupervised Visual Attribute Transfer with\nReconﬁgurable Generative Adversarial Networks\nTaeksoo Kim, Byoungjip Kim, Moonsu Cha, Jiwon Kim\nSK T-Brain\n{jazzsaxmafia,bjkim,ckanstnzja,jk}@sktbrain.com\nAbstract\nLearning to transfer visual attributes requires supervision dataset. Corresponding\nimages with varying attribute values with the same identity are required for learning\nthe transfer function. This largely limits their applications, because capturing them\nis often a difﬁcult task. To address the issue, we propose an unsupervised method\nto learn to transfer visual attribute. The proposed method can learn the transfer\nfunction without any corresponding images. Inspecting visualization results from\nvarious unsupervised attribute transfer tasks, we verify the effectiveness of the\nproposed method.\n1\nIntroduction\nWhat would you look like if you grow bang hair, dye your hair blond, or change your gender? Recent\nimage-to-image translation research [1, 2, 3] can be the solutions for these demands. By learning a\nﬁxed mapping function from a source domain to a target domain, it can make one have bang hair or\nchange her hair color into a speciﬁc color. More recently, image-to-image translation with unpaired\ndataset [2, 3] has been researched, getting rid of the necessity of tedious data collection.\nHowever, what if you do not just want a bang hair, but exactly the one that Uma Thurman had in\nthe movie Pulp Fiction? What about having the exact smile of Mona Risa? This is the part where\nimage-to-image translation cannot help due to its nature of treating the whole domain instead of each\ninstance.\nOn the other hand, researches on image stylization [4, 5, 6, 7, 8], that address instance-level visual\nattribute transfer have drawn signiﬁcant amount of attention recently. Despite the remarkable results\nof these works, most of these works are limited to holistic attribute transfer. For example, they transfer\ntexture, color or style that cover the whole image but not speciﬁc part of an image like smile or hair\ncolor.\nWe therefore seek a visual attribute transfer mechanism that is able to transfer certain part of an image\ninstance-level, without paired dataset. First, we formally deﬁne attributes as high-level features of\na dataset that are independent to each other, and can be disentangled in well-deﬁned feature space\n(hair color, bangs). We further denote attribute values as set of possible values for each attribute that\ncan be easily labeled (black/blond hair, with/without bangs), and domain as the set of data that have\nlabeled as certain attribute value (images of people with bangs). Each element in an attribute domain\nis unique in its details (every bang hair is different) so that instance level annotation is infeasible.\nSince we have only access to domain-level labels, we exploit domain transfer methods previously\nproposed by [2, 3] along with an additional objective that enforces instance-level transfer. After\ntransferring the target attribute of a reference image to the source image, in order to encourage\nthe transferred result image to belong to the target attribute domain, we use generative adversarial\nnetwork (GAN) [9] so that the result image is indistinguishable from the images in target domain.\nIn order to ensure the remaining non-target attributes are kept intact, we introduce back-transfer\nobjective inspired by [2, 3]. The target attribute of the source image is transferred back to the result\narXiv:1707.09798v1  [cs.CV]  31 Jul 2017\nimage, and it is forced to be the same as the original source image. Furthermore, in order to force\ninstance-level transfer, we introduce attribute-consistent objective: we transfer the target attribute\nof the result image back to the reference image, and encourage this to be the same as the original\nreference image. This objective pushes the result image to have the exact instance-level attribute\ndetails of the reference image.\nOur proposed framework has three distinctive features: 1) instead of changing an attribute of the\nsource image using a ﬁxed mapping function like image-to-image translation does, it can transfer the\ninstance-level attribute. Our method also works well for domain-level transfer, which corresponds\nto translation, as shown in various experiments. 2) It does not require paired dataset, nor densely\nannotated attribute information. By only using domain-level labeled data, our proposed model\nsuccessfully transfers the details of a reference image to the source image. 3) It can be used to change\nmultiple attributes of the source images using a single model, while image-to-image translation based\nmethods require training new model for each.\nThis paper is structured as follows. We summarize the related works in Section 2 and discuss relations\nand differences. In Section 3, we present the architecture and training method of the proposed model.\nOur experiment results are detailed in Section 4. We conclude the paper in Section 5.\n2\nRelated Work\n2.1\nImage-to-Image Translation\nIn computer vision and graphics, the concept of image-to-image translation is used to deal with a set\nof related problems. The goal of image-to-image translation is to learn the mapping from an input\nimage of a source domain A to an output image of a target domain B. Isola et al. [1] proposed a\nframework called pix2pix, which used generative adversarial networks for image-to-image translation.\nThis approach can be categorized as paired image-to-image translation, since the model requires\npaired image data for training. However, obtaining paired training data is usually expensive. To\naddress this issue, unpaired image-to-image translation [2, 3] has been proposed. This approach\nenables to learn the mapping between domains without paired input-output examples. However,\nto generate an output image where multiple effects are applied (e.g., a woman with black hair →\na man with brown hair), this approach requires to build many models for each simple translation.\nWhat differentiates our model is that single model enables such multiplex translations in a shared\nand reconﬁgurable manner. With similar aspects, there are research efforts classiﬁed as conditional\nimage generation. Xinchen Yan et al. [10] proposed a conditional image generation model called\nAttribute2Image by using Variational Auto-encoders (VAEs) [11]. However, this approach requires\ntraining image data that are densely labeled by attributes. In contrast, our model does not require such\ndensely-labeled training data.\n2.2\nGenerative Models\nGenerative models [9, 11, 12, 13, 14] are trained with large amount of data to generate similar data to\nthe training data. More formally, the objective of generative models is to minimize the difference\nbetween data distribution pdata(x) and generation distribution pg(x). In this problem setting, there are\nsome promising generative models including Variational Autoencoders (VAEs) [11] and Generative\nAdversarial Nets (GANs) [9].\nVAEs approach the generation problem by using the framework of probabilistic graphical models,\nand the objective is to maximize a lower bound on the log likelihood of the generation distribution\npθ(x|z).\nEqφ(z|x)[−log(pθ(x|z))] + KL(qφ(z|x)||p(z))\n(1)\nGANs approach the generation problem by using the adversarial learning framework that is considered\nas a game between two separate networks: a generator and a discriminator. The discriminator examines\nwhether samples are real or fake. The generator tries to confuse the discriminator by gradually\nlearning data distribution pdata(x). Since GANs enable to learn a loss function instead of specifying\n2\nEncoder\nGenerator\nGenerator\nEncoder\nG\n𝑳𝒃𝒂𝒄𝒌,𝑩𝒂𝒏𝒈𝒔\n𝒙𝒔𝒓𝒄\nTransfer\nBack-Transfer\n𝑧.\n𝑧/\n𝑧0\n𝑧1\n𝑧.\n 𝑧/ \n𝑧0\n𝑧1\n𝒙𝒃𝒂𝒄𝒌\n𝒙𝒕𝒓𝒂𝒏𝒔\nConverter\nConverter\n𝒙𝒓𝒆𝒇\nE\n𝑧.\n𝑧/\n𝑧0\n𝑧1\nG\nE\n𝑧.\n𝑧/\n𝑧̅0\n𝑧1\n𝑧.\n𝑧/\n𝑧̅0\n𝑧1\n𝑧.\n𝑧/\n𝑧0\n𝑧1\n𝒙𝒂𝒕𝒕𝒓\nD\nDiscriminator 𝑫𝑩𝒂𝒏𝒈𝒔\nReal/Fake\n𝑳𝒂𝒕𝒕𝒓,𝑩𝒂𝒏𝒈𝒔\nSmile/No Smile\nConvert\nBangs/No Bangs\nBlond/Black Hair \nColor\n𝑧.\n𝑧0\n𝑧/\nFigure 1: The network architecture of proposed model. In this ﬁgure, we show an example of bang\nattribute transfer. In transfer phase, the target attribute of the reference image is transferred into\nthe input image, and the result xtrans is encouraged to be indistinguishable from real images with\nbang hair. In back-transfer phase, the target attribute of the result image is transferred back into the\ninput image. In order to enforce xtrans to have the details of the reference image, the target attribute\nof xtrans is transferred into the reference image, which is enforced to be the same as the original\nreference image.\nit explicitly, they can usually generate very realistic data. However, it is known that training GANs is\nrather hard due to the dynamics of the adversarial training.\nmin\nG max\nD V (D, G) = Ex∼pdata(x)[log D(x)] + Ez∼pz(z)[log(1 −D(G(z)))]\n(2)\n2.3\nDisentangled Representations\nDisentangled representations [15, 16] allow conditional generative models that can generate sample\ndata by changing corresponding factor codes. More formally, conditional generative models are\ndescribed by pg(x|z, c) where z is a latent code and c is a disentangled factor code. Very recently,\nresearchers show a possibility that disentangled representations can be learned with GANs. For\nexample, Xi Chen et al. [17] proposed InfoGAN, an extension of GAN that can learn disentangled\nrepresentations for images. To learn disentangled representations (e.g., the width of digits), InfoGAN\nadds new objective that maximizes the mutual information between a factored code and the observa-\ntion. Michael Mathieu et al. [18] proposed a conditional generative model based on conditional VAE.\nThey addressed degeneration problem by adding the GAN objective into the VAE objective.\n3\nFormulation\nThe basic building blocks of our framework are an encoder E, a generator G and discriminator Dv\nfor each attribute value v of total m attribute values. Our model can perform transfer for multiple\nattributes at the expense of one encoder and one generator, along with as many discriminators as the\n3\ntotal number of attribute values. E takes an image x as input and encodes it into attribute features z\nas z = E(x). We assume the attributes can be disentangled in well-deﬁned feature space, so that z\ncan be expressed as a tuple of multiple slots: z = {z1, z2, ...zn}, where n is the number of attributes\nof interest and z1 is reserved as sample-speciﬁc attribute. Note the number of attribute n is different\nfrom the number of attribute values m. G takes the encoded attribute feature and generate an image:\ny = G(z1, z2, ...zn), where y indicates generated image.\nIn order to transfer target attribute from a reference image to the source image, both images xsrc ∼\npdata(x) and xref ∼pv(x) are ﬁrst encoded: zsrc = E(xsrc) and zref = E(xref), where pdata(x)\nis the data distribution of the entire dataset and pv(x) denotes the distribution of the domain of\nattribute value v. We omit the notation for the attribute index for simplicity. The target slot of zsrc\nis then replaced by that of zref, then G takes this attribute features to generate the result image,\nxtrans = G(zsrc\n1\n, zsrc\n2\n, ..., zref\ntar , ..., zsrc\nn )\nIn order for this image to have the target attribute from the reference image, we impose three\nconstraints. First of all, it has to belong to the same target attribute domain as the reference image.\nAlso, it needs to keep other attributes intact after transference. Finally, it should have the exact details\nof the reference image’s target attribute. To encourage these three constraints to be satisﬁed, we\nimpose objective for each constraint. We explain each of objective in detail below.\nTransfer\nTransferring target attribute so that the result image to belong to the corresponding domain\ncan be implemented using GAN, where we denote this objective as transfer objective. Consider the\ntarget attribute corresponds to hair color, and the reference image has black hair as attribute value,\nthen we enforce xtrans to be indistinguishable from images in black hair domain. Transfer objective\nfor the target attribute value is expressed as follows:\nLtrans,v = Ex∼pdata(x)[log Dv(xtrans)]\n(3)\nAlso, the discrimination objective of GAN is:\nLdis,v = Ex∼pdata(x)[log(1 −Dv(xtrans))] + Exref ∼pv(x)[log Dv(xref)].\n(4)\nBack-transfer\nTo encourage all of the remaining non-target attributes to be kept intact, we introduce\nback-transfer objective, inspired by cycle-consistency objective [2, 3]. Transferred image xtrans is\nagain encoded as ztrans = E(xtrans), then the original target slot of zsrc replaces that of ztrans as\nzback = {ztrans\n1\n, ztrans\n2\n, ..., zsrc\ntar, ..., ztrans\nn\n}. We impose the generated image from this feature to be\nthe same as the original source image: xback = G(zback) ≈xsrc.\nLback,v = Exsrc∼pdata(x)[dist(xsrc, xback)]\n(5)\nThis objective enforces all the non-target attributes of transferred image to be the same as those of the\nsource image.\nAttribute consistency\nTraining a model with transfer and back-transfer objective ensures the\ntransferred image to have the target attribute value, while the remaining non-target attributes are kept\nintact. However, these objectives do not ensure the transferred image to have the exact attribute details\nof the reference image. For example, it can have any type of bang, as long as it is indistinguishable\nfrom the images in bang hair domain. We therefore introduce attribute-consistency objective that\nencourages the transference of the details: xattr = G(zref\n1\n, zref\n2\n, ..., ztrans\ntar\n, ..., zref\nn\n) ≈xref. This\ncan be expressed formally as,\nLattr,v = Exref ∼pv(x)[dist(xattr, xtrans)]\n(6)\nThe distance dist can be chosen among any metrics including L1, L2 or Huber. In order this objective\nto be satisﬁed, target attribute feature of the transferred image ztrans\ntar\nneed to encode the details of\nthe target attribute of the reference image.\nFull objective\nThe full transfer objective for attribute value d is:\nLgen,v = λ1Ltrans,v + λ2Lback,v + λ3Lattr,v\n(7)\nwhere λ1, λ2, λ3 are importance weights of each objective. During training, for each attribute value\nv, the parameters of E and G are updated using Lgen,v, and the parameters of Dv are updated using\n4\nLdis,v. More speciﬁcally, each iteration of training is composed of m (total number of attribute\nvalues). In each step, reference images are sampled from v’s domain, then E, G and Dv are updated\nusing Lgen,v and Ldis,v. This procedure is conducted for all m attribute values every iteration. The\narchitecture of our model and training process for a speciﬁc v = Bangs is depicted in Fig. 1.\n4\nExperiments\nIn this section, we evaluate the performance of our proposed method. First, we show the results\nof instance-level attribute transfer for human face images. Speciﬁc facial attributes such as hair\ncolor or smile of reference images are transfered to input images. We also conduct domain-level\nattribute transfer experiments on several tasks: facial attribute, angle/object, and fashion garments\n. Domain-level attribute transfer changes a speciﬁc attribute values using ﬁxed function. It is done\nusing the average attribute vector of the target attribute value instead of the attribute vector of a\nspeciﬁc reference image zref\ntar . This average attribute vector is obtained by averaging the attribute\nvectors of all images in the domain of target attribute value. We then show the experiment results of\n“multiplex” case where the multiple domain-level attributes are transferred simultaneously.\nWhen training models for domain-level transfer tasks, mini-batch average of attribute vectors were\nused instead of full batch average in each iteration. We also set λ3 = 0 in (7) since we are no longer\ntreating single reference image in domain-level task.\n4.1\nInstance-level Attribute Transfer\nWe use the CelebA dataset [19] that consists of images of human face, along with the annotations of\ntheir attributes. We used three attributes: hair color, bang and smiling. For each attribute we used two\nattribute values: blonde/black for hair color, with/without bang for bang and with/without smile for\nsmiling. The results for visual attribute transfer is shown in Fig. 2, 3 and 4. Note that these are results\nof a single model rather than three different models. In each example, the details of the reference\nimage is transferred into the input image.\nFigure 2: Results of smiling attribute transfer. Images with green, red and blue bounding boxes\nindicate source, reference and transferred results, respectfully.\n5\nFigure 3: Results of hair color attribute transfer. Images with green, red and blue bounding boxes\nindicate source, reference and transferred results, respectfully.\nFigure 4: Results of bang hair attribute transfer. Images with green, red and blue bounding boxes\nindicate source, reference and transferred results, respectfully.\n6\n4.2\nDomain-level Attribute Transfer\n4.2.1\nFace\nWe use previously used CelebA dataset for domain-level attribute transfer experiments. We use the\nfollowing visual attributes and their values: hair colors (blond/black/brown), bang (with/without\nbang) and smiling(with/without smile).\nOur results shown in Fig. 5 illustrate that the proposed method successfully transfers desired visual\nimage attributes while maintaining all other attributes. Also, as shown in the t-SNE plot of Fig. 6, the\nencoder learns to separate different attribute values, and the distribution of attributes from the result\nimages closely match that from real images.\n(a) Input\n(b) Hair Color [Black, Brown, Blond]\n(c) Bangs\n(d) Smile\nFigure 5: Domain-level attribute transfer results for CelebA dataset. A speciﬁc attribute of input\nimages (a) are transferred into the target values. Unlike instace-level transfer, where a target attribute\nvector of a reference image is used, an average vector of the attribute value is used for domain-level\ntransfer. (b) Hair color transfer into black, brown and blonde, (c) bang attribute transfer, (d) smile\nattribute transfer.\n4.2.2\nAngle/Object\nWe consider a case where the visual attributes of two image domains differ greatly. To investigate\nwhether our model can cope with this, we combined two rendered 3D image datasets: 3D Car [20]\nand 3D Face [21]. The combined dataset consists of images of human faces and cars that rotate\nwith respect to azimuth angle. We use the object type and angle as visual attributes. The object type\nattribute can take on value of car or face, and the angle attribute takes on either of the three angles:\n-30, 0, 30 degrees. The transfer results are shown in the ﬁgure 7.\n4.2.3\nFashion\nIn order to further investigate the attribute transfer ability of our model, we conduct a fashion item\ntransfer. In this experiment, our model receives an input image of a person wearing speciﬁc tops\n7\nFake\n(-31, 51)\nReal\n(-42, 56)\nFake\n(20, -39)\nReal\n(33, -20)\nReal\n(16, 19)\nFake\n(6, -39)\nReal\n(-10, -15)\nFake\n(0, 12)\nReal\n(57, -48)\nFake\n(-68, 39)\nFake\n(-9, -13)\nReal\n(-19, -27)\nFale\n(56, -40)\nFake\n(-64, 39)\nReal\n(-6, 86)\nFake\n(81, 10)\nFake\n(-41, -75)\n(a) Bangs Factor\n(b) Smile Feature\n(C) Hair Color Feature\nFigure 6: t-SNE visualization of attribute vectors of the face dataset. (a) shows the distribution of the\n\"bang hair\" vectors. Red circles are real \"bang hair\" images, and red crosses are generated \"bang hair\"\nimages. (b) shows the distribution of the \"smile\" representations. (c) shows the distribution of the\n\"hair color\" representations. For every plot, colored x’s indicate encoded attribute vectors obtained\nfrom transferred images.\n(a) Input\n(b) Object\n(c) −30°\n(d) 0°\n(e) 30°\n(a) Input\n(b) Object\n(c) −30°\n(d) 0°\n(e) 30°\nFigure 7: Domain-level transfer of object type and azimuth angle attributes. The left part of the ﬁgure\nshows the case of face inputs. Column (b) shows the results of transferring the input image’s object\ntype attribute from face to car. As shown in the ﬁgure, while maintain the azimuth angle attribute, the\nobject type attribute is successfully changed. Columns (c), (d) and (e) show the results of transferring\nthe azimuth angle attribute to -30, 0, and 30 degrees respectively. The right half of the ﬁgure shows\nthe case of car inputs. Similar to the case of face inputs, the results show successful attribute transfer.\n(with/without jacket) and bottoms (trousers/skirt). We show that our model can change one of top and\nbottom garments, while preserving other attributes intact in Fig. 8.\n4.2.4\nMultiplex Visual Attribute Transfer\nAs we experimented the simple attribute transfer (changing one of many attributes) over the tasks,\nwe ran the same experiment on multiplex attribute transfer where multiple attributes are changed\nsimultaneously. While training models for multiplex attribute transfer, we randomly applied additional\nconversion: transferring one of non-target attribute randomly, in order to make the model robust to\nmultiplex transfer. As 9 shows, we could transfer a face with azimuth of 30◦into a car with azimuth of\n8\nFigure 8: Domain-level transfer of fashion garment attributes. Images on the ﬁrst row are the inputs.\nResult images where the bottoms are changed (trousers to skirt, skirt to trousers) is shown in the\nsecond row. The last row indicate generated images where the tops are changed (jacket to top, top to\njacket)\n(a) Input\n(b)\n(a) Input\n(c)\n(d)\n(b) \n(a) Input\n(c)\n(d)\n(b) \nFigure 9: Multiplex image attribute transfer. The left columns shows the results based on multiplex\nobject or angle attributes. Faces and cars are inputs, and different angles of different objects are\ntargets. The results are as follows: (b) -30◦, object type attribute; (c) 0◦, object type attribute; (d) 30◦,\nobject type attribute transfer. The middle columns are results of multiplex fashion attribute transfer.\nTarget images have opposite attributes of input images, and it transfers the input images suitably to\nits target images. (b) changes the tops and bottoms attributes simultaneously. The right-most columns\nare the results of multiplex face attribute transfer. The three rows from the bottom changes smiling\nfemale based on attributes of various hair colors and unsmiling female; and the top ﬁve rows show\nthe results of the opposite experiment. (b) black hair (c) blond hair (d) brown hair.\n−30◦, change both top and bottom garments of a person, and transfer unsmiling blond hair woman to\na smiling black hair woman, which means our experiment demonstrated that various attributes-based\ntransfers could be successfully made.\n9\n5\nConclusion\nThis paper introduces an unsupervised visual attribute transfer using reconﬁgurable generative\nadversarial network. We demonstrate a single network can be used to transfer attributes among\nimages in a controlled manner. In the future, unsupervised domain transfer can be explored to enable\nnon-visual applications.\nReferences\n[1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional\nadversarial networks. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[2] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to discover cross-\ndomain relations with generative adversarial networks. In arXiv:1703.05192, 2017.\n[3] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexi A. Efros. Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In arXiv 1703.10593, 2017.\n[4] E. REINHARD, M. ASHIKHMIN, B. GOOCH, and P. SHIRLEY. Color transfer between images. In\nIEEE Comput. Graph. and Appl, 2001.\n[5] Wei Zhang, Chen Cao, Shifeng Chen, Jianzhuang Liu, and Xiaoou Tang. Style transfer via image\ncomponent analysis. In IEEE Trans. on Multimedia, 2013.\n[6] Alexei Efros and William Freeman. Image quilting for texture synthesis and transfer. In ACM SIGGRAPH,\n2001.\n[7] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear cnn models for ﬁne-grained visual\nrecognition. In The IEEE International Conference on Computer Vision (ICCV), December 2015.\n[8] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on\ndeep networks. In Advances in Neural Information Processing Systems 29, 2016.\n[9] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. In Advances in Neural Information\nProcessing Systems (NIPS), 2014.\n[10] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image generation\nfrom visual attributes. In European Conference on Computer Vision (ECCV), 2016.\n[11] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on\nRepresentation Learning (ICLR), 2014.\n[12] Tejas D. Kulkarni, Will Whitney, Pushmeet Kohli, and Joshua B. Tenenbaum. Deep convolutional inverse\ngraphics network. In Advances in Neural Information Processing Systems (NIPS), 2015.\n[13] Anders Boesen Lindbo Larsen, Soren Kaae Sonderby, Hugo Larochelle, and Ole Winther. Autoencoding\nbeyond pixels using a learned similarity metric. In International Conference on Machine Learning (ICML),\n2016.\n[14] M. Mirza and S. Osindero. Conditional generative adversarial nets. In arXiv preprint arXiv:1411.1784,\n2014.\n[15] Y. Bengio. Learning deep architectures for AI. 2(1):1–127, 2009.\n[16] Y. Bengio, A. Courville, and P. Vincent.\nRepresentation learning: A review and new perspectives.\n35(8):1798–1828, 2013.\n[17] X. Chen, Y. Duan, R. Houthooft, R. Schulman, I. Sutskever, and Pieter Abbeel. InfoGAN: Interpretable\nrepresentation learning by information maximizing generative adversarial nets. In Advances in Neural\nInformation Processing Systems (NIPS), 2016.\n[18] Michael Mathieu, Junbo Zhao, Pablo Sprechmann, Aditya Ramesh, and Yann LeCun. Disentangling\nfactors of variation in deep representation using adversarial training. In Advances in Neural Information\nProcessing Systems (NIPS), 2016.\n10\n[19] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), 2015.\n[20] S. Fidler, S. Dickinson, and R. Urtasun. 3d object detection and viewpoint estimation with a deformable\n3d cuboid model. In Advances in Neural Information Processing Systems (NIPS), 2012.\n[21] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter. A 3d face model for pose and illumination\ninvariant face recognition. In Proceedings of the 6th IEEE International Conference on Advanced Video\nand Signal based Surveillance (AVSS) for Security, Safety and Monitoring in Smart Environments, 2009.\n11\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-07-31",
  "updated": "2017-07-31"
}