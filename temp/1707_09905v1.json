{
  "id": "http://arxiv.org/abs/1707.09905v1",
  "title": "Deep Discrete Supervised Hashing",
  "authors": [
    "Qing-Yuan Jiang",
    "Xue Cui",
    "Wu-Jun Li"
  ],
  "abstract": "Hashing has been widely used for large-scale search due to its low storage\ncost and fast query speed. By using supervised information, supervised hashing\ncan significantly outperform unsupervised hashing. Recently, discrete\nsupervised hashing and deep hashing are two representative progresses in\nsupervised hashing. On one hand, hashing is essentially a discrete optimization\nproblem. Hence, utilizing supervised information to directly guide discrete\n(binary) coding procedure can avoid sub-optimal solution and improve the\naccuracy. On the other hand, deep hashing, which integrates deep feature\nlearning and hash-code learning into an end-to-end architecture, can enhance\nthe feedback between feature learning and hash-code learning. The key in\ndiscrete supervised hashing is to adopt supervised information to directly\nguide the discrete coding procedure in hashing. The key in deep hashing is to\nadopt the supervised information to directly guide the deep feature learning\nprocedure. However, there have not existed works which can use the supervised\ninformation to directly guide both discrete coding procedure and deep feature\nlearning procedure in the same framework. In this paper, we propose a novel\ndeep hashing method, called deep discrete supervised hashing (DDSH), to address\nthis problem. DDSH is the first deep hashing method which can utilize\nsupervised information to directly guide both discrete coding procedure and\ndeep feature learning procedure, and thus enhance the feedback between these\ntwo important procedures. Experiments on three real datasets show that DDSH can\noutperform other state-of-the-art baselines, including both discrete hashing\nand deep hashing baselines, for image retrieval.",
  "text": "1\nDeep Discrete Supervised Hashing\nQing-Yuan Jiang, Xue Cui and Wu-Jun Li, Member, IEEE\nAbstract—Hashing has been widely used for large-scale search\ndue to its low storage cost and fast query speed. By using\nsupervised information, supervised hashing can signiﬁcantly\noutperform unsupervised hashing. Recently, discrete supervised\nhashing and deep hashing are two representative progresses in\nsupervised hashing. On one hand, hashing is essentially a discrete\noptimization problem. Hence, utilizing supervised information\nto directly guide discrete (binary) coding procedure can avoid\nsub-optimal solution and improve the accuracy. On the other\nhand, deep hashing, which integrates deep feature learning and\nhash-code learning into an end-to-end architecture, can enhance\nthe feedback between feature learning and hash-code learning.\nThe key in discrete supervised hashing is to adopt supervised\ninformation to directly guide the discrete coding procedure in\nhashing. The key in deep hashing is to adopt the supervised\ninformation to directly guide the deep feature learning procedure.\nHowever, there have not existed works which can use the\nsupervised information to directly guide both discrete coding\nprocedure and deep feature learning procedure in the same\nframework. In this paper, we propose a novel deep hashing\nmethod, called deep discrete supervised hashing (DDSH), to\naddress this problem. DDSH is the ﬁrst deep hashing method\nwhich can utilize supervised information to directly guide both\ndiscrete coding procedure and deep feature learning procedure,\nand thus enhance the feedback between these two important\nprocedures. Experiments on three real datasets show that DDSH\ncan outperform other state-of-the-art baselines, including both\ndiscrete hashing and deep hashing baselines, for image retrieval.\nIndex Terms—Image retrieval, deep learning, deep supervised\nhashing.\nI. INTRODUCTION\nD\nUe to the explosive increasing of data in real ap-\nplications, nearest neighbor (NN) [1] search plays a\nfundamental role in many areas including image retrieval,\ncomputer vision, machine learning and data mining. In many\nreal applications, there is no need to return the exact nearest\nneighbors for every given query and approximate nearest\nneighbor (ANN) is enough to achieve satisfactory search (re-\ntrieval) performance. Hence ANN search has attracted much\nattention in recent years [2], [3], [4], [5], [6].\nOver the last decades, hashing has become an active sub-\narea of ANN search [5], [7], [8]. The goal of hashing is\nto map the data points to binary codes with hash functions\nwhich can preserve the similarity in the original space of\nthe data points. With the binary hash code representation, the\nstorage cost for the data points can be dramatically reduced.\nFurthermore, hashing based ANN search is able to achieve\na constant or sub-linear time complexity [9]. Hence, hashing\nhas become a promising choice for efﬁcient ANN search in\nAll authors are with the National Key Laboratory for Novel Software\nTechnology, Department of Computer Science and Technology, Nanjing\nUniversity, Nanjing 210023, China. Wu-Jun Li is the corresponding author.\nE-mail: {jiangqy, cuix}@lamda.nju.edu.cn; liwujun@nju.edu.cn\nlarge-scale datasets because of its low storage cost and fast\nquery speed [10], [2], [11], [3], [12], [13], [9], [4], [6], [14].\nExisting hashing methods can be divided into two main cat-\negories: data-independent methods and data-dependent meth-\nods. Data-independent hashing methods usually adopt random\nprojections as hash functions to map the data points from the\noriginal space into a Hamming space of binary codes. That is\nto say, these methods do not use any training data to learn hash\nfunctions and binary codes. Representative data-independent\nhashing methods include locality-sensitive hashing (LSH) [2],\n[15], kernelized locality-sensitive hashing (KLSH) [11]. Typ-\nically, data-independent hashing methods need long binary\ncode to achieve satisfactory retrieval performance. Data-\ndependent hashing methods, which are also called learning to\nhash methods, try to learn the hash functions from data. Recent\nworks [16], [13], [9], [17], [18], [19], [20], [21] have shown\nthat data-dependent methods can achieve comparable or even\nbetter accuracy with shorter binary hash codes, compared with\ndata-independent methods. Therefore, data-dependent methods\nhave received more and more attention.\nExisting data-dependent hashing methods can be further\ndivided into unsupervised hashing methods and supervised\nhashing methods, based on whether supervised information\nis used for learning or not. Unsupervised hashing methods\naim to preserve the metric (Euclidean neighbor) structure\namong the training data. Representative unsupervised hashing\nmethods include spectral hashing (SH) [22], iterative quanti-\nzation (ITQ) [16], isotropic hashing (IsoHash) [7], spherical\nhashing (SPH) [13], inductive manifold hashing (IMH) [17],\nanchor graph hashing (AGH) [23], discrete graph hash-\ning (DGH) [24], latent semantic minimal hashing (LSMH) [25]\nand global hashing system (GHS) [26]. Due to the seman-\ntic gap [27], unsupervised hashing methods usually can not\nachieve satisfactory retrieval performance in real applications.\nUnlike unsupervised hashing methods, supervised hashing\nmethods aim to embed the data points from the original space\ninto the Hamming space by utilizing supervised information\nto facilitate hash function learning or hash-code learning.\nRepresentative supervised hashing methods include semantic\nhashing [28], self-taught hashing (STH) [3], supervised hash-\ning with kernels (KSH) [9], latent factor hashing (LFH) [29],\nfast supervised hashing (FastH) [19], supervised discrete\nhashing (SDH) [21] and column sampling based discrete\nsupervised hashing (COSDISH) [30]. By using supervised\ninformation for learning, supervised hashing can signiﬁcantly\noutperform unsupervised hashing in real applications such as\nimage retrieval.\nHashing is essentially a discrete optimization problem.\nBecause it is difﬁcult to directly solve the discrete optimization\nproblem, early hashing methods [12], [16], [18] adopt relax-\nation strategies to solve a proximate continuous problem which\narXiv:1707.09905v1  [cs.IR]  31 Jul 2017\n2\nmight result in a sub-optimal solution. Speciﬁcally, relaxation\nbased hashing methods utilize supervised information to guide\ncontinuous hash code learning at the ﬁrst stage. Then they\nconvert continuous hash code into binary code by using round-\ning technology at the second stage. Recently, several discrete\nhashing methods, e.g., DGH [24], FastH [19], SDH [21] and\nCOSDISH [30], which try to directly learn the discrete binary\nhash codes, have been proposed with promising performance.\nIn particular, several discrete supervised hashing methods,\nincluding FastH [19], SDH [21] and COSDISH [30], have\nshown better performance than traditional relaxation-based\ncontinuous hashing methods. The key in discrete supervised\nhashing is to adopt supervised information to directly guide\nthe discrete coding procedure, i.e., the discrete binary code\nlearning procedure.\nMost existing supervised hashing methods, including those\nintroduced above, are based on hand-crafted features. One\nmajor shortcoming for these methods is that they cannot\nperform feature learning. That is, these hand-crafted features\nmight not be optimally compatible with the hash code learn-\ning procedure. Hence, besides the progress about discrete\nhashing, there has appeared another progress in supervised\nhashing, which is called deep hashing [31], [32], [8], [33],\n[34], [35], [36], [37], [38]. Representative deep hashing in-\ncludes convolutional neural network hashing (CNNH) [31],\nnetwork in network hashing (NINH) [32], deep seman-\ntic ranking hashing (DSRH) [33], deep similarity compar-\nison hashing (DSCH) [8], deep pairwise-supervised hash-\ning (DPSH) [37], deep hashing network (DHN) [36], deep\nsupervised hashing (DSH) [35], and deep quantization net-\nwork (DQN) [36]. Deep hashing adopts deep learning [39],\n[40] for supervised hashing. In particular, most deep hashing\nmethods adopt deep feature learning to learn a feature repre-\nsentation for hashing. Many deep hashing methods integrate\ndeep feature representation learning and hashing code learn-\ning into an end-to-end architecture. Under this architecture,\nfeature learning procedure and hash-code learning procedure\ncan give feedback to each other in learning procedure. Many\nworks [37], [35] have shown that using the supervised informa-\ntion to directly guide the deep feature learning procedure can\nachieve better performance than other strategies [31] which\ndo not use supervised information to directly guide the deep\nfeature learning procedure. Hence, the key in deep hashing is\nto adopt the supervised information to directly guide the deep\nfeature learning procedure.\nBoth discrete supervised hashing and deep hashing have\nachieved promising performance in many real applications.\nHowever, there have not existed works which can use the\nsupervised information to directly guide both discrete (binary)\ncoding procedure and deep feature learning procedure in the\nsame framework. In this paper, we propose a novel deep hash-\ning method, called deep discrete supervised hashing (DDSH),\nto address this problem. The main contributions of DDSH are\noutlined as follows:\n• DDSH is the ﬁrst deep hashing method which can utilize\nsupervised information to directly guide both discrete\ncoding procedure and deep feature learning procedure.\n• By integrating the discrete coding procedure and deep\nTABLE I\nNOTATION.\nNotation\nMeaning\nB\nboldface uppercase, matrix\nb\nboldface lowercase, vector\nBij\nthe (i, j)th element in matrix B\nBT\ntranspose of matrix B\n∥b∥2\nEuclidean norm of vector b\nΩ\ncapital Greek letter, set of indices\n•\nHadamard product (i.e., element-wise product)\nb2\nelement-wise square of vector, i.e., b2 = b • b\nfeature learning procedure into the same end-to-end\nframework, these two important procedures can give\nfeedback to each other to make the hash codes and feature\nrepresentation more compatible.\n• Experiments on three real datasets show that our proposed\nDDSH can outperform other state-of-the-art baselines, in-\ncluding both discrete hashing and deep hashing baselines.\nThe rest of this paper is organized as follows. In Section II,\nwe brieﬂy introduce the notations and problem deﬁnition in\nthis paper. Then we describe DDSH in Section III. We discuss\nthe difference between DDSH and existing deep hashing\nmethods in Section IV. In Section V, we evaluate DDSH on\nthree datasets by carrying out the Hamming ranking task and\nhash lookup task. Finally, we conclude the paper in Section VI.\nII. NOTATION AND PROBLEM DEFINITION\nA. Notation\nSome representative notations we use in this paper are out-\nlined in Table I. More speciﬁcally, we use boldface uppercase\nletters like B to denote matrices. We use boldface lowercase\nletters like b to denote vectors. The (i, j)th element in matrix\nB is denoted as Bij. BT is the transpose of B and ∥b∥2 is the\nEuclidean norm of vector b. We use the capital Greek letter\nlike Ωto denote the set of indices. We use the symbol • to\ndenote the Hadamard product (i.e., element-wise product). The\nsquare of a vector (or a matrix) like b2 indicates element-wise\nsquare, i.e., b2 = b • b.\nB. Problem Deﬁnition\nAlthough supervised information can also be triplet la-\nbels [32], [8], [33], [34] or pointwise labels [21], in this\npaper we only focus on the setting with pairwise labels [31],\n[38], [37], [36], [35] which is a popular setting in supervised\nhashing. The technique in this paper can also be adapted to\nsettings with triplet labels, which will be pursued in our future\nwork.\nWe use X = {xi}n\ni=1 to denote a set of training points.\nIn supervised hashing with pairwise labels, the supervised\ninformation S = {−1, 1}n×n between data points is also\navailable for training procedure, where Sij is deﬁned as\nfollows:\nSij =\n(\n1, xi and xj are similar.\n−1, otherwise.\n3\nSupervised hashing aims at learning a hash function to map\nthe data points from the original space into the binary code\nspace (or called Hamming space), with the semantic (super-\nvised) similarity in S preserved in the binary code space. We\ndeﬁne the hash function as: h(x) ∈{−1, +1}c, ∀x ∈X,\nwhere c is the binary code length. The Hamming distance\nbetween binary codes bi = h(xi) and bj = h(xj) is deﬁned\nas follows:\ndistH(bi, bj) = 1\n2(c −bT\ni bj).\nTo preserve the similarity between data points, the Hamming\ndistance between the binary codes bi = h(xi) and bj = h(xj)\nshould be relatively small if the data points xi and xj are\nsimilar, i.e., Sij = 1. On the contrary, the Hamming distance\nbetween the binary codes bi = h(xi) and bj = h(xj) should\nbe relatively large if the data points xi and xj are dissimilar,\ni.e., Sij = −1. In other words, the goal of supervised hashing\nis to solve the following problem:\nmin\nh\nL(h) =\nn\nX\ni,j=1\nL(h(xi), h(xj); Sij)\n=\nn\nX\ni,j=1\nL(bi, bj; Sij),\n(1)\nwhere L(·) is a loss function.\nThere have appeared various loss functions in supervised\nhashing. For example, KSH [9] uses L2 function, which is\ndeﬁned as follows:\nL(bi, bj; Sij) = (Sij −1\nc\nc\nX\nm=1\nbm\ni bm\nj )2.\nwhere bm\ni is the mth element in vector bi. Please note that our\nDDSH is able to adopt many different kinds of loss functions.\nIn this paper, we only use L2 loss function as an example, and\nleave other loss functions for further study in future work.\nIII. DEEP DISCRETE SUPERVISED HASHING\nIn this section, we describe the details of DDSH, including\nthe model architecture and learning algorithm.\nA. Model Architecture\nDDSH is an end-to-end deep hashing method which is able\nto simultaneously perform feature learning and hash code\nlearning in the same framework. The model architecture of\nDDSH is shown in Figure 1, which contains two important\ncomponents: loss part and feature learning part. The loss part\ncontains the discrete coding procedure which aims to learn\noptimal binary code to preserve semantic pairwise similarity.\nThe feature learning part contains the deep feature learning\nprocedure which tries to learn a compatible deep neural net-\nwork to extract deep representation from scratch. For DDSH,\ndiscrete coding procedure and deep feature learning are inte-\ngrated into an end-to-end framework. More importantly, both\nprocedures are directly guided by supervised information.\n1) Loss Part: Inspired by COSDISH [30], we use column-\nsampling method to split the whole training set into two parts.\nMore speciﬁcally, we randomly sample a subset Ωof Φ =\n{1, 2, . . . , n} and generate Γ = Φ \\ Ω(|Γ| ≫|Ω|). Then we\nsplit the whole training set X into two subsets XΩand XΓ,\nwhere XΩand XΓ denote the training data points indexed by\nΩand Γ respectively.\nSimilarly, we sample |Ω| columns of S with the correspond-\ning sampled columns indexed by Ω. Then, we approximate the\noriginal problem in (1) by only using the sampled columns of\nS:\nmin\nh\nL(h) =\nX\ni∈Ω\nn\nX\nj=1\nL(h(xi), h(xj); Sij)\n=\nX\nxi∈XΩ\nX\nxj∈XΓ\nL(h(xi), h(xj); Sij)\n+\nX\nxi,xj∈XΩ\nL(h(xi), h(xj); Sij).\n(2)\nThen we introduce auxiliary variables to solve problem (2).\nMore speciﬁcally, we utilize auxiliary variables BΩ= {bi|i ∈\nΩ} with bi ∈{−1, +1}c to replace part of the binary codes\ngenerated by the hash function, i.e., h(XΩ). Here, h(XΩ) =\n{h(xi)|xi ∈XΩ}. Then we rewrite the problem (2) as follows:\nmin\nh,BΩL(h, BΩ) =\nX\ni∈Ω\nX\nxj∈XΓ\nL(bi, h(xj); Sij)\n+\nX\ni,j∈Ω\nL(bi, bj; Sij)\ns.t. bi ∈{−1, +1}c, ∀i ∈Ω\n(3)\nThe problem in (3) is the ﬁnal loss function (objective) to\nlearn by DDSH. We can ﬁnd that the whole training set is\ndivided into two subsets XΩand XΓ. The binary codes of\nXΩ, i.e., BΩ, are directly learned from the objective function\nin (3), but the binary codes of XΓ are generated by the hash\nfunction h(·). h(·) is deﬁned based on the output of the deep\nfeature learning part, which will be introduced in the following\nsubsection.\nThe learning of BΩcontains the discrete coding procedure,\nwhich is directly guided by the supervised information. The\nlearning of h(·) contains the deep feature learning procedure,\nwhich is also directly guided by the supervised information.\nHence, our DDSH can utilize supervised information to di-\nrectly guide both discrete coding procedure and deep feature\nlearning procedure in the same end-to-end deep framework.\nThis is different from existing deep hashing methods which\neither use relaxation strategy without discrete coding or do not\nuse the supervised information to directly guide the discrete\ncoding procedure.\nPlease note that “directly guided” in this paper means\nthat the supervised information is directly included in the\ncorresponding terms in the loss function. For example, the\nsupervised information Sij is directly included in all terms\nabout the discrete codes BΩin (3), which means that the\ndiscrete coding procedure is directly guided by the supervised\ninformation. Furthermore, the supervised information Sij is\nalso directly included in the term about the deep feature\n4\n…...\nOptimal Binary Code \nLearning\nPariwise Loss\nFeature Learning Part\nLoss Part\nSΓ\nSΩ\nBΓ\nBΩ\n1\nFig. 1. The model architecture of DDSH. DDSH is an end-to-end deep learning framework which consists of two main components: loss part and feature\nlearning part. The loss part contains the discrete coding procedure (to learn the binary codes BΩ), and the feature learning part contains the deep feature\nlearning procedure (to learn the F(x; Θ) for x indexed by Γ). During each iteration, we adopt an alternating strategy to learn binary codes and feature\nrepresentation alternatively, both of which are directly guided by supervised information. Hence, DDSH can enhance the feedback between the discrete coding\nprocedure and the deep feature learning procedure.\nlearning function h(xj) in (3), which means that the deep\nfeature learning procedure is also directly guided by the\nsupervised information. To the best of our knowledge, DDSH\nis the ﬁrst deep hashing method which can utilize supervised\ninformation to directly guide both discrete coding procedure\nand deep feature learning procedure, and thus enhance the\nfeedback between these two important procedures.\n2) Feature Learning Part: The binary codes of XΓ are\ngenerated by the hash function h(·), which is deﬁned based on\nthe output of the deep feature learning part. More speciﬁcally,\nwe deﬁne our hash function as: h(x) = sign(F(x; Θ)), where\nsign(·) is the element-wise sign function. F(x; Θ) denotes the\noutput of the feature learning part and Θ denotes all parameters\nof the deep neural network.\nWe adopt a convolutional neural network (CNN) from [41],\ni.e., CNN-F, as our deep feature learning part. We replace\nthe last layer of CNN-F as one fully-connected layer to\nproject the output of the second last layer to Rc space. More\nspeciﬁcally, the feature learning part contains 5 convolutional\nlayers (“conv1-conv5”) and 3 fully-connected layers (“full6-\nfull8”). The detailed conﬁguration of the 5 convolutional layers\nis shown in Table II. In Table II, “ﬁlter size” denotes the\nnumber of convolutional ﬁlters and their receptive ﬁeld size.\n“stride” speciﬁes the convolutional stride. “pad” indicates the\nnumber of pixels to add to each size of the input. “LRN”\ndenotes whether Local Response Normalization (LRN) [39] is\napplied or not. “pool” denotes the down-sampling factor. The\ndetailed conﬁguration of the 3 fully-connected layers is shown\nin Table III, where the “conﬁguration” shows the number of\nnodes in each layer.\nWe adopt the Rectiﬁed Linear Unit (ReLU) [39] as activa-\ntion function for all the ﬁrst seven layers. For the last layer,\nwe utilize identity function as the activation function.\nB. Learning\nAfter randomly sampling Ωat each iteration, we utilize an\nalternating learning strategy to solve problem (3).\nTABLE II\nCONFIGURATION OF THE CONVOLUTIONAL LAYERS IN DDSH.\nLayer\nConﬁguration\nﬁlter size\nstride\npad\nLRN\npool\nconv1\n64 × 11 × 11\n4 × 4\n0\nyes\n2 × 2\nconv2\n256 × 5 × 5\n1 × 1\n2\nyes\n2 × 2\nconv3\n256 × 3 × 3\n1 × 1\n1\nno\n-\nconv4\n256 × 3 × 3\n1 × 1\n1\nno\n-\nconv5\n256 × 3 × 3\n1 × 1\n1\nno\n2 × 2\nTABLE III\nCONFIGURATION OF THE FULLY-CONNECTED LAYERS IN DDSH.\nLayer\nConﬁguration\nfull6\n4096\nfull7\n4096\nfull8\nhash code length c\nMore speciﬁcally, each time we learn one of the variables\nBΩand h(F(x; Θ)) with the other ﬁxed. When h(F(x; Θ)) is\nﬁxed, we directly learn the discrete hash code BΩover binary\nvariables. When BΩis ﬁxed, we update the parameter Θ of\nthe deep neural network.\n1) Learn BΩwith h(F(x; Θ)) Fixed: When h(F(x; Θ)) is\nﬁxed, it’s easy to transform problem (3) into a binary quadratic\nprogramming (BQP) problem as that in TSH [18]. Each time\nwe optimize one bit for all points. Then, the optimization of\nthe kth bit of all points in BΩis given by:\nmin\nbk [bk]T Qkbk + [bk]T pk\ns.t. bk ∈{−1, +1}|Ω|\n(4)\n5\nwhere bk denotes the kth column of BΩ, and\nQk\nij\ni̸=j\n= −2(cSΩ\nij −\nk−1\nX\nm=1\nbm\ni bm\nj )\nQk\nii =0\npk\ni = −2\n|Γ|\nX\nl=1\nBΓ\nlk(cSΓ\nli −\nk−1\nX\nm=1\nBΓ\nlmbm\ni ).\nHere, bm\ni\ndenotes the mth bit of bi and pk\ni denotes the ith\nelement of pk.\nFollowing COSDISH, we can easily solve problem (4)\nby transforming the BQP problem into an equally clustering\nproblem [42].\n2) Learn h(F(x; Θ)) with BΩFixed: Because the deriva-\ntive of the hash function h(x) = sign(F(x; Θ)) is 0 ev-\nerywhere except at 0, we cannot use back-propagation (BP)\nmethods to update the neural network parameters. So we relax\nsign(·) as h(x) = tanh(F(x; Θ)) inspired by [20]. Then we\noptimize the following problem:\nmin\nh\nL(h) =\nX\ni∈Ω\nX\nxj∈XΓ\nL(bi, h(xj); Sij)\ns.t. h(xj) = tanh(F(xj; Θ))\n(5)\nTo learn the CNN parameter Θ, we utilize a back-\npropagation algorithm. That is, each time we sample a mini-\nbatch of data points, and then use BP algorithm based on the\nsampled data.\nWe deﬁne the output of CNN as zj = F(xj; Θ) and aj =\ntanh(zj). Then we can compute the gradient of aj and zj as\nfollows:\n∂L\n∂aj\n=\nX\ni∈Ω\n∂L(bi, aj; Sij)\n∂aj\n=\nX\ni∈Ω\n2(aT\nj bi −Sij)bi\n(6)\nand\n∂L\n∂zj\n= ∂L\n∂aj\n• (1 −a2\nj)\n=\nX\ni∈Ω\n2(aT\nj bi −Sij)bi • (1 −a2\nj)\n(7)\nThen, we can use chain rule to compute ∂L\n∂Θ based on\n∂L\n∂aj\nand\n∂L\n∂zj .\nWe summarize the whole learning algorithm for DDSH in\nAlgorithm 1.\nC. Out-of-Sample Extension for Unseen Data Points\nAfter training our DDSH model, we can adopt the learned\ndeep hashing framework to predict the binary code for any\nunseen data point during training.\nMore speciﬁcally, given any point xq /∈X, we use the\nfollowing formula to predict its binary code:\nbq = h(xq) = sign(F(xq; Θ)),\nwhere Θ is the deep neural network parameter learned by\nDDSH model.\nAlgorithm 1 The learning algorithm for DDSH\nInput:\nTraining set X;\nCode length c;\nSupervised information S.\nOutput:\nParameter Θ of the deep neural network.\nInitialization\nInitialize neural network parameter Θ, mini-batch size M\nand iteration number Tout, Tin\nInitialize B = {bi|i = 1, 2, · · · , n}\nfor iter = 1, 2, . . . , Tout do\nRandomly sample Ωand set Γ = Φ \\ Ω\nSplit training set X into XΩand XΓ.\nSplit B into BΩand BΓ.\nfor epoch = 1, 2, . . . , Tin do\nfor k = 1, 2, . . . , c do\nConstruct the BQP problem for the kth bit using (4).\nConstruct the clustering problem to solve the BQP\nproblem for the kth bit.\nend for\nfor t = 1, 2, . . . , |Γ|/M do\nRandomly sample M data points from XΓ to con-\nstruct a mini-batch.\nCalculate h(xj) for each data point xj in the mini-\nbatch by forward propagation.\nCalculate the gradient according to (7).\nUpdate the parameter Θ by using back propagation.\nUpdate bj = sign(h(xj)) for each data point xj in\nthe mini-batch.\nend for\nend for\nend for\nIV. COMPARISON TO RELATED WORK\nAlthough a lot of deep hashing methods have been pro-\nposed, none of these methods can utilize supervised informa-\ntion to directly guide both discrete coding procedure and deep\nfeature learning procedure.\nExisting deep hashing methods either use relaxation strat-\negy without discrete coding or do not use the supervised\ninformation to directly guide the discrete coding procedure.\nFor example, CNNH [31] is a two-step method which adopts\nrelaxation strategy to learn continuous code in the ﬁrst stage\nand performs feature learning in the second stage. The feature\nlearning procedure in CNNH is not directly guided by super-\nvised information. NINH [32], DHN [36] and DSH [35] adopt\nrelaxation strategy to learn continuous code. DPSH [37] and\nDQN [36] can learn binary code in the training procedure.\nHowever, DSPH and DQN do not utilize the supervised\ninformation to directly guide the discrete coding procedure.\nThe objective function of DPSH can be written as: LDPSH =\n−P\nSij∈S(SijΘij −log(1 + eΘij)) + η Pn\ni=1 ∥bi −ui∥2\nF\n1,\n1For DPSH, supervised information Sij is deﬁned on {0, 1}.\n6\nwhere Θij = 1\n2uT\ni uj and ui denotes the output of the deep\nneural network. We can ﬁnd that in DPSH the discrete coding\nprocedure is not directly guided by supervised information,\ni.e., the supervised information is not directly included in\nthe terms of {bi} in the objective function. The objective\nfunction of DQN can be written as: LDQN = P\nSij∈S(Sij −\nzT\ni zj\n∥zi∥∥zj∥)2+λ Pn\ni=1 ∥zi−Chi∥2\nF , where zi denotes the output\nof the deep neural network and Pn\ni=1 ∥zi −Chi∥2\nF denotes\nthe product quantization loss. The discrete coding procedure\nis only contained in the term Pn\ni=1 ∥zi −Chi∥2\nF . We can\nﬁnd that in DQN the discrete coding procedure is not directly\nguided by supervised information either.\nTo the best of our knowledge, our DDSH is the ﬁrst deep\nhashing method which can utilize supervised information to\ndirectly guide both discrete coding procedure and deep feature\nlearning procedure in the same framework.\nV. EXPERIMENT\nWe evaluate DDSH and other baselines on datasets from\nimage retrieval applications. The open source deep learning\nlibrary MatConvNet [43] is used to implement our model. All\nexperiments are performed on an NVIDIA K40 GPU server.\nA. Experimental Setting\n1) Datasets: We adopt three widely used image datasets\nto evaluate our proposed method. They are CIFAR-102 [39],\nSVHN3 [44] and NUS-WIDE4 [45].\nThe CIFAR-10 dataset contains 60,000 images which are\nmanually labeled into 10 classes including “airplane”, “auto-\nmobile”, “bird”, “cat”, “deer”, “dog”, “frog”, “horse”, “ship”\nand “truck”. It’s a single-label dataset. The size of each image\nis 32×32 pixels. Two images are treated as similar if they share\nthe same label, i.e., they belong to the same class. Otherwise,\nthey are considered to be dissimilar.\nThe SVHN dataset consists of 73,257 digits for training,\n26,032 digits for testing and 531,131 additional samples. It\nis a real-world image dataset for recognizing digital numbers\nin natural scene images. The images are categorized into 10\nclasses, each corresponding to a digital number. SVHN is also\na single-label dataset. Two images are treated as similar if\nthey share the same label. Otherwise, they are considered to\nbe dissimilar.\nThe NUS-WIDE dataset is a relatively large-scale image\ndataset which includes 269,648 images and the associated\ntags from Flickr website. It’s a multi-label dataset where each\nimage might be annotated with multi-labels. We select 186,577\ndata points that belong to the 10 most frequent concepts from\nthe original dataset. Two images are treated as similar if they\nshare at least one label. Otherwise, they are considered to be\ndissimilar.\nTable IV illustrates some example points from the above\nthree datasets.\n2https://www.cs.toronto.edu/˜kriz/cifar.html\n3http://uﬂdl.stanford.edu/housenumbers/\n4http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm\nTABLE IV\nEXAMPLE POINTS OF THE DATASETS.\nDataset\nExample\nLabel\nCIFAR-10\n“frog”.\n“deer”.\n“truck”.\nSVHN\n“2”.\n“3”.\n“9”.\nNUS-WIDE\n“person”, “sky”.\n“clouds”, “ocean”,\n“person”, “sky”, “water”.\n“road”, “clouds”,\n“sky”, “buildings”.\nFor CIFAR-10 dataset, we randomly take 1,000 images (100\nimages per class) as query set and the remaining images as\nretrieval set. For SVHN dataset, we randomly select 1,000\nimages (100 images per class) from testing set as query set and\nutilize the whole training set as retrieval set. For NUS-WIDE\ndataset, we randomly select 1,867 data points as query set and\nthe remaining data points as retrieval set. For all datasets, we\nrandomly select 5,000 data points from retrieval set as training\nset.\n2) Baselines and Evaluation Protocol: We compare DDSH\nwith nine state-of-the-art baselines, including LSH [15],\nITQ [16], LFH [29], FastH [19], SDH [21], COSDISH [30],\nDHN [38], DSH [35], and DPSH [37]. These baselines are\nbrieﬂy introduced as follows:\n• Locality-sensitive hashing (LSH) [15]: LSH is a repre-\nsentative data-independent hashing method. LSH utilizes\nrandom projection to generate hash function.\n• Iterative quantization (ITQ) [16]: ITQ is a representative\nunsupervised hashing method. ITQ ﬁrst projects data\npoints into low space by utilizing principal component\nanalysis (PCA). Then ITQ minimizes the quantization\nerror to learn binary code.\n• Latent factor hashing (LFH) [29]: LFH is a supervised\nhashing method which tries to learn binary code based\non latent factor models.\n• Fast supervised hashing (FastH) [19]: FastH is supervised\nhashing method. FastH directly adopts graph-cut method\nto learn discrete binary code.\n• Supervised discrete hashing (SDH) [21]: SDH is a point-\nwise supervised hashing method which utilizes the dis-\ncrete cyclic coordinate descent (DCC) algorithm to learn\ndiscrete hash code.\n• Column\nsampling\nbased\ndiscrete\nsupervised\nhash-\n7\ning (COSDISH) [30]: COSDISH is a supervised hashing\nmethod. COSDISH can directly learn discrete hash code.\n• Deep hashing network (DHN) [38]: DHN is a deep su-\npervised hashing method. DHN minimizes both pairwise\ncross-entropy loss and pairwise quantization loss.\n• Deep supervised hashing (DSH) [35]: DSH is a deep\nsupervised hashing method. DSH takes pairs of points\nas input and learns binary codes by maximizing the\ndiscriminability of the corresponding binary codes.\n• Deep pairwise-supervised hashing (DPSH) [37]: DPSH\nis a deep supervised hashing method. DPSH performs\nsimultaneous deep feature learning and hash-code learn-\ning with pairwise labels by minimizing negative log-\nlikelihood of the observed pairwise labels.\nAmong all these baselines, LSH is a data-independent hash-\ning method. ITQ is an unsupervised hashing method. LFH,\nFastH, COSDISH, and SDH are non-deep methods, which can-\nnot perform deep feature learning. LFH is a relaxation-based\nmethod. FastH, COSDISH and SDH are discrete supervised\nhashing methods. DHN, DSH, and DPSH are deep hashing\nmethods which can perform feature learning and hash-code\nlearning simultaneously.\nWe ﬁrst resize all images to be 224 × 224 pixels for three\ndatasets. Then the raw image pixels are directly utilized as\ninput for deep hashing methods. For fair comparison, all deep\nhashing methods, including deep baselines and our DDSH,\nadopt the same pre-trained CNN-F model on ImageNet 5 for\nfeature learning. We carefully implement DHN and DSH on\nMatConvNet. We ﬁx the mini-batch size to be 128 and tune the\nlearning rate from 10−6 to 10−2 by using a cross-validation\nstrategy. Furthermore, we set weight decay as 5 × 10−4 to\navoid overﬁtting. For DDSH, we set |Ω| = 100, Tout = 3\nand Tin = 50. Because NUS-WIDE is a multi-label dataset,\nwe reduce the similarity weight for those training points with\nmulti-labels when we train DDSH.\nFor non-deep hashing methods, including LFH, ITQ, LFH,\nFastH, SDH and COSDISH, we use 4,096-dim deep features\nextracted by the CNN-F model pre-trained on ImageNet as\ninput for fair comparison. Because SDH is a kernel-based\nmethods, we randomly sample 1,000 data points as anchors\nto construct the kernel by following the suggestion of the\nauthors of SDH [21]. For LFH, FastH and COSDISH, we\nutilize boosted decision tree for out-of-sample extension by\nfollowing the setting of FastH.\nIn our experiment, ground-truth neighbors are deﬁned based\non whether two data points share at least one class label.\nWe carry out Hamming ranking task and hash lookup task to\nevaluate DDSH and baselines. We report the Mean Average\nPrecision (MAP), Top-K precision, precision-recall curve and\ncase study for Hamming ranking task. Speciﬁcally, given a\nquery xq, we can calculate its average precision (AP) through\nthe following equation:\nAP(xq) = 1\nRk\nN\nX\nk=1\nP(k)I1(k),\n5We download the CNN-F model pre-trained on ImageNet from http://\nwww.vlfeat.org/matconvnet/pretrained/.\nwhere Rk is the number of the relevant samples, P(k) is the\nprecision at cut-off k in the returned sample list and I1(k) is\nan indicator function which equals 1 if the kth returned sample\nis a ground-truth neighbor of xq. Otherwise, I1(k) is 0. Given\nQ queries, we can compute the MAP as follows:\nMAP = 1\nQ\nQ\nX\nq=1\nAP(xq).\nBecause NUS-WIDE is relatively large, the MAP value on\nNUS-WIDE is calculated based on the top 5000 returned\nneighbors. The MAP values for other datasets are calculated\nbased on the whole retrieval set.\nFor hash lookup task, we report mean hash lookup success\nrate (SR) within Hamming radius 0, 1 and 2 [24]. When at\nleast one ground-truth neighbor is retrieved within a speciﬁc\nHamming radius, we call it a lookup success. The hash lookup\nsuccess rate (SR) can be calculated as follows:\nSR =\nQ\nX\nq=1\nI(number of retrieved ground-truth for query xq > 0)\nQ\nHere, I(·) is an indicator function, i.e., I(true) = 1 and\nI(false) = 0. Q is the total number of query images. All\nexperiments are run 5 times, and the average performance is\nreported.\nB. Experimental Result\n1) Hamming Ranking Task: Table V reports the MAP result\non three datasets. We can easily ﬁnd that our DDSH achieves\nthe state-of-the-art retrieval accuracy in all cases compared\nwith all baselines, including deep hashing methods, non-deep\nsupervised hashing methods, non-deep unsupervised hashing\nmethods and data-independent methods.\nBy comparing ITQ to LSH, we can ﬁnd that the data-\ndependent hashing methods can signiﬁcantly outperform data-\nindependent hashing methods. By comparing COSDISH,\nSDH, FastH and LFH to ITQ, we can ﬁnd that supervised\nmethods can outperform unsupervised methods because of\nthe effect of using supervised information. By comparing\nCOSDISH, SDH and FastH to LFH, we can ﬁnd that discrete\nsupervised hashing can outperform relaxation-based continu-\nous hashing, which means that discrete coding procedure is\nable to learn more optimal binary codes. By comparing deep\nhashing methods, i.e., DDSH, DPSH, DHN and DSH, to non-\ndeep hashing methods, we can ﬁnd that deep hashing can\noutperform non-deep hashing because deep supervised hashing\ncan perform deep feature learning compared with non-deep\nhashing methods. This experimental result demonstrates that\ndeep supervised hashing is a more compatible architecture for\nhashing learning.\nThe main difference between our proposed DDSH and other\ndiscrete supervised hashing methods like COSDISH, SDH\nand FastH is that our DDSH adopts supervised information\nto directly guide deep feature learning procedure but other\ndiscrete supervised hashing methods do not have deep feature\nlearning ability. The main difference between our DDSH and\nother deep hashing methods is that DDSH adopts supervised\n8\nTABLE V\nMAP OF THE HAMMING RANKING TASK. THE BEST ACCURACY IS SHOWN IN BOLDFACE.\nMethod\nCIFAR-10\nSVHN\nNUS-WIDE\n12 bits\n24 bits\n32 bits\n48 bits\n12 bits\n24 bits\n32 bits\n48 bits\n12 bits\n24 bits\n32 bits\n48 bits\nDDSH\n0.769\n0.829\n0.835\n0.819\n0.574\n0.674\n0.703\n0.718\n0.791\n0.815\n0.821\n0.827\nDSH\n0.646\n0.749\n0.786\n0.811\n0.370\n0.480\n0.523\n0.583\n0.762\n0.794\n0.797\n0.808\nDPSH\n0.684\n0.723\n0.740\n0.746\n0.379\n0.422\n0.434\n0.456\n0.788\n0.809\n0.817\n0.823\nDHN\n0.673\n0.711\n0.705\n0.713\n0.380\n0.410\n0.416\n0.430\n0.790\n0.810\n0.809\n0.818\nCOSDISH\n0.609\n0.683\n0.696\n0.716\n0.238\n0.295\n0.320\n0.341\n0.730\n0.764\n0.787\n0.799\nSDH\n0.520\n0.646\n0.658\n0.669\n0.151\n0.300\n0.320\n0.334\n0.739\n0.762\n0.770\n0.772\nFastH\n0.620\n0.673\n0.687\n0.716\n0.252\n0.296\n0.318\n0.344\n0.741\n0.783\n0.795\n0.809\nLFH\n0.401\n0.605\n0.657\n0.700\n0.193\n0.256\n0.284\n0.325\n0.705\n0.759\n0.778\n0.794\nITQ\n0.258\n0.272\n0.283\n0.294\n0.111\n0.114\n0.115\n0.116\n0.505\n0.504\n0.503\n0.505\nLSH\n0.147\n0.172\n0.180\n0.193\n0.107\n0.108\n0.109\n0.111\n0.341\n0.351\n0.351\n0.371\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n1\n(a) 12 bits @CIFAR-10\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n1\n(b) 24 bits @CIFAR-10\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n1\n(c) 32 bits @CIFAR-10\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n1\n(d) 48 bits @CIFAR-10\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n(e) 12 bits @SVHN\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n(f) 24 bits @SVHN\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n(g) 32 bits @SVHN\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.4\n0.6\n0.8\n(h) 48 bits @SVHN\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(i) 12 bits @NUS-WIDE\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(j) 24 bits @NUS-WIDE\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(k) 32 bits @NUS-WIDE\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(l) 48 bits @NUS-WIDE\nFig. 2. Performance of precision-recall curve on three datasets. The four sub-ﬁgures in each row are the precision-recall curves for 12 bits, 24 bits, 32 bits\nand 48 bits respectively.\ninformation to directly guide the discrete coding procedure but\nother deep hashing methods do not have this property. Hence,\nthe experimental results successfully demonstrate the motiva-\ntion of DDSH, i.e., utilizing supervised information to directly\nguide both deep feature learning procedure and discrete coding\nprocedure can further improve retrieval performance in real\napplications.\nFurthermore, we select three best baselines, i.e., DSH,\nDPSH and DHN, to compare the precision-recall and top-k\nprecision results. We report the precision-recall curve on all\nthree datasets in Figure 2. We can see that the proposed DDSH\nstill achieves the best performance in terms of precision-recall\ncurve in most cases.\nIn real applications, we might care about top-k retrieval\nresults more than the whole database. Hence we report the\ntop-k precision on three datasets based on the returned top-k\nsamples. In Figure 3, we show the top-k precision for dif-\nferent k on CIFAR-10 dataset, SVHN dataset and NUS-\nWIDE dataset respectively, where k is the number of returned\nsamples. Again, we can ﬁnd that DDSH can outperform other\ndeep hashing methods in most cases.\n2) Hash Lookup Task: In practice, retrieval with hash\nlookup can usually achieve constant or sub-linear search speed\nin real applications. Recent works like DGH [24] show that\ndiscrete hashing can signiﬁcantly improve hash lookup success\nrate.\nIn Figure 4, we present the mean hash lookup success\nrate within Hamming radius 0, 1 and 2 on all three datasets\n9\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n(a) 12 bits @CIFAR-10\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n(b) 24 bits @CIFAR-10\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n(c) 32 bits @CIFAR-10\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n(d) 48 bits @CIFAR-10\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n(e) 12 bits @SVHN\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(f) 24 bits @SVHN\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(g) 32 bits @SVHN\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n(h) 48 bits @SVHN\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.75\n0.76\n0.77\n0.78\n0.79\n0.8\n0.81\n(i) 12 bits @NUS-WIDE\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.75\n0.77\n0.79\n0.81\n0.83\n(j) 24 bits @NUS-WIDE\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.75\n0.77\n0.79\n0.81\n0.83\n(k) 32 bits @NUS-WIDE\nReturned samples\n500\n1000\n1500\n2000\nPrecision\n0.76\n0.78\n0.8\n0.82\n0.84\n(l) 48 bits @NUS-WIDE\nFig. 3. Performance of top-k precision on three datasets. The four sub-ﬁgures in each row are the top-k precision curves for 12 bits, 24 bits, 32 bits and 48\nbits respectively.\nfor all deep hashing methods. We can ﬁnd that DDSH can\nachieve the best mean hash lookup success rate on three\ndatasets, especially for long codes. Furthermore, the hash\nlookup success rate of DDSH is nearly above 0.9 in all cases\nfor Hamming radius 2.\n3) Further Analysis: To further demonstrate the effective-\nness of utilizing supervised information to directly guide\nboth discrete coding procedure and deep feature learning\nprocedure in the same end-to-end framework, we evaluate\nseveral variants of DDSH. These variants include “DDSH0”,\n“COSDISH-Linear”, “COSDISH-CNN” and “DDSH-MAC”.\nDDSH0 denotes the variant in which we ﬁx the parameters\nof the ﬁrst seven layers of CNN-F in DDSH during training\nprocedure. In other words, DDSH0 can not perform deep\nfeature learning procedure, and all the other parts are exactly\nthe same as those in DDSH. Comparison between DDSH0 and\nDDSH is to show the importance of deep feature learning.\nCOSDISH-Linear denotes a variant of COSDISH in which\nwe use linear function rather than boosted decision tree for\nout-of-sample extension. COSDISH-CNN denotes a variant of\nCOSDISH in which we learn optimal binary codes using COS-\nDISH ﬁrst, and then we use the CNN-F to approximate the\nbinary codes for out-of-sample extension. Because the discrete\ncoding procedure in DDSH is similar to that in COSDISH,\nCOSDISH-CNN can be considered as a two-stage variant of\nDDSH where the discrete coding stage is independent of the\nfeature learning stage. The comparison between COSDISH-\nCNN and DDSH is to show that integrating the discrete coding\nprocedure and deep feature learning procedure into the same\nframework is important.\nDDSH-MAC is a variant of DDSH by using the method of\nauxiliary coordinates (MAC) technique in AFFHash [46]. That\nis to say, we use loss function LCOSDISH(BΓ, BΩ) + λ∥B −\ntanh(F(X; Θ))∥2\nF to enhance the feedback between deep\nfeature learning and discrete code learning. Here, LCOSDISH(·)\nis the loss used in COSDISH. DDSH-MAC can integrate the\ndiscrete coding procedure and deep feature learning procedure\ninto the same framework. However, the supervised information\nSij isn’t directly included in the deep feature learning term\n∥B −tanh(F(X; Θ))∥2\nF in DDSH-MAC. That is to say, the\nsupervised information is not directly used to guide the deep\nfeature learning procedure.\nThe experimental results are shown in Table VI. By compar-\ning DDSH to its variants including DDSH0, COSDISH-Linear,\nCOSDISH-CNN and DDSH-MAC, we can ﬁnd that DDSH\ncan signiﬁcantly outperform all the other variants. It means\nthat utilizing supervised information to directly guide both\ndiscrete coding procedure and deep feature learning procedure\nin the same end-to-end framework is the key to make DDSH\nachieve state-of-the-art retrieval performance.\nFurthermore, to evaluate the approximation we used when\nwe update the parameter of deep neural network, we report\nthe distribution of the output for the deep neural network.\nFigure 5 shows the distribution of the output of tanh(F(X; Θ))\n10\n12\n24\n32\n48\n#bits\n0.4\n0.6\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(a) CIFAR-10 @radius 0\n12\n24\n32\n48\n#bits\n0.4\n0.6\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(b) CIFAR-10 @radius 1\n12\n24\n32\n48\n#bits\n0.6\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(c) CIFAR-10 @radius 2\n12\n24\n32\n48\n#bits\n0.1\n0.3\n0.5\n0.7\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(d) SVHN @radius 0\n12\n24\n32\n48\n#bits\n0.1\n0.3\n0.5\n0.7\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(e) SVHN @radius 1\n12\n24\n32\n48\n#bits\n0.1\n0.3\n0.5\n0.7\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(f) SVHN @radius 2\n12\n24\n32\n48\n#bits\n0.4\n0.6\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(g) NUS-WIDE @radius 0\n12\n24\n32\n48\n#bits\n0.4\n0.6\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(h) NUS-WIDE @radius 1\n12\n24\n32\n48\n#bits\n0.4\n0.6\n0.8\n0.9\n1\nSuccess Rate\nDDSH\nDSH\nDPSH\nDHN\n(i) NUS-WIDE @radius 2\nFig. 4. Hash lookup success rate. Each row includes three sub-ﬁgures and presents the hash lookup success rate results on CIFAR-10 ((a), (b), (c)), SVHN ((d),\n(e), (f)) and NUS-WIDE ((g), (h), (i)), respectively.\nTABLE VI\nMAP COMPARISON AMONG VARIANTS OF DDSH ON CIFAR-10. THE\nBEST ACCURACY IS SHOWN IN BOLDFACE.\nMethod\nCIFAR-10\n12 bits\n24 bits\n32 bits\n48 bits\nDDSH\n0.769\n0.829\n0.835\n0.819\nDDSH0\n0.579\n0.639\n0.654\n0.680\nCOSDISH-Linear\n0.212\n0.235\n0.258\n0.272\nCOSDISH-CNN\n0.374\n0.477\n0.468\n0.515\nDDSH-MAC\n0.412\n0.506\n0.528\n0.534\nwhen we ﬁnish the training procedure of DDSH on CIFAR-10\ndataset. The x-axis is the tanh(F(X; Θ)), and the y-axis is the\nnumber of points having the corresponding tanh(F(·)) value.\nIt’s easy to see that the tanh(·) can successfully approximate\nthe sign(·) function in real applications.\n4) Case Study: We randomly sample some queries and\nreturn top-20 results for each query as a case study on CIFAR-\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n10 5\n0\n5\n10\n15\nFig. 5. The effect of tanh(·) approximation on CIFAR-10\n10 to show the retrieval result intuitively. More speciﬁcally, for\neach given query image, we return top-20 nearest neighbors\n11\n(a) DDSH @32 bits\n(b) DSH @32 bits\n(c) DPSH @32 bits\n(d) DHN @32 bits\nFig. 6. Case study on CIFAR-10 with 32 bits. The ﬁrst column for each sub-ﬁgure is queries and the following twenty columns denote the top-20 returned\nresults. We use red box to denote the wrongly returned results.\nbased on its Hamming distance away from query. Then we use\nred box to indicate the returned results that are not a ground-\ntruth neighbor for the corresponding query image.\nThe result is shown in Figure 6. In each sub-ﬁgure, the\nﬁrst column is queries, including an airplane, a bird, two cats\nand a ship, and the following twenty columns denote the top-\n20 returned results. We utilize red box to denote the wrongly\nreturned results. It’s easy to ﬁnd that DDSH can achieve better\nretrieval performance than other deep hashing baselines.\nVI. CONCLUSION\nIn this paper, we propose a novel deep hashing method\ncalled deep discrete supervised hashing (DDSH) with appli-\ncation for image retrieval. On one hand, DDSH adopts a\ndeep neural network to perform deep feature learning from\npixels. On the other hand, DDSH also adopts a discrete coding\nprocedure to perform discrete hash code learning. Moreover,\nDDSH integrates deep feature learning procedure and discrete\ncoding procedure into the same architecture. To the best\nour knowledge, DDSH is the ﬁrst deep supervised hashing\n12\nmethod which can utilize supervised information to directly\nguide both discrete coding procedure and deep feature learning\nprocedure in the same end-to-end framework. Experiments on\nimage retrieval applications show that DDSH can signiﬁcantly\noutperform other baselines to achieve the state-of-the-art per-\nformance.\nREFERENCES\n[1] A. Andoni, “Nearest neighbor search in high-dimensional spaces,” in\nInternational Symposium on Mathematical Foundations of Computer\nScience, 2011, p. 1.\n[2] A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approx-\nimate nearest neighbor in high dimensions,” in IEEE Symposium on\nFoundations of Computer Science, 2006, pp. 459–468.\n[3] D. Zhang, J. Wang, D. Cai, and J. Lu, “Self-taught hashing for\nfast similarity search,” in ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, 2010, pp. 18–25.\n[4] K. He, F. Wen, and J. Sun, “K-means hashing: An afﬁnity-preserving\nquantization method for learning binary compact codes,” in IEEE\nConference on Computer Vision and Pattern Recognition, 2013, pp.\n2938–2945.\n[5] F. Shen, X. Zhou, Y. Yang, J. Song, H. T. Shen, and D. Tao, “A fast\noptimization method for general binary code learning,” IEEE Trans.\nImage Processing, vol. 25, no. 12, pp. 5610–5621, 2016.\n[6] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S. Chua, “Discrete\ncollaborative ﬁltering,” in ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, 2016, pp. 325–334.\n[7] W. Kong and W.-J. Li, “Isotropic hashing,” in Annual Conference on\nNeural Information Processing Systems, 2012, pp. 1655–1663.\n[8] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang, “Bit-scalable\ndeep hashing with regularized similarity learning for image retrieval\nand person re-identiﬁcation,” IEEE Trans. Image Processing, vol. 24,\nno. 12, pp. 4766–4779, 2015.\n[9] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang, “Supervised\nhashing with kernels,” in IEEE Conference on Computer Vision and\nPattern Recognition, 2012, pp. 2074–2081.\n[10] A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high\ndimensions via hashing,” in International Conference on Very Large\nData Bases, 1999, pp. 518–529.\n[11] B. Kulis and K. Grauman, “Kernelized locality-sensitive hashing for\nscalable image search,” in International Conference on Computer Vision,\n2009, pp. 2130–2137.\n[12] J. Wang, O. Kumar, and S.-F. Chang, “Semi-supervised hashing for\nscalable image retrieval,” in IEEE Conference on Computer Vision and\nPattern Recognition, 2010, pp. 3424–3431.\n[13] J.-P. Heo, Y. Lee, J. He, S.-F. Chang, and S.-E. Yoon, “Spherical hash-\ning,” in IEEE Conference on Computer Vision and Pattern Recognition,\n2012, pp. 2957–2964.\n[14] Y. Guo, G. Ding, L. Liu, J. Han, and L. Shao, “Learning to hash with\noptimized anchor embedding for scalable retrieval,” IEEE Trans. Image\nProcessing, vol. 26, no. 3, pp. 1344–1354, 2017.\n[15] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni, “Locality-sensitive\nhashing scheme based on p-stable distributions,” in Proceedings of the\n20th ACM Symposium on Computational Geometry, 2004, pp. 253–262.\n[16] Y. Gong and S. Lazebnik, “Iterative quantization: A procrustean ap-\nproach to learning binary codes,” in IEEE Conference on Computer\nVision and Pattern Recognition, 2011, pp. 817–824.\n[17] F. Shen, C. Shen, Q. Shi, A. van den Hengel, and Z. Tang, “Inductive\nhashing on manifolds,” in IEEE Conference on Computer Vision and\nPattern Recognition, 2013, pp. 1562–1569.\n[18] G. Lin, C. Shen, D. Suter, and A. van den Hengel, “A general two-\nstep approach to learning-based hashing,” in International Conference\non Computer Vision, 2013, pp. 2552–2559.\n[19] G. Lin, C. Shen, Q. Shi, A. van den Hengel, and D. Suter, “Fast\nsupervised hashing with decision trees for high-dimensional data,” in\nIEEE Conference on Computer Vision and Pattern Recognition, 2014,\npp. 1971–1978.\n[20] D. Song, W. Liu, R. Ji, D. A. Meyer, and J. R. Smith, “Top rank\nsupervised binary coding for visual search,” in International Conference\non Computer Vision, 2015, pp. 1922–1930.\n[21] F. Shen, C. Shen, W. Liu, and H. T. Shen, “Supervised discrete hashing,”\nin IEEE Conference on Computer Vision and Pattern Recognition, 2015,\npp. 37–45.\n[22] Y. Weiss, A. Torralba, and R. Fergus, “Spectral hashing,” in Annual\nConference on Neural Information Processing Systems, 2008, pp. 1753–\n1760.\n[23] W. Liu, J. Wang, S. Kumar, and S.-F. Chang, “Hashing with graphs,” in\nInternational Conference on Machine Learning, 2011, pp. 1–8.\n[24] W. Liu, C. Mu, S. Kumar, and S.-F. Chang, “Discrete graph hashing,”\nin Annual Conference on Neural Information Processing Systems, 2014,\npp. 3419–3427.\n[25] X. Lu, X. Zheng, and X. Li, “Latent semantic minimal hashing for image\nretrieval,” IEEE Trans. Image Processing, vol. 26, no. 1, pp. 355–368,\n2017.\n[26] D. Tian and D. Tao, “Global hashing system for fast image search,”\nIEEE Trans. Image Processing, vol. 26, no. 1, pp. 79–89, 2017.\n[27] A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain,\n“Content-based image retrieval at the end of the early years,” IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 22, no. 12, pp. 1349–1380, 2000.\n[28] R. Salakhutdinov and G. E. Hinton, “Semantic hashing,” Int. J. Approx.\nReasoning, vol. 50, no. 7, pp. 969–978, 2009.\n[29] P. Zhang, W. Zhang, W.-J. Li, and M. Guo, “Supervised hashing with\nlatent factor models,” in ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, 2014, pp. 173–182.\n[30] W.-C. Kang, W.-J. Li, and Z.-H. Zhou, “Column sampling based discrete\nsupervised hashing,” in AAAI Conference on Artiﬁcial Intelligence, 2016,\npp. 1230–1236.\n[31] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan, “Supervised hashing for\nimage retrieval via image representation learning,” in AAAI Conference\non Artiﬁcial Intelligence, 2014, pp. 2156–2162.\n[32] H. Lai, Y. Pan, Y. Liu, and S. Yan, “Simultaneous feature learning\nand hash coding with deep neural networks,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2015, pp. 3270–3278.\n[33] F. Zhao, Y. Huang, L. Wang, and T. Tan, “Deep semantic ranking\nbased hashing for multi-label image retrieval,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2015, pp. 1556–1564.\n[34] B. Zhuang, G. Lin, C. Shen, and I. Reid, “Fast training of triplet-based\ndeep binary embedding networks,” in IEEE Conference on Computer\nVision and Pattern Recognition, 2016.\n[35] H. Liu, R. Wang, S. Shan, and X. Chen, “Deep supervised hashing\nfor fast image retrieval,” in IEEE Conference on Computer Vision and\nPattern Recognition, 2016.\n[36] Y. Cao, M. Long, J. Wang, H. Zhu, and Q. Wen, “Deep quantization\nnetwork for efﬁcient image retrieval,” in AAAI Conference on Artiﬁcial\nIntelligence, 2016, pp. 3457–3463.\n[37] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep super-\nvised hashing with pairwise labels,” in International Joint Conference\non Artiﬁcial Intelligence, 2016, pp. 1711–1717.\n[38] H. Zhu, M. Long, J. Wang, and Y. Cao, “Deep hashing network for efﬁ-\ncient similarity retrieval,” in AAAI Conference on Artiﬁcial Intelligence,\n2016, pp. 2415–2421.\n[39] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Annual Conference on\nNeural Information Processing Systems, 2012, pp. 1106–1114.\n[40] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E.\nHubbard, and L. D. Jackel, “Handwritten digit recognition with a back-\npropagation network,” in Annual Conference on Neural Information\nProcessing Systems, 1989, pp. 396–404.\n[41] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of\nthe devil in the details: Delving deep into convolutional nets,” in British\nMachine Vision Conference, 2014.\n[42] R. Yang, New results on some quadratic programming problems.\nUni-\nversity of Illinois at Urbana-Champaign, 2013.\n[43] A. Vedaldi and K. Lenc, “Matconvnet: Convolutional neural networks\nfor MATLAB,” in Annual ACM Conference on Multimedia Conference,\n2015, pp. 689–692.\n[44] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,\n“Reading digits in natural images with unsupervised feature learning,” in\nNIPS Workshop on Deep Learning and Unsupervised Feature Learning,\n2011.\n[45] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, “NUS-WIDE:\na real-world web image database from national university of singapore,”\nin ACM International Conference on Image and Video Retrieval, 2009.\n[46] R. Raziperchikolaei and M. ´A. Carreira-Perpi˜n´an, “Learning hashing\nwith afﬁnity-based loss functions using auxiliary coordinates,” in Annual\nConference on Neural Information Processing Systems, 2016.\n13\nQing-Yuan Jiang received the BSc degree in com-\nputer science from Nanjing University, China, in\n2014. He is currently working toward the PhD\ndegree in the Department of Computer Science and\nTechnology, Nanjing University. His research inter-\nests are in machine learning and learning to hash.\nXue Cui received the BSc degree in computer\nscience and technology from Chongqing University,\nChina, in 2015. She is currently a Master student\nin the Department of Computer Science and Tech-\nnology, Nanjing University. Her research interests\nmainly include machine learning and data mining.\nWu-Jun Li received the BSc and MEng degrees\nin computer science from the Nanjing University\nof China, and the PhD degree in computer science\nfrom the Hong Kong University of Science and\nTechnology. He started his academic career as an\nassistant professor in the Department of Computer\nScience and Engineering, Shanghai Jiao Tong Uni-\nversity. He then joined Nanjing University where he\nis currently an associate professor in the Department\nof Computer Science and Technology. His research\ninterests are in machine learning, big data, and\nartiﬁcial intelligence. He is a member of the IEEE.\n",
  "categories": [
    "cs.IR"
  ],
  "published": "2017-07-31",
  "updated": "2017-07-31"
}