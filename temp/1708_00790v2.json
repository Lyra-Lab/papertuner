{
  "id": "http://arxiv.org/abs/1708.00790v2",
  "title": "Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition",
  "authors": [
    "Yong Jiang",
    "Wenjuan Han",
    "Kewei Tu"
  ],
  "abstract": "Unsupervised dependency parsing aims to learn a dependency parser from\nunannotated sentences. Existing work focuses on either learning generative\nmodels using the expectation-maximization algorithm and its variants, or\nlearning discriminative models using the discriminative clustering algorithm.\nIn this paper, we propose a new learning strategy that learns a generative\nmodel and a discriminative model jointly based on the dual decomposition\nmethod. Our method is simple and general, yet effective to capture the\nadvantages of both models and improve their learning results. We tested our\nmethod on the UD treebank and achieved a state-of-the-art performance on thirty\nlanguages.",
  "text": "Combining Generative and Discriminative Approaches to Unsupervised\nDependency Parsing via Dual Decomposition∗\nYong Jiang, Wenjuan Han and Kewei Tu\n{jiangyong,hanwj,tukw}@shanghaitech.edu.cn\nSchool of Information Science and Technology\nShanghaiTech University, Shanghai, China\nAbstract\nUnsupervised dependency parsing aims to\nlearn a dependency parser from unanno-\ntated sentences.\nExisting work focuses\non either learning generative models us-\ning the expectation-maximization algo-\nrithm and its variants, or learning dis-\ncriminative models using the discrimina-\ntive clustering algorithm. In this paper, we\npropose a new learning strategy that learns\na generative model and a discriminative\nmodel jointly based on the dual decom-\nposition method.\nOur method is simple\nand general, yet effective to capture the ad-\nvantages of both models and improve their\nlearning results. We tested our method on\nthe UD treebank and achieved a state-of-\nthe-art performance on thirty languages.\n1\nIntroduction\nDependency parsing is an important task in nat-\nural language processing. It identiﬁes dependen-\ncies between words in a sentence, which have been\nshown to beneﬁt other tasks such as semantic role\nlabeling (Lei et al., 2015) and sentence classiﬁca-\ntion (Ma et al., 2015). Supervised learning of a\ndependency parser requires annotation of a train-\ning corpus by linguistic experts, which can be time\nand resource consuming.\nUnsupervised depen-\ndency parsing eliminates the need for dependency\nannotation by directly learning from unparsed text.\nPrevious work on unsupervised dependency\nparsing mainly focuses on learning generative\nmodels, such as the dependency model with va-\nlence (DMV) (Klein and Manning, 2004) and\ncombinatory categorial grammars (CCG) (Bisk\nand Hockenmaier, 2012). Generative models have\n∗This work was supported by the National Natural Sci-\nence Foundation of China (61503248).\nmany advantages. For example, the learning ob-\njective function can be deﬁned as the marginal\nlikelihood of the training data, which is typically\neasy to compute in a generative model. In addi-\ntion, many types of inductive bias, such as those\nfavoring short dependency arcs (Smith and Eisner,\n2006), encouraging correlations between POS tags\n(Cohen et al., 2008; Cohen and Smith, 2009; Berg-\nKirkpatrick et al., 2010; Jiang et al., 2016), and\nlimiting center embedding (Noji et al., 2016), can\nbe incorporated into generative models to achieve\nbetter parsing accuracy.\nHowever, due to the\nstrong independence assumption in most genera-\ntive models, it is difﬁcult for these models to uti-\nlize context information that has been shown to\nbeneﬁt supervised parsing.\nRecently, a feature-rich discriminative model\nfor unsupervised parsing is proposed that captures\nthe global context information of sentences (Grave\nand Elhadad, 2015).\nInspired by discriminative\nclustering, learning of the model is formulated as\nconvex optimization of both the model parameters\nand the parses of training sentences. By utilizing\nlanguage-independent rules between pairs of POS\ntags to guide learning, the model achieves state-of-\nthe-art performance on the UD treebank dataset.\nIn this paper we propose to jointly train two\nstate-of-the-art models of unsupervised depen-\ndency parsing:\na generative model called LC-\nDMV (Noji et al., 2016) and a discriminative\nmodel called Convex-MST (Grave and Elhadad,\n2015).\nWe employ a learning algorithm based\non the dual decomposition (Dantzig and Wolfe,\n1960) inference algorithm, which encourages the\ntwo models to inﬂuence each other during train-\ning.\nWe evaluated our method on thirty languages\nand found that the jointly trained models surpass\ntheir separately trained counterparts in parsing ac-\ncuracy. Further analysis shows that the two models\narXiv:1708.00790v2  [cs.CL]  24 Sep 2017\npositively inﬂuence each other during joint train-\ning by implicitly sharing the inductive bias.\n2\nPreliminaries\n2.1\nDMV\nThe dependency model with valence (DMV)\n(Klein and Manning, 2004) is the ﬁrst generative\nmodel that outperforms the left-branching baseline\nin unsupervised dependency parsing.\nIn DMV,\na sentence is generated by recursively applying\nthree types of grammar rules to construct a parse\ntree from the top down. The probability of the gen-\nerated sentence and parse tree is the probability\nproduct of all the rules used in the generation pro-\ncess. To learn the parameters (rule probabilities)\nof DMV, the expectation maximization algorithm\nis often used. Noji et al. (2016) exploited two uni-\nversal syntactic biases in learning DMV: restrict-\ning the center-embedding depth and encouraging\nshort dependencies. They achieved a comparable\nperformance with state-of-the-art approaches.\n2.2\nConvex-MST\nConvex-MST (Grave and Elhadad, 2015) is a dis-\ncriminative model for unsupervised dependency\nparsing based on the ﬁrst-order maximum span-\nning tree dependency parser (McDonald et al.,\n2005). Given a sentence, whether each possible\ndependency exists or not is predicted based on a\nset of handcrafted features and a valid parse tree\nclosest to the prediction is identiﬁed by the mini-\nmum spanning tree algorithm.\nFor each sentence x, a ﬁrst-order dependency\ngraph is built over the words of the sentence. The\nweight of each edge is calculated by wT f(x, i, j),\nwhere w is the parameters and f(x, i, j) is the\nhandcrafted feature vector of the dependency from\nthe i-th word to the j-th word in sentence x. For\nsentence x of length n, we can represent it as ma-\ntrix X where each raw is a feature vector. The\nparse tree y is a spanning tree of the graph and\ncan be represented as a binary vector with length\nn×n where each element is 1 if the corresponding\narc is in the tree and 0 otherwise.\nLearning is based on discriminative clustering\nwith the following objective function:\n1\nN\nN\nX\nα=1\n\u0012 1\n2nα\n||yα −Xαw||2\n2 −µvT yα\n\u0013\n+λ\n2 ||w||2\n2\nwhere Xα is a matrix where each row is a feature\nrepresentation f(xα, i, j) of an edge in the depen-\ndency graph of sentence xα, v represents whether\neach dependency arc in yα satisﬁes a set of pre-\nspeciﬁed linguistic rules, and λ and µ are hyper-\nparameters.\nThe Frank-Wolfe algorithm is em-\nployed to optimize the objective function.\n2.3\nDual Decomposition\nDual decomposition (Dantzig and Wolfe, 1960),\na special case of Lagrangian relaxation, is an op-\ntimization method that decomposes a hard prob-\nlem into several small sub-problems.\nIt has\nbeen widely used in machine learning (Komodakis\net al., 2007) and natural language processing (Koo\net al., 2010; Rush and Collins, 2012).\nKomodakis et al. (2007) proposed using dual\ndecomposition to do MAP inference for Markov\nrandom ﬁelds. Koo et al. (2010) proposed a new\ndependency parser based on dual decomposition\nby combining a graph based dependency model\nand a non-projective head automata. In the work\nof Rush et al. (2010), they showed that dual de-\ncomposition can effectively integrate two lexical-\nized parsing models or two correlated tasks.\n2.4\nAgreement based Learning\nLiang et al.\n(2008) proposed agreement based\nlearning that trains several tractable generative\nmodels jointly and encourages them to agree on\ncertain latent variables.\nTo effectively train the\nsystem, a product EM algorithm was used. They\nshowed that the joint model can perform better\nthan each independent model on the accuracy or\nconvergence speed. They also showed that the ob-\njective function of the work of Klein and Manning\n(2004) is a special case of the product EM algo-\nrithm for grammar induction. Our approach has\na similar motivation to agreement based learning\nbut has two important advantages.\nFirst, while\ntheir approach only combines generative models,\nour approach can make use of both generative and\ndiscriminative models.\nSecond, while their ap-\nproach requires the sub-models to share the same\ndynamic programming structure when performing\ndecoding, our approach does not have such restric-\ntion.\n3\nJoint Training\nWe minimize the following objective function that\ncombines two different models of unsupervised\ndependency parsing:\nJ(MF, MG)\n=\nN\nX\nα=1\nmin\nyα∈Yα (F(xα, yα; MF) + G(xα, yα; MG))\nwhere N is the size of training data, MF and MG\nare the parameters of the ﬁrst and second model\nrespectively, F and G are their respective learn-\ning objectives, and Yα is the set of valid depen-\ndency parses of sentence xα. While in principle\nthis objective can be used to combine many differ-\nent types of models, here we consider two state-of-\nthe-art models of unsupervised dependency pars-\ning, a generative model LC-DMV (Noji et al.,\n2016) and a discriminative model Convex-MST\n(Grave and Elhadad, 2015). We denote the pa-\nrameters of LC-DMV by Θ and the parameters\nof Convex-MST by w. Their respective objective\nfunctions are,\nF(xα, yα; Θ) = −log (PΘ(xα, yα)f(xα, yα))\nG(xα, yα; w)\n=\n1\n2nα\n||yα −Xαw||2\n2 + λ\n2N ||w||2\n2 −µvT y\nwhere PΘ(xα, yα) is the joint probability of sen-\ntence xα and parse yα, f is a constraint factor, and\nthe notations in the second objective function are\nexplained in section 2.2.\n3.1\nLearning\nWe use coordinate descent to optimize the param-\neters of the two models. In each iteration, we ﬁrst\nﬁx the parameters and ﬁnd the best dependency\nparses of the training sentences (see section 3.2);\nwe then ﬁx the parses and optimize the parameters.\nThe detailed algorithm is shown in Algorithm 1.\nPretraining of the two models is done by run-\nning their original learning algorithms separately.\nWhen the parses of the training sentences are\nﬁxed, it is easy to show that the parameters of the\ntwo models can be optimized separately. Updat-\ning the parameters Θ of LC-DMV can be done by\nsimply counting the number of times each rule is\nused in the parse trees and then normalizing the\ncounts to get the maximum-likelihood probabili-\nties. The parameters w of Convex-MST can be up-\ndated by stochastic gradient descent. After updat-\ning Θ and w at each iteration, we additionally train\neach model separately for three iterations, which\nwe ﬁnd further improves learning.\nAlgorithm 1 Parameter Learning\nInput: Training sentence x1, x2, ..., xN\nPre-train Θ and w\nrepeat\nFix Θ and w and solve the decoding problem\nto get yα, α = 1, 2, . . . , N\nFix the parses and update Θ and w\nuntil Convergence\nAlgorithm 2 Decoding via Dual Decomposition\nInput: Sentence x, ﬁxed parameters w and Θ\nInitialize vector u of size n × n to 0\nrepeat\nˆy = arg miny∈Y F(x, y; Θ) + uT y\nˆz = arg minz∈Y G(x, z; w) −uT z\nif ˆy = ˆz then\nreturn ˆy\nelse\nu = u + τ (ˆy −ˆz)\nend if\nuntil Convergence\n3.2\nJoint Decoding\nGiven a training sample x and parameters w, Θ,\nthe goal of decoding is to ﬁnd the best parse tree:\nˆy = arg min\ny∈Y\n1\n2n||y−Xw||2\n2−µvT y−log PΘ(x, y)\nWe employ the dual decomposition algorithm to\nsolve this problem (shown in Algorithm 2), where\nτ represents the step size.\nThe most important part of the algorithm is\nsolving the two separate decoding problems:\nˆy = arg min\ny∈Y −log(PΘ(x, y)f(x, y)) + uT y\nˆz = arg min\nz∈Y\n1\n2n||z −Xw||2\n2 −µvT z −uT z\nThe ﬁrst decoding problem can be solved by a\nmodiﬁed CYK parsing algorithm that takes into\naccount the information in vector u. The second\ndecoding problem can be solved using the same al-\ngorithm of Grave and Elhadad (2015) (we use the\nprojective version in our approach).\n4\nExperiments\n4.1\nSetup\nWe use UD Treebank 1.4 as our datasets.\nWe\nsorted the datasets in the treebank by the number\nof training sentences of length ≤15 and selected\nthe top thirty datasets, which is similar to the setup\nof Noji et al. (2016). For each dataset, we trained\nour method on the training data with length ≤15\nand tested our method on the testing data with\nlength ≤40. We tuned the hyper-parameters of\nour method on the dataset of the English language\nand reported the results on the thirty datasets with-\nout any further parameter tuning. We compared\nour method with four baselines.\nThe ﬁrst two\nbaselines are Convex-MST and LC-DMV that are\nindependently trained. To construct the third base-\nline, we used the independently trained Convex-\nMST baseline to parse all the training sentences\nand then used the parses to initialize the training\nof LC-DMV. This can be seen as a simple method\nto combine two different approaches. On the other\nhand, we did not use the LC-DMV baseline to ini-\ntialize Convex-MST training because the objective\nfunction of Convex-MST is convex and therefore\nthe initialization does not matter.\n4.2\nResults\nIn Table 1, we compare our jointly trained models\nwith the four baselines. We can see that with joint\ntraining and independent decoding, LC-DMV and\nConvex-MST can achieve superior overall perfor-\nmance than when they are separately trained with\nor without mutual initialization. Joint decoding\nwith our jointly trained models performs worse\nthan independent decoding. We made the same\nobservation when applying joint decoding to the\nseparately trained models (not shown in the table).\nWe believe this is because unsupervised parsers\nhave relatively low accuracy and forcing them to\nreconcile would not lead to better parses. On the\nother hand, joint decoding during training helps\npropagate useful inductive biases between models\nand thus leads to better trained models.\n4.3\nAnalysis of Parsing Results\nWe analyze the parsing results from the two mod-\nels to see how they beneﬁt each other with joint\ntraining.\nNote that LC-DMV limits the depth\nof center embedding and encourages shorter de-\npendency length, while Convex-MST encourages\ndependencies satisfying pre-speciﬁed linguistic\nrules. Therefore, we would like to see whether\nthe jointly-trained LC-DMV produces more de-\npendencies satisfying the linguistic priors than its\nseparately-trained counterpart, and whether the\njointly-trained Convex-MST produces parse trees\nLanguage\nM\nD\nD-I\nM-J\nD-J\nDD\nA Greek\n43.4\n33.1\n38.8\n44.2\n44.9\n38.9\nA Greek-P\n50.4\n43.0\n44.7\n50.8\n52.9\n44.9\nBasque\n50.0\n45.4\n54.2\n52.1\n55.7\n50.2\nBulgarian\n61.6\n62.4\n60.3\n64.7\n73.8\n64.8\nCzech\n48.6\n17.4\n53.9\n48.7\n54.0\n53.5\nCzech-CAC\n50.4\n53.0\n53.9\n55.6\n62.3\n50.2\nDutch\n45.3\n34.1\n56.7\n48.2\n43.5\n40.7\nDutch-LS\n42.4\n27.0\n16.4\n43.2\n41.2\n36.3\nEnglish\n54.0\n56.0\n49.8\n57.3\n60.1\n53.4\nEstonian\n49.4\n31.8\n47.5\n48.7\n44.0\n44.4\nFinnish\n44.7\n26.9\n39.0\n44.2\n43.5\n31.2\nFinnish-FTB\n49.9\n31.0\n47.9\n47.7\n48.0\n36.5\nFrench\n62.0\n48.6\n57.0\n54.5\n57.0\n55.5\nGerman\n51.4\n50.5\n54.1\n49.3\n55.7\n48.6\nGothic\n52.7\n49.9\n47.3\n59.6\n56.4\n58.0\nHindi\n56.8\n54.2\n48.4\n52.1\n60.0\n49.1\nItalian\n69.1\n71.1\n67.4\n62.8\n70.3\n64.5\nJapanese\n44.8\n43.8\n43.8\n42.8\n45.8\n41.0\nLatin-ITTB\n38.8\n38.6\n42.3\n47.0\n42.2\n40.3\nLatin-PROIEL\n44.3\n34.8\n38.7\n46.8\n41.8\n42.9\nNorwegian\n55.3\n45.5\n51.4\n57.4\n60.8\n46.6\nOld Church S\n56.4\n26.6\n51.3\n58.3\n58.6\n42.0\nPolish\n63.4\n63.7\n61.5\n70.7\n74.2\n68.9\nPortuguese\n57.9\n67.2\n60.1\n56.1\n62.9\n57.4\nPortuguese-BR\n59.3\n63.1\n62.0\n65.5\n68.8\n58.3\nRussian-STR\n47.6\n51.7\n56.5\n52.1\n64.4\n52.6\nSlovak\n57.4\n59.3\n51.9\n61.7\n65.9\n58.7\nSlovenian\n54.0\n49.5\n56.3\n65.5\n69.6\n56.1\nSpanish\n61.9\n61.9\n60.3\n57.4\n68.0\n60.2\nSpanish-AC\n59.4\n59.5\n56.4\n56.8\n65.2\n57.6\nAverage\n52.7\n47.2\n50.3\n54.2\n56.5\n49.6\nAverage ≤15\n55.4\n48.9\n54.9\n57.3\n60.2\n53.8\nTable 1: Directed dependency accuracy on thirty datasets\nwith test sentences of length ≤40. The last row indicates\nthe average directed accuracy on sentences of length ≤15.\nM (Convex-MST) and D (LC-DMV) are the independently\ntrained baselines. D-I is the third baseline in which the LC-\nDMV training is initialized by the parses produced from the\ntrained Convex-MST model. With our jointly trained models,\nM-J and D-J denote separate decoding and DD denotes joint\ndecoding.\nwith less center embedding and shorter dependen-\ncies than its separately-trained counterpart.\nFigure 1 shows the percentages of dependen-\ncies satisfying linguistic rules when using the sep-\narately and jointly trained LC-DMV to parse the\ntest sentences in the English dataset. As we can\nsee, with joint training, LC-DMV is indeed inﬂu-\nenced by Convex-MST and produces more depen-\ndencies satisfying linguistic rules.\nTable 2 shows the average dependency length\nwhen using the separately and jointly trained\nConvex-MST to parse the English test dataset.\nThe dependency length can be seen to decrease\nwith joint training, showing the inﬂuence from\nLC-DMV. As to center embedding depth, we ﬁnd\nthat separately trained Convext-MST already pro-\nduces very few center embeddings of depth 2 or\nDependency Arc Percentage\n20\n30\n40\n50\n60\nNoun\nVerb\nAll\n59.8\n32.3\n26.4\n55.9\n31.7\n23.2\nSeparate Training\nJoint Training\nFigure 1: Percentages of dependencies satisfying linguistic\nrules in the LC-DMV parses of the English test dataset. Noun\nand Verb denote dependencies headed by nouns and verbs.\nMethods\nAverage Dependency Length\nSeparate Training\n1.673\nJoint Training\n1.627\nTable 2: Average dependency length in the Convex-MST\nparses of the English test dataset.\nmore, so the inﬂuence from the center embed-\nding constraint of LC-DMV during joint training\nis not obvious.\nWe note that the inﬂuence on\nConvex-MST from LC-DMV during joint train-\ning is relatively small, which may contribute to\nthe much smaller accuracy improvement (1.5%)\nof Convex-MST with joint training in compari-\nson with the 9.3% improvement of LC-DMV. We\nconducted an additional experiment that scaled\ndown the Convex-MST objective in joint train-\ning in order to increase the inﬂuence of LC-DMV.\nThe results show that LC-DMV indeed inﬂuences\nConvex-MST to a greater degree, but the parsing\naccuracies of the two models decrease.\n5\nConclusion\nIn this paper, we proposed a new learning strategy\nfor unsupervised dependency parsing that learns\na generative model and a discriminative model\njointly based on dual decomposition. We show\nthat with joint training, two state-of-the-art mod-\nels can positively inﬂuence each other and achieve\nbetter performance than their separately trained\ncounterparts.\nReferences\nTaylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,\nJohn DeNero, and Dan Klein. 2010. Painless un-\nsupervised learning with features. In Human Lan-\nguage Technologies: The 2010 Annual Conference\nof the North American Chapter of the Association\nfor Computational Linguistics, pages 582–590. As-\nsociation for Computational Linguistics.\nYonatan Bisk and Julia Hockenmaier. 2012. Simple ro-\nbust grammar induction with combinatory categorial\ngrammars.\nShay B Cohen, Kevin Gimpel, and Noah A Smith.\n2008. Logistic normal priors for unsupervised prob-\nabilistic grammar induction. In Advances in Neural\nInformation Processing Systems, pages 321–328.\nShay B Cohen and Noah A Smith. 2009. Shared lo-\ngistic normal distributions for soft parameter tying\nin unsupervised grammar induction.\nIn Proceed-\nings of Human Language Technologies: The 2009\nAnnual Conference of the North American Chap-\nter of the Association for Computational Linguistics,\npages 74–82. Association for Computational Lin-\nguistics.\nGeorge B Dantzig and Philip Wolfe. 1960.\nDecom-\nposition principle for linear programs. Operations\nresearch, 8(1):101–111.\nEdouard Grave and No´emie Elhadad. 2015.\nA con-\nvex and feature-rich discriminative approach to de-\npendency grammar induction. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 1375–1384, Beijing,\nChina. Association for Computational Linguistics.\nYong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-\nsupervised neural dependency parsing. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 763–771,\nAustin, Texas. Association for Computational Lin-\nguistics.\nDan Klein and Christopher D Manning. 2004. Corpus-\nbased induction of syntactic structure: Models of de-\npendency and constituency. In Proceedings of the\n42nd Annual Meeting on Association for Computa-\ntional Linguistics, page 478. Association for Com-\nputational Linguistics.\nNikos Komodakis, Nikos Paragios, and Georgios Tzir-\nitas. 2007. Mrf optimization via dual decomposi-\ntion: Message-passing revisited. In Computer Vi-\nsion, 2007. ICCV 2007. IEEE 11th International\nConference on, pages 1–8. IEEE.\nTerry Koo, Alexander M Rush, Michael Collins,\nTommi Jaakkola, and David Sontag. 2010.\nDual\ndecomposition for parsing with non-projective head\nautomata. In Proceedings of the 2010 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 1288–1298. Association for Compu-\ntational Linguistics.\nTao Lei, Yuan Zhang, Llu´ıs M`arquez, Alessandro\nMoschitti, and Regina Barzilay. 2015. High-order\nlow-rank tensors for semantic role labeling.\nIn\nProceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1150–1160, Denver, Colorado. Association\nfor Computational Linguistics.\nPercy S Liang, Dan Klein, and Michael I. Jordan. 2008.\nAgreement-based learning. In J. C. Platt, D. Koller,\nY. Singer, and S. T. Roweis, editors, Advances in\nNeural Information Processing Systems 20, pages\n913–920. Curran Associates, Inc.\nMingbo Ma, Liang Huang, Bing Xiang, and Bowen\nZhou. 2015. Dependency-based convolutional neu-\nral networks for sentence embedding. arXiv preprint\narXiv:1507.01839.\nRyan McDonald,\nKoby Crammer,\nand Fernando\nPereira. 2005. Online large-margin training of de-\npendency parsers. In Proceedings of the 43rd an-\nnual meeting on association for computational lin-\nguistics, pages 91–98. Association for Computa-\ntional Linguistics.\nHiroshi Noji, Yusuke Miyao, and Mark Johnson. 2016.\nUsing left-corner parsing to encode universal struc-\ntural constraints in grammar induction. In Proceed-\nings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 33–43,\nAustin, Texas. Association for Computational Lin-\nguistics.\nAlexander M. Rush and Michael Collins. 2012. A tuto-\nrial on dual decomposition and lagrangian relaxation\nfor inference in natural language processing. J. Ar-\ntif. Int. Res., 45(1):305–362.\nAlexander M Rush, David Sontag, Michael Collins,\nand Tommi Jaakkola. 2010. On dual decomposition\nand linear programming relaxations for natural lan-\nguage processing. In Proceedings of the 2010 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1–11. Association for Computa-\ntional Linguistics.\nNoah A Smith and Jason Eisner. 2006.\nAnnealing\nstructural bias in multilingual weighted grammar in-\nduction.\nIn Proceedings of the 21st International\nConference on Computational Linguistics and the\n44th annual meeting of the Association for Compu-\ntational Linguistics, pages 569–576. Association for\nComputational Linguistics.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-08-02",
  "updated": "2017-09-24"
}