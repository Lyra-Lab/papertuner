{
  "id": "http://arxiv.org/abs/1708.01018v1",
  "title": "CRF Autoencoder for Unsupervised Dependency Parsing",
  "authors": [
    "Jiong Cai",
    "Yong Jiang",
    "Kewei Tu"
  ],
  "abstract": "Unsupervised dependency parsing, which tries to discover linguistic\ndependency structures from unannotated data, is a very challenging task. Almost\nall previous work on this task focuses on learning generative models. In this\npaper, we develop an unsupervised dependency parsing model based on the CRF\nautoencoder. The encoder part of our model is discriminative and globally\nnormalized which allows us to use rich features as well as universal linguistic\npriors. We propose an exact algorithm for parsing as well as a tractable\nlearning algorithm. We evaluated the performance of our model on eight\nmultilingual treebanks and found that our model achieved comparable performance\nwith state-of-the-art approaches.",
  "text": "CRF Autoencoder for Unsupervised Dependency Parsing∗\nJiong Cai, Yong Jiang and Kewei Tu\n{caijiong,jiangyong, tukw}@shanghaitech.edu.cn\nSchool of Information Science and Technology\nShanghaiTech University, Shanghai, China\nAbstract\nUnsupervised dependency parsing, which\ntries to discover linguistic dependency\nstructures from unannotated data, is a very\nchallenging task.\nAlmost all previous\nwork on this task focuses on learning gen-\nerative models.\nIn this paper, we de-\nvelop an unsupervised dependency pars-\ning model based on the CRF autoencoder.\nThe encoder part of our model is discrim-\ninative and globally normalized which al-\nlows us to use rich features as well as uni-\nversal linguistic priors.\nWe propose an\nexact algorithm for parsing as well as a\ntractable learning algorithm. We evaluated\nthe performance of our model on eight\nmultilingual treebanks and found that our\nmodel achieved comparable performance\nwith state-of-the-art approaches.\n1\nIntroduction\nUnsupervised dependency parsing, which aims to\ndiscover syntactic structures in sentences from un-\nlabeled data, is a very challenging task in natural\nlanguage processing. Most of the previous work\non unsupervised dependency parsing is based on\ngenerative models such as the dependency model\nwith valence (DMV) introduced by Klein and\nManning (2004).\nMany approaches have been\nproposed to enhance these generative models, for\nexample, by designing advanced Bayesian priors\n(Cohen et al., 2008), representing dependencies\nwith features (Berg-Kirkpatrick et al., 2010), and\nrepresenting discrete tokens with continuous vec-\ntors (Jiang et al., 2016).\nBesides generative approaches, Grave and El-\nhadad (2015) proposed an unsupervised discrim-\n∗This work was supported by the National Natural Sci-\nence Foundation of China (61503248).\ninative parser. They designed a convex quadratic\nobjective function under the discriminative clus-\ntering framework.\nBy utilizing global features\nand linguistic priors, their approach achieves state-\nof-the-art performance. However, their approach\nuses an approximate parsing algorithm, which has\nno theoretical guarantee. In addition, the perfor-\nmance of the approach depends on a set of manu-\nally speciﬁed linguistic priors.\nConditional random ﬁeld autoencoder (Ammar\net al., 2014) is a new framework for unsupervised\nstructured prediction. There are two components\nof this model: an encoder and a decoder. The en-\ncoder is a globally normalized feature-rich CRF\nmodel predicting the conditional distribution of\nthe latent structure given the observed structured\ninput. The decoder of the model is a generative\nmodel generating a transformation of the struc-\ntured input from the latent structure. Ammar et\nal.\n(2014) applied the model to two sequential\nstructured prediction tasks, part-of-speech induc-\ntion and word alignment and showed that by uti-\nlizing context information the model can achieve\nbetter performance than previous generative mod-\nels and locally normalized models. However, to\nthe best of our knowledge, there is no previous\nwork applying the CRF autoencoder to tasks with\nmore complicated outputs such as tree structures.\nIn this paper, we propose an unsupervised dis-\ncriminative dependency parser based on the CRF\nautoencoder framework and provide tractable al-\ngorithms for learning and parsing. We performed\nexperiments in eight languages and show that our\napproach achieves comparable results with previ-\nous state-of-the-art models.\narXiv:1708.01018v1  [cs.CL]  3 Aug 2017\nThese stocks eventually reopened\nROOT\nx\ny1\ny2\ny3\ny4\nThese\nstocks\neventually\nreopened\nˆx4\nˆx3\nˆx2\nˆx1\nEncoder\nDecoder\nFigure 1: The CRF Autoencoder for the input sen-\ntence “These stocks eventually reopened” and its\ncorresponding parse tree (shown at the top).\nx\nand ˆx are the original and reconstructed sentence.\ny is the dependency parse tree represented by a\nsequence where yi contains the token and index\nof the parent of token xi in the parse tree, e.g.,\ny1 = ⟨stocks, 2⟩and y2 = ⟨reopened, 4⟩. The\nencoder is represented by a factor graph (with a\nglobal factor specifying valid parse trees) and the\ndecoder is represented by a Bayesian net.\n2\nMethod\n2.1\nModel\nFigure 1 shows our model with an example in-\nput sentence.\nGiven an input sentence x\n=\n(x1, x2, . . . , xn), we regard its parse tree as the\nlatent structure represented by a sequence y =\n(y1, y2, . . . , yn) where yi is a pair ⟨ti, hi⟩, ti is the\nhead token of the dependency connecting to token\nxi in the parse tree, and hi is the index of this head\ntoken in the sentence. The model also contains a\nreconstruction output, which is a token sequence\nˆx = (ˆx1, ˆx2, . . . , ˆxn). Throughout this paper, we\nset ˆx = x.\nThe encoder in our model is a log-linear model\nrepresented by a ﬁrst-order dependency parser.\nThe score of a dependency tree can be factorized\nas the sum of scores of its dependencies. For each\ndependency arc (x, i, j), where i and j are the in-\ndices of the head and child of the dependency, a\nfeature vector f(x, i, j) is speciﬁed. The score of\na dependency is deﬁned as the inner product of the\nfeature vector and a weight vector w,\nφ(x, i, j) = wT f(x, i, j)\nThe score of a dependency tree y of sentence x is\nφ(x, y) =\nn\nX\ni=1\nφ(x, hi, i)\nWe deﬁne the probability of parse tree y given sen-\ntence x as\nP(y|x) = exp(φ(x, y))\nZ(x)\nZ(x) is the partition function,\nZ(x) =\nX\ny′∈Y(x)\nexp(φ(x, y′))\nwhere Y(x) is the set of all valid parse trees of x.\nThe partition function can be efﬁciently computed\nin O(n3) time using a variant of the inside-outside\nalgorithm (Paskin, 2001) for projective tree struc-\ntures, or using the Matrix-Tree Theorem for non-\nprojective tree structures (Koo et al., 2007).\nThe decoder of our model consists of a set of\ncategorical conditional distributions θx|t, which\nrepresents the probability of generating token x\nconditioned on token t. So the probability of the\nreconstruction output ˆx given the parse tree y is\nP(ˆx|y) =\nn\nY\ni=1\nθˆxi|ti\nThe conditional distribution of ˆx, y given x is\nP(y, ˆx|x) = P(y|x)P(ˆx|y)\n2.1.1\nFeatures\nFollowing McDonald et al. (2005) and Grave et\nal. (2015), we deﬁne the feature vector of a de-\npendency based on the part-of-speech tags (POS)\nof the head, child and context words, the direc-\ntion, and the distance between the head and child\nof the dependency. The feature template used in\nour parser is shown in Table 1.\n2.1.2\nParsing\nGiven parameters w and θ, we can parse a sen-\ntence x by searching for a dependency tree y\nwhich has the highest probability P(ˆx, y|x).\ny∗= arg max\ny∈Y(x) log P(ˆx, y|x)\n= arg max\ny∈Y(x)\nn\nX\ni=1\n\u0000φ(x, hi, i) + log θˆxi|ti\n\u0001\nPOSi × dis × dir\nPOSj × dis × dir\nPOSi × POSj × dis × dir\nPOSi × POSi−1 × POSj × dis × dir\nPOSi × POSi+1 × POSj × dis × dir\nPOSi × POSj × POSj−1 × dis × dir\nPOSi × POSj × POSj+1 × dis × dir\nTable 1: Feature template of a dependency, where\ni is the index of the head, j is the index of the\nchild, dis = |i −j|, and dir is the direction of the\ndependency.\nFor projective dependency parsing, we can use\nEisners algorithm (1996) to ﬁnd the best parse\nin O(n3) time.\nFor non-projective dependency\nparsing, we can use the Chu-Liu/Edmond algo-\nrithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan,\n1977) to ﬁnd the best parse in O(n2) time.\n2.2\nParameter Learning\n2.2.1\nObjective Function\nSpitkovsky et al. (2010) shows that Viterbi EM\ncan improve the performance of unsupervised de-\npendency parsing in comparison with EM. There-\nfore, instead of using negative conditional log like-\nlihood as our objective function, we choose to use\nnegative conditional Viterbi log likelihood,\n−\nN\nX\ni=1\nlog\n\u0012\nmax\ny∈Y(xi) P(ˆxi, y|xi)\n\u0013\n+ λΩ(w) (1)\nwhere Ω(w) is a L1 regularization term of the\nencoder parameter w and λ is a hyper-parameter\ncontrolling the strength of regularization.\nTo encourage learning of dependency relations\nthat satisfy universal linguistic knowledge, we add\na soft constraint on the parse tree based on the\nuniversal syntactic rules following Naseem et al.\n(2010) and Grave et al. (2015). Hence our objec-\ntive function becomes\n−\nN\nX\ni=1\nlog\n\u0012\nmax\ny∈Y(xi)P(ˆxi, y|xi)Qα(xi, y)\n\u0013\n+λΩ(w)\nwhere Q(x, y) is a soft constraint factor over the\nparse tree, and α is a hyper-parameter controlling\nthe strength of the constraint factor. The factor Q\nis also decomposable by edges in the same way\nas the encoder and the decoder, and therefore our\nparsing algorithm can still be used with this factor\nVERB →VERB\nNOUN →NOUN\nVERB →NOUN\nNOUN →ADJ\nVERB →PRON\nNOUN →DET\nVERB →ADV\nNOUN →NUM\nVERB →ADP\nNOUN →CONJ\nADJ →ADV\nADP →NOUN\nTable 2: Universal linguistic rules\ntaken into account.\nQ(x, y) = exp\n X\ni\n1[(ti →xi) ∈R]\n!\nwhere 1[(ti →xi) ∈R] is an indicator function\nof whether dependency ti →xi satisﬁes one of\nthe universal linguistic rules in R. The universal\nlinguistic rules that we use are shown in Table 2\n(Naseem et al., 2010).\n2.2.2\nAlgorithm\nWe apply coordinate descent to minimize the ob-\njective function, which alternately updates w and\nθ. In each optimization step of w, we run two\nepochs of stochastic gradient descent, and in each\noptimization step of θ, we run two iterations of the\nViterbi EM algorithm.\nTo update w using stochastic gradient de-\nscent, for each sentence x, we ﬁrst run the pars-\ning algorithm to ﬁnd the best parse tree y∗=\narg maxy∈Y(x)(P(ˆx, y|x)Qα(x, y)); then we can\ncalculate the gradient of the objective function\nbased on the following derivation,\n∂log P(ˆx, y∗|x)\n∂w\n= ∂log P(y∗|x)\n∂w\n+ ∂log P(ˆx|y∗)\n∂w\n= ∂log P(y∗|x)\n∂w\n= ∂\n\u0000Pn\ni=1 wT f(x, hi, i) −log Z(x)\n\u0001\n∂w\n=\nX\n(i,j)∈D(x)\nf(x, i, j)\n\u0012\n1[y∗\nj = ⟨i, xi⟩] −µ(x, i, j)\n\u0013\nwhere D(x) is the set of all possible dependency\narcs of sentence x, 1[·] is the indicator function,\nand µ(x, i, j) is the expected count deﬁned as fol-\nlows,\nµ(x, i, j) =\nX\ny∈Y(x)\n\u00001[yj = ⟨i, xi⟩]P(y|x)\n\u0001\nBasque\nCzech\nDanish\nDutch\nPortuguese\nSlovene\nSwedish\nAvg\nLength: ≤10\nDMV(EM)\n41.1\n31.3\n50.8\n47.1\n36.7\n36.7\n43.5\n41.0\nDMV(Viterbi)\n47.1\n27.1\n39.1\n37.1\n32.3\n23.7\n42.6\n35.5\nNeural DMV (EM)\n46.5\n33.1\n55.6\n49.0\n30.4\n42.2\n44.3\n43.0\nNeural DMV (Viterbi)\n48.1\n28.6\n39.8\n37.2\n36.5\n39.9\n47.9\n39.7\nConvex-MST (No Prior)\n29.4\n36.5\n49.3\n31.3\n46.4\n33.7\n35.5\n37.4\nConvex-MST (With Prior)\n30.0\n46.1\n51.6\n35.3\n55.4\n63.7\n50.9\n47.5\nCRFAE (No Prior)\n49.0\n33.9\n28.8\n39.3\n47.6\n34.7\n51.3\n40.6\nCRFAE (With Prior)\n49.9\n48.1\n53.4\n43.9\n68.0\n52.5\n64.7\n54.3\nLength: All\nDMV(EM)\n31.2\n28.1\n40.3\n44.2\n23.5\n25.2\n32.0\n32.0\nDMV(Viterbi)\n40.9\n20.4\n32.6\n33.0\n26.9\n16.5\n36.2\n29.5\nNeural DMV (EM)\n38.5\n29.3\n46.1\n46.2\n16.2\n36.6\n32.8\n35.1\nNeural DMV (Viterbi)\n41.8\n23.8\n34.2\n33.6\n29.4\n30.8\n40.2\n33.4\nConvex-MST (No Prior)\n30.5\n33.4\n44.2\n29.3\n38.3\n32.2\n28.3\n33.7\nConvex-MST (With Prior)\n30.6\n40.0\n45.8\n35.6\n46.3\n51.8\n40.5\n41.5\nCRFAE (No Prior)\n39.8\n25.4\n24.2\n35.2\n52.2\n26.4\n40.0\n34.7\nCRFAE (With Prior)\n41.4\n36.8\n40.5\n38.6\n58.9\n43.3\n48.5\n44.0\nTable 4: Parsing accuracy on seven languages. Our model is compared with DMV (Klein and Manning,\n2004), Neural DMV (Jiang et al., 2016), and Convex-MST (Grave and Elhadad, 2015)\nMethods\nWSJ10\nWSJ\nBasic Setup\nFeature DMV (Berg-Kirkpatrick et al., 2010)\n63.0\n-\nUR-A E-DMV (Tu and Honavar, 2012)\n71.4\n57.0\nNeural E-DMV (Jiang et al., 2016)\n69.7\n52.5\nNeural E-DMV (Good Init) (Jiang et al., 2016)\n72.5\n57.6\nBasic Setup + Universal Linguistic Prior\nConvex-MST (Grave and Elhadad, 2015)\n60.8\n48.6\nHDP-DEP (Naseem et al., 2010)\n71.9\n-\nCRFAE\n71.7\n55.7\nSystems Using Extra Info\nLexTSG-DMV (Blunsom and Cohn, 2010)\n67.7\n55.7\nCS (Spitkovsky et al., 2013)\n72.0\n64.4\nMaxEnc (Le and Zuidema, 2015)\n73.2\n65.8\nTable 3: Comparison of recent unsupervised de-\npendency parsing systems on English. Basic setup\nis the same as our setup except that linguistic prior\nis not used.\nExtra info includes lexicalization,\nlonger training sentences, etc.\nThe expected count can be efﬁciently computed\nusing the Matrix-Tree Theorem (Koo et al., 2007)\nfor non-projective tree structures or using a variant\nof the inside-outside algorithm for projective tree\nstructures (Paskin, 2001).\nTo update θ using Viterbi EM, in the E-step we\nagain use the parsing algorithm to ﬁnd the best\nparse tree y∗for each sentence x; then in the M-\nstep we update θ by maximum likelihood estima-\ntion.\nθc|t =\nP\nx\nP\ni 1[xi = c, y∗\ni = ⟨·, t⟩]\nP\nc′\nP\nx\nP\ni 1[xi = c′, y∗\ni = ⟨·, t⟩]\n3\nExperiments\n3.1\nSetup\nWe experimented with projective parsing and used\nthe informed initialization method proposed by\nKlein and Manning (2004) to initialize our model\nbefore learning. We tested our model both with\nand without using the universal linguistic rules.\nWe used AdaGrad to optimize w. We used POS\ntags of the input sentence as the tokens in our\nmodel.\nWe learned our model on training sen-\ntences of length ≤10 and reported the directed\ndependency accuracy on test sentences of length\n≤10 and on all test sentences.\n3.2\nResults on English\nWe evaluated our model on the Wall Street Jour-\nnal corpus. We trained our model on section 2-\n21, tuned the hyperparameters on section 22, and\ntested our model on section 23. Table 3 shows the\ndirected dependency accuracy of our model (CR-\nFAE) compared with recently published results. It\ncan be seen that our method achieves a compara-\nble performance with state-of-the-art systems.\nWe also compared the performances of CRF au-\ntoencoder using an objective function with nega-\ntive log likelihood vs. using our Viterbi version\nof the objective function (Eq.1). We ﬁnd that the\nViterbi version leads to much better performance\n(55.7 vs. 41.8 in parsing accuracy of WSJ), which\nechoes Spitkovsky et al. ’s ﬁndings on Viterbi EM\n(2010).\n3.3\nMultilingual Results\nWe evaluated our model on seven languages from\nthe PASCAL Challenge on Grammar Induction\n(Gelling et al., 2012). We did not use the Arabic\ncorpus because the number of training sentences\nwith length ≤10 is less than 1000. The result is\nshown in Table 4. The accuracies of DMV and\nNeural DMV are from Jiang et.al (2016). Both\nour model (CRFAE) and Convex-MST were tuned\non the validation set of each corpus. It can be seen\nthat our method achieves the best results on av-\nerage. Besides, our method outperforms Convex-\nMST both with and without linguistic prior. From\nthe results we can also see that utilizing universal\nlinguistic prior greatly improves the performance\nof Convex-MST and our model.\n4\nConclusion\nIn this paper, we propose a new discriminative\nmodel for unsupervised dependency parsing based\non CRF autoencoder. Both learning and inference\nof our model are tractable. We tested our method\non eight languages and show that our model is\ncompetitive to the state-of-the-art systems.\nThe code is available at https://github.\ncom/caijiong/CRFAE-Dep-Parser\nReferences\nWaleed Ammar, Chris Dyer, and Noah A Smith. 2014.\nConditional random ﬁeld autoencoders for unsuper-\nvised structured prediction. In Advances in Neural\nInformation Processing Systems, pages 3311–3319.\nTaylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,\nJohn DeNero, and Dan Klein. 2010. Painless un-\nsupervised learning with features. In Human Lan-\nguage Technologies: The 2010 Annual Conference\nof the North American Chapter of the Association\nfor Computational Linguistics, pages 582–590. As-\nsociation for Computational Linguistics.\nPhil Blunsom and Trevor Cohn. 2010. Unsupervised\ninduction of tree substitution grammars for depen-\ndency parsing.\nIn Proceedings of the 2010 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1204–1213. Association for Com-\nputational Linguistics.\nYoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest\narborescence of a directed graph. Scientia Sinica,\n14(10):1396.\nShay B Cohen, Kevin Gimpel, and Noah A Smith.\n2008. Logistic normal priors for unsupervised prob-\nabilistic grammar induction. In Advances in Neural\nInformation Processing Systems, pages 321–328.\nJack Edmonds. 1967. Optimum branchings. Journal\nof Research of the National Bureau of Standards B,\n71(4):233–240.\nJason M Eisner. 1996. Three new probabilistic models\nfor dependency parsing: An exploration.\nIn Pro-\nceedings of the 16th conference on Computational\nlinguistics-Volume 1, pages 340–345. Association\nfor Computational Linguistics.\nDouwe Gelling, Trevor Cohn, Phil Blunsom, and Joao\nGrac¸a. 2012. The pascal challenge on grammar in-\nduction. In Proceedings of the NAACL-HLT Work-\nshop on the Induction of Linguistic Structure, pages\n64–80. Association for Computational Linguistics.\nEdouard Grave and No´emie Elhadad. 2015. A convex\nand feature-rich discriminative approach to depen-\ndency grammar induction. In ACL (1), pages 1375–\n1384.\nYong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-\nsupervised neural dependency parsing. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 763–771,\nAustin, Texas. Association for Computational Lin-\nguistics.\nDan Klein and Christopher D Manning. 2004. Corpus-\nbased induction of syntactic structure: Models of de-\npendency and constituency. In Proceedings of the\n42nd Annual Meeting on Association for Computa-\ntional Linguistics, page 478. Association for Com-\nputational Linguistics.\nTerry Koo, Amir Globerson, Xavier Carreras P´erez,\nand Michael Collins. 2007.\nStructured prediction\nmodels via the matrix-tree theorem. In Joint Con-\nference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning (EMNLP-CoNLL), pages 141–150.\nPhong Le and Willem Zuidema. 2015. Unsupervised\ndependency parsing: Let’s use supervised parsers.\nIn Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 651–661, Denver, Colorado. Association for\nComputational Linguistics.\nRyan McDonald,\nKoby Crammer,\nand Fernando\nPereira. 2005. Online large-margin training of de-\npendency parsers. In Proceedings of the 43rd an-\nnual meeting on association for computational lin-\nguistics, pages 91–98. Association for Computa-\ntional Linguistics.\nTahira Naseem, Harr Chen, Regina Barzilay, and Mark\nJohnson. 2010.\nUsing universal linguistic knowl-\nedge to guide grammar induction. In Proceedings of\nthe 2010 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1234–1244. Asso-\nciation for Computational Linguistics.\nMark A Paskin. 2001. Cubic-time parsing and learn-\ning algorithms for grammatical bigram models.\nCiteseer.\nValentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-\nrafsky. 2013.\nBreaking out of local optima with\ncount transforms and model recombination: A study\nin grammar induction.\nIn EMNLP, pages 1983–\n1995.\nValentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,\nand Christopher D Manning. 2010. Viterbi training\nimproves unsupervised dependency parsing. In Pro-\nceedings of the Fourteenth Conference on Computa-\ntional Natural Language Learning, pages 9–17. As-\nsociation for Computational Linguistics.\nRobert Endre Tarjan. 1977. Finding optimum branch-\nings. Networks, 7(1):25–35.\nKewei Tu and Vasant Honavar. 2012.\nUnambiguity\nregularization for unsupervised learning of proba-\nbilistic grammars. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning, pages 1324–1334. Association for\nComputational Linguistics.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-08-03",
  "updated": "2017-08-03"
}