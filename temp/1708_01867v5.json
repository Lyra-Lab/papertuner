{
  "id": "http://arxiv.org/abs/1708.01867v5",
  "title": "An Information-Theoretic Optimality Principle for Deep Reinforcement Learning",
  "authors": [
    "Felix Leibfried",
    "Jordi Grau-Moya",
    "Haitham Bou-Ammar"
  ],
  "abstract": "We methodologically address the problem of Q-value overestimation in deep\nreinforcement learning to handle high-dimensional state spaces efficiently. By\nadapting concepts from information theory, we introduce an intrinsic penalty\nsignal encouraging reduced Q-value estimates. The resultant algorithm\nencompasses a wide range of learning outcomes containing deep Q-networks as a\nspecial case. Different learning outcomes can be demonstrated by tuning a\nLagrange multiplier accordingly. We furthermore propose a novel scheduling\nscheme for this Lagrange multiplier to ensure efficient and robust learning. In\nexperiments on Atari, our algorithm outperforms other algorithms (e.g. deep and\ndouble deep Q-networks) in terms of both game-play performance and sample\ncomplexity. These results remain valid under the recently proposed dueling\narchitecture.",
  "text": "An Information-Theoretic Optimality Principle for\nDeep Reinforcement Learning\nFelix Leibfried, Jordi Grau-Moya, Haitham Bou-Ammar\nPROWLER.io\nCambridge, UK\n{felix,jordi,haitham}@prowler.io\nAbstract\nWe methodologically address the problem of Q-value overestimation in deep\nreinforcement learning to handle high-dimensional state spaces efﬁciently. By\nadapting concepts from information theory, we introduce an intrinsic penalty signal\nencouraging reduced Q-value estimates. The resultant algorithm encompasses a\nwide range of learning outcomes containing deep Q-networks as a special case.\nDifferent learning outcomes can be demonstrated by tuning a Lagrange multiplier\naccordingly. We furthermore propose a novel scheduling scheme for this Lagrange\nmultiplier to ensure efﬁcient and robust learning. In experiments on Atari, our\nalgorithm outperforms other algorithms (e.g. deep and double deep Q-networks)\nin terms of both game-play performance and sample complexity. These results\nremain valid under the recently proposed dueling architecture.\n1\nIntroduction\nReinforcement learning [1] (RL) is a discipline of artiﬁcial intelligence seeking to ﬁnd optimal be-\nhavioral policies that enable agents to collect maximal reward while interacting with the environment.\nA popular RL algorithm is Q-learning [2] that operates by estimating expected cumulative rewards\n(Q-values). Although successful in numerous applications [3], standard Q-learning suffers from two\ndrawbacks. First, due to its tabular nature in representing Q-values, it is not readily applicable to\nhigh-dimensional environments with large state and/or action spaces. Second, it initially overesti-\nmates Q-values, introducing a bias at early stages of training [4]. This bias has to be “unlearned” as\ntraining proceeds, thus decreasing sample efﬁciency.\nTo address the ﬁrst problem, Q-learning has been extended to high-dimensional environments by\nusing parametric function approximators instead of Q-tables [3]. One particularly appealing class of\napproximators are deep neural networks that learn “complex” relationships between high-dimensional\ninputs (e.g. images) and low-level actions. Building on this idea, deep Q-networks (DQNs) [5]\nwere proposed, attaining state-of-the-art results in large-scale domains, e.g. the Arcade Learning\nEnvironment for Atari games [6]. Though successful, DQNs fail to address the overestimation\nproblem, and are therefore rather sample-inefﬁcient [7].\nOne way of addressing Q-value overestimation is to introduce an intrinsic penalty signal in addition\nto instantaneous rewards. The intrinsic penalty affects the learned Q-values, eventually leading\nto lower estimates. Information theory provides a principled method to formalize such a penalty\nby interpreting the agent as an information-theoretic channel with limited transmission rate [8, 9].\nSpeciﬁcally, the state of the environment is interpreted as channel input, the action as channel output\nand the agent’s reward as quality of information transmission [10]. Interestingly, in the RL setting,\nlimits in transmission rate reﬂect limits in “information resources” the agent can spend to deviate\nfrom a given reference policy. The instantaneous deviation between the agent’s current policy and\nsuch a reference policy directly results in an intrinsic penalty to be subtracted from the reward.\nDeep Reinforcement Learning Workshop NIPS 2018\narXiv:1708.01867v5  [cs.AI]  20 Nov 2018\nInformation-theoretic RL approaches [11, 12, 4] have been designed for the tabular setting but do not\nreadily apply to high-dimensional environments that require parametric function approximators.\nSince we are interested in improving sample complexity of RL in high-dimensional state spaces, we\ncontribute by adapting information-theoretic concepts to phrase a novel optimization objective for\nlearning Q-values with deep parametric function approximators. The resultant algorithm encompasses\na wide range of learning outcomes that can be demonstrated by tuning a Lagrange multiplier. We show\nthat DQNs arise as a special case of our proposed approach. We further contribute by introducing a\ndynamic scheduling scheme for adapting the magnitude of intrinsic penalization based on temporal\nBellman error evolution. This allows us to outperform DQN and other methods, such as double DQN\n[7] and soft Q-learning [13], by large margins in terms of game score and sample complexity in the\nAtari domain. At the same time, our approach leads to decreased Q-value estimates, conﬁrming\nour hypothesis that overestimation leads to poor performance in practice. Finally, we show further\nperformance increase by adopting the dueling architecture from [14]. In short, our contributions are:\n1. applying information-theoretic concepts to large state spaces with function approximators;\n2. proposing a novel information-theoretically inspired optimization objective for deep RL;\n3. demonstrating a wide range of learning outcomes for deep RL with DQN as a special case;\n4. and outperforming DQN, double DQN, and soft Q-learning in the Atari domain.\n2\nReinforcement Learning\nIn RL, an agent, being in a state s ∈S, chooses an action a ∈A sampled from a behavioral policy\na ∼πbehave(a|s), where πbehave : S × A →[0, 1]. Resulting from this choice is a transition to a\nsuccessor state s′ ∼P (s′|s, a), where P : S ×A×S →[0, 1] is the unknown state transition model,\nand a reward r = R(s, a) that quantiﬁes instantaneous performance. After subsequent interactions\nwith the environment, the goal of the agent is to optimize for π⋆\nbehave that maximizes the expected\ncumulative return Eπbehave,P [P∞\nt=0 γtrt], with t denoting time and γ ∈(0, 1) the discount factor.\nClearly, to learn an optimal behavioral policy, the agent has to reason about long term consequences of\ninstantaneous actions. Q-learning, a famous RL algorithm, estimates these effects using state-action\nvalue pairs (Q-values) to quantify the performance of the policy. In Q-learning, updates are conducted\nonline after each interaction (s, a, r, s′) with the environment using\nQ (s, a) ←Q(s, a) + α\n\u0010\nr + γ max\na′ Q(s′, a′) −Q (s, a)\n\u0011\n,\n(1)\nwith α > 0 being a learning rate. Equation (1) assumes an old value, i.e. the prediction Q(s, a), and\ncorrects for its estimate based on new information, i.e. the target r + γ maxa′ Q(s′, a′).\nOptimistic Overestimation:\nUpon careful investigation of Equation (1), one comes to recognize\nthat Q-learning updates introduce a bias to the learning process caused by an overestimation of the\noptimal cumulative rewards [15, 16, 17, 18, 4]. Speciﬁcally, the usage of the maximum operator\nassumes that current guesses for Q-values reﬂect optimal cumulative rewards. Of course, this\nassumption is violated, especially early in the learning process, when a relatively small number of\nupdates has been performed. Due to the correlative effect of “bad” estimations between different\nstate-action pairs, these mistakes tend to propagate rapidly through the Q-table and have to be\nunlearned in the course of further training. Though such an optimistic bias is eventually unlearned,\nthe convergence speed (in terms of environmental interactions, i.e. sample complexity) of Q-learning\nis highly dependent on the quality of the initial Q-values.\nThe problem of optimistic overestimation only worsens in large state spaces, such as images in\nAtari. As mentioned earlier, high-dimensional representations are handled by generalizing tabular\nQ-learning to use parametric function approximators, e.g. deep neural networks [5]. Learning then\ncommences by ﬁtting weights of the approximators using stochastic gradients to minimize\nEs,a,r,s′\n\u0014\u0010\nr + γ max\na′ Qθ−(s′, a′) −Qθ(s, a)\n\u00112\u0015\n.\n(2)\nHere, the expectation E refers to samples drawn from a replay memory storing state transitions\n[19], and Qθ−(s′, a′) denotes a DQN at an earlier stage of training. The minimization objective in\n2\nEquation (2) resembles similarities to that used in the tabular setting. Again, old value estimates\nare updated based on new information, while introducing the max-operator bias. Although DQNs\ngeneralize well over a wide range of input states, they are “unaware” of the aforementioned overesti-\nmation problem [20]. However, when compared with the tabular setting, this problem is even more\nsevere due to the lack of any convergence guarantees to optimal Q-values when using parametric\napproximators, and the inability to explore the whole state-action space. Hence, the number of\nenvironmental interactions needed to unlearn the optimistic bias can become prohibitively expensive.\n3\nAddressing Optimistic Overestimation\nA potential solution to optimistic overestimation in Q-learning is to add an intrinsic penalty to\ninstantaneous rewards, thus reducing Q-value estimates. A principled way to introduce such a penalty\nis provided by the framework of information theory for decision-making. The rationale is to interpret\nthe agent as an information-theoretic channel with limited transmission rate [8, 21, 9, 10]. The\nenvironmental state s is considered as channel input, the agent’s action a as channel output and\nthe quality of information transmission is expressed by some reward or utility function U(s, a).\nAccording to Shannon’s noisy-channel coding theorem [22], the transmission rate is upper-bounded\nby the average Kullback-Leibler (KL) divergence between the behavioral policy πbehave and any\narbitrary reference policy with support in A [23, 24]. In the following, the reference policy is denoted\nas prior policy πprior. The KL-divergence, therefore, plays the role of a limited resource and may not\nexceed a maximum K > 0, such that KL (πbehave||πprior) ≤K.\nThe intuition behind the information-theoretic viewpoint is that the channel aims to map input s\nto output a, measuring the quality of the mapping in terms of U(s, a). Since the transmission\nrate is limited, the agent has to discard information in s that has little impact on U to obtain a\nutility-maximizing a without exceeding the transmission limit K. Importantly, the constraint in\ntransmission rate directly translates into an instantaneous penalty signal leading to reduced utility, as\noutlined next for a one-step decision-making problem.\nIn a one-step scenario, we obtain the following\nmax\nπbehave\nX\na∈A\nπbehave(a|s)U(s, a) s.t. KL (πbehave||πprior) ≤K,\nwhere log πbehave(a|s)\nπprior(a|s) reﬂects instantaneous penalty1. The above constrained optimization problem\ncan be expressed as a concave unconstrained objective by introducing a Lagrange multiplier λ > 0:\nL⋆(s, πprior, λ) = max\nπbehave\nX\na∈A\nπbehave(a|s)U(s, a) −1\nλKL (πbehave||πprior) ,\n(3)\nwhere λ trades off utility versus closeness to prior information. The optimum has a closed form:\nπ⋆\nbehave(a|s) =\nπprior(a|s) exp (λU(s, a))\nP\na′ πprior(a′|s) exp (λU(s, a′)).\n(4)\nNote that we are not the ﬁrst to propose such information-theoretic principles within the context\nof RL (and planning), where the utility function is usually assumed to be the expected cumulative\nreward, i.e. U(s, a) = Q(s, a). In fact, similar principles have recently received increased attention\nwithin policy search and identiﬁcation of optimal cumulative reward values, as outlined next.\nIn policy search, information-theoretic principles similar to Equation (3) can be categorized into\nthree classes depending on the choice of the prior πprior(a|s). The ﬁrst class adopts a ﬁxed prior\nthat remains unchanged during learning. Entropy regularisation [25, 26] is a special case within this\nclass (assuming a uniform prior policy). The second class uses a marginal prior policy obtained by\naveraging the behavioral policy over all environmental states. The information-theoretic intuition,\nhere, is to encourage the agent to neglect reward-irrelevant information in the environment [27, 28, 29].\nThe third class assumes an adaptive prior (e.g. a policy learned at an earlier stage of training) to\nensure incremental improvement steps in on-policy settings as learning proceeds [30, 31, 32, 33].\n1Note that although we use a state-independent prior in this work, the theoretical framework for Q-value\nreduction remains valid for state-conditioned πprior(a|s).\n3\nIn optimal cumulative reward value identiﬁcation, the KL-penalty is directly incorporated into Q-\nvalue estimates rather than using it for regularization. There are two distinct categories for value\nidentiﬁcation that utilize KL-constraints in different ways. The ﬁrst category considers a restricted\nclass of Markov Decision processes (MDPs), where instantaneous rewards incorporate a KL-penalty\nthat explicitly discourages deviations from uncontrolled environmental dynamics. Such restricted\nMDPs enable efﬁcient optimal value computation as outlined in [34, 35]. The second category\ncomprises MDPs with intrinsic penalty signals similar to Equation (3) where deviations from a prior\npolicy are penalized. Optimal values are either computed with generalized value iteration schemes\n[21, 36, 37], or in an RL setting similar to Q-learning [11, 12, 4].\nClosest to our work are the recent approaches in [38, 39, 13]. It is worth mentioning that apart from\nthe discrete action and high-dimensional state space setting, we tackle two additional problems not\naddressed previously. First, we consider dynamic adaptation for trading off rewards versus intrinsic\npenalties as opposed to the static scheme presented in [38, 39, 13]. Second, we deploy a robust\ncomputational approach that incorporates value-based advantages to ensure bounded exponentiation\nterms. Our approach also ﬁts into the work of how utilising entropy for reinforcement learning\nconnects policy search to optimal cumulative reward value identiﬁcation [38, 40, 41, 13]. In this\npaper, however, we focus on deep value-based approaches, which show improved performance, as\ndemonstrated in the experiments.\nDue to the intrinsic penalty signal, information-theoretic Q-learning algorithms provide a principled\nway of reducing Q-value estimates and are hence suited for addressing the overestimation problem\noutlined earlier. Although successful in the tabular setting, these algorithms are not readily applicable\nto high-dimensional environments that require parametric function approximators. In the next section,\nwe adapt information-theoretic concepts to high-dimensional state spaces with function approximators\nand demonstrate that other deep learning techniques (e.g. DQNs) emerge as a special case.\n3.1\nAddressing Overestimation in Deep RL\nWe aim to reduce optimistic overestimation in deep RL methodologically by leveraging ideas from\ninformation theory. Since Q-value overestimations are a source of sample-inefﬁciency, we improve\nlarge-scale reinforcement learning where current techniques exhibit high sample complexity [5].\nTo do so, we introduce an intrinsic penalty signal in line with the methodology put forward earlier.\nBefore commencing, however, it can be interesting to gather more insights into the range of possible\nlearners while tuning such a penalty. Plugging the optimal behavior policy π⋆\nbehave from Equation (4)\nback in Equation (3) yields\nL⋆(s, πprior, λ) = 1\nλ log\nX\na∈A\nπprior(a|s) exp (λU(s, a)) .\nThe Lagrange multiplier λ steers the magnitude of the penalty and thus leads to different learning\noutcomes. If λ is large, little penalization from the prior is introduced. As such, one would expect a\nlearning outcome that mostly considers maximizing utility. This is conﬁrmed as λ →∞, where\nlim\nλ→∞L⋆(s, πprior, λ) = max\na∈A U(s, a).\nOn the other hand, for small λ values, the deviation penalty is signiﬁcant and the prior policy should\ndominate. This is again conﬁrmed when λ →0, where we recover the expected utility under πprior:\nlim\nλ→0 L⋆(s, πprior, λ) =\nX\na∈A\nπprior(a|s)U(s, a) = Eπprior [U(s, a)] .\nCarrying this idea to deep RL by setting U(s, a) = Qθ(s, a), where Qθ(s, a) represents a deep\nQ-network, we notice that incorporating a penalty signal in the context of large-scale Q-learning with\nparameterized function approximators leads to\nL⋆\nθ(s, πprior, λ) = 1\nλ log\nX\na∈A\nπprior(a|s) exp (λQθ(s, a)) .\nWe use this operator to phrase an information-theoretic optimization objective for deep Q-learning:\nJλ(θ) = Es,a,r,s′\nh\n(r + γL⋆\nθ−(s′, πprior, λ) −Qθ(s, a))2i\n,\n(5)\n4\nwhere Es,a,r,s′ refers to samples drawn from a replay memory in each iteration of training, and θ−\nto the parameter values at an earlier stage of learning.\nThe above objective leads to a wide variety of learners and can be considered a generalization of\ncurrent methods, including deep Q-networks [5]. Namely, if λ →∞, we recover the approach in [5]\nthat poses the problem of optimistic overestimation:\nJλ→∞(θ) = Es,a,r,s′\n\" \nr + γ max\na′∈A Qθ−(s′, a′) −Qθ(s, a)\n!2#\n.\nOn the contrary, if λ →0, we obtain the following\nJλ→0(θ) = Es,a,r,s′\n\" \nr + γ\nX\na′∈A\nπprior(a′|s′)Qθ−(s′, a′) −Qθ(s, a)\n!2#\n.\n(6)\nEffectively, Equation (6) estimates future cumulative rewards using the prior policy as can be seen in\nthe term P\na′∈A πprior(a′|s′)Qθ−(s′, a′). From the above two special cases, we recognize that our\nformulation allows for a variety of learners, where λ steers outcomes between the above two limiting\ncases. Note, however, setting low values for λ introduces instead a pessimistic bias [4]. Since low\nλ-values introduce a pessimistic bias and large λ-values an optimistic bias, there must be a λ-value in\nbetween encouraging unbiased estimates. Unfortunately, it is not possible to compute such a λ in\nclosed form, which is why we propose a dynamical scheduling scheme based on temporal Bellman\nerror evolution in the next section. Note that we assume a ﬁxed prior πprior and we aim at scheduling\nλ. Another possibility would be to ﬁx λ and schedule the prior action probabilities instead. The latter\nis however practically less convenient compared to scheduling a scalar.\n4\nDynamic & Robust Deep RL\nA ﬁxed hyperparameter λ is undesirable in the course of training as the effect of the intrinsic penalty\nremains unchanged. Since overestimations are more severe at the start of the learning process, a\ndynamic scheduling scheme for λ with small values at the beginning (incurring strong penalization)\nand larger values towards the end (leading to less penalization) is preferable.\nAdaptive λ:\nA suitable candidate for dynamically adapting λ in the course of training is the\naverage squared loss (over replay memory samples) Jsquared(t, p) = (t −p)2 between target values\nt = r + γL⋆\nθ−(s′, πprior, λ) and predicted values p = Qθ(s, a). The rationale, here, is that λ should\nbe inversely proportional to the average squared loss. If Jsquared(t, p) is high on average, as is the\ncase during early episodes of training, low values of λ are favored. However, if Jsquared(t, p) is low\non average later in training, then high λ values are more suitable for the learning process.\nWe therefore propose to adapt λ with a running average over the loss between targets and predictions.\nThe running average Javg should emphasize recent history as opposed to samples that lie further in\nthe past since the parameters θ of the Q-value approximator change over time. This is achieved with\nan exponential window and the online update\nJavg ←\n\u0012\n1 −1\nτ\n\u0013\nJavg + 1\nτ Et,p [Jsquared(t, p)] ,\n(7)\nwhere τ is a time constant referring to the window size of the running average, and Et,p [Jsquared(t, p)]\nis a shorthand notation for Equation (5). This running average allows one to dynamically assign\nλ =\n1\nJavg at each training iteration.\nThe squared loss Jsquared(t, p) has an impeding impact on the stability of deep Q-learning, where\nthe parametric approximator is a deep neural net and parameters are updated with gradients and\nbackpropagation. To prevent loss values from growing too large, the squared loss is practically\nreplaced with an absolute loss if |t −p| > 1 [5], referred to as Huber loss JHuber(t, p). The Huber\nloss leads to a more robust adaptation of λ, as it uses an absolute loss for large errors instead of\na squared one. Furthermore, the squared loss is more sensitive to outliers and might penalize the\nlearning process unreasonably in the presence of sparse but large error values.\n5\nRobust Value Computation:\nThe dynamic adaptation of λ encourages learning of unbiased esti-\nmates of the optimal cumulative reward values. Presupposing Qθ(s, a) is bounded, L⋆\nθ(s, πprior, λ)\nis also bounded in the limits of λ:\nEπprior [Qθ(s, a)] ≤L⋆\nθ(s, πprior, λ) ≤max\na∈A Qθ(s, a).\nIn practice, however, this operator is prone to computational instability for large λ due to the\nexponential term exp (λQθ(s, a)). We address this problem by amending the term exp(λVθ(s))\nexp(λVθ(s)),\nwhere Vθ(s) = maxa Qθ(s, a):\nL⋆\nθ(s, πprior, λ) = 1\nλ log\nX\na∈A\nπprior(a|s) exp (λQθ(s, a)) exp (λVθ(s))\nexp (λVθ(s))\n= Vθ(s) + 1\nλ log\nX\na∈A\nπprior(a|s) exp (λ (Qθ(s, a) −Vθ(s)))\nThe ﬁrst term represents the maximum operator as in vanilla deep Q-learning. The second term\nis a log-partition sum with computationally stable elements due to the non-positive exponents\nλ(Qθ(s, a) −Vθ(s)) ≤0. As a result, the log-partition sum is non-positive and subtracts a portion\nfrom Vθ(s) that reﬂects cumulative reward penalization.\n0\n2\n4\nTraining iterations\n1e7\n4\n6\n8\n10\n12\n0\n2\n4\nTraining iterations\n1e7\n0\n2\n4\nTraining iterations\n1e7\n4000\n5000\n6000\n7000\n8000\n9000\n10000\n0\n2\n4\nTraining iterations\n1e7\n0\n2\n4\n1e7\n10\n15\n20\n25\n30\n0\n2\n4\n1e7\n0\n2\n4\n1e7\n20000\n25000\n30000\n35000\n40000\n45000\n0\n2\n4\n1e7\n0\n2\n4\n1e7\n4\n6\n8\n10\n12\n14\n16\nNormal architecture\n0\n2\n4\n1e7\nDueling architecture\n0\n2\n4\n1e7\n2000\n3000\n4000\n5000\n6000\n7000\nNormal architecture\n0\n2\n4\n1e7\nDueling architecture\nQ-values\nEpisodic rewards\nAsterix\nRoad Runner\nUp'n Down\nFigure 1: Q-values and episodic rewards for Asterix, Road Runner and Up’n Down for both normal\nand dueling architectures. Each plot shows three pairs of graphs, reporting the outcomes of two\ndifferent random seeds, in black for DQN, purple for double DQN (DDQN) and blue for our\ninformation-theoretic approach (DIN). Clearly, our approach leads to lower Q-value estimates\nresulting in signiﬁcantly better game play performance.\n5\nExperiments & Results\nWe hypothesize that addressing the overestimation problem results in improved sample efﬁciency and\noverall performance. To this end, we use the Atari domain [6] as a benchmark to evaluate our method.\nWe compare against deep Q-networks [5] that are susceptible to overestimations, and to double\ndeep Q-networks [7]—an alternative proposed to address the precise problem we target. Our results\ndemonstrate that our proposed method (titled deep information networks DIN) leads to signiﬁcantly\n6\nMedian normalized episodic rewards [%]\n0\n2\n4\nTraining iterations\n1e7\n0\n20\n40\n60\n80\n100\nNormal architecture\n0\n2\n4\nTraining iterations\n1e7\nDueling architecture\nFigure 2: Median normalized episodic rewards across 20 Atari games for normal and dueling\narchitectures. Each plot compares DQN (black), against double DQN (DDQN, purple) and our\napproach (DIN, blue). Our approach leads to signiﬁcantly higher median game score.\nlower Q-value estimates resulting in improved sample efﬁciency and game play performance. We\nalso show that these ﬁndings remain valid for the recently proposed dueling architecture [14]2.\nParameter settings for reproducibility can be found in the appendix. We compare our approach\nagainst deep Q-networks and double deep Q-networks. We conduct further experiments by replacing\nnetwork outputs with the dueling architecture [14]. The dueling architecture leverages the advantage\nfunction A(s, a) = Q(s, a) −maxa Q(s, a) and generalizes learning across actions. This results in\nimproved game play performance, as conﬁrmed in our experiments.\n5.1\nQ-Values and Game Play Performance\nWhen training, networks are stored every 105 iterations and used for ofﬂine evaluation. Evaluating a\nsingle network ofﬂine comprises 100 game play episodes lasting for at most 4.5 × 103 iterations. In\nevaluation mode, the agent follows an ϵ-greedy policy with ϵ = 0.05 [5]. We investigate 20 games.\nFigure 1 reports results from the ofﬂine evaluation on three individual games (Asterix, Road Runner\nand Up’n Down), illustrating average maximum Q-values and average episodic rewards as a function\nof training iterations. Note that episodic rewards are smoothed with an exponential window, similar\nto Equation (7) with τ = 10, to preserve a clearer view. On all three games, our approach leads to\nsigniﬁcantly lower Q-value estimates when compared to DQN and double DQN for both, the normal\nand the dueling architecture (see left plots in Figure 1). At the same time, this leads to signiﬁcant\nimprovements in game play performance (see right plots of Figure 1).\nAbsolute episodic rewards (score) may vary substantially between different games. To ensure compa-\nrability across games, we normalize episodic rewards (scorenorm) as scorenorm =\nscore−scorerandom\nscorehuman−scorerandom ·\n100%, where scorerandom and scorehuman refer to random and human baselines, see [5, 14]. Normalized\nepisodic rewards enable a comparison across all 20 Atari games by taking the median normalized\nscore over games [42]. The results of this analysis are depicted in Figure 2 as a function of training\niterations (smoothed with an exponential window using τ = 10). Our approach clearly outperforms\nDQN and double DQN for both normal and dueling architectures. The dueling architecture yields an\nadditional performance increase when combined with DIN. Our approach also yields superior results\nin terms of the best-performing agent (see the appendix for details).\n5.2\nSample Efﬁciency\nTo quantify sample efﬁciency, we identify the minimal number of training iterations required to attain\nmaximum deep Q-network performance. To this end, we compute the average episodic reward as in\nFigure 1 but smoothed with an exponential window τ = 100. We then identify for each approach the\nnumber of training iterations at which maximum deep Q-network performance is attained ﬁrst.\n2Our approach could be incorporated into the newly released Rainbow framework [42] that achieves state-\nof-the-art results by combining several independent DQN improvements over the past few years (one of them\nbeing double DQNs over which our approach achieves superior performance). Although we focus on Q-value\nidentiﬁcation in this work, ideas similar to DIN do apply as well to actor-critic methods like A3C [26, 13].\n7\nAsterix\nRoad Runner\nUp'n Down\n0\n1\n2\n3\n4\n5\nTraining iterations\n1e7\nNormal architecture\nAsterix\nRoad Runner\nUp'n Down\n0\n1\n2\n3\n5\nTraining iterations\n1e7\nDueling architecture\nNo improvement over DQN\nNormal\nDueling\n0\n1\n2\n3\n4\n5\nTraining iterations\n1e7\nMedian sampling complexity\nNo median improvement over DQN\nFigure 3: Sample efﬁciency for Asterix, Road Runner and Up’n Down under both normal and dueling\narchitectures (left two panels) and when taking the median over 20 games (right panel). The color\ncode is: DQN (black), double DQN (DDQN, purple) and our approach (DIN, blue). DINs are more\nsample-efﬁcient for both architectures on the three games depicted and on average across 20 games.\nThe results for Asterix, Road Runner and Up’n Down are shown in Figure 3 in the left two panels. It\ncan be seen that our approach leads to signiﬁcant improvements in sample efﬁciency when compared\nto DQN and double DQN. For instance, DINs require only about 2 × 107 training iterations in Road\nRunner compared to about 3 × 107 for double DQNs, and about 5 × 107 for standard DQNs using\nthe normal architecture. These improvements are also valid for the dueling setting. In order to assess\nsample efﬁciency across all 20 Atari games, we compute the median sampling efﬁciency over games,\nsee Figure 3 right panel. This analysis conﬁrms the overall improved sample complexity attained in a\nwide range of tasks by our approach compared to DQN and double DQN.\n0\n2\n4\nTraining iterations\n1e7\n2000\n3000\n4000\n5000\n6000\n7000\nNormal architecture\n0\n2\n4\nTraining iterations\n1e7\nDueling architecture\n0\n2\n4\nTraining iterations\n1e7\n2000\n3000\n4000\n5000\n6000\n7000\nNormal architecture\n0\n2\n4\nTraining iterations\n1e7\nDueling architecture\n0\n2\n4\nTraining iterations\n1e7\n0\n10000\n20000\n30000\n40000\nNormal architecture\n0\n2\n4\nTraining iterations\n1e7\nDueling architecture\nAsterix\nBeamrider\nRoad Runner\nEpisodic rewards\nFigure 4: Episodic rewards for Asterix, Beamrider and Road Runner comparing our method to SQL.\nClearly, our results show better performance in both the normal and dueling architecture without the\nnecessity of identifying an optimal λ in advance.\n5.3\nComparison to Soft Q-Learning (SQL)\nThe closest work to our approach is that of [13], where the authors consider information theory to\nbridge the gap between Q-learning and policy gradients RL. Our approach goes further by considering\ndynamic adaptation for λ in the course of training, and introduces robust computation based on value\nadvantages. We compare our method to SQL (where λ is ﬁxed) on the games Asterix, Beamrider and\nUp’n Down. Results depicted in Figure 4 demonstrate that our method can outperform SQL on these\nthree games by signiﬁcant margins without the requirement of pre-specifying λ. For instance, DINs\nachieve the best performance of SQL in about 5,000,000 iterations on the Road Runner game.\n6\nConclusions\nIn this paper, we proposed a novel method for reducing sample complexity in deep reinforcement\nlearning. Our technique introduces an intrinsic penalty signal by adapting principles from information\ntheory to high-dimensional state spaces. We showed that DQNs are a special case of our proposed\napproach for a speciﬁc choice of the Lagrange multiplier steering the intrinsic penalty. Finally, in\na set of experiments on 20 Atari games, we demonstrated that our technique indeed outperforms\ncompeting approaches in terms of performance and sample efﬁciency. These results remain valid for\nthe dueling architecture from [14] yielding a further performance boost.\n8\nReferences\n[1] R S Sutton and A G Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.\n[2] C J C H Watkins. Learning from delayed rewards. PhD thesis, University of Cambridge, 1989.\n[3] L Busoniu, R Babuska, B De Schutter, and D Ernst. Reinforcement Learning and Dynamic\nProgramming using Function Approximators. CRC Press, 2010.\n[4] R Fox, A Pakman, and N Tishby. Taming the noise in reinforcement learning via soft updates.\nIn Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence, 2016.\n[5] V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller,\nA K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran,\nD Wierstra, S Legg, and D Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, 2015.\n[6] M G Bellemare, Y Naddaf, J Veness, and M Bowling. The Arcade Learning Environment: An\nevaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,\n2013.\n[7] H van Hasselt, A Guez, and D Silver. Deep reinforcement learning with double Q-learning. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, 2016.\n[8] C A Sims. Rational inattention and monetary economics. In Handbook of Monetary Economics,\nvolume 3, chapter 4. Elsevier, 2010.\n[9] P A Ortega and D A Braun. Thermodynamics as a theory of decision-making with information-\nprocessing costs. Proceedings of the Royal Society A, 469(2153), 2013.\n[10] T Genewein, F Leibfried, J Grau-Moya, and D A Braun. Bounded rationality, abstraction,\nand hierarchical decision-making: An information-theoretic optimality principle. Frontiers in\nRobotics and AI, 2(27), 2015.\n[11] M G Azar, V Gomez, and H J Kappen. Dynamic policy programming. Journal of Machine\nLearning Research, 13:3207–3245, 2012.\n[12] K Rawlik, M Toussaint, and S Vijayakumar. On stochastic optimal control and reinforcement\nlearning by approximate inference. Proceedings Robotics: Science and Systems, 2012.\n[13] J Schulman, P Abbeel, and X Chen. Equivalence between policy gradients and soft Q-learning.\narXiv, 2017.\n[14] Z Wang, T Schaul, M Hessel, H van Hasselt, M Lanctot, and N de Freitas. Dueling network\narchitectures for deep reinforcement learning. In Proceedings of the International Conference\non Machine Learning, 2016.\n[15] H van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems,\n2010.\n[16] M G Azar, R Munos, M Ghavamzadeh, and H J Kappen. Speedy Q-learning. In Advances in\nNeural Information Processing Systems, 2011.\n[17] D Lee and W B Powell. An intelligent battery controller using bias-corrected Q-learning. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, 2012.\n[18] M G Bellemare, G Ostrovski, A Guez, P S Thomas, and R Munos. Increasing the action gap:\nNew operators for reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 2016.\n[19] L-J Lin. Reinforcement learning for robots using neural networks. PhD thesis, Carnegie Mellon\nUniversity, 1993.\n[20] S Thrun and A Schwartz. Issues in using function approximation for reinforcement learning. In\nProceedings of the Connectionist Models Summer School, 1993.\n[21] N Tishby and D Polani. Information theory of decisions and actions. In Perception-Action\nCycle, chapter 19. Springer, 2011.\n[22] C E Shannon. A mathematical theory of communication. Bell System Technical Journal,\n27:379–423,623–656, 1948.\n[23] I Csiszar and G Tusnady. Information geometry and alternating minimization procedures.\nStatistics and Decisions, (Supplement 1):205–237, 1984.\n9\n[24] N Tishby, F C Pereira, and W Bialek. The information bottleneck method. In Proceedings of\nthe Annual Allerton Conference on Communication, Control, and Computing, 1999.\n[25] R J Williams and J Peng. Function optimization using connectionist reinforcement learning\nalgorithms. Connection Science, 3(3):241–268, 1991.\n[26] V Mnih, A Puigdomenech Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, and\nK Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of\nthe International Conference on Machine Learning, 2016.\n[27] F Leibfried and D A Braun. A reward-maximizing spiking neuron as a bounded rational decision\nmaker. Neural Computation, 27(8):1686–1720, 2015.\n[28] F Leibfried and D A Braun. Bounded rational decision-making in feedforward neural networks.\nIn Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence, 2016.\n[29] Z Peng, T Genewein, F Leibfried, and D A Braun. An information-theoretic on-line update prin-\nciple for perception-action coupling. In Proceedings of the IEEE/RSJ International Conference\non Intelligent Robots and Systems, 2017.\n[30] J A Bagnell and J Schneider. Covariant policy search. Proceedings of the International Joint\nConference on Artiﬁcial Intelligence, 2003.\n[31] J Peters and S Schaal. Reinforcement learning of motor skills with policy gradients. Neural\nNetworks, 21:682–697, 2008.\n[32] J Peters, K Mulling, and Y Altun. Relative entropy policy search. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 2010.\n[33] J Schulman, S Levine, P Moritz, M Jordan, and P Abbeel. Trust region policy optimization. In\nProceedings of the International Conference on Machine Learning, 2015.\n[34] E Todorov. Efﬁcient computation of optimal actions. Proceedings of the National Academy of\nSciences of the United States of America, 106(28):11478–11483, 2009.\n[35] H J Kappen, V Gomez, and M Opper. Optimal control as a graphical model inference problem.\nMachine Learning, 87(2):159–182, 2012.\n[36] J Rubin, O Shamir, and N Tishby. Trading value and information in MDPs. In Decision Making\nwith Imperfect Decision Makers, chapter 3. Springer, 2012.\n[37] J Grau-Moya, F Leibfried, T Genewein, and D A Braun. Planning with information-processing\nconstraints and model uncertainty in Markov decision processes. In Proceedings of the European\nConference on Machine Learning and Principles and Practice of Knowledge Discovery in\nDatabases, 2016.\n[38] T Haarnoja, H Tang, P Abbeel, and S Levine. Reinforcement learning with deep energy-based\npolicies. Proceedings of the International Conference on Machine Learning, 2017.\n[39] T Haarnoja, A Zhou, P Abbeel, and S Levine. Soft actor-critic: Off-policy maximum entropy\ndeep reinforcement learning with a stochastic actor. Proceedings of the International Conference\non Machine Learning, 2018.\n[40] O Nachum, M Norouzi, K Xu, and D Schuurmans. Bridging the gap between value and policy\nbased reinforcement learning. Advances in Neural Information Processing Systems, 2017.\n[41] B O’Donoghue, R Munos, K Kavukcuoglu, and V Mnih. Combining policy gradient and\nQ-learning. Proceedings of the International Conference on Learning Representations, 2017.\n[42] M Hessel, J Modayil, H van Hasselt, T Schaul, G Ostrovski, W Dabney, D Horgan, B Piot,\nM Azar, and D Silver. Rainbow: Combining improvements in deep reinforcement learning. In\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, 2018.\n[43] T Kim. Deep reinforcement learning in TensorFlow, 2016.\n[44] T Tieleman and G E Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of\nits recent magnitude. In COURSERA: Neural Networks for Machine Learning, 2012.\n10\nA\nTraining Details\nWe conduct all experiments in Python with TensorFlow and OpenAI gym extending the GitHub\nproject from [43]. We use a deep convolutional neural network Qθ(s, a) as a function approximator\nfor Q-values, designed and trained according to [5]. Qθ(s, a) receives as input the current state of\nthe environment s that is composed of the last four video frames. The number of neurons in the\noutput layer is set to be the number of possible actions a. Numerical values of each output neuron\ncorrespond to the expected cumulative reward when taking the relevant action in state s.\nWe train the network for 5 × 107 iterations where one iteration corresponds to a single interaction\nwith the environment. Environment interactions (s, a, r, s′) are stored in a replay memory consisting\nof at most 106 elements. Every fourth iteration, a minibatch of size 32 is sampled from the replay\nmemory and a gradient update is conducted with a discount factor γ = 0.99. We use RMSProp\n[44] as the optimizer with learning rate 2.5 × 10−4, gradient momentum 0.95, squared gradient\nmomentum 0.95, and minimum squared gradient 0.01. Rewards r are clipped to {−1, 0, 1}. The\ntarget network Qθ−(s, a) is updated every 104 iterations. The time constant τ for dynamically\nupdating the hyperparameter λ is 105, and the prior policy πprior is uniform. A uniform prior ensures a\npessimistic baseline in case of small λ. This pessimistic baseline guarantees the existence of unbiased\nλ-conﬁgurations our scheduling scheme aims to detect.\nWhen the agent interacts with the environment, every fourth frame is skipped and the current action\nis repeated on the skipped frames. During training, the agent follows an ϵ-greedy policy where ϵ\nis initialized to 1 and linearly annealed over 106 iterations until a ﬁnal value of ϵ = 0.1. Training\nand ϵ-annealing start at 5 × 104 iterations. RGB-images from the Arcade Learning Environment\nare preprocessed by taking the pixel-wise maximum with the previous image. After preprocessing,\nimages are transformed to grey scale and down-sampled to 84 × 84 pixels. All our experiments are\nconducted in duplicate with two different initial random seeds. The random number of NOOP-actions\nat the beginning of each game episode is between 1 and 30.\nB\nPolicy Evaluation\nWe compare the performance of all approaches in terms of the best (non-smoothed) episodic reward\n(averaged over 100 episodes) obtained in the course of the entire evaluation procedure. To ensure\ncomparability between games, we again make use of normalized scores described earlier.\nOur results are summarized for the normal and dueling architecture in Tables 1 and 2 respectively. In\nboth cases, our approach achieves superior median normalized game performance compared to DQN\nand double DQN. In the normal setting, DIN achieves best performance across all three approaches\nin 11 out of 20 games, whereas in the dueling setting, DIN achieves best performance in 13 out\nof 20 games. We can conﬁrm that the dueling architecture, when combined with DIN, leads to a\nperformance increase in 15 out of 20 games, which is not reﬂected in the median performance.\n11\nTable 1: Normalized episodic rewards (normal architecture).\nGAME\nDQN\nDDQN\nDIN\nASSAULT\n198.8%\n214.9%\n233.5%\nASTERIX\n70.4%\n73.4%\n85.0%\nBANK HEIST\n76.2%\n68.3%\n87.8%\nBEAMRIDER\n123.6%\n117.5%\n127.2%\nBERZERK\n35.8%\n33.7%\n22.3%\nDOUBLE DUNK\n161.6%\n265.8%\n165.8%\nFISHING DERBY\n205.4%\n211.2%\n202.4%\nFREEWAY\n103.3%\n75.2%\n101.6%\nKANGAROO\n129.8%\n94.4%\n137.1%\nKRULL\n401.9%\n494.8%\n534.6%\nKUNG FU MASTER\n130.0%\n-0.8%\n144.5%\nQBERT\n22.1%\n31.9%\n21.4%\nRIVERRAID\n14.1%\n20.4%\n26.1%\nROAD RUNNER\n503.6%\n593.2%\n643.7%\nSEAQUEST\n4.0%\n1.2%\n2.9%\nSPACE INVADERS\n53.9%\n51.1%\n54.0%\nSTAR GUNNER\n560.5%\n571.1%\n595.0%\nTIME PILOT\n40.1%\n175.0%\n171.4%\nUP’N DOWN\n131.5%\n135.8%\n135.2%\nVIDEO PINBALL\n4385.8%\n5436.6%\n4654.1%\nMEDIAN\n126.7%\n106.0%\n136.2%\nTable 2: Normalized episodic rewards (dueling architecture).\nGAME\nDQN\nDDQN\nDIN\nASSAULT\n260.4%\n269.5%\n336.6%\nASTERIX\n45.6%\n75.7%\n104.1%\nBANK HEIST\n74.2%\n78.5%\n79.1%\nBEAMRIDER\n130.6%\n128.4%\n129.3%\nBERZERK\n32.5%\n33.4%\n24.6%\nDOUBLE DUNK\n223.9%\n241.6%\n222.6%\nFISHING DERBY\n177.2%\n163.4%\n29.3%\nFREEWAY\n103.8%\n102.9%\n104.6%\nKANGAROO\n347.6%\n186.8%\n437.4%\nKRULL\n480.8%\n433.6%\n574.3%\nKUNG FU MASTER\n114.8%\n118.4%\n129.6%\nQBERT\n70.2%\n84.2%\n82.8%\nRIVERRAID\n28.4%\n22.9%\n22.8%\nROAD RUNNER\n553.6%\n624.4%\n659.0%\nSEAQUEST\n14.4%\n0.5%\n5.1%\nSPACE INVADERS\n63.1%\n26.7%\n140.6%\nSTAR GUNNER\n139.5%\n145.1%\n169.4%\nTIME PILOT\n109.6%\n56.7%\n265.1%\nUP’N DOWN\n105.5%\n125.6%\n150.9%\nVIDEO PINBALL\n4461.6%\n4754.5%\n4982.8%\nMEDIAN\n112.2%\n122.0%\n135.1%\n12\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2017-08-06",
  "updated": "2018-11-20"
}