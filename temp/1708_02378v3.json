{
  "id": "http://arxiv.org/abs/1708.02378v3",
  "title": "Investigating Reinforcement Learning Agents for Continuous State Space Environments",
  "authors": [
    "David Von Dollen"
  ],
  "abstract": "Given an environment with continuous state spaces and discrete actions, we\ninvestigate using a Double Deep Q-learning Reinforcement Agent to find optimal\npolicies using the LunarLander-v2 OpenAI gym environment.",
  "text": " \n1\n  \nAbstractâ€”Given an environment with continuous state spaces and discrete actions, we investigate using a \nDouble Deep Q-learning Reinforcement Agent to find optimal policies using the LunarLander-v2 OpenAI \ngym environment.  \nI. INTRODUCTION \nFor this study, we examine performance of reinforcement learning (RL) algorithms for continuous state space \nMDPs, specifically OpenAI Gymâ€™s LunarLander-v2. In this environment, the goal is for the RL agent to learn to \nland successfully on a landing pad located a coordinate points (0,0) in the frame. The agent receives -0.03 points for \nfiring its main engine for each frame, and landing on the landing pad is 100-140 points, which can be lost if the \nagent moves away from the pad. Each leg contact with the ground is 10 points. The episode is terminated if the \nagent lands or crashes, in which case receiving -100 points for a crash or +100 for a successful landing. The MDP is \nconsidered solved if the agent receives 200 points.  The actions for the agent encompassed firing a left, center, or \nright booster rocket, or doing nothing, with a total of 4 discrete actions. \n \nAlthough generalized reinforcement learning algorithms such as Q-Learning do well in finding optimal policies for \nstochastic MDPs with discrete state and action spaces, problems can arise with state spaces that are continuous.  \nUsually this problem is attributed overestimation of actions values by positive bias of the Q-learner in \napproximating the maximum expected value with the maximum action value. In this case, varying strategies are \nused to approximate the Value function for the Q values, such as binning the continuous state space into discrete \nvariables, or using neural networks and other estimators to approximate value functions. Hado van Hasselt[1]  \nintroduced the concept of Double Q-Learning where two estimators are introduced to separate the concerns of the \nexpected value from the maximization of the action values, and showed that while this method may underestimate \nexpected values,  it generally converges to optimal policies more efficiently in comparison to standard Q-learning. \nII. \nMETHODS \nFor this problem we first considered Q-Learning, where the update rule for the Q-Value function is: \n1  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ğ‘„ğ‘ , ğ‘ Â â†’ğ‘Ÿ+  Â ğ›¾ Â ğ‘šğ‘ğ‘¥! Â ğ‘„(ğ‘ !, ğ‘) Â  \n \nHowever, we ran into problems where the learner was unable to maximize future expected rewards due to the \ncoupling of the maximization of the action in the update. In this case we researched [1] and [2] where van Hasselt et \nal. proposed Double Q-Learning for decoupling the maximization of actions from maximization of expected future \nrewards: \n \n2  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ğ‘„! Â  ğ‘ , ğ‘â†’ğ‘Ÿ+  Â ğ›¾ğ‘„! Â (ğ‘ !, ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥! Â ğ‘„! ğ‘ !, ğ‘) \n \nIn this case we use two separate estimators to map functions of max actions and expected rewards. Additionally, we \nused experience replay in our network updates. This was shown by Minh et. al [5] to improve learning updates.  \nInstead of sequentially learning at each time step in each episode with state s, we stored the (sâ€™, a, r, s ) tuples in a \nsequence S of length M. Before training, we randomly initialized experiences of tuples (sâ€™, a, r, s ), and at training \n \n \nInvestigating Reinforcement Learning Agents for \nContinuous State Space Environments \nDavid Von Dollen \nGeorgia Institute of Technology, djvd3@gatech.edu \n \n2\ntime we update the networks ğ‘„! Â and ğ‘„! Â with samples drawn from S of batch size B. The networks were trained for \none iteration on batches of sampled experiences from memory sequence S, with the two networks exchanging \nweight updates every 1200 action-steps. Additionally for each experience observed, S was updated such that new \nexperiences stored more recently in memory with older experiences dropped such that the sequence length M was \npreserved. For this study, optimal values in ranges for M = 120000 and B = 64 were observed. \n We consider this to be a version of Double \nDeep \nQ-Learning \nin \nthat \nwe \ndecouple \nestimators similar to (2), where we use Qprime \nto approximate action values and Q for reward \napproximation. For network architectures, \nvarious settings for activation functions and \nhidden layers were explored to find optimal \nvalues.  For network architecture, state layers \nof size 8 were fully connected to a first hidden \nlayer, with relu activation functions. This first \nlayer was then fully connected to a second fully \nconnected layer with tanh activation functions. \nThis second layer was then fed to an output \nlayer with linear activations of size 4, equal to \nthe available actions. The range of available \nnetwork hidden layer sizes tested were in range \ncombinations of 64, 128, 256, 512, and 1024 \nneurons. Eventually, the best settings were \nfound using a network with a hidden layer 1 of \n128 neurons and hidden layer 2 of 256 neurons.  \nFor all ranges MSE was used for a loss \nfunction for the neural networks. \nFor each episode, the agent choose a random \naction according to whether or not a random probability was less than ğœ€, if the value exceed the ğœ€ Â threshold, then the \nagent chose action a according to ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥! Â ğ‘„!(ğ‘ ). For each episode a decay was used at time step t where: \n3  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ğœ€! =  Â  1\nğ‘¡\nâˆ— Â ğœ† \nWhere ğœ† was a constant scaling factor for epsilon decay in the range of 0-1. For this study best values for ğœ† were \nobserved in the range of 0.3-0.6 as shown in figure 4. For all trials rewards were summed up over episodes, with \nmoving averages documented in 10 and 100 episode increments.  More details are provided in figure 1. \nIII. RESULTS \nFor training and testing, various network layer sizes and training episode ranges were evaluated. In figure 2., \n \nFigure 2: Cumulative Rewards for various network architectures over training episodes \nFigure 1: Algorithm Pseudo code for DDQN Adapted from [4], [5], [1] \n \n3\nWe observed the best hidden layer sizes to be (128, 256) with \n(64, 128) not converging as rapidly, and (512, 1024) taking \nmuch more time to train as well as having more variance in \nthe upper episodic ranges.  \nFor testing, we evaluated 100 test trials with the trained \nlearner with no updates. Predictably, the average cumulative \nreward increased with more training episodes: with 100 \ntraining episodes scoring -195.234 average rewards, 500 \ntraining episodes scoring 205.519 average rewards, and 1500 \nepisodes scoring 219.95 average rewards. We tested various \noptimizers, with the Adamax optimizer yielding the best \nresults, with a learning rate of 0.002. \n \nWe also evaluated ranges of gamma during this study. \nOverall, using a high value for gamma = 0.99 produced the \nbest performance. This is reasonable, considering that the \ndiscount factor applied further into future time steps had \nmore impact, or actions at the beginning of the episode had a \nlarger impact on the state of the lander at \nthe end of the episode. The best value for \ngamma was 0.99 for this environment.  \nOverall, using these parameters, the \nDDQN agent was able to learn to land in \nrelatively few episodes in the range of \n200-500. One interesting behavioral aspect \nobserved in the agent was, a slight \nâ€œbounceâ€, where the agent learned to \nmaximize rewards for lander leg making \ncontact with the landing pad. This can be \nnoticed in the video located at episode \n1000 in the following link: \nhttps://youtu.be/yDbTL1k9qg4 \n \nIV. CONCLUSION \nThis study illustrated the power in \nDDQNs in learning state/action values in \ncontinuous state space environments.  \nFuture work could entail exploring DDQNs in \nregards to environments with larger action \nnumbers, or partially observable environments. \n \nVIDEO LINK:  \n \nREFERENCES \n[1] \nHado van Hasselt, Arthur Guez, David Silver â€“ Deep Reinforcement Learning with Double Q-learning, arXiv:1509.06461, 2016 \n[2] \nHado van Hasselt â€“ Double Q-learning, Advances in Neural Information Processing Systems, 2010 \n[3] \nMnih et al. â€“ Human-level control through deep reinforcement learning, Nature 518, 2015 \n[4] \nhttps://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/ \n[5] \nhttps://www.cs.toronto.edu/~vmnih/docs/dqn.pdf \n[6] \nHado van Hasselt â€“ Reinforcement Learning in Continuous State Spaces, Springer, 2012 \n \n \n \nFigure 3: Cumulative Rewards For test trails on \nlearners trained with various episodic ranges \nFigure 4: Training performance for various ranges for \nlambda and gamma \n",
  "categories": [
    "cs.AI"
  ],
  "published": "2017-08-08",
  "updated": "2019-03-11"
}