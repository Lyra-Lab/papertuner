{
  "id": "http://arxiv.org/abs/1708.03615v1",
  "title": "Unsupervised Incremental Learning of Deep Descriptors From Video Streams",
  "authors": [
    "Federico Pernici",
    "Alberto Del Bimbo"
  ],
  "abstract": "We present a novel unsupervised method for face identity learning from video\nsequences. The method exploits the ResNet deep network for face detection and\nVGGface fc7 face descriptors together with a smart learning mechanism that\nexploits the temporal coherence of visual data in video streams. We present a\nnovel feature matching solution based on Reverse Nearest Neighbour and a\nfeature forgetting strategy that supports incremental learning with memory size\ncontrol, while time progresses. It is shown that the proposed learning\nprocedure is asymptotically stable and can be effectively applied to relevant\napplications like multiple face tracking.",
  "text": "UNSUPERVISED INCREMENTAL LEARNING OF DEEP DESCRIPTORS\nFROM VIDEO STREAMS\nFederico Pernici and Alberto Del Bimbo\nMICC ‚Äì University of Florence\nfederico.pernici@uniÔ¨Å.it, alberto.delbimbo@uniÔ¨Å.it\nABSTRACT\nWe present a novel unsupervised method for face identity\nlearning from video sequences.\nThe method exploits the\nResNet deep network for face detection and VGGface fc7\nface descriptors together with a smart learning mechanism\nthat exploits the temporal coherence of visual data in video\nstreams. We present a novel feature matching solution based\non Reverse Nearest Neighbour and a feature forgetting strat-\negy that supports incremental learning with memory size con-\ntrol, while time progresses. It is shown that the proposed\nlearning procedure is asymptotically stable and can be effec-\ntively applied to relevant applications like multiple face track-\ning.\n1. INTRODUCTION\nVisual data is massive and is growing faster than our ability to\nstore and index it, nurtured by the diffusion and widespread\nuse of social platforms. Their fundamental role in advancing\nobject representation, object recognition and scene classiÔ¨Åca-\ntion research have been undoubtedly assessed by the achieve-\nments of Deep Learning [1].\nHowever, the cost of super-\nvision, as necessary for effective training, remains the most\ncritical fact for the applicability of such learning methods. Ef-\nforts to collect large quantities of annotated images, such as\nImageNet [2], Microsoft coco [3], Megaface [4] and Visual\nGenome [5], while having had an important role in advancing\nobject recognition, don‚Äôt have the necessary scalability and\nare hard to be extended or replicated. Semi or unsupervised\nDeep Learning from image data still remains hard to achieve.\nAn attracting alternative would be to learn the object ap-\npearance from video streams with no supervision, both ex-\nploiting the large quantity of video available in the Internet\nand the fact that adjacent video frames contain semantically\nsimilar information, so providing the variety of conditions in\nwhich an object can be framed, and therefore a comprehen-\nsive representation of its appearance. According to this, track-\ning a subject in the video could, at least in principle, support\na sort of unsupervised incremental learning of its appearance.\nThis would avoid or reduce the cost of annotation as time it-\nself would provide a form of weak supervision. However, this\nsolution is not free of problems. On the one hand, parameter\nre-learning of Deep Networks, to adequately incorporate the\nnew information without catastrophic interference, is still an\nopen challenge [6, 7], especially when re-learning should be\ndone in real time, while tracking. On the other hand, classic\nobject tracking has substantially divergent goals from contin-\nuous incremental learning. While in tracking the object ap-\npearance is learned only for detecting the object in the next\nframe (the past information is gradually forgotten), continu-\nous incremental learning would require that all the past vi-\nsual information of the object is collected in a comprehen-\nsive representation. This requires that tracking does not drift\nin the presence of occlusions or appearance changes, and at\nthe same time memory overÔ¨Çow is avoided by retaining only\nthe most distinctive descriptors. Finally, incremental learn-\ning should be asymptotically stable in order to converge to an\nunivocal representation.\nIn this paper, we present unsupervised learning of subject\nidentities from video streams that exploits the ResNet deep\nnetwork [8] to detect faces in consecutive images and fc7 deep\ndescriptor of VGGface [9] for face representation, together\nwith a smart incremental learning mechanism that collects\nsuch descriptors and distills the most distinctive ones of them,\nin order to provide a sufÔ¨Åciently complete representation of\nthe individual identities and at the same time avoid memory\noverÔ¨Çow. It is shown that under reasonable assumptions our\nlearning procedure is asymptotically stable. The incremen-\ntal learning mechanism has been inspired by the research on\nlong-term tracking described in [10]. In that system distinc-\ntive SIFT local features of a target were detected in each frame\nand their descriptors were all collected in a template with ran-\ndom forgetting of the past. This ultimately allowed to learn a\ntemporally-updated collection of local features that was use-\nful to track the target in the long term with almost no drifting.\nIn the following, in Section 2, we cite a few works that\nhave been of inspiration for our work. In Section 3, we high-\nlight our contributions and expounded the approach in detail\nand Ô¨Ånally, in Section 4, some experimental results are given.\n978-1-5090-6067-2/17/$31.00 c‚Éù2017 IEEE\narXiv:1708.03615v1  [cs.CV]  11 Aug 2017\nùê®1\nùê®2\nùëÜ\nùê±ùëñ\nFig. 1. Reverse Nearest Neighbor for a repeated temporal\nvisual structure with the distance ratio criterion. All elements\nxi match with o1, for clarity only one of them is highlighted.\n2. RELATED WORK\nOne key point of the method is the exploitation of video tem-\nporal coherence as a form of weak supervision. This idea was\nsuggested by [11], but was essentially applied with some suc-\ncess to predict future frames with unsupervised feature learn-\ning, [12] among the most notable experiments.\nInclusion of a memory mechanisms in learning is another\nkey feature of our approach. Works on parameters re-learning\non domains that have some temporal coherence, have used re-\ninforcement learning, [13] [14] among the most recent ones.\nThey typically store the past experience in a replay memory\nwith some priority and sample mini-batches for training. This\nmakes it possible to break the temporal correlations by mix-\ning more and less recent experiences. More recently, Neural\nTuring Machine architectures have been proposed in [15] and\n[16] that implement an augmented memory to quickly encode\nand retrieve new information. These architectures have the\nability to rapidly bind never-before-seen information after a\nsingle presentation via an external memory module. How-\never, in these cases, training data are still provided supervis-\nedly and the methods don‚Äôt scale with massive video streams.\nFinally, another relevant research subject to our learning\nsetting is long-term object tracking [17]. Only a few works\non tracking have reported drift-free results on on very long\nvideo sequences ( [18, 19, 10, 20, 21] among the few), and\nonly few of them have provided convincing evidence on\nthe possibility of incremental learning strategies that are\nasymptotically stable [18] [10]. However, all of these works\nonly address tracking and perform incremental learning just\nto detect the target in the next frame.\n3. THE PROPOSED APPROACH\nIn our system, fc7 face descriptors of VGGface deep network\nare computed on face windows detected by ResNet and stored\nin a memory module as:\nM(t) = {(xi, Idi, ei)}N(t)\ni=1\n(1)\nwhere xi is the descriptor computed at the fc7layer, Idi is the\nobject identity (an incremental number), ei is the eligibility\nfactor (discussed in the following) and N(t) is the number of\ndescriptors at time t in the memory module.\nAs video frames are observed, new faces are detected and\ntheir descriptors are matched with those already in the mem-\nory. Each newly observed oi descriptor, will be assigned with\nthe object identity of its closest neighbour. Unmatched de-\nscriptors of the faces in the incoming frame are stored in the\nmemory module with a new Id. They ideally represent faces\nof new individuals that have not been observed already and\nwill eventually appear in the following frames. While match-\ning these descriptors with those already in the memory per-\nmits to track each individual face properly through the video\nframes, two distinct problems must however be solved in or-\nder to collect all the distinguishing descriptors and learn a\ncomprehensive identity of each observed subject. They re-\nspectively are concerned with matching in consecutive frames\nand control of the memory module. These are separately ad-\ndressed in the following subsections.\n3.1. Reverse Nearest Neighbour matching\nWhile tracking the faces in consecutive frames, it is likely\nthat the face of the same individual will have little differences\nfrom one frame to the following. In this case, highly sim-\nilar descriptors will be stored in the memory and quickly a\nnew face descriptor of the same individual will have compa-\nrable distances to the nearest and the second nearest descrip-\ntor already in the memory. In this case, the Nearest Neigh-\nbor (NN) classiÔ¨Åer distance-ratio [22] does not work properly\nand matching cannot be assessed. We solved this problem by\nperforming descriptor matching according to Reverse Nearest\nNeighbour (ReNN) [23]:\nM‚ãÜ=\nn\n(xi, Idi, ei) ‚ààM(t) | ||xi ‚àí1NNIt(xi)||\n||xi ‚àí2NNIt(xi)|| < ¬ØœÅ,\no\n(2)\nwhere ¬ØœÅ is the distance ratio threshold, xi is a face descrip-\ntor in the memory module and 1NNIt(xi) and 2NNIt(xi) are\nrespectively its nearest and second nearest neighbor face de-\nscriptor in the incoming frame It.\nFig. 1 shows the effects of this change of perspective: here\ntwo new observations are detected (two distinct faces, respec-\ntively marked as o1 and o2). They both have distance ra-\ntio close to 1 to the nearest xis in the memory (the dots in\nthe grey region). Therefore both their matchings are undecid-\nable. Differently from NN, ReNN is able to correctly detect\nthe nearest descriptor for each new descriptor in the incom-\ning frame. In fact, with ReNN, the roles of xi and oi are\nexchanged and the distance ratio is computed between each\nxi and the oi as shown in Ô¨Ågure for one of the xis (the yel-\nlow dot is associated to the newly observed red dot). Due to\nthe fact that with ReNN a large number of descriptors (those\naccumulated in the memory module) is matched against a rel-\natively small set of descriptors (those observed in the current\nimage), calculation of the ratio between distances could be\ncomputationally expensive if sorting is applied to the entire\nset. However, minimum distances can be efÔ¨Åciently obtained\nby performing twice a linear search, with parallel implemen-\ntation on GPU.\n3.2. Memory size control\nDescriptors that have been matched according to ReNN\nideally represent different appearances of a same subject\nface. However, collecting these descriptors indeÔ¨Ånitely could\nquickly determine memory overload. To detect redundant de-\nscriptors and discard them appropriately, we deÔ¨Åned a dimen-\nsionless quantity ei referred to as eligibility. This is set to\nei = 1 as a descriptor is entered in the memory module and\nhence decreased at each match with a newly observed descrip-\ntor, proportionally to the distance ratio:\nei(t + 1) = Œ∑i ei(t).\n(3)\nEligibility allows to take into account both spatial redundancy\n(close descriptors) and temporal updating (only the most re-\ncent matched descriptors are retained). In fact, as the eligi-\nbility ei of a face descriptor xi in the memory drops below a\ngiven threshold ¬Øe (that happens after a number of matches),\nthat descriptor is removed from the memory module:\nif (ei < ¬Øe) then M(t + 1) = M(t) \\ {(xi, Idi, ei)}.\n(4)\nThe value Œ∑i is computed according to:\nŒ∑i = 1\n¬ØœÅ\n\u0014d1\ni\nd2\ni\n\u0015Œ±\n,\n(5)\nwhere d1\ni and d2\ni are respectively the distances between xi\nand its Ô¨Årst and second nearest neighbour oi, the value ¬ØœÅ is\nthe distance-ratio threshold of Eq. 2, used for normalization\nand Œ± emphasizes the effect of the distance-ratio.\nSome of the features collected in the memory will never\nobtain matches. This effect is largely due to scene occluders\nor object features with very low repeatability. In the long run\nsuch features may waste critical space in the memory buffer.\nThey are handled by considering a max time lapse in which a\ndescriptor has not not been matched, after which the descrip-\ntor is discarded. The threshold can be set reasonably large to\navoid deletion of rare but useful descriptors.\n3.3. Asymptotic stability\nUnder the assumption that descriptors are sufÔ¨Åciently distinc-\ntive (as in the case of VGGface fc7 descriptors), the incremen-\ntal learning procedure described above stabilizes asymptoti-\ncally around the probability density function of the descrip-\ntors of each individual subject face. A key element which\nguarantees such theoretical asymptotic stability is that the\nReNN distance ratio is always below 1. In fact, it is easy\nto demonstrate that the updating rule of Eq. 3 is a contrac-\ntion and converges to its unique Ô¨Åxed point 0 according to the\nContraction Mapping theorem (Banach Ô¨Åxed-point theorem).\nThe asymptotic stability of the method and its robustness\nto erroneous matches is illustrated in Fig. 2 with a simple one-\ndimensional case. Two patterns of synthetic descriptors, re-\nspectively modeling the inliers and the outliers of some object\nidentity, are generated by two distinct 1D Gaussian distribu-\ntions. The learning method was ran for 1000 iterations for\nthree different conÔ¨Ågurations of the two distributions. The\nblue points represent the eligibility of the descriptors of the\nobject instances detected. The red curve is the inliers pdf. The\nblack curve is the outlier pdf. The histogram in yellow rep-\nresents the distribution of the inliers as incrementally learned\nby the system. The inliers pdf models the case of true pos-\nitive detections (the object descriptors have little differences\nfrom each other as subsequent frames have little motion and\ncontext changes). The outliers pdf instead models the case of\nfalse positives detected (the standard deviation of the descrip-\ntors is therefore larger). Mismatches might therefore corrupt\nthe inlier distribution.\nThe three Ô¨Ågures represent distinct cases in which the out-\nlier pdf is progressively overlapping the inlier pdf. When the\noutliers are sufÔ¨Åciently far from the inliers, that is the descrip-\ntors are distinctive (top), the density of the eligibility closely\nfollows the density of the data distribution (samples that are\nclose to the pdf mode are matched more frequently and their\neligibility decreases accordingly). As the outliers pdf gets\ncloser to the inliers pdf, the ReNN matching mechanism and\nthe memory control mechanism still keep the learned inlier\npdf close to the ground truth pdf (medium, bottom). Close\noutliers only determine some little bias of the distribution.\nDespite of the fact that part of the symmetric and peaked\nshape of the eligibility is lost, the mode and standard devi-\nation of the distribution does not substantially change.\n4. LEARNING FROM A VIDEO STREAM\nEvaluation of performance of our solution for unsupervised\nlearning from tracking in video streams presents some difÔ¨Å-\nculties. While for supervised learning cross-validation is typ-\nically used, and train and test datasets are availble that permit\ndirect comparison between learning methods, unfortunately,\nfor our unsupervised incremental learning, cross-validation\ncannot be applied. In fact it is impossible to know in advance\nthe exemplars of the moving objects and their variations [24].\nWe provide here two experiments that respectively show\nperformance of our learning mechanism in terms of preci-\nsion/recall at different learning rounds and the capability of\n-10\n-5\n0\n5\n10\n15\n0\n0.2\n0.4\n0.6\n0.8\n1\nEligibility of a sample\nInliers pdf\nOutliers pdf\nEstimated inliers pdf\n-10\n-5\n0\n5\n10\n15\n0\n0.2\n0.4\n0.6\n0.8\n1\nEligibility of a sample\nInliers pdf\nOutliers pdf\nEstimated inliers pdf\n-10\n-5\n0\n5\n10\n15\n0\n0.2\n0.4\n0.6\n0.8\n1\nEligibility of a sample\nInliers pdf\nOutliers pdf\nEstimated inliers pdf\nFig. 2. Asymptotic stability of incremental learning of a face\nidentity in a sample sequence\n.\nthe learning mechanism to support a multiple face tracking\napplication.\nIn the Ô¨Årst experiment, we collected a number of YouTube\nvideos (185) that contain the face of a well known public per-\nson, namely the former US President Barack Obama, with\nhigh probability (retrieved from the query: ‚ÄùBarack Obama‚Äù),\nfor a total of 58 hours (6.264.000 frames). These videos even-\ntually include the face of President Obama at different times,\nin either indoor or outdoor settings, under different illumi-\nnation and occlusion conditions. The original resolution of\nVideo data was 640 √ó 360. Video frames were re-sized to\n320√ó240 pixels and images to 320√ó240 pixel. These videos\nwere used as snapshots for training only.\nHence we issued the same query on Google Images, and\ncollected 5000 images of Barack Obama. Additional 5000\nimages that did not contain the subject were included in the\nimage dataset. We assumed that the videos and images sets\nwere someway correlated and that video data were sufÔ¨Åcient\nto learn some views of the appearances of the face of Pres-\nident Obama. All the images were manually annotated and\nused during test. The image test set was split in two subsets\nof 5000 images each. One (Subset A) was used to improve\nthe quality of learning and the other (Subset B) to derive a\nmeasure of performance of the learning mechanism.\nThe learner, trained on the videos, was applied to image\nsubset A repeatedly, for several passes, so that at each pass\nnew descriptors of the face of President Obama were included\nin the memory module and the eligibilities of the descriptors\nin the memory were updated so that the learner incrementally\nlearns more of the Obama‚Äôs face. At each pass the learner\nwas hence run onto test image subset B (for which a ground\ntruth is known) and the Precision and Recall were evaluated.\nFig.3 explains the internals of the learning mechanism. Three\n1¬∞ pass\n2¬∞ pass\n3¬∞ pass\nFig. 3. Histogram of matches at different overlaps for differ-\nent passes over image subset A, and corresponding Precision\nand recall values computed on image Subset B. Recall is com-\nputed considering 0.5 matching threshold.\ndifferent steps are shown with the histograms of the matches\nat different overlaps between the bounding box predicted by\nthe detector and the ground truth. The blue histogram (Ô¨Ånal\npass) reveals a distribution close to the ground truth (the sin-\ngle brown bar). in the white box, the values of Precision and\nRecall at the same passes are shown. It is clearly visible the\nconvergence of the learning mechanism. As a new pass over\nthe image subset A is performed, the performance increases\nsubstantially, nearly up to the optimal result.\nIn the second experiment we directly applied the incre-\nmental learner system to a video sequence and veriÔ¨Åed (qual-\nitatively) its capability to learn the distinct identities of the\nfaces unsupervisedly. Fig.4 shows a few frames of the video\nclip of ‚ÄùBruno Mars‚Äù from the dataset [25] and the learned\nidentities (the numbers superimposed to the faces detected).\nThe video clip was composed putting together different shots\ntaken in different places, where the faces of the characters\nappear in different orientations, illumination conditions, and\nposes. In some cases the same individual (see f.e. individ-\nual number 6) appears sensibly diverse (with and without\nhat). Nonetheless it can be observed that the learning mecha-\nnism is sufÔ¨Åciently capable to exploit the temporal coherence\nbetween frames and collect the distinguishing descriptors of\neach individual so that its identity is not lost.\n5. CONCLUSION\nIn this paper we exploited deep network based face detection\nand fc7 VGGface descriptor coupled with a novel learning\nmechanism that learns face identities from video sequences\nunsupervisedly, exploiting the temporal coherence of video\nframes.\nThe proposed method is simple, theoretically\nsound, asymptotically stable and follows the cumulative and\nconvergent nature of human learning.\nFig. 4. The unsupervised learning method applied to Multiple Face Tracking. Selected frames from the BrunoMars video\nsequence with the superimposed estimated identities are shown.\nAcknowledgment\nThis research is based upon work supported in part by the\nOfÔ¨Åce of the Director of National Intelligence (ODNI), In-\ntelligence Advanced Research Projects Activity (IARPA), via\nIARPA contract number 2014-14071600011. The views and\nconclusions contained herein are those of the authors and\nshould not be interpreted as necessarily representing the of-\nÔ¨Åcial policies or endorsements, either expressed or implied,\nof ODNI, IARPA, or the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute reprints for\nGovernmental purpose notwithstanding any copyright anno-\ntation thereon.\n6. REFERENCES\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, ‚ÄúIma-\ngenet classiÔ¨Åcation with deep convolutional neural networks.,‚Äù\nin NIPS, 2012, vol. 1, p. 4.\n[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nLi Fei-Fei,\n‚ÄúImagenet:\nA large-scale hierarchical image\ndatabase,‚Äù in Computer Vision and Pattern Recognition, 2009.\nCVPR 2009. IEEE Conference on. IEEE, 2009, pp. 248‚Äì255.\n[3] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick, ‚ÄúMicrosoft coco: Common objects in context,‚Äù in\nEuropean Conference on Computer Vision. Springer, 2014, pp.\n740‚Äì755.\n[4] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller,\nand Evan Brossard, ‚ÄúThe megaface benchmark: 1 million faces\nfor recognition at scale,‚Äù in Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 2016.\n[5] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji\nHata, Joshua Kravitz, Stephanie Chen, Yannis Kalanditis, Li-\nJia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei,\n‚ÄúVisual genome: Connecting language and vision using crowd-\nsourced dense image annotations,‚Äù 2016.\n[6] Zhizhong Li and Derek Hoiem, ‚ÄúLearning without forgetting,‚Äù\nin European Conference on Computer Vision. Springer, 2016,\npp. 614‚Äì629.\n[7] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hu-\nbert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan\nPascanu, and Raia Hadsell,\n‚ÄúProgressive neural networks,‚Äù\narXiv preprint arXiv:1606.04671, 2016.\n[8] Peiyun Hu and Deva Ramanan, ‚ÄúFinding tiny faces,‚Äù arXiv\npreprint arXiv:1612.04402, 2016.\n[9] O. M. Parkhi, A. Vedaldi, and A. Zisserman, ‚ÄúDeep face recog-\nnition,‚Äù in British Machine Vision Conference, 2015.\n[10] Federico Pernici and Alberto Del Bimbo, ‚ÄúObject tracking by\noversampling local features,‚Äù IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 99, no. PrePrints, pp.\n1, 2013.\n[11] Xiaolong Wang and Abhinav Gupta, ‚ÄúUnsupervised learning\nof visual representations using videos,‚Äù in Proceedings of the\nIEEE International Conference on Computer Vision, 2015, pp.\n2794‚Äì2802.\n[12] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba, ‚ÄúAn-\nticipating the future by watching unlabeled video,‚Äù\narXiv\npreprint arXiv:1504.08023, 2015.\n[13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex\nGraves, Ioannis Antonoglou, Daan Wierstra, and Martin Ried-\nmiller, ‚ÄúPlaying atari with deep reinforcement learning,‚Äù arXiv\npreprint arXiv:1312.5602, 2013.\n[14] Tom Schaul, John Quan, Ioannis Antonoglou, and David\nSilver,\n‚ÄúPrioritized experience replay,‚Äù\narXiv preprint\narXiv:1511.05952, 2015.\n[15] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan\nWierstra,\nand Timothy Lillicrap,\n‚ÄúOne-shot learning\nwith memory-augmented neural networks,‚Äù\narXiv preprint\narXiv:1605.06065, 2016.\n[16] Alex Graves, Greg Wayne, and Ivo Danihelka, ‚ÄúNeural turing\nmachines,‚Äù arXiv preprint arXiv:1410.5401, 2014.\n[17] ‚ÄúLong term detection and tracking workshop.,‚Äù in Conjunction\nWith The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops (LTDT2014), June 2014.\n[18] Z. Kalal, J. Matas, and K. Mikolajczyk, ‚ÄúP-n learning: Boot-\nstrapping binary classiÔ¨Åers by structural constraints,‚Äù in CVPR,\njune 2010.\n[19] Thang Ba Dinh, Nam Vo, and G. Medioni, ‚ÄúContext tracker:\nExploring supporters and distracters in unconstrained environ-\nments,‚Äù in CVPR, june 2011.\n[20] Yang Hua, Karteek Alahari, and Cordelia Schmid, ‚ÄúOcclusion\nand motion reasoning for long-term tracking,‚Äù in Computer\nVision‚ÄìECCV 2014, pp. 172‚Äì187. Springer, 2014.\n[21] Zhibin Hong, Zhe Chen, Chaohui Wang, Xue Mei, Danil\nProkhorov, and Dacheng Tao, ‚ÄúMulti-store tracker (muster):\nA cognitive psychology inspired approach to object tracking,‚Äù\nJune 2015.\n[22] David G. Lowe,\n‚ÄúDistinctive image features from scale-\ninvariant keypoints,‚Äù IJCV, vol. 60, pp. 91‚Äì110, 2004.\n[23] Flip Korn and S. Muthukrishnan, ‚ÄúInÔ¨Çuence sets based on re-\nverse nearest neighbor queries,‚Äù in Proceedings of the 2000\nACM SIGMOD International Conference on Management of\nData, New York, NY, USA, 2000, SIGMOD ‚Äô00, pp. 201‚Äì212,\nACM.\n[24] JoÀúao Gama, IndrÀôe ÀáZliobaitÀôe, Albert Bifet, Mykola Pechenizkiy,\nand Abdelhamid Bouchachia, ‚ÄúA survey on concept drift adap-\ntation,‚Äù ACM Comput. Surv., vol. 46, no. 4, pp. 44:1‚Äì44:37,\nMar. 2014.\n[25] Shun Zhang, Yihong Gong, Jia-Bin Huang, Jongwoo Lim, Jin-\njun Wang, Narendra Ahuja, and Ming-Hsuan Yang, ‚ÄúTrack-\ning persons-of-interest via adaptive discriminative features,‚Äù in\nEuropean Conference on Computer Vision. Springer, 2016, pp.\n415‚Äì433.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-08-11",
  "updated": "2017-08-11"
}