{
  "id": "http://arxiv.org/abs/1708.03704v1",
  "title": "Deep Incremental Boosting",
  "authors": [
    "Alan Mosca",
    "George D Magoulas"
  ],
  "abstract": "This paper introduces Deep Incremental Boosting, a new technique derived from\nAdaBoost, specifically adapted to work with Deep Learning methods, that reduces\nthe required training time and improves generalisation. We draw inspiration\nfrom Transfer of Learning approaches to reduce the start-up time to training\neach incremental Ensemble member. We show a set of experiments that outlines\nsome preliminary results on some common Deep Learning datasets and discuss the\npotential improvements Deep Incremental Boosting brings to traditional Ensemble\nmethods in Deep Learning.",
  "text": "arXiv:1708.03704v1  [stat.ML]  11 Aug 2017\nDeep Incremental Boosting\nAlan Mosca and George D. Magoulas\nDepartment of Computer Science and Information Systems\nBirkbeck, University of London\nMalet Street, London - UK\nAugust 15, 2017\nAbstract\nThis paper introduces Deep Incremental Boosting, a new technique derived\nfrom AdaBoost, speciﬁcally adapted to work with Deep Learning methods, that\nreduces the required training time and improves generalisation. We draw inspira-\ntion from Transfer of Learning approaches to reduce the start-up time to training\neach incremental Ensemble member. We show a set of experiments that outlines\nsome preliminary results on some common Deep Learning datasets and discuss the\npotential improvements Deep Incremental Boosting brings to traditional Ensemble\nmethods in Deep Learning.\n1\nIntroduction\nAdaBoost [9] is considered a successful Ensemble method and is commonly used in\ncombination with traditional Machine Learning algorithms, especially Boosted Deci-\nsion Trees [3]. One of the main principles behind it is the additional emphasis given to\nthe so-called hard to classify examples from a training set.\nDeep Neural Networks have also had great success on many visual problems, and\nthere are a number of benchmark datasets in this area where the state-of-the-art results\nare held by some Deep Learning algorithm [12, 4].\nIdeas from Transfer of Learning have found applications in Deep Learning; for ex-\nample, in Convolutional Neural Networks (CNNs), when sub-features learned early in\nthe training process can be carried forward to a new CNN in order to improve general-\nisation on a new problem of the same domain [13]. It has also been shown that these\nTransfer of Learning methods reduce the “warm-up” phase of the training, where a\nrandomly-initialised CNN would have to re-learn basic feature selectors from scratch.\nIn this paper, we explore the synergy of AdaBoost and Transfer of Learning to\naccelerate this initial warm-up phase of training each new round of boosting. The pro-\nposed method, named Deep Incremental Boosting, exploits additional capacity embed-\nded into each new round of boosting, which increases the generalisation without adding\nmuch training time. When tested in Deep Learning benchmarks, the new method is able\nto beat traditional Boosted CNNs on benchmark datasets, in a shorter training time.\n1\nThe paper is structured as follows. Section 2 presents an overview of prior work on\nwhich the new development is based. Section 3 presents the new learning algorithm.\nSection 4 reports the methodology of our preliminary experimentation and the results.\nSection 5 provides examples where state-of-the-art models have been used as the base\nclassiﬁers for Deep Incremental Boosting. Lastly, Section 6 makes conclusions on our\nexperiments, and shows possible avenues for further development.\n2\nPrior Work\nThis section gives an overview of previous work and algorithms on which our new\nmethod is based.\n2.1\nAdaBoost\nAdaBoost [9] is a well-known Ensemble method, which has a proven track record\nof improving performance. It is based on the principle of training Ensemble mem-\nbers in “rounds”, and at each round increasing the importance of training examples\nthat were misclassiﬁed in the previous round. The ﬁnal Ensemble is then aggregated\nusing weights α0..N calculated during the training. Algorithm 1 shows the common\nAdaBoost.M2 [10] variant. This variant is generally considered better for multi-class\nproblems, such as those used in our experimentation, however the same changes we\napply to AdaBoost.M2 can be applied to any other variant of AdaBoost.\nAlgorithm 1 AdaBoost.M2\nm = |X0|\nD0(i) = 1/m for all i\nt = 0\nB = {(i, y) : i ∈{1, . . . , m}, y ̸= yi}\nwhile t < T do\nXt ←pick from original training set X0 with distribution Dt\nht ←train new classiﬁer on Xt\nǫt = 1\n2\nP\n(i,y)∈B Dt(i)(1 −ht(xi, yi) + ht(xi, y))\nβt = ǫt/(1 −ǫt)\nDt+1(i) = Dt(i)\nZt\n· β(1/2)(1+ht(xi,yi)−ht(xi,y))\nwhere Zt is a normalisation factor such that Dt+1 is a distribution\nt = t + 1\nend while\nH(x) = argmaxy∈Y\nPT\nt=1(log 1\nβt ht(x, y)\n2.2\nTransfer of Learning applied to Deep Neural Networks\nOver the last few years a lot of progress has been made in the Deep Networks area\ndue to their ability to represent features at various levels of resolution. A recent study\n2\nanalysed how the low-layer features of Deep Networks are transferable and can be\nconsidered general in the problem domain of image recognition [13]. More speciﬁcally\nit has been found that, for example, the ﬁrst-layer of a CNN tends to learn ﬁlters that are\neither similar to Gabor ﬁlters or color blobs. Ref [2] studies Transfer of Learning in an\nunsupervised setting on Deep Neural Networks and also reached a similar conclusion.\nIn supervised Deep Learning contexts, transfer of learning can be achieved by set-\nting the initial weights of some of the layers of a Deep Neural Network to those of a\npreviously-trained network. Because of the ﬁndings on the generality of the ﬁrst few\nlayers of ﬁlters, this is traditionally applied mostly to those ﬁrst few layers. The train-\ning is then continued on the new dataset, with the beneﬁt that the already-learned initial\nfeatures provide a much better starting position than randomly initialised weights, and\nas such the generalisation power is improved and the time required to train the network\nis reduced.\n3\nDeep Incremental Boosting\n3.1\nMotivation\nTraditional AdaBoost methods, and related variants, re-train a new classiﬁer from\nscratch every round. While this, combined with the weighted re-sampling of the train-\ning set, appears at ﬁrst glance to be one of the elements that create diversity in the ﬁnal\nEnsemble, it may not be necessary to re-initialize the Network from scratch at every\nround.\nIt has already been previously shown that weights can be transferred between Net-\nworks, and in particular between subsets of a network, to accelerate the initial training\nphase. In the case of Convolutional Neural Networks, this particular approach is par-\nticularly fruitful, as the lower layers (those closest to the input) tend to consistently\ndevelop similar features.\n3.2\nApplying Transfer of Learning to AdaBoost\nIntuition 1 Because each subsequent round of AdaBoost increases the importance\ngiven to the errors made at the previous round, the network ht at a given round can be\nrepurposed at round t + 1 to learn the newly resampled training set.\nIn order for this to make sense, it is necessary to formulate a few conjectures.\nDeﬁnition 1 Let Xa be a set composed of n training example vectors Xa = {xa,1, xa,2, . . . xa,n},\nwith its corresponding set of correct label vectors Ya = {ya,1, ya,2, . . . ya,n}\nDeﬁnition 2 A training set Xa is mostly similar to another set Xb if the sets of unique\ninstances Xa and Xb have more common than different elements, and the difference\nset is smaller than an arbitrary signiﬁcant amount ǫ.\nThis can be expressed equivalently as:\n|Xa ∩Xb| >> |Xa ⊖Xb|\n(1)\n|Xa ∩Xb| >> |Xa −Xb| + |Xb −Xa|\n(2)\n3\nor\n|Xa ∪Xb| = |Xa ∩Xb| + ǫ\n(3)\nGiven the Jaccard Distance\nJ(A, B) = |A ∩B|\n|A ∪B|\n(4)\nthis can be formulated as\nJ(Xa, Xb) ≥1 −ǫ\n(5)\nConjecture 1 At a round of Boosting t + 1, the resampled training set Xt+1 and the\nprevious resampled training set Xt are mostly similar, as in Deﬁnition 2:\n|Xt ∩Xt+1| >> |Xt −Xt+1| + |Xt+1 −Xt|\n(6)\nor\n|Xt ∪Xt+1| = |Xt ∩Xt+1| + ǫ\n(7)\nIf we relax ǫ to be as large as we like, In the case of Boosting, we know this to be\ntrue because both Xt and Xt+1 are resampled from X0 with the weighting Dt+1 from\nthe initial dataset Xt=0, so the unique sets Xt and Xt+1 are large resampled subsets of\nthe initial training set Xt=0:\nXt ⊆Xt=0\n(8)\nXt+1 ⊆Xt=0\n(9)\n|Xt| = |Xt+1| = |Xt=0|\n(10)\nDeﬁnition 3 We introduce a mistake function E(ha, Xb) which counts the number of\nmistakes by the classiﬁer ha on dataset Xb:\nE(ha, Xb, Y ) =\n\f\f\f{xb,i|ha(xb,i) ̸= yi∀xb,i ∈Xb}\n\f\f\f\n(11)\nwhere yi is the ground truth for example i, taken from the correct label set y.\nConjecture 2 Given Conjecture 1 and provided that the dataset Xt and Xt+1 are\nmostly similar as per Deﬁnition 2, a classiﬁer ht that classiﬁes Xt better than randomly\nwill still perform better than randomly on a new dataset Xt+1.\nGiven that all sets are of the same size by deﬁnition, as they are resampled that\nway, we can ignore the fact that the error count on a dataset E(ht, Xt) would need to\nbe divided by the size of the dataset |Xt|, thus simplifying the notation.\nWe can therefore redeﬁne the errors made by ht on both Xt and Xt+1 as:\nE(ht, Xt, Y ) = E(ht, Xt ∩Xt+1, Y ) + E(ht, Xt −Xt+1, Y )\n(12)\nE(ht, Xt+1, Y ) = E(ht, Xt ∩Xt+1, Y ) + E(ht, Xt+1 −Xt, Y )\n(13)\n4\nFrom Conjecture 1, the last two terms are negligible, leaving:\nE(ht, Xt, Y ) = E(ht, Xt ∩Xt+1, Y ) + ǫt\n(14)\nE(ht, Xt+1, Y ) = E(ht, Xt ∩Xt+1, Y ) + ǫt+1\n(15)\ntherefore E(ht, Xt, Y ) ≈E(ht, Xt+1, Y ).\nAssumption 1 The weights and structure of a classiﬁer ht that correctly classiﬁes the\ntraining set Xt will not differ greatly from the classiﬁer ht+1 that correctly classiﬁes\nthe training set Xt+1, provided that the two sets are mostly similar.\nConjecture 3 Given Conjecture 2 classiﬁer ht and its classiﬁcation output Yt, it is\npossible to construct a derived classiﬁer ht+1 that learns the corrections on the resid-\nual set Xt+1 −Xt.\nWhen using Boosting in practice, we ﬁnd these assumptions to be true most of the\ntime. We can therefore establish a procedure by which we preserve the knowledge\ngained from round t into the next round t + 1:\n1. At t = 0, a new CNN is trained with random initialisations on the re-sampled\ndataset X0, for N iterations.\n2. The new dataset Xt+1 is selected. The calculation of the error ǫt, the sam-\npling distribution Dt and the classiﬁer weight αt remain the same as per Ad-\naBoost.M2.\n3. At every subsequent round, the struture of network ht is copied and extended\nby one additional hidden layer, at a given position in the network i, and all the\nlayers below i are copied into the new network. By doing so, we preserve the\nknowledge captured in the previous round, but allow for additional capacity to\nlearn the corrections on Xt+1−Xt. This new network is trained for M iterations,\nwhere M << N.\n4. Steps 2 and 3 are repeated iteratively until the number of rounds has been ex-\nhausted.\nBecause the Network ht+1 doesn’t have to re-learn basic features, and already in-\ncorporates some knowledge of the dataset, the gradients for the lower layers will be\nsmaller and the learning will be concentrated on the newly added hidden layer, and\nthose above it. This also means that all classiﬁers ht>1 will require a smaller number\nof epochs to converge, because many of the weights in the network are already starting\nfrom a favourable position to the dataset.\nAt test time, the full group of hypotheses is used, each with its respective weight\nαt, in the same way as AdaBoost.\nAlgoritm 2 shows the full algorithm in detail.\n5\nAlgorithm 2 Deep Incremental Boosting\nD0(i) = 1/M for all i\nt = 0\nW0 ←randomly initialised weights for ﬁrst classiﬁer\nwhile t < T do\nXt ←pick from original training set with distribution Dt\nut ←create untrained classiﬁer with additional layer of shape Lnew\ncopy weights from Wt into the bottom layers of ut\nht ←train ut classiﬁer on current subset\nWt+1 ←all weights from ht\nǫt = 1\n2\nP\n(i,y)∈B Dt(i)(1 −ht(xi, yi) + ht(xi, y))\nβt = ǫt/(1 −ǫt)\nDt+1(i) = Dt(i)\nZt\n· β(1/2)(1+ht(xi,yi)−ht(xi,y))\nwhere Zt is a normalisation factor such that Dt+1 is a distribution\nαt =\n1\nβt\nt = t + 1\nend while\nH(x) = argmaxy∈Y\nPT\nt=1(logαtht(x, y)\n4\nExperimental Analysis\nEach experiment was repeated 20 times, both for AdaBoost.M2 and Deep Incremental\nBoosting, using the same set of weight initialisations (one for each run), so that any\npossible ﬂuctuation due to favourable random starting conditions was neutralised. Each\nvariant ran for a ﬁxed 10 rounds of boosting. We trained each Ensemble member using\nAdam[5], and used a hold-out validation set to select the best model.\nAll the experiments were run on an Intel Core i5 3470 cpu with a nVidia GTX1080\nGPU using the toupee Ensemble library available online at https://github.com/nitbix/toupee.\nCode and parameters for these experiments is available online at https://github.com/nitbix/ensemble-\n4.1\nDatasets\n4.1.1\nMNIST\nMNIST [7] is a common computer vision dataset that associates pre-processed images\nof hand-written numerical digits with a class label representing that digit. The input\nfeatures are the raw pixel values for the 28 × 28 images, in grayscale, and the outputs\nare the numerical value between 0 and 9.\nThe CNN used for MNIST has the following structure:\n• An input layer of 784 nodes, with no dropout\n• 64 5 × 5 convolutions, with no dropout\n• 2 × 2 max-pooling\n6\n• 128 5 × 5 convolutions, with no dropout\n• 2 × 2 max-pooling\n• A fully connected layer of 1024 nodes, with 50% dropout\n• a Softmax layer with 10 outputs (one for each class)\nThis network has ≈2.3 million weights.\nThe layer added during each round of Deep Incremental Boosting is a convolutional\nlayer of 64 3 × 3 channels, with no dropout, added after the second max-pooling layer.\n4.1.2\nCIFAR-10\nCIFAR-10 is a dataset that contains 60000 small images of 10 categories of objects. It\nwas ﬁrst introduced in [6]. The images are 32 × 32 pixels, in RGB format. The output\ncategories are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. The\nclasses are completely mutually exclusive so that it is translatable to a 1-vs-all multi-\nclass classiﬁcation. Of the 60000 samples, there is a training set of 40000 instances,\na validation set of 10000 and a test set of another 10000. All sets have perfect class\nbalance.\nThe CNN used for CIFAR-10 has the following structure:\n• An input layer of 3096 nodes, with no dropout\n• 64 3 × 3 convolutions, with 25% dropout\n• 64 3 × 3 convolutions, with 25% dropout\n• 2 × 2 max-pooling\n• 128 3 × 3 convolutions, with 25% dropout\n• 128 3 × 3 convolutions, with 25% dropout\n• 2 × 2 max-pooling\n• 256 3 × 3 convolutions, with 25% dropout\n• 256 3 × 3 convolutions, with 25% dropout\n• 2 × 2 max-pooling\n• A fully connected layer of 1024 nodes, with 50% dropout\n• a Softmax layer with 10 outputs (one for each class)\nThis network has ≈5.4 million weights.\nThe layer added during each round of Deep Incremental Boosting is a convolutional\nlayer of 128 3×3 channels, with no dropout, added after the second max-pooling layer.\n7\nSingle Network\nAdaBoost.M2\nDeep Incremental Boosting\nCIFAR-10\n25.10 %\n23.57 %\n19.37 %\nCIFAR-100\n58.34 %\n57.09 %\n53.49 %\nMNIST\n0.68 %\n0.63 %\n0.55 %\nTable 1: Mean misclassiﬁcation rate on the test set\nAdaBoost.M2\nDeep Incremental Boosting\nCIFAR-100\n19\n6\nCIFAR-10\n18\n4\nMNIST\n14\n3\nTable 2: Typical “best epoch” during the 10th round of Boosting\n4.2\nCIFAR-100\nCIFAR-100 is a dataset that contains 60000 small images of 100 categories of objects,\ngrouped in 20 super-classes. It was ﬁrst introduced in [6]. The image format is the\nsame as CIFAR-10. Class labels are provided for the 100 classes as well as the 20\nsuper-classes. A super-class is a category that includes 5 of the ﬁne-grained class\nlabels (e.g. “insects” contains bee, beetle, butterﬂy, caterpillar, cockroach). Of the\n60000 samples, there is a training set of 40000 instances, a validation set of 10000 and\na test set of another 10000. All sets have perfect class balance.\nThe model we used has the same structure as the one we trained on CIFAR-10.\n4.3\nResults\nWe can see from these preliminary results in Table 1 that Deep Incremental Boosting\nis able to generalise better than AdaBoost.M2. We have also run AdaBoost.M2 with\nlarger CNNs, up to the size of the largest CNN used in Deep Incremental Boosting\n(e.g. with 10 additional layers) and found that the classiﬁcation performance was grad-\nually getting worse as the weak learners were overﬁtting the training set. We therefore\nassume that the additional capacity alone was not sufﬁcient to justify the improved\ngeneralisation and it was speciﬁcally due to the transferred weights from the previous\nround, and the new layer learning the “corrections” from the new training set.\n4.4\nTraining Time\nWe see from Table 2 that with Deep Incremental Boosting the best validation error is\nreached much earlier during the last boosting round. This conﬁrms our observation in\nSection 3 that the learning would converge at an earlier epoch at subsequent rounds\n(t > 1). Based on this we have used a shorter training schedule for these subsequent\nrounds, which means that we were able to save considerable time compared to the\noriginal AdaBoost, even though we have trained a network with a larger number of\nparameters. A summary of the improved training times is provided in Table 3.\n8\nAdaBoost.M2\nDeep Incremental Boosting\nCIFAR-100\n≈26hrs\n≈8hrs\nCIFAR-10\n≈9hrs\n≈3hrs\nMNIST\n≈4hrs\n≈1hrs\nTable 3: Mean training times for each dataset\nMethod\nMean Test Misclassiﬁcation\nMean Training Time\nNiN\n0.46%\n≈18 min\nAdaBoost.M2\n0.47%\n≈207 min\nDIB\n0.42%\n≈38 min\nTable 4: Network-in-Network results on MNIST\n5\nLarger models\nThe base classiﬁers we used in the experimentation in Section 4 are convenient for\nlarge numbers of repetitions with lock-stepped random initialisations, because they\ntrain relatively quickly. The longest base classiﬁer to train is the one used for CIFAR-\n100 and it took ≈3 hours. However, these models give results that are still far from\nthe state-of-the-art, so we experimented further with some of these more complicated\nmodels and applied Deep Incremental Boosting.\nBecause of the time required to train each model, and the differences in the prob-\nlem setup, we have not been able to run them with the same schedule as the main\nexperiments, therefore they have been documented separately.\n5.1\nMNIST\nThe best result on MNIST that doesn’t involve data augmentation or manipulation is\nobtained by applying Network in Network [8]. In the paper, a full model is described,\nwhich we have been able to reproduce. Because our goal is to train Ensembles quickly,\nwe reduced the training schedule to 100 epochs and applied Adam as the update rule,\nwhich also sped up the training signiﬁcantly. This network has a total of ≈0.35 million\nweights, however, it has a signiﬁcantly higher number of computations.\nAfter the ﬁrst dropout layer, we added a new convolutional layer of 64 5 × 5 ﬁlters,\nat each Deep Incremental Boosting round.\nTable 4 shows that, although the remaining examples to be learned are very few,\nDIB is able to improve where AdaBoost no longer offers any beneﬁts. In addition to\nthis, the training time has been reduced signiﬁcantly compared to AdaBoost.\n5.2\nCIFAR-10\nThe published models that achieve state-of-the-art performance on CIFAR-10 and CIFAR-\n100 do not make use of a hold-out validation set. Instead, they use the additional 10000\nexamples as additional training data. In order to reproduce similar test error results, the\nsame principle was applied to this experimental run.\n9\nMethod\nMean Test Misclassiﬁcation\nMean Training Time\nSingle Network\n16.10%\n104 mins\nAdaBoost.M2\n16.00%\n320 mins\nDIB\n15.10%\n220 mins\nTable 5: All-CNN results on CIFAR-10\nA very efﬁcient model of all- convolutional networks has been proposed, with state-\nof-the-art results on the CIFAR-10 dataset, which replaces the max-pooling with an\nadditional convolution with stride s > 1, and does not use a fully-connected layer after\nthe convolutions [11]. Instead, there are further convolutions to reduce the dimension-\nality of the output, until it is possible to perform Global Average Pooling. We based\nour larger model on this architecture, but in order to make the computations feasible\nfor an Ensemble we had to modify it slightly. The ﬁnal structure of the network is as\nfollows:\n• An input layer of 3096 nodes, with no dropout\n• 128 3 × 3 convolutions, with 25% dropout\n• 128 3 × 3 convolutions, with 25% dropout\n• 128 3 × 3 convolutions, with a stride length of 2\n• 256 3 × 3 convolutions, with 25% dropout\n• 256 3 × 3 convolutions, with 25% dropout\n• 256 3 × 3 convolutions, with a stride length of 2\n• 512 3 × 3 convolutions, with 25% dropout\n• 512 3 × 3 convolutions, with 25% dropout\n• 512 3 × 3 convolutions, with a stride length of 2\n• 2 × 2 max-pooling\n• A fully connected layer of 1024 nodes, with 50% dropout\n• a Softmax layer with 10 outputs (one for each class)\nThis network has ≈9.4 million weights and is considerably harder to train than the\none in the original experiment. The results are reported in Table 5, including training\ntime and a comparison with vanilla AdaBoost.\nEach original member was trained for 40 epochs, while each round of Deep In-\ncremental Boosting after the ﬁrst was only trained for 20 epochs. No additional layer\nwas created, due to GPU memory limitations, which is why the improvement is not as\ndramatic as seen in the original experiments. However, the time improvement alone is\nsufﬁcient to justify using this new method.\n10\n6\nConcluding Remarks\nIn this paper we have introduced a new algorithm, called Deep Incremental Boost-\ning, which combines the power of AdaBoost, Deep Neural Networks and Transfer of\nLearning principles, in a Boosting variant which is able to improve generalisation. We\nthen tested this new algorithm and compared it to AdaBoost.M2 with Deep Neural Net-\nworks and found that it generalises better on some benchmark image datasets, further\nsupporting our claims.\nOne ﬁnal observation that can be made is about the fact that we are still using the\nentire Ensemble at test time. In certain situations, it has been shown that a small model\ncan be trained to replicate a bigger one, without signiﬁcant loss of generalisation [1].\nIn future work we will investigate the possibility to modify Deep Incremental Boosting\nsuch that only one ﬁnal test-time Deep Neural Network will be necessary.\nReferences\n[1] Lei Jimmy Ba and Rich Caurana. Do deep nets really need to be deep? Advances\nin neural information processing systems, pages 2654–2662, 2014.\n[2] Yoshua Bengio. Deep learning of representations for unsupervised and transfer\nlearning. Unsupervised and Transfer Learning Challenges in Machine Learning,\n7:19, 2012.\n[3] Thomas G Dietterich. An experimental comparison of three methods for con-\nstructing ensembles of decision trees: Bagging, boosting, and randomization.\nMachine learning, 40(2):139–157, 2000.\n[4] Benjamin Graham. Fractional max-pooling. CoRR, abs/1412.6071, 2014.\n[5] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\n[6] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from\ntiny images, 2009.\n[7] Yann Lecun and Corinna Cortes. The MNIST database of handwritten digits.\n[8] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint\narXiv:1312.4400, 2013.\n[9] R. E. Schapire. The strength of weak learnability. Machine Learning, 5:197–227,\n1990.\n[10] R. E. Schapire and Y Freund. Experiments with a new boosting algorithm. Ma-\nchine Learning: proceedings of the Thirteenth International Conference, pages\n148–156, 1996.\n11\n[11] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Ried-\nmiller.\nStriving for simplicity:\nThe all convolutional net.\narXiv preprint\narXiv:1412.6806, 2014.\n[12] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regulariza-\ntion of neural networks using dropconnect. In Proceedings of the 30th Interna-\ntional Conference on Machine Learning (ICML-13), pages 1058–1066, 2013.\n[13] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable\nare features in deep neural networks? In Advances in Neural Information Pro-\ncessing Systems, pages 3320–3328, 2014.\n12\n",
  "categories": [
    "stat.ML",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2017-08-11",
  "updated": "2017-08-11"
}