{
  "id": "http://arxiv.org/abs/1708.05148v1",
  "title": "Natural Language Processing: State of The Art, Current Trends and Challenges",
  "authors": [
    "Diksha Khurana",
    "Aditya Koli",
    "Kiran Khatter",
    "Sukhdev Singh"
  ],
  "abstract": "Natural language processing (NLP) has recently gained much attention for\nrepresenting and analysing human language computationally. It has spread its\napplications in various fields such as machine translation, email spam\ndetection, information extraction, summarization, medical, and question\nanswering etc. The paper distinguishes four phases by discussing different\nlevels of NLP and components of Natural Language Generation (NLG) followed by\npresenting the history and evolution of NLP, state of the art presenting the\nvarious applications of NLP and current trends and challenges.",
  "text": "Natural Language Processing: State of The Art, Current Trends and \nChallenges \nDiksha Khurana1, Aditya Koli1, Kiran Khatter1,2\n and Sukhdev Singh1,2\n \n1Department of Computer Science and Engineering \nManav Rachna International University, Faridabad-121004, India \n2Accendere Knowledge Management Services Pvt. Ltd., India \n \nAbstract  \nNatural language processing (NLP) has recently gained much attention for representing and \nanalysing human language computationally. It has spread its applications in various fields \nsuch as machine translation, email spam detection, information extraction, summarization, \nmedical, and question answering etc. The paper distinguishes four phases by discussing \ndifferent levels of NLP and components of Natural Language Generation (NLG) followed by \npresenting the history and evolution of NLP, state of the art presenting the various \napplications of NLP and current trends and challenges.  \n \n1. Introduction \nNatural Language Processing (NLP) is a tract of Artificial Intelligence and Linguistics, \ndevoted to make computers understand the statements or words written in human languages. \nNatural language processing came into existence to ease the user’s work and to satisfy the \nwish to communicate with the computer in natural language. Since all the users may not be \nwell-versed in machine specific language, NLP caters those users who do not have enough \ntime to learn new languages or get perfection in it.  \nA language can be defined as a set of rules or set of symbol. Symbol are combined and used \nfor conveying information or broadcasting the information. Symbols are tyrannized by the \nRules. Natural Language Processing basically can be classified into two parts i.e. Natural \nLanguage Understanding and Natural Language Generation which evolves the task to \nunderstand and generate the text (Figure 1).  \n \n                                      Figure 1. Broad Classification of NLP \nLinguistics is the science of language which includes Phonology that refers to sound, \nMorphology word formation, Syntax sentence structure, Semantics syntax and Pragmatics \nwhich refers to understanding. \nNoah Chomsky, one of the first linguists of twelfth century that started syntactic theories, \nmarked a unique position in the field of theoretical linguistics because he revolutionised the \narea of syntax (Chomsky, 1965) [1]. Which can be broadly categorized into two levels Higher \nLevel which include speech recognition and Lower Level which corresponds to natural \nlanguage. Few of the researched tasks of NLP are Automatic Summarization, Co-Reference \nResolution, Discourse Analysis, Machine Translation, Morphological Segmentation, Named \nEntity Recognition, Optical Character Recognition, Part Of Speech Tagging etc. Some of \nthese tasks have direct real world applications such as Machine translation, Named entity \nrecognition, Optical character recognition etc. Automatic summarization produces an \nunderstandable summary of a set of text and provides summaries or detailed information of \ntext of a known type. Co-reference resolution it refers to a sentence or larger set of text that \ndetermines which word refer to the same object. Discourse analysis refers to the task of \nidentifying the discourse structure of connected text. Machine translation which refers to \nautomatic translation of text from one human language to another. Morphological \nsegmentation which refers to separate word into individual morphemes and identify the class \nof the morphemes. Named entity recognition (NER) it describes a stream of text, determine \nwhich items in the text relates to proper names. Optical character recognition (OCR) it gives \nan image representing printed text, which help in determining the corresponding or related \ntext. Part of speech tagging it describes a sentence, determines the part of speech for each \nword. Though NLP tasks are obviously very closely interweaved but they are used \nfrequently, for convenience. Some of the task such as automatic summarisation, co-reference \nanalysis etc. act as subtask that are used in solving larger tasks.  \nThe goal of Natural Language Processing is to accommodate one or more specialities of an \nalgorithm or system. The metric of NLP assess on an algorithmic system allows for the \nintegration of language understanding and language generation. It is even used in \nmultilingual event detection Rospocher et al. [2] purposed a novel modular system for cross-\nlingual event extraction for English, Dutch and Italian texts by using different pipelines for \ndifferent languages. The system incorporates a modular set of foremost multilingual Natural \nLanguage Processing (NLP) tools. The pipeline integrates modules for basic NLP processing \nas well as more advanced tasks such as cross-lingual named entity linking, semantic role \nlabelling and time normalization. Thus, the cross-lingual framework allows for the \ninterpretation of events, participants, locations and time, as well as the relations between \nthem. Output of these individual pipelines is intended to be used as input for a system that \nobtains event centric knowledge graphs. All modules behave like UNIX pipes: they all take \nstandard input, to do some annotation, and produce standard output which in turn is the input \nfor the next module pipelines are built as a data centric architecture so that modules can be \nadapted and replaced. Furthermore, modular architecture allows for different configurations \nand for dynamic distribution. \nMost of the work in Natural Language Processing is conducted by computer scientists while \nvarious other professionals have also shown interest such as linguistics, psychologist and \nphilosophers etc. One of the most ironical aspect of NLP is that it adds up to the knowledge \nof human language. The field of Natural Language Processing is related with different \ntheories and techniques that deal with the problem of natural language of communicating \nwith the computers. Ambiguity is one of the major problem of natural language which is \nusually faced in syntactic level which has subtask as lexical and morphology which are \nconcerned with the study of words and word formation. Each of these levels can produce \nambiguities that can be solved by the knowledge of the complete sentence. The ambiguity \ncan be solved by various methods such as Minimising Ambiguity, Preserving Ambiguity, \nInteractive Disambiguity and Weighting Ambiguity [3]. Some of the methods proposed by \nresearchers to remove ambiguity is preserving ambiguity, e.g. (Shemtov 1997; Emele & \nDorna 1998; Knight & Langkilde 2000) [3][4][5] Their objectives are closely in line with the \nlast of these: they cover a wide range of ambiguities and there is a statistical element implicit \nin their approach.  \n2. Levels of NLP \nThe ‘levels of language’ are one of the most explanatory method for representing the Natural \nLanguage processing which helps to generate the NLP text by realising Content Planning, \nSentence Planning and Surface Realization phases (Figure 2).  \n \n                                      Figure 2. Phases of NLP architecture \nLinguistic is the science which involves meaning of language, language context and various \nforms of the language. The various important terminologies of Natural Language Processing \nare: - \n1. Phonology \nPhonology is the part of Linguistics which refers to the systematic arrangement of sound. The \nterm phonology comes from Ancient Greek and the term phono- which means voice or \nsound, and the suffix –logy refers to word or speech. In 1993 Nikolai Trubetzkoy stated that \nPhonology is “the study of sound pertaining to the system of language\". Whereas Lass in \n1998 wrote that phonology refers broadly with the sounds of language, concerned with the to \nlathe sub discipline of linguistics, whereas it could be explained as, \"phonology proper is \nconcerned with the function, behaviour and organization of sounds as linguistic items. \nPhonology include semantic use of sound to encode meaning of any Human language.  \n(Clark et al.,2007) [6]. \n2. Morphology \nThe different parts of the word represent the smallest units of meaning known as Morphemes. \nMorphology which comprise of Nature of words, are initiated by morphemes. An example of \nMorpheme could be, the word precancellation can be morphologically scrutinized into three \nseparate morphemes: the prefix pre, the root cancella, and the suffix -tion. The interpretation \nof morpheme stays same across all the words, just to understand the meaning humans can \nbreak any unknown word into morphemes. For example, adding the suffix –ed to a verb, \nconveys that the action of the verb took place in the past. The words that cannot be divided \nand have meaning by themselves are called Lexical morpheme (e.g.: table, chair) The words \n(e.g. -ed, -ing, -est, -ly, -ful) that are combined with the lexical morpheme are known as \nGrammatical morphemes (eg. Worked, Consulting, Smallest, Likely, Use). Those \ngrammatical morphemes that occurs in combination called bound morphemes( eg. -ed, -ing) \nGrammatical morphemes can be divided into bound morphemes and derivational morphemes.     \n3. Lexical \nIn Lexical, humans, as well as NLP systems, interpret the meaning of individual words. \nSundry types of processing bestow to word-level understanding – the first of these being a \npart-of-speech tag to each word. In this processing, words that can act as more than one part-\nof-speech are assigned the most probable part-of speech tag based on the context in which \nthey occur. At the lexical level, Semantic representations can be replaced by the words that \nhave one meaning. In NLP system, the nature of the representation varies according to the \nsemantic theory deployed. \n4. Syntactic \nThis level emphasis to scrutinize the words in a sentence so as to uncover the grammatical \nstructure of the sentence. Both grammar and parser are required in this level. The output of \nthis level of processing is representation of the sentence that divulge the structural \ndependency relationships between the words. There are various grammars that can be \nimpeded, and which in twirl, whack the option of a parser. Not all NLP applications require a \nfull parse of sentences, therefore the abide challenges in parsing of prepositional phrase \nattachment and conjunction audit no longer impede that plea for which phrasal and clausal \ndependencies are adequate [7]. Syntax conveys meaning in most languages because order and \ndependency contribute to connotation. For example, the two sentences: ‘The cat chased the \nmouse.’ and ‘The mouse chased the cat.’ differ only in terms of syntax, yet convey quite \ndifferent meanings. \n5. Semantic \nIn semantic most people think that meaning is determined, however, this is not it is all the \nlevels that bestow to meaning. Semantic processing determines the possible meanings of a \nsentence by pivoting on the interactions among word-level meanings in the sentence. This \nlevel of processing can incorporate the semantic disambiguation of words with multiple \nsenses; in a cognate way to how syntactic disambiguation of words that can errand as \nmultiple parts-of-speech is adroit at the syntactic level. For example, amongst other \nmeanings, ‘file’ as a noun can mean either a binder for gathering papers, or a tool to form \none’s fingernails, or a line of individuals in a queue (Elizabeth D. Liddy,2001) [7]. The \nsemantic level scrutinizes words for their dictionary elucidation, but also for the elucidation \nthey derive from the milieu of the sentence. Semantics milieu that most words have more \nthan one elucidation but that we can spot the appropriate one by looking at the rest of the \nsentence. [8] \n6. Discourse \nWhile syntax and semantics travail with sentence-length units, the discourse level of NLP \ntravail with units of text longer than a sentence i.e, it does not interpret multi sentence texts as \njust sequence sentences, apiece of which can be elucidated singly. Rather, discourse focuses \non the properties of the text as a whole that convey meaning by making connections between \ncomponent sentences (Elizabeth D. Liddy,2001) [7]. The two of the most common levels are \nAnaphora Resolution - Anaphora resolution is the replacing of words such as pronouns, \nwhich are semantically stranded, with the pertinent entity to which they refer. Discourse/Text \nStructure Recognition - Discourse/text structure recognition sway the functions of sentences \nin the text, which, in turn, adds to the meaningful representation of the text. \n7. Pragmatic: \nPragmatic is concerned with the firm use of language in situations and utilizes nub over and \nabove the nub of the text for understanding the goal and to explain how extra meaning is read \ninto texts without literally being encoded in them. This requisite much world knowledge, \nincluding the understanding of intentions, plans, and goals. For example, the following two \nsentences need aspiration of the anaphoric term ‘they’, but this aspiration requires pragmatic \nor world knowledge (Elizabeth D. Liddy,2001) [7]. \n3. Natural Language Generation \nNatural Language Generation (NLG) is the process of producing phrases, sentences and \nparagraphs that are meaningful from an internal representation. It is a part of Natural \nLanguage Processing and happens in four phases: identifying the goals, planning on how \ngoals maybe achieved by evaluating the situation and available communicative sources and \nrealizing the plans as a text [Figure 3]. It is opposite to Understanding. \n \n                                      Figure 3. Components of NLG \nComponents of NLG are as follows: \nSpeaker and Generator – To generate a text we need to have a speaker or an application \nand a generator or a program that renders the application’s intentions into fluent phrase \nrelevant to the situation.  \nComponents and Levels of Representation -The process of language generation involves \nthe following interweaved tasks. Content selection: Information should be selected and \nincluded in the set. Depending on how this information is parsed into representational units, \nparts of the units may have to be removed while some others may be added by default. \nTextual Organization: The information must be textually organized according the grammar, it \nmust be ordered both sequentially and in terms of linguistic relations like modifications. \nLinguistic Resources: To support the information’s realization, linguistic resources must be \nchosen. In the end these resources will come down to choices of particular words, idioms, \nsyntactic constructs etc. Realization: The selected and organized resources must be realized \nas an actual text or voice output.  \nApplication or Speaker –  This is only for maintaining the model of the situation. Here the \nspeaker just initiates the process doesn’t take part in the language generation. It stores the \nhistory, structures the content that is potentially relevant and deploys a representation of what \nit actually knows. All these form the situation, while selecting subset of propositions that \nspeaker has. The only requirement is the speaker has to make sense of the situation. [9] \n \n4. History of NLP \nIn late 1940s the term wasn’t even in existence, but the work regarding machine translation \n(MT) had started. Research in this period was not completely localised. Russian and English \nwere the dominant languages for MT, but others, like Chinese were used for MT (Booth \n,1967) [10]. MT/NLP research was almost died in 1966 according to ALPAC report, which \nconcluded that MT is going nowhere. But later on some MT production systems were \nproviding output to their customers (Hutchins, 1986) [11]. By this time, work on the use of \ncomputers for literary and linguistic studies had also started.  \nAs early as 1960 signature work influenced by AI began, with the BASEBALL Q-A systems \n(Green et al., 1961) [12]. LUNAR (Woods ,1978) [13] and Winograd SHRDLU were natural \nsuccessors of these systems but they were seen as stepped up sophistication, in terms of their \nlinguistic and their task processing capabilities. There was a widespread belief that progress \ncould only be made on the two sides, one is ARPA Speech Understanding Research (SUR) \nproject (Lea, 1980) and other in some major system developments projects building database \nfront ends. The front-end projects (Hendrix et al., 1978) [14] were intended to go beyond \nLUNAR in interfacing the large databases. \nIn early 1980s computational grammar theory became a very active area of research linked \nwith logics for meaning and knowledge’s ability to deal with the user’s beliefs and intentions \nand with functions like emphasis and themes. \nBy the end of the decade the powerful general purpose sentence processors like SRI’s Core \nLanguage Engine (Alshawi,1992) [15] and Discourse Representation Theory (Kamp and \nReyle,1993) [16] offered a means of tackling more extended discourse within the \ngrammatico-logical framework. This period was one of the growing community. Practical \nresources, grammars, and tools and parsers became available (e.g the Alvey Natural \nLanguage Tools (Briscoe et al., 1987) [17]. The (D)ARPA speech recognition and message \nunderstanding (information extraction) conferences were not only for the tasks they \naddressed but for the emphasis on heavy evaluation, starting a trend that became a major \nfeature in 1990s (Young and Chase, 1998; Sundheim and Chinchor ,1993) [18][19]. Work on \nuser modelling (Kobsa and Wahlster , 1989) [20]  was one strand in research paper and on \ndiscourse structure serving this (Cohen et al., 1990)  [21]. At the same time, as McKeown \n(1985) [22] showed, rhetorical schemas could be used for producing both linguistically \ncoherent and communicatively effective text. Some researches in NLP marked important \ntopics for future like word sense disambiguation (Small et al., 1988) [23] and probabilistic \nnetworks, statistically coloured NLP, the work on the lexicon, also pointed in this direction.  \nStatistical language processing was a major thing in 90s (Manning and Schuetze,1999) [24], \nbecause this not only involves data analysts. Information extraction and automatic \nsummarising (Mani and Maybury ,1999) [25] was also a point of focus. \nRecent researches are mainly focused on unsupervised and semi-supervised learning \nalgorithms. \n5. Related Work \nMany researchers worked on NLP, building tools and systems which makes NLP what it is \ntoday. Tools like Sentiment Analyser, Parts of Speech (POS)Taggers, Chunking, Named \nEntity Recognitions (NER), Emotion detection, Semantic Role Labelling made NLP a good \ntopic for research.  \nSentiment analyser (Jeonghee etal.,2003) [26] works by extracting sentiments about given \ntopic. Sentiment analysis consists of a topic specific feature term extraction, sentiment \nextraction, and association by relationship analysis. Sentiment Analysis utilizes two linguistic \nresources for the analysis: the sentiment lexicon and the sentiment pattern database. It \nanalyses the documents for positive and negative words and try to give ratings on scale -5 to \n+5. \nParts of speech taggers for the languages like European languages, research is being done on \nmaking parts of speech taggers for other languages like Arabic, Sanskrit (Namrata Tapswi , \nSuresh Jain ., 2012) [27], Hindi (Pradipta Ranjan Ray et al., 2003 )[28] etc. It can efficiently \ntag and classify words as nouns, adjectives, verbs etc. The most procedures for part of speech \ncan work efficiently on European languages, but it won’t on Asian languages or middle \neastern languages. Sanskrit part of speech tagger is specifically uses treebank technique. \nArabic uses Support Vector Machine (SVM) (Mona Diab etal.,2004) [29] approach to \nautomatically tokenize, parts of speech tag and annotate base phrases in Arabic text. \n \nChunking – it is also known as Shadow Parsing, it works by labelling segments of sentences \nwith syntactic correlated keywords like Noun Phrase and Verb Phrase (NP or VP). Every \nword has a unique tag often marked as Begin Chunk (B-NP) tag or Inside Chunk (I-NP) tag. \nChunking is often evaluated using the CoNLL 2000 shared task.  CoNLL 2000 provides test \ndata for Chunking. Since then, a certain number of systems arised (Sha and Pereira, 2003; \nMcDonald et al., 2005; Sun et al., 2008) [30] [31] [32], all reporting around 94.3% F1 score. \nThese systems use features composed of words, POS tags, and tags. \n \nUsage of Named Entity Recognition in places such as Internet is a problem as people don’t \nuse traditional or standard English. This degrades the performance of standard natural \nlanguage processing tools substantially. By annotating the phrases or tweets and building \ntools trained on unlabelled, in domain and out domain data (Alan Ritter., 2011) [33]. It \nimproves the performance as compared to standard natural language processing tools. \n \nEmotion Detection (Shashank Sharma, 2016) [34] is similar to sentiment analysis, but it \nworks on social media platforms on mixing of two languages (English + Any other Indian \nLanguage). It categorizes statements into six groups based on emotions. During this process, \nthey were able to identify the language of ambiguous words which were common in Hindi \nand English and tag lexical category or parts of speech in mixed script by identifying the base \nlanguage of the speaker. \n \nSematic Role Labelling – SRL works by giving a semantic role to a sentence. For example in \nthe PropBank (Palmer et al., 2005) [35] formalism, one assigns roles to words that are \narguments of a verb in the sentence. The precise arguments depend on verb frame and if there \nexists multiple verbs  in a sentence, it might have multiple tags. State-of-the-art SRL systems \ncomprise of several stages: creating a parse tree, identifying which parse tree nodes represent \nthe arguments of a given verb, and finally classifying these nodes to compute the \ncorresponding SRL tags. \n \nEvent discovery in social media feeds (Edward Benson et al.,2011) [36], using a graphical \nmodel to analyse any social media feeds to determine whether it contains name of a person or \nname of a venue, place, time etc. The model operates on noisy feeds of data to extract records \nof events by aggregating multiple information across multiple messages, despite the noise of \nirrelevant noisy messages and very irregular message language, this model was able to extract \nrecords with high accuracy. However, there is some scope for improvement using broader \narray of features on factors. \n \n6. Applications of NLP \nNatural Language Processing can be applied into various areas like Machine Translation, \nEmail Spam detection, Information Extraction, Summarization, Question Answering etc. \n6.1 Machine Translation  \nAs most of the world is online, the task of making data accessible and available to all is a \nchallenge. Major challenge in making data accessible is the language barrier. There are \nmultitude of languages with different sentence structure and grammar. Machine Translation is \ngenerally translating phrases from one language to another with the help of a statistical \nengine like Google Translate. The challenge with machine translation technologies is not \ndirectly translating words but keeping the meaning of sentences intact along with grammar \nand tenses. The statistical machine learning gathers as many data as they can find that seems \nto be parallel between two languages and they crunch their data to find the likelihood that \nsomething in Language A corresponds to something in Language B. As for Google, in \nSeptember 2016, announced a new machine translation system based on Artificial neural \nnetworks and Deep learning . In recent years, various methods have been proposed to \nautomatically evaluate machine translation quality by comparing hypothesis translations with \nreference translations. Examples of such methods are word error rate, position-independent \nword error rate (Tillmann et al., 1997) [37], generation string accuracy (Bangalore et al., \n2000) [38], multi-reference word error rate (Nießen et al., 2000) [39], BLEU score (Papineni \net al., 2002) [40], NIST score (Doddington, 2002) [41]  All these criteria try to approximate \nhuman assessment and often achieve an astonishing degree of correlation to human subjective \nevaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002) [42][43].  \n6.2 Text Categorization  \nCategorization systems inputs a large flow of data like official documents, military casualty \nreports, market data, newswires etc. and assign them to predefined categories or indices. For \nexample, The Carnegie Group’s Construe system (Hayes PJ ,Westein ; 1991)[44] , inputs \nReuters articles and saves much time by doing the work that is to be done by staff or human \nindexers. Some companies have been using categorization systems to categorize trouble \ntickets or complaint requests and routing to the appropriate desks. Another application of text \ncategorization is email spam filters. Spam filters is becoming important as the first line of \ndefence against the unwanted emails. A false negative and false positive issues of spam filters \nare at the heart of NLP technology, its brought down to the challenge of extracting meaning \nfrom strings of text. A filtering solution that is applied to an email system uses a set of \nprotocols to determine which of the incoming messages are spam and which are not. There \nare several types of spam filters available. Content filters: Review the content within the \nmessage to determine whether it is a spam or not. Header filters: Review the email header \nlooking for fake information. General Blacklist filters: Stopes all emails from blacklisted \nrecipients. Rules Based Filters: It uses user-defined criteria. Such as stopping mails from \nspecific person or stopping mail including a specific word. Permission Filters: Require \nanyone sending a message to be pre-approved by the recipient. Challenge Response Filters: \nRequires anyone sending a message to enter a code in order to gain permission to send email. \n6.3 Spam Filtering  \nIt works using text categorization and in recent times, various machine learning techniques \nhave been applied to text categorization or Anti-Spam Filtering  like Rule Learning (Cohen \n1996)[45], Naïve Bayes (Sahami et al., 1998 ;Androutsopoulos et al.,2000b ;Rennie \n.,2000)[46][47][48],Memory based Learning (Androutsopoulos et al.,2000b)[47], Support \nvector machines (Druker et al., 1999)[49], Decision Trees (Carreras and Marquez , 2001)[50] \nMaximum Entropy Model (Berger et al. 1996)[51]. Sometimes combining different learners \n(Sakkis et al., 2001) [52]. Using these approaches is better as classifier is learned from \ntraining data rather than making by hand. The naïve bayes is preferred because of its \nperformance despite its simplicity (Lewis, 1998) [53] In Text Categorization two types of \nmodels have been used (McCallum and Nigam, 1998) [54]. Both modules assume that a fixed \nvocabulary is present. But in first model a document is generated by first choosing a subset of \nvocabulary and then using the selected words any number of times, at least once irrespective \nof order. This is called Multi-variate Bernoulli model. It takes the information of which \nwords are used in a document irrespective of number of words and order. In second model, a \ndocument is generated by choosing a set of word occurrences and arranging them in any \norder. this model is called multi-nomial model, in addition to the Multi-variate Bernoulli \nmodel, it also captures information on how many times a word is used in a document. Most \ntext categorization approaches to anti spam Email filtering have used multi variate Bernoulli \nmodel (Androutsopoulos et al.,2000b) [47] \n6.4 Information Extraction \nInformation extraction is concerned with identifying phrases of interest of textual data. For \nmany applications, extracting entities such as names, places, events, dates, times and prices is \na powerful way of summarize the information relevant to a user’s needs. In the case of a \ndomain specific search engine, the automatic identification of important information can \nincrease accuracy and efficiency of a directed search. There is use of hidden Markov models \n(HMMs) to extract the relevant fields of research papers. These extracted text segments are \nused to allow searched over specific fields and to provide effective presentation of search \nresults and to match references to papers. For example, noticing the pop up ads on any \nwebsites showing the recent items you might have looked on an online store with discounts. \nIn Information Retrieval two types of models have been used (McCallum and Nigam, 1998) \n[55]. Both modules assume that a fixed vocabulary is present. But in first model a document \nis generated by first choosing a subset of vocabulary and then using the selected words any \nnumber of times, at least once without any order. This is called Multi-variate Bernoulli \nmodel. It takes the information of which words are used in a document irrespective of number \nof words and order. In second model, a document is generated by choosing a set of word \noccurrences and arranging them in any order. this model is called multi-nomial model, in \naddition to the Multi-variate Bernoulli model , it also captures information on how many \ntimes a word is used in a document \nDiscovery of knowledge is becoming important areas of research over the recent years. \nKnowledge discovery research use a variety of techniques in order to extract useful \ninformation from source documents like  \nParts of Speech (POS) tagging, Chunking or Shadow Parsing, Stop-words (Keywords that \nare used and must be removed before processing documents), Stemming (Mapping words to \nsome base for, it has two methods, dictionary based stemming and Porter style stemming \n(Porter, 1980) [55]. Former one has higher accuracy but higher cost of implementation while \nlatter has lower implementation cost and is usually insufficient for IR). Compound or \nStatistical Phrases (Compounds and statistical phrases index multi token units instead of \nsingle tokens.) Word Sense Disambiguation (Word sense disambiguation is the task of \nunderstanding the correct sense of a word in context. When used for information retrieval, \nterms are replaced by their senses in the document vector.) \n \nIts extracted information can be applied on a variety of purpose, for example to prepare a \nsummary, to build databases, identify keywords, classifying text items according to some pre-\ndefined categories etc. For example   CONSTRUE, it was developed for Reuters, that is used \nin classifying news stories (Hayes, 1992) [57]. It has been suggested that many IE systems \ncan successfully extract terms from documents, acquiring relations between the terms is still a \ndifficulty. PROMETHEE is a system that extracts lexico-syntactic patterns relative to a \nspecific conceptual relation (Morin,1999) [58]. IE systems should work at many levels, from \nword recognition to discourse analysis at the level of the complete document. An application \nof the Blank Slate Language Processor (BSLP) (Bondale et al., 1999) [59] approach for the \nanalysis of a real life natural language corpus that consists of responses to open-ended \nquestionnaires in the field of advertising. \nThere’s a system called MITA (Metlife’s Intelligent Text Analyzer) (Glasgow et al. (1998) \n[60]) that extracts information from life insurance applications. Ahonen et al. (1998) [61] \nsuggested a mainstream framework for text mining that uses pragmatic and discourse level \nanalyses of text. \n6.5 Summarization \nOverload of information is the real thing in this digital age, and already our reach and access \nto knowledge and information exceeds our capacity to understand it. This trend is not slowing \ndown, so an ability to summarize the data while keeping the meaning intact is highly \nrequired. This is important not just allowing us the ability to recognize the understand the \nimportant information for a large set of data, it is used to understand the deeper emotional \nmeanings; For example, a company determine the general sentiment on social media and use \nit on their latest product offering. This application is useful as a valuable marketing asset. \nThe types of text summarization depends on the basis of the number of documents and  the \ntwo important categories are single document summarization and multi document \nsummarization (Zajic et al. 2008 [62]; Fattah and Ren 2009 [63]). Summaries can also be of \ntwo types: generic or query-focused (Gong and Liu 2001 [64]; Dunlavy et al. 2007 [65]; Wan \n2008 [66]; Ouyang et al. 2011 [67]). Summarization task can be either supervised or \nunsupervised (Mani and Maybury 1999 [68]; Fattah and Ren 2009 [63]; Riedhammer et al. \n2010 [69]). Training data is required in a supervised system for selecting relevant material \nfrom the documents. Large amount of annotated data is needed for learning techniques. Few \ntechniques are as follows– \n- \nBayesian Sentence based Topic Model (BSTM) uses both term-sentences and term \ndocument associations for summarizing multiple documents. (Wang et al. 2009 \n[70])   \n- \nFactorization with Given Bases (FGB) is a language model where sentence bases \nare the given bases and it utilizes document-term and sentence term matrices. \nThis approach groups and summarizes the documents simultaneously. (Wang et \nal. 2011) [71]) \n- \nTopic Aspect-Oriented Summarization (TAOS) is based on topic factors. These \ntopic factors are various features that describe topics such as capital words are \nused to represent entity. Various topics can have various aspects and various \npreferences of features are used to represent various aspects. (Fang et al. 2015 [72]) \n \n6.6 Dialogue System \nPerhaps the most desirable application of the future, in the systems envisioned by large \nproviders of end user applications, Dialogue systems, which focuses on a narrowly defined \napplications (like refrigerator or home theater systems) currently uses the phonetic and lexical \nlevels of language. It is believed that these dialogue systems when utilizing all levels of \nlanguage processing offer potential for fully automated dialog systems. (Elizabeth D. Liddy, \n2001) [7]. Whether on text or via voice. This could lead to produce systems that can enable \nrobots to interact with humans in natural languages. Examples like Google’s assistant, \nWindows Cortana, Apple’s Siri and Amazon’s Alexa are the software and devices that follow \nDialogue systems. \n6.7 Medicine \nNLP is applied in medicine field as well. The Linguistic String Project-Medical Language \nProcessor is one the large scale projects of NLP in the field of medicine [74][75][76][77][78]. \nThe LSP-MLP helps enabling physicians to extract and summarize information of any signs \nor symptoms, drug dosage and response data with aim of identifying possible side effects of \nany medicine while highlighting or flagging data items [74]. The National Library of \nMedicine is developing The Specialist System [79][80][81][82][83]. It is expected to function \nas Information Extraction tool for Biomedical Knowledge Bases, particularly Medline \nabstracts. The lexicon was created using MeSH (Medical Subject Headings), Dorland’s \nIllustrated Medical Dictionary and general English Dictionaries. The Centre d’Informatique \nHospitaliere of the Hopital Cantonal de Geneve is working on an electronic archiving \nenvironment with NLP features [84][85]. In first phase, patient records were archived . At \nlater stage the LSP-MLP has been adapted for French [86][87][88][89] , and finally , a proper \nNLP system called RECIT  [90][91][92][93] has been developed using a method called \nProximity Processing [94]. It’s task was to implement a robust and multilingual system able \nto analyze/comprehend medical sentences, and to preserve a knowledge of free text into a \nlanguage independent knowledge representation [95][96]. The Columbia university of New \nYork has developed an NLP system called MEDLEE (MEDical Language Extraction and \nEncoding System) that identifies clinical information in narrative reports and transforms the \ntextual information into structured representation [97]. \n7. Approaches \nRationalist approach or symbolic approach assume that crucial part of the knowledge in the \nhuman mind is not derived by the sense but is firm in advance, probably by genetic in \nheritance. Noam Chomsky was the strongest advocate of this approach. It was trusted that \nmachine can be  made to function like human brain by giving some fundamental knowledge \nand reasoning mechanism linguistics  knowledge is directly encoded in rule or other forms of \nrepresentation. This helps automatic process of natural languages. [98] Statistical and \nmachine learning entail evolution of algorithms that allow a program to infer patterns. An \niterative process is used to characterize a given algorithm’s underlying algorithm that are \noptimised by a numerical measure that characterize numerical parameters and learning phase. \nMachine-learning models can be predominantly categorized as either generative or \ndiscriminative. Generative methods can generate synthetic data because of which they create \nrich models of probability distributions. Discriminative methods are more functional and \nhave right estimating posterior probabilities and are based on observations.  \nSrihari [99] explains the different generative models as one with a resemblance that is used to \nspot an unknown speaker’s language and would bid the deep knowledge of numerous \nlanguage to perform the match. Whereas discriminative methods rely on a less knowledge-\nintensive approach and using distinction between language.  Whereas generative models, can \nbecome troublesome when many features are used and discriminative models allow use of \nmore features. [100] Few of the examples of discriminative methods are Logistic regression \nand conditional random fields (CRFs), generative methods are Naive Bayes classifiers and \nhidden Markov models (HMMs). \n7.1 Hidden Markov Model (HMM) \nAn HMM is a system where a shifting takes place between several states, generating feasible \noutput symbols with each switch. The sets of viable states and unique symbols may be large, \nbut finite and known. We can descry the outputs, but the system’s internals are hidden. Few \nof the problem could be solved are by Inference A certain sequence of output symbols, \ncompute the probabilities of one or more candidate states with sequences. Pattern matching \nthe state-switch sequence is realised are most likely to have generated a particular output-\nsymbol sequence. Training the output-symbol chain data, reckon the state-switch/output \nprobabilities that fit this data best. \nHidden Markov Models are extensively used for speech recognition, where the output \nsequence is matched to the sequence of individual phonemes. Frederick Jelinek, a statistical-\nNLP advocate who first instigated HMMs at IBM’s Speech Recognition Group, reportedly \njoked, every time a linguist leaves my group, the speech recognizer’s performance improves. \n[101] HMM is not restricted to this application it has several others such as bioinformatics \nproblems, for example, multiple sequence alignment [102]. Sonnhammer mentioned that \nPfam hold multiple alignments and hidden Markov model based profiles (HMM-profiles) of \nentire protein domains. The cue of domain boundaries, family members and alignment is \ndone semi-automatically found on expert knowledge, sequence similarity, other protein \nfamily databases and the capability of HMM-profiles to correctly identify and align the \nmembers. [103]  \n7.2 Naive Bayes Classifiers \n The choice of area is wide ranging covering usual items like word segmentation and \ntranslation but also unusual areas like segmentation for infant learning and identifying \ndocuments for opinions and facts. In addition, exclusive article was selected for its use of \nBayesian methods to aid the research in designing algorithms for their investigation. \n8. NLP in Talk \nThis section discusses the recent developments in the NLP projects implemented by various \ncompanies and these are as follows: \n8.1 ACE Powered GDPR Robot Launched by RAVN Systems [104] \nRAVN Systems, an leading expert in Artificial Intelligence (AI), Search and Knowledge \nManagement Solutions, announced the launch of a RAVN (\"Applied Cognitive Engine\") i.e \npowered software Robot to help and facilitate the GDPR (\"General Data Protection \nRegulation\") compliance. \nThe Robot uses AI techniques to automatically analyse documents and other types of data in \nany business system which is subject to GDPR rules. It allows users to quickly and easily \nsearch, retrieve, flag, classify and report on data mediated to be supersensitive under GDPR. \nUsers also have the ability to identify personal data from documents, view feeds on the latest \npersonal data that requires attention and provide reports on the data suggested to be deleted or \nsecured.  RAVN's GDPR Robot is also able to hasten requests for information (Data Subject \nAccess Requests - \"DSAR\") in a simple and efficient way, removing the need for a physical \napproach to these requests which tends to be very labour thorough. Peter Wallqvist, CSO at \nRAVN Systems commented, \"GDPR compliance is of universal paramountcy as it will \nexploit to any organisation that control and process data concerning EU citizens. \nLINK:http://markets.financialcontent.com/stocks/news/read/33888795/RAVN_Systems_Launch_the_ACE_Po\nwered_GDPR_Robot \n8.2 Eno A Natural Language Chatbot Launched by Capital One [105] \nCapital one announces chatbot for customers called Eno. Eno is a natural language chatbot \nthat people socialize through texting. Capital one claims that Eno is First natural language \nSMS chatbot from a U.S. bank that allows customer to ask questions using natural language. \nCustomers can interact with Eno asking questions about their savings and others using a text \ninterface. Eno makes such an environment that it feels that a human is interacting. Ken \nDodelin, Capital One’s vice president of digital product development, said “We kind of \nlaunched a chatbot and didn’t know it.”  \nThis provides a different platform than other brands that launch chatbots like Facebook \nMessenger and Skype. They believed that Facebook has too much access of private \ninformation of a person, which could get them into trouble with privacy laws of U.S. \nfinancial institutions work under. Like any Facebook Page admin can access full transcripts \nof the bot’s conversations. If that would be the case then the admins could easily view the \npersonal banking information of customers with is not correct \n LINK: https://www.macobserver.com/analysis/capital-one-natural-language-chatbot-eno/ \n8.3  Future of BI in Natural Language Processing [106] \nSeveral companies in Bi spaces are trying to get with the trend and trying hard to ensure that \ndata becomes more friendly and easily accessible. But still there is long way for this.BI will \nalso make it easier to access as GUI is not needed. Because now a days the queries are made \nby text or voice command on smartphones.one of the most common example is Google might \ntell you today what will be the tomorrows weather. But soon enough, we will be able to ask \nour personal data chatbot about customer sentiment today, and how do we feel about their \nbrand next week; all while walking down the street. Today, NLP tends to be based on turning \nnatural language into machine language. But with time the technology matures – especially \nthe AI component –the computer will get better at “understanding” the query and start to \ndeliver answers rather than search results. \n Initially, the data chatbot will probably ask the question as how have revenues changed over \nthe last three-quarters?’ and then return pages of data for you to analyse. But once it learns \nthe semantic relations and inferences of the question, it will be able to automatically perform \nthe filtering and formulation necessary to provide an intelligible answer, rather than simply \nshowing you data. \nLink: http://www.smartdatacollective.com/eran-levy/489410/here-s-why-natural-language-processing-future-bi \n8.4 Using Natural Language Processing and Network Analysis to \nDevelop a Conceptual Framework for Medication Therapy \nManagement Research [107] \nNatural Language Processing and Network Analysis to Develop a Conceptual Framework for \nMedication Therapy Management Research describes a theory derivation process that is used \nto develop conceptual framework for medication therapy management (MTM) research. The \nMTM service model and chronic care model are selected as parent theories. Review article \nabstracts target medication therapy management in chronic disease care that were retrieved \nfrom Ovid Medline (2000-2016). \nUnique concepts in each abstract are extracted using Meta Map and their pairwise \ncooccurrence are determined. Then the information is used to construct a network graph of \nconcept co-occurrence that is further analysed to identify content for the new conceptual \nmodel. 142 abstracts are analysed. Medication adherence is the most studied drug therapy \nproblem and co-occurred with concepts related to patient-centred interventions targeting self-\nmanagement. The enhanced model consists of 65 concepts clustered into 14 constructs. The \nframework requires additional refinement and evaluation to determine its relevance and \napplicability across a broad audience including underserved settings. \nLink: https://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract \n8.5 Meet the Pilot, world’s first language translating earbuds [108] \nThe world’s first smart earpiece Pilot will soon be transcribed over 15 languages. According \nto Spring wise, Waverly Labs’ Pilot can already transliterate five spoken languages, English, \nFrench, Italian, Portuguese and Spanish, and seven written affixed languages, German, Hindi, \nRussian, Japanese, Arabic, Korean and Mandarin Chinese. The Pilot earpiece is connected \nvia Bluetooth to the Pilot speech translation app, which uses speech recognition, machine \ntranslation and machine learning and speech synthesis technology. \nSimultaneously, the user will hear the translated version of the speech on the second earpiece. \nMoreover, it is not necessary that conversation would be taking place between two people \nonly the users can join in and discuss as a group. As if now the user may experience a few \nsecond lag interpolated the speech and translation, which Waverly Labs pursue to reduce. \nThe Pilot earpiece will be available from September, but can be pre-ordered now for $249. \nThe earpieces can also be used for streaming music, answering voice calls and getting audio \nnotifications. \nLink:https://www.indiegogo.com/projects/meet-the-pilot-smart-earpiece-language-translator-\nheadphones-travel#/ \n \nREFRENCES \n[1] Chomsky, Noam, 1965, Aspects of the Theory of Syntax, Cambridge, Massachusetts: \nMIT Press.  \n [2] Rospocher, M., van Erp, M., Vossen, P., Fokkens, A., Aldabe,I., Rigau, G., Soroa, A., \nPloeger, T., and Bogaard, T.(2016). Building event-centric knowledge graphs from news. \nWeb Semantics: Science, Services and Agents on the World Wide Web, In Press. \n[3] Shemtov, H. (1997). Ambiguity management in natural language generation. Stanford \nUniversity.  \n[4] Emele, M. C., & Dorna, M. (1998, August). Ambiguity preserving machine translation \nusing packed representations. In Proceedings of the 36th Annual Meeting of the Association \nfor Computational Linguistics and 17th International Conference on Computational \nLinguistics-Volume 1 (pp. 365-371). Association for Computational Linguistics. \n[5] Knight, K., & Langkilde, I. (2000, July). Preserving ambiguities in generation via \nautomata intersection. In AAAI/IAAI (pp. 697-702). \n[6] Nation, K., Snowling, M. J., & Clarke, P. (2007). Dissecting the relationship between \nlanguage skills and learning to read: Semantic and phonological contributions to new \nvocabulary learning in children with poor reading comprehension. Advances in Speech \nLanguage Pathology, 9(2), 131-139. \n[7] Liddy, E. D. (2001). Natural language processing. \n[8] Feldman, S. (1999). NLP Meets the Jabberwocky: Natural Language Processing in \nInformation Retrieval. ONLINE-WESTON THEN WILTON-, 23, 62-73. \n[9] \"Natural Language Processing.\" Natural Language Processing RSS. N.p., n.d. Web. 25 \nMar. 2017 \n[10] Hutchins, W. J. (1986). Machine translation: past, present, future (p. 66). Chichester: \nEllis Horwood. \n[11] Hutchins, W. J. (Ed.). (2000). Early years in machine translation: memoirs and \nbiographies of pioneers (Vol. 97). John Benjamins Publishing. \n[12] Green Jr, B. F., Wolf, A. K., Chomsky, C., & Laughery, K. (1961, May). Baseball: an \nautomatic question-answerer. In Papers presented at the May 9-11, 1961, western joint IRE-\nAIEE-ACM computer conference (pp. 219-224). ACM. \n[13] Woods, W. A. (1978). Semantics and quantification in natural language question \nanswering. Advances in computers, 17, 1-87. \n[14] Hendrix, G. G., Sacerdoti, E. D., Sagalowicz, D., & Slocum, J. (1978). Developing a \nnatural language interface to complex data. ACM Transactions on Database Systems \n(TODS), 3(2), 105-147. \n[15] Alshawi, H. (1992). The core language engine. MIT press. \n[16] Kamp, H., & Reyle, U. (1993). Tense and Aspect. In From Discourse to Logic (pp. 483-\n689). Springer Netherlands. \n[17] Lea , W.A Trends in speech recognition , Englewoods Cliffs , NJ: Prentice Hall , 1980. \n[18] Young, S. J., & Chase, L. L. (1998). Speech recognition evaluation: a review of the US \nCSR and LVCSR programmes. Computer Speech & Language, 12(4), 263-279. \n[19] Sundheim, B. M., & Chinchor, N. A. (1993, March). Survey of the message \nunderstanding conferences. In Proceedings of the workshop on Human Language \nTechnology (pp. 56-60). Association for Computational Linguistics. \n[20] Wahlster, W., & Kobsa, A. (1989). User models in dialog systems. In User models in \ndialog systems (pp. 4-34). Springer Berlin Heidelberg. \n[21] McKeown, K.R. Text generation , Cambridge: Cambridge University Press , 1985. \n[22] Small S.L., Cortell G.W., and Tanenhaus , M.K. Lexical Ambiguity Resolutions , San \nMateo , CA : Morgan Kauffman, 1988. \n[23] Manning, C. D., & Schütze, H. (1999). Foundations of statistical natural language \nprocessing (Vol. 999). Cambridge: MIT press. \n[24] Mani, I., & Maybury, M. T. (Eds.). (1999). Advances in automatic text \nsummarization (Vol. 293). Cambridge, MA: MIT press. \n[25] Yi, J., Nasukawa, T., Bunescu, R., & Niblack, W. (2003, November). Sentiment \nanalyzer: Extracting sentiments about a given topic using natural language processing \ntechniques. In Data Mining, 2003. ICDM 2003. Third IEEE International Conference on (pp. \n427-434). IEEE. \n[26] Yi, J., Nasukawa, T., Bunescu, R., & Niblack, W. (2003, November). Sentiment \nanalyzer: Extracting sentiments about a given topic using natural language processing \ntechniques. In Data Mining, 2003. ICDM 2003. Third IEEE International Conference on (pp. \n427-434). IEEE. \n[27] Tapaswi, N., & Jain, S. (2012, September). Treebank based deep grammar acquisition \nand Part-Of-Speech Tagging for Sanskrit sentences. In Software Engineering (CONSEG), \n2012 CSI Sixth International Conference on (pp. 1-4). IEEE. \n[28] Ranjan, P., & Basu, H. V. S. S. A. (2003). Part of speech tagging and local word \ngrouping techniques for natural language parsing in Hindi. In Proceedings of the 1st \nInternational Conference on Natural Language Processing (ICON 2003). \n[29] Diab, M., Hacioglu, K., & Jurafsky, D. (2004, May). Automatic tagging of Arabic text: \nFrom raw text to base phrase chunks. In Proceedings of HLT-NAACL 2004: Short \npapers (pp. 149-152). Association for Computational Linguistics. \n[30] Sha, F., & Pereira, F. (2003, May). Shallow parsing with conditional random fields. \nIn Proceedings of the 2003 Conference of the North American Chapter of the Association for \nComputational Linguistics on Human Language Technology-Volume 1 (pp. 134-141). \nAssociation for Computational Linguistics. \n[31] McDonald, R., Crammer, K., & Pereira, F. (2005, October). Flexible text segmentation \nwith structured multilabel classification. In Proceedings of the conference on Human \nLanguage Technology and Empirical Methods in Natural Language Processing (pp. 987-\n994). Association for Computational Linguistics. \n[32] Sun, X., Morency, L. P., Okanohara, D., & Tsujii, J. I. (2008, August). Modeling latent-\ndynamic in shallow parsing: a latent conditional model with improved inference. \nIn Proceedings of the 22nd International Conference on Computational Linguistics-Volume \n1 (pp. 841-848). Association for Computational Linguistics. \n[33] Ritter, A., Clark, S., & Etzioni, O. (2011, July). Named entity recognition in tweets: an \nexperimental study. In Proceedings of the Conference on Empirical Methods in Natural \nLanguage Processing (pp. 1524-1534). Association for Computational Linguistics. \n[34] Sharma, S., Srinivas, PYKL, & Balabantaray, RC (2016). Emotion Detection using \nOnline Machine Learning Method and TLBO on Mixed Script. In Proceedings of Language \nResources and Evaluation Conference 2016 (pp. 47-51). \n[35] Palmer, M., Gildea, D., & Kingsbury, P. (2005). The proposition bank: An annotated \ncorpus of semantic roles. Computational linguistics, 31(1), 71-106. \n[36] Benson, E., Haghighi, A., & Barzilay, R. (2011, June). Event discovery in social media \nfeeds. In Proceedings of the 49th Annual Meeting of the Association for Computational \nLinguistics: Human Language Technologies-Volume 1 (pp. 389-398). Association for \nComputational Linguistics. \n[37] Tillmann, C., Vogel, S., Ney, H., Zubiaga, A., & Sawaf, H. (1997, September). \nAccelerated DP based search for statistical translation. In Eurospeech. \n[38] Bangalore, S., Rambow, O., & Whittaker, S. (2000, June). Evaluation metrics for \ngeneration. In Proceedings of the first international conference on Natural language \ngeneration-Volume 14 (pp. 1-8). Association for Computational Linguistics \n[39] Nießen, S., Och, F. J., Leusch, G., & Ney, H. (2000, May). An Evaluation Tool for \nMachine Translation: Fast Evaluation for MT Research. In LREC \n[40] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for \nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting on \nassociation for computational linguistics (pp. 311-318). Association for Computational \nLinguistics \n[41] Doddington, G. (2002, March). Automatic evaluation of machine translation quality \nusing n-gram co-occurrence statistics. In Proceedings of the second international conference \non Human Language Technology Research (pp. 138-145). Morgan Kaufmann Publishers Inc  \n[42] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for \nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting on \nassociation for computational linguistics (pp. 311-318). Association for Computational \nLinguistics \n[43] Doddington, G. (2002, March). Automatic evaluation of machine translation quality \nusing n-gram co-occurrence statistics. In Proceedings of the second international conference \non Human Language Technology Research (pp. 138-145). Morgan Kaufmann Publishers Inc \n[44] Hayes, P. J. (1992). Intelligent high-volume text processing using shallow, domain-\nspecific techniques. Text-based intelligent systems: Current research and practice in \ninformation extraction and retrieval, 227-242. \n[45] Cohen, W. W. (1996, March). Learning rules that classify e-mail. In AAAI spring \nsymposium on machine learning in information access (Vol. 18, p. 25). \n[46] Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998, July). A Bayesian \napproach to filtering junk e-mail. In Learning for Text Categorization: Papers from the 1998 \nworkshop (Vol. 62, pp. 98-105). \n[47] Androutsopoulos, I., Paliouras, G., Karkaletsis, V., Sakkis, G., Spyropoulos, C. D., & \nStamatopoulos, P. (2000). Learning to filter spam e-mail: A comparison of a naive bayesian \nand a memory-based approach. arXiv preprint cs/0009009. \n[48] Rennie, J. (2000, August). ifile: An application of machine learning to e-mail filtering. \nIn Proc. KDD 2000 Workshop on Text Mining, Boston, MA \n[49] Drucker, H., Wu, D., & Vapnik, V. N. (1999). Support vector machines for spam \ncategorization. IEEE Transactions on Neural networks, 10(5), 1048-1054 \n[50] Carreras, X., & Marquez, L. (2001). Boosting trees for anti-spam email filtering. arXiv \npreprint cs/0109015 \n[51] BERGER, A. L., DELLA PIETRA, S. A., AND DELLA PIETRA, V. J. 1996. A \nmaximum entropy approach to natural language processing. Computational Linguistics 22, 1, \n39–71 \n[52] Sakkis, G., Androutsopoulos, I., Paliouras, G., Karkaletsis, V., Spyropoulos, C. D., & \nStamatopoulos, P. (2001). Stacking classifiers for anti-spam filtering of e-mail. arXiv preprint \ncs/0106040.. \n[53] Lewis, D. D. (1998, April). Naive (Bayes) at forty: The independence assumption in \ninformation retrieval. In European conference on machine learning (pp. 4-15). Springer \nBerlin Heidelberg \n[54] McCallum, A., & Nigam, K. (1998, July). A comparison of event models for naive bayes \ntext classification. In AAAI-98 workshop on learning for text categorization (Vol. 752, pp. \n41-48). \n[55] McCallum, A., & Nigam, K. (1998, July). A comparison of event models for naive bayes \ntext classification. In AAAI-98 workshop on learning for text categorization (Vol. 752, pp. \n41-48). \n[56] Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137 \n[57] Hayes, P. J. (1992). Intelligent high-volume text processing using shallow, domain-\nspecific techniques. Text-based intelligent systems: Current research and practice in \ninformation extraction and retrieval, 227-242 \n[58] Morin, E. (1999, August). Automatic acquisition of semantic relations between terms \nfrom technical corpora. In Proc. of the Fifth International Congress on Terminology and \nKnowledge Engineering-TKE’99. \n[59] Bondale, N., Maloor, P., Vaidyanathan, A., Sengupta, S., & Rao, P. V. (1999). \nExtraction of information from open-ended questionnaires using natural language processing \ntechniques. Computer Science and Informatics, 29(2), 15-22 \n[60] Glasgow, B., Mandell, A., Binney, D., Ghemri, L., & Fisher, D. (1998). MITA: An \ninformation-extraction approach to the analysis of free-form text in life insurance \napplications. AI magazine, 19(1), 59. \n[61] Ahonen, H., Heinonen, O., Klemettinen, M., & Verkamo, A. I. (1998, April). Applying \ndata mining techniques for descriptive phrase extraction in digital document collections. \nIn Research and Technology Advances in Digital Libraries, 1998. ADL 98. Proceedings. \nIEEE International Forum on (pp. 2-11). IEEE. \n[62] Zajic, D. M., Dorr, B. J., & Lin, J. (2008). Single-document and multi-document \nsummarization techniques for email threads using sentence compression. Information \nProcessing & Management, 44(4), 1600-1610. \n[63] Fattah, M. A., & Ren, F. (2009). GA, MR, FFNN, PNN and GMM based models for \nautomatic text summarization. Computer Speech & Language, 23(1), 126-144. \n[64] Gong, Y., & Liu, X. (2001, September). Generic text summarization using relevance \nmeasure and latent semantic analysis. In Proceedings of the 24th annual international ACM \nSIGIR conference on Research and development in information retrieval (pp. 19-25). ACM. \n[65] Dunlavy, D. M., O’Leary, D. P., Conroy, J. M., & Schlesinger, J. D. (2007). QCS: A \nsystem for querying, clustering and summarizing documents. Information processing & \nmanagement, 43(6), 1588-1605. \n[66] Wan, X. (2008). Using only cross-document relationships for both generic and topic-\nfocused multi-document summarizations. Information Retrieval, 11(1), 25-49. \n[67] Ouyang, Y., Li, W., Li, S., & Lu, Q. (2011). Applying regression models to query-\nfocused multi-document summarization. Information Processing & Management, 47(2), 227-\n237. \n[68] Mani, I., & Maybury, M. T. (Eds.). (1999). Advances in automatic text \nsummarization (Vol. 293). Cambridge, MA: MIT press. \n[69] Riedhammer, K., Favre, B., & Hakkani-Tür, D. (2010). Long story short–global \nunsupervised \nmodels \nfor \nkeyphrase \nbased \nmeeting \nsummarization. Speech \nCommunication, 52(10), 801-815. \n[70] Wang, D., Zhu, S., Li, T., & Gong, Y. (2009, August). Multi-document summarization \nusing sentence-based topic models. In Proceedings of the ACL-IJCNLP 2009 Conference \nShort Papers (pp. 297-300). Association for Computational Linguistics. \n[71] Wang, D., Zhu, S., Li, T., Chi, Y., & Gong, Y. (2011). Integrating document clustering \nand multidocument summarization. ACM Transactions on Knowledge Discovery from Data \n(TKDD), 5(3), 14. \n[72] Fang, H., Lu, W., Wu, F., Zhang, Y., Shang, X., Shao, J., & Zhuang, Y. (2015). Topic \naspect-oriented summarization via group selection. Neurocomputing, 149, 1613-1619. \n[73] Sager, N., Lyman, M., Nhan, N. T., & Tick, L. J. (1995). Medical language processing: \napplications to patient data representation and automatic encoding. Methods of information in \nmedicine, 34(1-2), 140-146. \n[74] Chi, E. C., Lyman, M. S., Sager, N., Friedman, C., & Macleod, C. (1985, November). A \ndatabase of computer-structured narrative: methods of computing complex relations. \nIn Proceedings of the Annual Symposium on Computer Application in Medical Care (p. 221). \nAmerican Medical Informatics Association. \n[75] Grishman, R., Sager, N., Raze, C., & Bookchin, B. (1973, June). The linguistic string \nparser. In Proceedings of the June 4-8, 1973, national computer conference and \nexposition (pp. 427-434). ACM. \n[76] Hirschman, L., Grishman, R., & Sager, N. (1976, June). From text to structured \ninformation: automatic processing of medical reports. In Proceedings of the June 7-10, 1976, \nnational computer conference and exposition (pp. 267-275). ACM. \n[77] Sager, N. (1981). Natural language information processing. Addison-Wesley Publishing \nCompany, Advanced Book Program. \n[78] Lyman, M., Sager, N., Friedman, C., & Chi, E. (1985, November). Computer-structured \nnarrative in ambulatory care: its use in longitudinal review of clinical data. In Proceedings of \nthe Annual Symposium on Computer Application in Medical Care (p. 82). American Medical \nInformatics Association. \n[79] McCray, A. T., & Nelson, S. J. (1995). The representation of meaning in the \nUMLS. Methods of information in medicine, 34(1-2), 193-201. \n[80] McGray, A. T., Sponsler, J. L., Brylawski, B., & Browne, A. C. (1987, November). The \nrole of lexical knowledge in biomedical text understanding. In Proceedings of the Annual \nSymposium on Computer Application in Medical Care (p. 103). American Medical \nInformatics Association. \n[81] McCray, A. T. (1991). Natural language processing for intelligent information retrieval. \nIn Engineering in Medicine and Biology Society, 1991. Vol. 13: 1991., Proceedings of the \nAnnual International Conference of the IEEE (pp. 1160-1161). IEEE. \n[82] McCray, A. T. (1991). Extending a natural language parser with UMLS knowledge. \nIn Proceedings of the Annual Symposium on Computer Application in Medical Care (p. 194). \nAmerican Medical Informatics Association. \n[83] McCray, A. T., Srinivasan, S., & Browne, A. C. (1994). Lexical methods for managing \nvariation in biomedical terminologies. In Proceedings of the Annual Symposium on \nComputer Application in Medical Care (p. 235). American Medical Informatics Association. \n[84] McCray, A. T., & Razi, A. (1994). The UMLS Knowledge Source server. Medinfo. \nMEDINFO, 8, 144-147. \n[85] Scherrer, J. R., Revillard, C., Borst, F., Berthoud, M., & Lovis, C. (1994). Medical office \nautomation integrated into the distributed architecture of a hospital information \nsystem. Methods of information in medicine, 33(2), 174-179. \n[86] Baud, R. H., Rassinoux, A. M., & Scherrer, J. R. (1992). Natural language processing \nand semantical representation of medical texts. Methods of information in medicine, 31(2), \n117-125. \n[87] Lyman, M., Sager, N., Chi, E. C., Tick, L. J., Nhan, N. T., Su, Y., ... & Scherrer, J. \n(1989, November). Medical Language Processing for Knowledge Representation and \nRetrievals. In Proceedings. Symposium on Computer Applications in Medical Care (pp. 548-\n553). American Medical Informatics Association. \n[88] Nhàn, N. T., Sager, N., Lyman, M., Tick, L. J., Borst, F., & Su, Y. (1989, November). A \nMedical Language Processor for Two Indo-European Languages. In Proceedings. \nSymposium on Computer Applications in Medical Care (pp. 554-558). American Medical \nInformatics Association. \n[89] Sager, N., Lyman, M., Tick, L. J., Borst, F., Nhan, N. T., Revillard, C., ... & Scherrer, J. \nR. (1989). Adapting a medical language processor from English to French. Medinfo, 89, 795-\n799. \n[90] Borst, F., Sager, N., Nhàn, N. T., Su, Y., Lyman, M., Tick, L. J., ... & Scherrer, J. R. \n(1989). Analyse automatique de comptes rendus d'hospitalisation. In Degoulet P, Stephan JC, \nVenot A, Yvon PJ, rédacteurs. Informatique et Santé, Informatique et Gestion des Unités de \nSoins, Comptes Rendus du Colloque AIM-IF, Paris (pp. 246-56). [5] \n[91] Baud, R. H., Rassinoux, A. M., & Scherrer, J. R. (1991). Knowledge representation of \ndischarge summaries. In AIME 91 (pp. 173-182). Springer Berlin Heidelberg. \n[92] Baud, R. H., Alpay, L., & Lovis, C. (1994). Let’s Meet the Users with Natural Language \nUnderstanding. Knowledge and Decisions in Health Telematics: The Next Decade, 12, 103. \n[93] Rassinoux, A. M., Baud, R. H., & Scherrer, J. R. (1992). Conceptual graphs model \nextension for knowledge representation of medical texts. MEDINFO, 92, 1368-1374. \n[94] Morel-Guillemaz, A. M., Baud, R. H., & Scherrer, J. R. (1990). Proximity Processing of \nMedical Text. In Medical Informatics Europe’90 (pp. 625-630). Springer Berlin Heidelberg. \n[95] Rassinoux, A. M., Michel, P. A., Juge, C., Baud, R., & Scherrer, J. R. (1994). Natural \nlanguage processing of medical texts within the HELIOS environment. Computer methods \nand programs in biomedicine, 45, S79-96. \n[96] Rassinoux, A. M., Juge, C., Michel, P. A., Baud, R. H., Lemaitre, D., Jean, F. C., ... & \nScherrer, J. R. (1995, June). Analysis of medical jargon: The RECIT system. In Conference \non Artificial Intelligence in Medicine in Europe (pp. 42-52). Springer Berlin Heidelberg. \n[97] Friedman, C., Cimino, J. J., & Johnson, S. B. (1993). A conceptual model for clinical \nradiology reports. In Proceedings of the Annual Symposium on Computer Application in \nMedical Care (p. 829). American Medical Informatics Association. \n[98] \"Natural Language Processing.\" Natural Language Processing RSS. N.p., n.d. Web. 23 \nMar. 2017.   \n[99] [Srihari S. Machine Learning: Generative and Discriminative Models. 2010. http:// \nwww.cedar.buffalo.edu/wsrihari/CSE574/Discriminative-Generative.pdf (accessed 31 May \n2011).] \n[100] [Elkan C. Log-Linear Models and Conditional Random Fields. 2008. http://cseweb. \nucsd.edu/welkan/250B/cikmtutorial.pdf (accessed 28 Jun 2011). 62. Hearst MA, Dumais ST, \nOsman E, et al. Support vector machines] \n[101] [Jurafsky D, Martin JH. Speech and Language Processing. 2nd edn. Englewood Cliffs, \nNJ: Prentice-Hall, 2008.] \n[102] [Sonnhammer ELL, Eddy SR, Birney E, et al. Pfam: Multiple sequence alignments and \nHMM-profiles of protein domains. Nucleic Acids Res 1998;26:320] \n[103] [Sonnhammer, E. L., Eddy, S. R., Birney, E., Bateman, A., & Durbin, R. (1998). Pfam: \nmultiple sequence alignments and HMM-profiles of protein domains. Nucleic acids \nresearch, 26(1), 320-322] \n[104] Systems, RAVN. \"RAVN Systems Launch the ACE Powered GDPR Robot - Artificial \nIntelligence to Expedite GDPR Compliance.\" Stock Market. PR Newswire, n.d. Web. 19 \nMar. 2017. \n [105] \"Here's Why Natural Language Processing is the Future of BI.\" SmartData Collective. \nN.p., n.d. Web. 19 Mar. 2017 \n[106] \"Using Natural Language Processing and Network Analysis to Develop a Conceptual \nFramework for Medication Therapy Management Research.\" AMIA ... Annual Symposium \nproceedings. AMIA Symposium. U.S. National Library of Medicine, n.d. Web. 19 Mar. 2017 \n[107] Ogallo, W., & Kanter, A. S. (2017, February 10). Using Natural Language Processing \nand Network Analysis to Develop a Conceptual Framework for Medication Therapy \nManagement \nResearch. \nRetrieved \nApril \n10, \n2017, \nfrom \nhttps://www.ncbi.nlm.nih.gov/pubmed/28269895?dopt=Abstract \n[108] Ochoa, A. (2016, May 25). Meet the Pilot: Smart Earpiece Language Translator. \nRetrieved April 10, 2017, from https://www.indiegogo.com/projects/meet-the-pilot-smart-\nearpiece-language-translator-headphones-travel \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-08-17",
  "updated": "2017-08-17"
}