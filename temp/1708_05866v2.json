{
  "id": "http://arxiv.org/abs/1708.05866v2",
  "title": "A Brief Survey of Deep Reinforcement Learning",
  "authors": [
    "Kai Arulkumaran",
    "Marc Peter Deisenroth",
    "Miles Brundage",
    "Anil Anthony Bharath"
  ],
  "abstract": "Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field.",
  "text": "IEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n1\nA Brief Survey of Deep Reinforcement Learning\nKai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath\nAbstract—Deep reinforcement learning is poised to revolu-\ntionise the ﬁeld of AI and represents a step towards building\nautonomous systems with a higher level understanding of the\nvisual world. Currently, deep learning is enabling reinforcement\nlearning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep\nreinforcement learning algorithms are also applied to robotics,\nallowing control policies for robots to be learned directly from\ncamera inputs in the real world. In this survey, we begin with\nan introduction to the general ﬁeld of reinforcement learning,\nthen progress to the main streams of value-based and policy-\nbased methods. Our survey will cover central algorithms in\ndeep reinforcement learning, including the deep Q-network,\ntrust region policy optimisation, and asynchronous advantage\nactor-critic. In parallel, we highlight the unique advantages of\ndeep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current\nareas of research within the ﬁeld.\nI. INTRODUCTION\nOne of the primary goals of the ﬁeld of artiﬁcial intelligence\n(AI) is to produce fully autonomous agents that interact with\ntheir environments to learn optimal behaviours, improving over\ntime through trial and error. Crafting AI systems that are\nresponsive and can effectively learn has been a long-standing\nchallenge, ranging from robots, which can sense and react\nto the world around them, to purely software-based agents,\nwhich can interact with natural language and multimedia.\nA principled mathematical framework for experience-driven\nautonomous learning is reinforcement learning (RL) [135]. Al-\nthough RL had some successes in the past [141, 129, 62, 93],\nprevious approaches lacked scalablity and were inherently\nlimited to fairly low-dimensional problems. These limitations\nexist because RL algorithms share the same complexity is-\nsues as other algorithms: memory complexity, computational\ncomplexity, and in the case of machine learning algorithms,\nsample complexity [133]. What we have witnessed in recent\nyears—the rise of deep learning, relying on the powerful\nfunction approximation and representation learning properties\nof deep neural networks—has provided us with new tools to\novercoming these problems.\nThe advent of deep learning has had a signiﬁcant impact\non many areas in machine learning, dramatically improving\nthe state-of-the-art in tasks such as object detection, speech\nrecognition, and language translation [70]. The most important\nproperty of deep learning is that deep neural networks can\nautomatically ﬁnd compact low-dimensional representations\n(features) of high-dimensional data (e.g., images, text and\naudio). Through crafting inductive biases into neural network\narchitectures, particularly that of hierarchical representations,\nmachine learning practitioners have made effective progress\nin addressing the curse of dimensionality [15]. Deep learning\nhas similarly accelerated progress in RL, with the use of\ndeep learning algorithms within RL deﬁning the ﬁeld of\n“deep reinforcement learning” (DRL). The aim of this survey\nis to cover both seminal and recent developments in DRL,\nconveying the innovative ways in which neural networks can\nbe used to bring us closer towards developing autonomous\nagents. For a more comprehensive survey of recent efforts in\nDRL, including applications of DRL to areas such as natural\nlanguage processing [106, 5], we refer readers to the overview\nby Li [78].\nDeep learning enables RL to scale to decision-making\nproblems that were previously intractable, i.e., settings with\nhigh-dimensional state and action spaces. Amongst recent\nwork in the ﬁeld of DRL, there have been two outstanding\nsuccess stories. The ﬁrst, kickstarting the revolution in DRL,\nwas the development of an algorithm that could learn to play\na range of Atari 2600 video games at a superhuman level,\ndirectly from image pixels [84]. Providing solutions for the\ninstability of function approximation techniques in RL, this\nwork was the ﬁrst to convincingly demonstrate that RL agents\ncould be trained on raw, high-dimensional observations, solely\nbased on a reward signal. The second standout success was\nthe development of a hybrid DRL system, AlphaGo, that\ndefeated a human world champion in Go [128], paralleling the\nhistoric achievement of IBM’s Deep Blue in chess two decades\nearlier [19] and IBM’s Watson DeepQA system that beat the\nbest human Jeopardy! players [31]. Unlike the handcrafted\nrules that have dominated chess-playing systems, AlphaGo\nwas composed of neural networks that were trained using\nsupervised and reinforcement learning, in combination with\na traditional heuristic search algorithm.\nDRL algorithms have already been applied to a wide range\nof problems, such as robotics, where control policies for robots\ncan now be learned directly from camera inputs in the real\nworld [74, 75], succeeding controllers that used to be hand-\nengineered or learned from low-dimensional features of the\nrobot’s state. In a step towards even more capable agents,\nDRL has been used to create agents that can meta-learn (“learn\nto learn”) [29, 156], allowing them to generalise to complex\nvisual environments they have never seen before [29]. In\nFigure 1, we showcase just some of the domains that DRL\nhas been applied to, ranging from playing video games [84]\nto indoor navigation [167].\nVideo games may be an interesting challenge, but learning\nhow to play them is not the end goal of DRL. One of the\ndriving forces behind DRL is the vision of creating systems\nthat are capable of learning how to adapt in the real world.\nFrom managing power consumption [142] to picking and\nstowing objects [75], DRL stands to increase the amount\nof physical tasks that can be automated by learning. How-\never, DRL does not stop there, as RL is a general way of\napproaching optimisation problems by trial and error. From\ndesigning state-of-the-art machine translation models [168] to\nconstructing new optimisation functions [76], DRL has already\nbeen used to approach all manner of machine learning tasks.\narXiv:1708.05866v2  [cs.LG]  28 Sep 2017\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n2\nFig. 1.\nA range of visual RL domains. (a) Two classic Atari 2600 video games, “Freeway” and “Seaquest”, from the Arcade Learning Environment\n(ALE) [10]. Due to the range of supported games that vary in genre, visuals and difﬁculty, the ALE has become a standard testbed for DRL algorithms\n[84, 95, 44, 122, 132, 157, 85]. As we will discuss later, the ALE is one of several benchmarks that are now being used to standardise evaluation in RL.\n(b) The TORCS car racing simulator, which has been used to test DRL algorithms that can output continuous actions [64, 79, 85] (as the games from the\nALE only support discrete actions). (c) Utilising the potentially unlimited amount of training data that can be amassed in robotic simulators, several methods\naim to transfer knowledge from the simulator to the real world [22, 115, 146]. (d) Two of the four robotic tasks designed by Levine et al. [74]: screwing\non a bottle cap and placing a shaped block in the correct hole. Levine et al. [74] were able to train visuomotor policies in an end-to-end fashion, showing\nthat visual servoing could be learned directly from raw camera inputs by using deep neural networks. (e) A real room, in which a wheeled robot trained to\nnavigate the building is given a visual cue as input, and must ﬁnd the corresponding location [167]. (f) A natural image being captioned by a neural network\nthat uses reinforcement learning to choose where to look [166]. By processing a small portion of the image for every word generated, the network can focus\nits attention on the most salient points. Figures reproduced from [10, 79, 146, 74, 167, 166], respectively.\nAnd, in the same way that deep learning has been utilised\nacross many branches of machine learning, it seems likely\nthat in the future, DRL will be an important component in\nconstructing general AI systems [68].\nII. REWARD-DRIVEN BEHAVIOUR\nBefore examining the contributions of deep neural networks\nto RL, we will introduce the ﬁeld of RL in general. The\nessence of RL is learning through interaction. An RL agent\ninteracts with its environment and, upon observing the conse-\nquences of its actions, can learn to alter its own behaviour in\nresponse to rewards received. This paradigm of trial-and error-\nlearning has its roots in behaviourist psychology, and is one\nof the main foundations of RL [135]. The other key inﬂuence\non RL is optimal control, which has lent the mathematical\nformalisms (most notably dynamic programming [13]) that\nunderpin the ﬁeld.\nIn the RL set-up, an autonomous agent, controlled by\na machine learning algorithm, observes a state st from its\nenvironment at timestep t. The agent interacts with the envi-\nronment by taking an action at in state st. When the agent\ntakes an action, the environment and the agent transition to\na new state st+1 based on the current state and the chosen\naction. The state is a sufﬁcient statistic of the environment\nand thereby comprises all the necessary information for the\nagent to take the best action, which can include parts of the\nagent, such as the position of its actuators and sensors. In the\noptimal control literature, states and actions are often denoted\nby xt and ut, respectively.\nThe best sequence of actions is determined by the rewards\nprovided by the environment. Every time the environment\ntransitions to a new state, it also provides a scalar reward\nrt+1 to the agent as feedback. The goal of the agent is to\nlearn a policy (control strategy) π that maximises the expected\nreturn (cumulative, discounted reward). Given a state, a policy\nreturns an action to perform; an optimal policy is any policy\nthat maximises the expected return in the environment. In\nthis respect, RL aims to solve the same problem as optimal\ncontrol. However, the challenge in RL is that the agent needs\nto learn about the consequences of actions in the environment\nby trial and error, as, unlike in optimal control, a model of the\nstate transition dynamics is not available to the agent. Every\ninteraction with the environment yields information, which the\nagent uses to update its knowledge. This perception-action-\nlearning loop is illustrated in Figure 2.\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n3\nFig. 2. The perception-action-learning loop. At time t, the agent receives state st from the environment. The agent uses its policy to choose an action at.\nOnce the action is executed, the environment transitions a step, providing the next state st+1 as well as feedback in the form of a reward rt+1. The agent\nuses knowledge of state transitions, of the form (st, at, st+1, rt+1), in order to learn and improve its policy.\nA. Markov Decision Processes\nFormally, RL can be described as a Markov decision process\n(MDP), which consists of:\n• A set of states S, plus a distribution of starting states\np(s0).\n• A set of actions A.\n• Transition dynamics T (st+1|st, at) that map a state-\naction pair at time t onto a distribution of states at time\nt + 1.\n• An\nimmediate/instantaneous\nreward\nfunction\nR(st, at, st+1).\n• A discount factor γ ∈[0, 1], where lower values place\nmore emphasis on immediate rewards.\nIn general, the policy π is a mapping from states to a\nprobability distribution over actions: π : S →p(A = a|S). If\nthe MDP is episodic, i.e., the state is reset after each episode of\nlength T, then the sequence of states, actions and rewards in an\nepisode constitutes a trajectory or rollout of the policy. Every\nrollout of a policy accumulates rewards from the environment,\nresulting in the return R = PT −1\nt=0 γtrt+1. The goal of RL is\nto ﬁnd an optimal policy, π∗, which achieves the maximum\nexpected return from all states:\nπ∗= argmax\nπ\nE[R|π].\n(1)\nIt is also possible to consider non-episodic MDPs, where\nT = ∞. In this situation, γ < 1 prevents an inﬁnite sum\nof rewards from being accumulated. Furthermore, methods\nthat rely on complete trajectories are no longer applicable,\nbut those that use a ﬁnite set of transitions still are.\nA key concept underlying RL is the Markov property—\nonly the current state affects the next state, or in other words,\nthe future is conditionally independent of the past given\nthe present state. This means that any decisions made at st\ncan be based solely on st−1, rather than {s0, s1, . . . , st−1}.\nAlthough this assumption is held by the majority of RL\nalgorithms, it is somewhat unrealistic, as it requires the states\nto be fully observable. A generalisation of MDPs are partially\nobservable MDPs (POMDPs), in which the agent receives an\nobservation ot ∈Ω, where the distribution of the observation\np(ot+1|st+1, at) is dependent on the current state and the\nprevious action [56]. In a control and signal processing con-\ntext, the observation would be described by a measurement/\nobservation mapping in a state-space-model that depends on\nthe current state and the previously applied action.\nPOMDP algorithms typically maintain a belief over the\ncurrent state given the previous belief state, the action taken\nand the current observation. A more common approach in\ndeep learning is to utilise recurrent neural networks (RNNs)\n[163, 44, 45, 85, 96], which, unlike feedforward neural\nnetworks, are dynamical systems. This approach to solving\nPOMDPs is related to other problems using dynamical systems\nand state space models, where the true state can only be\nestimated [16].\nB. Challenges in RL\nIt is instructive to emphasise some challenges faced in RL:\n• The optimal policy must be inferred by trial-and-error\ninteraction with the environment. The only learning signal\nthe agent receives is the reward.\n• The observations of the agent depend on its actions and\ncan contain strong temporal correlations.\n• Agents must deal with long-range time dependencies:\nOften the consequences of an action only materialise after\nmany transitions of the environment. This is known as the\n(temporal) credit assignment problem [135].\nWe will illustrate these challenges in the context of an\nindoor robotic visual navigation task: if the goal location is\nspeciﬁed, we may be able to estimate the distance remaining\n(and use it as a reward signal), but it is unlikely that we will\nknow exactly what series of actions the robot needs to take\nto reach the goal. As the robot must choose where to go as it\nnavigates the building, its decisions inﬂuence which rooms it\nsees and, hence, the statistics of the visual sequence captured.\nFinally, after navigating several junctions, the robot may ﬁnd\nitself in a dead end. There is a range of problems, from\nlearning the consequences of actions to balancing exploration\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n4\nagainst exploitation, but ultimately these can all be addressed\nformally within the framework of RL.\nIII. REINFORCEMENT LEARNING ALGORITHMS\nSo far, we have introduced the key formalism used in RL,\nthe MDP, and brieﬂy noted some challenges in RL. In the\nfollowing, we will distinguish between different classes of\nRL algorithms. There are two main approaches to solving\nRL problems: methods based on value functions and methods\nbased on policy search. There is also a hybrid, actor-critic\napproach, which employs both value functions and policy\nsearch. We will now explain these approaches and other useful\nconcepts for solving RL problems.\nA. Value Functions\nValue function methods are based on estimating the value\n(expected return) of being in a given state. The state-value\nfunction V π(s) is the expected return when starting in state s\nand following π henceforth:\nV π(s) = E[R|s, π]\n(2)\nThe optimal policy, π∗, has a corresponding state-value\nfunction V ∗(s), and vice-versa, the optimal state-value func-\ntion can be deﬁned as\nV ∗(s) = max\nπ\nV π(s)\n∀s ∈S.\n(3)\nIf we had V ∗(s) available, the optimal policy could be re-\ntrieved by choosing among all actions available at st and pick-\ning the action a that maximises Est+1∼T (st+1|st,a)[V ∗(st+1)].\nIn the RL setting, the transition dynamics T are unavailable.\nTherefore, we construct another function, the state-action-\nvalue or quality function Qπ(s, a), which is similar to V π,\nexcept that the initial action a is provided, and π is only\nfollowed from the succeeding state onwards:\nQπ(s, a) = E[R|s, a, π].\n(4)\nThe best policy, given Qπ(s, a), can be found by choosing a\ngreedily at every state: argmaxa Qπ(s, a). Under this policy,\nwe can also deﬁne V π(s) by maximising Qπ(s, a): V π(s) =\nmaxa Qπ(s, a).\nDynamic Programming: To actually learn Qπ, we exploit\nthe Markov property and deﬁne the function as a Bellman\nequation [13], which has the following recursive form:\nQπ(st, at) = Est+1[rt+1 + γQπ(st+1, π(st+1))].\n(5)\nThis means that Qπ can be improved by bootstrapping, i.e.,\nwe can use the current values of our estimate of Qπ to improve\nour estimate. This is the foundation of Q-learning [159] and\nthe state-action-reward-state-action (SARSA) algorithm [112]:\nQπ(st, at) ←Qπ(st, at) + αδ,\n(6)\nwhere α is the learning rate and δ = Y −Qπ(st, at) the tem-\nporal difference (TD) error; here, Y is a target as in a standard\nregression problem. SARSA, an on-policy learning algorithm,\nis used to improve the estimate of Qπ by using transitions\ngenerated by the behavioural policy (the policy derived from\nQπ), which results in setting Y = rt + γQπ(st+1, at+1). Q-\nlearning is off-policy, as Qπ is instead updated by transitions\nthat were not necessarily generated by the derived policy.\nInstead, Q-learning uses Y = rt+γ maxa Qπ(st+1, a), which\ndirectly approximates Q∗.\nTo ﬁnd Q∗from an arbitrary Qπ, we use generalised\npolicy iteration, where policy iteration consists of policy eval-\nuation and policy improvement. Policy evaluation improves\nthe estimate of the value function, which can be achieved\nby minimising TD errors from trajectories experienced by\nfollowing the policy. As the estimate improves, the policy can\nnaturally be improved by choosing actions greedily based on\nthe updated value function. Instead of performing these steps\nseparately to convergence (as in policy iteration), generalised\npolicy iteration allows for interleaving the steps, such that\nprogress can be made more rapidly.\nB. Sampling\nInstead of bootstrapping value functions using dynamic\nprogramming methods, Monte Carlo methods estimate the\nexpected return (2) from a state by averaging the return from\nmultiple rollouts of a policy. Because of this, pure Monte Carlo\nmethods can also be applied in non-Markovian environments.\nOn the other hand, they can only be used in episodic MDPs,\nas a rollout has to terminate for the return to be calculated.\nIt is possible to get the best of both methods by combining\nTD learning and Monte Carlo policy evaluation, as in done in\nthe TD(λ) algorithm [135]. Similarly to the discount factor,\nthe λ in TD(λ) is used to interpolate between Monte Carlo\nevaluation and bootstrapping. As demonstrated in Figure 3,\nthis results in an entire spectrum of RL methods based around\nthe amount of sampling utilised.\nAnother major value-function based method relies on learn-\ning the advantage function Aπ(s, a) [6, 43]. Unlike producing\nabsolute state-action values, as with Qπ, Aπ instead represents\nrelative state-action values. Learning relative values is akin\nto removing a baseline or average level of a signal; more\nintuitively, it is easier to learn that one action has better\nconsequences than another, than it is to learn the actual return\nfrom taking the action. Aπ represents a relative advantage\nof actions through the simple relationship Aπ = Qπ −V π,\nand is also closely related to the baseline method of variance\nreduction within gradient-based policy search methods [164].\nThe idea of advantage updates has been utilised in many recent\nDRL algorithms [157, 40, 85, 123].\nC. Policy Search\nPolicy search methods do not need to maintain a value\nfunction model, but directly search for an optimal policy\nπ∗. Typically, a parameterised policy πθ is chosen, whose\nparameters are updated to maximise the expected return E[R|θ]\nusing either gradient-based or gradient-free optimisation [26].\nNeural networks that encode policies have been successfully\ntrained using both gradient-free [37, 23, 64] and gradient-\nbased [164, 163, 46, 79, 122, 123, 74] methods. Gradient-free\noptimisation can effectively cover low-dimensional parameter\nspaces, but despite some successes in applying them to large\nnetworks [64], gradient-based training remains the method of\nchoice for most DRL algorithms, being more sample-efﬁcient\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n5\nFig. 3. Two dimensions of RL algorithms, based on the backups used to learn\nor construct a policy. At the extremes of these dimensions are (a) dynamic\nprogramming, (b) exhaustive search, (c) one-step TD learning and (d) pure\nMonte Carlo approaches. Bootstrapping extends from (c) 1-step TD learning\nto n-step TD learning methods [135], with (d) pure Monte Carlo approaches\nnot relying on bootstrapping at all. Another possible dimension of variation\nis choosing to (c, d) sample actions versus (a, b) taking the expectation over\nall choices. Recreated from [135].\nFig. 4.\nActor-critic set-up. The actor (policy) receives a state from the\nenvironment and chooses an action to perform. At the same time, the critic\n(value function) receives the state and reward resulting from the previous\ninteraction. The critic uses the TD error calculated from this information to\nupdate itself and the actor. Recreated from [135].\nwhen policies possess a large number of parameters.\nWhen constructing the policy directly, it is common to\noutput parameters for a probability distribution; for continuous\nactions, this could be the mean and standard deviations of\nGaussian distributions, whilst for discrete actions this could\nbe the individual probabilities of a multinomial distribution.\nThe result is a stochastic policy from which we can directly\nsample actions. With gradient-free methods, ﬁnding better\npolicies requires a heuristic search across a predeﬁned class\nof models. Methods such as evolution strategies essentially\nperform hill-climbing in a subspace of policies [116], whilst\nmore complex methods, such as compressed network search,\nimpose additional inductive biases [64]. Perhaps the greatest\nadvantage of gradient-free policy search is that they can also\noptimise non-differentiable policies.\nPolicy Gradients: Gradients can provide a strong learning\nsignal as to how to improve a parameterised policy. However,\nto compute the expected return (1) we need to average over\nplausible trajectories induced by the current policy parameter-\nisation. This averaging requires either deterministic approxi-\nmations (e.g., linearisation) or stochastic approximations via\nsampling [26]. Deterministic approximations can only be ap-\nplied in a model-based setting where a model of the underlying\ntransition dynamics is available. In the more common model-\nfree RL setting, a Monte Carlo estimate of the expected return\nis determined. For gradient-based learning, this Monte Carlo\napproximation poses a challenge since gradients cannot pass\nthrough these samples of a stochastic function. Therefore, we\nturn to an estimator of the gradient, known in RL as the REIN-\nFORCE rule [164], elsewhere known as the score function [34]\nor likelihood-ratio estimator [36]. The latter name is telling as\nusing the estimator is similar to the practice of optimising\nthe log-likelihood in supervised learning. Intuitively, gradient\nascent using the estimator increases the log probability of the\nsampled action, weighted by the return. More formally, the\nREINFORCE rule can be used to compute the gradient of an\nexpectation over a function f of a random variable X with\nrespect to parameters θ:\n∇θEX[f(X; θ)] = EX[f(X; θ)∇θ log p(X)].\n(7)\nAs this computation relies on the empirical return of a\ntrajectory, the resulting gradients possess a high variance.\nBy introducing unbiased estimates that are less noisy it is\npossible to reduce the variance. The general methodology\nfor performing this is to subtract a baseline, which means\nweighting updates by an advantage rather than the pure return.\nThe simplest baseline is the average return taken over several\nepisodes [164], but many more options are available [123].\nActor-critic Methods: It is possible to combine value\nfunctions with an explicit representation of the policy, resulting\nin actor-critic methods, as shown in Figure 4. The “actor”\n(policy) learns by using feedback from the “critic” (value\nfunction). In doing so, these methods trade off variance\nreduction of policy gradients with bias introduction from value\nfunction methods [63, 123].\nActor-critic methods use the value function as a baseline\nfor policy gradients, such that the only fundamental difference\nbetween actor-critic methods and other baseline methods are\nthat actor-critic methods utilise a learned value function. For\nthis reason, we will later discuss actor-critic methods as a\nsubset of policy gradient methods.\nD. Planning and Learning\nGiven a model of the environment, it is possible to use\ndynamic programming over all possible actions (Figure 3\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n6\n(a)), sample trajectories for heuristic search (as was done by\nAlphaGo [128]), or even perform an exhaustive search (Figure\n3 (b)). Sutton and Barto [135] deﬁne planning as any method\nwhich utilises a model to produce or improve a policy. This\nincludes distribution models, which include T and R, and\nsample models, from which only samples of transitions can\nbe drawn.\nIn RL, we focus on learning without access to the underly-\ning model of the environment. However, interactions with the\nenvironment could be used to learn value functions, policies,\nand also a model. Model-free RL methods learn directly\nfrom interactions with the environment, but model-based RL\nmethods can simulate transitions using the learned model,\nresulting in increased sample efﬁciency. This is particularly\nimportant in domains where each interaction with the environ-\nment is expensive. However, learning a model introduces extra\ncomplexities, and there is always the danger of suffering from\nmodel errors, which in turn affects the learned policy; a com-\nmon but partial solution in this latter scenario is to use model\npredictive control, where planning is repeated after small\nsequences of actions in the real environment [16]. Although\ndeep neural networks can potentially produce very complex\nand rich models [95, 132, 32], sometimes simpler, more data-\nefﬁcient methods are preferable [40]. These considerations\nalso play a role in actor-critic methods with learned value\nfunctions [63, 123].\nE. The Rise of DRL\nMany of the successes in DRL have been based on scaling\nup prior work in RL to high-dimensional problems. This is\ndue to the learning of low-dimensional feature representations\nand the powerful function approximation properties of neural\nnetworks. By means of representation learning, DRL can deal\nefﬁciently with the curse of dimensionality, unlike tabular and\ntraditional non-parametric methods [15]. For instance, convo-\nlutional neural networks (CNNs) can be used as components\nof RL agents, allowing them to learn directly from raw, high-\ndimensional visual inputs. In general, DRL is based on training\ndeep neural networks to approximate the optimal policy π∗,\nand/or the optimal value functions V ∗, Q∗and A∗.\nAlthough there have been DRL successes with gradient-\nfree methods [37, 23, 64], the vast majority of current works\nrely on gradients and hence the backpropagation algorithm\n[162, 111]. The primary motivation is that when available,\ngradients provide a strong learning signal. In reality, these\ngradients are estimated based on approximations, through\nsampling or otherwise, and as such we have to craft algorithms\nwith useful inductive biases in order for them to be tractable.\nThe other beneﬁt of backpropagation is to view the op-\ntimisation of the expected return as the optimisation of a\nstochastic function [121, 46]. This function can comprise of\nseveral parts—models, policies and value functions—which\ncan be combined in various ways. The individual parts, such as\nvalue functions, may not directly optimise the expected return,\nbut can instead embody useful information about the RL\ndomain. For example, using a differentiable model and policy,\nit is possible to forward propagate and backpropagate through\nentire rollouts; on the other hand, innacuracies can accumulate\nover long time steps, and it may be be pertinent to instead use\na value function to summarise the statistics of the rollouts [46].\nWe have previously mentioned that representation learning and\nfunction approximation are key to the success of DRL, but it\nis also true to say that the ﬁeld of deep learning has inspired\nnew ways of thinking about RL.\nFollowing our review of RL, we will now partition the\nnext part of the survey into value function and policy search\nmethods in DRL, starting with the well-known deep Q-\nnetwork (DQN) [84]. In these sections, we will focus on state-\nof-the-art techniques, as well as the historical works they are\nbuilt upon. The focus of the state-of-the-art techniques will be\non those for which the state space is conveyed through visual\ninputs, e.g., images and video. To conclude, we will examine\nongoing research areas and open challenges.\nIV. VALUE FUNCTIONS\nThe well-known function approximation properties of neural\nnetworks led naturally to the use of deep learning to regress\nfunctions for use in RL agents. Indeed, one of the earliest\nsuccess stories in RL is TD-Gammon, a neural network that\nreached expert-level performance in Backgammon in the early\n90s [141]. Using TD methods, the network took in the state of\nthe board to predict the probability of black or white winning.\nAlthough this simple idea has been echoed in later work\n[128], progress in RL research has favoured the explicit use\nof value functions, which can capture the structure underlying\nthe environment. From early value function methods in DRL,\nwhich took simple states as input [109], current methods\nare now able to tackle visually and conceptually complex\nenvironments [84, 122, 85, 96, 167].\nA. Function Approximation and the DQN\nWe begin our survey of value-function-based DRL al-\ngorithms with the DQN [84], pictured in Figure 5, which\nachieved scores across a wide range of classic Atari 2600 video\ngames [10] that were comparable to that of a professional\nvideo games tester. The inputs to the DQN are four greyscale\nframes of the game, concatenated over time, which are initially\nprocessed by several convolutional layers in order to extract\nspatiotemporal features, such as the movement of the ball\nin “Pong” or “Breakout.” The ﬁnal feature map from the\nconvolutional layers is processed by several fully connected\nlayers, which more implicitly encode the effects of actions.\nThis contrasts with more traditional controllers that use ﬁxed\npreprocessing steps, which are therefore unable to adapt their\nprocessing of the state in response to the learning signal.\nA forerunner of the DQN—neural ﬁtted Q iteration\n(NFQ)—involved training a neural network to return the Q-\nvalue given a state-action pair [109]. NFQ was later extended\nto train a network to drive a slot car using raw visual inputs\nfrom a camera over the race track, by combining a deep\nautoencoder to reduce the dimensionality of the inputs with\na separate branch to predict Q-values [69]. Although the pre-\nvious network could have been trained for both reconstruction\nand RL tasks simultaneously, it was both more reliable and\ncomputationally efﬁcient to train the two parts of the network\nsequentially.\nThe DQN [84] is closely related to the model proposed\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n7\nFig. 5. The deep Q-network [84]. The network takes the state—a stack of greyscale frames from the video game—and processes it with convolutional and\nfully connected layers, with ReLU nonlinearities in between each layer. At the ﬁnal layer, the network outputs a discrete action, which corresponds to one of\nthe possible control inputs for the game. Given the current state and chosen action, the game returns a new score. The DQN uses the reward—the difference\nbetween the new score and the previous one—to learn from its decision. More precisely, the reward is used to update its estimate of Q, and the error between\nits previous estimate and its new estimate is backpropagated through the network.\nby Lange et al. [69], but was the ﬁrst RL algorithm that\nwas demonstrated to work directly from raw visual inputs\nand on a wide variety of environments. It was designed such\nthat the ﬁnal fully connected layer outputs Qπ(s, ·) for all\naction values in a discrete set of actions—in this case, the\nvarious directions of the joystick and the ﬁre button. This not\nonly enables the best action, argmaxa Qπ(s, a), to be chosen\nafter a single forward pass of the network, but also allows the\nnetwork to more easily encode action-independent knowledge\nin the lower, convolutional layers. With merely the goal of\nmaximising its score on a video game, the DQN learns to\nextract salient visual features, jointly encoding objects, their\nmovements, and, most importantly, their interactions. Using\ntechniques originally developed for explaining the behaviour\nof CNNs in object recognition tasks, we can also inspect what\nparts of its view the agent considers important (see Figure 6).\nFig. 6. Saliency map of a trained DQN [84] playing “Space Invaders” [10].\nBy backpropagating the training signal to the image space, it is possible to\nsee what a neural-network-based agent is attending to. In this frame, the\nmost salient points—shown with the red overlay—are the laser that the agent\nrecently ﬁred, and also the enemy that it anticipates hitting in a few time\nsteps.\nThe true underlying state of the game is contained within\n128 bytes of Atari 2600 RAM. However, the DQN was\ndesigned to directly learn from visual inputs (210 × 160pixel\n8-bit RGB images), which it takes as the state s. It is\nimpractical to represent Qπ(s, a) exactly as a lookup table:\nWhen combined with 18 possible actions, we obtain a Q-\ntable of size |S| × |A| = 18 × 2563×210×160. Even if it were\nfeasible to create such a table, it would be sparsely populated,\nand information gained from one state-action pair cannot be\npropagated to other state-action pairs. The strength of the DQN\nlies in its ability to compactly represent both high-dimensional\nobservations and the Q-function using deep neural networks.\nWithout this ability, tackling the discrete Atari domain from\nraw visual inputs would be impractical.\nThe DQN addressed the fundamental instability problem\nof using function approximation in RL [145] by the use of\ntwo techniques: experience replay [80] and target networks.\nExperience replay memory stores transitions of the form\n(st, at, st+1, rt+1) in a cyclic buffer, enabling the RL agent\nto sample from and train on previously observed data ofﬂine.\nNot only does this massively reduce the amount of interactions\nneeded with the environment, but batches of experience can\nbe sampled, reducing the variance of learning updates. Fur-\nthermore, by sampling uniformly from a large memory, the\ntemporal correlations that can adversely affect RL algorithms\nare broken. Finally, from a practical perspective, batches\nof data can be efﬁciently processed in parallel by modern\nhardware, increasing throughput. Whilst the original DQN\nalgorithm used uniform sampling [84], later work showed\nthat prioritising samples based on TD errors is more effective\nfor learning [118]. We note that although experience replay\nis typically thought of as a model-free technique, it could\nactually be considered a simple model [150].\nThe second stabilising method, introduced by Mnih et al.\n[84], is the use of a target network that initially contains the\nweights of the network enacting the policy, but is kept frozen\nfor a large period of time. Rather than having to calculate the\nTD error based on its own rapidly ﬂuctuating estimates of the\nQ-values, the policy network uses the ﬁxed target network.\nDuring training, the weights of the target network are updated\nto match the policy network after a ﬁxed number of steps.\nBoth experience replay and target networks have gone on to\nbe used in subsequent DRL works [40, 79, 158, 89].\nB. Q-Function Modiﬁcations\nConsidering that one of the key components of the DQN is\na function approximator for the Q-function, it can beneﬁt from\nfundamental advances in RL. van Hasselt [148] showed that\nthe single estimator used in the Q-learning update rule over-\nestimates the expected return due to the use of the maximum\naction value as an approximation of the maximum expected\naction value. Double-Q learning provides a better estimate\nthrough the use of a double estimator [148]. Whilst double-\nQ learning requires an additional function to be learned, later\nwork proposed using the already available target network from\nthe DQN algorithm, resulting in signiﬁcantly better results\nwith only a small change in the update step [149]. A more\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n8\nradical proposal by Bellemare et al. [12] was to actually learn\nthe full value distribution, rather than just the expectation; this\nprovides additional information, such as whether the potential\nrewards come from a skewed or multimodal distribution. Al-\nthough the resulting algorithm—based on learning categorical\ndistributions—was used to construct the Categorical DQN, the\nbeneﬁts can potentially be applied to any RL algorithm that\nutilises learned value functions.\nYet another way to adjust the DQN architecture is to\ndecompose the Q-function into meaningful functions, such\nas constructing Qπ by adding together separate layers that\ncompute the state-value function V π and advantage function\nAπ [157]. Rather than having to come up with accurate Q-\nvalues for all actions, the duelling DQN [157] beneﬁts from a\nsingle baseline for the state in the form of V π, and easier-to-\nlearn relative values in the form of Aπ. The combination of the\nduelling DQN with prioritised experience replay [118] is one\nof the state-of-the-art techniques in discrete action settings.\nFurther insight into the properties of Aπ by Gu et al. [40]\nled them to modify the DQN with a convex advantage layer\nthat extended the algorithm to work over sets of continuous\nactions, creating the normalised advantage function (NAF)\nalgorithm. Beneﬁting from experience replay, target networks\nand advantage updates, NAF is one of several state-of-the-art\ntechniques in continuous control problems [40].\nSome RL domains, such as recommender systems, have\nvery large discrete action spaces, and hence may be difﬁcult to\ndirectly deal with. Dulac-Arnold et al. [30] proposed learning\n“action embeddings” over the large set of original actions,\nand then using k-nearest neighbors to produce “proto-actions”\nwhich can be used with traditional RL methods. The idea of\nusing representation learning to create distributed embeddings\nis a particular strength of DRL, and has been successfully\nutilised for other purposes [161, 100]. Another related scenario\nin RL is when many actions need to be made simultaneously,\nsuch as specifying the torques in a many-jointed robot, which\nresults in the action space growing exponentially. A naive but\nreasonable approach is to factorise the policy, treating each\naction independently [115]. An alternative is to construct an\nautoregressive policy, where each action in a single timestep\nis predicted conditionally on the state and previously chosen\nactions from the same timestep [106, 5, 168]. Metz et al.\n[81] used this idea in order to construct the sequential DQN,\nallowing them to discretise a large action space and outperform\nNAF—which is limited by its quadratic advantage function—\nin continous control problems. In a broader context, rather\nthan dealing directly with primitive actions directly, one may\nchoose to invoke “subpolicies” from higher-level policies\n[136]; this concept, known as hierarchical reinforcement learn-\ning (HRL), will be discussed later.\nV. POLICY SEARCH\nPolicy search methods aim to directly ﬁnd policies by means\nof gradient-free or gradient-based methods. Prior to the current\nsurge of interest in DRL, several successful methods in DRL\neschewed the commonly used backpropagation algorithm in\nfavour of evolutionary algorithms [37, 23, 64], which are\ngradient-free policy search algorithms. Evolutionary methods\nrely on evaluating the performance of a population of agents.\nHence, they are expensive for large populations or agents with\nmany parameters. However, as black-box optimisation meth-\nods they can be used to optimise arbitrary, non-differentiable\nmodels and naturally allow for more exploration in parameter\nspace. In combination with a compressed representation of\nneural network weights, evolutionary algorithms can even be\nused to train large networks; such a technique resulted in the\nﬁrst deep neural network to learn an RL task, straight from\nhigh-dimensional visual inputs [64]. Recent work has reignited\ninterest in evolutionary methods for RL as they can potentially\nbe distributed at larger scales than techniques that rely on\ngradients [116].\nA. Backpropagation through Stochastic Functions\nThe workhorse of DRL, however, remains backpropagation\n[162, 111]. The previously discussed REINFORCE rule [164]\nallows neural networks to learn stochastic policies in a task-\ndependent manner, such as deciding where to look in an\nimage to track [120], classify [83] or caption objects [166].\nIn these cases, the stochastic variable would determine the\ncoordinates of a small crop of the image, and hence reduce\nthe amount of computation needed. This usage of RL to make\ndiscrete, stochastic decisions over inputs is known in the deep\nlearning literature as hard attention, and is one of the more\ncompelling uses of basic policy search methods in recent years,\nhaving many applications outside of traditional RL domains.\nMore generally, the ability to backpropagate through stochastic\nfunctions, using techniques such as REINFORCE [164] or the\n“reparameterisation trick” [61, 108], allows neural networks\nto be treated as stochastic computation graphs that can be\noptimised over [121], which is a key concept in algorithms\nsuch as stochastic value gradients (SVGs) [46].\nB. Compounding Errors\nSearching directly for a policy represented by a neural\nnetwork with very many parameters can be difﬁcult and can\nsuffer from severe local minima. One way around this is to\nuse guided policy search (GPS), which takes a few sequences\nof actions from another controller (which could be constructed\nusing a separate method, such as optimal control). GPS learns\nfrom them by using supervised learning in combination with\nimportance sampling, which corrects for off-policy samples\n[73]. This approach effectively biases the search towards a\ngood (local) optimum. GPS works in a loop, by optimising\npolicies to match sampled trajectories, and optimising tra-\njectory distributions to match the policy and minimise costs.\nInitially, GPS was used to train neural networks on simulated\ncontinuous RL problems [72], but was later utilised to train\na policy for a real robot based on visual inputs [74]. This\nresearch by Levine et al. [74] showed that it was possible\nto train visuomotor policies for a robot “end-to-end”, straight\nfrom the RGB pixels of the camera to motor torques, and,\nhence, is one of the seminal works in DRL.\nA more commonly used method is to use a trust region, in\nwhich optimisation steps are restricted to lie within a region\nwhere the approximation of the true cost function still holds.\nBy preventing updated policies from deviating too wildly\nfrom previous policies, the chance of a catastrophically bad\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n9\nupdate is lessened, and many algorithms that use trust regions\nguarantee or practically result in monotonic improvement in\npolicy performance. The idea of constraining each policy\ngradient update, as measured by the Kullback-Leibler (KL)\ndivergence between the current and proposed policy, has a long\nhistory in RL [57, 4, 59, 103]. One of the newer algorithms in\nthis line of work, trust region policy optimisation (TRPO),\nhas been shown to be relatively robust and applicable to\ndomains with high-dimensional inputs [122]. To achieve this,\nTRPO optimises a surrogate objective function—speciﬁcally,\nit optimises an (importance sampled) advantage estimate, con-\nstrained using a quadratic approximation of the KL divergence.\nWhilst TRPO can be used as a pure policy gradient method\nwith a simple baseline, later work by Schulman et al. [123]\nintroduced generalised advantage estimation (GAE), which\nproposed several, more advanced variance reduction baselines.\nThe combination of TRPO and GAE remains one of the state-\nof-the-art RL techniques in continuous control. However, the\nconstrained optimisation of TRPO requires calculating second-\norder gradients, limiting its applicability. In contrast, the\nnewer proximal policy optimisation (PPO) algorithm performs\nunconstrained optimisation, requiring only ﬁrst-order gradient\ninformation [1, 47, 125]. The two main variants include an\nadaptive penalty on the KL divergence, and a heuristic clipped\nobjective which is independent of the KL divergence [125].\nBeing less expensive whilst retaining the performance of\nTRPO means that PPO (with or without GAE) is gaining\npopularity for a range of RL tasks [47, 125].\nC. Actor-Critic Methods\nInstead of utilising the average of several Monte Carlo\nreturns as the baseline for policy gradient methods, actor-\ncritic approaches have grown in popularity as an effective\nmeans of combining the beneﬁts of policy search methods\nwith learned value functions, which are able to learn from full\nreturns and/or TD errors. They can beneﬁt from improvements\nin both policy gradient methods, such as GAE [123], and value\nfunction methods, such as target networks [84]. In the last few\nyears, DRL actor-critic methods have been scaled up from\nlearning simulated physics tasks [46, 79] to real robotic visual\nnavigation tasks [167], directly from image pixels.\nOne recent development in the context of actor-critic algo-\nrithms are deterministic policy gradients (DPGs) [127], which\nextend the standard policy gradient theorems for stochastic\npolicies [164] to deterministic policies. One of the major\nadvantages of DPGs is that, whilst stochastic policy gradi-\nents integrate over both state and action spaces, DPGs only\nintegrate over the state space, requiring fewer samples in\nproblems with large action spaces. In the initial work on\nDPGs, Silver et al. [127] introduced and demonstrated an\noff-policy actor-critic algorithm that vastly improved upon\na stochastic policy gradient equivalent in high-dimensional\ncontinuous control problems. Later work introduced deep DPG\n(DDPG), which utilised neural networks to operate on high-\ndimensional, visual state spaces [79]. In the same vein as\nDPGs, Heess et al. [46] devised a method for calculating\ngradients to optimise stochastic policies, by “reparameterising”\n[61, 108] the stochasticity away from the network, thereby\nallowing standard gradients to be used (instead of the high-\nvariance REINFORCE estimator [164]). The resulting SVG\nmethods are ﬂexible, and can be used both with (SVG(0) and\nSVG(1)) and without (SVG(∞)) value function critics, and\nwith (SVG(∞) and SVG(1)) and without (SVG(0)) models.\nLater work proceeded to integrate DPGs and SVGs with\nRNNs, allowing them to solve continuous control problems\nin POMDPs, learning directly from pixels [45].\nValue functions introduce a broadly applicable beneﬁt in\nactor-critic methods—the ability to use off-policy data. On-\npolicy methods can be more stable, whilst off-policy methods\ncan be more data efﬁcient, and hence there have been several\nattempts to merge the two [158, 94, 41, 39, 42]. Earlier\nwork has either utilised a mix of on-policy and off-policy\ngradient updates [158, 94, 39], or used the off-policy data\nto train a value function in order to reduce the variance of\non-policy gradient updates [41]. The more recent work by\nGu et al. [42] uniﬁed these methods under interpolated policy\ngradients (IPGs), resulting in one of the newest state-of-the-\nart continuous DRL algorithms, and also providing insights for\nfuture research in this area. Together, the ideas behind IPGs\nand SVGs (of which DPGs can be considered a special case)\nform algorithmic approaches for improving learning efﬁciency\nin DRL.\nAn orthogonal approach to speeding up learning is to\nexploit parallel computation. In particular, methods for training\nnetworks through asynchronous gradient updates have been\ndeveloped for use on both single machines [107] and dis-\ntributed systems [25]. By keeping a canonical set of parameters\nthat are read by and updated in an asynchronous fashion\nby multiple copies of a single network, computation can be\nefﬁciently distributed over both processing cores in a single\nCPU, and across CPUs in a cluster of machines. Using a\ndistributed system, Nair et al. [91] developed a framework\nfor training multiple DQNs in parallel, achieving both better\nperformance and a reduction in training time. However, the\nsimpler asynchronous advantage actor-critic (A3C) algorithm\n[85], developed for both single and distributed machine set-\ntings, has become one of the most popular DRL techniques\nin recent times. A3C combines advantage updates with the\nactor-critic formulation, and relies on asynchronously updated\npolicy and value function networks trained in parallel over\nseveral processing threads. The use of multiple agents, situated\nin their own, independent environments, not only stabilises\nimprovements in the parameters, but conveys an additional\nbeneﬁt in allowing for more exploration to occur. A3C has\nbeen used as a standard starting point in many subsequent\nworks, including the work of Zhu et al. [167], who applied it\nto robotic navigation in the real world through visual inputs.\nFor simplicity, the underlying algorithm may be used with\njust one agent, termed advantage actor-critic (A2C) [156].\nAlternatively, segments from the trajectories of multiple agents\ncan be collected and processed together in a batch, with\nbatch processing more efﬁciently enabled by GPUs; this\nsynchronous version also goes by the name of A2C [125].\nThere have been several major advancements on the original\nA3C algorithm that reﬂect various motivations in the ﬁeld of\nDRL. The ﬁrst is actor-critic with experience replay [158, 39],\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n10\nwhich adds Retrace(λ) off-policy bias correction [88] to a\nQ-value-based A3C, allowing it to use experience replay in\norder to improve sample complexity. Others have attempted to\nbridge the gap between value and policy-based RL, utilising\ntheoretical advancements to improve upon the original A3C\n[89, 94, 124]. Finally, there is a growing trend towards ex-\nploiting auxiliary tasks to improve the representations learned\nby DRL agents, and, hence, improve both the learning speed\nand ﬁnal performance of these agents [77, 54, 82].\nVI. CURRENT RESEARCH AND CHALLENGES\nTo conclude, we will highlight some current areas of re-\nsearch in DRL, and the challenges that still remain. Previously,\nwe have focused mainly on model-free methods, but we will\nnow examine a few model-based DRL algorithms in more\ndetail. Model-based RL algorithms play an important role in\nmaking RL data-efﬁcient and in trading off exploration and\nexploitation. After tackling exploration strategies, we shall\nthen address HRL, which imposes an inductive bias on the\nﬁnal policy by explicitly factorising it into several levels. When\navailable, trajectories from other controllers can be used to\nbootstrap the learning process, leading us to imitation learning\nand inverse RL (IRL). For the ﬁnal topic speciﬁc to RL, we\nwill look at multi-agent systems, which have their own special\nconsiderations. We then bring to attention two broader areas—\nthe use of RNNs, and transfer learning—in the context of\nDRL. We then examine the issue of evaluating RL, and current\nbenchmarks for DRL.\nA. Model-based RL\nThe key idea behind model-based RL is to learn a tran-\nsition model that allows for simulation of the environment\nwithout interacting with the environment directly. Model-based\nRL does not assume speciﬁc prior knowledge. However, in\npractice, we can incorporate prior knowledge (e.g., physics-\nbased models [58]) to speed up learning. Model learning\nplays an important role in reducing the amount of required\ninteractions with the (real) environment, which may be limited\nin practice. For example, it is unrealistic to perform millions of\nexperiments with a robot in a reasonable amount of time and\nwithout signiﬁcant hardware wear and tear. There are various\napproaches to learn predictive models of dynamical systems\nusing pixel information. Based on the deep dynamical model\n[154], where high-dimensional observations are embedded\ninto a lower-dimensional space using autoencoders, several\nmodel-based DRL algorithms have been proposed for learning\nmodels and policies from pixel information [95, 160, 155]. If a\nsufﬁciently accurate model of the environment can be learned,\nthen even simple controllers can be used to control a robot\ndirectly from camera images [32]. Learned models can also\nbe used to guide exploration purely based on simulation of the\nenvironment, with deep models allowing these techniques to\nbe scaled up to high-dimensional visual domains [132].\nA compelling insight on the beneﬁts of neural-network-\nbased models is that they can overcome some of the problems\nincurred by planning with imperfect models; in effect, by\nembedding the activations and predictions (outputs) of these\nmodels into a vector, a DRL agent can not only obtain more\ninformation than just the ﬁnal result of any model rollouts, but\nit can also learn to downplay this information if it believes\nthat the model is inaccurate [161]. This can be more efﬁcient,\nthough less principled, than Bayesian methods for propagating\nuncertainty [52]. Another way to make use of the ﬂexiblity\nof neural-network-based models is to let them decide when to\nplan, that is, given a ﬁnite amount of computation, whether it is\nworth modelling one long trajectory, several short trajectories,\nanything in-between, or simply to take an action in the real\nenvironment [100].\nAlthough deep neural networks can make reasonable pre-\ndictions in simulated environments over hundreds of timesteps\n[21], they typically require many samples to tune the large\namount of parameters they contain. Training these models\noften requires more samples (interaction with the environment)\nthan simpler models. For this reason, Gu et al. [40] train\nlocally linear models for use with the NAF algorithm—\nthe continuous equivalent of the DQN [84]—to improve the\nalgorithm’s sample complexity in the robotic domain where\nsamples are expensive. In order to spur the adoption of deep\nmodels in model-based DRL, it is necessary to ﬁnd strategies\nthat can be used in order to improve their data efﬁciency [90].\nA less common but potentially useful paradigm exists\nbetween model-free and model-based methods—the successor\nrepresentation (SR) [24]. Rather than picking actions directly\nor performing planning with models, learning T is replaced\nwith learning expected (discounted) future occupancies (SRs),\nwhich can be linearly combined with R in order to calculate\nthe optimal action; this decomposition makes SRs more robust\nthan model-free methods when the reward structure changes\n(but still fallible when T changes). Work extending SRs to\ndeep neural networks has demonstrated its usefulness in multi-\ntask settings, whilst within a complex visual environment [66].\nB. Exploration vs. Exploitation\nOne of the greatest difﬁculties in RL is the fundamental\ndilemma of exploration versus exploitation: When should the\nagent try out (perceived) non-optimal actions in order to\nexplore the environment (and potentially improve the model),\nand when should it exploit the optimal action in order to make\nuseful progress? Off-policy algorithms, such as the DQN [84],\ntypically use the simple ϵ-greedy exploration policy, which\nchooses a random action with probability ϵ ∈[0, 1], and the\noptimal action otherwise. By decreasing ϵ over time, the agent\nprogresses towards exploitation. Although adding independent\nnoise for exploration is usable in continuous control problems,\nmore sophisticated strategies inject noise that is correlated\nover time (e.g., from stochastic processes) in order to better\npreserve momentum [79].\nThe observation that temporal correlation is important led\nOsband et al. [97] to propose the bootstrapped DQN, which\nmaintains several Q-value “heads” that learn different values\nthrough a combination of different weight initialisations and\nbootstrapped sampling from experience replay memory. At\nthe beginning of each training episode, a different head is\nchosen, leading to temporally-extended exploration. Usunier\net al. [147] later proposed a similar method that performed\nexploration in policy space by adding noise to a single output\nhead, using zero-order gradient estimates to allow backpropa-\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n11\ngation through the policy.\nOne of the main principled exploration strategies is the\nupper conﬁdence bound (UCB) algorithm, based on the prin-\nciple of “optimism in the face of uncertainty” [67]. The idea\nbehind UCB is to pick actions that maximise E[R] + κσ[R],\nwhere σ[R] is the standard deviation of the return and\nκ > 0. UCB therefore encourages exploration in regions with\nhigh uncertainty and moderate expected return. Whilst easily\nachievable in small tabular cases, the use of powerful density\nmodels [11], or conversely, hashing [139], has allowed this\nalgorithm to scale to high-dimensional visual domains with\nDRL. UCB is only one technique for trading off exploration\nand exploitation in the context of Bayesian optimisation [126];\nfuture work in DRL may beneﬁt from investigating other\nsuccessful techniques that are used in Bayesian optimisation.\nUCB can also be considered one way of implementing\nintrinsic motivation, which is a general concept that advocates\ndecreasing uncertainty/making progress in learning about the\nenvironment [119]. There have been several DRL algorithms\nthat try to implement intrinsic motivation via minimising\nmodel prediction error [132, 101] or maximising information\ngain [86, 52].\nC. Hierarchical RL\nIn the same way that deep learning relies on hierarchies\nof features, HRL relies on hierarchies of policies. Early work\nin this area introduced options, in which, apart from primi-\ntive actions (single-timestep actions), policies could also run\nother policies (multi-timestep “actions”) [136]. This approach\nallows top-level policies to focus on higher-level goals, whilst\nsubpolicies are responsible for ﬁne control. Several works in\nDRL have attempted HRL by using one top-level policy that\nchooses between subpolicies, where the division of states or\ngoals in to subpolicies is achieved either manually [2, 143, 65]\nor automatically [3, 151, 152]. One way to help construct\nsubpolicies is to focus on discovering and reaching goals,\nwhich are speciﬁc states in the environment; they may often be\nlocations, which an agent should navigate to. Whether utilised\nwith HRL or not, the discovery and generalisation of goals is\nalso an important area of ongoing research [117, 66, 152].\nD. Imitation Learning and Inverse RL\nOne may ask why, if given a sequence of “optimal” actions\nfrom expert demonstrations, it is not possible to use supervised\nlearning in a straightforward manner—a case of “learning\nfrom demonstration”. This is indeed possible, and is known\nas behavioural cloning in traditional RL literature. Taking\nadvantage of the stronger signals available in supervised learn-\ning problems, behavioural cloning enjoyed success in earlier\nneural network research, with the most notable success being\nALVINN, one of the earliest autonomous cars [104]. However,\nbehavioural cloning cannot adapt to new situations, and small\ndeviations from the demonstration during the execution of the\nlearned policy can compound and lead to scenarios where the\npolicy is unable to recover. A more generalisable solution is\nto use provided trajectories to guide the learning of suitable\nstate-action pairs, but ﬁne-tune the agent using RL [49].\nAlternatively, if the expert is still available to query during\ntraining, the agent can use active learning to gather extra data\nwhen it is unsure, allowing it to learn from states away from\nthe optimal trajectories [110]. This has been applied to a deep\nlearning setting, where a CNN trained in a visual navigation\ntask with active learning signiﬁcantly improved upon a pure\nimitation learning baseline [53].\nThe goal of IRL is to estimate an unknown reward function\nfrom observed trajectories that characterise a desired solution\n[92]; IRL can be used in combination with RL to improve\nupon demonstrated behaviour. Using the power of deep neural\nnetworks, it is now possible to learn complex, nonlinear reward\nfunctions for IRL [165]. Ho and Ermon [51] showed that poli-\ncies are uniquely characterised by their occupancies (visited\nstate and action distributions) allowing IRL to be reduced to\nthe problem of measure matching. With this insight, they were\nable to use generative adversarial training [38] to facilitate\nreward function learning in a more ﬂexible manner, resulting in\nthe generative adversarial imitation learning (GAIL) algorithm.\nGAIL was later extended to allow IRL to be applied even when\nreceiving expert trajectories from a different visual viewpoint\nto that of the RL agent [131]. In complementary work, Baram\net al. [7] exploit gradient information that was not used in\nGAIL to learn models within the IRL process.\nE. Multi-agent RL\nUsually, RL considers a single learning agent in a sta-\ntionary environment. In contrast, multi-agent RL (MARL)\nconsiders multiple agents learning through RL, and often the\nnon-stationarity introduced by other agents changing their\nbehaviours as they learn [18]. In DRL, the focus has been\non enabling (differentiable) communication between agents,\nwhich allows them to co-operate. Several approaches have\nbeen proposed for this purpose, including passing messages\nto agents sequentially [33], using a bidirectional channel\n(providing ordering with less signal loss) [102], and an all-\nto-all channel [134]. The addition of communication channels\nis a natural strategy to apply to MARL in complex scenarios\nand does not preclude the usual practice of modelling co-\noperative or competing agents as applied elsewhere in the\nMARL literature [18]. Other DRL works of note in MARL\ninvestigate the effects of learning and sequential decision\nmaking in game theory [48, 71].\nF. Memory and Attention\nAs one of the earliest works in DRL the DQN spawned\nmany extensions. One of the ﬁrst extensions was converting\nthe DQN into an RNN, which allows the network to better\ndeal with POMDPs by integrating information over long time\nperiods. Like recursive ﬁlters, recurrent connections provide an\nefﬁcient means of acting conditionally on temporally distant\nprior observations. By using recurrent connections between\nits hidden units, the deep recurrent Q-network (DRQN) in-\ntroduced by Hausknecht and Stone [44] was able to suc-\ncessfully infer the velocity of the ball in the game “Pong,”\neven when frames of the game were randomly blanked out.\nFurther improvements were gained by introducing attention—\na technique where additional connections are added from the\nrecurrent units to lower layers—to the DRQN, resulting in the\ndeep attention recurrent Q-network (DARQN) [130]. Attention\ngives a network the ability to choose which part of its next\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n12\ninput to focus on, and allowed the DARQN to beat both\nthe DQN and DRQN on games, which require longer-term\nplanning. However, the DQN outperformed the DRQN and\nDARQN on games requiring quick reactions, where Q-values\ncan ﬂuctuate more rapidly.\nTaking recurrent processing further, it is possible to add a\ndifferentiable memory to the DQN, which allows it to more\nﬂexibly process information in its “working memory” [96]. In\ntraditional RNNs, recurrent units are responsible for both per-\nforming calculations and storing information. Differentiable\nmemories add large matrices that are purely used for storing\ninformation, and can be accessed using differentiable read\nand write operations, analagously to computer memory. With\ntheir key-value-based memory Q-network (MQN), Oh et al.\n[96] constructed an agent that could solve a simple maze\nbuilt in Minecraft, where the correct goal in each episode\nwas indicated by a coloured block shown near the start of\nthe maze. The MQN, and especially its more sophisticated\nvariants, signiﬁcantly outperformed both DQN and DRQN\nbaselines, highlighting the importance of using decoupled\nmemory storage. More recent work, where the memory was\ngiven a 2D structure in order to resemble a spatial map, hints\nat future research where more specialised memory structures\nwill be developed to address speciﬁc problems, such as 2D or\n3D navigation [98]. Alternatively, differentiable memories can\nbe used as approximate hash tables, allowing DRL algorithms\nto store and retrieve successful experiences to facilitate rapid\nlearning [105].\nNote that RNNs are not restricted to value-function-based\nmethods but have also been successfully applied to policy\nsearch [163] and actor-critic methods [45, 85].\nG. Transfer Learning\nEven though DRL algorithms can process high-dimensional\ninputs, it is rarely feasible to train RL agents directly on\nvisual inputs in the real world, due to the large number of\nsamples required. To speed up learning in DRL, it is possible\nto exploit previously acquired knowledge from related tasks,\nwhich comes in several guises: transfer learning, multitask\nlearning [20] and curriculum learning [14] to name a few.\nThere is much interest in transferring learning from one task to\nanother, particularly from training in physics simulators with\nvisual renderers and ﬁne-tuning the models in the real world.\nThis can be achieved in a naive fashion, directly using the\nsame network in both the simulated and real phases [167], or\nwith more sophisticated training procedures that directly try\nto mitigate the problem of neural networks “catastrophically\nforgetting” old knowledge by adding extra layers when trans-\nferring domain [114, 115]. Other approaches involve directly\nlearning an alignment between simulated and real visuals\n[146], or even between two different camera viewpoints [131].\nA different form of transfer can be utilised to help RL in\nthe form of multitask training [77, 54, 82]. Especially with\nneural networks, supervised and unsupervised learning tasks\ncan help train features that can be used by RL agents, making\noptimising the RL objective easier to achieve. For example,\nthe “unsupervised reinforcement and auxiliary learning” A3C-\nbased agent is additionally trained with “pixel control” (maxi-\nmally changing pixel inputs), plus reward prediction and value\nfunction learning from experience replay [54]. Meanwhile, the\nA3C-based agent of Mirowski et al. [82] was additionally\ntrained to construct a depth map given RGB inputs, which\nhelps it in its task of learning to navigate a 3D environment.\nIn an ablation study, Mirowski et al. [82] showed the predicting\ndepth was more useful than receiving depth as an extra input,\nlending further support to the idea that gradients induced by\nauxiliary tasks can be extremely effective at boosting DRL.\nTransfer learning can also be used to construct more\ndata- and parameter-efﬁcient policies. In the student-teacher\nparadigm in machine learning, one can ﬁrst train a more\npowerful “teacher” model, and then use it to guide the training\nof a less powerful “student” model. Whilst originally applied\nto supervised learning, the neural network knowledge transfer\ntechnique known as distillation [50] has been utilised to both\ntransfer policies learned by large DQNs to smaller DQNs, and\ntransfer policies learned by several DQNs trained on separate\ngames to one single DQN [99, 113]. Together, the combination\nof multitask and transfer learning can improve the sample\nefﬁciency and robustness of current DRL algorithms [140].\nThese are important topics if we wish to construct agents that\ncan accomplish a wide range of tasks, since naively training\non multiple RL objectives at once may be infeasible.\nH. Benchmarks\nOne of the challenges in any ﬁeld in machine learning is\ndeveloping a standardised way to evaluate new techniques.\nAlthough much early work focused on simple, custom MDPs,\nthere shortly emerged control problems that could be used as\nstandard benchmarks for testing new algorithms, such as the\nCartpole [8] and Mountain Car [87] domains.\nHowever, these problems were limited to relatively small\nstate spaces, and therefore failed to capture the complexities\nthat would be encountered in most realistic scenarios. Ar-\nguably the initial driver of DRL, the ALE provided an interface\nto Atari 2600 video games, with code to access over 50 games\nprovided with the initial release [10]. As video games can vary\ngreatly, but still present interesting and challenging objectives\nfor humans, they provide an excellent testbed for RL agents.\nAs the ﬁrst algorithm to successfully play a range of these\ngames directly from their visuals, the DQN [84] has secured\nits place as a milestone in the development of RL algorithms.\nThis success story has started a trend of using video games\nas standardised RL testbeds, with several interesting options\nnow available. ViZDoom provides an interface to the Doom\nﬁrst-person shooter [60], and echoing the popularity of e-\nsports competitions, ViZDoom competitions are now held at\nthe yearly IEEE Conference on Computational Intelligence\nin Games. Facebook’s TorchCraft [137] and DeepMind’s\nStarCraft II Learning Environment [153] respectively provide\ninterfaces to the StarCraft and StarCraft II real-time strategy\ngames, presenting challenges in both micromanagement and\nlong-term planning. In an aim to provide more ﬂexible envi-\nronments, DeepMind Lab was developed on top of the Quake\nIII Arena ﬁrst-person shooter engine [9], and Microsoft’s\nProject Malmo exposed an interface to the Minecraft sandbox\ngame [55]. Both environments provide customisable platforms\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n13\nfor RL agents in 3D environments.\nMost DRL approaches focus on discrete actions, but some\nsolutions have also been developed for continuous control\nproblems. Many DRL papers in continuous control [122, 46,\n79, 85, 7, 131] have used the MuJoCo physics engine to\nobtain relatively realistic dynamics for multi-joint continuous\ncontrol problems [144], and there has now been some effort\nto standardise these problems [28].\nTo help with standardisation and reproducibility, most of\nthe aforementioned RL domains and more have been made\navailable in the OpenAI Gym, a library and online service\nthat allows people to easily interface with and publicly share\nthe results of RL algorithms on these domains [17].\nVII. CONCLUSION: BEYOND PATTERN RECOGNITION\nDespite the successes of DRL, many problems need to be\naddressed before these techniques can be applied to a wide\nrange of complex real-world problems [68]. Recent work with\n(non-deep) generative causal models demonstrated superior\ngeneralisation over standard DRL algorithms [85, 114] in\nsome benchmarks [10], achieved by reasoning about causes\nand effects in the environment [58]. For example, the schema\nnetworks of Kanksy et al. [58] trained on the game “Breakout”\nimmediately adapted to a variant where a small wall was\nplaced in front of the target blocks, whilst progressive (A3C)\nnetworks [114] failed to match the performance of the schema\nnetworks even after training on the new domain. Although\nDRL has already been combined with AI techniques, such as\nsearch [128] and planning [138], a deeper integration with\nother traditional AI approaches promises beneﬁts such as\nbetter sample complexity, generalisation and interpretability\n[35]. In time, we also hope that our theoretical understanding\nof the properties of neural networks (particularly within DRL)\nwill improve, as it currently lags far behind practice.\nTo conclude, it is worth revisiting the overarching goal\nof all of this research: the creation of general-purpose AI\nsystems that can interact with and learn from the world around\nthem. Interaction with the environment is simultaneously the\nadvantage and disadvantage of RL. Whilst there are many\nchallenges in seeking to understand our complex and ever-\nchanging world, RL allows us to choose how we explore\nit. In effect, RL endows agents with the ability to perform\nexperiments to better understand their surroundings, enabling\nthem to learn even high-level causal relationships. The avail-\nability of high-quality visual renderers and physics engines\nnow enables us to take steps in this direction, with works that\ntry to learn intuitive models of physics in visual environments\n[27]. Challenges remain before this will be possible in the real\nworld, but steady progress is being made in agents that learn\nthe fundamental principles of the world through observation\nand action. Perhaps, then, we are not too far away from\nAI systems that learn and act in more human-like ways in\nincreasingly complex environments.\nACKNOWLEDGMENTS\nThe authors would like to thank the reviewers and broader\ncommunity for their feedback on this survey; in particular,\nwe would like to thank Nicolas Heess for clariﬁcations on\nseveral points. Kai Arulkumaran would like to acknowledge\nPhD funding from the Department of Bioengineering, Imperial\nCollege London. This research has been partially funded by a\nGoogle Faculty Research Award to Marc Deisenroth.\nREFERENCES\n[1] Pieter Abbeel and John Schulman.\nDeep Reinforcement Learning\nthrough Policy Optimization, 2016. Tutorial at NIPS 2016.\n[2] Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, and Anil An-\nthony Bharath. Classifying Options for Deep Reinforcement Learning.\nIn IJCAI Workshop on Deep Reinforcement Learning: Frontiers and\nChallenges, 2016.\n[3] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The Option-Critic\nArchitecture. In AAAI, 2017.\n[4] J Andrew Bagnell and Jeff Schneider. Covariant Policy Search. In\nIJCAI, 2003.\n[5] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan\nLowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An Actor-\nCritic Algorithm for Sequence Prediction. In ICLR, 2017.\n[6] Leemon C Baird III. Advantage Updating. Technical report, DTIC,\n1993.\n[7] Nir Baram, Oron Anschel, and Shie Mannor.\nModel-Based Adver-\nsarial Imitation Learning. In NIPS Workshop on Deep Reinforcement\nLearning, 2016.\n[8] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neu-\nronlike Adaptive Elements That Can Solve Difﬁcult Learning Control\nProblems. IEEE Trans. on Systems, Man, and Cybernetics, (5):834–\n846, 1983.\n[9] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus\nWainwright, Heinrich K¨uttler, Andrew Lefrancq, Simon Green, V´ıctor\nVald´es, Amir Sadik, et al. DeepMind Lab. arXiv:1612.03801, 2016.\n[10] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowl-\ning. The Arcade Learning Environment: An Evaluation Platform for\nGeneral Agents. In IJCAI, 2015.\n[11] Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul,\nDavid Saxton, and R´emi Munos. Unifying Count-Based Exploration\nand Intrinsic Motivation. In NIPS, 2016.\n[12] Marc G Bellemare, Will Dabney, and R´emi Munos. A Distributional\nPerspective on Reinforcement Learning. In ICML, 2017.\n[13] Richard Bellman. On the Theory of Dynamic Programming. PNAS,\n38(8):716–719, 1952.\n[14] Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason We-\nston. Curriculum Learning. In ICML, 2009.\n[15] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation\nLearning: A Review and New Perspectives. IEEE Trans. on Pattern\nAnalysis and Machine Intelligence, 35(8):1798–1828, 2013.\n[16] Dimitri P Bertsekas. Dynamic Programming and Suboptimal Control:\nA Survey from ADP to MPC. European Journal of Control, 11(4-5):\n310–334, 2005.\n[17] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba.\nOpenAI Gym.\narXiv:1606.01540, 2016.\n[18] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A Compre-\nhensive survey of Multiagent Reinforcement Learning. IEEE Trans. on\nSystems, Man, And Cybernetics, 2008.\n[19] Murray Campbell, A Joseph Hoane, and Feng-hsiung Hsu. Deep Blue.\nArtiﬁcial Intelligence, 134(1-2):57–83, 2002.\n[20] Rich Caruana. Multitask Learning. Machine Learning, 28(1):41–75,\n1997.\n[21] Silvia Chiappa, S´ebastien Racaniere, Daan Wierstra, and Shakir Mo-\nhamed. Recurrent Environment Simulators. In ICLR, 2017.\n[22] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor\nBlackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba. Trans-\nfer from Simulation to Real World through Learning Deep Inverse\nDynamics Model. arXiv:1610.03518, 2016.\n[23] Giuseppe Cuccu, Matthew Luciw, J¨urgen Schmidhuber, and Faustino\nGomez.\nIntrinsically Motivated Neuroevolution for Vision-Based\nReinforcement Learning. In ICDL, volume 2, 2011.\n[24] Peter Dayan.\nImproving Generalization for Temporal Difference\nLearning: The Successor Representation. Neural Computation, 5(4):\n613–624, 1993.\n[25] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,\nMark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al.\nLarge Scale Distributed Deep Networks. In NIPS, 2012.\n[26] Marc P Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on\nPolicy Search for Robotics. Foundations and Trends R⃝in Robotics, 2\n(1–2), 2013.\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n14\n[27] Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter\nBattaglia, and Nando de Freitas.\nLearning to Perform Physics Ex-\nperiments via Deep Reinforcement Learning. In ICLR, 2017.\n[28] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter\nAbbeel. Benchmarking Deep Reinforcement Learning for Continuous\nControl. In ICML, 2016.\n[29] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever,\nand Pieter Abbeel.\nRL2: Fast Reinforcement Learning via Slow\nReinforcement Learning. In NIPS Workshop on Deep Reinforcement\nLearning, 2016.\n[30] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sune-\nhag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane We-\nber, Thomas Degris, and Ben Coppin. Deep Reinforcement Learning\nin Large Discrete Action Spaces. arXiv:1512.07679, 2015.\n[31] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David\nGondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric\nNyberg, John Prager, et al.\nBuilding Watson: An Overview of the\nDeepQA Project. AI Magazine, 31(3):59–79, 2010.\n[32] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine,\nand Pieter Abbeel. Deep Spatial Autoencoders for Visuomotor Learn-\ning. In ICRA, 2016.\n[33] Jakob Foerster, Yannis M Assael, Nando de Freitas, and Shimon White-\nson. Learning to Communicate with Deep Multi-Agent Reinforcement\nLearning. In NIPS, 2016.\n[34] Michael C Fu.\nGradient Estimation.\nHandbooks in Operations\nResearch and Management Science, 13:575–616, 2006.\n[35] Marta Garnelo, Kai Arulkumaran, and Murray Shanahan.\nTowards\nDeep Symbolic Reinforcement Learning. In NIPS Workshop on Deep\nReinforcement Learning, 2016.\n[36] Peter W Glynn. Likelihood Ratio Gradient Estimation for Stochastic\nSystems. Communications of the ACM, 33(10):75–84, 1990.\n[37] Faustino Gomez and J¨urgen Schmidhuber.\nEvolving Modular Fast-\nWeight Networks for Control. In ICANN, 2005.\n[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative Adversarial Nets. In NIPS, 2014.\n[39] Audrunas Gruslys, Mohammad Gheshlaghi Azar, Marc G Bellemare,\nand R´emi Munos.\nThe Reactor: A Sample-Efﬁcient Actor-Critic\nArchitecture. arXiv:1704.04651, 2017.\n[40] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine.\nContinuous Deep Q-Learning with Model-Based Acceleration.\nIn\nICLR, 2016.\n[41] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E\nTurner, and Sergey Levine. Q-Prop: Sample-Efﬁcient Policy Gradient\nwith an Off-Policy Critic. In ICLR, 2017.\n[42] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E\nTurner, Bernhard Sch¨olkopf, and Sergey Levine. Interpolated Policy\nGradient: Merging On-Policy and Off-Policy Gradient Estimation for\nDeep Reinforcement Learning. In NIPS, 2017.\n[43] Mance E Harmon and Leemon C Baird III.\nMulti-Player Residual\nAdvantage Learning with General Function Approximation. Technical\nreport, DTIC, 1996.\n[44] Matthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for\nPartially Observable MDPs. In AAAI Fall Symposium Series, 2015.\n[45] Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, and David Silver.\nMemory-Based Control with Recurrent Neural Networks.\nIn NIPS\nWorkshop on Deep Reinforcement Learning, 2015.\n[46] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez,\nand Yuval Tassa. Learning Continuous Control Policies by Stochastic\nValue Gradients. In NIPS, 2015.\n[47] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg\nWayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin\nRiedmiller, et al.\nEmergence of Locomotion Behaviours in Rich\nEnvironments. arXiv:1707.02286, 2017.\n[48] Johannes Heinrich and David Silver. Deep Reinforcement Learning\nfrom Self-Play in Imperfect-Information Games. 2016.\n[49] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom\nSchaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian\nOsband, John Agapiou, et al. Learning from Demonstrations for Real\nWorld Reinforcement Learning. arXiv:1704.03732, 2017.\n[50] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowl-\nedge in a Neural Network. 2014.\n[51] Jonathan Ho and Stefano Ermon.\nGenerative Adversarial Imitation\nLearning. In NIPS, 2016.\n[52] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck,\nand Pieter Abbeel. VIME: Variational Information Maximizing Explo-\nration. In NIPS, 2016.\n[53] Ahmed Hussein, Mohamed Medhat Gaber, and Eyad Elyan.\nDeep\nActive Learning for Autonomous Navigation. In EANN, 2016.\n[54] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom\nSchaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu.\nRe-\ninforcement Learning with Unsupervised Auxiliary Tasks. In ICLR,\n2017.\n[55] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell.\nThe Malmo Platform for Artiﬁcial Intelligence Experimentation. In\nIJCAI, 2016.\n[56] Leslie P Kaelbling, Michael L Littman, and Anthony R Cassandra.\nPlanning and Acting in Partially Observable Stochastic Domains.\nArtiﬁcial Intelligence, 101(1):99–134, 1998.\n[57] Sham M Kakade. A Natural Policy Gradient. In NIPS, 2002.\n[58] Ken Kansky, Tom Silver, David A M´ely, Mohamed Eldawy, Miguel\nL´azaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott\nPhoenix, and Dileep George. Schema Networks: Zero-Shot Transfer\nwith a Generative Causal Model of Intuitive Physics. In ICML, 2017.\n[59] Hilbert J Kappen. Path Integrals and Symmetry Breaking for Optimal\nControl Theory. JSTAT, 2005(11):P11011, 2005.\n[60] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and\nWojciech Ja´skowski. ViZDoom: A Doom-Based AI Research Platform\nfor Visual Reinforcement Learning. In CIG, 2016.\n[61] Diederik P Kingma and Max Welling.\nAuto-Encoding Variational\nBayes. In ICLR, 2014.\n[62] Nate Kohl and Peter Stone. Policy Gradient Reinforcement Learning\nfor Fast Quadrupedal Locomotion. In ICRA, volume 3, 2004.\n[63] Vijay R Konda and John N Tsitsiklis. On Actor-Critic Algorithms.\nSICON, 42(4):1143–1166, 2003.\n[64] Jan Koutn´ık, Giuseppe Cuccu, J¨urgen Schmidhuber, and Faustino\nGomez.\nEvolving Large-Scale Neural Networks for Vision-Based\nReinforcement Learning. In GECCO, 2013.\n[65] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh\nTenenbaum. Hierarchical Deep Reinforcement Learning: Integrating\nTemporal Abstraction and Intrinsic Motivation. In NIPS, 2016.\n[66] Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Ger-\nshman. Deep Successor Reinforcement Learning. In NIPS Workshop\non Deep Reinforcement Learning, 2016.\n[67] Tze Leung Lai and Herbert Robbins. Asymptotically Efﬁcient Adaptive\nAllocation Rules. Advances in Applied Mathematics, 6(1):4–22, 1985.\n[68] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and\nSamuel J Gershman. Building Machines That Learn and Think Like\nPeople. The Behavioral and Brain Sciences, page 1, 2016.\n[69] Sascha Lange, Martin Riedmiller, and Arne Voigtlander. Autonomous\nReinforcement Learning on Raw Visual Input Data in a Real World\nApplication. In IJCNN, 2012.\n[70] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep Learning.\nNature, 521(7553):436–444, 2015.\n[71] Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and\nThore Graepel.\nMulti-Agent Reinforcement Learning in Sequential\nSocial Dilemmas. In AAMAS, 2017.\n[72] Sergey Levine and Pieter Abbeel. Learning Neural Network Policies\nwith Guided Policy Search under Unknown Dynamics. In NIPS, 2014.\n[73] Sergey Levine and Vladlen Koltun. Guided Policy Search. In ICLR,\n2013.\n[74] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-\nto-End Training of Deep Visuomotor Policies.\nJMLR, 17(39):1–40,\n2016.\n[75] Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen.\nLearning Hand-Eye Coordination for Robotic Grasping with Deep\nLearning and Large-Scale Data Collection. In ISER, 2016.\n[76] Ke Li and Jitendra Malik. Learning to Optimize. 2017.\n[77] Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen,\nLi Deng, and Ji He.\nRecurrent Reinforcement Learning: A Hybrid\nApproach. arXiv:1509.03044, 2015.\n[78] Yuxi\nLi.\nDeep\nReinforcement\nLearning:\nAn\nOverview.\narXiv:1701.07274, 2017.\n[79] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,\nTom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous\nControl with Deep Reinforcement Learning. In ICLR, 2016.\n[80] Long-Ji Lin. Self-Improving Reactive Agents Based on Reinforcement\nLearning, Planning and Teaching. Machine Learning, 8(3–4):293–321,\n1992.\n[81] Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Dis-\ncrete Sequential Prediction of Continuous Actions for Deep RL.\narXiv:1705.05035, 2017.\n[82] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Bal-\nlard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n15\nKavukcuoglu, et al. Learning to Navigate in Complex Environments.\nIn ICLR, 2017.\n[83] Volodymyr\nMnih,\nNicolas\nHeess,\nAlex\nGraves,\nand\nKoray\nKavukcuoglu.\nRecurrent Models of Visual Attention.\nIn NIPS,\n2014.\n[84] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,\nJoel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller,\nAndreas K Fidjeland, Georg Ostrovski, et al. Human-Level Control\nthrough Deep Reinforcement Learning. Nature, 518(7540):529–533,\n2015.\n[85] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex\nGraves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. Asynchronous Methods for Deep Reinforcement Learn-\ning. In ICLR, 2016.\n[86] Shakir Mohamed and Danilo Jimenez Rezende. Variational Information\nMaximisation for Intrinsically Motivated Reinforcement Learning. In\nNIPS, 2015.\n[87] Andrew William Moore. Efﬁcient Memory-Based Learning for Robot\nControl. Technical report, University of Cambridge, Computer Labo-\nratory, 1990.\n[88] R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G Belle-\nmare. Safe and Efﬁcient Off-Policy Reinforcement Learning. In NIPS,\n2016.\n[89] Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.\nBridging the Gap Between Value and Policy Based Reinforcement\nLearning. arXiv:1702.08892, 2017.\n[90] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey\nLevine. Neural Network Dynamics for Model-Based Deep Reinforce-\nment Learning with Model-Free Fine-Tuning. arXiv:1708.02596, 2017.\n[91] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory\nFearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa\nSuleyman, Charles Beattie, Stig Petersen, et al.\nMassively Parallel\nMethods for Deep Reinforcement Learning. In ICML Workshop on\nDeep Learning, 2015.\n[92] Andrew Y Ng and Stuart J Russell. Algorithms for Inverse Reinforce-\nment Learning. In ICML, 2000.\n[93] Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie\nSchulte, Ben Tse, Eric Berger, and Eric Liang. Autonomous Inverted\nHelicopter Flight via Reinforcement Learning. Experimental Robotics,\npages 363–372, 2006.\n[94] Brendan\nO’Donoghue,\nR´emi\nMunos,\nKoray\nKavukcuoglu,\nand\nVolodymyr Mnih. PGQ: Combining Policy Gradient and Q-Learning.\nIn ICLR, 2017.\n[95] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and\nSatinder Singh.\nAction-Conditional Video Prediction using Deep\nNetworks in Atari Games. In NIPS, 2015.\n[96] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak\nLee. Control of Memory, Active Perception, and Action in Minecraft.\nIn ICLR, 2016.\n[97] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin\nVan Roy. Deep Exploration via Bootstrapped DQN. In NIPS, 2016.\n[98] Emilio Parisotto and Ruslan Salakhutdinov. Neural Map: Structured\nMemory for Deep Reinforcement Learning. arXiv:1702.08360, 2017.\n[99] Emilio Parisotto, Jimmy L Ba, and Ruslan Salakhutdinov.\nActor-\nMimic: Deep Multitask and Transfer Reinforcement Learning. In ICLR,\n2016.\n[100] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing,\nSebastien Racani`ere, David Reichert, Th´eophane Weber, Daan Wier-\nstra, and Peter Battaglia. Learning Model-Based Planning from Scratch.\narXiv:1707.06170, 2017.\n[101] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell.\nCuriosity-Driven Exploration by Self-supervised Prediction. In ICML,\n2017.\n[102] Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang,\nHaitao Long, and Jun Wang. Multiagent Bidirectionally-Coordinated\nNets: Emergence of Human-level Coordination in Learning to Play\nStarCraft Combat Games. arXiv:1703.10069, 2017.\n[103] Jan Peters, Katharina M¨ulling, and Yasemin Altun. Relative Entropy\nPolicy Search. In AAAI, 2010.\n[104] Dean A Pomerleau.\nALVINN, an Autonomous Land Vehicle in\na Neural Network.\nTechnical report, Carnegie Mellon University,\nComputer Science Department, 1989.\n[105] Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adri`a Puig-\ndom`enech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles\nBlundell. Neural Episodic Control. In ICML, 2017.\n[106] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech\nZaremba. Sequence Level Training with Recurrent Neural Networks.\nIn ICLR, 2016.\n[107] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.\nHogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient\nDescent. In NIPS, 2011.\n[108] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.\nStochastic Backpropagation and Approximate Inference in Deep Gen-\nerative Models. In ICML, 2014.\n[109] Martin Riedmiller. Neural Fitted Q Iteration—First Experiences with\na Data Efﬁcient Neural Reinforcement Learning Method. In ECML,\n2005.\n[110] St´ephane Ross, Geoffrey J Gordon, and Drew Bagnell. A Reduction\nof Imitation Learning and Structured Prediction to No-Regret Online\nLearning. In AISTATS, 2011.\n[111] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learn-\ning Representations by Back-Propagating Errors. Cognitive Modeling,\n5(3):1, 1988.\n[112] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using\nConnectionist Systems.\nUniversity of Cambridge, Department of\nEngineering, 1994.\n[113] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guil-\nlaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr\nMnih, Koray Kavukcuoglu, and Raia Hadsell. Policy Distillation. In\nICLR, 2016.\n[114] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert\nSoyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and\nRaia Hadsell. Progressive Neural Networks. arXiv:1606.04671, 2016.\n[115] Andrei A Rusu, Matej Vecerik, Thomas Roth¨orl, Nicolas Heess, Razvan\nPascanu, and Raia Hadsell. Sim-to-Real Robot Learning from Pixels\nwith Progressive Nets. In CoRL, 2017.\n[116] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever.\nEvolu-\ntion Strategies as a Scalable Alternative to Reinforcement Learning.\narXiv:1703.03864, 2017.\n[117] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal\nValue Function Approximators. In ICML, 2015.\n[118] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Pri-\noritized Experience Replay. In ICLR, 2016.\n[119] J¨urgen Schmidhuber.\nA Possibility for Implementing Curiosity and\nBoredom in Model-Building Neural Controllers. In SAB, 1991.\n[120] J¨urgen Schmidhuber and Rudolf Huber. Learning to Generate Artiﬁcial\nFovea Trajectories for Target Detection.\nIJNS, 2(01n02):125–134,\n1991.\n[121] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel.\nGradient Estimation using Stochastic Computation Graphs. In NIPS,\n2015.\n[122] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and\nPhilipp Moritz. Trust Region Policy Optimization. In ICML, 2015.\n[123] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and\nPieter Abbeel. High-Dimensional Continuous Control using General-\nized Advantage Estimation. In ICLR, 2016.\n[124] John Schulman, Pieter Abbeel, and Xi Chen. Equivalence Between\nPolicy Gradients and Soft Q-Learning. arXiv:1704.06440, 2017.\n[125] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\nand Oleg Klimov.\nProximal Policy Optimization Algorithms.\narXiv:1707.06347, 2017.\n[126] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and\nNando de Freitas.\nTaking the Human out of the Loop: A Review\nof Bayesian Optimization. Proc. of the IEEE, 104(1):148–175, 2016.\n[127] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wier-\nstra, and Martin Riedmiller. Deterministic Policy Gradient Algorithms.\nIn ICML, 2014.\n[128] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Lau-\nrent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis\nAntonoglou, Veda Panneershelvam, Marc Lanctot, et al.\nMastering\nthe Game of Go with Deep Neural Networks and Tree Search. Nature,\n529(7587):484–489, 2016.\n[129] Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker.\nOptimizing Dialogue Management with Reinforcement Learning: Ex-\nperiments with the NJFun System. JAIR, 16:105–133, 2002.\n[130] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov,\nand Anastasiia Ignateva.\nDeep Attention Recurrent Q-Network.\nIn\nNIPS Workshop on Deep Reinforcement Learning, 2015.\n[131] Bradley C Stadie, Pieter Abbeel, and Ilya Sutskever.\nThird Person\nImitation Learning. In ICLR, 2017.\n[132] Bradly C Stadie, Sergey Levine, and Pieter Abbeel.\nIncentivizing\nExploration in Reinforcement Learning with Deep Predictive Models.\nIn NIPS Workshop on Deep Reinforcement Learning, 2015.\n[133] Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION)\n16\nMichael L Littman.\nPAC Model-Free Reinforcement Learning.\nIn\nICML, 2006.\n[134] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.\nLearning\nMultiagent Communication with Backpropagation. In NIPS, 2016.\n[135] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An\nIntroduction. MIT Press, 1998.\n[136] Richard S Sutton, Doina Precup, and Satinder Singh.\nBetween\nMDPs and Semi-MDPs: A Framework for Temporal Abstraction in\nReinforcement Learning.\nArtiﬁcial Intelligence, 112(1–2):181–211,\n1999.\n[137] Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala,\nTimoth´ee Lacroix, Zeming Lin, Florian Richoux, and Nicolas Usunier.\nTorchCraft: A Library for Machine Learning Research on Real-Time\nStrategy Games. arXiv:1611.00625, 2016.\n[138] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel.\nValue Iteration Networks. In NIPS, 2016.\n[139] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen,\nYan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #Explo-\nration: A Study of Count-Based Exploration for Deep Reinforcement\nLearning. In NIPS, 2017.\n[140] Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan,\nJames Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu.\nDistral: Robust Multitask Reinforcement Learning. In NIPS, 2017.\n[141] Gerald Tesauro.\nTemporal Difference Learning and TD-Gammon.\nCommunications of the ACM, 38(3):58–68, 1995.\n[142] Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart, David\nLevine, Freeman Rawson, and Charles Lefurgy. Managing Power Con-\nsumption and Performance of Computing Systems using Reinforcement\nLearning. In NIPS, 2008.\n[143] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and\nShie Mannor. A Deep Hierarchical Approach to Lifelong Learning in\nMinecraft. In AAAI, 2017.\n[144] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A Physics\nEngine for Model-Based Control. In IROS, 2012.\n[145] John N Tsitsiklis and Benjamin Van Roy.\nAnalysis of Temporal-\nDifference Learning with Function Approximation. In NIPS, 1997.\n[146] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao\nPeng, Sergey Levine, Kate Saenko, and Trevor Darrell.\nTowards\nAdapting Deep Visuomotor Representations from Simulated to Real\nEnvironments. In WAFR, 2016.\n[147] Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala.\nEpisodic Exploration for Deep Deterministic Policies: An Application\nto StarCraft Micromanagement Tasks. In ICLR, 2017.\n[148] Hado van Hasselt. Double Q-Learning. In NIPS, 2010.\n[149] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement\nLearning with Double Q-Learning. In AAAI, 2016.\n[150] Harm Vanseijen and Rich Sutton.\nA Deeper Look at Planning as\nLearning from Replay. In ICML, 2015.\n[151] Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex\nGraves, Oriol Vinyals, John Agapiou, and Koray Kavukcuoglu. Strate-\ngic Attentive Writer for Learning Macro-Actions. In NIPS, 2016.\n[152] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas\nHeess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. FeUdal\nNetworks for Hierarchical Reinforcement Learning. In ICML, 2017.\n[153] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexan-\nder Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich\nK¨uttler, John Agapiou, Julian Schrittwieser, et al. StarCraft II: A New\nChallenge for Reinforcement Learning. arXiv:1708.04782, 2017.\n[154] Niklas Wahlstr¨om, Thomas B Sch¨on, and Marc P Deisenroth. Learning\nDeep Dynamical Models from Image Pixels.\nIFAC SYSID, 48(28),\n2015.\n[155] Niklas Wahlstr¨om, Thomas B Sch¨on, and Marc P Deisenroth. From\nPixels to Torques: Policy Learning with Deep Dynamical Models. In\nICML Workshop on Deep Learning, 2015.\n[156] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,\nJoel Z Leibo, R´emi Munos, Charles Blundell, Dharshan Kumaran, and\nMatt Botvinick. Learning to Reinforcement Learn. In CogSci, 2017.\n[157] Ziyu Wang, Nando de Freitas, and Marc Lanctot. Dueling Network\nArchitectures for Deep Reinforcement Learning. In ICLR, 2016.\n[158] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, R´emi\nMunos, Koray Kavukcuoglu, and Nando de Freitas. Sample Efﬁcient\nActor-Critic with Experience Replay. In ICLR, 2017.\n[159] Christopher JCH Watkins and Peter Dayan.\nQ-Learning.\nMachine\nLearning, 8(3-4):279–292, 1992.\n[160] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin\nRiedmiller.\nEmbed to Control: A Locally Linear Latent Dynamics\nModel for Control from Raw Images. In NIPS, 2015.\n[161] Th´eophane Weber, S´ebastien Racani`ere, David P Reichert, Lars\nBuesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom`enech\nBadia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al.\nImagination-\nAugmented Agents for Deep Reinforcement Learning. In NIPS, 2017.\n[162] Paul John Werbos.\nBeyond Regression: New Tools for Prediction\nand Analysis in the Behavioral Sciences. Technical report, Harvard\nUniversity, Applied Mathematics, 1974.\n[163] Daan Wierstra, Alexander F¨orster, Jan Peters, and J¨urgen Schmidhuber.\nRecurrent Policy Gradients. Logic Journal of the IGPL, 18(5):620–634,\n2010.\n[164] Ronald J Williams. Simple Statistical Gradient-Following Algorithms\nfor Connectionist Reinforcement Learning. Machine Learning, 8(3-4):\n229–256, 1992.\n[165] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner.\nMaximum\nEntropy Deep Inverse Reinforcement Learning. In NIPS Workshop on\nDeep Reinforcement Learning, 2015.\n[166] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C\nCourville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio.\nShow, Attend and Tell: Neural Image Caption Generation with Visual\nAttention. In ICML, volume 14, 2015.\n[167] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav\nGupta, Li Fei-Fei, and Ali Farhadi. Target-Driven Visual Navigation\nin Indoor Scenes using Deep Reinforcement Learning. In ICRA, 2017.\n[168] Barret Zoph and Quoc V Le.\nNeural Architecture Search with\nReinforcement Learning. In ICLR, 2017.\nKai Arulkumaran (ka709@imperial.ac.uk) is a Ph.D. candidate in the\nDepartment of Bioengineering at Imperial College London. He received a\nB.A. in Computer Science at the University of Cambridge in 2012, and an\nM.Sc. in Biomedical Engineering at Imperial College London in 2014. He\nwas a Research Intern in Twitter Magic Pony and Microsoft Research in\n2017. His research focus is deep reinforcement learning and transfer learning\nfor visuomotor control.\nMarc Peter Deisenroth (m.deisenroth@imperial.ac.uk) is a Lecturer in\nStatistical Machine Learning in the Department of Computing at Imperial\nCollege London and with PROWLER.io. He received an M.Eng. in Computer\nScience at the University of Karlsruhe in 2006 and a Ph.D. in Machine\nLearning at the Karlsruhe Institute of Technology in 2009. He has been\nawarded an Imperial College Research Fellowship in 2014 and received Best\nPaper Awards at ICRA 2014 and ICCAS 2016. He is a recipient of a Google\nFaculty Research Award and a Microsoft Ph.D. Scholarship. His research\nis centred around data-efﬁcient machine learning for autonomous decision\nmaking.\nMiles Brundage (miles.brundage@philosophy.ox.ac.uk) is a Ph.D. candidate\nin Human and Social Dimensions of Science and Technology at Arizona\nState University, and a Research Fellow at the University of Oxford’s Future\nof Humanity Institute. He received a B.A. in Political Science at George\nWashington University in 2010. His research focuses on governance issues\nrelated to artiﬁcial intelligence.\nAnil Anthony Bharath (a.bharath@imperial.ac.uk) is a Reader in the De-\npartment of Bioengineering at Imperial College London and a Fellow of the\nInstitution of Engineering and Technology. He received a B.Eng. in Electronic\nand Electrical Engineering from University College London in 1988, and\na Ph.D. in Signal Processing from Imperial College London in 1993. He\nwas an academic visitor in the Signal Processing Group at the University of\nCambridge in 2006. He is a co-founder of Cortexica Vision Systems. His\nresearch interests are in deep architectures for visual inference.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2017-08-19",
  "updated": "2017-09-28"
}