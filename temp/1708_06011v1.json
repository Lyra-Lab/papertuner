{
  "id": "http://arxiv.org/abs/1708.06011v1",
  "title": "Modelling Word Burstiness in Natural Language: A Generalised Polya Process for Document Language Models in Information Retrieval",
  "authors": [
    "Ronan Cummins"
  ],
  "abstract": "We introduce a generalised multivariate Polya process for document language\nmodelling. The framework outlined here generalises a number of statistical\nlanguage models used in information retrieval for modelling document\ngeneration. In particular, we show that the choice of replacement matrix M\nultimately defines the type of random process and therefore defines a\nparticular type of document language model. We show that a particular variant\nof the general model is useful for modelling term-specific burstiness.\nFurthermore, via experimentation we show that this variant significantly\nimproves retrieval effectiveness over a strong baseline on a number of small\ntest collections.",
  "text": "arXiv:1708.06011v1  [cs.IR]  20 Aug 2017\nModelling Word Burstiness in Natural Language\nA Generalised P´olya Process for Document Language Models in\nInformation Retrieval\nRonan Cummins\nAbstract\nWe introduce a generalised multivariate P´olya process for document\nlanguage modelling. The framework outlined here generalises a number\nof statistical language models used in information retrieval for modelling\ndocument generation. In particular, we show that the choice of replace-\nment matrix M ultimately deﬁnes the type of random process and there-\nfore deﬁnes a particular type of document language model. We show that\na particular variant of the general model is useful for modelling term-\nspeciﬁc burstiness. Furthermore, via experimentation we show that this\nvariant signiﬁcantly improves retrieval eﬀectiveness on a number of small\ntest collections.\n1\nIntroduction\nDocument language modelling is a crucial component in statistical approaches\nto information retrieval [10, 8, 12]. These types of approach are generative in\nnature, in so far as they aim to estimate a model of the document generation\nprocess. Often these generative document models are simple unigram mixture\nmodels that incorporate information from the background collection as well\nas information from a speciﬁc document. Thus, the main challenge is one of\nparameter estimation and consequently various smoothing approaches have been\nstudied [12] that aim to better estimate the parameters of the document models\ngiven a collection of documents. In information retrieval, once the document\nmodels have been estimated, documents can be ranked by the likelihood of their\ndocument model generating the query (i.e. the query-likelihood approach [10]).\nRecently, document language models that exhibit a self-reinforcing property,\nvia a multivariate P´olya process, have been shown to signiﬁcantly increase re-\ntrieval eﬀectiveness [5]. This approach captures word burstiness in a document-\nspeciﬁc way. In this paper, we take this approach further and outline a more\ngeneral process for statistical document modelling, one which encompasses a\nnumber of existing document language models as well as a number of novel\nvariants.\n1\nIn Section 2, we outline a general model of document generation in terms\nof an urn scheme which crucially models the dynamics of document generation\nusing a matrix M. Section 3 discusses speciﬁc instantiations of M, ones which\nlead to determining diﬀerent generative distributions. Section 4 outlines how\nthe new general model is used for document retrieval. Section 5 outlines our\nexperimental set-up. The results of a number of experiments are reported in\nSection 6. Finally, in Section 7 we conclude with a discussion and outline some\nfuture work.\n2\nGeneralised P´olya\nLet u0 be an urn initially containing |u0| balls in total where each ball is one\nof v distinct colours. Starting at time i = 0, a ball is drawn with replacement\nfrom the urn and a number of additional balls (possibly of diﬀerent colours)\nare added1 to the urn according to a replacement matrix M. Each row of this\nmatrix determines the number of additional balls of each colour to add to the\nurn and the row is selected according to the colour of the ball drawn from the\nurn. Therefore, the dynamical nature of this random process is deﬁned by this\nv2 matrix M.\nNow, if d = {ti} is a sequence of observations indexed from i = 0 to i = |d|−1\ndrawn from the urn, then the state of the urn can be describe as a recurrent\nprocess as follows:\nui+1 = ui + eti · M\n(1)\nwhere eti is a standard basis vector (a.k.a a one-hot vector2). At a particular\ntime i and for an observation ti, the process essentially selects row ti of the\nmatrix M and then combines it with the ith state of the urn ui. This imbues\nthe urn with a reinforcing property, and in speciﬁc cases results in the traditional\nP´olya urn scheme. The generalised P´olya process is thus deﬁned by both the\nreplacement matrix M and the initial parameters u0 and so the likelihood of\nseeing a speciﬁc sample d can be written as p(d|u0, M). One can interpret u0\nas completely deﬁning the probability of seeing a particular coloured ball on the\nﬁrst draw, and can interpret M as deﬁning the dynamics of the urn. These types\nof model are well-suited to modelling natural language where diﬀerent coloured\nballs represent diﬀerent word-types [7] and where the number of balls of each\ncolour in the urn is proportional to the probability of generating a speciﬁc word.\n3\nChoices of M\nIn this section, we outline four diﬀerent variants of M. The ﬁrst two of these\nvariants do not rely on estimating M from data and already appear in the\nliterature [11, 5] in one form or another. They have also been implemented\n1such that the mass of the urn never decreases\n2a vector that is a 1 in dimension ti and 0 elsewhere\n2\nand have resulted in increases in retrieval eﬀectiveness and in advances of a\ntheoretical nature. The latter two models have yet to be realised.\n3.1\nZero Matrix\nIf we choose M to be the zero (denoted 0) matrix, the initial state of the urn\ndoes not change during the process. As the drawing of a particular colour does\nnot eﬀect subsequent draws, choosing the zero matrix is equivalent to using a\nmultinomial language model.\nM =\n\n\n\n\n\n0\n0\n· · ·\n0\n0\n0\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\n0\n\n\n\n\n\n(2)\nEstimating the maximum-likelihood parameters of u0 is trivial as they have\nclosed-form solutions. Using this multinomial document language model (with\ndiﬀerent types of smoothing) is quite eﬀective for retrieval [12]. In particular,\nit was shown that a document language model using Dirichlet prior smoothing\nis particularly eﬀective.\n3.2\nIdentity Matrix\nIf we choose M to be the identity matrix (denoted 1), the state of the initial urn\nchanges with a self-reinforcing property, as the drawing of a particular coloured\nball only reinforces the urn with another copy of that particular colour. This\nprocess is equivalent to the multivariate P´olya urn or Dirichlet-compound multi-\nnomial (DCM) and has been shown to generate text according to the power-law\ncharacteristics of natural language.\nM =\n\n\n\n\n\n1\n0\n· · ·\n0\n0\n1\n· · ·\n0\n...\n...\n1\n...\n0\n0\n0\n1\n\n\n\n\n\n(3)\nThis document language model has been implemented recently and has shown\nsigniﬁcant increases in retrieval eﬀectiveness over the multinomial model [5].\nFurthermore, it has led to a number of theoretically interesting properties. It\nmodels word burstiness in a document-speciﬁc manner which in turn has led\nto a better understanding of both the scope and verbosity hypotheses [3]. This\ndistribution contains only one extra parameter compared to the multinomial (a\nparameter which models the burstiness of terms). However, one of the weak-\nnesses of the model is that is assumes that all terms are equally bursty.\n3\n3.3\nDiagonal Matrix\nIf we choose M to be any positive diagonal matrix, the state of the initial\nurn again changes with a self-reinforcing property, but this time the amount\nof reinforcement is diﬀerent depending on the colour of the ball drawn. This\nprocess can captures term-speciﬁc burstiness where certain words are more likely\nto repeat within a speciﬁc sample than others.\nM =\n\n\n\n\n\nm1,1\n0\n· · ·\n0\n0\nm2,2\n· · ·\n0\n...\n...\nmv−1,v−1\n...\n0\n0\n0\nmv,v\n\n\n\n\n\n(4)\nIn this case, we need to determine the v parameters in this matrix. In this\nmodel, each term has an initial probability of occurring but also a speciﬁc pa-\nrameter controlling its re-occurrence. We can set these extra parameters to\nsome intuitively motivated values or can estimate them directly from data. The\nlatter can be done using numerical optimisation or sampling methods. This is\nthe model that is the focus of the experiments in this paper.\n3.4\nFull Replacement Matrix\nIf we choose M to be any matrix with the constraint such that for any replace-\nment row vector mk then Pv\nj=1 mkj > 0, the state of the urn can change in\nall v dimensions for any draw. The constraint is necessary to ensure that the\nprocess can continue ad inﬁnitum (i.e. the urn is never left with less mass than\nprevious).\nM|v|,|v| =\n\n\n\n\n\n\nm1,1\nm1,2\n· · ·\n· · ·\nm2,1\nm2,2\n· · ·\n...\n...\n...\n...\n...\nmv,1\n0\n0\nmv,v\n\n\n\n\n\n\n(5)\nThis model can capture dependencies between diﬀerent word-types within docu-\nments. For example, it may be that the words dna and blood tend to occur in the\nsame documents. While this model is theoretically interesting, the remainder\nof this paper is focussed on the model presented in Section 3.3.\n3.5\nDiscussion\nThe general model outlined here (Eq. 1) is an intuitive statistical generative\nmodel of documents.\nThe vector ui can be seen as storing the state of the\nmodel at a particular time i.\nBoth the multinomial and multivariate P´olya\nurn (SPUD [5]) language model are speciﬁc instances of this model and are\ninstantiated by diﬀerent settings of M. Given that the SPUD language model\nsigniﬁcantly improves upon the multinomial model in information retrieval, the\n4\nfurther extensions hold the promise of improved performance and of greater the-\noretical understanding. Furthermore, it is worth noting that the dependencies\nthat the models3 capture, span a greater distance than n-gram models (i.e. a\nword occurring at the start of a document aﬀects the choice of word at the end\nof a document).\nThe main challenges to implementing the remaining model variants are in\nestimating M and u0 from a large background model (document collection D)\nand subsequently in inferring the initial state of each document model. For large\nscale collections this a computationally expensive inverse problem. However, the\nupcoming section will outline some promising initial experiments with regard to\nthe third variant of the general model (i.e. modelling term-speciﬁc burstiness\nfor retrieval). The fourth variant is left for future work.\n4\nParameter Estimation\nThis section is concerned with estimating from data the parameters of the gen-\neral background language model u0 and M, and the unsmoothed document\nmodel ud\n0.\n4.1\nBackground Model Estimation\nIn the language modelling approach to information retrieval, it is common to\nestimate a background collection model using all of the documents d from the\ncollection D. Therefore, we ﬁrst estimate the parameters of the background\nmodel (u0 and M) that has generated all the documents in the collection D.\nWe adopt a Bayesian approach to this problem and aim use the posterior mean\nas the point-estimate as follows:\nE(u0, M) =\nZ\n(u0, M) · p(u0, M|D) · du0,M\n(6)\nIn general, estimating these parameters is computationally expensive for large\nscale document collections. In this paper, we use smaller collections and make\nuse of MCMC sampling to approximate the posterior distribution. The main\nbottleneck in such an approach is estimating the likelihood of the data given\nthe parameters (for many parameter samples). However, there are alternative\napproaches to setting these parameters. For example, if M is set to a ﬁxed\nvalue, then we only need to estimate u0 from data. We will outline the speciﬁcs\nof the estimation approach in Section 5.\n4.2\nDocument Model Estimation\nOnce M is known4, the parameters of the unsmoothed document model (ud\n0)\nthat generated a speciﬁc document d need to be estimated. Again we take a\nBayesian approach and estimate the expectation of the posterior as follows:\n3those from from Section 3.2 onwards\n4via estimation or intuition\n5\nE(ud\n0) =\nZ\nud\n0 · p(ud\n0|M, d) · dud\n0\n(7)\nwhere it is worth noting that these parameters need to be estimated for each\ndocument d in the collection D. M is ﬁxed here as it represents the general dy-\nnamics of document generation (not of a speciﬁc document). Once both u0 and\nud\n0 are estimated, they can be linearly smoothed using a single hyperparameter\nas follows:\n(ud′\n0 ) = ((1 −ω) · ud\n0 + ω · u0)\n(8)\nwhere 0 ≤ω ≤1 is a tuning parameter and ud′\n0 is the ﬁnal document model.\nThe parameters of ud′\n0 can be interpreted as the initial proportions of words of\neach type in the document model before any draws have been made.\n4.3\nQuery-Likelihood for Retrieval\nWe adopt the well-known query likelihood approach.\nTherefore, each docu-\nment d can be ranked by determining the probability of their document model\ngenerating the query as follows:\np(q|ud′\n0 , Mq)\n(9)\nwhere Mq is a dynamical model for query generation. It may be the same as\nM (for documents) or could be something as simple as the zero matrix. In\nthis paper, we assume a zero matrix where is no query dynamics. We justify\nthis by noting that queries are typically much shorter than documents and are\nmotivated by a very diﬀerent need when compared to documents. Therefore, in\nthis paper we rank documents according to the following formula once both u0\nand ud\n0 are estimated:\nlog p(q|ud′\n0 , 0) =\nX\nt∈q\nlog(\nµd\nµd + µ · ud\n0t\n|ud\n0| +\nµ\nµd + µ\nu0t\n|u0|)\n(10)\nwhere |ud\n0| and |u0| are the mass of the document model and background model\nused to calculate actual probabilities. However, when estimating the parameters\nof ud\n0 of a single document d, the mass (i.e.\n|ud\n0|) is often not constrained.\nTherefore, we introduce µd as the initial mass of the document model and set\nit to the number of unique terms in d. Furthermore, by re-writing the retrieval\nformula, we have subsumed ω into µ, thus leaving µ the only free hyperparameter\nin the model.\n5\nExperiments\nThe aim of our experiments is to test the third variant (denoted GSPUD from\nnow on) of the general model and compare it to the models when M = 0 (MULT)\n6\nand M = 1 (DCM). This third model contains v extra parameters compared\nto the multivariate P´olya scheme (DCM). These extra parameters model the\nburstiness of each term individually. We outline two approaches to ﬁnding them.\nOne method involves setting these parameters according to some heuristic (i.e.\nintuition), and the second method involves estimating the parameters directly\nfrom the data using numerical methods. Before this we ﬁrst outline our use of\nMetropolis-Hastings MCMC sampling for estimating the parameters (u0 and\nud\n0) of the models outlined in this work.\n5.1\nEstimation\nFor all of the models implemented in this work, we make use of MCMC sampling\nto estimate the posterior expectation of the parameter distribution. We use the\nMetropolis-Hastings algorithm as it is easily implemented (i.e. one only needs\nto be able to calculate the likelihood of the data given a set of parameters), can\ndeal with complex distributions (i.e. we do not know the conjugate distribution\nof the generalised P´olya distribution), and can deal with high-dimensional data\n(i.e. the background model will contain thousands of parameters/words). Other\ntechniques may indeed prove to be faster, but for our purposes we only need a\nmethod of arriving at suitable estimates.\nIn particular, we use the Metropolis-Hastings algorithm with a Gaussian\nproposal distribution (variance of 0.01 for the background model and variance\nof 0.25 for each document model). For background models, we run the chain for\n500,000 samples discarding the ﬁrst 50,000 samples (i.e. burn-in period). For\nindividual document models, we run the chain for 200,000 samples discarding\nthe ﬁrst 20,000 samples. We estimate the expectation of the posterior parameter\ndistribution using the likelihood function with a uniform prior. We start the\nalgorithm with a uniform distribution (all parameters are set to 1.0).5\nWe\nused these techniques on all variants of our models. We used this method for\nthe multinomial model in order to compare the eﬀectiveness of the sampling\nalgorithm on models where we have closed-form maximum-likelihood estimates.\nThis was especially useful during development.6\n5.2\nSetting M Heuristically\nOne method of setting the v parameters of M is to set them according to a\nheuristic. One measure of term-burstiness that has been outlined in the litera-\nture is as follows:\nmt,t = bst = cft\ndtt\n(11)\nwhere cft is the frequency of the term in the collection, and dft is the document\nfrequency of the term. This quantity measures the average frequency of a word\n5In practice we sample in the log parameter space and then exponentiate to avoid negative\nvalues which are invalid for these models.\n6Code available at https://github.com/ronancummins/gen_polya\n7\nin a document given that it has occurred once.\nThe measure has appeared\nextensively in the information retrieval literature [9, 6, 4]. It has a lower bound\nof 1.0 (which in fact is the default value of term-burstiness in the multivariate\nP´olya scheme outlined in Section 3.2).\nOnce M is set in this way, we can\nestimate the initial parameters u0 and ud\n0 using MCMC.\n5.3\nData\nDue to the computationally expensive nature of the task, we use three small\ntest collections (Medline, Cranﬁeld, and CISI)7. Although, these collections are\nrather old and are quite small, when compared to modern large scale Web\ncollections such as ClueWeb, they can provide some insights and intuitions into\nwhat natural language characteristics are being captured by the new model. We\nremoved standard stopwords and stemmed the documents and queries. Table 1\nshows some of the characteristics of the collections after preprocessing.\nCollection\nMedline\nCranﬁeld\nCISI\n# docs\n1,033\n1,400\n1,460\n# vocab (v)\n8,764\n5,769\n7,062\n# tokens\n97,175\n153,276\n115,527\n# qrys\n30\n225\n76\nTable 1: Test Collections Details\n6\nResults\nIn this section we report our results along with some qualitative analysis that\naim to interpret them further.\n6.1\nBackground Estimation\nFirst, in Table 2 we look at how well our diﬀerent models ﬁt the background\ncollection. We use perplexity to compare the performance of our model on the\ndata on which the models were trained. Perplexity is a measure of how surprised\nour model is by the data (a lower perplexity indicating a better model). The\nnumber of parameters for each model is shown in the table, where the vocabulary\n(v) can be found in Table 1.\nAs a benchmark for our sampling algorithm, we used MCMC to estimate\nthe parameters of the multinomial model (M = 0). We see that our MCMC\nestimates (MULTmc) result in a model which has a very similar perplexity\nto the maximum-likelihood estimates (MULTmle). Furthermore, the DCMmc\nmodel (M = 1) improves substantially over the multinomial model with only\none extra parameter. The generalised P´olya model with v burstiness parameters\nset to bst (GSPUDbst) improves over the DCM model. Finally, the generalised\n7Available from http://ir.dcs.gla.ac.uk/resources/test_collections/\n8\nP´olya model with v burstiness parameters estimated via MCMC (GSPUDmc)\nhas the lowest perplexity of all models. While this tells us that the parameters\nare modelling aspects of the document collection such that they are better able\nto predict the sequence of words, it is unclear whether these results extend\nbeyond the collections on which they were trained. While these experiments\nprovide a useful sanity check regarding the ability of our models to better ﬁt to\ndata, we need to use these models in a retrieval setting to ultimately determine\nif they can improve eﬀectiveness.\nEstimation\nDatasets\nModel\nu0\nM\n# params\nMedline\nCranﬁeld\nCISI\nMULTmle\nmle\n0\nv −1\n2047.4\n971.6\n1309.7\nMULTmc\nmcmc\n0\nv −1\n2079.2\n980.2\n1325.0\nDCMmc\nmcmc\n1\nv\n1728.8\n883.1\n1282.2\nGSPUDbst\nmcmc\nbst\n2v\n1369.1\n688.4\n1248.4\nGSPUDmc\nmcmc\nmcmc\n2v\n1152.7\n597.8\n999.6\nTable 2: Perplexity (nats) of diﬀerent language models trained on the back-\nground corpus.\n6.2\nDocument Models and Retrieval Performance\nTable 3 shows the optimal performance of each model when µ is tuned per\ncollection over the values µ = 10, 50, 100, 200, 300, 400, 500, 1000, 10000. Fig. 1\nshows the trends when µ varies. In all cases we can see that the GSPUD mod-\nels outperform both the DCM and MULT models. The results are statistically\nsigniﬁcant. The best performing model is the GSPUD model that estimates\nits burstiness parameters directly from data rather then heuristically. Interest-\ningly, the MULTmc approach outperforms MULTmle suggesting that Bayesian\nestimates might be better than maximum-likelihood estimates for the retrieval\ntask.\nEstimation\nDatasets\nModel\nu0, ud\n0\nM\nMedline\nCranﬁeld\nCISI\nMULTmle\nmle\n0\n0.504\n0.402\n0.221\nMULTmc\nmcmc\n0\n0.506\n0.409\n0.225\nDCMmc\nmcmc\n1\n0.517m\n0.414m\n0.230m\nGSPUDbst\nmcmc\nbst\n0.523md\n0.427md\n0.233md\nGSPUDmc\nmcmc\nmcmc\n0.533md\ng\n0.432md\ng\n0.245md\ng\nTable 3: Performance (Mean Average Precision) of diﬀerent language models\nwith µ tuned per collection. The keys m, d, g mean that the result is statistically\nsigniﬁcant compared to MULT, DCM, and GSPUDbst respectively at the p <\n0.01 level using a permutation test.\n9\n 0.42\n 0.44\n 0.46\n 0.48\n 0.5\n 0.52\n 0.54\n 10\n 100\n 1000\n 10000\nGSPUDmc\nGSPUDbst\nDCMmc\nMULTmc\nMULTmle\n 0.3\n 0.32\n 0.34\n 0.36\n 0.38\n 0.4\n 0.42\n 0.44\n 10\n 100\n 1000\n 10000\nGSPUDmc\nGSPUDbst\nDCMmc\nMULTmc\nMULTmle\n 0.12\n 0.14\n 0.16\n 0.18\n 0.2\n 0.22\n 0.24\n 0.26\n 10\n 100\n 1000\n 10000\nGSPUDmc\nGSPUDbst\nDCMmc\nMULTmc\nMULTmle\nFigure 1: MAP of diﬀerent models for varying values of µ Medline, Cranﬁeld,\nand CISI.\n6.3\nQualitative Analysis\nIn order to qualitatively evaluate what our models are learning, we now analyse\na number of words that appear in the Medline collection.\nThe three words\nalso, dna, and refer are shown with some of their collection statistics. In a\nmultinomial model, also and dna would have a very similar discriminative value\nbecause they occur nearly the same number of times throughout the corpus.\nModels that only use a type of idft would assign a similar discriminative value\nto refer and dna.\nHowever, it is intuitive that dna is a much more useful\nword in general due to its burstiness. We can see that the burstiness of dna\nis much higher than the other terms when estimated from data (i.e. ˆmt,t). It\nis worth remembering that in the GSPUD model, each word is characterised\nby two parameters, one that controls the probability of being drawn from the\ninitial urn at time t = 0 and one that controls the burstiness. For the GSPUD\nmodel, the initial probability of a word being drawn from the urn is very closely\ncorrelated with the document frequency and is close to the probability\ndft\nP\nt′ dft′ .\nThe ideas of word burstiness have been around for quite a while with a number\nof interesting papers written by Church and Gale [2, 1] outlining the properties\nof important keywords in documents.\nterm (t)\ncft\ndft\nbst\nˆu0t\nˆmt,t\nalso\n216\n180\n1.20\n37.86\n10.22\ndna\n214\n47\n4.55\n9.28\n266.27\nrefer\n51\n47\n1.08\n9.08\n3.22\nTable 4:\nSome statistics and example estimates of the parameters of the\nGSPUDmc for three words\n7\nSummary\nThis paper has introduced a family of statistical language models inspired by\na classic urn model. We have shown that it is the replacement matrix M that\n10\ndeﬁnes the dynamics of the model. We have implemented a variant of the model\nwhich models burstiness in a term-speciﬁc manner. We have shown that the\nparameters of the model can be estimated from data using sampling techniques.\nFurthermore, we have incorporated the new language model into a retrieval\nframework and shown that retrieval eﬀectiveness improves signiﬁcantly over\na highly competitive baseline language model. Although our experiments are\nconducted on small test collections (because parameter estimation is compu-\ntationally expensive), the results are promising. We believe that this is ﬁrst\npaper that deals with term-speciﬁc burstiness in such a principled probabilistic\nmanner.\nReferences\n[1] Kenneth Church and William Gale. Inverse document frequency (idf): A\nmeasure of deviations from poisson. In Natural language processing using\nvery large corpora, pages 283–295. Springer, 1999.\n[2] Kenneth W. Church and William A. Gale.\nPoisson mixtures.\nNatural\nLanguage Engineering, 1:163–190, 1995.\n[3] Ronan Cummins.\nA study of retrieval models for long documents and\nqueries in information retrieval. In Proceedings of the 25th International\nConference on World Wide Web, WWW ’16, pages 795–805. International\nWorld Wide Web Conferences Steering Committee, 2016.\n[4] Ronan Cummins and Colm ORiordan. Evolving local and global weight-\ning schemes in information retrieval. Information Retrieval, 9(3):311–330,\n2006.\n[5] Ronan Cummins, Jiaul H. Paik, and Yuanhua Lv. A P´olya urn document\nlanguage model for improved information retrieval.\nACM Transactions\nInformation Systems, 33(4):21:1–21:34, May 2015.\n[6] Martin Franz and J. Scott McCarley. Word document density and relevance\nscoring (poster session). In Proceedings of the 23rd Annual International\nACM SIGIR Conference on Research and Development in Information Re-\ntrieval, SIGIR ’00, pages 345–347, New York, NY, USA, 2000. ACM.\n[7] Sharon Goldwater, Thomas L. Griﬃths, and Mark Johnson. Producing\npower-law distributions and damping word frequencies with two-stage lan-\nguage models. Journal of Machine Learning Research, 12:2335–2382, July\n2011.\n[8] Djoerd Hiemstra. Using language models for information retrieval. Taaluit-\ngeverij Neslia Paniculata, 2001.\n[9] K. L. Kwok. A new method of weighting query terms for ad-hoc retrieval.\nIn Proceedings of the 19th Annual International ACM SIGIR Conference\n11\non Research and Development in Information Retrieval, SIGIR ’96, pages\n187–195, New York, NY, USA, 1996. ACM.\n[10] Jay M. Ponte and W. Bruce Croft.\nA language modeling approach to\ninformation retrieval. In Proceedings of the 21st Annual International ACM\nSIGIR Conference on Research and Development in Information Retrieval,\nSIGIR ’98, pages 275–281, New York, NY, USA, 1998. ACM.\n[11] Chengxiang Zhai and John Laﬀerty. A study of smoothing methods for\nlanguage models applied to ad hoc information retrieval. In Proceedings of\nthe 24th Annual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, SIGIR ’01, pages 334–342, New\nYork, NY, USA, 2001. ACM.\n[12] Chengxiang Zhai and John Laﬀerty. A study of smoothing methods for\nlanguage models applied to information retrieval. ACM Transactions of\nInformation Systems, 22:179–214, April 2004.\n12\n",
  "categories": [
    "cs.IR"
  ],
  "published": "2017-08-20",
  "updated": "2017-08-20"
}