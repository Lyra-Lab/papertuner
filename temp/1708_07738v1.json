{
  "id": "http://arxiv.org/abs/1708.07738v1",
  "title": "A Function Approximation Method for Model-based High-Dimensional Inverse Reinforcement Learning",
  "authors": [
    "Kun Li",
    "Joel W. Burdick"
  ],
  "abstract": "This works handles the inverse reinforcement learning problem in\nhigh-dimensional state spaces, which relies on an efficient solution of\nmodel-based high-dimensional reinforcement learning problems. To solve the\ncomputationally expensive reinforcement learning problems, we propose a\nfunction approximation method to ensure that the Bellman Optimality Equation\nalways holds, and then estimate a function based on the observed human actions\nfor inverse reinforcement learning problems. The time complexity of the\nproposed method is linearly proportional to the cardinality of the action set,\nthus it can handle high-dimensional even continuous state spaces efficiently.\nWe test the proposed method in a simulated environment to show its accuracy,\nand three clinical tasks to show how it can be used to evaluate a doctor's\nproficiency.",
  "text": "arXiv:1708.07738v1  [cs.LG]  23 Aug 2017\nA Function Approximation Method for Model-based High-Dimensional\nInverse Reinforcement Learning\nKun Li1, Joel W. Burdick1\nAbstract— This works handles the inverse reinforcement\nlearning problem in high-dimensional state spaces, which re-\nlies on an efﬁcient solution of model-based high-dimensional\nreinforcement learning problems. To solve the computation-\nally expensive reinforcement learning problems, we propose\na function approximation method to ensure that the Bellman\nOptimality Equation always holds, and then estimate a function\nbased on the observed human actions for inverse reinforcement\nlearning problems. The time complexity of the proposed method\nis linearly proportional to the cardinality of the action set,\nthus it can handle high-dimensional even continuous state\nspaces efﬁciently. We test the proposed method in a simulated\nenvironment to show its accuracy, and three clinical tasks to\nshow how it can be used to evaluate a doctor’s proﬁciency.\nI. INTRODUCTION\nRecently, surgical robots, like Da Vinci Surgical System,\nhave been applied to many tasks, due to its reliability and\naccuracy. In these systems, a doctor operates the robot\nmanipulator remotely, and gets the visual feedback during\na surgery. With a sophisticated control system and high-\nresolution images, the surgery can be done with higher preci-\nsion and less accidents. However, this requires the doctor to\nconcentrate on robot operations and visual feedbacks during\nthe whole surgery, which may lead to fatigue and errors.\nTo solve the problem, some level of automation can be\nintroduced, considering that many surgeries contain repeating\natomic operations. For example, knot tying is a typical\nprocedure after many surgeries, as shown in Figure 1, and it\ncan be decomposed into a sequence of pre-trained standard\noperations for the robot. The automation can also be used\nto avoid possible mistakes committed by an inexperienced\ndoctor during a surgery, where alarm signal can be triggered\nwhen an unusual action is taken by the doctor, and the\namount of alarm signals can be used to evaluate the doctor\nas well.\nThe core of the automation system is a control policy,\npredicting which action to take under each state for typical\nsurgical robots. The control policy can be deﬁned manually,\nbut it is difﬁcult due to the possible number of states\noccurring during a surgery. Another solution is estimating\nthe policy by solving a Markov decision process, but it needs\nan accurate reward function, depending on too many factors\nto be deﬁned manually.\nAn alternative solution is learning the control policy from\nexperts’ demonstrations through imitation learning. Many\n*This work was supported by the National Institutes of Health, NIBIB.\n1Kun Li and Joel W. Burdick are with Department of Mechanical and\nCivil Engineering, California Institute of Technology, Pasadena, CA 91125,\nUSA kunli@caltech.edu\nFig. 1: Knot tying with Da Vinci robot: the photo is grabbed\nfrom JIGSAW dataset [1].\nalgorithms try to learn the policy from the state-action\npair directly in a supervised way, but the learned policy\nusually does not indicate how good a state-action pair is,\nwhich is useful for online doctor action evaluation. This\nproblem can be solved by inverse reinforcement learning\nalgorithms, which learns a reward function from the observed\ndemonstrations, and the optimality of a control policy can be\nestimated based on the reward function.\nExisting solutions of the inverse reinforcement learning\nproblem mainly work on small-scale problems, by collecting\na set of observations for reward estimation and using the es-\ntimated reward afterwards. For example, the methods in [2],\n[3], [4] estimate the agent’s policy from a set of observations,\nand estimate a reward function that leads to the policy. The\nmethod in [5] collects a set of trajectories of the agent, and\nestimates a reward function that maximizes the likelihood of\nthe trajectories. This strategy works for applications in small\nstate spaces. However, the state space of sensory feedback is\nhuge for surgical evaluation, and these method cannot handle\nit well due to the reinforcement learning problem in each\niteration of reward estimation.\nSome existing methods can be scaled to high-dimensional\nstate spaces and solve the problem without learning the\ntransition model. While they improve the learning efﬁciency,\nthey cannot utilize unsupervised data, or data from the\ndemonstrations of non-experts. These data cannot be used\nto learn the reward function, but they provide information\nabout the environment dynamics.\nIn this work, we ﬁnd that inverse reinforcement learning in\nhigh-dimensional space can be simpliﬁed under the condition\nthat the transition model and the set of action remain\nunchanged for the subject, where each reward function leads\nto a unique optimal value function. Based on this assumption,\nwe propose a function approximation method that learns the\nreward function and the optimal value function, but without\nthe computationally expensive reinforcement learning steps,\nthus it can be scaled to high dimensional state spaces.\nThis method can also solve model-based high-dimensional\nreinforcement learning problems, although it is not our main\nfocus.\nThe paper is organized as follows. We review existing\nwork on inverse reinforcement learning in Section II, and\nformulate the function approximation inverse reinforcement\nlearning method for high-dimensional problems in III. A\nsimulated experiment and a clinical experiment are shown\nin Section IV, with conclusions in Section V.\nII. RELATED WORKS\nApproximate dynamic programming for reinforcement\nlearning is a well-researched topic in Markov decision pro-\ncess. A good introduction is given in [6]. Some model-free\nmethods produce many promising results in recent years,\nlike deep Q network [7], double Q learning [8], advantage\nlearning [9], etc. But in many robotic applications, reward\nvalues are not available for all robot actions, and those\ndata is wasted in model-free learning. Common model-\nbased approximation methods use a function to approximate\nthe value function or the Q function, and the performance\ndepends on the selected features.\nInverse Reinforcement Learning problem is ﬁrstly for-\nmulated in [2], where the agent observes the states result-\ning from an assumingly optimal policy, and tries to learn\na reward function that makes the policy better than all\nalternatives. Since the goal can be achieved by multiple\nreward functions, this paper tries to ﬁnd one that maximizes\nthe difference between the observed policy and the second\nbest policy. This idea is extended by [10], in the name of\nmax-margin learning for inverse optimal control. Another\nextension is proposed in [3], where the purpose is not\nto recover the real reward function, but to ﬁnd a reward\nfunction that leads to a policy equivalent to the observed one,\nmeasured by the amount of rewards collected by following\nthat policy.\nSince a motion policy may be difﬁcult to estimate from\nobservations, a behavior-based method is proposed in [5],\nwhich models the distribution of behaviors as a maximum-\nentropy model on the amount of reward collected from each\nbehavior. This model has many applications and extensions.\nFor example, [11] considers a sequence of changing reward\nfunctions instead of a single reward function. [12] and [13]\nconsider complex reward functions, instead of linear one, and\nuse Gaussian process and neural networks, respectively, to\nmodel the reward function. [14] considers complex environ-\nments, instead of a well-observed Markov Decision Process,\nand combines partially observed Markov Decision Process\nwith reward learning. [15] models the behaviors based on the\nlocal optimality of a behavior, instead of the summation of\nrewards. [16] uses a multi-layer neural network to represent\nnonlinear reward functions.\nAnother method is proposed in [17], which models the\nprobability of a behavior as the product of each state-action’s\nprobability, and learns the reward function via maximum a\nposteriori estimation. However, due to the complex relation\nbetween the reward function and the behavior distribu-\ntion, the author uses computationally expensive Monte-Carlo\nmethods to sample the distribution. This work is extended\nby [4], which uses sub-gradient methods to simplify the\nproblem. Another extensions is shown in [18], which tries to\nﬁnd a reward function that matches the observed behavior.\nFor motions involving multiple tasks and varying reward\nfunctions, methods are developed in [19] and [20], which\ntry to learn multiple reward functions.\nMost of these methods need to solve a reinforcement\nlearning problem in each step of reward learning, thus prac-\ntical large-scale application is computationally infeasible.\nSeveral methods are applicable to large-scale applications.\nThe method in [2] uses a linear approximation of the value\nfunction, but it requires a set of manually deﬁned basis func-\ntions. The methods in [13], [21] update the reward function\nparameter by minimizing the relative entropy between the\nobserved trajectories and a set of sampled trajectories based\non the reward function, but they require a set of manually\nsegmented trajectories of human motion, where the choice\nof trajectory length will affect the result. The method in [22]\nonly learns an optimal value function, instead of the reward\nfunction.\nIII. HIGH-DIMENSIONAL INVERSE REINFORCEMENT\nLEARNING\nA. Markov Decision Process\nA Markov Decision Process is described with the follow-\ning variables:\n• S = {s}, a set of states\n• A = {a}, a set of actions\n• Pa\nss′, a state transition function that deﬁnes the probabil-\nity that state s becomes s′ after action a.\n• R = {r(s)}, a reward function that deﬁnes the immediate\nreward of state s.\n• γ, a discount factor that ensures the convergence of the\nMDP over an inﬁnite horizon.\nAn agent’s motion can be represented as a sequence of\nstate-action pairs:\nζ = {(si,ai)|i = 0,··· ,Nζ },\nwhere Nζ denotes the length of the motion, varying in\ndifferent observations. Given the observed sequence, inverse\nreinforcement learning algorithms try to recover a reward\nfunction that explains the motion.\nOne key problem is how to model the action in each state,\nor the policy, π(s) ∈A, a mapping from states to actions.\nThis problem can be handled by reinforcement learning\nalgorithms, by introducing the value function V(s) and the\nQ-function Q(s,a), described by the Bellman Equation [23]:\nV π(s) = ∑\ns′|s,π(s)\nPπ(s)\nss′ [r(s′)+ γ ∗V π(s′)],\n(1)\nQπ(s,a) = ∑\ns′|s,a\nPa\nss′[r(s′)+ γ ∗V π(s′)],\n(2)\nwhere V π and Qπ deﬁne the value function and the Q-\nfunction under a policy π.\nFor an optimal policy π∗, the value function and the\nQ-function should be maximized on every state. This is\ndescribed by the Bellman Optimality Equation [23]:\nV ∗(s) = max\na∈A ∑\ns′|s,a\nPa\nss′[r(s′)+ γ ∗V ∗(s′)],\n(3)\nQ∗(s,a) = ∑\ns′|s,a\nPa\nss′[r(s′)+ γ ∗max\na′∈A Q∗(s′,a′)].\n(4)\nIn typical inverse reinforcement learning algorithms, the\nBellman Optimality Equation needs to be solved once for\neach parameter updating of the reward function, thus it is\ncomputationally infeasible in high-dimensional state spaces.\nWhile several existing approaches solve the problem at the\nexpense of the optimality, we propose an approximation\nmethod to avoid the problem.\nB. Function Approximation Framework\nGiven the set of actions and the transition probability, a\nreward function leads to a unique optimal value function.\nTo learn the reward function from the observed motion,\ninstead of directly learning the reward function, we use a\nparameterized function, named as VR function, to represent\nthe summation of the reward function and the discounted\nvalue function:\nf(s,θ) = r(s)+ γ ∗V ∗(s).\n(5)\nThe function value of a state is named as VR value.\nSubstituting Equation (5) into Bellman Optimality Equa-\ntion, the optimal Q function is given as:\nQ∗(s,a) = ∑\ns′|s,a\nPa\nss′ f(s′,θ),\n(6)\nthe optimal value function is given as:\nV ∗(s) = max\na∈A Q∗(s,a)\n= max\na∈A ∑\ns′|s,a\nPa\nss′ f(s′,θ),\n(7)\nand the reward function can be computed as:\nr(s) = f(s,θ)−γ ∗V ∗(s)\n= f(s,θ)−γ ∗max\na∈A ∑\ns′|s,a\nPa\nss′ f(s′,θ).\n(8)\nNote that this formulation can be generalized to other\nextensions of Bellman Optimality Equation by replacing the\nmax operator with other types of Bellman backup opera-\ntors. For example, V ∗(s) = loga∈A expQ∗(s,a) is used in\nthe maximum-entropy method[5]; V ∗(s) = 1\nk loga∈A expk ∗\nQ∗(s,a) is used in Bellman Gradient Iteration [24].\nFor any VR function f and any parameter θ, the optimal Q\nfunction Q∗(s,a), optimal value function V ∗(s), and reward\nfunction r(s) constructed with Equation (6), (7), and (8)\nalways meet the Bellman Optimality Equation. Under this\ncondition, we try to recover a parameterized function f(s,θ)\nthat best explains the observed rewards for reinforcement\nlearning problems, and expert demonstrations ζ for inverse\nreinforcement learning problems.\nFor reinforcement learning problems, the Bellman backup\noperator should be a differentiable one, thus the function\nparameter can be updated based on the observed rewards.\nFor inverse reinforcement learning problems, combined\nwith different Bellman backup operators, this formulation\ncan extend many existing methods to high-dimensional\nspace, like the motion model in [25], p(a|s) = −v∗(s) −\nlog∑k ps,k exp(−v∗(k)), the motion model in [5], p(a|s) =\nexpQ∗(s,a)−V ∗(s), and the motion model in [17], p(a|s) ∝\nexpQ∗(s,a). The main limitation is the assumption of a\nknown transition model Pa\nss′, but it only requires a partial\nmodel on the visited states rather than a full environment\nmodel, and it can be learned independently in an unsuper-\nvised way.\nC. High-dimensional Reinforcement Learning\nAlthough it is not our main focus, we brieﬂy show how\nthe proposed method solves high-dimensional reinforcement\nlearning problems. Assuming the approximation function is a\nneural network, the parameter θ = {w,b}-weights and biases-\nin Equation (5) can be estimated from the observed sequence\nof rewards ˆRs via least-square estimation, where the objective\nfunction is:\nLSE(θ) = ∑\ns\n|| ˆRs −r(s)||2.\nThe reward function r(s) in Equation (8) is non-\ndifferentiable with the max function as the Bellman backup\noperator. By approximating it with the generalized softmax\nfunction [24], the gradient of the objective function is:\n∇θLSE(θ) = ∑\ns\n2 ∗|| ˆRs −r(s)||∗(−∇θr(s)),\nwhere\n∇θr(s) = ∇θ f(s,θ)−γ ∗∑\na∈A\nexp(kQ∗(s,a))\n∑a′∈A exp(kQ∗(s,a)) ∑\ns′|s,a\nPa\nss′∇θ f(s,θ),\nand k is the approximation level.\nThe parameter θ can be learned with gradient methods.\nThe algorithm is shown in Algorithm 1. With the learned\nparameter, the optimal value function and a control policy\ncan be estimated.\nD. High-dimensional Inverse Reinforcement Learning\nFor IRL problems, this work chooses max as the Bellman\nbackup operator and a motion model p(a|s) based on the\noptimal Q function Q∗(s,a) [17]:\nP(a|s) =\nexpb ∗Q∗(s,a)\n∑˜a∈A expb ∗Q∗(s, ˜a),\n(9)\nAlgorithm 1 Function Approximation RL with Neural Net-\nwork\n1: Data: R,S,A,P,γ,b,α\n2: Result:\noptimal value V ∗[S], optimal action value\nQ∗[S,A]\n3: create variable θ = {W,b} for a neural network\n4: build f[S,θ] as the output of the neural network\n5: build Q∗[S,A], V ∗[S], and R[S] based on Equation (5),\n(6), (7), and (8).\n6: build objective function LSE[θ] based on R[S]\n7: compute gradient ∇θLSE[θ]\n8: initialize θ\n9: while not converging do\n10:\nθ = θ + α ∗∇θLSE[θ]\n11: end while\n12: evaluate optimal value V ∗[S], optimal action value\nQ∗[S,A]\n13: return Q∗[S,A]\nwhere b is a parameter controlling the degree of conﬁdence\nin the agent’s ability to choose actions based on Q values. In\nthe remaining sections, we use Q∗(s,a) to denote the optimal\nQ values for simpliﬁed notations.\nAssuming the approximation function is a neural network,\nthe parameter θ = {w,b}-weights and biases-in Equation (5)\ncan be estimated from the observed sequence of state-action\npairs ζ via maximum-likelihood estimation:\nθ = argmax\nθ\nlogP(ζ|θ),\n(10)\nwhere the log-likelihood of P(ζ|θ) is given by:\nL(θ) = logP(ζ|θ)\n= log ∏\n(s,a)∈ζ\nP(a|θ;s)\n= log ∏\n(s,a)∈ζ\nexpb ∗Q∗(s,a)\n∑ˆa∈A expb ∗Q∗(s, ˆa)\n= ∑\n(s,a)∈ζ\n(b ∗Q∗(s,a)−log ∑\nˆa∈A\nexpb ∗Q∗(s, ˆa)),\n(11)\nand the gradient of the log-likelihood is given by:\n∇θL(θ) = ∑\n(s,a)∈ζ\n(b ∗∇θQ∗(s,a)\n−b ∗∑\nˆa∈A\nP((s, ˆa)|r(θ))∇θQ∗(s, ˆa)).\n(12)\nWith a differentiable approximation function,\n∇θQ∗(s,a) = ∑\ns′|s,a\nPa\nss′∇θ f(s′,θ),\nand\n∇θL(θ) = ∑\n(s,a)∈ζ\n(b ∗∑\ns′|s,a\nPa\nss′∇θ f(s′,θ)\n−b ∗∑\nˆa∈A\nP((s, ˆa)|r(θ)) ∑\ns′|s,a\nPa\nss′∇θ f(s′,θ)),\n(13)\nAlgorithm 2 Function Approximation IRL with Neural\nNetwork\n1: Data: ζ,S,A,P,γ,b,α\n2: Result:\noptimal value V ∗[S], optimal action value\nQ∗[S,A], reward value R[S]\n3: create variable θ = {W,b} for a neural network\n4: build f[S,θ] as the output of the neural network\n5: build Q∗[S,A], V ∗[S], and R[S] based on Equation (5),\n(6), (7), and (8).\n6: build loglikelihood L[θ] based on ζ and Q∗[S,A]\n7: compute gradient ∇θL[θ]\n8: initialize θ\n9: while not converging do\n10:\nθ = θ + α ∗∇θL[θ]\n11: end while\n12: evaluate optimal value V ∗[S], optimal action value\nQ∗[S,A], reward value R[S]\n13: return R[S]\nwhere ∇θ f(s′,θ) denotes the gradient of the neural network\noutput with respect to neural network parameter θ = {w,b}.\nIf the VR function f(s,θ) is linear, the objective function\nin Equation (11) is concave, and a global optimum exists.\nHowever, a multi-layer neural network works better to handle\nthe non-linearity in approximation and the high-dimensional\nstate space data.\nA gradient ascent method is used to learn the parameter\nθ:\nθ = θ + α ∗∇θL(θ),\n(14)\nwhere α is the learning rate.\nWhen the method converges, we can compute the optimal\nQ function, the optimal value function, and the reward func-\ntion based on Equation (5), (6), (7), and (8). The algorithm\nunder a neural network-based approximation function is\nshown in Algorithm 2.\nThis method does not involve solving the MDP problem\nfor each updated parameter θ, and large-scale state space can\nbe easily handled by an approximation function based on a\nmulti-layer neural network.\nObviously, the approximation function is not unique, but\nall of them will generate the same optimal values and rewards\nfor the observed state-action pairs after convergence. By\nchoosing a neural network with higher capacity, we may\noverﬁt the observed state-action distribution, and do not\ngeneralize well. Therefore, the choice of the approximation\nfunction depends on how well the observed motion matches\nthe ground truth one.\nIV. EXPERIMENTS\nWe ﬁrst test the proposed method in a simulated environ-\nment, to compare its accuracy under different approximation\nfunctions, and then apply the proposed method to surgical\ndata in JIGSAW dataset [1].\nA. Simulated Environment\nWe create a four-dimensional grids, with 10 grid in each\ndimension, thus 10000 states are generated. Several reward-\nemitting objects are put randomly in the grid, and each\nof them generates an exponentially decaying negative or\npositive reward value to all the grid based on the distances.\nThe true reward value of each grid is the summation of the\ngenerated rewards in the grid. An agent moves in the grids,\nand it can choose to move up, down, or stay still in each\ndimension, described by an action set of 34 = 81 actions.\nThe observable feature of a grid is the grid’s distances to the\nreward-generating objects.\nTo test the application of the proposed method to rein-\nforcement learning problems, we assume that the reward\nvalue of each state is available for the robot, and it has\nto learn an optimal value function from it. We compare\nthe ground truth value function, computed through value\niteration, and the value function recovered by the robot based\non the mean error of the optimal Q values.\nWe choose neural networks as the approximation function,\nand compare the errors under different neural net conﬁg-\nurations. We choose the conﬁguration by ﬁrstly ﬁxing the\nnumber of nodes in each hidden layer and increasing the\nnumber of layers, and then ﬁxing the number of hidden layers\nand increasing the number of nodes in each layer. Stochastic\ngradient descent is used in optimization, with batch size 50\nand learning rate 0.00001. The result is shown in Figure 2\nand 3.\nTo test the application of the proposed method to inverse\nreinforcement learning problems, we generate 200000 tra-\njectories with random initial position and length 10 based\non the true reward function, and try to recover a reward\nfunction based on the trajectories. We compute the accuracy\nbased on the correlation coefﬁcient between the ground truth\nreward function and the recovered reward function. Similarly,\nwe compare the accuracy under different neural network\nconﬁgurations. The result is shown in Figure 4 and 5.\nThe results show that the accuracies of learned value func-\ntion and reward function improve as the capacity of network\nincreases, and increasing network width works better.\nB. Surgical Robot Operator\nWe apply the proposed method to surgical robot operators\nin JIGSAW data set [1]. This data set describes three tasks,\nknot tying, needling passing, and suturing. An illustration\nof the tasks is shown in Figure 6. Each task is conducted\nby multiple robot operators, whose skills range from expert,\nintermediate to novice.\nThe data includes videos from two stereo cameras and\nrobot states synchronized to the images. We assume the\noperator’s actions change the linear and angular acceleration\nof the robot, and then we use k-means clustering to identify\n10000 actions from the dataset. The state set includes the\nrobot manipulator’s positions and velocities, represented by\na length-38 vector with continuous values. The transition\nprobability is computed based on physical law.\n0\n10\n20\n30\n40\n50\nTraining epochs\n0.0036\n0.0038\n0.0040\n0.0042\n0.0044\nMean Q value error\nMean Q value errors against network depth\n[10, 10, 1]\n[10, 10, 10, 1]\n[10, 10, 10, 10, 1]\n[10, 10, 10, 10, 10, 1]\n[10, 10, 10, 10, 10, 10, 1]\n[10, 10, 10, 10, 10, 10, 10, 1]\nFig. 2: The error of learned Q values versus network depth in\nRL: the compared networks range from three layers to eight\nlayers, including the input layer and the output layers. Under\neach network conﬁguration, a Q function is learned based\non the observed reward values, and the ground Q function is\ncomputed via value iteration. The mean error of the Q values\nis computed and plotted under different number of gradient\niterations.\n0\n10\n20\n30\n40\n50\nTraining epochs\n0.0034\n0.0036\n0.0038\n0.0040\n0.0042\n0.0044\n0.0046\n0.0048\nMean Q value error\nMean Q value errors against network width\n[10, 10, 1]\n[10, 20, 10, 1]\n[10, 30, 10, 1]\n[10, 40, 10, 1]\n[10, 50, 10, 1]\nFig. 3: The error of learned Q values versus network width in\nRL: the hidden nodes of the compared networks range from\nten to ﬁfty, in addition to an input layer, a common hidden\nlayer, and a output layers. Under each network conﬁguration,\na Q function is learned based on the observed reward values,\nand the ground Q function is computed via value iteration.\nThe mean error of the Q values is computed and plotted\nunder different number of gradient iterations.\nWe apply the model to surgical operator evaluation on\nthree tasks by training on all experts and testing on novice\nand intermediate operators. The results are shown in Figure\n7, 8 and 9.\nThe results show that the proposed method successfully\nidentiﬁes the difference between inexperienced operators and\nexperienced operators, thus it can be used in evaluation tasks.\nV. CONCLUSIONS\nThis work deals with the problem of high-dimensional in-\nverse reinforcement learning, where the state space is usually\n0\n10\n20\n30\n40\n50\nTraining epochs\n 0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReward accuracy\nReward accuracy against network depth\n[10, 10, 1]\n[10, 10, 10, 1]\n[10, 10, 10, 10, 1]\n[10, 10, 10, 10, 10, 1]\n[10, 10, 10, 10, 10, 10, 1]\n[10, 10, 10, 10, 10, 10, 10, 1]\nFig. 4: The accuracy of learned reward values versus network\ndepth in IRL: the compared networks range from three layers\nto eight layers, including the input layer and the output\nlayers. Under each network conﬁguration, a reward function\nis learned based on the observed actions, and the accuracy is\ncomputed as the correlation coefﬁcient between the learned\nreward and the ground truth reward.\n0\n10\n20\n30\n40\n50\nTraining epochs\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nReward accuracy\nReward accuracy against network width\n[10, 10, 1]\n[10, 20, 10, 1]\n[10, 30, 10, 1]\n[10, 40, 10, 1]\n[10, 50, 10, 1]\nFig. 5: The accuracy of learned reward values versus network\nwidth in IRL: the hidden nodes of the compared networks\nrange from ten to ﬁfty, in addition to an input layer, a\ncommon hidden layer, and a output layers. Under each\nnetwork conﬁguration, a reward function is learned based\non the observed actions, and the accuracy is computed as\nthe correlation coefﬁcient between the learned reward and\nthe ground truth reward.\n(a) Knot tying\n(b) Needle passing\n(c) Suturing\nFig. 6: Surgical operations in JIGSAW dataset: knot tying,\nneedle passing, and suturing.\n0\n20\n40\n60\n80\n100\nTraining epochs\n9.10\n9.15\n9.20\n9.25\n9.30\n9.35\n9.40\n9.45\nTest errors\nEvaluating operators with task Knot tying\nNew operators\nIntermediate operators\nFig. 7: Evaluation of operators on knot tying tasks: ”new\noperators” represent inexperienced operators, while ”inter-\nmediate operators” represent experienced operators. As the\ntraining epochs increase, the test error decreases, while\nexperienced operators have a relatively lower error rate.\n0\n20\n40\n60\n80\n100\nTraining epochs\n9.1\n9.2\n9.3\n9.4\n9.5\n9.6\n9.7\n9.8\n9.9\nTest errors\nEvaluating operators with task Needle passing\nNew operators\nIntermediate operators\nFig. 8: Evaluation of operators on needle passing tasks:\n”new operators” represent inexperienced operators, while\n”intermediate operators” represent experienced operators. As\nthe training epochs increase, the test error decreases, while\nexperienced operators have a relatively lower error rate.\n0\n20\n40\n60\n80\n100\nTraining epochs\n9.14\n9.16\n9.18\n9.20\n9.22\n9.24\n9.26\n9.28\n9.30\n9.32\nTest errors\nEvaluating operators with task Suturing\nNew operators\nIntermediate operators\nFig. 9: Evaluation of operators on suturing tasks: ”new oper-\nators” represent inexperienced operators, while ”intermediate\noperators” represent experienced operators. As the training\nepochs increase, the test error decreases, while experienced\noperators have a relatively lower error rate.\ntoo large for many existing solutions. We solve the problem\nwith a function approximation framework by approximating\nthe reinforcement learning solution. The method is ﬁrstly\ntested in a simulated environment, and then applied to the\nevaluation of surgical robot operators in three clinical tasks.\nIn current settings, each task has one reward function,\nassociated with an optimal value function. In future work,\nwe will extend this method for a robot to learn multiple\nreward functions. Besides, we will try to integrate transition\nmodel learning into the framework.\nREFERENCES\n[1] Y. Gao, S. S. Vedula, C. E. Reiley, N. Ahmidi, B. Varadarajan, H. C.\nLin, L. Tao, L. Zappella, B. B´ejar, D. D. Yuh et al., “Jhu-isi gesture\nand skill assessment working set (jigsaws): A surgical activity dataset\nfor human motion modeling,” in MICCAI Workshop: M2CAI, vol. 3,\n2014.\n[2] A. Y. Ng and S. Russell, “Algorithms for inverse reinforcement\nlearning,” in in Proc. 17th International Conf. on Machine Learning,\n2000.\n[3] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-\nforcement learning,” in Proceedings of the twenty-ﬁrst international\nconference on Machine learning.\nACM, 2004, p. 1.\n[4] G. Neu and C. Szepesv´ari, “Apprenticeship learning using in-\nverse reinforcement learning and gradient methods,” arXiv preprint\narXiv:1206.5264, 2012.\n[5] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum\nentropy inverse reinforcement learning,” in Proc. AAAI, 2008, pp.\n1433–1438.\n[6] W. B. Powell, Approximate Dynamic Programming: Solving the curses\nof dimensionality.\nJohn Wiley & Sons, 2007, vol. 703.\n[7] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,\nD. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement\nlearning,” arXiv preprint arXiv:1312.5602, 2013.\n[8] H. v. Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning,” in Proceedings of the Thirtieth AAAI Confer-\nence on Artiﬁcial Intelligence.\nAAAI Press, 2016, pp. 2094–2100.\n[9] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy\ngradient methods for reinforcement learning with function approxima-\ntion,” in Advances in neural information processing systems, 2000, pp.\n1057–1063.\n[10] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, “Maximum margin\nplanning,” in Proceedings of the 23rd international conference on\nMachine learning.\nACM, 2006, pp. 729–736.\n[11] Q. P. Nguyen, B. K. H. Low, and P. Jaillet, “Inverse reinforcement\nlearning with locally consistent reward functions,” in Advances in\nNeural Information Processing Systems, 2015, pp. 1747–1755.\n[12] S. Levine, Z. Popovic, and V. Koltun, “Nonlinear inverse reinforcement\nlearning with gaussian processes,” in Advances in Neural Information\nProcessing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett,\nF. Pereira, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2011,\npp. 19–27.\n[13] C. Finn, S. Levine, and P. Abbeel, “Guided cost learning: Deep\ninverse optimal control via policy optimization,” arXiv preprint\narXiv:1603.00448, 2016.\n[14] J. Choi and K.-E. Kim, “Inverse reinforcement learning in partially\nobservable environments,” Journal of Machine Learning Research,\nvol. 12, no. Mar, pp. 691–730, 2011.\n[15] S. Levine and V. Koltun, “Continuous inverse optimal control with\nlocally optimal examples,” arXiv preprint arXiv:1206.4617, 2012.\n[16] M. Wulfmeier, P. Ondruska, and I. Posner, “Deep inverse reinforce-\nment learning,” arXiv preprint arXiv:1507.04888, 2015.\n[17] D. Ramachandran and E. Amir, “Bayesian inverse reinforcement\nlearning,” in Proceedings of the 20th International Joint Conference\non Artiﬁcal Intelligence, ser. IJCAI’07.\nSan Francisco, CA, USA:\nMorgan Kaufmann Publishers Inc., 2007, pp. 2586–2591.\n[18] K. Mombaur, A. Truong, and J.-P. Laumond, “From human to hu-\nmanoid locomotionan inverse optimal control approach,” Autonomous\nrobots, vol. 28, no. 3, pp. 369–383, 2010.\n[19] C. Dimitrakakis and C. A. Rothkopf, “Bayesian multitask inverse\nreinforcement learning,” in European Workshop on Reinforcement\nLearning.\nSpringer, 2011, pp. 273–284.\n[20] J. Choi and K.-E. Kim, “Nonparametric bayesian inverse reinforce-\nment learning for multiple reward functions,” in Advances in Neural\nInformation Processing Systems, 2012, pp. 305–313.\n[21] A. Boularias, J. Kober, and J. R. Peters, “Relative entropy inverse\nreinforcement learning,” in International Conference on Artiﬁcial\nIntelligence and Statistics, 2011, pp. 182–189.\n[22] E. Todorov, “Linearly-solvable markov decision problems,” in Pro-\nceedings of the 19th International Conference on Neural Information\nProcessing Systems.\nMIT Press, 2006, pp. 1369–1376.\n[23] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press Cambridge, 1998, vol. 1, no. 1.\n[24] K. Li and J. W. Burdick, “Bellman Gradient Iteration for Inverse\nReinforcement Learning,” ArXiv e-prints, Jul. 2017.\n[25] E. Todorov, “Linearly-solvable markov decision problems,” in Ad-\nvances in neural information processing systems, 2007, pp. 1369–\n1376.\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2017-08-23",
  "updated": "2017-08-23"
}