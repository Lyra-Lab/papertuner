{
  "id": "http://arxiv.org/abs/1708.08062v2",
  "title": "Cross-view Asymmetric Metric Learning for Unsupervised Person Re-identification",
  "authors": [
    "Hong-Xing Yu",
    "Ancong Wu",
    "Wei-Shi Zheng"
  ],
  "abstract": "While metric learning is important for Person re-identification (RE-ID), a\nsignificant problem in visual surveillance for cross-view pedestrian matching,\nexisting metric models for RE-ID are mostly based on supervised learning that\nrequires quantities of labeled samples in all pairs of camera views for\ntraining. However, this limits their scalabilities to realistic applications,\nin which a large amount of data over multiple disjoint camera views is\navailable but not labelled. To overcome the problem, we propose unsupervised\nasymmetric metric learning for unsupervised RE-ID. Our model aims to learn an\nasymmetric metric, i.e., specific projection for each view, based on asymmetric\nclustering on cross-view person images. Our model finds a shared space where\nview-specific bias is alleviated and thus better matching performance can be\nachieved. Extensive experiments have been conducted on a baseline and five\nlarge-scale RE-ID datasets to demonstrate the effectiveness of the proposed\nmodel. Through the comparison, we show that our model works much more suitable\nfor unsupervised RE-ID compared to classical unsupervised metric learning\nmodels. We also compare with existing unsupervised RE-ID methods, and our model\noutperforms them with notable margins. Specifically, we report the results on\nlarge-scale unlabelled RE-ID dataset, which is important but unfortunately less\nconcerned in literatures.",
  "text": "Cross-view Asymmetric Metric Learning for\nUnsupervised Person Re-identiﬁcation\nHong-Xing Yu, Ancong Wu, Wei-Shi Zheng\nCode is available at the project page:\nhttps://github.com/KovenYu/CAMEL\nFor reference of this work, please cite:\nHong-Xing Yu, Ancong Wu, Wei-Shi Zheng. “Cross-view Asymmetric\nMetric Learning for Unsupervised Person Re-identiﬁcation.” Proceedings\nof the IEEE International Conference on Computer Vision. 2017.\nBib:\n@inproceedings{yu2017cross,\ntitle={Cross-view Asymmetric Metric Learning for Unsupervised Person\nRe-identiﬁcation},\nauthor={Yu, Hong-Xing and Wu, Ancong and Zheng, Wei-Shi},\nbooktitle={Proceedings of the IEEE International Conference on Computer\nVision},\nyear={2017}\n}\narXiv:1708.08062v2  [cs.CV]  18 Oct 2017\nCross-view Asymmetric Metric Learning for\nUnsupervised Person Re-identiﬁcation\nHong-Xing Yu1,5 , Ancong Wu2 , and Wei-Shi Zheng1,3,4∗\n1School of Data and Computer Science, Sun Yat-sen University, China\n2School of Electronics and Information Technology, Sun Yat-sen University, China\n3Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\n4Collaborative Innovation Center of High Performance Computing, NUDT, China\n5Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China\nxKoven@gmail.com, wuancong@mail2.sysu.edu.cn, wszheng@ieee.org\nAbstract\nWhile metric learning is important for Person re-\nidentiﬁcation (RE-ID), a signiﬁcant problem in visual\nsurveillance for cross-view pedestrian matching, existing\nmetric models for RE-ID are mostly based on supervised\nlearning that requires quantities of labeled samples in all\npairs of camera views for training. However, this limits\ntheir scalabilities to realistic applications, in which a large\namount of data over multiple disjoint camera views is avail-\nable but not labelled. To overcome the problem, we propose\nunsupervised asymmetric metric learning for unsupervised\nRE-ID. Our model aims to learn an asymmetric metric, i.e.,\nspeciﬁc projection for each view, based on asymmetric clus-\ntering on cross-view person images.\nOur model ﬁnds a\nshared space where view-speciﬁc bias is alleviated and thus\nbetter matching performance can be achieved. Extensive\nexperiments have been conducted on a baseline and ﬁve\nlarge-scale RE-ID datasets to demonstrate the effectiveness\nof the proposed model. Through the comparison, we show\nthat our model works much more suitable for unsupervised\nRE-ID compared to classical unsupervised metric learning\nmodels. We also compare with existing unsupervised RE-\nID methods, and our model outperforms them with notable\nmargins. Speciﬁcally, we report the results on large-scale\nunlabelled RE-ID dataset, which is important but unfortu-\nnately less concerned in literatures.\n1. Introduction\nPerson re-identiﬁcation (RE-ID) is a challenging prob-\nlem focusing on pedestrian matching and ranking across\nnon-overlapping camera views. It remains an open problem\nalthough it has received considerable exploration recently,\nin consideration of its potential signiﬁcance in security ap-\nplications, especially in the case of video surveillance. It\nhas not been solved yet principally because of the dramatic\nintra-class variation and the high inter-class similarity. Ex-\nisting attempts mainly focus on learning to extract robust\nand discriminative representations [33, 23, 19], and learning\nmatching functions or metrics [38, 14, 18, 22, 19, 20, 26]\nin a supervised manner. Recently, deep learning has been\nadopted to RE-ID community [1, 32, 28, 27] and has gained\npromising results.\nHowever, supervised strategies are intrinsically limited\ndue to the requirement of manually labeled cross-view train-\ning data, which is very expensive [31]. In the context of\nRE-ID, the limitation is even pronounced because (1) man-\nually labeling may not be reliable with a huge number of im-\nages to be checked across multiple camera views, and more\nimportantly (2) the astronomical cost of time and money\nis prohibitive to label the overwhelming amount of data\nacross disjoint camera views. Therefore, in reality super-\nvised methods would be restricted when applied to a new\nscenario with a huge number of unlabeled data.\nTo directly make full use of the cheap and valuable unla-\nbeled data, some existing efforts on exploring unsupervised\nstrategies [8, 35, 29, 13, 21, 24, 30, 12] have been reported,\nbut they are still not very satisfactory. One of the main rea-\nsons is that without the help of labeled data, it is rather dif-\nﬁcult to model the dramatic variances across camera views,\nsuch as the variances of illumination and occlusion condi-\ntions. Such variances lead to view-speciﬁc interference/bias\nwhich can be very disturbing in ﬁnding what is more distin-\nguishable in matching people across views (see Figure 1).\nIn particular, existing unsupervised models treat the sam-\nples from different views in the same manner, and thus the\neffects of view-speciﬁc bias could be overlooked.\nIn order to better address the problems caused by cam-\nera view changes in unsupervised RE-ID scenarios, we pro-\nClustering\nCamera-1\nCamera-2\nProjected by U1\nProjected by U2\nOriginal Space\nShared Space\nFigure 1. Illustration of view-speciﬁc interference/bias and our\nidea. Images from different cameras suffer from view-speciﬁc in-\nterference, such as occlusions in Camera-1, dull illumination in\nCamera-2, and the change of viewpoints between them. These fac-\ntors introduce bias in the original feature space, and therefore un-\nsupervised re-identiﬁcation is extremely challenging. Our model\nstructures data by clustering and learns view-speciﬁc projections\njointly, and thus ﬁnds a shared space where view-speciﬁc bias is\nalleviated and better performance can be achieved. (Best viewed\nin color)\npose a novel unsupervised RE-ID model named Clustering-\nbased Asymmetric1 MEtric Learning (CAMEL). The ideas\nbehind are on the two following considerations. First, al-\nthough conditions can vary among camera views, we as-\nsume that there should be some shared space where the data\nrepresentations are less affected by view-speciﬁc bias. By\nprojecting original data into the shared space, the distance\nbetween any pair of samples xi and xj is computed as:\nd(xi, xj) = ∥U Txi −U Txj∥2 =\np\n(xi −xj)TM(xi −xj),\n(1)\nwhere U is the transformation matrix and M = UU T.\nHowever, it can be hard for a universal transformation to\nimplicitly model the view-speciﬁc feature distortion from\ndifferent camera views, especially when we lack label in-\nformation to guide it. This motivates us to explicitly model\nthe view-speciﬁc bias. Inspired by the supervised asymmet-\nric distance model [4], we propose to embed the asymmetric\nmetric learning to our unsupervised RE-ID modelling, and\nthus modify the symmetric form in Eq. (1) to an asymmetric\none:\nd(xp\ni , xq\nj) = ∥U pTxp\ni −U qTxq\nj∥2,\n(2)\nwhere p and q are indices of camera views.\nAn asymmetric metric is more acceptable for unsuper-\nvised RE-ID scenarios as it explicitly models the variances\namong views by treating each view differently. By such\nan explicit means, we are able to better alleviate the distur-\nbances of view-speciﬁc bias.\nThe other consideration is that since we are not clear\nabout how to separate similar persons in lack of labeled\ndata, it is reasonable to pay more attention to better sep-\narating dissimilar ones. Such consideration motivates us\nto structure our data by clustering. Therefore, we develop\n1“Asymmetric” means speciﬁc transformations for each camera view.\nasymmetric metric clustering that clusters cross-view per-\nson images. By clustering together with asymmetric mod-\nelling, the data can be better characterized in the shared\nspace, contributing to better matching performance (see\nFigure 1).\nIn summary, the proposed CAMEL aims to learn view-\nspeciﬁc projection for each camera view by jointly learning\nthe asymmetric metric and seeking optimal cluster separa-\ntions. In this way, the data from different views is projected\ninto a shared space where view-speciﬁc bias is aligned to an\nextent, and thus better performance of cross-view matching\ncan be achieved.\nSo far in literatures, the unsupervised RE-ID models\nhave only been evaluated on small datasets which contain\nonly hundreds or a few thousands of images. However, in\nmore realistic scenarios we need evaluations of unsuper-\nvised methods on much larger datasets, say, consisting of\nhundreds of thousands of samples, to validate their scala-\nbilities. In our experiments, we have conducted extensive\ncomparison on datasets with their scales ranging widely.\nIn particular, we combined two existing RE-ID datasets\n[37, 36] to obtain a larger one which contains over 230,000\nsamples. Experiments on this dataset (see Sec. 4.4) show\nempirically that our model is more scalable to problems of\nlarger scales, which is more realistic and more meaningful\nfor unsupervised RE-ID models, while some existing unsu-\npervised RE-ID models are not scalable due to the expen-\nsive cost in either storage or computation.\n2. Related Work\nAt present, most existing RE-ID models are in a super-\nvised manner. They are mainly based on learning distance\nmetrics or subspace [38, 14, 18, 22, 19, 20, 26], learning\nview-invariant and discriminative features [33, 23, 19], and\ndeep learning frameworks [1, 32, 28, 27].\nHowever, all\nthese models rely on substantial labeled training data, which\nis typically required to be pair-wise for each pair of camera\nviews. Their performance depends highly on the quality\nand quantity of labeled training data. In contrast, our model\ndoes not require any labeled data and thus is free from pro-\nhibitively high cost of manually labeling and the risk of in-\ncorrect labeling.\nTo directly utilize unlabeled data for RE-ID, several un-\nsupervised RE-ID models [35, 29, 21, 13, 24] have been\nproposed.\nAll these models differ from ours in two as-\npects. On the one hand, these models do not explicitly ex-\nploit the information on view-speciﬁc bias, i.e., they treat\nfeature transformation/quantization in every distinct camera\nview in the same manner when modelling. In contrast, our\nmodel tries to learn speciﬁc transformation for each camera\nview, aiming to ﬁnd a shared space where view-speciﬁc in-\nterference can be alleviated and thus better performance can\nbe achieved. On the other hand, as for the means to learn\na metric or a transformation, existing unsupervised meth-\nods for RE-ID rarely consider clustering while we introduce\nan asymmetric metric clustering to characterize data in the\nlearned space.\nWhile the methods proposed in [4, 2, 3]\ncould also learn view-speciﬁc mappings, they are super-\nvised methods and more importantly cannot be generalized\nto handle unsupervised RE-ID.\nApart from our model, there have been some clustering-\nbased metric learning models [34, 25]. However, to our best\nknowledge, there is no such attempt in RE-ID community\nbefore. This is potentially because clustering is more sus-\nceptible to view-speciﬁc interference and thus data points\nfrom the same view are more inclined to be clustered to-\ngether, instead of those of a speciﬁc person across views.\nFortunately, by formulating asymmetric learning and fur-\nther limiting the discrepancy between view-speciﬁc trans-\nforms, this problem can be alleviated in our model. There-\nfore, our model is essentially different from these models\nnot only in formulation but also in that our model is able\nto better deal with cross-view matching problem by treating\neach view asymmetrically. We will discuss the differences\nbetween our model and the existing ones in detail in Sec.\n4.3.\n3. Methodology\n3.1. Problem Formulation\nUnder a conventional RE-ID setting, suppose we have\na surveillance camera network that consists of V camera\nviews, from each of which we have collected Np (p =\n1, · · · , V ) images and thus there are N = N1 + · · · + NV\nimages in total as training samples.\nLet X = [x1\n1, · · · , x1\nN1, · · · , xV\n1 , · · · , xV\nNV ] ∈RM×N\ndenote the training set, with each column xp\ni\n(i\n=\n1, · · · , Np; p\n=\n1, · · · , V ) corresponding to an M-\ndimensional representation of the i-th image from the p-\nth camera view.\nOur goal is to learn V mappings i.e.,\nU 1, · · · , U V , where U p ∈RM×T (p = 1, · · · , V ), cor-\nresponding to each camera view, and thus we can project\nthe original representation xp\ni from the original space RM\ninto a shared space RT in order to alleviate the view-speciﬁc\ninterference.\n3.2. Modelling\nNow we are looking for some transformations to map\nour data into a shared space where we can better separate\nthe images of one person from those of different persons.\nNaturally, this goal can be achieved by narrowing intra-\nclass discrepancy and meanwhile pulling the centers of all\nclasses away from each other. In an unsupervised scenario,\nhowever, we have no labeled data to tell our model how it\ncan exactly distinguish one person from another who has a\nconfusingly similar appearance with him. Therefore, it is\nacceptable to relax the original idea: we focus on gathering\nsimilar person images together, and hence separating rela-\ntively dissimilar ones. Such goal can be modelled by mini-\nmizing an objective function like that of k-means clustering\n[10]:\nmin\nU Fintra =\nK\nX\nk=1\nX\ni∈Ck\n∥U Txi −ck∥2,\n(3)\nwhere K is the number of clusters, ck denotes the centroid\nof the k-th cluster and Ck = {i|U Txi ∈k-th cluster}.\nHowever, clustering results may be affected extremely\nby view-speciﬁc bias when applied in cross-view problems.\nIn the context of RE-ID, the feature distortion could be\nview-sensitive due to view-speciﬁc interference like differ-\nent lighting conditions and occlusions [4]. Such interfer-\nence might be disturbing or even dominating in searching\nthe similar person images across views during clustering\nprocedure. To address this cross-view problem, we learn\nspeciﬁc projection for each view rather than a universal one\nto explicitly model the effect of view-speciﬁc interference\nand to alleviate it. Therefore, the idea can be further formu-\nlated by minimizing an objective function below:\nmin\nU1,··· ,UV Fintra =\nK\nX\nk=1\nX\ni∈Ck\n∥U pTxp\ni −ck∥2\ns.t.\nU pTΣpU p = I\n(p = 1, · · · , V ),\n(4)\nwhere the notation is similar to Eq. (3), with p denotes the\nview index, Σp = XpXpT/Np + αI and I represents the\nidentity matrix which avoids singularity of the covariance\nmatrix. The transformation U p that corresponds to each\ninstance xp\ni is determined by the camera view which xp\ni\ncomes from. The quasi-orthogonal constraints on U p en-\nsure that the model will not simply give zero matrices. By\ncombining the asymmetric metric learning, we actually re-\nalize an asymmetric metric clustering on RE-ID data across\ncamera views.\nIntuitively, if we minimize this objective function di-\nrectly, U p will largely depend on the data distribution from\nthe p-th view. Now that there is speciﬁc bias on each view,\nany U p and U q could be arbitrarily different. This result is\nvery natural, but large inconsistencies among the learned\ntransformations are not what we exactly expect, because\nthe transformations are with respect to person images from\ndifferent views: they are inherently correlated and homo-\ngeneous. More critically, largely different projection ba-\nsis pairs would fail to capture the discriminative nature of\ncross-view images, producing an even worse matching re-\nsult.\nHence, to strike a balance between the ability to capture\ndiscriminative nature and the capability to alleviate view-\nspeciﬁc bias, we embed a cross-view consistency regular-\nization term into our objective function. And then, in con-\nsideration of better tractability, we divide the intra-class\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n-1\n-0.5\n0\n0.5\n1\nView1, ID1\nView2, ID1\nView1, ID2\nView2, ID2\n(a) Original\n-1.5\n-1\n-0.5\n0\n0.5\n1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\nView1, ID1\nView2, ID1\nView1, ID2\nView2, ID2\n(b) Symmetric\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n-1\n-0.5\n0\n0.5\n1\nView1, ID1\nView2, ID1\nView1, ID2\nView2, ID2\n(c) Asymmetric\nFigure 2. Illustration of how symmetric and asymmetric metric\nclustering structure data using our method for the unsupervised\nRE-ID problem. The samples are from the SYSU dataset [4]. We\nperformed PCA for visualization. One shape (triangle or circle)\nstands for samples from one view, while one color indicates sam-\nples of one person. (a) Original distribution (b) distribution in the\ncommon space learned by symmetric metric clustering (c) distri-\nbution in the shared space learned by asymmetric metric cluster-\ning. (Best viewed in color)\nterm by its scale N, so that the regulating parameter would\nnot be sensitive to the number of training samples. Thus,\nour optimization task becomes\nmin\nU1,··· ,UV Fobj = 1\nN Fintra + λFconsistency\n= 1\nN\nK\nX\nk=1\nX\ni∈Ck\n∥U pTxp\ni −ck∥2 + λ\nX\np̸=q\n∥U p −U q∥2\nF\ns.t.\nU pTΣpU p = I\n(p = 1, · · · , V ),\n(5)\nwhere λ is the cross-view regularizer and ∥·∥F denotes the\nFrobenius norm of a matrix. We call the above model the\nClustering-based Asymmetric MEtric Learning (CAMEL).\nTo illustrate the differences between symmetric and\nasymmetric metric clustering in structuring data in the RE-\nID problem, we further show the data distributions in Figure\n2. We can observe from Figure 2 that the view-speciﬁc bias\nis obvious in the original space: triangles in the upper left\nand circles in the lower right. In the common space learned\nby symmetric metric clustering, the bias is still obvious. In\ncontrast, in the shared space learned by asymmetric metric\nclustering, the bias is alleviated and thus the data is bet-\nter characterized according to the identities of the persons,\ni.e., samples of one person (one color) gather together into\na cluster.\n3.3. Optimization\nFor convenience, we denote yi = U pTxp\ni . Then we have\nY ∈RT ×N, where each column yi corresponds to the pro-\njected new representation of that from X. For optimization,\nwe rewrite our objective function in a more compact form.\nThe ﬁrst term can be rewritten as follow [6]:\n1\nN\nK\nX\nk=1\nX\ni∈Ck\n∥yi −ck∥2 = 1\nN [Tr(Y TY ) −Tr(HTY TY H)],\n(6)\nwhere\nH =\n\u0002\nh1, ..., hK\n\u0003\n,\nhT\nk hl =\n(\n0\nk ̸= l\n1\nk = l\n(7)\nhk =\n\u0002\n0, · · · , 0, 1, · · · , 1, 0, · · · , 0, 1, · · ·\n\u0003T /√nk\n(8)\nis an indicator vector with the i-th entry corresponding to\nthe instance yi, indicating that yi is in the k-th cluster if the\ncorresponding entry does not equal zero. Then we construct\nf\nX =\n\n\nx1\n1\n· · ·\nx1\nN1\n0\n· · ·\n0\n· · ·\n0\n0\n· · ·\n0\nx2\n1\n· · ·\nx2\nN2\n· · ·\n0\n...\n...\n...\n...\n...\n...\n...\n...\n0\n· · ·\n0\n0\n· · ·\n0\n· · ·\nxV\nNV\n\n\n(9)\neU =\n\u0002\nU 1T, · · · , U V T\u0003T ,\n(10)\nso that\nY = eU T f\nX,\n(11)\nand thus Eq. (6) becomes\n1\nN\nK\nX\nk=1\nX\ni∈Ck\n∥yi −ck∥2\n= 1\nN Tr(f\nXT eU eU T f\nX) −1\nN Tr(HT f\nXT eU eU T f\nXH).\n(12)\nAs for the second term, we can also rewrite it as follow:\nλ\nX\np̸=q\n∥U p −U q∥2\nF = λTr( eU TD eU),\n(13)\nwhere\nD =\n\n\n(V −1)I\n−I\n−I\n· · ·\n−I\n−I\n(V −1)I\n−I\n· · ·\n−I\n...\n...\n...\n...\n...\n−I\n−I\n−I\n· · ·\n(V −1)I\n\n. (14)\nThen, it is reasonable to relax the constraints\nU pTΣpU p = I\n(p = 1, · · · , V )\n(15)\nto\nV\nX\np=1\nU pTΣpU p = eU T eΣ eU = V I,\n(16)\nwhere eΣ = diag(Σ1, · · · , ΣV ) because what we expect\nis to prevent each U p from shrinking to a zero matrix. The\nrelaxed version of constraints is able to satisfy our need, and\nit bypasses trivial computations.\nBy now we can rewrite our optimization task as follow:\nmin\ne\nU\nFobj = 1\nN Tr(f\nXT eU eU T f\nX) + λTr( eU TD eU)\n−1\nN Tr(HT f\nXT eU eU T f\nXH)\ns.t.\neU T eΣ eU = V I.\n(17)\nIt is easy to realize from Eq. (5) that our objective func-\ntion is highly non-linear and non-convex. Fortunately, in\nthe form of Eq. (17) we can ﬁnd that once H is ﬁxed, La-\ngrange’s method can be applied to our optimization task.\nAnd again from Eq. (5), it is exactly the objective of k-\nmeans clustering once eU is ﬁxed [10]. Thus, we can adopt\nan alternating algorithm to solve the optimization problem.\nFix H and optimize eU. Now we see how we optimize\neU. After ﬁxing H and applying the method of Lagrange\nmultiplier, our optimization task (17) is transformed into an\neigen-decomposition problem as follow:\nGu = γu,\n(18)\nwhere γ is the Lagrange multiplier (and also is the eigen-\nvalue here) and\nG = eΣ−1(λD + 1\nN\nf\nX f\nXT −1\nN\nf\nXHHT f\nXT).\n(19)\nThen,\neU\ncan\nbe\nobtained\nby\nsolving\nthis\neigen-\ndecomposition problem.\nFix eU and optimize H. As for the optimization of H,\nwe can simply ﬁx eU and conduct k-means clustering in the\nlearned space. Each column of H, hk, is thus constructed\naccording to the clustering result.\nBased on the analysis above, we can now propose the\nmain algorithm of CAMEL in Algorithm 1. We set maxi-\nmum iteration to 100. After obtaining eU, we decompose it\nback into {U 1, · · · , U V }. The algorithm is guaranteed to\nconvergence, as given in the following proposition:\nProposition 1. In Algorithm 1, Fobj is guaranteed to con-\nvergence.\nProof. In each iteration, when eU is ﬁxed, if H is the lo-\ncal minimizer, k-means remains H unchanged, otherwise\nit seeks the local minimizer. When H is ﬁxed, eU has a\nclosed-form solution which is the global minimizer. There-\nfore, the Fobj decreases step by step. As Fobj ≥0 has a\nlower bound 0, it is guaranteed to convergence.\n4. Experiments\n4.1. Datasets\nSince unsupervised models are more meaningful when\nthe scale of problem is larger, our experiments were con-\nducted on relatively big datasets except VIPeR [9] which\nis small but widely used. Various degrees of view-speciﬁc\nbias can be observed in all these datasets (see Figure 3).\nThe VIPeR dataset contains 632 identities, with two im-\nages captured from two camera views of each identity.\nThe CUHK01 dataset [16] contains 3,884 images of 971\nidentities captured from two disjoint views. There are two\nimages of every identity from each view.\nAlgorithm 1: CAMEL\nInput : e\nX, K, ϵ = 10−8\nOutput: e\nU\n1 Conduct k-means clustering with respect to each column of e\nX to initialize\nH according to Eq. (7) and (8).\n2 Fix H and solve the eigen-decomposition problem described by Eq. (18) and\n(19) to construct e\nU.\n3 while decrement of Fobj > ϵ & maximum iteration unreached do\n• Construct Y according to Eq. (11).\n• Fix e\nU and conduct k-means clustering with respect to each column\nof Y to update H according to Eq. (7) and (8).\n• Fix H and solve the eigen-decomposition problem described by\nEq. (18) and (19) to update e\nU.\n4 end\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 3. Samples of the datasets. Every two images in a column\nare from one identity across two disjoint camera views. (a) VIPeR\n(b) CUHK01 (c) CUHK03 (d) SYSU (e) Market (f) ExMarket.\n(Best viewed in color)\nDataset\nVIPeR\nCUHK01 CUHK03\nSYSU\nMarket\nExMarket\n# Samples\n1,264\n3,884\n13,164\n24,448\n32,668\n236,696\n# Views\n2\n2\n6\n2\n6\n6\nTable 1. Overview of dataset scales. “#” means “the number of”.\nThe CUHK03 dataset [17] contains 13,164 images of\n1,360 pedestrians captured from six surveillance camera\nviews. Besides hand-cropped images, samples detected by\na state-of-the-art pedestrian detector are provided.\nThe SYSU dataset [4] includes 24,448 RGB images of 502\npersons under two surveillance cameras. One camera view\nmainly captured the frontal or back views of persons, while\nthe other observed mostly the side views.\nThe Market-1501 dataset [37] (Market) contains 32,668\nimages of 1,501 pedestrians, each of which was captured\nby at most six cameras. All of the images were cropped by\na pedestrian detector. There are some bad-detected samples\nin this datasets as distractors as well.\nThe ExMarket dataset2. In order to evaluate unsupervised\nRE-ID methods on even larger scale, which is more real-\nistic, we further combined the MARS dataset [36] with\nMarket. MARS is a video-based RE-ID dataset which con-\n2Demo code for the model and the ExMarket dataset can be found on\nhttps://github.com/KovenYu/CAMEL.\ntains 20,715 tracklets of 1,261 pedestrians. All the identities\nfrom MARS are of a subset of those from Market. We then\ntook 20% frames (each one in every ﬁve successive frames)\nfrom the tracklets and combined them with Market to obtain\nan extended version of Market (ExMarket). The imbalance\nbetween the numbers of samples from the 1,261 persons and\nother 240 persons makes this dataset more challenging and\nrealistic. There are 236,696 images in ExMarket in total,\nand 112,351 images of them are of training set. A brief\noverview of the dataset scales can be found in Table 1.\n4.2. Settings\nExperimental protocols: A widely adopted protocol was\nfollowed on VIPeR in our experiments [19], i.e., randomly\ndividing the 632 pairs of images into two halves, one of\nwhich was used as training set and the other as testing set.\nThis procedure was repeated 10 times to offer average per-\nformance. Only single-shot experiments were conducted.\nThe experimental protocol for CUHK01 was the same as\nthat in [19]. We randomly selected 485 persons as train-\ning set and the other 486 ones as testing set. The evaluat-\ning procedure was repeated 10 times. Both multi-shot and\nsingle-shot settings were conducted.\nThe CUHK03 dataset was provided together with its rec-\nommended evaluating protocol [17]. We followed the pro-\nvided protocol, where images of 1,160 persons were chosen\nas training set, images of another 100 persons as validation\nset and the remainders as testing set. This procedure was re-\npeated 20 times. In our experiments, detected samples were\nadopted since they are closer to real-world settings. Both\nmulti-shot and single-shot experiments were conducted.\nAs for the SYSU dataset, we randomly picked 251\npedestrians’ images as training set and the others as testing\nset. In the testing stage, we basically followed the protocol\nas in [4]. That is, we randomly chose one and three images\nof each pedestrian as gallery for single-shot and multi-shot\nexperiments, respectively. We repeated the testing proce-\ndure by 10 times.\nMarket is somewhat different from others. The evalua-\ntion protocol was also provided along with the data [37].\nSince the images of one person came from at most six\nviews, single-shot experiments were not suitable. Instead,\nmulti-shot experiments were conducted and both cumula-\ntive matching characteristic (CMC) and mean average pre-\ncision (MAP) were adopted for evaluation [37]. The pro-\ntocol of ExMarket was identical to that of Market since the\nidentities were completely the same as we mentioned above.\nData representation: In our experiments we used the deep-\nlearning-based JSTL feature proposed in [32]. We imple-\nmented it using the 56-layer ResNet [11], which produced\n64-D features. The original JSTL was adopted to our imple-\nmentation to extract features on SYSU, Market and ExMar-\nket. Note that the training set of the original JSTL contained\nVIPeR, CUHK01 and CUHK03, violating the unsupervised\nsetting. So we trained a new JSTL model without VIPeR\nin its training set to extract features on VIPeR. The similar\nprocedures were done for CUHK01 and CUHK03.\nParameters: We set λ, the cross-view consistency regular-\nizer, to 0.01. We also evaluated the situation when λ goes\nto inﬁnite, i.e., the symmetric version of our model in Sec.\n4.4, to show how important the asymmetric modelling is.\nRegarding the parameter T which is the feature dimen-\nsion after the transformation learned by CAMEL, we set T\nequal to original feature dimension i.e., 64, for simplicity.\nIn our experiments, we found that CAMEL can align data\ndistributions across camera views even without performing\nany further dimension reduction. This may be due to the\nfact that, unlike conventional subspace learning models, the\ntransformations learned by CAMEL are view-speciﬁc for\ndifferent camera views and always non-orthogonal. Hence,\nthe learned view-speciﬁc transformations can already re-\nduce the discrepancy between the data distributions of dif-\nferent camera views.\nAs for K, we found that our model was not sensitive to\nK when N ≫K and K was not too small (see Sec. 4.4),\nso we set K = 500. These parameters were ﬁxed for all\ndatasets.\n4.3. Comparison\nUnsupervised models are more signiﬁcant when applied\non larger datasets. In order to make comprehensive and\nfair comparisons, in this section we compare CAMEL with\nthe most comparable unsupervised models on six datasets\nwith their scale orders varying from hundreds to hundreds\nof thousands. We show the comparative results measured\nby the rank-1 accuracies of CMC and MAP (%) in Table 2.\nComparison to Related Unsupervised RE-ID Models. In\nthis subsection we compare CAMEL with the sparse dictio-\nnary learning model (denoted as Dic) [13], sparse represen-\ntation learning model ISR [21], kernel subspace learning\nmodel RKSL [30] and sparse auto-encoder (SAE) [15, 5].\nWe tried several sets of parameters for them, and report the\nbest ones. We also adopt the Euclidean distance which is\nadopted in the original JSTL paper [32] as a baseline (de-\nnoted as JSTL).\nFrom Table 2 we can observe that CAMEL outperforms\nother models on all the datasets on both settings. In addi-\ntion, we can further see from Figure 4 that CAMEL outper-\nforms other models at any rank. One of the main reasons\nis that the view-speciﬁc interference is noticeable in these\ndatasets. For example, we can see in Figure 3(b) that on\nCUHK01, the changes of illumination are extremely severe\nand even human beings may have difﬁculties in recognizing\nthe identities in those images across views. This impedes\nother symmetric models from achieving higher accuracies,\nbecause they potentially hold an assumption that the invari-\nDataset\nVIPeR\nCUHK01 CUHK03\nSYSU\nMarket\nExMarket\nSetting\nSS\nSS/MS\nSS/MS\nSS/MS\nMS\nMS\nDic [13]\n29.9\n49.3/52.9 27.4/36.5\n21.3/28.6\n50.2(22.7) 52.2(21.2)\nISR [21]\n27.5\n53.2/55.7 31.1/38.5\n23.2/33.8\n40.3(14.3)\n-\nRKSL [30]\n25.8\n45.4/50.1 25.8/34.8\n17.6/23.0\n34.0(11.0)\n-\nSAE [15]\n20.7\n45.3/49.9 21.2/30.5\n18.0/24.2\n42.4(16.2) 44.0(15.1)\nJSTL [32]\n25.7\n46.3/50.6 24.7/33.2\n19.9/25.6\n44.7(18.4) 46.4(16.7)\nAML [34]\n23.1\n46.8/51.1 22.2/31.4\n20.9/26.4\n44.7(18.4) 46.2(16.2)\nUsNCA [25]\n24.3\n47.0/51.7 19.8/29.6\n21.1/27.2\n45.2(18.9)\n-\nCAMEL\n30.9\n57.3/61.9 31.9/39.4\n30.8/36.8\n54.5(26.3) 55.9(23.9)\nTable 2. Comparative results of unsupervised models on the six\ndatasets, measured by rank-1 accuracies and MAP (%). “-” means\nprohibitive time consumption due to time complexities of the\nmodels. “SS” represents single-shot setting and “MS” represents\nmulti-shot setting. For Market and ExMarket, MAP is also pro-\nvided in the parentheses due to the requirement in the protocol\n[37]. Such a format is also applied in the following tables.\nant and discriminative information can be retained and ex-\nploited through a universal transformation for all views. But\nCAMEL relaxes this assumption by learning an asymmetric\nmetric and then can outperform other models signiﬁcantly.\nIn Sec. 4.4 we will see the performance of CAMEL would\ndrop much when it degrades to a symmetric model.\nComparison to Clustering-based Metric Learning Mod-\nels. In this subsection we compare CAMEL with a typical\nmodel AML [34] and a recently proposed model UsNCA\n[25]. We can see from Fig. 4 and Table 2 that compared\nto them, CAMEL achieves noticeable improvements on all\nthe six datasets. One of the major reasons is that they do not\nconsider the view-speciﬁc bias which can be very disturb-\ning in clustering, making them unsuitable for RE-ID prob-\nlem. In comparison, CAMEL alleviates such disturbances\nby asymmetrically modelling. This factor contributes to the\nmuch better performance of CAMEL.\nComparison to the State-of-the-Art.\nIn the last sub-\nsections, we compared with existing unsupervised RE-ID\nmethods using the same features. In this part, we also com-\npare with the results reported in literatures. Note that most\nexisting unsupervised RE-ID methods have not been eval-\nuated on large datasets like CUHK03, SYSU, or Market,\nso Table 3 only reports the comparative results on VIPeR\nand CUHK01. We additionally compared existing unsuper-\nvised RE-ID models, including the hand-craft-feature-based\nSDALF [8] and CPS [7], the transfer-learning-based UDML\n[24], graph-learning-based model (denoted as GL) [12], and\nlocal-salience-learning-based GTS [29] and SDC [35]. We\ncan observe from Table 3 that our model CAMEL can out-\nperform the state-of-the-art by large margins on CUHK01.\nComparison to Supervised Models. Finally, in order to\nsee how well CAMEL can approximate the performance of\nsupervised RE-ID, we additionally compare CAMEL with\nits supervised version (denoted as CAMELs) which is easily\nRank\n5\n10\n15\n20\nMatching Accuracy (%)\n20\n30\n40\n50\n60\n70\nDIC\nISR\nUsNCA\nAML\nSAE\nRKSL\nJSTL\nCAMEL\n(a) VIPeR\nRank\n5\n10\n15\n20\nMatching Accuracy (%)\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\nDIC\nISR\nUsNCA\nAML\nSAE\nRKSL\nJSTL\nCAMEL\n(b) CUHK01\nRank\n5\n10\n15\n20\nMatching Accuracy (%)\n20\n30\n40\n50\n60\n70\n80\nDIC\nISR\nUsNCA\nAML\nSAE\nRKSL\nJSTL\nCAMEL\n(c) CUHK03\nRank\n5\n10\n15\n20\nMatching Accuracy (%)\n20\n30\n40\n50\n60\n70\nDIC\nISR\nUsNCA\nAML\nSAE\nRKSL\nJSTL\nCAMEL\n(d) SYSU\nRank\n5\n10\n15\n20\nMatching Accuracy (%)\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\nDIC\nISR\nUsNCA\nAML\nSAE\nRKSL\nJSTL\nCAMEL\n(e) Market\nRank\n5\n10\n15\n20\nMatching Accuracy (%)\n45\n50\n55\n60\n65\n70\n75\n80\nDIC\nAML\nSAE\nJSTL\nCAMEL\n(f) ExMarket\nFigure 4. CMC curves. For CUHK01, CUHK03 and SYSU, we\ntake the results under single-shot setting as examples. Similar pat-\nterns can be observed on multi-shot setting.\nModel\nSDALF\nCPS\nUDML\nGL\nGTS\nSDC\nCAMEL\n[8]\n[7]\n[24]\n[12]\n[29]\n[35]\nVIPeR\n19.9\n22.0\n31.5\n33.5\n25.2\n25.8\n30.9\nCUHK01\n9.9\n-\n27.1\n41.0\n-\n26.6\n57.3\nTable 3. Results compared to the state-of-the-art reported in liter-\natures, measured by rank-1 accuracies (%). “-” means no reported\nresult.\nderived by substituting the clustering results by true labels,\nand three standard supervised models, including the widely\nused KISSME [14], XQDA [19], the asymmetric distance\nmodel CVDCA [4]. The results are shown in Table 4. We\ncan see that CAMELs outperforms CAMEL by various de-\ngrees, indicating that label information can further improve\nCAMEL’s performance. Also from Table 4, we notice that\nCAMEL can be comparable to other standard supervised\nmodels on some datasets like CUHK01, and even outper-\nform some of them. It is probably because the used JSTL\nmodel had not been ﬁne-tuned on the target datasets: this\nwas for a fair comparison with unsupervised models which\nwork on completely unlabelled training data. Nevertheless,\nthis suggests that the performance of CAMEL may not be\nDataset\nVIPeR\nCUHK01\nCUHK03\nSYSU\nMarket\nExMarket\nSetting\nSS\nSS/MS\nSS/MS\nSS/MS\nMS\nMS\nKISSME [14]\n28.4\n53.0/57.1\n37.8/45.4\n24.7/31.8\n51.1(24.5)\n48.0(18.3)\nXQDA [19]\n28.9\n54.3/58.2\n36.7/43.7\n25.2/31.7\n50.8(24.4)\n47.4(18.1)\nCVDCA [4]\n37.6\n57.1/60.9\n37.0/44.6\n31.1/38.9\n52.6(25.3)\n51.5(22.6)\nCAMELs\n33.7\n58.5/62.7\n45.1/53.5\n31.6/37.6\n55.0(27.1)\n56.1(24.1)\nCAMEL\n30.9\n57.3/61.9\n31.9/39.4\n30.8/36.8\n54.5(26.3)\n55.9(23.9)\nTable 4. Results compared to supervised models using the same\nJSTL features.\nfar below the standard supervised RE-ID models.\n4.4. Further Evaluations\nThe Role of Asymmetric Modeling. We show what is go-\ning to happen if CAMEL degrades to a common symmet-\nric model in Table 5. Apparently, without asymmetrically\nmodelling each camera view, our model would be worsen\nlargely, indicating that the asymmetric modeling for cluster-\ning is rather important for addressing the cross-view match-\ning problem in RE-ID as well as in our model.\nSensitivity to the Number of Clustering Centroids. We\ntake CUHK01, Market and ExMarket datasets as examples\nof different scales (see Table 1) for this evaluation. Table 6\nshows how the performance varies with different numbers\nof clustering centroids, K. It is obvious that the perfor-\nmance only ﬂuctuates mildly when N ≫K and K is not\ntoo small. Therefore CAMEL is not very sensitive to K es-\npecially when applied to large-scale problems. To further\nexplore the reason behind, we show in Table 7 the rate of\nclusters which contain more than one persons, in the initial\nstage and convergence stage in Algorithm 1. We can see\nthat (1) in spite of that K is varying, there is always a num-\nber of clusters containing more than one persons in both\nthe initial stage and convergence stage. This indicates that\nour model works without the requirement of perfect clus-\ntering results. And (2), although the number is various, in\nthe convergence stage the number is consistently decreased\ncompared to initialization stage. This shows that the cluster\nresults are improved consistently. These two observations\nsuggests that the clustering should be a mean to learn the\nasymmetric metric, rather than an ultimate objective.\nAdaptation Ability to Different Features.\nAt last, we\nshow that CAMEL can be effective not only when adopt-\ning deep-learning-based JSTL features.\nWe additionally\nadopted the hand-crafted LOMO feature proposed in [19].\nWe performed PCA to produce 512-D LOMO features, and\nthe results are shown in Table 8. Among all the models, the\nresults of Dic and ISR are the most comparable (Dic and\nISR take all second places). So for clarity, we only compare\nCAMEL with them and L2 distance as baseline. From the\ntable we can see that CAMEL can outperform them.\nDataset\nVIPeR\nCUHK01\nCUHK03\nSYSU\nMarket\nExMarket\nSetting\nSS\nSS/MS\nSS/MS\nSS/MS\nMS\nMS\nCMEL\n27.5\n52.5/54.9\n29.8/37.5\n25.4/30.9\n47.6(21.5)\n48.7(20.0)\nCAMEL\n30.9\n57.3/61.9\n31.9/39.4\n30.8/36.8\n54.5(26.3)\n55.9(23.9)\nTable 5. Performances of CAMEL compared to its symmetric ver-\nsion, denoted as CMEL.\nK\n250\n500\n750\n1000\n1250\nCUHK01\n56.59\n57.35\n56.26\n55.12\n52.75\nMarket\n54.48\n54.45\n54.54\n54.48\n54.48\nExMarket\n55.49\n55.87\n56.17\n55.93\n55.67\nTable 6. Performances of CAMEL when the number of clusters,\nK, varies.\nMeasured by single-shot rank-1 accuracies (%) for\nCUHK01 and multi-shot for Market and ExMarket.\nK\n250\n500\n750\n1000\n1250\nInitial Stage\n77.6%\n57.0%\n26.3%\n11.6%\n6.0%\nConvergence Stage\n55.8%\n34.3%\n18.2%\n7.2%\n4.8%\nTable 7. Rate of clusters containing similar persons on CUHK01.\nSimilar trend can be observed on other datasets.\nDataset\nVIPeR\nCUHK01 CUHK03\nSYSU\nMarket\nExMarket\nSetting\nSS\nSS/MS\nSS/MS\nSS/MS\nMS\nMS\nDic [13]\n15.8\n19.6/23.6\n8.6/13.4\n14.2/24.4\n32.8(12.2) 33.8(12.2)\nISR [21]\n20.8\n22.2/27.1 16.7/20.7\n11.7/21.6\n29.7(11.0)\n-\nL2\n11.6\n14.0/18.6\n7.6/11.6\n10.8/18.9\n27.4(8.3)\n27.7(8.0)\nCAMEL\n26.4\n30.0/36.2 17.3/23.4\n23.6/35.6\n41.4(14.1) 42.2(13.7)\nTable 8. Results using 512-D LOMO features.\n5. Conclusion\nIn this work, we have shown that metric learning can be\neffective for unsupervised RE-ID by proposing clustering-\nbased asymmetric metric learning called CAMEL. CAMEL\nlearns view-speciﬁc projections to deal with view-speciﬁc\ninterference, and this is based on existing clustering (e.g.,\nthe k-means model demonstrated in this work) on RE-ID\nunlabelled data, resulting in an asymmetric metric cluster-\ning. Extensive experiments show that our model can out-\nperform existing ones in general, especially on large-scale\nunlabelled RE-ID datasets.\nAcknowledgement\nThis\nwork\nwas\nsupported\npartially\nby\nthe\nNa-\ntional\nKey\nResearch\nand\nDevelopment\nProgram\nof\nChina (2016YFB1001002), NSFC(61522115, 61472456,\n61573387, 61661130157, U1611461), the Royal Society\nNewton Advanced Fellowship (NA150459), Guangdong\nProvince Science and Technology Innovation Leading Tal-\nents (2016TX03X157).\nReferences\n[1] E. Ahmed, M. Jones, and T. K. Marks. An improved deep\nlearning architecture for person re-identiﬁcation. In CVPR,\n2015.\n[2] L. An, M. Kafai, S. Yang, and B. Bhanu. Reference-based\nperson re-identiﬁcation. In AVSS, 2013.\n[3] L. An, M. Kafai, S. Yang, and B. Bhanu.\nPerson re-\nidentiﬁcation with reference descriptor. TCSVT, 2015.\n[4] Y.-C. Chen, W.-S. Zheng, J.-H. Lai, and P. Yuen. An asym-\nmetric distance model for cross-view feature mapping in per-\nson re-identiﬁcation. TCSVT, 2015.\n[5] A. Coates, H. Lee, and A. Y. Ng. An analysis of single-layer\nnetworks in unsupervised feature learning. Ann Arbor, 2010.\n[6] C. H. Q. Ding and X. He. On the equivalence of nonnegative\nmatrix factorization and spectral clustering. In ICDM, 2005.\n[7] S. C. Dong, M. Cristani, M. Stoppa, L. Bazzani, and\nV. Murino. Custom pictorial structures for re-identiﬁcation.\nIn BMVC, 2011.\n[8] M. Farenzena, L. Bazzani, A. Perina, V. Murino, and\nM. Cristani. Person re-identiﬁcation by symmetry-driven ac-\ncumulation of local features. In CVPR, 2010.\n[9] D. Gray, S. Brennan, and H. Tao.\nEvaluating appearance\nmodels for recognition, reacquisition, and tracking. In PETS,\n2007.\n[10] J. A. Hartigan. Clustering algorithms. John Wiley & Sons\nInc, 1975.\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016.\n[12] E. Kodirov, T. Xiang, Z. Fu, and S. Gong.\nPerson re-\nidentiﬁcation by unsupervised\\ ell 1 graph learning.\nIn\nECCV, 2016.\n[13] E. Kodirov, T. Xiang, and S. Gong. Dictionary learning with\niterative laplacian regularisation for unsupervised person re-\nidentiﬁcation. In BMVC, 2015.\n[14] M. K¨ostinger, M. Hirzer, P. Wohlhart, P. M. Roth, and\nH. Bischof. Large scale metric learning from equivalence\nconstraints. In CVPR, 2012.\n[15] H. Lee, C. Ekanadham, and A. Y. Ng. Sparse deep belief net\nmodel for visual area v2. In NIPS, 2008.\n[16] W. Li, R. Zhao, and X. Wang. Human reidentiﬁcation with\ntransferred metric learning. In ACCV, 2012.\n[17] W. Li, R. Zhao, T. Xiao, and X. Wang. Deepreid: Deep ﬁlter\npairing neural network for person re-identiﬁcation. In CVPR,\n2014.\n[18] Z. Li, S. Chang, F. Liang, T. S. Huang, L. Cao, and J. R.\nSmith. Learning locally-adaptive decision functions for per-\nson veriﬁcation. In CVPR, 2013.\n[19] S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identiﬁcation\nby local maximal occurrence representation and metric\nlearning. In CVPR, 2015.\n[20] S. Liao and S. Z. Li. Efﬁcient psd constrained asymmetric\nmetric learning for person re-identiﬁcation. In ICCV, 2015.\n[21] G. Lisanti, I. Masi, A. D. Bagdanov, and A. Del Bimbo. Per-\nson re-identiﬁcation by iterative re-weighted sparse ranking.\nTPAMI, 2015.\n[22] G. Lisanti, I. Masi, and A. Del Bimbo. Matching people\nacross camera views using kernel canonical correlation anal-\nysis. In ICDSC, 2014.\n[23] B. Ma, Y. Su, and F. Jurie.\nCovariance descriptor based\non bio-inspired features for person re-identiﬁcation and face\nveriﬁcation. IVC, 2014.\n[24] P. Peng, T. Xiang, Y. Wang, M. Pontil, S. Gong, T. Huang,\nand Y. Tian. Unsupervised cross-dataset transfer learning for\nperson re-identiﬁcation. In CVPR, 2016.\n[25] C. Qin, S. Song, G. Huang, and L. Zhu. Unsupervised neigh-\nborhood component analysis for clustering. Neurocomput-\ning, 2015.\n[26] Y. Shen, W. Lin, J. Yan, M. Xu, J. Wu, and J. Wang. Person\nre-identiﬁcation with correspondence structure learning. In\nICCV, 2015.\n[27] R. R. Varior, M. Haloi, and G. Wang.\nGated siamese\nconvolutional neural network architecture for human re-\nidentiﬁcation. In ECCV, 2016.\n[28] F. Wang, W. Zuo, L. Lin, D. Zhang, and L. Zhang. Joint\nlearning of single-image and cross-image representations for\nperson re-identiﬁcation. In CVPR, 2016.\n[29] H. Wang, S. Gong, and T. Xiang.\nUnsupervised learning\nof generative topic saliency for person re-identiﬁcation. In\nBMVC, 2014.\n[30] H. Wang, X. Zhu, T. Xiang, and S. Gong. Towards unsuper-\nvised open-set person re-identiﬁcation. In ICIP, 2016.\n[31] X. Wang, W. S. Zheng, X. Li, and J. Zhang. Cross-scenario\ntransfer person reidentiﬁcation. TCSVT, 2015.\n[32] T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep fea-\nture representations with domain guided dropout for person\nre-identiﬁcation. In CVPR, 2016.\n[33] Y. Yang, J. Yang, J. Yan, S. Liao, D. Yi, and S. Z. Li. Salient\ncolor names for person re-identiﬁcation. In ECCV, 2014.\n[34] J. Ye, Z. Zhao, and H. Liu. Adaptive distance metric learning\nfor clustering. In CVPR, 2007.\n[35] R. Zhao, W. Ouyang, and X. Wang. Person re-identiﬁcation\nby saliency learning. TPAMI, 2016.\n[36] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and\nQ. Tian. Mars: A video benchmark for large-scale person\nre-identiﬁcation. In ECCV, 2016.\n[37] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian.\nScalable person re-identiﬁcation: A benchmark. In ICCV,\n2015.\n[38] W.-S. Zheng, S. Gong, and T. Xiang. Person re-identiﬁcation\nby probabilistic relative distance comparison.\nIn CVPR,\n2011.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-08-27",
  "updated": "2017-10-18"
}