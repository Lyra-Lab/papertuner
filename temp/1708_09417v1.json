{
  "id": "http://arxiv.org/abs/1708.09417v1",
  "title": "LangPro: Natural Language Theorem Prover",
  "authors": [
    "Lasha Abzianidze"
  ],
  "abstract": "LangPro is an automated theorem prover for natural language\n(https://github.com/kovvalsky/LangPro). Given a set of premises and a\nhypothesis, it is able to prove semantic relations between them. The prover is\nbased on a version of analytic tableau method specially designed for natural\nlogic. The proof procedure operates on logical forms that preserve linguistic\nexpressions to a large extent. %This property makes the logical forms easily\nobtainable from syntactic trees. %, in particular, Combinatory Categorial\nGrammar derivation trees. The nature of proofs is deductive and transparent. On\nthe FraCaS and SICK textual entailment datasets, the prover achieves high\nresults comparable to state-of-the-art.",
  "text": "arXiv:1708.09417v1  [cs.CL]  30 Aug 2017\nLANGPRO: Natural Language Theorem Prover\nLasha Abzianidze\nCLCG, University of Groningen\nThe Netherlands\nL.Abzianidze@rug.nl\nAbstract\nLangPro is an automated theorem prover\nfor natural language.1\nGiven a set of\npremises and a hypothesis, it is able to\nprove semantic relations between them.\nThe prover is based on a version of an-\nalytic tableau method specially designed\nfor natural logic. The proof procedure op-\nerates on logical forms that preserve lin-\nguistic expressions to a large extent. The\nnature of proofs is deductive and transpar-\nent. On the FraCaS and SICK textual en-\ntailment datasets, the prover achieves high\nresults comparable to state-of-the-art.\n1\nIntroduction\nNowadays many formal logics come with their\nown proof systems and with the automated theo-\nrem provers based on these systems. If we share\nMontagues’s famous belief that there is “no im-\nportant theoretical difference between natural lan-\nguages and the artiﬁcial languages of logicians”,\nthen there plausibly exists a proof system for nat-\nural languages too. On the other hand, studies on\nNatural Logic seek a formal logic whose formu-\nlas are as close as possible to linguistic expres-\nsions. Inspired by these research ideas, Muskens\n(2010) proposed an analytic tableau system for\nnatural logic, where higher-order logic based on\na simple type theory is used as natural logic and a\nversion of analytic tableau method is designed for\nit. Later, Abzianidze (2015b,a, 2016a) made the\ntableau system suitable for wide-coverage reason-\ning by extending it and implementing a theorem\nprover based on it.\nThis paper presents the Prolog implementation\nof the theorem prover, called LangPro, in detail\nand completes the previous publications in terms\n1https://github.com/kovvalsky/LangPro\nLangPro\nCCG\nparser\nLLFgen\n& Aligner\nNLog\nProver\np1...pn\nh\n⊑\n⊥\n#\ntrees\nLLFs\nIn\nOut\nFigure 1:\nLangPro checks whether a set of\npremises p1, . . . , pn entails (⊑), contradicts (⊥) or\nis neutral (#) to a hypothesis h.\nof the system description. The rest of the paper\nis organized as follows.\nFirst, we brieﬂy intro-\nduce the tableau system and the employed natu-\nral logic. Then we characterize the architecture\nand functionality of LangPro (see Figure 1). Be-\nfore concluding, we brieﬂy compare the prover to\nthe related textual entailment systems.\n2\nNatural Tableau\nAn analytic tableau method is a proof procedure\nwhich searches a model, i.e.\na possible situ-\nation, satisfying a set of logic formulas.\nThe\nsearch is performed by gradually applying infer-\nence rules, also called tableau rules, to the for-\nmulas. A tableau rule has antecedents and conse-\nquent and is easy to read, e.g., according to NOT\nin Figure 3, if no A is B, then for any entity c, ei-\nther it is not A or it is not B. A tableau proof,\nin short a tableau, is often depicted as an upside-\ndown tree with initial formulas at its root (Fig-\nure 2). After each rule application, new inferred\nformulas are introduced in the tableau. Depend-\ning on the applied rules, the tableau can branch or\ngrow in depth. A tableau branch models a situ-\nation that satisﬁes all the formulas in the branch.\nClosed branches, marked with ×, correspond to\ninconsistent situations.\nThe search for a possi-\nble situation fails if all branches are closed—the\ntableau is closed.\nThe natural tableau is a tableau method for a\n1 several pug bark : T\n2 every(which bark dog)(be vicious) : T\n3 no pug(be evil) : T\n4 pug:c:T\n5 bark:c:T\n6 which bark dog:c:F\n8 bark:c:F\n10 ×\n9 dog:c:F\n11 ×\n7 be vicious:c:T\n12 vicious:c:T\n13 pug:c:F\n15 ×\n14 be evil:c:F\n16 evil:c:F\n17 ×\n∃T[1]\n×[5,8]\n×[4,9]\nAUX[7]\n×[4,13]\nAUX[14]\n×[12,16]\n∀T[2]\n∧F[6]\nNOT[3]\nFigure 2: The tableau proves: several pugs bark.\nevery dog which barks is vicious. ⊥no pug is evil.\nversion of natural logic.2 The terms of the nat-\nural logic, called Lambda Logical Forms (LLFs),\nare simply typed λ-terms built up from variables\nand constant lexical terms with the help of func-\ntion application and λ-abstraction. The format of\na tableau entry, i.e. node, is a tuple consisting of a\nmodiﬁer list, an LLF, an argument list and a truth\nsign. The parts are delimited with a colon. The\nempty lists are omitted for conciseness. For ex-\nample, the entries (1) and (2) both mean that it is\ntrue that c barks loudly in Paris, where (α, β) is a\nfunctional type that expects an argument of type α\nand returns a value of type β.3\ninnp,vp,vpParisnp : loudlyvp,vpbarkvp : ce : T\n(1)\n(innp,vp,vpParisnp)(loudlyvp,vpbarkvp ce) : T\n(2)\nIn order to prove a certain logical relation\nbetween premises and a hypothesis, the natural\ntableau searches a situation for the counterexam-\nple of the relation. The relation is proved if the\nsituation is not found, otherwise it is refuted. An\nexample of a closed natural tableau is shown in\nFigure 2. It proves the contradiction relation as it\nfails to ﬁnd a situation for the counterexample—\nthe premises and the hypothesis being true. In or-\n2It is an extended version of Muskens’ original tableau\nsystem. The extension is three-fold and concerns the type\nsystem, the format of tableau entries and the inventory of\ntableau rules (Abzianidze, 2015b).\n3LLFs are typed with syntactic and semantic types. In-\nteraction between these types is established via the subtyping\nrelation, e.g., entities being a subtype of NPs, e <: np, makes\nbarkvp ce well-formed, where vp abbreviates (np, s).\nC A B : [#–\nC] : F\nA : [#–\nC] : F\nB : [#–\nC] : F\n∧F\nC ∈{and, which}\nAAUX B : [#–\nC] : X\nB : [#–\nC] : X\nAUX\nQ A B : T\nA : c : T\nB : c : T\n∃T\nQ ∈{several, a, . . .}\nc is fresh\nA : [#–\nC] : T\nB : [#–\nC] : F\n×\n×\nA ⊑B\nno A B : T\nA : c : T\nB : c : F\nNOn\nT\nevery A B : T\nA : c : F\nB : c : T\n∀T\nc is old\nno A B : T\nA : c : F\nB : c : F\nNOT\nc is old\nFigure 3: The inference rules employed in the\ntableau proof of Figure 2. An entity term is old\n(fresh) wrt a branch iff it is (not) in the branch.\nLLFgen\nCCG\nTree\nCCG\nTerm\nCorrected\nCCG Term\nLLFs\nFOL\nDRT\nRemoving\ndirectionality\nCorrecting\nanalyses\nType-raising\nquantiﬁed NPs\nFigure 4: The LLF generator produces a list of\nLLFs from a single CCG derivation tree.\nder to facilitate reading tableau proofs, type infor-\nmation is omitted, the entries are enumerated and\narcs are labeled with tableau rule applications. For\nexample, 4 and 5 are obtained by applying ∃T to\n1 : if it is true that several pugs bark, then there is\nsome entity c which is a pug and which barks.\n3\nLLF Generator\nA Natural Tableau-based theorem prover for nat-\nural language requires automatic generation of\nLLFs from raw text.\nTo do so, we implement\na module, called LLFgen, that generates LLFs\nfrom syntactic derivations of Combinatory Cate-\ngorial Grammar (CCG, Steedman 2000). Given\na CCG derivation, LLFgen returns several LLFs\nthat model different orders of quantiﬁer scopes\n(see Figure 4).4 Figure 5 displays a CCG deriva-\ntion where VP i abbreviates Si\\NP.\nLLFs are obtained from a CCG tree in three\nmajor steps (Figure 4): (i) removing directionality\nfrom CCG trees, (ii) correcting semantically inad-\nequate analyses, and (iii) type-raising quantiﬁed\nNPs (QNPs). Below we brieﬂy describe each of\nthese steps and give corresponding examples.\nDirectionality information encoded in CCG cat-\negories and combinatory rules is redundant from\na semantic perspective, therefore we discard it in\n4See Abzianidze (2016a, Ch. 3) for a detailed description.\nba[Sdcl]\nfa[VP dcl]\nfa[VP ng]\nfa[PP]\nlx[NP, N]\nwater\nN\nwater\nNN\nwith\nPP /NP\nwith\nIN\nfa[VP ng/PP ]\nfa[NP]\nsteak\nN\nsteak\nNN\na\nNP/N\na\nDT\nrinsing\n(VP ng/PP)/NP\nrinse\nVBG\nis\nVP dcl/VP ng\nbe\nVBZ\nnobody\nNP\nnobody\nDT\nFigure 5: The CCG tree by C&C for nobody is\nrinsing a steak with water (SICK-1379).\nthe ﬁrst step: CCG categories are converted into\ntypes (Y \\X and Y/X ⇝(x, y)), and argument\nconstituents are placed after function ones in bi-\nnary combinatory rules. Resulted structures are\ncalled CCG terms (Figure 6).\nObtained CCG terms are often semantically in-\nadequate. One of the reasons for this is lexical\n(i.e. type-changing) rules (e.g., N 7→NP in Fig-\nure 5) of the CCG parsers which still remain in\nCCG terms (e.g., [watern]np in Figure 6). These\nrules are destructive from a compositional point of\nview. We designed 13 schematic rewriting rules of\ngeneral type that correct CCG terms—make them\nsemantically more adequate and transparent. The\nrules make use of types, part-of-speech (POS) and\nnamed entity (NE) tags to match semantically in-\nadequate analyses:5\n• Certain non-compositional multiword expres-\nsions are treated as constant terms: a lot of, in\nfront of, a few, because of, next to, etc.\n• Type-changing rules are explained by changing\nlexical types, decomposing terms or inserting\nnew terms.\nThis step carries out conversions\nlike [europen]np\n⇝\neuropenp, [nobodyn]np\n⇝\nnon,nppersonn, and [watern]np ⇝an,npwatern (see\nFigure 7). Inserted an,np merely plays a role of\nan existential quantiﬁer.\n• Several CCG analyses are altered in order\nto reﬂect formal semantics, e.g., attributive\nmodiﬁers are pushed under a relative clause:\nbig (which run mouse) ⇝which run (big mouse);\nand PPs are attached to nouns rather than NPs:\nin (a box)(every pug) ⇝every (in (a box) pug).\n5 To handcraft the rules, we used a development set\nof 1.7K CCG derivations obtained by parsing the sen-\ntences from FraCaS (Cooper et al., 1996) and the trial\nportion of SICK (Marelli et al., 2014) with CCG-based\nparsers:\nC&C (Clark and Curran, 2007) and EasyCCG\n(Lewis and Steedman, 2014).\nsdcl\nnobody\nnp\nnobody\nDT\nvpdcl\nvpng\npp\nnp\nwater\nn\nwater\nNN\nwith\nnp, pp\nwith\nIN\npp, vpng\nnp\nsteak\nn\nsteak\nNN\na\nn, np\na\nDT\nrinsing\nnp, pp, vpng\nrinse\nVBG\nis\nvpng, vpdcl\nbe\nVBZ\nFigure 6: The CCG term obtained from the CCG\ntree of Figure 5. NB: the lexical rule remains.\nsdcl\nnp\nperson\nn\nperson\nNN\nno\nn, np\nno\nDT\nvpdcl\nvpng\npp\nnp\nwater\nn\nwater\nNN\na\nn, np\na\nDT\nwith\nnp, pp\nwith\nIN\npp, vpng\nnp\nsteak\nn\nsteak\nNN\na\nn, np\na\nDT\nrinsing\nnp, pp, vpng\nrinse\nVBG\nis\nvpng, vpdcl\nbe\nVBZ\nFigure 7: The corrected version of the CCG term\nof Figure 6, with inserted and decomposed terms.\nLLFs are obtained from corrected CCG terms\nby type-raising QNPs from np to the type (vp, s)\nof generalized quantiﬁers. Hence, several LLFs\nare produced from a single CCG tree due to quan-\ntiﬁer scope ambiguity, e.g., (3–5) are some of the\nLLFs obtained from the CCG term of Figure 7.6\nN\n\u0000be\n\u0000λz. S\n\u0000λx. W\n\u0000λy. rinse x (with y)z\n\u0001\u0001\u0001\u0001\n(3)\nN\n\u0000be\n\u0000λz. W\n\u0000λy. S\n\u0000λx. rinse x (with y)z\n\u0001\u0001\u0001\u0001\n(4)\nW\n\u0000λy. S\n\u0000λx. N(be\n\u0000rinse x (with y))\n\u0001\u0001\u0001\n(5)\nSince LLFs encode instructions for semantic\ncomposition, they can be used to composition-\nally derive semantics in other meaning represen-\ntations (Figure 4), e.g., ﬁrst-order logic (FOL) or\nDiscourse Representation Theory (DRT). For this\napplication, LLFgen can be used as an independent\ntool. Given a CCG derivation in the Prolog format\n(supported by both C&C and EasyCCG), LLFgen\ncan return LLFs in XML, HTML or LATEXformats.\nFor a CCG tree, it is also possible to get either only\nthe ﬁrst LLF, e.g., (3), often reﬂecting the natural\norder of quantiﬁers, or a list of LLFs with various\nquantiﬁer scope orders (possibly including seman-\ntically equivalent LLFs, like (3) and (4)).\n6The LLFs use the following abbreviations:\nS\n=\naq steakn, W = aq watern, and N = noq personn, where\nq = (n, vp, s).\n4\nNatural Logic Theorem Prover\nThe tableau theorem prover for natural logic\n(NLogPro) represents a core part of LangPro (Fig-\nure 1). It is responsible for checking a set of lin-\nguistic expressions on (in)consistency.\nNLogPro\nconsists of four components: the Proof Engine\nbuilds tableau proofs by applying the rules from\nthe inventory of Rules; the rule applications are\nvalidated by the properties of lexical terms (en-\ncoded in the Signature) and the lexical knowledge\n(available from the Knowledge Base). We used the\nsame development data for LLFgen and NLogPro.\n4.1\nSignature\nThe signature (SG) lists lexical terms that have\nalgebraic properties relevant for inference, e.g.,\nmonotonicity,\nintersectivity,\nand implicativity.\nThe lexical items in the SG come with an argument\nstructure where each argument position is associ-\nated with a set of algebraic properties. For exam-\nple, every is characterized in the SG as [dw,up],\nmeaning that in its ﬁrst argument every is down-\nward monotone while being upward monotone in\nthe second one.\nCurrently, the SG lists about\n20 lexical items, mostly generalized quantiﬁers\n(GQs), that were found in the development data.\n4.2\nThe Inventory of Rules\nThe inventory of rules (IR) contains all inference\nrules used by the prover. Currently there are ca. 80\nrules in the IR (some in Figure 3). Around a quar-\nter of the rules are from Muskens (2010) and the\nrest are manually collected while exploring the de-\nvelopment data. The rules cover a plethora of phe-\nnomena. Some of them are of a formal nature like\nBoolean connectives and monotonicity and others\nof linguistic nature: adjectives, prepositions, deﬁ-\nnite NPs, expletives, open compound nouns, light\nverbs, copula, passives and attitude verbs.\nThe IR involves around 25 derivable rules—the\nrules that represent shortcuts of several rule ap-\nplications.\nOne such rule is (NOn\nT) in Figure 3,\nwhich is a speciﬁc version of (NOT). Use of deriv-\nable rules yields shorter tableau proofs but raises\na problem of performing the same rule application\nseveral times.\nNLogPro avoids this by maintain-\ning a subsumption relation between the rules and\nkeeping track of rule applications per branch.\nA user can introduce new rules in the IR as Pro-\nlog rules (Code 1): the head of the rule encodes\nantecedent nodes ===> consequent nodes, and the\nbody is a list of Prolog goals specifying the condi-\ntions the rule has to meet.\nr(Name, Feats, ConstIndx, KeyWrd, KB,\nbr([nd(Mod1, LLF1, Arg1, Sign1),...\nnd(ModN, LLFN, ArgN, SignN)],\nSignature) ===>\n[br([nd(Mod3, LLF3, Arg3, Sign3),...],\nSignature3),\nbr([nd(Mod4, LLF4, Arg4, Sign4),...],\nSignature4)]\n:- Goal1, ..., GoalN. %conditions\nCode 1:\nThe Prolog format of tableau rules.\nFeats denotes efﬁciency features, ConstIndx\nand KB are the KB and indexing of constants re-\nspectively (ﬁxed for every rule), and KeyWrd de-\nnotes ﬁxed lexical terms occurring in the rule.\nEach branch maintains its own signature of enti-\nties introduced during the proof.\n4.3\nKnowledge Base\nThe knowledge base (BS) is based on the Pro-\nlog version of WordNet 3.0 (Fellbaum, 1998). At\nthis moment only the hyponymy/hypernymy, sim-\nilarity and antonymy relations are included in the\nKB. For simplicity, LangPro does not do any word\nsense disambiguation (WSD) but allows multi-\nple word senses for a lexical term. For example,\nA ⊑B iff SynSetA is a hyponym of SynSetB,\nor there are similar SenseA and SenseB, where\nSenseA ∈SynSetA and SenseB ∈SynSetB.\nIn the prover, a user can restrict the number of\nword senses per word by specifying a cutoff N,\ni.e. the N most frequent senses per word.\nIn addition to the WN relations, a user can in-\ntroduce new lexical relations in the KB as Prolog\nfacts, e.g., is_(crowd, group).\n4.4\nThe Proof Engine\nThe proof engine (PE) is the component that builds\nproof trees. While applying rules it takes into ac-\ncount computational efﬁciency of each rule where\nthe efﬁciency depends on the following categories:\n• Branching: a rule is either branching (e.g., ∀T)\nor non-branching (e.g., AUX).\n• Semantic equivalence: this depends whether the\nantecedents of a rule is semantically equivalent\nto its consequents. For example, (∧F ) encodes\nthe semantic equivalence while (NOT) does not.\n• Producing: depending on whether a rule pro-\nduces a fresh entity, it is a producer or a non-\nproducer. (∃T) is a producer while (∀T) is not.\n• Consuming: a rule is a consumer iff it employs\nan old entity from the branch during application.\nThe consumer rules are (∀T) and (NOT) but (∃T).\nThe most efﬁcient combination of these features\nis non-branching,\nsemantic equivalence,\nnon-\nproducing and non-consuming. Depending on a\npriority order between these categories, called an\nefﬁciency criterion, one can deﬁne a partial ef-\nﬁciency order over the rules.\nIn particular, (6)\nis one of the best efﬁciency criteria on SICK\n(Abzianidze, 2016a, Ch. 6). According to (6), (∧F)\nis more efﬁcient than (NOn\nT) since the equivalence\nis the most prominent category in (6), and (∧F) is\nequivalence in contrast to (NOn\nT)\n[equi, nonBr, nonProd, nonCons]\n(6)\nA user can change the default criterion (6) by pass-\ning a criterion via the Prolog predicate effCr/1.\nThe PE builds two structures: a tree (see Fig-\nure 2) and a list.\nThe latter represents a list of\nthe tree branches. The list structure is the main\ndata structure that guides the computation process\nwhile the tree structure is optional (activated with\nthe predicate prooftree/0) and is used for dis-\nplaying proofs in a compact way. A few of the\npredicates that control the proof procedure are:\n• ral/1 sets a rule application limit to n, which\nmeans that after n rules are applied the proof is\nterminated. n = 400 by default.\n• thE/0 always permits existential import from\ndeﬁnite NPs: it makes (∃T) applicable to the en-\ntry then,vp,s dogn barkvp : F.\n• allInt/0 allows to treat lexical modiﬁers of\nthe form cVB.|JJ|NN\nn,n\nas intersective by default un-\nless stated differently in the SG. This permits to\ninfer babyn : c : T and kangaroon : c : T from\nbabyn,nkangaroo : c : T, for better or worse.\n• the/0, a2the/0, and s2the/0 are used as ﬂags\nand treat bare, indeﬁnite, and plural NPs as def-\ninite NPs, respectively.\n5\nLangPro: Natural Language Prover\nThe tableau-based theorem prover for natural lan-\nguage is obtained by chaining a CCG parser,\nLLFgen and NLogPro.\nIn order to detect a se-\nmantic relation between a set of premises {pi}n\ni=1\nand a hypothesis h, ﬁrst the corresponding LLFs\n{Pi}n\ni=1 and H are obtained via a CCG parser and\nLLFgen (i.e. for simplicity, a single LLF per sen-\ntence).\nThen based on the lexical terms of the\nLLFs, relevant sets of relations K and rules R are\ncollected from the KB and the IR, respectively. To\nrefute both entailment and contradiction relations\nNLogPro builds two proof trees using K and R.\nOne starts with the counterexample (7) for entail-\nment and another with the counterexample (8) for\ncontradiction. The semantic relation which could\nnot be refuted (i.e. its tableau for the counterexam-\nple was closed) is said to be proved. The relation\nis considered to be neutral iff both tableaux have\nthe same closure status: open or closed.\n{P1 : T,\n. . . , Pn : T,\nH : F}\n(7)\n{P1 : T, . . . , Pn : T,\nH : T}\n(8)\nEntailment relations often do not depend on\nsemantics of phrases shared by premises and\nhypotheses.\nTo bypass analyzing the common\nphrases, LangPro can use an optional CCG term\naligner in LLFgen (Figure 1), which identiﬁes the\ncommon CCG sub-terms and treats them as con-\nstants. The sub-terms that are downward mono-\ntone or indeﬁnite NPs are excluded from align-\nments as they do not behave semantically as\nconstants.\nAfter aligning CCG terms, aligned\nLLFs are obtained from them via the type-raising.\nTableau proofs with aligned LLFs are shorter.\nThus, ﬁrst, a tableau with aligned LLFs is built,\nand if the tableau did not close, then non-aligned\nLLFs are used since alignment might prevent the\ntableau from closing. On SICK, the aligner boosts\nthe accuracy by 1%. If stronger alignment is used\n(i.e.\naligning indeﬁnite NPs), the accuracy on\nSICK is increased by 2%. Both weak and strong\nalignment options can be chosen in LangPro.\nThe parser component of LangPro can be ﬁlled\nby C&C or EasyCCG. This results in two versions\nof LangPro, ccLangPro and easyLangPro respec-\ntively.\nBoth versions achieve similar results on\nFraCaS and SICK, and a simple aggregation of\ntheir judgments (coLangPro) improves the accu-\nracy on the unseen portion of SICK by 1%.\nWith respect to its rule-based nature, LangPro\nis fast. Given ready CCG derivations, on average\n100 SICK problems are classiﬁed in 3.5 seconds.7\nDetails about speed and impact of parameters on\nthe performance are given in Abzianidze (2016a).\nIn addition to an entailment judgment, LangPro\ncan output the actual tableau proof trees (similar\n7This is measured on 8 × 2.4 GHz CPU machine, when\nproving problems in parallel (via the paralle/0 predicate)\nwith the strong aligner option and the rule application limit\n50—the conﬁguration that achieves high performance both\nin terms of speed and accuracy.\nto Figure 2) in three formats: a drawing of a proof\ntree via the XPCE GUI, a LATEX source code, an\nXML output, or an HTML ﬁle.\n6\nRelated work\nTheorem proving techniques (Bos and Markert,\n2005) or ideas from Natural Logic (MacCartney,\n2009) were already used in recognizing textual\nentailment (RTE). But the combination of these\ntwo is a novel approach to RTE. The underlying\nhigher-order logic of LangPro guarantees sound\nreasoning over several premises, including some\ncomplex semantic phenomena.\nThis is in con-\ntrast to the RTE systems that cannot reason over\nseveral premises or cannot account for Booleans\nand quantiﬁers, including the ones (MacCartney,\n2009) inspired by Natural Logic, and in contrast\nto those ones that use FOL representations and\ncannot cover higher-order phenomena like gener-\nalized quantiﬁers or subsective adjectives.\nLangPro\nachieves\nstate-of-the-art\nsemantic\ncompetence (with accuracy of 87%) on the\nFraCaS sections commonly used for evaluation\n(Abzianidze, 2016b,a).\nOn SICK, the prover\nobtains 82.1% of accuracy (Abzianidze, 2015a,\n2016a) while state-of-the-art systems score in\nthe range of 81-87% and average performance of\nhuman on the dataset is around 84%.\nDetailed\ncomparison of LangPro to the related RTE systems\nis discussed in (Abzianidze, 2015a, 2016b,a).\n7\nConclusion\nThe presented natural language prover involves a\nunique combination of natural logic, higher-order\nlogic and a tableau method.\nIts natural logic\nside simpliﬁes generation of the logical forms and\nmakes the prover to be relatively easily scaled\nup.\nDue to its higher-order virtue, the prover\neasily accounts for complex semantic phenomena\nuntameable in FOL. Because of its high reliabil-\nity (less than 3% of its entailment and contradic-\ntion judgments are incorrect), the judgments of the\nprover can be successfully borrowed by other RTE\nsystems. Further scaling-up for longer sentences\n(e.g., newswire text) and automated knowledge ac-\nquisition present future challenges to the prover.\nAcknowledgments\nThis work has been supported by the NWO-VICI\ngrant “Lost in Translation – Found in Meaning”\n(288-89-003).\nReferences\nLasha Abzianidze. 2015a. A tableau prover for natural\nlogic and language. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2492–2502. ACL.\nLasha Abzianidze. 2015b. Towards a wide-coverage\ntableau method for natural logic. In Tsuyoshi Mu-\nrata, Koji Mineshima, and Daisuke Bekki, editors,\nNew Frontiers in Artiﬁcial Intelligence: Revised Se-\nlected Papers of JSAI-isAI 2014 Workshops, LENLS,\nJURISIN, and GABA, pages 66–82. Springer.\nLasha Abzianidze. 2016a. A natural proof system for\nnatural language. Ph.D. thesis, Tilburg University.\nLasha Abzianidze. 2016b. Natural solution to fracas\nentailment problems.\nIn Proceedings of the Fifth\nJoint Conference on Lexical and Computational Se-\nmantics, pages 64–74. ACL.\nJohan Bos and Katja Markert. 2005. Recognising tex-\ntual entailment with logical inference. In Proceed-\nings of Human Language Technology Conference\nand Conference on Empirical Methods in Natural\nLanguage Processing, pages 628–635.\nStephen Clark and James R. Curran. 2007.\nWide-\ncoverage efﬁcient statistical parsing with CCG and\nlog-linear models. Computational Linguistics, 33.\nRobin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox,\nJosef Van Genabith, Jan Jaspars, Hans Kamp, David\nMilward, Manfred Pinkal, Massimo Poesio, Steve\nPulman, Ted Briscoe, Holger Maier, and Karsten\nKonrad. 1996. FraCaS: A Framework for Compu-\ntational Semantics. Deliverable D16.\nChristiane Fellbaum, editor. 1998. WordNet: An Elec-\ntronic Lexical Database. MIT Press.\nMike Lewis and Mark Steedman. 2014. A* CCG pars-\ning with a supertag-factored model.\nIn Proceed-\nings of the 2014 Conference on Empirical Methods\nin Natural Language Processing, pages 990–1000.\nACL.\nBill MacCartney. 2009. Natural language inference.\nPhd thesis, Stanford University.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. 2014. A sick cure for the evaluation of com-\npositional distributional semantic models. In Pro-\nceedings of LREC’14. ELRA.\nReinhard Muskens. 2010. An analytic tableau system\nfor natural logic. In Maria Aloni, Harald Bastiaanse,\nTikitu de Jager, and Katrin Schulz, editors, Logic,\nLanguage and Meaning, volume 6042 of LNCS,\npages 104–113. Springer.\nMark Steedman. 2000. The Syntactic Process. MIT\nPress, Cambridge, MA, USA.\n",
  "categories": [
    "cs.CL",
    "68T50",
    "I.2.7"
  ],
  "published": "2017-08-30",
  "updated": "2017-08-30"
}