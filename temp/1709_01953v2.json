{
  "id": "http://arxiv.org/abs/1709.01953v2",
  "title": "Implicit Regularization in Deep Learning",
  "authors": [
    "Behnam Neyshabur"
  ],
  "abstract": "In an attempt to better understand generalization in deep learning, we study\nseveral possible explanations. We show that implicit regularization induced by\nthe optimization method is playing a key role in generalization and success of\ndeep learning models. Motivated by this view, we study how different complexity\nmeasures can ensure generalization and explain how optimization algorithms can\nimplicitly regularize complexity measures. We empirically investigate the\nability of these measures to explain different observed phenomena in deep\nlearning. We further study the invariances in neural networks, suggest\ncomplexity measures and optimization algorithms that have similar invariances\nto those in neural networks and evaluate them on a number of learning tasks.",
  "text": "Implicit Regularization in Deep Learning\nby\nBehnam Neyshabur\nA thesis submitted\nin partial fulﬁllment of the requirements for\nthe degree of\nDoctor of Philosophy in Computer Science\nat the\nTOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO\nAugust, 2017\nThesis Committee:\nNathan Srebro (Thesis advisor),\nYury Makarychev,\nRuslan Salakhutdinov,\nGregory Shakhnarovich\narXiv:1709.01953v2  [cs.LG]  8 Sep 2017\nImplicit Regularization in Deep Learning\nby\nBehnam Neyshabur\nAbstract\nIn an attempt to better understand generalization in deep learning, we study several possible expla-\nnations. We show that implicit regularization induced by the optimization method is playing a key\nrole in generalization and success of deep learning models. Motivated by this view, we study how\ndifferent complexity measures can ensure generalization and explain how optimization algorithms can\nimplicitly regularize complexity measures. We empirically investigate the ability of these measures\nto explain different observed phenomena in deep learning. We further study the invariances in neural\nnetworks, suggest complexity measures and optimization algorithms that have similar invariances to\nthose in neural networks and evaluate them on a number of learning tasks.\nThesis Advisor: Nathan Srebro\nTitle: Professor\nIn memory of my dear friend, Sina Masihabadi\nAcknowledgments\nI would like to thank my advisor Nati Srebro without whom I would not be even pursuing a PhD in\nthe ﬁrst place. Nati’s classes made me interested in optimization and machine learning and later I\nwas delighted when he agreed to be my advisor. From the very ﬁrst few meetings, I realized how\nhis excitement and passion for research energizes me. I will not forget our countless long meetings\nthat did not seem long to me at all. Looking back, I think Nati was the best mentor I could have\nhoped for. Among the skills I started to pick up from Nati, my favorite is asking the right question\nand formalizing it. About three years ago, excited about recent advances in deep learning, I walked\ninto Nati’s ofﬁce and I told him that I want to understand what makes deep learning models perform\nwell in practice. He helped me to ask the right questions and formalize them. Nati has had a great\ninﬂuence in shaping my thoughts and therefore this dissertation. I am forever grateful for all I have\nlearned from him.\nI would like to thank all of my committee members from whom I beneﬁted during my PhD study. I am\nthankful to Ruslan Salakhutdinov who has been advising me in several projects and I have beneﬁted\na lot from discussions with him and his advices regarding my career. I thank Yury Makarychev for\nhis collaborations and advices during early years of PhD. I always felt free to knock at his ofﬁce door\nwhenever I was stuck with theoretical questions and needed more insight. I thank Greg Shakhnarovich\nwho has been kind enough to answer my questions on deep learning and metric learning whenever I\nshowed up at his ofﬁce. Moreover, I think his well-prepared machine learning course had a great\nimpact on making me interested in machine learning.\nTTIC’s faculty have a close and friendly relationship with students and I have beneﬁted from that. I\nam thankful to Jinbo Xu, my interim advisor who encouraged me to continue PhD in the area that\nﬁts my interests the best. I am pleased that we continued collaboration on computational biology\nprojects. Madhur Tulsiani deserves a special thanks for being a great director of graduate studies.\nMadhur’s desire to improve the PhD program at TTIC and his support made me feel comfortable\nand free to discuss my thoughts and suggestions with him many times. I would like to acknowledge\nthat I really enjoyed the mathematical foundation course taught by David McAllester and his views\non deep learning which he discussed in his deep learning course at TTIC. I regret that I only started\ncollaborating with him in last few months of my PhD and I wish I would have had more discussions\nwith him during these years. I was not able to collaborate with Karen Livescu and Kevin Gimpel\nduring my PhD years but I have enjoyed many conversations with them and I felt supported by them.\nI also thank Sadaoki Furui, Julia Chuzhoy and Matthew Walter for their effort in enhancing TTIC’s\nPhD program. I had the pleasure of chatting with Avrim Blum a few times since he has joined TTIC\nand I am excited about his new appointment at TTIC.\nDuring my PhD years, I have enjoyed collaborating with several research faculties at TTIC. I would\nlike to thank Srinadh Bhojanapalli with whom I have spent a lot of time in the last two years for\nbeing a good friend, mentor and collaborator. I am very grateful for everything he has offered me.\nMy earlier works in deep learning was in collaboration with Ryota Tomioka and I am thankful his\nhelp and support. Suriya Gunasekar is another research faculty at TTIC who has been an amazing\nfriend and collaborator. Other than her help and support, I have also enjoyed countless discussions\nwith her on random topics. I would like to thank Aly Khan and Ayan Chakrabarti for what I have\nlearned from them during our collaborations on different projects. Finally, I have learned from and\nenjoyed chatting with many other research faculty and postdocs at TTIC including but not limited to\nMichael Maire, Hammad Naveed, Mesrob Ohannessian, Mehrdad Mahdavi, Weiran Wang, Herman\nKamper, Ofer Meshi, Qixing Huangi, Subhransu Maji, Raman Arora, and George Papandreou.\nI would like to thank TTIC students. I am indebted to Payman Yadollahpour who kindly hosted\nme and my wife, Somaye, for several days after we arrived to US and helped us numerous times in\nvarious occasions. I am grateful for knowing him and for all the moments we have shared. Hao Tang\nalso deserves a special thanks. He was the person I used to discuss all my ideas and thoughts with.\n5\nTherefore, everything presented in this thesis is somehow impacted by the discussions with him. I\nthank Mrinalkanti Ghosh for the countless times I interrupted his work with a theoretical question\nand he patiently helped me try to investigate it. Shubhendu Trivedi is another student whom I had\nseveral fruitful discussion with. I am also thankful for things I have learned from and memories\nI have shared with Bahador Nooraei, Mohammadreza Mostajabi, Haris Angelidakis, Vikas Garg,\nKaustav Kundu, Abhishek Sen, Siqi Sun, Hai Wang, Heejin Choi, Qingming Tang, Lifu Tu, Blake\nWoodworth, Shubham Toshniwal, Shane Settle, Nicholas Kolkin, Falcon Dai, Charles Schaff, Rachit\nNimavat, and Ruotian Luo.\nI want to thank TTIC staff for making my life much easier and helping me whenever I had any\nproblems. On top of the list is Chrissy Novak. She has always patiently and kindly listened to my\ncomplaints, suggestions, and problems, and tried her best to resolve or improve any issues. Adam\nBohlander has always been helpful and quick in resolving any IT issues. I think Adam’s great\nexpertise has saved me several hundreds of hours. Liv Leader was one of TTIC’s staff when we\narrived to US. She helped us during the ﬁrst few months of being in US. The ﬁrst party we were\ninvited in US was Liv’s housewarming party. I also thank Mary Marre, Amy Minick, Jessica Johnston\nand other TTIC staffs for their effort.\nBeyond TTIC, I am grateful for my internship at MSR Silicon Valley with Rina Panigrahy. Perhaps,\nseveral hundred hours of meeting and discussions with Rina in this internship whose goal was to\nbetter understand neural networks theoretically had a great inﬂuence on making me interested in\ndeep learning. I am also thankful for collaboration and discussions I had with Robert Schapire,\nAlekh Agarwal, Haipeng Luo and John Langford during my internship at MSR New York City. I\nam grateful to Anna Choromanska for several discussions on neural networks and for great career\nadvices I received from her. I thank Tony Wu for being a good friend and collaborator. I have learned\na lot about deep learning from several meetings with him.\nMy deepest gratitude goes to my parents and my brother who have shaped who I am today. Words\ncannot capture how grateful I am to Somaye without whom everything in my life would have been\ndrastically different. I will thank her in person.\nI am dedicating this thesis to the memory of my dear friend Sina Massihabadi who went to the same\nhigh school and university as me. Sina was very brilliant and one of the ﬁnest human beings I have\never met. Later, he moved to United States to start a PhD in industrial engineering at Texas A&M\nuniversity. However, he did not make it to the end of the program. He died in a tragic car accident\nwhen he was driving to the airport to pick up his mother whom he was not able to visit for two years\ndue to visa restrictions on Iranians.\n6\nContents\n1\nIntroduction\n12\n1.1\nMain Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2\nPreliminaries\n15\n2.1\nThe Statistical Learning Framework . . . . . . . . . . . . . . . . . .\n15\n2.2\nFeedforward Neural Networks with Shared Weights . . . . . . . . . .\n16\nI\nImplicit Regularization and Generalization\n18\n3\nGeneralization and Capacity Control\n19\n3.1\nVC Dimension: A Cardinality-Based Arguments\n. . . . . . . . . . .\n19\n3.2\nNorms and Margins: Counting Real-Valued Functions . . . . . . . . .\n20\n3.3\nRobustness: Lipschitz Continuity with Respect to Input . . . . . . . .\n21\n3.4\nPAC-Bayesian Framework: Sharpness with Respect to Parameters . .\n22\n4\nOn the Role of Implicit Regularization in Generalization\n25\n4.1\nNetwork Size and Generalization . . . . . . . . . . . . . . . . . . . .\n26\n4.2\nA Matrix Factorization Analogy . . . . . . . . . . . . . . . . . . . .\n29\n5\nNorm-based Capacity Control\n31\n5.1\nGroup Norm Regularization\n. . . . . . . . . . . . . . . . . . . . . .\n32\n5.2\nPer-Unit and Path Regularization . . . . . . . . . . . . . . . . . . . .\n36\n5.3\nOverall Regularization\n. . . . . . . . . . . . . . . . . . . . . . . . .\n38\n5.4\nDepth Independent Regularization . . . . . . . . . . . . . . . . . . .\n40\n5.5\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n7\n5.6\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n6\nSharpness/PAC-Bayes Generalization Bounds\n52\n6.1\nSpectrally-Normalized Margin Bounds . . . . . . . . . . . . . . . . .\n52\n6.2\nGeneralization Bound based on Expected Sharpness . . . . . . . . . .\n53\n6.3\nSupporting Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n6.4\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n7\nEmpirical Investigation\n63\n7.1\nComplexity Measures . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n7.2\nExperiments Settings . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n7.3\nTrue Labels Vs. Random Labels . . . . . . . . . . . . . . . . . . . .\n65\n7.4\nDifferent Global Minima . . . . . . . . . . . . . . . . . . . . . . . .\n66\n7.5\nIncreasing Network Size\n. . . . . . . . . . . . . . . . . . . . . . . .\n67\nII\nGeometry of Optimization and Generalization\n69\n8\nInvariances\n70\n8.1\nInvariances in Feedforward and Recurrent Neural Networks . . . . . .\n70\n8.2\nUnderstanding Invariances . . . . . . . . . . . . . . . . . . . . . . .\n72\n8.3\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n9\nPath-Normalization for Feedforward and Recurrent Neural Networks\n80\n9.1\nPath-regularizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n9.2\nPath-SGD for Feedforward Networks\n. . . . . . . . . . . . . . . . .\n81\n9.3\nExtending to Networks with Shared Weights . . . . . . . . . . . . . .\n81\n9.4\nProofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n10 Experiments on Path-SGD\n87\n10.1 Experiments on Fully Connected Feedforward Networks . . . . . . .\n87\n10.2 Experiments on Recurrent Neural Networks . . . . . . . . . . . . . .\n90\n11 Data-Dependent Path Normalization\n93\n11.1 A Uniﬁed Framework . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n8\n11.2 DD-Path Normalization: A Batch Normalization Approach . . . . . .\n96\n11.3 DD-Path-SGD: A Steepest Descent Approach . . . . . . . . . . . . .\n98\n11.4 Node-wise invariance . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n11.5 Supporting Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\nConclusion\n103\nBibliography\n104\n9\nList of Figures\n4.1\nThe training and test error of a two-layer perceptron for varying number of hidden units\n. .\n27\n4.2\nThe generalization behavior a two-layer perceptron for varying number of hidden units . . .\n28\n6.1\nVerifying the conditions of Theorem 27 on a multi-layer perceptron\n. . . . . . . . . .\n54\n6.2\nCondition C1 in Theorem 27 on a learned network. . . . . . . . . . . . . . . . . .\n57\n6.3\nRatio of activations that ﬂip based on the magnitude of perturbation. . . . . . . . . . .\n57\n6.4\nCondition C3 in Theorem 27 for random initialization and learned network\n. . . . . . .\n58\n7.1\nComparing complexity measures on a VGG network trained with true or random labels.\n. .\n66\n7.2\nSharpness and PAC-Bayes measures on a VGG network trained with true or random labels. .\n66\n7.3\nExperiments on global minima with poor generalization. . . . . . . . . . . . . . . .\n67\n7.4\nThe effect of increasing network size on generalization. . . . . . . . . . . . . . . .\n68\n8.1\nInvariance in fully connected feedforward networks . . . . . . . . . . . . . . . . .\n71\n8.2\nInvariances in recurrent neural networks (RNNs). . . . . . . . . . . . . . . . . . .\n72\n8.3\nA 3 layer network with 10 parameters and 8 paths. . . . . . . . . . . . . . . . . .\n76\n8.4\nSchematic illustration of the linear dependence of four paths. . . . . . . . . . . . . .\n79\n10.1 Comparing Path-SGD to other optimization methods on 4 dataset without dropout . . . . .\n88\n10.2 Comparing Path-SGD to other optimization methods on 4 dataset with dropout . . . . . .\n89\n10.3 The contribution of the second term. . . . . . . . . . . . . . . . . . . . . . . .\n90\n10.4 Test errors for the addition problem of different lengths. . . . . . . . . . . . . . . .\n91\n11.1 An example of layered feedforward network and notation used in the chapter . . . . . . .\n94\n10\nList of Tables\n2.1\nForward computations for feedforward and recurrent networks\n. . . . . . . . . . . .\n17\n10.1 General information on datasets used in the experiments on feedforward networks.\n. . . .\n88\n10.2 Test error (MSE) for the adding problem and test classiﬁcation error for the sequential MNIST.\n91\n10.3 Test BPC for PTB and text8. . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n11.1 Some of the choices for Rv in the proposed uniﬁed framework. . . . . . . . . . . . .\n95\n11\nChapter 1\nIntroduction\nDeep learning refers to training typically complex and highly over-parameterized models that beneﬁt\nfrom learning a hierarchy of representations. The terms “neural networks” and “deep learning” are often\nused interchangeably as many modern deep learning models are slight modiﬁcations of different types\nof neural networks suggested originally around 1950-2000 [1]. Interest in deep learning was revived\naround 2006 [2, 3] and since then, it has had enormous practical successes in different areas [4]. The\nrapid growth of practical works and new concepts in this ﬁeld has created a considerable gap between\nour theoretical understanding of deep learning and practical advances.\nFrom the learning viewpoint, we often look into three different properties to investigate the effectiveness\nof a model: expressive power, optimization, and generalization. Given a function class/model the\nexpressive power is about understanding what functions can be realized or approximated by the functions\nin the function class. Given a loss function as an evaluation measure, the optimization aspect refers to the\nability to efﬁciently ﬁnd a function with a minimal loss on the training data and generalization addresses\nthe model’s ability to perform well on the new unseen data.\nAll above aspects of neural networks have been studied before. Neural networks have great expressive\npower. Universal approximation theorem states that for any given precision, feed-forward networks\nwith a single hidden layer containing a ﬁnite number of hidden units can approximate any continuous\nfunction [5]. More broadly, any O(T) time computable function can be captured by an O(T 2) sized\nnetwork, and so the expressive power of such networks is indeed great [6, Theorem 9.25].\nGeneralization of neural networks as a function of network size is also fairly well understood. With\nhard-threshold activations, the VC-dimension, and hence sample complexity, of the class of functions\nrealizable with a feed-forward network is equal, up to logarithmic factors, to the number of edges in\nthe network [7, 8], corresponding to the number of parameters. With continuous activation functions\nthe VC-dimension could be higher, but is fairly well understood and is still controlled by the size of the\nnetwork.1\nAt the same time, we also know that learning even moderately sized networks is computationally\nintractable—not only is it NP-hard to minimize the empirical error, even with only three hidden units,\nbut it is hard to learn small feed-forward networks using any learning method (subject to cryptographic\nassumptions). That is, even for binary classiﬁcation using a network with a single hidden layer and a\nlogarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly\ncaptured by such a small network, there is likely no efﬁcient algorithm that can ensure error better than\n1/2 [10, 11]—not if the algorithm tries to ﬁt such a network, not even if it tries to ﬁt a much larger\n1Using weights with very high precision and vastly different magnitudes it is possible to shatter a number of points quadratic in\nthe number of edges when activations such as the sigmoid, ramp or hinge are used [8, Chapter 20.4]. But even with such activations,\nthe VC dimension can still be bounded by the size and depth [9, 7, 8].\n12\nnetwork, and in fact no matter how the algorithm represents predictors. And so, merely knowing that\nsome not-too-large architecture is excellent in expressing does not explain why we are able to learn\nusing it, nor using an even larger network. These results, however, are not suggesting any insights on\nthe practical success of deep learning. In contrast to our theoretical understanding, it is possible to train\n(optimize) very large neural networks and despite their large sizes, they generalize to unseen data. Why\nis it then that we succeed in learning very large neural networks? Can we identify a property that makes\nthem possible to train? Why do these networks generalize to unseen data despite their large capacity in\nterms of the number of parameters?\nIn such an over-parameterized setting, the objective has multiple global minima, all minimize the training\nerror, but many of them do not generalize well. Hence, just minimizing the training error is not sufﬁcient\nfor learning: picking the wrong global minima can lead to bad generalization behavior. In such situations,\ngeneralization behavior depends implicitly on the algorithm used to minimize the training error. Different\nalgorithmic choices for optimization such as the initialization, update rule, learning rate, and stopping\ncondition, will lead to different global minima with different generalization behavior [12, 13, 14].\nWhat is the bias introduced by these algorithmic choices for neural networks? What is the relevant notion\nof complexity or capacity control?\nThe goal of this dissertation is to understand the implicit regularization by studying the optimization,\nregularization, and generalization in deep learning and the relationship between them. The dissertation is\ndivided into two parts. In the ﬁrst part, we study different approaches to explain generalization in neural\nnetworks. We discuss how some of the complexity measures derived by these approaches can explain\nimplicit regularization. In the second part, we investigate the transformations under which the function\ncomputed by a network remains the same and therefore argue for complexity measures and optimization\nalgorithms that have similar invariances. We ﬁnd complexity measures that have similar invariances to\nneural networks and optimization algorithms that implicitly regularize them. Using these optimization\nalgorithms for different learning tasks, we indeed observe that they have better generalization abilities.\n1.1\nMain Contributions\n1. Part I:\n(a) The Role of Implicit Regularization (Chapter 4) We design experiments to highlight the role\nof implicit regularization in the success of deep learning models.\n(b) Norm-based capacity control (Chapter 5): We prove generalization bounds for the class of\nfully connected feedforward networks with the bounded norm. We further show that for\nsome norms, this bound is independent of the number of hidden units.\n(c) Generalization Guarantee by PAC-Bayes Framework (Chapter 6): We show how PAC-Bayes\nframework can be employed to obtain generalization bounds for neural networks by making\na connection between sharpness and PAC-Bayes theory.\n(d) Implicit Regularization by SGD (Chapter 6): We show that networks learned by SGD satisfy\nseveral conditions that lead to ﬂat minima.\n(e) Empirical Investigation of Generalization in Deep Learning (Chapter 7): We design ex-\nperiments to compare the ability of different complexity measures to explain the implicit\nregularization and generalization in deep learning.\n2. Part II:\n(a) Invariances in neural networks (Chapter 8): We characterize a large class of invariances in\nfeedforward and recurrent neural networks caused by rescaling issues and suggest a measure\ncalled the Path-norm that is invariant to the rescaling of the weights.\n13\n(b) Path-normalized optimization (Chapter 9 and 10): Inspired by our understanding of invari-\nances in neural networks and the importance of implicit regularization, we suggest a new\nmethod called Path-SGD whose updates are the approximate steepest descent direction with\nrespect to the Path-norm. We show Path-SGD achieves better generalization error than SGD\nin both fully connected and recurrent neural networks on different benchmarks.\n(c) Data-dependent path normalization (Chapter 11): We propose a uniﬁed framework for neural\nnet normalization, regularization, and optimization, which includes Path-SGD and Batch-\nNormalization and interpolates between them across two different dimensions. Through this\nframework, we investigate the issue of invariance of the optimization, data dependence and\nthe connection with natural gradient.\n14\nChapter 2\nPreliminaries\nIn this chapter, we present the basic setup and notations used throughout this dissertation.\n2.1\nThe Statistical Learning Framework\nIn this section, we brieﬂy review the statistical learning framework. More details on the formal model\ncan be found in Shalev-Shwartz and Ben-David [8].\nIn the statistical batch learning framework, the learner is given a training set S = {(x1, y1), . . . , (xm, ym)}\nof m training points in X × Y that are independently and identically distributed (i.i.d.) according to an\nunknown distribution D. For simplicity, we will focus on the task of classiﬁcation where the goal of the\nlearner is to output a predictor f : X →Y with minimum expected error on samples generated from the\ndistribution D:\nLD(f) = P(x,y)∼D [f(x) ̸= y]\n(2.1.1)\nSince the learner does not have access to the distribution D, it cannot evaluate or minimize the expected\nloss. It can however, obtain an estimate of the expected loss of a predictor f using the training set S:\nLS(f) = |{(x, y) ∈S | f(x) ̸= y}|\nm\n(2.1.2)\nWhen the distribution D and training set S is clear from the context, we use L(f) and ˆL(f) instead of\nLD and LS(f) respectively. We also deﬁne the expected margin loss for any margin γ > 0, as follows:\nLγ(f) = P(x,y)∼D\n\u0014\nf(x)[y] ≤γ + max\nj̸=y f(x)[j]\n\u0015\n(2.1.3)\nLet ˆLγ(f) be the empirical estimate of the above expected margin loss. Since setting γ = 0 corresponds\nto the classiﬁcation loss, we will use L0(f) = L(f) and ˆL0(fw) = ˆL(f) to refer to the expected risk\nand the training error.\nMinimizing the loss in the equation (2.1.2) which is called the training error does not guarantee low\nexpected error. For example, a predictor that only memorizes the set S to output the right label for the\ndata in the training set can get zero training error while its expected loss might be very close to the\nrandom guess. We are therefore interested in controlling the difference LD(f) −LS(f) which we will\nrefer to as generalization error. This quantity reﬂects the difference between memorizing and learning.\nAn interesting observation is that if f is chosen in advance and is not dependent on the distribution D or\n15\ntraining set S, then the generalization error can be simply bounded by concentration inequalities such as\nHoeffding’s inequality and relatively small number of samples are required to get small generalization\nerror. However, since the predictor is chosen by the learning algorithm using the training set S, one\nneed to make sure that this bound holds for the set all predictors that could be chosen by the learning\nalgorithm. It is therefore preferred to limit the search space of the learning algorithm to a small enough\nset F of predictors called model class to be able to bound the generalization.\nWe consider the statistical capacity of a model class in terms of the number of examples required to\nensure generalization, i.e. that the population (or test error) is close to the training error, even when\nminimizing the training error. This also roughly corresponds to the maximum number of examples on\nwhich one can obtain small training error even with random labels.\nIn the next section, we deﬁne a meta-model class of feedforward networks with shared weights that\ninclude several well-known model classes such as fully connected, convolutional and recurrent neural\nnetworks.\n2.2\nFeedforward Neural Networks with Shared Weights\nWe denote a feedforward network by a triple (G, w, σ) where G = (V, E) is a directed acyclic graph\nover the set of nodes V that corresponds to units v ∈V in the network, including special input nodes\nVin ⊂V with no incoming edges and special output nodes Vout ⊆V with no outgoing edges, w : E →R\nis the weights assigned to the edges and σ : R →R is an activation function.\nFeedforward network (G, w, σ) computes the function fG,w,σ : Rnin →Rnout for a given input vector\nx ∈Rnin as follows: For any input node v ∈Vin, its output hv is the corresponding coordinate of x 1; for\nany internal node v (all nodes except the input and output nodes) the output value is deﬁned according to\nthe forward propagation equation:\nhv = σ\n\n\nX\n(u→v)∈E\nwu→v · hu\n\n\n(2.2.1)\nand for any output node v ∈Vout, no non-linearity is applied and its output hv = P\n(u→v)∈E wu→v · hu\ncorresponds to coordinates of the computed function fG,w,σ(x). When the graph structure G and the\nactivation function σ is clear from the context, we use the shorthand fw = fG,w,σ to refer to the function\ncomputed by weights w.\nWe will focus mostly on the hinge, or RELU (REctiﬁed Linear Unit) activation, which is currently in\npopular use [15, 16, 17], σRELU(z) = [z]+ = max(z, 0). When the activation will not be speciﬁed, we\nwill implicitly be referring to the RELU. The RELU has several convenient properties which we will\nexploit, some of them shared with other activation functions:\nLipshitz ReLU is Lipschitz continuous with Lipschitz constant one. This property is also shared by the\nsigmoid and the ramp activation σ(z) = min(max(0, z), 1).\nIdempotency ReLU is idempotent, i.e. σRELU(σRELU(z)) = σRELU(z). This property is also shared by\nthe ramp and hard threshold activations.\nNon-Negative Homogeneity For a non-negative scalar c ≥0 and any input z ∈R we have σRELU(c ·\nz) = c · σRELU(z). This property is important as it allows us to scale the incoming weights to a\nunit by c > 0 and scale the outgoing edges by 1/c without changing the function computed by the\nnetwork. For layered graphs, this means we can scale Wi by c and compensate by scaling Wi+1\nby 1/c.\n1We might want to also add a special “bias” node with hbias = 1, or just rely on the inputs having a ﬁxed “bias coordinate”.\n16\nInput nodes\nInternal nodes\nOutput nodes\nFF (shared weights)\nhv = x[v]\nhv = σ\n\u0010P\n(u→v)∈E wu→vhu\n\u0011\nhv = P\n(u→v)∈E wu→vhu\nFCNN notation\nh0 = x\nhi = σ\n\u0000Wihi−1\u0001\nhd = Wdhd−1\nRNN notation\nh0\nt = xt, hi\n0 = 0\nhi\nt = σ\n\u0000Wi\ninhi−1\nt\n+ Wi\nrechi\nt−1\n\u0001\nhd\nt = Wouthd−1\nt\nTable 2.1: Forward computations for feedforward nets with shared weights.\nWhen investigating a class of feedforward networks, in order to account for weight sharing, we separate\nthe weights from actual parameters. Given a parameter vector θ ∈Rn\nparam and a mapping π : E →\n{1, . . . , nparam} from edges to parameter indices, the weight of any edge e ∈E is we = θπ(e). We also\nrefer to the set of edges that share the ith parameter θi as Ei = {e ∈E|π(e) = i}. That is, for any\ne1, e2 ∈Ei, π(e1) = π(e2) and therefore we1 = we2 = θπ(e1). Given a graph G, activation function\nσ and mapping π, we consider the hypothesis class FG,σ,π =\n\b\nfG,w,σ|θ ∈Rk; ∀e∈E w(e) = θπ(e)\n\t\nof functions computable using some setting of parameters. When π is a one-to-one mapping, we use\nweights w to refer to the parameters θ and drop π and use FG,σ to refer to the hypothesis class.\nWe will refer to the size of the network, which is the overall number of edges nedge = |E|, the depth d of\nthe network, which is the length of the longest directed path in G, and the in-degree (or width) H of a\nnetwork, which is the maximum in-degree of a vertex in G.\nIf the mapping π is a one-to-one mapping, then there is no weight sharing and it corresponds to standard\nfeedforward networks. Fully connected neural networks (FCNNs) are a well-known family of standard\nfeedforward networks in which every hidden unit in each layer is connected to all hidden units in the\nprevious and next layers. On the other hand, weight sharing exists if π is a many-to-one mapping. Two\nwell-known examples of feedforward networks with shared weights are convolutional neural networks\n(CNNs) and recurrent neural networks (RNNs). We mostly use the general notation of feedforward\nnetworks with shared weights as this will be more comprehensive. However, when focusing on FCNNs\nor RNNs, it is helpful to discuss them using a more familiar notation which we brieﬂy introduce next.\nFully Connected Neural Networks\nLet us consider a layered fully-connected network where nodes\nare partitioned into layers. Let ni be the number of nodes in layer i. For all nodes v on layer i, we recover\nthe layered recursive formula hi = σ\n\u0000Wihi−1\u0001\nwhere hi ∈Rni is the vector of outputs in layer i and\nWi ∈Rni×ni−1 is the weight matrix in layer i with entries wu→v, for each u in layer i −1 and v in\nlayer i. This description ignores the bias term, which could be modeled as a direct connection from vbias\ninto every node on every layer, or by introducing a bias unit (with output ﬁxed to 1) at each layer.\nRecurrent Neural Networks\nTime-unfolded RNNs are feedforward networks with shared weights\nthat map an input sequence to an output sequence. Each input node corresponds to either a coordinate of\nthe input vector at a particular time step or a hidden unit at time 0. Each output node also corresponds to\na coordinate of the output at a speciﬁc time step. Finally, each internal node refers to some hidden unit\nat time t ≥1. When discussing RNNs, it is useful to refer to different layers and the values calculated\nat different time-steps. We use a notation for RNN structures in which the nodes are partitioned into\nlayers and hi\nt denotes the output of nodes in layer i at time step t. Let x = (x1, . . . , xT ) be the input\nat different time steps where T is the maximum number of propagations through time and we refer to\nit as the length of the RNN. For 0 ≤i < d, let Wi\nin ∈Rni×ni−1 and Wi\nrec ∈Rni×ni be the input and\nrecurrent parameter matrices of layer i and Wout ∈Rnd×nd−1 be the output parameter matrix.The output\nof the function implemented by RNN can then be calculated as fw,t(x) = hd\nt . Note that in this notations,\nweight matrices Win, Wrec and Wout correspond to “free” parameters of the model that are shared in\ndifferent time steps. Table 2.1 shows forward computations for layered feedforward networks and RNNs.\n17\nPart I\nImplicit Regularization and\nGeneralization\n18\nChapter 3\nGeneralization and Capacity Control\nIn section 2.1 we brieﬂy discussed viewing the statistical capacity of a model class in terms of the\nnumber of examples required to ensure generalization. Given a model class F, such as all the functions\nrepresentable by some feedforward or convolutional networks, one can consider the capacity of the entire\nclass F—this corresponds to learning with a uniform “prior” or notion of complexity over all models in\nthe class. Alternatively, we can also consider some complexity measure, which we take as a mapping that\nassigns a non-negative number to every predictor in the class - µ : {F, S} →R+, where S is the training\nset. It is then sufﬁcient to consider the capacity of the restricted class Fµ,α = {f : f ∈F, µ(f) ≤α}\nfor a given α ≥0. One can then ensure generalization of a learned predictor f in terms of the capacity\nof Fµ,µ(f). Having a good predictor with low complexity, and being biased toward low complexity (in\nterms of µ) can then be sufﬁcient for learning, even if the capacity of the entire F is high. And if we\nare indeed relying on µ for ensuring generalization (and in particular, biasing toward models with lower\ncomplexity under µ), we would expect a learned f with a lower value of µ(f) to generalize better.\nFor some complexity measures, we allow µ to depend also on the training set. If this is done carefully,\nwe can still ensure generalization for the restricted class Fµ,α.\nWhen considering a complexity measure µ, we can investigate whether it is sufﬁcient for generalization,\nand analyze the capacity of Fµ,α. Understanding the capacity corresponding to different complexity\nmeasures also allows us to relate between different measures and provides guidance as to what and\nhow we should measure: From the above discussion, it is clear that any monotone transformation of a\ncomplexity measures leads to an equivalent notion of complexity. Furthermore, complexity is meaningful\nonly in the context of a speciﬁc model class F, e.g. speciﬁc architecture or network size. The capacity, as\nwe consider it (in units of sample complexity), provides a yardstick by which to measure complexity (we\nshould be clear though, that we are vague regarding the scaling of the generalization error itself, and only\nconsider the scaling in terms of complexity and model class, thus we obtain only a very crude yardstick\nsufﬁcient for investigating trends and relative phenomena, not a quantitative yardstick).\nWe next look at different ways of controlling the capacity.\n3.1\nVC Dimension: A Cardinality-Based Arguments\nConsider a ﬁnite model class F. Given any predictor f ∈F, training error L(f) is the average of\nindependent random variables and the expected error the excepted value of the training error. We can\ntherefore use Hoeffding’s inequality upper bounds the generalization error with high probability:\nP\nh\nL(f) −ˆL(f) ≥t\ni\n≤e−2mt2\n(3.1.1)\n19\nThe above bound is for any given f. However, since the learning algorithm can output any predictor from\nclass F, we need to make sure that all predictors in F have low generalization error which can be done\nthrough a union bound over model class F:\nP\nh\n∃f∈F L(f) −ˆL(f) ≥t\ni\n≤\nX\nf∈F\nP\nh\nL(f) −ˆL(f) ≥t\ni\n≤|F|e−2mt2\n(3.1.2)\nSetting the r.h.s. of the above inequality to small probability 0 < δ < 1, we can say that with probability\n1 −δ over the choice of samples in the training set, the following generalization bound holds:\nL(f) ≤ˆL(f) +\nr\nln |F| + ln(1/δ)\nm\n(3.1.3)\nThe above simple yet effective approach gives us an intuition about the relationship between the capacity\nand generalization. Many of the approaches of controlling the capacity that we will study later follow\nsimilar arguments. Here, the term ln |F| corresponds to the complexity of the model class.\nEven though many model classes that we consider are not ﬁnite based on the deﬁnition, one can argue\nthat all parametrized model classes used in practice are ﬁnite since the parameters are stored with ﬁnite\nprecision For any model, if b bits are used to store each parameter, then we have ln |F| ≤bnparam which\nis is linear in the total number of parameters.\nEven without making an assumption on the precision of parameters, it is possible to get similar generaliza-\ntion bound using Vapnik-Chervonenkis dimension (VC dimension) which can be thought as the logarithm\nof the“intrinsic” cardinality. VC-dimension is deﬁned as the size of the largest set W = {xi}m\ni=1 such\nthat for any mapping g : W →{±}m, there is a predictor in F that achieves zero training error on the\ntraining set S = {(xi, g(xi) | xi ∈W}. The VC-dimension of many known model classes is a linear or\nlow-degree polynomial of the number of parameters. The following generalization bound then holds\nwith probability 1 −δ [18, 19]:\nL(f) ≤ˆL(f) + O\n r\nVC-dim(F) ln m + ln(1/δ)\nm\n!\n(3.1.4)\nFeedforward Networks\nThe VC dimension of feedforward networks can also be bounded in terms\nof the number of parameters nparam[20, 21, 22, 23]. In particular, Bartlett [24] and Harvey et al. [25],\nfollowing Bartlett et al. [22], give the following tight (up to logarithmic factors) bound on the VC\ndimension and hence capacity of feedforward networks with ReLU activations:\nVC-dim = ˜O(d ∗nparam)\n(3.1.5)\nIn the over-parametrized settings, where the number of parameters is more than the number of samples,\ncomplexity measures that depend on the total number of parameters are too weak and cannot explain the\ngeneralization behavior. Neural networks used in practice often have signiﬁcantly more parameters than\nsamples, and indeed can perfectly ﬁt even random labels, obviously without generalizing [26]. Moreover,\nmeasuring complexity in terms of number of parameters cannot explain the reduction in generalization\nerror as the number of hidden units increase [27]. We will discuss more details about network size as the\ncapacity control in Chapter 4.\n3.2\nNorms and Margins: Counting Real-Valued Functions\nThe model classes that we learn are often functions with real-valued outputs and for each task, we\nuse a different loss and prediction method based on the predicted scores. For example, for the binary\n20\nclassiﬁcation, thresholding the only real-valued output gives us the binary labels. For the multi-class\nclassiﬁcation, the output dimension is usually equal to the number of classes and the class with maximum\nscore is chosen as the predicted label. For simplicity, we focus on binary classiﬁcation here. Since\nthe model class has real-valued output, can not directly use VC-dimension here. Instead, we can use a\nsimilar concept called subgraph VC-dimension which is similar to VC-dimension with the difference\nbeing that here we count the number of different behavior with a given margin. This means for the binary\ncase, we require yf(x) ≥η for some margin η. There are different techniques that bound subgraph-VC\ndimension such as Covering Numbers and Rademacher Complexities. Here, we focus on the Rademacher\nComplexity since most of the results by Covering Numbers can be also proved through Rademacher\ncomplexities with less effort. The empirical Rademacher complexity of a class F of function mapping\nfrom X to R with respect to a set {x1, . . . , xm} is deﬁned as:\nRm(F) = Eξ∈{±1}m\n\"\n1\nm sup\nf∈F\n\f\f\f\f\f\nm\nX\ni=1\nξif(xi)\n\f\f\f\f\f\n#\n(3.2.1)\nThe relationship between Rademacher complexity and subgraph VC-dimension is as follows:\nRm(F) = O\n r\nVC-dim(F)\nm\n!\n(3.2.2)\nIt is possible to get the following generalization error for any margin γ > 0 with probability 1 −δ over\nthe choice of training examples for every f ∈F:\nL0(f) ≤ˆLγ(f) + 2Rm(F)\nγ\n+\nr\n8 ln(2/δ)\nm\n(3.2.3)\nFeedforward Networks\n[28] proved that the Rademacher complexity of fully connected feedforward\nnetworks on set S can be bounded based on the ℓ1 norm of the weights of hidden units in each layer as\nfollows:\nRm(F) ≤\ns\n4d ln (nin) Qd\ni=1 ∥Wi∥2\n1,∞maxx∈S ∥x∥∞\nm\n(3.2.4)\nwhere ∥Wi∥1,∞is the maximum over hidden units in layer i of the ℓ1 norm of incoming weights to the\nhidden unit [28]. This suggests that the capacity scales roughly as Qd\ni=1 ∥Wi∥2\n1,∞. In Chapter 5 we show\nhow the capacity can be controlled for a large family of norms.\n3.3\nRobustness: Lipschitz Continuity with Respect to Input\nSome of the measures/norms also control the Lipschitz constant of the model class with respect to its\ninput such as the capacity based on (3.2.4). Is the capacity control achieved through the bound on the\nLipschitz constant? Is bounding the Lipschitz constant alone enough for generalization? To answer these\nquestions, and in order to understand capacity control in terms of Lipschitz continuity more broadly, we\nreview here the relevant guarantees.\nGiven an input space X and metric M, a function f : X →R on a metric space (X, M) is called a\nLipschitz function if there exists a constant CM, such that |f(x) −f(y)| ≤CMM(x, y). Luxburg and\nBousquet [29] studied the capacity of functions with bounded Lipschitz constant on metric space (X, M)\nwith a ﬁnite diameter diamM(X) = supx,y∈X M(x, y) and showed that the capacity is proportional\nto\n\u0010\nCM\nγ\n\u0011n\ndiamM(X) where γ is the margin. This capacity bound is weak as it has an exponential\ndependence on input size.\n21\nAnother related approach is through algorithmic robustness as suggested by Xu and Mannor [30]. Given\nϵ > 0, the model fw found by a learning algorithm is K robust if X can be partitioned into K disjoint\nsets, denoted as {Ci}K\ni=1, such that for any pair (x, y) in the training set s ,1\nx, z ∈Ci ⇒|ℓ(w, x) −ℓ(w, z)| ≤ϵ\n(3.3.1)\nXu and Mannor [30] showed the capacity of a model class whose models are K-robust scales as K. For\nthe model class of functions with bounded Lipschitz C∥.∥, K is proportional to\nC∥.∥\nγ -covering number\nof the input domain X under norm ∥.∥where γ is the margin to get error ϵ. However, the covering\nnumber of the input domain can be exponential in the input dimension and the capacity can still grow as\n\u0010 C∥.∥\nγ\n\u0011n 2.\nFeedforward Networks\nReturning to our original question, the Cℓ∞and Cℓ2 Lipschitz constants of the\nnetwork can be bounded by Qd\ni=1 ∥Wi∥1,∞(hence ℓ1-path norm) and Qd\ni=1 ∥Wi∥2, respectively [30, 31].\nThis will result in a very large capacity bound that scales as\n\u0010 Qd\ni=1∥Wi∥2\nγ\n\u0011n\n, which is exponential in both\nthe input dimension and depth of the network. This shows that simply bounding the Lipschitz constant of\nthe network is not enough to get a reasonable capacity control and the capacity bounds of the previous\nSection are not merely a consequence of bounding the Lipschitz constant.\n3.4\nPAC-Bayesian Framework: Sharpness with Respect to Param-\neters\nThe notion of sharpness as a generalization measure was recently suggested by Keskar et al. [13] and\ncorresponds to robustness to adversarial perturbations on the parameter space:\nζα(W) = max|ui|≤α(|wi|+1) ˆL(fw+u) −ˆL(fw)\n1 + ˆL(fw)\n≃\nmax\n|ui|≤α(|wi|+1)\nˆL(fw+u) −ˆL(fw),\n(3.4.1)\nwhere the training error ˆL(fw) is generally very small in the case of neural networks in practice, so we\ncan simply drop it from the denominator without a signiﬁcant change in the sharpness value.\nInstead, we advocate viewing a related notion of expected sharpness in the context of the PAC-Bayesian\nframework. Viewed this way, it becomes clear that sharpness controls only one of two relevant terms,\nand must be balanced with some other measure such as norm. Together, sharpness and norm do provide\ncapacity control and can explain many of the observed phenomena. This connection between sharpness\nand the PAC-Bayes framework was also recently noted by Dziugaite and Roy [32].\nThe PAC-Bayesian framework [33, 34] provides guarantees on the expected error of a randomized\npredictor (hypothesis), drawn from a distribution denoted Q and sometimes referred to as a “posterior”\n(although it need not be the Bayesian posterior), that depends on the training data. Let fw be any\npredictor (not necessarily a neural network) learned from training data. We consider a distribution Q over\npredictors with weights of the form w + u, where w is a single predictor learned from the training set,\nand u is a random variable. Then, given a “prior” distribution P over the hypothesis that is independent\nof the training data, with probability at least 1 −δ over the draw of the training data, the expected error\n1Xu and Mannor [30] have deﬁned the robustness as a property of learning algorithm given the model class and the training set.\nHere since we are focused on the learned model, we introduce it as a property of the model.\n2Similar to margin-based bounds, we drop the term that depends on the diameter of the input space.\n22\nof fw+u can be bounded as follows [35]:\nEu [L(fw+u)] ≤Eu\nh\nˆL(fw+u)\ni\n+\nr\nEu\nh\nˆL(fw+u)\ni\nK + K\n(3.4.2)\nwhere K =\n2(KL(w+u∥P )+ln 2m\nδ )\nm−1\n. When the training loss Eu\nh\nˆL(fw+u)\ni\nis smaller than K, then the\nlast term dominates. This is often the case for neural networks with small enough perturbation. One can\nalso get the the following weaker bound:\nEu [L(fw+u)] ≤Eu\nh\nˆL(fw+u)\ni\n+ 2\ns\n2\n\u0000KL (w + u∥P) + ln 2m\nδ\n\u0001\nm −1\n(3.4.3)\nThe above inequality clearly holds for K ≥1 and for K < 1 it can be derived from Equation (3.4.2) by\nupper bounding the loss in the second term by 1. We can rewrite the above bound as follows:\nEu [L(fw+u)] ≤ˆL(fw) + Eu\nh\nˆL(fw+u)\ni\n−ˆL(fw)\n|\n{z\n}\nexpected sharpness\n+2\ns\n2\nm −1\n\u0012\nKL (w + u∥P) + ln 2m\nδ\n\u0013\n(3.4.4)\nAs we can see, the PAC-Bayes bound depends on two quantities - i) the expected sharpness and ii) the\nKullback Leibler (KL) divergence to the “prior” P. The bound is valid for any distribution measure P,\nany perturbation distribution u and any method of choosing w dependent on the training set.\nNext, we present a result that gives a margin-based generalization bound using the PAC-Bayesian\nframework. The proof of the lemma uses similar ideas as in the proof for the case of linear separators,\ndiscussed by Langford and Shawe-Taylor [36] and McAllester [35]. This is a general result that holds for\nany hypothesis class and not speciﬁc to neural networks.\nLemma 1. Let fw(x) : X →Rk be any predictor (not necessarily a neural network) with parameters\nw and P be any distribution on the parameters that is independent of the training data. For any γ > 0,\nconsider any set Sw of perturbations with the following property:\nSw ⊆\n\u001a\nw + u\n\f\f\f\f max\nx∈X |fw+u(x) −fw(x)|∞< γ\n4\n\u001b\nLet u be a random variable such that P [u ∈Sw] ≥1\n2. Then, for any δ > 0, with probability 1 −δ over\nthe training set, the generalization error can be bounded as follows:\nL0(fw) ≤ˆLγ(fw) + 4\ns\nKLSw (w + u∥P) + ln 4m\nδ\nm −1\nwhere KLSw(Q||P) =\nR\nSw q(x) ln q(x)\np(x)dx.\nProof. Let q be the probability density function for w + u. We consider the distribution ˜Q with the\nfollowing probability density function:\n˜q(r) = 1\nZ\n(\nq(r)\nr ∈Sw\n0\notherwise.\nwhere Z is a normalizing constant and by the lemma assumption Z = P [w + u ∈Sw] ≥1\n2. Therefore,\nwe have:\nKL( ˜Q∥P) =\nZ\n˜q(r) ln ˜q(r)\np(r)dr ≤2\nZ\nSw\nq(r) ln q(r)\np(r)dr + 1\n(3.4.5)\n23\nConsider w + ˜u to be the random perturbation centered at w drawn from ˜Q. By the deﬁnition of ˜Q, we\nknow that for any perturbation ˜u:\nmax\nx∈X |fw+˜u(x) −fw(x)|∞< γ\n4\n(3.4.6)\nTherefore, the perturbation ˜u can change the margin between two output units of fw by at most γ\n2 ; i.e.\nfor any perturbation ˜u drawn from ˜Q:\nmax\ni,j∈[k],x∈X |(|fw+˜u(x)[i] −fw+˜u(x)[j]|) −(|fw(x)[i] −fw(x)[j]|)| < γ\n2\nSince the above bound holds for any x in the domain X, we can get the following inequalities:\nL0(fw) ≤L γ\n2 (fw+˜u)\nˆL γ\n2 (fw+˜u) ≤ˆLγ(fw)\nNow using the above inequalities together with the equation (3.4), with probability 1−δ over the training\nset we have:\nL0(fw) ≤E˜u\nh\nL γ\n2 (fw+˜u)\ni\n≤E˜u\nh\nˆL γ\n2 (fw+˜u)\ni\n+ 2\ns\n2(KL (w + ˜u∥P) + ln 2m\nδ )\nm −1\n≤ˆLγ(fw) + 2\ns\n2(KL (w + ˜u∥P) + ln 2m\nδ )\nm −1\n≤ˆLγ(fw) + 4\ns\nKLSw (w + u∥P) + ln 4m\nδ\nm −1\n,\nFeedforward Networks\nThis connection between sharpness and the PAC-Bayesian framework was\nalso recently noticed by Dziugaite and Roy [32], who optimize the PAC-Bayes generalization bound\nover a family of multivariate Gaussian distributions, extending the work of Langford and Caruana [37].\nThey show that the optimized PAC-Bayes bounds are numerically non-vacuous for feedforward networks\ntrained on a binary classiﬁcation variant of MNIST dataset.\n24\nChapter 4\nOn the Role of Implicit Regularization\nin Generalization\nCentral to any form of learning is an inductive bias that induces some sort of capacity control (i.e. restricts\nor encourages predictors to be “simple” in some way), which in turn allows for generalization. The\nsuccess of learning then depends on how well the inductive bias captures reality (i.e. how expressive is the\nhypothesis class of “simple” predictors) relative to the capacity induced, as well as on the computational\ncomplexity of ﬁtting a “simple” predictor to the training data.\nLet us consider learning with feed-forward networks from this perspective. If we search for the weights\nminimizing the training error, we are essentially considering the hypothesis class of predictors repre-\nsentable with different weight vectors, typically for some ﬁxed architecture. We showed in Section 3.1\nthat the capacity can then be controlled by the size (number of weights) of the network. Our justiﬁcation\nfor using such networks is then that many interesting and realistic functions can be represented by\nnot-too-large (and hence bounded capacity) feed-forward networks. Indeed, in many cases we can show\nhow speciﬁc architectures can capture desired behaviors. More broadly, any O(T) time computable\nfunction can be captured by an O(T 2) sized network, and so the expressive power of such networks is\nindeed great [6, Theorem 9.25].\nAt the same time, we also know that learning even moderately sized networks is computationally\nintractable—not only is it NP-hard to minimize the empirical error, even with only three hidden units,\nbut it is hard to learn small feed-forward networks using any learning method (subject to cryptographic\nassumptions). That is, even for binary classiﬁcation using a network with a single hidden layer and a\nlogarithmic (in the input size) number of hidden units, and even if we know the true targets are exactly\ncaptured by such a small network, there is likely no efﬁcient algorithm that can ensure error better than\n1/2 [10, 11]—not if the algorithm tries to ﬁt such a network, not even if it tries to ﬁt a much larger\nnetwork, and in fact no matter how the algorithm represents predictors. And so, merely knowing that\nsome not-too-large architecture is excellent in expressing reality does not explain why we are able to learn\nusing it, nor using an even larger network. Why is it then that we succeed in learning using multilayer\nfeed-forward networks? Can we identify a property that makes them possible to learn? An alternative\ninductive bias?\nHere, we make our ﬁrst steps at shedding light on this question by going back to our understanding of\nnetwork size as the capacity control at play.\nOur main observation, based on empirical experimentation with single-hidden-layer networks of increas-\ning size (increasing number of hidden units), is that size does not behave as a capacity control parameter,\nand in fact there must be some other, implicit, capacity control at play. We suggest that this hidden\ncapacity control might be the real inductive bias when learning with deep networks.\n25\nIn order to try to gain an understanding at the possible inductive bias, we draw an analogy to matrix\nfactorization and understand dimensionality versus norm control there. Based on this analogy we suggest\nthat implicit norm regularization might be central also for deep learning, and also there we should think\nof bounded-norm models with capacity independent of number of hidden units.\n4.1\nNetwork Size and Generalization\nConsider training a feedforward network by ﬁnding the weights minimizing the training error. Speciﬁcally,\nwe will consider a fully connected feedforward networks with one hidden layer that includes H hidden\nunits. The weights learned by minimizing a soft-max cross entropy loss 1 on n labeled training examples.\nThe total number of weights is then H(|Vin| + |Vout|).\nWhat happens to the training and test errors when we increase the network size H? The training\nerror will necessarily decrease. The test error might initially decrease as the approximation error is\nreduced and the network is better able to capture the targets. However, as the size increases further, we\nloose our capacity control and generalization ability, and should start overﬁtting. This is the classic\napproximation-estimation tradeoff behavior.\nConsider, however, the results shown in Figure 4.1, where we trained networks of increasing size on the\nMNIST and CIFAR-10 datasets. Training was done using stochastic gradient descent with momentum\nand diminishing step sizes, on the training error and without any explicit regularization. As expected,\nboth training and test error initially decrease. More surprising is that if we increase the size of the\nnetwork past the size required to achieve zero training error, the test error continues decreasing! This\nbehavior is not at all predicted by, and even contrary to, viewing learning as ﬁtting a hypothesis class\ncontrolled by network size. For example for MNIST, 32 units are enough to attain zero training error.\nWhen we allow more units, the network is not ﬁtting the training data any better, but the estimation error,\nand hence the generalization error, should increase with the increase in capacity. However, the test error\ngoes down. In fact, as we add more and more parameters, even beyond the number of training examples,\nthe generalization error does not go up.\nWe also further tested this phenomena under some artiﬁcial mutilations to the data set. First, we wanted\nto artiﬁcially ensure that the approximation error was indeed zero and does not decrease as we add more\nunits. To this end, we ﬁrst trained a network with a small number H0 of hidden units (H0 = 4 on MNIST\nand H0 = 16 on CIFAR) on the entire dataset (train+test+validation). This network did have some\ndisagreements with the correct labels, but we then switched all labels to agree with the network creating\na “censored” data set. We can think of this censored data as representing an artiﬁcial source distribution\nwhich can be exactly captured by a network with H0 hidden units. That is, the approximation error is\nzero for networks with at least H0 hidden units, and so does not decrease further. Still, as can be seen in\nthe middle row of Figure 4.2, the test error continues decreasing even after reaching zero training error.\nNext, we tried to force overﬁtting by adding random label noise to the data. We wanted to see whether\nnow the network will use its higher capacity to try to ﬁt the noise, thus hurting generalization. However,\n1When using soft-max cross-entropy, the loss is never exactly zero for correct predictions with ﬁnite margins/conﬁdences.\nInstead, if the data is seperable, in order to minimize the loss the weights need to be scaled up toward inﬁnity and the cross entropy\nloss goes to zero, and a global minimum is never attained. In order to be able to say that we are actually reaching a zero loss solution,\nand hence a global minimum, we use a slightly modiﬁed soft-max which does not noticeably change the results in practice. This\ntruncated loss returns the same exact value for wrong predictions or correct prediction with conﬁdences less than a threshold but\nreturns zero for correct predictions with large enough margins: Let {si}k\ni=1 be the scores for k possible labels and c be the correct\nlabels. Then the soft-max cross-entropy loss can be written as ℓ(s, c) = ln P\ni exp(si −sc) but we instead use the differentiable\nloss function ˆℓ(s, c) = ln P\ni f(si −sc) where f(x) = exp(x) for x ≥−11 and f(x) = exp(−11)[x + 13]2\n+/4 otherwise.\nTherefore, we only deviate from the soft-max cross-entropy when the margin is more than 11, at which point the effect of this\ndeviation is negligible (we always have\n\f\f\fℓ(s, c) −ˆℓ(s, c)\n\f\f\f ≤0.000003k)—if there are any actual errors the behavior on them\nwould completely dominate correct examples with margin over 11, and if there are no errors we are just capping the amount by\nwhich we need to scale up the weights.\n26\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\n(a) MNIST\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\n(b) CIFAR10\nFigure 4.1: The training error and the test error based on different stopping criteria when 2-layer NNs with different\nnumber of hidden units are trained on MNIST and CIFAR-10. Images in both datasets are downsampled to 100 pixels.\nThe size of the training set is 50000 for MNIST and 40000 for CIFAR-10. The early stopping is based on the error\non a validation set (separate from the training and test sets) of size 10000. The training was done using stochastic\ngradient descent with momentum and mini-batches of size 100. The network was initialized with weights generated\nrandomly from the Gaussian distribution. The initial step size and momentum were set to 0.1 and 0.5 respectively.\nAfter each epoch, we used the update rule µ(t+1) = 0.99µ(t) for the step size and m(t+1) = min{0.9, m(t) +0.02}\nfor the momentum.\nas can be seen in the bottom row of Figure 4.2, even with ﬁve percent random labels, there is no\nsigniﬁcant overﬁtting and test error continues decreasing as network size increases past the size required\nfor achieving zero training error.\nWhat is happening here? A possible explanation is that the optimization is introducing some implicit\nregularization. That is, we are implicitly trying to ﬁnd a solution with small “complexity”, for some\nnotion of complexity, perhaps norm. This can explain why we do not overﬁt even when the number of\nparameters is huge. Furthermore, increasing the number of units might allow for solutions that actually\nhave lower “complexity”, and thus generalization better. Perhaps an ideal then would be an inﬁnite\nnetwork controlled only through this hidden complexity.\nWe want to emphasize that we are not including any explicit regularization, neither as an explicit penalty\nterm nor by modifying optimization through, e.g., drop-outs, weight decay, or with one-pass stochastic\nmethods. We are using a stochastic method, but we are running it to convergence—we achieve zero\nsurrogate loss and zero training error. In fact, we also tried training using batch conjugate gradient\ndescent and observed almost identical behavior. But it seems that even still, we are not getting to some\nrandom global minimum—indeed for large networks the vast majority of the many global minima of the\ntraining error would horribly overﬁt. Instead, the optimization is directing us toward a “low complexity”\nglobal minimum.\nAlthough we do not know what this hidden notion of complexity is, as a ﬁnal experiment we tried to\nsee the effect of adding explicit regularization in the form of weight decay. The results are shown in the\ntop row of ﬁgure 4.2. There is a slight improvement in generalization but we still see that increasing the\nnetwork size helps generalization.\n27\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\n0.2\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\nTest (regularization)\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\nTest (regularization)\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\n0.2\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\n0.2\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\n4\n8\n16\n32\n64\n128\n256\n512\n1K\n2K\n4K\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nH\nError\n \n \nTraining\nTest (at convergence)\nTest (early stopping)\nSmall MNIST\nSmall CIFAR-10\nFigure 4.2: The training error and the test error based on different stopping criteria when 2-layer NNs with\ndifferent number of hidden units are trained on small subsets of MNIST and CIFAR-10. Images in both datasets are\ndownsampled to 100 pixels. The sizes of the training and validation sets are 2000 for both MNIST and CIFAR-10\nand the early stopping is based on the error on the validation set. The top plots are the errors for the original datasets\nwith and without explicit regularization.The best weight decay parameter is chosen based on the validation error. The\nmiddle plots are on the censored data set that is constructed by switching all the labels to agree with the predictions\nof a trained network with a small number H0 of hidden units H0 = 4 on MNIST and H0 = 16 on CIFAR-10) on\nthe entire dataset (train+test+validation). The plots on the bottom are also for the censored data except we also add 5\npercent noise to the labels by randomly changing 5 percent of the labels. The optimization method is the same as the\nin Figure 1. The results in this ﬁgure are the average error over 5 random repetitions.\n28\n4.2\nA Matrix Factorization Analogy\nTo gain some understanding at what might be going on, let us consider a slightly simpler model which\nwe do understand much better. Instead of rectiﬁed linear activations, consider a feed-forward network\nwith a single hidden layer, and linear activations, i.e.:\nfU,V(x) = UVx\n(4.2.1)\nThis is of course simply a matrix-factorization model, where fW(x) = Wx and W = VU. Controlling\ncapacity by limiting the number of hidden units exactly corresponds to constraining the rank of W,\ni.e. biasing toward low dimensional factorizations. Such a low-rank inductive bias is indeed sensible,\nthough computationally intractable to handle with most loss functions.\nHowever, in the last decade we have seen much success for learning with low norm factorizations. In\nsuch models, we do not constrain the inner dimensionality H of U, V, and instead only constrain, or\nregularize, their norm. For example, constraining the Frobenius norm of U and V corresponds to using\nthe trace-norm as an inductive bias [38]:\n∥W∥tr =\nmin\nW=VU\n1\n2(∥U∥2\nF + ∥V∥2\nF ).\n(4.2.2)\nOther norms of the factorization lead to different regularizers.\nUnlike the rank, the trace-norm (as well as other factorization norms) is convex, and leads to tractable\nlearning problems [39, 38]. In fact, even if learning is done by a local search over the factor matrices U\nand V (i.e. by a local search over the weights of the network), if the dimensionality is high enough and\nthe norm is regularized, we can ensure convergence to a global minima [40]. This is in stark contrast\nto the dimensionality-constrained low-rank situation, where the limiting factor is the number of hidden\nunits, and local minima are abundant [41].\nFurthermore, the trace-norm and other factorization norms are well-justiﬁed as sensible inductive biases.\nWe can ensure generalization based on having low trace-norm, and a low-trace norm model corresponds\nto a realistic factor model with many factors of limited overall inﬂuence. In fact, empirical evidence\nsuggests that in many cases low-norm factorization are a more appropriate inductive bias compared to\nlow-rank models.\nWe see, then, that in the case of linear activations (i.e. matrix factorization), the norm of the factorization\nis in a sense a better inductive bias than the number of weights: it ensures generalization, it is grounded\nin reality, and it explain why the models can be learned tractably.\nRecently, Gunasekar et al. [42] provided empirical and theoretical evidence on the implicit regularization\nof gradient descent for matrix factorization. They showed that gradient descent on the full dimensional\nfactorizations without any explicit regularization indeed converges to the minimum trace norm solution\nwith initialization close enough to the origin and small enough step size.\nLet us interpret the experimental results of Section 4.1 in this light. Perhaps learning is succeeding not\nbecause there is a good representation of the targets with a small number of units, but rather because\nthere is a good representation with small overall norm, and the optimization is implicitly biasing us\ntoward low-norm models. Such an inductive bias might potentially explain both the generalization ability\nand the computational tractability of learning, even using local search.\nUnder this interpretation, we really should be using inﬁnite-sized networks, with an inﬁnite number of\nhidden units. Fitting a ﬁnite network (with implicit regularization) can be viewed as an approximation\nto ﬁtting the “true” inﬁnite network. This situation is also common in matrix factorization: e.g., a very\nsuccessful approach for training low trace-norm models, and other inﬁnite-dimensional bounded-norm\nfactorization models, is to approximate them using a ﬁnite dimensional representation [43, 44]. The\nﬁnite dimensionality is then not used at all for capacity (statistical complexity) control, but purely for\n29\ncomputational reasons. Indeed, increasing the allowed dimensionality generally improves generalization\nperformance, as it allows us to better approximate the true inﬁnite model. Inspired by these experiments,\nin order to understand the implicit regularization in deep learning, we will next look at ways of controlling\nthe capacity independent of the number of hidden units.\n30\nChapter 5\nNorm-based Capacity Control\nAs we discussed in Section 3.1 statistical complexity, or capacity, of unregularized feed-forward neural\nnetworks, as a function of the network size and depth, is fairly well understood. But feedforward\nnetworks are often trained with some kind of regularization, such as weight decay, early stopping, “max\nregularization”, or more exotic regularization such as drop-outs. We also showed in Chapter 4 that even\nwithout any explicit regularization, the capacity of neural networks is being controlled by a form of\nimplicit regularization caused by optimization which does not depend on the size of the network. What is\nthe effect of such regularization on the induced model class and its capacity?\nFor linear prediction (a one-layer feed-forward network) we know that using regularization the capacity\nof the class can be bounded only in terms of the norms, with no (or a very weak) dependence on the\nnumber of edges (i.e. the input dimensionality or number of linear coefﬁcients). E.g., we understand\nvery well how the capacity of ℓ2-regularized linear predictors can be bounded in terms of the norm alone\n(when the norm of the data is also bounded), even in inﬁnite dimension.\nA central question we ask is: can we bound the capacity of feed-forward network in terms of norm-based\nregularization alone, without relying on network size and even if the network size (number of nodes or\nedges) is unbounded or inﬁnite? What type of regularizers admit such capacity control? And how does\nthe capacity behave as a function of the norm, and perhaps other network parameters such as depth?\nBeyond the central question of capacity control, we also analyze the convexity of the resulting model\nclass—unlike unregularized size-controlled feed-forward networks, inﬁnite magnitude-controlled net-\nworks have the potential of yielding convex model classes (this is the case, e.g., when we move from\nrank-based control on matrices, which limits the number of parameters to magnitude based control with\nthe trace-norm or max-norm). A convex class might be easier to optimize over and might be convenient\nin other ways.\nIn this chapter we focus on two natural types of norm regularization: bounding the norm of the incoming\nweights of each unit (per-unit regularization) and bounding the overall norm of all the weights in\nthe system jointly (overall regularization, e.g. limiting the overall sum of the magnitudes, or square\nmagnitudes, in the system). We generalize both of these with a single notion of group-norm regularization:\nwe take the ℓp norm over the weights in each unit and then the ℓq norm over units. In Section 5.1 we\npresent this regularizer and obtain a tight understanding of when it provides for size-independent capacity\ncontrol and a characterization of when it induces convexity. We then apply these generic results to\nper-unit regularization (Section 5.2) and overall regularization (Section 5.3), noting also other forms\nof regularization that are equivalent to these two. In particular, we show how per-unit regularization is\nequivalent to a novel path-based regularizer and how overall ℓ2 regularization for two-layer networks\nis equivalent to so-called “convex neural networks” [45]. In terms of capacity control, we show that\nper-unit regularization allows size-independent capacity-control only with a per-unit ℓ1-norm, and that\n31\noverall ℓp regularization allows for size-independent capacity control only when p ≤2, even if the depth\nis bounded. In any case, even if we bound the sum of all magnitudes in the system, we show that an\nexponential dependence on the depth is unavoidable.\nAs far as we are aware, prior work on size-independent capacity control for feed-forward networks\nconsidered only per-unit ℓ1 regularization, and per-unit ℓ2 regularization for two-layered networks (see\ndiscussion and references at the beginning of Section 5.2). Recently, Bartlett et al. [46] have shown a\ngeneralization bound based on the product of spectral norm of the layers using covering numbers. In\nChapter 6, we show a simpler prove for a tighter bound. Here, we extend the scope signiﬁcantly, and\nprovide a broad characterization of the types of regularization possible and their properties. In particular,\nwe consider overall norm regularization, which is perhaps the most natural form of regularization used\nin practice (e.g. in the form of weight decay). We hope our study will be useful in thinking about,\nanalyzing and designing learning methods using feed-forward networks. Another motivation for us is that\ncomplexity of large-scale optimization is often related to scale-based, not dimension-based complexity.\nUnderstanding when the scale-based complexity depends exponentially on the depth of a network might\nhelp shed light on understanding the difﬁculties in optimizing deep networks.\nPreliminaries and Notations\nWe denote by Fd,H the class of fully connected feedforward networks\nwith a single output node and use the shorthand Fd = Fd,∞. We will consider various measures µ(w) of\nthe magnitude of the weights w. Such a measure induces a complexity measure on functions f ∈Fd,H\ndeﬁned by µd,H(f) = inffw=f µ(w). The sublevel sets of the complexity measure µd,H form a family\nof hypothesis classes Fd,H\nµ≤a = {f ∈Fd,H | µd,H(f) ≤a}.\nFor binary function g : {±1}nin →±1 we say that g is realized by f with unit margin if ∀xf(x)g(x) ≥1.\nA set of points Sin is shattered with unit margin by a model class F if all g : Sin →±1 can be realized\nwith unit margin by some fw ∈F.\n5.1\nGroup Norm Regularization\nConsidering the grouping of weights going into each edge of the network, we will consider the following\ngeneric group-norm type regularizer, parametrized by 1 ≤p, q ≤∞:\nµp,q(w) =\n\n\n\nX\nv∈V\n\n\nX\n(u→v)∈E\n|wu→v|p\n\n\nq/p\n\n\n1/q\n.\n(5.1.1)\nHere and elsewhere we allow q = ∞with the usual conventions that (P zq\ni )1/q = sup zi and 1/q = 0\nwhen it appears in other contexts. When q = ∞the group regularizer (9.1.1) imposes a per-unit\nregularization, where we constrain the norm of the incoming weights of each unit separately, and when\nq = p the regularizer (9.1.1) is an “overall” weight regularizer, constraining the overall norm of all\nweights in the system. E.g., when q = p = 1 we are paying for the sum of all magnitudes of weights in\nthe network, and q = p = 2 corresponds to overall weight-decay where we pay for the sum of square\nmagnitudes of all weights (i.e. the overall Euclidean norm of the weights).\n32\nFor a layered graph, we have:\nµp,q(w) =\n\n\n\nd\nX\nk=1\nH\nX\ni=1\n\n\nH\nX\nj=1\n\f\fW k[i, j]\n\f\fp\n\n\nq/p\n\n\n1/q\n= d1/q\n \n1\nd\nd\nX\nk=1\n\r\rW k\r\rq\np,q\n!1/q\n≥d1/q\n d\nY\nk=1\n\r\rW k\r\r\np,q\n!1/d\ndef\n= d1/q dq\nψp,q(w)\n(5.1.2)\nwhere ψp,q(w) =\nd\nY\nk=1\n\r\rW k\r\r\np,q aggregates the layers by multiplication instead of summation. The\ninequality (5.1.2) holds regardless of the activation function, and so for any σ we have:\nψd,H\np,q (f) ≤\n\u0012µd,H(f)p,q\nd1/q\n\u0013d\n.\n(5.1.3)\nBut due to the homogeneity of the RELU activation, when this activation is used we can always balance\nthe norm between the different layers without changing the computed function so as to achieve equality\nin (5.1.2):\nClaim 2. For any fw ∈Fd,H, µd,H\np,q (f) = d1/q dq\nψd,H\np,q (f).\nProof. Let w be weights that realizes f and are optimal with respect to ψp,g; i.e. ψp,q(w) = ψd,H\np,q (w).\nLet f\nW k =\ndp\nψp,q(w)W k/\n\r\rW k\r\r\np,q, and observe that they also realize f. We now have:\nµd,H\np,q (f) ≤µp,q(f\nW) =\n\u0010Xd\nk=1\n\r\r\rf\nW k\r\r\r\nq\np,q\n\u00111/q\n=\n\u0010\nd\n\u0010\nψp,q(w)\n\u0011q/d\u00111/q\n= d1/q dq\nψd,H\np,q (f)\nwhich together with (5.1.2) completes the proof.\nThe two measures are therefore equivalent when we use RELUs, and deﬁne the same level sets, or family\nof model classes, which we refer to simply as Fd,H\np,q . In the remainder of this Section, we investigate\nconvexity and generalization properties of these model classes.\n5.1.1\nGeneralization and Capacity\nIn order to understand the effect of the norm on the sample complexity, we bound the Rademacher\ncomplexity of the classes Fd,H\np,q . Recall that the Rademacher Complexity is a measure of the capacity of\na model class on a speciﬁc sample, which can be used to bound the difference between empirical and\nexpected error, and thus the excess generalization error of empirical risk minimization (see, e.g., [47] for\na complete treatment, and Section 5.5.1 for the exact deﬁnitions we use). In particular, the Rademacher\ncomplexity typically scales as\np\nC/m, which corresponds to a sample complexity of O(C/ϵ2), where\nm is the sample size and C is the effective measure of capacity of the model class.\nTheorem 3. For any d, q ≥1, any 1 ≤p < ∞and any set S = {x1, . . . , xm} ⊆Rnin:\nRm(Fd,H\nψp,q≤ψ) ≤ψ\n\u0010\n2H[ 1\np∗−1\nq ]+\u0011(d−1)\nRlinear\nm,p,nin\n≤\nv\nu\nu\ntψ2\n\u0010\n2H[ 1\np∗−1\nq ]+\u00112(d−1)\nmin{p∗, 4 log(2nin)} maxi ∥xi∥2\np∗\nm\n33\nand so:\nRm(Fd,H\nµp,q≤µ) ≤µd \u0010\n2H[ 1\np∗−1\nq ]+/\nq√\nd\n\u0011(d−1)\nRlinear\nm,p,nin\n≤\nv\nu\nu\ntµ2d\n\u0010\n2H[ 1\np∗−1\nq ]+/\nq√\nd\n\u00112(d−1)\nmin{p∗, 4 log(2nin)} maxi ∥xi∥2\np∗\nm\nwhere the second inequalities hold only if 1 ≤p ≤2, Rlinear\nm,p,nin is the Rademacher complexity of nin-\ndimensional linear predictors with unit ℓp norm with respect to a set of m samples and p∗is such that\n1\np∗+ 1\np = 1.\nProof sketch\nWe prove the bound by induction, showing that for any q, d > 1 and 1 ≤p < ∞,\nRm(Fd,H\nψp,q≤ψ) ≤2H[ 1\np∗−1\nq ]+Rm(Fd−1,H\nψp,q≤ψ).\nThe intuition is that when p∗< q, the Rademacher complexity increases by simply distributing the\nweights among neurons and if p∗≥q then the supremum is attained when the output neuron is connected\nto a neuron with highest Rademacher complexity in the lower layer and all other weights in the top layer\nare set to zero. For a complete proof, see Section 5.5.1.\n■\nNote that for 2 ≤p < ∞, the bound on the Rademacher complexity scales with m\n1\np (see Section 5.5.1)\nbecause:\nRlinear\nm,p,nin ≤\n√\n2 ∥X∥2,p∗\nm\n≤\n√\n2 maxi ∥xi∥p∗\nm\n1\np\n(5.1.4)\nThe bound in Theorem 3 depends on both the magnitude of the weights, as captured by µp,q(w) or\nψp,q(w), and also on the width H of the network (the number of nodes in each layer). However, the\ndependence on the width H disappears, and the bound depends only on the magnitude, as long as q ≤p∗\n(i.e. 1/p+1/q ≥1). This happens, e.g., for overall ℓ1 and ℓ2 regularization, for per-unit ℓ1 regularization,\nand whenever 1/p + 1/q = 1. In such cases, we can omit the size constraint and state the theorem for an\ninﬁnite-width layered network (i.e. a network with an inﬁnitely countable number of units, when the\nnumber of units is allowed to be as large as needed):\nCorollary 4. For any d ≥1, 1 ≤p < ∞and 1 ≤q ≤p∗= p/(p−1), and any set S = {x1, . . . , xm} ⊆\nRnin,\nRm(Fd,H\nψp,q≤ψ) ≤ψ2(d−1)Rlinear\nm,p,nin\n≤\nv\nu\nu\ntψ2\n\u0010\n2H[ 1\np∗−1\nq ]+\u00112(d−1)\nmin{p∗, 4 log(2nin)} maxi ∥xi∥2\np∗\nm\nand so:\nRm(Fd,H\nµp,q≤µ) ≤\n\u0010\n2µ/\nq√\nd\n\u0011d\nRlinear\nm,p,nin\n≤\nv\nu\nu\nt\n\u0010\n2µ/\nq√\nd\n\u00112d\nmin{p∗, 4 log(2nin)} maxi ∥xi∥2\np∗\nm\nwhere the second inequalities hold only if 1 ≤p ≤2 and Rlinear\nm,p,nin is the Rademacher complexity of\nnin-dimensional linear predictors with unit ℓp norm with respect to a set of m samples.\n34\n5.1.2\nTightness\nWe next investigate the tightness of the complexity bound in Theorem 3, and show that when 1/p+1/q <\n1 the dependence on the width H is indeed unavoidable. We show not only that the bound on the\nRademacher complexity is tight, but that the implied bound on the sample complexity is tight, even\nfor binary classiﬁcation with a margin over binary inputs. To do this, we show how we can shatter the\nm = 2nin points {±1}nin using a network with small group-norm:\nTheorem 5. For any p, q ≥1 (and 1/p∗+ 1/p = 1) and any depth d ≥2, the m = 2nin points {±1}nin\ncan be shattered with unit margin by Fd,H\nψp,q≤ψ with:\nψ ≤n1/p\nin\nm1/p+1/q H−(d−2)[1/p∗−1/q]+\nProof. Consider a size m subset Sm of 2nin vertices of the nin dimensional hypercube {−1, +1}nin.\nWe construct the ﬁrst layer using m units. Each unit has a unique weight vector consisting of +1 and\n−1’s and will output a positive value if and only if the sign pattern of the input x ∈Sm matches that\nof the weight vector. The second layer has a single unit and connects to all m units in the ﬁrst layer.\nFor any m dimensional sign pattern b ∈{−1, +1}m, we can choose the weights of the second layer\nto be b, and the network will output the desired sign for each x ∈Sm with unit margin. The norm\nof the network is at most (m · nq/p\nin )1/q · m1/p = n1/p\nin\n· m(1/p+1/q). This establishes the claim for\nd = 2. For d > 2 and 1/p + 1/q ≥1, we obtain the same norm and unit margin by adding d −2\nlayers with one unit in each layer connected to the previous layer by a unit weight. For d > 2 and\n1/p + 1/q < 1, we show the dependence on H by recursively replacing the top unit with H copies\nof it and adding an averaging unit on top of that. More speciﬁcally, given the above d = 2 layer\nnetwork, we make H copies of the output unit with rectiﬁed linear activation and add a 3rd layer with\none output unit with uniform weight 1/H to all the copies in the 2nd layer. Since this operation does\nnot change the output of the network, we have the same margin and now the norm of the network is\n(m · nq/p\nin )1/q · (Hmq/p)1/q · (H(1/Hp))1/p = n1/p\nin\n· m(1/p+1/q) · H1/q−1/p∗. That is, we have reduced\nthe norm by factor H1/q−1/p∗. By repeating this process, we get the geometric reduction in the norm\nH(d−2)(1/q−1/p∗), which concludes the proof.\nTo understand this lower bound, ﬁrst consider the bound without the dependence on the width H. We have\nthat for any depth d ≥2, ψ ≤mrnin = mr log m (since 1/p ≤1 always) where r = 1/p + 1/q ≤2.\nThis means that for any depth d ≥2 and any p, q the sample complexity of learning the class scales\nas m = Ω(ψ1/r/ log ψ) ≥˜Ω(√ψ). This shows a polynomial dependence on ψ, though with a lower\nexponent than the ψ2 (or higher for p > 2) dependence in Theorem 3. Still, if we now consider\nthe complexity control as a function of µp,q we get a sample complexity of at least Ω(µd/2/ log µ),\nestablishing that if we control the group-norm as in (9.1.1), we cannot avoid a sample complexity which\ndepends exponentially on the depth. Note that in our construction, all other factors in Theorem 3, namely\nmaxi ∥xi∥and log nin, are logarithmic (or double-logarithmic) in m.\nNext we consider the dependence on the width H when 1/p + 1/q < 1. Here we have to use depth\nd ≥3, and we see that indeed as the width H and depth d increase, the magnitude control ψ can decrease\nas H(1/p∗−1/q)(d−2) without decreasing the capacity, matching Theorem 1 up to an offset of 2 on the\ndepth. In particular, we see that in this regime we can shatter an arbitrarily large number of points with\narbitrarily low ψ by using enough hidden units, and so the capacity of Fd\np,q is indeed inﬁnite and it cannot\nensure any generalization.\n5.1.3\nConvexity\nFinally we establish a sufﬁcient condition for the model classes Fd\np,q to be convex. We are referring to\nconvexity of the functions in the Fd\np,q independent of a speciﬁc representation. If we consider a, possibly\n35\nregularized, empirical risk minimization problem on the weights, the objective (the empirical risk) would\nnever be a convex function of the weights (for depth d ≥2), even if the regularizer is convex in w\n(which it always is for p, q ≥1). But if we do not bound the width of the network, and instead rely on\nmagnitude-control alone, we will see that the resulting model class, and indeed the complexity measure,\nmay be convex (with respect to taking convex combinations of functions, not of weights).\nTheorem 6. For any d, p, q ≥1 such that 1\nq ≤\n1\nd−1\n\u00001 −1\np\n\u0001\n, ψd\np,q(f) is a semi-norm in Fd.\nIn particular, under the condition of the Theorem, ψd\np,q is convex, and hence its sublevel sets Fd\np,q are\nconvex, and so µd\np,q is quasi-convex (but not convex).\nProof sketch\nTo show convexity, consider two functions f, g ∈Fd\nψp,q≤ψ and 0 < α < 1, and let\nU and V be the weights realizing f and g respectively with ψp,q(U) ≤ψ and ψp,q(V ) ≤ψ. We will\nconstruct weights w realizing αf + (1 −α)g with ψp,q(w) ≤ψ. This is done by ﬁrst balancing U and\nV s.t. at each layer ∥Ui∥p,q =\ndp\nψp,q(U) and ∥Vi∥p,q =\ndp\nψp,q,(V ) and then placing U and V side by\nside, with no interaction between the units calculating f and g until the output layer. The output unit has\nweights αUd coming in from the f-side and weights (1 −α)Vd coming in from the g-side. In Section\n5.5.2 we show that under the condition in the theorem, ψp,q(w) ≤ψ. To complete the proof, we also\nshow ψd\np,q is homogeneous and that this is sufﬁcient for convexity.\n■\n5.2\nPer-Unit and Path Regularization\nIn this Section we will focus on the special case of q = ∞, i.e. when we constrain the norm of the\nincoming weights of each unit separately.\nPer-unit ℓ1-regularization was studied by [9, 48, 47] who showed generalization guarantees. A two-layer\nnetwork of this form with RELU activation was also considered by [49], who studied its approximation\nability and suggested heuristics for learning it. Per-unit ℓ2 regularization in a two-layer network was\nconsidered by [50], who showed it is equivalent to using a speciﬁc kernel. We now introduce Path\nregularization and discuss its equivalence to Per-Unit regularization.\nPath Regularization\nConsider a regularizer which looks at the sum over all paths from input nodes to\nthe output node, of the product of the weights along the path:\nφp(w) =\n\u0010\nX\nvin[i]\ne1\n→v1\ne2\n→v2···\nek\n→vout\nk\nY\ni=1\n|wei|p\u00111/p\n(5.2.1)\nwhere p ≥1 controls the norm used to aggregate the paths. We can motivate this regularizer as follows: if\na node does not have any high-weight paths going out of it, we really don’t care much about what comes\ninto it, as it won’t have much effect on the output. The path-regularizer thus looks at the aggregated\ninﬂuence of all the weights.\nReferring to the induced regularizer φG\np (f) = minfw=f φp(w) (with the usual shorthands for layered\ngraphs), we now observe that for layered graphs, path regularization and per-unit regularization are\nequivalent:\nTheorem 7. For p ≥1, any d and (ﬁnite or inﬁnite) H, for any fw ∈Fd,H: φd,H\np\n(f) = ψd,H\np,∞\nIt is important to emphasize that even for layered graphs, it is not the case that for all weights φp(w) =\nψp,∞(w). E.g., a high-magnitude edge going into a unit with no non-zero outgoing edges will affect\n36\nψp,∞(w) but not φp(w), as will having high-magnitude edges on different layers in different paths. In a\nsense path regularization is as more careful regularizer less fooled by imbalance. Nevertheless, in the\nproof of Theorem 7 in Section 5.5.3, we show we can always balance the weights such that the two\nmeasures are equal.\nThe equivalence does not extend to non-layered graphs, since the lengths of different paths might be\ndifferent. Again, we can think of path regularizer as more reﬁned regularizer taking into account the\nlocal structure. However, if we consider all DAGs of depth at most d (i.e. with paths of length at most d),\nthe notions are again equivalent (see proof in Section 5.5.3):\nTheorem 8. For any p ≥1 and any d: ψd\np,∞(f) =\nmin\nG ∈DAG(d) φG\np (f).\nIn particular, for any graph G of depth d, we have that φG\np (f) ≥ψd\np,∞(f). Combining this observation\nwith Corollary 4 allows us to immediately obtain a generalization bound for path regularization on any,\neven non-layered, graph:\nCorollary 9. For any graph G of depth d and any set S = {x1, . . . , xm} ⊆Rnin:\nRm(FG\nφ1≤φ) ≤\ns\n4d−1φ2 · 4 log(2nin) sup ∥xi∥2\n∞\nm\nNote that in order to apply Corollary 4 and obtain a width-independent bound, we had to limit ourselves\nto p = 1. We further explore this issue next.\nCapacity\nAs was previously noted, size-independent generalization bounds for bounded depth networks\nwith bounded per-unit ℓ1 norm have long been known (and make for a popular homework problem).\nThese correspond to a specialization of Corollary 4 for the case p = 1, q = ∞. Furthermore, the\nkernel view of [50] allows obtaining size-independent generalization bound for two-layer networks with\nbounded per-unit ℓ2 norm (i.e. a single inﬁnite hidden layer of all possible unit-norm units, and a bounded\nℓ2-norm output unit). However, the lower bound of Theorem 5 establishes that for any p > 1, once we\ngo beyond two layers, we cannot ensure generalization without also controlling the size (or width) of the\nnetwork.\nConvexity\nAn immediately consequence of Theorem 6 is that per-unit regularization, if we do not\nconstrain the network width, is convex for any p ≥1. In fact, ψd\np,∞is a (semi)norm. However, as\ndiscussed above, for depth d > 2 this is meaningful only for p = 1, as ψd\np,∞collapses for p > 1.\nHardness\nSince the classes Fd\n1,∞are convex, we might hope that this might make learning computa-\ntionally easier. Indeed, one can consider functional-gradient or boosting-type strategies for learning a\npredictor in the class [51]. However, as Bach [49] points out, this is not so easy as it requires ﬁnding the\nbest ﬁt for a target with a RELU unit, which is not easy. Indeed, applying results on hardness of learning\nintersections of halfspaces, which can be represented with small per-unit norm using two-layer networks,\nwe can conclude that, subject to certain complexity assumptions, it is not possible to efﬁciently PAC\nlearn Fd\n1,∞, even for depth d = 2 when ψ1,∞increases superlinearly:\nCorollary 10. Subject to the the strong random CSP assumptions in [11], it is not possible to efﬁciently\nPAC learn (even improperly) functions {±1}nin →{±1} realizable with unit margin by F2\n1,∞when\nψ1,∞= ω(nin) (e.g. when ψ1,∞= nin log nin). Moreover, subject to intractability of ˜Q(n1.5\nin )-unique\nshortest vector problem, for any ϵ > 0, it is not possible to efﬁciently PAC learn (even improperly)\nfunctions {±1}nin →{±1} realizable with unit margin by F2\n1,∞when ψ1,∞= n1+ϵ\nin\n.\n37\nThis is a corollary of Theorem 24 in the Section 5.5.4. Either versions of corollary 10 precludes the\npossibility of learning in time polynomial in ψ1,∞, though it still might be possible to learn in poly(nin)\ntime when ψ1,∞is sublinear.\nSharing\nWe conclude this Section with an observation on the type of networks obtained by per-unit, or\nequivalently path, regularization.\nTheorem 11. For any p ≥1 and d > 1 and any fw ∈Fd, there exists a layered graph G(V, E) of\ndepth d, such that fw ∈FG and ψG\np,∞(f) = φG\np (f) = ψd\np,∞(f), and the out-degree of every internal\n(non-input) node in G is one. That is, the subgraph of G induced by the non-input vertices is a tree\ndirected toward the output vertex.\nWhat the Theorem tells us is that we can realize every function as a tree with optimal per-unit norm. If\nwe think of learning with an inﬁnite fully-connected layered network, we can always restrict ourselves\nto models in which the non-zero-weight edges form a tree. This means that when using per-unit\nregularization we have no incentive to “share” lower-level units—each unit will only have a single\noutgoing edge and will only be used by a single down-stream unit. This seems to defy much of the\nintuition and power of using deep networks, where we expect lower layers to represent generic feature\nuseful in many higher-level features. In effect, we are not encouraging any transfer between learning\ndifferent aspects of the function (or between different tasks or classes, if we do have multiple output\nunits). Per-unit regularization therefore misses out on much of the inductive bias that we might like to\nimpose when using deep learning (namely, promoting sharing).\nProof. [of Theorem 11] For any fw ∈FDAG(d), we show how to construct such eG and ew. We ﬁrst sort\nthe vertices of G based on topological ordering such that the out-degree of the ﬁrst vertex is zero. Let\nG0 = G and w0 = w. At each step i, we ﬁrst set Gi = Gi−1 and wi = wi−1 and then pick the vertex u\nthat is the ith vector in the topological ordering. If the out-degree of u is at most 1. Otherwise, for any\nedge (u →v) we create a copy of vertex u that we call it uv, add the edge (uv →v) to Gi and connect\nall incoming edges of u with the same weights to every such uv and ﬁnally we delete the vertex u from\nGi together with all incoming and outgoing edges of u. It is easy to indicate that fGi,wi = fGi−1,wi−1.\nAfter at most |V | such steps, all internal nodes have out-degree one and hence the subgraph induced by\nnon-input vertices will be a tree.\n5.3\nOverall Regularization\nIn this Section, we will focus on “overall” ℓp regularization, corresponding to the choice q = p, i.e. when\nwe bound the overall (vectorized) norm of all weights in the system:\nµp,p(w) =\n\u0010X\ne∈E\n|w(e)|p\u00111/p\n.\nCapacity\nFor p ≤2, Corollary 4 provides a generalization guarantee that is independence of the width—\nwe can conclude that if we use weight decay (overall ℓ2 regularization), or any tighter ℓp regularization,\nthere is no need to limit ourselves to networks of ﬁnite size (as long as the corresponding dual-norm\nof the inputs are bounded). However, in Section 5.1.2 we saw that with d ≥3 layers, the regularizer\ndegenerates and leads to inﬁnite capacity classes if p > 2. In any case, even if we bound the overall\nℓ1-norm, the complexity increases exponentially with the depth.\n38\nConvexity\nThe conditions of Theorem 6 for convexity of Fd\n2,2 are ensured when p ≥d. For depth\nd = 1, i.e. a single unit, this just conﬁrms that ℓp-regularized linear prediction is convex for p ≥1.\nFor depth d = 2, we get convexity with ℓ2 regularization, but not ℓ1. For depth d > 2 we would need\np > d ≥3, however for such values of p we know from Theorem 5 that Fd\np,p degenerates to an inﬁnite\ncapacity class if we do not control the width (if we do control the width, we do not get convexity). This\nleaves us with F2\n2,2 as the interesting convex class. Below we show an explicit convex characterization\nof F2\n2,2 by showing it is equivalent to so-called “convex neural nets”.\nConvex Neural Nets [45] over inputs in Rnin are two-layer networks with a ﬁxed inﬁnite hidden layer\nconsisting of all units with weights w ∈G for some base class G ∈Rnin, and a second ℓ1-regularized\nlayer. Since over ﬁnite data the weights in the second layer can always be taken to have ﬁnite support\n(i.e. be non-zero for only a ﬁnite number of ﬁrst-layer units), and we can approach any function with\ncountable support, we can instead think of a network in F2 where the bottom layer is constraint to G and\nthe top layer is ℓ1 regularized. Focusing on G = {w | ∥w∥p ≤1}, this corresponds to imposing an ℓp\nconstraint on the bottom layer, and ℓ1 regularization on the top layer and yields the following complexity\nmeasure over F2:\nνp(f) =\ninf\nflayer(d),W =f,s.t.∀j∥W1[j,:]∥p≤1 ∥W2∥1 .\n(5.3.1)\nThis is similar to per-unit regularization, except we impose different norms at different layers (if p ̸= 1).\nWe can see that F2\nνp≤ν = ν · conv(σ(G)), and is thus convex for any p. Focusing on RELU activation\nwe have the equivalence:\nTheorem 12. µ2\n2,2(f) = 2ν2(f).\nThat is, overall ℓ2 regularization with two layers is equivalent to a convex neural net with ℓ2-constrained\nunits on the bottom layer and ℓ1 (not ℓ2!) regularization on the output.\nProof. We can calculate:\nmin\nfW =f µ2\n2,2(w) = min\nfW =f\nH\nX\nj=1\n nin\nX\ni=1\n|W1[j, i]|2 + |W2[j]|2\n!\n= min\nfW =f\nH\nX\nj=1\n2\nrXnin\ni=1 |W1[j, i]|2 · |W2[j]|\n(5.3.2)\n= 2 min\nfW =f\nH\nX\nj=1\n|W2[j]|\ns.t.\nrXnin\ni=1 |W1[j, i]|2 ≤1.\n(5.3.3)\nHere (5.3.2) is the arithmetic-geometric mean inequality for which we can achieve equality by balancing\nthe weights (as in Claim 2) and (5.3.3) again follows from the homogeneity of the RELU which allows\nus to rebalance the weights.\nHardness\nAs with Fd\n1,∞, we might hope that the convexity of F2\n2,2 might make it computationally\neasy to learn. However, by the same reduction from learning intersection of halfspaces (Theorem 24 in\nSection 5.5.4) we can again conclude that we cannot learn in time polynomial in µ2\n2,2:\nCorollary 13. Subject to the the strong random CSP assumptions in [11], it is not possible to efﬁciently\nPAC learn (even improperly) functions {±1}nin →{±1} realizable with unit margin by F2\np,p when\nµ2\np,p = ω(n\n1\np\nin). (e.g. when ψ1,∞= nin log nin). Moreover, subject to intractability of ˜Q(n1.5\nin )-unique\nshortest vector problem, for any ϵ > 0, it is not possible to efﬁciently PAC learn (even improperly)\nfunctions {±1}nin →{±1} realizable with unit margin by F2\n1,∞when ψ1,∞= n\n1\np +ϵ\nin\n.\n39\n5.4\nDepth Independent Regularization\nUp until now we discussed relying on magnitude-based regularization instead of directly controlling\nnetwork size, thus allowing unbounded and even inﬁnite width. But we still relied on a ﬁnite bound on\nthe depth in all our derivations. Can the explicit dependence on the depth be avoided, and replaced with\nonly a measure of scale of the weights?\nWe already know we cannot rely only on a bound on the group-norm µp,q when the depth is unbounded, as\nwe know from Theorem 5 that in terms of µp,q the sample complexity necessarily increases exponentially\nwith the depth: if we allow arbitrarily deep graphs we can shrink µp,q toward zero without changing the\nscale of the computed function. However, controlling the ψ-measure, or equivalently the path-regularizer\nφ, in arbitrarily-deep graphs is sensible, and we can deﬁne:\nψp,q = inf\nd≥1 ψd\np,q(f) = lim\nd→∞ψd\np,q(f)\nor:\nφp = inf\nG φG\np (f)\n(5.4.1)\nwhere the minimization is over any DAG. From Theorem 8 we can conclude that φp(f) = ψp,∞(f). In\nany case, ψp,q(f) is a sensible complexity measure, that does not collapse despite the unbounded depth.\nCan we obtain generalization guarantees for the class Fψp,q≤ψ ?\nUnfortunately, even when 1/p + 1/q ≥1 and we can obtain width-independent bounds, the bound in\nCorollary 4 still has a dependence on 4d, even if ψp,q is bounded. Can such a dependence be avoided?\nFor anti-symmetric Lipschitz-continuous activation functions (i.e. such that σ(−z) = −σ(z)), such as\nthe ramp, and for per-unit ℓp-regularization µd\n1,∞we can avoid the factor of 4d\nTheorem 14. For any anti-symmetric 1-Lipschitz function σ and any set S = {x1, . . . , xm} ⊆Rnin:\nRm(Fd\nµ1,∞≤µ) ≤\ns\n4µ2d log(2nin) sup ∥xi∥2\n∞\nm\nThe proof is again based on an inductive argument similar to Theorem 3 and you can ﬁnd it in Section\n5.5.1.\nHowever, the ramp is not homogeneous and so the equivalent between µ, ψ and φ breaks down. Can we\nobtain such a bound also for the RELU? At the very least, what we can say is that an inductive argument\nsuch that used in the proofs of Theorems 3 and 14 cannot be used to avoid an exponential dependence on\nthe depth. To see this, consider ψ1,∞≤1 (this choice is arbitrary if we are considering the Rademacher\ncomplexity), for which we have\nFd+1\nψ1,∞<1 =\nh\nconv(Fd\nψ1,∞<1)\ni\n+ ,\n(5.4.2)\nwhere conv(·) is the symmetric convex hull, and [·]+ = max(z, 0) is applied to each function in the\nclass. In order to apply the inductive argument without increasing the complexity exponentially with\nthe depth, we would need the operation [conv(H)]+ to preserve the Rademacher complexity, at least for\nnon-negative convex cones H. However we show a simple example of a non-negative convex cone H for\nwhich Rm ([conv(H)]+) > Rm (H).\nWe will specify H as a set of vectors in Rm, corresponding to the evaluation of h(xi) of different\nfunctions in the class on the m points xi in the sample. In our construction, we will have only\nm = 3 points. Consider H = conv({(∞, ′, ∞), (′, ∞, ∞)}), in which case H′ def\n= [conv(H)]+ =\nconv({(∞, ′, ∞), (′, ∞, ∞), (′.▽, ′, ′)}). It is not hard to verify that Rm(H′) = ∞∋\n∞̸ > ∞∈\n∞̸ = R⇕(H).\n40\n5.5\nProofs\n5.5.1\nRademacher Complexities\nThe sample based Rademacher complexity of a class F of function mapping from X to R with respect to\na set S = {x1, . . . , xm} is deﬁned as:\nRm(F) = E [ξ ∈{±1}m] 1\nm sup\nfw∈F\n\f\f\f\f\f\nm\nX\ni=1\nξif(xi)\n\f\f\f\f\f\nIn this section, we prove an upper bound for the Rademacher complexity of the class Fd,HRELU\nψp,q≤ψ, i.e., the\nclass of functions that can be represented as depth d, width H network with rectiﬁed linear activations,\nand the layer-wise group norm complexity ψp,q bounded by ψ. As mentioned in the main text, our\nproof is an induction with respect to the depth d. We start with d = 1 layer neural networks, which is\nessentially the class of linear separators.\nℓp-regularized Linear Predictors\nFor completeness, we prove the upper bounds on the Rademacher complexity of class of linear separators\nwith bounded ℓp norm. The upper bounds presented here are particularly similar to generalization bounds\nin [52] and [53]. We ﬁrst mention two already established lemmas that we use in the proofs.\nTheorem 15. (Khintchine-Kahane Inequality) For any 0 < p < ∞and S = {z1, . . . , zm}, if the\nrandom variable ξ is uniform over {±1}m, then\n \nE [ξ]\n\f\f\f\f\f\nm\nX\ni=1\nξizi\n\f\f\f\f\f\np! 1\np\n≤Cp\n m\nX\ni=1\n|zi|2\n! 1\n2\nwhere Cp is a constant depending only on p.\nThe sharp value of the constant Cp was found by Haagerup [54] but for our analysis, it is enough to note\nthat if p ≥1 we have Cp ≤√p.\nLemma 16. (Massart Lemma) Let A be a ﬁnite set of m dimensional vectors. Then\nE [ξ]max\na∈A\n1\nm\nm\nX\ni=1\nξiai ≤max\na∈A ∥a∥2\np\n2 log |A|\nm\n,\nwhere |A| is the cardinality of A.\nWe are now ready to show upper bounds on Rademacher complexity of linear separators with bounded\nℓp norm.\nLemma 17. (Rademacher complexity of linear separators with bounded ℓp norm) For any d, q ≥1, For\nany 1 ≤p ≤2,\nRm(F1\nψp,q≤ψ) ≤\ns\nψ2 min{p∗, 4 log(2nin)} maxi ∥xi∥2\np∗\nm\nand for any 2 < p < ∞\nRm(F1\nψp,q≤ψ) ≤\n√\n2ψ ∥X∥2,p∗\nm\n≤\n√\n2ψ maxi ∥xi∥p∗\nm\n1\np\n41\nwhere p∗is such that\n1\np∗+ 1\np = 1.\nProof. First, note that F1 is the class of linear functions and hence for any function fw ∈F1, we have\nthat ψp,q(w) = ∥w∥p. Therefore, we can write the Rademacher complexity for a set S = {x1, . . . , xm}\nas:\nRm(F1\nψp,q≤ψ) = E [ξ ∈{±1}m] 1\nm\nsup\n∥w∥p≤ψ\n\f\f\f\f\f\nm\nX\ni=1\nξiw⊤xi\n\f\f\f\f\f\n= E [ξ ∈{±1}m] 1\nm\nsup\n∥w∥p≤ψ\n\f\f\f\f\fw⊤\nm\nX\ni=1\nξixi\n\f\f\f\f\f\n= ψE [ξ ∈{±1}m] 1\nm\n\r\r\r\r\r\nm\nX\ni=1\nξixi\n\r\r\r\r\r\np∗\nFor 1 ≤p ≤min\nn\n2,\n2 log(2nin)\n2 log(2nin)−1\no\n(and therefore 2 log(2nin) ≤p∗), we have\nRm(F1\nψp,q≤ψ) = ψE [ξ ∈{±1}m] 1\nm\n\r\r\r\r\r\nm\nX\ni=1\nξixi\n\r\r\r\r\r\np∗\n≤n\n1\np∗\nin ψE [ξ ∈{±1}m]\n\"\n1\nm\n\r\r\r\r\r\nm\nX\ni=1\nξixi\n\r\r\r\r\r\n∞\n#\n≤n\n1\n2 log(2nin)\nin\nψE [ξ ∈{±1}m] 1\nm\n\r\r\r\r\r\nm\nX\ni=1\nξixi\n\r\r\r\r\r\n∞\n≤\n√\n2ψE [ξ ∈{±1}m] 1\nm\n\r\r\r\r\r\nm\nX\ni=1\nξixi\n\r\r\r\r\r\n∞\nWe now use the Massart Lemma viewing each feature (xi[j])m\ni=1 for j = 1, . . . , nin as a member of a\nﬁnite model class and obtain\nRm(F1\nψp,q≤ψ) ≤\n√\n2ψE [ξ ∈{±1}m] 1\nm\n\r\r\r\r\r\nm\nX\ni=1\nξixi\n\r\r\r\r\r\n∞\n≤2ψ\np\nlog(2nin)\nm\nmax\nj=1...,nin ∥(xi[j])m\ni=1∥2\n≤2ψ\nr\nlog(2nin)\nm\nmax\ni=1,...,m ∥xi∥∞\n≤2ψ\nr\nlog(2nin)\nm\nmax\ni=1,...,m ∥xi∥p∗\n42\nIf min\nn\n2,\n2 log(2nin)\n2 log(2nin)−1\no\n< p < ∞, by Khintchine-Kahane inequality we have\nRm(F1\nψp,q≤ψ) = ψE [ξ ∈{±1}m]\n\n1\nm\n\r\r\r\r\r\nm\nX\ni=1\nξixi\n\r\r\r\r\r\np∗\n\n\n≤ψ 1\nm\n\n\nnin\nX\nj=1\nE [ξ ∈{±1}m]\n\n\n\f\f\f\f\f\nm\nX\ni=1\nξixi[j]\n\f\f\f\f\f\np∗\n\n\n\n1/p∗\n≤ψ\n√p∗\nm\n\u0010Xnin\nj=1 ∥(xi[j])m\ni=1∥p∗\n2\n\u00111/p∗\n= ψ\n√p∗\nm ∥X∥2,p∗\nIf p∗≥2, by Minskowski inequality we have that ∥X∥2,p∗≤m1/2 maxi ∥xi∥p∗. Otherwise, by\nsubadditivity of the function f(z) = z\np∗\n2 , we get ∥X∥2,p∗≤m1/p∗maxi ∥xi∥p∗.\nTheorem 3\nWe deﬁne the model class Fd,H,H to be the class of functions from X to RH computed by a layered\nnetwork of depth d, layer size H and H outputs.\nFor the proof of theorem 3, we need the following two technical lemmas. The ﬁrst is the well-known\ncontraction lemma:\nLemma 18. (Contraction Lemma) Let function φ : R →R be Lipschitz with constant Lφ such\nthat φ satisﬁes φ(0) = 0. Then for any class F of functions mapping from X to R and any set\nS = {x1, . . . , xm}:\nE [ξ ∈{±1}m]\n\"\n1\nm sup\nfw∈F\n\f\f\f\f\f\nm\nX\ni=1\nξiφ(f(xi))\n\f\f\f\f\f\n#\n≤2LφE [ξ ∈{±1}m]\n\"\n1\nm sup\nfw∈F\n\f\f\f\f\f\nm\nX\ni=1\nξif(xi))\n\f\f\f\f\f\n#\nNext, the following lemma reduces the maximization over a matrix W ∈RH×H that appears in the\ncomputation of Rademacher complexity to H independent maximizations over a vector w ∈RH (the\nproof is deferred to subsubsection 5.5.1):\nLemma 19. For any p, q ≥1, d ≥2, ξ ∈{±1}m and fw ∈Fd,H,H we have\nsup\nW\n1\n∥W∥p,q\n\r\r\r\r\r\nm\nX\ni=1\nξi[W[f(xi)]+]+\n\r\r\r\r\r\np∗\n= H[ 1\np∗−1\nq ]+ sup\nw\n1\n∥w∥p\n\f\f\f\f\f\nm\nX\ni=1\nξi[w⊤[f(xi)]+]+\n\f\f\f\f\f\nwhere p∗is such that\n1\np∗+ 1\np = 1.\nTheorem 3. For any d, p, q ≥1 and any set S = {x1, . . . , xm} ⊆Rnin:\nRm(Fd,H\nψp,q≤ψ) ≤\nv\nu\nu\ntψ2\n\u0010\n2H[ 1\np∗−1\nq ]+\u00112(d−1)\nmin{p∗, 2 log(2nin)} sup ∥xi∥2\np∗\nm\nand so:\nRm(Fd,H\nµp,q≤µ) ≤\nv\nu\nu\ntµ2d\n\u0010\n2H[ 1\np∗−1\nq ]+/\nq√\nd\n\u00112(d−1)\nmin{p∗, 2 log(2nin)} sup ∥xi∥2\np∗\nm\n43\nwhere p∗is such that\n1\np∗+ 1\np = 1.\nProof. By the deﬁnition of Rademacher complexity if ξ is uniform over {±1}m, we have:\nRm(Fd,H\nψp,q≤ψ) = E [ξ]\n\n1\nm\nsup\nfw∈Fd,H\nψp,q≤ψ\n\f\f\f\f\f\nm\nX\ni=1\nξif(xi)\n\f\f\f\f\f\n\n\n= E [ξ]\n\"\n1\nm\nsup\nfw∈Fd,H\nψ\nψp,q(w)\n\f\f\f\f\f\nm\nX\ni=1\nξif(xi)\n\f\f\f\f\f\n#\n= E [ξ]\n\"\n1\nm\nsup\ng∈Fd−1,H,H sup\nw\nψ\nψp,q(g) ∥w∥p\n\f\f\f\f\f\nm\nX\ni=1\nξiw⊤[g(xi)]+\n\f\f\f\f\f\n#\n= E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H,H\nψ\nψp,q(g)\n\r\r\r\r\r\nm\nX\ni=1\nξi[g(xi)]+\n\r\r\r\r\r\np∗\n\n\n= E [ξ]\n\n1\nm\nsup\nh∈Fd−2,H,H\nψ\nψp,q(h) sup\nW\n1\n∥W∥p,q\n\r\r\r\r\r\nm\nX\ni=1\nξi[W[h(xi)]+]+\n\r\r\r\r\r\np∗\n\n\n= H[ 1\np∗−1\nq ]+E [ξ]\n\"\n1\nm\nsup\nh∈Fd−2,H,H\nψ\nψp,q(h) sup\nw\n1\n∥w∥p\n\f\f\f\f\f\nm\nX\ni=1\nξi[w⊤[h(xi)]+]+\n\f\f\f\f\f\n#\n(5.5.1)\n= H[ 1\np∗−1\nq ]+E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H\nψp,q≤ψ\n\f\f\f\f\f\nm\nX\ni=1\nξi[g(xi)]+\n\f\f\f\f\f\n\n\n≤2H[ 1\np∗−1\nq ]+E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H\nψp,q≤ψ\n\f\f\f\f\f\nm\nX\ni=1\nξig(xi)\n\f\f\f\f\f\n\n\n(5.5.2)\n= 2H[ 1\np∗−1\nq ]+Rm(Fd−1,H\nψp,q≤ψ)\nwhere the equality (5.5.1) is obtained by lemma 19 and inequality (5.5.2) is by Contraction Lemma. This\nwill give us the bound on Rademacher complexity of Fd,H\nψp,q≤ψ based on the Rademacher complexity of\nFd−1,H\nψp,q≤ψ. Applying the same argument on all layers and using lemma 17 to bound the complexity of the\nﬁrst layer completes the proof.\nProof of Lemma 19\nProof. It is immediate that the right hand side of the equality in the statement is always less than or equal\nto the left hand side because given any vector w in the right hand side, by setting each row of matrix w\nin the left hand side we get the equality. Therefore, it is enough to prove that the left hand side is less\nthan or equal to the right hand side. For the convenience of notations, let g(w)\ndef\n= | Pm\ni=1 ξiw⊤[f(xi)]+|.\nDeﬁne ew to be:\new\ndef\n= arg max\nw\ng(w)\n∥w∥p\n44\nIf q ≤p∗, then the right hand side of equality in the lemma statement will reduce to g( ew)/ ∥ew∥p and\ntherefore we need to show that for any matrix V ,\ng( ew)\n∥ew∥p\n≥\n∥g(V )∥p∗\n∥V ∥p,q\n.\nSince q ≤p∗, we have ∥V ∥p,p∗≤∥V ∥p,q and hence it is enough to prove the following inequality:\ng( ew)\n∥ew∥p\n≥\n∥g(V )∥p∗\n∥V ∥p,p∗.\nOn the other hand, if q > p∗, then we need to prove the following inequality holds:\nH\n1\np∗−1\nq g( ew)\n∥ew∥p\n≥\n∥g(V )∥p∗\n∥V ∥p,q\nSince q > p∗, we have that ∥V ∥p,p∗≤H\n1\np∗−1\nq ∥V ∥p,q. Therefore, it is again enough to show that:\ng( ew)\n∥ew∥p\n≥\n∥g(V )∥p∗\n∥V ∥p,p∗.\nWe can rewrite the above inequality in the following form:\nH\nX\ni=1\n \ng( ew) ∥Vi∥p\n∥ew∥p\n!p∗\n≥\nH\nX\ni=1\ng(Vi)p∗\nBy the deﬁnition of ew, we know that the above inequality holds for each term in the sum and hence the\ninequality is true.\nTheorem 14\nThe proof is similar to the proof of theorem 3 but here bounding µ1,∞by µ means the ℓ1 norm of input\nweights to each neuron is bounded by µ. We use a different version of Contraction Lemma in the proof\nthat is without the absolute value:\nLemma 20. (Contraction Lemma (without the absolute value)) Let function φ : R →R be Lipschitz with\nconstant Lφ. Then for any class F of functions mapping from X to R and any set S = {x1, . . . , xm}:\nE [ξ ∈{±1}m]\n\"\n1\nm sup\nfw∈F\nm\nX\ni=1\nξiφ(f(xi))\n#\n≤LφE [ξ ∈{±1}m]\n\"\n1\nm sup\nfw∈F\nm\nX\ni=1\nξif(xi))\n#\nTheorem 14. For any anti-symmetric 1-Lipschitz function σ and any set S = {x1, . . . , xm} ⊆Rnin:\nRm(Fd\nµ1,∞≤µ) ≤\ns\n2µ2d log(2nin) sup ∥xi∥2\n∞\nm\n45\nProof. Assuming ξ is uniform over {±1}m, we have:\nRm(Fd,H\nµ1,∞≤µ) = E [ξ]\n\n1\nm\nsup\nfw∈Fd,H\nµ1,∞≤µ\n\f\f\f\f\f\nm\nX\ni=1\nξif(xi)\n\f\f\f\f\f\n\n\n= E [ξ]\n\n1\nm\nsup\nfw∈Fd,H\nµ1,∞≤µ\nm\nX\ni=1\nξif(xi)\n\n\n= E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H,H\nµ1,∞≤µ\nsup\n∥w∥1≤µ\nw⊤\nm\nX\ni=1\nξiσ(g(xi))\n\n\n= E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H,H\nµ1,∞≤µ\n\r\r\r\r\r\nm\nX\ni=1\nξiσ(g(xi))\n\r\r\r\r\r\n∞\n\n\n= E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H\nµ1,∞≤µ\n\f\f\f\f\f\nm\nX\ni=1\nξiσ(g(xi))\n\f\f\f\f\f\n\n\n(5.5.3)\n= E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H\nµ1,∞≤µ\nm\nX\ni=1\nξiσ(g(xi))\n\n\n≤E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H\nµ1,∞≤µ\nm\nX\ni=1\nξig(xi)\n\n\n(5.5.4)\n= E [ξ]\n\n1\nm\nsup\ng∈Fd−1,H\nµ1,∞≤µ\n\f\f\f\f\f\nm\nX\ni=1\nξig(xi)\n\f\f\f\f\f\n\n\n= Rm(Fd−1,H\nµ1,∞≤µ)\nwhere the equality (5.5.3) is by anti-symmetric property of σ and inequality (5.5.4) is by the version of\nContraction Lemma without the absolute value. This will give us the bound on Rademacher complexity\nof Fd,H\nµ1,∞≤µ based on the Rademacher complexity of Fd−1,H\nµ1,∞≤µ. Applying the same argument on all\nlayers and using lemma 17 to bound the complexity of the ﬁrst layer completes the proof.\n5.5.2\nProof that ψd\np,q(w) is a semi-norm in Fd\nWe repeat the statement here for convenience.\nTheorem 6. For any d, p, q ≥1 such that 1\nq ≤\n1\nd−1\n\u00001 −1\np\n\u0001\n, ψd\np,q(f) is a semi-norm in Fd.\nProof. The proof consists of three parts. First we show that the level set Fd\nψdp,q≤ψ = {fw ∈Fd :\nψd\np,q(f) ≤ψ} is a convex set if the condition on d, p, q is satisﬁed. Next, we establish the non-negative\nhomogeneity of ψd\np,q(f). Finally, we show that if a function α : Fd →R is non-negative homogeneous\nand every sublevel set {fw ∈Fd : α(f) ≤ψ} is convex, then α satisﬁes the triangular inequality.\nConvexity of the level sets\nFirst we show that for any two functions f1, f2 ∈Fd\nψp,q≤ψ and 0 ≤α ≤1,\nthe function g = αf1 + (1 −α)f2 is in the model class Fd\nψp,q≤ψ. We prove this by constructing weights\n46\nw that realizes g. Let U and V be the weights of two neural networks such that ψp,q(U) = ψd\np,q(f1) ≤ψ\nand ψp,q(V ) = ψd\np,q(f2) ≤ψ. For every layer i = 1, . . . , d let\n˜Ui =\ndq\nψp,q(U)Ui/∥Ui∥p,q,\n˜Vi =\ndq\nψp,q(V )Vi/∥Vi∥p,q.\nand set W1 =\n\u0014 ˜U1\n˜V1\n\u0015\nfor the ﬁrst layer, Wi =\n\u0014 ˜Ui\n0\n0\n˜Vi\n\u0015\nfor the intermediate layers and Wd =\n\u0002\nα ˜Ud\n(1 −α) ˜Vd\n\u0003\nfor the output layer.\nThen for the deﬁned w, we have fW = αf1 + (1 −α)f2 for rectiﬁed linear and any other non-negative\nhomogeneous activation function. Moreover, for any i < d, the norm of each layer is\n∥Wi∥p,q =\n\u0010\nψp,q(U)\nq\nd + ψp,q(V )\nq\nd\n\u0011 1\nq ≤2\n1\nq ψ\n1\nd\n(5.5.5)\nand in layer d we have:\n∥Wd∥p =\n\u0010\nαpψp,q(U)\np\nd + (1 −α)pψp,q(V )\np\nd\n\u0011 1\np ≤21/p−1ψ1/d\n(5.5.6)\nCombining inequalities (5.5.5) and (5.5.6), we get ψd\np,q(fW ) ≤2\nd−1\nq\n+ 1\np ψ ≤ψ, where the last inequality\nholds because we assume that 1\nq ≤\n1\nd−1\n\u00001 −1\np\n\u0001\n. Thus for every ψ ≥0, Fd\nψp,q≤ψ is a convex set.\nNon-negative homogeneity\nFor any function fw ∈Fd and any α ≥0, let U be the weights realizing f\nwith ψd\np,q(f) = ψp,q(U). Then\nd√αU realizes αf establishing ψd\np,q(αf) ≤ψp,q( d√αU) = αψp,q(U) =\nαψd\np,q(U) = αψd\np,q(f). This establishes the non-negative homogeneity of ψd\np,q.\nConvex sublevel sets and homogeneity imply triangular inequality\nLet α(f) be non-negative ho-\nmogeneous and assume that every sublevel set {fw ∈Fd : α(f) ≤ψ} is convex. Then for f1, f2 ∈Fd,\ndeﬁning ψ1\ndef\n= α(f1), ψ2\ndef\n= α(f2), ˜f1\ndef\n= (ψ1 + ψ2)f1/ψ1, and ˜f2\ndef\n= (ψ1 + ψ2)f2/ψ2, we have\nα(f1 + f2) = α\n\u0012\nψ1\nψ1 + ψ2\n˜f1 +\nψ2\nψ1 + ψ2\n˜f2\n\u0013\n≤ψ1 + ψ2 = α(f1) + α(f2).\nHere the inequality is due to the convexity of the level set and the fact that α( ˜f1) = α( ˜f2) = ψ1 + ψ2,\nbecause of the homogeneity. Therefore α satisﬁes the triangular inequality and thus it is a seminorm.\n5.5.3\nPath Regularization\nTheorem 7\nLemma 21. For any function fw ∈Fd,H\nψp,∞≤ψ there is a layered network with weights w such that\nψp,∞(w) = ψd,H\np,∞(f) and for any internal unit v, P\n(u→v)∈E |wu→v|p = 1.\nProof. Let w be the weights of a network such that ψp,∞(w) = ψd,H\np,∞(f). We now construct a network\nwith weights ew such that ψp,∞(w) = ψd,H\np,∞(f) and for any internal unit v, P\n(u→v)∈E | ew(u →v)|p = 1.\nWe do this by an incremental algorithm. Let w0 = w. At each step i, we do the following.\n47\nConsider the ﬁrst layer, Set Vk to be the set of neurons in the layer k. Let x be the maximum of ℓp norms\nof input weights to each neuron in set V1 and let Ux ⊆V1 be the set of neurons whose ℓp norms of\ntheir input weight is exactly x. Now let y be the maximum of ℓp norms of input weights to each neuron\nin the set V1 \\ Ux and let Uy be the set of the neurons such that the ℓp norms of their input weights\nis exactly y. Clearly y < x. We now scale down the input weights of neurons in set Ux by y/x and\nscale up all the outgoing edges of vertices in Ux by x/y (y cannot be zero for internal neurons based\non the deﬁnition). It is straightforward that the new network realizes the same function and the ℓp,∞\nnorm of the ﬁrst layer has changed by a factor y/x. Now for every neuron v ∈V2, let r(v) be the ℓp\nnorm of the new incoming weights divided by ℓp norm of the original incoming weights. We know that\nr(v) ≤x/y. We again scaly down the input weights of everyv ∈V2 by 1/r(v) and scale up all the\noutgoing edges of v by r(v). Continuing this operation to on each layer, each time we propagate the\nratio to the next layer while the network always realizes the same function and for each layer k, we know\nthat for every v ∈Vk, r(v) ≤x/y. After this operation, in the network, the ℓp,∞norm of the ﬁrst layer\nis scaled down by y/x while the ℓp,∞norm of the last layer is scaled up by at most x/y and the ℓp,∞\nnorm of the rest of the layers has remained the same. Therefore, if wi is the new weight setting, we have\nψp,∞(wi) ≤ψp,∞(wi−1).\nAfter continuing the above step at most |V1| −1 times, the ℓp norm of input weights is the same for all\nneurons in V1. We can then run the same algorithm on other layers and at the end we have a network with\nweight setting ew such that the for each k < d, ℓp norm of input weight to each of the neurons in layer k\nis equal to each other and ψp,∞( ew) ≤ψp,∞(w). This is in fact an equality because weight setting w′\nrealizes function f and we know that ψp,∞(w) = ψd,H\np,∞(f). A simple scaling of weights in layers gives\ncompletes the proof.\nTheorem 7. For p ≥1, any d and (ﬁnite or inﬁnite) H, for any fw ∈Fd,H: φd,H\np\n(f) = ψd,H\np,∞.\nProof. By the Lemma 21, there is a layered network with weights ew such that ψp,∞( ew) = ψd,H\np,∞(f) and\nfor any internal unit v, P\n(u→v)∈E | ew(u →v)|p = 1. Let w be the weights of the layered network that\ncorresponds to the function ew. Then we have:\nvp( ˜w) =\n\n\n\nX\nvin[i]\ne1\n→v1\ne2\n→v2···\nek\n→vout\nk\nY\ni=1\n| ew(ei)|p\n\n\n\n1\np\n(5.5.7)\n=\n\n\nH\nX\nid−1=1\n· · ·\nH\nX\ni1=1\nnin\nX\ni0=1\n|Wd[id−1]|p\nd−1\nY\nk=1\n|W k[ik, ik−1]|p\n\n\n1\np\n(5.5.8)\n=\n\n\nH\nX\nid−1=1\n|Wd[id−1]|p · · ·\nH\nX\ni1=1\n|W k[i2, i1]|p\nnin\nX\ni0=1\n|W k[i1, i0]|p\n\n\n1\np\n(5.5.9)\n=\n\n\nH\nX\nid−1=1\n|Wd[id−1]|p · · ·\nH\nX\ni1=1\n|W k[i2, i1]|p\n\n\n1\np\n(5.5.10)\n=\n\n\nH\nX\nid−1=1\n|Wd[id−1]|p · · ·\nH\nX\ni2=1\n|W k[i3, i2]|p\n\n\n1\np\n(5.5.11)\n=\n\n\nH\nX\nid−1=1\n|Wd[id−1]|p\n\n\n1\np\n= ℓp(Wd) = ψp,∞(w)\n(5.5.12)\n(5.5.13)\n48\nwhere inequalities 5.5.8 to 5.5.12 are due to the fact that the ℓp norm of input weights to each internal\nneuron is exactly 1 and the last equality is again because ℓp,∞of all layers is exactly 1 except the layer\nd.\nProof of Theorem 8\nIn this section, without loss of generality, we assume that all the internal nodes in a DAG have incoming\nedges and outgoing edges because otherwise we can just discard them. Let nout(v) be the longest directed\npath from vertex v to vout and nin(v) be the longest directed path from any input vertex vin[i] to v. We\nsay graph G is a sublayered graph if G is a subgraph of a layered graph.\nWe ﬁrst show the necessary and sufﬁcient conditions under which a DAG is a sublayered graph.\nLemma 22. The graph G(E, V ) is a sublayered graph if and only if any path from input nodes to the\noutput nodes has length d where d is the length of the longest path in G\nProof. Since the internal nodes have incoming edges and outgoing edges; hence if G is a sublayered\ngraph it is straightforward by induction on the layers that for every vertex v in layer i, there is a vertex u\nin layer i + 1 such that (v →u) ∈E and this proves the necessary condition for being sublayered graph.\nTo show the sufﬁcient condition, for any internal node u, u has nin(v) distance from the input node in\nevery path that includes u (otherwise we can build a path that is longer than d). Therefore, for each\nvertex v ∈V , we can place vertex v in layer nin(v) and all the outgoing edges from v will be to layer\nnin(v) + 1.\nLemma 23. If the graph G(E, V ) is not a sublayered graph then there exists a directed edge (u →v)\nsuch that nin(u) + nout(v) < d −1 where d the length of the longest path in G.\nProof. We prove the lemma by an inductive argument. If G is not sublayered, by lemma 22, we know\nthat there exists a path v0 →. . . vi · · · →vd′ where v0 is an input node (nin(v0) = 0), vd′ = vout\n(nout(vd′ = 0) and d′ < d. Now consider the vertex v1. We need to have nout(v1) = d −1 otherwise if\nnout(v1) < d −1 we get nin(u) + nout(v) < d −1 and if nout(v1) > d −1 there will be path in G that is\nlonger than d. Also, since nout(v1) = d −1 and the longest path in G has length d, we have nin(v1) = 1.\nBy applying the same inductive argument on each vertex vi in the path we get nin(vi) = i and nout(vi) =\nd−i. Note that if the condition nin(u)+nout(v) < d−1 is not satisﬁed in one of the steps of the inductive\nargument, the lemma is proved. Otherwise, we have nin(vd′−1) = d′ −1 and nout(vd′−1) = d −d′ + 1\nand therefore nin(vd′−1) + nout(vout) = d′ −1 < d −1 that proves the lemma.\nTheorem 8. For any p ≥1 and any d: ψd\np,∞(f) =\nmin\nG ∈DAG(d) φG\np (f).\nProof. Consider any fw ∈FDAG(d) where the graph G(E, V ) is not sublayered. Let ρ be the total\nnumber of paths from input nodes to the output nodes. Let T be sum over paths of the length of the path.\nWe indicate an algorithm to change G into a sublayered graph ˜G of depth d with weights ˜w such that\nfw = f ˜\nG, ˜\nw and φ(w) = φ( ˜w). Let G0 = G and w0 = w.\nAt each step i, we consider the graph Gi−1. If Gi−1 is sublayered, we are done otherwise by lemma 23,\nthere exists an edge (u →v) such that nin(u) + nout(v) < d −1. Now we add a new vertex ˜vi to graph\nGi−1, remove the edge (u →v), add two edges (u →˜vi) and (˜vi →v) and return the graph as Gi and\nsince we had nin(u) + nout(v) < d −1 in Gi−1, the longest path in Gi still has length d. We also set\nw(u →˜vi) =\np\n|wu→v| and w(˜vi →v) = sign(wu→v)\np\n|wu→v|. Since we are using rectiﬁed linear\n49\nunits activations, for any x > 0, we have [x]+ = x and therefore:\nw(˜vi →v) [w(u →˜vi)o(u)]+ = sign(wu→v)\np\n|wu→v|\nhp\n|wu→v|o(u)\ni\n+\n= sign(wu→v)\np\n|wu→v|\np\n|wu→v|o(u)\n= wu→vo(u)\nSo we conclude that fGi,wi = fGi−1,wi−1. Clearly, since we didn’t change the length of any path from\ninput vertices to the output vertex, we have φ(w) = φ( ˜w). Let Ti be sum over paths of the length of the\npath in Gi. It is clear that Ti−1 ≤Ti because we add a new edge into a path at each step. We also know\nby lemma 22 that if Ti = ρd, then Gi is a sublayered graph. Therefore, after at most ρd −T0 steps, we\nreturn a sublayered graph ˜G and weights ˜w such that fw = f ˜\nG, ˜\nw. We can easily turn the sublayered\ngraph ˜G a layered graph by adding edges with zero weights and this together with Theorem 7 completes\nthe proof.\n5.5.4\nHardness of Learning Neural Networks\nDaniely et al. [11] show in Theorem 5.4 and in Section 7.2 that subject to the strong random CSP\nassumption, for any k = ω(1) the model class of intersection of homogeneous halfspaces over {±1}n\nwith normals in {±1} is not efﬁciently PAC learnable (even improperly)1. Furthermore, for any ϵ > 0,\n[55] prove this hardness result subject to intractability of ˜Q(n1.5\nin )-unique shortest vector problem for\nk = nϵ\nin.\nIf it is not possible to efﬁciently PAC learn intersection of halfspaces (even improperly), we can conclude\nit is also not possible to efﬁciently PAC learn any model class which can represent such intersection. In\nTheorem 24 we show that intersection of homogeneous half spaces can be realized with unit margin by\nneural networks with bound norm.\nTheorem 24. For any k > 0, the intersection of k homogeneous half spaces is realizable with unit\nmargin by F2\nψp,q≤ψ where ψ = 4n\n1\np\nink2.\nProof. The proof is by a construction that is similar to the one in [56]. For each hyperplane ⟨wi, x⟩> 0,\nwhere wi ∈{±1}nin, we include two units in the ﬁrst layer: g+\ni (x) = [⟨wi, x⟩]+ and g−\ni (x) =\n[⟨wi, x⟩−1]+. We set all incoming weights of the output node to be 1. Therefore, this network is\nrealizing the following function:\nf(x) =\nk\nX\ni=1\n([⟨wi, x⟩]+ −[⟨wi, x⟩−1]+)\nSince all inputs and all weights are integer, the outputs of the ﬁrst layer will be integer, ([⟨wi, x⟩]+ −[⟨wi, x⟩−1]+)\nwill be zero or one, and f realizes the intersection of the k halfspaces with unit margin. Now, we just\nneed to make sure that ψ2\np,q(f) is bounded by ψ = 4n\n1\np\nink2:\nψ2\np,q(f) = n\n1\np\nin(2k)\n1\nq (2k)\n1\np\n≤n\n1\np\nin(2k)2 = ψ.\n1Their Theorem 5.4 talks about unrestricted halfspaces, but the construction in Section 7.2 uses only data in {±1}nin and\nhalfspaces speciﬁed by ⟨w, x⟩> 0 with w ∈{±1}nin\n50\n5.6\nDiscussion\nWe presented a general framework for norm-based capacity control for feed-forward networks, and\nanalyzed when the norm-based control is sufﬁcient and to what extent capacity still depends on other\nparameters. In particular, we showed that in depth d > 2 networks, per-unit control with p > 1 and\noverall regularization with p > 2 is not sufﬁcient for capacity control without also controlling the network\nsize. This is in contrast with linear models, where with any p < ∞we have only a weak dependence\non dimensionality, and two-layer networks where per-unit p = 2 is also sufﬁcient for capacity control.\nWe also obtained generalization guarantees for perhaps the most natural form of regularization, namely\nℓ2 regularization, and showed that even with such control we still necessarily have an exponential\ndependence on the depth.\nAlthough the additive µ-measure and multiplication ψ-measure are equivalent at the optimum, they\nbehave rather differently in terms of optimization dynamics (based on anecdotal empirical experience)\nand understanding the relationship between them, as well as the novel path-based regularizer can be\nhelpful in practical regularization of neural networks.\nAlthough we obtained a tight characterization of when size-independent capacity control is possible, the\nprecise polynomial dependence of margin-based classiﬁcation (and other tasks) on the norm in might not\nbe tight and can likely be improved, though this would require going beyond bounding the Rademacher\ncomplexity of the real-valued class. In particular, Theorem 3 gives the same bound for per-unit ℓ1\nregularization and overall ℓ1 regularization, although we would expect the later to have lower capacity.\nBeyond the open issue regarding depth-independent ψ-based capacity control, another interesting open\nquestion is understanding the expressive power of Fd\nψp,q≤ψ, particularly as a function of the depth d.\nClearly going from depth d = 1 to depth d = 2 provides additional expressive power, but it is not clear\nhow much additional depth helps. The class F2 already includes all binary functions over {±1}nin and\nis dense among continuous real-valued functions. But can the ψ-measure be reduced by increasing the\ndepth? Viewed differently: ψd\np,q(f) is monotonically non-increasing in d, but are there functions for it\ncontinues decreasing? Although it seems obvious there are functions that require high depth for efﬁcient\nrepresentation, these questions are related to decade-old problems in circuit complexity and might not be\neasy to resolve.\n51\nChapter 6\nSharpness/PAC-Bayes Generalization\nBounds\nSo far we discussed norm based and sharpness based complexity measures to understand capacity. We\nalso have discussed how to combine these two notions and the tradeoff in scaling between them under\nthe PAC-Bayes framework. We next show how to utilize the general PAC-Bayes bound in Lemma 1 to\nprove generalization guarantees for feedforward networks based on the spectral norm of its layers.\n6.1\nSpectrally-Normalized Margin Bounds\nAs we discussed in Section 3.4, understanding the sharpness of the network is the key step to obtain a\ngeneralization bound using PAC-Bayes framework. The following lemma shows that the sharpness can\nbe bounded by the product of spectral norm of the layers.\nLemma 25 (Perturbation Bound). For any B, d > 0, let fw : XB,n →Rk be a d-layer network. Then\nfor any x ∈XB,n and any perturbation u such that ∥Ui∥2 ≤1\nd ∥Wi∥2, the sharpness of fw can be\nbounded as follows:\n|fw+u(x) −fw(x)|2 ≤eB\n d\nY\ni=1\n∥Wi∥2\n!\nd\nX\ni=1\n∥Ui∥2\n∥Wi∥2\n.\n(6.1.1)\nNext, we derive a generalization guarantee using Lemmas 1 and 25.\nTheorem 26 (Generalization Bound). For any B, d, h > 0, let fw : XB,n →Rk be a d-layer feed-\nforward network with ReLU activations. Then for any probability δ, margin γ > 0, the following\ngeneralization bound holds with probability 1 −δ over the training set:\nL0(fw) ≤ˆℓγ(fw) + O\n\n\ns\nB2d2h ln(dh)Πd\ni=1 ∥Wi∥2 Pd\ni=1\n\u0000∥Wi∥2\nF / ∥Wi∥2\n2\n\u0001\n+ ln dm\nδ\nγ2m\n\n.\n(6.1.2)\nProof. The proof involves mainly two steps. In the ﬁrst step we calculate what is the maximum allowed\nperturbation of parameters to satisfy a given margin condition γ, using Lemma 25. In the second step we\ncalculate the KL term in the PAC-Bayes bound in Lemma 1, for this value of perturbation.\nLet β =\n\u0000Πd\ni=1 ∥Wi∥2\n\u00011/d and consider the reparametrization f\nWi =\nβ\n∥Wi∥2 Wi. Since for feedforward\nnetwork with ReLU activations f ew = fw, the bound in the theorem statement is invariant to this\nreparametrization. W.l.o.g. we assume that for any layer i, ∥Wi∥2 = β. Choose the prior P to be\n52\nN(0, σ2\npI) and consider the random perturbation u ∼N(0, σ2\nqI). The following inequality holds on the\nspectral norm of Ui [57]:\nPUi∼N(0,σq) [∥Ui∥2 > t] ≤2he−t2/2hσ2\nq.\n(6.1.3)\nTaking a union bond over the layers, we get spectral norm of perturbation in each layer is bounded by\nσq\np\n2h ln(4dh). Deﬁne set S as\n\b\nu\n\f\f ∥Ui∥2 ≤σq\np\n2h ln(4dh)\n\t\n. Given the bound on spectral norm of\neach layer, u ∈S with probability at least 1\n2. Let ˆβ be an estimate of β that is picked before observing\ndata. If |ˆβ −β| ≤1\ndβ, then 1\neβd−1 ≤ˆβd−1 ≤eβd−1. Using Lemma 25 with probability at least 1\n2:\nmax\nx∈XB,n |fw+u(x) −fw(x)| ≤edBβd−1 ∥Ui∥2 ≤e2dB ˆβd−1σq\np\n2h ln(4dh) ≤γ\n4 ,\nwhere we choose σq =\nγ\n42dB ˆβd−1√\nh ln(4hd) to get the last inequality.\nLet q(z) be the density function of the posterior. We now calculate the KL-term in Lemma 1 for σp = σq\non the set S:\nKLS(w + u||P) =\nZ\nS\nq(z)2 ⟨z, w⟩−|w|2\n2σ2q\ndz ≤|w|2\n2σ2q\n≤O\n \nB2d2h ln(dh)Πd\ni=1 ∥Wi∥2\n2\nγ2\nd\nX\ni=1\n∥Wi∥2\nF\n∥Wi∥2\n2\n!\nFinally it remains to show how to ﬁnd the estimates ˆβ. We only need to consider values of β in the\nrange\n\u0000 γ\n2B\n\u00011/d ≤β ≤\n\u0010\nγ√m\n2B\n\u00111/d\n. For β outside this range the theorem statement holds trivially.\nRecall that LHS of the theorem statement, L0(fw) is always bounded by 1. If βd <\nγ\n2B then for any\nx, |fw(x)| ≤βdB ≤γ/2 and therefore Lγ = 1. Alternately, if βd > γ√m\n2B , then the second term in\nequation 3.4 is greater than one. Hence, we only need to consider values of β in the range discussed\nabove. Since we need |ˆβ −β| ≤1\ndβ ≤1\nd\n\u0000 γ\n2B\n\u00011/d, the size of this cover is dm\n1\n2d . Taking a union bound\nover this cover and using Lemma 1 gives us the theorem statement.\n6.2\nGeneralization Bound based on Expected Sharpness\nWe showed how bounding the sharpness could give us a generalization bound. We now establish sufﬁcient\nconditions to bound the expected sharpness of a feedforward network with ReLU activations. Such\nconditions serve as a useful guideline in studying what helps an optimization method to converge to less\nsharp optima. Unlike existing generalization bounds [28, 58, 29, 30, 31], our sharpness based bound\ndoes not suffer from exponential dependence on depth.\nNow we discuss the conditions that affect the sharpness of a network. As discussed earlier, weak\ninteractions between layers can cause the network to have high sharpness value. Condition C1 below\nprevents such weak interactions (cancellations). A network can also have high sharpness if the changes in\nthe number of activations is exponential in the perturbations to its weights, even for small perturbations.\nCondition C2 avoids such extreme situations on activations. Finally, if a non-active node with large\nweights becomes active because of the perturbations in lower layers, that can lead to huge changes to the\noutput of the network. Condition C3 prevents having such spiky (in magnitude) hidden units. This leads\nus to the following three conditions, that help in avoiding such pathological cases.\n(C1) : Given x, let x = W0 and D0 = I. Then, for all 0 ≤a < c < b ≤d, ∥\n\u0000Πb\ni=aDiWi\n\u0001\n∥F ≥\nµ\n√hc ∥Πb\ni=c+1DiWi∥F ∥(Πc\ni=aDiWi) ∥F .\n(C2) : Given x, for any level k,\n1\nhk\nP\ni∈[hk] 1Wk,iΠk−1\nj=1 DjWjx≤δ ≤C2δ.\n(C3) : For all i, ∥Wi∥2\n2,∞hi ≤C2\n3∥DiWi∥2\nF .\n53\ncondition number\n0\n1\n2\n3\n4\nprobability density\n0\n2\n4\n6\n8\n10\n12\ninput perturbation\n0\n0.5\n1\nratio of .ipping act.\n0\n0.2\n0.4\n0.6\n0.8\n1\nlayer i\nf(x) = 5x\nC3\n0\n1\n2\n3\nprobability density\n0\n0.5\n1\n1.5\n2\nFigure 6.1: Verifying the conditions of Theorem 27 on a 10 layer perceptron with 1000 hidden units in each\nlayer, i.e. more than 10,000,000 parameters on MNIST. We have numerically checked that all values are within the\ndisplayed range. Left: C1: condition number of the network, i.e. 1\nµ. Middle: C2: the ratio of activations that ﬂip\nbased on magnitude of perturbation. Right: C3 : the ratio of norm of incoming weights to each hidden units with\nrespect to average of the same quantity over hidden units in the layer.\nHere, Wk,i denotes the weights of the ith output node in layer k. ∥Wi∥2,∞denotes the maximum L2\nnorm of a hidden unit in layer i. Now we state our result on the generalization error of a ReLU network,\nin terms of average sharpness and its norm. Let ∥x∥= 1 and h = maxd\ni=1 hi.\nTheorem 27. Let Ui be a random hi × hi−1 matrix with each entry distributed according to N(0, σ2\ni ).\nThen, under the conditions C1, C2, C3, with probability ≥1 −δ,\nEu∼N (0,σ)n [L(fw+u)] −ˆℓ(fw) ≤O\n\u0010h\nΠd\ni=1 (1 + γi) −1\n+Πd\ni=1 (1 + γiC2C3)\n\u0010\nΠd\ni=1(1 + γiCδC2) −1\n\u0011i\nCL\nX\nx\n∥fw(x)∥F\nm\n!\n+\nv\nu\nu\nt 1\nm\n d\nX\ni=1\n∥Wi∥2\nF\nσ2\ni\n+ ln 2m\nδ\n!\n.\nwhere γi =\nσi\n√hi√\nhi−1\nµ2∥Wi∥F\nand Cδ = 2\np\nln(dh/δ).\nTo understand the above generalization error bound, consider choosing γi =\nσ\nCδd, and we get a bound\nthat simpliﬁes as follows:\nEu∼N(0,σ)n [L(fw+u] −ˆℓ(fw) ≤O\n\u0012\nσ (1 + (1 + σC2C3)C2) CL\nP\nx ∥fw(x)∥F\nm\n\u0013\n+\nv\nu\nu\nt 1\nm\n \nd2\nµ4\nd\nX\ni=1\nhihi−1\nσ2\n+ ln 2m\nδ\n!\nIf we choose large σ, then the network will have higher expected sharpness but smaller ’norm’ and\nvice versa. Now one can optimize over the choice of σ to balance between the terms on the right hand\nside and get a better capacity bound. For any reasonable choice of σ, the generalization error above,\ndepends only linearly on depth and does not have any exponential dependence, unlike other notions of\ngeneralization. Also the error gets worse with decreasing µ and increasing C2, C3 as the sharpness of the\nnetwork increases which is in accordance with our discussion of the conditions above.\nAdditionally the conditions C1 −C3 actually hold for networks trained in practice as we verify in\nFigure 6.1, and our experiments suggest that, µ ≥1/4, C2 ≤5 and C3 ≤3. More details on\nthe veriﬁcation and comparing the conditions on learned network with those of random weights, are\npresented in Section 6.4.\n54\nProof of Theorem 27\nWe bound the expectation as follows:\nE\n\f\f\fˆℓ(fw+u(x)) −ˆℓ(fw(x))\n\f\f\f\n≤CLE∥fw+u(x) −fw(x)∥F\n(i)\n= CLE∥(W + u)d\n\u0010\nΠd−1\ni=1 bDi(W + u)i\n\u0011\n∗x −Wd\n\u0000Πd−1\ni=1 DiWi\n\u0001\n∗x∥F\n≤CLE∥(W + u)d\n\u0000Πd−1\ni=1 Di(W + u)i\n\u0001\n∗x −Wd\n\u0000Πd−1\ni=1 DiWi\n\u0001\n∗x∥F\n+ CLE∥(W + u)d\n\u0010\nΠd−1\ni=1 bDi(W + u)i\n\u0011\n∗x −(W + u)d\n\u0000Πd−1\ni=1 Di(W + u)i\n\u0001\n∗x∥F\n≤CLE∥(W + u)d\n\u0000Πd−1\ni=1 Di(W + u)i\n\u0001\n∗x −Wd\n\u0000Πd−1\ni=1 DiWi\n\u0001\n∗x∥F + CLE∥Errd∥F ,\n(6.2.1)\nwhere Errd = ∥(W + u)d\n\u0010\nΠd−1\ni=1 bDi(W + u)i\n\u0011\n∗x −(W + u)d\n\u0000Πd−1\ni=1 Di(W + u)i\n\u0001\n∗x∥F . (i) bDi\nis the diagonal matrix with 0’s and 1’s corresponding to the activation pattern of the perturbed network\nfw+u(x).\nThe ﬁrst term in the equation (6.2.1) corresponds to error due to perturbation of a network with unchanged\nactivations (linear network). Intuitively this is small when any subset of successive layers of the network\ndo no interact weakly with each other (not orthogonal to each other). Condition C1 captures this intuition\nand we bound this error in Lemma 6.4.1.\nLemma 28. Let Ui be a random hi × hi−1 matrix with each entry distributed according to N(0, σ2\ni ).\nThen, under the condition C1,\nE∥(W + u)d\n\u0000Πd−1\ni=1 Di(W + u)i\n\u0001\n∗x −Wd\n\u0000Πd−1\ni=1 DiWi\n\u0001\n∗x∥F\n≤\n \nΠd\ni=1\n \n1 + σi\np\nhihi−1\nµ2∥DiWi∥F\n!\n−1\n!\n∥fw(x)∥F .\nThe second term in the equation (6.2.1) captures the perturbation error due to change in activations. If a\ntiny perturbation can cause exponentially many changes in number of active nodes, then that network\nwill have huge sharpness. Condition C2 and C3 essentially characterize the behavior of sensitivity of\nactivation patterns to perturbations, leading to a bound on this term in Lemma 29.\nLemma 29. Let Ui be a random hi × hi−1 matrix with each entry distributed according to N(0, σ2\ni ).\nThen, under the conditions C1, C2 and C3, with probability ≥1 −δ, for all 1 ≤k ≤d,\n∥bDk −Dk∥1 ≤O\n\u0000C2hkCδσk∥f k−1\nw\n∥F\n\u0001\nand\nE [∥]Errk∥F ≤O\n\u0000Πk\ni=1 (1 + γiC2C3)\n\u0000Πk\ni=1(1 + γiCδC2) −1\n\u0001\n∥f k\nw∥F\n\u0001\n.\nwhere γi =\nσi\n√hi√\nhi−1\nµ2∥DiWi∥F and Cδ = 2\np\nln(dh/δ).\nHence, from Lemma 6.4.1 and Lemma 29 we get,\nE\n\f\f\fˆℓ(fw+u(x)) −ˆℓ(fw(x))\n\f\f\f\n≤\nh\nΠd\ni=1 (1 + γi) −1 + Πd\ni=1 (1 + γiC2C3)\n\u0010\nΠd\ni=1(1 + γiCδC2) −1\n\u0011i\nCL∥fw(x)∥F .\nHere γi =\nσi\n√\nhi√\nhi−1\nµ2∥DiWi∥F . Substituting the above bound on expected sharpness in the PAC-Bayes result (equa-\ntion (3.4)), gives the result.\n55\n6.3\nSupporting Results\n6.3.1\nSupporting Lemma\nLemma 30. Let A ,B be n1 × n2 and n3 × n4 matrices and u be a n2 × n3 entrywise random Gaussian\nmatrix with uij ∼N(0, σ). Then,\nE [∥A ∗u ∗B∥F ] ≤σ∥A∥F ∥B∥F .\nProof. By Jensen’s inequality,\nE [∥A ∗u ∗B∥F ]2 ≤E\n\u0002\n∥A ∗u ∗B∥2\nF\n\u0003\n= E\n\n\n\nX\nij\nX\nkl\nAikuklBlj\n\n\n2\n\n=\nX\nij\nX\nkl\nA2\nikE\n\u0002\nu2\nkl\n\u0003\nB2\nlj\n= σ2∥A∥2\nF ∥B∥2\nF .\n6.3.2\nConditions in Theorem 27\nIn this section, we compare the conditions in Theorem 27 of a learned network with that of its random\ninitialization. We trained a 10-layer feedforward network with 1000 hidden units in each layer on MNIST\ndataset. Figures 6.2, 6.3 and 6.4 compare condition C1, C2 and C3 on learned weights to that of random\ninitialization respectively. Interestingly, we observe that the network with learned weights is very similar\nto its random initialization in terms of these conditions.\n6.4\nProofs\n6.4.1\nProof of Lemma 25\nProof. Let ∆i =\n\f\ff i\nw+u(x) −f i\nw(x)\n\f\f\n2 be the sharpness of layer i. We will prove using induction that\nfor any i ≥0:\n∆i ≤\n\u0012\n1 + 1\nd\n\u0013i  \ni\nY\nj=1\n∥Wj∥2\n!\n|x|2\ni\nX\nj=1\n∥Uj∥2\n∥Wj∥2\n.\nThe above inequality together with\n\u00001 + 1\nd\n\u0001d ≤e proves the lemma statement. The induction base\nclearly holds since ∆0 = |x −x|2 = 0. For any i ≥1, we have the following:\n56\ncondition number\n0\n1\n2\n3\n4\nprobability density\n0\n2\n4\n6\n8\n10\n12\ncondition number\n0\n1\n2\n3\n4\nprobability density\n0\n2\n4\n6\n8\n10\n12\ncondition number\n0\n1\n2\n3\n4\nprobability density\n0\n2\n4\n6\n8\n10\n12\ncondition number\n0\n1\n2\n3\n4\nprobability density\n0\n2\n4\n6\n8\n10\n12\ncondition number\n0\n1\n2\n3\n4\nprobability density\n0\n2\n4\n6\n8\n10\n12\ncondition number\n0\n1\n2\n3\n4\nprobability density\n0\n2\n4\n6\n8\n10\n12\nFigure 6.2: Condition C1: condition number 1\nµ of the network and its decomposition to two cases for random\ninitialization and learned weights. Top: random initialization Bottom: learned weights. Left: distribution of all\ncombinations of a ≤c ≤b −1. Middle: when a < c < b −1. Right: when c = a or c = b −1.\ninput perturbation\n0\n0.5\n1\nratio of .ipping act.\n0\n0.2\n0.4\n0.6\n0.8\n1\nlayer i\nf(x) = 5x\ninput perturbation\n0\n0.5\n1\nratio of .ipping act.\n0\n0.2\n0.4\n0.6\n0.8\n1\nlayer i\nf(x) = 5x\ninput perturbation\n0\n0.005\n0.01\nratio of .ipping act.\n0\n0.01\n0.02\n0.03\n0.04\n0.05\nlayer i\nf(x) = 5x\nFigure 6.3: Ratio of activations that ﬂip based on the magnitude of perturbation. Left: random initialization.\nMiddle: learned weights. Right: learned weights (zoomed in).\n∆i+1 =\n\f\f(Wi+1 + Ui+1) φi(f i\nw+u(x)) −Wi+1φi(f i\nw(x))\n\f\f\n2\n=\n\f\f(Wi+1 + Ui+1)\n\u0000φi(f i\nw+u(x)) −φi(f i\nw(x))\n\u0001\n+ Ui+1φi(f i\nw(x))\n\f\f\n2\n≤(∥Wi+1∥2 + ∥Ui+1∥2)\n\f\fφi(f i\nw+u(x)) −φi(f i\nw(x))\n\f\f\n2 + ∥Ui+1∥2\n\f\fφi(f i\nw(x))\n\f\f\n2\n≤(∥Wi+1∥2 + ∥Ui+1∥2)\n\f\ff i\nw+u(x) −f i\nw(x)\n\f\f\n2 + ∥Ui+1∥2\n\f\ff i\nw(x)\n\f\f\n2\n= ∆i (∥Wi+1∥2 + ∥Ui+1∥2) + ∥Ui+1∥2\n\f\ff i\nw(x)\n\f\f\n2,\nwhere the last inequality is by the Lipschitz property of the activation function and using φ(0) = 0. The\nℓ2 norm of outputs of layer i is bounded by |x|2Πi\nj=1 ∥Wj∥2 and by the lemma assumption we have\n57\nC3\n0\n1\n2\n3\nprobability density\n0\n1\n2\n3\n4\nC3\n0\n1\n2\n3\nprobability density\n0\n1\n2\n3\n4\noutput value\n-50\n0\n50\nprobability density\n0\n0.02\n0.04\n0.06\n0.08\n0.1\noutput value\n-50\n0\n50\nprobability density\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nFigure 6.4: From left to right: Condition C3 for random initialization and learned network, output values for\nrandom and learned network\n∥Ui+1∥2 ≤1\nd ∥Wi+1∥2. Therefore, using the induction step, we get the following bound:\n∆i+1 ≤∆i\n\u0012\n1 + 1\nd\n\u0013\n∥Wi+1∥2 + ∥Ui+1∥2 |x|2\ni\nY\nj=1\n∥Wj∥2\n≤\n\u0012\n1 + 1\nd\n\u0013i+1  i+1\nY\nj=1\n∥Wj∥2\n!\n|x|2\ni\nX\nj=1\n∥Uj∥2\n∥Wj∥2\n+ ∥Ui+1∥2\n∥Wi+1∥2\n|x|2\ni+1\nY\nj=1\n∥Wi∥2\n≤\n\u0012\n1 + 1\nd\n\u0013i+1  i+1\nY\nj=1\n∥Wj∥2\n!\n|x|2\ni+1\nX\nj=1\n∥Uj∥2\n∥Wj∥2\n.\nThis completes the proof.\n6.4.2\nProof of Lemma 28\nProof. Deﬁne g{W−i−j,ui,j}(x) as the network fW with weights in layers i, j,, Wi, Wj replaced by\nUi, uj. Hence,\n∥(W + u)d\n\u0000Πd−1\ni=1 Di(W + u)i\n\u0001\n∗x −Wd\n\u0000Πd−1\ni=1 DiWi\n\u0001\n∗x∥F\n≤∥\nX\ni\ng({W−i, Ui}, x)∥F + ∥\nX\ni,j\ng({W−i−j, ui,j}, x)∥F + · · · + ∥fu(x)∥F\n(6.4.1)\nBase case: First we show the bound for terms with one noisy layer. Let g({W−k, uk}, x) denote fW (x)\nwith weights in layer k, Wk replaced by uk. Now notice that,\nE∥g({W−k, uk}, x)∥F = E∥WdΠd−1\ni=k+1DiWi ∗Dkuk ∗\n\u0000Πk−1\ni=1 DiWi\n\u0001\n∗x∥F\n(i)\n≤σk∥WdΠd−1\ni=k+1DiWi∥F ∥∥\n\u0000Πk−1\ni=1 DiWi\n\u0001\n∗x∥F\n(ii)\n≤σk\np\nhkhk−1\nµ2∥DkWk∥F\n∥Wd\n\u0000Πd−1\ni=1 DiWi\n\u0001\n∗x∥F\n= σk\np\nhkhk−1\nµ2∥DkWk∥F\n∥fW (x)∥F .\n(i) follows from Lemma 30. (ii) follows from condition C1.\nInduction step: Let for any set s ⊂[d], |s| = k, the following holds:\nE∥g({W−i, ui}i∈s, x)∥F ≤∥fW (x)∥F Πi∈sσi\np\nhihi−1\nµ2∥DiWi∥F\n.\n58\nWe will prove this now for terms with k + 1 noisy layers.\nE∥g({W−i, ui}i∈s∪{j}, x)∥F ≤σj\np\nhjhj−1\nµ2∥DjWj∥E∥g({W−i, ui}i∈s, x)∥F\n≤σj\np\nhjhj−1\nµ2∥DjWj∥∥fW (x)∥F Πi∈sσi\np\nhihi−1\nµ2∥DiWi∥F\n= ∥fW (x)∥F Πi∈s∪{j}σi\np\nhihi−1\nµ2∥DiWi∥F\nSubstituting the above expression in equation (6.4.1) gives,\n∥(W + u)d\n\u0000Πd−1\ni=1 Di(W + u)i\n\u0001\n∗x −Wd\n\u0000Πd−1\ni=1 DiWi\n\u0001\n∗x∥F\n≤\n \nΠd\ni=1\n \n1 + σi\n√hi\np\nhi−1\nµ2∥DiWi∥F\n!\n−1\n!\n∥fW (x)∥F .\n6.4.3\nProof of Lemma 29\nProof. We prove this lemma by induction on k. Recall that bDi is the diagonal matrix with 0’s and 1’s\ncorresponding to the activation pattern of the perturbed network fW +u(x). Let 1E denote the indicator\nfunction, that is 1 if the event E is true, 0 else. We also use f k\nW (x) to denote the network truncated to\nlevel k, in particular f k\nW (x) = Πk\ni=1DkWkx.\nBase case:\n∥bD1 −D1∥1 =\nX\ni\n1⟨(W +u)1,i,x⟩∗⟨W1,i,x⟩<0 =\nX\ni\n1⟨(W )1,i,x⟩2<−⟨(u)1,i,x⟩∗⟨(W )1,i,x⟩\n≤\nX\ni\n1|⟨(W )1,i,x⟩|<|⟨(u)1,i,x⟩|.\nSince u1 is a random Gaussian matrix, and ∥x∥≤1, for any i, |⟨(u)1,i, x⟩| ≤σ1(1 + δ1)\np\n2 ln(h1)\nwith probability greater than 1 −δ1. Hence, with probability ≥1 −δ1,\n∥bD1 −D1∥1 ≤\nX\ni\n1|⟨(W )1,i,x⟩|≤σ1√\n20 ln(h1) ≤C2h1σ1(1 + δ1)\np\n2 ln(h1) = C2h1σ1Cδ1.\nThis completes the base case for k = 1. bD1 is a random variable that depends on u1. Hence, in the\nremainder of the proof, to avoid this dependence, we separately bound bD1 −D using the expression\nabove and compute expectation only with respect to u1. With probability ≥1 −δ1,\n59\nE∥Err1∥F = E∥bD1 ∗(W + u)1x −D1 ∗(W + u)1x∥F\n≤E∥( bD1 −D1) ∗W1x∥F + E∥( bD1 −D1) ∗u1x∥F\n(i)\n≤\np\nC2h1σ1Cδ1σ1 +\np\nC2h1σ1Cδ1σ1\n= 2\np\nC2h1σ1Cδ1σ1.\n(i) follows because, each hidden node in E∥( bD1 −D1)∗W1x∥F has norm less than σ1Cδ1 (as it changed\nits activation), number of such units is less than C2h1σ1Cδ1.\nk = 1 case does not capture all the intricacies and dependencies of higher layer networks. Hence we also\nevaluate the bounds for k = 2.\n∥bD2 −D2∥1 ≤\nX\ni\n1⟨(W +u)2,i,f 1\nW +u⟩∗⟨W2,i,f 1\nW⟩≤0 ≤\nX\ni\n1|⟨W2,i,f 1\nW⟩|≤|⟨u2,i,f 1\nW +u⟩|+|⟨W2,i,f 1\nW +u−f 1\nW⟩|\nLet Cδ2 = (1 + δ2)\np\n2 ln(h2). Then, with probability ≥1 −δ1 −δ2,\n\f\f\nu2,i, f 1\nW +u\n\u000b\f\f +\n\f\f\nW2,i, f 1\nW +u −f 1\nW\n\u000b\f\f\n≤Cδ2σ2\n\u0010\n∥f 1\nW ∥F + 2\np\nC2h1σ1Cδ1σ1\n\u0011\n+ ∥W2,i∥2\np\nC2h1σ1Cδ1σ1\n≤Cδ2σ2\n\u0010\n∥f 1\nW ∥F + 2\np\nC2h1σ1Cδ1σ1\n\u0011\n+ C3\n∥D2W2∥F\n√h2\n2\np\nC2h1σ1Cδ1σ1\n(i)\n≤Cδ2σ2\n \n∥f 1\nW ∥F + 2\ns\nˆσ1\np\nhi + hi−1\nˆσ1\n!\n+ 2 ˆσ1\nC3∥fW (x)∥\n1/d\nF\nµ\ns\nˆσ1\np\nhi + hi−1\n= Cδ2σ2\n\u0000∥f 1\nW ∥F + γ1 ˆσ1\n\u0001\n+ C3∥fW (x)∥\n1/d\nF\nµ\nγ1 ˆσ1\nwhere, γi = 2\nr\nˆ\nσ1\n√\nhi+hi−1 . (i) follows from condition C1, which results in Πd\ni=2\nµ∥DiWi∥F\n√hi\nµ∥D1W1x∥F\n√h1\n≤\n∥fW (x)∥F . Hence, if we consider the rebalanced network1 where all layers have same values for\nµ∥DiWi∥F\n√hi\n, we get, µ∥DiWi∥F\n√hi\n≤∥fW (x)∥\n1/d\nF . Also the above equations follow from setting, σi =\nˆσi\nC2Cδi\n√\nhi+hi−1 .\nHence, with probability ≥1 −δ1 −δ2,\n∥bD2 −D2∥1 ≤C2 ∗h2\n \nCδ2σ2\n\u0000∥f 1\nW ∥F + γ1 ˆσ1\n\u0001\n+ C3∥fW (x)∥\n1/d\nF\nµ\nγ1 ˆσ1\n!\n.\nSince, we choose σi to scale as some small number O(σ), in the above expression the ﬁrst term scales as\nO(σ) and the last two terms decay at least as O(σ32). Hence we do not include them in the computation\nof Err.\n1The parameters of ReLu networks can be scaled between layers without changing the function\n60\nE∥Err2∥F = E∥bD2(W + u)2 ∗bD1 ∗(W + u)1x −D2(W + u)2 ∗D1 ∗(W + u)1x∥F\n≤E∥( bD2 −D2)(W + u)2 ∗( bD1 −D1) ∗(W + u)1x∥F + E∥D2(W + u)2 ∗( bD1 −D1) ∗(W + u)1x∥F\n+ E∥( bD2 −D2)(W + u)2 ∗D1 ∗(W + u)1x∥F .\nWe will bound now the ﬁrst term in the above expression. With probability ≥1 −δ1 −δ2,\nE∥( bD2 −D2)(W + u)2 ∗( bD1 −D1) ∗(W + u)1x∥F\n≤E∥( bD2 −D2)W2 ∗( bD1 −D1) ∗W1x∥F + E∥( bD2 −D2)W2 ∗( bD1 −D1) ∗u1x∥F\n+ E∥( bD2 −D2)u2 ∗( bD1 −D1) ∗W1x∥F + E∥( bD2 −D2)u2 ∗( bD1 −D1) ∗u1x∥F\n≤2\nq\nC2 ∗h2Cδ2σ2∥f 1\nW ∥F Cδ2σ2∥f 1\nW ∥F\np\nC2 ∗h1 ∗Cδ1σ1Cδ1σ1\n+ 2\nq\nC2 ∗h2Cδ2σ2∥f 1\nW ∥F Cδ2σ2\np\nh1\np\nC2 ∗h1 ∗Cδ1σ1Cδ1σ1 + O(σ2)\n≤4∥f 2\nW ∥F\nCδ2σ2Cδ1σ1\n√h1\nµ∥D2W2∥F\nΠ2\ni=1\np\nC2hiCδiσi.\nInduction step:\nNow we assume the statement for all i ≤k and prove it for k + 1. ∥bDk −Dk∥1 ≤C2hkCδkσk∥f k−1\nW\n∥F\nand E [∥]Errk∥F ≤Πk\ni=1\n\u0012\n1 +\nσi√\nhi−1\nµ2Ci\n2,∞C2\n\u0013 \u0010\nΠk\ni=1(1 + ˆ\nσi\n3/2\nC2 ) −1\n\u0011\n∥f k\nw∥F . Now we prove the state-\nment for k + 1.\n∥bDk+1 −Dk+1∥1 =\nX\ni\n1⟨(W +u)k+1,i,Πk\ni=1 b\nDi(W +u)i∗x⟩∗⟨W2,i,D1W1x⟩≤0\n≤\nX\ni\n1|⟨Wk+1,i,Πk\ni=1 b\nDi(W +u)i∗x⟩|≤|⟨uk+1,i,Πk\ni=1 b\nDi(W +u)i∗x⟩|\n=\nX\ni\n1|⟨Wk+1,i,f k\nW +u⟩|≤|⟨uk+1,i,f k\nW +u⟩|\n≤\nX\ni\n1|⟨Wk+1,i,f k\nW⟩|≤|⟨uk+1,i,f k\nW⟩|+|⟨uk+1,i,f k\nW +u−f k\nW⟩|+|⟨Wk+1,i,f k\nW +u−f k\nW⟩|\nHence, with probability ≥1 −Pk\ni=1 δi,\n∥bDk+1 −Dk+1∥1 ≤C2hk+1\n\u0002\nCδkσk+1(∥f k\nW ∥F + ∥f k\nW +u −f k\nW ∥F ) + ∥Wk+1,i∥∥f k\nW +u −f k\nW ∥F\n\u0003\n≤C2hk+1Cδkσk+1∥f k\nW ∥F + C2hk+1Cδkσk+1∥f k\nW +u −f k\nW ∥F + C2hk+1∥Wk+1,i∥∥f k\nW +u −f k\nW ∥F .\nNow we will show that the last two terms in the above expression scale as O(σ2). For that, ﬁrst notice\nthat ∥f k\nW +u −f k\nW ∥F ≤\n\u0012\nΠk\ni=1\n\u0012\n1 +\nσi√\nhihi−1\nµ2∥DiWi∥F\n\u0013\n−1\n\u0013\n∥fW (x)∥F + Errk, from lemma 28. Note\nthat the second term in the above expression clearly scale as O(σ2).\nHence,\n∥bDk+1 −Dk+1∥1 ≤C2hk+1Cδkσk+1∥f k\nW ∥F + O(σ2)\n61\n∥Errk+1∥= ∥f k+1\nW +u −˜f k+1\nW +u∥F\n= ∥bDk+1(W + u)k+1Πk+1\ni=1 bDi(W + u)ix −Dk+1(W + u)k+1Πk+1\ni=1 Di(W + u)ix∥F\n≤∥( bDk+1 −Dk+1)(W + u)k+1Πk+1\ni=1 Di(W + u)ix∥F + ∥bDk+1(W + u)k+1Errk∥F\n≤∥( bDk+1 −Dk+1)(W + u)k+1Πk+1\ni=1 Di(W + u)ix∥F + ∥( bDk+1 −Dk+1)(W + u)k+1Errk∥F\n+ ∥Dk+1(W + u)k+1Errk∥F\nSubstituting the bounds for bDk+1 −Dk+1 and Errk gives us, with probability ≥1 −Pk+1\ni=1 δi.\nE∥Errk+1∥≤\nq\nC2hk+1Cδkσk+1∥f k\nW ∥F Cδkσk+1∥f k\nW ∥F E∥Πk+1\ni=1 Di(W + u)ix∥F\n+ E∥Errk∥F\n\u0012q\nC2hk+1Cδkσk+1∥f k\nW ∥F Cδkσk+1∥f k\nW ∥F + ∥Dk+1Wk+1∥F + σk+1\np\nhk+1\n\u0013\nNow we bound the above terms following the same approach as in proof of Lemma 28, by considering\nall possible replacements of Wi with Ui. That gives us the result.\n62\nChapter 7\nEmpirical Investigation\nIn this chapter we investigate the ability of the discussed measures to explain the different generalization\nphenomenon.\n7.1\nComplexity Measures\nCapacity control in terms of norm, when using a zero/one loss (i.e. counting errors) requires us in addition\nto account for scaling of the output of the neural networks, as the loss is insensitive to this scaling but\nthe norm only makes sense in the context of such scaling. For example, dividing all the weights by the\nsame number will scale down the output of the network but does not change the 0/1 loss, and hence it is\npossible to get a network with arbitrary small norm and the same 0/1 loss. Using a scale sensitive losses,\nsuch as the cross entropy loss, does address this issue (if the outputs are scaled down toward zero, the\nloss becomes trivially bad), and one can obtain generalization guarantees in terms of norm and the cross\nentropy loss.\nHowever, we should be careful when comparing the norms of different models learned by minimizing\nthe cross entropy loss, in particular when the training error goes to zero. When the training error goes\nto zero, in order to push the cross entropy loss (or any other positive loss that diminish at inﬁnity) to\nzero, the outputs of the network must go to inﬁnity, and thus the norm of the weights (under any norm)\nshould also go to inﬁnity. This means that minimizing the cross entropy loss will drive the norm toward\ninﬁnity. In practice, the search is terminated at some ﬁnite time, resulting in large, but ﬁnite norm. But\nthe value of this norm is mostly an indication of how far the optimization is allowed to progress—using a\nstricter stopping criteria (or higher allowed number of iterations) would yield higher norm. In particular,\ncomparing the norms of models found using different optimization approaches is meaningless, as they\nwould all go toward inﬁnity.\nInstead, to meaningfully compare norms of the network, we should explicitly take into account the\nscaling of the outputs of the network. One way this can be done, when the training error is indeed zero,\nis to consider the “margin” of the predictions in addition to the norms of the parameters. We refer to the\nmargin for a single data point x as the difference between the score of the correct label and the maximum\nscore of other labels, i.e.\nfw(x)[ytrue] −max\ny̸=ytrue fw(x)[y]\n(7.1.1)\nIn order to measure scale over an entire training set, one simple approach is to consider the “hard margin”,\nwhich is the minimum margin among all training points. However, this deﬁnition is very sensitive to\nextreme points as well as to the size of the training set. We consider instead a more robust notion that\nallows a small portion of data points to violate the margin. For a given training set and small value ϵ > 0,\n63\nwe deﬁne the margin γmargin as the lowest value of γ such that ⌈ϵm⌉data point have margin lower than γ\nwhere m is the size of the training set. We found empirically that the qualitative and relative nature of\nour empirical results is almost unaffected by reasonable choices of ϵ (e.g. between 0.001 and 0.1).\nThe norm-based measures we investigate in this work and their corresponding capacity bounds are as\nfollows 1:\n• ℓ2 norm with capacity proportional to\n1\nγ2\nmargin\nQd\ni=1 4 ∥Wi∥2\nF [58].\n• ℓ1-path norm with capacity proportional to\n1\nγ2\nmargin\n\u0010P\nj∈Qd\nk=0[hk]\n\f\f\fQd\ni=1 2Wi[ji, ji−1]\n\f\f\f\n\u00112\n[28, 58].\n• ℓ2-path norm with capacity proportional to\n1\nγ2\nmargin\nP\nj∈Qd\nk=0[hk]\nQd\ni=1 4hiW 2\ni [ji, ji−1].\n• spectral norm with capacity proportional to\n1\nγ2\nmargin\nQd\ni=1 hi ∥Wi∥2\n2.\nwhere Qd\nk=0[hk] is the Cartesian product over sets [hk]. The above bounds indicate that capacity can be\nbounded in terms of either ℓ2-norm or ℓ1-path norm independent of number of parameters. The ℓ2-path\nnorm dependence on the number of hidden units in each layer is unavoidable. However, it is not clear\nthat the dependence on the number of parameters is needed for the bound based on the spectral norm.\nPAC-Bayes Bound\nA simple way to instantiate the PAC-Based bound discussed in Section 3.4 is to\nset P to be a zero mean, σ2 variance Gaussian distribution. Choosing the perturbation ϵ to also be a zero\nmean spherical Gaussian with variance σ2 in every direction, yields the following guarantee (w.p. 1 −δ\nover the training set):\nEϵ∼N (0,σ)n [L(fw+ϵ)] ≤ˆL(fw) + Eϵ∼N (0,σ)n\nh\nˆL(fw+ϵ)\ni\n−ˆL(fw)\n|\n{z\n}\nexpected sharpness\n+4\nv\nu\nu\nu\nt\n1\nm\n\u0012 ∥w∥2\n2\n2σ2\n| {z }\nKL\n+ ln 2m\nδ\n\u0013\n,\n(7.1.2)\nAnother interesting approach is to set the variance of the perturbation to each parameter with respect to\nthe magnitude of the parameter. For example if σi = α|wi|+β, then the KL term in the above expression\nchanges to P\ni\nw2\ni\n2σ2\ni .\nThe above generalization guarantees give a clear way to think about capacity control jointly in terms of\nboth the expected sharpness and the norm, and as we discussed earlier indicates that sharpness by itself\ncannot control the capacity without considering the scaling. In the above generalization bound, norms\nand sharpness interact in a direct way depending on σ, as increasing the norm by decreasing σ causes\ndecrease in sharpness and vice versa. It is therefore important to ﬁnd the right balance between the norm\nand sharpness by choosing σ appropriately in order to get a reasonable bound on the capacity.\n7.2\nExperiments Settings\nIn experiment with different network sizes, we train a two layer perceptron with ReLU activation and\nvarying number of hidden units without Batch Normalization or dropout. In the rest of the experiments,\nwe train a modiﬁed version of the VGG architecture [59] with the conﬁguration 2 × [64, 3, 3, 1], 2 ×\n[128, 3, 3, 1], 2 × [256, 3, 3, 1], 2 × [512, 3, 3, 1] where we add Batch Normalization before ReLU\n1We have dropped the term that only depend on the norm of the input. The bounds based on ℓ2-path norm and spectral norm\ncan be derived directly from the those based on ℓ1-path norm and ℓ2 norm respectively. Without further conditions on weights,\nexponential dependence on depth is tight but the 4d dependence might be loose [58]. We will also discuss a rather loose bound on\nthe capacity based on the spectral norm in Section 3.3.\n64\nactivations and apply 2 × 2 max-pooling with window size 2 and dropout after each stack. Convolutional\nlayers are followed by 4 × 4 average pooling, a fully connected layer with 512 hidden units and ﬁnally a\nlinear layer is added for prediction.\nIn all experiments we train the networks using stochastic gradient descent (SGD) with mini-batch size 64,\nﬁxed learning rate 0.01 and momentum 0.9 without weight decay. In all experiments where achieving\nzero training error is possible, we continue training until the cross-entropy loss is less than 10−4.\nWhen calculating norms on a network with a Batch Normalization layer, we reparametrize the network\nto one that represents the exact same function without Batch Normalization as suggested in [60]. In\nall our ﬁgures we plot norm divided by margin to avoid scaling issues (see Section 7.1), where we\nset the margin over training set S to be 5th-percentile of the margins of the data points in S, i.e.\nPrc5 {fw(xi)[yi] −maxy̸=yi fw(x)[y]|(xi, yi) ∈S} . We have also investigated other versions of the\nmargin and observed similar behavior to this notion.\nWe calculate the sharpness, as suggested in [13] - for each parameter wi we bound the magnitude of\nperturbation by α(|wi| + 1) for α = 5.10−4. In order to compute the maximum perturbation (maximize\nthe loss), we perform 2000 updates of stochastic gradient ascent starting from the minimum, with\nmini-batch size 64, ﬁxed step size 0.01 and momentum 0.9.\nTo compute the expected sharpness, we perturb each parameter wi of the model with noise generated from\nGaussian distribution with zero mean and standard deviation, α(10|wi| + 1). The expected sharpness is\naverage over 1000 random perturbations each of which are averaged over a mini-batch of size 64. We\ncompute the expected sharpness for different choices of α. For each value of α the KL divergence can be\ncalculated as\n1\nα2\nP\ni\n\u0010\nwi\n(10|wi|+1)\n\u00112\n.\n7.3\nTrue Labels Vs. Random Labels\nAs an initial empirical investigation of the appropriateness of the different complexity measures, we\ncompared the complexity (under each of the above measures) of models trained on true versus random\nlabels. We would expect to see two phenomena: ﬁrst, the complexity of models trained on true\nlabels should be substantially lower than those trained on random labels, corresponding to their better\ngeneralization ability. Second, when training on random labels, we expect capacity to increase almost\nlinearly with the number of training examples, since every extra example requires new capacity in order\nto ﬁt it’s random label. However, when training on true labels we expect the model to capture the true\nfunctional dependence between input and output and thus ﬁtting more training examples should only\nrequire small increases in the capacity of the network. The results are reported in Figure 7.1. We indeed\nobserve a gap between the complexity of models learned on real and random labels for all four norms,\nwith the difference in increase in capacity between true and random labels being most pronounced for\nthe ℓ2 norm and ℓ2-path norm.\nIn our experiments on PAC-Bayes bound, we observe that looking at both sharpness and norm in\nEquation 7.1.1 jointly indeed makes a better predictor for the generalization error. As discussed earlier,\nDziugaite and Roy [32] numerically optimize the overall PAC-Bayes generalization bound over a family\nof multivariate Gaussian distributions (different choices of perturbations and priors). Since the precise\nway the sharpness and KL-divergence are combined is not tight, certainly not in (7.1.2), nor in the more\nreﬁned bound used by Dziugaite and Roy [32], we prefer shying away from numerically optimizing the\nbalance between sharpness and the KL-divergence. Instead, we propose using bi-criteria plots, where\nsharpness and KL-divergence are plotted against each other, as we vary the perturbation variance. For\nexample, in the center and right panels of Figure 7.2 we show such plots for networks trained on true\nand random labels respectively. We see that although sharpness by itself is not sufﬁcient for explaining\ngeneralization in this setting (as we saw in the left panel), the bi-criteria plots are signiﬁcantly lower for\nthe true labels. Even more so, the change in the bi-criteria plot as we increase the number of samples\n65\nsize of traning set\n10K 20K 30K 40K 50K\n1020\n1025\n1030\ntrue labels\nrandom labels\nsize of traning set\n10K 20K 30K 40K 50K\n1025\n1030\n1035\nsize of traning set\n10K 20K 30K 40K 50K\n100\n102\n104\nsize of traning set\n10K 20K 30K 40K 50K\n105\n1010\n1015\nℓ2 norm\nℓ1-path norm\nℓ2-path norm\nspectral norm\nFigure 7.1: Comparing different complexity measures on a VGG network trained on subsets of CIFAR10 dataset\nwith true (blue line) or random (red line) labels. We plot norm divided by margin to avoid scaling issues (see Sec-\ntion 7.1), where for each complexity measure, we drop the terms that only depend on depth or number of hidden units;\ne.g. for ℓ2-path norm we plot γ−2\nmargin\nP\nj∈Qd\nk=0[hk]\nQd\ni=1 W 2\ni [ji, ji−1].We also set the margin over training set S to\nbe 5th-percentile of the margins of the data points in S, i.e. Prc5 {fw(xi)[yi] −maxy̸=yi fw(x)[y]|(xi, yi) ∈S}.\nIn all experiments, the training error of the learned network is zero. The plots indicate that these measures can\nexplain the generalization as the complexity of model learned with random labels is always higher than the one\nlearned with true labels. Furthermore, the gap between the complexity of models learned with true and random\nlabels increases as we increase the size of the training set.\nsize of traning set\n10K\n20K\n30K\n40K\n50K\nsharpness\n0.4\n0.6\n0.8\n1\n1.2\ntrue labels\nrandom labels\nKL\n#108\n0\n1\n2\n3\nexpected sharpness\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n5K\n10K\n30K\n50K\nKL\n#108\n0\n1\n2\n3\nexpected sharpness\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n5K\n10K\n30K\n50K\nFigure 7.2: Sharpness and PAC-Bayes measures on a VGG network trained on subsets of CIFAR10 dataset with\ntrue or random labels. In the left panel, we plot max sharpness, which we calculate as suggested by Keskar et al. [13]\nwhere the perturbation for parameter wi has magnitude 5.10−4(|wi| + 1). The middle and right plots demonstrate\nthe relationship between expected sharpness and KL divergence in PAC-Bayes analysis for true and random labels\nrespectively. For PAC-Bayes plots, each point in the plot correspond to a choice of variable α where the standard\ndeviation of the perturbation for the parameter i is α(10|wi| + 1). The corresponding KL to each α is nothing but\nweighted ℓ2 norm where the weight for each parameter is the inverse of the standard deviation of the perturbation.\ntrue labels\nrandom labels\nis signiﬁcantly larger with random labels, correctly capturing the required increase in capacity. For\nexample, to get a ﬁxed value of expected sharpness such as ϵ = 0.05, networks trained with random\nlabels require higher norm compared to those trained with true labels. This behavior is in agreement\nwith our earlier discussion, that sharpness is sensitive to scaling of the parameters and is not a capacity\ncontrol measure as it can be artiﬁcially changed by scaling the network. However, combined with the\nnorm, sharpness does seem to provide a capacity measure.\n7.4\nDifferent Global Minima\nGiven different global minima of the training loss on the same training set and with the same model class,\ncan these measures indicate which model is going to generalize better? In order to verify this property,\nwe can calculate each measure on several different global minima and see if lower values of the measure\nimply lower generalization error. In order to ﬁnd different global minima for the training loss, we design\nan experiment where we force the optimization methods to converge to different global minima with\nvarying generalization abilities by forming a confusion set that includes samples with random labels. The\n66\n#random labels\n0\n1K\n2K\n3K\n4K\n5K\nerror\n0\n0.1\n0.2\n0.3\n0.4\ntraining\ntest\n#random labels\n0\n1K\n2K\n3K\n4K\n5K\nmeasure\n0\n0.2\n0.4\n0.6\n0.8\n1\n`2 norm\nspectral norm\npath-`1 norm\npath-`2 norm\nsharpness\nKL\n#107\n0\n1\n2\n3\n4\n5\nexpected sharpness\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0\n1K\n2K\n3K\n4K\n5K\nFigure 7.3: Experiments on global minima with poor generalization. For each experiment, a VGG network is\ntrained on union of a subset of CIFAR10 dataset with size 10000 containing samples with true labels and another\nsubset of CIFAR10 datasets with varying size containing random labels. The learned networks are all global minima\nfor the objective function on the subset with true labels. The left plot indicates the training and test errors based on\nthe size of the set with random labels. The plot in the middle shows change in different measures based on the size\nof the set with random labels. The plot on the right indicates the relationship between expected sharpness and KL in\nPAC-bayes for each of the experiments. Measures are calculated as explained in Figures 7.1 and 7.2.\noptimization is done on the loss that includes examples from both the confusion set and the training set.\nSince deep learning models have very high capacity, the optimization over the union of confusion set and\ntraining set generally leads to a point with zero error over both confusion and training sets which thus is\na global minima for the training set.\nWe randomly select a subset of CIFAR10 dataset with 10000 data points as the training set and our goal\nis to ﬁnd networks that have zero error on this set but different generalization abilities on the test set. In\norder to do that, we train networks on the union of the training set with ﬁxed size 10000 and confusion\nsets with varying sizes that consists of CIFAR10 samples with random labels; and we evaluate the learned\nmodel on an independent test set. The trained network achieves zero training error but as shown in Figure\n7.3, the test error of the model increases with increasing size of the confusion set. The middle panel of\nthis Figure suggests that the norm of the learned networks can indeed be predictive of their generalization\nbehavior. However, we again observe that sharpness has a poor behavior in these experiments. The right\npanel of this ﬁgure also suggests that PAC-Bayes measure of joint sharpness and KL divergence, has\nbetter behavior - for a ﬁxed expected sharpness, networks that have higher generalization error, have\nhigher norms.\n7.5\nIncreasing Network Size\nWe also repeat the experiments conducted by Neyshabur et al. [27] where a fully connected feedforward\nnetwork is trained on MNIST dataset with varying number of hidden units and we check the values\nof different complexity measures on each of the learned networks.The left panel in Figure 7.4 shows\nthe training and test error for this experiment. While 32 hidden units are enough to ﬁt the training\ndata, we observe that networks with more hidden units generalize better. Since the optimization is done\nwithout any explicit regularization, the only possible explanation for this phenomenon is the implicit\nregularization by the optimization algorithm. Therefore, we expect a sensible complexity measure to\ndecrease beyond 32 hidden units and behave similar to the test error. Different measures are reported for\nlearned networks. The middle panel suggest that all margin/norm based complexity measures decrease\nfor larger networks up to 128 hidden units. For networks with more hidden units, ℓ2 norm and ℓ1-path\nnorm increase with the size of the network. The middle panel suggest that ℓ2-path norm can provide\nsome explanation for this phenomenon. However, as we discussed in Section 7.1, the actual complexity\nmeasure based on ℓ2-path norm also depends on the number of hidden units and taking this into account\nindicates that the measure based on ℓ2-path norm cannot explain this phenomenon. This is also the case\nfor the margin based measure that depends on the spectral norm. In subsection 3.3 we discussed another\n67\n#hidden units\n8\n32\n128\n512\n2K\n8K\nerror\n0\n0.02\n0.04\n0.06\n0.08\ntraining\ntest\n#hidden units\n32\n128\n512\n2K\n8K\nmeasure\n0\n0.2\n0.4\n0.6\n0.8\n1\n`2 norm\nspectral norm\npath-`1 norm\npath-`2 norm\nsharpness\nKL\n#106\n0\n1\n2\nexpected sharpness\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n32\n128\n512\n2048\nFigure 7.4: The generalization of two layer perceptron trained on MNIST dataset with varying number of hidden\nunits. The left plot indicates the training and test errors. The test error decreases as the size increases. The middle\nplot shows different measures for each of the trained networks. The plot on the right indicates the relationship\nbetween expected sharpness and KL in PAC-Bayes for each of the experiments. Measures are calculated as explained\nin Figures 7.1 and 7.2.\ncomplexity measure that also depends the spectral norm through Lipschitz continuity or robustness\nargument. Even though this bound is very loose, it is monotonic with respect to the spectral norm that is\nreported in the plots. Unfortunately, we do observe some increase in spectral norm by increasing number\nof hidden units beyond 512. The right panel shows that the joint PAC-Bayes measure decrease for larger\nnetworks up to size 128 but fails to explain this generalization behavior for larger networks. This suggests\nthat the measures looked so far are not sufﬁcient to explain all the generalization phenomenon observed\nin neural networks.\n68\nPart II\nGeometry of Optimization and\nGeneralization\n69\nChapter 8\nInvariances\nIn Chapter 4, we discussed how optimization is related to generalization due to the implicit regularization.\nRevisiting the choice of gradient descent, we recall that optimization is also inherently tied to a choice\nof geometry or measure of distance, norm or divergence. Gradient descent for example is tied to the ℓ2\nnorm as it is the steepest descent with respect to ℓ2 norm in the parameter space, while coordinate descent\ncorresponds to steepest descent with respect to the ℓ1 norm and exp-gradient (multiplicative weight)\nupdates is tied to an entropic divergence. Moreover, at least when the objective function is convex,\nconvergence behavior is tied to the corresponding norms or potentials. For example, with gradient\ndescent, or SGD, convergence speeds depend on the ℓ2 norm of the optimum. The norm or divergence\ncan be viewed as a regularizer for the updates. There is therefore also a strong link between regularization\nfor optimization and regularization for learning: optimization may provide implicit regularization in\nterms of its corresponding geometry, and for ideal optimization performance the optimization geometry\nshould be aligned with inductive bias driving the learning [61].\nIs the ℓ2 geometry on the weights the appropriate geometry for the space of deep networks? Or can we\nsuggest a geometry with more desirable properties that would enable faster optimization and perhaps\nalso better implicit regularization? As suggested above, this question is also linked to the choice of an\nappropriate regularizer for deep networks.\nFocusing on networks with RELU activations in this section, we observe that scaling down the incoming\nedges to a hidden unit and scaling up the outgoing edges by the same factor yields an equivalent network\ncomputing the same function. Since predictions are invariant to such rescalings, it is natural to seek a\ngeometry, and corresponding optimization method, that is similarly invariant. In this chapter, we study\ninvariances in feedforward networks with shared weights.\n8.1\nInvariances in Feedforward and Recurrent Neural Networks\nFeedforward networks are highly over-parameterized, i.e. there are many weight settings w that represent\nthe same function fw. Since our true object of interest is the function f, and not the identity w of\nthe weghts, it would be beneﬁcial if optimization would depend only on fw and not get “distracted”\nby difference in w that does not affect fw. It is therefore helpful to study the transformations on the\nweights that will not change the function presented by the network and come up with methods that their\nperformance is not affected by such transformations.\nDeﬁnition 1. We say a class of neural networks is invariant to a transformation T if for any parameter\nsetting p and its corresponding weights w, fw = fT (w). Similarly, we say an update rule A is invariant\nto T if for any p and its corresponding w, fA(w) = fA(T (w)).\n70\n0\n100\n200\n300\n0\n0.5\n1\n1.5\n2\n2.5\nEpoch\nObjective\n \n \nBalanced\nUnbalanced\n(a) Training on MNIST\n100 \n10-4 \nSGD \nUpdate \n100 \n~100 \n~104 \n~100 \n1 \n1 \nRescaling \n1 \nu\nv\nu\nv\nu\nv\n≈ \n(b) Weight explosion in an unbalanced network\n8\n6\n8\n3\n7\n7\n8\n4\nv\nu\n6\n1\n1\n1\n4\n2\n1\n1\nv\nu\n60.2\n10.5\n70.1\n10.2\n30.4\n20.5\n70.1\n30.1\nv\nu\n60\n10\n0.1\n10\n0.4\n20\n0.1\n0.1\nv\nu\nSGD  \nUpdate\nSGD  \nUpdate\n≈\nRescaling\n(c) Poor updates in an unbalanced network\nFigure 8.1: (a): Evolution of the cross-entropy error function when training a feed-forward network on MNIST\nwith two hidden layers, each containing 4000 hidden units. The unbalanced initialization (blue curve) is generated\nby applying a sequence of rescaling functions on the balanced initializations (red curve). (b): Updates for a simple\ncase where the input is x = 1, thresholds are set to zero (constant), the stepsize is 1, and the gradient with respect\nto output is δ = −1. (c): Updated network for the case where the input is x = (1, 1), thresholds are set to zero\n(constant), the stepsize is 1, and the gradient with respect to output is δ = (−1, −1).\nInvariances have also been studied as different mappings from the parameter space to the same function\nspace [62] while we deﬁne the transformation as a mapping inside a ﬁxed parameter space. A very\nimportant invariance in feedforward networks is node-wise rescaling [63]. For any internal node v and\nany scalar α > 0, we can multiply all incoming weights into v (i.e. wu→v for any (u →v) ∈E) by α\nand all the outgoing weights (i.e. wv→u for any (v →u) ∈E) by 1/α without changing the function\ncomputed by the network. Not all node-wise rescaling transformations can be applied in feedforward\nnets with shared weights. This is due to the fact that some weights are forced to be equal and therefore,\nwe are only allowed to change them by the same scaling factor.\nDeﬁnition 2. Given a class of neural networks, we say an invariant transformation T that is deﬁned\nover edge weights is feasible for parameter mapping π if the shared weights remain equal after the\ntransformation, i.e. for any i and for any e, e′ ∈Ei, T (w)e = T (w)e′.\nWe have discussed the complete characterize all feasible node-wise invariances of RNNs in [64].\nUnfortunately, gradient descent is not rescaling invariant. The main problem with the gradient updates is\nthat scaling down the weights of an edge will also scale up the gradient which, as we see later, is exactly\nthe opposite of what is expected from a rescaling invariant update.\nFurthermore, gradient descent performs very poorly on “unbalanced” networks. We say that a network\nis balanced if the norm of incoming weights to different units are roughly the same or within a small\nrange. For example, Figure 8.1(a) shows a huge gap in the performance of SGD initialized with a\nrandomly generated balanced network, when training on MNIST, compared to a network initialized with\nunbalanced weights. Here the unbalanced weights are generated by applying a sequence of random\nrescaling functions on the balanced weights to create a rescaling equivalent unbalanced network.\nIn an unbalanced network, gradient descent updates could blow up the smaller weights, while keeping\nthe larger weights almost unchanged. This is illustrated in Figure 8.1(b). If this were the only issue, one\ncould scale down all the weights after each update. However, in an unbalanced network, the relative\nchanges in the weights are also very different compared to a balanced network. For example, Figure\n71\n𝑎\n𝑎\n𝑏\n𝑏\n𝑏𝑎\n#\n𝑎𝑏\n#\n1\n1\n𝑐𝑎\n⁄\n𝑐𝑏\n#\n𝑑𝑎\n#\n𝑑𝑏\n#\n1 𝑐\n#\n1 𝑑\n#\n1 𝑐\n#\n1 𝑑\n#\n1\n1\n𝑑𝑐\n#\n𝑐𝑑\n#\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nT\nFigure 8.2: An example of in-\nvariances in an RNN with two\nhidden layers each of which has\n2 hidden units. The dashed lines\ncorrespond to recurrent weights.\nThe network on the left hand\nside is equivalent (i.e.\nrepre-\nsents the same function) to the\nnetwork on the right for any\nnonzero α1\n1 = a, α1\n2 = b, α2\n1 =\nc, α2\n2 = d.\n8.1(c) shows how two rescaling equivalent networks could end up computing a very different function\nafter only a single update.\nTherefore, it is helpful to understand what are the feasible node-wise rescalings for RNNs. In the\nfollowing theorem, we characterize all feasible node-wise invariances in RNNs.\nTheorem 31. For any α such that αi\nj > 0, any Recurrent Neural Network with ReLU activation is\ninvariant to the transformation Tα ([Win, Wrec, Wout]) = [Tin,α (Win) , Trec,α (Wrec) , Tout,α (Wout)]\nwhere for any i, j, k:\nTin,α(Win)i[j, k] =\n(\nαi\njWi\nin[j, k]\ni = 1,\n\u0000αi\nj/αi−1\nk\n\u0001\nWi\nin[j, k]\n1 < i < d,\n(8.1.1)\nTrec,α(Wrec)i[j, k] =\n\u0000αi\nj/αi\nk\n\u0001\nWi\nrec[j, k],\nTout,α(Wout)[j, k] =\n\u00001/αd−1\nk\n\u0001\nWout[j, k].\nFurthermore, any feasible node-wise rescaling transformation can be presented in the above form.\nThe proof is given in Section 8.3. The above theorem shows that there are many transformations under\nwhich RNNs represent the same function. An example of such invariances is shown in Fig. 8.2. Therefore,\nwe would like to have optimization algorithms that are invariant to these transformations and in order to\ndo so, we need to look at measures that are invariant to such mappings.\n8.2\nUnderstanding Invariances\nThe goal of this section is to discuss whether being invariant to node-wise rescaling transformations is\nsufﬁcient or not.\nIdeally we would like our algorithm to be at least invariant to all the transformations to which the model\nG is invariant. Note that this is different than the invariances studied in [62], in that they study algorithms\nthat are invariant to reparametrizations of the same model but we look at transformations within the the\nparameter space that preserve the function in the model. This will eliminate the need for non-trivial\ninitialization. Thus our goal is to characterize the whole variety of transformations to which the model is\ninvariant and check if the algorithm is invariant to all of them.\nWe ﬁrst need to note that invariance can be composed. If a network G is invariant to transformations\nT1 and T2, it is also invariant to their composition T1 ◦T2. This is also true for an algorithm. If an\nalgorithm is invariant to transformations T1 and T2, it is also invariant to their composition. This is\nbecause fT2◦T1◦A(w) = fT2◦A(T1◦w) = fA(T2◦T1(w)).\nThen it is natural to talk about the basis of invariances. The intuition is that although there are inﬁnitely\nmany transformations to which the model (or an algorithm) is invariant, they could be generated as\ncompositions of ﬁnite number of transformations.\n72\nIn fact, in the inﬁnitesimal limit the directions of inﬁnitesimal changes in the parameters to which the\nfunction fw is insensitive form a subspace. This is because for a ﬁxed input x, we have\nfw+∆(x) = fw(x) +\nX\ne∈E\n∂fw(x)\n∂we\n· ∆e + O(∥∆∥2),\n(8.2.1)\nwhere E is the set of edges, due to a Taylor expansion around w. Thus the function fw is insensitive (up\nto O(∥∆∥2)) to any change in the direction ∆that lies in the (right) null space of the Jacobian matrix\n∂fw(x)/∂w for all input x simultaneously. More formally, the subspace can be deﬁned as\nN(w) =\n\\\nx∈R|Vin| Null\n\u0012∂fw(x)\n∂w\n\u0013\n.\n(8.2.2)\nAgain, any change to w in the direction ∆that lies in N(w) leaves the function fw unchanged (up\nto O(∥∆∥2)) at every input x. Therefore, if we can calculate the dimension of N(w) and if we\nhave dimN(w) = |Vinternal|, where we denote the number of internal nodes by |Vinternal|, then we\ncan conclude that all inﬁnitesimal transformations to which the model is invariant can be spanned by\ninﬁnitesimal node-wise rescaling transformations.\nNote that the null space N(w) and its dimension is a function of w. Therefore, there are some points\nin the parameter space that have more invariances than other points. For example, suppose that v is\nan internal node with ReLU activation that receives connections only from other ReLU units (or any\nunit whose output is nonnegative). If all the incoming weights to v are negative including the bias, the\noutput of node v will be zero regardless of the input, and the function fw will be insensitive to any\ntransformation to the outgoing weights of v. Nevertheless we conjecture that as the network size grows,\nthe chance of being in such a degenerate conﬁguration during training will diminish exponentially.\nWhen we study the dimension of N(w), it is convenient to analyze the dimension of the span of the row\nvectors of the Jacobian matrix ∂fw(x)/∂w instead. We deﬁne the degrees of freedom of model G at w\nas\ndG(w) = dim\n\u0012[\nx∈R|Vin| Span\n\u0012∂fw(x)\n∂w\n[v, :] : v ∈Vout\n\u0013\u0013\n,\n(8.2.3)\nwhere ∂fw(x)[v, :]/∂w denotes the vth row vector of the Jacobian matrix and x runs over all possible\ninput x. Intuitively, dG(w) is the dimension of the set of directions that changes fw(x) for at least one\ninput x.\nDue to the rank nullity theorem dG(w) and the dimension of N(w) are related as follows:\ndG(w) + dim (N(w)) = |E|,\nwhere |E| is the number of parameters. Therefore, again if dG(w) = |E| −|Vinternal|, then we can\nconclude that inﬁnitesimally speaking, all transformations to which the model is invariant can be spanned\nby node-wise rescaling transformations.\nConsidering only invariances that hold uniformly over all input x could give an under-estimate of the\nclass of invariances, i.e., there might be some invariances that hold for many input x but not all. An\nalternative approach for characterizing invariances is to deﬁne a measure of distance between functions\nthat the neural network model represents based on the input distribution, and inﬁnitesimally study the\nsubspace of directions to which the distance is insensitive. We can deﬁne distance between two functions\nf and g as\nD(f, g) = E [x ∼D] [m(f(x), g(x))] ,\nwhere m : R|Vout|×|Vout| →R is a (possibly asymmetric) distance measure between two vectors z, z′ ∈\n73\nR|Vout|, which we require that m(z, z) = 0 and ∂m/∂z′\nz=z′ = 0. For example, m(z, z′) = ∥z −z′∥2.\nThe second-order Taylor expansion of the distance D can be written as\nD(fw∥fw+∆) = 1\n2∆⊤· F(w) · ∆+ o(∥∆∥2),\nwhere\nF(w) = E [x ∼D]\n\"\u0012∂fw(x)\n∂w\n\u0013⊤\n· ∂2m(z, z′)\n∂z′2\n\f\f\f\f\nz=z′=fw(x)\n·\n\u0012∂fw(x)\n∂w\n\u0013#\nand ∂2m(z, z′)/∂z′2|z=z′=fw(x) is the Hessian of the distance measure m at z = z′ = fw(x).\nUsing the above expression, we can deﬁne the input distribution dependent version of N(w) and dG(w)\nas\nND(w) = NullF(w),\ndG,D(w) = rankF(w).\nAgain due to the rank-nullity theorem we have dG,D(w) + dim(ND(w)) = |E|.\nAs a special case, we obtain the Kullback-Leibler divergence DKL, which is commonly considered as\nthe way to study invariances, by choosing m as the conditional Kullback-Leibler divergence of output y\ngiven the network output as\nm(z, z′) = E [y ∼q(y|z)]\n\u0014\nlog q(y|z)\nq(y|z′)\n\u0015\n,\nwhere q(y|z) is a link function, which can be, e.g., the soft-max q(y|z) = ezy/ P|Vout|\ny′=1 ezy′. However,\nnote that the invariances in terms of DKL depends not only on the input distribution but also on the\nchoice of the link function q(y|z).\n8.2.1\nPath-based characterization of the network\nA major challenge in studying the degrees of freedom (8.2.3) is the fact that the Jacobian ∂fw(x)/∂w\ndepends on both parameter w and input x. In this section, we ﬁrst tease apart the two dependencies by\nrewriting fw(x) as the sum over all directed paths from every input node to each output node as follows:\nfw(x)[v] =\nX\np∈Π(v) gp(x) · πp(w) · x[head(p)],\n(8.2.4)\nwhere Π(v) is the set of all directed path from any input node to v, head(p) is the ﬁrst node of\npath p, gp(x) takes 1 if all the rectiﬁed linear units along path p is active and zero otherwise, and\nπp(w) = Q\ne∈E(p) w(e) is the product of the weights along path p; E(p) denotes the set of edges that\nappear along path p.\nLet Π = ∪v∈VoutΠ(v) be the set of all directed paths. We deﬁne the path-Jacobian matrix J(w) ∈\nR|Π|×|E| as J(w) = (∂πp(w)/∂we)p∈Π,e∈E. In addition, we deﬁne φ(x) as a |Π| dimensional vector\nwith gp(x) · x[head(p)] in the corresponding entry. The Jacobian of the network fw(x) can now be\nexpressed as\n∂fw(x)[v]\n∂w\n= Jv(w)⊤φv(x),\n(8.2.5)\nwhere where Jv(w) and φv(x) are the submatrix (or subvector) of J(w) and φ(x) that corresponds to\n74\noutput node v, respectively1. Expression (8.2.5) clearly separates the dependence to the parameters w\nand input x.\nNow we have the following statement (the proof is given in Section 8.3).\nTheorem 32. The degrees-of-freedom dG(w) of neural network model G is at most the rank of the\npath Jacobian matrix J(w). The equality holds if dim\n\u0000Span(φ(x) : x ∈R|Vin|)\n\u0001\n= |Π|; i.e. when the\ndimension of the space spanned by φ(x) equals the total number of paths |Π|.\nAn analogous statement holds for the input distribution dependent degrees of freedom dG,D(w), namely,\ndG,D(w) ≤rankJ(w) and the equality holds if the rank of the |Π| × |Π| path covariance matrix\n(E [x ∼D]\n\u0002\n∂2m(z, z′)/∂z′\nv∂z′\nv′φp(x)φp′(x)\n\u0003\n)p,p′∈Π is full, where v and v′ are the end nodes of paths\np and p′, respectively.\nIt remains to be understood when the dimension of the span of the path vectors φ(x) become full. The\nanswer depends on w. Unfortunately, there is no typical behavior as we know from the example of an\ninternal ReLU unit connected to ReLU units by negative weights. In fact, we can choose any number\nof internal units in the network to be in this degenerate state creating different degrees of degeneracy.\nAnother way to introduce degeneracy is to insert a linear layer in the network. This will superﬁcially\nincrease the number of paths but will not increase the dimension of the span of φ(x). For example,\nconsider a linear classiﬁer zout = ⟨w, x⟩with |Vin| inputs. If the whole input space is spanned by x,\nthe dimension of the span of φ(x) is |Vin|, which agrees with the number of paths. Now let’s insert a\nlinear layer with units V1 in between the input and the output layers. The number of paths has increased\nfrom |Vin| to |Vin| · |V1|. However the dimension of the span of φ(x) = ⃗1|V1| ⊗x is still |Vin|, because\nthe linear units are always active. Nevertheless we conjecture that there is a conﬁguration w such that\ndim\n\u0000Span(φ(x) : x ∈R|Vin|)\n\u0001\n= |Π| and the set of such w grows as the network becomes larger.\n8.2.2\nCombinatorial characterization of the rank of path Jacobian\nFinally, we show that the rank of the path-Jacobian matrix J(w) is determined purely combinatorially by\nthe graph G except a subset of the parameter space with zero Lebesgue measure. The proof is given in\nSection 8.3.\nTheorem 33. The rank of the path Jacobian matrix J(w) is generically (excluding set of parameters\nwith zero Lebesgue measure) equal to the number of parameters |E| minus the number of internal nodes\nof the network.\nNote that the dimension of the space spanned by node-wise rescaling equals the number of internal nodes.\nTherefore, node-wise rescaling is the only type of invariance for a ReLU network with ﬁxed architecture\nG, if dim\n\u0000Span(φ(x) : x ∈R|Vin|)\n\u0001\n= |Π| at parameter w.\nAs an example, let us consider a simple 3 layer network with 2 nodes in each layer except for the output\nlayer, which has only 1 node (see Figure 8.3). The network has 10 parameters (4, 4, and 2 in each layer\nrespectively) and 8 paths. The Jacobian (∂fw(x)/∂w) can be written as (∂fw(x)/∂w) = J(w)⊤·φ(x),\n1Note that although path activation gp(x) is a function of w, it is insensitive to an inﬁnitesimal change in the parameter, unless\nthe input to one of the rectiﬁed linear activation functions along path p is at exactly zero, which happens with probability zero.\nThus we treat gp(x) as constant here.\n75\nwhere\nJ(w) =\n\n\nw5w9\nw5w9\nw6w9\nw6w9\nw7w10\nw7w10\nw8w10\nw8w10\nw9w1\nw9w2\nw9w3\nw9w4\nw10w1\nw10w2\nw10w3\nw10w4\nw5w1\nw5w2\nw6w3\nw6w4\nw7w1\nw7w2\nw8w3\nw8w4\n\n\n(8.2.6)\nand\nφ(x)⊤=\n\u0002\ng1(x)x[1]\ng2(x)x[2]\ng3(x)x[1]\ng4(x)x[2]\ng5(x)x[1]\ng6(x)x[2]\ng7(x)x[1]\ng8(x)x[2]\n\u0003\n.\nThe rank of J(w) in (8.2.6) is (generically) equal to 10 −4 = 6, which is smaller than both the number\nof parameters and the number of paths.\n𝑤\"\n𝑤#\n𝑤$\n𝑤%\n𝑤&\n𝑤'\n𝑤(\n𝑤)\n𝑤*\n𝑤\"+\nFigure 8.3: A 3 layer network with 10 parameters and 8 paths.\n8.3\nProofs\n8.3.1\nProof of Theorem 31\nWe ﬁrst show that any RNN is invariant to Tα by induction on layers and time-steps. More speciﬁcally,\nwe prove that for any 0 ≤t ≤T and 1 ≤i < d, ⃗hi\nt\n\u0010\nTα( ⃗W)\n\u0011\n[j] = αi\nj⃗hi\nt( ⃗W)[j]. The statement is\nclearly true for t = 0; because for any i, j, ⃗hi\n0\n\u0010\nTα( ⃗W)\n\u0011\n[j] = αi\nj⃗hi\n0( ⃗W)[j] = 0.\nNext, we show that for i = 1, if we assume that the statement is true for t = t′, then it is also true for\nt = t′ + 1:\n⃗h1\nt′+1\n\u0010\nTα( ⃗W)\n\u0011\n[j] =\n\nX\nj′\nTin,α(Win)1[j, j′]⃗xt′+1[j′] + Trec,α(Wrec)1[j, j′]⃗h1\nt′\n\u0010\nTα( ⃗W)\n\u0011\n[j′]\n\n\n+\n=\n\nX\nj′\nα1\njW1\nin[j, j′]⃗xt′+1[j′] +\n\u0000α1\nj/α1\nj′\n\u0001\nW1\nrec[j, j′]α1\nj′⃗h1\nt′( ⃗W))[j′]\n\n\n+\n= α1\nj⃗hi\nt( ⃗W)[j]\n76\nWe now need to prove the statement for 1 < i < d. Assuming that the statement is true for t ≤t′ and the\nlayers before i, we have:\n⃗hi\nt′+1\n\u0010\nTα( ⃗W)\n\u0011\n[j] =\n\nX\nj′\nTin,α(Win)i[j, j′]⃗hi−1\nt′+1\n\u0010\nTα( ⃗W)\n\u0011\n[j′] + Trec,α(Wrec)i[j, j′]⃗hi\nt′\n\u0010\nTα( ⃗W)\n\u0011\n[j′]\n\n\n+\n=\n\nX\nj′\nαi\nj\nαi−1\nj′\nWi\nin[j, j′]αi−1\nj′ ⃗hi−1\nt′+1( ⃗W))[j′] + αi\nj\nαi\nj′ Wi\nrec[j, j′]αi\nj′⃗hi\nt′( ⃗W))[j′]\n\n\n+\n= αi\nj⃗hi\nt( ⃗W)[j]\nFinally, we can show that the output is invariant for any j at any time step t:\nfT ( ⃗\nW ),t(⃗xt)[j] =\nX\nj′\nTout,α(Wout)[j, j′]⃗hd−1\nt\n(Tα( ⃗W)[j′] =\nX\nj′\n(1/αd−1\nj′\n)Wout[j, j′]αd−1\nj′\n⃗hd−1\nt\n( ⃗W)[j′]\n=\nX\nj′\nWout[j, j′]⃗hd−1\nt\n( ⃗W)[j′] = f ⃗\nW ,t(⃗xt)[j]\nWe now show that any feasible node-wise rescaling can be presented as Tα. Recall that node-wise\nrescaling invariances for a general feedforward network can be written as f\nTβ(⃗w)u→v = (βv/βu)wu→v\nfor some β where βv > 0 for internal nodes and βv = 1 for any input/output nodes. An RNN with T = 0\nhas no weight sharing and for each node v with index j in layer i, we have βv = αi\nj. For any T > 0\nhowever, we there is no invariance that is not already counted. The reason is that by ﬁxing the values\nof βv for the nodes in time step 0, due to the feasibility, the values of β for nodes in other time-steps\nshould be tied to the corresponding value in time step 0. Therefore, all invariances are included and can\nbe presented in form of Tα.\n8.3.2\nProof of Theorem 32\nProof. First we see that (8.2.5) is true because\n∂fw(x)[v]\n∂w\n=\n\u0010 X\np∈Π(v)\n∂πp(w)\n∂we\n· gp(x) · x[head(p)]\n\u0011\ne∈E = Jv(w)⊤· φv(x).\nTherefore,\n[\nx∈R|Vin|\nSpan\n\u0012∂fw(x)[v]\n∂w\n: v ∈Vout\n\u0013\n=\n[\nx∈R|Vin|\nSpan (Jv(w)⊤· φv(x) : v ∈Vout)\n= J(w)⊤· Span\n\u0010\nφ(x) : x ∈R|Vin|\u0011\n.\n(8.3.1)\nConsequently, any vector of the form ( ∂fw(x)[v]\n∂we\n)e∈E for a ﬁxed input x lies in the span of the row vectors\nof the path Jacobian J(x).\nThe second part says dG(w) = rankJ(w) if dim\n\u0000Span(φ(x) : x ∈R|Vin|)\n\u0001\n= |Π|, which is the\nnumber of rows of J(w). We can see that this is true from expression (11.2.7).\n77\n8.3.3\nProof of Theorem 33\nProof. First, J(w) can be written as an Hadamard product between path incidence matrix M and a\nrank-one matrix as follows:\nJ(w) = M ◦\n\u0000w−1 · π⊤(w)\n\u0001\n,\nwhere M is the path incidence matrix whose i, j entry is one if the ith edge is part of the jth path, w−1\nis an entry-wise inverse of the parameter vector w, π(w) = (πp(w)) is a vector containing the product\nalong each path in each entry, and ⊤denotes transpose.\nSince we can rewrite\nJ(w) = diag(w−1) · M · diag(π(w)),\nwe see that (generically) the rank of J(w) is equal to the rank of zero-one matrix M.\nNote that the rank of M is equal to the number of linearly independent columns of M, in other words, the\nnumber of linearly independent paths. In general, most paths are not independent. For example, in Figure\n8.3, we can see that the column corresponding to the path w2w7w10 can be produced by combining 3\ncolumns corresponding to paths w1w5w9, w1w7w10, and w2w5w9.\nIn order to count the number of independent paths, we use mathematical induction. For simplicity,\nconsider a layered graph with d layers. All the edges from the (d −1)th layer nodes to the output layer\nnodes are linearly independent, because they correspond to different parameters. So far we have ndnd−1\nindependent paths.\nNext, take one node u0 (e.g., the leftmost node) from the (d −2)th layer. All the paths starting from\nthis node through the layers above are linearly independent. However, other nodes in this layer only\ncontributes linearly to the number of independent paths. This is the case because we can take an edge\n(u, v), where u is one of the remaining nd−2 −1 vertices in the (d −2)th layer and v is one of the nd−1\nnodes in the (d −1)th layer, and we can take any path (say p0) from there to the top layer. Then this is\nthe only independent path that uses the edge (u, v), because any other combination of edge (u, v) and\npath p from v to the top layer can be produced as follows (see Figure 8.4):\n(u, v) →p = (u, v) →p0 −(u0, v) →p0 + (u0, v) →p.\nTherefore after considering all nodes in the d −2th layer, we have\nndnd−1 + nd−1(nd−2 −1)\nindependent paths. Doing this calculation inductively, we have\nndnd−1 + nd−1(nd−2 −1) + · · · + n1(n0 −1)\nindependent paths, where n0 is the number of input units. This number is clearly equal to the number of\nparameters (ndnd−1 + · · · + n1n0) minus the number of internal nodes (nd−1 + · · · + n1).\n78\n𝑝\"\n𝑝\n𝑢\"\n𝑢\n𝑣\n𝑑th layer\n𝑑−1th layer\n𝑑−2th layer\n.  .  .\n.  .  .\nFigure 8.4: Schematic illustration of the linear dependence of the four paths (u0, v) →p0, (u0, v) →p,\n(u, v) →p0, and (u, v) →p. Because of this dependence, any additional edge (u, v) only contributes one additional\nindependent path.\n79\nChapter 9\nPath-Normalization for Feedforward\nand Recurrent Neural Networks\nAs we discussed, optimization is inherently tied to a choice of geometry, here represented by a choice of\ncomplexity measure or “norm”1. In Chapter 8, we studies the invariances in neural networks. We would\nto have a complexity measure that has similar invariance properties as neural networks. In Section 9.1\nwe introduce the path-regularizer which is invariant to node-wise rescaling transformations explained\nin Chapter 8. In Section 9.2, we derive Path-SGD optimization algorithm for standard feed-forward\nnetworks which is the steepest descent with respect to the path-regularizer. Finally, we extend Path-SGD\nto recurrent neural networks in Section 9.3.\n9.1\nPath-regularizer\nWe consider the generic group-norm type regularizer in equation (9.1.1). As we discussed before, two\nsimple cases of above group-norm are q1 = q2 = 1 and q1 = q2 = 2 that correspond to overall ℓ1\nregularization and weight decay respectively. Another form of regularization that is shown to be very\neffective in RELU networks is the max-norm regularization, which is the maximum over all units of\nnorm of incoming edge to the unit2 [67, 68]. The max-norm correspond to “per-unit\" regularization when\nwe set q2 = ∞in equation (9.1.1) and can be written in the following form (for q1 = 2):\nµ2,∞(w) = sup\nv∈V\n\n\nX\n(u→v)∈E\n\f\fw(u→v)\n\f\f2\n\n\n1/2\n(9.1.1)\nWeight decay is probably the most commonly used regularizer. On the other hand, per-unit regularization\nmight not seem ideal as it is very extreme in the sense that the value of regularizer corresponds to\nthe highest value among all nodes. However, the situation is very different for networks with RELU\nactivations (and other activation functions with non-negative homogeneity property). In these cases,\nper-unit ℓ2 regularization has shown to be very effective [68]. The main reason could be because RELU\nnetworks can be rebalanced in such a way that all hidden units haveneyshabur2015norm the same norm.\n1The path-norm which we deﬁne is a norm on functions, not on weights, but as we prefer not getting into this technical\ndiscussion here, we use the term “norm” very loosely to indicate some measure of magnitude [65].\n2This deﬁnition of max-norm is a bit different than the one used in the context of matrix factorization [66]. The later is similar\nto the minimum upper bound over ℓ2 norm of both outgoing edges from the input units and incoming edges to the output units in a\ntwo layer feed-forward network.\n80\nHence, per-unit regularization will not be a crude measure anymore.\nSince µp,∞is not rescaling invariant and the values of the scale measure are different for rescaling\nequivalent networks, it is desirable to look for the minimum value of a regularizer among all rescaling\nequivalent networks. Surprisingly, for a feed-forward network, the minimum ℓ2 per-unit regularizer\namong all rescaling equivalent networks can be calculated in close form and we call it the path-regularizer\n[65, 69].\nThe path-regularizer is the sum over all paths from input nodes to output nodes of the product of squared\nweights along the path. To deﬁne it formally, let P be the set of directed paths from input to output\nunits so that for any pathneyshabur2015path ζ =\n\u0000ζ0, . . . , ζlen(ζ)\n\u0001\n∈P of length len(ζ), we have that\nζ0 ∈Vin, ζlen(ζ) ∈Vout and for any 0 ≤i ≤len(ζ) −1, (ζi →ζi+1) ∈E. We also abuse the notation\nand denote e ∈ζ if for some i, e = (ζi, ζi+1). Then the path regularizer can be written as:\nγ2\nnet(w) =\nX\nζ∈P\nlen(ζ)−1\nY\ni=0\nw2\nζi→ζi+1\n(9.1.2)\nThe above formulation of the path-regularizer involves an exponential number of terms. However, it can\nbe computed efﬁciently by dynamic programming in a single forward step using the following equivalent\nrecursive deﬁnition:\nγ2\nv(w) =\nX\n(u→v)∈E\nγ2\nu(w)w2\nu→v ,\nγ2\nnet(w) =\nX\nu∈Vout\nγ2\nu(w)\n(9.1.3)\n9.2\nPath-SGD for Feedforward Networks\nWe consider an approximate steepest descent step with respect to the path-norm. More formally, for a\nnetwork without shared weights, where the parameters are the weights themselves, consider the diagonal\nquadratic approximation of the path-regularizer about the current iterate w(t):\nˆγ2\nnet(w(t) + ∆w) = γ2\nnet(w(t)) +\nD\n∇γ2\nnet(w(t)), ∆w\nE\n+ 1\n2∆w⊤diag\n\u0010\n∇2γ2\nnet(w(t))\n\u0011\n∆w (9.2.1)\nUsing the corresponding quadratic norm ∥w −w′∥2\nˆγ2\nnet(w(t)+∆w) = 1\n2\nP\ne∈E\n∂2γ2\nnet\n∂w2e (we −w′\ne)2, we\ncan deﬁne an approximate steepest descent step as:\nw(t+1) = min\nw η\nD\n∇L(w), w −w(t)E\n+\n\r\r\rw −w(t)\r\r\r\n2\nˆγ2\nnet(w(t)+∆w) .\n(9.2.2)\nSolving (9.2.2) yields the update:\nw(t+1)\ne\n= w(t)\ne\n−\nη\nκe(w(t))\n∂L\n∂we\n(w(t))\nwhere: κe(w) = 1\n2\n∂2γ2\nnet(w)\n∂w2e\n.\n(9.2.3)\nThe stochastic version that uses a subset of training examples to estimate\n∂L\n∂wu→v (w(t)) is called Path-\nSGD [69]. We now show how Path-SGD can be extended to networks with shared weights.\n9.3\nExtending to Networks with Shared Weights\nWhen the networks has shared weights, the path-regularizer is a function of parameters p and therefore\nthe quadratic approximation should also be with respect to the iterate p(t) instead of w(t) which results\n81\nin the following update rule:\np(t+1) = min\np η\nD\n∇L(p), p −p(t)E\n+\n\r\r\rp −p(t)\r\r\r\nˆγ2\nnet(p(t)+∆p) .\n(9.3.1)\nwhere ∥p −p′∥2\nˆγ2\nnet(p(t)+∆p) = 1\n2\nPm\ni=1\n∂2γ2\nnet\n∂p2\ni\n(pi −p′\ni)2. Solving (9.3.1) gives the following update:\np(t+1)\ni\n= p(t)\ni\n−\nη\nκi(p(t))\n∂L\n∂pi\n(p(t))\nwhere: κi(p) = 1\n2\n∂2γ2\nnet(p)\n∂p2\ni\n.\n(9.3.2)\nThe second derivative terms κi are speciﬁed in terms of their path structure as follows:\nLemma 34. κi(p) = κ(1)\ni (p) + κ(2)\ni (p) where\nκ(1)\ni (p) =\nX\ne∈Ei\nX\nζ∈P\n1e∈ζ\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1)\np2\nπ(ζj→ζj+1) =\nX\ne∈Ei\nκe(w),\n(9.3.3)\nκ(2)\ni (p) = p2\ni\nX\ne1,e2∈Ei\ne1̸=e2\nX\nζ∈P\n1e1,e2∈ζ\nlen(ζ)−1\nY\nj=0\ne1̸=(ζj →ζj+1)\ne2̸=(ζj →ζj+1)\np2\nπ(ζj→ζj+1),\n(9.3.4)\nand κe(w) is deﬁned in (9.2.3).\nThe second term κ(2)\ni (p) measures the effect of interactions between edges corresponding to the same\nparameter (edges from the same Ei) on the same path from input to output. In particular, if for any\npath from an input unit to an output unit, no two edges along the path share the same parameter, then\nκ(2)(p) = 0. For example, for any feedforward or Convolutional neural network, κ(2)(p) = 0. But for\nRNNs, there certainly are multiple edges sharing a single parameter on the same path, and so we could\nhave κ(2)(p) ̸= 0.\nThe above lemma gives us a precise update rule for the approximate steepest descent with respect to the\npath-regularizer. The following theorem conﬁrms that the steepest descent with respect to this regularizer\nis also invariant to all feasible node-wise rescaling for networks with shared weights.\nTheorem 35. For any feedforward networks with shared weights, the update (9.3.2) is invariant to all\nfeasible node-wise rescalings. Moreover, a simpler update rule that only uses κ(1)\ni (p) in place of κi(p)\nis also invariant to all feasible node-wise rescalings.\nEquations (9.3.3) and (9.3.4) involve a sum over all paths in the network which is exponential in depth\nof the network. We next show that both of these equations can be calculated efﬁciently.\n9.3.1\nSimple and Efﬁcient Computations\nWe show how to calculate κ(1)\ni (⃗p) and κ(2)\ni (⃗p) by considering a network with the same architecture but\nwith squared weights:\nTheorem 36. For any network N(G, π, p), consider N(G, π, ˜p) where for any i, ˜pi = p2\ni . Deﬁne the\nfunction g : R|Vin| →R to be the sum of outputs of this network: g(x) = P|Vout|\ni=1 f˜⃗p(x)[i]. Then κ(1) and\nκ(2) can be calculated as follows where ⃗1 is the all-ones input vector:\nκ(1)(⃗p) = ∇˜⃗pg(⃗1),\nκ(2)\ni (⃗p) =\nX\n(u→v),(u′→v′)∈Ei\n(u→v)̸=(u′→v′)\n˜pi\n∂g(⃗1)\n∂hv′(˜⃗p)\n∂hu′(˜⃗p)\n∂hv(˜⃗p)\nhu(˜⃗p).\n(9.3.5)\n82\nIn the process of calculating the gradient ∇˜⃗pg(⃗1), we need to calculate hu(˜⃗p) and ∂g(⃗1)/∂hv(˜⃗p) for any\nu, v. Therefore, the only remaining term to calculate (besides ∇˜pg(⃗1)) is ∂hu′(˜⃗p)/∂hv(˜⃗p).\nRecall that T is the length (maximum number of propagations through time) and d is the number of\nlayers in an RNN. Let H be the number of hidden units in each layer and B be the size of the mini-batch.\nThen calculating the gradient of the loss at all points in the minibatch (the standard work required for\nany mini-batch gradient approach) requires time O(BdTH2). In order to calculate κ(1)\ni (⃗p), we need to\ncalculate the gradient ∇˜⃗pg(1) of a similar network at a single input—so the time complexity is just an\nadditional O(dTH2). The second term κ(2)(⃗p) can also be calculated for RNNs in O(dTH2(T + H))\n3. Therefore, the ratio of time complexity of calculating the ﬁrst term and second term with respect to\nthe gradient over mini-batch is O(1/B) and O((T + H)/B) respectively. Calculating only κ(1)\ni (⃗p) is\ntherefore very cheap with minimal per-minibatch cost, while calculating κ(2)\ni (⃗p) might be expensive for\nlarge networks. Beyond the low computational cost, calculating κ(1)\ni (⃗p) is also very easy to implement\nas it requires only taking the gradient with respect to a standard feed-forward calculation in a network\nwith slightly modiﬁed weights—with most deep learning libraries it can be implemented very easily with\nonly a few lines of code.\n9.4\nProofs\n9.4.1\nProof of Lemma 34\nWe prove the statement simply by calculating the second derivative of the path-regularizer with respect\nto each parameter:\nκi(⃗p) = 1\n2\n∂2γ2\nnet\n∂p2\ni\n= 1\n2\n∂\n∂pi\n\n∂\n∂pi\nX\nζ∈P\nlen(ζ)−1\nY\nj=0\nw2\nζj→ζj+1\n\n\n= 1\n2\n∂\n∂pi\n\n∂\n∂pi\nX\nζ∈P\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n= 1\n2\nX\nζ∈P\n∂\n∂pi\n\n∂\n∂pi\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n\n3 For an RNN, κ(2)(Win) = 0 and κ(2)(Wout) = 0 because only recurrent weights are can be shared multiple\ntimes along an input-output path.\nκ(2)(Wrec) can be written and calculated in the matrix form:\nκ(2)(Wi\nrec)\n=\nW′i\nrec ⊙\nPT −3\nt1=0\n\n\n\u0012\u0010\nW′i\nrec\n\u0011t1\n\u0013⊤\n⊙PT −t1−1\nt2=2\n∂g(⃗1)\n∂⃗hi\nt1+t2+1(˜⃗p)\n\u0010\n⃗hi\nt2 (˜⃗p)\n\u0011⊤\n\nwhere for any i, j, k we have W′i\nrec[j, k] =\n\u0000Wi\nrec[j, k]\n\u00012. The\nonly terms that require extra computation are powers of Wrec which can be done in O(dTH3) and the rest of the matrix\ncomputations need O(dT 2H2).\n83\nTaking the second derivative then gives us both terms after a few calculations:\nκi(⃗p) = 1\n2\nX\nζ∈P\n∂\n∂pi\n\n∂\n∂pi\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n=\nX\nζ∈P\n∂\n∂pi\n\n\npi\nX\ne∈Ei\n⃗1e∈ζ\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1\np2\nπ(ζj→ζj+1)\n\n\n\n=\nX\nζ∈P\n\npi\n∂\n∂pi\n\n\n\nX\ne∈Ei\n⃗1e∈ζ\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1\np2\nπ(ζj→ζj+1)\n\n\n+\nX\ne∈Ei\n⃗1e∈ζ\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1\np2\nπ(ζj→ζj+1)\n\n\n= p2\ni\nX\ne1,e2∈Ei\ne1̸=e2\n\n\nX\nζ∈P\n⃗1e1,e2∈ζ\nlen(ζ)−1\nY\nj=0\ne1̸=(ζj →ζj+1)\ne2̸=(ζj →ζj+1)\np2\nπ(ζj→ζj+1)\n\n+\nX\ne∈Ei\n\n\nX\nζ∈P\n⃗1e∈ζ\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1)\np2\nπ(ζj→ζj+1)\n\n\n9.4.2\nProof of Theorem 35\nNode-wise rescaling invariances for a feedforward network can be written as Tβ(⃗w)u→v = (βv/βu)wu→v\nfor some β where βv > 0 for internal nodes and βv = 1 for any input/output nodes. Any feasible invari-\nance for a network with shared weights can also be written in the same form. The only difference is that\nsome of βvs are now tied to each other in a way that shared weights have the same value after transfor-\nmation. First, note that since the network is invariant to the transformation, the following statement holds\nby an induction similar to Theorem 31 but in the backward direction:\n∂L\n∂hv\n(Tβ(⃗p)) = 1\nβv\n∂L\n∂hu\n(⃗p)\n(9.4.1)\nfor any (u →v) ∈E. Furthermore, by the proof of the Theorem 31 we have that for any (u →v) ∈E,\nhu(Tβ(⃗p)) = βuhu(⃗p). Therefore,\n∂L\n∂Tβ(⃗p)i\n(Tβ(⃗p)) =\nX\n(u→v)∈Ei\n∂L\n∂hv\n(Tβ(⃗p))hu(Tβ(⃗p)) = βu′\nβv′\n∂L\n∂pi\n(⃗p)\n(9.4.2)\nwhere (u′ →v′) ∈Ei. In order to prove the theorem statement, it is enough to show that for any edge\n(u →v) ∈Ei, κi(Tβ(⃗p)) = (βu/βv)2κi(⃗p) because this property gives us the following update:\nTβ(⃗p)i −\nη\nκi(Tβ(⃗p))\n∂L(Tβ(⃗p))\n∂Tβ(⃗p)i\n= βv\nβu\npi −\nη\n(βu/βv)2κi(⃗p)\nβu\nβv\n∂L\n∂pi\n(⃗p) = Tβ(⃗p+)i\nTherefore, it is remained to show that for any edge (u →v) ∈Ei v, κi(Tβ(⃗p)) = (βu/βv)2κi(⃗p). We\nshow that this is indeed true for both terms κ(1) and κ(2) separately.\nWe ﬁrst prove the statement for κ(1). Consider each path ζ ∈P. By an inductive argument along the\npath, it is easy to see that multiplying squared weights along this path is invariant to the transformation:\nlen(ζ)−1\nY\nj=0\nTβ(⃗p)2\nπ(ζj→ζj+1) =\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n84\nTherefore, we have that for any edge e ∈E and any ζ ∈P,\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1)\nTβ(⃗p)2\nπ(ζj→ζj+1) =\n\u0012βu\nβv\n\u00132\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1)\np2\nπ(ζj→ζj+1)\nTaking sum over all paths ζ ∈P and all edges e = (u →v) ∈E completes the proof for κ(1). Similarly\nfor κ(2), considering any two edges e1 ̸= e2 and any path ζP, we have that:\nTβ(⃗p)2\ni\nlen(ζ)−1\nY\nj=0\ne1̸=(ζj →ζj+1)\ne2̸=(ζj →ζj+1)\nTβ(⃗p)2\nπ(ζj→ζj+1) =\n\u0012βv\nβu\n\u00132\np2\ni\n\u0012βu\nβv\n\u00134\nlen(ζ)−1\nY\nj=0\ne1̸=(ζj →ζj+1)\ne2̸=(ζj →ζj+1)\np2\nπ(ζj→ζj+1)\nwhere (u →v) ∈Ei. Again, taking sum over all paths ζ and all edges e1 ̸= e2 proves the statement for\nκ(2) and consequently for κ(1) + κ(2).\n9.4.3\nProof of Theorem 36\nFirst, note that based on the deﬁnitions in the theorem statement, for any node v, hv(˜⃗p) = γ2\nv(p) and\ntherefore g(⃗1) = γ2\nnet(p). Using Lemma 34, main observation here is that for each edge e ∈Ei and each\npath ζ ∈P, the corresponding term in κ(1) is nothing but product of the squared weights along the path\nexcept the weights that correspond to the edge e:\n⃗1e∈ζ\nlen(ζ)−1\nY\nj=0\ne̸=(ζj →ζj+1)\np2\nπ(ζj→ζj+1)\nThis path can therefore be decomposed into a path from input to edge e and a path from edge e to the\noutput. Therefore, for any edge e, we can factor out the number corresponding to the paths that go\nthrough e and rewrite κ(1) as follows:\nκ(1)(p) =\nX\n(u→v)∈Ei\n\n\n\n\nX\nζ∈Pin→u\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n\n\n\nX\nζ∈Pv→out\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n\n\n\n(9.4.3)\nwhere Pin→u is the set of paths from input nodes to node v and Pv→out is deﬁned similarly for the output\nnodes.\nBy induction on layers of N(G, π, ˜⃗p), we get the following:\nX\nζ∈Pin→u\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1) = hu(˜⃗p)\n(9.4.4)\nX\nζ∈Pv→out\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1) = ∂g(1)\n∂hv(˜⃗p)\n(9.4.5)\nTherefore, κ(1) can be written as:\nκ(1)(p) =\nX\n(u→v)∈Ei\n∂g(1)\n∂hv(˜⃗p)\nhu(˜⃗p) =\nX\n(u→v)∈Ei\n∂g(1)\n∂w′u→v\n= ∂g(1)\n∂˜pi\n(9.4.6)\n85\nNext, we show how to calculate the second term, i.e. κ(2). Each term in κ(2) corresponds to a path that\ngoes through two edges. We can decompose such paths and rewrite κ(2) similar to the ﬁrst term:\nκ(2)(p) = p2\ni\nX\n(u→v)∈Ei\n(u′→v′)∈Ei\n(u→v)̸=(u′→v′)\n\n\n\n\nX\nζ∈Pin→u\nlen(ζ)\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n\n\n\nX\nζ∈Pv→u′\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n\n\n\nX\nζ∈Pv′→out\nlen(ζ)−1\nY\nj=0\np2\nπ(ζj→ζj+1)\n\n\n\n\n=\nX\n(u→v)∈Ei\n(u′→v′)∈Ei\n(u→v)̸=(u′→v′)\n˜pi\n∂g(⃗1)\n∂hv′(˜⃗p)\n∂hu′(˜⃗p)\n∂hv(˜⃗p)\nhu(˜⃗p)\nwhere Pu→v is the set of all directed paths from node u to node v.\n86\nChapter 10\nExperiments on Path-SGD\nIn this Chapter, we compare Path-SGD to other optimization methods on fully connected and recurrent\nneural networks.\n10.1\nExperiments on Fully Connected Feedforward Networks\nWe compare ℓ2-Path-SGD to two commonly used optimization methods in deep learning, SGD and\nAdaGrad. We conduct our experiments on four common benchmark datasets: the standard MNIST dataset\nof handwritten digits [70]; CIFAR-10 and CIFAR-100 datasets of tiny images of natural scenes [71];\nand Street View House Numbers (SVHN) dataset containing color images of house numbers collected by\nGoogle Street View [72]. Details of the datasets are shown in Table 10.1.\nIn all of our experiments, we trained feed-forward networks with two hidden layers, each containing\n4000 hidden units. We used mini-batches of size 100 and the step-size of 10−α, where α is an integer\nbetween 0 and 10. To choose α, for each dataset, we considered the validation errors over the validation\nset (10000 randomly chosen points that are kept out during the initial training) and picked the one that\nreaches the minimum error faster. We then trained the network over the entire training set. All the\nnetworks were trained both with and without dropout. When training with dropout, at each update step,\nwe retained each unit with probability 0.5.\nWe tried both balanced and unbalanced initializations. In balanced initialization, incoming weights\nto each unit v are initialized to i.i.d samples from a Gaussian distribution with standard deviation\n1/\np\nfan-in(v). In the unbalanced setting, we ﬁrst initialized the weights to be the same as the balanced\nweights. We then picked 2000 hidden units randomly with replacement. For each unit, we multiplied its\nincoming edge and divided its outgoing edge by 10c, where c was chosen randomly from log-normal\ndistribution.\nThe optimization results are shown in Figure 10.1. For each of the four datasets, the plots for objective\nfunction (cross-entropy), the training error and the test error are shown from left to right where in\neach plot the values are reported on different epochs during the optimization. Although we proved\nthat Path-SGD updates are the same for balanced and unbalanced initializations, to verify that despite\nnumerical issues they are indeed identical, we trained Path-SGD with both balanced and unbalanced\ninitializations. Since the curves were exactly the same we only show a single curve. The dropout is used\nfor the experiments on CIFAR-100 and SVHN. Please see [69] for a more complete set of experimental\nresults.\nWe can see in Figure 10.2 that as expected, the unbalanced initialization considerably hurts the perfor-\nmance of SGD and AdaGrad (in many cases their training and test errors are not even in the range of the\n87\nTable 10.1: General information on datasets used in the experiments on feedforward networks.\nData Set\nDimensionality\nClasses\nTraining Set\nTest Set\nCIFAR-10\n3072 (32 × 32 color)\n10\n50000\n10000\nCIFAR-100\n3072 (32 × 32 color)\n100\n50000\n10000\nMNIST\n784 (28 × 28 grayscale)\n10\n60000\n10000\nSVHN\n3072 (32 × 32 color)\n10\n73257\n26032\n0\n20\n40\n60\n80\n100\n0\n0.5\n1\n1.5\n2\n2.5\n.\n0\n20\n40\n60\n80\n100\n0\n1\n2\n3\n4\n5\n.\n0\n20\n40\n60\n80\n100\n0\n0.5\n1\n1.5\n2\n2.5\n.\n0\n20\n40\n60\n80\n100\n0\n0.5\n1\n1.5\n2\n2.5\nEpoch\n.\n0\n20\n40\n60\n80\n100\n0\n0.05\n0.1\n0.15\n0.2\n.\n0\n20\n40\n60\n80\n100\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n.\n0\n20\n40\n60\n80\n100\n0\n0.005\n0.01\n0.015\n0.02\n.\n0\n20\n40\n60\n80\n100\n0\n0.05\n0.1\n0.15\n0.2\nEpoch\n.\n0\n20\n40\n60\n80\n100\n0.4\n0.45\n0.5\n0.55\n0.6\n.\n \n \nPath−SGD − Unbalanced\nSGD − Balanced\nSGD − Unbalanced\nAdaGrad − Balanced\nAdaGrad − Unbalanced\n0\n20\n40\n60\n80\n100\n0.65\n0.7\n0.75\n0.8\n0.85\n.\n0\n20\n40\n60\n80\n100\n0.015\n0.02\n0.025\n0.03\n0.035\n.\n0\n20\n40\n60\n80\n100\n0.14\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\nEpoch\n.\nCIFAR-10\nCIFAR-100\nMNIST\nSVHN\nCross-Entropy Training Loss\n0/1 Training Error\n0/1 Test Error\nFigure 10.1: Learning curves using different optimization methods for 4 datasets without dropout. Left panel\ndisplays the cross-entropy objective function; middle and right panels show the corresponding values of the training\nand test errors, where the values are reported on different epochs during the course of optimization. Best viewed in\ncolor.\nplot to be displayed), while Path-SGD performs essentially the same. Another interesting observation\nis that even in the balanced settings, not only does Path-SGD often get to the same value of objective\n88\n0\n20\n40\n60\n80\n100\n0\n0.5\n1\n1.5\n2\n2.5\n.\n0\n20\n40\n60\n80\n100\n0\n1\n2\n3\n4\n5\n.\n0\n20\n40\n60\n80\n100\n0\n0.5\n1\n1.5\n2\n2.5\n.\n0\n20\n40\n60\n80\n100\n0\n0.5\n1\n1.5\n2\n2.5\nEpoch\n.\n0\n20\n40\n60\n80\n100\n0\n0.1\n0.2\n0.3\n0.4\n.\n0\n20\n40\n60\n80\n100\n0\n0.2\n0.4\n0.6\n0.8\n.\n0\n20\n40\n60\n80\n100\n0\n0.02\n0.04\n0.06\n0.08\n.\n0\n20\n40\n60\n80\n100\n0\n0.1\n0.2\n0.3\n0.4\nEpoch\n.\n0\n20\n40\n60\n80\n100\n0.35\n0.4\n0.45\n0.5\n0.55\n.\n \n \nPath−SGD + Dropout\nSGD + Dropout\nAdaGrad + Dropout\n0\n20\n40\n60\n80\n100\n0.6\n0.65\n0.7\n0.75\n0.8\n.\n0\n20\n40\n60\n80\n100\n0.015\n0.02\n0.025\n0.03\n0.035\n.\n0\n20\n40\n60\n80\n100\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\n0.18\nEpoch\n.\nCIFAR-10\nCIFAR-100\nMNIST\nSVHN\nCross-Entropy Training Loss\n0/1 Training Error\n0/1 Test Error\nFigure 10.2: Learning curves using different optimization methods for 4 datasets with dropout. Left panel displays\nthe cross-entropy objective function; middle and right panels show the corresponding values of the training and test\nerrors. Best viewed in color.\nfunction, training and test error faster, but also the ﬁnal generalization error for Path-SGD is sometimes\nconsiderably lower than SGD and AdaGrad). The plots for test errors could also imply that implicit\nregularization due to steepest descent with respect to path-regularizer leads to a solution that generalizes\nbetter. This view is similar to observations in [73] on the role of implicit regularization in deep learning.\nThe results suggest that Path-SGD outperforms SGD and AdaGrad in two different ways. First, it can\nachieve the same accuracy much faster and second, the implicit regularization by Path-SGD leads to a\nlocal minima that can generalize better even when the training error is zero. This can be better analyzed\nby looking at the plots for more number of epochs which we have provided in [69]. We should also\npoint that Path-SGD can be easily combined with AdaGrad or Adam to take advantage of the adaptive\nstepsize or used together with a momentum term. This could potentially perform even better compare to\nPath-SGD.\n89\nEpoch\n0\n50\n100\n150\n200\nPerplexity\n0\n100\n200\n300\n400\n500\nSGD\nPath-SGD:5(1)\nPath-SGD:5(1)+5(2)\nEpoch\n0\n50\n100\n150\n200\nPerplexity\n0\n100\n200\n300\n400\n500\nSGD\nPath-SGD:5(1)\nPath-SGD:5(1)+5(2)\n!(#)\n#\n!(%)\n#\n&\n' = 400, , = 10\n0.00014\n' = 400, , = 40\n0.00022\n' = 100, , = 10\n0.00037\n' = 100, , = 10\n0.00048\nFigure 10.3: Path-SGD with/without the second term in word-level language modeling on PTB. We use the standard\nsplit (929k training, 73k validation and 82k test) and the vocabulary size of 10k words. We initialize the weights by\nsampling from the uniform distribution with range [−0.1, 0.1]. The table on the left shows the ratio of magnitude of\nﬁrst and second term for different lengths T and number of hidden units H. The plots compare the training and test\nerrors using a mini-batch of size 32 and backpropagating through T = 20 time steps and using a mini-batch of size\n32 where the step-size is chosen by a grid search.\nTest Error\nTraining Error\n10.2\nExperiments on Recurrent Neural Networks\n10.2.1\nThe Contribution of the Second Term\nAs we discussed in section 9.3.1, the second term κ(2) in the update rule can be computationally expensive\nfor large networks. In this section we investigate the signiﬁcance of the second term and show that\nat least in our experiments, the contribution of the second term is negligible. To compare the two\nterms κ(1) and κ(2), we train a single layer RNN with H = 200 hidden units for the task of word-level\nlanguage modeling on Penn Treebank (PTB) Corpus [74]. Fig. 10.3 compares the performance of SGD\nvs. Path-SGD with/without κ(2). We clearly see that both version of Path-SGD are performing very\nsimilarly and both of them outperform SGD signiﬁcantly. This results in Fig. 10.3 suggest that the ﬁrst\nterm is more signiﬁcant and therefore we can ignore the second term.\nTo better understand the importance of the two terms, we compared the ratio of the norms\n\r\rκ(2)\r\r\n2 /\n\r\rκ(1)\r\r\n2\nfor different RNN lengths T and number of hidden units H. The table in Fig. 10.3 shows that the contri-\nbution of the second term is bigger when the network has fewer number of hidden units and the length of\nthe RNN is larger (H is small and T is large). However, in many cases, it appears that the ﬁrst term has a\nmuch bigger contribution in the update step and hence the second term can be safely ignored. Therefore,\nin the rest of our experiments, we calculate the Path-SGD updates only using the ﬁrst term κ(1).\n10.2.2\nAddition problem\nTraining Recurrent Neural Networks is known to be hard for modeling long-term dependencies due to\nthe gradient vanishing/exploding problem [75, 76]. In this section, we consider synthetic problems that\nare speciﬁcally designed to test the ability of a model to capture the long-term dependency structure.\nSpeciﬁcally, we consider the addition problem and the sequential MNIST problem.\nAddition Problem: The addition problem was introduced in [77]. Here, each input consists of two\nsequences of length T, one of which includes numbers sampled from the uniform distribution with range\n[0, 1] and the other sequence serves as a mask which is ﬁlled with zeros except for two entries. These two\nentries indicate which of the two numbers in the ﬁrst sequence we need to add and the task is to output\nthe result of this addition.\nSequential MNIST: In sequential MNIST, each digit image is reshaped into a sequence of length 784,\nturning the digit classiﬁcation task into sequence classiﬁcation with long-term dependencies [78, 79].\nFor both tasks, we closely follow the experimental protocol in [78]. We train a single-layer RNN\n90\n0\n15\n30\n45\nnumber of Epochs\n0.00\n0.05\n0.10\n0.15\n0.20\nMSE\nAdding 100\nIRNN\nRNN−Path\n0\n80\n160\n240\nnumber of Epochs\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nAdding 400\nIRNN\nRNN−Path\n0\n100\n200\n300\n400\nnumber of Epochs\n0.00\n0.05\n0.10\n0.15\n0.20\nAdding 750\nIRNN\nRNN−Path\nFigure 10.4: Test errors for the addition problem of different lengths.\nAdding\nAdding\nAdding\n100\n400\n750\nsMNIST\nIRNN [78]\n0\n16.7\n16.7\n5.0\nuRNN [79]\n0\n3\n16.7\n4.9\nLSTM [79]\n0\n2\n16.7\n1.8\nnp-RNN[80]\n0\n2\n>2\n3.1\nIRNN\n0\n0\n16.7\n7.1\nRNN-Path\n0\n0\n0\n3.1\nTable 10.2: Test error (MSE) for the adding problem\nwith different input sequence lengths and test classiﬁcation\nerror for the sequential MNIST.\nPTB\ntext8\nRNN+smoothReLU [81]\n-\n1.55\nHF-MRNN [82]\n1.42\n1.54\nRNN-ReLU[83]\n1.65\n-\nRNN-tanh[83]\n1.55\n-\nTRec,β = 500[83]\n1.48\n-\nRNN-ReLU\n1.55\n1.65\nRNN-tanh\n1.58\n1.70\nRNN-Path\n1.47\n1.58\nLSTM\n1.41\n1.52\nTable 10.3: Test BPC for PTB and text8.\nconsisting of 100 hidden units with path-SGD, referred to as RNN-Path. We also train an RNN of\nthe same size with identity initialization, as was proposed in [78], using SGD as our baseline model,\nreferred to as IRNN. We performed grid search for the learning rates over {10−2, 10−3, 10−4} for both\nour model and the baseline. Non-recurrent weights were initialized from the uniform distribution with\nrange [−0.01, 0.01]. Similar to [79], we found the IRNN to be fairly unstable (with SGD optimization\ntypically diverging). Therefore for IRNN, we ran 10 different initializations and picked the one that did\nnot explode to show its performance.\nIn our ﬁrst experiment, we evaluate Path-SGD on the addition problem. The results are shown in Fig. 10.4\nwith increasing the length T of the sequence: {100, 400, 750}. We note that this problem becomes much\nharder as T increases because the dependency between the output (the sum of two numbers) and the\ncorresponding inputs becomes more distant. We also compare RNN-Path with the previously published\nresults, including identity initialized RNN [78] (IRNN), unitary RNN [79] (uRNN), and np-RNN1\nintroduced by [80]. Table 10.2 shows the effectiveness of using Path-SGD. Perhaps more surprisingly,\nwith the help of path-normalization, a simple RNN with the identity initialization is able to achieve a 0%\nerror on the sequences of length 750, whereas all the other methods, including LSTMs, fail. This shows\nthat Path-SGD may help stabilize the training and alleviate the gradient problem, so as to perform well\non longer sequence. We next tried to model the sequences length of 1000, but we found that for such\nvery long sequences RNNs, even with Path-SGD, fail to learn.\nNext, we evaluate Path-SGD on the Sequential MNIST problem. Table 10.2, right column, reports test\nerror rates achieved by RNN-Path compared to the previously published results. Clearly, using Path-SGD\nhelps RNNs achieve better generalization. In many cases, RNN-Path outperforms other RNN methods\n(except for LSTMs), even for such a long-term dependency problem.\n1The original paper does not include any result for 750, so we implemented np-RNN for comparison. However, in our\nimplementation the np-RNN is not able to even learn sequences of length of 200. Thus we put “>2” for length of 750.\n91\n10.2.3\nLanguage Modeling Tasks\nIn this section we evaluate Path-SGD on a language modeling task. We consider two datasets, Penn\nTreebank (PTB-c) and text8 2. PTB-c: We performed experiments on a tokenized Penn Treebank Corpus,\nfollowing the experimental protocol of [83]. The training, validations and test data contain 5017k, 393k\nand 442k characters respectively. The alphabet size is 50, and each training sequence is of length 50.\ntext8: The text8 dataset contains 100M characters from Wikipedia with an alphabet size of 27. We follow\nthe data partition of [82], where each training sequence has a length of 180. Performance is evaluated\nusing bits-per-character (BPC) metric, which is log2 of perplexity.\nSimilar to the experiments on the synthetic datasets, for both tasks, we train a single-layer RNN consisting\nof 2048 hidden units with path-SGD (RNN-Path). Due to the large dimension of hidden space, SGD can\ntake a fairly long time to converge. Instead, we use Adam optimizer [84] to help speed up the training,\nwhere we simply use the path-SGD gradient as input to the Adam optimizer.\nWe also train three additional baseline models: a ReLU RNN with 2048 hidden units, a tanh RNN with\n2048 hidden units, and an LSTM with 1024 hidden units, all trained using Adam. We performed grid\nsearch for learning rate over {10−3, 5 · 10−4, 10−4} for all of our models. For ReLU RNNs, we initialize\nthe recurrent matrices from uniform[−0.01, 0.01], and uniform[−0.2, 0.2] for non-recurrent weights. For\nLSTMs, we use orthogonal initialization [85] for the recurrent matrices and uniform[−0.01, 0.01] for\nnon-recurrent weights. The results are summarized in Table 10.3.\nWe also compare our results to an RNN that uses hidden activation regularizer [83] (TRec,β = 500),\nMultiplicative RNNs trained by Hessian Free methods [82] (HF-MRNN), and an RNN with smooth\nversion of ReLU [81]. Table 10.3 shows that path-normalization is able to outperform RNN-ReLU and\nRNN-tanh, while at the same time shortening the performance gap between plain RNN and other more\ncomplicated models (e.g. LSTM by 57% on PTB and 54% on text8 datasets). This demonstrates the\nefﬁcacy of path-normalized optimization for training RNNs with ReLU activation.\n2http://mattmahoney.net/dc/textdata\n92\nChapter 11\nData-Dependent Path Normalization\nIn this chapter, we focus on two efﬁcient alternative optimization approaches proposed recently for\nfeed-forward neural networks that are based on intuitions about parametrization, normalization and\nthe geometry of parameter space: Path-SGD [14] was derived as steepest descent algorithm with\nrespect to particular regularizer (the ℓ2-path regularizer, i.e. the sum over all paths in the network of\nthe squared product over all weights in the path [58]) and is invariant to weight reparametrization.\nBatch-normalization [86] was derived by adding normalization layers in the network as a way of\ncontrolling the variance of the input each unit receives in a data-dependent fashion. In this chapter, we\npropose a uniﬁed framework which includes both approaches, and allows us to obtain additional methods\nwhich interpolate between them. Using our uniﬁed framework, we can also tease apart and combine two\ndifferent aspects of these two approaches: data-dependence and invariance to weight reparametrization.\nOur uniﬁed framework is based on ﬁrst choosing a per-node complexity measure we refer to as γv (deﬁned\nin Section 11.1). The choice of complexity measure is parametrized by a choice of “normalization matrix”\nR, and different choices for this matrix incorporate different amounts of data dependencies: for path-\nSGD, R is a non-data-dependent diagonal matrix, while for batch normalization it is a data-dependent\ncovariance matrix, and we can interpolate between the two extremes. Once γv is deﬁned, and for\nany choice of R, we identify two different optimization approaches: one relying on a normalized re-\nparameterization at each layer, as in batch normalization (Section 11.2), and the other an approximate\nsteepest descent as in path-SGD, which we refer to as DDP-SGD (Data Dependent Path SGD) and can\nbe implemented efﬁciently via forward and backward propagation on the network (Section 11.3). We\ncan now mix and match between the choice of R (i.e. the extent of data dependency) and the choice of\noptimization approach.\nOne particular advantage of the approximate steepest descent approach (DDP-SGD) over the normal-\nization approach is that it is invariant to weight rebalancing (discussed in Section 11.4). This is true\nregardless of the amount of data-dependence used. That is, it operates more directly on the model (the\nfunction deﬁned by the weights) rather than the parametrization (the values of the weights themselves).\nThis brings us to a more general discussion of parametrization invariance in feedforward networks\n(Section 8.2).\nOur uniﬁed framework and study of in invariances also allows us to relate the different optimization\napproaches to Natural Gradients [87]. In particular, we show that DDP-SGD with full data-dependence\ncan be seen as an efﬁcient approximation of the natural gradient using only the diagonal of the Fisher\ninformation matrix (Section 11.3).\nNotation\nThis chapter requires more involved notation that is slightly different that the notation of the\nrest of the dissertation. The notation is summarized in Figure 11.1.\n93\n𝑣\nℎ#\n𝑢\n𝑁in(𝑣)\n𝐰→#\n𝐰#→\n𝑁Out(𝑣)\n.\t\r  .\t\r  .\n.\t\r  .\t\r  .\n.\t\r  .\t\r  .\n.\t\r  .\t\r  .\n.\t\r  .\t\r  .\n.\t\r  .\t\r  .\n.\t\r  .\t\r  .\n.\t\r  .\t\r  .\n𝑉in\n𝑉234\nSoftmax+cross-­entropy,\nSquared  error,\nor  …\nInput\t\r  vector\t\r  𝐱\n𝑓𝐰(𝑥)\nℓ𝓁(𝑓𝐰𝐱, 𝐲)\n= 𝒛𝒗E\n𝑧#= 𝒘→#,𝐡\t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \t\r  \n𝑁in(𝑣)\nSymbol\nMeaning\nSymbol\nMeaning\nX / ⃗y\ninput vector / label\nVin / Vout\nthe set of input / output nodes\nw\nthe parameter vector\nwu→v\nthe weight of the edge (u →v)\nwv→\nthe vector of incoming weights to v\nw→v\nthe vector of outgoing weights from v\nN in(v)\nthe set of nodes feeding into v\nN out(v)\nthe set of nodes that v feeds into\nhv\nthe output value of node v\nzv\nthe activation value of node v\nFigure 11.1: An example of layered feedforward network and notation used in the chapter\nRelated Works\nThere has been an ongoing effort for better understanding of the optimization in\ndeep networks and several heuristics have been suggested to improve the training [88, 89, 90, 91].\nNatural gradient algorithm [87] is known to have a very strong invariance property; it is not only\ninvariant to reparametrization, but also to the choice of network architecture. However it is known to be\ncomputationally demanding and thus many approximations have been proposed [92, 93, 94]. However,\nsuch approximations make the algorithms less invariant than the original natural gradient algorithm. [95]\nalso discuss the connections between Natural Gradients and some of the other proposed methods for\ntraining neural networks, namely Hessian-Free Optimization [96], Krylov Subspace Descent [97] and\nTONGA [98].\n[62] also recently studied the issue of invariance and proposed computationally efﬁcient approximations\nand alternatives to natural gradient. They study invariances as different mappings from parameter space\nto the same function space while we look at the invariances as transformations (inside a ﬁxed parameter\nspace) to which the function is invariant in the model space (see Section 8.2). Unit-wise algorithms\nsuggested in Olivier’s work are based on block-diagonal approximations of Natural Gradient in which\nblocks correspond to non-input units. The computational cost of the these unit-wise algorithms is\nquadratic in the number of incoming weights. To alleviate this cost, [62] also proposed quasi-diagonal\napproximations which avoid the quadratic dependence but they are only invariant to afﬁne transformations\nof activation functions. The quasi-diagonal approximations are more similar to DDP-SGD in terms of\ncomputational complexity and invariances (see Section 11.4). In particular, ignoring the non-diagonal\nterms related to the biases in quasi-diagonal natural gradient suggested in [62], it is then equivalent to\ndiagonal Natural Gradient which is itself equivalent to special case of DDP-SGD when Rv is the second\nmoment (see Table 11.1 and the discussion on relation to the Natural Gradient in Section 11.3).\n11.1\nA Uniﬁed Framework\nWe deﬁne a complexity measure on each node as follows:\nγv(w) =\nq\nw⊤\n→vRvw→v\n(11.1.1)\n94\nRv\nMeasure\nNormalized reparametrization Diagonal steepest descent\nD = diag\n\u0000γ2\nNin(v)\n\u0001\nPath-Norm\nUnit-wise Path-Normalization\nPath-SGD\nC = Cov\n\u0010\nHNin(v))\n\u0011\nVariance\nBatch-Normalization\nM = E\nh\nHNin(v))H⊤\nNin(v))\ni\nSecond Moment\nDiag. Natural Gradient\nαM + (1 −α)D\nαC + (1 −α)D\nDDP-Norm\nDDP-Normalization\nDDP-SGD\nNode-wise Rescaling Invariant\nYes\nNo\nYes\nTable 11.1: Some of the choices for Rv in the proposed uniﬁed framework.\nwhere Rv is a positive semideﬁnite matrix that could depend on the computations feeding into v, and\ncaptures both the complexity of the nodes feeding into v and possibly their interactions. We consider\nseveral possibilities for Rv, summarized also in Table 1.\nA ﬁrst possibility is to set Rv = diag\n\u0000γ2\nNin(v)\n\u0001\nto a diagonal matrix consisting of the complexities of the\nincoming units. This choice does not depend on the source distribution (i.e. the data), and also ignores the\neffect of activations (since the activation pattern depends on the input distribution) and of dependencies\nbetween different paths in the network. Intuitively, with this choice of Rv, the measure γv(w) captures\nthe “potential” (data independent) variability or instability at the node.\nAnother possibility is to set Rv to either the covariance (centralized second moment) or to the (uncen-\ntralized) second moment matrix of the outputs feeding into v. In this case, γ2\nv(w) would evaluate to the\nvariance or (uncentralized) second moment of zv. We could also linearly combined the data independent\nmeasure, which measures inherent instability, with one of these the data-dependent measure to obtain:\nγ2\nv(w) = αS(zv) + (1 −α)\nX\nu∈Nin(v)\nγ2\nu(w)w2\nu→v\n(v /∈Vin),\n(11.1.2)\nwhere S(zv) is either the variance or uncentralized second moment, and α is a parameter.\nThe complexity measure above is deﬁned for each node of the network separately, and propagates\nthrough the network. To get an overall measure of complexity we sum over the output units and deﬁne\nthe following complexity measure for the function fw as represented by the network:\nγ2\nnet(w) =\nX\nv∈Vout\nγ2\nv(w).\n(11.1.3)\nFor Rv = diag\n\u0000γ2\nNin(v)\n\u0001\n, this complexity measure agrees with the ℓ2-Path-regularizer as introduced by\n[58]. This is the sum over all paths in the network of the squared product of weights along the path. The\npath-regularizer is also equivalent to looking at the minimum over all “node rescalings” of w (i.e. all\npossibly rebalancing of weights yielding the same function fw) of the maxv ∥w→v∥. But, unlike this\nmax-norm measure, the path-regularizer does not depend on the rebalancing and is invariant to node\nrescalings [58].\nFor data-dependent choices of Rv, we also get a similar invariance property. We refer to the resulting\ncomplexity measure, γ2\nnet(w), as the Data-Dependent-Path (DDP) regularizer.\nAfter choosing Rv, we will think of γv as specifying the basic “geometry” and bias (for both optimization\nand learning) over weights. In terms of learning, we will (implicitly) prefer weights with smaller γv\nmeasure, and correspondingly in terms of optimization we will bias toward smaller γv “balls” (i.e. search\nover the part of the space where γv is smaller). We will consider two basic ways of doing this: In Section\n11.2 we will consider methods that explicitly try to keep γv small for all internal nodes in the network,\nthat is explicitly search over simpler weights. Any scaling is pushed to the output units, and this scaling\n95\nhopefully does not grow too much due. In Section 11.3 we will consider (approximate) steepest descent\nmethods with respect to the overall γnet, i.e. updates that aim at improving the training objective while\nbeing small in terms of their effect on γnet.\n11.2\nDD-Path Normalization: A Batch Normalization Approach\nIn this Section, we discuss an optimization approach based on ensuring γv for all internal nodes v are\nﬁxed and equal to one—that is, the complexity of all internal nodes is “normalized”, and any scaling\nhappens only at the output nodes. We show that with a choice of Rv = Cov\n\u0000hNin(v))\n\u0001\n, this is essentially\nequivalent to Batch Normalization [86].\nBatch-Normalization [86] was suggested as an alternate architecture, with special “normalization” layers,\nthat ensure the variance of node outputs are normalized throughout training. Considering a feed-forward\nnetwork as a graph, for each node v, the Batch-Normalization architecture has as parameters an (un-\nnormalized) incoming weight vector ˜w and two additional scalars cv, bv ∈R specifying scaling and\nshift respectively. The function computed by the network is then given by a forward propagation similar\nto standard feed-forward ReLU networks except that for each node an un-normalized activation is ﬁrst\ncomputed:\n˜zv =\n\n ˜w→v, hNin(v)\n\u000b\n(11.2.1)\nThen, this activation is normalized to obtain the normalized activation, which is also scaled and shifted,\nand the output of the unit is the output of the activation function for this activation value:\nzv = cv\n˜zv −E [ ˜zv]\np\nvar(˜zv)\n+ bv\nhv = [zv]+\n(11.2.2)\nThe variance and expectation are actually calculated on a “mini-batch” of training examples, giving the\nmethod its name. Batch-normalization then proceeds by training the architecture speciﬁed in (11.2.1)\nand (11.2.2) through mini-batch stochastic gradient descent, with each gradient mini-batch also used for\nestimating the variance and expectation in (11.2.2) for all points in the mini-batch.\nInstead of viewing batch-normalization as modifying the architecture, or forward propagation, we can\nview it as a re-parameterization, or change of variables, of the weights in standard feed-forward networks.\nIn particular, instead of specifying the weights directly through w, we specify them through ˜w, b and c,\nwith the mapping:\n˜γ2\nv = ˜w⊤\n→vRv ˜w→v\nRv = Cov(hNin(v))\n(11.2.3)\nwu→v =\n(\nc ˜\nwu→v\n˜γv\nu ̸= vbias\nb −c\nE[⟨˜w→v,hNin(v)⟩]\n˜γv\nu = vbias\n(11.2.4)\nThe model class of functions used by Batch-Normalization is thus exactly the same model class corre-\nsponding to standard feed-forward network, just the parameterization is different. However, the change\nof variables from w to ˜w, b, c changes the geometry implied by the parameter space, and consequently\nthe trajectory (in model space) of gradient updates—effectively transforming the gradient direction by\nthe Jacobian between the two parameterizations. Batch-Normalization can thus be viewed as an alternate\noptimization on the same model class as standard feed-forward networks, but with a different geometry.\nThe reparametrization ensures that γv(w) = cv for all nodes—that is, the complexity is explicit in\nthe parameterization and thus gets implicitly regularized through the implicit regularization inherent in\nstochastic gradient updates.\nThe re-parameterization (11.2.4) is redundant and includes more parameters than the original param-\n96\neterization w—in addition to one parameter per edge, it includes also two additional parameters per\nnode, namely the shift bv and scaling cv. The scaling parameters at internal nodes can be avoided and\nremoved by noting that in ReLU networks, due to the node-rescaling property, all scaling can be done at\nthe output nodes. That is, ﬁxing cv = 1 for all internal v does not actually change the model class (all\nfunctions realizable by the model can be realized this way). Similarly, we can also avoid the additional\nshift parameter bv and rely only on bias units and bias weights ˜wvbias→v that get renormalized together\nwith weights. The bias term ˜wvbias→v does not affect normalization (since it is deterministic and so has\nno effect on the variance), it just gets rescaled with the other weights.\nWe thus propose using a simpler reparametrization (change of variables), with the same number of\nparameters, using only ˜w and deﬁning for each internal unit:\nwu→v = ˜wu→v\n˜γv\n(11.2.5)\nwith ˜γv as in (11.2.3), and with the output nodes un-normalized: w→Vout = ˜w→Vout. This ensures that\nfor all internal nodes γv(w) = 1.\nGoing beyond Batch-Normalization, we can also use the same approach with other choices of Rv,\nincluding all those in Table 1: We work with a reparametrization ˜w, deﬁned through (11.2.3) and (11.2.5)\nbut with different choices of Rv, and take gradient (or stochastic gradient) steps with respect to ˜w.\nExpectations in the deﬁnition of Rv can be estimated on the stochastic gradient descent mini-batch\nas in Batch-Normalization, or on independent samples of labeled or unlabeled examples. We refer to\nsuch methods as “DDP-Normalized” optimization. Gradients in DDP-Normalization can be calculated\nimplemented very efﬁciently similar to Batch-Normalization (see Section 11.5.2).\nWhen using this type of DDP-Normalization, we ensure that for any internal node γv(w) = 1 (the value\nof ˜γv can be very different from 1, but what is ﬁxed is the value of γv as deﬁned in (11.1.1) in terms of\nthe weights w, which in turn can be derived from ˜w through (11.2.4)), and so the overall complexity\nγnet(w) depends only on the scaling at the output layer.\nAnother interesting property of DDP-Normalization updates is that for any internal node v, the updates\ndirection of ˜w→v is exactly orthogonal to the weights:\nTheorem 37. For any weight ˜w in DDP-Normalization and any non-input node v /∈Vin\n\u001c\n˜w→v,\n∂L\n∂˜w→v\n\u001d\n= 0\n(11.2.6)\nProof. First we see that (8.2.5) is true because\n∂fw(x)[v]\n∂w\n=\n\u0010 X\np∈Π(v)\n∂πp(w)\n∂we\n· gp(x) · x[head(p)]\n\u0011\ne∈E = Jv(w)⊤· φv(x).\nTherefore,\n[\nx∈R|Vin|\nSpan\n\u0012∂fw(x)[v]\n∂w\n: v ∈Vout\n\u0013\n=\n[\nx∈R|Vin|\nSpan (Jv(w)⊤· φv(x) : v ∈Vout)\n= J(w)⊤· Span\n\u0010\nφ(x) : x ∈R|Vin|\u0011\n.\n(11.2.7)\nConsequently, any vector of the form ( ∂fw(x)[v]\n∂we\n)e∈E for a ﬁxed input x lies in the span of the row vectors\nof the path Jacobian J(x).\nThe second part says dG(w) = rankJ(w) if dim\n\u0000Span(φ(x) : x ∈R|Vin|)\n\u0001\n= |Π|, which is the\nnumber of rows of J(w). We can see that this is true from expression (11.2.7).\n97\nThe fact that the gradient is orthogonal to the parameters means weight updates in DDP-Normalization\nare done in a way that it prevents the norm of weights to change considerably after each updates.\n11.3\nDD-Path-SGD: A Steepest Descent Approach\nWe now turn to a more direct approach of using our complexity measure for optimization. To do so,\nlet us ﬁrst recall the strong connection between geometry, regularization and optimization through the\nspeciﬁc example of gradient descent.\nGradient descent can be thought of as steepest descent with respect to the Euclidean norm—that is, it\ntakes a step in a direction that maximizes improvement in the objective while also being small in terms\nof the Euclidean norm of the step. The step can also be viewed as a regularized optimization of the linear\napproximation given by the gradient, where the regularizer is squared Euclidean norm. Gradient Descent\nis then inherently linked to the Euclidean norm—runtime of optimization is controlled by the Euclidean\nnorm of the optimum and stochastic gradient descent yields implicit Euclidean norm regularization. A\nchange in norm or regularizer, which we think of as a change of geometry, would then yield different\noptimization procedure linked to that norm.\nWhat we would like is to use the DDP-regularizer γnet(w) to deﬁne our geometry, and for that we need\na distance (or divergence) measure corresponding to it by which we can measure the “size” of each\nstep, and require steps to be small under this measure. We cannot quite do this, but instead we use a\ndiagonal quadratic approximation of γnet(w) about our current iterate, and then take a steepest descent\nstep w.r.t. the quadratic norm deﬁned by this approximation.\nSpeciﬁcally, given a choice of Rv and so complexity measure γnet(w), for the current iterate w(t) we\ndeﬁne the following quadratic approximation:\nˆγ2\nnet(w(t) +∆w) = γ2\nnet(w(t))+\nD\n∇γ2\nnet(w(t)), ∆w\nE\n+ 1\n2∆w⊤diag\n\u0010\n∇2γ2\nnet(w(t))\n\u0011\n∆w (11.3.1)\nand the corresponding quadratic norm:\n∥w′ −w∥2\nˆγ2\nnet = ∥w′ −w∥2\ndiag( 1\n2 ∇2γ2\nnet(w(t))) =\nX\n(u→v)∈G\n1\n2\n∂2γ2\nnet\n∂w2u→v\n(w′\nu→v −wu→v)2.\n(11.3.2)\nWe can now deﬁne the DDP-update as:\nw(t+1) = min\nw η\nD\n∇L(w), w −w(t)E\n+ 1\n2 ∥w′ −w∥2\nˆγ2\nnet .\n(11.3.3)\nAnother way of viewing the above approximation is as taking a diagonal quadratic approximation of the\nBergman divergence of the regularizer. Solving (11.3.3) yields the update:\nw(t+1)\nu→v = wu→v −\nη\nκu→v(w)\n∂L\n∂wu→v\n(w(t))\nwhere: κu→v(w) = 1\n2\n∂2γ2\nnet\n∂w2u→v\n.\n(11.3.4)\nInstead of using the full gradient, we can also use a limited number of training examples to obtain\nstochastic estimates of\n∂L\n∂wu→v (w(t))—we refer to the resulting updates as DDP-SGD.\nFor the choice Rv = diag(γ2\nNin(v)), we have that γ2\nnet is the Path-norm and we recover Path-SGD [14].\nAs was shown there, the Path-SGD updates can be calculated efﬁciently using a forward and backward\npropagation on the network, similar to classical back-prop. In Section 11.5.2 we show how this type of\ncomputation can be done more generally also for other choices of Rv in Table 1.\n98\nRelation to the Natural Gradient\nThe DDP updates are similar in some ways to Natural Gradient updates, and it is interesting to understand\nthis connection. Like the DDP, the Natural Gradients direction is a steepest descent direction, but it\nis based on a divergence measure calculated directly on the function fw, and not the parameterization\nw, and as such is invariant to reparametrizations. The natural gradient is deﬁned as a steepest descent\ndirection with respect to the KL-divergence between probability distributions, and so to refer to it we\nmust refer to some probabilistic model. In our case, this will be a conditional probability model for labels\nY conditioned on the inputs X, taking expectation with respect to the true marginal data distribution\nover X.\nWhat we will show that for the choice Rv = E[hN in(v)h⊤\nN in(v)], the DDP update can also be viewed\nas an approximate Natural Gradient update. More speciﬁcally, it is a diagonal approximation of the\nNatural Gradient for a conditional probability model q(Y|X; w) (of the labels Y given an input X)\nparametrized by w and speciﬁed by adding spherical Gaussian noise to the outputs of the network:\nY|X ∼N(fw(X), I|Vout|).\nGiven the conditional probability distribution q(Y|x; w), we can calculate the expected Fisher informa-\ntion matrix. This is a matrix indexed by parameters of the model, in our case edges e = (u →v) on the\ngraph and their corresponding weights we, with entries deﬁned as follows:\nF(w)[e, e′] = Ex∼p(X)EY∼q(Y|x;w)\n\u0014∂log q(Y|x; w)\n∂we\n∂log q(Y|x; w)\n∂we′\n\u0015\n,\n(11.3.5)\nwhere x ∼p(X) refers to the marginal source distribution (the data distribution). That is, we use the true\nmarginal distributing over X, and the model conditional distribution Y|X, ignoring the true labels. The\nNatural Gradient updates can then be written as(see Section 11.5.2 for more information):\nw(t+1) = w(t) −ηF(w(t))−1∇wL(w(t)).\n(11.3.6)\nIf we approximate the Fisher information matrix with its diagonal elements, the update step normalizes\neach dimension of the gradient with the corresponding element on the diagonal of the Fisher information\nmatrix:\nw(t+1)\ne\n= w(t)\ne\n−\nη\nF(w)[e, e]\n∂L\n∂we\n(w(t)).\n(11.3.7)\nUsing diagonal approximation of Fisher information matrix to normalize the gradient values has been\nsuggested before as a computationally tractable alternative to the full Natural Gradient [99, 100]. [62]\nalso suggested a “quasi-diagonal\" approximations that includes, in addition to the diagonal, also some\nnon-diagonal terms corresponding to the relationship between the bias term and every other incoming\nweight into a unit.\nFor our Gaussian probability model, where log q(Y|X) = 1\n2 ∥Y −fw(X)∥2 + const, the diagonal can\nbe calculated as:\nF(w)[e, e] = EX∼p(X)\n\" X\nv′∈Vout\n\u0012∂fw(X)[v′]\n∂we\n\u00132#\n,\n(11.3.8)\nusing (11.5.12). We next prove that this update is equivalent to DDP-SGD for a speciﬁc choice of Rv,\nnamely the second moment.\nTheorem 38. The Diagonal Natural Gradient indicated in equations (11.3.7) and (11.3.8) is equivalent\nto DDP-SGD for Rv = E\nh\nhNin(v)h⊤\nNin(v)\ni\n.\n99\nProof. We calculate the scaling factor κu→v(w) for DDP-SGD as follows:\nκu→v(w) = 1\n2\n∂2γ2\nnet\n∂w2u→v\n= 1\n2\nX\nv′∈Vout\n∂2E[z2\nv′]\n∂w2u→v\n=\nX\nv′∈Vout\n∂\n∂wu→v\n\u00121\n2\n∂E[z2\nv′]\n∂wu→v\n\u0013\n=\nX\nv′∈Vout\n∂\n∂wu→v\n\u0012\nE\n\u0014\nzv′\n∂zv′\n∂wu→v\n\u0015\u0013\n=\nX\nv′∈Vout\n∂\n∂wu→v\n\u0012\nE\n\u0014\nzv′hu\n∂zv′\n∂zv\n\u0015\u0013\n=\nX\nv′∈Vout\nE\n\"\nh2\nu\n\u0012∂zv′\n∂zv\n\u00132#\n= E\n\"\nh2\nu\nX\nv′∈Vout\n\u0012∂zv′\n∂zv\n\u00132#\n= E\n\" X\nv′∈Vout\n\u0012∂fw(X)[v′]\n∂we\n\u00132#\n= F(w)[u →v, u →v]\nTherefore, the scaling factors in DDP-SGD with Rv = E\nh\nhNin(v)h⊤\nNin(v)\ni\nare exactly the diagonal\nelements of the Fisher Information matrix used in the Natural Gradient updates.\n11.4\nNode-wise invariance\nIn this section, we show that DDP-SGD is invariant to node-wise rescalings, while DDP-Normalization\ndoes not have favorable invariance properties.\n11.4.1\nDDP-SGD on feedforward networks\nIn Chaper 8, we observed that feedforward ReLU networks are invariant to node-wise rescaling. To\nsee if DDP-SGD is also invariant to such rescaling, consider a rescaled w′ = T(w), where T is a\nrescaling by ρ at node v. Let w+ denote the weights after a step of DDP-SGD. To establish invariance to\nnode-rescaling we need to show that w′+ = T(w+). For the outgoing weights from v we have:\nw′+\nv→j = ρwv→j −\nρ2η\nκv→j(w)\n∂L\nρ∂wv→j\n(w)\n= ρ\n\u0012\nwv→j −\nη\nκv→j(w)\n∂L\n∂wv→j\n(w)\n\u0013\n= ρw+\nv→j\nSimilar calculations can be done for incoming weights to the node v. The only difference is that ρ will be\nsubstituted by 1/ρ. Moreover, note that due to non-negative homogeneity of ReLU activation function,\nthe updates for the rest of the weights remain exactly the same. Therefore, DDP-SGD is node-wise\nrescaling invariant.\n11.4.2\nSGD on DDP-Normalized networks\nSince DDP-Normalized networks are reparametrization of feedforward networks, their invariances are\ndifferent. Since the operations in DDP-Normalized networks are based on ˜w, we should study the\ninvariances for ˜w. The invariances in this case are given by rescaling of incoming weights into a node,\n100\ni.e. for an internal node v and scaling ρ > 0:\nT( ˜w)k→v = ρ ˜wk→v\n(∀k ∈N in(v))\nwhile all other weights are unchanged. The DDP-Normalized networks are invariant to the above\ntransformation because the output of each node is normalized. The SGD update rule is however not\ninvariant to this transformation:\nT( ˜w)+\nk→v = ρ ˜wk→v −η\n∂L\nρ∂˜wk→v\n( ˜w) ̸= ρ\n\u0012\n˜wk→v −η\n∂L\n∂˜wk→v\n( ˜w)\n\u0013\n= ρ ˜w+\nk→v\n11.5\nSupporting Results\n11.5.1\nImplementation of DDP-Normalization\nGiven any batch of n data points to estimate mean, variance and the gradient, the stochastic gradients for\nthe weight ˜w (weights in the DDP-Normalized network) can then be calculated through the chain rule:\n∂L\n∂˜w→v\n=\n1\nn˜γv\nn\nX\ni=1\n∂L\n∂z(i)\nv\n\nh(i)\nN in(v) −1\nn\nn\nX\nj=1\nh(j)\nNin(v) −ˆz(i)\nv\n2˜γ2v\n∂˜γ2\nv\n∂˜w→v\n\n\n(11.5.1)\n∂L\n∂z(i)\nu\n= 1\n˜γv\n\n\nX\nv∈Nout(u)\n˜wu→v\n\n∂L\n∂z(i)\nv\n−1\nn\nn\nX\nj=1\n∂L\n∂z(j)\nv\n \n1 −α ˆz(i)\nv ˆz(j)\nv\n˜γ2v\n!\n\n\n\nz(i)\nu ≥0\n(11.5.2)\nwhere ˆz(i)\nv\n= ˜z(i)\nv\n−1\nn\nPn\nj=1 ˜z(j)\nv\nand we have:\n∂˜γ2\nv\n∂˜w→v\n= 2(1 −α) ˜w→v + 2α\nn\nn\nX\ni=1\nˆz(i)\nv\n\nh(i)\nN in(v) −1\nn\nn\nX\nj=1\nh(j)\nNin(v)\n\n\n(11.5.3)\nSimilar to Batch-Normalization, all the above calculations can be efﬁciently carried out as vector\noperations with negligible extra memory and computations.\n11.5.2\nImplementation of DDP-SGD\nIn order to compute the second derivatives κe(w) = ∂2γ2\nnet\n∂w2\ne , we ﬁrst calculate the ﬁrst derivative. The\nbackpropagation can be done through γ2\nu and z(i)\nu but this makes it difﬁcult to ﬁnd the second derivatives.\nInstead we propagate the loss through γ2\nu and the second order terms of the form z(i)\nu1 z(i)\nu2 :\n∂γ2\nnet\n∂γ2u\n= (1 −α)\nX\nv∈Nout(u)\n∂γ2\nnet\n∂γ2v\nw2\nu→v\n(11.5.4)\n101\n∂γ2\nnet\n∂(z(i)\nu1 z(i)\nu2 )\n= α\n\u0014∂γ2\nnet\n∂γ2u1\n\u0015\nu1=u2\n+\n\n\nX\n(v1,v2)∈(Nout(u1))2\n∂γ2\nnet\n∂(z(i)\nv1 z(i)\nv2 )\nwu1→v1wu2→v2\n\n\nz(i)\nu1 >0,z(i)\nu2 >0\n(11.5.5)\nNow we can calculate the partials for wu→v as follows:\n∂γ2\nnet\n∂wu→v\n= 2(1 −α)∂γ2\nnet\n∂γ2v\nγ2\nuwu→v + 2\nn\nX\ni=1\nX\nv′∈Nout(u)\n∂γ2\nnet\n∂(z(i)\nv z(i)\nv′ )\nh(i)\nu z(i)\nv′\n(11.5.6)\nSince the partials ∂γ2\nnet\n∂γ2u and\n∂γ2\nnet\n∂(z(i)\nu1 z(i)\nu2 ) do not depend on wu→v, the second order derivative can be\ncalculated directly:\nκu→v(w) = 1\n2\n∂2γ2\nnet\n∂w2u→v\n= (1 −α)∂γ2\nnet\n∂γ2v\nγ2\nu +\nn\nX\ni=1\n∂γ2\nnet\n∂\n\u0010\nz(i)\nv\n2\u0011\n\u0010\nh(i)\nu\n\u00112\n(11.5.7)\n11.5.3\nNatural Gradient\nThe natural gradient algorithm [87] achieves invariance by applying the inverse of the Fisher information\nmatrix F(w(t)) at the current parameter w(t) to the negative gradient direction as follows:\nw(t+1) = w(t) + η∆(natural),\nwhere\n∆(natural) = argmin\n∆∈R|E|\n\u001c\n−∂L\n∂w(w(t)), ∆\n\u001d\n,\ns.t.\n∆⊤F(w(t))∆≤δ2\n(11.5.8)\n= −F −1(w(t)) ∂L\n∂w(w(t)).\n(11.5.9)\nHere F(w) is the Fisher information matrix at point w and is deﬁned with respect to the probabilistic\nview of the feedforward neural network model, which we describe in more detail below.\nSuppose that we are solving a classiﬁcation problem and the ﬁnal layer of the network is fed into a\nsoftmax layer that determines the probability of candidate classes given the input x. Then the neural\nnetwork with the softmax layer can be viewed as a conditional probability distribution\nq(y|x) =\nexp(fw(x)[vy])\nP\nv∈Vout exp(fw(x)[v]),\n(11.5.10)\nwhere vy is the output node corresponding to class y. If we are solving a regression problem a Gaussian\ndistribution is probably more appropriate for q(y|x).\nGiven the conditional probability distribution q(y|x), the Fisher information matrix can be deﬁned as\nfollows:\nF(w)[e, e′] = Ex∼p(X)Ey∼q(y|x)\n\u0014∂log q(y|x)\n∂we\n∂log q(y|x)\n∂we′\n\u0015\n,\n(11.5.11)\nwhere p(x) is the marginal distribution of the data.\n102\nSince we have\n∂log q(y|x)\n∂wu→v\n= ∂log q(y|x)\n∂zv\n· hu =\nX\nv′∈Vout\n∂log q(y|x)\n∂zv′\n· ∂zv′\n∂zv\n· hu\n(11.5.12)\nusing the chain rule, each entry of the Fisher information matrix can be computed efﬁciently by forward\nand backward propagations on a minibatch.\n103\nConclusion\nIn this dissertation, we tried to explain generalization in deep learning with a view that is central around\nimplicit regularization by the optimization algorithm, showing that the implicit regularization is the main\ncomponent that should be taken into account. We proved several generalization guarantees based on\ndifferent complexity measures for neural networks and investigated whether implicit regularization is\nindeed penalizing the complexity of the model based on any of those measures. Finally, we designed\noptimization algorithms to implicitly regularize complexity measures that are more suitable for neural\nnetworks and provided empirical evidence indicating that these algorithms lead to better generalization\nthan SGD for feedforward and recurrent networks.\n104\nBibliography\n[1] J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85–117,\n2015. doi: 10.1016/j.neunet.2014.09.003.\n[2] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural computation, 18(7):1527–1554, 2006.\n[3] Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. Greedy layer-wise training\nof deep networks. Advances in neural information processing systems, 19:153, 2007.\n[4] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,\n2015.\n[5] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are\nuniversal approximators. Neural networks, 2(5):359–366, 1989.\n[6] Michael Sipser. Introduction to the Theory of Computation. Thomson Course Technology, 2006.\n[7] Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cam-\nbridge University Press, 2009.\n[8] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to\nAlgorithms. Cambridge University Press, 2014.\n[9] Peter L. Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size of\nthe weights is more important than the size of the network. IEEE transactions on information\ntheory, 44(2):525–536, 1998.\n[10] Adam R Klivansand Alexander A Sherstov. Cryptographic hardness for learning intersections of\nhalfspaces. In Foundations of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium\non, pages 553–562. IEEE, 2006.\n[11] Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. From average case complexity to improper\nlearning complexity. STOC, 2014.\n[12] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing\ngradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.\n[13] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping\nTak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima.\narXiv preprint arXiv:1609.04836, 2016.\n[14] Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-SGD: Path-normalized\noptimization in deep neural networks. In Advanced in Neural Information Processsing Systems\n(NIPS), 2015.\n105\n[15] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\nIn Proceedings of the International Conference on Machine Learning (ICML), pages 807–814,\n2010.\n[16] Xavier Glorot Antoine Bordes and Yoshua Bengio. Deep sparse rectiﬁer networks. AISTATS,\n2011.\n[17] M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang, Q.V. Le, P. Nguyen, A. Senior, V. Van-\nhoucke, J. Dean, and G.E. Hinton. On rectiﬁed linear units for speech processing. ICASSP,\n2013.\n[18] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies\nof events to their probabilities. Theory of Probability & Its Applications, 16(2):264–280, 1971.\n[19] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam’s razor.\nInformation processing letters, 24(6):377–380, 1987.\n[20] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cam-\nbridge university press, 2009.\n[21] Peter L Bartlett. The sample complexity of pattern classiﬁcation with neural networks: the size of\nthe weights is more important than the size of the network. IEEE transactions on Information\nTheory, 44(2):525–536, 1998.\n[22] Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise\npolynomial networks. Neural computation, 10(8):2159–2173, 1998.\n[23] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to\nalgorithms. Cambridge university press, 2014.\n[24] P. L. Bartlett. The impact of the nonlinearity on the VC-dimension of a deep network. Preprint,\n2017.\n[25] Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise\nlinear neural networks. arXiv preprint arXiv:1703.02930, 2017.\n[26] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-\ning deep learning requires rethinking generalization. In International Conference on Learning\nRepresentations, 2017.\n[27] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On\nthe role of implicit regularization in deep learning. Proceeding of the International Conference on\nLearning Representations workshop track, 2015.\n[28] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and\nstructural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.\n[29] Ulrike von Luxburg and Olivier Bousquet. Distance-based classiﬁcation with lipschitz functions.\nJournal of Machine Learning Research, 5(Jun):669–695, 2004.\n[30] Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391–423,\n2012.\n[31] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Generalization error of\ninvariant classiﬁers. arXiv preprint arXiv:1610.04574, 2016.\n[32] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for\ndeep (stochastic) neural networks with many more parameters than training data. arXiv preprint\narXiv:1703.11008, 2017.\n106\n[33] David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual\nconference on Computational learning theory, pages 230–234. ACM, 1998.\n[34] David A McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual\nconference on Computational learning theory, pages 164–170. ACM, 1999.\n[35] David McAllester. Simpliﬁed pac-bayesian margin bounds. Lecture notes in computer science,\npages 203–215, 2003.\n[36] John Langford and John Shawe-Taylor. Pac-bayes & margins. In Advances in neural information\nprocessing systems, pages 439–446, 2003.\n[37] John Langford and Rich Caruana. (not) bounding the true error. In Proceedings of the 14th\nInternational Conference on Neural Information Processing Systems: Natural and Synthetic,\npages 809–816. MIT Press, 2001.\n[38] Nathan Srebro, Jason Rennie, and Tommi S. Jaakkola. Maximum-margin matrix factorization.\nAdvances in neural information processing systems, pages 1329–1336, 2004.\n[39] Maryam Fazel, Haitham Hindi, and Stephen P. Boyd. A rank minimization heuristic with appli-\ncation to minimum order system approximation. Proceedings of American Control Conference,\npages 4734–4739, 2001.\n[40] Samuel Burer and Changhui Choi. Computational enhancements in low-rank semideﬁnite pro-\ngramming. Optimization Methods and Software, 21(3):493–512, 2006.\n[41] Nathan Srebro and Tommi S. Jaakkola. Weighted low-rank approximations. ICML, pages 720–727,\n2003.\n[42] Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan\nSrebro. Implicit regularization in matrix factorization. arXiv preprint arXiv:1705.09280, 2017.\n[43] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative\nprediction. In Proceedings of the 22nd international conference on Machine learning, pages\n713–719. ACM, 2005.\n[44] Nathan Srebro and Ruslan Salakhutdinov. Collaborative ﬁltering in a non-uniform world: Learning\nwith the weighted trace norm. In Advances in Neural Information Processing Systems, pages\n2056–2064, 2010.\n[45] Yoshua Bengio, Nicolas L. Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex\nneural networks. Advances in neural information processing systems, pages 123–130, 2005.\n[46] Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for\nneural networks. arXiv preprint arXiv:1706.08498, 2017.\n[47] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds\nand structural results. The Journal of Machine Learning Research, pages 463–482, 2003.\n[48] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions and bounding the\ngeneralization error of combined classiﬁers. Annals of Statistics, pages 1–50, 2002.\n[49] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Technical report,\nHAL-01098505, 2014.\n[50] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. Advances in neural\ninformation processing systems, pages 342–350, 2009.\n107\n[51] Wee Sun Lee, Peter L Bartlett, and Robert C Williamson. Efﬁcient agnostic learning of neural\nnetworks with bounded fan-in. Information Theory, IEEE Transactions on, 42(6):2118–2132,\n1996.\n[52] Sham M Kakade, Karthik Sridharan, and AmbujTewari. On the complexity of linear prediction:\nRisk bounds, margin bounds, and regularization. Advances in neural information processing\nsystems, pages 793–800, 2009.\n[53] Maria-Florina Balcan and Christopher Berlind. A new perspective on learning linear separators\nwith large lqlp margins. Proceedings of the Seventeenth International Conference on Artiﬁcial\nIntelligence and Statistics, pages 68–76, 2014.\n[54] Uffe Haagerup. The best constants in the khintchine inequality. Studia Mathematica, 70(3):\n231–283, 1981.\n[55] Adam R Klivans and Alexander A Sherstov. Cryptographic hardness for learning intersections of\nhalfspaces. FOCS, pages 553–562, 2006.\n[56] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efﬁciency of training\nneural networks. Advances in Neural Information Processing Systems, pages 855–863, 2014.\n[57] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computa-\ntional mathematics, 12(4):389–434, 2012.\n[58] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural\nnetworks. In Proceeding of the 28th Conference on Learning Theory (COLT), 2015.\n[59] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n[60] Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent\npath normalization in neural networks. In the International Conference on Learning Representa-\ntions, 2016.\n[61] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent.\nIn Advances in neural information processing systems, pages 2645–2653, 2011.\n[62] Yann Ollivier. Riemannian metrics for neural networks ii: recurrent networks and learning\nsymbolic data sequences. Information and Inference, page iav007, 2015.\n[63] Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent\npath normalization in neural networks. In International Conference on Learning Representations,\n2016.\n[64] Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, and Nathan Srebro. Path-normalized\noptimization of recurrent neural networks with relu activations. Advances in Neural Information\nProcessing Systems, 2016.\n[65] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural\nnetworks. In The 28th Conference on Learning Theory, pages 1376–1401, 2015.\n[66] Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Learning Theory, pages\n545–560. Springer, 2005.\n[67] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron C. Courville, and Yoshua Bengio.\nMaxout networks. In Proceedings of the 30th International Conference on Machine Learning,\nICML, pages 1319–1327, 2013.\n108\n[68] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[69] Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized\noptimization in deep neural networks. In Advances in Neural Information Processing Systems,\npages 2413–2421, 2015.\n[70] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[71] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.\nComputer Science Department, University of Toronto, Tech. Rep, 1(4):7, 2009.\n[72] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\ndigits in natural images with unsupervised feature learning. In NIPS workshop on deep learning\nand unsupervised feature learning, 2011.\n[73] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:\nOn the role of implicit regularization in deep learning. International Conference on Learning\nRepresentations (ICLR) workshop track, 2015.\n[74] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[75] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem\nsolutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02),\n1998.\n[76] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with\ngradient descent is difﬁcult. Neural Networks, IEEE Transactions on, 5(2):157–166, 1994.\n[77] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8),\n1997.\n[78] Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks\nof rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015.\n[79] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.\narXiv preprint arXiv:1511.06464, 2015.\n[80] Sachin S. Talathi and Aniket Vartak. Improving performance of recurrent neural network with relu\nnonlinearity. In the International Conference on Learning Representations workshop track, 2014.\n[81] Marius Pachitariu and Maneesh Sahani. Regularization and nonlinearities for neural language\nmodels: when are they needed? arXiv preprint arXiv:1301.5650, 2013.\n[82] Tomáš\nMikolov,\nIlya\nSutskever,\nAnoop\nDeoras,\nHai-Son\nLe,\nStefan\nKom-\nbrink,\nand\nJ\nCernocky.\nSubword\nlanguage\nmodeling\nwith\nneural\nnetworks.\n(http://www.ﬁt.vutbr.cz/ imikolov/rnnlm/char.pdf), 2012.\n[83] David Krueger and Roland Memisevic. Regularizing RNNs by stabilizing activations. In Proceed-\ning of the International Conference on Learning Representations, 2016.\n[84] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceeding of\nthe International Conference on Learning Representations, 2015.\n109\n[85] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear\ndynamics of learning in deep linear neural networks. In International Conference on Learning\nRepresentations, 2014.\n[86] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In ICML, 2015.\n[87] Shun-Ichi Amari. Natural gradient works efﬁciently in learning. Neural computation, 10(2):\n251–276, 1998.\n[88] Yann Le Cun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller. Efﬁcient backprop. In\nNeural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer\nVerlag, 1998. URL .\n[89] Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, and Pascal Lamblin. Exploring strategies\nfor training deep neural networks. The Journal of Machine Learning Research, 10:1–40, 2009.\n[90] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward\nneural networks. In AISTATS, 2010.\n[91] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.\nOn the importance of\ninitialization and momentum in deep learning. In ICML, 2013.\n[92] Roger Grosse and Ruslan Salakhudinov. Scaling up natural gradient by sparsely factorizing the\ninverse Fisher matrix. In ICML, 2015.\n[93] James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approxi-\nmate curvature. In ICML, 2015.\n[94] Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and Koray Kavukcuoglu. Natural neural\nnetworks. arXiv preprint arXiv:1507.00210, 2015.\n[95] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In ICLR,\n2014.\n[96] James Martens. Deep learning via hessian-free optimization. In ICML, 2010.\n[97] Oriol Vinyals and Daniel Povey. Krylov subspace descent for deep learning. In ICML, 2011.\n[98] Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural\ngradient algorithm. In NIPS, 2008.\n[99] Yann LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Neural networks-tricks of\nthe trade. Springer Lecture Notes in Computer Sciences, 1524(5-50):7, 1998.\n[100] Tom Schaul, Sixin Zhang, and Yann Lecun. No more pesky learning rates. In ICML, 2013.\n110\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-09-06",
  "updated": "2017-09-08"
}