{
  "id": "http://arxiv.org/abs/1709.02076v1",
  "title": "Composition by Conversation",
  "authors": [
    "Donya Quick",
    "Clayton T. Morrison"
  ],
  "abstract": "Most musical programming languages are developed purely for coding virtual\ninstruments or algorithmic compositions. Although there has been some work in\nthe domain of musical query languages for music information retrieval, there\nhas been little attempt to unify the principles of musical programming and\nquery languages with cognitive and natural language processing models that\nwould facilitate the activity of composition by conversation. We present a\nprototype framework, called MusECI, that merges these domains, permitting\nscore-level algorithmic composition in a text editor while also supporting\nconnectivity to existing natural language processing frameworks.",
  "text": "Composition by Conversation\nDonya Quick\nStevens Institute of Technology\ndquick@stevens.edu\nClayton T. Morrison\nUniversity of Arizona\nclaytonm@email.arizona.edu\nABSTRACT\nMost musical programming languages are developed\npurely for coding virtual instruments or algorithmic com-\npositions. Although there has been some work in the do-\nmain of musical query languages for music information re-\ntrieval, there has been little attempt to unify the principles\nof musical programming and query languages with cogni-\ntive and natural language processing models that would\nfacilitate the activity of composition by conversation. We\npresent a prototype framework, called MusECI, that merges\nthese domains, permitting score-level algorithmic compo-\nsition in a text editor while also supporting connectivity to\nexisting natural language processing frameworks.\n1. INTRODUCTION\nWe have reached an age where natural language interfaces\nare increasingly available for our technology. These inter-\nfaces provide opportunities to interact more naturally with\nour devices. For example, it is becoming common for peo-\nple to speak directly to mobile devices to accomplish tasks\nwith no human on the other end of the interactions. How-\never, to date there has been little work on the support of\nconversation and interactions involving musical concepts\nand scores.\nDeep learning with neural networks is currently a popu-\nlar strategy in the domain of natural language processing\n(NLP), and has been applied to developing dialog systems\nwhere a machine attempts to carry on a conversation with\na human [1]. However, neural nets remain difﬁcult to an-\nalyze and extend once trained. In contrast, the classic ap-\nproach of explicit knowledge representation symbolically\nrepresents ideas and thought processes that can be directly\nanalyzed and extended, and provides a natural framework\nfor explaining decisions.\nFalling under the general strategy of knowledge represen-\ntation, elementary composable ideas, or ECIs, are simple,\natomic concepts that can be combined, or composed, to\nform more complex concepts and relationships. For exam-\nple, there is some evidence to suggest that “surface” is one\nsuch concept that infants have and use to build other con-\ncepts, such as collections of surfaces forming objects and\ninternal spaces [2]. This style of representation has been\nproposed to underly human natural language processing\ntasks such as describing and referring to items and events\nin visual environments [3, 4].\nCopyright: c⃝2017 Donya Quick et al. This is an open-access article dis-\ntributed under the terms of the Creative Commons Attribution License 4.0\nUnported, which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original author and source are credited.\nProgramming languages already follow an ECI-like\nmodel: there are primitives, such as integer and ﬂoating-\npoint numbers, and a collection of basic operations and\ndata structures over those entities that are used to repre-\nsent more complex concepts.\nObject-oriented program-\nming seeks to model computational problems much in the\nway we view our environment as composed of objects,\neach of which consists of parts that interact and can be\nmanipulated in particular ways.\nHowever, not all ideas\nﬁt neatly into this approach and there are many languages\nthat use very different strategies, such as functional pro-\ngramming languages. Additionally, appropriate language\nfeatures for a computer are not necessarily straightforward\nderivations from verbal descriptions—there is a reason why\nprogramming a computer is a complex skill that requires\nspecialized training.\nWe propose a basic collection of ECIs for music with an\naccompanying Python implementation called MusECI 1 .\nIn this paper we describe ECI-style musical primitives and\nstructures for grouping these primitives. We also provide\nbasic operations to facilitate conversational manipulation\nof music through a query-then-operation pattern of inter-\naction. This lays the foundation for eventually building\nlarger platforms that would facilitate real-time conversa-\ntions with machines about music. The goal of MusECI is\nnot to replace traditional score editing software, but rather\nto enable the creation of new systems to augment those\nenvironments and open new avenues for human-computer\ninteraction through music.\n2. PROGRAMMING LANGUAGES\nA central goal of the MusECI project is to create a lan-\nguage for score-level music representation that can be used\nin an algorithmic or automated composition setting while\nstill adhering closely to models of music cognition and\nexpression in natural language. We adopt the constraints\nthat the data structures used to represent music both have\na straightforward interpretation for editing and playback\nand that they can also be easily queried to identify fea-\ntures based on natural language descriptions and manipu-\nlated according to instructions.\nMany musical programming languages focus on the\nsound, or signal-level aspects of music generation. Ex-\namples of these include Csound [5], SuperCollider[6] 2 ,\nand Max/MSP[8]. While these languages are extremely\n1 Our implementation is available at musica.ml4ai.org/.\n2 SuperCollider also represents another language paradigm known as\nlive coding[7], which allows compilation and execution of fragments of\na larger program on demand. However, while gaining traction in music\nperformance, this kind of language paradigm has not been interfaced with\nNLP systems.\narXiv:1709.02076v1  [cs.SD]  7 Sep 2017\nuseful in the context of music creation, they do not offer\na rich variety of representations for higher level features\nin music. On the other extreme are languages that focus\non creating visually appealing musical scores. Languages\nsuch as LilyPond [9] focus on type-setting aspects of a\nscore, with emphasis on details of visual presentation and\nformatting and less on relationships between musical fea-\ntures that would be useful for modeling music cognition.\nWe seek a representation between the extremes of visual\ndetail and sound synthesis, focusing on common natural\nlanguage descriptors people use to talk about music: note,\nmelody, chord, phrase, and so on.\nOther languages for representing score-level information\nfavor grouping of musical features, such as melodies and\nharmonies, over the details of their visual presentation.\nMusicXML[10] is one such language. It is a markup lan-\nguage that describes score-level features in music in an\napplication-independent way such that a composition can\nbe passed between different score rendering programs. Mu-\nsical features are highly nested and groupings occur by vi-\nsual features in the score. Unlike MIDI format, time is\nmeasured in MusicXML by measures and beats, which are\nrelative to a particular tempo. MusECI’s implementation\nsupports MusicXML as an input format to specify musical\nstructures; the method for converting MusicXML’s stan-\ndard into our musical ECI data structures is described in\nsection 3.3.\n2.1 Algorithmic Score Analysis and Generation\nNone of the languages discussed so far support operations\nfor searching or manipulating the music.\nMuSQL [11]\nis a structured query language for operating on musical\ndatabases. It permits creation of, selection from, and alter-\nation to musical structures. Its representations of musical\nfeatures are generalized to address multiple kinds of pitch\ndata, although time representations are limited to seconds—\nthere is no method for describing more abstract metrical\nstructures and containers such as measures and beats.\nMuSQL uses standard SQL-style commands, which, al-\nthough intuitive to programmers with the right training, are\nnot necessarily easily derived from natural language state-\nments. One of MusECI’s goals is to bridge that gap.\nMusic21 [12] is a Python library that has become pop-\nular for hypothesis testing on large musical copora. It is\nable to read a variety of formats, including MIDI and Mu-\nsicXML, and uses an object-oriented style that is very close\nto the structures used by MusicXML. Music21 has been\nused in several computational musicology projects, includ-\ning analysis of the large Bach chorales MusicXML corpus.\nHowever, Music21 is primarily structured based on music\ntheoretic concepts, which, although potentially useful to\nanalyzing questions about music cognitive models, is not\nalways natural as a vehicle for mapping from natural lan-\nguage descriptions to operations on music.\nRepresentations for score-level structures in programming\nlanguages intended for algorithmic composition typically\nrevolve around the two classic computer science data struc-\ntures of lists and trees. The Jython Music library [13],\na Python version of JMusic [14], favors list representa-\ntions for constructing melodies and chords using notes and\nrests. Melodies are lists of notes and rests with the as-\nsumption that one element’s onset is determined by the\nprevious element’s end time. Chords are also represented\nas lists of notes having the same start time and duration.\nPython’s support for nested lists allows for chords to be\ncreated within a larger sequential, melodic structure. While\nuseful in pedagogical and algorithmic composition settings,\nthis list-based approach makes representation of features\nsuch as arpeggiated chords tricky and its style of nesting\nmusical concepts like parts and phrases is very rigid.\nIn the Euterpea library [15] for representing musical struc-\ntures in Haskell, musical ideas are represented as binary\ntrees, where leaves are notes and rests that store duration\ninformation but not onset time. The onset for a particu-\nlar leaf is determined by its placement in the larger tree,\nwhose intermediate nodes consist of two types of connec-\ntors: parallel and sequential. Sequentially composed sub-\ntrees are interpreted as following each other in time, and\nthose composed in parallel are assumed to have the same\nstart time (but not necessarily the same end time). Any\npattern of notes that can be represented in the standard\nMIDI ﬁle format (which follows a list-based representation\nof time-stamped events) can also be represented as one of\nEuterpea’s trees. 3 However, it is frequently the case that\na given musical concept can have more than one tree rep-\nresentation, making comparison of equality between tree\nstructures and identiﬁcation of shared features problem-\natic. It may also not be cognitively natural to nest relation-\nships in a strictly binary way—sometimes we may think of\nthree or more items as having the same level of hierarchy,\nmore like a list. Here, we use an n-ary tree representation\n(each node essentially being a list) to mitigate this issue in\ndeveloping a framework to connect to NLP systems.\nThe structures and operations we use here are also very\nsimilar to some representations used in Kulitta [16], which\nis a Haskell-based framework for automated composition\nin multiple styles. This lends our implementation to being\nuseful for automated composition tasks, while also allow-\ning querying of data structures in a NLP-compatible way.\n2.2 Modeling Natural Language Semantics\nA key constraint for MusECI is that it must accommodate\nthe semantics of how music is described in natural lan-\nguage. To accomplish this, we take inspiration from com-\nputational languages designed for representing the natural\nlanguage semantics of visual and physical environments,\nsuch as VoxML [4].\nThe VoxML modeling language encodes semantic knowl-\nedge of real-world objects represented as three-dimensional\nphysical models along with events and attributes that are\nrelated and change through actions and interactions. The\nframework encodes background knowledge about how in-\nteractions between objects affect their properties, allow-\ning simulation to ﬁll in information missing from language\ninput. With this representation, natural language expres-\nsions describing scenes, events, and actions on objects can\nbe speciﬁed in enough detail to be simulated and visu-\nalized. We similarly seek a language in which musical\nconcepts can be expressed in building-blocks that stand in\nclose correspondence to natural language expressions. The\n3 This is true for notes as they would appear on a score. Performance\nfeatures, such as pitch bends and aftertouch, do not have a corresponding\nrepresentation in Euterpea.\nframework must capture enough detail to support express-\ning background music knowledge typically assumed in or-\nder to interpret natural language descriptions of music as\nwell-deﬁned musical structures that can be represented as\nscores and played—a kind of “music simulation”.\n3. MUSICAL PRIMITIVES\nOur lowest level of representations for Musical ECIs are\ncalled musical primitives: concepts that represent funda-\nmental musical units. Because we are focused on music\nat the level of a paper score, our primitives are based on\nfeatures that appear on scores: notes and rests.\n3.1 Symbolic Primitives\nMusECI’s\natomic\nmusical\nconcepts\nare\nrelated\nto\npitch and duration, although these, by themselves, cannot\nbe directly represented on a musical score. Currently we\nonly consider Western tonal systems based on the chro-\nmatic scale and therefore represent pitch information using\nintegers. Since some of the atomic concepts are best repre-\nsented as numbers, we make use of two number types: Int\nand Float. We begin with the following two pitch-related\natomic primitives, both of which are simply musical inter-\npretations of numbers:\nPitchClass(Int)\nOctave(Int)\n(1)\nThese then yield the following derived primitives, which\nare built on atomic primitives.\nPitch(PitchClass, Octave)\nScaleIndex(Int, Contexts)\n(2)\nwhere Contexts refers to a collection of environmental\ninformation and other labels used for resolving ambiguity.\nFor ScaleIndex, the Context would need to include infor-\nmation about the current Scale. The pitch number for a\npitch can be computed by PitchClass+12(Octave+ k),\nwhere k has different standards in different areas of the lit-\nerature. 4\nThe following two atomic primitives are building-blocks\nfor metrical information:\nBeat(Float)\nMeasure(Int)\n(3)\nDerived primitives also exist for metrical information:\nOnset(Measure, Beat)\nDuration(Beat)\n(4)\nPitch and metrical primitives are composed to produce\nthe concepts of Note and a Rest, which are similar to the\nlowest levels of representation in Euterpea, JythonMusic\nand Music21. We deﬁne the set of all notes and rests to\ninclude the following primitives:\nNote(Pitch, Duration, Onset, Contexts)\nRest(Duration, Onset, Contexts)\n(5)\n4 Octave zero on a piano keyboard is not standardized and varies be-\ntween musical programming languages. Sometimes k = 0 such that\n(C,0) is pitch number 0, but it is also common to have k = 1 and occa-\nsionally even k = −1 depending on the particular application.\nWe denote the set of all such symbols as P. For brevity,\nin examples in this paper we will use N to refer to the\nPython-style constructor for Note and, in later sections,\nboth pitch and metrical values will be shown only as tuples.\nAn important difference between our approach and many\nother musical representations it that we do not require val-\nues to be declared for all properties of these concepts. The\nhigh degree of optional information in these constructs is\nneeded to reﬂect the various ways in which we speak about\nmusic, which can involve many levels of abstraction and\nalso may be ambiguous. For example, using “ ” to indi-\ncate a blank ﬁeld, we may capture the concept of “the C in\nmeasure 3” as follows 5 :\nN(Pitch(0,\n),\n, Onset(2,\n),\n)\n(6)\nThis speciﬁes a template that can be matched against a\nmusical representation with more concrete information to\nidentify the particular note being referred to. In MusECI,\nwe represent “ ” using Python’s None value. This vari-\nable degree of information is useful for specifying queries\nover musical structure and also for communicating musi-\ncal ideas at a variety of levels of abstraction that may not\nhave a uniform level of detail over the entire score.\nThe Contexts ﬁeld of Note and Rest indicates additional\nlabels or other contextual information that may be useful\nfor querying. Our implementation also supports volume as\na note property, but it is omitted from the descriptions and\nexamples in this paper.\n3.2 Connecting Primitives\nMusECI uses Euterpea-inspired representations for con-\nnecting musical structures in sequence and in parallel. To\nminimize the issue of having multiple tree structures as-\nsociated with Euterpea’s binary trees, we use n-ary trees.\nThis permits notes in a melody or chord that have the same\nlevel of conceptual hierarchy to appear at the same level\nwithin the representation. We also deﬁne normal forms\nfor grouping structures from a score format such as Mu-\nsicXML. The set of all possible sequential and parallel\nstructures, which we will call M, is deﬁned recursively.\nSeq({M|P}+, Contexts)\nPar({M|P}+, Contexts)\n(7)\nWe will use the square bracket notation, [...], to denote\nlists of items. Therefore, “the three note melody in part B”\ncould be represented as:\nSeq([N( ), N( ), N( )], “Part B”)\n(8)\nWe will use the term Music value to refer to any value that\nis in M ∪P. In other words, a Music value is some entity\nthat could be drawn on a score, whether a single note, a\nrest, a melody, a chord, etc. Later we describe a collection\nof operations that can be applied to any of these values.\n3.3 MIDI and MusicXML Conversion\nMusECI is able to parse both MIDI and MusicXML ﬁles\ninto a normal form with the representations discussed so\n5 In MusECI, measure and beat numbers both index from zero. There-\nfore, “measure 3” produces a value of 2.\nfar. We use the following algorithm to convert MIDI and\nMusicXML information into our system’s representations:\n1. Greedily group Notes with the same onset and dura-\ntion with Par.\n2. Greedily group temporally adjacent structures (po-\ntentially including chords from step 1) with Seq.\n3. Group any remaining temporally separated, but still\nsequential structures with Seq, using Rests to ﬁll\ntemporal gaps.\n4. Group any leftover items under a global Par.\nImportantly, this normal form for reading MusicXML is\nnot the only way to represent musical features. Other nor-\nmal forms are possible, and we hope to improve our nor-\nmal forms through learning in later work, such that nested\nstructures within melodies and phrases are identiﬁed in\ngenre-speciﬁc ways.\n4. SELECTION\nAn important operation for composition by conversation is\nselection. Much like the SELECT statement from SQL,\nwhich searches a database of tables for entries matching a\nquery, we wish to scan over a musical data structure and\nreturn references to the correct portions of it.\nWe use pattern matching as an integral part of our selec-\ntion process, which involves checking to see whether con-\ncrete deﬁnitions match features present in an incomplete or\nabstract deﬁnition.\nMusECI deﬁnes the select operation as a function that\ntakes two Music values: one to use as a pattern to search\nfor (a query) and the other to pattern match against. The\n“ ” values match any concrete value. For example, the\nstatement select(N( , , ...), m) will select all notes from\nthe Music value,, m and return a collection of references\nto the notes. Returning references in an object-oriented\nstyle allows for m, to be updated immediately after rele-\nvant parts of it are located:\nquery, operation = toQuery(parse)\noperation(select(query, m))\nwhere toQuery turns a parse of a natural language sen-\ntence into a query and an operation to perform on its result.\nBy leveraging Python’s functional language features,\nMusECI also permits conditionals and classiﬁers as part of\nquery statements. For example, ﬁnding notes above octave\n3, can be expressed as N(( , > 3), ...). Selection can also\nbe done for more complex queries using Seq and Par.\n5. OPERATIONS\nSupport for music creation and generation requires that\ncertain standard operators be deﬁned to manipulate the mu-\nsical data structures. The following are some examples of\noperations in MusECI that we use in examples later on:\n• invert(Music) and invertAt(Pitch, Music): gen-\neralized musical inversion (ﬂipping upside down\nalong the pitch axis) at the ﬁrst pitch and a speciﬁed\npitch respectively.\n• retrograde(Music): reverse a musical structure.\n• transpose(Int, Music): transpose a Music value\nby some number of half steps, either chromatically\nor according to scale degrees depending on the tonal\ncontext given.\n6. INTEGRATION WITH TRIPS PARSER\nIn our work we use the TRIPS natural language parser.\nTRIPS is a best-ﬁrst bottom-up chart parser that integrates\na grammar and lexicon to encode both syntactic and se-\nmantic features. These features are used to disambiguate\nand produce a logical form representation that captures the\nsemantic roles of terms in the utterance [17]. Figure 1\nshows the logical form graph that TRIPS produces after\nparsing the phrase, “Move the B in measure 2 up an oc-\ntave.” (Many details of the TRIPS logical form representa-\ntion have been removed for presentation.) Labels on arcs\n(surrounded by ‘< ... >’) represent semantic roles, and\nbold-face labels represent lexical terms preceded by their\nsemantic type (e.g., B is a MusicNote).\nSpeechAct REQUEST\nCauseMove MOVE\nPERSON\nMusicNote B\nDirectionUp UP\nPRESENT\n<AGENT>\n<AFFECTED>\n<RESULT>\n<TENSE>\n<CONTENT>\nMusicOctave OCTAVE\nMusicDomain MEASURE\nMeasure 2\nNumber\n*YOU*\n<GROUND>\n<SIZE>\n1\n<VALUE>\nInLoc IN\n<LOCATION>\n<GROUND>\n<NAME-OF>\nFigure 1. TRIPS parser-like representation of the phrase “Move the B in\nmeasure 2 up an octave.”\nThe logical form produced by TRIPS allows us to map\nfrom graphical patterns in the logical form to classes of\noperations on musical representations. From the example\nin Figure 1, a MOVE action in the direction UP affecting a\nNOTE corresponds to a transposition operation applied to\nour music representation. The source location of the Note\nspeciﬁes the query for a note with a pitch class of B and\nan onset within measure 2. Finally, moving up one octave\ncorresponds to transposing up 12 half steps. Together, this\ninformation speciﬁes the following operation:\ntranspose(12, select(N((B, ), , (1, )), m))\n(9)\n6.1 Resolving Ambiguities\nNatural language is extremely sensitive to conversational\nhistory. Sentences such as “give that to him” are impos-\nsible to semantically resolve without context for mapping\npronouns to entities. In a musical setting, references such\nas “the C next to it” also suffer from this issue.\nEven with sufﬁcient context to resolve references like\npronouns, there may be insufﬁcient detail to locate a pre-\ncise portion of a musical concept. Consider the pitch classes\nin the opening refrain for “Twinkle Twinkle Little Star:”\nC, C, G, G, A, A, G. Requesting to “move the G up a\nhalf step” creates an ambiguity about which G should be\nmoved,\nsince select(N((G, ), , ...)) will ﬁnd three\n44\n\n\n\n\nFigure 2. Starting melody.\n44\n\n\n\n\nFigure 3. The result of applying the change “Move the F up a whole step”\nto the melody in Figure 2.\n44\n\n\n\n\nFigure 4. The result of applying the change “Move the C on the ﬁrst beat\nof measure two down a half step.” to the melody in Figure 3.\nmatches. However, “the G” implies we only want to op-\nerate on one of them.\nA system for addressing referential ambiguity requires\ntwo features: a working memory of recent references, and\nan assumer algorithm or module that explores the working\nmemory using features from a parse tree to resolve am-\nbiguities. That resolution step may be straightforward or\nit may involve requesting additional information from the\nuser before proceeding. Disambiguation, therefore, has the\nfollowing workﬂow, where m is the target Music value.\nquery, operation = toQuery(parse)\nx = assumer.resolve(select(query, m))\noperation(x)\nThe creation of an appropriate assumer module is an area\nof ongoing work in conjunction with the TRIPS parser in-\ntegration. Because of this, the examples presented in the\nnext section are designed to avoid the two types of refer-\nential ambiguity described here. This means that all of the\nexamples can be parsed directly to code statements of the\nform operation(select(...)) with no need for ambiguity\nresolution before applying the operation derived from the\nnatural language sentence.\n6.2 Examples\nConsider the two-measure melody shown in Figure 2, and\nsuppose we wish to transform it into the melody shown\nin Figure 4 through natural language commands. A pre-\ncise way to describe this would be “move the F up a whole\nstep” followed by “move the C on the ﬁrst beat of measure\ntwo down a half step.” This will create the following inter-\nactions between the computer (C) and user (U).\nConversation 1\nC: m = the melody in Figure 2.\nU: “Move the F up a whole step.”\nC: transpose(2, select(N((F, ), , , ...), m))\nU: “Move the C on the ﬁrst beat of measure two down a\nhalf step.”\nC: transpose(−1, select(N((C, ), , (1, 0), ...), m))\nApplied sequentially,\nthese commands produce the\nmelodies shown in Figures 3 and 4 respectively. The data\nstructure for the starting melody is shown in Figure 7, along\n44\n\n\n\n\nFigure 5. The result of applying the change “invert the notes in measure\ntwo around G4” to the melody in Figure 3.\n44\n\n\n\n\nFigure 6. The result of applying the change “reverse the notes in measure\none” to the melody in Figure 5.\nwith the selection statements created by each of the com-\nmands and the resulting modiﬁcation to the data structure.\nStatements may also operate over collections of notes.\nThe following conversation illustrates this with inversion\nand reversal of sections of the music (retrograde). Figures\n5 and 6 show the result of each command in this conver-\nsation, and the data structure transformation is shown in\nFigure 8.\nConversation 2\nC: m = the melody in Figure 2.\nU: “Invert the notes in measure two around G4.”\nC: invertAt((G, 4), select(N( , , (1, ), ...), m))\nU: “Reverse the notes in measure one.”\nC: retro(select(N( , , (0, ), ...), m))\n7. CONCLUSION AND FUTURE WORK\nWe have proposed the MusECI framework for modeling\nmusic in a way that allows querying based on parsing of\nnatural language statements. Our Python implementation\nof the musical data structures supports normal algorithmic\ncomposition in a text editor in addition to the composition\nby conversation use case in a way that is easily integrated\nwith parsing systems like TRIPS.\nAlthough our data structures are robust for representing\ncomplex musical structures, the overall system is currently\na prototype limited to a fairly narrow collection of musical\noperations. This collection of operations needs to be ex-\npanded to feature more tree manipulation algorithms, such\nas different strategies for removing and re-arranging notes\nwithin a larger structure. Operations such as removal of\nindividual notes can have several potential interpretations,\nsuch as replacement with a rest of the same duration and\nremoval followed by time-shifting other elements of the\ndata structure to close the gap. Both operations are valid,\nbut which is most appropriate depends on the larger con-\ntext in which it is used. We are currently working on a\nmore diverse array of operations such as this to support a\nwider range of common score manipulations.\nExpansion of this framework for NLP-based music com-\nposition and manipulation will require adaptation of NLP\ntoolkits like the TRIPS parser. Standard parsers often do\nnot have a suitable dictionary of musically-relevant def-\ninitions to draw on when assigning semantics to nouns\nand verbs. While terms like “transpose” and “reverse” can\nbe interpreted correctly due to their usage in non-musical\nsettings, correct interpretation of other terms is trickier.\nWithout incorporation of music-speciﬁc terminology, the\nSeq N (C,4) (0,0)\nN (E,4) (0,1)\nN (F,4) (0,2)\nN (A,4) (0,3)\nN (C,5) (1,0)\nN (A,4) (1,0.5)\nN (G,4) (1,1)\nN (C,5) (1,2)\nN (G,4) (0,2)\nN (B,4) (1,0)\nN((F, _ ), _ )\nN((C, _ ), (1,0) )\n2\n1\nFigure 7. Data structure representation of the melody in Figure 2 and the two alterations produced by the operations from conversation 1 that yield the\nmelodies in Figures 3 and 4 respectively. Duration information is not referenced directly by any of the selection operations and is therefore omitted from\nnotes in this diagram; N is shorthand for a Note object and constructor.\nSeq N (C,4) (0,0)\nN (E,4) (0,1)\nN (F,4) (0,2)\nN (A,4) (0,3)\nN (C,5) (1,0)\nN (A,4) (1,0.5)\nN (G,4) (1,1)\nN (C,5) (1,2)\n2\n1\nN (A,4) (0,0)\nN (F,4) (0,1)\nN (E,4) (0,2)\nN (C,4) (0,3)\nN (D,4) (1,0)\nN (F,4) (1,0.5)\nN (G,4) (1,1)\nN (D,5) (1,2)\nFigure 8. Data structure representation of the melody in Figure 2 and the two alterations produced by the operations from conversation 2 that yield the\nmelodies in Figures 3 and 4 respectively. Duration information is omitted for the same reasons as in Figure 7.\nTRIPS parser identiﬁes“C” and “F” as temperature scales\nrather than as pitch classes. Just as humans learning about\nmusic must have their vocabulary expanded, we are devel-\noping a music-speciﬁc ontology that can be incorporated in\nthe TRIPS parser to support a more diverse range of musi-\ncal concepts and operations to achieve a correct parse tree\nfor sentences in a composition by conversation setting.\nThe addition of new representations for contexts is impor-\ntant for the creation of an assumer module. This requires\na way to infer the referents of ambiguous words like “it,”\n“this,” and so on as well as keeping track of what spaces or\nmetrics are currently in use.\nOur prototype framework is the beginning of an integrated\napproach for handling natural language and musical con-\ncepts. Currently, MusECI and its integration into NLP sys-\ntems is still a work in progress. However, we hope to even-\ntually achieve real-time interactive programs capable of in-\nteracting with real musicians in a musical setting, similar to\nhow AI on computers and mobile devices is currently able\nto respond to basic spoken commands in other domains.\nWe also aim for this communication to ultimately become\nbi-directional and incorporate other aspects of musical ar-\ntiﬁcial intelligence, perhaps even allowing the machine to\ncritique its human user’s musical ideas or offer its own sug-\ngestions to help complete a musical project.\n8. REFERENCES\n[1] I. V. Serban et al., “Building End-to-end Dialogue Sys-\ntems Using Generative Hierarchical Neural Network\nModels,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 2016, pp. 3776–3783.\n[2] J. Mandler, The foundations of mind: Origins of con-\nceptual thought.\nOxford University Press, 2004.\n[3] R. St.Amant et al., “An Image Schema Language,” in\nProceedings of the International Conference on Cog-\nnitive Modeling, 2006.\n[4] J. Pustejovsky and N. Krishnaswamy, “VoxML: A\nVistualization Modeling Language,” in Language Re-\nsources Evaluation Conference (LREC), 2016.\n[5] R. Boulanger, The Csound Book: Perspectives in Soft-\nware Synthesis, Sound Design, Signal Processing,and\nProgramming.\nMIT Press, 2000.\n[6] J. McCartney, “Rethinking the Computer Music Lan-\nguage:\nSuperCollider,” Computer Music Journal,\nvol. 26, no. 4, pp. 61–68, 2002.\n[7] A. R. Brown and A. Sorensen, “Interacting with Gener-\native Music through Live Coding,” Contemporary Mu-\nsic Review, vol. 28, no. 1, pp. 17–29, 2009.\n[8] Cycling ’74. (2017) Tools for Sound, Graphics, and\nInteractivity. [Online]. Available:\nhttps://cycling74.\ncom/\n[9] H. Nienhuys and J. Nieuwenhuizen, “LilyPond, a sys-\ntem for automated music engraving,” in Proceedings of\nthe Colloquium on Musical Informatics, 2003.\n[10] M. Good, “MusicXML for Notation and Analysis,” in\nThe Virtual Score: Representation, Retrieval, Restora-\ntion. Cambridge, MA: MIT Press, 2001, pp. 113–124.\n[11] C. Wang et al., “MuSQL: A Music Structured Query\nLanguage,” in Proceedings of the International Multi-\nmedia Modeling Conference, 2006, pp. 216–225.\n[12] Music21. [Online]. Available:\nhttp://web.mit.edu/\nmusic21/\n[13] B. Manaris and A. Brown, Making Music with Com-\nputers: Creative Programming in Python.\nNew York,\nNY, USA: Chapman and Hall, 2014.\n[14] A. Brown, Making Music with Java.\nlulu.com, 2009.\n[15] P. Hudak et al. (2017) Euterpea. [Online]. Available:\nhttp://hackage.haskell.org/package/Euterpea\n[16] D. Quick, “Composing with Kulitta,” in Proceedings of\nInternational Computer Music Conference, 2015, pp.\n306–309.\n[17] J. F. Allen, M. Swift, and W. de Beaumont, “Deep Se-\nmantic Analysis for Text Processing,” in Proceedings\nof the Symposium on Semantics in Systems for Text Pro-\ncessing, 2008.\n",
  "categories": [
    "cs.SD",
    "cs.CL",
    "cs.IR",
    "cs.PL",
    "H.5.1; H.5.5; I.2.4; I.2.5; I.2.7"
  ],
  "published": "2017-09-07",
  "updated": "2017-09-07"
}