{
  "id": "http://arxiv.org/abs/1709.04083v2",
  "title": "Pre-training Neural Networks with Human Demonstrations for Deep Reinforcement Learning",
  "authors": [
    "Gabriel V. de la Cruz Jr",
    "Yunshu Du",
    "Matthew E. Taylor"
  ],
  "abstract": "Deep reinforcement learning (deep RL) has achieved superior performance in\ncomplex sequential tasks by using a deep neural network as its function\napproximator and by learning directly from raw images. A drawback of using raw\nimages is that deep RL must learn the state feature representation from the raw\nimages in addition to learning a policy. As a result, deep RL can require a\nprohibitively large amount of training time and data to reach reasonable\nperformance, making it difficult to use deep RL in real-world applications,\nespecially when data is expensive. In this work, we speed up training by\naddressing half of what deep RL is trying to solve --- learning features. Our\napproach is to learn some of the important features by pre-training deep RL\nnetwork's hidden layers via supervised learning using a small set of human\ndemonstrations. We empirically evaluate our approach using deep Q-network (DQN)\nand asynchronous advantage actor-critic (A3C) algorithms on the Atari 2600\ngames of Pong, Freeway, and Beamrider. Our results show that: 1) pre-training\nwith human demonstrations in a supervised learning manner is better at\ndiscovering features relative to pre-training naively in DQN, and 2)\ninitializing a deep RL network with a pre-trained model provides a significant\nimprovement in training time even when pre-training from a small number of\nhuman demonstrations.",
  "text": "Pre-training Neural Networks with Human Demonstrations for\nDeep Reinforcement Learning\nGabriel V. de la Cruz, Jr.\nWashington State University\nPullman, Washington\ngabriel.delacruz@wsu.edu\nYunshu Du\nWashington State University\nPullman, Washington\nyunshu.du@wsu.edu\nMatthew E. Taylor\nWashington State University\nPullman, Washington\ntaylorm@eecs.wsu.edu\nABSTRACT\nDeep reinforcement learning (deep RL) has achieved superior per-\nformance in complex sequential tasks by using a deep neural net-\nwork as its function approximator and by learning directly from\nraw images. A drawback of using raw images is that deep RL must\nlearn the state feature representation from the raw images in ad-\ndition to learning a policy. As a result, deep RL often requires a\nprohibitively large amount of training time and data to reach rea-\nsonable performance, making it inapplicable in real-world settings,\nparticularly when data is expensive. In this work, we speed up\ntraining by addressing half of what deep RL is trying to solve —\nfeature learning. We show that using a small set of non-expert\nhuman demonstrations during a supervised pre-training stage al-\nlows significant improvements in training times. We empirically\nevaluate our approach using the deep Q-network and the asynchro-\nnous advantage actor-critic algorithms in the Atari 2600 games of\nPong, Freeway, and Beamrider. Our results show that pre-training\na deep RL network provides a significant improvement in train-\ning time, even when pre-training from a small number of noisy\ndemonstrations.\nKEYWORDS\nDeep Reinforcement Learning; Deep Learning; Human-Agent In-\nteraction\n1\nINTRODUCTION\nThe recent resurgence of neural networks in reinforcement learning\n(RL) can be attributed to the widespread success of Deep Reinforce-\nment Learning (deep RL), which uses deep neural networks for\nfunction approximation [15, 16]. One of the most impressive ac-\ncomplishments of deep RL is its ability to learn directly from raw\nimages, achieving state-of-the-art results. However, in order to\nbring the success of deep RL from virtual environments to real-\nworld applications, we must address the lengthy training time that\nis required to learn a policy.\nDeep RL suffers from poor initial performance like classic RL\nalgorithms since it learns tabula rasa [19]. In addition, deep RL\ninherently takes longer to learn because besides learning a policy it\nalso learns a state directly from raw images — instead of using\nhand-engineered features, deep RL needs to learn to construct\nrelevant high-level features from raw images. These problems are\nconsequential in real-world applications with expensive data, such\nas in robotics, finance, or medicine.\nLeveraging humans to provide demonstrations is one method to\nspeed up deep RL. Using human demonstrations in RL is not new\n[3] but only recently has this area gained traction as a possible way\nof speeding up deep RL [11, 13, 20].\nSpeeding up deep reinforcement learning can be achieved by ad-\ndressing the two problems it is trying to tackle: 1) feature learning\nand 2) policy learning. In this work, we will focus only on address-\ning the problem of feature learning by pre-training to learn the\nunderlying features in the hidden layers of the network. We show\nthat by learning better features, an RL agent can achieve better\nperformance without changing its policy learning strategies thus\naddressing the importance and usefulness of learning good features\nin an RL problem. We apply a common technique to deep RL that\nis widely used to speed up training in deep learning: pre-training a\nnetwork [8, 9, 24]. However, the success of this technique in deep\nsupervised learning is attributed to the large datasets that are avail-\nable and used to pre-train networks. In deep RL, data are often\nunavailable or difficult to collect.\nIn this work, we propose an approach to speed up deep rein-\nforcement learning algorithms using only a relatively small amount\nof non-expert human demonstrations. This approach starts by\npre-training a deep neural network using human demonstrations\nthrough supervised learning. Similar work has shown that this\nstep would learn to imitate the human demonstrator [3]. However,\nin this context, pre-training through supervised learning would\nimplicitly learn the underlying features.\nWe test our approach with two popular deep RL algorithms, the\nDeep Q-network (DQN) and the Asynchronous Advantage Actor-\nCritic (A3C), and evaluate its performance in the Atari 2600 games\nof Pong, Freeway, and Beamrider [4]. Our results show speed ups\nin five of the six cases. The improvements in Pong and Freeway\nwere quite large in DQN, and A3C’s improvement on Pong was\nespecially large. The generality of this approach means that it can\nbe easily incorporated into multiple deep RL algorithms.\n2\nRELATED WORK\nOur work is not precisely transfer learning, but it is similar to one of\nthe existing transfer learning methods in deep learning. In training\ndeep neural networks for image classification, Yosinski et al. [24]\nhave shown how transferring the features learned from existing\nmodels allow new models to learn faster, particularly when the\ndatasets are similar. In this work, we use a deep learning classifier\nas the source network to initialize the RL agent’s network.\nExisting work on pre-training in RL has shown learning can\nbe improved [1, 2]. However, their networks have a much smaller\nnumber of parameters and state dynamics of the domain are used\nas network input. In our approach, we use the raw images of the\ndomain as network input and also the RL agent needs to learn the\nlatent features while learning its policy.\narXiv:1709.04083v2  [cs.LG]  6 Apr 2019\nOur approach of using supervised learning for pre-training is\nalso similar in spirit to that of Anderson et al. [2]. They pre-train\nby learning to predict the state dynamics. We instead pre-train by\nusing the game’s image frames from the human demo as training\ndata, which are individually labeled by the action taken by the\nhuman demonstrator. This setup is similar to how one could derive\na policy when learning from demonstration [3].\nAnother approach to pre-training is to learn the latent features\nusing unsupervised learning through Deep Belief Networks [1]. Al-\nthough this pre-training approach differs — it falls under a different\nmachine learning paradigm — its goals are similar to our approach\nin that pre-trained networks learn better than randomly initialized\nnetworks.\nThere are more recent works that leverage humans in deep RL.\nChristiano et al. [7] use human feedback to learn a reward func-\ntion. Hester et al. [11] similarly pre-train the network with human\ndemonstrations in DQN. However, their pre-training combines the\nlarge margin supervised loss and the temporal difference loss, which\ntries to closely imitate the demonstrator. Our work differs in that\nonly the cross-entropy loss is used and we focus on the implicitly\nlearned features.\nThe work of Silver et al. [18] trained human demonstrations in\nsupervised learning and used the supervised learner’s network to\ninitialize RL’s policy network. They tested this approach in a single\ndomain and a huge amount of expert demonstration data was used\nto train the supervised learner. Our work will be the first to provide\na comparative analysis as to how this approach impacts deep RL al-\ngorithms and how well this approach can complement existing deep\nRL algorithms when human demonstrations are available. Silver et\nal. [18] also focus on optimizing the policy learned from humans,\nwhile our paper focuses on learning the underlying features. Our\nwork shows that: 1) using only a small set of demonstration data\nis sufficient enough to gain performance improvements, and 2) a\nsupervised learner can still learn important latent features even\nwhen demonstrated human data is from non-experts.\n3\nBACKGROUND: DEEP REINFORCEMENT\nLEARNING\nAn RL problem is typically modeled using a Markov decision pro-\ncess, represented by a 5-tuple ⟨S,A, P,R,γ⟩. An RL agent explores\nan unknown environment by taking an action a ∈A. Each action\nleads the agent to a state s ∈S. A reward r ∼R(s,a,s′) is given\nbased on the action the agent took and the next state s′ it reaches.\nThe goal of an RL agent is to learn to maximize the expected re-\nturn value Rt = Í∞\nk=0 γ krt+k for each state at time t. The discount\nfactor γ ∈(0, 1] determines the relative importance of future and\nimmediate rewards.\n3.1\nDeep Q-network\nThe first successful deep RL method, deep Q-network (DQN), learns\nto play 49 Atari games directly from screen pixels by combin-\ning Q-learning with a deep convolutional neural network [16]. In\nshallow Q-learning, an agent learns a state-action value function\nQ∗(s,a) = Es′[r + γ maxa′ Q∗(s′,a′)|s,a], which is the expected\ndiscounted reward determined by performing action a in state s\nand thereafter performing optimally [23]. The optimal policy π∗\ncan be deduced by following actions that have the maximum Q\nvalue, π∗= arдmaxaQ∗(s,a).\nDirectly computing the Q value is not feasible when the state\nspace is large or continuous (e.g., in Atari games). The DQN algo-\nrithm uses a convolutional neural network as a function approxima-\ntor to estimate the Q function Q(s,a;θ) ≈Q∗(s,a), where θ is the\nnetwork’s weight parameters. For each iteration i, DQN is trained\nto minimize the mean-squared error (MSE) between the Q-network\nand its target y = r + γmaxa′Q(s′,a′;θ−\ni ), where θ−\ni is the weight\nparameters for the target network that was generated from previ-\nous iterations. All rewards are clipped to 1 when positive, -1 when\nnegative, and 0 when unchanged. The loss function at iteration\ni can be expressed as Li(θi) = Es,a,r,s′[(y −Q(s,a;θi))2], where\n{s,a,r,s′} are state-action samples drawn from experience replay\nmemory with a minibatch of size 32. The use of a target network,\nreward clipping, and an experience replay memory are essential to\nstabilize learning. In addition, the ϵ-greedy policy is used by the\nagent to obtain sufficient exploration of the state space.\n3.2\nAsynchronous Advantage Actor-critic\nThere are a few drawbacks of using experience replay memory in\nthe DQN algorithm. First, storing all experiences is space-consuming\nand could slow down learning. Second, using replay memory limits\nDQN to off-policy algorithms. The asynchronous advantage actor-\ncritic (A3C) algorithm was proposed to overcome these problems\n[15].\nA3C combines the actor-critic algorithm with deep RL. It dif-\nfers from value-based algorithms (e.g., Q-learning) where only a\nvalue function is learned — an actor-critic algorithm is policy-based\nand maintains both a policy function π(at |st ;θ) and a value func-\ntion V (st ;θv) [19]. The policy function is called the actor, which\ntakes actions based on the current policy π. The value function is\ncalled the critic, which serves as a baseline to evaluate the qual-\nity of the action by returning the state value V π (st ;θt ) for the\ncurrent state under policy π. The policy is directly parameter-\nized and improved via policy-gradient. To reduce the variance in\npolicy gradient, an advantage function is used and calculated as\nA(at,st ;θ,θv) = Ík−1\ni=0 γ itt+i + γ kV (st+k;θv) −V (st ;θv) at time\nstep t for action at at state st , where k is upper-bounded by n, the\nnumber of steps used for n-step return update. The loss function\nfor A3C is L(θ) = ∇θ log π(at |st ;θ)A(at,st ;θ,θv).\nIn A3C, k actor-learners are running in parallel with their own\ncopies of the environment and the parameters for the policy and\nvalue function. This enables exploration of different parts of the\nenvironment and therefore observations will not be correlated.\nThis mimics the function of experience replay memory in DQN\nwhile being more efficient in space and training time. Each actor-\nlearner performs a parameter update every tmax actions, or when\na terminal state is reached — this is similar to using mini-batches,\nas is done in DQN. Updates are synchronized to a master learner\nthat maintains a central policy and value function, which will be\nthe final policy upon the completion of training.\n4\nPRE-TRAINING NETWORKS FOR DEEP RL\nDeep reinforcement learning can be divided into two sub-tasks:\nfeature learning and policy learning. Deep RL in itself has already\nsucceeded in learning both tasks simultaneously. However, learning\nboth tasks also makes learning in deep RL very slow. We believe that\nby addressing the feature learning task, deep RL agents can better\nfocus on learning the policy. We learn the features by pre-training\nthe network using human demonstrations from non-experts. We\nassume here that humans provide correct labels through actions\ndemonstrated while playing the game. We will refer to our approach\nas the pre-trained model.\nWe first apply the pre-trained model approach in DQN and refer\nto it as the pre-trained model for DQN (PMfDQN). In PMfDQN,\nwe train a multiclass-classification deep neural network with a\nsoftmax cross entropy loss function. The loss is minimized using\nthe Adam optimizer [12] with the following hyperparameters: step\nsize α = 0.0001, stability constant ϵ = 0.001, and Tensorflow’s\ndefault exponential decay rates β. The network architecture for\nthe classification follows the same structure of the hidden layers\nof DQN, which has three convolutional layers (conv1–conv3) and\none fully connected layer (fc1) [16]. The classifier’s output layer\nhas a single output for each valid action and is trained using the\ncross-entropy loss instead of the TD loss. The learned weights and\nbiases from the classification model’s hidden layers are used to\ninitialize the DQN network, instead of random initialization. When\nusing all layers of the pre-trained model (including the output layer),\nnormalization of the parameters of the output layer was necessary\nto achieve a positive result. To normalize the output layer, we keep\ntrack of the maximum value of the output layer during training,\nwhich is used as a divisor to all the weights and biases during\ninitialization with the pre-trained model. Without normalization,\nthe values of the output layer tend to explode. We also load the\nhuman demonstrations in the replay memory, thus removing the\nneed for DQN to take uniform random actions for 50,000 frames to\ninitially populate the replay memory [16].\nThe pre-trained model method can also be applied in A3C, which\nwe will refer to as the pre-trained model for A3C (PMfA3C). In\nPMfA3C, we pre-train the multiclass-classifier using the same hy-\nperparameters and optimization method as mentioned in PMfDQN\nwhile experimenting with two different network structures. The\nfirst network uses a structure with three convolutional layers (conv1–\nconv3) and one fully connected layer (fc1), but without the long\nshort-term memory (LSTM) cells [17]. The output layer is the\nsame as in PMfDQN. The second network is inspired by one-vs.-\nall multiclass-classification and multitask learning [6]. It differs\nfrom the first network as it uses multiple heads of output layers\nwhere each class or action has its own output layer. Each individ-\nual output layer becomes a one-vs.-all classification. During each\ntraining iteration, a uniform probability distribution is used to se-\nlect which output layer to train. In each iteration, gradients are\nbackpropagated to the shared hidden layers of the network. In both\nmulticlass networks, only the hidden layers are used to initialize\nA3C’s network.\nSince DQN uses experience replay memory [14], it is also possi-\nble to pre-train just by loading the human demonstrations in the\nreplay memory. We refer to this experiment as pre-training in DQN\n(PiDQN). While somewhat naive, this is still an interesting method\nas it allows the DQN agent to learn both the features and policy\nwithout any interaction with the actual Atari environment. How-\never, this pre-training method does not generalize to A3C and/or\nother deep RL algorithms that do not use a replay memory. We\nwould like to address this in future work by applying this naive ap-\nproach to an alternative version of A3C that uses a replay memory\n[21].\nLastly, we conducted additional experiments in DQN that com-\nbines PMfDQN and PiDQN, with the goal of exploring whether a\ncombined approach would achieve a greater performance in DQN.\n5\nEXPERIMENTAL DESIGN\nWe test our approach in three Atari games: Pong, Freeway, and\nBeamrider, as shown in Figure 1. The games have 6, 3, and 9 actions,\nrespectively. We use OpenAI Gym’s deterministic version of the\nAtari 2600 environment with an action repeat of four [5].\nFigure 1: Atari 2600 game screenshot of Pong, Freeway and\nBeamrider, from left to right.\nWe use the same network architecture and hyperparameters for\nDQN as was done in the original work [16]. For the LSTM-variant\nof A3C, we follow the work of Sharma et al. [17] as their work\nclosely replicates the results of the original A3C algorithm [15].\nHowever, note that there are two key differences from the original\nA3C work. First, while using the same network architecture with\nthree convolutional layers, the fully connected layer was modified\nto have 256 units (instead of 512) to connect with the 256 LSTM\ncells that followed. Second, we use tmax = 20 instead of tmax = 5.\nWe use 16 actor-learner threads for all A3C experiments.\nIn both DQN and A3C, we use the four most recent game frames\nas input to the network where each frame is pre-processed. We also\nuse the same evaluation technique for both DQN and A3C such that\nthe average reward over 125,000 steps was recorded. In addition,\nDQN is evaluated using a ϵ-greedy action selection method, where\nϵ = .05. In A3C, it is evaluated as a stochastic policy where it uses\nthe output policy as action probabilities.\n5.1\nCollection of Human Demonstration\nWe are using OpenAI Gym’s keyboard interface to allow a human\ndemonstrator to interact with the Atari environment. The demon-\nstrator is provided with game rules and a set of valid actions with\ntheir corresponding keyboard keys for each game. The action repeat\nis set to one to provide smoother transitions of the games during\nhuman play, whereas the action repeat is set to four during training\n[15, 16]. During the demonstration, we collect every fourth frame\nof the game, saving the game state using the game’s image, action\ntaken, reward received, and if the game’s current state is a terminal\nTable 1: Summary of pre-training experiments.\nMethod\nSummary\nPiDQN\npre-train in DQN for 150,000 iterations, batch size of 32\nPMfDQN\ninitialize DQN with pre-trained model, pre-train for 150,000 iterations, batch size of 32\nPMfDQN+PiDQN\ninitialize DQN with pre-trained model and continue to pre-train in DQN\nPMfDQN+PiDQN (ϵ = 0.1)\nlow initial exploration rate\nPMfDQN (random demo)\npre-train model with random demonstrations\nPMfDQN (no fc2)\ninitialize with pre-trained model excluding output layer\nPMfA3C\ninitialize A3C with pre-trained model, pre-train for 150,000 iterations, batch size 32\nPMfA3C (1-vs-all)\npre-train model using one-vs-all multi-class classification, longer pre-training\nPMfA3C (1-vs-all, 1-demo)\npre-train model using only one out of the five demonstrated game play\nTable 2: Human demonstration over five plays per game.\nGame\nWorst Score\nBest Score\n# of Frames\nBeamrider\n2,160\n3,406\n11,205\nFreeway\n28\n31\n10,241\nPong\n-10\n5\n11,265\nstate. The format of the stored data follows the structure of the\nexperience replay memory used in DQN.\nThe non-expert human demonstrator plays five rounds for each\ngame. Each round has a maximum of five minutes of playing time.\nThe demonstration ends when the game reaches the time limit or\nwhen game terminates — whichever comes first. Table 2 provides\na breakdown of human demonstration size for each game and the\nhuman performance level.\n6\nRESULTS\nThis section presents and discusses results from pre-training deep\nRL’s network for DQN and A3C.\n6.1\nDQN\nUsing PMfDQN, we trained one multiclass-classification network\nfor each Atari game with the human demonstration dataset. Each\ntraining run was conducted using a batch size of 32 for 150,000 train-\ning iterations. The number of training iterations was determined\nto be the shortest number of iterations where the training loss for\nall games converges approximately to zero. The trained classifiers\nprovided us the pre-trained models which were then used to initial-\nize DQN’s network weights and biases. Figure 2 shows the results\nthat PMfDQN speeds up training in all three Atari games. We also\ntested PiDQN with the same number of pre-training iterations as\nin PMfDQN. PiDQN for Beamrider has shown some performance\nimprovement with an average total reward of 5,120 compared to\nDQN’s average total reward of 4,894. However, PiDQN in Pong\nand Freeway either follows a similar learning trajectory as DQN or\nslightly worse. Our experiments show although naive pre-training\ncould provide speedups under some cases, supervised pre-training\nis the essential component when using demonstrations.\nTo see if we can further improve DQN through pre-training,\nwe used PMfDQN followed by PiDQN with 150,000 pre-training\niterations each. Figure 2 shows the result for the combined method\n(PMfDQN+PiDQN). This method showed a slight improvement\nin playing Freeway but less improvement in Pong and Beamrider\nwhen compared to only using PMfDQN. We found these results\nsurprising since we hypothesized that more improvement should\nbe expected with more pre-training. One possible reason for this\ncould be due to the high initial exploration rate ϵ = 1 in DQN at the\nbeginning of training. Under this setting, the agent would be taking\nentirely random actions until the value of ϵ has decayed to a much\nlower exploration rate. In the original DQN, ϵ is decayed over one\nmillion steps, resulting in a replay memory with a good amount of\nexperiences executed through random action [16]. This may have\nan adverse effect on what has already been learned from the pre-\ntraining steps. Therefore, we instead initialized ϵ = 0.1 when using\nthe PMfDQN+PiDQN combined method. Results for the combined\nmethod as shown in Figure 2 revealed that combining PMfDQN\nwith PiDQN using a low initial exploration rate was comparable\nto PMfDQN by itself and was even better for Freeway. DQN’s\nhigh initial exploration rate can be detrimental in costly real-world\napplications. By using a small amount of human demonstration,\nwe can now minimize exploration without affecting agent’s overall\nlearning performance.\nTo measure improvement for each pre-training method, we com-\nputed the average total reward for each trial and compared each\npre-training method against the DQN baseline. In PMfDQN, the av-\nerage total reward was higher than the baseline in all three games,\nalthough it was only statistically significant for Pong and Free-\nway. However, the improvements in both PMfDQN+PiDQN and\nPMfDQN+PiDQN (ϵ = 0.1) were statistically significant in all three\ngames as indicated by t-test with p < .05.\n6.1.1\nAblation Studies. We consider two modifications to PMfDQN\nto further analyze its performance. In our first ablation study, we\nreplaced human demonstrations with random demonstrations. We\nwere interested in knowing how important it is to use human\ndemonstrations in comparison with using a random agent. We con-\nducted this experiment in Pong and the results in Figure 3 showed\nthat pre-training with random demonstrations was worse than the\nDQN baseline. This experiment indicated that there was a need\nfor some level of competency from the demonstrator in order to\nextract useful features during pre-training. However, even with\nworse results, our approach appears to still converge to a policy\nsimilar to baseline DQN.\n0\n4\n8\n12\n16\n20\nEpoch\n20\n10\n0\n10\n20\nReward\nPong\nDQN\nPiDQN\nPMfDQN\nPMfDQN+PiDQN\nPMfDQN+PiDQN ( = 0.1)\n0\n4\n8\n12\n16\n20\nEpoch\n0\n5\n10\n15\n20\n25\n30\nReward\nFreeway\nDQN\nPiDQN\nPMfDQN\nPMfDQN+PiDQN\nPMfDQN+PiDQN ( = 0.1)\n0\n4\n8\n12\n16\n20\nEpoch\n0\n2000\n4000\n6000\n8000\nReward\nBeamrider\nDQN\nPiDQN\nPMfDQN\nPMfDQN+PiDQN\nPMfDQN+PiDQN ( = 0.1)\nFigure 2: Performance evaluation of the ablation studies for\nPong using DQN. The results are the average testing score\nover four trials where the shaded regions correspond to the\nstandard deviation.\n0\n4\n8\n12\n16\n20\nEpoch\n20\n10\n0\n10\n20\nReward\nPong\nDQN\nPMfDQN\nPMfDQN (random demo)\nPMfDQN (no fc2)\nFigure 3: Performance evaluation on the ablation studies for\nPong using DQN. The results are the average testing score\nover four trials where the shaded regions correspond to the\nstandard deviation.\nIn our second ablation study, we excluded the second fully con-\nnected layer (fc2) (i.e., the output layer) when initializing the DQN\nnetwork with the pre-trained model. This will allow us to know\nif supervised learning does learn important features, particularly\nin the hidden layers. Empirically, when excluding the output layer,\nresults in Figure 3 showed that even though the initial jumpstart\nwas lost, the training time to reach convergence is not different\nfrom the time when using all layers. This indicated that it was\nactually the features in the hidden layers that provided most of\nthe improvement in the training speed. This was not surprising\nsince the output layer of a classifier was trying to learn to predict\nwhat action to take given a state without any consideration for\nmaximizing the reward. Additionally, when learning from only a\nsmall amount of data where human performance was relatively\npoor (Table 2), the classifier’s policy would be far from optimal.\n6.2\nA3C\nUsing PMfA3C, we also pre-trained multiclass-classification net-\nworks for each Atari game with human demonstrations, similar to\nwhat was done in PMfDQN with a batch of 32 for 150,000 train-\ning iterations. Since the network for the LSTM-variant of A3C\nused LSTM cells with two output layers, we only initialize A3C’s\nnetwork with the pre-trained model’s hidden layers. In Figure 4,\nresults showed improvements in the training time in both Pong\nand Beamrider, with a much higher improvement in Pong.\nHowever, there was no improvement in Freeway. This was within\nour expectation since the baseline performance of Freeway was\npoor in the original A3C work [15] (shown in Figure 4 baseline).\nWe would like to emphasize that our approach focused on learning\nfeatures without addressing improvements in policy — no improve-\nments in Freeway with our approach were expected. Freeway in\nA3C needs a better way of exploring states in order to learn a\n0\n5\n10\n15\n20\n25\nSteps (in millions)\n20\n10\n0\n10\n20\n30\nReward\nPong\nA3C\nPMfA3C\nPMfA3C (1-vs-all)\nPMfA3C (1-vs-all, 1-demo)\n0\n10\n20\n30\n40\n50\nSteps (in millions)\n0.1\n0.0\n0.1\n0.2\n0.3\nReward\nFreeway\nA3C\nPMfA3C\n0\n20\n40\n60\n80\n100\nSteps (in millions)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nReward\nBeamrider\nA3C\nPMfA3C\nPMfA3C (1-vs-all)\nPMfA3C (1-vs-all, 1-demo)\nFigure 4: Performance of baseline and pre-training using\nA3C. The x-axis is the number of training steps which is also\nthe number of visited game frames among all parallel work-\ners with frame skip. The y-axis is the average testing score\nover four trials where the shaded regions correspond to the\nstandard deviation. Note that all PMfA3C experiments do\nnot use the output layer from the pre-trained model.\nTable 3: Evaluation of the similarity of features for each hid-\nden layer. The mean squared error (MSE) is computed be-\ntween the weights from a randomly initialized A3C network\n(baseline) and the final weights. Similarly, when using a pre-\ntrained model as the initial weights.\nLayer\nMSE (Pong)\nMSE (Beamrider)\nBaseline\nPre-train\nBaseline\nPre-train\nconv1\n1.03 × 10−2\n3.94 × 10−3\n3.32 × 10−2\n2.53 × 10−2\nconv2\n8.02 × 10−3\n8.00 × 10−4\n8.50 × 10−3\n4.35 × 10−3\nconv3\n7.13 × 10−3\n3.26 × 10−4\n7.11 × 10−3\n2.39 × 10−3\nfc1\n9.57 × 10−4\n7.54 × 10−5\n1.07 × 10−3\n3.29 × 10−4\nnear-optimal policy for the game. This is something we will try to\naddress in future work.\nWith strong improvements observed in A3C, can we still gain\nfurther improvements if we pre-train our classification network\nlonger? We then tried longer training using the one-vs.-all multiclass-\nclassification network with shared hidden layers. Since each class\nor action was trained independently, we can now observe the dif-\nferent convergence of the training loss for each class. This allowed\nus to use the same technique of training until the training loss for\nall classes was approximately zero. Using the one-vs.-all classifi-\ncation, we pre-trained for 450,000 iterations in Pong and 650,000\niterations in Beamrider. Training longer resulted in a very large\nimprovement in pong and a slight improvement for Beamrider, as\nshown in Figure 4.\nThe last experiment we conducted was to test whether important\nfeatures could still be learned even with a much smaller number of\ndemonstrations, in this case, a single round of the game that was\nonly five minutes of demonstration. We used one-vs.-all classifica-\ntion network to pre-train for Pong with only 2,253 game frames\nwith 250,000 training iterations and similarly for Beamrider with\n2,232 game frames with 300,000 training iterations. In Figure 4,\nresults for both Pong and Beamrider show improvement with only\nthis small number of demonstrations. It was even more remarkable\nin Beamrider, as results were as good as pre-training with the full\nset of the human demonstrations.\n6.3\nAdditional Analysis\nIn order to understand what was accomplished with pre-training,\nwe looked closer at the weights of the network layers (i.e., filters) to\ndetermine how much pre-trained features contributed to the final\nfeatures learned. Thus, we further investigated how similar the\ninitial weights ˆθ of a deep RL network were to its final weights θ\nfor each layer after learning a near-optimal policy. We can quantify\nthe similarity by finding the difference between the weights using\nthe mean squared error MSE =\n1\nn\nÍn\ni=1( ˆθi −θi)2. Smaller MSE\nindicates a higher similarity between layers. Table 3 shows that\nthere was a higher similarity in the pre-training approach compared\nto random weight initialization. Furthermore, we looked at the\nvisualization of each hidden layer and observed that the weights\nlearned from classification and used as initial values in deep RL’s\nnetwork provided features that were retained even after training\nin deep RL as shown in Figure 5.\npre-trained conv1 layer\nﬁnal conv1 layer\nﬁnal\npre-trained\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n-0.2\n-0.4\n-0.6\nFigure 5: Visualization of the normalized weights on Pong’s first convolutional layer using PMfA3C. The weights (filters) are\nfrom a pre-trained classification network trained for 150,000 iterations (left image), and from the final weights after 50 million\ntraining steps in A3C (right image). To better illustrate the similarity of the weights, we provided two zoomed-in images of a\nparticular filter from pre-trained conv1 (green box) and final conv1 (blue box).\n7\nDISCUSSION AND CONCLUSION\nThe pre-training approach worked very well in Pong. This suc-\ncess can be explained by the human demonstration data the classi-\nfier was pre-trained with, and the simplicity of Pong environment.\nPong’s states are highly repetitive when compared to the other\ngame environments that are more dynamic. The Beamrider has the\nmost complex environments among all three games because it has\ndifferent levels of varying difficulty. Although Freeway’s game state\nis also repetitive, A3C’s inability to learn a good policy is a problem\nthat leans more towards policy learning, which is not addressed in\nour approach.\nHuman demonstrations are an essential part of the success of\nour approach. It is important to understand how the demonstra-\ntor’s performance and the amount of demonstration data affect the\nbenefits of pre-training in future work. Future work will consider\nusing recently released human demonstration datasets for Atari\n[13] and Starcraft II [20].\nAnother issue that needs to be addressed in regards to the human\ndemonstrations is that they suffer from highly imbalanced classes\n(actions). This is attributed to: 1) sparsity of some actions (e.g., the\ntorpedo action in Beamrider is limited to three uses at each level),\n2) actions that are closely related (e.g., in Beamrider, there is a left\nand right action plus combined actions of left-fire and right-fire —\na demonstrator would usually just use the native actions of left and\nright action alone and use the fire action by itself), and 3) games\nhaving a default no-operation action.\nIn a previous study, the authors show that the classifier will learn\na policy that tends to bias towards the majority classes when the\nimbalance problem is not addressed [10]. It is interesting that the\nclassifier is still able to learn important features without handling\nthis issue. We see this as an interesting future work and hope\nto explore if better features can be learned by handling the class\nimbalance problem, therefore, leads to further improvements.\nAs we investigate further ways to improve our approach, we\nknow there is a limit to how much improvement pre-training can\nprovide without addressing policy learning. In our approach, we\nhave already trained a model with a policy that tries to imitate the\nhuman demonstrator thus we can extend this work by using the\npre-trained model’s policy to provide advice to the agent (e.g., [22]).\nOverall, learning a policy directly from raw images through deep\nneural networks is a major factor why learning is slow in deep RL.\nThis paper has demonstrated that our method of initializing deep\nRL’s network with a pre-trained model can significantly speed up\nlearning in deep RL.\nACKNOWLEDGMENTS\nThe A3C implementation was a modification of https://github.com/\nmiyosuda/async_deep_reinforce. The authors would like to thank\nSahil Sharma and Kory Matthewson for providing very useful in-\nsights on the actor-critic method. We also thank NVidia for donating\na graphics card uses in these experiments.\nREFERENCES\n[1] Farnaz Abtahi and Ian Fasel. 2011. Deep belief nets as function approximators\nfor reinforcement learning. Restricted Boltzmann Machine (RBM) 2 (2011), h3.\n[2] Charles W Anderson, Minwoo Lee, and Daniel L Elliott. 2015. Faster reinforce-\nment learning after pretraining deep networks to predict state dynamics. In\nNeural Networks (IJCNN), 2015 International Joint Conference on. IEEE, 1–7.\n[3] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. 2009. A\nsurvey of robot learning from demonstration. Robotics and autonomous systems\n57, 5 (2009), 469–483.\n[4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. 2013. The Arcade Learning\nEnvironment: An Evaluation Platform for General Agents. Journal of Artificial\nIntelligence Research 47 (jun 2013), 253–279.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John\nSchulman, Jie Tang, and Wojciech Zaremba. 2016.\nOpenAI Gym.\n(2016).\narXiv:arXiv:1606.01540\n[6] Rich Caruana. 1998. Multitask Learning. In Learning to learn. Springer, 95–133.\n[7] Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, and Dario\nAmodei. 2017. Deep reinforcement learning from human preferences. arXiv\npreprint arXiv:1706.03741 (2017).\n[8] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pas-\ncal Vincent, and Samy Bengio. 2010. Why Does Unsupervised Pre-training\nHelp Deep Learning? J. Mach. Learn. Res. 11 (March 2010), 625–660. http:\n//dl.acm.org/citation.cfm?id=1756006.1756025\n[9] Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, and\nPascal Vincent. 2009. The Difficulty of Training Deep Architectures and the\nEffect of Unsupervised Pre-Training. In Twelfth International Conference on Ar-\ntificial Intelligence and Statistics (AISTATS). 153–160. http://jmlr.csail.mit.edu/\nproceedings/papers/v5/erhan09a/erhan09a.pdf\n[10] Haibo He and Edwardo A Garcia. 2009. Learning from imbalanced data. IEEE\nTransactions on knowledge and data engineering 21, 9 (2009), 1263–1284.\n[11] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot,\nAndrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z.\nLeibo, and Audrunas Gruslys. 2018. Deep Q-learning from Demonstrations. In\nProceedings of the 32nd AAAI Conference on Artificial Intelligence.\n[12] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza-\ntion. arXiv preprint arXiv:1412.6980 (2014).\n[13] Vitaly Kurin, Sebastian Nowozin, Katja Hofmann, Lucas Beyer, and Bastian Leibe.\n2017. The Atari Grand Challenge Dataset. arXiv preprint arXiv:1705.10998 (2017).\n[14] Long-Ji Lin. 1992. Self-improving reactive agents based on reinforcement learning,\nplanning and teaching. Machine learning 8, 3-4 (1992), 293–321.\n[15] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timo-\nthy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchro-\nnous methods for deep reinforcement learning. In International Conference on\nMachine Learning. 1928–1937.\n[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. 2015. Human-level control through deep reinforcement learning.\nNature 518, 7540 (2015), 529–533.\n[17] Sahil Sharma, Aravind S Lakshminarayanan, and Balaraman Ravindran. 2017.\nLearning to repeat: Fine grained action repetition for deep reinforcement learning.\narXiv preprint arXiv:1702.06054 (2017).\n[18] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George\nVan Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural\nnetworks and tree search. Nature 529, 7587 (2016), 484–489.\n[19] Richard S Sutton and Andrew G Barto. 1998. Reinforcement learning: An intro-\nduction. Vol. 1. MIT press Cambridge.\n[20] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha\nVezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou,\nJulian Schrittwieser, et al. 2017. StarCraft II: A New Challenge for Reinforcement\nLearning. arXiv preprint arXiv:1708.04782 (2017).\n[21] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray\nKavukcuoglu, and Nando de Freitas. 2016. Sample efficient actor-critic with\nexperience replay. arXiv preprint arXiv:1611.01224 (2016).\n[22] Zhaodong Wang and Matthew E. Taylor. 2017. Improving Reinforcement Learning\nwith Confidence-Based Demonstrations. In Proceedings of the 26th International\nConference on Artificial Intelligence (IJCAI).\n[23] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine Learning\n8, 3-4 (1992), 279–292.\n[24] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transfer-\nable are features in deep neural networks?. In Advances in neural information\nprocessing systems. 3320–3328.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2017-09-12",
  "updated": "2019-04-06"
}