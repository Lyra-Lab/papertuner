{
  "id": "http://arxiv.org/abs/1709.05262v2",
  "title": "Supervising Unsupervised Learning",
  "authors": [
    "Vikas K. Garg",
    "Adam Kalai"
  ],
  "abstract": "We introduce a framework to leverage knowledge acquired from a repository of\n(heterogeneous) supervised datasets to new unsupervised datasets. Our\nperspective avoids the subjectivity inherent in unsupervised learning by\nreducing it to supervised learning, and provides a principled way to evaluate\nunsupervised algorithms. We demonstrate the versatility of our framework via\nsimple agnostic bounds on unsupervised problems. In the context of clustering,\nour approach helps choose the number of clusters and the clustering algorithm,\nremove the outliers, and provably circumvent the Kleinberg's impossibility\nresult. Experimental results across hundreds of problems demonstrate improved\nperformance on unsupervised data with simple algorithms, despite the fact that\nour problems come from heterogeneous domains. Additionally, our framework lets\nus leverage deep networks to learn common features from many such small\ndatasets, and perform zero shot learning.",
  "text": "Supervising Unsupervised Learning\nVikas K. Garg (MIT) 1 Adam T. Kalai (Microsoft Research) 2\nAbstract\nWe introduce a framework to leverage knowledge\nacquired from a repository of (heterogeneous) su-\npervised datasets to new unsupervised datasets.\nOur perspective avoids the subjectivity inherent in\nunsupervised learning by reducing it to supervised\nlearning, and provides a principled way to evalu-\nate unsupervised algorithms. We demonstrate the\nversatility of our framework via simple agnostic\nbounds on unsupervised problems. In the con-\ntext of clustering, our approach helps choose the\nnumber of clusters and the clustering algorithm,\nremove the outliers, and provably circumvent the\nKleinberg’s impossibility result. Experimental re-\nsults across hundreds of problems demonstrate\nimproved performance on unsupervised data with\nsimple algorithms, despite the fact that our prob-\nlems come from heterogeneous domains. Addi-\ntionally, our framework lets us leverage deep net-\nworks to learn common features from many such\nsmall datasets, and perform zero shot learning.\n1. Introduction\nUnsupervised Learning (UL) is an elusive branch of Ma-\nchine Learning (ML), including problems such as cluster-\ning and manifold learning, that seeks to identify structure\namong unlabeled data. UL is notoriously hard to evaluate\nand inherently undeﬁnable. To make this point as simply\nas possible, consider clustering the points on the line in\nFigure 1 (left). One can easily justify 2, 3, or 4 clusters.\nAs (Kleinberg, 2003) argues, it is impossible to give an ax-\niomatically consistent deﬁnition of the “right” clustering.\nHowever, now suppose that one can access a bank of prior\nclustering problems, drawn from the same distribution as\nthe current problem at hand, but for which ground-truth\nlabels are available. In this example, evidence may favor\ntwo clusters since the unlabeled data closely resembles two\nof the three 1-dimensional clustering problems, and all the\n1CSAIL, MIT\n2Microsoft Research.\nCorrespondence\nto:\nVikas K. Garg <vgarg@csail.mit.edu>, Adam T. Kalai\n<adum@microsoft.com>.\nclusterings share the common property of roughly equal size\nclusters. Given sufﬁciently many problems in high dimen-\nsions, one can learn to extract features of the data common\nacross problems to improve clustering.\nClustering problem\nin isolation:\n2˚\n15˚\n1˚\n14˚\n0 db\n3 db\n34˚\n65˚\nClustering repository:\nHow many clusters?\nFigure 1. In isolation (left), there is no basis to choose a clustering –\neven points on a line could be clustered in 2-4 clusters. A repository\nof clustering problems with ground-truth labels (right) can inform\nthe choice amongst clusterings or even offer common features for\nricher data.\nWe model UL problems as representative samples from a\nmeta-distribution, and offer a solution using an annotated\ncollection of prior datasets. Speciﬁcally, we propose a meta-\nunsupervised-learning (MUL) framework that, by consider-\ning a distribution over unsupervised problems, reduces UL\nto Supervised Learning (SL). Going beyond transfer learn-\ning, semi-supervised learning, and domain adaptation (Pan\n& Yang, 2010; Kingma et al. , 2014; Siddharth et al. , 2017;\nRohrbach et al. , 2013; Ganin & Lempitsky, 2015; Long\net al. , 2016; Bousmalis et al. , 2016) where problems have\nthe same dimensionality or at least the same type of data\n(text, images, etc.), our framework can be used to improve\nUL performance for problems of different representations\nand from different domains.\nEmpirically, we train meta-algorithms on the repository of\nclassiﬁcation problems from openml.org which has a\nvariety of datasets spanning domains such as NLP, com-\nputer vision, and bioinformatics, among others. Ignoring\nlabels, each dataset can be viewed as a UL problem. We\ntake a data-driven approach to quantitatively evaluate and\ndesign new UL algorithms, and merge classic ideas like un-\nsupervised learning and domain adaptation in a simple way.\nThis enables us to make UL problems, such as clustering,\nwell-deﬁned.\narXiv:1709.05262v2  [cs.AI]  16 Feb 2018\nSupervising Unsupervised Learning\nOur model requires a loss function measuring the quality\nof a solution (e.g., a clustering) with respect to some con-\nceptual ground truth. Note that we require the ground truth\nlabels for a training repository but not for test data (where\nthey may be impossible to acquire). We suppose that we\nhave a repository of datasets, annotated with ground truth\nlabels, that is drawn from a meta-distribution µ over prob-\nlems, and that the given data X was drawn from this same\ndistribution (though without labels). From this collection,\none could, at a minimum, learn which clustering algorithm\nworks best, or, even better, which type of algorithm works\nbest for which type of data. The same meta-approach could\nhelp in selecting how many clusters to have, which outliers\nto remove, and so forth.\nOur theoretical model is a meta-application of Agnostic\nLearning (Kearns et al. , 1994) where we treat each entire\nlabeled problem analogous to a training example in a su-\npervised learning task. We show how one can provably\nlearn to perform UL as well as the best algorithm in certain\nclasses of algorithms. Our work also relates to supervised\nlearning questions of meta-learning, sometimes referred to\nas auto-ml (e.g. Thornton et al. , 2013; Feurer et al. , 2015),\nlearning to learn (e.g. Thrun & Pratt, 2012), Bayesian op-\ntimization (e.g. Snoek et al. , 2012) and lifelong learning\n(e.g. Thrun & Mitchell, 1995; Balcan et al. , 2015). In the\ncase of supervised learning, where accuracy is easy to eval-\nuate, meta-learning enables algorithms to achieve accuracy\nmore quickly with less data. We argue that for unsupervised\nlearning, the meta approach offers a principled means of\ndeﬁning and evaluating unsupervised learning.\nOur main contributions are as follows. First, we show how\nto adapt knowledge acquired from a repository of small\ndatasets that come from different domains, to new unsu-\npervised datasets. Our MUL framework removes the sub-\njectivity due to rules of thumb or educated guesses, and\nprovides an objective evaluation methodology for UL. We\ngive algorithms for various problems including making an\ninformed decision on the number of clusters, learning com-\nmon features, choosing amongst clustering algorithms, and\nremoving outliers in a principled way.\nNext, the debate on what characterizes right clustering\nhas not made much headway since Kleinberg’s impossi-\nbility framework (Kleinberg, 2003) mainly owing to the\nsubjectivity or “artifacts of speciﬁc formalisms” (Zadeh\n& Ben-David, 2009; Ackerman & Ben-David, 2008). We\nadd a fresh perspective to this debate by designing a meta-\nclustering algorithm that circumvents the effect of these\nartifacts by learning across clustering problems, and thus\nmakes good clustering possible.\nFinally, in a completely different direction, we use deep\nlearning to automate learning of features across problems\nfrom different domains and of very different natures. We\nshow that these seemingly unrelated problems can be lever-\naged to improve average performance across UL datasets.\nIn this way, we effectively unite many heterogeneous “small\ndata” into sufﬁcient “big data” to beneﬁt from a single neu-\nral network. Moreover, recently, learning for resource con-\nstrained devices (Gupta et al. , 2017) has attracted consider-\nable attention, and several compression methods have been\nsuggested speciﬁcally for deep nets, see e.g. (Chen et al. ,\n2015; Han et al. , 2016; Luo et al. , 2017). However, these\nmethods assume homogenous data, and therefore maintain-\ning a compressed deep net separately for each dataset would\nbe prohibitive due to storage constrains. Our approach, in\nprinciple, may be combined with compression methods to\nprovide a practical solution for these devices.\n2. Learning Preliminaries\nWe ﬁrst deﬁne agnostic learning, in general, and then deﬁne\nmeta-learning as a special case where the examples are\nproblems themselves. A learning task consists of a universe\nX, labels Y and a bounded loss function ℓ: Y × Y →[0, 1],\nwhere ℓ(y, z) is the loss of predicting z when the true label\nwas y. A learner L : (X × Y)∗→YX takes a training set\nT = {(x1, y1), . . . (xn, yn)} consisting of a ﬁnite number\nof iid samples from µ and outputs a classiﬁer L(T) ∈YX ,\nwhere YX is the set of functions from X to Y. The loss of a\nclassiﬁer c ∈YX is ℓµ(c) = E(x,y)∼µ [ℓ(y, c(x))], and the\nexpected loss of L is ℓµ(L) = ET ∼µn[ℓµ(L(T))]. Learning\nis with respect to a concept class C ⊆YX .\nDeﬁnition 1 (Agnostic learning of C). For countable sets1\nX, Y and ℓ: Y × Y →[0, 1], learner L agnostically learns\nC ⊆YX if there exists a polynomial p such that for any\ndistribution µ over X × Y and for any n ≥p(1/ϵ, 1/δ),\nPrT ∼µn [ℓµ(L(T)) ≤minc∈C ℓµ(c) + ϵ] ≥1 −δ. Further,\nL and the classiﬁer L(T) must run in time polynomial in\nthe length of their inputs.\nPAC learning refers to the special case when µ is additional\nassumed to satisfy minc∈C ℓµ(c) = 0.\nMUL, which is the focus of this paper, simply refers to\ncase where µ is a meta-distribution over datasets X ∈X\nand ground truth labelings Y ∈Y. We use capital letters\nto represent datasets as opposed to individual examples. A\nmeta-classiﬁer c is a UL algorithm that take an entire dataset\nX as input, such as clustering algorithms, and outputs Z ∈\nY. As mentioned, true labels need only be observed for\nthe training datasets – we may never observe the true labels\nof any problem encountered after deployment. This differs\nfrom, say, online learning, where it is assumed that for\neach example, after you make a prediction, you ﬁnd out the\n1For simplicity of presentation, we assume that these sets are\ncountable, but with appropriate measure theoretic assumptions the\nanalysis in this paper can be extended to the inﬁnite case.\nSupervising Unsupervised Learning\nground truth.\nFor a ﬁnite set S, Π(S) denotes the set of clusterings\nor disjoint partitions of S into two or more sets, e.g.,\nΠ({1, 2, 3}) includes {{1}, {2, 3}} which is the partition\ninto clusters {1} and {2, 3}. For a clustering C, denote by\n∪C = ∪S∈CS the set of points clustered. Given two clus-\nterings Y, Z ∈Π(S), the Rand Index measures the fraction\nof pairs of points on which they agree:\nRI(Y, Z) = |{a ̸= b ∈S | dY (a, b) = dZ(a, b)}|\n|S|(|S| −1)\n,\nwhere for clustering C, we deﬁne the distance function\ndC(a, b) to be 0 if they are in the same cluster, i.e., a, b ∈R\nfor some R ∈C, and 1 otherwise. In our experiments,\nthe loss we will measure is the standard Adjusted Rand\nIndex (ARI) which attempts to correct the Rand Index by\naccounting for chance agreement (Hubert & Arabie, 1985).\nWe denote by ARI(Y, Z) the adjusted rand index between\ntwo clusterings Y and Z. We abuse notation and also write\nARI(Y, Z) when Y is a vector of class labels, by converting\nit to a clustering with one cluster for each class label. We\ndeﬁne the loss to be the fraction of pairs of points on which\nthey disagree, assuming the clusterings are on the same sets\nof points. If, for any reason the clusterings are not on the\nsame sets of points, the loss is deﬁned to be 1.\nℓ(Y, Z) =\n(\n1 −RI(Y, Z)\nif ∪Y = ∪Z\n1\notherwise.\n(1)\nIn Euclidean clustering, the points are Euclidean, so each\ndataset X ⊂Rd for some d ≥1. In meta-Euclidean-\nclustering, one aims to learn a clustering algorithm from\nseveral different training clustering problems (of potentially\ndifferent dimensionalities d).\nRand Index measures clustering quality with respect to an\nextrinsic ground truth. In many cases, such a ground truth\nis unavailable, and an intrinsic metric is useful. Such is the\ncase when choosing the number of clusters. Given different\nclusterings of size k = 2, 3, . . ., how can one compare\nand select? One popular approach is the so-called silhouette\nscore (Rousseeuw, 1987), deﬁned as follows for a Euclidean\nclustering:\nsil(C) =\n1\n| ∪C|\nX\nx∈∪C\nb(x) −a(x)\nmax{a(x), b(x)},\n(2)\nwhere a(x) denotes the average distance between point x\nand other points in its own cluster and b(x) denotes the aver-\nage distance between x and points in an alternative cluster,\nwhere the alternative cluster is the one (not containing x)\nwith smallest average distance to x.\n3. Meta-unsupervised problems\nThe simplest approach to MUL is Empirical Risk Minimiza-\ntion (ERM), namely choosing the unsupervised algorithm\nfrom some family C with lowest empirical error on training\nset T, which we write as ERMC(T). The following lemma\nimplies a logarithmic dependence on |C| and helps us solve\nseveral interesting MUL problems.\nLemma 1. For any ﬁnite family C of unsupervised learning\nalgorithms, any distribution µ over problems X, Y ∈X ×Y,\nand any n ≥1, δ > 0,\nPr\nT ∼µn\n\"\nℓµ(ERMC(T)) ≤min\nc∈C ℓµ(c) +\nr\n2\nn log |C|\nδ\n#\n≥1 −δ,\nwhere ERMC(T) ∈arg minc∈C\nP\nT ℓ(Y, c(X)) is any em-\npirical loss minimizer over c ∈C.\nProof. Fix U0 ∈arg minU∈U ℓµ(U). Let\nϵ = 2\nr\nlog(1/δ) + log |U|\n2m\n,\nand deﬁne S = {U ∈U | ℓµ(U) ≥ℓµ(U0) + ϵ}. Then\nChernoff bounds imply that\nPr\nT\n\n1\nm\nX\n(X,Y )∈T\nℓ(Y, U(X)) ≥ℓµ(U) + ϵ/2\n\n≤e−2m(ϵ/2)2.\nSimilarly, for each U ∈S,\nPr\nT\n\n1\nm\nX\n(X,Y )∈T\nℓ(Y, U(X)) ≤ℓµ(U) −ϵ/2\n\n≤e−2m(ϵ/2)2.\nIn order for ℓ(ERMU) ≥min ℓ(U) + ϵ, either some U ∈S\nmust have empirical error ≤ℓ(U) −ϵ/2 or the empirical\nerror of U0 must be ≥ℓ(U0)+ϵ/2. By the union bound, this\nhappens with probability at most |U|e−2m(ϵ/2)2 = δ.\n3.1. Selecting the clustering algorithm/number of\nclusters\nInstead of the ad hoc parameter selection heuristics cur-\nrently used in UL, MUL provides a data-driven alternative.\nSuppose one has m candidate clustering algorithms and/or\nparameter settings C1(X), . . . , Cm(X) for each data set\nX. These may be derived from m different clustering al-\ngorithms, or alternatively, they could represent the meta-k\nproblem, i.e., how many clusters to choose from a single\nclustering algorithm where parameter k ∈{2, . . . , m + 1}\ndetermines the number of clusters. In this section, we show\nthat choosing the right algorithm is essentially a multi-class\nclassiﬁcation problem given any set of problem metadata\nfeatures and cluster-speciﬁc features. Trivially, Lemma 1\nSupervising Unsupervised Learning\nimplies that with O(log m) training problem sets one can\nselect the Cj that performs best across problems. For meta-\nk, however, this would mean choosing the same number of\nclusters to use across all problems, analogous to choosing\nthe best single class for multi-class classiﬁcation.\nTo learn to choose the best Cj on a problem-by-problem\nbasis, suppose we have problem features φ(X) ∈Φ\nsuch as number of dimensions, number of points, domain\n(text/vision/etc.), and cluster features γ(Cj(X)) ∈Γ which\nmight include number of clusters, mean distance to cluster\ncenter, and silhouette score (eq. 2). Suppose we also have\na family F of functions f : Φ × Γm →{1, 2, . . . , m}, that\nselects the clustering based on features (i.e., any multi-class\nclassiﬁcation family may be used):\narg min\nf∈F\nX\ni\nℓ\n\u0000Yi, Cf(φ(Xi),γ(C1(Xi)),...,γ(Cm(Xi)))(Xi)\n\u0001\n.\nThe above ERMF\nis simply a reduction from the\nproblem of selecting Cj from X to the problem of\nmulti-class classiﬁcation based on features φ(X) and\nγ(C1(X)), . . . , γ(Cm(X)) and loss as deﬁned in eq. (1).\nAs long as F can be parametrized by a ﬁxed number of\nb-bit numbers, the ERM approach of choosing the “best” f\nwill be statistically efﬁcient. If ERMF cannot be computed\nexactly within time constraints, an approximate minimizer\nmay be used.\nFitting the threshold in single-linkage clustering. To il-\nlustrate a concrete efﬁcient algorithm, consider choosing the\nthreshold parameter of a single linkage clustering algorithm.\nFix the set of possible vertices V. Take X to consist of\nundirected weighted graphs X = (V, E, W) with vertices\nV ⊆V, edges E ⊆{{u, v} | u, v ∈V } and non-negative\nweights W : E →R+. The loss on clusterings Y = Π(V)\nis again as deﬁned in Eq. (1). Note that Euclidean data\ncould be transformed into, e.g., the complete graph with\nW({x, x′}) = ∥x −x′∥.\nSingle-linkage clustering with parameter r\n≥\n0,\nCr(V, E, w) partitions the data into connected components\nof the subgraph of (V, E) consisting of all edges whose\nweights are less than or equal to r. For generalization\nbounds, we simply assume that numbers are represented\nwith a constant number of bits, as is common today.\nTheorem 1. The class {Cr | r > 0} of single-linkage al-\ngorithms with threshold r where numbers are represented\nusing b bits, can be agnostically learned. In particular, a\nquasilinear time algorithm achieves error ≤minr ℓµ(Cr)+\np\n2(b + log 1/δ)/n, with prob. ≥1 −δ over n training\nproblems.\nProof. For generalization, we assume that numbers are\nrepresented using at most b bits. By Lemma 1, we see\nthat with m training graphs and |{Cr}| ≤2b, we have\nthat with probability ≥1 −δ, the error of ERM is within\np\n2(b + log 1/δ)/n of minr ℓµ(Cr).\nIt remains to show how one can ﬁnd the best single-linkage\nparameter in quasilinear time. It is trivial to see that one\ncan ﬁnd the best cutoff for r in polynomial time: for each\nedge weight r in the set of edge weights across all graphs,\ncompute the mean loss of Cr across the training set. Since\nCr runs in polynomial time, loss can be computed in poly-\nnomial time, and the number of different possible cutoffs is\nbounded by the number of edge weights which is polynomial\nin the input size, the entire algorithm runs in polynomial\ntime.\nFor a quasilinear-time algorithm (in the input size |T| =\nΘ(P\ni |Vi|2)), run Kruskal’s algorithm on the union graph\nof all of the graphs in the training set (i.e., the number of\nnodes and edges are the sum of the number of nodes and\nedges in the training graphs, respectively). As Kruskal’s\nalgorithm adds each new edge to its forest (in order of non-\ndecreasing edge weight), effectively two clusters in some\ntraining graph (Vi, Ei, Wi) have been merged. The change\nin loss of the resulting clustering can be computed from\nthe loss of the previous clustering in time proportional to\nthe product of the two clusters that are being merged, since\nthese are the only values on which Zi changed. Na¨ıvely,\nthis may seem to take order P\ni |Vi|3. However, note that,\neach pair of nodes begins separately and is updated, exactly\nonce during the course of the algorithm, to be in the same\ncluster. Hence, the total number of updates is O(P\ni |Vi|2),\nand since Kruskal’s algorithm is quasilinear time itself, the\nentire algorithm is quasilinear. For correctness, it is easy to\nsee that as Kruskal’s algorithm runs, Cr has been computed\nfor each possible r at the step just preceding when Kruskal\nadds the ﬁrst edge whose weight is greater than r.\n3.2. Outlier removal\nFor simplicity, we consider learning the single hyperpa-\nrameter of the fraction of examples, furthest from the\nmean, to remove.\nIn particular, suppose training prob-\nlems are classiﬁcation instances, i.e., Xi ∈Rdi×mi and\nYi ∈{1, 2, . . . , ki}mi. To be concrete, suppose one is using\nalgorithm C which is, say, K-means clustering. Choosing\nthe parameter θ which is the fraction of outliers to ignore\nduring ﬁtting, one might deﬁne Cθ with parameter θ ∈[0, 1)\non data x1, . . . , xn ∈Rd as follows: (a) compute the data\nmean µ = 1\nn\nP\ni xi, (b) set aside as outliers the θ fraction of\nexamples where xi is furthest from µ in Euclidean distance,\n(c) cluster the data with outliers removed using C, and (d)\nassign each outlier to the nearest cluster center.\nWe can trivially choose the best θ so as to optimize\nperformance.\nWith a single b-bit parameter θ, Lemma\n1 implies that this choice of θ will give a loss within\np\n2(b + log 1/δ)/n of the optimal θ, with probability ≥\nSupervising Unsupervised Learning\n1 −δ over the sample of datasets. The number of θ’s that\nneed to be considered is at most the total number of inputs\nacross problems, so the algorithm runs in polynomial time.\n3.3. Problem recycling\nFor this model, suppose that each problem belongs to a set\nof common problem categories, e.g., digit recognition, sen-\ntiment analysis, image classiﬁcation among the thousands\nof classes of ImageNet (Russakovsky et al. , 2015), etc. The\nidea is that one can recycle the solution to one version of\nthe problem in a later incarnation. For instance, suppose\nthat one trained a digit recognizer on a previous problem.\nFor a new problem, the input may be encoded differently\n(e.g., different image size, different pixel ordering, different\ncolor representation), but there is a transformation T that\nmaps this problem into the same latent space as the previ-\nous problem so that the prior solution can be re-used. In\nparticular, for each problem category i = 1, 2, . . . , N, there\nis a latent problem space Λi and a solver Si : Λi →Yi.\nEach problem X, Y of this category can be transformed\nto T(X) ∈Λi with low solution loss ℓ(Y, S(T(X))). In\naddition to the solvers, one also requires a meta-classiﬁer\nM : X →{1, 2, . . . , N} that, for a problem X, identiﬁes\nwhich solver i = M(X) to use. Finally, one has transform-\ners Ti : M −1(i) →Li that map any X such that M(X) = i\ninto latent space Λi. The output of the meta-classiﬁer is sim-\nply SM(X)(TM(X)(X)). Lemma 1 implies that if one can\noptimize over meta-classiﬁers and the parameters of the\nmeta-classiﬁer are represented by D b-bit numbers, then\none achieves loss within ϵ of the best meta-classiﬁer with\nm = O\n\u0000Db/ϵ2\u0001\nproblems.\n4. The possibility of meta-clustering\nIn this section, we point out how the framing of meta-\nclustering circumvents Kleinberg’s impossibility theorem\nfor clustering. To review, (Kleinberg, 2003) considers clus-\ntering ﬁnite sets of points X endowed with symmetric dis-\ntance functions d ∈D(X), where the set of valid distance\nfunctions is:\nD(X) = {d : X × X →R | ∀x, x′ ∈X\nd(x, x′) = d(x′, x) ≥0, d(x, x′) = 0 iff x = x′}.\n(3)\nA clustering algorithm A takes a distance function\nd ∈D(X) and returns a partition, i.e., A(d) ∈Π(X).\nKleinberg deﬁnes an axiomatic framework with the\nfollowing three desirable properties, and proves that no\nclustering algorithm A can satisfy all these properties\nsimultaneously:\nScale-Invariance.\nFor any distance function d and\nany α > 0, A(d) = A(α · d), where α · d is the distance\nfunction d scaled by α. That is, the clustering should not\nchange if the problem is scaled by a constant factor.\nRichness. For any ﬁnite X and clustering C ∈Π(X),\nthere exists d ∈D(X) such that A(d) = C. Richness\nimplies that for any partition there is an arrangement of\npoints where that partition is the correct clustering.\nConsistency. Let d, d′ ∈D(X) such that A(d) = C, and\nfor all x, x′ ∈X, if x, x′ are in the same cluster in C then\nd′(x, x′) ≤d(x, x′) while if x, x′ are in different clusters\nin C then d′(x, x′) ≥d(x, x′). Then the axiom demands\nA(d′) = A(d). That is, the clustering should not change if\nthe points within a cluster are pulled closer to each other,\nand the points in different clusters are pushed further apart.\nFor intuition, consider clustering two points where there\nis a single distance. Should they be in a single cluster or\ntwo clusters? By richness, there must be some distances\nδ1, δ2 > 0 such that if d(x1, x2) = δ1 then they are in the\nsame cluster while if d(x1, x2) = δ2 they are in different\nclusters. This, however violates Scale-Invariance, since the\ntwo problems are at a scale α = δ2/δ1 of each other. We\nshow that a natural meta-version of the axioms is satisﬁed\nby a simple meta-single-linkage clustering algorithm. The\nmain insight is that prior problems can be used to deﬁne\na scale in the meta-clustering framework. Suppose we\ndeﬁne the clustering problem with respect to a non-empty\ntraining set of clustering problems. So a meta-clustering\nalgorithm takes t\n≥\n1 training clustering problems\nM(d1, C1, . . . , dt, Ct)\n=\nA with their ground-truth\nclusterings (on corresponding sets Xi, i.e., di ∈D(Xi) and\nCi ∈Π(Xi)) and outputs a clustering algorithm A. We\ncan use these training clusterings to establish a scale. In\nparticular, we will show a meta-clustering algorithm whose\noutput A always satisﬁes the second two axioms and which\nsatisﬁes the following variant of Scale-Invariance:\nMeta-Scale-Invariance.\nFix any distance functions\nd1, d2, . . . , dt and ground truth clusterings C1, . . . , Ct\non sets X1, . . . , Xt.\nFor any α\n>\n0, and any dis-\ntance function d, if M(d1, C1, . . . , dt, Ct)\n=\nA and\nM(α · d1, C1, . . . , α · dt, Ct) = A′, then A(d) = A′(α · d).\nTheorem 2. There is a meta-clustering algorithm that sat-\nisﬁes Meta-Scale-Invariance and whose output always sat-\nisﬁes Richness and Consistency.\nProof. There are a number of such clustering algorithms,\nbut for simplicity we create one based on single-linkage\nclustering. Single-linkage clustering satisﬁes Richness and\nConsistency (see (Kleinberg, 2003), Theorem 2.2). With\nmeta-clustering, the scale can be established using train-\ning data. The question is how to choose its single-linkage\nparameter. One can choose it to be the minimum distance\nbetween any two points in different clusters across all train-\nSupervising Unsupervised Learning\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.105\n0.11\n0.115\n0.12\nTraining fraction\nAverage ARI\nSelecting the number of clusters\nSilhouette\nMeta\nFigure 2. Average ARI scores of the meta-algorithm and the base-\nline for choosing the number of clusters, versus the fraction of\nproblems used for training. We observe that the best k predicted\nby the meta approach registered a much higher ARI than that by\nSilhouette score maximizer. Also, as shown in the Supplementary,\nthe meta-algorithm achieved a much lower root-mean-square error.\ning problems. It is easy to see that if one scales the training\nproblems and d by the same factor α, the clusterings re-\nmain unchanged, and hence the meta-clustering algorithm\nsatisﬁes meta-scale-invariance.\n5. Experiments\nWe conducted several experiments to substantiate the efﬁ-\ncacy of the proposed framework under various unsupervised\nsettings. We downloaded all classiﬁcation datasets from\nOpenML (http://www.openml.org) that had at most 10,000\ninstances, 500 features, 10 classes, and no missing data to\nobtain a corpus of 339 datasets. We now describe in detail\nthe results of our experiments, highlighting the effect of our\nmeta paradigm in improving the performance.\n5.1. Selecting the number of clusters\nFor the purposes of this section, we ﬁx the clustering al-\ngorithm to be K-means and compare two approaches to\nchoosing the number of clusters, k from k = 2 to 10. More\ngenerally, one could vary k on, for instance, a logarithmic\nscale, or a combination of different scales. First, we con-\nsider a standard heuristic for the baseline choice of k: for\neach cluster size k and each dataset, we generate 10 clus-\nterings from different random starts for K-means and take\none with best Silhouette score among the 10. Then, over\nthe 9 different values of k, we choose the one with great-\nest Silhouette score so that the resulting clustering is the\none of greatest Silhouette score among all 90. Similar to\nthe approach for choosing the best algorithm above, in our\nmeta-k approach, the meta-algorithm outputs ˆk as a function\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\nTraining fraction\nAdjusted Rand Index (ARI)\nPerformance of different algorithms\nMeta\nKMeans\nKMeans-N\nWard\nWard-N\nAverage\nAverage-N\nComplete\nComplete-N\nSpectral\nSpectral-N\nFigure 3. ARI scores of different clustering (k=2) algorithms on\nOpenML binary classiﬁcation problems. The meta algorithm (95%\nconﬁdence intervals are shown) is compared with standard cluster-\ning baselines on both the original data as well as the transformed\ndata (denoted by “-N”) where each feature is normalized to have\nzero mean and unit variance. The meta-algorithm, given sufﬁcient\ntraining problems, is able to outperform all the other algorithms.\nof Silhouette score and k by outputting the ˆk with greatest\nestimated ARI. We evaluate on the same 90 clusterings for\nthe 339 datasets as the baseline. To estimate ARI in this\nexperiment, we used simple least-squares linear regression.\nIn particular, for each k ∈{2, . . . , 9}, we ﬁt ARI as a lin-\near function of Silhouette scores using all the data from\nthe meta-training set in the partition pertaining to k: each\ndataset in the meta-training set provided 10 target values,\ncorresponding to different runs where number of clusters\nwas ﬁxed to k. As is standard in the clustering literature, we\ndeﬁne the best-ﬁt k∗\ni for dataset i to be the one that yielded\nmaximum ARI score across the different runs, which is of-\nten different from ki, the number of clusters in the ground\ntruth (i.e., the number of class labels). We held out a fraction\nof the problems for test and used the remaining for training.\nWe evaluated two quantities of interest: the ARI and the\nroot-mean-square error (RMSE) between ˆk and k∗. Both\nquantities were better for the meta-algorithm than the base-\nline where we performed 1000 train-test splits to compute\nthe conﬁdence intervals (Fig. 2).\n5.2. Selecting the clustering algorithm\nWe now consider the question of which of a number of given\nclustering algorithms to use to cluster a given data set. We\nillustrate the main ideas with k = 2 clusters. First, one can\nrun each of the algorithms on the repository and see which\nalgorithm has the lowest average error. Error is calculated\nwith respect to the ground truth labels by the ARI (see Sec-\ntion 2). We compare algorithms on the 250 openml binary\nclassiﬁcation datasets with at most 2000 instances. The ten\nSupervising Unsupervised Learning\nbase clustering algorithms were chosen to be ﬁve clustering\nalgorithms from scikit-learn (Pedregosa et al. , 2011): K-\nMeans, Spectral, Agglomerative Single Linkage, Complete\nLinkage, and Ward, together with a second version of each\nin which each attribute is ﬁrst normalized to have zero mean\nand unit variance. Each algorithm is run with the default\nscikit-learn parameters. We implement the algorithm selec-\ntion approach of Section 3, learning to choose a different\nalgorithm for each problem based on problem and cluster-\nspeciﬁc features. Given clustering Π of X ∈Rd×m, the\nfeature vector Φ(X, Π) consists of the dimensionality, num-\nber of examples, minimum and maximum eigenvalues of\nthe covariance matrix, and the silhouette score (see Section\n2) of the clustering Π:\nΦ(X, Π) = (d, m, σmin(Σ(X)), σmax(Σ(X)), sil(Π)) ,\nwhere Σ(X) denotes the covariance matrix of X, and\nσmin(M) and σmax(M) denote the minimum and maxi-\nmum eigenvalues, respectively, of matrix M. Instead of\nchoosing the clustering with best Silhouette score, which is a\nstandard approach, the meta-clustering algorithm effectively\nlearns terms that can correct for over- or under-estimates,\ne.g., learning for which problems the Silhouette heuristic\ntends to produce too many clusters. To choose which of\nthe ten clustering algorithms on each problem, we ﬁt ten\nestimators of accuracy (ARI, see Section 2) based on these\nfeatures. That is for each clustering algorithm Cj, we ﬁt\nARI(Yi, Cj(Xi)) from features Φ(Xi, Cj(Xi)) ∈R5 over\nproblems Xi, Yi using ν-SVR regression, with default pa-\nrameters as implemented by scikit-learn. Call this estimator\nˆaj(X, Cj(X)). To cluster a new dataset X ∈Rd×m, the\nmeta-algorithm then chooses Cj(X) for the j with greatest\naccuracy estimate ˆaj(X, Cj(X)). The 250 problems were\ndivided into train and test sets of varying sizes. The results,\nshown in Figure 3, demonstrate two interesting features.\nFirst, one can see that the different baseline clustering algo-\nrithms had very different average performances, suggesting\nthat a principled approach like ours to select algorithms\ncan make a difference. Further, Figure 3 shows that the\nmeta-algorithm, given sufﬁciently many training problems,\nis able to outperform, on average, all the baseline algorithms.\nThis is despite the fact that the 250 problems have different\ndimensionalities and come from different domains. Based\non the trend, it is possible that with more training problems\nit would further surpass the baselines.\n5.3. Removing outliers\nWe also experimented to see if removing outliers improved\naverage performance on the same 339 classiﬁcation prob-\nlems. Our objective was to choose a single best fraction to\nremove from all the meta-test sets. For each data set X, we\nremoved a p ∈{0, 0.01, 0.02, . . . , 0.05} fraction examples\nwith the highest euclidean norm in X as outliers, and like-\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.1\n0.105\n0.11\n0.115\n0.12\n0.125\n0.13\nTraining fraction\nAverage Adjusted Rand Index\nPerformance with outlier removal\n5%\n4%\n3%\n2%\n1%\n0%\nFigure 4. Outlier removal results. Removing 1% of the instances\nas outliers improves on the ARI scores obtained without outlier re-\nmoval. Interestingly, going beyond 1% decreases the performance\nto that without outlier removal. Such data dependent observations\nwould be missed by the prevalent rules of thumb. Our algorithm\nnaturally ﬁgures out the right proportion (1%) in a principled way.\nwise for each meta-test set in the partition. The previous\nsection corresponds exactly to the case p = 0. We ﬁrst\nclustered the data without outliers, and obtained the corre-\nsponding Silhouette scores. We then put back the outliers\nby assigning them to their nearest cluster center, and com-\nputed the ARI score thereof. Then, following an identical\nprocedure to the meta-k algorithm of Section 5.1, we ﬁtted\nregression models for ARI corresponding to complete data\nusing the silhouette scores on pruned data, and measured the\neffect of outlier removal in terms of the true average ARI\n(corresponding to the best predicted ARI) over entire data.\nAgain, we report the results averaged over 10 independent\ntrain/test partitions. As Fig. 4 shows, by treating 1% of\nthe instances in each dataset as outliers, we achieved an\nimprovement in ARI scores relative to clustering with all\nthe data as in section 5.1. As the fraction of data considered\noutlier was increased to 2% or higher, however, performance\ndegraded. Clearly, we can learn what fraction to remove\nbased on data, and improve the performance considerably\neven with such a simple algorithm.\n5.4. Deep learning binary similarity function\nIn this section, we consider a new unsupervised problem\nof learning a binary similarity function (BSF) that predicts\nwhether two examples from a given problem should belong\nto the same cluster (i.e., have the same class label). Formally,\na problem is speciﬁed by a set X of data and meta-features\nφ. The goal is to learn a classiﬁer f(x, x′, φ) ∈{0, 1}\nthat takes two examples x, x′ ∈X and the corresponding\nproblem meta-features φ, and predicts 1 if the input pair\nwould belong to the same cluster (or have the same class la-\nSupervising Unsupervised Learning\nbels). In our experiments, we take Euclidean data X ⊆Rd\n(each problem may have different dimensionality d), and the\nmeta-features φ = Σ(X) consist of the covariance matrix\nof the unlabeled data. We restricted our experiments to the\n146 datasets with at most 1000 examples and 10 features.\nWe normalized each dataset to have zero mean and unit\nvariance along every coordinate (hence, every diagonal ele-\nment in the covariance matrix was set to 1). We randomly\nsampled pairs of examples from each dataset to form dis-\njoint meta-training and meta-test sets. For each pair, we\nconcatenated the features to create data with 20 features\n(padding examples with fewer than 10 features with zeros).\nWe then computed the empirical covariance matrix of the\ndataset, and vectorized the entries of the covariance matrix\non and above the leading diagonal to obtain an additional 55\ncovariance features. Concatenating these features with the\n20 features formed a 75-dimensional feature vector per pair.\nThus all pairs sampled from the same dataset shared the 55\ncovariance features. Moreover, we derived a new binary\nlabel dataset in the following way. We assigned a label 1 to\npairs formed by combining examples belonging to the same\nclass, and 0 to those resulting from the different classes.\nEach dataset in the ﬁrst category was used to sample data\npairs for both the meta-training and the meta-internal test\n(meta-IT) datasets, while the second category did not con-\ntribute any training data and was exclusively used to gen-\nerate only the meta-external test (meta-ET) dataset. Our\nprocedure ensured a disjoint intersection between the meta-\ntraining and the meta-IT data, and resulted in 10 separate\n(meta-training, meta-IT, meta-ET) triplets. Note that com-\nbining thousands of examples from each of hundreds of\nproblems yields hundreds of thousands of examples, turn-\ning small data into big data. This provides a means of\nmaking DNNs naturally applicable to data sets that might\nhave otherwise been too small. For each meta-training set,\nwe trained an independent deep net model. The complete\ndetails of the sampling process, the network architecture,\nand the training procedure are given in the Supplementary.\nWe tested our trained models on meta-IT and meta-ET data.\nFor each feature vector in meta-IT (respectively meta-ET),\nwe computed the predicted same class probability. We added\nthe predicted same class probability for the feature vector\nobtained with ﬂipped order, as described earlier for the\nfeature vectors in the meta-training set. We predicted the\ninstances in the corresponding pair to be in the same cluster\nonly if the average of these two probabilities exceeded 0.5.\nWe compared the meta approach to a hypothetical majority\nrule that had prescience about the class distribution. As\nthe name suggests, the majority rule predicted all pairs to\nhave the majority label, i.e., on a problem-by-problem basis\nwe determined whether 1 (same class) or 0 (different class)\nwas more accurate and gave the baseline the advantage of\nthis knowledge for each problem, even though it normally\nInternal test (IT)\nExternal test (ET)\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nAverage accuracy\nAverage binary similarity prediction accuracy\nMeta\nMajority\nFigure 5. Mean accuracy and standard deviation on meta-IT and\nmeta-ET data. Comparison between the fraction of pairs correctly\npredicted by the meta algorithm and the majority rule. Recall that\nmeta-ET, unlike meta-IT, was generated from a partition that did\nnot contribute any training data. Nonetheless, the meta approach\nsigniﬁcantly improved upon the majority rule even on meta-ET.\nwould not be available at classiﬁcation time. This informa-\ntion about the distribution of the labels was not accessible\nto our meta-algorithm. Fig. 5 shows the average fraction\nof similarity pairs correctly identiﬁed relative to the corre-\nsponding pairwise ground truth relations on the two test sets,\nand the corresponding standard deviations across the 10\nindependent (meta-training, meta-IT, meta-ET) collections.\nClearly, the meta approach outperforms the majority rule\non meta-IT, illustrating the beneﬁts of the meta approach in\na multi-task transductive setting. More interesting, still, is\nthe signiﬁcant improvement exhibited by the meta method\non meta-ET, despite having its category precluded from\ncontributing any data for training. The result clearly demon-\nstrates the beneﬁts of leveraging archived supervised data\nfor informed decision making in unsupervised settings.\n6. Conclusion\nTreating each UL problem in isolation has made UL no-\ntoriously difﬁcult to deﬁne.\nWe suggest that UL prob-\nlems be viewed as representative samples from some meta-\ndistribution µ.\nWe show how a repository of multiple\ndatasets annotated with ground truth labels can be used to\nimprove average performance on several unsupervised tasks,\neven with simple algorithms. Theoretically, this enables us\nto make UL problems, such as clustering, well-deﬁned.\nPrior datasets may prove useful for a variety of reasons,\nfrom simple to complex. They may help one choose the best\nclustering algorithm or parameter settings, or transfer shared\nfeatures that can be identiﬁed as useful. We demonstrate\nhow the combination of many small heterogeneous data sets\ncan form a large data set appropriate for training with a\ncommon deep network. This lets us accomplish zero shot\nlearning across small and diverse data in a natural way.\nSupervising Unsupervised Learning\nReferences\nAckerman, Margareta, & Ben-David, Shai. 2008. Measures\nof Clustering Quality: A Working Set of Axioms for\nClustering. In: NIPS.\nBalcan, Maria-Florina, Blum, Avrim, & Vempala, Santosh.\n2015. Efﬁcient representations for lifelong learning and\nautoencoding. In: Workshop on Computational Learning\nTheory (COLT).\nBousmalis, Konstantinos, Trigeorgis, George, Silberman,\nNathan, Krishnan, Dilip, & Erhan, Dumitru. 2016. Do-\nmain Separation Networks. In: NIPS.\nChen, Wenlin, Wilson, James, Tyree, Stephen, Weinberger,\nKilian, & Chen, Yixin. 2015. Compressing Neural Net-\nworks with the Hashing Trick. In: ICML.\nFeurer, Matthias, Klein, Aaron, Eggensperger, Katharina,\nSpringenberg, Jost, Blum, Manuel, & Hutter, Frank. 2015.\nEfﬁcient and robust automated machine learning. Pages\n2962–2970 of: NIPS.\nGanin, Yaroslav, & Lempitsky, Victor. 2015. Unsupervised\nDomain Adaptation by Backpropagation. In: ICML.\nGupta, Chirag, Suggala, Arun Sai, Goyal, Ankit, Simhadri,\nHarsha Vardhan, Paranjape, Bhargavi, Kumar, Ashish,\nGoyal, Saurabh, Udupa, Raghavendra, Varma, Manik, &\nJain, Prateek. 2017. ProtoNN: Compressed and Accurate\nkNN for Resource-scarce Devices. Pages 1331–1340\nof: Proceedings of the 34th International Conference on\nMachine Learning (ICML).\nHan, S., Mao, H., & Dally, W. J. 2016. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding. In: ICLR.\nHubert, Lawrence, & Arabie, Phipps. 1985. Comparing\npartitions. Journal of classiﬁcation, 2(1), 193–218.\nKearns, Michael J, Schapire, Robert E, & Sellie, Linda M.\n1994. Toward efﬁcient agnostic learning. Machine Learn-\ning, 17(2-3), 115–141.\nKingma, Diederik P., Rezende, Danilo J., Mohamed, Shakir,\n& Welling, Max. 2014. Semi-supervised Learning with\nDeep Generative Models. In: NIPS.\nKleinberg, Jon. 2003. An impossibility theorem for cluster-\ning. Pages 463–470 of: Advances in neural information\nprocessing systems (NIPS).\nLong, Mingsheng, Zhu, Han, Wang, Jianmin, & Jordan,\nMichael I. 2016. Unsupervised Domain Adaptation with\nResidual Transfer Networks. In: NIPS.\nLuo, Jian-Hao, Wu, Jianxin, & Lin, Weiyao. 2017. ThiNet:\nA Filter Level Pruning Method for Deep Neural Network\nCompression. In: ICCV.\nPan, Sinno Jialin, & Yang, Qiang. 2010.\nA Survey on\nTransfer Learning. IEEE Transactions on Knowledge and\nData Engineering (TKDE), 22, 1345–1359.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,\nThirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,\nWeiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-\nnapeau, D., Brucher, M., Perrot, M., & Duchesnay, E.\n2011. Scikit-learn: Machine Learning in Python. Journal\nof Machine Learning Research (JMLR), 12, 2825–2830.\nRohrbach, Marcus, Ebert, Sandra, & Schiele, Bernt. 2013.\nTransfer Learning in a Transductive Setting. In: NIPS.\nRousseeuw, Peter J. 1987. Silhouettes: a graphical aid to the\ninterpretation and validation of cluster analysis. Journal\nof computational and applied mathematics, 20, 53–65.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nthy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,\nAlexander C., & Fei-Fei, Li. 2015. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of\nComputer Vision (IJCV), 115(3), 211–252.\nSiddharth, N., Paige, Brooks, van de Meent, Jan-Willem,\nDesmaison, Alban, Goodman, Noah D., Kohli, Pushmeet,\nWood, Frank, & Torr, Philip H.S. 2017. Learning Dis-\nentangled Representations with Semi-Supervised Deep\nGenerative Models. In: NIPS.\nSnoek, Jasper, Larochelle, Hugo, & Adams, Ryan P. 2012.\nPractical bayesian optimization of machine learning algo-\nrithms. Pages 2951–2959 of: NIPS.\nThornton, Chris, Hutter, Frank, Hoos, Holger H, & Leyton-\nBrown, Kevin. 2013. Auto-WEKA: Combined selection\nand hyperparameter optimization of classiﬁcation algo-\nrithms. Pages 847–855 of: Proceedings of the 19th ACM\nSIGKDD international conference on Knowledge discov-\nery and data mining. ACM.\nThrun, Sebastian, & Mitchell, Tom M. 1995. Lifelong robot\nlearning. Pages 165–196 of: The biology and technology\nof intelligent autonomous agents. Springer.\nThrun, Sebastian, & Pratt, Lorien. 2012. Learning to learn.\nSpringer Science & Business Media.\nZadeh, Reza Bosagh, & Ben-David, Shai. 2009. A unique-\nness theorem for clustering. Pages 639–646 of: Pro-\nceedings of the twenty-ﬁfth conference on uncertainty in\nartiﬁcial intelligence (UAI). AUAI Press.\nZeiler, Matthew D. 2012. ADADELTA: an adaptive learning\nrate method. arXiv preprint arXiv:1212.5701.\nSupervising Unsupervised Learning\nA. Complete details: Deep learning a binary\nsimilarity function\nWe consider a new unsupervised problem of learning a\nbinary similarity function (BSF) that predicts whether two\nexamples from a given problem should belong to the same\ncluster (i.e., have the same class label). Formally, a problem\nis speciﬁed by a set X of data and meta-features φ. The goal\nis to learn a classiﬁer f(x, x′, φ) ∈{0, 1} that takes two\nexamples x, x′ ∈X and the corresponding problem meta-\nfeatures φ, and predicts 1 if the input pair would belong to\nthe same cluster (or have the same class labels).\nIn our experiments, we take Euclidean data X ⊆Rd (each\nproblem may have different dimensionality d), and the meta-\nfeatures φ = Σ(X) consist of the covariance matrix of the\nunlabeled data. We restricted our experiments to the 146\ndatasets with at most 1000 examples and 10 features. We\nnormalized each dataset to have zero mean and unit variance\nalong every coordinate (hence, every diagonal element in\nthe covariance matrix was set to 1).\nWe randomly sampled pairs of examples from each dataset\nto form meta-training and meta-test sets as described in the\nfollowing section. For each pair, we concatenated the fea-\ntures to create data with 20 features (padding examples with\nfewer than 10 features with zeros). We then computed the\nempirical covariance matrix of the dataset, and vectorized\nthe entries of the covariance matrix on and above the lead-\ning diagonal to obtain an additional 55 covariance features.\nConcatenating these features with the 20 features formed\na 75-dimensional feature vector per pair. Thus all pairs\nsampled from the same dataset shared the 55 covariance\nfeatures. Moreover, we derived a new binary label dataset\nin the following way. We assigned a label 1 to pairs formed\nby combining examples belonging to the same class, and 0\notherwise.\nA.0.1. SAMPLING PAIRS TO FORM META-TRAIN AND\nMETA-TEST DATASETS\nWe formed a partition of the new datasets by randomly\nassigning each dataset to one of the two categories with\nequal probability. Each dataset in the ﬁrst category was used\nto sample data pairs for both the meta-training and the meta-\ninternal test (meta-IT) datasets, while the second category\ndid not contribute any training data and was exclusively used\nto generate only the meta-external test (meta-ET) dataset.\nWe constructed meta-training pairs by sampling randomly\npairs from each dataset in the ﬁrst category. In order to\nmitigate the bias resulting from the variability in size of the\ndifferent datasets, we restricted the number of pairs sampled\nfrom each dataset to at most 2500. Likewise, we obtained\nthe meta-IT dataset by collecting randomly sampling each\ndataset subject to the maximum 2500 pairs. Speciﬁcally, we\nrandomly shufﬂed each dataset belonging to the ﬁrst cate-\ngory, and used the ﬁrst half (or 2500 examples, whichever\nwas fewer) of the dataset for the meta-training data, and\nthe following indices for the meta-IT data, again subject to\nmaximum 2500 instances. This procedure ensured a dis-\njoint intersection between the meta-training and the meta-IT\ndata. Note that combining thousands of examples from each\nof hundreds of problems yields hundreds of thousands of\nexamples, thereby turning small data into big data. This\nprovides a means of making DNNs naturally applicable to\ndata sets that might have otherwise been too small.\nWe created the meta-ET data using datasets belonging to the\nsecond category. Again, we sampled at most 2500 examples\nfrom each dataset in the second category. We emphasize\nthat the datasets in the second category did not contribute\nany training data for our experiments.\nWe performed 10 independent experiments to obtain mul-\ntiple partitions of the datasets into two categories, and re-\npeated the aforementioned procedure to prepare 10 separate\n(meta-training, meta-IT, meta-ET) triplets. This resulted in\nthe following (average size +/- standard deviation) statistics\nfor dataset sizes:\nmeta-training and meta-IT\n:\n1.73 × 105 ± 1.07 × 104\nmeta-ET\n:\n1.73 × 105 ± 1.19 × 104\nIn order to ensure symmetry of the binary similarity func-\ntion, we introduced an additional meta-training pair for each\nmeta-training pair in the meta-training set: in this new pair,\nwe swapped the order of the feature vectors of the instances\nwhile replicating the covariance features of the underlying\ndataset that contributed the two instances (note that since\nthe covariance features were symmetric, they carried over\nunchanged).\nA.0.2. TRAINING NEURAL MODELS\nFor each meta-training set, we trained an independent deep\nnet model with 4 hidden layers having 100, 50, 25, and\n12 neurons respectively over just 10 epochs, and used\nbatches of size 250 each. We updated the parameters of\nthe model via the Adadelta (Zeiler, 2012) implementation\nof the stochastic gradient descent (SGD) procedure supplied\nwith the Torch library2 with the default setting of the param-\neters, speciﬁcally, interpolation parameter equal to 0.9 and\nno weight decay. We trained the model via the standard neg-\native log-likelihood criterion (NLL). We employed ReLU\nnon-linearity at each hidden layer but the last one, where\nwe invoked the log-softmax function.\nWe tested our trained models on meta-IT and meta-ET data.\nFor each feature vector in meta-IT (respectively meta-ET),\nwe computed the predicted same class probability. We added\n2See https://github.com/torch/torch7 .\nSupervising Unsupervised Learning\nthe predicted same class probability for the feature vector ob-\ntained with ﬂipped order, as described earlier for the feature\nvectors in the meta-training set. We predicted the instances\nin the corresponding pair to be in the same cluster if the\naverage of these two probabilities exceeded 0.5, otherwise\nwe segregated them.\nA.0.3. RESULTS\nWe compared the meta approach to a hypothetical majority\nrule that had prescience about the class distribution. As\nthe name suggests, the majority rule predicted all pairs to\nhave the majority label, i.e., on a problem-by-problem basis\nwe determined whether 1 (same class) or 0 (different class)\nwas more accurate and gave the baseline the advantage of\nthis knowledge for each problem, even though it normally\nwouldn’t be available at classiﬁcation time. This informa-\ntion about the distribution of the labels was not accessible\nto our meta-algorithm.\nFig. 5 shows the average fraction of similarity pairs correctly\nidentiﬁed relative to the corresponding pairwise ground truth\nrelations on the two test sets, and the corresponding stan-\ndard deviations across the 10 independent (meta-training,\nmeta-IT, meta-ET) collections. Clearly, the meta approach\noutperforms the majority rule on meta-IT, illustrating the\nbeneﬁts of the meta approach in a multi-task transductive\nsetting. More interesting, still, is the signiﬁcant improve-\nment exhibited by the meta method on meta-ET, despite\nhaving its category precluded from contributing any data\nfor training. The result clearly demonstrates the beneﬁts of\nleveraging archived supervised data for informed decision\nmaking in unsupervised settings such as binary similarity\nprediction.\nB. Additional results (Selecting the number of\nclusters)\nFig. 6 shows how the root-mean-square error (RMSE) be-\ntween ˆk and k∗when we performed 1000 train-test splits\nto compute the conﬁdence intervals. Clearly, the meta-\nalgorithm registers a much lower RMSE than the Silhouette\nbaseline. Moreover the discrepancy between the two meth-\nods increases, in favor of our algorithm, with an increase in\nthe training data.\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\n3.4\n3.6\n3.8\n4\nTraining fraction\nAverage RMSE\nBest-k Root Mean Square Error\nSilhouette\nMeta\nFigure 6. RMSE of distance to the best k, i.e. k∗, across datasets,\ni.e., comparing\nq\n1\nn\nP(˜ki −k∗\ni )2 and\nq\n1\nn\nP(ˆki −k∗\ni )2, where\nˆk and ˜k are the output of the meta-k algorithm and Silhouette\nbaseline respectively. Clearly, the meta-k method outputs a number\nof clusters much closer to k∗than the Silhouette score maximizer.\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2017-09-14",
  "updated": "2018-02-16"
}