{
  "id": "http://arxiv.org/abs/1709.05870v1",
  "title": "ZhuSuan: A Library for Bayesian Deep Learning",
  "authors": [
    "Jiaxin Shi",
    "Jianfei Chen",
    "Jun Zhu",
    "Shengyang Sun",
    "Yucen Luo",
    "Yihong Gu",
    "Yuhao Zhou"
  ],
  "abstract": "In this paper we introduce ZhuSuan, a python probabilistic programming\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\nexisting deep learning libraries, which are mainly designed for deterministic\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\nincluding both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\nnetworks.",
  "text": "ZhuSuan: A Library for Bayesian Deep Learning∗\nJiaxin Shi1\nshijx15@mails.tsinghua.edu.cn\nJianfei Chen1\nchenjian14@mails.tsinghua.edu.cn\nJun Zhu1\ndcszj@tsinghua.edu.cn\nShengyang Sun2\nssy@cs.toronto.edu\nYucen Luo1\nluoyc15@mails.tsinghua.edu.cn\nYihong Gu1\ngyh15@mails.tsinghua.edu.cn\nYuhao Zhou1\nzhouyh16@mails.tsinghua.edu.cn\n1Department of Computer Science & Technology, TNList Lab, CBICR Center\n1State Key Lab of Intelligent Technology & Systems\n2Department of Electronic Engineering\nTsinghua University\nBeijing, 100084, China\nAbstract\nIn this paper we introduce ZhuSuan, a python probabilistic programming library for Bayesian\ndeep learning, which conjoins the complimentary advantages of Bayesian methods and deep\nlearning. ZhuSuan is built upon Tensorﬂow. Unlike existing deep learning libraries, which\nare mainly designed for deterministic neural networks and supervised tasks, ZhuSuan is\nfeatured for its deep root into Bayesian inference, thus supporting various kinds of proba-\nbilistic models, including both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic programming\non ZhuSuan, including Bayesian logistic regression, variational auto-encoders, deep sigmoid\nbelief networks and Bayesian recurrent neural networks.\nKeywords:\nBayesian Inference, Deep Learning, Probabilistic Programming, Deep Gen-\nerative Models\n1. Introduction\nThese years have seen great advances of deep learning (LeCun et al., 2015) and its suc-\ncess in many applications, such as speech recognition (Hinton et al., 2012), computer vi-\nsion (Krizhevsky et al., 2012), language processing (Sutskever et al., 2014), and computer\ngames (Silver et al., 2016). One good lesson we learned from the practice is that a deeply\narchitected model can be well ﬁt by leveraging advanced optimization algorithms (e.g.,\nstochastic gradient decent), a large training set (e.g., ImageNet), and powerful computing\ndevices (e.g., GPUs). Although the great expressiveness of deep neural networks (DNNs)\nhas made them a ﬁrst choice for many complex ﬁtting problems, especially those tasks\ninvolving a mapping from an input space to an output space (e.g., classiﬁcation and regres-\nsion), the results given by them are usually point estimates. Such a deterministic approach\ndoes not account for uncertainty, which is essential in every part of a learning machine (e.g.,\ndata, estimation, inference, prediction and decision-making) due to the randomness of the\n∗. J. Zhu is the corresponding author. S. Sun is now with Department of Computer Science, University of\nToronto.\n1\narXiv:1709.05870v1  [stat.ML]  18 Sep 2017\nShi et al.\nphysical world, incomplete information, and measurement noise. It has been demonstrated\nthat deterministic neural networks can be vulnerable to adversarial attacks (Szegedy et al.,\n2013; Nguyen et al., 2015) (partly because of the lack of uncertainty modeling), which may\nhinder their applications in the scenarios where representing uncertainty is of crucial im-\nportance, such as automated driving (Bojarski et al., 2016) and healthcare (Miotto et al.,\n2017).1 On the other hand, the probabilistic view of machine learning oﬀers mathematically\ngrounded tools for dealing with uncertainty, i.e., Bayesian methods (Ghahramani, 2015).\nBayesian methods also provide a theoretically sound approach to incorporating structural\nbias (Lake et al., 2015) and domain knowledge as prior or posterior constraints (Mei et al.,\n2014) to achieve eﬃcient learning with a small number of training samples. Thus it is ben-\neﬁcial to combine the complimentary advantages of deep learning and Bayesian methods,\nwhich in fact has been drawing tremendous attention in recent years (Gal, 2016).\nMoreover, as most of the success in deep learning comes from supervised tasks that re-\nquire a large number of labeled data, research in this area is paying more and more attention\nto unsupervised learning, a long-standing goal of artiﬁcial intelligence (AI). Probabilistic\nmodels and Bayesian methods are commonly treated as principled approaches to model-\ning unlabeled data for pure unsupervised learning or its hybrid with supervised learning\n(aka., semi-supervised learning). Recently, the popularity of deep generative models again\ndemonstrates the promise of combining deep neural networks with probabilistic modeling,\nwhich has shown superior results in image generation (Kingma and Welling, 2013; Goodfel-\nlow et al., 2014; Radford et al., 2015), semi-supervised classiﬁcation (Kingma et al., 2014;\nSalimans et al., 2016) and one-shot learning (Rezende et al., 2016).\nWe call such an arising direction that conjoins the advantages of Bayesian methods and\ndeep learning as Bayesian Deep Learning (BDL). The scope of BDL covers the traditional\nBayesian methods, the deep learning methods where probabilistic inference plays a key role,\nand their intersection. One unique feature of BDL is that the deterministic transformation\nbetween random variables can be automatically learned from data under an expressive\nparametric formulation typically using deep neural networks (Johnson et al., 2016), while\nin traditional Bayesian models, the transformation tends to have a simple analytical form\n(e.g., the exponential function or inner product).\nOne key challenge for Bayesian deep\nlearning is on posterior inference, which is typically intractable for such models and needs\nsophisticated approximation techniques. Although much progress has been made on both\nvariational inference and Monte Carlo methods (Zhu et al., 2017), it is still too diﬃcult\nfor practitioners to pick-up.\nMoreover, although variational inference and Monte Carlo\nmethods have their general recipes, it is still quite involved for an experienced researcher to\nderive the equations and implement every detail for a particular model. Such a procedure\nis error prone and takes a long time to debug.\nIn this paper, we present ZhuSuan, a probabilistic programming library for Bayesian\nDeep Learning.2 We build ZhuSuan upon Tensorﬂow (Abadi et al., 2016) to leverage its\ncomputation graphs for ﬂexible modeling. With ZhuSuan, users can enjoy powerful ﬁtting\n1. Some recent eﬀorts have been made towards improving the robustness of deep neural networks against\nadversarial samples by doing adversarial training (Szegedy et al., 2013), reverse cross-entropy train-\ning (Pang et al., 2017) or Bayesian averaging (Li and Gal, 2017).\n2. ZhuSuan is named after the Chinese name of abacus, which is the oldest calculating machine and has\nbeen recognized as the ﬁfth greatest innovation in China.\n2\nZhuSuan: A Library for Bayesian Deep Learning\nand multi-GPU training of deep learning, while at the same time they can use probabilis-\ntic models to model the complex world, exploit unlabeled data and deal with uncertainty\nby applying principled Bayesian inference. We provide running examples to illustrate how\nintuitive it is to program with ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent networks. More ex-\namples can be found in our code repository https://github.com/thu-ml/zhusuan. The\nkey diﬀerence of ZhuSuan from other probabilistic programming libraries lies in its ﬂex-\nibility, beneﬁted from the deep learning paradigm on which the library is built, and the\nkey treatment of model reuse. More detailed comparisons with two closely related works,\nEdward (Tran et al., 2016) and PyMC3 (Salvatier et al., 2016), are provided in this paper.\n2. Bayesian Deep Learning\nWe ﬁrst overview the basics of Bayesian methods and their intersection with deep learning,\nand deﬁne the notations.\n2.1 Modeling\nAs a conjunction of graph theory and probability theory, probabilistic graphical models\n(PGMs) (Koller and Friedman, 2009) provide a powerful language for probabilistic ma-\nchine learning. A PGM deﬁnes a joint distribution of a set of random variables using a\ngraph, which has an intuitive and compact semantic meaning to read out the conditional\nindependence structures. There are two major types of PGMs, namely, directed ones (aka.\nBayesian networks) and undirected ones (aka.\nMarkov random ﬁelds, MRFs).\nThough\nBayesian networks and MRFs have diﬀerent expressiveness, sometimes one can transform\na Bayesian network to an MRF or vice versa (Koller and Friedman, 2009). In ZhuSuan, we\nconcentrate on Bayesian networks, which provide an intuitive data generation process (Gel-\nman et al., 2014). This choice is also reﬂected in the literature of Bayesian deep learning.\nFor example, while modern deep generative models make their ﬁrst remarkable step with\nundirected graphs (Hinton et al., 2006), the use of directed models (Kingma and Welling,\n2013) has dominated the area due to cheap generation and fast inference.\n(a)\n(b)\n(c)\nFigure 1: Bayesian networks\nFigure 1a shows a Bayesian network with a directed acyclic graph (DAG) structure.\nThis represents a joint probability distribution over the nodes that factorizes according to\n3\nShi et al.\nthe graph:\np(Z, H, U, S, A, N) = p(Z)p(H)p(U|Z)p(S|Z, H)p(A|S)p(N|U, A).\n(1)\nThe joint probability is the product of two types of factors: the prior distribution of variables\nat the roots (e.g., p(Z)) and the conditional probability distribution of other variables given\ntheir parents (e.g., p(S|Z, H)). The conditional distributions are also known as conditional\nprobability tables (CPTs) for discrete variables and conditional probability density (CPD)\nfunctions for continuous variables.\nWe consider the general formulation, where there are two kinds of nodes in a Bayesian\nnetwork: deterministic nodes (Figure 1b, right, double line notation) and stochastic nodes\n(Figure 1b, left). A deterministic node calculates its value by a deterministic function of its\nparents, while a stochastic node is represented by a standard probabilistic distribution. Al-\nthough deterministic nodes are often not represented explicitly to make the graph structure\nconcise, it is beneﬁcial to do so especially when we consider Bayesian deep learning models.\nThis is because the transformation in a traditional model is typically in a simple analytical\nform, while a Bayesian deep learning model learns it from a very ﬂexible family (e.g., deep\nneural networks). Hence, explicitly representing deterministic nodes helps on both model\ndeﬁnition and inference, as shall be clear soon.\nIf all the variables are observed in the dataset, it is easy to do learning and inference for\na Bayesian network. However, in reality it is more common to have partially observed data\ndue to physical randomness, missing information and measurement noise, thereby some\nvariables are hidden.\nSuch models are known as latent variable models (LVMs), which\nprovide a suite of useful tools to unveil the underlying factors (e.g., topics or latent feature\nrepresentations). Figure 1c illustrates a generic latent variable model, where the gray nodes\nrepresent the observed variables (x1:n), the rest (h1:n, θ) are latent variables, and n is the\nnumber of observed data samples. This class of models include both global and local latent\nvariables. By local we mean the latent variable only has impact on its paired observation\n(e.g., h is paired with x), while global means it inﬂuences all observations (e.g., θ).\n2.2 Inference\nFor a latent variable model, posterior inference is the key step to unveil the latent factors for\na particular input. To make the notations simple, for this subsection we consider a highly\nabstract model p(z, x) = p(z)p(x|z), where z represents all latent variables, including the\nglobal and local ones, i.e., z = (h, θ), and x denotes observations. The vanilla Bayes’ rule\nprovides a principle to derive the posterior distribution:\np(z|x) = p(z, x)\np(x)\n= p(z)p(x|z)\np(x)\n.\n(2)\nAs analyzed in (Zhu et al., 2014), the vanilla Bayes’ rule can be generalized to regularized\nBayesian inference (RegBayes) by introducing posterior regularization under an information\ntheoretical formulation.\nIn general, the posterior inference using Bayes’ rule or the RegBayes formulation is\nintractable, except a few simple examples. Therefore, we have to resort to approximate\nBayesian inference methods. Many years of developments in this area has led to many fast,\n4\nZhuSuan: A Library for Bayesian Deep Learning\nwidely applicable approximate inference algorithms, mainly divided into two categories,\nvariational inference and Monte Carlo methods (Zhu et al., 2017). Both will be covered\nbelow.\n2.2.1 Variational Inference\nVariational inference (VI) is an optimization-based method for posterior approximation,\nin which a parametric distribution family qφ(z) is chosen to approximate the true poste-\nrior p(z|x) by minimizing the KL divergence between them (KL(qφ(z)∥p(z|x)).3 The KL-\ndivergence minimization is equal to maximizing a lower bound of the marginal log likelihood\nlog p(x):\nL(x; φ) = log p(x) −KL(qφ(z)∥p(z|x))\n= Eqφ(z) [log p(x|z)] −KL (qφ(z)∥p(z)) .\n(3)\nIn the VI literature, qφ(z) is called the variational posterior, and L(x; φ) is called the\nevidence lower bound (ELBO). Generally there are two steps for doing VI. First is to choose\nthe parametric family that will be used as the variational posterior. Then the second step\nis to solve the optimization problem with respect to the variational parameters (φ).\nIn recent years, beneﬁted from the joint eﬀort of the Bayesian and the deep learning\ncommunity, variational inference is undergoing many signiﬁcant changes, both in the algo-\nrithm and the choices of variational families. From the algorithm side, stochastic approxi-\nmation with data sub-sampling has enabled VI to scale to large datasets (Hoﬀman et al.,\n2013). Meanwhile, direct optimization of the variational lower bounds by gradient descent\nis replacing traditional analytic updates, which makes VI applicable to a broader range of\nmodels with non-conjugate dependencies (Graves, 2011; Titsias and L´azaro-Gredilla, 2014).\nThe key challenge that draws most attention in this direction is on the gradient estimates.\nMany gradient estimators have been developed with diﬀerent variance reduction techniques\n(Paisley et al., 2012; Mnih and Gregor, 2014; Mnih and Rezende, 2016). Specially, for con-\ntinuous latent variable models, an algorithm named Stochastic Gradient Variational Bayes\n(SGVB) has been very successful (Kingma and Welling, 2013), due to a clever trick to pass\nthe gradient through a stochastic node, which is now well-known as the reparameterization\ntrick. Besides, better lower bounds than the ELBO have also been developed, which can be\noptimized by gradient descent as well and have been applied to both discrete and continuous\nlatent variable models (Burda et al., 2015; Mnih and Rezende, 2016).\nOn the other side, considerable eﬀorts have also been put into the design of variational\nposteriors.\nUsing neural networks to parameterize the variational posterior has been a\ncommon practice adopted by aforementioned works (Mnih and Gregor, 2014; Kingma and\nWelling, 2013), serving as one of the key technologies to eﬃciently train deep generative\nmodels.\nThe network is usually fed with observations and is expected to amortize the\ninference cost for each data point by learning to do inference.\nThis type of scheme is\noften referred as amortized inference (Rezende and Mohamed, 2015). In summary, VI in\nthe Bayesian deep learning context is more stochastic, diﬀerentiable, and amortized than\nbefore.\n3. We do not explicitly condition z on x in q because it is not necessary to explicitly model this, though\nmany modern literatures do.\nIn fact, even without explicit modeling, the optimization process will\nconnect them together.\n5\nShi et al.\n2.2.2 Monte Carlo Methods\nUnlike VI that reformulates the inference as an optimization problem, Monte Carlo methods\n(Robert and Casella, 2005) are more direct ways to simulate samples or estimate properties\nof a particular distribution. Two main techniques extensively used for Bayesian inference\nare Importance Sampling and Markov Chain Monte Carlo.\nImportance Sampling The basic idea of importance sampling is as follows. To esti-\nmate µ = Epf(x) where p is a probability density deﬁned over Rd, a probability distribution\nq called the proposal is introduced. It is required that q(x) > 0 whenever p(x)f(x) > 0.\nThen\nµ =\nZ\nRd f(x)p(x) dx =\nZ\nRd\nf(x)p(x)\nq(x)\nq(x) dx = Eq\n\u0014f(x)p(x)\nq(x)\n\u0015\n.\n(4)\nThe Monte Carlo estimate of µ given by importance sampling is\nˆµ = 1\nN\nN\nX\ni=1\nf(xi)p(xi)\nq(xi)\n,\nxi ∼q(x).\n(5)\nIn the Bayesian inference context, it is often the case that we only have access to an\nunnormalized version of p, which we denote as ˜p. p = ˜p/Z, where Z is the normalizing\nconstant which is intractable. Self-normalized Importance Sampling is particularly useful\nin this situation, which gives the estimate\n˜µ =\nN\nX\ni=1\n˜wif(xi),\nwhere\n˜wi =\nwi\nPN\nj=1 wj\n, wi = ˜p(xi)\nq(xi).\n(6)\nIt can be proved that ˜µ is an asymptotically unbiased estimator of µ as N →∞. For more\ndetailed introduction, we refer the readers to (Owen, 2013).\nAs introduced above, importance sampling in the strict sense is not a sampling method\nbecause it does not directly draw samples from the target distribution. Instead, it provides\na method for estimating properties of a certain distribution (normalized or not). Thus the\nuse of importance sampling can be everywhere that needs an estimate of some integral over\na probability measure. Though the computation in eqs. (5) and (6) is rather direct, some\nof their application scenarios may not be obvious for users.\nHere we focus on application scenarios in Bayesian deep learning, which concentrate on\nmodel learning and evaluation. For model learning, it is shown in (Bornschein and Ben-\ngio, 2014) that self-normalized importance sampling can be used to estimate gradients of\nmarginal log likelihoods with respect to model parameters, where this technique was pro-\nposed to train deep generative models. Even if not used for learning, importance sampling\nis still a simple and eﬃcient method for evaluating marginal log likelihoods of latent vari-\nable models (Wallach et al., 2009). One of the drawbacks of using importance sampling\nis that the estimation has large variance if the proposal is not good enough (Owen, 2013).\nTherefore people have been exploring the use of adaptive proposals (Capp´e et al., 2008;\nCornuet et al., 2012). This idea is recently reformed into a new technique called neural\nadaptive proposals, i.e., a neural network is used to parameterize the proposal distribution,\nwhich is adapted towards the optimal proposal by gradient descent (Bornschein and Ben-\ngio, 2014). This technique proves to be successful in applications like dynamic systems (Gu\n6\nZhuSuan: A Library for Bayesian Deep Learning\net al., 2015), inference acceleration (Du et al., 2017; Paige and Wood, 2016) and learning\nattention models (Ba et al., 2015).\nMarkov Chain Monte Carlo Markov Chain Monte Carlo (MCMC) is a classic method\nto generate samples from the posterior distribution in Bayesian statistics. Unlike variational\ninference, MCMC is known to be asymptotically unbiased and allows the user to trade oﬀ\ncomputation for accuracy without limit. The basic idea of MCMC is to design a Markov\nchain whose stationary distribution is the target distribution, then samples can be generated\nby simulating the chain until convergence. Speciﬁcally, for a Markov chain speciﬁed by\nthe transition kernel T(z′|z), the most common suﬃcient condition for p(z|x) being its\nstationary distribution is the detailed balance condition:\np(z|x)T(z′|z, x) = p(z′|x)T(z|z′, x),\nfor all z, z′.\nAn arbitrarily chosen initial state z0 and the transition kernel deﬁne a joint distribution\np(z0, . . . , zt, . . . |x) and marginal distributions p(zt|x). It can be shown that under mild\nconditions, p(zt|x) converges to p(z|x) as t →∞(Robert and Casella, 2005). In practice,\nit is common to throw away samples from the initial stage before the chain converges. This\nstage is often referred as the burn-in (or warm-up) stage. Once we get samples from the\nMCMC methods, there are many ways to use them. For parameter estimation of LVMs, the\nsamples can be used in the Monte-Carlo EM (MCEM) algorithm (Wei and Tanner, 1990).\nThroughout the literature there are many ways to design the transition kernel. For\nexample, the simplest form of the Metropolis Hastings algorithm (Metropolis et al., 1953;\nHastings, 1970) combines a Gaussian random walk proposal with an accept-reject test for\ncorrection, which scales poorly with increasing dimension and complexity of the target dis-\ntribution. Gibbs sampling (Geman and Geman, 1984) utilizes the structure of the target\ndistribution by taking its element-wise conditional distribution as the transition proposal.\nHowever, it requires the conditionals to be analytically computable, which limits its appli-\ncation scope, especially in Bayesian deep learning.\nA powerful MCMC method that eﬃciently explores high-dimensional continuous distri-\nbutions is Hamiltonian Monte Carlo (HMC) (Neal et al., 2011). To sample from p(z|x),\nHMC introduces an auxiliary momentum variable p with the same dimensionality as z,\nand eﬀectively samples from the joint distribution p(z, p|x) = p(z|x) exp(−1\n2p⊤M−1p),\nwhere M is called the mass matrix. Samples are generated by simulating the Hamiltonian\ndynamics, which governs the evolution of the (z, p) system along continuous time t as:\n∂z\n∂t = ∇pH,\n∂p\n∂t = −∇zH.\n(7)\nHere H(z, p) = −log p(z, p|x) is the Hamiltonian of the system. The Hamiltonian dy-\nnamics deﬁnes a continuous-time transition kernel, and the stationary distribution of the\ncorresponding Markov chain is the desired p(z, p|x), due to the volume-preservation and\nthe Hamiltonian-conservation properties of the dynamics.\nIn practice one has to simu-\nlate the dynamics in discrete time steps. The leapfrog integrator (Leimkuhler and Reich,\n2004) is widely used, since it keeps the two vital properties of the Hamiltonian dynam-\nics: it is volume-preserving and approximately Hamiltonian-conserving. Combined with\na Metropolis-Hastings algorithm to correct the approximating error of the Hamiltonian,\nthe discrete-time simulation comes to our satisfactory: the Markov chain converges to our\n7\nShi et al.\ndesired distribution. The key advantage of HMC is that it exploits information about the\ngeometry of the target distribution (Betancourt, 2017) while only needs the distribution\ndensity and the gradient to proceed.\nPeople have been exploring the use of HMC in Bayesian deep learning. The early work\nby Radford Neal (Neal, 1995) applied HMC to the inference of Bayesian neural networks,\none of the representative models that apply Bayesian methods to capture uncertainty in\ndeep learning. Recent works on deep generative models have applied HMC to improve the\nvariational posterior (Salimans et al., 2015) as well as used HMC-based MCEM algorithm\nto directly learn the model parameters (Hoﬀman, 2017; Li et al., 2017; Titsias, 2017).\n3. Design Overview\nZhuSuan has been designed towards the basic needs of Bayesian deep learning, which, as\nstated in the last section, mainly include two parts: modeling and inference. As for the\nmodeling part, we follow the principle that the code reads like the model deﬁnition. This\nrequires model primitives that support:\n• Direct building of probabilistic models as computation graphs.\n• Easy reuse by changing the states of stochastic nodes (observed or not).\n• Arbitrary deterministic modeling with the full power of Tensorﬂow.\nOn the side of inference, to leverage recent advances in Bayesian deep learning while staying\napplicable to a broad class of models, ZhuSuan makes eﬀorts towards:\n• Unifying traditional and advanced diﬀerentiable inference methods in a deep learning\nparadigm.\n• Supporting inference for both continuous latent variables and discrete ones.\nAlso from users’ perspective, the design of ZhuSuan has been governed by two principles 4:\n• Modularity: Make ﬂexible abstractions that allow all parts to be used independently.\n• Transparency: Do not hide the whole inference procedure behind abstractions, to\nenable heavy customization by users.\nThis makes ZhuSuan particularly diﬀerent from existing probabilistic programming li-\nbraries, which will be discussed in later comparison (Section 5).\n4. Features\nIn this section we introduce the basic features of ZhuSuan, including model primitives and\ninference algorithms.\n4. Both principles are learned from Lasagne (Dieleman et al., 2015).\n8\nZhuSuan: A Library for Bayesian Deep Learning\n4.1 Model Primitives\nFor the reasons we have explained in Section 2.1, ZhuSuan’s model primitives are designed\nfor Bayesian networks. Since the basic structure is a DAG, we will introduce the node\nprimitives ﬁrst, and then will see how ZhuSuan stores and fetches information in the graph.\nDeterministic nodes In ZhuSuan, users are enabled to use any Tensorﬂow operation\nfor deterministic transformations.\nThis includes various arithmetic operators (e.g., tf.\nadd,tf.tensordot,tf.matrix_inverse), neural network layers (e.g., tf.layers.fully_\nconnected,tf.layers.conv2d) and even control ﬂows (e.g., tf.while_loop,tf.cond). In\nTensorﬂow computation graph, the output of an operation is named Tensor. ZhuSuan does\nnot add any higher-level abstraction on Tensors but directly treat them as deterministic\nnodes in the Bayesian networks. We will see that the other primitives work well directly\nwith Tensors.\nStochastic nodes For stochastic nodes, ZhuSuan provides an abstraction called Stocha\nsticTensor, which is named following its deterministic counterpart. Many commonly used\nprobabilistic distributions are implemented and wrapped in StochasticTensors (e.g., Nor-\nmal, Bernoulli, Categorical), together with some recently developed variants for Bayesian\ndeep learning, such as Gumbel-softmax or Concrete (Jang et al., 2016; Maddison et al.,\n2016). StochasticTensors inherit most behaviors of Tensors. The former can be directly\nfed into Tensorﬂow operations and can automatically be cast into Tensors when computing\nwith them. When cast into Tensors, the values they choose can be set to samples or obser-\nvations, or can be automatically determined according to their states given in the context,\nwhich will be introduced next.\nThe graph context Above we see a Bayesian network can be built transparently with\nmixes of Tensorﬂow operations and StochasticTensors. Because large and sophisticated\nmodels are generally common today, it is still painful for one to deal with all these nodes\nindividually. To help users manage the graph in a convenient way, ZhuSuan provides a\ngraph context called the BayesianNet, which keeps track of all named StochasticTensors\nconstructed in it.\nTo start a BayesianNet context, use the with statement in python:\nimport zhusuan as zs\nwith zs.BayesianNet() as model:\n# Build the graph.\nThe BayesianNet context supports making queries on the inner stochastic nodes.\nThe\nquery options include the current-state outputs and local probabilities (the current-state\nvalue of CPDs) at certain nodes.\nWe use several representative examples to illustrate the process of building models in\nZhuSuan, ranging from the simple Bayesian logistic regression to modern Bayesian deep\nmodels. In general, the programming of a probabilistic model on ZhuSuan is as intuitive as\nreading the corresponding graphical model, which is provided side-by-side.\nExample 1 (Bayesian Logistic Regression, BLR) The generative process of a Bayesian\nlogistic regression can be written as:\nw ∼N(0, α2I),\nyi ∼Bernoulli(σ(wT xi)),\ni = 1, . . . , n,\n(8)\n9\nShi et al.\nwhere w, xi ∈RD, yi ∈{0, 1}, and σ(·) is the sigmoid function. The graphical model and\nthe corresponding code is shown in Figure 2. Note that data is modeled in a batch.\n(a)\nimport tensorflow as tf\nwith zs.BayesianNet() as model:\n# Define inputs and parameters\nx = tf.placeholder([None, D], tf.float32)\nlog_alpha = tf.Variable(tf.zeros([D]))\n# w ~ N(0, alpha^2*I)\nw = zs.Normal('w', mean=0., logstd=log_alpha,\ngroup_ndims=1)\n# y_logit = w^Tx\ny_logit = tf.reduce_sum(\ntf.expand_dims(w, 0)*x, axis=1)\n# y ~ Bernoulli(sigmoid(y_mean))\ny = zs.Bernoulli('y', y_logit)\n(b)\nFigure 2: BLR: (a) graphical model; (b) programming on ZhuSuan.\nIn this example the deterministic transformation part is the linear model (wT x), which\nis implemented by the Tensorﬂow operations, tf.expand_dims,tf.reduce_sum and tf.\nmultiply (*) to enable batch processing of the inputs (xi, i = 1, . . . , n). The two random\nvariables y and w are created as zs.Bernoulli and zs.Normal respectively, which are the\nStochasticTensor for Bernoulli and normal distributions. The group_ndims argument\npassed to specify the StochasticTensor for w means that the last dimension of w is treated\nas a group, whose probabilities are computed together.\nExample 2 (Variational Auto-Encoders, VAE) Variational autoencoders (VAE) is one\nof the most popular products of Bayesian deep learning. The generative process of a VAE\nfor modeling binarized MNIST data is\nz ∼N(0, I),\nxlogits = fNN(z),\nx ∼Bernoulli(σ(xlogits)),\n(9)\nwhere z ∈RD, x ∈R784. This generative process is a stereotype for deep generative models,\nwhich starts with a latent representation (z) sampled from a simple distribution (such as the\nstandard normal). Then the samples are forwarded through a deep neural network (fNN) to\ncapture the complex generative process of high dimensional observations such as images (x).\nFinally, some noise is added to the output to get a tractable likelihood for the model. For\nbinarized MNIST, the observation noise is chosen to be Bernoulli, with its parameters output\nby the neural network. Because ZhuSuan is built upon Tensorﬂow, which has full support\nfor deep neural networks, the implementation of VAE is extremely easy and intuitive, as\nshown in Figure 3.\n10\nZhuSuan: A Library for Bayesian Deep Learning\n(a)\nfrom tensorflow.contrib import layers\nwith zs.BayesianNet() as model:\nz_mean = tf.zeros([n, D])\nz = zs.Normal('z', z_mean, std=1., group_ndims=1)\nh = layers.fully_connected(z, 500)\nx_logits = layers.fully_connected(h, 784,\nactivation_fn=None)\nx = zs.Bernoulli('x', x_logits, group_ndims=1)\n(b)\nFigure 3: VAE: (a) graphical model; (b) programming on ZhuSuan.\nExample 3 (Deep Sigmoid Belief Networks, DSBN) Sigmoid Belief Networks (SBN)\nis a directed discrete latent variable model that has close connections with feed-forward neu-\nral networks and Boltzmann Machines (Neal, 1992). In recent years, the return of neural\nnetworks has brought a new life to this old model. In fact, the well-known Deep Belief Net-\nworks (DBN), the earliest work on deep learning, is an inﬁnite-layer tied-weight SBN with\nthe bottom layers untied (Hinton et al., 2006). The generative process of a DSBN with L\nlayers is\nz(L) ∼Bernoulli(σ(p(L))),\nz(l−1) ∼Bernoulli(σ(w(l)T z(l))),\nl = L, . . . , 1\nx = z(0),\n(10)\nwhere σ is the sigmoid function; p(L) is the top layer parameters; w are hidden weights; z\nare latent variables, and x are observations. From the deﬁnition we can see that DSBN is a\nmodel with multi-layer stochastic nodes. The implementation of a two-layer DSBN (L = 2)\nis shown in Figure 4.\n(a)\nwith zs.BayesianNet() as model:\nz2_logits = tf.zeros([n, n_z])\nz2 = zs.Bernoulli('z2', z2_logits, dtype=tf.float32,\ngroup_ndims=1)\nz1_logits = layers.fully_connected(z2, n_z,\nactivation_fn=None)\nz1 = zs.Bernoulli('z1', z1_logits, dtype=tf.float32,\ngroup_ndims=1)\nx_logits = layers.fully_connected(z1, n_x,\nactivation_fn=None)\nx = zs.Bernoulli('x', x_logits, group_ndims=1)\n(b)\nFigure 4: DSBN: (a) graphical model; (b) programming on ZhuSuan.\n11\nShi et al.\nTo provide a more interesting example that utilizes the powerful control ﬂow operations\nfrom Tensorﬂow, we describe a Bayesian Recurrent Neural Network (Bayesian RNN) below,\nwhich can also be intuitively programmed on ZhuSuan.\nExample 4 (Bayesian RNN) We have mentioned previously that deterministic neural\nnetworks lack the ability to account for the uncertainty of its own predictions. A solution to\nthis given by Bayesian methods is a model named Bayesian Neural Network (Bayesian NN),\nwhich treats the network weights as random variables and infers a posterior distribution over\nthem given data. The generative process of a Bayesian NN for classiﬁcation tasks is\nW ∼N(0, I),\nπ = fNN(x; W),\ny ∼Cat(softmax(π)),\n(11)\nwhere fNN is a neural network with W as the weights, π is the predicted unnormalized log\nprobabilities for all classes, Cat represents the categorical distribution, and y is the class\nlabel. When fNN is chosen to be a recurrent network, the above process describes a Bayesian\nRNN. The graphical model for a Bayesian RNN is shown in Figure 5. Below we consider\na model for a two-class sequence classiﬁcation task.\nFor the RNN part, it uses a Long\nShort-Term Memory (LSTM) network.\nFigure 5: Bayesian RNN\nclass BayesianLSTMCell(object):\ndef __init__(self, num_units, forget_bias=1.0):\nself._forget_bias = forget_bias\nw_mean = tf.zeros([2 * num_units + 1, 4 * num_units])\nself._w = zs.Normal('w', w_mean, std=1., group_ndims=2)\ndef __call__(self, state, inputs):\nc, h = state\nbatch_size = tf.shape(inputs)[0]\nlinear_in = tf.concat([inputs, h, tf.ones([batch_size, 1])], axis=1)\nlinear_out = tf.matmul(linear_in, self._w)\n12\nZhuSuan: A Library for Bayesian Deep Learning\n# i = input_gate, j = new_input, f = forget_gate, o = output_gate\ni, j, f, o = tf.split(value=linear_out, num_or_size_splits=4, axis=1)\nnew_c = (c * tf.sigmoid(f + self._forget_bias) +\ntf.sigmoid(i) * tf.tanh(j))\nnew_h = tf.tanh(new_c) * tf.sigmoid(o)\nreturn new_c, new_h\ndef bayesian_rnn(cell, inputs, seq_len):\ninitializer = (tf.zeros([batch_size, 128]), tf.zeros([batch_size, 128]))\nc_list, h_list = tf.scan(cell, inputs, initializer=initializer)\nrelevant_outputs = tf.gather_nd(\nh_list, tf.stack([seq_len - 1, tf.range(batch_size)], axis=1))\nlogits = tf.squeeze(tf.layers.dense(relevant_outputs, 1), -1)\nreturn logits\nwith zs.BayesianNet() as model:\ncell = BayesianLSTMCell(128, forget_bias=0.)\nlogits = bayesian_rnn(cell, self.x, self.seq_len)\n_ = zs.Bernoulli('y', logits, dtype=tf.float32)\nThe code splits into three parts. First we build a BayesianLSTMCell that applies trans-\nformations to the inputs and the hidden states at each time step. Its only diﬀerence from\nthe tf.nn.rnn_cell.BasicLSTMCell in Tensorﬂow is that the weights are generated from a\nnormal StochasticTensor. Then the tf.scan operation is used to apply the cell function to\ninput sequences. This part utilizes the power of control ﬂows to deal with the variable-length\nsequences (tf.scan is internally implemented by the tf.while_loop operator). Finally, a\nBernoulli StochasticTensor generates the class label given the outputs from the RNN.\nModel reuse Unlike supervised neural networks, a key feature of probabilistic graphical\nmodels is polymorphism, i.e., a stochastic node can have two states: observed or latent.\nThis is a major diﬃculty when designing programming primitives. For example, consider\nthe above VAE case. If z is a Tensor sampled from the Normal distribution, when the model\nis again used in a case where z is observed, the common practice in Tensorﬂow programming\nis to write another piece of code that builds a graph with the observation of z as the input.\nWhen the set of stochastic nodes is large, the process is very painful.\nThe reusability problem is a shared problem for probabilistic programming libraries\nthat are based on computation graph toolkits, e.g., Theano (Al-Rfou et al., 2016) and Ten-\nsorﬂow (Abadi et al., 2016). Other libraries such as PyMC3 (Salvatier et al., 2016) and\nEdward (Tran et al., 2016) address this problem by directly manipulating the created com-\nputation graph. Speciﬁcally, PyMC3 (based on Theano) relies on the oﬃcially supported\ntheano.clone() function to copy and recreate subgraphs that are inﬂuenced when the state\nof a node changes. While Edward (based on Tensorﬂow) has to implement their own copy()\nfunction by looking into nonpublic low-level APIs of the Tensorﬂow computation graph. As\nwe will see later, the solutions by manipulating the created graphs induce problems and\nlimitations for these libraries.\n13\nShi et al.\nZhuSuan has been carefully designed towards reusability, but does not rely on altering\nthe created graphs. Speciﬁcally, an observation can be passed to StochasticTensor di-\nrectly, which enables model reuse by repeated calls of a function with diﬀerent arguments\npassed. Below is an illustration example:\ndef build_model(x_obs=None):\nx = zs.Normal('x', observed=a_obs)\nreturn f(x)\n# samples from the Normal distribution will be used for computation in f\nout = build_model()\n# x_obs will be used for computation in f\nout = build_model(x_obs=x_obs)\nFor models that have many stochastic nodes (e.g., x1,...,xN), in principle the poly-\nmorphism can be achieved by\ndef build_model(x1_obs=None, ..., xN_obs=None):\n...\nHowever, in practice this is often found to be redundant and makes it hard for automatic\ninference algorithms to ﬁt each model. This is where the BayesianNet context makes a\nbig diﬀerence. BayesianNet accepts an argument named observed, which is a dictionary\nmapping from names of StochasticTensors to their observations. Then the context will\nbe responsible for determining the states of StochasticTensors.\nThis makes it easier\nfor users to handle state changes of large sets of stochastic nodes, and more importantly,\nenables a uniﬁed form of model building functions that makes automatic inference possible.\nAn example is shown below.\ndef build_model(observed=None):\nwith zs.BayesianNet(observed=observed) as model:\nz = zs.Normal('z')\nx = zs.Normal('x', f(z))\n...\nreturn model\n# No observation\nm = build_model()\n# Observe z and x\nm = build_model({'z': z_obs, 'x': x_obs})\n4.2 Inference Algorithms\nIn Section 2.2 we have covered the major inference methods used in Bayesian deep learning.\nAlthough most of them are gradient-based, stochastic, and black-box (Ranganath et al.,\n2014), which is suitable for building a general inference framework, there is currently no\nsoftware that provides a complete support of them. To bridging the gap, ZhuSuan leverages\nrecent advances of diﬀerentiable inference in Bayesian deep learning, and provides a wide\nand ﬂexible support for both traditional and modern inference algorithms. These algorithms\nare provided in an API that ﬁts well into deep learning paradigms, which makes inference\n14\nZhuSuan: A Library for Bayesian Deep Learning\nObjective\nGradient\nestimator\nSupported latent-variable types\nImplementations\nin zs.variational\nELBO\nSGVB\n• continuous and reparameterizable\n• Concrete relaxation of discrete\nelbo().sgvb\nREINFORCE all types\nelbo().\nreinforce\nImportance\nweighted\nobjective\nSGVB\n(IWAE)\n• continuous and reparameterizable\n• Concrete relaxation of discrete\niw_objective().\nsgvb\nVIMCO\nall types\niw_objective().\nvimco\nKL(p∥q)\nRWS\nall types\nklpq().rws\nTable 1: Variational inference in ZhuSuan. Relevant references are SGVB (Kingma and\nWelling, 2013), REINFORCE (Williams, 1992; Mnih and Gregor, 2014), IWAE (Burda\net al., 2015), VIMCO (Mnih and Rezende, 2016), and RWS (Bornschein and Bengio, 2014).\nas easy as doing gradient descent in deterministic neural networks. Below, we use examples\nto explain both variational inference and Monte Carlo methods in ZhuSuan.\n4.2.1 Variational Inference\nAs reviewed in Section 2.2.1, a variational inference (VI) algorithm consists of two parts:\nthe construction of variational posteriors and the optimization of variational objectives.\nTypical VI implementations in probabilistic programming languages have mostly re-\nstricted themselves to simple variational posteriors.\nFor example, the ADVI algorithm\n(Kucukelbir et al., 2017) that serves as the main VI method in Stan (Carpenter et al.,\n2017) uses only Gaussian variational posteriors. In contrast, ZhuSuan supports building\nvery ﬂexible variational posteriors by leveraging Bayesian networks.\nThis opens up the\ndoor for rich variational families with user-speciﬁed dependency structures.\nAs for the optimization side, as mentioned in Section 2.2.1, many gradient-based vari-\national methods have emerged in recent progress of Bayesian deep learning (Kingma and\nWelling, 2013; Mnih and Gregor, 2014; Burda et al., 2015; Mnih and Rezende, 2016). These\nmethods diﬀer in the variational objectives and the gradient estimators they use. To make\nthem more automatic and easier to handle, ZhuSuan has wrapped them all into single func-\ntions, which computes a surrogate cost for users to directly take derivatives on. This means\nthat optimizing these surrogate costs is equally optimizing the corresponding variational\nobjectives using their well-developed gradient estimators. The currently supported varia-\n15\nShi et al.\ntional methods are summarized in Table 1. They are grouped by the objective they aim to\noptimize. Currently there are three kinds of objectives supported: the ELBO (equivalent\nto KL(q∥p)), the importance weighted objective (Burda et al., 2015) and KL(p∥q), where\np, q denotes the true and the variational posterior.\nIt is easy to program with ZhuSuan to implement a variational inference algorithm,\nfollowing these steps:\n1) Build the variational posterior using ZhuSuan’s modeling primitives.\n2) Get samples and their log probabilities from the variational posterior.\n3) Provide the log joint function of the model and call variational objectives.\n4) Choose the gradient estimator to use and get the surrogate cost to minimize.\n5) Call Tensorﬂow optimizers to run gradient descent on the surrogate cost.\nBelow we use a simple example to guide the readers through all these steps.\nExample 5 (BLR, continued) We consider applying variational inference to the BLR\nmodel in Example 1, following the above steps:\n1) Build the variational posterior. The true posterior of w is intractable and should have\ncorrelations across dimensions. Here we follow the common practice to make the mean-ﬁeld\nassumption that the variational posterior q(w) is factorized, i.e., q(w) = QD\nd=1 q(wd), where\nD is the dimension of the weights. The code for using factorized normal distribution as the\nvariational posterior is:\nwith zs.BayesianNet() as variational:\nw_mean = tf.Variable(tf.zeros([D]))\nw_logstd = tf.Variable(tf.zeros([D]))\nw = zs.Normal('w', w_mean, logstd=w_logstd, group_ndims=1)\n2) Get samples and their log probabilities from the variational posterior.\nThis is by\nquerying the stochastic node w about its outputs and local log probabilities.\nqw_samples, log_qw = variational.query(\n'w', outputs=True, local_log_prob=True)\n3) Provide the log joint function of the model and call variational objectives. The log\njoint value can be computed easily by taking sum of log probabilities at individual nodes of\nthe model:\ndef log_joint(observed):\nmodel = blr(observed, x)\nlog_py_xw, log_pw = model.local_log_prob(['y', 'w'])\nreturn log_py_xw + log_pw\nThen we call the ELBO objective, passing the log joint function, observed data and outputs\nof the variational posterior.\nlower_bound = zs.variational.elbo(log_joint,\nobserved={'y': y},\nlatent={'w': [qw_samples, log_qw]})\n16\nZhuSuan: A Library for Bayesian Deep Learning\n4) Choose the gradient estimator to use and get the surrogate cost to minimize. Because\nw is continuous and can be reparameterized as w = ϵσw + µw, where µw and σw are the\nmean and the standard deviation of the normal distribution, we choose the SGVB gradi-\nent estimator (see Table 1 for the scope of application of each estimator). The computed\nsurrogate costs are for a batch of data, which is then averaged.\ncost = tf.reduce_mean(elbo.sgvb())\n5) Call Tensorﬂow optimizers to run gradient descent on the surrogate cost. As previ-\nously explained, this will be optimizing the ELBO objective with the SGVB gradient estima-\ntor. We can also fetch the ELBO value to verify it.\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001)\ninfer_op = optimizer.minimize(cost)\nwith tf.Session() as sess:\nfor i in range(iters):\n_, elbo_value = sess.run([infer_op, elbo])\nThe above is a very simple example using factorized variational posteriors, while as\nmentioned previously, ZhuSuan supports building very ﬂexible variational posteriors. In\nthe following example, we will see how to do amortized inference (see Section 2.2.1) by\nleveraging neural networks in the variational posterior.\nExample 6 (VAE, continued) Consider the VAE model in Example 2. The key diﬀer-\nence compared to BLR (Example 1) is that VAE’s latent variables (z) are local instead of\nglobal. As the number of local latent variables scales linearly with the number of observations\n(x), it is hard to aﬀord a separate variational posterior for each of them. This is where the\namortized style of inference becomes useful. Speciﬁcally, we use a neural network with x\nas input to generate parameters of the variational posterior for each corresponding z. This\nnetwork is often referred as decoder or recognition model in VAE. The code is shown in\nFigure 6.\n(a)\nwith zs.BayesianNet() as variational:\nh = layers.fully_connected(x, n_h)\nz_mean = layers.fully_connected(h, n_z,\nactivation_fn=None)\nz_logstd = layers.fully_connected(h, n_z,\nactivation_fn=None)\nz = zs.Normal('z', z_mean, logstd=z_logstd,\ngroup_ndims=1)\n(b)\nFigure 6:\nVAE’s variational posterior:\n(a) graphical illustration; (b) programming on\nZhuSuan.\nHaving built the variational posterior, we can follow the above steps to ﬁnish the inference\nprocedure. Note that because the variational distribution for z is again normal, we can use\n17\nShi et al.\nthe SGVB gradient estimator for optimizing the ELBO objective.\nWe omit these steps\nbecause they are very similar to those illustrated in the previous example.\nIn deep, hierarchical models, there are often conditional dependencies between latent\nvariables that need consideration during inference. We use the DSBN model from Example 3\nto illustrate how to introduce structured dependencies when building variational posteriors\nfor hierarchical models.\nThis example also demonstrates applying VI to discrete latent\nvariables in ZhuSuan.\nExample 7 (DSBN, continued) A commonly used variational posterior in recent works\nfor the DSBN model is\nq(z(1:L)) =\nL\nY\nl=2\nq(z(l)|z(l−1))q(z(1)|x).\n(12)\nwhich has the same conditional dependency structure with the original model given x is\nobserved. The graphical model as well as the code for this structured posterior (when L = 2)\nare shown in Figure 7. It can be seen from the code that z(2) directly depends on z(1) by\n(a)\nwith zs.BayesianNet() as variational:\nz1_logits = layers.fully_connected(x, n_z,\nactivation_fn=None)\nz1 = zs.Bernoulli('z1', z1_logits, dtype=tf.float32)\nz2_logits = layers.fully_connected(z1, n_z,\nactivation_fn=None)\nz2 = zs.Bernoulli('z2', z2_logits, dtype=tf.float32)\n(b)\nFigure 7: DSBN’s variational posterior: (a) graphical illustration; (b) programming on\nZhuSuan.\na fully connected layer in neural networks. Note that here z(2) and z(1) are discrete latent\nvariables. In ZhuSuan there are generally two ways for doing variational inference with\nthem.\nOne way is to use the Concrete relaxation, or Gumbel-softmax trick (Maddison et al.,\n2016; Jang et al., 2016), namely, we use Concrete random variables to replace the Bernoulli\nones during training. Due to the Concrete distribution is continuous and reparameterizable,\nwe can keep on using the SGVB estimator. In test time, we switch back to the Bernoulli\nrandom variables using the same input parameters. Both the model and the posterior in this\nway are modiﬁed a little:\n# model with Concrete relaxation\nwith zs.BayesianNet() as model:\nz2_logits = tf.zeros([n, n_z])\n18\nZhuSuan: A Library for Bayesian Deep Learning\nif is_training:\nz2 = zs.BinConcrete('z2', z2_logits, group_ndims=1)\nelse:\nz2 = zs.Bernoulli('z2', z2_logits, group_ndims=1, dtype=tf.float32)\nz1_logits = layers.fully_connected(z2, n_z, activation_fn=None)\nif is_training:\nz1 = zs.BinConcrete('z1', z1_logits, group_ndims=1)\nelse:\nz1 = zs.Bernoulli('z1', z1_logits, group_ndims=1, dtype=tf.float32)\nx_logits = layers.fully_connected(z1, n_x, activation_fn=None)\nx = zs.Bernoulli('x', x_logits, group_ndims=1)\n# posterior with Concrete relaxation\nwith zs.BayesianNet() as variational:\nz1_logits = layers.fully_connected(x, n_z, activation_fn=None)\nif is_training:\nz1 = zs.BinConcrete('z1', z1_logits, group_ndims=1)\nelse:\nz1 = zs.Bernoulli('z1', z1_logits, group_ndims=1, dtype=tf.float32)\nz2_logits = layers.fully_connected(z1, n_z, activation_fn=None)\nif is_training:\nz2 = zs.BinConcrete('z2', z2_logits, group_ndims=1)\nelse:\nz2 = zs.Bernoulli('z2', z2_logits, group_ndims=1, dtype=tf.float32)\nThe other approach is to directly use a gradient estimator that applies to discrete latent\nvariables. This includes REINFORCE and VIMCO. Here we present the code for using\nVIMCO. Because VIMCO is optimizing the importance weighted bound, which is a multi-\nsample bound, the model and the variational posterior need to be changed to multi-sample\nversions as shown below.\n# model (multi-sample version)\nwith zs.BayesianNet() as model:\nz2_logits = tf.zeros([n, n_z])\nz2 = zs.Bernoulli('z2', z2_logits, group_ndims=1, dtype=tf.float32,\nn_samples=n_samples)\nz1_logits = layers.fully_connected(z2, n_z, activation_fn=None)\nz1 = zs.Bernoulli('z1', z1_logits, group_ndims=1, dtype=tf.float32)\nx_logits = layers.fully_connected(z1, n_x, activation_fn=None)\nx = zs.Bernoulli('x', x_logits, group_ndims=1)\n# posterior (multi-sample version)\nwith zs.BayesianNet() as variational:\nz1_logits = layers.fully_connected(x, n_z, activation_fn=None)\nz1 = zs.Bernoulli('z1', z1_logits, group_ndims=1, dtype=tf.float32,\nn_samples=n_samples)\nz2_logits = layers.fully_connected(z1, n_z, activation_fn=None)\nz2 = zs.Bernoulli('z2', z2_logits, dtype=tf.float32)\nNote the n_samples argument added in the code. This time we call the VIMCO gradient\nestimator to get the surrogate cost:\n19\nShi et al.\nlower_bound = zs.variational.iw_objective(\nlog_joint, observed={'x': x}, latent=qz_outputs, axis=0)\ncost = tf.reduce_mean(lower_bound.vimco())\nAll samples along the axis 0 will be used to obtain variance reduced gradient estimates.\n4.2.2 Monte Carlo Methods\nIn addition to variational inference, eﬀorts have also been made towards unifying Monte\nCarlo methods in the Bayesian deep learning context. We have covered the basics in Sec-\ntion 2.2.2. Below we walk through ZhuSuan’s support for both importance sampling and a\nblack-box MCMC method: Hamiltonian Monte Carlo.\nImportance Sampling As reviewed in Section 2.2.2, the major application scenarios\nfor importance sampling in Bayesian deep learning include model learning and evaluation.\nFor model learning, we have introduced that self-normalized importance sampling can be\nused to estimate the gradients of marginal log likelihoods with respect to model parameters\n(Bornschein and Bengio, 2014). It turns out that the gradients estimated in this way are\nequivalent to exact gradients of the importance weighted variational objective (mentioned\nin Section 4.2.1) with respect to model parameters. So the model learning procedure with\nimportance sampling can be implemented by directly optimizing the importance weighted\nobjective (zs.variational.iw_objective(), see Table 1) with respect to the model pa-\nrameters:\nlower_bound = zs.variational.iw_objective(\nlog_joint, observed, latent, axis=axis)\noptimizer = tf.train.AdamOptimizer(learning_rate)\nlearning_op = optimizer.optimize(lower_bound, var_list=model_params)\nThe only diﬀerence is that the meaning of the variational posterior has changed to the\nproposal distribution.\nBesides learning model parameters, we have mentioned that importance sampling is\nextensively used for model evaluation. Towards this need, ZhuSuan’s evaluation module\nprovides an is_loglikelihood() function for estimating the marginal log likelihoods of\nany given observations using simple importance sampling:\nll = zs.is_loglikelihood(log_joint, observed, latent, axis=axis)\nUntil now we haven’t covered how to build a proposal distribution for importance sam-\npling. In fact, this procedure is exactly the same as that of building a variational posterior.\nWith ZhuSuan’s modeling primitives, neural adaptive proposals mentioned in Section 2.2.2\ncan be easy to implement by leveraging neural networks in the proposal distribution. Adapt-\ning the proposal for the above two scenarios is also straightforward, where the true posterior\ndistribution is often an ideal choice. So the adaptation turns out a variational inference\nproblem, which can be solved by choosing an appropriate method in Table 1. Specially,\nwhen the KL(p∥q) objective is chosen and the gradient estimation is by importance sam-\npling, this recovers the method used in Reweighted Wake-Sleep (RWS) (Bornschein and\nBengio, 2014), which is why this estimator is named rws in ZhuSuan. We illustrate the\nprocess of training DSBN by importance sampling in the following example.\n20\nZhuSuan: A Library for Bayesian Deep Learning\nExample 8 (DSBN, continued) In this example we reproduce the algorithm proposed\nin the Reweighted Wake-Sleep paper (Bornschein and Bengio, 2014), which learns DSBN\nby importance sampling and a neural adaptive proposal. The code snippet below follows\nfrom the multi-sample version of the model and the posterior (now used as the proposal) in\nExample 7, and we omit the code to draw samples and compute their log probabilities from\nthe proposal distribution.\n# learning model parameters\nlower_bound = tf.reduce_mean(\nzs.variational.importance_weighted_objective(\nlog_joint, observed={'x': x_obs}, latent=latent, axis=0))\nmodel_grads = optimizer.compute_gradients(-lower_bound, model_params)\n# adapting the proposal\nklpq_obj = zs.variational.klpq(\nlog_joint, observed={'x': x_obs}, latent=latent, axis=0)\nklpq_cost = tf.reduce_mean(klpq_obj.rws())\nklpq_grads = optimizer.compute_gradients(klpq_cost, proposal_params)\ninfer_op = optimizer.apply_gradients(model_grads + klpq_grads)\nWe can see that the code clearly has two parts. The ﬁrst part is for learning model parame-\nters, by optimizing the importance weighted objective with respect to model parameters. The\nsecond part is for adapting the proposal, by minimizing the inclusive KL divergence between\nthe true posterior (the ideal proposal) and the current proposal. As the training proceeds, the\nadaptation part will help maintain a good proposal, reducing the variance of the marginal\nlog likelihood estimate by importance sampling.\nHamiltonian Monte Carlo In Section 2.2.2 we have brieﬂy analyzed existing MCMC\nmethods and have identiﬁed HMC as a powerful tool to address the posterior inference prob-\nlem in high-dimensional spaces and non-conjugate models, which is a perfect ﬁt for Bayesian\ndeep learning. However, in practice this algorithm involves lots of technical details and can\nbe hard to implement in a fast and eﬃcient way. Besides, despite all tuning parameters in\nHMC have clear physical meanings, it is still hard for users to tune them by hand because\nthe optimal choice always depends on unknown statistics of the underlying distribution.\nFor example, the mass matrix describes the variance of the underlying distribution, which\nis hard to know before we can draw samples from it.\nIn recent years there has been a rise of practical algorithms and high-performance soft-\nwares that target at these problems. The No-U-Turn Sampler, or NUTS (Hoﬀman and\nGelman, 2014) proposes to automatically determine the number of leapfrog steps. It also\ncomes along with a dual averaging scheme for automatically tuning the step size. The HMC\nimplementation in Stan (Carpenter et al., 2017) also includes a procedure that estimates\nthe mass matrix from the samples drawn in the warm-up stage.\nZhuSuan has learned from the existing libraries and provides a fast, automatic, and\ndeep-learning style implementation of HMC. Speciﬁcally, ZhuSuan’s HMC has the following\nfeatures:\n• Support running multiple chains in parallel on CPUs or GPUs.\n21\nShi et al.\n• Provide options for automatically tuning parameters, including the step size and the\nmass matrix. The NUTS algorithm for determining leapfrog steps is not included\nbecause it’s a recursive algorithm and each separate chain can have diﬀerent leapfrog\nsteps, thus hard to have a parallel implementation in static computation graphs.\n• The algorithm is provided in an API very similar to Tensorﬂow optimizers, which is\nillustrated in Figure 8. We hope it is easy to start with for people who are familiar\nwith deep learning libraries.\nz = tf.Variable(0.)\nhmc = zs.HMC(step_size=1e-3,\nn_leapfrogs=10)\nsample_op, hmc_info = hmc.sample(\nlog_joint, observed={'x': x},\nlatent={'z': z})\nwith tf.Session() as sess:\nfor i in range(iters):\n_ = sess.run(sample_op)\n(a) Using HMC in ZhuSuan\nz = tf.Variable(0.)\noptimizer = tf.train.AdamOptimizer(\nlearning_rate=1e-3)\noptimize_op = optimizer.minimize(\ncost(z))\nwith tf.Session() as sess:\nfor i in range(iters):\n_ = sess.run(optimize_op)\n(b) Using Tensorﬂow optimizers\nFigure 8: ZhuSuan’s HMC vs. Tensorﬂow optimizers\n5. Comparison\nWe compare ZhuSuan with the representatives of python probabilistic programming li-\nbraries, namely PyMC3 (Salvatier et al., 2016) and Edward (Tran et al., 2016). A detailed\nfeature comparison is shown in Table 2.\nAs seen from the table, all the three libraries\nbuild upon modern computation graph libraries and can transparently run on GPUs. The\nother features can be mainly divided into three categories: modeling capabilities, inference\nsupport as well as architecture and design.\nModeling capabilities All the three libraries use the primitives from the computation\ngraph toolkits they base on and are designed for directed graphs (Bayesian networks).\nPyMC3 solves the model reuse problem during inference by theano.copy() to copy the\nrelated subgraphs.\nEdward also does this but does not rely on the oﬃcial Tensorﬂow\nAPI. ZhuSuan avoids altering created graphs and build reuse on purely function reuse and\ncontext management. As a result, PyMC3 and ZhuSuan can correctly deal with control\nﬂow primitives like theano.scan() and tf.while_loop(), while Edward faces challenges\nwhen applying variational inference to this kind of models because it will require graph\ncopying when replacing latent variables with samples from the variational posterior, which\nis troublesome for control ﬂow operations given there is no oﬃcial support.\nInference support As for the range of inference methods, the three libraries have\ndiﬀerent emphases. PyMC3 started from MCMC methods, which is demonstrated by its\n22\nZhuSuan: A Library for Bayesian Deep Learning\nFeatures\nPyMC3\nEdward\nZhuSuan\nBased on\nTheano\nTensorﬂow\nTensorﬂow\nGPU support\n\u0013\n\u0013\n\u0013\nDeterministic\nmodeling\nAny Theano operation\nControl\nﬂows\n(tf.\nwhile_loop,tf.cond)\nare not properly han-\ndled\nAny Tensorﬂow op-\neration\nCustomizable\nvariational\nposterior\nOnly for reparameteriz-\nable settings\n\u0013\n\u0013\nHMC: Paral-\nlel chains\n\u0013\n\u0017\n\u0013\nModularity\nModeling and inference\nare tightly coupled\nModeling and inference\nare tightly coupled\nAll parts can be\nused independently\nTransparency\n\u0017\n\u0017\n\u0013\nTable 2: Comparison between ZhuSuan and other python probabilistic programming li-\nbraries.\nname. It has put much eﬀorts into sampling algorithms and have made them applicable\nto a broad class of models in Bayesian statistics. On the other hand, PyMC3’s support\nfor variational inference is limited and speciﬁc to several independent algorithms (ADVI,\nSVGD, etc.). Edward has a general inference abstraction and implements a large number of\nalgorithms as subclasses. However, as the form of abstraction induces constraints, many of\nthese implemented algorithms have limited behaviors and make too strong assumptions on\nthe constructed models (e.g., GANInference). ZhuSuan emphasizes more on the scenarios\nfrom Bayesian deep learning thus puts more eﬀorts into modern diﬀerentiable algorithms\nthat can be uniﬁed into the deep learning paradigm. Both Edward and ZhuSuan support\ncustomizable variational posteriors for all the VI methods while PyMC3 has only made this\nspeciﬁc to the reparameterizable settings.\nArchitecture and design The design philosophy of ZhuSuan has been introduced\nin Section 3, which emphasizes two principles—modularity and transparency. These two\nprinciples are very diﬀerent from the traditional designs of probabilistic programming, for\nwhich PyMC3 can be a stereotype. The PyMC3 programs strictly follow the form with\nmodel deﬁnition, inference function call, and predictive checks. Little customization can be\nmade except in the model deﬁnition and provided inference options. Edward also roughly\nfollows this framework but has made it more ﬂexible by general programmable variational\nposteriors. However, the inference procedure is also hidden due to its reliance on lots of\ninternal manipulations of the created computation graphs, which can be hard for plain\nusers to understand. Both the libraries build their modeling and inference functionalities\nin a tightly coupled way. This implies that if the model cannot be described using their\n23\nShi et al.\nmodeling primitives, then there is little possibility using their inference features. ZhuSuan\ndraws beneﬁts from deep learning paradigms and makes inference just as doing gradient\ndescent on a cost term. And all parts in the library can be used independently. This proves\nto be much more transparent and compositional, especially in models with a large set of\nstochastic activities, various observation behaviors and deep hierarchies.\n6. Conclusions\nWe have described ZhuSuan, a python probabilistic programming library for Bayesian deep\nlearning, built upon Tensorﬂow. ZhuSuan bridges the gap between Bayesian methods and\ndeep learning by providing deep learning style primitives and algorithms for building prob-\nabilistic models and applying Bayesian inference. Many examples are provided to illus-\ntrate the intuitiveness of programming with ZhuSuan. We have open-sourced ZhuSuan on\nGitHub5, aiming to accelerate research and applications of Bayesian deep learning.\nAcknowledgments\nWe thank Haowen Xu for helpful discussions on the API design, Chang Liu for advices on\npaper writing and Shizhen Xu, Dong Yan for initial attempts to try ZhuSuan on multi-card\nand distributed systems. We would like to acknowledge support for this project from the Na-\ntional 973 Basic Research Program of China (No. 2013CB329403), National NSF of China\n(Nos. 61620106010, 61322308, 61332007), the National Youth Top-notch Talents Support\nProgram, the NVIDIA NVAIL program, and Tsinghua Tiangong Institite for Intelligent\nTechnology.\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGreg S Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale\nmachine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467,\n2016.\nRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bah-\ndanau, Nicolas Ballas, et al.\nTheano: A Python framework for fast computation of\nmathematical expressions.\narXiv e-prints, abs/1605.02688, May 2016.\nURL https:\n//arxiv.org/abs/1605.02688.\nJimmy Ba, Ruslan R Salakhutdinov, Roger B Grosse, and Brendan J Frey. Learning wake-\nsleep recurrent attention models. In Advances in Neural Information Processing Systems,\npages 2593–2601, 2015.\nMichael Betancourt. A conceptual introduction to hamiltonian monte carlo. arXiv preprint\narXiv:1701.02434, 2017.\n5. GitHub address: https://github.com/thu-ml/zhusuan\n24\nZhuSuan: A Library for Bayesian Deep Learning\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,\nPrasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al.\nEnd to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\nJ¨org\nBornschein\nand\nYoshua\nBengio.\nReweighted\nwake-sleep.\narXiv\npreprint\narXiv:1406.2751, 2014.\nYuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders.\narXiv preprint arXiv:1509.00519, 2015.\nOlivier Capp´e, Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P Robert.\nAdaptive importance sampling in general mixture classes. Statistics and Computing, 18\n(4):447–459, 2008.\nBob Carpenter, Andrew Gelman, Matthew Hoﬀman, Daniel Lee, Ben Goodrich, Michael\nBetancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A proba-\nbilistic programming language. Journal of Statistical Software, Articles, 76(1):1–32, 2017.\nISSN 1548-7660. doi: 10.18637/jss.v076.i01. URL https://www.jstatsoft.org/v076/\ni01.\nJean Cornuet, JEAN-MICHEL MARIN, Antonietta Mira, and Christian P Robert. Adap-\ntive multiple importance sampling. Scandinavian Journal of Statistics, 39(4):798–812,\n2012.\nSander Dieleman, Jan Schlter, Colin Raﬀel, Eben Olson, Sren Kaae Snderby, Daniel Nouri,\net al. Lasagne: First release., August 2015. URL http://dx.doi.org/10.5281/zenodo.\n27878.\nC. Du, J. Zhu, and B. Zhang. Learning deep generative models with doubly stochastic\ngradient mcmc. IEEE Transactions on Neural Networks and Learning Systems, PP(99):\n1–13, 2017. ISSN 2162-237X. doi: 10.1109/TNNLS.2017.2688499.\nYarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.\nAndrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis,\nvolume 2. Chapman & Hall/CRC Boca Raton, FL, USA, 2014.\nStuart Geman and Donald Geman.\nStochastic relaxation, gibbs distributions, and the\nbayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, (6):721–741, 1984.\nZoubin Ghahramani. Probabilistic machine learning and artiﬁcial intelligence. Nature, 521\n(7553):452–459, 2015.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems, pages 2672–2680, 2014.\nAlex Graves. Practical variational inference for neural networks. In Advances in Neural\nInformation Processing Systems, pages 2348–2356, 2011.\n25\nShi et al.\nShixiang Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential monte\ncarlo. In Advances in Neural Information Processing Systems, pages 2629–2637, 2015.\nW Keith Hastings. Monte carlo sampling methods using markov chains and their applica-\ntions. Biometrika, 57(1):97–109, 1970.\nGeoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep\nJaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep\nneural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine, 29(6):82–97, 2012.\nGeoﬀrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep\nbelief nets. Neural computation, 18(7):1527–1554, 2006.\nMatthew D. Hoﬀman. Learning deep latent Gaussian models with Markov chain Monte\nCarlo. In Proceedings of the 34th International Conference on Machine Learning, pages\n1510–1519, 2017.\nMatthew D Hoﬀman and Andrew Gelman.\nThe no-u-turn sampler: adaptively setting\npath lengths in hamiltonian monte carlo. Journal of Machine Learning Research, 15(1):\n1593–1623, 2014.\nMatthew D Hoﬀman, David M Blei, Chong Wang, and John William Paisley. Stochastic\nvariational inference. Journal of Machine Learning Research, 14(1):1303–1347, 2013.\nEric Jang, Shixiang Gu, and Ben Poole.\nCategorical reparameterization with gumbel-\nsoftmax. arXiv preprint arXiv:1611.01144, 2016.\nMatthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R\nDatta. Composing graphical models with neural networks for structured representations\nand fast inference. In Advances in Neural Information Processing Systems, pages 2946–\n2954, 2016.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\nDiederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-\nsupervised learning with deep generative models. In Advances in Neural Information\nProcessing Systems, pages 3581–3589, 2014.\nDaphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.\n2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in Neural Information Processing Systems,\npages 1097–1105, 2012.\nAlp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei.\nAutomatic diﬀerentiation variational inference. Journal of Machine Learning Research,\n18(14):1–45, 2017. URL http://jmlr.org/papers/v18/16-107.html.\n26\nZhuSuan: A Library for Bayesian Deep Learning\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\nlearning through probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\nDeep learning.\nNature, 521(7553):\n436–444, 2015.\nBenedict Leimkuhler and Sebastian Reich. Simulating hamiltonian dynamics, volume 14.\nCambridge university press, 2004.\nYingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alphadi-\nvergences. arXiv preprint arXiv:1703.02914, 2017.\nYingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised\nmcmc. arXiv preprint arXiv:1702.08343, 2017.\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A contin-\nuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\nShike Mei, Jun Zhu, and Jerry Zhu. Robust regbayes: Selectively incorporating ﬁrst-order\nlogic domain knowledge into bayesian models. In Proceedings of the 31st International\nConference on Machine Learning (ICML-14), pages 253–261, 2014.\nNicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller,\nand Edward Teller. Equation of state calculations by fast computing machines. The\njournal of chemical physics, 21(6):1087–1092, 1953.\nRiccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley.\nDeep\nlearning for healthcare: review, opportunities and challenges. Brieﬁngs in Bioinformatics,\npage bbx044, 2017.\nAndriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks.\nIn Proceedings of the 31st International Conference on Machine Learning, 2014.\nAndriy Mnih and Danilo J Rezende. Variational inference for monte carlo objectives. arXiv\npreprint arXiv:1602.06725, 2016.\nRadford M Neal. Connectionist learning of belief networks. Artiﬁcial intelligence, 56(1):\n71–113, 1992.\nRadford M Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto,\n1995.\nRadford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of Markov Chain\nMonte Carlo, 2(11), 2011.\nAnh Nguyen, Jason Yosinski, and JeﬀClune. Deep neural networks are easily fooled: High\nconﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 427–436, 2015.\nArt B. Owen. Monte Carlo theory, methods and examples. 2013.\n27\nShi et al.\nBrooks Paige and Frank Wood. Inference networks for sequential monte carlo in graphical\nmodels. In International Conference on Machine Learning, pages 3040–3049, 2016.\nJohn Paisley, David Blei, and Michael Jordan. Variational bayesian inference with stochastic\nsearch. arXiv preprint arXiv:1206.6430, 2012.\nTianyu Pang, Chao Du, and Jun Zhu.\nRobust deep learning via reverse cross-entropy\ntraining and thresholding test. arXiv preprint arXiv:1706.00633, 2017.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with\ndeep convolutional generative adversarial networks.\narXiv preprint arXiv:1511.06434,\n2015.\nRajesh Ranganath, Sean Gerrish, and David Blei.\nBlack box variational inference.\nIn\nArtiﬁcial Intelligence and Statistics, pages 814–822, 2014.\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows.\narXiv preprint arXiv:1505.05770, 2015.\nDanilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wier-\nstra. One-shot generalization in deep generative models. arXiv preprint arXiv:1603.05106,\n2016.\nChristian P. Robert and George Casella. Monte Carlo Statistical Methods (Springer Texts in\nStatistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005. ISBN 0387212396.\nTim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and varia-\ntional inference: Bridging the gap. In Proceedings of the 32nd International Conference\non Machine Learning (ICML-15), pages 1218–1226, 2015.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and\nXi Chen. Improved techniques for training gans. In Advances in Neural Information\nProcessing Systems, pages 2226–2234, 2016.\nJohn Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic programming\nin python using pymc3. PeerJ Computer Science, 2:e55, 2016.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van\nDen Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc\nLanctot, et al. Mastering the game of go with deep neural networks and tree search.\nNature, 529(7587):484–489, 2016.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint\narXiv:1312.6199, 2013.\n28\nZhuSuan: A Library for Bayesian Deep Learning\nMichalis Titsias and Miguel L´azaro-Gredilla. Doubly stochastic variational bayes for non-\nconjugate inference.\nIn Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pages 1971–1979, 2014.\nMichalis K Titsias. Learning model reparametrizations: Implicit variational inference by\nﬁtting mcmc distributions. arXiv preprint arXiv:1708.01529, 2017.\nDustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang, and David M.\nBlei.\nEdward: A library for probabilistic modeling, inference, and criticism.\narXiv\npreprint arXiv:1610.09787, 2016.\nHanna M Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno.\nEvaluation\nmethods for topic models. In Proceedings of the 26th International Conference on Machine\nLearning, pages 1105–1112. ACM, 2009.\nGreg CG Wei and Martin A Tanner. A monte carlo implementation of the em algorithm\nand the poor man’s data augmentation algorithms. Journal of the American statistical\nAssociation, 85(411):699–704, 1990.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist rein-\nforcement learning. Machine Learning, 8(3-4):229–256, 1992.\nJun Zhu, Ning Chen, and Eric P Xing. Bayesian inference with posterior regularization\nand applications to inﬁnite latent svms. Journal of Machine Learning Research, 15:1799,\n2014.\nJun Zhu, Jianfei Chen, Wenbo Hu, and Bo Zhang. Big learning with bayesian methods.\nNational Science Review, page nwx044, 2017.\n29\n",
  "categories": [
    "stat.ML",
    "cs.AI",
    "cs.LG",
    "cs.NE",
    "stat.CO"
  ],
  "published": "2017-09-18",
  "updated": "2017-09-18"
}