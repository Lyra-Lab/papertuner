{
  "id": "http://arxiv.org/abs/1709.06683v2",
  "title": "OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning",
  "authors": [
    "Peter Henderson",
    "Wei-Di Chang",
    "Pierre-Luc Bacon",
    "David Meger",
    "Joelle Pineau",
    "Doina Precup"
  ],
  "abstract": "Reinforcement learning has shown promise in learning policies that can solve\ncomplex problems. However, manually specifying a good reward function can be\ndifficult, especially for intricate tasks. Inverse reinforcement learning\noffers a useful paradigm to learn the underlying reward function directly from\nexpert demonstrations. Yet in reality, the corpus of demonstrations may contain\ntrajectories arising from a diverse set of underlying reward functions rather\nthan a single one. Thus, in inverse reinforcement learning, it is useful to\nconsider such a decomposition. The options framework in reinforcement learning\nis specifically designed to decompose policies in a similar light. We therefore\nextend the options framework and propose a method to simultaneously recover\nreward options in addition to policy options. We leverage adversarial methods\nto learn joint reward-policy options using only observed expert states. We show\nthat this approach works well in both simple and complex continuous control\ntasks and shows significant performance increases in one-shot transfer\nlearning.",
  "text": "OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial\nInverse Reinforcement Learning\nPeter Henderson1, Wei-Di Chang2, Pierre-Luc Bacon1\nDavid Meger1, Joelle Pineau1, Doina Precup1\n1 School of Computer Science, McGill University, Montreal, Canada\n2 Department of Electrical, Computer, and Software Engineering, McGill University, Montreal, Canada\n{peter.henderson,wei-di.chang}@mail.mcgill.ca\npbacon@cs.mcgill.ca, dmeger@cim.mcgill.ca, {jpineau,dprecup}@cs.mcgill.ca\nAbstract\nReinforcement learning has shown promise in learning poli-\ncies that can solve complex problems. However, manually\nspecifying a good reward function can be difﬁcult, especially\nfor intricate tasks. Inverse reinforcement learning offers a\nuseful paradigm to learn the underlying reward function di-\nrectly from expert demonstrations. Yet in reality, the corpus\nof demonstrations may contain trajectories arising from a di-\nverse set of underlying reward functions rather than a single\none. Thus, in inverse reinforcement learning, it is useful to\nconsider such a decomposition. The options framework in re-\ninforcement learning is speciﬁcally designed to decompose\npolicies in a similar light. We therefore extend the options\nframework and propose a method to simultaneously recover\nreward options in addition to policy options. We leverage\nadversarial methods to learn joint reward-policy options us-\ning only observed expert states. We show that this approach\nworks well in both simple and complex continuous control\ntasks and shows signiﬁcant performance increases in one-shot\ntransfer learning.\nIntroduction\nA long term goal of Inverse Reinforcement Learning (IRL)\nis to be able to learn underlying reward functions and poli-\ncies solely from human video demonstrations. We call such\na case, where the demonstrations come from different con-\ntexts and the task must be performed in a novel environment,\none-shot transfer learning. For example, given only demon-\nstrations of a human walking on earth, can an agent learn to\nwalk on the moon?\nHowever, such demonstrations would undoubtedly come\nfrom a wide range of settings and environments and may\nnot conform to a single reward function. This proves detri-\nmental to current methods which might over-generalize and\ncause poor performance. In forward RL, decomposing a pol-\nicy into smaller specialized policy options has been shown\nto improve results for exactly such cases (Sutton, Precup,\nand Singh 1999; Bacon, Harb, and Precup 2017). Thus, we\nextend the options framework to IRL and decompose both\nthe reward function and policy. Our method is able to learn\ndeep policies which can specialize to the set of best-ﬁtting\nCopyright c⃝2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nexperts. Hence, it excels at one-shot transfer learning where\nsingle-approximator methods waver.\nTo accomplish this, we make use of the Generative Adver-\nsarial Imitation Learning (GAIL) framework (Ho and Ermon\n2016) and formulate a method for learning joint reward-\npolicy options with adversarial methods in IRL. As such,\nwe call our method OptionGAN. This method can implicitly\nlearn divisions in the demonstration state space and accord-\ningly learn policy and reward options. Leveraging a corre-\nspondence between Mixture-of-Experts (MoE) and one-step\noptions, we learn a decomposition of rewards and the policy-\nover-options in an end-to-end fashion. This decomposition is\nable to capture simple problems and learn any of the under-\nlying rewards in one shot. This gives ﬂexibility and beneﬁts\nfor a variety of future applications (both in reinforcement\nlearning and standard machine learning).\nWe evaluate OptionGAN in the context of continuous\ncontrol locomotion tasks, considering both simulated Mu-\nJoCo locomotion OpenAI Gym environments (Brockman et\nal. 2016), modiﬁcations of these environments for task trans-\nfer (Henderson et al. 2017), and a more complex Roboschool\ntask (Schulman et al. 2017). We show that the ﬁnal policies\nlearned using joint reward-policy options outperform a sin-\ngle reward approximator and policy network in most cases,\nand particularly excel at one-shot transfer learning.\nRelated Work\nOne goal in robotics research is to create a system which\nlearns how to accomplish complex tasks simply from ob-\nserving an expert’s actions (such as videos of humans per-\nforming actions). While IRL has been instrumental in work-\ning towards this goal, it has become clear that ﬁtting a single\nreward function which generalizes across many domains is\ndifﬁcult. To this end, several works investigate decomposing\nthe underlying reward functions of expert demonstrations\nand environments in both IRL and RL (Krishnan et al. 2016;\nSermanet, Xu, and Levine 2016; Choi and eung Kim 2012;\nBabes et al. 2011; van Seijen et al. 2017). For example,\nin (Krishnan et al. 2016), reward functions are decomposed\ninto a set of subtasks based on segmenting expert demonstra-\ntion transitions (known state-action pairs) by analyzing the\nchanges in “local linearity with respect to a kernel function”.\nSimilarly, in (Sermanet, Xu, and Levine 2016), techniques in\nvideo editing based on information-similarity are adopted to\narXiv:1709.06683v2  [cs.LG]  24 Nov 2017\ndivide a video demonstration into distinct sections which can\nthen be recombined into a differentiable reward function.\nHowever, simply decomposing the reward function may\nnot be enough, the policy must also be able to adapt to\ndifferent tasks. Several works have investigated learning\na latent dimension along with the policy for such a pur-\npose (Hausman et al. 2017; Wang et al. 2017; Li, Song,\nand Ermon 2017). This latent dimension allows multiple\ntasks to be learned by one policy and elicited via the latent\nvariable. In contrast, our work focuses on one-shot trans-\nfer learning. In the former work, the desired latent vari-\nable must be known and provided, whereas in our formu-\nlation the latent structure is inherently encoded in an un-\nsupervised manner. This is inherently accomplished while\nlearning to solve a task composed of a wide range of un-\nderlying reward functions and policies in a single frame-\nwork. Overall, this work contains parallels to all of the afore-\nmentioned and other works emphasizing hierarchical poli-\ncies (Daniel, Neumann, and Peters 2012; Dietterich 2000;\nMerel et al. 2017), but speciﬁcally focuses on leveraging\nMoEs and reward decompositions to ﬁt into the options\nframework for efﬁcient one-shot transfer learning in IRL.\nPreliminaries and Notation\nMarkov Decision Processes (MDPs) MDPs consist of\nstates S, actions A, a transition function P : S × A →\n(S →R), and a reward function r : S →R. We formu-\nlate our methods in the space of continuous control tasks\n(A ∈R, S ∈R) using measure-theoretic assumptions. Thus\nwe deﬁne a parameterized policy as the probability distri-\nbution over actions conditioned on states πθ : S × A →\n[0, 1], modeled by a Gaussian πθ ∼N(µ, σ2) where θ are\nthe policy parameters. The value of a policy is deﬁned as\nVπ(s) = Eπ[P∞\nt=0 γtrt+1|s0 = s] and the action-value\nis Qπ(s, a) = Eπ[P∞\nt=0 γtrt+1|s0 = s, a0 = a], where\nγ ∈[0, 1) is the discount factor.\nThe Options framework In reinforcement learning, an\noption (ω ∈Ω) can be deﬁned by a triplet (Iω, πω, βω). In\nthis deﬁnition, πω is called an intra-policy option, Iω ⊆S\nis an initiation set, and βω : S →[0, 1] is a termination\nfunction (i.e. the probability that an option ends at a given\nstate) (Sutton, Precup, and Singh 1999). Furthermore, πΩis\nthe policy-over-options. That is, πΩdetermines which op-\ntion πω an agent picks to use until the termination func-\ntion βω indicates that a new option should be chosen. Other\nworks explicitly formulate call-and-return options, but we\ninstead simplify to one-step options, where βω(s) = 1; ∀ω ∈\nΩ, ∀s ∈S. One-step options have long been discussed as\nan alternative to temporally extended methods and often\nprovide advantages in terms of optimality and value esti-\nmation (Sutton, Precup, and Singh 1999; Dietterich 2000;\nDaniel, Neumann, and Peters 2012). Furthermore, we ﬁnd\nthat our options still converge to temporally extended and\ninterpretable actions.\nMixture-of-Experts The idea of creating a mixture of\nexperts (MoEs) was initially formalized to improve learn-\ning of neural networks by dividing the input space among\nseveral networks and then combining their outputs through\na soft weighted average (Jacobs et al. 1991). It has since\ncome into prevalence for generating extremely large neu-\nral networks (Shazeer et al. 2017). In our formulation of\njoint reward-policy options, we leverage a correspondence\nbetween Mixture-of-Experts and options. In the case of one-\nstep options, the policy-over-options (πΩ) can be viewed as a\nspecialized gating function over experts (intra-options poli-\ncies πω(a|s)): P\nω πΩ(ω|s)πω(a|s). Several works inves-\ntigate convergence to a sparse and specialized Mixture-of-\nExperts (Jacobs et al. 1991; Shazeer et al. 2017). We lever-\nage these works to formulate a Mixture-of-Experts which\nconverges to one-step options.\nPolicy Gradients Policy gradient (PG) methods (Sutton\net al. 2000) formulate a method for optimizing a param-\neterized policy πθ through stochastic gradient ascent. In\nthe discounted setting, PG methods optimize ρ(θ, s0) =\nEπθ [P∞\nt=0 γtr(st)|s0]. The PG theorem states: δρ(θ,s0)\nδθ\n=\nP\ns µπθ(s|s0) P\na\nδπθ(a|s)\nδθ\nQπθ(s, a), where µπθ(s|s0)\n=\nP∞\nt=0 γtP(st = s|s0) (Sutton et al. 2000). In Trust Re-\ngion Policy Optimization (TRPO) (Schulman et al. 2015)\nand Proximal Policy Optimization (PPO) (Schulman et al.\n2017) this update is constrained and transformed into the ad-\nvantage estimation view such that the above becomes a con-\nstrained optimization: maxθ Et\nh\nπθ(at|st)\nπθold(at|st)At(st, at)\ni\nsub-\nject to Et [KL [πθold(·|st), πθ(·|st)]] ≤δ where At(st, at) is\nthe generalized advantage function according to (Schulman\net al. 2016). In TRPO, this is solved as a constrained conju-\ngate gradient descent problem, while in PPO the constraint\nis transformed into a penalty term or clipping objective.\nInverse Reinforcement Learning Inverse Reinforcement\nLearning was ﬁrst formulated in the context of MDPs by\n(Ng and Russell 2000). In later work, a parametrization of\nthe reward function is learned as a linear combination of\nthe state feature expectation so that the hyperdistance be-\ntween the expert and the novice’s feature expectation is\nminimized (Abbeel and Ng 2004). It has also been shown\nthat a solution can be formulated using the maximum en-\ntropy principle, with the goal of matching feature expecta-\ntion as well (Ziebart et al. 2008). Generative adversarial im-\nitation learning (GAIL) make use of adversarial techniques\nfrom (Goodfellow et al. 2014) to perform a similar feature\nexpectation matching (Ho and Ermon 2016). In this case,\na discriminator uses state-action pairs (transitions) from the\nexpert demonstrations and novice rollouts to learn a binary\nclassiﬁcation probability distribution. The probability that a\nstate belongs to an expert demonstration can then be used\nas the reward for a policy optimization step. However, un-\nlike GAIL, we do not assume knowledge of the expert ac-\ntions. Rather, we rely solely on observations in the discrim-\ninator problem. We therefore refer to our baseline approach\nas Generative Adversarial Inverse Reinforcement Learning\n(IRLGAN) as opposed to imitation learning. It is important\nto note that IRLGAN is GAIL without known actions, we\nadopt the different naming scheme to highlight this differ-\nence. As such, our adversarial game optimizes:\nmax\nπΘ min\nR ˆ\nΘ\n−\n\u0002\nEπΘ[log R ˆΘ(s)] + EπE[log(1 −R ˆΘ(s))]\n\u0003\n(1)\nStates, Actions\nPolicy \nOptimization\nNovice Rollouts\n(States, Actions, \nRewards)\nExpert Rollouts\n(States, Actions, \nRewards)\nDiscriminator\nRewards\nExpert and \nNovice \nObservations\n(States)\nExpert and \nNovice \nObservations\n(States)\nPolicy over \nOptions\nDiscriminator n\nDiscriminator 0\nStates, Actions\nNovice Rollouts\n(States, Actions, \nRewards)\nExpert Rollouts\n(States, Actions, \nRewards)\nRewards\n......\nPolicy \nOptimization 0\n......\nPolicy \nOptimization n\nExpert and \nNovice \nObservations\n(States)\nExpert and \nNovice \nObservations\n(States)\nRewards\nFigure 1: Generative Adversarial Inverse Reinforcement Learning (left) and OptionGAN (right) Architectures\nwhere πΘ and πE are the policy of the novice and expert\nparameterized by Θ and E, respectively, and R ˆΘ is the dis-\ncriminator probability that a sample state belongs to an ex-\npert demonstration (parameterized by ˆΘ). We use this nota-\ntion since in this case the discriminator approximates a re-\nward function. Similarly to GAIL, we use TRPO during the\npolicy optimization step for simple tasks. However, for com-\nplex tasks we adopt PPO. Figure 1 and Algorithm 1 show an\noutline for the general IRLGAN process.\nAlgorithm 1: IRLGAN\nInput : Expert trajectories τE ∼πE.\n1 Initialize Θ, ˆΘ\n2 for i = 0, 1, 2, . . . do\n3\nSample trajectories τN ∼πΘi\n4\nUpdate discriminator parameters (ˆΘ) according to:\nL ˆΘ = Es∼τN [log R ˆΘ(s)]+Es∼τE[log(1−R ˆΘ(s))]\n5\nUpdate policy (with constrained update step and\nparameters θ) according to:\nEτN [∇Θ log πΘi(a|s)EτN [log(R ˆΘi+1(s))|s0 = ¯s]]\n6 end\nReward-Policy Options Framework\nBased on the need to infer a decomposition of underlying\nreward functions from a wide range of expert demonstra-\ntions in one-shot transfer learning, we extend the options\nframework for decomposing rewards as well as policies. In\nthis way, intra-option policies, decomposed rewards, and the\npolicy-over-options can all be learned in concert in a co-\nhesive framework. In this case, an option is formulated by\na tuple: (Iω, πω, βω, rω). Here, rω is a reward option from\nwhich a corresponding intra-option policy πω is derived.\nThat is, each policy option is optimized with respect to its\nown local reward option. The policy-over-options not only\nchooses the intra-option policy, but the reward option as\nwell: πΩ→(rω, πω). For simplicity, we refer to the policy-\nover-reward-options as rΩ(in our formulation, rΩ= πΩ).\nThere is a parallel to be drawn from this framework to Feu-\ndal RL (Dayan and Hinton 1993), but here the intrinsic re-\nward function is statically bound to each worker (policy op-\ntion), whereas in that framework the worker dynamically re-\nceives a new intrinsic reward from the manager.\nTo learn joint reward-policy options, we present a method\nwhich ﬁts into the framework of IRLGAN. We reformu-\nlate the discriminator as a Mixture-Of-Experts and re-use\nthe gating function when learning a set of policy options.\nWe show that by properly formulating the discriminator loss\nfunction, the Mixture-Of-Experts converges to one-step op-\ntions. This formulation also allows us to use regularizers\nwhich encourage distribution of information, diversity, and\nsparsity in both the reward and policy options.\nLearning Joint Reward-Policy Options\nThe use of one-step options allows us to learn a policy-over-\noptions in an end-to-end fashion as a Mixture-of-Experts\nformulation. In the one-step case, selecting an option (πω,θ)\nusing the policy-over-options (πΩ,ζ) can be viewed as a mix-\nture of completely specialized experts such that: πΘ(a|s) =\nP\nω πΩ,ζ(ω|s)πω,θ(a|s). The reward for a given state is\ncomposed as: RΩ, ˆΘ(s) = P\nω πΩ,ζ(ω|s)rω,ˆθ(s), where\nζ, θ ∈Θ, ˆθ ∈ˆΘ are the parameters of the policy-over-\noptions, policy options, and reward options, respectively.\nThus, we reformulate our discriminator loss as a weighted\nmixture of completely specialized experts in Eq. 2. This al-\nlows us to update the parameters of the policy-over-options\nand reward options together during the discriminator update.\nLΩ= Eω\nh\nπΩ,ζ(ω|s)Lˆθ,ω\ni\n+ Lreg\n(2)\nHere, Lˆθ,ω is the sigmoid cross-entropy loss of the reward\noptions (discriminators). Lreg, as will be discussed later on,\nis a penalty or set of penalties which can encourage certain\nproperties of the policy-over-options or the overall reward\nsignal. As can be seen in Algorithm 2 and Figure 1, this loss\nfunction can ﬁt directly into the IRLGAN framework.\nAlgorithm 2: OptionGAN\nInput : Expert trajectories τE ∼πE.\n1 Initialize θ, ˆθ\n2 for i = 0, 1, 2, . . . do\n3\nSample trajectories τN ∼πΘi\n4\nUpdate discriminator options parameters ˆθ, ω and\npolicy-over-options parameters ζ, to minimize:\nLΩ= Eω\nh\nπΩ,ζ(ω|s)Lˆθ,ω\ni\n+ Lreg\nLˆθ,ω = EτN [log rˆθ,ω(s)] + EτE[log(1 −rˆθ,ω(s))]\n5\nUpdate policy options (with constrained update step\nand parameters θω ∈ΘΩ) according to:\nEτN [∇θ log πΘ(a|s)EτN [log(RΩ, ˆΘ(s))|s0 = ¯s]]\n6 end\nHaving updated the parameters of the policy-over-options\nand reward options, standard PG methods can be used to op-\ntimize the parameters of the intra-option policies. This can\nbe done by weighting the average of the intra-option policy\nactions with the policy-over-options πΩ,ζ. While it is possi-\nble to update each intra-option policy separately as in (Ba-\ncon, Harb, and Precup 2017), this Mixture-of-Experts for-\nmulation is equivalent, as discussed in the next section. Once\nthe gating function specializes over the options, all gradients\nexcept for those related to the intra-option policy selected\nwould be weighted by zero. We ﬁnd that this end-to-end pa-\nrameter update formulation leads to easier implementation\nand smoother learning with constraint-based methods.\nMixture-of-Experts as Options\nTo ensure that our MoE formulation converges to options\nin the optimal case, we must properly formulate our loss\nfunction such that the gating function specializes over ex-\nperts. While it may be possible to force a sparse selec-\ntion of options through a top-k choice as in (Shazeer et\nal. 2017), we ﬁnd that this leads to instability since for\nk = 1 the top-k function is not differentiable. As is spec-\niﬁed in (Jacobs et al. 1991), a loss function of the form\nL = (y −\n1\n||Ω||\nP\nω πΩ(ω|s)yω(s))2 draws cooperation\nbetween experts, but a reformulation of the loss, L =\n1\n||Ω||\nP\nω πΩ(ω|s)(y −yω(s))2, encourages specialization.\nIf we view our policy-over-options as a softmax (i.e.\nπΩ(ω|s) =\nexp(zω(s))\nP\ni exp(zi(s))), then the derivative of the loss\nfunction with respect to the gating function becomes:\ndL\ndzω\n=\n1\n||Ω||πΩ(ω|s)\n\u0000(y −yω(s))2 −L\n\u0001\n(3)\nThis can intuitively be interpreted as encouraging the gat-\ning function to increase the likelihood of choosing an expert\nwhen its loss is less than the average loss of all the experts.\nThe gating function will thus move toward deterministic se-\nlection of experts.\nAs we can see in Eq. 2, we formulate our discrimina-\ntor loss in the same way, using each reward option and the\npolicy-over-options as the experts and gating function re-\nspectively. This ensures that the policy-over-options special-\nizes over the state space and converges to a deterministic se-\nlection of experts. Hence, we can assume that in the optimal\ncase, our formulation of an MoE-style policy-over-options is\nequivalent to one-step options. Our characterization of this\nnotion of MoE-as-options is further backed by experimental\nresults. Empirically, we still ﬁnd temporal coherence across\noption activation despite not explicitly formulating call-and-\nreturn options as in (Bacon, Harb, and Precup 2017).\nRegularization Penalties\nDue to our formulation of Mixture-of-Experts as options, we\ncan learn our policy-over-options in an end-to-end manner.\nThis allows us to add additional terms to our loss function to\nencourage the appearance of certain target properties.\nSparsity and Variance Regularization\nTo ensure an even\ndistribution of activation across the options, we look to con-\nditional computation techniques that encourage sparsity and\ndiversity in hidden layer activations and apply these to our\npolicy-over-options (Bengio et al. 2015). We borrow three\npenalty terms Lb, Le, Lv (adopting a similar notation). In\nthe minibatch setting, these are formulated as:\nLb\n=\nX\nω\n||Es[πΩ(ω|s)] −τ||2\n(4)\nLe\n=\nEs\n\"\n||\n \n1\n||Ω||\nX\nω\nπΩ(ω|s)\n!\n−τ||2\n#\n(5)\nLv\n=\n−\nX\nω\nvarω{πΩ(ω|s)}\n(6)\nwhere τ is the target sparsity rate (which we set to .5 for\nall cases). Here, Lb encourages the activation of the policy-\nover-options with target sparsity τ “in expectation over the\ndata” (Bengio et al. 2015). Essentially, Lb encourages a uni-\nform distribution of options over the data while Le drives\ntoward a target sparsity of activations per example (doubly\nencouraging our mixtures to be sparse). Lv also encourages\nvaried πΩactivations while discouraging uniform selection.\nMutual Information Penalty\nTo ensure the specialization\nof each option to a speciﬁc partition of the state space, a mu-\ntual information (MI) penalty is added.1 We thus minimize\nmutual information pairwise between option distributions,\nsimilarly to (Liu and Yao 2002):\nI(Fi; Fj) = −1\n2 log(1 −ρ2\nij),\n(7)\nwhere Fi and Fj are the outputs of reward options i and j\nrespectively, and ρij the correlation coefﬁcient of Fi and Fj,\ndeﬁned as ρij = E[(Fi−E[Fi])(Fj−E[Fj])]\nσ2\ni σ2\nj\n.\n1While it may be simpler to use an entropy regularizer, we\nfound that in practice it performs worse. Entropy regularization en-\ncourages exploration (Mnih et al. 2016). In the OptionGAN setting,\nthis results in unstable learning, while the mutual information term\nencourages diversity in the options while providing stable learning.\nFigure 2: The policy-over-options elicits two interpretable behaviour modes per option, but temporal cohesion and specialization\nis seen between these behaviour modes across time within a sample rollout trajectory.\nThe resulting loss term is thus computed as:\nLMI =\nX\nω∈Ω\nX\nˆω∈Ω,ω̸=ˆω\nI(πω, πˆω).\n(8)\nThus the overall regularization term becomes:\nLreg = λbLb + λeLe + λvLv + λMILMI.\n(9)\nExperiments\nTo evaluate our method of learning joint reward-policy op-\ntions, we investigate continuous control tasks. We divide\nour experiments into 3 settings: simple locomotion tasks,\none-shot transfer learning, and complex tasks. We compare\nOptionGAN against IRLGAN in all scenarios, investigat-\ning whether dividing the reward and policy into options im-\nproves performance against the single approximator case.2\nTable 1 shows the overall results of our evaluations and we\nhighlight a subset of learning curves in Figure 3. We ﬁnd that\nin nearly every setting, the ﬁnal optionated policy learned by\nOptionGAN outperforms the single approximator case.\nExperimental Setup\nAll shared hyperparameters are held constant between IRL-\nGAN and OptionGAN evaluation runs. All evaluations are\naveraged across 10 trials, each using a different random\nseed. We use the average return of the true reward function\nacross 25 sample rollouts as the evaluation metric. Multi-\nlayer perceptrons are used for all approximators as in (Ho\nand Ermon 2016). For the OptionGAN intra-option policy\nand reward networks, we use shared hidden layers. That is\nrω, ∀ω ∈Ωall share hidden layers and πω, ∀ω ∈Ωshare\nhidden layers. We use separate parameters for the policy-\nover-options πΩ. Shared layers are used to ensure a fair com-\nparison against a single network of the same number of hid-\nden layers. For simple settings all hidden layers are of size\n2Extended experimental details and results can be found in the\nsupplemental. Code is located at:\nhttps://github.com/Breakend/OptionGAN.\n(64, 64) and for complex experiments are (128, 128). For\nthe 2-options case we set λe = 10.0, λb = 10.0, λv = 1.0\nbased on a simple hyperparameter search and reported re-\nsults from (Bengio et al. 2015). For the 4-options case we\nrelax the regularizer that encourages a uniform distribution\nof options (Lb), setting λb = .01.\nSimple Tasks\nFirst, we investigate simple settings without transfer learn-\ning for a set of benchmark locomotion tasks provided in\nOpenAI Gym (Brockman et al. 2016) using the MuJoCo\nsimulator (Todorov, Erez, and Tassa 2012). We use the\nHopper-v1, HalfCheetah-v1, and Walker2d-v1 locomotion\nenvironments. The results of this experiment are shown\nin Table 1 and sample learning curves for Hopper and\nHalfCheetah can be found in Figure 3. We use 10 expert roll-\nouts from a policy trained using TRPO for 500 iterations.\nIn these simple settings, OptionGAN converges to poli-\ncies which perform as well or better than the single approxi-\nmator setting. Importantly, even in these simple settings, the\noptions which our policy selects have a notion of temporal\ncoherence and interpretability despite not explicitly enforc-\ning this in the form of a termination function. This can be\nseen in the two option version of the Hopper-v1 task in Fig-\nure 2. We ﬁnd that generally each option takes on two be-\nhaviour modes. The ﬁrst option handles: (1) the rolling of\nthe foot during hopper landing; (2) the folding in of the foot\nin preparation for ﬂoating. The second option handles: (1)\nthe last part of take-off where the foot is hyper-extended and\nbody ﬂexed; (2) the part of air travel without any movement.\nOne-Shot Transfer Learning\nWe also investigate one-shot transfer learning. In this sce-\nnario, the novice is trained on a target environment, while\nexpert demonstrations come from a similar task, but from\nenvironments with altered dynamics (i.e. one-shot transfer\nfrom varied expert demonstrations to a new environment).\nTo demonstrate the effectiveness of OptionGAN in these set-\ntings, we use expert demonstrations from environments with\nTask\nExpert\nIRLGAN\nOptionGAN (2ops)\nOptionGAN (4ops)\nHopper-v1\n3778.8 ± 0.3\n3736.3 ± 152.4\n3641.2 ± 105.9\n3715.5 ± 17.6\nHalfCheetah-v1\n4156.9 ± 8.7\n3212.9 ± 69.9\n3714.7 ± 87.5\n3616.1 ± 127.3\nWalker2d-v1\n5528.5 ± 7.3\n4158.7 ± 247.3\n3858.5 ± 504.9\n4239.3 ± 314.2\nHopper (One-Shot)\n3657.7 ± 25.4\n2775.1 ± 203.3\n3409.4 ± 80.8\n3464.0 ± 67.8\nHalfCheetah (One-Shot)\n4156.9 ± 51.3\n1296.3 ± 177.8\n1679.0 ± 284.2\n2219.4 ± 231.8\nWalker (One-Shot)\n4218.1 ± 43.1\n3229.8 ± 145.3\n3925.3 ± 138.9\n3769.40 ± 170.4\nHopperSimpleWall-v0\n3218.2 ± 315.7\n2897.5 ± 753.5\n3140.3 ± 674.3\n3272.3 ± 569.0\nRoboschoolHumanoidFlagrun-v1\n2822.1 ± 531.1\n1455.2 ± 567.6\n1868.9 ± 723.7\n2113.6 ± 862.9\nTable 1: True Average Return with the standard error across 10 trials on the 25 ﬁnal evaluation rollouts using the ﬁnal policy.\nvarying gravity conditions as seen in (Henderson et al. 2017;\nChristiano et al. 2016). We vary the gravity (.5, .75, 1.25, 1.5\nof Earth’s gravity) and train experts using TRPO for each\nof these. We gather 10 expert trajectories from each gravity\nvariation, for a total of 40 expert rollouts, to train a novice\nagent on the normal Earth gravity environment (the default\n-v1 environment as provided in OpenAI Gym). We repeat\nthis for Hopper-v1, HalfCheetah-v1, and Walker2D-v1.\nThese gravity tasks are selected due to the demonstration\nin (Henderson et al. 2017) that learning sequentially on these\nvaried gravity environments causes catastrophic forgetting\nof the policy on environments seen earlier in training. This\nsuggests that the dynamics are varied enough that trajecto-\nries are difﬁcult to generalize across, yet still share some\nstate representations and task goals. As seen in Figure 3, us-\ning options can cause signiﬁcant performance increases in\nthis area, but performance gains can vary across the number\nof options and the regularization penalty as seen in Table 1.\nComplex Tasks\nLastly, we investigate slightly more complex tasks. We uti-\nlize the HopperSimpleWall-v0 environment provided by the\ngym-extensions framework (Henderson et al. 2017) and\nthe RoboschoolHumanoidFlagrun-v1 environment used in\n(Schulman et al. 2017). In the ﬁrst, a wall is placed ran-\ndomly in the path of the Hopper-v1 agent and simpliﬁed\nsensor readouts are added to the observations as in (Wang\net al. 2017). In the latter, the goal is to run and reach a fre-\nquently changing target. This is an especially complex task\nwith a highly varied state space. In both cases we use an ex-\npert trained with TRPO and PPO respectively, to generate\n40 expert rollouts. For the Roboschool environment, we ﬁnd\nthat TRPO does not allow enough exploration to perform ad-\nequately, and thus we switch our policy optimization method\nto the clipping-objective version of PPO.\nAblation Investigations\nConvergence of Mixtures to Options\nTo show that our\nformulation of Mixture-of-Experts decomposes to options\nin the optimal case, we investigate the distributions of our\npolicy-over-options. We ﬁnd that across 40 trials, 100% of\nactivations fell within a reasonable error bound of deter-\nministic selection across 1M samples. That is, in 40 to-\ntal trials across 4 environments (Hopper-v1, HalfCheetah-\nv1, Walker2d-v1, RoboschoolHumanoidFlagrun-v1), poli-\ncies were trained for 500 iterations (or 5k iterations in\nthe case of RoboschoolHumanoidFlagrun-v1). We collected\n25k samples at the end of each trial. Among the gating ac-\ntivations across the samples, we recorded the number of\ngating activations within the range {0 + ϵ, 1 −ϵ} for ϵ =\n0.1. 100% fell within this range. 98.72% fell within range\nϵ = 1−3. Thus at convergence, both intuitively and empiri-\ncally we can refer to our gating function over experts as the\npolicy-over-options and each of the experts as options.\nEffect of Uniform Distribution Regularizer\nWe ﬁnd that\nforcing a uniform distribution over options can potentially\nbe harmful. This can be seen in the experiment in Figure 4,\nwhere we evaluate the 4 option case with λb = {0.1, 10}.\nHowever, relaxing the uniform constraint results in rapid\nperformance increases, particularly in the HalfCheetah-v1\nwhere we see increases in learning speed with 4 options.\nThere is an intuitive explanation for this. In the 4-option\ncase, with a relaxed uniform distribution penalty, we allow\noptions to drop out during training. In the case of Hop-\nper and Walker tasks, generally 2 options drop out slowly\nover time, but in HalfCheetah, only one option drops out\nin the ﬁrst 20 iterations with a uniform distribution remain-\ning across the remaining options as seen in Figure 3. We\nposit that in the case of HalfCheetah there is enough mutu-\nally exclusive information in the environment state space to\ndivide across 3 options, quickly causing a rapid gain in per-\nformance, while the Hopper tasks do not settle as quickly\nand thus do not see that large gain in performance.\nLatent Structure in Expert Demonstrations\nAnother\nbeneﬁt of using options in the IRL transfer setting is that\nthe underlying latent division of the original expert envi-\nronments is learned by the policy-over-options. As seen in\nFigure 5, the expert demonstrations have a clear separation\namong options. We suspect that options further away from\nthe target gravity are not as specialized due to the fact that\ntheir state spaces are covered signiﬁcantly by a mixture of\nthe closer options (see supplemental material for support-\ning projected state space mappings). This indicates that the\npolicy-over-options specializes over the experts and is thus\ninherently beneﬁcial for use in one-shot transfer learning.\nDiscussion\nWe propose a direct extension of the options framework by\nadding joint reward-policy options. We learn these options in\nthe context of generative adversarial inverse reinforcement\nlearning and show that this method outperforms the single\n0\n100\n200\n300\n400\n500\nIterations\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage Return\nHopper-v1\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage Return\nHopper-v1 (One Shot Transfer)\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nActivation %\nHopper-v1 Gating Activations\nOption1\nOption2\nOption3\nOption4\n0\n100\n200\n300\n400\n500\nIterations\n−1000\n0\n1000\n2000\n3000\n4000\nAverage Return\nHalfCheetah-v1\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n−1500\n−1000\n−500\n0\n500\n1000\n1500\n2000\n2500\nAverage Return\nHalfCheetah-v1 (One Shot Transfer)\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n0.0\n0.1\n0.2\n0.3\n0.4\nActivation %\nHalfCheetah-v1 Gating Activations\nOption1\nOption2\nOption3\nOption4\nFigure 3: Left Column: Simple locomotion curves. Error bars indicate standard error of average returns across 10 trials on\n25 evaluation rollouts. Middle Column: One-shot transfer experiments with 40 expert demonstrations from varied gravity\nenvironments without any demonstrations on the novice training environment training on demonstrations from .5G, .75G,\n1.25G, 1.5G gravity variations. Right Column: Activations of policy-over-options over time with 4 options on training samples\nin the one-shot transfer setting with λb = .01.\n0\n100\n200\n300\n400\n500\nIterations\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage Return\nHopper-v1 (Uniform Distribution Penalty)\nOPTIONGAN (4 ops, λb = 10)\nOPTIONGAN (4 ops, λb = .01)\nFigure 4: Effect of uniform distribution regularizer. Aver-\nage πΩacross ﬁnal sample novice rollouts: λb = 10.0,\n[.27, .21, .25, .25]; λb = .01, [0., 0., .62, .38].\nFigure 5: Probability distribution of πΩover options on ex-\npert demonstrations. Inherent structure is found in the under-\nlying demonstrations. The .75G demonstration state spaces\nare signiﬁcantly assigned to Option 1 and similarly, the\n1.25G state spaces to Option 0.\npolicy case in a variety of tasks – particularly in transfer\nsettings. Furthermore, the learned options demonstrate tem-\nporal and interpretable cohesion without specifying a call-\nand-return termination function.\nOur formulation of joint reward-policy options as a Mix-\nture Of Experts allows for: potential upscaling to extremely\nlarge networks as in (Shazeer et al. 2017), reward shaping in\nforward RL, and using similarly specialized MoEs in gener-\native adversarial networks. This work presents an effective\nand extendable framework. Our optionated networks capture\nthe problem structure effectively, which allows strong gen-\neralization in one-shot transfer learning. Moreover, as adver-\nsarial methods are now commonly used across a myriad of\ncommunities, we believe the embedding of options within\nthis methodology is an excellent delivery mechanism to ex-\nploit the beneﬁts of hierarchical RL in many new ﬁelds.\nAcknowledgements\nWe thank CIFAR, NSERC, The Open Philanthropy Project,\nand the AWS Cloud Credits for Research Program for their\ngenerous contributions.\nReferences\nAbbeel, P., and Ng, A. Y. 2004. Apprenticeship learning\nvia inverse reinforcement learning. In Proceedings of the\nTwenty-ﬁrst International Conference on Machine Learning,\n1–. New York, NY, USA: ACM.\nBabes, M.; Marivate, V.; Subramanian, K.; and Littman,\nM. L. 2011. Apprenticeship learning about multiple inten-\ntions. In Proceedings of the 28th International Conference\non Machine Learning (ICML-11), 897–904.\nBacon, P.-L.; Harb, J.; and Precup, D. 2017. The option-\ncritic architecture. In AAAI, 1726–1734.\nBengio, E.; Bacon, P.-L.; Pineau, J.; and Precup, D. 2015.\nConditional computation in neural networks for faster mod-\nels. arXiv preprint arXiv:1511.06297.\nBrockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.;\nSchulman, J.; Tang, J.; and Zaremba, W. 2016. OpenAI\nGym.\nChoi, J., and eung Kim, K. 2012. Nonparametric bayesian\ninverse reinforcement learning for multiple reward func-\ntions. In Pereira, F.; Burges, C. J. C.; Bottou, L.; and Wein-\nberger, K. Q., eds., Advances in Neural Information Process-\ning Systems 25. Curran Associates, Inc. 305–313.\nChristiano, P.; Shah, Z.; Mordatch, I.; Schneider, J.; Black-\nwell, T.; Tobin, J.; Abbeel, P.; and Zaremba, W. 2016. Trans-\nfer from simulation to real world through learning deep in-\nverse dynamics model. arXiv preprint arXiv:1610.03518.\nDaniel, C.; Neumann, G.; and Peters, J. R. 2012. Hierarchi-\ncal relative entropy policy search. In International Confer-\nence on Artiﬁcial Intelligence and Statistics, 273–281.\nDayan, P., and Hinton, G. E. 1993. Feudal reinforcement\nlearning. In Advances in neural information processing sys-\ntems, 271–278.\nDietterich, T. G. 2000. Hierarchical reinforcement learning\nwith the MAXQ value function decomposition. Journal of\nArtiﬁcial Intelligence Research 13:227–303.\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.\n2014. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems, 2672–2680.\nHausman, K.; Chebotar, Y.; Schaal, S.; Sukhatme, G.; and\nLim, J.\n2017.\nMulti-modal imitation learning from un-\nstructured demonstrations using generative adversarial nets.\narXiv preprint arXiv:1705.10479.\nHenderson, P.; Chang, W.-D.; Shkurti, F.; Hansen, J.; Meger,\nD.; and Dudek, G.\n2017.\nBenchmark environments for\nmultitask learning in continuous domains. ICML Lifelong\nLearning: A Reinforcement Learning Approach Workshop.\nHo, J., and Ermon, S. 2016. Generative adversarial imita-\ntion learning. In Advances in Neural Information Processing\nSystems, 4565–4573.\nJacobs, R. A.; Jordan, M. I.; Nowlan, S. J.; and Hinton, G. E.\n1991. Adaptive mixtures of local experts. Neural computa-\ntion 3(1):79–87.\nKingma, D. P., and Ba, J.\n2014.\nAdam: A Method for\nStochastic Optimization. ArXiv e-prints.\nKrishnan, S.; Garg, A.; Liaw, R.; Miller, L.; Pokorny, F. T.;\nand Goldberg, K. 2016. Hirl: Hierarchical inverse reinforce-\nment learning for long-horizon tasks with delayed rewards.\narXiv preprint arXiv:1604.06508.\nLi, Y.; Song, J.; and Ermon, S.\n2017.\nInfoGAIL: In-\nterpretable imitation learning from visual demonstrations.\narXiv preprint arXiv:1703.08840.\nLiu, Y., and Yao, X. 2002. Learning and evolution by min-\nimization of mutual information. In International Confer-\nence on Parallel Problem Solving from Nature, 495–504.\nSpringer.\nMerel, J.; Tassa, Y.; Srinivasan, S.; Lemmon, J.; Wang, Z.;\nWayne, G.; and Heess, N. 2017. Learning human behaviors\nfrom motion capture by adversarial imitation. arXiv preprint\narXiv:1707.02201.\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asyn-\nchronous methods for deep reinforcement learning. In In-\nternational Conference on Machine Learning, 1928–1937.\nNg, A. Y., and Russell, S. J. 2000. Algorithms for inverse re-\ninforcement learning. In Proceedings of the Seventeenth In-\nternational Conference on Machine Learning, 663–670. San\nFrancisco, CA, USA: Morgan Kaufmann Publishers Inc.\nSchulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; and Moritz,\nP. 2015. Trust region policy optimization. In Proceedings\nof the 32nd International Conference on Machine Learning\n(ICML-15), 1889–1897.\nSchulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel,\nP. 2016. High-dimensional continuous control using gener-\nalized advantage estimation. In Proceedings of the Interna-\ntional Conference on Learning Representations (ICLR).\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347.\nSermanet, P.; Xu, K.; and Levine, S. 2016. Unsupervised\nperceptual rewards for imitation learning.\narXiv preprint\narXiv:1612.06699.\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;\nHinton, G.; and Dean, J. 2017. Outrageously large neu-\nral networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538.\nSutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour,\nY. 2000. Policy gradient methods for reinforcement learning\nwith function approximation. In Advances in neural infor-\nmation processing systems, 1057–1063.\nSutton, R. S.; Precup, D.; and Singh, S. 1999. Between\nMDPs and semi-MDPs: A framework for temporal abstrac-\ntion in reinforcement learning. Artiﬁcial intelligence 112(1-\n2):181–211.\nTodorov, E.; Erez, T.; and Tassa, Y.\n2012.\nMuJoCo: A\nphysics engine for model-based control. In 2012 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems,\nIROS 2012, Vilamoura, Algarve, Portugal, October 7-12,\n2012, 5026–5033.\nvan Seijen, H.; Fatemi, M.; Romoff, J.; Laroche, R.; Barnes,\nT.; and Tsang, J. 2017. Hybrid reward architecture for rein-\nforcement learning. arXiv preprint arXiv:1706.04208.\nWang, Z.; Merel, J.; Reed, S.; Wayne, G.; de Freitas, N.;\nand Heess, N. 2017. Robust imitation of diverse behaviors.\narXiv preprint arXiv:1707.02747.\nZiebart, B. D.; Maas, A.; Bagnell, J. A.; and Dey, A. K.\n2008. Maximum entropy inverse reinforcement learning. In\nProceedings of the 23rd National Conference on Artiﬁcial\nIntelligence - Volume 3, 1433–1438. AAAI Press.\nSupplemental Material\nExpanded Equations\nThe expectation over the discriminator loss for the option case can be expanded:\nLΩ= Eω\nh\nπΩ,ζ(ω|s)Lˆθ,ω\ni\n+ Lreg\n=\nX\nω\nπΩ,ζ(ω|s)Lˆθ,ω + Lreg\n(10)\nAs can the regularization terms:\nLb =\nX\nω\n||E[πΩ(ω)] −τ||2\n≈\nX\nω\n|| 1\nmb\nmb\nX\ni\n(πΩ(ω|si)) −τ||2\n(11)\nLe = E\n\"\n||\n \n1\n||Ω||\nX\nω\nπΩ(ω)\n!\n−τ||2\n#\n≈1\nmb\nmb\nX\ni\n||\n \n1\n||Ω||\nX\nω\nπΩ(ω|si)\n!\n−τ||2\n(12)\nLv = −\nX\nω\nvarω{πΩ(s)}\n≈−\nX\nω\n1\nmb\nmb\nX\ni\n \nπΩ(ω|si) −\n \n1\nmb\nmb\nX\ni\nπΩ(ω|si)\n!!\n(13)\nExpert Collection\nThe expert demonstration rollouts (state sequences) for all OpenAI Gym (Brockman et al. 2016) environments were obtained\nfrom policies trained for 1000 iterations using Trust Region Policy Optimization (Schulman et al. 2015) with parameters\nKLmax = .01, generalized advantage estimation λ = .97, discount factor γ = .99 and batch size 25, 000 (rollout timesteps\nshared when updating the discriminator and policy). For the Roboschool (Schulman et al. 2017) Flagrun-v1 environment, the\nrollouts were obtained using the PPO pre-trained expert provided with Roboschool.\nExperimental Setup and Hyperparameters\nObservations are not normalized in all cases as we found that it did not help or hurt performance. In all cases for advantage\nestimation we use a value approximator as in (Schulman et al. 2015), which uses L-BFGS optimization with a mixing fraction\nof .1. That is, it uses the current prediction Vθ(s) and mixes it with the actual discounted returns with .1 belonging to the actual\ndiscounted returns and .9 belonging to the current prediction. This is identical to the original Trust Region Policy Optimization\n(TRPO) code as provided at: https://github.com/joschu/modular_rl/. We perform a maximum of 20 L-BFGS\niterations per value function update. For all the environments, we let the agent act until the maximum allowed timesteps of the\nspeciﬁc environment (as set by default in OpenAI Gym), gather the rollouts and keep the number of timesteps per batch desired.\nFor all policy optimization steps in both IRLGAN and OPTIONGAN, we use TRPO with the with parameters set to the same\nvalues as the ones used for the expert collection (KLmax = .01, generalized advantage estimation λ = .97, discount factor\nγ = .99 and batch size 25, 000), except for the Roboschool Flagrun Experiment where PPO was used instead, as explained in\nits respective section below.\nSimple Tasks and Transfer Tasks\nFor simple tasks we use 10 expert rollouts while for transfer tasks we use 40 expert rollouts (10 from each environment\nvariation).\n0\n100\n200\n300\n400\n500\nIterations\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage Return\nHopper-v1 (One Shot Transfer)\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n−1500\n−1000\n−500\n0\n500\n1000\n1500\n2000\n2500\nAverage Return\nHalfCheetah-v1 (One Shot Transfer)\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n0\n1000\n2000\n3000\n4000\nAverage Return\nWalker2d-v1 (One Shot Transfer)\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\nFigure 6: Experiments from 40 expert demonstrations from varied gravity environments without any demonstrations on the\nnovice training environment. Average returns from expert demonstrations across all mixed environments: 3657.71 (Hopper-\nv1), 4181.97 (HalfCheetah-v1), 4218.12 (Walker2d-v1).\n0\n100\n200\n300\n400\n500\nIterations\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage Return\nHopper-v1\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n−1000\n0\n1000\n2000\n3000\n4000\nAverage Return\nHalfCheetah-v1\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n0\n1000\n2000\n3000\n4000\nAverage Return\nWalker2d-v1\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\nFigure 7: Simple locomotion task learning curves. The True Average Return provided by the expert demonstrations are: 3778.82\n(Hopper-v1), 4156.94 (HalfCheetah-v1), 5528.51 (Walker2d-v1). Error bars indicate standard error of True Average Return\nacross 10 trial runs.\n.\n0\n1000\n2000\n3000\n4000\n5000\nIterations\n−500\n0\n500\n1000\n1500\n2000\n2500\n3000\nAverage Return\nRoboschoolHumanoidFlagrun-v1\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\n0\n100\n200\n300\n400\n500\nIterations\n0\n1000\n2000\n3000\n4000\nAverage Return\nHopperSimpleWall-v0\nIRLGAN\nOPTIONGAN (2 ops)\nOPTIONGAN (4 ops)\nFigure 8: Evaluation on RoboschoolHumanoidFlagrun-v1 environment (Schulman et al. 2017). Average expert rollout returns:\n2822.13.\nIRLGAN\nWe use a Gaussian Multilayer Perceptron policy as in (Schulman et al. 2015) with two 64 units hidden layers and\ntanh hidden layer activations. The output of the network gives the Gaussian mean and the standard deviation is modeled by a\nsingle learned variable as in (Schulman et al. 2015). Similarly for our discriminator network, we use the same architecture with\na sigmoid output, tanh hidden layer activations, and a learning rate of 1 · 10−3 for the discriminator. We do not use entropy\nregularization or l2 regularization as it resulted in worse performance. For every policy update we perform 3 discriminator\nupdates as we found the policy optimization step is able to handle this and results in faster learning.\nOPTIONGAN\nAligning with the IRLGAN networks, we make use of a Gaussian Multilayer Perceptron policy as in (Schul-\nman et al. 2015) with 2 hidden layers of 64 units with tanh hidden layer activations for our shared hidden layers. These hidden\nlayers connect to ||Ω|| options depending on the experiment (2 or 4). In this case the output of the network gives the Gaussian\nmean for each option and the standard deviation is modeled by a single learned variable per option. The policy-over-options\nis also modeled by a 2 layered 64 units network with tanh activations and a softmax output corresponding to the number of\noptions. For our discriminator, we use the same architecture with tanh hidden layer activations, a sigmoid output and ||Ω||\noutputs, one for each option. We use the policy over options to create a specialized mixture of experts model with a specialized\nloss function which converges to options. We use a learning rate of 1 · 10−3 for the discriminator. Same as in IRLGAN, we\ndo not make use of entropy regularization or l2 regularization as we found either regularizers to hurt performance. Instead we\nuse scaling factors for the regularization terms included in the loss: λb = 10.0, λe = 10.0, λv = 1.0 for the 2 options case and\nλb = 0.01, λe = 10.0, λv = 1.0 for the 4 options case. Again, we perform 3 discriminator updates per policy update\nRoboschoolHumanoidFlagrun-v1\nFor Roboschool experiments we use proximal policy optimization (PPO) with a clipping objective (Schulman et al. 2017) (clip-\nping parameter set to ϵ = .02). We perform 5 Adam (Kingma and Ba 2014) policy updates on the PPO clipping objective with\na learning rate of 1 · 10−3. The value function and advantage estimation parameters from previous experiments are maintained\nwhile our network architecture sizes are increased to (128, 128) and use ReLU activations instead of tanh.\nDecomposition of Rewards over Expert Demonstrations\nWe show that the trained policy-over-options network shows some intrinsic structure over the expert demonstrations.\nFigure 9: Probability distribution of πΩover options on expert demonstrations. Inherent structure is found in the demonstrations.\nFigure 10: State distribution of expert demonstrations projected onto a 2D plane to pair with Figure 5. Axes correspond to\nprojected state dimensions from the original 6-D state space using SVD.\nIn Figure 9 are shown the activation of the gating function across expert rollouts after training. We see that the underlying\ndivision in expert demonstrations is learned by the policy-over-options, which indicates that our method for training the policy-\nover-options induces it to learn a latent structure to the expert demonstrations and thus can beneﬁt in the transfer case since\neach option inherently specializes to be used in different environments. We ﬁnd that options specialized more clearly over the\nexperts with environments closest to the normal gravity environment, while the others use an even mixture of options. This is\ndue to the fact that the mixing specialized options are able to cover the state space of the non-specialized options as we can\nobserve from the state distribution of the expert demonstrations shown in Figure 10.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-09-20",
  "updated": "2017-11-24"
}