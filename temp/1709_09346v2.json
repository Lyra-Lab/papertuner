{
  "id": "http://arxiv.org/abs/1709.09346v2",
  "title": "Cold-Start Reinforcement Learning with Softmax Policy Gradient",
  "authors": [
    "Nan Ding",
    "Radu Soricut"
  ],
  "abstract": "Policy-gradient approaches to reinforcement learning have two common and\nundesirable overhead procedures, namely warm-start training and sample variance\nreduction. In this paper, we describe a reinforcement learning method based on\na softmax value function that requires neither of these procedures. Our method\ncombines the advantages of policy-gradient methods with the efficiency and\nsimplicity of maximum-likelihood approaches. We apply this new cold-start\nreinforcement learning method in training sequence generation models for\nstructured output prediction problems. Empirical evidence validates this method\non automatic summarization and image captioning tasks.",
  "text": "Cold-Start Reinforcement Learning with\nSoftmax Policy Gradient\nNan Ding\nGoogle Inc.\nVenice, CA 90291\ndingnan@google.com\nRadu Soricut\nGoogle Inc.\nVenice, CA 90291\nrsoricut@google.com\nAbstract\nPolicy-gradient approaches to reinforcement learning have two common and un-\ndesirable overhead procedures, namely warm-start training and sample variance\nreduction. In this paper, we describe a reinforcement learning method based on a\nsoftmax value function that requires neither of these procedures. Our method com-\nbines the advantages of policy-gradient methods with the efﬁciency and simplicity\nof maximum-likelihood approaches. We apply this new cold-start reinforcement\nlearning method in training sequence generation models for structured output\nprediction problems. Empirical evidence validates this method on automatic sum-\nmarization and image captioning tasks.\n1\nIntroduction\nReinforcement learning is the study of optimal sequential decision-making in an environment [16]. Its\nrecent developments underpin a large variety of applications related to robotics [11, 5] and games [20].\nPolicy search in reinforcement learning refers to the search for optimal parameters for a given policy\nparameterization [5]. Policy search based on policy-gradient [26, 21] has been recently applied to\nstructured output prediction for sequence generations. These methods alleviate two common problems\nthat approaches based on training with the Maximum-likelihood Estimation (MLE) objective exhibit,\nnamely the exposure-bias problem [24, 19] and the wrong-objective problem [19, 15] (more on this\nin Section 2). As a result of addressing these problems, policy-gradient methods achieve improved\nperformance compared to MLE training in various tasks, including machine translation [19, 7], text\nsummarization [19], and image captioning [19, 15].\nPolicy-gradient methods for sequence generation work as follows: ﬁrst the model proposes a sequence,\nand the ground-truth target is used to compute a reward for the proposed sequence with respect to\nthe reward of choice (using metrics known to correlate well with human-rated correctness, such\nas ROUGE [13] for summarization, BLEU [18] for machine translation, CIDEr [23] or SPICE [1]\nfor image captioning, etc.). The reward is used as a weight for the log-likelihood of the proposed\nsequence, and learning is done by optimizing the weighted average of the log-likelihood of the\nproposed sequences. The policy-gradient approach works around the difﬁculty of differentiating the\nreward function (the majority of which are non-differentiable) by using it as a weight. However, since\nsequences proposed by the model are also used as the target of the model, they are very noisy and\ntheir initial quality is extremely poor. The difﬁculty of aligning the model output distribution with\nthe reward distribution over the large search space of possible sequences makes training slow and\ninefﬁcient∗. As a result, overhead procedures such as warm-start training with the MLE objective\nand sophisticated methods for sample variance reduction are required to train with policy gradient.\n∗Search space size is O(V T ), where V is the number of word types in the vocabulary (typically between 104\nand 106) and T is the the sequence length (typically between 10 and 50), hence between 1040 and 10300.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1709.09346v2  [cs.LG]  13 Oct 2017\nThe fundamental reason for the inefﬁciency of policy-gradient–based reinforcement learning is the\nlarge discrepancy between the model-output distribution and the reward distribution, especially in\nthe early stages of training. If, instead of generating the target based solely on the model-output\ndistribution, we generate it based on a proposal distribution that incorporates both the model-output\ndistribution and the reward distribution, learning would be efﬁcient, and neither warm-start training\nnor sample variance reduction would be needed. The outstanding problem is ﬁnding a value function\nthat induces such a proposal distribution.\nIn this paper, we describe precisely such a value function, which in turn gives us a Softmax Policy\nGradient (SPG) method. The softmax terminology comes from the equation that deﬁnes this value\nfunction, see Section 3. The gradient of the softmax value function is equal to the average of the\ngradient of the log-likelihood of the targets whose proposal distribution combines both model output\ndistribution and reward distribution. Although this distribution is infeasible to sample exactly, we\nshow that one can draw samples approximately, based on an efﬁcient forward-pass sampling scheme.\nTo balance the importance between the model output distribution and the reward distribution, we use\na bang-bang [8] mixture model to combine the two distributions. Such a scheme removes the need of\nﬁne-tuning the weights across different datasets and throughout the learning epochs. In addition to\nusing a main metric as the task reward (ROUGE, CIDEr, etc.), we show that one can also incorporate\nadditional, task-speciﬁc metrics to enforce various properties on the output sequences (Section 4).\nWe numerically evaluate our method on two sequence generation benchmarks, a headline-generation\ntask and an image-caption–generation task (Section 5). In both cases, the SPG method signiﬁcantly\nimproves the accuracy, compared to maximum-likelihood and other competing methods. Finally, it is\nworth noting that although the training and inference of the SPG method in the paper is mainly based\non sequence learning, the idea can be extended to other reinforcement learning applications.\n2\nLimitations of Existing Sequence Learning Regimes\nOne of the standard approaches to sequence-learning training is Maximum-likelihood Estimation\n(MLE). Given a set of inputs X =\n\b\nxi\t\nand target sequences Y =\n\b\nyi\t\n, the MLE loss function is:\nLMLE(θ) =\nX\ni\nLi\nMLE(θ), where Li\nMLE(θ) = −log pθ(yi|xi).\n(1)\nHere xi and yi =\n\b\nyi\n1, . . . , yi\nT\n\t\ndenote the input and the target sequence of the i-th example,\nrespectively. For instance, in the image captioning task, xi is the image of the i-th example, and yi is\nthe groundtruth caption of the i-th example.\nAlthough widely used in many different applications, MLE estimation for sequence learning suffers\nfrom the exposure-bias problem [24, 19]. Exposure-bias refers to training procedures that produce\nbrittle models that have only been exposed to their training data distribution but not to their own\npredictions. At training-time, log pθ(yi|xi) = P\nt log pθ(yi\nt|xi, yi\n1...t−1), i.e. the loss of the t-th\nword is conditional on the true previous-target tokens yi\n1...t−1. However, since yi\n1...t−1 are unavailable\nduring inference, replacing them with tokens zi\n1...t−1 generated by pθ(zi\n1...t−1|xi) yields a signiﬁcant\ndiscrepancy between how the model is used at training time versus inference time. The exposure-bias\nproblem has recently received attention in neural-network settings with the “data as demonstrator” [24]\nand “scheduled sampling” [3] approaches. Although improving model performance in practice, such\nproposals have been shown to be statistically inconsistent [10], and still need to perform MLE-based\nwarm-start training.\nA more general approach to MLE is the Reward Augmented Maximum Likelihood (RAML)\nmethod [17]. RAML makes the correct observation that, under MLE, all alternative outputs are\nequally penalized through normalization, regardless of their relationship to the ground-truth target.\nInstead, RAML corrects for this shortcoming using an objective of the form:\nLi\nRAML(θ) = −\nX\nzi\nrR(zi|yi) log pθ(zi|xi).\n(2)\nwhere rR(zi|yi) =\nexp(R(zi|yi)/τ)\nP\nzi exp(R(zi|yi)/τ). This formulation uses R(zi|yi) to denote the value of a\nsimilarity metric R between zi and yi (the reward), with yi = argmaxzi R(zi|yi); τ is a temperature\nhyper-parameter to control the peakiness of this reward distribution. Since the sum over all zi for\n2\nthe reward distribution rR(zi|yi) in Eq. (2) is infeasible to compute, a standard approach is to\ndraw J samples zij from the reward distribution, and approximate the expectation by Monte Carlo\nintegration:\nLi\nRAML(θ) ≃−1\nJ\nJ\nX\nj=1\nlog pθ(zij|xi).\n(3)\nAlthough a clear improvement over Eq. (1), the sampling for zij in Eq. (3) is solely based on\nrR(zi|yi) and completely ignores the model probability. At the same time, this technique does not\naddress the exposure bias problem at all.\nA different approach, based on reinforcement learning methods, achieves sequence learning following\na policy-gradient method [21]. Its appeal is that it not only solves the exposure-bias problem, but also\ndirectly alleviates the wrong-objective problem [19, 15] of MLE approaches. Wrong-objective refers\nto the critique that MLE-trained models tend to have suboptimal performance because such models\nare trained on a convenient objective (i.e., maximum likelihood) rather than a desirable objective\n(e.g., a metric known to correlate well with human-rated correctness). The policy-gradient method\nuses a value function VP G, which is equivalent to a loss LP G deﬁned as:\nLi\nP G(θ) = −V i\nP G(θ), V i\nP G(θ) = Epθ(zi|xi)[R(zi|yi)].\n(4)\nThe gradient for Eq. (4) is:\n∂\n∂θLi\nP G(θ) = −\nX\nzi\npθ(zi|xi)R(zi|yi) ∂\n∂θ log pθ(zi|xi).\n(5)\nSimilar to (3), one can draw J samples zij from pθ(zi|xi) to approximate the expectation by Monte-\nCarlo integration:\n∂\n∂θLi\nP G(θ) ≃−1\nJ\nJ\nX\nj=1\nR(zij|yi) ∂\n∂θ log pθ(zij|xi).\n(6)\nHowever, the large discrepancy between the model prediction distribution pθ(zi|xi) and the reward\nR(zi|yi)’s values, which is especially acute during the early training stages, makes the Monte-Carlo\nintegration extremely inefﬁcient. As a result, this method also requires a warm-start phase in which\nthe model distribution achieves some local maximum with respect to a reward-metric–free objective\n(e.g., MLE), followed by a model reﬁnement phase in which reward-metric–based PG updates are\nused to reﬁne the model [19, 7, 15]. Although this combination achieves better results in practice\ncompared to pure likelihood-based approaches, it is unsatisfactory from a theoretical and modeling\nperspective, as well as inefﬁcient from a speed-to-convergence perspective. Both these issues are\naddressed by the value function we describe next.\n3\nSoftmax Policy Gradient (SPG) Method\nIn order to smoothly incorporate both the model distribution pθ(zi|xi) and the reward metric R(zi|yi),\nwe replace the value function from Eq. 4 with a Softmax value function for Policy Gradient (SPG),\nVSP G, equivalent to a loss LSP G deﬁned as:\nLi\nSP G(θ) = −V i\nSP G(θ), V i\nSP G(θ) = log\n\u0000Epθ(zi|xi)[exp(R(zi|yi))]\n\u0001\n.\n(7)\nBecause the value function for example i is equal to Softmaxzi(log pθ(zi|xi) + R(zi|yi)), where\nSoftmaxzi(·) = log P\nzi exp(·), we call it the softmax value function. Note that the softmax value\nfunction from Eq. (7) is the dual of the entropy-regularized policy search (REPS) objective [5, 16]\nL(q) = Eq[R] + KL(q|pθ). However, our learning and sampling procedures are signiﬁcantly\ndifferent from REPS, as shown in what follows.\nThe gradient for Eq. (7) is:\n∂\n∂θLi\nSP G(θ) = −\n1\nP\nzi pθ(zi|xi) exp(R(zi|yi))\n X\nzi\npθ(zi|xi) exp(R(zi|yi)) ∂\n∂θ log pθ(zi|xi)\n!\n= −\nX\nzi\nqθ(zi|xi, yi) ∂\n∂θ log pθ(zi|xi)\n(8)\nwhere qθ(zi|xi, yi) =\n1\nP\nzi pθ(zi|xi) exp(R(zi|yi))pθ(zi|xi) exp(R(zi|yi)).\n3\nMLE target\nRAML targets\nPG targets\nSPG targets\nᬍᶚ\n᳋ᶚ\nrR\nFigure 1: Comparing the target samples for\nMLE, RAML (the rR distribution), PG (the\npθ distribution), and SPG (the qθ distribution).\nThere are several advantages associated with the gra-\ndient from Eq. (8).\nFirst, qθ(zi|xi, yi) takes into account both pθ(zi|xi)\nand R(zi|yi). As a result, Monte Carlo integration\nover qθ-samples approximates Eq. (8) better, and has\nsmaller variance compared to Eq. (5). This allows\nour model to start learning from scratch without the\nwarm-start and variance-reduction crutches needed\nby previously-proposed PG approaches.\nSecond, as Figure 1 shows, the samples for the SPG\nmethod (pentagons) lie between the ground-truth tar-\nget distribution (triangle and circles) and the model\ndistribution (squares). These targets are both easier\nto learn by pθ compared to ground-truth–only targets\nlike the ones for MLE (triangle) and RAML (circles),\nand also carry more information about the ground-truth target compared to model-only samples (PG\nsquares). This formulation allows us to directly address the exposure-bias problem, by allowing the\nmodel distribution to learn at training time how to deal with events conditioned on model-generated\ntokens, similar with what happens at inference time (more on this in Section 3.2). At the same time,\nthe updates used for learning rely heavily on the inﬂuence of the reward metric R(zi|yi), therefore\ndirectly addressing the wrong-objective problem. Together, these properties allow the model to\nachieve improved accuracy.\nThird, although qθ is infeasible for exact sampling, since both pθ(zi|xi) and exp(R(zi|yi)) are\nfactorizable across zi\nt (where zi\nt denotes the t-th word of the i-th output sequence), we can apply\nefﬁcient approximate inference for the SPG method as shown in the next section.\n3.1\nInference\nIn order to estimate the gradient from Eq. (8) with Monte-Carlo integration, one needs to be able\nto draw samples from qθ(zi|xi, yi). To tackle this problem, we ﬁrst decompose R(zi|yi) along the\nt-axis:\nR(zi|yi) =\nT\nX\nt=1\nR(zi\n1:t|yi) −R(zi\n1:t−1|yi)\n|\n{z\n}\n≜∆ri\nt(zi\nt|yi,zi\n1:t−1)\n,\nwhere R(zi\n1:t|yi) −R(zi\n1:t−1|yi) characterizes the reward increment for zi\nt. Using the reward\nincrement notation, we can rewrite:\nqθ(zi|xi, yi) =\n1\nZθ(xi, yi)\nT\nY\nt=1\nexp(log pθ(zi\nt|zi\n1:t−1, xi) + ∆ri\nt(zi\nt|yi, zi\n1:t−1))\nwhere Zθ(xi, yi) is the partition function equal to the sum over all conﬁgurations of zi. Since the\nnumber of such conﬁgurations grows exponentially with respect to the sequence-length T, directly\ndrawing from qθ(zi|xi, yi) is infeasible. To make the inference efﬁcient, we replace qθ(zi|xi, yi)\nwith the following approximate distribution:\n˜qθ(zi|xi, yi) =\nT\nY\nt=1\n˜qθ(zi\nt|xi, yi, zi\n1:t−1),\nwhere\n˜qθ(zi\nt|xi, yi, zi\n1:t−1) =\n1\n˜Zθ(xi, yi, zi\n1:t−1)\nexp(log pθ(zi\nt|zi\n1:t−1, xi) + ∆ri\nt(zi\nt|yi, zi\n1:t−1)).\nBy replacing qθ in Eq. (8) with ˜qθ, we obtain:\n∂\n∂θLi\nSP G(θ) = −\nX\nzi\nqθ(zi|xi, yi) ∂\n∂θ log pθ(zi|xi)\n≃−\nX\nzi\n˜qθ(zi|xi, yi) ∂\n∂θ log pθ(zi|xi) ≜∂\n∂θ\n˜Li\nSP G(θ)\n(9)\n4\nCompared to Zθ(xi, yi), ˜Zθ(xi, yi, zi\n1:t−1) sums over the conﬁgurations of one zi\nt only. Therefore,\nthe cost of drawing one zi from ˜qθ(zi|xi, yi) grows only linearly with respect to T. Furthermore, for\ncommon reward metrics such as ROUGE and CIDEr, the computation of ∆ri\nt(zi\nt|yi, zi\n1:t−1) can be\ndone in O(T) instead of O(V ) (where V is the size of the state space for zi\nt, i.e., vocabulary size).\nThat is because the maximum number of unique words in yi is T, and any words not in yi have the\nsame reward increment. When we limit ourselves to J = 1 sample for each example in Eq. (9), the\napproximate SPG inference time of each example is similar to the inference time for the gradient of\nthe MLE objective. Combined with the empirical ﬁndings in Section 5 (Figure 3) where the steps\nfor convergence are comparable, we conclude that the time for convergence for the SPG method is\nsimilar to the MLE based method.\n3.2\nBang-bang Rewarded SPG Method\nOne additional difﬁculty for the SPG method is that the model’s log-probability values\nlog pθ(zi\nt|zi\n1:t−1, xi) and the reward-increment values R(zi\n1:t|yi) −R(zi\n1:t−1|yi) are not on the\nsame scale. In order to balance the impact of these two factors, we need to weigh them appropriately.\nFormally, we achieve this by adding a weight wi\nt to the reward increments: ∆ri\nt(zi\nt|yi, zi\n1:t−1, wi\nt) ≜\nwi\nt·∆ri\nt(zi\nt|yi, zi\n1:t−1) so that the total reward R(zi|yi, wi) = PT\nt=1 ∆ri\nt(zi\nt|yi, zi\n1:t−1, wi\nt). The ap-\nproximate proposal distribution becomes ˜qθ(zi|xi, yi, wi) = QT\nt=1 ˜qθ(zi\nt|xi, yi, zi\n1:t−1, wi\nt), where\n˜qθ(zi\nt|xi, yi, zi\n1:t−1, wi\nt) ∝exp(log pθ(zi\nt|zi\n1:t−1, xi) + ∆ri\nt(zi\nt|yi, zi\n1:t−1, wi\nt)).\nThe challenge in this case is to choose an appropriate weight wi\nt, because log pθ(zi\nt|zi\n1:t−1, xi) varies\nheavily for different i, t, as well as across different iterations and tasks.\nIn order to minimize the efforts for ﬁne-tuning the reward weights, we propose a bang-bang rewarded\nsoftmax value function, equivalent to a loss LBBSP G deﬁned as:\nLi\nBBSP G(θ) = −\nX\nwi\np(wi) log\n\u0000Epθ(zi|xi)[exp(R(zi|yi, wi))]\n\u0001\n,\n(10)\nand\n∂\n∂θ\n˜Li\nBBSP G(θ) = −\nX\nwi\np(wi)\nX\nzi\n˜qθ(zi|xi, yi, wi) ∂\n∂θ log pθ(zi|xi)\n|\n{z\n}\n≜−∂\n∂θ ˜Li\nSP G(θ|wi)\n,\n(11)\nwhere p(wi) = Q\nt p(wi\nt) and p(wi\nt = 0) = pdrop = 1 −p(wi\nt = W). Here W is a sufﬁciently\nlarge number (e.g., 10,000), pdrop is a hyper-parameter in [0, 1]. The name bang-bang is borrowed\nfrom control theory [8], and refers to a system which switches abruptly between two extreme states\n(namely W and 0).\nt\n1\n2\n3\n4\n5\n6\n7\nyt\na\nman\nis\nsitting\nin\nthe\npark\nW\nW\nW\n0\nW\n...\n...\nwt\nzt\na\nman\nis\nin\nthe\n...\n...\nargmax ᵂr5(z5|y, z1:4) = ‘the’ ≠ y5 = ‘in’\nFigure 2: An example of sequence generation\nwith the bang-bang reward weights. z4 =\n”in” is sampled from the model distribution\nsince w4 = 0. Although w5 = W, z5 =\n”the” ̸= y5 because z4 = ”in”.\nWhen wi\nt = W, the term ∆ri\nt(zi\nt|yi, zi\n1:t−1, wi\nt)\noverwhelms log pθ(zi\nt|zi\n1:t−1, xi), so the sampling of\nzi\nt is decided by the reward increment of zi\nt. It is im-\nportant to emphasize that in general the groundtruth\nlabel yi\nt ̸= argmaxzi\nt ∆ri\nt(zi\nt|yi, zi\n1:t−1), because\nzi\n1:t−1 may not be the same as yi\n1:t−1 (see an ex-\nample in Figure 2). The only special case is when\npdrop = 0, which forces wi\nt to always equal W, and\nimplies zi\nt is always equal† to yi\nt (and therefore the\nSPG method reduces to the MLE method).\nOn the other hand, when wi\nt = 0, by deﬁnition\n∆ri\nt(zi\nt|yi, zi\n1:t−1, wi\nt) = 0. In this case, the sam-\npling of zi\nt is based only on the model prediction\ndistribution pθ(zi\nt|zi\n1:t−1, xi), the same situation we\nhave at inference time. Furthermore, we have the\nfollowing lemma (with the proof provided in the Supplementary Material):\n†This follows from recursively applying R’s property that yi\nt = argmaxzi\nt ∆ri\nt(zi\nt|yi, zi\n1:t−1 = yi\n1:t−1).\n5\nLemma 1 When wi\nt = 0,\nX\nzi\n˜qθ(zi|xi, yi, wi) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1) = 0.\nAs a result, ∂\n∂θ ˜Li\nSP G(θ|wi) is very different from traditional PG-method gradients, in that only the zi\nt\nwith wi\nt ̸= 0 are included. To see that, using the fact that log pθ(zi|xi) = PT\nt=1 log pθ(zi\nt|xi, zi\n1:t−1),\n∂\n∂θ\n˜Li\nSP G(θ|wi) = −\nX\nt\nX\nzi\n˜qθ(zi|xi, yi, wi) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1),\n(12)\nUsing the result of Lemma 1, Eq. (12) is equal to:\n∂\n∂θ\n˜Li\nSP G(θ|wi) = −\nX\n{t:wi\nt̸=0}\nX\nzi\n˜qθ(zi|xi, yi, wi) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1)\n= −\nX\nzi\n˜qθ(zi|xi, yi, wi)\nX\n{t:wi\nt̸=0}\n∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1)\n(13)\nUsing Monte-Carlo integration, we approximate Eq. (11) by ﬁrst drawing wij from p(wi) and then\niteratively drawing zij\nt from ˜qθ(zi\nt|xi, zi\n1:t−1, yi, wij\nt ) for t = 1, . . . , T. For larger values of pdrop,\nthe wij sample contains more wij\nt = 0 and the resulting zij contains proportionally more samples\nfrom the model prediction distribution (with a direct effect on alleviating the exposure-bias problem).\nAfter zij is obtained, only the log-likelihood of zij\nt when wij\nt ̸= 0 are included in the loss:\n∂\n∂θ\n˜Li\nBBSP G(θ) ≃−1\nJ\nJ\nX\nj=1\nX\nn\nt:w\nij\nt ̸=0\no\n∂\n∂θ log pθ(zij\nt |xi, zij\n1:t−1).\n(14)\nThe details about the gradient evaluation for the bang-bang rewarded softmax value function are\ndescribed in Algorithm 1 of the Supplementary Material.\n4\nAdditional Reward Functions\nBesides the main reward function R(zi|yi), additional reward functions can be used to enforce\ndesirable properties for the output sequences. For instance, in summarization, we occasionally ﬁnd\nthat the decoded output sequence contains repeated words, e.g. \"US R&B singer Marie Marie Marie\nMarie ...\". In this framework, this can be directly ﬁxed by using an additional auxiliary reward\nfunction that simply rewards negatively two consecutive tokens in the generated sequence:\nDUPi\nt =\n\u001a−1\nif zi\nt = zi\nt−1,\n0\notherwise.\nIn conjunction with the bang-bang weight scheme, the introduction of such a reward function has the\nimmediate effect of severely penalizing such “stuttering” in the model output; the decoded sequence\nafter applying the DUP negative reward becomes: \"US R&B singer Marie Christina has ...\".\nAdditionally, we can use the same approach to correct for certain biases in the forward sampling\napproximation. For example, the following function negatively rewards the end-of-sentence symbol\nwhen the length of the output sequence is less than that of the ground-truth target sequence |yi|:\nEOSi\nt =\n\u001a−1\nif zi\nt = </S> and t < |yi|,\n0\notherwise.\nA more detailed discussion about such reward functions is available in the Supplementary Material.\nDuring training, we linearly combine the main reward function with the auxiliary functions:\n∆ri\nt(zi\nt|yi, zi\n1:t−1, wi\nt) = wi\nt ·\n\u0000R(zi\n1:t|yi) −R(zi\n1:t−1|yi) + DUPi\nt + EOSi\nt\n\u0001\n,\nwith W = 10, 000. During testing, since the ground-truth target yi is unavailable, this becomes:\n∆ri\nt(zi\nt|yi, zi\n1:t−1, W) = W · DUPi\nt.\n6\n5\nExperiments\nWe numerically evaluate the proposed softmax policy gradient (SPG) method on two sequence\ngeneration benchmarks: a document-summarization task for headline generation, and an automatic\nimage-captioning task. We compare the results of the SPG method against the standard maximum\nlikelihood estimation (MLE) method, as well as the reward augmented maximum likelihood (RAML)\nmethod [17]. Our experiments indicate that the SPG method outperforms signiﬁcantly the other\napproaches on both the summarization and image-captioning tasks.\nWe implemented all the algorithms using TensorFlow 1.0 [6]. For the RAML method, we used\nτ = 0.85 which was the best performer in [17]. For the SPG algorithm, all the results were obtained\nusing a variant of ROUGE [13] as the main reward metric R, and J = 1 (sample one target for each\nexample, see Eq. (14)). We report the impact of the pdrop for values in {0.2, 0.4, 0.6, 0.8}.\nIn addition to using the main reward-metric for sampling targets, we also used it to weight the loss\nfor target zij, as we found that it improved the performance of the SPG algorithm. We also applied\na naive version of the policy gradient (PG) algorithm (without any variance reduction) by setting\npdrop = 0.0, W →0, but failed to train any meaningful model with cold-start. When starting from a\npre-trained MLE checkpoint, we found that it was unable to improve the original MLE result. This\nresult conﬁrms that variance-reduction is a requirement for the PG method to work, whereas our SPG\nmethod is free of such requirements.\n5.1\nSummarization Task: Headline Generation\nHeadline generation is a standard text generation task, taking as input a document and generating a\nconcise summary/headline for it. In our experiments, the supervised data comes from the English\nGigaword [9], and consists of news-articles paired with their headlines. We use a training set of\nabout 6 million article-headline pairs, in addition to two randomly-extracted validation and evaluation\nsets of 10K examples each. In addition to the Gigaword evaluation set, we also report results on the\nstandard DUC-2004 test set. The DUC-2004 consists of 500 news articles paired with four different\nhuman-generated groundtruth summaries, capped at 75 bytes.‡ The expected output is a summary of\nroughly 14 words, created based on the input article.\nMethod\nGigaword-10K\nDUC-2004\nMLE\n35.2 ± 0.3\n22.6 ± 0.6\nRAML\n36.4 ± 0.2\n23.1 ± 0.6\nSPG 0.2\n36.6 ± 0.2\n23.5 ± 0.6\nSPG 0.4\n37.8 ± 0.2\n24.3 ± 0.5\nSPG 0.6\n37.4 ± 0.2\n24.1 ± 0.5\nSPG 0.8\n37.3 ± 0.2\n24.6 ± 0.5\nTable 1: The F1 ROUGE-L scores (with\nstandard errors) for headline generation.\nWe use the sequence-to-sequence recurrent neural net-\nwork with attention model [2]. For encoding, we use\na three-layer, 512-dimensional bidirectional RNN ar-\nchitecture, with a Gated Recurrent Unit (GRU) as the\nunit-cell [4]; for decoding, we use a similar three-layer,\n512-dimensional GRU-based architecture. Both the en-\ncoder and decoder networks use a shared vocabulary\nand embedding matrix for encoding/decoding the word\nsequences, with a vocabulary consisting of 220K word\ntypes and a 512-dimensional embedding. We truncate\nthe encoding sequences to a maximum of 30 tokens, and the decoding sequences to a maximum of\n15 tokens. The model is optimized using ADAGRAD with a mini-batch size of 200, a learning rate\nof 0.01, and gradient clipping with norm equal to 4. We use 40 workers for computing the updates,\nand 10 parameter servers for model storing and (asynchronous and distributed) updating. We run\nthe training procedure for 10M steps and pick the checkpoint with the best ROUGE-2 score on the\nGigaword validation set.\nWe report ROUGE-L scores on the Gigaword evaluation set, as well as the DUC-2004 set, in Table 1.\nThe scores are computed using the standard pyrouge package§, with standard errors computed using\nbootstrap resampling [12]. As the numerical values indicate, the maximum performance is achieved\nwhen pdrop is in mid-range, with 37.8 F1 ROUGE-L at pdrop = 0.4 on the large Gigaword evaluation\nset (a larger range for pdrop between 0.4 and 0.8 gives comparable scores on the smaller DUC-2004\nset). These numbers are signiﬁcantly better compared to RAML (36.4 on Gigaword-10K), which in\nturn is signiﬁcantly better compared to MLE (35.2).\n‡This dataset is available by request at http://duc.nist.gov/data.html.\n§Available at pypi.python.org/pypi/pyrouge/0.1.3\n7\n5.2\nAutomatic Image-Caption Generation\nValidation-4K\nC40\nMethod\nCIDEr\nROUGE-L\nCIDEr\nMLE\n0.968\n37.7 ± 0.1\n0.94\nRAML\n0.997\n38.0 ± 0.1\n0.97\nSPG 0.2\n1.001\n38.0 ± 0.1\n0.98\nSPG 0.4\n1.013\n38.1 ± 0.1\n1.00\nSPG 0.6\n1.033\n38.2 ± 0.1\n1.01\nSPG 0.8\n1.009\n37.7 ± 0.1\n1.00\nTable 2: The CIDEr (with the coco-caption\npackage) and ROUGE-L (with the pyrouge\npackage) scores for image captioning on\nMSCOCO.\nFor the image-captioning task, we use the standard\nMSCOCO dataset [14]. The MSCOCO dataset contains\n82K training images and 40K validation images, each\nwith at least 5 groundtruth captions. The results are\nreported using the numerical values for the C40 testset\nreported by the MSCOCO online evaluation server¶.\nFollowing standard practice, we combine the training\nand validation datasets for training our model, and hold\nout a subset of 4K images as our validation set.\nOur model architecture is simple, following the ap-\nproach taken by the Show-and-Tell approach [25]. We\nuse a one 512-dimensional RNN architecture with an\nLSTM unit-cell, with a dropout rate equal of 0.3 ap-\nplied to both input and output of the LSTM layer. We use the same vocabulary size of 8,854\nword-types as in [25], with 512-dimensional word-embeddings. We truncate the decoding sequences\nto a maximum of 15 tokens. The input image is embedded by ﬁrst passing it through a pretrained\nInception-V3 network [22], and then projected to a 512-dimensional vector. The model is optimized\nusing ADAGRAD with a mini-batch size of 25, a learning rate of 0.01, and gradient clipping with\nnorm equal to 4. We run the training procedure for 4M steps and pick the checkpoint of the best\nCIDEr score [23] on our held-out 4K validation set.\n0\n500000\n1000000\n1500000\n2000000\n2500000\nSteps\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\n1.02\n1.04\nCIDER Score\nMLE\nRAML\nSPG 0.6\nFigure 3: Number of training steps vs.\nCIDEr scores (on Validation-4K) for var-\nious learning regimes.\nWe report both CIDEr and ROUGE-L scores on our\n4K Validation set, as well as CIDEr scores on the of-\nﬁcial C40 testset as reported by the MSCOCO online\nevaluation server, in Table 2. The CIDEr scores are re-\nported using the coco-caption evaluation toolkit∥, while\nROUGE-L scores are reported using the standard py-\nrouge package (note that these ROUGE-L scores are\ngenerally lower than those reported by the coco-caption\ntoolkit, as it reports an average score over multiple\nreference, while the latter reports the maximum).\nThe evaluation results indicate that the SPG method is\nsuperior to both the MLE and RAML methods. The\nmaximum score is obtained with pdrop = 0.6, with a\nCIDEr score of 1.01 on the C40 testset. In contrast,\non the same testset, the RAML method has a CIDEr\nscore of 0.97, and the MLE method a score of 0.94. In\nFigure 3, we show that the number of steps for SPG to converge is similar to the one for MLE/RAML.\nWith the per-step inference cost of those methods being similar (see Section 3.1), the overall conver-\ngence time for the SPG method is similar to the MLE and RAML methods.\n6\nConclusion\nThe reinforcement learning method presented in this paper, based on a softmax value function, is\nan efﬁcient policy-gradient approach that eliminates the need for warm-start training and sample\nvariance reduction during policy updates. We show that this approach allows us to tackle sequence\ngeneration tasks by training models that avoid two long-standing issues: the exposure-bias problem\nand the wrong-objective problem. Experimental results conﬁrm that the proposed method achieves\nsuperior performance on two different structured output prediction problems, one for text-to-text\n(automatic summarization) and one for image-to-text (automatic image captioning). We plan to\nexplore and exploit the properties of this method for other reinforcement learning problems as well\nas the impact of various, more-advanced reward functions on the performance of the learned models.\n¶Available at http://mscoco.org/dataset/#captions-eval.\n∥Available at https://github.com/tylin/coco-caption.\n8\nAcknowledgments\nWe greatly appreciate Sebastian Goodman for his contributions to the experiment code. We would\nalso like to acknowledge Ning Ye and Zhenhai Zhu for their help with the image captioning model\ncalibration as well as the anonymous reviewers for their valuable comments.\nReferences\n[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: semantic\npropositional image caption evaluation. In ECCV, 2016.\n[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align\nand translate. In Proceedings of ICLR, 2015.\n[3] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for\nsequence prediction with recurrent neural networks.\nIn Advances in Neural Information\nProcessing Systems 28, pages 1171–1179. 2015.\n[4] K. Cho, B. van Merrienboer, C. Gülçehre, D. Bahdanau, F. Bougares, H. Schwenk, and\nY. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine\ntranslation. In Proceedings of EMNLP, pages 1724–1734, 2014.\n[5] Marc P. Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics.\nFoundations and Trends R⃝in Robotics, 2(1–2):1–142, 2013. ISSN 1935-8253.\n[6] M. Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.\nURL http://tensorflow.org/.\n[7] Y. Wu et al. Google’s neural machine translation system: Bridging the gap between human and\nmachine translation. CoRR, abs/1609.08144, 2016.\n[8] L. C. Evans. An introduction to mathematical optimal control theory. Preprint, version 0.2.\n[9] David Graff and Christopher Cieri. English Gigaword Fifth Edition LDC2003T05. In Linguistic\nData Consortium, Philadelphia, 2003.\n[10] Ferenc Huszar. How (not) to train your generative model: Scheduled sampling, likelihood,\nadversary? CoRR, abs/1511.05101, 2015.\n[11] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238–1274, 2013.\n[12] Philipp Koehn. Statistical signiﬁcance tests for machine translation evaluation. In Proceedings\nof EMNLP, pages 388—-395, 2004.\n[13] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using\nlongest common subsequence and skip-bigram statistics. In Proceedings of ACL, 2004.\n[14] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO:\ncommon objects in context. CoRR, abs/1405.0312, 2014.\n[15] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Optimization of image\ndescription metrics using policy gradient methods. In International Conference on Computer\nVision (ICCV), 2017.\n[16] Gergely Neu, Anders Jonsson, and Vicenç Gómez. A uniﬁed view of entropy-regularized\nmarkov decision processes. CoRR, abs/1705.07798, 2017.\n[17] M. Norouzi, S. Bengio, Z. Chen, N. Jaitly, M. Schuster, Y. Wu, and D. Schuurmans. Reward\naugmented maximum likelihood for neural structured prediction. In Advances in Neural\nInformation Processing Systems 29, pages 1723–1731, 2016.\n[18] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic\nevaluation of machine translation. In Proceedings of ACL, 2002.\n9\n[19] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level\ntraining with recurrent neural networks. CoRR, abs/1511.06732, 2015.\n[20] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-\ntering the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489,\n2016.\n[21] RS Sutton, D McAllester, S Singh, and Y Mansour. Policy gradient methods for reinforcement\nlearning with function approximation. In NIPS, 1999.\n[22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. volume abs/1512.00567, 2015.\n[23] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In The IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2015.\n[24] Arun Venkatraman, Martial Hebert, and J. Andrew Bagnell. Improving multi-step prediction of\nlearned time series models. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial\nIntelligence, pages 3024–3030. AAAI Press, 2015.\n[25] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\nimage caption generator.\nIn Proc. of IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2015.\n[26] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Machine Learning, 8(3):229–256, 1992.\n10\nSupplementary Material: Cold-Start Reinforcement Learning with Softmax\nPolicy Gradient\n7\nBang-bang Rewarded SPG: Lemma 1\nWe provide here the proof for Lemma 1, as part of the derivation for the gradient computation method\nfor the Bang-bang rewarded SPG method.\nLemma 1 When wi\nt = 0,\nX\nzi\n˜qθ(zi|xi, yi, wi) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1) = 0.\nProof First of all,\n˜qθ(zi\nt|xi, yi, zi\n1:t−1, wi\nt) ∝exp\n\u0000log pθ(zi\nt|zi\n1:t−1, xi) + ∆ri\nt\n\u0001\n,\n(15)\nwhere ∆ri\nt = wi\nt · (R(zi\n1:t|yi) −R(zi\n1:t−1|yi)). When wi\nt = 0, ∆ri\nt = 0, therefore,\n˜qθ(zi\nt|xi, yi, zi\n1:t−1, wi\nt) ∝exp\n\u0000log pθ(zi\nt|zi\n1:t−1, xi)\n\u0001\n= pθ(zi\nt|zi\n1:t−1, xi).\nTherefore, the gradient component at time t of example i is:\nX\nzi\n˜qθ(zi|xi, yi, wi) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1)\n=\nX\nzi\n1:t\n˜qθ(zi\n1:t|xi, yi, wi) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1)\n=\nX\nzi\n1:t−1\n˜qθ(zi\n1:t−1|xi, yi, wi)\nX\nzi\nt\n˜qθ(zi\nt|xi, yi, zi\n1:t−1, wi\nt) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1)\n=\nX\nzi\n1:t−1\n˜qθ(zi\n1:t−1|xi, yi, wi)\nX\nzi\nt\npθ(zi\nt|xi, zi\n1:t−1) ∂\n∂θ log pθ(zi\nt|xi, zi\n1:t−1)\n=\nX\nzi\n1:t−1\n˜qθ(zi\n1:t−1|xi, yi, wi)\nX\nzi\nt\n∂\n∂θpθ(zi\nt|xi, zi\n1:t−1)\n=\nX\nzi\n1:t−1\n˜qθ(zi\n1:t−1|xi, yi, wi) ∂\n∂θ\nX\nzi\nt\npθ(zi\nt|xi, zi\n1:t−1)\n|\n{z\n}\n= ∂\n∂θ 1=0\n= 0.\n8\nAlgorithm 1: Gradient for the Bang-bang Rewarded Softmax Value\nFunction\nThe gradient computation for the Bang-bang Rewarded Softmax Value Function is formulated in\nAlgorithm 1.\nThe reward functions used by the algorithm above are the ones discussed in Section 4 of the main\npaper. We extend that discussion in the section below.\n9\nReward Functions for the SPG Method\n9.1\nMain Reward Function\nIn our experiments, the main reward metric is an average over ROUGE-1, ROUGE-2, and ROUGE-3\nF1 scores. We choose ROUGE-n [13] based on its good performance as an evaluation metric for\n11\nAlgorithm 1: GRADIENT FOR THE BANG-BANG REWARDED SOFTMAX VALUE FUNCTION\nInput: Data point (xi, yi), hyperparameter pdrop, W, J, model parameter θ.\nResult: Gradient of data point (xi, yi):\n∂\n∂θ ˜Li\nBBSP G(θ).\n∂\n∂θ ˜Li\nBBSP G(θ) = 0\nfor j ∈1, . . . , J do\nzij ←∅\nfor t ∈1, . . . , T do\nSample µij\nt ∼U[0, 1]\nif µij\nt > pdrop then\n∆rij\nt = W\n\u0010\nR(zij\n1:t|yi) −R(zij\n1:t−1|yi) + DUPij\nt + EOSij\nt\n\u0011\nSample zij\nt ∼exp\n\u0010\nlog pθ(zij\nt |zij\n1:t−1, xi) + ∆rij\nt\n\u0011\n/Z\n∂\n∂θ ˜Li\nBBSP G(θ) =\n∂\n∂θ ˜Li\nBBSP G(θ) −∂\n∂θ log pθ(zij\nt |xi, zij\n1:t−1)\nelse\nSample zij\nt ∼pθ(zij\nt |zij\n1:t−1, xi)\nend\nzij ←zij ∪\nn\nzij\nt\no\n.\nend\nend\nboth summarization and image-captioning, as well as because it is more computationally efﬁcient\ncompared to other scores such as CIDEr [23] or SPICE [1].\nThe reason we average up to n = 3 (instead of just 2) is illustrated in the following target example:\na man is standing on a street </S>\n(16)\nIn the above sentence, the word ’a’ appears twice. When using a ROUGE average up to n = 2 as\nthe reward metric, for zt−1 = ’a’, both words ’man’ and ’street’ have identical reward increments.\nTherefore, this reward metric cannot distinguish between them. More generally, if the metric used\ndoes not account for n-grams longer than 2, it is suboptimal for decisions following common words\n(like ’the’, ’of’, or ’a’).\n9.2\nROUGE-L as a Reward Function\nThe ROUGE-L metric [13] also cannot be applied as the main reward metric by itself. Using\nExample (16) above, when z1:t−1 = ’a’, all the remaining target words have identical reward\nincrements under ROUGE-L, because the length of the longest-common-subsequences is the same for\nall (i.e., 2). Furthermore, if z1:t−1 = ’a street’, all words (inside or outside the target) except ’</S>’\nhave a 0 reward increment value because it would not improve the length of the longest-common-\nsubsequence. Although not attempted in this paper, one may combine the ROUGE-L metric with\nother metrics, such as the one in Section 9.1 above. A similar proposal, albeit in a more traditional PG\nsetting, has been made in [15], taking advantage of the additional signal provided by various metrics.\n9.3\nEOS Reward Function\nIn the main paper, we introduce an EOS reward function which negatively rewards the end-of-sentence\nsymbol when the length of the output sequence is less than the length of the ground-truth target\nsequence |yi|:\nEOSi\nt =\n\u001a−1\nif zi\nt = </S> and t < |yi|,\n0\notherwise.\nWe illustrate the reason for this reward function using Example (16) again. If z1:t−1 = ’a street’, then\nthe word with the most reward increment is ’</S>’. However, target sequence z = ’a street </S>’ is\ntoo short and misses a lot information, since there are ﬁve remaining words in the ground-truth target\n12\nthat have not been exploited. The EOS function encourages the generation of longer sequences, by\ncorrecting the bias introduced by the greediness of the forward-pass sampling step.\n9.4\nBefore/After Examples when using the DUP Reward Function\nThe DUP function penalizes consecutive tokens in the generated sequence, which helps alleviating\n\"stuttering\" in the model output. The use of the DUP function helps improving the ROUGE-L score\nfor about 0.1 points on the Gigaword dataset. Although without a signiﬁcant boost on the ROUGE-L\nscore, we notice clear differences before and after applying the DUP function, as the examples in\nTable 3 help illustrating.\nBefore\nAfter\nReference\nbosnian pm’s resignation provokes\nbosnian pm’s resignation\nprime minister’s resignation throws\npolitical political political crisis\nprovokes political turmoil\nbosnia into crisis with yugoslavia\nsandelin sandelin sandelin wins\nsandelin wins spanish open\nsandelin wins spanish open eds: adds\nspanish open\nquotes from sandelin and spence\ncredit markets subdued amid stress\ncredit markets subdued amid\ndifﬁcult credit markets show\nstress crisis\nstress fears\nstrained banking system\nspanish ’belle rafael rafael azcona\nspanish ’belle rafael azcona\nspanish ’belle epoque’ scriptwriter\ndies at 81\ndies at 81\nrafael azcona dies aged 81\nnigerian productivity award\nnigerian productivity award\nproductivity award can be revoked,\nlicence licence\nlicence can be withdrawn\nsays nigerian ofﬁcial\nsports column : the big big big\nsports column : the big league\nin the big 12, basketball does the\nbig big big big ap photo <UNK>\nis a big place\nmuscle ﬂexing\nTable 3: Examples of the impact of the DUP function on model output.\n13\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-09-27",
  "updated": "2017-10-13"
}