{
  "id": "http://arxiv.org/abs/1710.00211v1",
  "title": "The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems",
  "authors": [
    "Weinan E",
    "Bing Yu"
  ],
  "abstract": "We propose a deep learning based method, the Deep Ritz Method, for\nnumerically solving variational problems, particularly the ones that arise from\npartial differential equations. The Deep Ritz method is naturally nonlinear,\nnaturally adaptive and has the potential to work in rather high dimensions. The\nframework is quite simple and fits well with the stochastic gradient descent\nmethod used in deep learning. We illustrate the method on several problems\nincluding some eigenvalue problems.",
  "text": "The Deep Ritz method: A deep learning-based\nnumerical algorithm for solving variational problems\nWeinan E1 and Bing Yu2\n1The Beijing Institute of Big Data Research, Department of Mathematics\nand PACM, Princeton University, School of Mathematical Sciences and\nBICMR, Peking University\n2School of Mathematical Sciences, Peking University\nOctober 3, 2017\nAbstract We propose a deep learning based method, the Deep Ritz Method, for numeri-\ncally solving variational problems, particularly the ones that arise from partial diﬀerential\nequations. The Deep Ritz method is naturally nonlinear, naturally adaptive and has the\npotential to work in rather high dimensions. The framework is quite simple and ﬁts well\nwith the stochastic gradient descent method used in deep learning.\nWe illustrate the\nmethod on several problems including some eigenvalue problems.\nKeywords Deep Ritz Method · Variational problems · PDE · Eigenvalue problems\nMathematical Subject Classiﬁcation 35Q68\n1\nIntroduction\nDeep learning has had great success in computer vision and other artiﬁcial intelligence\ntasks [1]. Underlying this success is a new way to approximate functions, from an additive\nconstruction commonly used in approximation theory to a compositional construction used\nin deep neural networks. The compositional construction seems to be particularly powerful\nin high dimensions. This suggests that deep neural network based models can be of use in\nother contexts that involve constructing functions. This includes solving partial diﬀerential\nequations, molecular modeling, model reduction, etc. These aspects have been explored\nrecently in [2, 3, 4, 5, 6, 7].\n1\narXiv:1710.00211v1  [cs.LG]  30 Sep 2017\nIn this paper, we continue this line of work and propose a new algorithm for solving\nvariational problems. We call this new algorithm the Deep Ritz method since it is based on\nusing the neural network representation of functions in the context of the Ritz method. The\nDeep Ritz method has a number of interesting and promising features, which we explore\nlater in the paper.\n2\nThe Deep Ritz Method\nAn explicit example of the kind of variational problems we are interested in is [8]\nmin\nu∈H I(u)\n(1)\nwhere\nI(u) =\nZ\nΩ\n\u00121\n2|∇u(x)|2 −f(x)u(x)\n\u0013\ndx\n(2)\nand H is the set of admissible functions (also called trial function, here represented by\nu), f is a given function, representing external forcing to the system under consideration.\nProblems of this type are fairly common in physical sciences. The Deep Ritz method is\nbased on the following set of ideas:\n1. Deep neural network based approximation of the trial function.\n2. A numerical quadrature rule for the functional.\n3. An algorithm for solving the ﬁnal optimization problem.\n2.1\nBuilding trial functions\nThe basic component of the Deep Ritz method is a nonlinear transformation x →\nzθ(x) ∈Rm deﬁned by a deep neural network. Here θ denotes the parameters, typically the\nweights in the neural network, that help to deﬁne this transformation. In the architecture\nthat we use, each layer of the network is constructed by stacking several blocks, each block\nconsists of two linear transformations, two activation functions and a residual connection,\nboth the input s and the output t of the block are vectors in Rm. The i-th block can be\nexpressed as:\nt = fi(s) = φ(Wi,2 · φ(Wi,1s + bi,1) + bi,2) + s\n(3)\nwhere Wi,1, Wi,2 ∈Rm×m, bi,1, bi,2 ∈Rm are parameters associated with the block. φ is the\n(scalar) activation function [1].\nOur experience has suggested that the smoothness of the activation function φ plays\na key role in the accuracy of the algorithm. To balance simplicity and accuracy, we have\ndecided to use\nφ(x) = max{x3, 0}\n(4)\n2\nThe last term in (3), the residual connection, makes the network much easier to train\nsince it helps to avoid the vanishing gradient problem [9]. The structure of the two blocks,\nincluding two residual connections, is shown in Figure 1.\ninput x\nFC layer (size m)\n+ activation\nresidual\nconnection\noutput u\nFC layer (size m)\n+ activation\nFC layer (size m)\n+ activation\nFC layer (size m)\n+ activation\nresidual\nconnection\nFC layer (size 1)\nFigure 1: The ﬁgure shows a network with two blocks and an output linear layer. Each\nblock consists of two fully-connected layers and a skip connection.\nThe full n-layer network can now be expressed as:\nzθ(x) = fn ◦... ◦f1(x)\n(5)\nθ denotes the set of all the parameters in the whole network. Note the input x for the ﬁrst\nblock is in Rd, not Rm. To handle this discrepancy we can either pad x by a zero vector\nwhen d < m, or apply a linear transformation on x when d > m. Having zθ, we obtain u\nby\nu(x; θ) = a · zθ(x) + b\n(6)\nHere in the left-hand side and in what follows, we will use θ to denote the full parameter\nset {θ, a, b}. Substituting this into the form of I, we obtain a function of θ, which we\nshould minimize.\nFor the functional that occurs in (2), denote:\ng(x; θ) = 1\n2|∇xu(x; θ)|2 −f(x)u(x; θ)\n(7)\n3\nthen we are left with the optimization problem:\nmin\nθ\nL(θ),\nL(θ) =\nZ\nΩ\ng(x; θ)dx\n(8)\n2.2\nThe stochastic gradient descent algorithm and the quadra-\nture rule\nTo ﬁnish describing the algorithm, we need to furnish the remaining two components:\nthe optimization algorithm and the discretization of the integral in I in (2) or L in (8).\nThe latter is necessary since computing the integral in I (or L) explicitly for functions of\nthe form (6) is quite an impossible task.\nIn machine learning, the optimization problem that one encounters often takes the\nform:\nmin\nx∈Rd\nL(θ) := 1\nN\nN\nX\ni=1\nLi(θ),\n(9)\nwhere each term at the right-hand side corresponds to one data point. n, the number\nof data points, is typically very large. For this problem, the algorithm of choice is the\nstochastic gradient descent (SGD) method, which can be described as follows:\nθk+1 = θk −η∇fγk(θk).\n(10)\nHere {γk} are i.i.d random variables uniformly distributed over {1, 2, · · · , n}. This is the\nstochastic version of the gradient descent algorithm (GD). The key idea is that instead\nof computing the sum when evaluating the gradient of L, we simply randomly choose\none term in the sum. Compared with GD, SGD requires only one function evaluation of n\nfunction evaluations at each iteration. In practice, instead of picking one term, one chooses\na ”mini-batch” of terms at each step.\nAt a ﬁrst sight, our problem seems diﬀerent from the ones that occur in machine learning\nsince there are no data involved. The connection becomes clear once we view the integral\nin I as a continuous sum, each point in Ωthen becomes a data point. Therefore, at each\nstep of the SGD iteration, one chooses a mini-batch of points to discretize the integral.\nThese points are chosen randomly and the same quadrature weight is used at every point.\nNote that if we use standard quadrature rules to discretize the integral, then we are\nbound to choose a ﬁxed set of nodes. In this case, we run into the risk where the integrand\nis minimized on these ﬁxed nodes but the functional itself is far from being minimized. It\nis nice that SGD ﬁts naturally with the needed numerical integration in this context.\nIn summary, the SGD in this context is given by:\nθk+1 = θk −η∇θ\n1\nN\nN\nX\nj=1\ng(xj,k; θk)\n(11)\n4\nwhere for each k, {xj,k} is a set of points in Ωthat are randomly sampled with uniform\ndistribution. To accelerate the training of the neural network, we use the Adam optimizer\nversion of the SGD [10].\n3\nNumerical Results\n3.1\nThe Poisson equation in two dimension\nConsider the Poisson equation:\n−∆u(x) = 1,\nx ∈Ω\nu(x) = 0,\nx ∈∂Ω\n(12)\nwhere Ω= (−1, 1)×(−1, 1)\\[0, 1)×{0}. The solution to this problem suﬀers from the well-\nknown ”corner singularity” caused by the nature of the domain [11]. A simple asymptotic\nanalysis shows that at the origin, the solution behaves as u(x) = u(r, θ) ∼r\n1\n2 sin θ\n2 [11].\nModels of this type have been extensively used to help developing and testing adaptive\nﬁnite element methods.\nThe network we used to solve this problem is a stack of four blocks (eight fully-connected\nlayers) and an output layer with m = 10. There are a total of 811 parameters in the model.\nAs far as we can tell, this network structure is not special in any way. It is simply the one\nthat we used.\nThe boundary condition causes some problems. Here for simplicity, we use a penalty\nmethod and consider the modiﬁed functional\nI(u) =\nZ\nΩ\n\u00121\n2|∇xu(x)|2 −f(x)u(x)\n\u0013\ndx + β\nZ\n∂Ω\nu(x)2ds\n(13)\nWe choose β = 500. The results from the Deep Ritz method is shown in see Figure 2(a). For\ncomparison, we also plot the result of the ﬁnite diﬀerence method with ∆x1 = ∆x2 = 0.1\n(1, 681 degrees of freedom), see Figure 2(b).\nTo analyze the error more quantitatively, we consider the following problem\n∆u(x) = 0,\nx ∈Ω\nu(x) = u(r, θ) = r\n1\n2 sin θ\n2,\nx ∈∂Ω\n(14)\nwhere Ω= (−1, 1) × (−1, 1)\\[0, 1) × {0}. This problem has an explicit solution u∗(x) =\nr\n1\n2 sin θ\n2 in polar coordinates. The error e = max |u∗(x) −uh(x)|, where u∗and uh are the\nexact and approximate solutions respectively, is shown in Table 1 for both the Deep Ritz\nmethod and the ﬁnite diﬀerence method (on uniform grids). We can see that with fewer\n5\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n-0.5\n-1.0\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n(a) Solution of Deep Ritz method, 811 parameters\n-1.0\n-0.5\n0.0\n0.5\n1.0\n1.0\n0.5\n0.0\n-0.5\n-1.0\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n(b) Solution of ﬁnite diﬀerence method, 1, 681 pa-\nrameters\nFigure 2: Solutions computed by two diﬀerent methods.\nTable 1: Error of Deep Ritz method (DRM) and ﬁnite diﬀerence method (FDM)\nMethod\nBlocks Num\nParameters\nrelative L2 error\nDRM\n3\n591\n0.0079\n4\n811\n0.0072\n5\n1031\n0.00647\n6\n1251\n0.0057\nFDM\n625\n0.0125\n2401\n0.0063\nparameters, the Deep Ritz method gives more accurate solution than the ﬁnite diﬀerence\nmethod.\nBeing a naturally nonlinear variational method, the Deep Ritz method is also naturally\nadaptive. We believe that this contributes to the better accuracy of the Deep Ritz method.\n3.2\nPoisson equation in high dimension\nExperiences in computer vision and other artiﬁcial intelligence tasks suggest that deep\nlearning-based methods are particularly powerful in high dimensions. This has been con-\nﬁrmed by the results of the Deep BSDE method [3]. In this subsection, we investigate the\nperformance of the Deep Ritz method in relatively high dimension.\n6\nConsider (d = 10)\n−∆u = 0,\nx ∈(0, 1)10\nu(x) =\n5\nX\nk=1\nx2k−1x2k,\nx ∈∂(0, 1)10 .\n(15)\nThe solution of this problem is simply u(x) = P5\nk=1 x2k−1x2k, and we will use the exact\nsolution to compute the error of our model later.\nFor the network structure, we stack six fully-connected layers with three skip connec-\ntions and a ﬁnal linear layer, and there are a total of 671 parameters.\nFor numerical\nintegration, at each step of the SGD iteration, we sample 1,000 points in Ωand 100 points\nat each hyperplane that composes ∂Ω. We set β = 103. After 50,000 iterations, the relative\nL2 error was reduced to about 0.4%. The training process is shown in Figure 3(a).\n0\n10000\n20000\n30000\n40000\n50000\n−5\n−4\n−3\n−2\n−1\n0\n(a) ln e and ln lossboundary, d=10\n0\n2500\n5000\n7500\n10000 12500 15000 17500 20000\n−3.5\n−3.0\n−2.5\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n(b) ln e and ln lossboundary, d=100\nFigure 3: The total error and error at the boundary during the training process. The\nx-axis represents the iteration steps. The blue curves show the relative error of u. The red\ncurves show the relative error on the boundary.\nAlso shown in Figure 3(b) is the training process for the problem:\n−∆u = −200\nx ∈(0, 1)d\nu(x) =\nX\nk\nx2\nk\nx ∈∂(0, 1)d\n(16)\nwith d = 100 with a similar network structure (stack 3 blocks of size m=100). The solution\nof this problem is u(x) = P\nk x2\nk. After 50000 iterations, the relative error is reduced to\nabout 2.2%.\n7\n3.3\nAn example with the Neumann boundary condition\nConsider:\n−∆u + π2u = 2π2 X\nk\ncos(πxk)\nx ∈[0, 1]d\n∂u\n∂n|∂[0,1]d = 0\nx ∈∂[0, 1]d\n(17)\nThe exact solution is u(x) = P\nk cos(πxk)\n0\n10000\n20000\n30000\n40000\n50000\n−4\n−3\n−2\n−1\n0\n(a) ln e, d=5\n0\n10000\n20000\n30000\n40000\n50000\n−4\n−3\n−2\n−1\n0\n(b) ln e, d=10\nFigure 4: The error during the training process (d = 5 and d = 10).\nIn this case, we can simply use\nI(u) =\nZ\nΩ\n\u00121\n2\n\u0000|∇u(x)|2 + π2u(x)2\u0001\n−f(x)u(x)\n\u0013\ndx\nwithout any penalty function for the boundary.\nWith a similar network structure the relative L2 error reaches 1.3% for d = 5 and 1.9%\nfor d = 10. The training process is shown in Figure 4.\n3.4\nTransfer learning\nAn important component of the training process is the initialization. Here we inves-\ntigate the beneﬁt of transferring weights in the network when the forcing function f is\nchanged.\n8\nConsider the problem:\n−∆u(x) = 6(1 + x1)(1 −x1)x2 + 2(1 + x2)(1 −x2)x2\nx ∈Ω\nu(x) = r\n1\n2 sin θ\n2 + (1 + x1)(1 −x1)(1 + x2)(1 −x2)x2\nx ∈∂Ω\n(18)\nwhere Ω= (−1, 1) × (−1, 1)\\[0, 1) × {0}. Here we used a mixture of rectangular and polar\ncoordinates. The exact solution is\nu(x) = r\n1\n2 sin θ\n2 + (1 + x1)(1 −x1)(1 + x2)(1 −x2)x2\n.\nThe network consists of a stack of 3 blocks with m=10, that is, six fully-connected\nlayers and three residual connections and a ﬁnal linear transformation layer to obtain u.\nWe show how the error and the weights in the layers change during the training period in\nFigure 5.\nWe also transfer the weights from the problem:\n−∆u(x) = 0,\nx ∈Ω\nu(x) = r\n1\n2 sin θ\n2\nx ∈∂Ω\n(19)\nwhere Ω= (−1, 1) × (−1, 1)\\[0, 1) × {0}.\nThe error and the weights during the training period are also shown in Figure 5. We\nsee that transferring weights speeds up the training process considerably during the initial\nstage of the training. This suggests that transferring weights is a particularly eﬀective\nprocedure if the accuracy requirement is not very strigent.\n3.5\nEigenvalue problems\nConsider the following problem:\n−∆u + v · u = λu,\nx ∈Ω\nu|∂Ω= 0\n(20)\nProblems of this kind occur often in quantum mechanics where v is the potential function.\nThere is a well-known variational principle for the smallest eigenvalue:\nmin\nR\nΩ|∇u|2dx +\nR\nΩvu2dx\nR\nΩu2dx\ns.t.\nu|∂Ω= 0\n(21)\nThe functional we minimize here is called the Rayleigh quotient.\n9\n0\n10000\n20000\n30000\n40000\n50000\n−4\n−3\n−2\n−1\n0\n1\n(a) ln err\n0\n10000\n20000\n30000\n40000\n50000\n−10\n−8\n−6\n−4\n−2\n0\n2\n(b) ln ||∆W||2\n2\nFigure 5: The red curves show the results of the training process with weight transfer.\nThe blue curves show the results of the training process with random initialization. The\nleft ﬁgure shows how the natural logarithm of the error changes during training. The right\nﬁgure shows how the natural logarithm of ||∆W||2\n2 changes during training, where ∆W is\nthe change in W after 100 training steps, W is the weight matrix.\nTo avoid getting the trivial optimizer u = 0, instead of using the functional\nL0(x) =\nR\nΩ|∇u|2dx +\nR\nΩvu2dx\nR\nΩu2dx\n+ β\nZ\n∂Ω\nu(x)2dx\nwe use\nmin\nR\nΩ|∇u|2dx +\nR\nΩvu2dx\nR\nΩu2dx\ns.t.\nZ\nΩ\n|∇u|2dx = 1\nu|∂Ω= 0\n(22)\nIn practice, we use\nL(u(x; θ)) =\nR\nΩ|∇u|2dx +\nR\nΩvu2dx\nR\nΩu2dx\n+ β\nZ\n∂Ω\nu(x)2dx + γ\n\u0012Z\nΩ\nu2dx −1\n\u00132\n(23)\nOne might suggest that with the last penalty term, the denominator in the Rayleigh\nquotient is no longer necessary. It turns out that we found in practice that this term still\nhelps in two ways: (1) In the presence of this denominator, there is no need to choose a\n10\nlarge value of γ. For the harmonic oscillator in d = 5, we choose β = 2000, γ to be 100\nand this seems to be large enough. (2) This term helps to speed up the training process.\nTo solve this problem, we build a deep neural network much like the Densenet [12].\nThere are skip connections between every pairwise layers, which help gradients ﬂow through\nthe whole network. The network structure is shown in Figure 6.\nyi = φ(Wi−1xi−1 + bi−1)\n(24)\nxi = [xi−1; yi]\n(25)\nWe use an activation function φ(x) = max(0, x)2. If we use the same activation function as\nbefore, we found that the gradients can become quite large and we may face the gradient\nexplosion problem.\ninput x\nFC layer (size m)\n+ activation\noutput u\nFC layer (size m)\n+ activation\nFC layer (size 1)\nFigure 6: Network structure used for the eigenvalue problem. There are skip connections\nbetween every pairwise layers. The triangles denote concatenation operations.\nThe remaining components of the algorithm are very much the same as before.\nExample 1: Inﬁnite potential well\nConsider the potential function\nv(x) =\n(\n0,\nx ∈[0, 1]d\n∞,\nx /∈[0, 1]d\n(26)\n11\nThe problem is then equivalent to solving:\n−∆u = Eu,\nx ∈[0, 1]d\nu(x) = 0,\nx ∈∂[0, 1]d\n(27)\nThe smallest eigenvalue is λ0 = dπ2.\nThe results of the Deep Ritz method in diﬀerent dimensions are shown in Table 2.\nTable 2: Error of deep Ritz method\nDimension d\nExact λ0\nApproximate\nError\n1\n9.87\n9.85\n0.20%\n5\n49.35\n49.29\n0.11%\n10\n98.70\n92.35\n6.43%\nExample 2: The harmonic oscillator\nThe potential function in Rd is v(x) = |x|2. For simplicity, we truncate the compu-\ntational domain from Rd to [−3, 3]d. Obviously, there are better strategies, but we leave\nimprovements to later work.\nThe results in diﬀerent dimensions are shown in Table 3.\nTable 3: Error of deep Ritz method\nDimension d\nExact λ0\nApproximate\nError\n1\n1\n1.0016\n0.16%\n5\n5\n5.0814\n1.6%\n10\n10\n11.26\n12.6%\nThe results deteriorate substantially as the dimension is increased. We believe that\nthere is still a lot of room for improving the results. We will leave this to future work.\n4\nDiscussion\nWe proposed a variational method based on representing the trial functions by deep\nneural networks. Our limited experience with this method suggests that it has the following\nadvantages:\n1. It is naturally adaptive.\n12\n2. It is less sensitive to the dimensionality of the problem and has the potential to work\nin rather high dimensions.\n3. The method is reasonably simple and ﬁts well with the stochastic gradient descent\nframework commonly used in deep learning.\nWe also see a number of disadvantages that need to be addressed in future work:\n1. The variational problem that we obtain at the end is not convex even when the initial\nproblem is. The issue of local minima and saddle points is non-trivial.\n2. At the present time, there is no consistent conclusion about the convergence rate.\n3. The treatment of the essential boundary condition is not as simple as for the tradi-\ntional methods.\nIn addition, there are still interesting issues regarding the choice of the network struc-\nture, the activation function and the minimization algorithm. The present paper is far\nfrom being the last word on the subject.\nAcknowledgement: We are grateful to Professor Ruo Li and Dr. Zhanxing Zhu for\nvery helpful discussions. The work of E and Yu is supported in part by the National Key\nBasic Research Program of China 2015CB856000, Major Program of NNSFC under grant\n91130005, DOE grant de-sc0009248, and ONR grant N00014-13-1-0338.\nReferences\n[1] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning. MIT Press, 2016.\n[2] W. E, “A proposal for machine learning via dynamical systems”, Communications in\nMathematics and Statistics, March 2017, Volume 5, Issue 1, pp 1-11.\n[3] J. Q. Han, A. Jentzen and W. E, “Overcoming the curse of dimensionality: Solv-\ning high-dimensional partial diﬀerential equations using deep learning”, submitted,\narXiv:1707.02568.\n[4] W. E, J. Q. Han and A. Jentzen, “Deep learning-based numerical methods for high-\ndimensional parabolic partial diﬀerential equations and backward stochastic diﬀeren-\ntial equations”, submitted, arXiv:1706.04702.\n[5] C. Beck, W. E and Arnulf Jentzen, “Machine learning approximation algorithms for\nhigh-dimensional fully nonlinear partial diﬀerential equations and second-order back-\nward stochastic diﬀerential equations”, submitted. arXiv:1709.05963.\n13\n[6] J. Q. Han, L. Zhang, R. Car and W. E, “Deep Potential: A general and “ﬁrst-principle”\nrepresentation of the potential energy”, submitted, arXiv:1707.01478.\n[7] L. Zhang, J.Q. Han, H. Wang, R. Car and W.E, “Deep Potential Molecular Dy-\nnamics: A scalable model with the accuracy of quantum mechanics”, submitted,\narXiv:1707.09571.\n[8] L. C. Evans, Partial Diﬀerential Equations, 2nd ed. American Mathematical Society,\n2010.\n[9] K. M. He, X. Y. Zhang, S. Q. Ren, J. Sun, “Deep residual learning for image recogni-\ntion”, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nvol. 00, no. , pp. 770-778, 2016, doi:10.1109/CVPR.2016.90\n[10] D. P. Kingma, and J. Ba. “Adam: A method for stochastic optimization.” arXiv\npreprint arXiv:1412.6980, 2014.\n[11] G. Strang and G. Fix, An Analysis of the Finite Element Method. Prentice-Hall, 1973.\n[12] G. Huang, Z. Liu, K. Q. Weinberger, V. D. M. Laurens, “Densely connected convolu-\ntional networks.”, arXiv preprint arXiv:1608.06993, 2016.\n14\n",
  "categories": [
    "cs.LG",
    "stat.ML",
    "35Q68"
  ],
  "published": "2017-09-30",
  "updated": "2017-09-30"
}