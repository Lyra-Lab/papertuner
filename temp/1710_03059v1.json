{
  "id": "http://arxiv.org/abs/1710.03059v1",
  "title": "Learning Graph Representations with Embedding Propagation",
  "authors": [
    "Alberto Garcia-Duran",
    "Mathias Niepert"
  ],
  "abstract": "We propose Embedding Propagation (EP), an unsupervised learning framework for\ngraph-structured data. EP learns vector representations of graphs by passing\ntwo types of messages between neighboring nodes. Forward messages consist of\nlabel representations such as representations of words and other attributes\nassociated with the nodes. Backward messages consist of gradients that result\nfrom aggregating the label representations and applying a reconstruction loss.\nNode representations are finally computed from the representation of their\nlabels. With significantly fewer parameters and hyperparameters an instance of\nEP is competitive with and often outperforms state of the art unsupervised and\nsemi-supervised learning methods on a range of benchmark data sets.",
  "text": "Learning Graph Representations with Embedding\nPropagation\nAlberto García-Durán\nNEC Labs Europe\nHeidelberg, Germany\nalberto.duran@neclab.eu\nMathias Niepert\nNEC Labs Europe\nHeidelberg, Germany\nmathias.niepert@neclab.eu\nAbstract\nWe propose Embedding Propagation (EP), an unsupervised learning framework for\ngraph-structured data. EP learns vector representations of graphs by passing two\ntypes of messages between neighboring nodes. Forward messages consist of label\nrepresentations such as representations of words and other attributes associated with\nthe nodes. Backward messages consist of gradients that result from aggregating the\nlabel representations and applying a reconstruction loss. Node representations are\nﬁnally computed from the representation of their labels. With signiﬁcantly fewer\nparameters and hyperparameters an instance of EP is competitive with and often\noutperforms state of the art unsupervised and semi-supervised learning methods on\na range of benchmark data sets.\n1\nIntroduction\nGraph-structured data occurs in numerous application domains such as social networks, bioinfor-\nmatics, natural language processing, and relational knowledge bases. The computational problems\ncommonly addressed in these domains are network classiﬁcation [40], statistical relational learn-\ning [12, 36], link prediction [22, 24], and anomaly detection [8, 1], to name but a few. In addition,\ngraph-based methods for unsupervised and semi-supervised learning are often applied to data sets\nwith few labeled examples. For instance, spectral decompositions [25] and locally linear embeddings\n(LLE) [38] are always computed for a data set’s afﬁnity graph, that is, a graph that is ﬁrst constructed\nusing domain knowledge or some measure of similarity between data points. Novel approaches to\nunsupervised representation learning for graph-structured data, therefore, are important contributions\nand are directly applicable to a wide range of problems.\nEP learns vector representations (embeddings) of graphs by passing messages between neighboring\nnodes. This is reminiscent of power iteration algorithms which are used for such problems as com-\nputing the PageRank for the web graph [33], running label propagation algorithms [47], performing\nisomorphism testing [16], and spectral clustering [25]. Whenever a computational process can be\nmapped to message exchanges between nodes, it is implementable in graph processing frameworks\nsuch as Pregel [29], GraphLab [23], and GraphX [44].\nGraph labels represent vertex attributes such as bag of words, movie genres, categorical features,\nand continuous features. They are not to be confused with class labels of a supervised classiﬁcation\nproblem. In the EP learning framework, each vertex v sends and receives two types of messages.\nLabel representations are sent from v’s neighboring nodes to v and are combined so as to reconstruct\nthe representations of v’s labels. The gradients resulting from the application of some reconstruction\nloss are sent back as messages to the neighboring vertices so as to update their labels’ representations\nand the representations of v’s labels. This process is repeated for a certain number of iterations or\nuntil a convergence threshold is reached. Finally, the label representations of v are used to compute a\nrepresentation of v itself.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1710.03059v1  [cs.LG]  9 Oct 2017\nDespite its conceptual simplicity, we show that EP generalizes several existing machine learning\nmethods for graph-structured data. Since EP learns embeddings by incorporating different label types\n(representing, for instance, text and images) it is a framework for learning with multi-modal data [31].\n2\nPrevious Work\nThere are numerous methods for embedding learning such as multidimensional scaling (MDS) [20],\nLaplacian Eigenmap [3], Siamese networks [7], IsoMap [43], and LLE [38]. Most of these approaches\nconstruct an afﬁnity graph on the data points ﬁrst and then embed the graph into a low dimensional\nspace. The corresponding optimization problems often have to be solved in closed form (for instance,\ndue to constraints on the objective that remove degenerate solutions) which is intractable for large\ngraphs. We discuss the relation to LLE [38] in more detail when we analyze our framework.\nGraph neural networks (GNN) [39] is a general class of recursive neural networks for graphs where\neach node is associated with one label. Learning is performed with the Almeida-Pineda algorithm\n[2, 35]. The computation of the node embeddings is performed by backpropagating gradients for a\nsupervised loss after running a recursive propagation model to convergence. In the EP framework\ngradients are computed and backpropagated immediately for each node. Gated graph sequence\nneural networks (GG-SNN) [21] modify GNN to use gated recurrent units and modern optimization\ntechniques. Recent work on graph convolutional networks (GCNs) uses a supervised loss to inject\nclass label information into the learned representations [18]. GCNs as well as GNNs and GG-SNNs,\ncan be seen as instances of the Message Passing Neural Network (MPNN) framework, recently\nintroduced in [13]. There are several signiﬁcant differences between the EP and MPNN framework:\n(i) all instances of MPNN use a supervised loss but EP is unsupervised and, therefore, classiﬁer\nagnostic; (ii) EP learns label embeddings for each of the different label types independently and\ncombines them into a joint node representation whereas all existing instances of MPNN do not provide\nan explicit method for combining heterogeneous feature types. Moreover, EP’s learning principle\nbased on reconstructing each node’s representation from neighboring nodes’ representations is highly\nsuitable for the inductive setting where nodes are missing during training.\nMost closely related to our work is DEEPWALK [34] which applies a word embedding algorithm\nto random walks. The idea is that random walks (node sequences) are treated as sentences (word\nsequences). A SKIPGRAM [30] model is then used to learn node embeddings from the random\nwalks. NODE2VEC [15] is identical to DEEPWALK with the exception that it explores new methods\nto generate random walks (the input sentences to WORD2VEC), at the cost of introducing more\nhyperparamenters. LINE [41] optimizes similarities between pairs of node embeddings so as to\npreserve their ﬁrst and second-order proximity. The main advantage of EP over these approaches is its\nability to incorporate graph attributes such as text and continuous features. PLANETOID [45] combines\na learning objective similar to that of DEEPWALK with supervised objectives. It also incorporates\nbag of words associated with nodes into these supervised objectives. We show experimentally that\nfor graph without attributes, all of the above methods learn embeddings of similar quality and that EP\noutperforms all other methods signiﬁcantly on graphs with word labels. We can also show that EP\ngeneralizes methods that learn embeddings for multi-relational graphs such as TRANSE [5].\n3\nEmbedding Propagation\n{a95}\n{bio, health, gene}\n{a237}\n{learning, rna}\n{a651}\n{margin, SVM, loss}\n{a23}\n{health, symptom}\n{a214}\n{bio, chemical, dna, rna}\nv\nFigure 1: A fragment of a citation network.\nA graph G = (V, E) consists of a set of vertices\nV and a set of edges E ⊆{(v, w) | v, w ∈V }.\nThe approach works with directed and undi-\nrected edges as well as with multiple edge types.\nN(v) is the set of neighbors of v if G is undi-\nrected and the set of in-neighbors if G is directed.\nThe graph G is associated with a set of k label\nclasses L = {L1, ..., Lk} where each Li is a\nset of labels corresponding to label type i. A\nlabel is an identiﬁer of some object and not to\nbe confused with a class label in classiﬁcation\nproblems. Labels allow us to represent a wide range of objects associated with the vertices such\nas words, movie genres, and continuous features. To illustrate the concept of label types, Figure 1\n2\nd1(               ,                )\na95\nbio\nhealth\ngene\na237\nhealth\nrna\ng1\ng2\nd2(               ,                )\ngradients\ngradients\nv's current\nlabel embeddings\nv's current\nlabel embeddings\na95\nbio\nhealth\ngene\na237\nhealth\nrna\nupdated\nembeddings\ng̃2\ng̃1\nupdated\nembeddings\na23\nhealth\nsymptom\nv\nh1(v)\nh1(v)\nh2(v)\nh2(v)\nembeddings\nembeddings\ncurrent\nembeddings\ncurrent\nembeddings\nv\nFigure 2: Illustration of the messages passed between a vertex v and its neighbors for the citation\nnetwork of Figure 1. First, the label embeddings are sent from the neighboring vertices to the vertex\nv (black node). These embeddings are fed into differentiable functions egi. Here, there is one function\nfor the article identiﬁer label type (yellow shades) and one for the natural language words label type\n(red shades). The gradients are derived from the distances di between (i) the output of the functions\negi applied to the embeddings sent from v’s neighbors and (ii) the output of the functions gi applied\nto v’s label embeddings. The better the output of the functions egi is able to reconstruct the output of\nthe functions gi, the smaller the value of the distance measure. The gradients are the messages that\nare propagated back to the neighboring nodes so as to update the corresponding embedding vectors.\nThe ﬁgure is best seen in color.\ndepicts a fragment of a citation network. There are two label types. One representing the unique\narticle identiﬁers and the other representing the identiﬁers of natural language words occurring in the\narticles.\nThe functions li : V →2Li map every vertex in the graph to a subset of the labels Li of label type\ni. We write l(v) = S\ni li(v) for the set of all labels associated with vertex v. Moreover, we write\nli(N(v)) = {li(u) | u ∈N(v)} for the multiset of labels of type i associated with the neighbors of\nvertex v.\nWe begin by describing the general learning framework of EP which proceeds in two steps.\n• First, EP learns a vector representation for every label by passing messages along the edges\nof the input graph. We write ℓfor the current vector representation of a label ℓ. For labels\nof label type i, we apply a learnable embedding function ℓ= fi(ℓ) that maps every label\nℓof type i to its embedding ℓ. The embedding functions fi have to be differentiable so\nas to facilitate parameter updates during learning. For each label type one can chose an\nappropriate embedding function such as a linear embedding function for text input or a more\ncomplex convolutional network for image data.\n• Second, EP computes a vector representation for each vertex v from the vector representa-\ntions of v’s labels. We write v for the current vector representation of a vertex v.\nLet v ∈V , let i ∈{1, ..., k} be a label type, and let di ∈N be the size of the embedding for label type\ni. Moreover, let hi(v) = gi ({ℓ| ℓ∈li(v)}) and let ehi(v) = egi ({ℓ| ℓ∈li(N(v))}), where gi and\negi are differentiable functions that map multisets of di-dimensional vectors to a single di-dimensional\nvector. We refer to the vector hi(v) as the embedding of label type i for vertex v and to ehi(v) as\nthe reconstruction of the embedding of label type i for vertex v since it is computed from the label\nembeddings of v’s neighbors. While the gi and egi can be parameterized (typically with a neural\nnetwork), in many cases they are simple parameter free functions that compute, for instance, the\nelement-wise average or maximum of the input.\nThe ﬁrst learning procedure is driven by the following objectives for each label type i ∈{1, ..., k}\nmin Li = min\nX\nv∈V\ndi\n\u0010\nehi(v), hi(v)\n\u0011\n,\n(1)\nwhere di is some measure of distance between hi(v), the current representation of label type i for\nvertex v, and its reconstruction ehi(v). Hence, the objective of the approach is to learn the parameters\n3\na23\nhealth\nsymptom\nr\nlabel\nembeddings\nvertex\nembedding\nv\nFigure 3: For each vertex v, the function r computes a vector representation of the vertex based on\nthe vector representations of v’s labels.\nof the functions gi and egi (if such parameters exist) and the vector representations of the labels such\nthat the output of egi applied to the type i label embeddings of v’s neighbors is close to the output\nof gi applied to the type i label embeddings of v. For each vertex v the messages passed to v from\nits neighbors are the representations of their labels. The messages passed back to v’s neighbors are\nthe gradients which are used to update the label embeddings. The gradients also update v’s label\nembeddings. Figure 2 illustrates the ﬁrst part of the unsupervised learning framework for a part of a\ncitation network. A representation is learned both for the article identiﬁers and the words occurring\nin the articles. The gradients are computed based on a loss between the reconstruction of the label\ntype embeddings and their current values.\nDue to the learning principle of EP, nodes that do not have any labels for label type i can be assigned\na new dummy label unique to the node and the label type. The representations learned for these\ndummy labels can then be used as part of the representation of the node itself. Hence, EP is also\napplicable in situations where data is missing and incomplete.\nThe embedding functions fi can be initialized randomly or with an existing model. For instance,\nembedding functions for words can be initialized using word embedding algorithms [30] and those\nfor images with pretrained CNNs [19, 11]. Initialized parameters are then reﬁned by the application\nof EP. We can show empirically, however, that random initializations of the embedding functions fi\nalso lead to effective vertex embeddings.\nThe second step of the learning framework applies a function r to compute the representations of the\nvertex v from the representations of v’s labels: v = r ({ℓ| ℓ∈l(v)}) . Here, the label embeddings\nand the parameters of the functions gi and egi (if such parameters exist) remain unchanged. Figure 3\nillustrates the second step of EP.\nWe now introduce EP-B, an instance of the EP framework that we have found to be highly effective\nfor several of the typical graph-based learning problems. The instance results from setting gi(H) =\negi(H) =\n1\n|H|\nP\nh∈H h for all label types i and all sets of embedding vectors H. In this case we have,\nfor any vertex v and any label type i,\nhi(v) =\n1\n|li(v)|\nX\nℓ∈li(v)\nℓ,\nehi(v) =\n1\n|li(N(v))|\nX\nu∈N(v)\nX\nℓ∈li(u)\nℓ.\n(2)\nIn conjunction with the above functions gi and egi, we can use the margin-based ranking loss1\nLi =\nX\nv∈V\nX\nu∈V \\{v}\nh\nγ + di\n\u0010\nehi(v), hi(v)\n\u0011\n−di\n\u0010\nehi(v), hi(u)\n\u0011i\n+ ,\n(3)\nwhere di is the Euclidean distance, [x]+ is the positive part of x, and γ > 0 is a margin hyperparameter.\nHence, the objective is to make the distance between ehi(v), the reconstructed embedding of label\ntype i for vertex v, and hi(v), the current embedding of label type i for vertex v, smaller than the\ndistance between ehi(v) and hi(u), the embedding of label type i of a vertex u different from v. We\nsolve the minimization problem with gradient descent algorithms and use one node u for every v in\neach learning iteration. Despite using only ﬁrst-order proximity information in the reconstruction\nof the label embeddings, this learning is effectively propagating embedding information across the\ngraph: an update of a label embedding affects neighboring label embeddings which, in other updates,\naffects their neighboring label embeddings, and so on; hence the name of this learning framework.\n1Directly minimizing Equation (1) could lead to degenerate solutions.\n4\nTable 1: Number of parameters and hyperparam-\neters for a graph without node attributes.\nMethod\n#params\n#hyperparams\nDEEPWALK [34]\n2d|V |\n4\nNODE2VEC [15]\n2d|V |\n6\nLINE [41]\n2d|V |\n2\nPLANETOID [45]\n≫2d|V |\n≥6\nEP-B\nd|V |\n2\nTable 2: Dataset statistics. k is the number of\nlabel types.\nDataset\n|V |\n|E|\n#classes\nk\nBlogCatalog\n10,312\n333,983\n39\n1\nPPI\n3,890\n76,584\n50\n1\nPOS\n4,777\n184,812\n40\n1\nCora\n2,708\n5,429\n7\n2\nCiteseer\n3,327\n4,732\n6\n2\nPubmed\n19,717\n44,338\n3\n2\nFinally, a simple instance of the function r is a function that concatenates all the embeddings hi(v)\nfor i ∈{1, ..., k} to form one single vector representation v for each node v\nv = concat [g1({ℓ| ℓ∈l1(v)}), ..., gk({ℓ| ℓ∈lk(v)})] = concat [h1(v), ..., hk(v)] .\n(4)\nFigure 3 illustrates the working of this particular function r. We refer to the instance of the learning\nframework based on the formulas (2),(3), and (4) as EP-B. The resulting vector representation of\nthe vertices can now be used for downstream learning problems such as vertex classiﬁcation, link\nprediction, and so on.\n4\nFormal Analysis\nWe now analyze the computation and model complexities of the EP framework and its connection to\nexisting models.\n4.1\nComputational and Model Complexity\nLet G = (V, E) be a graph (either directed or undirected) with k label types L = {L1, ..., Lk}.\nMoreover, let labmax = maxv∈V,i∈{1,...,k} |li(v)| be the maximum number of labels for any type\nand any vertex of the input graph, let degmax = maxv∈V |N(v)| be the maximum degree of the input\ngraph, and let τ(n) be the worst-case complexity of computing any of the functions gi and egi on n\ninput vectors of size di. Now, the worst-case complexity of one learning iteration is\nO (k|V |τ(labmaxdegmax)) .\nFor an input graph without attributes, that is, where the only label type represents node identities, the\nworst-case complexity of one learning iteration is O(|V |τ(degmax)). If, in addition, the complexity\nof the single reconstruction function is linear in the number of input vectors, the complexity is\nO(|V |degmax) and, hence, linear in both the number of nodes and the maximum degree of the input\ngraph. This is the case for most aggregation functions and, in particular, for the functions egi and\ngi used in EP-B, the particular instance of the learning framework deﬁned by the formulas (2),(3),\nand (4). Furthermore, the average complexity is linear in the average node degree of the input graph.\nThe worst-case complexity of EP can be limited by not exchanging messages from all neighbors but\nonly a sampled subset of size at most κ. We explore different sampling scenarios in the experimental\nsection.\nIn general, the number of parameters and hyperparameters of the learning framework depends on\nthe parameters of the functions gi and egi, the loss functions, and the number of distinct labels of the\ninput graph. For graphs without attributes, the only parameters of EP-B are the embedding weights\nand the only hyperparameters are the size of the embedding d and the margin γ. Hence, the number\nof parameters is d|V | and the number of hyperparameters is 2. Table 1 lists the parameter counts for\na set of state of the art methods for learning embeddings for graphs without attributes.\n4.2\nComparison to Existing Models\nEP-B is related to locally linear embeddings (LLE) [38]. In LLE there is a single function eg which\ncomputes a linear combination of the vertex embeddings. eg’s weights are learned for each vertex in\na separate previous step. Hence, unlike EP-B, eg does not compute the unweighted average of the\ninput embeddings. Moreover, LLE does not learn embeddings for the labels (attribute values) but\n5\ndirectly for vertices of the input graph. Finally, LLE is only feasible for graphs where each node\nhas at most a small constant number of neighbors. LLE imposes additional constraints to avoid\ndegenerate solutions to the objective and solves the resulting optimization problem in closed form.\nThis is not feasible for large graphs.\nIn several applications, the nodes of the graphs are associated with a set of words. For instance,\nin citation networks, the nodes which represent individual articles can be associated with a bag of\nwords. Every label corresponds to one of the words. Figure 1 illustrates a part of such a citation\nnetwork. In this context, EP-B’s learning of word embeddings is related to the CBOW model [30].\nThe difference is that for EP-B the context of a word is determined by the neighborhood of the\nvertices it is associated with and it is the embedding of the word that is reconstructed and not its\none-hot encoding.\nFor graphs with several different edge types such as multi-relational graphs, the reconstruction\nfunctions egi can be made dependent on the type of the edge. For instance, one could have, for any\nvertex v and label type i,\nehi(v) =\n1\n|li(N(v))|\nX\nu∈N(v)\nX\nℓ∈li(u)\n\u0000ℓ+ r(u,v)\n\u0001\n,\nwhere r(u,v) is the vector representation corresponding to the type of the edge (the relation) from\nvertex u to vertex v, and hi(v) could be the average embedding of v’s node id labels. In combination\nwith the margin-based ranking loss (3), this is related to embedding models for multi-relational\ngraphs [32] such as TRANSE [5].\n5\nExperiments\nThe objectives of the experiments are threefold. First, we compare EP-B to the state of the art on\nnode classiﬁcation problems. Second, we visualize the learned representations. Third, we investigate\nthe impact of an upper bound on the number of neighbors that are sending messages.\nWe evaluate EP with the following six commonly used benchmark data sets. BlogCatalog [46] is a\ngraph representing the social relationships of the bloggers listed on the BlogCatalog website. The\nclass labels represent user interests. PPI [6] is a subgraph of the protein-protein interactions for\nHomo Sapiens. The class labels represent biological states. POS [28] is a co-occurrence network\nof words appearing in the ﬁrst million bytes of the Wikipedia dump. The class labels represent\nthe Part-of-Speech (POS) tags. Cora, Citeseer and Pubmed [40] are citation networks where nodes\nrepresent documents and their corresponding bag-of-words and links represent citations. The class\nlabels represents the main topic of the document. Whereas BlogCatalog, PPI and POS are multi-label\nclassiﬁcation problems, Cora, Citeseer and Pubmed have exactly one class label per node. Some\nstatistics of these data sets are summarized in Table 2.\n5.1\nSet-up\nThe input to the node classiﬁcation problem is a graph (with or without node attributes) where a\nfraction of the nodes is assigned a class label. The output is an assignment of class labels to the test\nnodes. Using the node classiﬁcation data sets, we compare the performance of EP-B to the state\nof the art approaches DEEPWALK [34], LINE [41], NODE2VEC [15], PLANETOID [45], GCN [18],\nand also to the baselines WVRN [27] and MAJORITY. WVRN is a weighted relational classiﬁer that\nestimates the class label of a node with a weigthed mean of its neighbors’ class labels. Since all the\ninput graphs are unweighted, WVRN assigns the class label to a node v that appears most frequently\nin v’s neighborhood. MAJORITY always chooses the most frequent class labels in the training set.\nFor all data sets and all label types the functions fi are always linear embeddings equivalent to an\nembedding lookup table. The dimension of the embeddings is always ﬁxed to 128. We used this\ndimension for all methods which is in line with previous work such as DEEPWALK and NODE2VEC\nfor the data sets under consideration. For EP-B, we chose the margin γ in (3) from the set of values\n[1, 5, 10, 20] on validation data. For all approaches except LINE, we used the hyperparameter values\nreported in previous work since these values were tuned to the data sets. As LINE has not been applied\nto the data sets before, we set its number of samples to 20 million and negative samples to 5. This\nmeans that LINE is trained on (at least) an order of magnitude more examples than all other methods.\n6\nTable 3: Multi-label classiﬁcation results for BlogCatalog, POS and PPI in the transductive setting.\nThe upper and lower part list micro and macro F1 scores, respectively.\nBlogCatalog\nPOS\nPPI\nTr [%]\n10\n50\n90\n10\n50\n90\n10\n50\n90\nγ = 1\nγ = 10\nγ = 5\nEP-B\n35.05 ± 0.41\n39.44 ± 0.29\n40.41 ± 1.59\n46.97 ± 0.36\n49.52 ± 0.48\n50.05 ± 2.23\n17.82 ± 0.77\n23.30 ± 0.37\n24.74 ± 1.30\nDEEPWALK\n34.48 ± 0.40\n38.11 ± 0.43\n38.34 ± 1.82\n45.02 ± 1.09\n49.10 ± 0.52\n49.33 ± 2.39\n17.14 ± 0.89\n23.52 ± 0.65\n25.02 ± 1.38\nNODE2VEC\n35.54 ± 0.49\n39.31 ± 0.25\n40.03 ± 1.22\n44.66 ± 0.92\n48.73 ± 0.59\n49.73 ± 2.35\n17.00 ± 0.81\n23.31 ± 0.62\n24.75 ± 2.02\nLINE\n34.83 ± 0.39\n38.99 ± 0.25\n38.77 ± 1.08\n45.22 ± 0.86\n51.64 ± 0.65\n52.28 ± 1.87\n16.55 ± 1.50\n23.01 ± 0.84\n25.28 ± 1.68\nWVRN\n20.50 ± 0.45\n30.24 ± 0.96\n33.47 ± 1.50\n26.07 ± 4.35\n29.21 ± 2.21\n33.09 ± 2.27\n10.99 ± 0.57\n18.14 ± 0.60\n21.49 ± 1.19\nMAJORITY\n16.51 ± 0.53\n16.88 ± 0.35\n16.53 ± 0.74\n40.40 ± 0.62\n40.47 ± 0.51\n40.10 ± 2.57\n6.15 ± 0.40\n5.94 ± 0.66\n5.66 ± 0.92\nγ = 1\nγ = 10\nγ = 5\nEP-B\n19.08 +- 0.78\n25.11 ± 0.43\n25.97 ± 1.25\n8.85 ± 0.33\n10.45 ± 0.69\n12.17 ± 1.19\n13.80 ± 0.67\n18.96 ± 0.43\n20.36 ± 1.42\nDEEPWALK\n18.16 ± 0.44\n22.65 ± 0.49\n22.86 ± 1.03\n8.20 ± 0.27\n10.84 ± 0.62\n12.23 ± 1.38\n13.01 ± 0.90\n18.73 ± 0.59\n20.01 ± 1.82\nNODE2VEC\n19.08 ± 0.52\n23.97 ± 0.58\n24.82 ± 1.00\n8.32 ± 0.36\n11.07 ± 0.60\n12.11 ± 1.93\n13,32 ± 0.49\n18.57 ± 0.49\n19.66 ± 2.34\nLINE\n18.13 ± 0.33\n22.56 ± 0.49\n23.00 ± 0.92\n8.49 ± 0.41\n12.43 ± 0.81\n12.40 ± 1.18\n12,79 ± 0.48\n18.06 ± 0.81\n20.59 ± 1.59\nWVRN\n10.86 ± 0.87\n17.46 ± 0.74\n20.10 ± 0.98\n4.14 ± 0.54\n4.42 ± 0.35\n4.41 ± 0.53\n8.60 ± 0.57\n14.65 ± 0.74\n17.50 ± 1.42\nMAJORITY\n2.51 ± 0.09\n2.57 ± 0.08\n2.53 ± 0.31\n3.38 ± 0.13\n3.36 ± 0.14\n3.36 ± 0.44\n1.58 ± 0.25\n1.51 ± 0.27\n1.44 ± 0.35\nTable 4: Multi-label classiﬁcation results for BlogCatalog, POS and PPI in the inductive setting for\nTr = 0.1. The upper and lower part of the table list micro and macro F1 scores, respectively.\nBlogCatalog\nPOS\nPPI\nRemoved Nodes [%]\n20\n40\n20\n40\n20\n40\nγ = 10\nγ = 5\nγ = 10\nγ = 10\nγ = 10\nγ = 10\nEP-B\n29.22 ± 0.95\n27.30 ± 1.33\n43.23 ± 1.44\n42.12 ± 0.78\n16.63 ± 0.98\n14.87 ± 1.04\nDEEPWALK-I\n27.84 ± 1.37\n27.14 ± 0.99\n40.92 ± 1.11\n41.02 ± 0.70\n15.55 ± 1.06\n13.99 ± 1.18\nLINE-I\n19.15 ± 1.30\n19.96 ± 2.44\n40.34 ± 1.72\n40.08 ± 1.64\n14.89 ± 1.16\n13.55 ± 0.90\nWVRN\n19.36 ± 0.59\n19.07 ± 1.53\n23.35 ± 0.66\n27.91 ± 0.53\n8.83 ± 0.91\n9.41 ± 0.94\nMAJORITY\n16.84 ± 0.68\n16.81 ± 0.55\n40.43 ± 0.86\n40.59 ± 0.55\n6.09 ± 0.40\n6.39 ± 0.61\nγ = 10\nγ = 5\nγ = 10\nγ = 10\nγ = 10\nγ = 10\nEP-B\n12.12 ± 0.75\n11.24 ± 0.89\n5.47 ± 0.80\n5.16 ± 0.49\n11.55 ± 0.90\n10.38 ± 0.90\nDEEPWALK-I\n11.96 ± 0.88\n10.91 ± 0.95\n4.54 ± 0.32\n4.46 ± 0.57\n10.52 ± 0.56\n9.69 ± 1.14\nLINE-I\n6.64 ± 0.49\n6.54 ± 1.87\n4.67 ± 0.46\n4.24 ± 0.52\n9.86 ± 1.07\n9.15 ± 0.74\nWVRN\n9.45 ± 0.65\n9.18 ± 0.62\n3.74 ± 0.64\n3.87 ± 0.44\n6.90 ± 1.02\n6.81 ± 0.89\nMAJORITY\n2.50 ± 0.18\n2.59 ± 0.19\n3.35 ± 0.24\n3.27 ± 0.15\n1.54 ± 0.31\n1.55 ± 0.26\nWe did not simply copy results from previous work but used the authors’ code to run all experiments\nagain. For DEEPWALK we used the implementation provided by the authors of NODE2VEC (setting\np = 1.0 and q = 1.0). We also used the other hyperparameters values for DEEPWALK reported in\nthe NODE2VEC paper to ensure a fair comparison. We did 10 runs for each method in each of the\nexperimental set-ups described in this section, and computed the mean and standard deviation of\nthe corresponding evaluation metrics. We use the same sets of training, validation and test data for\neach method. All methods were evaluated in the transductive and inductive setting. The transductive\nsetting is the setting where all nodes of the input graph are present during training. In the inductive\nsetting, a certain percentage of the nodes are not part of the graph during unsupervised learning.\nInstead, these removed nodes are added after the training has concluded. The results computed for\nthe nodes not present during unsupervised training reﬂect the methods ability to incorporate newly\nadded nodes without retraining the model.\nFor the graphs without attributes (BlogCatalog, PPI and POS) we follow the exact same experimental\nprocedure as in previous work [42, 34, 15]. First, the node embeddings were computed in an\nunsupervised fashion. Second, we sampled a fraction Tr of nodes uniformly at random and used\ntheir embeddings and class labels as training data for a logistic regression classiﬁer. The embeddings\nand class labels of the remaining nodes were used as test data. EP-B’s margin hyperparameter γ\nwas chosen by 3-fold cross validation for Tr = 0.1 once. The resulting margin γ was used for\nthe same data set and for all other values of Tr. For each method, we use 3-fold cross validation\nto determine the L2 regularization parameter for the logistic regression classiﬁer from the values\n[0.01, 0.1, 0.5, 1, 5, 10]. We did this for each value of Tr and the F1 macro and F1 micro scores\nseparately. This proved to be important since the L2 regularization had a considerable impact on the\nperformance of the methods.\nFor the graphs with attributes (Cora, Citeseer, Pubmed) we follow the same experimental procedure\nas in previous work [45]. We sample 20 nodes uniformly at random for each class as training data,\n1000 nodes as test data, and a different 1000 nodes as validation data. In the transductive setting,\nunsupervised training was performed on the entire graph. In the inductive setting, the 1000 test nodes\nwere removed from the graph before training. The hyperparameter values of GCN for these same\ndata sets in the transductive setting are reported in [18]; we used these values for both the transductive\nand inductive setting. For EP-B, LINE and DEEPWALK, the learned node embeddings for the 20\nnodes per class label were fed to a one-vs-rest logistic regression classiﬁer with L2 regularization. We\n7\nTable 5: Classiﬁcation accuracy for Cora, Citeseer, and Pubmed. (Left) The upper and lower part of\nthe table list the results for the transuctive and inductive setting, respectively. (Right) Results for the\ntransductive setting where the directionality of the edges is taken into account.\nMethod\nCora\nCiteseer\nPubmed\nγ = 20\nγ = 10\nγ = 1\nEP-B\n78.05 ± 1.49\n71.01 ± 1.35\n79.56 ± 2.10\nDW+BOW\n76.15 ± 2.06\n61.87 ± 2.30\n77.82 ± 2.19\nPLANETOID-T\n71.90 ± 5.33\n58.58 ± 6.35\n74.49 ± 4.95\nGCN\n79.59 ± 2.02\n69.21 ± 1.25\n77.32 ± 2.66\nDEEPWALK\n71.11 ± 2.70\n47.60 ± 2.34\n73.49 ± 3.00\nBOW FEAT\n58.63 ± 0.68\n58.07 ± 1.72\n70.49 ± 2.89\nγ = 5\nγ = 5\nγ = 1\nEP-B\n73.09 ± 1.75\n68.61 ± 1.69\n79.94 ± 2.30\nDW-I+BOW\n68.35 ± 1.70\n59.47 ± 2.48\n74.87 ± 1.23\nPLANETOID-I\n64.80 ± 3.70\n61.97 ± 3.82\n75.73 ± 4.21\nGCN-I\n67.76 ± 2.11\n63.40 ± 0.98\n73.47 ± 2.48\nBOW FEAT\n58.63 ± 0.68\n58.07 ± 1.72\n70.49 ± 2.89\nMethod\nCora\nCiteseer\nPubmed\nγ = 20\nγ = 5\nγ = 1\nEP-B\n77.31 ± 1.43\n70.21 ± 1.17\n78.77 ± 2.06\nDEEPWALK\n14.82 ± 2.15\n15.79 ± 3.58\n32.82 ± 2.12\nchose the best value for EP-B’s margins and the L2 regularizer on the validation set from the values\n[0.01, 0.1, 0.5, 1, 5, 10]. The same was done for the baselines DW+BOW and BOW FEAT. Since\nPLANETOID jointly optimizes an unsupervised and supervised loss, we applied the learned models\ndirectly to classify the nodes. The authors of PLANETOID did not report the number of learning\niterations, so we ensured the training had converged. This was the case after 5000, 5000, and 20000\ntraining steps for Cora, Citeseer, and Pubmed, respectively. For EP-B we used ADAM [17] to learn\nthe parameters in a mini-batch setting with a learning rate of 0.001. A single learning epoch iterates\nthrough all nodes of the input graph and we ﬁxed the number of epochs to 200 and the mini-batch size\nto 64. In all cases, the parameteres were initilized following [14] and the learning always converged.\nEP was implemented with the Theano [4] wrapper Keras [9]. We used the logistic regression classiﬁer\nfrom LibLinear [10]. All experiments were run on commodity hardware with 128GB RAM, a single\n2.8 GHz CPU, and a TitanX GPU.\n5.2\nResults\nThe results for BlogCatalog, POS and PPI in the transductive setting are listed in Table 3. The best\nresults are always indicated in bold. We observe that EP-B tends to have the best F1 scores, with\nthe additional aforementioned advantage of fewer parameters and hyperparameters to tune. Even\nthough we use the hyperparameter values reported in NODE2VEC, we do not observe signiﬁcant\ndifferences to DEEPWALK. This is contrary to earlier ﬁndings [15]. We conjecture that validating the\nL2 regularization of the logistic regression classiﬁer is crucial and might not have been performed in\nsome earlier work. The F1 scores of EP-B, DEEPWALK, LINE, and NODE2VEC are signiﬁcantly\nhigher than those of the baselines WVRN and MAJORITY. The results for the same data sets in the\ninductive setting are listed in Table 4 for different percentages of nodes removed before unsupervised\ntraining. EP reconstructs label embeddings from the embeddings of labels of neighboring nodes.\nHence, with EP-B we can directly use the concatenation of the reconstructed embedding ehi(v) as\nthe node embedding for each of the nodes v that were not part of the graph during training. For\nDEEPWALK and LINE we computed the embeddings of those nodes that were removed during\ntraining by averaging the embeddings of neighboring nodes; we indicate this by the sufﬁx I. EP-B\noutperforms all these methods in the inductive setting.\nThe results for the data sets Cora, Citeseer and Pubmed are listed in Table 5. Since these data sets\nhave bag of words associated with nodes, we include the baseline method DW+BOW. DW+BOW\nconcatenates the embedding of a node learned by DEEPWALK with a vector that encodes the bag of\nwords of the node. PLANETOID-T and PLANETOID-I are the transductive and inductive formulation\nof PLANETOID [45]. GCN-I is an inductive variant of GCN [18] where edges from training to test\nnodes are removed from the graph but those from test nodes to training nodes are not. Contrary\nto other methods, EP-B’s F1 scores on the transductive and inductive setting are very similar,\ndemonstrating its suitability for the inductive setting. DEEPWALK cannot make use of the word\nlabels but we included it in the evaluation to investigate to what extent the word labels improve the\nperformance of the other methods. The baseline BOW FEAT trains a logistic regression classiﬁer\non the binary vectors encoding the bag of words of each node. EP-B signiﬁcantly outperforms all\nexisting approaches in both the transductive and inductive setting on all three data sets with one\n8\n(a)\n0\n50\n100\n150\n200\nEpoch\n0\n10\n20\n30\n40\n50\n60\n70\nAverage Batch Loss\nκ = 1\nκ = 5\nκ = 50\nκ = degmax = 3992\n(b)\nFigure 4: (a) The plot visualizes embeddings for the Cora data set learned from node identity labels\nonly (left), word labels only (center), and from the combination of the two (right). The Silhouette\nscore is from left to right 0.008, 0.107 and 0.158. (b) Average batch loss vs. number of epochs for\ndifferent values of the parameter κ for the BlogCatalog data set.\nexception: for the transductive setting on Cora GCN achieves a higher accuracy. Both PLANETOID-T\nand DW+BOW do not take full advantage of the information given by the bag of words, since the\nencoding of the bag of words is only exposed to the respective models for nodes with class labels and,\ntherefore, only for a small fraction of nodes in the graph. This could also explain PLANETOID-T’s\nhigh standard deviation since some nodes might be associated with words that occur in the test data\nbut which might not have been encountered during training. This would lead to misclassiﬁcations of\nthese nodes.\nFigure 4 depicts a visualization of the learned embeddings for the Cora citation network by applying t-\nsne [26] to the 128-dimensional embeddings generated by EP-B. Both qualitatively and quantitatively\n– as demonstrated by the Silhouette score [37] that measures clustering quality – it shows EP-B’s\nability to learn and combine embeddings of several label types.\nUp until now, we did not take into account the direction of the edges, that is, we treated all graphs as\nundirected. Citation networks, however, are intrinsically directed. The right part of Table 5 shows\nthe performance of EP-B and DEEPWALK when the edge directions are considered. For EP this\nmeans label representations are only sent along the directed edges. For DEEPWALK this means\nthat the generated random walks are directed walks. While we observe a signiﬁcant performance\ndeterioration for DEEPWALK, the accuracy of EP-B does not change signiﬁcantly. This demonstrates\nthat EP is also applicable when edge directions are taken into account.\nFor densely connected graphs with a high average node degree, it is beneﬁcial to limit the number of\nneighbors that send label representations in each learning step. This can be accomplished by sampling\na subset of at most size κ from the set of all neighbors and to send messages only from the sampled\nnodes. We evaluated the impact of this strategy by varying the parameter κ in Figure 4. The loss is\nsigniﬁcantly higher for smaller values of κ. For κ = 50, however, the average loss is almost identical\nto the case where all neighbors send messages while reducing the training time per epoch by an order\nof magnitude (from 20s per epoch to less than 1s per epoch).\n6\nConclusion and Future Work\nEmbedding Propagation (EP) is an unsupervised machine learning framework for graph-structured\ndata. It learns label and node representations by exchanging messages between nodes. It supports\narbitrary label types such as node identities, text, movie genres, and generalizes several existing\napproaches to graph representation learning. We have shown that EP-B, a simple instance of EP,\nis competitive with and often outperforms state of the art methods while having fewer parameters\nand/or hyperparameters. We believe that EP’s crucial advantage over existing methods is its ability to\nlearn label type representations and to combine these label type representations into a joint vertex\nembedding.\nDirection of future research include the combination of EP with multitask learning, that is, learning\nthe embeddings of labels and nodes guided by both an unsupervised loss and a supervised loss deﬁned\nwith respect to different tasks; a variant of EP that incorporates image and sequence data; and the\nintegration of EP with an existing distributed graph processing framework. One might also want to\ninvestigate the application of the EP framework to multi-relational graphs.\n9\nReferences\n[1] L. Akoglu, H. Tong, and D. Koutra. Graph based anomaly detection and description: a survey.\nData Mining and Knowledge Discovery, 29(3):626–688, 2015.\n[2] L. B. Almeida. Artiﬁcial neural networks. chapter A Learning Rule for Asynchronous Percep-\ntrons with Feedback in a Combinatorial Environment, pages 102–111. 1990.\n[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and\nclustering. In NIPS, volume 14, pages 585–591, 2001.\n[4] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-\nFarley, and Y. Bengio. Theano: A cpu and gpu math compiler in python. In Proc. 9th Python in\nScience Conf, pages 1–7, 2010.\n[5] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embeddings\nfor modeling multi-relational data. In Advances in Neural Information Processing Systems,\npages 2787–2795, 2013.\n[6] B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred,\nD. H. Lackner, J. Bähler, V. Wood, et al. The biogrid interaction database: 2008 update. Nucleic\nacids research, 36(suppl 1):D637–D640, 2008.\n[7] J. Bromley, I. Guyon, Y. Lecun, E. Säckinger, and R. Shah. Signature veriﬁcation using a\n\"siamese\" time delay neural network. In Neural Information Processing Systems, 1994.\n[8] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Comput. Surv.,\n41(3):15:1–15:58, 2009.\n[9] F. Chollet. Keras. URL http://keras. io, 2016.\n[10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large\nlinear classiﬁcation. Journal of machine learning research, 9(Aug):1871–1874, 2008.\n[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep\nvisual-semantic embedding model. In Advances in neural information processing systems,\npages 2121–2129, 2013.\n[12] L. Getoor and B. Taskar. Introduction to Statistical Relational Learning (Adaptive Computation\nand Machine Learning). The MIT Press, 2007.\n[13] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for\nquantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\n[14] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Aistats, volume 9, pages 249–256, 2010.\n[15] A. Grover and J. Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings of\nthe 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\npages 855–864, 2016.\n[16] K. Kersting, M. Mladenov, R. Garnett, and M. Grohe. Power iterated color reﬁnement. In\nTwenty-Eighth AAAI Conference on Artiﬁcial Intelligence, 2014.\n[17] D. Kingma and J. Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\n[18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks.\narXiv preprint arXiv:1609.02907, 2016.\n[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information processing systems, pages 1097–1105,\n2012.\n[20] J. B. Kruskal and M. Wish. Multidimensional scaling. Sage Publications, Beverely Hills,\nCalifornia, 1978.\n10\n[21] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks.\narXiv preprint arXiv:1511.05493, 2015.\n[22] D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. J. Am. Soc.\nInf. Sci. Technol., 58(7):1019–1031, 2007.\n[23] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and J. M. Hellerstein. Graphlab:\nA new parallel framework for machine learning. In Conference on Uncertainty in Artiﬁcial\nIntelligence (UAI), July 2010.\n[24] L. Lü and T. Zhou. Link prediction in complex networks: A survey. Physica A: Statistical\nMechanics and its Applications, 390(6):1150–1170, 2011.\n[25] U. Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395–416, 2007.\n[26] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning\nResearch, 9(Nov):2579–2605, 2008.\n[27] S. A. Macskassy and F. Provost. A simple relational classiﬁer. Technical report, DTIC Document,\n2003.\n[28] M. Mahoney. Large text compression benchmark. URL: http://www. mattmahoney. net/text/text.\nhtml, 2009.\n[29] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski.\nPregel: A system for large-scale graph processing. In Proceedings of the 2010 ACM SIGMOD\nInternational Conference on Management of Data, pages 135–146, 2010.\n[30] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of\nwords and phrases and their compositionality. In Advances in neural information processing\nsystems, pages 3111–3119, 2013.\n[31] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In\nProceedings of the 28th international conference on machine learning, pages 689–696, 2011.\n[32] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A review of relational machine learning\nfor knowledge graphs. Proceedings of the IEEE, 104(1):11–33, 2016.\n[33] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank citation ranking: bringing order\nto the web. 1999.\n[34] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations. In\nProceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, pages 701–710, 2014.\n[35] F. J. Pineda. Generalization of back-propagation to recurrent neural networks. Phys. Rev. Lett.,\n59:2229–2232, 1987.\n[36] L. D. Raedt, K. Kersting, S. Natarajan, and D. Poole. Statistical relational artiﬁcial intelligence:\nLogic, probability, and computation. Synthesis Lectures on Artiﬁcial Intelligence and Machine\nLearning, 10(2):1–189, 2016.\n[37] P. J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster\nanalysis. Journal of computational and applied mathematics, 20:53–65, 1987.\n[38] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.\nScience, 290(5500):2323–2326, 2000.\n[39] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural\nnetwork model. IEEE Transactions on Neural Networks, 20(1):61–80, 2009.\n[40] P. Sen, G. M. Namata, M. Bilgic, L. Getoor, B. Gallagher, and T. Eliassi-Rad. Collective\nclassiﬁcation in network data. AI Magazine, 29(3):93–106, 2008.\n11\n[41] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information network\nembedding. In Proceedings of the 24th International Conference on World Wide Web, pages\n1067–1077. ACM, 2015.\n[42] L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the\n15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages\n817–826. ACM, 2009.\n[43] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear\ndimensionality reduction. Science, 290(5500):2319–2323, 2000.\n[44] R. S. Xin, J. E. Gonzalez, M. J. Franklin, and I. Stoica. Graphx: A resilient distributed graph\nsystem on spark. In First International Workshop on Graph Data Management Experiences\nand Systems, pages 2:1–2:6, 2013.\n[45] Z. Yang, W. W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph\nembeddings. In Proceedings of the 33nd International Conference on Machine Learning, pages\n40–48, 2016.\n[46] R. Zafarani and H. Liu. Social computing data repository at asu. School of Computing,\nInformatics and Decision Systems Engineering, Arizona State University, 2009.\n[47] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation.\nTechnical Report CMU-CALD-02-107, 2002.\n12\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-10-09",
  "updated": "2017-10-09"
}