{
  "id": "http://arxiv.org/abs/1710.06798v1",
  "title": "Feature versus Raw Sequence: Deep Learning Comparative Study on Predicting Pre-miRNA",
  "authors": [
    "Jaya Thomas",
    "Sonia Thomas",
    "Lee Sael"
  ],
  "abstract": "Should we input known genome sequence features or input sequence itself in\ndeep learning framework? As deep learning more popular in various applications,\nresearchers often come to question whether to generate features or use raw\nsequences for deep learning. To answer this question, we study the prediction\naccuracy of precursor miRNA prediction of feature-based deep belief network and\nsequence-based convolution neural network. Tested on a variant of six-layer\nconvolution neural net and three-layer deep belief network, we find the raw\nsequence input based convolution neural network model performs similar or\nslightly better than feature based deep belief networks with best accuracy\nvalues of 0.995 and 0.990, respectively. Both the models outperform existing\nbenchmarks models. The results shows us that if provided large enough data,\nwell devised raw sequence based deep learning models can replace feature based\ndeep learning models. However, construction of well behaved deep learning model\ncan be very challenging. In cased features can be easily extracted,\nfeature-based deep learning models may be a better alternative.",
  "text": "Thomas et al.\nRESEARCH\nFeature versus Raw Sequence: Deep Learning\nComparative Study on Predicting Pre-miRNA\nJaya Thomas1†, Sonia Thomas1† and Lee Sael1,2*\n*Correspondence:\nsael@cs.stonybrook.edu\n1Department of Computer\nScience, SUNY Korea, 21985\nIncheon, Korea\n2Department of Computer\nScience, Stony Brook University,\n11794 Stony Brook, NY, USA\nFull list of author information is\navailable at the end of the article\n†Equal contributor\nAbstract\nBackground: Should we input known genome sequence features or input\nsequence itself in deep learning framework? As deep learning more popular in\nvarious applications, researchers often come to question whether to generate\nfeatures or use raw sequences for deep learning. To answer this question, we\nstudy the prediction accuracy of precursor miRNA prediction of feature-based\ndeep belief network and sequence-based convolution neural network.\nResults: Tested on a variant of six-layer convolution neural net and three-layer\ndeep belief network, we ﬁnd the raw sequence input based convolution neural\nnetwork model performs similar or slightly better than feature based deep belief\nnetworks with best accuracy values of 0.995 and 0.990, respectively. Both the\nmodels outperform existing benchmarks models. The results shows us that if\nprovided large enough data, well devised raw sequence based deep learning models\ncan replace feature based deep learning models. However, construction of well\nbehaved deep learning model can be very challenging. In cased features can be\neasily extracted, feature-based deep learning models may be a better alternative.\nKeywords: precursor miRNA; Convolution neural network; Deep belief network;\nDeep learning comparison\nIntroduction\nDeep learning methods have been popularized in bio-sequence analysis. More specif-\nically, convolution neural network (CNN) have been widely applied to characterize\nand classify raw sequence data []. Traditionally, to classify sequence data, sequence\nfeatures were generated, fed in to classiﬁcation algorithms, and predictions were\nmade. CNN simpliﬁed this process by removing the need to feature generation\nwhich could be challenging in cases. However, question arises of whether to use raw\nsequences when there are already good set of features. We answer this question in\nthe context of precursor micro RNA prediction where raw sequence data as well as\nset of working sequence features which are shown to give high accuracy.\nPrecursor miRNA\nMicroRNAs (miRNAs) are single-stranded small non-coding RNAs that are typi-\ncally 22 nucleotides long. A miRNA regulates gene expression at the post transcrip-\ntion level by base pairing with a complementary messenger RNA (mRNA) there by\nhindering the translation of the mRNA to proteins. The regulatory role of miRNAs\nare important in development, cell proliferation and cell death and their malfunction\nhas been connected with neuro-degenerative disease, cancer and metabolic disor-\narXiv:1710.06798v1  [cs.LG]  17 Oct 2017\nThomas et al.\nPage 2 of 12\nders [1]. Furthermore, informatics analysis predicts that 30% of human genes are\nregulated by miRNA [2].\nMiRNAs can be experimentally determined by directional cloning of endogenous\nsmall RNAs [3]. However, this is a time consuming process that require expensive\nlaboratory reagents. These drawbacks motivate the application of computational ap-\nproaches for predicting miRNAs. The goal of miRNA prediction is to correctly clas-\nsify pre-miRNAs from other pseudo hairpins. Via miRNA biogenesis, pre-miRNA\nbecomes a mature miRNA, however, other hairpins do not. The miRNA biogenesis\ninvolves number of steps. First, primary transcripts of miRNA (pri-miRNA) are\ntranscribed from introns of protein coding genes that are several kilobases long.\nThe pri-miRNAs are then clopped by Rnase-III enzyme Drosha into ∼70 base pairs\n(bp) long hair-pin-looped precursor miRNAs (pre-miRNAs). Then exportin-5 pro-\nteins transport pre-miRNA hairpins into the cytoplasm through nuclear pore. In\ncytoplasm, pre-miRNAs are further cleaved by Rnase-III enzyme Dicer to produce\na ∼20 bp double stranded intermediate called miRNA:miRNA*. Then a strand of\nthe duplex with the low thermodynamic energy becomes a mature miRNA.\nPrecursor miRNA prediction methods\nSeveral machine learning based methods have been proposed to predict miRNAs,\nthat is to determine the true pre-miRNAs from other pseudo hairpins, RNA se-\nquences that have similar stem-loop features to pre-miRNAs but does not contain\nmature miRNAs, with high accuracy. Most methods relies on features generated\nfrom sequence, folding measures, stem-loop features and statistical measures and\ncareful selection of features.\nMany tools have been developed based on the diﬀerent classiﬁcation techniques\nsuch as naive Bayes classiﬁer (NBC), artiﬁcial neural networks (ANN), support\nvector machines (SVM), and random forests (RF). Among the approaches, support\nvector machine (SVM) had been most extensively applied. Some of the notable\nSVM-based methods includes triplet-SVM [4], MiRFinder [5], miPred [6], micro-\nPred [7], yasMiR [8], MiRenSVM [9], MiRPara [10], YamiPred [11], and G2DE [12].\nAmong them, Triplet-SVM[4] is the classiﬁer that consider local structure-sequence\nfeatures that reﬂect characteristics of miRNAs. The approach report an accuracy\nof 90% considering pre-miRNAs from the other 11 species including plants and\nvirus without considering any other comparative genomics information. Another,\nmiPred[6] SVM approach considered Gaussian Radial Basis Function kernel (RBF)\nas a similarity measure for global and intrinsic hairpin folding attribute and resulted\nwith accuracy of around 93%. MicroPred[7] introduces some additional features for\nevaluation of miRNA using SVM based machine learning classiﬁer. Author’s re-\nport classiﬁcation results of high speciﬁcity of 97.28% and sensitivity of 90.02%.\nThe miR-SF classiﬁer [13] predicts the identiﬁed human pre-miRNAs in miRBase\nsource on the selected optimized feature subset including 13 features, generated\nby SVM and genetic algorithm. Finially, YamiPred [11] is a genetic algorithm and\nSVM based embedded classiﬁer that consider feature selection and parameters op-\ntimization for improving performance. Other notable methods are based on random\nforests (RF)[14] and artiﬁcial neural networks (ANN) [15, 16]. In MiRANN[15] pre-\ndictor, author’s consider neural network for pre-miRNA prediction by expanding\nThomas et al.\nPage 3 of 12\nthe network with more neurons and the hidden layers and reports an 99.9% ACC\non a human dataset. The network is designed to be impartial for any feature by\nintegrating exceptional weight initializing equation where closest neurons slightly\ndiﬀer in weights. MiRANN utilizes carefully selected features on a neural network\nstructure. However, to the best of our knowledge, raw sequence data have not been\nused for distinguish pre-miRNA from other hair-pin sequences.\nDeep learning approaches\nTwo types of neural network models, i.e., deep belief network and convolution neural\nnetwork, are used to to compare the prediction accuracy of feature-based learning\nand raw sequence based learning. Convolution neural network (CNN) has been\nused in several instances to directly process raw data as input. CNN has gained\nmomentum due to its success in improving the previously recorded state-of-the-\nart performance measures in a wide range of domains including genome sequence\nanalysis. Deep belief network (DBN), on the other hand has been popular with\nwhere there are large number of features. Whether the input is a raw data or a high\ndimensional feature, DNN uses multi-layer architecture to infer from data. The deep\narchitecture automatically extracts high-level feature necessary for classiﬁcation or\nclustering. That is, the multiple layers in deep learning helps exploit the inherent\ncomplexities of data.\nDeep belief network\nDeep belief network (DBN) is an architecture obtained by stacking multiple re-\nstricted boltzmann machines (RBMs), such that the ith hidden layer is the input to\nthe i + 1th hidden layer. Let X be the observed vector and hidden layer Hk, with\nN hidden layers, then the distribution is as follows [17]:\nP(X, H1, H2, ..., Hk) = P(HN−1, HN)(\nN−2\nY\nk=0\nP(Hk|Hk+1)),\n(1)\nwhere X = H0, P(Hk−1)|Hk) is the distribution for visible (input) unit on the\nkth level hidden layer, and P(Hk−1|Hk) is the top level layer distribution of the\nvisible-hidden layer. The ﬁrst step in training the DBN is to train the ﬁrst layer\n(visible layer) of the model such that is models the raw input X = H0. In the second\nstep the distribution of the input (i.e transformed data) is obtained as P(H1|H0)\nusing training results of the ﬁrst layer and is used as the input for the second\nlayer. In the third step, second layer of RBM is trained on sampling the learned\nconditional probability in the previous layer. Steps two and three is repeated to\ngenerate multiple layers. In the ﬁnal step, hyper parameters are ﬁne-tuned based\non the gradient descent based back propagations. The ﬁrst hidden layer learns from\nthe structure of the data through the input layer and the process is continued by\nadding the second layer. The ﬁrst hidden layer acts as the input, which is multiplied\nby weight at the nodes of second hidden layer and thus the probability for activating\nthe second hidden layers is calculated. This process results in sequential sets of\nactivations by grouping features of features resulting in a feature hierarchy, by\nwhich networks learn more complex and abstract representations of data, and can\nThomas et al.\nPage 4 of 12\nbe repeated several times to create a multi-layer network. A standard feed-forward\nneural network is added after the last hidden layer to predict the label, the input\nto the network being being the activation probabilities. The resulting DBN is put\ntogether to adjust the weights with stochastic gradient descent back propagation\n[18].\nConvolution neural network\nTypical CNN models consist of multiple layers of convolution layer and pooling layer\nalternations ﬁnalized by a fully connected layer. The convolution layer performs\nthe convolution operation between the input values and learned ﬁlters, matrix of\nweights. Let (m, n) be the ﬁlter size and W be the small matrix of weights, then\nthe convolution layer performs a modiﬁed convolution of the W with the input X\nby calculating the dot product W · x + b, where x a instance of X and b is the bias.\nTypically the ﬁlters are are share by using the same ﬁlter across diﬀerent positions of\nthe input. The step size by which the ﬁlter slides across the input is called the stride,\nand the ﬁlter area (m × n) is called the receptive ﬁeld. Weight sharing concept is\nthe important characteristics of a convolution network that reduces the complexity\nof the problem by reducing number of weights learned. It is also allows location\ninvariant learning, i.e., if a important pattern exists, a appropriate CNN model will\nlearn it no matter where in the sequence. The convolution layer is often followed by\nthe pooling layer that summaries the value learned in the convolution layer. The\npooling also allows invariance in the learning as well as reducing model complexity.\nPopular pooling methods are average pooling or max pooling. The ﬁnal layer is the\nfully connected layer which is connected to the output or the classiﬁcation layer\nContributions\nThe main contribution of the paper are summarized as follows:\n• Compare the performance of feature based and raw sequence based deep learn-\ning: a deep belief network models is proposed for integrating large number of\nheterogeneous features and convolution neural network model is proposed for\nthe raw input sequence data.\n• Provides a solution for class imbalance problem, allowing for unbiased perfor-\nmance measures.\n• Compares the performance of proposed model against existing machine learn-\ning classiﬁer on eleven diﬀerent species which extends the previous work on\nhuman dataset only [16].\nMethods\nDataset\nmiRNA data selection\nWe use experimentally validated pre-miRNAs as positive examples and pseudo\nhairpins as negative examples to train and test the proposed method. The human\npre-miRNA sequence was retrieved from the miRBase 18.0 release [19]. Similar to\nmiPred [6] approach, the multiple loops were discarded to obtain 1600 pre-miRNAs\n(positive) dataset. The positive sample sequence has an average length of 84 nt\nwith maximum and minimum length being 154 nt and 43 nt respectively. Similarly\nThomas et al.\nPage 5 of 12\nthe negative sample sequence has average length of 85nt with 63 nt and 120 nt as\nminimum and maximum length respectively. The negative dataset consists of 8494\npseudo hairpins as the false samples. They were extracted from the human protein-\ncoding regions as suggested by microPred [7]. The average length of the sequence\nis 85 nt with minimum as 63 nt and maximum as 120 nt. The diﬀerent ﬁltering\ncriteria, including non-overlapping sliding window, no multiple loops, lowest base\npair number set to 18, and minimum free energy less than 15kcal/mol were applied\non these sequences to resemble the real pre-miRNA properties.\nClass imbalance solution\nAnother problem that we have addressed here is the class imbalance problem in\nmiRNA predictions. Class imbalance is a machine learning problem where the num-\nber of data samples belonging to one class (positive) is far less compared to data\nsample belonging to other class (negative). The imbalance class is often solved\nby using either under or over sampling methods. In case of under sampling the\ndata samples are removed from the majority class, whereas for over sampling bal-\nance is created either by addition of new samples or duplication of the existing\nminority class samples. Class imbalance problem often arise in miRNA data clas-\nsiﬁcation problem due to abundance in pseudo hairpin structure compared to true\npre-miRNAs folds. In existing classiﬁers such as triplet-SVM[4] and miPred [6] han-\ndled the imbalance problem manually.\nWe address the class imbalance problem during the training phase by adopting\na modiﬁed under sampling approach [20]. In the modiﬁed approach, we divided\nthe majority class into subsets using k-means algorithm with k=5, and thus ob-\ntain clusters with slightly higher similarity amongst the group. The entire negative\nsamples is divided into subsets using k-means algorithm with k=5, and the cluster\nhaving the highest similarity index among the group is selected. Now using 8 fold\ncross validation, the negative samples is divided into training and testing dataset\nsuch that training dataset has 1400 negative samples and testing dataset has 200\nnegative samples. Similarly, the positive sample is divided into training and testing\ndataset using 8 fold cross validation such that training dataset has 1400 positive\nsample and testing dataset has 200 positive samples. Hence, the training dataset\nhas 2400 samples and testing dataset has 400 samples.\nModeling deep belief network\nmiRNA feature encoding\nFeature based learning require features as inputs. This work adopts 58 characteris-\ntic features, which are shown useful in existing studies for predicting miRNA [11].\nThe features includes sequences composition properties, folding measures, stem-loop\nfeatures, energy and statistical features, and 20 selected features to diﬀerentiate pre-\nmiRNAs from pseudo hairpins for candidate input of the DBN model. These features\nare extracted based on the knowledge based analysis of the existing methods for the\nmiRNA analysis. The common characteristics of pre-miRNAs used for evaluation\nconsists of sequences composition properties, secondary structures, folding measures\nand energy. The sequence characteristics include features related to the frequency\nof two and three adjacent nucleotide and aggregate dinucleotide frequency in the\nThomas et al.\nPage 6 of 12\nsequence. The secondary structure features from the perspectives of miRNA bio-\ngenesis relating diﬀerent thermodynamic stability proﬁles of pre-miRNAs. These\nstructures have lower free energy and often contain stem and loop regions. They in-\nclude diversity, frequency, entropy-related properties, enthalpy-related properties of\nthe structure. The other features are hairpin length, loop length, consecutive base-\npairs and ratio of loop length to hairpin length of pre-miRNA secondary structure.\nThe energy characteristic associated to the energy of secondary structure includes\nthe minimal free energy of the secondary structure, overall free energy NEFE, com-\nbined energy features and the energy required for dissolving the secondary structure.\nAll the features extracted are normalized to standardizing the inputs in order to\nimprove the training and to avoid getting stuck in local optima. The features used\nare summarized Table ?? and detailed in Table 7 .\nDeep belief network architecture\nFigure 1 A deep learning to predict miRNA with extracted features.\nThe proposed DBN based miRNA prediction method, we call miRNA-FDL, has\nthree hidden layers, and the model is denoted as X-100-70-35-1, where X being the\nsize of the input layer, 1 denotes the number of neuron in the output layer and\nthe remaining values denotes the number of neurons in each hidden layer. Figure 1\nillustrates the model architecture and layer-by-layer learning procedure described\nin . Diﬀerent model architectures were trained using the same learning procedure\nbut varying the number of hidden layer and nodes. Amongst the candidate network\nmodels, a better one was selected based on the classiﬁer accuracy.\nThe weights of the miRNA-FDL was trained with stochastic gradient descent base\nback propagation algorithm [18]] were the update rule is the following:\nwij(t + 1) = wij(t) + η ∂C\n∂wij\n,\n(2)\nwhere, wij(t+1) is the weight computed at t+1, ∂denotes the learning rate, and C\nis the cost function. For the given model, softmax is used as an activation function\nand the cost is computed using cross entropy. The softmax function is deﬁned as\npj =\nexp(xj)\nP\nk exp(xk),\n(3)\nwhere, pj is the output of the unit j, xj and xk denotes the total input to unit j\nand k, respectively for the same level. The cross entropy is given by\nC = −\nX\ni\ndjlog(pj),\n(4)\nwhere dj is the target probability for output unit j and pj is the probability output\nafter applying the activation function.\nThomas et al.\nPage 7 of 12\nModeling convolution neural network\nCNN input processing\nEach pre-miRNA is a RNA sequence of composed letters (A, C, G, U). Each of\nthe nucleotide is encoded using one-hot-code methods. That is, A is encoded as\n(1,0,0,0), C as (0,1,0,0), G as (0,0,1,0), and U as (0,0,0,1). The micro RNA is a\nnucleotide sequence of (A, C, G, U).\nConvolution neural network architecture\nVarious architecture of the CNN can be generated dependent on the choice of num-\nber of layers and on how to combine convolution layer with pooling layers. The Table\n3 shows the various variants of the CNN architecture considered for the study. In\nCNN model type 1, the CNN architecture has single layer of convolution followed by\na pooling layer which in turn is connected to the fully connected output layer. The\noutput layer is connected to the classiﬁcation layer which classiﬁes the predicted\nlabels. The model type 2 is variation to model type 1, such that the pooling layer\nof model type 1 is replaced by a fully connected layer. Hence the model type 2 has\ntwo fully connected layers. The further variation to the model type 1 leads to the\nmodel type 3 such that it has two convolution layers. All other layers as similar to\nthe base model 1. The model type 4 has three convolution layers. In all the mod-\nels global pooling is preferred over local pooling as it is observed that features are\nbetter learned in global pooling.\nThe architecture of the CNN model highly depends on the various hyper-\nparameters. We set the number of node in the input layer to be 4 × 160, 4 for\nthe one-hot-code encoding and 160 the for length of the sequence as the maximum\nlength of sequence in the database is 154bp. We set the output node of the the fully\nconnected layer to be three and add a classiﬁcation layer which identiﬁes the input\nsequence as whether it contains the pre-miRNA or not based on the three nodes\nresult. Other hyper-parameters tested are list in the Table 4. The various combina-\ntion of the hyper-parameters mentioned in Table 4 with the models mentioned in\nTable 3, are considered.\nResults\nWether to determine the eﬃcacy of raw sequence based learning and feature based\nlearning, we compared the accuracies of two DBN models that works one unselected\npre-miRNA feature set of size 58 and selected feature set of size 20 and the accuracy\nof one CNN model that works on raw pre-miRNA sequences. We also compared\nthe proposed methods on other machine learning methods. The proposed miRNA\nprediction models are implemented on MATLAB 2016 (b) platform with 2.30 GHz\nIntel Xenon GPU E5-2630 and 32 GB RAM. The most crucial aspect of the deep\nlearning was on the selection of the appropriate hyper-parameters. We describe the\nﬁnal models that was selected in the following. The performance of the proposed\nand compared methods are summarized in Table 6.\nDBN based precursor miRNA prediction model\nThe various candidate model for the DBN based precursor miRNA prediction model\nis obtained by varying the number of hidden nodes in the hidden layer as well as\nThomas et al.\nPage 8 of 12\nthe number of hidden layers. The best prediction accuracy is obtained for a DBN\nnetwork architecture [Fig. 3] of three hidden layers with ﬁrst, second and third layer\nhaving 100, 70, 35 hidden neurons respectively. Considering the stochastic nature of\nthe algorithm the output values are averaged for twenty executions. It is observed\nthat for 58 features as input, the DBN model (Input-100-70-35-output) gives an\naccuracy of 0.968 with F1-score of 0.957 Furthermore from the literature survey\n[11] it is learned that the most relevant features associated with the miRNA are\nmelting temperature, enthropy, enthalpy and free minimum energy. Thus the 20\nrelevant features mentioned are aggregate nucleotide frequency A+U, dinucleotide\nfrequencies AG, AU, CU, GA, UU, Minimum Free Energy Index 4 (MFEI4), Posi-\ntional Entropy, Normalized Ensem-ble free energy, Frequency of the MFE structure\n(Freq), Enthalpy normalized by the length of the sequence (dH/L), Melting tem-\nperature (Tm), Melting temperature normalized by length (Tm/L), Normalized\nbase-pair count by length, j G-Cj /L, Normalized average base pairs by number of\nstem loops (A-U)/ stems, (G-U)/stems, the length of the sequence (Len), Centroid\nenergy normalized by length (CE/L), and the Statistical Z-scores zG and zSP. The\nDBN model for the above 20 features gives an accuracy of 0.992 with F score 0.989\nwhich is slightly higher than the using all 58 features.\nCNN based precursor miRNA prediction model\nThe various candidate models obtained by the combination of Table 3 and Table 4\nare bested and two models that output highest accuracies on the validation data set\nare selected. Deeper layers were also tested, however, additional layers of convolution\ndoes not increases the performance of the miRNA prediction due to two factors due\nto limitation of available number of data. The two models are described bellow and\nsummarized in Table 5.\nThe architecture of the model type 2 could be explained as follows: the input\nlayer (raw sequence data) is convoluted with a ﬁlter (window) of size of 18, and\nthe window is shift-ed with value of 4 (using a stride of 4). The total number\nof ﬁlters used are 20. The output obtained after the convolution, is now fed into\na fully connected layer having 90 neurons. Furthermore the output from this fully\nconnected layer is again fed into another fully connected layer having 2 neurons. This\nlayer is also called the output layer. The output layer is connected to a classiﬁcation\nlayer which classiﬁes the label. In the model type 2, after the convolution layer, two\nfully connected layers are used before the ﬁnal (output) classiﬁcation layer. The\nfully connected layer helps in better learn-ing of the features that are extracted\nfrom the convolution layer.\nIn the model type 3 as depicted by Figure 2, the architecture is as follows: the\ninput layer (raw se-quence data) is convoluted with a ﬁlter (window) of size 12 and\nthe window is shifted with value 1 (Stride =1). The output of this convolution layer\nis again convoluted with another ﬁlter (window) of size 6 and Stride=1. For both the\nconvolutions the number ﬁlters used is 12. Now after the second convolution, a max\npooling layer is connected with window size 6 and the window is shifted with value\n4 (stride =4). The output of the pooling layer is connected to the fully connected\nlayer having 2 neurons, (i.e the output layer). The output layer is connected to a\nclassiﬁcation layer which classiﬁes the label. For both the models, the accuracy is\nat its best at the dropout ratio of 0.3 at the output layer.\nThomas et al.\nPage 9 of 12\nComparison with the existing computational methods\nProposed prediction model using CNN and DBN are compared to the existing\nbenchmark models. It is clearly observed that the prediction model based on deep\nlearning approaches outperforms compared methods. Two models of CNN and DBN\nmodel with 20 selected features has highest accuracy values above 0.99. The DBN\nmodel working on 58 features also has high accuracy of 0.968. This shows that DBN\nperforms well on large set of unselected features. Both the proposed models in this\nstudy, provided enough data, validate that deeper the network model, higher is the\nprecision eﬃcacy of the classiﬁer. Table 6.\nDiscussions and Conclusions\nIn this study, prediction model for prediction of precursor miRNA that contains\nmiRNA sequence is proposed using deep learning techniques using convolution neu-\nral networks on raw sequence input and deep belief networks on feature sets. Convo-\nlution neural network, when well modeled, were able to automatically learn relevant\nfeature from raw RNA sequence for predicting correct pre-miRNAs, hence develop-\ning a highly accurate classiﬁer. Deep learning framework, outperform all the existing\npopular learning algorithms including naive Bayes, random forest, k nearest neigh-\nbor, and SVM.\nAuthor details\n1Department of Computer Science, SUNY Korea, 21985 Incheon, Korea.\n2Department of Computer Science, Stony\nBrook University, 11794 Stony Brook, NY, USA.\nReferences\n1. Witkos, T.M., Koscianska, E., Krzyzosiak, W.J.: Practical aspects of microrna target prediction. Curr Mol Med\n11(2), 99–109 (2011). doi:10.2174/156652411794859250\n2. Ross, J.S., Carlson, J.A., Brock, G.: mirna: the new gene silencer. Am J Clin Pathol. 128(5), 830–836 (2007).\ndoi:10.1309/2JK279BU2G743MWJ\n3. Chen, P.Y., Manninga, H., Slanchev, K., Chien, M., Russo, J.J., Ju, J., Sheridan, R., John, B., Marks, D.S.,\nGaidatzis, D., Sander, C., Zavolan, M., Tuschl, T.: The developmental mirna proﬁles of zebraﬁsh as determined\nby small rna cloning. Genes and Development 19(11), 1288–1293 (2005). doi:10.1101/gad.1310605\n4. Xue, C., Li, F., He, T., Liu, G.P., Li, Y., Zhang, X.: Classiﬁcation of real and pseudo microrna precursors using\nlocal structure sequence features and support vector machine. BMC Bioinformatics 6, 310 (2005).\ndoi:10.1186/1471-2105-6-310\n5. Huang, T.H., Fan, B., Rothschild, M.F., Hu, Z.L., Li, K., Zhao, S.H.: Mirﬁnder: an improved approach and\nsoftware implementation for genome-wide fast microrna precursor scans. BMC Bioinformatics 8, 341 (2007).\ndoi:10.1186/1471-2105-8-341\n6. Ng, K.L.S., Mishra, S.K.: De novo svm classiﬁcation of precursor micrornas from genomic pseudo hairpins using\nglobal and intrinsic folding measures. BMC Bioinformatics 23(11), 1321–1330 (2007).\ndoi:10.1186/1471-2105-8-341\n7. Batuwita, R., Palade, V.: micropred: eﬀective classiﬁcation of pre-mirnas for human mirna gene prediction.\nBMC Bioinformatics 25(8), 989–995 (2009). doi:10.1093/bioinformatics/btp107\n8. Pasaila, D., Sucial, A., Mohorianu, I., Pantiru, S.T., Ciortuz, L.: Mirna recognition with the yasmir system: The\nquest for further improvements. Adv Exp Med Biol. 696, 17–25 (2011). doi:10.1007/978-1-4419-7046-6 2\n9. Ding, J., Zhou, S., Guan, J.: Mirensvm: towards better prediction of microrna precursors using an ensemble\nsvm classiﬁer with multi loop features. BMC Bioinformatics 14(11), 11–11 (2010).\ndoi:10.1186/1471-2105-11-S11-S11\n10. Wu, Y., Wei, B., Liu, H., Li, T., Rayner, S.: Mirpara: a svm-based software tool for prediction of most probable\nmicrorna coding regions in genome scale sequences. BMC Bioinformatics 12, 107 (2011).\ndoi:10.1186/1471-2105-12-107\n11. Kleftogiannis, D., Theoﬁlatos, K., Likothanassis, S., Mavroudi, S.: Yamipred: A novel evolutionary method for\npredicting pre-mirnas and selecting relevant features. IEEE ACM Transactions on Computational Biology and\nBioinformatics 12(5), 1183–1192 (2015). doi:10.1109/TCBB.2014.2388227\n12. Hsieh, C.H., Chang, D.T.H., Hsueh, C.H., Wu, C.Y., Oyang, Y.J.: Predicting microrna precursors with a\ngeneralized gaussian components based density estimation algorithm. BMC Bioinformatics 11, 1–52 (2010).\ndoi:10.1186/1471-2105-11-S1-S52\n13. Wang, Y., Chen, X., Jiang, W., Li, L., Li, W., Yang, L., Liao, M., Lian, B., Lv, Y., Wang, S., Wang, S., Li, X.:\nPredicting human microrna precursors based on an optimized feature subset generated by ga-svm. Genomics\n98(2), 73–78 (2011)\nThomas et al.\nPage 10 of 12\n14. Xiao, J., Tang, X., Li, Y., Fang, Z., Ma, D., He, Y., Li, M.: Identiﬁcation of microrna precursors based on\nrandom forest with network-level representation method of stem-loop structure. BMC Bioinformatics 12:165\n(2011). doi:10.1186/1471-2105-12-165\n15. Rahman, M.E., Islam, R., Islam, S., Mondal, S.I., Amin, M.R.l.: Mirann: A reliable approach for improved\nclassiﬁcation of precursor microrna using artiﬁcial neural network model. Genomics 99, 189–194 (2012)\n16. Thomas, J., Thomas, S., Sael, L.: DP-miRNA: an improved prediction of precursor microRNA using deep\nlearning mode. In: IEEE International Conference on Big Data and Smart Computing (IEEE BigComp 2017),\npp. 96–99 (2017). http://conf2017.bigcomputing.org/\n17. Hinton, G.E.: Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation 14(8),\n1771–1800 (2002)\n18. Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A.R., Jaitly, N., Vanhoucke, V., Nguyen, P., Sainath, T.,\nKingsbury, B.: Deep neural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine, 82–97 (2012). doi:10.1.1.248.3619\n19. Zhong, Y., Xuan, P., Han, K., Zhang, W., Li, J.: Improved pre-mirna classiﬁcation by reducing the eﬀect of\nclass imbalance. BioMed Research International 2015, 1–12 (2015). doi:10.1155/2015/960108\n20. Yen, S.J., Lee, Y.S.: Cluster-based under-sampling approaches for imbalanced data distributions. Expert\nSystems with Applications 36(3), 5718–5727 (2009). doi:10.1016/j.eswa.2008.06.108\nFigures\nFigure 2 Best performing CNN architecture.\nFigure 3 Best performing DBN architecture.\nTables\nAdditional Files\nFull list of pre-miRNA features\nFull list of pre-miRNA features used as inputs to deep belief network is listed.\nThomas et al.\nPage 11 of 12\nTable 1 Sample table title. This is where the description of the table should go.\nB1\nB2\nB3\nA1\n0.1\n0.2\n0.3\nA2\n...\n..\n.\nA3\n..\n.\n.\nTable 2 My caption\nCategory\nFeatures\nSequence composition properties\nfeatures related to the frequency of two and three adjacent nu-\ncleotide, aggregate dinucleotide frequency in the sequence, such\nas Dinucleotide pair frequency, Trinucleotide frequency, aggre-\ngate dinucleotide frequency\nSecondary structures\nthermodynamic stability proﬁles of pre-miRNAs\nStem and loop\ndiversity, frequency, entropy-related properties, enthalpy-related\nproperties of the structure, hairpin length, loop length, consecu-\ntive base-pair, ratio of loop length to hairpin length of pre-miRNA\nsecondary structure\nEnergy characteristics\nminimal free energy of the secondary structure, overall free en-\nergy NEFE, combined energy features, the energy required for\ndissolving the secondary structure\nStatistical measures\nZ-score of the folding measures zG, zQ, zSP, zP, zD\nTable 3 Tested CNN architectures\nModel Name\nNumber\nof\nLayers\nDescription of architecture\nType 1\n5\nLayer 1: Input layer, Layer 2: Convolution with no stride, Layer\n3 Pooling layer with stride, Layer 4 Fully connected to output\nlayer, Layer 5 classiﬁcation layer\nType 2\n5\nLayer 1: Input layer, Layer 2: Convolution with stride, Layer 3\nFully connected Layer , Layer 4 Fully connected to output layer,\nLayer 5 classiﬁcation layer\nType 3\n6\nLayer 1: Input layer, Layer 2: Convolution with no stride, Layer\n3: Convolution with no stride, Layer 4 Pooling layer with stride,\nLayer 5 Fully connected to output layer, Layer 6 classiﬁcation\nlayer\nType 4\n7\nLayer 1: Input layer, Layer 2: Convolution with no stride, Layer 3:\nConvolution with no stride, Layer 4: Convolution with no stride,\nLayer 5 Pooling layer with stride, Layer 6 Fully connected to\noutput layer, Layer 7 classiﬁcation layer\nTable 4 My caption\nHyper-parameter\nRange\nFilter size\n5 to 24\nNumber of ﬁlters\n5 to 20\nStride\n0 to 24\nPooling\nMax pooling 0 to 9\nDropout\n0 to 0.4\nNumber of convolution layers\n1 to 3\nTable 5 Two selected CNN model description.\nModel Type\nDescription\nof model\nPerformance Measure\nType 2\nLayer 1\nInput Sequence\nSE=1\nLayer 2\nConvolution Layer, Window size= 18,\nStride = 4,\nNumber of ﬁlters =20.\nLayer 3\nFully connected Layer (90 neurons)\nPrecision= 0.985\nLayer 4\nFully connected Layer (2 neurons)\nAcc=0.993\nLayer 5\nClassiﬁcation Layer\nType 3\nLayer 1\nInput Sequence\nSE=1\nLayer 2\nConvolution\nLayer\n(window\nsize=12,\nstride=1, Num. of ﬁlters=12)\nSP=0.990\nLayer 3\nConvolution\nLayer\n(window\nsize=6,\nstride=1, Num. of ﬁlters=12)\nPrecision=0.990\nLayer 4\nPooling Layer (max pooling, stride=4)\nAcc=0.995\nLayer 5\nFully Layer (2 neurons)\nLayer 6\nClassiﬁcation Layer\nThomas et al.\nPage 12 of 12\nTable 6 Comparison with existing computational intelligence techniques\nMethod\nSensitivity\nSpeciﬁcity\nAccuracy\nNaive Bayes\n0.943\n0.796\n0.914\nK nearest neighbors\n0.970\n0.657\n0.908\nRandom Forest\n0.979\n0.765\n0.937\nmiRNN\n0.963\n0.705\n0.917\nYamiPred\n0.937\n0.912\n0.932\nDeep RBM model [58 features]\n0.973\n0.942\n0.968\nDeep RBM model [20 features]\n0.995\n0.982\n0.990\nCNN model 1 (Type 2)\n1.00\n0.985\n0.993\nCNN model 2 (Type 3)\n1.00\n0.990\n0.995\nTable 7 Features for miRNA Prediction\nFeature\nNumber\nDescription\nXY,\nwhere\nX,Y∈\n{A,C,G,U}\n16\nDinucleotide pairs frequency\nXYZ, where X,Y,Z∈\n{A,C,G,U}\n64\nTrinucleotide pairs frequency\nA+U%\n1\nAggregate dinucleotide frequency (bases which are either A or U)\nG+C%\n1\nAggregate dinucleotide frequency (bases which are either G or C)\nL\n1\nStructure length\nFreq\n1\nStructural frequency property\ndP\n1\nAdjusted base pairing propensity given as total bases/L\ndG\n1\nAdjusted Minimum Free Energy of folding given as dG = MFE/L\ndD\n1\nAdjusted base pair distance\ndQ\n1\nAdjusted shannon entropy\ndF\n1\nCompactness of the tree-graph representation of the sequence\nMFEI1\n1\nMFEI1 = dG/%(C+G)\nMFEI2\n1\nMFEI2 = dG/number of stems\nMFEI3\n1\nMFEI3 = dG/number of loops\nMFEI4\n1\nMFEI4 = dG/total bases\nMFEI5\n1\nMFEI5= dG/%(A+U) ratio\ndS\n1\nStructure entropy\ndS/L\n1\nNormalized structure entropy\ndH\n1\nStructure Enthalpy\ndH/L\n1\nNormalized structure enthalpy\nTm\n1\nMelting Temperature\nTm/L\n1\nNormalized melting temperature\nBP/X, where X ∈\n{GC,GU,AU}\n3\nRatio of total bases to respective base pairs\nG/C\n1\nNumber of G,C bases\nAvg BP Stem\n1\nAverage number of base pairs in the stem region\n|A−U|/L,|G−C|/L,\n|G−U|/L\n3\n|X−Y| is the number of (X −Y) base pairs in the secondary\nstructure\n|A−U|%/n stems,\n|G −C|%/n stems,\nand\n|G\n−\nU|%/n stems\n3\nAverage number of base pairs in the stem region\nzP,zG,zD,zQ,zSP\n5\nstatistical Z-score of the folding measures\ndPs\n1\nPositional Entropy which estimates the structural volatility of the\nsecondary structure\nEAFE\n1\nNormalized Ensemble Free Energy\nCE/L\n1\nCentroid energy normalized by length\nDiﬀ\n1\nDiﬀ=| MFE-EFE|/L where, EFE is the ensemble free energy\nIH\n1\nHairpin length dangling ends\nIL\n1\nLoop length\nIC\n1\nMaximum consecutive base-pairs\n%L\n1\nRatio of loop length to hairpin length\n",
  "categories": [
    "cs.LG",
    "q-bio.GN"
  ],
  "published": "2017-10-17",
  "updated": "2017-10-17"
}