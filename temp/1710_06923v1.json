{
  "id": "http://arxiv.org/abs/1710.06923v1",
  "title": "Adapting general-purpose speech recognition engine output for domain-specific natural language question answering",
  "authors": [
    "C. Anantaram",
    "Sunil Kumar Kopparapu"
  ],
  "abstract": "Speech-based natural language question-answering interfaces to enterprise\nsystems are gaining a lot of attention. General-purpose speech engines can be\nintegrated with NLP systems to provide such interfaces. Usually,\ngeneral-purpose speech engines are trained on large `general' corpus. However,\nwhen such engines are used for specific domains, they may not recognize\ndomain-specific words well, and may produce erroneous output. Further, the\naccent and the environmental conditions in which the speaker speaks a sentence\nmay induce the speech engine to inaccurately recognize certain words. The\nsubsequent natural language question-answering does not produce the requisite\nresults as the question does not accurately represent what the speaker\nintended. Thus, the speech engine's output may need to be adapted for a domain\nbefore further natural language processing is carried out. We present two\nmechanisms for such an adaptation, one based on evolutionary development and\nthe other based on machine learning, and show how we can repair the\nspeech-output to make the subsequent natural language question-answering\nbetter.",
  "text": "Noname manuscript No.\n(will be inserted by the editor)\nAdapting general-purpose speech recognition engine\noutput for domain-speciﬁc natural language question\nanswering\nC. Anantaram · Sunil Kumar Kopparapu\nthe date of receipt and acceptance should be inserted later\nAbstract Speech-based natural language question-answering interfaces to enter-\nprise systems are gaining a lot of attention. General-purpose speech engines can be\nintegrated with NLP systems to provide such interfaces. Usually, general-purpose\nspeech engines are trained on large ‘general’ corpus. However, when such engines\nare used for speciﬁc domains, they may not recognize domain-speciﬁc words well,\nand may produce erroneous output. Further, the accent and the environmental\nconditions in which the speaker speaks a sentence may induce the speech en-\ngine to inaccurately recognize certain words. The subsequent natural language\nquestion-answering does not produce the requisite results as the question does not\naccurately represent what the speaker intended. Thus, the speech engine’s output\nmay need to be adapted for a domain before further natural language process-\ning is carried out. We present two mechanisms for such an adaptation, one based\non evolutionary development and the other based on machine learning, and show\nhow we can repair the speech-output to make the subsequent natural language\nquestion-answering better.\n1 Introduction\nSpeech-enabled natural-language question-answering interfaces to enterprise ap-\nplication systems, such as Incident-logging systems, Customer-support systems,\nMarketing-opportunities systems, Sales data systems etc., are designed to allow\nend-users to speak-out the problems/questions that they encounter and get au-\ntomatic responses. The process of converting human spoken speech into text is\nperformed by an Automatic Speech Recognition (ASR) engine. While functional\nexamples of ASR with enterprise systems can be seen in day-to-day use, most of\nC. Anantaram\nTCS Innovation Labs - Delhi, ASF Insignia, Gwal Pahari, Gurgaon, India\nE-mail: c.anantaram@tcs.com\nSunil Kumar Kopparapu\nTCS\nInnovation\nLabs\n-\nMumbai,\nYantra\nPark,\nThane\n(West)\nE-mail:\nsunilku-\nmar.kopparapu@tcs.com\narXiv:1710.06923v1  [cs.CL]  12 Oct 2017\n2\nAnantaram, Sunil\nthese work under constraints of a limited domain, and/or use of additional domain-\nspeciﬁc cues to enhance the speech-to-text conversion process. Prior speech-and-\nnatural language interfaces for such purposes have been rather restricted to either\nInteractive Voice Recognition (IVR) technology, or have focused on building a\nvery specialized speech engine with domain speciﬁc terminology that recognizes\nkey-words in that domain through an extensively customized language model, and\ntrigger speciﬁc tasks in the enterprise application system. This makes the interface\nextremely specialized, rather cumbersome and non-adaptable for other domains.\nFurther, every time a new enterprise application requires a speech and natural\nlanguage interface, one has to redevelop the entire interface again.\nAn alternative to domain-speciﬁc speech recognition engines has been to re-\npurpose general-purpose speech recognition engines, such as Google Speech API,\nIBM Watson Speech to text API which can be used across domains with natu-\nral language question answering systems. Such general-purpose automatic speech\nengines (gp-ASR) are deep trained on very large general corpus using deep neural\nnetwork (DNN) techniques. The deep learnt\nacoustic and language models en-\nhance the performance of a ASR. However, this comes with its own limitations.\nFor freely spoken natural language sentences, the typical recognition accuracy\nachievable even for state-of-the-art speech recognition systems have been observed\nto be about 60% to 90% in real-world environments (Lee et al., 2010). The recog-\nnition is worse if we consider factors such as domain-speciﬁc words, environmental\nnoise, variations in accent, poor ability to express on the part of the user, or in-\nadequate speech and language resources from the domain to train such speech\nrecognition systems. The subsequent natural language processing, such as that in\na question answering system, of such erroneously and partially recognized text\nbecomes rather problematic, as the domain terms may be inaccurately recognized\nor linguistic errors may creep into the sentence. It is, hence, important to improve\nthe accuracy of the ASR output text.\nIn this paper, we focus on the issues of using a readily available gp-ASR\nand adapting its output for domain-speciﬁc natural language question answering\n(Anantaram et al., 2015a). We present two mechanisms for adaptation, namely\n(a) an evolutionary development based artiﬁcial development mechanism of adap-\ntation (Evo-Devo), where we consider the output of ASR as a biological entity\nthat needs to adapt itself to the environment (in this case the enterprise do-\nmain) through a mechanism of repair and development of its genes and\n(b) a machine learning based mechanism where we examine the closest set of\nmatches with trained examples and the number of adaptive transformations\nthat the ASR output needs to undergo in order to be categorized as an accept-\nable natural language input for question-answering.\nWe present the results of these two adaptation and gauge the usefulness of each\nmechanism. The rest of the paper is organized as follows, in Section 2 we brieﬂy\ndescribe the work done in this area which motivates our contribution. The main\ncontribution of our work is captured in Section 3 and we show the performance of\nour approach through experiments in Section 4. We conclude in Section 5.\nAdapting general-purpose speech recognition engine for QA\n3\n2 Related Work\nMost work on ASR error detection and correction has focused on using conﬁ-\ndence measures, generally called the log-likelihood score, provided by the speech\nrecognition engine; the text with lower conﬁdence is assumed to be incorrect and\nsubjected to correction. Such conﬁdence based methods are useful only when we\nhave access to the internals of a speech recognition engine built for a speciﬁc do-\nmain. As mentioned earlier, use of domain-speciﬁc engine requires one to rebuild\nthe interface every time the domain is updated, or a new domain is introduced.\nAs mentioned earlier, our focus is to avoid rebuilding the interface each time the\ndomain changes by\nusing an existing ASR. As such\nour method is speciﬁcally\na post-ASR system. A post-ASR system provides greater ﬂexibility in terms of\nabsorbing\ndomain variations and adapting the output of ASR in ways that are\nnot possible during training a domain-speciﬁc ASR system (Ringger and Allen,\n1996).\nNote that an\nerroneous ASR output text will lead to an equally (or more)\nerroneous interpretation by the natural language question-answering system,\nresulting in a poor performance of the overall QA system\nMachine learning classiﬁers have been used in the past for the purpose of com-\nbining features to calculate a conﬁdence score for error detection. Non-linguistic\nand syntactic knowledge for detection of errors in ASR output, using a support\nvector machine to combine non-linguistic features was proposed in (Shi, 2008) and\nNaive Bayes classiﬁer to combine conﬁdence scores at a word and utterance level,\nand diﬀerential scores of the alternative hypotheses was used in (Zhou et al., 2005)\nBoth (Shi, 2008) and (Zhou et al., 2005) rely on the availability of conﬁdence scores\noutput by the ASR engine. A syllable-based noisy channel model combined with\nhigher level semantic knowledge for post recognition error correction, independent\nof the internal conﬁdence measures of the ASR engine is described in (Jeong et al.,\n2004). In (L´opez-C´ozar and Callejas, 2008) the authors propose a method to cor-\nrect errors in spoken dialogue systems. They consider several contexts to correct\nthe speech recognition output including learning a threshold during training to\ndecide when the correction must be carried out in the context of a dialogue sys-\ntem. They however use the conﬁdence scores associated with the output text to\ndo the correction or not. The correction is carried using syntactic-semantic and\nlexical models to decide whether a recognition result is correct.\nIn (Bassil and Semaan, 2012) the authors proposes a method to detect and\ncorrect ASR output based on Microsoft N-Gram dataset. They use a context-\nsensitive error correction algorithm for selecting the best candidate for correction\nusing the Microsoft N-Gram dataset which contains real-world data and word\nsequences extracted from the web which can mimic a comprehensive dictionary of\nwords having a large and all-inclusive vocabulary.\nIn (Jun and Lei, 2011) the authors assume the availability of pronunciation\nprimitive characters as the output of the ASR engine and then use domain-speciﬁc\nnamed entities to establish the context, leading to the correction of the speech\nrecognition output. The patent (Amento et al., 2007) proposes a manual correc-\ntion of the ASR output transcripts by providing visual display suggesting the\ncorrectness of the text output by ASR. Similarly, (Harwath et al., 2014) propose\na re-ranking and classiﬁcation strategy based on logistic regression model to esti-\n4\nAnantaram, Sunil\nmate the probability for choosing word alternates to display to the user in their\nframework of a tap-to-correct interface.\nOur proposed machine learning based system is along the lines of (Jeong et al.,\n2004) but with diﬀerences: (a) while they use a single feature (syllable count)\nfor training, we propose the use of multiple features for training the Naive Bayes\nclassiﬁer, and (b) we do not perform any manual alignment between the ASR and\nreference text – this is done using an edit distance based technique for sentence\nalignment. Except for (Jeong et al., 2004) all reported work in this area make\nuse of features from the internals of the ASR engine for ASR text output error\ndetection.\nWe assume the use of a gp-ASR in the rest of the paper. Though we use\nexamples of natural language sentences in the form of queries or questions, it\nshould be noted that the description is applicable to any conversational natural\nlanguage sentence.\n3 Domain adaptation of ASR output\n3.1 Errors in ASR output\nIn this paper we focus on question answering interfaces to enterprise systems,\nthough our discussion is valid for any kind of natural language processing sen-\ntences that are not necessarily a query. For example, suppose we have a retail-sales\nmanagement system domain, then end-users would be able to query the system\nthrough spoken natural language questions (S) such as\nS =\n(/What is the total sales of miscellaneous store\nretailers from year two thousand ten to year two\nthousand fifteen?/\nA perfect ASR would take S as the input and produce (T), namely,\nT =\n(what is the total sales of miscellaneous store\nretailers from year two thousand ten to year two\nthousand fifteen\nWe consider the situation where a ASR takes such a sentence (S) spoken by a\nperson as input, and outputs an inaccurately recognized text (T ′) sentence. In our\nexperiments, when the above question was spoken by a person and processed by\na popular ASR engine such as Google Speech API, the output text sentence was\n(T ′)\nT ′ =\n\u001awhat is the total sales of miscellaneous storyteller\nfrom the year two thousand ten to two thousand fifteen\nNamely\nS −→ASR −→T ′\nIt should be noted that an inaccurate output by the ASR engine maybe the\nresult of various factors such as background noise, accent of the person speaking\nthe sentence, the speed at which he or she is speaking the sentence, domain-\nspeciﬁc words that are not part of popular vocabulary etc. The subsequent natural\nAdapting general-purpose speech recognition engine for QA\n5\nlanguage question answering system cannot answer the above output sentence\nfrom its retail sales data. Thus the question we tackle here is – how do we adapt\nor repair the sentence (T ′) back to the original sentence (T) as intended by the\nspeaker. Namely\nT ′ −→adaptation, repair −→T\nWe present two mechanisms for adaptation or repair of the ASR output, namely\nT ′ −→T, in this paper: (a) an evolutionary development based artiﬁcial develop-\nment mechanism, and (b) a machine-learning mechanism.\n3.2 Evo-Devo based Artiﬁcial Development mechanism of adaption\nOur mechanism is motivated by Evolutionary Development (Evo-Devo) processes\nin biology (Harding and Banzhaf, 2008; Anantaram et al., 2015b; Tufte, 2008)\nto help adapt/repair the overall content accuracy of an ASR output (T ′) for a\ndomain. We give a very brief overview of Evo-Devo process in biological organisms\nand discuss how this motivates our mechanism. In a biological organism, evo-\ndevo processes are activated when a new biological cell needs to be formed or an\ninjured cell needs to be repaired/replaced. During such cell formation or repair,\nthe basic genetic structure consisting of the genes of the organism are replicated\ninto the cell – the resultant set of ’genes in the cell’ is called the genotype of the\ncell. Once this is done, the genotype of the cell is then specialized through various\ndevelopmental processes to form the appropriate cell for the speciﬁc purpose that\nthe cell is intended for, in order to factor-in the traits of the organism – called the\nphenotype of the cell. For example, if a person has blue-eyes then a blue-eye cell is\nproduced, or if a person has brown skin then a brown-skin cell is produced. During\nthis process, environmental inﬂuence may also play a role in the cell’s development\nand such inﬂuences are factored into the genotype-phenotype development process.\nThe ﬁeld of Evo-Devo has inﬂuenced the ﬁeld of Artiﬁcial Intelligence (AI) and a\nnew sub-ﬁeld called Artiﬁcial Development (Art-Dev) has been created that tries\nto apply Evo-Devo principles to ﬁnd elegant solutions to adaptation and repair\nproblems in AI.\nWe take inspiration from the Evo-Devo biological process and suitably tailor\nit to our research problem of repairing the ASR output (T ′). In our approach\nwe consider the erroneous ASR output text as the input for our method and\ntreat it as an ’injured biological cell’. We repair that ’injured cell’ through the\ndevelopment of the partial gene present in the input sentence with respect to\nthe genes present in the domain. We assume that we have been provided with\nthe domain ontology describing the terms and relationships of the domain. In\nour framework, we consider the domain ontology as the true ’genetic knowledge’\nof that ’biological organism’. In such a scenario, the ’genetic repair’ becomes a\nsequence of match-and-replace of words in the sentence with appropriate domain\nontology terms and relationships. Once this is done, the ’genotype-to-phenotype\nrepair’ is the repair of linguistic errors in the sentence after the ’genetic repair’.\nThe following sub-section describes our method in detail.\n6\nAnantaram, Sunil\n3.2.1 Repair method\nWe assume that all the instances of the objects in the domain are stored in a\ndatabase associated with the enterprise system, and can be expressed in relational\nform (such as [a R c]), for example ['INDUSTRY', 'has', 'PEAK SALES']. A rela-\ntional database will store it as a set of tables and we treat the data in the database\nas static facts of the domain. The ontology of the domain can then be generated\nfrom this database. We assume that the data schema and the actual data in the\nenterprise application forms a part of the domain terms and their relationships\nin the ontology. This identiﬁes the main concepts of the domain with a <subject-\npredicate-object> structure for each of the concepts. The ontology thus generated\ndescribes the relations between domain terms, for example ['SALES', 'has code',\n'NAICS CODE'] or ['OPTICAL GOODS', 'sales 2009', '8767 million'] and thus can\nbe expressed using OWL schema as <s-p-o > structure. Each <s-p-o> entry forms\nthe genes of the domain.\nWe start by ﬁnding matches between domain ontology terms and words that\nappear in the input sentence. Some words of the input sentence will match domain\nontology terms exactly. The corresponding domain ontology entry consisting of\nsubject-predicate-object triple is put into a candidate set. Next, other words in\nthe input sentence that are not exact matches of domain ontology terms but have\na ’closeness’ match with terms in the ontology are considered. This ’closeness’\nmatch is performed through a mix of phonetic match combined with Levenshtein\ndistance match. The terms that match help identify the corresponding domain\nontology entry (with its subject-predicate-object triple) is added to the candidate\nset. This set of candidate genes is a shortlist of the ’genes’ of the domain that is\nprobably referred to in the input sentence.\nNext, our mechanism evaluates the ‘ﬁttest’ domain ontology entry from the\ncandidate set to replace the partial gene in the sentence. A ﬁtness function is\ndeﬁned and evaluated for all the candidate genes short-listed. This is done for\nall words / phrases that appear in the input sentence except the noise words.\nThe ﬁttest genes replace the injured genes of the input sentence. The set of all\ngenes in the sentence forms the genotypes of the sentence. This is the ﬁrst-stage of\nadaptation.\nOnce the genotypes are identiﬁed, we grow them into phenotypes to remove\nthe grammatical and linguistic errors in the sentence. To do this, we ﬁnd parts\nof the sentence that is output by the ﬁrst-stage of adaptation (the gene-level\nrepair) and that violate well-known grammatical/ linguistic rules. The parts that\nviolate are repaired through linguistic rules. This is the second stage of adaptation/\nrepair. This process of artiﬁcial rejuvenation improves the accuracy of the sentence,\nwhich can then be processed by a natural language question answering system\n(Bhat et al., 2007). Thus, this bio-inspired novel procedure helps adapt/repair\nthe erroneously recognized text output by a speech recognition engine, in order to\nmake the output text suitable for deeper natural language processing. The detailed\nsteps are described below.\nStep 1: Genes Identiﬁcation:\nWe match the sub-parts (or sub-strings) of the\nASR-output sentence with the genes of the domain. The match may be partial\ndue to the error present in the sentence. The genes in the domain that match\nthe closest, evaluated by a phonetic and/or syntactic match between the ontology\nAdapting general-purpose speech recognition engine for QA\n7\nThe ﬁtness function F takes as input the asr word, the candidate gene, the Levenshtein\ndistance weight ( L), the Phonetic algorithm weight (wP ) and Threshold (T). Fitness function\nF then tries to ﬁnd the closeness of the match between asr word and the candidate gene. To\ndo that, the function calculates two scores:\n1. algoScore: is an aggregated score of the similarity of the gene with the asr word by various\nphonetic algorithms; and\n2. editScore: is the Levenshtein distance between the asr word and the gene.\nThe ﬁtness function then calculates the ﬁnal ﬁtness of the gene using the formula:\nfinalScore = wP ∗algoScore +  L ∗(1 −editScore).\n(1)\nIf the finalScore is greater than a given threshold T the asr word is replaced by the candidate\ngene, otherwise the asr word is kept as it is, namely,\nif(finalScore > T)\nasr word ←gene\nFig. 1 Fitness Function.\nentity and the selected sub-part, are picked up and form the candidates set for the\ninput sentence. For example, let the actual sentence that is spoken by an end-user\nbe \"which industry has the peak sales in nineteen ninety seven?\". In one of\nour experiments, when Google Speech API was used as the ASR engine for the\nabove sentence spoken by a user, then the speech engine’s output sentence was\n\"which industry has the pixel in nineteen ninety seven?\". This ASR output\nis erroneous (probably due to background noise or the accent of the speaker) and\nneeds repair/ adaptation for the domain.\nAs a ﬁrst step, the ASR output sentence is parsed and the Nouns and Verbs\nare identiﬁed from part-of-speech (POS) tags. Syntactic parsing also helps get\n<subject-verb-object> relations to help identify a potential set of <s-p-o> genes\nfrom the ontology. For each of the Nouns and Verbs and other syntactic relations,\nthe partially matching genes with respect to the domain ontology are identiﬁed; for\nthis particular sentence the partially matching genes are, \"industry\" and \"pixel\".\nThis leads us to identify the probable set of genes in the domain ontology that\nare most likely a possible match: 'INDUSTRY', 'has', 'PEAK SALES'. The set of all\nsuch probable genes need to be evaluated and developed further.\nStep 2: Developing the genes to identify the genotypes: Once the basic candi-\ndate genes are identiﬁed, we evaluate the genes to ﬁnd the best ﬁt for the situation\non hand with evolution and development of the genes, and then test a ﬁtness func-\ntion (see Fig. 3.2 and select the most probable gene that survives. This gives us\nthe set of genotypes that will form the correct ASR sentence. For example, the ba-\nsic genes \"INDUSTRY\" and \"PIXEL\" are used to match the substring \"industry has\nthe pixel\" with the gene \"INDUSTRY', 'has field', 'PEAK SALES’. This is done\nthrough a matching and ﬁtness function that would identify the most appropriate\ngene of the domain. We use a phonetic match function like Soundex, Metaphone,\nDouble-metaphone (Naumann, 2015) to match \"pixel\" with \"PEAK SALES\" or an\nedit-distance match function like Levenshtein distance (Naumann, 2015) to ﬁnd\nthe closeness of the match. In a large domain there may be many such probable\ncandidates. In such a case, a ﬁtness function is used to decide which of the matches\nare most suitable. The genes identiﬁed are now collated together to repair the input\n8\nAnantaram, Sunil\nsentence. This is done by replacing parts of the input sentence by the genes identi-\nﬁed in the previous step. In the above example the ASR sentence, \"Which industry\nhas the pixel in nineteen ninety seven?\" would be adapted/repaired to \"Which\nindustry has the peak sales in nineteen ninety seven?\".\nStep 3: Developing Genotypes to Phenotype of sentence: The repaired sen-\ntence may need further linguistic adaptation/ repair to remove the remaining\nerrors in the sentence. To achieve this, the repaired ASR sentence is re-parsed and\nthe POS tags are evaluated to ﬁnd any linguistic inconsistencies, and the incon-\nsistencies are then removed. For example, we may notice that there is a WP tag\nin a sentence that refers to a Wh-Pronoun, but a WDT tag is missing in the sen-\ntence that should provide the Determiner for the Wh-pronoun. Using such clues\nwe can look for phonetically matching words in the sentence that could possibly\nmatch with a Determiner and repair the sentence. Linguistic repairs such as these\nform the genotype to phenotype repair/ adaptation of the sentence. The repaired\nsentence can then be processed for question-answering.\nWe use open source tools like LanguageTool to correct grammatical errors. In\naddition we have added some domain speciﬁc grammar rules. As we understand,\nthe LanguageTool has 54 grammar rules, 5 style rules and 4 built-in Python rules\nfor grammar check and correction. Further we have added some 10 domain speciﬁc\nrules to our linguistic repair function. Our grammar rules can be extended or\nmodiﬁed for any domain.\n3.2.2 Algorithm of the Evo-Devo process\nThe algorithm has two main functions: ONTOLOGY BASED REPAIR (that en-\ncode Steps 1 and 2 described above) and LINGUISTIC REPAIR (encoding Step 3\nabove). The input sentence is POS tagged and the nouns and verbs are considered.\nA sliding window allows the algorithm to consider single words or multiple words\nin a domain term.\nLet S = w1, w2, w3, · · · , wn be the set of words in the ASR-output(asr out).\nLet D = dt1, dt2, dt3, · · · , dtm be the domain-ontology-terms. These terms may be\nconsidered as candidate genes that can possibly replace the ASR output (asr out)\nwords that may be erroneously recognized. A sliding window of length l consisting\nof words wi, · · · , wi+l−1 is considered for matching with domain-ontology-terms.\nThe length l may vary from 1 to p, where p may be decided based on the environ-\nmental information. For example, if the domain under consideration has ﬁnancial\nterms then p may be ﬁve words, while for a domain pertaining to car parts, p may\nbe two words. The part match functionality described below evaluates a cost func-\ntion, say C({wi, · · · , wi+l−1}, dtk) such that minimizing C({wi, · · · , wi+l−1}, dtk)\nwould result in dt∗which may be a possible candidate to replace {wi, · · · , wi+l−1},\nnamely,\ndt∗= min\nG C({wi, · · · , wi+l−1}, dtk)\nThe cost function\nC({wi, · · · , wi+l−1}, dtk) =\nb1 ∗soundex(φ{wi, · · · , wi+l−1}, φ{dtk}) +\nb2 ∗metaphone(φ{wi, · · · , wi+l−1}, φ{dtk}) +\nAdapting general-purpose speech recognition engine for QA\n9\nb3 ∗(edit distance(φ{wi, · · · , wi+l−1}, φ{dtk}) +\nb4 ∗(number of syllables(φ{wi, · · · , wi+l−1}) −number of syllables(φ{dtk}) +\nb5 ∗(word2vec(φ{wi, · · · , wi+l−1}) −word2vec(φ{dtk})2\nwhere weights, b1 + b2 + b3 + b4 + b5 = 1 and φ{k} represents each-element-in\nthe set k. If the value of the cost function C({wi, · · · , wi+l−1}, dtk) is greater than\nthe pre-determined threshold T then {wi, · · · , wi+l−1} may be replaced with the\ndt∗, otherwise the {wi, · · · , wi+l−1} is maintained as it is. The broad algorithm of\nEvolutionary Development mechanism is shown in Algorithm 1.\nAlgorithm 1 Evo-Devo Mechanism\nINPUT: ASR output sentence, sentence; domain ontology\nOUTPUT: Repaired sentence, repaired sentence\nstart\n1: // parse the input sentence\n2: parsed sentence ←POS tag(sentence)\n3: // repair process starts - do genetic repair and ﬁnd the genotypes\n4: part repaired sentence ←ontology based repair(parsed sentence)\n5: // grow the genotypes into phenotypes\n6: repaired sentence ←linguistic repair(parsed sentence,part repaired sentence)\nend\n7: function ontology based repair(parsed sentence)\n8:\nnouns verbs ←ﬁnd(parsed sentence, noun verb POStoken)\n9: // for each noun verb entry in nouns verbs do next 4 steps\n10: // ﬁnd partially matching genes: match nouns and verbs with entries in domain ontology\nwith phonetic algorithms and Levenshtein distance match\n11:\nconcepts referred ←part match(noun verb entry, domain ontology)\n12: // ﬁnd genes: get the subject-predicate-object for concepts\n13:\ncandidate genes ←add(spo entry, concepts referred)\n14: // simulate the development process of the genes - ﬁnd the ﬁttest gene from candidate\ngenes\n15:\nﬁt gene ←ﬁttest(candidate genes, POS token)\n16: // add ﬁttest gene into set of genotypes\n17:\ngenotypes ←add(ﬁt gene)\n18: // replace partially identiﬁed genes in input with genotypes identiﬁed\n19:\nrepaired sentence ←substitute(parsed sentence, nouns verbs, genotypes)\n20:\nreturn repaired sentence\n21: end function\n22: function linguistic repair(part repaired sentence)\n23:\nother POStags ←ﬁnd(part repaired sentence, remaining POStokens)\n24: // ﬁnd POS tags without linguistic completion\n25:\nling err ←linguistic check( other POStags, part repaired sentence)\n26: // ﬁnd candidate words for linguistic error\n27:\ncandidate words ←add(part repaired sentence, ling err)\n28: // ﬁnd the closest semantic match for error words\n29:\nﬁt word ←ﬁttest word(candidate words, ling err)\n30: // add ﬁttest word into repaired sentence\n31:\nﬁt words ←add(candidate word, ﬁt word)\n32: // create the repaired sentence\n33:\nrepaired sentence ←replace(part repaired sentence, ﬁt words, other POStags)\n34:\nreturn repaired sentence\n35: end function\n10\nAnantaram, Sunil\n3.2.3 Detailed example of our method\nLet us assume that we have the domain of retail sales data described in an ontology\nof <subject-predicate-object> structure as shown in Table 1.\nSubject\nPredicate\nObject\nINDUSTRY\nBUSINESS\nCAR DEALERS\nINDUSTRY\nBUSINESS\nOPTICAL GOODS\nCAR DEALERS\nSALES 2013\n737640 million\nCAR DEALERS\nSALES 2011\n610747 million\nCAR DEALERS\nSALES 2009\n486896 million\nOPTICAL GOODS\nSALES 2013\n10364 million\nOPTICAL GOODS\nSALES 2011\n10056 million\nOPTICAL GOODS\nSALES 2009\n8767 million\nTable 1 Ontology Structure.\nNow, let us consider that a user speaks the following sentence to Google\nNow speech engine: \"Which business has more sales in 2013: Car dealers or\noptical goods?\". In our experiment the Google Now speech engine produced the\nASR output sentence as \"which business has more sales in 2013 car dealers\nfor optical quotes\". The recognized ASR sentence has errors. In order to make\nthis ASR sentence more accurate, we input this sentence into the Evo-Devo mech-\nanism and run the process:\n– Genes Identiﬁcation (Step 1): We parse the ASR sentence and identify the\nparts-of-speech in it as: which/WDT, business/NN, has/VBZ, more/JJR, sales/NNS,\nin/IN, 2013/CD, car/NN, dealers/NNS, for/IN, optical/JJ, quotes/NNS.\nConsidering the words that have POS tags of Nouns (NN/NNS etc.) in the\nexample sentence we get the words \"business\", \"sales\", \"car\", \"dealers\",\n\"quotes\". Based on these words we extract all the partially matching subject-\npredicate-object instances of the domain ontology. For example, we obtain in-\nstances such as [OPTICAL GOODS SALES 2013 10364 million], [INDUSTRY BUSINESS\nOPTICAL GOODS] and [INDUSTRY BUSINESS CAR DEALERS], etc. from the domain\nontology that are partially matching with the words \"business\" and \"sales\"\nrespectively. POS tag 2013/CD also leads to reinforcing the above <s-p-o> in-\nstance.\n– Developing the genes to identify the genotypes (Step 2): We replace\nthe erroneous words in the sentence by using a ﬁtness function. The ﬁtness\nfunction is deﬁned using string similarity metric (Levenshtein distance) and\nan aggregated score of phonetic algorithms such as Soundex, Metaphone and\nDouble Metaphone as described in Fitness function in the section above. Thus\nwe get the following adaptation: which business has more sales in 2013 car\ndealers for optical goods?\n– Developing Genotypes to Phenotype (Step 3): We now ﬁnd the parts-of-\nspeech of the repaired sentence after the step 2 as: which/WDT, business/NN,\nhas/VBZ, more/JJR, sales/NNS, in/IN, 2013/CD, car/NN, dealers/NNS, for/IN,\noptical/JJ, goods/NNS.\nIn the linguistic repair step, we ﬁnd that since there is no direct ontological\nrelationship between \"car dealers\" and \"optical goods\", we cannot have the\nAdapting general-purpose speech recognition engine for QA\n11\npreposition for between these domain terms. Thus we have to ﬁnd a linguistic\nrelation that is permissible between these domain terms. One of the options\nis to consider linguistic relations like ‘conjunction’, ‘disjunction’ between do-\nmain terms. Thus, when we evaluate linguistic relations AND or OR between\nthese domain terms, we ﬁnd that OR matches closely with for through a pho-\nnetic match rather than AND. Thus we replace for with or in the sentence.\nHence the ﬁnal output of the Evo-Devo mechanism is \"which business has\nmore sales in 2013 car dealers or optical goods?\". This sentence can now\nbe processed by a question-answering (QA) system. In the above example, a QA\nsystem (Bhat et al., 2007) would parse the sentence, identify the known on-\ntological terms {business, sales, 2013, car dealers, optical goods}, ﬁnd\nthe unknown predicates {which business, more sales}, form the appropriate\nquery over the ontology, and return the answer \"CAR DEALERS\".\n3.2.4 Limitations of the method\nWe assume that there is a well-structured domain ontology for the domain and\nit is available in the form of <s-p-o> triples. We also assume that the speaker\nspeaks mostly grammatically correct sentences using terms in the domain. While\nthe method would work for grammatically incorrect sentences, the linguistic repair\nstep would suﬀer.\nWe assume that the speech is processed by a gp-ASR and the ASR-output\nforms the input sentence that needs repair. However, it is important to note that\nthe input sentence (i.e. the ASR output) need not necessarily contain <s-p-o>\ntriples for our method to work. The <s-p-o> triples that are short-listed from do-\nmain ontology aid in forming a candidate set of ’possible genes’ to consider and the\nﬁttest amongst them is considered (Step 2) in the context of the other words in the\nsentence. For example, if the input sentence was ’Who had pick sales’ would get\nrepaired to ’Who had peak sales’ since the domain term of ’peak sales’ would\nmatch with ’pick sales’ in our method. Further, input sentences need not neces-\nsarily be queries; these can be just statements about a domain. For example, if the\nabove ASR-output sentence was \"We hit pick sales this season\", the method\nwould repair it as \"We hit peak sales this season\" using the same set of steps\nfor repair. However, as of now, our method does not repair paraphrases of sentences\nlike \"which industry had biggest sales\" to \"which industry had peak sales\".\nSuch repairs need extension to our matching process.\nThe method does not impose any restriction on the sentence or its formation; it\ncan be a fully meaningful sentence in a domain or may contain partial information.\nThe method ﬁnds the ﬁttest repair for the inaccuracies occurring in an sentence,\npost-ASR recognition. It should also be noted that the method does not know the\noriginal sentence spoken by the speaker, but tries to get back the original sentence\nfor a particular domain.\n3.3 Machine Learning mechanism of adaptation\nIn the machine learning based mechanism of adaptation, we assume the availability\nof example pairs of (T ′, T) namely (ASR output, the actual transcription of the\nspoken sentence) for training. We further assume that such a machine-learnt model\n12\nAnantaram, Sunil\ncan help repair an unseen ASR output to its intended correct sentence. We address\nthe following hypothesis\nUsing the information from past recorded errors and the corresponding correc-\ntion, can we learn how to repair (and thus adapt to a new domain) the text\nafter ASR?\nNote that this is equivalent to, albiet loosely, learning the error model of a speciﬁc\nASR. Since we have a small training set, we have used the Naive Bayes classiﬁer\nthat is known to perform well for small datasets with high bias and low vari-\nance. We have used the NLTK (Bird et al., 2009) Naive Bayes classiﬁer in all our\nexperiments.\nLet T ′ be the erroneous text (which is the ASR output), T the corresponding\nreference text (which is the textual representation of the spoken sentence) and F\na feature extractor, such that\nfβ = F(T ′\nβ)\n(2)\nwhere\nfβ = (fβ1, fβ2, · · · fβn)\n(3)\nis a set of n features extracted from T ′β. Suppose there are several pairs say (T ′i,\nTi) for i = 1, 2, · · ·N. Then we can derive fi for each T ′i using (2). The probability\nthat T ′k belongs to the class Tk can be derived through the feature set fk as\nfollows.\nP(Tk|fk) = P(Tk) ∗P(fk|Tk)\nP(fk)\nwhere P(Tk) is the apriori probability of the class Tk and P(fk|Tk) is the probability\nof occurrence of the features fk in the class Tk, and P(fk) is the overall probability\nof the occurrence of the feature set fk. Making naive assumption of independence\nin the features fk1, fk2, · · ·fkn we get\nP(Tk|fk) = P(Tk) ∗(P(fk1|Tk) ∗P(fk2|Tk) ∗· · ·P(fkn|Tk))\nP(fk)\n(4)\nIn our experiments, the domain speciﬁc reference text Ti was spoken by several\npeople and the spoken speech was passed through a general purpose speech recog-\nnition engine (ASR) that produced a (possibly) erroneous hypothesis T ′i. Each\npair of reference and the ASR output (i.e. hypothesis) was then word aligned us-\ning edit distance, and the mismatching pairs of words were extracted as (T ′i, Ti)\npairs. For example, if we have the following spoken sentence:\nS1 :\n\u001a\n/In which year beer wine and liquor stores has\nsuccessful year/\nand the corresponding true transcription\nT1 :\n\u001aIn which year beer wine and liquor stores has\nsuccessful year\nOne of the corresponding ASR output T ′1 was\nT ′\n1 :\n\u001ain which year dear wine and liquor stores have\nsuccessful year\nAdapting general-purpose speech recognition engine for QA\n13\nIn this case the (T ′, T) pairs are (dear, beer) and (have, has). As another\nexample consider that T2 was spoken but T ′2 was recognized by the ASR.\nT2 :\n\u001aWhether the sales of jewelry business crosses fifty\nthousand in a year\nT ′\n2 :\n\u001awhether the sales of than twenty business crosses fifty\nthousand in a year\nClearly, in this case the (T ′, T) pair is (than twenty, jewelry).\nLet us assume two features, namely, fβ in (2) is of dimension n = 2. Let the\ntwo features be (number of words, number of syllables). Then, for the (T ′, T) pair\n(than twenty, jewelry) we have\nF((than twenty)) = (2, 3)\nsince the number of words in than twenty is 2 and than twenty contains 3 syllables.\nP(fk1|Tk) in this case would be the probability that the number of words in the\ninput are two (fk1 = 2) when the correction is jewelry. A third example is:\nT3 :\nnIn two thousand thirteen which industry had the peak\nsales\nT ′\n3 : {in two thousand thirteen which industry have the pixels\nNote that in this case the (T ′, T) pair is (peak sales, pixel).\nCalculating thus the values of P(Tk) for all reference corrections, P(fkj|Tk) for\nall feature values, fkj for all the j features in fk, we are in a position to calculate\nthe RHS of (4). When this trained classiﬁer is given an erroneous text, features\nare extracted from this text and the repair works by replacing the erroneous word\nby a correction that maximizes (4),\nT ∗\nk = max\nTk\nP(Tk|fk)\nNamely, the T ∗\nk for which P(Tk|fk) is maximum.\n4 Experiments and results\nWe present the results of our experiments with both the Evo-Devo and the Machine\nLearning mechanisms described earlier using the U.S. Census Bureau conducted\nAnnual Retail Trade Survey of U.S. Retail and Food Services Firms for the period\nof 1992 to 2013 (USCensus, 2015).\n14\nAnantaram, Sunil\nFig. 2 T ′ accuracy (y-axis) for the 250 utterance (x-axis) for Ga, Ki, Ku and Ps.\n4.1 Data Preparation\nWe downloaded this survey data and hand crafted a total of 293 textual questions\n(AwazYP, 2015) which could answer the survey data. A set of 6 people (L2 English)\ngenerated 50 queries each with the only constraint that these queries should be\nable to answer the survey data. In all a set of 300 queries were crafted of which\nduplicate queries were removed to leave 293 queries in all. Of these, we chose 250\nqueries randomly and distributed among 5 Indian speakers, who were asked to\nread aloud the queries into a custom-built audio data collecting application. So,\nin all we had access to 250 audio queries spoken by 5 diﬀerent Indian speakers;\neach speaking 50 queries.\nEach of these 250 audio utterances were passed through 4 diﬀerent ASR en-\ngines, namely, Google ASR (Ga), Kaldi with US acoustic models (Ku), Kaldi with\nIndian Acoustic models (Ki) and PocketSphinx ASR (Ps).\nIn particular, that\naudio utterances were in wave format (.wav) with a sampling rate of 8 kHz and\n16 bit. In case of Google ASR (Ga), each utterance was ﬁrst converted into .flac\nformat using the utility sound exchange (sox) commonly available on Unix ma-\nchines. The .flac audio ﬁles were sent to the cloud based Google ASR (Ga) one\nby one in a batch mode and the text string returned by Ga was stored. In all 7\nutterances did not get any text output, presumably Ga was unable to recognize\nthe utterance. For all the other 243 utterances a text output was received.\nIn case of the other ASR engines, namely, Kaldi with US acoustic models (Ku),\nKaldi with Indian Acoustic models (Ki) and PocketSphinx ASR (Ps) we ﬁrst took\nthe queries corresponding to the 250 utterances and built a statistical language\nmodel (SLM) and a lexicon using the scripts that are available with PocketSphinx\nAdapting general-purpose speech recognition engine for QA\n15\nS : {/Which stores has total sales more than two hundred thousand/\nT ′ : {which state has total sales more than twenty thousand\nGa; 70.00%\nT ′ : {which stores has total sales more than two in two thousand\nKi; 80.00%\nT ′ : {which stores has total sales more than point of sales\nKu; 70.00%\nT ′ : {list the total sales more than\nPs; 40.00%\nFig. 3 Sample output (T ′) of four diﬀerent ASR for the same spoken utterance (S). Also\nshown are the accuracy of the ASR output.\nASR engine\nResult\nNo result\nCorrect\nError\n>=70%\n<70%\n(A+B)\n(A)\n(B)\nGoogle ASR (Ga)\n243\n7\n55\n188\n143\n45\nKaldi US (Ku)\n250\n0\n103\n147\n123\n24\nKaldi IN (Ki)\n250\n0\n127\n123\n111\n12\nPocketSphinx (Ps)\n250\n0\n44\n206\n109\n97\nTotal\n993\n7\n329\n664\n486\n178\nTable 2 ASR engines and their output %accuracy\n(CMU, 2017) and Kaldi (Kaldi, 2017). This language model and lexicon was used\nwith the acoustic model that were readily available with Kaldi and Ps. In case of\nKu we used the American English acoustic models, while in case of Ki we used\nthe Indian English acoustic model. In case of Ps we used the Voxforge acoustic\nmodels (VoxForge, 2017). Each utterance was passed through Kaldi ASR for two\ndiﬀerent acoustic models to get T ′ corresponding to Ku and Ki. Similarly all the\n250 audio utterance were passed through the Ps ASR to get the corresponding T ′\nfor Ps. A sample utterance and the output of the four engines is shown in Figure\n3.\nFigure 2 and Table 2 capture the performance of the diﬀerent speech recogni-\ntion engines. The performance of the ASR engines varied, with Ki performing the\nbest with 127 of the 250 utterances being correctly recognized while Ps returned\nonly 44 correctly recognized utterances (see Table 2, Column 4 named ”Correct”)\nof 250 utterances. The accuracy of the ASR varied widely. For instance, in case of\nPs there were as many as 97 instances of the 206 erroneously recognized utterances\nwhich had an accuracy of less than 70%.\nNote that the accuracy is computed as the number of deletions, insertions,\nsubstitutions that are required to convert the ASR output to the textual reference\n(namely, T ′ →T) and is a common metric used in speech literature (Hunt, 1990).\nFor all our analysis, we used only those utterances that had an accuracy 70%\nbut less that 100%, namely, 486 instances (see Table 2, Figure 4). An example\nshowing the same utterance being recognized by four diﬀerent ASR engines is\nshown in Figure 3. Note that we used T ′ corresponding to Ga, Ki and Ku in our\nanalysis (accuracy ≥70%) and not T ′ corresponding to Ps which has an accuracy\nof 40% only. This is based on our observation that any ASR output that is lower\nthat 70% accurate is so erroneous that it is not possible to adapt and steer it\ntowards the expected output.\nThe ASR output (T ′) are then given as input in\nthe Evo-Devo and Machine Learning mechanism of adaptation.\n16\nAnantaram, Sunil\nFig. 4 All utterances that have and T ′ accuracy (y-axis) ≥70 and < 100 used in all our\nexperiments.\n4.2 Evo-Devo based experiments\nWe ran our Evo-Devo mechanism with the 486 ASR sentences (see Table 2) and\nmeasured the accuracy after each repair. On an average we have achieved about\n5 to 10% improvements in the accuracy of the sentences. Fine-tuning the repair\nand ﬁtness functions, namely Equation (1), would probably yield much better\nperformance accuracies. However, experimental results conﬁrm that the proposed\nEvo-Devo mechanism is an approach that is able to adapt T ′ to get closer to T.\nWe present a snapshot of the experiments with Google ASR (Ga) and calculate\naccuracy with respect to the user spoken question as shown in Table 3.\nTable 3 clearly demonstrates the promise of the evo-devo mechanism for adap-\ntation/repair. In our experiments we observed that the adaptation/repair of sub-\nparts in ASR-output (T ′) that most probably referred to domain terms occurred\nwell and were easily repaired, thus contributing to increase in accuracy. For non-\ndomain-speciﬁc linguistic terms the method requires one to build very good lin-\nguistic repair rules, without which the method could lead to a decrease in accuracy.\nOne may need to ﬁne-tune the repair, match and ﬁtness functions for linguistic\nterms. However, we ﬁnd the abstraction of evo-devo mechanism is very apt to use.\n4.3 Machine Learning experiments\nIn the machine learning technique of adaptation, we considers (T ′, T) pairs as the\npredominant entity and tests the accuracy of classiﬁcation of errors.\nAdapting general-purpose speech recognition engine for QA\n17\nUser’s Question (T), Google ASR out (T ′Ga), After Evo-devo (T ′ED)\nAcc\nT: {In two thousand fourteen which industry had the peak sales\nT ′Ga: {in two thousand fourteen which industry had the pixels\nGa: 80%\nT ′ED: {in two thousand fourteen which industry had the peak sales\nED: 100%\nT :\nnin which year did direct selling establishments make the\nmaximum sales and in which year did they do the minimum sales\nT ′Ga :\nnwhich year did direct selling establishments make the maximum\ncells and in which year did they do the many muscles\nGa:80.9%\nT ′ED :\nnwhich year did direct selling establishments make the maximum\nsales and in which year did they do the many musical\nED: 85.7%\nT :\n(Which one among the electronics and appliance store and food\nand beverage stores has sales in more than hundred thousand\nin at least three years in a row\nT ′Ga :\n(which one among the electronics and appliance store and food\nand beverages stores have sales in more than one lakh in at\nleast three years in a row\nGa: 85.7%\nT ′ED:\n(which one among the electronics and appliance store and food\nand beverage stores have sales in more than one lakh in at\nleast three years in a row\nED:89.3%\nTable 3 Evo-Devo experiments with Google ASR (Ga).\nIn our experiment, we used a total of 570 misrecognition errors (for example,\n(dear, beer) and (have, has) derived from (T ′1, T1) or (than twenty, jewelry)\nderived from (T ′2, T2)) in the 486 sentences. We performed 10-fold cross validation,\neach fold containing 513 (T ′, T) pairs for training and 57 pairs for testing, Note\nthat we assume the erroneous words in the ASR output being marked by a human\noracle, in the training as well as the testing set. Suppose the following example\n(T4) occurs in the training set:\nT4 :\nnWhich business has posted cumulative sales of more than\none million dollars from the 2007 to 2012\nT ′\n4 :\nnwhich business has posted latest stills of more than\none million dollars from 2007 to 2012\nThe classiﬁer is given the pair {F (latest stills), cumulative sales} to the clas-\nsiﬁer. And if the following example occurs in the testing set (T5),\nT5 :\nnDid sales remain the same in retail between two\nthousand thirteen and two thousand fourteen\nT ′\n5 :\nnsales wine same in retail between two thousand thirteen\nand two thousand fourteen\nthe trained model or the classiﬁer is provided F(wine) and successful repair would\nmean it correctly labels (adapts) it to remain the. The features used for classiﬁ-\ncation were (n = 6 in Equation (3))\nfβ1\n→\nLeft context (word to the left of T ′),\nfβ2\n→\nNumber of errors in the entire ASR sentence,\nfβ3\n→\nNumber of words in T ′,\nfβ4\n→\nRight context (word to the right of T ′),\nfβ5\n→\nBag of vowels of T ′ and\nfβ6\n→\nBag of consonants of T ′.\n18\nAnantaram, Sunil\nThe combination of features fβ6, fβ5, fβ1, fβ3 , fβ4 namely, (bag of consonants,\nbag of vowels, left context, number of words, right context) gave the best results\nwith 32.28% improvement in accuracy in classiﬁcation over 10-fold validation.\nThe experimental results for both evo-devo and machine learning based ap-\nproaches demonstrate that these techniques can be used to correct the erroneous\noutput of ASR. This is what we set out to establish in this paper.\n5 Conclusions\nGeneral-purpose ASR engines when used for enterprise domains may output er-\nroneous text, especially when encountering domain-speciﬁc terms. One may have\nto adapt/repair the ASR output in order to do further natural language process-\ning such as question-answering. We have presented two mechanisms for adapta-\ntion/repair of ASR-output with respect to a domain. The Evo-Devo mechanism\nprovides a bio-inspired abstraction to help structure the adaptation and repair\nprocess. This is one of the main contribution of this paper. The machine learning\nmechanism provides a means of adaptation and repair by examining the feature-\nspace of the ASR output. The results of the experiments show that both these\nmechanisms are promising and may need further development.\n6 Acknowledgments\nNikhil, Chirag, Aditya have contributed in conducting some of the experiments.\nWe acknowledge their contribution.\nReferences\nCheongjae Lee, Sangkeun Jung, Kyungduk Kim, Donghyeon Lee, and Gary Ge-\nunbae Lee. Recent approaches to dialog management for spoken dialog systems.\nJournal of Computing Science and Engineering, 4(1):1–22, 2010.\nC. Anantaram, Rishabh Gupta, Nikhil Kini, and Sunil Kumar Kopparapu. Adapt-\ning general-purpose speech recognition engine output for domain-speciﬁc natural\nlanguage question answering. In Workshop on Replicability and Reproducibility in\nNatural Language Processing: adaptive methods, resources and software at IJCAI\n2015, Buenos Aires, 2015a.\nE.K. Ringger and J.F. Allen. Error correction via a post-processor for continuous\nspeech recognition. In Acoustics, Speech, and Signal Processing, 1996. ICASSP-96.\nConference Proceedings., 1996 IEEE International Conference on, volume 1, pages\n427–430 vol. 1, May 1996. doi: 10.1109/ICASSP.1996.541124.\nYongmei Shi. An Investigation of Linguistic Information for Speech Recognition Error\nDetection. PhD thesis, University of Maryland, Baltimore County, October 2008.\nLina Zhou, Jinjuan Feng, A. Sears, and Yongmei Shi. Applying the naive bayes\nclassiﬁer to assist users in detecting speech recognition errors. In System Sciences,\n2005. HICSS ’05. Proceedings of the 38th Annual Hawaii International Conference\non, pages 183b–183b, Jan 2005. doi: 10.1109/HICSS.2005.99.\nAdapting general-purpose speech recognition engine for QA\n19\nMinwoo Jeong, Byeongchang Kim, and G Lee. Using higher-level linguistic knowl-\nedge for speech recognition error correction in a spoken q/a dialog. In HLT-\nNAACL special workshop on Higher-Level Linguistic Information for Speech Pro-\ncessing, pages 48–55, 2004.\nRam´on L´opez-C´ozar and Zoraida Callejas.\nAsr post-correction for spoken dia-\nlogue systems based on semantic, syntactic, lexical and contextual information.\nSpeech Commun., 50(8-9):745–766, August 2008. ISSN 0167-6393. doi: 10.1016/j.\nspecom.2008.03.008. URL http://dx.doi.org/10.1016/j.specom.2008.03.008.\nYoussef Bassil and Paul Semaan. ASR context-sensitive error correction based on\nmicrosoft n-gram dataset. CoRR, abs/1203.5262, 2012. URL http://arxiv.org/\nabs/1203.5262.\nJ. Jun and L. Lei. Asr post-processing correction based on ner and pronunciation\nprimitive. In 2011 7th International Conference on Natural Language Processing\nand Knowledge Engineering, pages 126–131, Nov 2011.\ndoi: 10.1109/NLPKE.\n2011.6138180.\nB. Amento, P. Isenhour, and L. Stead. Error correction in automatic speech recog-\nnition transcripts, September 6 2007. URL https://www.google.com/patents/\nUS20070208567. US Patent App. 11/276,476.\nDavid Harwath, Alexander Gruenstein, and Ian McGraw.\n”choosing useful\nword alternates for automatic speech recognition correction interfaces.\nIn\nINTERSPEECH-2014, pages 949–953, 2014.\nSimon Harding and Wolfgang Banzhaf. Artiﬁcial development. In Organic Com-\nputing, Understanding Complex Systems, pages 201–219. Springer Berlin Heidel-\nberg, 2008. ISBN 978-3-540-77656-7. doi: 10.1007/978-3-540-77657-4 9. URL\nhttp://dx.doi.org/10.1007/978-3-540-77657-4_9.\nC Anantaram, Nikhil Kini, Chirag Patel, and Sunil Kopparapu. Improving asr\nrecognized speech output for eﬀective nlp. In The Ninth International Conference\non Digital Society, pages 17–21, Lisbon, Portugal, Feb 2015b.\nGunnar Tufte.\nFrom Evo to EvoDevo: Mapping and Adaptation in Artiﬁcial\nDevelopment. Development, 2008.\nShefali Bhat, C. Anantaram, and Hemant Jain. Framework for text-based con-\nversational user-interface for business applications.\nIn Proceedings of the 2Nd\nInternational Conference on Knowledge Science, Engineering and Management,\nKSEM’07, pages 301–312, Berlin, Heidelberg, 2007. Springer-Verlag.\nISBN\n3-540-76718-5, 978-3-540-76718-3. URL http://dl.acm.org/citation.cfm?id=\n1775431.1775465.\nFelix Naumann.\nhttp://hpi.de/fileadmin/user_upload/fachgebiete/naumann/\nfolien/SS13/DPDC/DPDC_12_Similarity.pdf, 2015.\nSteven Bird, Ewan Klein, and Edward Loper.\nNatural Language Processing\nwith Python.\nO’Reilly Media, Inc., 1st edition, 2009.\nISBN 0596516495,\n9780596516499.\nUSCensus. http://www.census.gov/retail/index.html, 2015. Viewed Sep 2015.\nAwazYP. https://sites.google.com/site/awazyp/data/ijcai, 2015. Viewed Aug\n2017.\nCMU. Building language model for pocketsphinx, 2017. URL http://cmusphinx.\nsourceforge.net/wiki/tutoriallm.\nKaldi. Overview of graph creation in kaldi, 2017. URL http://kaldi-asr.org/\ndoc/graph.html.\n20\nAnantaram, Sunil\nVoxForge. Updated 8khz sphinx acoustic model, 2017. URL http://www.voxforge.\norg/home/news/news/updated-8khz-sphinx-acoustic-model.\nMelvyn J. Hunt. Figures of merit for assessing connected-word recognisers. Speech\nCommunication, 9(4):329 – 336, 1990. ISSN 0167-6393. doi: http://dx.doi.org/10.\n1016/0167-6393(90)90008-W.\nURL http://www.sciencedirect.com/science/\narticle/pii/016763939090008W.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2017-10-12",
  "updated": "2017-10-12"
}