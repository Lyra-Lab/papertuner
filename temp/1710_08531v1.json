{
  "id": "http://arxiv.org/abs/1710.08531v1",
  "title": "Benchmark of Deep Learning Models on Large Healthcare MIMIC Datasets",
  "authors": [
    "Sanjay Purushotham",
    "Chuizheng Meng",
    "Zhengping Che",
    "Yan Liu"
  ],
  "abstract": "Deep learning models (aka Deep Neural Networks) have revolutionized many\nfields including computer vision, natural language processing, speech\nrecognition, and is being increasingly used in clinical healthcare\napplications. However, few works exist which have benchmarked the performance\nof the deep learning models with respect to the state-of-the-art machine\nlearning models and prognostic scoring systems on publicly available healthcare\ndatasets. In this paper, we present the benchmarking results for several\nclinical prediction tasks such as mortality prediction, length of stay\nprediction, and ICD-9 code group prediction using Deep Learning models,\nensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA\nscores. We used the Medical Information Mart for Intensive Care III (MIMIC-III)\n(v1.4) publicly available dataset, which includes all patients admitted to an\nICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the\nbenchmarking tasks. Our results show that deep learning models consistently\noutperform all the other approaches especially when the `raw' clinical time\nseries data is used as input features to the models.",
  "text": "Benchmark of Deep Learning Models on Large\nHealthcare MIMIC Datasets\nSanjay Purushothama,∗, Chuizheng Mengb,∗, Zhengping Chea, Yan Liua\naUniversity of Southern California, Los Angeles, CA 90089, US\nbTsinghua University, Beijing 100084, China\nAbstract\nDeep learning models (aka Deep Neural Networks) have revolutionized many ﬁelds\nincluding computer vision, natural language processing, speech recognition, and\nis being increasingly used in clinical healthcare applications. However, few works\nexist which have benchmarked the performance of the deep learning models with\nrespect to the state-of-the-art machine learning models and prognostic scoring\nsystems on publicly available healthcare datasets. In this paper, we present\nthe benchmarking results for several clinical prediction tasks such as mortality\nprediction, length of stay prediction, and ICD-9 code group prediction using\nDeep Learning models, ensemble of machine learning models (Super Learner\nalgorithm), SAPS II and SOFA scores. We used the Medical Information Mart for\nIntensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes\nall patients admitted to an ICU at the Beth Israel Deaconess Medical Center\nfrom 2001 to 2012, for the benchmarking tasks. Our results show that deep\nlearning models consistently outperform all the other approaches especially when\nthe ‘raw’ clinical time series data is used as input features to the models.\nKeywords:\ndeep learning models, super learner algorithm, mortality prediction,\nlength of stay, ICD-9 code group prediction\n1. Introduction\nQuantifying patient health and predicting future outcomes is an important\nproblem in critical care research. Patient mortality and length of hospital stay\nare the most important clinical outcomes for an ICU admission, and accurately\npredicting them can help with the assessment of severity of illness; and determin-\ning the value of novel treatments, interventions and health care policies. With the\ngoal of accurately predicting these clinical outcomes, researchers have developed\nnovel machine learning models [1, 2] and scoring systems [3] while measuring the\n∗Co-ﬁrst authors.\nEmail addresses: spurusho@usc.edu (Sanjay Purushotham), mengcz95thu@gmail.com\n(Chuizheng Meng), zche@usc.edu (Zhengping Che), yanliu.cs@usc.edu (Yan Liu)\nPreprint submitted to Journal of Biomedical Informatics\nOctober 25, 2017\narXiv:1710.08531v1  [cs.LG]  23 Oct 2017\nimprovement using performance measures such as sensitivity, speciﬁcity and Area\nunder the ROC (AUROC). The availability of large healthcare databases such as\nMedical Information Mart for Intensive Care (MIMIC-II and III) databases [4, 5]\nhas accelerated the research in this important area as evidenced by a lot of recent\npublications [6, 7, 8, 2, 9, 10, 11, 1, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22].\nSeverity scores such as SAPS-II [3], SOFA [23], and APACHE [24] have\nbeen developed with the objective of predicting hospital mortality from baseline\npatient characteristics, deﬁned as the measurements obtained within the ﬁrst 24\nhours after ICU admission. Most of these scoring systems choose a small number\nof hand-picked explanatory predictors and use simple models such as logistic\nregression to predict mortality, while making linear and additive relationship\nassumptions between the outcome variable (mortality) and the predictors. Earlier\nstudies [25, 26] have shown that such assumptions are unrealistic and that\nnonparametric methods might perform better than standard logistic regression\nmodels in predicting ICU mortality.\nWith the recent advances and success of machine learning and deep learning,\nmany researchers have adopted these models for clinical prediction tasks for\nICU admissions. Early works [27, 28, 29] showed that machine learning models\nobtain good results on mortality prediction and forecasting length of stay in\nICU. Recently, Pirracchio [30] showed that a Super Learner algorithm [31]-an\nensemble of machine learning models, oﬀers improved performance for predicting\nhospital mortality in ICU patients and compared its performance to several\nseverity scores on the MIMIC-II dataset. Johnson et al. [6] compared several\npublished works against gradient boosting and logistic regression models using a\nsimple set of features extracted from MIMIC-III dataset [5] for ICU mortality\nprediction. Harutyunyan et al. [2] empirically validated four clinical prediction\nbenchmarking tasks on the MIMIC-III dataset using deep models. Even though\nsome of these recent eﬀorts have attempted to benchmark the machine learning\nmodels on MIMIC datasets, they do not provide a consistent and exhaustive\nset of benchmark comparison results of deep learning models for a variety of\nprediction tasks on the large healthcare datasets. Thus, in this paper, we report\nan exhaustive set of benchmarking results of applying deep learning models\nfor MIMIC-III dataset and compare it with state-of-the art machine learning\napproaches and scoring systems. Table 1 shows the comparison of benchmarking\nworks. We summarize the main contributions of this work below:\n• We present detailed benchmarking results of deep learning models on\nMIMIC-III dataset for three clinical prediction tasks including mortality\nprediction, forecasting length of stay, and ICD-9 code group prediction.\nOur experiments show that deep learning models consistently perform\nbetter than the several existing machine learning models and severity\nscoring systems.\n• We present benchmarking results on diﬀerent feature sets including ‘pro-\ncessed’ and ‘raw’ clinical time series. We show that deep learning models\nobtain better results on ‘raw’ features which indicates that rule-based\npreprocessing of clinical features is not necessary for deep learning models.\n2\nThe remainder of this paper is arranged as follows: in Section 2, we provide\nan overview of the related work; in Section 3, we describe MIMIC-III dataset\nand the pre-processing steps we employed to obtain the benchmark datasets;\nthe benchmarking experiments is discussed in Section 4; and we conclude with\nsummary in Section 5.\nTable 1: Comparison of benchmarking works.\nPirracchio\nHarutyunyan et al.\nJohnson et al.\nThis Work\n2016\n2017\n2017\nTime\nDurations\n24 hours\n✓\n✓\n✓\n48 hours\n✓\n✓\n✓\nNumber of\nFeatures\nSmaller feature set\n✓\n✓\n✓\n✓\nLarger feature set\n✓\nFeature\nType\nNon-time series\n✓\n✓\n✓\nTime-series\n✓\n✓\nDatabases\nMIMIC-II\n✓\nMIMIC-III\n✓\n✓\n✓\nMIMIC-III (CareVue)\n✓\nScoring\nSystems\nSAPS -II\n✓\n✓\nSOFA\n✓\n✓\nPrediction\nAlgorithms\nMachine learning models\n✓\n✓\n✓\nDeep learning models\n✓\n✓\nPrediction\nTasks\nIn-hospital mortality\n✓\n✓\n✓\n✓\nShort-term mortality\n✓\nLong-term mortality\n✓\nLength of stay\n✓\n✓\nPhenotyping\n✓\nICD-9 code group\n✓\n✓\n2. Related Work\nWe ﬁrst provide a brief review of machine learning and deep learning models\nfor healthcare applications, and then discuss the existing works on benchmarking\nhealthcare datasets.\nEarly works [32, 33] have shown that machine learning models obtain good\nresults on mortality prediction and medical risk evaluation. Physionet challenge1 -\na friendly competition platform - has resulted in development of machine learning\nmodels for addressing some of the open healthcare problems. With the recent\nadvances in deep learning techniques, there is a growing interest in applying\nthese techniques to healthcare applications due to the increasing availability of\nlarge-scale health care data [34, 7, 35, 36]. For example, Che et al. [7] developed a\n1https://physionet.org/challenge/\n3\nscalable deep learning framework which models the prior-knowledge from medical\nontologies to learn clinically relevant features for disease diagnosis. A recent\nstudy [37] showed that a neural network model can improve the prediction of\nseveral psychological conditions such as anxiety, behavioral disorders, depression,\nand post-traumatic stress disorder. Other recent works [38, 39, 40] have leveraged\nthe power of deep learning approaches to model diseases and clinical time series\ndata. These previous work have demonstrated the strong performance by deep\nlearning models in health care applications, which signiﬁcantly alleviates the\ntedious work on feature engineering and extraction.\nThe availability of deidentiﬁed public datasets such as Medical Information\nMart for Intensive Care (MIMIC-II [4] and MIMIC-III [5]) has enabled researchers\nto benchmark machine learning models for studying ICU clinical outcomes such\nas mortality and length of hospital stay. Pirracchio [30] used MIMIC II clinical\ndata [4] to predict mortality in the ICU and showed that the Super Learner\nalgorithm - an ensemble of machine learning models, performs better than\nSAPS II, APACHE II and SOFA scores. Their work showed that machine\nlearning models outperform the prognostic scores, but they did not compare\ntheir results with the recent deep learning models.\nHarutyunyan et al. [2]\nproposed a deep learning model called multi-task Recurrent Neural Networks to\nempirically validate four clinical prediction benchmarking tasks on the MIMIC-\nIII database. While, their work showed promising benchmark results of deep\nlearning models, they compared their proposed model only with a standard\nLogistic Regression model and a Long Short Term Memory Network [41], and\nomitted comparison with scoring systems (SAPS-II) or other machine learning\nmodels (such as Super Learner). Johnson et al. [6] studied the challenge of\nreproducing the published results on the public MIMIC-III dataset using a\ncase-study on mortality prediction task. They reviewed 28 publications and then\ncompared the performance reported in these studies against gradient boosting and\nlogistic regression models using a simple set of features extracted from MIMIC-III\ndataset. They demonstrated that the large heterogeneity in studies highlighted\nthe need for improvements in the way that prediction tasks are reported to\nenable fairer comparison between models. Our work advances the eﬀorts of\nthese previous benchmark works by providing a consistent and exhaustive set of\nbenchmarking results of deep learning models on several prediction tasks.\n3. MIMIC-III Dataset\nIn this section, we describe the MIMIC-III dataset and discuss the steps\nwe employed to preprocess and extract the features for our benchmarking\nexperiments.\n3.1. Dataset Description\nMIMIC III [5] is a publicly available critical care database maintained by the\nMassachusetts Institute of Technology (MIT)’s Laboratory for Computational\nPhysiology. This database integrates deidentiﬁed, comprehensive clinical data of\n4\npatients admitted to an Intensive Care Unit (ICU) at the Beth Israel Deaconess\nMedical Center (BIDMC) in Boston, Massachusetts during 2001 to 2012.\nMIMIC-III contains data associated with 53 423 distinct hospital admissions\nfor adult patients (aged 15 years or above) and 7870 neonates admitted to an\nICU at the BIDMC. The data covers 38 597 distinct adult patients with 49 785\nhospital admissions. To obtain consistent benchmarking datasets, in this paper\nwe only include the ﬁrst ICU admission of the patients. Table 2 shows the\nstatistics of our dataset, and Table 3 shows the baseline characteristics and\noutcome measures of our dataset. We observe that the median age of adult\npatients is 65.86 years (Quartile Q1 to Q3: 52.72 to 77.97) with 56.76 % patients\nare male, in-hospital mortality around 10.49 % and the median length of an\nhospital stay is 7.08 days (Q1 to Q3: 4.32 to 12.03).\nTable 2: Summary statistics of MIMIC-III dataset.\nData\nTotal\n# admissions in the MIMIC-III (v1.4) database\n58 576\n# admissions which are the ﬁrst admission of the patient\n46 283\n# admissions which are the ﬁrst admission of an adult patient (> 15 years old)\n38 425\n# admissions where adult patient died 24 hours after the ﬁrst admission\n35 627\n3.2. Dataset Preprocessing\nIn this section, we describe in detail the cohort selection, data extraction,\ndata cleaning and feature extraction methods we employed to preprocess our\nMIMIC-III dataset.\n3.2.1. Cohort Selection\nThe ﬁrst step of dataset preprocessing includes cohort selection. We used\ntwo sets of inclusion criterion to select the patients to prepare the benchmark\ndatasets. First, we identiﬁed all the adult patients by using the age recorded at\nthe time of ICU admission. Following previous studies [6], in our work, all the\npatients whose age was >15 years at the time of ICU admission is considered\nas an adult 2. Second, for each patient, we only use their ﬁrst admission in our\nbenchmark datasets and for subsequent analysis, and dropped all their later\nadmissions. This was done to prevent possible information leakage in the analysis,\nand to ensure similar experimental settings compared to the related works [6].\n3.2.2. Data Extraction\nThere are 26 tables in the MIMIC-III (v1.4) relational database. Charted\nevents such as laboratory tests, doctor notes and ﬂuids into/out of patients are\n2Note that in MIMIC III (v1.4), all the patients under the age of 15 years are referred to\nas neonates.\n5\nTable 3: Baseline characteristics and in-hospital mortality outcome measures. Con-\ntinuous variables are presented as Median [InterQuartile Range Q1-Q3]; binary or\ncategorical variables as Count (%).\nOverall\nDead at hospital\nAlive at hospital\nGeneral\n# admissions\n35627\n3738\n31889\nAge\n65.86 [52.72-77.97]\n73.85 [60.16-82.85]\n64.98 [52.04-77.21]\nGender (female)\n15409 (43.24%)\n1731 (46.31%)\n13678 (42.88%)\nFirst SAPS-II\n33.00 [25.00-42.00]\n48.00 [38.00-59.00]\n32.00 [24.00-40.00]\nFirst SOFA\n3.00 [2.00-6.00]\n6.00 [4.00-9.00]\n3.00 [2.00-5.00]\nOrigin\nMedical\n24720 (69.37%)\n2969 (79.43%)\n21751 (68.19%)\nEmergency surgery\n6134 (17.21%)\n663 (17.74%)\n5471 (17.15%)\nScheduled surgery\n4783 (13.42%)\n106 (2.84%)\n4677 (14.66%)\nSite\nMICU\n12621 (35.42%)\n1814 (48.53%)\n10807 (33.88%)\nMSICU\n5821 (16.33%)\n691 (18.49%)\n5130 (16.08%)\nCCU\n5180 (14.54%)\n523 (13.99%)\n4657 (14.60%)\nCSRU\n7264 (20.38%)\n245 (6.55%)\n7019 (22.00%)\nTSICU\n4751 (13.33%)\n465 (12.44%)\n4286 (13.44%)\nHR (bpm)\n84.00 [73.00-97.00]\n90.00 [75.00-107.00]\n84.00 [72.00-96.00]\nMAP (mmhg)\n76.00 [67.33-87.00]\n74.00 [64.67-86.00]\n77.00 [68.00-87.00]\nRR (cpm)\n18.00 [14.00-22.00]\n20.00 [16.00-24.00]\n18.00 [14.00-21.00]\nNa (mmol/l)\n138.00 [136.00-141.00]\n139.00 [135.00-142.00]\n138.00 [136.00-141.00]\nK (mmol/l)\n4.10 [3.80-4.60]\n4.20 [3.70-4.70]\n4.10 [3.80-4.60]\nHCO3 (mmol/l)\n24.00 [21.00-26.00]\n22.00 [18.00-25.00]\n24.00 [21.00-26.00]\nWBC (103/mm3)\n11.00 [7.90-14.90]\n12.30 [8.00-17.20]\n10.80 [7.90-14.60]\nP/F ratio\n257.50 [180.00-352.50]\n218.66 [140.00-331.86]\n262.50 [187.00-355.00]\nHt (%)\n31.00 [26.00-36.00]\n31.00 [27.00-36.00]\n31.00 [26.00-36.00]\nUrea (mmol/l)\n1577.00 [968.00-2415.00]\n1020.00 [518.50-1780.00]\n1640.00 [1035.00-2470.00]\nBilirubine (mg/dl)\n0.70 [0.40-1.70]\n1.00 [0.50-3.50]\n0.70 [0.40-1.50]\nHospital LOS (days)\n7.08 [4.32-12.03]\n7.21 [3.31-14.44]\n7.07 [4.40-11.88]\nICU death (%)\n2860 (8.03%)\n2860 (76.51%)\n–\nHospital death (%)\n3738 (10.49%)\n–\n–\nstored in a series of ’events’ tables. For the purpose of preparing benchmark\ndatasets to predict clinical tasks, we extracted data for the selected cohort from\nthe following tables: inputevents (inputevents cv/inputevents mv) (intake for\npatients monitored using Philips CareVue system/iMDSoft MetaVision system),\noutputevents (output information for patients while in the ICU), chartevents\n(all charted observations for patients), labevents (laboratory measurements for\npatients both within the hospital and in outpatient clinics), and prescriptions\n(medications ordered, and not necessarily administered, for a given patient).\nWe selected these tables as they provide the most relevant clinical features for\nthe prediction tasks considered in this work. We obtained the following two\nbenchmark datasets:\n6\n• MIMIC-III: This includes the data extracted from all the above tables for\nall the selected cohorts in the entire MIMIC-III database.\n• MIMIC-III (CareVue): This includes the data extracted from all the above\ntables for the selected cohorts who are included in the inputevents cv table\n(inputevents data recorded using Philips CareVue system) in the MIMIC-III\ndatabase. MIMIC-III (CareVue) is a subset of MIMIC-III dataset and it\nroughly corresponds to the MIMIC-II [4] dataset.\n3.2.3. Data Cleaning\nThe data extracted from MIMIC-III database has lots of erroneous entries\ndue to noise, missing values, outliers, duplicate or incorrect records, clerical\nmistakes etc. We identiﬁed and handled the following three issues with the\nextracted data. First, we observed that there is inconsistency in the recording\n(units) of certain variables. For example, some of the prescriptions are recorded\nin ‘dose’ and in ‘mg’ units; while some variables in chartevents and labevents\ntables are recorded in both numeric and string data type. Second, some variables\nhave multiple values recorded at the same time. Third, for some variables the\nobservation was recorded as a range rather than a single measurement. We\naddressed these issues by these procedures:\n• To handle inconsistent units: We ﬁrst obtain the percentage of each unit\nappearing in the database for a variable. If there is only one unit, we do\nnothing. For variables with multiple and inconsistent units, if a major unit\naccounts for ≥90 % of the total number of records then we just keep all\nthe records with the major unit and drop the other ones. For the rest of\nthe variables/features which do not have a major unit, we convert all the\nunits to a single unit based on accepted rules in literature 3 (For example:\nconvert ‘mg’ to ‘grams’, ‘dose’ to ‘ml’ or ‘mg’ based on the variable). We\ndrop the features for which we cannot ﬁnd correct rules for conversion.\n• To handle multiple recordings at the same time: Depending on the variable,\nwe either take the average or the summation of the multiple recordings\npresent at the same time.\n• To handle range of feature values: We take the median of the range to\nrepresent the value of the feature at a certain time point.\n3.2.4. Feature Selection and Extraction\nWe process the extracted benchmark datasets to obtain the features which\nwill be used for the prediction tasks. To enable an exhaustive benchmarking\ncomparison study, we select three sets of features as described below.\n3https://www.drugs.com/dosage/\n7\nTable 4: Feature Set A: 17 features used in SAPS-II scoring system.\nFeature\nItemid\nName of Item\nTable\nglasgow\ncoma\nscale\n723\nGCSVerbal\nchartevents\n454\nGCSMotor\nchartevents\n184\nGCSEyes\nchartevents\n223900\nVerbal Response\nchartevents\n223901\nMotor Response\nchartevents\n220739\nEye Opening\nchartevents\nsystolic\nblood\npressure\n51\nArterial BP [Systolic]\nchartevents\n442\nManual BP [Systolic]\nchartevents\n455\nNBP [Systolic]\nchartevents\n6701\nArterial BP #2 [Systolic]\nchartevents\n220179\nNon Invasive Blood Pressure systolic\nchartevents\n220050\nArterial Blood Pressure systolic\nchartevents\nheart rate\n211\nHeart Rate\nchartevents\n220045\nHeart Rate\nchartevents\nbody tempera-\nture\n678\nTemperature F\nchartevents\n223761\nTemperature Fahrenheit\nchartevents\n676\nTemperature C\nchartevents\n223762\nTemperature Celsius\nchartevents\npao2 / ﬁo2 ratio\n50821\nPO2\nlabevents\n50816\nOxygen\nlabevents\n223835\nInspired O2 Fraction (FiO2)\nchartevents\n3420\nFiO2\nchartevents\n3422\nFiO2 [Meas]\nchartevents\n190\nFiO2 set\nchartevents\nurine output\n40055\nUrine Out Foley\noutputevents\n43175\nUrine\noutputevents\n40069\nUrine Out Void\noutputevents\n40094\nUrine Out Condom Cath\noutputevents\n40715\nUrine Out Suprapubic\noutputevents\n40473\nUrine Out IleoConduit\noutputevents\n40085\nUrine Out Incontinent\noutputevents\n40057\nUrine Out Rt Nephrostomy\noutputevents\n40056\nUrine Out Lt Nephrostomy\noutputevents\n40405\nUrine Out Other\noutputevents\n40428\nOrine Out Straight Cath\noutputevents\n40086\nUrine Out Incontinent\noutputevents\n40096\nUrine Out Ureteral Stent #1\noutputevents\n40651\nUrine Out Ureteral Stent #2\noutputevents\n226559\nFoley\noutputevents\n226560\nVoid\noutputevents\n226561\nCondom Cath\noutputevents\n226584\nIleoconduit\noutputevents\n226563\nSuprapubic\noutputevents\n226564\nR Nephrostomy\noutputevents\n8\nTable 4: Feature Set A: 17 features used in SAPS-II scoring system.\nFeature\nItemid\nName of Item\nTable\n226565\nL Nephrostomy\noutputevents\n226567\nStraight Cath\noutputevents\n226557\nR Ureteral Stent\noutputevents\n226558\nL Ureteral Stent\noutputevents\n227488\nGU Irrigant Volume In\noutputevents\n227489\nGU Irrigant/Urine Volume Out\noutputevents\nserum urea ni-\ntrogen level\n51006\nUrea Nitrogen\nlabevents\nwhite\nblood\ncells count\n51300\nWBC Count\nlabevents\n51301\nWhite Blood Cells\nlabevents\nserum bicarbon-\nate level\n50882\nBICARBONATE\nlabevents\nsodium level\n950824\nSodium Whole Blood\nlabevents\n50983\nSodium\nlabevents\npotassium level\n50822\nPotassium, whole blood\nlabevents\n50971\nPotassium\nlabevents\nbilirubin level\n50885\nBilirubin Total\nlabevents\nage\n–\nintime\nicustays\ndob\npatients\nacquired immun-\nodeficiency syn-\ndrome\n–\nicd9 code\ndiagnoses icd\nhematologic\nmalignancy\n–\nicd9 code\ndiagnoses icd\nmetastatic can-\ncer\n–\nicd9 code\ndiagnoses icd\nadmission type\n–\ncurr service\nservices\nADMISSION TYPE\nadmissions\n• Feature Set A: This feature set consists of the 17 features used in the\ncalculation of the SAPS-II score [3]. For these features, we drop outliers\nin the data according to medical knowledge and merge relevant features.\nFor example, for the Glasgow Coma Scale score denoted as GCS score, we\nsum the GCSVerbal, GCSMotor and GCSEyes values; for the urine output,\nwe sum the features representing urine; and for body temperature, we\nconvert Fahrenheit to Celsius scale. Note that the SAPS II score features\nare hand-chosen and processed, and thus, we refer to them as ‘Processed’\nfeatures instead of ‘raw’ features. Table 4 lists all the 17 processed features\n9\nand their corresponding entries in the MIMIC-III database table. In our\nexperiments, some of these features such as chronic diseases, admission\ntype and age are treated as non-time series features, and the remaining\nfeatures are treated as time series features.\n• Feature Set B: This feature set consists of the 20 features related to the\n17 features used in SAPS-II score. Instead of preprocessing the 17 features\nas done to obtain Feature set A, here we consider all the raw values of the 17\nSAPS-II score features. In particular, we do not remove outliers and we only\ndrop values below 0. For the GCS score, we treat GCSVerbal, GCSMotor\nand GCSEyes as separate features. We also consider PaO2 and FiO2 as\nindividual features instead of calculating the PF-ratio (PaO2/FiO2 ratio).\nThis feature set was built to study how the prediction models perform on\nthe ‘raw’ clinical features.\n• Feature Set C: This feature set consists of 135 raw features selected from\nthe 5 tables mentioned in section 3.2.2 and includes the 20 features of\nFeature set B. These 135 features were chosen based on their low missing\nrate, from more than 20 000 features available in the 5 tables mentioned in\nsection 3.2.2. Similar to feature set B, we did not preprocess this dataset\n(i.e. did not apply hand-crafted processing rules) and used the raw values\nof the features. It is worth noting that a few features appear multiple times\nas they were present in multiple tables. For example, Glucose appears\nin both Chart and Lab events, and was included in the feature set. This\nfeature set was selected to study if the prediction models can automatically\nlearn feature representations from a large number of raw clinical time series\ndata and at the same time obtain better results on the prediction tasks.\nTable A.22 in the Appendix lists all the features of this feature set C.\nWe extract the above three feature sets from MIMIC-III and MIMIC-III\n(CareVue) datasets. After the feature selection, we obtained the non-time series\nand time series features which will be used in the experiments. We extracted the\nfeatures from ﬁrst 24 hours and ﬁrst 48 hours after admission to ICU. Each time\nseries feature is sampled every 1 hour. To ﬁll-in missing values, we performed\nforward and backward imputation. For some patients, certain features might be\ncompletely missing. We performed mean imputation for these cases during the\ntraining and validation stage of the experiments. We obtain summary statistics\nof time-series features for models which are not capable of handling temporal\ndata.\n4. Benchmarking Experiments\nIn this section, we describe in detail the benchmark prediction tasks, the\nprediction algorithms and their implementation, and report the experimental\nresults.\n10\n4.1. Benchmark Prediction Tasks\nHere, we describe the benchmark prediction tasks which represent some of\nthe important problems in critical care research. They have been well-studied in\nthe medical community [24, 42, 26], and these tasks have been commonly used\nto benchmark machine learning algorithms [30, 2].\n4.1.1. Mortality Prediction\nMortality prediction is one of the primary outcomes of interest of an hospital\nadmission. We formulate mortality as a binary classiﬁcation task, where the\nlabel indicates the death event for a patient. We deﬁne the following mortality\nprediction benchmark tasks:\n• In-hospital mortality prediction: Predict whether the patient dies during\nthe hospital stay after admitted to an ICU.\n• Short-term mortality prediction: Predict whether the death happens within\na short duration of time after the patient is admitted to the ICU. For this\ntask, we deﬁne the 2-day and 3-day mortality prediction tasks where the\npatient dies within 2-days and 3-days respectively after admitted to ICU.\nFor ﬁrst 24-hour data, we can predict 2-day and 3-day mortality, while for\nthe ﬁrst 48-hour data we only predict 3-day mortality.\n• Long-term mortality prediction: This task involves predicting if the patient\ndies after a long time since being discharged from the hospital. For this\ntask, we consider the 30-day and 1-year mortality prediction tasks where\nthe patient dies within 30-days or 1 year after being discharged from the\nhospital. Note that we still use only the ﬁrst 24-hour data and ﬁrst 48-hour\ndata to predict 30-days and 1-year mortality.\nTable 5 shows the mortality label statistics of the entire MIMIC-III dataset.\nThe details about how the mortality labels are obtained from MIMIC-III database\nis explained in the Appendix.\nTable 5: Label statistics of mortality prediction task.\nMIMIC-III\nDatasource\nMortality label ratio w.r.t total admissions\n# Admissions\nIn-hospital\n2-day\n3-day\n30-day\n1-year\nMetavision (2008-2012)\n0.096\n0.015\n0.014\n0.124\n0.232\n15 376\nCareVue (2001-2008)\n0.111\n0.014\n0.015\n0.134\n0.261\n20 261\nAll sources (2001-2012)\n0.105\n0.014\n0.015\n0.129\n0.248\n35 637\n4.1.2. ICD-9 Code Group Prediction\nIn this benchmarking task, we predict the ICD-9 diagnosis code group (e.g.\nrespiratory system diagnosis) for each admission. ICD (stands for International\nStatistical Classiﬁcation of Diseases and Related Health Problems) codes are\n11\nused to classify diseases and a wide variety of symptoms, signs, causes of injury\nor disease, etc. Nearly every health condition can be assigned an unique ICD-9\ncode group where each group usually include a set of similar diseases. In our\nwork, we group all the ICD-9 codes for an ICU admission into 20 diagnosis\ngroups4 and treat this task as a multi-task prediction problem. Table 6 shows\nthe ICD-9 code group label statistics of the MIMIC-III dataset.\nTable 6: ICD-9 code group label statistics. For each ICD-9 code group, the entry\ndenotes the ratio of number of patients who have been assigned that ICD-9 code to\nthe total number of patients in the dataset.\nICD-9 Code\nGroup\nICD-9 Code\nRange\nMIMIC-III\nMetavision\n(2008-2012)\nMIMIC-III\nCareVue\n(2001-2008)\nMIMIC-III\nAll Sources\n(2001-2012)\n1\n001 - 139\n0.302\n0.225\n0.258\n2\n140 - 239\n0.201\n0.151\n0.172\n3\n240 - 279\n0.765\n0.629\n0.688\n4\n280 - 289\n0.445\n0.311\n0.369\n5\n290 - 319\n0.416\n0.244\n0.318\n6\n320 - 389\n0.424\n0.195\n0.294\n7\n390 - 459\n0.846\n0.820\n0.831\n8\n460 - 519\n0.504\n0.468\n0.484\n9\n520 - 579\n0.461\n0.339\n0.391\n10\n580 - 629\n0.461\n0.352\n0.399\n11\n630 - 679\n0.003\n0.005\n0.004\n12\n680 - 709\n0.119\n0.090\n0.102\n13\n710 - 739\n0.266\n0.133\n0.190\n14\n740 - 759\n0.042\n0.032\n0.036\n15\n780 - 789\n0.411\n0.251\n0.320\n16\n790 - 796\n0.115\n0.064\n0.086\n17\n797 - 799\n0.050\n0.016\n0.030\n18\n800 - 999\n0.453\n0.448\n0.450\n19\nV Codes\n0.634\n0.362\n0.479\n20\nE Codes\n0.429\n0.263\n0.335\n4.1.3. Length of Stay Prediction\nIn this benchmarking task, we predict the length of stay for each admission.\nWe deﬁne the length of stay of an admission as total duration of hospital stay,\ni.e. the length of time interval between hospital admission and discharge from\n4http://tdrdata.com/ipd/ipd_SearchForICD9CodesAndDescriptions.aspx\n12\nthe hospital. We treat length of stay prediction task as a regression problem.\nFigure 1 shows the distribution of length of stay of the MIMIC-III benchmark\ndatasets.\n0\n500\n1000\n1500\n2000\nLength of Stay (Hours)\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nFrequency\nDatasource: Metavision (2008-2012)\n0\n500\n1000\n1500\n2000\nLength of Stay (Hours)\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nFrequency\nDatasource: Carevue (2001-2008)\n0\n500\n1000\n1500\n2000\nLength of Stay (Hours)\n0.000\n0.001\n0.002\n0.003\n0.004\n0.005\nFrequency\nDatasource: All (2001-2012)\nFigure 1: Distribution of length of stay. Values above 2000 hours are not shown in\nthe ﬁgure.\n4.2. Prediction Algorithms\nIn this section, we describe all the prediction algorithms and the scoring\nsystems that we have used for benchmarking tasks on MIMIC-III datasets.\n4.2.1. Scoring Methods\nSAPS-II. SAPS-II [3] stands for Simpliﬁed Acute Physiology Score and it is a\nICU scoring system designed to measure the severity of the disease for patients\nadmitted to an ICU. A point score is calculated for each of the 12 physiological\nfeatures mentioned in Table 4 and a ﬁnal SAPS-II score S is obtained as the\nsum of all point scores. Note that SAPS-II score is calculated using the data\ncollected within the ﬁrst 24 hours of an ICU admission. After SAPS-II score is\n13\nobtained, the individual mortality prediction can be calculated as [30]:\nlog\npdeath\n1 −pdeath\n= −7.7631 + 0.0737 · S + 0.9971 · log (1 + S)\nSOFA. SOFA [23] is the Sepsis-related Organ Failure Assessment score (also\nreferred to as the Sequential Organ Failure Assessment score) and it is used\nto describe organ dysfunction/failure of a patient in the ICU. The mortality\nprediction based on SOFA can be obtained by regressing the mortality on the\nSOFA score using a main-term logistic regression model.\nNew SAPS-II. A new SAPS-II scoring method was deﬁned by Pirracchio [30]. It\nis a modiﬁed version of SAPS-II and is obtained by ﬁtting a main-term logistic\nregression model using the same explanatory variables as those used in the\noriginal SAPS-II score calculation.\n4.2.2. Super Learner Models\nSuper Learner [43, 31] is a supervised learning algorithm that is designed\nto ﬁnd the optimal combination from a set of prediction algorithms. It repre-\nsents an asymptotically optimal learning system and is built on the theory of\ncross-validation. This algorithm requires a collection of user-deﬁned machine\nlearning algorithms such as logistic regression, regression trees, additive models,\n(shallow) neural networks, and random forest. The algorithm then estimates the\nrisk associated to each algorithm in the provided collection using cross-validation.\nOne round of cross-validation involves partitioning a sample of data into com-\nplementary subsets, performing the analysis on one subset (called the training\nset), and validating the analysis on the other subset (called the validation set\nor testing set). To reduce variability, multiple rounds of cross-validation are\nperformed using diﬀerent partitions, and the validation results are averaged over\nthe rounds. From this estimation of the risk associated with each candidate\nalgorithm, the Super Learner builds an aggregate algorithm obtained as the\noptimal weighted combination of the candidate algorithms. Table 7 shows the\nalgorithms used in the Super Learner algorithm [30] and their implementation\navailable in the R and Python languages. In the experiments section, we will\ncompare and discuss the results of Super Learner using these programming\nlanguages. Following Pirracchio [30], we consider two variants of Super Learner\nalgorithm, namely, Super Learner I: Super Learner with categorized variables,\nand Super Learner II: Super Learner with non-transformed variables. Note that\nSuper Learner-I is applicable only for Feature set A, while Super Learner-II\nalgorithm can be used with all the Feature sets A, B and C.\n4.2.3. Deep Learning Models\nDeep Learning Models (also called Deep Neural Networks or Deep mod-\nels) [44] have become a successful approach for automated extraction of complex\ndata representations for end-to-end training. Deep models consist of a layered,\nhierarchical architectures of neurons for learning and representing data. The\n14\nTable 7: Algorithms used in Super Learner with corresponding R packages and Python\nlibraries.\nAlgorithm\nR packages\nPython libraries\nStandard logistic regression\nSL.glm\nsklearn.linear model.LogisticRegression\nLogistic regression\nbased on the AIC\nSL.stepAIC\nsklearn.linear model.LassoLarsIC\nGeneralized additive model\nSL.gam\npygam.LinearGAM\npygam.LogisticGAM\nGeneralized linear model\nwith penalized maximum likelihood\nSL.glmnet\nsklearn.linear model.ElasticNet\nMultivariate adaptive polynomial\nspline regression\nSL.polymars\npyearth.Earth\nBayesian generalized linear model\nSL.bayesglm\nsklearn.linear model.BayesianRidge\nGeneralized boosted\nregression model\nSL.gbm\nsklearn.ensemble.GradientBoostingRegressor\nsklearn.ensemble.GradientBoostingClassiﬁer\nNeural network\nSL.nnet\nsklearn.neural network.MLPRegressor\nsklearn.neural network.MLPClassiﬁer\nBagging classiﬁcation trees\nSL.ipredbagg\nsklearn.ensemble.BaggingRegressor\nsklearn.ensemble.BaggingClassiﬁer\nPruned recursive partitioning\nand Regression Trees\nSL.rpartPrune\n–\nRandom forest\nSL.randomForest\nsklearn.ensemble.RandomForestRegressor\nsklearn.ensemble.RandomForestClassiﬁer\nBayesian additive regression trees\nSL.bartMachine\n–\nhierarchical learning architecture is motivated by artiﬁcial intelligence emulating\nthe deep, layered learning process of the primary sensorial areas of the neocortex\nin the human brain, which automatically extracts features and abstractions from\nthe underlying data [45, 46]. In a deep learning model, each neuron receives one\nor more inputs and sums them to produce an output (or activation). Each neuron\nin the hidden layers is assigned a weight that is considered for the outcome\nclassiﬁcation, but this weight is itself learned from its previous layers. The hidden\nlayers thus can use multidimensional input data and introduce progressively\nnon-linear weight combinations to the learning algorithm.\nThe main advantage of the deep learning approach is its ability to automati-\ncally learn good feature representations from raw data, and thus signiﬁcantly\nreduce the eﬀort of handcrafted feature engineering. In addition, deep models\nlearn distributed representations of data, which enables generalization to new\ncombinations of the values of learned features beyond those seen during the train-\ning process. Deep Learning models have yielded outstanding results in several\napplications, including speech recognition [47, 48], computer vision [49, 50, 51],\nand natural language processing [52, 53, 54, 55]. Recent research has shown that\ndeep learning methods achieve state-of-the-art performance in analyzing health-\nrelated data, such as ICU mortality prediction [6], phenotype discovery [7] and\ndisease prediction [36]. These works have demonstrated the strong performance\nby deep learning models in health care applications, which signiﬁcantly alleviates\n15\nthe tedious work on feature engineering and extraction. Here, we will ﬁrst brieﬂy\nintroduce two types of deep models namely Feedforward neural networks (FFN),\nwhich is a standard neural network structure, and Recurrent Neural Networks\n(RNN) which is used for modeling sequence and time series data. After that, we\nwill describe our proposed Multi-modal deep learning model, a combination of\nFFN and RNN, which will be used in the benchmarking experiments.\nFeedforward Neural Networks. A multilayer feedforward network [56] (FFN) is\na neural network with multiple nonlinear layers and possibly one prediction\nlayer on the top to solve classiﬁcation task. The ﬁrst layer takes X as the\ninput, and the output of each layer is used as the input of the next layer. The\ntransformation of each layer l can be written as\nX(l+1) = f (l)(X(l)) = s(l) \u0010\nW (l)X(l) + b(l)\u0011\nwhere W (l) and b(l) are respectively the weight matrix and bias vector of layer\nl, and s(l) is a nonlinear activation function, which usually is a logistic sigmoid,\ntanh, or ReLU [57] function. We optimize the cross-entropy prediction loss and\nget the prediction output from the topmost prediction layer.\n𝒉\n 𝒉\n𝒛\nIN\nOUT\n𝒓\n(a) Gated recurrent unit.\n𝑿𝑻𝑹,𝟏\nHidden layers\n𝒚\n⋯\nShared Representation\n𝑿𝑻𝑹,𝒎\nHidden layers\n(b) Multimodal deep learning mod-\nels.\nFigure 2: Deep learning models. In multimodal deep models, X(.) represents the\ndiﬀerent inputs including temporal and non-temporal features, and y is the output.\nRecurrent Neural Networks. Recurrent neural network (RNN) models have\nbeen shown to be successful at modeling sequences and time series data [58].\nRNN with simple activations are incapable of capturing long term dependencies,\nand hence their variants such as Long Short Term Memory (LSTM) [41] and\nGated Recurrent Unit (GRU) [59] have become popular due to their ability to\ncapture long-term dependencies using memory and gating units. GRU can be\nconsidered as a simpliﬁed version of LSTM and it has been shown that GRU has\nsimilar performance compared to LSTM [60]. The structure of GRU is shown\nin Figure 2(a). Let xt ∈RP denotes the variables at time t, where 1 ≤t ≤T.\nAt each time t, GRU has a reset gate rj\nt and an update gate zj\nt for each of the\n16\nhidden state hj\nt. The update function of GRU is shown as follows:\nzt = σ (Wzxt + Uzht−1 + bz)\nrt = σ (Wrxt + Urht−1 + br)\n˜ht = tanh (W xt + U(rt ⊙ht−1) + b)\nht = (1 −zt) ⊙ht−1 + zt ⊙˜ht\nwhere matrices Wz, Wr, W , Uz, Ur, U and vectors bz, br, b are model parame-\nters.\nMultimodal Deep Learning Model (MMDL). As our benchmarking datasets\ncome from multiple tables and includes both temporal and non-temporal data,\nmultimodal deep learning models [61] can be used to shared learn representations\nfor the prediction tasks. Here, we propose a deep learning framework called\nas Multimodal Deep Learning Model (MMDL) to learn shared representations\nfrom multiple modalities using an ensemble of FFN and GRU deep learning\nmodels. The key idea is to use a shared representation layer to capture the\ncorrelations of modalities or to learn a similarity of modalities in representation\nspace, which is beneﬁcial when limited data is available from multiple modalities.\nData from each of the diﬀerent tables can be treated as a separate modality. For\nsimplicity, in MMDL, we treat all the temporal features as one modality and all\nnon-temporal features as another modality. Figure 2(b) shows an illustration of\nour MMDL framework with a common layer to learn the shared representations\nof modalities. MMDL uses FFN and GRU to handle non-temporal and temporal\nfeatures respectively, and learns their shared latent representations for prediction\ntasks.\n4.3. Implementation Details\nWe implement the Super Learner algorithm using R packages and Python\nlibraries listed in Table 7.\nThe deep learning models are implemented in\nTheano [62] and Keras [63] platforms.\nFor all the prediction methods, we\nconduct a ﬁve-fold cross validation and report the mean and standard error of\nperformance scores. We use Area under the ROC curve (AUROC) and Area\nunder Precision-Recall Curve (AUPRC) as the evaluation metrics to report the\nprediction model’s performance on classiﬁcation tasks, and use Mean Squared\nError (MSE) to report results on the regression task.\nAll the deep learning models are trained with RMSProp optimizer method\nwith learning rate of 0.001 on classiﬁcation tasks and 0.005 on regression tasks.\nThe batch size is chosen as 100 and the max epoch number is ﬁxed at 250.\nEarly stopping with best weight and batch normalization are applied during\ntraining. MMDL is a combination of FFN and GRU, in which FFN part handles\nnon-temporal features and GRU part handles temporal features. The structure\nof MMDL used in our experiments is shown in Figure A.5 in the Appendix.\nAll the data is divided to 5 folds with stratiﬁed cross validation, and stan-\ndardization is done to the whole dataset with the mean and standard error of\nthe training set. To enable reproducibility of our results, we will be releasing our\npreprocessing codes and benchmark prediction task codes on the Github soon.\n17\n4.4. Results\nIn this section, we report the benchmarking results of all the prediction\nalgorithms on the MIMIC-III datasets. We answer the following questions: (a)\nHow do the Deep Learning models compare to the Super Learner algorithm and\nscoring systems? (b) What is the performance of prediction methods on the\ndiﬀerent feature sets?\n4.4.1. Performance of Super Learner Algorithm Implementations\nFirst, we compare the performance of Super Learner-R and Super Learner-\nPython softwares on in-hospital mortality prediction task using feature set A\ni.e. 17 processed features collected in the ﬁrst 24 hours of ICU admission from\nMIMIC-III dataset. The result in Table 8 shows that Super Learner-Python\nperforms slightly better than Super Leaner-R implementation. Moreover, Super\nLearner-Python can be evaluated signiﬁcantly faster than Super Learner-R.\nThus, in the following experiments, we will only report the results of Super\nLearner-Python version (unless otherwise stated) to evaluate and benchmark\nSuper Learner algorithm on diﬀerent tasks.\nTable 8: Comparison of Super Learner-R and Super Learner-Python software versions\non in-hospital mortality prediction task using Feature set A extracted from the ﬁrst\n24-hour data of MIMIC-III. Running time refers to total time taken to perform cross-\nvalidation evaluation.\nAUROC Score\nAUPRC Score\nRunning Time\nSuperLearner-I\nR version\n0.8402 ± 0.0021\n0.4304 ± 0.0130\n36 hours\nPython version\n0.8448 ± 0.0038\n0.4351 ± 0.0139\n30 minutes\nSuperLearner-II\nR version\n0.8646 ± 0.0023\n0.4917 ± 0.0093\n28 hours\nPython version\n0.8701 ± 0.0053\n0.4991 ± 0.0107\n25 minutes\n4.4.2. Mortality Prediction Task Evaluation\nHere, we report the performance of all methods described in Section 4.2 on\nthe mortality prediction tasks for benchmark datasets MIMIC-III and MIMIC-III\n(CareVue). We report the mean and standard deviation of AUROC and AUPRC\nfor all the tasks.\nIn-hospital Mortality Prediction. Tables 9 and 10 show the in-hospital mortality\nprediction task results of all the prediction algorithms on Feature Set A of\nMIMIC-III and MIMIC-III (CareVue) datasets for both 24 hour and 48 hour\ndata. From these tables, we observe that deep learning models such as MMDL\nand RNN perform better than all the other models on 48-hour data. On 24-hour\ndata, we observe that Super Learner II model obtains slightly better results than\ndeep learning model.\nTables 11, 12, 13, and 14 show the in-hospital mortality prediction task results\non Features set B and C of MIMIC-III and MIMIC-III (CareVue) datasets on\n18\nthe 24-hour and 48-hour data. We observe that: (i) Super Learner performs\nbetter than all algorithms used in SuperLearner library, (ii) On both the Feature\nSet B and Feature Set C, the deep learning model (MMDL) obtains the best\nresults in terms of AUROC and AUPRC score, (iii) We can observe that the\nresults on ﬁrst 48-hour data are similar with those on ﬁrst 24-hour data, showing\nTable 9: In-hospital mortality task on MIMIC-III using feature set A.\nMethod\nAlgorithm\nFeature Set A, 24-hour data\nFeature Set A, 48-hour data\nAUROC Score\nAUPRC Score\nAUROC Score\nAUPRC Score\nScore Methods\nSAPS-II\n0.8035 ± 0.0044\n0.3586 ± 0.0052\n0.8046 ± 0.0083\n0.3373 ± 0.0141\nNew SAPS-II\n0.8235 ± 0.0042\n0.3989 ± 0.0120\n0.8252 ± 0.0036\n0.3823 ± 0.0119\nSOFA\n0.7322 ± 0.0038\n0.3191 ± 0.0085\n0.7347 ± 0.0094\n0.2852 ± 0.0167\nSuper Learner\nSL.glm\n0.8235 ± 0.0042\n0.3987 ± 0.0120\n0.8251 ± 0.0037\n0.3828 ± 0.0112\nSL.gbm\n0.8435 ± 0.0034\n0.4320 ± 0.0125\n0.8452 ± 0.0052\n0.4163 ± 0.0121\nSL.nnet\n0.8388 ± 0.0044\n0.4200 ± 0.0135\n0.8381 ± 0.0055\n0.3989 ± 0.0131\nSL.ipredbagg\n0.7556 ± 0.0064\n0.3104 ± 0.0084\n0.7510 ± 0.0078\n0.2811 ± 0.0121\nSL.randomforest\n0.7576 ± 0.0085\n0.3104 ± 0.0084\n0.7538 ± 0.0095\n0.2830 ± 0.0121\nSuperLearner-I\n0.8448 ± 0.0038\n0.4351 ± 0.0139\n0.8465 ± 0.0057\n0.4190 ± 0.0124\nSL.glm\n0.8024 ± 0.0043\n0.3804 ± 0.0043\n0.8013 ± 0.0021\n0.3559 ± 0.0238\nSL.gbm\n0.8628 ± 0.0037\n0.4840 ± 0.0078\n0.8518 ± 0.0049\n0.4259 ± 0.0209\nSL.nnet\n0.8490 ± 0.0079\n0.4587 ± 0.0058\n0.8383 ± 0.0058\n0.4028 ± 0.0180\nSL.ipredbagg\n0.8060 ± 0.0069\n0.4087 ± 0.0110\n0.7816 ± 0.0028\n0.3455 ± 0.0159\nSL.randomforest\n0.7977 ± 0.0079\n0.3958 ± 0.0124\n0.7813 ± 0.0059\n0.3496 ± 0.0200\nSuperLearner-II\n0.8673 ± 0.0045\n0.4968 ± 0.0097\n0.8595 ± 0.0035\n0.4422 ± 0.0200\nDeep Learning\nFFN\n0.8496 ± 0.0047\n0.4632 ± 0.0074\n0.8375 ± 0.0041\n0.4090 ± 0.0169\nRNN\n0.8544 ± 0.0053\n0.4519 ± 0.0145\n0.8618 ± 0.0059\n0.4458 ± 0.0144\nMMDL\n0.8664 ± 0.0056\n0.4776 ± 0.0162\n0.8737 ± 0.0045\n0.4714 ± 0.0176\nTable 10: In-hospital mortality task on MIMIC-III (Carvue) using feature set A.\nMethod\nAlgorithm\nFeature Set A, 24-hour data\nFeature Set A, 48-hour data\nAUROC Score\nAUPRC Score\nAUROC Score\nAUPRC Score\nScore Methods\nSAPS-II\n0.8005 ± 0.0080\n0.3625 ± 0.0065\n0.8030 ± 0.0132\n0.3448 ± 0.0219\nNew SAPS-II\n0.8217 ± 0.0047\n0.4037 ± 0.0069\n0.8226 ± 0.0129\n0.3873 ± 0.0163\nSOFA\n0.7263 ± 0.0100\n0.3273 ± 0.0067\n0.7309 ± 0.0105\n0.2996 ± 0.0199\nSuper Learner\nSL.glm\n0.8212 ± 0.0052\n0.4018 ± 0.0068\n0.8227 ± 0.0132\n0.3883 ± 0.0170\nSL.gbm\n0.8405 ± 0.0056\n0.4377 ± 0.0112\n0.8414 ± 0.0111\n0.4187 ± 0.0315\nSL.nnet\n0.8332 ± 0.0041\n0.4182 ± 0.0059\n0.8309 ± 0.0061\n0.3905 ± 0.0253\nSL.ipredbagg\n0.7567 ± 0.0040\n0.3063 ± 0.0098\n0.7483 ± 0.0167\n0.2921 ± 0.0211\nSL.randomforest\n0.7553 ± 0.0058\n0.3005 ± 0.0121\n0.7538 ± 0.0150\n0.2914 ± 0.0154\nSuperLearner-I\n0.8417 ± 0.0052\n0.4387 ± 0.0122\n0.8415 ± 0.0096\n0.4169 ± 0.0305\nSL.glm\n0.8027 ± 0.0038\n0.3931 ± 0.0105\n0.8009 ± 0.0149\n0.3683 ± 0.0209\nSL.gbm\n0.8581 ± 0.0062\n0.4810 ± 0.0126\n0.8457 ± 0.0080\n0.4349 ± 0.0233\nSL.nnet\n0.8461 ± 0.0103\n0.4674 ± 0.0173\n0.8238 ± 0.0157\n0.4017 ± 0.0412\nSL.ipredbagg\n0.7921 ± 0.0077\n0.3850 ± 0.0180\n0.7782 ± 0.0052\n0.3434 ± 0.0213\nSL.randomforest\n0.7930 ± 0.0063\n0.3830 ± 0.0091\n0.7733 ± 0.0115\n0.3455 ± 0.0253\nSuperLearner-II\n0.8651 ± 0.0075\n0.4964 ± 0.0135\n0.8520 ± 0.0101\n0.4493 ± 0.0246\nDeep Learning\nFFN\n0.8488 ± 0.0082\n0.4702 ± 0.0168\n0.8326 ± 0.0112\n0.4109 ± 0.0193\nRNN\n0.8456 ± 0.0032\n0.4505 ± 0.0091\n0.8485 ± 0.0090\n0.4246 ± 0.0214\nMMDL\n0.8561 ± 0.0045\n0.4764 ± 0.0144\n0.8564 ± 0.0107\n0.4520 ± 0.0305\n19\nTable 11: In-hospital mortality task on MIMIC-III using feature set B.\nMethod\nAlgorithm\nFeature Set B, 24-hour data\nFeature Set B, 48-hour data\nAUROC Score\nAUPRC Score\nAUROC Score\nAUPRC Score\nSuper Learner\nSL.glm\n0.7745 ± 0.0055\n0.3134 ± 0.0112\n0.7869 ± 0.0015\n0.3103 ± 0.0212\nSL.gbm\n0.8381 ± 0.0057\n0.4059 ± 0.0156\n0.8398 ± 0.0044\n0.3932 ± 0.0155\nSL.nnet\n0.8170 ± 0.0036\n0.3650 ± 0.0124\n0.8232 ± 0.0074\n0.3591 ± 0.0135\nSL.ipredbagg\n0.7641 ± 0.0070\n0.3127 ± 0.0085\n0.7627 ± 0.0118\n0.3011 ± 0.0140\nSL.randomforest\n0.7582 ± 0.0080\n0.3100 ± 0.0116\n0.7604 ± 0.0042\n0.2895 ± 0.0138\nSuperLearner-II\n0.8426 ± 0.0068\n0.4160 ± 0.0136\n0.8471 ± 0.0036\n0.4055 ± 0.0155\nDeep Learning\nMMDL\n0.8730 ± 0.0065\n0.4765 ± 0.0109\n0.8783 ± 0.0037\n0.4706 ± 0.0178\nTable 12: In-hospital mortality task on MIMIC-III using feature set C.\nMethod\nAlgorithm\nFeature Set C, 24-hour data\nFeature Set C, 48-hour data\nAUROC Score\nAUPRC Score\nAUROC Score\nAUPRC Score\nSuper Learner\nSL.glm\n0.8341 ± 0.0072\n0.4045 ± 0.0164\n0.8594 ± 0.0079\n0.4254 ± 0.0196\nSL.gbm\n0.8628 ± 0.0056\n0.4705 ± 0.0138\n0.8833 ± 0.0054\n0.4954 ± 0.0223\nSL.nnet\n0.7568 ± 0.0106\n0.3424 ± 0.0139\n0.7973 ± 0.0060\n0.3690 ± 0.0154\nSL.ipredbagg\n0.7895 ± 0.0077\n0.3664 ± 0.0099\n0.8074 ± 0.0100\n0.3796 ± 0.0282\nSL.randomforest\n0.7720 ± 0.0054\n0.3427 ± 0.0045\n0.7945 ± 0.0081\n0.3616 ± 0.0143\nSuperLearner-II\n0.8664 ± 0.0058\n0.4821 ± 0.0142\n0.8875 ± 0.0055\n0.5059 ± 0.0214\nDeep Learning\nMMDL\n0.9410 ± 0.0082\n0.7857 ± 0.0132\n0.9401 ± 0.0099\n0.7721 ± 0.0078\nthat a longer record length helps little on the in-hospital mortality prediction\ntask.\nFrom the in-hospital mortality prediction task results, we make the following\nobservations: (i) Deep learning models (MMDL) outperform all the other models\nwhen the raw features (Feature set B and C) are used for evaluation, (ii) All\nthe models perform much better when more features are used for prediction, i.e.\nmodels perform better on Feature set C which has 135 raw features compared\nto Feature Set B which has 20 features. This implies that deep models can\nlearn better feature representations from multiple data modalities (instead of\nusing hand-picked features as in Feature Set A) which results in obtaining better\nprediction results on the in-hospital mortality benchmark task. The comparisons\nof SuperLearner-II and MMDL on three feature sets shown in Figures 3 and 4\nvalidate our observations. From these ﬁgues, we see that on Feature set C, deep\nlearning models obtain around 7-8% and 50% improvement over SuperLearner\nmodels for AUROC and AUPRC respectively. Also, deep learning models obtain\n8% improvement for Feature set C compared to Feature set A.\nShort-term and Long-term Mortality Prediction. Tables 15, 16, 17, and 18 show\nthe short-term and long-term mortality prediction task results on all the feature\nsets of MIMIC-III dataset on the 24-hour and 48-hour data. We observe that:\n(i) Super Learner-II and MMDL deep learning models have similar performance\non the feature set A for both short-term and long-term mortality prediction, and\nboth these algorithms perform better than all other prediction algorithms, (ii)\nOn both the Feature Set B and Feature Set C, the MMDL deep learning model\n20\nTable 13: In-hospital mortality task on MIMIC-III (CareVue) using feature set B.\nMethod\nAlgorithm\nFeature Set B, 24-hour data\nFeature Set B, 48-hour data\nAUROC Score\nAUPRC Score\nAUROC Score\nAUPRC Score\nSuper Learner\nSL.glm\n0.7724 ± 0.0129\n0.3177 ± 0.0185\n0.7872 ± 0.0139\n0.3253 ± 0.0236\nSL.gbm\n0.8284 ± 0.0064\n0.4048 ± 0.0110\n0.8298 ± 0.0089\n0.3823 ± 0.0189\nSL.nnet\n0.8133 ± 0.0060\n0.3740 ± 0.0169\n0.7976 ± 0.0201\n0.3339 ± 0.0305\nSL.ipredbagg\n0.7525 ± 0.0066\n0.3158 ± 0.0152\n0.7536 ± 0.0136\n0.2991 ± 0.0197\nSL.randomforest\n0.7497 ± 0.0118\n0.3123 ± 0.0110\n0.7502 ± 0.0094\n0.2952 ± 0.0074\nSuperLearner-II\n0.8347 ± 0.0062\n0.4164 ± 0.0148\n0.8340 ± 0.0099\n0.3907 ± 0.0128\nDeep Learning\nMMDL\n0.8617 ± 0.0074\n0.4612 ± 0.0245\n0.8633 ± 0.0080\n0.4425 ± 0.0166\nTable 14: In-hospital mortality task on MIMIC-III (CareVue) using feature set C.\nMethod\nAlgorithm\nFeature Set C, 24-hour data\nFeature Set C, 48-hour data\nAUROC Score\nAUPRC Score\nAUROC Score\nAUPRC Score\nSuper Learner\nSL.glm\n0.8282 ± 0.0089\n0.3931 ± 0.0136\n0.8497 ± 0.0088\n0.4128 ± 0.0087\nSL.gbm\n0.8550 ± 0.0049\n0.4605 ± 0.0209\n0.8753 ± 0.0026\n0.4846 ± 0.0042\nSL.nnet\n0.7412 ± 0.0067\n0.3216 ± 0.0127\n0.7946 ± 0.0132\n0.3535 ± 0.0198\nSL.ipredbagg\n0.7737 ± 0.0064\n0.3216 ± 0.0127\n0.7976 ± 0.0065\n0.3696 ± 0.0127\nSL.randomforest\n0.7690 ± 0.0080\n0.3435 ± 0.0184\n0.7900 ± 0.0101\n0.3558 ± 0.0132\nSuperLearner-II\n0.8592 ± 0.0055\n0.4694 ± 0.0207\n0.8808 ± 0.0030\n0.4945 ± 0.0054\nDeep Learning\nMMDL\n0.9200 ± 0.0213\n0.7546 ± 0.0297\n0.9251 ± 0.0120\n0.7451 ± 0.0093\nconsistently obtains the best results in terms of AUROC and AUPRC score, (iii)\nall models obtain better AUPRC scores on the long-term mortality prediction\ntask compared to the short-term mortality prediction task.\nSL-II\nMMDL\n0.80\n0.85\n0.90\n0.95\nAUROC Score\nFirst 24-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nSL-II\nMMDL\n0.40\n0.55\n0.70\nAUPRC Score\nFirst 24-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nSL-II\nMMDL\n0.80\n0.85\n0.90\n0.95\nAUROC Score\nFirst 48-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nSL-II\nMMDL\n0.40\n0.55\n0.70\nAUPRC Score\nFirst 48-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nFigure 3: In-hospital mortality task on MIMIC-III data.\nSL-II\nMMDL\n0.80\n0.85\n0.90\n0.95\nAUROC Score\nFirst 24-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nSL-II\nMMDL\n0.40\n0.55\n0.70\nAUPRC Score\nFirst 24-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nSL-II\nMMDL\n0.80\n0.85\n0.90\n0.95\nAUROC Score\nFirst 48-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nSL-II\nMMDL\n0.40\n0.55\n0.70\nAUPRC Score\nFirst 48-hour data.\nFeature Set A\nFeature Set B\nFeature Set C\nFigure 4: In-hospital mortality task on MIMIC-III (CareVue) data.\n21\nTable 15: AUROC scores of short-term and long-term mortality prediction tasks on\nMIMIC-III with 24-hour data.\nFeature Set\nAlgorithm\nAUROC Score\n2-day Mortality\n3-day Mortality\n30-day Mortality\n1-year Mortality\nFeature Set A\nSAPS-II Score\n0.8453 ± 0.0088\n0.8218 ± 0.0057\n0.7921 ± 0.0051\n0.7614 ± 0.0035\nNew SAPS-II Score\n0.8575 ± 0.0075\n0.8370 ± 0.0053\n0.8148 ± 0.0035\n0.8042 ± 0.0013\nSOFA Score\n0.7559 ± 0.0276\n0.7412 ± 0.0076\n0.7041 ± 0.0074\n0.6611 ± 0.0036\nSuperLearner-I\n0.8808 ± 0.0063\n0.8627 ± 0.0079\n0.8384 ± 0.0031\n0.8260 ± 0.0019\nSuperLearner-II\n0.8851 ± 0.0105\n0.8770 ± 0.0094\n0.8620 ± 0.0063\n0.8467 ± 0.0022\nFFN\n0.8673 ± 0.0069\n0.8493 ± 0.0128\n0.8475 ± 0.0050\n0.8390 ± 0.0019\nRNN\n0.8773 ± 0.0117\n0.8612 ± 0.0083\n0.8326 ± 0.0085\n0.7958 ± 0.0026\nMMDL\n0.8815 ± 0.0102\n0.8725 ± 0.0063\n0.8585 ± 0.0059\n0.8450 ± 0.0019\nFeature Set B\nSuperLearner-II\n0.8667 ± 0.0097\n0.8535 ± 0.0128\n0.8395 ± 0.0031\n0.8347 ± 0.0046\nMMDL\n0.8862 ± 0.0059\n0.8769 ± 0.0107\n0.8620 ± 0.0072\n0.8452 ± 0.0008\nFeature Set C\nSuperLearner-II\n0.8837 ± 0.0047\n0.8746 ± 0.0073\n0.8629 ± 0.0033\n0.8589 ± 0.0032\nMMDL\n0.9084 ± 0.0207\n0.9295 ± 0.0225\n0.9169 ± 0.0054\n0.8872 ± 0.0084\nTable 16: AUPRC scores of short-term and long-term mortality prediction tasks on\nMIMIC-III with 24-hour data.\nFeature Set\nAlgorithm\nAUPRC Score\n2-day Mortality\n3-day Mortality\n30-day Mortality\n1-year Mortality\nFeature Set A\nSAPS-II Score\n0.1361 ± 0.0153\n0.1730 ± 0.0214\n0.4140 ± 0.0131\n0.5084 ± 0.0154\nNew SAPS-II Score\n0.1587 ± 0.0226\n0.1919 ± 0.0234\n0.4589 ± 0.0125\n0.5778 ± 0.0109\nSOFA Score\n0.1027 ± 0.0278\n0.1373 ± 0.0201\n0.3497 ± 0.0167\n0.4176 ± 0.0088\nSuperLearner-I\n0.1967 ± 0.0205\n0.2219 ± 0.0263\n0.5053 ± 0.0173\n0.6258 ± 0.0073\nSuperLearner-II\n0.2463 ± 0.0111\n0.2775 ± 0.0382\n0.5652 ± 0.0186\n0.6609 ± 0.0090\nFFN\n0.2429 ± 0.0332\n0.2449 ± 0.0315\n0.5367 ± 0.0199\n0.6453 ± 0.0081\nRNN\n0.2491 ± 0.0293\n0.2752 ± 0.0164\n0.5028 ± 0.0178\n0.5725 ± 0.0062\nMMDL\n0.2529 ± 0.0338\n0.2839 ± 0.0207\n0.5483 ± 0.0187\n0.6485 ± 0.0099\nFeature Set B\nSuperLearner-II\n0.1767 ± 0.0319\n0.2173 ± 0.0266\n0.4926 ± 0.0090\n0.6328 ± 0.0100\nMMDL\n0.2475 ± 0.0364\n0.1863 ± 0.0273\n0.5458 ± 0.0231\n0.6457 ± 0.0082\nFeature Set C\nSuperLearner-II\n0.2048 ± 0.0085\n0.2717 ± 0.0321\n0.5530 ± 0.0096\n0.6764 ± 0.0056\nMMDL\n0.3831 ± 0.0336\n0.5139 ± 0.0193\n0.7668 ± 0.0170\n0.7690 ± 0.0077\n4.4.3. ICD-9 Code Prediction Task Evaluation\nTables 19 and 20 show the performance (AUPRC and AUROC scores) of all\nmethods for the ﬁrst 24-hour data of MIMIC-III on ICD-9 code prediction task.\nWe observe that the MMDL deep models trained on Feature Set C outperforms\nSuper Learner models trained on Feature Sets A, B, and C on almost all the\nICD-9 Code prediction task, and on an average obtains 4-5% improvement.\n4.4.4. Length of Stay Prediction Task Evaluation\nTable 21 shows the performance measured by Mean Squared Error (MSE)\nof all methods on the task of forecasting length of stay task with ﬁrst 24-hour\ndata and ﬁrst 48-hour data of MIMIC-III dataset. We observe that (i) all\ndeep learning models such as FFN, RRN and MMDL trained on Feature set\nC outperform Super Learner models trained on Feature sets A,B, and C. (ii)\nMMDL model obtains best performance in terms of mean squared error (in\nhours), and signiﬁcantly outperforms Super Learner II by nearly 50%.\n22\nTable 17: AUROC scores of short-term and long-term mortality prediction tasks on\nMIMIC-III with 48-hour data.\nFeature Set\nAlgorithm\nAUROC Score\n3-day Mortality\n30-day Mortality\n1-year Mortality\nFeature Set A\nSAPS-II Score\n0.8366 ± 0.0109\n0.7841 ± 0.0072\n0.7490 ± 0.0041\nNew SAPS-II Score\n0.8471 ± 0.0072\n0.8104 ± 0.0047\n0.7991 ± 0.0037\nSOFA Score\n0.7465 ± 0.0179\n0.6953 ± 0.0104\n0.6454 ± 0.0052\nSuperLearner-I\n0.8675 ± 0.0046\n0.8364 ± 0.0033\n0.8222 ± 0.0047\nSuperLearner-II\n0.8706 ± 0.0095\n0.8531 ± 0.0043\n0.8409 ± 0.0031\nFFN\n0.8466 ± 0.0186\n0.8385 ± 0.0061\n0.8309 ± 0.0048\nRNN\n0.8633 ± 0.0116\n0.8374 ± 0.0087\n0.7966 ± 0.0036\nMMDL\n0.8596 ± 0.0124\n0.8612 ± 0.0059\n0.8418 ± 0.0049\nFeature Set B\nSuperLearner-II\n0.8448 ± 0.0162\n0.8427 ± 0.0071\n0.8360 ± 0.0057\nMMDL\n0.8682 ± 0.0240\n0.8628 ± 0.0111\n0.8438 ± 0.0053\nFeature Set C\nSuperLearner-II\n0.8473 ± 0.0114\n0.8802 ± 0.0037\n0.8673 ± 0.0051\nMMDL\n0.8713 ± 0.0494\n0.9173 ± 0.0064\n0.8702 ± 0.0054\nTable 18: AUPRC scores of short-term and long-term mortality prediction tasks on\nMIMIC-III with 48-hour data.\nFeature Set\nAlgorithm\nAUPRC Score\n3-day Mortality\n30-day Mortality\n1-year Mortality\nFeature Set A\nSAPS-II Score\n0.1082 ± 0.0150\n0.3849 ± 0.0118\n0.4845 ± 0.0092\nNew SAPS-II Score\n0.1307 ± 0.0225\n0.4342 ± 0.0119\n0.5647 ± 0.0090\nSOFA Score\n0.0663 ± 0.0092\n0.3156 ± 0.0143\n0.3898 ± 0.0115\nSuperLearner-I\n0.1344 ± 0.0247\n0.4898 ± 0.0139\n0.6171 ± 0.0088\nSuperLearner-II\n0.1955 ± 0.0245\n0.5255 ± 0.0152\n0.6448 ± 0.0084\nFFN\n0.1672 ± 0.0331\n0.4962 ± 0.0153\n0.6272 ± 0.0116\nRNN\n0.2371 ± 0.0336\n0.4974 ± 0.0149\n0.5691 ± 0.0080\nMMDL\n0.2131 ± 0.0344\n0.5423 ± 0.0164\n0.6421 ± 0.0116\nFeature Set B\nSuperLearner-II\n0.1225 ± 0.0286\n0.4892 ± 0.0197\n0.6297 ± 0.0067\nMMDL\n0.1659 ± 0.0434\n0.5290 ± 0.0372\n0.6444 ± 0.0133\nFeature Set C\nSuperLearner-II\n0.0771 ± 0.0125\n0.5479 ± 0.0079\n0.6870 ± 0.0038\nMMDL\n0.1510 ± 0.0246\n0.7314 ± 0.0149\n0.7344 ± 0.0062\n4.4.5. Computation Time\nOur Python implementation of the Super Learner algorithm took about\n25-30 mins for evaluating the in-hospital mortality task using Feature Set A,\nand it took about 3 hours for the Feature Set C. A deep Feed forward neural\n(FFN) network implemented using Keras took around 90 and 100 minutes for\nevaluating the same mortality task using Feature sets A and C respectively,\nwhile the MMDL model (shown in Figure A.5) took around 30 minutes and 1\nhour for Feature sets A and C respectively. All our experiments was run on a\n32-core Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz machine with NVIDIA\nTITAN-X GPU processor.\n23\nTable 19: ICD-9 code prediction AUPRC scores on MIMIC-III with ﬁrst 24-hour data.\nICD-9\nTask\nSuper Learner\nDeep Learning\nSuper Learner-I\nSuper Learner-II\nSuper Learner-II\nSuper Learner-II\nFFN\nRNN\nMMDL\non Feature Set A\non Feature Set A\non Feature Set B\non Feature Set C\non Feature Set C\non Feature Set C\non Feature Set C\n1\n0.5356 ± 0.0027\n0.5861 ± 0.0090\n0.5695 ± 0.0075\n0.6273 ± 0.0077\n0.5807 ± 0.0072\n0.5978 ± 0.0211\n0.6491 ± 0.0121\n2\n0.7290 ± 0.0141\n0.7512 ± 0.0130\n0.7478 ± 0.0097\n0.7756 ± 0.0134\n0.7422 ± 0.0064\n0.4715 ± 0.0132\n0.8024 ± 0.0192\n3\n0.8095 ± 0.0057\n0.8235 ± 0.0054\n0.8302 ± 0.0057\n0.8631 ± 0.0042\n0.8377 ± 0.0055\n0.8614 ± 0.0022\n0.8690 ± 0.0115\n4\n0.5454 ± 0.0072\n0.5850 ± 0.0065\n0.5836 ± 0.0037\n0.6882 ± 0.0128\n0.6556 ± 0.0162\n0.6812 ± 0.0125\n0.7149 ± 0.0180\n5\n0.4714 ± 0.0047\n0.5168 ± 0.0073\n0.5218 ± 0.0044\n0.5624 ± 0.0019\n0.5058 ± 0.0100\n0.5220 ± 0.0048\n0.5590 ± 0.0212\n6\n0.4112 ± 0.0046\n0.4348 ± 0.0064\n0.4507 ± 0.0067\n0.5155 ± 0.0074\n0.4566 ± 0.0175\n0.5261 ± 0.0076\n0.5624 ± 0.0112\n7\n0.9547 ± 0.0026\n0.9596 ± 0.0025\n0.9585 ± 0.0019\n0.9701 ± 0.0018\n0.9592 ± 0.0013\n0.9552 ± 0.0040\n0.9729 ± 0.0021\n8\n0.6960 ± 0.0061\n0.7366 ± 0.0045\n0.7306 ± 0.0079\n0.7649 ± 0.0038\n0.7493 ± 0.0050\n0.8067 ± 0.0062\n0.8290 ± 0.0113\n9\n0.5814 ± 0.0080\n0.6350 ± 0.0107\n0.6326 ± 0.0087\n0.6961 ± 0.0091\n0.6393 ± 0.0033\n0.6748 ± 0.0106\n0.7034 ± 0.0148\n10\n0.7213 ± 0.0034\n0.7442 ± 0.0047\n0.7328 ± 0.0063\n0.7928 ± 0.0053\n0.7715 ± 0.0036\n0.8071 ± 0.0020\n0.8227 ± 0.0112\n11\n0.0700 ± 0.0180\n0.0965 ± 0.0193\n0.0857 ± 0.0082\n0.2303 ± 0.0345\n0.1498 ± 0.0565\n0.1623 ± 0.0504\n0.4888 ± 0.0631\n12\n0.1629 ± 0.0106\n0.1802 ± 0.0109\n0.1862 ± 0.0138\n0.2146 ± 0.0213\n0.1930 ± 0.0117\n0.2019 ± 0.0149\n0.3155 ± 0.0421\n13\n0.2436 ± 0.0070\n0.2551 ± 0.0062\n0.2554 ± 0.0051\n0.3144 ± 0.0074\n0.2641 ± 0.0078\n0.2945 ± 0.0155\n0.3435 ± 0.0293\n14\n0.1318 ± 0.0177\n0.1357 ± 0.0219\n0.1370 ± 0.0198\n0.1329 ± 0.0183\n0.1134 ± 0.0225\n0.0775 ± 0.0068\n0.1918 ± 0.0216\n15\n0.4745 ± 0.0061\n0.5036 ± 0.0032\n0.4908 ± 0.0061\n0.5343 ± 0.0096\n0.4848 ± 0.0096\n0.5182 ± 0.0079\n0.5564 ± 0.0195\n16\n0.1164 ± 0.0057\n0.1279 ± 0.0082\n0.1264 ± 0.0080\n0.1711 ± 0.0097\n0.1488 ± 0.0093\n0.1433 ± 0.0104\n0.2244 ± 0.0279\n17\n0.0632 ± 0.0113\n0.0649 ± 0.0028\n0.0736 ± 0.0048\n0.0913 ± 0.0101\n0.0742 ± 0.0055\n0.0604 ± 0.0072\n0.3700 ± 0.0577\n18\n0.5934 ± 0.0069\n0.6302 ± 0.0053\n0.6300 ± 0.0059\n0.6826 ± 0.0072\n0.6430 ± 0.0133\n0.6675 ± 0.0068\n0.7037 ± 0.0246\n19\n0.5946 ± 0.0029\n0.6205 ± 0.0053\n0.6361 ± 0.0065\n0.7138 ± 0.0076\n0.6684 ± 0.0041\n0.6951 ± 0.0049\n0.7184 ± 0.0086\n20\n0.4820 ± 0.0069\n0.5220 ± 0.0128\n0.5382 ± 0.0054\n0.6006 ± 0.0103\n0.5506 ± 0.0066\n0.5723 ± 0.0041\n0.6184 ± 0.0208\nAverage\n0.4694 ± 0.0076\n0.4955 ± 0.0083\n0.4959 ± 0.0073\n0.5471 ± 0.0102\n0.5094 ± 0.0111\n0.5148 ± 0.0107\n0.6008 ± 0.0224\n24\nTable 20: ICD-9 code prediction AUROC scores on MIMIC-III with ﬁrst 24-hour data.\nICD-9\nTask\nSuper Learner\nDeep Learning\nSuper Learner-I\nSuper Learner-II\nSuper Learner-II\nSuper Learner-II\nFFN\nRNN\nMMDL\non Feature Set A\non Feature Set A\non Feature Set B\non Feature Set C\non Feature Set C\non Feature Set C\non Feature Set C\n1\n0.7371 ± 0.0023\n0.7757 ± 0.0049\n0.7635 ± 0.0018\n0.7971 ± 0.0044\n0.7643 ± 0.0047\n0.7879 ± 0.0133\n0.8194 ± 0.0078\n2\n0.8342 ± 0.0091\n0.8530 ± 0.0082\n0.8472 ± 0.0056\n0.8770 ± 0.0092\n0.8467 ± 0.0062\n0.7651 ± 0.0067\n0.8912 ± 0.0154\n3\n0.6835 ± 0.0073\n0.7022 ± 0.0070\n0.7108 ± 0.0074\n0.7532 ± 0.0061\n0.7203 ± 0.0055\n0.7460 ± 0.0059\n0.7604 ± 0.0158\n4\n0.6714 ± 0.0052\n0.7048 ± 0.0037\n0.7038 ± 0.0023\n0.7814 ± 0.0049\n0.7575 ± 0.0067\n0.7831 ± 0.0038\n0.8048 ± 0.0131\n5\n0.6540 ± 0.0036\n0.6833 ± 0.0047\n0.6825 ± 0.0042\n0.7163 ± 0.0036\n0.6680 ± 0.0082\n0.6921 ± 0.0032\n0.7184 ± 0.0169\n6\n0.6222 ± 0.0028\n0.6505 ± 0.0068\n0.6526 ± 0.0027\n0.7107 ± 0.0061\n0.6651 ± 0.0071\n0.7109 ± 0.0061\n0.7387 ± 0.0122\n7\n0.8270 ± 0.0071\n0.8457 ± 0.0067\n0.8424 ± 0.0073\n0.8753 ± 0.0060\n0.8435 ± 0.0052\n0.8339 ± 0.0068\n0.8942 ± 0.0045\n8\n0.6976 ± 0.0049\n0.7340 ± 0.0038\n0.7315 ± 0.0066\n0.7636 ± 0.0027\n0.7447 ± 0.0029\n0.7957 ± 0.0066\n0.8148 ± 0.0093\n9\n0.6518 ± 0.0063\n0.6980 ± 0.0093\n0.6993 ± 0.0075\n0.7512 ± 0.0107\n0.7023 ± 0.0047\n0.7298 ± 0.0102\n0.7527 ± 0.0105\n10\n0.7807 ± 0.0024\n0.7978 ± 0.0039\n0.7881 ± 0.0041\n0.8266 ± 0.0043\n0.8089 ± 0.0016\n0.8354 ± 0.0022\n0.8512 ± 0.0105\n11\n0.9454 ± 0.0094\n0.9635 ± 0.0060\n0.9592 ± 0.0038\n0.9692 ± 0.0081\n0.8200 ± 0.0778\n0.8268 ± 0.0322\n0.9035 ± 0.0450\n12\n0.6340 ± 0.0151\n0.6578 ± 0.0117\n0.6638 ± 0.0161\n0.6969 ± 0.0189\n0.6670 ± 0.0068\n0.6554 ± 0.0132\n0.7378 ± 0.0141\n13\n0.5908 ± 0.0067\n0.6078 ± 0.0049\n0.6141 ± 0.0040\n0.6727 ± 0.0055\n0.6185 ± 0.0060\n0.6409 ± 0.0088\n0.6873 ± 0.0133\n14\n0.7170 ± 0.0172\n0.7172 ± 0.0157\n0.7204 ± 0.0211\n0.7166 ± 0.0143\n0.7003 ± 0.0128\n0.6191 ± 0.0172\n0.7346 ± 0.0251\n15\n0.6507 ± 0.0046\n0.6774 ± 0.0048\n0.6603 ± 0.0050\n0.6938 ± 0.0028\n0.6583 ± 0.0057\n0.6897 ± 0.0063\n0.7148 ± 0.0135\n16\n0.5901 ± 0.0156\n0.6200 ± 0.0117\n0.6176 ± 0.0144\n0.6615 ± 0.0192\n0.6243 ± 0.0121\n0.6300 ± 0.0116\n0.7011 ± 0.0207\n17\n0.6769 ± 0.0211\n0.6917 ± 0.0094\n0.6991 ± 0.0074\n0.7530 ± 0.0236\n0.6976 ± 0.0151\n0.6144 ± 0.0223\n0.8104 ± 0.0381\n18\n0.6405 ± 0.0076\n0.6683 ± 0.0067\n0.6676 ± 0.0066\n0.7050 ± 0.0076\n0.6722 ± 0.0115\n0.7007 ± 0.0057\n0.7319 ± 0.0240\n19\n0.6385 ± 0.0054\n0.6619 ± 0.0068\n0.6762 ± 0.0066\n0.7358 ± 0.0052\n0.6931 ± 0.0023\n0.7127 ± 0.0019\n0.7348 ± 0.0070\n20\n0.6263 ± 0.0071\n0.6538 ± 0.0095\n0.6656 ± 0.0043\n0.7175 ± 0.0089\n0.6719 ± 0.0053\n0.7108 ± 0.0054\n0.7426 ± 0.0136\nAverage\n0.6935 ± 0.0080\n0.7182 ± 0.0073\n0.7183 ± 0.0069\n0.7587 ± 0.0086\n0.7172 ± 0.0104\n0.7240 ± 0.0095\n0.7772 ± 0.0165\n25\nTable 21: Length of stay task on MIMIC-III with ﬁrst 24/48-hour data. Mean squared\nerror (MSE) shown is in hours.\nModel and feature set\nFirst 24-hour data\nFirst 48-hour data\nSuper Learner\nSuper Learner-I on Feature Set A\n56 420.6077 ± 3739.0173\n58 561.0081 ± 4223.1785\nSuper Learner-II on Feature Set A\n54 593.3317 ± 3265.8292\n57 454.7028 ± 4349.3598\nSuper Learner-II on Feature Set B\n55 844.4209 ± 3248.3224\n54 666.0875 ± 4859.4577\nSuper Learner-II on Feature Set C\n54 608.1099 ± 2923.9972\n54 400.5845 ± 1582.4523\nDeep Learning\nFFN on Feature Set C\n53 410.0918 ± 3207.9849\n52 642.6508 ± 4373.4239\nRNN on Feature Set C\n48 702.7641 ± 3768.5154\n49 556.8024 ± 3794.0471\nMMDL on Feature Set C\n36 338.2015 ± 2672.3832\n36 924.2312 ± 3566.4318\n5. Summary\nIn this paper, we presented exhaustive benchmarking evaluation results of\ndeep learning models, several machine learning models and ICU scoring systems\non various clinical prediction tasks using the publicly available MIMIC-III\ndatasets. We demonstrated that deep learning models consistently outperform\nall the other approaches especially when a large number of raw clinical time\nseries data is used as input features to the prediction models.\nAcknowledgments\nThis material is based upon work supported by the NSF research grants\nIIS-1134990, IIS-1254206, and Samsung GRO Grant. Any opinions, ﬁndings,\nand conclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reﬂect the views of the funding agencies.\nReferences\n[1] Z. Che, S. Purushotham, K. Cho, D. Sontag, Y. Liu, Recurrent neural\nnetworks for multivariate time series with missing values, arXiv preprint\narXiv:1606.01865 (2016).\n[2] H. Harutyunyan, H. Khachatrian, D. C. Kale, A. Galstyan, Multitask\nlearning and benchmarking with clinical time series data, arXiv preprint\narXiv:1703.07771 (2017).\n[3] J.-R. Le Gall, S. Lemeshow, F. Saulnier, A new simpliﬁed acute physiology\nscore (saps ii) based on a european/north american multicenter study, Jama\n270 (1993) 2957–2963.\n[4] J. Lee, D. J. Scott, M. Villarroel, G. D. Cliﬀord, M. Saeed, R. G. Mark,\nOpen-access mimic-ii database for intensive care research, in: Engineering in\nMedicine and Biology Society, EMBC, 2011 Annual International Conference\nof the IEEE, IEEE, 2011, pp. 8315–8318.\n[5] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghas-\nsemi, B. Moody, P. Szolovits, L. A. Celi, R. G. Mark, Mimic-iii, a freely\naccessible critical care database, Scientiﬁc data 3 (2016).\n26\n[6] A. E. Johnson, T. J. Pollard, R. G. Mark, Reproducibility in critical care: a\nmortality prediction case study, Machine Learning for Healthcare (MLHC)\n(2017).\n[7] Z. Che, D. Kale, W. Li, M. T. Bahadori, Y. Liu, Deep computational\nphenotyping, in: Proceedings of the 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, ACM, 2015, pp.\n507–516.\n[8] Z. Che, S. Purushotham, R. Khemani, Y. Liu, Interpretable deep models\nfor icu outcome prediction, in: AMIA Annual Symposium Proceedings,\nvolume 2016, American Medical Informatics Association, 2016, p. 371.\n[9] K. L. Caballero Barajas, R. Akella, Dynamically modeling patient’s health\nstate from electronic medical records: a time series approach, in: Proceedings\nof the 21th ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, ACM, 2015, pp. 69–78.\n[10] J. Calvert, Q. Mao, A. J. Rogers, C. Barton, M. Jay, T. Desautels, H. Mo-\nhamadlou, J. Jan, R. Das, A computational approach to mortality prediction\nof alcohol use disorder inpatients, Computers in biology and medicine 75\n(2016) 74–79.\n[11] L. A. Celi, S. Galvin, G. Davidzon, J. Lee, D. Scott, R. Mark, A database-\ndriven decision support system: customized mortality prediction, Journal\nof personalized medicine 2 (2012) 138–148.\n[12] M. Ghassemi, M. A. Pimentel, T. Naumann, T. Brennan, D. A. Clifton,\nP. Szolovits, M. Feng, A multivariate timeseries modeling approach to sever-\nity of illness assessment and forecasting in icu with sparse, heterogeneous\nclinical data., in: AAAI, 2015, pp. 446–453.\n[13] M. Ghassemi, T. Naumann, F. Doshi-Velez, N. Brimmer, R. Joshi,\nA. Rumshisky, P. Szolovits, Unfolding physiological state: Mortality mod-\nelling in intensive care units, in: Proceedings of the 20th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, ACM,\n2014, pp. 75–84.\n[14] M. Hoogendoorn, A. el Hassouni, K. Mok, M. Ghassemi, P. Szolovits,\nPrediction using patient comparison vs. modeling: A case study for mortality\nprediction, in: Engineering in Medicine and Biology Society (EMBC), 2016\nIEEE 38th Annual International Conference of the, IEEE, 2016, pp. 2464–\n2467.\n[15] R. Joshi, P. Szolovits, Prognostic physiology: modeling patient severity in\nintensive care units using radial domain folding, in: AMIA Annual Sympo-\nsium Proceedings, volume 2012, American Medical Informatics Association,\n2012, p. 1276.\n27\n[16] J. Lee, D. M. Maslove, Customization of a severity of illness score using\nlocal electronic medical record data, Journal of intensive care medicine 32\n(2017) 38–47.\n[17] L.-w. Lehman, M. Saeed, W. Long, J. Lee, R. Mark, Risk stratiﬁcation of\nicu patients using topic models inferred from unstructured progress notes,\nin: AMIA annual symposium proceedings, volume 2012, American Medical\nInformatics Association, 2012, p. 505.\n[18] Y. Luo, Y. Xin, R. Joshi, L. A. Celi, P. Szolovits, Predicting icu mortality\nrisk by grouping temporal trends from a multivariate panel of physiologic\nmeasurements., in: AAAI, 2016, pp. 42–50.\n[19] S. Joshi, S. Gunasekar, D. Sontag, G. Joydeep, Identiﬁable phenotyping\nusing constrained non-negative matrix factorization, in: Machine Learning\nfor Healthcare Conference, 2016, pp. 17–41.\n[20] J. Lee, D. M. Maslove, J. A. Dubin, Personalized mortality prediction\ndriven by electronic medical data and a patient similarity metric, PloS one\n10 (2015) e0127428.\n[21] J. Lee,\nPatient-speciﬁc predictive modeling using random forests: An\nobservational study for the critically ill, JMIR medical informatics 5 (2017).\n[22] Y.-F. Luo, A. Rumshisky, Interpretable topic features for post-icu mortal-\nity prediction, in: AMIA Annual Symposium Proceedings, volume 2016,\nAmerican Medical Informatics Association, 2016, p. 827.\n[23] J.-L. Vincent, R. Moreno, J. Takala, S. Willatts, A. De Mendon¸ca, H. Bru-\nining, C. Reinhart, P. Suter, L. Thijs, The sofa (sepsis-related organ failure\nassessment) score to describe organ dysfunction/failure, Intensive care\nmedicine 22 (1996) 707–710.\n[24] W. A. Knaus, J. E. Zimmerman, D. P. Wagner, E. A. Draper, D. E. Lawrence,\nApache-acute physiology and chronic health evaluation: a physiologically\nbased classiﬁcation system., Critical care medicine 9 (1981) 591–597.\n[25] R. Dybowski, V. Gant, P. Weller, R. Chang, Prediction of outcome in\ncritically ill patients using artiﬁcial neural network synthesised by genetic\nalgorithm, The Lancet 347 (1996) 1146–1150.\n[26] S. Kim, W. Kim, R. W. Park, A comparison of intensive care unit mortality\nprediction models through the use of data mining techniques, Healthcare\ninformatics research 17 (2011) 232–243.\n[27] J. V. Tu, M. R. Guerriere, Use of a neural network as a predictive instru-\nment for length of stay in the intensive care unit following cardiac surgery,\nComputers and biomedical research 26 (1993) 220–229.\n28\n[28] G. Doig, K. Inman, W. Sibbald, C. Martin, J. Robertson, Modeling mortality\nin the intensive care unit: comparing the performance of a back-propagation,\nassociative-learning neural network with multivariate logistic regression., in:\nProceedings of the Annual Symposium on Computer Application in Medical\nCare, American Medical Informatics Association, 1993, p. 361.\n[29] C. W. Hanson III, B. E. Marshall, Artiﬁcial intelligence applications in the\nintensive care unit, Critical care medicine 29 (2001) 427–435.\n[30] R. Pirracchio, Mortality prediction in the icu based on mimic-ii results from\nthe super icu learner algorithm (sicula) project, in: Secondary Analysis of\nElectronic Health Records, Springer, 2016, pp. 295–313.\n[31] E. C. Polley, M. J. Van der Laan, Super learner in prediction, U.C. Berkeley\nDivision of Biostatistics Working Paper Series (2010).\n[32] R. Caruana, S. Baluja, T. Mitchell, Using the future to” sort out” the\npresent: Rankprop and multitask learning for medical risk evaluation, in:\nAdvances in neural information processing systems, 1996, pp. 959–965.\n[33] G. F. Cooper, C. F. Aliferis, R. Ambrosino, J. Aronis, B. G. Buchanan,\nR. Caruana, M. J. Fine, C. Glymour, G. Gordon, B. H. Hanusa, et al., An\nevaluation of machine-learning methods for predicting pneumonia mortality,\nArtiﬁcial intelligence in medicine 9 (1997) 107–138.\n[34] T. A. Lasko, J. C. Denny, M. A. Levy, Computational phenotype discovery\nusing unsupervised feature learning over noisy, sparse, and irregular clinical\ndata, PloS one 8 (2013) e66341.\n[35] A. Oellrich, N. Collier, T. Groza, D. Rebholz-Schuhmann, N. Shah, O. Bo-\ndenreider, M. R. Boland, I. Georgiev, H. Liu, K. Livingston, et al., The\ndigital revolution in phenotyping, Brieﬁngs in bioinformatics (2015) bbv083.\n[36] Z. Che, S. Purushotham, R. Khemani, Y. Liu, Distilling knowledge from\ndeep networks with applications to healthcare domain,\narXiv preprint\narXiv:1512.03542 (2015).\n[37] F. Dabek, J. J. Caban,\nA neural network based model for predicting\npsychological conditions, in: Brain Informatics and Health, Springer, 2015,\npp. 252–261.\n[38] N. Y. Hammerla, J. M. Fisher, P. Andras, L. Rochester, R. Walker, T. Pl¨otz,\nPd disease state assessment in naturalistic environments using deep learning,\nin: Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015, pp. 1742–\n1748.\n[39] Z. C. Lipton, D. C. Kale, C. Elkan, R. Wetzell, Learning to diagnose with\nlstm recurrent neural networks, arXiv preprint arXiv:1511.03677 (2015).\n29\n[40] S. Purushotham, W. Carvalho, T. Nilanon, Y. Liu, Variational recurrent\nadversarial deep domain adaptation, International Conference on Learning\nRepresentations (ICLR) (2017).\n[41] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computa-\ntion 9 (1997) 1735–1780.\n[42] I. Silva, G. Moody, D. J. Scott, L. A. Celi, R. G. Mark, Predicting in-\nhospital mortality of icu patients: The physionet/computing in cardiology\nchallenge 2012, in: Computing in Cardiology (CinC), 2012, IEEE, 2012, pp.\n245–248.\n[43] M. J. Van der Laan, E. C. Polley, A. E. Hubbard, Super learner, Statistical\napplications in genetics and molecular biology 6 (2007).\n[44] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015) 436–444.\n[45] H. Larochelle, Y. Bengio, J. Louradour, P. Lamblin, Exploring strategies\nfor training deep neural networks, Journal of Machine Learning Research\n10 (2009) 1–40.\n[46] Y. Bengio, A. Courville, P. Vincent, Representation learning: A review\nand new perspectives, Pattern Analysis and Machine Intelligence, IEEE\nTransactions on 35 (2013) 1798–1828.\n[47] G. Dahl, A.-r. Mohamed, G. E. Hinton, et al., Phone recognition with the\nmean-covariance restricted boltzmann machine, in: Advances in neural\ninformation processing systems, 2010, pp. 469–477.\n[48] G. E. Dahl, D. Yu, L. Deng, A. Acero, Context-dependent pre-trained deep\nneural networks for large-vocabulary speech recognition, IEEE Transactions\non audio, speech, and language processing 20 (2012) 30–42.\n[49] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with\ndeep convolutional neural networks, in: Advances in neural information\nprocessing systems, 2012, pp. 1097–1105.\n[50] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, T. Darrell,\nCaﬀe: Convolutional architecture for fast\nfeature embedding, in: Proceedings of the 22nd ACM international confer-\nence on Multimedia, ACM, 2014, pp. 675–678.\n[51] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: Proceed-\nings of the IEEE conference on computer vision and pattern recognition,\n2015, pp. 1–9.\n[52] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock`y, Empirical\nevaluation and combination of advanced language modeling techniques, in:\nTwelfth Annual Conference of the International Speech Communication\nAssociation, 2011, pp. 605–608.\n30\n[53] A. Bordes, X. Glorot, J. Weston, Y. Bengio, Joint learning of words and\nmeaning representations for open-text semantic parsing,\nin: Artiﬁcial\nIntelligence and Statistics, 2012, pp. 127–135.\n[54] T. Mikolov, W.-t. Yih, G. Zweig, Linguistic regularities in continuous space\nword representations., in: hlt-Naacl, volume 13, 2013, pp. 746–751.\n[55] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed rep-\nresentations of words and phrases and their compositionality, in: Advances\nin neural information processing systems, 2013, pp. 3111–3119.\n[56] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\nare universal approximators, Neural networks 2 (1989) 359–366.\n[57] V. Nair, G. E. Hinton, Rectiﬁed linear units improve restricted boltzmann\nmachines, in: Proceedings of the 27th International Conference on Machine\nLearning (ICML), 2010, pp. 807–814.\n[58] R. J. Williams, D. Zipser, A learning algorithm for continually running\nfully recurrent neural networks, Neural computation 1 (1989) 270–280.\n[59] K. Cho, B. van Merri¨enboer, D. Bahdanau, Y. Bengio, On the properties\nof neural machine translation: Encoder-decoder approaches, arXiv preprint\narXiv:1409.1259 (2014).\n[60] J. Chung, C. Gulcehre, K. Cho, Y. Bengio,\nEmpirical evaluation of\ngated recurrent neural networks on sequence modeling, arXiv preprint\narXiv:1412.3555 (2014).\n[61] N. Srivastava, R. R. Salakhutdinov, Multimodal learning with deep boltz-\nmann machines, in: Advances in neural information processing systems,\n2012, pp. 2222–2230.\n[62] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron,\nN. Bouchard, D. Warde-Farley, Y. Bengio, Theano: new features and speed\nimprovements, arXiv preprint arXiv:1211.5590 (2012).\n[63] F. Chollet, Keras: Theano-based deep learning library, Code: https://github.\ncom/fchollet. Documentation: http://keras. io (2015).\n31\nAppendix A. Appendix\nAppendix A.1. Feature Set C\nTable A.22 lists features in Feature set C.\nTable A.22: List of 135 features in feature set C.\nFeature Name\nTable Name\nAlbumin 5%\ninputevents\nFresh Frozen Plasma\ninputevents\nLorazepam (Ativan)\ninputevents\nCalcium Gluconate\ninputevents\nMidazolam (Versed)\ninputevents\nPhenylephrine\ninputevents\nFurosemide (Lasix)\ninputevents\nHydralazine\ninputevents\nNorepinephrine\ninputevents\nMagnesium Sulfate\ninputevents\nNitroglycerin\ninputevents\nInsulin - Regular\ninputevents\nMorphine Sulfate\ninputevents\nPotassium Chloride\ninputevents\nPacked Red Blood Cells\ninputevents\nGastric Meds\ninputevents\nD5 1/2NS\ninputevents\nLR\ninputevents\nSolution\ninputevents\nSterile Water\ninputevents\nPiggyback\ninputevents\nOR Crystalloid Intake\ninputevents\nPO Intake\ninputevents\nGT Flush\ninputevents\nKCL (Bolus)\ninputevents\nMagnesium Sulfate (Bolus)\ninputevents\nepinephrine\ninputevents\nvasopressin\ninputevents\ndopamine\ninputevents\nmidazolam\ninputevents\nfentanyl\ninputevents\npropofol\ninputevents\nGastric Tube\noutputevents\nStool Out Stool\noutputevents\nUrine Out Incontinent\noutputevents\nUltraﬁltrate\noutputevents\nFecal Bag\noutputevents\nChest Tube #1\noutputevents\nChest Tube #2\noutputevents\nJackson Pratt #1\noutputevents\nOR EBL\noutputevents\n32\nTable A.22: List of 135 features in feature set C.\nFeature Name\nTable Name\nPre-Admission\noutputevents\nTF Residual\noutputevents\nurinary output sum\noutputevents\nHEMATOCRIT\nlabevents\nPLATELET COUNT\nlabevents\nHEMOGLOBIN\nlabevents\nMCHC\nlabevents\nMCH\nlabevents\nMCV\nlabevents\nRED BLOOD CELLS\nlabevents\nRDW\nlabevents\nCHLORIDE\nlabevents\nANION GAP\nlabevents\nCREATININE\nlabevents\nGLUCOSE\nlabevents\nMAGNESIUM, TOTAL\nlabevents\nCALCIUM\nlabevents\nPHOSPHATE\nlabevents\nINR(PT)\nlabevents\nPT\nlabevents\nPTT\nlabevents\nLYMPHOCYTES\nlabevents\nMONOCYTES\nlabevents\nNEUTROPHILS\nlabevents\nBASOPHILS\nlabevents\nEOSINOPHILS\nlabevents\nPH\nlabevents\nBASE EXCESS\nlabevents\nCALCULATED TOTAL CO2\nlabevents\nPCO2\nlabevents\nSPECIFIC GRAVITY\nlabevents\nLACTATE\nlabevents\nALANINE AMINOTRANSFERASE (ALT)\nlabevents\nASPARATE AMINOTRANSFERASE (AST)\nlabevents\nALKALINE PHOSPHATASE\nlabevents\nALBUMIN\nlabevents\npao2\nlabevents\nserum urea nitrogen level\nlabevents\nwhite blood cells count mean\nlabevents\nserum bicarbonate level mean\nlabevents\nsodium level mean\nlabevents\npotassium level mean\nlabevents\nbilirubin level\nlabevents\nhgb\nlabevents\nchloride\nlabevents\npeep\nlabevents\n33\nTable A.22: List of 135 features in feature set C.\nFeature Name\nTable Name\nAspirin\nprescriptions\nBisacodyl\nprescriptions\nDocusate Sodium\nprescriptions\nHumulin-R Insulin\nprescriptions\nMetoprolol Tartrate\nprescriptions\nPantoprazole\nprescriptions\nArterialBloodPressurediastolic\nchartevents\nArterialBloodPressuremean\nchartevents\nRespiratoryRate\nchartevents\nAlarmsOn\nchartevents\nMinuteVolumeAlarm-Low\nchartevents\nPeakinsp.Pressure\nchartevents\nPEEPset\nchartevents\nMinuteVolume\nchartevents\nTidalVolume(observed)\nchartevents\nMinuteVolumeAlarm-High\nchartevents\nMeanAirwayPressure\nchartevents\nCentralVenousPressure\nchartevents\nRespiratoryRate(Set)\nchartevents\nPulmonaryArteryPressuremean\nchartevents\nO2Flow\nchartevents\nGlucoseﬁngerstick\nchartevents\nHeartRateAlarm-Low\nchartevents\nPulmonaryArteryPressuresystolic\nchartevents\nTidalVolume(set)\nchartevents\nPulmonaryArteryPressurediastolic\nchartevents\nSpO2DesatLimit\nchartevents\nRespAlarm-High\nchartevents\nSkinCare\nchartevents\ngcsverbal\nchartevents\ngcsmotor\nchartevents\ngcseyes\nchartevents\nsystolic blood pressure abp mean\nchartevents\nheart rate\nchartevents\nbody temperature\nchartevents\nﬁo2\nchartevents\nie ratio mean\nchartevents\ndiastolic blood pressure mean\nchartevents\narterial pressure mean\nchartevents\nspo2 peripheral\nchartevents\nglucose\nchartevents\nweight\nchartevents\nheight\nchartevents\n34\nAppendix A.2. Mortality Prediction Task Labels\nThe labels of in-hospital mortality are derived from table ADMISSION, in\nwhich the column DEATHTIME records either a valid death time of an admission\nif the patient dies in hospital or a null value if the patient dies after discharge.\nTherefore, we assign the in-hospital mortality label of an admission to 1 if its\nDEATHTIME is not null, else we assign the label to 0.\nThe labels of short-term mortality are generated with values in column\nINTIME from table ICUSTAY, which are in-time records of icu stays, and values\nin column DOD from table PATIENTS, which are records of death time of\npatients. We calculate the length of time interval between INTIME and DOD\nof an admission and assign its labels by comparing it with pre-deﬁned lengths.\nThe labels of long-term mortality are generated with values in column DIS-\nCHTIME from table ADMISSION, which are in-time records of icu stays, and\nvalues in column DOD from table PATIENTS, which are records of death time\nof patients. We calculate the length of time interval between INTIME and DOD\nof an admission and assign its labels by comparing it with pre-deﬁned lengths.\n35\nAppendix A.3. MMDL model\nFigure A.5: Structure of the MMDL model with Feature Set B as input.\n36\n",
  "categories": [
    "cs.LG",
    "cs.CY",
    "stat.ML"
  ],
  "published": "2017-10-23",
  "updated": "2017-10-23"
}