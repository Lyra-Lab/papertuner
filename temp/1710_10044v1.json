{
  "id": "http://arxiv.org/abs/1710.10044v1",
  "title": "Distributional Reinforcement Learning with Quantile Regression",
  "authors": [
    "Will Dabney",
    "Mark Rowland",
    "Marc G. Bellemare",
    "Rémi Munos"
  ],
  "abstract": "In reinforcement learning an agent interacts with the environment by taking\nactions and observing the next state and reward. When sampled\nprobabilistically, these state transitions, rewards, and actions can all induce\nrandomness in the observed long-term return. Traditionally, reinforcement\nlearning algorithms average over this randomness to estimate the value\nfunction. In this paper, we build on recent work advocating a distributional\napproach to reinforcement learning in which the distribution over returns is\nmodeled explicitly instead of only estimating the mean. That is, we examine\nmethods of learning the value distribution instead of the value function. We\ngive results that close a number of gaps between the theoretical and\nalgorithmic results given by Bellemare, Dabney, and Munos (2017). First, we\nextend existing results to the approximate distribution setting. Second, we\npresent a novel distributional reinforcement learning algorithm consistent with\nour theoretical formulation. Finally, we evaluate this new algorithm on the\nAtari 2600 games, observing that it significantly outperforms many of the\nrecent improvements on DQN, including the related distributional algorithm C51.",
  "text": "Distributional Reinforcement Learning with Quantile Regression\nWill Dabney\nDeepMind\nMark Rowland\nUniversity of Cambridge∗\nMarc G. Bellemare\nGoogle Brain\nR´emi Munos\nDeepMind\nAbstract\nIn reinforcement learning an agent interacts with the environ-\nment by taking actions and observing the next state and re-\nward. When sampled probabilistically, these state transitions,\nrewards, and actions can all induce randomness in the ob-\nserved long-term return. Traditionally, reinforcement learn-\ning algorithms average over this randomness to estimate the\nvalue function. In this paper, we build on recent work ad-\nvocating a distributional approach to reinforcement learning\nin which the distribution over returns is modeled explicitly\ninstead of only estimating the mean. That is, we examine\nmethods of learning the value distribution instead of the value\nfunction. We give results that close a number of gaps between\nthe theoretical and algorithmic results given by Bellemare,\nDabney, and Munos (2017). First, we extend existing results\nto the approximate distribution setting. Second, we present\na novel distributional reinforcement learning algorithm con-\nsistent with our theoretical formulation. Finally, we evaluate\nthis new algorithm on the Atari 2600 games, observing that\nit signiﬁcantly outperforms many of the recent improvements\non DQN, including the related distributional algorithm C51.\nIntroduction\nIn reinforcement learning, the value of an action a in state s\ndescribes the expected return, or discounted sum of rewards,\nobtained from beginning in that state, choosing action a, and\nsubsequently following a prescribed policy. Because know-\ning this value for the optimal policy is sufﬁcient to act opti-\nmally, it is the object modelled by classic value-based meth-\nods such as SARSA (Rummery and Niranjan 1994) and Q-\nLearning (Watkins and Dayan 1992), which use Bellman’s\nequation (Bellman 1957) to efﬁciently reason about value.\nRecently, Bellemare, Dabney, and Munos (2017) showed\nthat the distribution of the random returns, whose expecta-\ntion constitutes the aforementioned value, can be described\nby the distributional analogue of Bellman’s equation, echo-\ning previous results in risk-sensitive reinforcement learning\n(Heger 1994; Morimura et al. 2010; Chow et al. 2015). In\nthis previous work, however, the authors argued for the use-\nfulness in modeling this value distribution in and of itself.\nTheir claim was asserted by exhibiting a distributional rein-\nforcement learning algorithm, C51, which achieved state-of-\n∗Contributed during an internship at DeepMind.\nthe-art on the suite of benchmark Atari 2600 games (Belle-\nmare et al. 2013).\nOne of the theoretical contributions of the C51 work was\na proof that the distributional Bellman operator is a contrac-\ntion in a maximal form of the Wasserstein metric between\nprobability distributions. In this context, the Wasserstein\nmetric is particularly interesting because it does not suffer\nfrom disjoint-support issues (Arjovsky, Chintala, and Bot-\ntou 2017) which arise when performing Bellman updates.\nUnfortunately, this result does not directly lead to a practical\nalgorithm: as noted by the authors, and further developed by\nBellemare et al. (2017), the Wasserstein metric, viewed as a\nloss, cannot generally be minimized using stochastic gradi-\nent methods.\nThis negative result left open the question as to whether it\nis possible to devise an online distributional reinforcement\nlearning algorithm which takes advantage of the contraction\nresult. Instead, the C51 algorithm ﬁrst performs a heuristic\nprojection step, followed by the minimization of a KL di-\nvergence between projected Bellman update and prediction.\nThe work therefore leaves a theory-practice gap in our un-\nderstanding of distributional reinforcement learning, which\nmakes it difﬁcult to explain the good performance of C51.\nThus, the existence of a distributional algorithm that oper-\nates end-to-end on the Wasserstein metric remains an open\nquestion.\nIn this paper, we answer this question afﬁrmatively. By\nappealing to the theory of quantile regression (Koenker\n2005), we show that there exists an algorithm, applicable in\na stochastic approximation setting, which can perform distri-\nbutional reinforcement learning over the Wasserstein metric.\nOur method relies on the following techniques:\n• We “transpose” the parametrization from C51: whereas\nthe former uses N ﬁxed locations for its approxima-\ntion distribution and adjusts their probabilities, we assign\nﬁxed, uniform probabilities to N adjustable locations;\n• We show that quantile regression may be used to stochas-\ntically adjust the distributions’ locations so as to minimize\nthe Wasserstein distance to a target distribution.\n• We formally prove contraction mapping results for our\noverall algorithm, and use these results to conclude that\nour method performs distributional RL end-to-end under\nthe Wasserstein metric, as desired.\narXiv:1710.10044v1  [cs.AI]  27 Oct 2017\nThe main interest of the original distributional algorithm\nwas its state-of-the-art performance, despite still acting by\nmaximizing expectations. One might naturally expect that a\ndirect minimization of the Wasserstein metric, rather than its\nheuristic approximation, may yield even better results. We\nderive the Q-Learning analogue for our method (QR-DQN),\napply it to the same suite of Atari 2600 games, and ﬁnd that it\nachieves even better performance. By using a smoothed ver-\nsion of quantile regression, Huber quantile regression, we\ngain an impressive 33% median score increment over the al-\nready state-of-the-art C51.\nDistributional RL\nWe model the agent-environment interactions by a Markov\ndecision process (MDP) (X, A, R, P, γ) (Puterman 1994),\nwith X and A the state and action spaces, R the random\nvariable reward function, P(x′|x, a) the probability of tran-\nsitioning from state x to state x′ after taking action a, and\nγ ∈[0, 1) the discount factor. A policy π(·|x) maps each\nstate x ∈X to a distribution over A.\nFor a ﬁxed policy π, the return, Zπ = P∞\nt=0 γtRt, is a\nrandom variable representing the sum of discounted rewards\nobserved along one trajectory of states while following π.\nStandard RL algorithms estimate the expected value of Zπ,\nthe value function,\nV π(x) := E [Zπ(x)] = E\n\" ∞\nX\nt=0\nγtR(xt, at) | x0 = x\n#\n.\n(1)\nSimilarly, many RL algorithms estimate the action-value\nfunction,\nQπ(x, a) := E [Zπ(x, a)] = E\n\" ∞\nX\nt=0\nγtR(xt, at)\n#\n,\n(2)\nxt ∼P(·|xt−1, at−1), at ∼π(·|xt), x0 = x, a0 = a.\nThe ϵ-greedy policy on Qπ chooses actions uniformly\nat random with probability ϵ and otherwise according to\narg maxa Qπ(x, a).\nIn distributional RL the distribution over returns (i.e. the\nprobability law of Zπ), plays the central role and replaces\nthe value function. We will refer to the value distribution\nby its random variable. When we say that the value func-\ntion is the mean of the value distribution we are saying\nthat the value function is the expected value, taken over\nall sources of intrinsic randomness (Goldstein, Misra, and\nCourtage 1981), of the value distribution. This should high-\nlight that the value distribution is not designed to capture\nthe uncertainty in the estimate of the value function (Dear-\nden, Friedman, and Russell 1998; Engel, Mannor, and Meir\n2005), that is the parametric uncertainty, but rather the ran-\ndomness in the returns intrinsic to the MDP.\nTemporal difference (TD) methods signiﬁcantly speed up\nthe learning process by incrementally improving an estimate\nof Qπ using dynamic programming through the Bellman op-\nerator (Bellman 1957),\nT πQ(x, a) = E [R(x, a)] + γEP,π [Q(x′, a′)] .\nΦT ⇡Z\nz1\nz2\nq1\nq2\nT ⇡Z\nDKL(ΦT ⇡ZkZ)\nT ⇡Z\n4z 4z 4z\n4z\nFigure 1: Projection used by C51 assigns mass inversely\nproportional to distance from nearest support. Update mini-\nmizes KL between projected target and estimate.\nSimilarly, the value distribution can be computed through\ndynamic programming using a distributional Bellman oper-\nator (Bellemare, Dabney, and Munos 2017),\nT πZ(x, a) :\nD= R(x, a) + γZ(x′, a′),\n(3)\nx′ ∼P(·|x, a), a′ ∼π(·|x′),\nwhere Y :\nD= U denotes equality of probability laws, that is\nthe random variable Y is distributed according to the same\nlaw as U.\nThe C51 algorithm models Zπ(x, a) using a discrete dis-\ntribution supported on a “comb” of ﬁxed locations z1 ≤\n· · · ≤zN uniformly spaced over a predetermined interval.\nThe parameters of that distribution are the probabilities qi,\nexpressed as logits, associated with each location zi. Given\na current value distribution, the C51 algorithm applies a pro-\njection step Φ to map the target T πZ onto its ﬁnite ele-\nment support, followed by a Kullback-Leibler (KL) mini-\nmization step (see Figure 1). C51 achieved state-of-the-art\nperformance on Atari 2600 games, but did so with a clear\ndisconnect with the theoretical results of Bellemare, Dab-\nney, and Munos (2017). We now review these results before\nextending them to the case of approximate distributions.\nThe Wasserstein Metric\nThe p-Wasserstein metric Wp, for p ∈[1, ∞], also known\nas the Mallows metric (Bickel and Freedman 1981) or the\nEarth Mover’s Distance (EMD) when p = 1 (Levina and\nBickel 2001), is an integral probability metric between dis-\ntributions. The p-Wasserstein distance is characterized as the\nLp metric on inverse cumulative distribution functions (in-\nverse CDFs) (M¨uller 1997). That is, the p-Wasserstein met-\nric between distributions U and Y is given by,1\nWp(U, Y ) =\n\u0012Z 1\n0\n|F −1\nY (ω) −F −1\nU (ω)|pdω\n\u00131/p\n,\n(4)\nwhere for a random variable Y , the inverse CDF F −1\nY\nof Y\nis deﬁned by\nF −1\nY (ω) := inf{y ∈R : ω ≤FY (y)} ,\n(5)\nwhere FY (y) = Pr(Y ≤y) is the CDF of Y . Figure 2 il-\nlustrates the 1-Wasserstein distance as the area between two\nCDFs.\n1For p = ∞, W∞(Y, U) = supω∈[0,1] |F −1\nY (ω) −F −1\nU (ω)|.\nRecently, the Wasserstein metric has been the focus of in-\ncreased research due to its appealing properties of respect-\ning the underlying metric distances between outcomes (Ar-\njovsky, Chintala, and Bottou 2017; Bellemare et al. 2017).\nUnlike the Kullback-Leibler divergence, the Wasserstein\nmetric is a true probability metric and considers both the\nprobability of and the distance between various outcome\nevents. These properties make the Wasserstein well-suited to\ndomains where an underlying similarity in outcome is more\nimportant than exactly matching likelihoods.\nConvergence of Distributional Bellman Operator\nIn the context of distributional RL, let Z be the space of\naction-value distributions with ﬁnite moments:\nZ = {Z : X × A →P(R)|\nE [|Z(x, a)|p] < ∞, ∀(x, a), p ≥1}.\nThen, for two action-value distributions Z1, Z2 ∈Z, we will\nuse the maximal form of the Wasserstein metric introduced\nby (Bellemare, Dabney, and Munos 2017),\n¯dp(Z1, Z2) := sup\nx,a Wp(Z1(x, a), Z2(x, a)).\n(6)\nIt was shown that ¯dp is a metric over value distributions. Fur-\nthermore, the distributional Bellman operator T π is a con-\ntraction in ¯dp, a result that we now recall.\nLemma 1 (Lemma 3, Bellemare, Dabney, and Munos 2017).\nT π is a γ-contraction: for any two Z1, Z2 ∈Z,\n¯dp(T πZ1, T πZ2) ≤γ ¯dp(Z1, Z2).\nLemma 1 tells us that ¯dp is a useful metric for studying\nthe behaviour of distributional reinforcement learning algo-\nrithms, in particular to show their convergence to the ﬁxed\npoint Zπ. Moreover, the lemma suggests that an effective\nway in practice to learn value distributions is to attempt to\nminimize the Wasserstein distance between a distribution Z\nand its Bellman update T πZ, analogous to the way that TD-\nlearning attempts to iteratively minimize the L2 distance be-\ntween Q and T Q.\nUnfortunately, another result shows that we cannot in gen-\neral minimize the Wasserstein metric (viewed as a loss) us-\ning stochastic gradient descent.\nTheorem 1 (Theorem 1, Bellemare et al. 2017). Let ˆYm :=\n1\nm\nPm\ni=1 δYi be the empirical distribution derived from sam-\nples Y1, . . . , Ym drawn from a Bernoulli distribution B. Let\nBµ be a Bernoulli distribution parametrized by µ, the prob-\nability of the variable taking the value 1. Then the minimum\nof the expected sample loss is in general different from the\nminimum of the true Wasserstein loss; that is,\narg min\nµ\nE\nY1:m\n\u0002\nWp( ˆYm, Bµ)\n\u0003\n̸= arg min\nµ\nWp(B, Bµ).\nThis issue becomes salient in a practical context, where\nthe value distribution must be approximated. Crucially,\nthe C51 algorithm is not guaranteed to minimize any p-\nWasserstein metric. This gap between theory and practice\nin distributional RL is not restricted to C51. Morimura et\nSpace of Returns\nProbability Space\n⌧1\n⌧2\n⌧3\n⌧4 = 1\n⌧0 = 0\nˆ⌧1\nˆ⌧2\nˆ⌧3\nˆ⌧4\nq1\nq2\nq3\nq4\nZ 2 Z\n⇧W1Z 2 ZQ\nz1 = F −1\nZ (ˆ⌧1)\nz2\nz3\nz4\nFigure 2: 1-Wasserstein minimizing projection onto N = 4\nuniformly weighted Diracs. Shaded regions sum to form the\n1-Wasserstein error.\nal. (2010) parameterize a value distribution with the mean\nand scale of a Gaussian or Laplace distribution, and min-\nimize the KL divergence between the target T πZ and the\nprediction Z. They demonstrate that value distributions\nlearned in this way are sufﬁcient to perform risk-sensitive Q-\nLearning. However, any theoretical guarantees derived from\ntheir method can only be asymptotic; the Bellman operator\nis at best a non-expansion in KL divergence.\nApproximately Minimizing Wasserstein\nRecall that\nC51 approximates the distribution at each\nstate by attaching variable (parametrized) probabilities\nq1, . . . , qN to ﬁxed locations z1 ≤· · · ≤zN. Our approach\nis to “transpose” this parametrization by considering ﬁxed\nprobabilities but variable locations. Speciﬁcally, we take\nuniform weights, so that qi = 1/N for each i = 1, . . . , N.\nEffectively, our new approximation aims to estimate\nquantiles of the target distribution. Accordingly, we will call\nit a quantile distribution, and let ZQ be the space of quan-\ntile distributions for ﬁxed N. We will denote the cumulative\nprobabilities associated with such a distribution (that is, the\ndiscrete values taken on by the CDF) by τ1, . . . , τN, so that\nτi =\ni\nN for i = 1, . . . , N. We will also write τ0 = 0 to\nsimplify notation.\nFormally, let θ : X ×A →RN be some parametric model.\nA quantile distribution Zθ ∈ZQ maps each state-action\npair (x, a) to a uniform probability distribution supported\non {θi(x, a)}. That is,\nZθ(x, a) := 1\nN\nN\nX\ni=1\nδθi(x,a),\n(7)\nwhere δz denotes a Dirac at z ∈R.\nCompared to the original parametrization, the beneﬁts of\na parameterized quantile distribution are threefold. First, (1)\nwe are not restricted to prespeciﬁed bounds on the support,\nor a uniform resolution, potentially leading to signiﬁcantly\nmore accurate predictions when the range of returns vary\ngreatly across states. This also (2) lets us do away with the\nunwieldy projection step present in C51, as there are no is-\nsues of disjoint supports. Together, these obviate the need\nfor domain knowledge about the bounds of the return dis-\ntribution when applying the algorithm to new tasks. Finally,\n(3) this reparametrization allows us to minimize the Wasser-\nstein loss, without suffering from biased gradients, speciﬁ-\ncally, using quantile regression.\nThe Quantile Approximation\nIt is well-known that in reinforcement learning, the use of\nfunction approximation may result in instabilities in the\nlearning process (Tsitsiklis and Van Roy 1997). Speciﬁcally,\nthe Bellman update projected onto the approximation space\nmay no longer be a contraction. In our case, we analyze the\ndistributional Bellman update, projected onto a parameter-\nized quantile distribution, and prove that the combined op-\nerator is a contraction.\nQuantile Projection\nWe are interested in quantifying the\nprojection of an arbitrary value distribution Z ∈Z onto ZQ,\nthat is\nΠW1Z := arg min\nZθ∈ZQ\nW1(Z, Zθ),\nLet Y be a distribution with bounded ﬁrst moment and U\na uniform distribution over N Diracs as in (7), with support\n{θ1, . . . , θN}. Then\nW1(Y, U) =\nN\nX\ni=1\nZ τi\nτi−1\n|F −1\nY (ω) −θi|dω.\nLemma 2. For any τ, τ ′ ∈[0, 1] with τ < τ ′ and cumulative\ndistribution function F with inverse F −1, the set of θ ∈R\nminimizing\nZ τ ′\nτ\n|F −1(ω) −θ|dω ,\nis given by\n\u001a\nθ ∈R\n\f\f\f\fF(θ) =\n\u0012τ + τ ′\n2\n\u0013\u001b\n.\nIn particular, if F −1 is the inverse CDF, then F −1((τ +\nτ ′)/2) is always a valid minimizer, and if F −1 is continuous\nat (τ +τ ′)/2, then F −1((τ +τ ′)/2) is the unique minimizer.\nThese quantile midpoints will be denoted by ˆτi = τi−1+τi\n2\nfor 1 ≤i ≤N. Therefore, by Lemma 2, the values\nfor {θ1, θ1, . . . , θN} that minimize W1(Y, U) are given by\nθi = F −1\nY (ˆτi). Figure 2 shows an example of the quantile\nprojection ΠW1Z minimizing the 1-Wasserstein distance to\nZ.2\nQuantile Regression\nThe original proof of Theorem 1 only states the existence\nof a distribution whose gradients are biased. As a result, we\nmight hope that our quantile parametrization leads to unbi-\nased gradients. Unfortunately, this is not true.\n2We save proofs for the appendix due to space limitations.\nProposition 1. Let Zθ be a quantile distribution, and ˆZm\nthe empirical distribution composed of m samples from Z.\nThen for all p ≥1, there exists a Z such that\narg min E[Wp( ˆZm, Zθ)] ̸= arg min Wp(Z, Zθ).\nHowever, there is a method, more widely used in eco-\nnomics than machine learning, for unbiased stochastic ap-\nproximation of the quantile function. Quantile regression,\nand conditional quantile regression, are methods for ap-\nproximating the quantile functions of distributions and con-\nditional distributions respectively (Koenker 2005). These\nmethods have been used in a variety of settings where\noutcomes have intrinsic randomness (Koenker and Hallock\n2001); from food expenditure as a function of household in-\ncome (Engel 1857), to studying value-at-risk in economic\nmodels (Taylor 1999).\nThe quantile regression loss, for quantile τ ∈[0, 1], is\nan asymmetric convex loss function that penalizes overesti-\nmation errors with weight τ and underestimation errors with\nweight 1−τ. For a distribution Z, and a given quantile τ, the\nvalue of the quantile function F −1\nZ (τ) may be characterized\nas the minimizer of the quantile regression loss\nLτ\nQR(θ) := E ˆ\nZ∼Z[ρτ( ˆZ −θ)] , where\nρτ(u) = u(τ −δ{u<0}), ∀u ∈R.\n(8)\nMore generally, by Lemma 2 we have that the minimizing\nvalues of {θ1, . . . , θN} for W1(Z, Zθ) are those that mini-\nmize the following objective:\nN\nX\ni=1\nE ˆ\nZ∼Z[ρˆτi( ˆZ −θi)]\nIn particular, this loss gives unbiased sample gradients.\nAs a result, we can ﬁnd the minimizing {θ1, . . . , θN} by\nstochastic gradient descent.\nQuantile Huber Loss\nThe quantile regression loss is not\nsmooth at zero; as u →0+, the gradient of Equation 8 stays\nconstant. We hypothesized that this could limit performance\nwhen using non-linear function approximation. To this end,\nwe also consider a modiﬁed quantile loss, called the quantile\nHuber loss.3 This quantile regression loss acts as an asym-\nmetric squared loss in an interval [−κ, κ] around zero and\nreverts to a standard quantile loss outside this interval.\nThe Huber loss is given by (Huber 1964),\nLκ(u) =\n\u001a 1\n2u2,\nif |u| ≤κ\nκ(|u| −1\n2κ),\notherwise .\n(9)\nThe quantile Huber loss is then simply the asymmetric vari-\nant of the Huber loss,\nρκ\nτ(u) = |τ −δ{u<0}|Lκ(u).\n(10)\nFor notational simplicity we will denote ρ0\nτ = ρτ, that is, it\nwill revert to the standard quantile regression loss.\n3Our quantile Huber loss is related to, but distinct from that of\nAravkin et al. (2014).\nCombining Projection and Bellman Update\nWe are now in a position to prove our main result, which\nstates that the combination of the projection implied by\nquantile regression with the Bellman operator is a contrac-\ntion. The result is in ∞-Wasserstein metric, i.e. the size of\nthe largest gap between the two CDFs.\nProposition 2. Let ΠW1 be the quantile projection deﬁned\nas above, and when applied to value distributions gives the\nprojection for each state-value distribution. For any two\nvalue distributions Z1, Z2 ∈Z for an MDP with countable\nstate and action spaces,\n¯d∞(ΠW1T πZ1, ΠW1T πZ2) ≤γ ¯d∞(Z1, Z2).\n(11)\nWe therefore conclude that the combined operator\nΠW1T π has a unique ﬁxed point ˆZπ, and the repeated appli-\ncation of this operator, or its stochastic approximation, con-\nverges to ˆZπ. Because ¯dp ≤¯d∞, we conclude that conver-\ngence occurs for all p ∈[1, ∞]. Interestingly, the contraction\nproperty does not directly hold for p < ∞; see Lemma 5 in\nthe appendix.\nDistributional RL using Quantile Regression\nWe can now form a complete algorithmic approach to distri-\nbutional RL consistent with our theoretical results. That is,\napproximating the value distribution with a parameterized\nquantile distribution over the set of quantile midpoints, de-\nﬁned by Lemma 2. Then, training the location parameters\nusing quantile regression (Equation 8).\nQuantile Regression Temporal Difference Learning\nRecall the standard TD update for evaluating a policy π,\nV (x) ←V (x) + α(r + γV (x′) −V (x)),\na ∼π(·|x), r ∼R(x, a), x′ ∼P(·|x, a).\nTD allows us to update the estimated value function with\na single unbiased sample following π. Quantile regression\nalso allows us to improve the estimate of the quantile func-\ntion for some target distribution, Y (x), by observing sam-\nples y ∼Y (x) and minimizing Equation 8.\nFurthermore, we have shown that by estimating the quan-\ntile function for well-chosen values of τ ∈(0, 1) we can ob-\ntain an approximation with minimal 1-Wasserstein distance\nfrom the original (Lemma 2). Finally, we can combine this\nwith the distributional Bellman operator to give a target dis-\ntribution for quantile regression. This gives us the quantile\nregression temporal difference learning (QRTD) algorithm,\nsummarized simply by the update,\nθi(x) ←θi(x) + α(ˆτi −δ{r+γz′<θi(x))}),\n(12)\na ∼π(·|x), r ∼R(x, a), x′ ∼P(·|x, a), z′ ∼Zθ(x′),\nwhere Zθ is a quantile distribution as in (7), and θi(x) is the\nestimated value of F −1\nZπ(x)(ˆτi) in state x. It is important to\nnote that this update is for each value of ˆτi and is deﬁned\nfor a single sample from the next state value distribution.\nIn general it is better to draw many samples of z′ ∼Z(x′)\nand minimize the expected update. A natural approach in\nthis case, which we use in practice, is to compute the update\nfor all pairs of (θi(x), θj(x′)). Next, we turn to a control\nalgorithm and the use of non-linear function approximation.\nQuantile Regression DQN\nQ-Learning is an off-policy reinforcement learning algo-\nrithm built around directly learning the optimal action-value\nfunction using the Bellman optimality operator (Watkins and\nDayan 1992),\nT Q(x, a) = E [R(x, a)] + γ\nE\nx′∼P\nh\nmax\na′ Q(x′, a′)\ni\n.\nThe distributional variant of this is to estimate a state-\naction value distribution and apply a distributional Bellman\noptimality operator,\nT Z(x, a) = R(x, a) + γZ(x′, a′),\n(13)\nx′ ∼P(·|x, a), a′ = arg maxa′ Ez∼Z(x′,a′) [z] .\nNotice in particular that the action used for the next state is\nthe greedy action with respect to the mean of the next state-\naction value distribution.\nFor a concrete algorithm we will build on the DQN archi-\ntecture (Mnih et al. 2015). We focus on the minimal changes\nnecessary to form a distributional version of DQN. Speciﬁ-\ncally, we require three modiﬁcations to DQN. First, we use a\nnearly identical neural network architecture as DQN, only\nchanging the output layer to be of size |A| × N, where\nN is a hyper-parameter giving the number of quantile tar-\ngets. Second, we replace the Huber loss used by DQN4,\nLκ(rt + γ maxa′ Q(xt+1, a′) −Q(xt, at)) with κ = 1,\nwith a quantile Huber loss (full loss given by Algorithm 1).\nFinally, we replace RMSProp (Tieleman and Hinton 2012)\nwith Adam (Kingma and Ba 2015). We call this new algo-\nrithm quantile regression DQN (QR-DQN).\nAlgorithm 1 Quantile Regression Q-Learning\nRequire: N, κ\ninput x, a, r, x′, γ ∈[0, 1)\n# Compute distributional Bellman target\nQ(x′, a′) := P\nj qjθj(x′, a′)\na∗←arg maxa′ Q(x, a′)\nT θj ←r + γθj(x′, a∗),\n∀j\n# Compute quantile regression loss (Equation 10)\noutput PN\ni=1 Ej\n\u0002\nρκ\nˆτi(T θj −θi(x, a))\n\u0003\nUnlike C51, QR-DQN does not require projection onto the\napproximating distribution’s support, instead it is able to ex-\npand or contract the values arbitrarily to cover the true range\nof return values. As an additional advantage, this means that\nQR-DQN does not require the additional hyper-parameter\ngiving the bounds of the support required by C51. The only\nadditional hyper-parameter of QR-DQN not shared by DQN is\nthe number of quantiles N, which controls with what reso-\nlution we approximate the value distribution. As we increase\nN, QR-DQN goes from DQN to increasingly able to estimate\nthe upper and lower quantiles of the value distribution. It\nbecomes increasingly capable of distinguishing low proba-\nbility events at either end of the cumulative distribution over\nreturns.\n4DQN uses gradient clipping of the squared error that makes it\nequivalent to a Huber loss with κ = 1.\nxS\n0 1 2 2 2 0 0 0 0 0 0\nxG\nZ(xS)\nFZ(xS)\nReturns\nReturns\nZ⇡\nMC\nZ✓\n(a)\n(b)\n(c)\n[V ⇡\nMC(xS) −V (xS)]2\nW1(Z⇡\nMC(xS), Z(xS))\nEpisodes\n(d)\n(e)\nFigure 3: (a) Two-room windy gridworld, with wind magnitude shown along bottom row. Policy trajectory shown by blue path,\nwith additional cycles caused by randomness shown by dashed line. (b, c) (Cumulative) Value distribution at start state xS,\nestimated by MC, Zπ\nMC, and by QRTD, Zθ. (d, e) Value function (distribution) approximation errors for TD(0) and QRTD.\nExperimental Results\nIn the introduction we claimed that learning the distribution\nover returns had distinct advantages over learning the value\nfunction alone. We have now given theoretically justiﬁed al-\ngorithms for performing distributional reinforcement learn-\ning, QRTD for policy evaluation and QR-DQN for control. In\nthis section we will empirically validate that the proposed\ndistributional reinforcement learning algorithms: (1) learn\nthe true distribution over returns, (2) show increased robust-\nness during training, and (3) signiﬁcantly improve sample\ncomplexity and ﬁnal performance over baseline algorithms.\nValue Distribution Approximation Error\nWe begin our\nexperimental results by demonstrating that QRTD actually\nlearns an approximate value distribution that minimizes the\n1-Wasserstein to the ground truth distribution over returns.\nAlthough our theoretical results already establish conver-\ngence of the former to the latter, the empirical performance\nhelps to round out our understanding.\nWe use a variant of the classic windy gridworld domain\n(Sutton and Barto 1998), modiﬁed to have two rooms and\nrandomness in the transitions. Figure 3(a) shows our ver-\nsion of the domain, where we have combined the transition\nstochasticity, wind, and the doorway to produce a multi-\nmodal distribution over returns when anywhere in the ﬁrst\nroom. Each state transition has probability 0.1 of moving in\na random direction, otherwise the transition is affected by\nwind moving the agent northward. The reward function is\nzero until reaching the goal state xG, which terminates the\nepisode and gives a reward of 1.0. The discount factor is\nγ = 0.99.\nWe compute the ground truth value distribution for opti-\nmal policy π, learned by policy iteration, at each state by per-\nforming 1K Monte-Carlo (MC) rollouts and recording the\nobserved returns as an empirical distribution, shown in Fig-\nure 3(b). Next, we ran both TD(0) and QRTD while following\nπ for 10K episodes. Each episode begins in the designated\nstart state (xS). Both algorithms started with a learning rate\nof α = 0.1. For QRTD we used N = 32 and drop α by half\nevery 2K episodes.\nLet Zπ\nMC(xS) be the MC estimated distribution over re-\nturns from the start state xS, similarly V π\nMC(xS) its mean.\nIn Figure 3 we show the approximation errors at xS for both\nalgorithms with respect to the number of episodes. In (d)\nwe evaluated, for both TD(0) and QRTD, the squared error,\n(V π\nMC −V (xS))2, and in (e) we show the 1-Wasserstein\nmetric for QRTD, W1(Zπ\nMC(xS), Z(xS)), where V (xS) and\nZ(xS) are the expected returns and value distribution at\nstate xS estimated by the algorithm. As expected both al-\ngorithms converge correctly in mean, and QRTD minimizes\nthe 1-Wasserstein distance to Zπ\nMC.\nEvaluation on Atari 2600\nWe now provide experimental results that demonstrate the\npractical advantages of minimizing the Wasserstein metric\nend-to-end, in contrast to the C51 approach. We use the 57\nAtari 2600 games from the Arcade Learning Environment\n(ALE) (Bellemare et al. 2013). Both C51 and QR-DQN build\non the standard DQN architecture, and we expect both to\nbeneﬁt from recent improvements to DQN such as the du-\neling architectures (Wang et al. 2016) and prioritized replay\n(Schaul et al. 2016). However, in our evaluations we com-\npare the pure versions of C51 and QR-DQN without these\nadditions. We present results for both a strict quantile loss,\nκ = 0 (QR-DQN-0), and with a Huber quantile loss with\nκ = 1 (QR-DQN-1).\nWe performed hyper-parameter tuning over a set of ﬁve\ntraining games and evaluated on the full set of 57 games\nusing these best settings (α = 0.00005, ϵADAM = 0.01/32,\nand N = 200).5 As with DQN we use a target network when\ncomputing the distributional Bellman update. We also allow\nϵ to decay at the same rate as in DQN, but to a lower value\nof 0.01, as is common in recent work (Bellemare, Dabney,\nand Munos 2017; Wang et al. 2016; van Hasselt, Guez, and\nSilver 2016).\nOut training procedure follows that of Mnih et al.\n(2015)’s, and we present results under two evaluation pro-\ntocols: best agent performance and online performance. In\nboth evaluation protocols we consider performance over all\n57 Atari 2600 games, and transform raw scores into human-\nnormalized scores (van Hasselt, Guez, and Silver 2016).\n5We swept over α in (10−3, 5 × 10−4, 10−4, 5 × 10−5, 10−5);\nϵADAM in (0.01/32, 0.005/32, 0.001/32); N (10, 50, 100, 200)\n10%\n20%\n30%\n40%\n50%\n20%\n30%\n40%\n50%\n20%\n30%\n40%\n50%\n20%\n30%\n40%\n50%\nFigure 4: Online evaluation results, in human-normalized scores, over 57 Atari 2600 games for 200 million training samples.\n(Left) Testing performance for one seed, showing median over games. (Right) Training performance, averaged over three seeds,\nshowing percentiles (10, 20, 30, 40, and 50) over games.\nMean\nMedian\n>human\n>DQN\nDQN\n228%\n79%\n24\n0\nDDQN\n307%\n118%\n33\n43\nDUEL.\n373%\n151%\n37\n50\nPRIOR.\n434%\n124%\n39\n48\nPR. DUEL.\n592%\n172%\n39\n44\nC51\n701%\n178%\n40\n50\nQR-DQN-0\n881%\n199%\n38\n52\nQR-DQN-1\n915%\n211%\n41\n54\nTable 1: Mean and median of best scores across 57 Atari\n2600 games, measured as percentages of human baseline\n(Nair et al. 2015).\nBest agent performance\nTo provide comparable results\nwith existing work we report test evaluation results un-\nder the best agent protocol. Every one million training\nframes, learning is frozen and the agent is evaluated for\n500K frames while recording the average return. Evalua-\ntion episodes begin with up to 30 random no-ops (Mnih\net al. 2015), and the agent uses a lower exploration rate\n(ϵ = 0.001). As training progresses we keep track of the\nbest agent performance achieved thus far.\nTable 1 gives the best agent performance, at 200 million\nframes trained, for QR-DQN, C51, DQN, Double DQN (van\nHasselt, Guez, and Silver 2016), Prioritized replay (Schaul\net al. 2016), and Dueling architecture (Wang et al. 2016). We\nsee that QR-DQN outperforms all previous agents in mean\nand median human-normalized score.\nOnline performance\nIn this evaluation protocol (Fig-\nure 4) we track the average return attained during each test-\ning (left) and training (right) iteration. For the testing perfor-\nmance we use a single seed for each algorithm, but show on-\nline performance with no form of early stopping. For train-\ning performance, values are averages over three seeds. In-\nstead of reporting only median performance, we look at the\ndistribution of human-normalized scores over the full set of\ngames. Each bar represents the score distribution at a ﬁxed\npercentile (10th, 20th, 30th, 40th, and 50th). The upper per-\ncentiles show a similar trend but are omitted here for visual\nclarity, as their scale dwarfs the more informative lower half.\nFrom this, we can infer a few interesting results. (1) Early\nin learning, most algorithms perform worse than random for\nat least 10% of games. (2) QRTD gives similar improvements\nto sample complexity as prioritized replay, while also im-\nproving ﬁnal performance. (3) Even at 200 million frames,\nthere are 10% of games where all algorithms reach less than\n10% of human. This ﬁnal point in particular shows us that\nall of our recent advances continue to be severely limited on\na small subset of the Atari 2600 games.\nConclusions\nThe importance of the distribution over returns in reinforce-\nment learning has been (re)discovered and highlighted many\ntimes by now. In Bellemare, Dabney, and Munos (2017) the\nidea was taken a step further, and argued to be a central part\nof approximate reinforcement learning. However, the paper\nleft open the question of whether there exists an algorithm\nwhich could bridge the gap between Wasserstein-metric the-\nory and practical concerns.\nIn this paper we have closed this gap with both theoreti-\ncal contributions and a new algorithm which achieves state-\nof-the-art performance in Atari 2600. There remain many\npromising directions for future work. Most exciting will be\nto expand on the promise of a richer policy class, made pos-\nsible by action-value distributions. We have mentioned a few\nexamples of such policies, often used for risk-sensitive deci-\nsion making. However, there are many more possible deci-\nsion policies that consider the action-value distributions as a\nwhole.\nAdditionally, QR-DQN is likely to beneﬁt from the im-\nprovements on DQN made in recent years. For instance, due\nto the similarity in loss functions and Bellman operators\nwe might expect that QR-DQN suffers from similar over-\nestimation biases to those that Double DQN was designed\nto address (van Hasselt, Guez, and Silver 2016). A natu-\nral next step would be to combine QR-DQN with the non-\ndistributional methods found in Table 1.\nAcknowledgements\nThe authors acknowledge the vital contributions of their col-\nleagues at DeepMind. Special thanks to Tom Schaul, Au-\ndrunas Gruslys, Charles Blundell, and Benigno Uria for their\nearly suggestions and discussions on the topic of quantile\nregression. Additionally, we are grateful for feedback from\nDavid Silver, Yee Whye Teh, Georg Ostrovski, Joseph Mo-\ndayil, Matt Hoffman, Hado van Hasselt, Ian Osband, Mo-\nhammad Azar, Tom Stepleton, Olivier Pietquin, Bilal Piot;\nand a second acknowledgement in particular of Tom Schaul\nfor his detailed review of an previous draft.\nReferences\nAravkin, A. Y.; Kambadur, A.; Lozano, A. C.; and Luss, R.\n2014. Sparse Quantile Huber Regression for Efﬁcient and\nRobust Estimation. arXiv.\nArjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasser-\nstein Generative Adversarial Networks. In Proceedings of\nthe 34th International Conference on Machine Learning\n(ICML).\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. Journal of Artiﬁcial Intelli-\ngence Research 47:253–279.\nBellemare, M. G.; Danihelka, I.; Dabney, W.; Mohamed, S.;\nLakshminarayanan, B.; Hoyer, S.; and Munos, R. 2017. The\nCramer Distance as a Solution to Biased Wasserstein Gradi-\nents. arXiv.\nBellemare, M. G.; Dabney, W.; and Munos, R. 2017. A\nDistributional Perspective on Reinforcement Learning. Pro-\nceedings of the 34th International Conference on Machine\nLearning (ICML).\nBellman, R. E. 1957. Dynamic Programming. Princeton,\nNJ: Princeton University Press.\nBickel, P. J., and Freedman, D. A. 1981. Some Asymptotic\nTheory for the Bootstrap. The Annals of Statistics 1196–\n1217.\nChow, Y.; Tamar, A.; Mannor, S.; and Pavone, M. 2015.\nRisk-Sensitive and Robust Decision-Making: a CVaR Op-\ntimization Approach. In Advances in Neural Information\nProcessing Systems (NIPS), 1522–1530.\nDearden, R.; Friedman, N.; and Russell, S. 1998. Bayesian\nQ-learning. In Proceedings of the National Conference on\nArtiﬁcial Intelligence.\nEngel, Y.; Mannor, S.; and Meir, R. 2005. Reinforcement\nLearning with Gaussian Processes. In Proceedings of the\nInternational Conference on Machine Learning (ICML).\nEngel, E.\n1857.\nDie Productions-und Consum-\ntionsverh¨altnisse des K¨onigreichs Sachsen. Zeitschrift des\nStatistischen Bureaus des K¨oniglich S¨achsischen Ministeri-\nums des Innern 8:1–54.\nGoldstein, S.; Misra, B.; and Courtage, M. 1981. On Intrin-\nsic Randomness of Dynamical Systems. Journal of Statisti-\ncal Physics 25(1):111–126.\nHeger, M. 1994. Consideration of Risk in Reinforcement\nLearning. In Proceedings of the 11th International Confer-\nence on Machine Learning, 105–111.\nHuber, P. J. 1964. Robust Estimation of a Location Param-\neter. The Annals of Mathematical Statistics 35(1):73–101.\nKingma, D., and Ba, J. 2015. Adam: A Method for Stochas-\ntic Optimization. Proceedings of the International Confer-\nence on Learning Representations.\nKoenker, R., and Hallock, K. 2001. Quantile Regression: An\nIntroduction. Journal of Economic Perspectives 15(4):43–\n56.\nKoenker, R. 2005. Quantile Regression. Cambridge Univer-\nsity Press.\nLevina, E., and Bickel, P. 2001. The Earth Mover’s Distance\nis the Mallows Distance: Some Insights from Statistics. In\nThe 8th IEEE International Conference on Computer Vision\n(ICCV). IEEE.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve-\nness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fid-\njeland, A. K.; Ostrovski, G.; et al.\n2015.\nHuman-level\nControl through Deep Reinforcement Learning.\nNature\n518(7540):529–533.\nMorimura, T.; Hachiya, H.; Sugiyama, M.; Tanaka, T.; and\nKashima, H. 2010. Parametric Return Density Estimation\nfor Reinforcement Learning. In Proceedings of the Confer-\nence on Uncertainty in Artiﬁcial Intelligence (UAI).\nM¨uller, A. 1997. Integral Probability Metrics and their Gen-\nerating Classes of Functions. Advances in Applied Proba-\nbility 29(2):429–443.\nNair, A.; Srinivasan, P.; Blackwell, S.; Alcicek, C.; Fearon,\nR.; De Maria, A.; Panneershelvam, V.; Suleyman, M.; Beat-\ntie, C.; and Petersen, S. e. a. 2015. Massively Parallel Meth-\nods for Deep Reinforcement Learning. In ICML Workshop\non Deep Learning.\nPuterman, M. L. 1994. Markov Decision Processes: Dis-\ncrete stochastic dynamic programming. John Wiley & Sons,\nInc.\nRummery, G. A., and Niranjan, M.\n1994.\nOn-line Q-\nlearning using Connectionist Systems.\nTechnical report,\nCambridge University Engineering Department.\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In Proceedings of the Inter-\nnational Conference on Learning Representations (ICLR).\nSutton, R. S., and Barto, A. G. 1998. Reinforcement Learn-\ning: An Introduction. MIT Press.\nTaylor, J. W. 1999. A Quantile Regression Approach to Esti-\nmating the Distribution of Multiperiod Returns. The Journal\nof Derivatives 7(1):64–78.\nTieleman, T., and Hinton, G. 2012. Lecture 6.5: Rmsprop.\nCOURSERA: Neural Networks for Machine Learning 4(2).\nTsitsiklis, J. N., and Van Roy, B. 1997. An Analysis of\nTemporal-Difference Learning with Function Approxima-\ntion. IEEE Transactions on Automatic Control 42(5):674–\n690.\nvan Hasselt, H.; Guez, A.; and Silver, D. 2016. Deep Rein-\nforcement Learning with Double Q-Learning. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence.\nWang, Z.; Schaul, T.; Hessel, M.; Hasselt, H. v.; Lanctot, M.;\nand de Freitas, N. 2016. Dueling Network Architectures\nfor Deep Reinforcement Learning. In Proceedings of the\nInternational Conference on Machine Learning (ICML).\nWatkins, C. J., and Dayan, P. 1992. Q-learning. Machine\nLearning 8(3):279–292.\nAppendix\nProofs\nLemma 2. For any τ, τ ′ ∈[0, 1] with τ < τ ′ and cumulative\ndistribution function F with inverse F −1, the set of θ ∈R\nminimizing\nZ τ ′\nτ\n|F −1(ω) −θ|dω ,\nis given by\n\u001a\nθ ∈R\n\f\f\f\fF(θ) =\n\u0012τ + τ ′\n2\n\u0013\u001b\n.\nIn particular, if F −1 is the inverse CDF, then F −1((τ +\nτ ′)/2) is always a valid minimizer, and if F −1 is continuous\nat (τ +τ ′)/2, then F −1((τ +τ ′)/2) is the unique minimizer.\nProof. For any ω ∈[0, 1], the function θ 7→|F −1(ω) −θ|\nis convex, and has subgradient given by\nθ 7→\n\n\n\n1\nif θ < F −1(ω)\n[−1, 1]\nif θ = F −1(ω)\n−1\nif θ > F −1(ω) ,\nso the function θ 7→\nR τ ′\nτ |F −1(ω)−θ|dω is also convex, and\nhas subgradient given by\nθ 7→\nZ F (θ)\nτ\n−1dω +\nZ τ ′\nF (θ)\n1dω .\nSetting this subgradient equal to 0 yields\n(τ + τ ′) −2F(θ) = 0 ,\n(14)\nand since F ◦F −1 is the identity map on [0, 1], it is clear\nthat θ = F −1((τ +τ ′)/2) satisﬁes Equation 14. Note that in\nfact any θ such that F(θ) = (τ + τ ′)/2 yields a subgradient\nof 0, which leads to a multitude of minimizers if F −1 is not\ncontinuous at (τ + τ ′)/2.\nProposition 1. Let Zθ be a quantile distribution, and ˆZm\nthe empirical distribution composed of m samples from Z.\nThen for all p ≥1, there exists a Z such that\narg min E[Wp( ˆZm, Zθ)] ̸= arg min Wp(Z, Zθ).\nProof. Write Zθ = PN\ni=1\n1\nN δθi, with θ1 ≤· · · ≤θN. We\ntake Z to be of the same form as Zθ. Speciﬁcally, consider\nZ given by\nZ =\nN\nX\ni=1\n1\nN δi ,\nsupported on the set {1, . . . , N}, and take m = N. Then\nclearly the unique minimizing Zθ for Wp(Z, Zθ) is given by\ntaking Zθ = Z. However, consider the gradient with respect\nto θ1 for the objective\nE[Wp( ˆZN, Zθ)] .\nWe have\n∇θ1E[Wp( ˆZN, Zθ)]|θ1=1 = E[∇θ1Wp( ˆZN, Zθ)|θ1=1] .\nIn the event that the sample distribution ˆZN has an atom at\n1, then the optimal transport plan pairs the atom of Zθ at\nθ1 = 1 with this atom of ˆZN, and gradient with respect to\nθ1 of Wp( ˆZN, Zθ) is 0. If the sample distribution ˆZN does\nnot contain an atom at 1, then the left-most atom of ˆZN is\ngreater than 1 (since Z is supported on {1, . . . , N}. In this\ncase, the gradient on θ1 is negative. Since this happens with\nnon-zero probability, we conclude that\n∇θ1E[Wp( ˆZN, Zθ)]|θ1=1 < 0 ,\nand therefore Zθ\n=\nZ cannot be the minimizer of\nE[Wp( ˆZN, Zθ)].\nProposition 2. Let ΠW1 be the quantile projection deﬁned\nas above, and when applied to value distributions gives the\nprojection for each state-value distribution. For any two\nvalue distributions Z1, Z2 ∈Z for an MDP with countable\nstate and action spaces,\n¯d∞(ΠW1T πZ1, ΠW1T πZ2) ≤γ ¯d∞(Z1, Z2).\n(11)\nProof. We assume that instantaneous rewards given a state-\naction pair are deterministic; the general case is a straight-\nforward generalization. Further, since the operator T π is a\nγ-contraction in d∞, it is sufﬁcient to prove the claim in the\ncase γ = 1. In addition, since Wasserstein distances are in-\nvariant under translation of the support of distributions, it is\nsufﬁcient to deal with the case where r(x, a) ≡0 for all\n(x, a) ∈X × A. The proof then proceeds by ﬁrst reducing\nto the case where every value distribution consists only of\nsingle Diracs, and then dealing with this reduced case using\nLemma 3.\nWe write Z(x, a) = PN\nk=1\n1\nN δθk(x,a) and Y (x, a) =\nPN\nk=1\n1\nN δψk(x,a), for some functions θ, ψ : X × A →Rn.\nLet (x, a) be a state-action pair, and let ((xi, ai))i∈I be all\nthe state-action pairs that are accessible from (x′, a′) in a\nsingle transition, where I is a (ﬁnite or countable) index-\ning set. Write pi for the probability of transitioning from\n(x′, a′) to (xi, ai), for each i ∈I. We now construct a new\nMDP and new value distributions for this MDP in which\nall distributions are given by single Diracs, with a view\nto applying Lemma 3. The new MDP is of the following\nform. We take the state-action pair (x′, a′), and deﬁne new\nstates, actions, transitions, and a policy eπ, so that the state-\naction pairs accessible from (x′, a′) in this new MDP are\ngiven by ((exj\ni, eaj\ni)i∈I)N\nj=1, and the probability of reaching\nthe state-action pair (exj\ni, eaj\ni) is pi/n. Further, we deﬁne new\nvalue distributions eZ, eY as follows. For each i ∈I and\nj = 1, . . . , N, we set:\neZ(exj\ni, eaj\ni) = δθj(xi,ai)\neY (exj\ni, eaj\ni) = δψj(xi,ai) .\nThe construction is illustrated in Figure 5.\nSince, by Lemma 4, the d∞distance between the 1-\nWasserstein projections of two real-valued distributions is\nthe max over the difference of a certain set of quantiles, we\nFigure 5: Initial MDP and value distribution Z (top), and\ntransformed MDP and value distribution eZ (bottom).\nmay appeal to Lemma 3 to obtain the following:\nd∞(ΠW1(T eπ eZ)(x′, a′), ΠW1(T eπ eY )(x′, a′))\n≤\nsup\ni=1∈I\nj=1,...,N\n|θj(xi, ai) −ψj(xi, ai)|\n= sup\ni=1∈I\nd∞(Z(xi, ai), Y (xi, ai))\n(15)\nNow note that by construction, (T eπ eZ)(x′, a′) (re-\nspectively, (T eπ eY )(x′, a′)) has the same distribution as\n(T πZ)(x′, a′) (respectively, (T πY )(x′, a′)), and so\nd∞(ΠW1(T eπ eZ)(x′, a′), ΠW1(T eπ eY )(x′, a′))\n= d∞(ΠW1(T πZ)(x′, a′), ΠW1(T πY )(x′, a′)) .\nTherefore, substituting this into the Inequality 15, we obtain\nd∞(ΠW1(T πZ)(x′, a′), ΠW1(T πY )(x′, a′))\n≤sup\ni∈I\nd∞(Z(xi, ai), Y (xi, ai)) .\nTaking suprema over the initial state (x′, a′) then yields the\nresult.\nSupporting results\nLemma 3. Consider an MDP with countable state and ac-\ntion spaces. Let Z, Y be value distributions such that each\nstate-action distribution Z(x, a), Y (x, a) is given by a sin-\ngle Dirac. Consider the particular case where rewards are\nidentically 0 and γ = 1, and let τ ∈[0, 1]. Denote by Πτ\nthe projection operator that maps a probability distribution\nonto a Dirac delta located at its τ th quantile. Then\nd∞(ΠτT πZ, ΠτT πY ) ≤d∞(Z, Y )\nProof. Let Z(x, a) = δθ(x,a) and Y (x, a) = δψ(x,a) for\neach state-action pair (x, a) ∈X × A, for some functions\nψ, θ : X ×A →R. Let (x′, a′) be a state-action pair, and let\n((xi, ai))i∈I be all the state-action pairs that are accessible\nfrom (x′, a′) in a single transition, with I a (ﬁnite or count-\nably inﬁnite) indexing set. To lighten notation, we write θi\nfor θ(xi, ai) and ψi for ψ(xi, ai). Further, let the probability\nof transitioning from (x′, a′) to (xi, ai) be pi, for all i ∈I.\nThen we have\n(T πZ)(x′, a′) =\nX\ni∈I\npiδθi\n(16)\n(T πY )(x′, a′) =\nX\ni∈I\npiδψi .\n(17)\nNow consider the τ th quantile of each of these distributions,\nfor τ ∈[0, 1] arbitrary. Let u ∈I be such that θu is equal to\nthis quantile of (T πZ)(x′, a′), and let v ∈I such that ψv is\nequal to this quantile of (T πY )(x′, a′). Now note that\nd∞(ΠτT πZ(x′, a′), ΠτT πY (x′, a′)) = |θu −ψv|\nWe now show that\n|θu −ψv| > |θi −ψi| ∀i ∈I\n(18)\nis impossible, from which it will follow that\nd∞(ΠτT πZ(x′, a′), ΠτT πY (x′, a′)) ≤d∞(Z, Y ) ,\nand the result then follows by taking maxima over state-\naction pairs (x′, a′). To demonstrate the impossibility of\n(18), without loss of generality we take θu ≤ψv.\nWe now introduce the following partitions of the indexing\nset I. Deﬁne:\nI≤θu = {i ∈I|θi ≤θu} ,\nI>θu = {i ∈I|θi > θu} ,\nI<ψv = {i ∈I|ψi < ψv} ,\nI≥ψv = {i ∈I|ψi ≥ψv} ,\nand observe that we clearly have the following disjoint\nunions:\nI = I≤θu ∪I>θu ,\nI = I<ψv ∪I≥ψv .\nIf (18) is to hold, then we must have I≤θu ∩I≥ψv = ∅.\nTherefore, we must have I≤θu ⊆I<ψv. But if this is the\ncase, then since θu is the τ th quantile of (T πZ)(x′, a′), we\nmust have\nX\ni∈I≤θu\npi ≥τ ,\nand so consequently\nX\ni∈I<ψv\npi ≥τ ,\nfrom\nwhich\nwe\nconclude\nthat\nthe\nτ th\nquantile\nof\n(T πY )(x′, a′) is less than ψv, a contradiction. Therefore\n(18) cannot hold, completing the proof.\nLemma 4. For any two probability distributions ν1, ν2 over\nthe real numbers, and the Wasserstein projection operator\nΠW1 that projects distributions onto support of size n, we\nhave that\nd∞(ΠW1ν1, ΠW1ν2)\n= max\ni=1,...,n\n\f\f\f\fF −1\nν1\n\u00122i −1\n2n\n\u0013\n−F −1\nν2\n\u00122i −1\n2n\n\u0013\f\f\f\f .\nProof. By the discussion surrounding Lemma 2, we have\nthat ΠW1νk = Pn\ni=1\n1\nnδF −1\nνk ( 2i−1\n2n ) for k = 1, 2. Therefore,\nthe optimal coupling between ΠW1ν1 and ΠW1ν2 must be\ngiven by F −1\nν1 ( 2i−1\n2n ) 7→F −1\nν2 ( 2i−1\n2n ) for each i = 1, . . . , n.\nThis immediately leads to the expression of the lemma.\nFurther theoretical results\nLemma 5. The projected Bellman operator ΠW1T π is in\ngeneral not a non-expansion in dp, for p ∈[1, ∞).\nProof. Consider the case where the number of Dirac deltas\nin each distribution, N, is equal to 2, and let γ = 1. We con-\nsider an MDP with a single initial state, x, and two terminal\nstates, x1 and x2. We take the action space of the MDP to\nbe trivial, and therefore omit it in the notation that follows.\nLet the MDP have a 2/3 probability of transitioning from\nx to x1, and 1/3 probability of transitioning from x to x2.\nWe take all rewards in the MDP to be identically 0. Further,\nconsider two value distributions, Z and Y , given by:\nZ(x1) = 1\n2δ0 + 1\n2δ2 , Y (x1) = 1\n2δ1 + 1\n2δ2 ,\nZ(x2) = 1\n2δ3 + 1\n2δ5 , Y (x2) = 1\n2δ4 + 1\n2δ5 ,\nZ(x) = δ0 , Y (x) = δ0 .\nThen note that we have\ndp(Z(x1), Y (x1)) =\n\u00121\n2|1 −0|\n\u00131/p\n=\n1\n21/p ,\ndp(Z(x2), Y (x2)) =\n\u00121\n2|4 −3|\n\u00131/p\n=\n1\n21/p ,\ndp(Z(x), Y (x)) = 0 ,\nand so\ndp(Z, Y ) =\n1\n21/p .\nWe now consider the projected backup for these two value\ndistributions at the state x. We ﬁrst compute the full backup:\n(T πZ)(x) = 1\n3δ0 + 1\n3δ2 + 1\n6δ3 + 1\n6δ5 ,\n(T πY )(x) = 1\n3δ1 + 1\n3δ2 + 1\n6δ4 + 1\n6δ5 .\nAppealing to Lemma 2, we note that when projected these\ndistributions onto two equally-weighted Diracs, the loca-\ntions of these Diracs correspond to the 25% and 75% quan-\ntiles of the original distributions. We therefore have\n(ΠW1T πZ)(x) = 1\n2δ0 + 1\n2δ3 ,\n(ΠW1T πY )(x) = 1\n2δ1 + 1\n2δ4 ,\nand we therefore obtain\nd1(ΠW1T πZ, ΠW1T πY ) =\n\u00121\n2(|1 −0|p + |4 −3|p)\n\u00131/p\n=1 >\n1\n21/p = d1(Z, Y ) ,\ncompleting the proof.\nNotation\nHuman-normalized scores are given by (van Hasselt, Guez,\nand Silver 2016),\nscore = agent −random\nhuman −random,\nwhere agent, human and random represent the per-game\nraw scores for the agent, human baseline, and random agent\nbaseline.\nTable 2: Notation used in the paper\nSymbol\nDescription of usage\nReinforcement Learning\nM\nMDP (X, A, R, P, γ)\nX\nState space of MDP\nA\nAction space of MDP\nR, Rt\nReward function, random variable reward\nP\nTransition probabilities, P(x′|x, a)\nγ\nDiscount factor, γ ∈[0, 1)\nx, xt ∈X\nStates\na, a∗, b ∈A\nActions\nr, rt ∈R\nRewards\nπ\nPolicy\nT π\n(dist.) Bellman operator\nT\n(dist.) Bellman optimality operator\nV π, V\nValue function, state-value function\nQπ, Q\nAction-value function\nα\nStep-size parameter, learning rate\nϵ\nExploration rate, ϵ-greedy\nϵADAM\nAdam parameter\nκ\nHuber-loss parameter\nLκ\nHuber-loss with parameter κ\nDistributional Reinforcement Learning\nZπ, Z\nRandom return, value distribution\nZπ\nMC\nMonte-Carlo value distribution under policy π\nZ\nSpace of value distributions\nˆZπ\nFixed point of convergence for ΠW1T π\nz ∼Z\nInstantiated return sample\np\nMetric order\nWp\np-Wasserstein metric\nLp\nMetric order p\n¯dp\nmaximal form of Wasserstein\nΦ\nProjection used by C51\nΠW1\n1-Wasserstein projection\nρτ\nQuantile regression loss\nρκ\nτ\nHuber quantile loss\nq1, . . . , qN\nProbabilities, parameterized probabilities\nτ0, τ1, . . . , τN\nCumulative probabilities with τ0 := 0\nˆτ1, . . . , ˆτN\nMidpoint quantile targets\nω\nSample from unit interval\nδz\nDirac function at z ∈R\nθ\nParameterized function\nB\nBernoulli distribution\nBµ\nParameterized Bernoulli distribution\nZQ\nSpace of quantile (value) distributions\nZθ\nParameterized quantile (value) distribution\nY\nRandom variable over R\nY1, . . . , Ym\nRandom variable samples\nˆYm\nEmpirical distribution from m-Diracs\nDQN\nC51\nQR-DQN-0\nQR-DQN-1\nFigure 6: Online training curves for DQN, C51, and QR-DQN on 57 Atari 2600 games. Curves are averages over three seeds,\nsmoothed over a sliding window of 5 iterations, and error bands give standard deviations.\nGAMES\nRANDOM\nHUMAN\nDQN\nPRIOR. DUEL.\nC51\nQR-DQN-0\nQR-DQN-1\nAlien\n227.8\n7,127.7\n1,620.0\n3,941.0\n3,166\n9,983\n4,871\nAmidar\n5.8\n1,719.5\n978.0\n2,296.8\n1,735\n2,726\n1,641\nAssault\n222.4\n742.0\n4,280.4\n11,477.0\n7,203\n19,961\n22,012\nAsterix\n210.0\n8,503.3\n4,359.0\n375,080.0\n406,211\n454,461\n261,025\nAsteroids\n719.1\n47,388.7\n1,364.5\n1,192.7\n1,516\n2,335\n4,226\nAtlantis\n12,850.0\n29,028.1\n279,987.0\n395,762.0\n841,075\n1,046,625\n971,850\nBank Heist\n14.2\n753.1\n455.0\n1,503.1\n976\n1,245\n1,249\nBattle Zone\n2,360.0\n37,187.5\n29,900.0\n35,520.0\n28,742\n35,580\n39,268\nBeam Rider\n363.9\n16,926.5\n8,627.5\n30,276.5\n14,074\n24,919\n34,821\nBerzerk\n123.7\n2,630.4\n585.6\n3,409.0\n1,645\n34,798\n3,117\nBowling\n23.1\n160.7\n50.4\n46.7\n81.8\n85.3\n77.2\nBoxing\n0.1\n12.1\n88.0\n98.9\n97.8\n99.8\n99.9\nBreakout\n1.7\n30.5\n385.5\n366.0\n748\n766\n742\nCentipede\n2,090.9\n12,017.0\n4,657.7\n7,687.5\n9,646\n9,163\n12,447\nChopper Command\n811.0\n7,387.8\n6,126.0\n13,185.0\n15,600\n7,138\n14,667\nCrazy Climber\n10,780.5\n35,829.4\n110,763.0\n162,224.0\n179,877\n181,233\n161,196\nDefender\n2,874.5\n18,688.9\n23,633.0\n41,324.5\n47,092\n42,120\n47,887\nDemon Attack\n152.1\n1,971.0\n12,149.4\n72,878.6\n130,955\n117,577\n121,551\nDouble Dunk\n-18.6\n-16.4\n-6.6\n-12.5\n2.5\n12.3\n21.9\nEnduro\n0.0\n860.5\n729.0\n2,306.4\n3,454\n2,357\n2,355\nFishing Derby\n-91.7\n-38.7\n-4.9\n41.3\n8.9\n37.4\n39.0\nFreeway\n0.0\n29.6\n30.8\n33.0\n33.9\n34.0\n34.0\nFrostbite\n65.2\n4,334.7\n797.4\n7,413.0\n3,965\n4,839\n4,384\nGopher\n257.6\n2,412.5\n8,777.4\n104,368.2\n33,641\n118,050\n113,585\nGravitar\n173.0\n3,351.4\n473.0\n238.0\n440\n546\n995\nH.E.R.O.\n1,027.0\n30,826.4\n20,437.8\n21,036.5\n38,874\n21,785\n21,395\nIce Hockey\n-11.2\n0.9\n-1.9\n-0.4\n-3.5\n-3.6\n-1.7\nJames Bond\n29.0\n302.8\n768.5\n812.0\n1,909\n1,028\n4,703\nKangaroo\n52.0\n3,035.0\n7,259.0\n1,792.0\n12,853\n14,780\n15,356\nKrull\n1,598.0\n2,665.5\n8,422.3\n10,374.4\n9,735\n11,139\n11,447\nKung-Fu Master\n258.5\n22,736.3\n26,059.0\n48,375.0\n48,192\n71,514\n76,642\nMontezuma’s Revenge\n0.0\n4,753.3\n0.0\n0.0\n0.0\n75.0\n0.0\nMs. Pac-Man\n307.3\n6,951.6\n3,085.6\n3,327.3\n3,415\n5,822\n5,821\nName This Game\n2,292.3\n8,049.0\n8,207.8\n15,572.5\n12,542\n17,557\n21,890\nPhoenix\n761.4\n7,242.6\n8,485.2\n70,324.3\n17,490\n65,767\n16,585\nPitfall!\n-229.4\n6,463.7\n-286.1\n0.0\n0.0\n0.0\n0.0\nPong\n-20.7\n14.6\n19.5\n20.9\n20.9\n21.0\n21.0\nPrivate Eye\n24.9\n69,571.3\n146.7\n206.0\n15,095\n146\n350\nQ*Bert\n163.9\n13,455.0\n13,117.3\n18,760.3\n23,784\n26,646\n572,510\nRiver Raid\n1,338.5\n17,118.0\n7,377.6\n20,607.6\n17,322\n9,336\n17,571\nRoad Runner\n11.5\n7,845.0\n39,544.0\n62,151.0\n55,839\n67,780\n64,262\nRobotank\n2.2\n11.9\n63.9\n27.5\n52.3\n61.1\n59.4\nSeaquest\n68.4\n42,054.7\n5,860.6\n931.6\n266,434\n2,680\n8,268\nSkiing\n-17,098.1\n-4,336.9\n-13,062.3\n-19,949.9\n-13,901\n-9,163\n-9,324\nSolaris\n1,236.3\n12,326.7\n3,482.8\n133.4\n8,342\n2,522\n6,740\nSpace Invaders\n148.0\n1,668.7\n1,692.3\n15,311.5\n5,747\n21,039\n20,972\nStar Gunner\n664.0\n10,250.0\n54,282.0\n125,117.0\n49,095\n70,055\n77,495\nSurround\n-10.0\n6.5\n-5.6\n1.2\n6.8\n9.7\n8.2\nTennis\n-23.8\n-8.3\n12.2\n0.0\n23.1\n23.7\n23.6\nTime Pilot\n3,568.0\n5,229.2\n4,870.0\n7,553.0\n8,329\n9,344\n10,345\nTutankham\n11.4\n167.6\n68.1\n245.9\n280\n312\n297\nUp and Down\n533.4\n11,693.2\n9,989.9\n33,879.1\n15,612\n53,585\n71,260\nVenture\n0.0\n1,187.5\n163.0\n48.0\n1,520\n0.0\n43.9\nVideo Pinball\n16,256.9\n17,667.9\n196,760.4\n479,197.0\n949,604\n701,779\n705,662\nWizard Of Wor\n563.5\n4,756.5\n2,704.0\n12,352.0\n9,300\n26,844\n25,061\nYars’ Revenge\n3,092.9\n54,576.9\n18,098.9\n69,618.1\n35,050\n32,605\n26,447\nZaxxon\n32.5\n9,173.3\n5,363.0\n13,886.0\n10,513\n7,200\n13,112\nFigure 7: Raw scores across all games, starting with 30 no-op actions. Reference values from Wang et al. (2016) and Bellemare,\nDabney, and Munos (2017).\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2017-10-27",
  "updated": "2017-10-27"
}