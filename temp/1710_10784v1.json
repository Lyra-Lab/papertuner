{
  "id": "http://arxiv.org/abs/1710.10784v1",
  "title": "How deep learning works --The geometry of deep learning",
  "authors": [
    "Xiao Dong",
    "Jiasong Wu",
    "Ling Zhou"
  ],
  "abstract": "Why and how that deep learning works well on different tasks remains a\nmystery from a theoretical perspective. In this paper we draw a geometric\npicture of the deep learning system by finding its analogies with two existing\ngeometric structures, the geometry of quantum computations and the geometry of\nthe diffeomorphic template matching. In this framework, we give the geometric\nstructures of different deep learning systems including convolutional neural\nnetworks, residual networks, recursive neural networks, recurrent neural\nnetworks and the equilibrium prapagation framework. We can also analysis the\nrelationship between the geometrical structures and their performance of\ndifferent networks in an algorithmic level so that the geometric framework may\nguide the design of the structures and algorithms of deep learning systems.",
  "text": "1\nHow deep learning works — The geometry of deep learning\nXiao Dong, Jiasong Wu, Ling Zhou\nFaculty of Computer Science and Engineering, Southeast University, Nanjing, China\nWhy and how that deep learning works well on different tasks remains a mystery from a theoretical perspective. In this paper\nwe draw a geometric picture of the deep learning system by ﬁnding its analogies with two existing geometric structures, the\ngeometry of quantum computations and the geometry of the diffeomorphic template matching. In this framework, we give the\ngeometric structures of different deep learning systems including convolutional neural networks, residual networks, recursive neural\nnetworks, recurrent neural networks and the equilibrium prapagation framework. We can also analysis the relationship between\nthe geometrical structures and their performance of different networks in an algorithmic level so that the geometric framework\nmay guide the design of the structures and algorithms of deep learning systems.\nIndex Terms—Deep learning, geometry, quantum computation, computational anatomy\nI. INTRODUCTION\nIn the last decade, deep learning systems show a fascinating\nperformance in solving different complex tasks. We have\ndesigned different system structures for different problems and\nrevealed some general rules for the designing of deep learning\nsystems. There are also theoretical attempts to understand\ndeep learning systems from both mathematical and physical\nperspectives[1]. But still we are lacking of a theoretical\nframework to answer the question, why and how deep learning\nsystems works. Also it’s highly desired that a theoretical\nframework of deep learning systems can be used to guide\nthe design the structures of deep learning systems from an\nalgorithmic level.\nIn this paper we try to ﬁll this gap by drawing a geometric\npicture of deep learning systems. We build our geometric\nframework to understand deep learning systems by comparing\nthe deep learning system with other two existing matural\ngeometric structures, the geometry of quantum computations\nand the geometry of diffeomorphic template matching in the\nﬁeld of computational anatomy. We show that deep learning\nsystems can be formulated in a geometric language, by which\nwe can draw geometric pictures of different deep learning\nsystems including convolutional neural networks, residual\nnetworks, recursive neural networks, fractal neural networks\nand recurrent neural networks. What’s more, these geometric\npictures can be used to analysis the performance of different\ndeep learning structures and provide guidance to the design of\ndeep learning systems from an algorithmic level.\nThe rest of this paper is arranged as follows. We will\nﬁrst give a brief overview of the geometry of quantum\ncomputations and the geometry of diffeomorphic template\nmatching. Then we will explain the geometric framework\nof deep learning systems and apply our framework to draw\ncorrespondent geometric pictures of different deep learning\nnetworks. Finally we will give a general optimization based\nframework of deep learning systems, which can be used to\naddress the equilibrium propagation algorithm.\nII. WHAT CAN GEOMETRY TEACH US\nIt’s well known that geometry is not only a core concept of\nmathematics, but also it plays a key role in modern physics.\nThe great success of geometrization of physics tells us that\nthe soul of physical systems lies in their geometric structures.\nIt’s natural to ask if geometry can help to reveal the secret\nof our human intelligence and our state-of-the-art artiﬁcial\nintelligence systems, deep learning systems. The answer is\nYES.\nWe will ﬁrst introduce two interesting geometric structures\nin the ﬁelds of quantum computation and computational\nanatomy. We will see, their geometric structures share some\nsimilarities with deep learning systems and the geometric\nframework of deep learning systems can be built based on\nan understanding of these structures.\nA. Geometry of quantum computation\nGeometry concepts have been widely discussed in formulat-\ning quantum mechanics and quantum information processing\nsystems, including the geometry of quantum states and their\nevolution[2][3][4], the geometry of entanglement[5][6] and\nalso the relationship between the spacetime structure and the\ngeometry of quantum states[7][8][9].\nIn [10][11] a geometric framework was proposed for the\ncomplexity of quantum computations. Its basic idea is to\nintroduce a Riemannian metric to the space of n-qubit uni-\ntary operators so that the quantum computation complexity\nbecomes a geometric concept as given by the slogan quantum\ncomputation as free falling. The key components of this\nframework can be summarized as follows.\n• An algorithm of a n-qubit quantum computation system\nis a unitary operation U ∈U(2n), which can evolve the\nn-qubit initial state |ψini⟩= |00...0⟩to the ﬁnal state\n|ψfin⟩= U|ψ⟩ini. U(2n) is the space of the unitary\noperations of n-qubits, which is both a manifold and a\nLie group.\n• Any physical realization of the algorithm U is a curve\nU(t) ∈U(2n), t ∈[0, 1] with U(0) = I, U(1) = U,\nwhere I is the identity operator. A smooth curve U(t)\ncan be achieved by a Hamiltonian H(t) so that ˙U(t) =\n−iH(t)U(t), where H(t) is in the tangent space of\nU(2n) at U(t) and also an element of the Lie algebra\nu(2n).\narXiv:1710.10784v1  [cs.LG]  30 Oct 2017\n2\n• A Riemannian metric ⟨·, ·⟩U can be introduced to the\nmanifold U(2n) so that a Riemannian structure can be\nbuilt on U(2n). The metric is deﬁned as\n⟨H, J⟩U = [tr(HP(J)) + qtr(HQ(J))]/2n\n(1)\nwhere J, P are tangent vectors in the tangent space at\nU, and P(J) is the projection of J to a simple local\nHamiltonian space P, Q = 1 −P and q >> 1. With this\nmetric any ﬁnite length curve U(t) connecting the identity\nI and U will only contain local Hamiltonian when q →\n∞. Usually the geodesic Ugeo(t) connecting I and U has\nthe minimal length, which is deﬁned as the complexity\nof the algorithm U. We also know the geodesic is given\nby the EPDiff equation, which is obtained by solving the\noptimization problem minH(t)\nR\n⟨H(t), H(t)⟩1/2\nU dt.\n• The quantum circuit model of quantum computations tells\nus that any U can be approximated with any accuracy\nwith a small universal gate set, which contains only\nsimple local operators, for example only a ﬁnite number\nof 1 and 2 qubit operators. The quantum circuit model\ncan then be understood as approximating U(t) with a\npiecewise smooth curve, where each segment corresponds\nto a universal gate. It’s proven in [10][11] that the number\nof universal gates to approximate U within some constant\nerror is given by O(ntd(I, U)3), where d(I, U) is the\ngeodesic distance between I and U. So an algorithm U\ncan be efﬁciently realized by the quantum circuit model\nonly when its geodesic distance to the identity operator\nI is polynomial with respect to n.\n• The set of operators that can be reached by a geodesic\nwith a length that is polynomial with the qubit number\nn is only a small subset of U(2n). Equivalently, starting\nfrom a simple initial state |ψ⟩ini = |000...0⟩, the quantum\nstate that can be obtained efﬁciently by local quantum\noperations is a small subset of the quantum state space.\nThis deﬁnes the complexity of a quantum state.\n• Finding the geodesic U geo(t) can be achieved by either\nthe lifted Jacobi ﬁeld or standard geodesic shooting al-\ngorithm. The performance of the optimization procedure\nto ﬁnd Ugeo(t) highly depends on the curvature of the\nRiemannian manifold. For the metric given in (1), the\nsectional curvature is almost negative everywhere when\nq is large enough. This means the geodesic is unstable\nand it’s general difﬁcult to ﬁnd a geodesic.\nThe key lesson of the geometry of quantum computations\nis that: Under a local Hamiltonian preferred metric, the\ncomplexity of a general quantum algorithm U is expo-\nnential with the number of qubit n. If the complexity\nU is polynomial, it can be achieved efﬁciently with local\noperators, either by a continuous local Hamiltonian H(t)\nor a set of universal local operators. The performance of\nthe optimization procedure to ﬁnd a good realization of U\ndepends on the curvature of the Riemannian manifold.\nWe will see that this observation is closely related with the\nquestion ofwhy and how cheap deep learning works.\nFig. 1.\nGeometry of the quantum computation. The yellow area gives the\nspace of quantum algorithms that can be efﬁciently achieved by the quantum\ncircuit model. Given a Riemannian structure on U(2n), there exist an arbitrary\nnumber of ways to achieve an algorithm U and its complexity is given by the\nlength of the geodesic connecting I and U. Any efﬁcient realization curve\nU(t) can approximated by an universal gate set with a polynomial cost as\nshown by the red segmented line.\nB. Geometry of diffeomorphic template matching\nThe diffeomorphic framework of computational anatomy\naims to achieve a diffeomorphic matching between images\non a Riemannian structure or a Lie group. The geometry of\ndiffeomorphic template matching not only provides a math-\nematical framework for image registration but also validates\nthe possibility of statistical analysis of anatomical structures\non manifolds[12][13][14][15][16][17][18].\nWe can summarize the geometry of diffeomorphic template\nmatching as follows:\n• The Riemannian geometry of diffeomorphic template\nmatching starts with the claim that a diffeomorphic\nmatching between two images I0 and I1 deﬁned on a\nspace M = Rn (n = 2 for 2 dimensional images or\nn = 3 for 3 dimensional volume data) is achieved by a\ndiffeomorphic transform g ∈Diff(M). Diff(M) the\ndiffeomorphism transformation group of the space M.\n• A matching between I0 and I1 is achieved by a smooth\ncurve g(t) on Diff(M) connecting the identity trans-\nform I and g. The optimal g(t) is obtained by an\noptimization problem\nmin\nu(t) E = min\nu(t)\nZ 1\n0\n⟨u(t), u(t)⟩vdt + 1\nσ2 ∥I1 −I0 ◦g1∥2\nL2\n(2)\nwhere\n˙\ng(t) = u(t) ◦g(t), g(0) = I and ⟨u(t), u(t)⟩v =\n⟨Lu(t), Lu(t)⟩L2 is a metric on the tangent space of the\nmanifold Diff(M) with L a linear operator.\nThis is the LDDMM framework[12] and the optimization\nresults in the Euler-Lagrange equation given by\n∇u(t)E = 2ut −K( 2\nσ2 |Dgut\nt,1|∇J0\nt (J0\nt −J1\nt )) = 0 (3)\nwhere J0\nt\n= I0 ◦gut\nt,0,J1\nt\n= I1 ◦gut\nt,1 and gut\ns,t is the\ntransformation generated by u(t) during the time internal\n3\nFig. 2.\nThe geometry of the diffeomorphic template matching. Left: A\ndiffeomorphic matching is a curve g(t) in Diff(M), the space of the\ndiffeomorphic transformationg of the image; Right: The image deﬁned on\nM is smoothly deformed from I0 to a near neighbour of I1 by the curve\ng(t).\ns to t. K is the operator deﬁned by ⟨a, b⟩L2 = ⟨Ka, b⟩v,\ni.e., K(L+L) = 1. In computational anatomy K is\nusually taken as a Gaussian kernel, whose effect is to\ngenerate a smooth velocity ﬁeld u(t) that leads to a\nsmooth transformation g(t).\nThe geometric picture of LDDMM is to ﬁnd a geodesic\ng(t) that transforms I0 = Io ◦g(0) to a near neighbour\nof I1 so that I1 ≈I0 ◦g(1). The optimal velocity ﬁeld\nu(t) is obtained with a gradient descent algorithm.\n• An\nalternative\napproach\nis\nthe\ngeodesic\nshooting\nalgorithm[19][20][21], which is essentially a variational\nproblem on the initial velocity u(0) of the geodesic g(t)\nand the EPDiff equation is taken as a constraint.\n• The sectional curvature of the volume preserving dif-\nfeomorphisms of the ﬂat torus with a weak L2 right\ninvariant metric is negative in may directions so the\ngeodesic on this manifold can be unstable. We indicate\nthis point to show that here we might also face a similar\nnegative curvature case as in the geometry of the quantum\ncomputation.\n• If instead we do not introduce the metric ⟨u(t), u(t)⟩v in\nDiff(M), Diff(M) can be regarded as a Lie group.\nThen an alternative matching curve g(t) can be formu-\nlated as a Lie group exponential map exp(gt), t ∈[0, 1]\nand g ∈diff(M) with diff(M) as the Lie algebra\nof Diff(M). This is how the SVF framework[13][14]\nformulates the template matching problem.\n• There exists a mathematically strict framework for the\ngeometry of template matching which shares lots of sim-\nilarities with mechanical systems. For more information\nof the geometry of template matching, please refer to\n[14][16][17][18][22].\nThe key concept of the geometry of template matching is:\nThe template matching can be achieved by either a Rie-\nmannian geodesic or a Lie group exponential mapping in\nDiff(M) that transforms the source image to approximate\nthe destination image. Finding the optimal curve can be\nformulated as an optimization problem. The Riemannian\nmetric or the Lie group structure of Diff(M) play a\ncentral role in the optimization procedure.\nObviously the two geometric structures share lots of com-\nmon characteristics. Both systems consider the geometry of\nFig. 3.\nThe geometry of the diffeomorphic computational anatomy. Left:\nThe geometry of the template matching on Riemannian manifolds, where\nthe optimal matching is achieved by a geodesic; Right: The geometry of the\ntemplate matching on a Lie group, where the optimal matching is given by\nthe Lie group exponential mapping.\na mapping T × V →V , where V is a vector space (the\nstate space of n-qubits or the space of images on M = Rn\nrespectively) and T is the space of transformations on\nV (SU(2n) or Diff(M)). By introducing a Riemannian\nmetric on T and an inner product on V , both of them\naddress the problem of ﬁnding a curve on a Riemannian\nmanifold (SU(2n) or Diff(M)) to connect the identity\noperator with a target operator(U or g). Equivalently this\ncurve can achieve a transformation from an initial state\n(|ψ⟩ini or I0) to a near neighbourhood (deﬁned by the\ninner product on V ) of a ﬁnal state (|ψ⟩fin or I1) in V .\nThe problem can be formalized as an optimization task and\nthe optimal curve is a Riemannian geodesic (an Lie group\nexponential for SVF). The stability of geodesics and the\nconvergence property of the optimization procedure highly\ndepend on the Riemannian metric of the Riemannian\nmanifold. In both geometric structure, we have to fact\nthe negative sectional curvature problem.\nBesides their similarities, there exist a key difference be-\ntween these two geometric structures. The geometry of the\nquantum computation addresses the problem to reach a point\nefﬁciently by local operations. So it focuses on the com-\nputability of a physical system. For more discussion on the\ncomputability and physical systems, please refer to [10]. On\nthe contrary, in the template matching problem, we do not\nemphasis to use local operations. The reason falls in that\nthe deformation curve g(t) is in fact generated purely by\nlocal operations since it’s an integration curve of a time-\nvarying velocity ﬁeld, which is essentially local. Also in the\ntemplate matching problem, it seems that we do not care if a\ntransformation g ∈Diff(M) can be efﬁciently computed.\nThe reason for the difference between them is related to the\nconcept of the diameter of a Riemannian manifold, which is\ndeﬁned by the maximal geodesic distance between arbitrary\npoints on the manifold. So for the geometry of the quantum\ncomputation, the manifold has an exponentially large diameter\nwith respect to the qubit number n. So only a subset of the\noperations can be efﬁciently carried out. For the template\nmatching problem, the diameter is usually ﬁnite. In fact the\nreal situation is quite complex. For example, if we work on\nthe volume-preserving diffeomorphism group SDiff(M n) of\na n-dimensional cube M n in Rn, the diameter is ﬁnite for\n4\nn ≥3 but inﬁnite for n = 2 if a L2 right-invariant metric is\nused. For Diff(M n), a H1 metric results in a ﬁnite diameter,\nand a L2 metric can even lead to a vanishing geodesic distance,\nwhich means any two points can be connected with a geodesic\nwith an arbitrary small distance. So generally we assume all\nthe transformations in the template matching problem can be\nefﬁciently computed.\nWe will see that the diameter of the Riemannian manifold of\nthe deep learning systems is closed related with the question\nwhy deep learning works.\nIII. GEOMETRY OF DEEP LEARNING\nBased on the above mentioned geometric structures, we\npropose a geometric framework to understand deep learning\nsystems. We will ﬁrst focus on the most popular deep learning\nstructure, the deep convolution neural network(CNN). Then\nthe same framework can be used to draw geometric pictures\nof other systems including the Residual Network(ResNet), the\nrecursive neural network, the fractal neural network and the\nrecurrent network.\nA. Geometry of convolutional neural networks\nThe convolutional neural network is the most well studied\ndeep learning structure, which can achieve reliable object\nrecognition in computer vision systems by stacking mul-\ntiple CONV-ReLU-Pooling operations followed by a fully\nconnected network. From a geometric point of view, CNN\nachieves the object classiﬁcation by performing a nonlinear\ntransformation UCNN from the input image space, where the\nobject classiﬁcation is difﬁcult, to the output vector space, in\nwhich the classiﬁcation is much easier. We will show that there\nexists an exact correspondence between CNN and the geom-\netry of the quantum circuit model of quantum computation,\nwhere UCNN corresponds to the unitary operator U of the\nquantum computation and the CONV-ReLU-Pooling operation\nplays the role of local universal gates.\nThe geometric structure of CNN is constructed as follows.\n• The manifold\nWe ﬁrst construct the mapping TCNN ×VCNN →VCNN.\nIn a CNN with L-layers, each layer accomplishes a\ntransformation of the input data. We can easily embed the\ninput and output data of each step into a Euclidean space\nRn with a high enough dimension n. We call it VCNN\nand its dimension is determined by the structure of the\nCNN including the input data size and the kernel number\nof each level. Accordingly TCNN can be taken as the\nautomorphism of VCNN. The goal of CNN is then to ﬁnd\nand realize a proper transformation UCNN ∈TCNN by\nconstructing a curve UCNN(t) on TCNN, which consists\nof L line segments, to reach UCNN.\n• The Riemannian metric\nIt’s easy to see that the local CONV-ReLU-Pooling\noperation of each layer corresponds to the local unitary\noperations in the quantum computation system. The de-\ntails of each CONV-ReLU-Pooling operation deﬁne the\nallowed local operations, i.e., the set P in the quantum\ncomputation system.\nNow we have two different ways to understand the metric\non TCNN. We can deﬁne a similar metric as (1), which\nmeans that at each layer of the CNN, this metric only\nassigns a ﬁnite length to the allowed local operations\ndeﬁned by the CONV-ReLU-Pooling operation of this\nlayer. So in this picture the metric changes during the\nforward propagation along the CNN since the conﬁg-\nuration of the CONV-ReLU-Pooling operator differs at\neach layer. So UCMM(t) is a curve on a manifold with a\ntime-varying metric. To remedy our Riemannian structure\nfrom this complicated situation, we can deﬁne another\ncomputational complexity oriented metric by scaling the\nmetric at each layer by the complexity of the CONV-\nReLU-Pooling operation. For example, if at a certain\nlayer we have Nk kernels with a kernel size Kx×Ky×Kz\nand the input data size is Sx × Sy × Sz, then we\nscale the metric by a factor NkKxKyKzSxSySz. With\nthis metric, UCNN(t) is a curve on TCNN with this\ncomplexity oriented metric and the length of UCNN(t)\nroughly corresponds to the complexity of the CNN, if the\nnonlinear operations are omitted. Of course, the nonlinear\nReLU and pooling operation in fact also change the\nmetric. Existing research works showed that they have a\nstrong inﬂuence on the metric as will be explained below.\n• The curvature\nIf the CNN is formulated as a curve on a Riemannian\nmanifold, then obviously the convergence of the training\nprocedure to ﬁnd the curve highly depends on the curva-\nture of the manifold. It’s difﬁcult to compute explicitly\nthe curvature of TCNN. But we may have a reasonable\nguess since we know that the Riemannian manifold of the\nquantum computation, which has a very similar metric,\nhas a curvature which is almost negative everywhere. Of\ncourse the metric of the CNN is more complex since here\nwe also have the nonlinear component. If TCNN also has\na almost negative curvature under our ﬁrst deﬁnition of\nthe metric shown above, then our second scaled metric\nwill result in a highly curved manifold with a large\nnegative curvature depending on the size of kernels and\nthe selection of the activation function. Roughly speaking,\na larger kernel size and a stronger nonlinearity of the\nactivation function lead to a higher negative curvature.\n• The target transformation UCNN\nDifferent with the case of the quantum computation,\nwhere the algorithm U is known, here we do not have\nUCNN. UCNN is found in a similar way as in the case\nof the template matching, where the target transformation\ng is obtained by an optimization to minimize the error\nbetween the transformed source image and the destination\nimage ∥I1 −I0 ◦g1∥2\nL2. In CNNs, we ﬁnd UCNN\nin a similar way. The only difference is that now the\noptimization is carried out on an ensemble of source\nand destination points, i.e., the training data with their\ncorresponding labeling.\n• The curve UCNN(t)\nUCNN(t) is a piece-wise smooth curve consisting of L\nline segments, where the length of each line segment\ncorresponds to the computational complexity of each\n5\nFig. 4.\nThe geometry of CNNs. Similar with the geometry of quantum\ncomputation, TCNN is the space of all possible transformations that can be\nrealized by a CNN. The Riemannian metric on TCNN is determined by the\ndesign of the CONV-ReLU-Pooling operations of the CNN and accordingly\nthe curvature of the Riemannian manifold will inﬂuence the optimization\nprocedure during the training phase of CNNs. The yellow area is the subspace\nof all the transformations that can be efﬁciently achieved by CNNs. Any\ngiven CNN structure corresponds a curve in TCNN, where each line segment\ncorresponds to the CONV-ReLU-Pooling operation of each layer and the\nlength of the line segment is proportional to the computational complexity\nof this layer. Compared with the geometry of template matching, a CNN\ndoes not aim to ﬁnd the optimal curve, i.e. the geodesic, instead it aims to\nﬁnd a curve with a ﬁxed length L which is proportional with the depth of a\nCNN.\nlayer. So geometrically a CNN is to ﬁnd a curve with\na given geometrical structure to connect I and UCNN on\nTCNN.\nThe above given Riemannian is built on the transformation\nspace TCNN. In fact just like the cases in both the quantum\ncomputation and the template matching, equivalently we can\nhave another Riemannian manifold on the data space VCNN.\nWe can regard the CNN achieves a change of the metric from\nthe input data space to the ﬁnal output data space deﬁned by\n⟨w, v⟩in = ⟨U∗w, U∗v⟩out, where ⟨·, ·⟩in and ⟨·, ·⟩out are the\nmetric on the input and output manifolds respectively and U∗is\nthe differential of the transformation U. So in this construction,\nthe metric of the data space changes along the data ﬂow in\nthe network. As we indicated before, the input data can not be\neasily classiﬁed in the input data space by deﬁning a simple\ndistance between points, but it’s much easier in the output\ndata space even with an Euclidean distance. This means by\napplying the transformation UCNN, the CNN transforms a\nhighly curved input data space to a ﬂatter output data space\nby a curvature ﬂow like mechanism. This gives an intuitive\npicture on how a complex problem can be solved with the\nCNN by ﬂattening a highly curved manifold as in [23]. But\nit’s difﬁcult to estimate how the curvature of the input data\nspace except for some special cases as in [10]. In the rest\nof this paper we will focus on the Riemannian structure on\nTCNN since it’s relatively easier to analysis the curvature of\nTCNN.\nWe now show how the design of the CNN structure inﬂu-\nence the geometry and how the geometric structure can affect\nthe performance of CNNs.\nFirst we need to clarify how the components of CNNs will\naffect the geometric structure.\n• Fully connected network\nUsually there is a fully connected network at the ﬁnal\nstage of CNNs. From existing experimental works we\nknow that its main function is to collect multiple modes of\na certain category of object. For example, if the task is to\nidentify cats, the FC network is in charge of collecting the\ninformation of cats with different locations and gestures.\nAlso we know with the increase of the kernel number and\ndepth, the FC part can be omitted. In another word, the\nconvolutional network before the FC network in fact has\nalready achieved an over classiﬁcation of the objects, the\nFC network only achieve a clustering of the information.\nSo the FC network does not play an important role in the\ngeometry of CNN.\n• Depth of CNN\nThe depth of a CNN is related to the total length of\nthe curve UCNN(t), i.e., the complexity of the CNN.\nObviously a deeper CNN can potentially achieve more\ncomplex transformation in TCNN.\n• CONV operation\nThe size and the number of kernel of the CONV operaion\nleads to different VCNN and TCNN as described above.\nAlso as a component of the local CONV-ReLU-Pooling\noperation, CONV inﬂuences the complexity of local\noperators and the metric as well. Generally a bigger\nkernel size and kernel number increase the complexity\nof allowed local operators. This can be understood if we\ncompare it with the Riemannian metric deﬁned in the\ngeometry of quantum computations. Also the CONV of\neach layer changes the structure of the curve UCNN(t).\n• ReLU operation\nThe role of ReLU operation (or a general activation\nfunction) falls in two folds: Firstly it provides the neces-\nsary nonlinear component to achieve the target nonlinear\ntransformation UCNN. Secondly different selections of\nthe activation function show different nonlinear property,\nso it also inﬂuences the complexity of local operations\nand the Riemannian metric. The existing experimental\nworks on the inﬂuence of the activation function on the\nconvergence property of CNN indicate that it may have a\nvery strong effect on the Riemannian metric, which leads\nto a change of the curvature of the Riemannian manifold\nTCNN. This observation helps to analysis the inﬂuence of\nthe activation function on the convergence of the training\nprocedure.\n• Pooling operation\nThe function of the pooling operation is also two folds:\nOn one hand, it can reduce the complexity by changing\nVCNN and TCNN. On the other hand it also plays a role\nin constructing the metric, correspondent to the kernel\nK in the LDDMM framework. By using pooling oper-\nations, we can improve the smoothness and robustness\n6\nof the curve UCNN(t) since UCNN is estimated by a\nstatistical optimization using samples from input image\nspace, i.e., the training data. The pooling operation can\nbe understood as an extrapolation of the samples so that\nthe information of each sample is propagated to its near\nneighbourhood.\nGiven the above analysis, we can answer the following\nquestions from a geometric point of view:\n• Why can CNNs work well in computer vision systems?\nThere are some theoretical considerations to answer this\nquestion. In [1] it’s claimed that the special structure of\nthe image classiﬁcation task enables the effectiveness of\nCNN, where the problem is formulated as the efﬁcient\napproximation of a local Hamiltonian system with local\noperations of CNNs. By checking the analogy between\nthe CNN and the quantum computation, we see that\nthe argument of [1] is equivalent to say that for a task\nthat can be solved by a deep CNN, its UCNN falls in\nthe subset that can be achieved by local CONV-ReLU-\nPooling operators, just as the subset of U(2n) that can be\napproximated by simple local universal gates in quantum\ncomputations. So the answer to this question is trivial,\nCNN works since the problem falls in the set of solvable\nproblems of CNNs. A direct question is, can we design\na task that does not belong to this solvable problem set?\nA possible design of such a task is to classify images\nthat are segmented into small rectangular blocks and\nthen the blocks are randomly permuted. Obviously such\nimages are generated by global operations which can\nnot be effectively approximated by local operations, and\ntherefore the classiﬁcation of such images can not be\nachieved efﬁciently by structures like CNN. So we can\nsay, the diameter of the Riemannian manifold TCNN is\nexponentially large with respect to the size of the input\ndata and only those problems which can be solved by\na transformation UCNN that has a polynomially large\ngeodesic distance to the identity transformation I can be\nsolved by CNNs.\n• Can we justify if a problem can be efﬁciently solved by\nCNNs? Generally this is difﬁcult since this is the same\nquestion as to ask if a unitary operator can be decomposed\ninto a tensor product of simple local unitary operators, or\nto ask if a n-qubit quantum state has a polynomial state\ncomplexity with respect to n. We already know both of\nthem are difﬁcult questions.\n• Why deep CNN?\nGeometrically, training a CNN with a given deepth L is\nequivalent to ﬁnd a curve of length roughly proportional\nto L in TCNN to reach the unknown transformation\nUCNN. In deep learning systems, this is achieved by a\ngradient descent based optimization procedure, i.e., the\ntraining of CNNs. It should be noted that the CNN is\nnot to ﬁnd a geodesic, instead it aims to ﬁnd a curve\nwith a ﬁxed geometric structure, i.e., a curve consisted\nof multiple line segments with given lengthes determined\nby the CNN structure. The reason to use the deep CNN\nis just that the length of UCNN(t) should be longer\nthan the length of the geodesic connecting the identity\ntransformation I and UCNN on the Riemannian manifold\nTCNN. Otherwise it will not work since a short UCNN(t)\nwill never reach UCNN. On the other side, it’s not\ndifﬁcult to guess that a too deep CNN may also fail to\nconverge if the curve UCNN(t) is too long due to both the\ndepth L and an improper design of CONV-ReLU-Pooling\noperations. This can be easily understood since to reach\na point UCNN with a longer curve UCNN(t) is just to\nﬁnd an optima in a larger parameter space. Intuitively we\nprefer a design of CNNs that leads to a curve UCNN(t)\nwith a length just above the geodesic distance between I\nand UCNN. To achieve this, we need a balance between\nthe network depth and the complexity of CONV-ReLU-\nPooling operations. This observation will play a key role\nin the analysis of ResNet below.\n• Why does ReLU work better than the sigmoid in deep\nCNNs?\nA typical answer to this problem is that ReLU can\nimprove the vanishing gradient problem so that a better\nperformance can be achieved. Another idea is that ReLU\ncan change the Fisher information metric to improve the\nconvergence but the sigmoid function can not. Here we\nexplain this from a complexity point of view. As men-\ntioned before, the selection of the activation function will\ninﬂuence the complexity of the local operation. Obviously\nReLU is a weaker nonlinear function than the sigmoid.\nThis means for a given CNN structure, using ReLU will\nhave a lower representation capability than using the\nsigmoid so that the CNN using ReLU has a smaller\ncomplexity. This is to say that the a CNN using a sigmoid\nneed to search for an optima in a larger state space. It’s\nobvious that as far as the complexity of the CNN is above\nthe length of the geodesic, it’s preferable to use a simpler\nCNN so that a better convergence can be achieved. It\nseems there exists a balance between the complexity of\nthe CONV operation and the activation function so that\nthe complexity of the complete system can be distributed\nequally along the network. With our ﬁrst deﬁnition of the\nmetric, the intuitive geometric picture is that the sigmoid\nfunction will introduce a metric so that the curvature of\nthe Riemannian manifold will be more negative, if our\nguess of the curvature of TCNN is true. A higher negative\ncurvature of course will make the curve UCNN(t) more\nunstable so that a small change of the parameters of the\nCNN will result in a large deviation of the ﬁnal point\nof UCNN(t). This corresponds to the observed vanishing\ngradient problem during the back-propagation procedure.\nSo for a general deep CNN, the sigmoid is too complex\nand ReLU is a proper activation function that can produce\na Riemannian manifold with a proper curvature.\n• How difﬁcult is it to optimize the structure of CNNs? By\nthe optimization of the structure of CNNs, we mean to\nﬁnd the optimal CNN structure (for example the depth,\nthe number and sizes of kernels of a CNN), which can\naccomplish the classiﬁcation task with the minimal com-\nputational complexity. Geometrically this is to construct\nand deﬁne a metric on the Riemannian fold TCNN, under\n7\nFig. 5. The geometry of ResNets. (a)The original residual unit of ResNets\nand (b) a new designed residual unit proposed in [25]. (c) The deep ResNet\nbuilt on residual units (a)(b). (d)ResNets hold a similar geometric picture as\nCNNs. The new feature of ResNet is that the curve UResNet is a smooth\ncurve consisted of a large number of simple near-identity transformations so\nthat they can be efﬁciently approximated by the residual unit structure.\nwith the length of a curve is proportional to the compu-\ntational complexity of a CNN structure; and (b)realize\nthe geodesic U geo\nCNN that connects the identity operator I\nwith UCNN. Generally this is difﬁcult, especially when\nthe curvature property of the Riemannian manifold is un-\nknown. This is the same as to ﬁnd the optimal realization\nof an algorithm with a given universal logic gate set. For\nthe geometry of quantum computation, the curvature is\nalmost negative so that it’s difﬁcult to achieve a general\noperator[10]. If the manifold of the fundamental law, the\nquantum computation, has a negative curvature, it will not\nbe surprising if we assume the geometry of deep learning\nhas a similar property. This point seems to be partially\nconﬁrmed by the recent work[23], where a manifold\nwith an exponentially growing curvature with the depth\nof a CNN can be constructed. If a CNN is applied to\nsuch an input data manifold with an exponentially large\nnegative curvature, then the optimization will be difﬁcult\nto converge.\nB. Geometry of residual networks\nResNet is a super-deep network structure that shows su-\nperior performance than normal CNNs[24][25]. Given the\ngeometry of CNNs, it’s straight forward to draw the geometric\npicture of ResNets.\nSimilar with CNNs, ResNets also aim to ﬁnd a curve\nUResNet(t), t ∈[0, 1] to reach UResNet. The difference is\nthat UResNet(t) consists of a large number of short line\nsegments corresponding to simple operations UResNet(lδ), l =\n1, 2, ..., L, Lδ = 1. It’s easy to tell that when L is big enough,\nUResNet(lδ) is near the identity operation I so that the ﬁrst-\norder approximation of UResNet(lδ) gives UResNet(lδ) ≈\nI+HResNet(lδ), where HResNet(lδ) is a weak local nonlinear\ntransformation. This gives the structure of the ResNet, i.e.,\neach layer of the ResNet is the addition of the original and\nthe residual information.\nThe reason that ResNet works better falls in the following\nfacts:\n• The ResNet achieves a roughly uniform distribution of the\ncomplexity of the problem along the curve UResNet(t).\nSuch a structure may lead to a smooth or stable infor-\nmation ﬂow along the network and help to improve the\nconvergence of the optimization procedure.\n• The fact that UResNet(lδ) ≈I reduces the searching\nspace of HResNet(lδ). For example, if the space TResNet\nis U(2n), then HResNet is in the Lie algebra u(2n),\nwhich has a smaller dimension than U(2n). This may\nimprove the performance of the optimization procedure.\nWe can also use this framework to analysis the properties\nof ResNets observed in [25]. In [25] the ResNet is formulated\nin a general form\nyl\n=\nh(xl) + F(xl, Wl)\n(4)\nxl+1\n=\nf(yl)\n(5)\nwhere xl and xl+1 are input and output of the lth-level and F\nis a residual function. In the original ResNet, h(xl) = xl and\nf = ReLU.\nThe main results of [25] are:\n• If both the shortcut connection h(xl) and the after-\naddition activation f(yl) are identity mappings, the sig-\nnal could be directly propagated in both the forward\nand backward directions. Training procedure in general\nbecomes ealier when the architecture is closer to these\nconditions.\n• When h(xl) is selected far from the identity mapping, for\nexample as multiplicative manipulations(scaling, gating,\n1×1 convolutions and dropout), the information propaga-\ntions is hampered and this leads to optimization problems.\n• The pre-activation of ReLU by putting the ReLU as part\nof F and setting f as the identity operation improves the\nperformance. But the impact of f = ReLU is less severe\nwhen the ResNet has fewer layers.\n• The success of using ReLU in ResNet also conﬁrms our\nconclusion that ReLU is a proper weak nonlinear function\nfor deep networks and the sigmoid is too strong for deep\nnetworks.\nFrom the geometric point of view, the above mentioned\nobservation are really natural.\nFor a deep ResNet, UResNet(lδ) = I +HResNet(lδ), this is\nexactly the case that both f and h are identical mappings. If h\nis far from the identity mapping, then the difference between\nthe identity mapping and h need to be compensated by F,\nthis will make F more complex and leads to optimization\nproblems.\nThe pre-activation also aims to set f as the identity mapping\nand the weak nonlinear ReLU is absorbed in F. If the ResNet\nhas fewer layers, then UResNet(lδ) is a little far from the\nidentity mapping so that UResNet(lδ) = I + HResNet(lδ) ≈\nReLU(I+H′\nResNet) is valid. This can explain the observation\nthat the pre-activation is less crucial in a shallower ResNet.\nThe geometric picture easily conﬁrms that the best ResNet\nstructure is to replicate UResNet(lδ) = I + HResNet(lδ).\nAnother related observation is that the highway network[26]\nusually performs worse than the ResNet since it introduces\nunnecessary complexity to the system by adding extra non-\nlinear transformations to the system so the structure of the\n8\nFig. 6. The geometry of recursive neural networks. (a) The recursive neural\nnetwork architecture which parses images and natural language sentences\n[27]. (b)The ﬁxed operation of a recursive neural network[27]. (c)The Lie\ngroup exponential mapping can be taken as the geometric picture of a\nrecursive neural network. (d)The recursive neural network can optimize both\nthe representation mapping and a component of the Lie algebra of the Lie\ngroup that can reach the destination oparation.\nhighway network does not match the natural structure given\nby the ﬁrst-order approximation.\nC. Geometry of recursive neural networks\nRecursive neural network is commonly used in the natural\nscene images or natural language processing tasks.\nTaking the recursive neural network for the structure predic-\ntion described in [27][28] as an example, the goal is to learn\na function f : X →Y, where Y is the set of all possible\nbinary parse trees. An input x ∈X consists of (a) a set\nof activation vectors which represent input elements such as\nimage segments or words of a sentence, and (b) a symmetric\nadjacency matrix to deﬁne which elements can be merged.\nThe recursive neural network accomplishes this task by\nﬁnding the two sub mappings: (a) A mapping from the words\nof natural languages or segments of images to the semantic\nrepresentation space. Combining this map with the adjacent\nmatrix, the original natural input data is represented in a space\n˜\nX; (b)A ﬁxed rule to recursively parse the input data ˜\nX to\ngenerate a parsing tree.\nGeometrically we are interested in the following aspects:\n• What’s the correspondent geometric picture of the ﬁxed\nrecursive rule?\n• Why both the recursive rule and the representation map-\nping can be learned during training?\nIt’s easy to see that given the representation mapping and\nthe adjacent matrix, the recursive operation is nothing but a\ntransformation on ˜\nX. The ﬁxed operation rule reminds us its\nsimilarity with the Lie group exponential mapping. Though\nwe can not say\n˜\nX is a Lie group, but conceptually the\nrecursive neural network can be understood as a discretized\nLie group exponential mappings. Given any input in ˜\nX, the\ntransformation between ˜\nX and Y is achieved by this Lie group\nexponential mapping.\nIn CNN and ResNet, the goal is to ﬁnd a free curve with\na ﬁxed length, which is neither a geodesic nor a Lie group\nexponential mapping. So the complexity or the dimension\nof freedom of the curve is much higher. In recursive neural\nnetworks, the exponential mapping like curve is much simpler,\nthis is why we have the capability to optimize both the\nrepresentation mapping and the exponential mapping simul-\ntaneously during training.\nWe can also observe that the recursive neural network does\nnot emphasis on local operations as in CNNs or ResNets. This\nmeans that the recursive neural network is dealing a class of\nproblems that is similar with the template matching problem.\nAnd its analogy with the SVF framework of the template\nmatching conﬁrms this point.\nD. Geometry of recurrent neural networks\nAnother category of neural networks is the recurrent neural\nnetworks[29]. The key feature of the RNN is that it allows us\nto process sequential data and exploit the dependency among\ndata. RNNs are widely used in language related tasks such as\nthe language modeling, text generating, machine translation,\nspeech recognition and image caption generation. Commonly\nused RNN structures include bi-directional RNNs, LSTM\nnetworks and deep RNNs.\nGiven a sequence of input data x = {x1, x2, ......, xT },\na\nstandard\nRNN\ncompute\nthe\nhidden\nvector\nsequence\nh\n=\n{h1, h2, ......, hT } and the output sequence y\n=\n{y1, y2, ......, yT } for every time step t = 1, 2, ..., T as:\nht\n=\nf(Wihxt + Whhht−1 + bh)\n(6)\nyt\n=\nWhoht + bo\n(7)\nwhere Wih, Whh, Who and bh, bo are the weight matrices and\nthe bias vectors. f is the activation function of the hidden\nlayer.\nHere we are not going to give a complete overview to the\ndifferent variations of RNNs. Instead we will try to understand\nthe feature of RNNs from a geometrical point of view.\nLet’s ﬁrst go back to the geometry of CNNs, where a CNN\nis a curve on TCNN to reach a point UCNN. For each given\ndata point on VCNN, it’s transformed by UCNN to another\npoint on VCNN. Geometrically a CNN accomplishes a point-\nto-point mapping with a simple curve as its trajectory on\nVCNN.\nRNN is more complex than CNN. We can check the\ngeometric picture of RNNs in two different ways.\nThe ﬁrst picture is to regard the sequential input data\nas a series of points on VRNN, then the geometric picture\nof a RNN is a mapping of string, a series of intercon-\nnected points x = {x1, x2, ......, xT }, to a destination string\ny = {y1, y2, ......, yT } with an intermediate trajectory h =\n{h1, h2, ......, hT }. This is not a collection of independent par-\nallel trajectories as (x1, h1, y1), (x2, h2, y2), ..., (xT , hT , yT )\nsince they are coupled by the recursive structure of the RNN\nthrough the hidden states ht in (6).\nAnother picture of RNNs is to regard the sequential data,\nthe string in the ﬁrst picture, as a single data point, then the\n9\ntrajectory of the input data is a curve in a higher dimensional\nspace VRNN. Accordingly TRNN is the space of transforma-\ntions on VRNN. This is similar with the geometric picture of\nCNNs.\nThe difference of these two pictures is that in the ﬁrst\npicture, the transformation working on the trajectory of each\npoint of the string can be any transformation in TRNN and the\ntrajectories of different points are coupled with each other. In\nthe second picture, the transformation working on the data\npoint, the complete sequence of data x = x1, x2, ......, xT ,\nis now local operations in TRNN, which only work on each\nindividual data xi, i = 1, 2, ..., T and pairs of successive data\nxi, xi+1 also through ht as in ??. So the second picture is\nsomehow trivial since now the RNN is only a special case\nof the CNN. So we from now on we will focus on the ﬁrst\npicture to see how this picture can help us to understand the\nRNN related topics.\nFor a standard RNN, only earlier trajectories will inﬂuence\nlater trajectories so that a later input data feels the force of ear-\nlier data. For bi-directional RNNs, the force is bi-directional.\nFor deep bi-directional RNNs, the trajectory is a complex sheet\nconsisting of multiple interconnected trajectories.\nThis string-sheet picture reminds us of their counterparts\nin physics. A deep CNN corresponds to the world-line of a\nparticle, which can achieve human vision functions. A deep\nRNN corresponds to the world-sheet of a string, which can be\nused to solve linguistic problems. It’s natural to ask, what’s\nthe correspondent structure of the trajectory of a membrane\nand what kind of brain function can be achieved by such a\nstructure? Just as that RNNs can interconnect points into a\nstring, we can also interconnect multiple strings to build a\nmembrane. We can even further to interconnect multiple mem-\nbranes to generate higher dimensional membranes. To build\nsuch structures and investigate their potential functionalities\nmay lead to interesting results.\nWe show that in fact the LSTM, the stacked LSTM, the\nattention mechanism of RNN and the grid LSTM are all\nspecial cases of this membrane structure.\n• LSTM\nThe structure of the standard LSTM is shown in Fig. 8.\nIt’s straight forward to see that it’s the evolution of two\nparallel but coupled strings, the hidden states h and the\nmemory m. Compared with normal RNNs where there is\nonly a single information passage, it’s possible that the\nexistence of the parallel information passages in LSTM\nmakes the LSTM system capable of modeling long range\ndependencies.\n• Stacked LSTM\nThe stacked LSTM is in fact a deep LSTM whose\ngeometric picture is a cascaded double string as in Fig.\n8.\n• Grid LSTM\nThe grid LSTM[30] is an extension of the LSTM to\nhigher dimensions. Accordingly the strings in the stan-\ndard LSTM are replaced by higher dimensional mem-\nbranes as given in [30], where a 2d deep grid LSTM\ncan be understood as stacked multiple 2d membranes. So\nif the deep LSTM is a cascaded coupled double strings,\nthen the deep 2d grid LSTM is a cascaded membranes\n(Fig. 10). Similar geometric pictures can be constructed\nfor higher dimensional and deep grid LSTM systems.\n• Attention mechanism\nThe attention mechanism is getting popular in the decoder\nof the machine translation and the image caption genera-\ntion. The key feature is that during the text generation, an\nalignment or an attention model (a feed-forward network\nin [31] or a multiple layer perceptron network in [32])\nis used to generate a time-varying attention vector to\nindicate which part of the input data should be paid\nmore attention to. This structure can be understood as\na coupling between the normal decoder network (a RNN\nor a LSTM) with the attention network as shown in Fig.\n9.\nIt need to be noted that there is an essential difference be-\ntween the LSTM systems and the attention mechanism. We can\nsee that the LSTM systems are the coupling of homogeneous\nsubsystems but the attention mechanism is in fact a coupling of\ntwo heterogeneous subsystems, i.e. one decoder RNN/LSTM\nnetwork and one attention model with a different structure.\nOther similar systems, such as the recurrent models of visual\nattention[33] and the neural Turing machines[34][35], also\nhold a similar picture of coupling heterogeneous subnetworks.\nIt can be expected that more complex and powerful systems\ncan be constructed by such a mechanism. It will be interesting\nto see how such a mechanism may help us to understand\nkey human intelligence components such as our memory,\nimagination and creativity.\nSimilar with the case of CNNs, we can also check the metric\nand the curvature of the manifold of the coupled networks.\nRoughly we are now working with the direct product of the\nmanifold of each subsystem with the tangent spaces of the\nsubsystems as orthogonal subspaces of the tangent space of\nthe complete manifold. This is very similar with the case of\nthe semi-direct group product in the metamorphsis framework\nin the template matching system[18]. It can be expected\nthat the structure of the manifold is much more complex\nthan the relatively simple CNN case. But if the curvature\nof each subsystem’s manifold is almost negative as in the\nquantum computation, then the convergence of the training\nof the coupled system naturally will be more difﬁcult, just as\nindicated in the case of neural Turing machines.\nE. Geometry of GANs\nGenerative adversarial networks (GANs) is a framework\nfor unsupervised learning. The key idea of GANs is to\ntrain a generative model that can approximate the real data\ndistribution. This is achieved by the competition between a\ngenerative model G and a discriminative model D. In the\nstandard conﬁguration of GANS, both G and D are CNN-\nlike networks.\nWe now give a geometrical description of GANs and try to\nanswer the following questions:\n• Why is it difﬁcult to train GANs?\n• Why can the reﬁned GANs, including LAPGANs,\nGRANs and infoGANs, perform better than standard\nGANs?\n10\nFig. 7.\nThe geometry of RNNs. (a)The structure of the unrolled standard\nRNN. (b)The geometrical picture of the RNN is given by the trajectory of a\nstring from the initial green string to the red destination string. This is the ﬁrst\nway to understand the geometry of RNNs. (c) The second way to understand\nthe geometry of RNNs is to take the input sequence as a single data point,\nthen the structure of RNNs is a curve as the same as CNNs. The curve is\nnow achieved by local operations on the data. (d) A deep RNN network with\nits correspondent geometric picture as a sheet swept by a string shown in (e)\nor a long curve shown in (f). The arrows indicate information ﬂows\nFig. 8.\nThe geometry of LSTM networks. (a)The standard LSTM network\nwhere m and h are the memory and hidden states of the system. (b)The\ngeometry of the LSTM network is given by two coupled strings, the yellow\nhidden state string and the blue memory string. (c) The structure of a stacked\nor deep LSTM network, whose geometric picture is a cascaded structure of\ncoupled double strings as shown in (d). The arrows indicate information ﬂows.\nSince both G and D are realized by CNNs, the geometric\npicture of GANs can be built on the geometry of CNNs. From\nthe geometric model of CNNs, it’s easy to give the geometric\npicture of the standard DCGANs as a two-piece curve in\nTCNN as shown in Fig 11.\nThe two curve pieces represent the generator G and the\ndiscriminator D respectively. The goal of the training proce-\ndure is to ﬁnd the two transformations that can minmax the\ncost function of GANs. GANs are different from CNNs in the\nfollowing aspects:\n• The goal of CNNs is to ﬁnd a curve UCNN(t) that can\nreach a transformation UCNN from I. For GANs, we\nneed to ﬁnd two transformations UD and UG−1 by con-\nstructing two curves to reach them from I simultaneously.\n• The target transformation of D is relatively simple but\nthe destination transformation of G is much ﬂexible\nthan UCNN. This can be easily understood since we\nset more constraints on UCNN by labeling the training\nFig. 9.\nThe geometry of the attention mechanism. We take the language\ntranslation system in [32] as an example. The encoder of this system is a bi-\ndirectional RNN network which generates encoded information hi from input\nseries xi. The decoder is a coupling of a RNN network (the light blue string)\nand an alignment model (the brown string) by exchanging state information\nsi of the RNN and the attention information ai from the alignment model,\nwhich generates the output yi from the encoded data hi. This is a coupling\nof two heterogeneous networks. The arrows also indicate information ﬂows\nFig. 10. The geometry of the 2d deep grid LSTM. The 2d deep grid LSTM\n(left) has a geometric picture of multiple stacked membranes, where each\nmembrane is constructed by coupling multiple LSTM strings in 2d space.\ndata in CNN based object classiﬁcation systems. An easy\nexample to illustrate this point is shown in Fig. 12. Both\nCNNs and the inverse of G aim to transform the training\ndata into an Euclidean space so that the training data are\nproperly clustered. For CNNs, the labeling of training\ndata results in that the output clustering pattern of CNNs\nis relatively unique. But for the unsupervised learning\nof the generator of GANs, their are a set of equivalent\nclustering patterns, which may compete with each other\nduring the training procedure.\n• The information pathways are also different in GANs and\nCNNs as shown in Fig. 11. Firstly we can see that in the\ndesign of GANs, the basic idea is to learn a representation\nof the training data with G−1 so that the generated\nexamples from G can have the same distribution of\nthe training data, i.e. the generated data can fool the\ndiscriminator D. So the information ﬂow in this design\nis to start from the training data, go through the reverse\ngenerator G−1, return back through G and pass the\n11\ndiscriminator D. Only at this point, the output of D can\nsend error information back to guide the update structure\nof G. During the network training, the information ﬂow\nfor D is similar with a normal CNN. But the optimal G\nis deﬁned by the cost function of GANs, which need\nan information passage passing both G and D to set\nconstraints on G, which is longer than the CNN case.\n• The geometric picture of GANs is to ﬁnd a two-segment\ncurve connecting the input space of G to the output space\nof D, which is asked to pass the neighbourhood of the\ntraining data in a proper way. But we can only use the\ninformation from the output of D to justify if the curve\nfulﬁlls our requirement. So any loss of information at\nthe output of D, due to either the mismatch between the\ntraining of G and D or an improper cost function that\nfails to measure the distance between the curve and the\ntraining data, will deteriorate the convergency of GANs.\nGiven the above observations, the reason that GANs are\ndifﬁcult to be trained is straight forward.\n• Compared with CNNs, GANs have the same Riemannian\nmanifold as TCNN, which has a high possibility of own-\ning a negative sectional curvature. GANs have to optimize\ntwo curves and a more ﬂexible target transformation G\non this manifold.\n• The information pathways are longer in GANs than in\nCNNs. Also the information pathways in the training\nprocedure do not coincide with the information pathways\nin the designing idea of GANs. This mismatch may lead\nto a deterioration of the system performance since the\ninformation ﬂow in the G−1 direction is lost in the\ntraining procedure.\nTo eliminate the difﬁculties of GANs, there exist the fol-\nlowing three possible strategies:\n• To decrease the ﬂexibility of the generator transformation\nUG by adding more constraints to UG.\n• To improve the structures of the curves of G and D.\nFrom the analysis of CNNs, we know this is to change the\nmetric of the manifold TCNN or equivalently to rearrange\nthe lengths of the line segments in the curves of G and\nD.\n• To eliminate the information loss at the output of D so\nthat it’s more reliable to justify how the two-segment\ncurve of GAN passes the neighbourhood of the training\ndata. Basically this is exactly what the WGAN does.\n• To change the information pathways during the training\nprocedure. This may improve the efﬁciency of the infor-\nmation ﬂow during the training so that possibly a better\nconvergence may happen.\nNow we explain how LAPGANs, GRANs and infoGANs\ncan improve the performance of GANs utilizing the above\ngiven strategies.\n• InfoGAN\nInfoGANs aim to decrease the ﬂexibility of the trans-\nformation UG−1 or equivalently UG by introducing extra\nconstraints on UG. The original GAN cost function is\nbased on the sample based expectation of the network\noutputs. On the contrary, infoGANs enhance the con-\nFig. 11.\nThe geometry of GANs. The generator G and discriminator D\nof a GAN correspond to two curves connecting the identity operation I\nwith the transformations UD and UG−1 respectively. During the training\nprocedure, the information ﬂow (the feed-forward and the back-propagation\nof information) of D and G are shown by the blue and red dash lines. In the\ndesign of the GANs, the information ﬂow is shown in the black dash line. The\nunsupervised learning of the generator G of GANs leads to a much ﬂexible\ntarget transformation UG−1 compared with the target transformation UCNN\nin CNNs.\nFig. 12. The comparison of the target transformation of CNNs and GANs.\nWe consider a simple two-cluster case where the training data are images\nof cats and birds. Direct clustering and interpolation on the image manifold\n(left) is difﬁcult. Both the CNN and the GAN map the training data into a 2\ndimensional Euclidean space to achieve a proper data clustering (right). For\nthe CNN, the output clustering pattern is unique. But for the GAN, we have\nmultiple equivalent patterns that are all optima of the cost function of GAN\nsystems.\n12\nFig. 13.\nComparisons of the hierarchical structures in LAPGANs,GRANs\nand infoGANs. Left column: The training data space; Central column: The\nstructure of different GAN systems; Right column: The representation space\nat the input of generators. LAPGANs: The hierarchical structure is built on\nthe frequency bands of all the training data and a corresponding structure in\nthe representation space is also constructed by LAPGAN’s pyramid structure.\nGRANs: The hierarchical structure is a kind of self-similarity or a recursive\nstructure on the training data space encoded in the RNN based generator.\ninfoGANs: The hierarchical structure is based on the clustering of the training\ndata, where the clustering are indicated by the latent variables ({ci}) in the\ngenerator’s input data space, so that this cluster based structure is kept by the\ngenerator and can be veriﬁed by the mutual information used in infoGANs. In\neach system, a hierarchical correspondence between the training data space\nand the representation space is built, which can effectively introduce more\nconstraints on the transformation UG so that the convergence of the training\nprocedure can be improved.\nstraints on UG by setting constraints on expectations of\nclusters of sample data. This means that the clustering\npattern in the representation space of the training data,\nindicated by the latent variables in the input data space\nof the generator G, should be kept by the generator,\nso that in the hidden states of the discriminator (as a\nkind of decoder of the generator to reveal the clustering\npattern) the same clustering pattern can be observed. This\nis exactly why the infoGAN add a mutual information\nitem, which is used to check if the clustering patten, to\nits cost function. This cluster based constraints in fact\nconstructs a hierarchical structure in both the training data\nspace and the representation space. Asking the generator\nG to keep this hierarchical structure effectively reduce the\nﬂexibility of UG and improves the convergence stability.\n• LAPGAN\nThe key feature of LAPGANs is to decompose the image\nspace into orthogonal subspaces with different frequency\nbands and run different GANs sequentially on subspaces\nat higher frequency bands conditioned on the results\non lower frequency bands. From our geometrical point\nof view, the decomposition of input data space results\nin a correspondent decomposition of the transforma-\ntion space TGAN into multiple subspaces T k\nGAN where\nk = 1, 2, ..., K with K the number of bands of the\nparamid. So the pyramid based LAPGAN is essentially\nto construct the curve UD(t) and UG−1(t) in TGAN by a\nset of curves in each subspace T k\nGAN using simpler local\noperations. This is similar to achieving a displacement in\n3 dimensional space by three displacements in x, y and\nz directions.\nThe reason such a structure can improve the convergence\nof LAPGANs can be understood from different points of\nview.\nFirstly LAPGANs replace a complex curve UG(t) by a\nset of simpler curves U k\nG(t) in each orthogonal subspace\nT k\nGAN and then optimize those curves sequentially. So\nthe local operations constructing the curve U k\nG(t) are\nsimpler and the curve U k\nG(t) is also shorter than UG(t).\nFrom our above analysis of CNNs and ResNets, simpler\nlocal operations will introduce a ﬂatter transformation\nmanifold or equivalently a curve consisted of shorter line\nsegments. This will improve the convergence property of\neach curve U k\nG. This is to say a curve UG(t) with complex\nlocal operations by a series of shorter curves {U k\nG} with\nsimpler local operations in subspaces {T k\nGAN}.\nSecondly in LAPGANs, a set of generators {Gk} and\ndiscriminators {Dk} are trained with each subband of\ntraining data. Obviously each pair of Gk and Dk de-\ntermines a representation pattern of one data subband.\nAlso these representations are consistent since during\nthe training procedure, the generator Gk at a higher\nfrequency band is conditioned on the lower band image.\nThis means LAPGANs replace the original cost function\non the expectation of the complete data set by a set of\ncost functions on the expectation of each frequency band\nof the input data. This is in fact to construct a hierarchical\nstructure on the training data, i.e. the paramid, and set a\nset of consistent constraints on this hierarchical structure.\nSo essentially we add more constraints to the generator\ntransformation UG.\n• GRAN\nGRANs improve the performance of GANs by a RNN\nbased generator so that the ﬁnal generated data is con-\nstructed from the sequential output of the RNN network.\nWe already know that the RNN essentially decompose\na single input data point of a normal CNN network\ninto a data sequence and therefore the trajectory of a\ndata point of CNNs becomes a sheet swept by a string\nin RNNs. From this point of view, the operations of a\nRNN are essentially all simpler local operations of the\ncomplete input data. What’s more wimilar with LAP-\nGANs, GRANs also explore a hierarchical structure of the\nsystem which is encoded in the RNN structure. But this\nhierarchical structure is not so explicit as in LAPGANs.\nIn another word, LAPGANs explore the structure in\nfrequency domain and GRANs explore a kind of self-\nsimilarity like structure of the input data.\nAs a summary, LAPGANs, GRANs and infoGANs all im-\nprove the performance by constructing a hierarchical structure\nin the system and adding more constraints on UG. Their\ndifference lies in their ways to construct the hierarchy. What’s\nmore, LAPGANs and GRANs can potentially further improve\nthe performance by changing the structure of the curve UG(t)\n13\nby only using simpler local operations compared with the\nstandard GANs.\nAnother possible way to improve the performance is to\nchange the information passway. For example in the training\nof G and D, the training data information is fed to D directly\nbut to G indirectly. Feeding the training data directly to G in\nthe inverse direction (as the information passway given by the\nblack dash arrow in Fig. 11) can potentially further explore\nthe structure of the representation space z.\nF. Geometry of equilibrium propagation\nEquilibrium propagation[36] is a new framework for ma-\nchine learning, where the prediction and the objective function\nare deﬁned implicitly through an energy function of the data\nand the parameters of the model, rather than explicitly as in a\nfeedforward net. The energy function F(θ, β, s, v) is deﬁned\nto model all interactions within the system and the actions with\nthe external world of the system, where θ is the parameter to\nbe learned, β is a parameter to control the level of the inﬂuence\nof the external world, v is the state of the external world(input\nand expected output) and s is the state of the system.\nThe cost function is deﬁne by\nCδ\nβ(θ, s, v) := δT · ∂F\n∂β (θ, β, sβ\nθ,v, v)\n(8)\nwhere δ is a directional vector in the space of β so the\ncost function is the directional derivative of the function β 7→\nF(θ, β, s, v) at the point β in the direction δ.\nFor ﬁxed θ,β and v, a local mininum sβ\nθ,v of F which\ncorresponds to the prediction from the model, is given by\n∂F\n∂s (θ, β, sβ\nθ,v, v) = 0\n(9)\nThen the equilibrium propagation is a two-phase procedure\nas follows:\n1) Run a 0-phase until the system settle to a 0-ﬁxed poin\nsβ\nθ,v and collect the statistics ∂F\n∂θ (θ, β, sβ\nθ,v, v).\n2) Run a ξ-phase for some small ξ ̸= 0 to a ξ-ﬁxed poin\nsβ+ξδ\nθ,v\nand collect the statistics ∂F\n∂θ (θ, β, sβ+ξδ\nθ,v\n, v).\n3) Update the parameter θ by\n∆θ ∝−1\nξ (∂F\n∂θ (θ, β, sβ+ξδ\nθ,v\n, v) −∂F\n∂θ (θ, β, sβ\nθ,v, v)).\n(10)\nAn equivalent constrained optimization formulation of the\nproblem is to ﬁnd\nargθ,s min Cβ\nθ,s(θ, s, v)\n(11)\nsubject to the constrain\n∂F\n∂s (θ, β, sβ\nθ,v, v) = 0.\n(12)\nThis leads to the Lagrangian\nL(θ, s, λ) = δ· ∂F\n∂β (θ, β, sβ+ξ\nθ,v , v)+λ· ∂F\n∂s (θ, β, sβ\nθ,v, v) (13)\nBy this formulation we can see the update of θ in equi-\nlibrium propagation is just one step of gradient descent on L\nwith respect to θ as\n∆θ ∝−∂L\n∂θ (θ, s∗, λ∗)\n(14)\nwhere s∗, λ∗is determined by\n∂L\n∂λ (θ, s∗, λ∗) = 0\n(15)\nand\n∂L\n∂s (θ, s∗, λ∗) = 0\n(16)\nIt’s claimed that the ξ-phase can be understood as perform-\ning the back-propagation of errors. Because when a small ξδ\non β is introduced, the energy function F induces a new\n’external force’ acting on the output units of the system to\ndrive the output to output target. This perturbation at the output\nlayer will propagates backward across the hidden layers of the\nnetwork so that it can be thought of a back-propagation.\nHere\nwe\nwill\nshow\nthat\nthe\ngeodesic\nshooting\nalgorithm[20][19] in the diffeomorphic template matching\nis in fact an example of the equilibrium propagation. We\nwill use it to give a geometrically intuitive picture of the\nequilibrium propagation to show how the back-propagation is\nachieved in the ξ-phase.\nThe geodesic shooting addresses that diffeomorphic tem-\nplate matching as ﬁnd the optimal matching g(t), ˙g(t) =\nu(t) ◦g(t) between two images I0 and I1. Under the smooth\ntransformation of g(t), we deﬁne the transformed source image\nas I(t) = I0 ◦g(t). The energy function is given as\nF =\nZ 1\n0\n⟨u(t), u(t)⟩vdt + β∥I1 −I0 ◦g1∥2\nL2\n(17)\nIn the language of the equilibrium propagation, v corre-\nsponds I0 and I1 as the input and output data, the state of\nthe system s is given by I(t), g(t), u(t)). The cost function is\nCβ\nθ,s(θ, s, v) = δ∥I1 −I0 ◦g1∥2\nL2.The prediction of the state\ns∗is obtained by ∂F\n∂s = 0, which leads to the Euler-Pointcare\nequation given by\n˙I(t)\n=\n−ˆP(0)I(t) · u(t)\n(18)\n˙ˆI(t)\n=\n−ˆP(0) · (ˆI(t)u(t))\n(19)\nLu(t)\n=\n−−ˆP(0)I(t) · ˆI(t)\n(20)\nˆI(1)\n=\n−(I(1) −I1)\n(21)\nwhere Lu(t) is the momentum map and ˆI(t) is the adjoint\nvariable of I(t). It’s easy to know that the optimal state s∗is\ncompletely determined by u(0), which can be regarded as the\nparameter θ to be optimized.\nTaking the EP equation as the constraint, the variation of\n13 results in\n14\n˙I(t)\n=\n−∇I(t) · u(t)\n(22)\n˙P(t)\n=\n−∇· (P(t)u(t))\n(23)\nLu(t)\n=\n−−ˆP(0)I(t) · P(t)\n(24)\n˙ˆI(t)\n=\n−∇· (ˆI(t)u(t)) −∇· ((K ∗ˆu(t)P(t)))(25)\n˙ˆP(t)\n=\n−∇ˆP(t) · u(t) + (K ∗ˆI(t)) · ∇I(t)\n(26)\nK ∗ˆI(t)\n=\n−u(t) −K ∗(ˆI(t)∇I(t) −∇ˆP(t)P(t))(27)\nˆI(1)\n=\n−(I(1) −I1)\n(28)\nˆP(1)\n=\n0\n(29)\n−ˆP(0)\n=\n∇P (0)Cβ\nθ,s(θ, s, v)\n(30)\nwhere P(t) = Lu(t) is the momentum map of u(t).\nIn the geodesic shooting framework, the above equations\ninclude two components:\n• Shooting The ﬁrst three equations determine I(t), P(t)\ngiven the initial momentum P(0) and I(0) = I0.\n• Adjoint advection\nThe second ﬁve equations determines ˆI(t), ˆP(t) by solv-\ning the equations in the reverse time direction given the\nboundary conditions of ˆI(1), ˆP(1).\n• Gradient\nThe last equation given the gradient of the cost function\nCβ\nθ,s(θ, s, v) on the initial momentum P(0), while updat-\ning P(0) equals to update the parameter θ = u(0).\nIt’s very easy to see that the three components correspond\nexactly to the three step in the equilibrium propagation. The\nshooting and advection phases correspond to the 0-phase and\nξ-phase. The gradient ∇P(0)Cβ\nθ,s(θ, s, v) is equivalent to the\ngradient descent ∆θ ∝−∂L\n∂θ (θ, s∗, λ∗).\nThis example gives an intuitive picture of the equilibrium\npropagation as follows:\n• The 0-phase is to shoot a geodesic staring from a given\npoint with a given initial velocity vector.\n• The ξ-phase is to propagate the error information (the\ndistance between the end point of the geodesic I(1) and\nthe expected destination I1) in a backward direction.\n• The update of parameter θ is based on the information\n−ˆP(0) obtained by the back-propagation procedure.\nOf course, the equilibrium propagation is a general frame-\nwork for deep learning. We can not give a concrete geometric\ndescription of it due to its ﬂexibility. Here we just show that in\nsome special cases, the equilibrium propagation does have a\ngeometric picture, in which how the back-propagation of error\ninformation can be directly observed.\nAnother observation is that the equilibrium propagation is\nnot only a framework for training the so called implicitly\ndeﬁned deep learning systems as described in [36], in fact\nit’s also potentially a framework to optimize the structure of\nthe explicitly deﬁned deep learning systems as well.\nTo see this point, taking the deep CNN as an example\nwhere the prediction is explicitly deﬁned by the structure of\nthe CNNs. We can deﬁne an energy as\nF(θ, β, s, v) = Ecomp(θ, β, v) + ECNN(θ, β, s, v)\n(31)\nwhere the parameter θ to be learned includes also the param-\neters of the structure of a CNN, for example the depth, the\nnumber and size of each layer, besides the normal papameters\nof a CNN. ECNN(θ, β, s, v) is then the cost function for a\nCNN with a given structure deﬁned by θ and Emeta(θ, β)\ndeﬁnes the complexity of the CNN, i.e., the number of\noperations of the feed-forward network structure deﬁned by\nθ, β. Ecomp(θ, β, v) does not depend on s since the state of\nthe CNN s is determined by θ and v.\nIf we run the equilibrium propagation on this system, the\nsystem prediction s∗can be achieved by the current gradient\ndescent based CNN training method, i.e., training a CNN\nwith a ﬁxed conﬁguration. Then the optimization on θ is\nin fact a global optimization framework to ﬁnd the optimal\nCNN structure deﬁned by θ∗. Geometrically this is to ﬁnd a\nRiemannian manifold TCNN θ∗with its metric gtheta∗\nCNN deﬁned\nby θ∗, so that the optimal CNN can realize a geodesic\nU geo\nCNN(t), which gives a minimal system complexity, i.e., the\nmost efﬁcient CNN structure that can achieve the task.\nOf course, the discrete parameters of the structure of the\nCNN can not be updated by the gradient descent strategy. This\nis the same as in the reinforcement learning neural Turing\nmachines[34] if we regard the discrete structure parameters\nas the actions in the neural Turing machines. So theoretically\nwe can train the systems also with the reinforcement learning\nto ﬁnd the optimal structure parameters, while the back-\npropagation in [34] is replaced by the equilibrium propagation.\nAlso this framework might not work due to the complexity in\npractice, just as the neural Turing machine can only achieve\nsimple algorithmic tasks.\nIV. CONCLUSIONAL REMARKS\nThis paper is based on the belief that geometry plays a\ncentral role in understanding physical systems. Based on a\nbrief overview of the geometric structures of two ﬁelds, the\nquantum computation and the diffeomorphic computational\nanatomy, we show that deep learning systems hold a similar\ngeometric picture and the properties of deep learning systems\ncan be understood from this geometric perspective.\nWe summarize our observations on different deep learning\nsystems as follows:\nConvolutional neural networks\n• The structure of a CNN deﬁnes a manifold, the trans-\nformation space TCNN, and the Riemannian metric on\nit.\n• The goal of a CNN is to ﬁnd a transformation UCNN on\nthis Riemannian manifold, which is achieved by a curve\nUCNN(t) deﬁned by the local operations of the CNN\nstructure.\n• A CNN is not to ﬁnd the geodesic but to ﬁnd a curve\nwith a given structure to reach a point in the Riemannian\nmanifold. A deep CNN works only when the curve it\ndeﬁnes is longer than the geodesic between the identity\noperator I and UCNN. A too shallow network will never\nwork efﬁciently.\n• The convergence property of a CNN depends on both\nthe curvature of the Riemannian manifold and the com-\nplexity distribution of the curve. The linear complexity\n15\ndeﬁned by the conﬁguration of kernels and the nonlinear\ncomplexity deﬁned by the activation function should be\nbalanced. A too deep network with improper complexity\ndistribution may also fail.\n• Generally to ﬁnd the optimal CNN structure is difﬁcult.\n• How the structure of a CNN inﬂuence the curvature of\nthe manifold and the performance of the network needs\nto be investigated.\n• An alternative geometric structure is built on the data\nspace VCNN, where a CNN works by disentangling a\nhighly curved input manifold into a ﬂatter output mani-\nfold.\nResidual networks\n• Residual networks is a special case of CNNs, where the\ncurve to reach the target transformation consists of a large\nnumber of short line segments.\n• Each line segment corresponds to a near-identity trans-\nformation on the manifold, which can be achieved by the\none-order approximation. The optimal ResNet structure\nis determined by this one-order approximation.\n• The superior performance of ResNet comes from the\nbalanced complexity distribution along the curve and a\npotentially smaller parameter space deﬁned by the ResNet\nstructure.\nRecursive neural networks\n• Recursive neural networks have a structure that can be\nunderstood as a Lie group exponential mapping if the\nmanifold of transforamtions is regarded as a Lie group.\n• The relative simple structure of the recursive neural net-\nwork makes it possible to ﬁnd the representation mapping\nand the recursive operation during the training procedure.\nRecurrent neural networks\n• Recurrent neural networks can be understood as a sheet\nswept by a string on the data space VRNN.\n• A stacked or deep LSTM network accomplishes a cas-\ncaded structure of a pair of coupled strings.\n• The grid LSTM network extends the coupled stings into\nweaved membranes.\n• The attention mechanism and the neural Turing machine\nare coupled structure of heterogeneous subnetworks.\nGenerative adversarial networks\n• Generative adversarial networks are the simultaneous\noptimization of two curves leading to UG and UD, in\nTGAN, where each curve has a similar structure as CNNs.\nThe unsupervised learning of the target transformation\nUG is much ﬂexible than than the supervised learning of\nUCNN. These features explain the difﬁculty of training\nGANs.\n• LAPGANs achieve a better performance by both explor-\ning the frequency domain structure of the training data\nand constructing the curve UG by simpler local operations\non subspaces of the training data.\n• GRANs also work in a similar way as LAPGAN. The\ndifference is that GRANs construct a self-similarity like\nstructure which is encoded in the RNN structure of G.\nAlso GRANs only apply simpler local operations in\nTGAN.\n• InfoGANs reduce the ﬂexibility of UG by setting extra\nconstraints on clusters of data samples.\nEquilibrium propagation\n• The geodesic shooting algorithm of the template match-\ning is an example of the equilibrium propagation. It\nprovides an intuitive picture of how the equilibrium prop-\nagation can ﬁll the gap between the implicit framework of\noptimization of a cost function and the back-propagation\nof information in the explicit system.\n• Theoretically the equilibrium propagation can be used as\na framework for ﬁnd the optimal deep learning system.\nREFERENCES\n[1] H. W. Lin and M. Tegmark, “Why does deep and cheap learning work\nso well?,” arXiv:1608.08225, 2016.\n[2] A. Ashtekar and T. A. Schilling, “Geometrical formulation of quantum\nmechanics,” arXiv:gr-qc/9706069, 1997.\n[3] H.\nHeydari,\n“Geometric\nformulation\nof\nquantum\nmechanics,”\narXiv:1503.00238, 2015.\n[4] O. Andersson and H. Heydari, “Geometry of quantum evolution for\nmixed quantum states,” Physica Scripta, arXiv:1312.3360v1, 2014.\n[5] P. Levay, “The geometry of entanglement: metrics, connections and the\ngeometric phase,” Journal of Physics A: Mathematical and General,\narXiv:quant-ph/0306115v1, 2004.\n[6] P.\nLevay,\n“The\ntwistor\ngeometry\nof\nthree-qubit\nentanglement,”\narXiv:quant-ph/0403060v1, 2003.\n[7] M. V. Raamsdonk, “Building up spacetime with quantum entanglement,”\nInternational Journal of Modern Physics D, arXiv:1005.3035v1, 2010.\n[8] D. Stanford and L. Susskind, “Complexity and shock wave geometries,”\narXiv:1406.2678v2, 2014.\n[9] L. Susskind and Y. Zhao, “Switchbacks and the bridge to nowhere,”\narXiv:1408.2823v1, 2014.\n[10] M. G. M. A. Nielsen, M. R. Dowling and A. C. Doherty, “Quantum\ncomputation as geometry,” Science 311,1133, 2006.\n[11] M. R. Dowling and M. A. Nielsen, “The geometry of quantum compu-\ntation,” Quantum Information and Computation, vol. 8, no. 10, pp. 861–\n899, 2008.\n[12] M. F. Beg, M. I. Miller, A. Trouve, and L. Younes, “Computing large\ndeformation metric mappings via geodesic ﬂow of diffeomorphisms,”\nInt. Journal of Computer Vision, vol. 61, pp. 139–157, 2005.\n[13] M. Hernandez, M. Bossa, and S. Olmos, “Registration of anatomical\nimages using paths of diffeomorphisms parameterized with station-\nary vector ﬂows,” International Journal of Computer Vision, vol. 85,\npp. 291–306, 2009.\n[14] M. Lorenzi and X. Pennec, “Geodesics, parallel transport and one-\nparameter subgroups for diffeomorphic image registration,” Interna-\ntional Journal of Computer Vision, pp. 12:1–17, 2012.\n[15] S. Durrleman, X. Pennec, A. Trouv´e, G. Gerig, and N. Ayache,\n“Spatiotemporal atlas estimation for developmental delay detection in\nlongitudinal datasets,” in MICCAI, vol. 12 of 297-304, 2009.\n[16] D. D. H. M. Bruveris, F. Gay-Balmaz and T. Ratiu, “The momentum\nmap representation of images,” Journal of Nonlinear Science, vol. 21,\nno. 1, pp. 115–150, 2011.\n[17] M. Bruveris and D. Holm, “Geometry of image registration: The\ndiffeomorphism group and momentum maps,” Lecture Notes, 2013.\n[18] D. D. Holm, A. Trouve, and L. Younes, “The euler-poincare theory of\nmetamorphosis,” Quarterly of Applied Mathematics, vol. 67, pp. 661–\n685, 2009.\n[19] F. X. Vialard, L. Risser, D. Rueckert, and D. D. Holm, “Diffeomorphic\natlas estimation using geodesic shooting on volumetric images,” Annals\nof the BMVA, vol. 5, pp. 1–12, 2012.\n[20] F. X. Vialard, L. Risser, D. Rueckert, and C. J. Cotter, “Diffeomorphic\n3d image registration via geodesic shooting using an efﬁcient adjoint\ncalculation,” Int. Journal of Computer Vision, vol. 97, pp. 229–241,\n2012.\n[21] F. X. Vialard, L. Risser, D. D. Holm, and D. Rueckert, “Diffeomorphic\natlas estimation using karcher mean and geodesic shooting on volumetric\nimages,” in Medical Image Understanding and Analysis, 2011.\n[22] M. M. Postnikov, Geometry VI: Riemannian Geometry, Encyclopedia of\nmathematical science. Springer, 2001.\n16\n[23] B. Poole, S. Lahiri, M. Raghu, J. Sohldickstein, and S. Ganguli,\n“Exponential expressivity in deep neural networks through transient\nchaos,” 2016.\n[24] S. R. K. He, X. Zhang and J. Sun, “Deep residual learning for image\nrecognition,” arXiv:1512.03385, 2015.\n[25] S. R. K. He, X. Zhang and J. Sun, “Identity mappings in deep residual\nnetworks,” arXiv:1603.05027, 2016.\n[26] K. G. R. K. Srivastava and J. Schmidhuber, “Highway networks,”\narXiv:1505.00387, 2015.\n[27] A. Y. N. R. Socher, C. Lin and C. D. Manning, “Parsing natural scenes\nand natural language with recursive neural networks,” in Proceedings of\nthe 28 th International Conference on Machine Learning, 2011.\n[28] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and\nC. Potts, “Recursive deep models for semantic compositionality over a\nsentiment treebank,” 2013.\n[29] J. B. Z. C. Lipton and C. Elkan, “A critical review of recurrent neural\nnetworks for sequence learning,” arXiv:1506.00019, 2015.\n[30] I. D. N. Kalchbrenner and A. Graves, “Grid long short-term memory,”\narXiv:1507.01526, 2015.\n[31] R. K. K. C. A. C. R. S. R. Z. K. Xu, J. L. Ba and Y. Bengio, “Show,\nattend and tell: neural image caption generation with visual attention,”\n2015.\n[32] K. C. D. Bahdanau and Y. Bengio, “Neural machine translation by jointly\nlearning to align and translate,” arXiv:1409.0473, 2014.\n[33] A. G. V. Mnih, N. Heess and K. Kavukcuoglu, “Recurrent models of\nvisual attention,” pp. 2204–2212, 2014.\n[34] G. W. A. Graves and I. Danihelka, “Neural turing machines,”\narXiv:1410.5401, 2014.\n[35] W. Zaremba and I. Sutskever, “Reinforcement learning neural turing\nmachines - revised,” arXiv:1505.00521, 2015.\n[36] B. Scellier and Y. Bengio, “Equilibrium propagation: Bridging the gap\nbetween energy-based models and backpropagation,” arXiv:1602.05179,\n2016.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2017-10-30",
  "updated": "2017-10-30"
}