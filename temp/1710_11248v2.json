{
  "id": "http://arxiv.org/abs/1710.11248v2",
  "title": "Learning Robust Rewards with Adversarial Inverse Reinforcement Learning",
  "authors": [
    "Justin Fu",
    "Katie Luo",
    "Sergey Levine"
  ],
  "abstract": "Reinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the\nneed for extensive feature and reward engineering. Deep reinforcement learning\nmethods can remove the need for explicit engineering of policy or value\nfeatures, but still require a manually specified reward function. Inverse\nreinforcement learning holds the promise of automatic reward acquisition, but\nhas proven exceptionally difficult to apply to large, high-dimensional problems\nwith unknown dynamics. In this work, we propose adverserial inverse\nreinforcement learning (AIRL), a practical and scalable inverse reinforcement\nlearning algorithm based on an adversarial reward learning formulation. We\ndemonstrate that AIRL is able to recover reward functions that are robust to\nchanges in dynamics, enabling us to learn policies even under significant\nvariation in the environment seen during training. Our experiments show that\nAIRL greatly outperforms prior methods in these transfer settings.",
  "text": "Published as a conference paper at ICLR 2018\nLEARNING ROBUST REWARDS WITH ADVERSARIAL\nINVERSE REINFORCEMENT LEARNING\nJustin Fu, Katie Luo, Sergey Levine\nDepartment of Electrical Engineering and Computer Science\nUniversity of California, Berkeley\nBerkeley, CA 94720, USA\njustinjfu@eecs.berkeley.edu,katieluo@berkeley.edu,\nsvlevine@eecs.berkeley.edu\nABSTRACT\nReinforcement learning provides a powerful and general framework for decision\nmaking and control, but its application in practice is often hindered by the need\nfor extensive feature and reward engineering. Deep reinforcement learning meth-\nods can remove the need for explicit engineering of policy or value features, but\nstill require a manually speciﬁed reward function. Inverse reinforcement learning\nholds the promise of automatic reward acquisition, but has proven exceptionally\ndifﬁcult to apply to large, high-dimensional problems with unknown dynamics. In\nthis work, we propose AIRL, a practical and scalable inverse reinforcement learn-\ning algorithm based on an adversarial reward learning formulation. We demon-\nstrate that AIRL is able to recover reward functions that are robust to changes\nin dynamics, enabling us to learn policies even under signiﬁcant variation in the\nenvironment seen during training. Our experiments show that AIRL greatly out-\nperforms prior methods in these transfer settings.\n1\nINTRODUCTION\nWhile reinforcement learning (RL) provides a powerful framework for automating decision making\nand control, signiﬁcant engineering of elements such as features and reward functions has typically\nbeen required for good practical performance. In recent years, deep reinforcement learning has al-\nleviated the need for feature engineering for policies and value functions, and has shown promising\nresults on a range of complex tasks, from vision-based robotic control (Levine et al., 2016) to video\ngames such as Atari (Mnih et al., 2015) and Minecraft (Oh et al., 2016). However, reward engineer-\ning remains a signiﬁcant barrier to applying reinforcement learning in practice. In some domains,\nthis may be difﬁcult to specify (for example, encouraging “socially acceptable” behavior), and in\nothers, a na¨ıvely speciﬁed reward function can produce unintended behavior (Amodei et al., 2016).\nMoreover, deep RL algorithms are often sensitive to factors such as reward sparsity and magnitude,\nmaking well performing reward functions particularly difﬁcult to engineer.\nInverse reinforcement learning (IRL) (Russell, 1998; Ng & Russell, 2000) refers to the problem of\ninferring an expert’s reward function from demonstrations, which is a potential method for solv-\ning the problem of reward engineering. However, inverse reinforcement learning methods have\ngenerally been less efﬁcient than direct methods for learning from demonstration such as imitation\nlearning (Ho & Ermon, 2016), and methods using powerful function approximators such as neural\nnetworks have required tricks such as domain-speciﬁc regularization and operate inefﬁciently over\nwhole trajectories (Finn et al., 2016b). There are many scenarios where IRL may be preferred over\ndirect imitation learning, such as re-optimizing a reward in novel environments (Finn et al., 2017) or\nto infer an agent’s intentions, but IRL methods have not been shown to scale to the same complexity\nof tasks as direct imitation learning. However, adversarial IRL methods (Finn et al., 2016b;a) hold\npromise for tackling difﬁcult tasks due to the ability to adapt training samples to improve learning\nefﬁciency.\nPart of the challenge is that IRL is an ill-deﬁned problem, since there are many optimal policies\nthat can explain a set of demonstrations, and many rewards that can explain an optimal policy (Ng\n1\narXiv:1710.11248v2  [cs.LG]  13 Aug 2018\nPublished as a conference paper at ICLR 2018\net al., 1999). The maximum entropy (MaxEnt) IRL framework introduced by Ziebart et al. (2008)\nhandles the former ambiguity, but the latter ambiguity means that IRL algorithms have difﬁculty\ndistinguishing the true reward functions from those shaped by the environment dynamics. While\nshaped rewards can increase learning speed in the original training environment, when the reward\nis deployed at test-time on environments with varying dynamics, it may no longer produce optimal\nbehavior, as we discuss in Sec. 5. To address this issue, we discuss how to modify IRL algorithms\nto learn rewards that are invariant to changing dynamics, which we refer to as disentangled rewards.\nIn this paper, we propose adversarial inverse reinforcement learning (AIRL), an inverse reinforce-\nment learning algorithm based on adversarial learning. Our algorithm provides for simultaneous\nlearning of the reward function and value function, which enables us to both make use of the efﬁ-\ncient adversarial formulation and recover a generalizable and portable reward function, in contrast\nto prior works that either do not recover a reward functions (Ho & Ermon, 2016), or operates at\nthe level of entire trajectories, making it difﬁcult to apply to more complex problem settings (Finn\net al., 2016b;a). Our experimental evaluation demonstrates that AIRL outperforms prior IRL meth-\nods (Finn et al., 2016b) on continuous, high-dimensional tasks with unknown dynamics by a wide\nmargin. When compared to GAIL (Ho & Ermon, 2016), which does not attempt to directly recover\nrewards, our method achieves comparable results on tasks that do not require transfer. However,\non tasks where there is considerable variability in the environment from the demonstration setting,\nGAIL and other IRL methods fail to generalize. In these settings, our approach, which can effec-\ntively disentangle the goals of the expert from the dynamics of the environment, achieves superior\nresults.\n2\nRELATED WORK\nInverse reinforcement learning (IRL) is a form of imitation learning and learning from demonstra-\ntion (Argall et al., 2009). Imitation learning methods seek to learn policies from expert demonstra-\ntions, and IRL methods accomplish this by ﬁrst inferring the expert’s reward function. Previous IRL\napproaches have included maximum margin approaches (Abbeel & Ng, 2004; Ratliff et al., 2006),\nand probabilistic approaches such as Ziebart et al. (2008); Boularias et al. (2011). In this work, we\nwork under the maximum causal IRL framework of Ziebart (2010). Some advantages of this frame-\nwork are that it removes ambiguity between demonstrations and the expert policy, and allows us to\ncast the reward learning problem as a maximum likelihood problem, connecting IRL to generative\nmodel training.\nOur proposed method most closely resembles the algorithms proposed by Uchibe (2017); Ho & Er-\nmon (2016); Finn et al. (2016a). Generative adversarial imitation learning (GAIL) (Ho & Ermon,\n2016) differs from our work in that it is not an IRL algorithm that seeks to recover reward functions.\nThe critic or discriminator of GAIL is unsuitable as a reward since, at optimality, it outputs 0.5 uni-\nformly across all states and actions. Instead, GAIL aims only to recover the expert’s policy, which\nis a less portable representation for transfer. Uchibe (2017) does not interleave policy optimization\nwith reward learning within an adversarial framework. Improving a policy within an adversarial\nframework corresponds to training an amortized sampler for an energy-based model, and prior work\nhas shown this is crucial for performance (Finn et al., 2016b). Wulfmeier et al. (2015) also consider\nlearning cost functions with neural networks, but only evaluate on simple domains where analyt-\nically solving the problem with value iteration is tractable. Previous methods which aim to learn\nnonlinear cost functions have used boosting (Ratliff et al., 2007) and Gaussian processes (Levine\net al., 2011), but still suffer from the feature engineering problem.\nOur IRL algorithm builds on the adversarial IRL framework proposed by Finn et al. (2016a), with\nthe discriminator corresponding to an odds ratio between the policy and exponentiated reward dis-\ntribution. The discussion in Finn et al. (2016a) is theoretical, and to our knowledge no prior work\nhas reported a practical implementation of this method. Our experiments show that direct imple-\nmentation of the proposed algorithm is ineffective, due to high variance from operating over entire\ntrajectories. While it is straightforward to extend the algorithm to single state-action pairs, as we\ndiscuss in Section 4, a simple unrestricted form of the discriminator is susceptible to the reward\nambiguity described in (Ng et al., 1999), making learning the portable reward functions difﬁcult.\nAs illustrated in our experiments, this greatly limits the generalization capability of the method: the\nlearned reward functions are not robust to environment changes, and it is difﬁcult to use the algo-\n2\nPublished as a conference paper at ICLR 2018\nrithm for the purpose of inferring the intentions of agents. We discuss how to overcome this issue in\nSection 5.\nAmin et al. (2017) consider learning reward functions which generalize to new tasks given multiple\ntraining tasks. Our work instead focuses on how to achieve generalization within the standard IRL\nformulation.\n3\nBACKGROUND\nOur inverse reinforcement learning method builds on the maximum causal entropy IRL frame-\nwork (Ziebart, 2010), which considers an entropy-regularized Markov decision process (MDP), de-\nﬁned by the tuple (S, A, T , r, γ, ρ0). S, A are the state and action spaces, respectively, γ ∈(0, 1) is\nthe discount factor. The dynamics or transition distribution T (s′|a, s), the initial state distribution\nρ0(s), and the reward function r(s, a) are unknown in the standard reinforcement learning setup and\ncan only be queried through interaction with the MDP.\nThe goal of (forward) reinforcement learning is to ﬁnd the optimal policy π∗that maximizes the\nexpected entropy-regularized discounted reward, under π, T , and ρ0:\nπ∗= arg maxπ Eτ∼π\n\" T\nX\nt=0\nγt(r(st, at) + H(π(·|st)))\n#\n,\nwhere τ = (s0, a0, ...sT , aT ) denotes a sequence of states and actions induced by the policy\nand dynamics.\nIt can be shown that the trajectory distribution induced by the optimal policy\nπ∗(a|s) takes the form π∗(a|s) ∝exp{Q∗\nsoft(st, at)} (Ziebart, 2010; Haarnoja et al., 2017), where\nQ∗\nsoft(st, at) = rt(s, a) + E(st+1,...)∼π[PT\nt′=t γt′(r(st′, at′) + H(π(·|st′))] denotes the soft Q-\nfunction.\nInverse reinforcement learning instead seeks infer the reward function r(s, a) given a set of demon-\nstrations D = {τ1, ..., τN}. In IRL, we assume the demonstrations are drawn from an optimal policy\nπ∗(a|s). We can interpret the IRL problem as solving the maximum likelihood problem:\nmax\nθ\nEτ∼D [log pθ(τ)] ,\n(1)\nWhere pθ(τ) ∝p(s0) QT\nt=0 p(st+1|st, at)eγtrθ(st,at) parametrizes the reward function rθ(s, a) but\nﬁxes the dynamics and initial state distribution to that of the MDP. Note that under determinis-\ntic dynamics, this simpliﬁes to an energy-based model where for feasible trajectories, pθ(τ) ∝\ne\nPT\nt=0 γtrθ(st,at) (Ziebart et al., 2008).\nFinn et al. (2016a) propose to cast optimization of Eqn. 1 as a GAN (Goodfellow et al., 2014)\noptimization problem. They operate in a trajectory-centric formulation, where the discriminator\ntakes on a particular form (fθ(τ) is a learned function; π(τ) is precomputed and its value “ﬁlled\nin”):\nDθ(τ) =\nexp{fθ(τ)}\nexp{fθ(τ)} + π(τ),\n(2)\nand the policy π is trained to maximize R(τ) = log(1 −D(τ)) −log D(τ). Updating the discrim-\ninator can be viewed as updating the reward function, and updating the policy can be viewed as\nimproving the sampling distribution used to estimate the partition function. If trained to optimality,\nit can be shown that an optimal reward function can be extracted from the optimal discriminator as\nf ∗(τ) = R∗(τ)+const, and π recovers the optimal policy. We refer to this formulation as generative\nadversarial network guided cost learning (GAN-GCL) to discriminate it from guided cost learning\n(GCL) (Finn et al., 2016a). This formulation shares similarities with GAIL (Ho & Ermon, 2016),\nbut GAIL does not place special structure on the discriminator, so the reward cannot be recovered.\n4\nADVERSARIAL INVERSE REINFORCEMENT LEARNING (AIRL)\nIn practice, using full trajectories as proposed by GAN-GCL can result in high variance estimates\nas compared to using single state, action pairs, and our experimental results show that this results in\n3\nPublished as a conference paper at ICLR 2018\nvery poor learning. We could instead propose a straightforward conversion of Eqn. 2 into the single\nstate and action case, where:\nDθ(s, a) =\nexp{fθ(s, a)}\nexp{fθ(s, a)} + π(a|s).\nAs in the trajectory-centric case, we can show that, at optimality, f ∗(s, a) = log π∗(a|s) = A∗(s, a),\nthe advantage function of the optimal policy. We justify this, as well as a proof that this algorithm\nsolves the IRL problem in Appendix A .\nThis change results in an efﬁcient algorithm for imitation learning. However, it is less desirable\nfor the purpose of reward learning. While the advantage is a valid optimal reward function, it is a\nheavily entangled reward, as it supervises each action based on the action of the optimal policy for\nthe training MDP. Based on the analysis in the following Sec. 5, we cannot guarantee that this reward\nwill be robust to changes in environment dynamics. In our experiments we demonstrate several cases\nwhere this reward simply encourages mimicking the expert policy π∗, and fails to produce desirable\nbehavior even when changes to the environment are made.\n5\nTHE REWARD AMBIGUITY PROBLEM\nWe now discuss why IRL methods can fail to learn robust reward functions. First, we review the\nconcept of reward shaping. Ng et al. (1999) describe a class of reward transformations that preserve\nthe optimal policy. Their main theoretical result is that under the following reward transformation,\nˆr(s, a, s′) = r(s, a, s′) + γΦ(s′) −Φ(s) ,\n(3)\nthe optimal policy remains unchanged, for any function Φ : S →R. Moreover, without prior knowl-\nedge of the dynamics, this is the only class of reward transformations that exhibits policy invariance.\nBecause IRL methods only infer rewards from demonstrations given from an optimal agent, they\ncannot in general disambiguate between reward functions within this class of transformations, un-\nless the class of learnable reward functions is restricted.\nWe argue that shaped reward functions may not be robust to changes in dynamics. We formalize this\nnotion by studying policy invariance in two MDPs M, M ′ which share the same reward and differ\nonly in the dynamics, denoted as T and T ′, respectively.\nSuppose an IRL algorithm recovers a shaped, policy invariant reward ˆr(s, a, s′) under MDP M\nwhere Φ ̸= 0. Then, there exists MDP pairs M, M ′ where changing the transition model from T\nto T ′ breaks policy invariance on MDP M ′. As a simple example, consider deterministic dynamics\nT(s, a) →s′ and state-action rewards ˆr(s, a) = r(s, a) + γΦ(T(s, a)) −Φ(s). It is easy to see that\nchanging the dynamics T to T ′ such that T ′(s, a) ̸= T(s, a) means that ˆr(s, a) no longer lies in the\nequivalence class of Eqn. 3 for M ′.\n5.1\nDISENTANGLING REWARDS FROM DYNAMICS\nFirst, let the notation Q∗\nr,T (s, a) denote the optimal Q-function with respect to a reward function\nr and dynamics T, and π∗\nr,T (a|s) denote the same for policies. We ﬁrst deﬁne our notion of a\n”disentangled” reward.\nDeﬁnition 5.1 (Disentangled Rewards). A reward function r′(s, a, s′) is (perfectly) disentangled\nwith respect to a ground-truth reward r(s, a, s′) and a set of dynamics T such that under all dynam-\nics T ∈T , the optimal policy is the same: π∗\nr′,T (a|s) = π∗\nr,T (a|s)\nWe could also expand this deﬁnition to include a notion of suboptimality. However, we leave this\ndirection to future work.\nUnder maximum causal entropy RL, the following condition is equivalent to two optimal policies\nbeing equal, since Q-functions and policies are equivalent representations (up to arbitrary functions\nof state f(s)):\nQ∗\nr′,T (s, a) = Q∗\nr,T (s, a) −f(s)\nTo remove unwanted reward shaping with arbitrary reward function classes, the learned reward func-\ntion can only depend on the current state s. We require that the dynamics satisfy a decomposability\n4\nPublished as a conference paper at ICLR 2018\nAlgorithm 1 Adversarial inverse reinforcement learning\n1: Obtain expert trajectories τ E\ni\n2: Initialize policy π and discriminator Dθ,φ.\n3: for step t in {1, ..., N} do\n4:\nCollect trajectories τi = (s0, a0, ..., sT , aT ) by executing π.\n5:\nTrain Dθ,φ via binary logistic regression to classify expert data τ E\ni from samples τi.\n6:\nUpdate reward rθ,φ(s, a, s′) ←log Dθ,φ(s, a, s′) −log(1 −Dθ,φ(s, a, s′))\n7:\nUpdate π with respect to rθ,φ using any policy optimization method.\n8: end for\ncondition where functions over current states f(s) and next states g(s′) can be isolated from their\nsum f(s) + g(s′). This can be satisﬁed for example by adding self transitions at each state to\nan ergodic MDP, or any of the environments used in our experiments. The exact deﬁnition of the\ncondition, as well as proof of the following statements are included in Appendix B.\nTheorem 5.1. Let r(s) be a ground-truth reward, and T be a dynamics model satisfying the de-\ncomposability condition. Suppose IRL recovers a state-only reward r′(s) such that it produces an\noptimal policy in T:\nQ∗\nr′,T (s, a) = Q∗\nr,T (s, a) −f(s)\nThen, r′(s) is disentangled with respect to all dynamics.\nTheorem 5.2. If a reward function r′(s, a, s′) is disentangled for all dynamics functions, then it\nmust be state-only. i.e. If for all dynamics T,\nQ∗\nr,T (s, a) = Q∗\nr′,T (s, a) + f(s) ∀s, a\nThen r′ is only a function of state.\nIn the traditional IRL setup, where we learn the reward in a single MDP, our analysis motivates\nlearning reward functions that are solely functions of state. If the ground truth reward is also only a\nfunction of state, this allows us to recover the true reward up to a constant.\n6\nLEARNING DISENTANGLED REWARDS WITH AIRL\nIn the method presented in Section 4, we cannot learn a state-only reward function, rθ(s), meaning\nthat we cannot guarantee that learned rewards will not be shaped. In order to decouple the reward\nfunction from the advantage, we propose to modify the discriminator of Sec. 4 with the form:\nDθ,φ(s, a, s′) =\nexp{fθ,φ(s, a, s′)}\nexp{fθ,φ(s, a, s′)} + π(a|s),\nwhere fθ,φ is restricted to a reward approximator gθ and a shaping term hφ as\nfθ,φ(s, a, s′) = gθ(s, a) + γhφ(s′) −hφ(s).\n(4)\nThe additional shaping term helps mitigate the effects of unwanted shaping on our reward approx-\nimator gθ (and as we will show, in some cases it can account for all shaping effects). The entire\ntraining procedure is detailed in Algorithm 1. Our algorithm resembles GAIL (Ho & Ermon, 2016)\nand GAN-GCL (Finn et al., 2016a), where we alternate between training a discriminator to classify\nexpert data from policy samples, and update the policy to confuse the discriminator.\nThe advantage of this approach is that we can now parametrize gθ(s) as solely a function of the state,\nallowing us to extract rewards that are disentangled from the dynamics of the environment in which\nthey were trained. In fact, under this restricted case, we can show the following under deterministic\nenvironments with a state-only ground truth reward (proof in Appendix C):\ng∗(s) = r∗(s) + const,\nh∗(s) = V ∗(s) + const,\nwhere r∗is the true reward function. Since f ∗must recover to the advantage as shown in Sec. 4, h\nrecovers the optimal value function V ∗, which serves as the reward shaping term.\n5\nPublished as a conference paper at ICLR 2018\nTo be consistent with Sec. 4, an alternative way to interpret the form of Eqn. 4 is to view fθ,φ as the\nadvantage under deterministic dynamics\nf ∗(s, a, s′) = r∗(s) + γV ∗(s′)\n|\n{z\n}\nQ(s,a)\n−V ∗(s)\n| {z }\nV (s)\n= A∗(s, a)\nIn stochastic environments, we can instead view f(s, a, s′) as a single-sample estimate of A∗(s, a).\n7\nEXPERIMENTS\nIn our experiments, we aim to answer two questions:\n1. Can AIRL learn disentangled rewards that are robust to changes in environment dynamics?\n2. Is AIRL efﬁcient and scalable to high-dimensional continuous control tasks?\nTo answer 1, we evaluate AIRL in transfer learning scenarios, where a reward is learned in a training\nenvironment, and optimized in a test environment with signiﬁcantly different dynamics. We show\nthat rewards learned with our algorithm under the constraint presented in Section 5 still produce\noptimal or near-optimal behavior, while na¨ıve methods that do not consider reward shaping fail. We\nalso show that in small MDPs, we can recover the exact ground truth reward function.\nTo answer 2, we compare AIRL as an imitation learning algorithm against GAIL (Ho & Ermon,\n2016) and the GAN-based GCL algorithm proposed by Finn et al. (2016a), which we refer to as\nGAN-GCL, on standard benchmark tasks that do not evaluate transfer. Note that Finn et al. (2016a)\ndoes not implement or evaluate GAN-GCL and, to our knowledge, we present the ﬁrst empirical\nevaluation of this algorithm. We ﬁnd that AIRL performs on par with GAIL in a traditional imitation\nlearning setup while vastly outperforming it in transfer learning setups, and outperforms GAN-GCL\nin both settings. It is worth noting that, except for (Finn et al., 2016b), our method is the only IRL\nalgorithm that we are aware of that scales to high dimensional tasks with unknown dynamics, and\nalthough GAIL (Ho & Ermon, 2016) resembles an IRL algorithm in structure, it does not recover\ndisentangled reward functions, making it unable to re-optimize the learned reward under changes in\nthe environment, as we illustrate below.\nFor our continuous control tasks, we use trust region policy optimization (Schulman et al., 2015)\nas our policy optimization algorithm across all evaluated methods, and in the tabular MDP task, we\nuse soft value iteration. We obtain expert demonstrations by training an expert policy on the ground\ntruth reward, but hide the ground truth reward from the IRL algorithm. In this way, we simulate a\nscenario where we wish to use RL to solve a task but wish to refrain from manual reward engineering\nand instead seek to learn a reward function from demonstrations. Our code and additional supple-\nmentary material including videos will be available at https://sites.google.com/view/\nadversarial-irl, and hyper-parameter and architecture choices are detailed in Appendix D.\n7.1\nRECOVERING TRUE REWARDS IN TABULAR MDPS\nWe ﬁrst consider MaxEnt IRL in a toy task with randomly generated MDPs. The MDPs have 16\nstates, 4 actions, randomly drawn transition matrices, and a reward function that always gives a\nreward of 1.0 when taking an action from state 0. The initial state is always state 1.\nThe optimal reward, learned reward with a state-only reward function, and learned reward using\na state-action reward function are shown in Fig. 1. We subtract a constant offset from all reward\nfunctions so that they share the same mean for visualization - this does not inﬂuence the optimal\npolicy. AIRL with a state-only reward function is able to recover the ground truth reward, but AIRL\nwith a state-action reward instead recovers a shaped advantage function.\nWe also show that in the transfer learning setup, under a new transition matrix T ′, the optimal policy\nunder the state-only reward achieves optimal performance (it is identical to the ground truth reward)\nwhereas the state-action reward only improves marginally over uniform random policy. The learning\ncurve for this experiment is shown in Fig 2.\n6\nPublished as a conference paper at ICLR 2018\nFigure 1:\nGround truth (a) and learned rewards (b, c) on\nthe random MDP task. Dark blue corresponds to a reward\nof 1, and white corresponds to 0. Note that AIRL with a\nstate-only reward recovers the ground truth, whereas the\nstate-action reward is shaped.\nFigure 2:\nLearning curve for the\ntransfer learning experiment on tabular\nMDPs. Value iteration steps are plot-\nted on the x-axis, against returns for the\npolicy on the y-axis.\n7.2\nDISENTANGLING REWARDS IN CONTINUOUS CONTROL TASKS\nTo evaluate whether our method can learn disentangled rewards in higher dimensional environments,\nwe perform transfer learning experiments on continuous control tasks. In each task, a reward is\nlearned via IRL on the training environment, and the reward is used to reoptimize a new policy on\na test environment. We train two IRL algorithms, AIRL and GAN-GCL, with state-only and state-\naction rewards. We also include results for directly transferring the policy learned with GAIL, and\nan oracle result that involves optimizing the ground truth reward function with TRPO. Numerical\nresults for these environment transfer experiments are given in Table 1.\nThe ﬁrst task involves a 2D point mass navigating to a goal position in a small maze when the\nposition of the walls are changed between train and test time. At test time, the agent cannot simply\nmimic the actions learned during training, and instead must successfully infer that the goal in the\nmaze is to reach the target. The task is shown in Fig. 3. Only AIRL trained with state-only rewards\nis able to consistently navigate to the goal when the maze is modiﬁed. Direct policy transfer and\nstate-action IRL methods learn rewards which encourage the agent to take the same path taken in\nthe training environment, which is blocked in the test environment. We plot the learned reward in\nFig. 4.\nIn our second task, we modify the agent itself. We train a quadrupedal “ant” agent to run forwards,\nand at test time we disable and shrink two of the front legs of the ant such that it must signiﬁcantly\nchange its gait.We ﬁnd that AIRL is able to learn reward functions that encourage the ant to move\nforwards, acquiring a modiﬁed gait that involves orienting itself to face the forward direction and\ncrawling with its two hind legs. Alternative methods, including transferring a policy learned by\nGAIL (which achieves near-optimal performance with the unmodiﬁed agent), fail to move forward\nat all. We show the qualitative difference in behavior in Fig. 5.\nWe have demonstrated that AIRL can learn disentangled rewards that can accommodate signiﬁcant\ndomain shift even in high-dimensional environments where it is difﬁcult to exactly extract the true\nreward. GAN-GCL can presumably learn disentangled rewards, but we ﬁnd that the trajectory-\ncentric formulation does not perform well even in learning rewards in the original task, let alone\ntransferring to a new domain. GAIL learns successfully in the training domain, but does not acquire\na representation that is suitable for transfer to test domains.\n7\nPublished as a conference paper at ICLR 2018\nFigure 3:\nIllustration of the shifting maze\ntask, where the agent (blue) must reach the goal\n(green).\nDuring training the agent must go\naround the wall on the left side, but during test\ntime it must go around on the right.\nFigure 4:\nReward learned on the point mass\nshifting maze task. The goal is located at the\ngreen star and the agent starts at the white circle.\nNote that there is little reward shaping, which en-\nables the reward to transfer well.\nFigure 5:\nTop row: An ant running forwards (right in the picture) in the training environment.\nBottom row: Behavior acquired by optimizing a state-only reward learned with AIRL on the disabled\nant environment. Note that the ant must orient itself before crawling forward, which is a qualitatively\ndifferent behavior from the optimal policy in the original environment, which runs sideways.\nTable 1: Results on transfer learning tasks. Mean scores (higher is better) are reported over 5 runs.\nWe also include results for TRPO optimizing the ground truth reward, and the performance of a\npolicy learned via GAIL on the training environment.\nState-Only?\nPoint Mass-Maze\nAnt-Disabled\nGAN-GCL\nNo\n-40.2\n-44.8\nGAN-GCL\nYes\n-41.8\n-43.4\nAIRL (ours)\nNo\n-31.2\n-41.4\nAIRL (ours)\nYes\n-8.82\n130.3\nGAIL, policy transfer\nN/A\n-29.9\n-58.8\nTRPO, ground truth\nN/A\n-8.45\n315.5\n7.3\nBENCHMARK TASKS FOR IMITATION LEARNING\nFinally, we evaluate AIRL as an imitation learning algorithm against the GAN-GCL and the state-\nof-the-art GAIL on several benchmark tasks. Each algorithm is presented with 50 expert demonstra-\ntions, collected from a policy trained with TRPO on the ground truth reward function. For AIRL,\nwe use an unrestricted state-action reward function as we are not concerned with reward transfer.\nNumerical results are presented in Table 2.These experiments do not test transfer, and in a sense can\nbe regarded as “testing on the training set,” but they match the settings reported in prior work (Ho &\nErmon, 2016).\n8\nPublished as a conference paper at ICLR 2018\nWe ﬁnd that the performance difference between AIRL and GAIL is negligible, even though AIRL\nis a true IRL algorithm that recovers reward functions, while GAIL does not. Both methods achieve\nclose to the best possible result on each task, and there is little room for improvement. This result\ngoes against the belief that IRL algorithms are indirect, and less efﬁcient that direct imitation learn-\ning algorithms (Ho & Ermon, 2016). The GAN-GCL method is ineffective on all but the simplest\nPendulum task when trained with the same number of samples as AIRL and GAIL. We ﬁnd that a\ndiscriminator trained over trajectories easily overﬁts and provides poor learning signal for the policy.\nOur results illustrate that AIRL achieves the same performance as GAIL on benchmark imitation\ntasks that do not require any generalization. On tasks that require transfer and generalization, illus-\ntrated in the previous section, AIRL outperforms GAIL by a wide margin, since our method is able\nto recover disentangled rewards that transfer effectively in the presence of domain shift.\nTable 2: Results on imitation learning benchmark tasks. Mean scores (higher is better) are reported\nacross 5 runs.\nPendulum\nAnt\nSwimmer\nHalf-Cheetah\nGAN-GCL\n-261.5\n460.6\n-10.6\n-670.7\nGAIL\n-226.0\n1358.7\n140.2\n1642.8\nAIRL (ours)\n-204.7\n1238.6\n139.1\n1839.8\nAIRL State Only (ours)\n-221.5\n1089.3\n136.4\n891.9\nExpert (TRPO)\n-179.6\n1537.9\n141.1\n1811.2\nRandom\n-654.5\n-108.1\n-11.5\n-988.4\n8\nCONCLUSION\nWe presented AIRL, a practical and scalable IRL algorithm that can learn disentangled rewards and\ngreatly outperforms both prior imitation learning and IRL algorithms. We show that rewards learned\nwith AIRL transfer effectively under variation in the underlying domain, in contrast to unmodiﬁed\nIRL methods which tend to recover brittle rewards that do not generalize well and GAIL, which\ndoes not recover reward functions at all. In small MDPs where the optimal policy and reward are\nunambiguous, we also show that we can exactly recover the ground-truth rewards up to a constant.\nACKNOWLEDGEMENTS\nThis research was supported by the National Science Foundation through IIS-1651843, IIS-1614653,\nand IIS-1637443. We would like to thank Roberto Calandra for helpful feedback on the paper.\nREFERENCES\nPieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In\nInternational Conference on Machine Learning (ICML), 2004.\nKareem Amin, Nan Jiang, and Satinder P. Singh. Repeated inverse reinforcement learning. In\nAdvances in Neural Information Processing Systems (NIPS), 2017.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-\ncrete problems in AI safety. ArXiv Preprint, abs/1606.06565, 2016.\nBrenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning\nfrom demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.\nAbdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning.\nIn International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2011.\nChelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative\nadversarial networks, inverse reinforcement learning, and energy-based models. abs/1611.03852,\n2016a.\n9\nPublished as a conference paper at ICLR 2018\nChelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control\nvia policy optimization. In International Conference on Machine Learning (ICML), 2016b.\nChelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, and Sergey Levine. Generalizing skills with semi-\nsupervised reinforcement learning.\nIn International Conference on Learning Representations\n(ICLR), 2017.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-\nmation Processing Systems (NIPS). 2014.\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with\ndeep energy-based policies. In International Conference on Machine Learning (ICML), 2017.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural\nInformation Processing Systems (NIPS), 2016.\nSergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse reinforcement learning with\ngaussian processes. In Advances in Neural Information Processing Systems (NIPS), 2011.\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-\nmotor policies. Journal of Machine Learning (JMLR), 2016.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, feb 2015. ISSN 0028-0836.\nAndrew Ng and Stuart Russell. Algorithms for reinforcement learning. In International Conference\non Machine Learning (ICML), 2000.\nAndrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:\nTheory and application to reward shaping. In International Conference on Machine Learning\n(ICML), 1999.\nJunhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory, active\nperception, and action in minecraft. In International Conference on Machine Learning (ICML),\n2016.\nNathan Ratliff, David Bradley, J. Andrew Bagnell, and Joel Chestnutt. Boosting structured pre-\ndiction for imitation learning. In Advances in Neural Information Processing Systems (NIPS),\n2007.\nNathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In\nInternational Conference on Machine Learning (ICML), 2006.\nStuart Russell. Learning agents for uncertain environments. In Conference On Learning Theory\n(COLT), 1998.\nJohn Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust Region\nPolicy Optimization. In International Conference on Machine Learning (ICML), 2015.\nEiji Uchibe. Model-free deep inverse reinforcement learning by logistic regression. Neural Process-\ning Letters, 2017. ISSN 1573-773X. doi: 10.1007/s11063-017-9702-7.\nMarkus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforce-\nment learning. In arXiv preprint arXiv:1507.04888, 2015.\nBrian Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal en-\ntropy. PhD thesis, Carnegie Mellon University, 2010.\nBrian Ziebart, Andrew Maas, Andrew Bagnell, and Anind Dey. Maximum entropy inverse rein-\nforcement learning. In AAAI Conference on Artiﬁcial Intelligence (AAAI), 2008.\n10\nPublished as a conference paper at ICLR 2018\nAPPENDICES\nA\nJUSTIFICATION OF AIRL\nIn this section, we show that the objective of AIRL matches that of solving the maximum causal\nentropy IRL problem. We use a similar method as Finn et al. (2016a), which shows the justiﬁcation\nof GAN-GCL for the trajectory-centric formulation. For simplicity we derive everything in the\nundiscounted case.\nA.1\nSETUP\nAs mentioned in Section 3, the goal of IRL can be seen as training a generative model over trajecto-\nries as:\nmax\nθ\nJ(θ) = max\nθ\nEτ∼D[log pθ(τ)]\nWhere the distribution pθ(τ) is parametrized as pθ(τ) ∝p(s0) QT −1\nt=0 p(st+1|st, at)erθ(st,at). We\ncan compute the gradient with respect to θ as follows:\n∂\n∂θJ(θ) = ED[ ∂\n∂θ log pθ(τ)] == ED[\nT\nX\nt=0\n∂\n∂θrθ(st, at)] −∂\n∂θ log Zθ\n= ED[\nT\nX\nt=0\n∂\n∂θrθ(st, at)] −Epθ[\nT\nX\nt=0\n∂\n∂θrθ(st, at)]\nLet pθ,t(st, at) =\nR\nst′̸=t,at′̸=t pθ(τ) denote the state-action marginal at time t. Rewriting the above\nequation, we have:\n∂\n∂θJ(θ) =\nT\nX\nt=0\nED[ ∂\n∂θrθ(st, at)] −Epθ,t[ ∂\n∂θrθ(st, at)]\nAs it is difﬁcult to draw samples from pθ, we instead train a separate importance sampling distribu-\ntion µ(τ). For the choice of this distribution, we follow Finn et al. (2016a) and use a mixture policy\nµ(a|s) = 1\n2π(a|s) + 1\n2 ˆp(a|s), where ˆp(a|s) is a rough density estimate trained on the demonstra-\ntions. This is justiﬁed as reducing the variance of the importance sampling estimate when the policy\nπ(a|s) has poor coverage over the demonstrations in the early stages of training. Thus, our new\ngradient is:\n∂\n∂θJ(θ) =\nT\nX\nt=0\nED[ ∂\n∂θrθ(st, at)] −Eµt[pθ,t(st, at)\nµt(st, at)\n∂\n∂θrθ(st, at)]\n(5)\nWe additionally wish to adapt the importance sampler π to reduce variance,\nby min-\nimizing\nDKL(π(τ)||pθ(τ)).\nThe\npolicy\ntrajectory\ndistribution\nfactorizes\nas\nπ(τ)\n=\np(s0) QT −1\nt=0 p(st+1|st, at)π(at|st). The dynamics and initial state terms inside π(τ) and pθ(τ)\ncancel, leaving the entropy-regularized policy objective:\nmax\nπ\nEπ[\nT\nX\nt=0\nrθ(st, at) −log π(at|st))]\n(6)\nIn AIRL, we replace the cost learning objective with training a discriminator of the following form:\nDθ(s, a) =\nexp{fθ(s, a)}\nexp{fθ(s, a)} + π(a|s)\n(7)\nThe objective of the discriminator is to minimize cross-entropy loss between expert demonstrations\nand generated samples:\nL(θ) =\nT\nX\nt=0\n−ED [log Dθ(st, at)] −Eπt [log(1 −Dθ(st, at))]\n11\nPublished as a conference paper at ICLR 2018\nWe replace the policy optimization objective with the following reward:\nˆr(s, a) = log(Dθ(s, a)) −log(1 −Dθ(s, a))\nA.2\nDISCRIMINATOR OBJECTIVE\nFirst, we show that training the gradient of the discriminator objective is the same as Eqn. 5. We\nwrite the negative loss to turn the minimization problem into maximization, and use µ to denote a\nmixture between the dataset and policy samples.\n−L(θ) =\nT\nX\nt=0\nED [log Dθ(st, at)] + Eπt [log(1 −Dθ(st, at))]\n=\nT\nX\nt=0\nED\n\u0014\nlog\nexp{fθ(st, at)}\nexp{fθ(st, at)} + π(at|st)\n\u0015\n+ Eπt\n\u0014\nlog\nπ(at|st)\nexp{fθ(st, at)} + π(at|st)\n\u0015\n=\nT\nX\nt=0\nED [fθ(st, at)] + Eπt [log π(at|st)] −2E¯µt [log (exp{fθ(st, at)} + π(at|st))]\nTaking the derivative w.r.t. θ,\n∂\n∂θL(θ) =\nT\nX\nt=0\nED\n\u0014 ∂\n∂θfθ(st, at)\n\u0015\n−Eµt\n\u0014\nexp{fθ(st, at)}\n1\n2 exp{fθ(st, at)} + 1\n2π(at|st)\n∂\n∂θfθ(st, at)\n\u0015\nMultiplying the top and bottom of the fraction in the second expectation by the state marginal\nπ(st) =\nR\na πt(st, at), and grouping terms we get:\n∂\n∂θL(θ) =\nT\nX\nt=0\nED\n\u0014 ∂\n∂θfθ(st, at)\n\u0015\n−Eµ\n\u0014 ˆpθ,t(st, at)\nˆµt(st, at)\n∂\n∂θfθ(st, at)\n\u0015\nWhere we have written ˆpθ,t(st, at) = exp{fθ(st, at)}πt(st), and ˆµ to denote a mixture between\nˆpθ(s, a) and policy samples.\nThis expression matches Eqn. 5, with fθ(s, a) serving as the reward function, when π maximizes\nthe policy objective so that ˆpθ(s, a) = pθ(s, a).\nA.3\nPOLICY OBJECTIVE\nNext, we show that the policy objective matches that of the sampler of Eqn. 6. The objective of the\npolicy is to maximize with respect to the reward ˆrt(s, a). First, note that:\nˆrt(s, a) = log(Dθ(s, a)) −log(1 −Dθ(s, a))\n= log\nefθ(s,a)\nefθ(s,a) + π(a|s) −log\nπ(a|s)\nefθ(s,a) + π(a|s)\n= fθ(s, a) −log π(a|s)\nThus, when ˆr(s, a) is summed over entire trajectories, we obtain the entropy-regularized policy\nobjective\nEπ\n\" T\nX\nt=0\nˆrt(st, at)\n#\n= Eπ\n\" T\nX\nt=0\nfθ(st, at) −log π(at|st)\n#\nWhere fθ serves as the reward function.\nA.4\nfθ(s, a) RECOVERS THE ADVANTAGE\nThe global minimum of the discriminator objective is achieved when π = πE, where π denotes the\nlearned policy (the ”generator” of a GAN) and πE denotes the policy under which demonstrations\nwere collected (Goodfellow et al., 2014). At this point, the output of the discriminator is 1\n2 for all\nvalues of s, a, meaning we have exp{fθ(s, a)} = πE(a|s), or f ∗(s, a) = log πE(a|s) = A∗(s, a).\n12\nPublished as a conference paper at ICLR 2018\nB\nSTATE-ONLY INVERSE REINFORCEMENT LEARNING\nIn this section we include proofs for Theorems 5.1 and 5.2, and the condition on the dynamics\nnecessary for them to hold.\nDeﬁnition B.1 (Decomposability Condition). Two states s1, s2 are deﬁned as ”1-step linked” under\na dynamics or transition distribution T(s′|a, s) if there exists a state s that can reach s1 and s2 with\npositive probability in one time step. Also, we deﬁne that this relationship can transfer through\ntransitivity: if s1 and s2 are linked, and s2 and s3 are linked, then we also consider s1 and s3 to be\nlinked.\nA transition distribution T satisﬁes the decomposability condition if all states in the MDP are linked\nwith all other states.\nThe key reason for needing this condition is that it allows us to decompose the functions state\ndependent f(s) and next state dependent g(s′) from their sum f(s) + g(s′), as stated below:\nLemma B.1. Suppose the dynamics for an MDP satisfy the decomposability condition. Then, for\nfunctions a(s), b(s), c(s), d(s), if for all s, s′:\na(s) + b(s′) = c(s) + d(s′)\nThen for for all s,\na(s) = c(s) + const\nb(s) = d(s) + const\nProof. Rearranging, we have:\na(s) −c(s) = b(s′) −d(s′)\nLet us rewrite f(s) = a(s)−c(s). This means we have f(s) = b(s′)−d(s′) for some function only\ndependent on s. In order for this to be representable, the term b(s′) −d(s′) must be equal for all\nsuccessor states s′ from s. Under the decomposability condition, all successor states must therefore\nbe equal in this manner through transitivity, meaning we have b(s′) −d(s′) must be constant with\nrespect to s. Therefore, a(s) = c(s) + const. We can then substitute this expression back in to the\noriginal equation to derive b(s) = d(s) + const.\nWe consider the case when the ground truth reward is state-only. We now show that if the learned re-\nward is also state-only, then we guarantee learning disentangled rewards, and vice-versa (sufﬁciency\nand necessity).\nTheorem 5.1. Let r(s) be a ground-truth reward, and T be a dynamics model satisfying the de-\ncomposability condition. Suppose IRL recovers a state-only reward r′(s) such that it produces an\noptimal policy in T:\nQ∗\nr′,T (s, a) = Q∗\nr,T (s, a) −f(s)\nThen, r′(s) is disentangled with respect to all dynamics.\nProof. We show that r′(s) must equal the ground-truth reward up to constants (modifying rewards\nby constants does not change the optimal policy).\nLet r′(s) = r(s) + φ(s) for some arbitrary function of state φ(s). We have:\nQ∗\nr(s, a) = r(s) + γEs′[softmax\na′\nQ∗\nr(s′, a′)]\nQ∗\nr(s, a) −f(s) = r(s) −f(s) + γEs′[softmax\na′\nQ∗\nr(s′, a′)]\nQ∗\nr(s, a) −f(s) = r(s) + γEs′[f(s′)] −f(s) + γEs′[softmax\na′\nQ∗\nr(s′, a′) −f(s′)]\nQ∗\nr′(s, a) = r(s) + γEs′[f(s′)] −f(s) + γEs′[softmax\na′\nQ∗\nr′(s′, a′)]\nFrom here, we see that:\nr′(s) = r(s) + γEs′[f(s′)] −f(s)\n13\nPublished as a conference paper at ICLR 2018\nMeaning we must have for all s, a:\nφ(s) = γEs′[f(s′)] −f(s)\nThis places the requirement that all successor states from s must have the same potential f(s).\nUnder the decomposability condition, every state in the MDP can be linked with such an equality\nstatement, meaning that f(s) is constant. Thus, r′(s) = r(s) + const.\nTheorem 5.2. If a reward function r′(s, a, s′) is disentangled for all dynamics functions, then it\nmust be state-only. i.e. If for all dynamics T,\nQ∗\nr,T (s, a) = Q∗\nr′,T (s, a) + f(s) ∀s, a\nThen r′ is only a function of state.\nProof. We show the converse, namely that if r′(s, a, s′) can depend on a or s′, then there exists\na dynamics model T such that the optimal policy is changed, i.e. Q∗\nr,T (s, a) ̸= Q∗\nr′,T (s, a) +\nf(s) ∀s, a.\nConsider the following 3-state MDP with deterministic dynamics and starting state S:\nS\nA\nB\na, 0\nb, 0\ns, +1\ns, -1\nWe denote the action with a small letter, i.e. taking the action a from S brings the agent to state A,\nreceiving a reward of 0. For simplicity, assume the discount factor γ = 1. The optimal policy here\ntakes the a action, returns to s, and repeat for inﬁnite positive reward. An action-dependent reward\nwhich induces the same optimal policy would be to move the reward from the action returning to s\nto the action going to a or s:\nr′(s, a) =\nState\nAction\nReward\nS\na\n+1\nS\nb\n-1\nA\ns\n0\nB\ns\n0\nThis corresponds to the shaping potential φ(S) = 0, φ(A) = 1, φ(B) = −1.\nNow suppose we modify the dynamics such that action a leads to B and action b leads to A:\nS\nA\nB\nb, 0\na, 0\ns, +1\ns, -1\nOptimizing r′ on this new MDP results in a different policy than optimizing r, as the agent visits B,\nresulting in inﬁnite negative reward.\nC\nAIRL RECOVERS REWARDS UP TO CONSTANTS\nIn this section, we prove that AIRL can recover the ground truth reward up to constants if the ground\ntruth is only a function of state r(s). For simplicity, we consider deterministic environments, so that\ns′ is uniquely deﬁned by s, a, and we restrict AIRL’s reward estimator g to only be a function of\nstate.\n14\nPublished as a conference paper at ICLR 2018\nTheorem C.1. Suppose we use AIRL with a discriminator of the form\nf(s, a, s′) = gθ(s) + γhφ(s′) −hφ(s)\nWe also assume we have deterministic dynamics, in addition to the decomposability condition on\nthe dynamics from Thm 5.1.\nThen if AIRL recovers the optimal f ∗(s, a, s′), we have\ng∗\nθ(s) = r(s) + const\nh∗\nφ(s) = V ∗(s) + const\nProof. From Appendix A.4, we have f ∗(s, a, s′) = A∗(s, a), so f ∗(s, a, s′) = Q∗(s, a) −V ∗(s) =\nr(s) + γV ∗(s′) −V ∗(s).\nSubstituting the form of f, we have for all s, s′:\ng∗(s) + γh∗(s′) −h∗(s) = r(s) + γV ∗(s′) −V ∗(s)\nApplying Lemma B.1 with a(s) = g∗(s) −h∗(s), b(s′) = γh∗(s′), c(s) = r(s) −V ∗(s), and\nd(s′) = γV ∗(s′) we have the result.\nD\nEXPERIMENT DETAILS\nIn this section we detail hyperparameters and training procedures used for our experiments. These\nhyperparameters were selected via a grid search.\nD.1\nNETWORK ARCHITECTURES\nFor the tabular MDP environment, we also use a tabular representation for function approximators.\nFor continuous control experiments, we use a two-layer ReLU network with 32 units for the discrim-\ninator of GAIL and GAN-GCL. For AIRL, we use a linear function approximator for the reward term\ng and a 2-layer ReLU network for the shaping term h. For the policy, we use a two-layer (32 units)\nReLU gaussian policy.\nD.2\nOTHER HYPERPARAMETERS\nEntropy regularization: We use an entropy regularizer weight of 0.1 for Ant, Swimmer, and\nHalfCheetah across all methods. We use an entropy regularizer weight of 1.0 on the point mass\nenvironment.\nTRPO Batch Size: For Ant, Swimmer and HalfCheetah environments, we use a batch size of 10000\nsteps per TRPO update. For pendulum, we use a batch size of 2000.\nD.3\nOTHER TRAINING DETAILS\nIRL methods commonly learn rewards which explain behavior locally for the current policy, because\nthe reward can ”forget” the signal that it gave to an earlier policy. This makes rewards obtained at\nthe end of training difﬁcult to optimize from scratch, as they overﬁt to samples from the current\niteration. To somewhat migitate this effect, we mix policy samples from the previous 20 iterations\nof training as negatives when training the discriminator. We use this strategy for both AIRL and\nGAN-GCL.\n15\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-10-30",
  "updated": "2018-08-13"
}