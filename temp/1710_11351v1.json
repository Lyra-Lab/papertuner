{
  "id": "http://arxiv.org/abs/1710.11351v1",
  "title": "ChainerMN: Scalable Distributed Deep Learning Framework",
  "authors": [
    "Takuya Akiba",
    "Keisuke Fukuda",
    "Shuji Suzuki"
  ],
  "abstract": "One of the keys for deep learning to have made a breakthrough in various\nfields was to utilize high computing powers centering around GPUs. Enabling the\nuse of further computing abilities by distributed processing is essential not\nonly to make the deep learning bigger and faster but also to tackle unsolved\nchallenges. We present the design, implementation, and evaluation of ChainerMN,\nthe distributed deep learning framework we have developed. We demonstrate that\nChainerMN can scale the learning process of the ResNet-50 model to the ImageNet\ndataset up to 128 GPUs with the parallel efficiency of 90%.",
  "text": "ChainerMN: Scalable Distributed Deep Learning\nFramework ∗\nTakuya Akiba\nPreferred Networks, Inc.\nakiba@preferred.jp\nKeisuke Fukuda\nPreferred Networks, Inc.\nkfukuda@preferred.jp\nShuji Suzuki\nPreferred Networks, Inc.\nssuzuki@preferred.jp\nAbstract\nOne of the keys for deep learning to have made a breakthrough in various ﬁelds\nwas to utilize high computing powers centering around GPUs. Enabling the use of\nfurther computing abilities by distributed processing is essential not only to make\nthe deep learning bigger and faster but also to tackle unsolved challenges. We\npresent the design, implementation, and evaluation of ChainerMN, the distributed\ndeep learning framework we have developed. We demonstrate that ChainerMN can\nscale the learning process of the ResNet-50 model to the ImageNet dataset up to\n128 GPUs with the parallel efﬁciency of 90%.\n1\nIntroduction\nIt has turned out that deep learning achieves far better predicting performance than existing methods\nin image recognition, natural language processing, speech recognition and many other ﬁelds where\nmachine learning is being applied. The basic technology of neural networks used in deep learning\nhas a long history dating back to the 1950’s. As we entered the 2010’s, the neural network technology\nwith its long history has made the breakthrough as “deep learning” as described above because it\nis thought to have successfully combined all the advances of algorithms, large-scale data, and high\ncomputing powers. Even today, it would be difﬁcult to achieve an outstanding predicting performance\nby deep learning if one of the three lacks. In this article, we focus on one of the three pillars supporting\ndeep learning: computing performance.\nIt has become a standard approach to use highly efﬁcient GPUs for training in many deep learning\ntasks. Nevertheless, the training process is still time-consuming even with the latest GPUs because\nmodels have also grown massive and complex. For example, training Resnet-50 [6] for the ImageNet\ndataset [4] typically takes as long as one week with a single GPU. Taking a long time on training\nmeans you have a limited number of times to do trial and error for models and parameters needed to\nachieve high accuracy, making it difﬁcult to produce a good predicting performance. It also means\nthere is a limit to the usable data size. Thus, using multiple GPUs in parallel is crucial in accelerating\ncalculation.\nWe introduce ChainerMN, an add-on package to Chainer [9], a programming framework for deep\nlearning applications written in Python, to provide a distributed learning capability. In the course of\ndeveloping ChainerMN, we took the following features into consideration:\n• Flexibility: Chainer is a ﬂexible framework based on its Deﬁne-by-Run approach and\nChainerMN is designed not to ruin the ﬂexibility aspect. This allows for easy distributed\nlearning even in complex use cases such as dynamic neural networks, generative adversarial\nnetworks, and reinforced deep learning.\n∗This paper is based on our invited article in TSUBAME e-Science Journal (ESJ)[3]. ESJ is a non-academic\nand non-peer-reviewed newsletter from Global Scientiﬁc Information Center, Tokyo Institute of Technology.\narXiv:1710.11351v1  [cs.DC]  31 Oct 2017\n• High performance: We selected technologies assuming practical workloads in deep learn-\ning from the very beginning of designing ChainerMN as well as exercised ingenuity with\nrespect to implementation so that hardware performance is fully utilized.\nThe rest of the paper is organized as follows. First, we explain the basic elements of distributed deep\nlearning, followed by the design and implementation of ChainerMN. Finally, we will present the\nresults of our evaluation experiment and related work.\n2\nPreliminaries\n2.1\nBasics of Deep Learning\nWe can express the prediction by neural networks against input data x as f(x; θ) where θ is a\nparameter for neural networks. Learning in neural networks using backpropagation and stochastic\ngradient descent or its variations is an iterative algorithm. Each iteration is composed of the following\nthree steps: forward computation, backward computation, and optimization.\nIn the forward-computation step, ﬁrst, the prediction f(x; θ) is calculated against an input data point\nx. Then, the loss is calculated to represent the difference from the correct output for. Here, the cross\nentropy and other indicators may be used.\nIn the backward-computation step, g = δE\nδθ , the gradient of the parameter θ in the direction of\ndecreasing the loss E, is calculated. Gradients for all parameters are calculated using the chain rule\nwhile going backward from the output layer to the input layer.\nIn the optimize step, the parameter θ is updated using the gradient g . The simplest rule is to update θ\nto θ −µg where µ is a parameter called a learning rate.\nIn practice, instead of using a single training example in an iteration, the forward and backward\ncalculations are performed simultaneously against multiple training examples and optimization is\nexecuted using the average of gradients against all the examples. The input examples used in an\niteration is called a minibatch while its size is called a batch size. A typical batch size ranges from\nseveral tens to several hundred.\nPlease note that the above description is based on a standard supervised learning. Nonetheless, in\ncase that neural networks are applied to other algorithms such as unsupervised learning and semi-\nsupervised learning, the parallelizing method we will explain below is applicable and ChainerMN is\nalso usable.\n2.2\nData Parallel and Model Parallel approaches\nThere are two major approaches to parallelize training by distributed processing: data parallel and\nmodel parallel. In the data-parallel approach, each worker has a model replica and calculate gradients\nof different minibatches. Workers use these gradients to update the model collaboratively. In the\nmodel parallel approach, each worker has a portion of the model and work in cooperation with others\nto do the calculation for one minibatch. Figure 1 shows the difference between the two approaches.\n(a) Data parallelism.\n(b) Model parallelism.\nFigure 1: Data parallel and model parallel ap-\nproaches.\nAllreduce\nForward\nForward\nForward\nBackward\nBackward\nBackward\nOptimize\nOptimize\nOptimize\nFigure 2: The four steps that constitute an iteration\nof synchronous data-parallel approach.\n2\nThe model-parallel approach was actively used in the days when GPU memory was small. At present,\nthe model parallel is rarely used in its basic form as the data parallel approach is being used. In the\nmeantime, some issues with the data paralleled approach have surfaced while research on a new form\nof the model parallel is underway. The model parallel and the data parallel can be used at the same\ntime as well.\n2.3\nSynchronous vs. Asynchronous\nIn this subsection, we will focus on the data-parallel approach which is commonly used now. The\ndata-parallel approach is roughly divided into synchronous and asynchronous types, and we explain\nabout the former ﬁrst. Each iteration in synchronous, data-parallel deep learning is composed of the\nfollowing four steps: forward computation, backward computation, Allreduce communication, and\noptimization. Figure 2 illustrates the four steps.\nThis has an additional step Allreduce to the regular iteration described earlier. In this step, workers\ncommunicate with each other to ﬁnd the average of gradients calculated by individual workers and\ndistribute the average. All workers update the model using the gradient they have obtained through\nthe communication. If we deﬁne the batch size processed by each worker as b and the number of\nworkers as n, the gradient obtained through communication is equivalent to the gradient in the batch\nsize bn. This means gradients are calculated using more training data in one iteration, improving the\ngradient quality and accelerating the learning process.\nAsynchronous type, on the other hand, uses special workers called a parameter server. The parameter\nserver controls model parameters. Normal workers send gradients to the parameter server once the\ngradients are obtained by forward and backward calculations. The parameter server receives and uses\nthe gradients to update the model. Workers receive new model parameters and calculate gradients\nagain.\n3\nDesign and Implementation\n3.1\nParallelization Approaches\nWe discuss the design decision of ChainerMN in this section. As we discussed in section 2, there are\ntwo major parallelization approaches and two synchronization approaches. We adopt a synchronous\nand data parallel approach for ChainerMN.\nWe use the data parallel approach because existing deep learning applications would easily be\nextensible and faster training process through data parallel was highly expected. Data parallelization\nis tantamount to increasing a minibatch size in a typical deep learning application and has its\nadvantage of being applicable without having to make signiﬁcant changes in algorithms and codes of\nexisting programs.\nWhether the synchronous or asynchronous type is desirable is also a nontrivial question since different\nkinds of strategies have been taken in each implementation and results would vary depending on\ntasks or settings. The paper [8] shows experimental results that the asynchronous type is less stable\nregarding convergence whereas it is faster to converge in the synchronization. Also, we can beneﬁt\nfrom the optimized and proven group communication mechanism of MPI, the de-facto standard\ncommunication library interface, while in the asynchronous model the implementation scheme uses a\nparameter server in general.\n3.2\nChainer\nChainer is a framework with its Deﬁne-by-Run feature. Deﬁne-by-Run is a model that takes advantage\nof the ﬂexibility of script languages where learning models and computational ﬂows are deﬁned at\nruntime. A Deﬁne-and-Run approach, on the other hand, is a model that pre-deﬁnes a structure of\nnetworks, after which data is input and calculation are done. While potentially easier to optimize\nperformance, this approach is said to lack ﬂexibility.\nChainer provides programming models that enable to deﬁne complex neural networks ﬂexibly or\nmake modiﬁcations during runtime thanks to its Deﬁne-by-Run approach. This lets researchers and\nengineers work on new models or complex models through trial and error with ease and therefore\n3\nListing 1: Example of ChainerMN\n1\nmodel = L.Classiﬁer(MLP(args.unit, 10))\n2\n3\n# Create a communicator\n4\ncomm = chainermn.create_communicator()\n5\n6\n# Distribute a dataset\n7\ntrain = chainermn.scatter_dataset(train, comm, shufﬂe=True)\n8\n9\n# Create and use multi_node_optimizer\n10\noptimizer = chainermn.create_multi_node_optimizer(\n11\nchainer.optimizers.Adam(), comm)\n12\noptimizer.setup(model)\n13\n14\n# Use Chainer’s Trainer class to simplify\n15\n# a forward−backward−optimization loop\n16\ntrain_iter = chainer.iterators.SerialIterator(train, args.batchsize)\n17\nupdater = training.StandardUpdater(train_iter, optimizer, device=device)\n18\ntrainer = training.Trainer(updater, (args.epoch, ’epoch’), out=args.out)\nis suitable for research and development of machine learning. Upon development, we carefully\ndesigned the ChainerMN API with the objective of making it easily portable from existing Chainer\nprograms without putting limitations on the ﬂexibility of Chainer.\n3.3\nAPI Design\nWe describe the design of library interface of ChainerMN by describing minimal steps to extend an\nexisting deep learning program written in Chainer to support distributed execution using ChainerMN.\nListing 1 shows a simpliﬁed ChainerMN program of a model to solve MNIST classiﬁcation prob-\nlem [7]. For a complete program code, refer to ChainerMN’s code repository [1]. There are three\nmajor steps: (1) add a communicator component, (2) create and use mutli_node_optimizer, and\n(3) add code to distribute a dataset.\nA process of modifying an application starts from adding a communication component called\nCommunicator to existing Chainer programs. A communicator is a central component of ChainerMN,\nand it is designed after MPI’s communicator concept and controls all inter-process communication in\nChainerMN program.\nmutli_node_optimizer is the most important component in ChainerMN. It wraps Chainer’s normal\noptimizer and exchanges the gradient across processes using Allreduce operation before optimizing\nthe model. multi_node_optimizer behaves identically as the original optimizer except for the\ncommunication, so the extension is seamlessly integrated into Chainer’s existing Trainer ecosystem.\nOn top of this, basic porting can be done just by adding the scattering step which distributes data\nfor data parallel computations. One needs to split the dataset into equal chunks and distribute them\nover the processes. This operation is also known as Scatter in MPI. Other parts, i.e.Iterator,\nUpdater, and Evaluator do not need to be changed in basic use cases. Because of this API design,\nit allows various Chainer programs to be ported with minimal modiﬁcations while making the most\nof the advantage given by Deﬁne-by-Run.\n3.4\nImplementation and Performance Optimization\nThe communication pattern of synchronous and data parallel deep learning applications is relatively\nsimple from the point of view of HPC applications. Roughly speaking, the only major communication\nis Allreduce, a process to exchange gradients which are training and evaluation results. Auxiliary\nparts include Scatter, which arranges necessary data over distributed processes before starting\ntraining.\nAs mentioned above, one of the design goals of ChainerMN is to achieve high performance by\nleveraging existing and proven HPC technologies. Allreduce is a component that especially\nrequires speed because it is called in every training iteration and needs to process a large amount\nof data. We attempt to minimize the communication time by using NCCL [2] library developed by\n4\nNVIDIA. NCCL is a highly-optimized communication library which provides a faster Allreduce\noperation between NVIDIA GPUs within and across nodes.\n4\nEvaluation\n4.1\nExperimental Environment and Settings\nWe conducted our experiments on our in-house cluster. It consists of 32 computing nodes. Each node\nis equipped with two Intel Xeon CPUs (E5-2623 v3, 3.00 GHz, four cores for each), 128 GB of main\nmemory, and four GeForce GTX TITAN X GPUs. Thus, we used 128 GPUs in total. The nodes\nare interconnected by Mellanox Inﬁniband FDR 4X. We used CUDA version 8, Python version 3.5,\nMvapich2 2.2 and Chainer version 1.2 running on Ubuntu 14.04 LTS.\nTo demonstrate the performance and scalability of ChainerMN, we use ResNet-50 [6] model and\nImageNet [4] dataset. Since the dataset is large and the majority part of access is read, we copied all\nthe dataset to all computing nodes’ local SSD in advance.\nWe used 32 as the batch size per GPU, which means 4096 for 128 GPUs. One of the factors making\ndistributed deep learning difﬁcult is that improving throughput does not necessarily mean better\nlearning efﬁciency. We note that the batch size 4096 is a healthy setting where the learning efﬁciency\nand the resulting model accuracy are maintained, as shown by Goyal et al. [5]\n4.2\nScalability Result\nFigure 3 shows the scalability of ChainerMN up to 128 GPUs. In this ﬁgure, ChainerMN scales\nwell up to 128 GPUs. Table 1 shows the relative runtimes over one-GPU execution. In this table,\nChainerMN on 128 GPUs achieves 79 % and 90 % parallelization efﬁciency of the one-GPU and one-\nnode (four GPUs) executions, respectively. It means that the parallelization efﬁciency of ChainerMN\non 128 GPUs is as high as the state-of-the-art [5].\nFigure 3: Scalability of ChainerMN\nTable 1: Relative speed-up and parallelization efﬁciency\n#GPUs\nSpeed-up\nPar. Eff.\n1\n1.00\n100.00%\n2\n1.85\n92.66%\n4\n3.53\n88.34%\n8\n7.09\n88.67%\n16\n13.42\n83.88%\n32\n26.63\n83.22%\n64\n50.52\n78.94%\n128\n101.32\n79.16%\n5\nConclusions\nWe have described the design and implementation of ChainerMN and demonstrated its scalability.\nChainer and ChainerMN are designed to have both high ﬂexibility and scalability with its primary\nobject of accelerating research and development in deep learning. We will continue making improve-\nments by tackling challenges such as model parallel, overlapping communication and computation,\nasynchronous computation among workers, optimized communication by compressed gradients, and\nfault tolerance.\nAcknowledgements\nThe authors thank K. Ueno, T. Mitsuishi, and N. Yoshifuji for help on the development of ChainerMN. We\nalso thank T. Sudo, Y. Doi, G. Watanabe, R. Okuta, and M. Sakata for help for experiments. We are grateful to\nT. Miyato and S. Tokui for fruitful discussions as well.\n5\nReferences\n[1] ChainerMN. https://github.com/chainer/chainermn, 2017.\n[2] NVIDIA Collective Communications Library (NCCL). https://developer.nvidia.com/nccl, 2017.\n[3] TSUBAME e-Science Journal. http://www.gsic.titech.ac.jp/TSUBAME_ESJ, 11 2017.\n[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR09, 2009.\n[5] P. Goyal, P. Dollár, R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He.\nAccurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677, 2017.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages\n770–778, 2016.\n[7] Y. Lecun and C. Cortes. The MNIST database of handwritten digits.\n[8] X. Pan, J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. Revisiting distributed synchronous sgd. ICLR\nWorkshop Track, 2016, 02 2017.\n[9] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a next-generation open source framework for deep\nlearning. In LearningSys, 2015.\n6\n",
  "categories": [
    "cs.DC",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2017-10-31",
  "updated": "2017-10-31"
}