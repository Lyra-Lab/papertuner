{
  "id": "http://arxiv.org/abs/1711.00247v1",
  "title": "Improved Text Language Identification for the South African Languages",
  "authors": [
    "Bernardt Duvenhage",
    "Mfundo Ntini",
    "Phala Ramonyai"
  ],
  "abstract": "Virtual assistants and text chatbots have recently been gaining popularity.\nGiven the short message nature of text-based chat interactions, the language\nidentification systems of these bots might only have 15 or 20 characters to\nmake a prediction. However, accurate text language identification is important,\nespecially in the early stages of many multilingual natural language processing\npipelines.\n  This paper investigates the use of a naive Bayes classifier, to accurately\npredict the language family that a piece of text belongs to, combined with a\nlexicon based classifier to distinguish the specific South African language\nthat the text is written in. This approach leads to a 31% reduction in the\nlanguage detection error.\n  In the spirit of reproducible research the training and testing datasets as\nwell as the code are published on github. Hopefully it will be useful to create\na text language identification shared task for South African languages.",
  "text": "Improved Text Language Identiﬁcation for the\nSouth African Languages\nBernardt Duvenhage\nbernardt@feersum.io\nFeersum Engine\nPraekelt Consulting\nJohannesburg, South Africa\nMfundo Ntini\nmfundo@praekelt.com\nEngineering Team\nPraekelt Consulting\nJohannesburg, South Africa\nPhala Ramonyai\nphala@praekelt.com\nEngineering Team\nPraekelt Consulting\nJohannesburg, South Africa\nAbstract—Virtual assistants and text chatbots have recently\nbeen gaining popularity. Given the short message nature of\ntext-based chat interactions, the language identiﬁcation systems\nof these bots might only have 15 or 20 characters to make\na prediction. However, accurate text language identiﬁcation is\nimportant, especially in the early stages of many multilingual\nnatural language processing pipelines.\nThis paper investigates the use of a naive Bayes classiﬁer, to\naccurately predict the language family that a piece of text belongs\nto, combined with a lexicon based classiﬁer to distinguish the\nspeciﬁc South African language that the text is written in. This\napproach leads to a 31% reduction in the language detection\nerror.\nIn the spirit of reproducible research the training and testing\ndatasets as well as the code are published on github. Hopefully\nit will be useful to create a text language identiﬁcation shared\ntask for South African languages.\nIndex Terms—Naive Bayesian text classiﬁcation, lexicon based\ntext classiﬁcation, text language identiﬁcation\nI. INTRODUCTION\nVirtual assistants and text chatbots seem to be gaining\nmuch popularity, but to be accessible to South Africans\nthese software agents need to understand our local languages.\nSouth Africa has 11 ofﬁcial languages belonging to a couple\ndifferent language families. Afrikaans (afr) and English (eng)\nare Germanic languages. isiNdebele (nbl), isiXhosa (xho),\nisiZulu (zul) and siSwati (ssw) belong to the Nguni family\nof languages. Sepedi (nso), Sesotho (sot) and Setswana (tsn)\nbelong to the Sotho-Tswana family of languages. Finally, Xit-\nsonga (tso) belong to the Tswa-Ronga family and Tshivenda\n(ven) belong to the Venda family. Many of these languages are\nunder-resourced and further work is required to build software\nagents that are ﬂuent in the country’s rich vernacular.\nText language identiﬁcation (LID) is an important early\nstep in many multi-lingual natural language processing (NLP)\npipelines because many of the later steps are still language\ndependent. Given the short message nature of text based chat\ninteractions and the possibility of code switching the language\nidentiﬁcation system might only have 15 or 20 characters\nto make a prediction. However, lower LID accuracies may\nbe expected for short text due to fewer text features being\navailable during classiﬁcation. Any errors that occur early in\nan NLP pipeline are also potentially compounded by later\nprocessing steps.\nThis paper gives an overview of the related LID literature\nin Section II, a discussion of the chosen baseline classiﬁer\nin Section III followed by the discussion of the paper’s\ncontribution to the improvement of LID on short pieces of text\nin Section IV. Comparative results are presented in Section V\nfollowed by some concluding remarks and suggested future\nwork in Section VI.\nA further contribution of this work is that the training\nand testing datasets as well as the code are published on\ngithub. Hopefully it will be useful to create a text language\nidentiﬁcation shared task for South African languages.\nII. RELATED WORKS\nAn LID system for long texts based on normalised his-\ntograms of character n-grams is presented in [1]. A similar\nsystem that also successfully used character n-grams for doing\nLID of long texts is presented in [2]\nA frequency based n-gram difference based classiﬁer and\na support vector machine (SVM) that uses the n-gram fre-\nquencies as features are discussed in [3]. Error rates of\napproximately 0.3% are achieved over large text window sizes.\nIt is also found that the SVM’s performance is better than the\nn-gram based estimator’s, but at a much greater computational\ncost.\nIn [4] a spell-checker from the South African Centre for\nText Technology (CTexT) is applied to do LID. A sentence\nlevel accuracy of 97.9% is achieved on texts of approximately\n400 characters in length.\nA naive Bayes classiﬁer with various character n-gram text\nfeatures, called langid, is discussed in [5]. In the current paper\nlangid is also trained on the South African languages and used\nas an LID reference in Section V.\nA difference in n-gram frequencies classiﬁer, a naive Bayes\ntext classiﬁer with n-gram features and an SVM are evaluated\nfor LID in [6]. The Bayesian classiﬁer is reported to be\nthe most accurate in practise at 17% error on texts of 15\ncharacters.\nAn SVM and a naive Bayes classiﬁer for language identi-\nﬁcation of individual words are compared in [7]. The system\nwas trained to identify afr, eng, sot and zul which, except for\nafr and eng, are all from different South African language\narXiv:1711.00247v1  [cs.CL]  1 Nov 2017\nAvrg. accuracy = 1.0, F-Score = 1.0\nGermanic\nNguni\nSotho–Tswana\nTswa–\nRonga\nVenda\n240 characters\nafr\neng\nzul\nxho\nssw\nnbl\nnso\nsot\ntsn\ntso\nven\nafr\n1.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\neng\n0.000\n1.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nzul\n0.000\n0.000\n1.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nxho\n0.000\n0.000\n0.000\n1.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nssw\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nnbl\n0.000\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\n0.000\n0.000\n0.000\n0.000\nnso\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\n0.000\n0.000\n0.000\nsot\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\n0.000\n0.000\ntsn\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\n0.000\ntso\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\nven\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1.000\n0.1\n0.3\n0.5\n0.7\n0.8\n0.9\nFig. 1. Confusion matrix of the baseline classiﬁer and a test set with strings of length 200-300 characters.\nfamilies. Accuracies of around 85% - 95% on single 10 - 15\ncharacter words are reported.\nIn [8] joint sequence models are used to further improve\nthe accuracy of LID of single words 10 - 15 characters long.\nAccuracies of around 97.2% are reported when labelling text\nas afr, eng, sot or zul. The training data used is from the\nNational Centre for Human language Technology’s language\ndictionary word lists.\nIn [9] Char2Vec and an LSTM are used to do end-to-end\ntrained LID. Char2Vec is used to get word embeddings which\nare then combined via an LSTM. Once trained the LSTM is\nable to predict a language for each word in the sentence. The\nsigniﬁcant text features are automatically learned and text pre-\nprocessing and cleanup is not required. Near state-of-the-art\nperformance is reported on code switching LID shared tasks.\nRecently a lexicon based LID [10] was applied to under-\nresourced languages. It is not clear what the character length\nof the training and testing samples were, but the reported LID\naccuracies are in the low 90’s.\nIII. LID BASELINE FOR SOUTH AFRICAN LANGUAGES\nUSING CHARACTER N-GRAMS\nThe use of a naive Bayes classiﬁer with character n-gram\ntext features has become the standard for sentence level\ntext LID [6]. The baseline used in this paper is sklearn’s\nmultinomial naive Bayes classiﬁer with character 5-grams.\nThe data used in this paper is the NCHLT Text Corpora [11]\n[12]. The NCHLT Text Corpora data was cleaned up a bit by\nreplacing numbers and all punctuation except ’-’ with spaces.\nAll other characters such as ˇs were left unmodiﬁed.\n3000 training samples and 1000 test samples per language\nwere randomly chosen from the subset of full sentences in\nthe CText data that are 200-300 characters long. Using more\ntraining data doesn’t signiﬁcantly improve the LID accuracy\nfor long sentences as shown later in Figure 3 in the results\nsection. Binary text features were used as opposed to integer\nfeature counts. Later in the paper a classiﬁer trained on all\n4000 of these long sentences are reused for classiﬁcation of\nshort sentences.\nInitially the trained classiﬁer had an accuracy and F-score\nof 99.5%. However, some of the mis-predicted sentences\nwere spotted to be mislabelled in the NCHLT data. The\nmis-predicted sentences were few enough to all be checked\nmanually and were indeed found to be mislabelled in the\nCTexT data. Approximately 0.468% of the data was correctly\nrelabeled in this manner. The updated datasets are hosted\nwith the LID code on github at https://github.com/praekelt/\nfeersum-lid-shared-task.\nAfter cleaning up the data the trained classiﬁer had an\naccuracy and F-score of 99.9909% ≈100.0%. This baseline\nclassiﬁer already outperforms previous work [6] on long\nsentences. 1 Figure 1 shows the confusion matrix for the test\nset.\nThe Google Translate API was also used to verify the results\nof the n-gram classiﬁer for the languages it understands (i.e.\nafr, eng, sot, xho and zul). The Google results correlated with\nall predictions except for some differences between isiXhosa\nand isiZulu - which belong to the same language family.\nApproximately 0.09% of the Google results differed from\nthe baseline results, but again all of these could be checked\nmanually and were found to have been incorrectly labelled by\n1The authors of the earlier work didn’t attribute their data to CTEXT, but\nit is curious that they achieved the same accuracy as we did before the data\ncleanup.\nAvrg. accuracy = 0.930, F-Score = 0.929\nGermanic\nNguni\nSotho–Tswana\nTswa–\nRonga\nVenda\n15 characters\nafr\neng\nzul\nxho\nssw\nnbl\nnso\nsot\ntsn\ntso\nven\nafr\n0.981\n0.008\n0.000\n0.002\n0.000\n0.000\n0.000\n0.002\n0.007\n0.000\n0.000\neng\n0.004\n0.987\n0.003\n0.000\n0.000\n0.000\n0.001\n0.002\n0.001\n0.002\n0.000\nzul\n0.000\n0.008\n0.808\n0.052\n0.044\n0.083\n0.001\n0.001\n0.001\n0.001\n0.001\nxho\n0.000\n0.003\n0.057\n0.894\n0.011\n0.033\n0.000\n0.000\n0.000\n0.000\n0.002\nssw\n0.000\n0.002\n0.014\n0.001\n0.973\n0.010\n0.000\n0.000\n0.000\n0.000\n0.000\nnbl\n0.000\n0.002\n0.052\n0.016\n0.022\n0.907\n0.000\n0.000\n0.000\n0.001\n0.000\nnso\n0.001\n0.005\n0.002\n0.000\n0.000\n0.001\n0.873\n0.046\n0.071\n0.001\n0.000\nsot\n0.000\n0.003\n0.000\n0.000\n0.000\n0.000\n0.019\n0.924\n0.052\n0.001\n0.001\ntsn\n0.004\n0.003\n0.000\n0.000\n0.000\n0.000\n0.064\n0.030\n0.898\n0.000\n0.001\ntso\n0.000\n0.004\n0.000\n0.000\n0.001\n0.001\n0.000\n0.000\n0.000\n0.991\n0.003\nven\n0.000\n0.002\n0.002\n0.001\n0.000\n0.000\n0.001\n0.000\n0.000\n0.002\n0.992\n0.1\n0.3\n0.5\n0.7\n0.8\n0.9\nFig. 2. Confusion matrix of the baseline classiﬁer and a test set with strings of length 15 characters.\nGoogle Translate. A side effect of this validation is that one\ncan be assured that Google’s API is relatively accurate for the\nSouth African languages it supports.\nThe baseline model trains and tests in 90 minutes on a\nsingle core of a 3.30 GHz i5 CPU, uses below 2GB of RAM\nduring training and the trained model is approximately 50MB\nin size. Long sentence language detection therefore seems to\nbe a solved problem for at least the 11 ofﬁcial South African\nlanguages and given that the training data is from a similar\ndomain as one’s production environment. Others [7] have also\nnoted that one easily achieves 100% LID accuracy given 300\ncharacters of text.\nIV. LID OF SHORT STRINGS\nShort sentence data was derived from the cleaned dataset\nby selecting from the start of the long sentences 200, 100, 50,\n30 or 15 characters plus any characters required to not split a\nword. A potential problem with the alternative sliding window\napproach that others have used is that the fragmented start and\nend words affect the classiﬁer performance for short sentences\nand such an approach would also prohibit the use of a lexicon\nbased classiﬁcation algorithm.\nThe classiﬁer’s F-score on short pieces of text are shown\nin Figure 3 for training set sizes from 1000 to 4000 samples.\nThe datasets size prevents using more samples to train the\nbaseline classiﬁer, but from the graph it seems that the short\nsentence performance could beneﬁt from using more than 4000\ntraining samples. Although not the focus of this paper note that\nour baseline already outperforms earlier reported results [6] of\n1.5% on 100 char strings and 17% for 15 chars. The current\nbaseline achieves 0.1% error on 100 char strings and 7.0%\nerror for 15 chars.\nFig. 3.\nThe baseline classiﬁer’s F-score for shorter text fragments. The\ndifferent graphs show how the number of full sentence training samples\ninﬂuence the short sentence LID results.\nFigure 2 shows the confusion matrix for short 15 char\nstrings. As others have noted, there is some confusion between\nlanguages of the same family. This can be clearly seen from\nthe widening of the diagonal into the family blocks. Note also\nthe limited confusion between language families.\nFigure 4 shows the family confusion matrix for short\nsentences of 15 characters. The limited confusion between\nlanguage families should be clear. Also note that the average\naccuracy and F-score of classifying a short string into language\nfamilies is 99.2% while the average accuracy of classifying a\nshort string all the way into a language is only 93.0%\nAvrg. accuracy = 0.992, F-Score = 0.992\n15 characters\nGermanic Nguni\nSotho–\nTswana\nTswa–\nRonga\nVenda\nGermanic\n0.990\n0.003\n0.006\n0.001\n0.000\nNguni\n0.004\n0.994\n0.001\n0.001\n0.001\nSotho–Tswana\n0.005\n0.001\n0.992\n0.001\n0.001\nTswa–Ronga\n0.004\n0.002\n0.000\n0.991\n0.003\nVenda\n0.002\n0.003\n0.001\n0.002\n0.992\n0.1\n0.3\n0.5\n0.7\n0.8\n0.9\nFig. 4.\nConfusion matrix of language families of a test set with strings of\nlength 15 characters.\nThe baseline classiﬁer therefore performs very well (99.2%\naccurate) at classifying even short 15 character sentences into\ntheir language families. Such accurate classiﬁcation of the\nlanguage family is possibly good enough to enable a software\nagent to interpret and act on short sentences. However, in some\ncases one might want to identify the speciﬁc language that a\npiece of text is written in.\nA key realisation is that although Sesotho, Setswana and\nSepedi are strongly related, certain words might appear in one\nlanguage, but not in the others within the family. The same is\ntrue of isiZulu, siSwati, isiXhosa and isiNdebele. Therefore,\na second lexicon based classiﬁer may be useful to distinguish\nlanguages within the same language family.\nTo test this idea, a lexicon is created from all the sentences\nin the cleaned language corpuses (4.1k - 25k samples per\nlanguage). During language identiﬁcation the naive Bayes\nclassiﬁer result is used to classify the text as belonging to\na language family after which the language lexicons are used\nto count how many words of each language in the family is\npresent in the input. If one language in the family dominates\nthen it is chosen as the language label otherwise the naive\nBayes result is taken as the most informative and used as the\nlanguage label.\nV. MORE RESULTS AND ANALYSIS\nFigure 5 shows the comparative accuracies of langid 97,\nlangid za, Google Translate’s language detection API and\nthe naive Bayes baseline classiﬁers. langid 97 is the langid\nmodel included with the langid package that was trained on\n97 languages. langid 97 and Google translate’s detector are\npretrained and were tested on the full 4000 (training + testing)\ncleaned samples per language. The South African languages\nthat langid 97 supports are afr, eng, xho and zul. Google’s\ndetector additionally supports sot. For the pre-trained models\nonly the supported languages were included in the accuracy\nand F-score estimates. The other langid model, langid za,\nwe trained on the cleaned-up long (200-300 character) full\nsentences used in this paper. An ideal accuracy for language\nidentiﬁcation is above 99% so that less than one in a hundred\npredictions fail.\nThe proposed lexicon classiﬁer on its own achieves an\naccuracy of only 89.8% and an F-score of 89.7%. However,\nFig. 5. Accuracy of the baseline NB, Google Translate, langid ZA (trained on\ncleaned data) and langid 97 (built-in langid model trained on 97 languages).\nwhen used in combination with the baseline classiﬁer the\nlexicon classiﬁer’s result is only used when it responds with a\nhigh conﬁdence which results in an overall reduction in error.\nFigure 6 shows the updated confusion matrix for short 15 char\nstrings classiﬁed using the simple two stage classiﬁer. The\nnoise level in the results using the 1000 testing samples seem\nto be in the order of 0.001.\nThe resulting short sentence LID accuracy is 95.2% which\nis 31% reduction in LID error over the baseline classiﬁer. The\nfamily LID accuracy stays unchanged at 99.2% accuracy. As\nmentioned previously, earlier works by Botha and Barnard [6]\ndid a full language classiﬁcation, but only achieved a 17%\nerror (83% accuracy) for short sentences of 15 characters.\nGiwa and Davel [8] achieved what is essentially a language\nfamily LID accuracy of just over 97% for single words of 10\n- 15 characters long.\nVI. CONCLUSION\nA. Summary\nThe baseline naive Bayes classiﬁer is shown to be more\naccurate than Google Translate’s pre-trained language iden-\ntiﬁcation API, the pre-trained langid 97 and a langid model\ntrained on the cleaned data used in this paper. The baseline\nalso outperforms earlier reported results on South African\nlanguages as discussed in Section IV.\nAdding a lexicon to the baseline classiﬁer reduced LID error\nby 31%. The resulting short sentence LID accuracy is 95.2%.\nWhen compared to the previous reported result [6] of 83% the\ncurrent model reduced the error from 17% to 4.8% which is\na 3x reduction in error.\nThe improved dataset and code are hosted at https://github.\ncom/praekelt/feersum-lid-shared-task. It is possible that the\nprocess of shortening a sentence changes the certainty of its\nlanguage label when distinguishing words and other features\nare are lost. This would result in a performance ceiling for\nshort sentence LID.\nAvrg. accuracy = 0.952, F-Score = 0.952\nGermanic\nNguni\nSotho–Tswana\nTswa–\nRonga\nVenda\n15 characters\nafr\neng\nzul\nxho\nssw\nnbl\nnso\nsot\ntsn\ntso\nven\nafr\n0.981\n0.008\n0.000\n0.002\n0.000\n0.000\n0.000\n0.002\n0.007\n0.000\n0.000\neng\n0.004\n0.987\n0.003\n0.000\n0.000\n0.000\n0.001\n0.002\n0.001\n0.002\n0.000\nzul\n0.000\n0.008\n0.893\n0.026\n0.019\n0.049\n0.001\n0.001\n0.001\n0.001\n0.001\nxho\n0.000\n0.003\n0.035\n0.939\n0.002\n0.019\n0.000\n0.000\n0.000\n0.000\n0.002\nssw\n0.000\n0.002\n0.012\n0.000\n0.982\n0.004\n0.000\n0.000\n0.000\n0.000\n0.000\nnbl\n0.000\n0.002\n0.034\n0.010\n0.006\n0.947\n0.000\n0.000\n0.000\n0.001\n0.000\nnso\n0.001\n0.005\n0.001\n0.001\n0.000\n0.001\n0.911\n0.034\n0.045\n0.001\n0.000\nsot\n0.000\n0.003\n0.000\n0.000\n0.000\n0.000\n0.017\n0.937\n0.041\n0.001\n0.001\ntsn\n0.004\n0.003\n0.000\n0.000\n0.000\n0.000\n0.057\n0.026\n0.909\n0.000\n0.001\ntso\n0.000\n0.004\n0.000\n0.000\n0.001\n0.001\n0.000\n0.000\n0.000\n0.991\n0.003\nven\n0.000\n0.002\n0.002\n0.001\n0.000\n0.000\n0.001\n0.000\n0.000\n0.002\n0.992\n0.1\n0.3\n0.5\n0.7\n0.8\n0.9\nFig. 6. Confusion matrix of the lexicon improved classiﬁer and a test set with strings of length 15 characters.\nB. Future work\nThe Multinomial NB classiﬁer was used with binary n-gram\nfeatures. It should be interesting to compare the results of\nusing the normalised feature counts as well.\nThe short sentence language labels need to be veriﬁed and\nit is important to also gather data from other domains and\non modern usage of the various languages. The effect of\nthe lexicon size on the performance of the classiﬁer could\nalso be investigated. It would be interesting to estimate the\nperformance ceiling on LID of short sentences.\nStemming of the lexicon could possibly ensure that the\nLID generalises better to unseen words. However, stemming\nin many of the South African languages hasn’t been addressed\nyet.\nIt should also be interesting to train an end-to-end Deep\nRNN or CNN to do language ident in the South African\ncontext as opposed to manually engineering the two stage\nclassiﬁer.\nREFERENCES\n[1] H. Combrinck and E. Botha, “Text-based automatic language identiﬁ-\ncation.” in In Proceedings of the 6th Annual Symposium of the Pattern\nRecognition Association of South Africa, 1994.\n[2] W. Cavnar and J. Trenkle, “N-gram-based text categorization,” in In\nProceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis\nand Information Retrieval, 1997.\n[3] G. Botha, V. Zimu, and E. Barnard, “Text-based language identiﬁcation\nfor the south african languages,” in SAIEE Africa Research Journal,\n2006.\n[4] W. Pienaar and D. Snyman, “Spelling checker-based language identiﬁ-\ncation for the eleven ofﬁcial south african languages.” in In Proceedings\nof the Twenty-First Annual Symposium of the Pattern Recognition\nAssociation of South Africa, 2010.\n[5] M. Lui and T. Baldwin, “Langid.py: An off-the-shelf language identi-\nﬁcation tool,” in Proceedings of the ACL 2012 System Demonstrations,\n2012.\n[6] G. Botha and E. Barnard, “Factors that affect the accuracy of text-based\nlanguage identiﬁcation,” Comput. Speech Lang., vol. 26, no. 5, pp.\n307–320, Oct. 2012. [Online]. Available: http://dx.doi.org/10.1016/j.csl.\n2012.01.004\n[7] O. Giwa and M. Davel, “N-gram based language identiﬁcation of\nindividual words,” in Proceedings of the Annual Symposium of the\nPattern Recognition Association of South Africa (PRASA), 2013.\n[8] O. Giwa and M. H. Davel, “Language identiﬁcation of individual words\nwith joint sequence models,” in Proceedings of the Annual Conference of\nthe International Speech Communication Association INTER-SPEECH,\n2014.\n[9] A. Jaech, G. Mulcaire, S. Hathi, M. Ostendorf, and N. Smith, “A\nneural model for language identiﬁcation in code-switched tweets,” in\nProceedings of the Second Workshop on Computational Approaches to\nCode Switching, 2016.\n[10] A. Selamat and N. Akosu, “Word-length algorithm for language\nidentiﬁcation of under-resourced languages,” Journal of King Saud\nUniversity - Computer and Information Sciences, vol. 28, no. 4, pp.\n457 – 469, 2016. [Online]. Available: http://www.sciencedirect.com/\nscience/article/pii/S1319157815000609\n[11] S. A. D. of Arts and S. A. Culture & Centre for Text Technology (CTexT,\nNorth-West University, “Nchlt text corpora,” 2014, available from\nhttp://www.nwu.ac.za/ctext.\n[12] “Developing text resources for ten south african languages.” in In\nProceedings of the 9th International Conference on Language Resources\nand Evaluation, 2014.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-11-01",
  "updated": "2017-11-01"
}