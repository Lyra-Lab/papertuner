{
  "id": "http://arxiv.org/abs/1711.01100v1",
  "title": "One Model to Rule them all: Multitask and Multilingual Modelling for Lexical Analysis",
  "authors": [
    "Johannes Bjerva"
  ],
  "abstract": "When learning a new skill, you take advantage of your preexisting skills and\nknowledge. For instance, if you are a skilled violinist, you will likely have\nan easier time learning to play cello. Similarly, when learning a new language\nyou take advantage of the languages you already speak. For instance, if your\nnative language is Norwegian and you decide to learn Dutch, the lexical overlap\nbetween these two languages will likely benefit your rate of language\nacquisition. This thesis deals with the intersection of learning multiple tasks\nand learning multiple languages in the context of Natural Language Processing\n(NLP), which can be defined as the study of computational processing of human\nlanguage. Although these two types of learning may seem different on the\nsurface, we will see that they share many similarities.\n  The traditional approach in NLP is to consider a single task for a single\nlanguage at a time. However, recent advances allow for broadening this\napproach, by considering data for multiple tasks and languages simultaneously.\nThis is an important approach to explore further as the key to improving the\nreliability of NLP, especially for low-resource languages, is to take advantage\nof all relevant data whenever possible. In doing so, the hope is that in the\nlong term, low-resource languages can benefit from the advances made in NLP\nwhich are currently to a large extent reserved for high-resource languages.\nThis, in turn, may then have positive consequences for, e.g., language\npreservation, as speakers of minority languages will have a lower degree of\npressure to using high-resource languages. In the short term, answering the\nspecific research questions posed should be of use to NLP researchers working\ntowards the same goal.",
  "text": "One Model to Rule them all\nMultitask and Multilingual Modelling for Lexical Analysis\nJohannes Bjerva\narXiv:1711.01100v1  [cs.CL]  3 Nov 2017\nThe work in this thesis has been carried out under the auspices of the Center for\nLanguage and Cognition Groningen (CLCG) of the Faculty of Arts of the University\nof Groningen.\nGroningen Dissertations in Linguistics 164\nISSN: 0928-0030\nISBN: 978-94-034-0224-6 (printed version)\nISBN: 978-94-034-0223-9 (electronic version)\nc⃝2017, Johannes Bjerva\nDocument prepared with LATEX2ε and typeset by pdfTEX\n(Droid Serif and Lato fonts)\nCover art: Cortical Columns. c⃝2014, Greg Dunn\n21K, 18K, and 12K gold, ink, and dye on aluminized panel.\nPrinted by Off Page (www.offpage.nl) on G-print 115g paper.\nOne Model to Rule them all\nMultitask and Multilingual Modelling\nfor Lexical Analysis\nProefschrift\nter verkrijging van de graad van doctor aan de\nRijksuniversiteit Groningen\nop gezag van de\nrector magniﬁcus prof. dr. E. Sterken\nen volgens besluit van het College voor Promoties.\nDe openbare verdediging zal plaatsvinden op\ndonderdag 7 december 2017 om 14.30 uur\ndoor\nJohannes Bjerva\ngeboren op 21 maart 1990\nte Oslo, Noorwegen\nPromotor\nProf. dr. ing. J. Bos\nCopromotor\nDr. B. Plank\nBeoordelingscommissie\nProf. dr. A. Søgaard\nProf. dr. J. Tiedemann\nProf. dr. L. R. B. Schomaker\nAcknowledgements\nThis has been a bumpy ride, to say the least, and having reached\nthe end of this four-year journey, I owe a debt of gratitude to all of\nmy friends, family, and colleagues. Your support and guidance has\ncertainly helped smooth out most of the ups and downs I’ve experi-\nenced.\nFirst of all, I would like to thank my PhD supervisors. Johan, the\nfreedom you allowed me during the past four years has been one of\nthe things I’ve appreciated the most in the whole experience. This\nmeant that I could pursue the track of research I felt was the most\ninteresting, without which writing a whole book would have been\nmuch more arduous – thank you! Barbara, thank you for agreeing to\njoin as co-supervisor so late in my project, and for putting in so much\ntime during the last couple of months of my PhD. The thesis would\nlikely have looked quite different if you hadn’t started in Groningen\nwhen you did. I owe you a huge thanks, especially for the ﬁnal weeks\n– reading and commenting on the whole thesis in less than 24 hours\nduring your vacation. Just, wow!\nNext, I would like to thank Anders Søgaard, Jörg Tiedemann, and\nLambert Schomaker for agreeing to form my thesis assessment com-\nmittee. I feel honoured that you took the time to read this thesis,\nand that you deemed it scientiﬁcally sound. I also want to thank ev-\neryone who I have collaborated with throughout these years, both in\nGroningen and in Stockholm. Thanks Calle for agreeing to work with\na sign language novice such as myself. Raf, it was an enlightening\nvi\nexperience to work with you and see the world of ‘real’ humanities\nresearch. Robert, we should deﬁnitely continue with our one-week\nshared task submissions (with various degrees of success).\nMost of the last few years were spent at the computational linguis-\ntics group in Groningen. I would especially like to thank all of my\nfellow PhD students throughout the years. Thanks Kilian, Noortje,\nValerio, and Harm for welcoming me with open arms when I joined\nthe group. Special thanks to Kilian for being so helpful with answer-\ning all of my billions and billions of questions involved in ﬁnishing\nthis thesis. Also, a special thanks to both Noortje and Harm for the\ntimes we shared when I had just moved here (especially that ﬁrst\nNew Year’s eve!). Hessel, thanks for all the help with administra-\ntive matters while I was abroad, especially for sending a gazillion\nof travel declarations for me. Rik and Anna, it was great getting to\nknow you both better during the last few months – hopefully you\nﬁnd the bookmark to be suﬃciently sloth-y. Dieke and Rob, thanks\nfor being such great laid back drinking buddies and travel compan-\nions. To all of you, and Pauline – I hope we will continue the tra-\ndition of going all out whenever I come to visit Groningen! A big\nthanks to the rest of the computational linguistics group, especially\nGertjan, Malvina, Gosse, John, Leonie, and Martijn. Also thanks to\nall other PhD students who started with me: Luis, Simon, Raf, Ruben,\nAynur, Jonne, and everyone else whose name I’ve failed to mention.\nA special thanks to Ben and Kaja - I hope we keep up our board-game\ncentred visits to one another, no matter where we happen to pursue\nour careers.\nI spent most of my ﬁnal year in Stockholm, and it was great being\nin that relaxed atmosphere during one of the more intense periods\nof the past few years. Most of all, I am sincerely grateful to Calle, to\nJohan (and Klara, Iris, and Vive), and to Bruno. Your support is truly\ninvaluable, and by my lights, there is not much more to say than\n<dank>:(since a pal’s always needed). I’d also like to thank Johan and\nvii\nCalle especially for agreeing to observing weird Dutch traditions by\nbeing my paranymphs. Joseﬁna, Elísabet, and David, thank you all\nfor being there for me during the past year. Thanks to the computa-\ntional linguistics group for hosting me during this period, especially\nMats, Robert, Kina, and Gintar˙e. Finally, thanks to everyone else at\nthe Department of Linguistics at Stockholm University.\nHaving moved to Copenhagen this autumn has been an extremely\npleasant experience. I would like to thank the entire CoAStal NLP\ngroup for simply being the coolest research group there is. Especially,\nI’d like to thank Isabelle for making the whole process of moving to\nDenmark so easy. Thanks to Anders, Maria, Mareike, Joachim, Dirk,\nand Ana for being so welcoming. I’d also like to thank everyone else\nat the image section at DIKU, especially the running buddies at the\ndepartment, and most especially Kristoffer and Niels.\nFinally, I would like to thank my family. This bumpy ride would\nhave been challenging to get through without their support through\neverything. Kiitos, mamma! Takk/tack Aksel, Paulina, Julian, och\nLucas! Takk Olav, og takk Amanda!\nLet’s see where the journey goes next!\nCopenhagen, November 2017\nContents\nContents\nviii\n1\nIntroduction\n1\n1.1\nChapter guide\n. . . . . . . . . . . . . . . . . . . . . . . .\n4\n1.2\nPublications\n. . . . . . . . . . . . . . . . . . . . . . . . .\n6\nI\nBackground\n11\n2\nAn Introduction to Neural Networks\n13\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2.2\nRepresentation of NNs, terminology, and notation\n. . .\n15\n2.3\nFeed-forward Neural Networks\n. . . . . . . . . . . . . .\n17\n2.3.1\nFeature representations\n. . . . . . . . . . . . . .\n22\n2.3.2\nActivation Functions\n. . . . . . . . . . . . . . . .\n23\n2.3.3\nLearning . . . . . . . . . . . . . . . . . . . . . . .\n24\n2.4\nRecurrent Neural Networks\n. . . . . . . . . . . . . . . .\n32\n2.4.1\nLong Short-Term Memory . . . . . . . . . . . . .\n36\n2.4.2\nCommon use-cases of RNNs in NLP . . . . . . . .\n39\n2.5\nConvolutional Neural Networks . . . . . . . . . . . . . .\n44\n2.5.1\nLocal receptive ﬁelds . . . . . . . . . . . . . . . .\n45\n2.5.2\nWeight sharing\n. . . . . . . . . . . . . . . . . . .\n46\n2.5.3\nPooling . . . . . . . . . . . . . . . . . . . . . . . .\n48\n2.6\nResidual Networks . . . . . . . . . . . . . . . . . . . . . .\n50\n2.7\nNeural Networks and the Human Brain\n. . . . . . . . .\n51\n2.8\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n3\nMultitask Learning and Multilingual Learning\n55\n3.1\nMultitask Learning\n. . . . . . . . . . . . . . . . . . . . .\n56\nContents\nix\n3.1.1\nNon-neural Multitask Learning . . . . . . . . . .\n57\n3.1.2\nNeural Multitask Learning . . . . . . . . . . . . .\n58\n3.1.3\nEffectivity of Multitask Learning\n. . . . . . . . .\n62\n3.1.4\nWhen MTL fails . . . . . . . . . . . . . . . . . . .\n63\n3.2\nMultilingual Learning . . . . . . . . . . . . . . . . . . . .\n63\n3.2.1\nHuman Annotation . . . . . . . . . . . . . . . . .\n65\n3.2.2\nAnnotation Projection\n. . . . . . . . . . . . . . .\n65\n3.2.3\nModel Transfer\n. . . . . . . . . . . . . . . . . . .\n66\n3.2.4\nModel Transfer with Multilingual Input Repre-\nsentations\n. . . . . . . . . . . . . . . . . . . . . .\n67\n3.2.5\nContinuous Space Word Representations . . . . .\n69\n3.3\nOutlook . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\nII\nMultitask Learning\n79\n4\nMultitask Semantic Tagging\n81\n4.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n4.2\nSemantic Tagging\n. . . . . . . . . . . . . . . . . . . . . .\n83\n4.3\nMethod . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n4.3.1\nInception model . . . . . . . . . . . . . . . . . . .\n88\n4.3.2\nDeep Residual Networks . . . . . . . . . . . . . .\n90\n4.3.3\nModelling character information and residual\nbypass\n. . . . . . . . . . . . . . . . . . . . . . . .\n90\n4.3.4\nSystem description\n. . . . . . . . . . . . . . . . .\n92\n4.4\nEvaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4.4.1\nExperiments on semantic tagging . . . . . . . . .\n96\n4.4.2\nExperiments on Part-of-Speech tagging . . . . . .\n96\n4.4.3\nThe Inception architecture . . . . . . . . . . . . .\n96\n4.4.4\nEffect of pre-trained embeddings . . . . . . . . .\n98\n4.5\nDiscussion\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n4.5.1\nPerformance on semantic tagging . . . . . . . . .\n98\n4.5.2\nPerformance on Part-of-Speech tagging\n. . . . .\n99\n4.5.3\nInception . . . . . . . . . . . . . . . . . . . . . . . 100\nx\nContents\n4.5.4\nResidual bypass . . . . . . . . . . . . . . . . . . . 100\n4.5.5\nPre-trained embeddings\n. . . . . . . . . . . . . . 101\n4.6\nConclusions . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5\nInformation-theoretic Perspectives on Multitask Learning\n103\n5.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . 104\n5.2\nInformation-theoretic Measures . . . . . . . . . . . . . . 105\n5.2.1\nEntropy . . . . . . . . . . . . . . . . . . . . . . . . 105\n5.2.2\nConditional Entropy . . . . . . . . . . . . . . . . . 105\n5.2.3\nMutual Information . . . . . . . . . . . . . . . . . 107\n5.2.4\nInformation Theory and MTL in NLP . . . . . . . 107\n5.3\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n5.3.1\nMorphosyntactic Tasks . . . . . . . . . . . . . . . 109\n5.3.2\nSemantic Tasks\n. . . . . . . . . . . . . . . . . . . 111\n5.4\nMethod . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n5.4.1\nArchitecture and Hyperparameters . . . . . . . . 113\n5.4.2\nExperimental Overview\n. . . . . . . . . . . . . . 114\n5.4.3\nReplicability and Reproducibility . . . . . . . . . 114\n5.5\nResults and Analysis . . . . . . . . . . . . . . . . . . . . . 115\n5.5.1\nMorphosyntactic Tasks . . . . . . . . . . . . . . . 115\n5.5.2\nLanguage-dependent results . . . . . . . . . . . . 118\n5.5.3\nSemantic Tasks\n. . . . . . . . . . . . . . . . . . . 118\n5.6\nConclusions . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nIII Multilingual Learning\n123\n6\nMultilingual Semantic Textual Similarity\n125\n6.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.2\nCross-lingual Semantic Textual Similarity\n. . . . . . . . 128\n6.3\nMethod . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n6.3.1\nMultilingual word representations . . . . . . . . 131\n6.3.2\nSystem architecture . . . . . . . . . . . . . . . . . 132\n6.3.3\nData for Semantic Textual Similarity . . . . . . . 135\nContents\nxi\n6.4\nExperiments and Results . . . . . . . . . . . . . . . . . . 135\n6.4.1\nComparison with Monolingual Representations . 136\n6.4.2\nSingle-source training . . . . . . . . . . . . . . . . 137\n6.4.3\nMulti-source training . . . . . . . . . . . . . . . . 138\n6.4.4\nResults on SemEval-2017 . . . . . . . . . . . . . . 140\n6.4.5\nResults on SemEval-2016 . . . . . . . . . . . . . . 141\n6.5\nConclusions . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n7\nComparing Multilinguality and Monolinguality\n143\n7.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . 144\n7.2\nSemantic Tagging\n. . . . . . . . . . . . . . . . . . . . . . 146\n7.2.1\nBackground\n. . . . . . . . . . . . . . . . . . . . . 146\n7.2.2\nData . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.2.3\nMethod . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.2.4\nExperiments and Analysis . . . . . . . . . . . . . 148\n7.2.5\nSummary of Results on Semantic Tagging . . . . 154\n7.3\nTagging Tasks in the Universal Dependencies . . . . . . 154\n7.3.1\nData . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n7.3.2\nMethod . . . . . . . . . . . . . . . . . . . . . . . . 155\n7.3.3\nResults and Analysis\n. . . . . . . . . . . . . . . . 157\n7.3.4\nSummary of Results on the Universal Dependen-\ncies\n. . . . . . . . . . . . . . . . . . . . . . . . . . 158\n7.4\nMorphological Inﬂection . . . . . . . . . . . . . . . . . . 160\n7.4.1\nMethod . . . . . . . . . . . . . . . . . . . . . . . . 160\n7.4.2\nResults and Analysis\n. . . . . . . . . . . . . . . . 162\n7.4.3\nSummary of Results on Morphological Inﬂection 164\n7.5\nEstimating Language Similarities . . . . . . . . . . . . . 164\n7.5.1\nData-driven Similarity\n. . . . . . . . . . . . . . . 165\n7.5.2\nLexical Similarity . . . . . . . . . . . . . . . . . . 166\n7.5.3\nResults and Analysis\n. . . . . . . . . . . . . . . . 167\n7.5.4\nWhen is Multilinguality Useful? . . . . . . . . . . 170\n7.6\nConclusions . . . . . . . . . . . . . . . . . . . . . . . . . . 171\nxii\nContents\nIV Combining Multitask and Multilingual Learning\n173\n8\nMultitask Multilingual Learning\n175\n8.1\nCombining Multitask Learning and Multilinguality . . . 176\n8.2\nData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n8.2.1\nLabelled data\n. . . . . . . . . . . . . . . . . . . . 179\n8.2.2\nUnlabelled data\n. . . . . . . . . . . . . . . . . . . 180\n8.3\nMethod . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n8.3.1\nArchitecture . . . . . . . . . . . . . . . . . . . . . 181\n8.3.2\nHyperparameters . . . . . . . . . . . . . . . . . . 181\n8.4\nExperiments and Analysis\n. . . . . . . . . . . . . . . . . 182\n8.5\nDiscussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . 193\n8.6\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . 195\nV\nConclusions\n197\n9\nConclusions\n199\n9.1\nPart II - Multitask Learning . . . . . . . . . . . . . . . . . 199\n9.2\nPart III - Multilingual Learning\n. . . . . . . . . . . . . . 201\n9.3\nPart IV - Combining Multitask Learning and Multilin-\nguality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n9.4\nFinal words . . . . . . . . . . . . . . . . . . . . . . . . . . 203\nAppendices\n205\nA\nCorrelation ﬁgures for all languages in Chapter 5\n207\nB\nBibliographical abbreviations\n211\nBibliography\n213\nSummary\n239\nSamenvatting\n243\nCHAPTER 1\nIntroduction\nWhen learning a new skill, you take advantage of your preexisting\nskills and knowledge. For instance, if you are a skilled violinist, you\nwill likely have an easier time learning to play cello. Similarly, when\nlearning a new language you take advantage of the languages you\nalready speak. For instance, if your native language is Norwegian\nand you decide to learn Dutch, the lexical overlap between these two\nlanguages will likely beneﬁt your rate of language acquisition. This\nthesis deals with the intersection of learning multiple tasks and learn-\ning multiple languages in the context of Natural Language Processing\n(NLP), which can be deﬁned as the study of computational processing\nof human language. Although these two types of learning may seem\ndifferent on the surface, we will see that they share many similari-\nties.\nTraditionally, NLP practitioners have looked at solving a single\nproblem for a single task at a time. For instance, considerable time\nand effort might be put into engineering a system for part-of-speech\n(PoS) tagging for English. However, although the focus has been on\nconsidering a single task at a time, fact is that many NLP tasks are\nhighly related. For instance, different lexical tag sets will likely ex-\nhibit high correlations with each other. As an example, consider the\n2\n1. Introduction\nfollowing sentence annotated with Universal Dependencies (UD) PoS\ntags (Nivre et al., 2016a), and semantic tags (Bjerva et al., 2016b).1,2\n(1.1) We\nPRON\nmust\nAUX\ndraw\nVERB\nattention\nNOUN\nto\nADP\nthe\nDET\ndistribution\nNOUN\nof\nADP\nthis\nDET\nform\nNOUN\nin\nADP\nthose\nDET\ndialects\nNOUN\n.\nPUNCT\n(1.2) We\nPRO\nmust\nNEC\ndraw\nEXS\nattention\nCON\nto\nREL\nthe\nDEF\ndistribution\nCON\nof\nAND\nthis\nPRX\nform\nCON\nin\nREL\nthose\nDST\ndialects\nCON\n.\nNIL\nWhile these tag sets are certainly different, the distinctions they make\ncompared to one another in this example are few, as there are only\ntwo apparent systematic differences. Firstly, the semantic tags offer\na difference between deﬁnite (DEF), proximal (PRX), and distal deter-\nminers (DST), whereas UD lumps these together as DET (highlighted\nin green). Secondly, the semantic tags also differentiate between re-\nlations (REL) and conjunctions (AND), which are both represented by\nthe ADP PoS tag, highlighted in blue. Hence, although these tasks are\nundoubtedly different, there are considerable correlations between\nthe two, as the rest of the tags exhibit a one-to-one mapping in this ex-\nample. This raises the question of how this fact can be exploited, as\nit seems like a colossal waste to not take advantage of such inter-task\ncorrelations. In this thesis I approach this by exploring multitask\nlearning (MTL, Caruana, 1993; 1997), which has been beneﬁcial for\nmany NLP tasks. In spite of such successes, however, it is not clear\nwhen or why MTL is beneﬁcial.\n1PMB 01/3421. Original source: Tatoeba. UD tags obtained using UD-Pipe\n(Straka et al., 2016)\n2The semantic tag set consists of 72 tags, and is developed for multilingual\nsemantic parsing. The tag set is described further in Chapter 4.\n3\nSimilarly to how different tag sets correlate with each other, lan-\nguages also share many commonalities with one another. These re-\nsemblances can occur on various levels, with languages sharing, for\ninstance, syntactic, morphological, or lexical features. Such similar-\nities can have many different causes, such as common language an-\ncestry, loan words, or being a result of universals and constraints in\nthe properties of natural language itself (see, e.g., Chomsky,2005, and\nHauser et al., 2002). Consider, for instance, the following German\ntranslation of the previous English example, annotated with seman-\ntic tags.3\n(1.3) Wir\nPRO\nmüssen\nNEC\ndie\nDEF\nVerbreitung\nCON\ndieser\nPRX\nForm\nCON\nin\nREL\ndiesen\nPRX\nDialekten\nCON\nbeachten\nEXS\n.\nNIL\nComparing the English and German annotations, there is a high over-\nlap between the semantic tags used, and a high lexical overlap. As in\nthe case of related NLP tasks, this begs the question of how multilin-\nguality can be exploited, as it seems like an equally colossal waste to\nnot consider using, e.g., Norwegian PoS data when training a Swedish\nPoS tagger. There are several approaches to exploiting multilingual\ndata, such as annotation projection and model transfer, as detailed\nin Chapter 3. The approach in this thesis is a type of model trans-\nfer, in which such inter-language relations are exploited by exploring\nmultilingual word representations, which have also been beneﬁcial\nfor many NLP tasks. As with MTL, in spite of the fact that such ap-\nproaches have been successful for many NLP tasks, it is not clear in\nwhich cases it is an advantage to go multilingual.\nGiven the large amount of data available for many languages in\ndifferent annotations, it is tempting to investigate possibilities of com-\n3PMB 01/3421. Original source: Tatoeba.\n4\n1. Introduction\nbining the paradigms of multitask learning and multilingual learning\nin order to take full advantage of this data. Hence, as the title of the\nthesis suggests, the ﬁnal effort in this thesis is to arrive at One Model\nto rule them all.\nThis thesis approaches these two related aspects of NLP by ex-\nperimenting with deep neural networks, which represent a family\nof learning architectures which are exceptionally well suited for the\naforementioned purposes (described in Chapter 2).\nFor one, it is\nfairly straightforward to implement the sharing of parameters be-\ntween tasks, thus enabling multitask learning (discussed in Chap-\nter 3).\nAdditionally, providing such an architecture with multilin-\ngual input representations is also straightforward (discussed in Chap-\nter 3). Experiments in this thesis are run on a large collection of tasks,\nboth semantic and morphosyntactic in nature, and a total of 60 lan-\nguages are considered, depending on the task at hand.\n1.1\nChapter guide\nThe thesis is divided into ﬁve parts, totalling 9 chapters, aiming to\nprovide answers to the following general research questions (RQs):\nRQ 1 To what extent can a semantic tagging task be informative for\nother NLP tasks?\nRQ 2 How can multitask learning effectivity in NLP be quantiﬁed?\nRQ 3 To what extent can multilingual word representations be used\nto enable zero-shot learning in semantic textual similarity?\nRQ 4 In which way can language similarities be quantiﬁed, and what\ncorrelations can we ﬁnd between multilingual model perfor-\nmance and language similarities?\nRQ 5 Can a multitask and multilingual approach be combined to gen-\neralise across languages and tasks simultaneously?\n1.1. Chapter guide\n5\nPart I – Background\nThe goal of Part I is to provide the reader with suﬃcient background\nknowledge to understand the material in this thesis. Chapter 2 con-\ntains a crash-course in neural networks, introducing the main con-\ncepts and architectures used in NLP. Chapter 3 provides an introduc-\ntion to multitask learning and multilingual learning, which are the\ntwo central topics of this work.\nPart II – Multitask Learning\nIn Part II, the goal is to investigate multitask learning (MTL), in par-\nticular by looking at the effects of this paradigm in NLP sequence\nprediction tasks. In Chapter 4 we present a semantic tag set tailored\nfor multilingual semantic parsing. We attempt to use this tag set as\nan auxiliary task for PoS tagging, observing what effects this yields,\nanswering RQ 1. Chapter 5 then delves deeper into MTL, attempt-\ning to ﬁnd when MTL is effective, and how this effectiveness can be\npredicted by using information-theoretic measures (RQ 2).\nPart III – Multilingual Learning\nHaving looked at similarities between tasks, we turn to similarities\nbetween languages in Part III. In Chapter 6, we attempt to make a\nlanguage-agnostic solution for semantic textual similarity, by exploit-\ning multilingual word representations, thus answering RQ 3. Having\nseen the results of combining related languages in this task, we try to\nquantify these effects in Chapter 7, aiming to answer RQ 4.\nPart IV – Combining Multitask and Multilingual Learning\nIn Part IV we want to combine the paradigms of multitask learning\nand multilingual learning in order to make One Model to rule them all.\nChapter 8 presents a pilot study taking a step in this direction, looking\n6\n1. Introduction\nat predicting labels for an unseen task–language combination while\nexploiting other task–language combinations.\nPart V – Conclusions\nFinally, Chapter 9 contains an overview of the conclusions from this\nthesis. In addition to this, we provide an outlook for future work\nin this direction, in particular focussing on the combined multitask–\nmultilingual paradigm.\n1.2\nPublications\nThis thesis is based on the following publications:\n1. Bjerva, J., Plank, B., and Bos, J. (2016b). Semantic tagging with\ndeep residual networks.\nIn Proceedings of COLING 2016, the\n26th International Conference on Computational Linguistics: Tech-\nnical Papers, pages 3531–3541\n2. Bjerva, J. (2017b).\nWill my auxiliary tagging task help? Esti-\nmating Auxiliary Tasks Effectivity in Multi-Task Learning. In\nProceedings of the 21st Nordic Conference on Computational Lin-\nguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden, num-\nber 131, pages 216–220. Linköping University Electronic Press,\nLinköpings universitet. Best short paper.\n3. Bjerva, J. and Östling, R. (2017a). Cross-lingual Learning of Se-\nmantic Textual Similarity with Multilingual Word Representa-\ntions.\nIn Proceedings of the 21st Nordic Conference on Com-\nputational Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg,\nSweden, number 131, pages 211–215. Linköping University Elec-\ntronic Press, Linköpings universitet\n1.2. Publications\n7\n4. Bjerva, J. and Östling, R. (2017b). Multilingual word representa-\ntions for semantic textual similarity. In Proceedings of SemEval\n2017: International Workshop on Semantic Evaluation\n5. Bjerva, J. (2017a). Quantifying the Effects of Multilinguality in\nNLP Sequence Prediction Tasks. Under review\nSome parts of the thesis may also refer to the following peer-reviewed\npublications completed in the course of the PhD:\n6. Bjerva, J. (2014). Multi-class animacy classiﬁcation with seman-\ntic features. In Proceedings of the Student Research Workshop at\nthe 14th Conference of the European Chapter of the Association\nfor Computational Linguistics, pages 65–75\n7. Bjerva, J., Bos, J., Van der Goot, R., and Nissim, M. (2014). The\nmeaning factory: Formal semantics for recognizing textual en-\ntailment and determining semantic similarity. In Proceedings\nof the 8th International Workshop on Semantic Evaluation (Se-\nmEval 2014), pages 642–646, Dublin, Ireland\n8. Bjerva, J. and Praet, R. (2015). Word embeddings pointing the\nway for late antiquity. In 9th SIGHUM Workshop on Language\nTechnology for Cultural Heritage, Social Sciences and Humani-\nties (LaTeCH 2015), pages 53–57\n9. Bjerva, J. and Börstell, C. (2016). Morphological Complexity In-\nﬂuences Verb-Object Order in Swedish Sign Language. In Pro-\nceedings of the Workshop on Computational Linguistics for Lin-\nguistic Complexity (CL4LC), pages 137–141\n10. Bjerva, J. (2016). Byte-based language identiﬁcation with deep\nconvolutional networks. In Proceedings of the Third Workshop\non NLP for Similar Languages, Varieties and Dialects (VarDial3),\npages 119–125\n8\n1. Introduction\n11. Busger op Vollenbroek, M., Carlotto, T., Kreutz, T., Medvedeva,\nM., Pool, C., Bjerva, J., Haagsma, H., and Nissim, M. (2016). Gron-\nUP: Groningen user proﬁling. In Proceedings of CLEF 2016\n12. Haagsma, H. and Bjerva, J. (2016). Detecting novel metaphor\nusing selectional preference information. In Proceedings of the\nFourth Workshop on Metaphor in NLP, pages 10–17\n13. Bjerva, J., Bos, J., and Haagsma, H. (2016a). The Meaning Fac-\ntory at SemEval-2016 Task 8: Producing AMRs with Boxer. In\nProceedings of the 10th International Workshop on Semantic Eval-\nuation (SemEval-2016), pages 1179–1184\n14. Bjerva, J. and Praet, R. (2016). Rethinking intertextuality through\na word-space and social network approach – the case of Cas-\nsiodorus.\nJournal of Data Mining and Digital Humanities, ac-\ncepted, in revision.\n15. Bos, J., Basile, V., Evang, K., Venhuizen, N. J., and Bjerva, J. (2017).\nThe Groningen Meaning Bank, pages 463–496. Springer Nether-\nlands, Dordrecht\n16. Abzianidze, L., Bjerva, J., Evang, K., Haagsma, H., van Noord,\nR., Ludmann, P., Nguyen, D.-D., and Bos, J. (2017). The Paral-\nlel Meaning Bank: Towards a Multilingual Corpus of Transla-\ntions Annotated with Compositional Meaning Representations.\nIn EACL, pages 242–247\n17. Östling, R. and Bjerva, J. (2017). SU-RUG at the CoNLL-SIGMOR-\nPHON 2017 shared task: Morphological Inﬂection with Atten-\ntional sequence-to-sequence models. In Proceedings of the 2017\nMeeting of SIGMORPHON, Vancouver, Canada. Association for\nComputational Linguistics\n1.2. Publications\n9\n18. Sjons, J., Hörberg, T., Östling, R., and Bjerva, J. (2017). Articu-\nlation rate in swedish child-directed speech increases as a func-\ntion of the age of the child even when surprisal is controlled for.\nIn Proceedings of Interspeech 2017, Stockholm, Sweden\n19. Kulmizev, A., Blankers, B., Bjerva, J., Nissim, M., van Noord, G.,\nPlank, B., and Wieling, M. (2017).\nThe power of character n-\ngrams in native language identiﬁcation. In Proceedings of Shared\nTask on NLI at BEA17\n20. Bjerva, J., Grigonyt˙e, G., Östling, R., and Plank, B. (2017). Neural\nnetworks and spelling features for native language identiﬁca-\ntion. In Proceedings of Shared Task on NLI at BEA17\nPART I\nBackground\nCHAPTER 2\nAn Introduction\nto Neural Networks\nAbstract|Deep Neural Networks are at the forefront of many state-of-the\nart approaches to Natural Language Processing (NLP). The ﬁeld of NLP\nis currently awash with papers building on this method, to the extent\nthat it has quite aptly been described as a tsunami (Manning, 2015).\nWhile a large part of the ﬁeld is familiar with this family of learning\narchitectures, it is the intention of this thesis to be available for a larger\naudience. Hence, although the rest of this thesis assumes familiarity with\nneural networks, this chapter is meant to be a foundational introduction\nfor those with limited experience in this area. The reader is assumed\nto have some familiarity with machine learning and NLP, but not much\nbeyond that.\nWe begin by exploring the basics of neural networks, and look at the\nthree most commonly used general architectures for neural networks in\nNLP: Feed-forward Neural Networks, Recurrent Neural Networks, and\nConvolutional Neural Networks. Some common NLP scenarios are then\noutlined together with suggestions for suitable architectures.\n14\n2. An Introduction to Neural Networks\n2.1\nIntroduction\nThe term deep learning is used to refer to a family of learning models,\nwhich represent some of the most powerful learning models avail-\nable today. The power of this type of model lies in part in its intrin-\nsic hierarchical processing of input features, which allows for learn-\ning representations at multiple levels of abstraction (LeCun et al.,\n2015). This type of model is commonly referred to by several um-\nbrella terms, such as deep learning, and (deep) neural networks.1 In\nthis chapter, I aim to introduce the basic concepts of NNs, at a level\nsuﬃcient to understand the work in this thesis. The following sec-\ntions are meant to cover the most basic workings in an intuitive, and\ntheoretically supported, manner. In addition to this background, I\ngive an overview of the scenarios in which different recurrent NN\narchitectures might be suitable in NLP (Section 2.4.2).\nHistory\nNeural networks have a long history, and have been popular in three\nmain waves.2 In the ﬁrst wave, roughly between the 40s −60s, they\nappeared under the moniker of cybernetics (e.g., Wiener, 1948), in-\nspired by the Hebbian learning rule (Hebb, 1949). In this wave, the\nperceptron was ﬁrst outlined (Rosenblatt, 1957), which is still rela-\ntively popular today (see Section 2.3). Next, in the 80s and 90s, con-\nnectionism was on the rise. In this wave, the algorithm for backwards\npropagation of errors (Rumelhart et al., 1985) was described, which\nis at the core of how neural networks are trained (see Section 2.3.3).\n1While some make a distinction between deep and non-deep neural networks\n(NNs) depending on the amount of layers used, there is no real consensus on\nwhere the line between these models should be. For the sake of consistency, I\nattempt to refer to this family of models as NNs, or some speciﬁcation thereof, as\nconsistently as possible.\n2Only a very brief overview of the history is given here. For more details, the\nreader is referred to, e.g., Wang et al., 2017, or Goodfellow et al., 2016.\n2.2. Representation of NNs, terminology, and notation\n15\nAfter an AI winter lasting roughly from 97 to 06, we ﬁnally arrive\nat the current wave (or tsunami), in which the term deep learning is\nfavoured. This wave was initiated by works on deep belief networks\n(Hinton et al., 2006), and has been the subject of much attention after\nsuccesses in, e.g., reducing error rates in some tasks by more than\n50% (LeCun et al., 2015). The recent advances made in the current\nwave further include breakthroughs in both recognition (He et al.,\n2016) and generation (Goodfellow et al., 2014) of images, in NLP tasks\nsuch as machine translation (Bahdanau et al., 2015; Wu et al., 2016)\nand parsing (Chen and Manning, 2014), as well as in the strategic\nboard game Go (Silver et al., 2016), and the ﬁrst-person shooter Doom\n(Lample and Chaplot, 2017).\n2.2\nRepresentation of NNs, terminology, and notation\nBefore embarking upon this journey and exploring the wondrous\nworld of NNs, it is necessary to equip ourselves with some common\nground in terms of terminology, notation, and how NNs are generally\nrepresented in this thesis. Figure 2.1 contains a NN, which we will go\nthrough in detail.\nFigure 2.1: A basic Neural Network.\n16\n2. An Introduction to Neural Networks\nFirst, note that the network is divided into three vertical slices. Each\nsuch slice represents a layer, marked by a light grey ﬁeld. Each layer\ncontains one or more white circles, each representing a unit, or neu-\nron.3 In this network, each unit has a connection to every unit in the\nfollowing layer. These connections are represented by arrows, which\ndenote some weighting of the output of the unit at the start of the ar-\nrow, for the input of the unit at the end of the arrow. Each layer can\nbe described mathematically as a vector of activations. In the case\nof the ﬁrst layer (the input layer), these activations are equal to the\ninput (i.e. ⃗α0 = ⃗x). The ﬁnal layer encodes the output of the network,\nwhich is denoted by ˆy. Each layer up until the ﬁnal layer also con-\ntains a special unit, marked by +1, which is called the bias unit.4 The\ncollection of all arrows between two layers, can be described math-\nematically as a matrix of weights (e.g., W0).5 The application of the\nweight matrix to the input of the network can be described in linear\nalgebraic notation as\n⃗z1 = W0⃗x = W0⃗α0,\n(2.1)\nwhere ⃗z1 is the vector (i.e. a series of numbers) resulting from this lin-\near transformation. The ⃗z-vectors can be referred to as pre-activation\nvectors. Each hidden layer thus ﬁrst encodes the sum of the multi-\nplications of each of the activations in the previous layer by some\nweight. For instance, if we set W0 to be matrix of ones, then the pre-\nactivation value of the ﬁrst unit in the ﬁrst hidden layer z1\n0 = Σ4\ni=0⃗xi.\nThe ﬁnal piece of the puzzle is to calculate the output of each unit\nin the layer, by applying an activation function to the pre-activation\n3In this thesis, the term unit is preferred. While this is conventional in much\nof NLP, it is also debatable whether borrowing terminology for neural networks\nfrom neuroscience is motivated at all (this is discussed further in Section 2.7).\n4Although this can be discussed at length, suﬃce it to say that including bias\nunits facilitates learning.\n5We will cover ways in which to learn these weights later in this chapter\n(Section 2.3.3).\n2.3. Feed-forward Neural Networks\n17\nvector,\n⃗α1 = σ(⃗z1),\n(2.2)\nwhere σ is some non-linear activation function.6 Essentially, this is\nall that a basic FFNN is – a series of matrix-vector multiplications,\nwith non-linearities applied to it. Now, how can this be used to solve\nproblems, and how does the network learn to do this? The answers\nto these questions will be made clear in the course of the following\nfew pages.\nNotation\nBefore we continue, a brief note on the notation used in this thesis.\nScalars are represented with lower case letters (x, y), vectors are rep-\nresented with lower case letters with arrows (⃗x, ⃗y), and matrices are\nrepresented with blackboard upper case letters (X, Y). Subscripts are\nused to denote the layer number, and where necessary, a superscript\nis used to denote indexation, to indicate the unit number in the case\nof deep networks. For instance, αj\ni indicates the activation of unit j\nin layer i, and Wi indicates the weight matrix for layer i. Activation\nfunctions (Section 2.3.2) are denoted with σ, occasionally subscripted\nwith the actual function used (σReLU).\n2.3\nFeed-forward Neural Networks\nA Feed-forward Neural Network (FFNN), also known as a multilayer\nperceptron, is perhaps the most basic variant of neural networks,\nand is the kind depicted in the previous ﬁgure. As mentioned, a neu-\nral network can be seen as a collection of non-linear functions ap-\nplied to a collection of matrices and vectors, thus mapping from one\n6Traditionally the activation function (σ) used is some sigmoidal function,\nsuch as the logistic function. However, many functions are suitable, given that\nthey satisfy certain properties (see Section 2.3.2).\n18\n2. An Introduction to Neural Networks\ndomain (e.g., words) to another (e.g., PoS tags). Let us consider a con-\ncrete example, in which X contains information about the current\nweather, and Y = {0, 1} denotes human-annotated labels denoting\nwhether or not the weather is considered good. In this case, X con-\ntains several variables, each representing a certain type of weather\n(e.g. x1 = calm weather, and x2 = sunny weather).7 Table 2.1 repre-\nsents the weather judgements of this example, where y = 1 indicates\ngood weather, and y = 0 indicates not-so-good weather.\nTable 2.1: Weather appraisal (mimicking the logical AND function).\nCalm (x1)\nSunny (x2)\nLabel (y)\n0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n1\nThe table shows the judgements of someone who considers weather\nto be good (i.e. y = 1) only when it is both calm and sunny (i.e. the log-\nical AND function). Let us now consider our second neural network,\nwhich can solve the problem of determining whether the weather is\ngood, based on this person’s judgements, in Figure 2.2.\nTable 2.2: Weather appraisal by the neural network in Figure 2.2.\nCalm (x1)\nSunny (x2)\nz1\nσ(z1) = σ(α1) = ˆy\nLabel (y)\n0\n0\n−15\n≈0\n0\n0\n1\n−5\n≈0\n0\n1\n0\n−5\n≈0\n0\n1\n1\n5\n≈1\n1\nApplying the calculations detailed in the previous section to this net-\n7Note that we begin numbering of features with 1, as the index 0 is reserved\nfor the bias terms.\n2.3. Feed-forward Neural Networks\n19\nInput\nLayer 1\nOutput\n10\n10\n+1\n-15\nFigure 2.2: A Neural network coding the AND logical function.\nwork yields the results shown in Table 2.2. If only one of x1, x2 is\nactive, the activation α1 is approximately 0, while the activation is 1\nif both x1 and x2 are active. We get these values, by applying the per-\nhaps most commonly used activation function to z. This is the logistic\nfunction, deﬁned as\nf(z) =\n1\n1 + e−z ,\n(2.3)\nwhere e is Euler’s number. Plotting this function, yields the graph in\nFigure 2.3. Hence, the value of f(x) approaches 0 when x < 0, and\napproaches 1 when x > 0.\nThe simple neural network considered here, is what is also referred\nto as a perceptron, although, technically, a perceptron uses the step\nfunction as its activation function – in other words, if x < λ where λ\nis some threshold, then f(x) = 0, and if x > λ, then f(x) = 1 (Rosen-\nblatt, 1957). This is a very simple and useful architecture, but there\nare many problems which can not be easily solved by a perceptron,\nsuch as those in which the decision boundary to be learned is non-\nlinear. Take, for instance, the problem given in Table 2.3.\n20\n2. An Introduction to Neural Networks\nFigure 2.3: Plot of the logistic function.\nTable 2.3: Weather appraisal (mimicking the logical XOR function).\nSnowy (x1)\nSunny (x2)\nLabel (y)\n0\n0\n0\n1\n0\n1\n0\n1\n1\n1\n1\n0\nThis table shows the labels provided by some annotator who consid-\ners weather to be good (i.e. y = 1) if it is either snowy or sunny – but\nnot both (i.e. the logical XOR function). As mentioned, a single unit\n(i.e. a perceptron) is not able to learn this decision boundary (Min-\nsky and Papert, 1988).8 Let us now have a look at a neural network\nwhich encodes this function. This is depicted in Figure 2.4. For the\nsake of clarity, all weights between x1, x2 and a1, a2 are set to 5, but\nonly one of these weights is shown.\nApplying the calculations detailed in the previous section to this\n8This is frequently cited as a potential catalyst for the the AI winter, in which\nfunding and interest in artiﬁcial intelligence was at a low. Nonetheless, it was\nknown at the time that the XOR problem could be solved by a neural network\nwith more hidden units (Rumelhart et al., 1985).\n2.3. Feed-forward Neural Networks\n21\nInput\nLayer 1\nLayer 2\nOutput\n+1\n+1\n-6\n10\n5\n-8\n-1\n-6\nFigure 2.4: A Neural network coding the XOR logical function.\nnetwork yields the results shown in Table 2.4. If both or none of\nx1, x2 are active, then the network will output 0, whereas if one and\nonly one of x1, x2 are active, the network will output 1. Exactly what\nwe want!\nTable 2.4: Weather appraisal by the neural network in Figure 2.4.\nCalm (x1)\nSunny (x2)\nz1\n1\nz1\n1\nz1\n2\nσ(z1\n2) = α2\n1 = ˆy\ny\n0\n0\n−8\n−1\n−6\n≈0\n0\n1\n0\n−3\n4\n4\n≈1\n1\n0\n1\n−3\n4\n4\n≈1\n1\n1\n1\n3\n9\n−2\n≈0\n0\nThe last two examples have shown how neural networks can encode\ncertain simple functions. It turns out that neural networks can do\nmuch more than this, and are in fact universal function approxima-\ntors (Cybenko, 1989; Hornik et al., 1989). What this means, is that no\nmatter the function, there is guaranteed to be a neural network with\na single layer and a ﬁnite number of hidden units, such that for each\npotential input x, (a close approximation of) the value f(x) is output\n22\n2. An Introduction to Neural Networks\nfrom the network.\nThe class of networks discussed here is useful for many tasks in\nNLP, and can be used as a simple replacement for other classiﬁers.\nFurthermore, they can be expanded by adding more units to each\nlayer, or by adding more layers. This allows such networks to learn\nto solve interesting NLP problems, like language modelling (Bengio\net al., 2003; Vaswani et al., 2013), and sentiment classiﬁcation (Iyyer\net al., 2015).\nBefore going into other NN architectures, we will ﬁrst consider\nsome of the inner workings of NNs. This includes how we represent\nour input, how weights are obtained, and ﬁnally some limitations\nwhich motivate the use of more complex architectures than FFNNs.\n2.3.1\nFeature representations\nIn many NLP problems, we are interested in mapping from some tex-\ntual language representation (x) to some label (y). This textual rep-\nresentation can take many forms, both depending on the problem at\nhand, and on the choices made when approaching the problem. As\nan example, say we are interested in doing sentiment analysis, i.e.,\ngiven a text (x), predict whether the text is positive or negative in\nsentiment (y). The perhaps simplest way of representing the text is\nto count the occurrences of each word in the text. The intuition be-\nhind this is that if a text contains many negative words (horrible, bad,\nappalling), it is more likely to be negative in sentiment than if it con-\ntains many positive words (wonderful, good, exquisite). Since these\nfeatures (i.e. counts of each word) need to be passed to an FFNN,\nthey need to be represented as a single ﬁxed-length vector ⃗x. What\none might then do, is to assign an index to each unique word, and\nassign the count of each word to that index in the vector. This can be\nreferred to as a bag-of-words model.\nAlthough this type of feature representation is suﬃcient for some\nproblems, and is traditionally used extensively, more recent develop-\n2.3. Feed-forward Neural Networks\n23\nments include using other types of representations based on distribu-\ntional semantics. This is covered in more detail in the next chapter,\nin Section 3.2.5.\n2.3.2\nActivation Functions\nAs stated in Section 2.3, each hidden unit applies an activation func-\ntion to the sum of its weighted inputs. While many functions might\nbe used, an activation function should have certain properties. One\nsuch property is that the function needs to be non-linear. It is for\nthis kind of function that it has been proven that a two-layer neural\nnetwork is a universal function approximator (Cybenko, 1989). Ad-\nditionally, the function should be monotonic, as the error surface as-\nsociated with a single-layer model will then be convex (Wu, 2009).9\nThere are several other important properties, which are not covered\nhere.\nSome of the more commonly used activation functions are\nlisted in Table 2.5.\nTable 2.5: Commonly used activation functions in neural networks.\nName\nFunction\nLogistic (aka. sigmoid)\nf(x) =\n1\n1+e−x\nHyperbolic Tangent (tanh)\nf(x) =\n2\n1+e−2x −1\nRectiﬁed Linear Unit (ReLU)\nf(x) =\n\u001a 0\nfor\nx < 0\nx\nfor\nx ≥0\nLeaky ReLU\nf(x) =\n\u001a 0.01x\nfor\nx < 0\nx\nfor\nx ≥0\nSoftmax\nf(⃗x)i =\nexi\nPK\nk=1 exk for i = 1, ..., K\n9A monotonic function is either non-increasing or non-decreasing in its en-\ntirety.\n24\n2. An Introduction to Neural Networks\nThe traditionally popular logistic function was already described\nin Figure 2.3. We will now consider some other commonly used acti-\nvation functions. Activation functions turn out to be one of the areas\nin which biological inspiration has been directly applicable to the de-\nvelopment of neural networks. The Rectiﬁed Linear Unit (ReLU) is\nin fact remarkably similar to what happens in a biological neuron\n(Hahnloser et al., 2000; Hahnloser and Seung, 2001). That is to say,\nwhen the input is below a certain threshold, the neuron does not ﬁre,\nand when the input is above this threshold, the neuron ﬁres with a\ncurrent proportional to its input. ReLUs have been found to make it\nsubstantially easier to train deep networks (Nair and Hinton, 2010),\nand are currently very widely used. One disadvantage of ReLUs is\nthat they can wind up in a state in which they are inactive for al-\nmost all inputs, meaning that no gradients ﬂow backward through\nthe unit. This, in turn, means that the unit is perpetually stuck in\nan inactive state, which at a large scale can decrease the network’s\noverall capacity. This is mitigated by using leaky ReLUs, for which\neven input < 0 leads to some activity, allowing for error propagation\ngiven any input value.\nThe softmax function is generally only used at the ﬁnal layer in\nclassiﬁcation problems, as it yields a probability distribution based\non its input.\n2.3.3\nLearning\nLearning in an FFNN happens in two phases. First, in the forward\npropagation pass, the network sends a given input through the net-\nwork, and produces some output. Then, the error of this output is\ncalculated, as compared to some target label, and this error is sent\nback through the network, updating the weights of the network so\nas to make a more accurate prediction given the input in the next\n2.3. Feed-forward Neural Networks\n25\nforward pass.10\nWe have already seen the largest part of the forward pass, as in\nthe examples with the AND and XOR functions in Section 2.3. The only\nremaining part of the forward pass, is how the error of the network\nis calculated – for this, a loss function is necessary.\nLoss functions\nThe loss functions used in NNs, generally fall into two classes – those\nused for classiﬁcation problems (i.e. when attempting to predict some\ndiscrete class label, out of a ﬁnite set of labels), and those used for\nregression problems (i.e. when attempting to predict some continu-\nous score). Classiﬁcation is one of the most common cases in NLP\n(e.g. in POS tagging, NER, language identiﬁcation, and so on).\nIn\nsuch cases, the activation function of the ﬁnal layer is the softmax\nfunction, which allows for interpreting the layer’s activations as a\nprobability distribution over the labels under consideration. Most\noften, the cross-entropy between this predicted probability distribu-\ntion and the target probability distribution is used to calculate the\nerror, or loss L, such that\nLcross−entropy(⃗ˆy, ⃗y) = −\nX\ni\n⃗yi log ⃗ˆyi,\n(2.4)\nwhere L denotes the loss function, ⃗y is the target probability distribu-\ntion over labels, ⃗ˆy is the model’s predicted model distribution given\nan input x. A high error thus indicates that the predicted probability\ndistribution is not consistent with the target probability distribution,\nand therefore changes should be made accordingly in the backward\npropagation pass.\n10This is referred to as backward propagation of errors, and is covered later in\nthis section.\n26\n2. An Introduction to Neural Networks\nAnother loss function, common in regression, is the squared error\nfunction, deﬁned as\nLsquared = (ˆy −y)2,\n(2.5)\nwhere ˆy is the predicted label, and y is the true label. This function is\ncommonly used in regression, and is especially handy for explaining\nbackpropagation, as in the next section.\nBackpropagation\nBackward propagation of errors, or backprop (Rumelhart et al., 1985;\nLeCun et al., 1998b), is an algorithm for calculating the gradient of\nthe loss function, for each weight. The gradient can, in turn, be used\nto update the weights by using an optimisation algorithm, such as\ngradient descent (discussed further in Section 2.3.3). Intuitively seen,\ngradient-based methods operate by viewing the errors as a geomet-\nric area, and use the slope of the area in which they are (i.e., the\ngradient) in order to shift weights towards obtaining an error in a\nminimum of this area, as in Figure 2.5.\nFigure 2.5: Non-convex error surface.\nBackprop relies on the fact that the partial derivative of the error\nof a certain weight Wj\ni, with respect to the loss function, can be eas-\n2.3. Feed-forward Neural Networks\n27\nily calculated if we know the partial derivative of the outputs in the\nlayer following that weight. It turns out that this is, indeed, the case,\nas the derivative of the output layer is quite easily obtained. The out-\nput error of a given unit, δi, is calculated as\nδi =\n\n\n\nαi(1 −αi)(αi −yi)\nfor output units i,\nαi(1 −αi)(αi\nP\nℓ∈L δℓWiℓ)\nfor other units i,\n(2.6)\nwhere αi is the activation of the current unit, yi is the target output,\nL is the collection of all units receiving input from the current unit,\nand Wiℓis the weight from the current unit to unit ℓ. Let us consider\na concrete example, and go through the forward pass, calculation of\nthe error, and the backward pass. The network in Figure 2.6 shows\na neural network with its weights.\nInput\nLayer 1\nLayer 2\nOutput\n0.2\n0.4\n0.7\n0.3\n0.4\n0.1\n0.7\n0.3\nFigure 2.6: A neural network with weights for our backpropagation\nexample.\nAssuming that the activation function used is the logistic function,\nthe following calculations hold:\nα1 = σ(x1 × 0.3 + x2 × 0.1) = σ(0.4 × 0.3 + 0.7 × 0.1) = 0.547,\nα2 = σ(x1 × 0.4 + x2 × 0.2) = σ(0.4 × 0.4 + 0.7 × 0.2) = 0.574,\nα3 = σ(α1 × 0.7 + α2 × 0.3) = σ(0.19 × 0.7 + 0.3 × 0.3) = 0.635.\n(2.7)\n28\n2. An Introduction to Neural Networks\nAssuming that the target output is y = 0.4, we can now calculate the\nerror. Applying equation 2.6, we can obtain the error of the output,\nnamely\nδ3 = α3(1 −α3)(α3 −y),\n= 0.635(1 −0.635)(0.635 −0.4),\n= 0.054.\n(2.8)\nThe errors of the two hidden units can also be calculated, yielding\nδ2 = α2(1 −α2)(α2δ3W2ℓ),\n= 0.547(1 −0.547)(0.547 × 0.027 × 0.3),\n= 0.001,\n(2.9)\nand\nδ1 = α1(1 −α1)(α1δ3W1ℓ),\n= 0.547(1 −0.547)(0.547 ∗0.027 ∗0.7),\n= 0.002.\n(2.10)\nWe now need to update the weights used, via gradient descent. This\ncan be done by shifting the weights with some constant with respect\nto the error obtained,\n∆Wij = −γαiδj,\n(2.11)\nwhere ∆Wij is the amount with which to change Wij (i.e., the weight\nbetween the ﬁring and receiving unit), γ is some learning rate, αi\nis the activation of the ﬁring unit, and and δj is the error of the re-\nceiving unit. Hence, if we set γ = 1, the changes of the weights are\ncalculated as\n∆Wx1,α1 = −γx1δ1 = −1 × 0.4 × 0.002 = −0.0008,\n∆Wx1,α2 = −γx1δ2 = −1 × 0.4 × 0.001 = −0.0004,\n∆Wx2,α1 = −γx2δ1 = −1 × 0.7 × 0.002 = −0.0014,\n∆Wx2,α2 = −γx2δ2 = −1 × 0.7 × 0.001 = −0.0007,\n∆Wα1,α3 = −γα1δ3 = −1 × 0.547 × 0.054 = −0.030,\n∆Wα2,α3 = −γα2δ3 = −1 × 0.574 × 0.054 = −0.031.\n(2.12)\n2.3. Feed-forward Neural Networks\n29\nUsing the new weights yields the following activations in the next\nforward pass, given the same input:\nα1 = σ(0.4 × (0.3 + ∆Wx1,α1) + 0.7 × (0.1 + ∆Wx2,α1) = 0.547,\nα2 = σ(0.4 × (0.4 + ∆Wx2,α1) + 0.7 × (0.2 + ∆Wx2,α2) = 0.574,\nα3 = σ(0.547 × (0.7 + ∆Wα1,α3) + 0.574 × (0.3 + ∆Wα2,α3) = 0.627,\n(2.13)\nand the output error\nδ3 = α3(1 −α3)(α3 −y),\n= 0.318(1 −0.318)(0.318 −0.4),\n= 0.053,\n(2.14)\nwhich is smaller than the previous error where δ3 = 0.054. This pro-\ncess is repeated with other training examples, until some criterion is\nreached, such as a suﬃciently low average loss.\nOptimisation Methods\nBackpropagation, as described in the previous section, can provide\nus with the derivatives of the error surface. This can be used in a\nvariety of ways to update the weights. What we just saw, in Equa-\ntion 2.11, is known as gradient descent. One of the most commonly\nused optimisation methods is Stochastic Gradient Descent (SGD). In\nSGD, a minibatch of n samples is drawn from the training set, the gra-\ndient is calculated based on this batch, and the weights are then up-\ndated accordingly (Bottou, 1998). Other algorithms, such as AdaGrad\n(Duchi et al., 2011) and RMSProp (Hinton, 2012), learn and adapt the\nlearning rate (γ) for each weight. Modifying the learning rate in this\nmanner can both increase the rate at which the error decreases, and\nlead to lower overall errors. A recent and increasingly popular op-\ntimisation method is Adam, which is similar to RMSProp and yields\nbetter results on a many problems (Kingma and Ba, 2014). The choice\n30\n2. An Introduction to Neural Networks\nof optimisation method is not all that straightforward, and no real\nconsensus exists for how this should be done (Schaul et al., 2014).\nHence, commonly, trial-and-error is applied in order to make this\nchoice, by experimentally investigating performance on a develop-\nment set.\nFinding the global minimum\nThe goal of an optimisation algorithm is to ﬁnd the global minimum,\nas shown in Figure 2.5. What so-called gradient-based optimisation\nalgorithms do, is to calculate the derivative with respect to this error\nsurface, and shift the weights so as to move towards the closest of all\nlocal minima. Such local minima can, however, be the source of a\nhost of problems, if the loss is high compared to the global minimum.\nThis is a frequently occurring issue, and it is possible to construct\nsmall neural networks in which this scenario appears (Sontag and\nSussmann, 1989; Brady et al., 1989; Gori and Tesi, 1992). It turns out,\nhowever, that practically speaking, when considering larger neural\nnetworks, it is not particularly important to ﬁnd the global minimum.\nThis has to do with the fact that, in the case of supervised learning\nwith deep neural networks, most local minima appear to have a low\nloss function value, roughly equivalent to that of the true global min-\nimum (Saxe et al., 2013; Dauphin et al., 2014; Goodfellow et al., 2015;\nChoromanska et al., 2015).\nParameter Initialisation\nThere are several methods for initialising the weights in a neural net-\nwork. Naively, one might think to set the all weight matrices W = 1,\nhowever due to how backpropagation works, this will result in all\nhidden units representing the same function, and receiving the ex-\nact same weight updates.\nTherefore, some random process is re-\nquired. Common methods include those introduced by Glorot and\n2.3. Feed-forward Neural Networks\n31\nBengio (2010), and Saxe et al. (2013).\nWhen employing the ReLU\nactivation function, He et al. (2015b) show that weights should be\ninitialised based on a Gaussian distribution with standard deviation\nq\n2\ndin , where din is the input dimensionality. In the case of recurrent\nneural networks, which are covered in Section 2.4, particular care\nneeds to be taken, and weight initialisation is often done using or-\nthogonal matrices (cf. Goodfellow et al.[p.404], 2016).\nRegularisation in Neural Networks\nOne of the most common problems when training an ML system in\ngeneral, is that of overﬁtting – and neural networks are no exception.\nOverﬁtting occurs when the network does not generalise to data out-\nside of the training set, while having a low loss on the training set\nitself. Generalisation is one of the most important parts of learning,\nas learning without generalisation is simply the memorisation of a\ntraining set. A model which has only memorised the training set is\nof little practical value, as it will most likely fail miserably on unseen\nexamples. In order to avoid overﬁtting, regularisation techniques\nare typically employed. The probably most common regularisation\ntechnique used today, is dropout (Srivastava et al., 2014). In dropout,\nevery activation has a probability p of not being included in the for-\nward and backward passes, during training. This procedure leads to\nsigniﬁcantly lower generalisation error, as the network needs to be\nmore robust, and less reliant on speciﬁc units. In the case of recur-\nrent neural networks, which are covered next, speciﬁc variants of\ndropout exist. such as recurrent dropout (Semeniuta et al., 2016), or\nvariational dropout (Gal and Ghahramani, 2016) in which the same\ndropout mask is used for each time step. Another commonly used\nmanner of regularisation is weight decay, in which the magnitudes of\nweights are decreased according to some criterion (Krogh and Hertz,\n1992).\n32\n2. An Introduction to Neural Networks\n2.4\nRecurrent Neural Networks\nAlthough FFNNs are suitable for many problems, they do not take\nthe structure of the input into account. Although it is possible to at-\ntempt to enforce this in such a network, this has several disadvan-\ntages, such as the fact that the amount of parameters which need to\nbe tuned can become prohibitively large. Luckily, there are architec-\ntures for dealing with structure, as this is not entirely unimportant\nwhen considering natural language. Two such approaches are cov-\nered in the following sections.\nRecurrent Neural Networks (RNNs) are an extension of feed-forward\nneural networks, which are designed for sequential data (Elman, 1990).\nThey can be thought of as a sequence of copies of the same FFNN,\neach with a connection to the following time step in the sequence,\nsharing parameters between time steps. RNNs take a sequence of ar-\nbitrary length as input (x1, x2, . . . , xt), and return another sequence\n(ˆy1, ˆy2, . . . , ˆyt). Each xt in the input sequence is a vector representa-\ntion of element nt in the sequence. Each ˆyt in the output sequence\ncan take advantage of information in the sequence up to step t in the\ninput sequence. This is illustrated in Figure 2.7. Each layer is shown\nas containing only one unit, which is here meant as an abstraction de-\npicting the entire internal representation of the RNN. The left side of\nthe ﬁgure depicts an FFNN with a loop, whereas the right side shows\nthe unrolled version of the network. The output of the hidden layer\nis passed as an input to the hidden layer in the next time step.\nAn RNN is essentially a group of FFNNs with connections to one\nanother. This connection is a sort of loop, going from the hidden\nlayer of the network at time xt to the hidden layer at xt+1. In other\nwords, ⃗ˆyt is calculated as\n2.4. Recurrent Neural Networks\n33\n1\n2\n3\nt\n=\ns1\ns2\ns3\nt\n1\n2\n3\nt\nt\n1\n2\n3\nt\nt\nFigure 2.7: A simple RNN with a connection from the hidden state\nof the previous time step to the current side step. Left side shows\nthe FFNN with the loop, whereas the right side shows the unrolled\nnetwork.\n⃗zt = Ws⃗xt + U⃗st−1,\n⃗st = σs(⃗zt),\n⃗ˆyt = σy(Wy⃗st),\n(2.15)\nwhere Ws is the matrix of weights for the current time step’s input\n(⃗xt), U is a weight matrix for the connections from the previous time\nstep, ⃗st is a state vector representing the history of the sequence, t\nis the index of the current time step, Wy is the matrix of weights for\nthe output, and the rest is deﬁned as for FFNNs. This is what is also\nreferred to as an Elman net, or a Simple RNN (Elman, 1990). The ad-\nvantage of having access to ⃗s, is that the network can take advantage\nof preceding information when outputting ⃗ˆyt. For instance, in the\ncase of POS tagging, if the current input is ﬂy, and the state vector\nshows that the previous word was to, we most likely want to output\nthe tag verb. Hence, in this way, the prediction at each time step is\nconditioned on the inputs in the entire preceding sequence. There\nare also variants of this, in which the net’s outputs are used to cal-\n34\n2. An Introduction to Neural Networks\nculate the state vector, as in the case of Jordan nets (Jordan, 1997),\nwhich is deﬁned such that\n⃗zt = Ws⃗xt + U⃗ˆyt−1.\n(2.16)\nAlthough RNNs, in theory, can learn long dependencies (i.e. that\nan output at a certain time step is dependant on the state at a time\nstep far back in the history), and can handle input sequences of ar-\nbitrary length, they are in practice heavily biased to the most recent\nitems in the given sequence, and thus diﬃcult to train on long se-\nquences with long dependencies (Bengio et al., 1994). For instance, in\nthe case of language modelling, given a sentence such as My mother\nis from Finland, so I speak ﬂuent . . ., it is quite likely that the omit-\nted word should be Finnish. However, as the distance between such\ndependencies grows, it becomes increasingly diﬃcult for an RNN to\nmake use of such contextual information. In general, this is because\ndeep neural networks suffer from having unstable gradients, as the\ngradients calculated by backprop (Section 2.3.3) are dependant on\nthe output of the network, which can be quite far away from the ﬁrst\nlayers in the network. One problem with this is that this can lead\nto vanishing gradients (i.e. the gradient becomes very small). This\nhappens since the gradient in early layers of the network are the re-\nsult of a large number of multiplication operators on numbers < 1.\nOne might consider the fact that, since these multiplications involve\nthe weights in the network, we might just set the weights to be re-\nally large. Although this might seem like a good idea, this will likely\nlead to the converse of the issue one is trying to avoid, namely that of\nexploding gradients. Since most common optimisation methods are\ngradient-based, this is problematic (see Section 2.3.3 for optimisation\ndetails).\n2.4. Recurrent Neural Networks\n35\nt\ntanh\nt-1\ntanh\nt+1\ntanh\nt\nt-1\nt+1\n+\n+\nFigure 2.8: Internal view of an RNN in three time steps.\ntanh\nX\nX\nX\ntanh\nX\nX\n  t-1\ntanh\nX\n+\nX\nX\ntanh\nX\nX\n   t+1\ntanh\nX\n+\nX\nX\ntanh\n+\nt\nt-1\nt+1\nFigure 2.9: Internal view of an LSTM in three time steps.\n36\n2. An Introduction to Neural Networks\n2.4.1\nLong Short-Term Memory\nPrevious work has attempted to solve this problem by adapting the\noptimisation method used (Bengio et al., 2013; Pascanu et al., 2013;\nSutskever et al., 2014), however the more successful approach has\nbeen to modify the neural network architecture itself. Because of\nsuch efforts, there are several types of RNNs speciﬁcally designed to\ncope with this issue, essentially by enforcing a type of protection of\nthe memory of the history of the input sequence, storing and main-\ntaining important features, while neglecting and forgetting unimpor-\ntant features. One such method, namely Long Short-Term Memory\n(LSTM), was described in Hochreiter and Schmidhuber (1997), and\nsaw an explosion in popularity around 2014, following several inﬂu-\nential papers (e.g. Sundermeyer et al. (2012); Sutskever et al. (2014);\nDyer et al. (2015)). An LSTM is an extension of RNNs, with mem-\nory cells, engineered to cope with the issue of unstable gradients,\nand have been shown to be able to capture long-range dependencies\n(Hochreiter and Schmidhuber, 1997; Cho, 2015). For an overview of\nthe many variations of LSTMs which appear in the literature, see Gr-\neff et al. (2016).\nWhereas an RNN only has a single internal layer (Figure 2.8), typ-\nically with a tanh (hyperbolic tangent) activation, an LSTM is some-\nwhat more complicated (Figure 2.9).\nAn LSTM contains gates, de-\nnoted in the ﬁgure by the σ layers, which are used to modify the\nextent to which old information is remembered or forgotten. Part\nof the explanation for LSTMs involves the observation that they, on\nthe surface, can be seen as a combination of Elman nets and Jordan\nnets, in that both the cell state and the hypothesis are passed between\n2.4. Recurrent Neural Networks\n37\nstates. In detail, an LSTM is implemented as follows\nft = σ(Wfxt + Uf ˆyt−1 + bf),\nit = σ(Wixt + Uiˆyt−1 + bi),\not = σ(Woxt + Uoˆyt−1 + bo),\nct = ft ◦ct−1 + it ◦σc(Wcxt + Ucˆyt−1 + bc),\nˆyt = ot ◦σ(ct),\n(2.17)\nwhere ft represents the output of the forget gate, it represents the\noutput of the input gate, ot represents the output of the output gate,\nct represents the cell state, ˆyt represents the output vector, W and U\nrepresent the weight matrices, xt represents the current input, and b\nrepresents bias units. Each of these parts are covered in detail in the\nfollowing sections.\nCell state\nThe cell state, ct, is the line coded in blue in Figure 2.9. It is similar\nto the state vector, ⃗s, in a simple RNN, but its content is maintained\nand protected by three gates. The forget gate’s output ft determines\nto which extent each feature in the cell state should be kept. This is\ndone by observing the current input xt, and the previous time step’s\noutput ˆyt−1.\nFor instance, say we have a POS tagger which at time t observes a\nword which could be either a noun or a verb (e.g. ﬂy). If the previous\nword was to, this would be useful to keep in mind in order to better\npredict the next tag. Following this time step, however, we might\nwant to forget about this word, when predicting the next tag.\nAdding information to the cell state happens in two steps. We ﬁrst\ndecide on which values in the cell state to update again observing xt\nand ˆyt−1, again deciding for each dimension the extent to which we\nwill add information. This is denoted by the input gate it. The vector\nwhich is added to the cell state, is calculated by passing xt and ˆyt−1\n38\n2. An Introduction to Neural Networks\nthrough a non-linearity, marked by tanh in the ﬁgure. In the POS\ntagging example, we want to add the information regarding the pre-\nceding determiner. These two vectors are then summed, resulting in\nour new cell state ct.\nOutput\nFinally, we need to output a value from the current time step. This\noutput is based on the cell state, ct, which is ﬁrst run through a non-\nlinearity (usually tanh), and ﬁltered again by ot, which also observes\nxt and ˆyt−1, when deciding on which dimensions to keep, and to what\nextent.\nGated Recurrent Units\nIn addition to the many LSTM variants (Greff et al., 2016), Gated\nRecurrent Units (GRUs) represent a different variant of gated RNNs\nwhich was independently developed to LSTMs and similar in both\npurpose and implementation (Cho et al., 2014). The main difference\nbetween LSTMs and GRUs is the fact that GRUs do not have separate\nmemory cells, and only include two gates – an update gate, and a\nreset gate (Chung et al., 2014). This in turn means that GRUs are\ncomputationally somewhat more eﬃcient than LSTMs. In practice,\nboth LSTMs and GRUs have been found to yield comparable results\n(Chung et al., 2014; Jozefowicz et al., 2015). On a general level, the per-\nformance of various gated RNN architectures is, at least in the case\nof large amounts of data, closely tied to the number of parameters\n(Collins et al., 2017; Melis et al., 2017).\nBi-directionality\nMany properties of language depend on both preceding and proceed-\ning contexts, so it is useful to have knowledge of both of these con-\ntexts simultaneously. This can be done by using a bi-directional RNN\n2.4. Recurrent Neural Networks\n39\nvariant, which makes both forward and backward passes over se-\nquences, allowing it to use both contexts simultaneously for the task\nat hand (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005;\nGoldberg, 2015). Bi-directional GRUs and LSTMs have been shown\nto yield high performance on several NLP tasks, such as POS tagging,\nnamed entity tagging, and chunking (Wang et al., 2015; Yang et al.,\n2016; Plank et al., 2016).\n2.4.2\nCommon use-cases of RNNs in NLP\nIn NLP, there are four general scenarios for producing some sort of\nanalysis for a given text. Consider that we have the following sen-\ntence as input:\n(2.18) I’m not fussy.11\nWe might want to analyse this unit as a whole, for instance in order\nto judge that the text is in English, and not in some other language, or\nto determine the native language of the person writing it, or the sen-\ntiment of the text itself. This can be referred to as a many-to-one sce-\nnario, since we have several smaller units (e.g. words or characters),\nwhich we want to translate into a single score or class, depending on\nthe task at hand.\nOn the other hand, we might want to analyse the sentence word\nby word, by assigning, e.g., a part-of-speech (POS) tag or a semantic\ntag to each word in the sentence. This can be referred to as a one-\nto-one scenario, since every single unit in the text (e.g. each word\ntoken) has a direct correspondence to a single tag.12\n11PMB 76/2032, Original source: Tatoeba\n12The term ’one-to-one’ is also used for simple classiﬁcation cases where there\nis no sentential context available. We see this as simply being a special case in\nwhich the sequence length = 1. This is equivalent to the relation between FFNNs\nand RNNs, in which an FFNN can be seen as a special case of RNNs (or vice versa).\n40\n2. An Introduction to Neural Networks\nWe might also want to carry out some task in which the sentence\nshould be translated to some other form, for instance translating the\nsentence to German, or some other language. If the sentence was\nwritten in some non-standard form of English, we might want to pro-\nduce a normalised version of the sentence, or in a different setting\nwe might want to generate an inﬂected form of some word in the\nsame language. This can be referred to as a many-to-many scenario,\nas there is no structural one-to-one correspondence between the in-\nput X and the output Y .\nA ﬁnal logically possible case, is the one-to-many scenario. This\nis a highly uncommon scenario, as it is not generally the case that\none tries to predict several things from an atomic unit. Although one\ncould argue that some tasks ﬁt this scenario, such as caption genera-\ntion, this is not really a one-to-many scenario, as the image is not an\natomic unit, but is read by the NN as a matrix of pixels.\nA schematic overview of the three relevant scenarios is given in\nFigure 2.10. The versatility of these three scenarios is evident when\nobserving the current NLP scene, in which common practise is to cast\na problem to ﬁt one of these scenarios, and to then throw a Bi-LSTM\nat the problem.\nMany-to-one\nMany NLP tasks deal with going from several smaller units to a single\nprediction. This essentially means that these units need to be com-\npressed into a single vector, onto which a softmax layer can be ap-\nplied in order to arrive at a probability distribution over the classes\nat hand (e.g. a set of languages to identify). For this type of problem,\na number of possibilities exist, such as\n1. Averaging the vectors representing each unit in the sentence\n(i.e. average pooling);\n2.4. Recurrent Neural Networks\n41\n4\ns1\ns2\ns3\n1\n2\n3\n4\n1\n2\n3\n4\nI\n‘m\nnot\nfussy\nEnglish\n1\n2\n3\n4\ns1\ns2\ns3\n1\n2\n3\n4\n1\n2\n3\n4\nI\n‘m\nnot\nfussy\nPRO\nNOT\nIST\nNOW\ns1\ns2\ns3\n1\n2\n3\n4\n1\n2\n3\n4\nI\n‘m\nnot\nfussy\n1\n2\n3\ns1\ns2\n1\n2\n3\nNo\nesigente\nsono\nFigure 2.10: Common scenarios in which RNNs are applied in NLP.\nFrom top to bottom: many-to-one, one-to-one, and many-to-many.\n42\n2. An Introduction to Neural Networks\n2. Applying an RNN and using the ﬁnal state output vector as a\nrepresentation of the sentence (depicted in Figure 2.10);\n3. Applying convolutions in order to arrive at a condensed repre-\nsentation.13\nApproach 1) is the most simplistic of these, and has been success-\nfully applied in previous work (e.g. Socher et al. (2013a); Zhang et al.\n(2015a)). The main advantage of this approach is indeed its simplicity,\nas calculating the mean of the vectorial representations of a sentence\nis both a very cheap operation, as well as an operation which allows\nfor the application of a simple FFNN on top of this vector. With this\napproach, however, the structure inherent in the natural language\nsignal is left unexploited.\nApproaches 2) and 3) both offer more expressive power as com-\npared to approach 1), as they both take advantage of the structure\ninherent in the input signal.\nThis is not the case when summing\nthe vectors, which is in a way analogous to a bag-of-words approach.\nStructure is naturally of paramount importance in natural language,\nso taking advantage of this is good. Although structure in natural lan-\nguage is generally hierarchical, even using the sequential structure is\nbetter than assuming no structure at all. There are, however, some\nrecent architectures which do encode the hierarchical structure of\nlanguage, such as tree LSTMs (Tai et al., 2015), and RNN-Grammars\n(Dyer et al., 2016).\nThese three approaches are meant to give an overview of some\nstraight-forward manners of obtaining such representations. Other,\nmore sophisticated approaches, include skip-thought vectors, in which\nsentence-level representations are learned with the objective of be-\ning able to predict surrounding sentences in a document (Kiros et al.,\n2015). A systematic overview of such methods is given by Hill et al.\n13Convolutional Neural Networks are described in Section 2.5.\n2.4. Recurrent Neural Networks\n43\n(2016), who conclude that the best suited approach depends on the\nintended application of such representations.\nAnother use case for this approach is when building hierarchical\nmodels, in the sense that one, e.g., may want to have word represen-\ntations which are aware of what is going on on a sub-word level. For\nthis, one might apply a many-to-one RNN, and use the ﬁnal state out-\nput vector as a word vector (Ballesteros et al., 2015; Plank et al., 2016).\nAlternatively, one can use convolutions to arrive at this type of word\nvector, as in dos Santos and Zadrozny (2014).\nOne-to-one\nThe one-to-one case is perhaps one of the most common scenarios\nin NLP. This covers tagging task scenarios, as well as simple classiﬁ-\ncation scenarios. Many NLP tagging tasks have seen relatively large\nimprovements when applying variants of RNNs similarly to what is\ndepicted in Figure 2.10. Recently, gated variants such as LSTMs and\nGRUs, often in a bi-directional incarnation are applied (Wang et al.,\n2015; Huang et al., 2015; Yang et al., 2016; Plank et al., 2016). Such\nRNNs are highly suited for this type of task, as it is highly informative\nfor, e.g., POS tagging to know which words occur both before and af-\nter the word at hand. Such dependencies might also have quite large\nspans, which both LSTMs and GRUs are able to capture well. To con-\ntrast with older feature-based models, it would require a fair bit of\nfeature engineering to decide on what types of spans to include in\nfeature representations, lest one wishes to suffer from the sparsity\nof simply using n-gram features with large values of n.\nMany-to-many\nThis paradigm is also what is frequently referred to as an encoder-\ndecoder architecture in the literature, or sequence-to-sequence learn-\ning problems (Bahdanau et al., 2015; Sutskever et al., 2014). A fre-\n44\n2. An Introduction to Neural Networks\nquent approach here, for instance in machine translation, is to apply\nan RNN from which one takes the ﬁnal time step’s output to be a\nrepresentation of the entire sentence, as represented in Figure 2.10,\nwhich may seem like a bold thing to do.14 It turns out that this is in\nfact often not suﬃcient, although one can obtain surprisingly good\ntranslations this way. However, results improve dramatically when\ngoing a step further by incorporating an attentional mechanisms. This\nis not focussed upon in this thesis, and will not be explained in full\ndetail. Essentially, an attention mechanism can learn which parts of\nthe source sentence to attend to, when producing the target sentence\ntranslation. For instance, such a mechanism might learn an implicit\nweighted word-alignment between the source and target sentences,\nthus facilitating translation.\nMany NLP tasks can be solved with a many-to-many approach.\nMachine translation has already been mentioned, and has in no small\ndegree been the driving force behind research in this direction. Apart\nfrom this, the approach has been applied to morphological inﬂection\n(Kann and Schütze, 2016; Cotterell et al., 2016; Östling and Bjerva,\n2017; Cotterell et al., 2017), AMR parsing (Barzdins and Gosko, 2016;\nKonstas et al., 2017; van Noord and Bos, 2017b,a), language modelling\n(e.g. Vinyals et al., 2015), generation of Chinese poetry (Yi et al., 2016),\nhistorical text normalisation (Korchagina, 2017), and a whole host of\nother tasks.\n2.5\nConvolutional Neural Networks\nCertain machine learning problems, such as image recognition, deal\nwith input data in which spatial relationships are of utmost impor-\ntance. While simpler image recognition problems, such as handwrit-\n14You can’t cram the meaning of a whole sentence into a single vector!\n–Ray Mooney, as communicated by Kyunghyun Cho in his NoDaLiDa 2017 keynote\n(https://play.gu.se/media/1_xt08m5je)\n2.5. Convolutional Neural Networks\n45\nten digit recognition, can be carried out relatively successfully with\nsimple FFNNs, this is often not suﬃcient. Recall that the input for an\nFFNN is simply a single vector ⃗x, meaning that the network has no\nnotion of adjacency between, e.g., two pixels. A Convolutional Neu-\nral Network (CNN) is a type of network explicitly designed to take\nadvantage of the spatial structure of its input. The origins of CNNs\ngo back to the 1970s, but the seminal paper for modern CNNs is con-\nsidered to be LeCun et al. (1998a), although other work exists in the\nsame direction (e.g., LeCun et al. (1989); Waibel et al. (1989)). CNNs\nhave been used extensively in NLP, and can in many cases be used in-\nstead of an RNN (contrast, e.g., dos Santos and Zadrozny (2014) who\nuse CNNs for character-based word representations, and Plank et al.\n(2016) who use RNNs for the same purpose).\nAlthough NLP is the focus of this thesis, we will approach CNNs\nfrom an image recognition perspective, as this is somewhat more in-\ntuitive. This is in part due to the fact that image recognition was the\nintended application of CNNs upon their conception. On a general\nlevel, convolutions can be carried out on input of arbitrary dimen-\nsionality. As mentioned, two-dimensional input (e.g. images) were\nthe original target for CNNs. More recent work has extended this to\nthree-dimensional input (e.g. videos). In the case of NLP, it is often\nthe case that one-dimensional input is used, for instance applying a\nCNN to a text string. There are three basic notions which CNNs rely\nupon: local receptive ﬁelds, weight sharing, and pooling.\n2.5.1\nLocal receptive ﬁelds\nIn the case of image recognition, an image of n×n pixels can be coded\nas an input layer of n × n units.15 In a CNN, this input is processed\nby sliding a window of size m×m across this image. This window, or\npatch, is known as a local receptive ﬁeld. After passing this window\n15This is assuming greyscale, i.e., one value per pixel.\n46\n2. An Introduction to Neural Networks\nover the input image, the following layer contains a representation\nbased on m × m sized slices of the input image. Intuitively, this can\nbe seen as blurring the input image somewhat, as the spatial dimen-\nsions of the image are generally reduced through this process.\nThe length with which this window moves is referred to as its\nstride, and is most often set to 1, meaning that the window simply\nshifts by one pixel at a time. Although stride lengths of 2 and 3 are en-\ncountered in the literature, it is fairly uncommon to see larger stride\nlengths than this. Figure 2.11 shows a convolution, where n = 4,\nm = 2, a stride of 2 is used, which yields a new layer with size 2 × 2.\n2.5.2\nWeight sharing\nA key notion of CNNs is the fact that weights are shared between\neach such local receptive ﬁeld, and the units to which they are at-\ntached. Hence, in our example, rather than having to learn n×i = 64\nweights (where i = n × n is the total number of units in the ﬁrst\nhidden layer), as in an FFNN, only m × m = 4 parameters need to\nbe learned. Therefore, all units in the ﬁrst hidden layer capture the\nsame type of features from the input image in various locations. For\ninstance, imagine you want to identify whether a picture contains\ncats (as in Figure 2.12). In this ﬁgure, units in the ﬁrst hidden layer\nmight encode some sort of cat detector. The fact that weights are\nshared in this manner, results in CNNs being robust to translation\ninvariance. This means that a feature is free to occur in different\nregions in the input image. Intuitively, taking our cat detector as an\nexample, it is naturally the case that a cat is a cat, regardless of where\nin the image it happens to hide.\nEach such cat detector, is referred to as a feature map, or a chan-\nnel.16 For a CNN to be useful, normally more than one feature map\n16The channel terminology makes sense when considering an input image in,\ne.g., RGB formatting, in which the intensities of each colour is represented in a\nseparate colour channel.\n2.5. Convolutional Neural Networks\n47\nFigure 2.11: Illustration of a local receptive ﬁeld of size 2x2, which\nresults in a new image of size 2x2 due to the stride length being 2.\nFigure 2.12: Weight sharing example with a cat. The dotted line rep-\nresents the local receptive ﬁeld used, the ﬁrst square represents the\nentire input domain, and the second square represents the following\nconvolutional layer.\n48\n2. An Introduction to Neural Networks\nis learnt. That is to say, while the ﬁgures shown so far only show a\nsingle feature map, an input image is normally mapped to several\nsmaller images. As an example, another feature map in Figure 2.12\nmight learn a dog detector. A more realistic example can be found\nin facial recognition, in which one feature map might learn to detect\neyes, while another learns to detect ears, and yet another learns to\ndetect mouths. Local receptive ﬁelds generally speaking look at all\nfeature maps of the previous layers, hence the combination of eyes,\nears, and mouths might be used to learn a feature map representing\nan entire face.\nThe fact that we generally map to several feature maps means\nthat, at each layer, the spatial size of the image shrinks (i.e. m < n),\nwhile the depth of the image increases, as shown in Figure 2.13. Fi-\nnally, following a series of convolutional layers, it is common practise\nto attach an FFNN prior to outputting predictions.\nIn NLP, the situation is somewhat different, as we normally do\nnot have an image as input, but rather some sort of textual represen-\ntation. Commonly, this will either be a string of words, characters,\nor bytes. An intuition for how this works, is that something resem-\nbling an n-gram feature detector is learnt given a window. This type\nof approach has been applied successfully in various tasks, for in-\nstance to obtain word-level representations which take advantage of\nsub-word information (dos Santos and Zadrozny, 2014; Bjerva et al.,\n2016b), and for sentence classiﬁcation (Kim, 2014).\n2.5.3\nPooling\nA pooling layer takes a feature map and condenses this into a smaller\nfeature map. Each unit in a pooling layer summarises a region in the\nprevious layer, generally using a simple arithmetic operation. Fre-\nquently, operations like maximum pooling (max pooling) or average\npooling are used (Zhou and Chellappa, 1988). These operations take,\ne.g., the maximum of some region to be a representation of that en-\n2.5. Convolutional Neural Networks\n49\nFigure 2.13: General CNN structure. Each layer shrinks the width\nand height of the input image, and increases the number of feature\nmaps. The grey regions denote the sizes of the local receptive ﬁelds.\ntire region, thus reducing dimensionality by essentially applying sim-\nple non-linear downsampling. Max pooling can thus be seen as a\nway for each max pooling unit to encode whether or not a feature\nfrom the previous layer was found anywhere in the region which\nthe unit covers. The intuition is that the downsampled version of the\nfeature map, which yields feature locations which are rough, rather\nthan precise, is suﬃcient in combination with the relative location\nto other such downsampled features. Importantly, this operation re-\nduces the dimensionality of feature maps, thus reducing the number\nof parameters needed in later layers.\n50\n2. An Introduction to Neural Networks\n+\nFigure 2.14: Illustration of a residual network (right) as compared to\na standard network without skip connections (left). The skip connec-\ntion here passes the input vector ⃗x and adds this to the activations ⃗α3,\nthus providing the network with a shortcut. The squares represent\nan abstract block of weights, such as, e.g., a fully connected layer in\nan FFNN, a convolutional block in a CNN, or an LSTM cell.\n2.6\nResidual Networks\nResidual Networks (ResNets) deﬁne a special class of networks with\nskip connections between layers, as depicted in Figure 2.14. This fa-\ncilitates the training of deeper networks, as these skip connections\nease the propagation of errors back to earlier layers in the network.\nSuch skip connections are referred to as residual connections,\nand can be expressed as\nyl = h(xl) + F(xl, Wl),\nxl+1 = f(yl),\n(2.19)\nwhere xl and xl+1 are the input and output of the l-th layer, Wl is the\n2.7. Neural Networks and the Human Brain\n51\nweights for the l-th layer, and F is a residual function (He et al., 2016)\nsuch as the identity function (He et al., 2015a), which we also use in\nour experiments. Although ResNets were developed for the use in\nCNNs, the skip connections are currently being used, e.g., in LSTMs\n(Wu et al., 2016). ResNets can be intuitively understood by thinking\nof residual functions as paths through which information can prop-\nagate easily. This means that, in every layer, a ResNet learns more\ncomplex feature combinations, which it combines with the shallower\nrepresentation from the previous layer. This architecture allows for\nthe construction of much deeper networks. ResNets have recently\nbeen found to yield impressive performance in image recognition\ntasks, with networks as deep as 1001 layers (He et al., 2015a, 2016),\nand are thus an interesting and effective alternative to simply stack-\ning layers. Another useful feature of ResNets is that they act as en-\nsembles of relatively shallow networks, which may help to explain\nwhy they are relatively robust to overﬁtting in spite of their large\nnumber of parameters (Veit et al., 2016).\nResNets have recently been applied in NLP to morphological re-\ninﬂection (Östling, 2016), language identiﬁcation (Bjerva, 2016), sen-\ntiment analysis and text categorisation (Conneau et al., 2016), seman-\ntic tagging (Bjerva et al., 2016b), as well as machine translation (Wu\net al., 2016). Recently proposed variants include wide residual net-\nworks, with relatively wide convolutional blocks, showing that resnets\ndo not necessarily need to be as deep as the 1001 layers used in pre-\nvious work (Zagoruyko and Komodakis, 2016).\n2.7\nNeural Networks and the Human Brain\nWhile there are numerous reasons to use neural networks, there are\ncamps which might argue for applying NNs because they are ’bio-\nlogically motivated’. While it can be tempting to use the conceptual\nmetaphor (Lakoff and Johnson, 1980) of NEURAL NETWORKS ARE THE\n52\n2. An Introduction to Neural Networks\nBRAIN, this can be rather misleading.\nWhile usage of misleading\nmetaphors can seem innocent, it has been shown that they do affect\nreasoning (Thibodeau and Boroditsky, 2013). Therefore, a mislead-\ning metaphor should certainly not be used as an argument for the\nusage of neural networks – there are plenty of other reasons for that.\nNow, you might ask, is this metaphor really as misleading as it\nseems? At best, neural networks (as used in NLP) are a mere carica-\nture of the human brain. On a physiological level, there is evidence\nthat neurons do more than simply outputting some activation value\nbased on a weighting of its inputs. For instance, recent research has\nshown that a single neuron can encode temporal response patterns,\nwithout relying on temporal information in input signals. Hence, the\nnature of how neurons work is quite different from what is encoded\nin a neural network, for instance in terms of information storage ca-\npacity (Jirenhed et al., 2017). There is in fact compelling evidence\nthat memory is not coded in (sets of) synapses, but rather internally\nin neurons (cf. Gallistel and King, 2011, and Gallistel, 2016 and ref-\nerences therein, notably Johansson et al. (2014)). Additionally, back-\npropagation is not biologically plausible, although there is work on\nmaking biologically plausible neural networks (Bengio et al., 2015).\nConvolutional Neural Networks and the Brain\nSimilarly to the ReLUs discussed in Section 2.3.2, CNNs are biologi-\ncally inspired. When CNNs were invented (LeCun et al., 1998a), this\nwas inspired by work which proposed an explanation for how the\nworld is visually perceived by mammals (Hubel and Wiesel, 1959,\n1962, 1968). The attempts to reverse-engineer a similar mechanism,\nas in CNNs, have proven fruitful, as CNNs are indeed highly suitable\nfor image recognition. Furthermore, recent research has found some\ncorrelations between the representations used by a CNN, and those\nencoded in the brain in a study where the CNN could identify which\nimage a human participant was looking at roughly 20% of the time\n2.8. Summary\n53\n(Seeliger et al., 2017). However, it remains to be seen whether any-\nthing similar to this is even plausible for natural language.\n2.8\nSummary\nIn this chapter, an intuitive and theoretically supported overview of\nneural networks was given, including a practical overview NLP. We\nhave seen that many common NLP problems can be classiﬁed into\nthree categories: one-to-one, one-to-many, and many-to-many. Ap-\npropriate deep learning architectures suitable for each of these cate-\ngories were suggested.\nWhile this chapter is meant to be a suﬃcient introduction to neu-\nral networks to understand this thesis, it is by no means a complete\naccount of the topic. For a more in-depth description of neural net-\nworks in general, I refer the readers to Goodfellow et al. (2016). For\na primer which is more geared towards NLP, see Goldberg (2015).\nCHAPTER 3\nMultitask Learning\nand Multilingual Learning\nAbstract|In this chapter, we build upon the background knowledge\nof neural networks presented in the previous chapter. We will focus\non different, but highly related, paradigms – multitask learning, and\nmultilingual model transfer. Multitask learning is ﬁrst presented in a\ngeneral context, and then in the context of neural networks, which is the\nprimary focus of this thesis. We will then look at multilingual approaches\nin NLP, again ﬁrst in a general context, and then in the context of model\ntransfer with multilingual word representations, which is the secondary\nfocus of this thesis. In this thesis, we consider the ﬁrst setting in Part\nII, the second setting in Part III, and include an outlook for a combined\nmultilingual/multitask paradigm in Part IV.\n56\n3. Multitask Learning and Multilingual Learning\n3.1\nMultitask Learning\nIn Natural Language Processing (NLP), and machine learning (ML) in\ngeneral, the focus is generally on solving a single task at a time. For\ninstance, one might invest signiﬁcant amounts of time in making a\nPart-of-Speech tagger or a parser. However, fact is that many tasks\nare related to one another. The aim of multitask learning (MTL) is to\ntake advantage of this fact, by attempting to solve several tasks simul-\ntaneously, while taking advantage of the overlapping information in\nthe training signals of related tasks (Caruana, 1993, 1997). When the\ntasks are related to each other, this approach can improve generali-\nsation, partially since it provides a source of inductive bias, and since\nit allows for leveraging larger amounts of more diverse data.1 Addi-\ntionally, since related tasks can often make use of similar represen-\ntations, this can lead to the tasks being learnt even better than when\ntraining on a single task in isolation.\nThe use of MTL is skyrocketing in NLP, and has been applied suc-\ncessfully to a wide range of tasks, for instance sequence labelling\nsuch as POS tagging (Collobert and Weston, 2008; Plank et al., 2016),\nsemantic tagging (Bjerva et al., 2016b), as well as chunking and su-\npertagging (Søgaard and Goldberg, 2016). In addition to this, it is\nthe primary focus of this thesis, and having some background knowl-\nedge on this will be useful for the following chapters. The ﬁrst part\nof this chapter is an attempt at providing an understanding of what\nMTL is and how it is applied. While some general MTL scenarios are\ncovered, the focus will be on MTL in the context of neural networks,\nand in the context of NLP.\n1Generally speaking, it is beneﬁcial to have access to more data when training\nan ML model.\n3.1. Multitask Learning\n57\n3.1.1\nNon-neural Multitask Learning\nBefore going into MTL in neural networks (NNs), we ﬁrst take a look\nat the usage of this paradigm in other frameworks. Generally speak-\ning, we seek to exploit the fact that there are many tasks which are\nsomehow related to one another (Caruana, 1993, 1997; Thrun and\nPratt, 1998). For instance, MTL can have the role of being a distant\nsupervision signal, in the sense that the tasks used might be fairly\ndistantly related.\nAdditionally, since MTL plays the role as a reg-\nulariser (see Chapter 2), and lowers the risk of overﬁtting (Baxter,\n1997; Baxter et al., 2000), MTL often improves generalisation. This\nis in part because MTL reduces Rademacher complexity (Baxter et al.,\n2000; Maurer, 2006).2 Furthermore, MTL will push the weights of a\nmodel towards representations which are useful for more than one\ntask. Finally, MTL can be seen as a method of dataset augmentation,\nas it allows for using more data than when only considering a single\ntask at a time.\nA commonly made assumption in MTL is that only a handful of pa-\nrameters or weights (see Chapter 2) ought to be shared between tasks,\nand conversely that most parameters should not be shared (Argyriou\net al., 2007). This can intuitively be understood by considering that\nonly a few features useful for a task t1 might be useful for another\ntask t2. For instance, imagine that we are building a joint POS tagger\nand language identiﬁcation system. A feature capturing capitalised\nwords preceded by a determiner will both be a decent indicator of\nthe language being, e.g., German, as well as that the capitalised word\nis a noun. Other features, on the other hand, such as one indicating\nthat the language is likely to be Norwegian or Danish if the letter ø\nis encountered, is not likely to be beneﬁcial for POS tagging at all. In\nother words, this type of parameter sparsity can be phrased as that\n2A lower Rademacher complexity essentially indicates that a class of functions\nis easier to learn.\n58\n3. Multitask Learning and Multilingual Learning\nmost parameters should not be shared, as many parameters are task\nspeciﬁc. In this type of approach, all shared parameters are generally\nconsidered by all tasks involved. This puts the system at a relatively\nlarge risk of negative transfer, if one tries to combine this approach\nwith tasks which are only slightly related. In NLP we are often in-\nterested in exploiting even relatively weak training signals, which\nmakes this particularly problematic.\nAnother approach is to learn clusters of tasks, which allows for\nletting related tasks share certain parameters, and relatively unre-\nlated tasks perhaps only a few. Such approaches have in common\nthat they assume that the parameters which are beneﬁcial for each\nother are geometrically close to one another in n-dimensional space\n(Evgeniou and Pontil, 2004; Kang et al., 2011). Other work has come\nup with other deﬁnitions of task similarities. For instance, Thrun\nand O’Sullivan (1995) consider two tasks to be similar simply if one\nimproves performance on the other. While other approaches to MTL\nhave been used in the past, such as Daumé III (2009) who approach\nMTL from a Bayesian perspective, and Toutanova et al. (2005) who\ntrain a joint classiﬁer for semantic role labelling with automatically\ngenerated auxiliary tasks, the perhaps most popular approach in NLP\nis parameter sharing in NNs.\n3.1.2\nNeural Multitask Learning\nWe now turn to the main method used in this thesis, namely neural\nMTL. There are two main approaches to this, differing in the manner\nin which parameters are shared – hard and soft parameter sharing.\nCurrently, the less popular variant of the two in NLP is soft param-\neter sharing, and will not be covered in detail. Brieﬂy put, in this\nsetting, parameters are constrained in a similar manner to the pa-\nrameter sparsity approach. That is to say, the parameters between\ntasks are encouraged to be similar to one another, which allows for\nsome transfer between tasks, or between languages (Duong et al.,\n3.1. Multitask Learning\n59\n2015). However, as parameters are not explicitly shared between\ntasks, the risks of negative transfer are relatively low in this setting.\nThis approach is not explored in this thesis as hard parameter shar-\ning offers several advantages, including ease of implementation, and\ncomputational effectivity, as the amount of parameters is kept almost\nconstant as compared to having a single task.\nHard parameter sharing is currently more common, perhaps mainly\ndue to the ease with which a neural MTL system with several tasks\ncan be created.\nThis is the type of MTL discussed in the seminal\nworks by Caruana (1993, 1997). In this thesis we consider research\nquestions tied to this type of MTL in the context of NLP, partially due\nto the versatility of the paradigm. Apart from allowing for consider-\ning data from several tasks simultaneously, even corpora in different\nlanguages might be used in this approach, given some sort of uniﬁed\ninput representations.3 Then, if the output labels between tasks cor-\nrelate with one another to some extent, it seems quite intuitive that\nthis approach should be beneﬁcial.\nIn NLP, MTL is generally approached from the perspective that\nthere is some main task, i.e., the task in which we are interested, and\nsome auxiliary task, which should improve the main task. It is impor-\ntant to note, however, that these labels are quite arbitrary.4 There is\nnot necessarily anything to distinguish a main task from an auxiliary\ntask in an NN. One might lower the weighting of the auxiliary task\n(i.e. multiply the loss for each batch by some λ < 1), but this strategy\nappears to be relatively rare in the literature.\nA common way of implementing hard parameter sharing, is to\nhave a stack of layers for which weights are updated with respect\nto all tasks, with at least two output layers, each with task-speciﬁc\nweights (see Figure 3.1). Concretely, consider that we have t corpora\n3This is covered further in the second half of this chapter.\n4The exception being cases in which the performance on the auxiliary task is\ndisregarded in favour of the main task performance.\n60\n3. Multitask Learning and Multilingual Learning\nInput\nLayer 1\nLayer 2\nOutput\nFigure 3.1: Common MTL architecture (bias units omitted for clarity).\nwith different annotations, each containing pairs of input and out-\nput sequences (⃗x, ⃗yt) for a single task. While the inputs x will largely\nbe part of the same vocabulary, and can be shared across tasks, the\ntag sets used, and therefore the labels (y0 . . . yt) differ. Note that the\nvocabularies in the tasks at hand do not necessarily need to over-\nlap, but when considering a single natural language, this tends to be\nthe case. A common approach when training is to randomly sam-\nple such sequence pairs, predict a label distribution ⃗ˆyt, and update\nmodel parameters as calculated by the loss relative to the true label\ndistribution ⃗yt with backpropagation (see Chapter 2 for an overview\n3.1. Multitask Learning\n61\nof this). Each task t has a task-speciﬁc classiﬁer (ft=main, ft=aux) with\nits own weight matrix (Wmain, Waux). The output of the task-speciﬁc\nlayer is then calculated using the softmax function (cf. Section 2.3.2),\nsuch that\n⃗ˆyt = softmax(Wt⃗αn + b),\n(3.1)\nwhere ⃗αn denotes the activations of the layer before the output layer.\nThis architecture is common in NLP, with weights typically shared\nbetween the main and auxiliary task at all layers, up to the task-\nspeciﬁc classiﬁcation layer (i.e. the output layer).\nA multitude of\nother possibilities do exist, as the task-speciﬁc output layers can be\nattached anywhere in the network. This can be advantageous, as\nSøgaard and Goldberg (2016) found that including the lower-level\ntask supervision at lower levels in the network was useful, in the\ncase of using the low-level task of POS tagging in combination with\nCCG supertagging (i.e. assigning CCG lexical categories). Most related\nwork, including the experiments in this thesis, apply multitask learn-\ning akin to what is shown in Figure 3.1.\nNeural Multitask Learning in Natural Language Processing\nHard parameter sharing in NNs is the target of considerable atten-\ntion in the recent NLP literature. Practically speaking, there appear\nto be two main approaches to MTL in the NLP literature. Some work,\nsuch as Ando and Zhang (2005), Collobert and Weston (2008), Sø-\ngaard and Goldberg (2016), Plank et al. (2016), Bjerva et al. (2016b),\nand Augenstein and Søgaard (2017) take the approach of exploiting\nseemingly related NLP tasks, based on some linguistic annotation.\nOther work, e.g., Plank (2016), and Klerke et al. (2016), take the ap-\nproach of exploiting data from non-linguistic sources (keystroke data\nand eye gaze data, respectively). While these approaches are both\nuseful and interesting, the focus of this thesis is the ﬁrst approach,\n62\n3. Multitask Learning and Multilingual Learning\nspeciﬁcally in which an NLP sequence prediction task is used as an\nauxiliary task for some other NLP sequence prediction task. This is\npartially motivated by the fact that using a word-level input for all\ntasks, allows for a one-to-one mapping between labels in different\ntag sets (given a speciﬁc token in context), which in turn opens up\nfor the information-theoretic approach considered in Chapter 5.\n3.1.3\nEffectivity of Multitask Learning\nPlenty of studies demonstrate the success of MTL, such as in com-\nputer vision (Torralba et al., 2007; Loeff and Farhadi, 2008; Quattoni\net al., 2008), genomics (Obozinski et al., 2010), and the aforemen-\ntioned NLP studies. Apart from relatively straight-forward results\nshowing that MTL is often beneﬁcial, efforts have been put into ex-\nperimentally investigating when and why MTL is advantageous in\nNLP. Martínez Alonso and Plank (2017) look at a collection of seman-\ntic main tasks while using morphosyntactic and frequency-based tasks\nas auxiliary tasks. They ﬁnd that the success of an auxiliary tasks de-\npends on the distribution of the auxiliary task labels, e.g., the distri-\nbution’s entropy and kurtosis.5 Bingel and Søgaard (2017) present a\nlarge systematic study of MTL in a collection of NLP tasks. They ﬁnd\nthat certain dataset characteristics are predictors of auxiliary task\neffectivity, corroborating the ﬁndings of Martínez Alonso and Plank\n(2017), and also show that MTL can help target tasks out of local min-\nima in the optimisation process. In Bjerva (2017b), it is argued that\nentropy is not suﬃcient for explaining auxiliary task effectivity, and\nthat measures which take the joint distribution between tasks into ac-\ncount offer more explanatory value (this is elaborated in Chapter 5).\nIn terms of data sizes Benton et al. (2017) suggest that MTL is ef-\nfective given limited training data for the main task. Luong et al.\n(2015), however, highlight that the auxiliary task data should not out-\n5The kurtosis of a distribution is essentially a measure of its tailedness.\n3.2. Multilingual Learning\n63\nsize the main task data – this is contradicted by Augenstein and Sø-\ngaard (2017), who highlight the usefulness of an auxiliary task when\nabundant data is available for such a task, and little for the main\ntask. Finally, Mou et al. (2016) investigate transferability of neural\nnetwork parameters, by attempting to initialise a network for a main\ntask with weights which are pre-trained on an auxiliary task, and\nhighlight the importance of similarities between tasks in such a set-\nting. Finally, a promising recent innovation is that of sluice networks,\nin which a NN learns which parts of hidden layers to share between\ntasks, and to what extent (Ruder et al., 2017).\n3.1.4\nWhen MTL fails\nThe cases in which MTL does not work are also deserving of attention.\nWhile up until now we have assumed that applying MTL is a piece of\ncake, there are times when one adds an auxiliary task, causing the\nsystem to collapse like a house of cards. This type of performance\nloss is referred to as negative transfer, and can occur when two un-\nrelated tasks share parameters. This is generally something to avoid,\nas there are few, if any, advantages to worsening the generalisation\nability of the network. However, such results are rarely shared in the\ncommunity, in part due to the ﬁle drawer problem (Rosenthal, 1979).\nIn short, the problem is that it is impossible to access, or even know\nof, studies which have been conducted and not published. In the\ncase of MTL, this issue might be alleviated by publishing results on\nall auxiliary tasks experimented with, even if only one or two such\ntasks improved performance.\n3.2\nMultilingual Learning\nIn the second half of this chapter, we turn to multilingual approaches.\nMany languages are similar to each other in some respect, and sim-\nilarly to related tasks, this fact can also be exploited in order to im-\n64\n3. Multitask Learning and Multilingual Learning\nprove model performance with respect to, e.g., a given language. While\nthere are many approaches to multilingual NLP, with various use\ncases, the focus in this thesis is on model transfer, in which a sin-\ngle model is shared between languages. We will nonetheless begin\nwith an overview of the most common approaches.\nAs an example, consider NLP tagging tasks, which can be summed\nup as learning to assign a sequence of tags from a tag set t to a se-\nquence of tokens in language l. In cross-lingual multitask NLP set-\ntings, there are many l/t pairs which do not have any annotated data.\nFor instance, there is (at the time of writing) no annotated data for\nWelsh in the Universal Dependencies (Nivre et al., 2017). However,\nmany NLP systems require input data from speciﬁc tag sets.\nFor\ninstance, the Stanford Neural Network Dependency parser requires\nPOS tags in its input (Chen and Manning, 2014), whereas the seman-\ntic parser Boxer requires semantic tags in its input (Bos, 2008; Abzian-\nidze et al., 2017). Hence, for such tools to be applicable in multilin-\ngual settings, the tags they rely on need to be available for other lan-\nguages as well, which highlights the importance of approaches which\ndeal with this. There are three frequently used approaches to solving\nthis problem:\n1. human annotation;\n2. annotation projection;\n3. model transfer.\nAlthough serious efforts have gone into furthering these approaches,\nthey all have considerable drawbacks. In brief, human annotation\nis time consuming and expensive, annotation projection is only ap-\nplicable to texts which are both translated and aligned, and model\ntransfer is generally only used in mono-lingual or mono-task settings.\n3.2. Multilingual Learning\n65\n3.2.1\nHuman Annotation\nGenerally speaking, annotating data manually is a very expensive\nand time-consuming manner of, e.g., producing some sort of linguis-\ntic labels for a sentence. Although the process can be alleviated with\ngamiﬁcation (Venhuizen et al., 2013; Chamberlain, 2014; Jurgens and\nNavigli, 2014; Bos and Nissim, 2015), considerable time and effort still\nneeds to be invested into creating such crowd-sourcing systems.\n3.2.2\nAnnotation Projection\nGiven an annotated sentence in a source language and a translation\nof that sentence in a target language, it is possible to transfer, or\nproject, the annotation from the source language to the target lan-\nguage. This approach is known as annotation projection, and relies\non having access to parallel text for which at least one source lan-\nguage is annotated (Yarowsky et al., 2001; Hwa et al., 2005). Usually,\nword alignments are used in order to project linguistic labels from\nsource to target. The resulting annotations can then be used to train\na new monolingual system for the target language(s). This approach\nhas been applied successfully to various tasks, primarily syntactic\nparsing (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015;\nAgić et al., 2016), POS tagging (Yarowsky et al., 2001), and recently\nalso semantic parsing (Evang and Bos, 2016; Evang, 2016).\nAnnotation projection has two main drawbacks. Primarily, it is\nonly applicable to texts which are both translated and aligned, whereas\nthe majority of available texts are monolingual. Furthermore, this\napproach relies heavily on the quality of the automatic word align-\nments. Word-aligning parallel text is not always successful, for in-\nstance with very dissimilar languages, insuﬃcient statistics, or bad\ntranslations (Östling, 2014, 2015). Another approach for annotation\nprojection relies on automatic translation. This works by applying a\nmachine translation (MT) system to generate a parallel text for which\n66\n3. Multitask Learning and Multilingual Learning\nsource language annotation exists (Tiedemann et al., 2014). In other\nwords, in addition to the diﬃculties of the annotation projection ap-\nproach, this method places high requirements on availability of par-\nallel texts for training an MT model. In addition to these prerequi-\nsites, the involvement of a fully-ﬂedged MT system in an annotation\npipeline, will in itself increase its complexity severely.\n3.2.3\nModel Transfer\nModel transfer deals with learning a single model which is shared be-\ntween several languages (Zeman and Resnik, 2008; McDonald et al.,\n2011a).6 This type of approach has been explored extensively in pre-\nvious work. Multilingual model transfer has been successfully ap-\nplied to, e.g., POS tagging (Täckström et al., 2013), and syntactic pars-\ning (Täckström, 2013; Ammar et al., 2016). This is commonly done by\nusing delexicalised input representations, as in the case of parsing\n(Zeman and Resnik, 2008; McDonald et al., 2011a; Täckström et al.,\n2012, 2013). A related situation, is the case of exploiting language\nsimilarities in order to train models for low-resource languages (see,\ne.g., Georgi et al. (2010)).\nIn this thesis, model transfer is framed as a special case of MTL.\nThat is to say, each language in the model can be seen analogously to\na task. This means that we are also free to choose whether we want\nto code tag predictions jointly as a single output layer, or have one\nseparate output layer per language. As with MTL with multiple tasks,\nwe consider the same speciﬁc type of MTL across languages, namely\nhard parameter sharing in neural networks.\nA common approach in parsing is to delexicalise the input repre-\nsentations in order to enforce uniformity across languages, by train-\ning a parser on sequences of PoS tags rather than sequences of words\n(Zeman and Resnik, 2008; McDonald et al., 2011a). However, as we\n6Note the similarities to multitask learning with hard parameter sharing.\n3.2. Multilingual Learning\n67\nare looking at predicting such tags, we approach this by using in-\nput representations which are shared across languages. This allows\nfor training a neural network for several languages simultaneously\nin a language-agnostic manner, while still taking lexical semantics\ninto account. Apart from this advantage, implementing a system in\nthis manner is straightforward. Additionally, this approach offers\nthe possibility of out-of-the-box zero-shot learning, as simply adding\ninput representations for a different language is suﬃcient to enable\nthis.\nZero-shot learning is the problem of learning to predict labels y\nwhich have not been seen during training. This is especially rele-\nvant in cases such as MT, in which, e.g., many of the target forms\nwhich need to be produced for languages with rich morphology have\nnot been seen. In recent years, zero-shot learning has become in-\ncreasingly popular, for instance in image recognition (Palatucci et al.,\n2009; Socher et al., 2013b). Recently, it has also been applied to MT,\nresulting in a model which even allows for translation into unseen\nlanguages (Johnson et al., 2016). One way of enabling zero-shot learn-\ning, is to use shared input representations. For instance, in the case\nof character-based models, we can simply use the same alphabet in\nthe inputs and outputs of each system, as in Östling and Bjerva (2017)\nand Bjerva (2017a). In the case of word level input representations,\none can employ word embeddings living in the same space, regard-\nless of language.\n3.2.4\nModel Transfer with Multilingual Input Representations\nLooking further at the problem of model transfer across languages,\nconsider the following example, of an English sentence and its trans-\nlation, as two separate input sequences to a neural network, with\ntheir corresponding annotated output sequences.7\n7PMB 01/3421. Original source: Tatoeba.\n68\n3. Multitask Learning and Multilingual Learning\n(3.2) We\nPRON\nmust\nVERB\ndraw\nVERB\nattention\nNOUN\nto\nADP\nthe\nDET\ndistribution\nNOUN\nof\nADP\nthis\nDET\nform\nNOUN\nin\nADP\nthose\nDET\ndialects\nNOUN\n.\nPUNCT\n(3.3) Wir\nPRON\nmüssen\nVERB\ndie\nDET\nVerbreitung\nNOUN\ndieser\nDET\nForm\nNOUN\nin\nADP\ndiesen\nDET\nDialekten\nNOUN\nbeachten\nVERB\n.\nPUNCT\nAlthough the surface forms of these two sentences differ, as one is in\nEnglish and one in German, multilingual word representations for\nthe corresponding words in these two sentences ought to be close to\none another. Hence, if the NN only sees the English sentence in train-\ning, and the German sentence during test time, it ought to be fairly\nsuccessful in tagging this ’unseen’ sentence with suitable tags.8 How-\never, one question is whether having access to the same sentence in\na typologically more distance language such as Japanese also would\nbe useful (this is approached in Chapter 5).\nIn order for such an approach to work, it is necessary that words\nwith similar meanings in different languages are represented in a\nfairly similar way. How do we arrive at word representations with\nsuch properties? In the next few sections we will look at this, begin-\nning at simple monolingual representations, and leading up to bilin-\ngual and multilingual representations.\n8Considering that the semantic content of the two sentences ought to be\nhighly similar, one could regard the translated sentence to be ’seen’ if the original\nsentence was in the training data. This has the further implication that one, in\nthis type of experiments, must take care not to allow corresponding sentences to\noccur in both training and evaluation data.\n3.2. Multilingual Learning\n69\n3.2.5\nContinuous Space Word Representations\nIn many NLP problems, we are concerned with processing some word-\nlike unit, in order to arrive at some linguistically motivated and ap-\npropriate label. In Section 2.3.1, we considered bag-of-words models\nfor tasks such as this. To recap, in this type of model we assign an\nindex to each unique word. Each word is then represented by a vec-\ntor ⃗x, with a dimensionality equal to the size of the vocabulary, since\neach word requires its own index. As an example, consider a vocabu-\nlary size of ﬁve words, with three of those words being cat, dog, and\ncoastal, with their corresponding vector representations, such that\n⃗xcat = [0, 0, 1, 0, 0],\n⃗xdog = [0, 0, 0, 0, 1],\n⃗xcoastal = [0, 1, 0, 0, 0].\n(3.4)\nIn NLP, we are often interested in comparing words with one another,\neither simply in order to have some measure of their similarity, or be-\ncause we are interested in the fact that similar words tend to have\nsimilar properties in down-stream tasks.\nFor instance, the words\ncat and dog are likely to have the same or similar linguistic analy-\nses in many cases, such as both being tagged with the PoS tag NOUN.\nA commonly used similarity measure between vectors is the cosine\ndistance. In this setting, a word representation as presented above is\nsomewhat problematic, as the distances between the three words are\nequal, although we want a higher similarity between cat and dog.\nThe representation we have seen so far is known as a sparse fea-\nture representation, as each word is represented by a vector of ze-\nroes with one element set to one, also known as a one-hot vector.\nApart from the drawback of similarity, this type of input represen-\ntation can run into other problems, such as the dimensionality of\nthe representations becoming too large to handle as vocabulary size\ngrows. This can be remedied in many ways, for instance by applying\n70\n3. Multitask Learning and Multilingual Learning\ndimensionality reduction algorithms.\nCommonly used algorithms\ninclude singular value decomposition (SVD), and random indexing\n(Kanerva et al., 2000; Sahlgren, 2005). This does not help with the\nproblem of similarities, however.\nIt turns out that one can arrive at word representations with nice\nproperties of similarity by taking advantage of the distributional hy-\npothesis:\n’Semantics is partly a function of the statistical distribution\nof words.’\n–Harris (1954)\n’You shall know a word by the company it keeps.’\n–Firth (1957, p.11)\nThis means that the semantic content of a given word is related to\nother words occurring in similar contexts. Furthermore, Harris (1954)\nclaims that the strength of this relation is proportional to the similar-\nity between two words, such that if two words w1 and w2 are more\nsimilar in meaning than w1 and w3, then the relative distribution of\nthe ﬁrst pair will be more similar than that of the second pair. One\nway of implementing this type of distributional semantics is to count\nword co-occurrences in a large corpus. Let us now assign the two re-\nmaining indices in the ﬁve-dimensional representation used above\nto the words pet and water, such that\n⃗xpet = [1, 0, 0, 0, 0],\n⃗xwater = [0, 0, 0, 1, 0].\n(3.5)\nThese ﬁve vectors can then be used to generate new distributional\nvectors, ⃗y, by representing each word by the sum of the vectors ⃗x of\nthe words with which it co-occurs. If cat and dog frequently co-occur\nwith each other, and with pet, whereas coastal mainly co-occurs with\nwater, the resulting representations may be similar to\n3.2. Multilingual Learning\n71\n⃗ycat = [25, 0, 20, 0, 10],\n⃗ydog = [30, 0, 10, 0, 20],\n⃗ycoastal = [0, 10, 0, 100, 0].\n(3.6)\nIn this representation, cat and dog are more similar to one another\nthan they are to coastal, which is exactly what we want. A visualisa-\ntion of such a word space is given in Figure 3.2.\ncat\ndog\npet\ncoastal water\nFigure 3.2: An example of a word space.\nThe type of word representation discussed up until now is also\nknown as a count-based representation, as opposed to a prediction-\nbased representation (Baroni et al., 2014).9 Whereas a count-based\nrepresentation can be seen as counting the words in a given con-\ntext, a prediction-based representation can be made by attempting\nto predict that context. Doing this with a neural network, the error\nobtained when attempting to make such predictions is used to up-\ndate the representations, until a low error is obtained (see Chapter 2\nfor details on neural networks). With the entry of the deep learning\ntsunami on the NLP scene (Manning, 2015), this type of dense word\nrepresentations has become increasingly popular. The availability\nof tools implementing such algorithms, such as word2vec, undoubt-\nedly helped push the popularity of this approach further. This trend\n9Whereas Baroni et al. (2014) suggest that prediction-based methods outper-\nform count-based ones, Levy and Goldberg (2014b) show that the underlying\ndifferences between the approaches are small.\n72\n3. Multitask Learning and Multilingual Learning\nwas introduced by Collobert and Weston (2008), Turian et al. (2010),\nand Collobert et al. (2011), and was further spearheaded by papers\nsuch as Mikolov et al. (2013c), which showed that a simple neural\nmodel would encapsulate linguistic regularities in its embedded vec-\ntor space. The now infamous example of this property is shown in\na ﬁgure in Mikolov et al. (2013c), replicated in Figure 3.3, where the\nfollowing relation holds\n−−→\nking + −−−−→\nwoman −−−→\nman = −−−→\nqueen.\n(3.7)\nIn other words, the distance between man and woman is similar to\nthat between king and queen, so adding this difference to the vector\nof king results in a vector close to queen.\nFigure 3.3: A word space in which adding the difference between\nwoman and man to king results in queen.\nIn a prediction-based approach, the terminology used is that a\nword is embedded into n-dimensional space. Typically, this dimen-\nsionality is much lower (e.g. around 100) than what is generally used\nin count-based approaches (e.g. around 1000). In the case of neural\nnetworks, these embeddings can be trained together with the rest of\nthe network. This results in a matrix of word vectors in which words\nwith similar properties (under the task at hand) are close to one an-\nother.\n3.2. Multilingual Learning\n73\nDistributional vs. Distributed representations\nAn useful distinction to make is that of distributed vs. distributional\nrepresentations. A distributional word representation is based on a\nco-occurrence matrix, taking advantage of the distributional hypoth-\nesis. Similarities between the resulting distributional word represen-\ntations thus represent the extent to which they co-occur, and there-\nfore also their semantic similarity. A distributed representation, on\nthe other hand, is simply one that is continuous. That is to say, a word\nis represented by a dense, real-valued, and usually low-dimensional\nvector. Such representations are generally known as word embed-\ndings, with each dimension representing some latent feature of the\nword at hand. One way to remember this is that such representations\nare distributed across some n-dimensional space.\nThe ﬁrst repre-\nsentations we saw were therefore distributional, but not distributed\n(Turian et al., 2010). The word embeddings, on the other hand, can\nbe said to be both.\nBilingual and Multilingual Word Representations\nGoing from monolingual to bilingual word representations has been\nthe subject of much attention in recent years. One of the ﬁrst ap-\nproaches to bilingual word representations was shown by Klemen-\ntiev et al. (2012), followed by work such as Wolf et al. (2014), and\nCoulmance et al. (2015). Parallel to approaches which aim at mak-\ning good multilingual embeddings, are attempts at producing better\nmonolingual embeddings by exploiting bilingual contexts, as in Guo\net al. (2014), Šuster et al. (2016), and Šuster (2016).\nIn essence, the approaches to building such representations can\nbe divided up into several categories.\nCross-lingual mapping can\nbe done by ﬁrst learning monolingual embeddings for separate lan-\nguages, and then using a bilingual lexicon to map representations\nfrom one space to the other (Mikolov et al., 2013b).\nAnother ap-\n74\n3. Multitask Learning and Multilingual Learning\nproach is to mix contexts from different languages, and training pre-\nexisting systems, such as word2vec, on this mixed data (Gouws and\nSøgaard, 2015). The approach under consideration in this thesis is\nbased on exploiting parallel texts, by jointly optimising a loss func-\ntion when predicting multilingual contexts (Guo et al., 2016).\nThe true power of multilinguality is not unlocked until we can\nconsider an arbitrary number of languages at a time. Whereas bilin-\ngual word representations only encode two languages, a multilingual\nword space contains representations from several languages in the\nsame space. As before, we here also have the property that words\nwith similar meanings are close to one another irrespective of the\nlanguage (see Figure 3.4).\ncat\ngato\nkat\nperro\nhond\ndog\nFigure 3.4: An example of a multilingual word space.\nOne such is the multilingual skip-gram model, as outlined by Guo\net al. (2016).10 As a variant of this model is used in Part III and Part\nIV, we will now cover this in more detail.\n10The skip-gram method to create word embeddings, in which a neural net-\nwork attempts to predict the context of a word, is not to be confused with skip-\ngrams in the sense of n-grams which are not necessarily consecutive. In the\nsecond sense, we can deﬁne a k-skip-n-gram as a sequence of length n, in which\nwords occur at distance k from each other. In this thesis, only the ﬁrst sense is of\nimportance.\n3.2. Multilingual Learning\n75\nMultilingual Skip-gram\nThe skip-gram model has become one of the most popular manners\nof learning word representations in NLP (Mikolov et al., 2013a). This\nis in part owed to its speed and simplicity, as well as the performance\ngains observed when incorporating the resulting word embeddings\ninto almost any NLP system. The model takes a word w as its input,\nand predicts the surrounding context c. Formally, the probability dis-\ntribution of c given w is deﬁned as\np(c|w; θ) =\nexp(⃗cT ⃗w)\nΣc∈V exp(⃗cT ⃗w),\n(3.8)\nwhere V is the vocabulary, and θ the parameters of word embeddings\n(⃗w) and context embeddings (⃗c). The parameters of this model can\nthen be learned by maximising the log-likelihood over (w, c) pairs in\nthe corpus C,\nJ(θ) =\nX\n(w,c)∈D\nlog p(c|w; θ).\n(3.9)\nGuo et al. (2016) provide a multilingual extension for the skip-\ngram model, by requiring the model to not only learn to predict En-\nglish contexts, but also multilingual ones. This can be seen as a sim-\nple adaptation of Firth (1957, p.11), i.e., you shall know a word by the\nmultilingual company it keeps. Hence, the vectors for, e.g., dog and\nperro ought to be close to each other in such a model. This assumes\naccess to multilingual parallel data, as word alignments are used in\norder to determine which words comprise the multilingual context\nof a word.\nFormally, the learning objective in multilingual skip-gram is de-\n76\n3. Multitask Learning and Multilingual Learning\nﬁned in Guo et al. (2016) as\nJ = α\nX\nl∈L\nJmonol + β\nX\nl∈L,{EN}\nJbil,EN\nJmonol =\nX\n(w,c)∈Dl↔l\nlog p(c|w; θ)\nJbil,EN =\nX\n(w,c)∈Dl↔EN\nlog p(c|w; θ),\n(3.10)\nwhere L denotes the set of all languages, and α and β are weight\nparameters for the monolingual and bilingual contexts, respectively.\nIn our work, however, we do not rely on always using English as\na pivot, and rather use all bilingual pairings to generate contexts. In\nother words, we also predict the French context based on the Spanish\nword, and vice versa, rather than only predicting from or to English.\nThis is visualised in Figure 3.5, in which the dashed lines indicate the\nadditional predictions made using the loss described here, and used\nin Bjerva and Östling (2017a).\nFormally, the joint objective function used here is deﬁned as\nJ = α\nX\nl∈L\nJmonol + β\nX\nl1∈L\nX\nl2̸=l1∈L\nJbil1,bil2\nJmonol =\nX\n(w,c)∈Dl↔l\nlog p(c|w; θ)\nJbil1,bil2 =\nX\n(w,c)∈Dl1↔l2\nlog p(c|w; θ).\n(3.11)\n3.3\nOutlook\nIn the ﬁrst part of this chapter, we considered multitask learning,\nwhich is the focus of Part II of this thesis. We will ﬁrst see a case\nstudy, in which a MTL paradigm is shown to improve performance\non two sequence labelling tasks. Then we turn to a more theoretical\ninvestigation into why this is the case.\n3.3. Outlook\n77\nFigure 3.5: Multilingual skip-gram utilising multilingual contexts.\nDashed lines indicate the additions of our loss function, i.e., predic-\ntions between every language pair.\nFollowing this, Part III also begins with a case study on multilin-\nguality in a single NLP task. The subsequent chapter then includes\nan empirical study of multilinguality in several tasks, and looks at\nchange in performance when multilinguality is employed.\nPART II\nMultitask Learning\nCHAPTER 4\n∗Multitask Semantic Tagging\nwith Residual Networks\nAbstract|In this chapter, a semantic tag set is presented, which is tailored\nfor multilingual semantic parsing. As a ﬁrst step towards exploring\nmultitask learning, we will look at the effects of jointly learning this task,\ntogether with POS tagging, compared to learning the two tasks separately.\nFurthermore, we will see a deep neural network tagger, which is the\nﬁrst tagger to use deep residual networks (ResNets). The tagger uses\nboth word and character representations, and includes a novel residual\nbypass architecture. We evaluate the tagger separately on the semantic\ntagging task, on POS tagging, and in the multitask learning setting. In the\nmultitask setting, the tagger signiﬁcantly outperforms prior results on\nEnglish Universal Dependencies POS tagging reaching 95.71% accuracy\non UD v1.2 and 95.67% accuracy on UD v1.3.\n∗Chapter adapted from: Bjerva, J., Plank, B., and Bos, J. (2016). Semantic\ntagging with deep residual networks. In Proceedings of COLING 2016, the 26th\nInternational Conference on Computational Linguistics: Technical Papers, pages\n3531–3541. The COLING 2016 Organizing Committee\n82\n4. Multitask Semantic Tagging\n4.1\nIntroduction\nA key issue in computational semantics is the transferability of se-\nmantic information across languages. Many semantic parsing sys-\ntems depend on sources of information such as POS tags (Pradhan\net al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and\nLiang, 2014). However, these tags are often customised for the lan-\nguage at hand (Marcus et al., 1993) or massively abstracted, such as\nthe Universal Dependencies tagset (Nivre et al., 2017, 2016a). Fur-\nthermore, POS tags are syntactically oriented, and therefore often\ncontain both irrelevant and insuﬃcient information for semantic anal-\nysis and deeper semantic processing. This means that, although POS\ntags are highly useful for many downstream tasks, they are unsuit-\nable both for semantic parsing in general, and for tasks such as recog-\nnising textual entailment.\nWe present a novel set of semantic labels tailored for the pur-\npose of multilingual semantic parsing. This tagset (i) abstracts over\nPOS and named entity types; (ii) ﬁlls gaps in semantic modelling by\nadding new categories (for instance for phenomena like negation,\nmodality, and quantiﬁcation); and (iii) generalises over speciﬁc lan-\nguages (see Section 4.2). We introduce and motivate this new task in\nthis chapter, and refer to it as semantic tagging. Our experiments aim\nto answer the following two research questions, in order to answer\nRQ 1:\nRQ 1a Can we use recent neural network architectures to implement\na state-of-the-art neural sequence tagger, which can easily be\nexpanded for multitask learning?\nRQ 1b Semantic tagging is essential for deep semantic parsing, but\ncan we ﬁnd evidence that semtags are effective also for other\nNLP tasks, in a multitask learning setting?\n4.2. Semantic Tagging\n83\nWe will ﬁrst look at the semantic tag set, before going into the de-\ntails of the neural network architecture used, and exploring these\nresearch questions.\n4.2\nSemantic Tagging\nSemantic tagging, or semtagging, is the task of assigning semantic\nclass categories to the smallest meaningful units in a sentence. In the\ncontext of this chapter these units can be morphemes, words, punctu-\nation, or multi-word expressions. These tags are designed so as to fa-\ncilitate semantic analysis and parsing in cross-linguistic settings, and\nto be as language neutral as possible, so as to be applicable to several\nlanguages (Abzianidze et al., 2017). The tag set is motivated by the ob-\nservation that linguistic information traditionally obtained for deep\nprocessing is insuﬃcient for ﬁne-grained lexical semantic analysis.\nThe widely used Penn Treebank (PTB) Part-of-Speech tagset (Marcus\net al., 1993) does not make the necessary semantic distinctions, in ad-\ndition to containing redundant information for semantic processing.\nIn particular, there are signiﬁcant differences in meaning between\nthe determiners every (universal quantiﬁcation), no (negation), and\nsome (existential quantiﬁcation), but they all receive the DT (deter-\nminer) POS label in PTB. Since determiners form a closed class, one\ncould enumerate all word forms for each class. Indeed some recent\nimplementations of semantic parsing follow this strategy (Bos, 2008;\nButler, 2010).\nThis might work for a single language, but it falls\nshort when considering a multilingual setting. Furthermore, deter-\nminers like any can have several interpretations and need to be dis-\nambiguated in context.\nIn addition to this, consider the following examples of redundant\ninformation of some POS tagsets, when considering semantic anal-\nysis. For instance, the tagset used in the PTB includes a distinction\nbetween VBP (present simple) and VBZ (present simple third person).\n84\n4. Multitask Semantic Tagging\nIn the context of semantic analysis, this type of distinction is not nec-\nessary.\nSemantic tagging does not only apply to determiners, but reaches\nall parts of speech. Other examples where semantic classes disam-\nbiguate are reﬂexive versus emphasising pronouns (both POS-tagged\nas PRP, personal pronoun); the comma, that could be a conjunction,\ndisjunction, or apposition; intersective vs. subsective and privative\nadjectives (all POS-tagged as JJ, adjective); proximal vs. medial and\ndistal demonstratives (see Example 2.1); subordinate vs. coordinate\ndiscourse relations; role nouns vs. entity nouns. ROL is used to sep-\narate roles from concepts, which is crucial in order to get accurate\nsemantic behaviour (Abzianidze et al., 2017).\nThe set of semantic tags that we use in this chapter is established\nin a data-driven manner, considering four languages in a parallel\ncorpus (English, German, Dutch and Italian).\nThis ﬁrst inventory\nof classes comprises 13 coarse-grained tags and 73 ﬁne-grained tags\n(see Table 4.1).2 As can be seen from this table and the examples\ngiven below, the tagset also includes named entity classes.\n(4.1) We\nPRO\nmust\nNEC\ndraw\nEXS\nattention\nCON\nto\nREL\nthe\nDEF\ndistribution\nCON\nof\nAND\nthis\nPRX\nform\nCON\nin\nREL\nthose\nDST\ndialects\nCON\n.\nNIL\n(4.2) Ukraine\nGPE\n’s\nHAS\nglory\nCON\nhas\nENT\nnot\nNOT\nyet\nIST\nperished\nEXT\n,\nNIL\nneither\nNOT\nher\nHAS\nfreedom\nCON\n.\nNIL\nIn Example 2.1,3 both this and those would be tagged as DT. However,\n2Experiments in this chapter are based on the tag inventory as detailed in\nthis chapter. This is based on version 0.3 of the semantic tags used in the PMB in\nJune 2016, and has since been revised.\n3PMB 01/3421, Original source: Tatoeba\n4.2. Semantic Tagging\n85\nwith our semantic tagset, they are disambiguated as PRX (proximal)\nand DST (distal). In Example 2.2,4 Ukraine is tagged as GPE rather than\nNNP.\nAnnotated data\nWe use two semtag datasets. The Groningen Meaning Bank (GMB)\ncorpus of English texts (1.4 million words) containing silver standard\nsemantic tags obtained by running a simple rule-based semantic tag-\nger (Bos et al., 2017). This tagger uses POS and named entity tags\navailable in the GMB (automatically obtained with the C&C tools (Cur-\nran et al., 2007) and then manually corrected), as well as a set of\nmanually crafted rules to output semantic tags. Some tags related to\nspeciﬁc phenomena were hand-corrected in a second stage.\nOur second dataset, the PMB, is smaller but equipped with gold\nstandard semantic tags and used for testing (Abzianidze et al., 2017).\nIt comprises a selection of 400 sentences of the English part of a par-\nallel corpus. It has no overlap with the GMB corpus. For this dataset,\nwe used the Elephant tokeniser, which performs word, multi-word\nand sentence segmentation (Evang et al., 2013). We then used the\nsimple rule-based semantic tagger described above to get an initial\nset of tags. These tags were then corrected by a human annotator.\nIn order to enable the multitask learning setting, we look at the\nPOS annotation in the English portion of the Universal Dependencies\ndataset, version 1.2 and 1.3 (Nivre et al., 2016a). An overview of the\ndata used is shown in Table 4.2. We use the oﬃcial training, develop-\nment, and test splits on the UD data. For the semantic silver standard\ndata set we split the data into 70% for training, 10% for development,\nand 20% for testing. We do not use any gold semantic tagging data\nfor training or development, reserving the entire set for testing.\n4PMB 05/0936, Original source: Tatoeba\n86\n4. Multitask Semantic Tagging\nTable 4.1: Semantic tags used in this chapter.\nANA\nPRO\npronoun\nMOD\nNOT\nnegation\nDEF\ndeﬁnite\nNEC\nnecessity\nHAS\npossessive\nPOS\npossibility\nREF\nreﬂexive\nENT\nCON\nconcept\nEMP\nemphasizing\nROL\nrole\nACT\nGRE\ngreeting\nNAM\nGPE\ngeo-political ent.\nITJ\ninterjection\nPER\nperson\nHES\nhesitation\nLOC\nlocation\nQUE\ninterrogative\nORG\norganisation\nATT\nQUA\nquantity\nART\nartifact\nUOM\nmeasurement\nNAT\nnatural obj./phen.\nIST\nintersective\nHAP\nhappening\nREL\nrelation\nURL\nurl\nRLI\nrel. inv. scope\nEVE\nEXS\nuntensed simple\nSST\nsubsective\nENS\npresent simple\nINT\nintensiﬁer\nEPS\npast simple\nSCO\nscore\nEFS\nfuture simple\nLOG\nALT\nalternative\nEXG\nuntensed prog.\nEXC\nexclusive\nENG\npresent prog.\nNIL\nempty\nEPG\npast prog.\nDIS\ndisjunct./exist.\nEFG\nfuture prog.\nIMP\nimplication\nEXT\nuntensed perfect\nAND\nconjunct./univ.\nENT\npresent perfect\nBUT\ncontrast\nEPT\npast perfect\nCOM\nEQA\nequative\nEFT\nfuture perfect\nMOR\ncomparative pos.\nETG\nperfect prog.\nLES\ncomparative neg.\nETV\nperfect passive\nTOP\npos. superlative\nEXV\npassive\nBOT\nneg. superlative\nTNS\nNOW\npresent tense\nORD\nordinal\nPST\npast tense\nDEM\nPRX\nproximal\nFUT\nfuture tense\nMED\nmedial\nTIM\nDOM\nday of month\nDST\ndistal\nYOC\nyear of century\nDIS\nSUB\nsubordinate\nDOW\nday of week\nCOO\ncoordinate\nMOY\nmonth of year\nAPP\nappositional\nDEC\ndecade\nCLO\nclocktime\n4.3. Method\n87\nTable 4.2: Overview of the semantic tagging data (ST Silver from the\nGMB, ST Gold from the PMB) and the Universal Dependencies data\n(UD), as of November 2016.\nCORPUS\nTRAIN (SENTS/TOKS)\nDEV (SENTS/TOKS)\nTEST (SENTS/TOKS)\nTAGS\nST Silver\n42,599 / 930,201\n6,084 / 131,337\n12,168 / 263,516\n66\nST Gold\nn/a\nn/a\n356 / 1,718\n66\nUD\n12,543 / 204,586\n2,002 / 25,148\n2,077 / 25,096\n17\n4.3\nMethod\nTo address RQ 1a, we will look at convolutional neural networks\n(CNNs) and recurrent neural networks (RNNs), which are both highly\nprominent approaches in the recent natural language processing (NLP)\nliterature (see Chapter 2). A recent development is the emergence of\ndeep residual networks (ResNets), a building block for CNNs (see Sec-\ntion 2.6). In short, ResNets consist of several stacked residual units,\nwhich can be thought of as a collection of convolutional layers cou-\npled with a shortcut which aids the propagation of the signal in a\nneural network. This allows for the construction of much deeper\nnetworks, since keeping a relatively clean information path in the\nnetwork facilitates optimisation (He et al., 2016). ResNets have re-\ncently shown state-of-the-art performance for image classiﬁcation\ntasks (He et al., 2015a, 2016), and have also seen some recent use\nin NLP (Östling, 2016; Conneau et al., 2016; Bjerva, 2016; Wu et al.,\n2016). However, no previous work has attempted to apply ResNets\nto NLP tagging tasks.\nOur tagger is a hierarchical deep neural network consisting of a\nbidirectional Gated Recurrent Unit (GRU) network at the upper level,\n88\n4. Multitask Semantic Tagging\nand a CNN or a ResNet at the lower level, including an optional novel\nresidual bypass function (cf. Figure 4.1).\nBi-directional GRUs and\nLSTMs have been shown to yield high performance on several NLP\ntasks, such as POS tagging, named entity tagging, and chunking (Wang\net al., 2015; Yang et al., 2016; Plank et al., 2016). We build on pre-\nvious approaches by combining bi-GRUs with character representa-\ntions from a basic CNN and ResNets.\n4.3.1\nInception model\nDue to the success of GoogLeNet on the ImageNet 2014 challenge,\nwe experiment with a variant of their model codenamed Inception\n(Szegedy et al., 2015). The Inception model applies convolutions of\ndifferent sizes (e.g. 1 × 1, 3 × 3, 5 × 5) in parallel to the output of the\nprevious layer. In the system used by Szegedy et al. (2015), several\nsuch modules are stacked, forming a network of 22 layers. The in-\ntuition behind building a deep network with modules using varying\nconvolutional patch sizes, is that the features learned by each layer\nare expected to be both more abstract and less spatially concentrated.\nThus, in the lower layers of the model, the smaller patches are ex-\npected to capture most of the correlation statistics of the previous\nlayer, whereas in deeper layers the larger patches are expected to do\nthis (cf. Arora et al. (2014)). Adding a large amount of convolutions\nnaïvely would explode the amount of parameters to be optimised.\nHence, 1 × 1 convolutions are added before the expensive larger con-\nvolutions. This addition can be thought of as a dimensionality reduc-\ntion and information compression step, analogous to the success of\nword embeddings (Szegedy et al., 2015).\nWe apply Inception modules directly following the layer contain-\ning embedded character representations, and pass this information\nthrough to the bi-GRU. Our main modiﬁcation on the original Incep-\ntion module is that we apply it in a sequence-to-sequence prediction\ntask.\n4.3. Method\n89\nFigure 4.1: Model architecture. Left: Architecture with basic CNN char representations (⃗c), Middle:\nbasic CNN with char and word representations and bypass (⃗cbp∧⃗w), Right: ResNet with an auxiliary\ntask and residual bypass (+AUXbp).\n90\n4. Multitask Semantic Tagging\n4.3.2\nDeep Residual Networks\nWe use Deep Residual Networks (ResNets), as introduced in Section 2.6.\nResNets have recently been found to yield impressive performance\nin image recognition tasks, with networks as deep as 1001 layers (He\net al., 2015a, 2016), and are thus an interesting and effective alterna-\ntive to simply stacking layers. In this chapter we use the assymetric\nvariant of ResNets similarly to what is described in Equation 9 in He\net al. (2016), namely\nzl+1 = zl + F(σ(zl)),\n(4.3)\nwhere zl is the pre-activation of the lth layer, σ(zl) = αl, F is a convo-\nlutional function. In other words, we add the pre-activation output\nof layer l to the output of a convolutional block over the same out-\nput, and set this to be the pre-activation of the following layer. This\neffectively gives the network a shortcut to the previous layer, which\nis useful when propagating errors backwards through the network.\nIn NLP, ResNets have been recently applied to morphological re-\ninﬂection (Östling, 2016), native language identiﬁcation (Bjerva et al.,\n2017; Kulmizev et al., 2017), sentiment analysis and text categorisa-\ntion (Conneau et al., 2016), as well as machine translation (Wu et al.,\n2016). Our work is the ﬁrst to apply ResNets to NLP sequence tagging\ntasks. We further contribute to the literature on ResNets by introduc-\ning a residual bypass function. The intuition is to combine both deep\nand shallow processing, which opens a path of easy signal propaga-\ntion between lower and higher layers in the network.\n4.3.3\nModelling character information and residual bypass\nUsing sub-token representations instead of, or in combination with,\nword-level representations has recently obtained a lot of attention\ndue to their effectiveness (Sutskever et al., 2011; Chrupała, 2013; Zhang\net al., 2015b; Chung et al., 2016; Gillick et al., 2015). There is much to\n4.3. Method\n91\nbe said for approaching NLP using sub-token information. Doing so\nallows for much more compact models, since it is no longer neces-\nsary to learn weights associated with a large vocabulary, as when\nusing word embeddings. Additionally, not relying on the linguistic\nartefact of white-space delimited text, opens up for learning what-\never internal structure is the most appropriate for the task at hand\n(Gillick et al., 2015). Sub-token information can be seen as both a\npotential replacement for token information, or simply as a supple-\nment, which is the approach we take in this chapter.\nThe use of sub-token representations can be approached in sev-\neral ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical\nbi-directional RNN, ﬁrst passing over characters in order to create\nword-level representations. Gillick et al. (2015) similarly apply an\nLSTM-based model using byte-level information directly. CNNs are\nused by dos Santos and Zadrozny (2014), who construct character-\nbased word-level representations by running a CNN over the char-\nacter representations of each word. All of these approaches have in\ncommon that the character-based representation is passed through\nthe entire remainder of the network. Our work is the ﬁrst to combine\nthe use of character-level representations with both deep processing\n(i.e., passing this representation through the network) and shallow\nprocessing (i.e., bypassing the network in our residual bypass func-\ntion). We achieve this by applying our novel residual bypass function\nto our character representations, inspired by the success of ResNets\n(depicted in Figure 4.1). In particular, we ﬁrst apply the bypass to a\nCNN-based model achieving large gains over a plain CNN, and later\nevaluate its effectiveness in a ResNet. The bypass function allows\nboth lower-level and higher-level features to be taken directly into\naccount in the ﬁnal layers of the network. The intuition behind using\nsuch a global residual function in NLP is that character information\nprimarily ought to be of importance for the prediction of the current\nword. Hence, allowing these representations to bypass our bi-GRU\n92\n4. Multitask Semantic Tagging\nmight be beneﬁcial. This residual bypass function is not dependent\non the usage of ResNets, and can be combined with other NN architec-\ntures as in our experiments. We deﬁne the penultimate layer, αn−1,\nof a network with n layers, using a residual bypass, as\nαn−1 = σ(zn−1) + αc,\n(4.4)\nwhere σ is some activation function, zn−1 is the pre-activation of layer\nn −1, and αc is the activation of the character-level CNN, in the case\nof our experiments.\n4.3.4\nSystem description\nThe core of our architecture consists of a bi-GRU taking an input\nbased on words and/or characters, with an optional residual bypass\nas deﬁned in subsection 4.3.3.\nWe experiment with a basic CNN,\nResNets, a variant of the Inception model (Szegedy et al., 2015), and\nour novel residual bypass function. Our system is implemented in\nKeras using the Tensorﬂow backend (Chollet, 2015; Abadi et al., 2016).5\nWe represent each sentence using both a character-based repre-\nsentation (Sc) and a word-based representation (Sw). The character-\nbased representation is a 3-dimensional matrix Ss×w×dc\nc\n, where s is\nthe zero-padded sentence length, w is the zero-padded word length,\nand dc is the dimensionality of the character embeddings. The word-\nbased representation is a 2-dimensional matrix Ss×dw\nw\n, where s is the\nzero-padded sentence length and dw is the dimensionality of the word\nembeddings. We use the English Polyglot embeddings (Al-Rfou et al.,\n2013) in order to initialise the word embedding layer, but also exper-\niment with randomly initialised word embeddings.\nWord embeddings are passed directly into a two-layer bi-GRU (Chung\net al., 2014). We also experimented using a bi-LSTM. However, we\nfound GRUs to yield comparatively better validation data performance\n5System code available at https://github.com/bjerva/semantic-tagging.\n4.3. Method\n93\non semtags.\nWe also observe better validation data performance\nwhen running two consecutive forward and backward passes before\nconcatenating the GRU layers, rather than concatenating after each\nforward/backward pass as is commonplace in NLP literature.\nWe use CNNs for character-level modelling. Our basic CNN is in-\nspired by dos Santos and Zadrozny (2014), who use character rep-\nresentations to produce local features around each character of a\nword, and combine these with a maximum pooling operation in or-\nder to create ﬁxed-size character-level word embeddings. The con-\nvolutions used in this manner cover a few neighbouring letters at a\ntime, as well as the entire character vector dimension (dc). In con-\ntrast to dos Santos and Zadrozny (2014), we treat a word analogously\nto an image. That is to say, we see a word of n characters embed-\nded in a space with dimensionality dc as an image of dimensionality\nn × dc. This view gives us additional freedom in terms of sizes of con-\nvolutional patches used, which offers more computational ﬂexibility\nthan using only, e.g., 4 × dc convolutions. This view is applied to all\nCNN variations explored in this work.\nTo answer RQ1b, we investigate the effect of using semantic tags\nas an auxiliary task for POS tagging, in a multitask learning paradigm\n(see Chapter 3). Since POS tags are useful for many NLP tasks, it\nfollows that semantic tags must be useful if they can improve POS\ntagging.\nA neural network is trained with respect to a given loss\nfunction, such as the cross-entropy between the predicted tag prob-\nability distribution and the target probability distribution. Recent\nwork has shown that the addition of an auxiliary loss function can\nbe beneﬁcial to several tasks. For instance, Cheng et al. (2015) use\nthis paradigm for language modelling, by predicting the next token\nwhile also predicting whether the sentence at hand contains a name.\nPlank et al. (2016) use the log frequency of the current token as an\nauxiliary task, and ﬁnd this to improve POS tagging accuracy. Since\nour semantic tagging task is based on predicting ﬁne semtags, which\n94\n4. Multitask Semantic Tagging\ncan be mapped to coarse semtags, we add the prediction of these\ncoarse semtags as an auxiliary task for the semtagging experiments.\nSimilarly, we also experiment with POS tagging, where we use the\nﬁne semtags as an auxiliary information.\nHyperparameters\nAll hyperparameters are tuned with respect to loss on the semtag\nvalidation set. We use rectiﬁed linear units (ReLUs) for all activation\nfunctions (Nair and Hinton, 2010), and apply dropout with p = 0.1\nto both input weights and recurrent weights in the bi-GRU (Srivas-\ntava et al., 2014). In the CNNs, we apply batch normalisation (Ioffe\nand Szegedy, 2015) followed by dropout with p = 0.5 after each layer.\nIn our basic CNN, we apply a 4 × 8 convolution, followed by 2 × 2\nmaximum pooling, followed by 4 × 4 convolution and another 2 × 2\nmaximum pooling. Our ResNet has the same setup, with the addition\nof a residual connection. We also experimented with using average\npooling instead of maximum pooling, but this yielded lower valida-\ntion data performance on the semantic tagging task. We set both dc\nand dw to 64. All GRU layers have 100 hidden units. All experiments\nwere run with early stopping monitoring validation set loss, using a\nmaximum of 50 epochs. We use a batch size of 500. Optimisation is\ndone using the ADAM algorithm (Kingma and Ba, 2014), with the cat-\negorical cross-entropy loss function as training objective. The main\nand auxiliary loss functions have a weighting parameter, λ. In our\nexperiments, we weight the auxiliary task with λ = 0.1, as set on the\nsemtag auxiliary task, and a weighting of λ = 1.0 for the main task.\nMulti-word expressions (MWEs) are especially prominent in the\nsemtag data, where they are annotated as single tokens. Pre-trained\nword embeddings are unlikely to include entries such as ‘Interna-\ntional Organization for Migration’, so we apply a simple heuristic in\norder to avoid treating most MWEs as unknown words. That is to\nsay, the representation of a MWE is set to the sum of the individual\n4.4. Evaluation\n95\nembeddings of each constituent word, such that\n−−−→\nmwe =\nX\nw∈mwe\n⃗w,\n(4.5)\nwhere mwe is the MWE at hand, w ∈mwe is every word in this mwe,\nand ⃗w is the embedded vector representation of that word.\n4.4\nEvaluation\nWe evaluate our tagger on two tasks: semantic tagging and POS tag-\nging. Note that the tagger is developed solely on the semantic tag-\nging task, using the GMB silver training and validation data. Hence,\nno further ﬁne-tuning of hyperparameters for the POS tagging task\nis performed. We calculate signiﬁcance using bootstrap resampling\n(Efron and Tibshirani, 1994). The following independent variables\nare manipulated in our experiments:\n1. character and word representations (⃗w,⃗c);\n2. residual bypass for character representations (⃗cbp);\n3. convolutional representations (Basic CNN and ResNets);\n4. auxiliary tasks (using coarse semtags on ST and ﬁne semtags on\nUD).\nWe compare our results to four baselines:\n1. the most frequent baseline per word (MFC), where we assign\nthe most frequent tag for a word in the training data to that\nword in the test data, and unseen words get the global majority\ntag;\n2. the trigram statistic based TNT tagger which offers a slightly\ntougher baseline (Brants, 2000);\n96\n4. Multitask Semantic Tagging\n3. the BI-LSTM baseline, running the off-the-shelf state-of-the-art\nPOS tagger for the UD dataset (Plank et al., 2016) (using default\nparameters with pre-trained Polyglot embeddings);\n4. we also use a baseline consisting of running our own system\nwith only a BI-GRU using word representations (⃗w), with pre-\ntrained Polyglot embeddings.\n4.4.1\nExperiments on semantic tagging\nWe evaluate our system on two semantic tagging (ST) datasets: our\nsilver semtag dataset and our gold semtag dataset. For the +AUX con-\ndition we use coarse semtags as an auxiliary task. Results from these\nexperiments are shown in Table 4.3.\n4.4.2\nExperiments on Part-of-Speech tagging\nWe evaluate our system on v1.2 and v1.3 of the English part of the\nUniversal Dependencies (UD) data. We report results for POS tagging\nalone, comparing to commonly used baselines and prior work using\nLSTMs, as well as using the ﬁne-grained semantic tags as auxiliary\ninformation. For the +AUX condition, we train a single joint model\nusing a multi-task objective, with POS and ST as our two tasks. This\nmodel is trained on the concatenation of the ST silver data with the\nUD data, updating the loss of the respective task of an instance in\neach iteration. Hence, the weights leading to the UD output layer are\nnot updated on the ST silver portion of the data, and vice-versa for\nthe ST output layer on the UD portion of the data. Results from these\nexperiments are shown in Table 4.3.\n4.4.3\nThe Inception architecture\nFor comparison with the ResNet, we evaluate the Inception architec-\nture on ST Silver data, and UD v1.2 and v1.3 (see Table 4.4).\n4.4. Evaluation\n97\nTable 4.3: Experiment results on semtag (ST) and Universal Depen-\ndencies (UD) test sets (% accuracy). MFC indicates the per-word most\nfrequent class baseline, TNT indicates the TNT tagger, and BI-LSTM in-\ndicates the system by Plank et al. (2016). BI-GRU indicates the ⃗w only\nbaseline. ⃗w indicates usage of word representations, ⃗c indicates us-\nage of character representations, and ⃗cbp indicates usage of residual\nbypass of character representations. The +AUX column indicates the\nusage of an auxiliary task.\nST Silver\nST Gold\nUD v1.2\nUD v1.3\nBASELINES\nMFC\n84.64\n77.39\n85.06\n85.07\nTNT\n92.09\n80.73\n92.66\n92.69\nBI-LSTM\n94.98\n82.96\n95.17\n95.04\nBI-GRU\n94.26\n80.26\n94.39\n94.32\nBASIC CNN\n⃗c\n91.39\n69.21\n77.63\n77.51\n⃗cbp\n90.18\n65.77\n83.53\n82.89\n⃗cbp ∧⃗w\n94.63\n76.83\n94.68\n94.89\n+AUXbp\n94.53\n80.73\n95.19\n95.34\nRESNET\n⃗c\n94.39\n76.89\n92.65\n92.63\n⃗c ∧⃗w\n95.14\n83.64\n94.92\n94.88\n+AUX\n94.23\n74.84\n95.71\n95.67\n⃗cbp\n94.23\n75.84\n92.45\n92.86\n⃗cbp ∧⃗w\n95.15\n82.18\n94.73\n94.69\n+AUXbp\n94.58\n73.73\n95.51\n95.57\nTable 4.4: Results when using the Inception architecture on ST and\nUD data. ⃗w indicates usage of word representations, ⃗c indicates us-\nage of character representations, and ⃗cbp indicates usage of residual\nbypass of character representations.\n⃗c\n⃗cbp\n⃗cbp ∧⃗w\nResNet\nST Silver\n94.40\n93.32\n94.64\n95.15\nUD v1.2\n90.82\n89.78\n95.07\n94.73\nUD v1.3\n91.12\n89.55\n94.90\n94.69\n98\n4. Multitask Semantic Tagging\n4.4.4\nEffect of pre-trained embeddings\nIn our main experiments, we initialise the word embedding layer\nwith pre-trained polyglot embeddings. We compare this with ran-\ndomly initialising this layer from a uniform distribution over the in-\nterval [−0.05, 0.05), without any pre-training. Results from these ex-\nperiments are shown in Table 4.5.\nTable 4.5: Results under the ⃗cbp ∧⃗w and ⃗cbp ∧⃗w+AUX conditions, on ST\nand UD data, using randomly initialised word embeddings. Change\nin accuracy is indicated in brackets.\n⃗cbp ∧⃗w\n+AUX\nST Silver\n95.11 (-0.04)\n94.57 (-0.01)\nUD v1.2\n91.94 (-2.79)\n94.90 (-0.61)\nUD v1.3\n92.00 (-2.69)\n94.96 (-0.61)\n4.5\nDiscussion\n4.5.1\nPerformance on semantic tagging\nThe overall best system is the ResNet combining both word and char-\nacter representations⃗c∧⃗w. It outperforms all baselines, including the\nrecently proposed RNN-based bi-LSTM. On the ST silver data, a signif-\nicant difference (p < 0.01) is found when comparing our best system\nto the strongest baseline (BI-LSTM). On the ST gold data, we observe\nsigniﬁcant differences at the alpha values recommended by Søgaard\net al. (2014), with p < 0.0025. The residual bypass effectively helps\nimprove the performance of the basic CNN. However, the tagging ac-\ncuracy of the CNN falls below baselines. In addition, the large gap\nbetween gold and silver data for the CNN shows that the CNN model\nis more prone to overﬁtting, thus favouring the use of the ResNet.\n4.5. Discussion\n99\nAdding the coarse-grained semtags as an auxiliary task only helps\nfor the weaker CNN model. The ResNet does not beneﬁt from this ad-\nditional information, which is already captured in the ﬁne-grained\nlabels.\nIt is especially noteworthy that the ResNet character-only system\nperforms remarkably well, as it outperforms the BI-GRU and TNT\nbaselines, and is considerably better than the basic CNN. Since per-\nformance increases further when adding in ⃗w, it is clear that the char-\nacter and word representations are complimentary in nature. The\nhigh results for characters only are particularly promising for multi-\nlingual language processing, as such representations allow for much\nmore compact models (see, e.g., Gillick et al. (2015)). This further\nindicates that ResNet-based character representations can almost ac-\ncount for the same amount of compositionality as word representa-\ntions.\n4.5.2\nPerformance on Part-of-Speech tagging\nOur system was tuned solely on semtag data. This is reﬂected in, e.g.,\nthe fact that even though our ⃗c ∧⃗w ResNet system outperforms the\nPlank et al. (2016) system on semtags, we are substantially outper-\nformed on UD 1.2 and 1.3 in this setup. However, adding an auxil-\niary task based on our semtags markedly increases performance on\nPOS tagging. In this setting, our tagger outperforms the BI-LSTM sys-\ntem, and results in new state-of-the-art results on both UD 1.2 (95.71%\naccuracy) and 1.3 (95.67% accuracy). The difference between the BI-\nLSTM system and our best system is signiﬁcant at p < 0.0025.\nThe fact that the semantic tags improve POS tagging performance\nreﬂects two properties of semantic tags. Firstly, it indicates that the\nsemantic tags carry important information which aids the prediction\nof POS tags. This should come as no surprise, considering the fact\nthat the semtags abstract over and carry more information than POS\ntags. Secondly, it indicates that the new semantic tagset and released\n100\n4. Multitask Semantic Tagging\ndataset are useful for downstream NLP tasks. This could be done\nindirectly, by running a POS tagger which has been trained in a multi-\ntask setting with semtags. Alternatively, the semtags would likely\nbe useful features for downstream tasks. In this chapter we show\nthis by using semtags as an auxiliary task. In future work we aim to\ninvestigate the effect of introducing the semtags directly as features\ninto the embedded input representation.\n4.5.3\nInception\nThe experiments using the Inception architecture showed that the\nResNet architecture we used performs better on semantic tagging.\nThe ﬁrst of these results is in line with results in, e.g., image recog-\nnition, where ResNets are also superior to Inception (He et al., 2015a,\n2016).\nOn UD PoS tagging, however, results using Inception were\nmarginally better. This might be explained by the fact that the ResNet\narchitecture was rather heavily tuned on semantic tagging, whereas\nInception was tuned to a lesser extent. Nonetheless, both architec-\ntures outperform the use of a standard relatively shallow CNN, indi-\ncating that they may indeed be more suitable for similar tasks, given\nsimilar amounts of data.\n4.5.4\nResidual bypass\nOur novel residual bypass function outperforms corresponding mod-\nels without residual bypass in some cases. Notably for POS tagging\nwith a standard CNN, the increase in tagging accuracy is around 5%.\nCombining this with a ResNet does not have a large effect on tag-\nging performance, resulting in slightly higher accuracy on UD 1.3,\nbut lower on UD 1.2 and semtags. This indicates that, although us-\ning a residual bypass allows for the character information to propa-\ngate more easily, this is not crucial for the model to capture subtoken\ninformation and use this effectively. An interesting possibility for\n4.6. Conclusions\n101\nfuture research is to investigate the use of a residual bypass on word-\nlevel representations.\n4.5.5\nPre-trained embeddings\nFor semantic tagging, the difference with random initialisation is\nnegligible, with pre-trained embeddings yielding an increase in about\n0.04% accuracy.\nFor POS tagging, however, using pre-trained em-\nbeddings increased accuracy by almost 3 percentage points for the\nResNet.\n4.6\nConclusions\nIn this chapter, we ﬁrst introduced a semantic tagset tailored for mul-\ntilingual semantic parsing. We compared tagging performance using\nstandard CNNs and the recently emerged ResNets. For semantic tag-\nging, ResNets are more robust and result in our best model. Combin-\ning word and ResNet-based character representations helps to out-\nperform state-of-the-art taggers on semantic tagging, while allowing\nfor straightforward extension to an MTL paradigm (RQ 1a). Since we\nwere interested in seeing whether the new tagset could be informa-\ntive for other tasks, we used semantic tagging as an auxiliary task for\nPoS tagging. This yielded state-of-the-art performance on the English\nUD 1.2 and 1.3 POS datasets, showing that semantic tags are informa-\ntive for other NLP tasks (RQ 1b). The fact that using semantic tags\naided POS tagging raises the question of in which cases it is useful to\nhave an auxiliary tagging task (RQ 2), which is explored in the follow-\ning chapter.\nCHAPTER 5\n∗Information-theoretic\nPerspectives on Multitask\nLearning Effectivity\nAbstract|In the previous chapter, we saw that multitask learning im-\nproved the performance on POS tagging, when using semantic tagging\nas an auxiliary task. In fact, multitask learning often improves system\nperformance for various tasks in ML in general, and NLP in particular.\nHowever, the question of when and why this is the case has yet to be an-\nswered satisfactorily. Although previous work has hypothesised that this\nis linked to the label distributions of the auxiliary task, it can be argued\nthat this is not suﬃcient. In this chapter, we will see that information-\ntheoretic measures which consider the joint label distributions of the\nmain and auxiliary tasks offer far more explanatory value. The ﬁndings\nin this chapter are empirically supported by experiments on morphosyn-\ntactic tasks on 39 languages, and by experiments on several semantic\ntasks for English.\n∗Chapter adapted from: Bjerva, J. (2017) Will my auxiliary tagging task help?\nEstimating Auxiliary Tasks Effectivity in Multi-Task Learning, in Proceedings of\nthe 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May\n2017, Gothenburg, Sweden, number 131, pages 216–220. Linköping University\nElectronic Press, Linköpings universitet. Best short-paper award.\n104\n5. Information-theoretic Perspectives on Multitask Learning\n5.1\nIntroduction\nWhen attempting to solve a natural language processing (NLP) task,\none can consider the fact that many such tasks are highly related to\none another. As discussed in Chapter 3, a common way of taking ad-\nvantage of this is to apply multitask learning (MTL, Caruana (1997)).\nMTL has been successfully applied to many linguistic sequence pre-\ndiction tasks, both syntactic and semantic in nature (Collobert and\nWeston, 2008; Cheng et al., 2015; Søgaard and Goldberg, 2016; Bjerva\net al., 2016b; Ammar et al., 2016; Plank et al., 2016; Martínez Alonso\nand Plank, 2017; Bingel and Søgaard, 2017).\nThis trend is in part\nowed to the fact that a speciﬁc type of MTL, namely hard parame-\nter sharing in neural networks, is relatively easy to implement and\noften quite effective. It is, however, unclear when an auxiliary task is\nuseful, although previous work has provided some insights (Caruana,\n1997; Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017).\nFor a further overview of MTL, see Chapter 3.\nCurrently, considerable time and effort need to be employed in\norder to experimentally investigate the usefulness of any given main\ntask / auxiliary task combination. In this chapter the aim is to alle-\nviate this process by providing a means to empirically investigating\nthe potential effectivity of an auxiliary task. We aim to answer the\nfollowing two research questions, in order to answer RQ 2:\nRQ 2a Which information-theoretic measures can be used to estimate\nauxiliary task effectivity?\nRQ 2b To what extent do correlations between information-theoretic\nmeasures and auxiliary task effectivity generalise across lan-\nguages and NLP tasks?\nConcretely, we apply information-theoretic measures to a collection\nof data- and tag sets, and investigate correlations between these mea-\nsures and auxiliary task effectivity. We investigate this both exper-\n5.2. Information-theoretic Measures\n105\nimentally on a collection of syntactically oriented tasks on 39 lan-\nguages, as well as on several semantically oriented tasks for English.\nWe take care to structure our experiments so as to generalise across\nmany common real-world situations in which MTL is applied. Con-\ncretely, we apply neural multitask learning (see Chapter 5), using a\nbi-directional GRU (bi-GRU, see Section 2.4), as introduced in Chap-\nter 4, using hard parameter sharing.\n5.2\nInformation-theoretic Measures\nWe wish to give an information-theoretic perspective on when an\nauxiliary task will be useful for a given main task. For this purpose,\nwe introduce some common information-theoretic measures which\nwill be used throughout this work.2\n5.2.1\nEntropy\nThe entropy of a probability distribution, originally described in Shan-\nnon and Weaver (1949), is a measure of its unpredictability. That is to\nsay, high entropy indicates a uniformly distributed tag set, while low\nentropy indicates a more skewed distribution. Formally, the entropy\nof a tag set can be deﬁned as\nH(X) = −\nX\nx∈X\np(x) log p(x),\n(5.1)\nwhere x is a given tag in tag set X.\n5.2.2\nConditional Entropy\nIt may be more informative to take the joint probabilities of the main\nand auxiliary tag sets in question into account, for instance using\nconditional entropy. This is depicted in Figure 5.1 as H(X|Y ) and\n2See Cover and Thomas (2012) for an in-depth overview.\n106\n5. Information-theoretic Perspectives on Multitask Learning\nFigure 5.1: Information theory overview. The left circle denotes H(X),\nand the right circle H(Y). Blue (H(X|Y )) indicates the conditional en-\ntropy of X given Y , red (H(Y |X)) indicates the opposite, and purple\n(I(X; Y )) indicates the mutual information of X and Y .\nH(Y |X), with red and blue respectively. Formally, the conditional\nentropy of a distribution Y given the distribution X is deﬁned as\nH(Y |X) =\nX\nx∈X\nX\ny∈Y\np(x, y) log p(x)\np(x, y),\n(5.2)\nwhere x and y are all variables in the given distributions, p(x, y) is\nthe joint probability of variable x cooccurring with variable y, and\np(x) is the probability of variable x occurring at all. That is to say, if\nthe auxiliary tag of a word is known, this is highly informative when\ndeciding what the main tag should be. In the case of a multitask\nsetup, Y and X are the distributions of the main and auxiliary task\ntag sets respectively. The variables y and x are speciﬁc tags in these\ntag sets.\n5.2. Information-theoretic Measures\n107\n5.2.3\nMutual Information\nThe mutual information (MI) of two tag sets is a measure of the\namount of information that is obtained of one tag set, given the other\ntag set. MI can be deﬁned as\nI(X; Y ) =\nX\nx∈X\nX\ny∈Y\np(x, y) log p(x, y)\np(x) p(y),\n(5.3)\nwhere x and y are all variables in the given distributions, p(x, y) is\nthe joint probability of variable x cooccurring with variable y, and\np(x) is the probability of variable x occurring at all. This is depicted\nin Figure 5.1 as I(X; Y ), in purple, which also illustrates the alter-\nnative deﬁnition of MI, namely expressed in terms of entropy and\nconditional entropy as\nI(X; Y ) ≡H(X) −H(X|Y ) ≡H(Y ) −H(Y |X).\n(5.4)\nIn the ﬁgure, this is depicted as that subtracting either H(X) from\nH(X|Y ) or H(Y ) from H(Y |X) will result in I(X; Y ). MI describes\nhow much information is shared between X and Y , and can there-\nfore be considered a measure of ‘correlation’ between tag sets. Should\ntwo tag sets be completely independent from each other, then know-\ning Y would not give any information about X.\n5.2.4\nInformation Theory and MTL in NLP\nEntropy has in the literature been hypothesised to be related to the\nusefulness of an auxiliary task (Martínez Alonso and Plank, 2017).\nWe argue that this explanation is not entirely suﬃcient. Take, for\ninstance, two tag sets X and X′, applied to the same corpus and con-\ntaining the same tags. Consider the case where the annotations differ\nin that the labels in every sentence using X′ have been randomly re-\nordered. Such a situation is shown in the following examples:\n108\n5. Information-theoretic Perspectives on Multitask Learning\n(5.5) The\nDET\nquick\nADJ\nbrown\nADJ\nfox\nNOUN\njumps\nVERB\nover\nADP\nthe\nDET\nlazy\nADJ\ndog\nNOUN\n.\nPUNCT\n(5.6) The\nADJ\nquick\nADJ\nbrown\nDET\nfox\nDET\njumps\nNOUN\nover\nNOUN\nthe\nADJ\nlazy\nADP\ndog\nVERB\n.\nPUNCT\nThe tag distributions in X and X′ do not change as a result of a re-\nordering as in the examples, hence the tag set entropies will be the\nsame.3 However, the tags in X′ are now likely to have a vanishingly\nlow correspondence with any sort of natural language signal (as in\nthe second sentence), hence X′ is highly unlikely to be a useful aux-\niliary task for X. Measures taking joint probabilities into account\nwill capture this lack of correlation between X and X′. In this work\nwe show that measures such as conditional entropy and MI are much\nmore informative for the effectivity of an auxiliary task than entropy.\n5.3\nData\nFor our syntactic experiments, we use the Universal Dependencies\n(UD) treebanks on 39 out of the 40 languages found in version 1.3\n(Nivre et al., 2016a).4 We experiment with POS tagging as a main task,\nand various dependency relation classiﬁcation tasks (as deﬁned in\nSection 5.3.1) as auxiliary tasks. We also investigate whether our hy-\npothesis ﬁts with recent results in the literature, and train sequence\ntaggers on the collection of semantically oriented tasks presented in\nMartínez Alonso and Plank (2017), as well as on the semantic tagging\ntask in Bjerva et al. (2016b).\nAlthough calculation of joint probabilities requires jointly labelled\ndata, this issue can be bypassed without losing much accuracy. As-\nsuming that (at least) one of the tasks under consideration can be\n3Note that we look at the entropy of the marginal distribution of each tag set,\nas this is what has been hypothesised to be of importance in previous work.\n4Japanese was excluded due to treebank unavailability.\n5.3. Data\n109\ncompleted automatically with high accuracy, we ﬁnd that the esti-\nmates of joint probabilities are very close to actual joint probabilities\non gold standard data. In this work, we estimate joint probabilities\nby tagging the auxiliary task data sets with a state-of-the-art POS tag-\nger.5,6\n5.3.1\nMorphosyntactic Tasks\nDependency Relation Classiﬁcation is the task of predicting the de-\npendency tag (and its direction) for a given token. This is a task that\nhas not received much attention, although it has been shown to be a\nuseful feature for parsing (Ouchi et al., 2014). We choose to look at\nseveral instantiations of this task, as it allows for a controlled setup\nunder a number of conditions for MTL, and since data is available\nfor a large number of typologically varied languages.\nPrevious work has suggested various possible instantiations of de-\npendency relation classiﬁcation labels, differing in the amount of in-\nformation they encode (Ouchi et al., 2014, 2016). In this work, we\nuse labels designed to range from highly complex and informative,\nto relatively basic ones.7 The labelling schemes used are shown in\nTable 5.1. As an example, consider the following dependency graph:\nNo\n,\nit\nwas\nn’t\nBlack\nMonday\n.\nROOT\nVMOD\nP\nSUB\nVMOD\nPRD\nNMOD\nP\n5Since the dependency relation auxiliary task data overlaps with POS tag-\nging data, this allowed us to conﬁrm that the differences between the measures\nobtained with estimated and gold data in this case are negligible (i.e. ≤5%).\n6The POS-tagger used is the deep bi-GRU ResNet-tagger described in Chapter 4.\n7Labels are automatically derived from the UD dependency annotations.\n110\n5. Information-theoretic Perspectives on Multitask Learning\nTable 5.1:\nDependency relation labels used in this work, with en-\ntropy in bits (H) measured on English. The labels differ in the granu-\nlarity and/or inclusion of the category and/or directionality.\nCategory\nDirectionality\nExample\nH\nFull\nFull\nnmod:poss/R_L\n3.77\nFull\nSimple\nnmod:poss/R\n3.35\nSimple\nFull\nnmod/R_L\n3.00\nSimple\nNone\nnmod\n2.03\nNone\nFull\nR_L\n1.54\nNone\nSimple\nR\n0.72\nTable 5.2 shows examples of dependency relation labels based on\nthis graph.\nThe labels encode the head of each word, as well as\nthe relative position, or direction. For instance, the word it is the\nsubject of a word on its right. The more complex tags, for instance\nROOT+SUB/L_PRD/R, include information regarding the dependents\nof each word. In this case, the word was has an obligatory sub depen-\ndent to the left, and an obligatory prd dependent on the right.\nTable 5.2:\nExamples of some dependency relation instantiations\nin context.\nThe columns indicate the granularity used (cate-\ngory/directionality).\nWord\nFull/Full\nSimple/Simple Simple/None None/Simple\nNo\nVMOD/R\nVMOD/R\nVMOD\nR\n,\nP/R\nP/R\nP\nR\nit\nSUB/R\nSUB/R\nSUB\nR\nwas\nROOT+SUB/L_PRD/R ROOT\nROOT\nn’t\nVMOD/L\nVMOD/L\nVMOD\nL\nBlack\nNMOD/R\nNMOD/R\nNMOD\nR\nMonday PRD/L+L\nPRD/L\nPRD\nL\n.\nP/L\nP/L\nP\nL\n5.3. Data\n111\nTable 5.3: Data splitting scheme. The training set is split into three\nequal parts. The annotations in each part differ per condition.\nCondition\nPart I\nPart II\nPart III\nIdentity\nPoS\nPoS ∧DepRel\nn/a\nOverlap\nPoS\nPoS ∧DepRel\nDepRel\nDisjoint\nPoS\nPoS\nDepRel\nThe systems in the syntactic experiments are trained on main task\ndata (Dmain), and on auxiliary task data (Daux). Generally, the amount\nof overlap between such pairs of data sets differs, and can roughly be\ndivided into three categories: i) identity (identical data sets); ii) over-\nlap (some overlap between data sets); and iii) disjoint (no overlap\nbetween data sets). To ensure that we cover several possible experi-\nmental situations, we experiment using all three categories. We gen-\nerate (Dmain, Daux) pairs by splitting each UD training set into three\nportions. The ﬁrst and second portions always contain POS labels. In\nthe identity condition, the second portion contains dependency rela-\ntions. In the overlap condition, the second and ﬁnal portions contain\ndependency relations. In the disjoint condition, the ﬁnal portion con-\ntains dependency relations. Hence, the system always sees the exact\nsame POS tagging data, whereas the amount of dependency relation\ndata and overlap differs between conditions. Each dependency rela-\ntion instantiation is used in our experiments, paired with PoS tagging.\nThe data splitting scheme is shown in Table 5.3.\n5.3.2\nSemantic Tasks\nMartínez Alonso and Plank (2017) experiment with using POS tag-\nging, chunking, dependency relation tagging, and a frequency based\nmeasure as auxiliary tasks, with main tasks based on several seman-\n112\n5. Information-theoretic Perspectives on Multitask Learning\ntically oriented tasks. In this chapter, we limit ourselves to consider-\ning the PoS tagging auxiliary task, for the following semantic main\ntasks.\nNamed Entity Recognition\nFor NER, we use the CONLL2003 shared-task data (e.g. Person, Loc,\netc., Tjong Kim Sang and De Meulder (2003)).\nFrames\nWe use FrameNet 1.5 (Baker et al., 1998) with the same data splits\nas Das et al. (2014) and Hermann et al. (2014). This data set is anno-\ntated for the joint task of frame detection and identiﬁcation. As in\nMartínez Alonso and Plank (2017), we approach this task as a stan-\ndard sequence prediction task.\nSupersenses\nWe experiment with the supersense version of SemCor (Miller et al.,\n1993) from Ciaramita and Altun (2006), using course-grained seman-\ntic labels (e.g. noun.person).\nSemtraits\nWe use the conversions of Martínez Alonso and Plank (2017) of super-\nsenses into coarser semantic traits (e.g. Animate, UnboundedEvent,\netc.), for which they used the EuroWordNet list of ontological types\nfor senses from Vossen et al. (1998).\nMulti-Perspective Question Answering\nWe also consider the Multi-Perspective Question Answering (MPQA)\ncorpus, using the coarse level of annotation (Deng and Wiebe, 2015).\n5.4. Method\n113\nSemantic Tags\nWe also investigate the semantic tagging task of Chapter 4, using the\nsame data splits (Bjerva et al., 2016b).\nWe use the same setup as for our syntactic experiments, by using\nthese semantic tasks as auxiliary tasks with POS tagging as the main\ntask.\n5.4\nMethod\n5.4.1\nArchitecture and Hyperparameters\nWe apply a deep neural network with the exact same hyperparam-\neter settings in each syntactic experiment, with reasonably default\nparameter settings, similar to what was used in Chapter 4. Our sys-\ntem consists of a two layer deep bi-GRU (100 dimensions per layer),\ntaking an embedded word representation (64 dimensions) as input\n(see Figure 5.2). We apply dropout (p = 0.4) between each layer in\nour network (Srivastava et al., 2014). The output of the ﬁnal bi-GRU\nlayer, is connected to two output layers – one per task. Both tasks are\nalways weighted equally. Optimisation is done using the Adam algo-\nrithm (Kingma and Ba, 2014), with the categorical cross-entropy loss\nfunction. We use a batch size of 100 sentences, training over a max-\nimum of 50 epochs, using early stopping and monitoring validation\nloss on the main task.\nWe do not use pre-trained embeddings. We also do not use any\ntask-speciﬁc features, similarly to Collobert et al. (2011), and we do\nnot optimise any hyper-parameters with regard to the task(s) at hand.\nAlthough these choices are likely to affect the overall accuracy of our\nsystems negatively, the goal of our experiments is to investigate the\neffect in change in accuracy when adding an auxiliary task - not ac-\ncuracy in itself.\n114\n5. Information-theoretic Perspectives on Multitask Learning\nFigure 5.2: System architecture used in the multitask learning exper-\niments.\n5.4.2\nExperimental Overview\nIn the syntactic experiments, we train one system per language, de-\npendency label category, and split condition. For sentences where\nonly one tag set is available, we do not update weights based on the\nloss for the absent task.\n5.4.3\nReplicability and Reproducibility\nIn order to facilitate the replicability and reproducibility of our re-\nsults, we take two methodological steps. To ensure replicability, we\nrun all experiments 10 times, in order to mitigate the effect of ran-\ndom processes on our results.8 To ensure reproducibility, we release\na collection including: i) A Docker ﬁle containing all code and depen-\n8Approximately 10,000 runs using 400,000 CPU hours.\n5.5. Results and Analysis\n115\ndencies required to obtain all data and run our experiments used in\nthis work; and ii) a notebook containing all code for the statistical\nanalyses performed in this work.9\n5.5\nResults and Analysis\nTable 5.4: Morphosyntactic tasks. Correlation scores and associated\np-values, between change in accuracy (∆acc) and entropy (H(Y )),\nconditional entropy (H(X|Y ), H(Y |X)), and mutual information\n(I(X; Y )), calculated with Spearman’s ρ, across all languages and la-\nbel instantiations. Bold indicates the strongest signiﬁcant correla-\ntions.\nCondition\nρ(∆acc, H(Y )) ρ(∆acc, H(X|Y )) ρ(∆acc, H(Y |X)) ρ(∆acc, I(X; Y ))\nIdentity\n−0.06 (p=0.214)\n0.10 (p=0.020)\n0.12 (p=0.013)\n0.08 (p=0.114)\nOverlap\n0.07 (p=0.127)\n0.23 (p<0.001)\n0.27 (p<0.001) 0.43 (p≪0.001)\nDisjoint\n0.08 (p=0.101)\n0.26 (p<0.001)\n0.25 (p<0.001) 0.41 (p≪0.001)\nTable 5.5: Change in accuracy, and information theoretic measures,\nfor the semantic tasks.\nAuxiliary task\n∆acc\nH(Y )\nH(X|Y )\nH(Y |X)\nI(X; Y )\nFrames\n-14.64\n1.6\n2.7\n1.4\n0.2\nMPQA\n-5.62\n1.1\n2.6\n1.0\n0.1\nSupersenses\n-2.86\n1.8\n2.7\n1.6\n0.2\nNER\n-1.36\n0.8\n2.6\n0.7\n0.1\nSemtraits\n0.67\n1.3\n3.0\n0.8\n0.5\nSemtagging\n0.79\n3.0\n2.0\n1.5\n1.5\n5.5.1\nMorphosyntactic Tasks\nWe use Spearman’s ρ in order to calculate correlation between aux-\niliary task effectivity (as measured using ∆acc) and the information-\ntheoretic measures. Following the recommendations in Søgaard et al.\n9https://github.com/bjerva/mtl-cond-entropy\n116\n5. Information-theoretic Perspectives on Multitask Learning\nFigure 5.3: Correlations between ∆acc and entropy. Each data point\nrepresents a single experiment run.\nFigure 5.4: Correlations between ∆acc and mutual information. Each\ndata point represents a single experiment run.\n5.5. Results and Analysis\n117\n(2014), we set our p cut-off value to p < 0.0025.\nTable 5.4 shows\nthat MI correlates signiﬁcantly with auxiliary task effectivity in the\nmost commonly used settings (overlap and disjoint). The fact that no\ncorrelation is found in the identity condition between ∆acc and any\ninformation-theoretic measure yields some interesting insights into\nthe issue at hand. Intuitively, this makes sense considering that the\nlack of any extra data in the identity setting does not offer much op-\nportunity for the network to learn from the auxiliary data. In other\nwords, if the model is allowed to train on the same data with essen-\ntially the same label (i.e., if MI is high), this does not allow the model\nto learn anything new. This is supported by the signiﬁcant positive\ncorrelation between MI and ∆acc in the overlap/disjoint conditions,\nin which the model does have access to more data. This further sug-\ngests that in some cases, one of the most effective auxiliary tasks is\nsimply more data for the same task (i.e., the highest MI achievable).\nAdditionally, this shows that high MI between tag sets in identical\ndata is not necessarily helpful, and that in such a setting it may even\nbe advantageous to have a less similar auxiliary task.\nAs hypothesised, entropy has no signiﬁcant correlation with aux-\niliary task effectivity, whereas conditional entropy offers some ex-\nplanation. We further observe that these results hold for almost all\nlanguages, although the correlation is weaker for some languages,\nindicating that there are some other effects at play here. For a sam-\nple of the languages, the correlations between ∆acc and Entropy are\nshown in Figure 5.3, and the correlations between ∆acc and Mutual\nInformation are shown in Figure 5.4.10 We also analyse whether sig-\nniﬁcant differences can be found with respect to whether or not we\nhave a positive ∆acc, using a bootstrap sample test with 10,000 itera-\ntions (Efron and Tibshirani, 1994). We observe a signiﬁcant relation-\nship (p < 0.001) for MI. We also observe a signiﬁcant relationship for\n10These ﬁgures only show a subset of the languages under evaluation. See\nAppendix A for a complete overview.\n118\n5. Information-theoretic Perspectives on Multitask Learning\nconditional entropy (p < 0.001), and again ﬁnd no signiﬁcant differ-\nence for entropy (p ≥0.07).\n5.5.2\nLanguage-dependent results\nResults per language are shown in Table 5.6. While not all languages\nexhibit correlations below our selected α-level, the non-signiﬁcant\nlanguages still exhibit interesting trends in the same direction. Note\nthat there are two cases where Entropy is a fair predictor of ∆acc,\nnamely for Latvian and Turkish. However, in both of these cases the\ncorrelation is stronger still with MI. Furthermore, the correlations be-\ntween ∆acc and entropy vary wildly between languages, sometimes\nexhibiting negative correlations.\n5.5.3\nSemantic Tasks\nWe do not have access to suﬃcient data points to run statistical anal-\nyses on the results obtained by Martínez Alonso and Plank (2017), or\nby Bjerva et al. (2016b), and the results in Table 5.5 do not reveal any\nobvious patterns. A grouping of these results by whether or not ∆acc\nwas positive can be seen in Figure 5.5, which offers some support to\nthe results from the morphosyntactic tasks. However, the lack of a\nclear pattern when looking at individual results per dataset serves to\nhighlight the issue at hand, namely that even though MI offers some\nexplanatory value, the interactions behind the workings of MTL are\nmore complex than what can be explained purely by comparing joint\ndistributions of tag sets.\n5.6\nConclusions\nWe have examined the relation between auxiliary task effectivity\nand three information-theoretic measures. The ﬁrst research ques-\ntion which we aimed to answer in this chapter (RQ 2a) was related\n5.6. Conclusions\n119\nTable 5.6:\nCorrelation scores and associated p-values, between\nchange in accuracy (∆acc) and entropy (H(Y )), conditional entropy\n(H(Y |X)), and mutual information (I(X; Y )), calculated with Spear-\nman’s ρ. Bold indicates the strongest signiﬁcant correlations per row.\nGroup\nLanguage\nρ(∆acc, H(Y ))\nρ(∆acc, H(Y |X))\nρ(∆acc, I(X; Y ))\nGermanic\nDanish\n0.27 (p=0.116)\n0.42 (p=0.011)\n0.78 (p≪0.001)\nDutch\n0.31 (p=0.070)\n0.16 (p=0.337)\n0.55 (p<0.001)\nEnglish\n0.30 (p=0.076)\n0.19 (p=0.280)\n0.58 (p<0.001)\nGerman\n0.03 (p=0.849)\n0.13 (p=0.448)\n0.18 (p=0.293)\nNorwegian\n-0.03 (p=0.858)\n0.23 (p=0.183)\n0.23 (p=0.177)\nSwedish\n-0.03 (p=0.843)\n0.29 (p=0.091)\n0.31 (p=0.068)\nRomance\nCatalan\n0.34 (p=0.042)\n0.33 (p=0.047)\n0.72 (p≪0.001)\nFrench\n0.06 (p=0.734)\n0.38 (p=0.023)\n0.48 (p=0.003)\nGalician\n0.10 (p=0.574)\n0.18 (p=0.304)\n0.28 (p=0.099)\nItalian\n0.12 (p=0.503)\n0.52 (p=0.001)\n0.67 (p≪0.001)\nPortuguese\n-0.02 (p=0.921)\n0.61 (p<0.001)\n0.66 (p<0.001)\nRomanian\n-0.31 (p=0.067)\n0.34 (p=0.040)\n0.04 (p=0.825)\nSpanish\n0.02 (p=0.890)\n0.60 (p<0.001)\n0.70 (p≪0.001)\nSlavic\nBulgarian\n0.20 (p=0.242)\n0.50 (p=0.002)\n0.76 (p≪0.001)\nCroatian\n-0.24 (p=0.159)\n0.43 (p=0.009)\n0.22 (p=0.189)\nCzech\n-0.15 (p=0.376)\n0.49 (p=0.002)\n0.39 (p=0.017)\nO.C. Slavonic\n-0.08 (p=0.634)\n0.34 (p=0.044)\n0.35 (p=0.038)\nPolish\n0.13 (p=0.437)\n0.40 (p=0.015)\n0.59 (p<0.001)\nRussian\n0.29 (p=0.086)\n0.40 (p=0.015)\n0.81 (p≪0.001)\nSlovene\n-0.24 (p=0.156)\n0.41 (p=0.014)\n0.19 (p=0.259)\nTurkic\nKazakh\n0.23 (p=0.172)\n0.04 (p=0.817)\n0.36 (p=0.030)\nTurkish\n0.50 (p=0.002)\n-0.17 (p=0.317)\n0.43 (p=0.008)\nUralic\nEstonian\n0.45 (p=0.006)\n-0.14 (p=0.430)\n0.39 (p=0.017)\nFinnish\n0.02 (p=0.924)\n0.37 (p=0.025)\n0.50 (p=0.002)\nHungarian\n0.14 (p=0.413)\n0.09 (p=0.594)\n0.27 (p=0.116)\nOther\nArabic\n-0.16 (p=0.362)\n0.53 (p<0.001)\n0.47 (p=0.004)\nBasque\n0.41 (p=0.014)\n-0.01 (p=0.952)\n0.49 (p=0.002)\nChinese\n-0.15 (p=0.399)\n0.46 (p=0.005)\n0.41 (p=0.012)\nFarsi\n0.20 (p=0.244)\n0.41 (p=0.012)\n0.75 (p≪0.001)\nGreek\n0.20 (p=0.248)\n0.19 (p=0.264)\n0.44 (p=0.007)\nHebrew\n0.06 (p=0.724)\n0.37 (p=0.028)\n0.52 (p=0.001)\nHindi\n-0.26 (p=0.121)\n0.24 (p=0.161)\n0.00 (p=0.979)\nIrish\n-0.24 (p=0.150)\n0.54 (p<0.001)\n0.35 (p=0.034)\nIndonesian\n-0.42 (p=0.011)\n0.51 (p=0.001)\n0.11 (p=0.510)\nLatin\n0.19 (p=0.271)\n0.16 (p=0.362)\n0.47 (p=0.004)\nLatvian\n0.64 (p<0.001)\n-0.23 (p=0.171)\n0.53 (p<0.001)\nTamil\n0.16 (p=0.337)\n0.12 (p=0.482)\n0.31 (p=0.067)\n120\n5. Information-theoretic Perspectives on Multitask Learning\nFigure 5.5:\nComparison of information theoretic measures and\nchange in accuracy for the semantic tasks. Results are grouped by\nnegative and positive change in ∆acc. Dashed lines are included for\nclarity, and are not meant to imply a linear correlation.\nto which information-theoretic measures are useful for estimation\nof auxiliary task effectivity. While previous research hypothesises\nthat entropy plays a central role, we show experimentally that this\nis not suﬃcient, and that conditional entropy is a somewhat better\npredictor, and that MI is the best predictor under consideration here.\nThis claim is corroborated when we correlate MI and change in ac-\ncuracy with results found in the literature. It is especially interest-\ning that MI is a better predictor than conditional entropy, since MI\nis a symmetric measure, as it does not consider the order between\nmain and auxiliary tasks. For conditional entropy itself, the results\nfor the two directionalities did not differ to a large extent. Our ﬁnd-\n5.6. Conclusions\n121\nings should prove helpful for researchers when considering which\nauxiliary tasks might be helpful for a given main task. Furthermore,\nit provides an explanation for the fact that there is no universally ef-\nfective auxiliary task, as a purely entropy-based hypothesis assumes.\nThe fact that MI is informative when determining the effectivity\nof an auxiliary task can be explained by considering an auxiliary task\nto be similar to adding a feature. That is to say, useful features are\nlikely to be useful auxiliary tasks. Interestingly, however, the gains\nof adding an auxiliary task are visible at test time for the main task,\nwhen no explicit auxiliary label information is available.\nThe second research question which we aimed to answer in this\nchapter (RQ 2b), related to the generalisation ability of the information-\ntheoretic measures as a measure of auxiliary task effectivity, across\nlanguages and NLP tasks. We tested our hypothesis on 39 languages,\nrepresenting a wide typological range, as well as a wide range of data\nsizes. Our experiments were run on syntactically oriented tasks of\nvarious granularities. We also corroborated our ﬁndings with results\nfrom semantically oriented tasks in the literature.\nWhile the correlations between MI and ∆acc were higher than for\nother information-theoretic measures, it is by no means a perfect pre-\ndictor. This highlights the fact that the interactions between tasks is\nmore complex than simply the joint distribution between the tag sets\nat hand. One possibility for future work is to take distributions over\ntags and words into account simultaneously.\nHaving considered interactions between tasks in MTL, and ﬁnd-\ning that task similarity is to some extent predictive of MTL effectivity,\nthis raises the question of what the situation is in the case of mul-\ntilingual learning. This is explored further in the next part of this\nthesis.\nPART III\nMultilingual Learning\nCHAPTER 6\n∗Multilingual\nSemantic Textual Similarity\nAbstract|Up until now, we have seen parameter sharing between tasks,\nin the context of multitask learning. Another possibility is to share pa-\nrameters between languages, in a sense casting model multilinguality as\na type of multitask learning. As a ﬁrst example of multilingual learning,\nwe look at cross-lingual semantic textual similarity. This task can be ap-\nproached by leveraging multilingual distributional word representations,\nin which similar words in different languages are close to each other in\nsemantic space. The availability of parallel data allows us to train such\nrepresentations for a large number of languages. Such representations\nhave the added advantage of allowing for leveraging semantic similarity\ndata for languages for which no such data exists. In this chapter, the\nfocus is on to what extent such an approach allows for enabling zero-shot\nlearning for the task at hand. We also investigate whether language\nrelatedness has an effect on how successful this is. We train and evaluate\non six language pairs for semantic textual similarity, including English,\nSpanish, Arabic, and Turkish.\n∗Chapter adapted from: Bjerva, J. and Östling, R. (2017a). Cross-lingual Learn-\ning of Semantic Textual Similarity with Multilingual Word Representations. In\nProceedings of the 21st Nordic Conference on Computational Linguistics, NoDaL-\niDa, 22-24 May 2017, Gothenburg, Sweden, number 131, pages 211–215. Linköping\nUniversity Electronic Press, Linköpings universitet.\n126\n6. Multilingual Semantic Textual Similarity\n6.1\nIntroduction\nIn order to determine how similar a pair of sentences in two dif-\nferent languages are to one another, it is necessary to have a grasp\nof multilingual semantics.2 Continuous space word representations,\nwhich lend their strength from distributional semantics, are a clear\ncandidate for this problem, as distances in such a space can be di-\nrectly interpreted as semantic similarities (see Chapter 3). Given the\nconstantly increasing amount of available parallel data (e.g., Koehn\n(2005), Tiedemann (2012), Ziemski et al. (2016)), it is possible to learn\nmultilingual word representations for many languages. In this chap-\nter, we approach tasks on cross-lingual semantic textual similarity\nfrom SemEval-2016 (Agirre et al., 2016) and SemEval-2017 (Agirre\net al., 2017) using such word representations. This approach has the\nadvantage that it allows for zero-shot learning while training on mul-\ntiple languages, i.e., exploiting data from several source languages\nfor an unseen target language. We aim to answer the following spec-\niﬁed research question, in order to answer RQ 3:\nRQ 3a To what extent can multilingual word representations be used\nin a simple STS system so as to enable zero-shot learning for un-\nseen languages?\nRQ 3b To what extent is the success of zero-shot learning dependent\nof language relatedness in this setting?\nRelated work\nSemantic Textual Similarity (STS) is the task of assessing the degree\nto which two sentences are similar in their meanings. In the long-\nrunning SemEval STS shared task series, this is measured on a scale\nranging from 0, indicating no semantic similarity, to 5, indicating\n2The exception to this is methods based on machine translation, which are\noutlined in the following section.\n6.1. Introduction\n127\ncomplete semantic similarity (see Agirre et al., 2012; 2013; 2014; 2015;\n2016; 2017). Monolingual STS is an important task, for instance for\nevaluation of machine translation (MT) systems, where estimating\nthe semantic similarity between a system’s translation and the target\ntranslation can aid both system evaluation and development. STS is\nalso a fundamental problem for natural language understanding, as\nbeing able to estimate the similarity between two sentences in their\nmeaning content can be seen as a prerequisite for understanding.\nThe task is already a challenging one in a monolingual setting, such\nas when estimating the similarity between two English sentences. In\nthis chapter, we tackle the more diﬃcult case of cross-lingual STS,\ne.g., estimating the similarity between an English and an Arabic sen-\ntence, in the context of shared tasks on cross-lingual STS at SemEval-\n2016 (Agirre et al., 2016) and SemEval-2017 (Agirre et al., 2017).3,4\nPrevious approaches to the problem of cross-lingual STS have fo-\ncussed on two main approaches. The primary, and most successful\napproach, is to apply a MT system to non-English sentences, and\ntranslating these to English (e.g., Tian et al., 2017, and Wu et al.,\n2017). The advantage of this approach is that the problem essentially\nboils down to estimating the similarity of two sentences in English.\nThere are at least two advantages to this. First of all, the amount\nof resources available for English eclipse what is available for most,\nif not all, other languages. Additionally, comparing two sentences\nin the same language allows for straight-forward application of fea-\ntures based on the surface forms of the sentences, such as word over-\nlap, common substrings, and so on. MT approaches tend to outper-\nform purely multilingual approaches, with the winner of SemEval-\n2017, and many of the top systems of SemEval-2016 relying on this\napproach (Agirre et al., 2016, 2017).\nThere are at least two draw-\n3SemEval-2016 Task 1: Semantic Textual Similarity: A Uniﬁed Framework for\nSemantic Processing and Evaluation – Cross-lingual STS Subtask.\n4SemEval-2017 Task 1: Semantic Textual Similarity.\n128\n6. Multilingual Semantic Textual Similarity\nbacks to this method, however. Primarily, involving a fully-ﬂedged\nMT system severely increases the complexity of a system. Further-\nmore, such methods can be seen as bypassing the problem of cross-\nlingual STS, rather than tackling it directly, as no actual multilingual\nsimilarity assessments are necessarily carried out.\nThe amount of true multilingual approaches in the literature are\nsomewhat more limited, with notable examples from SemEval-2016\nsuch as Lo et al. (2016), who make use of bilingual embedding space\nphrase similarities, in combination with cross-lingual machine trans-\nlation metrics. Another approach is represented by Aldarmaki and\nDiab (2016), who apply bilingual word representations in a matrix\nfactorisation method, so as to assess STS without translation. The\nmethod that bears the most resemblance to the approach taken in\nthis chapter is Ataman et al. (2016), who combine bilingual embed-\ndings with machine translation quality estimation features (Specia\net al., 2013). We expand upon this method by using multilingual word\nembeddings as input, rather than bilingual ones. One advantage of\nour approach, is that it allows for zero-shot learning while training\non multiple languages (see Chapter 3), as it does not depend on an-\nnotated STS training data for the target language, and only places\nrequirements on the availability of parallel data. This denotes the ap-\nproach we take to RQ 3a, as well as RQ 3b. Additionally, our method\ndiffers from Ataman et al. (2016) in that we choose a simpler archi-\ntecture, using only such word representations as input.\n6.2\nCross-lingual Semantic Textual Similarity\nWe will now look at the task of (cross-lingual) STS in more detail.\nGiven two sentences, s1 and s2, the task in STS is to assess how se-\nmantically similar these are to each other. This is commonly mea-\nsured using a scale ranging from 0–5, with 0 indicating no semantic\noverlap, and 5 indicating nearly identical content. In the SemEval\n6.2. Cross-lingual Semantic Textual Similarity\n129\nSTS shared tasks, the following descriptions are used:\n0. The two sentences are completely dissimilar.\n1. The two sentences are not equivalent, but are on the same topic.\n2. The two sentences are not equivalent, but share some details.\n3. The two sentences are roughly equivalent, but some important\ninformation differs/missing.\n4. The two sentences are mostly equivalent, but some unimpor-\ntant details differ.\n5. The two sentences are completely equivalent, as they mean the\nsame thing.\nAs an example of sentence similarities, consider the sentence pairs\nand their human-annotated similarity scores in Table 6.1. These ex-\namples are taken from the SemEval-2014 edition of the shared task\non STS and Recognising Textual Entailment (RTE), giving us access to\nentailment information in addition to similarity scores for the pur-\nposes of the example (Marelli et al., 2014).5\nAttempting to assess\nthe semantic content of two sentences with a simple score notably\ndoes not take important semantic features such as negation into ac-\ncount, and STS can therefore be seen as complimentary to textual\nentailment. For instance, in sentence No. 219 in Table 6.1, the sen-\ntences have a high similarity score, even though their meanings are\nthe opposite of one another. It is also worth to note that STS is highly\nrelated to paraphrasing, as replacing an n-gram with a paraphrase\nthereof ought to alter the semantic similarity of two sentences to a\nvery low degree.\n5RTE is the task of assessing whether the meaning of one sentence (the hy-\npothesis) can be inferred from the other (the text).\n130\n6. Multilingual Semantic Textual Similarity\nTable 6.1: Examples of sentence similarities and corresponding en-\ntailment judgements.\nNo.\nText / Hypothesis\nScore\nRelation\n8678\nA skateboarder is jumping off a ramp\n4.8\nentailment\nA skateboarder is making a jump off a ramp\n2709\nThere is no person boiling noodles\n2.9\ncontradiction\nA woman is boiling noodles in water\n219\nThere is no girl in white dancing\n4.2\ncontradiction\nA girl in white is dancing\nTable 6.2: Examples of cross-lingual sentence similarities.\nEnglish / Spanish\nScore\nThe NATO mission oﬃcially ended Oct. 31.\n5\nLa misión de la OTAN terminó oﬁcialmente oct. 31.\nMass Slaughter on a Personal Level\n3\nEl sacriﬁcio masivo en un nivel personal\nSupport Workers’ Union Will Sue City Over Layoffs\n1\nApoyo a los trabajadores \"Unión va a demandar ciudad más despidos\nSuccessful monolingual approaches in the past have taken advan-\ntage of both the relatedness with this task to paraphrasing, and to\nRTE. Bjerva et al. (2014) attempt to replace words in s1 with para-\nphrases obtained from the Paraphrase Database (PPDB, Ganitkevitch\net al., 2013), in order to increase the surface similarity with s2. Ad-\nditionally, both Bjerva et al. (2014) and Beltagy et al. (2016) make\nuse of (features from) an RTE system to perform the task of STS. Ap-\nproaches similar to these can be applied in cross-lingual STS, if the\nsentence pair is translated to a language for which such resources\nexist.\nAs an example of sentence similarities in cross-lingual STS, con-\n6.3. Method\n131\nsider the sentence pairs and their human-annotated similarity scores\nin Table 6.2. The ﬁrst example contains a Spanish sentence which is a\nfaithful translation of the English one, and has the highest similarity\nscore (5). Although the second example conveys a similar meaning,\nthe translation expresses ’Mass Slaughter’ in Spanish as ’A massive\nsacriﬁce’, resulting in a lower similarity score (3), indicating a loose\ntranslation.6\nIn the third example sentence, the Spanish sentence\nconveys the opposite meaning of the English sentence, and has the\nlowest similarity score (1).\n6.3\nMethod\nAs mentioned in Section 6.1, we approach the task of multilingual\nSTS in a similar manner to Ataman et al. (2016), with the addition\nthat we use multilingual input representations, rather than bilingual\nones. We will now look at how our system is constructed, starting\nwith the input representations.\n6.3.1\nMultilingual word representations\nThere are several methods available for obtaining multilingual word\nrepresentations, as described in Chapter 3. In this chapter, we use\na variant of the multilingual skip-gram method (Guo et al., 2016), as\ndetailed in Chapter 3. This method was chosen as it is both relatively\nsimple, and yields high-quality representations for down-stream tasks,\nas compared to other approaches (Guo et al., 2016).\nThe original\nmethod relies on using cross-lingual contexts, with English as a pivot\nlanguage. For instance, a Spanish word might be used to predict an\nEnglish context, or the other way around. Our approach differs in\nthat we augment the learning objective so as to include multilingual\n6See Bos (2014) for a further discussion of faithful, informative, and loose\ntranslations in the context of parallel corpora.\n132\n6. Multilingual Semantic Textual Similarity\ncontexts, such that we also use, for instance, a Spanish word to pre-\ndict a French word (Figure 3.5 in Chapter 3).\nWe train 100-dimensional multilingual embeddings on the Europarl\n(Koehn, 2005) and UN corpora (Ziemski et al., 2016), including data\nfrom bible translations.7,8 This data was chosen partially since it al-\nlows us to learn such embeddings for a large number of languages,\nin addition to the availability of these corpora.\nThe dimensional-\nity of the embeddings was chosen by balancing a suﬃciently high\nnumber of dimensions with the computational resources necessary\nto compute these embeddings with the extended version of the mul-\ntilingual skip-gram method. Word alignment, which is required for\nthe training of this type of multilingual embeddings, is performed\nusing a tool based on the Efmaral word-alignment tool (Östling and\nTiedemann, 2016).9 This allows us to extract a large amount of mul-\ntilingual (word, context) pairs. We then use these pairs in order to\nlearn multilingual embeddings, by applying the word2vecf tool (Levy\nand Goldberg, 2014a). In our experiments, we use the same param-\neter settings as Guo et al. (2016), training using negative sampling\n(Mikolov et al., 2013a), and with equal weighting of monolingual and\ncross-lingual contexts.10\n6.3.2\nSystem architecture\nWe use a relatively simple neural network architecture, consisting\nof an input layer with pre-trained word embeddings and a network\nof fully connected layers. This means that we need a sentence-level\nrepresentation, based on the multilingual word representations, of-\n7Using the New Testament (approximately 140,000 tokens), available at\nhttp://homepages.inf.ed.ac.uk/s0787820/bible/.\n8Training multilingual embeddings on this data yields a vocabulary coverage\nof over 85% on the development sets of the languages at hand.\n9We use the eﬂomal tool, which uses less memory than efmaral. Default\nparameters are used. Available at https://github.com/robertostling/eflomal.\n10Note that we do not use the same implementation as (Guo et al., 2016).\n6.3. Method\n133\nfering us a choice between methods such as those presented in Chap-\nter 2, Section 2.4.2. Given 100-dimensional word representations for\neach word in our sentence, we opt for the simplistic approach of av-\neraging the vectors across each sentence, such that\n⃗s = 1\n|s|\nX\nw∈s\n⃗w,\n(6.1)\nwhere w is a word in the sentence s, and ⃗w and ⃗s are their vectorial\nrepresentations. This is the same approach that is taken by Ataman\net al. (2016). The resulting sentence-level representations are then\nconcatenated and passed through two fully connected layers with\nReLU activation functions (200 and 100 units, respectively), prior to\nthe output layer. In order to prevent any shift from occurring in the\nembeddings, we do not update these during training. The intuition\nhere is that we do not want the representation for, e.g., dog to be\nupdated, which might push it further away from that of perro. We\nexpect this to be especially important in cases where we train on a\nsingle language, and evaluate on another. The system architecture is\ndepicted in Figure 6.1.\nWe apply dropout (p = 0.5) between each layer (Srivastava et al.,\n2014). All weights are initialised using the approach from Glorot and\nBengio (2010). We use the Adam optimisation algorithm (Kingma and\nBa, 2014), monitoring the categorical cross-entropy of the sentence\nsimilarity score, while sanity-checking against the scores obtained as\nmeasured with Pearson correlation. All systems are trained using a\nbatch size of 40 sentence pairs, over a maximum of 50 epochs, using\nearly stopping monitoring the loss on the validation set. We report\nresults using the model with the lowest validation loss. Hyperparam-\neters are kept constant in all conditions.\n134\n6. Multilingual Semantic Textual Similarity\nLa misión de la OTAN terminó oﬁcialmente oct. 31.\n…\n…\nThe NATO mission ofﬁcially ended Oct. 31. \n…\n…\n5\nAverage Pooling\nAverage Pooling\nFigure 6.1: System architecture used in the semantic textual similarity task.\n6.4. Experiments and Results\n135\n6.3.3\nData for Semantic Textual Similarity\nAs the SemEval STS task series has been running for several years,\nthere is a substantial amount of data available. We use data from\nprevious editions of the tasks on (cross-lingual) STS.11 For English–\nEnglish this includes data from SemEval 2012 through 2015 (Agirre\net al., 2012, 2013, 2014, 2015). For English–Spanish this includes data\nfrom SemEval 2016 and 2017 (Agirre et al., 2016, 2017). For Spanish–\nSpanish this includes data from SemEval 2014 and 2015. For English–\nArabic this includes data from SemEval 2017. Finally, for Arabic–\nArabic, this includes data from SemEval 2017. We use the concatena-\ntion of the training sets of previous editions for training, and validate\nand test on the most recent data for each language pair. An overview\nof the available data is shown in Table 6.3.\nTable 6.3: Training data used for (cross-lingual) STS from the Se-\nmEval shared task series.\nLanguage pair\nN sentence pairs\nSemEval edition(s)\nEnglish – English\n3,000\n2012 – 2015\nEnglish – Spanish\n3,900\n2016 – 2017\nSpanish – Spanish\n1,500\n2014 – 2015\nEnglish – Arabic\n2,000\n2017\nArabic – Arabic\n900\n2017\n6.4\nExperiments and Results\nWe investigate whether using a multilingual input representation\nand shared weights allow us to ignore languages in STS, after map-\nping words to their multilingual representations. This, in turn, is one\napproach for enabling zero-shot learning for this task (RQ 3a). We\n11This is what is generally recommended by the shared task organisers, and is\nfollowed by most participating systems.\n136\n6. Multilingual Semantic Textual Similarity\nﬁrst train and evaluate single-source trained systems (i.e. on a single\nlanguage pair), and evaluate this both using the same language pair\nas target, and on all other target language pairs. In doing so, we in-\nvestigate the extent to which the availability of parallel data allows\nus to train STS systems without access to STS training data for a given\nlanguage.\nSecondly, we investigate the effect of bundling training data to-\ngether, in multi-source training, investigating which language pair-\nings are helpful for each other. Concretely, in single-source train-\ning, we only train on one out of the language pairs at a time, and\nevaluate the resulting single-source model on all language pairs. In\nmulti-source training, however, a model is trained on several lan-\nguage pairs at a time, and the resulting model is evaluated as in the\nsingle-source training setting. This is done so as to offer insight into\nRQ 3b.\nWe measure performance between target similarities and system\noutput using the Pearson correlation measure, as this is standard in\nthe SemEval STS shared tasks. We ﬁrst present results on the devel-\nopment sets, and ﬁnally the oﬃcial shared task evaluation results.\n6.4.1\nComparison with Monolingual Representations\nAs a baseline, we compare multilingual embeddings with the perfor-\nmance obtained using the pre-trained monolingual Polyglot embed-\ndings (Al-Rfou et al., 2013). Training and evaluating on the same lan-\nguage pair yields comparable results regardless of embeddings (Ta-\nble 6.4). This shows that our multilingual embeddings, at the very\nleast, have comparable quality to purely monolingual embeddings\nin a monolingual setting.\n6.4. Experiments and Results\n137\nTable 6.4: Single-source training results (Pearson correlations) with\nmonolingual embeddings (polyglot) as compared to multilingual em-\nbeddings (multilingual skipgram) on the SemEval-2017 development\nset. Rows indicate evaluation language, and rows indicate the em-\nbeddings used. Bold numbers indicate best results per row.\nPolyglot\nMultilingual Skipgram\nEnglish\n0.68\n0.69\nSpanish\n0.65\n0.65\nArabic\n0.70\n0.71\nTable 6.5: Single-source training results with multilingual embed-\ndings on the SemEval-2017 development set (Pearson correlations).\nColumns indicate training language pairs, and rows indicate testing\nlanguage pairs. Bold numbers indicate best results per row.\nPPPPPPPP\nTest\nTrain\nen–en\nen–es\nen–ar\nes–es\nar–ar\nen–en\n0.69\n0.07\n-0.04\n0.64\n0.54\nen–es\n0.19\n0.27\n0.00\n0.18\n-0.04\nen–ar\n-0.44\n0.37\n0.73\n-0.10\n0.62\nes–es\n0.61\n0.07\n0.12\n0.65\n0.50\nar–ar\n0.59\n0.52\n0.73\n0.59\n0.71\n6.4.2\nSingle-source training\nResults when training on a single source corpus, using multilingual\nembeddings, are shown in Table 6.5. Training on the target language\npair generally yields the highest results, except for one case. When\nevaluating on Arabic–Arabic sentence pairs, training on English–Arabic\ntexts yields comparable, or slightly better, performance than when\ntraining on Arabic–Arabic. Observing the results from a zero-shot\nlearning perspective, it seems that certain language combinations\n138\n6. Multilingual Semantic Textual Similarity\ncan beneﬁt from this approach. Mainly, it seems to be the case that\nthis approach is suitable when training on a monolingual source pair\n(such as English–English), and evaluating the model on a monolin-\ngual target pair (such as Spanish–Spanish). The gap in performance\nbetween such cases, and a system where target and source languages\nare identical is relatively small. One example of this is the results\nof evaluating on English–English when training on English–English\n(0.69) as compared to when training on Spanish–Spanish (0.64). We\ncan also observe that zero-shot learning in this setting is more suc-\ncessful between the Indo-European languages Spanish and English,\nthan when involving the Semitic language Arabic.\n6.4.3\nMulti-source training\nWe combine training sets from two language pairs in order to inves-\ntigate how this affects evaluation performance on target language\npairs. We copy the single-source setup, except for that we also add\nin the data belonging to the source-pair at hand, e.g., training on\nboth English–Arabic and Arabic–Arabic when evaluating on Arabic–\nArabic (see Table 6.6).\nTable 6.6: Results with one source language in addition to target-\nlanguage data with multilingual embeddings on the SemEval-2017\ndevelopment set (Pearson correlations).\nColumns indicate added\nsource language pairs, and rows indicate target language pairs. Bold\nnumbers indicate best results per row.\nPPPPPPPP\nTest\nTrain\nen–en\nen–es\nen–ar\nes–es\nar–ar\nen–en\n0.69\n0.68\n0.67\n0.69\n0.71\nen–es\n0.22\n0.27\n0.30\n0.22\n0.24\nen–ar\n0.72\n0.72\n0.73\n0.71\n0.72\nes–es\n0.63\n0.60\n0.63\n0.65\n0.66\nar–ar\n0.71\n0.72\n0.75\n0.70\n0.71\n6.4. Experiments and Results\n139\nWe observe that the monolingual language pairings (English–English,\nSpanish–Spanish, Arabic–Arabic) appear to be beneﬁcial for one an-\nother, also in this setting. Interestingly, adding Arabic–Arabic train-\ning data seems to improve the performance on both English–English\nand Spanish–Spanish. Although, this difference is small and likely\nnot signiﬁcant, it is interesting that the models performance does\nnot worsen as an effect of adding this data. This might have been\nthe case, if model capacity is wasted on solving the task for Arabic–\nArabic.\nFinally, we run a multilingual ablation experiment, in which we\ntrain on two out of three of these language pairs, and evaluate on all\nthree. Notably, excluding all Spanish training data yields compara-\nble performance to including it (Table 6.7). Additionally, we see the\nlargest drop in performance when evaluating on Arabic data without\nhaving trained on it. This adds further support to the ﬁndings in Ta-\nble 6.5, indicating that language relatedness is of importance for the\nsuccess of zero-shot learning as applied here.\nTable 6.7: Multilingual ablation results with multilingual embed-\ndings on the SemEval-2017 development set (Pearson correlations).\nColumns indicate ablated language pairs, and rows indicate testing\nlanguage pairs. The none column indicates no ablation, i.e., training\non all three monolingual pairs. The bold diagonal indicates results\nwhen the target language pair is not used as a source language pair.\nXXXXXXXXXX\nX\nTest\nAblated\nen–en\nes–es\nar–ar\nnone\nen–en\n0.60\n0.69\n0.69\n0.65\nes–es\n0.64\n0.64\n0.67\n0.60\nar–ar\n0.68\n0.66\n0.58\n0.72\n140\n6. Multilingual Semantic Textual Similarity\nTable 6.8: Results with multilingual embeddings on SemEval-2017\nShared Task Test sets. In the multi-source and ablation conditions,\nwe use the systems with the best validation performance for the tar-\nget language pair.\nThe columns indicate the evaluation language,\nand the Primary column indicate the aggregated results over all lan-\nguages, as used in SemEval-2017 (Agirre et al., 2017). The wmt col-\numn denotes the es–en test set drawn from WMT’s quality estima-\ntion track.\nRows indicate our three systems, compared with the\nECNU system (Tian et al., 2017), and the LIPN–IIMAS system (Arroyo-\nFernández and Ruiz, 2017).\nPrimary ar–ar ar–en es–es es–en wmt en–en en–tr\nSingle-source\n0.315 0.289 0.105 0.661 0.239 0.030\n0.691 0.188\nMulti-source\n0.294 0.312 0.129 0.692 0.100 0.016\n0.688 0.120\nAblation\n0.215 0.003 0.110 0.547 0.226 0.020\n0.506 0.090\nLIPN–IIMAS\n0.107 0.047 0.077 0.153 0.172 0.145\n0.074 0.080\nECNU\n0.732 0.744 0.749 0.856 0.813 0.336\n0.852 0.771\n6.4.4\nResults on SemEval-2017\nIn order to compare our system’s performance in itself with state-\nof-the-art systems, we participated in the oﬃcial shared task results\nof SemEval-2017 (Bjerva and Östling, 2017b). The results from the\noﬃcial evaluation are shown in Table 6.8. Although our results for\nSpanish–Spanish and English–English are in line with our develop-\nment results, the results for all other language pairs are far lower\nthan expected, and worse than the best performing ECNU system\n(Tian et al., 2017). The fact that the system was low in the ranking\nlist for the shared task can be explained by several factors. On the\none hand, our approach was very simplistic, whereas other systems\ntook more involved approaches. For instance, the ECNU submission\nﬁrst translates all sentences to English, and then use an ensemble\nof four deep neural network models and three feature engineered\nmodels. The features used included word alignments, summarisa-\n6.4. Experiments and Results\n141\ntion and MT evaluation metrics, kernel similarities of bags of words,\nbags of dependencies, n-gram overlap, edit distances, length of com-\nmon preﬁxes, suﬃxes, and substrings, tree kernels, and pooled word\nembeddings (Tian et al., 2017). In contrast, our system only uses the\nlatter of these features. On the other hand, our system did outper-\nform other systems in individual language tracks. Additionally, in all\ntracks we outperform the LIPN–IIMAS system, which approaches the\ntask using an attentional LSTM (Arroyo-Fernández and Ruiz, 2017).\nUnderﬁtting might be an explanation for the low results obtained\nas compared to development, indicating that the approach we have\ntaken is simply not suﬃcient to solve the task of (cross-lingual) STS\nwell. Comparing our approach with other systems, such as the ECNU\nsystem, does indeed reveal a staggering difference in complexity.\n6.4.5\nResults on SemEval-2016\nIn order to further evaluate our system, we compare with an ap-\nproach which is relatively similar to ours, namely the FBK HLT-MT\nsubmission described by Ataman et al. (2016). We replicate the train-\ning, development, and test setting used in Shared task SemEval-2016,\nwith the exception that we only evaluate on one of the domains (Agirre\net al., 2016). The results from the news domain (English-Spanish) are\nshown in Table 6.9. On this dataset, the difference between our sys-\ntem and that of Ataman et al. (2016) is relatively small.\nTable 6.9: SemEval 2016 system comparison, comparing our single-\nsource system with the FBK HLT-MT system (Ataman et al., 2016).\nThe runs from FBK HLT-MT differ in the features used.\nSystem\nScore\nFBK HLT-MT – Run 1\n0.243\nFBK HLT-MT – Run 2\n0.244\nFBK HLT-MT – Run 3\n0.255\nOur system\n0.241\n142\n6. Multilingual Semantic Textual Similarity\n6.5\nConclusions\nAlthough our system faired relatively poorly in the oﬃcial results for\nSemEval-2017, the multilingual experiments presented in this chap-\nter offer insights into research question (RQ 3). Multilingual word\nrepresentations allow us to leverage a large amount of data from par-\nallel corpora, opening up for multilingual learning of semantic tex-\ntual similarity. This allows for zero-shot learning, which yielded rela-\ntively good performance on unseen target languages on the develop-\nment sets under consideration. This indicates that multilingual word\nrepresentations are indeed suitable for enabling zero-shot learning\nfor STS (RQ 3a). As for language relatedness, we found that apply-\ning zero-shot learning, and sharing parameters, between the Indo-\nEuropean languages Spanish and English was more beneﬁcial in gen-\neral than when involving the Semitic language Arabic (RQ 3b). Hav-\ning seen that language similarities to some extent are indicative of\nperformance in zero-shot STS, this raises the question of whether this\ngeneralises to other tasks, to what extent language similarities are\nimportant for this, and how such language similarities can be quan-\ntiﬁed. We approach this in the following chapter, where we will look\nat RQ 4.\nCHAPTER 7\n∗Quantifying the Effects of\nMultilinguality in NLP\nSequence Prediction Tasks\nAbstract|The fact that languages tend to share certain properties can\nbe exploited by, e.g., sharing parameters between languages. This type\nof model multilinguality is relatively common, as taking advantage of\nlanguage similarities can be beneﬁcial. However, the question of when\nmultilinguality is a useful addition in terms of monolingual model per-\nformance is left unanswered. In this chapter, we explore this issue by\nexperimenting with a sample of 60 languages on a selection of tasks: se-\nmantic tagging, part-of-speech tagging, dependency relation tagging, and\nmorphological inﬂection. We compare results under various multilin-\ngual model transfer conditions, and ﬁnally observe correlations between\nmodel effectivity and two measures of language similarity.\n∗Chapter adapted from: Bjerva, J. (in review) Quantifying the Effects of\nMultilinguality in NLP Sequence Prediction Tasks.\n144\n7. Comparing Multilinguality and Monolinguality\n7.1\nIntroduction\nLanguages tend to resemble each other on various levels, for instance\nby sharing syntactic, morphological, or lexical features. Such similar-\nities can have many different causes, such as common language an-\ncestry, loan words, or being a result of universals and constraints in\nthe properties of natural language itself (cf. Chomsky (2005); Hauser\net al. (2002)). Several current approaches to problems in Natural Lan-\nguage Processing (NLP) take advantage of these similarities. For in-\nstance, in model-transfer settings, parsers are frequently trained on\n(delexicalised) versions of entire treebanks, whereas in annotation\nprojection, word alignments between translated sentences are used\n(McDonald et al., 2011b; Täckström et al., 2012; Tiedemann, 2015; Am-\nmar et al., 2016; Vilares et al., 2016; Agić et al., 2016).2 In the case\nof language modelling, multilinguality can be taken advantage of,\ne.g., in order to model domain-speciﬁc or diachronically speciﬁc lan-\nguage variants (Östling and Tiedemann, 2017). In addition to these\nspeciﬁc examples, multilinguality in NLP models has been used in a\nwhole host of other tasks, such as part-of-speech (PoS) tagging and se-\nmantic textual similarity, and is especially useful for NLP for low re-\nsource languages (Georgi et al., 2010; Täckström et al., 2013; Faruqui\nand Lample, 2016; Agić et al., 2017). One concrete advantage of tak-\ning a multilingual approach, is that this allows for the exploitation\nof much larger amounts of data, as compared to using monolingual\napproaches. This fact, together with the prevalence of multilingual\napproaches in modern NLP, highlight the importance of further re-\nsearch in this area.\nAlthough the literature contains a large amount of successful mul-\ntilingual approaches, it is not suﬃciently clear in which cases multi-\nlinguality is likely to be helpful. The previous chapter served as an\nexample for this, with some tentative indications of which combina-\n2These approaches are covered in Chapter 3.\n7.1. Introduction\n145\ntions were useful, based on typological relatedness. Hence, it may be\nreasonable to assume that choosing typologically similar languages\nwhen building a multilingual model will be beneﬁcial, however this\nis not always the case, and relying on intuition or personal language\nknowledge in such matters has its limitations. Hence, the go-to ap-\nproach when considering multilinguality as a means of performance\nimprovement in an NLP setting, is the time-consuming and resource-\nexhausting process of trial and error. In this chapter, the aim is to\nprovide insight into how one might approach the selection of lan-\nguages when considering model multilinguality. We investigate the\nfollowing research questions, in order to provide an answer to RQ 4:\nRQ 4a Given a model trained on a language, l1, does adding data for\nanother language, l2, increase the performance on l1 if those\nlanguages are similar?\nRQ 4b In which way can such similarities be quantiﬁed?\nRQ 4c What correlations can we ﬁnd between model performance\nand language similarities?\nWe experiment on four NLP tasks: semantic tagging, part-of-speech\ntagging, dependency relation tagging, and morphological inﬂection.\nThe language sample we use differs per task, and covers a total of\n60 languages from a typologically diverse sample. After covering re-\nlated work, we ﬁrst present experiments in multilingual settings for\neach of these tasks (Sections 7.2, 7.3, and 7.4). We then investigate\nthe correlations between two different measures of language similar-\nity, and the change in system performance observed in multilingual\nsettings, in order to provide an answer to our research questions\n(Section 7.5). An approach which can be considered as parallel to\nthis effort, is works similar to Rosa et al. (2017), in which similarities\nbetween languages are exploited when deciding on which features\nto use in a cross-lingual parser.\n146\n7. Comparing Multilinguality and Monolinguality\n7.2\nSemantic Tagging\n7.2.1\nBackground\nThe ﬁrst task under consideration is semantic tagging, as introduced\nin Bjerva et al. (2016b), and described in more detail in Chapter 4.\nIn this chapter, we consider this task in a multilingual setting, which\nis possible since the Parallel Meaning Bank (PMB, Abzianidze et al.,\n2017) includes such data for four languages: English, Dutch, German,\nand Italian.\n7.2.2\nData\nWe use semantic tagging data obtained from the PMB (Abzianidze\net al., 2017). There is a relatively large amount of gold standard an-\nnotation for English, which we use in our experiments. Note that, we\nonly use data from the PMB in this setting, as opposed to the setting\nin Chapter 4 in which we also use data from the Groningen Meaning\nBank (GMB, Bos et al., 2017). For the languages other than English,\ni.e., Dutch, German, and Italian, we rely on the projected tags based\non this gold standard annotation. These tags were projected as de-\nscribed by Abzianidze et al. (2017), using word alignments obtained\nwith GIZA++ (Och and Ney, 2003). As the amount of parallel text dif-\nfers per language, this yields the data amounts listed in Table 7.1.\nTable 7.1: Overview of the semantic tagging data used in this work.\nLanguage\nTokens\nSentences\nStatus\nEnglish\n20,098\n2,814\nGold\nDutch\n3,446\n506\nProjected\nGerman\n13,702\n1,960\nProjected\nItalian\n11,376\n1,711\nProjected\n7.2. Semantic Tagging\n147\n7.2.3\nMethod\nWe employ a relatively simple neural network tagger for all of the tag-\nging tasks in this study. The tagger used is a bi-directional Long Short-\nTerm Memory model (Bi-LSTM, Hochreiter and Schmidhuber (1997);\nGraves and Schmidhuber (2005)). We use a single hidden layer for\neach direction, as shown in Figure 7.1. The input-representations\nused are 100-dimensional multilingual word embeddings trained on\nUN (Ziemski et al., 2016), Europarl (Koehn, 2005), and Bible data, us-\ning multilingual skip-gram (Guo et al., 2016), based on word align-\nments obtained with a variant of EFMARAL (Östling and Tiedemann,\n2016).3,4\nThe neural architecture used in this experiment is simpler than,\ne.g., the deep residual network Bi-GRU presented in Chapter 4. This\nchoice was made mainly for three reasons. The primary motivation\nis that we wanted to rely only on multilingual word representations,\nseeing how far this will get us, without dealing with morphologi-\ncal dissimilarities, or exploiting morphological similarities directly.\nAdditionally, using only word representations is one way of using\nfewer parameters, meaning that fewer computational resources are\nneeded. Finally, although systems using character-based represen-\ntations generally perform better than ones using only word-based\nrepresentations, we are not interested in absolute performance per\nse, but rather relative changes in performance when building multi-\nlingual models.\nHyperparameters\nWe use the same hyperparameter settings for each of the experimen-\ntal settings, so as to ensure comparability. These are detailed in Ta-\n3Using the default parameter settings for eﬂomal:\nhttps://github.com/robertostling/eflomal.\n4These are the same embeddings used in the previous chapter.\n148\n7. Comparing Multilinguality and Monolinguality\nFigure 7.1: Sketch of the Bi-LSTM architecture used for our tagging\ntasks.\nTable 7.2: Hyperparameters used for semantic tagging.\nHyperparameter\nSetting\nNotes\nLibrary\nChainer (Tokui et al., 2015)\nLoss function\nCategorical Cross-Entropy\nOptimiser\nAdam (Kingma and Ba, 2014)\nTraining iterations\nEarly stopping, best val loss\nBatch size\n4 sentences\nRegularisation\nDropout (Srivastava et al., 2014)\np = 0.5\nRegularisation\nWeight decay (Krogh and Hertz, 1992)\nϵ = 10−4\nble 7.2. We always train for a maximum of 50 epochs, using the epoch\nat which validation loss was the best for evaluation.\n7.2.4\nExperiments and Analysis\nWe ﬁrst look at a high-resource to low-resource scenario, using the\nlanguages for which we have a large amount of data (English, Ger-\nman, and Italian) as source languages, and all four languages as tar-\nget languages. We then run further experiments using all four lan-\nguages as source languages.\nSince only relatively few tokens are\n7.2. Semantic Tagging\n149\navailable for Dutch we train on 1000 tokens, as this allows us to com-\npare all four languages equally. We reserve 500 tokens for each lan-\nguage as test data, and split the remaining data into 80% for training\nand 20% for validation. Since we deal with parallel texts, we make\nsure that the training, development, and test sets used for l1 and any\nl2 do not overlap in any way during training.\nTransfer from high-resource to low-resource languages\nThe goal of this experiment is to investigate zero-shot learning be-\ntween languages for semantic tagging, in a scenario in which we\nuse all training data available, giving us access to between 10,000\nand 20,000 tokens of annotated data for the source languages. Given\nthe tentative results of the previous chapter, we expect that the Ger-\nmanic languages will be more beneﬁcial for one another, as com-\npared to Italian.\nTable 7.3: Results on high-resource to low-resource transfer. Bold\nindicates the best source language for each target language, not con-\nsidering the cases in which the source and target languages are iden-\ntical, which are denoted by italics.\nPPPPPPPP\nTest\nTrain\nEnglish\nGerman\nItalian\nEnglish\n75.03%\n49.20%\n35.45%\nDutch\n49.30%\n56.90%\n36.31%\nGerman\n41.99%\n67.41%\n41.17%\nItalian\n35.74%\n39.11%\n70.89%\nThe results from these experiments are shown in Table 7.3, in\nwhich bold represent the best source languages for each target lan-\nguage, and italics denote the cases in which source and target lan-\nguages are the same. We can observe that the results when the source\nand target languages are both Germanic are higher than when the\n150\n7. Comparing Multilinguality and Monolinguality\nRomance language Italian is involved. For instance, using German\nas a source language for Dutch results in relatively high accuracies.\nThis can be explained by two factors. For one, it is likely that the qual-\nity of the multilingual word embeddings is higher when comparing\nGerman and Dutch. Additionally, the extensive similarities between\nthese two languages is likely to make zero-shot learning relatively\neasy.\nNote that these results are not strictly comparable to those in\nChapter 4, since we both use different and less training and evalu-\nation data. In absolute terms, performance on similar data is around\n5% worse in this experimental setting, although we train on approxi-\nmately one order of magnitude less data than in Chapter 4.5\nTransfer from low-resource source languages\nIn the low-resource source scenario, with 1000 training tokens per\nlanguage, we compare some linguistic and input representation con-\nditions. We run experiments with two input representation settings:\ni) with frozen pre-trained word representations; ii) with updated pre-\ntrained word representations. By frozen word representations, we\nrefer to representations which are kept at their initial states during\nlearning. That is to say, errors are not back-propagated into the em-\nbeddings. In the updated condition, however, embeddings are up-\ndated during training. This comparison is done so as to investigate\nwhether there is a difference when enforcing the multilingual em-\nbedding space to remain in its initial state, thus preserving multilin-\ngual distances.\nIn combination with the two representation settings, we also use\ntwo linguistic settings: a) monolingual training; and b) multilingual\ntraining. The monolingual training serves as a baseline, indicating\n5In order to compare the architectures used, we also train and evaluate our\ntagger on the same data as in Chapter 4, in which case we obtain approximately\nthe same performance as the baseline using only word representations.\n7.2. Semantic Tagging\n151\nhow well we can transfer semantic tags from a source language to a\ntarget language, by only training on the target language. In the mul-\ntilingual setting, we add training data for the target language, com-\nparing transfer between languages in a multilingual setting.\nThis\ncomparison is done so as to investigate to what extent we can take\nadvantage of data from two low resource languages, with the goal of\nbeneﬁtting both languages.\nWe ﬁrst present results from the monolingual training in both\ninput-representation settings, in Tables 7.4a, and 7.4b.\nTable 7.4: Results on monolingual semantic tagging.\n(a) Training on 1k monolingual tokens, frozen embeddings.\nPPPPPPPP\nTest\nTrain\nEnglish\nDutch\nGerman\nItalian\nEnglish\n64.54%\n37.24%\n36.40%\n28.54%\nDutch\n36.32%\n53.20%\n44.80%\n28.75%\nGerman\n35.10%\n39.12%\n54.36%\n37.31%\nItalian\n30.29%\n30.43%\n29.40%\n61.20%\n(b) Training on 1k monolingual tokens, updated embeddings.\nPPPPPPPP\nTest\nTrain\nEnglish\nDutch\nGerman\nItalian\nEnglish\n64.90%\n38.82%\n37.43%\n31.51%\nDutch\n39.86%\n56.78%\n40.93%\n30.81%\nGerman\n39.14%\n40.23%\n55.46%\n39.07%\nItalian\n25.09%\n34.02%\n31.21%\n49.63%\nNot surprising, monolingual models trained on the target language\nconsistently perform better than when the source language is dif-\nferent from the target language, as in zero-shot learning. Nonethe-\nless, all results when source and target languages differ outperform a\nmost frequent class baseline (17.35%) by far. This is expected, as the\n152\n7. Comparing Multilinguality and Monolinguality\npre-trained models have been trained on a relatively large amount\nof parallel data, and have formed word spaces which are uniﬁed\nacross languages, which allows them to generalise somewhat across\nlanguages, and conﬁrms the quality of the embeddings themselves.\nComparing these results to those of the high-resource to low-resource\nsetting, we can observe a steep drop in performance. This is due to\nthe fact that we have approximately an order of magnitude less data\nin the current setting.\nSurprisingly, updating the pre-trained word embeddings during\ntraining increases results for most source/target combinations (En-\nglish/Italian being the exception). This was unexpected, as it is usu-\nally beneﬁcial to update such embeddings during training, as this\nallows them to learn representations which are tuned for the task\nat hand. It was, however, not expected that this would be the case\nwhen applying model transfer, as we expected that tuning, say, the\nEnglish representation for dog to be more task-speciﬁc, would skew\nit away from that of the Dutch equivalent hond. It would be interest-\ning to explore this further, observing the resulting word-space after\nupdating only one language in a multilingual language space. Italian,\ninterestingly, sees a severe drop in performance when updating em-\nbeddings in a monolingual setting. This might be explained by the\nupdated embeddings being overﬁt, and not generalising to the test\nset.\nTransfer between low-resource source languages\nWe now turn to transfer between low-resource source languages. In\nthis setting, we are mainly interested in seeing whether more related\nlanguages, i.e. English, Dutch, and German, are more beneﬁcial to\ncombine with one another, than with the typologically more distant\nItalian language.6\n6We will consider how these similarities can be quantiﬁed in Section 7.5.\n7.2. Semantic Tagging\n153\nTable 7.5: Results on multilingual semantic tagging.\n(a) Training on 1k+1k multilingual tokens, frozen embeddings.\nPPPPPPPP\nTest\nTrain\nEnglish\nDutch\nGerman\nItalian\nEnglish\n64.54%\n65.38%\n64.98%\n65.08%\nDutch\n56.07%\n53.20%\n60.20%\n54.96%\nGerman\n54.96%\n55.30%\n54.36%\n54.49%\nItalian\n47.69%\n47.25%\n47.29%\n61.20%\n(b) Training on 1k+1k multilingual tokens, updated embeddings.\nPPPPPPPP\nTest\nTrain\nEnglish\nDutch\nGerman\nItalian\nEnglish\n64.90%\n39.53%\n64.98%\n22.85%\nDutch\n37.56%\n56.78%\n60.20%\n26.69%\nGerman\n39.21%\n40.12%\n55.46%\n21.87%\nItalian\n26.27%\n34.47%\n47.29%\n49.63%\nTables 7.5a, and 7.5b contain the results from semantic tagging\nwith multilingual training. Considering the rows in each table, it is\ngenerally the case that training a model with a combination of En-\nglish, Dutch and German, improves results more than combining one\nof these with Italian. This seems to hold in both conditions, with and\nwithout updating the pre-trained vectors in training.\nIn contrast to the monolingual training case, we here do observe\nthat freezing the vectors during training is beneﬁcial for model per-\nformance. It may be the case that, since the weights of the embed-\ndings in both l1 and l2 are optimised, they are pushed even further\napart than in the monolingual training case in which only one lan-\nguage’s embeddings are affected. Hence, in the frozen case, the in-\n154\n7. Comparing Multilinguality and Monolinguality\ntegrity of the multilingual language space is maintained, allowing the\nmodel to learn cross-lingually. A potential explanation for the drop\nin results on Italian when updating the embeddings might be the ex-\ntent to which the languages are similar. Since English, German, and\nDutch are all Germanic languages, it is possible that this relatedness\nsuﬃces to preserve the multilingual quality of the word space.\n7.2.5\nSummary of Results on Semantic Tagging\nWe have observed that training on similar languages is helpful for\nsemantic tagging, in the sense that combining Germanic languages\ntended to be beneﬁcial. Additionally, training in a high-resource sce-\nnario on, e.g., German and using Dutch as a source language yielded\nbetter results than when training on low-resource Dutch (see Tables 7.3\nand 7.4a). This leads us to ask whether similar patterns can be found\nwhen observing a larger sample of languages, and on other tasks.\n7.3\nTagging Tasks in the Universal Dependencies\nThe Universal Dependencies treebank offers an excellent testing ground\nfor experiments on NLP model multilinguality. The corpus collection\ncontains many languages, with several layers of uniform annotation\nacross languages (Nivre et al., 2016b). We use version 2.0 of the UD\ntreebanks for experiments in two tasks: PoS tagging and dependency\nrelation tagging (Nivre et al., 2017). We evaluate on the 48 languages\nfor which training data is available.\n7.3.1\nData\nIn order to balance our experiments for differing data sizes, we bal-\nance all training sets so as to have an equal number of tokens. We\nset this amount to 20,000 tokens, in order to allow for inclusion of\nthe smallest language in the UD (Vietnamese, n = 20285). Hence, the\n7.3. Tagging Tasks in the Universal Dependencies\n155\noverall results obtained will be relatively low, but should make the ef-\nfects of multilingual modelling clearer. A part of the evaluation will\ndeal with grouping languages per language group. An overview of\nwhich languages are included in the Germanic, Romance, and Slavic\nfamilies in these evaluations is given in Table 7.6.\n7.3.2\nMethod\nWe employ the same tagger as described in the semantic tagging\nexperiments of this chapter, detailed in Section 7.2.3. We also use\nthe same hyperparameter settings, as shown in Table 7.2, and the\nsame input representations. The main difference with the semantic\ntagging experiments is therefore simply the tasks at hand, and the\namount of languages under consideration.\nExperimental Setup\nWe only use the setting with frozen word representations, as estab-\nlished in the semantic tagging experiments. We focus on results from\nthe multilingual training settings, as we are interested in how these\nresults differ between language pairs. Additionally, we consider two\ntasks in this setup: PoS tagging, and dependency relation tagging. De-\npendency relation tagging is the task of predicting the dependency\ntag (and its direction) for a given token. This is a task that has not re-\nceived much attention, although it has been shown to be a useful fea-\nture for parsing (Ouchi et al., 2014, 2016).7 These deprel tags can be\nderived directly from UD dependency parse trees, making it straight-\nforward to evaluate on this task for the same sample of languages\nin the same settings. In this setting, we use the dependency relation\ninstantiations with simple granularity and simple directionality (i.e.,\nencoding the head and its relative position, for each word), described\nfurther in Chapter 5 (Table 5.1).\n7Dependency relation labels are discussed in more detail in Chapter 5.\n156\n7. Comparing Multilinguality and Monolinguality\nTable 7.6: Language grouping of the Germanic, Romance, and Slavic\nlanguages used in our experiments.\nLanguage family\nLanguage\nGermanic\nAfrikaans\nDanish\nDutch\nEnglish\nGerman\nNorwegian Bokmål\nNorwegian Nynorsk\nSwedish\nSlavic\nBelarusian\nBulgarian\nCroatian\nCzech\nOld Church Slavonic\nPolish\nRussian\nSerbian\nSlovak\nSlovenian\nUkrainian\nRomance\nCatalan\nFrench\nGalician\nItalian\nLatin\nPortuguese\nBrazilian Portuguese\nRomanian\nSpanish\n7.3. Tagging Tasks in the Universal Dependencies\n157\n7.3.3\nResults and Analysis\nDue to the large number of language pairs, we discuss the results\nfrom the mean accuracy on a language group when trained in com-\nbination with a sample of languages.8\nPoS Tagging\nTable 7.7 contains PoS tagging results with frozen embeddings. Some\nnoteworthy ﬁndings include the highest accuracies per language group,\nmarked in bold. These are generally obtained by languages which\nare in the same language group, although there are exceptions to this\npattern. Note that we do not develop on the language which we use\nfor evaluation. That is to say, e.g., when evaluating on Danish, the\nGermanic column is calculated as the mean accuracy over German,\nNorwegian Bokmål and Nynorsk, and not including Danish.\nThere are examples in which training on a language from the\nsame language group worsens performance overall. Some notable\ncases include training on Dutch for the Germanic languages. The rel-\natively poor performance as compared to Danish might be explained\nby the fact that this group includes four Scandinavian languages, mean-\ning that Danish has three such languages which it is likely helpful for.\nDutch, on the other hand, has only two languages to which it is highly\nsimilar in the Germanic group, namely Afrikaans and German.\nIt is nonetheless somewhat puzzling that some non-Germanic lan-\nguages yield better performance in the Germanic group than Dutch.\nConsidering the baseline column in the table, however, it is clear that\nthe model only sees increases in performance in a few cases, with a\nloss in performance in nearly all cases. Therefore a potential expla-\nnation to the overall drop in results might be the fact that, although\nall languages within a single group are related to one another, this\nrelatedness might still be too distant to be exploited in the current\n8Results covering all languages are presented later in this chapter.\n158\n7. Comparing Multilinguality and Monolinguality\nsetup. For instance, the languages in the Slavic group represent a\nrelatively large variety.\nTable 7.7: PoS results – Training on 20k+20k multilingual tokens,\nfrozen embeddings.\nColumns indicate average results over lan-\nguages in that language group.\nLanguage\nGermanic\nRomance\nSlavic\nBaseline\n83.97%\n84.35%\n81.12%\nBulgarian\n83.89%\n82.93%\n76.53%\nCzech\n83.79%\n82.34%\n76.28%\nDanish\n84.14%\n84.20%\n79.73%\nFinnish\n83.87%\n83.58%\n78.41%\nFrench\n82.59%\n84.54%\n79.73%\nItalian\n83.83%\n83.77%\n79.67%\nDutch\n82.46%\n84.32%\n79.40%\nPolish\n81.93%\n84.44%\n78.58%\nPortuguese\n83.35%\n81.14%\n78.42%\nRussian\n83.85%\n82.77%\n82.26%\nDependency Relation Tagging\nTable 7.8 contains results from dependency relation tagging in the\nfrozen embeddings setting. Interestingly, although the top results for\nGermanic and Slavic are from in-group languages, we observe the\nbest results here for out-of-group languages for the Romance group.\nThe results of these experiments show a similar trend to those in the\nPoS experiments, with almost all results being worse than the base-\nline.\n7.3.4\nSummary of Results on the Universal Dependencies\nFor semantic tagging, we saw increases in performance when com-\nbining the Germanic languages with one another. The results from\n7.3. Tagging Tasks in the Universal Dependencies\n159\nTable 7.8:\nDepRel results – Training on 20k+20k multilingual to-\nkens, frozen embeddings. Columns indicate average results over lan-\nguages in that language group.\nLanguage\nGermanic\nRomance\nSlavic\nBaseline\n65.10%\n70.31%\n65.25%\nBulgarian\n59.01%\n69.52%\n60.31%\nCzech\n64.05%\n67.91%\n61.95%\nDanish\n65.56%\n68.20%\n61.74%\nFinnish\n64.87%\n63.34%\n61.78%\nFrench\n64.99%\n67.63%\n62.23%\nItalian\n63.94%\n67.97%\n61.91%\nDutch\n64.71%\n66.33%\n62.29%\nPolish\n64.15%\n64.68%\n60.98%\nPortuguese\n64.92%\n67.36%\n62.31%\nRussian\n64.17%\n68.32%\n65.30%\ntagging tasks on the UD languages reveal that this granularity of lan-\nguage similarity is not suﬃcient to determine whether this type of\nmodel multilinguality will be successful, under the experimental con-\nditions used here. In fact, observing results aggregated by the lan-\nguage families Germanic, Romance, and Slavic, revealed a decrease\nin performance when transferring from almost all languages in these\nfamilies, with some exceptions. These results thus shed some light on\ntwo potential issues. On the one hand, describing language similar-\nities in terms of typological families is perhaps not suﬃcient for the\npurposes of this chapter. On the other hand, the multilingual model\narchitecture used in the tagging experiments of this chapter might\nnot be suﬃcient to fully take advantage of language similarities.\n160\n7. Comparing Multilinguality and Monolinguality\n7.4\nMorphological Inﬂection\nHaving investigated two sequence labelling tasks, we now turn to a\nsequence-to-sequence prediction task, namely morphological inﬂec-\ntion. The 2017 shared task on morphological inﬂection offers a large\namount of data for 52 languages (Cotterell et al., 2017). Whereas the\nshared task has two sub-tasks, namely inﬂection and paradigm cell\nﬁlling, we only evaluate on the inﬂection task. Furthermore, we use\nthe high-resource setting, in which we have access to 10,000 training\nexamples per language. The inﬂection subtask is to generate a tar-\nget inﬂected form, given a lemma with its part-of-speech, as in the\nfollowing example:\nSource form and features: release V;NFIN\nTarget tag:\nV;V.PTCP;PRS\nTarget form:\nreleasing\n7.4.1\nMethod\nWe employ a deep neural network for the experiments in morpholog-\nical inﬂection. This consists of an attentional sequence-to-sequence\nmodel, as described in Östling and Bjerva (2017).9,10 The system takes\nembedded character representations as input to a Bi-LSTM encoder.\nThe output of the encoder is passed through an attention mechanism,\nto an LSTM decoder which also takes the target form’s morphological\ntags as features. All layers in the network has 128 hidden units. Op-\ntimisation is done using Adam (Kingma and Ba, 2014) with default\nparameters. Whereas Östling and Bjerva (2017) explore learning a\nsingle model per language, in this chapter we experiment with learn-\ning joint models across languages. Additionally, we do not use an\n9Available at https://github.com/bjerva/sigmorphon2017.\n10In the SIGMORPHON shared task, this team placed as the 4th best (Cotterell\net al., 2017).\n7.4. Morphological Inﬂection\n161\nensemble for the results presented in this chapter. The system archi-\ntecture is visualised in Figure 7.2\nh1\nh2\nh3\nh4\nhn\n+\nLSTM Encoder\nAttention\nLSTM Decoder\nh1\nh2\nh3\nh4\nhn\nsapandınız\nh1\nh1\ns\nh2\nh2\na\nh3\nh3\np\nh4\nh4\na\nhn\nhn\nn\nEmbed\nhn\nhn\nn\nN;LGSPEC1;2P;SG;PST\nOne-hot\nConcat.\nFigure 7.2: Architecture used for morphological inﬂection, consisting\nof an encoder-decoder with attention. The example depicts the pro-\nduction of the Turkish inﬂected form sapandınız, based on the input\nsapan and the tags N;LGSPEC1;2P;SG;PST.\nExperimental Setup\nWe train our system using joint input and output representations. In\norder to examine the effect of adding a language to the mix, we train\neach model as follows. Given each language in the set of languages\nl ∈L, we sample all language combinations l1, l2. We then train on\nthe entire high dataset of l1 (i.e., 10,000 examples), combined with\nn training examples from l2, with n = [0, 20, 21, . . . , 213].11 In other\nwords, l1 is our source language, and l2 our target language. This\nyields a total of |L| × |L| × |n| = 24, 000 experiments. Note that the\n11The SIGMORPHON-2017 shared task dataset contains three resource settings:\nlow (100 examples), medium (1000 examples), and high (10000 examples).\n162\n7. Comparing Multilinguality and Monolinguality\nmodel is language agnostic, and apart from orthographic similarities\nbetween languages, has no way of knowing whether a certain string\nbelongs to, e.g., Norwegian Nynorsk or Bokmål. We train each model\nfor a total of 36 hours on a single CPU core, and report results using\nthe model with the best validation loss.\n7.4.2\nResults and Analysis\nEvaluation is done using the standard metric for this task, namely\nthe Levenshtein distance between the predicted form and the target\nform (i.e. lower is better). Figure 7.3 shows results group-mean re-\nsults on l2 accuracy for training size n. The green lines show results\nwhen the l1 = Swedish, the red lines when the l1 = Spanish, and the\nblue when l1 = Slovak. Note that for performance is always better\nwhen training is combined with a language from the same language\ngroup. Notable is the performance with l1 = Spanish in the Nordic\nlanguage group, where performance in fact drops when adding more\nl2 samples at ﬁrst. This indicates that transfer from languages which\nare more similar is beneﬁcial, as compared to transfer from less re-\nlated languages. This should come as no surprise, as the morpholog-\nical similarities between, e.g., Norwegian and Danish are very pro-\nnounced, whereas similarities between Norwegian and Spanish are\nlimited, if any exist. As a transfer baseline, the bottom right shows\nan average across all language families, showing that none of the\nlanguages are inherently better as source languages, as conﬁdence\nintervals overlap for almost all amounts of l2 samples.\nThe results when using fewer than 256−512 l2 samples are, across\nthe board, below baseline levels. This can be explained by the fact\nthat the system setup used in these experiments was not suﬃcient\nfor cross-lingual transfer to be particularly successful. Changes in\nthe architecture, such as including language vectors, as described by\nÖstling and Tiedemann (2017) and Malaviya et al. (2017) is one possi-\n7.4. Morphological Inﬂection\n163\n(a) Mean Nordic l2 Levenshtein.\n(b) Mean Germanic l2 Levenshtein.\n(c) Mean Romance l2 Levenshtein.\n(d) Mean Slavic l2 Levenshtein.\n(e) Mean Uralic l2 Levenshtein.\n(f) Mean l2 Levenshtein for all l2s.\nFigure 7.3: Results on morphological inﬂection, with Slovak (blue,\nfull, triangles), Swedish (green, dashed, circles), and Spanish (red,\ndotted, squares) as l1.\n164\n7. Comparing Multilinguality and Monolinguality\nbility of improving this.12\nIn all cases, when suﬃcient l2 data has been observed, the differ-\nences in performance with different l1 is close to zero. This indicates\nthat the model is not relying on information from the l1 in such cases.\nThe results obtained by the multilingual system when observing 213\nl2 samples are similar to those obtained by the monolingual systems\nin Östling and Bjerva (2017). There are slight drops in performance\nacross the board, which can be explained by two factors. On the one\nhand, some net capacity is wasted (from the perspective of l2 perfor-\nmance), as we encode several languages in the same model. Addition-\nally, we only observe 213 l2 samples, whereas the systems in Östling\nand Bjerva (2017) use all 10,000 samples available for training.\n7.4.3\nSummary of Results on Morphological Inﬂection\nSimilarly to the semantic tagging results, we observe that typologi-\ncally related languages do tend to fare better in this transfer setting.\nPerhaps most convincing are the results when using Swedish as the\nsource language and evaluating on Nordic target languages.\nThis\nmight be caused by the fact that the languages included in the Nordic\n(Danish, and Norwegian Bokmål and Nynorsk) group are highly sim-\nilar to Swedish, whereas the other languages and language groups\nunder consideration are more distinct.\n7.5\nEstimating Language Similarities\nSo far, we have considered the research questions dealing with the\neffects of hard parameter sharing between languages. The results\nhave differed per task and per language combination, with the gen-\neral trend that languages which seem similar, tend to be beneﬁcial in\ncombination with one another. This brings us to the ﬁnal research\n12Language vectors are described further in Section 7.5.1.\n7.5. Estimating Language Similarities\n165\nquestion addressed in this chapter, namely, with what type of simi-\nlarity measures does multilingual effectivity correlate?\nAs grouping by typological language families yielded a relatively\nlarge spread in results, one possibility is that language similarities\nshould be quantiﬁed in a different manner. We investigate two dif-\nferent measures to estimate the similarity between languages. These\nmeasures have in common that the requirements to produce them\nare vanishingly small, meaning that they are not restricted to a few\nlanguages. In fact, the measures are readily available for a signif-\nicant portion of the languages in the world. Furthermore, the two\nmeasures are quite different from one another – one directly ob-\ntained in a data-driven manner, and one based on edit distances on\na lexical level.\n7.5.1\nData-driven Similarity\nThe data-driven similarity measure which we employ is based on\ntraining language embeddings together with a Long Short-Term Mem-\nory language model (Hochreiter and Schmidhuber, 1997). The vec-\ntors are learned by conditioning the LSTM’s prediction on an em-\nbedded language representation, when training the language model\non a large collection of languages. This leads to the model learning\nrepresentations which encapsulate some type of language similar-\nity, which means that the vectors can be used to calculate similari-\nties between languages, and is presented by Östling and Tiedemann\n(2017).13 Furthermore, the method is applicable to languages with\nvery limited data, such as all languages with, for instance, a trans-\nlation of the New Testament (i.e. ≈1000 languages). This approach\nto obtaining distributed can be compared to Malaviya et al. (2017), in\nwhich similar representations are learned in a neural machine trans-\n13Thanks to Robert Östling for providing us with access to this resource.\n166\n7. Comparing Multilinguality and Monolinguality\nlation system. We use the cosine distance between the vectors of two\nlanguages as a measure of their similarity.\n7.5.2\nLexical Similarity\nWe calculate lexical similarity as in Rama and Borin (2015), by us-\ning normalised Levenshtein distance (LDN) between aligned word\nlists. LDN is calculated by summing length normalised Levenshtein\ndistances for pairs of words using, e.g., Swadesh lists.14\nWhile ef-\nfects such as similarity between phoneme inventories could cause\nunrelated languages to seem related, LDN has the advantage that it\ncompensates for such effects (Rama and Borin, 2015).\nLexically aligned lists, similar to the Swadesh lists, are obtained\nfrom the Automated Similarity Judgement Program (ASJP) database\n(Wichmann et al., 2016).15 The ASJP aims to offer 40-word lists for\nall of the world’s languages, and currently offers such lists for 4664\nlanguages.16 These lists are linked on the meaning level, which al-\nlows for comparison of words across languages (see Table 7.9 for\nan example). The lists do not contain the orthographic representa-\ntions of these words, but rather a phonemic representation. Such\na representation is beneﬁcial for our purposes, as differences in or-\nthography resulting from historical artefacts might otherwise skew\nthe results. For instance, while the orthographic representations of\nthe 1st person singular pronoun in English and Norwegian have the\nmaximum possible Levenshtein distance for the word pair (I vs. jeg),\ntheir phonemic representations reveal the commonalities (Ei vs yEi).\nFigure 7.4 further illustrates the lexical distance measure. Lan-\nguages which are typologically similar to each other are automati-\n14Swadesh lists are standardised word lists, covering semantic concepts which\nare normally found in a given language, developed for the purposes of historical-\ncomparative linguistics.\n15http://asjp.clld.org/\n16As of 11-05-2017.\n7.5. Estimating Language Similarities\n167\nTable 7.9:\nExamples from ASJP for English, Dutch, Norwegian,\nFinnish, and Estonian.\nWord/Meaning\nEnglish\nDutch\nNorwegian\nFinnish\nEstonian\nI\nEi\nik\nyEi\nminE\nmina\nyou\nyu\nyEi\nd3\nsinE\nsina\nwe\nwi\nvEi\nvi\nme\nme\none\nw3n\nen\nEn\niksi\nuks\ntwo\ntu\ntve\ntu\nkaksi\nkaks\ncally grouped together, using the hierarchical clustering algorithm\nUPGMA (Unweighted Pair Grouping Method with Arithmetic-mean,\ncf. Saitou and Nei, 1987).\n7.5.3\nResults and Analysis\nWe will now consider the correlations observed between these lan-\nguage measures, and the results obtained from the multilingual ex-\nperiments outlined in this chapter. The results from the semantic\ntagging are not included in this analysis, as we have too few data\npoints available for the semantic tagging task to allow for reliably\nquantitative analysis. However, it is worth noting that the Germanic\nlanguages tend to help each other out, whereas Italian is generally\nless beneﬁcial to performance.\nTagging Tasks in the Universal Dependencies\nFigure 7.5a shows language correlations with language vector simi-\nlarities, across languages and conditions in the PoS tagging task (Spear-\nman ρ = −0.14 (p = 0.001)). Figure 7.5b contains the corresponding\nplot for the dependency relation task (Spearman ρ = −0.19 (p ≪\n0.001)). Although these correlations are statistically signiﬁcant, it is\n168\n7. Comparing Multilinguality and Monolinguality\nFigure 7.4: Distances calculated using LDN between ASJP lists, clus-\ntered with UPGMA.\n7.5. Estimating Language Similarities\n169\ndebatable whether or not they are practically signiﬁcant. Given this\namount of data points, statistically signiﬁcant results are relatively\nlikely, as the p-value indicates the risk of the correlation coeﬃcient\nbeing equal to zero, given the data.\nAlthough the correlations themselves are rather weak, it is inter-\nesting to observe that the patterns for both language similarities are\nrather similar. This is likely due to the fact that both of these mea-\nsures offer some explanatory value for the problem at hand, and\nmight also be a side-effect of the fact that these two measures cor-\nrelate rather well with one another (ρ = 0.7, p ≪0.001).\n(a) Language vector distances\ncompared to change in accuracy\non PoS tagging.\n(b) Language vector distances\ncompared to change in accuracy\non dependency relation tagging.\nFigure 7.5: Language vector distances: Correlations between accu-\nracy and language similarities.\nMorphological Inﬂection\nThe correlations between vector distances and performance in mor-\nphological inﬂection are weak, as seen in Figure 7.6a (Spearman ρ =\n0.075, p < 0.001).\nThe correlation coeﬃcient is somewhat higher\n170\n7. Comparing Multilinguality and Monolinguality\nwhen comparing with Levenshtein distance, as seen in Figure 7.6b\n(Spearman ρ = 0.16, p ≪0.001).\n(a) Language vector distances.\n(b) Levenshtein distances.\nFigure 7.6: Morphological inﬂection: correlations between Leven-\nshtein distance and language similarities.\n7.5.4\nWhen is Multilinguality Useful?\nAs we used relatively little data for training the tagging models, so\nas to allow for inclusion of a large number of languages, the abso-\nlute performance obtained is quite low. However, there appears to\nbe some relation between the usefulness of adding in one more lan-\nguage to a model, and how similar those languages are. Although\nthis seems is quite intuitive, the effects observed in our training set-\nting were more subtle than expected. For instance, in many cases\nlanguages which are not particularly related appear to also increase\nsystem performance. This might be explained by two factors. On\nthe one hand, it is possible that the quality of the word embeddings\nused is high enough so as to make the model fairly language agnos-\ntic. An alternative explanation, is that the network simply uses the\ninformation from a second language to further adjust its prior.\n7.6. Conclusions\n171\nIn the case of morphological inﬂection, we saw that the edit-distance\nbased measure of language similarity was more informative than the\nlanguage vectors. The fact that a measure based on edit distances is\nmore successful here, is not altogether surprising as the task deals\nwith minimising the Levenshtein distance between the predicted in-\nﬂected form and the target form.\nThe effects seen in this work were weaker than expected, indicat-\ning that additional factors to language similarity as deﬁned in this\nwork govern the usefulness of multilinguality. However, the weak\ncorrelations still hold, indicating that choosing languages which are\nsimilar either in terms of lexical distance, or in terms of language\nvectors, might be a good place to start. An interesting prospect for\nfuture work, is to incorporate, e.g., the language vectors as a feature.\nThis might make it easier for the model to learn between which lan-\nguages it is the most beneﬁcial to share certain parameters (e.g. be-\ntween Nordic languages), and between which languages such shar-\ning would likely lead to negative transfer.\n7.6\nConclusions\nWe investigated multilinguality in four NLP tasks, and observed cor-\nrelations between performance in multilingual models with two mea-\nsures of language similarity, in addition to a preliminary comparison\nbased on typological language families. On a general level, we found\nsome cases in which using a source language related to the target lan-\nguage was beneﬁcial, mainly in the case of semantic tagging (RQ 4a).\nWe then looked at two measures of language similarities (RQ 4b),\nwhich showed some correlations with multilingual model effectivity.\nThe correlations found were, however, rather weak, indicating that\nlanguage similarities as deﬁned in this work are not suﬃcient for\nexplaining such improvements to a large degree (RQ 4c).\nIn the next chapter, we will nonetheless continue on this path, at-\ntempting to both exploit language similarities, as well as similarities\nbetween tasks (RQ 5).\nPART IV\nCombining Multitask and\nMultilingual Learning\nCHAPTER 8\nOne Model to rule them all:\nMultitask Multilingual Learning\nAbstract|Multitask learning and multilingual learning share many simi-\nlarities, and partially build on the same assumptions. One such assump-\ntion is that similarities between tasks, or between languages, can be\nexploited with beneﬁcial effects. A natural extension of these two sepa-\nrate paradigms is to combine them, in order to take advantage of such\nsimilarities across both modalities simultaneously. In this chapter, such\na combined paradigm is explored, with the goal of building One Model to\nrule them all. The pilot experiments presented here take a ﬁrst step in this\ndirection, by looking at Part-of-Speech tagging and dependency relation\nlabelling for a large selection of source languages. We restrict ourselves\nto looking at three target languages representing some typological va-\nriety: Finnish, Italian, and Slovene. Furthermore, we run experiments\nwith a relatively simple model, using simple hard parameter sharing, and\nmultilingual input representations. In spite of this simplicity, promising\nresults are obtained for these three languages. For instance, a model\nwhich has not seen a single target language PoS tag performs almost\nequally to a model trained on target language PoS tagging only.\n176\n8. Multitask Multilingual Learning\n8.1\nCombining Multitask Learning and Multilinguality\nWhile Part I of this thesis focussed on multitask learning (MTL), and\nPart II focussed on multilingual learning (MLL), we now turn to a\ncombined paradigm. In other words, in this ﬁnal chapter of the the-\nsis, we both consider several tasks and languages at the same time.\nLet us ﬁrst consider why such an approach might be useful. For one,\njoint multilingual and multitask learning allows for taking advan-\ntage of the increasing amount of multilingual corpora with overlap-\nping annotation layers, such as the Universal Dependencies (Nivre\net al., 2017), and the Parallel Meaning Bank (Abzianidze et al., 2017).1\nHence, this approach might signiﬁcantly reduce data waste, as one\ntraditionally only considers a single task–language combination at a\ntime. An additional advantage of this paradigm is that it opens up\nfor simultaneous model transfer between languages and tasks. This\nessentially allows for applying zero-shot learning, in the sense of pre-\ndicting labels for an unseen task–language combination while taking\nadvantage of other task–language combinations.2 This approach has\nnot been the subject of much attention in the ﬁeld, perhaps due to\nits reliance on the combination of both MTL and MLL, which have\nonly recently become popular. Another issue is the fact that such\ncombined systems put rather large demands on both access to data\n(alleviated by the UD project), and access to suﬃcient computing re-\nsources. Although a full exploration of the possibilities of this paradigm\nis not carried out in this chapter, we do take a ﬁrst step in this direc-\ntion. The main aim of this chapter is to provide an answer to the\nfollowing research question, in order to answer RQ 5.\nRQ 5a To what extent can a combined MTL/MLL system generate\nsensible predictions for an unseen task–language combination?\n1In particular, we are interested in the fact that several languages have anno-\ntations within the same theoretical framework, following the same annotation\nguidelines, rather than a single language having several layers of annotation.\n2I.e., zero-shot learning in a similar sense to Johnson et al. (2016).\n8.1. Combining Multitask Learning and Multilinguality\n177\nAnswering this question is considered as a step in the direction of\nOne Model to rule them all.\nIf successful, this will allow for boot-\nstrapping off of more-or-less related languages and tasks, which in\nturn will be highly useful for both low-resource languages and low-\nresource tasks.\nRelated work\nFor related work on the separate paradigms of multilingual and mul-\ntitask learning, the reader is referred to Chapter 3.\nIn this chap-\nter, we are concerned with a combined multilingual and multitask\nlearning paradigm. Although little work has been done in multilin-\ngual multitask NLP, Yang et al. (2016) make some preliminary exper-\niments in this direction by contrasting the two approaches, experi-\nmenting with NER, PoS tagging, and chunking on English, Dutch, and\nSpanish. Their approach uses hard parameter sharing for certain\nlayers, either between languages, or between tasks. In the case of\nmonotask MLL, they share character embeddings and weights of a\ncharacter-based RNN, whereas in their monolingual MTL setup, they\nattach a task-speciﬁc conditional random ﬁeld for each task. Since\ntheir MLL setup depends on sharing character-based features, the ap-\nproach is restricted to relatively related languages, and is not likely\nto work well for less related ones. Indeed, Yang et al. (2016) apply\ntheir method only to the relatively closely-related languages English,\nDutch and Spanish. Recent work by Fang and Cohn (2017) exploits\nbilingual dictionaries in order to obtain cross-lingual embeddings,\nwhich are used to train a PoS tagger for a source language, which\nis then applied to a target language with embeddings in the same\nspace.\nThis chapter expands on previous work by unifying MTL and MLL\nin a single system, using hard parameter sharing. This allows for tak-\ning advantage of similarities between tasks and between languages\nsimultaneously. Rather than sharing character-level features, we fo-\n178\n8. Multitask Multilingual Learning\ncus on using multilingual word-level input representations. One ad-\nvantage of avoiding character-level features in a setup using hard pa-\nrameter sharing, is that reliance on morphological similarities is re-\nduced, which might otherwise lead to negative transfer when consid-\nering distantly related languages. The work presented here therefore\ndiffers from Yang et al. (2016) in two main ways: i) our method is not\nrestricted to morphologically similar languages, and is applicable to\na large portion of (combinations of) the languages in the world; ii) we\naim to combine a vast amount of data sources to generate reasonable\npredictions for a given unobserved task–language pair. Additionally,\nour motivations are quite different. Where Yang et al. (2016) aim to\nimprove performance on a task–language pair for which annotated\ndata exists, by adding a distant supervision signal from a different\nlanguage for the same task, or a different task for the same language,\nwe aim to induce tags for task–language combinations for which no\nannotated data exists. This is important, since a multitask multilin-\ngual setting will usually resemble the scenario depicted in Table 8.1.3\nLet us consider language l6 and task t3, as highlighted in red in the\ntable. In order to ﬁll this gap, we can choose from a few approaches:\ni) Spend an enormous effort in ﬁnding, hiring and training annota-\ntors; ii) Apply annotation projection to the text snippets which hap-\npen to be parallel text with languages for which annotation exists\nfor t3, or ﬁrst translate the data from l6 to such a language (cf. the\ntranslation approach described in Chapter 3);4 iii) Train a multilin-\ngual system with supervision from only languages with annotation\nfor t3 (cross-lingual model transfer); or iv) Train a system on several\ntask–language pairs in the matrix, including l7 for other tasks.5 Our\n3We will refer to such tables as gap tables, as they contain some ﬁlled (black)\ncells, and several white gaps without data.\n4Note that the requirements for annotated/parallel data are quite high in this\ncase (cf. Tiedemann et al. (2014)).\n5The ﬁrst three approaches can be considered traditional approaches, and\nare detailed in Chapter 3.\n8.2. Data\n179\napproach is essentially this ﬁnal approach (iv).\nTable 8.1: Black cells indicate the availability of annotated data for\na given task–language pair. Some languages have (almost) all cells\nﬁlled, whereas some have a large amount of gaps. The red cell in-\ndicates a potential target task–language combination, for which no\nannotated data exists.\nl1\nl2\nl3\nl4\nl5\nl6\nl7\nl8\nl9\nl10\nl11\nl12\n· · ·\nln\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nt9\nt10\nt11\nt12\n· · ·\ntn\n8.2\nData\n8.2.1\nLabelled data\nWhile the goal is to extend this approach to a matrix such as in Ta-\nble 8.1, we will only look at two tasks in this chapter. Additionally,\nsince the goal in this pilot experiment is to see whether the proposed\nMTL/MLL paradigm is at all feasible, a setting in which very high cor-\nrelations between main and auxiliary tasks can be found was chosen.\nWe therefore focus on PoS tagging and dependency relation (DepRel)\nlabelling, as data for both of these tasks is available for a relatively\nlarge amount of languages through the Universal Dependencies (UD)\nproject. There are many possible ways of deﬁning dependency rela-\n180\n8. Multitask Multilingual Learning\ntion labels, and in this chapter we use the simple/simple paradigm de-\nscribed in Chapter 5 (Table 5.1). Furthermore, positive results have\nbeen obtained for this particular task combination, e.g., in Chapter 5.\nIn the experiments of this chapter, UD v1.2 is used (Nivre et al., 2015).\n8.2.2\nUnlabelled data\nWe take a similar approach to enabling model multilinguality as in\nPart III. That is to say, we use uniﬁed input representations, in the\nsense that we use multilingual word embeddings. Although many\noptions exist, we use multilingual skip-gram (Guo et al., 2016), for\nthe same reasons as in Part III.\nWe train the embeddings in two resource settings. The ﬁrst is a\nhigh resource setting, in which we have access to a large amount of\nparallel text. In this setting, we train embeddings on the Europarl\ncorpus (Koehn, 2005). The second is a low resource setting, in which\nwe only require very limited amounts of parallel data, and train em-\nbeddings on a collection of Bible corpora. The low resource setting is,\nindeed, truly low-resource, as we only require approximately 140,000\ntokens of training data.6 Additionally, although the use of Bible cor-\npora for multilingual representations can be criticised for many rea-\nsons, including the speciﬁcity of the domain, and the archaicness of\nthe language, this data has the advantage that it is available for more\nthan 1,000 languages. While this does leave a long tail of approxi-\nmately 5,000 languages for which such resources are not available,\nit nonetheless constitutes a leap forward from requiring Europarl-\nlevels of data.\n6This is the approximate token count for the English New Testament, and is\nbound to differ for other languages.\n8.3. Method\n181\n8.3\nMethod\n8.3.1\nArchitecture\nThe system used is the same bi-GRU as described in Chapter 7. To\nrecap, the bi-GRU is two layers deep, and uses only word-level mul-\ntilingual embeddings as input. The network has two output layers –\none for PoS tags, and one for dependency relation tags. In the set-\ntings in which no dependency relations are observed, the weights of\nthe corresponding layer are left unaltered. This includes the baseline\nsetting, and the settings with transfer solely from PoS tags.\nAlthough using character-level representations would likely yield\nhigher performance for some cases, there are two main reasons why\nthis is not done. Primarily, it is likely that using such representations\nwould lead to negative transfer between less related languages. This\nwould need to be dealt with in a more sophisticated way than sim-\nple hard parameter sharing, for instance by using sluice networks,\nin which the parameter sharing itself is learnt (Ruder et al., 2017).\nAdditionally, the goal of the experiments in this chapter is not to ob-\ntain the highest possible results, which is the trend in much of cur-\nrent NLP, but rather to investigate the differences between different\ntransfer settings.\n8.3.2\nHyperparameters\nHyperparameters were tuned to a small extent on the English devel-\nopment set when training on only English PoS tags, with the goal\nof asserting that the system performs on-par with the word-based Bi-\nGRU in Chapter 4. The aim was to perform a relatively low amount of\ntuning, keeping parameters at fairly standard values. These hyperpa-\nrameters were used for all experiments in this chapter. We use recti-\nﬁed linear units (ReLUs) for all activation functions (Nair and Hinton,\n2010). We apply dropout (p = 0.2) at the input level, and recurrent\n182\n8. Multitask Multilingual Learning\ndropout (Semeniuta et al., 2016) between the layers in the network.\nWe use the Adam optimisation algorithm (Kingma and Ba, 2014) with\na batch-size of 10 sentences (randomly sampled from all source lan-\nguages under consideration). Training is done over a maximum of\n50 epochs, using early stopping monitoring the loss on development\nsets of all source languages in the given experimental condition. The\nweighting parameter λ, deﬁning the weight of the auxiliary task is\nset to λ = 1.0, i.e., weighting the main and auxiliary tasks equally.\n8.4\nExperiments and Analysis\nFor the purposes of evaluating whether the proposed approach is fea-\nsible, we consider several potential scenarios. In all experiments,\nwe look at ﬁlling the gap of Finnish, Italian, and Slovene PoS tags.\nThese languages were chosen so as to represent some level of typo-\nlogical diversity, with one language from outside the Indo-European\nfamily (Finnish), and two fairly dissimilar Indo-European languages,\nof which one is a Romance language (Italian), and one is a Slavic\nlanguage (Slovene). The evaluation metric used in the experiments\nis the accuracy of PoS tagging on each of these languages, as evalu-\nated on their UD development sets.7 We will successively increase\nthe amount of, and variety of, data which the models are trained\non. We will also investigate the effect of adding more or less related\nlanguages to the training data. Language relatedness is displayed in\nTable 8.2, and is deﬁned heuristically, based on typological related-\nness.\nEvery system is trained on the concatenation of the entire train-\ning set of all source languages involved in the setup at hand. Vali-\ndation is done on the concatenation of the development sets of all\nsource languages in the setup at hand.\n7No tuning is performed on this set.\n8.4. Experiments and Analysis\n183\nTable 8.2:\nSource language overview table.\nColumns indicate\nwhether the languages are considered to be related to the header of\nthat column.\nLanguage\nFinnish\nItalian\nSlovene\nBasque\nBulgarian\nx\nCroatian\nx\nCzech\nx\nDanish\nDutch\nEnglish\nEstonian\nx\nFrench\nx\nGerman\nHebrew\nHindi\nHungarian\nx\nIndonesian\nIrish\nKazakh\nLatin\nx\nGreek\nNorwegian\nPersian\nPortuguese\nx\nRomanian\nx\nSpanish\nx\nSwedish\nTamil\nTurkish\n184\n8. Multitask Multilingual Learning\nTraining on English PoS\nThroughout the experimental overview, we will consider a gapped\ntable as shown in Table 8.3. The black cells denote the source task–\nlanguage combinations, and the red cells denote the target task–language\ncombinations. Note that, although we could train a joint system for\nall target languages, separate systems are trained for each target lan-\nguage. This is because that, in following experiments, we look at\nlanguages which are related to the target languages to a smaller or\nlarger extent. As our target languages are typologically quite differ-\nent, this requires us to train separate systems, as it would otherwise\nbe impossible to add a language which is equally related to, e.g., both\nFinnish and Italian. In the ﬁrst experiment, we train on English PoS\ntags only, and evaluate on Finnish, Italian, and Slovene (Table 8.3).\nWe also train a monolingual baseline system for each of the target\nlanguages which is used throughout this chapter, with the same gen-\neral setup as the other systems, using the high-resource multilingual\nembeddings.\nTable 8.3: Gap table – Training on English, PoS only. Evaluation is on\nthe target languages Finnish, Italian, and Slovene.\nLanguage\nPoS\nDepRel\nEnglish\nTarget languages\nThe results from this setting can be observed in Figure 8.1. The\nred bars indicate the systems trained on English PoS, with a black\nborder around the system using high resource embeddings, and no\nborder for the system using low resource embeddings. The black\nbars indicate the monolingual baseline systems.\nNot surprisingly,\ntransfer from English is not particularly successful, with performance\nfar below baseline. This shows that training on a single relatively un-\nrelated language is not suﬃcient in this setting. As expected, results\n8.4. Experiments and Analysis\n185\nFigure 8.1: Red bars indicate training on English PoS. No border in-\ndicates training with low resource embeddings, and a black border\nindicates training with high resource embeddings. The black bars\ndenote the monolingual baselines.\nwhen using embeddings trained on Europarl are somewhat higher\nthan when using low resource embeddings.\nTraining on PoS from several languages\nNext, we add PoS training data from several languages, all relatively\nunrelated to the target languages. In this setting, the system for each\ntarget language is trained on all languages labelled as unrelated to\nthe source language in Table 8.2, as depicted in Table 8.4.\nTable 8.4: Gap table – Training on unrelated languages, PoS only.\nLanguage\nPoS\nDepRel\nUnrelated languages\nTarget languages\nThe results can be seen in Figure 8.2. Adding more languages to\n186\n8. Multitask Multilingual Learning\nthe training material does not affect results noticeably for Finnish.\nFor Italian and Slovene, however, the results improve somewhat,\nmost notably when using high resource embeddings.\nThis might\nbe due to the fact that the UD dataset contains an Indo-European\nbias, meaning that the so-called unrelated languages which we have\nadded still share fairly distant ancestry. We also see a rather large\nincrease in the performance on Italian as compared to Slovene. This\ncan be explained by the fact that many of the unrelated languages\nadded in this setting are Germanic.\nMorphological complexity of\nGermanic languages is arguably relatively similar to Romance lan-\nguages, such as Italian. On the other hand, Slavic languages such as\nSlovene are much more morphologically complex. This might have\nan effect on the quality of the multilingual word embeddings, leading\nto training on Germanic languages being more beneﬁcial for Italian\nthan it is for Slovene. Finnish, being from the Finno-Ugric language\nbranch, does not beneﬁt from this setting, perhaps due to its typolog-\nical distance from the added languages being larger.\nFigure 8.2: Red bars indicate training on English PoS. Orange bars in-\ndicate adding unrelated language PoS. No border indicates training\nwith low resource embeddings, and a black border indicates training\nwith high resource embeddings. The black bars denote the monolin-\ngual baselines.\n8.4. Experiments and Analysis\n187\nAdding source language dependency relations\nWe now add dependency relation data for the same collection of lan-\nguages as in the previous setting (see Table 8.5). This is the ﬁrst set-\nting in which MTL is combined with the MLL experiments, as the\nnetwork is now trained on both PoS tagging and dependency rela-\ntion labelling. The idea is that the correlations between PoS tags and\nDepRel labels can be learnt by the network in an implicit manner,\nwhich might be beneﬁcial for system performance. However, we do\nnot expect positive results in this particular setting, considering that\nthe mutual information between PoS tags and dependency relations\nis relatively high, and we are not adding any extra data (see Chap-\nter 5).\nTable 8.5: Gap table – Training on unrelated languages, PoS and de-\npendency relations.\nLanguage\nPoS\nDepRel\nUnrelated languages\nTarget languages\nFigure 8.3 shows that, indeed, this addition does not affect results\nto a large extent. In fact, results drop somewhat in most settings,\nwhich may be owed to the fact that some of the net capacity is wasted,\nsince two tasks need to be learned. As expected, since the system has\nnot seen any data for either task for the target languages, adding this\ndata does not improve much, which can be explained by the ﬁndings\nin Chapter 5.\nAdding target language dependency relations\nIn this experiment, we add dependency relation data for the target\nlanguages in training (Table 8.6). The intuition behind this, is that\nthe neural network ought to be able to make use of the implicitly\n188\n8. Multitask Multilingual Learning\nFigure 8.3: Green bars indicate adding source language dependency\nrelations.\nNo border indicates training with low resource embed-\ndings, and a black border indicates training with high resource em-\nbeddings.\nlearn correlations between tasks, thus learning to produce sensible\nPoS tags for the target languages, in spite of never having actually ob-\nserved such tags. In a sense, this is the ﬁrst real combined MTL/MLL\nexperiment in this chapter.\nTable 8.6: Gap table – Training on English and unrelated languages,\nPoS and dependency relations.\nLanguage\nPoS\nDepRel\nUnrelated languages\nTarget languages\nResults in Figure 8.4 show high resource embeddings almost reach-\ning ceiling performance. This can be interpreted as showing that the\nnetwork has learned the correlations between the two tasks, allow-\ning for generating sensible PoS tags for the target languages. Another\npotential explanation is that adding extra data with high mutual in-\n8.4. Experiments and Analysis\n189\nFigure 8.4: Blue bars indicate adding target language dependency\nrelations.\nNo border indicates training with low resource embed-\ndings, and a black border indicates training with high resource em-\nbeddings.\nformation with PoS tagging ought to be useful, based on the ﬁndings\nin Chapter 5.\nAdding related languages, no target dependency relations\nWe here add data for related languages, as deﬁned in Table 8.2. This\nis the same as the third experimental setting (Table 8.5), except we\nalso look at related languages (depicted in Table 8.7). That is to say,\nwe do not see any dependency relation tags for the target languages\nin this setting.\nTable 8.7: Gap table – Training on unrelated and related languages,\nPoS and dependency relations.\nLanguage\nPoS\nDepRel\nUnrelated languages\nRelated languages\nTarget languages\n190\n8. Multitask Multilingual Learning\nFigure 8.5: Green bars indicate adding source language dependency\nrelations. Yellow bars indicate training on related languages. No\nborder indicates training with low resource embeddings, and a black\nborder indicates training with high resource embeddings.\nComparing the yellow and green bars in Figure 8.5, the change in\nresults is not as large as what might have been anticipated. This is\nsomewhat surprising, as one could expect adding related languages\nto the mix to improve results signiﬁcantly. Nonetheless, especially\nin the high resource setting, some gains can be observed. These re-\nsults support the ﬁndings of Chapter 7, in that training on similar\nlanguages can be beneﬁcial in a multilingual scenario. The relative\ngain for Finnish is especially high, which can be explained by the fact\nthat the model ﬁnally has access to source data which to some ex-\ntent resembles the target data. As for Slovene and Italian, the gains\nin getting access to Slavic and Romance data, respectively, does not\nprovide a very large beneﬁt as compared to having access to Indo-\nEuropean data. As for the low resource settings in this experiment,\nvery small differences can be observed. This hints at the possibility\nthat, without access to any target language data, the Bible-based em-\nbeddings are close to a performance ceiling.\n8.4. Experiments and Analysis\n191\nAdding related languages, with target dependency relations\nFinally, we also add in the dependency relation data for the target\nlanguages (Table 8.8). This denotes the most complete experimental\nsetting, as we only have a single gap to ﬁll in the table, and have\naccess to the largest possible amount of data.\nTable 8.8: Gap table – Training on unrelated and related languages,\nPoS and dependency relations, as well as target language depen-\ndency relations.\nLanguage\nPoS\nDepRel\nUnrelated languages\nRelated languages\nTarget languages\nThe results for this experiment are positive for all three languages,\nshowing that it is possible to output PoS tags of decent quality, with-\nout having seen a single target-language PoS tag (Figure 8.6). Per-\nformance on Italian is especially promising, reaching the same level\nas the ceiling baseline, while Finnish and Slovene also show positive\nresults.8\nNotably, although we use training data from related lan-\nguages, the change in performance between this setting (purple bars)\nand the corresponding setting with unrelated languages (blue bars)\nis quite small. Also noteworthy is the small distance between the\ntwo embedding types in this setting. While ceiling performance is ob-\nserved when using high resource embeddings, the low resource sce-\nnario also yields positive results. Whereas most of the experiments\ndid not provide much difference in the results with low resource em-\nbeddings, the two settings in which we have access to target-language\ndata show that it may be suﬃcient with this resource scenario. Should\n8An important caveat, however, is the fact that the setup is rather artiﬁcial,\nas one rarely will have dependency relation annotation for a language, without\naccess to PoS tags. This is discussed further in Section 8.5.\n192\n8. Multitask Multilingual Learning\nFigure 8.6: Red bars indicate training on English PoS. Orange bars indicate adding unrelated lan-\nguage PoS. Green bars indicate adding source language dependency relations. Blue bars indicate\nadding target language dependency relations. Yellow bars indicate training on related languages.\nPurple bars indicate adding target language dependency relations. No border indicates training\nwith low resource embeddings, and a black border indicates training with high resource embed-\ndings. The black bars denote the monolingual baselines.\n8.5. Discussion\n193\nthese results generalise to more exotic languages than the ones used\nin this study, then this type of multitask multilingual learning might\nindeed be a useful step towards improving NLP for low-resource lan-\nguages.\nFigure 8.7: Accuracy of monolingual models on UD Dev compared to\nthe vocabulary coverage of the high and low resource embeddings.\n8.5\nDiscussion\nWhile the general results in the high-resource scenario were posi-\ntive, and indicate that the approach taken here is, at the very least,\nmethodologically sound, the results were more varied in the low-\nresource scenario. In general, the Bible-based embeddings did not\nyield any positive results, save for the experiments in which we also\nhad access to some target-language training data. This might be ex-\nplained further by taking a look at model performance as compared\nto vocabulary coverage. Figure 8.7 shows the accuracy of monolin-\ngual models on UD Dev compared to the vocabulary coverage of the\nhigh resource and low resource embeddings on the same UD Dev\n194\n8. Multitask Multilingual Learning\nset. These models are trained in the same way as the ceiling base-\nline from the experimental setup in this chapter.\nThere is a rela-\ntively strong correlation between accuracy and vocabulary coverage,\nwith most of the low-coverage region naturally occupied by the low-\nresource embeddings. The fact that the link with vocabulary cover-\nage is as pronounced as what we observe explains the large increases\nwe observe when adding target language training data. In doing so,\nwe effectively increase the portion of the target language vocabulary\nwhich the model has observed, thus increasing performance on tar-\nget language PoS tags.\nWhile the results of the experiments in this chapter seem promis-\ning, there are some points which can be criticised. One such matter,\nis the fact that it is hard to imagine a situation which is exactly as\nwhat was described here. The assumption of the experiments was\nthat we did have access to dependency relation annotations for the\ntarget languages, but did not have PoS tagged data. As dependency\nrelations constitute a more detailed level of description, this scenario\nis most likely not a very common one. An interesting direction would\ntherefore be to invert this setting, by trying to ﬁll a dependency re-\nlation gap. This is likely much more challenging than the current\nsetup, as the mapping from PoS tags to dependency relations is more\nheterogenous than the inverse. However, even though ﬁlling a gap\nfor more intricate annotations than one has for a language is an in-\nteresting problem, ﬁlling a gap with annotations at a similar level as\nwhat already exists is also a useful application. For instance, some\nlanguages have their own PoS tagged corpora, while they do not have\nany UD annotations. This might be solved by mapping from one PoS\ntag set to another with the approach described in this chapter.\nIn spite of the aforementioned issues, the aim of the pilot study is\nto investigate whether or not the proposed combination of MTL and\nMLL is at all feasible. The fact that we have seen positive results in\nsuch a simplistic setting, using hard parameter sharing and multilin-\n8.6. Conclusion\n195\ngual input representations, certainly indicates that this is the case. A\npotential approach for dealing with languages for which parallel text\ndoes not exist in suﬃcient quantities, is to rely on bilingual dictionar-\nies instead, as done by Fang and Cohn (2017).\nReﬁning this approach in the future therefore constitutes a highly\ninteresting research direction, for which some approaches are de-\ntailed in the next and ﬁnal chapter of this thesis, in Section 9.4.\n8.6\nConclusion\nWe attempted to combine the paradigms of multilingual and multi-\ntask learning. Providing the model with data for the target task for\nsource languages, as well as auxiliary task data for the target and\nsource languages, yielded promising results. In fact, the results are\nalmost on par with training a system directly on the target/source\nlanguage, indicating that combining the paradigms of MTL and MLL\nhas potential (RQ 5a). Although the experimental setup was some-\nwhat artiﬁcial, as we assumed access to a more complex level of an-\nnotation than what we aimed at producing, this approach constitutes\na research direction which is worthwhile pursuing in the future.\nPART V\nConclusions\nCHAPTER 9\nConclusions\nWhile traditional NLP approaches consider a single task or language\nat a time, the aim of this thesis was to answer several research ques-\ntions dealing with pushing past this boundary. In doing so, the hope\nis that in the long term, low-resource languages can beneﬁt from\nthe advances made in NLP which are currently to a large extent re-\nserved for high-resource languages. This, in turn, may then have\npositive consequences for, e.g., language preservation, as speakers\nof minority languages will have a lower degree of pressure to using\nhigh-resource languages. In the short term, answering the speciﬁc\nresearch questions posed should be of use to NLP researchers work-\ning towards the same goal. We will now see the conclusions which\ncan be drawn from each research part of this thesis.\n9.1\nPart II - Multitask Learning\nIn the ﬁrst research part of the thesis, we began by exploring the\nfollowing research question in Chapter 4.\n’To what extent can a semantic tagging task be informative\nfor other NLP tasks?’\n–RQ 1\n200\n9. Conclusions\nWe found that semantic tags are informative for the task of PoS\ntagging. Furthermore, the results obtained when exploiting this were\nstate-of-the-art results at the time. Additionally, we found that using\ncoarse-grained semantic tags was not informative for semantic tag-\nging. This then raised more questions. Why were the semantic tags\nuseful for PoS tags, while coarse-grained semantic tags were not use-\nful for semantic tagging? A look at correlations between the tag sets\nshowed that these were high in both cases. Coarse-grained semantic\ntags have a one-to-one mapping with semantic tags. Semantic tags do\nnot have a one-to-one mapping with PoS tags, but still exhibit large\ncorrelations. The idea was, then, that such high correlations between\ntag sets might correlate with auxiliary task effectivity, given differing\ndata sets. Thus, we aimed at answering the next research question\nin Chapter 5:\n’How can multitask learning effectivity in NLP be quanti-\nﬁed?’\n–RQ 2\nTaking an information-theoretic perspective, we found that these\ncorrelations could be quantiﬁed fairly well by using mutual infor-\nmation. Running experiments in various data overlap settings, on\na large selection of languages and tasks, showed that the hypothe-\nsis was supported. That is to say, providing the model with different\ndata including annotations which correlate highly with the main task\nyields gains in performance. However, providing the model with the\nsame data with such highly correlated auxiliary annotations, does\nnot yield any increase at all.\nIntuitively, this makes sense if one\nthinks about it as follows. The model has already seen sentence x\nwith some annotation. Giving it the same sentence x with highly cor-\nrelated annotation does not give the model anything more to learn\nfrom – after all, this example has practically already been observed!\n9.2. Part III - Multilingual Learning\n201\nHowever, giving the model a different sentence y with highly corre-\nlated annotation essentially entails giving the model an extra train-\ning example.\n9.2\nPart III - Multilingual Learning\nIn the second content part of the thesis, the aim was to investigate\nsimilar research questions to Part I, focussing on similarities between\nlanguages rather than between tasks. We began by asking the follow-\ning research question.\n’To what extent can multilingual word representations be\nused to enable zero-shot learning in semantic textual simi-\nlarity?’\n–RQ 3\nIn Chapter 6 we found that a simple language-agnostic feed-forward\nneural network using multilingual word representations was able to\nsolve the task of semantic textual similarity assessment to some ex-\ntent. Although results were below the current state-of-the-art for this\ntask, some useful insights were gained. Mainly, we found that lan-\nguages which are more similar to one another are more suited for\nthis approach, indicating that language similarity is important for\nthe effectivity of model multilinguality. This is similar to the case in\nMTL, where task relatedness is an important factor, and raised the\nfollowing research question which we approached in Chapter 7.\n’In which way can language similarities be quantiﬁed, and\nwhat correlations can we ﬁnd between multilingual model\nperformance and language similarities?’\n–RQ 4\nWe looked at correlations between language similarity and multi-\nlingual model effectivity in two sequence prediction tasks, namely se-\n202\n9. Conclusions\nmantic tagging and PoS tagging, as well as in a sequence-to-sequence\ntask, namely morphological inﬂection. The overall results indicate\nthat both measures of language similarity under consideration offer\nsome explanatory value. One interesting ﬁnding in the case of se-\nmantic tagging, was the fact that English, Dutch, and German bene-\nﬁtted from having their input representations updated during joint\ntraining. Combining these languages with Italian and updated em-\nbeddings, however, resulted in a serious drop in performance. A\npotential reason for this is that language relatedness plays a large\nrole in maintaining the quality of the multilingual embedding space\nin such a context. In future work, it would therefore be interesting\nto observe the resulting word-space after updating word representa-\ntions in such a setting.\n9.3\nPart IV - Combining Multitask Learning and Multilinguality\nIn the ﬁnal research part of the thesis, the aim was to probe the pos-\nsibilities of combining the paradigms of multitask learning and mul-\ntilingual learning. Chapter 7 aimed at providing an answer to the\nfollowing research question.\n’Can a multitask and multilingual approach be combined to\ngeneralise across languages and tasks simultaneously?’\n–RQ 5\nWe looked at predicting labels for an unseen task–language com-\nbination, by taking advantage of other task–language combinations.\nIn the admittedly somewhat artiﬁcial setup, the target task was PoS\ntagging for three languages offering some typological diversity, namely\nFinnish, Italian, and Slovene. In a high-resource scenario, assuming\naccess to parallel text similar to Europarl, sensible tags could be pro-\nduced for the target languages without seeing any annotated data for\nthat target language. In the low-resource scenario, assuming access\n9.4. Final words\n203\nto parallel text similar to the New Testament, similar results have the\nadditional requirement of also having access to target-language an-\nnotations of some sort. Finally, access to the high-resource scenario\nas well as target-language annotations yielded results on par with a\nmonolingual monotask PoS tagger for the target language – and that\nwithout seeing a single PoS tag for the target language.\n9.4\nFinal words\nA large part of this thesis was motivated by the intuition that similar-\nities between tasks and languages is one of the most important fac-\ntors when considering a multitask or a multilingual approach. Even\nthough some correlations were found in experiments, attempting to\ncorrelate measures of task and language similarities with change in\nmodel performance, much of the change that is observed is left unac-\ncounted for. This highlights the case that even if such similarities are\nimportant, the situation is more complex than what can be explained\npurely by measures of correlation.\nThe successful experiments dealing with the combination of mul-\ntitask and multilingual learning show the most potential for future\nresearch based on this thesis. A plethora of new studies based on\nthis idea can be imagined. One could take advantage of morpholog-\nical similarities by looking at character-level representations, inves-\ntigating to what extent an architecture such as sluice networks can\nlearn to share parameters for similar task–language combinations.\nAnother option is to probe into how much annotation is needed in\norder to bootstrap off of other languages than the target language at\nhand in order to predict reasonable labels for the target language.\nA concrete proposal toward One Model to rule them all at a larger\nscale, involving more languages and tasks, is to model this in a sluice\nnetwork (Ruder et al., 2017). In this recently proposed architecture,\nthe sharing of layers itself is learned by the network. Combining a\n204\n9. Conclusions\nlarge amount of tasks in such a network should therefore allow for\ntaking advantage of relevant similarities between tasks, while not\nsharing parameters in the case of dissimilarities which may lead to\nnegative transfer. Taking this one step further, by also involving mul-\ntilingual learning as in this thesis, could also allow for learning be-\ntween which languages to share parameters. For instance, this ar-\nchitecture might be able to learn which parts of a character-RNN to\nshare between which languages, for instance learning to only share\nthese parameters between closely related languages, thus avoiding\nany negative transfer in this setting. Further combining this approach\nwith language vectors (Östling and Tiedemann, 2017; Malaviya et al.,\n2017) might facilitate exploitation of language similarities. This ap-\nproach might therefore alleviate many of the problems with hard\nparameter sharing, by allowing the model to only utilise parameter\nsharing for similarities between languages, while learning separate\nparameters for language-speciﬁc features. As the amount of both\nunannotated parallel data, and annotated data with various univer-\nsal annotation schemes increases, it is only a matter of choosing the\nright approach in order to arrive at One Model to rule them all.\nAppendices\nAPPENDIX A\nCorrelation ﬁgures for all\nlanguages in Chapter 5\n208\nA. Correlation ﬁgures for all languages in Chapter 5\nFigure A.1: Correlations between ∆acc and entropy. Each data point\nrepresents a single experiment run.\n209\nFigure A.2: Correlations between ∆acc and mutual information. Each\ndata point represents a single experiment run.\nAPPENDIX B\nBibliographical abbreviations\n• AAAI →Conference on Artiﬁcial Intelligence\n• ACL →Annual Meeting of the Association for Computational\nLinguistics\n• COLING →International Conference on Computational Linguis-\ntics\n• CoNLL →Conference on Computational Natural Language Learn-\ning\n• EACL →Conference of the European Chapter of the Association\nfor Computational Linguistics\n• EMNLP →Conference on Empirical Methods in Natural Lan-\nguage Processing\n• HLT →Conference on Human Language Technology\n• ICLR →International Conference on Learning Representations\n• ICML →International Conference on Machine learning\n• IJCNLP →International Joint Conference on Natural Language\nProcessing\n• LREC →Language Resources and Evaluation Conference\n• NAACL →Conference of the North American Chapter of the As-\nsociation for Computational Linguistics\n• NIPS →Neural Information Processing Systems Conference\n• NoDaLiDa →Nordic Conference on Computational Linguistics\n• *SEM →Joint Conference on Lexical and Computational Seman-\ntics\n• SemEval →International Workshop on Semantic Evaluation\nBibliography\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S.,\nDavis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I. J., Harp, A., Irv-\ning, G., Isard, M., Jia, Y., Józefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J.,\nMane, D., Monga, R., Moore, S., Murray, D. G., Olah, C., Schuster, M., Shlens, J.,\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P. A., Vanhoucke, V., Vasudevan,\nV., Viégas, F. B., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and\nZheng, X. (2016). Tensorﬂow: Large-scale machine learning on heterogeneous\ndistributed systems. arXiv preprint arXiv:1603.04467.\nAbzianidze, L., Bjerva, J., Evang, K., Haagsma, H., van Noord, R., Ludmann, P.,\nNguyen, D.-D., and Bos, J. (2017). The Parallel Meaning Bank: Towards a\nMultilingual Corpus of Translations Annotated with Compositional Meaning\nRepresentations. In EACL, pages 242–247.\nAgić, Ž., Johannsen, A., Plank, B., Alonso, H. M., Schluter, N., and Søgaard, A.\n(2016). Multilingual projection for parsing truly low-resource languages. TACL,\n4:301–312.\nAgić, Ž., Plank, B., and Søgaard, A. (2017). Cross-lingual tagger evaluation without\ntest data. In EACL, pages 248–253.\nAgirre, E., Banea, C., Cardie, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Guo, W.,\nLopez-Gazpio, I., Maritxalar, M., Mihalcea, R., Rigau, G., Uria, L., and Wiebe, J.\n(2015). Semeval-2015 task 2: Semantic textual similarity, english, spanish and\npilot on interpretability. In SemEval 2015, pages 252–263.\n214\nBibliography\nAgirre, E., Banea, C., Cardie, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Guo, W.,\nMihalcea, R., Rigau, G., and Wiebe, J. (2014). Semeval-2014 task 10: Multilingual\nsemantic textual similarity. In SemEval, pages 81–91.\nAgirre, E., Banea, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Mihalcea, R., Rigau,\nG., and Wiebe, J. (2016). Semeval-2016 task 1: Semantic textual similarity,\nmonolingual and cross-lingual evaluation. In SemEval, pages 497–511.\nAgirre, E., Cer, D., Diab, M., and Gonzalez-Agirre, A. (2012). Semeval-2012 task 6:\nA pilot on semantic textual similarity. In SemEval, pages 385–393.\nAgirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A., and Guo, W. (2013). *SEM 2013\nshared task: Semantic textual similarity. In *SEM-SemEval, pages 32–43.\nAgirre, E., Cer, D., Diab, M., Lopez-Gazpio, I., and Specia, L. (2017). Semeval-\n2017 task 1: Semantic textual similarity multilingual and crosslingual focused\nevaluation. In SemEval.\nAl-Rfou, R., Perozzi, B., and Skiena, S. (2013). Polyglot: Distributed word repre-\nsentations for multilingual nlp. CoNLL-2013.\nAldarmaki, H. and Diab, M. (2016). GWU NLP at SemEval-2016 Shared Task 1:\nMatrix factorization for crosslingual STS. In SemEval, pages 663–667.\nAmmar, W., Mulcaire, G., Ballesteros, M., Dyer, C., and Smith, N. (2016). Many\nlanguages, one parser. TACL, 4:431–444.\nAndo, R. K. and Zhang, T. (2005). A framework for learning predictive structures\nfrom multiple tasks and unlabeled data. Journal of Machine Learning Research,\n6(Nov):1817–1853.\nArgyriou, A., Evgeniou, T., and Pontil, M. (2007). Multi-task feature learning. In\nAdvances in neural information processing systems, pages 41–48.\nArora, S., Bhaskara, A., Ge, R., and Ma, T. (2014). Provable bounds for learning\nsome deep representations. In ICML, pages 584–592.\nArroyo-Fernández, I. and Ruiz, I. V. M. (2017). Lipn-iimas at semeval-2017 task 1:\nSubword embeddings, attention recurrent neural networks and cross word\nalignment for semantic textual similarity. In SemEval.\nBibliography\n215\nAtaman, D., de Souza, J. G. C., Turchi, M., and Negri, M. (2016). Fbk hlt-mt at\nsemeval-2016 task 1: Cross-lingual semantic similarity measurement using\nquality estimation features and compositional bilingual word embeddings. In\nSemEval, pages 570–576.\nAugenstein, I. and Søgaard, A. (2017). Multi-task learning of keyphrase boundary\nclassiﬁcation. In ACL, pages 341–346.\nBahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly\nlearning to align and translate.\nBaker, C. F., Fillmore, C. J., and Lowe, J. B. (1998).\nThe Berkeley FrameNet\nproject. In ACL-COLING, pages 86–90, Université de Montréal, Montreal, Que-\nbec, Canada.\nBallesteros, M., Dyer, C., and Smith, N. A. (2015). Improved transition-based\nparsing by modeling characters instead of words with lstms. In EMNLP, pages\n349–359.\nBaroni, M., Dinu, G., and Kruszewski, G. (2014). Don’t count, predict! a systematic\ncomparison of context-counting vs. context-predicting semantic vectors. In\nACL, volume 1.\nBarzdins, G. and Gosko, D. (2016).\nRiga: Impact of smatch extensions and\ncharacter-level neural translation on amr parsing accuracy. In SemEval.\nBaxter, J. (1997). A bayesian/information theoretic model of learning to learn via\nmultiple task sampling. Machine learning, 28(1):7–39.\nBaxter, J. et al. (2000). A model of inductive bias learning. J. Artif. Intell. Res.(JAIR),\n12(149-198):3.\nBeltagy, I., Roller, S., Cheng, P., Erk, K., and Mooney, R. J. (2016). Representing\nmeaning with a combination of logical and distributional models. Computa-\ntional Linguistics.\nBengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2013). Advances in\noptimizing recurrent networks. In Acoustics, Speech and Signal Processing\n(ICASSP), 2013 IEEE International Conference on, pages 8624–8628. IEEE.\n216\nBibliography\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155.\nBengio, Y., Lee, D.-H., Bornschein, J., Mesnard, T., and Lin, Z. (2015). Towards\nbiologically plausible deep learning. arXiv preprint arXiv:1502.04156.\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependen-\ncies with gradient descent is diﬃcult. IEEE transactions on neural networks,\n5(2):157–166.\nBenton, A., Mitchell, M., and Hovy, D. (2017). Multitask learning for mental health\nconditions with limited social media data. In EACL, volume 1, pages 152–162.\nBerant, J. and Liang, P. (2014). Semantic parsing via paraphrasing. In ACL, pages\n1415–1425.\nBingel, J. and Søgaard, A. (2017). Identifying beneﬁcial task relations for multi-\ntask learning in deep neural networks. In EACL, pages 164–169.\nBjerva, J. (2014). Multi-class animacy classiﬁcation with semantic features. In\nProceedings of the Student Research Workshop at the 14th Conference of the\nEuropean Chapter of the Association for Computational Linguistics, pages 65–75.\nBjerva, J. (2016). Byte-based language identiﬁcation with deep convolutional\nnetworks. In Proceedings of the Third Workshop on NLP for Similar Languages,\nVarieties and Dialects (VarDial3), pages 119–125.\nBjerva, J. (2017a). Quantifying the Effects of Multilinguality in NLP Sequence\nPrediction Tasks. Under review.\nBjerva, J. (2017b). Will my auxiliary tagging task help? Estimating Auxiliary Tasks\nEffectivity in Multi-Task Learning. In Proceedings of the 21st Nordic Conference\non Computational Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden,\nnumber 131, pages 216–220. Linköping University Electronic Press, Linköpings\nuniversitet. Best short paper.\nBjerva, J. and Börstell, C. (2016). Morphological Complexity Inﬂuences Verb-\nObject Order in Swedish Sign Language. In Proceedings of the Workshop on\nComputational Linguistics for Linguistic Complexity (CL4LC), pages 137–141.\nBibliography\n217\nBjerva, J., Bos, J., and Haagsma, H. (2016a). The Meaning Factory at SemEval-2016\nTask 8: Producing AMRs with Boxer. In Proceedings of the 10th International\nWorkshop on Semantic Evaluation (SemEval-2016), pages 1179–1184.\nBjerva, J., Bos, J., Van der Goot, R., and Nissim, M. (2014). The meaning factory:\nFormal semantics for recognizing textual entailment and determining seman-\ntic similarity. In Proceedings of the 8th International Workshop on Semantic\nEvaluation (SemEval 2014), pages 642–646, Dublin, Ireland.\nBjerva, J., Grigonyt˙e, G., Östling, R., and Plank, B. (2017). Neural networks and\nspelling features for native language identiﬁcation. In Proceedings of Shared\nTask on NLI at BEA17.\nBjerva, J. and Östling, R. (2017a). Cross-lingual Learning of Semantic Textual\nSimilarity with Multilingual Word Representations.\nIn Proceedings of the\n21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May\n2017, Gothenburg, Sweden, number 131, pages 211–215. Linköping University\nElectronic Press, Linköpings universitet.\nBjerva, J. and Östling, R. (2017b). Multilingual word representations for semantic\ntextual similarity. In Proceedings of SemEval 2017: International Workshop on\nSemantic Evaluation.\nBjerva, J., Plank, B., and Bos, J. (2016b). Semantic tagging with deep residual\nnetworks. In Proceedings of COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages 3531–3541.\nBjerva, J. and Praet, R. (2015).\nWord embeddings pointing the way for late\nantiquity. In 9th SIGHUM Workshop on Language Technology for Cultural\nHeritage, Social Sciences and Humanities (LaTeCH 2015), pages 53–57.\nBjerva, J. and Praet, R. (2016). Rethinking intertextuality through a word-space\nand social network approach – the case of Cassiodorus. Journal of Data Mining\nand Digital Humanities, accepted, in revision.\nBos, J. (2008). Wide-Coverage Semantic Analysis with Boxer. In Bos, J. and\nDelmonte, R., editors, Semantics in Text Processing. STEP 2008 Conference\nProceedings, volume 1 of Research in Computational Semantics, pages 277–286.\nCollege Publications.\n218\nBibliography\nBos, J. (2014).\nSemantic annotation issues in parallel meaning banking.\nIn\nProceedings 10th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic\nAnnotation, page 17.\nBos, J., Basile, V., Evang, K., Venhuizen, N. J., and Bjerva, J. (2017). The Groningen\nMeaning Bank, pages 463–496. Springer Netherlands, Dordrecht.\nBos, J. and Nissim, M. (2015). Uncovering noun-noun compound relations by\ngamiﬁcation. In Proceedings of the 20th Nordic Conference of Computational\nLinguistics, NoDaLiDa 2015, May 11-13, 2015, Vilnius, Lithuania, number 109,\npages 251–255. Linköping University Electronic Press.\nBottou, L. (1998). Online algorithms and stochastic approximations. In Saad, D.,\neditor, Online Learning. Cambridge University Press, Cambridge, UK.\nBrady, M. L., Raghavan, R., and Slawny, J. (1989). Back propagation fails to sepa-\nrate where perceptrons succeed. IEEE Transactions on Circuits and Systems,\n36(5):665–674.\nBrants, T. (2000). Tnt: a statistical part-of-speech tagger. In Proceedings of the\nsixth conference on Applied natural language processing, pages 224–231.\nBusger op Vollenbroek, M., Carlotto, T., Kreutz, T., Medvedeva, M., Pool, C., Bjerva,\nJ., Haagsma, H., and Nissim, M. (2016). Gron-UP: Groningen user proﬁling. In\nProceedings of CLEF 2016.\nButler, A. (2010). The Semantics of Grammatical Dependencies, volume 23. Emerald\nGroup Publishing Limited.\nCaruana, R. (1997). Multitask learning. Machine Learning, 28 (1):41–75.\nCaruana, R. A. (1993). Multitask connectionist learning. In In Proceedings of the\n1993 Connectionist Models Summer School. Citeseer.\nChamberlain, J. (2014). Groupsourcing: Distributed problem solving using social\nnetworks. In Second AAAI Conference on Human Computation and Crowdsourc-\ning.\nChen, D. and Manning, C. D. (2014). A fast and accurate dependency parser using\nneural networks. In EMNLP, pages 740–750.\nBibliography\n219\nCheng, H., Fang, H., and Ostendorf, M. (2015). Open-domain name error detection\nusing a multi-task rnn. In EMNLP.\nCho, K. (2015). Natural language understanding with distributed representation.\narXiv preprint arXiv:1511.07916.\nCho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,\nH., and Bengio, Y. (2014). Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. In EMNLP.\nChollet, F. (2015). Keras. https://github.com/fchollet/keras.\nChomsky, N. (2005). Three factors in language design. Linguistic inquiry, 36(1):1–\n22.\nChoromanska, A., Henaff, M., Mathieu, M., Arous, G. B., and LeCun, Y. (2015). The\nloss surfaces of multilayer networks. In AISTATS.\nChrupała, G. (2013). Text segmentation with character-level text embeddings. In\nWorkshop on Deep Learning for Audio, Speech and Language Processing, ICML.\nChung, J., Cho, K., and Bengio, Y. (2016). A character-level decoder without explicit\nsegmentation for neural machine translation. ACL.\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation\nof gated recurrent neural networks on sequence modeling. arXiv preprint\narXiv:1412.3555.\nCiaramita, M. and Altun, Y. (2006). Broad-coverage sense disambiguation and\ninformation extraction with a supersense sequence tagger. In EMNLP, pages\n594–602.\nCollins, J., Sohl-Dickstein, J., and Sussillo, D. (2017). Capacity and trainability in\nrecurrent neural networks.\nCollobert, R. and Weston, J. (2008). A uniﬁed architecture for natural language\nprocessing: Deep neural networks with multitask learning. In Proceedings of\nthe 25th international conference on Machine learning, pages 160–167. ACM.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P.\n(2011). Natural language processing (almost) from scratch. Journal of Machine\nLearning Research, 12(Aug):2493–2537.\n220\nBibliography\nConneau, A., Schwenk, H., Barrault, L., and Lecun, Y. (2016).\nVery deep\nconvolutional networks for natural language processing.\narXiv preprint\narXiv:1606.01781.\nCopestake, A., Flickinger, D., Sag, I., and Pollard, C. (2005). Minimal recursion\nsemantics: An introduction. Journal of Research on Language and Computation,\n3(2–3):281–332.\nCotterell, R., Kirov, C., Sylak-Glassman, J., Walther, G., Vylomova, E., Xia, P.,\nFaruqui, M., Kübler, S., Yarowsky, D., Eisner, J., and Hulden, M. (2017). The\nCoNLL-SIGMORPHON 2017 shared task: Universal morphological reinﬂection\nin 52 languages. In CoNLL-SIGMORPHON.\nCotterell, R., Kirov, C., Sylak-Glassman, J., Yarowsky, D., Eisner, J., and Hulden, M.\n(2016). The sigmorphon 2016 shared task—morphological reinﬂection. In ACL\n2016, page 10.\nCoulmance, J., Marty, J.-M., Wenzek, G., and Benhalloum, A. (2015). Trans-gram,\nfast cross-lingual word-embeddings. In EMNLP, pages 1109–1113, Lisbon,\nPortugal.\nCover, T. M. and Thomas, J. A. (2012). Elements of information theory. John Wiley\n& Sons.\nCurran, J., Clark, S., and Bos, J. (2007). Linguistically Motivated Large-Scale NLP\nwith C&C and Boxer. In ACL, pages 33–36.\nCybenko, G. (1989). Approximation by superpositions of a sigmoidal function.\nMathematics of Control, Signals, and Systems (MCSS), 2(4):303–314.\nDas, D., Chen, D., Martins, A. F., Schneider, N., and Smith, N. A. (2014). Frame-\nsemantic parsing. Computational linguistics, 40(1):9–56.\nDaumé III, H. (2009). Bayesian multitask learning with latent hierarchies. In Pro-\nceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence,\npages 135–142. AUAI Press.\nDauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014).\nIdentifying and attacking the saddle point problem in high-dimensional non-\nconvex optimization. In Advances in neural information processing systems,\npages 2933–2941.\nBibliography\n221\nDeng, L. and Wiebe, J. (2015). Mpqa 3.0: An entity/event-level sentiment corpus.\nIn HLT-NAACL, pages 1323–1328.\ndos Santos, C. N. and Zadrozny, B. (2014). Learning character-level representa-\ntions for part-of-speech tagging. In ICML, pages 1818–1826.\nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for\nonline learning and stochastic optimization. The Journal of Machine Learning\nResearch, 12:2121–2159.\nDuong, L., Cohn, T., Bird, S., and Cook, P. (2015). Low resource dependency\nparsing: Cross-lingual parameter sharing in a neural network parser. In ACL,\npages 845–850.\nDyer, C., Ballesteros, M., Ling, W., Matthews, A., and Smith, N. A. (2015). Transition-\nbased dependency parsing with stack long short-term memory. In ACL-IJCNLP,\npages 334–343, Beijing, China.\nDyer, C., Kuncoro, A., Ballesteros, M., and Smith, N. A. (2016). Recurrent neural\nnetwork grammars. In NAACL, pages 199–209.\nEfron, B. and Tibshirani, R. J. (1994). An introduction to the bootstrap. CRC press.\nElman, J. L. (1990). Finding structure in time. Cognitive science, 14(2):179–211.\nEvang, K. (2016). Cross-lingual Semantic Parsing with Categorial Grammars. PhD\nthesis, University of Groningen.\nEvang, K., Basile, V., Chrupała, G., and Bos, J. (2013). Elephant: Sequence labeling\nfor word and sentence segmentation. In EMNLP, pages 1422–1426.\nEvang, K. and Bos, J. (2016). Cross-lingual learning of an open-domain semantic\nparser. In COLING.\nEvgeniou, T. and Pontil, M. (2004). Regularized multi–task learning. In Proceedings\nof the tenth ACM SIGKDD international conference on Knowledge discovery and\ndata mining, pages 109–117. ACM.\nFang, M. and Cohn, T. (2017). Model transfer for tagging low-resource languages\nusing a bilingual dictionary. In ACL, pages 587–593.\n222\nBibliography\nFaruqui, Y. T. S. S. M. and Lample, G. (2016). Polyglot neural language models: A\ncase study in cross-lingual phonetic representation learning. In NAACL-HLT,\npages 1357–1366.\nFirth, J. R. (1957). A synopsis of linguistic theory. pages 1930–1955. 1952–1959:1–\n32.\nGal, Y. and Ghahramani, Z. (2016). A theoretically grounded application of dropout\nin recurrent neural networks. In Advances in neural information processing\nsystems, pages 1019–1027.\nGallistel, C. (2016). The neurobiological bases for the computational theory of\nmind. Perspectives on the Work of Jerry Fodor. L. Gleitman and R. G. d. Almeida,\nEditors. New York, Oxford University Press.\nGallistel, C. R. and King, A. P. (2011). Memory and the computational brain: Why\ncognitive science will transform neuroscience, volume 6. John Wiley & Sons.\nGanitkevitch, J., Van Durme, B., and Callison-Burch, C. (2013). PPDB: The para-\nphrase database. In NAACL-HLT, pages 758–764.\nGeorgi, R., Xia, F., and Lewis, W. (2010). Comparing language similarity across\ngenetic and typologically-based groupings. In COLING, pages 385–393.\nGillick, D., Brunk, C., Vinyals, O., and Subramanya, A. (2015). Multilingual lan-\nguage processing from bytes. In NAACL-HLT.\nGlorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep\nfeedforward neural networks. In Aistats, volume 9, pages 249–256.\nGoldberg, Y. (2015). A primer on neural network models for natural language\nprocessing. arXiv preprint arXiv:1510.00726.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.\nhttp://www.deeplearningbook.org.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances\nin neural information processing systems, pages 2672–2680.\nBibliography\n223\nGoodfellow, I. J., Vinyals, O., and Saxe, A. M. (2015). Qualitatively characteriz-\ning neural network optimization problems. In International Conference on\nLearning Representations.\nGori, M. and Tesi, A. (1992). On the problem of local minima in backpropagation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 14(1):76–86.\nGouws, S. and Søgaard, A. (2015). Simple task-speciﬁc bilingual word embeddings.\nIn HLT-NAACL, pages 1386–1390.\nGraves, A. and Schmidhuber, J. (2005). Framewise phoneme classiﬁcation with\nbidirectional lstm and other neural network architectures. Neural Networks,\n18(5):602–610.\nGreff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., and Schmidhuber, J.\n(2016). Lstm: A search space odyssey. IEEE transactions on neural networks\nand learning systems.\nGuo, J., Che, W., Wang, H., and Liu, T. (2014). Learning sense-speciﬁc word\nembeddings by exploiting bilingual resources. In COLING, pages 497–507.\nGuo, J., Che, W., Yarowsky, D., Wang, H., and Liu, T. (2016). A representation\nlearning framework for multi-source transfer parsing. In Proc. of AAAI.\nHaagsma, H. and Bjerva, J. (2016). Detecting novel metaphor using selectional\npreference information. In Proceedings of the Fourth Workshop on Metaphor\nin NLP, pages 10–17.\nHahnloser, R. H., Sarpeshkar, R., Mahowald, M. A., Douglas, R. J., and Seung, H. S.\n(2000). Digital selection and analogue ampliﬁcation coexist in a cortex-inspired\nsilicon circuit. Nature, 405(6789):947–951.\nHahnloser, R. H. and Seung, H. S. (2001). Permitted and forbidden sets in symmet-\nric threshold-linear networks. In Advances in Neural Information Processing\nSystems, pages 217–223.\nHarris, Z. (1954). Distributional structure. Word, 10:146—-162.\nHauser, M. D., Chomsky, N., and Fitch, W. T. (2002). The faculty of language: What\nis it, who has it, and how did it evolve? Science, 298(5598):1569–1579.\n224\nBibliography\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015a). Deep residual learning for image\nrecognition. arXiv preprint arXiv:1512.03385.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015b). Delving deep into rectiﬁers: Sur-\npassing human-level performance on imagenet classiﬁcation. In Proceedings\nof the IEEE international conference on computer vision, pages 1026–1034.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Identity mappings in deep residual\nnetworks. arXiv preprint arXiv:1603.05027.\nHebb, D. O. (1949). The organization of behavior: A neuropsychological approach.\nJohn Wiley & Sons.\nHermann, K. M., Das, D., Weston, J., and Ganchev, K. (2014). Semantic frame\nidentiﬁcation with distributed word representations. In ACL, pages 1448–1458.\nHill, F., Cho, K., and Korhonen, A. (2016). Learning distributed representations of\nsentences from unlabelled data. arXiv preprint arXiv:1602.03483.\nHinton, G. E. (2012).\nLecture 6.5 - rmsprop, coursera: Neural networks for\nmachine learning. Technical report.\nHinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for\ndeep belief nets. Neural computation, 18(7):1527–1554.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural\ncomputation, 9(8):1735–1780.\nHornik, K., Stinchcombe, M., and White, H. (1989).\nMultilayer feedforward\nnetworks are universal approximators. Neural networks, 2(5):359–366.\nHuang, Z., Xu, W., and Yu, K. (2015). Bidirectional lstm-crf models for sequence\ntagging. arXiv preprint arXiv:1508.01991.\nHubel, D. H. and Wiesel, T. N. (1959). Receptive ﬁelds of single neurones in the\ncat’s striate cortex. The Journal of Physiology, 148(3):574–591.\nHubel, D. H. and Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction and\nfunctional architecture in the cat’s visual cortex. The Journal of Physiology\n(London), 160(1):106–154.\nBibliography\n225\nHubel, D. H. and Wiesel, T. N. (1968). Receptive ﬁelds and functional architecture\nof monkey striate cortex. The Journal of Physiology (London), 195(1):215–243.\nHwa, R., Resnik, P., Weinberg, A., Cabezas, C., and Kolak, O. (2005). Bootstrap-\nping parsers via syntactic projection across parallel texts. Natural Language\nEngineering, 11(03):311–325.\nIoffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.\nIyyer, M., Manjunatha, V., Boyd-Graber, J. L., and Daumé III, H. (2015). Deep\nunordered composition rivals syntactic methods for text classiﬁcation. In ACL,\npages 1681–1691.\nJirenhed, D.-A., Rasmussen, A., Johansson, F., and Hesslow, G. (2017). Learned\nresponse sequences in cerebellar purkinje cells. Proceedings of the National\nAcademy of Sciences, 114(23):6127–6132.\nJohansson, F., Jirenhed, D.-A., Rasmussen, A., Zucca, R., and Hesslow, G. (2014).\nMemory trace and timing mechanism localized to cerebellar Purkinje cells.\nProceedings of the National Academy of Sciences, 111(41):14930–14934.\nJohnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N., Viégas,\nF., Wattenberg, M., Corrado, G., et al. (2016). Google’s multilingual neural\nmachine translation system: enabling zero-shot translation. arXiv preprint\narXiv:1611.04558.\nJordan, M. I. (1997). Serial order: A parallel distributed processing approach.\nAdvances in psychology, 121:471–495.\nJozefowicz, R., Zaremba, W., and Sutskever, I. (2015). An empirical exploration\nof recurrent network architectures. In Proceedings of the 32nd International\nConference on Machine Learning (ICML-15), pages 2342–2350.\nJurgens, D. and Navigli, R. (2014). It’s all fun and games until someone annotates:\nVideo games with a purpose for linguistic annotation. TACL, 2:449–464.\nKanerva, P., Kristoferson, J., and Holst, A. (2000). Random indexing of text samples\nfor latent semantic analysis. In Proceedings of the Cognitive Science Society,\nvolume 1.\n226\nBibliography\nKang, Z., Grauman, K., and Sha, F. (2011). Learning with whom to share in multi-\ntask feature learning. In Proceedings of the 28th International Conference on\nMachine Learning (ICML-11), pages 521–528.\nKann, K. and Schütze, H. (2016). Med: The lmu system for the sigmorphon\n2016 shared task on morphological reinﬂection. In Proceedings of the 14th\nSIGMORPHON Workshop on Computational Research in Phonetics, Phonology,\nand Morphology, pages 62–70, Berlin, Germany.\nKim, Y. (2014). Convolutional neural networks for sentence classiﬁcation. arXiv\npreprint arXiv:1408.5882.\nKingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and\nFidler, S. (2015). Skip-thought vectors. In Advances in neural information\nprocessing systems, pages 3294–3302.\nKlementiev, A., Titov, I., and Bhattarai, B. (2012). Inducing crosslingual distributed\nrepresentations of words. In COLING, pages 1459–1474, Mumbai, India.\nKlerke, S., Goldberg, Y., and Søgaard, A. (2016). Improving sentence compression\nby learning to predict gaze. arXiv preprint arXiv:1604.03357.\nKoehn, P. (2005). Europarl: A parallel corpus for statistical machine translation.\nIn The Tenth Machine Translation Summit., Phuket, Thailand.\nKonstas, I., Iyer, S., Yatskar, M., Choi, Y., and Zettlemoyer, L. (2017). Neural\namr: Sequence-to-sequence models for parsing and generation. arXiv preprint\narXiv:1704.08381.\nKorchagina, N. (2017). Normalizing medieval german texts: from rules to deep\nlearning. In Proceedings of the NoDaLiDa 2017 Workshop on Processing His-\ntorical Language, number 133, pages 12–17. Linköping University Electronic\nPress, Linköpings universitet.\nKrogh, A. and Hertz, J. A. (1992). A simple weight decay can improve generaliza-\ntion. In Advances in neural information processing systems, pages 950–957.\nBibliography\n227\nKulmizev, A., Blankers, B., Bjerva, J., Nissim, M., van Noord, G., Plank, B., and\nWieling, M. (2017). The power of character n-grams in native language identiﬁ-\ncation. In Proceedings of Shared Task on NLI at BEA17.\nLakoff, G. and Johnson, M. (1980). Metaphors We Live By. Chicago: University of\nChicago Press.\nLample, G. and Chaplot, D. S. (2017). Playing fps games with deep reinforcement\nlearning. In AAAI, pages 2140–2146.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature, 521(7553):436–\n444.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998a). Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.\nLeCun, Y., Bottou, L., Orr, G. B., and Müller, K.-R. (1998b). Eﬃcient backprop. In\nNeural Networks: Tricks of the Trade, pages 9–50. Springer.\nLeCun, Y. et al. (1989). Generalization and network design strategies. Connection-\nism in perspective, pages 143–155.\nLevy, O. and Goldberg, Y. (2014a). Dependency-based word embeddings. In ACL,\npages 302–308.\nLevy, O. and Goldberg, Y. (2014b). Neural word embedding as implicit matrix\nfactorization. In Advances in neural information processing systems, pages\n2177–2185.\nLo, C.-k., Goutte, C., and Simard, M. (2016). Cnrc at semeval-2016 task 1: Experi-\nments in crosslingual semantic textual similarity. SemEval, pages 668–673.\nLoeff, N. and Farhadi, A. (2008). Scene discovery by matrix factorization. In\nEuropean Conference on Computer Vision, pages 451–464. Springer.\nLuong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., and Kaiser, L. (2015). Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114.\nMalaviya, C., Neubig, G., and Littell, P. (2017). Learning language representations\nfor typology prediction. arXiv preprint arXiv:1707.09569.\n228\nBibliography\nManning, C. D. (2015). Computational linguistics and deep learning. Computa-\ntional Linguistics, 41(4):701–707.\nMarcus, M., Santorini, B., and Marcinkiewicz, M. (1993). Building a Large An-\nnotated Corpus of English: The Penn Treebank. Computational Linguistics,\n19(2):313–330.\nMarelli, M., Bentivogli, L., Baroni, M., Bernardi, R., Menini, S., and Zamparelli,\nR. (2014). Semeval-2014 task 1: Evaluation of compositional distributional\nsemantic models on full sentences through semantic relatedness and textual\nentailment. In SemEval, pages 1–8.\nMartínez Alonso, H. and Plank, B. (2017). When is multitask learning effective?\nsemantic sequence prediction under varying data conditions. In EACL, pages\n44–53.\nMaurer, A. (2006). Bounds for linear multi-task learning. Journal of Machine\nLearning Research, 7(Jan):117–139.\nMcDonald, R., Petrov, S., and Hall, K. (2011a). Multi-source transfer of delexical-\nized dependency parsers. In Proceedings of EMNLP, pages 62–72.\nMcDonald, R., Petrov, S., and Hall, K. (2011b). Multi-source transfer of delexical-\nized dependency parsers. In EMNLP, pages 62–72.\nMelis, G., Dyer, C., and Blunsom, P. (2017). On the state of the art of evaluation in\nneural language models. arXiv preprint arXiv:1707.05589.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Eﬃcient estimation of\nword representations in vector space. arXiv preprint arXiv:1301.3781.\nMikolov, T., Le, Q. V., and Sutskever, I. (2013b). Exploiting similarities among\nlanguages for machine translation. arXiv preprint arXiv:1309.4168.\nMikolov, T., Yih, W.-t., and Zweig, G. (2013c). Linguistic regularities in continuous\nspace word representations. In NAACL-HLT, pages 746–751.\nMiller, G. A., Leacock, C., Tengi, R., and Bunker, R. T. (1993). A semantic concor-\ndance. In Proceedings of the workshop on Human Language Technology, pages\n303–308.\nBibliography\n229\nMinsky, M. and Papert, S. (1988). Perceptrons: an introduction to computational\ngeometry (expanded edition). MIT Press, Cambridge, Ma.\nMou, L., Meng, Z., Yan, R., Li, G., Xu, Y., Zhang, L., and Jin, Z. (2016). How transfer-\nable are neural networks in nlp applications? arXiv preprint arXiv:1603.06111.\nNair, V. and Hinton, G. E. (2010). Rectiﬁed linear units improve restricted boltz-\nmann machines. In Proceedings of the 27th International Conference on Machine\nLearning (ICML-10), pages 807–814.\nNivre, J., Agić, Ž., Ahrenberg, L., Aranzabe, M. J., Asahara, M., Atutxa, A., Balles-\nteros, M., Bauer, J., Bengoetxea, K., Bhat, R. A., Bick, E., Bosco, C., Bouma, G.,\nBowman, S., Candito, M., Cebiroğlu Eryiğit, G., Celano, G. G. A., Chalub, F., Choi,\nJ., Çöltekin, Ç., Connor, M., Davidson, E., de Marneffe, M.-C., de Paiva, V., Diaz de\nIlarraza, A., Dobrovoljc, K., Dozat, T., Droganova, K., Dwivedi, P., Eli, M., Erjavec,\nT., Farkas, R., Foster, J., Freitas, C., Gajdošová, K., Galbraith, D., Garcia, M., Gin-\nter, F., Goenaga, I., Gojenola, K., Gökırmak, M., Goldberg, Y., Gómez Guinovart,\nX., Gonzáles Saavedra, B., Grioni, M., Gr¯uz¯itis, N., Guillaume, B., Habash, N.,\nHajič, J., Hà My, L., Haug, D., Hladká, B., Hohle, P., Ion, R., Irimia, E., Johannsen,\nA., Jørgensen, F., Kaşıkara, H., Kanayama, H., Kanerva, J., Kotsyba, N., Krek,\nS., Laippala, V., Lê Hong, P., Lenci, A., Ljubešić, N., Lyashevskaya, O., Lynn, T.,\nMakazhanov, A., Manning, C., Mărănduc, C., Mareček, D., Martínez Alonso, H.,\nMartins, A., Mašek, J., Matsumoto, Y., McDonald, R., Missilä, A., Mititelu, V.,\nMiyao, Y., Montemagni, S., More, A., Mori, S., Moskalevskyi, B., Muischnek, K.,\nMustaﬁna, N., Müürisep, K., Nguyen Thi, L., Nguyen Thi Minh, H., Nikolaev, V.,\nNurmi, H., Ojala, S., Osenova, P., Øvrelid, L., Pascual, E., Passarotti, M., Perez,\nC.-A., Perrier, G., Petrov, S., Piitulainen, J., Plank, B., Popel, M., Pretkalnin, a,\nL., Prokopidis, P., Puolakainen, T., Pyysalo, S., Rademaker, A., Ramasamy, L.,\nReal, L., Rituma, L., Rosa, R., Saleh, S., Sanguinetti, M., Saul¯ite, B., Schuster,\nS., Seddah, D., Seeker, W., Seraji, M., Shakurova, L., Shen, M., Sichinava, D.,\nSilveira, N., Simi, M., Simionescu, R., Simkó, K., Šimková, M., Simov, K., Smith,\nA., Suhr, A., Sulubacak, U., Szántó, Z., Taji, D., Tanaka, T., Tsarfaty, R., Tyers,\nF., Uematsu, S., Uria, L., van Noord, G., Varga, V., Vincze, V., Washington, J. N.,\nŽabokrtský, Z., Zeldes, A., Zeman, D., and Zhu, H. (2017). Universal dependen-\ncies 2.0. LINDAT/CLARIN digital library at the Institute of Formal and Applied\nLinguistics, Charles University.\n230\nBibliography\nNivre, J., Agić, Ž., Aranzabe, M. J., Asahara, M., Atutxa, A., Ballesteros, M., Bauer,\nJ., Bengoetxea, K., Bhat, R. A., Bosco, C., Bowman, S., Celano, G. G. A., Connor,\nM., de Marneffe, M.-C., Diaz de Ilarraza, A., Dobrovoljc, K., Dozat, T., Erjavec, T.,\nFarkas, R., Foster, J., Galbraith, D., Ginter, F., Goenaga, I., Gojenola, K., Goldberg,\nY., Gonzales, B., Guillaume, B., Hajič, J., Haug, D., Ion, R., Irimia, E., Johannsen,\nA., Kanayama, H., Kanerva, J., Krek, S., Laippala, V., Lenci, A., Ljubešić, N.,\nLynn, T., Manning, C., Mărănduc, C., Mareček, D., Martínez Alonso, H., Mašek,\nJ., Matsumoto, Y., McDonald, R., Missilä, A., Mititelu, V., Miyao, Y., Montemagni,\nS., Mori, S., Nurmi, H., Osenova, P., Øvrelid, L., Pascual, E., Passarotti, M., Perez,\nC.-A., Petrov, S., Piitulainen, J., Plank, B., Popel, M., Prokopidis, P., Pyysalo, S.,\nRamasamy, L., Rosa, R., Saleh, S., Schuster, S., Seeker, W., Seraji, M., Silveira,\nN., Simi, M., Simionescu, R., Simkó, K., Simov, K., Smith, A., Štěpánek, J., Suhr,\nA., Szántó, Z., Tanaka, T., Tsarfaty, R., Uematsu, S., Uria, L., Varga, V., Vincze,\nV., Žabokrtský, Z., Zeman, D., and Zhu, H. (2015). Universal dependencies 1.2.\nLINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics,\nCharles University in Prague.\nNivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic, J., Manning, C. D.,\nMcDonald, R., Petrov, S., Pyysalo, S., Silveira, N., et al. (2016a). Universal\ndependencies v1: A multilingual treebank collection. In LREC.\nNivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y., Hajic, J., Manning, C. D.,\nMcDonald, R., Petrov, S., Pyysalo, S., Silveira, N., et al. (2016b). Universal\ndependencies v1: A multilingual treebank collection. In LREC, pages 1659–\n1666.\nObozinski, G., Taskar, B., and Jordan, M. I. (2010). Joint covariate selection and\njoint subspace selection for multiple classiﬁcation problems. Statistics and\nComputing, 20(2):231–252.\nOch, F. J. and Ney, H. (2003). A systematic comparison of various statistical\nalignment models. Computational linguistics, 29(1):19–51.\nÖstling, R. (2014). Bayesian word alignment for massively parallel texts. In EACL,\npages 123–127.\nÖstling, R. (2015). Bayesian models for multilingual word alignment. PhD thesis,\nDepartment of Linguistics, Stockholm University.\nBibliography\n231\nÖstling, R. (2016). Morphological reinﬂection with convolutional neural net-\nworks. In Proceedings of the 2016 Meeting of SIGMORPHON, Berlin, Germany.\nAssociation for Computational Linguistics.\nÖstling, R. and Bjerva, J. (2017). SU-RUG at the CoNLL-SIGMOR-PHON 2017 shared\ntask: Morphological Inﬂection with Attentional sequence-to-sequence mod-\nels. In Proceedings of the 2017 Meeting of SIGMORPHON, Vancouver, Canada.\nAssociation for Computational Linguistics.\nÖstling, R. and Tiedemann, J. (2016). Eﬃcient word alignment with markov chain\nmonte carlo. The Prague Bulletin of Mathematical Linguistics, 106(1):125–146.\nÖstling, R. and Tiedemann, J. (2017). Continuous multilinguality with language\nvectors. In EACL, pages 644–649.\nOuchi, H., Duh, K., and Matsumoto, Y. (2014). Improving dependency parsers\nwith supertags. In EACL, pages 154–158.\nOuchi, H., Duh, K., and Matsumoto, Y. (2016). Transition-based dependency\nparsing exploiting supertags. In IEEE/ACM Transactions on Audio, Speech and\nLanguage Processing, volume 24.\nPalatucci, M., Pomerleau, D., Hinton, G. E., and Mitchell, T. M. (2009). Zero-\nshot learning with semantic output codes. In Advances in neural information\nprocessing systems, pages 1410–1418.\nPascanu, R., Mikolov, T., and Bengio, Y. (2013).\nOn the diﬃculty of training\nrecurrent neural networks. ICML (3), 28:1310–1318.\nPlank, B. (2016). Keystroke dynamics as signal for shallow syntactic parsing. In\nCOLING, pages 609–619.\nPlank, B., Søgaard, A., and Goldberg, Y. (2016). Multilingual part-of-speech tagging\nwith bidirectional long short-term memory models and auxiliary loss. In ACL.\nPradhan, S. S., Ward, W., Hacioglu, K., Martin, J. H., and Jurafsky, D. (2004).\nShallow semantic parsing using support vector machines. In HLT-NAACL,\npages 233–240.\n232\nBibliography\nQuattoni, A., Collins, M., and Darrell, T. (2008). Transfer learning for image\nclassiﬁcation with sparse prototype representations. In Computer Vision and\nPattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE.\nRama, T. and Borin, L. (2015). Comparative evaluation of string similarity mea-\nsures for automatic language classiﬁcation. In Quantitative Linguistics, vol-\nume 69. De Gruyter Mouton.\nRasooli, M. and Collins, M. (2015). Density-driven cross-lingual transfer of depen-\ndency parsers. In Proceedings of EMNLP.\nRosa, R., Zeman, D., Mareček, D., and Žabokrtský, Z. (2017). Slavic forest, norwe-\ngian wood. pages 210–219.\nRosenblatt, F. (1957). The perceptron: a perceiving and recognizing automaton.\nTechnical report, Cornell Aeronautical Laboratory, inc.\nRosenthal, R. (1979). The ﬁle drawer problem and tolerance for null results.\nPsychological bulletin, 86(3):638.\nRuder, S., Bingel, J., Augenstein, I., and Søgaard, A. (2017). Sluice networks:\nLearning what to share between loosely related tasks.\narXiv preprint\narXiv:1705.08142.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). Learning internal\nrepresentations by error propagation. Technical report, California Univ San\nDiego La Jolla Inst for Cognitive Science.\nSahlgren, M. (2005).\nAn introduction to random indexing.\nIn Methods and\napplications of semantic indexing workshop at the 7th international conference\non terminology and knowledge engineering, TKE, volume 5.\nSaitou, N. and Nei, M. (1987). The neighbor-joining method: a new method for\nreconstructing phylogenetic trees. Molecular biology and evolution, 4(4):406–\n425.\nSaxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the\nnonlinear dynamics of learning in deep linear neural networks. In ICLR.\nSchaul, T., Antonoglou, I., and Silver, D. (2014). Unit tests for stochastic optimiza-\ntion. In International Conference on Learning Representations.\nBibliography\n233\nSchuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks.\nIEEE Transactions on Signal Processing, 45(11):2673–2681.\nSeeliger, K., Fritsche, M., Güçlü, U., Schoenmakers, S., Schoffelen, J.-M., Bosch, S.,\nand van Gerven, M. (2017). Cnn-based encoding and decoding of visual object\nrecognition in space and time. bioRxiv, page 118091.\nSemeniuta, S., Severyn, A., and Barth, E. (2016). Recurrent dropout without\nmemory loss. arXiv preprint arXiv:1603.05118.\nShannon, C. E. and Weaver, W. (1949). The mathematical theory of communication.\nUrbana, Ill.: University of Illinois press, 1964.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016).\nMastering the game of go with deep neural networks and tree search. Nature,\n529(7587):484–489.\nSjons, J., Hörberg, T., Östling, R., and Bjerva, J. (2017). Articulation rate in swedish\nchild-directed speech increases as a function of the age of the child even when\nsurprisal is controlled for. In Proceedings of Interspeech 2017, Stockholm,\nSweden.\nSocher, R., Chen, D., Manning, C. D., and Ng, A. (2013a). Reasoning with neu-\nral tensor networks for knowledge base completion. In Advances in neural\ninformation processing systems, pages 926–934.\nSocher, R., Ganjoo, M., Manning, C. D., and Ng, A. (2013b). Zero-shot learning\nthrough cross-modal transfer. In Advances in neural information processing\nsystems, pages 935–943.\nSøgaard, A. and Goldberg, Y. (2016). Deep multi-task learning with low level tasks\nsupervised at lower layers. In ACL, volume 2, pages 231–235.\nSøgaard, A., Johannsen, A., Plank, B., Hovy, D., and Martinez, H. (2014). What’s in\na p-value in nlp? In Proceedings of the eighteenth conference on computational\nnatural language learning (CONLL’14), pages 1–10.\nSontag, E. D. and Sussmann, H. J. (1989). Backpropagation can give rise to spurious\nlocal minima even for networks without hidden layers. Complex Systems,\n3(1):91–106.\n234\nBibliography\nSpecia, L., Shah, K., De Souza, J. G., and Cohn, T. (2013). Quest-a translation quality\nestimation framework. In ACL (Conference System Demonstrations), pages\n79–84.\nSrivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.\n(2014). Dropout: a simple way to prevent neural networks from overﬁtting.\nJournal of Machine Learning Research, 15(1):1929–1958.\nStraka, M., Hajic, J., and Straková, J. (2016). Ud-pipe: Trainable pipeline for\nprocessing conll-u ﬁles performing tokenization, morphological analysis, pos\ntagging and parsing. In LREC.\nSundermeyer, M., Schlüter, R., and Ney, H. (2012). Lstm neural networks for\nlanguage modeling. In Interspeech, pages 194–197.\nŠuster, S. (2016). Empirical studies on word representations. PhD thesis, University\nof Groningen.\nSutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recur-\nrent neural networks. In Proceedings of the 28th International Conference on\nMachine Learning (ICML-11), pages 1017–1024.\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with\nneural networks. In Advances in neural information processing systems, pages\n3104–3112.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Van-\nhoucke, V., and Rabinovich, A. (2015). Going deeper with convolutions. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 1–9.\nTäckström, O. (2013). Predicting Linguistic Structure with Incomplete and Cross-\nLingual Supervision. PhD thesis, PhD Thesis - Uppsala University.\nTäckström, O., Das, D., Petrov, S., McDonald, R., and Nivre, J. (2013). Token and\ntype constraints for cross-lingual part-of-speech tagging. TACL, 1:1–12.\nTäckström, O., McDonald, R., and Uszkoreit, J. (2012). Cross-lingual word clusters\nfor direct transfer of linguistic structure. In Proceedings of the 2012 conference\nof the North American chapter of the association for computational linguistics:\nHuman language technologies, pages 477–487.\nBibliography\n235\nTai, K. S., Socher, R., and Manning, C. D. (2015). Improved semantic representa-\ntions from tree-structured long short-term memory networks. In ACL-IJCNLP,\npages 1556–1566.\nThibodeau, P. H. and Boroditsky, L. (2013). Natural language metaphors covertly\ninﬂuence reasoning. PloS one, 8(1):e52961.\nThrun, S. and O’Sullivan, J. (1995). Clustering learning tasks and the selective\ncross-task transfer of knowledge. Technical report, CARNEGIE-MELLON UNIV\nPITTSBURGH PA DEPT OF COMPUTER SCIENCE.\nThrun, S. and Pratt, L. (1998). Learning to Learn. Kluwer Academic Publishers,\nNorwell, MA, USA.\nTian, J., Zhou, Z., Lan, M., and Wu, Y. (2017). ECNU at SemEval-2017 Task 1:\nLeverage kernelbased traditional nlp features and neural networks to build a\nuniversal model for multilingual and cross-lingual semantic textual similarity.\nIn SemEval.\nTiedemann, J. (2012). Parallel data, tools and interfaces in opus. In LREC. Euro-\npean Language Resources Association (ELRA).\nTiedemann, J. (2014). Rediscovering annotation projection for cross-lingual parser\ninduction. In COLING, pages 1854–1864.\nTiedemann, J. (2015). Cross-lingual dependency parsing with universal depen-\ndencies and predicted pos labels. Depling 2015, page 340.\nTiedemann, J., Agić, Ž., and Nivre, J. (2014). Treebank translation for cross-lingual\nparser induction. In Proceedings of CoNLL.\nTjong Kim Sang, E. F. and De Meulder, F. (2003). Introduction to the conll-2003\nshared task: Language-independent named entity recognition. In Proceedings\nof the seventh conference on Natural language learning at HLT-NAACL 2003-\nVolume 4, pages 142–147.\nTokui, S., Oono, K., Hido, S., and Clayton, J. (2015). Chainer: a next-generation\nopen source framework for deep learning. In Proceedings of workshop on\nmachine learning systems (LearningSys) in the twenty-ninth annual conference\non neural information processing systems (NIPS).\n236\nBibliography\nTorralba, A., Murphy, K. P., and Freeman, W. T. (2007). Sharing visual features\nfor multiclass and multiview object detection. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 29(5).\nToutanova, K., Haghighi, A., and Manning, C. D. (2005). Joint learning improves se-\nmantic role labeling. In Proceedings of the 43rd Annual Meeting on Association\nfor Computational Linguistics, pages 589–596.\nTurian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: a simple and\ngeneral method for semi-supervised learning. In ACL, pages 384–394.\nvan Noord, R. and Bos, J. (2017a).\nNeural semantic parsing by character-\nbased translation: Experiments with abstract meaning representations. arXiv\npreprint arXiv:1705.09980.\nvan Noord, R. and Bos, J. (2017b).\nThe Meaning Factory at SemEval-2017\nTask 9: Producing AMRs with Neural Semantic Parsing.\narXiv preprint\narXiv:1704.02156.\nVaswani, A., Zhao, Y., Fossum, V., and Chiang, D. (2013). Decoding with large-scale\nneural language models improves translation. In EMNLP, pages 1387–1392.\nVeit, A., Wilber, M. J., and Belongie, S. (2016). Residual networks behave like\nensembles of relatively shallow networks. In Advances in Neural Information\nProcessing Systems, pages 550–558.\nVenhuizen, N., Basile, V., Evang, K., and Bos, J. (2013). Gamiﬁcation for word sense\nlabeling. In Proc. 10th International Conference on Computational Semantics\n(IWCS-2013), pages 397–403.\nVilares, D., Gómez-Rodrıguez, C., Alonso, M. A., and LyS, G. (2016). One model,\ntwo languages: training bilingual parsers with harmonized treebanks. In ACL,\npage 425.\nVinyals, O., Kaiser, Ł., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. (2015).\nGrammar as a foreign language. In Advances in Neural Information Processing\nSystems, pages 2773–2781.\nVossen, P., Bloksma, L., Rodriguez, H., Climent, S., Calzolari, N., Roventini, A.,\nBertagna, F., Alonge, A., and Peters, W. (1998). The eurowordnet base concepts\nand top ontology. Deliverable D017 D, 34:D036.\nBibliography\n237\nŠuster, S., Titov, I., and van Noord, G. (2016). Bilingual learning of multi-sense\nembeddings with discrete autoencoders. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1346–1356, San Diego, California.\nWaibel, A., Hanazawa, T., Hinton, G. E., Shikano, K., and Lang, K. J. (1989).\nPhoneme recognition using time-delay neural networks. IEEE transactions on\nacoustics, speech, and signal processing, 37(3):328–339.\nWang, H., Raj, B., and Xing, E. P. (2017). On the origin of deep learning. arXiv\npreprint arXiv:1702.07800.\nWang, P., Qian, Y., Soong, F. K., He, L., and Zhao, H. (2015). A uniﬁed tagging\nsolution: Bidirectional lstm recurrent neural network with word embedding.\narXiv preprint arXiv:1511.00215.\nWichmann, S., Holman, E. W., and Brown (eds.), C. H. (2016). The ASJP Database\n(version 17).\nWiener, N. (1948). Cybernetics. Scientiﬁc American, 179(5):14–19.\nWolf, L., Hanani, Y., Bar, K., and Dershowitz, N. (2014). Joint word2vec networks\nfor bilingual semantic representations. In Proceedings of CICLING 2014.\nWu, H. (2009). Global stability analysis of a general class of discontinuous neu-\nral networks with linear growth activation functions. Information Sciences,\n179(19):3432–3441.\nWu, H., Huang, H., Jian, P., Guo, Y., and Su, C. (2017). BIT at SemEval-2017 Task 1:\nUsing semantic information space to evaluate semantic textual similarity. In\nSemEval.\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M.,\nCao, Y., Gao, Q., Macherey, K., et al. (2016). Google’s neural machine translation\nsystem: Bridging the gap between human and machine translation. arXiv\npreprint arXiv:1609.08144.\nYang, Z., Salakhutdinov, R., and Cohen, W. (2016). Multi-Task Cross-Lingual\nSequence Tagging from Scratch. arXiv preprint arXiv:1603.06270.\n238\nBibliography\nYarowsky, D., Ngai, G., and Wicentowski, R. (2001). Inducing multilingual text\nanalysis tools via robust projection across aligned corpora. In Proceedings\nof the ﬁrst international conference on Human language technology research,\npages 1–8.\nYi, X., Li, R., and Sun, M. (2016). Generating chinese classical poems with rnn\nencoder-decoder. arXiv preprint arXiv:1604.01537.\nZagoruyko, S. and Komodakis, N. (2016). Wide residual networks. In Proceed-\nings of the 27th British Machine Vision Conference (BMVC), arXiv preprint\narXiv:1605.07146.\nZeman, D. and Resnik, P. (2008). Cross-language parser adaptation between\nrelated languages. In Proceedings of the IJCNLP Workshop on NLP for Less\nPrivileged Languages.\nZhang, D., Yuan, B., Wang, D., and Liu, R. (2015a). Joint semantic relevance\nlearning with text data and graph knowledge. In ACL-IJCNLP 2015, page 32.\nZhang, X., Zhao, J., and LeCun, Y. (2015b). Character-level convolutional networks\nfor text classiﬁcation. In Advances in Neural Information Processing Systems,\npages 649–657.\nZhou, Y. and Chellappa, R. (1988). Computation of optical ﬂow using a neural\nnetwork. In IEEE International Conference on Neural Networks, volume 1998,\npages 71–78.\nZiemski, M., Junczys-Dowmunt, M., and Pouliquen, B. (2016). The united nations\nparallel corpus v1.0. In Calzolari, N., Choukri, K., Declerck, T., Goggi, S., Grobel-\nnik, M., Maegaard, B., Mariani, J., Mazo, H., Moreno, A., Odijk, J., and Piperidis,\nS., editors, LREC, Paris, France. European Language Resources Association\n(ELRA).\nSummary\nWhen learning a new skill, you take advantage of your preexisting\nskills and knowledge. For instance, if you are a skilled violinist, you\nwill likely have an easier time learning to play cello. Similarly, when\nlearning a new language you take advantage of the languages you\nalready speak. For instance, if your native language is Norwegian\nand you decide to learn Dutch, the lexical overlap between these two\nlanguages will likely beneﬁt your rate of language acquisition. This\nthesis deals with the intersection of learning multiple tasks and learn-\ning multiple languages in the context of Natural Language Processing\n(NLP), which can be deﬁned as the study of computational processing\nof human language. Although these two types of learning may seem\ndifferent on the surface, we will see that they share many similari-\nties.\nThe traditional approach in NLP is to consider a single task for a\nsingle language at a time. However, recent advances allow for broad-\nening this approach, by considering data for multiple tasks and lan-\nguages simultaneously. This is an important approach to explore fur-\nther as the key to improving the reliability of NLP, especially for low-\nresource languages, is to take advantage of all relevant data when-\never possible. In Part I of this thesis, we begin with an introduction\nto neural networks with a focus on NLP (Chapter 2), since such archi-\ntectures are particularly well suited to combined learning of multiple\ntasks and languages. We will then look at some ways in which neural\nnetworks can consider multiple tasks and languages at the same time\n240\nSummary\n(Chapter 3). Speciﬁcally, we will consider multitask learning (MTL),\nand several common multilingual approaches.\nIn Part II of this thesis, I look at exploiting the fact that many NLP\ntasks are highly related to one another. This is done by experiment-\ning with MTL using hard parameter sharing, which has proven ben-\neﬁcial for a variety of NLP tasks. In spite of such successes, however,\nit is not clear when or why MTL is beneﬁcial in NLP. Chapter 4 con-\ntains a case study in which semantic tagging is shown to be beneﬁcial\nfor POS tagging. This further highlights the question of when MTL is\nbeneﬁcial in NLP tagging tasks, which is explored using information-\ntheoretic measures in Chapter 5.\nMultilingual models can leverage the fact that many languages\nshare commonalities with one another. These resemblances can oc-\ncur on various levels, with languages sharing, for instance, syntac-\ntic, morphological, or lexical features. While there are many pos-\nsibilities for exploiting these commonalities, the focus in this the-\nsis is on using multilingual word representations, as they allow for\nstraight-forward integration in a neural network. As with MTL, it is\nnot clear in which cases it is an advantage to go multilingual. In Part\nIII, I begin with presenting a case study on multilingual semantic\ntextual similarity (Chapter 6). Following this, I explore how similar\nlanguages need to be, and in which way, in order to for it to be useful\nto go multilingual (Chapter 7).\nIn Part IV of this thesis, I experiment with a combined paradigm,\nin which a neural network is trained on several languages and tasks\nsimultaneously (Chapter 8). Finally, the thesis is concluded in Part V\n(Chapter 9). The experiments in this thesis are run on a large col-\nlection of mainly lexically oriented tasks, both semantic and mor-\nphosyntactic in nature, and on a total of 60 languages, representing\na relatively wide typological range.\nWhile traditional NLP approaches consider a single task or lan-\nguage at a time, the aim of this thesis was to answer several research\n241\nquestions dealing with pushing past this boundary. In doing so, the\nhope is that in the long term, low-resource languages can beneﬁt\nfrom the advances made in NLP which are currently to a large extent\nreserved for high-resource languages. This, in turn, may then have\npositive consequences for, e.g., language preservation, as speakers\nof minority languages will have a lower degree of pressure to using\nhigh-resource languages. In the short term, answering the speciﬁc re-\nsearch questions posed should be of use to NLP researchers working\ntowards the same goal.\nSamenvatting\nWanneer je een nieuwe vaardigheid leert maak je gebruik van de\nvaardigheden en kennis die je al bezit. Als je bijvoorbeeld een er-\nvaren violist bent, is het waarschijnlijk makkelijker om ook cello te\nleren spelen. Ook bij het leren van een nieuwe taal maak je gebruik\nvan de talen die je al beheerst. Bijvoorbeeld, als je moedertaal Noors\nis en je besluit dat je Nederlands wil gaan leren, dan maakt het grote\naantal woorden dat op elkaar lijkt in die twee talen het waarschijn-\nlijk makkelijker om de nieuwe taal te leren. In dit proefschrift kijk ik\nnaar het leren van meerdere taken, het leren van meerdere talen en\nde combinatie daarvan, in het kader van natural language processing\n(NLP): de computationele analyse van menselijke taal. Hoewel deze\ntwee soorten van leren aan de oppervlakte misschien anders lijken,\nzullen we zien dat ze meerdere overeenkomsten hebben.\nDe traditionele aanpak in NLP is om op één taak voor één taal\nte focussen. Nieuwe ontwikkelingen maken het echter mogelijk om\ndeze aanpak uit te breiden, door tegelijkertijd meerdere taken én\nmeerdere talen te bekijken. Dit is een veelbelovende richting voor\nverder onderzoek, aangezien het gebruiken van zoveel mogelijk re-\nlevante data de sleutel is tot het verbeteren van de betrouwbaarheid\nvan NLP, met name voor kleinere talen. Deel I van dit proefschrift be-\ngint met een inleiding over neurale netwerken, met een focus op NLP\n(Hoofdstuk 2). Deze architecturen zijn bijzonder geschikt voor het ge-\ncombineerd leren van meerdere taken en talen. Daarna zien we hoe\ngecombineerd leren werkt, door een aantal manieren waarop neu-\nrale netwerken tegelijkertijd meerdere taken en talen kunnen leren\n244\nSamenvatting\nte beschrijven (Hoofdstuk 3). In het bijzonder kijken we naar multi-\ntask learning (MTL) en enkele meertalige benaderingen van NLP.\nIn Deel II van dit proefschrift wordt onderzocht hoe we het feit\ndat veel taken in NLP sterk met elkaar zijn verbonden kunnen benut-\nten. Dit wordt gedaan door te experimenteren met MTL met hard\nparameter sharing, dat voor veel taken in NLP succesvol is gebleken.\nOndanks deze successen is het echter niet helemaal duidelijk wan-\nneer of waarom het gebruik van MTL voordelig is voor NLP. Hoofd-\nstuk 4 bevat een casestudy van MTL waarin ik laat zien dat het toe-\nkennen van semantische labels aan woorden een verbetering ople-\nvert bij het herkennen van woordsoorten. Deze vinding benadrukt\nhet belang van het beantwoorden van de vraag ‘Wanneer is het ge-\nbruik van MTL voordelig voor NLP?’. In Hoofdstuk 5 wordt dit on-\nderzocht aan de hand van begrippen uit de informatietheorie.\nMeertalige modellen kunnen gebruik maken van het feit dat ta-\nlen vaak overeenkomsten met elkaar hebben. Deze overeenkomsten\nkunnen op verschillende niveaus optreden, bijvoorbeeld op syntac-\ntisch, morfologisch of lexicaal vlak. Hoewel er veel mogelijkheden\nzijn om gebruik te maken van deze overeenkomsten, kijken we in dit\nproefschrift naar het gebruik van meertalige woordrepresentaties,\nomdat deze erg goed te combineren zijn met neurale netwerken. Net\nals bij MTL is het hier niet duidelijk in welke gevallen het een voor-\ndeel is om meertalige modellen te bouwen. Deel III begint met een\ncasestudy over meertalige semantische tekstuele gelijkenis (Hoofd-\nstuk 6). Daarna onderzoek ik in welke mate talen op elkaar moeten\nlijken, en op welke manier, om voordeel te kunnen halen uit het bou-\nwen van een meertalig model (Hoofdstuk 7).\nIn Deel IV van dit proefschrift experimenteer ik met een gecom-\nbineerd paradigma waarin een neuraal netwerk verschillende talen\nén taken tegelijkertijd leert (Hoofdstuk 8). Tenslotte worden de con-\nclusies uit dit proefschrift gepresenteerd in Deel V (Hoofdstuk 9).\nDe experimenten in dit proefschrift worden uitgevoerd op een\n245\ngrote verzameling voornamelijk lexicaal georiënteerde taken, zowel\nsemantisch en morfosyntactisch van aard, en op in totaal 60 talen,\ndie een relatief grote typologische diversiteit vertegenwoordigen.\nTerwijl de traditionele NLP-benaderingen slechts een enkele taak\nof taal tegelijk behandelen, is het doel van dit proefschrift juist om\nonderzoeksvragen die deze grenzen overschrijden te beantwoorden.\nDit heeft als doel dat kleinere talen op de lange termijn kunnen proﬁ-\nteren van de vooruitgang die in NLP wordt geboekt, aangezien deze\nmomenteel grotendeels ten goede komt aan grotere talen. Dit kan\npositieve gevolgen hebben voor bijvoorbeeld taalbehoud, omdat, on-\nder andere, sprekers van minderheidstalen een lagere druk zullen\nvoelen om grotere talen te gebruiken. Op de korte termijn zullen de\nantwoorden op de speciﬁeke onderzoeksvragen van dit proefschrift\nnuttig zijn voor NLP-onderzoekers die naar hetzelfde doel streven.\nGroningen dissertations in linguistics (GRODIL) \n \n  1. Henriëtte de Swart (1991). Adverbs of Quantification: A Generalized Quantifier Approach. \n  2. Eric Hoekstra (1991). Licensing Conditions on Phrase Structure. \n  3. Dicky Gilbers (1992). Phonological Networks. A Theory of Segment Representation. \n  4. Helen de Hoop (1992). Case Configuration and Noun Phrase Interpretation. \n  5. Gosse Bouma (1993). Nonmonotonicity and Categorial Unification Grammar. \n  6. Peter I. Blok (1993). The Interpretation of Focus. \n  7. Roelien Bastiaanse (1993). Studies in Aphasia. \n  8. Bert Bos (1993). Rapid User Interface Development with the Script Language Gist. \n  9. Wim Kosmeijer (1993). Barriers and Licensing. \n10. Jan-Wouter Zwart (1993). Dutch Syntax: A Minimalist Approach. \n11. Mark Kas (1993). Essays on Boolean Functions and Negative Polarity. \n12. Ton van der Wouden (1994). Negative Contexts. \n13. Joop Houtman (1994). Coordination and Constituency: A Study in Categorial Grammar. \n14. Petra Hendriks (1995). Comparatives and Categorial Grammar. \n15. Maarten de Wind (1995). Inversion in French. \n16. Jelly Julia de Jong (1996). The Case of Bound Pronouns in Peripheral Romance. \n17. Sjoukje van der Wal (1996). Negative Polarity Items and Negation: Tandem Acquisition. \n18. Anastasia Giannakidou (1997). The Landscape of Polarity Items. \n19. Karen Lattewitz (1997). Adjacency in Dutch and German. \n20. Edith Kaan (1997). Processing Subject-Object Ambiguities in Dutch. \n21. Henny Klein (1997). Adverbs of Degree in Dutch. \n22. Leonie Bosveld-de Smet (1998). On Mass and Plural Quantification: The case of French \n‘des’/‘du’-NPs. \n23. Rita Landeweerd (1998). Discourse semantics of perspective and temporal structure. \n24. Mettina Veenstra (1998). Formalizing the Minimalist Program. \n25. Roel Jonkers (1998). Comprehension and Production of Verbs in aphasic Speakers. \n26. Erik F. Tjong Kim Sang (1998). Machine Learning of Phonotactics. \n27. Paulien Rijkhoek (1998). On Degree Phrases and Result Clauses. \n28. Jan de Jong (1999). Specific Language Impairment in Dutch: Inflectional Morphology and \nArgument Structure. \n29. H. Wee (1999). Definite Focus. \n30. Eun-Hee Lee (2000). Dynamic and Stative Information in Temporal Reasoning: Korean tense and \naspect in discourse.\n31. Ivilin P. Stoianov (2001). Connectionist Lexical Processing. \n32. Klarien van der Linde (2001). Sonority substitutions. \n33. Monique Lamers (2001). Sentence processing: using syntactic, semantic, and thematic \ninformation. \n34. Shalom Zuckerman (2001). The Acquisition of \"Optional\" Movement. \n35. Rob Koeling (2001). Dialogue-Based Disambiguation: Using Dialogue Status to Improve Speech \nUnderstanding.  \n36. Esther Ruigendijk (2002). Case assignment in Agrammatism: a cross-linguistic study. \n37. Tony Mullen (2002). An Investigation into Compositional Features and Feature Merging for \nMaximum Entropy-Based Parse Selection. \n38. Nanette Bienfait (2002). Grammatica-onderwijs aan allochtone jongeren. \n39. Dirk-Bart den Ouden (2002). Phonology in Aphasia: Syllables and segments in level-specific \ndeficits. \n40. Rienk Withaar (2002). The Role of the Phonological Loop in Sentence Comprehension. \n41. Kim Sauter (2002). Transfer and Access to Universal Grammar in Adult Second Language \nAcquisition. \n42. Laura Sabourin (2003). Grammatical Gender and Second Language Processing: An ERP Study. \n43. Hein van Schie (2003). Visual Semantics. \n44. Lilia Schürcks-Grozeva (2003). Binding and Bulgarian. \n45. Stasinos Konstantopoulos (2003). Using ILP to Learn Local Linguistic Structures. \n46. Wilbert Heeringa (2004). Measuring Dialect Pronunciation Differences using Levenshtein \nDistance. \n47. Wouter Jansen (2004). Laryngeal Contrast and Phonetic Voicing: A Laboratory Phonology. \n48. Judith Rispens (2004). Syntactic and phonological processing in developmental dyslexia. \n49. Danielle Bougaïré (2004). L'approche communicative des campagnes de sensibilisation en \nsanté publique au Burkina Faso: Les cas de la planification familiale, du sida et de l'excision.  \n50. Tanja Gaustad (2004). Linguistic Knowledge and Word Sense Disambiguation. \n51. Susanne Schoof (2004). An HPSG Account of Nonfinite Verbal Complements in Latin. \n52. M. Begoña Villada Moirón (2005). Data-driven identification of fixed expressions and their \nmodifiability. \n53. Robbert Prins (2005). Finite-State Pre-Processing for Natural Language Analysis. \n54. Leonoor van der Beek (2005) Topics in Corpus-Based Dutch Syntax. \n55. Keiko Yoshioka (2005). Linguistic and gestural introduction and tracking of referents in L1 \nand L2 discourse. \n56. Sible Andringa (2005). Form-focused instruction and the development of second language \nproficiency. \n57. Joanneke Prenger (2005). Taal telt! Een onderzoek naar de rol van taalvaardigheid en \ntekstbegrip in het realistisch wiskundeonderwijs. \n58. Neslihan Kansu-Yetkiner (2006). Blood, Shame and Fear: Self-Presentation Strategies of \nTurkish Women’s Talk about their Health and Sexuality. \n59. Mónika Z. Zempléni (2006). Functional imaging of the hemispheric contribution to language \nprocessing. \n60. Maartje Schreuder (2006). Prosodic Processes in Language and Music. \n61. Hidetoshi Shiraishi (2006). Topics in Nivkh Phonology. \n62. Tamás Biró (2006). Finding the Right Words: Implementing Optimality Theory with Simulated \nAnnealing. \n63. Dieuwke de Goede (2006). Verbs in Spoken Sentence Processing: Unraveling the Activation \nPattern of the Matrix Verb. \n64. Eleonora Rossi (2007). Clitic production in Italian agrammatism. \n65. Holger Hopp (2007). Ultimate Attainment at the Interfaces in Second Language Acquisition: \nGrammar and Processing.  \n66. Gerlof Bouma (2008). Starting a Sentence in Dutch: A corpus study of subject- and object-\nfronting.  \n67. Julia Klitsch (2008). Open your eyes and listen carefully. Auditory and audiovisual speech \nperception and the McGurk effect in Dutch speakers with and without aphasia. \n68. Janneke ter Beek (2008). Restructuring and Infinitival Complements in Dutch. \n69. Jori Mur (2008). Off-line Answer Extraction for Question Answering. \n70. Lonneke van der Plas (2008). Automatic Lexico-Semantic Acquisition for Question Answering. \n71. Arjen Versloot (2008). Mechanisms of Language Change: Vowel reduction in 15th century \nWest Frisian. \n72. Ismail Fahmi (2009). Automatic term and Relation Extraction for Medical Question Answering \nSystem. \n73. Tuba Yarbay Duman (2009). Turkish Agrammatic Aphasia: Word Order, Time Reference and \nCase. \n74. Maria Trofimova (2009). Case Assignment by Prepositions in Russian Aphasia. \n75. Rasmus Steinkrauss (2009). Frequency and Function in WH Question Acquisition. A Usage-\nBased Case Study of German L1 Acquisition. \n76. Marjolein Deunk (2009). Discourse Practices in Preschool. Young Children’s Participation in \nEveryday Classroom Activities. \n77. Sake Jager (2009). Towards ICT-Integrated Language Learning: Developing an \nImplementation Framework in terms of Pedagogy, Technology and Environment. \n78. Francisco Dellatorre Borges (2010). Parse Selection with Support Vector Machines. \n79. Geoffrey Andogah (2010). Geographically Constrained Information Retrieval. \n80. Jacqueline van Kruiningen (2010). Onderwijsontwerp als conversatie. Probleemoplossing in \ninterprofessioneel overleg. \n81. Robert G. Shackleton (2010). Quantitative Assessment of English-American Speech \nRelationships. \n82. Tim Van de Cruys (2010). Mining for Meaning: The Extraction of Lexico-semantic Knowledge \nfrom Text. \n83. Therese Leinonen (2010). An Acoustic Analysis of Vowel Pronunciation  \nin Swedish Dialects. \n84. \nErik-Jan Smits (2010). Acquiring Quantification. How Children Use Semantics and \nPragmatics to Constrain Meaning. \n85. \nTal Caspi (2010). A Dynamic Perspective on Second Language Development. \n86. \nTeodora Mehotcheva (2010). After the fiesta is over. Foreign language attrition of Spanish in \nDutch and German Erasmus Student. \n87. \nXiaoyan Xu (2010). English language attrition and retention in Chinese and Dutch university \nstudents.  \n88. Jelena Prokić (2010). Families and Resemblances. \n89. Radek Šimík (2011). Modal existential wh-constructions. \n90. Katrien Colman (2011). Behavioral and neuroimaging studies on language processing in \nDutch speakers with Parkinson’s disease. \n91. Siti Mina Tamah (2011). A Study on Student Interaction in the Implementation of the Jigsaw \nTechnique in Language Teaching. \n92. Aletta Kwant (2011).Geraakt door prentenboeken. Effecten van het gebruik van prentenboeken \nop de sociaal-emotionele ontwikkeling van kleuters. \n93. Marlies Kluck (2011). Sentence amalgamation. \n94. Anja Schüppert (2011). Origin of asymmetry: Mutual intelligibility of spoken Danish and \nSwedish. \n95. Peter Nabende (2011). Applying Dynamic Bayesian Networks in Transliteration Detection and \nGeneration. \n96. Barbara Plank (2011). Domain Adaptation for Parsing. \n97. Cagri Coltekin (2011).Catching Words in a Stream of Speech: Computational simulations of \nsegmenting transcribed child-directed speech. \n98. \nDörte Hessler (2011). Audiovisual Processing in Aphasic and Non-Brain-Damaged Listeners: \nThe Whole is More than the Sum of its Parts. \n99. \nHerman Heringa (2012). Appositional constructions. \n100. Diana Dimitrova (2012). Neural Correlates of Prosody and Information Structure. \n101. Harwintha Anjarningsih (2012). Time Reference in Standard Indonesian Agrammatic \nAphasia. \n102. Myrte Gosen (2012). Tracing learning in interaction. An analysis of shared reading of \npicture books at kindergarten. \n103. Martijn Wieling (2012). A Quantitative Approach to Social and Geographical Dialect \nVariation. \n104. Gisi Cannizzaro (2012). Early word order and animacy. \n105. Kostadin Cholakov (2012). Lexical Acquisition for Computational Grammars. A Unified \nModel. \n106. Karin Beijering (2012). Expressions of epistemic modality in Mainland Scandinavian. A study \ninto the lexicalization-grammaticalization-pragmaticalization interface. \n107. Veerle Baaijen (2012). The development of understanding through writing. \n108. Jacolien van Rij (2012). Pronoun processing: Computational, behavioral, and \npsychophysiological studies in children and adults. \n109. Ankelien Schippers (2012). Variation and change in Germanic long-distance dependencies. \n110. Hanneke Loerts (2012).Uncommon gender: Eyes and brains, native and second language \nlearners, & grammatical gender. \n111. Marjoleine Sloos (2013). Frequency and phonological grammar: An integrated approach. \nEvidence from German, Indonesian, and Japanese. \n112. Aysa Arylova. (2013) Possession in the Russian clause. Towards dynamicity in syntax. \n113. Daniël de Kok (2013). Reversible Stochastic Attribute-Value Grammars. \n114. Gideon Kotzé (2013). Complementary approaches to tree alignment: Combining statistical \nand rule-based methods. \n115. Fridah Katushemererwe (2013). Computational Morphology and Bantu Language Learning: \nan Implementation for Runyakitara. \n116. Ryan C. Taylor (2013). Tracking Referents: Markedness, World Knowledge and Pronoun \nResolution. \n117. Hana Smiskova-Gustafsson (2013). Chunks in L2 Development: A Usage-based Perspective.  \n118. Milada Walková (2013). The aspectual function of particles in phrasal verbs. \n119. Tom O. Abuom (2013). Verb and Word Order Deficits in Swahili-English bilingual \nagrammatic speakers. \n \n120. Gülsen Yılmaz (2013). Bilingual Language Development among the First Generation Turkish \nImmigrants in the Netherlands. \n121. Trevor Benjamin (2013). Signaling Trouble: On the linguistic design of other-initiation of \nrepair in English conversation. \n122. Nguyen Hong Thi Phuong (2013). A Dynamic Usage-based Approach to Second Language \nTeaching.  \n123. Harm Brouwer (2014). The Electrophysiology of Language Comprehension: A \nNeurocomputational Model. \n124. Kendall Decker (2014). Orthography Development for Creole Languages. \n125. Laura S. Bos (2015). The Brain, Verbs, and the Past: Neurolinguistic Studies on Time \nReference. \n126. Rimke Groenewold (2015). Direct and indirect speech in aphasia: Studies of spoken \ndiscourse production and comprehension.  \n127. Huiping Chan (2015). A Dynamic Approach to the Development of Lexicon and Syntax in a \nSecond Language. \n128. James Griffiths (2015). On appositives. \n129. Pavel Rudnev (2015). Dependency and discourse-configurationality: A study of Avar. \n130. Kirsten Kolstrup (2015). Opportunities to speak. A qualitative study of a second language in \nuse. \n131. Güliz Güneş (2015). Deriving Prosodic structures. \n132. Cornelia Lahmann (2015). Beyond barriers. Complexity, accuracy, and fluency in long-term \nL2 speakers’ speech. \n133. Sri Wachyunni (2015). Scaffolding and Cooperative Learning: Effects on Reading \nComprehension and Vocabulary Knowledge in English as a Foreign Language. \n134. Albert Walsweer (2015). Ruimte voor leren. Een etnogafisch onderzoek naar het verloop van \neen interventie gericht op versterking van het taalgebruik in een knowledge building \nenvironment op kleine Friese basisscholen. \n135. Aleyda Lizeth Linares Calix (2015). Raising Metacognitive Genre Awareness in L2 Academic \nReaders and Writers.  \n136. Fathima Mufeeda Irshad (2015). Second Language Development through the Lens of a \nDynamic Usage-Based Approach. \n137. Oscar Strik (2015). Modelling analogical change. A history of Swedish and Frisian verb \ninflection.  \n138. He Sun (2015). Predictors and stages of very young child EFL learners’ English development \nin China. \n139 \nMarieke Haan (2015). Mode Matters. Effects of survey modes on participation and answering \nbehavior. \n140. Nienke Houtzager (2015). Bilingual advantages in middle-aged and elderly populations. \n141. Noortje Joost Venhuizen (2015). Projection in Discourse: A data-driven formal semantic \nanalysis. \n142. Valerio Basile (2015). From Logic to Language: Natural Language Generation from Logical \nForms. \n143. Jinxing Yue (2016). Tone-word Recognition in Mandarin Chinese: Influences of lexical-level \nrepresentations. \n144.  Seçkin Arslan (2016). Neurolinguistic and Psycholinguistic Investigations on Evidentiality in \nTurkish. \n145. Rui Qin (2016). Neurophysiological Studies of Reading Fluency. Towards Visual and \nAuditory Markers of Developmental Dyslexia. \n146. Kashmiri Stec (2016). Visible Quotation: The Multimodal Expression of Viewpoint. \n147. Yinxing Jin (2016). Foreign language classroom anxiety: A study of Chinese university \nstudents of Japanese and English over time. \n148. Joost Hurkmans (2016). The Treatment of Apraxia of Speech. Speech and Music Therapy, an \nInnovative Joint Effort. \n149. Franziska Köder (2016). Between direct and indirect speech: The acquisition of pronouns in \nreported speech. \n150. Femke Swarte (2016). Predicting the mutual intelligibility of Germanic languages from \nlinguistic and extra-linguistic factors. \n151. Sanne Kuijper (2016). Communication abilities of children with ASD and ADHD. \nProduction, comprehension, and cognitive mechanisms. \n152. Jelena Golubović (2016). Mutual intelligibility in the Slavic language area. \n153. Nynke van der Schaaf (2016). “Kijk eens wat ik kan!” Sociale praktijken in de interactie \ntussen kinderen van 4 tot 8 jaar in de buitenschoolse opvang. \n154. Simon Šuster (2016). Empirical studies on word representations. \n155. Kilian Evang (2016). Cross-lingual Semantic Parsing with Categorial Grammars. \n156. Miren Arantzeta Pérez (2017). Sentence comprehension in monolingual and bilingual \naphasia: Evidence from behavioral and eye-tracking methods. \n157. Sana-e-Zehra Haidry (2017). Assessment of Dyslexia in the Urdu Language. \n158. Srđan Popov (2017). Auditory and Visual ERP Correlates of Gender Agreement Processing \nin Dutch and Italian. \n159. Molood Sadat Safavi (2017). The Competition of Memory and Expectation in Resolving Long- \n        Distance Dependencies: Psycholinguistic Evidence from Persian Complex Predicates. \n160. Christopher Bergmann (2017). Facets of native-likeness: First-language attrition among \n        German emigrants to Anglophone North America. \n161. Stefanie Keulen (2017). Foreign Accent Syndrome: A Neurolinguistic Analysis. \n162. Franz Manni (2017). Linguistic Probes into Human History. \n163. Margreet Vogelzang (2017). Reference and cognition: Experimental and computational \ncognitive modeling studies on reference processing in Dutch and Italian. \n164. Johannes Bjerva (2017). One Model to Rule them all: Multitask and Multilingual Modelling for \nLexical Analysis. \n \nGRODIL \nCenter for Language and Cognition Groningen (CLCG) \nP.O. Box 716 \n9700 AS Groningen \nThe Netherlands \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-11-03",
  "updated": "2017-11-03"
}