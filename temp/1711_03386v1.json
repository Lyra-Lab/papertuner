{
  "id": "http://arxiv.org/abs/1711.03386v1",
  "title": "Performance Evaluation of Deep Learning Tools in Docker Containers",
  "authors": [
    "Pengfei Xu",
    "Shaohuai Shi",
    "Xiaowen Chu"
  ],
  "abstract": "With the success of deep learning techniques in a broad range of application\ndomains, many deep learning software frameworks have been developed and are\nbeing updated frequently to adapt to new hardware features and software\nlibraries, which bring a big challenge for end users and system administrators.\nTo address this problem, container techniques are widely used to simplify the\ndeployment and management of deep learning software. However, it remains\nunknown whether container techniques bring any performance penalty to deep\nlearning applications. The purpose of this work is to systematically evaluate\nthe impact of docker container on the performance of deep learning\napplications. We first benchmark the performance of system components (IO, CPU\nand GPU) in a docker container and the host system and compare the results to\nsee if there's any difference. According to our results, we find that\ncomputational intensive jobs, either running on CPU or GPU, have small overhead\nindicating docker containers can be applied to deep learning programs. Then we\nevaluate the performance of some popular deep learning tools deployed in a\ndocker container and the host system. It turns out that the docker container\nwill not cause noticeable drawbacks while running those deep learning tools. So\nencapsulating deep learning tool in a container is a feasible solution.",
  "text": "Performance Evaluation of Deep Learning Tools in\nDocker Containers\nPengfei Xu\nDepartment of Computer Science\nHong Kong Baptist University\nEmail: pengfeixu@comp.hkbu.edu.hk\nShaohuai Shi\nDepartment of Computer Science\nHong Kong Baptist University\nEmail: csshshi@comp.hkbu.edu.hk\nXiaowen Chu\nDepartment of Computer Science\nHong Kong Baptist University\nEmail: chxw@comp.hkbu.edu.hk\nAbstract—With the success of deep learning techniques in\na broad range of application domains, many deep learning\nsoftware frameworks have been developed and are being updated\nfrequently to adapt to new hardware features and software\nlibraries, which bring a big challenge for end users and system\nadministrators. To address this problem, container techniques are\nwidely used to simplify the deployment and management of deep\nlearning software. However, it remains unknown whether con-\ntainer techniques bring any performance penalty to deep learning\napplications. The purpose of this work is to systematically\nevaluate the impact of docker container on the performance of\ndeep learning applications. We ﬁrst benchmark the performance\nof system components (IO, CPU and GPU) in a docker container\nand the host system and compare the results to see if there’s any\ndifference. According to our results, we ﬁnd that computational\nintensive jobs, either running on CPU or GPU, have small\noverhead indicating docker containers can be applied to deep\nlearning programs. Then we evaluate the performance of some\npopular deep learning tools deployed in a docker container and\nthe host system. It turns out that the docker container will not\ncause noticeable drawbacks while running those deep learning\ntools. So encapsulating deep learning tool in a container is a\nfeasible solution.\nI. INTRODUCTION\nEver since the great success of deep learning techniques\nin many application domains, more and more deep learning\nsoftware tools have been developed by different research\ninstitutions and companies for both academic research and\ncommercial use [1]. Popular tools like Caffe [2], CNTK [3],\nMXNet [4], TensorFlow [5], Torch [6], etc. are still being\nactively developed and their new versions are being released\nfrequently, which brings signiﬁcant software management\nchallenge to system administrators. It is even worse when\ndifferent tools or different versions of the same tool need to\nbe installed in a system that is shared by multiple users. A\npractical solution to simplify the management of deep learning\ntools is to make use of docker containers so that environmental\nsetting conﬂicts can be easily resolved by packaging a software\nand its all required libraries into a single image [7]. Despite its\npopularity in practical usage, there lacks a systematic analysis\non the performance overhead brought by docker containers for\ndeep learning tools. This paper aims to investigate the impact\nof docker containers on the performance of deep learning tools.\nA typical deep learning training workﬂow involves data\naccess from/to disk drives and intensive data processing on\nCPUs and/or accelerators such as GPUs [8]. Therefore we\nevaluate the performance of CPU, GPU, disk I/O, and the\noverall deep learning training with and without dock container,\nrespectively. For CPU performance, we make use of two\nclassical and representative benchmarks, HPL and HPCG. For\nGPU performance, a set of GPU programs are selected to test\ndifferent types of GPU operations. Disk I/O performance can\nbe another important factor when huge amount of data are\nfed to neural networks during training process [9]. We test\nI/O performance from several aspects, including I/O access\nlatency, random access throughput, and sequential access\nthroughput. At last, we evaluate the training performance of\nﬁve popular deep learning software tools with different neural\nnetwork models and datasets. Based on our experimental re-\nsults, we ﬁnd that docker containers have negligible overhead\nin computing-intensive tasks on both CPU and GPU. The I/O\nperformance of sequential access under the docker container\nis found to be at the same level as the host system. When\nit comes to random access, we observe even shorter response\ntime on docker container than on the host system using one\nof the tested disk drives. This is because docker containers\ncan make better use of the NAND cache on the hard disk to\ngain faster random data access. Since each factor mentioned\nabove has satisfactory results on docker, it is not surprising\nto ﬁnd that running deep learning tools in docker containers\nhas negligible overhead compared to running on host systems\ndirectly.\nThis paper is organized as follows. Background of deep\nlearning and docker containers and related work are introduced\nin Section II. The design of our experiments is presented in\nSection III. We show our experimental results and analysis in\nSection IV. We conclude our work in Section V.\nII. BACKGROUND\nA. Deep Learning\nDeep learning is a class of machine learning techniques\nwhich powers great number of facets in our everyday life.\nDeep neural networks are built of many processing layers\nand are able to learn features from a mass of data with\nvarious stages of abstraction [9]. This technology has many\napplications like speech recognition [10] [11] [12] [13], image\nrecognition [14] [15] [16] [17], natural language processing\n[18] [19] [20], and the list is getting longer and longer.\nComparing with conventional machine leaning techniques,\narXiv:1711.03386v1  [cs.DC]  9 Nov 2017\ndeep learning has less limitation on the data fed to the\ncomputer to learn [9]. But training a deep neural network for\na certain problem is not an easy task and it requires signiﬁcant\ncomputational power.\nTo this end, many-core parallel processors like GPUs are\nwidely used to facilitate deep learning tasks [8]. Some stages\nof deep learning process can be eventually mapped to linear al-\ngebra operations which can usually be efﬁciently implemented\non parallel processors. As a matter of fact, many popular\ndeep learning software such as Caffe [2], CNTK [3], MXNet\n[4], TensorFlow [5], and Torch [6], have all implemented the\nsupport of GPUs whose performance is signiﬁcantly better\nthan CPUs [1].\nB. Docker Container\nDocker is a container virtualization technology which be-\nhaves similar to a light-weighted virtual machine, and it is\nthe most popular open source application-oriented approach\n[21]. Docker isolates each independent container running on\nthe same instance of operating system by making use of\nLinux kernel features like control groups and namespaces\n[21]. A simple illustration of docker can be found in Figure\n1. The simple architecture of docker leads to less overhead\nFig. 1: Docker Architecture\nas compared to other multi-layered types of virtualization\ntechnology. Each docker container encapsulates an application\nand its required dependencies, and can be run on different\nmachines on top of a docker engine. Docker images can also\nbe easily shared and distributed once they have been built.\nC. Related work\nDocker as a light-weighted virtualization solution has drawn\nattentions from the research community to explore its potential\nin different applications. A few studies have done great jobs\nin comparing the performance of docker with other types of\nvirtualization solution [22] [23] [24].\nIt has been shown that the docker container outperforms\nQEMU [22] when running popular GPU gaming benchmark,\nand it runs as good as native OS. The purpose of [22] is\nto study the possibilities of hosting cloud gaming service\nusing docker containers. We can only see the overall gaming\nperformance from it. Another work [25] also tests the per-\nformance of GPUs in docker containers and VMs by running\nthe algorithm SGEMM which performs matrix multiplication\nfollowed by an addition operation: C = αA × B + βC.\nIt indicates that invoking GPUs in docker containers will\nnot cause much overhead. Others also put docker into high\nperformance computing conditions [24] and compare it with\nvirtual machines. Chung’s work [24] evaluates the perfor-\nmance by running HPL benchmark which is compiled with\nOpenMPI and OpenBLAS and run with different problem\nsizes. According to their experimental results, docker container\nmanages to have better performance than VMs while using\nless RAM. Docker also has better scalability than VMs. As\nthe number of VM instances and docker containers increases,\ndocker can keep its performance without dramatically losing\ncomputing ability due to large overhead like VMs do. Different\nfrom the previous work, we aim to quantitatively measure\nthe performance overhead caused by docker containers when\nrunning deep learning software.\nIII. EXPERIMENTAL DESIGN\nIn this section, we will introduce the design of experiments\nto compare the performance between using docker container\nand without docker container. Four groups of experiments (i.e.,\nCPU, GPU, disk I/O, and deep learning tools) are designed to\nevaluate the performance differences of the same task running\nin docker container environment and in host system directly.\nTo make a fair comparison, we make sure that the irrelevant\nvariables like compiler versions and software libraries are the\nsame in docker container and host system. All experiments are\nperformed on two hardware platforms: a desktop PC and a rack\nserver. Detailed information of our hardware conﬁgurations\ncan be found in Table I and II. All reported results are the\naverage of 20 runs unless otherwise speciﬁed.\nItem\nModel\nCPU\nIntel i7-6800K\nMotherboard\nASUS X99-A II\nRAM\nKingston 64G DDR4\nGPU1\nNvidia GTX TITAN X\nGPU2\nNvidia GTX 980\nHard Drive\nSeagate ST3000DM008 7200RPM HDD\nTABLE I: Hardware Conﬁguration of Desktop PC Platform\nItem\nModel\nCPU\nIntel Xeon E5-2620 v3\nRAM\nSamsung 32G DDR4\nHard Drive\nLenovo System X 10K RPM 600GB (SAS 12Gb)\nTABLE II: Hardware Conﬁguration of Server Platform\n(Lenovo x3650 M5)\nSince we will perform our experiments with NVIDIA GPUs,\nwe use NVIDIA docker1 which is a thin wrapper on top\nof docker. When we start the NVIDIA docker, it calls the\ndocker and relies on NVIDIA Docker plugin to load GPU\ndriver and communicate with GPUs directly. NVIDIA docker\nonly changes the behavior of docker run and docker create\ncommands as stated in its ofﬁcial document.\nA. CPU Performance\nIn order to test the performance of CPU, we run both\nHPL and HPCG benchmarks from Intel MKL library in Intel\nParallel Studio 2017 Update 2. Both of them are measured in\nGFlops (Giga ﬂoating point operations per second).\nFor HPL benchmark, the problem sizes are set from 2,000 to\n45,000, simulating different levels of computational intensity.\nThe detailed experimental settings are shown in Table III.\nHPL is originated from Linpack benchmark that measures\nProblem Size\nLeading Dimension\n2000\n2000\n5000\n5008\n10000\n10000\n15000\n15000\n18000\n18008\n20000\n20016\n22000\n22008\n25000\n25000\n26000\n26000\n27000\n27000\n30000\n30000\n35000\n35000\n40000\n40000\n45000\n45000\nTABLE III: HPL Conﬁguration\nthe ﬂoating-point performance by solving a linear system of\nequations of order n (i.e., problem size) [26]:\nAx = b; A ∈ℜn×n; x, b ∈ℜn\nTo solve this linear system, it ﬁrst computes the LU factor-\nization with row partial pivoting of the n-by-n + 1 coefﬁcient\nmatrix:\n[A b] = [[L, U]y]\nSince the lower triangular factor L is applied to b as the\nfactorization progresses, the solution x is obtained by solving\nthe upper triangular system Ux = y. The lower triangular\nmatrix L is left unpivoted and the array of pivots is not\nreturned [26]. HPL serves as system stress test due to its\nintense computing property.\nHPCG is another popular benchmark designed for HPC\nsystems to be closer to real application. Basically, high per-\nformance conjugate gradient (HPCG) is consisted of compu-\ntations and data access patterns which are more commonly\n1Details about NVIDIA docker: https://github.com/NVIDIA/nvidia-docker\nseen in real applications [27]. This benchmark program also\nsolves a linear system Ax = b, but with a conjugate gradient\nmethod. As mentioned in [28], a system that is designed for\ngood HPL performance can result in wrong choices for adding\nunnecessary components or complexity to the system. We run\nHPCG with the problem dimension of 192 and running time\nbeing 1800 seconds to get valid benchmark results.\nB. GPU Performance\nAs for comparing GPU performance, we choose some\nrepresentative applications from CUDA 8.0 samples. Each of\nthem performs different operations on GPU.\nFirst we test the effective data transmission throughput from\nCPU to GPU, GPU to CPU, and GPU to GPU, aiming to check\nif docker container will affect the speed of data transmission\nwhich is crucial in training neural networks as huge amount\nof training data need to be delivered between CPU and GPU.\nThen we perform a convolution operation on a 18432 ×\n18432 image, which is commonly used in deep learning\nto extract features from training data like images [29], and\nmeasure the throughput in mega-pixels per second.\nMatrix multiplication is our third GPU application, as it\nis widely used in not only fully-connected layers, but also\nconvolutional layers [29]. So the efﬁciency of performing\nmatrix multiplication on GPU is crucial to deep learning\nperformance. We generate matrix A of size 23040 × 17280\nand matrix B of size 17280 × 11520 and calculate matrix C\nby calling CUBLAS function:\nA × B = C\nwhere\nA ∈ℜ23040×17280, B ∈ℜ17280×11520, C ∈ℜ23040×11520.\nSuch sizes are selected to ﬁll the 4GB memory of GTX 980\nso as to make the workload sufﬁciently large.\nNext, we benchmark the performance of self-deﬁned kernels\nby calculating 64-bin and 256-bin histogram of 67108864\nrandom numbers ranging from 0 to 255. The pseudo-code of\nthe algorithm is shown in Algorithm 1. The CUDA kernel\nfunction divides the data into many individual parts. Each\nthread processes one part and stores the sub-histogram in its\nown storage space. Then it merges all sub-histograms to get\nthe ﬁnal result [30].\nInput: Random number array data\nOutput: Histogram array result\nfor counter < BIN_COUNT do\nresult[counter] = 0;\nend\nfor counter < number of data do\nresult[data[counter]]++;\nend\nAlgorithm 1: Histogram Calculation\nLastly, we test different versions of matrix transpose which\nare memory intensive [31] [32]. We consider 8 different\nFig. 2: A Simple Example of CNN(LeNet) [35]\napproaches including simple copy, simple copy with shared\nmemory, naive transpose, coalesced transpose, shared memory\nbank conﬂicts, decomposing transpose, partition camping, and\ndiagonal block reordering from [33]. We use matrix transpose\nto test the efﬁciency of GPU memory access [33]. Matrix\ntranspose operations are also commonly used in deep learning\ntools [34].\nC. I/O Performance\nIn this part we test disk I/O performance in different ways\nwith dd and ioping tools. dd is used to write multiple ﬁles\nwith different sizes to hard disk and we can get access speed\nfrom the outputs of dd directly. Below shows an example of\nwriting out a 1 GB ﬁle:\ndd if=/dev/zero of=bigfile bs=64k \\\ncount=16k conv=fdatasync\nBasically, this command reads a stream of null characters\nin blocks and each block contains 64KB of data. Then it\nphysically writes to an output ﬁle named bigﬁle to make I/O\noperations. There are 16,000 blocks, which mean that in total\n16, 000 × 64KB = 1GB of data are written to the hard drive\nfor measuring the sequential writing throughput. We can use\nthe number of blocks, i.e., the count ﬂag, to control the ﬁle\nsize. Speciﬁcally, we set count to 16, 1600, 8k, and 16k to test\nwith ﬁles of size 1MB, 100MB, 512MB, and 1GB respectively.\nThen we use ioping to benchmark different access types\nincluding cache I/O random access test, direct I/O random\naccess test, as well as I/O latency test. Details are listed in\nTable IV.\nPurpose\nBash cmd\nIO latency\nioping -c 10 .\nCached IO random access\nioping -C -R -w 5 .\nDirect IO random access\nioping -D -R -w 5 .\nTABLE IV: ioping Tests\nD. Performance of Deep Learning Tools\nAfter measuring the performance of individual factors, we\ncome to evaluate the training performance of three represen-\ntative neural networks: Fully Connected Networks (FCNs),\nConvolutional Neural Networks (CNNs), and Recurrent Neural\nNetworks (RNNs) using different deep learning tools, includ-\ning Caffe [2], CNTK [3], MXNet [4], TensorFlow [5], and\nTorch [6]. The tested software versions are shown in Table V.\nWe measure the speed in unit of second per batch.\nSoftware\nMajor Version\nGitHub Commit ID\ncuDNN\nCaffe\n1.0\n39f28e4\nv5.1\nCNTK\n2.0\n1ae666d\nv5.1\nMXNet\n0.9\n32dc3a2\nv5.1\nTensorFlow\n1.0\n4ac9c09\nv5.1\nTorch\n7\n748f5e3\nv5.1\nTABLE V: Deep Learning Tools\n1) FCN: Fully connected networks are the simplest neural\nnetwork model. Each neuron performs a simple forward active\nfunction and sends the result to all the neurons in the next layer\n(see Fig. 3).\nFig. 3: Fully Connected Network\nWe design a FCN with 5 layers (including the input and\noutput layers) whose conﬁguration is shown in Table VI. We\nname it FCN5 and train it with MNIST dataset [36] which\nconsists of 60000 labeled handwriting images.\nLayer\n# of nerons\nActive function\nInput\n784\n-\nHidden 1\n2048\nSigmoid\nHidden 2\n4096\nSigmoid\nHidden 3\n1024\nSigmoid\nOutput\n10\nSoftmax\nTABLE VI: FCN5 Conﬁguration\n2) CNN: Convolutional Neural Networks (CNNs) are a set\nof models inspired by biology studies to simulate the way\nanimal brains process images by introducing convolutional\nlayers in artiﬁcial neural networks. As shown in Figure 2,\nin front of fully connected layers several convolutional layers\nare used to extract features from input images so that the\nclassiﬁer can better distinguish them with different labels. In\nthis part, we select AlexNet [16] and ResNet [14] to train\nCifar10 dataset [37].\n3) RNN: Recurrent Neural Networks (RNNs) are widely\nused in applications like speech recognition, machine transla-\ntion, language modeling, etc. [38]. Long short-term memory\n(LSTM) network is one of the most commonly used types in\nthis category.\nFig. 4: Example of LSTM layer\nAs Figure 4 illustrates, each LSTM layer packs a few\nsub-models and forms very deep neural networks by putting\nthem together. In our experiments, we build a 2-layer LSTM\nnetwork and train with PTB dataset. Each layer has 256 states\nand input texts are pre-processed into a character sequence\nwith the length of 32.\nIn summary, there are three categories of neural networks\nand four models will be included in our experiments. We will\nalso train each model with different batch sizes to simulate the\nreal training environment. Our experimental conﬁgurations are\nshown in Table VII.\nNetwork Type\nModel\nDataset\nBatch Size\nFCN5\nMNIST\n512\nFCN\nFCN5\nMNIST\n1024\nFCN5\nMNIST\n2048\nAlexnet\nCifar10\n256\nAlexnet\nCifar10\n512\nCNN\nAlexnet\nCifar10\n1024\nResnet50\nCifar10\n16\nResnet50\nCifar10\n32\nResnet50\nCifar10\n64\nLSTM\nPTB\n128\nRNN\nLSTM\nPTB\n256\nLSTM\nPTB\n512\nTABLE VII: Deep Learning Tools Experiment Design\nIV. EXPERIMENTAL RESULTS AND ANALYSIS\nIn this section, we will present our detailed experimental\nresults. We mainly focus on presenting the performance differ-\nence between using docker container and without using docker\ncontainer. We introduce the symbol Diff% to represent the\ndifference which is deﬁned as follows:\nDiff% = H −D\nH\n× 100%\n(1)\nwhere H is the result without using docker container and D\nis result of using docker container.\nA. CPU Performance\n1) HPL Benchmark: We run HPL experiments on both Intel\nXeon E5-2620 v3 and Intel i7-6800K platforms with different\nworkloads. We gradually increase the problem size from 2,000\nto 45,000. As illustrated in Fig. 5, HPL tests on CPU performs\nnearly the same in docker container and host system in general.\nThe differences are mostly less than 1%. It is very interesting\nto notice that the HPL performance of using docker container\ncan be even better on our server platform.\nFig. 5: HPL Benchmark Results\n2) HPCG Benchmark: HPCG benchmark experiments also\nshow that there is little overhead when using docker container.\nTable VIII illustrates that the differences between docker\ncontainer and host system are merely 0.24425% with E5-2620\nv3 and 0.55624% with i7-6800k respectively.\nDocker\nHost\nDifference\nDiff%\nE5-2620 v3\n5.525\n5.539\n0.013\n0.244%\ni7-6800k\n5.288\n5.317\n0.029\n0.556%\nTABLE VIII: HPCG Benchmark Results\nAbove all, minor performance differences are found in both\nHPL and HPCG benchmark experiments meaning that using\nCPU in a docker container won’t introduce much overhead.\nB. GPU Performance\nWe perform our GPU tests on two generations of NVIDIA\nGPUs which are built in different architectures: the latest GTX\nTitan X (Pascal) and GTX 980 (Maxwell). GPU performance\nis a crucial factor for all the deep learning tools shown in\nTable V. The experimental results are summarized in Fig. 6.\nWe can see that the bandwidth tests show tiny performance\ndifference in all three data transmission experiments. The\nmost widely used matrix operation, matrix multiplication,\nalso shows little performance difference. As for self-deﬁned\nkernels like the histogram experiments, it is shown that the\nhost system does perform better than in the docker con-\ntainer, especially when running on GTX980. A set of matrix\ntranspose operations serves as a comprehensive evaluation of\nutilizing GPU for general purpose computation. Again there\nis no huge performance difference found in this set of ex-\nperiments. GPU performance varies because of many physical\nenvironmental factors such as temperature change. Overall, it\nis quite promising that the maximum absolute Diff% value is\nonly 0.61% (in the experiment of histogram256). These most\nfrequently used operations in deep learning processes like data\ntransmission, matrix multiplication, convolution operation, and\nmatrix transpose all perform very well under docker container.\nC. I/O Performance\n1) dd Test: We use dd command to sequentially write\nseveral ﬁles of different sizes to hard disks and collect speed\ninformation from outputs.\nPC Platform Diff%\nServer Platform Diff%\n1MiB ﬁle\n-4.263%\n-4.023%\n100MiB ﬁle\n-1.408%\n6.893%\n512MiB ﬁle\n0.000%\n5.714%\n1GiB ﬁle\n0.636%\n3.448%\nTABLE X: dd Write Test\nFrom Table X we can see that I/O speeds are very close\nwhen comparing docker containers and host systems. Ac-\ncessing smaller ﬁles tends to have a higher speed in docker\nFig. 6: GPU Benchmark Results\ncontainers, whilst accessing large ﬁles in the host system are\na little bit better than in docker containers. We also measure\nthe standard deviation among each set of 20-runs to estimate\nthe stability as illustrated in Table XI. The values of standard\ndeviation from docker container and host system are very\nclose. We can claim that docker containers are as stable as\nhost system in sequentially writing out large ﬁles.\n2) ioping Test: To measure the random access speed, we\nmake use of ioping command. The results are rather interesting\ncompared with dd tests. As we can see in Table XII, random\nI/O access results are found to be close in our server platform.\nHowever, we observe huge performance differences in our\nPC platform. Docker containers are extremely faster than host\nsystem in direct I/O test and I/O latency test. We go deeper into\nthis phenomenon and ﬁnd that the speed of direct I/O random\naccess in docker container is almost the same as accessing the\ndisk cache.\nBest\nAverage\nSTD\nServer with docker\n322\n306.476\n19.6069\nServer w/o docker\n309\n292.333\n21.6272\nPC with docker\n103\n100.561\n0.97408\nPC w/o docker\n103\n102.04\n0.64888\nTABLE XI: dd Sequential Access Test\nCaffe GTX Titan\nCaffe GTX 980\nCNTK GTX Titan\nCNTK GTX 980\nMXNet GTX Titan\nMXNet GTX 980\nTF GTX Titan\nTF GTX 980\nTorch GTX Titan\nTorch GTX 980\nFCN 512\n1.084%\n-1.901%\n0.077%\n4.299%\n3.157%\n1.047%\n-5.139%\n2.944%\n6.891%\n4.096%\nFCN 1024\n-0.313%\n-1.084%\n-0.923%\n4.814%\n-0.315%\n0.115%\n2.966%\n3.134%\n8.641%\n4.611%\nFCN 2048\n1.033%\n0.006%\n-1.993%\n4.540%\n0.190%\n0.324%\n-2.013%\n4.165%\n9.615%\n4.863%\nAlexnet 256\n-0.848%\n-3.308%\n-0.357%\n1.397%\n-1.603%\n0.193%\n-0.473%\n1.166%\n3.369%\n2.007%\nAlexnet 512\n-0.241%\n0.705%\n-5.474%\n1.619%\n-1.799%\n-0.397%\n0.028%\n2.336%\n3.858%\n1.978%\nAlexnet 1024\n0.535%\n-0.061%\n-1.222%\n1.996%\n-3.607%\n-0.714%\n0.388%\n1.182%\n3.767%\n1.771%\nResnet 16\n-0.282%\n-0.552%\n-3.189%\n-0.704%\n-0.043%\n3.014%\n-0.581%\n2.001%\n1.407%\n-0.300%\nResnet 32\n-0.010%\n-0.467%\n0.120%\n0.108%\n0.291%\n0.689%\n3.108%\n2.143%\n0.308%\n0.061%\nResnet 64\n0.394%\n-0.043%\n-0.173%\n-0.119%\n0.940%\n1.174%\n2.524%\n0.905%\n0.146%\n0.174%\nLSTM128\nNot Support\nNot Support\n-3.058%\n0.089%\n-1.390%\n-0.399%\n1.084%\n0.540%\n0.929%\n0.080%\nLSTM256\nNot Support\nNot Support\n1.326%\n-0.208%\n3.870%\n0.024%\n0.708%\n0.429%\n0.998%\n0.031%\nLSTM512\nNot Support\nNot Support\n3.486%\n0.310%\n-4.484%\n0.339%\n0.759%\n0.558%\n-1.237%\nNot Support\nTABLE IX: Deep Learning Tools Benchmark Batch Time Diff%\nThe results in Table XII show that docker container reacts to\nI/O request almost 100% faster than it is in the host system. We\ntake a look at the speciﬁcation of our hard drive in GPU server\nand notice that it come equipped with Multi-Tier Caching\nTechnology(MTC). Basically in addition to the actual spinning\ndisk storing data, there are multiple layers of NAND Flash\ninstalled on the hard drive for quick access of frequently\nused data for ongoing processes. Docker containers depend\non docker engine running in the background taking advantage\nof MTC for fast small data access and IO requests. Because of\nthat, ioping doesn’t go into the actual disk of hard drive during\nioping tests and that’s why direct IO random access speed and\nIO latency time have such huge performance difference.\nServer Diff%\nPC Diff%\nCache IO random\n17.447%\n23.065%\naccess speed\nDirect IO random\n-2.521%\n-282.075%\naccess speed\nIO latency time\n3.509%\n98.999%\nTABLE XII: ioping Test Result\nD. Deep Learning Tools\nAfter we evaluate individual factors that may affect the\nperformance of deep learning tools, we then test each deep\nlearning tool by training different types of neural networks.\nTable IX shows the results of all networks trained by all the\ntools. Different tools have their own metrics of measuring\nperformance, so we convert them into the time they take to\ntrain one batch of data. The left most column indicates the\ntype of neural network and the number of samples we put\ninto each batch.\nNote that positive numbers in Table IX illustrate that the\ndocker container outperforms our host system because the\nhost system needs more time to train one batch of data. As\nTable IX shows that deep learning tools perform well in docker\ncontainer overall. For large networks Resnet and LSTM, we\ncan see that the performance of deep learning tools in docker\ncontainer is as good as in the host system. In those cases\nthat docker container runs slower than the host system, the\ndifference is usually within 5%. As for smaller networks like\nFCN and Alexnet, we also ﬁnd that the performance of each\ntool running in docker container and in the host system are\nsimilar in terms of computational time costs. An interesting\nphenomenon is found on MXNet, whose training time of the\nﬁrst epoch in docker container is much shorter than the host,\nas shown in Table XIII. Notice that starting from the second\nepoch, the difference drops back to normal. This is because\nthe ﬁrst epoch includes the initial I/O time which has different\nperformance under docker container and host system, while\nlater on the I/O time is hidden by the computing time. These\ndeep learning frameworks implement parallel data loading that\ndata for the next run are pre-loaded during the training process,\nso that the I/O time is covered by computing time and we don’t\nsee much difference after the ﬁrst epoch.\nMXNet GTX980\nEpoch 1 Diff%\nEpoch 2 Diff%\nFCN 512\n75.707%\n-1.654%\nFCN 1024\n77.169%\n0.000%\nFCN 2048\n74.563%\n1.720%\nAlexnet 256\n69.468%\n-0.798%\nAlexnet 512\n68.451%\n0.145%\nAlexnet 1024\n68.871%\n-0.428%\nTABLE XIII: MXNet 1st Epoch vs. 2nd Epoch\nV. CONCLUSION AND FUTURE WORK\nIn conclusion, even though there are extra layers lying\nbetween applications and hardware resources by using docker\ncontainers, docker engine manages to minimize the overhead\npretty well. We don’t ﬁnd noticeable drawbacks of docker\ncontainers in CPU and GPU tests. Testing programs running\nin docker containers perform just as good as in the host\nsystem. So putting deep learning tools into docker containers\nis a feasible solution that we can beneﬁt from its ﬂexibility,\nlightweight, and resource isolation abilities. Different deep\nlearning tools or the same tools with different version numbers\ncan coexist in one system yet maintaining good performance.\nSystem administrators of shared servers and cloud platforms\ncan install docker engine in the system and let users download\nand run their desired images by their own. System administra-\ntors can prepare docker images with deep learning tools pre-\ninstalled and properly conﬁgured so that users only need to\nfocus on their models and algorithms without getting annoyed\nby dependencies and environment settings.\nIn the future, this work can be further extended on multiple\nmachines that train large-scale neural networks on a cluster\nto gain even more acceleration. In this situation, data trans-\nmission efﬁciency needs to be tested among docker containers\nlocated in different physical machines. Even within the same\nnode there can be multiple GPUs installed, efﬁciently making\nuse of more than one GPU at the same time is also important\nin deep learning. On the other hand, more types of hardware\nplatform can be included. In our work, we mainly focus\non the combination of CPU + NVIDIA GPUs. There are\nother accelerators such as AMD GPUs and Intel Xeon Phi\nprocessors. The results we get from our docker container are\nbased on NVIDIA docker which has ofﬁcial support directly\nfrom the GPU manufacturer. Whether it is efﬁcient or not to\ninvoke computing devices from other kind needs to be further\nstudied.\nVI. ACKNOWLEDGEMENT\nThe authors would like to thank all the reviewers for\ntheir insightful comments and valuable suggestions. This work\nis supported by Shenzhen Basic Research Grant SCI-2015-\nSZTIC-002.\nREFERENCES\n[1] S. Shi, Q. Wang, P. Xu, and X. Chu, “Benchmarking state-of-the-art\ndeep learning software tools,” arXiv preprint arXiv:1608.07249, 2016.\n[2] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\nfast feature embedding,” in Proceedings of the 22nd ACM international\nconference on Multimedia.\nACM, 2014, pp. 675–678.\n[3] D. Yu, A. Eversole, M. Seltzer, K. Yao, Z. Huang, B. Guenter,\nO. Kuchaiev, Y. Zhang, F. Seide, H. Wang et al., “An introduction\nto computational networks and the computational network toolkit,”\nMicrosoft Technical Report MSR-TR-2014–112, 2014.\n[4] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu,\nC. Zhang, and Z. Zhang, “Mxnet: A ﬂexible and efﬁcient machine\nlearning library for heterogeneous distributed systems,” arXiv preprint\narXiv:1512.01274, 2015.\n[5] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin et al., “Tensorﬂow: Large-scale\nmachine learning on heterogeneous distributed systems,” arXiv preprint\narXiv:1603.04467, 2016.\n[6] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A matlab-like\nenvironment for machine learning,” in BigLearn, NIPS Workshop, no.\nEPFL-CONF-192376, 2011.\n[7] S. Fu, J. Liu, X. Chu, and Y. Hu, “Toward a standard interface for cloud\nproviders: the container as the narrow waist,” IEEE Internet Computing,\nvol. 20, no. 2, pp. 66–71, 2016.\n[8] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,\nand E. Shelhamer, “cuDNN: Efﬁcient primitives for deep learning,”\narXiv preprint arXiv:1410.0759, 2014.\n[9] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[10] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep neural\nnetworks for acoustic modeling in speech recognition: The shared views\nof four research groups,” IEEE Signal Processing Magazine, vol. 29,\nno. 6, pp. 82–97, 2012.\n[11] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent pre-\ntrained deep neural networks for large-vocabulary speech recognition,”\nIEEE Transactions on Audio, Speech, and Language Processing, vol. 20,\nno. 1, pp. 30–42, 2012.\n[12] L. Deng, G. Hinton, and B. Kingsbury, “New types of deep neural\nnetwork learning for speech recognition and related applications: An\noverview,” in Acoustics, Speech and Signal Processing (ICASSP), 2013\nIEEE International Conference on.\nIEEE, 2013, pp. 8599–8603.\n[13] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with\ndeep recurrent neural networks,” in Acoustics, speech and signal pro-\ncessing (icassp), 2013 ieee international conference on.\nIEEE, 2013,\npp. 6645–6649.\n[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 770–778.\n[15] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are\neasily fooled: High conﬁdence predictions for unrecognizable images,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015, pp. 427–436.\n[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural infor-\nmation processing systems, 2012, pp. 1097–1105.\n[17] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in Computer Vision and\nPattern Recognition, 2009. CVPR 2009. IEEE Conference on.\nIEEE,\n2009, pp. 248–255.\n[18] R. Collobert and J. Weston, “A uniﬁed architecture for natural language\nprocessing: Deep neural networks with multitask learning,” in Proceed-\nings of the 25th international conference on Machine learning.\nACM,\n2008, pp. 160–167.\n[19] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard, and\nD. McClosky, “The Stanford CoreNLP natural language processing\ntoolkit,” in ACL (System Demonstrations), 2014, pp. 55–60.\n[20] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\nP. Kuksa, “Natural language processing (almost) from scratch,” Journal\nof Machine Learning Research, vol. 12, no. Aug, pp. 2493–2537, 2011.\n[21] M. Plauth, L. Feinbube, and A. Polze, “A performance evaluation of\nlightweight approaches to virtualization,” Cloud Computing 2017, p. 14,\n2017.\n[22] T. Kämäräinen, Y. Shan, M. Siekkinen, and A. Ylä-Jääski, “Virtual\nmachines vs. containers in cloud gaming systems,” in Proceedings of\nthe 2015 International Workshop on Network and Systems Support for\nGames.\nIEEE Press, 2015, p. 1.\n[23] D. Beserra, E. D. Moreno, P. T. Endo, and J. Barreto, “Performance\nevaluation of a lightweight virtualization solution for HPC I/O scenar-\nios,” in Systems, Man, and Cybernetics (SMC), 2016 IEEE International\nConference on.\nIEEE, 2016, pp. 004 681–004 686.\n[24] M. T. Chung, N. Quang-Hung, M.-T. Nguyen, and N. Thoai, “Using\ndocker in high performance computing applications,” in Communica-\ntions and Electronics (ICCE), 2016 IEEE Sixth International Conference\non.\nIEEE, 2016, pp. 52–57.\n[25] N. Haydel, S. Gesing, I. Taylor, G. Madey, A. Dakkak, S. G. De Gon-\nzalo, and W.-M. W. Hwu, “Enhancing the usability and utilization of\naccelerated architectures via docker,” in Utility and Cloud Computing\n(UCC), 2015 IEEE/ACM 8th International Conference on. IEEE, 2015,\npp. 361–367.\n[26] J. J. Dongarra and P. Luszczek, “Introduction to the HPC challenge\nbenchmark suite,” DTIC Document, Tech. Rep., 2004.\n[27] J. Dongarra and M. A. Heroux, “Toward a new metric for ranking high\nperformance computing systems,” Sandia Report, SAND2013-4744, vol.\n312, 2013.\n[28] J. Dongarra, M. A. Heroux, and P. Luszczek, “HPGC benchmark: A new\nmetric for ranking high performance computing systems,” Knoxville,\nTennessee, 2015.\n[29] V. Sze, Y.-H. Chen, T.-J. Yang, and J. Emer, “Efﬁcient process-\ning of deep neural networks: A tutorial and survey,” arXiv preprint\narXiv:1703.09039, 2017.\n[30] V. Podlozhnyuk, “Histogram calculation in CUDA,” NVIDIA Corpora-\ntion, White Paper, 2007.\n[31] X. Mei, K. Zhao, C. Liu, and X. Chu, “Benchmarking the memory\nhierarchy of modern GPUs,” in Network and Parallel Computing.\nSpringer, 2014, pp. 144–156.\n[32] X. Mei and X. Chu, “Dissecting GPU memory hierarchy through\nmicrobenchmarking,” IEEE Transactions on Parallel and Distributed\nSystems, vol. 28, no. 1, pp. 72–86, 2017.\n[33] G. Ruetsch and P. Micikevicius, “Optimizing matrix transpose in\nCUDA,” Nvidia CUDA SDK Application Note, vol. 18, 2009.\n[34] S. Shi, P. Xu, and X. Chu, “Improving the performance of fully\nconnected neural networks by out-of-place matrix transpose,” arXiv\npreprint arXiv:1702.03192, 2017.\n[35] Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker,\nH. Drucker, I. Guyon, U. Muller, E. Sackinger et al., “Comparison of\nlearning algorithms for handwritten digit recognition,” in International\nconference on artiﬁcial neural networks, vol. 60. Perth, Australia, 1995,\npp. 53–60.\n[36] Y. LeCun, C. Cortes, and C. J. Burges, “The MNIST database of\nhandwritten digits,” 1998.\n[37] “Cifar-10 and cifar-100 datasets.” [Online]. Available: https://www.cs.\ntoronto.edu/~kriz/cifar.html\n[38] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network\nregularization,” arXiv preprint arXiv:1409.2329, 2014.\n",
  "categories": [
    "cs.DC",
    "cs.LG",
    "cs.PF"
  ],
  "published": "2017-11-09",
  "updated": "2017-11-09"
}