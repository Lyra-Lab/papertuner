{
  "id": "http://arxiv.org/abs/1711.03705v1",
  "title": "Online Deep Learning: Learning Deep Neural Networks on the Fly",
  "authors": [
    "Doyen Sahoo",
    "Quang Pham",
    "Jing Lu",
    "Steven C. H. Hoi"
  ],
  "abstract": "Deep Neural Networks (DNNs) are typically trained by backpropagation in a\nbatch learning setting, which requires the entire training data to be made\navailable prior to the learning task. This is not scalable for many real-world\nscenarios where new data arrives sequentially in a stream form. We aim to\naddress an open challenge of \"Online Deep Learning\" (ODL) for learning DNNs on\nthe fly in an online setting. Unlike traditional online learning that often\noptimizes some convex objective function with respect to a shallow model (e.g.,\na linear/kernel-based hypothesis), ODL is significantly more challenging since\nthe optimization of the DNN objective function is non-convex, and regular\nbackpropagation does not work well in practice, especially for online learning\nsettings. In this paper, we present a new online deep learning framework that\nattempts to tackle the challenges by learning DNN models of adaptive depth from\na sequence of training data in an online learning setting. In particular, we\npropose a novel Hedge Backpropagation (HBP) method for online updating the\nparameters of DNN effectively, and validate the efficacy of our method on\nlarge-scale data sets, including both stationary and concept drifting\nscenarios.",
  "text": "Online Deep Learning: Learning Deep Neural Networks on the Fly\nDoyen Sahoo, Quang Pham, Jing Lu, Steven C.H. Hoi\nSchool of Information Systems, Singapore Management Univeristy\n{doyens,hqpham,jing.lu.2014,chhoi}@smu.edu.sg\nAbstract\nDeep Neural Networks (DNNs) are typically trained by back-\npropagation in a batch learning setting, which requires the\nentire training data to be made available prior to the learn-\ning task. This is not scalable for many real-world scenarios\nwhere new data arrives sequentially in a stream form. We\naim to address an open challenge of “Online Deep Learn-\ning” (ODL) for learning DNNs on the ﬂy in an online setting.\nUnlike traditional online learning that often optimizes some\nconvex objective function with respect to a shallow model\n(e.g., a linear/kernel-based hypothesis), ODL is signiﬁcantly\nmore challenging since the optimization of the DNN ob-\njective function is non-convex, and regular backpropagation\ndoes not work well in practice, especially for online learning\nsettings. In this paper, we present a new online deep learning\nframework that attempts to tackle the challenges by learning\nDNN models of adaptive depth from a sequence of training\ndata in an online learning setting. In particular, we propose a\nnovel Hedge Backpropagation (HBP) method for online up-\ndating the parameters of DNN effectively, and validate the ef-\nﬁcacy of our method on large-scale data sets, including both\nstationary and concept drifting scenarios.\nIntroduction\nRecent years have witnessed tremendous success of deep\nlearning techniques in a wide range of applications (LeCun,\nBengio, and Hinton 2015; Bengio, Courville, and Vincent\n2013; Bengio, Goodfellow, and Courville 2015; Krizhevsky,\nSutskever, and Hinton 2012; He et al. 2016). Learning Deep\nNeural Networks (DNN) faces many challenges, including\n(but not limited to) vanishing gradient, diminishing fea-\nture reuse (Srivastava, Greff, and Schmidhuber 2015), sad-\ndle points (and local minima) (Choromanska et al. 2015;\nDauphin et al. 2014), immense number of parameters to\nbe tuned, internal covariate shift during training (Ioffe and\nSzegedy 2015), difﬁculties in choosing a good regularizer,\nchoosing hyperparameters, etc. Despite many promising ad-\nvances (Nair and Hinton 2010; Ioffe and Szegedy 2015;\nHe et al. 2016; Srivastava, Greff, and Schmidhuber 2015),\netc., which are designed to address speciﬁc problems for op-\ntimizing deep neural networks, most of these existing ap-\nproaches assume that the DNN models are trained in a batch\nlearning setting which requires the entire training data set to\nbe made available prior to the learning task. This is not pos-\nsible for many real world tasks where data arrives sequen-\ntially in a stream, and may be too large to be stored in mem-\nory. Moreover, the data may exhibit concept drift (Gama et\nal. 2014). Thus, a more desired option is to learn the models\nin an online setting.\nUnlike batch learning, online learning (Zinkevich 2003;\nCesa-Bianchi and Lugosi 2006) represents a class of learn-\ning algorithms that learn to optimize predictive models over\na stream of data instances sequentially. The on-the-ﬂy learn-\ning makes online learning highly scalable and memory ef-\nﬁcient. However, most existing online learning algorithms\nare designed to learn shallow models (e.g., linear or ker-\nnel methods (Crammer et al. 2006; Kivinen, Smola, and\nWilliamson 2004; Hoi et al. 2013)) with online convex op-\ntimization, which cannot learn complex nonlinear functions\nin complicated application scenarios.\nIn this work, we attempt to bridge the gap between online\nlearning and deep learning by addressing the open problem\nof “Online Deep Learning” (ODL) — how to learn Deep\nNeural Networks (DNNs) from data streams in an online\nsetting. A possible way to do ODL is to put the process of\ntraining DNNs online by directly applying a standard Back-\npropagation training on only a single instance at each online\nround. Such an approach is simple but falls short due to some\ncritical limitations in practice. One key challenge is how to\nchoose a proper model capacity (e.g., depth of the network)\nbefore starting to learn the DNN online. If the model is\ntoo complex (e.g., very deep networks), the learning process\nwill converge too slowly (vanishing gradient and diminish-\ning feature reuse), thus losing the desired property of online\nlearning. On the other extreme, if the model is too simple,\nthe learning capacity will be too restricted, and without the\npower of depth, it would be difﬁcult to learn complex pat-\nterns. In batch learning literature, a common way to address\nthis issue is to do model selection on validation data. Unfor-\ntunately, it is not realistic to have validation data in online\nsettings, and is thus infeasible to apply traditional model se-\nlection in online learning scenarios. In this work, we present\na novel framework for online deep learning, which is able to\nlearn DNN models from data streams sequentially, and more\nimportantly, is able to adapt its model capacity from simple\nto complex over time, nicely combining the merits of both\nonline learning and deep learning.\nWe aim to devise an online learning algorithm that is able\nto start with a shallow network that enjoys fast convergence;\nthen gradually switch to a deeper model (meanwhile shar-\ning certain knowledge with the shallow ones) automatically\nwhen more data has been received to learn more complex\nhypotheses, and effectively improve online predictive per-\nformance by adapting the capacity of DNNs. To achieve this,\nwe need to address questions such as: when to change the\ncapacity of network? how to change the capacity of the net-\nwork? and how to do both in an online setting? We design\nan elegant solution to do all this in a uniﬁed framework in\na data-driven manner. We ﬁrst amend the existing DNN ar-\nchitecture by attaching every hidden layer representation to\nan output classiﬁer. Then, instead of using a standard Back-\npropagation, we propose a novel Hedge Backpropagation\nmethod, which evaluates the online performance of every\noutput classiﬁer at each online round, and extends the Back-\npropagation algorithm to train the DNNs online by exploit-\ning the classiﬁers of different depths with the Hedge algo-\nrithm (Freund and Schapire 1997). This allows us to train\nDNNs of adaptive capacity meanwhile enabling knowledge\nsharing between shallow and deep networks.\nRelated Work\nOnline Learning\nOnline Learning represents a family of scalable and efﬁcient\nalgorithms that learn to update models from data streams se-\nquentially (Cesa-Bianchi and Lugosi 2006; Shalev-Shwartz\n2007; Hoi, Wang, and Zhao 2014; Wu et al. 2017). Many\ntechniques are based on maximum-margin classiﬁcation,\nfrom Perceptron (Rosenblatt 1958) to Online Gradient De-\nscent (Zinkevich 2003), Passive Aggressive (PA) algorithms\n(Crammer et al. 2006), Conﬁdence-Weighted (CW) Algo-\nrithms, (Dredze, Crammer, and Pereira 2008) etc. These are\nprimarily designed to learn linear models. Online Learning\nwith kernels (Kivinen, Smola, and Williamson 2004) offered\na solution for online learning with nonlinear models. These\nmethods received substantial interest from the community,\nand models of higher capacity such as Online Multiple Ker-\nnel Learning were developed (Jin, Hoi, and Yang 2010;\nHoi et al. 2013; Sahoo, Hoi, and Li 2014; Lu et al. 2015b;\nSahoo, Hoi, and Zhao 2016). While these models learn non-\nlinearity, they are still shallow. Moreover, deciding the num-\nber and type of kernels is non-trivial; and these methods are\nnot explicitly designed to learn a feature representation.\nOnline Learning can be directly applied to DNNs (”online\nbackpropagation”) but they suffer from convergence issues,\nsuch as vanishing gradient and diminishing feature reuse.\nMoreover, the optimal depth to be used for the network is\nusually unknown, and cannot be validated easily in the on-\nline setting. Further, networks of different depth would be\nsuitable for different number of instances to be processed,\ne.g., for small number of instances - a quick convergence\nwould be of high priority, and thus shallow networks would\nbe preferred, whereas, for a large number of instances, the\nlong run performance would be enhanced by using a deeper\nnetwork. This makes model architecture selection very chal-\nlenging. There have been attempts at making deep learning\ncompatible with online learning (Zhou, Sohn, and Lee 2012;\nLee et al. 2016) and (Lee et al. 2017). However, they operate\nvia a sliding window approach with a (mini)batch training\nstage, making them unsuitable for a streaming data setting.\nDeep Learning\nDue to the difﬁculty in training deep networks, there has\nbeen a large body of emerging works adopting the princi-\nple of ”shallow to deep”. This is similar to the principle we\nadopt in our work. This approach exploits the intuition that\nshallow models converge faster than deeper models, and this\nidea has been executed in several ways. Some approaches\ndo this explicitly by Growing of Networks via the function\npreservation principle (Chen, Goodfellow, and Shlens 2016;\nWei et al. 2016), where the (student) network of higher ca-\npacity is guaranteed to be at least as good as the shallower\n(teacher) network. Other approaches perform this more im-\nplicitly by modifying the network architecture and objective\nfunctions to enable the network to allow the input to ﬂow\nthrough the network, and slowly adapt to deep represen-\ntation learning, e.g., Highway Nets(Srivastava, Greff, and\nSchmidhuber 2015), Residual Nets(He et al. 2016), Stochas-\ntic Depth Networks (Huang et al. 2016) and Fractal Nets\n(Larsson, Maire, and Shakhnarovich 2016).\nHowever, they are all designed to optimize the loss func-\ntion based on the output obtained from the deepest layer.\nDespite the improved batch convergence, they cannot yield\ngood online performances (particularly for the instances ob-\nserved in early part of the stream), as the inference made by\nthe deepest layer requires substantial time for convergence.\nFor the online setting, such existing deep learning tech-\nniques could be trivially beaten by a very shallow network.\nDeeply Supervised Nets (Lee et al. 2015), shares a similar\narchitecture as ours, which uses companion objectives at ev-\nery layer to address vanishing gradient and to learn more dis-\ncriminative features at shallow layers. However, the weights\nof companions are set heuristically, where the primary goal\nis to optimize the classiﬁcation performance based on fea-\ntures learnt by the deepest hidden layer, making it suitable\nonly for batch settings, which suffers from the same draw-\nbacks as others.\nRecent years have also witnessed efforts in learning the\narchitecture of the neural networks (Srinivas and Babu 2015;\nAlvarez and Salzmann 2016), which incorporate architec-\nture hyperparameters into the optimization objective. Start-\ning from an overcomplete network, they use regularizers that\nhelp in eliminating neurons from the network. For example,\n(Alvarez and Salzmann 2016) use a group sparsity regular-\nization to reduce the width of the network. (Zoph and Le\n2016) use reinforcement learning to search for the optimal\narchitecture. Our proposed technique is related to these in\nthe sense that we use an overcomplete network, and auto-\nmatically adapt the effective depth of the network to learn\nan appropriate capacity network based on the data. Unlike\nother model selection techniques which work only in the\nbatch learning setting, our method is designed for the online\nlearning setting.\nOnline Deep Learning\nProblem Setting\nWithout loss of generality, consider an online classiﬁcation\ntask. The goal of online deep learning is to learn a func-\ntion F : Rd →RC based on a sequence of training ex-\namples D = {(x1, y1), . . . , (xT , yT )}, that arrive sequen-\ntially, where xt ∈Rd is a d-dimensional instance repre-\nsenting the features and yt ∈{0, 1}C is the class label as-\nsigned to xt and C is the number of classes. The prediction\nis denoted by ˆyt, and the performance of the learnt functions\nare usually evaluated based on the cumulative prediction er-\nror: ϵT =\n1\nT\nPT\nt=1 I( ˆ\nyt̸=yt), where I is the indicator func-\ntion resulting in 1 if the condition is true, and 0 otherwise.\nTo minimize the classiﬁcation error over the sequence of T\ninstances, a loss function (e.g., squared loss, cross-entropy,\netc.) is often chosen for minimization. In every online itera-\ntion, when an instance xt is observed, and the model makes\na prediction, the environment then reveals the ground truth\nof the class label, and ﬁnally the learner makes an update to\nthe model (e.g., using online gradient descent).\nBackpropagation: Preliminaries and Limitations\nFor typical online learning algorithms, the prediction func-\ntion F is either a linear or kernel-based model. In the case\nof Deep Neural Networks (DNN), it is a set of stacked lin-\near transformations, each followed by a nonlinear activation.\nGiven an input x ∈Rd, the prediction function of DNN with\nL hidden layers (h(1), . . . , h(L)) is recursively given by:\nF(x)\n= softmax(W (L+1)h(L))\nwhere\nh(l)\n= σ(W (l)h(l−1))\n∀l = 1, . . . , L;\nh(0) = x\nwhere σ is an activation function, e.g., sigmoid, tanh,\nReLU, etc. This equation represents a feedforward step of\na neural network. The hidden layers h(l) are the feature rep-\nresentations learnt during the training procedure. To train a\nmodel with such a conﬁguration, we use the cross-entropy\nloss function denoted by L(F(x), y). We aim to estimate\nthe optimal model parameters Wi for i = 1, . . . (L + 1) by\napplying Online Gradient Descent (OGD) on this loss func-\ntion. Following the online learning setting, the update of the\nmodel in each iteration by OGD is given by:\nW (l)\nt+1 ←W (l)\nt\n−η∇W (l)\nt L(F(xt), yt)\n∀l = 1, . . . , L + 1\nwhere η is the learning rate. Using backpropagation, the\nchain rule of differentiation is applied to compute the gra-\ndient of the loss with respect to W (l) for l ≤L.\nUnfortunately, using such a model for an online learn-\ning (i.e. Online Backpropagation) task faces several issues\nwith convergence. Most notably: (i) For such models, a ﬁxed\ndepth of the neural network has to be decided a priori, and\nthis cannot be changed during the training process. This\nis problematic, as determining the depth is a difﬁcult task.\nMoreover, in an online setting, different depths may be suit-\nable for a different number of instances to be processed, e.g.\nbecause of convergence reasons, shallow networks maybe\npreferred for small number of instances, and deeper net-\nworks for large number of instances. Our aim is to exploit\nthe fast convergence of shallow networks at the initial stages,\nand the power of deep representation at a later stage; (ii) van-\nishing gradient is well noted problem that slows down learn-\ning. This is even more critical in an online setting, where\nthe model needs to make predictions and learn simultane-\nously; (iii) diminishing feature reuse, according to which\nmany useful features are lost in the feedforward stage of the\nprediction. This is very critical for online learning, where it\nis imperative to quickly ﬁnd the important features, to avoid\npoor performance for the initial training instances.\nTo address these issues, we design a training scheme for\nOnline Deep Learning through a Hedging strategy: Hedge\nBackpropagation (HBP). Speciﬁcally, HBP uses an over-\ncomplete network, and automatically decides how and when\nto adapt the depth of the network in an online manner.\n𝒉𝟎\n𝒉𝟏\n𝒉𝟐\n𝒉𝑳\n𝑭𝒕\nt\n𝒇𝟏\n𝒇𝟐\n𝒇𝑳\n𝒚𝑡\n𝒇𝟎\nhedge\n𝜶(𝟎)\n𝜶(𝟏)\n𝜶(𝟐)\n𝜶(𝑳)\n𝒚𝑡\n𝒚𝑡\n𝒚𝑡\n𝒙𝒕\nhedge\nhedge\nhedge\nFigure 1: Online Deep Learning framework using Hedge\nBackpropagation (HBP). Blue lines represent feedforward\nﬂow for computing hidden layer features. Orange lines in-\ndicate softmax output followed by the hedging combination\nat prediction time. Green lines indicate the online updating\nﬂows with the hedge backpropagation approach.\nHedge Backpropagation (HBP)\nFigure 1 illustrates the online deep learning framework, for\ntraining DNNs using Hedge Backpropagation.\nConsider a deep neural network with L hidden layers (i.e.\nthe maximum capacity of the network that can be learnt is\none with L hidden layers), the prediction function for the\nproposed Hedged Deep Neural Network is given by:\nF(x)\n=\nL\nX\nl=0\nα(l)f (l)\nwhere\n(1)\nf (l)\n=\nsoftmax(h(l)Θ(l)), ∀l = 0, . . . , L\nh(l)\n=\nσ(W (l)h(l−1)), ∀l = 1, . . . , L\nh(0)\n=\nx\nHere we have designed a new architecture, and introduced\ntwo sets of new parameters Θ(l) (parameters for f (l)) and α,\nthat have to be learnt. Unlike the original network, in which\nthe ﬁnal prediction is given by a classiﬁer using the feature\nrepresentation h(L), here the prediction is weighted combi-\nnation of classiﬁers learnt using the feature representations\nfrom h(0), . . . , h(L). Each classiﬁer f (l) is parameterized by\nΘ(l). Note that there are a total of L+1 classiﬁers. The ﬁnal\nprediction of this model is a weighted combination of the\npredictions of all classiﬁers, where the weight of each clas-\nsiﬁer is denoted by α(l), and the loss suffered by the model\nis L(F(x), y) = PL\nl=0 α(l)L(f (l)(x), y). During the online\nlearning procedure, we need to learn α(l), Θ(l) and W (l).\nWe propose to learn α(l) using the Hedge Algorithm (Fre-\nund and Schapire 1997). At the ﬁrst iteration, all weights α\nare uniformly distributed, i.e., α(l) =\n1\nL+1, l = 0, . . . , L. At\nevery iteration, the classiﬁer f (l) makes a prediction ˆyt\n(l).\nWhen the ground truth is revealed, the classiﬁer’s weight is\nupdated based on the loss suffered by the classiﬁer as:\nα(l)\nt+1 ←α(l)\nt βL(f (l)(x),y)\nwhere β ∈(0, 1) is the discount rate parameter, and\nL(f (l)(x), y) ∈(0, 1) (Freund and Schapire 1997). Thus,\na classiﬁer’s weight is discounted by a factor of βL(f (l)(x),y)\nin every iteration. At the end of every round, the weights α\nare normalized such that P\nl α(l)\nt\n= 1.\nLearning the parameters Θ(l) for all the classiﬁers can be\ndone via online gradient descent (Zinkevich 2003), where\nthe input to the lth classiﬁer is h(l). This is similar to the up-\ndate of the weights of the output layer in the original feed-\nforward networks. This update is given by:\nΘ(l)\nt+1\n←\nΘ(l)\nt\n−η∇Θ(l)\nt L(F(xt, yt))\n(2)\n=\nΘ(l)\nt\n−ηα(l)∇Θ(l)\nt L(f (l), yt)\nUpdating the feature representation parameters W (l) is\nmore tricky. Unlike the original backpropagation scheme,\nwhere the error derivatives are backpropagated from the out-\nput layer, here, the error derivatives are backpropagated from\nevery classiﬁer f (l). Thus, using the adaptive loss function\nL(F(x), y) = PL\nl=0 α(l)L(f (l)(x), y) and applying OGD\nrule, the update rule for W (l) is given by:\nW (l)\nt+1 ←W (l)\nt\n−η\nL\nX\nj=l\nα(j)∇W (l)L(f (j), yt)\n(3)\nwhere ∇W (l)L(f (j), yt) is computed via backpropagation\nfrom error derivatives of f (j). Note that the summation (in\nthe gradient term) starts at j = l because the shallower clas-\nsiﬁers do not depend on W (l) for making predictions. In ef-\nfect, we are computing the gradient of the ﬁnal prediction\nwith respect to the backpropagated derivatives of a predic-\ntor at every depth weighted by α(l) (which is an indicator of\nthe performance of the classiﬁer). Hedge enjoys a regret of\nRT ≤\n√\nT ln N, where N is the number of experts (Freund\nand Schapire 1999), which in our case is the network depth.\nThis gives an effective model selection approach to adapt to\nthe optimal network depth automatically online.\nBased on the intuition that shallower models tend to con-\nverge faster than deeper models (Chen, Goodfellow, and\nShlens 2016; Larsson, Maire, and Shakhnarovich 2016;\nGulcehre et al. 2016), using a hedging strategy would lower\nα weights of deeper classiﬁers to a very small value (due\nto poor initial performance as compared to shallower clas-\nsiﬁers), which would affect the update in Eq. (3), and re-\nsult in deeper classiﬁers having slow learning. To alleviate\nthis, we introduce a smoothing parameter s ∈(0, 1) which\nis used to set a minimum weight for each classiﬁer. After the\nweight update of the classiﬁers in each iteration, the weights\nare set as: α(l) ←max\n\u0010\nα(l), s\nL\n\u0011\nThis maintains a min-\nimum weight for a classiﬁer of every depth and helps us\nachieve a tradeoff between exploration and exploitation. s\nencourages all classiﬁers at every depth to affect the back-\nprop update (exploring high capacity deep classiﬁers, and\nenabling deep classiﬁers to perform as good as shallow\nones), while hedging the model exploits the best perform-\ning classiﬁer. Similar strategies have been used in Multi-\narm bandit setting, and online learning with expert advice\nto trade off exploration and exploitation (Auer et al. 2002;\nHoi et al. 2013). Algorithm 1 outlines ODL using HBP.\nAlgorithm 1 Online Deep Learning (ODL) using HBP\nINPUTS: Discounting Parameter: β ∈(0, 1);\nLearning rate Parameter: η; Smoothing Parameter: s\nInitialize: F(x) = DNN with L hidden layers and L + 1\nclassiﬁers f (l), ∀l = 0, . . . , L; α(l) =\n1\nL+1, ∀l = 0, . . . , L\nfor t = 1,...,T do\nReceive instance: xt\nPredict ˆyt = Ft(xt) = PL\nl=0 α(l)\nt f (l)\nt\nas per Eq. (1)\nReveal true value yt\nSet L(l)\nt\n= L(f (l)\nt (xt), yt), ∀l, . . . , L;\nUpdate Θ(l)\nt+1, ∀l = 0, . . . , L as per Eq. (2);\nUpdate W (l)\nt+1, ∀l = 1, . . . , L as per Eq. (3);\nUpdate α(l)\nt+1 = α(l)\nt βL(l)\nt , ∀l = 0, . . . , L;\nSmoothing α(l)\nt+1 = max(α(l)\nt+1, s\nL), ∀l = 0, . . . , L ;\nNormalize α(l)\nt+1 =\nα(l)\nt+1\nZt\nwhere Zt =\nL\nP\nl=0\nα(l)\nt+1\nend for\nDiscussion\nHBP has the following properties: (i) it identiﬁes a neu-\nral network of an appropriate depth based on the perfor-\nmance of the classiﬁer at that depth. This is a form of On-\nline Learning with Expert Advice,(Cesa-Bianchi and Lu-\ngosi 2006), where the experts are DNNs of varying depth,\nmaking the DNN robust to depth. (ii) it offers a good ini-\ntialization for deeper networks, which are encouraged to\nmatch the performance of shallower networks (if unable to\nbeat them). This facilitates knowledge transfer from shallow\nto deeper networks ((Chen, Goodfellow, and Shlens 2016;\nWei et al. 2016)), thus simulating student-teacher learning;\n(iii) it makes the learning robust to vanishing gradient and\ndiminishing feature reuse by using a multi-depth architec-\nture where gradients are backpropagated from shallower\nclassiﬁers, and the low level features are used for the ﬁ-\nnal prediction (by hedge weighted prediction); (iv) it can\nbe viewed as an ensemble of multi-depth networks which\nare competing and collaborating to improve the ﬁnal predic-\ntion performance. The competition is induced by Hedging ,\nwhereas the collaboration is induced by sharing feature rep-\nresentations; (v) This allows our algorithms to continuously\nlearn and adapt as and when it sees more data, enabling\na form of life-long learning (Lee et al. 2016); (vi) In con-\ncept drifting scenarios (Gama et al. 2014), traditional online\nbackpropagation would suffer from slow convergence for\ndeep networks (when the concepts would change), whereas,\nHBP is able to adapt quickly due to hedging; and (vii) While\nHBP could be trivially adapted to Convolutional Networks\nfor computer vision tasks, these tasks typically have many\nclasses with few instances per class, which makes it hard to\nobtain good results in just one-pass through the data (on-\nline setting). Thus our focus is on pure online settings where\na large number of instances arrive in a stream and exhibit\ncomplex nonlinear patterns.\nExperiments\nDatasets\nWe consider several large scale datasets. Higgs and Susy are\nPhysics datasets from UCI repository. For Higgs, we sam-\npled 5 million instances. We generated 5 million instances\nfrom Inﬁnite MNIST generator (Loosli, Canu, and Bottou\n2007). We also evaluated on 3 synthetic datasets. The ﬁrst\n(Syn8) is generated from a randomly initialized DNN com-\nprising 8-hidden layers (of width 100 each). The other two\nare concept drift datasets CD1 and CD2. In CD1, 2 concepts\n(C1 and C2), appear in the form C1-C2-C1, with each seg-\nment comprising a third of the data stream. Both C1 and C2\nwere generated from a 8-hidden layer network. CD2 has 3\nconcepts with C1-C2-C3, where C1 and C3 are generated\nfrom a 8-hidden layer network, and C2 from a shallower 6-\nhidden layer network. Other details are in Table 1.\nTable 1: Datasets\nData\n#Features\n#Instances\nType\nHiggs\n28\n5m\nStationary\nSusy\n18\n5m\nStationary\ni-mnist\n784\n5m\nStationary\nSyn8\n50\n5m\nStationary\nCD1\n50\n4.5m\nConcept Drift\nCD2\n50\n4.5m\nConcept Drift\nLimitations of traditional Online BP: Difﬁculty in\nDepth Selection prior to Training\nFirst we demonstrate the difﬁculty of DNN model selection\nin the online setting. We compare the error rate of DNNs of\nvarying depth, in different segments of the data. All mod-\nels were trained online, and we evaluate their performance\nin different windows (or stages) of the learning process. See\nTable 2. In the ﬁrst 0.5% of the data, the shallowest network\nobtains the best performance indicating faster convergence\n- which would indicate that we should use the shallow net-\nwork for the task. In the segment from [10-15]%, a 4-layer\nDNN seems to have the best performance in most cases. And\nin the segment from [60-80]% of the data, an 8-layer net-\nwork gives a better performance. This indicates that deeper\nnetworks took a longer time to converge, but at a later stage\ngave a better performance. Looking at the ﬁnal error, it does\nnot give us conclusive evidence of what depth of network\nwould be the most suitable. Furthermore, if the datastream\nhad more instances, a deeper network may have given an\noverall better performance. This demonstrates the difﬁculty\nin model selection for learning DNNs online, where typical\nvalidation techniques are ineffective. Ideally we want to ex-\nploit the convergence of the shallow DNNs in the beginning\nand the power of deeper representations later.\nBaselines\nWe aim to learn a 20 layer DNN in the online setting, with\n100 units in each hidden layer. As baselines, we learn the 20\nlayer network online using OGD (Online Backpropagation),\nOGD Momentum, OGD Nesterov, and Highway Networks.\nSince a 20 layer network would be very difﬁcult to learn\nin the online setting, we also compare the performance of\nshallower networks — DNNs with fewer layers (2,3,4,8,16),\ntrained using Online BP. We used ReLU Activation, and a\nﬁxed learning rate of 0.01 (chosen, as it gave the best ﬁ-\nnal error rate for all DNN-based baselines). For momentum\ntechniques, a ﬁxed learning rate of 0.001 was used, and we\nﬁnetuned the momentum parameter to obtain the best perfor-\nmance for the baselines. For HBP, we attached a classiﬁer to\neach of the 19 hidden layers (and not directly to the input).\nThis gave 19 classiﬁers each with depth from 2, . . . , 20. We\nset β = 0.99 and the smoothing parameter s = 0.2. Im-\nplementation was in Keras (Chollet 2015). We also com-\npared with representative state of the art linear online learn-\ning algorithms (OGD, Adaptive Regularization of Weights\n(AROW), Soft-Conﬁdence Weighted Learning (SCW) (Hoi,\nWang, and Zhao 2014)) and kernel online learning algo-\nrithms (Fourier Online Gradient Descent (FOGD) and Nys-\ntrom Online Gradient Descent (NOGD)(Lu et al. 2015a)) .\nEvaluation of Online Deep Learning Algorithms\nThe ﬁnal cumulative error obtained by all the baselines\nand the proposed HBP technique can be seen in Table 3.\nFirst, traditional online learning algorithms (linear and ker-\nnel) have relatively poor performance on complex datasets.\nNext, in learning with a 20-layer network, the convergence\nis slow, resulting in poor overall performance. While second\norder methods utilizing momentum and highway networks\nare able to offer some advantage over simple Online Gradi-\nent Descent, they can be easily beaten by a relatively shal-\nlower networks in the online setting. We observed before\nthat relatively shallower networks could give a competitive\nperformance in the online setting, but lacked the ability to\nexploit the power of depth at a later stage. In contrast, HBP\nenjoyed the best of both worlds, by allowing for faster con-\nvergence initially, and making use of the power of depth at\na later stage. This way HBP was able to do automatic model\nselection online, enjoying merits of both shallow and deep\nnetworks, and this resulted in HBP outperforming all the\nDNNs of different depths, in terms of online performance.\nTable 2: Online error rate of DNNs of varying Depth. All models were trained online t = 1, . . . , T. We evaluate the performance\nin different segments of the data. L is the number of layers in the DNN.\nFinal Cumulative Error\nSegment [0-0.5]% Error\nSegment [10-15]% Error\nSegment [60-80]% Error\nL\nHiggs\nSusy\nSyn8\nHiggs\nSusy\nSyn8\nHiggs\nSusy\nSyn8\nHiggs\nSusy\nSyn8\n3\n0.2724\n0.2016\n0.3936\n0.3584\n0.2152\n0.4269\n0.2797\n0.2029\n0.4002\n0.2668\n0.2004\n0.3901\n4\n0.2688\n0.2014\n0.3920\n0.3721\n0.2197\n0.4339\n0.2775\n0.2030\n0.3989\n0.2617\n0.2004\n0.3876\n8\n0.2682\n0.2016\n0.3936\n0.3808\n0.2218\n0.4522\n0.2794\n0.2036\n0.4018\n0.2613\n0.1997\n0.3888\n16\n0.2731\n0.2037\n0.4025\n0.4550\n0.2312\n0.4721\n0.2831\n0.2050\n0.4121\n0.2642\n0.2027\n0.3923\nIt should be noted that the optimal depth for DNN is not\nknown before the learning process, and even then HBP out-\nperforms all DNNs at any depth.\nIn Figure 2, we can see the convergence behavior of all the\nalgorithms on the stationary as well as concept drift datasets.\nIn the stationary datasets, HBP shows consistent outperfor-\nmance over all the baselines. The only exception is in the\nvery initial stages of the online learning phase, where shal-\nlower baselines are able to outperform HBP. This is not sur-\nprising, as HBP has many more parameters to learn. How-\never, HBP is able to quickly outperform the shallow net-\nworks. The performance of HBP in concept drifting sce-\nnarios demonstrates its ability to adapt to the change fairly\nquickly, enabling usage of DNNs in the concept drifting sce-\nnarios. Looking at the performance of simple 20-layer (and\n16-layer) networks on concept drifting data, we can see dif-\nﬁculty in utilizing deep representation for such scenarios.\nAdapting the Effective Depth of the DNN\nIn this section we look at the weight distribution learnt by\nHBP over time. We analyse the mean weight distribution in\ndifferent segments of the Online Learning phase in Figure\n3. In the initial phase (ﬁrst 0.5%), the maximum weight has\ngone to the shallowest classiﬁer (with just one hidden layer).\nIn the second phase (10-15%), slightly deeper classiﬁers\n(classiﬁers with 4-5 layers) have picked up some weight,\nand in the third segment (60-80%), even deeper classiﬁers\nhave gotten more weight (classiﬁers with 5-7 layers). The\nshallow and the very deep classiﬁers receive little weight in\nthe last segment showing HBPs ability to perform model se-\nlection. Few classiﬁers having similar depth indicates that\nthe intermediate features learnt are themselves of discrimi-\nnatory nature, which are being used by the deeper classiﬁers\nto potentially learn better features.\nPerformance in Different Learning Stages\nWe evaluate HBP performance in different segments of the\ndata to see how the proposed HBP algorithm performed as\ncompared to the DNNs of different depth in different stages\nof learning. In Figure 4, we can see, HBP matches (and even\nbeats) the performance of the best depth network in both\nthe beginning and at a later stage of the training phase. This\nshows its ability to exploit faster convergence of shallow net-\nworks in the beginning, and power of deep representation\ntowards the end. Not only is it able to do automatic model\nselection, but also it is able to offer a good initialization for\nthe deeper representation, so that the depth of the network\ncan be exploited sooner, thus beating a DNN of every depth.\n2\n3\n4\n8\n16\n20 HBP\n0.25\n0.26\n0.27\n0.28\n0.29\n0.3\n(a) Error in 10-15% of\ndata\n2\n3\n4\n8\n16\n20 HBP\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n(b) Error in 60-80% of\ndata\nFigure 4: Error Rate in different segments of the Data. Red\nrepresents HBP using a 20-layer network. Blue are OGD\nusing DNN with layers = 2,3,4,8, 16 and 20.\nRobustness to Depth of Base-Network\nWe evaluate HBP performance with varying depth of the\nbase network. We consider 12, 16, 20, and a 30-layer DNNs\ntrained using HBP and compare their performance on Higgs\nagainst simple Online BP. See Table 4 for the results, where\nthe performance variation with depth does not signiﬁcantly\nalter HBPs performance, while for simple Online BP, signif-\nicant increase in depth hurts the learning process.\nTable 4: Robustness of HBP to depth of the base network\ncompared to traditional DNN\nDepth\n12\n16\n20\n30\nOnline BP\n0.2692\n0.2731\n0.2868\n0.4770\nHBP\n0.2609\n0.2613\n0.2615\n0.2620\nConclusion\nWe identiﬁed several issues which prevented existing DNNs\nfrom being used in an online setting, which meant that they\ncould not be used for streaming data, and necessarily re-\nquired storage of the entire data in memory. These issues\narose from difﬁculty in model selection (appropriate depth\nDNN), and convergence difﬁculties from vanishing gradi-\nent and diminishing feature reuse. We used the ”shallow\nto deep” principle, and designed Hedge Backpropagation,\nwhich enabled the usage of Deep Neural Networks in an on-\nline setting. HBP used a hedging strategy to make predic-\ntions with multiple outputs from different hidden layers of\nthe network, and the backpropagation algorithm was modi-\nﬁed to allow for knowledge sharing among the deeper and\nshallower networks. This approach automatically identiﬁed\nhow and when to modify the effective network capacity in a\ndata-drive manner, based on the observed data complexity.\nWe validated the proposed method through extensive exper-\niments on large datasets.\nTable 3: Final Online Cumulative Error Rate obtained by the algorithms. Best performance is in bold.\nModel\nMethod\nLayers\nHiggs\nSusy\ni-mnist\nSyn8\nCD1\nCD2\nLinear OL\nOGD\n1\n0.3620\n0.2160\n0.1230\n0.4070\n0.4360\n0.4270\nAROW\n1\n0.3630\n0.2160\n0.1250\n0.4050\n0.4340\n0.4250\nSCW\n1\n0.3530\n0.2140\n0.1230\n0.4050\n0.4340\n0.4250\nKernel OL\nFOGD\n2\n0.2973\n0.2021\n0.0495\n0.3962\n0.4329\n0.4193\nNOGD\n2\n0.3488\n0.2045\n0.1045\n0.4146\n0.4455\n0.4356\nDNNs\nOGD (Online BP)\n2\n0.2938\n0.2028\n0.0199\n0.3976\n0.4146\n0.3797\nOGD (Online BP)\n3\n0.2724\n0.2016\n0.0190\n0.3936\n0.4115\n0.3772\nOGD (Online BP)\n4\n0.2688\n0.2014\n0.0196\n0.3920\n0.4110\n0.3766\nOGD (Online BP)\n8\n0.2682\n0.2016\n0.0219\n0.3936\n0.4145\n0.3829\nOGD (Online BP)\n16\n0.2731\n0.2037\n0.0232\n0.4025\n0.4204\n0.3939\nOGD (Online BP)\n20\n0.2868\n0.2064\n0.0274\n0.4472\n0.4928\n0.4925\nOGD+Momentum\n20\n0.2711\n0.2012\n0.0310\n0.4062\n0.4312\n0.3897\nOGD+Nesterov\n20\n0.2711\n0.2076\n0.0247\n0.3942\n0.4191\n0.3917\nHighway\n20\n0.2736\n0.2019\n0.0241\n0.4313\n0.4928\n0.4925\nHedge BP (proposed)\n20\n0.2615\n0.2003\n0.0156\n0.3896\n0.4079\n0.3739\n0\n1\n2\n3\n4\n5\nx 10\n6\n0.26\n0.27\n0.28\n0.29\n0.3\n0.31\n0.32\n0.33\n0.34\n0.35\n#Instances\nError Rate\n \n \nDNN−2\nDNN−3\nDNN−4\nDNN−8\nDNN−16\nDNN−20\nMomentum\nNesterov\nHighway\nHBP\n(a) HIGGS\n0\n1\n2\n3\n4\n5\nx 10\n6\n0.2\n0.205\n0.21\n0.215\n0.22\n0.225\n0.23\n#Instances\nError Rate\n \n \nDNN−2\nDNN−3\nDNN−4\nDNN−8\nDNN−16\nDNN−20\nMomentum\nNesterov\nHighway\nHBP\n(b) SUSY\n0\n1\n2\n3\n4\n5\nx 10\n6\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n#Instances\nError Rate\n \n \nDNN−2\nDNN−3\nDNN−4\nDNN−8\nDNN−16\nDNN−20\nMomentum\nNesterov\nHighway\nHBP\n(c) Inf-MNIST (i-mnist)\n0\n1\n2\n3\n4\n5\nx 10\n6\n0.38\n0.39\n0.4\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n#Instances\nError Rate\n \n \nDNN−2\nDNN−3\nDNN−4\nDNN−8\nDNN−16\nDNN−20\nMomentum\nNesterov\nHighway\nHBP\n(d) SYN8\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nx 10\n6\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n#Instances\nError Rate\n \n \n← C1 →\n← C1 →\n← C2 →\nDNN−2\nDNN−3\nDNN−4\nDNN−8\nDNN−16\nDNN−20\nMomentum\nNesterov\nHighway\nHBP\n(e) Concept Drift 1 (CD1)\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nx 10\n6\n0.37\n0.38\n0.39\n0.4\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n#Instances\nError Rate\n \n \n← C2 →\n← C1 →\n← C3 →\nDNN−2\nDNN−3\nDNN−4\nDNN−8\nDNN−16\nDNN−20\nMomentum\nNesterov\nHighway\nHBP\n(f) Concept Drift 2 (CD2)\nFigure 2: Convergence behavior of DNNs in Online Setting on stationary and concept drifting data.\n0\n5\n10\n15\n20\n25\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nWeight\nDepth (Number of layers in Output Classifier)\n(a) First 0.5% of Data\n0\n5\n10\n15\n20\n25\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nWeight\nDepth (Number of layers in Output Classifier)\n(b) 10-15% of Data\n0\n5\n10\n15\n20\n25\n0\n0.05\n0.1\n0.15\n0.2\n0.25\nWeight\nDepth (Number of layers in Output Classifier)\n(c) 60-80% of Data\nFigure 3: Evolution of weight distribution of the classiﬁers over time using HBP on HIGGS dataset.\nReferences\n[Alvarez and Salzmann 2016] Alvarez, J. M., and Salzmann,\nM. 2016. Learning the number of neurons in deep networks.\nIn NIPS.\n[Auer et al. 2002] Auer, P.; Cesa-Bianchi, N.; Freund, Y.;\nand Schapire, R. E. 2002. The nonstochastic multiarmed\nbandit problem. SIAM Journal on Computing 32(1):48–77.\n[Bengio, Courville, and Vincent 2013] Bengio,\nY.;\nCourville, A.; and Vincent, P.\n2013.\nRepresentation\nlearning: A review and new perspectives.\nIEEE trans-\nactions on pattern analysis and machine intelligence\n35(8):1798–1828.\n[Bengio, Goodfellow, and Courville 2015] Bengio,\nY.;\nGoodfellow, I. J.; and Courville, A. 2015. Deep learning.\nAn MIT Press book in preparation. Draft chapters available\nat http://www. iro. umontreal. ca/ bengioy/dlbook.\n[Cesa-Bianchi and Lugosi 2006] Cesa-Bianchi, N., and Lu-\ngosi, G. 2006. Prediction, learning, and games. Cambridge\nUniversity Press.\n[Chen, Goodfellow, and Shlens 2016] Chen, T.; Goodfellow,\nI.; and Shlens, J. 2016. Net2net: Accelerating learning via\nknowledge transfer. ICLR.\n[Chollet 2015] Chollet, F.\n2015.\nKeras.\nhttps://\ngithub.com/fchollet/keras.\n[Choromanska et al. 2015] Choromanska, A.; Henaff, M.;\nMathieu, M.; Arous, G.; and LeCun, Y.\n2015.\nThe loss\nsurfaces of multilayer networks. In AISTATS.\n[Crammer et al. 2006] Crammer, K.; Dekel, O.; Keshet, J.;\nShalev-Shwartz, S.; and Singer, Y. 2006. Online passive-\naggressive algorithms. JMLR.\n[Dauphin et al. 2014] Dauphin, Y.; Pascanu, R.; Gulcehre,\nC.; Cho, K.; Ganguli, S.; and Bengio, Y. 2014. Identifying\nand attacking the saddle point problem in high-dimensional\nnon-convex optimization. In NIPS.\n[Dredze, Crammer, and Pereira 2008] Dredze,\nM.;\nCram-\nmer, K.; and Pereira, F. 2008. Conﬁdence-weighted linear\nclassiﬁcation. In ICML.\n[Freund and Schapire 1997] Freund, Y., and Schapire, R.\n1997. A decision-theoretic generalization of on-line learn-\ning and an application to boosting. Journal of computer and\nsystem sciences.\n[Freund and Schapire 1999] Freund, Y., and Schapire, R.\n1999. Adaptive game playing using multiplicative weights.\nGames and Economic Behavior.\n[Gama et al. 2014] Gama, J.; Zliobaite, I.; Bifet, A.; Pech-\nenizkiy, M.; and Bouchachia, A. 2014. A survey on concept\ndrift adaptation. ACM Computing Surveys (CSUR) 46(4):44.\n[Gulcehre et al. 2016] Gulcehre, C.; Moczulski, M.; Visin,\nF.; and Bengio, Y. 2016. Mollifying networks. preprint\narXiv:1608.04980.\n[He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.\nDeep residual learning for image recognition. CVPR.\n[Hoi et al. 2013] Hoi, S.; Jin, R.; Zhao, P.; and Yang, T. 2013.\nOnline multiple kernel classiﬁcation. Machine Learning.\n[Hoi, Wang, and Zhao 2014] Hoi, S.; Wang, J.; and Zhao, P.\n2014. Libol: A library for online learning algorithms. JMLR.\n[Huang et al. 2016] Huang, G.; Sun, Y.; Liu, Z.; Sedra, D.;\nand Weinberger, K. 2016. Deep networks with stochastic\ndepth. arXiv preprint arXiv:1603.09382.\n[Ioffe and Szegedy 2015] Ioffe, S., and Szegedy, C.\n2015.\nBatch normalization: Accelerating deep network train-\ning by reducing internal covariate shift.\narXiv preprint\narXiv:1502.03167.\n[Jin, Hoi, and Yang 2010] Jin, R.; Hoi, S.; and Yang, T.\n2010. Online multiple kernel learning: Algorithms and mis-\ntake bounds.\nIn Algorithmic Learning Theory. Springer\nBerlin Heidelberg.\n[Kivinen, Smola, and Williamson 2004] Kivinen, J.; Smola,\nA.; and Williamson, R. 2004. Online learning with kernels.\nIEEE TSP 52(8):2165–2176.\n[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,\nA.;\nSutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁca-\ntion with deep convolutional neural networks. In Advances\nin neural information processing systems, 1097–1105.\n[Larsson, Maire, and Shakhnarovich 2016] Larsson,\nG.;\nMaire, M.; and Shakhnarovich, G. 2016. Fractalnet: Ultra-\ndeep neural networks without residuals.\narXiv preprint\narXiv:1605.07648.\n[LeCun, Bengio, and Hinton 2015] LeCun, Y.; Bengio, Y.;\nand Hinton, G.\n2015.\nDeep learning.\nNature\n521(7553):436–444.\n[Lee et al. 2015] Lee, C.-Y.; Xie, S.; Gallagher, P.; Zhang, Z.;\nand Tu, Z.\n2015.\nDeeply-supervised nets.\nIn AISTATS,\nvolume 2, 6.\n[Lee et al. 2016] Lee, S.-W.; Lee, C.-Y.; Kwak, D.-H.; Kim,\nJ.; Kim, J.; and Zhang, B.-T.\n2016.\nDual-memory deep\nlearning architectures for lifelong learning of everyday hu-\nman behaviors. In IJCAI, 1669–1675.\n[Lee et al. 2017] Lee, J.; Yun, J.; Hwang, S.; and Yang, E.\n2017. Lifelong learning with dynamically expandable net-\nworks. arXiv preprint arXiv:1708.01547.\n[Loosli, Canu, and Bottou 2007] Loosli, G.; Canu, S.; and\nBottou, L. 2007. Training invariant support vector machines\nusing selective sampling. Large scale kernel machines 301–\n320.\n[Lu et al. 2015a] Lu, J.; Hoi, S.; Wang, J.; Zhao, P.; and Liu,\nZ.\n2015a.\nLarge scale online kernel learning.\nJ. Mach.\nLearn. Res.\n[Lu et al. 2015b] Lu, J.; Hoi, S. C.; Sahoo, D.; and Zhao,\nP. 2015b. Budget online multiple kernel learning. arXiv\npreprint arXiv:1511.04813.\n[Nair and Hinton 2010] Nair, V., and Hinton, G. E.\n2010.\nRectiﬁed linear units improve restricted boltzmann ma-\nchines. In Proceedings of the 27th International Conference\non Machine Learning (ICML-10), 807–814.\n[Rosenblatt 1958] Rosenblatt, F. 1958. The perceptron: a\nprobabilistic model for information storage and organization\nin the brain. Psychological review 65(6):386.\n[Sahoo, Hoi, and Li 2014] Sahoo, D.; Hoi, S.; and Li, B.\n2014. Online multiple kernel regression. In ACM SIGKDD.\n[Sahoo, Hoi, and Zhao 2016] Sahoo, D.; Hoi, S.; and Zhao,\nP. 2016. Cost sensitive online multiple kernel classiﬁcation.\nIn Asian Conference on Machine Learning, 65–80.\n[Shalev-Shwartz 2007] Shalev-Shwartz, S.\n2007.\nOnline\nlearning: Theory, algorithms, and applications.\n[Srinivas and Babu 2015] Srinivas, S., and Babu, R. V. 2015.\nLearning neural network architectures using backpropaga-\ntion. arXiv preprint arXiv:1511.05497.\n[Srivastava, Greff, and Schmidhuber 2015] Srivastava,\nR. K.; Greff, K.; and Schmidhuber, J.\n2015.\nTraining\nvery deep networks.\nIn Advances in neural information\nprocessing systems, 2377–2385.\n[Wei et al. 2016] Wei, T.; Wang, C.; Rui, R.; and Chen, C. W.\n2016. Network morphism. ICML.\n[Wu et al. 2017] Wu, Y.; Hoi, S. C.; Liu, C.; Lu, J.; Sahoo,\nD.; and Yu, N.\n2017.\nSol: A library for scalable online\nlearning algorithms. Neurocomputing.\n[Zhou, Sohn, and Lee 2012] Zhou, G.; Sohn, K.; and Lee, H.\n2012. Online incremental feature learning with denoising\nautoencoders. AISTATS 1001:48109.\n[Zinkevich 2003] Zinkevich, M. 2003. Online convex pro-\ngramming and generalized inﬁnitesimal gradient ascent.\n[Zoph and Le 2016] Zoph, B., and Le, Q. V.\n2016.\nNeu-\nral architecture search with reinforcement learning. arXiv\npreprint arXiv:1611.01578.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2017-11-10",
  "updated": "2017-11-10"
}