{
  "id": "http://arxiv.org/abs/1711.05890v2",
  "title": "Occlusion Aware Unsupervised Learning of Optical Flow",
  "authors": [
    "Yang Wang",
    "Yi Yang",
    "Zhenheng Yang",
    "Liang Zhao",
    "Peng Wang",
    "Wei Xu"
  ],
  "abstract": "It has been recently shown that a convolutional neural network can learn\noptical flow estimation with unsupervised learning. However, the performance of\nthe unsupervised methods still has a relatively large gap compared to its\nsupervised counterpart. Occlusion and large motion are some of the major\nfactors that limit the current unsupervised learning of optical flow methods.\nIn this work we introduce a new method which models occlusion explicitly and a\nnew warping way that facilitates the learning of large motion. Our method shows\npromising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets.\nEspecially on KITTI dataset where abundant unlabeled samples exist, our\nunsupervised method outperforms its counterpart trained with supervised\nlearning.",
  "text": "Occlusion Aware Unsupervised Learning of Optical Flow\nYang Wang1\nYi Yang1\nZhenheng Yang2\nLiang Zhao1\nPeng Wang1\nWei Xu1,3\n1Baidu Research\n2 University of Southern California\n3National Engineering Laboratory for Deep Learning Technology and Applications\n{wangyang59, yangyi05, zhaoliang07, wangpeng54, wei.xu}@baidu.com zhenheny@usc.edu\nAbstract\nIt has been recently shown that a convolutional neural\nnetwork can learn optical ﬂow estimation with unsuper-\nvised learning. However, the performance of the unsuper-\nvised methods still has a relatively large gap compared to\nits supervised counterpart. Occlusion and large motion are\nsome of the major factors that limit the current unsuper-\nvised learning of optical ﬂow methods. In this work we\nintroduce a new method which models occlusion explicitly\nand a new warping way that facilitates the learning of large\nmotion.\nOur method shows promising results on Flying\nChairs, MPI-Sintel and KITTI benchmark datasets. Espe-\ncially on KITTI dataset where abundant unlabeled samples\nexist, our unsupervised method outperforms its counterpart\ntrained with supervised learning.\n1. Introduction\nVideo motion prediction, or namely optical ﬂow, is a fun-\ndamental problem in computer vision. With the accurate\noptical ﬂow prediction, one could estimate the 3D structure\nof a scene [18], segment moving objects based on motion\ncues [38], track objects in a complicated environment [11],\nand build important visual cues for many high level vision\ntasks such as video action recognition [45] and video object\ndetection [60].\nTraditionally, optical ﬂow is formulated as a variational\noptimization problem with the goal of ﬁnding pixel cor-\nrespondences between two consecutive video frames [23].\nWith the recent development of deep convolutional neural\nnetworks (CNNs) [32], deep learning based methods have\nbeen adopted to learn optical ﬂow estimation, where the\nnetworks are either trained to compute discriminative im-\nage features for patch matching [21] or directly output the\ndense ﬂow ﬁelds in an end-to-end manner [16]. One major\nadvantage of the deep learning based methods compared to\nclassical energy-based methods is the computational speed,\nwhere most state-of-the-art energy-based methods require\n1-50 minutes to process a pair of images, while deep nets\nonly need less than 100 milliseconds with a modern GPU.\nSince most deep networks are built to predict ﬂow using\ntwo consecutive frames and trained with supervised learn-\ning [26], it would require a large amount of training data\nto obtain reasonably high accuracy [35].\nUnfortunately,\nmost large-scale ﬂow datasets are from synthetic movies\nand ground-truth motion labels in real world videos are gen-\nerally hard to annotate [29]. To overcome this problem, un-\nsupervised learning framework is proposed to utilize the re-\nsources of unlabeled videos [30]. The overall strategy be-\nhind those unsupervised methods is that instead of directly\ntraining the neural nets with ground-truth ﬂow, they use a\nphotometric loss that measures the difference between the\ntarget image and the (inversely) warped subsequent image\nbased on the dense ﬂow ﬁeld predicted from the fully con-\nvolutional networks. This allows the networks to be trained\nend-to-end with a large amount of unlabeled image pairs,\novercoming the limitation from the lack of ground-truth\nﬂow annotations.\nHowever, the performance of the unsupervised methods\nstill has a relatively large gap compared to their supervised\ncounterparts [41]. To further improve unsupervised ﬂow\nestimation, we realize that occlusion and large motion are\namong the major factors that limit the current unsupervised\nlearning methods. In this paper, we propose a new end-to-\nend deep neural architecture that carefully addresses these\nissues.\nMore speciﬁcally, the original baseline networks esti-\nmate motion and attempt to reconstruct every pixel in the\ntarget image. During reconstruction, there will be a fraction\nof pixels in the target image that have no source pixels due\nto occlusion. If we do not address this issue, it could limit\nthe optical ﬂow estimation accuracy since the loss function\nwould prefer to compensate the occluded regions by moving\nother pixels. For example, in Fig. 1, we would like to esti-\nmate the optical ﬂow from frame 1 to frame 2, and recon-\nstruct frame 1 by warping frame 2 with the estimated ﬂow.\nLet us focus on the chair in the bottom left corner of the\nimage. It moves in the down-left direction, and some part\nof the background is occluded by it. When we warp frame\n1\narXiv:1711.05890v2  [cs.CV]  4 Apr 2018\nFigure 1: (a) Input frame 1. (b) Input frame 2. (c) Ground-truth optical ﬂow. (d) Image warped by ground-truth optical ﬂow.\n(e) Forward optical ﬂow estimated by our method. (f) Image warped by our forward optical ﬂow. (g) Backward optical ﬂow\nestimated by our method. (h) Occlusion map for input frame 1 estimated by our backward optical ﬂow. (i) Optical ﬂow from\n[41]. (j) Image warped by [41].\n2 back to frame 1 using the ground-truth ﬂow (Fig. 1c), the\nresulting image (Fig. 1d) has two chairs in it. The chair on\nthe top-right is the real chair, while the chair on the bottom-\nleft is due to the occluded part of the background. Because\nthe ground-truth ﬂow of the background is zero, the chair\nin frame 2 is carried back to frame 1 to ﬁll in the occluded\nbackground. Therefore, frame 2 warped by the ground-truth\noptical ﬂow does not fully reconstruct frame 1. From the\nother perspective, if we use photometric loss of the entire\nimage to guide the unsupervised learning of optical ﬂow,\nthe occluded area would not get the correct ﬂow, which is\nillustrated in Fig. 1i. It has an extra chair in the ﬂow trying\nto ﬁll the occluded background with nearby pixels of simi-\nlar appearance, and the corresponding warped image Fig. 1j\nhas only one chair in it.\nTo address this issue, we explicitly allow the network\nto exploit the occlusion prediction caused by motion and\nincorporate it into the loss function. More concretely, we\nestimate the backward optical ﬂow (Fig. 1g) and use it to\ngenerate the occlusion map for the warped frame (Fig. 1h).\nThe white area in the occlusion map denotes the area in\nframe 1 that does not have a correspondence in frame 2.\nWe train the network to only reconstruct the non-occluded\narea and do not penalize differences in the occluded area,\nso that the image warped by our estimated forward optical\nﬂow (Fig. 1e) can have two chairs in it (Fig. 1f) without\nincurring extra loss for the network.\nOur work differs from previous unsupervised learning\nmethods in four aspects. 1) We proposed a new end-to-end\nneural network that handles occlusion. 2) We developed a\nnew warping method that can facilitate unsupervised learn-\ning of large motion. 3) We further improved the previous\nFlowNetS by introducing extra warped inputs during the de-\ncoder phase. 4) We introduced histogram equalization and\nchannel representation that are useful for optical ﬂow esti-\nmation. The last three components are created to mainly\ntackle the issue of large motion estimation.\nAs a result, our method signiﬁcantly improves the un-\nsupervised learning based optical ﬂow estimation on multi-\nple benchmark dataset including Flying Chairs, MPI-Sintel\nand KITTI. Our unsupervised networks even outperforms\nits supervised counterpart [16] on KITTI benchmark, where\nlabeled data is limited compared to unlabeled data.\n2. Related Work\nOptical ﬂow has been intensively studied in the past few\ndecades [23, 34, 10, 49, 37]. Due to page limitation, we will\nbrieﬂy review the classical approaches and the recent deep\nlearning approaches.\nOptical ﬂow estimation. Optical ﬂow estimation was\nintroduced as a fundamental computer vision problem since\nthe pioneering works [23, 34].\nStarting from then, the\naccuracy of optical ﬂow estimation has been improving\nsteadily as evidenced by the results on Middlebury [8] and\nMPI-Sintel [14] benchmark dataset. Most classical opti-\ncal ﬂow algorithms belong to the variants of the energy\nminimization problem with the brightness constancy and\nspatial smoothness assumptions [12, 42]. Other trends in-\nclude a coarse-to-ﬁne estimation or a hierarchical frame-\nwork to deal with large motion [13, 55, 15, 6], a design of\nloss penalty to improve the robustness to lighting change\nand motion blur [59, 46, 22, 54], and a more sophisticated\nframework to handle occlusion [2, 50] which we will de-\nscribe in more details in the next subsection.\nOcclusion-aware optical ﬂow estimation.\nSince oc-\nclusion is a consequence of depth and motion, it is in-\nevitable to model occlusion in order to accurately estimate\nﬂow. Most existing methods jointly estimate optical ﬂow\nand occlusion. Based on the methodology, we divide them\ninto three major groups. The ﬁrst group treats occlusion\nas outliers and predict target pixels in the occluded regions\nas a constant value or through interpolation [47, 3, 4, 52].\nThe second group deals with occlusion by exploiting the\nsymmetric property of optical ﬂow and ignoring the loss\npenalty on predicted occluded regions [51, 2, 25]. The last\ngroup builds more sophisticated frameworks such as mod-\neling depth or a layered representation of objects to reason\nabout occlusion [50, 48, 58, 43]. Our model is similar to\nthe second group, such that we do not take account the dif-\nference where the occlusion happens into the loss function.\nTo the best of our knowledge, we are the ﬁrst to incorporate\nsuch kind of method with a neural network in an end-to-end\ntrainable fashion. This helps our model to obtain more ro-\nbust ﬂow estimation around the occlusion boundary [27, 9].\nDeep learning for optical ﬂow. The success of deep\nlearning innovates new optical ﬂow models. [21] uses deep\nnets to extract discriminative features to compute optical\nﬂow through patch matching. [5] further extends the patch\nmatching based methods by adding additional semantic in-\nformation. Later, [7] proposes a robust thresholded hinge\nloss for Siamese networks to learn CNN-based patch match-\ning features. [56] accelerates the processing of patch match-\ning cost volume and obtains optical ﬂow results with high\naccuracy and fast speed.\nMeanwhile, [16, 26] propose FlowNet to directly com-\npute dense ﬂow prediction on every pixel through fully con-\nvolutional neural networks and train the networks with end-\nto-end supervised learning.\n[40] demonstrates that with\na spatial pyramid network predicting in a coarse-to-ﬁne\nfashion, a simple and small network can work quite ac-\ncurately and efﬁciently on ﬂow estimation.\nLater, [24]\nproposes a method for jointly estimating optical ﬂow and\ntemporally consistent semantic segmentation with CNN.\nThe deep learning based methods obtain competitive ac-\ncuracy across many benchmark optical ﬂow datasets in-\ncluding MPI-Sintel [56] and KITTI [26] with a relatively\nfaster computational speed. However, the supervised learn-\ning framework limits the extensibility of these works due\nto the lack of ground-truth ﬂow annotation in other video\ndatasets.\nUnsupervised learning for optical ﬂow. [39] ﬁrst intro-\nduces an end-to-end differentiable neural architecture that\nallows unsupervised learning for video motion prediction\nand reports preliminary results on a weakly-supervised se-\nmantic segmentation task. Later, [30, 41, 1] adopt a sim-\nilar unsupervised learning architecture with a more de-\ntailed performance study on multiple optical ﬂow bench-\nmark datasets. A common philosophy behind these meth-\nods is that instead of directly supervising with ground-truth\nFigure 2: Our network architecture. It contains two copies\nof FlowNetS[16] with shared parameters which estimates\nforward and backward optical ﬂow respectively. The for-\nward warping module generates an occlusion map from the\nbackward ﬂow. The backward warping module generates\nthe warped image that is used to compare against the orig-\ninal frame 1 over the non-occluded area. There is also a\nsmoothness term applied to the forward optical ﬂow.\nﬂow, these methods utilize the Spatial Transformer Net-\nworks [28] to warp the current images to produce a target\nimage prediction and use photometric loss to guide back-\npropagation [17]. The whole framework can be further ex-\ntended to estimate the depth, camera motion and optical\nﬂow simultaneously in an end-to-end manner [53]. This\novercomes the ﬂow annotation problem, but the ﬂow esti-\nmation accuracy in previous works still lags behind the su-\npervised learning methods. In this paper, we show that un-\nsupervised learning can obtain competitive results to super-\nvised learning models. After the initial submission of this\npaper, we became aware of a concurrent work [36] which\ntries to solve the occlusion problem in unsupervised optical\nﬂow learning with a symmetric-based approach.\n3. Network Structure and Method\nWe ﬁrst give an overview of our network structure and\nthen describe each of its components in details.\nOverall structure. The schematic structure of our neu-\nral network is depicted in Fig. 2. Our network contains\ntwo copies of FlowNetS with shared parameters. The up-\nper FlowNetS takes two stacked images (I1 and I2) as input\nand outputs the forward optical ﬂow (F12) from I1 to I2.\nThe lower FlowNetS takes the reverse stacked images (I2\nand I1) as input and outputs the backward ﬂow (F21) from\nI2 to I1.\nThe forward ﬂow F12 is used to warp I2 to reconstruct\neI1 through a Spatial Transformer Network similar to [30].\nWe call this backward warping, since the warping direction\nis different from the ﬂow direction. The backward ﬂow F21\nis used to generate the occlusion map (O) by forward warp-\ning. The occlusion map indicates the region in I1 that is\ncorrespondingly occluded in I2 (i.e. region in I1 that does\nFigure 3:\nIllustration of the forward warping module\ndemonstrating how the occlusion map is generated using\nthe backward optical ﬂow. Here we only have horizontal\ncomponent optical ﬂow F x\n12 and F x\n21 where 1 denotes mov-\ning right, -1 denote moving left and 0 denotes stationary. In\nthe occlusion map, 0 denotes occluded and 1 denotes non-\noccluded.\nnot have a correspondence in I2).\nThe loss for training our network contains two parts: a\nphotometric term (Lp) and a smoothness term (Ls). For the\nphotometric term, we compare the warped image eI1 and the\noriginal target image I1 in the non-occluded region to ob-\ntain the photometric loss Lp. Note that this is a key differ-\nence between our method and previous unsupervised learn-\ning methods. We also add a smoothness loss Ls applied to\nF12 to encourage a smooth ﬂow solution.\nForward warping and occlusion map. We model the\nnon-occluded region in I1 as the range of F21 [2], which\ncan be calculated with the following equation,\nV (x, y) =\nW\nX\ni=1\nH\nX\nj=1\nmax (0, 1 −|x −(i + F x\n21(i, j))|)\n· max (0, 1 −|y −(j + F y\n21(i, j))|)\nwhere V (x, y) is the range map value at location (x, y).\n(W, H) are the image width and height, and (F x\n21, F y\n21) are\nthe horizontal and vertical components of F21.\nSince F21 is continuous, the location of a pixel after\nbeing translated by a ﬂoating number might not be ex-\nactly on an image grid.\nWe use reversed bilinear sam-\npling to distribute the weight of the translated pixel to its\nnearest neighbors. The occlusion map O can be obtained\nby simply thresholding the range map V at the value of\n1 and results in a soft map with value between 0 and 1.\nO(x, y) = min(1, V (x, y)). The whole forward warping\nmodule is differentiable and can be trained end-to-end with\nthe rest of the network.\nFigure 4: Illustration of the backward warping module with\nan enlarged search space. The large green box on the right\nside is a zoom view of the small green box on the left side.\nIn order to better illustrate the forward warping module,\nwe provide a toy example in Fig. 3. I1 and I2 have only\n4 pixels each, in which different letters represent different\npixel values. The ﬂow and reversed ﬂow only have horizon-\ntal component which we show as F x\n12 and F x\n21. The motion\nfrom I1 to I2 is that pixel A moves to the position of B\nand covers it, while pixel E in the background appears in\nI2. To calculate the occlusion map, we ﬁrst create an image\nﬁlled with ones and then translate them according to F21.\nTherefore, the one at the top-right corner is translated to the\ntop-left corner leaving the top-right corner at the value of\nzero. The top-right corner (B) of I1 is occluded by pixel\nA and can not ﬁnd its corresponding pixel in I2 which is\nconsistent with the formulation we discussed above.\nBackward warping with a larger search space. The\nbackward warping module is used to reconstruct eI1 from I2\nwith forward optical ﬂow F12. The method adopted here\nis similar to [30, 41] except that we include a larger search\nspace. The problem with the original warping method is\nthat the warped pixel value only depends on its four near-\nest neighbors, so if the target position is far away from the\nproposed position, the network will not get meaningful gra-\ndient signals. For example in Fig. 4, a particular pixel lands\nin the position of (x2, y2) proposed by the estimated opti-\ncal ﬂow, and its value is a weighted sum of its four nearest\nneighbors. However, if the true optical ﬂow land the pixel\nat (ˆx, ˆy), the network would not learn the correct gradient\ndirection, and thus stuck at a local minimum. This prob-\nlem is particularly severe in the case of large motion. Al-\nthough one could use a multi-scale image pyramid to tackle\nthe large motion problem, if the moving object is small or\nhas a similar color to the background, the motion might not\nbe visible in small scale images.\nMore concretely, when we use the estimated optical ﬂow\nF12 to warp I2 back to reconstruct eI1 at a grid point (x1, y1),\nwe ﬁrst translate the grid point (x1, y1) in I1 (the yellow\nsquare) to (x2, y2) = (x1 + F x\n12(x1, y1), y1 + F y\n12(x1, y1))\nin I2. Because the point (x2, y2) is not on the grid point\nin I2, we need to do bilinear sampling to obtain its value.\nNormally, the value at (x2, y2) is a weighted sum of its four\nnearest neighbors (black dots in the zoomed view on the\nright side of Fig. 4). We instead ﬁrst search an enlarged\nneighbor (e.g. the blue dots at the outer circle in Fig. 4\ntogether with the four nearest neightbors) around the point\n(x2, y2). For instance, if in the enlarged neighbor of point\n(x2, y2), the point that has the closest value to the target\nvalue I1(x1, y1) is (ˆx, ˆy), we assign the value at the point\n(x2, y2) to be a weighted sum of values at (ˆx, ˆy) and three\nother symmetrical points (points labeled with red crosses in\nFig. 4) with respect to point (x2, y2). By doing this, we can\nprovide the neural network with gradient pointing towards\nthe location of (ˆx, ˆy).\nLoss term. The loss of our network contains two com-\nponents: a photometric loss (Lp) and a smoothness loss\n(Ls). We compute the photometric loss using the Char-\nbonnier penalty formula Ψ(s) =\n√\ns2 + 0.0012 over the\nnon-occluded regions with both image brightness and im-\nage gradient.\nL1\np =\n\u0002 X\ni,j\nΨ(eI1(i, j) −I1(i, j)) · O(i, j)\n\u0003\n/\n\u0002 X\ni,j\nO(i, j)\n\u0003\nL2\np =\n\u0002 X\ni,j\nΨ(∇eI1(i, j) −∇I1(i, j)) · O(i, j)\n\u0003\n/\n\u0002 X\ni,j\nO(i, j)\n\u0003\nwhere O is the occlusion map deﬁned in the above section,\nand i, j together indexes over pixel coordinates. The loss\nis normalized by the total non-occluded area size to prevent\ntrivial solutions.\nFor the smoothness loss, we adopt an edge-aware formu-\nlation similar to [20], because motion boundaries usually\ncoincide with image boundaries. Since the occluded area\ndoes not have a photometric loss, the optical ﬂow estima-\ntion in the occluded area is solely guided by the smoothness\nloss. By using an edge-aware smoothness penalty, the opti-\ncal ﬂow in the occluded area would be similar to the values\nin its neighbor that has the closest appearance. We use both\nﬁrst-order and second-order derivatives of the optical ﬂow\nin the smoothness loss term.\nL1\ns =\nX\ni,j\nX\nd∈x,y\nΨ\n\u0010\n|∂dF12(i, j)|e−α|∂dI1(i,j)|\u0011\nL2\ns =\nX\ni,j\nX\nd∈x,y\nΨ\n\u0010\n|∂2\ndF12(i, j)|e−α|∂dI1(i,j)|\u0011\nwhere α controls the weight of edges, and d indexes over\npartial derivative on x and y directions. The ﬁnal loss is a\nweighted sum of the above four terms,\nL = γ1L1\np + γ2L2\np + γ3L1\ns + γ4L2\ns\nFlow network details.\nOur inner ﬂow network is\nadopted from FlowNetS [16]. Same as FlowNetS, we use\nFigure 5: Our modiﬁcation to the FlowNetS structure at one\nof the decoding stage. On the left, we show the original\nFlowNetS structure. On the right, we show our modiﬁca-\ntion of the FlowNetS structure. conv6 and conv5 1 are fea-\ntures extracted in the encoding phase and named after [16].\nImage1 6 and Image2 6 are input images downsampled 64\ntimes. The decoding stages at other scales are modiﬁed ac-\ncordingly.\na multi-scale scheme to guide the unsupervised learning\nby down-sampling images to different smaller scales. The\nonly modiﬁcation we made to the FlowNetS structure is that\nfrom coarser to ﬁner scale during the reﬁnement phase, we\nadd the image warped by the coarser optical ﬂow estima-\ntion and its corresponding photometric error map as extra\ninputs to estimate the ﬁner scale optical ﬂow in a fashion\nsimilar to FlowNet2 [26]. By doing this, each layer only\nneeds to estimate the residual between the coarse and ﬁne\nscale. The detailed network structure can be found in Fig. 5.\nOur modiﬁcation only increases the number of parameters\nby 2% compared to the original FlownetS, and it moderately\nimproves the result as seen in the later ablation study.\nPreprocessing. In order to have better contrast for mov-\ning objects in the down-sampled images, we preprocess the\nimage pairs by applying histogram equalization and aug-\nment the RGB image with a channel representation. The\ndetailed channel representation can be found in [44]. We\nﬁnd both preprocessing steps improve the ﬁnal optical ﬂow\nestimation results.\n4. Experimental Results\nWe evaluate our methods on standard optical ﬂow bench-\nmark datasets including Flying Chairs [16], MPI-Sintel [14]\nand KITTI [19], and compare our results to existing deep\nlearning based optical ﬂow estimation (both supervised and\nunsupervised methods). We use the standard endpoint error\n(EPE) measure as the evaluation metric, which is the aver-\nage Euclidean distance between the predicted ﬂow and the\nground truth ﬂow over all pixels.\nMethods\nChairs\nSintel Clean\nSintel Final\nKITTI 2012\nKITTI 2015\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\nSupervise\nFlowNetS [16]\n2.71\n4.50\n7.42\n5.45\n8.43\n8.26\n–\n–\n–\nFlowNetS+ft [16]\n–\n(3.66)\n6.96\n(4.44)\n7.76\n7.52\n9.1\n–\n–\nSpyNet [40]\n2.63\n4.12\n6.69\n5.57\n8.43\n9.12\n–\n–\n–\nSpyNet+ft [40]\n–\n(3.17)\n6.64\n(4.32)\n8.36\n8.25\n10.1\n–\n–\nFlowNet2 [26]\n–\n2.02\n3.96\n3.14\n6.02\n4.09\n–\n10.06\n–\nFlowNet2+ft [26]\n–\n(1.45)\n4.16\n(2.01)\n5.74\n(1.28)\n1.8\n(2.3)\n11.48%\nUnsupervise\nDSTFlow [41]\n5.11\n6.93\n10.40\n7.82\n11.11\n16.98\n–\n24.30\n–\nDSTFlow-best [41]\n5.11\n(6.16)\n10.41\n(6.81)\n11.27\n10.43\n12.4\n16.79\n39%\nBackToBasic [30]\n5.3\n–\n–\n–\n–\n11.3\n9.9\n–\n–\nOurs\n3.30\n5.23\n8.02\n6.34\n9.08\n12.95\n–\n21.30\n–\nOurs+ft-Sintel\n3.76\n(4.03)\n7.95\n(5.95)\n9.15\n12.9\n–\n22.6\n–\nOurs-KITTI\n–\n7.41\n–\n7.92\n–\n3.55\n4.2\n8.88\n31.2%\nTable 1: Quantitative evaluation of our method on different benchmarks. The numbers reported here are all average end-\npoint-error (EPE) except for the last column (KITTI2015 test) which is the percentage of erroneous pixels (Fl-all). A pixel\nis considered to be correctly estimated if the ﬂow end-point error is <3px or <5%. The upper part of the table contains\nsupervised methods and lower part of the table contains unsupervised methods. For all metrics, smaller is better. The best\nnumber for each category is highlighted in bold. The numbers in parentheses are results from network trained on the same\nset of data, and hence are not directly comparable to other results.\n4.1. Implementation Details\nOur network is trained end-to-end using Adam opti-\nmizer [31] with β1 = 0.9 and β2 = 0.999. The learning\nrate is set to be 10−4 for training from scratch and 10−5 for\nﬁne-tuning. The experiments are performed on two Titan\nZ GPUs with a batch size of 8 or 16 depending on the in-\nput image resolution. The training converges after roughly\na day.\nDuring training, we ﬁrst assign equal weights to\nloss from different image scales and then progressively in-\ncrease the weight on the larger scale image in a way similar\nto [35]. The hyper-parameters (γ1, γ2, γ3, γ4, α) are set to\nbe (1.0, 1.0, 10.0, 0.0, 10.0) for Flying Chairs and MPI-\nSintel datasets, and (0.03, 3.0, 0.0, 10.0, 10.0) for KITTI\ndataset.Here we used higher weights of image gradient pho-\ntometric loss and second-order smoothness loss for KITTI\nbecause the data has more lightning changes and its opti-\ncal ﬂow has more continuously varying intrinsic structure.\nIn terms of data augmentaion, we only used horizontal ﬂip-\nping, vertical ﬂipping and image pair order switching. Dur-\ning testing, our network only predicts forward ﬂow, the total\ncomputational time on a Flying Chairs image pair is roughly\n90 milliseconds with our Titian Z GPUs. Adding an ex-\ntra 8 milliseconds for histogram equalization (an OpenCV\nCPU implementation), the total prediction time is around\n100 milliseconds.\n4.2. Quantitative and Qualitative Results\nTable 1 summarizes the EPE of our method and pre-\nvious state-of-the-art deep learning methods, including\nFlowNet [16], SpyNet [40], FlowNet2 [26], DSTFlow [41]\nand BackToBasic [30]. Because DSTFlow reported mul-\ntiple variations of their results, we cite their best number\nacross all of their results in ”DSTFlow-best” here.\nFlying Chairs. Flying Chairs is a synthetic dataset cre-\nated by superimposing images of chairs on background im-\nages from Flickr.\nIt was originally created for training\nFlowNet in a supervised manner [16]. We use it to train our\nnetwork without using any ground-truth ﬂow. We randomly\nsplit the dataset into 95% training and 5% testing. We label\nthis model as ”Ours” in Table 1. Our EPE is signiﬁcantly\nsmaller than the previous unsupervised methods (i.e. EPE\ndecreases from 5.11 to 3.30) and is approaching the level of\nits corresponding supervised learning result (2.71).\nMPI-Sintel. Since MPI-Sintel is relatively small and\nonly contains around a thousand image pairs, we use\nthe training data from both clean and ﬁnal pass (without\nground-truth) to ﬁne-tune our network pretrained on Fly-\ning Chairs and the resulting model is labeled as ”Ours+ft-\nSintel”.\nCompared to other unsupervised methods, we\nachieve a much better performance (e.g., EPE decreases\nfrom 10.40 to 7.95 on Sintel Clean test). Note that ﬁne-\ntuning did not improve much here, largely due to the small\nnumber of training data. Fig.6 illustrates the qualitative re-\nsult of our method on MPI-Sintel.\nKITTI. The KITTI dataset is recorded under real-world\ndriving conditions, and it has more unlabeled data than la-\nbeled data. Unsupervised learning methods would have an\nadvantage in this scenario since they can learn from the\nlarge amount of unlabeled data. The training data we use\nhere is similar to [41] which consists of multi-view exten-\nFigure 6: Qualitative examples for Sintel dataset. The top three rows are from Sintel Clean and the bottom three rows are\nfrom Sintel Final.\nFigure 7: Qualitative examples for KITTI dataset. The top three rows are from KITTI 2012 and the bottom three rows are\nfrom KITTI 2015.\nsions (20 frames for each sequence) from both KITTI2012\nand KITTI2015. During training, we exclude two neigh-\nboring frames from the image pairs with ground-truth ﬂow\nand testing pairs to avoid mixing training and testing data\n(i.e. not including frame number 9-12 in each multi-view\nsequence). We train the model from scratch since the opti-\ncal ﬂow in KITTI dataset has its own domain spatial struc-\nture (different from Flying Chairs) and abundant data. We\nlabel this model as ”Ours-KITTI” in Table 1.\nTable 1 suggests that our method not only signiﬁcantly\noutperforms existing unsupervised learning methods (i.e.\nimproves EPE from 9.9 to 4.2 on KITTI 2012 test), but\nalso outperforms its supervised counterpart (FlowNetS+ft)\nby a large margin, although there is still a gap compared\nto the state-of-the-art supervised network FlowNet2. Fig. 7\nillustrates the qualitative results on KITTI. Our model cor-\nrectly captures the occluded area caused by moving out of\nthe frame. Our ﬂow results are also free from the artifacts\nseen in DSTFlow (see [41] Figure 4c) in the occlusion area.\nOcclusion Estimation. We also evaluate our occlusion\nestimation on MPI-Sintel and KITTI dataset which pro-\nvide ground-truth occlusion labels between two consecutive\nocclusion\nenlarged\nmodiﬁed\ncontrast\nChairs\nSintel Clean\nSintel Final\nhandling\nsearch\nFlowNet\nenhancement\ntest\ntrain\ntrain\n5.11\n6.93\n7.82\n✓\n4.51\n6.80\n7.32\n✓\n✓\n4.27\n6.49\n7.11\n✓\n✓\n✓\n4.14\n6.38\n7.08\n✓\n4.62\n6.60\n7.33\n✓\n✓\n4.04\n6.09\n7.04\n✓\n✓\n✓\n3.76\n5.70\n6.54\n✓\n✓\n✓\n✓\n3.30\n5.23\n6.34\nTable 2: Ablation study\nMethod\nSintel\nSintel\nKITTI\nKITTI\nClean\nFinal\n2012\n2015\nOur\n0.54\n0.48\n0.95\n0.88\nS2D [33]\n–\n0.57\n–\n–\nMODOF [57]\n–\n0.48\n–\n–\nTable 3: Occlusion estimation evaluation. The numbers we\npresent here is maximum F-measure. The S2D method is\ntrained with ground-truth occlusion labels.\nframes. Among the literatures, we only ﬁnd limited reports\non occlusion estimation accuracy. Table 3 shows the occlu-\nsion estimation performance by calculating the maximum\nF-measure introduced in [33]. On MPI-Sintel, our method\nhas a comparable result with previous non-neural-network\nbased methods [33, 57].\nOn KITTI we obtain 0.95 and\n0.88 for KITTI2012 and KITTI2015 respectively (we did\nnot ﬁnd published occlusion estimation result on KITTI).\nNote that S2D used ground-truth occlusion maps to do su-\npervised training of their occlusion model.\n4.3. Ablation Study\nWe conduct systematic ablation analysis on different\ncomponents added in our method. Table 2 shows the over-\nall effects of them on Flying Chairs and MPI-Sintel. Our\nstarting network is a FlowNetS without occlusion handling,\nwhich is the same conﬁguration as [41].\nOcclusion handling. The top two rows in Table 2 sug-\ngest that by only adding occlusion handling to the baseline\nnetwork, the model improves its EPE from 5.11 to 4.51 on\nFlying-Chairs and from 7.82 to 7.32 on MPI-Sintel Final,\nwhich is signiﬁcant.\nEnlarged search. The effect of enlarged search is also\nsigniﬁcant.\nThe bottom two rows in Table 2 show that\nadding enlarged search, the ﬁnal EPE improves from 3.76 to\n3.30 on Flying-Chairs and from 6.54 to 6.34 on MPI-Sintel\nFinal.\nModiﬁed FlowNet.\nA small modiﬁcation to the\nFlowNet also improves signiﬁcantly, as suggested in the 5-\nth row in Table 2. By only adding a 2% more parameters\nand computation, the EPE improves from 5.11 to 4.62 on\nFlying-Chairs and from 7.82 to 7.33 on MPI-Sintel Final.\nContrast enhancement. We ﬁnd that contrast enhance-\nment is also a simple but very effective preprocessing step\nto improve the unsupervised optical ﬂow learning. By com-\nparing the 4th row and last row in Table 2, we ﬁnd the ﬁnal\nEPE improves from 4.14 to 3.30 on Flying-Chairs and 7.08\nto 6.34 on MPI-Sintel Final.\nCombining all components. We also ﬁnd that some-\ntimes one component is not signiﬁcant by itself, but the\noverall model improves dramatically when we add all the\n4 components into our framework.\nEffect of data. We have tried to use more data from\nKITTI raw videos (60,000 samples compared to 25,000\nsamples used in the paper) to train our model, but we did\nnot ﬁnd any improvement. We have also tried to adopt the\nnetwork structure from SpyNet [40] and train them using\nour unsupervised method. However we did not get better re-\nsult either, which suggests that the learning capability of our\nmodel is still the limiting factor, although we have pushed\nthis forward by a large margin.\n5. Conclusion\nWe present a new end-to-end unsupervised learning\nframework for optical ﬂow prediction. We show that with\nmodeling occlusion and large motion, our unsupervised ap-\nproach yields competitive results on multiple benchmark\ndatasets. This is promising since it opens a new path for\ntraining neural networks to predict optical ﬂow with a vast\namount of unlabeled videos and apply the ﬂow estimation\nfor more higher level computer vision tasks.\nReferences\n[1] A. Ahmadi and I. Patras. Unsupervised convolutional neural\nnetworks for motion estimation. In Image Processing (ICIP),\n2016 IEEE International Conference on, pages 1629–1633.\nIEEE, 2016.\n[2] L. Alvarez, R. Deriche, T. Papadopoulo, and J. S´anchez.\nSymmetrical dense optical ﬂow estimation with occlu-\nsions detection. International Journal of Computer Vision,\n75(3):371–385, 2007.\n[3] A. Ayvaci, M. Raptis, and S. Soatto. Occlusion detection and\nmotion estimation with convex optimization. In Advances\nin neural information processing systems, pages 100–108,\n2010.\n[4] A. Ayvaci, M. Raptis, and S. Soatto. Sparse occlusion de-\ntection with optical ﬂow. International Journal of Computer\nVision, 97(3):322–338, 2012.\n[5] M. Bai, W. Luo, K. Kundu, and R. Urtasun. Exploiting se-\nmantic information and deep matching for optical ﬂow. In\nEuropean Conference on Computer Vision, pages 154–170.\nSpringer, 2016.\n[6] C. Bailer, B. Taetz, and D. Stricker. Flow ﬁelds: Dense corre-\nspondence ﬁelds for highly accurate large displacement opti-\ncal ﬂow estimation. In Proceedings of the IEEE International\nConference on Computer Vision, pages 4015–4023, 2015.\n[7] C. Bailer, K. Varanasi, and D. Stricker.\nCnn-based patch\nmatching for optical ﬂow with thresholded hinge embedding\nloss. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[8] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and\nR. Szeliski. A database and evaluation methodology for opti-\ncal ﬂow. International Journal of Computer Vision, 92(1):1–\n31, 2011.\n[9] C. Ballester, L. Garrido, V. Lazcano, and V. Caselles.\nA\ntv-l1 optical ﬂow method with occlusion detection. Pattern\nRecognition, pages 31–40, 2012.\n[10] M. J. Black and P. Anandan. The robust estimation of multi-\nple motions: Parametric and piecewise-smooth ﬂow ﬁelds.\nComputer vision and image understanding, 63(1):75–104,\n1996.\n[11] J.-Y. Bouguet. Pyramidal implementation of the afﬁne lu-\ncas kanade feature tracker description of the algorithm. Intel\nCorporation, 5(1-10):4, 2001.\n[12] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-\ncuracy optical ﬂow estimation based on a theory for warping.\nComputer Vision-ECCV 2004, pages 25–36, 2004.\n[13] T. Brox and J. Malik. Large displacement optical ﬂow: de-\nscriptor matching in variational motion estimation.\nIEEE\ntransactions on pattern analysis and machine intelligence,\n33(3):500–513, 2011.\n[14] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A\nnaturalistic open source movie for optical ﬂow evaluation. In\nEuropean Conference on Computer Vision, pages 611–625.\nSpringer, 2012.\n[15] Z. Chen, H. Jin, Z. Lin, S. Cohen, and Y. Wu. Large dis-\nplacement optical ﬂow from nearest neighbor ﬁelds. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 2443–2450, 2013.\n[16] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas,\nV. Golkov, P. van der Smagt, D. Cremers, and T. Brox.\nFlownet: Learning optical ﬂow with convolutional networks.\nIn Proceedings of the IEEE International Conference on\nComputer Vision, pages 2758–2766, 2015.\n[17] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learn-\ning for physical interaction through video prediction. In Ad-\nvances in Neural Information Processing Systems, pages 64–\n72, 2016.\n[18] D. Forsyth and J. Ponce. Computer vision: a modern ap-\nproach.\nUpper Saddle River, NJ; London: Prentice Hall,\n2011.\n[19] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-\ntonomous driving? the kitti vision benchmark suite. In Com-\nputer Vision and Pattern Recognition (CVPR), 2012 IEEE\nConference on, pages 3354–3361. IEEE, 2012.\n[20] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised\nmonocular depth estimation with left-right consistency. In\nCVPR, volume 2, page 7, 2017.\n[21] F. G¨uney and A. Geiger. Deep discrete ﬂow. In Asian Con-\nference on Computer Vision, pages 207–224. Springer, 2016.\n[22] D. Hafner, O. Demetz, and J. Weickert. Why is the census\ntransform good for robust optic ﬂow computation? In Inter-\nnational Conference on Scale Space and Variational Meth-\nods in Computer Vision, pages 210–221. Springer, 2013.\n[23] B. K. Horn and B. G. Schunck. Determining optical ﬂow.\nArtiﬁcial intelligence, 17(1-3):185–203, 1981.\n[24] J. Hur and S. Roth. Joint optical ﬂow and temporally con-\nsistent semantic segmentation. In European Conference on\nComputer Vision, pages 163–177. Springer, 2016.\n[25] J. Hur and S. Roth. Mirrorﬂow: Exploiting symmetries in\njoint optical ﬂow and occlusion estimation. arXiv preprint\narXiv:1708.05355, 2017.\n[26] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and\nT. Brox. Flownet 2.0: Evolution of optical ﬂow estimation\nwith deep networks. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), volume 2, 2017.\n[27] S. Ince and J. Konrad. Occlusion-aware optical ﬂow estima-\ntion. IEEE Transactions on Image Processing, 17(8):1443–\n1451, 2008.\n[28] M. Jaderberg, K. Simonyan, A. Zisserman, et al.\nSpatial\ntransformer networks. In Advances in Neural Information\nProcessing Systems, pages 2017–2025, 2015.\n[29] J. Janai, F. Gney, J. Wulff, M. Black, and A. Geiger. Slow\nﬂow: Exploiting high-speed cameras for accurate and diverse\noptical ﬂow reference data. In Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2017.\n[30] J. Y. Jason, A. W. Harley, and K. G. Derpanis. Back to basics:\nUnsupervised learning of optical ﬂow via brightness con-\nstancy and motion smoothness. In Computer Vision–ECCV\n2016 Workshops, pages 3–10. Springer, 2016.\n[31] D. Kingma and J. Ba. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980, 2014.\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097–1105, 2012.\n[33] M. Leordeanu, A. Zanﬁr, and C. Sminchisescu.\nLocally\nafﬁne sparse-to-dense matching for motion and occlusion es-\ntimation. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 1721–1728, 2013.\n[34] B. D. Lucas, T. Kanade, et al. An iterative image registration\ntechnique with an application to stereo vision. 1981.\n[35] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers,\nA. Dosovitskiy, and T. Brox. A large dataset to train con-\nvolutional networks for disparity, optical ﬂow, and scene\nﬂow estimation.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 4040–\n4048, 2016.\n[36] S. Meister, J. Hur, and S. Roth.\nUnFlow: Unsupervised\nlearning of optical ﬂow with a bidirectional census loss. In\nAAAI, New Orleans, Louisiana, Feb. 2018.\n[37] M. Menze, C. Heipke, and A. Geiger. Discrete optimization\nfor optical ﬂow. In German Conference on Pattern Recogni-\ntion, pages 16–28. Springer, 2015.\n[38] D. Pathak, R. Girshick, P. Doll´ar, T. Darrell, and B. Hariha-\nran. Learning features by watching objects move. In Proc.\nCVPR, volume 2, 2017.\n[39] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal\nvideo autoencoder with differentiable memory.\narXiv\npreprint arXiv:1511.06309, 2015.\n[40] A. Ranjan and M. J. Black. Optical ﬂow estimation using a\nspatial pyramid network. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), volume 2, 2017.\n[41] Z. Ren, J. Yan, B. Ni, B. Liu, X. Yang, and H. Zha. Unsu-\npervised deep learning for optical ﬂow estimation. In AAAI,\npages 1495–1501, 2017.\n[42] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.\nEpicﬂow: Edge-preserving interpolation of correspondences\nfor optical ﬂow.\nIn Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 1164–\n1172, 2015.\n[43] L. Sevilla-Lara, D. Sun, V. Jampani, and M. J. Black. Op-\ntical ﬂow with semantic segmentation and localized layers.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3889–3898, 2016.\n[44] L. Sevilla-Lara, D. Sun, E. G. Learned-Miller, and M. J.\nBlack. Optical ﬂow estimation with channel constancy. In\nEuropean Conference on Computer Vision, pages 423–438.\nSpringer, 2014.\n[45] K. Simonyan and A. Zisserman. Two-stream convolutional\nnetworks for action recognition in videos.\nIn Advances\nin neural information processing systems, pages 568–576,\n2014.\n[46] F. Stein. Efﬁcient computation of optical ﬂow using the cen-\nsus transform. In DAGM-symposium, volume 2004, pages\n79–86. Springer, 2004.\n[47] C. Strecha, R. Fransens, and L. J. Van Gool. A probabilistic\napproach to large displacement optical ﬂow and occlusion\ndetection. In ECCV Workshop SMVP, pages 71–82. Springer,\n2004.\n[48] D. Sun, C. Liu, and H. Pﬁster. Local layering for joint motion\nestimation and occlusion detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 1098–1105, 2014.\n[49] D. Sun, S. Roth, and M. J. Black. Secrets of optical ﬂow\nestimation and their principles. In Computer Vision and Pat-\ntern Recognition (CVPR), 2010 IEEE Conference on, pages\n2432–2439. IEEE, 2010.\n[50] D. Sun, E. B. Sudderth, and M. J. Black. Layered image\nmotion with explicit occlusions, temporal consistency, and\ndepth ordering. In Advances in Neural Information Process-\ning Systems, pages 2226–2234, 2010.\n[51] J. Sun, Y. Li, S. B. Kang, and H.-Y. Shum. Symmetric stereo\nmatching for occlusion handling. In Computer Vision and\nPattern Recognition, 2005. CVPR 2005. IEEE Computer So-\nciety Conference on, volume 2, pages 399–406. IEEE, 2005.\n[52] M. Unger, M. Werlberger, T. Pock, and H. Bischof. Joint\nmotion estimation and segmentation of complex scenes with\nlabel costs and occlusion modeling.\nIn Computer Vision\nand Pattern Recognition (CVPR), 2012 IEEE Conference on,\npages 1878–1885. IEEE, 2012.\n[53] S. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar,\nand K. Fragkiadaki. Sfm-net: Learning of structure and mo-\ntion from video. arXiv preprint arXiv:1704.07804, 2017.\n[54] C. Vogel, S. Roth, and K. Schindler. An evaluation of data\ncosts for optical ﬂow.\nIn German Conference on Pattern\nRecognition, pages 343–353. Springer, 2013.\n[55] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid.\nDeepﬂow: Large displacement optical ﬂow with deep match-\ning. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 1385–1392, 2013.\n[56] J. Xu, R. Ranftl, and V. Koltun. Accurate optical ﬂow via\ndirect cost volume processing.\nIn The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July\n2017.\n[57] L. Xu, J. Jia, and Y. Matsushita. Motion detail preserving op-\ntical ﬂow estimation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 34(9):1744–1757, 2012.\n[58] K. Yamaguchi, D. McAllester, and R. Urtasun. Efﬁcient joint\nsegmentation, occlusion labeling, stereo and ﬂow estimation.\nIn European Conference on Computer Vision, pages 756–\n771. Springer, 2014.\n[59] R. Zabih and J. Woodﬁll. Non-parametric local transforms\nfor computing visual correspondence. In European confer-\nence on computer vision, pages 151–158. Springer, 1994.\n[60] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei. Flow-guided\nfeature aggregation for video object detection. arXiv preprint\narXiv:1703.10025, 2017.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-11-16",
  "updated": "2018-04-04"
}