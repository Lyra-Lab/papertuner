{
  "id": "http://arxiv.org/abs/1711.06929v1",
  "title": "Deep Gaussian Mixture Models",
  "authors": [
    "Cinzia Viroli",
    "Geoffrey J. McLachlan"
  ],
  "abstract": "Deep learning is a hierarchical inference method formed by subsequent\nmultiple layers of learning able to more efficiently describe complex\nrelationships. In this work, Deep Gaussian Mixture Models are introduced and\ndiscussed. A Deep Gaussian Mixture model (DGMM) is a network of multiple layers\nof latent variables, where, at each layer, the variables follow a mixture of\nGaussian distributions. Thus, the deep mixture model consists of a set of\nnested mixtures of linear models, which globally provide a nonlinear model able\nto describe the data in a very flexible way. In order to avoid\noverparameterized solutions, dimension reduction by factor models can be\napplied at each layer of the architecture thus resulting in deep mixtures of\nfactor analysers.",
  "text": "arXiv:1711.06929v1  [stat.ML]  18 Nov 2017\nDeep Gaussian Mixture Models\nCinzia Viroli\nGeoﬀrey J. McLachlan\nNovember 21, 2017\nAbstract\nDeep learning is a hierarchical inference method formed by sub-\nsequent multiple layers of learning able to more eﬃciently describe\ncomplex relationships. In this work, Deep Gaussian Mixture Mod-\nels are introduced and discussed. A Deep Gaussian Mixture model\n(DGMM) is a network of multiple layers of latent variables, where, at\neach layer, the variables follow a mixture of Gaussian distributions.\nThus, the deep mixture model consists of a set of nested mixtures\nof linear models, which globally provide a nonlinear model able to\ndescribe the data in a very ﬂexible way. In order to avoid overpa-\nrameterized solutions, dimension reduction by factor models can be\napplied at each layer of the architecture thus resulting in deep mix-\ntures of factor analysers.\nKeywords: Unsupervised Classiﬁcation, Mixtures of Factor Analyzers,\nStochastic EM Algorithm.\n1\nIntroduction\nIn the recent years, there has been an increasing interest on Deep Learning\nfor supervised classiﬁcation (LeCun et al., 2015). It is very diﬃcult to give\nan exact deﬁnition of what it is due to its wide applicability in diﬀerent con-\ntexts and formulations, but it can be thought of as a set of algorithms able\nto gradually learn a huge number of parameters in an architecture composed\nby multiple non-linear transformations, called multi-layer structure. Deep\nNeural Networks have achieved great success in supervised classiﬁcation and\nan important example of it is given by the so-called Facebook’s DeepFace\nsoftware: a deep learning facial recognition system that employs a nine-layer\n1\nneural network with over 120 million connection weights. It can identify hu-\nman faces in digital images with an accuracy of 97.35%, at the same level\nas the human visual capability (Hodson, 2014). Deep learning architectures\nare now widely used for speech recognition, object detection, pattern recog-\nnition, image processing and many other supervised classiﬁcation tasks; for a\ncomprehensive historical survey and its applications, see Schmidhuber (2015)\nand the references therein.\nDespite the success of deep models for supervised tasks, there has been\nlimited research in the machine learning and statistics community on deep\nmethods for clustering. In this paper we will present and discuss deep Gaus-\nsian mixtures for clustering purposes, a powerful generalization of classical\nGaussian mixtures to multiple layers. Identiﬁability of the model is discussed\nand an innovative stochastic estimation algorithm is proposed for parameter\nestimation. Despite the fact that in recent years research on mixture mod-\nels has been intense and proliﬁc in many directions, we will show how deep\nmixtures can be very useful for clustering in complex problems.\nThe paper is organized as follows. In the next section classical Gaussian\nmixture models will be reviewed. In Section 3 deep Gaussian mixtures are\ndeﬁned and their main probabilistic properties presented. Identiﬁability is\nalso discussed. In Section 4 dimensionally reduced deep mixtures are pre-\nsented. Section 5 is devoted to the estimation algorithm for ﬁtting the model.\nExperimental results on simulated and real data are presented in Section 6.\nWe conclude this paper with some ﬁnal remarks (Section 7).\n2\nGaussian Mixture Models\nFinite mixture models (McLachlan and Peel, 2000) have gained growing pop-\nularity in the last decades as a tool for model-based clustering (Fraley and Raftery,\n2002). They are now widely used in several areas such as pattern recogni-\ntion, data mining, image analysis, machine learning and in many problems\ninvolving clustering and classiﬁcation methods.\nLet yi be a p-dimensional random vector containing p quantitative vari-\nables of interest for the statistical unit ith, with i = 1, . . . , n. Then yi is\ndistributed as a Gaussian Mixture Model (GMM) with k components if\nf(yi; θ) =\nk\nX\nj=1\nπjφ(p)(yi; µj, Σj),\n2\nwhere the πj are positive weights subject to Pk\nj=1 πj = 1 and the µj, Σj are\nthe parameters of the Gaussian components. Note an interesting property\nthat will be very useful in deﬁning our proposal: a Gaussian mixture model\nhas a related factor-analytic representation via a linear model with a certain\nprior probability as\nyi = µj + Λjzi + ui\nwith prob. πj,\nwhere zi is p-dimensional a latent variable with a multivariate standard\nGaussian distribution and ui is an independent vector of random errors\nwith ui ∼N(0, Ψj), where the Ψj are diagonal matrices. The component-\ncovariance matrices can then be decomposed as Σj = ΛjΛ⊤\nj + Ψj.\n3\nDeep Mixture Models\nDeep learning is a hierarchical inference method organized in a multilayered\narchitecture, where the subsequent multiple layers of learning are able to\neﬃciently describe complex relationships. In the similar perspective of deep\nneural networks, we deﬁne a Deep Gaussian Mixture model (DGMM) as a\nnetwork of multiple layers of latent variables. At each layer, the variables\nfollow a mixture of Gaussian distributions. Thus, the deep mixture model\nconsists of a set of nested mixtures of linear models that globally provide a\nnonlinear model able to describe the data in a very ﬂexible way.\n3.1\nDeﬁnition\nSuppose there are h layers. Given the set of observed data y with dimension\nn × p at each layer a linear model to describe the data with a certain prior\nprobability is formulated as follows:\n(1)\nyi = η(1)\ns1 + Λ(1)\ns1 z(1)\ni\n+ u(1)\ni\nwith prob. π(1)\ns1 , s1 = 1, . . . , k1,\n(2)\nz(1)\ni\n= η(2)\ns2 + Λ(2)\ns2 z(2)\ni\n+ u(2)\ni\nwith prob. π(2)\ns2 , s2 = 1, . . . , k2,\n...\n(1)\n(h)\nz(h−1)\ni\n= η(h)\nsh + Λ(h)\nsh z(h)\ni\n+ u(h)\ni\nwith prob. π(h)\nsh , t = 1, . . . , kh,\nwhere z(h)\ni\n∼N(0, Ip) (i = 1, . . . , n) and u(1)\ni , . . . , u(h)\ni\nare speciﬁc random\nerrors that follow a Gaussian distribution with zero expectation and covari-\nance matrices Ψ(1)\ns1 , . . . , Ψ(h)\nsh , respectively, η(1)\ns1 , . . . , η(h)\nsh are vectors of length\n3\nFigure 1: Structure of a DGMM with h = 3 and number of layer components\nk1 = 3, k2 = 3 and k3 = 2\np, Λ(1)\ns1 , . . . , Λ(h)\nsh are square matrices of dimension p. The speciﬁc random\nvariables u are assumed to be independent of the latent variables z. From\nthis representation it follows that at each layer the conditional distribution of\nthe response variables given the regression latent variables is a (multivariate)\nmixture of Gaussian distributions.\nTo illustrate the DGMM, consider h = 3 and let the number of layer\ncomponents be k1 = 3, k2 = 3 and k3 = 2.\nThe structure is shown in\nFigure 1. Thus, at the ﬁrst layer we have that the conditional distribution\nof the observed data given z(1) is a mixture with 3 components and so on.\nMore precisely, by considering the data as the zero layer, y = z(0), all the\nconditional distributions follow a ﬁrst order Markov ﬁrst order property that\nis f(z(l)|z(l+1), z(l+2), . . . , z(h); Θ) = f(z(l)|z(l+1); Θ) for l = 0, . . . , h −1. At\neach layer, we have\nf(z(l)|z(l+1); Θ) =\nkl+1\nX\ni=1\nπ(l+1)\ni\nN(η(l+1)\ni\n+ Λ(l+1)\ni\nz(l+1), Ψ(l+1)\ni\n).\n(2)\nMoreover, with the DGMM with k1 = 3, k2 = 3 and k3 = 2 will have a ‘global’\nnumber of M = 8 sub-components (M = Ph\nl=1 πl), but ﬁnal k = 18 possible\npaths for the statistical units (k = Qh\nl=1 πl) that share and combine the\nparameters of the M sub-components. Thanks to this tying, the number of\nparameters to be estimated is proportional to the number of sub-components,\n4\nthus reducing the computational cost to learning directly a model with k = 18\ncomponents.\nLet Ωbe the set of all possible paths through the network. The generic\npath s = (s1, . . . , sh) has a probability πs of being sampled, with\nX\ns∈Ω\nπs =\nX\ns1,...,sh\nπ(s1,...,sh) = 1.\nThe DGMM can be written as\nf(y; Θ)\n=\nX\ns∈Ω\nπsN(y; µs, Σs),\n(3)\nwhere\nµs\n=\nη(1)\ns1 + Λ(1)\ns1 (η(2)\ns2 + Λ(2)\ns2 (. . . (η(h−1)\nsh−1 + Λ(h−1)\nsh−1 η(h)\nh )))\n=\nη(1)\ns1 +\nh\nX\nl=2\n l−1\nY\nm=1\nΛ(m)\nsm\n!\nη(l)\nsl\nand\nΣs\n=\nΨ(1)\ns1 + Λ(1)\ns1 (Λ(2)\ns2 (. . . (Λ(h)\nsh Λ(h)⊤\nsh\n+ Ψ(h)\nsh ) . . .)Λ(2)⊤\ns2\n)Λ(1)⊤\ns1\n=\nΨ(1)\ns1 +\nh\nX\nl=2\n l−1\nY\nm=1\nΛ(m)\nsm\n!\nΨ(l)\nsl\n l−1\nY\nm=1\nΛ(m)\nsm\n!⊤\n.\nThus globally the deep mixture can be viewed as a mixture model with\nk components and a fewer number of parameters shared through the path.\nIn a DGMM, not only the conditional distributions, but also the marginal\ndistributions of the latent variables z(l) are Gaussian mixtures. This can be\nestablished by integrating out the bottom latent variables, so that at each\nlayer\nf(z(l); Θ)\n=\nX\n˜s=(sl+1,...,sh)\nπ˜sN(z(l); ˜µ(l+1)\n˜s\n, ˜Σ\n(l+1)\n˜s\n),\n(4)\n5\nwhere ˜µ(l+1)\n˜s\n= η(l+1)\nsl+1 + Λ(l+1)\nsl+1 (η(l+2)\nsl+2 + Λ(l+2)\nsl+2 (. . . (η(h−1)\nsh−1 + Λ(h−1)\nsh−1 η(h)\nh ))) and\n˜Σ\n(l+1)\n˜s\n= Ψ(l+1)\nsl+1 + Λ(l+1)\nsl+1 (Λ(l+2)\nsl+2 (. . . (Λ(h)\nsh Λ(h)⊤\nsh\n+ Ψ(h)\nsh ) . . .)Λ(l+2)⊤\nsl+2\n)Λ(l+1)⊤\nsl+1\n.\nA deep mixture model for modeling natural images has been proposed\nby van den Oord and Schrauwen (2014). However, this model suﬀers from\nserious identiﬁability issues as discussed in the next section.\n3.2\nModel-based clustering and identiﬁability\nAs previously observed in a DGMM the total number of components (poten-\ntially identifying the groups) is given by the total number possible paths, k.\nIn case the true number of groups, say k∗, is known, one could limit the es-\ntimation problem by considering only the models with k1 = k∗(k1 < k) and\nperform clustering through the conditional distribution f(y|z(1); Θ). This\nhas the merit to have a nice interpretation: the remaining components of\nthe bottom layers act as density approximations to the global non-Gaussian\ncomponents. In this perspective, the model represents an automatic tool for\nmerging mixture components (Hennig, 2010; Baudry et al., 2010; Melnykov,\n2016) and the deep mixtures can be viewed as a special mixture of mixtures\nmodel (Li, 2005).\nHowever, in the general situation without further restrictions, the DGMM\ndeﬁned in the previous session suﬀers from serious identiﬁability issues related\nto the number of components at the diﬀerent layers and the possible equiv-\nalent paths they could form. For instance, if h = 2, a DGMM with k1 = 2,\nk2 = 3 components may be indistinguishable from a DGMM with k1 = 3,\nk2 = 2 components, both giving a total number of possible k = 6 (= k1 · k2)\npaths.\nNotice that even if k∗is known and we ﬁx k1 = k∗there is still\nnon-identiﬁability for models with more than two layers.\nMoreover, in all cases, there is a serious second identiﬁability issue related\nto parameter estimation.\nIn order to address the ﬁrst issue, the we introduce an important assump-\ntion on the model dimensionality: the latent variables at the diﬀerent layers\nhave progressively decreasing dimension, r1, r2, . . . , rh, where p > r1 > r2 >\n. . . , > rh ≥1. As a consequence, the parameters at the diﬀerent levels will\ninherit diﬀerent dimensionality as well. This constraint has also the merit to\navoid over-parameterized models, especially when p is high.\nThe second identiﬁability issue arises from the presence of latent variables\nand it is similar in its nature to the identiﬁability issue that aﬀects factor\n6\nmodels. In particular, given an invertible matrix A of dimension r × r, with\nr < p, the factor model y = η+Λz+u, with u ∼N(0, Ψ), and the transformed\nfactor model y = η + ΛAA−1z + u are indistinguishable, where A is an\northogonal matrix and the factors have zero mean and identity covariance\nmatrix. Thus there are r(r −1)/2 fewer free parameters. This ambiguity\ncan be avoided by imposing the constraint that Λ⊤Ψ−1Λ is diagonal with\nelements in decreasing order (see, for instance, Mardia et al. (1976)).\nMoving along the same lines, in the DGMM, at each layer from 1 to\nh −1, we assume that the conditional distribution of the latent variables\nf(z(l)|z(l+1); Θ) has zero mean and identity covariance matrix and the same\ndiagonality constraint on the parameters at each level.\n4\nDeep dimensionally reduced Gaussian mix-\nture models\nStarting from the model (1), dimension reduction is obtained by considering\nlayers that are sequentially described by latent variables with a progressively\ndecreasing dimension, r1, r2, . . . , rh, where p > r1 > r2 > . . . , > rh ≥1. The\ndimension of the parameters in (1) changes accordingly.\nConsider as an illustrative example a two-layer deep model (h = 2).\nIn this case, the dimensionally reduced DGMM consists of the system of\nequations:\n(1)\nyi = η(1)\ns1 + Λ(1)\ns1 z(1)\ni\n+ u(1)\ni\nwith prob. π(1)\ns1 , j = 1, . . . , k1,\n(2)\nz(1)\ni\n= η(2)\ns2 + Λ(2)\ns2 z(2)\ni\n+ u(2)\ni\nwith prob. π(2)\ns2 , i = 1, . . . , k2,\nwhere z(2)\ni\n∼N(0, Ir2), Λ(1)\ns1 is a (factor loading) matrix of dimension p ×\nr1, Λ(2)\ns2 has dimension r1 × r2, and Ψ(1)\ns1 and Ψ(2)\ns2 are squared matrices of\ndimension p × p and r1 × r1, respectively.\nThe two latent variables have\ndimension r1 and r2, respectively with p > r1 > r2 ≥1.\nThe model generalizes and encompasses several model-based clustering\nmethods. Gaussian mixtures are trivially obtained in absence of any layer\nand dimension reduction.\nMixtures of factor analyzers (McLachlan et al.,\n2003) may be considered as a one-layer deep model, where Ψ(1)\ns1 are diago-\nnal and z(1)\ni\n∼N(0, Ir1). When h = 2 with k1 = 1, Ψ(1) is diagonal, and\nΛ(2)\ns2 = {0}, the deep dimensionally reduced mixture coincides with mixtures\n7\nof factor analyzers with common factor loadings (Baek et al., 2010) and het-\neroscedastic factor mixture analysis (Montanari and Viroli, 2010). The so-\ncalled mixtures of factor mixture analyzers introduced by Viroli (2010) is a\ntwo-layer deep mixture with k1 > 1, Ψ(1)\ns1 diagonal and Λ(2)\ns2 = {0}. Under\nthe constraints that h = 2, Ψ(1)\ns1 and Ψ(2)\ns2 are diagonal, the model is a deep\nmixture of factor analyzers (Tang et al., 2012). In this work, the authors pro-\npose to learn one layer at a time. After estimating the parameters at each\nlayer, samples from the posterior distributions for that layer are used as data\nfor learning the next step in a greedy layer-wise learning algorithm. Despite\nits computational eﬃciency this multi-stage estimation process suﬀers from\nthe uncertainty in the sampling of the latent variable generated values. A\nbias introduced at a layer will aﬀect all the remaining ones and the problem\ngrows with h, with the number of components and under unbalanced possible\npaths. In the next section we will present a uniﬁed estimation algorithm for\nlearning all the model parameters simultaneously.\n5\nFitting Deep Gaussian Mixture Models\nBecause of the hierarchical formulation of a deep mixture model, the EM\nalgorithm represents the natural method for parameter estimation. The al-\ngorithm alternates between two steps and it consists of maximizing (M-step)\nand calculating the conditional expectation (E-step) of the complete-data\nlog-likelihood function given the observed data, evaluated at a given set of\nparameters, say Ω′:\nEz(1),...,z(h),s|y;Θ′ [log Lc(Θ)] .\n(5)\nThis implies that we need to compute the posterior distributions of the\nlatent variables given the data in the E-step of the algorithm. In contrast\nto the classical GMM, where this computation involves only the allocation\nlatent variable s for each mixture component, in a deep mixture model the\nderivation of bivariate (or multivariate) posteriors is required, thus making\nthe estimation algorithm very slow and not applicable to large data.\nTo further clarify this, consider the expansion of the conditional expecta-\ntion in (5) as sum of speciﬁc terms. For a model with h = 2 layers, it takes\nthe following form\n8\nEz,s|y;Θ′ [log Lc(Θ)] =\nX\ns∈Ω\nZ\nf(z(1), s|y; Θ′) log f(y|z(1), s; Θ)dz(1)\n+\nX\ns∈Ω\nZ Z\nf(z(1), z(2), s|y; Θ′) log f(z(1)|z(2), s; Θ)dz(1)dz(2)\n+\nZ\nf(z(2)|y; Θ′) log f(z(2))dz(2) +\nX\ns∈Ω\nf(s|y; Θ′) log f(s; Θ).\n(6)\nA proper way to overcome these computational diﬃculties is to adopt a\nstochastic version of the EM algorithm (SEM), (Celeux and Diebolt, 1985) or\nits Monte Carlo alternative (MCEM) (Wei and Tanner, 1990). The principle\nunderlying the handling of the latent variables is to draw observations (SEM)\nor samples of observations (MCEM) from the conditional density of the latent\nvariables given the observed data, in order to simplify the computation of\nthe E-step.\nThe strategy adopted is to draw pseudorandom observations at each layer\nof the network through the conditional density f(z(l)|z(l−1), s; Θ′), starting\nfrom l = 1 to l = h, by considering as ﬁxed, the variables at the upper level of\nthe model for the current ﬁt of parameters, where at the ﬁrst layer z(0) = y.\nThe conditional density f(z(l)|z(l−1), s; Θ′) can be expressed as\nf(z(l)|z(l−1), s; Θ′) = f(z(l−1)|z(l), s; Θ′)f(z(l)|s)\nf(z(l−1)|s; Θ′)\n,\n(7)\nwhere the denominator does not depend on z(l) and acts as a normaliza-\ntion constant, and the two terms in the numerator, conditionally on s, are\nGaussian distributed according to equations (4) and (2):\nf(z(l−1)|z(l), s; Θ′) = N(η(l)\nsl + Λ(l)\nsl z(l), Ψ(l)\nsl ),\nf(z(l)|s; Θ′) = N(˜µ(l+1)\nsl\n, ˜Σ\n(l+1)\nsl\n).\nBy substituting them in (7), after some simple algebra, it is possible to\nshow that\nf(z(l)|z(l−1), s) = N\n\u0000ρsl(z(l−1)), ξsl\n\u0001\n,\n(8)\n9\nwhere\nρsl(z(l−1)) = ξsl\n\u0012\u0000Λ(l)\nsl\n\u0001⊤\u0000Ψ(l)\nsl\n\u0001−1 (z(l−1) −η(l)\nsl ) +\n\u0010\n˜Σ\n(l+1)\nsl\n\u0011−1\n˜µ(l+1)\nsl\n\u0013\nand\nξsl =\n\u0012\u0010\n˜Σ\n(l+1)\nsl\n\u0011−1\n+\n\u0000Λ(l)\nsl\n\u0001⊤\u0000Ψ(l)\nsl\n\u0001−1 Λ(l)\nsl\n\u0013−1\n.\nThis is the core of the stochastic perturbation of the EM algorithm. Due\nto the sequential hierarchical structure of the random variable generation, the\nE and M steps of the algorithm can be computed for each layer. Considering\nthe sample of n observations, at the layer l = 1, . . . , h, we maximize\nEz(l),s|z(l−1);θ′\n\" n\nX\ni=1\nlog f(z(l−1)\ni\n|z(l)\ni , s; Θ)\n#\n=\nn\nX\ni=1\nZ\nf(z(l)\ni , s|z(l−1)\ni\n; Θ′) log f(z(l−1)\ni\n|z(l)\ni , s; Θ)dzi\n(9)\nwith respect to Λ(l)\nsl , Ψ(l)\nsl , and η(l)\nsl . By considering f(z(l−1)|z(l), s) = N(η(l)\nsl +\nΛsl(l)z(l), Ψ(l)\nsl ), we can compute the score of (9) to derive the estimates for\nthe new parameters given the provisional ones.\nTherefore, the complete\nstochastic EM algorithm can be schematized as follows. For l = 1, . . . , h:\n- S-STEP (z(l−1)\ni\nis known)\nGenerate M replicates z(l)\ni,m from f(z(l)\ni |z(l−1)\ni\n, s; Θ′).\n- E-STEP - Approximate:\nE[z(l)\ni |z(l−1)\ni\n, s; Θ′] ∼=\nPM\nm=1 z(l)\ni,m\nM\nand\nE[z(l)\ni z(l)⊤\ni\n|z(l−1)\ni\n, s; Θ′] ∼=\nPM\nm=1 z(l)\ni,mz(l)⊤\ni,m\nM\n.\n- M-STEP - Compute:\n10\nˆΛ(l)\nsl\n=\nPn\ni=1 p(s|z(l−1)\ni\n)(z(l−1)\ni\n−η(l)\nsl )E[z(l)⊤\ni\n|z(l−1)\ni\n, s]E[z(l)\ni z(l)⊤\ni\n|z(l−1)\ni\n, s]−1\nPn\ni=1 p(s|z(l−1)\ni\n)\n,\nˆΨ(l)\nsl\n=\nPn\ni=1 p(s|z(l−1)\ni\n)\nh\n(z(l−1)\ni\n−ηsl)(z(l−1)\ni\n−ηsl)⊤−(z(l−1)\ni\n−ηsl)E[z(l)⊤\ni\n|z(l−1)\ni\n, s]ˆΛ⊤\nsl\ni\nPn\ni=1 p(s|z(l−1)\ni\n)\n,\nˆη(l)\nsl\n=\nPn\ni=1 p(s|z(l−1)\ni\n)\nh\nz(l−1)\ni\n−ΛslE[z(l)⊤\ni\n|z(l−1)\ni\n, s]\ni\nPn\ni=1 p(s|z(l−1)\ni\n)\n,\nˆπ(l)\ns\n=\nn\nX\ni=1\nf (sl|yi) ,\nwhere f (sl|yi) is the posterior probability of the allocation variable given the\nobserved data that can be computed via Bayes’ formula.\n6\nSimulated and Real Application\n6.1\nSmiley Data\nIn this simulation experiment we have generated n = 1000 observations from\nfour classes in 3-dimensional space. The ﬁrst two variables are relevant for\nclustering and have been generated by using the R package mlbench. They\nare structured into two Gaussian eyes, a triangular nose and a parabolic\nmouth, as shown in Figure 2.\nWe have taken the standard deviation for\neyes and mouth equal to 0.45 and 0.35, respectively. The third variable is\na noise variable, independently generated from a Gaussian distribution with\nstandard deviation 0.5.\nData have been independently generated 100 times. On each replicate,\nwe applied DGMM with two-layers with r1 = 2, r2 = 1, k1 = 4, and k2\nranging from 1 to 5. We ﬁtted the models 10 times in a multistart procedure\nand we selected the best ﬁt according to BIC.\nWe compared the DGMM results with several clustering methods by ﬁx-\ning the number of groups equal to the true k = 4 for all strategies.\nWe\nﬁtted a Gaussian Mixture Model (GMM) by using the R package Mclust\n(Fraley et al., 2012), skew-normal and skew-t Mixture Models (SNmm and\nSTmm) by using the R package EMMIXskew (Wang et al., 2009), k-means,\nPartition around Medoids (PAM), and by Ward’s method (Hclust) imple-\nmented hierarchically. Clustering performance is measured by the Adjusted\n11\n−1\n0\n1\n2\n−1\n0\n1\n2\nFigure 2: Smiley Data\nRand Index (ARI) and the misclassiﬁcation rate. The average of the two\nindicators across the 100 replicates together with their standard errors are\nreported in Table 1.\nFigure 3 shows the box plots of the Adjusted Rand Indices and miclas-\nsiﬁcation rates (m.r.’s) across the 100 replicates. The results indicate that\nDGMM achieves the best classiﬁcation performance compared to the other\nmethods.\n6.2\nReal Data\nIn this section we shall apply the deep mixture model to some benchmark\ndata used by the clustering and classiﬁcation community. We shall consider:\n• Wine Data: this dataset comes from a study (Forina et al., 1986) on\n27 chemical and physical properties of three types of wine from the\nPiedmont region of Italy: Barolo (59), Grignolino (71), and Barbera\n(48). The clusters are well separated and most clustering methods give\nhigh clustering performance on this data.\n• Olive Data: The dataset contains the percentage composition of eight\nfatty acids found by lipid fraction of 572 Italian olive oils (Forina and Tiscornia,\n12\nkmeans\npam\nhclust\nmclust\nmsn\nmst\ndeepmixt\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAdjusted Rand Index\nkmeans\npam\nhclust\nmclust\nmsn\nmst\ndeepmixt\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nMiclassification Index\nFigure 3: Smiley Data: Box plots of the Adjusted Rand Indices and Miclas-\nsiﬁcation rates across the 100 replicates.\n13\nTable 1: Results on Smiley datasets: average of Adjusted Rand Index and\nmisclassiﬁcation rates across the 100 replicated. Standard errors are reported\nin brackets.\nMethod\nARI\nm.r.\nk-means\n0.661\n(0.003)\n0.134\n(0.001)\nPAM\n0.667\n(0.004)\n0.132\n(0.001)\nHclust\n0.672\n(0.013)\n0.141\n(0.006)\nGMM\n0.653\n(0.008)\n0.178\n(0.006)\nSNmm\n0.535\n(0.006)\n0.251\n(0.006)\nSTmm\n0.566\n(0.006)\n0.236\n(0.004)\nDGMM\n0.788\n(0.005)\n0.087\n(0.002)\n1982). The data come from three regions: Southern Italy (323), Sar-\ndinia (98), and Northern Italy (151) and the aim is to distinguish be-\ntween them. Also in this case, the clustering is not a very diﬃcult task\neven if the clusters are not balanced.\n• Ecoli Data: data consist of n = 336 proteins classiﬁed into their various\ncellular localization sites based on their amino acid sequences. There\nare p = 7 variables and k = 8 really unbalanced groups that make\nthe clustering task rather diﬃcult: cp cytoplasm (143), inner mem-\nbrane without signal sequence (77), perisplasm (52), inner membrane,\nuncleavable signal sequence (35), outer membrane (20), outer mem-\nbrane lipoprotein (5), inner membrane lipoprotein (2), inner membrane,\ncleavable signal sequence (2). These data are available from the UCI\nmachine learning repository.\n• Vehicle Data: the dataset contains k = 4 types of vehicles: a double\ndecker bus (218), Cheverolet van (199), Saab 9000 (217) and an Opel\nManta 400 (212).\nThe aim is to cluster them on the basis of their\nsilhouette represented from many diﬀerent angles for a total of p = 18\nvariables. This is a diﬃcult classiﬁcation task. In particular, the bus,\nthe van and the cars are distinguishable, but it is very diﬃcult to\ndistinguish between the cars. The data are taken from the R library\nmlbench.\n14\n• Satellite Data: the data derive from multi-spectral, scanner images\npurchased from NASA by the Australian Centre for Remote Sensing.\nThey consist of 4 digital images of the same scene in diﬀerent spectral\nbands structured into 3 × 3 square neighborhood of pixels. Therefore,\nthere are p = 36 variables. The number of images is n = 6435 coming\nfrom k = 6 groups of images: red soil (1533), cotton crop (703), grey\nsoil (1358), damp grey soil (626), soil with vegetation stubble (707) and\nvery damp grey soil (1508). This is notoriously a diﬃcult clustering\ntask not only because there are 6 unbalanced classes, but also because\nclassical methods may suﬀer from the dimensionality p = 36. The data\nare available from the UCI machine learning repository.\nOn these data we compared the DGMM model with Gaussian Mixture\nModels (GMM), skew-normal and skew-t Mixture Models (SNmm and STmm),\nk-means and the Partition Around Medoids (PAM), hierarchical clustering\nwith Ward distance (Hclust), Factor Mixture Analysis (FMA), and Mix-\nture of Factor Analyzers (MFA). For all methods, we assumed the number\nof groups to be known. This assumption is made in order to compare the\nrespective clustering performances. Note that in the case of an unknown\nnumber of groups, model selection for the DGMM can be done similarly to\nall the other mixture based approaches by using information criteria. There-\nfore, we considered the DGMM with h = 2 and h = 3 layers, a number of\nsubcomponents in the hidden layers ranging from 1 to 5 (while k1 = k∗)\nand all possible models with diﬀerent dimensionality for the latent variables\nunder the constraint p > r1 > ... > rh ≥1. Moreover, we considered 10\ndiﬀerent starting points for all possible models. For the GMM we considered\nall the possible submodels according to the family based on the covariance\ndecomposition implemented in mclust. Finally, we ﬁtted FMA and MFA by\nusing the R package MFMA available from the ﬁrst author’s webpage with dif-\nferent starting points and diﬀerent number of latent variables ranging from\n1 to the maximum admissible number.\nIn all cases we selected the best model according to BIC.\nFor the smaller dataset (Wine, Olive, Ecoli, Vehicle) the best DGMM\nsuggested by BIC was the model with h = 2 layers, while h = 3 layers were\nsuggested for the Satellite data. The Wine data are quite simple to classify.\nMost methods performed quite well. The best DGMM model was obtained\nwith r1 = 3, r2 = 2 and k1 = 3, k2 = 1. The Olive data are not very well dis-\ntinguished by classical methods such as k-means and hierarchical clustering,\n15\nTable 2: Results on Real Data: Adjusted Rand Index (ARI) and misclassiﬁ-\ncation rates (m.r.).\nDataset\nWine\nOlive\nEcoli\nVehicle\nSatellite\nARI\nm.r.\nARI\nm.r.\nARI\nm.r.\nARI\nm.r.\nARI\nm.r.\nk-means\n0.930\n0.022\n0.448\n0.234\n0.548\n0.298\n0.071\n0.629\n0.529\n0.277\nPAM\n0.863\n0.045\n0.725\n0.107\n0.507\n0.330\n0.073\n0.619\n0.531\n0.292\nHclust\n0.865\n0.045\n0.493\n0.215\n0.518\n0.330\n0.092\n0.623\n0.446\n0.337\nGMM\n0.917\n0.028\n0.535\n0.195\n0.395\n0.414\n0.089\n0.621\n0.461\n0.374\nSNmm\n0.964\n0.011\n0.816\n0.168\n-\n-\n0.125\n0.566\n0.440\n0.390\nSTmm\n0.085\n0.511\n0.811\n0.171\n-\n-\n0.171\n0.587\n0.463\n0.390\nFMA\n0.361\n0.303\n0.706\n0.213\n0.222\n0.586\n0.093\n0.595\n0.367\n0.426\nMFA\n0.983\n0.006\n0.914\n0.052\n0.525\n0.330\n0.090\n0.626\n0.589\n0.243\nDGMM\n0.983\n0.006\n0.997\n0.002\n0.749\n0.187\n0.191\n0.481\n0.604\n0.249\nwhile model-based clustering strategies produce better performance. Here\ndeep learning with r1 = 5, r2 = 1 and k1 = 3 k2 = 1 suggested by BIC, gives\nexcellent results with only 1 misclassiﬁed unit.\nThe challenging aspect of a cluster analysis on Ecoli data is the high num-\nber of (unbalanced) classes. On these data SNmm and STmm did not reach\nconvergence due to their being unable to handle satisfactorily the presence of\ntwo variables that each took on only two distinct values. The best clustering\nmethod also in this case is given by the deep mixture with r1 = 2, r2 = 1\nand k1 = 8, k2 = 1.\nDeep mixtures performed better than the other methods also for the\ndiﬃcult task to distinguish between silhouettes of vehicles with progressively\ndimension reduction of r1 = 7, r2 = 1 and components k1 = 4, k2 = 3.\nFinally, for the Satellite data a DGMM with h = 3 layers and r1 =\n13, r2 = 2, r1 = 1 and k1 = 6, k2 = 2, k1 = 1 is preferred in terms of BIC.\nResults here are comparable with MFA with 4 factors, its having slightly\nhigher ARI but with less corrected classiﬁed units in the total.\n7\nFinal remarks\nIn this work a deep Gaussian mixture model (DGMM) for unsupervised clas-\nsiﬁcation has been investigated. The model is a very general framework that\n16\nencompasses classical mixtures, mixtures of mixtures models, and mixture\nof factor analyzers as particular cases. Since DGMM is a generalization of\nclassical model-based clustering strategies, it is guaranteed to work as well\nas these methods. We demonstrate the greater ﬂexibility of DGMM with\nits higher complexity; for this reason it is particularly suitable for data with\nlarge sample size.\nWe illustrated the model on simulated and real data. From the experi-\nmental study we conducted, the method works eﬃciently and it gives a good\nclustering performance with h = 2 and h = 3 layers where, as suggested,\nmodel choice can be undertaken according to information criteria.\nReferences\nBaek, J., G. McLachlan, and L. Flack (2010). Mixtures of factor analyz-\ners with common factor loadings: applications to the clustering and vi-\nsualization of high-dimensional data.\nIEEE Trans Pattern Anal Mach\nIntell. 32(7), 1298 – 1309.\nBaudry, J.-P., A. E. Raftery, G. Celeux, K. Lo, and R. Gottardo (2010).\nCombining mixture components for clustering. Journal of Computational\nand Graphical Statistics 19(2), 332–353.\nCeleux, G. and J. Diebolt (1985). The SEM algorithm: a probabilistic teacher\nalgorithm derived from the EM algorithm for the mixture problem. Com-\nputational statistics quarterly 2(1), 73–82.\nForina, M., C. Armanino, M. Castino, and M. Ubigli (1986). Multivariate\ndata analysis as a discriminating method of the origin of wines. Vitis 25(3),\n189–201.\nForina, M. and E. Tiscornia (1982).\nPattern-recognition methods in the\nprediction of italian olive oil origin by their fatty-acid content. Annali di\nChimica 72(3-4), 143–155.\nFraley, C. and A. Raftery (2002). Model-based clustering, discriminant anal-\nysis and density estimation. Journal of the American Statistical Associa-\ntion 97, 611–631.\n17\nFraley, C., A. E. Raftery, T. B. Murphy, and L. Scrucca (2012).\nmclust\nVersion 4 for R: Normal Mixture Modeling for Model-Based Clustering,\nClassiﬁcation, and Density Estimation.\nHennig, C. (2010). Methods for merging gaussian mixture components. Ad-\nvances in Data Analysis and Classiﬁcation 4(1), 3–34.\nHodson, H. (2014). Facebook as good as a human at recognising faces.\nLeCun, Y., Y. Bengio, and G. Hinton (2015).\nDeep learning.\nNa-\nture 521(7553), 436–444.\nLi, J. (2005). Clustering based on a multilayer mixture model. Journal of\nComputational and Graphical Statistics 14(3), 547–568.\nMardia, K. V., J. T. Kent, and J. M. Bibby (1976). Multivariate Analysis.\nOxford: Academic Press.\nMcLachlan, G., D. Peel, and R. Bean (2003). Modelling high-dimensional\ndata by mixtures of factor analyzers.\nComputational Statistics & Data\nAnalysis 41(3), 379 – 388.\nMcLachlan, G. J. and D. Peel (2000). Finite Mixture Models. Wiley.\nMelnykov, V. (2016). Merging mixture components for clustering through\npairwise overlap. Journal of Computational and Graphical Statistics 25(1),\n66–90.\nMontanari, A. and C. Viroli (2010). Heteroscedastic factor mixture analysis.\nStatistical Modelling 10(4), 441–460.\nSchmidhuber, J. (2015). Deep learning in neural networks: An overview.\nNeural Networks 61, 85–117.\nTang, Y., G. E. Hinton, and R. Salakhutdinov (2012). Deep mixtures of factor\nanalysers. In J. Langford and J. Pineau (Eds.), Proceedings of the 29th In-\nternational Conference on Machine Learning (ICML-12), New York, NY,\nUSA, pp. 505–512. ACM.\nvan den Oord, A. and B. Schrauwen (2014). Factoring variations in natural\nimages with deep gaussian mixture models. In Z. Ghahramani, M. Welling,\n18\nC. Cortes, N. D. Lawrence, and K. Q. Weinberger (Eds.), Advances in Neu-\nral Information Processing Systems 27, pp. 3518–3526. Curran Associates,\nInc.\nViroli, C. (2010).\nDimensionally reduced model-based clustering through\nmixtures of factor mixture analyzers. Journal of Classiﬁcation 27(3), 363–\n388.\nWang, K., S.-K. Ng, and G. J. McLachlan (2009). Multivariate skew t mix-\nture models: applications to ﬂuorescence-activated cell sorting data. In\nDigital Image Computing: Techniques and Applications, 2009. DICTA’09.,\npp. 526–531. IEEE.\nWei, G. C. and M. A. Tanner (1990). A Monte Carlo implementation of the\nEM algorithm and the poor man’s data augmentation algorithms. Journal\nof the American statistical Association 85(411), 699–704.\n19\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2017-11-18",
  "updated": "2017-11-18"
}