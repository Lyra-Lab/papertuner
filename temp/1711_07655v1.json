{
  "id": "http://arxiv.org/abs/1711.07655v1",
  "title": "Genetic Algorithms for Evolving Deep Neural Networks",
  "authors": [
    "Eli David",
    "Iddo Greental"
  ],
  "abstract": "In recent years, deep learning methods applying unsupervised learning to\ntrain deep layers of neural networks have achieved remarkable results in\nnumerous fields. In the past, many genetic algorithms based methods have been\nsuccessfully applied to training neural networks. In this paper, we extend\nprevious work and propose a GA-assisted method for deep learning. Our\nexperimental results indicate that this GA-assisted approach improves the\nperformance of a deep autoencoder, producing a sparser neural network.",
  "text": "arXiv:1711.07655v1  [cs.NE]  21 Nov 2017\nGenetic Algorithms for Evolving Deep Neural Networks\nEli (Omid) David\nBar-Ilan University\nRamat-Gan 52900, Israel\nmail@elidavid.com\nIddo Greental\nTel Aviv University\nTel Aviv 69978, Israel\niddo.greental@gmail.com\nABSTRACT\nIn recent years, deep learning methods applying unsupervised\nlearning to train deep layers of neural networks have achieved\nremarkable results in numerous ﬁelds. In the past, many ge-\nnetic algorithms based methods have been successfully applied\nto training neural networks. In this paper, we extend previ-\nous work and propose a GA-assisted method for deep learn-\ning. Our experimental results indicate that this GA-assisted\napproach improves the performance of a deep autoencoder,\nproducing a sparser neural network.\nCategories and Subject Descriptors: I.2.6 [Artiﬁcial In-\ntelligence]: Learning—Connectionism and neural nets\nGeneral Terms: Algorithms.\nKeywords: Genetic algorithms, Deep learning, Neural net-\nworks, Autoencoders\n1.\nINTRODUCTION\nWhile the motivation for creating deep neural networks con-\nsisting of several hidden layers has been present for many\nyears, supported by a growing body of knowledge on the deep\narchitecture of the brain and advocated on solid theoretical\ngrounds [1, 3], until recently it was very diﬃcult to train neu-\nral networks with more than one or two hidden layers.\nRecently, deep learning methods which facilitate the train-\ning of neural networks with several hidden layers have been\nthe subject of increased interest, owing to the discovery of\nseveral novel methods.\nCommon approaches employ either\nautoencoders [2, 10] or restricted Boltzmann machines [5, 8,\n9] to train one layer at a time in an unsupervised manner.\nIn the past, genetic algorithms have been applied success-\nfully to training neural networks of shallow depths (one or\ntwo hidden layers) [11]. In this paper we demonstrate how\ngenetic algorithms can be applied to improve the training of\ndeep autoencoders.\n2.\nDEEP AUTOENCODERS\nIn this section, we brieﬂy describe autoencoders and explain\nhow they are used in the context of deep learning.\nAn autoencoder is an unsupervised neural network which\nsets the target values (of the output layer) to be equal to the\ninputs, i.e., the number of neurons at the input and output\nlayers is equal, and the optimization goal for output neuron\ni is set to yi = xi, where xi is the value of the input neuron\ni. A hidden layer of neurons is used between the input and\noutput layers, and the number of neurons in the hidden layer is\nusually set to fewer than those in the input and output layers,\nthus creating a bottleneck, with the intention of forcing the\nnetwork to learn a higher level representation of the input.\nThe weights of the encoder layer (W ) and the weights of the\ndecoder layer (W ′) can be tied (i.e., deﬁning W ′ = W T ).\nAutoencoders are typically trained using backpropagation.\nWhen an autoencoder’s training is completed, we can discard\nthe decoder layer, ﬁx the values of the encoder layer (so the\nlayer can no longer be modiﬁed), and treat the outputs of\nthe hidden layer as the inputs to a new autoencoder added on\ntop of the previous autoencoder. This new autoencoder can be\ntrained similarly. Using such layer-wise unsupervised training,\ndeep stacks of autoencoders can be assembled to create deep\nneural networks consisting of several hidden layers (forming a\ndeep belief network). Given an input, it will be passed through\nthis deep network, resulting in high level outputs. In a typical\nimplementation, the outputs may then be used for supervised\nclassiﬁcation if required, serving as a compact higher level\nrepresentation of the data.\n3.\nGA-ASSISTED DEEP LEARNING\nGenetic algorithms (GA) have been successfully employed\nfor training neural networks [11]. Speciﬁcally, GAs have been\nused as substitute for the backpropagation algorithm, or used\nin conjunction with backpropagation to improve the overall\nperformance.\nWe now propose a simple GA-assisted method which (ac-\ncording to our initial results presented in the next section)\nimproves the performance of an autoencoder, and produces a\nsparser network.\nWhen training an autoencoder with tied weights (i.e., the\nweights of the encoding layer are tied to those of the decod-\ning layer), we store multiple sets of weights (W ) for the layer.\nThat is, in our GA population each chromosome is one set of\nweights for the autoencoder.\nFor each chromosome (which\nrepresents the weights of an autoencoder), the root mean\nsquared error (RMSE) is calculated for the training samples\n(the error for each training sample is deﬁned as the diﬀerence\nbetween the values of the input and output layers). The ﬁtness\nfor chromosome i is deﬁned as fi = 1/RMSEi. After calculat-\ning the ﬁtness score for all the chromosomes, they are sorted\nfrom the ﬁttest to the least ﬁt. The weights of the high rank-\nRef: ACM Genetic and Evolutionary Computation Conference (GECCO), pages 1451–1452, Vancouver, Canada, July 2014.\ning chromosomes are updated using backpropagation, and the\nlower ranking chromosomes are removed from the population.\nThe removed chromosomes are replaced by the oﬀsprings of\nthe high ranking chromosomes.\nThe selection is performed\nuniformly with each of the remaining chromosomes having an\nequal probability for selection (regardless of the ﬁtness val-\nues of the chromosomes, i.e., the ﬁtness score is used only for\ndetermining which chromosomes are removed from the popu-\nlation). Given two parents, one oﬀspring is created as follows:\nCrossover is performed by randomly selecting weights from\nthe parents, and mutation is performed by replacing a small\nnumber of weights with zero.\nGradient descent methods such as backpropagation are sus-\nceptible to trapping at local minima.\nOur method assists\nbackpropagation in this respect, reducing the probability of\ntrapping at local minima. Additionally, mutating the weights\nto zero encourages sparsity in the network (fewer active weights).\nSparse representations are appealing due to information dis-\nentangling, eﬃcient variable-size representation, linear sepa-\nrability, and distributed sparsity [4].\nNote that when training of an autoencoder is complete, the\nvalues of the best chromosome are selected for that autoen-\ncoder. These values are ﬁxed and shared amongst all chromo-\nsomes when a new autoencoder layer is added on top of the\npreviously trained layer.\nThus, each chromosome contains\nonly the values of the layer currently being trained.\n4.\nEXPERIMENTAL RESULTS\nFor our experiments we used the popular MNIST hand-\nwritten digit recognition database [7]. In the MNIST dataset,\neach sample contains 784 pixels (28x28 image), each having a\ngrayscale value between 0 to 255 (which we scale to a 0 to 1\nrange). Each sample also contains a target classiﬁcation label\n(between 0 and 9), which is used for the subsequent super-\nvised classiﬁcation phase (using the high level representations\ngenerated by the unsupervised autoencoder).\nOur deep neural network uses a stack of 5 layers. The ﬁrst\nlayer has 784 neurons, followed by four higher level layers con-\nsisting of 500, 250, 100, and 50 neurons. Each layer is trained\nseparately, with the next layer added only once training is\ncomplete: ﬁrst we train the 784-500 layer, then use the 500\noutput neurons as inputs to the 500-250 layer, and similarly\nfor the 250-100 and 100-50 layers.\nThe GA implementation uses a population of 10 chromo-\nsomes. In each generation, the ﬁve worst chromosomes (half\nthe population) are removed and replaced by the oﬀsprings of\nthe ﬁve best chromosomes. We used crossover and mutation\nrates of 0.8 and 0.01 accordingly.\nTo compare the performance of our GA-assisted method\nwith traditional backpropagation, we ran both methods un-\nder similar conditions.\nFirst, we ran the traditional back-\npropagation version 10 times and selected the result with the\nleast reconstruction error (best tuned). Next, we ran the GA-\nassisted method only once, allowing the same total runtime\nas the previous method. Comparing the reconstruction errors\nof the two approaches, the GA-assisted method consistently\nyielded a smaller reconstruction error, as well as a sparser\nnetwork.\nIn order to compare the classiﬁcation accuracy of the two\nmethods, we ran 10,000 new test samples through the two\ntrained networks and recorded the 50 output values for each\nsample. Recall that in this test phase the weights of the net-\nwork are already ﬁxed, hence an input sample of 784 values\nis passed through the layers of 500, 250, 100, and 50 neurons\nwithout modifying their weights.\nThe representation qual-\nity of the networks can be compared by applying supervised\nclassiﬁcation to the higher level values produced by the 50\nneurons of the output layer. We used SVM classiﬁcation with\na radial basis function (RBF) kernel. Using SVM, the tradi-\ntional autoencoder achieved a 1.85% classiﬁcation error, while\nthe GA-assisted method’s classiﬁcation error was 1.44%.\n5.\nCONCLUDING REMARKS\nIn this paper we presented a simple GA-assisted approach,\nwhich according to our initial results improves the perfor-\nmance of a deep autoencoder.\nWhile our implementation\nused an autoencoder, the same method is applicable to other\nforms of deep learning such as restricted Boltzmann machines\n(RBM).\nIn recent years, several improvements upon traditional au-\ntoencoders and RBM have been proposed which improve their\ngeneralization. Such improvements include dropout [6], which\nrandomly disables some neurons during training, dropconnect\n[13], which randomly disables some weights during training,\nand denoising autoencoders [12], which randomly add noise\nby removing a portion of the training data. The improved\nperformance of the GA-assisted autoencoder could arise from\na similar principle, since mutation randomly disables some of\nthe weights during training. It is important to compare the\nGA-assisted approach to the above mentioned alternative im-\nprovements in future research.\n6.\nREFERENCES\n[1] Y. Bengio and Y. LeCun. Scaling learning algorithms towards\nAI. Large Scale Kernel Machines. MIT Press, 2007.\n[2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy\nlayer-wise training of deep networks. Advances in Neural\nInformation Processing Systems 19, pages 153–160. MIT Press,\n2007.\n[3] Y. Bengio. Learning deep architectures for AI. Foundations and\nTrends in Machine Learning, 2(1):1–127, 2009.\n[4] X. Glorot, A. Bordes and Y Bengio. Deep sparse rectiﬁer neural\nnetworks, 14th International Conference on Artiﬁcial\nIntelligence and Statistics, pages 315–323, 2011.\n[5] G.E. Hinton, S. Osindero, and Y.W. Teh. A fast learning\nalgorithm for deep belief nets. Neural Computation,\n18:1527–1554, 2006.\n[6] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.\nSalakhutdinov. Improving neural networks by preventing\nco-adaptation of feature detectors. CoRR abs/1207.0580, 2012.\n[7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner. Gradient-based\nlearning applied to document recognition. Proceedings of the\nIEEE, 86(11):2278–2324, 1998.\n[8] G.E. Hinton and R. Salakhutdinov. Reducing the dimensionality\nof data with neural networks. Science, 313(5786):504–507, 2006.\n[9] H. Lee, C. Ekanadham, and A. Ng. Sparse deep belief net model\nfor visual area V2. Advances in Neural Information Processing\nSystems 20, pages 873–880, MIT Press, 2008.\n[10] M. Ranzato, C.S. Poultney, S. Chopra, and Y. LeCun. Eﬃcient\nlearning of sparse representations with an energy-based model.\nAdvances in Neural Information Processing Systems 19, pages\n1137–1144. MIT Press, 2007.\n[11] J.D. Schaﬀer, D. Whitley, and L.J. Eshelman. Combinations of\ngenetic algorithms and neural networks: a survey of the state of\nthe art. International Workshop on Combinations of Genetic\nAlgorithms and Neural Networks, pages 1–37, 1992.\n[12] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol.\nStacked denoising autoencoders: Learning useful representations\nin a deep network with a local denoising criterion. Journal of\nMachine Learning Research, 11:3371-3408, 2010.\n[13] L. Wan, M.D. Zeiler, S. Zhang, Y. LeCun, and R. Fergus.\nRegularization of neural networks using dropconnect.\nInternational Conference on Machine Learning, pages\n1058–1066. JMLR.org, (2013)\n",
  "categories": [
    "cs.NE",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2017-11-21",
  "updated": "2017-11-21"
}