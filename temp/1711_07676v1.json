{
  "id": "http://arxiv.org/abs/1711.07676v1",
  "title": "Transferring Agent Behaviors from Videos via Motion GANs",
  "authors": [
    "Ashley D. Edwards",
    "Charles L. Isbell Jr"
  ],
  "abstract": "A major bottleneck for developing general reinforcement learning agents is\ndetermining rewards that will yield desirable behaviors under various\ncircumstances. We introduce a general mechanism for automatically specifying\nmeaningful behaviors from raw pixels. In particular, we train a generative\nadversarial network to produce short sub-goals represented through motion\ntemplates. We demonstrate that this approach generates visually meaningful\nbehaviors in unknown environments with novel agents and describe how these\nmotions can be used to train reinforcement learning agents.",
  "text": "Transferring Agent Behaviors from Videos via\nMotion GANs\nAshley D. Edwards\nCollege of Computing\nGeorgia Institute of Technology\nAtlanta, GA 30332\naedwards8@gatech.edu\nCharles L. Isbell Jr.\nCollege of Computing\nGeorgia Institute of Technology\nAtlanta, GA 30332\nisbell@cc.gatech.edu\nAbstract\nA major bottleneck for developing general reinforcement learning agents is deter-\nmining rewards that will yield desirable behaviors under various circumstances. We\nintroduce a general mechanism for automatically specifying meaningful behaviors\nfrom raw pixels. In particular, we train a generative adversarial network to produce\nshort sub-goals represented through motion templates. We demonstrate that this\napproach generates visually meaningful behaviors in unknown environments with\nnovel agents and describe how these motions can be used to train reinforcement\nlearning agents.\n1\nIntroduction\nReinforcement Learning (RL) has been shown to be a successful approach for solving complex\nproblems such as games and robotics tasks. Still, progress is often hindered because the learned\npolicies do not generalize well to multiple agents and environments, resulting in a need for a new\nreward function each time the problem changes. Traditional approaches use hand-crafted rewards to\nspecify goals, or for more complex tasks, one can demonstrate the desired behavior and train agents\nvia imitation learning. These representations can successfully train behaviors for known agents, but\ntypically require domain knowledge that does not generalize to unexplored environments.\nWe often have a broad expectation for how objects should move in the real-world, even if we have\nnot seen them before. We do not expect pigs to sprout wings and ﬂy into the air, or clouds to ﬂoat\ndown onto the earth’s surface. Inanimate materials should not become animate, nor should rivers ﬂow\nupstream. Our expectations are shaped by the events we have experienced in the world. Inspired by\nthis, we aim to use deep learning to learn about how objects should move in artiﬁcial and real-world\nenvironments, and use this information to inform how agents should act.\nRather than requiring task-speciﬁc engineering for each problem, our approach aims to learn a general\nrepresentation of motion from videos that can then be used to visually generate desired behaviors\nin the agent’s environment. We develop a generative model of common motions by training an\nimage-to-image model [11] to compute motion templates [4, 5] from still images within a wide array\nof environments. With this model, we can generate a plan of motion for agents and environments that\nare similar to those that were observed in the training set. We then aim to use this visual plan to learn\na policy that imitates the desired behavior.\nIn this initial work, we focus on developing the framework for predicting motions. We will demon-\nstrate that the learned model can generate motions in environments that it trained on as well as in\nunfamiliar environments consisting of novel agents.\narXiv:1711.07676v1  [cs.LG]  21 Nov 2017\nFigure 1: Motion templates computed from videos obtained from the WildLife Documentary (WLD)\nDataset [12] of a polar bear, tiger, and group of people.\n2\nBackground\nWe now brieﬂy discuss methods we use in our approach: reinforcement learning, motion templates,\nand image-to-image translation.\n2.1\nReinforcement Learning\nReinforcement Learning (RL) problems are described through a Markov Decision Process\n⟨S, A, T, R⟩[22]. The set S consists of the states s ∈S in the environment. An agent takes\nactions a ∈A and receives rewards r ∈R(s) that specify the goals for the problem. The transition\nfunction T(s, a, s′) represents the probability that the agent will land in state s′ after taking action a\nin state s. A policy π(s, a) represents the probability of taking action a in state s and we typically\naim to ﬁnd policies that maximize the total long-term expected reward.\nAn action-value, or Q-value, Q(s, a) represents the expected discounted cumulative reward an agent\nwill receive after taking action a in state s, then following π thereafter. We typically are interested in\ncomputing optimal Q-values:\nQ∗(s, a) = max\nπ\nE\n\" ∞\nX\nk=0\nγkrt+k+1|st = s, at = a, π\n#\nwhere 0 ≤γ < 1 is a discount factor that encodes how rewards retain their value over-time.\n2.2\nMotion Templates\nIn our approach, we represent goals as motion templates (see Figure 1), which are 2D spatio-temporal\nrepresentations of motion obtained from a sequence of images—typically from the segmented frames\nof a video [4, 5]. Movement that occurred more recently in time has a higher pixel intensity in the\ntemplate than earlier motion and depicts both where and when motion occurred.\nCalculating a motion template is an iterative process. The ﬁrst step is to obtain a silhouette image of\nthe motion that has occurred between each frame. The silhouette is computed by taking the absolute\ndifference between two images and then computing the binary threshold, which sets all pixels below\na threshold to 0 and all pixels above the threshold to 1.\nA function Ψ(I) computes the motion template µ for a sequence of images i1, i2, . . . , in ∈I. Let\nσt represent a silhouette image at time t. To calculate the motion template µ of I, we ﬁrst compute\na silhouette image σ1, σ2, . . . , σn−1 between all consecutive images (i1, i2), (i2, i3), . . . , (in−1, in).\nThen ∀x,y, where x and y are respective column and row pixel locations, we can compute µt,x,y for\ntime t = 1, 2, . . . , n:\nµt,x,y =\n\n\n\nτ,\nif σt,x,y > 0\n0,\nelse if µt−1,x,y < (τ −δ)\nµt−1,x,y,\notherwise\nIn words, the function increases the intensity of the pixel at x, y if movement has occurred at the\ncurrent iteration t. Here, δ and τ are both parameters that inﬂuence how much µt is decayed. The\nparameter τ is a representation for the current time in the sequence and increases as t increases. The\nparameter δ represents the duration of the motion template and controls how quickly pixels decay.\nEssentially, Ψ(I) layers the silhouette images and weights them by time.\n2\n2.3\nImage-to-image translation\nWe use image-to-image translation—a recent approach that uses Generative Adversarial Networks\n(GANs) to translate one image into another [11]—to automatically generate motion templates. Image-\nto-image models have been used to convert images of edges to handbags and images of day to\nnight. Given an input image, a generative model attempts to generate the translated output, while a\ndiscriminative model predicts whether the pair was obtained from the training data or generated by\nthe GAN. The generative network is trained to fool the discriminator and to output images that match\nthe ground truth, while the discriminative model is trained to make correct predictions.\n3\nRelated work\nGoals in reinforcement learning are traditionally deﬁned through rewards that indicate when desired\nstates have been reached. A major beneﬁt of RL is that good policies can often be learned from\na single reward, but a consequence of this sparsity is that learning the policies can be slow and\ninefﬁcient. In general, we argue that RL suffers from a reward shortage, particularly because new\ntasks often require engineering more rewards, even when the environment has been seen before.\nRepresenting goals separately from rewards offers more portability, for example by using target\nimages [7, 9], but this still requires specifying goals for each problem. An alternative is to learn\npolicies that can generalize across environments. Transfer learning aims to port learned behaviors\nfrom one domain to another [24], for example by initializing the parameters of policies in unsolved\ntasks [2], or by transferring skills across untrained robots [6]. Training in simulation and then\ntransferring the knowledge to the real-world can often be more efﬁcient than training there directly\n(e.g.\n[17, 18, 25, 26, 28]). Such “sim-to-real” approaches tend to focus on transferring across\nseparate realizations of similar domains. Learning from Demonstration (LfD) can also be used when\nspecifying a goal is difﬁcult, or when a problem is too challenging for an agent to solve on its own.\nInverse RL aims to infer a reward function from expert demonstrations [1] or policies can be learned\ndirectly from demonstrations [19]. Many recent works have trained agents such as simulated and real\nrobots from the raw pixels of videos. [7, 21, 20, 14]. These approaches often focus on learning tasks\nfrom speciﬁc demonstrations for these problems. Our approach aims to learn general models from\nvideos. In particular, we develop a hierarchy for specifying and satisfying goals, similar to works that\ndevelop temporally-extended sequences of actions known as options and controllers for selecting\nfrom them [23, 3, 13, 27]. Finally, many approaches have learned models of the world (see [16] for a\nsurvey), but these methods often require access to the underlying MDP. Our approach learns a general\nmodel from pixels that can be applied to novel environments.\n4\nApproach\nOur approach aims to learn a general model of motion from videos. We use the recent image-to-image\narchitecture to predict motion templates from still images. We call this representation Motion GAN\n(MoGAN). This paper focuses on how to train this model, but we will also discuss how we can\nuse these generated motions to construct short-term policies for unknown agents within unobserved\nenvironments.\nMoGANs are inspired by previous work for learning to act from motion templates [7]. In that\napproach, a simulated robot was trained by comparing motion templates of the robot to those of\nhumans. Motion templates allow for a denser reward function that naturally shapes the desired\nbehavior. The reward was speciﬁed as the similarity between the templates, but utilized hand-crafted\nfeatures for comparability. We aim to automatically generate plans of motion that are appropriate for\nthe current environment. In particular, we develop a hierarchy in which a meta-controller (MoGAN)\ngenerates a high-level plan of action in the form of motion and a controller (the policy) then learns\nactions that satisfy this goal. Developing the controller remains as future work.\n4.1\nMeta-controller\nWe now describe how we train the MoGAN model, or meta-controller. Given a dataset of videos,\nwe segment each video into a sequence of n frames. Then we use each mini-sequence to compute a\nmotion template. We construct frame-motion pairs by taking the initial frame of each sequence and\n3\nFigure 2: Generated push motion templates on unseen test images. The top row represents the initial\nframe used to compute the motion templates. The middle row represents the ground truth motion\ntemplate. The bottom row represents the generated motion template.\nFigure 3: Sample of frame-motion training pairs obtained from video games. The left image represents\nthe initial frame and the right represents the computed motion template.\npairing with the respective motion template. With these pairs, we can train the image-to-image model\nto predict motions from still images. We aim to use generated behaviors as goals for RL.\n4.2\nMultimodal outputs\nOne problem with image-to-image translation is the model is forced to learn a one-to-one mapping,\nwhen in fact there are many possible outputs of the network. To address this problem, we develop\na multimodal network that allows for multiple outputs. We expect to ﬁnd that the network will\noutput visual options that indicate multiple behaviors. In this network, we still train each output to\nfool the discriminator, but now the generator is only penalized on the minimum distance between the\nground truth and each of the generated outputs.\n5\nExperiments and results\nOur experiments aim to demonstrate that MoGANs can transfer behaviors from videos. For now, we\nfocus on qualitative analysis of generated behaviors.\n4\nFigure 4: Generated motion templates from screenshots of unseen video games obtained from\npygame.org. The top row represents the initial frame and the bottom row represents the generated\nmotion template.\nFigure 5: Generated motion templates for unseen real-world images from the WLD Dataset. The top\nrow represents the initial frame and the bottom row represents the generated motion template.\n5.1\nIntra-domain behaviors\nWe ﬁrst evaluate intra-domain predictions. We collected videos from Google’s Push Dataset [8] and\naimed to predict motions within this environment. This dataset contains videos of a robotic arm\npushing objects within a bin. To obtain the training set, we segmented each video into 5 frames\nand computed motion templates for each segment. The resulting training set consisted of 1,200\nframe-motion pairs. Figure 2 shows the results. It is clear that the model learns to predict movement\nof the arm. Additionally, while objects that are unlikely to move are ignored from the scene, objects\nthat are close to the gripper are predicted to move.\n5.2\nTransferred behaviors\nThe next experiments aimed to evaluate the generality of MoGANs. We trained MoGANs to generate\nmotions from videos of video game play-throughs obtained from archive.org 1. To obtain the training\nset, we segmented each video into 10 frames and computed motion templates for each segment.\nThe resulting training set consisted of 101,227 frame-motion pairs (see Figure 3 for examples). We\nexpected to ﬁnd that we could predict motions of video games outside of this training set. We tested\non unseen platform games from pygame.org. We show results in Figure 4. We again found that the\nmodel was able to segment salient objects from the screen. The model predicted that the background\nwould remain stationary while the player and other objects would move from the left to right, which\nis standard behavior for platformers.\nInterestingly, we also found that MoGAN learned to detect salient objects in the real world, even\nthough it was trained only on video games, as shown in Figure 5. One possible reason is that in order\nto detect motion in video games, MoGAN must ﬁrst learn to attend to important regions in the scene,\nand then predict what the motions will be. Another reason could be that the domain randomization\nforces the model to be robust to environment changes, which is an argument for why some sim-to-real\napproaches have been successful [25]. These results demonstrate the robustness of MoGANs and\nadditionally suggest that we can use the model for attention in RL problems.\n1https://archive.org/details/speed_runs\n5\nFigure 6: Learned visual options in video games from pygame.org and a real-world image of people\nwalking [15]. The left represents the input, and each other column represents an output of the network.\n5.3\nMultimodal behaviors\nFinally, we evaluate performance for multimodal outputs. We indeed observed that the model learned\nvisual option-like behavior, as one output predicted moving to the left, one to the right, and one\njumping in the air.\n6\nConclusion and future work\nIn this paper, we have shown how behaviors can be generated for both seen and unseen environments.\nWe introduced MoGAN, a meta-controller for producing goal motions from still images. We should\npoint out that our approach does suffer from a common problem with GANs, as the model sometimes\npredicts meaningless behaviors. We aim to use recent techniques for improving the quality of the\noutputs to improve the model. Future work will entail using the generated motions to plan with\nreinforcement learning. We now outline a plan for this approach.\n6.1\nController\nWe now describe how we can train an RL agent. We assume that the agent’s environment consists\nof visual inputs. Given an observation, we aim to use the learned meta-controller to generate the\nexpected behavior. In particular, every n steps, the meta-controller generates a new goal given the\ncurrent observation. We additionally use the last n frames from the current episode to compute a\nmotion template. We represent the agent’s state as its current observation, the generated goal, and its\ncurrent motion template. As such, it should learn to plan based on its current goal.\n6.2\nReward Function\nWe aim to use two components for the reward function. First, we can compute the similarity between\nthe generated and computed motion templates by extracting features from the discriminative network\nand taking the L1 distance between the templates. We can additionally use the discriminator to\ndetermine if the agent’s computed motion template looks like a motion that should occur naturally. The\ngenerated motion template acts as a visual plan for meaningful behaviors, while the discriminator aims\nto discourage unnatural ones. This mechanism for training is motivated by Generative Adversarial\nImitation Learning (GAIL) [10], which aims to generate behaviors that fool a discriminator into\npredicting if behaviors were executed by the demonstrator.\n6\nReferences\n[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the\ntwenty-ﬁrst international conference on Machine learning, page 1. ACM, 2004.\n[2] H. B. Ammar, E. Eaton, P. Ruvolo, and M. E. Taylor. Unsupervised cross-domain transfer in policy gradient\nreinforcement learning via manifold alignment. 2015.\n[3] P.-L. Bacon, J. Harb, and D. Precup. The option-critic architecture. 2017.\n[4] A. F. Bobick and J. W. Davis. The recognition of human movement using temporal templates. Pattern\nAnalysis and Machine Intelligence, IEEE Transactions on, 23(3):257–267, 2001.\n[5] J. Davis. Recognizing movement using motion histograms. Technical Report 487, MIT Media Lab,\n1(487):1, 1999.\n[6] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural network policies for\nmulti-task and multi-robot transfer. arXiv preprint arXiv:1609.07088, 2016.\n[7] A. Edwards, C. Isbell, and A. Takanishi. Perceptual reward functions. arXiv preprint arXiv:1608.03824,\n2016.\n[8] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learning for physical interaction through video\nprediction. In Advances in Neural Information Processing Systems, pages 64–72, 2016.\n[9] C. Finn and S. Levine. Deep visual foresight for planning robot motion. In Robotics and Automation\n(ICRA), 2017 IEEE International Conference on, pages 2786–2793. IEEE, 2017.\n[10] J. Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural Information\nProcessing Systems, pages 4565–4573, 2016.\n[11] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial\nnetworks. arxiv, 2016.\n[12] C. C. L. D. L. Kai Chen, Hang Song. Discover and learn new objects from documentaries. In Proceedings\nof IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n[13] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement learning:\nIntegrating temporal abstraction and intrinsic motivation. In Advances in Neural Information Processing\nSystems, pages 3675–3683, 2016.\n[14] Y. Liu, A. Gupta, P. Abbeel, and S. Levine. Imitation from observation: Learning to imitate behaviors from\nraw video via context translation. arXiv preprint arXiv:1707.03374, 2017.\n[15] V. Mahadevan and N. Vasconcelos. Spatiotemporal saliency in dynamic scenes. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 32(1):171–177, 2010.\n[16] A. S. Polydoros and L. Nalpantidis. Survey of model-based reinforcement learning: Applications on\nrobotics. Journal of Intelligent & Robotic Systems, 86(2):153–173, 2017.\n[17] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell. Sim-to-real robot learning from\npixels with progressive nets. arXiv preprint arXiv:1610.04286, 2016.\n[18] F. Sadeghi and S. Levine. (cad)2rl: Real single-image ﬂight without a single real image. arXiv preprint\narXiv:1611.04201, 2016.\n[19] S. Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3(6):233–242,\n1999.\n[20] P. Sermanet, C. Lynch, J. Hsu, and S. Levine. Time-contrastive networks: Self-supervised learning from\nmulti-view observation. arXiv preprint arXiv:1704.06888, 2017.\n[21] P. Sermanet, K. Xu, and S. Levine. Unsupervised perceptual rewards for imitation learning. arXiv preprint\narXiv:1612.06699, 2016.\n[22] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. Cambridge Univ Press,\n1998.\n[23] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal\nabstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999.\n7\n[24] M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal of\nMachine Learning Research, 10(Jul):1633–1685, 2009.\n[25] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring\ndeep neural networks from simulation to the real world. arXiv preprint arXiv:1703.06907, 2017.\n[26] E. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine, K. Saenko, and T. Darrell. Adapting deep\nvisuomotor representations with weak pairwise constraints. In Workshop on the Algorithmic Foundations\nof Robotics (WAFR), 2016.\n[27] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. Feudal\nnetworks for hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161, 2017.\n[28] F. Zhang, J. Leitner, M. Milford, B. Upcroft, and P. Corke. Towards vision-based deep reinforcement\nlearning for robotic motion control. arXiv preprint arXiv:1511.03791, 2015.\n8\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2017-11-21",
  "updated": "2017-11-21"
}