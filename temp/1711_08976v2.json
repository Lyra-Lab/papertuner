{
  "id": "http://arxiv.org/abs/1711.08976v2",
  "title": "Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval",
  "authors": [
    "Yi Yu",
    "Suhua Tang",
    "Francisco Raposo",
    "Lei Chen"
  ],
  "abstract": "Little research focuses on cross-modal correlation learning where temporal\nstructures of different data modalities such as audio and lyrics are taken into\naccount. Stemming from the characteristic of temporal structures of music in\nnature, we are motivated to learn the deep sequential correlation between audio\nand lyrics. In this work, we propose a deep cross-modal correlation learning\narchitecture involving two-branch deep neural networks for audio modality and\ntext modality (lyrics). Different modality data are converted to the same\ncanonical space where inter modal canonical correlation analysis is utilized as\nan objective function to calculate the similarity of temporal structures. This\nis the first study on understanding the correlation between language and music\naudio through deep architectures for learning the paired temporal correlation\nof audio and lyrics. Pre-trained Doc2vec model followed by fully-connected\nlayers (fully-connected deep neural network) is used to represent lyrics. Two\nsignificant contributions are made in the audio branch, as follows: i)\npre-trained CNN followed by fully-connected layers is investigated for\nrepresenting music audio. ii) We further suggest an end-to-end architecture\nthat simultaneously trains convolutional layers and fully-connected layers to\nbetter learn temporal structures of music audio. Particularly, our end-to-end\ndeep architecture contains two properties: simultaneously implementing feature\nlearning and cross-modal correlation learning, and learning joint\nrepresentation by considering temporal structures. Experimental results, using\naudio to retrieve lyrics or using lyrics to retrieve audio, verify the\neffectiveness of the proposed deep correlation learning architectures in\ncross-modal music retrieval.",
  "text": "1\nDeep Cross-Modal Correlation Learning for Audio\nand Lyrics in Music Retrieval\nYi Yu1, Suhua Tang2, Francisco Raposo3, Lei Chen4\n1Digital Content and Media Sciences Research Division, National Institute of Informatics, Tokyo\n2Dept. of Communication Engineering and Informatics, The University of Electro-Communications, Tokyo\n3Instituto Superior T´ecnico, Universidade de Lisboa, Lisbon\n4Department of Computer Science and Engineering, Hong Kong University of Science and Technology\nAbstract—Deep cross-modal learning has successfully demon-\nstrated excellent performances in cross-modal multimedia re-\ntrieval, with the aim of learning joint representations between\ndifferent data modalities. Unfortunately, little research focuses\non cross-modal correlation learning where temporal structures\nof different data modalities such as audio and lyrics are taken\ninto account. Stemming from the characteristic of temporal\nstructures of music in nature, we are motivated to learn the\ndeep sequential correlation between audio and lyrics. In this\nwork, we propose a deep cross-modal correlation learning ar-\nchitecture involving two-branch deep neural networks for audio\nmodality and text modality (lyrics). Different modality data\nare converted to the same canonical space where inter modal\ncanonical correlation analysis is utilized as an objective function\nto calculate the similarity of temporal structures. This is the ﬁrst\nstudy on understanding the correlation between language and\nmusic audio through deep architectures for learning the paired\ntemporal correlation of audio and lyrics. Pre-trained Doc2vec\nmodel followed by fully-connected layers (fully-connected deep\nneural network) is used to represent lyrics. Two signiﬁcant\ncontributions are made in the audio branch, as follows: i) pre-\ntrained CNN followed by fully-connected layers is investigated\nfor representing music audio. ii) We further suggest an end-to-\nend architecture that simultaneously trains convolutional layers\nand fully-connected layers to better learn temporal structures\nof music audio. Particularly, our end-to-end deep architecture\ncontains two properties: simultaneously implementing feature\nlearning and cross-modal correlation learning, and learning joint\nrepresentation by considering temporal structures. Experimental\nresults, using audio to retrieve lyrics or using lyrics to retrieve\naudio, verify the effectiveness of the proposed deep correlation\nlearning architectures in cross-modal music retrieval.\nIndex Terms—Convolutional neural networks, deep cross-\nmodal models, correlation learning between audio and lyrics,\ncross-modal music retrieval, music knowledge discovery\nI. INTRODUCTION\nMusic audio and lyrics provide complementary information\nin understanding the richness of human beings’ cultures and\nactivities [1]. Music1 is an art expression whose medium is\nsound organized in time. Lyrics2 as natural language represent\nmusic theme and story, which are a very important element for\ncreating a meaningful impression of the music. Starting from\nthe late 2014, Google provides music search results containing\nFrancisco was involved in this work during his internship in National\nInstitute of Informatics (NII), Tokyo.\n1https://en.wikipedia.org/wiki/Music\n2https://en.wikipedia.org/wiki/Lyrics\nFig. 1.\nGoogle lyrics for song title “another brick in the wall (part II)”.\nsong lyrics as shown in Fig. 1 when given a speciﬁc song title.\nHowever, searching lyrics in this way is insufﬁcient because\nsometimes people might lack exact song title but know a\nsegment of music audio instead, or want to search an audio\ntrack with part of the lyrics. Then, a natural question arises:\nhow to retrieve the lyrics by a segment of music audio, and\nvice versa?\nSearching lyrics by audio was almost impossible years ago\ndue to the limited availability of large volumes of music audio\nand lyrics. The profusion of online music audio and lyrics\nfrom music sharing websites such as YouTube, MetroLyrics,\nAzlyrics, and Genius shows the opportunity to understand\nmusical knowledge from content-based audio and lyrics by\nleveraging large volumes of cross-modal music data aggre-\ngated in Internet.\nMotivated by the fact that audio content and lyrics are very\nfundamental aspects for understanding what kind of cultures\nand activities a song wants to convey to us, this research\npays attentions to deep correlation learning between audio and\nlyrics for cross-modal music retrieval and considers two real-\nworld tasks: using audio to retrieve lyrics or using lyrics to\nretrieve audio. Several contributions are made in this paper, as\nfollows:\ni) To the best of our knowledge, this work is the ﬁrst\nresearch where a deep correlation learning architecture with\ntwo-branch neural networks and correlation learning model is\nstudied for cross-modal music retrieval by using either audio\nor lyrics as a query modality.\nii) Different music modality data are projected to the shared\nspace where inter modal canonical correlation analysis is\narXiv:1711.08976v2  [cs.IR]  29 Nov 2017\n2\nexploited as an objective function to calculate the similarity\nof temporal structures. Fully-connected deep neural networks\n(DNNs) and an end-to-end DNN are proposed to learn audio\nrepresentation, where the pre-trained Doc2vec model followed\nby fully-connected layers is employed to extract lyrics feature.\niii) Extensive experiments conﬁrm the effectiveness of our\ndeep correlation learning architecture for audio-lyrics music\nretrieval, which are meaningful results and studies for attract-\ning more efforts on mining music knowledge structure and\ncorrelation between different modality data.\nThe rest of this paper is structured as follows. Research\nmotivation and background are introduced in Sec.II. Sec.III\ngives the preliminaries of Convolutional Neural Networks\n(CNNs) and Deep Canonical Correlation Analysis (DCCA).\nThen, Sec.IV presents why and how we exploit CNNs and\nDCCA to build a deep correlation learning architecture for\naudio-lyrics music retrieval. The task of cross-modal music\nretrieval in our work is described in Sec.V. Experimental\nevaluation results are shown in Sec.VI. Finally, conclusions\nare pointed out in Sec.VII.\nII. MOTIVATION AND BACKGROUND\nMusic has permeated our daily life, which contains different\nmodalities in real-world scenarios such as temporal audio\nsignal, lyrics with meaningful sentences, high-level semantic\ntags, and temporal visual content. However, correlation learn-\ning between lyrics and audio for cross-modal music retrieval\nhas not been sufﬁciently studied. Previous works [2], [3],\n[4] mainly focused on content-based music retrieval with\nsingle modality. With the widespread availability of large-scale\nmultimodal music data, it brings us research opportunity to\ntackle cross-modal music retrieval.\nA. Lyrics and Audio in Music\nRecent research has shown that lyrics, audio, or the com-\nbination of audio and lyrics are mainly applied to semantic\nclassiﬁcation such as emotion or genre in music. For example,\nauthors in [5] proposed an unsupervised learning method\nfor mood recognition where Canonical Correlation Analysis\n(CCA) was applied to identify correlations between lyrics and\naudio, and the evaluation of mood classiﬁcation was done\nbased on the valence-arousal space. An interesting corpus with\neach song in the MIDI format and emotion annotation is in-\ntroduced in [6]. Coarse-grained classiﬁcation for six emotions\nis learned by support vector machines (SVM), and this work\nshowed that either textual feature or audio feature can be\nused for emotion classiﬁcation, and their joint use leads to\na signiﬁcant improvement. Emotion lyrics datasets in English\n[7] are annotated with continuous arousal and valence values.\nSpeciﬁc text emotion attributes are considered to complement\nmusic emotion recognition. Experiments on the regression and\nclassiﬁcation of music lyrics by quadrant, arousal, and valence\ncategories are performed. Application of hierarchical attention\nnetwork is proposed in [8] to handle genre classiﬁcation of\nintact lyrics. This network is able to pay attention to words,\nlines, and segments of the song lyrics, where the importance\nof words, lines, and segments in layer structure is learned.\nDistinct from intensive research on music classiﬁcation by\nusing lyrics and audio, our work focuses on audio-lyrics cross-\nmodal music retrieval: using audio to retrieve lyrics or vice\nversa. This is a very natural way for us to retrieve lyrics\nor audio on the Internet. However, no much research has\ninvestigated this task.\nB. Cross-modal Music Retrieval\nSome existing researches on cross-modal music retrieval\nintensively focus on investigating music and visual modal-\nities [9], [10], [11], [12], [13], [14], [15], [16]. Similarity\nbetween audio features extracted from music and image\nfeatures extracted from the album covers are trained by a\nJava SOMToolbox framework in [11]. Then, according to this\nsimilarity, people can organize a music collection and make\nuse of album cover as visual content to retrieve a song over\nmultimodal music data. Based on multi-modal mixture models,\na statistical method to jointly modeling music, images, and text\n[12] is used to support retrieval over a multimodal dataset.\nTo generate a soundtrack for the outdoor video, an effective\nheuristic ranking method is suggested based on heterogeneous\nlate fusion by jointly considering venue categories, visual\nscene, and user listening history [13]. Conﬁdence scores,\nproduced by SVM-hmm models constructed from geographic,\nvisual, and audio features, are combined to obtain different\ntypes of video characteristics. To learn the semantic correlation\nbetween music and video, a novel approach to selecting\nfeatures and statistical novelty based on kernel methods [14]\nis proposed for music segmentation. Co-occurring changes in\naudio and video content of music videos can be detected,\nwhere the correlations can be used in cross-modal audio-visual\nmusic retrieval. Lyrics-based music attributes are utilized for\nimage representation in [16]. Cross-modal ranking analysis is\nsuggested to learn semantic similarity between music and im-\nage, with the aim of obtaining the optimal embedding spaces\nfor music and image. Distinct from intensive research on\nconsidering the use of metadata for different music modalities\nin cross-modal music retrieval, our work focuses on deep\narchitecture based on correlation learning between audio and\nlyrics for content-based cross-modal music retrieval.\nC. Deep Cross-modal Learning\nWe have witnessed several efforts devoted to investigating\ncross-modal learning between different modalities, such as\n[17], [18], [19], [20], [21], [22], to facilitate cross-modal\nmatching and retrieval. Most importantly, latest studies ex-\ntensively pay attention to deep cross-modal learning between\nimage and textual descriptions such as [17], [19], [22], [23].\nMost existing deep models with two-branch sub-networks\nexplore pre-trained convolutional neural network (CNN) [24]\nas image branch [19] and utilize pre-trained document-level\nembedding model [25] or hand-crafted feature extraction such\nas bag of words [17] as text branch. Image and text modalities\nare converted to the joint embedding space calculating a\nsingle ranking loss function by feed-forward way. Image-text\nbenchmarks such as [26], [27] are applied to evaluate the\nperformances of cross-modal matching and retrieval. There\n3\nare two features for existing deep cross-modal retrieval: i)\ncross-modal correlation between image and text is learned\nwithout considering temporal sequences. ii) Pre-trained models\nare directly applied to represent image or text. Distinct from\nexisting deep cross-modal retrieval architectures, this work\ntakes into account temporal sequences to learn the correlation\nbetween audio and lyrics for facilitating audio-lyrics cross-\nmodal music retrieval, where sequential audio and lyrics are\nconverted to the canonical space. A neural network with two-\nbranch sequential structures for audio and lyrics is trained.\nIII. PRELIMINARIES\nWe focus on developing a two-branch deep architecture for\nlearning the correlation between audio and lyrics in the cross-\nmodal music retrieval, where several variants of deep learning\nmodels are investigated for processing audio sequence while\npre-trained Doc2vec [25] is used for processing lyrics. A brief\nreview of CNNs and DCCA exploited in this work is addressed\nin the following.\nA. Convolutional Neural Networks (CNNs)\nCNNs have been exploited to handle not only various tasks\nin the ﬁeld of computer vision and multimedia [28], [29], but\nalso the tasks of music information retrieval such as genre clas-\nsiﬁcation [30], acoustic event detection [31], automatic music\ntagging [32]. Generally speaking, when lacking computational\npower and large annotated datasets, it is preferred to directly\nuse pre-trained CNNs such as VGG16 [28] to extract features\n[20][31], or further combine it with fully-connected layers to\nextract semantic features [19][23][31].\nDifferent from plain spatial convolutional operation, CNN\ntries to use different kernels (ﬁlters) to capture different\nlocal patterns, and this will generate multiple intermediate\nfeature maps (called channels). Speciﬁcally, the convolutional\noperation in one convolutional layer is deﬁned as\nxj = f(\nK−1\nX\nk=0\nHjk ⊗sk + aj),\n(1)\nwhere the superscripts j, k are channel indices, sk is the\nk-th channel input, xj is the j-th channel output, ⊗is the\nconvolutional operation, Hjk is the convolutional kernel (or\nthe ﬁlter) that associates the k-th input channel with the j-\nth output channel, aj is the bias for j-th channel, and f(·)\nis a non-linear activation function. All weights that deﬁne a\nconvolutional layer are represented as a 4-dimensional array\nwith a shape of (h, l, K, J), where h and l determine the kernel\nsize, and K and J are the number of input and output channels,\nrespectively. When mel-spectrogram is used as the input of the\nﬁrst convolutional layer, it only has one channel.\nA 2D convolutional kernel Hjk, as a common ﬁlter, is\napplied to the whole input channel. This kernel is shifted\nalong both (frequency and time) axes and a local correlation\nis computed between the kernel and input. The kernels are\ntrained to ﬁnd local salient patterns that maximize the overall\nobjective. As a kernel sweeps the input, it generates a new\noutput in order, which preserves the spatiality of the input,\ni.e., the frequency and time constraint of the spectrogram.\nConvolutional layers are often followed by pooling layers,\nwhich reduce the size of feature map by down sampling them.\nThe max function is a typical pooling operation. This selects\nthe maximal value from a pooling region, instead of keeping\nall information in the region. This pooling operation also\nenables distortion and translation invariances by discarding\nthe original location of the selected value, and the capability\nof such invariance within each pooling layer is determined\nby the pooling size. With a small pooing size, the network\ndoes not have enough distortion invariance, while a too large\npooling size may completely loose the location of a salient\nfeature. Instead of using a large pooling size in one layer,\nusing multiple small pooling sizes at different pooling layers\nwill enable the system to gradually abstract the features to be\nmore compact and more semantic.\nB. Deep Canonical Correlation Analysis (DCCA)\nCCA has been a very popular method for embedding\nmultimodal data in a shared space. Before presenting our deep\nmultimodal correlation learning between audio and lyrics, we\nﬁrst give an overview of CCA and DCCA.\nLet x ∈Rm (e.g., audio feature) and y ∈Rn (e.g.,\ntextual feature) be zero mean random (column) vectors with\ncovariances Cxx, Cyy and cross-covariance Cxy. When a\nlinear projection is performed, CCA [33] tries to ﬁnd two\ncanonical weights wx and wy, so that the correlation between\nthe linear projections u = wT\nx x and v = wT\ny y is maximized.\n(wx, wy)\n=\nargmax\n(wx,wy)\ncorr(wT\nx x, wT\ny y)\n=\nargmax\n(wx,wy)\nwT\nx Cxywy\nq\nwTx Cxxwx · wTy Cyywy\n. (2)\nOne of the known shortcoming of CCA is that its linear\nprojection may not well model the nonlinear relation between\ndifferent modalities.\nDCCA [34] tries to calculate non-linear correlations be-\ntween different modalities by a combination of DNNs (deep\nneural networks) and CCA. Different from KCCA which\nrelies on kernel functions (corresponding to a logical high\ndimensional (sparse) space), DNN has the extra capability\nof compressing features to a low dimensional (dense) space,\nand then CCA is implemented in the objective function.\nThe DNNs, which realize the non-linear mapping (ϕx(·) and\nϕy(·)), and the canonical weights (wx and wy that model the\nCCA between ϕx(x) and ϕy(y)), are trained simultaneously\nto maximize the correlation after the non-linear mapping, as\nfollows.\n(wx, wy, ϕx, ϕy) =\nargmax\n(wx,wy,ϕx,ϕy)\ncorr(wT\nx ϕx(x), wT\ny ϕy(y)).\n(3)\n4\nIV. DEEP AUDIO-LYRICS CORRELATION LEARNING\nWe develop a deep cross-modal correlation learning ar-\nchitecture that predicts latent alignment between audio and\nlyrics, which enables audio-to-lyrics or lyrics-to-audio music\nretrieval. In this section, we explain how our deep architecture\nis learned. Speciﬁcally, we investigate different deep network\nmodels for correlation analysis and different deep learning\nmethods for audio feature extraction.\nA. Learning Strategy\nOn one hand, lyrics as natural language express semantic\nmusic theme and story; on the other hand, music audio con-\ntains some properties such as tonality and temporal over time\nand frequency. They are correlated in the semantic sense. How-\never, audio and lyrics belong to different modality and cannot\nbe compared directly. Therefore, we extract their features\nseparately, and then map them to the same semantic space for\na similarity comparison. Because linear mapping in CCA does\nnot work well, we design deep networks to realize non-linear\nmapping before CCA. Consequently, deep correlation models\nfor learning temporal structures are considered for representing\nlyrics branch and audio branch.\nWe investigate two deep network architectures. i) Separate\nfeature extraction, completely independent of the following\nDCCA analysis. Text branch follows this architecture, where\nthe pre-trained Doc2vec [25] model is used to compute a\ncompact textual feature vector. As for audio, directly using\nthe pre-trained CNN model [32] belongs to this architecture\nas well. ii) Joint training of audio feature extraction and\nDCCA analysis between audio and lyrics. In this way, feature\nextraction is also correlated with the subsequent DCCA. Here,\nfor the audio branch, a CNN model is trained from the ground\ntogether with the following fully-connected layers, based on\nan end-to-end learning procedure. It is expected that this CNN\nis adapted to the DNN so as to extract more meaningful audio\nfeatures.\nB. Network Architecture\nFigure 2 shows an end-to-end deep convolutional DCCA\nnetwork, which aims at simultaneously learning the feature\nextraction and the deep correlation between audio and lyrics.\nThis model is degenerated to a simple DCCA network, when\nthe CNN model marked in pink dashed line is replaced by a\npre-trained model.\nFrom the sequence of words in the lyrics, textual feature is\ncomputed, more speciﬁcally, by a pre-trained Doc2vec model.\nMusic audio signal is represented as a 2D spectrogram, which\npreserves both its spectral and temporal properties. However,\nit is difﬁcult to directly use this for the DCCA analysis, due to\nits high dimension. Therefore, we investigate two variants for\nthe dimension reduction. (i) Audio feature is extracted by a\npre-trained convolutional model, and we study the pure effect\nof DCCA in analyzing the correlation. i.e., sub DNNs with\nfully connected layers are trained to maximize the correlation\nbetween audio and textual features. (ii) An end-to-end deep\nnetwork for audio branch that integrates convolutional layers\nConvolutional layers\nDoc2vec\nFully-connected layers\nFully-connected layers\nCCA Embedding\nMFCC feature sequence\nFig. 2.\nDeep correlation learning between audio and lyrics.\nfor feature extraction and non-linear mapping for correlation\nlearning together, is trained. In the future work, we will also\nconsider the integration of Doc2Vec with its subsequent DNN.\n1) Audio feature extraction: The audio signal is represented\nas a spectrogram. We mainly focus on mel-frequency cepstral\ncoefﬁcients (MFCCs), because MFCCs are very efﬁcient fea-\ntures for semantic genre classiﬁcation [35] and music audio\nsimilarity comparison [36]. We will also compare MFCC with\nMel-spectrum, which contains more detailed information. To\ncompute a single feature vector for correlation analysis, we\nsuccessively apply convolutional layers with different kernels\nto capture local salient features, and use pooling layers to\nreduce the dimension.\nBy inserting the pooling layer between adjacent convo-\nlutional layers, a kernel in the late layer corresponds to a\nlarger kernel in the previous layer, and has more capacity in\nrepresenting semantic information. Then, using small kernels\nin different convolutional layers can achieve the function of\na large kernel in one convolutional layer, but is more robust\nto scale variance. In this sense, a combination of successive\nconvolutional layers and pooling layers can capture features at\ndifferent scales, and the kernels can learn to represent complex\npatterns.\nFor each audio signal, a slice of 30s is resampled to\n22,050Hz with a single channel. With frame length 2048 and\nstep 1024, there are 646 frames. For the end-to-end learning,\na sequence of MFCCs (20x646) are computed. By initial\nexperiments we found that our approach is not very sensitive to\nthe time resolution. Therefore, we decimate the spectrogram\ninto 4 sub sequences, each with 161 frames and associated\nwith the same lyrics.\nFor implementing an end-to-end deep learning, the conﬁgu-\nration of CNN used for audio branch in this work is shown in\nTable I. It consists of 3 convolutional layers and 3 max pooling\nlayers, and outputs a feature vector with a size of 1536. We\ntried to add more convolutional layer but see no signiﬁcant\ndifference. Rectiﬁed linear unit (ReLU) is used as an activation\nfunction in each convolutional layer except the last one. Batch\n5\nTABLE I\nCONFIGURATION OF CNNS FOR AUDIO BRANCH\nMFCC: 20x646/4\nConvolution, 3x3x48\nMax-pooling (2,2), output 10x80x48\nConvolution: 3x3x96\nMax-pooling (3,3), output 3x26x96\nConvolution: 3x3x192\nMax-pooling (3,3), output 1536\nTABLE II\nSTRUCTURE OF SUB-DNNS\nSub-DNN1 (Audio)\nSub-DNN2 (Text)\n1st layer\n1024, sigmoid\n1024, sigmoid\n2nd layer\n1024, sigmoid\n1024, sigmoid\n3rd layer (output)\nD, linear\nD, linear\nnormalization is used before activation. Convolutional kernels\n(3x3) are used in every convolutional layer. These kernels help\nto learn local spectral-tempo structures. In this way, CNN\nconverts an audio feature sequence (a 2D matrix) to a high\ndimensional vector, and retains some astonishing properties\nsuch as tempo invariances, which can be very helpful for\nlearning musical features in semantic correlation learning\nbetween lyrics and audio.\nWith the input spectrogram s, the feature output by the\nconvolutional layers is x = f3(H3 ⊗f2(H2 ⊗f1(H1 ⊗s +\na1) + a2) + a3), where Hi, ai and fi are the convolutional\nkernel, bias, and activation function in the ith layer.\nAs for the pre-trained model, we apply the pre-trained\nCNN model in [32], which has 5 convolutional layers, each\nwith either average pooling or standard deviation pooling,\ngenerating a 30-dimension vector per layer. Concatenating all\nof them together generates a feature vector of 320 dimension.\n2) Textual feature extraction: Lyrics text of each song\nis tokenized by using coreNLP [37], and passed to the in-\nfer vector module of the Doc2Vec model [25], generating a\n300-dimensional feature for each song. We use the pretrained\napnews dbow weights3 in the experiment.\n3) Non-linear mapping of features: Audio features and\ntextual features are further converted into low dimensional\nfeatures in a shared D-dimensional semantic space by using\ndifferent sub DNNs composed of fully connected layers.\nThe details of sub DNNs are shown in Table II. These two\nsub DNNs (each with 3 fully connected layers) implement the\nnon-linear mapping of DCCA. The audio feature generated\nby the feature extraction part is denoted as x ∈Rm (m\nvaries with each method) and deep textual feature is denoted as\ny ∈R300. The overall functions of sub-DNNs are denoted as\nϕx(x) = g3(Ψ3 ·g2(Ψ2 ·g1(Ψ1x+b1)+b2)+b3), where Ψi\nand bi are the weight matrix and bias for the ith layer and gi(·)\nis the activation function. And ϕy(y) is computed in a similar\nway. Then, ϕx(x) is the overall result of the convolutional\nlayer and its subsequent DNN, given the input spectrogram s.\n4) Objective function of CCA: Assume the batch size in\nthe training is N, X ∈RD×N and Y\n∈RD×N are the\noutputs of sub DNN of the two batches, corresponding to audio\n3https://ibm.ent.box.com/s/9ebs3c759qqo1d8i7ed323i6shv2js7e\n(ϕx(x)) and lyrics (ϕy(y) ), respectively. Let covariance of\nϕx(x) and ϕy(y) be CXX, CY Y and their cross-covariance\nbe CXY . With the linear projection matrices W X and W Y ,\nthe correlation between the canonical components (W T\nXX\nand W T\nY Y ) can be computed. This correlation indicates the\nassociation between the two modalities and is used as an\noverall objective function, which is maximized to ﬁnd all\nparameters (convolutional kernels H(·), non-linear projections\nϕx(·) and ϕy(·), linear projection matrices W X and W Y ).\n(H, W X,W Y ,ϕx,ϕy) =\nargmax\n(H,W X,W Y ,ϕx,ϕy)\ncorr(W T\nXX, W T\nY Y ).\nAt ﬁrst, with H, ϕx, ϕy being ﬁxed, W X and W Y\nare\ncomputed by\n(W X, W Y ) = argmax\n(W X,W Y )\nW T\nXCXY W Y\nq\nW T\nXCXXW X · W T\nY CY Y W Y\n.\nThis can be rewritten in the trace-form\n(W X,W Y ) =argmax\n(W X,W Y )\ntr(W T\nXCXY W Y ),\n(4)\nsubject to : W T\nXCXXW X =W T\nY CY Y W Y =I.\nHere, covariance CXX, CY Y and cross-covariance CXY are\ncomputed as follows\nCXX =\n1\nN −1\nˆX ˆX\nT + rI,\n(5)\nCY Y =\n1\nN −1\nˆY ˆY\nT + rI,\n(6)\nCXY =\n1\nN −1\nˆX ˆY\nT ,\n(7)\nˆX = X −X, ˆY = Y −Y\nwhere X and Y are average of ϕx(x) and ϕy(y) within the\nbatch, and r is a small positive constant used to ensure the\npositive deﬁniteness of CXX and CY Y .\nBy deﬁning T ≜C−1/2\nXX CXY C−1/2\nY Y\nand performing sin-\ngular value decomposition on T as T = UDV T , W X and\nWY can be computed by [34]\nW X = C−1/2\nXX U, W Y = C−1/2\nY Y V .\n(8)\nThen, Eq.(4) can be rewritten as\ntr((W T\nXCXY W Y )T · W T\nXCXY W Y ) = tr(T T T ).\n(9)\nAccordingly, the gradient of the correlation with respect to X\nis given by\n1\nN −1(2∇XX ˆX + ∇XY ˆY ),\n(10)\n∇XX = −1\n2C−1/2\nXX UDU T C−1/2\nXX ,\n6\n∇XY = C−1/2\nXX UV T C−1/2\nY Y .\nAnd the gradient of the correlation with respect to Y can be\ncomputed in a similar way.\nThen, the gradients are back propagated, ﬁrst in the sub\nDNN, where ϕx(x) and ϕx(y) are updated. As for the\naudio branch, the gradients are further back propagated to the\nconvolutional layers, and the kernel ﬁlters H are updated. The\nwhole procedure is shown in Algorithm 1.\nAlgorithm 1 Joint training of CNN and DCCA\n1: procedure JOINTTRAIN(A, L)\n▷A: audio, L: lyrics\n2:\nInitialize convolutional net, sub-networks for mapping\n3:\nCompute MFCC spectrogram from audio A, →ΩA\n4:\nCompute textual feature from lyrics L, →ΩL\n5:\nfor each epoch do\n6:\nRandomly divide ΩA, ΩL to batches\n7:\nfor each batch (ωA, ωL) of audio and lyrics do\n8:\nfor each pair (s, l) ∈(ωA, ωL) do\n9:\ns →x by convolutions\n10:\nl →y by pretrained Doc2Vec model\n11:\nx →ϕx(x) by non-linear mapping\n12:\ny →ϕy(y) by non-linear mapping\n13:\nend for\n14:\nGet converted batch (X, Y )\n15:\nApply CCA on (X, Y ) to compute W X, W Y\n16:\nCompute the gradient with respect to X, Y\n17:\nBack propagate to the sub network\n18:\nBack propagate to the convolutional network\n19:\nend for\n20:\nend for\n21: end procedure\nV. MUSIC CROSS-MODAL RETRIEVAL TASKS\nTwo kinds of retrieval tasks are deﬁned to evaluate the\neffectiveness of our algorithms: instance-level and category-\nlevel. Instance-level cross-modal music retrieval is to retrieve\nlyrics when given music audio as input or vice versa. Category-\nlevel cross-modal music retrieval is to retrieve lyrics or audio,\nsearching most similar audio or lyrics with the same mood\ncategory.\nWith a given input (either audio slice or lyrics), its canonical\ncomponent is computed, and its similarity with the canonical\ncomponents of the other modality in the database is computed\nusing the cosine similarity metric, and the results are ranked\nin the decreasing order of the similarity score.\nVI. EXPERIMENTS\nThe performances of the proposed DCCA variants are\nevaluated and compared with some baselines such as variants\nof CCA and deep multi-view embedding approach [38].\nA. Experiment Setting\nProposed methods. As discussed in Sec. IV, two vari-\nants of DCCA in combination with CNN are investigated:\n1) PretrainCNN-DCCA (the application of DCCA on the\npretrained CNN model [32]), 2) JointTrain-DCCA (the joint\ntraining of CNN and DCCA).\nBaseline methods include some shallow correlation learn-\ning methods (without fully connected layers between feature\nextraction and CCA), such as 3) Spotify-CCA (which ap-\nplies CCA on the 65-dimensional audio features provided by\nSpotify4), 4) PretrainCNN-CCA (which applies CCA on the\nfeatures extracted by the pretrained CNN model), and multi-\nview methods such as 5) Spotify-MVE (Spotify feature with\ndeep multi-view embedding method similar to [38] where\narbitrary mappings of two different views are embedded in the\njoint space based on considering matched pairs with minimal\ndistance and mismatched pairs with maximal distance), 6)\nPretrainCNN-MVE. We also evaluated 7) Spotify-DCCA. In\nall these methods, the lyrics branch uses the features extracted\nby the pretrained Doc2vec model.\nBesides MFCC, we also evaluate the feature of Mel-\nspectrum. The dimension for Mel-spectrum is 96 per frame,\nand there are four convolutional layers, where each of the\nﬁrst three is followed by a max pooling layer, and the\nﬁnal output is 3072 dimension. As for the MVE methods,\nboth branches share the same parameters (activation function,\nnumber of neurons and so on) and both have 3 fully connected\nlayers (with 512, 256, and 128 neurons respectively). Batch\nnormalization is used before each layer and tanh activation\nfunction is applied after each layer.\nAudio-lyrics dataset. Currently, there is no large audio/lyrics\ndataset publically available for cross-modal music retrieval.\nTherefore, we build a new audio-lyrics dataset. Spotify is a\nmusic streaming on-demand service, which provides access\nto over 30 million songs, where songs can be searched by\nvarious parameters such as artist, playlist, and genre. Users can\ncreate, edit, and share playlists on Spotify. Initially, we take 20\nmost frequent mood categories (aggressive, angry, bittersweet,\ncalm, depressing, dreamy, fun, gay, happy, heavy, intense,\nmelancholy, playful, quiet, quirky, sad, sentimental, sleepy,\nsoothing, sweet) [9] as playlist seeds to invoke Spotify API.\nFor each mood category, we ﬁnd the top 500 popular English\nsongs according to the popularity provided by Spotify, and\nfurther crawl 30s audio slices of these songs from YouTube,\nwhile lyrics are collected from Musixmatch. Altogether there\nare 10,000 pairs of audio and lyrics.\nEvaluation metric. In the retrieval evaluation, we use mean\nreciprocal rank 1 (MRR1) and recall@N as the metrics.\nBecause there is only one relevant audio or lyrics, MRR1 is\nable to show the rank of the result. MRR1 is deﬁned by\nMRR1 = 1\nNq\nNq\nX\ni=1\n1\nranki(1),\n(11)\nwhere Nq is the number of the queries and ranki(1) corre-\nsponds to the rank of the relevant item in the ith query. We\n4https://developer.spotify.com/web-api/get-audio-features/\n7\n0\n0.1\n0.2\n0.3\n0\n50\n100\n150\n200\n250\n300\n350\n400\nMRR1\nNumber of epochs\nSpotify\nPretrainCNN\nJointTrain(MFCC)\nJointTrain(Melspec)\nFig. 3.\nMRR1 with respect to the numbers of epochs (Using audio as query\nto search lyrics, #CCA-component=30)\nalso evaluate recall@N to see how often the relevant item is\nincluded in the top of the ranked list. Assume Sq is the set of\nits relevant items (|Sq| = 1) in the database for a given query\nand the system outputs a ranked list Kq (|Kq| = N). Then,\nrecall is computed by\nrecall = |Sq\nTKq|\n|Sq|\n(12)\nand is averaged over all queries.\nWe use 8,000 pairs of audio and lyrics as the training\ndataset, and the rest 2,000 pairs for the retrieval testing.\nBecause we generate 4 sub-sequences from each original\nMFCC sequence, there are 32,000 pairs of audio/lyric pairs\nin JoinTrain. In each run, the split of audio-lyrics pairs into\ntraining/testing is random, and a new model is trained. All\nresults are averaged over 5 runs (cross-validations). In the\nbatch-based training, the batch size is uniﬁed to 1000 samples\nin all methods, and the training takes 200 epochs for Joint-\nTrain and 400 epochs for other DCCA methods. Furthermore,\ntraining MVE requires the presence of non-paired instances.\nTo this end, we randomly selected 1 non-paired instance for\neach song in the dataset. The margin hyper-parameter was set\nto 0.3, according to our preliminary experiments. Then, we\ntrained MVE for 1280 epochs.\nExperiment environment. The evaluations are performed on\na Centos7.2 server, which is conﬁgured with two E5-2620v4\nCPU (2.1GHz), three GTX 1080 GPU (11GB), and DDR4-\n2400 Memory (128G). Moreover, it contains CUDA8.0,\nConda3-4.3 (python 3.5), Tensorﬂow 1.3.0, and Keras 2.0.5.\nB. Performance under Different Numbers of Epochs\nFig.\n3\nshows\nthe\nMRR1\nresults\nof\nSpotify-DCCA,\nPretrainCNN-DCCA,\nJointTrain-DCCA\nwith\nMFCC\nand\nJointTrain-DCCA with Mel-spectrum, under different numbers\nof epochs. In all methods, MRR1 increases with the number\nof epochs, but with different trend. It is clear that MFCC\nhas similar performance as Mel-spectrum, converging much\nfast than the other two methods and achieving higher MRR1.\nHereafter, we only use MFCC as the raw feature for JointTrain.\nC. Impact of the Numbers of CCA Components\nHere, we evaluate the impact of the number of CCA/MVE\ncomponents, which affects the performance of both the base-\nline methods and the proposed methods. The number of\nCCA/MVE components is adjusted from 10 to 100. The results\nof MRR1 and recall of Spotify-CCA are marked as N/A\nwhen the number of CCA components is greater than 65, the\ndimension of Spotify feature.\nThe MRR1 results, with audio feature as query to search\nlyrics, are shown in Table III. Clearly, with the linear CCA,\nSpotify-CCA and PretrainCNN-CCA have poor performance,\nalthough the performance increases with the number of CCA\ncomponents. In comparison, with DCCA, the MRR1 results\nare much improved in Spotify-DCCA and PretrainCNN-\nDCCA. The MRR1 performance increases with the number\nof CCA components, and approaches a constant value in\nPretrainCNN-DCCA. MRR1 decreases a little in Spotify-\nDCCA when the number of CCA components gets greater\nthan 65, the dimension of Spotify feature. Using MVE, the\npeak performance of Spotify-MVE and PretrainCNN-MVE\nlies between that of CCA and DCCA. With the end-to-\nend training, the MRR1 performance is further improved in\nJointTrain-DCCA, and is almost insensitive to the number of\nCCA components. But a further increase in the number of\nCCA components will lead to the SVD failure in CCA.\nTable IV shows the MRR1 results achieved using lyrics as\nquery to search audio in the database, which has a similar\ntrend as in Table III. Generally, when audio and lyrics are\nconverted to the same semantic space, they share the same\nstatistics, and can be retrieved mutually.\nTable V and Table VI show the results of recall@1 and\nresult@5. Recall@N in these tables is only a little greater\nthan MRR1 in Table III and Table IV, which indicates that\nfor most queries, its relevant item either appears at the ﬁrst\nplace, or not in the top-n list at all. This infers that for some\nsongs, lyrics and audio, even after being mapped to the same\nsemantic space, are not similar enough.\nTable VII and Table VIII show the MRR1 results per\ncategory, where the ﬁrst item with the same mood category\nas the query is regarded as relevant. Compared with the\ninstance-level retrieval, the MRR1 result per category is about\n12% larger in all methods, but cannot be improved more by\nincreasing the number of CCA/MVE components. Because\nthere are 20 mood categories, and some mood categories have\nsimilar meaning, this increases the difﬁculty of distinguishing\nsongs in the category level.\nD. Impact of the number of training samples\nHere we investigate the impact of the number of training\nsamples, by adjusting the percentage of samples for training\nfrom 20% to 80%. The percentage of samples for the retrieval\ntest remains 20%, and the number of training samples is\nchosen in such a way that there are the same number of songs\nper mood category.\nFig. 4 and Fig. 5 show the MRR1 results in the instance-\nlevel retrieval. Spotify-CCA and PretrainCNN-CCA do not\nbeneﬁt from the increase of the training samples. Spotify-MVE\n8\nTABLE III\nINSTANCE-LEVEL MRR1 WITH RESPECT TO DIFFERENT NUMBERS OF CCA/MVE COMPONENTS (USING AUDIO AS QUERY)\n#CCA/MVE\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrainCNN-DCCA\nJointTrain-DCCA\n10\n0.023\n0.022\n0.121\n0.166\n0.125\n0.189\n0.247\n20\n0.029\n0.040\n0.134\n0.187\n0.168\n0.225\n0.254\n30\n0.034\n0.054\n0.095\n0.158\n0.183\n0.236\n0.256\n40\n0.039\n0.069\n0.084\n0.115\n0.183\n0.239\n0.256\n50\n0.039\n0.078\n0.067\n0.107\n0.178\n0.237\n0.256\n60\n0.040\n0.085\n0.065\n0.094\n0.177\n0.240\n0.257\n70\nN/A\n0.090\n0.061\n0.085\n0.174\n0.239\n0.256\n80\nN/A\n0.094\n0.056\n0.080\n0.171\n0.237\n0.257\n90\nN/A\n0.098\n0.054\n0.063\n0.164\n0.238\n0.257\n100\nN/A\n0.099\n0.043\n0.072\n0.154\n0.237\n0.257\nTABLE IV\nINSTANCE-LEVEL MRR1 WITH RESPECT TO DIFFERENT NUMBERS OF CCA/MVE COMPONENTS (USING LYRICS AS QUERY)\n#CCA/MVE\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrainCNN-DCCA\nJointTrain-DCCA\n10\n0.022\n0.022\n0.114\n0.157\n0.124\n0.190\n0.248\n20\n0.029\n0.038\n0.119\n0.179\n0.168\n0.225\n0.254\n30\n0.034\n0.053\n0.083\n0.147\n0.184\n0.236\n0.256\n40\n0.038\n0.065\n0.067\n0.100\n0.183\n0.240\n0.254\n50\n0.041\n0.076\n0.056\n0.097\n0.180\n0.236\n0.256\n60\n0.041\n0.083\n0.053\n0.082\n0.176\n0.241\n0.257\n70\nN/A\n0.089\n0.049\n0.074\n0.174\n0.240\n0.256\n80\nN/A\n0.094\n0.048\n0.068\n0.170\n0.237\n0.257\n90\nN/A\n0.099\n0.044\n0.053\n0.163\n0.239\n0.256\n100\nN/A\n0.102\n0.035\n0.062\n0.152\n0.237\n0.256\nTABLE V\nINSTANCE-LEVEL RECALL @ N WITH RESPECT TO DIFFERENT NUMBERS OF CCA COMPONENTS (USING AUDIO AS QUERY)\nSpotify@1\nPretrainCNN@1\nJointTrain@1\nSpotify@5\nPretrainCNN@5\nJointTrain@5\nCCA\nDCCA\nCCA\nDCCA\nDCCA\nCCA\nDCCA\nCCA\nDCCA\nDCCA\n10\n0.006\n0.094\n0.007\n0.160\n0.233\n0.025\n0.150\n0.025\n0.217\n0.257\n20\n0.010\n0.138\n0.020\n0.204\n0.243\n0.034\n0.193\n0.047\n0.243\n0.262\n30\n0.014\n0.155\n0.031\n0.217\n0.245\n0.043\n0.205\n0.068\n0.252\n0.263\n40\n0.019\n0.155\n0.045\n0.221\n0.245\n0.047\n0.205\n0.085\n0.255\n0.262\n50\n0.020\n0.150\n0.053\n0.220\n0.246\n0.049\n0.200\n0.095\n0.250\n0.262\n60\n0.020\n0.151\n0.060\n0.222\n0.246\n0.051\n0.197\n0.102\n0.254\n0.263\n70\nN/A\n0.147\n0.065\n0.222\n0.246\nN/A\n0.197\n0.107\n0.253\n0.263\n80\nN/A\n0.144\n0.068\n0.220\n0.246\nN/A\n0.191\n0.112\n0.250\n0.264\n90\nN/A\n0.137\n0.071\n0.220\n0.247\nN/A\n0.186\n0.120\n0.253\n0.263\n100\nN/A\n0.129\n0.073\n0.220\n0.246\nN/A\n0.175\n0.121\n0.251\n0.263\nTABLE VI\nINSTANCE-LEVEL RECALL @ N WITH RESPECT TO DIFFERENT NUMBERS OF CCA COMPONENTS (USING LYRICS AS QUERY)\nSpotify@1\nPretrainCNN@1\nJointTrain@1\nSpotify@5\nPretrainCNN@5\nJointTrain@5\nCCA\nDCCA\nCCA\nDCCA\nDCCA\nCCA\nDCCA\nCCA\nDCCA\nDCCA\n10\n0.005\n0.090\n0.007\n0.160\n0.235\n0.024\n0.151\n0.022\n0.219\n0.257\n20\n0.009\n0.138\n0.019\n0.204\n0.242\n0.034\n0.193\n0.048\n0.242\n0.261\n30\n0.014\n0.157\n0.031\n0.219\n0.245\n0.042\n0.205\n0.064\n0.250\n0.263\n40\n0.018\n0.155\n0.040\n0.223\n0.244\n0.048\n0.205\n0.081\n0.252\n0.261\n50\n0.021\n0.154\n0.050\n0.218\n0.246\n0.051\n0.199\n0.092\n0.250\n0.262\n60\n0.021\n0.150\n0.057\n0.224\n0.247\n0.051\n0.197\n0.101\n0.254\n0.263\n70\nN/A\n0.147\n0.064\n0.224\n0.245\nN/A\n0.196\n0.108\n0.252\n0.263\n80\nN/A\n0.144\n0.069\n0.221\n0.247\nN/A\n0.190\n0.113\n0.250\n0.264\n90\nN/A\n0.137\n0.072\n0.222\n0.246\nN/A\n0.186\n0.119\n0.253\n0.263\n100\nN/A\n0.126\n0.077\n0.221\n0.247\nN/A\n0.172\n0.121\n0.249\n0.262\nand PretrainCNN-MVE beneﬁts a little. In comparison, when\nDCCA is used, the increase of training samples enables the\nsystem to learn more diverse aspect of audio/lyric features,\nand the MRR1 performance almost linearly increases. In the\nfuture, we will try to crawl more data for training a better\nmodel to improve the retrieval performance.\nThe MRR1 result, with lyrics as query to search audio, as\nshown in Fig. 5, has a similar trend as that in Fig. 4.\nFig. 6 and Fig. 7 show the MRR1 results when the retrieval\nis performed in the category level. This has a similar trend as\nthe result of instance-level retrieval.\nVII. CONCLUSION\nUnderstanding the correlation between different music\nmodalities is very useful for content-based cross-modal music\n9\nTABLE VII\nCATEGORY-LEVEL MRR1 WITH RESPECT TO DIFFERENT NUMBERS OF CCA/MVE COMPONENTS (USING AUDIO AS QUERY)\n#CCA/MVE\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrain-DCCA\nJointTrain-DCCA\n10\n0.177\n0.172\n0.249\n0.286\n0.260\n0.313\n0.364\n20\n0.180\n0.187\n0.265\n0.313\n0.296\n0.344\n0.367\n30\n0.182\n0.199\n0.230\n0.284\n0.307\n0.349\n0.372\n40\n0.187\n0.212\n0.222\n0.246\n0.307\n0.356\n0.368\n50\n0.189\n0.218\n0.211\n0.237\n0.304\n0.358\n0.370\n60\n0.188\n0.225\n0.206\n0.230\n0.302\n0.355\n0.373\n70\nN/A\n0.230\n0.203\n0.221\n0.298\n0.358\n0.370\n80\nN/A\n0.234\n0.196\n0.215\n0.294\n0.352\n0.370\n90\nN/A\n0.235\n0.192\n0.203\n0.294\n0.356\n0.370\n100\nN/A\n0.233\n0.188\n0.208\n0.282\n0.354\n0.374\nTABLE VIII\nCATEGORY-LEVEL MRR1 WITH RESPECT TO DIFFERENT NUMBERS OF CCA/MVE COMPONENTS (USING LYRICS AS QUERY)\n#CCA/MVE\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrain-DCCA\nJointTrain-DCCA\n10\n0.178\n0.170\n0.246\n0.277\n0.256\n0.314\n0.366\n20\n0.176\n0.188\n0.249\n0.304\n0.294\n0.344\n0.368\n30\n0.179\n0.198\n0.222\n0.273\n0.305\n0.351\n0.372\n40\n0.185\n0.208\n0.204\n0.235\n0.307\n0.358\n0.365\n50\n0.191\n0.220\n0.199\n0.228\n0.306\n0.355\n0.373\n60\n0.190\n0.223\n0.195\n0.221\n0.302\n0.356\n0.374\n70\nN/A\n0.231\n0.190\n0.208\n0.298\n0.360\n0.371\n80\nN/A\n0.236\n0.191\n0.205\n0.290\n0.354\n0.370\n90\nN/A\n0.237\n0.186\n0.194\n0.288\n0.356\n0.369\n100\nN/A\n0.238\n0.180\n0.203\n0.280\n0.355\n0.375\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage of samples for training\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nMRR1\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrainCNN-DCCA\nJointTrain-DCCA\nFig. 4.\nInstance-level MRR1 under different percentages of training samples\n(Using audio as query to search text lyrics, #CCA-component=30, 20% for\ntesting)\nretrieval and recommendation. Audio and lyrics are most\ninteresting aspects for storytelling music theme and events.\nIn this paper, a deep correlation learning between audio and\nlyrics is proposed to understand music audio and lyrics.\nThis is the ﬁrst research for deep cross-modal correlation\nlearning between audio and lyrics. Some efforts are made to\ngive a deep study on i) deep models for processing audio\nbranch are investigated such as pre-trained CNN with or\nwithout being followed by fully-connected layers. ii) An\nend-to-end convolutional DCCA is further proposed to learn\ncorrelation between audio and lyrics where feature extrac-\ntion and correlation learning are simultaneously performed\nand joint representation is learned by considering temporal\nstructures. iii) Extensive evaluations show the effectiveness\nof the proposed deep correlation learning architecture where\nconvolutional DCCA performs best when considering retrieval\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage of samples for training\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nMRR1\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrainCNN-DCCA\nJointTrain-DCCA\nFig. 5.\nInstance-level MRR1 under different percentages of training samples\n(Using text lyrics as query to search audio signal, #CCA-component=30, 20%\nfor testing)\naccuracy and converging time. More importantly, we apply our\narchitecture to the bidirectional retrieval between audio and\nlyrics, e.g., searching lyrics with audio and vice versa. Cross-\nmodal retrieval performance is reported at instance level and\nmood category level.\nThis work mainly pays attention to studying deep models\nfor processing music audio while keeping pre-trained Doc2vec\nfor processing lyrics in correlation learning. We are collecting\nmore audio-lyrics pairs to further improve the retrieval per-\nformance, and will integrate different music modality data to\nimplement personalized music recommendation. In the future\nwork, we will investigate some deep models for processing\nlyrics branch. Lyrics contain a hierarchical composition such\nas verse, chorus, bridge. We will extend our deep architec-\nture to complement musical composition (given music audio)\nwhere Long Short Term Memory (LSTM) will be applied for\n10\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage of samples for training\n0\n0.1\n0.2\n0.3\n0.4\nMRR1\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrainCNN-DCCA\nJointTrain-DCCA\nFig. 6.\nCategory-level MRR1 under different percentages of training samples\n(Using audio signal as query to search text lyrics, #CCA-component=30, 20%\nfor testing)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPercentage of samples for training\n0\n0.1\n0.2\n0.3\n0.4\nMRR1\nSpotify-CCA\nPretrainCNN-CCA\nSpotify-MVE\nPretrainCNN-MVE\nSpotify-DCCA\nPretrainCNN-DCCA\nJointTrain-DCCA\nFig. 7.\nCategory-level MRR1 under different percentages of training samples\n(Using text lyrics as query to search audio signal, #CCA-component=30, 20%\nfor testing)\nlearning lyrics dependencies.\nREFERENCES\n[1] B. Nettl, “An ethnomusicologist contemplates universals in musical\nsound and musical culture,” In N. Wallin, B. Merker, and S. Brown,\neditors, The origins of music, MIT Press, Cambridge, MA, pp. 463–472,\n2000.\n[2] Y. Yu, M. Crucianu, V. Oria, and E. Damiani, “Combining multi-\nprobe histogram and order-statistics based lsh for scalable audio content\nretrieval,” in Proceedings of the 18th ACM International Conference on\nMultimedia, ser. MM’10, 2010, pp. 381–390.\n[3] Y. Yu, R. Zimmermann, Y. Wang, and V. Oria, “Scalable content-based\nmusic retrieval using chord progression histogram and tree-structure lsh,”\nIEEE Transactions on Multimedia, vol. 15, no. 8, pp. 1969–1981, 2013.\n[4] Y. Yu, M. Crucianu, V. Oria, and L. Chen, “Local summarization and\nmulti-level lsh for retrieving multi-variant audio tracks,” in Proceedings\nof the 17th ACM International Conference on Multimedia, ser. MM ’09,\n2009, pp. 341–350.\n[5] M. McVicar, T. Freeman, and T. De Bie, “Mining the correlation\nbetween lyrical and audio features and the emergence of mood,” in\n12th International Society for Music Information Retrieval Conference,\nProceedings, ser. ISMIR ’11, 2011, pp. 783–788.\n[6] R. Mihalcea and C. Strapparava, “Lyrics, music, and emotions,” in\nProceedings of the 2012 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language\nLearning, ser. EMNLP-CoNLL ’12, 2012, pp. 590–599.\n[7] R. Malheiro, R. Panda, P. Gomes, and R. P. Paiva, “Emotionally-\nrelevant features for classiﬁcation and regression of music lyrics,” IEEE\nTransactions on Affective Computing, vol. PP, no. 99, pp. 1–1, 2016.\n[8] A. Tsaptsinos, “Lyrics-based music genre classiﬁcation using a hierar-\nchical attention network,” CoRR, vol. abs/1707.04678, 2017.\n[9] Y. Yu, Z. Shen, and R. Zimmermann, “Automatic music soundtrack\ngeneration for outdoor videos from contextual sensor information,” in\nProceedings of the 20th ACM International Conference on Multimedia,\nser. MM ’12, 2012, pp. 1377–1378.\n[10] E. Acar, F. Hopfgartner, and S. Albayrak, “Understanding affective con-\ntent of music videos through learned representations,” in Proceedings of\nthe 20th Anniversary International Conference on MultiMedia Modeling\n- Volume 8325, ser. MMM 2014, 2014, pp. 303–314.\n[11] R. Mayer, “Analysing the similarity of album art with self-organising\nmaps,” in Advances in Self-Organizing Maps - 8th International Work-\nshop, WSOM 2011, Espoo, Finland, June 13-15, 2011. Proceedings,\n2011, pp. 357–366.\n[12] E. Brochu, N. de Freitas, and K. Bao, “The sound of an album\ncover: Probabilistic multimedia and information retrieval,” in Artiﬁcial\nIntelligence and Statistics (AISTATS), 2003.\n[13] R. R. Shah, Y. Yu, and R. Zimmermann, “Advisor: Personalized video\nsoundtrack recommendation by late fusion with heuristic rankings,” in\nProceedings of the 22Nd ACM International Conference on Multimedia,\nser. MM ’14, 2014, pp. 607–616.\n[14] O. Gillet, S. Essid, and G. Richard, “On the correlation of automatic\naudio and visual segmentations of music videos,” IEEE Transactions on\nCircuits and Systems for Video Technology, vol. 17, no. 3, 2007.\n[15] L. Nanni, Y. M. Costa, A. Lumini, M. Y. Kim, and S. R. Baek,\n“Combining visual and acoustic features for music genre classiﬁcation,”\nExpert Syst. Appl., vol. 45, no. C, pp. 108–117, 2016.\n[16] X. Wu, Y. Qiao, X. Wang, and X. Tang, “Bridging music and image\nvia cross-modal ranking analysis,” IEEE Transactions on Multimedia,\nvol. 18, no. 7, pp. 1305–1318, 2016.\n[17] Q. Jiang and W. Li, “Deep cross-modal hashing,” CoRR, vol.\nabs/1602.02255, 2016.\n[18] Y. Cao, M. Long, J. Wang, and S. Liu, “Collective deep quantization\nfor efﬁcient cross-modal retrieval,” in Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017, San\nFrancisco, California, USA., 2017, pp. 3974–3980.\n[19] Y. Yu, S. Tang, K. Aizawa, and A. Aizawa, “Venuenet: Fine-grained\nvenue discovery by deep correlation learning,” in Proceedings of the\n19th IEEE International Symposium on Multimedia, ser. ISM’17, 2017,\npp. –.\n[20] C. Zhong, Y. Yu, S. Tang, S. Satoh, and K. Xing, Deep Multi-label\nHashing for Large-Scale Visual Search Based on Semantic Graph.\nSpringer International Publishing, 2017, pp. 169–184.\n[21] Y. Huang, W. Wang, and L. Wang, “Instance-aware image and\nsentence matching with selective multimodal LSTM,” CoRR, vol.\nabs/1611.05588, 2016.\n[22] Y. Yu, H. Ko, J. Choi, and G. Kim, “Video captioning and retrieval\nmodels with semantic attention,” CoRR, vol. abs/1610.02947, 2016.\n[23] F. Yan and K. Mikolajczyk, “Deep correlation for matching images and\ntext,” in IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2015, 2015, pp. 3441–3450.\n[24] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.\n[25] J. H. Lau and T. Baldwin, “An empirical evaluation of doc2vec with\npractical insights into document embedding generation,” CoRR, vol.\nabs/1607.05368, 2016.\n[26] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,\nP. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick, “Microsoft COCO:\ncommon objects in context,” CoRR, vol. abs/1405.0312, 2014.\n[27] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet,\nR. Levy, and N. Vasconcelos, “A new approach to cross-modal multi-\nmedia retrieval,” in ACM MM’10, 2010, pp. 251–260.\n[28] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” CoRR, vol. abs/1409.1556, 2014.\n[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” Commun. ACM, vol. 60, no. 6,\npp. 84–90, 2017.\n[30] Y. M. Costa, L. S. Oliveira, and C. N. Silla, “An evaluation of convo-\nlutional neural networks for music classiﬁcation using spectrograms,”\nAppl. Soft Comput., vol. 52, no. C, pp. 28–38, 2017.\n[31] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen,\nR. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, M. Slaney,\nR. J. Weiss, and K. Wilson, “Cnn architectures for large-scale audio\nclassiﬁcation,” in 2017 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2017, pp. 131–135.\n[32] K. Choi, G. Fazekas, and M. B. Sandler, “Automatic tagging using deep\nconvolutional neural networks,” CoRR, vol. abs/1606.00298, 2016.\n[33] H. Hotelling, “Relations between two sets of variates,” Biometrika,\nvol. 28, no. 3/4, pp. 321–377, 1936.\n11\n[34] G. Andrew, R. Arora, J. Bilmes, and K. Livescu, “Deep canonical cor-\nrelation analysis,” in Proceedings of the 30th International Conference\non International Conference on Machine Learning - Volume 28, ser.\nICML’13, 2013, pp. III–1247–III–1255.\n[35] S. Sigtia and S. Dixon, “Improved music feature learning with deep\nneural networks,” in 2014 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2014, pp. 6959–6963.\n[36] P. Hamel, M. E. P. Davies, K. Yoshii, and M. Goto, “Transfer learning in\nmir: Sharing learned latent representations for music audio classiﬁcation\nand similarity,” in ISMIR, 2013.\n[37] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard,\nand D. McClosky, “The stanford corenlp natural language processing\ntoolkit,” in Proceedings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics, ACL 2014, 2014, pp. 55–60.\n[38] W. He, W. Wang, and K. Livescu, “Multi-view recurrent neural acoustic\nword\nembeddings,”\nCoRR,\nvol.\nabs/1611.04496,\n2016.\n[Online].\nAvailable: http://arxiv.org/abs/1611.04496\n",
  "categories": [
    "cs.IR",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2017-11-24",
  "updated": "2017-11-29"
}