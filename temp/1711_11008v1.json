{
  "id": "http://arxiv.org/abs/1711.11008v1",
  "title": "Security Risks in Deep Learning Implementations",
  "authors": [
    "Qixue Xiao",
    "Kang Li",
    "Deyue Zhang",
    "Weilin Xu"
  ],
  "abstract": "Advance in deep learning algorithms overshadows their security risk in\nsoftware implementations. This paper discloses a set of vulnerabilities in\npopular deep learning frameworks including Caffe, TensorFlow, and Torch.\nContrast to the small code size of deep learning models, these deep learning\nframeworks are complex and contain heavy dependencies on numerous open source\npackages. This paper considers the risks caused by these vulnerabilities by\nstudying their impact on common deep learning applications such as voice\nrecognition and image classifications. By exploiting these framework\nimplementations, attackers can launch denial-of-service attacks that crash or\nhang a deep learning application, or control-flow hijacking attacks that cause\neither system compromise or recognition evasions. The goal of this paper is to\ndraw attention on the software implementations and call for the community\neffort to improve the security of deep learning frameworks.",
  "text": "Security Risks in Deep Learning Implementations\nQixue Xiao1, Kang Li2, Deyue Zhang1, Weilin Xu3\n1Qihoo 360 Security Research Lab\n2University of Georgia\n3University of Virginia\nAbstract—Advance in deep learning algorithms overshadows\ntheir security risk in software implementations. This paper dis-\ncloses a set of vulnerabilities in popular deep learning frameworks\nincluding Caffe, TensorFlow, and Torch. Contrast to the small\ncode size of deep learning models, these deep learning frameworks\nare complex and contain heavy dependencies on numerous open\nsource packages. This paper considers the risks caused by these\nvulnerabilities by studying their impact on common deep learning\napplications such as voice recognition and image classiﬁcations.\nBy exploiting these framework implementations, attackers can\nlaunch denial-of-service attacks that crash or hang a deep\nlearning application, or control-ﬂow hijacking attacks that cause\neither system compromise or recognition evasions. The goal of\nthis paper is to draw attention on the software implementations\nand call for the community effort to improve the security of deep\nlearning frameworks.\nI.\nINTRODUCTION\nArtiﬁcial intelligence becomes an attention focus in recent\nyears partially due to the success of deep learning applications.\nAdvances in GPUs and deep learning algorithms along with\nlarge datasets allow deep learning algorithms to address real-\nworld problems in many areas, from image classiﬁcation\nto health care prediction, and from auto game playing to\nreverse engineering. Many scientiﬁc and engineering ﬁelds are\npassionately embracing deep learning.\nThese passionate adoptions of new machine learning algo-\nrithms has sparked the development of multiple deep learning\nframeworks, such as Caffe [3], TensorFlow [1], and Torch [6].\nThese frameworks enable fast development of deep learning\napplications. A framework provides common building blocks\nfor layers of a neural network. By using these frameworks,\ndevelopers can focus on model design and application speciﬁc\nlogic without worrying the coding details of input parsing,\nmatrix multiplication, or GPU optimizations.\nIn this paper, we examine the implementation of three\npopular deep learning frameworks: Caffe, TensorFlow, and\nTorch. And we collected their software dependencies based on\nthe sample applications released along with the framework.\nThe implementation of these frameworks are complex (often\nwith hundreds of thousands lines of code) and are often built\nover numerous 3rd party software packages, such as image and\nvideo processing, scientiﬁc computation libraries.\nA common challenge for the software industry is that\nimplementation complexity often leads to software vulnera-\nbilities. Deep learning frameworks face the same challenge.\nThrough our examination, we found multiple dozens of im-\nplementation ﬂaws. Among them, 15 ones have been assigned\nwith CVE numbers. The types of ﬂaws cover multiple common\ntypes of software bugs, such as heap overﬂow, integer overﬂow,\nuse-after-free.\nWe made a preliminary study on the threats and risks\ncaused by these vulnerabilities. With a wide variety of deep\nlearning applications being built over these frameworks, we\nconsider a range of attack surfaces including malformed data\nin application inputs, training data, and models. The potential\nconsequences from these vulnerabilities include denial-of-\nservice attack, evasion to classiﬁcations, and even to system\ncompromise. This paper provides a brief summary of these\nvulnerabilities and the potential risks that we anticipate for\ndeep learning applications built over these frameworks.\nThrough our preliminary study of three deep learning\nframeworks, we make the following contributions:\n• This paper exposes the dependency complexity of popular\ndeep learning frameworks.\n• This paper presents a preliminary study of the attack\nsurface for deep learning applications.\n• Through this paper, we show that multiple vulnerabilities\nexist in the implementation of these frameworks.\n• We also study the impact of these vulnerabilities and\ndescribe the potential security risks to applications built\non these vulnerable frameworks.\nII.\nLAYERED IMPLEMENTATION OF DEEP LEARNING\nAPPLICATIONS\nDeep learning frameworks enable fast development of ma-\nchine learning applications. Equipped with pre-implemented\nneural network layers, deep learning frameworks allow devel-\nopers to focus on application logic. Developers can design,\nbuild, and train scenario speciﬁc models on a deep learning\nframework without worrying about the coding details of input\nparsing, matrix multiplication, or GPU optimizations.\nProgram Logic\nModel\nData\nTensorFlow Caffe\ntheano\nTorch\nNumPy \nGNU LibC \nDL\nFrameworks\nFramework\nDependencies\nDL\nApps\nFig. 1: The Layered Approach for Deep Learning Applications.\nThe exact implementation of deep learning applications\nvaries, but those built on deep learning frameworks are usually\narXiv:1711.11008v1  [cs.CR]  29 Nov 2017\nconsisted of software in three layers. Figure 1 shows the\nlayers of typical deep learning applications. The top layer\ncontains the application logic, the deep learning model and\ncorresponding data resulted from the training stage. These are\ncomponents usually visible to the developers. The middle layer\nis the implementation of the deep learning frameworks, such as\ntensor components and various ﬁlters. The interface between\nthe top two layers are usually speciﬁed in the programming\nlanguage used to implement the middle layer. For examples,\nthe choices of programming language interfaces include C++,\nPython, and Lua for Caffe, TensorFlow, and Torch respectively.\nThe bottom layers are building blocks used by the frameworks.\nThese build blocks are components to accomplish tasks such\nas video and audio processing and model representations (e.g.\nprotobuf). The selection of building blocks varies depending on\nthe design of a framework. For example, TensorFlow contains\nits own implementations of video and image processing built\nover 3rd party packages such as librosa and numpy, whereas\nCaffe chooses to directly use open source libraries, such as\nOpenCV and Libjasper, to parse media inputs. Even the bottom\nand the middle layers are often invisible to the developers of\nthe deep learning applications, these components are essential\npart of deep learning applications.\nTable I provides some basic statistics of the implementa-\ntions of deep learning frameworks. In our study, the versions\nof TensorFlow and Caffe that we analyzed are 1.2.1 and 1.0.\nThe study also include Torch7. As the default Torch package\nonly support limited image formats, we choose to study the\nversion of Torch7 that combines OpenCV [9] that support\nvarious image formats such as bmp, gif, and tiff.\nWe measure the complexity of a deep learning framework\nby two metrics, the lines of code and the number of software\ndependency packages. We count the lines of code by using\nthe cloc tool on Linux. As described in table I, all these\nimplementation’s code bases are not small. Tensorﬂow has\nmore 887 thousands lines of code, Torch has more than 590K\nlines of code, and Caffe has more than 127K. In addition, they\nall depends on numerous 3rd party packages. Caffe is based\non more than 130 depending libraries (measured by the Linux\nldd utility), and Tensorﬂow and Torch depend on 97 Python\nmodules and 48 Lua modules respectively, which was counted\nby the import or require modules.\nTABLE I: DL frameworks and Their Dependencies\nDL\nFramework\nlines of code\nnumber\nof\ndep. package\nsample packages\nTensorﬂow\n887K+\n97\nlibrosa,numpy\nCaffe\n127K+\n137\nlibprotobuf,libz,opencv\nTorch\n590K+\n48\nxlua,qtsvg,opencv\nLayered approach is a common practice for software\nengineering. Layering does not introduce ﬂaws directly, but\ncomplexity in general increases the risks of vulnerabilities.\nAny ﬂaw in the framework or its building components affects\napplications building on it. The next section of this paper\npresents some preliminary ﬁndings of ﬂaws in implementa-\ntions.\nIII.\nVULNERABILITIES AND THREATS\nWhile there are numerous discussion about deep learning\nand artiﬁcial intelligence applications, the security of these\napplications draws less attention. To illustrate the risks and\nthreats related to deep learning applications, we ﬁrst present\nthe attack surfaces of machine learning applications and then\nconsider the type of risks resulted from implementation vul-\nnerabilities.\nA. Attack Surfaces\nWithout losing generality, here we use MNIST handwriting\ndigits [11] recognition as an example to consider the attack\nsurface of deep learning applications. We believe an image\nrecognition application like MNIST can be exploited from the\nfollowing three angles:\n• Attack Surface 1 – Malformed Input Image: Many current\ndeep learning applications, once being trained, usually\nwork on input data for classiﬁcation and recognition pur-\nposes. For an application that read inputs from ﬁles or the\nnetwork, attackers potentially can construct malformed\ninput. This applies to the MNIST image recognition\napplication, which read inputs from ﬁles. The attack\nsurface is signiﬁcantly reduced for applications that take\ninput from a sensor such as a directed connected camera.\nBut the risk of malformed input is not eliminated in those\ncases, and we will discuss it in the next section.\n• Attack Surface 2 – Malformed Training Data: Image\nrecognition applications take training samples, which can\nbe polluted or mislabeled if training data come from\nexternal sources. This is often known as data poisoning\nattack.\nData poisoning attack does not need to rely on software\nvulnerabilities. However, ﬂaws in implementations can\nmake data poisoning easier (or at least harder to be\ndetected). For example, we have observed inconsistency\nof the image parsing procedure in the framework and\ncommon desktop applications (such as image viewer).\nThis inconsistency can enable a sneaky data pollution\nwithout being noticed by people managing the training\nprocess.\n• Attack Surface 3 – Malformed Models: Deep learning\napplications can also be attacked if the developers use\nmodels developed by others. Although many developers\ndesign and build models from scratch, many models\nare made available for developers with less sophisticated\nmachine learning knowledge to use. In such case, these\nmodels becomes potential sources that can be manip-\nulated by attackers. Similar to data poisoning attacks,\nattackers can threat those applications carrying external\nmodels without exploiting any vulnerabilities. However,\nimplementation ﬂaws, such as a vulnerability in the model\nparsing code help attackers to hide malformed models and\nmake the threat more realistic.\nCertainly, the attack surface varies based on each speciﬁc\napplication, but we believe these three attack surfaces cover\nmost of the space from where attackers threat deep learning\napplications.\nB. Type of Threats\nWe have studied several deep learning frameworks and\nfound a dozen of implementation ﬂaws. Table II summarizes a\nportion of these ﬂaws that have been assigned with CVE num-\nbers. These implementation ﬂaws make applications vulnerable\nto a wide range of threats. Due to the space limitation, here\nwe only present the threats caused by malformed input, and\nwe assume the applications take input from ﬁles or networks.\nTABLE II: CVEs Found for DL frameworks and Dependencies\nDL Framework\ndep. packages\nCVE-ID\nPotential Threats\nTensorﬂow\nnumpy\nCVE-2017-12852\nDOS\nTensorﬂow\nwave.py\nCVE-2017-14144\nDOS\nCaffe\nlibjasper\nCVE-2017-9782\nheap overﬂow\nCaffe\nopenEXR\nCVE-2017-12596\ncrash\nCaffe/Torch\nopencv\nCVE-2017-12597\nheap overﬂow\nCaffe/Torch\nopencv\nCVE-2017-12598\ncrash\nCaffe/Torch\nopencv\nCVE-2017-12599\ncrash\nCaffe/Torch\nopencv\nCVE-2017-12600\nDOS\nCaffe/Torch\nopencv\nCVE-2017-12601\ncrash\nCaffe/Torch\nopencv\nCVE-2017-12602\nDOS\nCaffe/Torch\nopencv\nCVE-2017-12603\ncrash\nCaffe/Torch\nopencv\nCVE-2017-12604\ncrash\nCaffe/Torch\nopencv\nCVE-2017-12605\ncrash\nCaffe/Torch\nopencv\nCVE-2017-12606\ncrash\nCaffe/Torch\nopencv\nCVE-2017-14136\ninteger overﬂow\n• Threat 1 – DoS attacks : The most common vulnerabilities\nthat we found in deep learning frameworks are software\nbugs that cause programs to crash, or enter an inﬁnite\nloop, or exhaust all of memory. The direct threat caused\nby such bugs are denial-of-service attacks to applications\nrunning on top of the framework. The list below shows\nthe patch to a bug found in the numpy python package,\nwhich is a building block for the TensorFlow framework.\nThe numpy package is used for matrix multiplication and\nrelated processing. It is commonly used by applications\nbuilt over TensorFlow. The particular bug occurs in the\npad() function, which contains a while loop that would not\nterminate for inputs not anticipated by the developers. The\nﬂaws occur because of the variable safe-pad in the loop\ncondition is set to a negative value when an empty vector\nis passed from a caller. Because of this bug, we showed\nthat popular sample TensoFlow applications, such as the\nUrban Sound Classiﬁcation [7], will hang with special\ncrafted sound ﬁles.\nListing 1: numpy patch example\n--- a/numpy/lib/arraypad.py\n+++ b/numpy/lib/arraypad.py\n@@ -1406,7 +1406,10 @@ def pad(array, pad_width, mode, **kwargs):\nnewmat = _append_min(newmat, pad_after, chunk_after, axis)\nelif mode == ’reflect’:\n-\nfor axis, (pad_before, pad_after) in enumerate(pad_width):\n+\nif narray.size == 0:\n+\nraise ValueError(\"There aren’t any elements to reflect in ’array’!\")\n+\n+\nfor axis, (pad_before, pad_after) in enumerate(pad_width):\n... ...\nmethod = kwargs[’reflect_type’]\nsafe_pad = newmat.shape[axis] - 1\nwhile ((pad_before > safe_pad) or (pad_after > safe_pad)):\n... ...\n• Threat 2 – Evasion attacks: Evasion attacks occur when\nan attacker can construct inputs that should be classiﬁed\nas one category but being misclassiﬁed by deep learning\napplications as a different category. Machine learning\nresearchers have spent a considerable amount of research\neffort on generating evasion input through adversarial\nlearning methods [5, 10]. When faced with vulnerable\ndeep learning framework, attackers can instead achieve\nthe goal of evasion by exploiting software bugs. We\nfound multiple memory corruption bugs in deep learn-\ning frameworks that can potentially cause applications\nto generate wrong classiﬁcation outputs. Attackers can\nachieve evasion by exploiting these bugs in two way: 1)\noverwriting classiﬁcation results through vulnerabilities\nthat given attackers ability to modify speciﬁc memory\ncontent, 2) hijacking the application control ﬂow to skip\nor reorder model execution.\nThe list below shows an out-of-bounds write vulnerability\nand the corresponding patch. The data pointer could be\nset to any value in the readData function, and then a\nspeciﬁed data could be written to the address pointed by\ndata. So it can potentially overwrite classiﬁcation results.\nListing 2: OpenCV patch example\nbool\nBmpDecoder::readData( Mat& img )\n{\nuchar* data = img.ptr();\n....\nif( m_origin &=&\nIPL_ORIGIN_BL )\n{\ndata += (m_height - 1)*(size_t)step; // result an out bound write\nstep = -step;\n}\n....\nif( color )\nWRITE_PIX( data, clr[t] );\nelse\n*data = gray_clr[t];\n....\n}\nindex 3b23662..5ee4ca3 100644\n--- a/modules/imgcodecs/src/loadsave.cpp\n+++ b/modules/imgcodecs/src/loadsave.cpp\n+\n+static Size validateInputImageSize(const Size& size)\n+{\n+\nCV_Assert(size.width > 0);\n+\nCV_Assert(size.width <= CV_IO_MAX_IMAGE_WIDTH);\n+\nCV_Assert(size.height > 0);\n+\nCV_Assert(size.height <= CV_IO_MAX_IMAGE_HEIGHT);\n+\nuint64 pixels = (uint64)size.width * (uint64)size.height;\n+\nCV_Assert(pixels <= CV_IO_MAX_IMAGE_PIXELS);\n+\nreturn size;\n+}\n@@ -408,14 +426,26 @@ imread_( const String& filename, int flags, int hdrtype, Mat* mat=0 )\n// established the required input image size\n-\nCvSize size;\n-\nsize.width = decoder->width();\n-\nsize.height = decoder->height();\n+\nSize size = validateInputImageSize(Size(decoder->width(), decoder->height()));\n• Threat 3 – System Compromise: For software bugs that\nallows an attacker to hijack control ﬂow, attackers can\npotentially leverage the software bug and remotely com-\npromise the system that hosts deep learning applications.\nThis occurs when deep learning applications run as a\ncloud service to input feed from the network.\nThe list below shows a patch to a simple buffer overﬂow\nfound in the OpenCV library. The OpenCV library is\na computer vision library which designed for computa-\ntional efﬁciency and with a strong focus on real-time\napplications. OpenCV supports the deep learning frame-\nworks, such as TensorFlow, Torch/PyTorch and Caffe.\nThe buffer overﬂow occurs in the readHeader function\nin grfmt_bmp.cpp. The variable m_palatte represents a\nbuffer whose size is 256*4 bytes, however, the value\nof clrused is from an input image which can be set to\nan arbitrary value by attackers. Therefore, a malformed\nBMP image could result to buffer overﬂow from the get-\nBytes() call. Through our investigation, this vulnerability\nprovides the ability to make arbitrary memory writes and\nwe have successfully forced sample programs (such as\ncpp_classiﬁcation [2] in Caffe) spawning a remote shell\nbased on our crafted image input.\nWhile doing this work, we found another group of re-\nsearchers [8] that have also studied the vulnerabilities\nand impact of OpenCV on machine learning applications.\nAlthough their idea of exploring OpenCV for system\ncompromise shares a similar goal with our effort, they\ndid not ﬁnd or release vulnerabilities that are conﬁrmed\nby OpenCV developers [4]. In contrast, our ﬁndings have\nbeen conﬁrmed by corresponding developers and many\nof them have been patched based on our suggestion.\nIn addition, we have also developed proof-of-concept\nexploitation that has successfully demonstrated remote\nsystem compromise (by remotely gaining a shell) through\nthe vulnerabilities found by us.\nListing 3: OpenCV patch example\nindex 86cacd3..257f97c 100644\n--- a/modules/imgcodecs/src/grfmt_bmp.cpp\n+++ b/modules/imgcodecs/src/grfmt_bmp.cpp\n@@ -118,8 +118,9 @@ bool\nBmpDecoder::readHeader()\nif( m_bpp <= 8 )\n{\n-\nmemset( m_palette, 0, sizeof(m_palette));\n-\nm_strm.getBytes( m_palette, (clrused == 0? 1<<m_bpp : clrused)*4 );\n+\nCV_Assert(clrused < 256);\n+\nmemset(m_palette, 0, sizeof(m_palette));\n+\nm_strm.getBytes(m_palette, (clrused == 0? 1<<m_bpp : clrused)*4 );\niscolor = IsColorPalette( m_palette, m_bpp );\n}\nelse if( m_bpp == 16 && m_rle_code == BMP_BITFIELDS )\nIV.\nDISCUSSION AND FUTURE WORK\nThe previous section presents software vulnerabilities in\nthe implementations of deep learning frameworks. These vul-\nnerabilities are only a set of factors that affect the overall ap-\nplication security. There are multiple other factors to consider,\nsuch as where does an application take input from, whether\ntraining data are well formatted, that also affect the security\nrisks. We brieﬂy discussed a few related issues here.\nA. Security Risks for Applications in Closed Environments\nMany sample deep learning applications are designed to\nbe used in a closed environment, in which the application\nacquires input directly from sensors closely coupled with the\napplication. For example, the machine learning implementation\nrunning on a camera only takes data output from the built-in\ncamera sensor. Arguably the risk of malformed input is lower\nthan an application takes input from network or ﬁles controlled\nby users. However, a closely coupled sensor does not eliminate\nthreats of malformed input. For example, there are risks\nassociated with sensor integrity, which can be compromised.\nIf the sensor communicates with a cloud server where the\ndeep learning applications run, attackers could reverse the\ncommunication protocol and directly attack the backend.\nB. Detect Vulnerabilities in Deep Learning Applications\nWe applied traditional bug ﬁnding methods, especially\nfuzzing, to ﬁnd the software vulnerabilities presented in this\npaper. We expect all conventional static and dynamic analysis\nmethods apply to the deep learning framework implementation.\nHowever, we found that coverage-based fuzzing tools are not\nideal for testing deep learning applications, especially for\ndiscovering errors in the execution of models. Taking the\nMNIST image classiﬁer as an example, almost all images cover\nthe same execution path because all inputs go through the same\nlayers of calculation. Therefore, simple errors such as divide-\nby-zero would not be easily found by coverage-based fuzzers\nsince the path coverage feedback is less effective in this case.\nC. Security Risks due to Logical Errors or Data Manipulation\nOur preliminary work focused on the “conventional” soft-\nware vulnerabilities that lead to program crash, control ﬂow\nhijacking or denial-of-service. It is interesting to consider if\nthere are types of bugs speciﬁc to deep learning and need\nspecial detection methods. Evasion attack or data poisoning\nattack do not have to relies on conventional software ﬂaws such\nas buffer overﬂow. It is enough to create an evasion if there are\nmistakes allowing training or classiﬁcation to use more data\nthan what an application suppose to have. The mismatch of\ndata consumption can be caused by a small inconsistency in\ndata parsing between the framework implementation and the\nconventional desktop software.\nOne additional challenge for detecting logical errors in\ndeep learning applications is the difﬁculty to differentiate\ninsufﬁcient training from intended manipulation, which targets\nto have a particular group of inputs misclassiﬁed. We plan to\ninvestigate methods to detect such type of errors.\nV.\nCONCLUSION\nThe purpose of this work is to raise awareness of the\nsecurity threats caused by software implementation mistakes.\nDeep Learning Frameworks are complex software and thus\nit is almost unavoidable for them to contain implementation\nbugs. This paper presents an overview of the implementation\nvulnerabilities and the corresponding risks in popular deep\nlearning frameworks. We discovered multiple vulnerabilities\nin popular deep learning frameworks and libraries they use.\nThe types of potential risks include denial-of-service, evasion\nof detection, and system compromise. Although closed appli-\ncations are less risky in terms of their control of the input,\nthey are not completely immune to these attacks. Considering\nthe opaque nature of deep learning applications which buries\nthe implicit logic in its training data, the security risks caused\nby implementation ﬂaws can be difﬁcult to detect. We hope\nour preliminary results in this paper can remind researchers to\nnot forget conventional threats and actively look for ways to\ndetect ﬂaws in the software implementations of deep learning\napplications.\nREFERENCES\n[1] Gardener and Benoitsteiner, “An open-source software library for Machine Intelli-\ngence,” https://www.tensorﬂow.org/, 2017.\n[2] Y. Jia, “Classifying ImageNet: using the C++ API,” https://github.com/BVLC/caffe/\ntree/master/examples/cpp_classiﬁcation, 2017.\n[3] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama,\nand T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,” arXiv\npreprint arXiv:1408.5093, 2014.\n[4] Opencv Developers, “Opencv issue 5956,” https://github.com/opencv/opencv/issues/\n5956, 2017, accessed 2017-09-03.\n[5] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami,\n“Practical black-box attacks against machine learning,” in Proceedings of the 2017\nACM on Asia Conference on Computer and Communications Security, ser. ASIA\nCCS ’17.\nNew York, NY, USA: ACM, 2017, pp. 506–519.\n[6] Ronan, Clément, Koray, and Soumith, “Torch: A SCIENTIFIC COMPUTING\nFRAMEWORK FOR LUAJIT,” http://torch.ch/, 2017.\n[7] A. Saeed, “Urban Sound Classiﬁcation,” https://devhub.io/zh/repos/aqibsaeed-\nUrban-Sound-Classiﬁcation, 2017.\n[8] R. Stevens, O. Suciu, A. Ruef, S. Hong, M. W. Hicks, and T. Dumitras,\n“Summoning demons: The pursuit of exploitable bugs in machine learning,” CoRR,\nvol. abs/1701.04739, 2017. [Online]. Available: http://arxiv.org/abs/1701.04739\n[9] VisionLabs, “OpenCV bindings for LuaJIT+Torch,” https://github.com/VisionLabs/\ntorch-opencv, 2017.\n[10] W. Xu, Y. Qi, and D. Evans, “Automatically evading classiﬁers,” in Network and\nDistributed System Security Symposium, 2016.\n[11] L. Yann, C. Corinna, and J. B. Christopher, “The MNIST Database of handwritten\ndigits,” http://yann.lecun.com/exdb/mnist/, 2017.\n",
  "categories": [
    "cs.CR"
  ],
  "published": "2017-11-29",
  "updated": "2017-11-29"
}