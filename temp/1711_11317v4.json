{
  "id": "http://arxiv.org/abs/1711.11317v4",
  "title": "Unsupervised Learning for Cell-level Visual Representation in Histopathology Images with Generative Adversarial Networks",
  "authors": [
    "Bo Hu",
    "Ye Tang",
    "Eric I-Chao Chang",
    "Yubo Fan",
    "Maode Lai",
    "Yan Xu"
  ],
  "abstract": "The visual attributes of cells, such as the nuclear morphology and chromatin\nopenness, are critical for histopathology image analysis. By learning\ncell-level visual representation, we can obtain a rich mix of features that are\nhighly reusable for various tasks, such as cell-level classification, nuclei\nsegmentation, and cell counting. In this paper, we propose a unified generative\nadversarial networks architecture with a new formulation of loss to perform\nrobust cell-level visual representation learning in an unsupervised setting.\nOur model is not only label-free and easily trained but also capable of\ncell-level unsupervised classification with interpretable visualization, which\nachieves promising results in the unsupervised classification of bone marrow\ncellular components. Based on the proposed cell-level visual representation\nlearning, we further develop a pipeline that exploits the varieties of cellular\nelements to perform histopathology image classification, the advantages of\nwhich are demonstrated on bone marrow datasets.",
  "text": "1\nUnsupervised Learning for Cell-level Visual\nRepresentation in Histopathology Images with\nGenerative Adversarial Networks\nBo Hu♯, Ye Tang♯, Eric I-Chao Chang, Yubo Fan, Maode Lai and Yan Xu*\nAbstract—The visual attributes of cells, such as the nuclear\nmorphology and chromatin openness, are critical for histopathol-\nogy image analysis. By learning cell-level visual representation,\nwe can obtain a rich mix of features that are highly reusable for\nvarious tasks, such as cell-level classiﬁcation, nuclei segmentation,\nand cell counting. In this paper, we propose a uniﬁed generative\nadversarial networks architecture with a new formulation of loss\nto perform robust cell-level visual representation learning in an\nunsupervised setting. Our model is not only label-free and easily\ntrained but also capable of cell-level unsupervised classiﬁcation\nwith interpretable visualization, which achieves promising results\nin the unsupervised classiﬁcation of bone marrow cellular com-\nponents. Based on the proposed cell-level visual representation\nlearning, we further develop a pipeline that exploits the varieties\nof cellular elements to perform histopathology image classiﬁca-\ntion, the advantages of which are demonstrated on bone marrow\ndatasets.\nKeywords—unsupervised learning, representation learning, gen-\nerative adversarial networks, classiﬁcation, cell.\nI.\nINTRODUCTION\nH\nISTOPATHOLOGY images are considered to be the\ngold standard in the diagnosis of many diseases [1].\nIn many situations, the cellular components are an important\ndeterminant. For example, in the biopsy sections of bone mar-\nrow, the abnormal cellular constitution indicates the presence\nof blood disease [2]. Bone marrow is the key component\nof both the hematopoietic system and the lymphatic system\nThis work is supported by the Technology and Innovation Commission of\nShenzhen in China under Grant shenfagai2016-627, Microsoft Research under\nthe eHealth program, the National Natural Science Foundation in China under\nGrant 81771910, the National Science and Technology Major Project of the\nMinistry of Science and Technology in China under Grant 2017YFC0110903,\nthe Beijing Natural Science Foundation in China under Grant 4152033, Beijing\nYoung Talent Project in China, the Fundamental Research Funds for the\nCentral Universities of China under Grant SKLSDE-2017ZX-08 from the\nState Key Laboratory of Software Development Environment in Beihang\nUniversity in China, the 111 Project in China under Grant B13003. * indicates\ncorresponding author; ♯indicates equal contribution.\nBo Hu, Ye Tang, Yubo Fan and Yan Xu are with the State Key Lab-\noratory of Software Development Environment and the Key Laboratory\nof Biomechanics and Mechanobiology of Ministry of Education and Re-\nsearch Institute of Beihang University in Shenzhen and Beijing Advanced\nInnovation Centre for Biomedical Engineering, Beihang University, Beijing\n100191, China (email: bohu1996@gmail.com; yetang1995@gmail.com; yubo-\nfan@buaa.edu.cn; xuyan04@gmail.com).\nMaode Lai is with the Department of Pathology, School of Medicine,\nZhejiang University (email: lmd@zju.edu.cn).\nEric I-Chao Chang, and Yan Xu are with Microsoft Research, Beijing\n100080, China (email: echang@microsoft.com; v-yanx@microsoft.com).\nFig. 1.\nExamples of ﬁve types of cellular elements in bone marrow: (a)\ngranulocytes precursors such as myeloblasts, (b) cells with dark, dense, and\nclose phased nuclei, the candidates of which are most likely lymphocytes and\nnormoblasts, (c) granulocytes such as neutrophils, (d) monocytes, and (e)\nmegakaryocytes. Five types of cells can be distinguished by the chromatin\nopenness, the density of nuclei, and if nuclei show the appearance of being\nsegmented. Megakaryocytes appear the least often, as well are the most\ndistinguished due to their massive size.\n(a) abnormal\n(b) normal\nFig. 2.\nExamples of bone marrow images sliced from Whole Slide Images\n(WSI). Too many myeloblasts in (a) indicate the presence of blood disease.\nby producing large amounts of blood cells. The cell lines\nundergoing maturation in the marrow mostly include myeloid\ncells (granulocytes, monocytes, megakaryocytes, and their\nprecursors), erythroid cells (normoblasts), and lymphoid cells\n(lymphocytes and their precursors). Figure 1 are examples\nof ﬁve main cellular components in bone marrow. These\ncomponents are signiﬁcant to both the systemic circulation and\nthe immune system. Several kinds of cancer are characterized\narXiv:1711.11317v4  [cs.CV]  7 Jul 2018\n2\nby the cellular constitution in bone marrow [2]. For instance,\ntoo many granulocytes precursors such as myeloblasts indicate\nthe presence of chronic myeloid leukemia. Having large,\nabnormal lymphocytes heralds the presence of lymphoma.\nFigure 2 shows the difference between normal and abnormal\nbone marrow histopathology images from the perspective of\ncells.\nAs described above, cell-level information is irreplaceable\nfor histopathology image analysis. Cell-level visual attributes\nsuch as the morphological features of nuclei and the open-\nness of chromatin are helpful for various tasks such as cell-\nlevel classiﬁcation and nuclei segmentation. We deﬁne cell-\nlevel images as the output from nuclei segmentation. Each\ncell-level image contains only one cell. We opt to perform\nrepresentation learning on these cell-level images, in which the\nvisual attributes such as the nuclei morphology and chromatin\nopenness are distinguished. The learned features are further\nutilized to assist tasks such as cell counting to highlight the\nquantiﬁcation of certain types of cells.\nTo achieve this, the main obstacle is the labeling of cells.\nThere are massive amounts of cells in each histopathology\nimage, which makes manual labeling ambiguous and laborious.\nTherefore, an unsupervised cell-level visual representation\nlearning method based on unlabeled data is believed to be more\nreasonable than fully supervised methods. Unsupervised cell-\nlevel visual representation learning is known to be difﬁcult.\nFirst, geometrical and morphological appearances of cells\nfrom the same category can have a distinct diversity due to\nfactors such as cell cycles. Furthermore, the staining conditions\nof histopathology images can be pretty diverse, resulting in\ninconsistent color characteristics of nuclei and cytoplasm.\nRecently, deep learning has been proven to be powerful\nin histopathology image analysis such as classiﬁcation [3],\n[4], segmentation [5], [6], and detection [7], [8]. Generative\nAdversarial Networks (GANs) [9] are a class of generative\nmodels that use unlabeled data to perform representation\nlearning. GAN is capable of transforming noise variables\ninto visually appealing image samples by learning a model\ndistribution that imitates the real data distribution. Several\nGAN architectures such as Deep Convolutional Generative\nAdversarial Nets (DCGAN) [10] have proven their advantages\nin various natural images datasets. Recently, Wasserstein-GAN\n(WGAN) [11] and WGAN with gradient penalty (WGAN-GP)\n[12] have greatly improved the stability of training GAN. More\ncomplex network structures such as residual networks [13] can\nnow be fused into GAN models.\nMeanwhile, Information Maximizing Generative Adversar-\nial Networks (InfoGAN) [14] makes a modiﬁcation that en-\ncourages GAN to learn interpretable and meaningful represen-\ntations. InfoGAN maximizes the mutual information between\nthe chosen random variables and the observations to make\nvariables represent interpretable semantic features. The prob-\nlem is that InfoGAN utilizes a DCGAN architecture, which\nrequires meticulous attention towards hyperparameters. For our\nproblem, it suffers a severe convergence problem.\nInspired by WGAN-GP and InfoGAN, we present an unsu-\npervised representation learning method for cell-level images\nusing a uniﬁed GAN architecture with a new formulation\nof loss, which inherits the superiority from both WGAN-\nGP and InfoGAN. We observe great improvements followed\nby the setting of WGAN-GP. Introducing mutual information\ninto our formulation, we are capable of learning interpretable\nand disentangled cell-level visual representations, as well as\nallocate cells into different categories according to their most\nsigniﬁcant semantic features. Our method achieves promising\nresults in the unsupervised classiﬁcation of bone marrow\ncellular components.\nBased on the cell-level visual representations, the quantiﬁ-\ncation of each cellular component can be obtained by the\ntrained model. Followed by this, cell proportions for each\nhistopathology image can then be calculated to assist image-\nlevel classiﬁcation. We further develop a pipeline combining\ncell-level unsupervised classiﬁcation and nuclei segmentation\nto conduct image-level classiﬁcation of histopathology images,\nwhich shows its advantages via experimentations on bone\nmarrow datasets.\nThe contributions of this work include the following: (1) We\npresent an unsupervised framework to perform cell-level visual\nrepresentation learning using generative adversarial networks.\n(2) A uniﬁed GAN architecture with a new formulation of\nloss is proposed to generate representations that are both high-\nquality and interpretable, which also endows our model the ca-\npability of cell-level unsupervised classiﬁcation. (3) A pipeline\nis developed that exploits the varieties of cell-level elements to\nperform image-level classiﬁcation of histopathology images.\nII.\nRELATED WORKS\nA. Directly Related Works\n1) Generative Adversarial Networks: Goodfellow et al. [9]\npropose GANs, a class of unsupervised generative models\nconsisting of a generator neural network and an adversarial\ndiscriminator neural network. While the generator is encour-\naged to produce synthetic samples, the discriminator learns to\ndiscriminate between generated and real samples. This process\nis described as a minimax game. Radford et al. [10] propose\none of the most frequently used GAN architectures DCGAN.\nArjovsky et al. [11] propose WGAN, which modiﬁes the\nobjective function, securing the training process to be more\nstable. For regular GANs, the training process optimizes a\nlower bound of the Jensen-Shannon (JS) divergence between\nthe generator distribution and the real data distribution. WGAN\nmodiﬁes this by optimizing an approximation of the Earth-\nMover (EM) distance. The only challenge is how to enforce\nthe Lipschitz constraint on the discriminator. While Arjovsky\net al. [11] use weight-clipping, Gulrajani et al. [12] propose\nWGAN-GP, which adds a gradient penalty on the discrimi-\nnator. For our bone marrow datasets, even if we have tried\nmultiple hyperparameters, DCGAN still suffers from a severe\nconvergence difﬁculty. While DCGAN leads to the failure for\nour datasets, WGAN-GP greatly eases this problem.\nChen et al. [14] introduce mutual information into GAN\narchitecture. Mutual information describes the dependencies\nbetween two separate variables. Maximizing mutual informa-\ntion between the chosen random variables and the generated\n3\nsamples, InfoGAN produces representations that are meaning-\nful and interpretable. To exploit the varieties of cellular compo-\nnents, the superior ability of InfoGAN in learning disentangled\nand discrete representations is what a regular GAN lacks.\nTherefore, we propose a uniﬁed GAN architecture with a\nnew formulation of loss, which inherits the superiority of\nboth WGAN-GP and InfoGAN. The outstanding stability of\nWGAN-GP eases the difﬁculty in tuning the complicated\nhyperparameters of InfoGAN. Introducing mutual information\ninto our model, we are capable of learning interpretable cell-\nlevel visual representations, as well as allocate cells into\ndifferent categories according to their most signiﬁcant semantic\nfeatures.\n2) Classiﬁcation of Blood Disease: Nazlibilek et al. [15]\npropose a system to help automatically diagnose acute lympho-\ncytic leukemia. This system consists of several stages: nuclei\nsegmentation, feature extraction, cell-level classiﬁcation, and\ncell counting. In their future work, they claim that the result\nof cell counting can be used for further diagnosis of acute\nlymphocytic leukemia.\nIn our work, we design a similar workﬂow which consists\nof nuclei segmentation, cell-level classiﬁcation, and image-\nlevel classiﬁcation. Our advantages lie in the novelty of an\nunsupervised setting and the convincing performance of image-\nlevel classiﬁcation based on the calculated cell proportions.\nB. Cell-level Representation\nThe representation of individual cells can be used for a\nvariety of tasks such as cell classiﬁcation. Traditional cell-\nlevel visual representation for classiﬁcation tasks can be cate-\ngorized into four categories [16]: morphological [17], texture\n[18], [19], intensity [20], and cytology features [21]. These\ntraditional methods have been employed in the representation\nof white blood cells [22], [23], [24]. However, the features used\nabove need to be manually designed by experienced experts\naccording to the characteristics of different types of cells.\nWhile images suffer from a distinct variance, discovering,\ncharacterizing and selecting good handcraft features can be\nextremely difﬁcult.\nTo remedy the limitations of manual features in cell classi-\nﬁcation, Convolutional Neural Network (CNN) learns higher-\nlevel latent features, whose convolution layer can act as a\nfeature extractor [25]. Xie et al. [26] propose Deep Embedding\nClustering (DEC) that simultaneously learns feature represen-\ntations and cluster assignments using deep neural networks.\nVariational Autoencoder (VAE) [27] serves as a convincing\nunsupervised strategy in cell-level visual representation learn-\ning [28], [29], [30]. However, how to use VAE to learn cate-\ngorical and discrete latent variables is still under investigation.\nDilokthanakul et al. [31] and Jiang et al. [32] design models\ncombining VAE with Gaussian Mixture Model (GMM). But\nthey demonstrate their experiment on one-dimensional datasets\nsuch as MNIST. To perform clustering and embedding on a\nhigher-dimensional dataset, their methods still need a feature\nextractor.\nGANs such as Categorical GAN [33] can merge categorical\nvariables into the model with little effort, which makes learned\nrepresentations disentangled and interpretable. This ability is\ncritical in medical image analysis where accountability is\nespecially needed.\nC. Cell-level Histopathology Image Analysis\n1) Classiﬁcation: Cell classiﬁcation has been performed in\ndiverse histopathology related works such as breast cancer\n[34], acute lymphocytes leukemia [35], [36], and colon cancer\n[37].\nBased on the result of cell classiﬁcation, some approaches\nhave been proposed to determine the presence or location of\ncancer [21], [38]. In prostate cancer, Nguyen et al. [21] innova-\ntively employ cell classiﬁcation for automatic cancer detection\nand grading. They distinguish the cancer nuclei and normal\nnuclei, which are combined with textural features to classify\nthe image as normal or cancerous and then detect and grade\nthe cancer regions. In the diagnosis of Glioma, Hou et al. [38]\napply CNN to the classiﬁcation of morphological attributes\nof nuclei. They also claim that the nuclei classiﬁcation result\nprovides clinical information for diagnosing and classifying\nglioma into subtypes and grades. Zhang et al. [39], [40], [41]\nand Shi et al. [42] use either supervised or semi-supervised\nhashing models for cell-level analysis.\nAll of these works require a large amount of accurately an-\nnotated data. Obtaining such annotated data is time-consuming\nand labor-intensive while GAN can optimally leverage the\nwealth of unlabeled data.\n2) Segmentation: Nuclei segmentation is of great impor-\ntance for cell-level classiﬁcation. Nuclei segmentation methods\ncan be roughly categorized as follows: intensity thresholding\n[43], [44], morphology operation [45], [46], deformable mod-\nels [47], watershed transform [48], clustering [49], [50], and\ngraph-based methods [51], [52]. The methods above have been\nbroadly applied to the segmentation of white blood cells.\nD. Generative Adversarial Networks in Medical Images\nRecently, several works involving GAN have gathered great\nattention in medical image analysis.\nIn medical image synthesizing, Nie et al. [53] estimate the\nCT image from its corresponding MR image with context-\naware GAN. In medical image reconstruction, Li et al. [54]\nuse GAN to reconstruct medical images with the thinner sliced\nthickness from regular thick-slice images. Mahapatra et al.\n[55] propose a super resolution method that takes a low-\nresolution input fundus image to generate a high-resolution\nsuper-resolved image. Wolterink et al. [56] employ GAN to\nreduce the noise in low-dose CT images. All these recent works\ndemonstrate the great potential of GAN in solving complicated\nmedical problems.\nIII.\nMETHODS\nIn this section, we ﬁrst introduce an unsupervised method\nfor cell-level visual representation learning using GAN. Then\nwe present the details of how image-level classiﬁcation is\nperformed on histopathology images based on cell-level rep-\nresentation.\n4\n(a) Training process. Random variables are composed of Gaussian variables z\nand the discrete variable c. Besides playing the minimax game between the\ngenerator (G) and the discriminator (D) through the EM distance, we also\nminimize the negative Log-likelihood between c and the output of the auxiliary\nnetwork (Q(c|G(c, z)) to maximize mutual information.\n(b) Test process. Real samples are classiﬁed into ﬁve categories by the auxiliary\nnetwork Q. At the same time, fake samples are generated by giving noises with\nthe chosen c for each class. In the example of generated samples (fake), one\nrow contains ﬁve samples from the same category in c, and a column shows\nthe generated images for 5 possible categories in c with z ﬁxed.\n(c) Illustration of residual blocks (resblocks) in the architecture. There are three\ndifferent types of residual blocks considering whether they include nearest-\nneighbor upsampling or mean pooling for downsampling. Batch normalization\nlayers are used in our generator to help stabilize training.\nFig. 3.\nNetwork architecture of our cell-level visual representation learning:\n(a) Training process. (b) Test process. (c) The architecture of residual blocks\n(written as resblock in (a) and (b)).\nA. Cell-level Visual Representation Learning\nGiven cell-level images that come from nuclei segmenta-\ntion as the real data, we deﬁne a generator network G, a\ndiscriminator network D, and an auxiliary network Q. The\narchitecture of these networks are shown in Figure 3. In the\ntraining process, we learn a generator distribution that matches\nthe real data distribution by playing a minimax game between\nG and D by optimizing an approximation of the Earth-Mover\n(EM) distance. Meanwhile, we maximize mutual informa-\ntion between the chosen random variables and the generated\nsamples using an auxiliary network Q. In the test process,\nthe generator generates the representations for each category\nof cells according to different values of the chosen random\nvariables. Cell images can be allocated to the corresponding\ncategories by the auxiliary network Q.\n1) Training Process: Given cell-level images sampled from\nthe real data distribution x ∼Pr, the ﬁrst goal is to learn a\ngenerator distribution Pg that matches the real data distribution\nPr.\nWe ﬁrst deﬁne a random noise variable z. The input noise z\nis transformed by the generator into a sample ˜x = G(z), z ∼\np(z). ˜x can be viewed as following the generator distribution\nPg. Inspired by WGAN [11], we optimize networks through\nthe WGAN objective W(Pr, Pg):\nW(Pr, Pg) =\nsup\n∥f∥L≤1\nEx∼Pr[f(x)] −E˜x∼Pg[f(˜x)].\n(1)\nW(Pr, Pg) is an efﬁcient approximation of the EM distance,\nwhich is constructed using the Kantorovich-Rubinstein duality\n[11]. The EM distance measures how close the generator\ndistribution and the data distribution are. To distinguish two\ndistributions Pg and Pr, the adversarial discriminator network\nD is trained to learn the function f that maximizes W(Pr, Pg).\nTo make Pg approach Pr, the generator instead is trained to\nminimize W(Pr, Pg). The value function V (D, G) is written\nas follows:\nV (D, G) = Ex∼Pr[D(x)] −Ez∼p(z)[D(G(z))].\n(2)\nThis minimax game between the generator and the discrimi-\nnator is written as:\nmin\nG max\nD∈D V (D, G).\n(3)\nFollowed by the work of WGAN-GP [12], a gradient penalty\nis added on the discriminator to enforce the Lipschitz con-\nstraint to make sure that the discriminator lies within the space\nof 1-Lipschitz functions D ∈D. The loss of the discriminator\nwith a hyperparameter λ1 is written as:\nLD = Ez∼p(z)[D(G(z))] −Ex∼Pr[D(x)] + λ1Eˆx∼Pˆx[||∇ˆxD(ˆx)||p −1]2,\n(4)\nwhere Pˆx is deﬁned sampling uniformly along straight lines\nbetween pairs of points sampled from the data distribution Pr\nand the generator distribution Pg.\nIn this way, our model is capable of generating visually\nappealing cell-level images. But still, it fails to exploit infor-\nmation of categories of cells since the noise variable z doesn’t\ncorrespond to any interpretable feature. Motivated by this, our\nsecond goal is to make the chosen variables represent mean-\ningful and interpretable semantic features of cells. Inspired\nby InfoGAN [14], we introduce mutual information into our\nmodel:\nI(X; Y ) = H(X) −H(X|Y ) = H(Y ) −H(Y |X).\n(5)\nI(X; Y ) describes the dependencies between two separate\nvariables X and Y . It measures the different aspects of\n5\nFig. 4.\nOverview of our pipeline as follows: (a) Nuclei segmentation is performed on histopathology images. (b) Using the trained GAN architecture, Cell-level\nclustering is performed using the learned auxiliary network Q. Cell proportions are then calculated for each histopathology image. (c) Image-level prediction is\ngiven based on cell proportions. (d) For visualization, the generator G can generate the interpretable representation for each category of cells by changing the\nnoises.\nthe association between two random variables. If the chosen\nrandom variables correspond to certain semantic features,\nit’s reasonable to assume that mutual information between\ngenerated samples and random variables should be high.\nWe deﬁne a latent variable c sampled from a ﬁxed noise\ndistribution p(c). The concatenation of the random noise\nvariable z and the latent variable c is then transformed by\nthe generator G into a sample G(z, c). Since we encourage\nthe latent variable to correspond with meaningful semantic\nfeatures, there should be high mutual information between c\nand G(z, c). Therefore, the next step is to maximize mutual\ninformation I(c; G(z, c)), which can be written as:\nI(c; G(z, c)) = H(c) −H(c|G(z, c)).\n(6)\nFollowed by this, a lower bound LI is given by:\nLI(G, Q) = Ez∼p(z),c∼p(c)[log Q(c|G(z, c))] + H(c),\n(7)\nwhere H(c) is the entropy of the variable sampled from a ﬁxed\nnoise distribution. Maximizing this lower bound, we maximize\nmutual information I(c; G(z, c)). The proof can be found in\nInfoGAN [14].\nSince we introduce the latent variable c into the model, the\nvalue function V (D, G) is replaced by:\nV (D, G) ←Ex∼Pr[D(x)] −Ez∼p(z),c∼p(c)[D(G(z, c))]. (8)\nAs we combine the adversarial process with the process of\nmaximizing mutual information, this information-regularized\nminimax game with a hyperparameter λ2 can be written as\nfollows:\nmin\nG,Q max\nD∈D V (D, G) −λ2LI(G, Q).\n(9)\nThe loss of D can be replaced by:\nLD ←Ez∼p(z),c∼p(c)[D(G(z, c))] −Ex∼Pr[D(x)] + λ1Eˆx∼Pˆx[||∇ˆxD(ˆx)||p −1]2,\n(10)\nSince H(c) can be viewed as a constant, the loss of the\nauxiliary network Q can be written as the negative log-\nlikelihood between Q(c|G(c, z)) and the discrete variable c.\nThe losses of G and Q can be interpreted as below:\nLG = −Ez∼p(z),c∼p(c)[D(G(z, c))],\n(11)\nLQ = −λ2Ez∼p(z),c∼p(c)[log Q(c|G(z, c))].\n(12)\nFigure 5 shows how noises are transformed into interpretable\nsamples during the training process.\nFig. 5.\nExample of how a set of noise vectors are transformed into\ninterpretable image samples over generator iterations. We use a 5-dimensional\ncategorical variable c and 32 Gaussian noise variables z as input. Different\nrows correspond to different values of z. Different columns correspond to\ndifferent values of c. The value of c largely corresponds to cell types.\n2) Test Process: In the training process, a generator dis-\ntribution is learned to imitate the real data distribution. An\nauxiliary distribution is learned to maximize the lower bound.\nEspecially if c is sampled from a categorical distribution, a\nsoftmax function is applied as the ﬁnal layer of Q. Under this\ncircumstance, Q can act as a classiﬁer in the test process, since\nthe posterior Q(c|x) is discrete. Assuming that each category\nin c corresponds to a type of cells, the auxiliary network Q\ncan divide cell-level images into different categories while the\ngenerator G can generate the interpretable representation for\neach category of cells.\nB. Image-level Classiﬁcation\nBased on the cell-level visual representation learning, we\npropose a pipeline combining nuclei segmentation and cell-\nlevel visual representation to highlight the varieties of cellular\nelements. Image-level classiﬁcation is performed using the\ncalculated cell proportions. The illustration of this pipeline is\nshown in Figure 4.\n1) Nuclei Segmentation: An unsupervised nuclei segmenta-\ntion approach is ultilized consisting of four stages: normaliza-\ntion, unsupervised color deconvolution, intensity thresholding\n6\nFig. 6.\nOverview of segementation process: (a) the cropped image, (b)\nthe normalized image, (c) the separated hematoxylin stain image using color\ndeconvolution, (d) the binary image generated by intensity thresholding, (e)\nthe labeled image after postprocessing where different grayscale values stand\nfor different segmented instances, and (f) the ﬁnal segmentation image.\nand postprocessing to segment nuclei from the background.\nFigure 6 is an overview of our segmentation pipeline.\nColor Normalization: We employ Reinhard color normal-\nization [57] to convert the color characteristics of all images\ninto the desired standard by computing the mean and standard\ndeviations of a target image in LAB space.\nColor Deconvolution: Using the PCA-based ‘Macenko’\nmethod [58], unsupervised color deconvolution is performed\nto separate the normalized image into two stains. We project\npixels onto a best-ﬁt plane, wherein it selects the stain vectors\nas percentiles in the ‘angle distribution’ of the corresponding\nplane. With the correct stain matrix for color deconvolution,\nthe normalized image can be separated into hematoxylin stain\nand eosin stain.\nIntensity Thresholding: To sufﬁciently segment cells, we\napply intensity thresholding in the hematoxylin stain image\nwhere the intensity distribution of cells is consistently distinct\nfrom the background. By converting the hematoxylin stain\nimage into a binary image with a constant global threshold,\nthe cells are roughly segmented.\nPostprocessing: In image postprocessing, objects with\nfewer pixels than the minimum area threshold will be removed\nfrom the binary image. Then we employ the method in [44]\nto remove thin protrusions from cells. Furthermore, we use\nopening operation to separate a few touched cells.\n2) Classiﬁcation: We utilize the model distribution trained\nin our unsupervised representation learning as the cell-level\nclassiﬁer. Assuming that we use a k-dimensional categorical\nvariable as the chosen variable in the training process, the real\ndata (cell-level images) distribution is allocated into k dimen-\nsions. In the test process, cell-level images are unsupervised\nclassiﬁed into k corresponding categories.\nFor each histopathology image, we count the numbers of\ncell-level instances in each category as the representation of\nits cellular constitution, denoted as {X1, X2, X3, . . . , Xk}. For\ncellular element i, the ratio of the number of this cellular\nelement to the total number of the cellular constitution in this\nimage is calculated by Pi =\nXi\nPk\ni=1 Xi . We deﬁne Pi as the cell\nproportion of cellular element i.\nGiven cell proportions {P1, P2, P3, . . . , Pk} as the feature\nvector of histopathology images, we utilize either k-means or\nSVM to give image-level predictions.\nIV.\nEXPERIMENTS AND RESULTS\nA. Dataset\nAll our experiments are conducted on bone marrow\nhistopathology images stained with hematoxylin and eosin. As\ndescribed before, the cellular constitution in bone marrow is a\ndeterminant in diagnoses of blood disease.\nDataset A: Publicly available dataset [59] which consists\nof eleven images of healthy bone marrow with a resolution of\n1200×1200 pixels. Each image contains around 200 cells. The\nwhole dataset includes 1995 cell-level images in total. We label\nall cell-level images into four categories: 34 neutrophils, 751\nmyeloblasts, 495 monocytes, and 715 lymphocytes. Images are\ncarefully labeled by two pathologists. When the two patholo-\ngists disagree on a particular image, a senior pathologist makes\na decision over the discord.\nDataset B: Dataset provided by the First Afﬁliated Hospital\nof Zhejiang University which contains whole slides of bone\nmarrow from 24 patients with blood diseases. Each patient\nmatchs with one whole slide. We randomly crop 29 images\nwith a resolution of 1500 × 800 pixels from all whole slides.\nDataset B contains around 12000 cells in total. For this\ndataset, we label 600 cell-level images into three categories\nfor evaluation: 200 myeloblasts, 200 monocytes, and 200\nlymphocytes. The labeling process is conducted in the same\nmanner as Dataset A.\nDataset C: Combination of Datasets A and B, which results\nin 29 abnormal and 11 normal histopathology images.\nDataset D: Dataset includes whole slides from 28 patients\nwith bone marrow hematopoietic tissue hyperplasia (negative)\nand 56 patients with leukemia (positive). Each patient matchs\nwith one whole slide. We randomly crop images with a reso-\nlution of 1500 × 800 pixels from all whole slides. This results\nin 72 negative and 132 positive images. After segmentation,\nDataset D contains around 80000 cells in total.\nB. Implementation\nNetwork Parameters: Our generator G, discriminator D\nand auxiliary network Q all have the structures of residual\nnetworks. In the training process, all three networks are\nupdated by Adam optimizer (α = 0.0001, β1 = 0.5, β2 = 0.9,\nlr = 2×10−4) [61] with a batch size of 64. All our experiments\nuse hyperparameters λ1 = 10 and λ2 = 1. For each training\niteration, we update D, G and Q in turn. One training iteration\nconsists of ﬁve discriminator iterations, one generator iteration,\nand one auxiliary network iteration. For each training process,\nwe augment the training set by rotating images with angles\n90◦, 180◦, 270◦. We train ten epochs for our model in each\nexperiment.\nNoise Sources: The noise fed into the network is the combi-\nnation of a 5-dimensional categorical variable and 32 Gaussian\nnoise variables for the training of Dataset A or Dataset B. We\nuse the combination of a 5-dimensional categorical variable\nand 64 Gaussian noise variables for Dataset C.\nSegmentation Parameters: The mean value of the standard\nimage in three channels is [8.98±0.64, 0.08±0.11, 0.02±0.03]\nfor color normalization. Vectors for color deconvolution are\npicked from 1% to 99% angle distribution while the magnitude\nbelow 16 is excluded from the computation. We use the\nthreshold value of 120 for intensity thresholding. In the post-\nprocess, objects with pixels smaller than 200 will be removed.\nAn opening operation with 7 × 7 kernel size is performed to\n7\nseparate touched cells. When the edge of the bounding box of\na cell-level image is larger than 32 pixels, we rescale the image\nto make the larger edge match to 32. Each cell is centered in a\n32×32 pixel image where blank is ﬁlled with [255, 255, 255].\nBounding Box: To prevent the color and texture contrast\nfrom troubling the feature extraction process, we use instances\nwithout segmentation for baseline methods. If we depose the\nnuclei in the center with the loose bounding box in the same\nmanner as our previous experiments, cells will suffer from\nsevere overlapping. Thus, we crop the minimum bounding box\nregion along each segmented instance, and then resize it into\n32 × 32 pixels as our dataset.\nSoftware: We implement our experiments on framework\nPytorch for deep learning models and framework HistomicsTK\nfor nuclei segmentation. Our model is compared with multiple\nsources of baselines. Three main types of baselines are claimed\nto be relevant as follows: (1) feature extractors including\nmanual features, HOG and DNN extractor; (2) supervised\nclassiﬁers including SVM and DNN; (3) clustering algorithms\nincluding DEC and K-means. The rich mix of different sources\nof baselines, including deep learning algorithms, provides\na stronger demonstration to our experiments. We utilize k-\nmeans++ [60] to choose the initial values when using k-\nmeans to perform clustering. The feature code1 is Python\nimplementation in all these algorithms.\nHardware: For hardware, we use one pair of Tesla K80\nGPU for parallel training and testing of neural network models.\nOther baseline experiments are conducted on Intel(R) Xeon(R)\nCPU E5-2690 v3 @ 2.60GHz. For our model, with a batch size\nof 64, using one pair of K80 GPU for parallel computation,\neach generator iteration costs 3.2 seconds in the training\nprocess when each batch costs 0.18 seconds in the test process.\nC. Cell-level Classiﬁcation Using Various Features\nTo demonstrate the quality of our representation learning, we\napply the trained model as a feature extractor. The experiment\nis conducted on Dataset A. In this experiment, 1596 cell-level\nimages are used for training; 399 cell-level images are used\nfor testing.\nComparison: (1) MF: 188-dimensional manual feature\ncombined of SIFT [62], LBP [63], and L × a × b color\nhistogram. (2) DNN: DNN+k-means: DNN features extracted\nby ResNet-50 trained on Imagenet-1K, on top of which k-\nmeans is performed. (3) Our Method: We downsample the\nfeatures after each residual block of the discriminator into\na 4 × 4 spatial grid using max pooling. These features are\nﬂattened and concatenated to form an 8192-dimensional vector.\nOn top of the feature vectors, an L2-SVM is trained to perform\nclassiﬁcation.\nDifferent processing strategies are used as follows: (1) w/\nSeg: using the output generated by nuclei segmentation; (2)\nw/o Seg: using the minimum bounding box along each cell-\nlevel instance.\nEvaluation: For each class, we denote the number of true\npositives TP, the number of false positives FP and the\n1Implementation details can be found at https://github.com/bohu615/nu gan\nnumber of false negatives FN. The precision, recall and F-\nscore (F1) for each class are deﬁned as follows:\nprecision =\nTP\nTP + FP ,\nrecall =\nTP\nTP + FN ,\nF1 = 2 · precision · recall\nprecision + recall .\n(13)\nThe average precision, recall and F-socre are calculated\nweighted by support (the number of true instances of each\nclass).\nResults: We randomly choose correctly classiﬁed and mis-\nclassiﬁed samples displayed in Figure 7. The comparison of\nresults is shown as Table I, which proves the advantages of our\nrepresentation learning method. The manual feature extractor\ncan generate a better result based on the bounding box regions,\nbut its performance is still lower than ours. The color of\nthe background can provide useful information for the color\nhistogram channel in manual features but is viewed as noise for\nthe DNN based extractor. Though the dimensions of the feature\nvectors of our method are higher, the clustering ability of our\nmodel ensures further unsupervised applications. Furthermore,\nwe apply mean pooling on top of feature maps to prove that\nusing less dimensional features can also generate a comparable\nresult. In this manner, we achieve 0.850 F-score using 2048\ndimensional features and 0.840 F-score using 512 dimensional\nfeatures.\nFig. 7. Visualization of cell-level classiﬁcation performed on Dataset A: (up)\ncorrectly classiﬁed samples and (down) misclassiﬁed samples. misclassiﬁed\nsamples can be illegible for pathologists either.\nTABLE I.\nPerformance of cell-level classiﬁcation using various features.\nMethods\nPrecision\nRecall\nF-score\nw/ Seg\nw/o Seg\nw/ Seg\nw/o Seg\nw/ Seg\nw/o Seg\nMF\n0.821\n0.837\n0.803\n0.847\n0.811\n0.842\nDNN\n0.838\n0.760\n0.817\n0.769\n0.827\n0.764\nOur Method\n0.865\n/\n0.848\n/\n0.857\n/\nD. Cell-level Clustering\nAs the priority of image-level classiﬁcation of histopathol-\nogy images, cell-level clustering is performed using the trained\nauxiliary network Q. We conduct experiments on the three\ndatasets described in Section IV-A.\nComparison: (1) MF+k-means: Manual features with k-\nmeans. (2) DNN+k-means: DNN features extracted by ResNet-\n50 trained on Imagenet-1K, on top of which k-means is\nperformed. (3) HOG+DEC: Deep Embedded Clustering (DEC)\n[26] on 2048-dimensional HOG features. (4) Our Method:\nCell images are unsupervised allocated to ﬁve clusters by the\nauxiliary network Q. We also test models such as Categorical\n8\nGAN (CatGAN) [33], InfoGAN (under DCGAN architecture),\nand Gaussian Mixture VAE (GMVAE) [31] on our datasets\nunder different hyperparameters, but ﬁnd them fail to converge.\nThe following processing strategies are also used: (1) w/\nSeg: using the output generated by nuclei segmentation; (2)\nw/o Seg: using the minimum bounding box along each cell-\nlevel instance.\nEvaluation: We evaluate the performance of clustering us-\ning the average F-score, purity, and entropy. For the set of clus-\nters {ω1, ω2, . . . , ωK} and the set of classes {c1, c2, . . . , cJ},\nwe assume that each cluster ωk is assigned to only one class\nargmaxj(|ωk ∩cj|). The F-score for class cj is then given by\nEquation 13. The average F-score is given calculated by the\nnumber of true instances in each class.\nPurity and Entropy are also used as evaluation metrics,\nwhich are written as follows:\npurity = 1\nN\nX\nk\nmax\nj\n|ωk ∩cj|,\nentropy = −1\nN\nX\nk\n|ωk| log |ωk|\nN .\n(14)\nLarger purity and smaller entropy indicate better clustering\nresults.\nFor nuclei segmentation, we use Intersection over Union\n(IoU) and the F-score as evaluation metrics. A segmented\ninstance (I) is matched with the ground truth (G) only if\nthey intersect at least 50% (i.e., |I ∩G| > 0.5G). For each\nmatched instance and its ground truth, the overlapping pixels\nare counted as true positive (TP). The pixels of instance\nremain unmatched are counted as false positive (FP) while\nthe pixels of ground truth remaining unmatched are counted\nas false negative (FN). The F-score is then calculated using\nEquation 13.\nFor k-means based methods, the average F-score is approxi-\nmately the same (±0.02) using either four, ﬁve, or six clusters.\nAnnotations: To evaluate the capability of nuclei segmen-\ntation, We randomly choose 20 patches from Dataset C with\na resolution of 200 × 200 pixels. The ground truth is care-\nfully labeled by two pathologists. When the two pathologists\ndisagree on a particular image, a senior pathologist makes a\ndecision over the discord.\nResults: For nuclei segmentation, our method achieves 0.56\nmean IoU and 0.70 F-score.\nFor cell-level clustering, the comparison shown as Table II\nshows the superiority of our method. To explicitly reveal\nthe semantic features our model has captured, we randomly\nchoose 60 samples from each of the ﬁve clusters displayed\nin Figure 8, which shows a distinct consistency within each\ncluster. Reasonable interpretations can be given. Cells are clus-\ntered according to the semantic features such as the chromatin\nopenness, the darkness and density of nuclei, and if nuclei\nshow the appearance of being segmented.\nWhen it comes to unsupervised classiﬁcation, none of the\nbaseline methods can beneﬁt from the bounding box. We ob-\nserve that the color context of the background can be disturbing\nwhen the classiﬁcation is under the fully unsupervised manner.\nTABLE II.\nPerformance of cell-level clustering.\nDataset\nMethods\nPurity\nEntropy\nF-score\nw/ Seg\nw/o Seg\nw/ Seg\nw/o Seg\nw/ Seg\nw/o Seg\nA\nMF+k-means\n0.579\n0.442\n1.376\n1.598\n0.603\n0.510\nDNN+k-means\n0.667\n0.470\n1.256\n1.552\n0.677\n0.501\nHOG+DEC\n0.729\n0.637\n1.086\n1.167\n0.737\n0.664\nOur Method\n0.855\n/\n0.750\n/\n0.863\n/\nB\nMF+k-means\n0.392\n0.421\n1.561\n1.545\n0.409\n0.454\nDNN+k-means\n0.719\n0.406\n0.844\n1.557\n0.760\n0.435\nHOG+DEC\n0.771\n0.681\n0.697\n1.161\n0.812\n0.693\nOur Method\n0.874\n/\n0.431\n/\n0.841\n/\nC\nMF+k-means\n0.459\n0.446\n1.533\n1.597\n0.484\n0.514\nDNN+k-means\n0.578\n0.458\n1.377\n1.575\n0.601\n0.485\nHOG+DEC\n0.667\n0.602\n1.217\n1.334\n0.682\n0.621\nOur Method\n0.769\n/\n0.977\n/\n0.777\n/\nEspecially for Dataset A, Figure 9(a) shows the convergence\nof V (D, G) (see Equation (8)) and LQ (see Equation (12)).\nV (D, G) is used to evaluate how well the generator distribu-\ntion matches the real data distribution [12]. LQ approaching\nzero indicates that mutual information is maximized [14].\nFigure 9(b) shows how the purity of clustering increases in\nthe training process.\n(a)\n(b)\nFig. 9.\nVisualization of cell-level clustering performed on Dataset A: (a)\nTraining losses converge as the network trains. (b) The purity increases\ngradually over generator iterations.\nImpacts of the Number of Clusters: For our method, it\nis easy to change the number of clusters by sampling the\ncategorical noise from a different dimension. We compare the\nresults of choosing different numbers of clusters shown in\nTable III, which shows there is no distinct difference between\nchoosing four and ﬁve clusters. We choose ﬁve clusters (a\n5-dimensional categorical random variable) in change for a\nslightly better performance.\nTABLE III.\nPerformance when choosing different numbers of clusters.\nClusters\n4\n5\n6\nF-score\n0.831\n0.863\n0.789\nImpacts of Uninformative Representations: The uninfor-\nmative representations such as the staining color and rotations\ncan be interference factors in the process of classiﬁcation.\nBesides using color normalization and data augmentation to\nease this problem, we also demonstrate that these features are\nmore likely to be latent encoded in Gaussian random variables\nwhich do not inﬂuence the classiﬁcation task. As is shown in\nFigure 10, we ﬁx the value of the chosen categorical variable c\nwhile walking through the random space of the Gaussian noise\nvariable z. The result shows that uninformative representations\ntend to be encoded in noise variables through the process of\nmaximizing the mutual information.\n9\nFig. 8.\nVisualization of clustering. We randomly select 60 samples from each one of ﬁve clusters, displayed as (a) to (e). Instances in the same cluster have a\ndistinct consistency. In (b), cells in marrow with dark, dense, and close phased nuclei tend to be lymphocytes or erythroid precursors. In (c) and (e), cells with\ndispersed chromatin are most likely granulocytes precursors such as myeloblasts.\n(a)\n(b)\nFig. 10.\nExamples of how uninformative representations are encoded in\nGaussian noise variables z. Different columns share the same value of the\nchosen categorical variable c. A random walk is performed between two points\nin the space of z. It can be seen that (a) the staining color and (b) the rotation\nare both latent encoded in the Gaussian noise variables.\nE. Image-level Classiﬁcation\nWe\nperform\nimage-level\nclassiﬁcation\nexperiments\non\nDataset C and Dataset D respectively. Dataset C includes\n29 positive and 11 negative images. Dataset D includes 132\npositive and 72 negative images. Each dataset is randomly split\ninto four folds for the 4-fold cross-validation. Each score is\nreported averagely. Each experiment is repeated for four times\nwith different random split for cross-validation. The scores are\nreported four times to show conﬁdence intervals.\nComparison: (1) DNN (cell-level based): We use ResNet-\n50 features extracted from cell-level instances to perform\n10\ncell-level clustering. Then we train an L2-SVM on top of\nthe cell proportions to perform image-level classiﬁcation. (2)\nDNN (image-level based): We use ResNet-50 pre-trained on\nImagenet-1K as an image-level feature extractor. Images with\na resolution of 1500 × 800 are normalized and center cropped\nto 800 × 800 pixels, then resized into 224 × 224 pixels. An\nL2-SVM is trained on the feature vectors. We observe this\nproduces a better result than ﬁne-tuning or directly training\na ResNet-50 without pre-train. (3) Our method (w/ k-means):\nWe ﬁrst train our GAN architecture on the training set, then\nconduct the cell-level clustering on both the training set and\ntest set using the trained model. Cluster centers are calculated\ngiven cell proportions of each sample in the training set. The\npredict label is given by the closest cluster that each sample in\nthe test set belongs to. (4) Our method (w/ SVM): An L2-SVM\ninstead of k-means is used as the ﬁnal classiﬁer.\nEvaluation: We use the precision, recall and F-score for\nevaluation, the details of which have been described in Equa-\ntion 13. The difference is that the labels are binary in this\nexperiment.\nResults: Following the proposed pipeline, the GAN ar-\nchitecture is trained on the segmentation output of the split\ntraining set. For cell-level clustering task, we achieve 0.791\nF-score trained on 12000 training instances of Dataset C and\n0.771 F-score trained on 60000 training instances of Dataset\nD, both evaluated by labeled cells of Dataset A.\nGiven the cell proportions, when using k-means to perform\nimage-level unsupervised classiﬁcation, we achieve 0.931 F-\nscore on Dataset C and 0.875 F-score on Dataset D, which\nis comparative to the DNN method with 0.933 and 0.888\nF-score. The advantage is that our model is interpretable.\nThe proportion of which category of cells is irregular is\nrecognizable.\nSince there are a large number of cell-level images on both\nDataset C and D, it is difﬁcult to test our method under\nfull-supervision with a similar pipeline. We instead train an\nL2-SVM on cell proportions, taking image-level labels of\nhistopathology images as targets. As the comparison shown\nin Table IV, our method achieves 0.950 F-score on Dataset C\nand 0.902 F-score on Dataset D.\nOn Dataset C, we use Principal Components Analysis (PCA)\nto perform a dimensionality reduction, cell proportions of\neach histopathology image are projected onto a two-dimension\nplane to show that there is a distinct difference between normal\nand abnormal images, shown in Figure 11.\nImpacts of the Segmentation Parameters: To validate the\nimpacts of the segmentation performance on the image-level\nclassiﬁcation result, we change the value of intensity threshold\nin the segmentation process of experiments on Dataset C. We\nrandomly choose 20 patches with a resolution of 200 × 200\npixels in Dataset C for evaluation, which includes 335 nuclei as\ncounted. We use missing instances (nuclei that are missing in\noutputs), false alarms (mis-segmented background instances),\nand the F-score for evaluation.\nAs is shown in Table V, both results of segmentation and\nclassiﬁcation are the highest when the intensity threshold\nremains 120. Followed by the decreasing of segmentation\nperformance, the classiﬁcation performance will stay within\nFig. 11.\nVisualization of unsupervised classiﬁcation using cell proportions.\nIt can be observed that the points representing normal and abnormal samples\nare distinctly distributed in two different clusters.\nan acceptable range. Too bad segmentation performance will\nworsen the classiﬁcation result since the quality and quantity of\nthe segmentation outputs are not enough to reveal the distinct\nrepresentation of each image-level instance.\nTABLE V.\nPerformance when changing the segmentation parameters.\nIntensity threshold\n60\n80\n100\n120\n140\n160\n180\nMissing Instances\n127\n48\n21\n7\n14\n64\n184\nFalse Alarms\n3\n4\n15\n5\n20\n30\n35\nSegmentation F-score\n0.315\n0.413\n0.602\n0.701\n0.656\n0.534\n0.218\nClassiﬁcation F-score\n0.579\n0.814\n0.932\n0.950\n0.941\n0.901\n0.576\nImpacts of the Number of Clusters: For image-level\nclassiﬁcation of Dataset C, we conduct experiments choosing\ndifferent number of clusters. Table VI shows that there is\nno distinct difference of performance between choosing ﬁve\nand six clusters. We still choose ﬁve clusters for a better\nperformance.\nTABLE VI.\nPerformance when choosing different numbers of clusters.\nClusters\n4\n5\n6\n7\nCell-level Classiﬁcation F-score\n0.711\n0.791\n0.762\n0.710\nImage-level Classiﬁcation F-score\n0.897\n0.950\n0.944\n0.899\nPatch-level Classiﬁcation: We perform classiﬁcation based\non patches. Using a sliding window with a window size of\n224 and a stride of 224, we separately transfer the normalized\nimages from the training set and test set from Dataset C into\nlabeled image patches. This results in 588 positive and 288\nnegative patches for training, 224 positive and 108 negative\npatches for testing. If 50% of the patches of an image-level\ninstance are positive, we will consider this instance as positive.\nIn this manner, we achieve 0.851 F-score using DNN feature\nextractor with SVM and 0.831 F-score using our method,\nwhich is not comparative to our image-level classiﬁcation\nresults.\nDiscussion: Analyzing the results, we ﬁnd that the cell\nproportions {P1, P2, · · · , P5} can indicate the presence of\nblood diseases.\n11\nTABLE IV.\nPerformance of image-level classiﬁcation. Each experiment is repeated for four times with different random split for cross-validation. The\nscores are reported four times to show conﬁdence intervals.\nDatasets\nMethods\nPrecision\nRecall\nF-score\nC\nDNN (cell-level based)\n0.539\n0.598\n0.688\n0.524\n0.711\n0.723\n0.734\n0.678\n0.636\n0.678\n0.701\n0.621\nDNN (image-level based)\n0.906\n0.913\n0.901\n0.921\n0.969\n0.958\n0.943\n0.965\n0.933\n0.929\n0.924\n0.937\nOur Method (w/ k-means)\n0.936\n0.945\n0.939\n0.937\n0.933\n0.944\n0.946\n0.938\n0.931\n0.941\n0.948\n0.939\nOur Method (w/ SVM)\n0.950\n0.948\n0.940\n0.946\n0.969\n0.968\n0.950\n0.966\n0.950\n0.949\n0.940\n0.949\nD\nDNN (cell-level based)\n0.469\n0.579\n0.498\n0.581\n0.697\n0.654\n0.643\n0.665\n0.558\n0.612\n0.583\n0.621\nDNN (image-level based)\n0.863\n0.900\n0.887\n0.869\n0.863\n0.886\n0.871\n0.865\n0.863\n0.888\n0.879\n0.866\nOur Method (w/ k-means)\n0.858\n0.879\n0.881\n0.868\n0.857\n0.868\n0.873\n0.865\n0.862\n0.870\n0.875\n0.867\nOur Method (w/ SVM)\n0.864\n0.897\n0.901\n0.882\n0.858\n0.892\n0.898\n0.878\n0.863\n0.891\n0.902\n0.880\nFor our experiment, cell-level clustering shows that {P1,\nP4} correspond to myeloblasts, {P5} corresponds to lym-\nphocytes and erythroid precursors, and {P2, P3} correspond\nto monocytes and glanulocytes. For all normal images, P1\nand P4 are relatively lower. This matches the constitution in\nnormal bone marrow where the lymphocytes, glanulocytes and\nerythroid precursors are in the majority when the percentage\nof cells with open phased nuclei (such as myeloblasts, under\nsome circumstances plasma cells) is relatively lower (less than\n10%). In Figure 11, abnormal images that are conﬁdently\ndiscriminated are reﬂected in the numerous presence of the\nsupposed minority myeloblasts or plasma cells, which in turn\nis reﬂected in the sharp increase of P1 and P4.\nHowever, there are three abnormal images that are excep-\ntional. To analyze what causes the failure, we display the\nexample image in Figure 12.\nFig. 12.\nExample of the failed samples. Too many erythroid precursors\nindicate the presence of blood disease. The overlap of nuclei and the lousy\nstaining condition add to the difﬁculties of cell-level classiﬁcation.\nIn these images, the irregular proportion of erythroid pre-\ncursors indicates the presence of blood disease. We ﬁnd that\nour model does not correctly classify these cells. The reason\ncould be that the staining condition of these cells is not as\ngood as expected. A typical erythroid precursor should have a\nclose phased, dark-staining nucleus that appears almost black.\nAs Figure 13 shows, the color of nuclei segmented from these\nimages differ from the rest of the dataset. Particularly in these\nimages, our model is still not robust enough to capture the\nmost signiﬁcant semantic variance in an unsupervised setting.\nTherefore, acquiring high-quality histopathology images is still\na priority.\n(a)\n(b)\n(c)\n(d)\nFig. 13.\nVariance of staining conditions. (a) and (b) are erythroid precursors\nand myeloblasts randomly chosen from failed images. (c) and (d) are samples\nselected from correctly predicted images. Our model mistakes erythroid\nprecursors for myeloblasts particularly in failed images.\nV.\nCONCLUSION\nIn this paper, we introduce a uniﬁed GAN architecture with\na new formulation of the loss function into cell-level visual\nrepresentation learning of histopathology images. Cell-level\nunsupervised classiﬁcation with interpretable visualization is\nperformed by maximizing mutual information. Based on this\nmodel, we exploit cell-level information by calculating the\ncell proportions of histopathology images. Followed by this,\nwe propose a novel pipeline combining cell-level visual rep-\nresentation learning and nuclei segmentation to highlight the\nvarieties of cellular elements, which achieves promising results\nwhen tested on bone marrow datasets.\nIn future work, some improvements can be made to our\nmethod. First, the segmentation method and the computational\ntime can be further improved. The gradient penalty added\non the network architecture requires the computation of the\nsecond order derivative, which is time-consuming in the train-\ning process. Secondly, in addition to cell proportions, other\ninformation about the patients should be carefully considered,\nsuch as clinical trials and gene expression data. By allocating\nand annotating the relevant genetic variants, the risk can be\nre-evaluated. In clinical practice, doctors need to consolidate\nmore critical information to make a conﬁdent diagnosis. For\nexample, bone marrow cells of children might not be as\nvaried as those of adults’. To classify cells in a more ﬁne-\ngrained manner, the peculiar distribution information such as\nerythroid cells more likely form clusters (erythroid islands) can\nbe considered.\nVI.\nACKNOWLEDGMENT\nThe authors would like to thank the First Afﬁliated Hospital\nof Zhejiang University and Dr. Xiaodong Teng from Depart-\nment of Pathology, the First Afﬁliated Hospital of Zhejiang\nUniversity for providing data and help.\n12\nREFERENCES\n[1]\nM. N. Gurcan, L. E. Boucheron, A. Can, A. Madabhushi, N. M. Rajpoot,\nand B. Yener, “Histopathological image analysis: A review,” IEEE\nReviews in Biomedical Engineering, vol. 2, no. 1, pp. 147–171, 2009.\n[2]\nJ. M. Bennett, D. Catovsky, M. T. Daniel, G. Flandrin, D. A. Galton,\nH. R. Gralnick, and C. Sultan, “Proposals for the classiﬁcation of\nthe acute leukaemias. french-american-british (fab) co-operative group,”\nBritish Journal of Haematology, vol. 33, no. 4, p. 451458, 1976.\n[3]\nY. Xu, T. Mo, Q. Feng, P. Zhong, M. Lai, and I. C. Chang, “Deep\nlearning of feature representation with multiple instance learning for\nmedical image analysis,” in IEEE International Conference on Acous-\ntics, Speech and Signal Processing, pp. 1626–1630, 2014.\n[4]\nY. Xu, Z. Jia, Y. Ai, F. Zhang, M. Lai, I. Eric, and C. Chang, “Deep\nconvolutional activation features for large scale brain tumor histopathol-\nogy image classiﬁcation and segmentation,” in IEEE International\nConference on Acoustics, Speech and Signal Processing, pp. 947–951,\n2015.\n[5]\nJ. Xu, X. Luo, G. Wang, H. Gilmore, and A. Madabhushi, “A deep\nconvolutional neural network for segmenting and classifying epithelial\nand stromal regions in histopathological images,” Neurocomputing,\nvol. 191, no. 1, pp. 214–223, 2016.\n[6]\nH. Chen, X. Qi, L. Yu, and P.-A. Heng, “Dcan: Deep contour-aware\nnetworks for accurate gland segmentation,” in Proceedings of the IEEE\nconference on Computer Vision and Pattern Recognition, pp. 2487–\n2496, 2016.\n[7]\nT. Chen and C. Chefdhotel, “Deep learning based automatic immune\ncell detection for immunohistochemistry images,” in International\nWorkshop on Machine Learning in Medical Imaging, pp. 17–24, 2014.\n[8]\nD. C. Cires¸an, A. Giusti, L. M. Gambardella, and J. Schmidhuber,\n“Mitosis detection in breast cancer histology images with deep neural\nnetworks,” in International Conference on Medical Image Computing\nand Computer-assisted Intervention, pp. 411–418, 2013.\n[9]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nin Advances in neural information processing systems, pp. 2672–2680,\n2014.\n[10]\nA. Radford, L. Metz, and S. Chintala, “Unsupervised representation\nlearning with deep convolutional generative adversarial networks,”\narXiv preprint arXiv:1511.06434, 2015.\n[11]\nM. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv\npreprint arXiv:1701.07875, 2017.\n[12]\nI.\nGulrajani,\nF.\nAhmed,\nM.\nArjovsky,\nV.\nDumoulin,\nand\nA. Courville, “Improved training of wasserstein gans,” arXiv preprint\narXiv:1704.00028, 2017.\n[13]\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pp. 770–778, 2016.\n[14]\nX. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and\nP. Abbeel, “Infogan: Interpretable representation learning by informa-\ntion maximizing generative adversarial nets,” in Advances in neural\ninformation processing systems, pp. 2172–2180, 2016.\n[15]\nS. Nazlibilek, D. Karacor, T. Ercan, M. H. Sazli, O. Kalender, and\nY. Ege, “Automatic segmentation, counting, size determination and\nclassiﬁcation of white blood cells,” Measurement, vol. 55, no. 3, pp. 58–\n65, 2014.\n[16]\nY. Sun and P. A. Sermon, “Methods for nuclei detection, segmentation,\nand classiﬁcation in digital histopathology: A review–current status and\nfuture potential,” IEEE Reviews in Biomedical Engineering, vol. 7,\nno. 1-5, p. 97, 2014.\n[17]\nM. Muthu, Rama Krishnan, C. Chakraborty, R. R. Paul, and A. K. Ray,\n“Hybrid segmentation, characterization and classiﬁcation of basal cell\nnuclei from histopathological images of normal oral mucosa and oral\nsubmucous ﬁbrosis,” Expert Systems with Applications, vol. 39, no. 1,\npp. 1062–1077, 2012.\n[18]\nX. Xu, F. Lin, C. Ng, and K. P. Leong, “Dual spatial pyramid on rotation\ninvariant texture feature for hep-2 cell classiﬁcation,” in International\nJoint Conference on Neural Networks, pp. 1–8, 2015.\n[19]\nJ. V. Lorenzo-Ginori, W. Curbelo-Jardines, J. D. Lpez-Cabrera, and\nS. B. Huergo-Surez, Cervical Cell Classiﬁcation Using Features Related\nto Morphometry and Texture of Nuclei.\nSpringer Berlin Heidelberg,\n2013.\n[20]\nM. M. Dundar, S. Badve, G. Bilgin, V. Raykar, R. Jain, O. Sertel, and\nM. N. Gurcan, “Computerized classiﬁcation of intraductal breast lesions\nusing histopathological images.,” IEEE Transactions on Biomedical\nEngineering, vol. 58, no. 7, pp. 1977–1984, 2011.\n[21]\nK. Nguyen, A. K. Jain, and B. Sabata, “Prostate cancer detection: Fusion\nof cytological and textural features,” Journal of Pathology Informatics,\nvol. 2, no. 1, p. 1, 2011.\n[22]\nW. L. Tai, R. M. Hu, C. W. H. Han, R. M. Chen, and J. J. P. Tsai,\n“Blood cell image classiﬁcation based on hierarchical svm,” in IEEE\nInternational Symposium on Multimedia, pp. 129–136, 2011.\n[23]\nL. Putzu, G. Caocci, and C. D. Ruberto, “Leucocyte classiﬁcation\nfor leukaemia detection using image processing techniques,” Artiﬁcial\nIntelligence in Medicine, vol. 62, no. 3, pp. 179–191, 2014.\n[24]\nM. C. Su, C. Y. Cheng, and P. C. Wang, “A neural-network-based\napproach to white blood cell classiﬁcation,” The scientiﬁc world journal,\nvol. 2014, no. 4, p. 796371, 2014.\n[25]\nY. Xu, Z. Jia, L.-B. Wang, Y. Ai, F. Zhang, M. Lai, I. Eric, and C. Chang,\n“Large scale tissue histopathology image classiﬁcation, segmentation,\nand visualization via deep convolutional activation features,” BMC\nbioinformatics, vol. 18, no. 1, p. 281, 2017.\n[26]\nJ. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for\nclustering analysis,” in International Conference on Machine Learning,\npp. 478–487, 2016.\n[27]\nD. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[28]\nJ. Xu, L. Xiang, Q. Liu, H. Gilmore, J. Wu, J. Tang, and A. Madabhushi,\n“Stacked sparse autoencoder (ssae) for nuclei detection on breast can-\ncer histopathology images.,” IEEE Transactions on Medical Imaging,\nvol. 35, no. 1, pp. 119–130, 2016.\n[29]\nA. A. Cruzroa, J. E. Arevalo Ovalle, A. Madabhushi, and F. A.\nGonzlez Osorio, “A deep learning architecture for image representa-\ntion, visual interpretability and automated basal-cell carcinoma cancer\ndetection.,” in International Conference on Medical Image Computing\nand Computer-Assisted Intervention, pp. 403–410, 2013.\n[30]\nX. Zhang, W. Liu, H. Dou, T. Ju, J. Xu, and S. Zhang, “Fusing hetero-\ngeneous features from stacked sparse autoencoder for histopathological\nimage analysis,”IEEE Journal of Biomedical and Health Informatics,\nvol. 20, no. 5, pp. 1377–1383, 2016.\n[31]\nN. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salim-\nbeni, K. Arulkumaran, and M. Shanahan, “Deep unsupervised clus-\ntering with gaussian mixture variational autoencoders,” arXiv preprint\narXiv:1611.02648, 2016.\n[32]\nZ. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou, “Variational deep\nembedding: An unsupervised and generative approach to clustering,” in\nInternational Joint Conference on Artiﬁcial Intelligence, 2017.\n[33]\nJ. T. Springenberg, “Unsupervised and semi-supervised learning\nwith categorical generative adversarial networks,” arXiv preprint\narXiv:1511.06390, 2015.\n[34]\nC. D. Malon and C. Eric, “Classiﬁcation of mitotic ﬁgures with\nconvolutional neural networks and seeded blob features,” Journal of\nPathology Informatics, vol. 4, no. 1, p. 9, 2013.\n[35]\nS. Mohapatra, D. Patra, and S. Satpathy, An ensemble classiﬁer system\nfor early diagnosis of acute lymphoblastic leukemia in blood micro-\nscopic images. Springer-Verlag, 2014.\n[36]\nJ. Zhao, M. Zhang, Z. Zhou, J. Chu, and F. Cao, “Automatic de-\ntection and classiﬁcation of leukocytes using convolutional neural\nnetworks,” Medical & biological engineering & computing, vol. 55,\nno. 8, pp. 1287–1301, 2017.\n13\n[37]\nK. Sirinukunwattana, S. E. Ahmed Raza, Y. W. Tsang, D. R. Snead,\nI. A. Cree, and N. M. Rajpoot, “Locality sensitive deep learning for\ndetection and classiﬁcation of nuclei in routine colon cancer histology\nimages,” IEEE Transactions on Medical Imaging, vol. 35, no. 5, p. 1196,\n2016.\n[38]\nL. Hou, K. Singh, D. Samaras, T. M. Kurc, Y. Gao, R. J. Seidman, and\nJ. H. Saltz, “Automatic histopathology image analysis with cnns,” in\nScientiﬁc Data Summit (NYSDS), pp. 1–6, 2016.\n[39]\nX. Zhang, H. Su, L. Yang, and S. Zhang, “Weighted hashing with\nmultiple cues for cell-level analysis of histopathological images,” in\nInternational Conference on Information Processing in Medical Imag-\ning, pp. 303–314, Springer, 2015.\n[40]\nX. Zhang, W. Liu, M. Dundar, S. Badve, and S. Zhang, “Towards\nlarge-scale histopathological image analysis: Hashing-based image re-\ntrieval,”IEEE Transactions on Medical Imaging, vol. 34, no. 2, pp. 496–\n506, 2015.\n[41]\nX. Zhang, F. Xing, H. Su, L. Yang, and S. Zhang, “High-throughput\nhistopathological image analysis via robust cell segmentation and hash-\ning,”Medical image analysis, vol. 26, no. 1, pp. 306–315, 2015.\n[42]\nX. Shi, F. Xing, Y. Xie, H. Su, and L. Yang, “Cell encoding for\nhistopathology image classiﬁcation,” in International Conference on\nMedical Image Computing and Computer-Assisted Intervention, pp. 30–\n38, Springer, 2017.\n[43]\nC. Callau, M. Lejeune, A. Korzynska, M. Garca, G. Bueno, R. Bosch,\nJ. Jan, G. Orero, T. Salvad, and C. Lpez, “Evaluation of cytokeratin-\n19 in breast cancer tissue samples: a comparison of automatic and\nmanual evaluations of scanned tissue microarray cylinders.,” Biomedical\nEngineering Online, vol. 14, no. S2, p. S2, 2015.\n[44]\nS. Wienert, D. Heim, S. Kai, A. Stenzinger, M. Beil, P. Hufnagl, M. Di-\netel, C. Denkert, and F. Klauschen, “Detection and segmentation of\ncell nuclei in virtual microscopy images: A minimum-model approach,”\nScientiﬁc Reports, vol. 2, no. 7, p. 503, 2012.\n[45]\nL. B. Dorini, R. Minetto, and N. J. Leite, “Semiautomatic white blood\ncell segmentation based on multiscale analysis.,” IEEE Journal of\nBiomedical and Health Informatics, vol. 17, no. 1, p. 250, 2013.\n[46]\nO. Schmitt and M. Hasse, “Morphological multiscale decomposition of\nconnected regions with emphasis on cell clusters,” Computer Vision &\nImage Understanding, vol. 113, no. 2, pp. 188–201, 2009.\n[47]\nO. Dzyubachyk, W. A. van Cappellen, J. Essers, W. J. Niessen, and\nE. Meijering, “Advanced level-set-based cell tracking in time-lapse\nﬂuorescence microscopy.,” IEEE Transactions on Medical Imaging,\nvol. 29, no. 3, pp. 852–867, 2010.\n[48]\nF. Long, H. Peng, X. Liu, S. K. Kim, and E. Myers, “A 3d digital\natlas of c. elegans and its application to single-cell analyses.,” Nature\nMethods, vol. 6, no. 9, p. 667, 2009.\n[49]\nS. Hai, F. Xing, J. D. Lee, C. A. Peterson, and Y. Lin, “Automatic\nmyonuclear detection in isolated single muscle ﬁbers using robust\nellipse ﬁtting and sparse representation,” IEEE/ACM Transactions on\nComputational Biology & Bioinformatics, vol. 11, no. 4, pp. 714–726,\n2014.\n[50]\nG. Bueno, R. Gonzlez, O. Dniz, M. Garcarojo, J. Gonzlezgarca, M. M.\nFernndezcarrobles, N. Vllez, and J. Salido, “A parallel solution for\nhigh resolution histological image analysis.,” Computer Methods &\nPrograms in Biomedicine, vol. 108, no. 1, pp. 388–401, 2012.\n[51]\nH. Chang, J. Han, A. Borowsky, L. Loss, J. W. Gray, P. T. Spellman, and\nB. Parvin, “Invariant delineation of nuclear architecture in glioblastoma\nmultiforme for clinical and molecular association,” IEEE Transactions\non Medical Imaging, vol. 32, no. 4, pp. 670–682, 2013.\n[52]\nS. Arslan, T. Ersahin, R. Cetin-Atalay, and C. Gunduz-Demir, “At-\ntributed relational graphs for cell nucleus segmentation in ﬂuorescence\nmicroscopy images,” IEEE Transactions on Medical Imaging, vol. 32,\nno. 6, pp. 1121–1131, 2013.\n[53]\nD. Nie, R. Trullo, J. Lian, C. Petitjean, S. Ruan, Q. Wang, and D. Shen,\n“Medical image synthesis with context-aware generative adversarial\nnetworks,” in International Conference on Medical Image Computing\nand Computer-Assisted Intervention, pp. 417–425, 2017.\n[54]\nZ. Li, Y. Wang, and J. Yu, “Reconstruction of thin-slice medical images\nusing generative adversarial network,” in International Workshop on\nMachine Learning in Medical Imaging, pp. 325–333, Springer, 2017.\n[55]\nD. Mahapatra, B. Bozorgtabar, S. Hewavitharanage, and R. Garnavi,\n“Image super resolution using generative adversarial networks and local\nsaliency maps for retinal image analysis,” in International Conference\non Medical Image Computing and Computer-Assisted Intervention,\npp. 382–390, 2017.\n[56]\nJ. M. Wolterink, T. Leiner, M. A. Viergever, and I. Isgum, “Gener-\native adversarial networks for noise reduction in low-dose ct.,” IEEE\nTransactions on Medical Imaging, vol. PP, no. 99, p. 1, 2017.\n[57]\nE. Reinhard, M. Ashikhmin, B. Gooch, and P. Shirley, “Color transfer\nbetween images,” IEEE Computer Graphics & Applications, vol. 21,\nno. 5, pp. 34–41, 2001.\n[58]\nM. Macenko, M. Niethammer, J. S. Marron, D. Borland, J. T. Woosley,\nX. Guan, C. Schmitt, and N. E. Thomas, “A method for normalizing\nhistology slides for quantitative analysis,” in IEEE International Con-\nference on Symposium on Biomedical Imaging: From Nano To Macro,\npp. 1107–1110, 2009.\n[59]\nP. Kainz, M. Urschler, S. Schulter, P. Wohlhart, and V. Lepetit, “You\nshould use regression to detect cells,” in International Conference\non Medical Image Computing and Computer Assisted Intervention,\npp. 276–283, 2015.\n[60]\nD. Arthur and S. Vassilvitskii, “k-means++: The advantages of careful\nseeding,” in Proceedings of the eighteenth annual ACM-SIAM sympo-\nsium on Discrete algorithms, pp. 1027–1035, Society for Industrial and\nApplied Mathematics, 2007.\n[61]\nD. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[62]\nD. G. Lowe, “Distinctive image features from scale-invariant keypoints,”\nInternational journal of computer vision, vol. 60, no. 2, pp. 91–110,\n2004.\n[63]\nT. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale\nand rotation invariant texture classiﬁcation with local binary patterns,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 24, no. 7, pp. 971–987, 2002.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-11-30",
  "updated": "2018-07-07"
}