{
  "id": "http://arxiv.org/abs/1712.00006v2",
  "title": "Comparing Deep Reinforcement Learning and Evolutionary Methods in Continuous Control",
  "authors": [
    "Shangtong Zhang",
    "Osmar R. Zaiane"
  ],
  "abstract": "Reinforcement Learning and the Evolutionary Strategy are two major approaches\nin addressing complicated control problems. Both are strong contenders and have\ntheir own devotee communities. Both groups have been very active in developing\nnew advances in their own domain and devising, in recent years, leading-edge\ntechniques to address complex continuous control tasks. Here, in the context of\nDeep Reinforcement Learning, we formulate a parallelized version of the\nProximal Policy Optimization method and a Deep Deterministic Policy Gradient\nmethod. Moreover, we conduct a thorough comparison between the state-of-the-art\ntechniques in both camps fro continuous control; evolutionary methods and Deep\nReinforcement Learning methods. The results show there is no consistent winner.",
  "text": "Comparing Deep Reinforcement Learning and Evolutionary Methods\nin Continuous Control\nShangtong Zhang1, Osmar R. Zaiane2\n12 Dept. of Computing Science, University of Alberta\nshangtong.zhang@ualberta.ca, osmar.zaiane@ualberta.ca\nAbstract\nReinforcement Learning and the Evolu-\ntionary Strategy are two major approaches\nin addressing complicated control prob-\nlems.\nBoth are strong contenders and\nhave their own devotee communities. Both\ngroups have been very active in developing\nnew advances in their own domain and de-\nvising, in recent years, leading-edge tech-\nniques to address complex continuous con-\ntrol tasks. Here, in the context of Deep\nReinforcement Learning, we formulate a\nparallelized version of the Proximal Policy\nOptimization method and a Deep Deter-\nministic Policy Gradient method. More-\nover, we conduct a thorough comparison\nbetween the state-of-the-art techniques in\nboth camps fro continuous control; evolu-\ntionary methods and Deep Reinforcement\nLearning methods. The results show there\nis no consistent winner.\n1\nIntroduction\nThe biological basis of reinforcement learning (RL)\nis the behavioral learning process of animals, where\nan individual learns knowledge by trial-and-error.\nHowever, the evolutionary strategy (ES) comes from\nthe evolution of the species, where randomness hap-\npens at every time and only individuals with positive\nmutations can survive. Both mechanisms widely\nexist in nature and both are crucial to the develop-\nment of the species. Similarly, both RL and ES have\ngained huge success in solving difﬁcult tasks. How-\never, different from nature, where individual learning\nand specie evolution are combined in a reasonable\nway, we have not had a uniﬁed framework to com-\nbine RL and ES until now. As a result, we need\nto understand the advantages and the weaknesses\nof both methods in order to select the proper ap-\nproach when faced with different tasks. Our main\ncontribution is a systematic comparison between the\nstate-of-the-art techniques in both domains in contin-\nuous control problems. Deep learning has recently\nachieved great success in various domains, so in our\ncomparison, we always use neural networks as func-\ntion approximators, i.e. deep RL. Moreover, all the\nalgorithms are implemented in a parallelized manner\nacross multiple processes to exploit the advance of\nmodern computation resources. Our other contri-\nbution is that we formulate the parallelized version\nof the Proximal Policy Optimization [Schulman et\nal., 2017] and Deep Deterministic Policy Gradient\n[Lillicrap et al., 2015] and showcase their power in\ncontinuous control tasks.\nWe conducted our comparison in terms of both\nwall time and environment time steps, i.e.\nthe\namount of interactions with the environment, to gain\na better empirical understanding of the running speed\nand data efﬁciency of all the algorithms.\n2\nCompared Algorithms\nConsidering a Markov Decision Process with a state\nset S, an action set A, a reward function r : S×A →\nR and a transition function p : S × A →(S →\n[0, 1]), the goal of the agent is to learn an optimal\npolicy to maximize the expected discounted return\nGt = PT −1\nk=0 γkRt+k+1 at every time step, where\nT denotes the terminal time and γ is the discount\nfactor. In the continuous control domain, an action\narXiv:1712.00006v2  [cs.LG]  7 Mar 2018\na is usually a vector, i.e. a ∈Rd, where d is the\ndimension of the action. We use π and µ to represent\na stochastic and a deterministic policy respectively,\nand we assume the policy is parameterized by θ in\nthe following sections, which is a neural network.\n2.1\nEvolutionary Methods\nEvolutionary methods solve the control problem by\nevolving the control policy. Different evolutionary\nmethods adopt different mechanisms to generate in-\ndividuals in a generation, where individuals with\nbetter performance are selected to produce the next\ngeneration.\nCovariance Matrix Adaptation Evolution\nStrategy (CMAES)\nIn CMAES [Hansen and Ostermeier, 1996], the struc-\nture of the neural network is predeﬁned, θ here only\nrepresents the weights of the network. Each gen-\neration G consists of many candidate parameters,\ni.e. G = {θ1, . . . , θn}. Each θi is sampled from a\nmultivariate Gaussian distribution in the following\nfashion: θi ∼µ + σN(0, Σ), where N(0, Σ) is a\nmultivariate Gaussian distribution with zero mean\nand covariance matrix Σ, µ is the mean value of\nthe search distribution and σ is the overall standard\ndeviation (aka step size). All the candidates in G are\nevaluated by the environment. According to the per-\nformance, a certain selection mechanism will select\nsome candidates to update the sample distribution,\ni.e. update µ, σ and Σ, and the next generation is\nsampled from the updated distribution.\nNeuroEvolution of Augmenting Topologies\n(NEAT)\nNEAT [Stanley and Miikkulainen, 2002] has shown\ngreat success in many difﬁcult control tasks. The ba-\nsic idea of NEAT is to evolve both the structure and\nthe weights of the network. Thus θ now represents\nboth the weights and the structure of the network. At\nevery generation, NEAT selects several best individu-\nals to crossover. And their descendants, with various\nmutations, will form the next generation. NEAT in-\ntroduced a genetic encoding to represent the network\nefﬁciently and used the historical markings to per-\nform a crossover among networks in a reasonable\nway, which helped avoid expensive topological anal-\nysis. Furthermore, NEAT incorporates an innovation\nnumber to protect innovations, which allowed the\nnetwork to evolve from the simplest structure.\nNatural Evolution Strategy (NES)\nIn NES [Wierstra et al., 2008], the structure of the\nnetwork is given, θ here only represents the weights.\nNES assumes the population of θ is drawn from a\ndistribution pφ(θ), parameterized by φ, and aims to\nmaximize the expected ﬁtness J(φ) = Eθ∼pφF(θ),\nwhere F is the ﬁtness function to evaluate an in-\ndividual θ.\nSalimans et al.\n[2017] instantiated\nthe population distribution pφ as a multivariate\nGaussian distribution, with mean φ and a ﬁxed co-\nvariance σ2I. Thus J can be rewritten in terms\nof θ directly, i.e.\nJ(θ) = Eϵ∼N(0,I)F(θ + σϵ).\nIn a fashion similar to policy gradient, we have\n∇θJ(θ) =\n1\nσEϵ∼N(0,I)F(θ + σϵ)ϵ. In practice,\nat every generation, the population {ϵ1, . . . , ϵn} is\ndrawn from N(0, I). The update rule for θ is, there-\nfore, θ ←θ + α 1\nnσ\nPn\ni=1 F(θ + σϵi)ϵi, where α is\nthe step size and n is the population size.\n2.2\nDeep Reinforcement Learning\nMethods\nIn deep RL methods, the structure of the network is\npredeﬁned, θ simply represents the numeric weights\nof the neural network.\nContinuous Asynchronous Advantage\nActor-Critic (CA3C)\nThe goal of the actor-critic method is to maxi-\nmize the value of the initial state vπ(s0), where\nvπ(s) is the value of the state s under policy π,\ni.e. vπ(s) .= Eπ[PT −1\nk=0 γkRt+k+1 | St = s]. Let\nqπ(s, a) .= Eπ[PT −1\nk=0 γkRt+k+1 | St = s, At =\na] denote the value of the state action pair (s, a), we\nhave vπ(s) = P\na∈A π(a|s)qπ(s, a). Our objective,\ntherefore, is to maximize J(θ) = vπ(s0). According\nto the policy gradient theorem [Sutton et al., 2000],\nwe have ∇θJ(θ) = Eπ[∇θlogπ(At|St, θ)(Rt+1 +\nγvπ(St+1) −vπ(St))]. The value function vπ is\nupdated in a semi-gradient TD fashion [Sutton and\nBarto, 1998]. In continuous action domain, the pol-\nicy π is often parameterized in the form of the prob-\nability density function of the multivariate Gaussian\ndistribution, i.e. π(a|s, θ) .= N(µ(s, θ), Σ(s, θ)).\nIn practice, setting the covariance matrix Σ(s, θ) ≡\nI is a good choice to increase stability and simplify\nthe parameterization. Beyond the standard advan-\ntage actor-critic method, Mnih et al. [2016] intro-\nduced asynchronous workers to gain uncorrelated\ndata, speed up learning and reduce variance, where\nmultiple workers were distributed across processes\nand every worker interacted with the environment\nseparately to collect data. The computation of the\ngradient was also in a non-centered manner.\nParallelized Proximal Policy Optimization\n(P3O)\nSchulman et al. [2015] introduced an iterative pro-\ncedure to monotonically improve policies named\nTrust Region Policy Optimization, which aims to\nmaximize an objective function L(θ) within a trust\nregion, i.e.\nmaximize\nθ\nL(θ) = E[ πθ(at|st)\nπθold(at|st)At]\nsubject to\nE[DKL(πθ(· |st), πθold(· |st))] ≤δ\nwhere At is the advantage function, DKL is\nthe KL-divergence and δ is some threshold.\nIn\npractice, solving the corresponding unconstrained\noptimization problem with a penalty term is\nmore efﬁcient, i.e.\nwe maximize LKL(θ)\n=\nL(θ) −βE[DKL(πθ(· |st), πθold(· |st))] for some\ncoefﬁcient β.\nFurthermore, Schulman et al.\n[2017] proposed the clipped objective function,\nresulting in the Proximal Policy Optimization\n(PPO) algorithm.\nWith rt(θ)\n.=\nπθ(at|st)\nπθold(at|st),\nthe objective function of PPO is LCLIP (θ) =\nE[min(rt(θ)At, clip(rt(θ), 1 −ϵ, 1 + ϵ)At)] where\nϵ is a hyperparameter, e.g.\nϵ = 0.2.\nIn prac-\ntice, Schulman et al.\n[2017] designed a trun-\ncated generalized advantage function, i.e. At =\nδt + (γλ)δt+1 + · · · + (γλ)T −t+1δT −1 where T is\nthe length of the trajectory, δt is the TD error, i.e.\nδt = rt +vπ(st+1)−vπ(st), and λ is a hyperparam-\neter, e.g. λ = 0.97. The value function is updated in\na semi-gradient TD manner.\nWe parallelize the PPO mainly following the\ntraining protocol of A3C and the Distributed PPO\n(DPPO) method [Heess et al., 2017]. In P3O, we dis-\ntribute multiple workers across processes like A3C.\nFollowing DPPO, each worker in P3O also has its\nown experience replay buffer to store its transitions.\nThe optimization that happens in the worker is solely\nbased on this replay buffer. Different from DPPO,\nwhere the objective function was LKL(θ), we use\nLCLIP (θ) in P3O, as it was reported to be able to\noutperform LKL(θ) [Schulman et al., 2017]. Heess\net al. [2017] reported that synchronized gradient\nupdate can actually outperform asynchronous update\nin DPPO. So in P3O we also used the synchronized\ngradient update. However our synchronization pro-\ntocol is quite simple. We use A3C-style update with\nan extra shared lock for synchronization and all the\ngradients are never dropped, while DPPO adopted\na quite complex protocol and some gradients may\nget dropped. Moreover, a worker in P3O only does\none batch update based on all the transitions in the\nreplay buffer as we ﬁnd this can increase the stability.\nIn DPPO, however, each worker often did multiple\nmini-batch updates.\nDistributed Deep Deterministic Policy Gradient\n(D3PG)\nSimilar to CA3C, the goal of DDPG is also\nto maximize J(θ)\n=\nvµ(s0).\nAccording\nto the deterministic policy gradient theorem\n[Silver et al.,\n2014],\nwe have ∇θJ(θ)\n=\nEµ′[∇aqµ(s, a)|s=st,a=µ(st|θ)∇θµ(s|θ)|s=st],\nwhere µ′ is the behavior policy.\nThe behavior\npolicy usually combines the target policy with some\nnoise, i.e. µ′(s) = µ(s) + N, where N is some\nrandom noise. Following [Lillicrap et al., 2015],\nwe use an Ornstein-Uhlenbeck process [Uhlenbeck\nand Ornstein, 1930] as the noise. To stabilize the\nlearning process, Lillicrap et al. [2015] introduced\nexperience replay and target network, resulting in\nthe DDPG algorithm.\nWe formulate the distributed version of DDPG in\nanalogy to P3O, except the experience replay buffer\nis shared among different workers. Each worker\ninteracts with the environment, and acquired transi-\ntions are added to a shared replay buffer. At every\ntime step, a worker will sample a batch of transitions\nfrom the shared replay buffer and compute the gradi-\nents following the DDPG algorithm. The update is\nsynchronized and the target network is shared.\n3\nExperiments\n3.1\nTestbeds\nWe investigated three kinds of tasks for our evalua-\ntion. The ﬁrst kind of task is the classical toy task\nto verify the implementation. The second kind of\ntasks needs careful exploration and the third kind of\ntasks involves rich dynamics. We used the Pendu-\nlum task as a representative of the classical toy task.\nRecently MuJoCo has become popular as a bench-\nmark in the continuous control domain. However\nthere have been many empirical results on MuJoCo\nin the community. In our comparison, we considered\nthe Box2D [Catto, 2011] environment, especially the\n(a) Pendulum\n(b) Continuous Lunar\nLander\n(c) Bipedal Walker\n(d) Bipedal Waler Hard-\ncore\nFigure 1: Four tasks. The background of the Continuous\nLunar Lander was edited for better display.\nContinuous Lunar Lander task, the Bipedal Walker\ntask and the Bipedal Waler Hardcore task. Con-\ntinuous Lunar Lander is a typical task that needs\ncareful exploration. Negative rewards are continu-\nously given during the landing so the algorithm can\neasily get trapped in a local minima, where it avoids\nnegative rewards by doing nothing after certain steps\nuntil timeout. Bipedal Walker is a typical task that\nincludes rich dynamics and Bipedal Walker Hard-\ncore increases the difﬁculty for the agent to learn\nhow to walk by putting some obstacles in the path.\nThe three Box2D tasks have comparable complexity\nwith MuJoCo tasks, and all the four tasks are free\nand available in OpenAI Gym. All the tasks have\nlow-dimensional vector state space and continuous\naction. Some screenshots of the tasks are shown in\nFigure 1.\n3.2\nPerformance Measurement\nFor evolutionary methods, we performed 10 test\nepisodes for the best individual in each generation\nand averaged the test results to represent the perfor-\nmance at those time steps (wall time). For CA3C\nand P3O, an additional test process run determin-\nistic test episodes continuously. We performed 10\nindependent runs for each algorithm and each task\nwithout any ﬁxed random seed. Our training was\nbased on episodes (generations), while the length\n(environment steps and wall time) of episodes is\ndifferent. So the statistics are not aligned across dif-\nferent runs. We used linear interpolation to average\nstatistics across different runs.\n3.3\nPolicy Representation\nFollowing [Schulman et al., 2017], we used two\ntwo-hidden-layer neural networks to parameterize\nthe policy function and the value function for all the\nevaluated methods except NEAT. To investigate the\nscalability of the algorithms in terms of the amount\nof the parameters, we tested small networks (hidden\nlayers with 16 hidden units) and large networks (hid-\nden layers with 64 hidden units) respectively. We\nalways used the hyperbolic tangent activation func-\ntion and the two networks did not have shared layers.\nThe output units were linear to produce the mean\nof the Gaussian distribution (for P3O and CA3C) or\nthe action (for D3PG, CMAES and NES). Moreover,\nwe used the identity covariance matrix for P3O and\nCA3C, thus the entropy penalty was simply set to\nzero. For NEAT, the initial architecture had only one\nhidden unit, as NEAT allows the evolution to start\nfrom the simplest architecture.\n3.4\nWall Time\nWe implemented a multi-process version of CMAES\nand NEAT based on Lib CMA 1 and NEAT-Python 2,\nwhile all other algorithms were implemented from\nscratch in PyTorch for a like-to-like comparison. For\ndeep RL algorithms, the most straightforward paral-\nlelization method is to only parallelize data collec-\ntion, while the computation of the gradients remains\ncentralized. However this approach needs careful\nbalance design between data collection and gradi-\nents computation. Moreover, it is unfriendly to data\nefﬁciency. So we adopted algorithm dependent par-\nallelization rather than this uniform solution. We run\nall the algorithms in the same server 3. Although\nP3O and D3PG can be accelerated by GPU, we did\nnot introduce GPU in our experiments as most of the\nevaluated algorithms are not compatible with GPU.\nAlthough the relative wall time is highly dependent\non the implementation, the comparison we did here\ncan still give us some basic intuition. We made all\n1https://github.com/CMA-ES/pycma\n2https://github.com/CodeReclaimers/\nneat-python\n3Intel® Xeon® CPU E5-2620 v4\nthe implementations publicly available 4 5.\n3.5\nHyper-parameter Tuning\nDue to the huge amount of the hyper-parameters\nand the complexity of the tasks, we can not afford\na thorough grid search for all the tasks and hyper-\nparameters. We only did the grid search for one\nor two key hyper-parameters for each algorithms\nin the Pendulum task and tuned all other hyper-\nparameters empirically or used the default value\nof the package. For other tasks, we used same pa-\nrameters as Pendulum without further tuning. This\nalso gives us an intuition of the robusticity of the\nalgorithms in terms of the hyper-parameters. For\nCA3C, D3PG and P3O, we used the Adam opti-\nmizer for both the policy function and the value\nfunction, and the initial learning rates for the policy\nfunction and the value function were the same. We\nsearched the learning rate in {10−4, . . . , 10−1}. We\nfound 10−4 gave best performance for CA3C and\nD3PG while 10−3 gave best performance for P3O.\nFor NES, we searched the variance σ and the learn-\ning rate α in {10−2, . . . , 100} × {10−3, . . . , 100}.\nSetting both σ and α to 0.1 gave best performance.\nFor CMAES, we searched the overall standard de-\nviation σ in {10−2, . . . , 101} and found 1 gave the\nbest performance.\n3.6\nNormalization\nFor all the evaluated methods, we normalized the\nstate with running estimation of the mean and the\nvariance. All the running statistics were shared and\nupdated across parallelized workers. For CA3C, P3O\nand D3PG, we also normalized the reward in the\nsame manner as the state. For NES and CMAES, we\nadopted the reward shaping as reported in [Salimans\net al., 2017].\n3.7\nResults\nWe reported the performance in terms of both the\nenvironment steps and wall time in Figure 2 and Fig-\nure 3. All the curves were averaged over 10 indepen-\ndent runs. For each run the maximum environment\nstep is set to 107. As deep RL methods had larger\nvariance than evolutionary methods, all the curves of\n4https://github.com/ShangtongZhang/\nDeepRL\n5https://github.com/ShangtongZhang/\nDistributedES\nCA3C, P3O and D3PG were smoothed by a sliding\nwindow of size 50.\n4\nDiscussion\nWe can hardly say which algorithm is the best, as no\nalgorithm can consistently outperform others across\nall the tasks in terms of all the measurements. How-\never we can still learn some general properties of\nmethods in different domains.\n4.1\nFinal Performance\nThe relative ﬁnal performance was quite task de-\npendent. One interesting observation is that NEAT\nachieved a good performance level in the three\nBox2D tasks, but failed the simplest Pendulum task.\nMoreover, all the evolutionary methods solved the\nContinuous Lunar Lander task, but most deep RL\nmethods appeared to get trapped in some local min-\nima, which denotes that evolutionary methods are\nbetter at exploration than the deep RL methods.\nHowever when it comes to the two Walker tasks,\nwhere rich dynamics are involved, most deep RL\nmethods worked better and many evolutionary meth-\nods appeared to get trapped in some local minima,\nwhich denotes that the deep RL methods can handle\nrich dynamics better than the evolutionary methods.\nIn general, NEAT, D3PG and P3O are good choices.\n4.2\nLearning Speed\nIn the Pendulum task the comparison is clear, deep\nRL methods perform better in terms of both envi-\nronment steps (data efﬁciency) and wall time than\nevolutionary methods. This conclusion still holds\nin the Bipedal Walker task, although there is an ex-\nception that D3PG runs quite slowly in this task. As\nmany deep RL methods failed the Continuous Lunar\nLander task and many evolutionary methods failed\nthe Bipedal Walker Hardcore task, we did not take\nthose two tasks into consideration in the comparison\nof the learning speed.\n4.3\nStability\nIn all the experiments, deep RL methods appeared to\nhave larger variance than evolutionary methods. This\nobservation conforms with the fact that evolutionary\nmethods perform better in tasks which need careful\nexploration. As evolutionary methods have better ex-\nploration, they are less sensitive to the initialization\nand randomness, resulting in better stability.\nFigure 2: Performance in terms of environment steps. Left to right: Pendulum, Continuous Lunar Lander, BipedalWalker,\nBipedalWalkerHardcore. Solid line: large networks. Dashed line: small networks.\nFigure 3:\nPerformance in terms of wall time. Left to right: Pendulum, Continuous Lunar Lander, BipedalWalker,\nBipedalWalkerHardcore. Solid line: large networks. Dashed line: small networks. X-axis is in log scale\n4.4\nScalability\nIn most experiments, the performance of deep RL\nmethods improved with the increase of the network\nsize. However sometimes small networks gave better\nperformance for evolutionary methods than larger\nnetworks, e.g. NES with a small network reached a\nreasonable performance level in the Bipedal Walker\nHardcore task, however it almost did not learn any-\nthing with a large network. In general our experi-\nments showed that deep RL methods can make bet-\nter use of complexity function parameterization and\nscale better to large network than evolutionary meth-\nods.\n5\nFuture Work\nAlthough our testbeds include several representative\ntasks, our comparison is still limited in tasks with low\ndimensional vector state space. With the popularity\nof the Atari games, image input has become a new\nnorm in the discrete action domain. Our future work\nwill involve continuous control tasks with image\ninput.\n6\nRelated Work\nTaylor et al. [2006] compared NEAT with SARSA,\nbut their work was limited on the football game Keep-\naway, which is a simple discrete control task. This\ncomparison was extended later on by Whiteson et\nal. [2010], where new criterion was involved but the\ncomparison was still limited to primitive RL methods\nwhere deep neural networks were not included. Duan\net al. [2016] compared various deep RL methods\nwith some evolutionary methods. However they did\nnot include the NEAT paradigm and there has been\nexciting progress in both RL and ES, e.g. [Schul-\nman et al., 2017] and [Salimans et al., 2017] after\ntheir work. Therefore these comparisons are not\nrepresentative enough of the current state of the art.\nMoreover our comparison is conducted mainly on\ndifferent tasks and measurements and focused on\nparallelized implementations.\nAcknowledgments\nThe authors thank G. Zacharias Holland and Yi Wan\nfor their thoughtful comments.\nReferences\n[Catto, 2011] Erin Catto. Box2d: A 2d physics en-\ngine for games, 2011.\n[Duan et al., 2016] Yan Duan,\nXi Chen,\nRein\nHouthooft, John Schulman, and Pieter Abbeel.\nBenchmarking deep reinforcement learning for\ncontinuous control. In International Conference\non Machine Learning, pages 1329–1338, 2016.\n[Hansen and Ostermeier, 1996] Nikolaus\nHansen\nand Andreas Ostermeier. Adapting arbitrary nor-\nmal mutation distributions in evolution strategies:\nThe covariance matrix adaptation. In Evolution-\nary Computation, 1996., Proceedings of IEEE In-\nternational Conference on, pages 312–317. IEEE,\n1996.\n[Heess et al., 2017] Nicolas Heess, Srinivasan Sri-\nram, Jay Lemmon, Josh Merel, Greg Wayne, Yu-\nval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Mar-\ntin Riedmiller, et al. Emergence of locomotion\nbehaviours in rich environments. arXiv preprint\narXiv:1707.02286, 2017.\n[Lillicrap et al., 2015] Timothy\nP\nLillicrap,\nJonathan J Hunt, Alexander Pritzel, Nicolas\nHeess, Tom Erez, Yuval Tassa, David Silver,\nand Daan Wierstra.\nContinuous control with\ndeep reinforcement learning.\narXiv preprint\narXiv:1509.02971, 2015.\n[Mnih et al., 2016] Volodymyr Mnih, Adria Puig-\ndomenech Badia, Mehdi Mirza, Alex Graves,\nTimothy Lillicrap, Tim Harley, David Silver, and\nKoray Kavukcuoglu. Asynchronous methods for\ndeep reinforcement learning. In International\nConference on Machine Learning, pages 1928–\n1937, 2016.\n[Salimans et al., 2017] Tim Salimans, Jonathan Ho,\nXi Chen, and Ilya Sutskever. Evolution strategies\nas a scalable alternative to reinforcement learning.\narXiv preprint arXiv:1703.03864, 2017.\n[Schulman et al., 2015] John Schulman,\nSergey\nLevine, Pieter Abbeel, Michael Jordan, and\nPhilipp Moritz. Trust region policy optimization.\nIn Proceedings of the 32nd International Con-\nference on Machine Learning (ICML-15), pages\n1889–1897, 2015.\n[Schulman et al., 2017] John Schulman, Filip Wol-\nski, Prafulla Dhariwal, Alec Radford, and Oleg\nKlimov. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\n[Silver et al., 2014] David Silver, Guy Lever, Nico-\nlas Heess, Thomas Degris, Daan Wierstra, and\nMartin Riedmiller. Deterministic policy gradient\nalgorithms. In Proceedings of the 31st Interna-\ntional Conference on Machine Learning (ICML-\n14), pages 387–395, 2014.\n[Stanley and Miikkulainen, 2002] Kenneth O Stan-\nley and Risto Miikkulainen. Evolving neural net-\nworks through augmenting topologies. Evolution-\nary computation, 10(2):99–127, 2002.\n[Sutton and Barto, 1998] Richard S Sutton and An-\ndrew G Barto. Reinforcement learning: An intro-\nduction, volume 1. MIT press Cambridge, 1998.\n[Sutton et al., 2000] Richard S Sutton, David A\nMcAllester, Satinder P Singh, and Yishay Man-\nsour. Policy gradient methods for reinforcement\nlearning with function approximation.\nIn Ad-\nvances in neural information processing systems,\npages 1057–1063, 2000.\n[Taylor et al., 2006] Matthew E Taylor, Shimon\nWhiteson, and Peter Stone. Comparing evolu-\ntionary and temporal difference methods in a re-\ninforcement learning domain. In Proceedings of\nthe 8th annual conference on Genetic and evo-\nlutionary computation, pages 1321–1328. ACM,\n2006.\n[Uhlenbeck and Ornstein, 1930] George E Uhlen-\nbeck and Leonard S Ornstein. On the theory of\nthe brownian motion. Physical review, 36(5):823,\n1930.\n[Whiteson et al., 2010] Shimon\nWhiteson,\nMatthew E Taylor, and Peter Stone.\nCriti-\ncal factors in the empirical performance of\ntemporal difference and evolutionary methods for\nreinforcement learning. Autonomous Agents and\nMulti-Agent Systems, 21(1):1–35, 2010.\n[Wierstra et al., 2008] Daan Wierstra, Tom Schaul,\nJan Peters, and Juergen Schmidhuber. Natural\nevolution strategies. In Evolutionary Computa-\ntion, 2008. CEC 2008.(IEEE World Congress on\nComputational Intelligence). IEEE Congress on,\npages 3381–3387. IEEE, 2008.\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2017-11-30",
  "updated": "2018-03-07"
}