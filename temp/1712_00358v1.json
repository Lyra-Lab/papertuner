{
  "id": "http://arxiv.org/abs/1712.00358v1",
  "title": "Unsupervised Generative Adversarial Cross-modal Hashing",
  "authors": [
    "Jian Zhang",
    "Yuxin Peng",
    "Mingkuan Yuan"
  ],
  "abstract": "Cross-modal hashing aims to map heterogeneous multimedia data into a common\nHamming space, which can realize fast and flexible retrieval across different\nmodalities. Unsupervised cross-modal hashing is more flexible and applicable\nthan supervised methods, since no intensive labeling work is involved. However,\nexisting unsupervised methods learn hashing functions by preserving inter and\nintra correlations, while ignoring the underlying manifold structure across\ndifferent modalities, which is extremely helpful to capture meaningful nearest\nneighbors of different modalities for cross-modal retrieval. To address the\nabove problem, in this paper we propose an Unsupervised Generative Adversarial\nCross-modal Hashing approach (UGACH), which makes full use of GAN's ability for\nunsupervised representation learning to exploit the underlying manifold\nstructure of cross-modal data. The main contributions can be summarized as\nfollows: (1) We propose a generative adversarial network to model cross-modal\nhashing in an unsupervised fashion. In the proposed UGACH, given a data of one\nmodality, the generative model tries to fit the distribution over the manifold\nstructure, and select informative data of another modality to challenge the\ndiscriminative model. The discriminative model learns to distinguish the\ngenerated data and the true positive data sampled from correlation graph to\nachieve better retrieval accuracy. These two models are trained in an\nadversarial way to improve each other and promote hashing function learning.\n(2) We propose a correlation graph based approach to capture the underlying\nmanifold structure across different modalities, so that data of different\nmodalities but within the same manifold can have smaller Hamming distance and\npromote retrieval accuracy. Extensive experiments compared with 6\nstate-of-the-art methods verify the effectiveness of our proposed approach.",
  "text": "Unsupervised Generative Adversarial Cross-modal Hashing\nJian Zhang, Yuxin Peng∗, and Mingkuan Yuan\nInstitute of Computer Science and Technology, Peking University\nBeijing 100871, China\npengyuxin@pku.edu.cn\nAbstract\nCross-modal hashing aims to map heterogeneous multime-\ndia data into a common Hamming space, which can real-\nize fast and ﬂexible retrieval across different modalities. Un-\nsupervised cross-modal hashing is more ﬂexible and appli-\ncable than supervised methods, since no intensive labeling\nwork is involved. However, existing unsupervised methods\nlearn hashing functions by preserving inter and intra cor-\nrelations, while ignoring the underlying manifold structure\nacross different modalities, which is extremely helpful to cap-\nture meaningful nearest neighbors of different modalities for\ncross-modal retrieval. To address the above problem, in this\npaper we propose an Unsupervised Generative Adversarial\nCross-modal Hashing approach (UGACH), which makes full\nuse of GAN’s ability for unsupervised representation learning\nto exploit the underlying manifold structure of cross-modal\ndata. The main contributions can be summarized as follows:\n(1) We propose a generative adversarial network to model\ncross-modal hashing in an unsupervised fashion. In the pro-\nposed UGACH, given a data of one modality, the generative\nmodel tries to ﬁt the distribution over the manifold structure,\nand select informative data of another modality to challenge\nthe discriminative model. The discriminative model learns to\ndistinguish the generated data and the true positive data sam-\npled from correlation graph to achieve better retrieval accu-\nracy. These two models are trained in an adversarial way to\nimprove each other and promote hashing function learning.\n(2) We propose a correlation graph based approach to cap-\nture the underlying manifold structure across different modal-\nities, so that data of different modalities but within the same\nmanifold can have smaller Hamming distance and promote\nretrieval accuracy. Extensive experiments compared with 6\nstate-of-the-art methods on 2 widely-used datasets verify the\neffectiveness of our proposed approach.\nIntroduction\nMultimedia retrieval has become an important application\nover the past decades, which can retrieve multimedia con-\ntents that users have interests in. However, it is a big chal-\nlenge to retrieve multimedia data efﬁciently from large scale\ndatabases, due to the explosive growth of multimedia infor-\nmation. To address this issue, there are many hashing meth-\nods (Wang et al. 2016; Gionis, Indyk, and Motwani 1999;\n∗Corresponding author.\nCopyright c⃝2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nZhang and Peng 2017) proposed to accomplish efﬁcient re-\ntrieval. The goal of hashing methods is to map high dimen-\nsional representations in the original space to short binary\ncodes in the Hamming space. By using these binary hash\ncodes, faster Hamming distance computation can be applied\nbased on bit operations that can be implemented efﬁciently.\nMoreover, binary codes take much less storage compared\nwith original high dimensional representations.\nThere are large numbers of hashing methods applied to\nsingle modality retrieval (Wang et al. 2016), by which users\ncan only retrieve data by a query with the same modality,\nsuch as text retrieval (Baeza-Yates and Ribeiro-Neto 1999)\nand image retrieval (Wang et al. 2016). Nevertheless, single\nmodality retrieval can not meet users’ increasing demands,\ndue to the different modalities of multimedia data. For ex-\nample, by single modality retrieval, it is impracticable to\nsearch an image by using a textual sentence that describes\nthe semantic content of the image. Therefore, cross-modal\nhashing has been proposed to meet this kind of retrieval de-\nmands in large scale cross-modal databases. Owing to the ef-\nfectiveness and ﬂexibility of cross-modal hashing, users can\nsubmit whatever they have to retrieve whatever they want\n(Peng, Huang, and Zhao 2017; Peng et al. 2017).\n“Heterogeneous gap” is the key challenge of cross-modal\nhashing, which means the similarity of between different\nmodalities cannot be measured directly. Consequently, some\ncross-modal hashing methods (Kumar and Udupa 2011;\nRastegari et al. 2013; Ding et al. 2016; Zhang and Li 2014;\nZhuang et al. 2014) have been proposed to bridge this gap.\nExisting cross-modal hashing methods can be categorized\ninto traditional methods and Deep Neural Networks (DNN)\nbased methods. Moreover, traditional methods can be di-\nvided into unsupervised methods and supervised methods\nby whether semantic information is leveraged.\nUnsupervised cross-modal hashing methods usually\nproject data from different modalities into a common\nHamming space to maximize their correlations, which\nhold the similar idea with Canonical Correlation Analy-\nsis (CCA) (Hardoon, Szedmak, and Shawe-Taylor 2004).\nSong et al. propose Inter-Media Hashing (IMH) (Song et al.\n2013) to establish a common Hamming space by preserving\ninter-media and intra-media consistency. Cross-view Hash-\ning (CVH) (Kumar and Udupa 2011) is proposed to consider\nboth intra-view and inter-view similarities, which is an ex-\narXiv:1712.00358v1  [cs.CV]  1 Dec 2017\ntension of image hashing method named Spectral Hashing\n(SH) (Weiss, Torralba, and Fergus 2009). Predictable Dual-\nview Hashing (PDH) (Rastegari et al. 2013) designs an ob-\njective function to keep the predictability of pre-generated\nbinary codes. Ding et al. propose Collective Matrix Factor-\nization Hashing (CMFH) (Ding et al. 2016) to learn uniﬁed\nhash codes by collective matrix factorization. Composite\nCorrelation Quantization (CCQ) (Long et al. 2016) jointly\nlearns the correlation-maximal mappings that transform dif-\nferent modalities into an isomorphic latent space, and learns\ncomposite quantizers that convert the isomorphic latent fea-\ntures into compact binary codes.\nSupervised cross-modal hashing methods utilize labeled\nsemantic information to learn hashing functions. Bronstein\net al. propose Cross-Modality Similarity Sensitive Hash-\ning (CMSSH) (Bronstein et al. 2010) to model hashing\nlearning by a classiﬁcation paradigm with a boosting man-\nner. Wei et al. propose Heterogeneous Translated Hashing\n(HTH) (Wei et al. 2014), which learns translators to align\nseparate Hamming spaces of different modalities to perform\ncross-modal hashing. Semantic Correlation Maximization\n(SCM) (Zhang and Li 2014) is proposed to learn hashing\nfunctions by constructing and preserving the semantic sim-\nilarity matrix. Semantics-Preserving Hashing (SePH) (Lin\net al. 2015) transforms the semantic matrix into a probabil-\nity distribution and minimizes the KL-divergence in order\nto approximate the distribution with learned hash codes in\nHamming space.\nDNN based methods are inspired by the successful\napplications of deep learning, such as image classiﬁca-\ntion (Krizhevsky, Sutskever, and Hinton 2012). Cross-Media\nNeural Network Hashing (CMNNH) (Zhuang et al. 2014) is\nproposed to learn cross-modal hashing functions by preserv-\ning intra-modal discriminative ability and inter-modal pair-\nwise correlation. Cross Autoencoder Hashing (CAH) (Cao\net al. 2016b) is based on deep autoencoder structure to max-\nimize the feature correlation and the semantic correlation be-\ntween different modalities. Cao et al. propose Deep Visual-\nsemantic Hashing (DVH) (Cao et al. 2016a) as an end-to-end\nframework that combines both representation learning and\nhashing function learning. Jiang et al. propose Deep Cross-\nmodal Hashing (DCMH) (Jiang and Li 2017), which per-\nforms feature learning and hashing function learning simul-\ntaneously.\nCompared with unsupervised paradigm, supervised meth-\nods use labeled semantic information that requires mas-\nsive labor to collect, resulting in a high labor cost in real\nworld applications. On the contrary, unsupervised cross-\nmodal hashing methods can leverage unlabeled data to re-\nalize efﬁcient cross-modal retrieval, which is more ﬂexible\nand applicable in real world applications. However, most un-\nsupervised methods learn hashing functions by preserving\ninter and intra correlations, while ignoring the underlying\nmanifold structure across different modalities, which is ex-\ntremely helpful to capture meaningful nearest neighbors of\ndifferent modalities. To address this problem, in this paper,\nwe exploit correlation information from underlying mani-\nfold structure of unlabeled data across different modalities\nto enhance cross-modal hashing learning.\nInspired by recent progress of Generative Adversarial\nNetwork (GAN) (Goodfellow et al. 2014; Reed et al. 2016;\nZhao and Gao 2017; Wang et al. 2017), which has shown\nits ability to model the data distribution in an unsupervised\nfashion. In this paper, we propose an unsupervised gener-\native adversarial cross-modal hashing (UGACH) approach.\nWe design a graph-based unsupervised correlation method\nto capture the underlying manifold structure across different\nmodalities, and a generative adversarial network to learn the\nmanifold structure and further enhance the performance by\nan adversarial boosting paradigm. The main contributions of\nthis paper can be summarized as follows:\n• We propose a generative adversarial network to model\ncross-modal hashing in an unsupervised fashion. In the\nproposed UGACH, given the data of any modality, the\ngenerative model tries to ﬁt the distribution over the man-\nifold structure, and selects informative data of another\nmodality to challenge the discriminative model. While the\ndiscriminative model learns to distinguish the generated\ndata and the true positive data sampled from correlation\ngraph to achieve better retrieval accuracy.\n• We propose a correlation graph based learning approach\nto capture the underlying manifold structure across dif-\nferent modalities, so that data of different modalities but\nwithin the same manifold can have smaller Hamming dis-\ntance and promote retrieval accuracy. We also integrate\nthe proposed correlation graph into proposed generative\nadversarial network to provide manifold correlation guid-\nance to promote the cross-modal retrieval accuracy.\nExtensive experiments compared with 6 state-of-the-art\nmethods on 2 widely-used datasets verify the effectiveness\nof our proposed approach.\nThe rest of this paper is organized as follows. In “The Pro-\nposed Approach” section, we present our UGACH approach\nin detail. The experimental results and analyses are reported\nin “Experiment” section. Finally, we conclude this paper in\n“Conclusion” section.\nThe Proposed Approach\nFigure 1 presents the overview of our proposed approach,\nwhich consists of three parts, namely feature extraction, gen-\nerative model G and discriminative model D. The feature\nextraction part employs image feature and text feature ex-\ntraction to represent unlabeled data of different modalities\nas original features. The detailed implementation of this part\nwill be described at “Experiment” section. Given a data of\none modality, G attempts to select informative data from an-\nother modality to generate a pair of data and send them to\nD. In D, we construct a correlation graph, which can cap-\nture the manifold structure among the original features. D\nreceives the generated pairs as inputs, and also samples pos-\nitive data from constructed graph to form a true manifold\npair. Then D tries to distinguish the manifold and generated\npairs in order to get better discriminate ability. These two\nmodels play a minimax game to boost each other, and the\nﬁnally trained D can be used as cross-modal hashing model.\nWe denote the cross-modal dataset as D = {I, T}, where\nI represents image modality and T represents text modal-\ncel ebr it y dave tv  \ncool  hil l  si nger  \nact or  ku wai t  \nbashar  aziz \nkuw ai ty  7ar akat  \nal jasm i  al shat ti  \nal rai  alj assm i \nhar akat  w oor t hy\ngor geou s j ohnny  \nact or  p \nj ohnnyd epp depp \nom g goodl ook ing  \nal bandr i\nboy  peop le m an \nguy  cel ebr it y \nf r iend  m ovi es \nact or  t r ibu te \nj akegyl l enhaal  \nheat hl edger  \ngod fat her m ati l da\noscar  sq uir r el  \nsqui r rel s smi l ey \nact or  \nacadem yaw ar ds \nsquar l et t bas\nst reet  bw f esti val  \nedi nbur gh act or  \nper f or m er 2b \nm onoch rom i a \nhou rof t hedi am ondl\ni ght  \nhum anb odygal l ery\nbw\npor t r ait  m an \ncel ebr it y \nsungl asses \ndecem ber  fu ji  \nbok eh f l ash act or  \nj el o \ncol um bi auni ver sit\ny teac hersco l ege\nar t f il m  vint age \nnahp ro gr ay sal y \nact re s act or  \nsal lyg ray  \nbr i ti shf il m act or\nImage Feature Extraction\nImages\nTexts\nText Feature Extraction\nGenerated Pairs\nHT\nHashing Layers\nDiscriminative Model D\nGenerative Model G\nCorrelation Graph\nManifold Pairs\nce le brity dave tv cool \nhill s inge r ac tor \nkuwa it bashar az iz  \nkuwa ity 7ara ka t \nalja smi alsha tti a lrai \nalja ssmi ha rakat \nwoorthy\ngorgeous  johnny ac tor \np johnnyde pp de pp \nomg goodlooking \nalbandri\n le ic a f10 ac tor \nyakushima c rusa de r \nm5 shortfilm  fortias p \nkingdomofhea ve n \nja ywe st noctlux \nte ngugaiden\nboy people  man guy \nce le brity friend \nmovies actor tribute  \nja kegylle nhaa l \nheathledger \ngodfathe rm atilda\noscar squirrel \nsquirre ls smiley a ctor \nac ademyawa rds \nsqua rlet tba s\nstre et bw festival \nedinburgh ac tor \nperforme r 2b \nmonoc hromia  \nhourofthe diamondligh\nt \nhuma nbodygalle rybw\nportra it ma n celebrity \nsunglasses  dece mbe r \nfuji bokeh fla sh ac tor \nje llo \ncolumbia unive rs ity \nte ac he rsc olle ge\nart film vinta ge \nnahpro gray sally \nac tress  ac tor sallygra y \nbritishfilm actor\nHI\nHashing Layers\nFeature Extraction\nGenerative \nProbability\nReward Loss\nRelevance \nScore\nRanking Loss\nReward\nportra it ma n celebrity \nsunglasses  dece mbe r \nfuji bokeh fla sh ac tor \nje llo \ncolumbia unive rs ity \nte ac he rsc olle ge\nboy people  man guy \nce le brity friend \nmovies actor tribute  \nja kegylle nhaa l \nheathledger \ngodfathe rm atilda\nce le brity dave tv cool \nhill s inge r ac tor \nkuwa it bashar az iz  \nkuwa ity 7ara ka t \nalja smi alsha tti a lrai \nalja ssmi ha rakat \nwoorthy\nportra it ma n celebrity \nsunglasses  dece mbe r \nfuji bokeh fla sh ac tor \nje llo \ncolumbia unive rs ity \nte ac he rsc olle ge\nboy people  man guy \nce le brity friend \nmovies actor tribute  \nja kegylle nhaa l \nheathledger \ngodfathe rm atilda\nce le brity dave tv cool \nhill s inge r ac tor \nkuwa it bashar az iz  \nkuwa ity 7ara ka t \nalja smi alsha tti a lrai \nalja ssmi ha rakat \nwoorthy\n le ic a f10 ac tor \nyakushima c rusa de r \nm5 shortfilm  fortias p \nkingdomofhea ve n \nja ywe st noctlux \nte ngugaiden\nHT\nHashing Layers\nHI\nHashing Layers\nFigure 1: The overall framework of our proposed unsupervised generative adversarial cross-modal hashing approach (UGACH),\nwhich consists of feature extraction part, generative model G and discriminative model D.\nity. In this paper, D = {I, T} is further split into a re-\ntrieval database Ddb = {Idb, Tdb} and a query set Dq =\n{Iq, Tq}. In the retrieval database Ddb, Idb = {ip}n\np=1,\nTdb = {tp}n\np=1 and n is the number of data pairs. The re-\ntrieval database Ddb is also the training set. In the query\nset Dq, Iq = {ip}t\np=1, Tq = {tp}t\np=1 and t is the num-\nber of data pairs. The aim is to learn cross-modal hashing\nfunctions to generate hash codes for data CI = HI(I) and\nCT = HT (T), so that different modal data that has similar\nsemantic are close in the common Hamming space. In addi-\ntion, we denote the hash code length as l. By the generated\nhash codes, we can retrieve the relevant data by a query of\nany modality from database of another modality efﬁciently.\nGenerative Model\nThe network of generative model has a two-pathway archi-\ntecture, which receives the original features of both images\nand texts as inputs. Each pathway consists of a common rep-\nresentation layer and a hashing layer, whose implementa-\ntions are two fully-connected layers. The ﬁrst layer can con-\nvert the modality speciﬁc features to common representa-\ntions, which make the instances of different modalities mea-\nsurable in a common space. The representation produced by\nthis layer can be denoted as follows:\nφc(x) = tanh(Wcx + bc)\n(1)\nwhere x denotes the original features of images or texts, Wc\nis the weight parameter of the common representation layer,\nand bc is the bias parameter.\nThe hashing layer can map the common representations\ninto binary hash codes, so that the similarity between differ-\nent modalities can be measured by fast Hamming distance\ncalculation. The continuous real values of hash code is de-\nﬁned as:\nh(x) = sigmoid(Whφc(x) + bh)\n(2)\nwhere Wh is the weight parameter and bh is the bias pa-\nrameter. Then we can get the binary codes by a thresholding\nfunction:\nb(x) = sgn(hk(x) −0.5),\nk = 1, 2, · · · , l\n(3)\nwhere l denotes the hash code length. Considering that it is\nhard to optimize binary codes directly, we use relaxed con-\ntinuous real valued hash codes h(x) in the training process.\nGiven a data of one modality, the goal of generative model\nG is to ﬁt the distribution over the manifold structure and se-\nlect informative data of another modality to challenge the\ndiscriminative model. The generative probability of G is\npθ(xU|q), which is the foundation to select relevant instance\nof one modality from unpaired data when given a query of\nanother modality. For example, given a image query qi, the\ngenerative model tries to select relevant text tU from Tdb.\nThe generative probability pθ(xU|q) is deﬁned as a soft-\nmax function:\npθ(xU|q) =\nexp(−∥h(q) −h(xU)∥2)\nP\nxU exp(−∥h(q) −h(xU)∥2)\n(4)\nGiven a query, we can use equation (4) to calculate the prob-\nability of each candidate, which indicates the possibility of\nbecoming a relevant sample.\nDiscriminative Model\nThe network structure of discriminative model is same as the\ngenerative model. The input of this network is the generated\npairs by generative model, and the manifold pairs provided\nby a correlation graph. The goal of discriminative model is\nto distinguish whether an input pair is generated or from the\ncorrelation graph.\nFirst of all, we introduce the correlation graph in the dis-\ncriminative model. We propose a correlation graph to guide\nthe training of discriminative model. The correlation graph\ncan capture the underlying manifold structure across differ-\nent modalities, so that data of different modalities but within\nthe same manifold can have small Hamming distance and\npromote retrieval accuracy.\nSpeciﬁcally,\nwe\nﬁrst\nconstruct\nundirected\ngraphs\nGraphi = (V, Wi) and Grapht = (V, Wt) for image and\ntext modality respectively, where V denotes the vertices\nand Wi and Wt are the similarity matrix. Wi is deﬁned as\nfollows:\nw(p, q) =\n\u001a\n1 :\nxp ∈NNk(xq)\n0 :\notherwise\n(5)\nwhere NNk(xq) denotes the k-nearest neighbors of xq in\nthe training set, similarly we can deﬁne Wt. Then we sam-\nple the data of the true distributions based on the constructed\ngraph. For real pair ptrue(x|qj) provided by the dataset, we\nselect xk as the true relevant instance of given query qj\nand associate them as manifold pair pmanifold(xk|qj), when\nw(k, j) = 1. It is noted that pairwise information naturally\nexists in cross-modal data, thus if the corresponding text tk\nis within the same manifold with text query qj, the paired\nimage ik is also in the same manifold with qj and vice versa.\nBy this deﬁnition, we intend to utilize the underlying data\nmanifold of different modality to guide the training of dis-\ncriminative model. Intuitively, we want the data of different\nmodality but within the same manifold to have small Ham-\nming distance (e.g. small Hamming distance between text\nquery qj and image ik).\nAfter receiving the generated and manifold pairs, discrim-\ninative model predicts a relevance score between each pair\nas the judgment result. So the relevance score between in-\nstance x and its query q is deﬁned as fφ(x, q). The goal of\ndiscriminative model is to distinguish the true relevant data\n(manifold pairs) and non-relevant data (generated pairs) for\na query q accurately.\nThe relevance score of fφ(xG, q) is deﬁned by triplet\nranking loss as follows:\nfφ(xG, q) = max(0, m + ∥h(q) −h(xM)∥2\n−∥h(q) −h(xG)∥2)\n(6)\nwhere xM is a manifold paired instance with query q se-\nlected from the correlation graph, xG is the selected instance\nby generative model, and m is a margin parameter which\nis set to be 1 in our proposed approach. The above equa-\ntion means that we want the distance between manifold pair\n(q, xM) smaller than that of generated pair (q, xG) by a mar-\ngin m, so that the discriminative model can draw a clear dis-\ntinguishing line between the manifold and generated pairs.\nThen discriminative model D uses the relevance score to\nproduce predicted probability of instance x by a sigmoid\nfunction:\nD(x|q) = sigmoid(fφ(x, q)) =\nexp(fφ(x, q))\n1 + exp(fφ(x, q))\n(7)\nThe generative model tries to select informative data to\nchallenge the discriminative model, which limits its ca-\npability to perform cross-modal retrieval. By contrast, the\ndiscriminative model is suitable for retrieving data across\ndifferent modalities, after being promoted greatly by the\ngenerative model. Therefore after the proposed UGACH is\ntrained, we use the discriminative model to perform cross-\nmodal retrieval via produced hash codes.\nAdversarial Learning\nGiven the deﬁnitions of the generative and discriminative\nmodels, we can conduct a minimax game for training them.\nGiven a query, the generative model attempts to generate a\npair which is close to the manifold pair to fool the discrim-\ninative model. The discriminative model tries to distinguish\nbetween manifold pair sampled from the correlation graph\nand the generated pair, which forms an adversarial process\nagainst the generative model. Inspired by the GAN (Good-\nfellow et al. 2014), this adversarial process can be deﬁned:\nV(G, D) = min\nθ\nmax\nφ\nn\nX\nj=1\n(Ex∼ptrue(xM|qj)[log(D(xM|qj))]\n+ Ex∼pθ(xG|qj)[log(1 −D(xG|qj))])\n(8)\nThe generative and discriminative models can be learned\niteratively by maximizing and minimizing the above ob-\nject function. As general training process, the discriminative\nmodel tries to maximize equation (8), while the generative\nmodel attempts to minimize equation (8) and ﬁt the distribu-\ntion over the manifold structure. The learning process of the\ndiscriminative model is ﬁxed when the generative model can\nbe trained as follows:\nθ∗= arg min\nθ\nn\nX\nj=1\n(Ex∼ptrue(xG|qj )[log(sigmoid(fφ∗(xM, qj)))]\n+ Ex∼pθ(xG|qj )[log(1 −sigmoid(fφ∗(xG, qj)))])\n(9)\nwhere fφ∗denotes the discriminative model at previous it-\neration. The traditional GAN uses continuous noise vector\nto generate new data and is trained by stochastic gradient\ndescent algorithm. By contrast, the generative model of our\nproposed UGACH selects data from unlabeled data to gen-\nerate pairs and can not be optimized continuously due to the\ndiscrete selective strategy. We utilize reinforcement learning\nbased parameters update policy to train the generative model\nas follows:\n∇θEx∼pθ(xG|qj)[log(1 + exp(fφ(xG, qj)))]\n=\nm\nX\nk=1\n∇θpθ(xG\nk |qj)log(1 + exp(fφ(xG\nk , qj)))\n=\nm\nX\nk=1\npθ(xU\nk |qj)∇θlogpθ(xG\nk |qj)log(1 + exp(fφ(xG\nk , qj)))\n= Ex∼pθ(xG|qj)[∇θlogpθ(xG|qj)log(1 + exp(fφ(xG, qj)))]\n≃1\nm\nm\nX\nk=1\n∇θlogpθ(xG\nk |qj)log(1 + exp(fφ(xG\nk , qj)))\n(10)\nwhere k denotes the k-th instance selected by generative\nmodel according to a query qj. From the perspective of re-\ninforcement learning, according to the environment qk, xG\nk\nis the action taken by policy logpθ(xG\nk |qj), and log(1 +\nexp(fφ(xG\nk , qj))) acts as the reward, which encourages the\ngenerative model to select data close to the distribution over\nmanifold structure. Finally the trained discriminative model\ncan be used to generate binary codes for any input data of\nany modality, and cross modal retrieval can be performed\nby fast Hamming distance computation between query and\neach data in the database.\nExperiments\nIn this section, we present the experimental results of our\nproposed UGACH approach. We ﬁrst introduce the datasets,\nevaluation metrics and implementation details. Then we\ncompare and analyze the results of UGACH with 6 state-\nof-the-art methods and 2 baseline methods.\nDataset\nIn the experiments, we conduct cross-modal hashing on 2\nwidely-used datasets: NUS-WIDE (Chua et al. 2009) and\nMIRFLICKR (Huiskes and Lew 2008).\n• NUS-WIDE dataset (Chua et al. 2009) is a relatively\nlarge-scale image/tag dataset with 269498 images. Each\nimage has corresponding textual tags, which are regarded\nas the text modality in our experiments. NUS-WIDE\ndataset has 81 categories, but there are overlaps among\nthe categories. Following (Lin et al. 2015), we select the\n10 largest categories and the corresponding 186557 im-\nages. We take 1% data of NUS-WIDE dataset as the query\nset, and the rest as the retrieval database. We randomly\nselected 5000 images as training set for the supervised\nmethods. We represent each image by 4096 dimensional\ndeep features extracted from 19-layer VGGNet, and each\ntext by 1000 dimensional BoW.\n• MIRFlickr dataset (Huiskes and Lew 2008) has 25000\nimages collected from Flickr, which has 24 categories.\nEach image is also associated with text tags. Follow-\ning (Lin et al. 2015), we take 5% of the dataset as the\nquery set and the remaining as the retrieval database. We\nalso randomly select 5000 images as training set for su-\npervised methods. Similarly, we represent each image by\n4096 dimensional deep features extracted from 19-layer\nVGGNet, and each texts by 1000 dimensional BoW.\nCompared Methods\nIn order to verify the effectiveness of our proposed approach,\nthere are 4 unsupervised methods and 2 supervised methods\ncompared in the experiment, including unsupervised meth-\nods CVH (Kumar and Udupa 2011), PDH (Rastegari et al.\n2013), CMFH (Ding et al. 2016) and CCQ (Long et al.\n2016), and supervised methods CMSSH (Bronstein et al.\n2010) and SCM (Zhang and Li 2014). Besides state-of-the-\nart methods, we also compare our UGACH approach with 2\nbaseline methods to verify the effectiveness of our contribu-\ntions.\n• Baseline: We design a baseline method without the cor-\nrelation graph and adversarial training, we denote this\nmethod as Baseline. It is implemented by training the\ndiscriminative model alone with a triplet ranking loss in\nequation (6), where the positive data is only the paired\ndata provided by cross-modal datasets.\n• Baseline-GAN: We add the adversarial training to Base-\nline, which means that we further promote discriminative\nmodel in Baseline by adversarial training deﬁned in equa-\ntion (8).\nComparing Baseline-GAN with Baseline, we can verify the\neffectiveness of our proposed generative adversarial network\nfor cross-modal hashing, and comparing our ﬁnal approach\nUGACH with Baseline-GAN, we can verify the effective-\nness of proposed correlation graph.\nRetrieval Tasks and Evaluation Metrics\nIn the experiments, two retrieval tasks are performed: re-\ntrieving text by image query (image→text) and retrieving\nimages by text query (text→image). Speciﬁcally, we ﬁrst\nobtain the hash codes for the images and texts in the query\nand retrieval database with our UGACH approach and all\nthe compared methods. Then we take one of the images as\nquery, compute the Hamming distance with all text in re-\ntrieval database, and evaluate the ranking list by 3 evaluation\nmetrics to measure the retrieval effectiveness: Mean Average\nPrecision (MAP), precision recall curve (PR-curve) and pre-\ncision at top k returned results (topK-precision), which are\ndeﬁned as follows:\n• The MAP scores are computed as the mean of average\nprecision (AP) for all queries, and AP is computed as:\nAP = 1\nR\nn\nX\nk=1\nk\nRk\n× relk\n(11)\nwhere n is the size of database, R is the number of rel-\nevant images in database, Rk is the number of relevant\nimages in the top k returns, and relk = 1 if the image\nranked at k-th position is relevant and 0 otherwise.\n• Precision recall curve (PR-curve): The precision at certain\nlevel of recall of the retrieved ranking list, which is widely\nused to measure the information retrieval performance.\n• Precision at top k returned results (topK-precision): The\nprecision with respect to different numbers of retrieved\nsamples from the ranking list.\nTable 1: The MAP scores of two retrieval tasks on NUS-WIDE dataset with different lengths of hash codes.\nMethods\nimage→text\ntext→image\n16\n32\n64\n128\n16\n32\n64\n128\nCVH (Kumar and Udupa 2011)\n0.458\n0.432\n0.410\n0.392\n0.474\n0.445\n0.419\n0.398\nPDH (Rastegari et al. 2013)\n0.475\n0.484\n0.480\n0.490\n0.489\n0.512\n0.507\n0.517\nCMFH (Ding et al. 2016)\n0.517\n0.550\n0.547\n0.520\n0.439\n0.416\n0.377\n0.349\nCCQ (Long et al. 2016)\n0.504\n0.505\n0.506\n0.505\n0.499\n0.496\n0.492\n0.488\nCMSSH (Bronstein et al. 2010)\n0.512\n0.470\n0.479\n0.466\n0.519\n0.498\n0.456\n0.488\nSCM orth (Zhang and Li 2014)\n0.389\n0.376\n0.368\n0.360\n0.388\n0.372\n0.360\n0.353\nSCM seq (Zhang and Li 2014)\n0.517\n0.514\n0.518\n0.518\n0.518\n0.510\n0.517\n0.518\nBaseline\n0.540\n0.537\n0.573\n0.598\n0.554\n0.555\n0.583\n0.608\nBaseline-GAN\n0.575\n0.594\n0.602\n0.623\n0.580\n0.609\n0.617\n0.629\nUGACH (Ours)\n0.613\n0.623\n0.628\n0.631\n0.603\n0.614\n0.640\n0.641\nTable 2: The MAP scores of two retrieval tasks on MIRFlickr dataset with different lengths of hash codes.\nMethods\nimage→text\ntext→image\n16\n32\n64\n128\n16\n32\n64\n128\nCVH (Kumar and Udupa 2011)\n0.602\n0.587\n0.578\n0.572\n0.607\n0.591\n0.581\n0.574\nPDH (Rastegari et al. 2013)\n0.623\n0.624\n0.621\n0.626\n0.627\n0.628\n0.628\n0.629\nCMFH (Ding et al. 2016)\n0.659\n0.660\n0.663\n0.653\n0.611\n0.606\n0.575\n0.563\nCCQ (Long et al. 2016)\n0.637\n0.639\n0.639\n0.638\n0.628\n0.628\n0.622\n0.618\nCMSSH (Bronstein et al. 2010)\n0.611\n0.602\n0.599\n0.591\n0.612\n0.604\n0.592\n0.585\nSCM orth (Zhang and Li 2014)\n0.585\n0.576\n0.570\n0.566\n0.585\n0.584\n0.574\n0.568\nSCM seq (Zhang and Li 2014)\n0.636\n0.640\n0.641\n0.643\n0.661\n0.664\n0.668\n0.670\nBaseline\n0.619\n0.631\n0.633\n0.646\n0.625\n0.635\n0.634\n0.649\nBaseline-GAN\n0.630\n0.643\n0.651\n0.664\n0.660\n0.657\n0.670\n0.688\nUGACH (Ours)\n0.685\n0.693\n0.704\n0.702\n0.673\n0.676\n0.686\n0.690\nIt should be noted that the MAP score is computed for all the\nretrieval results with 4 different lengths of hash codes, while\nPR-curve and topK-precision are evaluated on 128 bit hash\ncodes.\nImplementation Details\nIn this section, we present the implementation details of our\nUGACH in the experiments. We take 4096 dimensional fea-\nture extracted from 19-layer VGGNet for images, and use\nthe 1000 dimensional BoW feature for texts. We implement\nthe proposed UGACH by tensorﬂow1. The dimension of\ncommon representation layer is set to be 4096, while the\nhashing layer’s dimension is set to be the same as hash code\nlength.\nMoreover, we train the proposed UGACH in a mini-batch\nway and set the batch size as 64 for discriminative and gen-\nerative models. We train the proposed UGACH iteratively.\nAfter the discriminative model is trained in 1 epoch, the gen-\nerative model respectively will be trained in 1 epoch. The\nlearning rate of UGACH is decreased by a factor of 10 each\ntwo epochs, while it is initialized as 0.01.\nFor the compared methods, we apply the implementations\nprovided by their authors, and follow their best settings to\npreform the experiments. And it is noted that for a fair com-\nparison between different methods, we use the same image\nand text features for all compared methods.\n1https://www.tensorﬂow.org\nExperiment Results\nTables 1 and 2 show the MAP scores of our UGACH and the\ncompared methods on NUS-WIDE and MIRFlickr datasets.\nCompared with state-of-the-art methods, it can be seen\nthat our proposed UGACH approach achieves the best re-\ntrieval accuracy on all 2 datasets. For convenience, we cate-\ngorize these result tables into three parts: unsupervised com-\npared methods, supervised compared methods and baseline\nmethods. On NUS-WIDE dataset, our proposed UGACH\nkeeps the best average MAP score of 0.624 on image→text\nand 0.625 on text→image tasks. Compared with the best un-\nsupervised methods CCQ (Long et al. 2016), our UGACH\nachieves an inspiring accuracy improvement from 0.505 to\n0.624 on image→text task, and improves the average MAP\nscore from 0.494 to 0.625 on text→image task. Even com-\npared with supervised methods SCM seq (Zhang and Li\n2014), our UGACH also improves average MAP scores from\n0.517 to 0.624 on image→text task, and from 0.516 to 0.625\non text→image task. We can observe the similar trends on\nMIRFlickr dataset from Tables 2.\nFigures 2 and 3 show the topK-precision and precision-\nrecall curves on the two datasets with 128 bit code length.\nWe can observe that on both image→text and text→image\ntasks, UGACH achieves the best accuracy among all com-\npared unsupervised methods. And UGACH even achieves\nbetter retrieval accuracy than compared supervised methods\non most of the evaluation metrics, which further demon-\nstrates the effectiveness of our proposed approach.\n0\n200\n400\n600\n800\n1000\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(a) Image−>text task on NUS−WIDE dataset\nTop K\nPrecision\n0\n200\n400\n600\n800\n1000\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n(b) Image−>text task on MIRFlickr dataset\nTop K\nPrecision\n0\n200\n400\n600\n800\n1000\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(c) Text−>image task on NUS−WIDE dataset\nTop K\nPrecision\n0\n200\n400\n600\n800\n1000\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n(d) Text−>image task on MIRFlickr dataset\nTop K\nPrecision\n \n \nUGACH(Ours)\nSCM_seq\nSCM_orth\nCMSSH\nCCQ\nCMFH\nPDH\nCVH\nFigure 2: The topK-precision curves with 128 bit hash codes. The left two ﬁgures present the result of image→text task on\nNUS-WIDE and MIRFlickr datasets, while the right two ﬁgures show the result of text→image task.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(a) Image−>text task on NUS−WIDE dataset\nRecall\nPrecision\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n(b) Image−>text task on MIRFlickr dataset\nRecall\nPrecision\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(c) Text−>image task on NUS−WIDE dataset\nRecall\nPrecision\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n(d) Text−>image task on MIRFlickr dataset\nRecall\nPrecision\n \n \nUGACH(Ours)\nSCM_seq\nSCM_orth\nCMSSH\nCCQ\nCMFH\nPDH\nCVH\nFigure 3: The precision-recall curves with 128 bit hash codes. The left two ﬁgures present the result of image→text task on\nNUS-WIDE and MIRFlickr datasets, while the right two ﬁgures show the result of text→image task.\nCompared with 2 baseline methods on NUS-WIDE\ndataset, we can observe that Baseline-GAN has an im-\nprovement of 0.037 and 0.034 on two tasks, which demon-\nstrates the effectiveness of our proposed generative adver-\nsarial network for cross-modal hashing. Compared our pro-\nposed UGACH with Baseline-GAN, we can observe an im-\nprovement of 0.025 and 0.016 on two tasks, which demon-\nstrates the effectiveness of our proposed correlation graph.\nSimilar trends can be also observed on MIRFlickr dataset.\nConclusion\nIn this paper, we have proposed an Unsupervised Genera-\ntive Adversarial Cross-modal Hashing approach (UGACH),\nwhich intends to make full use of GAN’s ability of unsuper-\nvised representation learning to exploit the underlying mani-\nfold structure of cross-modal data. On one hand, we propose\na generative adversarial network to model cross-modal hash-\ning in an unsupervised fashion. In the proposed UGACH, the\ngenerative model tries to ﬁt the distribution over the mani-\nfold structure, and select informative data of another modal-\nity to challenge the discriminative model. While the discrim-\ninative model learns to preserve traditional inter correlation,\nand the manifold correlations provided by generative model\nto achieve better retrieval accuracy. Those two models are\ntrained in an adversarial way to improve each other and\nachieve better retrieval accuracy. On the other hand, we pro-\npose a graph based correlation learning approach to capture\nthe underlying manifold structure across different modali-\nties, so that data of different modalities but within the same\nmanifold can have smaller Hamming distance and promote\nretrieval accuracy. Experiments compared with 6 state-of-\nthe-art methods on 2 widely-used datasets verify the effec-\ntiveness of our proposed approach.\nThe future works lie in two aspects. Firstly, we will focus\non extending our approach to support retrieval across mul-\ntiple modalities, such as cross-modal retrieval across image,\ntext, video and audio. Secondly, we attempt to extend cur-\nrent framework to other scenarios such as image caption to\nverify its versatility.\nAcknowledgments\nThis work was supported by National Natural Science Foun-\ndation of China under Grant 61771025 and Grant 61532005.\nReferences\n[Baeza-Yates and Ribeiro-Neto 1999] Baeza-Yates, R., and\nRibeiro-Neto, B. 1999. Modern information retrieval, vol-\nume 463. ACM press New York.\n[Bronstein et al. 2010] Bronstein, M. M.; Bronstein, A. M.;\nMichel, F.; and Paragios, N.\n2010.\nData fusion through\ncross-modality metric learning using similarity-sensitive\nhashing.\nIn Computer Vision and Pattern Recognition\n(CVPR), 2010 IEEE Conference on, 3594–3601. IEEE.\n[Cao et al. 2016a] Cao, Y.; Long, M.; Wang, J.; Yang, Q.; and\nYuy, P. S. 2016a. Deep visual-semantic hashing for cross-\nmodal retrieval. In Proceedings of the ACM SIGKDD In-\nternational Conference on Knowledge Discovery and Data\nMining, 1445.\n[Cao et al. 2016b] Cao, Y.; Long, M.; Wang, J.; and Zhu,\nH. 2016b. Correlation autoencoder hashing for supervised\ncross-modal search. In Proceedings of the 2016 ACM on In-\nternational Conference on Multimedia Retrieval, 197–204.\nACM.\n[Chua et al. 2009] Chua, T.-S.; Tang, J.; Hong, R.; Li, H.;\nLuo, Z.; and Zheng, Y. 2009. Nus-wide: a real-world web\nimage database from national university of singapore.\nIn\nProceedings of the ACM international conference on image\nand video retrieval, 48. ACM.\n[Ding et al. 2016] Ding, G.; Guo, Y.; Zhou, J.; and Gao, Y.\n2016. Large-scale cross-modality search via collective ma-\ntrix factorization hashing. IEEE Transactions on Image Pro-\ncessing 25(11):5427–5440.\n[Gionis, Indyk, and Motwani 1999] Gionis, A.; Indyk, P.;\nand Motwani, R. 1999. Similarity search in high dimen-\nsions via hashing. In VLDB, volume 99, 518–529.\n[Goodfellow et al. 2014] Goodfellow, I.; Pouget-Abadie, J.;\nMirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville,\nA.; and Bengio, Y. 2014. Generative adversarial nets. In\nAdvances in neural information processing systems, 2672–\n2680.\n[Hardoon, Szedmak, and Shawe-Taylor 2004] Hardoon,\nD. R.; Szedmak, S.; and Shawe-Taylor, J. 2004. Canonical\ncorrelation analysis: An overview with application to\nlearning methods. Neural computation 16(12):2639–2664.\n[Huiskes and Lew 2008] Huiskes, M. J., and Lew, M. S.\n2008. The mir ﬂickr retrieval evaluation. In Proceedings\nof the 1st ACM international conference on Multimedia in-\nformation retrieval, 39–43. ACM.\n[Jiang and Li 2017] Jiang, Q.-Y., and Li, W.-J. 2017. Deep\ncross-modal hashing. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 3232–\n3240.\n[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky,\nA.;\nSutskever, I.; and Hinton, G. E. 2012. Imagenet classiﬁca-\ntion with deep convolutional neural networks. In Advances\nin neural information processing systems, 1097–1105.\n[Kumar and Udupa 2011] Kumar, S., and Udupa, R. 2011.\nLearning hash functions for cross-view similarity search. In\nIJCAI proceedings-international joint conference on artiﬁ-\ncial intelligence, volume 22, 1360.\n[Lin et al. 2015] Lin, Z.; Ding, G.; Hu, M.; and Wang, J.\n2015.\nSemantics-preserving hashing for cross-view re-\ntrieval. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 3864–3872.\n[Long et al. 2016] Long, M.; Cao, Y.; Wang, J.; and Yu, P. S.\n2016. Composite correlation quantization for efﬁcient mul-\ntimodal retrieval. In Proceedings of the 39th International\nACM SIGIR conference on Research and Development in In-\nformation Retrieval, 579–588. ACM.\n[Peng et al. 2017] Peng, Y.; Zhu, W.; Zhao, Y.; Xu, C.;\nHuang, Q.; Lu, H.; Zheng, Q.; Huang, T.; and Gao, W. 2017.\nCross-media analysis and reasoning: advances and direc-\ntions. Frontiers of Information Technology & Electronic En-\ngineering 18(1):44–57.\n[Peng, Huang, and Zhao 2017] Peng, Y.; Huang, X.; and\nZhao, Y. 2017. An overview of cross-media retrieval: Con-\ncepts, methodologies, benchmarks and challenges.\nIEEE\nTransactions on Circuits and Systems for Video Technology\nPP(99):1–14.\n[Rastegari et al. 2013] Rastegari, M.; Choi, J.; Fakhraei, S.;\nHal, D.; and Davis, L. 2013. Predictable dual-view hash-\ning. In Proceedings of the 30th International Conference on\nMachine Learning (ICML-13), 1328–1336.\n[Reed et al. 2016] Reed, S.; Akata, Z.; Yan, X.; Logeswaran,\nL.; Schiele, B.; and Lee, H. 2016. Generative adversarial\ntext to image synthesis. In Proceedings of the 33rd Interna-\ntional Conference on International Conference on Machine\nLearning-Volume 48, 1060–1069.\n[Song et al. 2013] Song, J.; Yang, Y.; Yang, Y.; Huang, Z.;\nand Shen, H. T. 2013. Inter-media hashing for large-scale\nretrieval from heterogeneous data sources. In Proceedings of\nthe 2013 ACM SIGMOD International Conference on Man-\nagement of Data, 785–796. ACM.\n[Wang et al. 2016] Wang, J.; Liu, W.; Kumar, S.; and Chang,\nS. F. 2016. Learning to hash for indexing big data-a survey.\nProceedings of the IEEE 104(1):34–57.\n[Wang et al. 2017] Wang, J.; Yu, L.; Zhang, W.; Gong, Y.;\nXu, Y.; Wang, B.; Zhang, P.; and Zhang, D. 2017. Irgan:\nA minimax game for unifying generative and discriminative\ninformation retrieval models.\nIn Proceedings of the 40th\nInternational ACM SIGIR Conference on Research and De-\nvelopment in Information Retrieval, 515–524.\n[Wei et al. 2014] Wei, Y.; Song, Y.; Zhen, Y.; Liu, B.; and\nYang, Q. 2014. Scalable heterogeneous translated hashing.\nIn Proceedings of the 20th ACM SIGKDD international con-\nference on Knowledge discovery and data mining, 791–800.\nACM.\n[Weiss, Torralba, and Fergus 2009] Weiss, Y.; Torralba, A.;\nand Fergus, R. 2009. Spectral hashing. In Advances in\nneural information processing systems, 1753–1760.\n[Zhang and Li 2014] Zhang, D., and Li, W.-J. 2014. Large-\nscale supervised multimodal hashing with semantic correla-\ntion maximization. In AAAI, volume 1, 7.\n[Zhang and Peng 2017] Zhang, J., and Peng, Y. 2017. Ssdh:\nSemi-supervised deep hashing for large scale image re-\ntrieval. IEEE Transactions on Circuits and Systems for Video\nTechnology PP(99):1–14.\n[Zhao and Gao 2017] Zhao, Xin, D. G. G. Y. H. J., and Gao,\nY. 2017. Tuch: Turning cross-view hashing into single-view\nhashing via generative adversarial nets. In Proceedings of\nthe Twenty-Sixth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-17, 3511–3517.\n[Zhuang et al. 2014] Zhuang, Y.; Yu, Z.; Wang, W.; Wu, F.;\nTang, S.; and Shao, J. 2014. Cross-media hashing with neu-\nral networks. In Proceedings of the 22nd ACM international\nconference on Multimedia, 901–904. ACM.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2017-12-01",
  "updated": "2017-12-01"
}