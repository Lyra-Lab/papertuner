{
  "id": "http://arxiv.org/abs/1712.04101v1",
  "title": "Deep Reinforcement Learning Boosted by External Knowledge",
  "authors": [
    "Nicolas Bougie",
    "Ryutaro Ichise"
  ],
  "abstract": "Recent improvements in deep reinforcement learning have allowed to solve\nproblems in many 2D domains such as Atari games. However, in complex 3D\nenvironments, numerous learning episodes are required which may be too time\nconsuming or even impossible especially in real-world scenarios. We present a\nnew architecture to combine external knowledge and deep reinforcement learning\nusing only visual input. A key concept of our system is augmenting image input\nby adding environment feature information and combining two sources of\ndecision. We evaluate the performances of our method in a 3D\npartially-observable environment from the Microsoft Malmo platform.\nExperimental evaluation exhibits higher performance and faster learning\ncompared to a single reinforcement learning model.",
  "text": "Deep Reinforcement Learning Boosted by External Knowledge\nNicolas Bougie\nNational Institute of Informatics\n2-1-2 Hitotsubashi, Chiyoda\nTokyo, Japan 101-8430\nnicolas-bougie@nii.ac.jp\nRyutaro Ichise\nNational Institute of Informatics\n2-1-2 Hitotsubashi, Chiyoda\nTokyo, Japan 101-8430\nichise@nii.ac.jp\nABSTRACT\nRecent improvements in deep reinforcement learning have allowed\nto solve problems in many 2D domains such as Atari games. How-\never, in complex 3D environments, numerous learning episodes\nare required which may be too time consuming or even impossible\nespecially in real-world scenarios. We present a new architecture\nto combine external knowledge and deep reinforcement learning\nusing only visual input. A key concept of our system is augmenting\nimage input by adding environment feature information and com-\nbining two sources of decision. We evaluate the performances of\nour method in a 3D partially-observable environment from the Mi-\ncrosof Malmo platform. Experimental evaluation exhibits higher\nperformance and faster learning compared to a single reinforcement\nlearning model.\nCCS CONCEPTS\n•Computing methodologies →Reasoning about belief and\nknowledge; Sequential decision making; Neural networks; Partially-\nobservable Markov decision processes;\nKEYWORDS\nReinforcement Learning, Object Recognition, External Knowledge,\nDeep Learning, Knowledge Reasoning\nACM Reference format:\nNicolas Bougie and Ryutaro Ichise. 2018. Deep Reinforcement Learning\nBoosted by External Knowledge. In Proceedings of SAC 2018: Symposium on\nApplied Computing , Pau, France, April 9–13, 2018 (SAC 2018), 8 pages.\nDOI: 10.1145/3167132.3167165\n1\nINTRODUCTION\nReinforcement learning is a technique which automatically learns\na strategy to solve a task by interacting with the environment and\nlearning from its mistakes. By combining reinforcement learning\nand deep learning to extract features from the input, a wide variety\nof tasks such as Atari 2600 games [14] are eﬃciently solved. How-\never, these techniques applied to 2D domains struggle in complex\nenvironments such as three-dimensional virtual worlds resulting a\nprohibitive training time and an ineﬃcient learned policy.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permited. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\nand/or a fee. Request permissions from permissions@acm.org.\nSAC 2018, Pau, France\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n978-1-4503-5191-1/18/04...$15.00\nDOI: 10.1145/3167132.3167165\nA powerful recent idea to tackle the problem of computational\nexpenses is to modularise the models into an ensemble of experts\n[10]. Since each expert focuses on learning a stage of the task,\nthe reduction of the actions to consider leads to a shorter learning\nperiod. Although this approach is conceptually simple, it does not\nhandle very complex environments and environments with a large\nset of actions.\nA similar idea of extending the information extracted from low-\nlevel architectural modules [11] with high-level ones have been\npreviously used in the area of cognitive systems [15] but does not\ndirectly relies on RL and was limited to a supervised classiﬁcation\nproblem. Te idea was to leverage information about videos with\nexternal ontologies to detect events in videos.\nAnother technique is called Hierarchical Learning [20][1] and is\nused to solve complex tasks, such as ”simulating human brain” [9].\nIt is inspired by human learning which uses previous experiences\nto face new situations. Instead of learning directly the entire task,\ndiﬀerent sub-tasks are learned by the agent. By reusing knowledge\nacquired from the previous sub-tasks, the learning is faster and\neasier. Some limitations are the necessity to re-train the model\nwhich is time consuming and problems related to catastrophic\nforgeting of knowledge on previous tasks.\nIn this paper, our approach focuses on combining deep reinforce-\nment learning and external knowledge. Using external knowledge\nis a way to supervise the learning and enhance information given\nto the agent by introducing human expertise. We augment the\ninput of a reinforcement learning model whose input is raw pixels\nby adding high-level information created from simple knowledge\nabout the task and recognized objects. We combine this model with\na knowledge based decision algorithm using Q-learning [24]. In\nour experiments, we demonstrate that our framework successfully\nlearns in real time to solve a food gathering task in a 3D partially\nobservable environment by only using visual inputs. We evaluate\nour technique on the Malmo platform built on top of a 3D virtual\nenvironment, Minecraf. Our model is especially suitable for tasks\ninvolving navigation, orientation or exploration, in which we can\neasily provide external knowledge.\nTe paper is organized as follows. Section 2 gives an overview of\nreinforcement learning and most recent models. Te environment\nis presented in Section 3. Te main contribution of the paper is\ndescribed in Sections 4. Results are presented in Section 5. Section\n6 presents the main conclusions drawn from the work.\n2\nRELATED WORK\nBelow we give a brief introduction to reinforcement learning and\nthe models used into our system architecture.\narXiv:1712.04101v1  [cs.LG]  12 Dec 2017\nSAC 2018, April 9–13, 2018, Pau, France\nN. Bougie et al.\n2.1\nReinforcement Learning\nReinforcement learning consists of an agent learning a policy by\ninteracting with an environment. At each time-step the agent\nreceives an observation st and choose an action at . Te agent\ngets a feedback from the environment called a reward rt . Given\nthis reward and the observation, the agent can update its policy to\nimprove the future rewards.\nGiven a discount factor γ, the future discounted reward, called\nreturn Rt , is deﬁned as follows :\nRt =\nT\nÕ\nt′=t\nγ t′−trt′\n(1)\nTe goal of reinforcement learning is to learn to select the action\nwith the maximum return Rt achievable for a given observation\n[19]. From Equation (1), we can deﬁne the action value Qπ at a\ntime t as the expected reward for selecting an action a for a given\nstate st and following a policy π.\nQπ (s,a) = E [Rt | st = s,a]\n(2)\nTe optimal policy is deﬁned as selecting the action with the opti-\nmal Q-value, the highest expected return, followed by an optimal\nsequence of actions. Tis obeys the Bellman optimality equation:\nQ∗(s,a) = E\n\u0014\nr + γ max\na′ Q∗(s\n′,a\n′) | s,a\n\u0015\n(3)\nWhen the state space or the action space is too large to be repre-\nsented, it is possible to use an approximator to estimate the action-\nvalue Q∗(s,a):\nQ∗(s,a) ≈Q(s,a;θ)\n(4)\nNeural networks are a common way to approximate the action-\nvalue. Te parameters of the neural network θ can be optimized\nto minimize a loss function Li deﬁned as the expected temporal\ndiﬀerence error of Equation (3):\nLi(θi) = Es,a,r,s′ \u0002\n(yi −Q(s,a;θi))2\u0003\n(5)\nwhere yt = rt + γ maxa′ Qθtarдet (st+1,a\n′)\nTe gradient of the loss function with respect to the weights is the\nfollowing :\n∇θiLi(θi) = Es,a,r,s′\nh\n(r + γ maxa′ Q(s\n′,a\n′;θi−1) −Q(s,a;θi))∇θiQ(s,a;θi)\ni\n(6)\nMnih et al. (2013) used this idea and created the famous method\ncalled Deep Q-learning (DQN) [14]. However, the learning may be\nslow due to the propagation of the reward to the previous states\nand actions.\nSimilarly, the value function V π (s) which represents the expected\nreturn for a state s following a policy π is deﬁned as follows:\nV π (s) = E [Rt | st = s]\n(7)\nSome reinforcement learning models such as Actor-Critic or Dueling\nNetwork decompose the Q-values Q(s,a) into two more fundamen-\ntal values, the value functionV (s) and the advantage functionA(a,s)\nwhich is the beneﬁt of taking an action compared to the others.\nFigure 1: Actor-critic model\nA(s,a) = Q(s,a) −V (s)\n(8)\n2.2\nAsynchronous Advantage Actor-Critic\n(A3C)\nIt was shown that combining methods of deep learning and rein-\nforcement learning is very unstable. To deal with this challenge,\nmany solutions store the agent’s data into a memory, then the data\ncan be batched from the memory. It is done because sequences of\ndata are highly correlated and can lead to learn from its mistakes\nresulting in a worse and worse policy. A3C [13] avoids computa-\ntional and memory problems by using asynchronous learning. It\nallows the usage of on-policy reinforcement learning algorithms\nsuch as Q-learning [24] or advantage actor-critic. Te learning is\nstabilized without using experience replay and the training time is\nreduced linearly in the number of learners.\nTe learners of A3C which use their own copy of the environ-\nment are trained in parallel. Each process will learn a diﬀerent\npolicy and hence will explore the environment in a diﬀerent way\nleading to a much more eﬃcient exploration of the environment\nthan with a replay memory. A process updates its own policy based\non an advantage actor-critic model [8] (Figure 1). Te actor-critic\nmodel is composed by an actor which acts out a policy and a critic\nwhich evaluates the policy. Te main thread is updated periodically\nusing the accumulated gradients of the diﬀerent processes.\nTe critic takes as input the state and the reward and outputs a score\nto criticize the current policy. In the case of advantage actor-critic\nmodel, the critic estimates the advantage function which requires\nto estimate V and Q.\nTe actor does not have access to the reward but only to the state\nand the advantage value outputed by the critic. Contrary to the\ncritic which is value based, the actor directly works into the policy\nspace and changes the policy towards the best direction estimated\nby the critic. Optimization techniques such as stochastic gradient\ndescent are used to ﬁnd θ that maximizes the policy objective\nfunction J(θ). Te policy gradient objective function ∇θ J(θ) is\ndeﬁned as follows:\n∇θ J(θ) = Eπ,θ\n\u0002\n∇θloдπθ (s,a)Aw(s,a)\n\u0003\n(9)\nwhere Aw(s,a) is a long term estimation of the reward to allow the\nactor to go in the direction that the critic considers the best.\nDeep Reinforcement Learning Boosted by External Knowledge\nSAC 2018, April 9–13, 2018, Pau, France\nFigure 2: Dueling network architecture\n2.3\nDueling Network\nTe idea is to separately compute the advantage function and the\nvalue function and combine these two values at the ﬁnal layer\n(Figure 2). Te dueling network [23] may not need to care about\nboth values and the advantage at any given time. Te estimation\nof a state value is more robust by decoupling it from the necessity\nof being atached to a speciﬁc action. Tis is particularly useful\nin states where its actions do not aﬀect the environment in any\nrelevant way. For example, moving lef or right only maters when\na collision is upcoming. Te second stream, which estimates the\nadvantage function values, is relevant when the model needs to\nmake a choice over the actions in a state. Te Bellman’s equation\n(3) becomes now:\nQ(s,a;θ,α, β) = V (s,θ, β)+(A(s,a;θ,α)−max\na′ ∈A\nA(s,a\n′;θ,α)) (10)\nAnd by changing the max by a mean:\nQ(s,a;θ,α, β) = V (s,θ, β) + (A(s,a;θ,α) −\n1\n|A|\nÕ\na′\nA(s,a\n′;θ,α))\n(11)\nWith θ the shared parameters of the neural network, α the parame-\nters of the stream of the advantage function A and β the parameters\nof the stream of the value function V. Since the output of the two\nstreams produces a Q function, it can be trained with many existing\nalgorithms such as Double Deep Q-learning (DDQN) [22] or SARSA\n[18]. Te main advantage is that for each update of the Q-values,\nthe value function is updated whereas with traditional Q-learning\nonly one action-value is updated.\n3\nTASK & ENVIRONMENT\nWe built an environment on the top of the Malmo platform [5] to\nevaluate our idea. Malmo is an open-source platform that allows\nus to create scenarios with Minecraf engine. To test our model, we\ntrained an agent to collect foods in a ﬁeld with obstacles. Te agent\ncan only receive partial information of the environment from his\nviewpoint. We only use image frames to solve the scenario. An\nexample of screenshot with the object recognition results is shown\nin Figure 3.\nTe goal of the agent is to learn to have a healthy diet. It involves\nto recognize the objects and learn to navigate into a 3D environ-\nment. Te task consists in picking up food from the ground for\n30 seconds. Food is randomly spread across the environment and\nfour obstacles are randomly generated. Each of the 20 kinds of food\nFigure 3: Screenshot of the environment\nhas an associated reward when the agent picks it up. Tis reward\nis a number between +2 (healthy) and -2 (unhealthy). Tey are\ndistributed equitably, meaning that a random agent should get a\nreward of 0.\nTe setings were: window size: 400 × 400 pixels, actions: turn\nlef, turn right, crouch, jump, move straight and move back , number\nof objects: 200, number of obstacles: 4. Te actions turn lef and\nturn right are continuous actions to make the learning smoother as\nconsecutive frames are more similar.\n4\nSYSTEM ARCHITECTURE\n4.1\nGeneral Idea\nFigure 4 describes the global architecture of our new framework\ncalled DRL-EK. It consists of four modules: an Object Recognition\nModule, a Reinforcement Learning Module, a Knowledge Based\nDecision Module, and an Action Selection Module.\nTe object recognition module identiﬁes the objects within the\ncurrent image and generates high-level features. Tese features of\nthe environment are then used to augment the raw image input\nto the reinforcement learning module. In parallel, the knowledge\nbased decision module selects another action by combining external\nknowledge and the object recognition module outputs. To manage\nthe trade-oﬀbetween these two sources of decision we use an action\nselection module. Te chosen action is then acted by the agent and\nthe modules are updated from the obtained reward.\n4.2\nObject Recognition Module\nInjecting external knowledge requires to understand the scene at a\nhigh-level in order to be interpreted by a human. Te easiest way\nto understand an image is to identify the objects. For example, it is\nintuitive to give more importance to the actions turn or jump than\nthe action move straiдht when an obstacle is in front of the agent.\nTo recognize the objects, the module uses You Only Look Once\n(YOLO) [16][17] library which is based on a deep convolutional\nneural network. As input, we use an RGB image of size 400×400\npixels. YOLO predicts in real time the bounding boxes, the labels\nand conﬁdence scores between 0 and 100 of the objects. An example\nSAC 2018, April 9–13, 2018, Pau, France\nN. Bougie et al.\nObjects\nKnowledge Based\nDecision Module\nAction1\nReinforcement Learning\nModule\nAction 2\nImage\nYolo\nYolo\nMeta-features\nlearning model\nA3C\nQ-learning\nLong time\nplanning model\nObject Recognition\nModule\nFinal action\nAction Selection\nModule\nFigure 4: Global architecture of DRL-EK\nis shown in Figure 3. We trained YOLO on a dataset of 25 000\nimages with twenty diﬀerent classes corresponding to the food that\nis presented in the environment.\nTe model is trained oﬀ-line before starting the learning into the\nenvironment. Te neural network architecture is adapted from the\none proposed by Redmon et al. (2016) for the Pascal VOC dataset\n[17]. In order to recognize small objects, the size of cells is decreased\nfrom 7 to 5 pixels and the number of bounding boxes for each cell\nis increased from 2 to 4.\nIn addition to the identiﬁed objects, the module creates feature\ninformation about the current frame. To generate these high-level\nabstraction features we combine the recognized objects and external\nknowledge. Tey are then used as input by the reinforcement\nlearning module and the knowledge based decision module. We\ndesigned two types of features presence of objects and important\narea.\n4.2.1\nPresence Of Objects Features. Te ﬁrst type of features is\na vector of booleans which indicates whether an object appears or\nnot within the current image. Te size of this vector is the number\nof diﬀerent objects in the environment. Since some objects are not\nhelpful to solve the task, we can decide to only take some of the\nobjects into account based on our knowledge about the task.\n4.2.2\nImportant Area Features. As the position of objects is im-\nportant, we encode information about objects within each area of\nFigure 5: Important areas of an image\nInputs\nFigure 6: Injection of new features into the reinforcement\nlearning module (A3C)\nthe image. We split the image into k rectangles vertically and hori-\nzontally. So, the number of areas is k2 and for each one we compute\na score (Figure 5). Te score of an area is the sum of the score of\nthe objects within this area. External knowledge can be introduced\nby shaping the score of the objects. From our knowledge about\nthe task, we manually deﬁned the scores to indicate whether or\nnot an object is important to solve the task. To tackle problems\nwith partially observable environments, we keep track of recent\ninformation by concatenating the array of scores of the current\nframe with the arrays of the two previous frames.\nIn our experiments, the top half of the images only contains\nthe sky so we computed the important area features on the half\nbotom of the images. We gave a score of -15/+5 to foods we think is\nunhealthy (cake,cookie) / healthy (meat, fruit) and 0 for the others.\nTat way, if an area contains a healthy food such as a fruit and a\nsweet food, then the score of the area will be lower than an area\ncontaining only a fruit or no object. We set the number of rectangles\nto 3 (9 areas in total: 3×3). We found that with a higher number of\nareas the amount of encoded information is bigger but information\nquality of each area is worse than with 3 areas.\n4.3\nReinforcement Learning Module\nFor a computer, learning from an image is diﬃcult and requires a\nlot of training steps. To deal with it, the entry point of most of the\nreinforcement learning models is a recurrent convolutional neural\nnetwork [3] to extract temporal and spatial features of the image.\nDeep Reinforcement Learning Boosted by External Knowledge\nSAC 2018, April 9–13, 2018, Pau, France\nWe trained a deep reinforcement learning model to perform\npolicy learning and we modiﬁed the neural network structure to\nincorporate external knowledge. In addition to the image input, we\ninjected presence of objects or important area features which are\ncreated by the object recognition module. In the neural network,\nwe give to a Long Short Term Memory (LSTM) [4] the output of\nthe last convolutional layer concatenated with the new features\n(Figure 6). Te next layers of the neural network are two separated\nfully-connected layers to estimate the value function V (s) and the\npolicy π(a|st ). Te purpose is to help the model at the beginning\nof the training to recognize and focus on objects. Te new features\naugment the raw image input to the reinforcement learning model\nby adding high-level information. For example, from presence of\nobjects features the model can decide which actions are allowed or\nnot. If a door is detected some of the actions may become irrelevant\nsuch as jumpinд.\nTe choice of the reinforcement learning model highly depends\non the environment. Since the model at each time-step takes an\ninput and outputs an action, we can easily substitute most of the\nreinforcement learning techniques such as Deep Q-learning (DQN)\n[14], Deep Deterministic Gradient Policy (DDPG) [12], Dueling Net-\nwork [23] or Asynchronous Actor-Critic Agents(A3C) [13] by using\na recurrent convolutional neural network as state approximator.\nA3C is the most suitable model to solve our task. We tested and\nempirically searched the best parameters such as a good convolu-\ntional neural network architecture and the choice of the optimizer\nof this model. It provides a baseline to evaluate the importance of\neach module of our architecture on the ﬁnal policy.\nWorking directly with 400 × 400 pixel images is too computa-\ntionally demanding. We apply image preprocessing before training\nA3C. Te raw frame is resized to 200 × 200 pixels. To decrease the\nstorage cost of the images we convert the image scale from 0 −255\nto 0 −1.\nWe set the number of workers of A3C to 3 and a convolutional\nrecurrent neural network is used to approximate the states. Te\nreason why we use a recurrent neural network is because the envi-\nronment is partially observable. Te input of the neural network\nof A3C estimator consists in a 200×200×3 image. Te 4 ﬁrst layers\nconvolve with the following parameters (ﬁlter: 32,32,32,32, kernel\nsize: 8×8,4×4,3×3,2×2, stride size: 2,2,2,1) and apply a rectiﬁer non-\nlinearity. It is followed by a LSTM layer of size 128 to incorporate\nthe temporal features of the environment. Two separate fully con-\nnected layers predict the value function and a policy function, a\ndistribution of probability over the actions. We use RMSProp [21]\nas optimization technique with ϵ = 10−6 and minibatches of size\n32 for training.\n4.4\nKnowledge Based Decision Module\nWe believe that the agent is not able to accurately understand and\ntake into account the objects of the environment. A human can\neasily understand and make a decision from high-level features such\nas the utility or name of an object. Geting this level of abstraction\nis diﬃcult but we can help the machine by giving it less low-level\ninformation such as color of pixels but more high-level information\nsuch as the importance of an area of the image.\nMoreover, when the reinforcement learning module is fed with\nthe images and the presence of objects or important areas fea-\ntures, the training time is long due to the size of state space. Te\nknowledge based decision module is able to select an action using\nexternal knowledge and high-level features generated by the object\nrecognition module and without direct access to the image. We\npropose two diﬀerent approaches, a long time planning model or a\nmeta-feature learning model.\n4.4.1\nLong Time Planning Model. Our approach to solving the\ntask is based on planning a sequence of actions. We designed and\ndeveloped a long time planning model without learning which\ncombines traditional test-case algorithms and planning. Te model\ntakes as input the probabilities and the bounding boxes of the\nobjects detected within the current frame.\nWe store in an array the sequence of planned actions. At each\ntime-step, the model checks if the previously planned sequence of\nactions is still the optimal one and if it is not the case (for example\nthe next action is jump but there is no obstacle) the algorithm\nupdates it, otherwise the ﬁrst action in the array is returned.\nTo update or plan a sequence of actions, the model uses the\ninformation about the objects and manually created rules. First, a\ntest-case veriﬁcation selects the possible actions. An example of\nsimple rule is, if the object cookie is on the lef of the image then the\naction turn lef is forbidden. Ten, to decide of the action among\nthe remaining actions we use a priority list. If the selected action is\nrelated to the movement, the model estimates the best angle and\nthe necessary number of steps to perform it. Finally, the ﬁrst of the\nplanned actions is returned.\nTo decrease the number of rules we discretized the image space\ninto four areas: center, lef, right, other. We designed 43 rules to\nprevent the agent from going in the direction of the food we think\nis dangerous. To avoid static behaviour, we give more priority to\nthe actions turn lef, turn right, move straight than the others in the\npriority list.\n4.4.2\nMeta-feature Learning Model. In our previous approach,\nwe manually create rules to reason on high-level features. To auto-\nmatically learn the rules and select the optimal action from them,\nwe use a deep reinforcement learning model such as DQN or duel-\ning network. Unlike the reinforcement learning module which uses\nthe image, the only input is high-level features such as important\nareas or presence of objects. As the input is much smaller than an\nimage, a simple neural network can be trained to approximate the\nstates. Te smaller number of parameters leads to a faster learning\nthan a model trained from visual information.\nIn experiments, we trained a dueling network combined with a\ndouble deep Q-learning (DDQN). It empirically gives a smoother\nlearning than most of the other reinforcement learning models.\nA neural network approximates the states. It consists in 3 fully\nconnected layers of size 100 with a rectiﬁer nonlinearity activa-\ntion function. Network was trained using the Adam algorithm [7],\nlearning rate of 10−3 and minibatches of size 32. As input, we used\na slightly modiﬁed version of the important area features outputed\nby the object recognition module. To create important area features,\nwe ﬁltered the objects too far and the objects with a conﬁdence\nscore less than 0.25. Taking into account an object such as grass is\nSAC 2018, April 9–13, 2018, Pau, France\nN. Bougie et al.\nirrelevant and makes the learning more diﬃcult. We only used dan-\ngerous or very healthy (10 objects out of 20) objects and removed 2\nobjects that we know are diﬃcult to distinguish.\n4.5\nAction Selection Module\nTe module aggregates the actions proposed by the reinforcement\nlearning module and the knowledge based decision module to select\nthe action that the agent will perform in the environment. Te\ngoal is to take advantage of the fast learning of the knowledge\nbased decision module and the quality of the policy learned by the\nreinforcement learning module. An important aspect of the action\nselection, is selecting an action with the highest expected return\nbut also detecting error paterns to correct them. An error must be\ndetected when an action which has not been proposed could oﬀer\na higher return.\nTis is achieved by training a Deep Q-learning model to select\nthe best action, detect and correct the error paterns. Tere is no\nrestriction on the possible actions meaning that the ﬁnal action may\nbe diﬀerent from the two proposed actions if an error is detected. We\nencode the proposed action by the two modules into two indicator\nvectors. An indicator vector is a binary vector with only one unit\nturned on to indicate the recommended action. Te neural network\ninput is the concatenation of these two vectors.\nTe Q-learning algorithm is trained using a Boltzmann distribu-\ntion (Equation 12) as explorer and experience replay (each experi-\nence is stored into memory and the algorithm is run on randomly\nsampled batch) with a memory of size 106. Equation 12 gives the\nprobability of selecting an action in a given state s.\nPs(a) =\nexp(Q(s,a)/τ)\nÍ\na′ ∈A exp(Q(s,a\n′)/τ)\n(12)\nTe Q-network is composed of 2 hidden fully connected layers of\nsize 50 and are followed by rectiﬁed linear units.\n5\nEXPERIMENTS\nWe conducted several experiments for evaluating our architecture.\nIn all our experiments, we set the discount factor to 1.0. According\nto our diﬀerent tests, on average the best reward that a perfect\nagent can get in 30 seconds is 9.\n5.1\nObject Recognition\nWe evaluated our object recognition module for understanding\nthe correctness of obtained object information in the environment.\nFigure 7 reports the object recognition module performance. In this\nexperiment, we measured the mean average precision (mAP) as the\nerror metric. Te results are similar to the results presented by the\nauthors (Redmon et al., 2017) [17] on the Pascal VOC. dataset [2].\nWe obtained a mean average precision of 53.47. Although other\nlibraries could oﬀer higher performance, the real-time detection\nwas the main criterion for selecting YOLO.\nWe noticed that most of the errors are false positives (68.3%)\nwhereas the false negatives (31.7%) are uncommon. It leads to an\nagent with a policy more greedy and safer. As shown in the ﬁgure,\nthe average precision is similar for every class. Te performance of\nYOLO is slightly aﬀected by the complexity of the objects such as\ntheir shape, color or size.\nFigure 7: Average precision over all the classes obtained by\nthe object recognition module\nFigure 8: Evolution of the reward of the long time planning\nmodel\n5.2\nLong Time Planning Model\nNext, we tested the long time planning model in the knowledge\nbased decision module to evaluate the eﬀectiveness of this approach.\nWe only utilized the object recognition module and the knowledge\nbased decision module in our framework. Te long time planning\nmodel performs much beter than a random agent with an average\nreward of 3.3 (Figure 8), since expected reward of random agent is\n0.\nTe most likely cause of the wide variance is the diﬃculty to\nhandle all possible cases with manually created rules. For instance,\nthe agent has diﬃculty in gathering food near obstacles. Since there\nis no learning, the quality of the agent only depends of the quality\nof the rules and is not able to converge. On the other hand, from\nthe ﬁrst episode the average reward is much higher than any other\nlearning based models.\nDeep Reinforcement Learning Boosted by External Knowledge\nSAC 2018, April 9–13, 2018, Pau, France\nFigure 9: Average rewards of the meta-feature learning\nmodel with diﬀerent parameters. Te rewards were aver-\naged over 200 episodes afer 5000 training episodes\nFigure 10: Average reward using meta-feature learning\n5.3\nMeta-feature Learning Model\nTo evaluate performance of the meta-feature learning model we\nuse as baseline the long time planning model. We optimized its\nparameters, by sampling hyper-parameters from categorical distri-\nbutions:\n• Number of areas sampled from {4, 9, 16, 25}\n• Number of hidden layers from [1, 5]\n• Size of hidden layers sampled from {25, 50, 100, 200, 300}\nFigure 9 reports an example of hyper-parameter optimization re-\nsults. Each cell corresponds to a conﬁguration of parameters. As\ncan be seen on the ﬁgure, a number of hidden layers larger than\ntwo or a large number of areas results in lower performance. Te\nbest hyper-parameters are 9 areas, and a neural network with 3\nhidden fully-connected layers of size 100. Training time is about 4\nhours for each conﬁguration on a Nvidia Titan-X GPU.\nFigure ? shows how the average total reward of the meta-feature\nlearning model evolves during training with the optimal setings.\nTe dueling network architecture eﬀectively learns to solve the task\nfrom the important area features. Te learning is fast during the ﬁrst\n3000 episodes and the average reward quickly converges around\n5.4. It is also interesting to note that this approach rapidly achieves\nhigher performance than the long time planning model. Automatic\nrule learning is more eﬀective than manual rule construction. Unfor-\ntunately, the rules cannot be represented in a human-interpretable\nway.\nFigure 11: Frequency of selection of each action\nFigure 12: Performance of DRL-EK comparing to DQN, Du-\neling Network and A3C\n5.4\nAction Selection\nWe also evaluated the characteristics of the action selection module.\nWe report the percentage of actions which is selected from the\nknowledge based decision module (action 1) and from the reinforce-\nment learning module (action 2) against other actions. We measured\nthe frequency of selection of each action every 350 episodes. As\nshown in Figure 11 the action selection module at the beginning\nselects equally the actions then more the action 1 and gradually\ngive more importance to the action 2. Te results conﬁrm our in-\ntuition, the module selects the action of the most eﬃcient module\nand adapts over time the trade-oﬀbetween the sources of decision\nto always select the best one.\n5.5\nGlobal Model Evaluation\nFinally, we report the average reward of our whole framework\ntrained using the injection of important areas features into the\nreinforcement learning module and a meta-feature learning model\nSAC 2018, April 9–13, 2018, Pau, France\nN. Bougie et al.\nTable 1: Te table compares average reward for various fea-\ntures injected into A3C. Te reinforcement learning module\nwas evaluated alone for 12 000 episodes.\nSetings\nRewards\nA3C\n5.6\nA3C + presence of objects features\n5.8\nA3C + important area features\n6.1\nas knowledge based decision module. Figure 12 compares our\nproposed method with the best performing reinforcement learning\nmethods. Tese models learn the policy only using raw pixels.\nDRL-EK boosts A3C by injecting important area features. To\nselect these features, we compared A3C+presence of objects features\nand A3C+important area features (Table 1). In both cases, the results\nshow that adding a new input to the reinforcement learning module\nimproves the quality of the policy.\nAs can be seen, DQN gives the worst results with an average\nreward of 3.2, ≈40% less than A3C afer converging. Afer 12 000\nepisodes, the average reward of the dueling network architecture\ntrained with a double deep Q-learning is around 4.4 while A3C is\nable to achieve an average reward of 5.6. Surprisingly the meta-\nfeature learning model trained alone (Figure 10) achieves higher\nperformance than learning only from the image with a dueling\nnetwork or a DQN model.\nAsynchronous advantage actor-critic tends to learn faster than\nany other reinforcement learning based models. We believe this\nis due to the 3 parallel workers of A3C which oﬀer a nonlinear\nsigniﬁcant speedup.\nTese results show that our architecture, DRL-EK, outperforms\nthe baselines. Its average reward is around 15% beter than A3C\nafer 14 000 episodes. Moreover, the performance at the beginning\nof the training and the learned policy of DRL-EK is signiﬁcantly\nbeter than all other models. One thing to note is that the action\nselection module tends to select an action diﬀerent from action 1\nand 2 (Figure 11). Te continuous increase of the average reward\nof DRL-EK and this observation indicates that the action selection\nmodule is partially able to learn to correct the errors.\nTe experiments demonstrate the importance of each module of\nour system. With an average time for one step of 0.43 seconds on a\nNvidia Titan-X (Pascal) GPU, DRL-EK can be trained in real time.\n6\nCONCLUSION\nWe proposed a new architecture to combine reinforcement learn-\ning with external knowledge. We demonstrated its ability to solve\ncomplex tasks in 3D partially observable environments with image\nas input. Our central thesis is enhancing the image by generating\nhigh-level features of the environment. Further beneﬁts stem from\neﬃciently combining two sources of decision. Moreover, our ap-\nproach can be easily adapted to solve new tasks with a very limited\namount of human work. We have demonstrated the eﬃcacy of our\narchitecture to decrease the training time and to learn a beter and\nmore eﬃcient policy.\nIn the future, a promising research area is building an agent\nthat incorporates human feedback. Another challenge is how to\nintegrate complex and structured external knowledge such as on-\ntologies or textual data into our model. Finally, we are interested in\nextending our experiments to new environments such as VizDoom\n[6].\nREFERENCES\n[1] Andrew G. Barto and Sridhar Mahadevan. 2003. Recent Advances in Hierarchical\nReinforcement Learning. Discrete Event Dynamic Systems 13, 4 (2003), 341–379.\n[2] Mark Everingham, Luc J. Van Gool, Christopher K. I. Williams, John M. Winn,\nand Andrew Zisserman. 2010. Te Pascal Visual Object Classes (VOC) Challenge.\nInternational Journal of Computer Vision 88, 2 (June 2010), 303–338.\n[3] Mathew J. Hausknecht and Peter Stone. 2015. Deep Recurrent Q-Learning for\nPartially Observable MDPs. CoRR abs/1507.06527 (2015).\n[4] Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long Short-Term Memory.\nNeural Computation 9, 8 (Nov. 1997), 1735–1780.\n[5] Mathew Johnson, Katja Hofmann, Tim Huton, and David Bignell. 2016. Te\nMalmo Platform for Artiﬁcial Intelligence Experimentation. In Proceedings of the\nTwenty-Fifh International Joint Conference on Artiﬁcial Intelligence, New York, NY,\nUSA, 9-15 July 2016, Subbarao Kambhampati (Ed.). IJCAI/AAAI Press, 4246–4247.\n[6] Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Woj-\nciech Ja´skowski. 2016. Vizdoom: A doom-based ai research platform for visual\nreinforcement learning. In Proceedings of IEEE conference on Computational Intel-\nligence and Games (CIG). IEEE, 1–8.\n[7] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic\nOptimization. CoRR (2014).\n[8] Vijay R. Konda and John N. Tsitsiklis. 1999. Actor-Critic Algorithms. In Proceed-\nings of Neural Information Processing Systems, Sara A. Solla, Todd K. Leen, and\nKlaus-Robert M¨uller (Eds.). Te MIT Press, 1008–1014.\n[9] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gersh-\nman. 2016. Building Machines Tat Learn and Tink Like People. Behavioral\nand Brain Sciences (2016), 1–101.\n[10] Guillaume Lample and Devendra Singh Chaplot. 2017. Playing FPS Games with\nDeep Reinforcement Learning. In Proceedings of AAAI. 2140–2146.\n[11] Antonio Lieto, Christian Lebiere, and Alessandro Oltramari. 2017. Te knowledge\nlevel in cognitive architectures: Current limitations and possible developments.\nCognitive Systems Research (2017).\n[12] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess,\nTom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015.\nContinu-\nous control with deep reinforcement learning.\nArXiv e-prints (Sept. 2015).\narXiv:cs.LG/1509.02971\n[13] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timo-\nthy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchro-\nnous Methods for Deep Reinforcement Learning. In Proceedings of International\nConference on Machine Learning. 1928–1937.\n[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and\nM. Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. ArXiv\ne-prints (Dec. 2013). arXiv:cs.LG/1312.5602\n[15] Oltramari and Lebiere. 2012. Using Ontologies in a Cognitive-Grounded System:\nAutomatic Action Recognition in Video-Surveillance. (2012).\n[16] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2015.\nYou Only Look Once: Uniﬁed, Real-Time Object Detection. CoRR abs/1506.02640.\nhtp://arxiv.org/abs/1506.02640\n[17] Joseph Redmon and Ali Farhadi. 2016. YOLO9000: Beter, Faster, Stronger. CoRR\nabs/1612.08242 (2016).\n[18] G. A. Rummery and M. Niranjan. 1994. On-Line Q-Learning Using Connectionist\nSystems. Technical Report. University of Cambridge.\n[19] Richard S Suton and Andrew G Barto. 1998. Reinforcement learning: An intro-\nduction. MIT press Cambridge.\n[20] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, and Shie Man-\nnor. 2017. A Deep Hierarchical Approach to Lifelong Learning in Minecraf. In\nProceedings of AAAI Conference on Artiﬁcial Intelligence. 1553–1561.\n[21] Tijmen Tieleman and Geoﬀrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the\ngradient by a running average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning (April 2012).\n[22] Hado van Hasselt, Arthur Guez, and David Silver. 2015. Deep Reinforcement\nLearning with Double Q-learning. CoRR abs/1509.06461 (2015).\n[23] Ziyu Wang, Tom Schaul, Mateo Hessel, Hado van Hasselt, Marc Lanctot, and\nNando de Freitas. 2016. Dueling Network Architectures for Deep Reinforcement\nLearning. In Proceedings of the 33rd International Conference on Machine Learning,\nICML 2016, New York City, NY, USA, June 19-24, 2016, Maria-Florina Balcan and\nKilian Q. Weinberger (Eds.). 1995–2003.\n[24] Christopher J. C. H. Watkins and Peter Dayan. 1992. Q-learning. Machine\nLearning 8, 3 (01 May 1992), 279–292.\n",
  "categories": [
    "cs.LG",
    "I.2.6"
  ],
  "published": "2017-12-12",
  "updated": "2017-12-12"
}