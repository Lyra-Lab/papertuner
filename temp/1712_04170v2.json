{
  "id": "http://arxiv.org/abs/1712.04170v2",
  "title": "Interpretable Policies for Reinforcement Learning by Genetic Programming",
  "authors": [
    "Daniel Hein",
    "Steffen Udluft",
    "Thomas A. Runkler"
  ],
  "abstract": "The search for interpretable reinforcement learning policies is of high\nacademic and industrial interest. Especially for industrial systems, domain\nexperts are more likely to deploy autonomously learned controllers if they are\nunderstandable and convenient to evaluate. Basic algebraic equations are\nsupposed to meet these requirements, as long as they are restricted to an\nadequate complexity. Here we introduce the genetic programming for\nreinforcement learning (GPRL) approach based on model-based batch reinforcement\nlearning and genetic programming, which autonomously learns policy equations\nfrom pre-existing default state-action trajectory samples. GPRL is compared to\na straight-forward method which utilizes genetic programming for symbolic\nregression, yielding policies imitating an existing well-performing, but\nnon-interpretable policy. Experiments on three reinforcement learning\nbenchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark,\ndemonstrate the superiority of our GPRL approach compared to the symbolic\nregression method. GPRL is capable of producing well-performing interpretable\nreinforcement learning policies from pre-existing default trajectory data.",
  "text": "Interpretable Policies for Reinforcement Learning by Genetic Programming\nDaniel Heina,b,∗, Steﬀen Udluftb, Thomas A. Runklera,b\naTechnical University of Munich, Department of Informatics, Boltzmannstr. 3, 85748 Garching, Germany\nbSiemens AG, Corporate Technology, Otto-Hahn-Ring 6, 81739 Munich, Germany\nAbstract\nThe search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for\nindustrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable\nand convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are\nrestricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL)\napproach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy\nequations from pre-existing default state-action trajectory samples. GPRL is compared to a straight-forward method\nwhich utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but\nnon-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart-pole balancing,\nand industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression\nmethod. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing\ndefault trajectory data.\nKeywords:\ninterpretable, reinforcement learning, genetic programming, model-based, symbolic regression, industrial\nbenchmark\n1. Introduction\nThis work introduces a genetic programming (GP) ap-\nproach for autonomously learning interpretable reinforce-\nment learning (RL) policies from previously recorded state\ntransitions. Despite the search of interpretable RL policies\nbeing of high academic and industrial interest, little has\nbeen published concerning human interpretable and under-\nstandable policies trained by data driven learning methods\n(Maes, Fonteneau, Wehenkel, and Ernst, 2012). Recent\nresearch results show that using fuzzy rules in batch RL\nsettings can be considered an adequate solution to this\ntask (Hein, Hentschel, Runkler, and Udluft, 2017b). How-\never, in many cases the successful use of fuzzy rules requires\nprior knowledge about the shape of the membership func-\ntions, the number of fuzzy rules, the relevant state features,\netc. Moreover, for some problems the policy representa-\ntion as a set of fuzzy rules might be generally unfavor-\nable by some domain experts. Our genetic programming\nfor reinforcement learning (GPRL) approach learns policy\nrepresentations which are represented by basic algebraic\nequations of low complexity.\nThe GPRL approach is motivated by typical industrial\napplication scenarios like wind or gas turbines. For in-\ndustrial systems, low-level control is realized by dedicated\nexpert-designed controllers, which guarantee safety and\nstability. However, we observed that high-level control is\n∗Corresponding author\nEmail address: daniel.hein@in.tum.de (Daniel Hein)\nusually implemented by default control strategies, provided\nby best practice approaches or domain experts who are\nmaintaining the system, based on personal experience and\nknowledge about the system’s dynamics. One reason for\nthe lack of autonomously generated real-world controllers\nis that modeling system dependencies for high-level con-\ntrol by a ﬁrst principle model is a complicated and often\ninfeasible approach. Since in many real-world applications\nsuch representations cannot be found, training high-level\ncontrollers has to be performed on data samples from the\nsystem. RL is capable of yielding high-level controllers\nbased solely on available system data.\nRL is concerned with learning a policy for a system\nthat can be modeled as a Markov decision process (Sutton\nand Barto, 1998). This policy maps from system states to\nactions in the system. Repeatedly applying an RL policy\ngenerates a trajectory in the state-action space (Section 3).\nBased on our experience, learning such RL controllers in a\nway that produces interpretable high-level controllers is of\nhigh interest, especially for real-world industry problems,\nsince interpretable solutions are expected to yield higher\nacceptance from domain experts than black-box solutions.\nIn batch RL, we consider applications where online\nlearning approaches, such as classical temporal-diﬀerence\nlearning (Sutton, 1988), are prohibited for safety reasons,\nsince these approaches require exploration of system dy-\nnamics. In contrast, batch RL algorithms generate a policy\nbased on existing data and deploy this policy to the system\nafter training. In this setting, either the value function or\nPreprint submitted to Engineering Applications of Artiﬁcial Intelligence\nApril 5, 2018\narXiv:1712.04170v2  [cs.AI]  4 Apr 2018\nthe system dynamics are trained using historic operational\ndata comprising a set of four-tuples of the form (observation,\naction, reward, next observation), which is referred to as a\ndata batch. Research from the past two decades (Gordon,\n1995; Ormoneit and Sen, 2002; Lagoudakis and Parr, 2003;\nErnst, Geurts, Wehenkel, and Littman, 2005) suggests\nthat such batch RL algorithms satisfy real-world system\nrequirements, particularly when involving neural networks\n(NNs) modeling either the state-action value function (Ried-\nmiller, 2005a,b; Schneegaß, Udluft, and Martinetz, 2007a,b;\nRiedmiller, Gabel, Hafner, and Lange, 2009) or system dy-\nnamics (Bakker, 2004; Sch¨afer, 2008; Depeweg, Hern´andez-\nLobato, Doshi-Velez, and Udluft, 2016). Moreover, batch\nRL algorithms are data-eﬃcient (Riedmiller, 2005a; Sch¨afer,\nUdluft, and Zimmermann, 2007) because batch data is uti-\nlized repeatedly during the training phase.\nTo the best of our knowledge, GP-generated policies\nhave never been combined with a model-based batch RL\napproach (Section 2). In the proposed GPRL approach, the\nperformance of a population of basic algebraic equations\nis evaluated by testing the individuals on a world model\nusing the Monte Carlo method (Sutton and Barto, 1998).\nThe combined return value of a number of action sequences\nis the ﬁtness value that is maximized iteratively from GP\ngeneration to generation.\nGPRL is a novel model-based RL approach, i.e., training\nis conducted on an environment approximation referred to\nas world model. Generating a world model from real system\ndata in advance and training a GP policy using this model\nhas several advantages. (i) In many real-world scenarios,\ndata describing system dynamics is available in advance\nor is easily collected. (ii) Policies are not evaluated on\nthe real system, thereby avoiding the detrimental eﬀects of\nexecuting a bad policy. (iii) Expert-driven reward function\nengineering, yielding a closed-form diﬀerentiable equation,\nutilized during policy training is not required, i.e., it is\nsuﬃcient to sample from the system’s reward function and\nmodel the underlying dependencies by using supervised\nmachine learning.\nThe remainder of this paper is organized as follows.\nThe RL and GP methods employed in our framework are\nreviewed in Sections 3 and 4. Speciﬁcally, the problem of\nﬁnding policies via RL is formalized as an optimization task.\nIn addition, GP in general and the speciﬁc implementation\nthat we used for experiments are motivated and presented.\nAn overview of how the proposed GPRL approach is derived\nfrom diﬀerent methods is given in Section 5. Experiments\nusing three benchmark problems, i.e., the mountain car\n(MC) problem, the cart-pole balancing (CPB) task, and\nthe industrial benchmark (IB), are described in Section 6.\nExperimental results are discussed in Section 7. The re-\nsults demonstrate that the proposed GPRL approach can\nsolve the benchmark problems and is able to produce inter-\npretable RL policies. To benchmark GPRL, we compare\nthe obtained results to an alternative approach in which GP\nis used to mimic an existing non-interpretable NN policy\nby symbolic regression.\n2. Related Work\nGP has been utilized for creating rule-based policies\nsince its introduction by Koza (1992). Since then, the ﬁeld\nof GP has grown signiﬁcantly and has produced numerous\nresults that can compete with human-produced results,\nincluding controllers, game playing, and robotics (Koza,\n2010). Keane, Koza, and Streeter (2002) automatically\nsynthesized a controller by using GP, outperforming con-\nventional PID controllers for an industrially representative\nset of plants. Another approach using genetic algorithms for\nRL policy design is to learn a set of fuzzy “if-then” rules, by\nmodifying membership functions, rule sets and consequent\ntypes (Juang, Lin, and Lin, 2000). Recently, Koshiyama,\nEscovedo, Vellasco, and Tanscheit (2014) introduced GP-\nFIS, a genetic fuzzy controller based on multi-gene GP, and\ndemonstrated the superiority in relation to other genetic\nfuzzy controllers on the cart-centering and the inverted\npendulum problems. On the same benchmark, a movable\ninverted pendulum, Shimooka and Fujimoto (1999) applied\nGP to generate equations for calculating the control force\nby evaluating the individuals’ performances on predeﬁned\nﬁtness functions.\nA fundamental drawback with all of the former methods\nis that in many real-world scenarios such dedicated expert\ngenerated ﬁtness functions do not exist. In RL the goal is to\nderive well-performing policies only by (i) interacting with\nthe environment, or by (ii) extracting knowledge out of\npre-generated data, running the system with an arbitrary\npolicy (Sutton and Barto, 1998).\n(i) is referred to as\nthe online RL problem, for which Q-learning methods are\nknown to produce excellent results. For (ii), the oﬀline RL\nproblem, model-based algorithms are usually more stable\nand yield better performing policies (Hein et al., 2017b).\nGP in conjunction with online RL Q-learning has been\nused in (Downing, 2001) on standard maze search problems\nand in (Kamio and Iba, 2005) to enable a real robot to\nadapt its action to a real environment. Katagiri, Hira-\nsawa, Hu, Murata, and Kosaka (2002) introduced genetic\nnetwork programming (GNP), which has been applied to\nonline RL in (Mabu, Hirasawa, Hu, and Murata, 2002)\nand improved by Q-tables in (Mabu, Hirasawa, and Hu,\n2004). In these publications, the eﬃciency of GNP for gen-\nerating RL policies has been discussed. This performance\ngain, in comparison to standard GP, comes at the cost of\ninterpretability, since complex network graphs have to be\ntraversed to compute the policy outputs.\nGearhart (2003) examined GP as a policy search tech-\nnique for Markov Decision Processes. Given a simulation\nof the Freecraft tactical problem, he performed Monte\nCarlo simulations to evaluate the ﬁtness of each individual.\nNote that such exact simulations are usually not avail-\nable in industry. Similarly, in (Maes et al., 2012) Monte\nCarlo simulations have been drawn in order to identify the\nbest policies. However, the policy search itself has been\nperformed by formalizing a search over a space of simple\nclosed-form formulas as a multi-armed bandit problem.\n2\nThis means that all policy candidates have to be created\nin an initial step at once and are subsequently evaluated.\nThe computational eﬀort to follow this approach combi-\nnatorially explodes as soon as more complex solutions are\nrequired to solve more complicated control problems.\n3. Model-based Reinforcement Learning\nInspired by behaviorist psychology, RL is concerned\nwith how software agents ought to take actions in an envi-\nronment in order to maximize their received accumulated\nrewards. In RL, the acting agent is not explicitly told\nwhich actions to implement. Instead, the agent must learn\nthe best action strategy from the observed environment’s\nrewards in response to the agent’s actions. Generally, such\nactions aﬀect both the next reward and subsequent re-\nwards (Sutton and Barto, 1998).\nIn\nRL\nformalism,\nat\neach\ndiscrete\ntime\nstep\nt = 0, 1, 2, . . ., the agent observes the system’s state\nst ∈S and applies an action at ∈A, where S is the state\nspace and A is the action space. Depending on st and at,\nthe system transitions to the next state st+1 and the agent\nreceives a real-value reward rt+1 ∈R. In deterministic\nsystems the state transition can be expressed as a function\ng : S × A →S with g(st, at) = st+1. The related reward\nis given by a reward function r : S × A × S →R with\nr(st, at, st+1) = rt+1. Hence, the desired solution to an\nRL problem is a policy that maximizes the expected\naccumulated rewards.\nIn our proposed setup, the goal is to ﬁnd the best\npolicy π among Π the set of all possible equations which\ncan be built from a pre-deﬁned set of function building\nblocks, with respect to a certain maximum complexity. For\nevery state st, the policy outputs an action, i.e., π(st) =\nat. The policy’s performance, when starting from st, is\nmeasured by the return R(st, π), i.e., the accumulated\nfuture rewards obtained by executing the policy π. To\naccount for increasing uncertainties when accumulating\nfuture rewards, the reward rt+k for k future time steps is\nweighted by γk, where γ ∈[0, 1]. Furthermore, adopting\na common approach, we include only a ﬁnite number of\nT > 1 future rewards in the return (Sutton and Barto,\n1998), which is expressed as follows:\nR(st, π) =\nT −1\nX\nk=0\nγkr(st+k, π(st+k), st+k+1),\nwith\nst+k+1 = g(st+k, at+k).\n(1)\nHerein, we select the discount factor γ such that, at the\nend of time horizon T, the last reward accounted for is\nweighted by q ∈[0, 1], yielding γ = q1/(T −1). The overall\nstate-independent policy performance F(π) is obtained\nby averaging over all starting states st ∈S ⊂S, using\ntheir respective probabilities wst as weight factors. Thus,\noptimal solutions to the RL problem are policies π with\nˆπ ∈arg max\nπ∈Π\nF(π),\nwith\nF(π) = 1\n|S|\nX\nst∈S\nwstR(st, π).\n(2)\nIn optimization terminology, the policy performance func-\ntion F(π) is referred to as a ﬁtness function.\nFor most real-world industrial control problems, the cost\nof executing a potentially bad policy is prohibitive. There-\nfore, in model-based RL (Busoniu, Babuska, De Schutter,\nand Ernst, 2010), the state transition function g is approx-\nimated using a model ˜g, which can be a ﬁrst principle\nmodel or can be created from previously gathered data. By\nsubstituting ˜g in place of the real system g in (1), we ob-\ntain a model-based approximation ˜F(π) of the true ﬁtness\nfunction (2). In this study, we employ models based on\nNNs. However, the proposed method can be extended to\nother models, such as Bayesian NNs (Depeweg et al., 2016)\nand Gaussian process models (Rasmussen and Williams,\n2006).\n4. Genetic Programming\nGP is a technique, which encodes computer programs\nas a set of genes. Applying a so-called genetic algorithm\n(GA) on these genes to modify (evolve) them drives the\noptimization of the population. Generally, the space of\nsolutions consists of computer programs, which perform\nwell on predeﬁned tasks (Koza, 1992). Since we are inter-\nested in interpretable equations as RL policies, the genes\nin our setting include basic algebraic functions, as well as\nconstant ﬂoat numbers and state variables. Such basic\nalgebraic functions can be depicted as function trees and\nstored eﬃciently in memory arrays.\nThe GA drives the optimization by applying selection\nand reproduction on the populations. The basis for both\nconcepts is a ﬁtness value F which represents the qual-\nity of performing the predeﬁned task for each individual.\nSelection means that only the best portion of the cur-\nrent generation will survive each iteration and continue\nexisting in the next generation. Analogous to biological\nsexual breeding, two individuals are selected for reproduc-\ntion based on their ﬁtness, and two oﬀspring individuals\nare created by crossing their chromosomes. Technically,\nthis is realized by selecting compatible cutting points in\nthe function trees and interchanging the subtrees beneath\nthese cuts. Subsequently, the two resulting individuals are\nintroduced to the population of the next generation (Fig-\nure 1). Herein, we applied tournament selection (Blickle\nand Thiele, 1995) to select the individuals to be crossed.\nIn our experiments, it has shown to be advantageous\nto apply automatic equation cancellation to a certain\namount (deﬁned by auto cancellation ratio ra) of the best-\nperforming individuals of one generation. For canceling,\nan algorithm is applied, which searches the chromosomes\nfor easy-to-cancel subtree structures, calculates the result\nof the subtree and replaces it by this result. For example,\n3\nGeneration 1\nGeneration 2\nFigure 1: GP individuals as function trees. Depicted are four ex-\nemplary GP individuals πi from two consecutive generations with\ntheir respective complexity measures Ci. Crossover cutting points are\nmarked in the tree diagrams of policies π1 and π2.\nif a subtree with a root node + is found, whose children\nare two ﬂoat terminal nodes a and b, the subtree can be\nreplaced by one ﬂoat terminal node c, given by c = a + b.\nSimilar cancelation rules can be derived for all functions\nin the function set. Since the tree depth is limited in our\nGP setup, this algorithm can reduce the complexity of\nsubstructures and even the whole individual, as well as\ngenerate space for potentially more important subtrees.\nAs a mutation operator, we adopted the so-called Gaus-\nsian mutator for ﬂoat terminals, which is common in evo-\nlutionary algorithms (Schwefel, 1981, 1995). In each gener-\nation, a certain portion (according to terminal mutation\nratio rm) of the best-performing individuals for each com-\nplexity is selected.\nSubsequently, these individuals are\ncopied and their original ﬂoat terminals z are mutated\nby drawing replacement terminals z′ from a normal distri-\nbution N(z, 0.1|z|). If the performance of the best copy\nis superior to that of the original individual, it is added\nto the new population. This strategy provides an option\nfor conducting a local search in the policy search space,\nbecause the basic structure of the individual’s genotype\nremains untouched.\nInitially, the population is generated randomly, as well\nas a certain portion of each population every new generation\n(according to a new random individual ratio rn). A common\nstrategy to randomly generate valid individuals is to apply\nthe so-called grow method. In our implementation, growing\na chromosome is realized as follows:\n1. Randomly draw tree depth d from [dmin, dmax]\n2. select next gene(d)\n3. Procedure: select next gene(d)\n(a) If d < 1\nRandomly draw gene g from the set of terminals\nand variables\nElse\nRandomly draw gene g from the set of functions\n(b) Add g to chromosome c\n(c) Randomly select one leaf of g: i\n(d) Build chromosome ci ←select next gene(d −1)\n(e) Add ci to c\n(f) For all leafs of node j ̸= i\ni. Randomly draw subtree depth dj from\n[0, d −1]\nii. Build chromosome cj ←select next gene(dj)\niii. Add cj to c\n(g) Return c\nNote that this algorithm enforces a broad variety of indi-\nviduals, since it randomizes the length of each subtree (1.\nand 3.(f).i.), as well as the position of the biggest subtree\n(3.(c)). Both properties save the GA from generating only\nsmall initial chromosomes, which eventually would result\nin an early convergence of the population.\nThe overall GA used in the experiments is given as\nfollows:\n1. Randomly initialize the population of size N\n2. Determine ﬁtness value of each individual (in parallel)\n3. Evolve next generation\n(a) Crossover (depending on crossover ratio rc)\ni. Select individuals by tournament selection\nii. Cross two tournament winners\niii. Add resulting individuals to new population\n(b) Reproduction (depending on reproduction ratio\nrr)\ni. Select individuals by tournament selection\nii. Add tournament winner to new population\n(c) Automatic cancelation and terminal adjustment\n(depending on auto cancel ratio ra and terminal\nadjustment ration rm)\ni. Apply automatic cancelation on all individ-\nuals\nii. Add canceled individuals according to ra\niii. Select best individuals of old population\niv. Randomly mutate ﬂoat terminals (z′ ∼z +\n0.1z · N(0, 1)) and create N · ra adjusted\nindividuals from each best\nv. Determine ﬁtness value of each individual\n(in parallel)\nvi. Add best adjusted individuals to new pop-\nulation according to rm\n(d) Fill new population with new randomly gener-\nated individuals (new individuals ratio rn)\n(e) Determine ﬁtness value of each individual (in\nparallel)\n(f) If none of the stopping criteria is met\ni. Go back to 3.\n4\nVariables\n1\nTerminals\n1\n+, −, ·\n1\n/\n2\n∧, ∨\n4\ntanh, abs\n4\nif\n5\nTable 1: GP complexities\n4. Return best individual found so far for each com-\nplexity level\nSince in this work we are searching for interpretable\nsolutions, a measure of complexity has to be established.\nMeasuring the complexity of an individual can generally\nbe stated with respect to its genotype (structural) or with\nrespect to its phenotype (functional) (Le, Xuan, Brabazon,\nand Thi, 2016). Here, we decided to use a simple node-\ncounting measuring strategy where diﬀerent types of func-\ntions, variables and terminals are counted with diﬀerent\nweightings. Hence, the domain experts, to whom the RL\npolicies might be delivered to, can pre-deﬁne which types\nof genes they are more likely to accept as interpretable\ncompared to others. In particular, we adopted the com-\nplexity weightings from Eureqa1, a commercial available\nsoftware for symbolic regression (Dubˇc´akov´a, 2011). Ta-\nble 1 lists the weightings we applied in our experiments\nand Figure 1 gives four examples on how to calculate the\nrespective complexities.\nTable 2 gives an overview of the GP parameters and\nmethods we used in the experiments below. Note that\nwe decided to employ rather big population sizes (up to\n1000) in combination with a higher new individuals ratio\n(rn = 0.3), compared to other publications (Koshiyama\net al., 2014). By doing so, we empirically observed that\nGPRL converges to better solutions faster, compared to a\nsetting with smaller population and more generations. The\nlatter setting very often converged to suboptimal solutions\ntoo early. One reason for that might be the lack of diversity\nobservable in smaller populations with a limited number\nof new individuals in each generation. Furthermore, with\nthe parallel computing options of today’s computational\nresources, big populations can be evaluated in parallel,\nwhile many consecutive generations have to be evaluated\nsequentially.\n5. Genetic Programming Reinforcement Learning\nThe basis for the proposed GPRL approach is a data set\nD that contains state transition samples gathered from the\ndynamics of a real system. These samples are represented\nby tuples (s, a, s′, r), where, in state s, action a was applied\n1https://www.nutonian.com/products/eureqa\nIndividual representation\ntree-based\nInitialization method\ngrow method\nSelection method\ntournament selection (size=3)\nTerminal set\nstate variables, random ﬂoat\nnumbers z ∼[−20.0, 20.0],\n⊤, ⊥\nFunction set\n+, −, ∗, /, ∧, ∨, if, >, <,\ntanh, abs\nMaximal gene amount\n100\nMaximal tree depth\n5\nMaximal complexity\n100\nRatios for new generation\ncrossover rc = 0.45\nreproduction rr = 0.05\nauto cancel ra = 0.1\nterminal mutation rm = 0.1\nnew random individuals\nrn = 0.3\nPopulation/generations/\nMC: 100/1,000/1,000\ntraining states\nCPB: 1,000/1,000/1,000\nfor GPRL\nIB: 1,000/1,000/100\nPopulation/generations/\nMC: 1,000/1,000/70,000\nsamples\nCPB: 10,000/1,000/70,000\nfor symbolic regression\nIB: 10,000/1,000/100,000\nTable 2: GP parameters. Note, that the population and generation\nnumbers have been determined empirically. The generation num-\nbers are chosen such that after these points no substantially better\nindividuals have been found throughout the population.\n5\nand resulted in state transition to s′. Subsequently, this\ntransition yielded a real value reward r. Note that generally\nD can be generated using any (even a random) policy\nprior to policy training as long as suﬃcient exploration is\ninvolved (Sutton and Barto, 1998).\nIn a ﬁrst step, we generate world models ˜g with inputs\n(s, a) to predict s′, using data set D.\nTo yield better\napproximative quality, we observed that for many problems\nit is advantageous to learn the diﬀerences between s and s′\nand to train a single model per state variable separately:\n∆s′\n1 = ˜gs1(s1, s2, . . . , sm, a),\n∆s′\n2 = ˜gs2(s1, s2, . . . , sm, a),\n. . .\n∆s′\nm = ˜gsm(s1, s2, . . . , sm, a).\nHence, the resulting state is calculated according to s′ =\n(s1 + ∆s′\n1, s2 + ∆s′\n2, . . . , sm + ∆s′\nm). Note that the reward\nis also given in data set D; thus, the reward function can\nalso be approximated using r = ˜r(s, a, s′).\nThe interpretable policies we are generating applying\nour GPRL approach in Section 6 are basic algebraic equa-\ntions. Given that GPRL is able to ﬁnd rather short (non-\ncomplex) equations, we expect to reveal substantial knowl-\nedge about underlying coherencies between available state\nvariables and well-performing control policies with respect\nto a certain RL problem. To rate the quality of each policy\ncandidate a ﬁtness value has to be provided for the GP\nalgorithm to advance. For our GPRL approach, the ﬁtness\n˜F of each individual is calculated by generating trajectories\nusing the world model ˜g starting from a ﬁxed set of initial\nbenchmark states (Section 3).\nThe performance of GPRL is compared to a rather\nstraightforward approach, which utilizes GP to conduct\nsymbolic regression on a data set ˆD generated by a well-\nperforming but non-interpretable RL policy ˆπ. ˆD contains\ntuples (s, ˆa), where ˆa are the generated actions of policy ˆπ\non state s. The states originate from trajectories created\nby policy ˆπ on world model ˜g. One might think that given\nan adequate policy of any form and using GP to mimic\nthis policy by means of some regression error with respect\nto ˆa, could also yield successful interpretable RL policies.\nHowever, our results clearly indicate that this strategy is\nonly successful for rather small and simple problems and\nproduces highly non-stable and unsatisfactory results for\nmore complex tasks.\nIn our experiments, the well-performing but non-\ninterpretable RL policy ˆπ we used to generate data set ˆD\nis a NN policy. To yield comparable results we always\ntrained the weights of this policy by model-based RL on\nthe very same world models as applied for GPRL. Note,\nthat usually we expect ˆπ to yield higher ﬁtness values\nduring training, since it is able to utilize signiﬁcantly more\ndegrees of freedom to compute an optimal state action\nmapping than basic algebraic equations found by GPRL.\nNote that we use NNs as world models ˜g for the GPRL\nexperiments (not to be confused with NN policy ˆπ). In\nmany real-world industrial problem domains, i.e., continu-\nous and rather smooth system dynamics, NNs are known\nto serve as adequate world models with excellent general-\nization properties. Given a batch of previously generated\ntransition samples, the NN training process is known to\nbe data-eﬃcient.\nMoreover, the training errors are ex-\ncellent indicators of how well the model will perform in\nmodel-based RL training. Nevertheless, for other problem\ndomains, alternative types of world models might be prefer-\nable. For example, Gaussian processes (Rasmussen and\nWilliams, 2006) provide a good approximation of the mean\nof the target value, and this technique indicates the level of\nconﬁdence about this prediction, which may be of value for\nstochastic system dynamics. Another alternative modeling\ntechnique is the use of regression trees (Breiman, Friedman,\nOlshen, and Stone, 1984). While typically lacking data\neﬃciency, regression tree predictions are less aﬀected by\nnonlinearities perceived by system dynamics because they\ndo not rely on a closed-form functional approximation.\nFigure 2 gives an overview of the relationships between\nthe diﬀerent policy implementations, i.e., GPRL, GP for\nsymbolic regression, and NN policy, and the environment\ninstances, i.e., NN system model and the real dynamics,\nused for training and evaluation, respectively.\n6. Experiments\n6.1. Mountain Car\nIn the MC benchmark, an underpowered car must be\ndriven to the top of a hill (Figure 3) (Moore, 1990). This\nis achieved by building suﬃcient potential energy by ﬁrst\ndriving in the direction opposite to the ﬁnal direction. The\nsystem is fully described by the two-dimensional state space\ns = (ρ, ˙ρ) representing the cars position ρ and velocity ˙ρ.\nWe conducted MC experiments using the freely available\nCLS2 software (’clsquare’)2, which is an RL benchmark\nsystem that applies the Runge-Kutta fourth-order method\nto approximate closed loop dynamics. The task for the\nRL agent is to ﬁnd a policy producing action sequence\nat, at+1, at+2, . . . ∈[−1, 1] that drive the car up the hill,\nwhich is achieved when reaching position ρ ≥0.6.\nThe agent receives a reward of\nr(s′) =\n®\n0,\nif ρ′ ≥0.6,\n−1,\notherwise,\n(3)\nsubsequent to each state transition s′ = g(s, a). When\nthe car reaches the goal position, i.e., ρ ≥0.6, its position\nbecomes ﬁxed and the agent receives the maximum reward\nin each following time step regardless of the applied actions.\n6.2. Cart-pole Balancing\nThe CPB experiments described in the following sec-\ntion were also conducted using the CLS2 software. The\n2http://ml.informatik.uni-freiburg.de/research/clsquare\n6\nGP Model-based Policy\nGP Regression Policy\nReal Dynamics\nNN Model-based Policy\nNN Model\nTraining\nEvaluation\nA\nB.1\nB.2\nC\nD.1\nD.2\nD.3\nFigure 2: The proposed GPRL approach and its integration in the experimental setup. The world model is the result of supervised ML\nregression on data originated from the real dynamics (A). GPRL generates a GP model-based policy by training on this world model (B.1),\nwhich is an NN model in our experimental setup. Similarly, the NN policy is trained in a model-based manner by utilizing the same NN model\n(B.2). In contrast to both other policy training approaches, the GP regression policy mimics an already existing policy by learning to minimize\nan error with respect to the existing policy’s action (C). All of the policies are ﬁnally evaluated by comparing their performance on the real\ndynamics (D.1-D.3).\nFigure 3: Mountain car benchmark. The task is to ﬁrst build up\nmomentum by driving to the left in order to subsequently reach the\ntop of the hill on the right at ρ = 0.6.\nobjective of the CPB benchmark is to apply forces to a cart\nmoving on a one-dimensional track to keep a pole hinged\nto the cart in an upright position (Figure 4). Here, the\nfour Markov state variables are the pole angle θ, the pole\nangular velocity ˙θ, the cart position ρ, and the cart velocity\n˙ρ. These variables describe the Markov state completely,\ni.e., no additional information about the system’s past be-\nhavior is required. The task for the RL agent is to ﬁnd a\nsequence of force actions at, at+1, at+2, . . . that prevent the\npole from falling over (Fantoni and Lozano, 2002).\nIn the CPB task, the angle of the pole and the cart’s po-\nsition are restricted to intervals of [−0.7, 0.7] and [−2.4, 2.4]\nrespectively. Once the cart has left the restricted area, the\nepisode is considered a failure and the system remains in\nthe failure state for the rest of the episode. The RL policy\ncan apply force actions on the cart from −10 N to +10 N\nin time intervals of 0.025 s.\nThe reward function for the balancing problem is given\n!\n\"\n#\n$\n%\n&\n'\nFigure 4: Cart-pole benchmark. The task is to balance the pole\naround θ = 0 while moving the cart to position ρ = 0 by applying\npositive or negative force to the cart.\nas follows:\nr(s′) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0,\nif |θ′| < 0.25\nand |ρ′| < 0.5,\n−1.0,\nif |θ′| > 0.7\nor |ρ′| > 2.4,\n−0.1,\notherwise.\n(4)\nBased on this reward function, the primary goal of the\npolicy is to avoid reaching the failure state. The secondary\ngoal is to drive the system to the goal state region where\nr = 0 and keep it there for the rest of the episode.\n6.3. Industrial Benchmark\nThe IB3 was designed to emulate several challenging\naspects eminent in many industrial applications (Hein, De-\npeweg, Tokic, Udluft, Hentschel, Runkler, and Sterzing,\n2017a; Hein, Udluft, Tokic, Hentschel, Runkler, and Sterz-\ning, 2017c; Hein, Hentschel, Runkler, and Udluft, 2018).\nIt is not designed to be an approximation of any speciﬁc\n3http://github.com/siemens/industrialbenchmark\n7\nreal-world system, but to pose a comparable hardness and\ncomplexity found in many industrial applications.\nState and action spaces are continuous. Moreover, the\nstate space is high-dimensional and only partially observ-\nable. The actions consist of three continuous components\nand aﬀect three control inputs. Moreover, the IB includes\nstochastic and delayed eﬀects. The optimization task is\nmulti-criterial in the sense that there are two reward compo-\nnents that show opposite dependencies on the actions. The\ndynamical behavior is heteroscedastic with state-dependent\nobservation noise and state-dependent probability distribu-\ntions, based on latent variables. Furthermore, it depends\non an external driver that cannot be inﬂuenced by the\nactions.\nAt any time step t the RL agent can inﬂuence the IB\nvia actions at that are three dimensional vectors in [−1, 1]3.\nEach action can be interpreted as three proposed changes\nto three observable state control variables. Those variables\nare: velocity v, gain g, and shift h. Each variable is limited\nto [0, 100] and calculated as follows:\nat = (∆vt, ∆gt, ∆ht) ,\nvt+1 = max (0, min (100, vt + dv∆vt)) ,\ngt+1 = max (0, min (100, gt + dg∆gt)) ,\nht+1 = max \u00000, min \u0000100, ht + dh∆ht\n\u0001\u0001 ,\nwith scaling factors dv = 1, dg = 10, and dh = 5.75.\nAfter applying the action at, the environment transi-\ntions to the next time step t + 1, yielding the internal state\nst+1. State st and successor state st+1 are the Markovian\nstates of the environment, which are only partially observ-\nable by the agent. In addition to the three control variables\nvelocity v, gain g, and shift h, a setpoint p is applied to\nthe system. Setpoint p simulates an external force like\nthe demanded load in a power plant or the wind speed\nactuating a wind turbine, which cannot be controlled by\nthe agent, but still has a major inﬂuence on the system\ndynamics. Depending on the setpoint pt and the choice\nof control values at, the system suﬀers from detrimental\nfatigue ft and consumes resources such as power, fuel, etc.,\nrepresented by consumption ct. In each time step, the IB\ngenerates output values for ct+1 and ft+1, which are part\nof the internal state st+1. The reward is solely determined\nby st+1 as follows:\nrt+1 = −ct+1 −3ft+1.\n(5)\nNote that the complete Markov state s of the IB remains\nunobservable. Only an observation vector o ⊂s consisting\nof:\n• the current control variables velocity vt, gain gt, and\nshift ht,\n• the external driver set point pt, and\n• the reward relevant variables consumption ct and\nfatigue ft,\ncan be observed externally.\nIn\nSection\n3\nthe\noptimization\ntask\nin\nmodel-\nbased RL is described as working on the Markovian\nstate s of the system dynamics.\nSince this state is\nnot observable in the IB environment st is approxi-\nmated by a suﬃcient amount of historic observations\n( ot−H, ot−H+1, . . . , ot) with time horizon H.\nGiven a\nsystem model g ( ot−H, ot−H+1, . . . , ot, at) = (ot+1, rt+1)\nwith H = 30 an adequate prediction performance could be\nachieved during IB experiments. Note that observation\nsize |o| = 6 in combination with time horizon H = 30\nresults in a 180-dimensional approximation vector of the\nMarkovian state.\n6.4. Neural Network World Models\nThe model-based policy training has been performed\non NN world models, which yielded approximative ﬁtness\nfunctions ˜F(x) (Section 3). For these experiments, we\ncreated one NN for each state variable. Prior to training,\nthe respective data sets were split into blocks of 80%,\n10%, and 10% (training, validation and generalization sets,\nrespectively). While the weight updates during training\nwere computed by utilizing the training sets, the weights\nthat performed best given the validation sets were used as\ntraining results. Finally, those weights were evaluated using\nthe generalization sets to rate the overall approximation\nquality on unseen data.\nThe MC NNs were trained with data set DMC contain-\ning tuples (s, a, g(s, a), r) from trajectories generated by\napplying random actions on the benchmark dynamics. The\nstart states for these trajectories were uniformly sampled\nas s = (ρ, ˙ρ) ∈[−1.2, 0.6] × {0}, i.e., at a random position\non the track with zero velocity. DMC contains 10,000 tran-\nsition samples. The following three NNs were trained to\napproximate the MC task:\n∆ρt+1 = ˜gρ(ρt, ˙ρt, at),\n∆˙ρt+1 = ˜g ˙ρ(ρt, ˙ρt, at),\nrt+1 = ˜r(st, at, st+1),\nwith st+1 = (ρt + ∆ρt+1, ˙ρt + ∆˙ρt+1).\nSimilarly,\nfor\nthe\nCPB\ndynamic\nmodel\nstate\nst = (θt, ˙θt, ρt, ˙ρt) we created the following four net-\nworks:\n∆θt+1 = ˜gθ(θt, ˙θt, ρt, ˙ρt, at)\n∆˙θt+1 = ˜g ˙θ(θt, ˙θt, ρt, ˙ρt, at)\n∆ρt+1 = ˜gρ(θt, ˙θt, ρt, ˙ρt, at)\n∆˙ρt+1 = ˜g ˙ρ(θt, ˙θt, ρt, ˙ρt, at).\nAn approximation of the next state is given by the following\nformula:\nst+1 = (θt+∆θt+1, ˙θt+∆˙θt+1, ρt+∆ρt+1, ˙ρt+∆˙ρt+1). (6)\n8\nThe result of this formula can subsequently be used to\napproximate the state transition’s reward by\nrt+1 = ˜r(st, at, st+1).\n(7)\nFor the training set DCPB of the CPB benchmark, the\nsamples originate from trajectories of 100 state transitions\ngenerated by a random walk on the benchmark dynamics.\nThe start states (θ, ˙θ, ρ, ˙ρ) for these trajectories were sam-\npled uniformly from [−0.7, 0.7] × {0} × [−2.4, 2.4] × {0}.\nDCPB contains 10,000 transition samples.\nThe experiments for MC and CPB were conducted\nwith a network complexity of three hidden layers with 10\nhidden neurons each and rectiﬁer activation functions. For\ntraining, we used the Vario-Eta algorithm (Neuneier and\nZimmermann, 2012).\nFor the IB benchmark two recurrent neural networks\n(RNNs) have been trained:\nct+1 = ˜gc(ot \\ {ct}, at),\nft+1 = ˜gf(ot, at),\nrt+1 = −ct+1 −3ft+1.\nNote that the Markov decision process extraction topol-\nogy (Duell, Udluft, and Sterzing, 2012) of the RNNs that\nwe applied here is well-suited for partially observable prob-\nlems like the IB. Detailed information on this topology,\nother design decisions, the training data, and the train-\ning process have been previously published by Hein et al.\n(2017c).\n7. Results\n7.1. Mountain Car\nWe conducted the MC experiments using a time horizon\nT of 200 and a discount vector γ of 0.985 (q = 0.05). For\nthe MC experiments, a non-interpretable NN policy with\nﬁtness value F = −41.0 (equivalent to a penalty value\nof 41.0) has been trained prior to the GP experiments.\nA policy with this ﬁtness value is capable of driving the\ncar to the top of the hill from every state in the test set.\nThe NN policy has two hidden layers with tanh activation\nfunction and 10 hidden neurons on each layer. Note that\nrecreating such a policy with function trees as considered\nfor our GPRL approach would result in a complexity value\nof 1581.\nThe ten GPRL runs learned interpretable policies with\na median model penalty of 41.8 for complexities ≥5 (Fig-\nure 5a). However, even the policies of complexity 1 man-\naged to drive the car to the top of the hill, by simply\napplying the force along the direction of the car’s veloc-\nity, i.e., π(ρ, ˙ρ) = ˙ρ. Though, policies with lower penalty\nmanaged to reach the top of the hill in fewer time steps.\nThe resulting GPRL Pareto front individuals from com-\nplexity 1 to 15 are shown in Figure 6.\nPerforming ten symbolic regression runs on the non-\ninterpretable NN policy yielded interpretable policies with\n0\n20\n40\n60\n80\n100\ncomplexity\n41\n41.5\n42\n42.5\n43\n43.5\nmodel penalty\nModel-based training\nGP Model-based Policy\nGP Regression Policy\nReal Dynamics\nNN Model-based Policy\nNN Model\n(a) MC\n0\n20\n40\n60\n80\n100\ncomplexity\n25\n30\n35\n40\n45\n50\nmodel penalty\n(b) CPB\n0\n20\n40\n60\n80\n100\ncomplexity\n1.6\n1.7\n1.8\n1.9\n2\n2.1\n2.2\n2.3\nmodel penalty\n104\n(c) IB\nFigure 5: Pareto fronts from ten model-based GPRL trainings. De-\npicted is the median (green line) together with the minimum and\nmaximum (semi-transparent green area) Pareto front penalty from\nall experiments. The dashed line depicts the performance baseline of\nthe NN policy on the NN model.\n9\n1\n3\n5\n7\n9\n11\n13\n15\n8\n12\nFigure 6: Interpretable MC policy results by our GPRL approach for complexities 1-15\na median of the regression errors of 0.028 at best (Fig-\nure 7a). This rather low regression error suggests a good\nperformance on imitating the NN policy.\nIn Figure 8a the performances of GPRL and the GP\nregression approaches are evaluated by testing the policies\nof their Pareto fronts with diﬀerent start states on the real\nMC dynamics. Note that on average our GPRL approach\nproduced the best interpretable policies for all complexi-\nties. However, the performance of the symbolic regression\napproach is quite similar, which suggests that for the MC\nbenchmark such a procedure of creating interpretable RL\npolicies is not impossible.\n7.2. Cart-pole Balancing\nWe conducted the CPB experiments using a time hori-\nzon T of 100 and a discount vector γ of 0.97 (q = 0.05).\nFor the CPB experiments, a non-interpretable NN policy\nwith ﬁtness value F = −27.1 (equivalent to a penalty value\nof 27.1) has been trained prior to the GP experiments.\nBased on our experience, this performance value represents\na successful CPB policy. The NN policy has two hidden\nlayers with tanh activation function and 10 hidden neurons\non each layer, i.e., complexity 2471.\nIn Figure 5b the results of the GP model-based training\nare compared to the NN policy baseline. Note that all ten\nindependent GP runs produced Pareto fronts of very similar\nperformance. In comparison to the NN policy, individuals\nwith complexity < 5 performed signiﬁcantly worse with\nrespect to the model penalty. Individuals with complexity\n≥13 on the other hand yielded a median penalty of 27.5\nor below, which corresponds to an excellent CPB policy\nsuitability.\nFigure 9 depicts all individuals of the Pareto fronts of\nthe ten experiment runs from complexity 1 to 15. Note how\nthe solutions agree not only on the utilized state variables\nbut also on the ﬂoat values of the respective factors. Diﬀer-\nences often only arise due to the multiplication of the whole\nterms with diﬀerent factors, i.e., the ratios between the im-\nportant state variables remain very similar. Provided with\nsuch a policy Pareto chart, experts are more likely to suc-\nceed selecting interpretable policies, since common policy\nconcepts are conveniently identiﬁed with respect to both,\ntheir complexity as well as their model-based performance.\nThe Pareto front results of the GP regression experi-\nments are presented in Figure 7b. Here, the ﬁtness value\ndriving the GP optimization was the regression error with\nrespect to the NN policy. As expected, the individuals of\nhigher complexity achieve lower errors. Note that compared\nto GP model-based training the Pareto fronts results of the\n10 experiments are spread throughout a bigger area. This\nfact suggests that the NN policy might be highly non-linear\nin its outputs, which makes it harder for the GP to ﬁnd\nsolutions in this huge search space.\nTo evaluate the true performance of the two approaches\nGP model-based training and GP regression training, the\nindividuals of both sets of Pareto fronts have been tested\non the true CPB dynamics. Figure 8b shows the resulting\nsquashed Pareto fronts compared to the performance of the\nNN policy. It is obvious that almost for every complexity\nthe GP model-based approach GPRL is superior to the\nGP regression idea. Not only are the median results of\nsigniﬁcantly lower penalty, but the variance of the GPRL\nsolution is also much lower compared to the GP regression\nresult. Interestingly, the median GP results for complexity\n11 and above even outperformed the NN policy result. This\nindicates that the NN policy already overﬁtted the NN\nmodel and exploited its inaccuracies, while the simple GP\npolicy equations generalize better because of their rather\nrestricted structure.\n7.3. Industrial Benchmark\nWe conducted the IB experiments using a time horizon\nT of 100 and a discount vector γ of 1 (q = 1). For the IB\nexperiments, a non-interpretable NN policy with ﬁtness\nvalue F = −165.5 (equivalent to a penalty value of 165.5)\nhas been trained prior to the GP experiments. Based on our\nexperiences, this performance value represents a successful\nIB policy. The NN policy consists of three separate NNs\nwith one hidden layer, tanh activation functions, and 20\nhidden neurons on each layer, i.e., complexity value 43, 617.\n10\n0\n20\n40\n60\n80\n100\ncomplexity\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nregression error\nRegression training\nGP Model-based Policy\nGP Regression Policy\nReal Dynamics\nNN Model-based Policy\nNN Model\n(a) MC\n0\n20\n40\n60\n80\n100\ncomplexity\n0.16\n0.18\n0.2\n0.22\n0.24\n0.26\n0.28\n0.3\n0.32\n0.34\n0.36\nregression error\n(b) CPB\n0\n20\n40\n60\n80\n100\ncomplexity\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\n1.2\n1.3\nregression error\n(c) IB\nFigure 7: Pareto fronts from ten symbolic regression GP trainings.\nDepicted is the median (blue line) together with the minimum and\nmaximum (semi-transparent blue area) Pareto front regression error\nfrom all experiments.\n0\n20\n40\n60\n80\n100\ncomplexity\n40.2\n40.4\n40.6\n40.8\n41\n41.2\n41.4\n41.6\n41.8\n42\nreal dynamics penalty\nEvaluation\nGP Model-based Policy\nGP Regression Policy\nReal Dynamics\nNN Model-based Policy\nNN Model\n(a) MC\n0\n20\n40\n60\n80\n100\ncomplexity\n35\n40\n45\n50\n55\n60\nreal dynamics penalty\n(b) CPB\n0\n20\n40\n60\n80\n100\ncomplexity\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n5.5\n6\nreal dynamics penalty\n104\n(c) IB\nFigure 8: Squashed Pareto fronts from evaluating both GP results\nwith a diﬀerent set of states on the real benchmark dynamics. On\naverage, the GPRL individuals (green) outperform the GP regression\nindividuals (blue).\n11\n1\n3\n5\n7\n9\n11\n13\n15\nFigure 9: Interpretable CPB policy results by our GPRL approach for complexities 1-15\nThe results of ten individual GPRL runs are depicted\nin Figure 5c. Despite the median model penalty of the\nPareto fronts is worse compared to the non-interpretable\nNN policy, one of the GPRL results produced a slightly\nbetter performing policy. This comes as a surprise, since\ngenerally the NN policy has an advantage in degrees of\nfreedom to ﬁnd the optimal policy.\nThe resulting GPRL policies between complexity 21 and\n29 are presented in Figure 10. Note that independently\nlearned policies share similar concepts of how policy actions\nare computed from related state variables with similar time\nlags. For example, the equations for ∆h always use a linear\ncombination of shift value h from time lags -2, -3, or -4\nand the constant setpoint value p. Another example is\nthe computation of ∆v, for which a velocity value v with\ntime lags ≥−10 is always used. Moreover, it is possible to\nreveal common concepts and relevant diﬀerences between\na rich set of possible solutions. This presentation could\nprovide the domain experts with important insights on how\nwell-performing policies for the system at hand look like, on\nwhich state variables they react, and how they generalize\nin state space areas where currently insuﬃcient training\ndata is available.\nThe results of applying GP symbolic regression on a non-\ninterpretable NN policy are shown in Figure 7c. For each\npolicy action, an independent GP run has been conducted.\nAfter the training multi-dimensional policies have been\ncreated in such a way that the accumulated regression errors\nof ∆v, ∆g, and ∆s are as low as possible for every possible\ncomplexity value. This procedure has been repeated ten\ntimes to yield ten independent IB policies.\nIn the ﬁnal step of the experiment, the GPRL and the\nGP regression solutions have been evaluated on the real\nIB dynamics. Figure 8c clearly reveals the strengths of our\nGPRL approach. First, the model-based GP training per-\nforms signiﬁcantly better than the symbolic regression train-\ning. Despite even in the MC and CPB experiments GPRL\noutperformed the regression approach, the experiments\nwith IB illustrate the complete performance breakdown\nof the latter. Second, the good generalization properties\nof GPRL led to interpretable policies which even outper-\nformed a non-interpretable NN policy from complexity 11\non. Note that given this result and the superior perfor-\nmance during model-based NN policy training, it can be\nconcluded that the NN policy started to overﬁt the policy\nwith respect to the NN model penalty.\n8. Conclusion\nOur GPRL approach conducts model-based batch RL\nto learn interpretable policies for control problems on the\nbasis of already existing default system trajectories. The\npolicies can be represented by compact algebraic equations\nor Boolean logic terms. Autonomous learning of such in-\nterpretable policies is of high interest for industry domain\nexperts. Presented with a number of GPRL results for a\npreferred range of complexity, new concepts for controlling\nan industrial plant can be revealed. Moreover, safety con-\ncerns can more easily be addressed, if the policy at hand\nitself, as well as its generalization to certain state space\nareas, are completely understandable.\nThe complete GPRL procedure of (i) training a model\nfrom existing system trajectories, (ii) learning interpretable\npolicies by GP, (iii) selecting a favorable solution candidate\nfrom a Pareto front result has been evaluated for three\nRL benchmarks, i.e., MC, CPB, and IB. First, the con-\ntrol performance was compared to a non-interpretable NN\npolicy. This comparison showed that the GPRL perfor-\nmance on the approximation model can be slightly worse\ncompared to the NN policy result. However, when eval-\nuated on the real system dynamics, even interpretable\npolicies of rather low complexity could outperform the non-\ninterpretable approach in many occasions. This suggests\nthat simple algebraic equations used as policies generalize\nbetter on new system states. In a second evaluation, our\nGPRL approach has been compared to a straightforward\n12\n21\n23\n25\n27\n28\n29\nFigure 10: Interpretable IB policies for complexities 21-29. The presented 3-dimensional policies are the result of ten independent GPRL\ntrainings. Each box contains three policy equations for ∆v0,∆g0, and ∆h0 (from top to bottom) to calculate actions for time step t = 0.\nThe input variables’ indices represent the respective negative time lag in which they have been recorded, e.g., h3 represents the value of shift\nthree time steps ago at t = −3. The actions are limited to -1 and +1 before they are applied on the system dynamics. The input variables\n(p, v, g, h, f, c) are normalized by subtracting their respective mean (55.0, 48.75, 50.53, 49.45, 37.51, 166.33) and dividing by their respective\nstandard deviation (28.72, 12.31, 29.91, 29.22, 31.17, 139.44). These values can easily be calculated from the training data set.\n13\nGP utilization as a symbolic regression tool, i.e., ﬁtting\nthe existing non-interpretable NN policy by GP to yield\ninterpretable policies of similar control performance. All of\nour experiments showed that this strategy is signiﬁcantly\nless suitable to produce policies of adequate performance.\nEspecially the experiments with the IB indicated that\nthe application of the proposed GPRL approach in indus-\ntry settings could prove to be of signiﬁcant interest. In\nmany cases, data from systems is readily available and inter-\npretable simple algebraic policies are favored over black-box\nRL solutions, such as non-interpretable NN policies.\nAcknowledgment\nThe project on which this report is based was supported\nwith funds from the German Federal Ministry of Education\nand Research under project number 01IB15001. The sole\nresponsibility for the report’s contents lies with the authors.\nReferences\nBakker, B., 2004. The state of mind: Reinforcement learning with\nrecurrent neural networks. Ph.D. thesis, Leiden University, Nether-\nlands.\nBlickle, T., Thiele, L., 1995. A mathematical analysis of tournament\nselection. In: ICGA. pp. 9–16.\nBreiman, L., Friedman, J., Olshen, R., Stone, C., 1984. Classiﬁcation\nand Regression Trees. CRC Press, Boca Raton, FL.\nBusoniu, L., Babuska, R., De Schutter, B., Ernst, D., 2010. Rein-\nforcement Learning and Dynamic Programming Using Function\nApproximators. CRC Press.\nDepeweg, S., Hern´andez-Lobato, J. M., Doshi-Velez, F., Udluft, S.,\n2016. Learning and policy search in stochastic dynamical systems\nwith Bayesian neural networks. arXiv preprint arXiv:1605.07127.\nDowning, K. L., 2001. Adaptive genetic programs via reinforcement\nlearning. In: Proceedings of the 3rd Annual Conference on Genetic\nand Evolutionary Computation. GECCO’01. Morgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, pp. 19–26.\nDubˇc´akov´a, R., 2011. Eureqa: software review. Genetic programming\nand evolvable machines 12 (2), 173–178.\nDuell, S., Udluft, S., Sterzing, V., 2012. Solving partially observable\nreinforcement learning problems with recurrent neural networks.\nIn: Neural Networks: Tricks of the Trade. Springer, pp. 709–733.\nErnst, D., Geurts, P., Wehenkel, L., Littman, L., 2005. Tree-based\nbatch mode reinforcement learning. Journal of Machine Learning\nResearch 6, 503–556.\nFantoni, I., Lozano, R., 2002. Non-linear control for underactuated\nmechanical systems. Springer.\nGearhart, C., 2003. Genetic programming as policy search in markov\ndecision processes. In: Koza, J. R. (Ed.), Genetic Algorithms\nand Genetic Programming at Stanford 2003. Stanford Bookstore,\nStanford, California, 94305-3079 USA, pp. 61–67.\nGordon, G., 1995. Stable function approximation in dynamic pro-\ngramming. In: In Machine Learning: Proceedings of the Twelfth\nInternational Conference. Morgan Kaufmann.\nHein, D., Depeweg, S., Tokic, M., Udluft, S., Hentschel, A., Runkler,\nT. A., Sterzing, V., 2017a. A benchmark environment motivated\nby industrial control problems. In: 2017 IEEE Symposium Series\non Computational Intelligence (SSCI). pp. 1–8.\nHein, D., Hentschel, A., Runkler, T. A., Udluft, S., 2017b. Parti-\ncle swarm optimization for generating interpretable fuzzy rein-\nforcement learning policies. Engineering Applications of Artiﬁcial\nIntelligence 65, 87–98.\nHein, D., Hentschel, A., Runkler, T. A., Udluft, S., 2018. Particle\nswarm optimization for model predictive control in reinforcement\nlearning environments. In: Shi, Y. (Ed.), Critical Developments\nand Applications of Swarm Intelligence. IGI Global, Hershey, PA,\nUSA, Ch. 16, pp. 401–427.\nHein, D., Udluft, S., Tokic, M., Hentschel, A., Runkler, T. A., Sterz-\ning, V., 2017c. Batch reinforcement learning on the industrial\nbenchmark: First experiences. In: 2017 International Joint Con-\nference on Neural Networks (IJCNN). pp. 4214–4221.\nJuang, C.-F., Lin, J.-Y., Lin, C.-T., Apr. 2000. Genetic reinforcement\nlearning through symbiotic evolution for fuzzy controller design.\nTrans. Sys. Man Cyber. Part B 30 (2), 290–302.\nKamio, S., Iba, H., Jun. 2005. Adaptation technique for integrating\ngenetic programming and reinforcement learning for real robots.\nTrans. Evol. Comp 9 (3), 318–333.\nKatagiri, H., Hirasawa, K., Hu, J., Murata, J., Kosaka, M., 2002.\nNetwork structure oriented evolutionary model: Genetic network\nprogramming. Transactions of the Society of Instrument and Con-\ntrol Engineers 38 (5), 485–494.\nKeane, M. A., Koza, J. R., Streeter, M. J., 2002. Automatic synthesis\nusing genetic programming of an improved general-purpose con-\ntroller for industrially representative plants. In: Proceedings of the\n2002 NASA/DoD Conference on Evolvable Hardware (EH’02). EH\n’02. IEEE Computer Society, Washington, DC, USA, pp. 113–123.\nKoshiyama, A. S., Escovedo, T., Vellasco, M. M. B. R., Tanscheit,\nR., July 2014. Gpﬁs-control: A fuzzy genetic model for control\ntasks. In: 2014 IEEE International Conference on Fuzzy Systems\n(FUZZ-IEEE). pp. 1953–1959.\nKoza, J. R., 1992. Genetic Programming: On the Programming of\nComputers by Means of Natural Selection. MIT Press, Cambridge,\nMA, USA.\nKoza, J. R., 2010. Human-competitive results produced by genetic pro-\ngramming. Genetic Programming and Evolvable Machines 11 (3),\n251–284.\nLagoudakis, M., Parr, R., 2003. Least-squares policy iteration. Journal\nof Machine Learning Research, 1107–1149.\nLe, N., Xuan, H. N., Brabazon, A., Thi, T. P., 2016. Complexity\nmeasures in genetic programming learning: A brief review. In:\nEvolutionary Computation (CEC), 2016 IEEE Congress on. IEEE,\npp. 2409–2416.\nMabu, S., Hirasawa, K., Hu, J., 2004. Genetic Network Programming\nwith Reinforcement Learning and Its Performance Evaluation.\nSpringer Berlin Heidelberg, Berlin, Heidelberg, pp. 710–711.\nMabu, S., Hirasawa, K., Hu, J., Murata, J., 2002. Online learn-\ning of genetic network programming (GNP). In: Fogel, D. B.,\nEl-Sharkawi, M. A., Yao, X., Greenwood, G., Iba, H., Marrow,\nP., Shackleton, M. (Eds.), Proceedings of the 2002 Congress on\nEvolutionary Computation CEC2002. IEEE Press, pp. 321–326.\nMaes, F., Fonteneau, R., Wehenkel, L., Ernst, D., 2012. Policy search\nin a space of simple closed-form formulas: towards interpretability\nof reinforcement learning. Discovery Science, 37–50.\nMoore, A. W., nov 1990. Eﬃcient memory-based learning for robot\ncontrol. Tech. Rep. UCAM-CL-TR-209, University of Cambridge,\nComputer Laboratory.\nURL\nhttp://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.\npdf\nNeuneier, R., Zimmermann, H.-G., 2012. How to train neural net-\nworks. In: Montavon, G., Orr, G., M¨uller, K.-R. (Eds.), Neural\nNetworks: Tricks of the Trade, Second Edition. Springer, pp. 369–\n418.\nOrmoneit, D., Sen, S., 2002. Kernel-based reinforcement learning.\nMachine learning 49 (2), 161–178.\nRasmussen, E., Williams, C., 2006. Gaussian processes for machine\nlearning (adaptive computation and machine learning). Mit Press\nLtd.\nRiedmiller, M., 2005a. Neural ﬁtted Q iteration — ﬁrst experiences\nwith a data eﬃcient neural reinforcement learning method. In:\nMachine Learning: ECML 2005. Vol. 3720. Springer, pp. 317–328.\nRiedmiller, M., 2005b. Neural reinforcement learning to swing-up\nand balance a real pole. In: Systems, Man and Cybernetics, 2005\nIEEE International Conference on. Vol. 4. pp. 3191–3196.\nRiedmiller, M., Gabel, T., Hafner, R., Lange, S., 2009. Reinforcement\nlearning for robot soccer. Autonomous Robots 27 (1), 55–73.\nSch¨afer, A. M., 2008. Reinforcement learning with recurrent neural\n14\nnetworks. Ph.D. thesis, University of Osnabr¨uck, Germany.\nSch¨afer, A. M., Udluft, S., Zimmermann, H.-G., 2007. A recurrent\ncontrol neural network for data eﬃcient reinforcement learning. In:\nProceedings of IEEE Symposium on Adaptive Dynamic Program-\nming and Reinforcement Learning. pp. 151–157.\nSchneegaß, D., Udluft, S., Martinetz, T., 2007a. Improving optimality\nof neural rewards regression for data-eﬃcient batch near-optimal\npolicy identiﬁcation. Proceedings of the International Conference\non Artiﬁcial Neural Networks.\nSchneegaß, D., Udluft, S., Martinetz, T., 2007b. Neural rewards\nregression for near-optimal policy identiﬁcation in Markovian and\npartial observable environments. In: Proceedings the European\nSymposium on Artiﬁcial Neural Networks. pp. 301–306.\nSchwefel, H.-P., 1981. Numerical optimization of computer models.\nJohn Wiley & Sons, Inc.\nSchwefel,\nH.-P.,\n1995. Evolution and optimum seeking. sixth-\ngeneration computer technology series.\nShimooka, H., Fujimoto, Y., 1999. Generating equations with ge-\nnetic programming for control of a movable inverted pendulum. In:\nSelected Papers from the Second Asia-Paciﬁc Conference on Simu-\nlated Evolution and Learning on Simulated Evolution and Learning.\nSEAL’98. Springer-Verlag, London, UK, UK, pp. 179–186.\nSutton, R., 1988. Learning to predict by the methods of temporal\ndiﬀerences. Machine learning 3 (1), 9–44.\nSutton, R., Barto, A., 1998. Reinforcement learning: an introduction.\nA Bradford book.\n15\n",
  "categories": [
    "cs.AI",
    "cs.NE",
    "cs.SY"
  ],
  "published": "2017-12-12",
  "updated": "2018-04-04"
}