{
  "id": "http://arxiv.org/abs/1712.04371v2",
  "title": "Music Generation by Deep Learning - Challenges and Directions",
  "authors": [
    "Jean-Pierre Briot",
    "François Pachet"
  ],
  "abstract": "In addition to traditional tasks such as prediction, classification and\ntranslation, deep learning is receiving growing attention as an approach for\nmusic generation, as witnessed by recent research groups such as Magenta at\nGoogle and CTRL (Creator Technology Research Lab) at Spotify. The motivation is\nin using the capacity of deep learning architectures and training techniques to\nautomatically learn musical styles from arbitrary musical corpora and then to\ngenerate samples from the estimated distribution. However, a direct application\nof deep learning to generate content rapidly reaches limits as the generated\ncontent tends to mimic the training set without exhibiting true creativity.\nMoreover, deep learning architectures do not offer direct ways for controlling\ngeneration (e.g., imposing some tonality or other arbitrary constraints).\nFurthermore, deep learning architectures alone are autistic automata which\ngenerate music autonomously without human user interaction, far from the\nobjective of interactively assisting musicians to compose and refine music.\nIssues such as: control, structure, creativity and interactivity are the focus\nof our analysis. In this paper, we select some limitations of a direct\napplication of deep learning to music generation, analyze why the issues are\nnot fulfilled and how to address them by possible approaches. Various examples\nof recent systems are cited as examples of promising directions.",
  "text": "Music Generation by Deep Learning\n– Challenges and Directions∗\nJean-Pierre Briot†\nFran¸cois Pachet‡\n† Sorbonne Universit´es, UPMC Univ Paris 06, CNRS, LIP6, Paris, France\nJean-Pierre.Briot@lip6.fr\n‡ Spotify Creator Technology Research Lab, Paris, France\nfrancois@spotify.com\nAbstract: In addition to traditional tasks such as prediction, classiﬁcation and translation, deep learning\nis receiving growing attention as an approach for music generation, as witnessed by recent research groups such\nas Magenta at Google and CTRL (Creator Technology Research Lab) at Spotify. The motivation is in using\nthe capacity of deep learning architectures and training techniques to automatically learn musical styles from\narbitrary musical corpora and then to generate samples from the estimated distribution. However, a direct\napplication of deep learning to generate content rapidly reaches limits as the generated content tends to mimic\nthe training set without exhibiting true creativity. Moreover, deep learning architectures do not oﬀer direct\nways for controlling generation (e.g., imposing some tonality or other arbitrary constraints). Furthermore, deep\nlearning architectures alone are autistic automata which generate music autonomously without human user\ninteraction, far from the objective of interactively assisting musicians to compose and reﬁne music. Issues such\nas: control, structure, creativity and interactivity are the focus of our analysis. In this paper, we select some\nlimitations of a direct application of deep learning to music generation, analyze why the issues are not fulﬁlled\nand how to address them by possible approaches. Various examples of recent systems are cited as examples of\npromising directions.\n1\nIntroduction\n1.1\nDeep Learning\nDeep learning has become a fast growing domain and is now used routinely for classiﬁcation and prediction\ntasks, such as image and voice recognition, as well as translation. It emerged about 10 years ago, when a\ndeep learning architecture signiﬁcantly outperformed standard techniques using handcrafted features on an\nimage classiﬁcation task [HOT06]. We may explain this success and reemergence of artiﬁcial neural networks\narchitectures and techniques by the combination of:\n1. technical progress, such as: convolutions, which provide motif translation invariance [CB98], and LSTM\n(Long Short-Term Memory), which resolved ineﬃcient training of recurrent neural networks [HS97];\n2. availability of multiple data sets;\n3. availability of eﬃcient and cheap computing power, e.g., oﬀered by graphics processing units (GPU).\nThere is no consensual deﬁnition of deep learning. It is a repertoire of machine learning (ML) techniques,\nbased on artiﬁcial neural networks1. The common ground is the term deep, which means that there are multiple\nlayers processing multiple levels of abstractions, which are automatically extracted from data, as a way to\nexpress complex representations in terms of simpler representations.\nMain applications of deep learning are within the two traditional machine learning tasks of classiﬁcation\nand prediction, as a testimony of the initial DNA of neural networks: logistic regression and linear regression.\nBut a growing area of application of deep learning techniques is the generation of content: text, images, and\nmusic, the focus of this article.\n∗To appear in Special Issue on Deep learning for music and audio, Neural Computing & Applications, Springer Nature, 2018.\n1With many variants such as convolutional networks, recurrent networks, autoencoders, restricted Boltzmann machines, etc.\n[GBC16].\n1\narXiv:1712.04371v2  [cs.SD]  30 Sep 2018\n1.2\nDeep Learning for Music Generation\nThe motivation for using deep learning, and more generally machine learning techniques, to generate musical\ncontent is its generality. As opposed to handcrafted models for, e.g., grammar-based [Ste84] or rule-based music\ngeneration systems [Ebc88], a machine-learning-based generation system can automatically learn a model, a\nstyle, from an arbitrary corpus of music. Generation can then take place by using prediction (e.g., to predict\nthe pitch of the next note of a melody) or classiﬁcation (e.g., to recognize the chord corresponding to a melody),\nbased on the distribution and correlations learnt by the deep model which represent the style of the corpus.\nAs stated by Fiebrink and Caramiaux in [FC16], beneﬁts are: 1) it can make creation feasible when the\ndesired application is too complex to be described by analytical formulations or manual brute force design; 2)\nlearning algorithms are often less brittle than manually-designed rule sets and learned rules are more likely to\ngeneralize accurately to new contexts in which inputs may change.\n1.3\nChallenges\nA direct application of deep learning architectures and techniques to generation, although it could produce\nimpressing results2, suﬀers from some limitations. We consider here3:\n• Control, e.g., tonality conformance, maximum number of repeated notes, rhythm, etc.;\n• Structure, versus wandering music without a sense of direction;\n• Creativity, versus imitation and risk of plagiarism;\n• Interactivity, versus automated single-step generation.\n1.4\nRelated Work\nA comprehensive survey and analysis by Briot et al. of deep learning techniques to generate musical content is\navailable in a book [BHP18]. In [HCC17], Herremans et al. propose a function-oriented taxonomy for various\nkinds of music generation systems.\nExamples of surveys about of AI-based methods for algorithmic music\ncomposition are by Papadopoulos and Wiggins [PW99] and by Fern´andez and Vico [FV13], as well as books by\nCope [Cop00] and by Nierhaus [Nie09]. In [Gra14], Graves analyses the application of recurrent neural networks\narchitectures to generate sequences (text and music). In [FC16], Fiebrink and Caramiaux address the issue of\nusing machine learning to generate creative music. We are not aware of a comprehensive analysis dedicated to\ndeep learning (and artiﬁcial neural networks techniques) that systematically analyzes limitations and challenges,\nsolutions and directions, in other words that is problem-oriented and not just application-oriented.\n1.5\nOrganization\nThe article is organized as follows. Section 1 (this section) introduces the general context of deep learning-based\nmusic generation and lists some important challenges. It also includes a comparison to some related work. The\nfollowing sections analyze each challenge and some solutions, while illustrating through examples of actual\nsystems: control/section 2, structure/section 3, creativity/section 4 and interactivity/section 5.\n2\nControl\nMusicians usually want to adapt ideas and patterns borrowed from other contexts to their own objective, e.g.,\ntransposition to another key, minimizing the number of notes. In practice this means the ability to control\ngeneration by a deep learning architecture.\n2.1\nDimensions of control strategies\nSuch arbitrary control is actually a diﬃcult issue for current deep learning architectures and techniques, because\nstandard neural networks are not designed to be controlled. As opposed to Markov models which have an\noperational model where one can attach constraints onto their internal operational structure in order to control\nthe generation4, neural networks do not oﬀer such an operational entry point. Moreover, the distributed nature\nof their representation does not provide a direct correspondence to the structure of the content generated. As\na result, strategies for controlling deep learning generation that we will analyze have to rely on some external\nintervention at various entry points (hooks), such as:\n2Music diﬃcult to distinguish from the original corpus.\n3Additional challenges are analyzed in [BHP18].\n4Two examples are Markov constraints [PRB11] and factor graphs [PPR17].\n2\n• Input;\n• Output;\n• Encapsulation/reformulation.\n2.2\nSampling\nSampling a model5 to generate content may be an entry point for control if we introduce constraints on the\noutput generation (this is called constraint sampling).\nThis is usually implemented by a generate-and-test\napproach, where valid solutions are picked from a set of generated random samples from the model6. As we\nwill see, a key issue is how to guide the sampling process in order to fulﬁll the objectives (constraints), thus\nsampling will be often combined with other strategies.\n2.3\nConditioning\nThe strategy of conditioning (sometimes also named conditional architecture) is to condition the architecture on\nsome extra conditioning information, which could be arbitrary, e.g., a class label or data from other modalities.\nExamples are:\n• a bass line or a beat structure, in the rhythm generation system [MKPKK17];\n• a chord progression, in the MidiNet architecture [YCY17];\n• a musical genre or an instrument, in the WaveNet architecture [vdODZ+16];\n• a set of positional constraints, in the Anticipation-RNN architecture [HN17].\nIn practice, the conditioning information is usually fed into the architecture as an additional input layer.\nConditioning is a way to have some degree of parameterized control over the generation process.\n2.3.1\nExample 1: WaveNet Audio Speech and Music Generation\nThe WaveNet architecture by van der Oord et al. [vdODZ+16] is aimed at generating raw audio waveforms. The\narchitecture is based on a convolutional feedforward network without pooling layer7. It has been experimented\non generation for three audio domains: multi-speaker, text-to-speech (TTS) and music.\nThe WaveNet architecture uses conditioning as a way to guide the generation, by adding an additional tag\nas a conditioning input. Two options are considered: global conditioning or local conditioning, depending if the\nconditioning input is shared for all time steps or is speciﬁc to each time step.\nAn example of application of conditioning WaveNet for a text-to-speech application domain is to feed\nlinguistic features (e.g., North American English or Mandarin Chinese speakers) in order to generate speech\nwith a better prosody.\nThe authors also report preliminary experiments on conditioning music models to\ngenerate music given a set of tags specifying, e.g., genre or instruments.\n2.3.2\nExample 2: Anticipation-RNN Bach Melody Generation\nHadjeres and Nielsen propose a system named Anticipation-RNN [HN17] for generating melodies with unary\nconstraints on notes (to enforce a given note at a given time position to have a given value). The limitation\nwhen using a standard note-to-note iterative strategy for generation by a recurrent network is that enforcing\nthe constraint at a certain time step may retrospectively invalidate the distribution of the previously generated\nitems, as shown in [PRB11].\nThe idea is to condition the recurrent network (RNN) on some information\nsummarizing the set of further (in time) constraints as a way to anticipate oncoming constraints, in order to\ngenerate notes with a correct distribution.\nTherefore, a second RNN architecture8, named Constraint-RNN, is used and it functions backward in time\nand ts outputs are used as additional inputs of the main RNN (named Token-RNN), resulting in the architecture\nshown at Figure 1, with:\n5The model can be stochastic, such as a restricted Boltzmann machine (RBM) [GBC16], or deterministic, such as a feedforward\nor a recurrent network. In that latter case, it is common practice to sample from the softmax output in order to introduce variability\nfor the generated content [BHP18].\n6Note that this may be a very costly process and moreover with no guarantee to succeed.\n7An important speciﬁcity of the architecture (not discussed here) is the notion of dilated convolution, where convolution ﬁlters\nare incrementally dilated in order to provide very large receptive ﬁelds with just a few layers, while preserving input resolution and\ncomputational eﬃciency [vdODZ+16].\n8Both are 2-layer LSTMs [HS97].\n3\nFigure 1: Anticipation-RNN architecture. Reproduced from [HN17] with permission of the authors\nFigure 2: Examples of melodies generated by Anticipation-RNN. Reproduced from [HN17] with permission of\nthe authors\n• ci is a positional constraint;\n• oi is the output at index i (after i iterations) of Constraint-RNN – it summarizes constraint informations\nfrom step i to ﬁnal step (end of the sequence) N. It will be concatenated (⊕) to input si−1 of Token-RNN\nin order to predict next item si.\nThe architecture has been tested on a corpus of melodies taken from J. S. Bach chorales. Three examples of\nmelodies generated with the same set of positional constraints (indicated with notes in green within a rectangle)\nare shown at Figure 2. The model is indeed able to anticipate each positional constraint by adjusting its direction\ntowards the target (lower-pitched or higher-pitched note).\n2.4\nInput Manipulation\nThe strategy of input manipulation has been pioneered for images by DeepDream [MOT15]. The idea is that\nthe initial input content, or a brand new (randomly generated) input content, is incrementally manipulated in\norder to match a target property. Note that control of the generation is indirect, as it is not being applied to\nthe output but to the input, before generation. Examples are:\n• maximizing the activation of a speciﬁc unit, to exaggerate some visual element speciﬁc to this unit, in\nDeepDream [MOT15];\n• maximizing the similarity to a given target, to create a consonant melody, in DeepHear [Sun17];\n• maximizing both the content similarity to some initial image and the style similarity to a reference style\nimage, to perform style transfer [GEB15];\n• maximizing the similarity of structure to some reference music, to perform style imposition [LGW16].\nInterestingly, this is done by reusing standard training mechanisms, namely back-propagation to compute\nthe gradients, as well as gradient descent to minimize the cost.\n4\nFigure 3: Generation in DeepHear. Extension of a ﬁgure reproduced from [Sun17] with permission of the author\n2.4.1\nExample 1: DeepHear Ragtime Melody Accompaniment Generation\nThe DeepHear architecture by Sun [Sun17] is aimed at generating ragtime jazz melodies. The architecture is a\n4-layer stacked autoencoders (that is 4 hierarchically nested autoencoders), with a decreasing number of hidden\nunits, down to 16 units.\nAt ﬁrst, the model is trained9 on a corpus of 600 measures of Scott Joplin’s ragtime music, split into 4-\nmeasure long segments. Generation is performed by inputing random data as the seed into the 16 bottleneck\nhidden layer units and then by feedforwarding it into the chain of decoders to produce an output (in the same\n4-measure long format of the training examples), as shown at Figure 3.\nIn addition to the generation of new melodies, DeepHear is used with a diﬀerent objective: to harmonize a\nmelody, while using the same architecture as well as what has already been learnt10. The idea is to ﬁnd a label\ninstance of the set of features i.e. a set of values for the 16 units of the bottleneck hidden layer of the stacked\nautoencoders which will result in some decoded output matching as much as possible a given melody. A simple\ndistance function is deﬁned to represent the dissimilarity between two melodies (in practice, the number of\nnot matched notes). Then a gradient descent is conducted onto the variables of the embedding, guided by the\ngradients corresponding to the distance function until ﬁnding a suﬃciently similar decoded melody. Although\nthis is not a real counterpoint but rather the generation of a similar (consonant) melody, the results do produce\nsome naive counterpoint with a ragtime ﬂavor.\n2.4.2\nExample 2: VRAE Video Game Melody Generation\nNote that input manipulation of the hidden layer units of an autoencoder (or stacked autoencoders) bears some\nanalogy with variational autoencoders11, such as for instance the VRAE (Variational Recurrent Auto-Encoder)\narchitecture of Fabius and van Amersfoort [FvA15]. Indeed in both cases, there is some exploration of possible\nvalues for the hidden units (latent variables) in order to generate variations of musical content by the decoder (or\nthe chain of decoders). The important diﬀerence is that in the case of variational autoencoders, the exploration\nof values is user-directed, although it could be guided by some principle, for example an interpolation to create\na medley of two songs, or the addition or subtraction of an attribute vector capturing a given characteristic\n(e.g., high density of notes as in Figure 4). In the case of input manipulation, the exploration of values is\nautomatically guided by the gradient following mechanism, the user having priorly speciﬁed a cost function to\nbe minimized or an objective to be maximized.\n2.4.3\nExample 3: Image and Audio Style Transfer\nStyle transfer has been pioneered by Gatys et al. [GEB15] for images. The idea, summarized at Figure 5, is to\nuse a deep learning architecture to independently capture:\n• the features of a ﬁrst image (named the content),\n9Autoencoders are trained with the same data as input and output and therefore have to discover signiﬁcative features in order\nto be able to reconstruct the compressed data.\n10Note that this is a simple example of transfer learning [GBC16], with a same domain and a same training, but for a diﬀerent\ntask.\n11A variational autoencoder (VAE) [KW14] is an autoencoder with the added constraint that the encoded representation (its\nlatent variables) follows some prior probability distribution (usually a Gaussian distribution). Therefore, a variational autoencoder\nis able to learn a “smooth” latent space mapping to realistic examples.\n5\nFigure 4: Example of melody generated (bottom) by MusicVAE by adding a “high note density” attribute\nvector to the latent space of an existing melody (top). Reproduced from [RER+18a] with permission of the\nauthors\n• and the style (the correlations between features) of a second image (named the style),\n• and then, to use gradient following to guide the incremental modiﬁcation of an initially random third\nimage, with the double objective of matching both the content and the style descriptions12.\nTransposing this style transfer technique to music was a natural direction and it has been experimented\nindependently for audio, e.g., in [UL16] and [FYR16], both using a spectrogram (and not a direct wave signal)\nas input. The result is eﬀective, but not as interesting as in the case of painting style transfer, being somehow\nmore similar to a sound merging of the style and of the content.\nWe believe that this is because of the\nanisotropy13 of global music content representation.\n2.4.4\nExample 4: C-RBM Mozart Sonata Generation\nThe C-RBM architecture proposed by Lattner et al. [LGW16] uses a restricted Boltzmann machine (RBM) to\nlearn the local structure, seen as the musical texture, of a corpus of musical pieces (in practice, Mozart sonatas).\nThe architecture is convolutional (only) on the time dimension, in order to model temporally invariant motives,\nbut not pitch invariant motives which would break the notion of tonality. The main idea is in imposing by\nconstraints onto the generated piece some more global structure (form, e.g., AABA, as well as tonality), seen as a\nstructural template inspired from the reference of an existing musical piece. This is called structure imposition14,\nalso coined as templagiarism (short for template plagiarism) by Hofstadter [Hof01].\nGeneration is done by sampling from the RBM with three types of constraints:\n• Self-similarity, to specify a global structure (e.g., AABA) in the generated music piece. This is modeled by\nminimizing the distance between the self-similarity matrices of the reference target and of the intermediate\nsolution;\n• Tonality constraint, to specify a key (tonality). To estimate the key in a given temporal window, the\ndistribution of pitch classes is compared with the key proﬁles of the reference;\n• Meter constraint, to impose a speciﬁc meter (also named a time signature, e.g., 4/4) and its related\nrhythmic pattern (e.g., accent on the third beat). The relative occurrence of note onsets within a measure\nis constrained to follow that of the reference.\n12Note that one may balance between content and style objectives through some α and β parameters in the Ltotal combined loss\nfunction shown at top of Figure 5.\n13In the case of an image, the correlations between visual elements (pixels) are equivalent whatever the direction (horizontal axis,\nvertical axis, diagonal axis or any arbitrary direction), in other words correlations are isotropic. In the case of a global representation\nof musical content (see, e.g., Figure 12), where the horizontal dimension represents time and the vertical dimension represents the\nnotes, horizontal correlations represent temporal correlations and vertical correlations represent harmonic correlations, which have\nvery diﬀerent nature.\n14Note that this also some kind of style transfer [DZX18], although of a high-level structure and not a low-level timbre as in\nSection 2.4.3.\n6\nFigure 5: Style transfer full architecture/process. Reproduced with permission of the authors\nGeneration is performed via constrained sampling, a mechanism to restrict the set of possible solutions in\nthe sampling process according to some pre-deﬁned constraints. The principle of the process (illustrated at\nFigure 6) is as follows. At ﬁrst, a sample is randomly initialized, following the standard uniform distribution.\nA step of constrained sampling is composed of n runs of gradient descent to impose the high-level structure,\nfollowed by p runs of selective Gibbs sampling to selectively realign the sample onto the learnt distribution. A\nsimulated annealing algorithm is applied in order to decrease exploration in relation to a decrease of variance\nover solutions.\nResults are quite convincing. However, as discussed by the authors, their approach is not exact, as for\ninstance by the Markov constraints approach proposed in [PRB11].\n2.5\nReinforcement\nThe strategy of reinforcement is to reformulate the generation of musical content as a reinforcement learn-\ning problem, while using the output of a trained recurrent network as an objective and adding user deﬁned\nconstraints, e.g., some tonality rules according to music theory, as an additional objective.\nLet us at ﬁrst quickly remind the basic concepts of reinforcement learning, illustrated at Figure 7:\n• An agent sequentially selects and performs actions within an environment;\n• Each action performed brings it to a new state,\n• with the feedback (by the environment) of a reward (reinforcement signal), which represents some adequa-\ntion of the action to the environment (the situation).\n• The objective of reinforcement learning is for the agent to learn a near optimal policy (sequence of actions)\nin order to maximize its cumulated rewards (named its gain).\nGeneration of a melody may be formulated as follows (as in Figure 8): the state s represents the musical\ncontent (a partial melody) generated so far and the action a represents the selection of next note to be generated.\n2.5.1\nExample: RL-Tuner Melody Generation\nThe reinforcement strategy has been pioneered by the RL-Tuner architecture by Jaques et al. [JGTE16]. The\narchitecture, illustrated at Figure 8, consists in two reinforcement learning architectures, named Q Network and\n7\nFigure 6: C-RBM Architecture\nFigure 7: Reinforcement learning (Conceptual model) – Reproduced from [DU05]\n8\nFigure 8: RL-Tuner architecture\nTarget Q Network15 and two recurrent network (RNN) architectures, named Note RNN and Reward RNN.\nAfter training Note RNN on the corpus, a ﬁxed copy named Reward RNN is used as a reference for the\nreinforcement learning architecture. The reward r of Q Network is deﬁned as a combination of two objectives:\n• Adherence to what has been learnt, by measuring the similarity of the action selected (next note to be\ngenerated) to the note predicted by Reward RNN in a similar state (partial melody generated so far);\n• Adherence to user-deﬁned constraints (e.g., consistency with current tonality, avoidance of excessive rep-\netitions. . . ), by measuring how well they are fulﬁlled.\nAlthough preliminary, results are convincing. Note that this strategy has the potential for adaptive genera-\ntion by incorporating feedback from the user.\n2.6\nUnit Selection\nThe unit selection strategy relies in querying successive musical units (e.g., a melody within a measure) from a\ndata base and in concatenating them in order to generate some sequence according to some user characteristics.\n2.6.1\nExample: Unit Selection and Concatenation Melody Generation\nThis strategy has been pioneered by Bretan et al. [BWH16] and is actually inspired by a technique commonly\nused in text-to-speech (TTS) systems and adapted in order to generate melodies (the corpus used is diverse and\nincludes jazz, folk and rock). The key process here is unit selection (in general each unit is one measure long),\nbased on two criteria: semantic relevance and concatenation cost. The architecture includes one autoencoder\nand two LSTM recurrent networks.\nThe ﬁrst preparation phase is feature extraction of musical units. 10 manually handcrafted features are\nconsidered, following a bag-of-words (BOW) approach (e.g., counts of a certain pitch class, counts of a certain\npitch class rhythm tuple, if ﬁrst note is tied to previous measure, etc.), resulting in 9,675 actual features.\nThe key of the generation is the process of selection of a best (or at least, very good) successor candidate to\na given musical unit. Two criteria are considered:\n• Successor semantic relevance – It is based on a model of transition between units, as learnt by a LSTM\nrecurrent network. In other words, that relevance is based on the distance to the (ideal) next unit as\npredicted by the model;\n• Concatenation cost – It is based on another model of transition16, this time between the last note of the\nunit and the ﬁrst note of the next unit, as learnt by another LSTM recurrent network.\nThe combination of the two criteria (illustrated at Figure 9) is handled by a heuristic-based dynamic ranking\nprocess. As for a recurrent network, generation is iterated in order to create, unit by unit (measure by measure),\nan arbitrary length melody.\n15They use a deep learning implementation of the Q-learning algorithm. Q Network is trained in parallel to Target Q Network\nwhich estimates the value of the gain) [vHGS15].\n16At a more ﬁne-grained level, note-to-note level, than the previous one.\n9\nFigure 9: Unit selection based on semantic cost\nNote that the unit selection strategy actually provides entry points for control, as one may extend the\nselection framework based on two criteria: successor semantic relevance and concatenation cost with user\ndeﬁned constraints/criteria.\n3\nStructure\nAnother challenge is that most existing systems have a tendency to generate music with “no sense of direction”.\nIn other words, although the style of the generated music corresponds to the corpus learnt, the music lacks\nsome structure and appears to wander without some higher organization, as opposed to human composed music\nwhich usually exhibits some global organization (usually named a form) and identiﬁed components, such as:\n• Overture, Allegro, Adagio or Finale for classical music;\n• AABA or AAB in Jazz;\n• Refrain, Verse or Bridge for songs.\nNote that there are various possible levels of structure. For instance, an example of ﬁner grain structure is\nat the level of melodic patterns that can be repeated, often transposed in order to adapt to a new harmonic\nstructure.\nReinforcement (as used by RL-Tuner at Section 2.5.1) and structure imposition (as used by C-RBM at\nSection 2.4.4) are approaches to enforce some constraints, possibly high-level, onto the generation. An alternative\ntop-down approach is followed by the unit selection strategy (see Section 2.6), by incrementally generating an\nabstract sequence structure and ﬁlling it with musical units, although the structure is currently ﬂat. Therefore,\na natural direction is to explicitly consider and process diﬀerent levels (hierarchies) of temporality and of\nstructure.\n3.1\nExample: MusicVAE Multivoice Generation\nRoberts et al. propose a hierarchical architecture named MusicVAE [RER+18b] following the principles of a\nvariational autoencoder encapsulating recurrent networks (RNNs, in practice LSTMs) such as VRAE introduced\nat Section 2.4.2, with two diﬀerences:\n• the encoder is a bidirectional RNN;\n• the decoder is a hierarchical 2-level RNN composed of:\n10\nFigure 10: MusicVAE architecture. Reproduced from [RER+18b] with permission of the authors\n– a high-level RNN named the Conductor producing a sequence of embeddings;\n– a bottom-layer RNN using each embedding as an initial state17 and also as an additional input\nconcatenated to its previously generated token to produce each subsequence.\nThe resulting architecture is illustrated at Figure 10. The authors report that an equivalent “ﬂat” (with-\nout hierarchy) architecture, although accurate in modeling the style in the case of 2-measure long examples,\nturned out inaccurate in the case of 16-measure long examples, with a 27% error increase for the autoencoder\nreconstruction. Some preliminary evaluation has also been conducted with a comparison by listeners of three\nversions: ﬂat architecture, hierarchical architecture and real music for three types of music: melody, trio and\ndrums, showing a very signiﬁcant gain with the hierarchical architecture.\n4\nCreativity\nThe issue of the creativity of the music generated is not only an artistic issue but also an economic one, because\nit raises a copyright issue18.\nOne approach is a posteriori, by ensuring that the generated music is not too similar (e.g., in not having\nrecopied a signiﬁcant amount of notes of a melody) to an existing piece of music. To this aim, existing tools to\ndetect similarities in texts may be used.\nAnother approach, more systematic but more challenging, is a priori, by ensuring that the music generated\nwill not recopy a given portion of music from the training corpus19. A solution for music generation from Markov\nchains has been proposed [PRP14]. It is based on a variable order Markov model and constraints over the order\nof the generation through some min order and max order constraints, in order to attain some sweet spot between\njunk and plagiarism. However, there is none yet equivalent solution for deep learning architectures.\n17In order to prioritize the Conductor RNN over the bottom layer RNN, its initial state is reinitialized with the decoder generated\nembedding for each new subsequence.\n18On this issue, see a recent paper [Del17].\n19Note that this addresses the issue of avoiding a signiﬁcant recopy from the training corpus, but it does not prevent to reinvent\nan existing music outside of the training corpus.\n11\nFigure 11: Creative adversarial networks (CAN) architecture\n4.1\nConditioning\n4.1.1\nExample: MidiNet Melody Generation\nThe MidiNet architecture by Yang et al.\n[YCY17], inspired by WaveNet (see Section 2.3.1), is based on\ngenerative adversarial networks (GAN) [GPAM+14] (see Section 4.2). It includes a conditioning mechanism\nincorporating history information (melody as well as chords) from previous measures. The authors discuss two\nmethods to control creativity:\n• by restricting the conditioning by inserting the conditioning data only in the intermediate convolution\nlayers of the generator architecture;\n• by decreasing the values of the two control parameters of feature matching regularization, in order to less\nenforce the distributions of real and generated data to be close.\nThese experiments are interesting although the approach remains at the level of some ad hoc tuning of some\nhyper-parameters of the architecture.\n4.2\nCreative Adversarial Networks\nAnother more systematic and conceptual direction is the concept of creative adversarial networks (CAN) pro-\nposed by El Gammal et al. [ELEM17], as an extension of generative adversarial networks (GAN) architecture,\nby Goodfellow et al. [GPAM+14] which trains simultaneously two networks:\n• a Generative model (or generator) G, whose objective is to transform random noise vectors into faked\nsamples, which resemble real samples drawn from a distribution of real images; and\n• a Discriminative model (or discriminator) D, that estimates the probability that a sample came from the\ntraining data rather than from G.\nThe generator is then able to produce user-appealing synthetic samples (e.g., images or music) from noise\nvectors. The discriminator may then be discarded.\nElgammal et al. propose in [ELEM17] to extend a GAN architecture into a creative adversarial networks\n(CAN) architecture, shown at Figure 11, where the generator receives from the discriminator not just one but\ntwo signals:\n• the ﬁrst signal, analog to the case of the standard GAN, speciﬁes how the discriminator believes that the\ngenerated item comes from the training dataset of real art pieces;\n• the second signal is about how easily the discriminator can classify the generated item into established\nstyles. If there is some strong ambiguity (i.e., the various classes are equiprobable), this means that the\ngenerated item is diﬃcult to ﬁt within the existing art styles.\nThese two signals are thus contradictory forces and push the generator to explore the space for generating\nitems that are at the same time close to the distribution of existing art pieces and with some style originality.\nNote that this approach assumes the existence of a prior style classiﬁcation and it also reduces the idea of\ncreativity to exploring new styles (which indeed has some grounding in the art history).\n12\nFigure 12: Strategies for instantiating notes during generation\n5\nInteractivity\nIn most of existing systems, the generation is automated, with little or no interactivity. As a result, local\nmodiﬁcation and regeneration of a musical content is usually not supported, the only available option being a\nwhole regeneration (and the loss of previous attempt). This is in contrast to the way a musician works, with\nsuccessive partial reﬁnement and adaptation of a composition20. Therefore, some requisites for interactivity\nare the incrementality and the locality of the generation, i.e. the way the variables of the musical content are\ninstantiated.\n5.1\nInstantiation Strategies\nLet us consider the example of the generation of a melody. The two most common strategies (illustrated at\nFigure 12)21 for instantiating the notes of the melody are:\n• Single-step/Global – A global representation including all time steps is generated in a single step by a\nfeedforward architecture. An example is DeepHear [Sun17] at Section 2.4.1.\n• Iterative/Time-slice – A time slice representation corresponding to a single time step is iteratively gener-\nated by a recurrent architecture (RNN). An example is Anticipation-RNN [HN17] at Section 2.3.2.\nLet us now consider an alternative strategy, incremental variable instantiation. It relies on a global represen-\ntation including all time steps. But, as opposed to single-step/global generation, generation is done incrementally\nby progressively instantiating and reﬁning values of variables (notes), in a non deterministic order. Thus, it is\npossible to generate or to regenerate only an arbitrary part of the musical content, for a speciﬁc time interval\nand/or for a speciﬁc subset of voices (shown as selective regeneration in Figure 12), without regenerating the\nwhole content.\n5.2\nExample: DeepBach Chorale Generation\nThis incremental instantiation strategy has been used by Hadjeres et al. in the DeepBach architecture [HPN17]\nfor generation of Bach chorales22. The architecture, shown at Figure 13, combines two recurrent and two feed-\nforward networks. As opposed to standard use of recurrent networks, where a single time direction is considered,\nDeepBach architecture considers the two directions forward in time and backwards in time. Therefore, two re-\ncurrent networks (more precisely, LSTM) are used, one summing up past information and another summing\nup information coming from the future, together with a non recurrent network for notes occurring at the same\n20An example of interactive composition environment is FlowComposer [PRP16].\nIt is based on various techniques such as\nMarkov models, constraint solving and rules.\n21The representation shown is of type piano roll with two simultaneous voices (tracks). Parts already processed are in light grey;\nparts being currently processed have a thick line and are pointed as “current”; notes to be played are in blue.\n22J. S. Bach chose various given melodies for a soprano and composed the three additional ones (for alto, tenor and bass) in a\ncounterpoint manner.\n13\nFigure 13: DeepBach architecture\nCreate four lists V = (V1; V2; V3; V4) of length L;\nInitialize them with random notes drawn from the ranges of the corresponding voices\nfor m from 1 to max number of iterations do\nChoose voice i uniformly between 1 and 4;\nChoose time t uniformly between 1 and L;\nRe-sample V t\ni from pi(V t\ni |V\\i,t, θi)\nend for\nFigure 14: DeepBach incremental generation/sampling algorithm\ntime. Their three outputs are merged and passed as the input of a ﬁnal feedforward neural network. The ﬁrst\n4 lines of the example data on top of the Figure 13 correspond to the 4 voices23. Actually this architecture is\nreplicated 4 times, one for each voice (4 in a chorale).\nTraining, as well as generation, is not done in the conventional way for neural networks. The objective is to\npredict the value of current note for a a given voice (shown with a red ? on top center of Figure 13), using as\ninformation surrounding contextual notes. The training set is formed on-line by repeatedly randomly selecting\na note in a voice from an example of the corpus and its surrounding context. Generation is done by sampling,\nusing a pseudo-Gibbs sampling incremental and iterative algorithm (shown in Figure 14, see details in [HPN17])\nto produce a set of values (each note) of a polyphony, following the distribution that the network has learnt.\nThe advantage of this method is that generation may be tailored. For example, if the user changes only\none or two measures of the soprano voice, he can resample only the corresponding counterpoint voices for these\nmeasures.\nThe user interface of DeepBach, shown at Figure 15, allows the user to interactively select and control global\nor partial (re)generation of chorales. It opens up new ways of composing Bach-like chorales for non experts in an\ninteractive manner, similarly to what is proposed by FlowComposer for lead sheets [PRP16]. It is implemented\nas a plugin for the MuseScore music editor.\n23The two bottom lines correspond to metadata (fermata and beat information), not detailed here.\n14\nFigure 15: DeepBach user interface\n6\nConclusion\nThe use of deep learning architectures and techniques for the generation of music (as well as other artistic\ncontent) is a growing area of research.\nHowever, there remain open challenges such as control, structure,\ncreativity and interactivity, that standard techniques do not directly address. In this article, we have discussed\na list of challenges, introduced some strategies to address them and have illustrated them through examples of\nactual architectures24. We hope that the analysis presented in this article will help at a better understanding of\nissues and possible solutions and therefore may contribute to the general research agenda of deep learning-based\nmusic generation.\nAcknowledgements\nWe thank Ga¨etan Hadjeres and Pierre Roy for related discussions. This research was partly conducted within\nthe Flow Machines project which received funding from the European Research Council under the European\nUnion Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n. 291156.\nReferences\n[BHP18]\nJean-Pierre Briot, Ga¨etan Hadjeres, and Fran¸cois Pachet. Deep Learning Techniques for Music\nGeneration. Computational Synthesis and Creative Systems. Springer Nature, 2018.\n[BWH16]\nMason Bretan, Gil Weinberg, and Larry Heck. A unit selection methodology for music generation\nusing deep neural networks, December 2016. arXiv:1612.03789v1.\n[CB98]\nYann Le Cun and Yoshua Bengio. Convolutional networks for images, speech, and time-series. In\nThe handbook of brain theory and neural networks, pages 255–258. MIT Press, Cambridge, MA,\nUSA, 1998.\n[Cop00]\nDavid Cope. The Algorithmic Composer. A-R Editions, 2000.\n[Del17]\nJean-Marc Deltorn. Deep creations: Intellectual property and the automata. Frontiers in Digital\nHumanities, 4, February 2017. Article 3.\n[DU05]\nKenji Doya and Eiji Uchibe. The Cyber Rodent project: Exploration of adaptive mechanisms for\nself-preservation and self-reproduction. Adaptive Behavior, 13(2):149–160, 2005.\n[DZX18]\nShuqi Dai, Zheng Zhang, and Gus Guangyu Xia. Music style transfer issues: A position paper,\nMarch 2018. arXiv:1803.06841v1.\n[Ebc88]\nKemal Ebcio˘glu. An expert system for harmonizing four-part chorales. Computer Music Journal\n(CMJ), 12(3):43–51, Autumn 1988.\n[ELEM17]\nAhmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, and Marian Mazzone.\nCAN: Creative\nadversarial networks generating “art” by learning about styles and deviating from style norms,\nJune 2017. arXiv:1706.07068v1.\n[FC16]\nRebecca Fiebrink and Baptiste Caramiaux. The machine learning algorithm as creative musical\ntool, November 2016. arXiv:1611.00379v1.\n24A more complete survey and analysis is [BHP18].\n15\n[FV13]\nJose David Fern´andez and Francisco Vico. AI methods in algorithmic composition: A compre-\nhensive survey. Journal of Artiﬁcial Intelligence Research (JAIR), (48):513–582, 2013.\n[FvA15]\nOtto Fabius and Joost R. van Amersfoort.\nVariational Recurrent Auto-Encoders, June 2015.\narXiv:1412.6581v6.\n[FYR16]\nDavis Foote, Daylen Yang, and Mostafa Rohaninejad. Audio style transfer – Do androids dream\nof electric beats?, December 2016. https://audiostyletransfer.wordpress.com.\n[GBC16]\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n[GEB15]\nLeon A. Gatys, Alexander S. Ecker, and Matthias Bethge. A neural algorithm of artistic style,\nSeptember 2015. arXiv:1508.06576v2.\n[GPAM+14] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-\njil Ozairy, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial nets, June 2014.\narXiv:1406.2661v1.\n[Gra14]\nAlex Graves. Generating sequences with recurrent neural networks, June 2014. arXiv:1308.0850v5.\n[HCC17]\nDorien Herremans, Ching-Hua Chuan, and Elaine Chew. A functional taxonomy of music gener-\nation systems. ACM Computing Surveys (CSUR), 50(5), September 2017.\n[HN17]\nGa¨etan Hadjeres and Frank Nielsen. Interactive music generation with positional constraints using\nAnticipation-RNN, September 2017. arXiv:1709.06404v1.\n[Hof01]\nDouglas Hofstadter. Staring Emmy straight in the eye–and doing my best not to ﬂinch. In David\nCope, editor, Virtual Music – Computer Synthesis of Musical Style, pages 33–82. MIT Press, 2001.\n[HOT06]\nGeoﬀrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural Computation, 18(7):1527–1554, July 2006.\n[HPN17]\nGa¨etan Hadjeres, Fran¸cois Pachet, and Frank Nielsen. DeepBach: a steerable model for Bach\nchorales generation, June 2017. arXiv:1612.01010v2.\n[HS97]\nSepp Hochreiter and J¨urgen Schmidhuber.\nLong short-term memory.\nNeural Computation,\n9(8):1735–1780, 1997.\n[JGTE16]\nNatasha Jaques, Shixiang Gu, Richard E. Turner, and Douglas Eck. Tuning recurrent neural\nnetworks with reinforcement learning, November 2016. arXiv:1611.02796.\n[KW14]\nDiederik\nP.\nKingma\nand\nMax\nWelling.\nAuto-encoding\nvariational\nBayes,\nMay\n2014.\narXiv:1312.6114v10.\n[LGW16]\nStefan Lattner, Maarten Grachten, and Gerhard Widmer.\nImposing higher-level structure in\npolyphonic music generation using convolutional restricted Boltzmann machines and constraints,\nDecember 2016. arXiv:1612.04742v2.\n[MKPKK17] Dimos Makris, Maximos Kaliakatsos-Papakostas, Ioannis Karydis, and Katia Lida Kermanidis.\nCombining LSTM and feed forward neural networks for conditional rhythm composition. In Gi-\nacomo Boracchi, Lazaros Iliadis, Chrisina Jayne, and Aristidis Likas, editors, Engineering Appli-\ncations of Neural Networks: 18th International Conference, EANN 2017, Athens, Greece, August\n25–27, 2017, Proceedings, pages 570–582. Springer Nature, 2017.\n[MOT15]\nAlexander\nMordvintsev,\nChristopher\nOlah,\nand\nMike\nTyka.\nDeep\nDream,\n2015.\nhttps://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html.\n[Nie09]\nGerhard Nierhaus.\nAlgorithmic Composition:\nParadigms of Automated Music Generation.\nSpringer Nature, 2009.\n[PPR17]\nFran¸cois Pachet, Alexandre Papadopoulos, and Pierre Roy. Sampling variations of sequences for\nstructured music generation. In Proceedings of the 18th International Society for Music Informa-\ntion Retrieval Conference (ISMIR 2017), Suzhou, China, October 23–27, 2017, pages 167–173,\n2017.\n[PRB11]\nFran¸cois Pachet, Pierre Roy, and Gabriele Barbieri. Finite-length markov processes with con-\nstraints. In Proceedings of the 22nd International Joint Conference on Artiﬁcial Intelligence (IJ-\nCAI 2011), pages 635–642, Barcelona, Spain, July 2011.\n16\n[PRP14]\nAlexandre Papadopoulos, Pierre Roy, and Fran¸cois Pachet. Avoiding plagiarism in Markov se-\nquence generation. In Proceedings of the 28th AAAI Conference on Artiﬁcial Intelligence (AAAI\n2014), pages 2731–2737, Qu´ebec, PQ, Canada, July 2014.\n[PRP16]\nAlexandre Papadopoulos, Pierre Roy, and Fran¸cois Pachet. Assisted lead sheet composition using\nFlowComposer. In Michel Rueher, editor, Principles and Practice of Constraint Programming:\n22nd International Conference, CP 2016, Toulouse, France, September 5-9, 2016, Proceedings,\npages 769–785. Springer Nature, 2016.\n[PW99]\nGeorge Papadopoulos and Geraint Wiggins. AI methods for algorithmic composition: A survey, a\ncritical view and future prospects. In AISB 1999 Symposium on Musical Creativity, pages 110–117,\nApril 1999.\n[RER+18a]\nAdam Roberts, Jesse Engel, Colin Raﬀel, Curtis Hawthorne, and Douglas Eck. A hierarchical\nlatent vector model for learning long-term structure in music, June 2018. arXiv:1803.05428v2.\n[RER+18b]\nAdam Roberts, Jesse Engel, Colin Raﬀel, Curtis Hawthorne, and Douglas Eck. A hierarchical\nlatent vector model for learning long-term structure in music.\nIn Proceedings of the 35th In-\nternational Conference on Machine Learning (ICML 2018). ACM, Montr´eal, PQ, Canada, July\n2018.\n[Ste84]\nMark Steedman. A generative grammar for Jazz chord sequences. Music Perception, 2(1):52–77,\n1984.\n[Sun17]\nFelix Sun. DeepHear – Composing and harmonizing music with neural networks, Accessed on\n21/12/2017. https://fephsun.github.io/2015/09/01/neural-music.html.\n[UL16]\nDmitry Ulyanov and Vadim Lebedev. Audio texture synthesis and style transfer, December 2016.\nhttps://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/.\n[vdODZ+16] A¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for\nraw audio, December 2016. arXiv:1609.03499v2.\n[vHGS15]\nHado van Hasselt, Arthur Guez, and David Silver.\nDeep reinforcement learning with double\nQ-learning, December 2015. arXiv:1509.06461v3.\n[YCY17]\nLi-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. MidiNet: A convolutional generative adversarial\nnetwork for symbolic-domain music generation. In Proceedings of the 18th International Society\nfor Music Information Retrieval Conference (ISMIR 2017), Suzhou, China, October 2017.\n17\n",
  "categories": [
    "cs.SD",
    "cs.LG",
    "eess.AS"
  ],
  "published": "2017-12-09",
  "updated": "2018-09-30"
}