{
  "id": "http://arxiv.org/abs/1712.04741v1",
  "title": "Mathematics of Deep Learning",
  "authors": [
    "Rene Vidal",
    "Joan Bruna",
    "Raja Giryes",
    "Stefano Soatto"
  ],
  "abstract": "Recently there has been a dramatic increase in the performance of recognition\nsystems due to the introduction of deep architectures for representation\nlearning and classification. However, the mathematical reasons for this success\nremain elusive. This tutorial will review recent work that aims to provide a\nmathematical justification for several properties of deep networks, such as\nglobal optimality, geometric stability, and invariance of the learned\nrepresentations.",
  "text": "Mathematics of Deep Learning\nRen´e Vidal\nJoan Bruna\nRaja Giryes\nStefano Soatto\nAbstract— Recently there has been a dramatic increase in the\nperformance of recognition systems due to the introduction of\ndeep architectures for representation learning and classiﬁcation.\nHowever, the mathematical reasons for this success remain\nelusive. This tutorial will review recent work that aims to\nprovide a mathematical justiﬁcation for several properties of\ndeep networks, such as global optimality, geometric stability,\nand invariance of the learned representations.\nI. INTRODUCTION\nDeep networks [1] are parametric models that perform se-\nquential operations on their input data. Each such operation,\ncolloquially called a “layer”, consists of a linear transfor-\nmation, say, a convolution of its input, followed by a point-\nwise nonlinear “activation function”, e.g., a sigmoid. Deep\nnetworks have recently led to dramatic improvements in\nclassiﬁcation performance in various applications in speech\nand natural language processing, and computer vision. The\ncrucial property of deep networks that is believed to be the\nroot of their performance is that they have a large number of\nlayers as compared to classical neural networks; but there are\nother architectural modiﬁcations such as rectiﬁed linear acti-\nvations (ReLUs) [2] and residual “shortcut” connections [3].\nOther major factors in their success is the availability of\nmassive datasets, say, millions of images in datasets like\nImageNet [4], and efﬁcient GPU computing hardware for\nsolving the resultant high-dimensional optimization problem\nwhich may have up to 100 million parameters.\nThe empirical success of deep learning, especially con-\nvolutional neural networks (CNNs) for image-based tasks,\npresents numerous puzzles to theoreticians. In particular,\nthere are three key factors in deep learning, namely the\narchitectures, regularization techniques and optimization al-\ngorithms, which are critical to train well-performing deep\nnetworks and understanding their necessity and interplay is\nessential if we are to unravel the secrets of their success.\nA. Approximation, depth, width and invariance properties\nAn important property in the design of a neural network\narchitecture is its ability to approximate arbitrary functions\nof the input. But how does this ability depend on parameters\nof the architecture, such as its depth and width? Earlier work\nshows that neural networks with a single hidden layer and\nR. Vidal is with the Center for Imaging Science, Biomedical Engineering,\nJohns Hopkins University, Baltimore, USA rvidal@cis.jhu.edu\nJ. Bruna is with the Courant Institute of Mathematical Sciences, Center\nfor Data Science, New York University, USA bruna@cims.nyu.edu\nR. Giryes is with the School of Electrical Engineering, Tel-Aviv Univer-\nsity, Tel Aviv, Israel raja@tauex.tau.ac.il\nS. Soatto is with the Department of Computer Science, University of\nCalifornia, Los Angeles, USA soatto@ucla.edu\nsigmoidal activations are universal function approximators\n[5], [6], [7], [8]. However, the capacity of a wide and shallow\nnetwork can be replicated by a deep network with signiﬁcant\nimprovements in performance. One possible explanation is\nthat deeper architectures are able to better capture invariant\nproperties of the data compared to their shallow counterparts.\nIn computer vision, for example, the category of an object\nis invariant to changes in viewpoint, illumination, etc. While\na mathematical analysis of why deep networks are able to\ncapture such invariances remains elusive, recent progress\nhas shed some light on this issue for certain sub-classes\nof deep networks. In particular, scattering networks [9] are\na class of deep networks whose convolutional ﬁlter banks\nare given by complex, multi-resolution wavelet families.\nAs a result of this extra structure, they are provably stable\nand locally invariant signal representations, and reveal the\nfundamental role of geometry and stability that underpins\nthe generalization performance of modern deep convolutional\nnetwork architectures; see Section IV.\nB. Generalization and regularization properties\nAnother critical property of a neural network architecture\nis its ability to generalize from a small number of training\nexamples. Traditional results from statistical learning theory\n[10] show that the number of training examples needed to\nachieve good generalization grows polynomially with the\nsize of the network. In practice, however, deep networks are\ntrained with much fewer data than the number of parameters\n(N ≪D regime) and yet they can be prevented from over-\nﬁtting using very simple (and seemingly counter-productive)\nregularization techniques like Dropout [11], which simply\nfreezes a random subset of the parameters at each iteration.\nOne possible explanation for this conundrum is that deeper\narchitectures produce an embedding of the input data that\napproximately preserves the distance between data points\nin the same class, while increasing the separation between\nclasses. This tutorial will overview the recent work of [12],\nwhich uses tools from compressed sensing and dictionary\nlearning to prove that deep networks with random Gaussian\nweights perform a distance-preserving embedding of the data\nin which similar inputs are likely to have a similar output.\nThese results provide insights into the metric learning prop-\nerties of the network and lead to bounds on the generalization\nerror that are informed by the structure of the input data.\nC. Information-theoretic properties\nAnother key property of a network architecture is its abil-\nity to produce a good “representation of the data”. Roughly\nspeaking, a representation is any function of the input data\narXiv:1712.04741v1  [cs.LG]  13 Dec 2017\nthat is useful for a task. An optimal representation would\nbe one that is “most useful” as quantiﬁed, for instance, by\ninformation-theoretic, complexity or invariance criteria [13].\nThis is akin to the “state” of the system and is what an agent\nwould store in its memory in lieu of the data to predict future\nobservations. For example, the state of a Kalman ﬁlter is\nan optimal representation for predicting data generated by a\nlinear dynamical system with Gaussian noise; in other words,\nit is a minimal sufﬁcient statistic for prediction. For complex\ntasks where the data may be corrupted by “nuisances”\nthat do not contain information about the task, one may\nalso wish for this representation to be “invariant” to such\nnuisances so as not to affect future predictions. In general,\noptimal representations for a task can be deﬁned as sufﬁcient\nstatistics (of past or “training” data) that are also minimal,\nand invariant to nuisance variability affecting future (“test”)\ndata [14]. Despite a large interest in representation learning, a\ncomprehensive theory that explains the performance of deep\nnetworks as constructing optimal representations does not yet\nexist. Indeed, even basic concepts such as sufﬁciency and in-\nvariance have received diverse treatment [9], [14], [15].\nRecent work [16], [17], [18] has begun to establish\ninformation-theoretic foundations for representations learned\nby deep networks. These include the observation that the\ninformation bottleneck loss [13], which deﬁnes a relaxed\nnotion of minimal sufﬁciency, can be used to compute\noptimal representations. The information bottleneck loss can\nbe re-written as the sum of a cross-entropy term, which is\nprecisely the most commonly used loss in deep learning,\nwith an additional regularization term. The latter can be im-\nplemented by introducing noise, similar to adaptive dropout\nnoise, in the learned representation [17]. The resulting form\nof regularization, called information dropout in [17], shows\nimproved learning under resource constraints, and can be\nshown to lead to “maximally disentangled” representations,\ni.e., the (total) correlation among components of the repre-\nsentation is minimal, thereby making the features indicators\nof independent characteristics of data. Moreover, similar\ntechniques show improved robustness to adversarial pertur-\nbations [18]. Information theory is hence expected to play a\nkey role in formalizing and analyzing the properties of deep\nrepresentations and suggesting new classes of regularizers.\nD. Optimization properties\nThe classical approach to training neural networks is to\nminimize a (regularized) loss using backpropagation [19],\na gradient descent method specialized to neural networks.\nModern versions of backpropagation rely on stochastic gradi-\nent descent (SGD) to efﬁciently approximate the gradient for\nmassive datasets. While SGD has been rigorously analyzed\nonly for convex loss functions [20], in deep learning the loss\nis a non-convex function of the network parameters, hence\nthere are no guarantees that SGD ﬁnds the global minimizer.\nIn practice, there is overwhelming evidence that SGD\nroutinely yields good solutions for deep networks. Recent\nwork on understanding the quality of training argues that\ncritical points are more likely to be saddle points rather\nx1\nx2\nx3\nx4\ny1\ny2\nHidden\nlayer\nInput\nlayer\nOutput\nlayer\nFig. 1.\nIllustration of a neural neural network with D = d1 = 4 inputs,\nd2 = 5 hidden layers, and C = d3 = 2 outputs. Here, the output can be\nwritten as y = (y1, y2) = ψ2(ψ1(xW 1)W 2), where x = (x1, . . . , x4)\nis the input, W 1 ∈R4×5 is the matrix of weights from the input layer to\nthe hidden layer, W 2 ∈R5×2 is the matrix of weights from the hidden to\nthe output layer, and ψ1 and ψ2 are activation functions.\nthan spurious local minima [21] and that local minima\nconcentrate near the global optimum [22]. Recent work has\nalso revealed that local minima discovered by SGD that\nlead to good generalization error belong to very ﬂat regions\nof the parameter space [23]. This motivates algorithms like\nEntropy-SGD that are specialized to ﬁnd such regions and\ndraw from similar results in the analysis of binary per-\nceptrons in statistical physics [24]; they have been shown\nto perform well on deep networks [25]. Surprisingly, these\ntechniques from statistical physics are intimately connected\nto the regularization properties of partial differential equa-\ntions (PDEs) [26]. For instance, local entropy, the loss that\nEntropy-SGD minimizes, is the solution of the Hamilton-\nJacobi-Bellman PDE and can therefore be written as a\nstochastic optimal control problem, which penalizes greedy\ngradient descent. This direction further leads to connections\nbetween variants of SGD with good empirical performance\nand standard methods in convex optimization such as inf-\nconvolutions and proximal methods. Researchers are only\nnow beginning to unravel the loss functions of deep networks\nin terms of their topology, which dictates the complexity of\noptimization, and their geometry, which seems to be related\nto generalization properties of the classiﬁers [27], [28], [29].\nThis tutorial will overview recent work showing that the\nerror surface for high-dimensional non-convex optimization\nproblems such as deep learning has some benign properties.\nFor example, the work of [30], [31] shows that for certain\nclasses of neural networks for which both the loss function\nand the regularizer are sums of positively homogeneous\nfunctions of the same degree, a local optimum such that many\nof its entries are zero is also a global optimum. These results\nwill also provide a possible explanation for the success of\nRELUs, which are positively homogeneous functions. Par-\nticular cases of this framework include, in addition to deep\nlearning, matrix factorization and tensor factorization [32].\nE. Paper outline\nThe remainder of this paper is organized as follows.\nSection II describes the input-output map of a deep network.\nSection III studies the problem of training deep networks\nand establishes conditions for global optimality. Section\nIV studies invariance and stability properties of scattering\nnetworks. Section V studies structural properties of deep\nnetworks, such as metric properties of the embedding as well\nas bounds on the generalization error. Section VI studies\ninformation-theoretic properties of deep representations.\nII. PRELIMINARIES\nA deep network is a hierarchical model where each layer\napplies a linear transformation followed by a non-linearity to\nthe preceding layer. Speciﬁcally, let X ∈RN×D be the input\ndata,1 where each row of X is a D-dimensional data point\n(e.g., a grayscale image with D pixels) and N is the number\nof training examples. Let W k ∈Rdk−1×dk be a matrix\nrepresenting a linear transformation applied to the output of\nlayer k −1, Xk−1 ∈RN×dk−1, to obtain a dk-dimensional\nrepresentation Xk−1W k ∈RN×dk at layer k. For example,\neach column of W k could represent a convolution with some\nﬁlter (as in convolutional neural networks) or the application\nof a linear classiﬁer (as in fully connected networks). Let ψk :\nR →R be a non-linear activation function, e.g., a hyperbolic\ntangent ψk(x) = tanh(x), a sigmoid ψk(x) = (1 + e−x)−1,\nor a rectiﬁed linear unit (ReLU) ψk(x) = max{0, x}.2 This\nnon-linearity is applied to each entry of Xk−1W k to generate\nthe kth layer of a neural network as Xk = ψk(Xk−1W k).\nThe output XK of the network is thus given by (see Fig. 1):\nΦ(X, W 1, . . . , W K) = ψK(ψK−1(· · ·\nψ2(ψ1(XW 1)W 2) · · · W K−1)W K).\n(1)\nNote that Φ is an N × C matrix, where C = dK is the\ndimension of the output of the network, which is equal to\nthe number of classes in the case of a classiﬁcation problem.\nNotice also that the map Φ can be seen as a function of the\nnetwork weights W = {W k}K\nk=1 with a ﬁxed input X, as\nwill be the case when we discuss the training problem in\nIII. Alternatively, we can view the map Φ as a function of\nthe input data X with ﬁxed weights W, as will be the case\nwhen we discuss properties of this map in Sections IV-VI.\nIII. GLOBAL OPTIMALITY IN DEEP LEARNING\nThis section studies the problem of learning the parameters\nW = {W k}K\nk=1 of a deep network from N training examples\n(X, Y ). In a classiﬁcation setting, each row of X ∈RN×D\ndenotes a data point in RD and each row of Y ∈{0, 1}N×C\ndenotes the membership of each data point to one out of C\nclasses, i.e., Yjc = 1 if the jth row of X belongs to class c ∈\n{1, . . . , C} and Yjc = 0 otherwise. In a regression setting,\nthe rows of Y ∈RN×C denote the dependent variables for\nthe rows of X. The problem of learning the network weights\nW is formulated as the following optimization problem:\nmin\n{W k}K\nk=1\nℓ(Y, Φ(X, W 1, . . . , W K)) + λΘ(W 1, . . . , W K),\n(2)\n1For the sake of simplicity, we assume that inputs lie in RD, but many\nof the results in this paper apply also to more general inputs such as tensors\n(e.g., RGB data), in which case the weights become tensors as well.\n2More broadly, ψk could be a many to one function, such as max pooling.\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\nFig. 2.\nExample critical points of a non-convex function (shown in red).\n(a,c) Plateaus. (b,d) Global minima. (e,g) Local maxima. (f,h) Local minima.\nwhere ℓ(Y, Φ) is a loss function that measures the agree-\nment between the true output, Y , and the predicted output,\nΦ(X, W) in (1), Θ is a regularization function designed to\nprevent overﬁtting, e.g., weight decay via ℓ2 regularization\nΘ(W)=PK\nk=1 ∥W k∥2\nF , and λ>0 is a balancing parameter.\nA. The challenge of non-convexity in neural network training\nA key challenge in neural network training is that the\noptimization problem in (2) is non-convex because, even if\nthe loss ℓ(Y, Φ) is typically a convex function of Φ, e.g.,\nthe squared loss ℓ(Y, Φ) = ∥Y −Φ∥2\nF , the map Φ(X, W) is\nusually a non-convex function of W due to the product of the\nW k variables and the nonlinearities ψk in (1). This presents\nsigniﬁcant challenges to existing optimization algorithms –\nincluding (but certainly not limited to) gradient descent,\nstochastic gradient descent, alternating minimization, block\ncoordinate descent, back-propagation, and quasi-Newton\nmethods – which are typically only guaranteed to converge to\na critical point of the objective function [33], [34], [35], [36].\nHowever, for non-convex problems, the set of critical points\nincludes not only global minima, but also local minima, local\nmaxima, saddle points and saddle plateaus, as illustrated\nin Fig. 2. As a result, the non-convexity of the problem\nleaves the model somewhat ill-posed in the sense that it is\nnot just the model formulation that is important but also\nimplementation details, such as how the model is initialized\nand particulars of the optimization algorithm, which can have\na signiﬁcant impact on the performance of the model.\nTo address the issue of non-convexity, a common strategy\nused in deep learning is to initialize the network weights\nat random, update these weights using local descent, check\nif the training error decreases sufﬁciently fast, and if not,\nchoose another initialization. In practice, it has been observed\nthat if the size of the network is large enough, this strategy\ncan lead to markedly different solutions for the network\nweights, which give nearly the same objective values and\nclassiﬁcation performance [22]. It has also been observed\nthat when the size of the network is large enough and\nthe non-linearity is chosen to be a ReLU, many weights\nare zero, a phenomenon known as dead neurons, and the\nclassiﬁcation performance signiﬁcantly improves [37], [38],\n[39], [40]. While this empirically suggests that when the size\nof the network is large enough and ReLU non-linearities are\nused all local minima could be global, there is currently\nno rigorous theory that provides a precise mathematical\nexplanation for these experimentally observed phenomena.\nB. Optimality for neural networks with a single hidden layer\nEarlier work on global optimality for neural networks\n[41] showed that for networks with a single hidden layer\n(i)\nFig. 3. Illustration of the properties of the framework of [30], [31]. Starting\nfrom any initialization, a non-increasing path exists to a global minimizer.\nStarting from points on a plateau, a simple “sliding” method exists to ﬁnd\nthe edge of the plateau (green points).\nand linear activations, the squared loss has a unique global\nminimum and all other critical points are saddle points. How-\never, when the activations are nonlinear, [42] gives examples\nof networks where the backpropagation algorithm [19] fails\neven with separable data. The examples are, however, not\ngeneric, and [43], [44] show that backpropagation generally\nﬁnds a global minimizer for linearly separable data.\nLater work [45] showed that for neural networks with a\nsingle hidden layer, if the number of neurons in the hidden\nlayer is not ﬁxed, but instead ﬁt to the data through a\nsparsity inducing regularization, then the process of training\na globally optimal neural network is analogous to selecting\na ﬁnite number of hidden units from a potentially inﬁnite\ndimensional space of all possible hidden units. A weighted\nsum of the selected hidden units is then taken to produce the\noutput. The speciﬁc optimization problem is of the form\nmin\nw ℓ(Y,\nX\ni\nhi(X)wi) + λ∥w∥1,\n(3)\nwhere hi(X) represents one possible hidden unit activa-\ntion in response to the training data X from an inﬁnite\ndimensional space hi(X) ∈H of all possible hidden unit\nactivations. Clearly (3) is a convex optimization problem on\nw (assuming ℓ(Y, w) is convex on w) and straightforward to\nsolve for a ﬁnite set of hi(X) activations. However, since\nH is an inﬁnite dimensional space the primary difﬁculty\nlies in how to select the appropriate hidden unit activations.\nNonetheless, by using arguments from gradient boosting, it is\npossible to show that problem (3) can be globally optimized\nby sequentially adding hidden units to the network until one\ncan no longer ﬁnd a hidden unit whose addition will decrease\nthe objective function [45], [46], [47].\nC. Optimality for networks with random inputs and weights\nRecent work has analyzed the problem of training neural\nnetworks with random inputs and weights. For example,\nthe authors of [48] study random networks with a single\nhidden layer. Rather than training the network by solving\nan optimization problem such as (2), the authors use tensor\ndecomposition methods to estimate the network weights from\nhigh order statistical moments of the network mapping.\nMoreover, the authors show that under certain assumptions\non the loss and data distribution, polynomial-time training is\npossible [48]. Further, the authors of [49] study the problem\nwhen a given initialization of a neural network is likely to\nbe within the basin of attraction of a global minimizer and\nprovide conditions that ensure a random initialization will be\nwithin the basin of a global minimizer with high probability.\nSeveral recent works have also analyzed the error surface\nof multilayer neural networks using tools from random ma-\ntrix theory and statistical physics. For example, the authors of\n[21] argue that, under certain assumptions, it is vastly more\nlikely that a critical point of a high-dimensional optimization\nproblem be a saddle point rather than a local minimizer.\nTherefore, avoiding saddle points is the key difﬁculty in\nhigh-dimensional, non-convex optimization. Likewise, the\nauthors of [22] show that, under certain assumptions on the\ndistributions of the training data and the network weight\nparameters, as the number of hidden units in a network\nincreases, the distribution of local minima becomes increas-\ningly concentrated in a small band of objective function\nvalues near the global optimum (and thus all local minima\nbecome increasingly close to being global minima).\nD. Global optimality for positively homogeneous networks\nRecent work [30], [31] largely echoes ideas from the above\nwork, but takes a markedly different approach. Speciﬁcally,\n[30], [31] analyzes the optimization problem in (2) using\na purely deterministic approach which does not make any\nassumptions regarding the distribution of the input data, the\nnetwork weight parameter statistics, or the network initializa-\ntion. With this approach, [30], [31] shows that saddle points\nand plateaus are the only critical points that one needs to be\nconcerned with due to the fact that for networks of sufﬁcient\nsize, local minima that require one to climb the objective\nsurface to escape from, such as (f) and (h) in Fig. 2, are\nguaranteed not to exist.\nMore speciﬁcally, [30] studies conditions under which\nthe optimization landscape for the non-convex optimization\nproblem in (2) is such that all critical points are either global\nminimizers or saddle points/plateaus, as shown in Fig. 3. The\nauthors show that if the network size is large enough and\nthe functions Φ and Θ are sums of positively homogeneous\nfunctions of the same degree, any local minimizer such that\nsome of its entries are zero is also a global minimizer.\nInterestingly, ReLU and max-pooling non-linearities are pos-\nitively homogeneous, while sigmoids are not, which could\nprovide a possible explanation for the improved performance\nof ReLU and max pooling. Furthermore, many state-of-the-\nart networks are not trained with classical regularization,\nsuch as an ℓ1 or ℓ2 norm penalty on the weight parameters\nbut instead rely on techniques such as dropout [11]. The\nresults of [30], [31] also provide strong guidance on the\ndesign of network regularization to ensure the non-existence\nof spurious local minima, showing that traditional weight\ndecay is not appropriate for deep networks. However, more\nrecently proposed forms of regularization such as Path-SGD\n[50] or batch normalization [51] can be easily incorporated\ninto the analysis framework of [30], [31], and stochastic\nregularization methods, such as dropout [11], have strong\nsimilarities to their framework.\nIV. GEOMETRIC STABILITY IN DEEP LEARNING\nAn important question in the path towards understanding\ndeep learning models is to mathematically characterize its in-\nductive bias; i.e., deﬁne the class of regression/classiﬁcation\ntasks for which they are predesigned to perform well, or at\nleast better than classical alternatives.\nIn the particular case of computer vision tasks, convolu-\ntional archictectures provide a fundamental inductive bias at\nthe origin of most successful deep learning vision models. As\nwe explain next, the notion of geometric stability provides a\npossible framework to understand its success.\nLet Ω= [0, 1]d ⊂Rd be a compact d-dimensional\nEuclidean domain on which square-integrable functions X ∈\nL2(Ω) are deﬁned (for example, in image analysis applica-\ntions, images can be thought of as functions on the unit\nsquare Ω= [0, 1]2). In a supervised learning task, an un-\nknown function f : L2(Ω) →Y is observed on a training set\n{Xi ∈L2(Ω), Yi = f(Xi)}i∈I ,\n(4)\nwhere the target space Y can be thought as being discrete\nin a standard classiﬁcation setup (with C = |Y| being the\nnumber of classes), or Y = RC in a regression task.\nIn the vast majority of computer vision and speech analysis\ntasks, the unknown function f typically satisﬁes the follow-\ning crucial assumptions:\n1) Stationarity: Consider a translation operator3\nTvX(u) = X(u −v),\nu, v ∈Ω,\n(5)\nacting on functions X ∈L2(Ω). Depending on the task, we\nassume that the function f is either invariant or equivariant\nwith respect to translations. In the former case, we have\nf(TvX) = f(X) for any X\n∈L2(Ω) and v\n∈Ω.\nThis is typically the case in object classiﬁcation tasks. In\nthe latter, we have f(TvX) = Tvf(X), which is well-\ndeﬁned when the output of the model is a space in which\ntranslations can act upon (for example, in problems of object\nlocalization, semantic segmentation, or motion estimation).\nOur deﬁnition of invariance should not be confused with the\ntraditional notion of translation invariant systems in signal\nprocessing, which corresponds to translation equivariance in\nour language (since the output translates whenever the input\ntranslates).\n2) Local deformations and scale separation: Similarly, a\ndeformation Lτ, where τ : Ω→Ωis a smooth vector ﬁeld,\nacts on L2(Ω) as LτX(u) = X(u −τ(u)). Deformations\ncan model local translations, changes in viewpoint, rotations\nand frequency transpositions [9]. Most tasks studied in\ncomputer vision are not only translation invariant/equivariant,\nbut, more importantly, also stable with respect to local\ndeformations [52], [9]. In tasks that are translation invariant\nwe have\n|f(LτX) −f(X)| ≈∥∇τ∥,\n(6)\nfor all X, τ, where ∥∇τ∥measures the smoothness of a given\ndeformation ﬁeld. In other words, the quantity to be predicted\n3 We assume periodic boundary conditions to ensure that the operation\nis well-deﬁned over L2(Ω).\ndoes not change much if the input image is slightly deformed.\nIn tasks that are translation equivariant, we have\n|f(LτX) −Lτf(X)| ≈∥∇τ∥.\n(7)\nThis property is much stronger than stationarity, since the\nspace of local deformations has high dimensionality – of the\norder of RD when we discretize images with D pixels, as\nopposed to the d-dimensional translation group, which has\nonly d = 2 dimensions in the case of images.\nAssumptions (6)–(7) can be leveraged to approximate f\nfrom features Φ(X) that progressively reduce the spatial\nresolution. Indeed, extracting, demodulating and downsam-\npling localized ﬁlter responses creates local summaries that\nare insensitive to local translations, but this loss of loss\nof sensitivity does not affect our ability to approximate f,\nthanks to (6)–(7). To illustrate this principle, denote by\nZ(a1, a2; v) = Prob(X(u) = a1 and X(u+v) = a2) (8)\nthe joint distribution of two image pixels at an offset v\nfrom each other. In the presence of long-range statistical\ndependencies, this joint distribution will not be separable for\nany v. However, the deformation stability prior states that\nZ(a1, a2; v) ≈Z(a1, a2; v(1+ϵ)) for small ϵ. In other words,\nwhereas long-range dependencies indeed exist in natural\nimages and are critical to object recognition, they can be\ncaptured and down-sampled at different scales. Although this\nprinciple of stability to local deformations has been exploited\nin the computer vision community in models other than\nCNNs, for instance, deformable parts models [53], CNNs\nstrike a good balance in terms of approximation power,\noptimization, and invariance.\nIndeed, stationarity and stability to local translations are\nboth leveraged in convolutional neural networks (CNN). A\nCNN consists of several convolutional layers of the form\n˜X = CW (X), acting on a p-dimensional input X(u) =\n(X1(u), . . . , Xp(u)) by applying a bank of ﬁlters W =\n(wl,l′), l = 1, . . . , q, l′ = 1, . . . , p and point-wise non-\nlinearity ψ,\n˜Xl(u)\n=\nψ\n \np\nX\nl′=1\n(Xl′ ⋆wl,l′)(u)\n!\n,\n(9)\nproducing\na\nq-dimensional\noutput\n˜X(u)\n=\n( ˜X1(u), . . . , ˜Xq(u))\noften\nreferred\nto\nas\nthe\nfeature\nmaps. Here,\n(X ⋆w)(u) =\nZ\nΩ\nX(u −u′)w(u′)du′\n(10)\ndenotes the standard convolution. According to the local\ndeformation prior, the ﬁlters W have compact spatial support.\nAdditionally, a downsampling or pooling layer ˜X = P(X)\nmay be used, deﬁned as\n˜Xl(u) = P({Xl(u′) : u′ ∈N(u)}), l = 1, . . . , q,\n(11)\nwhere N(u) ⊂Ωis a neighborhood around u and P is a\npermutation-invariant function such as an average-, energy-,\nor max-pooling).\nA convolutional network is constructed by composing sev-\neral convolutional and optionally pooling layers, obtaining a\ngeneric hierarchical representation\nΦW (X) = (CW (K) · · · P · · · ◦CW (2) ◦CW (1))(X),\n(12)\nwhere W = {W (1), . . . , W (K)} is the hyper-vector of the\nnetwork parameters (all the ﬁlter coefﬁcients). The output\nfeatures enjoy translation invariance/covariance depending\non whether spatial resolution is progressively lost by means\nof pooling or kept ﬁxed. Moreover, if one speciﬁes the convo-\nlutional tensors to be complex wavelet decomposition opera-\ntors and uses complex modulus as point-wise nonlinearities,\none can provably obtain stability to local deformations [54].\nAlthough this stability is not rigorously proved for generic\ncompactly supported convolutional tensors, it underpins the\nempirical success of CNN architectures across a variety of\ncomputer vision applications [1].\nA key advantage of CNNs explaining their success in\nnumerous tasks is that the geometric priors on which CNNs\nare based result in a sample complexity that avoids the\ncurse of dimensionality. Thanks to the stationarity and local\ndeformation priors, the linear operators at each layer have a\nconstant number of parameters, independent of the input size\nD (number of pixels in an image). Moreover, thanks to the\nmultiscale hierarchical property, the number of layers grows\nat a rate O(log D), resulting in a total learning complexity\nof O(log D) parameters.\nFinally, recently there has been an effort to extend the\ngeometric stability priors to data that is not deﬁned over an\nEuclidean domain, where the translation group is generally\nnot deﬁned. In particular, researchers are exploiting geometry\nin general graphs via the spectrum of graph Laplacians and\nits spatial counterparts; see [55] for a recent survey on those\nadvances.\nV. STRUCTURE BASED THEORY FOR DEEP LEARNING\nA. Structure of the data throughout a neural network\nAn important aspect for understanding better deep learning\nis the relationship between the structure of the data and the\ndeep network. For a formal analysis, consider the case of a\nnetwork that has random i.i.d. Gaussian weights, which is\na common initialization in training deep networks. Recent\nwork [56] shows that such networks with random weights\npreserve the metric structure of the data as they propagate\nalong the layers, allowing for stable recovery of the original\ndata from the features calculated by the network – a property\nthat is often encountered in general deep networks [57], [58].\nMore precisely, the work of [56] shows that the input to\nthe network can be recovered from the network’s features\nat a certain layer if their size is proportional to the intrinsic\ndimension of the input data. This is similar to data recon-\nstruction from a small number of random projections [59],\n[60]. However, while random projections preserve the Eu-\nclidean distance between two inputs up to a small distortion,\neach layer of a deep network with random weights distorts\nthis distance proportionally to the angle between the two\ninputs: the smaller the angle, the stronger the shrinkage of\nthe distance. Therefore, the deeper the network, the stronger\nthe shrinkage achieved. Note that this does not contradict\nthe fact that it is possible to recover the input from the\noutput; even when properties such as lighting, pose and\nlocation are removed from an image (up to certain extent),\nthe resemblance to the original image is still maintained.\nAs random projection is a universal sampling strategy for\nlow-dimensional data [59], [60], [61], deep networks with\nrandom weights are a universal system that separates any\ndata (belonging to a low-dimensional model) according to the\nangles between the data points, where the general assumption\nis that there are large angles between different classes [62],\n[63]. As the training of the projection matrix adapts it to\nbetter preserve speciﬁc distances over others, the training of\na network prioritizes intra-class angles over inter-class ones.\nThis relation is alluded to by the proof techniques in [56]\nand is empirically manifested by observing the angles and\nEuclidean distances at the output of trained networks.\nBy using the theory of 1-bit compressed sensing, [64]\nshows that each layer of a network preserves the metric\nof its input data in the Gromov-Hausdorff sense up to a\nsmall constant δ, under the assumption that these data reside\nin a low-dimensional manifold denoted by K. This allows\ndrawing conclusions on the tessellation of the space created\nby each layer and the relationship between the operation\nof these layers and local sensitive hashing (LSH). It also\nimplies that it is possible to retrieve the input of a layer, up\nto certain accuracy, from its output. This shows that every\nlayer preserves the important information of the data.\nAn analysis of the behavior of the Euclidean distances and\nangles in the data along the network reveals an important\neffect of the ReLU. Without a non-linearity, each layer\nis simply a random projection for which Euclidean dis-\ntances are approximately preserved. The addition of a ReLU\nmakes the system sensitive to the angles between points.\nIn this case, the network tends to decrease the Euclidean\ndistances between points with small angles between them\n(“same class”), more than the distances between points with\nlarge angles between them (“different classes”). Still, low-\ndimensional data at the input remain such throughout the\nentire network, i.e., deep networks (almost) do not increase\nthe intrinsic dimension of the data [56]. This is related to\nthe recent work in [65] that claims that deep networks may\nbe viewed a sparse coding procedure leading to guarantees\non the uniqueness of the representation calculated by the\nnetwork and its stability.\nAs random networks are blind to the data labels, training\nmay select discriminatively the angles that cause the dis-\ntance deformation. Therefore, it will cause distances between\ndifferent classes to increase more than the distances within\nthe same class. This is demonstrated in several simulations\nfor different deep networks. It is observed that a potential\nmain goal of the training of the network is to treat the\nclass boundary points while keeping the other distances\napproximately the same.\nB. Generalization error\nThe above suggests that there is a relation between the\nstructure of the data and the error the network achieves in\ntraining, which leads to study the relationship between the\ngeneralization error in deep networks and the data structure.\nGeneralization error – the difference between the empirical\nerror and the expected error – is a fundamental concept in\nstatistical learning theory. Generalization error bounds offer\ninsight into why learning from training samples is possible.\nConsider a classiﬁcation problem with a data point X ∈\nX\n⊆\nRD that has a corresponding class label Y\n∈\nY, where C is the number of classes. A training set of\nN samples drawn from a distribution P is denoted by\nΥN = {(Xi, Yi)}N\ni=1 and the loss function is denoted by\nℓ(Y, Φ(X, W)), which measures the discrepancy between\nthe true label Y and the estimated label Φ(X, W) provided\nby the classiﬁer. The empirical loss of a network Φ(·, W)\nassociated with the training set ΥN is deﬁned as\nℓemp(Φ) = 1\nN\nX\nXi∈ΥN\nℓ(Yi, Φ(Xi, W)) ,\n(13)\nand the expected loss as\nℓexp(Φ) = E(X,Y )∼P [ℓ(Y, Φ(X, W))] .\n(14)\nThe generalization error is then given as:\nGE(Φ) = |ℓexp(Φ) −ℓemp(Φ)| .\n(15)\nVarious measures such as the the VC-dimension [66], [67],\nthe Rademacher or Gaussian complexities [68] and algorithm\nrobustness [69] have been used to bound the GE in the\ncontext of deep networks. However, these measures do not\noffer an explanation for good deep network generalization in\npractice where the number of parameters can often be larger\nthan the number of training samples [70] or the networks are\nvery deep [71]. For example, the VC-dimension of a deep\nnetwork with the hard-threshold non-linearity is equal to the\nnumber of parameters in the network, which implies that\nthe sample complexity is linear in the number of parameters\nof the network. On the other hand, the work [72] bounds\nthe generalization error independently of the number of\nparameters. Yet, in its bound the generalization error of deep\nnetwork with ReLUs scales exponentially with the network\ndepth. Similarly, the authors in [69] show that deep networks\nare robust provided that the ℓ1-norm of the weights in each\nlayer is bounded. The bounds are exponential in the ℓ1-norm\nof the weights if the norm is greater than 1.\nAn alternative route followed by [28], [29], [69], [73]\nbounds the generalization error in terms of the networks’\nclassiﬁcation margin, which is independent of the depth\nand size but takes into account the structure of the data\n(considering its covering number) and therefore avoids the\nabove issues. As it is hard to calculate the input margin\ndirectly, in [28] it is tied to the Jacobian matrix and the\nloss of the deep networks showing that bounding the spectral\nnorm of the Jacobian matrix reduces the generalization error.\nThis analysis is general to arbitrary network structures, types\nTABLE I\nCLASSIFICATION ACC. [%] OF RESNET CIFAR-10\n# train samples\nResNet\nResNet + Jac. reg.\n2500\n55.69\n62.79\n10000\n71.79\n78.70\n50000 + aug.\n93.34\n94.32\nof non-linearities and pooling operations. Furthermore, a\nrelationship between the generalization error, the invariance\nin the data and the network is formally characterized in [29].\nUsing the relationship between the generalization error and\nthe network Jacobian matrix, a new Jacobian based regu-\nlarization strategy is developed in [29] and its advantage is\ndemonstrated for several networks and datasets. For example,\nTable I shows the improvement achieved when using this\nregularization with the Wide ResNet architecture [74] for\nCIFAR-10 with different numbers of training examples.\nA related theory to the one presented above is the one\nin [50], [75]. These works study the relationship between\nthe generalization error and the minimization of the network\nloss using SGD. They provide modiﬁcations for SGD that\nimproves the error achieved by the network.\nVI. TOWARDS AN INFORMATION-THEORETIC\nFRAMEWORK\nThe loss function of choice for training deep networks\nto solve supervised classiﬁcation problems is the empirical\ncross-entropy\n˜ℓ(W) = EP (X,Y )(−log Φ(X, W)),\n(16)\nThis loss function is prone to overﬁtting, as the network\ncould trivially memorize the training data instead of learn-\ning the underlying distribution. This problem is usually\naddressed by regularization, which can be explicit (e.g., the\nnorm of W, known as weight decay), or implicit in stochastic\ngradient descent. It was suggested by [76] almost a quarter\ncentury ago that better regularization, hence less overﬁtting,\nmight be achieved by limiting the information stored in the\nweight, KL( p(W|X, Y ) ∥p(W) ), where p(W) is a prior\non the weights. Choosing this regularizer leads to the loss\nfunction\nℓ(W) = H(Y |X, W) + λ KL( p(W|X, Y ) ∥p(W) )\n(17)\nwhere the ﬁrst term denotes the empirical conditional cross-\nentropy obtained from ˜ℓ(W). For λ\n=\n1, this is the\nvariational lower bound on the observed data distribution\npθ(Y |X), and can therefore be seen as a form of Bayesian\ninference of the weights. More generally, this is equivalent\nto the information bottleneck Lagrangian. The ﬁrst term\nis the same as the empirical cross-entropy and ensures\nthat information stored in the weights is sufﬁcient for the\ntask Y , while the second term minimizes the amount of\ninformation stored. Thus, the weights learned by minimizing\ncross-entropy, with a KL regularizer, approximate a minimal\nsufﬁcient statistic of the training set. Computing the KL\nterm and optimizing ℓwas considered a show-stopper until\nrecently, when advances in Stochastic Gradient Variational\nBayes [77], [78] made efﬁcient optimization possible.\nBut for a representation to be useful, it should not just\nefﬁciently memorize past (training) data. It should also\nreduce the effect of nuisance variability affecting future (test)\ndata. Indeed, most of the variability in imaging data can\nbe ascribed to the effects of illumination and viewpoint,\nquotienting out which leaves a thin set [79]. As we have\nalready pointed out, deep networks are known to reduce\nthe effects of nuisance variability in test data. This can be\npartly achieved through the architecture, in particular multi-\nscale convolutional structures. Some may be ascribed to the\noptimization procedure, that converges to “ﬂat minima.” But\nthe choice of regularizer is also responsible for the networks’\nability to discount nuisance variability. Denoting by X be\nthe input sample, Y the target variable, and Z ∼p(Z|X)\nthe (stochastic) representation of X learned by a layer in the\nnetwork, the tradeoff between sufﬁciency an minimality of\nZ is formalized by the information bottleneck Lagrangian\nℓ(W) = H(Y | Z, W) + λI(Z; X),\n(18)\nwhere the ﬁrst term ensures that the representation Z is suf-\nﬁcient for Z, while the second term ensures its information\ncontent remains minimal, i.e. that nuisances are ﬁltered out.\nNotice that, while formally equivalent, the losses in (17)\nand (18) are conceptually different: In the former, the weights\nare a representation of the training set that is minimal\nand sufﬁcient. In the latter, the activations are a minimal\nrepresentation of the test sample. We conjecture that the two\nare related, and that the relation informs the generalization\nproperties of the network, but speciﬁc bounds have yet been\nshown.\nDifferent choices of the noise distribution, and different\nmodels of the marginal distribution p(Z) lead to slightly\ndifferent analyses. For example, [18] considers the case\nof additive Gaussian noise and Gaussian marginals, while\n[17] study multiplicative noise distributions, and considers\nboth a scale invariant log-uniform marginal (the only one\ncompatible with the fact that networks with ReLU activations\nare scale invariant), and a log-normal marginal distribution.\nInterestingly, the special case in which the multiplicative\nnoise is chosen from a Bernoulli distribution reduces to\nthe well-known practice of Dropout [76], while choosing\nfrom a multiplicative Gaussian distribution leads to Gaussian\nDropout [78].\nIn order to compute the term I(Z; X), it is commonly\nassumed that the activations are mutually independent, that\nis, that the marginal p(Z) is factorized; [17] shows that\nmaking this assumption corresponds to minimizing a mod-\niﬁed loss function, which also reduces the total correlation\nTC(Z) of the activations. Therefore, a choice dictated by\nconvenience in order to explicitly compute the information\nbottleneck Lagrangian, yields a disentangled representation,\nwith entanglement measured by total correlation.\nIt is remarkable that empirical practice has managed to\nconverge to the use of the cross-entropy loss with dropout,\nthat happens to be what would have been prescribed by ﬁrst\nprinciples, since for certain choices of distribution, training\nis equivalent to minimizing the information bottleneck La-\ngrangian, that yields an approximation of a minimal sufﬁcient\ninvariant statistic, which deﬁne an optimal representation.\nIt is only recently that developments in theory have made\nthis association possible, and developments in optimization\nand hardware have made deep neural networks a sensational\nsuccess.\nVII. ACKNOWLEDGMENTS\nRen´e Vidal acknowledges grant NSF 1618485. Raja\nGiryes acknowledges the Global Innovation Fund (GIF). Ste-\nfano Soatto acknowledges grants ONR N00014-15-1-2261,\nARO W911NF-15-1-0564/66731-CS, and AFOSR FA9550-\n15-1-0229.\nREFERENCES\n[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning.\nNature, 521(7553):436–444, 2015.\n[2] V. Nair and G.E. Hinton.\nRectiﬁed linear units improve restricted\nboltzmann machines. In International conference on machine learning,\npages 807–814, 2010.\n[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity\nmappings in deep residual networks.\nIn European Conference on\nComputer Vision, pages 630–645. Springer, 2016.\n[4] Jia Deng, Wei Dong, R Socher, Li-Jia Li, and Li Fei-Fei. ImageNet:\nA large-scale hierarchical image database. In IEEE Conference on\nComputer Vision and Pattern Recognition, pages 248–255. IEEE, June\n2009.\n[5] G. Cybenko. Approximation by superpositions of a sigmoidal function.\nMathematics of Control Signals and Systems, 2(4):303–314, 1989.\n[6] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward\nnetworks are universal approximators. Neural networks, 2(5):359–366,\n1989.\n[7] Kurt Hornik. Approximation capabilities of multilayer feedforward\nnetworks. Neural Networks, 4(2):251–257, 1991.\n[8] Andrew R Barron. Approximation and estimation bounds for artiﬁcial\nneural networks. Machine Learning, 14(1):115–133, 1994.\n[9] J. Bruna and S. Mallat.\nInvariant scattering convolution networks.\nTrans. PAMI, 35(8):1872–1886, 2013.\n[10] P. Bartlett and W. Maass. Vapnik-Chervonenkis dimension of neural\nnets. The handbook of brain theory and neural networks, pages 1188–\n1192, 2003.\n[11] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov.\nDropout: A simple way to prevent neural networks from\noverﬁtting. The Journal of Machine Learning Research, 15(1):1929–\n1958, 2014.\n[12] R. Giryes, G. Sapiro, and A. Bronstein. Deep neural networks with\nrandom gaussian weights: A universal classiﬁcation strategy? IEEE\nTransactions on Signal Processing, 64(13):3444–3457, 2016.\n[13] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck\nmethod. In Proc. of the Allerton Conf., 2000.\n[14] S. Soatto and A. Chiuso. Visual representations: Deﬁning properties\nand deep approximations.\nProc. of the Intl. Conf. on Learning\nRepresentations (ICLR); ArXiv: 1411.7676, May 2016.\n[15] F. Anselmi, L. Rosasco, and T. Poggio. On invariance and selectivity\nin representation learning. arXiv preprint arXiv:1503.05938, 2015.\n[16] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep\nneural networks via information. arXiv preprint arXiv:1703.00810,\n2017.\n[17] A. Achille and S. Soatto.\nInformation dropout: Learning optimal\nrepresentations through noisy computation. arXiv:1611.01353, 2016.\n[18] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin\nMurphy.\nDeep variational information bottleneck.\narXiv preprint\narXiv:1612.00410, 2016.\n[19] PJ Werbos. Beyond regression: New tools for predictions and analysis\nin the behavioral science. Cambridge, MA, itd. PhD thesis, Harvard\nUniversity, 1974.\n[20] Mark Schmidt, Nicolas Le Roux, and Francis Bach.\nMinimizing\nﬁnite sums with the stochastic average gradient.\nMathematical\nProgramming, pages 1–30, 2013.\n[21] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho,\nSurya Ganguli, and Yoshua Bengio.\nIdentifying and attacking the\nsaddle point problem in high-dimensional non-convex optimization.\nIn Neural Information Processing Systems, pages 2933–2941, 2014.\n[22] A. Choromanska, M. Henaff, M. Mathieu, G.B Arous, and Y. LeCun.\nThe loss surfaces of multilayer networks. In International Conference\non Artiﬁcial Intelligence and Statistics, pages 192–204, 2015.\n[23] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Ben-\ngio.\nSharp minima can generalize for deep nets.\narXiv preprint\narXiv:1703.04933, 2017.\n[24] C. Baldassi, A. Ingrosso, C. Lucibello, L. Saglietti, and R. Zecchina.\nLocal entropy as a measure for sampling solutions in constraint\nsatisfaction problems. Journal of Statistical Mechanics: Theory and\nExperiment, 2016(2):023301, 2016.\n[25] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi,\nC. Borgs, J. Chayes, L. Sagun, and R. Zecchina.\nEntropy-SGD:\nBiasing gradient descent into wide valleys. International Conference\nof Learning and Representations, 2017.\n[26] Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, and\nGuillaume Carlier. Deep relaxation: partial differential equations for\noptimizing deep neural networks. arXiv:1704.04932, 2017.\n[27] C Daniel Freeman and Joan Bruna. Topology and geometry of half-\nrectiﬁed network optimization. ICLR, 2017.\n[28] J. Sokoli´c, R. Giryes, G. Sapiro, and M. Rodrigues.\nRobust large\nmargin deep neural networks.\nto appear in IEEE Transactions on\nSignal Processing, 2017.\n[29] J. Sokoli´c, R. Giryes, G. Sapiro, and M. Rodrigues. Generalization\nerror of invariant classiﬁers. In AISTATS, 2017.\n[30] B. Haeffele and R. Vidal. Global optimality in neural network training.\nIn IEEE Conference on Computer Vision and Pattern Recognition,\n2017.\n[31] B. Haeffele and R. Vidal. Global optimality in tensor factorization,\ndeep learning, and beyond. arXiv, abs/1506.07540, 2015.\n[32] B. Haeffele, E. Young, and R. Vidal. Structured low-rank matrix fac-\ntorization: Optimality, algorithm, and applications to image processing.\nIn International Conference on Machine Learning, pages 2007–2015,\n2014.\n[33] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online\nlearning for matrix factorization and sparse coding. The Journal of\nMachine Learning Research, 11:19–60, 2010.\n[34] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learn-\ning representations by back-propagating errors. Cognitive Modeling,\n5, 1988.\n[35] Stephen J Wright and Jorge Nocedal.\nNumerical Optimization,\nvolume 2. Springer New York, 1999.\n[36] Yangyang Xu and Wotao Yin. A block coordinate descent method for\nregularized multiconvex optimization with applications to nonnegative\ntensor factorization and completion.\nSIAM Journal on Imaging\nSciences, 6(3):1758–1789, 2013.\n[37] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving\ndeep neural networks for lvcsr using rectiﬁed linear units and dropout.\nIn IEEE International Conference on Acoustics, Speech and Signal\nProcessing, pages 8609–8613, 2013.\n[38] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer non-\nlinearities improve neural network acoustic models. In International\nConference on Machine Learning, volume 30, 2013.\n[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn Neural\nInformation Processing Systems, pages 1097–1105, 2012.\n[40] Matthew D. Zeiler, M. Ranzato, Rajat Monga, M. Mao, K. Yang,\nQuoc Viet Le, Patrick Nguyen, A. Senior, Vincent Vanhoucke, Jeffrey\nDean, and Geoffry E. Hinton.\nOn rectiﬁed linear units for speech\nprocessing. In IEEE International Conference on Acoustics, Speech\nand Signal Processing, pages 3517–3521, 2013.\n[41] P. Baldi and K. Hornik. Neural networks and principal component\nanalysis: Learning from examples without local minima.\nNeural\nNetworks, 2(1):53–58, 1989.\n[42] Martin Brady, Raghu Raghavan, and Joseph Slawny. Back propagation\nfails to separate where perceptrons succeed. IEEE Transactions on\nCircuits and Systems, 36(5):665–674, 1989.\n[43] M. Gori and A. Tesi. Backpropagation converges for multi-layered\nnetworks and linearly-separable patterns. In International Joint Con-\nference on Neural Networks, volume 2, page 896. IEEE, 1991.\n[44] M. Gori and A. Tesi. On the problem of local minima in backpropaga-\ntion. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n14(1):76–86, 1992.\n[45] Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau,\nand Patrice Marcotte. Convex neural networks. In Neural Information\nProcessing Systems, pages 123–130, 2005.\n[46] Jerome H Friedman.\nGreedy function approximation: a gradient\nboosting machine. Annals of Statistics, pages 1189–1232, 2001.\n[47] Llew Mason, Jonathan Baxter, Peter L. Bartlett, and Marcus R. Frean.\nBoosting algorithms as gradient descent.\nIn Neural Information\nProcessing Systems, pages 512–518, 2000.\n[48] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the\nperils of non-convexity: Guaranteed training of neural networks using\ntensor methods. arXiv preprint arXiv:1506.08473, 2015.\n[49] Itay Safran and Ohad Shamir.\nOn the quality of the initial basin\nin overspeciﬁed neural networks.\nIn International Conference on\nMachine Learning, pages 774–782, 2016.\n[50] B. Neyshabur, R. Salakhutdinov, and N. Srebro.\nPath-SGD: Path-\nnormalized optimization in deep neural networks. In Neural Informa-\ntion Processing Systems, pages 2422–2430, 2015.\n[51] S. Ioffe and C. Szegedy.\nBatch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In International\nConference on Machine Learning, pages 448–456, 2015.\n[52] St´ephane Mallat. Understanding deep convolutional networks. Phil.\nTrans. R. Soc. A, 374(2065), 2016.\n[53] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object\ndetection with discriminatively trained part-based models. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 32(9):1627–\n1645, 2010.\n[54] St´ephane Mallat. Group invariant scattering. Communications on Pure\nand Applied Mathematics, 65(10):1331–1398, 2012.\n[55] M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst.\nGeometric deep learning: going beyond Euclidean data. arXiv preprint\narXiv:1611.08097, 2016.\n[56] R. Giryes, G. Sapiro, and A. Bronstein. Deep neural networks with\nrandom Gaussian weights: a universal classiﬁcation strategy? IEEE\nTrans. Sig. Proc., 64(13):3444–3457, July 2016.\n[57] J. Bruna, A. Szlam, and Y. LeCun. Signal recovery from lp pooling\nrepresentations.\nIn Int. Conf. on Machine Learning (ICML), pages\n307–315, 2014.\n[58] A. Mahendran and A. Vedaldi. Understanding deep image represen-\ntations by inverting them. In IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5188–5196, 2015.\n[59] E. J. Cand`es and T. Tao. Near-optimal signal recovery from random\nprojections: Universal encoding strategies? IEEE Trans. Inf. Theory,\n52(12):5406 –5425, Dec. 2006.\n[60] V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky.\nThe convex geometry of linear inverse problems.\nFoundations of\nComputational Mathematics, 12(6):805–849, 2012.\n[61] R. Giryes, Y. Eldar, A. Bronstein, and G. Sapiro. Tradeoffs between\nconvergence speed and reconstruction accuracy in inverse problems.\narXiv:1605.09232, 2017.\n[62] L. Wolf and A. Shashua. Learning over sets using kernel principal\nangles. Journal of Machine Learning Research, 4:913–931, Oct. 2003.\n[63] E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm,\ntheory, and applications. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 35(11):2765–2781, 2013.\n[64] Y. Plan and R. Vershynin. Dimension reduction by random hyperplane\ntessellations. Discrete and Computational Geometry, 51(2):438–461,\n2014.\n[65] V. Papyan, Y. Romano, and M. Elad. Convolutional Neural Networks\nAnalyzed via Convolutional Sparse Coding. arXiv:1607.08194, 2016.\n[66] Vladimir N Vapnik. An overview of statistical learning theory. IEEE\nTransactions on Neural Networks, 10(5):988–999, September 1999.\n[67] S Shalev-Shwartz and S Ben-David. Understanding machine learning:\nfrom theory to algorithms. Cambridge University Press, 2014.\n[68] Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian\ncomplexities: risk bounds and structural results. Journal of Machine\nLearning Research, 3:463–482, 2002.\n[69] Huan Xu and Shie Mannor. Robustness and generalization. Machine\nLearning, 86(3):391–423, 2012.\n[70] C. Zhang, S. Bengio, M. Hardt, and B. Recht.\nUnderstanding\ndeep learning requires rethinking generalization.\nIn International\nConference on Learning Representations, 2017.\n[71] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep\nresidual learning for image recognition.\nIn IEEE Conference on\nComputer Vision and Pattern Recognition, December 2016.\n[72] B. Neyshabur, R. Tomioka, and N. Srebro.\nNorm-based capacity\ncontrol in neural networks. In Conference on Learning Theory, pages\n1376–1401, 2015.\n[73] J. Huang, Q. Qiu, G. Sapiro, and R. Calderbank. Discriminative robust\ntransformation learning. In Neural Information Processing Systems,\npages 1333–1341, 2015.\n[74] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks.\narXiv:1605.07146, 2016.\n[75] Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, gen-\neralize better: Stability of stochastic gradient descent. In Proceedings\nof the 33rd International Conference on International Conference\non Machine Learning - Volume 48, ICML’16, pages 1225–1234.\nJMLR.org, 2016.\n[76] G. Hinton and D. Van Camp. Keeping the neural networks simple\nby minimizing the description length of the weights.\nIn Annual\nConference on Computational Learning Theory, pages 5–13. ACM,\n1993.\n[77] D. Kingma, T. Salimans, and M. Welling. Variational dropout and\nthe local reparameterization trick. In Neural Information Processing\nSystems, pages 2575–2583, 2015.\n[78] D. Kingma and M. Welling. Auto-encoding variational bayes. arXiv\npreprint arXiv:1312.6114, 2013.\n[79] G. Sundaramoorthi, P. Petersen, V. S. Varadarajan, and S. Soatto. On\nthe set of images modulo viewpoint and contrast changes. In IEEE\nConference on Computer Vision and Pattern Recognition, 2009.\n",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "published": "2017-12-13",
  "updated": "2017-12-13"
}