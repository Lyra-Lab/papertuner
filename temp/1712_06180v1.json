{
  "id": "http://arxiv.org/abs/1712.06180v1",
  "title": "Towards a Deep Reinforcement Learning Approach for Tower Line Wars",
  "authors": [
    "Per-Arne Andersen",
    "Morten Goodwin",
    "Ole-Christoffer Granmo"
  ],
  "abstract": "There have been numerous breakthroughs with reinforcement learning in the\nrecent years, perhaps most notably on Deep Reinforcement Learning successfully\nplaying and winning relatively advanced computer games. There is undoubtedly an\nanticipation that Deep Reinforcement Learning will play a major role when the\nfirst AI masters the complicated game plays needed to beat a professional\nReal-Time Strategy game player. For this to be possible, there needs to be a\ngame environment that targets and fosters AI research, and specifically Deep\nReinforcement Learning. Some game environments already exist, however, these\nare either overly simplistic such as Atari 2600 or complex such as Starcraft II\nfrom Blizzard Entertainment. We propose a game environment in between Atari\n2600 and Starcraft II, particularly targeting Deep Reinforcement Learning\nalgorithm research. The environment is a variant of Tower Line Wars from\nWarcraft III, Blizzard Entertainment. Further, as a proof of concept that the\nenvironment can harbor Deep Reinforcement algorithms, we propose and apply a\nDeep Q-Reinforcement architecture. The architecture simplifies the state space\nso that it is applicable to Q-learning, and in turn improves performance\ncompared to current state-of-the-art methods. Our experiments show that the\nproposed architecture can learn to play the environment well, and score 33%\nbetter than standard Deep Q-learning which in turn proves the usefulness of the\ngame environment.",
  "text": "Towards a Deep Reinforcement Learning\nApproach for Tower Line Wars\nPer-Arne Andersen, Morten Goodwin, and Ole-Christoﬀer Granmo\nUniversity of Agder, Grimstad, Norway\ncair-internal@uia.no\nAbstract. There have been numerous breakthroughs with reinforce-\nment learning in the recent years, perhaps most notably on Deep Rein-\nforcement Learning successfully playing and winning relatively advanced\ncomputer games. There is undoubtedly an anticipation that Deep Re-\ninforcement Learning will play a major role when the ﬁrst AI masters\nthe complicated game plays needed to beat a professional Real-Time\nStrategy game player. For this to be possible, there needs to be a game\nenvironment that targets and fosters AI research, and speciﬁcally Deep\nReinforcement Learning. Some game environments already exist, how-\never, these are either overly simplistic such as Atari 2600 or complex\nsuch as Starcraft II from Blizzard Entertainment.\nWe propose a game environment in between Atari 2600 and Starcraft II,\nparticularly targeting Deep Reinforcement Learning algorithm research.\nThe environment is a variant of Tower Line Wars from Warcraft III,\nBlizzard Entertainment. Further, as a proof of concept that the envi-\nronment can harbor Deep Reinforcement algorithms, we propose and\napply a Deep Q-Reinforcement architecture. The architecture simpliﬁes\nthe state space so that it is applicable to Q-learning, and in turn im-\nproves performance compared to current state-of-the-art methods. Our\nexperiments show that the proposed architecture can learn to play the\nenvironment well, and score 33% better than standard Deep Q-learning\n— which in turn proves the usefulness of the game environment.\nKeywords: Reinforcement Learning, Q-Learning, Deep Learning, Game\nEnvironment\n1\nIntroduction\nDespite many advances in AI for games, no universal reinforcement learning\nalgorithm can be applied to Real-Time Strategy Games (RTS) without data\nmanipulation or customization. This includes traditional games such as Warcraft\nIII, Starcraft II, and Tower Line Wars. Reinforcement Learning (RL) has been\napplied to simpler games such as games for the Atari 2600 platform but has to\nthe best of our knowledge not successfully been applied to RTS games. Further,\nexisting game environments that target AI research are either overly simplistic\nsuch as Atari 2600 or complex such as Starcraft II.\narXiv:1712.06180v1  [cs.AI]  17 Dec 2017\nReinforcement Learning has had tremendous progress in recent years in learn-\ning to control agents from high-dimensional sensory inputs like vision. In simple\nenvironments, this has been proven to work well [1], but are still an issue for\ncomplex environments with large state and action spaces [2]. In games where\nthe objective is easily observable, there is a short distance between action and\nreward which fuels the learning. This is because the consequence of any action\nis quickly observed, and then easily learned. When the objective is more com-\nplicated the game objectives still need to be mapped to the reward function,\nbut it becomes far less trivial. For the Atari 2600 game Ms. Pac-Man this was\nsolved through a hybrid reward architecture that transforms the objective to a\nlow-dimensional representation [3]. Similarly, the OpenAI’s bot is able to beat\nworld’s top professionals at 1v1 in DotA 2. It uses reinforcement learning while\nit plays against itself, learning to predict the opponent moves.\nReal-Time Strategy Games, including Warcraft III, is a genre of games much\nmore comparable to the complexity of real-world environments. It has a sparse\nstate space with many diﬀerent sensory inputs that any game playing algorithm\nmust be able to master in order to perform well within the environment. Due\nto the complexity and because many action sequences are required to constitute\na reward, standard reinforcement learning techniques including Q-learning are\nnot able to master the games successfully.\nThis paper introduces a two-player version of the popular Tower Line Wars\nmodiﬁcation from the game Warcraft III. We refer to this variant as Deep Line\nWars. Note that Tower Line Wars is not an RTS game, but has many similar\nelements such as time-delayed objectives, resource management, oﬀensive, and\ndefensive strategy planning. To prove that the environment is working we, in-\nspired by recent advances from van Seijen et al. [3], apply a method of separating\nthe abstract reward function of the environment into smaller rewards. This ap-\nproach uses a Deep Q-Network using a Convolutional Neural Network to map\nactions to states and can play the game successfully and perform better than\nstandard Deep Q-learning by 33%.\nRest of the paper is organized as follows: We ﬁrst investigate recent discover-\nies in Deep RL in section 2. We then brieﬂy outline how Q-Learning works and\nhow we interpret Bellman’s equation for utilizing Neural Networks as a function\napproximator in section 3. We present our contribution in section 4 and present\na comparison of other game environments that are widely used in reinforcement\nlearning. We introduce a variant of Deep Q-Learning in section 5 and present a\ncomparison to other RL models used in state-of-the-art research. Finally we show\nresults in section 6, deﬁne a roadmap of future work in section 7 and conclude\nour work in section 8\n2\nRelated Work\nThere have been several breakthroughs related to reinforcement learning per-\nformance in recent years [4]. Q-Learning together with Deep Learning was a\ngame-changing moment, and has had tremendous success in many single agent\nenvironments on the Atari 2600 platform [1]. Deep Q-Learning as proposed by\nMnih et al. [1] as shown in Figure 1 used a neural network as a function approx-\nimator and outperformed human expertise in over half of the games [1].\nFig. 1: Deep Q-Learning architecture\nHasselt et al. proposed Double DQN, which reduced the overestimation of\naction values in the Deep Q-Network [5]. This led to improvements in some of\nthe games on the Atari platform.\nWang et al. then proposed a dueling architecture of DQN which introduced\nestimation of the value function and advantage function [6]. These two functions\nwere then combined to obtain the Q-Value. Dueling DQN were implemented\nwith the previous work of van Hasselt et al. [6].\nHarm van Seijen et al. recently published an algorithm called Hybrid Re-\nward Architecture (HRA) which is a divide and conquer method where several\nagents estimate a reward and a Q-value for each state [3]. The algorithm per-\nformed above human expertise in Ms. Pac-Man, which is considered one of the\nhardest games in the Atari 2600 collection and is currently state-of-the-art in\nthe reinforcement learning domain [3]. The drawback of this algorithm is that\ngeneralization of Minh et al. approach is lost due to a huge number of separate\nagents that have domain-speciﬁc sensory input.\nThere have been few attempts at using Deep Q-Learning on advanced simu-\nlators speciﬁcally made for machine-learning. It is probable that this is because\nthere are very few environments created for this purpose.\n3\nQ-Learning\nReinforcement learning can be considered hybrid between supervised and un-\nsupervised learning. We implement what we call an agent that acts in our en-\nvironment. This agent is placed in the unknown environment where it tries to\nmaximize the environmental reward [7].\nMarkov Decision Process (MDP) is a mathematical method of modeling\ndecision-making within an environment. We often use this method when uti-\nlizing model-based RL algorithms. In Q-Learning, we do not try to model the\nMDP. Instead, we try to learn the optimal policy by estimating the action-value\nfunction Q∗(s, a), yielding maximum expected reward in state s executing action\na. The optimal policy can then be fosund by\nπ(s) = argmaxaQ∗(s, a)\n(1)\nThis is derived from Bellman’s Equation, because we can consider U(s) =\nmaxaQ(s, a), the Utility function to be true. This gives us the ability to derive\nfollowing update-rule equation from Bellman’s work:\nQ(s, a) ←Q(s, a)+\nα\n|{z}\nLearning Rate\n \nR(s)\n|{z}\nReward\n+\nγ\n|{z}\nDiscount\nmaxa′Q(s\n′, a\n′)\n|\n{z\n}\nNew Estimate\n−\nQ(s, a)\n| {z }\nOld Estimate\n!\n(2)\nThis is an iterative process of propagating back the estimated Q-value for\neach discrete time-step in the environment. It is guaranteed to converge towards\nthe optimal action-value function, Qi →Q∗as i →∞[7,1]. At the most basic\nlevel, Q-Learning utilize a table for storing (s, a, r, s\n′) pairs. But we can instead\nuse a non-linear function approximation in order to approximate Q(s, a; θ). θ de-\nscribes tunable parameters for approximator. Artiﬁcial Neural Networks (ANN)\nare a popular function approximator, but training using ANN is relatively un-\nstable. We deﬁne the loss function as following.\nL(θi) = E\nh\n(r + γmaxa′Q(s\n′, a\n′; θi) −Q(s, a; θi))2i\n(3)\nAs we can see, this equation uses Bellman equation to calculate the loss for\nthe gradient descent. To combat training instability, we use Experience Replay.\nThis is a memory module which stores memories from experienced states and\ndraws a uniform distribution of experiences to train the network [1]. This is what\nwe call a Deep Q-Network and are as described in its most primitive form. See\nrelated work for recent advancements in DQN.\n4\nDeep Line Wars\nFor a player to play RTS games well, he typically needs to master high diﬃculty\nstrategies. Most RTS strategies incorporate\n– Build strategies,\n– Economy management,\n– Defense evaluation, and\n– Oﬀense evaluation.\nThese objectives are easy to master when separated but become hard to perfect\nwhen together. Starcraft II is one of the most popular RTS games, but due to\nits complexity, it is not expected that an AI-based system can beat this game\nanytime soon. At the very least, state-of-the-art Deep Q-Learning is not directly\napplicable. Blizzard entertainment and Google DeepMind has collaborated on\nan interface to the Starcraft II game. [8,9]. Starcraft II is for many researchers\nconsidered the next big goal in AI research. Warcraft III is relatable to Starcraft\nII as they are the same genre and have near identical game mechanics.\nCurrent state-of-the-art algorithms struggle to learn objectives in the state-\nspace because the action-space is too abstract. [10]. State and action spaces\ndeﬁne the range of possible conﬁgurations a game board can have. Existing DQN\nmodels use pixel data as input and objectively maps state to action [1]. This\nworks when the game objective is closely linked to an action, such as controlling\na paddle in Breakout, where the correct action is quickly rewarded, and a wrong\naction quickly punished. This is not possible in RTS games. If the objective is\nto win the game, an action will only be rewarded or punished after minutes or\neven hours of gameplay. Furthermore, gameplay would consist of thousands of\nactions and only combined will they result in a reward or punishment.\nFig. 2: Properties of selected game environments\nCollected data in Figure 2 argues that games that have been solved by current\nstate-of-the-art is usually non-stochastic and is fully observable. Also, current AI\nprefers environments which are not simultaneous, meaning they can be paused\nbetween each state transition. This makes sense because hardware still limits\nadvances in AI.\nBy doing rough estimations of the state-space in-game environments from\nFigure 2, it is clear that state-of-the-art has done a big leap in recent years. With\nthe most recent contribution being Ms. Pac-Man [3]. However, by computing\nthe state-space of a regular Starcraft II map only taking unit compositions into\naccount, the state space can be calculated to be (128x128)400 = 16384400 =\n101685 [11].\nFig. 3: State-space complexity of selected game environments\nThe predicament is that the diﬀerence in complexity between Ms. Pac-Man\nand Starcraft II is tremendous. Figure 3 illustrates a relative and subjective com-\nparison between state-complexity in relevant game environments. State-space\ncomplexity describes approximately how many diﬀerent game conﬁgurations a\ngame can have. It is based on map size, unit position, and unit actions. The com-\nparison is a bit arbitrary because the games are complex in diﬀerent manners.\nHowever, there is no doubt that the distance between Ms. Pac-Man, perhaps the\nmost advanced computer game mastered so far, and Starcraft II is colossal. To\nadvance AI solutions towards Starcraft II, we argue that there is a need for sev-\neral new game environments that exceed the complexity of existing games and\nchallenge researches on multi-agent issues closely related to Starcraft II [12]. We,\ntherefore, introduce Deep Line Wars as a two-player variant of Tower Line Wars.\nDeep Line Wars is a game simulator aimed at ﬁlling the gap between Atari 2600\nand Starcraft II. It features the most important aspects of an RTS game.\nFig. 4: Graphical Interface of Deep Line Wars\nThe objective of this game is as seen in Figure 4 to invade the opposing player\nwith units until all health is consumed. The opposing players health is reduced\nfor each friendly unit that enters the red area of the map. A unit spawns at a\nrandom location on the red line of the controlling players side and automatically\nwalks towards the enemy base. To protect your base against units, the player can\nbuild towers which shoot projectiles at enemy units. When an enemy unit dies,\na fair percentage of the unit value is given to the player. When a player sends a\nunit, the income variable is increased by a deﬁned percentage of the unit value.\nPlayers gold are increased at regular intervals determined in the conﬁguration\nﬁles. To master Deep Line Wars, the player must learn following skill-set:\n– oﬀensive strategies of spawning units,\n– defending against the opposing player’s invasions, and\n– maintain a healthy balance between oﬀensive and defensive in order to max-\nimize income\nand is guaranteed a victory if mastered better than the opposing player.\nBecause the game is speciﬁcally targeted towards machine learning, the game-\nstate is deﬁned as a multi-dimensional matrix. Figure 5 represents a 5x30x11\nstate-space that contains all relevant board information at current time-step.\nIt is therefore easy to cherry-pick required state-information when using it in\nalgorithms. Deep Line Wars also features possibilities of making an abstract\nrepresentation of the state-space, seen in Figure 6. This is a heat-map that\nrepresent the state (Figure 5) as a lower-dimensional state-space. Heat-maps\nalso allows the developer to remove noise that causes the model to diverge from\nthe optimal policy, see Formula 3.\nWe need to reduce the complexity of the state-space to speed up training.\nUsing heat-maps made it possible to encode the ﬁve-dimensional state informa-\ntion into three dimensions. These dimensions are RGB values that we can ﬁnd\nFig. 5: Game-state representation\nFig. 6: State abstraction using gray-scale heat-maps\nin imaging. Figure 6 show how the state is seen from the perspective of player 1\nusing gray-scale heatmaps. We deﬁne\n– red pixels as friendly buildings,\n– green pixels as enemy units, and\n– teal pixels as the mouse cursor.\nWe also included an option to reduce the state-space to a one-dimensional matrix\nusing gray-scale imaging. Each of the above features is then represented by a\nvalue between 0 and 1. We do this because Convolutional Neural Networks are\ncomputational demanding, and by reducing input dimensionality, we can speed\nup training. [1] We do not down-scale images because the environment is only\n30x11 pixels large. The state cannot be described fully by these heat-maps as\nthere are economics, health, and income that must be interpreted separately.\nThis is solved by having a 1-dimensional vectorized representation of the data,\nthat can be fed into the model.\n5\nDeepQRewardNetwork\nThe main contribution in this paper is the game environment presented in Sec-\ntion 4. A key element is to show that the game environment is working properly\nand we, therefore, introduce a learning algorithm trying to play the game. This\nis in no way meant as a perfect solver for Deep Line Wars, but rather as a proof\nof concept that learning algorithms can be applied in the Deep Line Wars en-\nvironment. In our solution we consider the environment as a MDP having state\nset S, action set A, and a reward function set R. Each of the weighted reward\nfunctions derives from a speciﬁc agent within the MDP and deﬁnes the absolute\nreward of the environment Renv with following equation:\nRenv(s, a) =\nn\nX\ni=1\nwiRi(s, a)\n(4)\nWhere Renv(s, a) is the weighted sum wi of reward function(s) Ri(s, a). The\nproposed algorithm model is a method of dividing the ultimate problem into\nseparate smaller problems which can be trivialized with certain kinds of generic\nalgorithms.\nWhen reward for the observed state is calculated, we calculate the Q-value\nof Q(s, a) utilizing Renv by using a variant of DQN.\n6\nExperiments\nWe conducted experiments with several deep learning algorithms in order to\nbenchmark current state-of-the-art put up against a multi-agent, multi-sensory\nenvironment. The experiments were conducted in Deep Line Wars, a multi-\nagent, multi-sensory environment. All algorithms were benchmarked with iden-\ntical game parameters.\nFig. 7: Separation of the reward function\nFig. 8: Property matrix of tested algorithms\nWe tested DeepQNetwork, a state-of-the-art DQN from Mnih et al[1], Deep-\nQRewardNetwork, rule-based, and random behaviour. Each of the algorithms\nwas tested with several conﬁgurations, seen in Figure 8. We did not expect any\nof these algorithms to beat the rule-based challenge due to the diﬃculty of the\nAI. The extended execution graph algorithm (see Section 7) was not part of\nthe test bed because it was not able to compete with any of the simpler DQN\nalgorithms without guided mouse management.\nTests were done using Intel I7-4770k, 64GB RAM and NVIDIA Geforce GTX\n1080TI. Each of the algorithms was trained/executed for 1500 episodes. Each\nepisode is considered to be a game that either of the players wins, or the 600\nseconds time limit is reached. DQN had a discount-factor of 0.99, learning rate\nof 0.001 and batch-size of 32.\nFig. 9: Income after each episode\nThroughout the learning process, we can see that DeepQNetwork and Deep-\nQRewardNetwork learn to perform resource management correctly. Figure 9 il-\nlustrates income throughout learning from 1500 episodes. The random player is\npresented as an aggregated average of 1500 games, but the remaining algorithms\nare only single instances. It is not practical to perform more than a single run\nof the Deep Learning algorithms because it takes several minutes per episode to\nﬁnish which sums up to a huge learning time.\nFigure 9 shows that the proposed algorithms outperform random behavior af-\nter relatively few episodes. DeepQRewardNetwork performs approximately 33%\nbetter than DeepQNetwork. We believe that this is because the reward func-\ntion R(s, a) is better deﬁned and therefore easier to learn the optimal policy in\na shorter period of time. These results show that DeepQRewardNetwork con-\nverges towards the optimal policy better, but as seen in Figure 9 diverges after\napproximately 1300 games. The reason for the divergence is that experience re-\nplay does not correctly batch important memories to the model. This causes\nthe model to train on unimportant memories and diverges the model. This is\nconsidered a part of future work and is addressed more thoroughly in section\n7. The rule-based algorithm can be regarded as an average player and can be\ncompared to human level in this game environment.\nFig. 10: Victory distribution of tested algorithms\nFigure 10 shows that DeepQNetwork and DeepQRewardNetwork have about\n63-67% win ratio throughout the learning process. Compared to the rule-based\nAI it does not qualify to be near mastering the game, but we can see that it\noutperforms random behavior in the game environment.\n7\nFuture Work\nThis paper introduced a new learning environment for reinforcement learning\nand applied state-of-the-art Deep-Q Learning to the problem. Some initial re-\nsults showed progress towards an AI that could beat a rule-based AI. There are\nstill several challenges that must be addressed for an unsupervised AI to learn\ncomplex environments like Line Tower Wars. Mouse input based games are diﬃ-\ncult to map to an abstract state representation, because there are a huge number\nof sequenced mouse clicks that are required, to correctly act in the game. DQN\ncannot at current state handle long sequences of actions and must be guided in-\norder to succeed. Finding a solution to this problem without guiding is thought\nto be the biggest blocker for these types of environments, and will be the focus\nfor future work.\nDeepQNetwork and DeepQRewardNetwork had issues with divergence after\napproximately 1300 episodes. This is because our experience replay algorithm\ndid not take into account that the majority of experiences are bad. It could not\nsuccessfully prioritize the important memories. As future work, we propose to\ninstead use prioritized experience replay from Schaul et al. [13].\nFig. 11: Divide & Conquer Execution graph\nFigure 7 show that diﬀerent sensors separate the reward from the environ-\nment to obtain a more precise reward bound to an action. In our research, we\ndeveloped an algorithm that utilizes diﬀerent models based on which state the\nplayer has. Figure 11 show the general idea, where the state is categorized into\nthree diﬀerent types Oﬀensive, Defensive, and No Action. This state is eval-\nuated by a Convolutional Neural Network and outputs a one-hot vector that\nsignal which state the player is currently in. Each of the blocks in Figure 11\nthen represents a form of state-modeling that is determined by the program-\nmer. Our initial tests did not yield any promising results, but according to the\nBellman equations, it is a qualiﬁed way of evaluating the state and successfully\nperform learning, on an iterative basis.\n8\nConclusion\nDeep Line Wars is a simple but yet advanced Real-Time (strategy) game simu-\nlator, which attempts to ﬁll the gap between Atari 2600 and Starcraft II. DQN\nshows promising initial results but is far from perfect in current state-of-the-art.\nAn attempt in making abstractions in the reward signal yielded some improved\nperformance, but at the cost of a more generalized solution. Because of the enor-\nmous state-space, DQN cannot compete with simple rule-based algorithms. We\nbelieve that this is caused by speciﬁcally the mouse input which requires some\nunderstanding of the state to perform well. This also causes the algorithm to\noverestimate some actions, speciﬁcally the oﬀensive actions, because the algo-\nrithm is not able to correctly build defensive without getting negative rewards.\nIt is imperative that a solution of the mouse input actions are found before DQN\ncan perform better. A potential approach could be using the StarCraft II API\nto get additional training data, including mouse sequences [14].\nReferences\n1. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D.,\nRiedmiller, M.: Playing atari with deep reinforcement learning. In: NIPS Deep\nLearning Workshop. (2013)\n2. Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A.J., Banino, A., Denil,\nM., Goroshin, R., Sifre, L., Kavukcuoglu, K., Kumaran, D., Hadsell, R.: Learning\nto navigate in complex environments. CoRR abs/1611.03673 (2016)\n3. van Seijen, H., Fatemi, M., Romoﬀ, J., Laroche, R., Barnes, T., Tsang, J.: Hybrid\nreward architecture for reinforcement learning. abs/1706.04208 (2017)\n4. Gosavi, A.: Reinforcement learning: A tutorial survey and recent advances. IN-\nFORMS Journal on Computing 21(2) (2009) 178–192\n5. van Hasselt, H., Guez, A., Silver, D.: Deep reinforcement learning with double\nq-learning. CoRR abs/1509.06461 (2015)\n6. Wang, Z., de Freitas, N., Lanctot, M.:\nDueling network architectures for deep\nreinforcement learning. CoRR abs/1511.06581 (2015)\n7. Sutton, R.S., Barto, A.G.: Reinforcement Learning : An Introduction. MIT Press\n(1998)\n8. Traysent: Starcraft ii api – technical design. https://us.battle.net/forums/en/\nsc2/topic/20751114921 (Nov)\n9. Vinyals,\nO.:\nDeepmind\nand\nblizzard\nto\nrelease\nstarcraft\nii\nas\nan\nai\nresearch\nenvironment.\nhttps://deepmind.com/blog/\ndeepmind-and-blizzard-release-starcraft-ii-ai-research-environment/\n(Nov 2016)\n10. Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,\nD., Wierstra, D.: Continuous control with deep reinforcement learning. CoRR\nabs/1509.02971 (2015)\n11. Uriarte, A., Ontan, S.: Game-tree search over high-level game states in rts games\n(Oct 2014)\n12. Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: The arcade learning en-\nvironment: An evaluation platform for general agents.\nCoRR abs/1207.4708\n(2012)\n13. Schaul, T., Quan, J., Antonoglou, I., Silver, D.:\nPrioritized experience replay.\nCoRR abs/1511.05952 (2015)\n14. Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Sasha Vezhnevets, A., Yeo,\nM., Makhzani, A., K¨uttler, H., Agapiou, J., Schrittwieser, J., Quan, J., Gaﬀney,\nS., Petersen, S., Simonyan, K., Schaul, T., van Hasselt, H., Silver, D., Lillicrap, T.,\nCalderone, K., Keet, P., Brunasso, A., Lawrence, D., Ekermo, A., Repp, J., Tsing,\nR.: StarCraft II: A New Challenge for Reinforcement Learning. ArXiv e-prints\n(August 2017)\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2017-12-17",
  "updated": "2017-12-17"
}