{
  "id": "http://arxiv.org/abs/1712.06674v1",
  "title": "word representation or word embedding in Persian text",
  "authors": [
    "Siamak Sarmady",
    "Erfan Rahmani"
  ],
  "abstract": "Text processing is one of the sub-branches of natural language processing.\nRecently, the use of machine learning and neural networks methods has been\ngiven greater consideration. For this reason, the representation of words has\nbecome very important. This article is about word representation or converting\nwords into vectors in Persian text. In this research GloVe, CBOW and skip-gram\nmethods are updated to produce embedded vectors for Persian words. In order to\ntrain a neural networks, Bijankhan corpus, Hamshahri corpus and UPEC corpus\nhave been compound and used. Finally, we have 342,362 words that obtained\nvectors in all three models for this words. These vectors have many usage for\nPersian natural language processing.",
  "text": " \n \n1 \n \n \n \nWord representation or word embedding in Persian texts  \n Siamak Sarmady \n Erfan Rahmani  \n (Abstract) Text processing is one of the sub-branches of natural language processing. Recently, the use of machine learning \nand neural networks methods has been given greater consideration. For this reason, the representation of words has become \nvery important. This article is about word representation or converting words into vectors in Persian text. In this research \nGloVe, CBOW and skip-gram methods are updated to produce embedded vectors for Persian words. In order to train a neural \nnetworks, Bijankhan corpus, Hamshahri corpus and UPEC corpus have been compound and used. Finally, we have 342,362 \nwords that obtained vectors in all three models for this words. These vectors have many usage for Persian natural language \nprocessing. \n \n1. INTRODUCTION \nThe Persian language is a Hindi-European language \nwhere the ordering of the words in the sentence is \nsubject, object, verb (SOV). According to various \nstructures in Persian language, there are challenges that \ncannot be found in languages such as English [1]. \nMorphologically the Persian is one of the hardest \nlanguages \nfor \nthe \npurposes \nof \nprocessing \nand \nprograming. In addition to common problems with other \nlanguages such as ambiguities and distinct combinations, \nPersian has other challenges. In Persian we have two \ndifferent type of space character that have different \nUnicode encodings, The white space that designates \nword boundaries and pseudo-space that using in  \ncompound words [2]. \nWords on a computer can be stored as strings, but the \noperation on strings are very inefficient. Computers can \nmanage numbers much better than strings. Here word-to-\nvector conversion methods have been introduced and the \nperformance of each one is expressed. \nWhen a word is basically stored as a string on a \ncomputer, this does not say anything about the concept \nof the word, and only allows the detection of specific \nsimilarities in particular circumstances (For example, \nwhen a suffix is added to a word like cats). Of course, it \nshould be noted that many of the words that are \nconceptually similar do not have the same characters. In \naddition, doing operations on strings is very inefficient. \nBetter representations also require faster computing \nthat's why now using vector representation for words [3]. \nIn the second section, a variety of word-to-vector \nconversion methods are presented. In the third section, \nthe models used are introduced. In fourth section, the \narticle methodology is described. In the last section the \nconclusion is expressed. \n2. METHODS OF CONVERTING WORD TO \nVECTOR \nThe first method of converting a word into the vector \nwas the one-hot vector method. When an application \nprocesses text, it can place a word with an index in the \nvocabulary (E.g. cat, 12424). This is a compact \nrepresentation for a word. These numbers can be \nconverted to a vector V next to zero in the whole cell, \nand a bit set in the word index. One-hot vector method \nmisses the similarity between strings that are specified \nby their characters [3]. \nThe second method is distributed vectors. The words \nin the same context (neighboring words) may have \nsimilar meanings (For example, lift and elevator both \nseem to be accompanied by locks such as up, down, \nbuildings, floors, and stairs). This idea influences the \nrepresentation of words using their context. Suppose that \neach word is represented by a v dimensional vector. Each \ncell of the vector is related to the one of the words in the \nvocabulary.  Usually, the size of the vocabulary is very \nlarge (at least a few hundred thousand words), and \nworking with vectors with these dimensions involves \ncomputational obstacles [3]. The third method is the \nword embedding which is explained in the next section. \n3. WORD EMBEDDING \nThe basic concept of words embedding is to store \ncontext information in a small-sized vector. Each word is \nrepresented by a D-dimensional vector where D is a \nsmall value (Usually between 50 and 1000). Instead of \ncounting the number of repetitions of neighbor words, \nvectors are learned. This work was first performed in \n2003 by Bengio et al [4]. The work of Tomas Mikolov et \nal. [5] and Jeffrey Pennington et al. [6] is the best done, \nand this two papers are used in this study. \n \n3.1. CBOW model \nAs shown in Figure 1, this model is a language model \nin which we have a three-layer neural network. Take a \nfew words before and a few words after a word and in \noutput assign a probability score for each of the words in \nthe vocabulary as the probability that the word will \nappear at the position w (t) (current word). Actually, its \npurpose is to estimate the current word according to the \nlast few words and the next few words [5]. \n2 \n \n \nFigure 1: CBOW model [5] \n \n3.2. Skip-gram model \nAs shown in Figure 2, the architecture of this model is \nsimilar to CBOW, with the difference that according to \nthe current word, estimates the last few words and the \nnext few words [5]. \n \nFigure 2: skip-gram model [5] \n3.3. GLoVe model \nThis method has a term-term occurrence matrix called \nX. In which Xij is the number of repetitions of the word i \nnext to the word j, Pij shows the probability of this \noccurrence. The idea behind this model is to look at the \nsimilarity of the two words with respect to the number of \nrepetitions of each of these two words alongside the third \nword of the text. This is done by the Pik / Pjk rate. k is a \nword of the context. So the purpose of the paper is to \nobtain the word vectors and the function F, which is by \ngiving the vectors to the F function obtain the rate that \nspecified above. This means that we have: \n(\n,\n,\n)\n/\ni\nj\nk\nik\njk\nF w w\nw\np\np\n\n \n \n(1) \nWhere w is a vector of words. Finally, with respect to \nthis function, a cost function J is defined which aims to \nminimize it and By minimizing this cost function, \noptimal vectors are obtained for words. Cost function J \nis:   \n2\n,\n1\n(\n)(\nlog\n)\nij\nv\nX\nT\nij\ni\nj\ni\nj\ni j\nJ\nf x\nw w\nb\nb\n\n\n\n\n\n\n  \n(2) \n \nWhere W is the vector of words and f is a weight \nfunction. The weighting function assigns a weight to rare \nevents and events that occur a lot. \nmax\nmax\n(\n/\n)\n(\n)\n( )\n1(\n)\na\nx\nx\nif\nx\nx\nf\nx\notherwise\n\n\n\n\n\n\n\n  \n(3) \nIn the end, to test the quality of the vectors, \nexperiments were carried out on comparing words, \nsimilarities of words, and NER using GloVe vectors, and \nthe results showed that the GloVe vectors performed \nbetter than others [6]. \n \n4. RESEARCH METHODOLOGY \nIn this section, the methods that used in this article are \npresented. First, the corpuses that used in this article and \nthe operations performed on them are described. Then, \ndetails of the implementation of the three methods of \nCBOW, skip-gram and GLoVe, and the preparation of \nembedded vectors for Persian words are expressed. \n \n4.1. Corpuses and preprocessing \nIn this paper, Bijankhan corpus [7], Hamshahri corpus \n[8] and UPEC corpus [2] has been used. In the UPEC \ncorpus, the rules of white space and pseudo-space are \nobserved. But there is no pseudo-space in the BijanKhan \ncorpus and Hamshahri corpus. For example, a word such \nas \"کتاب\nها\" in the UPEC corpus has been written with \npseudo-space and this is correctness. But in the \nBijanKhan and Hamshahri, they are written in two \nforms: \"کتابها\" and \"کتاب ها\". In this paper, all three \ncorpuses are combined to learn the algorithms of \nconverting words into vectors. To combine these three \ncorpuses, it was necessary to convert all three corpuses \ninto a simple text format. \n \n \n3 \n \n \n \nBijankhan's corpus is available in the form of files in \nthe LBL format that each line has a word and its tag \n(Figure 3). In order to prepare Bijankhan's corpus in this \nstudy, a program has been prepared. This program \nextracts text from LBL files, Deletes the tags of words, \nAnd at the end, it examines pseudo-space. To correct \npseudo-space, the spacing character has become pseudo-\nspace in terms like \"کتاب ها\". But there is no change in the \nplaces where the same word was brought in the form of \n\"کتابها\". At the end, the corpus is converted from the form \nof each word in a single line to the normal text and \nstored in a text file. \nThe UPEC corpus has the same format as the \nBijankhan corpus (Each word and tag in a single line \nsuch as Figure 3) but the entire corpus is available as a \ntxt file. To be able to use this corpus to generate \nembedded vector, we need to convert this corpus to \nnormal text same as Bijankhan corpus. In this research, a \nprogram is written for the UPEC corpus that eliminates \nthe labels and converts the whole corpus into normal \ntext. \n \nFigure 3: shape of Bijankhan corpus and UPEC corpus \n \nThe Hamshahri corpus is available in 39,411 files in \nthe form of a documentary on the Internet. These \ndocumented files are provided in html format. \n \nFigure 4: shape of Hamshahri corpus documents \n \nAs seen in the Figure 4, this corpus is written in the \nform of html files on the Internet. First, a crawler was \ncreated and corpus files have been downloaded (39,411 \nfiles). In this corpus, there is no pseudo-space. Unlike \ntwo other corpus, this corpus has normal format for text \nand maybe the marks (For example a question mark) \nadhere to the words and tokenization are not properly. \nAccording to the specifications of the Hamshahri \ncorpus, a program has been prepared for this research \nthat first extracts text from html files then a group of \nrepetitive suffixes and prefixes that are listed in the \nTable 1 will become pseudo-space. For example, when \nthe word \"یم رود\" with the space written, it becomes \npseudo-space (\"می\nرود\"). But in cases where the word is \nwritten as \"م\nیرود\", there is no change in the word. \n \nTable 1: Repetitive suffixes and prefixes \nSuffix or \nprefix \nexample \nSuffix or \nprefix \nexample \n\" \" می می\nرود \n\"\"ایم رفته\nایم \n\"نمی\"\n نمی\nرود \n\"اید\"\n رفته\nاید \n\"ها\"\n کتاب\nها \n\"\"اند رفته\nاند \n\"\"های کتاب\nهای \n\"\"ی مجموعه\nی \n\"هایی\"\n کتاب\nهایی \n\"\"تر سریع\nتر \n\"\"ام رفته\nام \n\"ترین\"\n سریع\nترین \n\"\"ای \n رفته\nای \n \n \n \nNext, the redundant characters that may be attached to \nthe words are separated. These symbols are shown in the \nTable 2. \n \n \n \n4 \n \nTable 2: Repetitive marks \nmark \nmark \nmark \n0 \n»« \n)( \n، ؛ \n/\n \n \n: \n- \n \n=  \n ؟ \n... \n%\n \n \n! \n[] \n \n \nIn the end, all html files and their text are combined \nand stored in a text file. At this point, the text obtained is \nready to use to generate words vector. \n \n4.2. Preparation of GLoVe vectors \nThe implementation code of GLoVe in the Python \nprogramming environment is available and called tf-\nglove [9]. These codes use the TensorFlow library. \nTensorFlow is an open source software library for \nmachine learning that It is used to train neural networks \nand is provided by Google [10]. \nIn this research, we have written a program that uses \ntf-glove codes and use the TensorFlow machine learning \nlibrary to prepare Persian language vectors. In this \nprogram, the data is received as a list of sentences in \nwhich the words of each sentence are separated. For this \npurpose, the Persian text is read from the corpus and the \nsentences are separated by a tool that is in the NLTK \nlibrary and they are stored in a list of sentences, and in \nthe next step, the words related to each sentence in the \nlist are tokenized. \nThe word tokenization functions provided in the \nPython NLTK Library create problems for Persian words \nand the pseudo-space character is considered as a word \nby itself. The split function of the Python can be used to \nseparate words with space. In this case, the function \ntakes words that have pseudo-space correctly as a single \nword. So after modifying pseudo-space, the same \nfunction is used to tokenize words. After completing \nthese steps, the data is ready for presentation to the tf-\nglove library. \nThe word \"UNK\" with \"0\" id (for anonymous words) \nand the word \"UNK_PAD\" with the \"1\" id (for padding \noperations in different neural networks) has also been \nadded to the vocabulary. \nAfter learning the word vectors, two files are created \nas output. The first file is a vocabulary containing words \nand their indexes (Figure 5A), and the second file is the \nword vectors, in which each vector is identified by the \nword id (Figure 5B). \n \n \nFigure 5: (A) - Part of the vocabulary file (B) - Part of \nthe word vectors   \n \n \nIn Table 3, the parameters used in the learning process \nand implementation of this model are presented. \n \nTable 3: GLoVe Model Parameters \nparameters \nvalue \ndescribtion \nBatch_size \n64 \nNumber of training  batchs \nEmbedding_size \n128 \nDimensions of word vectors \nContext_size \n5 \nThe number of considered \nwords in left and right for \ntarget word \nMin_occurences \n1 \nLowest repeat for a word \nCooccourence_cap \n100 \nin equation ..  x_max  Most \nrepeated for each word,  \nLearning_rate \n0.05 \nLearning rate \nScaling_factor \n3/4 \nAlpha value in equation .. \nNum_epoch \n20 \nNumber of epoch \n \n4.3. Preparation of CBOW vectors \nIn this study, the CBOW model in the Python \nenvironment and using the TensorFlow library, is used \n[11]. This implementation is aimed at generating \nappropriate embedded vectors and examining the \nsemantic and syntactic similarity of words in these \nvectors [5]. But in this research, the goal is to extract \nword vectors (the weights of this neural network). Due to \nthe difference in the display of Persian words and \nEnglish words, there are irregularities in Persian display \nof \nwords. \nIn \nthis \nimplementation, \nas \nin \nthe \nimplementation of GLoVe, words are stored as a \nvocabulary (identified by numeric indexes) in a file.  \nAnd word vectors are also stored in another file (with the \nsame characteristic indexes). Depending on the word \nindex in the vocabulary, you can find the vector of each \nword in the vectors file. In this program, the first text of \nthe corpus is read from the input file then the words are \ntokenized and the number of repetitions of each word is \ncounted and the words are placed in descending order in \ntheir vocabulary according to the number of repetitions. \nFor anonymous words, the words that may later be \nseen, but not in the vocabulary the word \"UNK\" is used. \n \n \n5 \n \n \n \nFor padding, the word \"UNK_PAD\" has been added to \nthe vocabulary. \nFor word tokenization in Persian like the previous \nsection, after correcting pseudo-space in the corpus, the \nsplit function in python is used which works correctly \nwith pseudo-space. As stated in the introduction of the \nCBOW model, this code predicts the target word with \nrespect to a few words before and a few words next \n(context words). After the end of learning, the weights of \nthe neural network represent the vector of words. After \nexecuting this program, word vectors and vocabulary are \nstored in two separate files. For example, let's consider \nthe following sentences: \n\"the quick brown fox jumped over the lazy dog\" \nIf the two words are \"the\" and \"brown\", or \"quick\" and \n\"fox\" and we want to predict the middle word, the data is \nstored as follows. \nBatch = [ [the , brown] , [quick , fox] , …] \nAnd the target words fall into the same category. \nPredict = [ [quick] , [brown] , …] \nIn Table 4, we observe the important parameters used \nin the production of vectors embedded by the CBOW \nmodel. \nTable 4: CBOW model parameters  \nparameters \nvalue \ndescribtion \nBatch_size \n128 \nNumber of training batch \nEmbedding_size \n128 \nDimensions of word vectors \nSkip_window \n1 \nThe number of considered \nwords in left and right for \ntarget word \nNum_step \n100000 \nNumber of epoch \n \n4.4. Preparation of skip-gram vectors \nThe skip-gram model is very similar to the CBOW \nmodel. With the difference that in this model, according \nto the current word, a few words before target word and \na few words next the target word estimated. \nThe implementation of this model is available as well \nas CBOW in Python and using TensorFlow [12]. In this \ncode, weighs of neural network after the end of learning \nshow the word vectors. In this study, for the program, as \nwell as the CBOW program, changes have been made to \nsave vectors and vocabulary. The output of this program \nalso includes vocabulary and words vector files. Here the \ndata format that used in this model is shown with an \nexample. Consider the following sentence: \n\"the quick brown fox jumped over the lazy dog\" \nIf the context window length is equal to 3 in which the \ntarget word in the middle and the words in the left and \nright fields are located, the data set will be as follows: \n[quick,[the,brown]] \n[brown,[quick,fox]] \n… \nThe goal is to predict the '' the '' and 'brown\" words by \nknowing the word \"quick\" as well as predicting the \nwords \"quick\" and \"fox\" by knowing the word \"brown\". \nThen the data is converted into inputs and outputs to \nenter the neural network: \n(quick , the) \n(quick , brown) \n(brown , quick) \n(brown , fox) \nThe important parameters used to generate skip grams \nembedded vectors are quite similar to the CBOW model \nand its table. \n5. CONCLUSION \nIn this article, the production of embedded vectors for \nPersian words was discussed. For the production of word \nvectors, the corpus from the combination of BijanKhan \ncorpus, Hamshahri corpus and UPEC corpus has been \nused that has a length of 18,000,000 words. The \nvocabulary contains 342,364 words. And word vectors \nare derived from three GLoVe, CBOW and skip-gram \nmodels that have been updated to work with Persian text. \nAll implementations produce two output files. The first \nfile contains a vocabulary that each of the words in it is \ncharacterized by an index. The second file provides the \nembedded word vectors. Each vector is identified by its \ncorresponding word index (in the vocabulary). These \nembedded word vectors can be used for many Persian \nlanguage processing projects such as NER tagging and \nPOS-tagging. \n \n \n1 \n \n \n \n \nReferences \n \n \n1. \nOkhovvat, M. and B.M. Bidgoli, A \nhidden Markov model for Persian \npart-of-speech tagging. Procedia \nComputer Science, 2011. 3: p. 977-\n981. \n2. \nSeraji, M., B. Megyesi, and J. Nivre. A \nbasic language resource kit for \nPersian. in Eight International \nConference on Language Resources \nand Evaluation (LREC 2012), 23-25 \nMay 2012, Istanbul, Turkey. 2012. \nEuropean Language Resources \nAssociation. \n3. \nTurian, J., L. Ratinov, and Y. Bengio. \nWord representations: a simple and \ngeneral method for semi-supervised \nlearning. in Proceedings of the 48th \nannual meeting of the association for \ncomputational linguistics. 2010. \nAssociation for Computational \nLinguistics. \n4. \nBengio, Y., et al., A neural \nprobabilistic language model. Journal \nof machine learning research, 2003. \n3(Feb): p. 1137-1155. \n5. \nMikolov, T., et al., Efficient estimation \nof word representations in vector \nspace. arXiv preprint \narXiv:1301.3781, 2013. \n6. \nPennington, J., R. Socher, and C. \nManning. Glove: Global vectors for \nword representation. in Proceedings \nof the 2014 conference on empirical \nmethods in natural language \nprocessing (EMNLP). 2014. \n7. \nBijankhan, M., The role of the corpus \nin writing a grammar: An introduction \nto a software. Iranian Journal of \nLinguistics, 2004. 19(2). \n8. \nAleAhmad, A., et al., Hamshahri: A \nstandard Persian text collection. \nKnowledge-Based Systems, 2009. \n22(5): p. 382-387. \n9. \nAn implementation of GloVe in \nTensorFlow n.d.  [cited 2017 10/2]; tf-\nglove ]. Available from: \nhttps://github.com/GradySimon/tensor\nflow-glove. \n10. \nAn open-source software library for \nMachine Intelligence n.d.  [cited 2017 \n12/3]; TensorFlow]. Available from: \nhttp://tensorflow.org/. \n11. \nBasic implementation of CBOW \nword2vec with TensorFlow. . n.d.  \n[cited 2017 11/8]; CBOW in \nTensorFlow]. Available from: \nhttps://gist.github.com/yxtay/a94d971\n955d901c4690129580a4eafb9. \n12. \nAn implementation for skip-gram in \nTensorFlow. n.d.  [cited 2017 11/8]; \nskip-gram in TensorFlow]. Available \nfrom: \nhttps://github.com/tensorflow/tensorfl\now/blob/r1.3/tensorflow/examples/tut\norials/word2vec/word2vec_basic.py. \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2017-12-18",
  "updated": "2017-12-18"
}