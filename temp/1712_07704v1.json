{
  "id": "http://arxiv.org/abs/1712.07704v1",
  "title": "Unsupervised learning of dynamical and molecular similarity using variance minimization",
  "authors": [
    "Brooke E. Husic",
    "Vijay S. Pande"
  ],
  "abstract": "In this report, we present an unsupervised machine learning method for\ndetermining groups of molecular systems according to similarity in their\ndynamics or structures using Ward's minimum variance objective function. We\nfirst apply the minimum variance clustering to a set of simulated tripeptides\nusing the information theoretic Jensen-Shannon divergence between Markovian\ntransition matrices in order to gain insight into how point mutations affect\nprotein dynamics. Then, we extend the method to partition two chemoinformatic\ndatasets according to structural similarity to motivate a train/validation/test\nsplit for supervised learning that avoids overfitting.",
  "text": "Unsupervised learning of dynamical and molecular\nsimilarity using variance minimization\nBrooke E. Husic and Vijay S. Pande\nDepartment of Chemistry\nStanford University\nStanford, CA 94305\n{bhusic, pande}@stanford.edu\nAbstract\nIn this report, we present an unsupervised machine learning method for determining\ngroups of molecular systems according to similarity in their dynamics or structures\nusing Wardâ€™s minimum variance objective function. We ï¬rst apply the minimum\nvariance clustering to a set of simulated tripeptides using the information theoretic\nJensen-Shannon divergence between Markovian transition matrices in order to\ngain insight into how point mutations affect protein dynamics. Then, we extend\nthe method to partition two chemoinformatic datasets according to structural sim-\nilarity to motivate a train/validation/test split for supervised learning that avoids\noverï¬tting.\n1\nIntroduction\nScientists have sought to understand the dynamical behavior of proteins at atomic resolution since\nthe ï¬rst molecular dynamics (MD) simulation of the 58-amino acid bovine pancreatic trypsin in-\nhibitor (BPTI) in 1977 [McCammon et al., 1977]. In the past 40 years, computational chemists have\nseen major improvements in molecular dynamics methods [Adcock and McCammon, 2006], and\nmodern datasets can reach biologically relevant timescales (tens of milliseconds using femtosecond\ntime steps) due to specialized hardware [Shaw et al., 2008] and distributed computing platforms\nsuch as Folding@home [Shirts and Pande, 2000], GPUGRID [Buch et al., 2010], and Google Exacy-\ncle [Kohlhoff et al., 2014].\nThe enormous size of modern MD datasets requires complementary methods to understand\nand analyze the data in a statistically rigorous way.\nMarkov state models (MSMs) are a\npopular framework for this type of analysis that use a master equation to represent the ther-\nmodynamics and kinetics of a molecular system [Bowman et al., 2014].\nRecent advances in\nMSM applications include complex, multi-system dynamics such as protein-protein associa-\ntion [Plattner et al., 2017, Zhou et al., 2017] or aggregated datasets containing simulations in multiple\nforce ï¬elds [McKiernan et al., 2017, Olsson et al., 2017]. It is thus necessary to develop comple-\nmentary tools for understanding these types of aggregated datasets and quantifying the dynamical\nsimilarity among the different systems. Minimum variance cluster analysis (MVCA), a recently\nintroduced unsupervised learning method to coarse-grain a single MSM, has the versatility to be used\nboth within a single model (for coarse-graining) and among a set of models for dynamical cluster-\ning [Husic et al., 2017]. While the authors focus on the coarse-graining application, they conclude\nwith a motivation of the latter application in which they analyze separate folding simulations of a\nsmall protein in nine different protein and water force ï¬eld combinations.\nIn this report, we focus on the ability of the unsupervised MVCA algorithm to identify groups of\nmolecular systems. We ï¬rst provide a theoretical background of the MSM transition matrix in order\nto motivate the development of MVCA for identifying dynamical groups. We then demonstrate the\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1712.07704v1  [physics.bio-ph]  20 Dec 2017\nmethod on MSMs of capped amino acids to gain insight into protein dynamics for larger systems.\nFinally, we showcase the versatility of MVCA by applying it to a completely different problem: the\nselection of training, validation, and test sets for cross-validating a chemoinformatic supervised\nlearning task.\n2\nTheory background\n2.1\nThe Markovian transition matrix\nWe begin with a continuous-time Markovian process, which means that the probability of transitioning\nfrom state x to state y after a time interval of Ï„ does not depend on any states occupied before the\nsystem was in x. We assume this process is time-homogeneous, stochastic, and reversible with respect\nto its stationary distribution, Âµ. The process is also irreducible, which means that there is a path from\neach state to every other state given sufï¬cient time, and aperiodic. If we represent our system by a\nprobability distribution pt at time t, and the transition density kernel for x â†’y as p(x, y), we can\nwrite the new probability distribution at time t + Ï„ as,\npt+Ï„(y)/Âµ(y) =\nZ\nâ„¦\ndx pt(x)p(x, y) = T (Ï„) â—¦pt(y)/Âµ(y).\n(1)\nThe continuous transfer operator T , which is characterized by its lag time Ï„, is compact and self-\nadjoint with respect to the stationary distribution Âµ. The transfer operator admits a decomposition\ninto eigenvalues and eigenfunctions,\nT (Ï„) â—¦Ïˆi =Î»iÏˆi,\n(2)\nwhere the eigenvalues Î»i are real and indexed in decreasing order. The eigenvalue Î»1 = 1 is unique\nand corresponds to the stationary process. All subsequent eigenvalues are on the interval |Î»i>1| < 1\nand correspond to dynamical processes in the time series.\nTo build a MSM, we decompose the subset of conformation space explored by the MD simulation into\ndiscrete, disjoint states. By counting the transitions between these states, we calculate the maximum\nlikelihood estimator of the the transition probability matrix to obtain a discrete approximation to T .\nThis transition matrix is the MSM master equation.\n2.2\nDistance between MSM transition matrices\nTo cluster MSM transition matrices, we review the theory presented in the original MVCA pa-\nper [Husic et al., 2017]. For two MSMs with row-stochastic transition probability matrices P and\nQ, the divergence from the ith row of Q to the ith row of P can be written as the Kullback-Leibler\ndivergence,\ndivKL(Pi||Qi) â‰¡\nX\nj\nPi(j) log Pi(j)\nQi(j),\n(3)\nwhere Pi can be thought of as the â€œreferenceâ€ distribution and Qi as a â€œtestâ€ distribu-\ntion [Bowman et al., 2010]. The information theoretic Jensen-Shannon divergence [Lin, 1991] is a\nrelated symmetric formulation that utilizes M, the elementwise mean of P and Q,\ndivJS(Pi||Qi) â‰¡1\n2divKL(Pi||Mi) + 1\n2divKL(Qi||Mi).\n(4)\nSince\nit\nhas\nbeen\nshown\nthat\nthe\nsquare\nroot\nof\n(4)\nobeys\nthe\ntriangle\ninequal-\nity [Endres and Schindelin, 2003], we can write the following distance metric,\n2\ndivâˆš\nJS(Pi||Qi) â‰¡\np\ndivJS(Pi||Qi).\n(5)\nFinally, for a scalar distance between the two transition matrices P and Q, each of which contains i\nrows, we deï¬ne the sum [Husic et al., 2017],\nDâˆš\nJS â‰¡\nX\ni\ndivâˆš\nJS(Pi||Qi).\n(6)\n2.3\nHierarchical agglomerative clustering\nHierarchical agglomerative clustering is an unsupervised learning method that is initiated with a\nset of pairwise distances between data points and iteratively merges the two closest clusters or\nsingletons. Hierarchical agglomerative clustering thus requires a similarity function that quantiï¬es\nthe distance between all data points and an objective function that determines how to use those\ndistances to determine which existing clusters to merge at each agglomerating step. Common\nexamples of objective functions used for hierarchical agglomerative clustering are single, average,\nand complete linkages, which deï¬ne the distance between two clusters as the shortest, average, or\ngreatest distance, respectively, between any point in one cluster and any point in the other cluster.\nAnother objective function used for hierarchical agglomerative clustering is Wardâ€™s minimum variance\ncriterion [Ward, 1963]. The agglomeration is performed by merging the two clusters or singletons\nsuch that the resulting increase in intra-cluster variance is minimized. Wardâ€™s method is usually\nimplemented according to the following recursive distance update [MÃ¼llner, 2013],\nd(u, v) =\nr\n|v| + |s|\nT\nd(v, s)2 + |v| + |t|\nT\nd(v, t)2 âˆ’|v|\nT d(s, t)2,\n(7)\nwhere the clusters s and t have just been merged to create a new cluster, u, and the new distance\nbetween u and some other cluster v needs to be updated. |c| represents the number of data points\ncontained in cluster c, and T â‰¡|s| + |t| + |v|. A nonrecursive formula for the distance update when s\nor t is a singleton has also been derived [Husic and Pande, 2017]. We note that the minimum variance\nformulation is rigorously deï¬ned for Euclidean distances, but that the objective function can be used\nfor any similarity function with the understanding that it no longer corresponds to Euclidean variance,\nwhich requires the l2-norm.\nIn this paper, we ï¬rst use hierarchical agglomerative clustering with Wardâ€™s minimum variance\nobjective function and the Dâˆš\nJS similarity function (6) to quantify the similarity between multiple\nmodels for related dynamical systems. We then apply Wardâ€™s minimum variance objective function\nwith different similarity functions to the hierarchical clustering of molecular structures and reaction\nï¬ngerprints in order to motivate a cross-validation scheme for a supervised learning model.\n3\nResults\n3.1\nClustering dynamically restrained tripeptides\nIt is often desirable to modify the dynamics of a protein by introducing a sequence mutation,\ni.e. substituting a selected amino acid for the one present in the wild type protein. Simulating mutated\nproteins using MD can provide a window into whether or not the mutation has affected the protein\ndynamics. The MVCA algorithm can be used to cluster a set of mutants based on their dynamics,\nand can lend insight into which mutations produce similar effects. It is known that all solvated\namino acids except glycine and proline occupy certain regions of the space deï¬ned by their Ï• and\nÏˆ backbone dihedral angles [Vitalini et al., 2016]. Figure 1 shows the Ï• and Ïˆ angles on alanine\n(left), the structure of which can be compared with glycine (center) and proline (right). Glycine, the\nsmallest amino acid, occupies more conformations due to its ï¬‚exibility, and proline occupies fewer\nconformations due to its 5-member ring.\n3\nð›— \nð›™ \nala\ngly\npro\nFigure 1: Amino acids are differentiated by their side chains, and the dihedrals about the bonds\nadjacent to the side chains (light blue, left) occupy known regions of Ï• Ã— Ïˆ space. Alanine (left) is\nan amino acid with a standard range of side chain motion. Glycine (center) is more ï¬‚exible because\nit does not have a side chain. Proline (right) is less ï¬‚exible because its side chain forms a ring.\nTo demonstrate MVCA on a dataset for which we can visualize the dynamical degrees of freedom,\nwe consider a set of 42 proline-containing tripeptides simulated for 1 Âµs each in the AMBER ff99SB-\nILDN force ï¬eld at 300 K. We hypothesize that the dynamics of the central amino acid in a tripeptide\nare affected by the presence of proline at any of the three positions. To investigate the number of\ndynamical groups represented by this tripeptide dataset, we ï¬rst create a MSM at a 100 ps lag time for\neach system using a regular spatial clustering of the Ï• and Ïˆ angles of the central amino acid. Since\nall MSMs were built using the same state deï¬nitions, we can assess the similarity of their transition\nmatrices using MVCA with Dâˆš\nJS. The MVCA analysis shows that the 42 tripeptides cluster into ï¬ve\nnatural groups, which is clear from a dendrogram representation of the hierarchical tree (Fig. 2, top).\nPlotting the free energy surfaces of the central amino acid for each tripeptide shows that the energy\nlandscapes within each cluster are very similar. Three large clusters identify tripeptides with proline\nas the ï¬rst, second, and third amino acid (Fig. 2; bottom; blue, purple, and green, respectively). Two\nsingleton clusters are also identiï¬ed, which represent the two systems in the dataset for which glycine\nis the central amino acid (Fig. 2, gray). It is clear from their free energy landscapes that these systems\nare dynamically very different from the others.\nWe suspect that the systems will differentiate according to the location of the proline because it is\neasy to understand the degrees of freedom for this simple system. This analysis is important because\nit has ï¬delity to the expected result, and can successfully group the tripeptides based on only the\ntransition matrices of their MSMs. The analysis does not produce a reasonable result when the single,\naverage, or complete linkage objective function is used instead of Wardâ€™s minimum variance objective\nfunction (see Appendix A).\n3.2\nClustering small molecules for supervised machine learning\nPredicting properties from molecular structures such as solubility and binding afï¬nity is a signiï¬cant\nchallenge, and state of the art approaches such as atomic convolutional networks [Gomes et al., 2017]\nand graph convolutions [Kearnes et al., 2016] have been used for supervised learning of these quanti-\nties. When using supervised learning to predict molecular properties from a representation of the\nmolecular structure, it is important to use cross-validation when assessing the modelâ€™s accuracy. A\nstandard cross-validation split involves dividing the labeled data into three sets: training, validation,\nand test. The model is trained on the training set and is evaluated on the validation set during\ndevelopment. Once hyperparameters have been selected, the ï¬nal model is evaluated on the test set.\nFor chemoinformatic models designed to predict the property of extremely novel new compounds,\ncare must be taken in choosing how to divide the data into these three sets such that the model is\nnot overï¬t to the training data. When using neural network architectures, for example, the goal is to\nproduce a model that has learned some complex underlying feature from the data, but not a model that\nhas memorized every data point and simply recalls what it has memorized. A train/validation/test split\nmotivated by differentiating these two options is therefore critical for evaluating supervised learning\nmodels in the context of characterizing the properties of novel compounds. Common partitions for\nchemoinformatic studies include random splits, temporal splits, stratiï¬ed splits according to the quan-\ntity being learned, and scaffold splits [Bemis and Murcko, 1996] in which molecules are assigned to\na set based on the frequency of the molecular scaffold. The latter method is generally difï¬cult for\nmodels with deep architectures, since the model must apply what it has learned to a different type of\n4\nile-ala-pro\nglu-ala-pro\ngly-ala-pro\nval-ser-pro\nhis-ala-pro\ntyr-asp-pro\ntyr-lys-pro\ngly-ile-pro\nlys-leu-pro\nglu-arg-pro\nleu-glu-pro\nglu-gln-pro\nthr-ile-pro\npro-leu-phe\npro-leu-val\npro-asp-phe\npro-gln-glu\npro-glu-thr\npro-ile-gly\npro-phe-glu\npro-tyr-thr\npro-ser-thr\npro-ser-gly\npro-cys-lys\npro-ala-val\npro-gly-trp\nthr-gly-pro\npro-pro-tyr\narg-pro-asp\ngly-pro-cys\nasp-pro-glu\ngln-pro-phe\nser-pro-gln\nser-pro-ile\npro-pro-gly\nile-pro-leu\nala-pro-ser\nile-pro-ala\nlys-pro-leu\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\n5\n10\n15\n20\n25\nVariance\nFigure 2: A dendrogram from clustering 42 transition matrices using Dâˆš\nJS and Wardâ€™s minimum\nvariance objective function shows that the systems cluster into ï¬ve groups (top). The energy landscape\nof each system is plotted for âˆ’180 < Ï• < 180 on the x-axis and âˆ’180 < Ïˆ < 180 on the y-axis,\nwith darker colors representing more stable conformations (bottom). The ï¬ve clusters are identiï¬ed\nusing boxes corresponding to the colors in the dendrogram. Since we have analyzed a system for\nwhich the degrees of freedom are interpretable, we can see that the clustering analysis identiï¬es\ngroups with similar free energy surfaces.\ndata [Gomes et al., 2017]. A â€œrealistically novelâ€ training/test split for kinase virtual screening has\nalso been recently published with the goal of avoiding overï¬t models [Martin et al., 2017].\nIn the spirit of scaffold and â€œrealistically novelâ€ splittings, here we apply MVCA to choose the\ntraining, validation, and test subsets from a molecular database. Although it is much too small to use\nfor training a deep network with many hyperparameters, we present an example for a reduced group\nof molecules so we can interpret the results. We thus select the 59 compounds with molecular weight\nless than 75 g/mol from a dataset containing aqueous solubilities [Delaney, 2004]. In this dataset, the\nmolecules are represented by the simpliï¬ed molecular-input line-entry system (SMILES), which are\none-dimensional string representations of molecular structures. To quantify the similarity between\neach pair of molecules, we use the Levenshtein distance ratio between the SMILES strings, which is\na function of the number of single character edits that must be made to convert one representation to\nthe other and is normalized for string length. Since the Levenshtein distance ratio ranges from 0 to\n1, where a value of 1 means the strings are identical, we encode the similarity between each pair of\nstrings as the Levenshtein distance ratio subtracted from 1 so that smaller values correspond to closer\nstrings. Then, we apply MVCA to the pairwise SMILES representations (instead of pairwise Dâˆš\nJS\n5\nFigure 3: A dendrogram from clustering 59 small molecules using Levenshtein distance between their\nSMILES representations and Wardâ€™s minimum variance objective function shows that the molecules\ncluster into four groups (top). The solubilities of the groups are shown in the box plot (bottom).\nWe identify training (green), validation (blue), and test (purple) sets such that the development set\n(training and validation) and the test set each span the range of solubilities in the dataset. We note\nthat the structural groups separate by bond saturation, presence of nitrogen or sulfur, and molecular\nweight, but do not separate alkanes, alkenes, and alcohols. Furane is an outlier based on this analysis.\nvalues as in the previous section) and cluster the molecules according to Wardâ€™s minimum variance\nobjective function.\nBased on the dendrogram representation (Fig. 3, top), we then partition the dataset into four groups.\nWe see from Fig. 3 (bottom) that three of the variance-minimized groups according to Levenshtein\ndistance among their SMILES representations contain 11, 25, and 22 molecules, and one group is\na singleton containing just furane. From this partitioning, we would use the largest group as the\ntraining set, the 11-member group as the validation set, and the 22-member group as the test set, since\nthe development set (i.e., training and validation) and the test set each span nearly the full range of\nsolubilities. We might choose to include furane in the test set. As in Sec. 3.1, the single, average, and\ncomplete linkage functions fail to produce suitable groups for this application (see Appendix A).\nWhat can we learn about molecules represented by SMILES strings from this analysis, and how can\nwe use it to improve the supervised prediction of molecular properties such as solubility for novel\ncompounds? We see that the training set contains mostly molecules with unsaturated bonds, more\nthan half of the validation set contains compounds with nitrogen or sulfur, and the test set contains\nlarger molecules (no molecular weight lower than 56 g/mol). However, we also see that all three\nnon-singleton sets contain alkanes, alkenes, and alcohols. In terms of training a supervised model,\n6\nTable 1: Distribution of reactions from 10 classes into two groups determined by MVCA using the\nTanimoto coefï¬cient between 4096-bit structural reaction ï¬ngerprints. All 10 reaction classes are\ndivided amongst the two groups, and most group allocations are similar to the 78%/22% overall\nsplitting.\nClass\nReaction\nQuantity\nPercent in group 1\nPercent in group 2\n1\nHeteroatom alkylation and arylation\n569\n79\n21\n2\nAcylation and related processes\n208\n62\n38\n3\nCarbon-carbon bond formation\n133\n75\n25\n4\nHeterocycle formation\n31\n74\n26\n5\nProtections\n27\n93\n7\n6\nDeprotections\n244\n70\n30\n7\nReductions\n487\n83\n17\n8\nOxidations\n86\n78\n22\n9\nFunctional group interconversion\n208\n85\n15\n10\nFunctional group addition\n7\n71\n29\nTotal\n2000\n78\n22\nit is important to quantify which SMILES representations are the most similar and can be grouped\ntogether with Wardâ€™s minimum variance objective function, and using these groups to design training,\nvalidation, and test sets will help determine if a model is suitable to predict properties for different\nkinds of molecular structures or if it is overï¬tting. Unlike scaffold splitting, this cross-validation\nframework is tuned speciï¬cally for the choice of molecular representation.\nAt a higher level, we can use such results to assess the advantages and drawbacks of a given\nmolecular representation by comparing the MVCA grouping to how a domain expert would\ngroup a set of molecules. To illustrate this, we ran a separate analysis of chemical reactions\nobtained from the United States patent literature and processed as described in [Liu et al., 2017]\nfor the prediction of reaction products.\nEach chemical reaction is classiï¬ed into one of ten\nreaction types [Schneider et al., 2016].\nWe create structural ï¬ngerprints for each reaction us-\ning the CreateStructuralFingerprintForReaction command in the RDKit Python pack-\nage [rdkit.org]. To quantify the similarity between reaction ï¬ngerprints we use the Tanimoto\ncoefï¬cient subtracted from 1 so that smaller values correspond to closer distances, as above. We\nthen use MVCA to group the 2000 reactions with the lowest total molecular weight. The resulting\ndendrogram shows two natural groups with 78% of the reactions in one group and 22% of the\nreactions in the other group. When we calculate the percent allocation of each structural class in\nTable 1, we see that unsupervised clustering of reaction ï¬ngerprints does not partition the reactions\naccording to their classes.\nFrom the perspective of a modeler seeking to design a train/validation/test split for predicting reaction\nproducts, there are two choices for designing a cross-validation scheme. First, the modeler can\ndetermine this split according to the reaction classes in order to represent different types of reactions\nin the train, validation, and test sets. However, this may result in an overï¬t model, since the validation\nand test sets will contain reactions with representations similar to those in the training set according\nto their Tanimoto coefï¬cients. Alternatively, the modeler can determine the split according to the\nsimilarity of reaction representations. In this case, the train, validation, and test sets would be chosen\naccording to minimum variance groupings of ï¬ngerprints. For the reaction dataset analyzed here,\neach group would likely contain reactions from all 10 classes. Performing the same analysis with\nthe original reaction SMILES strings and the Levenshtein distance ratio produces nearly identical\nresults, since 98% of the group assignments are the same as in the ï¬ngerprint analysis. This is\nexpected because the ï¬ngerprints are generated from the SMILES strings. The ï¬ngerprints used\nfor this analysis were 4096-bit, and the same analysis was performed for ï¬ngerprints with sizes\n27 through 212. The minimum similarity between any pair of MVCA assignments for ï¬ngerprints\nof different sizes was 93%. The minimum similarity between SMILES string assignments and\nï¬ngerprint assignments was 92% for the 128-bit ï¬ngerprints.\nUnsupervised clustering of reaction ï¬ngerprints shows that the calculated similarity in molecular\nrepresentations may not align with the similarity assessed by a domain expert, and it is important\nto consider the similarity of the data according to its representation using an appropriate metric.\n7\nChoosing a cross-validation scheme according to human intuition may therefore lead to overï¬t\nmodels when the molecular representations are quantitatively similar but heuristically dissimilar.\nSince overï¬t models have been shown to be a problem in chemoinformatic supervised learning\ntasks [Martin et al., 2017], we anticipate this method will be useful to the chemoinformatic machine\nlearning community.\n4\nDiscussion\nIn this report, we present the unsupervised learning of peptide dynamics and of molecular structures\nusing MVCA for hierarchical agglomerative clustering with Wardâ€™s minimum variance objective\nfunction. We ï¬rst demonstrate MVCA with the Jensen-Shannon divergence between MSM transition\nmatrices to identify dynamical groups from a dataset of 42 tripeptides containing proline. Then, we\napply MVCA in a completely new way, using the Levenshtein edit distance ratio between SMILES\nrepresentations of small molecules, and the Tanimoto coefï¬cient between reaction ï¬ngerprints, to par-\ntition the datasets for cross-validation. This analysis is intended to address a knowledge gap speciï¬c\nto supervised machine learning for chemoinformatic analyses designed to predict the properties of\nnovel molecules: namely, how to perform cross-validation such that model generalizability is properly\nmeasured. Here, we suggest constructing training, validation, and test sets for model cross-validation\naccording to minimum variance in order to assess model performance with a practical test set; i.e.,\none that contains newly designed compounds. While common machine learning wisdom dictates that\nthe training, validation, and test sets should be drawn from the same distribution, for the prediction of\na novel compoundâ€™s chemical properties it is crucial for us to demonstrate that models can generalize\nto new kinds of molecules. If it turns out to be the case that maximally novel test sets break such\nmodels, then it is important for the ï¬eld as a whole to question whether supervised machine learning\napproaches, in particular those with deep neural network architectures, are appropriate for predicting\nchemical properties of new compounds.\nMachine learning plays a crucial role in both modern MD analyses and the prediction of molecular\nproperties from structure. We thus anticipate that MVCA will be broadly applicable to various\napplications in machine learning for molecular data. The MVCA algorithm is available in the open-\nsource MSMBuilder software package [Harrigan et al., 2017], which was used to build the MSMs\nin Sec. 3.1. The MVCA analyses presented in this report can also be implemented using the SciPy\nPython package [Jones et al., 2001].\nAcknowledgments\nB.E.H. is grateful to Evan Feinberg, Zhenqin Wu, and Bowen Liu for discussions during the prepara-\ntion of this manuscript. The authors thank Matt Harrigan for providing the tripeptide dataset. We\nacknowledge the National Institutes of Health under No. NIH R01-GM62868 for funding. V.S.P. is\na consultant & SAB member of Schrodinger, LLC and Globavir, sits on the Board of Directors of\nApeel Inc, Freenome Inc, Omada Health, Patient Ping, Rigetti Computing, and is a General Partner at\nAndreessen Horowitz. An earlier version of this paper was accepted to the Machine Learning for\nMolecules and Materials Workshop at NIPS 2017 in Long Beach, CA. The workshop proceedings\ncan be found at http://www.quantum-machine.org/workshops/nips2017/.\nReferences\n[Adcock and McCammon, 2006] Adcock, S. A. and McCammon, J. A. (2006). Molecular dynamics survey of\nmethods for simulating the activity of proteins. Chem. Rev., 106(5):1589â€“1615.\n[Bemis and Murcko, 1996] Bemis, G. W. and Murcko, M. A. (1996). The properties of known drugs. 1.\nmolecular frameworks. J. Med. Chem., 39(15):2887â€“2893.\n[Bowman et al., 2010] Bowman, G. R., Ensign, D. L., and Pande, V. S. (2010). Enhanced modeling via network\ntheory: Adaptive sampling of Markov state models. J. Chem. Theory Comput., 6(3):787â€“794.\n[Bowman et al., 2014] Bowman, G. R., Pande, V. S., and NoÃ©, F. (2014). An introduction to Markov state\nmodels and their application to long timescale molecular simulation. volume 797. Springer.\n[Buch et al., 2010] Buch, I., Harvey, M. J., Giorgino, T., Anderson, D. P., and Fabritiis, G. D. (2010). High-\nthroughput all-atom molecular dynamics simulations using distributed computing. J. Chem. Inf. Model.,\n50(3):397â€“403.\n8\n[Delaney, 2004] Delaney, J. S. (2004). Esol: Estimating aqueous solubility directly from molecular structure. J.\nChem. Inf. Comput. Sci., 44(3):1000â€“1005.\n[Endres and Schindelin, 2003] Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability\ndistributions. IEEE Transactions on Information Theory, 49(7):1858â€“1860.\n[Gomes et al., 2017] Gomes, J., Ramsundar, B., Feinberg, E. N., and Pande, V. S. (2017). Atomic convolutional\nnetworks for predicting protein-ligand binding afï¬nity. arXiv preprint arXiv:1703.10603.\n[Harrigan et al., 2017] Harrigan, M. P., Sultan, M. M., HernÃ¡ndez, C. X., Husic, B. E., Eastman, P., Schwantes,\nC. R., Beauchamp, K. A., McGibbon, R. T., and Pande, V. S. (2017). MSMBuilder: Statistical models for\nbiomolecular dynamics. Biophys. J., 112(1):10â€“15.\n[Husic et al., 2017] Husic, B. E., McKiernan, K. A., Wayment-Steele, H. K., Sultan, M. M., and Pande, V. S.\n(2017). A minimum variance clustering approach produces robust and interpretable coarse-grained models. J.\nChem. Theory Comput. Just Accepted Manuscript.\n[Husic and Pande, 2017] Husic, B. E. and Pande, V. S. (2017). Ward clustering improves cross-validated\nMarkov state models of protein folding. J. Chem. Theory Comput., 13(3):963â€“967.\n[Jones et al., 2001] Jones, E., Oliphant, T., Peterson, P., et al. (2001). SciPy: Open source scientiï¬c tools for\nPython.\n[Kearnes et al., 2016] Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. (2016). Molecular graph\nconvolutions: moving beyond ï¬ngerprints. J. Comput. Aided Mol. Des., 30(8):595â€“608.\n[Kohlhoff et al., 2014] Kohlhoff, K. J., Shukla, D., Lawrenz, M., Bowman, G. R., Konerding, D. E., Belov,\nD., Altman, R. B., and Pande, V. S. (2014). Cloud-based simulations on Google Exacycle reveal ligand\nmodulation of GPCR activation pathways. Nature Chem., 6(1):15â€“21.\n[Lin, 1991] Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on\nInformation Theory, 37(1):145â€“151.\n[Liu et al., 2017] Liu, B., Ramsundar, B., Kawthekar, P., Shi, J., Gomes, J., Nguyen, Q. L., Ho, S., Sloane, J.,\nWender, P., and Pande, V. (2017). Retrosynthetic reaction prediction using neural sequence-to-sequence\nmodels. arXiv preprint arXiv:1706.01643.\n[Martin et al., 2017] Martin, E. J., Polyakov, V. R., Tian, L., and Perez, R. C. (2017). Proï¬le-QSAR 2.0: Kinase\nvirtual screening accuracy comparable to four-concentration IC50s for realistically novel compounds. J.\nChem. Inf. Model., 57(8):2077â€“2088.\n[McCammon et al., 1977] McCammon, J. A., Gelin, B. R., and Karplus, M. (1977). Dynamics of folded\nproteins. Nature, 267(5612):585â€“590.\n[McKiernan et al., 2017] McKiernan, K. A., Husic, B. E., and Pande, V. S. (2017). Modeling the mechanism of\ncln025 beta-hairpin formation. J. Chem. Phys., 147(10):104107.\n[MÃ¼llner, 2013] MÃ¼llner, D. (2013). fastcluster: Fast hierarchical, agglomerative clustering routines for R and\nPython. J. Stat. Soft., 53(1):1â€“18.\n[Olsson et al., 2017] Olsson, S., Wu, H., Paul, F., Clementi, C., and NoÃ©, F. (2017). Combining experimental and\nsimulation data of molecular processes via augmented Markov models. Proc. Natl. Acad. Sci., 114(31):8265â€“\n8270.\n[Plattner et al., 2017] Plattner, N., Doerr, S., De Fabritiis, G., and NoÃ©, F. (2017). Complete proteinâ€“protein\nassociation kinetics in atomic detail revealed by molecular dynamics simulations and markov modelling. Nat.\nChem., 9(10):1005â€“1011.\n[Schneider et al., 2016] Schneider, N., Stieï¬‚, N., and Landrum, G. A. (2016). Whatâ€™s what: The (nearly)\ndeï¬nitive guide to reaction role assignment. J. Chem. Inf. Model., 56(12):2336â€“2346.\n[Shaw et al., 2008] Shaw, D. E., Deneroff, M. M., Dror, R. O., Kuskin, J. S., Larson, R. H., Salmon, J. K.,\nYoung, C., Batson, B., Bowers, K. J., Chao, J. C., Eastwood, M. P., Gagliardo, J., Grossman, J. P., Ho, C. R.,\nIerardi, D. J., KolossvÃ¡ry, I., Klepeis, J. L., Layman, T., McLeavey, C., Moraes, M. A., Mueller, R., Priest,\nE. C., Shan, Y., Spengler, J., Theobald, M., Towles, B., and Wang, S. C. (2008). Anton, a special-purpose\nmachine for molecular dynamics simulation. Commun. ACM, 51(7):91â€“97.\n[Shirts and Pande, 2000] Shirts, M. and Pande, V. S. (2000). Screen savers of the world unite!\nScience,\n290(5498):1903â€“1904.\n[Vitalini et al., 2016] Vitalini, F., NoÃ©, F., and Keller, B. (2016). Molecular dynamics simulations data of the\ntwenty encoded amino acids in different force ï¬elds. Data in Brief, 7:582â€“590.\n[Ward, 1963] Ward, J. H. (1963). Hierarchical grouping to optimize an objective function. J. Amer. Statist.\nAssoc., 58(301):236â€“244.\n[Zhou et al., 2017] Zhou, G., Pantelopulos, G. A., Mukherjee, S., and Voelz, V. A. (2017). Bridging microscopic\nand macroscopic mechanisms of p53-MDM2 binding with kinetic network models. Biophys. J., 113(4):785â€“\n793.\n9\nA\nClustering with other objective functions\nIt is interesting to contrast single, average, and complete linkage functions for hierarchical\nagglomerative clustering with Wardâ€™s method.\nThe following analysis was performed using\nScipy [Jones et al., 2001].\npro-gly-trp\nthr-gly-pro\npro-ser-thr\npro-ser-gly\npro-ala-val\npro-cys-lys\npro-phe-glu\npro-ile-gly\npro-tyr-thr\npro-asp-phe\npro-gln-glu\npro-leu-val\npro-glu-thr\npro-leu-phe\ngly-ile-pro\nhis-ala-pro\nglu-arg-pro\nlys-leu-pro\nleu-glu-pro\nglu-ala-pro\nile-ala-pro\nval-ser-pro\ntyr-asp-pro\nglu-gln-pro\ntyr-lys-pro\nthr-ile-pro\ngly-ala-pro\npro-pro-tyr\ngly-pro-cys\nasp-pro-glu\narg-pro-asp\ngln-pro-phe\npro-pro-gly\nala-pro-ser\nser-pro-gln\nile-pro-leu\nser-pro-ile\nile-pro-ala\nlys-pro-leu\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\n5\n10\n15\nShortest distance\npro-gly-trp\nthr-gly-pro\npro-ser-thr\npro-ser-gly\npro-ala-val\npro-cys-lys\npro-phe-glu\npro-ile-gly\npro-tyr-thr\npro-asp-phe\npro-leu-val\npro-gln-glu\npro-leu-phe\npro-glu-thr\ngly-ile-pro\npro-pro-tyr\narg-pro-asp\npro-pro-gly\ngly-pro-cys\nasp-pro-glu\ngln-pro-phe\nile-pro-leu\nala-pro-ser\nser-pro-ile\nser-pro-gln\nile-pro-ala\nlys-pro-leu\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\nhis-ala-pro\nlys-leu-pro\ntyr-asp-pro\ntyr-lys-pro\nglu-arg-pro\nval-ser-pro\nleu-glu-pro\ngly-ala-pro\nglu-gln-pro\nthr-ile-pro\nglu-ala-pro\nile-ala-pro\n5\n10\n15\nAverage distance\npro-gly-trp\nthr-gly-pro\npro-ser-gly\npro-ser-thr\npro-ala-val\npro-cys-lys\npro-phe-glu\npro-tyr-thr\npro-ile-gly\npro-leu-phe\npro-leu-val\npro-asp-phe\ngly-pro-cys\nasp-pro-glu\ngln-pro-phe\nser-pro-gln\nser-pro-ile\npro-pro-tyr\narg-pro-asp\npro-pro-gly\nile-pro-leu\nala-pro-ser\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\nile-pro-ala\nlys-pro-leu\ngly-ile-pro\nlys-leu-pro\nval-ser-pro\nglu-ala-pro\nile-ala-pro\nhis-ala-pro\ntyr-asp-pro\ntyr-lys-pro\nglu-arg-pro\nleu-glu-pro\ngly-ala-pro\nglu-gln-pro\nthr-ile-pro\npro-gln-glu\npro-glu-thr\n5\n10\n15\nGreatest distance\nFigure 4: Dendrograms created from clustering 42 transition matrices using Dâˆš\nJS and single (top),\naverage (center), and complete (bottom) linkage objective functions. These dendrograms, which do\nnot identify intuitively natural groups, can be contrasted with Fig. 2 (top). In all three cases, the\nclustering algorithm cannot separate the system dynamics according to the location of the proline,\nand can only separate the two glycine-containing tripeptides from the other 40 systems. All three\nalternative objective functions identify many other singletons, and no coherent groups can be obtained\nfrom the results.\n10\nFurane\nMethyl hydrazine\nNitromethane\nMethane\nAcetonitrile\nPropionitrile\nAcrylonitrile\n1,2-Propylene oxide\nAcetamide\nUrea\nEthanethiol\nDimethyl sulfide\nEthane\nMethanol\ntrans-2-Pentene \ncis-2-Pentene\nEthanol\nEthyne\nEthylene\nMethyl acetate\n2-Butanone\n2-Methyl-1,3-Butadiene \n3-Methyl-1-Butene\nButan-2-ol\n2-Propanol\n2-Methylpropan-1-ol\n2-Methylpropane\n2-Methy-2-Butene\n2-Methylbutane\n2-Methylpropene\n2-Methyl-1-Butene\nPropane\nt-Crotonaldehyde\nChloroethane\nChloroethylene\nPropylene\nMethyl propyl ether \n1-Propanol\nMethyl formate\nEthyl formate\nPropionaldehyde\n1-Butanol\nAcrolein\nButyraldehyde\n2-butenal\nButane\nPropyne\nDiethyl ether \n1-Butyne\n1-Pentyne\nEthyl vinyl ether\n1-Butene\nPentane\n1,3-Butadiene\n1-Pentene \n1,4-Pentadiene \nTetrahydrofurane \nCyclopentane \nCyclopentene \n0.0\n0.2\n0.4\n0.6\nShortest distance\nFurane\nMethyl hydrazine\nEthanethiol\nEthylene\nDimethyl sulfide\nEthane\nMethane\nMethanol\nEthanol\nNitromethane\n2-Butanone\nMethyl acetate\nAcetamide\nUrea\nAcetonitrile\nPropionitrile\nAcrylonitrile\n1,2-Propylene oxide\nTetrahydrofurane \nCyclopentane \nCyclopentene \n2-Methyl-1,3-Butadiene \n3-Methyl-1-Butene\n2-Propanol\nButan-2-ol\n2-Methylpropane\n2-Methylpropan-1-ol\n2-Methylpropene\n2-Methyl-1-Butene\n2-Methy-2-Butene\n2-Methylbutane\ntrans-2-Pentene \ncis-2-Pentene\nMethyl formate\nEthyl formate\nt-Crotonaldehyde\nButyraldehyde\n2-butenal\nAcrolein\nPropionaldehyde\nEthyne\nPropyne\n1-Butyne\nChloroethylene\nChloroethane\nPropane\nPropylene\n1-Butanol\n1-Propanol\nMethyl propyl ether \nButane\nEthyl vinyl ether\nDiethyl ether \nPentane\n1-Pentyne\n1-Pentene \n1,4-Pentadiene \n1,3-Butadiene\n1-Butene\n0.0\n0.5\n1.0\nAverage distance\nFurane\nAcetonitrile\nPropionitrile\nAcrylonitrile\nEthyne\nPropyne\n1-Butyne\nEthylene\nEthanethiol\nDimethyl sulfide\nEthane\nMethyl formate\nEthyl formate\nt-Crotonaldehyde\nButyraldehyde\n2-butenal\nAcrolein\nPropionaldehyde\nChloroethylene\nChloroethane\nPropane\nPropylene\n1,2-Propylene oxide\n1-Butanol\n1-Propanol\nMethyl propyl ether \nButane\n2-Propanol\nButan-2-ol\n2-Methy-2-Butene\n2-Methylbutane\n2-Methylpropene\n2-Methyl-1-Butene\n2-Methylpropane\n2-Methylpropan-1-ol\nEthyl vinyl ether\nDiethyl ether \nTetrahydrofurane \nCyclopentane \nCyclopentene \n2-Methyl-1,3-Butadiene \n3-Methyl-1-Butene\n1-Pentene \n1,4-Pentadiene \n1,3-Butadiene\n1-Butene\nPentane\n1-Pentyne\ntrans-2-Pentene \ncis-2-Pentene\nMethyl hydrazine\nMethane\nMethanol\nEthanol\nNitromethane\n2-Butanone\nMethyl acetate\nAcetamide\nUrea\n0.0\n0.5\n1.0\nGreatest distance\nFigure 5: Dendrogram created from clustering 59 small molecules using Levenshtein distance\nbetween their SMILES representations and single (top), average (center), and complete (bottom)\nlinkage objective functions. These dendrograms, which do not identify intuitively natural groups, can\nbe contrasted with Fig. 3 (top). While all three objective functions identiï¬ed furane as a singleton,\nthey produce no easily identiï¬able groups.\n11\n",
  "categories": [
    "physics.bio-ph",
    "q-bio.BM",
    "q-bio.QM",
    "stat.ML"
  ],
  "published": "2017-12-20",
  "updated": "2017-12-20"
}