{
  "id": "http://arxiv.org/abs/1712.07704v1",
  "title": "Unsupervised learning of dynamical and molecular similarity using variance minimization",
  "authors": [
    "Brooke E. Husic",
    "Vijay S. Pande"
  ],
  "abstract": "In this report, we present an unsupervised machine learning method for\ndetermining groups of molecular systems according to similarity in their\ndynamics or structures using Ward's minimum variance objective function. We\nfirst apply the minimum variance clustering to a set of simulated tripeptides\nusing the information theoretic Jensen-Shannon divergence between Markovian\ntransition matrices in order to gain insight into how point mutations affect\nprotein dynamics. Then, we extend the method to partition two chemoinformatic\ndatasets according to structural similarity to motivate a train/validation/test\nsplit for supervised learning that avoids overfitting.",
  "text": "Unsupervised learning of dynamical and molecular\nsimilarity using variance minimization\nBrooke E. Husic and Vijay S. Pande\nDepartment of Chemistry\nStanford University\nStanford, CA 94305\n{bhusic, pande}@stanford.edu\nAbstract\nIn this report, we present an unsupervised machine learning method for determining\ngroups of molecular systems according to similarity in their dynamics or structures\nusing Ward’s minimum variance objective function. We ﬁrst apply the minimum\nvariance clustering to a set of simulated tripeptides using the information theoretic\nJensen-Shannon divergence between Markovian transition matrices in order to\ngain insight into how point mutations affect protein dynamics. Then, we extend\nthe method to partition two chemoinformatic datasets according to structural sim-\nilarity to motivate a train/validation/test split for supervised learning that avoids\noverﬁtting.\n1\nIntroduction\nScientists have sought to understand the dynamical behavior of proteins at atomic resolution since\nthe ﬁrst molecular dynamics (MD) simulation of the 58-amino acid bovine pancreatic trypsin in-\nhibitor (BPTI) in 1977 [McCammon et al., 1977]. In the past 40 years, computational chemists have\nseen major improvements in molecular dynamics methods [Adcock and McCammon, 2006], and\nmodern datasets can reach biologically relevant timescales (tens of milliseconds using femtosecond\ntime steps) due to specialized hardware [Shaw et al., 2008] and distributed computing platforms\nsuch as Folding@home [Shirts and Pande, 2000], GPUGRID [Buch et al., 2010], and Google Exacy-\ncle [Kohlhoff et al., 2014].\nThe enormous size of modern MD datasets requires complementary methods to understand\nand analyze the data in a statistically rigorous way.\nMarkov state models (MSMs) are a\npopular framework for this type of analysis that use a master equation to represent the ther-\nmodynamics and kinetics of a molecular system [Bowman et al., 2014].\nRecent advances in\nMSM applications include complex, multi-system dynamics such as protein-protein associa-\ntion [Plattner et al., 2017, Zhou et al., 2017] or aggregated datasets containing simulations in multiple\nforce ﬁelds [McKiernan et al., 2017, Olsson et al., 2017]. It is thus necessary to develop comple-\nmentary tools for understanding these types of aggregated datasets and quantifying the dynamical\nsimilarity among the different systems. Minimum variance cluster analysis (MVCA), a recently\nintroduced unsupervised learning method to coarse-grain a single MSM, has the versatility to be used\nboth within a single model (for coarse-graining) and among a set of models for dynamical cluster-\ning [Husic et al., 2017]. While the authors focus on the coarse-graining application, they conclude\nwith a motivation of the latter application in which they analyze separate folding simulations of a\nsmall protein in nine different protein and water force ﬁeld combinations.\nIn this report, we focus on the ability of the unsupervised MVCA algorithm to identify groups of\nmolecular systems. We ﬁrst provide a theoretical background of the MSM transition matrix in order\nto motivate the development of MVCA for identifying dynamical groups. We then demonstrate the\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1712.07704v1  [physics.bio-ph]  20 Dec 2017\nmethod on MSMs of capped amino acids to gain insight into protein dynamics for larger systems.\nFinally, we showcase the versatility of MVCA by applying it to a completely different problem: the\nselection of training, validation, and test sets for cross-validating a chemoinformatic supervised\nlearning task.\n2\nTheory background\n2.1\nThe Markovian transition matrix\nWe begin with a continuous-time Markovian process, which means that the probability of transitioning\nfrom state x to state y after a time interval of τ does not depend on any states occupied before the\nsystem was in x. We assume this process is time-homogeneous, stochastic, and reversible with respect\nto its stationary distribution, µ. The process is also irreducible, which means that there is a path from\neach state to every other state given sufﬁcient time, and aperiodic. If we represent our system by a\nprobability distribution pt at time t, and the transition density kernel for x →y as p(x, y), we can\nwrite the new probability distribution at time t + τ as,\npt+τ(y)/µ(y) =\nZ\nΩ\ndx pt(x)p(x, y) = T (τ) ◦pt(y)/µ(y).\n(1)\nThe continuous transfer operator T , which is characterized by its lag time τ, is compact and self-\nadjoint with respect to the stationary distribution µ. The transfer operator admits a decomposition\ninto eigenvalues and eigenfunctions,\nT (τ) ◦ψi =λiψi,\n(2)\nwhere the eigenvalues λi are real and indexed in decreasing order. The eigenvalue λ1 = 1 is unique\nand corresponds to the stationary process. All subsequent eigenvalues are on the interval |λi>1| < 1\nand correspond to dynamical processes in the time series.\nTo build a MSM, we decompose the subset of conformation space explored by the MD simulation into\ndiscrete, disjoint states. By counting the transitions between these states, we calculate the maximum\nlikelihood estimator of the the transition probability matrix to obtain a discrete approximation to T .\nThis transition matrix is the MSM master equation.\n2.2\nDistance between MSM transition matrices\nTo cluster MSM transition matrices, we review the theory presented in the original MVCA pa-\nper [Husic et al., 2017]. For two MSMs with row-stochastic transition probability matrices P and\nQ, the divergence from the ith row of Q to the ith row of P can be written as the Kullback-Leibler\ndivergence,\ndivKL(Pi||Qi) ≡\nX\nj\nPi(j) log Pi(j)\nQi(j),\n(3)\nwhere Pi can be thought of as the “reference” distribution and Qi as a “test” distribu-\ntion [Bowman et al., 2010]. The information theoretic Jensen-Shannon divergence [Lin, 1991] is a\nrelated symmetric formulation that utilizes M, the elementwise mean of P and Q,\ndivJS(Pi||Qi) ≡1\n2divKL(Pi||Mi) + 1\n2divKL(Qi||Mi).\n(4)\nSince\nit\nhas\nbeen\nshown\nthat\nthe\nsquare\nroot\nof\n(4)\nobeys\nthe\ntriangle\ninequal-\nity [Endres and Schindelin, 2003], we can write the following distance metric,\n2\ndiv√\nJS(Pi||Qi) ≡\np\ndivJS(Pi||Qi).\n(5)\nFinally, for a scalar distance between the two transition matrices P and Q, each of which contains i\nrows, we deﬁne the sum [Husic et al., 2017],\nD√\nJS ≡\nX\ni\ndiv√\nJS(Pi||Qi).\n(6)\n2.3\nHierarchical agglomerative clustering\nHierarchical agglomerative clustering is an unsupervised learning method that is initiated with a\nset of pairwise distances between data points and iteratively merges the two closest clusters or\nsingletons. Hierarchical agglomerative clustering thus requires a similarity function that quantiﬁes\nthe distance between all data points and an objective function that determines how to use those\ndistances to determine which existing clusters to merge at each agglomerating step. Common\nexamples of objective functions used for hierarchical agglomerative clustering are single, average,\nand complete linkages, which deﬁne the distance between two clusters as the shortest, average, or\ngreatest distance, respectively, between any point in one cluster and any point in the other cluster.\nAnother objective function used for hierarchical agglomerative clustering is Ward’s minimum variance\ncriterion [Ward, 1963]. The agglomeration is performed by merging the two clusters or singletons\nsuch that the resulting increase in intra-cluster variance is minimized. Ward’s method is usually\nimplemented according to the following recursive distance update [Müllner, 2013],\nd(u, v) =\nr\n|v| + |s|\nT\nd(v, s)2 + |v| + |t|\nT\nd(v, t)2 −|v|\nT d(s, t)2,\n(7)\nwhere the clusters s and t have just been merged to create a new cluster, u, and the new distance\nbetween u and some other cluster v needs to be updated. |c| represents the number of data points\ncontained in cluster c, and T ≡|s| + |t| + |v|. A nonrecursive formula for the distance update when s\nor t is a singleton has also been derived [Husic and Pande, 2017]. We note that the minimum variance\nformulation is rigorously deﬁned for Euclidean distances, but that the objective function can be used\nfor any similarity function with the understanding that it no longer corresponds to Euclidean variance,\nwhich requires the l2-norm.\nIn this paper, we ﬁrst use hierarchical agglomerative clustering with Ward’s minimum variance\nobjective function and the D√\nJS similarity function (6) to quantify the similarity between multiple\nmodels for related dynamical systems. We then apply Ward’s minimum variance objective function\nwith different similarity functions to the hierarchical clustering of molecular structures and reaction\nﬁngerprints in order to motivate a cross-validation scheme for a supervised learning model.\n3\nResults\n3.1\nClustering dynamically restrained tripeptides\nIt is often desirable to modify the dynamics of a protein by introducing a sequence mutation,\ni.e. substituting a selected amino acid for the one present in the wild type protein. Simulating mutated\nproteins using MD can provide a window into whether or not the mutation has affected the protein\ndynamics. The MVCA algorithm can be used to cluster a set of mutants based on their dynamics,\nand can lend insight into which mutations produce similar effects. It is known that all solvated\namino acids except glycine and proline occupy certain regions of the space deﬁned by their ϕ and\nψ backbone dihedral angles [Vitalini et al., 2016]. Figure 1 shows the ϕ and ψ angles on alanine\n(left), the structure of which can be compared with glycine (center) and proline (right). Glycine, the\nsmallest amino acid, occupies more conformations due to its ﬂexibility, and proline occupies fewer\nconformations due to its 5-member ring.\n3\n𝛗 \n𝛙 \nala\ngly\npro\nFigure 1: Amino acids are differentiated by their side chains, and the dihedrals about the bonds\nadjacent to the side chains (light blue, left) occupy known regions of ϕ × ψ space. Alanine (left) is\nan amino acid with a standard range of side chain motion. Glycine (center) is more ﬂexible because\nit does not have a side chain. Proline (right) is less ﬂexible because its side chain forms a ring.\nTo demonstrate MVCA on a dataset for which we can visualize the dynamical degrees of freedom,\nwe consider a set of 42 proline-containing tripeptides simulated for 1 µs each in the AMBER ff99SB-\nILDN force ﬁeld at 300 K. We hypothesize that the dynamics of the central amino acid in a tripeptide\nare affected by the presence of proline at any of the three positions. To investigate the number of\ndynamical groups represented by this tripeptide dataset, we ﬁrst create a MSM at a 100 ps lag time for\neach system using a regular spatial clustering of the ϕ and ψ angles of the central amino acid. Since\nall MSMs were built using the same state deﬁnitions, we can assess the similarity of their transition\nmatrices using MVCA with D√\nJS. The MVCA analysis shows that the 42 tripeptides cluster into ﬁve\nnatural groups, which is clear from a dendrogram representation of the hierarchical tree (Fig. 2, top).\nPlotting the free energy surfaces of the central amino acid for each tripeptide shows that the energy\nlandscapes within each cluster are very similar. Three large clusters identify tripeptides with proline\nas the ﬁrst, second, and third amino acid (Fig. 2; bottom; blue, purple, and green, respectively). Two\nsingleton clusters are also identiﬁed, which represent the two systems in the dataset for which glycine\nis the central amino acid (Fig. 2, gray). It is clear from their free energy landscapes that these systems\nare dynamically very different from the others.\nWe suspect that the systems will differentiate according to the location of the proline because it is\neasy to understand the degrees of freedom for this simple system. This analysis is important because\nit has ﬁdelity to the expected result, and can successfully group the tripeptides based on only the\ntransition matrices of their MSMs. The analysis does not produce a reasonable result when the single,\naverage, or complete linkage objective function is used instead of Ward’s minimum variance objective\nfunction (see Appendix A).\n3.2\nClustering small molecules for supervised machine learning\nPredicting properties from molecular structures such as solubility and binding afﬁnity is a signiﬁcant\nchallenge, and state of the art approaches such as atomic convolutional networks [Gomes et al., 2017]\nand graph convolutions [Kearnes et al., 2016] have been used for supervised learning of these quanti-\nties. When using supervised learning to predict molecular properties from a representation of the\nmolecular structure, it is important to use cross-validation when assessing the model’s accuracy. A\nstandard cross-validation split involves dividing the labeled data into three sets: training, validation,\nand test. The model is trained on the training set and is evaluated on the validation set during\ndevelopment. Once hyperparameters have been selected, the ﬁnal model is evaluated on the test set.\nFor chemoinformatic models designed to predict the property of extremely novel new compounds,\ncare must be taken in choosing how to divide the data into these three sets such that the model is\nnot overﬁt to the training data. When using neural network architectures, for example, the goal is to\nproduce a model that has learned some complex underlying feature from the data, but not a model that\nhas memorized every data point and simply recalls what it has memorized. A train/validation/test split\nmotivated by differentiating these two options is therefore critical for evaluating supervised learning\nmodels in the context of characterizing the properties of novel compounds. Common partitions for\nchemoinformatic studies include random splits, temporal splits, stratiﬁed splits according to the quan-\ntity being learned, and scaffold splits [Bemis and Murcko, 1996] in which molecules are assigned to\na set based on the frequency of the molecular scaffold. The latter method is generally difﬁcult for\nmodels with deep architectures, since the model must apply what it has learned to a different type of\n4\nile-ala-pro\nglu-ala-pro\ngly-ala-pro\nval-ser-pro\nhis-ala-pro\ntyr-asp-pro\ntyr-lys-pro\ngly-ile-pro\nlys-leu-pro\nglu-arg-pro\nleu-glu-pro\nglu-gln-pro\nthr-ile-pro\npro-leu-phe\npro-leu-val\npro-asp-phe\npro-gln-glu\npro-glu-thr\npro-ile-gly\npro-phe-glu\npro-tyr-thr\npro-ser-thr\npro-ser-gly\npro-cys-lys\npro-ala-val\npro-gly-trp\nthr-gly-pro\npro-pro-tyr\narg-pro-asp\ngly-pro-cys\nasp-pro-glu\ngln-pro-phe\nser-pro-gln\nser-pro-ile\npro-pro-gly\nile-pro-leu\nala-pro-ser\nile-pro-ala\nlys-pro-leu\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\n5\n10\n15\n20\n25\nVariance\nFigure 2: A dendrogram from clustering 42 transition matrices using D√\nJS and Ward’s minimum\nvariance objective function shows that the systems cluster into ﬁve groups (top). The energy landscape\nof each system is plotted for −180 < ϕ < 180 on the x-axis and −180 < ψ < 180 on the y-axis,\nwith darker colors representing more stable conformations (bottom). The ﬁve clusters are identiﬁed\nusing boxes corresponding to the colors in the dendrogram. Since we have analyzed a system for\nwhich the degrees of freedom are interpretable, we can see that the clustering analysis identiﬁes\ngroups with similar free energy surfaces.\ndata [Gomes et al., 2017]. A “realistically novel” training/test split for kinase virtual screening has\nalso been recently published with the goal of avoiding overﬁt models [Martin et al., 2017].\nIn the spirit of scaffold and “realistically novel” splittings, here we apply MVCA to choose the\ntraining, validation, and test subsets from a molecular database. Although it is much too small to use\nfor training a deep network with many hyperparameters, we present an example for a reduced group\nof molecules so we can interpret the results. We thus select the 59 compounds with molecular weight\nless than 75 g/mol from a dataset containing aqueous solubilities [Delaney, 2004]. In this dataset, the\nmolecules are represented by the simpliﬁed molecular-input line-entry system (SMILES), which are\none-dimensional string representations of molecular structures. To quantify the similarity between\neach pair of molecules, we use the Levenshtein distance ratio between the SMILES strings, which is\na function of the number of single character edits that must be made to convert one representation to\nthe other and is normalized for string length. Since the Levenshtein distance ratio ranges from 0 to\n1, where a value of 1 means the strings are identical, we encode the similarity between each pair of\nstrings as the Levenshtein distance ratio subtracted from 1 so that smaller values correspond to closer\nstrings. Then, we apply MVCA to the pairwise SMILES representations (instead of pairwise D√\nJS\n5\nFigure 3: A dendrogram from clustering 59 small molecules using Levenshtein distance between their\nSMILES representations and Ward’s minimum variance objective function shows that the molecules\ncluster into four groups (top). The solubilities of the groups are shown in the box plot (bottom).\nWe identify training (green), validation (blue), and test (purple) sets such that the development set\n(training and validation) and the test set each span the range of solubilities in the dataset. We note\nthat the structural groups separate by bond saturation, presence of nitrogen or sulfur, and molecular\nweight, but do not separate alkanes, alkenes, and alcohols. Furane is an outlier based on this analysis.\nvalues as in the previous section) and cluster the molecules according to Ward’s minimum variance\nobjective function.\nBased on the dendrogram representation (Fig. 3, top), we then partition the dataset into four groups.\nWe see from Fig. 3 (bottom) that three of the variance-minimized groups according to Levenshtein\ndistance among their SMILES representations contain 11, 25, and 22 molecules, and one group is\na singleton containing just furane. From this partitioning, we would use the largest group as the\ntraining set, the 11-member group as the validation set, and the 22-member group as the test set, since\nthe development set (i.e., training and validation) and the test set each span nearly the full range of\nsolubilities. We might choose to include furane in the test set. As in Sec. 3.1, the single, average, and\ncomplete linkage functions fail to produce suitable groups for this application (see Appendix A).\nWhat can we learn about molecules represented by SMILES strings from this analysis, and how can\nwe use it to improve the supervised prediction of molecular properties such as solubility for novel\ncompounds? We see that the training set contains mostly molecules with unsaturated bonds, more\nthan half of the validation set contains compounds with nitrogen or sulfur, and the test set contains\nlarger molecules (no molecular weight lower than 56 g/mol). However, we also see that all three\nnon-singleton sets contain alkanes, alkenes, and alcohols. In terms of training a supervised model,\n6\nTable 1: Distribution of reactions from 10 classes into two groups determined by MVCA using the\nTanimoto coefﬁcient between 4096-bit structural reaction ﬁngerprints. All 10 reaction classes are\ndivided amongst the two groups, and most group allocations are similar to the 78%/22% overall\nsplitting.\nClass\nReaction\nQuantity\nPercent in group 1\nPercent in group 2\n1\nHeteroatom alkylation and arylation\n569\n79\n21\n2\nAcylation and related processes\n208\n62\n38\n3\nCarbon-carbon bond formation\n133\n75\n25\n4\nHeterocycle formation\n31\n74\n26\n5\nProtections\n27\n93\n7\n6\nDeprotections\n244\n70\n30\n7\nReductions\n487\n83\n17\n8\nOxidations\n86\n78\n22\n9\nFunctional group interconversion\n208\n85\n15\n10\nFunctional group addition\n7\n71\n29\nTotal\n2000\n78\n22\nit is important to quantify which SMILES representations are the most similar and can be grouped\ntogether with Ward’s minimum variance objective function, and using these groups to design training,\nvalidation, and test sets will help determine if a model is suitable to predict properties for different\nkinds of molecular structures or if it is overﬁtting. Unlike scaffold splitting, this cross-validation\nframework is tuned speciﬁcally for the choice of molecular representation.\nAt a higher level, we can use such results to assess the advantages and drawbacks of a given\nmolecular representation by comparing the MVCA grouping to how a domain expert would\ngroup a set of molecules. To illustrate this, we ran a separate analysis of chemical reactions\nobtained from the United States patent literature and processed as described in [Liu et al., 2017]\nfor the prediction of reaction products.\nEach chemical reaction is classiﬁed into one of ten\nreaction types [Schneider et al., 2016].\nWe create structural ﬁngerprints for each reaction us-\ning the CreateStructuralFingerprintForReaction command in the RDKit Python pack-\nage [rdkit.org]. To quantify the similarity between reaction ﬁngerprints we use the Tanimoto\ncoefﬁcient subtracted from 1 so that smaller values correspond to closer distances, as above. We\nthen use MVCA to group the 2000 reactions with the lowest total molecular weight. The resulting\ndendrogram shows two natural groups with 78% of the reactions in one group and 22% of the\nreactions in the other group. When we calculate the percent allocation of each structural class in\nTable 1, we see that unsupervised clustering of reaction ﬁngerprints does not partition the reactions\naccording to their classes.\nFrom the perspective of a modeler seeking to design a train/validation/test split for predicting reaction\nproducts, there are two choices for designing a cross-validation scheme. First, the modeler can\ndetermine this split according to the reaction classes in order to represent different types of reactions\nin the train, validation, and test sets. However, this may result in an overﬁt model, since the validation\nand test sets will contain reactions with representations similar to those in the training set according\nto their Tanimoto coefﬁcients. Alternatively, the modeler can determine the split according to the\nsimilarity of reaction representations. In this case, the train, validation, and test sets would be chosen\naccording to minimum variance groupings of ﬁngerprints. For the reaction dataset analyzed here,\neach group would likely contain reactions from all 10 classes. Performing the same analysis with\nthe original reaction SMILES strings and the Levenshtein distance ratio produces nearly identical\nresults, since 98% of the group assignments are the same as in the ﬁngerprint analysis. This is\nexpected because the ﬁngerprints are generated from the SMILES strings. The ﬁngerprints used\nfor this analysis were 4096-bit, and the same analysis was performed for ﬁngerprints with sizes\n27 through 212. The minimum similarity between any pair of MVCA assignments for ﬁngerprints\nof different sizes was 93%. The minimum similarity between SMILES string assignments and\nﬁngerprint assignments was 92% for the 128-bit ﬁngerprints.\nUnsupervised clustering of reaction ﬁngerprints shows that the calculated similarity in molecular\nrepresentations may not align with the similarity assessed by a domain expert, and it is important\nto consider the similarity of the data according to its representation using an appropriate metric.\n7\nChoosing a cross-validation scheme according to human intuition may therefore lead to overﬁt\nmodels when the molecular representations are quantitatively similar but heuristically dissimilar.\nSince overﬁt models have been shown to be a problem in chemoinformatic supervised learning\ntasks [Martin et al., 2017], we anticipate this method will be useful to the chemoinformatic machine\nlearning community.\n4\nDiscussion\nIn this report, we present the unsupervised learning of peptide dynamics and of molecular structures\nusing MVCA for hierarchical agglomerative clustering with Ward’s minimum variance objective\nfunction. We ﬁrst demonstrate MVCA with the Jensen-Shannon divergence between MSM transition\nmatrices to identify dynamical groups from a dataset of 42 tripeptides containing proline. Then, we\napply MVCA in a completely new way, using the Levenshtein edit distance ratio between SMILES\nrepresentations of small molecules, and the Tanimoto coefﬁcient between reaction ﬁngerprints, to par-\ntition the datasets for cross-validation. This analysis is intended to address a knowledge gap speciﬁc\nto supervised machine learning for chemoinformatic analyses designed to predict the properties of\nnovel molecules: namely, how to perform cross-validation such that model generalizability is properly\nmeasured. Here, we suggest constructing training, validation, and test sets for model cross-validation\naccording to minimum variance in order to assess model performance with a practical test set; i.e.,\none that contains newly designed compounds. While common machine learning wisdom dictates that\nthe training, validation, and test sets should be drawn from the same distribution, for the prediction of\na novel compound’s chemical properties it is crucial for us to demonstrate that models can generalize\nto new kinds of molecules. If it turns out to be the case that maximally novel test sets break such\nmodels, then it is important for the ﬁeld as a whole to question whether supervised machine learning\napproaches, in particular those with deep neural network architectures, are appropriate for predicting\nchemical properties of new compounds.\nMachine learning plays a crucial role in both modern MD analyses and the prediction of molecular\nproperties from structure. We thus anticipate that MVCA will be broadly applicable to various\napplications in machine learning for molecular data. The MVCA algorithm is available in the open-\nsource MSMBuilder software package [Harrigan et al., 2017], which was used to build the MSMs\nin Sec. 3.1. The MVCA analyses presented in this report can also be implemented using the SciPy\nPython package [Jones et al., 2001].\nAcknowledgments\nB.E.H. is grateful to Evan Feinberg, Zhenqin Wu, and Bowen Liu for discussions during the prepara-\ntion of this manuscript. The authors thank Matt Harrigan for providing the tripeptide dataset. We\nacknowledge the National Institutes of Health under No. NIH R01-GM62868 for funding. V.S.P. is\na consultant & SAB member of Schrodinger, LLC and Globavir, sits on the Board of Directors of\nApeel Inc, Freenome Inc, Omada Health, Patient Ping, Rigetti Computing, and is a General Partner at\nAndreessen Horowitz. An earlier version of this paper was accepted to the Machine Learning for\nMolecules and Materials Workshop at NIPS 2017 in Long Beach, CA. The workshop proceedings\ncan be found at http://www.quantum-machine.org/workshops/nips2017/.\nReferences\n[Adcock and McCammon, 2006] Adcock, S. A. and McCammon, J. A. (2006). Molecular dynamics survey of\nmethods for simulating the activity of proteins. Chem. Rev., 106(5):1589–1615.\n[Bemis and Murcko, 1996] Bemis, G. W. and Murcko, M. A. (1996). The properties of known drugs. 1.\nmolecular frameworks. J. Med. Chem., 39(15):2887–2893.\n[Bowman et al., 2010] Bowman, G. R., Ensign, D. L., and Pande, V. S. (2010). Enhanced modeling via network\ntheory: Adaptive sampling of Markov state models. J. Chem. Theory Comput., 6(3):787–794.\n[Bowman et al., 2014] Bowman, G. R., Pande, V. S., and Noé, F. (2014). An introduction to Markov state\nmodels and their application to long timescale molecular simulation. volume 797. Springer.\n[Buch et al., 2010] Buch, I., Harvey, M. J., Giorgino, T., Anderson, D. P., and Fabritiis, G. D. (2010). High-\nthroughput all-atom molecular dynamics simulations using distributed computing. J. Chem. Inf. Model.,\n50(3):397–403.\n8\n[Delaney, 2004] Delaney, J. S. (2004). Esol: Estimating aqueous solubility directly from molecular structure. J.\nChem. Inf. Comput. Sci., 44(3):1000–1005.\n[Endres and Schindelin, 2003] Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability\ndistributions. IEEE Transactions on Information Theory, 49(7):1858–1860.\n[Gomes et al., 2017] Gomes, J., Ramsundar, B., Feinberg, E. N., and Pande, V. S. (2017). Atomic convolutional\nnetworks for predicting protein-ligand binding afﬁnity. arXiv preprint arXiv:1703.10603.\n[Harrigan et al., 2017] Harrigan, M. P., Sultan, M. M., Hernández, C. X., Husic, B. E., Eastman, P., Schwantes,\nC. R., Beauchamp, K. A., McGibbon, R. T., and Pande, V. S. (2017). MSMBuilder: Statistical models for\nbiomolecular dynamics. Biophys. J., 112(1):10–15.\n[Husic et al., 2017] Husic, B. E., McKiernan, K. A., Wayment-Steele, H. K., Sultan, M. M., and Pande, V. S.\n(2017). A minimum variance clustering approach produces robust and interpretable coarse-grained models. J.\nChem. Theory Comput. Just Accepted Manuscript.\n[Husic and Pande, 2017] Husic, B. E. and Pande, V. S. (2017). Ward clustering improves cross-validated\nMarkov state models of protein folding. J. Chem. Theory Comput., 13(3):963–967.\n[Jones et al., 2001] Jones, E., Oliphant, T., Peterson, P., et al. (2001). SciPy: Open source scientiﬁc tools for\nPython.\n[Kearnes et al., 2016] Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. (2016). Molecular graph\nconvolutions: moving beyond ﬁngerprints. J. Comput. Aided Mol. Des., 30(8):595–608.\n[Kohlhoff et al., 2014] Kohlhoff, K. J., Shukla, D., Lawrenz, M., Bowman, G. R., Konerding, D. E., Belov,\nD., Altman, R. B., and Pande, V. S. (2014). Cloud-based simulations on Google Exacycle reveal ligand\nmodulation of GPCR activation pathways. Nature Chem., 6(1):15–21.\n[Lin, 1991] Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions on\nInformation Theory, 37(1):145–151.\n[Liu et al., 2017] Liu, B., Ramsundar, B., Kawthekar, P., Shi, J., Gomes, J., Nguyen, Q. L., Ho, S., Sloane, J.,\nWender, P., and Pande, V. (2017). Retrosynthetic reaction prediction using neural sequence-to-sequence\nmodels. arXiv preprint arXiv:1706.01643.\n[Martin et al., 2017] Martin, E. J., Polyakov, V. R., Tian, L., and Perez, R. C. (2017). Proﬁle-QSAR 2.0: Kinase\nvirtual screening accuracy comparable to four-concentration IC50s for realistically novel compounds. J.\nChem. Inf. Model., 57(8):2077–2088.\n[McCammon et al., 1977] McCammon, J. A., Gelin, B. R., and Karplus, M. (1977). Dynamics of folded\nproteins. Nature, 267(5612):585–590.\n[McKiernan et al., 2017] McKiernan, K. A., Husic, B. E., and Pande, V. S. (2017). Modeling the mechanism of\ncln025 beta-hairpin formation. J. Chem. Phys., 147(10):104107.\n[Müllner, 2013] Müllner, D. (2013). fastcluster: Fast hierarchical, agglomerative clustering routines for R and\nPython. J. Stat. Soft., 53(1):1–18.\n[Olsson et al., 2017] Olsson, S., Wu, H., Paul, F., Clementi, C., and Noé, F. (2017). Combining experimental and\nsimulation data of molecular processes via augmented Markov models. Proc. Natl. Acad. Sci., 114(31):8265–\n8270.\n[Plattner et al., 2017] Plattner, N., Doerr, S., De Fabritiis, G., and Noé, F. (2017). Complete protein–protein\nassociation kinetics in atomic detail revealed by molecular dynamics simulations and markov modelling. Nat.\nChem., 9(10):1005–1011.\n[Schneider et al., 2016] Schneider, N., Stieﬂ, N., and Landrum, G. A. (2016). What’s what: The (nearly)\ndeﬁnitive guide to reaction role assignment. J. Chem. Inf. Model., 56(12):2336–2346.\n[Shaw et al., 2008] Shaw, D. E., Deneroff, M. M., Dror, R. O., Kuskin, J. S., Larson, R. H., Salmon, J. K.,\nYoung, C., Batson, B., Bowers, K. J., Chao, J. C., Eastwood, M. P., Gagliardo, J., Grossman, J. P., Ho, C. R.,\nIerardi, D. J., Kolossváry, I., Klepeis, J. L., Layman, T., McLeavey, C., Moraes, M. A., Mueller, R., Priest,\nE. C., Shan, Y., Spengler, J., Theobald, M., Towles, B., and Wang, S. C. (2008). Anton, a special-purpose\nmachine for molecular dynamics simulation. Commun. ACM, 51(7):91–97.\n[Shirts and Pande, 2000] Shirts, M. and Pande, V. S. (2000). Screen savers of the world unite!\nScience,\n290(5498):1903–1904.\n[Vitalini et al., 2016] Vitalini, F., Noé, F., and Keller, B. (2016). Molecular dynamics simulations data of the\ntwenty encoded amino acids in different force ﬁelds. Data in Brief, 7:582–590.\n[Ward, 1963] Ward, J. H. (1963). Hierarchical grouping to optimize an objective function. J. Amer. Statist.\nAssoc., 58(301):236–244.\n[Zhou et al., 2017] Zhou, G., Pantelopulos, G. A., Mukherjee, S., and Voelz, V. A. (2017). Bridging microscopic\nand macroscopic mechanisms of p53-MDM2 binding with kinetic network models. Biophys. J., 113(4):785–\n793.\n9\nA\nClustering with other objective functions\nIt is interesting to contrast single, average, and complete linkage functions for hierarchical\nagglomerative clustering with Ward’s method.\nThe following analysis was performed using\nScipy [Jones et al., 2001].\npro-gly-trp\nthr-gly-pro\npro-ser-thr\npro-ser-gly\npro-ala-val\npro-cys-lys\npro-phe-glu\npro-ile-gly\npro-tyr-thr\npro-asp-phe\npro-gln-glu\npro-leu-val\npro-glu-thr\npro-leu-phe\ngly-ile-pro\nhis-ala-pro\nglu-arg-pro\nlys-leu-pro\nleu-glu-pro\nglu-ala-pro\nile-ala-pro\nval-ser-pro\ntyr-asp-pro\nglu-gln-pro\ntyr-lys-pro\nthr-ile-pro\ngly-ala-pro\npro-pro-tyr\ngly-pro-cys\nasp-pro-glu\narg-pro-asp\ngln-pro-phe\npro-pro-gly\nala-pro-ser\nser-pro-gln\nile-pro-leu\nser-pro-ile\nile-pro-ala\nlys-pro-leu\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\n5\n10\n15\nShortest distance\npro-gly-trp\nthr-gly-pro\npro-ser-thr\npro-ser-gly\npro-ala-val\npro-cys-lys\npro-phe-glu\npro-ile-gly\npro-tyr-thr\npro-asp-phe\npro-leu-val\npro-gln-glu\npro-leu-phe\npro-glu-thr\ngly-ile-pro\npro-pro-tyr\narg-pro-asp\npro-pro-gly\ngly-pro-cys\nasp-pro-glu\ngln-pro-phe\nile-pro-leu\nala-pro-ser\nser-pro-ile\nser-pro-gln\nile-pro-ala\nlys-pro-leu\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\nhis-ala-pro\nlys-leu-pro\ntyr-asp-pro\ntyr-lys-pro\nglu-arg-pro\nval-ser-pro\nleu-glu-pro\ngly-ala-pro\nglu-gln-pro\nthr-ile-pro\nglu-ala-pro\nile-ala-pro\n5\n10\n15\nAverage distance\npro-gly-trp\nthr-gly-pro\npro-ser-gly\npro-ser-thr\npro-ala-val\npro-cys-lys\npro-phe-glu\npro-tyr-thr\npro-ile-gly\npro-leu-phe\npro-leu-val\npro-asp-phe\ngly-pro-cys\nasp-pro-glu\ngln-pro-phe\nser-pro-gln\nser-pro-ile\npro-pro-tyr\narg-pro-asp\npro-pro-gly\nile-pro-leu\nala-pro-ser\narg-pro-ser\nglu-pro-pro\nleu-pro-pro\nile-pro-ala\nlys-pro-leu\ngly-ile-pro\nlys-leu-pro\nval-ser-pro\nglu-ala-pro\nile-ala-pro\nhis-ala-pro\ntyr-asp-pro\ntyr-lys-pro\nglu-arg-pro\nleu-glu-pro\ngly-ala-pro\nglu-gln-pro\nthr-ile-pro\npro-gln-glu\npro-glu-thr\n5\n10\n15\nGreatest distance\nFigure 4: Dendrograms created from clustering 42 transition matrices using D√\nJS and single (top),\naverage (center), and complete (bottom) linkage objective functions. These dendrograms, which do\nnot identify intuitively natural groups, can be contrasted with Fig. 2 (top). In all three cases, the\nclustering algorithm cannot separate the system dynamics according to the location of the proline,\nand can only separate the two glycine-containing tripeptides from the other 40 systems. All three\nalternative objective functions identify many other singletons, and no coherent groups can be obtained\nfrom the results.\n10\nFurane\nMethyl hydrazine\nNitromethane\nMethane\nAcetonitrile\nPropionitrile\nAcrylonitrile\n1,2-Propylene oxide\nAcetamide\nUrea\nEthanethiol\nDimethyl sulfide\nEthane\nMethanol\ntrans-2-Pentene \ncis-2-Pentene\nEthanol\nEthyne\nEthylene\nMethyl acetate\n2-Butanone\n2-Methyl-1,3-Butadiene \n3-Methyl-1-Butene\nButan-2-ol\n2-Propanol\n2-Methylpropan-1-ol\n2-Methylpropane\n2-Methy-2-Butene\n2-Methylbutane\n2-Methylpropene\n2-Methyl-1-Butene\nPropane\nt-Crotonaldehyde\nChloroethane\nChloroethylene\nPropylene\nMethyl propyl ether \n1-Propanol\nMethyl formate\nEthyl formate\nPropionaldehyde\n1-Butanol\nAcrolein\nButyraldehyde\n2-butenal\nButane\nPropyne\nDiethyl ether \n1-Butyne\n1-Pentyne\nEthyl vinyl ether\n1-Butene\nPentane\n1,3-Butadiene\n1-Pentene \n1,4-Pentadiene \nTetrahydrofurane \nCyclopentane \nCyclopentene \n0.0\n0.2\n0.4\n0.6\nShortest distance\nFurane\nMethyl hydrazine\nEthanethiol\nEthylene\nDimethyl sulfide\nEthane\nMethane\nMethanol\nEthanol\nNitromethane\n2-Butanone\nMethyl acetate\nAcetamide\nUrea\nAcetonitrile\nPropionitrile\nAcrylonitrile\n1,2-Propylene oxide\nTetrahydrofurane \nCyclopentane \nCyclopentene \n2-Methyl-1,3-Butadiene \n3-Methyl-1-Butene\n2-Propanol\nButan-2-ol\n2-Methylpropane\n2-Methylpropan-1-ol\n2-Methylpropene\n2-Methyl-1-Butene\n2-Methy-2-Butene\n2-Methylbutane\ntrans-2-Pentene \ncis-2-Pentene\nMethyl formate\nEthyl formate\nt-Crotonaldehyde\nButyraldehyde\n2-butenal\nAcrolein\nPropionaldehyde\nEthyne\nPropyne\n1-Butyne\nChloroethylene\nChloroethane\nPropane\nPropylene\n1-Butanol\n1-Propanol\nMethyl propyl ether \nButane\nEthyl vinyl ether\nDiethyl ether \nPentane\n1-Pentyne\n1-Pentene \n1,4-Pentadiene \n1,3-Butadiene\n1-Butene\n0.0\n0.5\n1.0\nAverage distance\nFurane\nAcetonitrile\nPropionitrile\nAcrylonitrile\nEthyne\nPropyne\n1-Butyne\nEthylene\nEthanethiol\nDimethyl sulfide\nEthane\nMethyl formate\nEthyl formate\nt-Crotonaldehyde\nButyraldehyde\n2-butenal\nAcrolein\nPropionaldehyde\nChloroethylene\nChloroethane\nPropane\nPropylene\n1,2-Propylene oxide\n1-Butanol\n1-Propanol\nMethyl propyl ether \nButane\n2-Propanol\nButan-2-ol\n2-Methy-2-Butene\n2-Methylbutane\n2-Methylpropene\n2-Methyl-1-Butene\n2-Methylpropane\n2-Methylpropan-1-ol\nEthyl vinyl ether\nDiethyl ether \nTetrahydrofurane \nCyclopentane \nCyclopentene \n2-Methyl-1,3-Butadiene \n3-Methyl-1-Butene\n1-Pentene \n1,4-Pentadiene \n1,3-Butadiene\n1-Butene\nPentane\n1-Pentyne\ntrans-2-Pentene \ncis-2-Pentene\nMethyl hydrazine\nMethane\nMethanol\nEthanol\nNitromethane\n2-Butanone\nMethyl acetate\nAcetamide\nUrea\n0.0\n0.5\n1.0\nGreatest distance\nFigure 5: Dendrogram created from clustering 59 small molecules using Levenshtein distance\nbetween their SMILES representations and single (top), average (center), and complete (bottom)\nlinkage objective functions. These dendrograms, which do not identify intuitively natural groups, can\nbe contrasted with Fig. 3 (top). While all three objective functions identiﬁed furane as a singleton,\nthey produce no easily identiﬁable groups.\n11\n",
  "categories": [
    "physics.bio-ph",
    "q-bio.BM",
    "q-bio.QM",
    "stat.ML"
  ],
  "published": "2017-12-20",
  "updated": "2017-12-20"
}