{
  "id": "http://arxiv.org/abs/1712.07805v1",
  "title": "Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning Applications",
  "authors": [
    "Qixue Xiao",
    "Kang Li",
    "Deyue Zhang",
    "Yier Jin"
  ],
  "abstract": "This paper considers security risks buried in the data processing pipeline in\ncommon deep learning applications. Deep learning models usually assume a fixed\nscale for their training and input data. To allow deep learning applications to\nhandle a wide range of input data, popular frameworks, such as Caffe,\nTensorFlow, and Torch, all provide data scaling functions to resize input to\nthe dimensions used by deep learning models. Image scaling algorithms are\nintended to preserve the visual features of an image after scaling. However,\ncommon image scaling algorithms are not designed to handle human crafted\nimages. Attackers can make the scaling outputs look dramatically different from\nthe corresponding input images.\n  This paper presents a downscaling attack that targets the data scaling\nprocess in deep learning applications. By carefully crafting input data that\nmismatches with the dimension used by deep learning models, attackers can\ncreate deceiving effects. A deep learning application effectively consumes data\nthat are not the same as those presented to users. The visual inconsistency\nenables practical evasion and data poisoning attacks to deep learning\napplications. This paper presents proof-of-concept attack samples to popular\ndeep-learning-based image classification applications. To address the\ndownscaling attacks, the paper also suggests multiple potential mitigation\nstrategies.",
  "text": "Wolf in Sheep’s Clothing – The Downscaling Attack\nAgainst Deep Learning Applications\nQixue Xiao1, Kang Li2, Deyue Zhang1, Yier Jin3\n1 Qihoo 360 Security Research Lab\n2University of Georgia\n3University of Florida\nAbstract—This paper considers security risks buried in the\ndata processing pipeline in common deep learning applications.\nDeep learning models usually assume a ﬁxed scale for their\ntraining and input data. To allow deep learning applications to\nhandle a wide range of input data, popular frameworks, such as\nCaffe, TensorFlow, and Torch, all provide data scaling functions\nto resize input to the dimensions used by deep learning models.\nImage scaling algorithms are intended to preserve the visual\nfeatures of an image after scaling. However, common image\nscaling algorithms are not designed to handle human crafted\nimages. Attackers can make the scaling outputs look dramatically\ndifferent from the corresponding input images.\nThis paper presents a downscaling attack that targets the data\nscaling process in deep learning applications. By carefully crafting\ninput data that mismatches with the dimension used by deep\nlearning models, attackers can create deceiving effects. A deep\nlearning application effectively consumes data that are not the\nsame as those presented to users. The visual inconsistency enables\npractical evasion and data poisoning attacks to deep learning\napplications. This paper presents proof-of-concept attack samples\nto popular deep-learning-based image classiﬁcation applications.\nTo address the downscaling attacks, the paper also suggests\nmultiple potential mitigation strategies.\nI.\nINTRODUCTION\nDeep learning applications are continuously gaining popu-\nlarity in both academic and industry in recent years. Advances\nin GPUs and deep learning algorithms along with large datasets\nallow deep learning algorithms to address real-world problems\nin many areas, from image classiﬁcation, health care predic-\ntion, auto game playing, to reverse engineering.\nAlong with this increasing popularity comes with new\nsecurity concerns to deep learning applications. Data poisoning\nattacks [1, 4], such as mixing mislabeled data or polluted\nsamples in training datasets, have drawn substantive attention.\nResearchers [10, 16] have shown that deep learning applica-\ntions, such as imaging recognition and self-driving, can be\nmanipulated to make wrong decisions on benign inputs due to\ndata poisoning.\nIn practice, data poisoning attacks are not the main concern\nto deep learning application developers and users. One possible\nreason is that users often believe the risk can be largely\nmitigated by having human carefully inspect the training data.\nUsing image classiﬁcation applications as examples, people\ngenerally believe they can visually detect those mislabelled\nimages (e.g. having a human verify an image label by looking\nat it).\nThis paper presents downscaling attacks, which target a\nsecurity risk buried in the data processing pipeline of deep\nlearning applications. Downscaling attacks take advantage of\na limitation in the data ﬂow processing in deep learning\napplications – deep learning model are typically trained on\nwell formatted data. For design simplicity, most deep learning\napplications use ﬁxed size input scales. The designer of deep\nneural networks assume both training data and application\ninputs fall on the same scale. In practice, many deep learning\napplications often need to deal with inputs that are formatted\nin different scales. Arbitrary inputs therefore need to be resized\nto the model scale in order to be used by the deep learning\nneural network. To support this particular need, popular deep\nlearning frameworks all offer rescaling functions; and for most\ndeep learning applications we examined, data rescaling is a\ncommon component in their data processing pipeline.\nIn a downscaling attack, attackers feed input images con-\ntaining camouﬂage data to a deep learning application. The\ncamouﬂage data are arranged in an input image such that they\nwill be ﬁltered or changed by the scaling algorithm. The result\nis an image with a different visual presentation being fed to the\napplication’s deep learning neural network. Consequently this\nattack creates visually deceiving effect and potentially allows\nattackers to launch more practical data poisoning and evasion\nattacks [1, 4, 11, 13, 20].\nIn the paper, we investigate the common scaling implemen-\ntations in three popular deep learning frameworks: Caffe [9],\nTensorFlow [7], and Torch [15]. We ﬁnd that all the default\ndata scaling algorithms are vulnerable to the downscaling\nattack. Attackers can injects poisoning or deceiving data to\ntraining samples or input data. The injected camouﬂaged data\nare visible to users but are discarded by scaling functions,\nand thus these data are eventually not used by deep learning\nalgorithms. For example, a human considers an image (with\ncamouﬂage) only contains sheep, but the image becomes a\npicture of wolf after rescaling. For deep learning applications\nthat take the camouﬂaged sheep image, it would be the wolf\nimage eventually being proceeded by applications.\nThis paper describes our preliminary study on the threat\nrelated to data scaling, and we provide proof of concept\ninputs to illustrate the potential danger of downscaling attacks.\nThrough our preliminary study of the commonly used scaling\noptions in deep learning applications, we make the following\ncontributions:\n• This paper presents a preliminary study of the data scaling\noptions in popular deep learning frameworks.\n• We ﬁnd that data scaling is actually a common need in\ndeep learning applications.\n• We show the risk of downscaling attacks is real, and\nwe provide multiple proof-of-concept images targeting\narXiv:1712.07805v1  [cs.CR]  21 Dec 2017\npopular deep learning applications.\n• To reduce the potential threats from downscaling attacks,\nwe offer a few mitigation strategies, such as input ﬁl-\ntering, robust scaling implementations, and downscaling\nattack detections.\nII.\nDOWNSCALING ATTACK EXAMPLES\nTo illustrate the potential risks and the deceiving effect of\ndownscaling attacks, we present a few sample attack inputs in\nthis section.\nFigure 1 presents the ﬁrst group of examples that were\ncrafted for image classiﬁcation applications. The targeted ap-\nplication used here is the cppclassiﬁcation[8] sample released\nalong with the Caffe framework.\nTo show the effect of these images on deep learning\napplications, we used the GoogleNet model provided by the\nBAIR lab of BVLC [6], which assumes the input data is in\nthe scale of 224*224. When an image with a different size\nis provided, the application uses the resize() function in the\nCaffe framework to rescale the input to the size used by the\nmodel (224*224). The exact classiﬁcation setup details and the\nprogram output are presented in the appendix of this paper.\nFig. 1: Two Examples Showing Sample Effect of Downscale-\ning Attack. (Left-side: What Human See; Right-side: What ML\nModels See)\nWe specially crafted input images that are in a different\nsize (224*672). The images on the left column of Figure 1 are\ninputs to deep learning applications. The images on the right\ncolumn are the output of the scaling function, and thus are the\neffective images used by the deep learning algorithms.\nWhile the input on the left column are sheep, the deep\nlearning model takes the images on the right column as the real\ninputs. The corresponding image are classiﬁed into instances\nof “White Wolf” and “lynx Cat” respectively.\nTABLE I: PoC Sample Image Information\nImage File\nMD5 Checksum\nImage Size\nwolf-in-\nsheep.png\nfce6aa3c1b03ca4ae3289f5be22e7e65\n224*672\nwolf.png\ned2d62fbb8720d802c250fb386c254f6\n224*224\ncat-in-\nsheep.png\n6d489fe75f74ad32e8ada01ff7da9450\n224*672\ncat.png\n01820651a302fb3730ac0a5ffd95c23c\n224*224\nFig. 2: Four pairs of Downscaling attack examples for the\nMNIST application (Within each pair, left-side: What Human\nSee; right-side: What ML Models See)\nFigure 2 shows the attack examples using the MNIST\nhandwriting digits recognition [21] as the target application.\nFour pairs of images are presented in this ﬁgure. Within each\npair, the crafted inputs (visible to human) are in the left column\nand the scaling results (visible to models) are in the right\ncolumn. The ﬁgure shows that the images of digits change\ndramatically after the scaling call.\nIII.\nDATA SCALING AND DEEP LEARNING APPLICATIONS\nData scaling is not limited to images, and various re-\nsampling approaches can be applied to voice and other types\nof data. For simplicity, this paper focuses on the scaling effort\nfor image data. Image scaling refers to the resizing of a\ndigital image [19]. When scaling an image, the downscaling\n(or upscaling) process generates a new image with a lower (or\nhigher) number of pixels compared to the original image. In\nthe case of decreasing the pixel number (downscaling), this\nusually results in a visible quality loss.\nA. The Practical Need of Rescaling\nAlthough scaling is not a necessary component for deep\nlearning algorithms, data scaling is actually quite common in\ndeep learning applications. First, open-input applications, such\nas image classiﬁcations as an Internet service, would have to\ninvolve scaling in their data processing pipeline. For design\nsimplicity and manageable training process, a deep learning\nneural network model usually handles a ﬁxed scale input size.\nAlthough a deep learning neural network can be designed with\nlayers that take the largest possible scale among all possible\ninputs, this approach is inefﬁcient in terms both training and\nclassiﬁcation.\nFor those deep learning applications that take input data\nfrom a ﬁxed input source, such as sensors like video camera,\nthe input data format are naturally uniform. Even in such\nsituation, resize is used in certain cases.\nOne common situation we observe is the use of pre-\ntrained model. For example, NVIDIA offers multiple self-\ndriving sample models [5], and all these models use an input\nsize 200x66. However, on the recommended camera [12] spec-\niﬁcation provided by NVIDIA, the image generated are mostly\nof the size 1928x1208. Therefore, for system developers that\ndo not want to design and train their own models, they are\nlikely have to use scaling functions somewhere in their data\nprocessing pipeline. Recent research work, such as the sample\napplications used in the study of DeepXplore [14], also show\nthat the resizing operation is commonly used in self-driving\napplications to ﬁt camera output size to the size of models.\nB. Data Scaling in Deep Learning Image Applications\nMost deep learning frameworks provide data scaling func-\ntions. We examine the sample programs come with popular\ndeep learning frameworks, such as Tensorﬂow, Caffe, and\nTorch, and we found many programs call scaling functions in\ntheir data processing pipelines. For example, we found that the\nimage classiﬁcation application cppclassiﬁcation [8] in Caffe\nframework uses data scale in function Classiﬁer::Preprocess.\nBoth C++ and Python image recognition demo using Tensor-\nﬂow use function ResizeBilinear for data scaling[17]. Torch7\nand PyTorch are also using data scaling in their tutorials [2,\n18]. The corresponding code snippets are provided in the\nAppendix.\nBesides examples from popular deep learning frame-\nworks, we also found popular deep learning tools, such\nas deepdetect[3], use data scaling in their data processing\npipeline.\nTABLE II: Data scale algorithm in deep learning frameworks\nDL\nFramework\nLibrary\nDefault Scal-\ning Algorithm\nCaffe\nopencv\nBilinear\nTensorﬂow\npython-opencv\nBilinear\nTensorﬂow\npillow\nNeareast\nTensorﬂow\ntf.image\nBilinear\nTorch\ntorch-opencv\nBilinear\nTorch\nlua.image\nBilinear\nC. Image Scaling Algorithms in Deep Learning Frameworks\nImage scaling is a well studied ﬁeld, and there exist\nmultiple approaches when scaling an image from one size to\nanother. The most common used algorithms include Nearest-\nNeighbors, Bilinear, and Bicubic interpolation [19].\nThe nearest-neighbor interpolation algorithm uses a rela-\ntively simple approach. It uses the value of the nearest point in\nthe scaling process. To take neighboring points into consider-\nation, linear interpolation approach uses a mathematical mean\nto represent a region. Image is often considered as 2D data,\nand bilinear approach is an extension of linear interpolation\non a 2D grid.\nMost of deep learning frameworks use bilinear interpo-\nlation as the default data scale algorithm. We inspected the\nimplementation of scaling functions in deep learning frame-\nworks. Table II shows the data scale algorithms used by Caffe,\nTensorFlow and Torch. Most frameworks support Nearest-\nNeighbors, Bilinear, Bicubic algorithms along with a few\nothers depends on the speciﬁc framework.\nIV.\nTHE DOWNSCALING ATTACK\nThis section provides an overview of the downscaling\nattack method against deep learning image applications.\nA. The Attack Goal and Constraints\nThe intent of image scaling algorithms is to adjust the\nimage size while preserve visual features after scaling. The\npurpose of downscaling attack is to create effect against this\nintent. A downscaling attack is to craft images that produce\ndownscaled outputs with dramatic visual differences from the\ninputs.\nAlthough scaling in either direction (up and down scaling)\ncould result in distortions and potentially deceiving effect, we\nonly discuss the downscaling functions in this paper.\nWe can consider the scaling effect as a function F, which\nconvert a source image in a m ∗n dimension (in[m][n]) to a\nnew image in different scale (out[m′][n′]).\nF(in[m][n]) = out[m′][n′](m′ <= m, n′ <= n)\n(1)\nAlthough not all downscaling attacks need to generate\na speciﬁc image, for simplicity of discussion, we consider\nthe following attack goal – Given a speciﬁc image target, a\nsuccessful downscaling attack need to meet the following three\nconstraints:\n1) The\nattacker\nneeds\nto\ncompose\nan\nimage\n(crftImg[m][n])\nso\nthat\nthe\noutput\nof\nF(crftImg[m][n]) is the target Image (dstImg[m′][n′]);\n2) The crafted image crftImg[m][n] needs to be visually\nmeaningful to human,\n3) The crafted image crftImg[m][n] and target image\ndstImg[m′][n′] contain different semantical meanings.\nB. The Nearest-neighbor Algorithm and Camouﬂage Regions\nDownscaling algorithms generally do not take specially\ncrafted “malicious” inputs into consideration when considering\nhow to extract “sampling” data from input images. Here\nwe take the simplest scaling algorithm as an example to\ndemonstrate the downscaling attack.\nFigure 3 illustrates the process of converting a 4x4 input\nimage to the scale of 2x2. In this scaling process, only\none fourth of the input image needs to contain the exact\nvalue of the output image pixels. This scaling process leaves\nthree quarters of input data under attacker’s control. We call\nthese areas camouﬂaging regions. Regardless what values were\nstored in these camouﬂaging regions, the scaling output is the\nsame. Therefore, all the attackers need to do is to use these\ncamouﬂaging regions to create deceiving effect.\nCertainly what exact content to put in the camouﬂage areas\ndepends on the target image and the deceiving effect the at-\ntacker wants to achieve. Currently we are working on software\ntools to automatic interleaving pre-captured camouﬂage images\nover a given target image.\nC. Parameters and Consideration in Downscaling Attacks\nTo use downscaling attacks to achieve deceiving effects,\nattackers have to consider multiple factors, such as what to\nput in the camouﬂage regions and what size to pick for the\ninput images.\nFirst, although the early examples of nearest-neighbors\nalgorithm leave camouﬂaging regions for attacker to inject\narbitrary data, other scaling algorithms might impose different\nconstraints. For example, Bilinear scaling algorithms requires\na region’s weighted average pixel values to be equivalent\n0 \n1 \n2 \n3 \n0 \n1 \n2 \n3 \n4*4 \n2*2 \n0 \n1 \n0 \n1 \nd00 \nd20 \nd02 \nd22 \ndownscaling \nd00 d20 \nd02 d22 \nFig. 3: Nearest-neighbor interpolation Illustration.\nto the value of a corresponding pixel in the target image.\nNevertheless, even with these constraints, attackers are given\nopportunities to provide content that will be partially discarded\nby the scaling function. In fact, the sample attack images\nin the previous sections are crafted against Bilinear scaling\nalgorithms, and they can successfully trigger misclassiﬁcations\nin deep learning applications.\nSecond, the amount of data controlled by attackers is\nproportionally related to the scaling factor. An image that\nis much larger than the scale used by deep learning models\ngenerally provides more space for attackers to create deceiving\neffect. However, images with extremely uncommon sizes are\nlikely trigger warnings to users who inspect inputs. Therefore,\nattackers need to consider the trade-off between image sizes\nand the levels of awkwardness potentially raised by the abnor-\nmal sizes.\nV.\nDISCUSSION\nA. Practical Challenges for Applying Downscaling Attacks to\nSensor-based Applications\nDownscaling attacker certainly only affects applications\nthat use scaling in the data processing pipeline. It does not\nthreat applications that only consume data in the same scale\nas used by the deep learning models.\nCertainly, even resize is used in deep learning applica-\ntions, attackers still need to ﬁnd out ways to inject crafted\ninputs in order to generate downscaling attacks. Launching\nsuch attack directly from the analog domain is challenging\nbut not impossible. Moreover, sensors themselves potentially\ncan be potentially compromised and malicious sensor inputs\ncan be generated in those cases to attack the backend deep\nlearning implementations. In the case of a compromised sensor,\ncertainly it can directly feed more damaging input to the\nbackend systems. However, downscaling attacks would still be\npreferred because they provide an additional level of deception.\nB. Mitigation of Downscaling Attacks\nDeep learning application designers have multiple options\nto mitigate the risks from downscaling attacks. We have\nconsidered the following possible approaches: 1) limiting the\ninput size, 2) using robust scaling algorithms, and 3) detecting\ndramatic changes from scaling,\nThe most straightforward way to avoid downscaling attack\nis to ignore inputs that do not match the scale used by the\ndeep learning models. This approach ﬁts well to applications\nthat deal with input generated by sensor or other applications\ncontrol by users. However, as we discussed above, not all\napplications can take this solution, and many deep learning\napplications often have to deal with input generated (and thus\ncontrolled) by Internet users.\nAnother possible solution is to adopt a robust scaling\noption. Although we have shown that the default scaling\nimplementation in deep learning frameworks are vulnerable\nto downscaling attacks, there are scaling algorithms (such\nas Bicubic interpolation) that are more robust in terms of\npreserving the content of the original images, and thus are less\nvulnerable to attacks. Among the major frameworks we have\ninspected, Caffe, TensorFlow, and Torch all allow applications\nto choose different scaling algorithms through a parameter of\nthe resize function.\nThe third viable solution is to detect dramatic changes in\ninput features during the scaling process. The downscaling\nattack’s deceiving effect is caused by a dramatic change to the\nimage content before and after the scaling action. Therefore,\napplications can potentially detect such attacks by measuring\nfeatures that are related to visual presentation. For example, the\ncolor histogram is likely changed in the case that the rescaled\nimage is dramatically different from the original image.\nVI.\nCONCLUSION\nThe purpose of this work is to raise awareness of the\nsecurity threats buried in the data processing pipeline in deep\nlearning applications.\nDeep learning applications usually make strong assump-\ntions of a uniformed input data scale. All the training and\nclassiﬁcation algorithms focus on processing well formatted\ndata. Although there are situations where data inputs uniformly\nmatch the scale used by the background models, many deep\nlearning applications (such as image classiﬁcations) are un-\navoidable to process data that are in different scales.\nMost deep learning frameworks provide scaling and re-\nsizing utilities to convert data to the scale used by deep\nlearning models. This data processing stage is vulnerable to\nthe downscaling attack presented in this paper. We showed\nthat attackers can prepare visually deceiving images to deep\nlearning applications. The result is a mismatch between what\nusers see and what deep learning applications use. We studied\nmultiple deep learning frameworks and showed that most of\ntheir default scaling functions are vulnerable to such attack.\nWe hope our preliminary results in this paper can remind\nresearchers to not ignore threats in the data processing pipeline\nand actively look for ways to detect risks in deep learning\napplications.\nREFERENCES\n[1] S. Alfeld, X. Zhu, and P. Barford, “Data poisoning attacks against autoregressive\nmodels.” in AAAI, 2016, pp. 1452–1458.\n[2] alkyantejani and soumith, “ImageNet training in PyTorch,” http://github.com/\npytorch/examples/tree/master/imagenet, 2017.\n[3] beniz, “Deep Learning API and Server in C++11 with Python bindings and support\nfor Caffe, Tensorﬂow, XGBoost and TSNE,” https://github.com/beniz/deepdetect,\n2017.\n[4] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector\nmachines,” arXiv preprint arXiv:1206.6389, 2012.\n[5] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D.\nJackel, M. Monfort, U. Muller, J. Zhang et al., “End to end learning for self-driving\ncars,” arXiv: Computer Vision and Pattern Recognition, 2016.\n[6] BVLC, “BAIR/BVLC GoogleNet Model,” http://dl.caffe.berkeleyvision.org/bvlc_\ngooglenet.caffemodel, 2017.\n[7] Gardener and Benoitsteiner, “An open-source software library for Machine Intelli-\ngence,” https://www.tensorﬂow.org/, 2017.\n[8] Y. Jia, “Classifying ImageNet: using the C++ API,” https://github.com/BVLC/caffe/\ntree/master/examples/cpp_classiﬁcation, 2017.\n[9] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama,\nand T. Darrell, “Caffe: Convolutional architecture for fast feature embedding,” arXiv\npreprint arXiv:1408.5093, 2014.\n[10] A. Mogelmose, D. Liu, and M. M. Trivedi, “Trafﬁc sign detection for u.s.\nroads: Remaining challenges and a case for tracking,” International conference\non intelligent transportation systems, pp. 1394–1399, 2014.\n[11] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily fooled:\nHigh conﬁdence predictions for unrecognizable images,” in The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2015.\n[12] NVIDIA developers, “the latest products and services compatible with the DRIVE\nPlatform of NVIDIA’s ecosystem ,” https://developer.nvidia.com/drive/ecosystem,\n2017.\n[13] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami,\n“Practical black-box attacks against machine learning,” in Proceedings of the 2017\nACM on Asia Conference on Computer and Communications Security, ser. ASIA\nCCS ’17.\nNew York, NY, USA: ACM, 2017, pp. 506–519.\n[14] K. Pei, Y. Cao, J. Yang, and S. Jana, “Deepxplore: Automated whitebox testing of\ndeep learning systems,” in Proceedings of the 26th ACM Symposium on Operating\nSystems Principles (SOSP ’17), October 2017.\n[15] Ronan, Clément, Koray, and Soumith, “Torch: A SCIENTIFIC COMPUTING\nFRAMEWORK FOR LUAJIT,” http://torch.ch/, 2017.\n[16] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to a crime:\nReal and stealthy attacks on state-of-the-art face recognition,” in Proceedings of\nthe 2016 ACM SIGSAC Conference on Computer and Communications Security.\nACM, 2016, pp. 1528–1540.\n[17] Tensorﬂow developers, “TensorFlow C++ and Python Image Recognition Demo,”\nhttps://www.github.com/tensorﬂow/tensorﬂow/tree/master/tensorﬂow/examples/\nlabel_image, 2017.\n[18] Torch developers, “Tutorials for Torch7,” http://github.com/torch/tutorials/tree/\nmaster/7_imagenet_classiﬁcation, 2017.\n[19] Wikipedia, “Image scaling,” https://en.wikipedia.org/wiki/Image_scaling, 2017.\n[20] W. Xu, Y. Qi, and D. Evans, “Automatically evading classiﬁers,” in Network and\nDistributed System Security Symposium, 2016.\n[21] L. Yann, C. Corinna, and J. B. Christopher, “The MNIST Database of handwritten\ndigits,” http://yann.lecun.com/exdb/mnist/, 2017.\nAPPENDIX\nThis appendix provides code snippet of deep learning\nframeworks using data scaling, and the brief information\nrelated to our experimental setup. All the software implemen-\ntation and deep learning model were obtained online without\nchange.\nA. Code snippet of data-scaling used by deep learning frame-\nworks and applications (The use of resizing or scaling function\nare highlighted in red color)\nListing 1: preprocess in label_image of tensorﬂow\ndef read_tensor_from_image_file(file_name, input_height=299, input_width=299,\ninput_mean=0, input_std=255):\ninput_name = \"file_reader\"\noutput_name = \"normalized\"\nfile_reader = tf.read_file(file_name, input_name)\nif file_name.endswith(\".png\"):\nimage_reader = tf.image.decode_png(file_reader, channels = 3,\nname=’png_reader’)\nelif file_name.endswith(\".gif\"):\nimage_reader = tf.squeeze(tf.image.decode_gif(file_reader,\nname=’gif_reader’))\nelif file_name.endswith(\".bmp\"):\nimage_reader = tf.image.decode_bmp(file_reader, name=’bmp_reader’)\nelse:\nimage_reader = tf.image.decode_jpeg(file_reader, channels = 3,\nname=’jpeg_reader’)\nfloat_caster = tf.cast(image_reader, tf.float32)\ndims_expander = tf.expand_dims(float_caster, 0);\nresized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\nnormalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\nsess = tf.Session()\nresult = sess.run(normalized)\nreturn result\nListing 2: preprocess in cppclassiﬁcation of Caffe\n189 void Classifier::Preprocess(const cv::Mat& img,\n190\nstd::vector<cv::Mat>* input_channels) {\n191\n/* Convert the input image to the input image format of the network. */\n192\ncv::Mat sample;\n193\nif (img.channels() == 3 && num_channels_ == 1)\n194\ncv::cvtColor(img, sample, cv::COLOR_BGR2GRAY);\n195\nelse if (img.channels() == 4 && num_channels_ == 1)\n196\ncv::cvtColor(img, sample, cv::COLOR_BGRA2GRAY);\n197\nelse if (img.channels() == 4 && num_channels_ == 3)\n198\ncv::cvtColor(img, sample, cv::COLOR_BGRA2BGR);\n199\nelse if (img.channels() == 1 && num_channels_ == 3)\n200\ncv::cvtColor(img, sample, cv::COLOR_GRAY2BGR);\n201\nelse\n202\nsample = img;\n203\n204\ncv::Mat sample_resized;\n205\nif (sample.size() != input_geometry_)\n206\ncv::resize(sample, sample_resized, input_geometry_);\n207\nelse\n208\nsample_resized = sample;\n209\n210\ncv::Mat sample_float;\n211\nif (num_channels_ == 3)\n212\nsample_resized.convertTo(sample_float, CV_32FC3);\n213\nelse\n214\nsample_resized.convertTo(sample_float, CV_32FC1);\n215\n216\ncv::Mat sample_normalized;\n217\ncv::subtract(sample_float, mean_, sample_normalized);\n218\n219\n/* This operation will write the separate BGR planes directly to the\n220\n* input layer of the network because it is wrapped by the cv::Mat\n221\n* objects in input_channels. */\n222\ncv::split(sample_normalized, *input_channels);\n223\n224\nCHECK(reinterpret_cast<float*>(input_channels->at(0).data)\n225\n== net_->input_blobs()[0]->cpu_data())\n226\n<< \"Input channels are not wrapping the input layer of the network.\";\n227 }\nListing 3: Imagenet classiﬁcation with Torch7\nfunction preprocess(im, img_mean)\n-- rescale the image\nlocal im3 = image.scale(im,224,224,’bilinear’)\n-- subtract imagenet mean and divide by std\nfor i=1,3 do im3[i]:add(-img_mean.mean[i]):div(img_mean.std[i]) end\nreturn im3\nend\nListing 4: Imagenet classiﬁcation with PyTorch\ndef main():\nglobal args, best_prec1\nargs = parser.parse_args()\n...\n# Data loading code\ntraindir = os.path.join(args.data, ’train’)\nvaldir = os.path.join(args.data, ’val’)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\nstd=[0.229, 0.224, 0.225])\ntrain_dataset = datasets.ImageFolder(\ntraindir,\ntransforms.Compose([\ntransforms.RandomResizedCrop(224),\ntransforms.RandomHorizontalFlip(),\ntransforms.ToTensor(),\nnormalize,\n]))\nif args.distributed:\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\nelse:\ntrain_sampler = None\ntrain_loader = torch.utils.data.DataLoader(\ntrain_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\nnum_workers=args.workers, pin_memory=True, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(\ndatasets.ImageFolder(valdir, transforms.Compose([\ntransforms.Resize(256),\ntransforms.CenterCrop(224),\ntransforms.ToTensor(),\nnormalize,\n])),\nbatch_size=args.batch_size, shuffle=False,\nnum_workers=args.workers, pin_memory=True)\nListing 5: code snippnet in deepdetect\nint read_file(const std::string &fname)\n{\ncv::Mat img = cv::imread(fname,_bw ? CV_LOAD_IMAGE_GRAYSCALE :\nCV_LOAD_IMAGE_COLOR);\nif (img.empty())\n{\nLOG(ERROR) << \"empty image\";\nreturn -1;\n}\n_imgs_size.push_back(std::pair<int,int>(img.rows,img.cols));\ncv::Size size(_width,_height);\ncv::Mat rimg;\ncv::resize(img,rimg,size,0,0,CV_INTER_CUBIC);\n_imgs.push_back(rimg);\nreturn 0;\n}\nB. Software Version and Model Information for Attack Demon-\nstration\nHere we present the software setup for the attack demon-\nstration. Although the example used here targets applications\nfrom the Caffe platform, the risk is not limited to Caffe. We\nhave tested the scaling functions in Caffe, TensorFlow and\nTorch. All of them are vulnerable to downscaling attacks.\nThe Caffe package and the corresponding image classi-\nﬁcation examples were checked-out directly from the ofﬁ-\ncial GitHub on October 25, 2017, and the OpenCV used\nwas the latest stable version from the following URL:\nhttps://github.com/opencv/opencv/archive/2.4.13.4.zip\nWe used the BVLC CaffeNet Model in our proof of concept\nexploitation. The model is the result of training based on the\ninstructions provided by the instruction in the original Caffe\npackage. To avoid any mistakes in model setup, we download\nthe model ﬁle directly from BVLC’s ofﬁcial GitHub page.\nDetail information about the model is provided in the list\nbelow.\nListing 6: Image Classiﬁer Model\nname: BAIR/BVLC GoogleNet Model\ncaffemodel: bvlc_googlenet.caffemodel\ncaffemodel_url: http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel\ncaffe_commit: bc614d1bd91896e3faceaf40b23b72dab47d44f5\nC. Command Lines\nThe downscaling threat was demonstrated based on the\ndefault Caffe example CPPClassiﬁcation. The exact command\nline was shown in the list below.\nListing 7: Image Classiﬁcation Command Line\n./classification.bin models/bvlc_googlenet/deploy.prototxt\nmodels/bvlc_googlenet/bvlc_googlenet.caffemodel\ndata/ilsvrc12/imagenet_mean.binaryproto\ndata/ilsvrc12/synset_words.txt\nIMAGE_FILE\nD. Sample Output\nThe lists below is the classiﬁcation results for the sample\nimages used in the example section.\nListing 8: Sample Classiﬁcation Results\n# wolf-in-sheep.png [Image size: 224*672]\n./classification.bin\nmodels/bvlc_googlenet/deploy.prototxt\nmodels/bvlc_googlenet/bvlc_googlenet.caffemodel\ndata/ilsvrc12/imagenet_mean.binaryproto\ndata/ilsvrc12/synset_words.txt\n/tmp/sample/wolf-in-sheep.png\n---------- Prediction for /tmp/sample/wolf-in-sheep.png ----------\n0.8890 - \"n02114548 white wolf, Arctic wolf, Canis lupus tundrarum\"\n0.0855 - \"n02120079 Arctic fox, white fox, Alopex lagopus\"\n0.0172 - \"n02134084 ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\"\n0.0047 - \"n02114367 timber wolf, grey wolf, gray wolf, Canis lupus\"\n0.0019 - \"n02111889 Samoyed, Samoyede\"\n# wolf.png [Image size: 224*224]\n./classification.bin\nmodels/bvlc_googlenet/deploy.prototxt\nmodels/bvlc_googlenet/bvlc_googlenet.caffemodel\ndata/ilsvrc12/imagenet_mean.binaryproto\ndata/ilsvrc12/synset_words.txt\n/tmp/sample/wolf.png\n---------- Prediction for /tmp/sample/wolf.png ----------\n0.8890 - \"n02114548 white wolf, Arctic wolf, Canis lupus tundrarum\"\n0.0855 - \"n02120079 Arctic fox, white fox, Alopex lagopus\"\n0.0172 - \"n02134084 ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus\"\n0.0047 - \"n02114367 timber wolf, grey wolf, gray wolf, Canis lupus\"\n0.0019 - \"n02111889 Samoyed, Samoyede\"\n# cat-in-sheep.png [Image size: 224*672]\n./classification.bin\nmodels/bvlc_googlenet/deploy.prototxt\nmodels/bvlc_googlenet/bvlc_googlenet.caffemodel\ndata/ilsvrc12/imagenet_mean.binaryproto\ndata/ilsvrc12/synset_words.txt\n/tmp/sample/cat-in-sheep.png\n---------- Prediction for /tmp/sample/cat-in-sheep.png ----------\n0.1312 - \"n02127052 lynx, catamount\"\n0.1103 - \"n02441942 weasel\"\n0.1068 - \"n02124075 Egyptian cat\"\n0.1000 - \"n04493381 tub, vat\"\n0.0409 - \"n04209133 shower cap\"\n# cat.png [Image size: 224*224]\n./classification.bin\nmodels/bvlc_googlenet/deploy.prototxt\nmodels/bvlc_googlenet/bvlc_googlenet.caffemodel\ndata/ilsvrc12/imagenet_mean.binaryproto\ndata/ilsvrc12/synset_words.txt\n/tmp/sample/cat.png\n---------- Prediction for /tmp/sample/cat.png ----------\n0.1312 - \"n02127052 lynx, catamount\"\n0.1103 - \"n02441942 weasel\"\n0.1068 - \"n02124075 Egyptian cat\"\n0.1000 - \"n04493381 tub, vat\"\n0.0409 - \"n04209133 shower cap\"\n",
  "categories": [
    "cs.CR",
    "cs.CV",
    "cs.LG"
  ],
  "published": "2017-12-21",
  "updated": "2017-12-21"
}