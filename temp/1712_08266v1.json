{
  "id": "http://arxiv.org/abs/1712.08266v1",
  "title": "Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning",
  "authors": [
    "Saurabh Kumar",
    "Pararth Shah",
    "Dilek Hakkani-Tur",
    "Larry Heck"
  ],
  "abstract": "We present a framework combining hierarchical and multi-agent deep\nreinforcement learning approaches to solve coordination problems among a\nmultitude of agents using a semi-decentralized model. The framework extends the\nmulti-agent learning setup by introducing a meta-controller that guides the\ncommunication between agent pairs, enabling agents to focus on communicating\nwith only one other agent at any step. This hierarchical decomposition of the\ntask allows for efficient exploration to learn policies that identify globally\noptimal solutions even as the number of collaborating agents increases. We show\npromising initial experimental results on a simulated distributed scheduling\nproblem.",
  "text": "Federated Control with Hierarchical Multi-Agent\nDeep Reinforcement Learning\nSaurabh Kumar∗\nGeorgia Tech\nskumar311@gatech.edu\nPararth Shah∗\nGoogle\npararth@google.com\nDilek Hakkani-Tür\nGoogle\ndilekh@ieee.org\nLarry Heck\nGoogle\nlarry.heck@ieee.org\nAbstract\nWe present a framework combining hierarchical and multi-agent deep reinforce-\nment learning approaches to solve coordination problems among a multitude of\nagents using a semi-decentralized model. The framework extends the multi-agent\nlearning setup by introducing a meta-controller that guides the communication be-\ntween agent pairs, enabling agents to focus on communicating with only one other\nagent at any step. This hierarchical decomposition of the task allows for efﬁcient\nexploration to learn policies that identify globally optimal solutions even as the\nnumber of collaborating agents increases. We show promising initial experimental\nresults on a simulated distributed scheduling problem.\n1\nIntroduction\nMulti-agent reinforcement learning [3] can be applied to many real-world coordination problems, e.g.\nnetwork packet routing or urban trafﬁc control, to ﬁnd decentralized policies that jointly optimize the\nprivate value functions of participating agents. However, multi-agent RL algorithms scale poorly with\nproblem size. Since communication possibilities increase quadratically as the number of agents, the\nagents must explore a larger combined action space before receiving feedback from the environment.\nIn a separate line of research, hierarchical reinforcement learning (HRL) [4] has enabled learning\ngoal-directed behavior from sparse feedback in complex environments. HRL divides the overall task\ninto independent goals and trains a meta-controller to pick the next goal, while a controller learns to\nreach individual goals. Consequently, HRL requires the task to be divisible into independent subtasks\nthat can be solved sequentially. Multi-agent problems do not directly ﬁt this criteria as a subtask\nwould entail coordination between multiple agents, each having partial observability on the global\nstate. Recently, [5] trained a pair of agents to negotiate and agree upon a joint action, but they do not\nexplore scaling to settings with many agents.\nWe propose Federated Control with Reinforcement Learning (FCRL), a framework for combining\nhierarchical and multi-agent deep RL to solve multi-agent coordination problems with a semi-\ndecentralized model. Similar to HRL, the model consists of a meta-controller and controllers, which\nare hierarchically organized deep reinforcement learning modules that operate at separate time scales.\nIn contrast to HRL, we modify the notion of a controller to characterize a decentralized agent which\nreceives a partial view of the state and chooses actions that maximize its private value function. The\nmodel supports a variable number of controllers, where each controller is intrinsically motivated\n∗Equal contribution. Work done while Saurabh Kumar interned at Google Research.\nHierarchical Reinforcement Learning Workshop at the 31st Conference on Neural Information Processing\nSystems (HRL@NIPS 2017), Long Beach, CA, USA.\narXiv:1712.08266v1  [cs.AI]  22 Dec 2017\nFigure 1: Federated control model\nto negotiate with another controller and agree upon a joint action under some constraints, e.g. a\ndivision of available resources or a consistent schedule. The meta-controller chooses a sequence\nof pairs of controllers that must negotiate with each other as well as a constraint provided to each\npair, and it is rewarded by the environment for efﬁciently surfacing a globally consistent set of\ncontroller actions. Since a controller needs to communicate with a single other controller at any step,\nthe controller’s policy can be trained separately via self-play to maximize expected future intrinsic\nreward with gradient descent. As the details of individual negotiations are abstracted away from the\nmeta-controller, it can efﬁciently explore the space of choices of controller pairs even as number of\ncontrollers increases, and it is trained to maximize expected future extrinsic reward with gradient\ndescent.\nFCRL can be applied to a variety of real-world coordination problems where privacy of agents’ data\nis paramount. An example is multi-task dialogue with an automated assistant, where the assistant\nmust help a user to complete multiple interdependent tasks, for example making a plan to take a train\nto the city, watch a movie and then get dinner. Each task requires querying a separate third-party\nWeb service which has a private database of availabilities, e.g. a train ticket purchase, a movie ticket\npurchase or a restaurant table reservation service. Each Web service is a decentralized controller\nwhich aims to maximize its utilization, while the assistant is a meta-controller which aims to obtain a\nglobally viable schedule for the user. Another example is urban trafﬁc control, where each vehicle is\na controller having a destination location that it desires to keep private. A meta-controller guides the\ntrafﬁc ﬂow through a grid of intersections, aiming to maintain a normal level of trafﬁc on all roads\nin the grid. The meta-controller iteratively picks a pair of controllers and road segments, and the\ncontrollers must negotiate with each other and assign different road segments among themselves.\nIn the next section, we formally describe the Federated RL model, and in Section 3 we mention\nrelated work. In Section 4 we present preliminary experiments with a simulated multi-task dialogue\nproblem. We conclude with a discussion and present directions for future work in Section 5.\n2\nModel\nReinforcement Learning (RL) problems are characterized by an agent interacting with a dynamic\nenvironment with the objective of maximizing a long term reward [10]. The basic RL model casts\nthe task as a Markov Decision Process (MDP) deﬁned by the tuple {S, A, T, R, γ} of states, actions,\ntransition function, reward, and discount factor.\nAgents As in the h-DQN setting [4], we construct a two-stage hierarchy with a meta-controller and a\ncontroller that operate at different temporal scales. However, rather than utilizing a single controller\nwhich learns to complete multiple subtasks, FCRL employs multiple controllers, each of which learns\nto communicate with another controller to collectively complete a prescribed subtask.\nTemporal Abstractions As shown in Figure 1, the meta-controller receives a state st from the\nenvironment and selects a subtask gt from the set of all subtasks and a constraint ct from the set\nof all constraints. Constraints ensure that individual subtasks focus on disjoint parts of the overall\nproblem, allowing each problem to be solved independently by a subset of the controllers. The\nmeta-controller’s goal is to pick a sequence of subtasks and associated constraints to maximize the\n2\ncumulative discounted extrinsic reward provided by the environment. A subtask gt is associated with\ntwo controllers, Ci and Cj, who must communicate with each other to complete the task. Ci and Cj\nreceive separate partial views of the environment state through states it and jt. For K −1 time steps,\nthe subtask gt and constraint ct remain ﬁxed while Ci and Cj negotiate to decide on a set of output\nactions, ai and aj, which are outputted at the Kth time step. The controllers are trained with an\nintrinsic reward provided by an internal critic. The reward is shared between the controller pairs that\ncommunicated with each other, and they are rewarded for choosing actions that satisfy the constraints\nprovided by the environment as well as the meta-controller.\n3\nRelated Work\nMulti-agent communication with Deep RL Multi-agent RL involves multiple agents that must\neither cooperate or compete in order to successfully complete a task. A number of recent works have\ndemonstrated the success of this approach and have applied it to tasks in which agents communicate\nwith one another [2, 7, 9]. Two agents learned to communicate in natural language with one another\nin order to complete a negotiation task in [5] and an image guessing task in [1]. The demonstrated\nsuccess of multi-agent communication is promising, but it may be difﬁcult to scale to greater numbers\nof agents. In our work, we combine multi-agent RL with a meta-controller that selects subsets of\nagents to communicate so that the overall task is optimally completed.\nHierarchical Deep RL The h-DQN algorithm [4] splits an agent into two components: a meta-\ncontroller that selects subtasks to complete and a controller that selects primitive actions given a\nsubtask as input from the meta-controller. FCRL uses multiple controllers and extends the notion of a\nsubtask to involve a subset of the controllers that must collectively communicate to satisfy certain\nconstraints. Therefore, each controller can be pre-trained to complete a distinct goal and may receive\ninformation only relevant for that particular goal. This is in contrast to the work by [4], which trains\na single controller to complete multiple subtasks.\nHierarchical Deep RL for dialogue In task-oriented dialogues, a dialogue agent assists a user in\ncompleting a particular task, such as booking movie tickets or making a restaurant reservation.\nComposite tasks are those in which multiple tasks must be completed by the dialogue agent. Recent\nwork by [8] applies the h-DQN technique to composite task completion. While this work successfully\ntrains agents on composite task dialogues, a drawback with the straightforward application of h-DQN\nto dialogue is that only one goal is in focus at any given time, which must be completed prior to\nanother goal being addressed. This prevents the dialogue agent from handling cross-goal constraints.\nThe FCRL algorithm addresses cross-goal constraints by modeling a subtask as a negotiation between\ntwo controllers that must simultaneously complete their individual goals.\n4\nExperiments\nWe present a preliminary experiment applying our method to a simulated distributed scheduling\nproblem. The goal is to validate our approach against baseline approaches in a controlled setup. We\nplan to run further experiments on realistic scenarios in future work.\n4.1\nEnvironment\nWe consider a distributed scheduling problem which is inspired by the multi-domain dialogue\nmanagement setup described in the introduction. Formally, this consists of N agents, each having\na private database of available time entries, who must each pick a time such that the relative order\nof times chosen by all agents is consistent with the order speciﬁed by the environment. At the start\nof an episode, the environment randomly chooses m agents and provides an ordering2 of agents\nC1, C2, . . . , Cm and agent databases D1, . . . , Dm, where Di is a bit vector of size B specifying the\ntimes that are available to agent i. The agents can communicate amongst each other for K −1 rounds,\nafter which each agent must output an action 0 ≤ai < B. The environment emits a reward R = 1 if\n2We model this directly as a sequence of agent IDs, but an extension is to generate or sample crowd-sourced\nutterances for the constraints (“I want to watch a movie and get dinner. Also I’ll need a cab to get there.”) and\ntrain the agents to parse the natural language into an agent order.\n3\nthe actions a1, . . . am are such that a1 < a2 < . . . < am, and ai ∈Di ∀i, else it emits a reward of\nR = 0. In our setup we used N = 20, B = 8, and experimented with m ∈{2, 4, 6}.\n4.2\nAgents\nWe evaluate three approaches for solving the distributed scheduling problem. Our proposed approach\n(FCRL) consists of a meta-controller that picks a pair of controllers and a constraint vector, and\ncontroller agents which communicate in pairs and output times that satisfy their private databases\nas well as the meta-controller’s constraint. The two baselines, Multi-agent RL (MARL) and Hierar-\nchical RL (HRL), compare our approach with the settings without a meta-controller or multi-agent\ncommunication, respectively.\nFederated Control (FCRL) We use an FCRL agent with K = 2 communication steps between\nthe controllers. Note that this means they communicate for K −1 steps and then produce the\noutput actions at the Kth time step. The controller and meta-controller Q-networks have the same\nstructure with two hidden layers of sizes 100, 50, each followed by a tanh nonlinearity, and a ﬁnal\nfully-connected layer outputting Q-values for each action. The controller has B actions, one for\neach possible time value, and the meta-controller has B −1 actions, corresponding to constraint\nwindows of sizes B/2j, j ∈[0, logB]. (We assume that B is a power of 2.) The meta-controller\niterates through agent pairs in the order expected by the environment, and for the pair selected at time\nt, it chooses a constraint vector ct. This constraint is applied to the two agents’ databases, and the\ncontrollers then communicate with each other for K = 2 rounds. If the controllers are able to come\nup with a valid order, they are rewarded by the intrinsic critic, and the meta-controller moves on to\nthe next pair. Otherwise, the meta-controller retries the same pair of controllers. The meta-controller\nis given a maximum of 10 total invocations of controller pairs, after which the episode is terminated\nwith reward R = 0.\nFor a controller Ci communicating with another controller Cj, Ci’s state is a concatenation of the\ndatabase vector Di from the environment, a one-hot vector of size 2 denoting the position of that\nagent in the relative order between Ci and Cj, and a communication vector of size B which is a\none-hot vector denoting Cj’s output in the previous round. (In the ﬁrst round the communication\nvector is zeroed out.) The meta-controller’s state is a concatenation of a one-hot vector of size B\ndenoting the latest time entry that has been selected so far, and a multi-hot vector of size B −1,\ndenoting the constraints that have been tried for the current controller pair.\nMulti-agent RL (MARL) As a baseline, we consider a setup without a meta-controller, which is the\nstandard multi-agent setup where agents communicate with each other and emit individual actions.\nThe controller agent is same as that in FCRL, except that the position vector is a one-hot vector of\nsize m, denoting Ci’s position in the overall order, and the communication vector is an average of the\noutputs of all other agents in the previous round, similar to CommNet described in [9].\nHierarchical RL (HRL) We also consider a Hierarchical RL baseline as described in [4]. The\ncontrollers do not communicate with each other but instead independently achieve the task of emitting\na time value that is consistent with their database and the meta-controller’s constraint. The meta-\ncontroller is the same as FCRL, except that it picks one agent at a time and assigns it a constraint\nvector.\n4.3\nFCRL Training\nBelow, we present the pseudocode for training the FCRL agent.\nAll controllers share the same replay buffer and the same weights. Additionally, by randomly\nsampling meta-controller constraints, the controllers can be pre-trained to communicate and complete\nsubtasks prior to the joint training as described above. In this case, Q2 will start with these pre-trained\nweights rather than being initialized to be random in the above algorithm.\nFor the distributed scheduling task as described in the experiments, the Critic provides an intrinsic\nreward rintrinsic = 1.0 only if (i) the controllers’ actions are valid according to their constrained\ndatabases (ai ∈Di ∧ct and aj ∈Dj ∧ct), and (ii) the actions are in the correct order (ai < aj). For\nNextControllerPair, we use the heuristic of emitting controller pairs in the order expected by the\nenvironment: {(C1, C2), . . . , (CN−1, CN)}. Alternatively, a separate Q-network could be trained\n4\nAlgorithm 1 Learning algorithm for FCRL agent\n1: Initialize experience replay buffer RM for meta-controller and RC for the controllers\n2: Intialize meta-controller’s Q-network, Q1, and controllers’ Q-network, Q2, with random weights\n3: for episode = 1:N do\n4:\nEnvironment selects m controllers C1, C2, ..., Cm with databases D1, D2, ..., Dm\n5:\ns ←{dp, dt, tc} ←{φ, φ φ}\n▷Meta-controller state is: done pairs (dp), done times (dt),\ntried constraints (tc)\n6:\nt ←0\n7:\nwhile s is not terminal do\n8:\nCi, Cj ←NextControllerPair(s)\n9:\nct ←epsilon_greedy(π(ct|s), ϵM)\n10:\nsCi ←{Di, ct, φ}\n11:\nsCj ←{Dj, ct, φ}\n12:\nfor communication turn = 1:K do\n13:\nai ←epsilon_greedy(π(ai|sCi), ϵCi)\n14:\naj ←epsilon_greedy(π(aj|sCj), ϵCj)\n15:\ns′\nCi ←{Di, ct, aj}\n16:\ns′\nCj ←{Dj, ct, ai}\n17:\nrintrinsic ←Critic(s, ai, aj)\n18:\nStore transition (sCi, ai, rintrinsic, s′\nCi) in RC\n19:\nStore transition (sCj, aj, rintrinsic, s′\nCj) in RC\n20:\nsCi ←s′\nCi\n21:\nsCj ←s′\nCj\n22:\nSample minibatch of transitions from RC and update Q2 weights\n23:\nif rintrinsic > 0 then\n▷Controller pair found a valid schedule\n24:\ndp ←append(dp, (Ci, Cj))\n25:\ndt ←append(dt, ai, aj)\n26:\ntc ←φ\n27:\nelse\n28:\ntc ←append(tc, ct)\n29:\ns′ ←{dp, dt, tc}\n30:\nre ←extrinsic reward from environment\n31:\nStore transition (s, ct, re, s′) in RM\n32:\nSample minibatch of transitions from RM and update Q1 weights\n33:\ns ←s′\n34:\nt ←t + 1\nto select controller pairs, which would be useful in domains where a sequencing of controllers for\npairwise communication is not manifest from the task description.\n4.4\nResults\nWe alternate training and evaluation for 1000 episodes each. Figure 2 plots the average reward on the\nevaluation episodes over the course of training. Each curve is the average of 5 independent runs with\nthe same conﬁguration. We ran three experiments by varying m, i.e. the number of agents that are\npart of the requested schedule and must communicate to come up with a valid plan.\nFor m = 2, all three approaches are able to ﬁnd the optimal policy as it requires only two agents\nto communicate. For m = 4, HRL performs poorly as there is no inter-agent communication and\nthe meta-controller must do all the work of picking the right sequence of constraints to surface a\nvalid schedule. MARL does better as agents can communicate their preferences and get a chance to\nupdate their choices based on what other agents picked. FCRL does better than both baselines, as\nthe meta-controller learns to guide the communications by constraining each pair of agents to focus\non disjoint slices of the database, while the controllers have to only communicate with one other\ncontroller making it easy to agree upon a good pair of actions.\nFor m = 6, both HRL and MARL are unable to see a positive reward, as ﬁnding a valid schedule\nrequires signiﬁcantly more exploration for the meta-controller and controller, respectively. FCRL is\n5\nFigure 2: Comparing FCRL with baselines MARL and HRL, on three environments: (a) easy (m=2),\n(b) medium (m=4), and (c) hard (m=6).\nable to do better by dividing the problem into disjoint subtasks and leveraging temporal abstractions.\nHowever, the meta-controller’s optimal policy is more intricate in this case, as it needs to learn to\nstart with smaller constraint windows and try larger ones if the smaller one fails, so that the earlier\nagent pairs do not choose farther apart times when closer ones are possible.\n5\nDiscussion\nWe presented a framework for combining hierarchical and multi-agent RL to beneﬁt from temporal\nabstractions to reduce the communication complexity for ﬁnding globally consistent solutions with\ndistributed policies. Our experimental results show that this approach scales better than baseline\napproaches as the number of communicating agents increases.\nFuture work The effect of increasing the size of the database or number of communication rounds\nwill be interesting to study. Multi-agent training creates a non-stationary environment for the agent\nas other agents’ policies change over the course of training. While we employ the standard DQN\nalgorithm [6] to train the meta-controller, the controllers can be trained using recent policy gradient\nbased methods (eg. counterfactual gradients [3] ) which address this problem.\nReferences\n[1] A. Das, S. Kottur, J. Moura, and D. Batra. Learning cooperative visual dialog agents with deep\nreinforcement learning. In International Conference on Computer Vision, 2017.\n[2] J. Foerster, Y. Assael, N. Freitas, and S. Whiteson. Learning to communicate with deep\nmulti-agent reinforcement learning. arXiv:1605.06676, 2016.\n[3] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent\npolicy gradients. arXiv preprint arXiv:1705.08926, 2017.\n[4] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement\nlearning: Integrating temporal abstraction and intrinsic motivation. In Advances in Neural\nInformation Processing Systems, pages 3675–3683, 2016.\n[5] M. Lewis, D. Yarats, Y. Dauphin, D. Parikh, and D. Batra. Deal or no deal? end-to-end learning\nof negotiation dialogues. In Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing, pages 2433–2443, 2017.\n[6] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.\nPlaying atari with deep reinforcement learning. arXiv:1312.5602, 2013.\n[7] I. Mordatch and P. Abbeel. Emergence of compositional language in multi-agent populations.\narXiv:1703.04908, 2017.\n[8] B. Peng, X. Li, L. Li, J. Gao, A. Celikyilmaz, S. Lee, and K.-F. Wong.\nCompos-\nite task-completion dialogue policy learning via hierarchical deep reinforcement learning.\narXiv:1704.03084, 2017.\n[9] S. Sukhbaatar, R. Fergus, et al. Learning multiagent communication with backpropagation. In\nAdvances in Neural Information Processing Systems, pages 2244–2252, 2016.\n[10] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction, volume 1. MIT press\nCambridge, 1998.\n6\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2017-12-22",
  "updated": "2017-12-22"
}