{
  "id": "http://arxiv.org/abs/1801.00631v1",
  "title": "Deep Learning: A Critical Appraisal",
  "authors": [
    "Gary Marcus"
  ],
  "abstract": "Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.",
  "text": "Deep Learning:  \nA Critical Appraisal \nGary Marcus  \n1\nNew York University \nAbstract \nAlthough deep learning has historical roots going back decades, neither the term “deep \nlearning” nor the approach was popular just over five years ago, when the field was \nreignited by papers such as Krizhevsky, Sutskever and Hinton’s now classic 2012 \n(Krizhevsky, Sutskever, & Hinton, 2012)deep net model of Imagenet. \nWhat has the field discovered in the five subsequent years? Against a background of \nconsiderable progress in areas such as speech recognition, image recognition, and game \nplaying, and considerable enthusiasm in the popular press, I present ten concerns for deep \nlearning, and suggest that deep learning must be supplemented by other techniques if we \nare to reach artificial general intelligence. \n!   Departments of Psychology and Neural Science, New York University, gary.marcus at nyu.edu. I thank Christina \n1\nChen, François Chollet, Ernie Davis, Zack Lipton, Stefano Pacifico, Suchi Saria, and Athena Vouloumanos for \nsharp-eyed comments, all generously supplied on short notice during the holidays at the close of 2017.\n \n \nPage !  of !\n1\n27\nFor most problems where deep learning has enabled \ntransformationally better solutions (vision, speech), we've \nentered diminishing returns territory in 2016-2017. \nFrançois Chollet, Google, author of Keras \nneural network library  \nDecember 18, 2017 \n‘Science progresses one funeral at a time.' The future \ndepends on some graduate student who is deeply suspicious \nof everything I have said. \nGeoff Hinton, grandfather of deep learning \nSeptember 15, 2017 \n1. Is deep learning approaching a wall? \nAlthough deep learning has historical roots going back decades(Schmidhuber, 2015), it \nattracted relatively little notice until just over five years ago. Virtually everything \nchanged in 2012, with the publication of  a series of highly influential papers such as \nKrizhevsky, Sutskever and Hinton’s 2012 ImageNet Classification with Deep \nConvolutional Neural Networks (Krizhevsky, Sutskever, & Hinton, 2012), which \nachieved state-of-the-art results on the object recognition challenge known as ImageNet \n(Deng et al., ). Other labs were already working on similar work (Cireşan, Meier, Masci, \n& Schmidhuber, 2012). Before the year was out, deep learning made the front page of \nThe New York Times , and it rapidly became the best known technique in artificial \n2\nintelligence, by a wide margin. If the general idea of training neural networks with \nmultiple layers was not new, it was, in part because of increases in computational power \nand data, the first time that deep learning truly became practical. \nDeep learning has since yielded numerous state of the art results, in domains such as \nspeech recognition, image recognition , and language translation and plays a role in a \nwide swath of current AI applications. Corporations have invested billions of dollars \nfighting for deep learning talent. One prominent deep learning advocate, Andrew Ng, has \ngone so far to suggest that “If a typical person can do a mental task with less than one \nsecond of thought, we can probably automate it using AI either now or in the near \n http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-\n2\nintelligence.html\n \n \nPage !  of !\n2\n27\nfuture.” (A, 2016).   A recent New York Times Sunday Magazine article , largely about \n3\ndeep learning, implied that the technique is “poised to reinvent computing itself.” \nYet deep learning may well be approaching a wall, much as I anticipated earlier, at \nbeginning of the resurgence (Marcus, 2012), and as leading figures like Hinton (Sabour, \nFrosst, & Hinton, 2017) and Chollet (2017) have begun to imply in recent months.    \nWhat exactly is deep learning, and what has its shown about the nature of intelligence? \nWhat can we expect it to do, and where might we expect it to break down? How close or \nfar are we from “artificial general intelligence”, and a point at which machines show a \nhuman-like flexibility in solving unfamiliar problems?  The purpose of this paper is both \nto temper some irrational exuberance and also to consider what we as a field might need \nto move forward. \nThis paper is written simultaneously for researchers in the field, and for a growing set of \nAI consumers with less technical background who may wish to understand where the \nfield is headed. As such I will begin with a very brief, nontechnical introduction  aimed at \n4\nelucidating what deep learning systems do well and why (Section 2), before turning to an \nassessment of deep learning’s weaknesses (Section 3) and some fears that arise from \nmisunderstandings about deep learning’s capabilities (Section 4), and closing with  \nperspective on going forward (Section 5).  \nDeep learning is not likely to disappear, nor should it. But five years into the field’s \nresurgence seems like a good moment for a critical reflection, on what deep learning has \nand has not been able to achieve.  \n2. What deep learning is, and what it does well \nDeep learning, as it is primarily used, is essentially a statistical technique for classifying \npatterns, based on sample data, using neural networks with multiple layers.  \n5\n https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html\n3\n For more technical introduction,  there are many excellent recent  tutorials on deep learning including (Chollet, \n4\n2017) and (Goodfellow, Bengio, & Courville, 2016), as well as insightful blogs and online resources from Zachary \nLipton, Chris Olah, and many others.\n Other applications of deep learning beyond classification are possible, too, though currently less popular, and \n5\noutside of the scope of the current article. These include using deep learning as an alternative to regression, as a \ncomponent in generative models that create (e.g.,) synthetic images, as a tool for compressing images, as a tool for \nlearning probability distributions, and (relatedly) as an important technique for approximation known as variational \ninference.\n \n \nPage !  of !\n3\n27\nNeural networks in the deep learning literature typically consist of a set of input units that \nstand for things like pixels or words, multiple hidden layers (the more such layers, the \ndeeper a network is said to be) containing hidden units (also known as nodes or neurons), \nand a set output units, with connections running between those nodes.  In a typical \napplication such a network might be trained on a large sets of handwritten digits (these \nare the inputs, represented as images) and labels (these are the outputs) that identify the \ncategories to which those inputs belong (this image is a 2, that one is a 3, and so forth). \n \nOver time, an algorithm called back-propagation allows a process called gradient descent \nto adjust the connections between units using a process, such that any given input tends to \nproduce the corresponding output. \nCollectively, one can think of the relation between inputs and outputs that a neural \nnetwork learns as a mapping. Neural networks, particularly those with multiple hidden \nlayers (hence the term deep) are remarkably good at learning input-output mappings, \nSuch systems are commonly described as neural networks because the input nodes, \nhidden nodes, and output nodes can be thought of as loosely analogous to biological \nneurons, albeit greatly simplified, and the connections between nodes can be thought of \nas in some way reflecting connections between neurons. A longstanding question, outside \nthe scope of the current paper, concerns the degree to which artificial neural networks are \nbiologically plausible. \nMost deep learning networks make heavy use of a technique called  convolution (LeCun, \n1989), which constrains the neural connections in the network such that they innately \ncapture a property known as translational invariance. This is essentially the idea that an \nobject can slide around an image while maintaining its identity; a circle in the top left can \nbe presumed, even absent direct experience) to be the same as a circle in the bottom right. \n \n \nPage !  of !\n4\n27\nInput layer\nOutput layer\nHidden layers\n...\n...\n...\n...\nDeep learning is also known for its ability to self-generate intermediate representations, \nsuch as internal units that may respond to things like horizontal lines, or more complex \nelements of pictorial structure.   \nIn principle, given infinite data, deep learning systems are powerful enough to represent \nany finite deterministic “mapping” between any given set of inputs and a set of \ncorresponding outputs, though in practice whether they can learn such a mapping \ndepends on many factors. One common concern is getting caught in local minima, in \nwhich a systems gets stuck on a suboptimal solution, with no better solution nearby in the \nspace of solutions being searched. (Experts use a variety of techniques to avoid such \nproblems, to reasonably good effect). In practice, results with large data sets are often \nquite good, on a wide range of potential mappings. \nIn speech recognition, for example, a neural network learns a mapping between a set of \nspeech sounds, and set of labels (such as words or phonemes). In object recognition, a \nneural network learns a mapping between a set of images and a set of labels (such that, \nfor example, pictures of cars are labeled as cars). In DeepMind’s Atari game system \n(Mnih et al., 2015), neural networks learned mappings between pixels and joystick \npositions.  \nDeep learning systems are most often used as classification system in the sense that the \nmission of a typical network is to decide which of a set of categories (defined by the \noutput units on the neural network) a given input belongs to. With enough imagination, \nthe power of classification is immense; outputs can represent words, places on a Go \nboard, or virtually anything else.   \nIn a world with infinite data, and infinite computational resources, there might be little \nneed for any other technique.  \n3. Limits on the scope of deep learning \nDeep learning’s limitations begin with the contrapositive: we live in a world in which \ndata are never infinite. Instead, systems that rely on deep learning frequently have to \ngeneralize beyond the specific data that they have seen, whether to a new pronunciation \nof a word or to an image that differs from one that the system has seen before, and where \ndata are less than infinite, the ability of formal proofs to guarantee high-quality \nperformance is more limited. \n \n \nPage !  of !\n5\n27\nAs discussed later in this article, generalization can be thought of as coming in two \nflavors, interpolation between known examples, and  extrapolation, which requires going \nbeyond a space of known training examples (Marcus, 1998a).    \nFor neural networks to generalize well, there generally must be a large amount of data, \nand the test data must be similar to the training data, allowing new answers to be \ninterpolated in between old ones. In Krizhevsky et al’s paper (Krizhevsky, Sutskever, & \nHinton, 2012), a nine layer convolutional neural network with 60 million parameters and \n650,000 nodes was trained on roughly a million distinct examples drawn from \napproximately one thousand categories.   \n6\nThis sort of brute force approach worked well in the very finite world of ImageNet, into \nwhich all stimuli can be classified into a comparatively small set of categories. It also \nworks well in stable domains like speech recognition in which exemplars are mapped in \nconstant way onto a limited set of speech sound categories, but for many reasons deep \nlearning cannot be considered (as it sometimes is in the popular press) as a general \nsolution to artificial intelligence. \nHere are ten challenges faced by current deep learning systems: \n3.1. Deep learning thus far is data hungry   \nHuman beings can learn abstract relationships in a few trials. If I told you that a schmister \nwas a sister over the age of 10 but under the age of 21, perhaps giving you a single \nexample, you could immediately infer whether you had any schmisters, whether your best \nfriend had a schmister, whether your children or parents had any schmisters, and so forth. \n(Odds are, your parents no longer do, if they ever did, and you could rapidly draw that \ninference, too.)  \nIn learning what a schmister is, in this case through explicit definition, you rely not on \nhundreds or thousands or millions of training examples, but on a capacity to represent \nabstract relationships between algebra-like variables. \nHumans can learn such abstractions, both through explicit definition and more implicit \nmeans (Marcus, 2001). Indeed even  7-month old infants can do so, acquiring learned \nabstract language-like rules from a small number of unlabeled examples, in just two \n Using a common technique known as data augmentation, each example was actually presented along with its label \n6\nin a many different locations, both in its original form and in mirror reversed form. A second type of data \naugmentation varied the brightness of the images, yielding still more examples for training, in order to train the \nnetwork to recognize images with different intensities. Part of the art of machine learning involves knowing what \nforms of data augmentation will and won’t help within a given system.\n \n \nPage !  of !\n6\n27\nminutes (Marcus, Vijayan, Bandi Rao, & Vishton, 1999). Subsequent work by Gervain \nand colleagues (2012) suggests that newborns are capable of similar computations. \nDeep learning currently lacks a mechanism for learning abstractions through explicit, \nverbal definition, and works best when there are thousands, millions or even billions of \ntraining examples, as in DeepMind’s work on board games and Atari. As Brenden Lake \nand his colleagues have recently emphasized in a series of papers, humans are far more \nefficient in learning complex rules than deep learning systems are (Lake, Salakhutdinov, \n& Tenenbaum, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2016). (See also related \nwork by George et al (2017), and my own work with Steven Pinker on children’s \noverregularization errors in comparison to neural networks (Marcus et al., 1992).)  \nGeoff Hinton has also worried about deep learning’s reliance on large numbers of labeled \nexamples, and expressed this concern in his recent work on capsule networks with his \ncoauthors (Sabour et al., 2017) noting that convolutional neural networks (the most \ncommon deep learning architecture) may face “exponential inefficiencies that may lead to \ntheir demise. A good candidate is the difficulty that convolutional nets have in \ngeneralizing to novel viewpoints [ie perspectives on object in visual recognition tasks]. \nThe ability to deal with translation[al invariance] is built in, but for the other ... [common \ntype of] transformation we have to chose between replicating feature detectors on a grid \nthat grows exponentially ... or increasing the size of the labelled training set in a similarly \nexponential way.”   \nIn problems where data are limited, deep learning often is not an ideal solution. \n3.2.Deep learning thus far is shallow and has limited capacity for \ntransfer \nAlthough deep learning is capable of some amazing things, it is important to realize that \nthe word “deep” in deep learning refers to a technical, architectural property (the large \nnumber of hidden layers used in a modern neural networks, where there predecessors \nused only one) rather than a conceptual one (the representations acquired by such \nnetworks don’t, for example, naturally apply to abstract concepts like “justice”, \n“democracy” or “meddling”). \nEven more down-to-earth concepts like “ball” or “opponent” can lie out of reach. \nConsider for example DeepMind’s Atari game work (Mnih et al., 2015) on deep \nreinforcement learning, which combines deep learning with reinforcement learning (in \nwhich a learner tries to maximize reward). Ostensibly, the results are fantastic: the system \nmeets or beats human experts on a large sample of games using a single set of \n“hyperparameters” that govern properties such as the rate at which a network alters its \nweights, and no advance knowledge about specific games, or even their rules.  But it is \n \n \nPage !  of !\n7\n27\neasy to wildly overinterpret what the results show. To take one example, according to a \nwidely-circulated video of the system learning to play the brick-breaking Atari game \nBreakout, “after 240 minutes of training, [the system] realizes that digging a tunnel \nthought the wall is the most effective technique to beat the game”.  \nBut the system has learned no such thing; it doesn’t really understand what a tunnel, or \nwhat a wall is; it has just learned specific contingencies for particular scenarios. Transfer \ntests — in which the deep reinforcement learning system is confronted with scenarios \nthat differ in minor ways from the one ones on which the system was trained show that \ndeep reinforcement learning’s solutions are often extremely superficial. For example, a \nteam of researchers at Vicarious showed that a more efficient successor technique, \nDeepMind’s Atari system [Asynchronous Advantage Actor-Critic; also known as A3C], \nfailed on a variety of minor perturbations to Breakout (Kansky et al., 2017) from the \ntraining set, such as moving the Y coordinate (height) of the paddle, or inserting a wall \nmidscreen. These demonstrations make clear that  it is misleading to credit deep \nreinforcement learning with inducing concept like wall or paddle; rather, such remarks \nare what comparative (animal) psychology sometimes call overattributions. It’s not that \nthe Atari system genuinely learned a concept of wall that was robust but rather the system \nsuperficially approximated breaking through walls within a narrow set of highly trained \ncircumstances.    \n7\nMy own team of researchers at a startup company called Geometric Intelligence (later \nacquired by Uber) found similar results as well, in the context of a slalom game, In 2017, \na team of researchers at Berkeley and OpenAI has shown that it was not difficult to \nconstruct comparable adversarial examples in a variety of games, undermining not only \nDQN (the original DeepMind algorithm) but also A3C and several other related \ntechniques (Huang, Papernot, Goodfellow, Duan, & Abbeel, 2017). \nRecent experiments by Robin Jia and Percy Liang (2017) make a similar point, in a \ndifferent domain: language. Various neural networks were trained on a question \nanswering task known as SQuAD (derived from the Stanford Question Answering \nDatabase), in which the goal is to highlight the words in a particular passage that \ncorrespond to a given question. In one sample, for instance, a trained system correctly, \nand impressively, identified the quarterback on the winning of Super Bowl XXXIII as \nJohn Elway, based on a short paragraph. But Jia and Liang showed the mere insertion of \ndistractor sentences (such as a fictional one about the alleged victory of Google’s Jeff \n In the same paper, Vicarious proposed an alternative to deep learning called schema networks (Kansky et al., 2017) \n7\nthat can handle a number of variations in the Atari game Breakout, albeit apparently without the multi-game \ngenerality of DeepMind’s Atari system. \n \n \nPage !  of !\n8\n27\nDean in another Bowl game ) caused performance to drop precipitously. Across sixteen \n8\nmodels, accuracy dropped from a mean of 75% to a mean of 36%.  \nAs is so often the case, the patterns extracted by deep learning are more superficial than \nthey initially appear. \n3.3.Deep learning thus far has no natural way to deal with \nhierarchical structure \nTo a linguist like Noam Chomsky, the troubles Jia and Liang documented would be \nunsurprising. Fundamentally, most current deep-learning based language models \nrepresent sentences as mere sequences of words, whereas Chomsky has long argued that \nlanguage has a hierarchical structure, in which larger structures are recursively \nconstructed out of smaller components.  (For example, in the sentence the teenager who \npreviously crossed the Atlantic set a record for flying around the world, the main clause is \nthe teenager set a record for flying around the world, while the embedded clause who \npreviously crossed the Atlantic is an embedded clause that specifies which teenager.) \nIn the 80’s Fodor and Pylyshyn (1988)expressed similar concerns, with respect to an \nearlier breed of neural networks. Likewise, in (Marcus, 2001), I conjectured that single \nrecurrent neural networks (SRNs; a forerunner to today’s more sophisticated  deep \nlearning based recurrent neural networks, known as RNNs; Elman, 1990) would have \ntrouble systematically representing and extending recursive structure to various kinds of \nunfamiliar sentences (see the cited articles for more specific claims about which types).  \nEarlier this year, Brenden Lake and Marco Baroni (2017) tested whether such pessimistic \nconjectures continued to hold true. As they put it in their title, contemporary neural nets \nwere “Still not systematic after all these years”. RNNs could “generalize well when the \ndifferences between training and test ...  are small [but] when generalization requires \nsystematic compositional skills, RNNs fail spectacularly”.  \nSimilar issues are likely to emerge in other domains, such as planning and motor control, \nin which complex hierarchical structure is needed, particular when a system is likely to \nencounter novel situations. One can see indirect evidence for this in the struggles with \ntransfer in Atari games mentioned above, and more generally in the field of robotics, in \nwhich systems generally fail to generalize abstract plans well in novel environments. \n Here’s the full Super Bowl passage; Jia and Liang’s distractor sentence that confused the model is at the end. \n8\nPeyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also \nthe oldest quarterback ever to play in a Super Bowl at age 39. The past record was held by John Elway, who led the \nBroncos to victory in Super Bowl XXXIII at age 38 and is currently Denver’s Executive Vice President of Football \nOperations and General Manager. Quarterback Jeff Dean had jersey number 37 in Champ Bowl XXXIV.\n \n \nPage !  of !\n9\n27\nThe core problem, at least at present, is that deep learning learns correlations between \nsets of features that are themselves “flat” or nonhierachical, as if in a simple, unstructured \nlist, with every feature on equal footing. Hierarchical structure (e.g., syntactic trees that \ndistinguish between main clauses and embedded clauses in a sentence) are not inherently \nor directly represented in such systems, and as a result deep learning systems are forced \nto use a variety of proxies that are ultimately inadequate, such as the sequential position \nof a word presented in a sequences.  \nSystems like Word2Vec (Mikolov, Chen, Corrado, & Dean, 2013) that represent \nindividuals words as vectors have been modestly successful; a number of systems that \nhave used clever tricks  \ntry to represent complete sentences in deep-learning compatible vector spaces (Socher, \nHuval, Manning, & Ng, 2012). But, as Lake and Baroni’s experiments make clear.  \nrecurrent networks continue limited in their capacity to represent and generalize rich \nstructure in a faithful manner. \n3.4.Deep learning thus far has struggled with open-ended inference  \nIf you can’t represent nuance like the difference between “John promised Mary to leave” \nand “John promised to leave Mary”, you can’t draw inferences about who is leaving \nwhom, or what is likely to happen next. Current machine reading systems have achieved \nsome degree of success in tasks like SQuAD, in which the answer to a given  \nquestion is explicitly contained within a text, but far less success in tasks in which \ninference goes beyond what is explicit in a text, either by combining multiple sentences \n(so called multi-hop inference) or by combining explicit sentences with background \nknowledge that is not stated in a specific text selection.  Humans, as they read texts, \nfrequently derive wide-ranging inferences that are both novel and only implicitly \nlicensed, as when they, for example, infer the intentions of a character based only on \nindirect dialog.  \nAltough Bowman and colleagues (Bowman, Angeli, Potts, & Manning, 2015; Williams, \nNangia, & Bowman, 2017) have taken some important steps in this direction, there is, at \npresent, no deep learning system that can draw open-ended inferences based on real-\nworld knowledge with anything like human-level accuracy. \n3.5.Deep learning thus far is not sufficiently transparent  \nThe relative opacity of “black box” neural networks has been a major focus of discussion \nin the last few years (Samek, Wiegand, & Müller, 2017; Ribeiro, Singh, & Guestrin, \n2016). In their current incarnation, deep learning systems have millions or even billions \nof parameters, identifiable to their developers not in terms of the sort of human \n \n \nPage !\n of !\n10\n27\ninterpretable labels that canonical programmers use (“last_character_typed”) but only in \nterms of their geography within a complex network (e.g., the activity value of the ith node \nin layer j in network module k). Although some strides have been in visualizing the \ncontributions of individuals nodes in complex networks (Nguyen, Clune, Bengio, \nDosovitskiy, & Yosinski, 2016), most observers would acknowledge that neural networks \nas a whole remain something of a black box.  \nHow much that matters in the long run remains unclear (Lipton, 2016). If systems are \nrobust and self-contained enough it might not matter; if it is important to use them in the \ncontext of larger systems, it could be crucial for debuggability.  \nThe transparency issue, as yet unsolved, is a potential liability when using deep learning \nfor problem domains like financial trades or medical diagnosis, in which human users \nmight like to understand how a given system made a given decision. As Catherine \nO’Neill (2016) has pointed out, such opacity can also lead to serious issues of bias. \n  \n3.6.Deep learning thus far has not been well integrated with prior \nknowledge \nThe dominant approach in deep learning is hermeneutic, in the sense of being self-\ncontained and isolated from other, potentially usefully knowledge. Work in deep learning \ntypically consists of finding a training database, sets of inputs associated with respective \noutputs, and learn all that is required for the problem by learning the relations between \nthose inputs and outputs, using whatever clever architectural variants one might devise, \nalong with techniques for cleaning and augmenting the data set.  With just a handful of \nexceptions, such as LeCun’s convolutional constraint on how neural networks are \nwired(LeCun, 1989), prior knowledge is often deliberately minimized.  \nThus, for example, in a system like Lerer et al’s (2016) efforts to learn about the physics \nof falling towers, there is no prior knowledge of physics (beyond what is implied in \nconvolution). Newton’s laws, for example, are not explicitly encoded; the system instead \n(to some limited degree) approximates them by learning contingencies from raw, pixel \nlevel data. As I note in a forthcoming paper in innate (Marcus, in prep) researchers in \ndeep learning appear to have a very strong bias against including prior knowledge even \nwhen (as in the case of physics) that prior knowledge is well known.  \nIt also not straightforward in general how to integrate prior knowledge into a deep \nlearning system:, in part because the knowledge represented in deep learning systems \npertains mainly to (largely opaque) correlations between features, rather than to \nabstractions like quantified statements (e.g. all men are mortal), see discussion of \nuniversally-quantified one-to-one-mappings in Marcus (2001), or generics (violable \n \n \nPage !\n of !\n11\n27\nstatements like dogs have four legs or mosquitos carry West Nile virus (Gelman, Leslie, \nWas, & Koch, 2015)). \nA related problem stems from a culture in machine learning that emphasizes competition \non problems that are inherently self-contained, without little need for broad general \nknowledge. This tendency is well exemplified by the machine learning contest platform \nknown as Kaggle, in which contestants vie for the best results on a given data set. \nEverything they need for a given problem is neatly packaged, with all the relevant input \nand outputs files. Great progress has been made in this way; speech recognition and some \naspects of image recognition can be largely solved in the Kaggle paradigm.  \nThe trouble, however, is that life is not a Kaggle competition; children don’t get all the \ndata they need neatly packaged in a single directory. Real-world learning offers data \nmuch more sporadically, and problems aren’t so neatly encapsulated. Deep learning \nworks great on problems like speech recognition in which there are lots of labeled \nexamples, but scarcely any even knows how to apply it to more open-ended problems. \nWhat’s the best way to fix a bicycle that has a rope caught in its spokes? Should I major \nin math or neuroscience? No training set will tell us that.  \nProblems that have less to do with categorization and more to do with commonsense \nreasoning essentially lie outside the scope of what deep learning is appropriate for, and so \nfar as I can tell, deep learning has little to offer such problems. In a recent review of \ncommonsense reasoning, Ernie Davis and I (2015) began with a set of easily-drawn \ninferences that people can readily answer without anything like direct training, such as \nWho is taller, Prince William or his baby son Prince George? Can you make a salad out \nof a polyester shirt? If you stick a pin into a carrot, does it make a hole in the carrot or in the \npin?  \nAs far as I know, nobody has even tried to tackle this sort of thing with deep learning. \n  \nSuch apparently simple problems require humans to integrate knowledge across vastly disparate \nsources, and as such are a long way from the sweet spot of deep learning-style perceptual \nclassification. Instead, they are perhaps best thought of as a sign that entirely different \nsorts of tools are needed, along with deep learning, if we are to reach human-level \ncognitive flexibility.  \n3.7.Deep learning thus far cannot inherently distinguish causation \nfrom correlation \nIf it is a truism that causation does not equal correlation, the distinction between the two \nis also a serious concern for deep learning. Roughly speaking, deep learning learns \ncomplex correlations between input and output features, but with no inherent \n \n \nPage !\n of !\n12\n27\nrepresentation of causality. A deep learning system can easily learn that height and \nvocabulary are, across the population as a whole, correlated, but less easily represent the \nway in which that correlation derives from growth and development (kids get bigger as \nthey learn more words, but that doesn’t mean that growing tall causes them to learn more \nwords, nor that learning new words causes them to grow). Causality has been central \nstrand in some other approaches to AI (Pearl, 2000) but, perhaps because deep learning is \nnot geared towards such challenges, relatively little work within the deep learning \ntradition has tried to address it.  \n9\n3.8.Deep learning presumes a largely stable world, in ways that may \nbe problematic \nThe logic of deep learning is such that it is likely to work best in highly stable worlds, \nlike the board game Go, which has unvarying rules, and less well in systems such as \npolitics and economics that are constantly changing. To the extent that deep learning is \napplied in tasks such as stock prediction, there is a good chance that it will eventually \nface the fate of Google Flu Trends, which initially did a great job of predicting \nepidemological data on search trends, only to complete miss things like the peak of the \n2013 flu season  (Lazer, Kennedy, King, & Vespignani, 2014). \n3.9. Deep learning thus far works well as an approximation, but its \nanswers often cannot be fully trusted \nIn part as a consequence of the other issues raised in this section, deep learning systems \nare quite good at some large fraction of a given domain, yet easily fooled. \nAn ever-growing array of papers has shown this vulnerability, from the linguistic \nexamples of Jia and Liang mentioned above to a wide range of demonstrations in the \ndomain of vision, where deep learning systems have mistaken yellow-and-black patterns \nof stripes for school buses (Nguyen, Yosinski, & Clune, 2014) and sticker-clad parking \nsigns for well-stocked refrigerators (Vinyals, Toshev, Bengio, & Erhan, 2014) in the \ncontext of a captioning system that otherwise seems impressive. \nMore recently, there have been real-world stop signs, lightly defaced, that have been \nmistaken for speed limit signs (Evtimov et al., 2017) and 3d-printed turtles that have been \nmistake for rifles  (Athalye, Engstrom, Ilyas, & Kwok, 2017). A recent news story \n One example of interesting recent work is (Lopez-Paz, Nishihara, Chintala, Schölkopf, & Bottou, 2017), albeit \n9\nfocused specifically on an rather unusual sense of the term causation as it relates to the presence or absence of \nobjects (e.g., “the presence of cars cause the presence of wheel[s]). This strikes me as quite different from the sort of \ncausation one finds in the relation between a disease and the symptoms it causes.\n \n \nPage !\n of !\n13\n27\nrecounts the trouble a British police system has had in distinguishing nudes from sand \ndunes.  \n10\nThe “spoofability” of deep learning systems was perhaps first noted by Szegedy et \nal(2013). Four years later, despite much active research, no robust solution has been \nfound.   \n11\n3.10. Deep learning thus far is difficult to engineer with \nAnother fact that follows from all the issues raised above is that is simply hard to do \nrobust engineering with deep learning. As a team of authors at Google put it in 2014, in \nthe title of an important, and as yet unanswered essay (Sculley, Phillips, Ebner, \nChaudhary, & Young, 2014), machine learning is “the high-interest credit card of \ntechnical debt”, meaning that is comparatively easy to make systems that work in some \nlimited set of circumstances (short term gain), but quite difficult to guarantee that they \nwill work in alternative circumstances with novel data that may not resemble previous \ntraining data (long term debt, particularly if one system is used as an element in another \nlarger system).  \nIn an important talk at ICML, Leon Bottou (2015) compared machine learning to the \ndevelopment of an airplane engine, and noted that while the airplane design relies on \nbuilding complex systems out of simpler systems for which it was possible to create \nsound guarantees about performance, machine learning lacks the capacity to produce \ncomparable guarantees. As Google’s Peter Norvig (2016) has noted, machine learning as \nyet lacks the incrementality, transparency and debuggability of classical programming, \ntrading off a kind of simplicity for deep challenges in achieving robustness.  \nHenderson and colleagues have recently extended these points, with a focus on deep \nreinforcement learning, noting some serious issues in the field related to robustness and \nreplicability (Henderson et al., 2017). \nAlthough there has been some progress in automating the process of developing machine \nlearning systems (Zoph, Vasudevan, Shlens, & Le, 2017), there is a long way to go. \n https://gizmodo.com/british-cops-want-to-use-ai-to-spot-porn-but-it-keeps-m-1821384511/amp\n10\n Deep learning’s predecessors were vulnerable to similar problems, as Pinker and Prince (1988)pointed out, in a \n11\ndiscussion of neural networks that produced bizarre past tense forms for a subset of its inputs. The verb to mail, for \nexample, was inflected in the past tense as membled, the verb tour as toureder. Children rarely if ever make mistakes \nlike these.\n \n \nPage !\n of !\n14\n27\n3.11. Discussion \n  \nOf course, deep learning, is by itself, just mathematics; none of the problems identified \nabove are because the underlying mathematics of deep learning are somehow flawed. In \ngeneral, deep learning is a perfectly fine way of optimizing a complex system for \nrepresenting a mapping between inputs and outputs, given a sufficiently large data set. \nThe real problem lies in misunderstanding what deep learning is, and is not, good for. The \ntechnique excels at solving closed-end classification problems, in which a wide range of \npotential signals must be mapped onto a limited number of categories, given that there is \nenough data available and the test set closely resembles the training set.  \nBut deviations from these assumptions can cause problems; deep learning is just a \nstatistical technique, and all statistical techniques suffer from deviation from their \nassumptions.  \nDeep learning systems work less well when there are limited amounts of training data \navailable, or when the test set differs importantly from the training set, or when the space \nof examples is broad and filled with novelty. And some problems cannot, given real-\nworld limitations, be thought of as classification problems at all. Open-ended natural \nlanguage understanding, for example, should not be thought of as a classifier mapping \nbetween a large finite set of sentences and large, finite set of sentences, but rather a \nmapping between a potentially infinite range of input sentences and an equally vast array \nof meanings, many never previously encountered. In a problem like that, deep learning \nbecomes a square peg slammed into a round hole, a crude approximation when there \nmust be a solution elsewhere. \nOne clear way to get an intuitive sense of why something is amiss to consider a set of \nexperiments I did long ago, in 1997, when I tested some simplified aspects of language \ndevelopment on a class of neural networks that were then popular in cognitive science. \nThe 1997-vintage networks were, to be sure, simpler than current models — they used no \nmore than three layers (inputs nodes connected to hidden nodes connected to outputs \nnode), and lacked Lecun’s powerful convolution technique. But they were driven by \nbackpropagation just as today’s systems are, and just as beholden to their training data.  \nIn language, the name of the game is generalization — once I hear a sentence like John \npilked a football to Mary, I can infer that is also grammatical to say John pilked Mary the \nfootball, and Eliza pilked the ball to Alec; equally if I can infer what the word pilk means, \nI can infer what the latter sentences would mean, even if I had not hear them before. \n \n \nPage !\n of !\n15\n27\nDistilling the broad-ranging problems of language down to a simple example that I \nbelieve still has resonance now, I ran a series of experiments in which I trained three-\nlayer perceptrons (fully connected in today’s technical parlance, with no convolution) on \nthe identity function, f(x) = x, e.g, f(12)=12.  \nTraining examples were represented by a set of input nodes (and corresponding output \nnodes) that represented numbers in terms of binary digits. The number 7 for example, \nwould be represented by turning on the input (and output) nodes representing 4, 2, and 1.  \nAs a test of generalization, I trained the network on various sets of even numbers, and \ntested it all possible inputs, both odd and even. \nEvery time I ran the experiment, using a wide variety of parameters, the results were the \nsame: the network would (unless it got stuck in local minimum) correctly apply the \nidentity function to the even numbers that it had seen before (say 2, 4, 8 and 12), and to \nsome other even numbers (say 6 and 14) but fail on all the odds numbers, yielding, for \nexample f(15) = 14.  \nIn general, the neural nets I tested could learn their training examples, and interpolate to a \nset of test examples that were in a cloud of points around those examples in n-\ndimensional space (which I dubbed the training space), but they could not extrapolate \nbeyond that training space. \nOdd numbers were outside the training space, and the networks could not generalize \nidentity outside that space.  Adding more hidden units didn’t help, and nor did adding \n12\nmore hidden layers. Simple multilayer perceptrons simply couldn’t generalize outside \ntheir training space (Marcus, 1998a; Marcus, 1998b; Marcus, 2001). (Chollet makes quite \nsimilar points in the closing chapters of his his (Chollet, 2017) text.) \nWhat we have seen in this paper is that challenges in generalizing beyond a space of \ntraining examples persist in current deep learning networks, nearly two decades later. \nMany of the problems reviewed in this paper — the data hungriness, the vulnerability to \nfooling, the problems in dealing with open-ended inference and transfer —   can be seen \nas extension of this fundamental problem. Contemporary neural networks do well on \nchallenges that remain close to their core training data, but start to break down on cases \nfurther out in the periphery.  \n Of course, the network had never seen an odd number before, but pretraining the network on odd numbers in a \n12\ndifferent context didn’t help. And of course people, in contrast, readily generalize to novel words immediately upon \nhearing them. Likewise, the experiments I did with seven-month-olds consisted entirely of novel words.\n \n \nPage !\n of !\n16\n27\nThe widely-adopted addition of convolution guarantees that one particular class of \nproblems that are akin to my identity problem can be solved: so-called translational \ninvariances, in which an object retains its identity when it is shifted to a location.  But the \nsolution is not general, as for example Lake’s recent demonstrations show. (Data \naugmentation offers another way of dealing with deep learning’s challenges in \nextrapolation, by trying to broaden the space of training examples itself, but such \ntechniques are more useful in 2d vision than in language).  \nAs yet there is no general solution within deep learning to the problem of generalizing \noutside the training space. And it is for that reason, more than any other, that we need to \nlook to different kinds of solutions if we want to reach artificial general intelligence. \n4. Potential risks of excessive hype \nOne of the biggest risks in the current overhyping of AI is another AI winter, such as the \none that devastated the field in the 1970’s, after the Lighthill report (Lighthill, 1973), \nsuggested that AI was too brittle, too narrow and too superficial to be used in practice. \nAlthough there are vastly more practical applications of AI now than there were in the \n1970s, hype is still a major concern. When a high-profile figure like Andrew Ng writes in \nthe Harvard Business Review promising a degree of imminent automation that is out of \nstep with reality, there is fresh risk for seriously dashed expectations. Machines cannot in \nfact do many things that ordinary humans can do in a second, ranging from reliably \ncomprehending the world to understanding sentences. No healthy human being would \never mistake a turtle for a rifle or parking sign for a refrigerator. \nExecutives investing massively in AI may turn out to be disappointed, especially given \nthe poor state of the art in natural language understanding. Already, some major projects \nhave been largely abandoned, like Facebook’s M project, which was launched in August \n2015 with much publicity  as a general purpose personal assistant, and then later \n13\ndowngraded to a significantly smaller role, helping users with a vastly small range of \nwell-defined tasks such as calendar entry. \nIt is probably fair to say that chatbots in general have not lived up to the hype they \nreceived a couple years ago. If, for example, driverless car should also, disappoint, \nrelative to their early hype, by proving unsafe when rolled out at scale, or simply not \nachieving full autonomy after many promises, the whole field of AI could be in for a \nsharp downturn, both in popularity and funding. We already may be seeing hints of this, \n https://www.wired.com/2015/08/how-facebook-m-works/\n13\n \n \nPage !\n of !\n17\n27\nas in a just published Wired article  that was entitled “After peak hype, self-driving cars \n14\nenter the trough of disillusionment.”  \nThere are other serious fears, too, and not just of the apocalyptic variety (which for now \nto still seem to be stuff of science fiction).  My own largest fear is that the field of AI \ncould get trapped in a local minimum, dwelling too heavily in the wrong part of \nintellectual space, focusing too much on the detailed exploration of a particular class of \naccessible but limited models that are geared around capturing low-hanging fruit — \npotentially neglecting riskier excursions that might ultimately lead to a more robust path.  \nI am reminded of Peter Thiel’s famous (if now slightly outdated) damning of an often \ntoo-narrowly focused tech industry: “We wanted flying cars, instead we got 140 \ncharacters”. I still dream of Rosie the Robost, a full-service domestic robot that take of \nmy home; but for now, six decades into the history of AI, our bots do little more than play \nmusic, sweep floors, and bid on advertisements. \nIf didn’t make more progress, it would be a shame. AI comes with risk, but also great \npotential rewards. AI’s greatest contributions to society, I believe, could and should \nultimately come in domains like automated scientific discovery, leading among other \nthings towards vastly more sophisticated versions of medicine than are currently possible. \nBut to get there we need to make sure that the field as whole doesn’t first get stuck in a \nlocal minimum.  \n5. What would be better?  \nDespite all of the problems I have sketched,  I don’t think that we need to abandon deep \nlearning.  \nRather, we need to reconceptualize it: not as a universal solvent, but simply as one tool \namong many,  a power screwdriver in a world in which we also need hammers, wrenches, \nand pliers, not to mentions chisels and drills, voltmeters, logic probes, and oscilloscopes.  \nIn perceptual classification, where vast amounts of data are available, deep learning is a \nvaluable tool; in other, richer cognitive domains, it is often far less satisfactory. \nThe question is, where else should we look? Here are four possibilities.  \n https://www.wired.com/story/self-driving-cars-challenges/\n14\n \n \nPage !\n of !\n18\n27\n5.1.Unsupervised learning \nIn interviews, deep learning pioneers Geoff Hinton and Yann LeCun have both recently \npointed to unsupervised learning as one key way in which to go beyond supervised, data-\nhungry versions of deep learning. \nTo be clear, deep learning and unsupervised learning are not in logical opposition. Deep \nlearning has mostly been used in a supervised context with labeled data, but there are \nways of using deep learning in an unsupervised fashion. But there is certainly reasons in \nmany domains to move away from the massive demands on data that supervised deep \nlearning typically requires. \nUnsupervised learning, as the term is commonly used, tends to refer to several kinds of \nsystems. One common type of system “clusters” together inputs that share properties, \neven without having them explicitly labeled. Google’s cat detector model (Le et al., 2012) \nis perhaps the most publicly prominent example of this sort of approach. \nAnother approach, advocated researchers such as Yann LeCun (Luc, Neverova, Couprie, \nVerbeek, & LeCun, 2017), and not mutually exclusive with the first, is to replace labeled \ndata sets with things like movies that change over time. The intuition is that systems \ntrained on videos can use each pair of successive frames as a kind of ersatz teaching \nsignal, in which the goal is to predict the next frame; frame t becomes a predictor for \nframe t1, without the need for any human labeling.  \nMy view is that both of these approaches are useful (and so are some others not discussed \nhere), but that neither inherently solve the sorts of problems outlined in section 3. One is \nstill left with data hungry systems that lack explicit variables, and I see no advance there \ntowards open-ended inference, interpretability or debuggability. \nThat said, there is a different notion of unsupervised learning, less discussed, which I find \ndeeply interesting: the kind of unsupervised learning that human children do. Children \noften y set themselves a novel task, like creating a tower of Lego bricks or climbing \nthrough a small aperture, as my daughter recently did in climbing through a chair, in the \nspace between the seat and the chair back . Often, this sort of exploratory problem \nsolving involves (or at least appears to involve) a good deal of autonomous goal setting  \n(what should I do?) and high level problem solving (how do I get my arm through the \nchair, now that the rest of my body has passed through?), as well the integration of \nabstract knowledge (how bodies work, what sorts of apertures and affordances various \nobjects have, and so forth).  If we could build systems that could set their own goals and \ndo reasoning and problem-solving at this more abstract level, major progress might \nquickly follow. \n \n \nPage !\n of !\n19\n27\n5.2.Symbol-manipulation, and the need for hybrid models \nAnother place that we should look is towards classic, “symbolic” AI, sometimes referred \nto as GOFAI (Good Old-Fashioned AI). Symbolic AI takes its name from the idea, central \nto mathematics, logic, and computer science, that abstractions can be represented by \nsymbols. Equations like f = ma allow us to calculate outputs for a wide range of inputs, \nirrespective of whether we have seen any particular values before; lines in computer \nprograms do the same thing (if the value of variable x is greater than the value of variable \ny, perform action a).  \nBy themselves, symbolic systems have often proven to be brittle, but they were largely \ndeveloped in era with vastly less data and computational power than we have now. The \nright move today may be to integrate deep learning, which excels at perceptual \nclassification, with symbolic systems, which excel at inference and abstraction. One \nmight think such a potential merger on analogy to the brain; perceptual input systems, \nlike primary sensory cortex, seem to do something like what deep learning does, but there \nare other areas, like Broca’s area and prefrontal cortex, that seem to operate at much \nhigher level of abstraction. The power and flexibility of the brain comes in part from its \ncapacity  to dynamically integrate many different computations in real-time. The process \nof scene perception, for instance, seamlessly integrates direct sensory information with \ncomplex abstractions about objects and their properties, lighting sources, and so forth.  \nSome tentative steps towards integration already exist, including neurosymbolic \nmodeling (Besold et al., 2017) and recent trend towards systems such as differentiable \nneural computers (Graves et al., 2016), programming with differentiable interpreters \n(Bošnjak, Rocktäschel, Naradowsky, & Riedel, 2016), and neural programming with \ndiscrete operations (Neelakantan, Le, Abadi, McCallum, & Amodei, 2016). While none \nof this work has yet fully scaled towards anything like full-service artificial general \nintelligence, I have long argued (Marcus, 2001) that more on integrating microprocessor-\nlike operations into neural networks could be extremely valuable.  \nTo the extent that the brain might be seen as consisting of “a broad  array of reusable \ncomputational primitives—elementary units of  processing akin to sets of basic  \ninstructions in a microprocessor—perhaps wired together in parallel, as in the \nreconfigurable integrated circuit type known as the field-programmable gate array”, as I \nhave argued elsewhere(Marcus, Marblestone, & Dean, 2014), steps towards enriching the \ninstruction set out of which our computational systems are built can only be a good thing. \n \n \nPage !\n of !\n20\n27\n5.3.More insight from cognitive and developmental psychology \nAnother potential valuable place to look is human cognition (Davis & Marcus, 2015; \nLake et al., 2016; Marcus, 2001; Pinker & Prince, 1988). There is no need for machines \nto literally replicate the human mind, which is, after all, deeply error prone, and far from \nperfect. But there remain many areas, from natural language understanding to \ncommonsense reasoning, in which humans still retain a clear advantage; learning the \nmechanisms underlying those human strengths could lead to advances in AI, even the \ngoal is not, and should not be, an exact replica of human brain. \nFor many people, learning from humans means neuroscience; in my view, that may be \npremature. We don’t yet know enough about neuroscience to literally reverse engineer the \nbrain, per se, and may not for several decades, possibly until AI itself gets better. AI can \nhelp us to decipher the brain, rather than the other way around. \nEither way, in the meantime, it should certainly be possible to use techniques and insights \ndrawn from cognitive and developmental and psychology, now, in order to build more \nrobust and comprehensive artificial intelligence, building models that are motivated not \njust by mathematics but also by clues from the strengths of  human psychology.  \nA good starting point might be to first to try understand the innate machinery in humans \nminds, as a source of hypotheses into mechanisms that might be valuable in developing \nartificial intelligences; in companion article to this one (Marcus, in prep) I summarize a \nnumber of possibilities, some drawn from my own earlier work (Marcus, 2001) and \nothers from Elizabeth Spelke’s (Spelke & Kinzler, 2007).  Those drawn from my own \nwork focus on how information might be represented and manipulated, such as by \nsymbolic  mechanisms for representing variables and distinctions between kinds and \nindividuals from a class; those drawn from Spelke focus on how infants might represent \nnotions such as space, time, and object. \nA second focal point might be on common sense knowledge, both in how it develops \n(some might be part of our innate endowment, much of it is learned), how it is \nrepresented, and how it is integrated on line in the process of our interactions with the \nreal world (Davis & Marcus, 2015). Recent work by Lerer et al (2016), Watters and \ncolleagues (2017),  Tenenbaum  and colleagues(Wu, Lu, Kohli, Freeman, & Tenenbaum, \n2017) and Davis and myself (Davis, Marcus, & Frazier-Logue, 2017) suggest some \ncompeting approaches to how to think about this, within the domain of everyday physical \nreasoning.  \n \n \nPage !\n of !\n21\n27\nA third focus might be on human understanding of narrative, a notion long ago suggested \nby Roger Schank and Abelson (1977) and due for a refresh (Marcus, 2014; Kočiský et al., \n2017). \n5.4.Bolder challenges \nWhether deep learning persists in current form, morphs into something new, or gets \nreplaced altogether, one might consider a variety of challenge problems that push systems \nto move beyond what can be learned in supervised learning paradigms with large \ndatasets. Drawing in part of from a recent special issue of AI Magazine  devoted to \nmoving beyond the Turing Test that I edited with Francesca Rossi, Manuelo Veloso \n(Marcus, Rossi, Veloso - AI Magazine, & 2016, 2016), here are a few suggestions: \n• A comprehension challenge (Paritosh & Marcus, 2016; Kočiský et al., 2017)] which \nwould require a system to watch an arbitrary video (or read a text, or listen to a \npodcast) and answer open-ended questions about what is contained therein. (Who is the \nprotagonist? What is their motivation? What will happen if the antagonist succeeds in \nher mission?) No specific supervised training set can cover all the possible \ncontingencies; infererence and real-world knowledge integration are necessities. \n• Scientific reasoning and understanding, as in the Allen AI institute’s 8th grade science \nchallenge (Schoenick, Clark, Tafjord, P, & Etzioni, 2017; Davis, 2016). While the \nanswers to many basic science questions can simply be retrieved from web searches, \nothers require inference beyond what is explicitly stated, and the integration of general \nknowledge. \n• General game playing (Genesereth, Love, & Pell, 2005), with transfer between games \n(Kansky et al., 2017), such that, for example, learning about one first-person shooter \nenhances performance on another with entirely different images, equipment and so \nforth. (A system that can learn many games, separately, without transfer between them, \nsuch as DeepMind’s Atari game system, would not qualify; the point is to acquire \ncumulative, transferrable knowledge). \n• A physically embodied test an AI-driven robot that could build things (Ortiz Jr, 2016), \nranging from tents to IKEA shelves, based on instructions and real-world physical \ninteractions with the objects parts, rather than vast amounts trial-and-error. \nNo one challenge is likely to be sufficient. Natural intelligence is multi-dimensional \n(Gardner, 2011), and given the complexity of the world, generalized artificial intelligence \nwill necessarily be multi-dimensional as well.  \nBy pushing beyond perceptual classification and into a broader integration of inference \nand knowledge, artificial intelligence will advance, greatly. \n \n \nPage !\n of !\n22\n27\n6.  Conclusions \nAs a measure of progress, it is worth considering a somewhat pessimistic piece I wrote \nfor The New Yorker five years ago , conjecturing that “deep learning is only part of the \n15\nlarger challenge of building intelligent machines” because “such techniques lack ways of \nrepresenting causal relationships (such as between diseases and their symptoms), and are \nlikely to face challenges in acquiring abstract ideas like “sibling” or “identical to.” They \nhave no obvious ways of performing logical inferences, and they are also still a long way \nfrom integrating abstract knowledge, such as information about what objects are, what \nthey are for, and how they are typically used.” \nAs we have seen, many of these concerns remain valid, despite major advances in \nspecific domains like speech recognition, machine translation, and board games, and \ndespite equally impressive advances in infrastructure and the amount of data and compute \navailable. \nIntriguingly, in the last year, a growing array of other scholars, coming from an \nimpressive range of perspectives, have begun to emphasize similar limits. A partial list \nincludes Brenden Lake and Marco Baroni (2017), François Chollet (2017), Robin Jia and \nPercy Liang (2017), Dileep George and others at Vicarious (Kansky et al., 2017) and \nPieter Abbeel and colleagues at Berkeley (Stoica et al., 2017).  \nPerhaps most notably of all, Geoff Hinton has been courageous enough to reconsider has \nown beliefs, revealing in an August interview with the news site Axios  that he is \n16\n“deeply suspicious” of back-propagation, a key enabler of deep learning that he helped \npioneer, because of his concern about its dependence on labeled data sets.  \nInstead, he suggested (in Axios’ paraphrase) that “entirely new methods will probably \nhave to be invented.”  \nI share Hinton’s excitement in seeing what comes next. \n https://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence\n15\n https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html\n16\n \n \nPage !\n of !\n23\n27\nReferences \nAthalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2017). Synthesizing Robust Adversarial \nExamples. arXiv, cs.CV. \nBesold, T. R., Garcez, A. D., Bader, S., Bowman, H., Domingos, P., Hitzler, P. et al. (2017). \nNeural-Symbolic Learning and Reasoning: A Survey and Interpretation. arXiv, cs.AI. \nBošnjak, M., Rocktäschel, T., Naradowsky, J., & Riedel, S. (2016). Programming with a \nDifferentiable Forth Interpreter. arXiv. \nBottou, L. (2015). Two big challenges in machine learning. Proceedings from 32nd International \nConference on Machine Learning. \nBowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus for \nlearning natural language inference. arXiv, cs.CL. \nChollet, F. (2017). Deep Learning with Python. Manning Publications. \nCireşan, D., Meier, U., Masci, J., & Schmidhuber, J. (2012). Multi-column deep neural network \nfor traffic sign classification. Neural networks. \nDavis, E., & Marcus, G. (2015). Commonsense reasoning and commonsense knowledge in \nartificial intelligence. Communications of the ACM, 58(9)(9), 92-103. \nDavis, E. (2016). How to Write Science Questions that Are Easy for People and Hard for \nComputers. AI magazine, 37(1)(1), 13-22. \nDavis, E., Marcus, G., & Frazier-Logue, N. (2017). Commonsense reasoning about containers \nusing radically incomplete information. Artificial Intelligence, 248, 46-84. \nDeng, J., Dong, W., Socher, R., Li, L. J., Li - Computer Vision and, K., & 2009 Imagenet: A \nlarge-scale hierarchical image database. Proceedings from Computer Vision and Pattern \nRecognition, 2009. CVPR 2009. IEEE Conference on. \nElman, J. L. (1990). Finding structure in time. Cognitive science, 14(2)(2), 179-211. \nEvtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A. et al. (2017). Robust \nPhysical-World Attacks on Deep Learning Models. arXiv, cs.CR. \nFodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: a critical \nanalysis. Cognition, 28(1-2)(1-2), 3-71. \nGardner, H. (2011). Frames of mind: The theory of multiple intelligences. Basic books. \nGelman, S. A., Leslie, S. J., Was, A. M., & Koch, C. M. (2015). Children’s interpretations of \ngeneral quantifiers, specific quantifiers, and generics. Lang Cogn Neurosci, 30(4)(4), \n448-461. \nGenesereth, M., Love, N., & Pell, B. (2005). General game playing: Overview of the AAAI \ncompetition. AI magazine, 26(2)(2), 62. \nGeorge, D., Lehrach, W., Kansky, K., Lázaro-Gredilla, M., Laan, C., Marthi, B. et al. (2017). A \ngenerative vision model that trains with high data efficiency and breaks text-based \nCAPTCHAs. Science, 358(6368)(6368). \nGervain, J., Berent, I., & Werker, J. F. (2012). Binding at birth: the newborn brain detects identity \nrelations and sequential position in speech. J Cogn Neurosci, 24(3)(3), 564-574. \nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press. \n \n \nPage !\n of !\n24\n27\nGraves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A. et al. \n(2016). Hybrid computing using a neural network with dynamic external memory. Nature, \n538(7626)(7626), 471-476. \nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep \nReinforcement Learning that Matters. arXiv, cs.LG. \nHuang, S., Papernot, N., Goodfellow, I., Duan, Y., & Abbeel, P. (2017). Adversarial Attacks on \nNeural Network Policies. arXiv, cs.LG. \nJia, R., & Liang, P. (2017). Adversarial Examples for Evaluating Reading Comprehension \nSystems. arXiv. \nKahneman, D. (2013). Thinking, fast and slow (1st pbk. ed. ed.). New York: Farrar, Straus and \nGiroux. \nKansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla, M., Lou, X. et al. (2017). \nSchema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive \nPhysics. arXIv, cs.AI. \nKočiský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G. et al. (2017). The \nNarrativeQA Reading Comprehension Challenge. arXiv, cs.CL. \nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep \nconvolutional neural networks. In (pp. 1097-1105). \nLake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning \nthrough probabilistic program induction. Science, 350(6266)(6266), 1332-1338. \nLake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2016). Building Machines \nThat Learn and Think Like People. Behav Brain Sci, 1-101. \nLake, B. M., & Baroni, M. (2017). Still not systematic after all these years: On the compositional \nskills of sequence-to-sequence recurrent networks. arXiv. \nLazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). Big data. The parable of Google Flu: \ntraps in big data analysis. Science, 343(6176)(6176), 1203-1205. \nLe, Q. V., Ranzato, M.-A., Monga, R., Devin, M., Chen, K., Corrado, G. et al. (2012). Building \nhigh-level features using large scale unsupervised learning. Proceedings from International \nConference on Machine Learning. \nLeCun, Y. (1989). Generalization and network design strategies. Technical Report CRG-TR-89-4. \nLerer, A., Gross, S., & Fergus, R. (2016). Learning Physical Intuition of Block Towers by \nExample. arXiv, cs.AI. \nLighthill, J. (1973). Artificial Intelligence: A General Survey. Artificial Intelligence: a paper \nsymposium. \nLipton, Z. C. (2016). The Mythos of Model Interpretability. arXiv, cs.LG. \nLopez-Paz, D., Nishihara, R., Chintala, S., Schölkopf, B., & Bottou, L. (2017). Discovering \ncausal signals in images. Proceedings from Proceedings of Computer Vision and Pattern \nRecognition (CVPR). \nLuc, P., Neverova, N., Couprie, C., Verbeek, J., & LeCun, Y. (2017). Predicting Deeper into the \nFuture of Semantic Segmentation. International Conference on Computer Vision (ICCV \n2017). \n \n \nPage !\n of !\n25\n27\nMarcus, G., Rossi, F., Veloso - AI Magazine, M., & 2016. (2016). Beyond the Turing Test. AI \nMagazine, Whole issue. \nMarcus, G., Marblestone, A., & Dean, T. (2014). The atoms of neural computation. Science, \n346(6209)(6209), 551-552. \nMarcus, G. (in prep). Innateness, AlphaZero, and Artificial Intelligence. \nMarcus, G. (2014). What Comes After the Turing Test? The New Yorker.  \nMarcus, G. (2012). Is “Deep Learning” a Revolution in Artificial Intelligence? The New Yorker.  \nMarcus, G. F. (2008). Kluge : the haphazard construction of the human mind. Boston: Houghton \nMifflin. \nMarcus, G. F. G. F. (2001). The Algebraic Mind: Integrating Connectionism and cognitive \nscience. Cambridge, Mass.: MIT Press. \nMarcus, G. F. (1998a). Rethinking eliminative connectionism. Cogn Psychol, 37(3)(3), 243-282. \nMarcus, G. F. (1998b). Can connectionism save constructivism? Cognition, 66(2)(2), 153-182. \nMarcus, G. F., Pinker, S., Ullman, M., Hollander, M., Rosen, T. J., & Xu, F. (1992). \nOverregularization in language acquisition. Monogr Soc Res Child Dev, 57(4)(4), 1-182. \nMarcus, G. F., Vijayan, S., Bandi Rao, S., & Vishton, P. M. (1999). Rule learning by seven-\nmonth-old infants. Science, 283(5398)(5398), 77-80. \nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word \nRepresentations in Vector Space. arXiv. \nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G. et al. (2015). \nHuman-level control through deep reinforcement learning. Nature, 518(7540)(7540), \n529-533. \nNeelakantan, A., Le, Q. V., Abadi, M., McCallum, A., & Amodei, D. (2016). Learning a Natural \nLanguage Interface with Neural Programmer. arXiv. \nNg, A. (2016). What Artificial Intelligence Can and Can’t Do Right Now. Harvard Business \nReview. \nNguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A., & Yosinski, J. (2016). Plug & Play \nGenerative Networks: Conditional Iterative Generation of Images in Latent Space. arXiv, \ncs.CV. \nNguyen, A., Yosinski, J., & Clune, J. (2014). Deep Neural Networks are Easily Fooled: High \nConfidence Predictions for Unrecognizable Images. arXiv, cs.CV. \nNorvig, P. (2016). State-of-the-Art AI: Building Tomorrow’s Intelligent Systems. Proceedings \nfrom EmTech Digital, San Francisco. \nO’Neil, C. (2016). Weapons of math destruction : how big data increases inequality and threatens \ndemocracy. \nOrtiz Jr, C. L. (2016). Why we need a physically embodied Turing test and what it might look \nlike. AI magazine, 37(1)(1), 55-63. \nParitosh, P., & Marcus, G. (2016). Toward a comprehension challenge, using crowdsourcing as a \ntool. AI Magazine, 37(1)(1), 23-31. \nPearl, J. (2000). Causality : models, reasoning, and inference /. Cambridge, U.K.; New York : \nCambridge University Press. \n \n \nPage !\n of !\n26\n27\nPinker, S., & Prince, A. (1988). On language and connectionism: analysis of a parallel distributed \nprocessing model of language acquisition. Cognition, 28(1-2)(1-2), 73-193. \nRibeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the \nPredictions of Any Classifier. arXiv, cs.LG. \nSabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic Routing Between Capsules. arXiv, \ncs.CV. \nSamek, W., Wiegand, T., & Müller, K.-R. (2017). Explainable Artificial Intelligence: \nUnderstanding, Visualizing and Interpreting Deep Learning Models. arXiv, cs.AI. \nSchank, R. C., & Abelson, R. P. (1977). Scripts, Plans, Goals and Understanding: an Inquiry into \nHuman Knowledge Structures. Hillsdale, NJ: L. Erlbaum. \nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks. \nSchoenick, C., Clark, P., Tafjord, O., P, T., & Etzioni, O. (2017). Moving beyond the Turing Test \nwith the Allen AI Science Challenge. Communications of the ACM, 60 (9)(9), 60-64. \nSculley, D., Phillips, T., Ebner, D., Chaudhary, V., & Young, M. (2014). Machine learning: The \nhigh-interest credit card of technical debt. Proceedings from SE4ML: Software \nEngineering for Machine Learning (NIPS 2014 Workshop). \nSocher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality through \nrecursive matrix-vector spaces. Proceedings from Proceedings of the 2012 joint conference \non empirical methods in natural language processing and computational natural language \nlearning. \nSpelke, E. S., & Kinzler, K. D. (2007). Core knowledge. Dev Sci, 10(1)(1), 89-96. \nStoica, I., Song, D., Popa, R. A., Patterson, D., Mahoney, M. W., Katz, R. et al. (2017). A \nBerkeley View of Systems Challenges for AI. arXiv, cs.AI. \nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. et al. (2013). \nIntriguing properties of neural networks. arXiv, cs.CV. \nVinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2014). Show and Tell: A Neural Image Caption \nGenerator. arXiv, cs.CV. \nWatters, N., Tacchetti, A., Weber, T., Pascanu, R., Battaglia, P., & Zoran, D. (2017). Visual \nInteraction Networks. arXiv. \nWilliams, A., Nangia, N., & Bowman, S. R. (2017). A Broad-Coverage Challenge Corpus for \nSentence Understanding through Inference. arXiv, cs.CL. \nWu, J., Lu, E., Kohli, P., Freeman, B., & Tenenbaum, J. (2017). Learning to See Physics via \nVisual De-animation. Proceedings from Advances in Neural Information Processing \nSystems. \nZoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2017). Learning Transferable Architectures for \nScalable Image Recognition. arXiv, cs.CV. \n \n \nPage !\n of !\n27\n27\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML",
    "97R40",
    "I.2.0; I.2.6"
  ],
  "published": "2018-01-02",
  "updated": "2018-01-02"
}