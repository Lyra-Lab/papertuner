{
  "id": "http://arxiv.org/abs/1801.00904v4",
  "title": "ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks",
  "authors": [
    "Tae-Hoon Kim",
    "Jonghyun Choi"
  ],
  "abstract": "We propose to learn a curriculum or a syllabus for supervised learning and\ndeep reinforcement learning with deep neural networks by an attachable deep\nneural network, called ScreenerNet. Specifically, we learn a weight for each\nsample by jointly training the ScreenerNet and the main network in an\nend-to-end self-paced fashion. The ScreenerNet neither has sampling bias nor\nrequires to remember the past learning history. We show the networks augmented\nwith the ScreenerNet achieve early convergence with better accuracy than the\nstate-of-the-art curricular learning methods in extensive experiments using\nthree popular vision datasets such as MNIST, CIFAR10 and Pascal VOC2012, and a\nCart-pole task using Deep Q-learning. Moreover, the ScreenerNet can extend\nother curriculum learning methods such as Prioritized Experience Replay (PER)\nfor further accuracy improvement.",
  "text": "ScreenerNet: Learning Self-Paced Curriculum for\nDeep Neural Networks\nTae-Hoon Kim\nIntel Corporation\nSeoul, Korea\npete.kim@intel.com\nJonghyun Choi\nAllen Institute for Artiﬁcial Intelligence\nSeattle, WA 98103\njonghyunc@allenai.org\nAbstract\nWe propose to learn a curriculum or a syllabus for deep reinforcement learning\nand supervised learning with deep neural networks by an attachable deep neural\nnetwork, called ScreenerNet. Speciﬁcally, we learn a weight for each sample\nby jointly training the ScreenerNet and the main network in an end-to-end self-\npaced fashion. The ScreenerNet has neither sampling bias nor memory for the\npast learning history. We show the networks augmented with the ScreenerNet\nconverge faster with better accuracy than the state-of-the-art curricular learning\nmethods in extensive experiments of a Cart-pole task using Deep Q-learning\nand supervised visual recognition task using three vision datasets such as Pascal\nVOC2012, CIFAR10, and MNIST. Moreover, the ScreenerNet can be combined\nwith other curriculum learning methods such as Prioritized Experience Replay\n(PER) for further accuracy improvement.\n1\nIntroduction\nTraining a machine learning model with chosen training samples in a certain order improves the speed\nof learning and is called Curriculum Learning [3]. The curriculum learning recently gains much\nattention due to the difﬁculty of training deep models for reinforcement learning [1, 8, 24]. However,\nselecting and ordering samples is a hard-decision process and signiﬁcantly lowers the chance of\nsamples to be selected in the later iterations and changes solution to which the model converges if the\nsamples are rejected earlier (sampling bias). In addition, the decision criteria are mostly deﬁned by a\nset of hand-crafted rules such as classiﬁcation error or conﬁdence of the main network in majority of\nprevious work [2, 4, 8]. Those hand-crafted rules require additional rules to handle the sampling bias.\nScreenerNet\nTraining \nSamples\n0.1\n0.9\n0.8\nMain Network\nLoss\nSoft Curriculum\nby weights\nRules\nReplay\nDiscarded\nSamples\nJoint Training\n✓\nFigure 1: ScreenerNet learns the valuation of train-\ning samples for a main network. It regresses the\nsample into its signiﬁcance for training; signiﬁcant\nif it is hard for the current performance of the main\nnetwork, and non-signiﬁcant if it is easy.\nTo alleviate the sampling bias, we present a\nscheme to determine a soft decision by a weight\nvalue of every training sample for building a cur-\nriculum. Moreover the soft decision is learned\nin a self-paced manner [16, 11, 10, 25, 21] with-\nout requiring to memorize the historic loss val-\nues [23]. This is a generalization of the hard\ndecision by considering all samples at every cur-\nriculum update, thus never ignores any samples\nat any iteration to maximize the efﬁcacy of the\ncurricular learning, motivated from the recent\nwork of efﬁciently exploring spaces of state, ac-\ntion, and goal through the learning [1, 24, 11]\nand is similar to [9, 23, 11]. Moreover, to dis-\ncover the rules for curriculum at each training iteration that are beyond our intuition, we learn the\ncurriculum by an attachable neural network along with the main network being trained. We call the\nPreprint. Work in progress.\narXiv:1801.00904v4  [cs.CV]  6 Jun 2018\nattachable network as ScreenerNet. The ScreenerNet is jointly trained with the main network in an\nend-to-end fashion as illustrated in Figure 1. The ScreenerNet provides the locally most accurate\nweights per the main network being trained in a self-paced manner.\nUnlike the many recent approaches that are mostly applied to deep reinforcement learning prob-\nlems [23, 1, 7], we empirically show that our ScreenerNet can also be applied to various types of\nnetworks including convolutional neural network for visual recognition as well as deep reinforcement\nlearning. The ScreenerNet not only expedites the convergence of the main network but also improves\nthe accuracy at the end of the training. In particular, in a real world application e.g., a multi-camera\nsurveillance system, pre-training all target objects is non trivial due to varying environments. The\napplication strongly requires incremental updates of neural network models. The networks in such\nembedded systems are designed to be small for computational beneﬁt and energy efﬁciency in the\ninference time. ScreenerNet should help in such use cases. Finally, the ScreenerNet can extend the\nstochastic sampling based curriculum learning approaches for further improvement of ﬁnal accuracy.\n2\nRelated Work\nAs a pioneering work of curriculum learning, Hinton [9] introduced an error-based non-uniform\nsampling scheme by importance sampling to signiﬁcantly speed-up training of a network on a digit\nclassiﬁcation using MNIST dataset. The idea of error-based correction has been widely used as one\nof the most popular cues in the curriculum learning literature. Since Bengio et al. [4] coin the term\ncurriculum learning for the method, number of approaches [8, 18, 23, 19, 29] have been proposed to\nsample, weigh, or sort the training examples to improve accuracy and/or expedite the training. These\nmethods require to design a hand-crafted rule and/or to run the training procedure twice to obtain\naccurate estimate of sample valuation. To avoid second training, Jiang et al. propose to estimate the\nsample valuation at the same time of training a learning model [11] by adding a term in the objective\nfunction. This is an extension of Kumar et al.’s work on Self-Paced learning [16] in the curriculum\nlearning literature. Our method also regresses the sample signiﬁcance at the time of training.\nRecently, Schaul et al. [23] proposed a sampling scheme to increase the replay probability of training\nsamples that have a high expected-learning-progress determined by an optimization loss. They\nalso proposed a weighted importance sampling method to address the bias of sampling, which is\nsimilar to our weighting scheme. In stochastic gradient-based optimization perspective, Loshchilov\nand Hutter [20] also proposed to sample mini-batches non-uniformly, called AdaDelta [28] and\nAdam [13].\nMost recently, Graves et al. [8] proposed an automatic curriculum learning method for LSTM for the\nNLP application. They deﬁne a stochastic syllabus by a non-stationary multi-armed bandit algorithm\nof getting a reward signal from each training sample. They deﬁned mappings from the rates of\nincrease in prediction accuracy and network complexity to the reward signal.\nKoh and Liang [14] adopted inﬂuence functions from robust statistics to measure the effect of\nparameter changes or perturbations of the training data such as pixel values. It was applied to\ndebugging models, detecting dataset error, and training-set attack. Although the direct application to\ncurriculum learning is not presented, the authors show a way of predicting the signiﬁcance of training\nsamples, which can be useful for learning curriculum.\nSimilar to our idea of attaching another network to the main one, self-play between the policy and a\ntask-setting was proposed by Sukhbaatar et al. [24]. The task-setter tries to ﬁnd the simplest tasks\nthat the policy cannot complete, which makes learning for the policy easy because the selected task is\nlikely to be only just beyond the capability of the policy. Similarly, Andrychowicz et al. [1] recently\nproposed to learn a curriculum but with a hindsight replay. They use unshaped reward signals even\nwhen they are sparse and binary, without requiring domain knowledge. These recent approaches\naddress the sparsity and complexity of solution spaces in deep reinforcement learning, which is not a\nmain issue for the supervised learning.\nVery recently, Zhou et al. [29] proposed an adaptive feeding method that classiﬁes an input sample as\neasy or hard one in order to forward the input to an appropriate one of a fast but less accurate neural\nnetwork and an accurate but slow network for object detection. However, they only showed the speed\nup the inference preserving the accuracy of the main classiﬁer.\nIn addition, Jiang et al. [12] propose an attachable network to regularize the network not to overﬁt to\nthe data with corrupted labels, called ‘MentorNet’. The MentorNet uses feature embedding of the\n2\nsample by the main network, referred as StudentNet, and compute a weight for each sample. The\nMentorNet is very different from ScreenerNet as it is pretrained on other datasets than the dataset of\ninterest for better regularization of corruptedly labeled samples whereas ScreenerNet aims for faster\nand better convergence of the training on the dataset of interest.\nUnlike the previous approaches, our ScreenerNet has three beneﬁts: First, it includes all samples to\nupdate weight even though the samples have weight values close to zero. This beneﬁt is particularly\nsigniﬁcant in stochastic sampling approaches, if an important sample is assigned a low sampling\npriority at the early stage of training, it may not be likely to be picked again and may not be\nsigniﬁcantly used for the training until other samples have low sampling priority as well. Second,\nit can learn a direct mapping of a training sample to its signiﬁcance, even if the training sample\nis unseen, unlike the other memory-based methods. Finally, it can extend the stochastic sampling\ncurriculum learning approaches to improve further by taking the beneﬁts of both methods.\n3\nApproach\nWe formulate the problem of building a curriculum by learning the importance of each sample for\ntraining the main network by a scalar weight in a joint learning framework. Speciﬁcally, we deﬁne\nthe importance of a training sample x at each iteration of training as a random variable wx. Since\nwe want to train the main network better in speed and accuracy, the objective of the curriculum\nlearner maximizes the likelihood by sample weight wx given the error E between the prediction of\nthe main network for x and its target. The maximum likelihood estimator of wx ( c\nwx) can be written\nas follows:\nc\nwx = arg max\nwx P(E|wx, Wc) = arg max\nwx\nP(wx|E, Wc)P(E|Wc)\nP(wx|Wc)\n,\n(1)\nwhere Wc is a parameter of the main network. Since E is a function of Wc, Eq. 1 can be reduced to\nc\nwx = arg max\nwx\nP(wx|E)P(E|Wc)\nP(wx|Wc)\n.\n(2)\nThe ScreenerNet is a neural network to optimize the objective.\n3.1\nScreenerNet\n0\nex\n0\n0.5\n0.2\n1\n0.4\nwx\n0.6\n0.5\n0.8\n1\n1\n0\nFigure 2: Loss function LS(ex, wx) with M = 1\nwithout the L1 regularizer. We bound wx to be in\n(0, 1) for our experiments.\nValuating the exact signiﬁcance of each training\nsample to maximize the ﬁnal accuracy of the\nmain network is computationally intractable [8].\nThe inﬂuence functions in [14] could be a poten-\ntial solution to estimate the ﬁnal accuracy with\nless computational burden but still requires sig-\nniﬁcant computational cost at the initialization\nof every iteration of training the main network.\nInstead of estimating the ﬁnal accuracy, we pro-\npose to simplify the problem of sample-wise\nsigniﬁcance valuation to a local optimal policy\nthat predicts the weights of training samples at\nthe current iteration of the training.\nLet wx be a weight of training sample, x, pre-\ndicted by ScreenerNet, S. Let LF(F(x), tx) be\nan objective function for the main network F(·) to compute an error between F(x) and its target\nlabel tx. We deﬁne an objective function that ScreenerNet minimizes, LS(·), as follows:\nX\nx∈X\n\u0000(1 −wx)2ex + w2\nx max(M −ex, 0)\n\u0001\n+ α\nX\np∈WS\n∥p∥1,\n(3)\nwhere wx = S(x), ex = LF(F(x), tx), WS is parameters of the ScreenerNet S, and α is a\nbalancing hyper-parameter for the regularizer of the ScreenerNet. X is a set of training images and\nM is a margin hyper-parameter. We plot LS with M = 1 except the L1 regularizer in Figure 2.\n3\nAs shown in the Figure 2, the objective function is a non-negative saddle like function with minima\nat (wx, ex) = (0, 0) or (1, 1) and with maxima at (wx, ex) = (0, 1) or (1, 0). Thus, the LS promotes\nthe sample weight to be high when the error (loss) of the sample by the main network is high and vice\nversa. Therefore, the ScreenerNet encourages to increase weight wx when error of the main network\nex is high and promotes to decrease weight when the error is low.\nNote that we bound the weight value by the ScreenerNet to be in (0, 1) by a Sigmoid layer at the end\nof the ScreenerNet, since its multiplication to the gradient without the bound may cause overshooting\nor undershooting of the main network [9].\nS\nScreenerNet\nMain Network\nF\nLS\nwx\nx\nInput Image\nˆy\nLF(ˆy, wx)\nLF(ˆy, 1)\newx\neweighted\nx\nex\nBack-propagation\nBack-propagation\n1\n2\n3\n4\n5\n6\n7\nFigure 3: Optimization path of a ScreenerNet attached net-\nwork. The numbers in blue boxes show the order of gradient\nupdate.\nOptimization. Optimizing the loss of\nthe ScreenerNet augmented network\nis not trivial as the loss is non convex.\nThus, we employ the block-coordinate\ndescent to optimize the network and\ndepict the gradient path of the subset\nof variables in Figure 3. The numbers\nin the blue boxes in the ﬁgure denotes\nthe order of updating the gradient.\nBy the block coordinate descent, at\neach iteration of the epochs in the\ntraining phase, we predict the weight,\nwx of the training sample x to update\nthe main network F using the error,\nex from x and the weight wx. Then,\nwe update ScreenerNet using the error,\nex. The training procedure of ScreenerNet augmented deep network is summarized as Algorithm 1.\nAlgorithm 1: Training a network with ScreenerNet\nGiven: training samples (X, tX), main network F, ScreenerNet S\nInitialize F, S\nfor each iteration of the training do\nx ←Sample(X).\n/* sample a mini-batch from X */\nwx ←S(x).\n/* predict an weight of a sample */\nˆy ←F(x).\n/* prediction from the main network */\neweighted\nx\n←LF(ˆy, wx).\n/* compute a weighted error */\nex ←LF(ˆy, 1).\n/* compute an error of the sample */\nUpdateNetwork(F, ew\nx ).\n/* train main network */\nUpdateNetwork(S, ex).\n/* train ScreenerNet */\nend\nArchitecture of ScreenerNet. A larger ScreenerNet than the main network would have larger\ncapacity than the main network, then it may require more sample to reliably predict the signiﬁcance\nof the samples. A simple ScreenerNet would not predict the signiﬁcance well and likely predicts it\nlike a uniform distribution. We empirically found that ScreenerNet whose architecture is similar to\nthat of the main network or slightly simpler performs the best. Further, we also try weight sharing\nwith the main network (refer to the parameter sharing argument in Section 4.6).\n3.2\nExtending Stochastic Sampling Based Approaches\nSince the ScreenerNet is a weight regression network, it can extend the stochastic sampling approaches\nsuch as Prioritized Expereience Replay (PER) [23]. The extension has two beneﬁts; ﬁrst, it may\nfurther improve the convergence speed and the ﬁnal accuracy than the single deployment of either\nmethod. Second, it reduces the computation for the ScreenerNet. For instance, if we combine the\nScreenerNet with the PER. The PER determines the probability of a training sample to be selected\nby P(x) =\npα\nx\nP\n˜x∈X pα\n˜x , where px > 0 is the priority of each sample x, and α controls how much\nprioritization is used. When α = 0, it is equivalent to the uniform sampling. The priority is deﬁned\nas: px = |ex| + ϵ, where ϵ is a very small constant to prevent from assigning zero priority to x. To\n4\nextend the PER or any other sampling based methods by the ScreenerNet, we predict weights of x\nthat the PER or other sampling based methods select (refer to the Sec. 4.4).\n4\nExperiments\n4.1\nDatasets\nWe have evaluated our algorithm on two tasks: a Cart-pole task using the deep Q-learning [26, 22],\nwhich is one of the most popular tasks in deep reinforcement learning and a supervised visual\nrecognition using three popular vision datasets; Pascal VOC 2012 [6], CIFAR10 [15] and MNIST [17].\nFor the Cart-pole experiment, we use Cart-pole-v0 in OpenAI Gym [5], which gets 4-dimensional\ninput of the state of the cart and pole, and outputs 2 discrete actions to move left or right. Pascal VOC\n2012 has 5, 717 images in training and 5, 823 images in validation set. It is one of the most popular\nbenchmark datasets along with the ImageNet. CIFAR10 has 50, 000 images with size of 32×32 in\ntraining and 10, 000 images in testing set. MNIST dataset has 60, 000 images with size of 28×28 in\ntraining and 10, 000 images in testing set. Both MNIST and CIFAR10 are widely used for neural\nnetwork training analyses.\n4.2\nExperimental Set-up\nWe describe the details of the neural network architecture of the main network and the ScreenerNet\nused in our experiments on each dataset and their important hyper-parameters in the supplementary\nmaterial. We compare the ScreenerNet augmented network with baselines; the main network only\nand the state-of-the-art stochastic sampling based curriculum learning method, Prioritized Experience\nReplay (PER) [23], which is the only comparable to ours. For the PER, we used importance-sampling\nweights wi =\n\u0010\n1\nN ·\n1\nP (i)\n\u0011β\n, where β is linearly annealed from 0.4 to 1.0 until 40, 000 steps then\nis ﬁxed to 1.0. We set the discount parameter γ to be 0.99 and the replay memory to be a sliding\nwindow memory of size 50, 000. The PER processes mini-batches of 32 samples (visual recognition\ntask) or 32 transitions (deep Q-learning) sampled from the memory.\nWe compare the speed of convergence in training and the average reward per episode for the Cart-pole\ntask or the ﬁnal classiﬁcation accuracy for the supervised visual recognition tasks (Pascal VOC 2012,\nCIFAR10 and MNIST).\n4.3\nFaster and Better Convergence by ScreenerNet\nWe compare the test accuracy curves of the baseline network (main network only) and the ScreenerNet\naugmented network (detnoted as ‘ScreenerNet’) in Figure 4 for all four experiments.\n10000\n20000\n30000\n40000\n50000\nSteps\n160\n165\n170\n175\n180\n185\n190\nAverage Reward\nCartpole\nBaseline (Double DQN)\nDDQN + ScreenerNet\n(a) Cart-pole\n10\n20\n30\n40\n50\n60\nEpoch\n50.0%\n60.0%\n70.0%\n80.0%\n90.0%\nAccuracy\nVOC2012 classiﬁcation\nBaseline (train)\nBaseline (test)\nScreenerNet (train)\nScreenerNet (test)\n(b) Pascal VOC 2012\n10\n20\n30\n40\n50\nEpoch\n40.0%\n50.0%\n60.0%\n70.0%\n80.0%\nAccuracy\nCIFAR10 classiﬁcation\nBaseline (train)\nBaseline (test)\nScreenerNet (train)\nScreenerNet (test)\n(c) CIFAR10\n20\n40\n60\n80\n100\nEpoch\n97.5%\n98.0%\n98.5%\n99.0%\n99.5%\nAccuracy\nMNIST classiﬁcation\nBaseline\nScreenerNet\n(d) MNIST\nFigure 4: Comparison of the test accuracy of main network (Blue) and ScreenerNet augmented\nnetwork (Red)\nIn the Cart-pole task, the average reward obtained at every 5, 000 training step is shown in Figure 4-(a).\nThe ScreenerNet begins with higher average reward than the baseline Double DQN (DDQN) and\nclearly shows the higher overall gain comparing to the baseline during all epochs.\nFor the supervised visual recognition tasks on three vision datasets, ScreenerNet also improves\nthe convergence speed and the overall accuracy. In the experiments with Pascal VOC 2012 dataset\n(Figure 4-(b)), ScreenerNet helps the main network to improve the learning speed at early epochs even\nthough VGG-19 is pre-trained with a much larger dataset (ImageNet). ScreenerNet also improves the\naccuracy when the networks are close to convergence though the gain diminishes. We believe that\nthe ScreenerNet helps better training rather than overﬁtting as the the sample signiﬁcance that the\n5\nScreenerNet learns is quite different from the classiﬁcation objective that the main network learns\n(refer to an empirical evidence for this argument in the discussion in Sec. 4.6). With CIFAR10 dataset\nshown in Figure 4-(c), with the simple main network architecture, we observe that ScreenerNet yields\nimprovements in the learning speed and ﬁnal accuracy. With MINST dataset shown in Figure 4-(d),\nScreenerNet augmented networks converge slightly faster than the baseline and marginally improve\nthe accuracy. It is because there is not much room to improve as the baseline already performs nearly\nperfect. MNIST and CIFAR10 experiments are mainly for the analysis purposes (Sec. 4.5 and 4.6).\nIn addition, it is of interest that when the network capacity varies, how much the ScreenerNet helps.\nWe show effect of variously large ScreenerNet networks for the accuracy and the convergence speed\nin Section 2 in the supplementary material.\nCompare with Prioritized Experience Replay (PER). While the PER selects samples by a hard\ndecision, ScreenerNet selects samples by a soft decision in a form of weight. They could be both\ncomparable and able to be combined together, which we will discuss further in Sec. 4.4. To see the\nbeneﬁt of ScreenerNet over the PER [23], we compare ScreenerNet with PER for all four datasets in\nFigure 5. We observe that ScreenerNet exhibits better learning curves of the main network than the\nPER in all four evaluations.\n10000\n20000\n30000\n40000\n50000\nCartpole\n160\n165\n170\n175\n180\n185\n190\nAverage Reward\nCartpole\nBaseline (Double DQN)\nDDQN + PER\nDDQN + ScreenerNet\n(a) Cart-pole\n10\n20\n30\n40\n50\nEpoch\n70.5%\n72.5%\n74.5%\n76.5%\n78.5%\n80.5%\n82.5%\n84.5%\nmAP\nPascal VOC 2012 classiﬁcation\nBaseline\nScreenerNet\nPER\n(b) Pascal VOC 2012\n10\n20\n30\n40\n50\nEpoch\n40.5%\n50.5%\n60.5%\n70.5%\nAccuracy\nCIFAR10 classiﬁcation\nBaseline\nScreenerNet\nPER\n(c) CIFAR10\n20\n40\n60\n80\n100\nEpoch\n98.0%\n98.5%\n99.0%\n99.5%\n100.0%\nAccuracy\nMNIST classiﬁcation\nBaseline\nScreenerNet\nPER\n(d) MNIST\nFigure 5: Comparison with PER (Green), Baseline (the main network only, Blue) and ScreenerNet\naugmented Network (Red)\n4.4\nCombined with Stochastic Sampling Methods for Deep Q-Learning\nSince the stochastic sampling methods determine whether the samples to be included in the training\nor not by a deﬁned probability, we can extend them by the ScreenerNet to investigate the synergistic\neffect of both methods. Particularly, we can extend the ScreenerNet in two ways; ﬁrst, ScreenerNet\nprovides weights on the selected samples by the stochastic sampling methods. Second, the sampling\nselects the sample by the ScreenerNet’s weight. We evaluate both scenarios with the Prioritized\nExperience Replay (PER) as a choice of stochastic sampling methods for the deep Q-learning task\nusing the cart-pole task as shown in Figure 6.\n4.4.1\nScreenerNet Weighs Samples Selected by the Prioritized Experience Replay (PER)\n10000\n20000\n30000\n40000\n50000\nSteps\n160\n165\n170\n175\n180\n185\n190\nAverage Reward\nBaseline (Double DQN)\nDDQN + PER\nDDQN + ScreenerNet\nDDQN + PER + ScreenerNet\nDDQN + ScreenerNet Sampling\nFigure 6: Learning curves for the cart-pole prob-\nlem by average reward. Extension of combination\nof ScreenerNet and PER (Orange) and stochastic\nsampling using ScreenerNet (Purple)\nAs a ﬁrst scenario, we weigh the samples\nby the ScreenerNet on the samples selected\nby the PER (denoted as DDQN + PER +\nScreenerNet (orange solid line in Figure 6).\nThis extension outperforms the DDQN + PER\n(green dotted line) but performs worse than\nthe DDQN + ScreenerNet (red dotted); since\nScreenerNet weighs already selected sam-\nples, DDQN+PER+ScreenerNet performance\ncould be bounded by the performance of the\nDDQN+ScreenerNet.\nInterestingly, the ScreenerNet the PER and\nPER+ScreenerNet\nshow\nsimilar\nlearning\nprogress; it begins with high accuracy but falls\nthen gradually increases. Since they all have a\nsample weighting scheme, we believe that the\nmethods with weighting schemes quickly learn the easy solutions in the beginning of learning (e.g.,\n6\nmove the pole slightly from the initial position) then learn more difﬁcult solutions thus the rewards\ndrop in the early stage of learning then increase.\n4.4.2\nPER with Probability by ScreenerNet\nIn the PER, the probability that determines the priority of samples to be selected is deﬁned by the\nerror of the main network. For the second scenario of the ScreenerNet extension, we replace the\nprobablity px in Eq. 3.2 with the output of ScreenerNet (px = S(x) + ϵ) and setting α = 1 in Eq. 3.2.\nThe result is shown in purple solid line (DDQN+ScreenerNet Sampling in Figure 6). Comparing it to\nthe DDQN + PER (green dotted line), the DDQN + PER outperforms the ScreenerNet Sampling in\nthe beginning of training where ScreenerNet is not trained enough but eventually the ScreenerNet\nprobability outperforms the direct error value.\n4.5\nQualitative Analysis\nError Analysis with MNIST. To better understand the effect of the ScreenerNet, we investigate\nfailure cases of ScreenerNet augmented network (ScreenerNet) and the main network alone (baseline).\nWe use MNIST dataset for the qualitative analses as it is better explanable than other image data or\nthe Cart-pole task (Figure 7 and 8).\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPredicted label\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTrue label\n0\n0\n2\n0\n0\n0\n3\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n2\n0\n0\n0\n1\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n1\n0\n0\n4\n0\n1\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n4\n1\n0\n0\n5\n0\n0\n2\n1\n0\n0\n4\n2\n0\n0\n1\n4\n0\n0\n0\n0\n0\n3\n9\n0\n1\n0\n0\n0\n0\n3\n3\n0\n2\n1\n1\n2\n0\n2\n0\n2\n2\n2\n0\n4\n6\n2\n0\n5\n2\n0\nConfusion matrix of Baseline\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPredicted label\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTrue label\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n4\n0\n0\n0\n0\n0\n0\n0\n0\n1\n3\n0\n0\n0\n0\n0\n0\n0\n2\n0\n3\n2\n0\n0\n0\n1\n0\n0\n0\n2\n0\n1\n8\n1\n0\n0\n6\n0\n0\n2\n1\n0\n0\n2\n2\n0\n0\n1\n1\n0\n0\n5\n0\n0\n2\n6\n0\n0\n0\n0\n0\n0\n3\n2\n0\n2\n1\n0\n0\n0\n1\n0\n2\n1\n1\n0\n0\n2\n4\n0\n4\n2\n0\nConfusion matrix of ScreenerNet\n0\n1\n2\n3\n4\n5\n6\n7\n8\nFigure 7: Confusion matrix only with the failed\nexamples for visual clarity. Each number in\nthe cell shows the number of mis-classiﬁcation.\nNote that the diagonal of the matrix is zero be-\ncause successes are excluded for the visualiza-\ntion. (Left) by the network without ScreenerNet\n(Right) by ScreenerNet augmented network.\n995 779 887 660 771 993 006 880 553 998\n994 995 335 991 665 993 550 774 993 997\n221 665 335 994 994 990 884 006 772 449\n993 997 006 002 772 885 772 114 885 556\n(a) Baseline only fails\n622 866 944 599 355 311 599\n711 655 944 866 899 866 944\n733 055 944 644 355 244 944\n977 599 599 100 722\n(b) ScreenerNet only\nfails\nFigure 8: Comparison of exclusive failure ex-\namples. (a) baseline only fails but ScreenerNet\nsucceeds and (b) ScreenerNet only fails but base-\nline succeeds. The three numbers under each\nsample image denote classiﬁcation from Screen-\nerNet (in red), ground-truth (in green), and the\nbaseline (in blue), respectively from left to right.\n20\n40\n60\n80\n100\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nwx\nGround Truth\n5\n5\n5\n3\n(a) Weight progress of MNIST\n5\n0.649\n3\n0.544\n3\n0.542\n3\n0.537\n3\n0.522\n(b) Ends with highest weights\n3\n0.010\n3\n0.012\n2\n0.012\n4\n0.012\n6\n0.012\n(c) Ends with lowest weights\nFigure 9: Weight progress curves and their corre-\nsponding training images in MNIST. In (b) and (c)\nGreen number denotes the ground truth label and\nthe number below denotes the ﬁnal weight value.\nWe present confusion matrices of the baseline\nand ScreenerNet in Figure 7 only with failed ex-\namples for visual clarity. We also show qualtita-\ntive comparison that the failure cases that either\nonly the ScreenerNet or the baseline classiﬁes\nincorrectly in Figure 8.\nIt is observed that the widely spread confusions\nof the baseline are reduced overall by Screen-\nerNet in Figure 7. But it also has a few new\nfailures like mis-classiﬁcation of 4 into 9. Note\nthat the failure examples of 4 into 9 (Figure 8-\n(b)’s 944 examples) are extremely challenging.\nInstead, ScreenerNet increases the precision of\nrecognizing 4, being more strict with classify-\ning 1, 7, 8, and 9 into 4. Similarly ScreenerNet\nincreases recall for 8 at the expense of precision.\nEasy and Difﬁcult Training Samples Se-\nlected by ScreenerNet.\nAs the learning of\nScreenerNet augmented neural network pro-\nceeds, difﬁcult samples should receive high at-\ntention in the training procedure by high weight, while easy samples should receive low attention by\nlow weight. We plot the tracked weight of some easy and difﬁcult samples as the learning proceeds\n7\nin Figure 9-(a) and also present the samples with highest and lowest weights at the end of the training\nin Figure 9-(b) and (c).\nThe samples with high weights are visually difﬁcult to distinguish, while the ones with low weights\nare visually distinctive to the other class thus training with these in the later epoch would not add\nmuch value to improve the accuracy.\n4.6\nParameter Sharing\n10\n20\n30\n40\n50\nEpoch\n40.0%\n50.0%\n60.0%\n70.0%\nAccuracy\nCIFAR10 classiﬁcation for shared parameters\nBaseline (test)\nScreenerNet (test)\nParameter sharing 1 (train)\nParameter sharing 1 (test)\nParameter sharing 2 (train)\nParameter sharing 2 (test)\nFigure 10: Accuracies of different ScreenerNet con-\nﬁgurations (amount of parameter shared) on CIFAR10\ndataset with small architecture for the main network.\nWe tried the network parameter sharing be-\ntween common layers of the main network\nand the ScreenerNet. If the parameter shar-\ning improves the accuracy, it implies that\nScreenerNet simply adds more capacity to\nthe main network and helps overﬁt to the\ndata.\nWe conduct the experiments using CI-\nFAR10 dataset since it has a good balance\nbetween the complexity and size. There\nare two parameter sharing scenarios we ex-\nplored; ﬁrst, ScreenerNet does not change\nthe parameter of shared layers but ﬁne-\ntunes the last FC_1 layer (sharing 1). Sec-\nond, ScreenerNet also updates the param-\neter of the shared layers (sharing 2). As\nillustrated in Figure 10, both parameter sharing scenarios decrease the accuracy. It implies that as\nthe ScreenerNet does not learn the same objective that the main network learns, rather it learns the\nbehavior of the main network; even the low level signal to the ScreenerNet is different from that of\nthe main network. We argue that the ScreenerNet helps the main network to learn faster and better\nnot helps it overﬁt.\n5\nConclusion\nWe propose to estimate the signiﬁcance of the training samples for effective curriculum learning by\naugmenting a deep neural network, called ScreenerNet, to the main network and jointly train them.\nWe demonstrated that the ScreenerNet helps training deep neural networks both by fast and better\nconvergence in various tasks including visual classiﬁcation and deep reinforcement learning such\nas deep Q-learning. Moreover, the ScreenerNet not only outperforms the state-of-the-art sampling\nbased curriculum learning method such as the Prioritized Experience Replay (PER) (Figure 5) but\nalso can extend it for further improvement (Figure 6).\nNote that the learning objective of ScreenerNet is not the same as that of the main network. Instead,\nthe ScreenerNet estimates the probability that the main network will correctly classify the given\nsample or not. The architecture of the ScreenerNet is simpler than the target network and thus can be\ntrained ahead of it in terms of training maturity, which we empirically found to perform the best.\nSince the ScreenerNet is not a memory-based model, it can also be considered as an error estimator\nof the current state of the main network. Thus, the ScreenerNet can estimate the sample conﬁdence at\ninference time, which is particularly useful for the real world reinforcement learning system, similarly\nto the adaptive classiﬁer proposed by [29].\nIn contract to [12], the ScreenerNet possibly boosts weight values of mislabeled training samples\nwhich may perturb a decision boundary of the main network, since its objective function views\ntraining samples with large errors as signiﬁcant ones to train the main network.\nFuture work. We can extend the ScreenerNet to be progressively expanding its complexity from a\nsimple network to a complex one by adding new layers as the training progresses, similarly to [27].\nSince the newly added layers to the ScreenerNet might lead the learning to be unstable, we can use a\nmomentum in updating the weight as wx = λSold(x) + (1 −λ)Snew(x), where Sold and Snew are\nrespectively previous and current networks, and λ is a hyper-parameter in [0, 1].\n8\nReferences\n[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,\nBob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay\n. CoRR, abs/1707.01495, 2017.\n[2] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction\nwith recurrent neural networks. In NIPS, 2015.\n[3] S. Bengio, J. Weston, and D. Grangier. Label Embedding Trees for Large Multi-Class Tasks. In\nNIPS, 2010.\n[4] Y. Bengio, Jérôme Louradour, R. Collobert, and J. Weston. Curriculum Learning. In ICML,\n2009.\n[5] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. OpenAI Gym, 2016.\n[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.\nThe\nPASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html, 2012.\n[7] Y. Fan, F. Tian, T. Qin, J. Bian, and Tie-Yan Liu. Learning What Data to Learn. arXiv\npreprint:1702.08635, 2017.\n[8] Alex Graves, Marc G. Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu.\nAutomated Curriculum Learning for Neural Networks. In Proceedings of ICML, 2017.\n[9] G. E. Hinton. To Recognize Shapes, First Learn to Generate Images. Progress in brain research,\n165:535–547, 2007.\n[10] L. Jiang, D. Meng, Shoou-I Yu, Z. Lan, S. Shan, and A. G. Hauptmann. Self-Paced Learning\nwith Diversity. In NIPS, 2014.\n[11] L. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann. Self-Paced Curriculum Learning.\nIn AAAI, 2015.\n[12] L. Jiang, Z. Zhou, T. Leung, Li-Jia Li, and L. Fei-Fei. MentorNet: Regularizing Very Deep\nNeural Networks on Corrupted Labels. arXiv preprint:1712.05055, 2017.\n[13] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980,\n2014.\n[14] P. W. Koh and P. Liang. Understanding Black-box Predictions via Inﬂuence Functions. arXiv\npreprint:1703.04730, 2017.\n[15] Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto,\n2012.\n[16] M. Pawan Kumar, B. Packer, and D. Koller. Self-Paced Learning for Latent Variable Models.\nIn NIPS, 2010.\n[17] Y. Lecun and C. Cortes. The MNIST Database of Handwritten Digits. NYU, 1992.\n[18] Y. J. Lee and K. Grauman. Learning the Easy Things First: Self-Paced Visual Category\nDiscovery. In CVPR, 2011.\n[19] Tsung-Yi Lin, P. Goyal, R. Girshick, K. He, and P. Dollar. Focal Loss for Dense Object\nDetection. In CVPR, 2017.\n[20] I. Loshchilov and F. Hutter. Online Batch Selection for Faster Training of Neural Networks. In\nProceedings of ICLR Workshop, 2016.\n[21] F. Ma, D. Meng, Q. Xie, Z. Li, and X. Dong. Self-Paced Co-training. In ICML, 2017.\n[22] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.\nPlaying Atari with Deep Reinforcement Learning. In NIPS Deep Learning Workshop, 2013.\n[23] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay.\nIn Proceedings of ICLR, 2016.\n[24] Sainbayar Sukhbaatar, Ilya Kostrikov, Arthur Szlam, and Rob Fergus. Intrinsic Motivation and\nAutomatic Curricula via Asymmetric Self-Play. CoRR, abs/1703.05407, 2017.\n[25] J. S. Supancic and D. Ramanan. Self-paced learning for long-term tracking. In CVPR, 2013.\n9\n[26] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double\nQlearning. In AAAI, 2016.\n[27] Yu-Xiong Wang, D. Ramanan, and M. Hebert. Growing a Brain: Fine-Tuning by Increasing\nModel Capacity. In CVPR, 2017.\n[28] Matthew D Zeiler. Adadelta: An adaptive learning rate method. arXiv preprint:1212.5701,\n2012.\n[29] Hong-Yu Zhou, Bin-Bin Gao, and Jianxin Wu. Adaptive feeding: Achieving fast and accurate\ndetections by adaptively combining object detectors. In Proceedings of ICCV, 2017.\n10\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-01-03",
  "updated": "2018-06-06"
}