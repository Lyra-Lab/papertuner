{
  "id": "http://arxiv.org/abs/1801.01968v4",
  "title": "Faster Deep Q-learning using Neural Episodic Control",
  "authors": [
    "Daichi Nishio",
    "Satoshi Yamane"
  ],
  "abstract": "The research on deep reinforcement learning which estimates Q-value by deep\nlearning has been attracted the interest of researchers recently. In deep\nreinforcement learning, it is important to efficiently learn the experiences\nthat an agent has collected by exploring environment. We propose NEC2DQN that\nimproves learning speed of a poor sample efficiency algorithm such as DQN by\nusing good one such as NEC at the beginning of learning. We show it is able to\nlearn faster than Double DQN or N-step DQN in the experiments of Pong.",
  "text": "Faster Deep Q-learning\nusing Neural Episodic Control\nDaichi Nishio\nInstitute of Science and Engineering\nKanazawa University\nKanazawa, Japan\nEmail: dnishio@csl.ec.t.kanazawa-u.ac.jp\nSatoshi Yamane\nInstitute of Science and Engineering\nKanazawa University\nKanazawa, Japan\nEmail: syamane@is.t.kanazawa-u.ac.jp\nAbstract‚ÄîThe research on deep reinforcement learning which\nestimates Q-value by deep learning has been attracted the interest\nof researchers recently. In deep reinforcement learning, it is\nimportant to efÔ¨Åciently learn the experiences that an agent has\ncollected by exploring environment. We propose NEC2DQN that\nimproves learning speed of a poor sample efÔ¨Åciency algorithm\nsuch as DQN by using good one such as NEC at the beginning\nof learning. We show it is able to learn faster than Double DQN\nor N-step DQN in the experiments of Pong.\nIndex Terms‚ÄîDeep reinforcement learning; DQN; Neural\nEpisodic Control; Sample efÔ¨Åciency\nI. INTRODUCTION\nDeep Q-Network (DQN) [1] have made a success of deep\nreinforcement learning end-to-end, and various algorithms\nhave been proposed since then [2]. However learning a task\nwith large state space is difÔ¨Åcult, and many learning steps\nare necessary especially in an environment where rewards are\nsparse.\nTABLE I\nTHE DIFFERENCES BETWEEN DQN AND NEC\nDQN\nNEC\nEstimation\nNeural Network\nNeural Network + DND\nSampling\nRandom or with Priority\nRandom\nMemory cost\nLarge\nHuge\nIn order to solve them, it is necessary to efÔ¨Åciently use expe-\nriences obtained by exploration. DQN uses Experience Replay\n[3] which stores experiences in memory called Replay Buffer\nand It trains with minibatch randomly. Prioritized Experience\nReplay [4] has been proposed to learn more efÔ¨Åciently than\nrandom sampling. It considers experiences with large train\nerror as important experiences, and greatly improves learning\nspeed and performance.\nNeural Episodic Control (NEC) [5] is another way to\nefÔ¨Åciently learn. It uses the memory module called Differen-\ntiable Neural Dictionary (DND) to learn stably with a smaller\nnumber of learning steps. The agent can decide its action\nby taking advantage of past similar experiences stored in\nDND. In addition, it is able to learn end-to-end because it\nis differentiable inside a neural network.\nHowever, DND needs a large memory for each action. It\nalso requires a lot of calculation time and memory usage. We\nshow their relations in Table I.\nIn this research, we propose a method of improving learning\nefÔ¨Åciency and speed by adapting NEC‚Äôs learning efÔ¨Åciency to\na simple network like DQN.\nII. DEEP REINFORCEMENT LEARNING\nWe target reinforcement learning assuming general Markov\nDecision Process(MDP). We deÔ¨Åne the state of the environ-\nment at time t as st, and the agent selects the action at by\nthe policy œÄ. Then it obtains the reward rt and the next state\nst+1 corresponding to at from the environment. The revenue\nis discounted return Gt = P\nt(Œ≥trt), where Œ≥ is discount\nrate as the degree of consideration of the future. The action-\nvalue function the agent uses for selecting action is deÔ¨Åned\nas QœÄ(s, a) = EœÄ[Gt | s, a]. The optimal Q-value is based on\nthe Bellman optimal equation [6] as follows.\nQ‚àó(s, a) = E[r + Œ≥ max\na‚Ä≤ Q(s‚Ä≤, a‚Ä≤) | s, a]\n(1)\nQ-learning [7] is used to obtain the optimal Q-value.\nQ(s, a) ‚ÜêQ(s, a) + Œ±(r + Œ≥ max\na‚Ä≤ Q(s‚Ä≤, a‚Ä≤) ‚àíQ(s, a)) (2)\nIn Q-learning, assuming that samples (s, a, r, s‚Ä≤) can be\nobtained inÔ¨Ånitely from all pairs of (s, a), (2) obtains the\noptimal Q-value function Q‚àó(s, a). Also, it converges to the\nsame value since the Q-value function does not depend on the\npolicy. On the other hand, the convergence may take a long\ntime if there are pairs (s, a) that are not tried.\nIn DQN, the agent uses the Œµ-greedy policy for tradeoff\nbetween exploration and exploitation. This Œµ(0 < Œµ < 1) is a\nconstant, or there is a method of linear decaying as increasing\nlearning steps.\nœÄ(a | s) =\n\u001a 1 ‚àíŒµ (a = argmaxa Q(s, a))\nŒµ\n(otherwise)\n(3)\nDQN aiming at learning from images uses Convolutional\nNeural Network (CNN) [8] as a state feature extractor. The\nQ-value for embedding h obtained from CNN is estimated by\nfully-connected layers.\nThe agent stores tuples of (st, at, rt, st+1) in Replay Buffer\nso that it can learn with a minibatch formed randomly from\narXiv:1801.01968v4  [cs.LG]  3 Jun 2018\nthe buffer. In addition, for stability of the target value, it uses a\ntarget network for calculating a target value separately from a\nneural network for learning. The target network uses parameter\nŒ∏‚àíwhich is slightly older than the current parameter Œ∏ of the\nlearning network.\nA neural network learns from the loss function L(Œ∏) =\nE[yt ‚àíQ(s, a; Œ∏)] using the target value in (4).\nyt = rt + Œ≥ max\na‚Ä≤ Q(st+1, a‚Ä≤; Œ∏‚àí)\n(4)\nHasselt et al. [9] show that DQN overestimates the action-\nvalue when the number of experience samples obtained from\na environment is small, and Double Q-learning is a solution to\nit. Double Q-learning updates the Q-value using two Q-value\nestimators A and B.\nQA(s, a) ‚ÜêQA(s, a) + Œ±(r + Œ≥QB(s‚Ä≤, a‚àó) ‚àíQA(s, a)) (5)\nQB(s, a) ‚ÜêQB(s, a) + Œ±(r + Œ≥QA(s‚Ä≤, b‚àó) ‚àíQB(s, a)) (6)\nwhere a‚àó= argmax\na\nQA(s‚Ä≤, a), b‚àó= argmax\na\nQB(s‚Ä≤, a)\nQA prevents QB from overestimated, and QB prevents QA\nfrom it, respectively.\nThe algorithm called Double DQN [10] using Double Q-\nlearning for DQN has been also proposed. Equation (4) is\nrewritten as follows.\nyt = rt + Œ≥Q(st+1, argmax\na\nQ(st+1, a; Œ∏); Œ∏)\n(7)\nHasselt et al. have incorporated the idea of Double Q-\nlearning into (7).\nyDouble\nt\n= rt + Œ≥Q(st+1, argmax\na\nQ(st+1, a; Œ∏); Œ∏‚àí)\n(8)\nThis makes it possible to get higher scores with 90% of\ngames played by DQN, and it is still widely used as an better\nalgorithm than DQN.\nIII. RELATED WORK\nNeural Episodic Control(NEC) is one of the method of\nefÔ¨Åcient sampling from Replay Buffer. It is the algorithm\nbased on episodic memory and improved Model-Free Episodic\nControl (MFEC) [11] to learn end-to-end from state mappings\nto estimations of Q-values.\nDifferentiable Neural Dictionary(DND) has been proposed\nto make it successful. DND for a action a ‚ààA is a dictionary\nMa = (Ka, Va) which saves a pair of key Ka and value Va.\nThe key is the embedding h which is the feature extracted the\nstate s ‚ààS with CNN, and the value is the Q-value.\nWe can perform two kinds of operations for DND, Lookup\nand Write. In Lookup, when h featured by CNN and cor-\nresponding action a are entered, we lookup the top p-nearest\nneighbors for h in Ma using kd-trees [12]. We weight the\nvalue vi corresponding to p keys and we set it as Qa.\nwi =\nk(h, hi)\nP\nj k(h, hj)\n(9)\nQa =\nX\ni\nwivi\n(10)\nk(h, hi) is a kernel function for h and hi. In DND, (11) is\nused.\nk(h, hi) =\n1\n‚à•h ‚àíhi‚à•2\n2 + Œ¥\n(11)\nAlthough Œ¥ is a parameter to prevent division by zero, we\nshould make it little larger such as Œ¥ = 10‚àí3 because each\nvalue of p-nearest neighbors is referred to a certain extent.\nIn Write, we write an input h and a corresponding Q-\nvalue in DND. If the key that already matches the input h\nexists in Ma, update the corresponding Q-value according to\nthe following with a learning rate Œ±.\nQi ‚ÜêQi + Œ±(Q(N)(s, a) ‚àíQi)\n(12)\nWhen the size of the dictionary reaches the upper limit, we\noverwrites a pair that has not been referred to recently as the\ntop p-nearest neighbor value according to Least Recently Used\n(LRU).\nNEC uses N-step Q-learning [13] as a target value.\nQ(N)(st, a) =\nN‚àí1\nX\nj=0\nŒ≥jrt+j + Œ≥N max\na‚Ä≤ Q(st+N, a‚Ä≤)\n(13)\nHowever, N-step Q-learning is hard to be stabilized by off-\npolicy algorithm [14].\nWe use NEC in our proposed algorithm, but we replace the\noutput Q-value of NEC network with the Q-value deÔ¨Åned in\nChapter IV.\nIV. PROPOSED ALGORITHM\nNEC currently has the following problems.\n1) As the number of pairs of key and value in DND\nincrease, the computation time for Ô¨Ånding top p-nearest\nneighbors increases.\n2) The number of dictionaries M of DND is the same as\nthe size of the action space |A|.\n3) State space and action space need to be discrete because\nQ-learning is used.\nRegarding 3, Matsumori et al. [15] are addressing research\nin an environment where the state space is continuous and\nPartially Observable Markov Decision Process (POMDP).\nOur work focuses on the problems 1 and 2. Continuing\nto use NEC requires many computational resources due to\nconstraints of time computational quantity and space com-\nputational quantity. Therefore, we will address using NEC‚Äôs\nsampling efÔ¨Åciency only in early learning of other deep Q-\nlearning algorithms. We will use DQN as an example which\nis the simplest Deep Q-learning network and we call this\nalgorithm NEC2DQN(N2D).\nAs mentioned in [6], there is always one optimal action-\nvalue Q‚àó, and if it is the same policy, it always converges to\nthe same value. For this reason, both DQN algorithm and NEC\nalgorithm are possible to head to the same Q‚àó. Therefore, the\nQ-value estimated by other algorithms can be taken as the\nùë∏‚àó\nùë∏ùë´ùë∏ùëµ\nùë∏ùëµùë¨ùë™\nùëªùíÇùíìùíàùíÜùíïùëµùë¨ùë™\nùë∏‚àó\nùë∏ùë´ùë∏ùëµ\nùë∏ùëµùë¨ùë™\nùëªùíÇùíìùíàùíÜùíïùëµùë¨ùë™\nùëªùíÇùíìùíàùíÜùíïùë´ùë∏ùëµ\nLearning\nùë∏‚àó\nùë∏ùëµùë¨ùë™\nùë∏‚àó\nùë∏ùë´ùë∏ùëµ\nùë∏ùëµùë¨ùë™\nùë∏ùë´ùë∏ùëµ\nùë∏ùëµùë¨ùë™\nùë∏ùë´ùë∏ùëµ\nùë∏ùëµùë¨ùë™\nùë∏ùë´ùë∏ùëµ\nNEC,DQN\nNEC2DQN\nLearning\nFigure 1.\nThe image of the convergence of NEC2DQN: QNEC and QDQN will converge to the same optimal state-action value Q‚àó. The top shows the\nprogress of convergence when NEC and DQN learn respectively. The bottom is the progress of convergence of each Q-value by using NEC2DQN. Although\nQ‚àóis unknown and cannot be observed directly, NEC and DQN should be close to Q‚àó. In the early stage of learning, it is better to use the TargetNEC\nas shown in the Ô¨Ågure. Therefore, we want DQN to learn by approaching not TargetDQN but TargetNEC.\ntarget value. Hence it is easier to converge by using a better\ntarget value, the value approaches to Q‚àófaster. We show this\nsimple image in Figure 1.\nWe set QA(s, a) = QDQN(s, a), QB(s, a) = QNEC(s, a)\nin (5) and (6). Then we rewrite them as follow.\nQDQN(s, a) ‚ÜêQDQN(s, a)+Œ±(r+Œ≥QNEC(s‚Ä≤, a‚àó)‚àíQDQN(s, a))\n(14)\nQNEC(s, a) ‚ÜêQNEC(s, a)+Œ±(r+Œ≥QDQN(s‚Ä≤, b‚àó)‚àíQNEC(s, a))\n(15)\nwhere a‚àó= argmax\na\nQDQN(s‚Ä≤, a), b‚àó= argmax\na\nQNEC(s‚Ä≤, a)\nDouble DQN is one-step Q-learning, but NEC uses N-step\nreturns. Thus we want to rewrite (14) and (15) like N-step\nDouble DQN. Instead of preparing a target network, NEC\nstores the target value Q(N)\nNEC in Replay Buffer. In order to Ô¨Åt\nthis target value, we adopt N-step returns for DQN(hereinafter,\nwe call it N-step DQN).\nQDQN(s, a) ‚ÜêQDQN(s, a)+Œ±(Q(N)\nNEC(s, a‚àó)‚àíQDQN(s, a))\n(16)\nQNEC(s, a) ‚ÜêQNEC(s, a)+Œ±(Q(N)\nDQN(s, b‚àó)‚àíQNEC(s, a))\n(17)\nEquations (16) and (17) show Q(N)\nNEC(s, a‚àó) is necessary\nfor learning QDQN(s, a), and Q(N)\nDQN(s, b‚àó) is necessary for\nlearning QNEC(s, a). However, QNEC may not be able to\nlearn well until DQN learns as a network, and there is a\npossibility that QDQN cannot be learned well due to the\ninÔ¨Çuence. Especially, NEC should be able to learn earlier by\nlearning using the original Q(N)\nNEC. Therefore we rewrite (17)\nlike (19).\nQDQN(s, a) ‚ÜêQDQN(s, a)+Œ±(Q(N)\nNEC(s, a‚àó)‚àíQDQN(s, a))\n(18)\nQNEC(s, a) ‚ÜêQNEC(s, a)+Œ±(Q(N)\nNEC(s, b‚àó)‚àíQNEC(s, a))\n(19)\nQDQN(s, a) can learn with a better target value by NEC\nalgorithm at the beginning of learning. Even if learning this\nway, QDQN converges to the same value as QNEC. But\nwe need to consider the learning has advanced because this\nresearch aims to use NEC only in the early stage of learning.\nSince QDQN maybe estimated correctly to some extent, it can\nbe learned like Double DQN using N-step returns.\nQDQN(s, a) ‚ÜêQDQN(s, a)+Œ±(Q(N)\nDQN(s, a‚àó)‚àíQDQN(s, a))\n(20)\nQNEC(s, a) ‚ÜêQNEC(s, a)+Œ±(Q(N)\nDQN(s, b‚àó)‚àíQNEC(s, a))\n(21)\nComparing (18) with (19), they use the same target value\nQ(N)\nNEC. Similarly, both (20) and (21) use Q(N)\nDQN. Hence we\nreplace them with the same Q-value Q(N)\nN2D.\nQDQN(s, a) ‚ÜêQDQN(s, a)+Œ±(Q(N)\nN2D(s, a‚àó)‚àíQDQN(s, a))\n(22)\nQNEC(s, a) ‚ÜêQNEC(s, a)+Œ±(Q(N)\nN2D(s, b‚àó)‚àíQNEC(s, a))\n(23)\nFrom here, we deÔ¨Åne QN2D(s, a) to satisfy the above\nproperty. We prepare networks of NEC and DQN separately as\nshown in Figure 2. Both of the networks output their Q-value\nfor the state st, and we combine them as QN2D(st, a).\nQN2D(st, a) = Œª(t)QNEC(st, a) + (1 ‚àíŒª(t))QDQN(st, a)\n(24)\nThis Œª(t) is a monotonously decreasing function related to\nthe current learning step t. It represents how much to consider\nCNN\nFC\nCNN\nDND\nNEC\nDQN\nùúÜ(ùë°)\n1 ‚àíùúÜ(ùë°)\nùë∏ùëµùüêùë´(ùíî,ùíÇ)\nùëáùëéùëüùëîùëíùë°  ùíöùíï\nùëáùëéùëüùëîùëíùë°  ùíöùíï\nùë†\nùë∏ùëµùë¨ùë™(ùíî,ùíÇ)\nùë∏ùë´ùë∏ùëµ(ùíî,ùíÇ)\nFigure 2. NEC2DQN Network\nNEC. We make it linear decay from 1 to 0 as increasing\nlearning steps such as (25).\nŒª(t) =\n\u001a 1 ‚àí\nt\nCS\n(t < CS)\n0\n(otherwise)\n(25)\nAt the beginning of learning, it refers to the Q-value of\nNEC, and gradually refers to the Q-value of DQN as learning\nprogresses. Thus it is possible to switch naturally.\nWe set the steps CS to start to fully depend on DQN. It\nis not necessary to calculate QNEC(st, a), and the calculation\ntime is also reduced because Œª(t) = 0 after CS.\nLoss functions of NEC and DQN are required respectfully\nbecause they are separate networks, but we use the same target\nvalue yt. Although we use a‚àóand b‚àóin (22) and (23), we do\nnot use Double Q-learning but simply use N-step returns since\nwe do not want to use Q(N)\nN2D(s, a‚àó).\nyt = Q(N)\nN2D(s, a) =\nN‚àí1\nX\nj=0\nŒ≥jrt+j + Œ≥N max\na‚Ä≤ QN2D(st+N, a‚Ä≤)\n(26)\nThis is based on the fact that it is hard for overestimation\nof the Q-value to occur when the amount of experience\naccumulated in Replay Buffer is small because NEC can refer\nto DND.\nThese loss functions also use same target yt.\nLNEC(Œ∏t) = E[yt ‚àíQNEC(st, at)]\n(27)\nLDQN(Œ∏t) = E[yt ‚àíQDQN(st, at)]\n(28)\nSimilarly to original NEC, Replay Buffer stores a tuple of\n(st, at, yt). We accumulate the trajectory of the episode at the\nend of each episode because yt requires N steps of reward\ndata and the subsequent state st+N.\nWe show the overall algorithm in Algorithm 1.\nAlgorithm 1 NEC2DQN\n1: Initialize the number of entire timesteps S to 0.\n2: Initialize the change step CS for Œª(t)\n3: Initialize replay memory D to capacity CD.\n4: Initialize DND memories Ma to capacity CMa.\n5: Initialize action-value function QNEC and QDQN with\nrandom weights.\n6: for each episode do\n7:\nInitialize trajectory memory G .\n8:\nfor t = 1, 2, ...; T do\n‚ñ∑Explore and train.\n9:\nReceive observation st from environment.\n10:\nReceive QDQN(st, a).\n11:\nif S < CS then\n‚ñ∑If S < CS, we use NEC.\n12:\nReceive embedding h and QNEC(st, a).\n13:\nelse\n14:\nSet QNEC(st, a) to free values.\n15:\nend if\n16:\nCalculate QN2D(st, a).\n‚ñ∑Calculate by (24).\n17:\nat ‚ÜêŒµ-greedy policy based on QN2D(st, a).\n18:\nTake action at, receive reward rt.\n19:\nAppend (st, at, rt) to G.\n20:\nTrain on a random minibatch from D.\n21:\nS ‚ÜêS + 1\n22:\nend for\n23:\nfor t = 1, 2, ...; T do\n‚ñ∑Calculate N-step returns.\n24:\nCalculate yt.\n‚ñ∑Calculate by (26).\n25:\nAppend (st, at, yt) to D.\n26:\nif S < CS then\n27:\nAppend (ht, yt) to Mat.\n28:\nend if\n29:\nend for\n30: end for\nFigure 3. A Pong frame\nFigure 4. The comparing NEC2DQN with other algorithms\nFigure 5. The difference of NEC2DQN performance by Replay Buffer size\nFigure 6. The Ô¨Årst 5M steps in Figure 5.\nV. IMPLEMENTATION\nWe use Pong of Atari 2600 provided with OpenAI Gym [16]\nwhich can easily share the result of library and reinforcement\nlearning algorithms.\nPong is the game of 21 points win. If the opponent cannot\nhit the ball, we get a reward of +1, and if we cannot, we\nget a reward of -1. That is, the reward set R = {‚àí1, 0, 1}.\nMany of the algorithms targeting versatility such as DQN\nuse Reward Clipping that clips rewards gained from games\nto [‚àí1, 1]. It is a technique that we enables an agent to learn\nwithout changing other parameters. However, Reward Clipping\nmay make it impossible to distinguish between a high reward\nand a small reward with a large absolute value, so there is a\npossibility that it will not try to obtain high rewards [17]. We\ndo not consider it in this research because the Pong‚Äôs reward\nis r ‚ààR.\nLike the DQN, we process one frame of the game such as\nFigure 3 to 84√ó84 and convert to grayscale and normalize it.\nThen the state is set with the consecutive 4 frames together. We\nshow the main parameters and the network parameters setting\nfor the experiment in Table II and Table III as Appendix.\nWe evaluate the learning speed of NEC2DQN, N-step DQN\nand Double DQN. Also we observe the learning result by the\ndifference in the size of Replay Buffer. We test 5 times every\n50,000 learning steps, and we use greedy policy (Œµ = 0) in\nevery test.\nVI. RESULT\nFigure 4 is a comparison with NEC2DQN, N-step DQN\n(N = 10) and Double DQN. Although Double DQN required\nmore than 30M steps to earn more than 10 points, NEC2DQN\nachieved it in about 3M steps. Furthermore, NEC2DQN has\nbetter learning efÔ¨Åciency and performance than N-step DQN.\nFigure 5 and Figure 6 show the difference in results de-\npending on Replay Buffer size. The size affects the stability\nof sampling. The larger it is, the better the performance is.\nHowever, since the target value is also stored in Replay Buffer,\nthe target value is too old to proceed well if it is too large like\nsize = 500, 000. Looking at Figure 6, the smaller size is, the\nfaster learning is due to the newness of the target value at\nthe 2M frame. But as learning progresses, it turns out that the\ngame score is not obtained well in the case the size is small like\nsize = 100, 000. NEC2DQN needs considering this balance.\nVII. CONCLUSION\nIn this research, we have veriÔ¨Åed DQN is possible to use the\ngood sampling efÔ¨Åciency of NEC in the beginning of learning\nusing a Pong example. We have showed that the learning speed\nis faster than only DQN by using the same and better target\nvalues for NEC and DQN. Indeed, we observed a signiÔ¨Åcant\nlearning speed improvement by using NEC during 2M steps\nonly.\nHowever it is necessary to conÔ¨Årm whether this method\nsucceeds also in other games. Particularly, although NEC does\nnot need to Reward Clipping, DQN is better to do Reward\nClipping. Thus, there is a possibility that we cannot use the\nNEC‚Äôs advantage.\nMoreover, Since NEC and DQN do not connect as the same\nnetwork, updating weights of the networks do not directly\naffect each other. Therefore, it is easy to replace it with a\nnetwork other deep Q-learning algorithm. It is necessary to\nverify whether other deep Q-learning algorithms also works\nas well as DQN by changing QDQN used for QN2D to the\nQ-value of others.\nThe most important issue is that multiple networks learn\nwhile choosing appropriate target values. In this paper, we\nhave used NEC since it is excellent in Q-value estimation at\nthe beginning of learning, but NEC is not necessarily useful\nfor estimating Q-value. Deep Q-learning from Demonstrations\n(DQfD) [18] is used to estimate the Q-value with reference to\nhuman play. It is sometimes hard to go well in tasks that\nhumans cannot do very well (such as Pong), but if it is a task\nthat humans can do well, it is good for learning faster than\nNEC. However, it is difÔ¨Åcult to use human demos when we\nmake our agent learn more complicated and time-consuming\ntasks. It is important that various networks of deep Q-learning\ncooperate by learning while choosing an appropriate target\nvalue automatically.\nREFERENCES\n[1] Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei\nA, Veness, Joel, Bellemare, Marc G, Graves, Alex, Riedmiller, Martin,\nFidjeland, Andreas K, Ostrovski, Georg, et al. Human-level control\nthrough deep reinforcement learning. In Nature, 518(7540):529-533,\n2015.\n[2] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil\nAnthony Bharath. A Brief Survey of Deep Reinforcement Learning.\narXiv:1708.05866, 2017.\n[3] Long-Ji Lin. Self-Improving Reactive Agents Based on Reinforce-\nment Learning, Planning and Teaching. Machine Learning, 8(3-4):293-\n321,1992.\n[4] Schaul, Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.\nPrioritized Experience Replay. In ICLR, 2016.\n[5] Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria‚Äò Puig-\ndome‚Äònech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles\nBlundell. Neural Episodic Control. arXiv:1703.01988, 2017.\n[6] Richard Bellman. On the Theory of Dynamic Programming. In PNAS,\n38(8):716-719, 1952.\n[7] Christopher JCH Watkins and Peter Dayan. Q-Learning. Machine Learn-\ning, 8(3-4):279-292, 1992.\n[8] Alex Krizhevsky, Ilya Sutskever, Geoffrey E,Hinton. ImageNet classiÔ¨Å-\ncation with deep convolutional neural networks. In ANIPS, 2012.\n[9] Hado van Hasselt. Double Q-Learning. In NIPS, 2010.\n[10] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement\nLearning with Double Q-Learning. In AAAI, 2016.\n[11] Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham\nRuderman, Joel Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis.\nModel-Free Episodic Control. arXiv:1606.04460, 2016.\n[12] Bentley, Jon Louis. Multidimensional binary search trees used for\nassociative searching. Commun. ACM, 18(9): 509-517, 1975.\n[13] Jing Peng and Ronald Williams. Incremental multi-step Q-learning.\nMachine Learning, 22:283- 290, 1996.\n[14] Remi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G Belle-\nmare. Safe and EfÔ¨Åcient Off-Policy Reinforcement Learning. In NIPS,\n2016.\n[15] Shoya Matsumori, Takuma Seno, Toshiki Kikuchi, Yusuke Takimoto,\nMasahiko Osawa, Michita Imai. Embedding Cognitive Map in Neural\nEpisodic Control. In SIG-AGI, 2017\n[16] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym.\narXiv:1606.01540, 2016.\n[17] Hado van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, David\nSilver. Learning values across many orders of magnitudes. In NIPS,\n2016.\n[18] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom\nSchaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian\nOsband, John Agapiou, et al. Deep Q-learning from Demonstrations.\narXiv:1704.03732, 2017.\nAPPENDIX\nTABLE II\nHYPER PARAMETER\nParameter\nValue\nOptimizer\nRMSProp(Œµ = 0.01)‚Ä†\nOptimizer learning rate\n0.000025\nOptimizer momentum\n0.95‚Ä†\nExplore Œµ\n1 ‚Üí0.01 over 1M steps\nReplay buffer size\n300,000\nDND learning rate\n0.1\nDND size\n500,000 per action‚Ä†‚Ä†\np for KDTree\n50‚Ä†‚Ä†\nN-step returns N\n10\nNEC embedding size\n64\nDQN embedding size\n512‚Ä†\nReplay period\nevery 4 learning steps‚Ä†\nMinibatch size\n32‚Ä†\nDiscount rate\n0.99‚Ä†\nNEC2DQN change step CS\n2M steps\n‚Ä†same as Double DQN [10] ‚Ä†‚Ä†same as NEC [5]\nTABLE III\nNETWORK\nParameter\nValue\nNEC,DQN: CNN channels\n32, 64, 64\nNEC,DQN: CNN Ô¨Ålter size\n8 √ó 8, 4 √ó 4, 3 √ó 3\nNEC,DQN: CNN stride\n4, 2, 1\nNEC : embedding size\n64\nDQN : hidden layer\n512\nDQN : output units\nNumber of actions\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2018-01-06",
  "updated": "2018-06-03"
}