{
  "id": "http://arxiv.org/abs/1801.02243v1",
  "title": "Trading the Twitter Sentiment with Reinforcement Learning",
  "authors": [
    "Catherine Xiao",
    "Wanfeng Chen"
  ],
  "abstract": "This paper is to explore the possibility to use alternative data and\nartificial intelligence techniques to trade stocks. The efficacy of the daily\nTwitter sentiment on predicting the stock return is examined using machine\nlearning methods. Reinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The predicting power of\nthe sentiment signal is more significant if the stock price is driven by the\nexpectation of the company growth and when the company has a major event that\ndraws the public attention. The optimal trading strategy based on reinforcement\nlearning outperforms the trading strategy based on the machine learning\nprediction.",
  "text": "Trading the Twitter Sentiment with Reinforcement Learning\nCatherine Xiao\ncatherine.xiao1@gmail.com\nWanfeng Chen\nwanfengc@gmail.com\nAbstract— This paper is to explore the possibility to use\nalternative data and artiﬁcial intelligence techniques to trade\nstocks. The efﬁcacy of the daily Twitter sentiment on predicting\nthe stock return is examined using machine learning methods.\nReinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The\npredicting power of the sentiment signal is more signiﬁcant\nif the stock price is driven by the expectation on the company\ngrowth and when the company has a major event that draws\nthe public attention. The optimal trading strategy based on\nreinforcement learning outperforms the trading strategy based\non the machine learning prediction.\nI. INTRODUCTION\nIn a world where traditional ﬁnancial information is ubiq-\nuitous and the ﬁnancial models are largely homogeneous,\nﬁnding hidden information that has not been priced in from\nalternative data is critical. The recent development in Natural\nLanguage Processing provides such opportunities to look\ninto text data in addition to numerical data. When the\nmarket sets the stock price, it is not uncommon that the\nexpectation of the company growth outweighs the company\nfundamentals. Twitter, a online news and social network\nwhere the users post and interact with messages to express\nviews about certain topics, contains valuable information on\nthe public mood and sentiment. A collection of research [1]\n[6] have shown that there is a positive correlation between\nthe ”public mood” and the ”market mood”. Other research[2]\nalso shows that signiﬁcant correlation exists between the\nTwitter sentiment and the abnormal return during the peaks\nof the Twitter volume during a major event.\nOnce a signal that has predicting power on the stock\nmarket return is constructed, a trading strategy to express the\nview of the signal is needed. Traditionally, the quantitative ﬁ-\nnance industry relies on backtest, a process where the trading\nstrategies are tuned during the simulations or optimizations.\nReinforcement learning provides a way to ﬁnd the optimal\npolicy by maximizing the expected future utility. There are\nrecent attempts from the Artiﬁcial Intelligence community\nto apply reinforcement learning to asset allocation [5], algo-\nrithmic trading[3][7], and portfolio management[4].\nThe contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\nespecially when the Twitter volume is high. Second, we\nproposed a trading strategy based on reinforcement learning\n(Q-learning) that takes the sentiment features as part of its\nstates.\nThe paper is constructed as follows: In the second section,\nscraping Tweets from Twitter website and preprocessing the\ndata are described in details. In the third section, assigning\nsentiment scores to the text data is discussed. In the fourth\nsection, feature engineering and prediction based on the\nsentiment score is discussed. In the ﬁfth section, how the\nreinforcement learning is applied to generate the optimal\ntrading strategy is described.\nII. TWITTER DATA SCRAPING AND PREPROCESSING\nThere are two options of getting the Tweets. First, Twitter\nprovides an API to download the Tweets. However, rate limit\nand history limit make it not an option for this paper. Second,\nscrapping Tweets directly from Twitter website. Using the\nsecond option, the daily Tweets for stocks of interest from\n2015 January to 2017 June were downloaded.\nThe predicting power of Twitter sentiment varies from\nstock to stock. For stocks that are mostly driven by the com-\npany fundamentals and hold by the institutional investors,\nthe predicting power of the Twitter sentiment is limited.\nFor stocks that are priced by the public expectation on the\ncompany’s future growth, Twitter sentiment describes the\nconﬁdence and expectation level of the investors.\nFor this reason, two companies from the same industry,\nTesla and Ford are investigated on how Twitter sentiment\ncould impact the stock price. Tesla is an electronic car\ncompany that shows consecutive negative operating cash ﬂow\nand net income but carries very high expectation from the\npublic. Ford, is a traditional auto maker whose stock prices\nhas been stabilized to represent the company fundamentals.\nTo investigate how different key words impact the predict-\ning power of the sentiment score, two Tweet sets, a ticker\nset and a product set, are prepared for each stock. The ﬁrst\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company’s products and news. The keywords for the\nsecond dataset are deﬁned according to the top twenty related\nkeywords of the stock ticker according to Google Trend, a\nweb facility shows how often a certain word is searched\nrelative to Google’s total search volume. For example, ”Elon\nMusk” is among the set of keywords that retrieve the second\ntweets set for Tesla.\nTweets contain irregular symbols, url and emoji etc which\nhas to be preprocessed so that the NLP algorithm can extract\nthe relevant information efﬁciently. Examples of preprocess-\ning are described as below:\n• Filter out tweets that contains http or .com Motiva-\ntion: They’re usually ads e.g. #Fhotoroom #iPhone\narXiv:1801.02243v1  [cs.AI]  7 Jan 2018\nhttps://www.fhotoroom.com/fhotos/\n• Remove #hashtag, @user, tabs and extra spaces\n• Filter out tweets that contain consecutive two question\nmarks (?). The reason is the coding of these tweets is\nusually not recognizable.\n• Filtering\nout\nnone-English\ntweets\nusing\nGoogles\nLangdetect package.\nIII. SENTIMENT SCORE\nTo translate each tweet into a sentiment score, the Stanford\ncoreNLP software was used. Stanford CoreNLP is designed\nto make linguistic analysis accessible to the general public. It\nprovides named Entity Recognition, co-reference and basic\ndependencies and many other text understanding applica-\ntions. An example that illustrate the basic functionality of\nStanford coreNLP is shown in Figure.1\nFig. 1.\nDemo of the functionalities provided by Stanford CoreNLP\ncoreNLP can compute a sentiment score for each sentence\nwith value ranging from 0 to 4 , where 0 stands for negative,\nand 4 stands for very positive. For tweets with multiple\nsentences, the average of the sentiment scores of all sentences\nis used as the sentiment score of the Tweets.\nThe number of Tweets varies everyday from a couple of\nhundreds to over ten thousands, depends on if the company\nhas a major event that attracts the public attention. The\nsentiment scores are normalized between 0 to 1, and features\nbased on the sentiment score is constructed and normalized.\nFigure 2 shows the relationship between Tesla stock return\nand stock sentiment score. According the distribution of the\nsentiment score, the sentiment on Tesla is slightly skewed\ntowards positive during the testing period. The price has\nbeen increased signiﬁcantly during the testing period, which\nreﬂected the positive sentiment. The predicting power of\nsentiment score is more signiﬁcant when the sentiment is\nmore extreme and less so when the sentiment is neutral.\nIV. THE SENTIMENT MACHINE LEARNING MODEL\nA. Feature Engineering\nFeature engineering is the process to extract meaningful\ninformation from the raw data in order to improve the\nperformance of machine learning mode. Domain knowledge\nand intuition are often applied to keep the number of the\nFig. 2.\nHistogram\nfeatures reasonable relative to the training data size. Two\ncategories of features are deﬁnes: technical features and\nsentiment features. The technical features include previous\nday’s return and volume, price momentum and volatility. The\nsentiment features include number of Tweets, daily average\nsentiment score, cross-section sentiment volatility, sentiment\nmomentum and reversal.\nB. Machine Learning Prediction Model\nThe logistic regression with L1 regularization and RBF-\nkernel SVM are applied to predict a binary outcome, i.e.\nwhether the stock return will be positive or negative in the\nnext day. Both technical and sentiment-based features carry\nimportant information about the stock price and therefore are\nprovided as the model inputs. Half of the dataset is used for\ntraining and the rest is used for testing.\nThe 3 fold cross validation is applied to learn the model\nhyper-parameters. Speciﬁcally, the hyper-parameters C of\nboth models and of RBF-kernel SVM are learned such that\nthe dev set accuracy is maximized. The hyper-parameter C\nin logistic regression determines the degree of regularization.\nSmaller C means more regularization, i.e. high bias and low\nvariance. RBF-kernel SVM has two hyper-parameters, C and\n. C controls the width of soft margin, smaller C allows\nplacing more samples on the wrong side of the margin. is a\nparameter in RBF kernel. A larger means a Gaussian with\nsmaller variance and thus less inﬂuence of support vectors.\nTypically, small C and large\nlead to high bias and low\nvariance.\nTo evaluate if the sentiment feature improves the pre-\ndiction accuracy, a baseline model is deﬁned. The baseline\napplies linear logistic regression to a set of stock technical\nsignals to predict the following days stock return sign (+/).\nNo sentiment features are provided to the baseline model.\nC. Predicting using ticker dataset and product dataset\nThe predicting power for the ticker dataset and product\ndataset are compared. The ticker dataset contains tweets\nthat searched strictly according to the stock ticker. The\n2\nFig. 3.\nThe chart displays the accuracy on predicting the ”alpha”, which\ndeﬁnes as the return of the stock minus the return of its sector ETF.\nproduct dataset is searched using keywords that related to\nthe company’s product and other related topic(see session II\nfor more details). The former dataset represents the investors’\nsentiment, while the latter dataset represents customers sen-\ntiment.\nIn the Tesla case, using product tweets consistently outper-\nforms using the ticker tweets(accuracy 0.6 vs 0.5), it is less\nso in the Ford case(0.58 vs 0.55). The result is displayed\nin Figure 3 First, this is because Tesla’s stock price is\ndriven more by the sentiment on its product instead of the\nstock itself. For Ford, not many people actually express their\nopinion about Ford’s product via Twitter. Secondly, Tesla has\nmany more product tweets than ticker tweets, but Ford is\nopposite.\nD. Predicting using logistic regression and SVM\nIn most cases, SVM performs only slightly better than\nlogistic regression in validation set, although much better\nin testing set. This may be because the dataset is not\nlarge enough to prevent SVM overﬁtting. The comparision\nbetween the logistic regression and the SVM is displayed in\nFigure 3\nE. Predicting Total Return vs Alpha\nIt is important to identify which is a better target for the\nprediction. Two targets, predicting ”alpha or predicting ”total\nreturn” are compared. ”Alpha” deﬁnes as the excess stock\nreturn over its sector ETF. ”Total return” is the absolution\nstock return. Predicting ”alpha” achieves better performance\nthan predicting total return. This is because the sentiment\nis more related to stocks idiosyncratic. Good sentiments\ntowards a speciﬁc company or its stock wont override the\noverall stock market or sectors impact on the stock return.\nF. Tesla vs Ford\nThe prediction accuracy on Tesla is higher than Ford\naccording to Figure3. The reason is because Tesla’s stock\nprice largely reﬂects the sentiment and conﬁdence level of\nthe public. The company has consecutive negative cash ﬂow\nand net income, making prediction based on its fundamental\ninformation unrealistic. On the other hand, the stock price\nof Ford, which is a traditional automaker, is not that related\nto the public sentiment.\nG. Feature selection and overﬁtting\nTo improve the model accuracy, more features were con-\nstructed. However, more features do not result in better\naccuracy. For example, in Figure 4, adding more features\nimprove the training accuracy but deteriorates out-of-sample\naccuracy due to overﬁtting.\nFig. 4.\nThe chart shows an example of overﬁtting in the SVM model. The\noverﬁtting is caused by adding too many features to the model inputs but\nnot providing enough data for the model to generalize. Different lines shows\nthe SVM performance under different γ parameter. None of the parameter\nachieves better accuracy than a restricted set of features.\nThe recursive feature elimination and cross validation\n(RFECV) for feature selection is experimented during the\nfeature selection phase. However, only similar or even\nslightly worse performance was achieved by RFECV than\nselecting features according to domain knowledge and intu-\nition. This is because recursive feature elimination is a greedy\nalgorithm and thus doesnt guarantee optimal solution.\nV. Q-LEARNING\nQ-learning is a model-free reinforcement learning tech-\nnique. Speciﬁcally, Q-learning can be used to ﬁnd an optimal\npolicy given a Markov Decision Process(MDP). Instead of\nlearning the transition probability, Q-learning directly learns\nthe expected utility of taking an action from a certain state.\nBy maximizing the expected utility of the certain state, the\noptimal policy is found.\nTraditionally, quants propose trading strategies according\nto backtest, where the optimal parameters are tuned by\nmaximizing the objective function based on historical data.\nHowever, this common practice adopted by the investment\nindustry has drawbacks. First, it over-ﬁts the historical data\nand doesn’t generalize to out of sample data. In addition, the\nmodel need to be recalibrated periodically due to the eco-\nnomic regime change. A strategy signiﬁcantly outperforms\nin a high volatility environment might suffer signiﬁcantly in\na low volatility environment.\n3\nThe Q-learning, in the opposite, learns from the feedback\nfrom the market, and generates the optimal trading strategy\naccording to past experience, and automatically adapts to the\nnew market regime.\nIn this paper, the Q-learning algorithm is applied to gen-\nerate the optimal trading strategy. The market is modeled as\na Markov Decision Process where the outcomes are random\nand not under the control of the decision maker. The states\ncontain information of three categories: technical indicators,\nsentiment features and portfolio information. The actions\ncontains buy, sell and hold. The reward is the next day market\nreturn. The limit of leverage and the loss-cutting threshold\nare implemented in the relation ship of successor state and\naction. For example, if the leverage constrain has been met,\nthe actions that valid for this state are only ”hold” or ”sell”. If\nthe loss cutting threshold has been triggered, say the portfolio\nlost half of the capital and this is the maximum tolerance of\nloss, only the action that exit current position is valid.\nA. Learning\nFormally, the learning process deﬁnes as below. In Q-\nlearning the optimal expected utility of a (state, action) pair\nˆQopt(s, a) is updated with the rewards r and the expected\nutility of the subsequent state ˆVopt(s′) after taking the action\na.\nˆQopt(s, a) ←(1 −η) ˆQopt(s, a) + η(r + γ ˆVopt(s′)\n(1)\nVopt(s′) =\nmax\na′∈Actions(s′)\nˆQopt(s′, a′)\n(2)\nThe optimal policy is proposed by Q-learning as\nπopt(s) = arg max\na∈act(s) Qopt(s, a)\n(3)\nB. Function Approximation\nFunction approximation refers to the method to generalize\nunseen states by applying machine learning methods. The Q-\ntable stores the expected utility for each (state,action) pair\nthat has been explored. When predicting the expected utility\nfor a certain (state, action) pair, we will look up the Q-table.\nWhen the MDP has many states and actions, it is very likely\nthat a (state, action) pair has not been explored yet so the\nestimate is not accurate. It is too slow to look up a gigantic\ntable and most likely there is not enough training data to\nlearn each of the state individually. Function approximation\nuses features to capture the characteristics of the states and\napplies stochastic gradient descent to update the weights on\neach feature. More speciﬁcally, below equation is applied\nto generalize the unseen state in this paper. Deﬁne features\nφ(s, a) and weights w, then\nˆQopt(s, a; w) = w · φ(s, a)\n(4)\nFor each (s, a, r, s′), apply stochastic gradient descent to\nupdate the weights.\nw ←w −η[ ˆQopt(s, a; w) −(r + γ ˆVopt(s′))]φ(s, a)\n(5)\nwhere η is the learning rate, r is the reward and γ is the\ndiscount factor.\nC. Exploration and Exploitation\nIt is necessary to balance the exploration and exploitation.\nOne might suggest naively to take action only according\nto the optimal policy estimated by maximizing ˆQopt(s, a).\nHowever, this greedy strategy is equivalent to stay in the\ncomfortable zone all the time in life, without gaining new\nexperience and unable to give reasonable prediction when\nencounters unseen situations. Another extreme is to always\nexplore by choosing an action randomly. Without apply-\ning the hard lesson learned and obtaining the rewards,\nthe algorithm can lead to unsatisﬁable utility at the end.\nTherefore, in this paper the Epsilon-greedy strategy is applied\nfor exploration. For a certain probability, the algorithm\nacts randomly(exploration), for the rest the algorithm acts\noptimally(exploitation).\nD. Result and Discussion\nFigure 5 shows the cumulative return over 1 year period.\nThe strategy trades daily. The Q-learning states include\nportfolio position, sentiment features and technical indicators\nsuch as price momentum. The machine learning strategy\npredicts the binary movement (+ or -) of next trading day\nprice based on sentiment features and technical indicators.\nThe backtest rule based on the machine learning prediction\nis to long the stock if the prediction is +, short the stock if -.\nThe baseline is the same with machine learning except only\nthe technical indicator was used as the feature. The oracle\nmodel of this project is a trader who has insider information\nabout the stock and be able to bet and act correctly on every\nsingle day of the testing period. The oracle model is able\nto achieve 6 times of the initial capital at the end of testing\nperiod.\nFig. 5.\nThe chart shows the trading strategy derived from Q-learning(in\nblue) outperform the backtest result using machine learning features(in red).\nBoth of Q-learning strategy and machine learning strategy outperform the\nbaseline(in green).\nThere are observations that worth a discussion. At the\nbeginning of the testing period, the Q-learning has not learnt\nhow to estimate the expected utility of a certain action yet.\nThe performance of the initial period is more unstable than\n4\nlater. Q-learning does better when the state is more common\nbecause it accumulates more experience about the situation\nbut might not take the best action when a outlier state is\npresented. The performance of the q-learning varies during\ndifferent batch due to the random nature of exploitation\nand exploration. In general Q-learning is able to deliver\nbetter performance than using the binary prediction from\nthe machine learning models. Both of the Q-learning and\nmachine learning model outperform the baseline model.\nVI. FUTURE WORK\nThere are many areas that can be improved given more\nresource and data. Below is a list of the improvement that\ncould make this idea more robust.\n• Use intraday data to test the sentiment signal and Q-\nlearning. By training with more data and trading more\npromptly, we expect both sentiment machine learning\nmodel and Q-learning to do better.\n• With more data, more features can be considered and\nincorporated into the model.\n• Apply different function approximators, for example,\nneural net, to better generalize the states and provide\nmore stable behavior\n• Add another class to the existing binary classiﬁer in-\nsigniﬁcant price change. The is motivated by preventing\nthe classiﬁer to ﬁt to the noise inherent in stock market\nprice movement, and lumps small, statistically insignif-\nicant upward or downward movements indiscriminately\nwith large ones.\n• Add crude oil future price as a feature to predict Tesla\nstock alpha return sign\n• Extend the sentiment analysis to other stocks and Cryp-\ntocurrency, which is an asset class that driven even more\nby the public sentiment.\nVII. CONCLUSIONS\nThe paper explores the possibility to predict stock price\nusing text data and reinforcement learning technique. Pre-\ndicting stock price direction using Twitter sentiment is chal-\nlenging but promising. Which stock and what to predict is\nmore important than how to predict. For example, Tesla,\na company driven by the expectation of the company’s\ngrowth is a better target than Ford, a traditional auto maker.\nReinforcement learning is applied to ﬁnd the optimal trading\npolicy by learning the feedbacks from the market. The Q-\nlearning is able to adapt automatically if the market regime\nshifts and avoid backtesting, a process applied by investment\nindustry that often overﬁt the historical data. Both of the ma-\nchine learning model and the Q-learning model outperforms\nthe baseline model, which is a logistic regression without\nsentiment features.\nACKNOWLEDGMENT\nWe would like to thank Anna Wang, who is the project\nmentor, gives very practical suggestions and guidance. We\nwould like to thank Standford University for the very chal-\nlenging and exciting CS221 course materials and Prof. Percy\nLiang who did such a great job getting us interested in\nsentiment analysis and reinforcement learning\nREFERENCES\n[1] Johan Bollen and Huina Mao. Twitter mood as a stock market predictor.\nComputer, 44(10):91–94, 2011.\n[2] Johan Bollen, Huina Mao, and Xiao-Jun Zeng. Twitter mood predicts\nthe stock market. Journal of Computational Science, 2(1):1–8, March\n2011. arXiv: 1010.3003.\n[3] James Cumming, Dalal Alrajeh, and Luke Dickens. An Investigation\ninto the Use of Reinforcement Learning Techniques within the Algo-\nrithmic Trading Domain. 2015.\n[4] Zhengyao Jiang, Dixing Xu, and Jinjun Liang. A Deep Reinforcement\nLearning Framework for the Financial Portfolio Management Problem.\narXiv preprint arXiv:1706.10059, 2017.\n[5] J. Moody and M. Saffell. Learning to trade via direct reinforcement.\nIEEE Transactions on Neural Networks, 12(4):875–889, July 2001.\n[6] Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie\nDeng. Exploiting Topic based Twitter Sentiment for Stock Prediction.\nACL (2), 2013:24–29, 2013.\n[7] Jonah Varon and Anthony Soroka. Stock Trading with Reinforcement\nLearning. 2016.\n5\n",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.SI"
  ],
  "published": "2018-01-07",
  "updated": "2018-01-07"
}