{
  "id": "http://arxiv.org/abs/1801.03230v3",
  "title": "Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches",
  "authors": [
    "Sarfaraz Hussein",
    "Pujan Kandel",
    "Candice W. Bolan",
    "Michael B. Wallace",
    "Ulas Bagci"
  ],
  "abstract": "Risk stratification (characterization) of tumors from radiology images can be\nmore accurate and faster with computer-aided diagnosis (CAD) tools. Tumor\ncharacterization through such tools can also enable non-invasive cancer\nstaging, prognosis, and foster personalized treatment planning as a part of\nprecision medicine. In this study, we propose both supervised and unsupervised\nmachine learning strategies to improve tumor characterization. Our first\napproach is based on supervised learning for which we demonstrate significant\ngains with deep learning algorithms, particularly by utilizing a 3D\nConvolutional Neural Network and Transfer Learning. Motivated by the\nradiologists' interpretations of the scans, we then show how to incorporate\ntask dependent feature representations into a CAD system via a\ngraph-regularized sparse Multi-Task Learning (MTL) framework. In the second\napproach, we explore an unsupervised learning algorithm to address the limited\navailability of labeled training data, a common problem in medical imaging\napplications. Inspired by learning from label proportion (LLP) approaches in\ncomputer vision, we propose to use proportion-SVM for characterizing tumors. We\nalso seek the answer to the fundamental question about the goodness of \"deep\nfeatures\" for unsupervised tumor classification. We evaluate our proposed\nsupervised and unsupervised learning algorithms on two different tumor\ndiagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans,\nrespectively, and obtain the state-of-the-art sensitivity and specificity\nresults in both problems.",
  "text": "Accepted for publication in IEEE Transactions on Medical Imaging 2019\n1\nLung and Pancreatic Tumor Characterization in the\nDeep Learning Era: Novel Supervised and\nUnsupervised Learning Approaches\nSarfaraz Hussein, Pujan Kandel, Candice W. Bolan, Michael B. Wallace, and Ulas Bagci‚àó, Senior Member, IEEE.\nAbstract‚ÄîRisk stratiÔ¨Åcation (characterization) of tumors from\nradiology images can be more accurate and faster with computer-\naided diagnosis (CAD) tools. Tumor characterization through\nsuch tools can also enable non-invasive cancer staging, prog-\nnosis, and foster personalized treatment planning as a part of\nprecision medicine. In this study, we propose both supervised\nand unsupervised machine learning strategies to improve tumor\ncharacterization. Our Ô¨Årst approach is based on supervised\nlearning for which we demonstrate signiÔ¨Åcant gains with deep\nlearning algorithms, particularly by utilizing a 3D Convolu-\ntional Neural Network and Transfer Learning. Motivated by\nthe radiologists‚Äô interpretations of the scans, we then show how\nto incorporate task dependent feature representations into a\nCAD system via a graph-regularized sparse Multi-Task Learning\n(MTL) framework.\nIn the second approach, we explore an unsupervised learning\nalgorithm to address the limited availability of labeled train-\ning data, a common problem in medical imaging applications.\nInspired by learning from label proportion (LLP) approaches\nin computer vision, we propose to use proportion-SVM for\ncharacterizing tumors. We also seek the answer to the fun-\ndamental question about the goodness of ‚Äúdeep features‚Äù for\nunsupervised tumor classiÔ¨Åcation. We evaluate our proposed\nsupervised and unsupervised learning algorithms on two different\ntumor diagnosis challenges: lung and pancreas with 1018 CT\nand 171 MRI scans, respectively, and obtain the state-of-the-art\nsensitivity and speciÔ¨Åcity results in both problems.\nIndex Terms‚ÄîUnsupervised Learning, Lung cancer, 3D CNN,\nIPMN, Pancreatic cancer.\nI. INTRODUCTION\nApproximately 40% of people will be diagnosed with cancer\nat some point during their lifetime with an overall mortality of\n171.2 per 100,000 people per year (based on deaths between\n2008-2012) [1]. Lung and pancreatic cancers are two of the\nmost common cancers. While lung cancer is the largest cause\nof cancer-related deaths in the world, pancreatic cancer has\nthe poorest prognosis with a 5-year survival rate of only 7%\nin the United States [1]. With regards to pancreatic cancer,\nspeciÔ¨Åcally in this work, we focus on the challenging prob-\nlem of automatic diagnosis of Intraductal Papillary Mucinous\nNeoplasms (IPMN). IPMN is a pre-malignant condition and\nif left untreated, it can progress to invasive cancer. IPMN\nis mucin-producing neoplasm that can be found in the main\npancreatic duct and its branches. They are radiographically\n‚àóindicates corresponding author (ulasbagci@gmail.com).\nS. Hussein is with Center for Advanced Machine Learning (CAML) at\nSymantec Corporation. U. Bagci is with Center for Research in Computer\nVision (CRCV) at University of Central Florida (UCF), Orlando, FL; P.\nKandel, C. Bolan, and M. Wallace are with Mayo Clinic, Jacksonville, FL\nComputer \nAided \nDiagnosis \n(CAD)\nDeep Learning \n(CNN)\nMulti-Task Learning\nClustering\nProportion - SVM\nLung nodule \ncharacterization \nIPMN \nclassification\nCase Studies\nSupervised Learning (Section III)\nUnsupervised Learning (Section IV)\nFigure 1: A block diagram to represent different schemes,\nmethods and experimental case studies presented in this pa-\nper. We develop both supervised and unsupervised learning\nalgorithms to characterize tumors. For the supervised learning\nscheme, we propose a new 3D CNN architecture based on\na Graph Regularized Sparse Multi-task learning and perform\nevaluations for lung nodule characterization from CT scans.\nFor unsupervised learning scheme, we propose a new clus-\ntering algorithm, ‚àùSVM, and test it for the categorization of\nlung nodules from CT scans and pancreatic cysts (IPMN) from\nMRI scans.\nidentiÔ¨Åable precursors to pancreatic cancer [2]. Detection and\ncharacterization of these lung and pancreatic tumors can aid\nin early diagnosis; hence, increased survival chance through\nappropriate treatment/surgery plans.\nConventionally, the CAD systems are designed to assist\nradiologists in making accurate and fast decisions by reduc-\ning the number of false positives and false negatives. For\ndiagnostic decision making, a higher emphasis is laid on\nincreased sensitivity: a false-Ô¨Çag is more tolerable than a\ntumor being missed or incorrectly classiÔ¨Åed as benign. In this\nregard, a computerized analysis of imaging features becomes\na key instrument for radiologists to improve their diagnostic\ndecisions. In the literature, automated detection and diagnosis\nmethods had been developed for tumors in different organs\nsuch as breast, colon, brain, lung, liver, prostate, and others.\nAs typical in such studies, a CAD includes preprocessing\nand feature engineering steps (including feature extraction and\nselection) followed by a classiÔ¨Åcation step [3], [4], [5], [6].\nHowever, with the success of deep learning, a transition from\nfeature engineering to feature learning has been observed in\narXiv:1801.03230v3  [cs.CV]  18 Jan 2019\n2\nmedical image analysis literature. Those systems comprise\nConvolutional Neural Networks (CNN) as feature extractor\nfollowed by a conventional classiÔ¨Åer such as Random Forest\n(RF) [7], [8]. In scenarios where a large number of labeled\ntraining examples are available, however, end-to-end trainable\ndeep learning approaches can be employed [9].\nThis paper includes two main approaches for tumor charac-\nterization from radiology scans: supervised and unsupervised\nlearning algorithms. In the Ô¨Årst part, we focus on novel\nsupervised algorithms, which is a signiÔ¨Åcant extension to our\nIPMI 2017 study [10]. SpeciÔ¨Åcally, we Ô¨Årst present a novel su-\npervised learning strategy to perform risk-stratiÔ¨Åcation of lung\nnodules from low-dose CT scans. For this strategy, we per-\nform a 3D CNN based discriminative feature extraction from\nradiology scans. We contend that 3D networks are important\nfor the characterization of lung nodules in CT images which\nare inherently 3-dimensional. The use of conventional 2D\nCNN methods, whereas, leads to the loss of vital volumetric\ninformation which can be crucial for precise risk assessment\nof lung nodules. In the absence of a large number of labeled\ntraining examples, we utilize a pre-trained 3D CNN archi-\ntecture and Ô¨Åne-tune the network with lung nodules dataset.\nAlso, inspired by the signiÔ¨Åcance of lung nodule attributes\nfor clinical determination of malignancy [11], we utilize the\ninformation about six high-level nodule attributes such as\ncalciÔ¨Åcation, spiculation, sphericity, lobulation, margin, and\ntexture (Figure 2-A) to improve automatic benign-malignant\nclassiÔ¨Åcation. Then, we integrate these high-level features into\na novel graph regularized multi-task learning (MTL) frame-\nwork to yield the Ô¨Ånal malignancy output. We analyze the\nimpact of the aforementioned lung nodule attributes in-depth\nfor malignancy determination and Ô¨Ånd these attributes to be\ncomplementary when obtaining the malignancy scores. From\na technical perspective, we also exploit different regularizers\nand multi-task learning approaches such as trace-norm and\ngraph regularized MTL for regression.\nIn the second part of the paper, inspired by the success-\nful application of unsupervised learning methods in other\ndomains, we explore the potential of unsupervised learning\nstrategies in lung nodule and IPMN classiÔ¨Åcation. First, we\nextract discriminative information from a large amount of\nunlabeled imaging data. We analyze both hand-crafted and\ndeep learning features and assess how good those features are\nwhen applied to tumor characterization. In order to obtain an\ninitial set of labels in an unsupervised fashion, we cluster the\nsamples into different groups in the feature domain. We next\npropose to train Proportion-Support Vector Machine (‚àùSVM)\nalgorithm using label proportions rather than instance labels.\nThe trained model is then employed to learn malignant-benign\ncategorization of the tumors.\nThis paper is organized as follows. Section 2 describes re-\nlated work pertaining to supervised and unsupervised learning\nfor the diagnosis of lung nodules and IPMN. We present\nour MTL based supervised learning algorithm in Section 3.\nIn Section 4, we introduce an unsupervised learning method\nadapted for the diagnosis of lung nodules and IPMN from\nCT and MRI scans, respectively. The experiments and results\nare discussed in Section 5. In the same section, we also\nstudy the inÔ¨Çuence of different deep learning features for\nunsupervised learning and establish an upper bound on the\nclassiÔ¨Åcation performance using supervised learning methods.\nFinally, Section 6 states discussions and concluding remarks.\nII. RELATED WORK\nThis section summarizes the advances in machine learning\napplied to medical imaging and CAD systems developed for\nlung cancer diagnosis. Since the automatic characterization\nof IPMN from MRI scans has not been extensively studied\nin the literature, relevant works are mostly selected from the\nclinical studies. Our work is the Ô¨Årst in this regard.\nImaging Features and ClassiÔ¨Åers: Conventionally, the risk\nstratiÔ¨Åcation (classiÔ¨Åcation) of lung nodules may require\nnodule segmentation, computation and selection of low-level\nfeatures from the image, and the use of a classiÔ¨Åer/regressor.\nIn the approach by [12], different physical statistics includ-\ning intensity measures were extracted and class labels were\nobtained using ArtiÔ¨Åcial Neural Networks. In [3], lung nod-\nules were segmented using appearance-based models followed\nby shape analysis using spherical harmonics. The last step\ninvolved k-nearest neighbor based classiÔ¨Åcation. Another ap-\nproach extended 2D texture features including Local Binary\nPatterns, Gabor and Haralick to 3D [4]. ClassiÔ¨Åcation using\nSupport Vector Machine (SVM) was performed as the Ô¨Ånal\nstep. In a different study, Way et al. [5], implemented nod-\nule segmentation via 3D active contours, and then applied\nrubber band straightening transform. A Linear Discriminant\nAnalysis (LDA) classiÔ¨Åer was applied to get class labels.\nLee et al. [6] introduced a feature selection based approach\nutilizing both clinical and imaging data. Information content\nand feature relevance were measured using an ensemble of\ngenetic algorithm and random subspace method. Lastly, LDA\nwas applied to obtain Ô¨Ånal classiÔ¨Åcation on the condensed\nfeature set. In a recent work, spherical harmonics features\nwere fused with deep learning features [8] and then RF\nclassiÔ¨Åcation was employed for lung nodule characterization.\nHitherto, the application of CNN for nodule characterization\nhas been limited to 2D space [13], thus falling short of\nincorporating vital contextual and volumetric information. In\nanother approach, Shin et al. [14] employed CNN for the\nclassiÔ¨Åcation of lung nodules. Other than not completely 3D\nCNN, the approach didn‚Äôt take into account high-level nodule\nattributes and required training an off-the-shelf classiÔ¨Åer such\nas RF and SVM.\nThe information about different high-level image attributes\nhad been found useful in the malignancy characterization of\nlung nodules. In a study exploring the correlation between\nmalignancy and nodule attributes, [11] found that 82% of the\nlobulated, 93% of the ragged, 97% of the densely spiculated,\nand 100% of the halo nodules were malignant in a particular\ndataset. Automatic determination of lung nodule attributes and\ntypes had been explored in [15]. The objective was to perform\nthe classiÔ¨Åcation of six different nodule types such as solid,\nnon-solid, part-solid, calciÔ¨Åed, periÔ¨Åssural and spiculated nod-\nules. However, the approach is based on 2D CNN and fell short\n3\nof estimating the malignancy of lung nodules. Furthermore,\n66% of the round nodules were determined as benign.\nAlthough, not an objective in this paper, the detection of\nlung nodules has also been an active subject of interest among\nresearchers [16], [17], [18]. A short but informative review\nof the most recent detection studies can be found in [18].\nPancreatic\nCysts\n(IPMN):\nAlthough\nthere\nhas\nbeen\nconsiderable progress in developing automatic approaches to\nsegment pancreas and its cysts [19], [20], the use of advanced\nmachine learning algorithms to perform fully automatic\nrisk-stratiÔ¨Åcation of IPMNs is limited. The approach by\nHanania et al. [21] investigated the inÔ¨Çuence of 360 imaging\nfeatures ranging from intensity, texture, and shape to stratify\nsubjects as low or high-grade IPMN. In another example,\nGazit et al. [22] extracted texture and features from the solid\ncomponent of segmented cysts followed by a feature selection\nand classiÔ¨Åcation scheme. Both of these approaches [21], [22]\nrequired segmentation of cysts or pancreas and are evaluated\non CT scans only.\nUnsupervised Learning: Typically, the visual recognition and\nclassiÔ¨Åcation tasks are addressed using labeled data (supervi-\nsion). However, for tasks where manually generating labels\ncorresponding to large datasets is laborious and expensive, the\nuse of unsupervised learning methods is of signiÔ¨Åcant value.\nUnsupervised techniques had been used to solve problems\nin various domains ranging from object categorization [23],\nspeech processing [24] and audio classiÔ¨Åcation [25]. These\nmethods conventionally relied on some complementary infor-\nmation provided with the data to improve learning, which\nmay not be available for several classiÔ¨Åcation tasks in medical\nimaging.\nIn medical imaging, there have been different approaches\nthat used unsupervised learning for detection and diagnosis\nproblems. The approach by Shin et al. [26] used stacked au-\ntoencoders for multiple organ detection in MRI scans. Vaidhya\net al. [27] presented a brain tumor segmentation method with\nstacked denoising autoencoder evaluated on multi-sequence\nMRI images. In a work by Sivakumar et al. [28], the segmenta-\ntion of lung nodules is performed with unsupervised clustering\nmethods. In another study, Kumar et al. [7] used features from\nautoencoder for lung nodule classiÔ¨Åcation. These auto-encoder\napproaches, however, did not yield satisfactory classiÔ¨Åcation\nresults. Other than these, unsupervised deep learning has also\nbeen explored for mammographic risk prediction and breast\ndensity segmentation [29].\nUnsupervised feature learning remains an active research\narea for the medical imaging community, more recently\nwith Generative Adversarial Networks (GAN) [30]. In order\nto explore the information from unlabeled images, Zhang\net al. [31] described a semi-supervised method for the\nclassiÔ¨Åcation of four types of nodules. In sharp contrast\nto these approaches, the unsupervised learning strategies\npresented in this paper don‚Äôt involve feature learning using\nauto-encoders. Using sets of hand-crafted as well as pre-\ntrained deep learning features, we propose a new unsupervised\nlearning algorithm where an initially estimated label set is\nprogressively improved via proportion-SVM.\nOur Contributions\nA block diagram representing different supervised and unsu-\npervised schemes is presented in Figure 1. Overall, our main\ncontributions in this work can be summarized as follows:\n‚Ä¢ For lung nodule characterization, we present a 3D CNN\nbased supervised learning approach to fully appreciate the\nanatomical information in 3D, which would be otherwise\nlost in the conventional 2D approaches. We use Ô¨Åne-tuning\nstrategy to avoid the requirement for a large number of\nvolumetric training examples for 3D CNN. For this purpose.\nwe use a pre-trained network (which is trained on 1 million\nvideos) and Ô¨Åne-tune it on the CT data.\n‚Ä¢ We introduce a graph regularized sparse MTL platform\nto integrate the complementary features from lung nodule\nattributes so as to improve malignancy prediction. Figure 2-\nA shows high-level lung nodule attributes having varying\nlevels of prominence.\n‚Ä¢ We evaluate the proposed supervised and unsupervised\nlearning algorithms to determine the characterization of lung\nnodules and IPMN cysts (Table I). In the era where the wave\nof deep learning has swept into almost all domains of visual\nanalysis, we investigate the contribution of features extracted\nfrom different deep learning architectures. To the best of our\nknowledge, this is the Ô¨Årst work to investigate the automatic\ndiagnosis of IPMNs from MRI.\n‚Ä¢ In the proposed unsupervised learning algorithm, instead of\nhard assigning labels, we estimate the label proportions in\na data-driven manner. Additionally, to alleviate the effect\nof noisy labels (i.e. mislabels) obtained during clustering,\nwe propose to employ ‚àùSVM, which is trained on label\nproportions only.\nIII. SUPERVISED LEARNING METHODS\nA. Problem Formulation\nLet X = [x1, x2 . . . xn]T\n‚ààRn√ód represent the input\nfeatures obtained from n images of lung nodules each having\na dimension d. Each data sample has an attribute/malignancy\nscore given by Y = [y1, y2 . . . yn], where Y T ‚ààRn√ó1. While\nX consists of features extracted from radiology images, and\nY represents the malignancy score over 1-5 scale where 1\nrepresents benign and 5 represents malignant. In supervised\nlearning, the labeled training data is used to learn the coefÔ¨Å-\ncient vector or the regression estimator W ‚ààRd. In testing,\nW is used to estimate Y for an unseen feature/example.\nFor regression, a regularizer is often added to prevent over-\nÔ¨Åtting. Hence, a classical least square regression turns into a\nconstrained optimization problem with ‚Ñì1 regularization as:\nmin\nW ‚à•XW ‚àíY ‚à•2\n2 , s.t. ‚à•W‚à•1 ‚â§t.\n(1)\n4\n (a)       (b)        (c)        (d)       (e)         (f) \nLow \nHigh \nCal \nSph \nMar \nLob \nSpic \nTex \n0\n50\n100\n150\n200\n250\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n#  of  Nodules \nMalignancy Scores \n(g)  \n(A) Lung nodule attributes\nInput volume \n3D Convolution \nMax pooling \nFully Connected layer \n4096-D \nAttributes: \n \n‚Ä¢ Malignant? ‚Äì Y \n \n‚Ä¢ Calcified? ‚Äì N \n \n‚Ä¢ Lobulated? ‚Äì Y \n \n‚Ä¢ Spherical? ‚Äì Y \n \n‚Ä¢ Spiculated? ‚Äì Y \n \n‚Ä¢ Margin? ‚Äì Y \n \n‚Ä¢ Texture ? ‚Äì Y \n3D Convolution \nMax pooling \nFully Connected layer \n4096-D \n3DCNN for attribute  1 \n3D CNN for attribute ‚ÄòM‚Äô \nGraph Regularized Sparse \nRepresentation \nMalignancy \nscore \n \nMulti-Task Learning \n(B) An overview of the proposed supervised approach\nFigure 2: (A) A visualization of lung nodules having different levels of attributes. On moving from the top (attribute absent)\nto the bottom (attribute prominently visible), the prominence level of the attribute increase. Different attributes including\ncalciÔ¨Åcation, sphericity, margin, lobulation, spiculation and texture can be seen in (a-f). The graph in (g) depicts the number\nof nodules with different malignancy levels in our experiments using the publicly available dataset [32]. An overview of the\nproposed 3D CNN based graph regularized sparse MTL approach is presented in (B).\nIn the above equation, the sparsity level of the coefÔ¨Åcient\nvector W = [w1, w2 . . . wd] is controlled by a parameter t.\nSince the function in Eq. (1) is convex and the constraints\ndeÔ¨Åne a convex set, a local minimizer of the objective function\nis subjected to constraints corresponding to a global mini-\nmizer. In the following subsections, we extend this supervised\nlearning setting with deep learning and MTL concepts to\ncharacterize lung nodules as benign or malignant.\nB. 3D Convolution Neural Network (CNN) and Fine-Tuning\nWe use 3D CNN [33] trained on Sports-1M dataset [34]\nand Ô¨Åne-tune it on the lung nodule CT dataset. The Sports-\n1M dataset consists of 487 classes with 1 million videos.\nAs the lung nodule dataset doesn‚Äôt have a large number of\ntraining examples, Ô¨Åne-tuning is done to acquire dense feature\nrepresentation from the Sports-1M. The 3D CNN architecture\nconsists of 5 sets of convolution, 2 fully-connected and 1 soft-\nmax classiÔ¨Åcation layers. Each convolution set is followed by\na max-pooling layer. The input to the 3D CNN comprises\ndimensions of 128x171x16, where 16 denotes the number of\nslices. Note that the images in the dataset are resized to have\nconsistent dimensions such that the number of channels is 3\nand the number of slices is Ô¨Åxed to 16. Hence, the overall input\ndimension can be considered as 3x16x128x171. The number\nof Ô¨Ålters in the Ô¨Årst 3 convolution layers are 64, 128 and 256\nrespectively, whereas there are 512 Ô¨Ålters in the last 2 layers.\nThe fully-connected layers have a dimension 4096 which is\nalso the length of feature vectors used as an input to the MTL\nframework. Implementation details are mentioned in section\nV-C.\nC. Multi-task learning (MTL)\nMulti-task learning is an approach of learning multiple tasks\nsimultaneously while considering disparities and similarities\nacross those tasks. Given M tasks, the goal is to improve\nthe learning of a model for task i, (i ‚ààM) by using the\ninformation contained in the M tasks. We formulate the\nmalignancy prediction of lung nodules as an MTL problem,\nwhere visual attributes of lung nodules are considered as\ndistinct tasks (Figure 2A). In a typical MTL problem, ini-\ntially, the correlation between M tasks and the shared feature\nrepresentations are not known. The aim in the MTL approach\nis to learn a joint model while exploiting the dependencies\namong visual attributes (tasks) in feature space. In other words,\nwe utilize visual attributes and exploit their feature level\ndependencies so as to improve regressing malignancy using\nother attributes.\nAs shown in Figure 2B, we design lung tumor characteriza-\ntion as an MTL problem, where each task has model param-\neters Wi, which are utilized to characterize the corresponding\ntask i. When W = [W1, W2 . . . WM] ‚ààRd√óM constitutes\na rectangular matrix, rank can be considered as a natural\nextension to cardinality, and nuclear/trace norm leads to low\nrank solutions. In some cases nuclear norm regularization can\nbe considered as the ‚Ñì1-norm of the singular values [35]. Trace\nnorm, the sum of singular values, is the convex envelope of\nthe rank of a matrix (which is non-convex), where the matrices\nare considered on a unit ball. After substituting, ‚Ñì1-norm by\ntrace norm, the least square loss function with trace norm\nregularization can be formulated as:\nmin\nW\nM\nX\ni=1\n‚à•XiWi ‚àíYi‚à•2\n2 + œÅ ‚à•W‚à•‚àó,\n(2)\nwhere œÅ adjusts the rank of the matrix W, and ‚à•W‚à•‚àó=\nP\ni=1 œÉi(W) is the trace-norm where œÉ denotes singular\nvalues. However, as in trace-norm, the assumption about\nmodels sharing a common subspace is restrictive for some\napplications.\nAs the task relationships are often unknown and are learned\nfrom data, we represent tasks and their relations in the form\n5\nof a graph. Let Œ• = (V, E) represent a complete graph in\nwhich nodes V correspond to the tasks and the edges E model\nany afÔ¨Ånity between the tasks. In such case, a regularization\ncan be applied on the graph modeling task dependencies [36].\nThe complete graph can be modeled as a structure matrix S =\n[e1, e2 . . . e‚à•E‚à•] ‚ààRM√ó‚à•E‚à•where the deviation between the\npairs of tasks can be regularized as:\n‚à•WS‚à•2\nF =\n‚à•E‚à•\nX\ni=1\n\r\rWei\r\r2\n2 =\n‚à•E‚à•\nX\ni=1\n\r\r\rWeia ‚àíWei\nb\n\r\r\r\n2\n2 ,\n(3)\nhere, ei\na, ei\nb are the edges between the nodes a and b, and\nei ‚ààRM. The matrix S deÔ¨Ånes an incidence matrix where ei\na\nand ei\nb are assigned to 1 and -1, respectively, if nodes a and\nb are connected in the graph. Eq. (3) can be further explained\nas:\n‚à•WS‚à•2\nF = tr((WS)T (WS)) = tr(WSST WT ) = tr(WLWT ),\n(4)\nwhere L = SST is the Laplacian matrix and ‚Äòtr‚Äô represents\nthe trace of a matrix. The method to compute structure matrix\nS is discussed in Section V-C.\nThe malignancy prediction equation can be further regu-\nlarized because there are still other uncertainties to consider,\ni.e., disagreement between radiologists‚Äô visual interpretations\nfor a given nodule. For instance, while one radiologist may\ngive a malignancy score of xj\n1 for a nodule j, the other may\ngive a score of xj\n2 for the same nodule. In order to reÔ¨Çect\nthese uncertainties in our algorithm, we formulate a scoring\nfunction which models such inconsistencies:\nŒ®(j) =\n\u0012\nexp(‚àíP\nr(xj\nr ‚àí¬µj)2\n2œÉj\n)\n\u0013‚àí1\n.\n(5)\nFor a particular example j, this inconsistency measure can\nbe represented as Œ®(j). xj\nr is the score given by the rth\nradiologist (expert) whereas ¬µj and œÉj represent mean and\nstandard deviation of the scores, respectively. We calculate\nthis inconsistency score for all the tasks under consideration\nand for simplicity we have omitted the index for the tasks. The\nÔ¨Ånal objective function of graph regularized sparse least square\noptimization with the inconsistency measure can expressed as:\nmin\nW\nM\nX\ni=1\n1‚Éù\nz\n}|\n{\n‚à•(Xi + Œ®i)Wi ‚àíYi‚à•2\n2 +\n2‚Éù\nz\n}|\n{\nœÅ1 ‚à•WS‚à•2\nF +\n3‚Éù\nz\n}|\n{\nœÅ2 ‚à•W‚à•1,\n(6)\nwhere œÅ1 tunes the penalty degree for graph structure and\nœÅ2 handles the sparsity level. In Eq. (6), the least square\nloss function 1‚Éùobserves decoupling of tasks whereas 2‚Éù\nand 3‚Éùmodel their interdependencies, so as to learn joint\nrepresentation.\nD. Optimization\nIn order to solve Eq. (6), the conventional approach is to\nuse standard gradient descent as an optimization algorithm.\nHowever, standard gradient descent cannot be applied here\nbecause the ‚Ñì1‚àínorm is not differentiable at W = 0 and gra-\ndient descent approach fails to provide sparse solutions [37].\nSince the optimization function in the above equation has\nboth smooth and non-smooth convex parts, it can be solved\nafter replacing the non-smooth part with its estimates. In\nother words, the ‚Ñì1-norm in the above equation is the non-\nsmooth part and the proximal operator can be used for its\nestimation. For this purpose, we utilize accelerated proximal\ngradient method [38], the Ô¨Årst order gradient method having a\nconvergence rate of O(1/m2), where m controls the number\nof iterations.\nIV. UNSUPERVISED LEARNING METHODS\nSince annotating medical images is laborious, expensive\nand time-consuming, in the second part of this paper, we\nexplore the potential of unsupervised learning approaches for\ntumor characterization problems. As illustrated in Figure 3, our\nproposed unsupervised framework includes three steps. First,\nwe perform clustering on the appearance features obtained\nfrom the images to estimate an initial set of labels. Then,\nusing the obtained initial labels, we compute label proportions\ncorresponding to each cluster. Finally, we use the initial cluster\nassignments and label proportions to learn the categorization\nof tumors.\nA. Initial Label Estimation\nLet X = [x1, x2 . . . xn]T ‚ààRn√ód represent the input matrix\nwhich contains features from n images such that x ‚ààRd.\nWe then cluster the data into 2 ‚â§k < n clusters using k-\nmeans algorithm. Let A represent |X| √ó k assignment matrix\nwhich denotes the membership assignment of each sample to a\ncluster. The optimal clustering would minimize the following\nobjective function:\nargmin\n¬µv,A\nk\nX\nv=1\nA(u, v) ‚à•xu ‚àí¬µv‚à•2 ,\ns.t. A(u, v) = 0 ‚à®1,\nX\nv\nA(u, v) = 1\n(7)\nwhere ¬µv is the mean of the samples in cluster v. The\nassignment matrix A can then be used to estimate labels c.\nThese labels are only used for estimating label proportions\nof the clustered data for the purpose of training a new algo-\nrithm which we adapt for our problem, i.e., proportion-SVM\n(‚àùSVM). The rationale behind this proportion comes from\nthe clustering notion where data is divided into groups/clusters\nand each cluster corresponds to a particular class. In our work,\nspeciÔ¨Åcally, clustering is only an initial step to estimate cluster\nassignments that are progressively reÔ¨Åned in the subsequent\nsteps.\nB. Learning with the Estimated Labels\nSince our initial label estimation approach is unsupervised,\nthere are uncertainties associated with them. It is, therefore,\nreasonable to assume that learning a discriminative model\nbased on these noisy instance level labels can deteriorate\nclassiÔ¨Åcation performance. In order to address this issue, we\n6\nCluster 1\np1\nCluster 2\np2\nLabel Proportions\nInput Images\n(Lung nodules/IPMN)\nClustering\nInitial Labels\nŒ±SVM\nFinal Classification\nMalignant/IPMN\nBenign/Normal\nFeature \nExtraction\nùíëùíó= ùíè‚àíùüè‡∑ç\nùíñ=ùüè\nùíè\nùë∞(ùíöùíñ= ùíó)\nStep 1\nStep 2\nStep 3\nFigure 3: An outline of the proposed unsupervised approach. Given the input images, we compute GIST features and perform\nk-means clustering to get the initial set of labels which can be noisy. Using the set of labels, we compute label proportions\ncorresponding to each cluster/group (Eq. (9)). We Ô¨Ånally employ ‚àùSVM to learn a discriminative model using the features\nand label proportions.\nTABLE I: List and details of different experiments performed for supervised and unsupervised learning along with their\nevaluation sets.\nExperiments\nDetails\nEvaluation Set\nE1\nSupervised learning, 3D CNN based\nMulti-task learning with attributes,\nÔ¨Åne-tuning (C3D) network\n3D dataset:\nMalignancy score\nregression of Lung\nnodules (CT)\nE2\nUnsupervised learning, GIST features,\nProportion-SVM\n2D dataset:\nLung nodules\n(CT) and IPMN\nclassiÔ¨Åcation\n(MRI)\nE3\nUnsupervised learning, features from\ndifferent layers of 2D VGG network\nE4\nSupervised learning to establish\nclassiÔ¨Åcation upper-bound, GIST and VGG\nfeatures with SVM and RF\nmodel the instance level labels as latent variables and thereby\nconsider group/bag level labels.\nInspired by ‚àùSVM approach [39], which models the latent\ninstance level variables using the known group level label\nproportions, we formulate our learning problem such that\nclusters are analogous to the groups. In our formulation, each\ncluster v can be represented as a group such that the majority\nof samples belong to the class v. Considering the groups to\nbe disjoint such that Sk\nv=1‚Ñ¶v= 1, 2, . . . n , and ‚Ñ¶represents\ngroups; the objective function of the large-margin ‚àùSVM after\nconvex relaxation can be formulated as:\nmin\nc‚ààC min\nw\n \n1\n2wT w + K\nn\nX\nu=1\nL(cu, wT œÜ(x))\n!\nC =\n\u001a\nc\n\f\f\f\f | epv(c) ‚àípv| ‚â§œµ, cu ‚àà{‚àí1, 1} ‚àÄk\nv=1\n\u001b\n,\n(8)\nwhere ep and p represent the estimated and true label propor-\ntions, respectively. In Eq. (8), c is the set of instance level\nlabels, œÜ(.) is the input feature, K denotes cost parameter and\nL(.) represents the hinge-loss function for maximum-margin\nclassiÔ¨Åers such as SVM. An alternative approach based on\ntraining a standard SVM classiÔ¨Åer with clustering assignments\nis discussed in Section V-D.\nThe optimization in Eq. (8) is, in fact, an instance of\nMultiple Kernel Learning, which can be solved using the\ncutting plane method where the set of active constraints is\nincrementally computed. The goal is to Ô¨Ånd the most violated\nconstraint, however, the objective function still decreases even\nby further relaxation and aiming for any violated constraint.\nFurther details about optimization can be studied in [39].\nC. Calculating Label Proportions\nIn the conventional ‚àùSVM approach, the label proportions\nare known a priori. Since our approach is unsupervised, both\ninstance level labels and group label proportions are unknown.\nMoreover, establishing strong assumptions about the label\nproportions may affect learning. It is, however, reasonable to\nassume that a large number of instances in any group carry\nthe same label and there may be a small number of instances\nwhich are outliers. The label proportions serve as a soft-label\nfor a bag where a bag can be considered as a super-instance.\nIn order to determine the label proportions in a data-driven\nmanner, we use the estimated labels obtained from clustering.\nThe label proportion pv corresponding to the group v can be\nrepresented as:\npv = n‚àí1\nn\nX\nu=1\nI(yu = v),\n(9)\nwhere I(.) is the indicator function which yields 1 when\nyu = v. The ‚àùSVM is trained using the image features and\nlabel proportions to classify the testing data. It is important to\nmention that the ground truth labels (benign/malignant labels)\nare used only to evaluate the proposed framework and are\n7\nNormal Pancreas \nPancreas with IPMN \nFigure 4: Axial T2 MRI scans illustrating pancreas. The top row shows different ROIs of pancreas, along with a magniÔ¨Åed\nview of a normal pancreas (outlined in blue). The bottom row shows ROIs from subjects with IPMN in the pancreas, which\nis outlined in red.\nnot used in estimating label proportions or training of the\nproportion-SVM. In addition, clustering and label proportion\ncalculation are only performed on the training data and the\ntesting data remains completely unseen for ‚àùSVM. The num-\nber of clusters is Ô¨Åxed as 2, i.e. benign and malignant classes\nand the result was checked to assign benign and malignant\nlabels to the clusters.\nV. EXPERIMENTS\nA. Data for Lung Nodules\nFor test and evaluation, we used LIDC-IDRI dataset from\nLung Image Database Consortium [32], which is one of the\nlargest publicly available lung nodule dataset. The dataset\ncomprises 1018 CT scans with a slice thickness varying from\n0.45 mm to 5.0 mm. At most four radiologists annotated those\nlung nodules which have diameters equal to or greater than 3.0\nmm.\nWe considered nodules which were interpreted by at least\nthree radiologists for evaluations. The number of nodules\nfulÔ¨Ålling this criterion was 1340. As a nodule may have\ndifferent malignancy and attribute scores provided by different\nradiologists, their mean scores were used. The nodules have\nscores corresponding to these six attributes: (i) calciÔ¨Åcation,\n(ii) lobulation, (iii) spiculation, (iv) sphericity, (v) margin and\n(vi) texture as well as malignancy (Figure 2). The malignancy\nscores ranged from 1 to 5 where 1 denoted benign and 5\nmeant highly malignant nodules. To account for malignancy\nindecision among radiologists, we excluded nodules with\na mean score of 3. The Ô¨Ånal evaluation set included 509\nmalignant and 635 benign nodules. As a pre-processing step,\nthe images were resampled to be isotropic so as to have 0.5\nmm spacing in each dimension.\nB. Data for IPMN\nThe data for the classiÔ¨Åcation of IPMN contains T2 MRI\naxial scans from 171 subjects. The scans were labeled by a\nradiologist as normal or IPMN. Out of 171 scans, 38 subjects\nwere normal, whereas the rest of 133 were from subjects\ndiagnosed with IPMN. The in-plane spacing (xy-plane) of\nthe scan was ranging from 0.468 mm to 1.406 mm. As pre-\nprocessing, we Ô¨Årst employ N4 bias Ô¨Åeld correction [40] to\neach image in order to normalize variations in image intensity.\nWe then apply curvature anisotropic image Ô¨Ålter to smooth\nimage while preserving edges. For experiments, 2D axial slices\nwith pancreas (and IPMN) are cropped to generate Region\nof Interest (ROI) as shown in Figure 4. The large intra-class\nvariation, especially due to varying shapes of the pancreas can\nalso be observed in Figure 4. A list of different supervised and\nunsupervised learning experiments along with their evaluation\nsets is tabulated in Table I.\nC. Evaluation and Results- Supervised Learning\nWe Ô¨Åne-tuned the 3D CNN network trained on Sports-\n1M dataset [34] which had 487 classes. In order to train\nthe network with binary labels for malignancy and the six\nattributes we used the mid-point as a pivot and labeled samples\nas positive (or negative) based on their scores being greater\n(or lesser) than the pivot. In our context, malignancy and\nattributes are characterized as tasks. The C3D was Ô¨Åne-tuned\nwith these 7 tasks and 10 fold cross-validation was conducted.\nThe requirement to have a large amount of labeled training\ndata was evaded by Ô¨Åne-tuning the network. Since the input\nto the network required 3 channel image sequences with at\nleast 16 slices, we concatenated the gray level axial channel\nas the other two channels.\nAdditionally, in order to ascertain that all input vol-\numes have 16 slices, we performed interpolation where war-\nranted. The Ô¨Ånal feature representation was obtained from the\n8\nTABLE II: The comparison of the proposed approach with other methods using regression accuracy and mean absolute score\ndifference for lung nodule characterization.\nMethods\nAccuracy\nMean Score\n%\nDifference\nGIST features + LASSO\n76.83\n0.675\nGIST features + RR\n76.48\n0.674\n3D CNN features + LASSO (Pre-trained)\n86.02\n0.530\n3D CNN features + RR (Pre-trained)\n82.00\n0.597\n3D CNN features + LASSO (Fine-tuned)\n88.04\n0.497\n3D CNN features + RR (Fine-tuned)\n84.53\n0.550\n3D CNN MTL with Trace norm\n80.08\n0.626\nProposed (3D CNN with Multi-task Learning- Eq. 7)\n91.26\n0.459\nÔ¨Årst fully connected layer of 3D CNN consisting of 4096-\ndimensions.\nFor computing structure matrix S, we calculate the corre-\nlation between different tasks by estimating the normalized\ncoefÔ¨Åcient matrix W via least square loss function with lasso\nfollowed by the calculation of correlation coefÔ¨Åcient matrix\n[36]. In order to get a binary graph structure matrix, we\nthresholded the correlation coefÔ¨Åcient matrix. As priors in\nEq. (6) we used œÅ1 and œÅ2 as 1 and 10 respectively. Finally, to\nobtain the malignancy score for test images, the features from\nthe network trained on malignancy were multiplied with the\ncorresponding task coefÔ¨Åcient vector W.\nWe evaluated our proposed approach using both classiÔ¨Å-\ncation and regression metrics. For classiÔ¨Åcation, we consid-\nered a nodule to be successfully classiÔ¨Åed if its predicted\nscore lies in ¬±1 of the ground truth score. For regression,\nwe calculated average absolute score difference between the\npredicted score and the true score. The comparison of our\nproposed MTL approach with approaches including GIST\nfeatures [41], 3D CNN features from pre-trained network +\nLASSO, Ridge Regression (RR) and 3D CNN MTL+trace\nnorm is tabulated in Table II. It can be observed that our\nproposed graph regularized MTL performs signiÔ¨Åcantly better\nthan other approaches both in terms of classiÔ¨Åcation accuracy\nas well as the mean score difference. The gain in classiÔ¨Åcation\naccuracy was found to be 15% and 11% for GIST and trace-\nnorm respectively. In comparison with the pre-trained network,\nwe obtain an improvement of 5% with proposed MTL. In\naddition, our proposed approach reduces the average absolute\nscore difference for GIST by 32% and for trace-norm by 27%.\nD. Evaluations and Results- Unsupervised Learning\nFor unsupervised learning, evaluations were performed on\nboth lung nodules and IPMN datasets. In order to compute\nimage level features, we used GIST descriptors [41]. The\nnumber of clusters is Ô¨Åxed as 2, which accounts for benign\nand malignant classes. The clustering result was checked to\nassign benign and malignant labels to the clusters. We used 10\nfold cross-validation to evaluate our proposed approach. The\ntraining samples along with the label proportions generated\nusing clustering served as the input to ‚àùSVM with a linear\nkernel.\nTo evaluate our unsupervised approach we used accuracy,\nsensitivity and speciÔ¨Åcity as metrics. It can be observed\nin Table III that the proposed combination of clustering\nand ‚àùSVM signiÔ¨Åcantly outperforms other approaches in\naccuracy and sensitivity. In comparison with clustering+SVM,\nthe proposed framework yields almost 21% improvement in\nsensitivity for lung nodules and around 7% improvement for\nIPMN classiÔ¨Åcation. The low sensitivity and high speciÔ¨Åcity\nof clustering, clustering+SVM, and clustering+RF approaches\ncan be explained by disproportionate assignment of instances\nas benign (normal) by these approaches, which is not found\nin the proposed approach. At the same time, the proposed\napproach records around 24% and 9% improvement in\naccuracy as compared to clustering for lung nodules and\nIPMN, respectively.\nAre Deep Features good for Unsupervised ClassiÔ¨Åcation?\nGiven the success of deep learning features for image classiÔ¨Å-\ncation and their popularity with the medical imaging commu-\nnity, we explored their performance to classify lung nodules\nand IPMN in an unsupervised manner. For this purpose, we\nused a pre-trained deep CNN architecture to extract features\nand then perform clustering to obtain baseline classiÔ¨Åcation\nperformance. We extracted features from fully-connected lay-\ners 7 and 8 of Fast-VGG [42] with and without applying\nReLU non-linearity. ClassiÔ¨Åcation accuracy, using clustering\nover these features is shown in Figure 5.\nIt can be seen in Figure 5 that the features with non-\nlinearity (ReLU) are more discriminative for classiÔ¨Åcation\nusing clustering as compared to without ReLU. The same\ntrend can be observed for both lung nodules and IPMN\nclassiÔ¨Åcation using VGG-fc7 and VGG-fc8 layers. Owing\nto the larger evaluation set, the inÔ¨Çuence of ReLU is\nmore prominent for lung nodules as compared to IPMN.\nAlthough the results between VGG-fc7 and VGG-fc8 are\nnot substantially different, the highest accuracy for IPMN\ncan be obtained by using VGG-fc7-ReLU features and\nfor lung nodules by using VGG-fc8-ReLU features. The\nnon-linearity induced by ReLU clips the negative values to\n9\nTABLE III: Average classiÔ¨Åcation accuracy, sensitivity, and speciÔ¨Åcity of the proposed unsupervised approach for IPMN and\nlung nodule classiÔ¨Åcation with other methods\nEvaluation Set\nMethods\nAccuracy Sensitivity SpeciÔ¨Åcity\nIPMN\nClassiÔ¨Åcation\nClustering\n49.18%\n45.34%\n62.83%\nClustering + RF\n53.20%\n51.28%\n69.33%\nClustering + SVM\n52.03%\n51.96%\n50.5%\nProposed approach\n58.04%\n58.61%\n41.67%\nLung Nodule\nClassiÔ¨Åcation\nClustering\n54.83%\n48.69%\n60.04%\nClustering + RF\n76.74%\n58.59%\n91.40%\nClustering + SVM\n76.04%\n57.08%\n91.28%\nProposed approach\n78.06%\n77.85%\n78.28%\n0\n10\n20\n30\n40\n50\n60\n70\nVGG-fc7\nVGG-fc7-ReLU\nVGG-fc8\nVGG-fc8-ReLU\n47.11 \n54.3 \n52.99 \n55.42 \nPercentage (%) \nUnsupervised Classification of Lung nodules using various \nDeep Features \nAccuracy\nSensitivity\nSpecificity\n0\n10\n20\n30\n40\n50\n60\nVGG-fc7\nVGG-fc7-ReLU\nVGG-fc8\nVGG-fc8-ReLU\n49.12 \n51.47 \n50.36 \n50.92 \nPercentage (%) \nUnsupervised Classification of IPMN using various  \nDeep Features \nAccuracy\nSensitivity\nSpecificity\nFigure 5: InÔ¨Çuence of deep learning features obtained from different layers of a VGG network with and without ReLU\nnon-linearities. The graph on the left shows accuracy, sensitivity and speciÔ¨Åcity for unsupervised lung nodule classiÔ¨Åcation\n(clustering), whereas the right one shows the corresponding results for IPMN.\nzero, which can sparsify the feature vector and can reduce\noverÔ¨Åtting. Additionally, it can be seen that GIST features\nyield comparable performance than deep features (Table III).\nThis can be explained by the fact that the deep networks were\ntrained on ImageNet dataset so the Ô¨Ålters in the networks were\nmore tuned to the variations in natural images than medical\nimages. ClassiÔ¨Åcation improvement can be expected with\nunsupervised feature learning techniques such as GANs [30].\nClassiÔ¨Åcation using Supervised Learning\nIn order to establish the upper-bound on the classiÔ¨Åcation\nperformance, we trained linear SVM and Random Forest\nusing GIST and different deep learning features with ground\ntruth labels on the same 10 fold cross-validations sets.\nTable IV lists the classiÔ¨Åcation accuracy, sensitivity, and\nspeciÔ¨Åcity using GIST, VGG-fc7 and VGG-fc8 features\nfor both IPMN and lung nodules. For both VGG-fc7 and\nVGG-fc8, we used features after ReLU since they are found\nto be more discriminative (Figure 5). Interestingly, for lung\nnodules, VGG-fc7 features along with RF classiÔ¨Åer are\nreported to have comparable results to the combination of\nGIST and RF classiÔ¨Åer. This can be explained by the fact\nthat deep networks are pre-trained on ImageNet dataset as\ncompared to handcrafted features such as GIST, which don‚Äôt\nrequire any training. On the other hand, for smaller datasets\nsuch as IPMN, deep features are found to perform better\nas compared to GIST. In order to balance the number of\npositive (IPMN) and negative (normal) examples, which can\nbe a critical drawback otherwise, we performed Adaptive\nSynthetic Sampling [43]. This was done to generate synthetic\nexamples in terms of features from the minority class (normal).\nVI. DISCUSSION AND CONCLUDING REMARKS\nIn this study, we present a framework for the malignancy\ndetermination of lung nodules with 3D CNN based graph\nregularized sparse MTL. To the best of our knowledge, this\nis the Ô¨Årst work where MTL and transfer learning are studied\nfor 3D deep networks to improve risk stratiÔ¨Åcation of lung\nnodules. Usually, the data sharing for medical imaging is\nhighly regulated and the accessibility of experts (radiologists)\nto label these images is limited. As a consequence, the access\nto the crowdsourced and publicly gathered and annotated data\nsuch as videos may help in obtaining discriminative features\nfor medical image analysis.\nWe also analyzed the signiÔ¨Åcance of different imaging\nattributes corresponding to lung nodules including spiculation,\ntexture, calciÔ¨Åcation and others for risk assessment. Instead\n10\nTABLE IV: ClassiÔ¨Åcation of IPMN and Lung Nodules using different features and supervised learning classiÔ¨Åers.\nEvaluation Set\nFeatures\nClassiÔ¨Åers\nAccuracy (%)\nSensitivity (%)\nSpeciÔ¨Åcity (%)\nIPMN\nClassiÔ¨Åcation\nGIST\nSVM\n76.05\n83.65\n52.67\nRF\n81.9\n93.69\n43.0\nVGG-fc7\nSVM\n84.18\n96.91\n44.83\nRF\n81.96\n94.61\n42.83\nVGG-fc8\nSVM\n84.22\n97.2\n46.5\nRF\n80.82\n93.4\n45.67\nLung Nodule\nClassiÔ¨Åcation\nGIST\nSVM\n81.56\n71.31\n90.02\nRF\n81.64\n76.47\n85.97\nVGG-fc7\nSVM\n77.97\n75.2\n80.6\nRF\n81.73\n78.24\n84.59\nVGG-fc8\nSVM\n78.76\n74.67\n82.29\nRF\n80.51\n76.03\n84.24\nof manually modeling these attributes we utilized 3D CNN\nto learn rich feature representations associated with these\nattributes. The graph regularized sparse MTL framework was\nemployed to integrate 3D CNN features from these attributes.\nWe have found the features associated with these attributes\ncomplementary to those corresponding to malignancy.\nIn the second part of this study, we explored the potential\nof unsupervised learning for malignancy determination. Since\nin most medical imaging tasks radiologists are required to\nget annotations, acquiring labels to learn machine learning\nmodels is more cumbersome and expensive as compared to\nother computer vision tasks. In order to address this challenge,\nwe employed clustering to obtain an initial set of labels\nand progressively reÔ¨Åned them with ‚àùSVM. We obtained\npromising results and our proposed approach outperformed\nthe other methods in evaluation metrics.\nFollowing up on the application of deep learning for almost\nall tasks in the visual domain, we studied the inÔ¨Çuence of dif-\nferent pre-trained deep networks for lung nodule classiÔ¨Åcation.\nFor some instances, we found that commonly used imaging\nfeatures such as GIST have comparable results as those\nobtained from pre-trained network features. This observation\ncan be explained by the fact that the deep networks were\ntrained on ImageNet classiÔ¨Åcation tasks so the Ô¨Ålters in CNN\nwere more tuned to the nuances in natural images as compared\nto medical images.\nTo the best of our knowledge, this is one of the Ô¨Årst and the\nlargest evaluation of a CAD system for IPMN classiÔ¨Åcation.\nCAD systems for IPMN classiÔ¨Åcation are relatively newer\nresearch problems and there is a need to explore the use of dif-\nferent imaging modalities to improve classiÔ¨Åcation. Although\nMRI remains the most common modality to study pancreatic\ncysts, CT images can also be used as a complementary imaging\nmodality due to its higher resolution and its ability to capture\nsmaller cysts. Additionally, a combination of T2-weighted,\ncontrast-enhanced and unenhanced T1-weighted sequences can\nhelp improve detection and diagnosis of IPMN [44]. In this\nregard, multi-modal deep learning architectures can be deemed\nuseful [45]. The detection and segmentation of pancreas can\nalso be useful to make a better prediction about the presence\nof IPMN and cysts. Due to its anatomy, the pancreas is a\nchallenging organ to segment, particularly in MRI images.\nTo address this challenge, other imaging modalities can be\nutilized for joint segmentation and diagnosis of pancreatic\ncysts and IPMN. Furthermore, visualization of activation maps\ncan be quite useful for the clinicians to identify new imaging\nbiomarkers that can be employed for diagnosis in the future.\nThe future prospects of using different architectures to\nperform unsupervised representation learning using GAN are\npromising. Instead of using hand-engineered priors of sam-\npling in the generator, the work in [46] learned priors using\ndenoising auto-encoders. For measuring the sample similarity\nfor complex distributions such as those in the images, [47]\njointly trained variational autoencoders and GANs. Moreover,\nthe applications of CatGAN [48] and InfoGAN [49] for semi-\nsupervised and unsupervised classiÔ¨Åcation tasks in medical\nimaging are worth exploring as well.\nMedical imaging has unique challenges associated with the\nscarcity of labeled examples. Moreover, unless corroborated\nby biopsy, there may exist a large variability in labeling\nfrom different radiologists. Although Ô¨Åne-tuning has helped\nto address the lack of annotated examples, the performance is\nlimited due to large differences in domains. It is comparatively\neasier to obtain scan level labels than slice level labels. In\nthis regard, weakly supervised approaches such as multiple\ninstance learning (MIL) can be of great value. Active learning\ncan be another solution to alleviate the difÔ¨Åculty in labeling. In\naddition to these directions, unsupervised learning approaches\nwill surely be pursued to address unique medical imaging\nchallenges.\nREFERENCES\n[1] Society, A.C.: Cancer Facts & Figures. American Cancer Society (2016)\n[2] Sadot, E., Basturk, O., Klimstra, D.S., G¬®onen, M., Anna, L., Do, R.K.G.,\nDAngelica, M.I., DeMatteo, R.P., Kingham, T.P., Jarnagin, W.R., et al.:\nTumor-associated neutrophils and malignant progression in intraductal\npapillary mucinous neoplasms: an opportunity for identiÔ¨Åcation of high-\nrisk disease. Annals of surgery 262(6), 1102 (2015)\n[3] El-Baz, A., Nitzken, M., Khalifa, F., Elnakib, A., Gimelfarb, G., Falk,\nR., El-Ghar, M.A.: 3D shape analysis for early diagnosis of malignant\nlung nodules. In: IPMI. pp. 772‚Äì783. Springer (2011)\n[4] Han, F., Wang, H., Zhang, G., Han, H., Song, B., Li, L., Moore, W.,\nLu, H., Zhao, H., Liang, Z.: Texture feature analysis for computer-aided\ndiagnosis on pulmonary nodules. Journal of Digital Imaging 28(1), 99‚Äì\n115 (2015)\n11\n[5] Way, T.W., Hadjiiski, L.M., Sahiner, B., Chan, H.P., Cascade, P.N.,\nKazerooni, E.A., Bogot, N., Zhou, C.: Computer-aided diagnosis of\npulmonary nodules on CT scans: segmentation and classiÔ¨Åcation using\n3D active contours. Medical Physics 33(7), 2323‚Äì2337 (2006)\n[6] Lee, M., Boroczky, L., Sungur-Stasik, K., Cann, A., Borczuk, A.,\nKawut, S., Powell, C.: Computer-aided diagnosis of pulmonary nodules\nusing a two-step approach for feature selection and classiÔ¨Åer ensemble\nconstruction. ArtiÔ¨Åcial Intelligence in Medicine 50(1), 43‚Äì53 (2010)\n[7] Kumar, D., Wong, A., Clausi, D.A.: Lung nodule classiÔ¨Åcation using\ndeep features in CT images. In: Computer and Robot Vision (CRV),\n2015 12th Conference on. pp. 133‚Äì138. IEEE (2015)\n[8] Buty, M., Xu, Z., Gao, M., Bagci, U., Wu, A., Mollura, D.J.: Characteri-\nzation of Lung Nodule Malignancy Using Hybrid Shape and Appearance\nFeatures. In: MICCAI. pp. 662‚Äì670. Springer (2016)\n[9] Saouli, R., Akil, M., Kachouri, R., et al.: Fully automatic brain tumor\nsegmentation using end-to-end incremental deep neural networks in mri\nimages. Computer methods and programs in biomedicine 166, 39‚Äì49\n(2018)\n[10] Hussein, S., Cao, K., Song, Q., Bagci, U.: Risk StratiÔ¨Åcation of Lung\nNodules Using 3D CNN-Based Multi-task Learning. In: International\nConference on Information Processing in Medical Imaging. pp. 249‚Äì\n260. Springer (2017)\n[11] Furuya, K., Murayama, S., Soeda, H., Murakami, J., Ichinose, Y.,\nYauuchi, H., Katsuda, Y., Koga, M., Masuda, K.: New classiÔ¨Åcation\nof small pulmonary nodules by margin characteristics on highresolution\nCT. Acta Radiologica 40(5), 496‚Äì504 (1999)\n[12] Uchiyama, Y., Katsuragawa, S., Abe, H., Shiraishi, J., Li, F., Li, Q.,\nZhang, C.T., Suzuki, K., Doi, K.: Quantitative computerized analysis of\ndiffuse lung disease in high-resolution computed tomography. Medical\nPhysics 30(9), 2440‚Äì2454 (2003)\n[13] Chen, S., Ni, D., Qin, J., Lei, B., Wang, T., Cheng, J.Z.: Bridging\ncomputational features toward multiple semantic features with multi-\ntask regression: A study of CT pulmonary nodules. In: MICCAI. pp.\n53‚Äì60. Springer (2016)\n[14] Shen, W., Zhou, M., Yang, F., Yang, C., Tian, J.: Multi-scale convo-\nlutional neural networks for lung nodule classiÔ¨Åcation. In: IPMI. pp.\n588‚Äì599. Springer (2015)\n[15] Ciompi, F., Chung, K., Van Riel, S.J., Setio, A.A.A., Gerke, P.K., Jacobs,\nC., Scholten, E.T., Schaefer-Prokop, C., Wille, M.M., Marchian`o, A.,\net al.: Towards automatic pulmonary nodule management in lung cancer\nscreening with deep learning. ScientiÔ¨Åc reports 7, 46479 (2017)\n[16] Setio, A.A.A., Ciompi, F., Litjens, G., Gerke, P., Jacobs, C., van Riel,\nS.J., Wille, M.M.W., Naqibullah, M., S¬¥anchez, C.I., van Ginneken, B.:\nPulmonary nodule detection in CT images: false positive reduction using\nmulti-view convolutional networks. IEEE TMI 35(5), 1160‚Äì1169 (2016)\n[17] Setio, A.A.A., Traverso, A., De Bel, T., Berens, M.S., van den Bogaard,\nC., Cerello, P., Chen, H., Dou, Q., Fantacci, M.E., Geurts, B., et al.:\nValidation, comparison, and combination of algorithms for automatic\ndetection of pulmonary nodules in computed tomography images: the\nluna16 challenge. Medical image analysis 42, 1‚Äì13 (2017)\n[18] Khosravan, N., Bagci, U.: S4ND: Single-Shot Single-Scale Lung Nodule\nDetection. arXiv preprint arXiv:1805.02279 (2018)\n[19] Zhou, Y., Xie, L., Fishman, E.K., Yuille, A.L.: Deep Supervision for\nPancreatic Cyst Segmentation in Abdominal CT Scans. arXiv preprint\narXiv:1706.07346 (2017)\n[20] Cai, J., Lu, L., Zhang, Z., Xing, F., Yang, L., Yin, Q.: Pancreas\nsegmentation in MRI using graph-based decision fusion on convolutional\nneural networks. In: MICCAI. pp. 442‚Äì450. Springer (2016)\n[21] Hanania, A.N., Bantis, L.E., Feng, Z., Wang, H., Tamm, E.P., Katz,\nM.H., Maitra, A., Koay, E.J.: Quantitative imaging to evaluate malignant\npotential of IPMNs. Oncotarget 7(52), 85776 (2016)\n[22] Gazit, L., Chakraborty, J., Attiyeh, M., Langdon-Embry, L., Allen,\nP.J., Do, R.K., Simpson, A.L.: QuantiÔ¨Åcation of CT Images for the\nClassiÔ¨Åcation of High-and Low-Risk Pancreatic Cysts. In: SPIE Medical\nImaging. pp. 101340X‚Äì101340X. International Society for Optics and\nPhotonics (2017)\n[23] Sivic, J., Russell, B.C., Efros, A.A., Zisserman, A., Freeman, W.T.:\nDiscovering objects and their location in images. In: ICCV. vol. 1, pp.\n370‚Äì377. IEEE (2005)\n[24] Kamper, H., Jansen, A., Goldwater, S.: Fully unsupervised small-\nvocabulary speech recognition using a segmental Bayesian model. In:\nInterspeech (2015)\n[25] Lee, H., Pham, P., Largman, Y., Ng, A.Y.: Unsupervised feature learning\nfor audio classiÔ¨Åcation using convolutional deep belief networks. In:\nAdvances in neural information processing systems. pp. 1096‚Äì1104\n(2009)\n[26] Shin, H.C., Orton, M.R., Collins, D.J., Doran, S.J., Leach, M.O.:\nStacked autoencoders for unsupervised feature learning and multiple\norgan detection in a pilot study using 4d patient data. IEEE transactions\non pattern analysis and machine intelligence 35(8), 1930‚Äì1943 (2013)\n[27] Vaidhya, K., Thirunavukkarasu, S., Alex, V., Krishnamurthi, G.: Multi-\nmodal brain tumor segmentation using stacked denoising autoencoders.\nIn: International Workshop on Brainlesion: Glioma, Multiple Sclerosis,\nStroke and Traumatic Brain Injuries. pp. 181‚Äì194. Springer (2015)\n[28] Sivakumar, S., Chandrasekar, C.: Lung nodule segmentation through\nunsupervised clustering models. Procedia engineering 38, 3064‚Äì3073\n(2012)\n[29] Kallenberg, M., Petersen, K., Nielsen, M., Ng, A.Y., Diao, P., Igel,\nC., Vachon, C.M., Holland, K., Winkel, R.R., Karssemeijer, N., et al.:\nUnsupervised deep learning applied to breast density segmentation and\nmammographic risk scoring. IEEE transactions on medical imaging\n35(5), 1322‚Äì1331 (2016)\n[30] Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning\nwith deep convolutional generative adversarial networks. arXiv preprint\narXiv:1511.06434 (2015)\n[31] Zhang, F., Song, Y., Cai, W., Zhou, Y., Fulham, M., Eberl, S., Shan,\nS., Feng, D.: A ranking-based lung nodule image classiÔ¨Åcation method\nusing unlabeled image knowledge. In: IEEE ISBI. pp. 1356‚Äì1359. IEEE\n(2014)\n[32] Armato III, S., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer,\nC.R., Reeves, A.P., Zhao, B., Aberle, D.R., Henschke, C.I., Hoffman,\nE.A., et al.: The Lung Image Database Consortium (LIDC) and Image\nDatabase Resource Initiative (IDRI): a completed reference database of\nlung nodules on CT scans. Medical Physics 38(2), 915‚Äì931 (2011)\n[33] Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning\nspatiotemporal features with 3D convolutional networks. In: ICCV. pp.\n4489‚Äì4497. IEEE (2015)\n[34] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei,\nL.: Large-scale video classiÔ¨Åcation with convolutional neural networks.\nIn: IEEE CVPR. pp. 1725‚Äì1732 (2014)\n[35] Recht, B., Fazel, M., Parrilo, P.A.: Guaranteed minimum-rank solutions\nof linear matrix equations via nuclear norm minimization. SIAM review\n52(3), 471‚Äì501 (2010)\n[36] Zhou, J., Chen, J., Ye, J.: MALSAR: Multi-task learning via structural\nregularization (2012)\n[37] Shalev-Shwartz, S., Tewari, A.: Stochastic methods for l1-regularized\nloss minimization. Journal of Machine Learning Research 12(Jun),\n1865‚Äì1892 (2011)\n[38] Nesterov, Y.: Introductory lectures on convex optimization: A basic\ncourse, vol. 87. Springer Science & Business Media (2013)\n[39] Yu, F., Liu, D., Kumar, S., Tony, J., Chang, S.F.: ‚àùSVM for Learning\nwith Label Proportions. In: Proceedings of The 30th International\nConference on Machine Learning. pp. 504‚Äì512 (2013)\n[40] Tustison, N.J., Avants, B.B., Cook, P.A., Zheng, Y., Egan, A., Yushke-\nvich, P.A., Gee, J.C.: N4ITK: Improved N3 bias correction. IEEE\nTransactions on Medical Imaging 29(6), 1310‚Äì1320 (2010)\n[41] Oliva, A., Torralba, A.: Modeling the shape of the scene: A holistic\nrepresentation of the spatial envelope. IJCV 42(3), 145‚Äì175 (2001)\n[42] ChatÔ¨Åeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the\ndevil in the details: Delving deep into convolutional nets. In: British\nMachine Vision Conference (2014)\n[43] He, H., Bai, Y., Garcia, E.A., Li, S.: ADASYN: Adaptive synthetic\nsampling approach for imbalanced learning. In: Neural Networks, 2008.\nIJCNN 2008.(IEEE World Congress on Computational Intelligence).\nIEEE International Joint Conference on. pp. 1322‚Äì1328. IEEE (2008)\n[44] Kalb, B., Sarmiento, J.M., Kooby, D.A., Adsay, N.V., Martin, D.R.: MR\nimaging of cystic lesions of the pancreas. Radiographics 29(6), 1749‚Äì\n1765 (2009)\n[45] Ma, L., Lu, Z., Shang, L., Li, H.: Multimodal convolutional neural\nnetworks for matching image and sentence. In: IEEE ICCV. pp. 2623‚Äì\n2631 (2015)\n[46] Nguyen, A., Yosinski, J., Bengio, Y., Dosovitskiy, A., Clune, J.: Plug &\nplay generative networks: Conditional iterative generation of images in\nlatent space. arXiv preprint arXiv:1612.00005 (2016)\n[47] Larsen, A.B.L., S√∏nderby, S.K., Larochelle, H., Winther, O.: Autoencod-\ning beyond pixels using a learned similarity metric. In: ICML (2016)\n[48] Springenberg,\nJ.T.:\nUnsupervised\nand\nsemi-supervised\nlearning\nwith\ncategorical\ngenerative\nadversarial\nnetworks.\narXiv\npreprint\narXiv:1511.06390 (2015)\n[49] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel,\nP.: Infogan: Interpretable representation learning by information maxi-\nmizing generative adversarial nets. In: Advances in Neural Information\nProcessing Systems. pp. 2172‚Äì2180 (2016)\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "q-bio.QM",
    "q-bio.TO"
  ],
  "published": "2018-01-10",
  "updated": "2019-01-18"
}