{
  "id": "http://arxiv.org/abs/1801.03230v3",
  "title": "Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches",
  "authors": [
    "Sarfaraz Hussein",
    "Pujan Kandel",
    "Candice W. Bolan",
    "Michael B. Wallace",
    "Ulas Bagci"
  ],
  "abstract": "Risk stratification (characterization) of tumors from radiology images can be\nmore accurate and faster with computer-aided diagnosis (CAD) tools. Tumor\ncharacterization through such tools can also enable non-invasive cancer\nstaging, prognosis, and foster personalized treatment planning as a part of\nprecision medicine. In this study, we propose both supervised and unsupervised\nmachine learning strategies to improve tumor characterization. Our first\napproach is based on supervised learning for which we demonstrate significant\ngains with deep learning algorithms, particularly by utilizing a 3D\nConvolutional Neural Network and Transfer Learning. Motivated by the\nradiologists' interpretations of the scans, we then show how to incorporate\ntask dependent feature representations into a CAD system via a\ngraph-regularized sparse Multi-Task Learning (MTL) framework. In the second\napproach, we explore an unsupervised learning algorithm to address the limited\navailability of labeled training data, a common problem in medical imaging\napplications. Inspired by learning from label proportion (LLP) approaches in\ncomputer vision, we propose to use proportion-SVM for characterizing tumors. We\nalso seek the answer to the fundamental question about the goodness of \"deep\nfeatures\" for unsupervised tumor classification. We evaluate our proposed\nsupervised and unsupervised learning algorithms on two different tumor\ndiagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans,\nrespectively, and obtain the state-of-the-art sensitivity and specificity\nresults in both problems.",
  "text": "Accepted for publication in IEEE Transactions on Medical Imaging 2019\n1\nLung and Pancreatic Tumor Characterization in the\nDeep Learning Era: Novel Supervised and\nUnsupervised Learning Approaches\nSarfaraz Hussein, Pujan Kandel, Candice W. Bolan, Michael B. Wallace, and Ulas Bagci∗, Senior Member, IEEE.\nAbstract—Risk stratiﬁcation (characterization) of tumors from\nradiology images can be more accurate and faster with computer-\naided diagnosis (CAD) tools. Tumor characterization through\nsuch tools can also enable non-invasive cancer staging, prog-\nnosis, and foster personalized treatment planning as a part of\nprecision medicine. In this study, we propose both supervised\nand unsupervised machine learning strategies to improve tumor\ncharacterization. Our ﬁrst approach is based on supervised\nlearning for which we demonstrate signiﬁcant gains with deep\nlearning algorithms, particularly by utilizing a 3D Convolu-\ntional Neural Network and Transfer Learning. Motivated by\nthe radiologists’ interpretations of the scans, we then show how\nto incorporate task dependent feature representations into a\nCAD system via a graph-regularized sparse Multi-Task Learning\n(MTL) framework.\nIn the second approach, we explore an unsupervised learning\nalgorithm to address the limited availability of labeled train-\ning data, a common problem in medical imaging applications.\nInspired by learning from label proportion (LLP) approaches\nin computer vision, we propose to use proportion-SVM for\ncharacterizing tumors. We also seek the answer to the fun-\ndamental question about the goodness of “deep features” for\nunsupervised tumor classiﬁcation. We evaluate our proposed\nsupervised and unsupervised learning algorithms on two different\ntumor diagnosis challenges: lung and pancreas with 1018 CT\nand 171 MRI scans, respectively, and obtain the state-of-the-art\nsensitivity and speciﬁcity results in both problems.\nIndex Terms—Unsupervised Learning, Lung cancer, 3D CNN,\nIPMN, Pancreatic cancer.\nI. INTRODUCTION\nApproximately 40% of people will be diagnosed with cancer\nat some point during their lifetime with an overall mortality of\n171.2 per 100,000 people per year (based on deaths between\n2008-2012) [1]. Lung and pancreatic cancers are two of the\nmost common cancers. While lung cancer is the largest cause\nof cancer-related deaths in the world, pancreatic cancer has\nthe poorest prognosis with a 5-year survival rate of only 7%\nin the United States [1]. With regards to pancreatic cancer,\nspeciﬁcally in this work, we focus on the challenging prob-\nlem of automatic diagnosis of Intraductal Papillary Mucinous\nNeoplasms (IPMN). IPMN is a pre-malignant condition and\nif left untreated, it can progress to invasive cancer. IPMN\nis mucin-producing neoplasm that can be found in the main\npancreatic duct and its branches. They are radiographically\n∗indicates corresponding author (ulasbagci@gmail.com).\nS. Hussein is with Center for Advanced Machine Learning (CAML) at\nSymantec Corporation. U. Bagci is with Center for Research in Computer\nVision (CRCV) at University of Central Florida (UCF), Orlando, FL; P.\nKandel, C. Bolan, and M. Wallace are with Mayo Clinic, Jacksonville, FL\nComputer \nAided \nDiagnosis \n(CAD)\nDeep Learning \n(CNN)\nMulti-Task Learning\nClustering\nProportion - SVM\nLung nodule \ncharacterization \nIPMN \nclassification\nCase Studies\nSupervised Learning (Section III)\nUnsupervised Learning (Section IV)\nFigure 1: A block diagram to represent different schemes,\nmethods and experimental case studies presented in this pa-\nper. We develop both supervised and unsupervised learning\nalgorithms to characterize tumors. For the supervised learning\nscheme, we propose a new 3D CNN architecture based on\na Graph Regularized Sparse Multi-task learning and perform\nevaluations for lung nodule characterization from CT scans.\nFor unsupervised learning scheme, we propose a new clus-\ntering algorithm, ∝SVM, and test it for the categorization of\nlung nodules from CT scans and pancreatic cysts (IPMN) from\nMRI scans.\nidentiﬁable precursors to pancreatic cancer [2]. Detection and\ncharacterization of these lung and pancreatic tumors can aid\nin early diagnosis; hence, increased survival chance through\nappropriate treatment/surgery plans.\nConventionally, the CAD systems are designed to assist\nradiologists in making accurate and fast decisions by reduc-\ning the number of false positives and false negatives. For\ndiagnostic decision making, a higher emphasis is laid on\nincreased sensitivity: a false-ﬂag is more tolerable than a\ntumor being missed or incorrectly classiﬁed as benign. In this\nregard, a computerized analysis of imaging features becomes\na key instrument for radiologists to improve their diagnostic\ndecisions. In the literature, automated detection and diagnosis\nmethods had been developed for tumors in different organs\nsuch as breast, colon, brain, lung, liver, prostate, and others.\nAs typical in such studies, a CAD includes preprocessing\nand feature engineering steps (including feature extraction and\nselection) followed by a classiﬁcation step [3], [4], [5], [6].\nHowever, with the success of deep learning, a transition from\nfeature engineering to feature learning has been observed in\narXiv:1801.03230v3  [cs.CV]  18 Jan 2019\n2\nmedical image analysis literature. Those systems comprise\nConvolutional Neural Networks (CNN) as feature extractor\nfollowed by a conventional classiﬁer such as Random Forest\n(RF) [7], [8]. In scenarios where a large number of labeled\ntraining examples are available, however, end-to-end trainable\ndeep learning approaches can be employed [9].\nThis paper includes two main approaches for tumor charac-\nterization from radiology scans: supervised and unsupervised\nlearning algorithms. In the ﬁrst part, we focus on novel\nsupervised algorithms, which is a signiﬁcant extension to our\nIPMI 2017 study [10]. Speciﬁcally, we ﬁrst present a novel su-\npervised learning strategy to perform risk-stratiﬁcation of lung\nnodules from low-dose CT scans. For this strategy, we per-\nform a 3D CNN based discriminative feature extraction from\nradiology scans. We contend that 3D networks are important\nfor the characterization of lung nodules in CT images which\nare inherently 3-dimensional. The use of conventional 2D\nCNN methods, whereas, leads to the loss of vital volumetric\ninformation which can be crucial for precise risk assessment\nof lung nodules. In the absence of a large number of labeled\ntraining examples, we utilize a pre-trained 3D CNN archi-\ntecture and ﬁne-tune the network with lung nodules dataset.\nAlso, inspired by the signiﬁcance of lung nodule attributes\nfor clinical determination of malignancy [11], we utilize the\ninformation about six high-level nodule attributes such as\ncalciﬁcation, spiculation, sphericity, lobulation, margin, and\ntexture (Figure 2-A) to improve automatic benign-malignant\nclassiﬁcation. Then, we integrate these high-level features into\na novel graph regularized multi-task learning (MTL) frame-\nwork to yield the ﬁnal malignancy output. We analyze the\nimpact of the aforementioned lung nodule attributes in-depth\nfor malignancy determination and ﬁnd these attributes to be\ncomplementary when obtaining the malignancy scores. From\na technical perspective, we also exploit different regularizers\nand multi-task learning approaches such as trace-norm and\ngraph regularized MTL for regression.\nIn the second part of the paper, inspired by the success-\nful application of unsupervised learning methods in other\ndomains, we explore the potential of unsupervised learning\nstrategies in lung nodule and IPMN classiﬁcation. First, we\nextract discriminative information from a large amount of\nunlabeled imaging data. We analyze both hand-crafted and\ndeep learning features and assess how good those features are\nwhen applied to tumor characterization. In order to obtain an\ninitial set of labels in an unsupervised fashion, we cluster the\nsamples into different groups in the feature domain. We next\npropose to train Proportion-Support Vector Machine (∝SVM)\nalgorithm using label proportions rather than instance labels.\nThe trained model is then employed to learn malignant-benign\ncategorization of the tumors.\nThis paper is organized as follows. Section 2 describes re-\nlated work pertaining to supervised and unsupervised learning\nfor the diagnosis of lung nodules and IPMN. We present\nour MTL based supervised learning algorithm in Section 3.\nIn Section 4, we introduce an unsupervised learning method\nadapted for the diagnosis of lung nodules and IPMN from\nCT and MRI scans, respectively. The experiments and results\nare discussed in Section 5. In the same section, we also\nstudy the inﬂuence of different deep learning features for\nunsupervised learning and establish an upper bound on the\nclassiﬁcation performance using supervised learning methods.\nFinally, Section 6 states discussions and concluding remarks.\nII. RELATED WORK\nThis section summarizes the advances in machine learning\napplied to medical imaging and CAD systems developed for\nlung cancer diagnosis. Since the automatic characterization\nof IPMN from MRI scans has not been extensively studied\nin the literature, relevant works are mostly selected from the\nclinical studies. Our work is the ﬁrst in this regard.\nImaging Features and Classiﬁers: Conventionally, the risk\nstratiﬁcation (classiﬁcation) of lung nodules may require\nnodule segmentation, computation and selection of low-level\nfeatures from the image, and the use of a classiﬁer/regressor.\nIn the approach by [12], different physical statistics includ-\ning intensity measures were extracted and class labels were\nobtained using Artiﬁcial Neural Networks. In [3], lung nod-\nules were segmented using appearance-based models followed\nby shape analysis using spherical harmonics. The last step\ninvolved k-nearest neighbor based classiﬁcation. Another ap-\nproach extended 2D texture features including Local Binary\nPatterns, Gabor and Haralick to 3D [4]. Classiﬁcation using\nSupport Vector Machine (SVM) was performed as the ﬁnal\nstep. In a different study, Way et al. [5], implemented nod-\nule segmentation via 3D active contours, and then applied\nrubber band straightening transform. A Linear Discriminant\nAnalysis (LDA) classiﬁer was applied to get class labels.\nLee et al. [6] introduced a feature selection based approach\nutilizing both clinical and imaging data. Information content\nand feature relevance were measured using an ensemble of\ngenetic algorithm and random subspace method. Lastly, LDA\nwas applied to obtain ﬁnal classiﬁcation on the condensed\nfeature set. In a recent work, spherical harmonics features\nwere fused with deep learning features [8] and then RF\nclassiﬁcation was employed for lung nodule characterization.\nHitherto, the application of CNN for nodule characterization\nhas been limited to 2D space [13], thus falling short of\nincorporating vital contextual and volumetric information. In\nanother approach, Shin et al. [14] employed CNN for the\nclassiﬁcation of lung nodules. Other than not completely 3D\nCNN, the approach didn’t take into account high-level nodule\nattributes and required training an off-the-shelf classiﬁer such\nas RF and SVM.\nThe information about different high-level image attributes\nhad been found useful in the malignancy characterization of\nlung nodules. In a study exploring the correlation between\nmalignancy and nodule attributes, [11] found that 82% of the\nlobulated, 93% of the ragged, 97% of the densely spiculated,\nand 100% of the halo nodules were malignant in a particular\ndataset. Automatic determination of lung nodule attributes and\ntypes had been explored in [15]. The objective was to perform\nthe classiﬁcation of six different nodule types such as solid,\nnon-solid, part-solid, calciﬁed, periﬁssural and spiculated nod-\nules. However, the approach is based on 2D CNN and fell short\n3\nof estimating the malignancy of lung nodules. Furthermore,\n66% of the round nodules were determined as benign.\nAlthough, not an objective in this paper, the detection of\nlung nodules has also been an active subject of interest among\nresearchers [16], [17], [18]. A short but informative review\nof the most recent detection studies can be found in [18].\nPancreatic\nCysts\n(IPMN):\nAlthough\nthere\nhas\nbeen\nconsiderable progress in developing automatic approaches to\nsegment pancreas and its cysts [19], [20], the use of advanced\nmachine learning algorithms to perform fully automatic\nrisk-stratiﬁcation of IPMNs is limited. The approach by\nHanania et al. [21] investigated the inﬂuence of 360 imaging\nfeatures ranging from intensity, texture, and shape to stratify\nsubjects as low or high-grade IPMN. In another example,\nGazit et al. [22] extracted texture and features from the solid\ncomponent of segmented cysts followed by a feature selection\nand classiﬁcation scheme. Both of these approaches [21], [22]\nrequired segmentation of cysts or pancreas and are evaluated\non CT scans only.\nUnsupervised Learning: Typically, the visual recognition and\nclassiﬁcation tasks are addressed using labeled data (supervi-\nsion). However, for tasks where manually generating labels\ncorresponding to large datasets is laborious and expensive, the\nuse of unsupervised learning methods is of signiﬁcant value.\nUnsupervised techniques had been used to solve problems\nin various domains ranging from object categorization [23],\nspeech processing [24] and audio classiﬁcation [25]. These\nmethods conventionally relied on some complementary infor-\nmation provided with the data to improve learning, which\nmay not be available for several classiﬁcation tasks in medical\nimaging.\nIn medical imaging, there have been different approaches\nthat used unsupervised learning for detection and diagnosis\nproblems. The approach by Shin et al. [26] used stacked au-\ntoencoders for multiple organ detection in MRI scans. Vaidhya\net al. [27] presented a brain tumor segmentation method with\nstacked denoising autoencoder evaluated on multi-sequence\nMRI images. In a work by Sivakumar et al. [28], the segmenta-\ntion of lung nodules is performed with unsupervised clustering\nmethods. In another study, Kumar et al. [7] used features from\nautoencoder for lung nodule classiﬁcation. These auto-encoder\napproaches, however, did not yield satisfactory classiﬁcation\nresults. Other than these, unsupervised deep learning has also\nbeen explored for mammographic risk prediction and breast\ndensity segmentation [29].\nUnsupervised feature learning remains an active research\narea for the medical imaging community, more recently\nwith Generative Adversarial Networks (GAN) [30]. In order\nto explore the information from unlabeled images, Zhang\net al. [31] described a semi-supervised method for the\nclassiﬁcation of four types of nodules. In sharp contrast\nto these approaches, the unsupervised learning strategies\npresented in this paper don’t involve feature learning using\nauto-encoders. Using sets of hand-crafted as well as pre-\ntrained deep learning features, we propose a new unsupervised\nlearning algorithm where an initially estimated label set is\nprogressively improved via proportion-SVM.\nOur Contributions\nA block diagram representing different supervised and unsu-\npervised schemes is presented in Figure 1. Overall, our main\ncontributions in this work can be summarized as follows:\n• For lung nodule characterization, we present a 3D CNN\nbased supervised learning approach to fully appreciate the\nanatomical information in 3D, which would be otherwise\nlost in the conventional 2D approaches. We use ﬁne-tuning\nstrategy to avoid the requirement for a large number of\nvolumetric training examples for 3D CNN. For this purpose.\nwe use a pre-trained network (which is trained on 1 million\nvideos) and ﬁne-tune it on the CT data.\n• We introduce a graph regularized sparse MTL platform\nto integrate the complementary features from lung nodule\nattributes so as to improve malignancy prediction. Figure 2-\nA shows high-level lung nodule attributes having varying\nlevels of prominence.\n• We evaluate the proposed supervised and unsupervised\nlearning algorithms to determine the characterization of lung\nnodules and IPMN cysts (Table I). In the era where the wave\nof deep learning has swept into almost all domains of visual\nanalysis, we investigate the contribution of features extracted\nfrom different deep learning architectures. To the best of our\nknowledge, this is the ﬁrst work to investigate the automatic\ndiagnosis of IPMNs from MRI.\n• In the proposed unsupervised learning algorithm, instead of\nhard assigning labels, we estimate the label proportions in\na data-driven manner. Additionally, to alleviate the effect\nof noisy labels (i.e. mislabels) obtained during clustering,\nwe propose to employ ∝SVM, which is trained on label\nproportions only.\nIII. SUPERVISED LEARNING METHODS\nA. Problem Formulation\nLet X = [x1, x2 . . . xn]T\n∈Rn×d represent the input\nfeatures obtained from n images of lung nodules each having\na dimension d. Each data sample has an attribute/malignancy\nscore given by Y = [y1, y2 . . . yn], where Y T ∈Rn×1. While\nX consists of features extracted from radiology images, and\nY represents the malignancy score over 1-5 scale where 1\nrepresents benign and 5 represents malignant. In supervised\nlearning, the labeled training data is used to learn the coefﬁ-\ncient vector or the regression estimator W ∈Rd. In testing,\nW is used to estimate Y for an unseen feature/example.\nFor regression, a regularizer is often added to prevent over-\nﬁtting. Hence, a classical least square regression turns into a\nconstrained optimization problem with ℓ1 regularization as:\nmin\nW ∥XW −Y ∥2\n2 , s.t. ∥W∥1 ≤t.\n(1)\n4\n (a)       (b)        (c)        (d)       (e)         (f) \nLow \nHigh \nCal \nSph \nMar \nLob \nSpic \nTex \n0\n50\n100\n150\n200\n250\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n#  of  Nodules \nMalignancy Scores \n(g)  \n(A) Lung nodule attributes\nInput volume \n3D Convolution \nMax pooling \nFully Connected layer \n4096-D \nAttributes: \n \n• Malignant? – Y \n \n• Calcified? – N \n \n• Lobulated? – Y \n \n• Spherical? – Y \n \n• Spiculated? – Y \n \n• Margin? – Y \n \n• Texture ? – Y \n3D Convolution \nMax pooling \nFully Connected layer \n4096-D \n3DCNN for attribute  1 \n3D CNN for attribute ‘M’ \nGraph Regularized Sparse \nRepresentation \nMalignancy \nscore \n \nMulti-Task Learning \n(B) An overview of the proposed supervised approach\nFigure 2: (A) A visualization of lung nodules having different levels of attributes. On moving from the top (attribute absent)\nto the bottom (attribute prominently visible), the prominence level of the attribute increase. Different attributes including\ncalciﬁcation, sphericity, margin, lobulation, spiculation and texture can be seen in (a-f). The graph in (g) depicts the number\nof nodules with different malignancy levels in our experiments using the publicly available dataset [32]. An overview of the\nproposed 3D CNN based graph regularized sparse MTL approach is presented in (B).\nIn the above equation, the sparsity level of the coefﬁcient\nvector W = [w1, w2 . . . wd] is controlled by a parameter t.\nSince the function in Eq. (1) is convex and the constraints\ndeﬁne a convex set, a local minimizer of the objective function\nis subjected to constraints corresponding to a global mini-\nmizer. In the following subsections, we extend this supervised\nlearning setting with deep learning and MTL concepts to\ncharacterize lung nodules as benign or malignant.\nB. 3D Convolution Neural Network (CNN) and Fine-Tuning\nWe use 3D CNN [33] trained on Sports-1M dataset [34]\nand ﬁne-tune it on the lung nodule CT dataset. The Sports-\n1M dataset consists of 487 classes with 1 million videos.\nAs the lung nodule dataset doesn’t have a large number of\ntraining examples, ﬁne-tuning is done to acquire dense feature\nrepresentation from the Sports-1M. The 3D CNN architecture\nconsists of 5 sets of convolution, 2 fully-connected and 1 soft-\nmax classiﬁcation layers. Each convolution set is followed by\na max-pooling layer. The input to the 3D CNN comprises\ndimensions of 128x171x16, where 16 denotes the number of\nslices. Note that the images in the dataset are resized to have\nconsistent dimensions such that the number of channels is 3\nand the number of slices is ﬁxed to 16. Hence, the overall input\ndimension can be considered as 3x16x128x171. The number\nof ﬁlters in the ﬁrst 3 convolution layers are 64, 128 and 256\nrespectively, whereas there are 512 ﬁlters in the last 2 layers.\nThe fully-connected layers have a dimension 4096 which is\nalso the length of feature vectors used as an input to the MTL\nframework. Implementation details are mentioned in section\nV-C.\nC. Multi-task learning (MTL)\nMulti-task learning is an approach of learning multiple tasks\nsimultaneously while considering disparities and similarities\nacross those tasks. Given M tasks, the goal is to improve\nthe learning of a model for task i, (i ∈M) by using the\ninformation contained in the M tasks. We formulate the\nmalignancy prediction of lung nodules as an MTL problem,\nwhere visual attributes of lung nodules are considered as\ndistinct tasks (Figure 2A). In a typical MTL problem, ini-\ntially, the correlation between M tasks and the shared feature\nrepresentations are not known. The aim in the MTL approach\nis to learn a joint model while exploiting the dependencies\namong visual attributes (tasks) in feature space. In other words,\nwe utilize visual attributes and exploit their feature level\ndependencies so as to improve regressing malignancy using\nother attributes.\nAs shown in Figure 2B, we design lung tumor characteriza-\ntion as an MTL problem, where each task has model param-\neters Wi, which are utilized to characterize the corresponding\ntask i. When W = [W1, W2 . . . WM] ∈Rd×M constitutes\na rectangular matrix, rank can be considered as a natural\nextension to cardinality, and nuclear/trace norm leads to low\nrank solutions. In some cases nuclear norm regularization can\nbe considered as the ℓ1-norm of the singular values [35]. Trace\nnorm, the sum of singular values, is the convex envelope of\nthe rank of a matrix (which is non-convex), where the matrices\nare considered on a unit ball. After substituting, ℓ1-norm by\ntrace norm, the least square loss function with trace norm\nregularization can be formulated as:\nmin\nW\nM\nX\ni=1\n∥XiWi −Yi∥2\n2 + ρ ∥W∥∗,\n(2)\nwhere ρ adjusts the rank of the matrix W, and ∥W∥∗=\nP\ni=1 σi(W) is the trace-norm where σ denotes singular\nvalues. However, as in trace-norm, the assumption about\nmodels sharing a common subspace is restrictive for some\napplications.\nAs the task relationships are often unknown and are learned\nfrom data, we represent tasks and their relations in the form\n5\nof a graph. Let Υ = (V, E) represent a complete graph in\nwhich nodes V correspond to the tasks and the edges E model\nany afﬁnity between the tasks. In such case, a regularization\ncan be applied on the graph modeling task dependencies [36].\nThe complete graph can be modeled as a structure matrix S =\n[e1, e2 . . . e∥E∥] ∈RM×∥E∥where the deviation between the\npairs of tasks can be regularized as:\n∥WS∥2\nF =\n∥E∥\nX\ni=1\n\r\rWei\r\r2\n2 =\n∥E∥\nX\ni=1\n\r\r\rWeia −Wei\nb\n\r\r\r\n2\n2 ,\n(3)\nhere, ei\na, ei\nb are the edges between the nodes a and b, and\nei ∈RM. The matrix S deﬁnes an incidence matrix where ei\na\nand ei\nb are assigned to 1 and -1, respectively, if nodes a and\nb are connected in the graph. Eq. (3) can be further explained\nas:\n∥WS∥2\nF = tr((WS)T (WS)) = tr(WSST WT ) = tr(WLWT ),\n(4)\nwhere L = SST is the Laplacian matrix and ‘tr’ represents\nthe trace of a matrix. The method to compute structure matrix\nS is discussed in Section V-C.\nThe malignancy prediction equation can be further regu-\nlarized because there are still other uncertainties to consider,\ni.e., disagreement between radiologists’ visual interpretations\nfor a given nodule. For instance, while one radiologist may\ngive a malignancy score of xj\n1 for a nodule j, the other may\ngive a score of xj\n2 for the same nodule. In order to reﬂect\nthese uncertainties in our algorithm, we formulate a scoring\nfunction which models such inconsistencies:\nΨ(j) =\n\u0012\nexp(−P\nr(xj\nr −µj)2\n2σj\n)\n\u0013−1\n.\n(5)\nFor a particular example j, this inconsistency measure can\nbe represented as Ψ(j). xj\nr is the score given by the rth\nradiologist (expert) whereas µj and σj represent mean and\nstandard deviation of the scores, respectively. We calculate\nthis inconsistency score for all the tasks under consideration\nand for simplicity we have omitted the index for the tasks. The\nﬁnal objective function of graph regularized sparse least square\noptimization with the inconsistency measure can expressed as:\nmin\nW\nM\nX\ni=1\n1⃝\nz\n}|\n{\n∥(Xi + Ψi)Wi −Yi∥2\n2 +\n2⃝\nz\n}|\n{\nρ1 ∥WS∥2\nF +\n3⃝\nz\n}|\n{\nρ2 ∥W∥1,\n(6)\nwhere ρ1 tunes the penalty degree for graph structure and\nρ2 handles the sparsity level. In Eq. (6), the least square\nloss function 1⃝observes decoupling of tasks whereas 2⃝\nand 3⃝model their interdependencies, so as to learn joint\nrepresentation.\nD. Optimization\nIn order to solve Eq. (6), the conventional approach is to\nuse standard gradient descent as an optimization algorithm.\nHowever, standard gradient descent cannot be applied here\nbecause the ℓ1−norm is not differentiable at W = 0 and gra-\ndient descent approach fails to provide sparse solutions [37].\nSince the optimization function in the above equation has\nboth smooth and non-smooth convex parts, it can be solved\nafter replacing the non-smooth part with its estimates. In\nother words, the ℓ1-norm in the above equation is the non-\nsmooth part and the proximal operator can be used for its\nestimation. For this purpose, we utilize accelerated proximal\ngradient method [38], the ﬁrst order gradient method having a\nconvergence rate of O(1/m2), where m controls the number\nof iterations.\nIV. UNSUPERVISED LEARNING METHODS\nSince annotating medical images is laborious, expensive\nand time-consuming, in the second part of this paper, we\nexplore the potential of unsupervised learning approaches for\ntumor characterization problems. As illustrated in Figure 3, our\nproposed unsupervised framework includes three steps. First,\nwe perform clustering on the appearance features obtained\nfrom the images to estimate an initial set of labels. Then,\nusing the obtained initial labels, we compute label proportions\ncorresponding to each cluster. Finally, we use the initial cluster\nassignments and label proportions to learn the categorization\nof tumors.\nA. Initial Label Estimation\nLet X = [x1, x2 . . . xn]T ∈Rn×d represent the input matrix\nwhich contains features from n images such that x ∈Rd.\nWe then cluster the data into 2 ≤k < n clusters using k-\nmeans algorithm. Let A represent |X| × k assignment matrix\nwhich denotes the membership assignment of each sample to a\ncluster. The optimal clustering would minimize the following\nobjective function:\nargmin\nµv,A\nk\nX\nv=1\nA(u, v) ∥xu −µv∥2 ,\ns.t. A(u, v) = 0 ∨1,\nX\nv\nA(u, v) = 1\n(7)\nwhere µv is the mean of the samples in cluster v. The\nassignment matrix A can then be used to estimate labels c.\nThese labels are only used for estimating label proportions\nof the clustered data for the purpose of training a new algo-\nrithm which we adapt for our problem, i.e., proportion-SVM\n(∝SVM). The rationale behind this proportion comes from\nthe clustering notion where data is divided into groups/clusters\nand each cluster corresponds to a particular class. In our work,\nspeciﬁcally, clustering is only an initial step to estimate cluster\nassignments that are progressively reﬁned in the subsequent\nsteps.\nB. Learning with the Estimated Labels\nSince our initial label estimation approach is unsupervised,\nthere are uncertainties associated with them. It is, therefore,\nreasonable to assume that learning a discriminative model\nbased on these noisy instance level labels can deteriorate\nclassiﬁcation performance. In order to address this issue, we\n6\nCluster 1\np1\nCluster 2\np2\nLabel Proportions\nInput Images\n(Lung nodules/IPMN)\nClustering\nInitial Labels\nαSVM\nFinal Classification\nMalignant/IPMN\nBenign/Normal\nFeature \nExtraction\n𝒑𝒗= 𝒏−𝟏෍\n𝒖=𝟏\n𝒏\n𝑰(𝒚𝒖= 𝒗)\nStep 1\nStep 2\nStep 3\nFigure 3: An outline of the proposed unsupervised approach. Given the input images, we compute GIST features and perform\nk-means clustering to get the initial set of labels which can be noisy. Using the set of labels, we compute label proportions\ncorresponding to each cluster/group (Eq. (9)). We ﬁnally employ ∝SVM to learn a discriminative model using the features\nand label proportions.\nTABLE I: List and details of different experiments performed for supervised and unsupervised learning along with their\nevaluation sets.\nExperiments\nDetails\nEvaluation Set\nE1\nSupervised learning, 3D CNN based\nMulti-task learning with attributes,\nﬁne-tuning (C3D) network\n3D dataset:\nMalignancy score\nregression of Lung\nnodules (CT)\nE2\nUnsupervised learning, GIST features,\nProportion-SVM\n2D dataset:\nLung nodules\n(CT) and IPMN\nclassiﬁcation\n(MRI)\nE3\nUnsupervised learning, features from\ndifferent layers of 2D VGG network\nE4\nSupervised learning to establish\nclassiﬁcation upper-bound, GIST and VGG\nfeatures with SVM and RF\nmodel the instance level labels as latent variables and thereby\nconsider group/bag level labels.\nInspired by ∝SVM approach [39], which models the latent\ninstance level variables using the known group level label\nproportions, we formulate our learning problem such that\nclusters are analogous to the groups. In our formulation, each\ncluster v can be represented as a group such that the majority\nof samples belong to the class v. Considering the groups to\nbe disjoint such that Sk\nv=1Ωv= 1, 2, . . . n , and Ωrepresents\ngroups; the objective function of the large-margin ∝SVM after\nconvex relaxation can be formulated as:\nmin\nc∈C min\nw\n \n1\n2wT w + K\nn\nX\nu=1\nL(cu, wT φ(x))\n!\nC =\n\u001a\nc\n\f\f\f\f | epv(c) −pv| ≤ϵ, cu ∈{−1, 1} ∀k\nv=1\n\u001b\n,\n(8)\nwhere ep and p represent the estimated and true label propor-\ntions, respectively. In Eq. (8), c is the set of instance level\nlabels, φ(.) is the input feature, K denotes cost parameter and\nL(.) represents the hinge-loss function for maximum-margin\nclassiﬁers such as SVM. An alternative approach based on\ntraining a standard SVM classiﬁer with clustering assignments\nis discussed in Section V-D.\nThe optimization in Eq. (8) is, in fact, an instance of\nMultiple Kernel Learning, which can be solved using the\ncutting plane method where the set of active constraints is\nincrementally computed. The goal is to ﬁnd the most violated\nconstraint, however, the objective function still decreases even\nby further relaxation and aiming for any violated constraint.\nFurther details about optimization can be studied in [39].\nC. Calculating Label Proportions\nIn the conventional ∝SVM approach, the label proportions\nare known a priori. Since our approach is unsupervised, both\ninstance level labels and group label proportions are unknown.\nMoreover, establishing strong assumptions about the label\nproportions may affect learning. It is, however, reasonable to\nassume that a large number of instances in any group carry\nthe same label and there may be a small number of instances\nwhich are outliers. The label proportions serve as a soft-label\nfor a bag where a bag can be considered as a super-instance.\nIn order to determine the label proportions in a data-driven\nmanner, we use the estimated labels obtained from clustering.\nThe label proportion pv corresponding to the group v can be\nrepresented as:\npv = n−1\nn\nX\nu=1\nI(yu = v),\n(9)\nwhere I(.) is the indicator function which yields 1 when\nyu = v. The ∝SVM is trained using the image features and\nlabel proportions to classify the testing data. It is important to\nmention that the ground truth labels (benign/malignant labels)\nare used only to evaluate the proposed framework and are\n7\nNormal Pancreas \nPancreas with IPMN \nFigure 4: Axial T2 MRI scans illustrating pancreas. The top row shows different ROIs of pancreas, along with a magniﬁed\nview of a normal pancreas (outlined in blue). The bottom row shows ROIs from subjects with IPMN in the pancreas, which\nis outlined in red.\nnot used in estimating label proportions or training of the\nproportion-SVM. In addition, clustering and label proportion\ncalculation are only performed on the training data and the\ntesting data remains completely unseen for ∝SVM. The num-\nber of clusters is ﬁxed as 2, i.e. benign and malignant classes\nand the result was checked to assign benign and malignant\nlabels to the clusters.\nV. EXPERIMENTS\nA. Data for Lung Nodules\nFor test and evaluation, we used LIDC-IDRI dataset from\nLung Image Database Consortium [32], which is one of the\nlargest publicly available lung nodule dataset. The dataset\ncomprises 1018 CT scans with a slice thickness varying from\n0.45 mm to 5.0 mm. At most four radiologists annotated those\nlung nodules which have diameters equal to or greater than 3.0\nmm.\nWe considered nodules which were interpreted by at least\nthree radiologists for evaluations. The number of nodules\nfulﬁlling this criterion was 1340. As a nodule may have\ndifferent malignancy and attribute scores provided by different\nradiologists, their mean scores were used. The nodules have\nscores corresponding to these six attributes: (i) calciﬁcation,\n(ii) lobulation, (iii) spiculation, (iv) sphericity, (v) margin and\n(vi) texture as well as malignancy (Figure 2). The malignancy\nscores ranged from 1 to 5 where 1 denoted benign and 5\nmeant highly malignant nodules. To account for malignancy\nindecision among radiologists, we excluded nodules with\na mean score of 3. The ﬁnal evaluation set included 509\nmalignant and 635 benign nodules. As a pre-processing step,\nthe images were resampled to be isotropic so as to have 0.5\nmm spacing in each dimension.\nB. Data for IPMN\nThe data for the classiﬁcation of IPMN contains T2 MRI\naxial scans from 171 subjects. The scans were labeled by a\nradiologist as normal or IPMN. Out of 171 scans, 38 subjects\nwere normal, whereas the rest of 133 were from subjects\ndiagnosed with IPMN. The in-plane spacing (xy-plane) of\nthe scan was ranging from 0.468 mm to 1.406 mm. As pre-\nprocessing, we ﬁrst employ N4 bias ﬁeld correction [40] to\neach image in order to normalize variations in image intensity.\nWe then apply curvature anisotropic image ﬁlter to smooth\nimage while preserving edges. For experiments, 2D axial slices\nwith pancreas (and IPMN) are cropped to generate Region\nof Interest (ROI) as shown in Figure 4. The large intra-class\nvariation, especially due to varying shapes of the pancreas can\nalso be observed in Figure 4. A list of different supervised and\nunsupervised learning experiments along with their evaluation\nsets is tabulated in Table I.\nC. Evaluation and Results- Supervised Learning\nWe ﬁne-tuned the 3D CNN network trained on Sports-\n1M dataset [34] which had 487 classes. In order to train\nthe network with binary labels for malignancy and the six\nattributes we used the mid-point as a pivot and labeled samples\nas positive (or negative) based on their scores being greater\n(or lesser) than the pivot. In our context, malignancy and\nattributes are characterized as tasks. The C3D was ﬁne-tuned\nwith these 7 tasks and 10 fold cross-validation was conducted.\nThe requirement to have a large amount of labeled training\ndata was evaded by ﬁne-tuning the network. Since the input\nto the network required 3 channel image sequences with at\nleast 16 slices, we concatenated the gray level axial channel\nas the other two channels.\nAdditionally, in order to ascertain that all input vol-\numes have 16 slices, we performed interpolation where war-\nranted. The ﬁnal feature representation was obtained from the\n8\nTABLE II: The comparison of the proposed approach with other methods using regression accuracy and mean absolute score\ndifference for lung nodule characterization.\nMethods\nAccuracy\nMean Score\n%\nDifference\nGIST features + LASSO\n76.83\n0.675\nGIST features + RR\n76.48\n0.674\n3D CNN features + LASSO (Pre-trained)\n86.02\n0.530\n3D CNN features + RR (Pre-trained)\n82.00\n0.597\n3D CNN features + LASSO (Fine-tuned)\n88.04\n0.497\n3D CNN features + RR (Fine-tuned)\n84.53\n0.550\n3D CNN MTL with Trace norm\n80.08\n0.626\nProposed (3D CNN with Multi-task Learning- Eq. 7)\n91.26\n0.459\nﬁrst fully connected layer of 3D CNN consisting of 4096-\ndimensions.\nFor computing structure matrix S, we calculate the corre-\nlation between different tasks by estimating the normalized\ncoefﬁcient matrix W via least square loss function with lasso\nfollowed by the calculation of correlation coefﬁcient matrix\n[36]. In order to get a binary graph structure matrix, we\nthresholded the correlation coefﬁcient matrix. As priors in\nEq. (6) we used ρ1 and ρ2 as 1 and 10 respectively. Finally, to\nobtain the malignancy score for test images, the features from\nthe network trained on malignancy were multiplied with the\ncorresponding task coefﬁcient vector W.\nWe evaluated our proposed approach using both classiﬁ-\ncation and regression metrics. For classiﬁcation, we consid-\nered a nodule to be successfully classiﬁed if its predicted\nscore lies in ±1 of the ground truth score. For regression,\nwe calculated average absolute score difference between the\npredicted score and the true score. The comparison of our\nproposed MTL approach with approaches including GIST\nfeatures [41], 3D CNN features from pre-trained network +\nLASSO, Ridge Regression (RR) and 3D CNN MTL+trace\nnorm is tabulated in Table II. It can be observed that our\nproposed graph regularized MTL performs signiﬁcantly better\nthan other approaches both in terms of classiﬁcation accuracy\nas well as the mean score difference. The gain in classiﬁcation\naccuracy was found to be 15% and 11% for GIST and trace-\nnorm respectively. In comparison with the pre-trained network,\nwe obtain an improvement of 5% with proposed MTL. In\naddition, our proposed approach reduces the average absolute\nscore difference for GIST by 32% and for trace-norm by 27%.\nD. Evaluations and Results- Unsupervised Learning\nFor unsupervised learning, evaluations were performed on\nboth lung nodules and IPMN datasets. In order to compute\nimage level features, we used GIST descriptors [41]. The\nnumber of clusters is ﬁxed as 2, which accounts for benign\nand malignant classes. The clustering result was checked to\nassign benign and malignant labels to the clusters. We used 10\nfold cross-validation to evaluate our proposed approach. The\ntraining samples along with the label proportions generated\nusing clustering served as the input to ∝SVM with a linear\nkernel.\nTo evaluate our unsupervised approach we used accuracy,\nsensitivity and speciﬁcity as metrics. It can be observed\nin Table III that the proposed combination of clustering\nand ∝SVM signiﬁcantly outperforms other approaches in\naccuracy and sensitivity. In comparison with clustering+SVM,\nthe proposed framework yields almost 21% improvement in\nsensitivity for lung nodules and around 7% improvement for\nIPMN classiﬁcation. The low sensitivity and high speciﬁcity\nof clustering, clustering+SVM, and clustering+RF approaches\ncan be explained by disproportionate assignment of instances\nas benign (normal) by these approaches, which is not found\nin the proposed approach. At the same time, the proposed\napproach records around 24% and 9% improvement in\naccuracy as compared to clustering for lung nodules and\nIPMN, respectively.\nAre Deep Features good for Unsupervised Classiﬁcation?\nGiven the success of deep learning features for image classiﬁ-\ncation and their popularity with the medical imaging commu-\nnity, we explored their performance to classify lung nodules\nand IPMN in an unsupervised manner. For this purpose, we\nused a pre-trained deep CNN architecture to extract features\nand then perform clustering to obtain baseline classiﬁcation\nperformance. We extracted features from fully-connected lay-\ners 7 and 8 of Fast-VGG [42] with and without applying\nReLU non-linearity. Classiﬁcation accuracy, using clustering\nover these features is shown in Figure 5.\nIt can be seen in Figure 5 that the features with non-\nlinearity (ReLU) are more discriminative for classiﬁcation\nusing clustering as compared to without ReLU. The same\ntrend can be observed for both lung nodules and IPMN\nclassiﬁcation using VGG-fc7 and VGG-fc8 layers. Owing\nto the larger evaluation set, the inﬂuence of ReLU is\nmore prominent for lung nodules as compared to IPMN.\nAlthough the results between VGG-fc7 and VGG-fc8 are\nnot substantially different, the highest accuracy for IPMN\ncan be obtained by using VGG-fc7-ReLU features and\nfor lung nodules by using VGG-fc8-ReLU features. The\nnon-linearity induced by ReLU clips the negative values to\n9\nTABLE III: Average classiﬁcation accuracy, sensitivity, and speciﬁcity of the proposed unsupervised approach for IPMN and\nlung nodule classiﬁcation with other methods\nEvaluation Set\nMethods\nAccuracy Sensitivity Speciﬁcity\nIPMN\nClassiﬁcation\nClustering\n49.18%\n45.34%\n62.83%\nClustering + RF\n53.20%\n51.28%\n69.33%\nClustering + SVM\n52.03%\n51.96%\n50.5%\nProposed approach\n58.04%\n58.61%\n41.67%\nLung Nodule\nClassiﬁcation\nClustering\n54.83%\n48.69%\n60.04%\nClustering + RF\n76.74%\n58.59%\n91.40%\nClustering + SVM\n76.04%\n57.08%\n91.28%\nProposed approach\n78.06%\n77.85%\n78.28%\n0\n10\n20\n30\n40\n50\n60\n70\nVGG-fc7\nVGG-fc7-ReLU\nVGG-fc8\nVGG-fc8-ReLU\n47.11 \n54.3 \n52.99 \n55.42 \nPercentage (%) \nUnsupervised Classification of Lung nodules using various \nDeep Features \nAccuracy\nSensitivity\nSpecificity\n0\n10\n20\n30\n40\n50\n60\nVGG-fc7\nVGG-fc7-ReLU\nVGG-fc8\nVGG-fc8-ReLU\n49.12 \n51.47 \n50.36 \n50.92 \nPercentage (%) \nUnsupervised Classification of IPMN using various  \nDeep Features \nAccuracy\nSensitivity\nSpecificity\nFigure 5: Inﬂuence of deep learning features obtained from different layers of a VGG network with and without ReLU\nnon-linearities. The graph on the left shows accuracy, sensitivity and speciﬁcity for unsupervised lung nodule classiﬁcation\n(clustering), whereas the right one shows the corresponding results for IPMN.\nzero, which can sparsify the feature vector and can reduce\noverﬁtting. Additionally, it can be seen that GIST features\nyield comparable performance than deep features (Table III).\nThis can be explained by the fact that the deep networks were\ntrained on ImageNet dataset so the ﬁlters in the networks were\nmore tuned to the variations in natural images than medical\nimages. Classiﬁcation improvement can be expected with\nunsupervised feature learning techniques such as GANs [30].\nClassiﬁcation using Supervised Learning\nIn order to establish the upper-bound on the classiﬁcation\nperformance, we trained linear SVM and Random Forest\nusing GIST and different deep learning features with ground\ntruth labels on the same 10 fold cross-validations sets.\nTable IV lists the classiﬁcation accuracy, sensitivity, and\nspeciﬁcity using GIST, VGG-fc7 and VGG-fc8 features\nfor both IPMN and lung nodules. For both VGG-fc7 and\nVGG-fc8, we used features after ReLU since they are found\nto be more discriminative (Figure 5). Interestingly, for lung\nnodules, VGG-fc7 features along with RF classiﬁer are\nreported to have comparable results to the combination of\nGIST and RF classiﬁer. This can be explained by the fact\nthat deep networks are pre-trained on ImageNet dataset as\ncompared to handcrafted features such as GIST, which don’t\nrequire any training. On the other hand, for smaller datasets\nsuch as IPMN, deep features are found to perform better\nas compared to GIST. In order to balance the number of\npositive (IPMN) and negative (normal) examples, which can\nbe a critical drawback otherwise, we performed Adaptive\nSynthetic Sampling [43]. This was done to generate synthetic\nexamples in terms of features from the minority class (normal).\nVI. DISCUSSION AND CONCLUDING REMARKS\nIn this study, we present a framework for the malignancy\ndetermination of lung nodules with 3D CNN based graph\nregularized sparse MTL. To the best of our knowledge, this\nis the ﬁrst work where MTL and transfer learning are studied\nfor 3D deep networks to improve risk stratiﬁcation of lung\nnodules. Usually, the data sharing for medical imaging is\nhighly regulated and the accessibility of experts (radiologists)\nto label these images is limited. As a consequence, the access\nto the crowdsourced and publicly gathered and annotated data\nsuch as videos may help in obtaining discriminative features\nfor medical image analysis.\nWe also analyzed the signiﬁcance of different imaging\nattributes corresponding to lung nodules including spiculation,\ntexture, calciﬁcation and others for risk assessment. Instead\n10\nTABLE IV: Classiﬁcation of IPMN and Lung Nodules using different features and supervised learning classiﬁers.\nEvaluation Set\nFeatures\nClassiﬁers\nAccuracy (%)\nSensitivity (%)\nSpeciﬁcity (%)\nIPMN\nClassiﬁcation\nGIST\nSVM\n76.05\n83.65\n52.67\nRF\n81.9\n93.69\n43.0\nVGG-fc7\nSVM\n84.18\n96.91\n44.83\nRF\n81.96\n94.61\n42.83\nVGG-fc8\nSVM\n84.22\n97.2\n46.5\nRF\n80.82\n93.4\n45.67\nLung Nodule\nClassiﬁcation\nGIST\nSVM\n81.56\n71.31\n90.02\nRF\n81.64\n76.47\n85.97\nVGG-fc7\nSVM\n77.97\n75.2\n80.6\nRF\n81.73\n78.24\n84.59\nVGG-fc8\nSVM\n78.76\n74.67\n82.29\nRF\n80.51\n76.03\n84.24\nof manually modeling these attributes we utilized 3D CNN\nto learn rich feature representations associated with these\nattributes. The graph regularized sparse MTL framework was\nemployed to integrate 3D CNN features from these attributes.\nWe have found the features associated with these attributes\ncomplementary to those corresponding to malignancy.\nIn the second part of this study, we explored the potential\nof unsupervised learning for malignancy determination. Since\nin most medical imaging tasks radiologists are required to\nget annotations, acquiring labels to learn machine learning\nmodels is more cumbersome and expensive as compared to\nother computer vision tasks. In order to address this challenge,\nwe employed clustering to obtain an initial set of labels\nand progressively reﬁned them with ∝SVM. We obtained\npromising results and our proposed approach outperformed\nthe other methods in evaluation metrics.\nFollowing up on the application of deep learning for almost\nall tasks in the visual domain, we studied the inﬂuence of dif-\nferent pre-trained deep networks for lung nodule classiﬁcation.\nFor some instances, we found that commonly used imaging\nfeatures such as GIST have comparable results as those\nobtained from pre-trained network features. This observation\ncan be explained by the fact that the deep networks were\ntrained on ImageNet classiﬁcation tasks so the ﬁlters in CNN\nwere more tuned to the nuances in natural images as compared\nto medical images.\nTo the best of our knowledge, this is one of the ﬁrst and the\nlargest evaluation of a CAD system for IPMN classiﬁcation.\nCAD systems for IPMN classiﬁcation are relatively newer\nresearch problems and there is a need to explore the use of dif-\nferent imaging modalities to improve classiﬁcation. Although\nMRI remains the most common modality to study pancreatic\ncysts, CT images can also be used as a complementary imaging\nmodality due to its higher resolution and its ability to capture\nsmaller cysts. Additionally, a combination of T2-weighted,\ncontrast-enhanced and unenhanced T1-weighted sequences can\nhelp improve detection and diagnosis of IPMN [44]. In this\nregard, multi-modal deep learning architectures can be deemed\nuseful [45]. The detection and segmentation of pancreas can\nalso be useful to make a better prediction about the presence\nof IPMN and cysts. Due to its anatomy, the pancreas is a\nchallenging organ to segment, particularly in MRI images.\nTo address this challenge, other imaging modalities can be\nutilized for joint segmentation and diagnosis of pancreatic\ncysts and IPMN. Furthermore, visualization of activation maps\ncan be quite useful for the clinicians to identify new imaging\nbiomarkers that can be employed for diagnosis in the future.\nThe future prospects of using different architectures to\nperform unsupervised representation learning using GAN are\npromising. Instead of using hand-engineered priors of sam-\npling in the generator, the work in [46] learned priors using\ndenoising auto-encoders. For measuring the sample similarity\nfor complex distributions such as those in the images, [47]\njointly trained variational autoencoders and GANs. Moreover,\nthe applications of CatGAN [48] and InfoGAN [49] for semi-\nsupervised and unsupervised classiﬁcation tasks in medical\nimaging are worth exploring as well.\nMedical imaging has unique challenges associated with the\nscarcity of labeled examples. Moreover, unless corroborated\nby biopsy, there may exist a large variability in labeling\nfrom different radiologists. Although ﬁne-tuning has helped\nto address the lack of annotated examples, the performance is\nlimited due to large differences in domains. It is comparatively\neasier to obtain scan level labels than slice level labels. In\nthis regard, weakly supervised approaches such as multiple\ninstance learning (MIL) can be of great value. Active learning\ncan be another solution to alleviate the difﬁculty in labeling. In\naddition to these directions, unsupervised learning approaches\nwill surely be pursued to address unique medical imaging\nchallenges.\nREFERENCES\n[1] Society, A.C.: Cancer Facts & Figures. American Cancer Society (2016)\n[2] Sadot, E., Basturk, O., Klimstra, D.S., G¨onen, M., Anna, L., Do, R.K.G.,\nDAngelica, M.I., DeMatteo, R.P., Kingham, T.P., Jarnagin, W.R., et al.:\nTumor-associated neutrophils and malignant progression in intraductal\npapillary mucinous neoplasms: an opportunity for identiﬁcation of high-\nrisk disease. Annals of surgery 262(6), 1102 (2015)\n[3] El-Baz, A., Nitzken, M., Khalifa, F., Elnakib, A., Gimelfarb, G., Falk,\nR., El-Ghar, M.A.: 3D shape analysis for early diagnosis of malignant\nlung nodules. In: IPMI. pp. 772–783. Springer (2011)\n[4] Han, F., Wang, H., Zhang, G., Han, H., Song, B., Li, L., Moore, W.,\nLu, H., Zhao, H., Liang, Z.: Texture feature analysis for computer-aided\ndiagnosis on pulmonary nodules. Journal of Digital Imaging 28(1), 99–\n115 (2015)\n11\n[5] Way, T.W., Hadjiiski, L.M., Sahiner, B., Chan, H.P., Cascade, P.N.,\nKazerooni, E.A., Bogot, N., Zhou, C.: Computer-aided diagnosis of\npulmonary nodules on CT scans: segmentation and classiﬁcation using\n3D active contours. Medical Physics 33(7), 2323–2337 (2006)\n[6] Lee, M., Boroczky, L., Sungur-Stasik, K., Cann, A., Borczuk, A.,\nKawut, S., Powell, C.: Computer-aided diagnosis of pulmonary nodules\nusing a two-step approach for feature selection and classiﬁer ensemble\nconstruction. Artiﬁcial Intelligence in Medicine 50(1), 43–53 (2010)\n[7] Kumar, D., Wong, A., Clausi, D.A.: Lung nodule classiﬁcation using\ndeep features in CT images. In: Computer and Robot Vision (CRV),\n2015 12th Conference on. pp. 133–138. IEEE (2015)\n[8] Buty, M., Xu, Z., Gao, M., Bagci, U., Wu, A., Mollura, D.J.: Characteri-\nzation of Lung Nodule Malignancy Using Hybrid Shape and Appearance\nFeatures. In: MICCAI. pp. 662–670. Springer (2016)\n[9] Saouli, R., Akil, M., Kachouri, R., et al.: Fully automatic brain tumor\nsegmentation using end-to-end incremental deep neural networks in mri\nimages. Computer methods and programs in biomedicine 166, 39–49\n(2018)\n[10] Hussein, S., Cao, K., Song, Q., Bagci, U.: Risk Stratiﬁcation of Lung\nNodules Using 3D CNN-Based Multi-task Learning. In: International\nConference on Information Processing in Medical Imaging. pp. 249–\n260. Springer (2017)\n[11] Furuya, K., Murayama, S., Soeda, H., Murakami, J., Ichinose, Y.,\nYauuchi, H., Katsuda, Y., Koga, M., Masuda, K.: New classiﬁcation\nof small pulmonary nodules by margin characteristics on highresolution\nCT. Acta Radiologica 40(5), 496–504 (1999)\n[12] Uchiyama, Y., Katsuragawa, S., Abe, H., Shiraishi, J., Li, F., Li, Q.,\nZhang, C.T., Suzuki, K., Doi, K.: Quantitative computerized analysis of\ndiffuse lung disease in high-resolution computed tomography. Medical\nPhysics 30(9), 2440–2454 (2003)\n[13] Chen, S., Ni, D., Qin, J., Lei, B., Wang, T., Cheng, J.Z.: Bridging\ncomputational features toward multiple semantic features with multi-\ntask regression: A study of CT pulmonary nodules. In: MICCAI. pp.\n53–60. Springer (2016)\n[14] Shen, W., Zhou, M., Yang, F., Yang, C., Tian, J.: Multi-scale convo-\nlutional neural networks for lung nodule classiﬁcation. In: IPMI. pp.\n588–599. Springer (2015)\n[15] Ciompi, F., Chung, K., Van Riel, S.J., Setio, A.A.A., Gerke, P.K., Jacobs,\nC., Scholten, E.T., Schaefer-Prokop, C., Wille, M.M., Marchian`o, A.,\net al.: Towards automatic pulmonary nodule management in lung cancer\nscreening with deep learning. Scientiﬁc reports 7, 46479 (2017)\n[16] Setio, A.A.A., Ciompi, F., Litjens, G., Gerke, P., Jacobs, C., van Riel,\nS.J., Wille, M.M.W., Naqibullah, M., S´anchez, C.I., van Ginneken, B.:\nPulmonary nodule detection in CT images: false positive reduction using\nmulti-view convolutional networks. IEEE TMI 35(5), 1160–1169 (2016)\n[17] Setio, A.A.A., Traverso, A., De Bel, T., Berens, M.S., van den Bogaard,\nC., Cerello, P., Chen, H., Dou, Q., Fantacci, M.E., Geurts, B., et al.:\nValidation, comparison, and combination of algorithms for automatic\ndetection of pulmonary nodules in computed tomography images: the\nluna16 challenge. Medical image analysis 42, 1–13 (2017)\n[18] Khosravan, N., Bagci, U.: S4ND: Single-Shot Single-Scale Lung Nodule\nDetection. arXiv preprint arXiv:1805.02279 (2018)\n[19] Zhou, Y., Xie, L., Fishman, E.K., Yuille, A.L.: Deep Supervision for\nPancreatic Cyst Segmentation in Abdominal CT Scans. arXiv preprint\narXiv:1706.07346 (2017)\n[20] Cai, J., Lu, L., Zhang, Z., Xing, F., Yang, L., Yin, Q.: Pancreas\nsegmentation in MRI using graph-based decision fusion on convolutional\nneural networks. In: MICCAI. pp. 442–450. Springer (2016)\n[21] Hanania, A.N., Bantis, L.E., Feng, Z., Wang, H., Tamm, E.P., Katz,\nM.H., Maitra, A., Koay, E.J.: Quantitative imaging to evaluate malignant\npotential of IPMNs. Oncotarget 7(52), 85776 (2016)\n[22] Gazit, L., Chakraborty, J., Attiyeh, M., Langdon-Embry, L., Allen,\nP.J., Do, R.K., Simpson, A.L.: Quantiﬁcation of CT Images for the\nClassiﬁcation of High-and Low-Risk Pancreatic Cysts. In: SPIE Medical\nImaging. pp. 101340X–101340X. International Society for Optics and\nPhotonics (2017)\n[23] Sivic, J., Russell, B.C., Efros, A.A., Zisserman, A., Freeman, W.T.:\nDiscovering objects and their location in images. In: ICCV. vol. 1, pp.\n370–377. IEEE (2005)\n[24] Kamper, H., Jansen, A., Goldwater, S.: Fully unsupervised small-\nvocabulary speech recognition using a segmental Bayesian model. In:\nInterspeech (2015)\n[25] Lee, H., Pham, P., Largman, Y., Ng, A.Y.: Unsupervised feature learning\nfor audio classiﬁcation using convolutional deep belief networks. In:\nAdvances in neural information processing systems. pp. 1096–1104\n(2009)\n[26] Shin, H.C., Orton, M.R., Collins, D.J., Doran, S.J., Leach, M.O.:\nStacked autoencoders for unsupervised feature learning and multiple\norgan detection in a pilot study using 4d patient data. IEEE transactions\non pattern analysis and machine intelligence 35(8), 1930–1943 (2013)\n[27] Vaidhya, K., Thirunavukkarasu, S., Alex, V., Krishnamurthi, G.: Multi-\nmodal brain tumor segmentation using stacked denoising autoencoders.\nIn: International Workshop on Brainlesion: Glioma, Multiple Sclerosis,\nStroke and Traumatic Brain Injuries. pp. 181–194. Springer (2015)\n[28] Sivakumar, S., Chandrasekar, C.: Lung nodule segmentation through\nunsupervised clustering models. Procedia engineering 38, 3064–3073\n(2012)\n[29] Kallenberg, M., Petersen, K., Nielsen, M., Ng, A.Y., Diao, P., Igel,\nC., Vachon, C.M., Holland, K., Winkel, R.R., Karssemeijer, N., et al.:\nUnsupervised deep learning applied to breast density segmentation and\nmammographic risk scoring. IEEE transactions on medical imaging\n35(5), 1322–1331 (2016)\n[30] Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning\nwith deep convolutional generative adversarial networks. arXiv preprint\narXiv:1511.06434 (2015)\n[31] Zhang, F., Song, Y., Cai, W., Zhou, Y., Fulham, M., Eberl, S., Shan,\nS., Feng, D.: A ranking-based lung nodule image classiﬁcation method\nusing unlabeled image knowledge. In: IEEE ISBI. pp. 1356–1359. IEEE\n(2014)\n[32] Armato III, S., McLennan, G., Bidaut, L., McNitt-Gray, M.F., Meyer,\nC.R., Reeves, A.P., Zhao, B., Aberle, D.R., Henschke, C.I., Hoffman,\nE.A., et al.: The Lung Image Database Consortium (LIDC) and Image\nDatabase Resource Initiative (IDRI): a completed reference database of\nlung nodules on CT scans. Medical Physics 38(2), 915–931 (2011)\n[33] Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning\nspatiotemporal features with 3D convolutional networks. In: ICCV. pp.\n4489–4497. IEEE (2015)\n[34] Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei,\nL.: Large-scale video classiﬁcation with convolutional neural networks.\nIn: IEEE CVPR. pp. 1725–1732 (2014)\n[35] Recht, B., Fazel, M., Parrilo, P.A.: Guaranteed minimum-rank solutions\nof linear matrix equations via nuclear norm minimization. SIAM review\n52(3), 471–501 (2010)\n[36] Zhou, J., Chen, J., Ye, J.: MALSAR: Multi-task learning via structural\nregularization (2012)\n[37] Shalev-Shwartz, S., Tewari, A.: Stochastic methods for l1-regularized\nloss minimization. Journal of Machine Learning Research 12(Jun),\n1865–1892 (2011)\n[38] Nesterov, Y.: Introductory lectures on convex optimization: A basic\ncourse, vol. 87. Springer Science & Business Media (2013)\n[39] Yu, F., Liu, D., Kumar, S., Tony, J., Chang, S.F.: ∝SVM for Learning\nwith Label Proportions. In: Proceedings of The 30th International\nConference on Machine Learning. pp. 504–512 (2013)\n[40] Tustison, N.J., Avants, B.B., Cook, P.A., Zheng, Y., Egan, A., Yushke-\nvich, P.A., Gee, J.C.: N4ITK: Improved N3 bias correction. IEEE\nTransactions on Medical Imaging 29(6), 1310–1320 (2010)\n[41] Oliva, A., Torralba, A.: Modeling the shape of the scene: A holistic\nrepresentation of the spatial envelope. IJCV 42(3), 145–175 (2001)\n[42] Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the\ndevil in the details: Delving deep into convolutional nets. In: British\nMachine Vision Conference (2014)\n[43] He, H., Bai, Y., Garcia, E.A., Li, S.: ADASYN: Adaptive synthetic\nsampling approach for imbalanced learning. In: Neural Networks, 2008.\nIJCNN 2008.(IEEE World Congress on Computational Intelligence).\nIEEE International Joint Conference on. pp. 1322–1328. IEEE (2008)\n[44] Kalb, B., Sarmiento, J.M., Kooby, D.A., Adsay, N.V., Martin, D.R.: MR\nimaging of cystic lesions of the pancreas. Radiographics 29(6), 1749–\n1765 (2009)\n[45] Ma, L., Lu, Z., Shang, L., Li, H.: Multimodal convolutional neural\nnetworks for matching image and sentence. In: IEEE ICCV. pp. 2623–\n2631 (2015)\n[46] Nguyen, A., Yosinski, J., Bengio, Y., Dosovitskiy, A., Clune, J.: Plug &\nplay generative networks: Conditional iterative generation of images in\nlatent space. arXiv preprint arXiv:1612.00005 (2016)\n[47] Larsen, A.B.L., Sønderby, S.K., Larochelle, H., Winther, O.: Autoencod-\ning beyond pixels using a learned similarity metric. In: ICML (2016)\n[48] Springenberg,\nJ.T.:\nUnsupervised\nand\nsemi-supervised\nlearning\nwith\ncategorical\ngenerative\nadversarial\nnetworks.\narXiv\npreprint\narXiv:1511.06390 (2015)\n[49] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel,\nP.: Infogan: Interpretable representation learning by information maxi-\nmizing generative adversarial nets. In: Advances in Neural Information\nProcessing Systems. pp. 2172–2180 (2016)\n",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "q-bio.QM",
    "q-bio.TO"
  ],
  "published": "2018-01-10",
  "updated": "2019-01-18"
}