{
  "id": "http://arxiv.org/abs/1801.05086v1",
  "title": "Autonomous UAV Navigation Using Reinforcement Learning",
  "authors": [
    "Huy X. Pham",
    "Hung M. La",
    "David Feil-Seifer",
    "Luan V. Nguyen"
  ],
  "abstract": "Unmanned aerial vehicles (UAV) are commonly used for missions in unknown\nenvironments, where an exact mathematical model of the environment may not be\navailable. This paper provides a framework for using reinforcement learning to\nallow the UAV to navigate successfully in such environments. We conducted our\nsimulation and real implementation to show how the UAVs can successfully learn\nto navigate through an unknown environment. Technical aspects regarding to\napplying reinforcement learning algorithm to a UAV system and UAV flight\ncontrol were also addressed. This will enable continuing research using a UAV\nwith learning capabilities in more important applications, such as wildfire\nmonitoring, or search and rescue missions.",
  "text": "Autonomous UAV Navigation Using Reinforcement Learning\nHuy X. Pham, Hung. M. La, David Feil-Seifer, Luan V. Nguyen\nAbstract— Unmanned aerial vehicles (UAV) are commonly\nused for missions in unknown environments, where an\nexact mathematical model of the environment may not be\navailable. This paper provides a framework for using rein-\nforcement learning to allow the UAV to navigate successfully\nin such environments. We conducted our simulation and\nreal implementation to show how the UAVs can successfully\nlearn to navigate through an unknown environment. Tech-\nnical aspects regarding to applying reinforcement learning\nalgorithm to a UAV system and UAV ﬂight control were also\naddressed. This will enable continuing research using a UAV\nwith learning capabilities in more important applications,\nsuch as wildﬁre monitoring, or search and rescue missions.\nI. INTRODUCTION\nUsing unmanned aerial vehicles (UAV), or drones,\nin missions involving navigating through unknown en-\nvironment, such as wildﬁre monitoring [1], target track-\ning [2]–[4], or search and rescue [5], is becoming more\nwidespread, as they can host a wide range of sensors\nto measure the environment with relative low operation\ncosts and high ﬂexibility. One issue is that most current\nresearch relies on the accuracy of the model describing\nthe target, or prior knowledge of the environment [6], [7].\nIt is, however, very difﬁcult to attain this in most realistic\nimplementations, since the knowledge and data regarding\nthe environment are normally limited or unavailable.\nUsing reinforcement learning (RL) is a good approach\nto overcome this issue because it allows a UAV or a\nUAV team to learn and navigate through the changing\nenvironment without a model of the environment [8].\nRL algorithms have already been extensively re-\nsearched in UAV applications, as in many other ﬁelds\nof robotics [9], [10]. Many papers focus on applying RL\nalgorithm into UAV control to achieve desired trajectory\ntracking/following. In [11], Faust et al. proposed a frame-\nwork using RL in motion planning for UAV with sus-\npended load to generate trajectories with minimal residual\noscillations. Bou-Ammar et al. [12] used RL algorithm\nwith ﬁtted value iteration to attain stable trajectories for\nUAV maneuvers comparable to model-based feedback\nlinearization controller. A RL-based learning automata\ndesigned by Santos et al. [13] allowed parameters tuning\nfor a PID controller for UAV in a tracking problem, even\nunder adversary weather conditions. Waslander et al. [14]\nproposed a test-bed applying RL for accommodating the\nnonlinear disturbances caused by complex airﬂow in UAV\ncontrol. Other papers discussed problems in improving\nHuy Pham and Luan Nguyen are PhD students, and Dr. Hung La is\nthe director of the Advanced Robotics and Automation (ARA) Labora-\ntory. Dr. David Feil-Seifer is an Assistant Professor of Department of\nComputer Science and Engineering, University of Nevada, Reno, NV\n89557, USA. Corresponding author: Hung La, email: hla@unr.edu\nRL performance in UAV application. Imanberdiyev et\nal. [15] used a platform named TEXPLORE which pro-\ncessed the action selection, model learning, and planning\nphase in parallel to reduce the computational time. Zhang\net al. [16] proposed a geometry-based Q-learning to\nextend the RL-based controller to incorporate the distance\ninformation in the learning, thus lessen the time needed\nfor an UAV to reach a target.\nHowever, to the best of our knowledge, there are not\nmany papers discussing about using RL algorithm for\nUAVs in high-level context, such as navigation, monitor-\ning or other complex task-based applications. Many pa-\npers often did not provide details on the practical aspects\nof implementation of the learning algorithm on physical\nUAV systems. In this paper, we provide a detailed imple-\nmentation of a UAV that can learn to accomplish tasks in\nan unknown environment. Using a simple RL algorithm,\nthe drone can navigate successfully from an arbitrary\nstarting position to a goal position in shortest possible\nway. The main contribution of the paper is to provide a\nframework for applying a RL algorithm to enable UAV to\noperate in such environment. The remaining of the paper\nis organized as follows. Section II provides more detail\non problem formulation, and the approach we use to solve\nthe problem. Basics in RL and how we design the learning\nalgorithm are discussed in section III. We conduct a\nsimulation of our problem on section IV, and provide\ndetails on UAV control in section V. Subsequently, a\ncomprehensive implementation of the algorithm will be\ndiscussed in section VI. Finally, we conclude our paper\nand provide future work in section VII.\nII. PROBLEM FORMULATION\nFig. 1.\nA UAV navigating in closed environment with discretized state\nspace, represented by discrete circles. The red circle is the UAV’s current\nstate, the green circles are the options that the UAV can choose in the\nnext iteration. The goal is marked by a red ﬂag.\nSuppose that we have a closed environment in which\nthe prior information about it is limited. We would like a\narXiv:1801.05086v1  [cs.RO]  16 Jan 2018\nﬂying robot, for example a quadcopter-type UAV, start at\nan arbitrary position to reach a goal that is pre-described\nto the robot (Figure 1). We assume that at any position, the\nUAV can observe its state, i.e. its position. If we have full\ninformation about the environment, for instance, the exact\ndistance to the target or the locations of the obstacles,\na robot motion planning can be constructed based on\nthe model of the environment, and the problem becomes\ncommon. Traditional control methods, such as potential\nﬁeld [17], [18], are available to solve such problem. In\nmany realistic cases, however, building models is not\npossible because the environment is insufﬁciently known,\nor the data of the environment is not available or difﬁcult\nto obtain. Since RL algorithms can rely only on the data\nobtained directly from the system, it is a natural option\nto consider for our problem. In the learning process, the\nagent needs to map the situations it faces to appropriate\nactions so as to maximize a numerical signal, called\nreward, that measures the performance of the agent.\nTo carry out the given task, the UAV must have a\nlearning component to enable it to ﬁnd the way to the\ngoal in an optimal fashion. Based on its current state sk\n(e.g, UAV’s position) and its learning model, the UAV\ndecides the action to the next state sk+1 it wants to\nbe. A desired position then will be taken as input to\nthe position controller, that calculates the control input\nu(t) to a lower-level propellers controller. This low-level\ncontroller will control the motors of the UAV to generate\nthrust force τ to drive it to the desired position. Note\nthat the position controller must be able to overcome the\ncomplex nonlinear dynamics of UAV system, to achieve\nstable trajectories for the UAV when ﬂying, as well as\nhovering in the new state. Figure 2 shows the block\ndiagram of our controller.\nFig. 2.\nReinforcement Learning model.\nIII. REINFORCEMENT LEARNING AND Q LEARNING\nRL becomes popular recently thanks to its capabilities\nin solving learning problem without relying on a model of\nthe environment. The learning model can be described as\nan agent–environment interaction in Figure 3. An agent\nbuilds up its knowledge of the surrounding environment\nby accumulating its experience through interacting with\nthe environment. Assuming that the environment has\nMarkovian property, where the next state and reward of an\nagent only depends on the current state [8]. The learning\nFig. 3.\nReinforcement Learning model.\nmodel can be generalized as a tuple < S, A, T, R >,\nwhere:\n• S is a ﬁnite state list, and sk ∈S is the state of the\nagent at step k;\n• A is a ﬁnite set of actions, and ak ∈A is the action\nthe agent takes at step k;\n• T is the transition probability function, T : S × A ×\nS →[0, 1], is the probability of agent that takes\naction ak to move from state sk to state sk+1. In this\npaper, we consider our problem as a deterministic\nproblem, so as T(sk, ak, sk+1) = 1.\n• R is the reward function: R : S × A →R that\nspeciﬁes the immediate reward of the agent for\ngetting to state sk+1 from sk after taking action ak.\nWe have: R(sk, ak) = rk+1.\nThe objective of the agent is to ﬁnd a course of\nactions based on its states, called a policy, that ultimately\nmaximizes its total amount of reward it receives over time.\nIn each state, a state - action value function Q(sk, ak),\nthat quantiﬁes how good it is to choose an action in\na given state, can be used for the agent to determine\nwhich action to take. The agent can iteratively compute\nthe optimal value of this function, and from which derives\nan optimal policy. In this paper, we apply a popular RL\nalgorithm known as Q-learning [19], in which the agent\ncomputes optimal value function and records them into a\ntabular database, called Q-table. This knowledge can be\nrecalled to decide which action it would take to optimize\nits rewards over the learning episodes. For each iteration,\nthe estimation of the optimal state - action value function\nis updated following the Bellman equation [8]:\nQk+1(sk, ak) ←(1 −α)Qk(sk, ak)\n+ α[rk+1 + γmax\na′ Qk(sk+1, a′)],\n(1)\nwhere 0 ≤α ≤0 and 0 ≤γ ≤0 are learning rate\nand discount factor of the learning algorithm, respectively.\nTo keep balance between exploration and exploitation\nactions, the paper uses a simple policy called ϵ greedy,\nwith 0 < ϵ < 1, as follows:\nπ(s) =\n\n\n\na random action a,\nwith probability ϵ;\na ∈arg max\na′\nQk(sk, a′),\notherwise.\n(2)\nIn order to use Q-learning algorithm, one must deﬁne\nthe set of states S, actions A and rewards R for an agent\nin the system. Since the continuous space is too large to\nguarantee the convergence of the algorithm, in practice,\nnormally these set will be represented as discrete ﬁnite\nsets approximately [20]. In this paper, we consider the\nenvironment as a ﬁnite set of spheres with equal radius\nd, and their centers form a grid. The center of the sphere\nnow represents a discrete location of the environment,\nwhile the radius d is the error deviation from the center.\nIt is assumed that the UAV can generate these spheres for\nany unknown environment. The state of an UAV is then\ndeﬁned as their approximate position in the environment,\nsk ≜c = [xc, yc, zc] ∈S, where xc, yc, zc are the\ncoordinates of the center of a spheres c at time step k.\nFor simplicity, in this paper we will keep the altitude of\nthe UAV as constant to reduce the number of states. The\nenvironment becomes a 2-D environment and the spheres\nnow become circles. The state of the drone at time step k\nis the lateral position of center c of a circle sk = [xc, yc].\nFigure 1 shows the discrete state space of the UAV used\nin this paper.\nIn each state, the UAV can take an action ak from\na set of four possible actions A: heading North, West,\nSouth or East in lateral direction, while maintaining the\nsame altitude. After an action is decided, the UAV will\nchoose an adjacent circle where position is corresponding\nto the selected action. Note that the its new state sk+1 is\nnow associated with the center of the new circle. Figure\n1 shows number of options the UAV can take (in green\ncolor) in a particular state. Note that if the UAV stays in\na state near the border of the environment, and selects an\naction that takes it out of the space, it should stay still\nin the current state. The rewards that an UAV can get\ndepend whether it has reached the pre-described goal G,\nrecognized by the UAV using a speciﬁc landmark, where\nit will get a big reward. Reaching other places that is not\nthe desired goal will result in a small penalty (negative\nreward):\nrk+1 =\n(\n100,\nif sk+1 ≡G\n−1,\notherwise.\n(3)\nIV. CONTROLLER DESIGN AND ALGORITHM\nIn this section, we provide a simple position controller\ndesign to help a quadrotor-type UAV to perform the action\nak to translate from current location sk to new location\nsk+1 and stay hovering over the new state within a small\nerror radius d. Deﬁne pt is the real-time position of the\nUAV at time t, we start with a simple proportional gain\ncontroller:\nu(t) = Kp(p(t) −sk+1) = Kpe(t),\n(4)\nwhere u(t) is the control input, Kp is the proportional\ncontrol gain, and e(t) is the tracking error between real-\ntime position p(t) and desired location sk+1. Because\nof the nonlinear dynamics of the quadrotor [18], we\nexperienced excessive overshoots of the UAV when it\nnavigates from one state to another (Figure 5), making the\nUAV unstable after reaching a state. To overcome this, we\nused a standard PID controller [21] (Figure 4). Although\nthe controller cannot effectively regulate the nonlinearity\nof the system, work such as [22], [23] indicated that\nusing PID controller could still yield relatively good\nstabilization during hovering.\nu(t) = Kpe(t) + Ki\nZ\ne(t)dt + Kd\nde\ndt .\n(5)\nAlgorithm 1: PID + Q-LEARNING.\nInput: Learning parameters: Discount factor γ,\nlearning rate α, number of episode N\nInput: Control parameters: Control gains\nKp, Kp, Kd, error radius d\n1 Initialize Q0(s, a) ←0, ∀s0 ∈S, ∀a0 ∈A;\n2 for episode = 1 : N do\n3\nMeasure initial state s0\n4\nfor k = 0, 1, 2, ... do\n5\nChoose ak from A using policy (2)\n6\nTake action ak that leads to new state sk+1:\n7\nfor t = 0, 1, 2, ... do\n8\nu(t) = Kpe(t) + Ki\nZ\ne(t)dt + Kd\nde\ndt\n9\nuntil ||p(t) −sk+1|| ≤d\n10\nObserve immediate reward rk+1\n11\nUpdate:\n12\nQk+1(sk, ak) ←(1 −α)Qk(sk, ak)\n+ α[rk+1 + γmax\na′ Qk(sk+1, a′)]\n13\nuntil sk+1 ≡G\nFig. 4.\nThe PID control diagram with 3 components: proportional,\nintegral and derivative terms.\nGenerally, the derivative component can help decrease\nthe overshoot and the settling time, while the integral\ncomponent can help decrease the steady-state error, but\ncan cause increasing overshoot. During the tuning pro-\ncess, we increased the Derivative gain while eliminated\nthe Integral component of the PID control to achieve\nstable trajectory. Note that u(t) is calculated in the Inertial\nFig. 5.\nDistance error between the UAV and the target before tuning.\nFig. 6.\nDistance error between the UAV and the target after tuning.\nframe, and should be transformed to the UAV’s Body\nframe before feeding to the propellers controller as linear\nspeed [18]. Figure 6 shows the result after tuning. The\nUAV is now able to remain inside a radius of d =\n0.3m from the desired state. The exact values of these\nparameters will be provided in section VI. Algorithm 1\nshows the PID + Q learning algorithm used in this paper.\nV. SIMULATION\nIn this section, we conducted a simulation on MATLAB\nenvironment to prove the navigation concept using RL.\nWe deﬁned our environment as a 5 by 5 board (Figure\n7). Suppose that the altitude of the UAV was constant,\nit actually had 25 states, from (1, 1) to (5, 5). The UAV\nwas expected to navigate from starting position at (1, 1)\nto goal position at (5, 5) in shortest possible way. Each\nUAV can take four possible actions to navigate: forward,\nbackward, go left, go right. The UAV will have a big\npositive reward of +100 if it reaches the goal position,\notherwise it will take a negative reward (penalty) of -\n1. We chose a learning rate α = 0.1, and discount rate\nγ = 0.9.\nFigure 8 shows the result of our simulation on MAT-\nLAB. It took 39 episodes to train the UAV to ﬁnd out the\nFig. 7.\nThe simulated environment at time step t = 17. Label S shows\nthe original starting point, and Label G shows the goal.\nFig. 8.\nThe time steps taken in each episode of the simulation.\nFig. 9.\nMotion capture system from Motion Analysis.\noptimal course of actions it should take to reach the target\nfrom a certain starting position. The optimal number of\nsteps the UAV should take was 8 steps, resulting in\nreaching the target in shortest possible way.\nVI. IMPLEMENTATION\nFor our real-time implementation, we used a quadrotor\nParrot AR Drone 2.0, and the Motion Capture System\nfrom Motion Analysis [24] installed in our Advanced\nFig. 10.\nMATLAB GUI for control the learning process of the UAV.\nFig. 11.\nNumber of steps in each episode in real implementation.\nRobotics and Automation (ARA) lab (Figure 9). The\nUAV could be controlled by altering the linear/angular\nspeed, and the motion capture system provides the UAV’s\nrelative position inside the room. To carry out the algo-\nrithm, the UAV should be able to transit from one state\nto another, and stay there before taking new action. We\nimplemented the PID controller in section IV to help the\nUAV carry out its action. Obviously, the learning process\nwas a lengthy one. Therefore, to overcome the physical\nconstraint on UAV’s battery life cycle, we also designed a\nGUI on MATLAB to help discretize the learning process\ninto episodes (Figure\n10). For better control of the\nlearning progress, the GUI shows information of the\ncurrent position of the UAV within the environment, the\nsteps the UAV has taken, the current values of Q table, and\nthe result of this episode comparing to previous episodes.\nIt also helped to save the data in case a UAV failure\nhappened, allowing us to continue the learning progress\nafter the disruption.\nWe carried out the experiment using identical param-\neters to the simulation. The UAV operated in a closed\nroom, which is discretized as a 5 by 5 board. It did not\nhave any knowledge of the environment, except that it\nknew when the goal is reached. Given that the altitude of\nthe UAV was kept constant, the environment actually has\n25 states. The objective for the UAV was to start from a\nstarting position at (1, 1) and navigate successfully to the\ngoal state (5, 5) in shortest way. Similar to the simulation,\nthe UAV will have a big positive reward of +100 if it\nreaches the goal position, otherwise it will take a negative\nreward (penalty) of -1. For the learning part, we selected a\nlearning rate α = 0.1, and discount rate γ = 0.9. For the\nUAV’s PID controller, the proportional gain Kp = 0.8,\nderivative gain Kd = 0.9, and integral gain Ki = 0.\nSimilar to our simulation, it took the UAV 38 episodes to\nﬁnd out the optimal course of actions (8 steps) to reach\n(a) t = 1\n(b) t = 2\n(c) t= 3\n(d) t = 4\n(a) t = 5\n(b) t = 6\n(c) t= 7\n(d) t = 8\nFig. 12.\nTrajectory of the UAV during the last episode. It shows that the UAV reaches the target in the shortest possible way.\nto the goal from a certain starting position (Figure 11).\nThe difference between the ﬁrst episode and the last ones\nwas obvious: it took 100 steps for the UAV to reach the\ntarget in the ﬁrst one, while it took only 8 steps in the\nlast ones. Figure 12 shows the optimal trajectory of the\nUAV during the last episode.\nVII. CONCLUSION\nThis paper presented a technique to train a quadrotor\nto learn to navigate to the target point using a PID+\nQ-learning algorithm in an unknown environment. The\nimplementation and simulation outputs similar result,\nshowed that the UAVs can successfully learn to nav-\nigate through the environment without the need of a\nmathematical model. This paper can serve as a simple\nframework for using RL to enable UAVs to work in an en-\nvironment where its model is unavailable. For real-world\ndeployment, we should consider stochastic learning model\nwhere uncertainties, such as wind and other dynamics of\nthe environment, present in the system [4], [25]. In the\nfuture, we will also continue to work on using UAV with\nlearning capabilities in more important application, such\nas wildﬁre monitoring, or search and rescue missions. The\nresearch can be extended into multi-agent systems [26],\n[27], where the learning capabilities can help the UAVs\nto have better coordination and effectiveness in solving\nreal-world problem.\nREFERENCES\n[1] H. X. Pham, H. M. La, D. Feil-Seifer, and M. Deans, “A distributed\ncontrol framework for a team of unmanned aerial vehicles for dy-\nnamic wildﬁre tracking,” arXiv preprint arXiv:1704.02630, 2017.\n[2] A. C. Woods, H. M. La, and Q. P. Ha, “A novel extended potential\nﬁeld controller for use on aerial robots,” in Automation Science\nand Engineering (CASE), 2016 IEEE International Conference on.\nIEEE, 2016, pp. 286–291.\n[3] A. C. Woods and H. M. La, “Dynamic target tracking and obstacle\navoidance using a drone,” in International Symposium on Visual\nComputing.\nSpringer, 2015, pp. 857–866.\n[4] F. Mu˜noz, E. Quesada, E. Steed, H. M. La, S. Salazar, S. Commuri,\nand L. R. Garcia Carrillo, “Adaptive consensus algorithms for\nreal-time operation of multi-agent systems affected by switching\nnetwork events,” International Journal of Robust and Nonlinear\nControl, vol. 27, no. 9, pp. 1566–1588, 2017.\n[5] T. Tomic, K. Schmid, P. Lutz, A. Domel, M. Kassecker, E. Mair,\nI. L. Grixa, F. Ruess, M. Suppa, and D. Burschka, “Toward a\nfully autonomous uav: Research platform for indoor and outdoor\nurban search and rescue,” IEEE robotics & automation magazine,\nvol. 19, no. 3, pp. 46–56, 2012.\n[6] H. M. La, “Multi-robot swarm for cooperative scalar ﬁeld map-\nping,” Handbook of Research on Design, Control, and Modeling\nof Swarm Robotics, p. 383, 2015.\n[7] H. M. La, W. Sheng, and J. Chen, “Cooperative and active\nsensing in mobile sensor networks for scalar ﬁeld mapping,” IEEE\nTransactions on Systems, Man, and Cybernetics: Systems, vol. 45,\nno. 1, pp. 1–12, 2015.\n[8] R. S. Sutton and A. G. Barto, Reinforcement learning: An intro-\nduction.\nMIT press Cambridge, 1998, vol. 1, no. 1.\n[9] H. M. La, R. Lim, and W. Sheng, “Multirobot cooperative learning\nfor predator avoidance,” IEEE Transactions on Control Systems\nTechnology, vol. 23, no. 1, pp. 52–63, 2015.\n[10] H. M. La, R. S. Lim, W. Sheng, and J. Chen, “Cooperative ﬂocking\nand learning in multi-robot systems for predator avoidance,” in\nCyber Technology in Automation, Control and Intelligent Systems\n(CYBER), 2013 IEEE 3rd Annual International Conference on.\nIEEE, 2013, pp. 337–342.\n[11] A. Faust, I. Palunko, P. Cruz, R. Fierro, and L. Tapia, “Learning\nswing-free trajectories for uavs with a suspended load,” in Robotics\nand Automation (ICRA), 2013 IEEE International Conference on.\nIEEE, 2013, pp. 4902–4909.\n[12] H. Bou-Ammar, H. Voos, and W. Ertel, “Controller design for\nquadrotor uavs using reinforcement learning,” in Control Appli-\ncations (CCA), 2010 IEEE International Conference on.\nIEEE,\n2010, pp. 2130–2135.\n[13] S. R. B. dos Santos, C. L. Nascimento, and S. N. Givigi, “Design\nof attitude and path tracking controllers for quad-rotor robots using\nreinforcement learning,” in Aerospace Conference, 2012 IEEE.\nIEEE, 2012, pp. 1–16.\n[14] S. L. Waslander, G. M. Hoffmann, J. S. Jang, and C. J. Tomlin,\n“Multi-agent quadrotor testbed control design: Integral sliding\nmode vs. reinforcement learning,” in Intelligent Robots and Sys-\ntems, 2005.(IROS 2005). 2005 IEEE/RSJ International Conference\non.\nIEEE, 2005, pp. 3712–3717.\n[15] N. Imanberdiyev, C. Fu, E. Kayacan, and I.-M. Chen, “Au-\ntonomous navigation of uav by using real-time model-based rein-\nforcement learning,” in Control, Automation, Robotics and Vision\n(ICARCV), 2016 14th International Conference on.\nIEEE, 2016,\npp. 1–6.\n[16] B. Zhang, Z. Mao, W. Liu, and J. Liu, “Geometric reinforcement\nlearning for path planning of uavs,” Journal of Intelligent &\nRobotic Systems, vol. 77, no. 2, pp. 391–409, 2015.\n[17] S. S. Ge and Y. J. Cui, “Dynamic motion planning for mobile\nrobots using potential ﬁeld method,” Autonomous robots, vol. 13,\nno. 3, pp. 207–222, 2002.\n[18] A. C. Woods and H. M. La, “A novel potential ﬁeld controller for\nuse on aerial robots,” IEEE Transactions on Systems, Man, and\nCybernetics: Systems, 2017.\n[19] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning,\nvol. 8, no. 3-4, pp. 279–292, 1992.\n[20] L. Busoniu, R. Babuska, B. De Schutter, and D. Ernst, Re-\ninforcement learning and dynamic programming using function\napproximators.\nCRC press, 2010, vol. 39.\n[21] R. C. Dorf and R. H. Bishop, Modern control systems.\nPearson,\n2011.\n[22] J. Li and Y. Li, “Dynamic analysis and pid control for a quadro-\ntor,” in Mechatronics and Automation (ICMA), 2011 International\nConference on.\nIEEE, 2011, pp. 573–578.\n[23] K. U. Lee, H. S. Kim, J. B. Park, and Y. H. Choi, “Hovering control\nof a quadrotor,” in Control, Automation and Systems (ICCAS),\n2012 12th International Conference on. IEEE, 2012, pp. 162–167.\n[24] “Motion analysis corporation.” [Online]. Available: https://www.\nmotionanalysis.com/\n[25] H. M. La and W. Sheng, “Flocking control of multiple agents in\nnoisy environments,” in 2010 IEEE International Conference on\nRobotics and Automation, May 2010, pp. 4964–4969.\n[26] ——, “Dynamic target tracking and observing in a mobile\nsensor network,” Robotics and Autonomous Systems, vol. 60,\nno.\n7,\npp.\n996\n–\n1009,\n2012.\n[Online].\nAvailable:\nhttp:\n//www.sciencedirect.com/science/article/pii/S0921889012000565\n[27] ——, “Distributed sensor fusion for scalar ﬁeld mapping using mo-\nbile sensor networks,” IEEE Transactions on Cybernetics, vol. 43,\nno. 2, pp. 766–778, April 2013.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2018-01-16",
  "updated": "2018-01-16"
}