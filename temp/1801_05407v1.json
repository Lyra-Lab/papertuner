{
  "id": "http://arxiv.org/abs/1801.05407v1",
  "title": "Deep Canonically Correlated LSTMs",
  "authors": [
    "Neil Mallinar",
    "Corbin Rosset"
  ],
  "abstract": "We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear\ntransformations of variable length sequences and embed them into a correlated,\nfixed dimensional space. We use LSTMs to transform multi-view time-series data\nnon-linearly while learning temporal relationships within the data. We then\nperform correlation analysis on the outputs of these neural networks to find a\ncorrelated subspace through which we get our final representation via\nprojection. This work follows from previous work done on Deep Canonical\nCorrelation (DCCA), in which deep feed-forward neural networks were used to\nlearn nonlinear transformations of data while maximizing correlation.",
  "text": "Deep Canonically Correlated LSTMs\nMallinar, Neil\nnmallinar@gmail.com\nRosset, Corbin\ncorbyrosset@gmail.com\nAbstract— We examine Deep Canonically Correlated LSTMs\nas a way to learn nonlinear transformations of variable length\nsequences and embed them into a correlated, ﬁxed dimensional\nspace. We use LSTMs to transform multi-view time-series data\nnon-linearly while learning temporal relationships within the\ndata. We then perform correlation analysis on the outputs of\nthese neural networks to ﬁnd a correlated subspace through\nwhich we get our ﬁnal representation via projection. This work\nfollows from previous work done on Deep Canonical Corre-\nlation (DCCA), in which deep feed-forward neural networks\nwere used to learn nonlinear transformations of data while\nmaximizing correlation.\nI. INTRODUCTION\nIt is common in modern data sets to have multiple views\nof data collected of a phenomenon, for instance, a set of\nimages and their captions in text, or audio and video data\nof the same event. If there exist labels, the views are con-\nditionally uncorrelated on them, and it is typically assumed\nthat noise sources between views are uncorrelated so that the\nrepresentations are discriminating of the underlying semantic\ncontent. To distinguish it from multi-modal learning, multi-\nview learning trains a model or classiﬁer for each view, the\napplication of which depends on what data is available at\ntest time. Typically it is desirable to ﬁnd representations for\neach view that are predictive of - and predicted by - the other\nviews so that if one view is not available at test time, it can\nserve to denoise the other views, or serve as a soft supervisor\nproviding pseudo-labels. The beneﬁts of training on multiple\nviews include reduced sample complexity for prediction\nscenarios\n[1], relaxed separation conditions for clustering\n[2], among others. CCA techniques are used successfully\nacross a wide array of downstream tasks (often unsupervised)\nfrom fMRI analysis\n[3], to retrieval, categorization, and\nclustering of text documents\n[4], [5], to acoustic feature\nlearning [6]–[8].\nCanonical Correlation Analysis (CCA) is a widely used\nprocedure motivated by the belief that multiple sources of in-\nformation might ease learnability of useful, low-dimensional\nrepresentations of data [9]. Speciﬁcally, CCA learns linear\nprojections of vectors from two views that are maximally\ncorrelated [10]. While CCA is afﬁne invariant, the represen-\ntations it learns are not sufﬁcient for data that lies on a man-\nifold. Kernel Canonical Correlation Analysis (KCCA) ﬁnds\nsimilar non-linear low dimensional projections of data [11].\nWe apply a scalable1 extension of kernel CCA (KCCA) with\nGaussian and polynomial kernels. However, kernel methods\n1Although the dimension of the XRMB data is on the order of 102, the\nnumber of samples (≈50,000) would force the Gram matrix to exceed\nmemory capacity\nscale poorly in both feature dimension (a dot product or\nnorm is required) and number of samples (Gram matrix is\npolynomial in size).\nTo remedy this, we apply deep methods, which enjoy\nlearning a parametric form of a nonlinear target trans-\nformation. One such method is that of split autoencoders\n(SplitAE)2. This method was originally proposed by Ngiam\net al. in their paper on multimodal autoencoders [12] and\nwas shown to be comparable to correlation-based methods\nby Wang et al. in their survey of multi-view methods\n[13]. Andrew et al. proposed Deep Canonical Correlation\nAnalysis (DCCA) as a deep correlation-based model for\nmulti-view representation learning [14]. DCCA maps various\nviews of data into a lower dimension via deep feed-forward\nneural networks and performs linear CCA on the output of\nthese networks to learn deep correlation representations of\ndata in an unsupervised fashion. The obvious extension to\nthis method was to include the reconstruction objective of\nmultimodal autoencoders with the correlation objective to\nget Deep Canonically Correlated Auto-Encoders (DCCAE),\nwhich outperformed standard DCCA [13].\nHowever, we note that these deep methods for correlation\nanalysis require using ﬁxed length feature vectors, which\nremoving the long-term time dimension from our data. In\nthis paper, we explore the use of Long Short Term Memory\n(LSTM) [15] cells to learn temporal relationships within our\nsequences. Recurrent Neural Networks (RNN’s) have been\nshown to work very well with time-series data, including\nbut not limited to audio and video sequences. LSTM cells\nare extensions of standard RNNs that allow for the network\nto persist what it learns over longer periods of time\n[16].\nThese networks have been shown to perform exceptionally\nwell at tasks such as speech recognition [17], [18], acoustic\nmodeling [19], sentence embedding [20] and more.\nWe present results of a baseline LSTM classiﬁer and how\nwell it clusters phonemes, as well as baseline linear CCA\nwith a classiﬁer on the projected data, and ﬁnally DCC-\nLSTM with a classiﬁer on the ﬁnal projected data. We then\ncompare these results to other deep methods such as SplitAE\nand DCCAE.\nII. BACKGROUND AND RELATED WORK\nHere we will review past work done in correlation analy-\nsis, deep methods for multi-view representation learning, and\nLSTMs in order to provide a solid theoretical justiﬁcation for\nthe methods we are using.\n2As described later, SplitAE reconstructs each view from a shared\nrepresentation, which is not correlation analysis\narXiv:1801.05407v1  [stat.ML]  16 Jan 2018\nA. Canonical Correlation Analysis\nCCA can be interpreted as an extension of principle\ncomponent analysis to multiple data sets with the added\nconstraint that the principle components learned in each\nsubspace are maximally correlated. Concretely, given n\nobservations (xi, yi), xi ∈Rdx and yi ∈Rdy comprising\ntwo views of data X, Y described by an unknown joint\ndistribution D, ﬁnd k pairs of vectors (uj, vj) of the same\ndimensions to\nmax correlation(u⊤\ni x, v⊤\ni y)\n(1)\nsubject to the constraint that (uj, vj) is uncorrelated to all\nother (ur, vr), j ̸= r, 1 ≤j ≤k. After ﬁnding the ﬁrst pair\n(u1, v1), subsequent pairs are found via deﬂation of the views\nsubject to the uncorrelation constraint above. Expanding the\ndeﬁnition of correlation for the vectors ui and vi:\nmax\nui∈Rdx,vi∈Rdy\nE(x,y)∼D\n\u0002\nu⊤\ni xy⊤vi\n\u0003\nq\nEx[u⊤\ni xx⊤ui]Ey[v⊤\ni yy⊤vi]\n(2)\nNote that scaling of ui or vi does not change the objective,\ntherefore the variance of u and v can be normalized to one.\nExpressed equivalently for all u and v to be learned,\nmax\nU∈Rdx×k,V ∈Rdy×kE(x,y)∼D\n\u0002\ntrace\n\u0000U ⊤xy⊤V\n\u0001\u0003\nsubject to\nE[U ⊤xx⊤U] = I\nE[V ⊤yy⊤V ] = I\n(3)\nSolving for u and v using the Lagrange multiplier method,\nlinear CCA takes the form of a generalized eigenvalue\nproblem, for which the solution is a product of covari-\nance3 and cross-covariance matrices\n[21]: U is the top\nk left eigenvectors (sorted by decreasing eigenvalue) of\nC−1\nxx CxyC−1\nyy Cyx, which when regularized becomes (Cxx +\nrxI)−1/2Cxy(Cyy + ryI)−1/2Cyx. The i’th column of V is\nthen chosen as C−1\nyy Cyxui\n√λi\n, which is regularized similarly.\nA second interpretation of CCA is that optimal U and\nV projection matrices minimize the squared error of recon-\nstructing x (respectively, y) given y (resp. x). Hence, the\noptimization problems given in Equations 1, 2, 3, and 4 are\nall equivalent (for i = 1...k, where applicable).\nmin\nU∈Rdx×k,V ∈Rdy×kE(x,y)∼D\n\u0002\n∥U ⊤x −V ⊤y∥2\n2\n\u0003\nsubject to\nE[U ⊤CxxU] = Ik\nE[V ⊤CyyV ] = Ik\n(4)\nThere are a number of settings to which CCA, and all\ncorrelation analysis variants, can be applied. The ﬁrst is when\nall views are available at test and train time. The second is\nwhen only a non-empty subset of the views is available at\ntest time. Either of these settings can be enhanced by labels.\n3For numerical stability, a scaled identity matrix is added to a covariance\nmatrix before it is inverted\nB. Kernel Canonical Correlation Analysis\nKCCA by Akaho et al. generalizes the transformations\nlearned by CCA to nonlinear functions living in a repro-\nducing kernel hilbert space (RKHS), making it suitable for\nmanifold learning. Given F and G are function classes living\nin reproducing kernel hilbert spaces reserved for X and Y\nrespectively, the goal is to ﬁnd two sets of k functions,\n{f1, ..., fk} : Rdx →R, {g1, ..., gk} : Rdy →R such that for\nany i, fi and gi minimize the squared error of reconstructing\neach other in the RKHS. That is, given x ∈X and y ∈Y,\nf and g are maximally predictive of, and predictable by, the\nother,\nmin\nU∈Rdx×k,V ∈Rdy×k\nE(x,y)∼D\n\u0002\n∥fi(x) −gi(y)∥2\n2\n\u0003\nsubject to\nE[fi(x)fj(x)] = δij,\nE[gi(y)gj(y)] = δij\n(5)\nfor δij an indicator random variable that assumes one if\ni = j and zero otherwise. We will also introduce equiv-\nalent interpretations similar to the above: maximize the\ncorrelation of f(x) and g(x) or maximize the covariance\nE(x,y)∼D [fi(x)gi(y)] with the same constraints as Equation\n5. As before, every pair of functions (fj, gj) is uncorrelated\nwith all other pairs of functions for all k.\nFor n data vectors from each of X and Y, the Gram\nmatrices Kx and Ky from Rn×n can be constructed4 given\na positive deﬁnite kernel function κ(·, ·) : X × Y →R\nwhich is by construction equal to the dot product of its\narguments in the feature space5, k(u, v) = Φ(u)⊤Φ(v)\nfor a nonlinear feature map Φ : Rd\n→H . It is a\nproperty of an RKHS that each of its constituent functions\nf and g from each view can be represented as a linear\ncombination of the n observed feature vectors, fi(x) =\nPn\ni=1 α(i)\nj κ(x, xj) = αKx(x). Similarly, gi(y) = βKy(y).\nMirroring arguments from CCA6, minimizing the objective\nin Equation 5 is equivalent to maximizing trace\n\u0000α⊤KxKyβ\n\u0001\nwhere E [fi(x)fj(x)] = 1\nnα⊤\ni K2\nxαj. Absorbing the 1\nn into α,\nEquation 5 can be interpreted as\nmax\nα∈Rn×k,β∈Rn×k\ntrace α⊤KxKyβ\nsubject to\nα⊤K2\nxα = Ik,\nβ⊤K2\nyβ = Ik\n(6)\nAs before, α can be solved as a generalized eigenvalue\nproblem, whose regularized solution is α = K−1\nx KyK−1\ny Kx\nand the ith β vector is β(i) =\n1\nλi (Ky + ryI)−1Kxα. This\nsolution takes O(n3) time and O(n2) space. When k << n,\nit is wasteful to compute all n columns of α and β. Scalable\nimplementations of KCCA employ rank-m approximations\nof the eigendecompositions that give the solutions to α and\nβ.\n4e.g. [Kx]ij = κ(xi, xj). Note also that the matrices are assumed to be\ncentered\n5known as the kernel trick\n6An unconstrained KCCA objective can be written by moving the square\nroot of the product of the constraints in Equation 6 to the denominator and\nremoving the trace operator as in Equation 2.\nThe primary drawback of KCCA is its nonparametric\nform, which requires α to be stored for use on all unseen\nexamples (test sets). Also, computation of dot products for\nthe kernel functions cannot be avoided; O(n2) of these dot\nproducts are needed at training time. The binding of each\ninstance of KCCA to a speciﬁc kernel also limits the function\nclass of our solution. In high dimensional settings, KCCA\nis also susceptible to overﬁtting, that is, learning spurious\ncorrelations. A remedy is to regularize with a scaled identity\nmatrix those Gram matrices to be inverted as in Equation 7.\nThe regularization parameters also need to be tuned.\nmax\nα∈Rn×k,β∈Rn×k\nα⊤KxKyβ\nq\nK(reg)\nx\nK(reg)\ny\n(7)\nwhere\nK(reg)\nx\n= α⊤K2\nxα + rxα⊤Kxα\nK(reg)\ny\n= β⊤K2\nyβ + ryβ⊤Kyβ\nThe regularized solution for α is the top eigenvectors sorted\nby decreasing eigenvalue of the matrix (kx+rxI)−1Ky(Ky+\nryI)−1Kx and the ith β vector is as before. A scalable\napproach to kernel CCA was presented in [22].\nC. Split AutoEncoders\nThe work of Ngiam et al. on multimodal autoencoders\nintroduced a deep network architecture for learning a single\nrepresentation from one or more views of data [12]. This\nwas later followed up by Wang et al. with applications to\nthe Wisconsin X-Ray dataset, as well as a constructed multi-\nview version of the MNIST data set [13]. The goal of deep\nautoencoder architectures in multiview learning is to ﬁnd\nsome shared representation that minimizes the reconstruction\nerror for all views. These deep autoencoders can take as input\none or many views at the same time, depending on which\nare available for testing. For the sake of this paper, we will\nrestrict ourselves to two views.\nThere are two architectures given by Ngiam et al. that ﬁnd\na shared representation of two views [12]. The dataset used\nto train these have two views available at train time and one\nview available at test time, as is the case with the XRMB\ndataset that we are using in this paper.\nmin\nWf ,Wg,Wp,Wq\n1\n2\nN\nX\ni=1\n(∥xi −p(f(xi))∥2 +∥yi −q(g(yi))∥2)\n(8)\nThe ﬁrst architecture trains using information from both\nviews, minimizing Equation 8 which is the sum of the L2\nNorm of the reconstruction for both views. Both inputs are\nfed into the autoencoder separately and go through multiple\nhidden layers before being combined into a shared hidden\nlayer representation of the two views. The decoders are\nthen symmetrical to the encoders. At test time, all of the\nweights from the decoder of the view not available at test\ntime are ignored, and the shared hidden layer representation\ncalculated from the single view available is used.\nmin\nWf ,Wp,Wq\n1\n2\nN\nX\ni=1\n(∥xi−p(f(xi))∥2+∥yi−q(f(xi))∥2) (9)\nThe second architecture, used by Wang et al, has a single\nencoder that takes as input the view available at train time.\nIt then attempts to learn a shared representation and sets of\nweights that can reconstruct both views. The decoder for\nthe view available at test time is symmetric to the encoder.\nThe decoder for the view that’s only available at train time\ncan be a multilayer decoder or a single layer decoder that is\nexperimentally tuned for number of layers and nodes.\nD. Deep Canonical Correlation Analysis\nDeep CCA, introduced by Andrew et al., is a parametric\ntechnique to simultaneously learn nonlinear mappings for\neach view which are maximally correlated. It is similar to\nKCCA in that both are computing nonlinear mappings to\nmaximize canonical correlation between views. However,\nKCCA has a signiﬁcant cost in that KCCA requires large\nkernel matrices which may not often be practical. DCCA on\nthe other hand computes the nonlinear mappings using deep\nneural networks, which are capable of representing nonlinear,\nhigh-level abstractions on top of the input data. The use\nof neural networks makes DCCA much more scalable than\nstandard KCCA, as the size of the network is not tied to the\nsize of the dataset.\nGiven views X and Y , DCCA learns representations F\nand G such that F=f(X) and G=g(Y ) where f and g are\nthe transformations computed by two deep neural networks,\nwhich are described by network weights wf and wg re-\nspectively. DCCA trains f and g according to the following\nobjective, which maximizes the canonical correlation at the\noutput layer between the two views.\nmax\nU,V,wf ,wg\n1\nN tr(U T FGT V )\nsubject to\nU T (FF T\nN\n+ rxI)U = I\nV T (GGT\nN\n+ ryI)V = I\n(10)\nAn alternative objective, that we use in training, for DCCA\ncan be expressed via the centered covariance matrices of\nthe view 1 and view 2 data, ˆΣ11 and ˆΣ22, and the centered\ncross-covariance matrices from the two views, ˆΣ12 and ˆΣ21.\nWe note ﬁrst that DCCA must be trained in minibatches as\nthere is no incremental CCA that is stable in this context and\nso we let m be the batch size and center the transformed\ndata batches via: ¯F = F −\n1\nmF1m×m (resp. G). Deﬁne\nˆΣ11 =\n1\nm−1 ¯F ¯F T +rxI (resp. ˆΣ22). Here we take rx, ry > 0\nto be regularization parameters for the covariance matrices\nof F, G respectively. Then deﬁne ˆΣ12 =\n1\nm−1 ¯F ¯GT . From\nAndrew et al. we deﬁne the matrix T = ˆΣ−1/2\n11\nˆΣ12 ˆΣ−1/2\n22\n.\nIf the number of correlation components, k, that we seek to\ncapture is the same as the output dimensionality, o, of the\nneural networks f, g then we can express correlation as\ncorr(F, G) = tr(T T T)1/2 = tr(ˆΣ21 ˆΣ−1\n11 ˆΣ12 ˆΣ−1\n22 )1/2\n(11)\nwhich is equivalent to the sum of the top k singular values\nof the matrix T.\nFig. 1.\nSchematic of the traditional LSTM cell (forward and bidirectional cell variants), including all element-wise operations, inputs, outputs, and updates.\nE. Long Short Term Memory Cell\nLong Short Term Memory (LSTM) Cells came out of\nrecognition of many of the problems with recurrent neural\nnetworks (RNN). As such, it is a variant of the RNN architec-\nture that solves many of these problems. The purpose of such\nrecurrent neural networks is to handle time-series data in a\nmore natural way, e.g. learning patterns across time. This is\ncalled sequence learning and can be seen as doing sequence-\nto-sequence or sequence-to-ﬁxed dimensional learning.\nThe traditional LSTM cell consists of four networks that\nare trained in parallel. These four networks are: the forget\ngate, the input gate, the cell state, and the output. Of\nparticular note is the cell state, which forms a ”constant error\ncarousel.” The cell state avoids the problem of vanishing\ngradients and forward activation exploding because it is not\nupdated with a gradient and is controlled by the forget and\ninput gates (which are [0, 1] bounded). [16].\nA common variant on LSTMs is to add a peephole\nconnction, in which you allow for the four aforementioned\ngates to look at the previous cell state (as opposed to just the\nprevious time steps output and the current time steps input).\nAnother common extension is to use bidirectional LSTMs\ninstead of simply forward LSTMs. In the forward LSTM\ncell all of the past time steps inﬂuence the current one being\nevaluated; however, in the bidirectional LSTM cell all of the\npast and future time steps inﬂuence the current step being\nevaluated.\nWe will now give a more formal treatment of the LSTM\ncell, using Figure 1 for reference. To update the peephole\nLSTM cell, we must evaluate\nft = σ(Wf[ct−1, ht−1, xt] + bf)\nit = σ(Wi[ct−1, ht−1, xt] + bi)\ngt = tanh(Wg[ht−1, xt] + bg)\nct = ft × ct−1 + it × gt\not = σ(Wo[ct−1, ht−1, xt] + bo)\nht = ot × tanh(ct)\nat every time step.\nThe input to an LSTM cell is [ct−1, ht−1, xt] where ct−1\nis the cell state at the previous time step, ht−1 is the output\nrepresentation of the data at the previous time step, and xt\nis the input data at the current time step. This is then pass\nthrough a two separate sigmoid networks in order to get the\nforget and input gates.\nThe forget gate, ft, is [0, 1] bounded. This represents what\ninformation from the past we want to forget now, where a\nvalue of 0 would mean to completely forget any given piece\nof information (element of the cell state) and a value of\n1 would mean to not forget. The input gate, it, operates\nidentically and is also [0, 1] bounded, but it represents what\nnew information we want to learn.\nNext comes the new candidate values, gt, which are the\nraw updates to the cell state. The ﬁnal step, in order to\nupdate the cell state to the current time step, is to perform\nan element-wise multiplication of the previous cell state with\nthe output of the forget gate and similarly the new candidate\nvalues with the output of the input game. Lastly, an element-\nwise addition is applied to get the new cell state.\nThe input to the cell is fed through a ﬁnal sigmoid layer\nin order to get the output gate layer, ot, which represents the\nraw cell output values. This is the element-wise multiplied\nwith the cell state (under the tanh operator) in order to modu-\nlate the output with the information we’ve learned/forgotten.\nThe bidirectional LSTM cell operates using two forward\nLSTMs that traverse the data in opposite directions and is\nupdated via\nhf\nt = LSTMf(xt, hf\nt−1)\nhb\nt = LSTMb(xt, hb\nt−1)\nht = Wfhf\nt + Wbhb\nt + b\nin which LSTMf and LSTMb are the forward and backward\nLSTMs, and the ﬁnal output representation at the given time\nstep is a weighted linear combination of the two cells outputs.\nIII. DEEP CANONICALLY CORRELATED LSTMS\nIn this paper we show the use of training deep LSTMs\nusing correlation as an unsupervised objective. The Deep\nCanonically\nCorrelated\nLSTM\n(DCC-LSTM)\ncomputes\nlower dimensional representations of (time-series) data using\ndeep LSTM (DLSTM) networks on each view of the data.\nThe ﬁnal representations are ﬁxed at the top layer of the\nFig. 2.\nDCC-LSTM schematic in which two time-series views are fed through deep LSTM networks and optimized using the correlation objective at the\ntop.\nDLSTMs and a linear CCA objective is maximized on the\ntransformed data. We restrict ourselves to two views for the\nsake of this paper and the data we are working with, but the\nnatural extension for more than two views would be to use\nGeneralized Canonical Correlation Analysis (gCCA) on the\noutput layers of the networks. [23].\nA. Implementation\nOne goal of this paper was to implement the correlation\nobject (and therefore DCCA as well as DCC-LSTM) using a\nmore accessible framework, such as Tensorﬂow and Theano.\nThere is one implementation of DCCA available in Theano,\nthat we are aware of, but it contains numerous mistakes in\nthe source code as well in the mathematics of the objective\nfunction (as expressed in Equation 11).\nOur ﬁrst attempt at implementing the correlation objective\nwas in Tensorﬂow. We ran into problems when trying to\ncompute the matrix square root, as there is no Tensorﬂow\noperation for it. We attempted to compute the matrix square\nroot as\nA = V DV T\nA1/2 = V D1/2V T\nwhere D1/2 is the element-wise square root of the eigen-\nvalues of A. However, Tensorﬂow does not have a gradient\ndeﬁned for the eigenvalue decomposition operation and so\nwithout manually deﬁning such a gradient or deﬁning a\nnew Tensorﬂow operation we were unable to implement the\ncorrelation objective.\nThe alternative method would be to break outside of the\ncomputation graph generated by Tensorﬂow, compute linear\nCCA manually, and use those projections in the objective\nfunction. However, this would cause the backpropagation to\nbe disconnected from the actual neural nets that generated\nthe output representations. This is not desirable because\nwe would have to manually specify the gradient values\nat each layer, and backpropogate them, rather than defer\nto Tensorﬂows symbolic differentiation routine (as we had\nhoped, in order to have a cleaner implementation of the\ncorrelation objective).\nOur second attempt at implementing the correlation objec-\ntive was in Theano. We were able to successfully deﬁne the\nmatrix square root as speciﬁc above, and defer to Theanos\ncomputation graph and symbolic differentiation to handle the\nupdates to the neural networks in an optimal fashion.\nIt is worth nothing that the function “theano.scan” is used\nto achieve the recurrence relation for each time step of the\nLSTM, and is not well optimized for the GPU as opposed\nto the CPU. In addition, multiple calls to scan per update\nresults in signiﬁcant overhead that causes DCC-LSTM to\ntake roughly 2-3 times longer to train than DCCA.\nB. Training and Tuning\nCorrelation is a population objective and thus training\nusing minibatches provides a sample correlation value that is\nnot the true population correlation. However, if two assump-\ntions are held then we get a bound on the error between\nthe true population correlation and the sample correlation.\nThis bound shows us that we can train DCCA and DCC-\nLSTM using minibatches and stochastic gradient descent,\nrather than approximation methods such as (limited-memory)\nBFGS or full batch learning [24].\nIn order to properly train DCCA using minibatches, we\nmust ﬁrst ensure that the data instances used to construct\nthe minibatch are sampled independently and are identically\ndistributed (i.i.d.). Secondly, we must ensure there is an\nupper bound on the magnitude of the neural networks. This\nis achieved by using nonlinear activation functions that are\nbounded, such as the sigmoid function or hyperbolic tangent\nfunction. Given these constraints the difference between the\nsample correlation and population correlation, in expectation,\nis bounded by a function of the regularization parameters\nchosen and the minibatch size. As expected, a larger mini-\nbatch size reduces the error.\nFor DCC-LSTM the same bounds hold as the LSTM\ncell uses only hyperbolic tangent and sigmoid activation\nfunctions, which are both bounded. In addition, we construct\nour batches by sampling i.i.d. sequences. This allows us to\ntrain DCC-LSTM using batch gradient descent (BGD), or\nother variants on BGD (such as ADAM, Adadelta, etc.).\nLSTMs are not susceptible to vanishing gradients; how-\never, they are sensitive to exploding gradients. During train-\ning, we have found several ways around this problem. The\nbest methods to deal with this are: gradient clipping (ﬁnd\nbounds on the value of the gradient experimentally); random\nnormal initialization of the weight matrices ∈[0.1, 0.1];\ninitialize weights with orthogonal matrices (eigenvalues lie\non unit circle and determinant is ±1 helps with numerical\nstability).\nLSTMs are trained using backpropagation through time\n(BPTT) or truncated BPTT (TBPTT). To perform BPTT,\neach layer of the LSTM (single LSTM cell) is unfolded\nthrough the number of time steps, n, it is given to create\na linear chain of LSTM cells that represents the same com-\nputation as the loop in a single LSTM cell. After evaluation,\ngradients are calculated at each step along the unfolded\nnetwork. Finally, all of these gradients are averaged and each\ncell is updated with this same average. TBPTT operates the\nsame but rather than averaging gradients over all n time\nsteps, it only averages over the last t < n time steps.\nThe process of BPTT (and TBPTT) is handled through\nTheano when calling the symbolic differentiation routines on\nthe “theano.scan” function that we discussed above. This is\noften where the slow-down that we discussed occurs, in the\nunfolding and averaging (especially when sequence lengths\nare long, as desired for training LSTMs).\nNow, when sampling i.i.d. sequences we have the option\nof using variable length sequences or ﬁxed length sequences.\nThe ﬁnal representation that goes into the minibatch is the\noutput from the last LSTM cell after feeding in the sequence\n(which contains information about the previous n time steps).\nAfter randomly sampling m such sequences, we use their\noutputs as the minibatch and calculate correlation at the top\nlayer. We then perform BPTT at each layer of the LSTMs,\nwith respect to both views.\nThe power of LSTMs can really be seen in this aspect\nof giving it a variable length sequence and receiving a\nﬁxed dimensional representation of that sequence. This is\nparticularly powerful for the problem of speech recognition,\nwhich we study in this paper, as phoneme boundaries are\nusually not ﬁxed and LSTMs can learn these boundaries\nvia training with variable length sequences (in which the\nsequence length is also sampled randomly).\nIV. EXPERIMENTS\nOther than the correlation captured, we evaluated the\nquality of the representations of the two views of data learned\nby the DCC-LSTM using a few common downstream tasks:\nreconstruction of one view’s feature from the other, sequence\nlabeling, and classiﬁcation.\nMethod\n% Test Accuracy\nBaseline K-NN\n79.70\nCCA\n83.20\nKCCA\n77.60\nSplitAE\n84.58\nTABLE I\nACCURACY ACHIEVED WITH BEST PARAMETERS FOR EACH\nCORRELATION ANALYSIS METHOD IMPLEMENTED. FOR CCA AND\nKCCA, THE BEST k WAS FOUND TO 60, AND THE NUMBER OF\nNEIGHBORS FOR K-NN WAS 4. THE SIZE OF THE OUTPUT LAYER OF\nSPLITAE WAS 50. THE AUTHORS BELIEVE THAT KCCA REQUIRES\nMORE TUNING.\nA. Data\nWe use the Wisconsin X-Ray Microbeam (XRMB) dataset\nfor our study. This dataset consists of ﬁfty two speakers, and\neach speaker contains two views. The ﬁrst view consists of\nthirteen mel-frequency cepstral coefﬁcient (MFCC) features\nand their ﬁrst and second derivatives. These features are\nhand-crafted based on the raw audio signal, and the thirteen\nMFCCs are calculated in windows of 10ms each. The thirteen\nMFCCs and ﬁrst and second derivatives then form a thirty-\nnine dimensional vector for a given 10ms frame of audio.\nEach instance in this view is a single frame (∈R39).\nThe second view consists of articulatory measurements\nretrieved from eight pellets placed on the speakers lips,\ntongue, and jaw. Each pellet is measured for vertical and\nhorizontal displacement as the person speaks. So for each\n10ms frame, the second view consists of sixteen total mea-\nsurements (∈R16).\nAs each phoneme is not contained wholly in 10ms of\naudio, we stack seven frames for context. Therefore there\nis a central frame which contains the label of the phoneme\nand three frames of left context and three frames of right\ncontext. So our MFCC feature vectors are x1 ∈R273 and\nour articulatory feature vectors are x2 ∈R112.\nFor the sake of this paper, we subsample the speakers\nfor quicker train times. We use the ﬁrst four speakers7\nfor unsupervised training. The test speakers8, referred to as\n”downstream speakers”, were held out. Their data was trans-\nformed into the feature space using the parameters learned\nduring training. We then use the downstream speakers to\nevaluate additional metrics.\nB. Classiﬁcation and Clustering\nFor CCA, KCCA, and SplitAE we computed clusters\nusing t-SNE in order to see how well the three methods\npreserve phoneme clusters. We have presented the three\ncluster charts in 3 based on the test data that was transformed\nby the aforementioned methods. Using the same test data, we\nperformed k-NN classiﬁcation and give the results in Table\n7JW{11, 12, 13, 14}\n8JW{18, 29, 33, 34, 36, 42, 45, 48, 51, 53, 58, 60}\nFig. 3.\nPhoneme clusters as computed via CCA, KCCA, and SplitAE\nBaseline\nDCC-LSTM\nCorrelation Captured\n15.1242\n24.2\nCorrelation of Top 20 Components\n7.6268\n11.2986\nSum of Distances to Nearest Neighbors\n15,778\n3,296\nReconstruction Error (L2 norm)\n43,949\n26,285\nPer-Sample Reconstruction Error\n4.395\n2.6286\n** Per-Vector-Per-Component Error\n0.21975\n0.13143\nTABLE II\nNEAREST NEIGHBOR RECONSTRUCTION TASK: RECONSTRUCT\nARTICULATORY TEST DATA FROM ACOUSTIC TEST DATA IN 20-DIM\nSHARED CCA SPACE (10K RANDOM TEST SAMPLES)\nI. For KCCA, we did not use approximation methods in\norder to compute the kernel matrices and so only provide this\nanalysis for the sake of completeness. It is computationally\nintractable to compute kernel matrices on large datasets\nwithout approximation methods (such as Fourier features\nor Nystrom approximation). This is the reason why KCCA\nperforms worse at classiﬁcation as compared to the baseline,\nCCA, and SplitAE.\nAs we can see, however, correlation methods perform bet-\nter than the baseline. In addition, SplitAE shows the power\nof deep neural networks in unsupervised learning and so it\nis clear that deep neural networks trained with correlation\n(DCCA and DCC-LSTM) would outperform CCA, KCCA,\nand SplitAE.\nC. Reconstruction\nThe downstream speakers were further split into a 10,000\nsample test set, while the remaining were train. Linear\nCCA was performed on the feature space representations\nof the train samples, projecting both of their views into\na shared CCA space. Because articulatory information is\nrarely available at test time, we reconstructed the 10,000\narticulatory vectors from their nearest neighbors in the CCA\nspace and recorded the average ℓ2 reconstruction error.\nTable II shows the comparison between baseline recon-\nstruction and DCC-LSTM reconstruction. As we can see,\nthe data transformed by DCC-LSTM signiﬁcantly improves\non the reconstruction error. Additionally, we can see from\nthis table that the top twenty correlated components capture\nabout half of the total correlation.\nV. CONCLUSION\nIt’s not immediately clear that DCCA and DCC-LSTM\nare comparable. DCC-LSTM requires contiguous frames to\nlearn, and so randomization must be done with sequences\nwhereas DCCA randomizes the frames it learns from in\nbatches. XRMB data has lots of spaces/pauses which end\nup having high correlation as nothing is happening in the\narticulatory feature space when there is a space. We’ve\nseen that DCCA performs well when trained on data that\nhas spaces/pauses removed, whereas DCC-LSTM cannot be\ntrained on such a dataset.\nWith ﬁfty output dimensions and one hidden layer, DCCA\nis able to capture 35±5 correlation per speaker (view 1 and\nview 2 hidden widths of 1800 and 1200, respectively). DCC-\nLSTM is able to capture 27±3 correlation per speaker (view\n1 and view 2 hidden widths of 400 each). It’s clear that\nDCC-LSTM is able to capture correlation, and be trained\nin this unsupervised manner. In addition, we are able to\ncapture quite a bit of correlation with signiﬁcantly less\nparameters/network width and depth. This is an indication\nthat with enough parameter tuning and training, DCC-LSTM\nhas the potential to outperform DCCA on time-series multi-\nview data.\nThere is much work to be done still, in ﬁguring out the\nefﬁcacy of DCC-LSTM. We hope to have given enough\nproof in this paper to warrant exploring this method further.\nCurrent attempts to train LSTMs in an unsupervised fashion\nuse reconstruction as the objective, nobody has attempted to\nuse correlation as of yet. This is an important result as we\nsee that it is possible to use correlation. In addition, there\nis an extension of DCCA called DCCAE (Deep Canonically\nCorrelated AutoEncoders) in which the correlation objective\nis added to the reconstruction objective. It is seen to do much\nbetter than DCCA, so the natural extension for DCC-LSTM\nwould be to add the sequence reconstruction or sequence\nprediction objective into the correlation objective.\nREFERENCES\n[1] S. M. Kakade and D. P. Foster, “Multi-view regression via canonical\ncorrelation analysis,” in Learning theory.\nSpringer, 2007, pp. 82–96.\n[2] K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan, “Multi-\nview clustering via canonical correlation analysis,” in Proceedings of\nthe 26th annual international conference on machine learning. ACM,\n2009, pp. 129–136.\n[3] D. R. Hardoon, J. Mourao-Miranda, M. Brammer, and J. Shawe-\nTaylor, “Unsupervised analysis of fmri data using kernel canonical\ncorrelation,” NeuroImage, vol. 37, no. 4, pp. 1250–1259, 2007.\n[4] A. Vinokourov, N. Cristianini, and J. S. Shawe-Taylor, “Inferring a\nsemantic representation of text via cross-language correlation analy-\nsis,” in Advances in neural information processing systems, 2002, pp.\n1473–1480.\n[5] A. Benton, R. Arora, and M. Dredze, “Learning multiview embeddings\nof twitter users,” in Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics, vol. 2, 2016, pp. 14–19.\n[6] S. Bharadwaj, R. Arora, K. Livescu, and M. Hasegawa-Johnson,\n“Multiview acoustic feature learning using articulatory measurements,”\nin Intl. Workshop on Stat. Machine Learning for Speech Recognition,\n2012.\n[7] R. Arora and K. Livescu, “Multi-view cca-based acoustic features\nfor phonetic recognition across speakers and domains,” in Acoustics,\nSpeech and Signal Processing (ICASSP), 2013 IEEE International\nConference on.\nIEEE, 2013, pp. 7135–7139.\n[8] W. Wang, R. Arora, K. Livescu, and J. A. Bilmes, “Unsupervised\nlearning of acoustic features via deep canonical correlation analysis,”\nin Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE\nInternational Conference on.\nIEEE, 2015, pp. 4590–4594.\n[9] H. Hotelling, “Canonical correlation analysis (cca),” Journal of Edu-\ncational Psychology, 1935.\n[10] ——, “Relations between two sets of variates,” Biometrika, vol. 28,\nno. 3/4, pp. 321–377, 1936.\n[11] S. Akaho, “A kernel method for canonical correlation analysis,” arXiv\npreprint cs/0609071, 2006.\n[12] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\n“Multimodal deep learning,” in Proceedings of the 28th international\nconference on machine learning (ICML-11), 2011, pp. 689–696.\n[13] W. Wang, R. Arora, K. Livescu, and J. Bilmes, “On deep multi-\nview representation learning,” in Proceedings of the 32nd International\nConference on Machine Learning (ICML-15), 2015, pp. 1083–1092.\n[14] G. Andrew, R. Arora, J. Bilmes, and K. Livescu, “Deep canonical cor-\nrelation analysis,” in Proceedings of the 30th International Conference\non Machine Learning, 2013, pp. 1247–1255.\n[15] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[16] F. Gers, “Long short-term memory in recurrent neural networks,”\nPh.D. dissertation, Universit¨at Hannover, 2001.\n[17] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep\nspeech: Scaling up end-to-end speech recognition,” arXiv preprint\narXiv:1412.5567, 2014.\n[18] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory based\nrecurrent neural network architectures for large vocabulary speech\nrecognition,” arXiv preprint arXiv:1402.1128, 2014.\n[19] H. Sak, A. W. Senior, and F. Beaufays, “Long short-term memory\nrecurrent neural network architectures for large scale acoustic model-\ning.” in INTERSPEECH, 2014, pp. 338–342.\n[20] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song,\nand R. Ward, “Deep sentence embedding using the long short term\nmemory network: Analysis and application to information retrieval,”\narXiv preprint arXiv:1502.06922, 2015.\n[21] P. Rastogi, B. Van Durme, and R. Arora, “Multiview lsa: Representa-\ntion learning via generalized cca,” in Proceedings of NAACL, 2015.\n[22] R. Arora and K. Livescu, “Kernel cca for multi-view acoustic feature\nlearning using articulatory measurements,” Proceedings of the MLSLP,\n2012.\n[23] A. Tenenhaus and M. Tenenhaus, “Regularized generalized canonical\ncorrelation analysis,” Psychometrika, vol. 76, no. 2, pp. 257–284,\n2011.\n[24] W. Wang, R. Arora, K. Livescu, and J. Bilmes, “On Deep Multi-\nView Representation Learning: Objectives and Optimization,” ArXiv\ne-prints, Feb. 2016.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-01-16",
  "updated": "2018-01-16"
}