{
  "id": "http://arxiv.org/abs/1801.06889v3",
  "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers",
  "authors": [
    "Fred Hohman",
    "Minsuk Kahng",
    "Robert Pienta",
    "Duen Horng Chau"
  ],
  "abstract": "Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.",
  "text": "1\nVisual Analytics in Deep Learning:\nAn Interrogative Survey for the Next Frontiers\nFred Hohman, Member, IEEE, Minsuk Kahng, Member, IEEE, Robert Pienta, Member, IEEE,\nand Duen Horng Chau, Member, IEEE\nAbstract—Deep learning has recently seen rapid development and received signiﬁcant attention due to its state-of-the-art\nperformance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural\nnetworks, the underlying decision making processes for why these models are achieving such performance are challenging and\nsometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of\ndeep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance.\nStandardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been\ndeveloped to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual\nanalytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art\nusing a human-centered interrogative framework, focusing on the Five W’s and How (Why, Who, What, How, When, and Where). We\nconclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both\nvisual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans\na diverse range of domains.\nIndex Terms—Deep learning, visual analytics, information visualization, neural networks\n!\n1\nINTRODUCTION\nD\nEEP learning is a speciﬁc set of techniques from the\nbroader ﬁeld of machine learning (ML) that focus on\nthe study and usage of deep artiﬁcial neural networks to\nlearn structured representations of data. First mentioned\nas early as the 1940s [1], artiﬁcial neural networks have\na rich history [2], and have recently seen a dominate and\npervasive resurgence [3], [4], [5] in many research domains\nby producing state-of-the-art results [6], [7] on a number of\ndiverse big data tasks [8], [9]. For example, the premiere ma-\nchine learning, deep learning, and artiﬁcial intelligence (AI)\nconferences have seen enormous growth in attendance and\npaper submissions since early 2010s. Furthermore, open-\nsource toolkits and programming libraries for building,\ntraining, and evaluating deep neural networks have become\nmore robust and easy to use, democratizing deep learning.\nAs a result, the barrier to developing deep learning models\nis lower than ever before and deep learning applications are\nbecoming pervasive.\nWhile this technological progress is impressive, it comes\nwith unique and novel challenges. For example, the lack\nof interpretability and transparency of neural networks,\nfrom the learned representations to the underlying decision\nprocess, is an important problem to address. Making sense\nof why a particular model misclassiﬁes test data instances\nor behaves poorly at times is a challenging task for model\ndevelopers. Similarly, end-users interacting with an applica-\ntion that relies on deep learning to make critical decisions\nmay question its reliability if no explanation is given by the\nmodel, or become bafﬂed if the explanation is convoluted.\n•\nF. Hohman, M. Kahng, R. Pienta, and D. H. Chau are with the College of\nComputing, Georgia Tech, Atlanta, Georgia 30332, U.S.A.\nE-mail: {fredhohman, kahng, pientars, polo}@gatech.edu\nWhile explaining neural network decisions is important,\nthere are numerous other problems that arise from deep\nlearning, such as AI safety and security (e.g., when using\nmodels in applications such as self-driving vehicles), and\ncompromised trust due to bias in models and datasets, just\nto name a few. These challenges are often compounded, due\nto the large datasets required to train most deep learning\nmodels. As worrisome as these problems are, they will\nlikely become even more widespread as more AI-powered\nsystems are deployed in the world. Therefore, a general\nsense of model understanding is not only beneﬁcial, but\noften required to address the aforementioned issues.\nData visualization and visual analytics excel at knowl-\nedge communication and insight discovery by using encod-\nings to transform abstract data into meaningful represen-\ntations. In the seminal work by Zeiler and Fergus [10], a\ntechnique called deconvolutional networks enabled projection\nfrom a model’s learned feature space back to the pixel space.\nTheir technique and results give insight into what types\nof features deep neural networks are learning at speciﬁc\nlayers, and also serve as a debugging tool for improving\na model. This work is often credited for popularizing vi-\nsualization in the machine learning and computer vision\ncommunities in recent years, putting a spotlight on it as\na powerful tool that helps people understand and improve\ndeep learning models. However, visualization research for\nneural networks started well before [11], [12], [13]. Over\njust a handful of years, many different techniques have\nbeen introduced to help interpret what neural networks are\nlearning. Many such techniques generate static images, such\nas attention maps and heatmaps for image classiﬁcation,\nindicating which parts of an image are most important\nto the classiﬁcation. However, interaction has also been\narXiv:1801.06889v3  [cs.HC]  14 May 2018\n2\nWHEN \nWhen in the deep learning \nprocess is visualization used? \nDuring Training \nAfter Training\n§8\n?\nWHAT \nWhat data, features, and relationships \nin deep learning can be visualized? \nComputational Graph & Network Architecture \nLearned Model Parameters \nIndividual Computational Units \nNeurons In High-dimensional Space \nAggregated Information\nWHO \nWho would use and benefit \nfrom visualizing deep learning? \nModel Developers & Builders \nModel Users \nNon-experts\nHOW \nHow can we visualize deep learning \ndata, features, and relationships? \nNode-link Diagrams for Network Architecture \nDimensionality Reduction & Scatter Plots \nLine Charts for Temporal Metrics  \nInstance-based Analysis & Exploration \nInteractive Experimentation \nAlgorithms for Attribution & Feature Visualization\nWHY \nWhy would one want to use  \nvisualization in deep learning? \nInterpretability & Explainability \nDebugging & Improving Models \nComparing & Selecting Models \nTeaching Deep Learning Concepts\nWHERE \nWhere has deep learning \nvisualization been used? \nApplication Domains & Models \nA Vibrant Research Community\nVisual Analytics in Deep Learning\n§5\n§6\n§7\n§9\nInterrogative Survey Overview\n§4\nFig. 1. A visual overview of our interrogative survey, and how each of the six questions, ”Why, Who, What, How, When, and Where,” relate to one\nanother. Each question corresponds to one section of this survey, indicated by the numbered tag, near each question title. Each section lists its\nmajor subsections discussed in the survey.\nincorporated into the model understanding process in visual\nanalytics tools to help people gain insight [14], [15], [16].\nThis hybrid research area has grown in both academia and\nindustry, forming the basis for many new research papers,\nacademic workshops, and deployed industry tools.\nIn this survey, we summarize a large number of deep\nlearning visualization works using the Five W’s and How\n(Why, Who, What, How, When, and Where). Figure 1\npresents a visual overview of how these interrogative ques-\ntions reveal and organize the various facets of deep learning\nvisualization research and their related topics. By framing\nthe survey in this way, many existing works ﬁt a description\nas the following ﬁctional example:\nTo interpret representations learned by deep models\n(why), model developers (who) visualize neuron activa-\ntions in convolutional neural networks (what) using t-\nSNE embeddings (how) after the training phase (when)\nto solve an urban planning problem (where).\nThis framing captures the needs, audience, and techniques\nof deep learning visualization, and positions new work’s\ncontributions in the context of existing literature.\nWe conclude by highlighting prominent research direc-\ntions and open problems. We hope that this survey acts as\na companion text for researchers and practitioners wishing\nto understand how visualization supports deep learning\nresearch and applications.\n2\nOUR CONTRIBUTIONS & METHOD OF SURVEY\n2.1\nOur Contributions\nC1. We present a comprehensive, timely survey on visual-\nization and visual analytics in deep learning research,\nusing a human-centered, interrogative framework. This\nmethod enables us to position each work with respect\nto its Five Ws and How (Why, Who, What, How, When,\nand Where), and ﬂexibly discuss and highlight existing\nworks’ multifaceted contributions.\n• Our human-centered approach using the Five W’s\nand How — based on how we familiarize ourselves\nwith new topics in everyday settings — enables\nreaders to quickly grasp important facets of this\nyoung and rapidly growing body of research.\n• Our interrogative process provides a framework to\ndescribe existing works, as well as a model to base\nnew work off of.\nC2. To highlight and align the cross-cutting impact that\nvisual analytics has had on deep learning across a\nbroad range of domains, our survey goes beyond\nvisualization-focused venues, extending a wide scope\nthat encompasses most relevant works from many top\nvenues in artiﬁcial intelligence, machine learning, deep\nlearning, and computer vision. We highlight how visual\nanalytics has been an integral component in solving\nsome of AI’s biggest modern problems, such as neural\nnetwork interpretability, trust, and security.\n3\nC3. As deep learning, and more generally AI, touches more\naspects of our daily lives, we highlight important re-\nsearch directions and open problems that we distilled\nfrom the survey. These include improving the capa-\nbilities of visual analytics systems for furthering in-\nterpretability, conducting more effective design studies\nfor evaluating system usability and utility, advocating\nhumans’ important roles in AI-powered systems, and\npromoting proper and ethical use of AI applications to\nbeneﬁt society.\n2.2\nSurvey Methodology & Summarization Process\nWe selected existing works from top computer science jour-\nnals and conferences in visualization (e.g., IEEE Transac-\ntions on Visualization and Computer Graphics (TVCG)),\nvisual analytics (e.g., IEEE Conference on Visual Analyt-\nics Science and Technology (VAST)) and deep learning\n(e.g., Conference on Neural Information Processing Sys-\ntems (NIPS) and the International Conference on Machine\nLearning (ICML)). Since deep learning visualization is rel-\natively new, much of the relevant work has appeared in\nworkshops at the previously mentioned venues; therefore,\nwe also include those works in our survey. Table 1 lists\nsome of the most prominent publication venues and their\nacronyms. We also inspected preprints posted on arXiv\n(https://arxiv.org/), an open-access, electronic repository\nof manuscript preprints, whose computer science subject\nhas become a hub for new deep learning research. Finally,\naside from the traditional aforementioned venues, we in-\nclude non-academic venues with signiﬁcant attention such\nas Distill, industry lab research blogs, and research blogs\nof inﬂuential ﬁgures. Because of the rapid growth of deep\nlearning research and the lack of a perfect ﬁt for publishing\nand disseminating work in this hybrid area, therefore, the\ninclusion of these non-traditional sources are important to\nreview, as they are highly inﬂuential and impactful to the\nﬁeld.\nVisualization takes many forms throughout the deep\nlearning literature. This survey focuses on visual analytics\nfor deep learning. We also include related works from\nthe AI and computer vision communities that contribute\nnovel static visualizations. So far, the majority of work\nsurrounds convolutional neural networks (CNNs) and im-\nage data; more recent work has begun to visualize other\nmodels, e.g., recurrent neural networks (RNNs), long short-\nterm memory units (LSTMs), and generative adversarial\nnetworks (GANs). For each work, we recorded the following\ninformation if present:\n• Metadata (title, authors, venue, and year published)\n• General approach and short summary\n• Explicit contributions\n• Future work\n• Design component (e.g. user-centered design method-\nologies, interviews, evaluation)\n• Industry involvement and open-source code\nWith this information, we used the Five W’s and How\n(Why, Who, What, How, When, and Where) to organize\nthese existing works and the current state-of-the-art of vi-\nsualization and visual analytics in deep learning.\nTABLE 1\nRelevant visualization and AI venues, in the order of: journals,\nconferences, workshops, open access journals, and preprint\nrepositories. In each category, visualization venues precede AI venues.\nTVCG\nIEEE Transactions on Visualization and Computer Graphics\nVAST\nIEEE Conference on Visual Analytics Science and Technology\nInfoVis\nIEEE Information Visualization\nVIS\nIEEE Visualization Conference (VAST+InfoVis+SciVis)\nCHI\nACM Conference on Human Factors in Computing Systems\nNIPS\nConference on Neural Information Processing Systems\nICML\nInternational Conference on Machine Learning\nCVPR\nConference on Computer Vision and Pattern Recognition\nICLR\nInternational Conference on Learning Representations\nVADL\nIEEE VIS Workshop on Visual Analytics for Deep Learning\nHCML\nCHI Workshop on Human Centered Machine Learning\nIDEA\nKDD Workshop on Interactive Data Exploration & Analytics\nICML Workshop on Visualization for Deep Learning\nWHI\nICML Workshop on Human Interpretability in ML\nNIPS Workshop on Interpreting, Explaining and Visualizing\nDeep Learning\nNIPS Interpretable ML Symposium\nFILM\nNIPS Workshop on Future of Interactive Learning Machines\nACCV Workshop on Interpretation and Visualization of Deep\nNeural Nets\nICANN Workshop on Machine Learning and Interpretability\nDistill\nDistill: Journal for Supporting Clarity in Machine Learning\narXiv\narXiv.org e-Print Archive\n2.3\nRelated Surveys\nWhile there is a larger literature for visualization for ma-\nchine learning, including predictive visual analytics [17],\n[18], [19] and human-in-the-loop interactive machine learn-\ning [20], [21], to our knowledge there is no comprehen-\nsive survey of visualization and visual analytics for deep\nlearning. Regarding deep neural networks, related surveys\ninclude a recent book chapter that discusses visualization of\ndeep neural networks related to the ﬁeld of computer vi-\nsion [22], an unpublished essay that proposes a preliminary\ntaxonomy for visualization techniques [23], and an article\nthat focuses on describing interactive model analysis, which\nmentions deep learning in a few contexts while describing\na high-level framework for general machine learning mod-\nels [24]. A recent overview article by Choo and Liu [25] is\nthe closest in spirit to our survey. Our survey provides wider\ncoverage and more detailed analysis of the literature.\nDifferent from all the related articles mentioned above,\nour survey provides a comprehensive, human-centered, and\ninterrogative framework to describe deep learning visual\nanalytics tools, discusses the new, rapidly growing commu-\nnity at large, and presents the major research trajectories\nsynthesized from existing literature.\n4\n2.4\nSurvey Overview & Organization\nSection 3 introduces common deep learning terminology.\nFigure 1 shows a visual overview of this survey’s struc-\nture and Table 2 summarizes representative works. Each\ninterrogative question (Why, Who, What, How, When, and\nWhere) is given its own section for discussion, ordered to\nbest motivate why visualization and visual analytics in deep\nlearning is such a rich and exciting area of research.\n§§§ 4\nWhy do we want to visualize deep learning?\nWhy and for what purpose would one want to use\nvisualization in deep learning?\n§§§ 5\nWho wants to visualize deep learning?\nWho are the types of people and users that would use\nand stand to beneﬁt from visualizing deep learning?\n§§§ 6\nWhat can we visualize in deep learning?\nWhat data, features, and relationships are inherent to\ndeep learning that can be visualized?\n§§§ 7\nHow can we visualize deep learning?\nHow can we visualize the aforementioned data, fea-\ntures, and relationships?\n§§§ 8\nWhen can we visualize deep learning?\nWhen in the deep learning process is visualization\nused and best suited?\n§§§ 9\nWhere is deep learning visualization being used?\nWhere has deep learning visualization been used?\nSection 10 presents research directions and open prob-\nlems that we gathered and distilled from the literature\nsurvey. Section 11 concludes the survey.\n3\nCOMMON TERMINOLOGY\nTo enhance readability of this survey, and to provide quick\nreferences for readers new to deep learning, we have tab-\nulated a sample of relevant and common deep learning\nterminology used in this work, shown in Table 3. The reader\nmay want to refer to Table 3 throughout this survey for\ntechnical terms, meanings, and synonyms used in various\ncontexts of discussion. The table serves as an introduction\nand summarization of the state-of-the-art. For deﬁnitive\ntechnical and mathematical descriptions, we encourage the\nreader to refer to excellent texts on deep learning and neural\nnetwork design, such as the Deep Learning textbook [26].\n4\nWHY VISUALIZE DEEP LEARNING\n4.1\nInterpretability & Explainability\nThe most abundant, and to some, the most important reason\nwhy people want to visualize deep learning is to under-\nstand how deep learning models make decisions and what\nrepresentations they have learned, so we can place trust in\na model [60]. This notion of general model understanding\nhas been called the interpretability or explainability when\nreferring to machine learning models [60], [61], [62]. How-\never, neural networks particularly suffer from this problem\nsince oftentimes real world and high-performance models\ncontain a large number of parameters (in the millions) and\nexhibit extreme internal complexity by using many non-\nlinear transformations at different stages during training.\nMany works motivate this problem by using phrases such\nas “opening and peering through the black-box,” “trans-\nparency,” and “interpretable neural networks,” [13], [56],\n[63], referring the internal complexity of neural networks.\n4.1.1\nDiscordant Deﬁnitions for Interpretability\nUnfortunately, there is no universally formalized and agreed\nupon deﬁnition for explainability and interpretability in\ndeep learning, which makes classifying and qualifying inter-\npretations and explanations troublesome. In Lipton’s work\n“The Mythos of Model Interpretability [60],” he surveys\ninterpretability-related literature, and discovers diverse mo-\ntivations for why interpretability is important and is oc-\ncasionally discordant. Despite this ambiguity, he attempts\nto reﬁne the notion of interpretability by making a ﬁrst\nstep towards providing a comprehensive taxonomy of both\nthe desiderata and methods in interpretability research.\nOne important point that Lipton makes is the difference\nbetween interpretability and an explanation; an explanation\ncan show predictions without elucidating the mechanisms\nby which models work [60].\nIn another work originally presented as a tutorial at the\nInternational Conference on Acoustics, Speech, and Signal\nProcessing by Montavona et al. [61], the authors propose ex-\nact deﬁnitions of both an interpretation and an explanation.\nFirst, an interpretation is “the mapping of an abstract con-\ncept (e.g., a predicted class) into a domain that the human\ncan make sense of.” They then provide some examples of\ninterpretable domains, such as images (arrays of pixels) and\ntext (sequences of words), and noninterpretable domains,\nsuch as abstract vector spaces (word embeddings). Second,\nan explanation is “the collection of features of the inter-\npretable domain, that have contributed for a given example\nto produce a decision (e.g., classiﬁcation or regression).”\nFor example, an explanation can be a heatmap highlighting\nwhich pixels of the input image most strongly support an\nimage classiﬁcation decision, or in natural language process-\ning, explanations can highlight certain phrases of text.\nHowever, both of the previous works are written by\nmembers of the AI community, whereas work by Miller\ntitled “Explanation in Artiﬁcial Intelligence: Insights from\nthe Social Sciences” [62] postulates that much of the current\nresearch uses only AI researchers’ intuition of what consti-\ntutes a “good” explanation. He suggests that if the focus\non explaining decisions or actions to a human observer is\nthe goal, then if these techniques are to succeed, the expla-\nnations they generate should have a structure that humans\naccept. Much of Miller’s work highlights vast and valuable\nbodies of research in philosophy, psychology, and cognitive\nscience for how people deﬁne, generate, select, evaluate,\nand present explanations, and he argues that interpretability\nand explainability research should leverage and build upon\nthis history [62]. In another essay, Offert [64] argues that to\nmake interpretability more rigorous, we must ﬁrst identify\nwhere it is impaired by intuitive considerations. That is, we\nhave to “consider it precisely in terms of what it is not.”\nWhile multiple works bring different perspectives, Lipton\nmakes the keen observation that for the ﬁeld to progress,\nthe community must critically engage with this problem\nformulation issue [60]. Further research will help solidify\nthe notions of interpretation and explanation.\n5\nTABLE 2\nOverview of representative works in visual analytics for deep learning. Each row is one work; works are sorted alphabetically by ﬁrst author’s last\nname. Each column corresponds to a subsection from the six interrogative questions. A work’s relevant subsection is indicated by a colored cell.\nWHY\nWHO\nWHAT\nHOW\nWHEN\nWHERE\nWork\n4.1 Interpretability & Explainability\n4.2 Debugging & Improving Models\n4.3 Comparing & Selecting Models\n4.4 Teaching Deep Learning Concepts\n5.1 Model Developers & Builders\n5.2 Model Users\n5.3 Non-experts\n6.1 Computational Graph & Network Architecture\n6.2 Learned Model Parameters\n6.3 Individual Computational Units\n6.4 Neurons in High-dimensional Space\n6.5 Aggregated Information\n7.1 Node-link Diagrams for Network Architecture\n7.2 Dimensionality Reduction & Scatter Plots\n7.3 Line Charts for Temporal Metrics\n7.4 Instance-based Analysis & Exploration\n7.5 Interactive Experimentation\n7.6 Algorithms for Attribution & Feature Visualization\n8.1 During Training\n8.2 After Training\n9.2 Publication Venue\nAbadi, et al., 2016 [27]\narXiv\nBau, et al., 2017 [28]\nCVPR\nBilal, et al., 2017 [29]\nTVCG\nBojarski, et al., 2016 [30]\narXiv\nBruckner, 2014 [31]\nMS Thesis\nCarter, et al., 2016 [32]\nDistill\nCashman, et al., 2017 [33]\nVADL\nChae, et al., 2017 [34]\nVADL\nChung, et al., 2016 [35]\nFILM\nGoyal, et al., 2016 [36]\narXiv\nHarley, 2015 [37]\nISVC\nHohman, et al., 2017 [38]\nCHI\nKahng, et al., 2018 [39]\nTVCG\nKarpathy, et al., 2015 [40]\narXiv\nLi, et al., 2015 [41]\narXiv\nLiu, et al., 2017 [14]\nTVCG\nLiu, et al., 2018 [42]\nTVCG\nMing, et al., 2017 [43]\nVAST\nNorton & Qi, 2017 [44]\nVizSec\nOlah, 2014 [45]\nWeb\nOlah, et al., 2018 [46]\nDistill\nPezzotti, et al., 2017 [47]\nTVCG\nRauber, et al., 2017 [48]\nTVCG\nRobinson, et al., 2017 [49]\nGeoHum.\nRong & Adar, 2016 [50]\nICML VIS\nSmilkov, et al., 2016 [51]\nNIPS WS.\nSmilkov, et al., 2017 [16]\nICML VIS\nStrobelt, et al., 2018 [52]\nTVCG\nTzeng & Ma, 2005 [13]\nVIS\nWang, et al., 2018 [53]\nTVCG\nWebster, et al., 2017 [54]\nWeb\nWongsuphasawat, et al., 2018 [15]\nTVCG\nYosinski, et al., 2015 [55]\nICML DL\nZahavy, et al., 2016 [56]\nICML\nZeiler, et al., 2014 [10]\nECCV\nZeng, et al., 2017 [57]\nVADL\nZhong, et al., 2017 [58]\nICML VIS\nZhu, et al., 2016 [59]\nECCV\n6\nTABLE 3\nFoundational deep learning terminology used in this paper, sorted by importance. In a term’s “meaning” (last column), deﬁned terms are italicized.\nTechnical Term\nSynonyms\nMeaning\nNeural Network\nArtiﬁcial neural net,\nnet\nBiologically-inspired models that form the basis of deep learning; approximate functions dependent\nupon a large and unknown amount of inputs consisting of layers of neurons\nNeuron\nComputational unit,\nnode\nBuilding blocks of neural networks, entities that can apply activation functions\nWeights\nEdges\nThe trained and updated parameters in the neural network model that connect neurons to one another\nLayer\nHidden layer\nStacked collection of neurons that attempt to extract features from data; a layer’s input is connected to\na previous layer’s output\nComputational\nGraph\nDataﬂow graph\nDirected graph where nodes represent operations and edges represent data paths; when implement-\ning neural network models, often times they are represented as these\nActivation\nFunctions\nTransform function\nFunctions embedded into each layer of a neural network that enable the network represent complex\nnon-linear decisions boundaries\nActivations\nInternal\nrepresentation\nGiven a trained network one can pass in data and recover the activations at any layer of the network\nto obtain its current representation inside the network\nConvolutional\nNeural Network\nCNN, convnet\nType of neural network composed of convolutional layers that typically assume image data as input;\nthese layers have depth unlike typical layers that only have width (number of neurons in a layer); they\nmake use of ﬁlters (feature & pattern detectors) to extract spatially invariant representations\nLong Short-Term\nMemory\nLSTM\nType of neural network, often used in text analysis, that addresses the vanishing gradient problem by\nusing memory gates to propagate gradients through the network to learn long-range dependencies\nLoss Function\nObjective function,\ncost function, error\nAlso seen in general ML contexts, deﬁnes what success looks like when learning a representation,\ni.e., a measure of difference between a neural network’s prediction and ground truth\nEmbedding\nEncoding\nRepresentation of input data (e.g., images, text, audio, time series) as vectors of numbers in a high-\ndimensional space; oftentimes reduced so data points (i.e., their vectors) can be more easily analyzed\n(e.g., compute similarity)\nRecurrent Neural\nNetwork\nRNN\nType of neural network where recurrent connections allow the persistence (or “memory“) of previous\ninputs in the network’s internal state which are used to inﬂuence the network output\nGenerative\nAdversarial\nNetworks\nGAN\nMethod to conduct unsupervised learning by pitting a generative network against a discriminative\nnetwork; the ﬁrst network mimics the probability distribution of a training dataset in order to fool the\ndiscriminative network into judging that the generated data instance belongs to the training set\nEpoch\nData pass\nA complete pass through a given dataset; by the end of one epoch, a neural network will have seen\nevery datum within the dataset once\n4.1.2\nInterpretation as Qualitative Support for Model Eval-\nuation in Various Application Domains\nWhile research into interpretation itself is relatively new, its\nimpact has already been seen in applied deep learning con-\ntexts. A number of applied data science and AI projects that\nuse deep learning models include a section on interpretation\nto qualitatively evaluate and support the model’s predic-\ntions and the work’s claims overall. An example of this is an\napproach for end-to-end neural machine translation. In the\nwork by Johnson et al. [65], the authors present a simple and\nefﬁcient way to translate between multiple languages using\na single model, taking advantage of multilingual data to im-\nprove neural machine translation for all languages involved.\nThe authors visualize an embedding of text sequences, for\nexample, sentences from multiple languages, to support\nand hint at a universal interlingua representation. Another\nwork that visualizes large machine learning embeddings is\nby Zahavy et al. [56], where the authors analyze deep Q-\nnetworks (DQN), a popular reinforcement learning model,\nto understand and describe the policies learned by DQNs\nfor three different Atari 2600 video games. An application\nfor social good by Robinson et al. [49] demonstrates how\nto apply deep neural networks on satellite imagery to\nperform population prediction and disaggregation, jointly\nanswering the questions “where do people live?” and “how\nmany people live there?”. In general, they show how their\nmethodology can be an effective tool for extracting informa-\ntion from inherently unstructured, remotely-sensed data to\nprovide effective solutions to social problems.\nThese are only a few domains where visualization and\ndeep learning interpretation have been successfully used.\nOthers include building trust in autonomous driving ve-\nhicles [30], explaining decisions made by medical imaging\nmodels, such as MRIs on brain scans, to provide medical\nexperts more information for making diagnoses [66], and\nusing visual analytics to explore automatically-learned fea-\ntures from street imagery to gain perspective into identity,\nfunction, demographics, and afﬂuence in urban spaces,\nwhich is useful for urban design and planning [67].\nIn this survey we will mention interpretation and ex-\nplanation often, as they are the most common motiva-\ntions for deep learning visualization. Later, we will discuss\nthe different visualization techniques and visual analytics\nsystems that focus on neural network interpretability for\nembeddings [51], text [32], [40], [41], quantifying inter-\npretability [28], and many different image-based techniques\nstemming from the AI communities [4], [10], [68], [69], [70].\n4.2\nDebugging & Improving Models\nBuilding machine learning models is an iterative design\nprocess [71], [72], [73], and developing deep neural net-\n7\nworks is no different. While mathematical foundations have\nbeen laid, deep learning still has many open research ques-\ntions. For example, ﬁnding the exact combinations of model\ndepth, layer width, and ﬁnely tuned hyperparameters is\nnontrivial. In response to this, many visual analytics sys-\ntems have been proposed to help model developers build\nand debug their models, with the hope of expediting the\niterative experimentation process to ultimately improve per-\nformance [15], [47], [52]. Oftentimes this requires monitoring\nmodels during the training phase [42], [58], identifying\nmisclassiﬁed instances and testing a handful of well-known\ndata instances to observe performance [29], [39], [50], and\nallowing a system to suggest potential directions for the\nmodel developer to explore [34]. This reason for why we\nwish to visualize deep learning ultimately provides better\ntools to speed up model development for engineers and re-\nsearchers so that they can quickly identify and ﬁx problems\nwithin a model to improve overall performance.\n4.3\nComparing & Selecting Models\nWhile certainly related to model debugging and improve-\nment, model comparison and selection are slightly different\ntasks in which visualization can be useful [74], [75], [76]. Of-\ntentimes model comparison describes the notion of choosing\na single model among an ensemble of well-performing mod-\nels. That is, no debugging needs to be done; all models have\n“learned” or have been trained semi-successfully. Therefore,\nthe act of selecting a single, best-performing model requires\ninspecting model metrics and visualizing parts of the model\nto pick the one that has the highest accuracy, the lowest loss,\nor is the most generalizable, while avoiding pitfalls such as\nmemorizing training data or overﬁtting.\nSome systems take a high-level approach and compare\nuser-deﬁned model metrics, like accuracy and loss, and\naggregate them on interactive charts for performance com-\nparison [27]. Other frameworks compare neural networks\ntrained on different random initializations (an important\nstep in model design) to discover how they would af-\nfect performance, while also quantifying performance and\ninterpretation [28]. Some approaches compare models on\nimage generation techniques, such as performing image\nreconstruction from the internal representations of each\nlayer of different networks to compare different network\narchitectures [77]. Similar to comparing model architectures,\nsome systems solely rely on data visualization representa-\ntions and encodings to compare models [43], while others\ncompare different snapshots of a single model as it trains\nover time, i.e., comparing a model after n1 epochs and the\nsame model after n2 epochs of training time [57].\n4.4\nTeaching Deep Learning Concepts\nApart from AI experts, another important reason why we\nmay wish to visualize deep learning is to educate non-\nexpert users about AI. The exact deﬁnition of non-experts\nvaries by source and is discussed further in Section 5.3.\nAn example that targets the general public is Teachable\nMachines [54], a web-based AI experiment that explores\nand teaches the foundations of an image classiﬁer. Users\ntrain a three-way image classiﬁer by using their computer’s\nwebcam to generate the training data. After providing three\ndifferent examples of physical objects around the user (e.g.,\nholding up a pencil, a coffee mug, and a phone), the system\nthen performs real-time inference on whichever object is\nin view of the webcam, and shows a bar chart with the\ncorresponding classiﬁcation scores. Since inference is com-\nputed in real-time, the bar charts wiggles and jumps back\nand forth as the user removes an object, say the pencil,\nfrom the view and instead holds up the coffee mug. The\nvisualization used is a simple bar chart, which provides\nan approachable introduction into image classiﬁcation, a\nmodern-day computer vision and AI problem.\nAnother example for teaching deep learning concepts,\nthe Deep Visualization Toolbox [55] discussed later in this\nsurvey, also uses a webcam for instant feedback when\ninteracting with a neural network. Taking instantaneous\nfeedback a step further, some works have used direct ma-\nnipulation to engage non-experts in the learning process.\nTensorFlow Playground [16], a robust, web-based visual\nanalytics tool for exploring simple neural networks, uses\ndirect manipulation to reinforce deep learning concepts, and\nimportantly, evokes the user’s intuition about how neural\nnetworks work. Other non-traditional mediums have been\nused to teach deep learning concepts and build an intuition\nfor how neural networks behave too. Longform, interactive\nscrollytelling works focusing on particular AI topics that\nuse interactive visualizations as supporting evidence are\ngaining popularity. Examples include “How to Use t-SNE\nEffectively,” where users can play with hundreds of small\ndatasets and vary single parameters to observe their effect\non an embedding\n[78], and a similar interactive article\ntitled “Visualizing MNIST” that visualizes different types\nof embeddings produced by different algorithms [45].\n5\nWHO USES DEEP LEARNING VISUALIZATION\nThis section describes the groups of people who may stand\nto beneﬁt from deep learning visualization and visual an-\nalytics. We loosely organize them into three non-mutually\nexclusive groups by their level of deep learning knowledge\n(most to least): model developers, model users, and non-experts.\nNote that many of the works discussed can beneﬁt multiple\ngroups, e.g., a model developer may use a tool aimed at\nnon-experts to reinforce their own intuition for how neural\nnetworks learn.\n5.1\nModel Developers & Builders\nThe ﬁrst group of people who use deep learning visual-\nization are individuals whose job is primarily focused on\ndeveloping, experimenting with, and deploying deep neural\nnetworks. These model developers and builders, whether\nthey are researchers or engineers, have a strong under-\nstanding of deep learning techniques and a well-developed\nintuition surrounding model building. Their knowledge\nexpedites key decisions in deep learning workﬂows, such\nas identifying the which types of models perform best on\nwhich types of data. These individuals wield mastery over\nmodels, e.g., knowing how to vary hyperparameters in the\nright fashion to achieve better performance. These individ-\nuals are typically seasoned in building large-scale models\nand training them on high-performance machines to solve\n8\nreal-world problems [24]. Therefore, tooling and research for\nthese users is much more technically focused, e.g., exposing\nmany hyperparameters for detailed model control.\nOf the existing deep learning visual analytics tools pub-\nlished, a handful tackle the problem of developing tools for\nmodel developers, but few have seen widespread adoption.\nArguably the most well-known system is TensorBoard [27]:\nGoogle’s included open-source visualization platform for its\ndataﬂow graph library TensorFlow. TensorBoard includes a\nnumber of built-in components to help model developers\nunderstand, debug, and optimize TensorFlow programs. It\nincludes real-time plotting of quantitative model metrics\nduring training, instance-level predictions, and a visualiza-\ntion of the computational graph. The computational graph\ncomponent was published separately by Wongsuphasawat\net al. [15] and works by applying a series of graph transfor-\nmations that enable standard layout techniques to produce\ninteractive diagrams of TensorFlow models.\nOther tools, such as DeepEyes [47], assist in a num-\nber of model building tasks, e.g., identifying stable layers\nduring the training process, identifying unnecessary layers\nand degenerated ﬁlters that do not contribute to a model’s\ndecisions, pruning such entities, and identifying patterns\nundetected by the network, indicating that more ﬁlters or\nlayers may be needed. Another tool, Blocks [29], allows a\nmodel builder to accelerate model convergence and alle-\nviate overﬁtting, through visualizing class-level confusion\npatterns. Other research has developed new metrics beyond\nmeasures like loss and accuracy, to help developers inspect\nand evaluate networks while training them [58].\nSome tools also address the inherent iterative nature\nof training neural networks. For example, ML-o-scope [31]\nutilizes a time-lapse engine to inspect a model’s training dy-\nnamics to better tune hyperparameters, while work by Chae\net al. [34] visualizes classiﬁcation results during training\nand suggests potential directions to improve performance\nin the model building pipeline. Lastly, visual analytics tools\nare beginning to be built for expert users who wish to\nuse models that are more challenging to work with. For\nexample, DGMTracker [42] is a visual analytics tool built\nto help users understand and diagnose the training process\nof deep generative models: powerful networks that per-\nform unsupervised and semi-supervised learning where the\nprimary focus is to discover the hidden structure of data\nwithout resorting to external labels.\n5.2\nModel Users\nThe second group of people who may beneﬁt from deep\nlearning visualization are model users. These are users\nwho may have some technical background but are neural\nnetwork novices. Common tasks include using well-known\nneural network architectures for developing domain speciﬁc\napplications, training smaller-scale models, and download-\ning pre-trained model weights online to use as a starting\npoint. This group of users also include machine learning\nartists who use models to enable and showcase new forms\nof artistic expression.\nAn example visual analytics system for these model\nusers is ActiVis [39]: a visual analytics system for inter-\npreting the results of neural networks by using a novel\nFig. 2.\nActiVis [39]: a visual analytics system for interpreting neural\nnetwork results using a novel visualization that uniﬁes instance- and\nsubset-level inspections of neuron activations deployed at Facebook.\nvisual representation that uniﬁes instance- and subset-level\ninspections of neuron activations. Model users can ﬂexibly\nspecify subsets using input features, labels, or any inter-\nmediate outcomes in a machine learning pipeline. ActiVis\nwas built for engineers and data scientists at Facebook to\nexplore and interpret deep learning models results and is\ndeployed on Facebook’s internal system. LSTMVis [52] is\na visual analysis tool for recurrent neural networks with a\nfocus on understanding hidden state dynamics in sequence\nmodeling. The tool allows model users to perform hypoth-\nesis testing by selecting an input range to focus on local\nstate changes, then to match these states changes to similar\npatterns in a large dataset, and ﬁnally align the results with\nstructural annotations. The LSTMVis work describes three\ntypes of users: architects, those who wish to develop new\ndeep learning methodologies; trainers, those who wish to\napply LSTMs to a task in which they are domain experts in;\nand end users, those who use pretrained models for various\ntasks. Lastly, Embedding Projector [51], while not speciﬁ-\ncally deep learning exclusive, is a visual analytics tool to\nsupport interactive visualization and interpretation of large-\nscale embeddings, which are common outputs from neural\nnetwork models. The work presents three important tasks\nthat model users often perform while using embeddings;\nthese include exploring local neighborhoods, viewing the\nglobal geometry to ﬁnd clusters, and ﬁnding meaningful\ndirections within an embedding.\n5.3\nNon-experts\nThe third group of people whom visualization could aid\nare non-experts in deep learning. These are individuals who\ntypically have no prior knowledge about deep learning, and\nmay or may not have a technical background. Much of the\nresearch targeted at this group is for educational purposes,\ntrying to explain what a neural network is and how it works\nat a high-level, sometimes without revealing deep learning\nis present. These group also includes people who simply use\nAI-powered devices and consumer applications.\nApart from Teachable Machines [54] and the Deep Vi-\nsualization Toolbox [55] mentioned in Section 4.4, Tensor-\nFlow Playground [16], a web-based interactive visualization\nof a simple dense network, has become a go-to tool for\n9\ngaining intuition about how neural networks learn. Tensor-\nFlow Playground uses direct manipulation experimentation\nrather than coding, enabling users to quickly build an in-\ntuition about neural networks. The system has been used\nto teach students about foundational neural network prop-\nerties by using “living lessons,” and also makes it straight-\nforward to create a dynamic, interactive educational experi-\nence. Another web-browser based system, ShapeShop [38],\nallows users to explore and understand the relationship be-\ntween input data and a network’s learned representations.\nShapeShop uses a feature visualization technique called\nclass activation maximization to visualize speciﬁc classes of\nan image classiﬁer. The system allows users to interactively\nselect classes from a collection of simple shapes, select a\nfew hyperparameters, train a model, and view the generated\nvisualizations all in real-time.\nTools built for non-experts, particularly with an ed-\nucational\nfocus,\nare\nbecoming\nmore\npopular\non\nthe\nweb. A number of web-based JavaScript frameworks\nfor training neural networks and inference have been\ndeveloped; however, ConvNetJS (http://cs.stanford.edu/\npeople/karpathy/convnetjs/) and TensorFlow.js (https://\njs.tensorﬂow.org/) are the most used and have enabled de-\nvelopers to create highly interactive explorable explanations\nfor deep learning models.\n6\nWHAT TO VISUALIZE IN DEEP LEARNING\nThis section discusses the technical components of neural\nnetworks that could be visualized. This section is strongly\nrelated to the next section, Section 7 “How,” which describes\nhow the components of these networks are visualized in\nexisting work. By ﬁrst describing what may be visualized\n(this section), we can more easily ground our discussion on\nhow to visualize them (next section).\n6.1\nComputational Graph & Network Architecture\nThe ﬁrst thing that can be visualized in a deep learning\nmodel is the model architecture. This includes the compu-\ntational graph that deﬁnes how a neural network model\nwould train, test, save data to disk, and checkpoint after\nepoch iterations [27]. Also called the dataﬂow graph [27],\nthis deﬁnes how data ﬂows from operation to operation to\nsuccessfully train and use a model. This is different than\nthe neural network’s edges and weights, discussed next,\nwhich are the parameters to be tweaked during training.\nThe dataﬂow graph can be visualized to potentially inform\nmodel developers of the types of computations occurring\nwithin their model, as discussed in Section 7.1.\n6.2\nLearned Model Parameters\nOther components that can be visualized are the learned\nparameters in the network during and after training.\n6.2.1\nNeural Network Edge Weights\nNeural network models are built of many, and sometimes\ndiverse, constructions of layers of computational units [26].\nThese layers send information throughout the network by\nusing edges that connect layers to one another, oftentimes\nin a linear manner, yet some more recent architectures\nhave shown that skipping certain layers and combining\ninformation in unique ways can lead to better performance.\nRegardless, each node has an outgoing edge with an ac-\ncompanying weight that sends signal from one neuron in\na layer to potentially thousands of neurons in an adjacent\nlayer [16]. These are the parameters that are tweaked during\nthe backpropagation phase of training a deep model, and\ncould be worthwhile to visualize for understanding what\nthe model has learned, as seen in Section 7.1.\n6.2.2\nConvolutional Filters\nConvolutional neural networks are built using a particular\ntype of layer, aptly called the convolutional layer. These\nconvolutional layers apply ﬁlters over the input data, of-\ntentimes images represented as a two-dimensional matrix\nof values, to generate smaller representations of the data to\npass to later layers in the network. These ﬁlters, like the\npreviously mentioned traditional weights, are then updated\nthroughout the training process, i.e., learned by the network,\nto support a given task. Therefore, visualizing the learned\nﬁlters could be useful as an alternate explanation for what a\nmodel has learned [10], [55], as seen in Section 7.6.\n6.3\nIndividual Computational Units\nAlbeit reductionist, neural networks can be thought as a\ncollection of layers of neurons connected by edge weights.\nAbove, we discussed that the edges can be visualized, but\nthe neurons too can be a source of data to investigate.\n6.3.1\nActivations\nWhen given a trained model, one can perform inference on\nthe model using a new data instance to obtain the neural\nnetwork’s output, e.g., a classiﬁcation or a speciﬁc predicted\nvalue. Throughout the network, the neurons compute acti-\nvations using activation functions (e.g., weighted sum) to\ncombine the signal from the previous layer into a new\nnode [26], [55]. This mapping is one of the characteristics\nthat allows a neural network to learn. During inference,\nwe can recover the activations produced at each layer. We\ncan use activations in multiple ways, e.g., as a collection\nof individual neurons, spatial positions, or channels [46].\nAlthough these feature representations are typically high-\ndimensional vectors of the input data at a certain stage\nwithin the network [46], it could be valuable in helping\npeople visualize how input data is transformed into higher-\nlevel features, as seen in Section 7.2. Feature representations\nmay also shed light upon how the network and its compo-\nnents respond to particular data instances [55], commonly\ncalled instance-level observation; we will discuss this in\ndetail in Section 7.4 and 7.5.\n6.3.2\nGradients for Error Measurement\nTo train a neural network, we commonly use a process\nknown as backpropagation [26]. Backpropagation, or some-\ntimes called the backpropagation of errors, is a method to\ncalculate the gradient of a speciﬁed loss function. When\nused in combination with an optimization algorithm, e.g.,\ngradient descent, we can compute the error at the output\nlayer of a neural network and redistribute the error by\nupdating the model weights using the computed gradient.\n10\nThese gradients ﬂow over the same edges deﬁned in the\nnetwork that contain the weights, but ﬂow in the opposite\ndirection., e.g., from the output layer to the input layer.\nTherefore, it could be useful to visualize the gradients of\na network to see how much error is produced at certain\noutputs and where it is distributed [33], [35], as mentioned\nin Section 7.6.\n6.4\nNeurons in High-dimensional Space\nContinuing the discussion of visualizing activations of a\ndata instance, we can think of the feature vectors recovered\nas vectors in a high-dimensional space. Each neuron in a\nlayer then becomes a “dimension.” This shift in perspective\nis powerful, since we can now take advantage of high-\ndimensional visualization techniques to visualize extracted\nactivations [48], [79]. Sometimes, people use neural net-\nworks simply as feature vector generators, and defer the\nactual task to other computational techniques, e.g., tradi-\ntional machine learning models [4], [49]. In this perspective,\nwe now can think of deep neural networks as feature gener-\nators, whose output embeddings could be worth exploring.\nA common technique is to use dimensionality reduction to\ntake the space spanned by the activations and embed it\ninto 2D or 3D for visualization purposes [48], [51], [79], as\ndiscussed in Section 7.2.\n6.5\nAggregated Information\n6.5.1\nGroups of Instances\nAs mentioned earlier, instance-level activations allow one\nto recover the mapping from data input to a feature vector\noutput. While this can be done for a single data instance,\nit can also be done on collections of instances. While at\nﬁrst this does not seem like a major differentiation from be-\nfore, instance groups provide some unique advantages [39],\n[43]. For example, since instance groups by deﬁnition are\ncomposed of many instances, one can compute all the\nactivations simultaneously. Using visualization, we can now\ncompare these individual activations to see how similar or\ndifferent they are from one another. Taking this further,\nwith instance groups, we can now take multiple groups,\npotentially from differing classes, and compare how the\ndistribution of activations from one group compares or\ndiffers from another. This aggregation of known instances\ninto higher-level groups could be useful for uncovering the\nlearned decision boundary in classiﬁcation tasks, as seen in\nSection 7.2 and Section 7.4.\n6.5.2\nModel Metrics\nWhile instance- and group-level activations could be useful\nfor investigating how neural networks respond to particular\nresults a-priori, they suffer from scalability issues, since\ndeep learning models typically wrangle large datasets. An\nalternative object to visualize are model metrics, including\nloss, accuracy, and other measures of error [27]. These\nsummary statistics are typically computed every epoch and\nrepresented as a time series over the course of a model’s\ntraining phase. Representing the state of a model through\na single number, or handful of numbers, abstracts away\nmuch of the subtle and interesting features of deep neural\nnetworks; however, these metrics are key indicators for\ncommunicating how the network is progressing during the\ntraining phase [47]. For example, is the network “learning”\nanything at all or is it learning “too much” and is simply\nmemorizing data causing it to overﬁt? Not only do these\nmetrics describe notions of a single model’s performance\nover time, but in the case of model comparison, these\nmetrics become more important, as they can provide a quick\nand easy way to compare multiple models at once. For this\nreason, visualizing model metrics can be an important and\npowerful tool to consider for visual analytics, as discussed\nin Section 7.3.\n7\nHOW TO VISUALIZE DEEP LEARNING\nIn the previous section, we described what technical com-\nponents of neural networks could be visualized. In this\nsection, we summarize how the components are visualized\nand interacted with in existing literature. For most neural\nnetwork components, they are often visualized using a few\ncommon approaches. For example, network architectures\nare often represented as node-link diagrams; embeddings of\nmany activations are typically represented as scatter plots;\nand model metrics over epoch time are almost always rep-\nresented as line charts. In this section, we will also discuss\nother representations, going beyond the typical approaches.\n7.1\nNode-link Diagrams for Network Architectures\nGiven a neural network’s dataﬂow graph or model architec-\nture, the most common way to visualize where data ﬂows\nand the magnitude of edge weights is a node-link diagram.\nNeurons are shown as nodes, and edge weights as links.\nFor computational and dataﬂow graphs, Kahng et al. [39]\ndescribe two methods for creating node-link diagrams. The\nﬁrst represents only operations as nodes, while the second\nrepresents both operations and data as nodes. The ﬁrst way\nis becoming the standard due to the popularity of Ten-\nsorBoard [27] and the inclusion of its interactive dataﬂow\ngraph visualization [15]. However, displaying large num-\nbers of links from complex models can generate “hairball”\nvisualizations where many edge crossings impede pattern\ndiscovery. To address this problem, Wongsuphasawat et\nal. [15] extracts high-degree nodes (responsible for many\nof the edge crossings), visualizes them separately from the\nmain graph, and allow users to deﬁne super-groups within\nthe code. Another approach to reduce clutter is to place\nmore information on each node; DGMTracker [42] provides\na quick snapshot of the dataﬂow in and out of a node by\nvisualizing its activations within each node.\nRegarding neural network architecture, many visual an-\nalytics systems use node-link diagrams (neurons as nodes,\nweights as links) [13], [14], [16], [35], [37]. The weight\nmagnitude and sign can then be encoded using color or\nlink thickness. This technique was one of the the ﬁrst to\nbe proposed [13], and the trend has continued on in litera-\nture. Building on this technique, Harley [37] visualizes the\nconvolution windows on each layer and how the activations\npropagate through the network to make the ﬁnal classiﬁca-\ntion. Similar to the dataﬂow graph examples above, some\nworks include richer information inside each node besides\n11\nFig. 3. Each point is a data instance’s high-dimensional activations at a\nparticular layer inside of a neural network, dimensionally reduced, and\nplotted in 2D. Notice as the data ﬂows through the network the activation\npatterns become more discernible (left to right) [39].\nan activation value, such as showing a list of images that\nhighly activate that neuron or the activations at a neuron as\na matrix [14]. As mentioned in the dataﬂow graph visual-\nizations, node-link diagrams for network architecture work\nwell for smaller networks [16], but they also suffer from\nscalabilty issues. CNNVis [14], a visual analytics system\nthat visualizes convolutional neural networks, proposes to\nuse a bi-clustering-based edge bundling technique to reduce\nvisual clutter caused by too many links.\n7.2\nDimensionality Reduction & Scatter Plots\nIn Section 6, “What,” we discussed different types of high-\ndimensional embeddings: text can be represented as vectors\nin word embeddings for natural language processing and\nimages can be represented as feature vectors inside of a\nneural network. Both of these types of embeddings are\nmathematically represented as large tensors, or sometimes\nas 2D matrices, where each row may correspond to an\ninstance and each column a feature.\nThe most common technique to visualize these embed-\ndings is performing dimensionality reduction to reduce\nthe number of columns (e.g., features) to two or three.\nProjecting onto two dimensions would mean computing\n(x, y) coordinates for every data instance; for three dimen-\nsions, we compute an additional z component, resulting in\n(x, y, z). In the 2D case, we can plot all data instances as\npoints in a scatter plot where the axes may or may not\nhave interpretable meaning, depending on the reduction\ntechnique used, e.g., principal component analysis (PCA)\nor t-distributed stochastic neighbor embedding (t-SNE) [79].\nIn the 3D case, we can still plot each data instance as a\npoint in 3D space and use interactions to pan, rotate, and\nnavigate this space [51]. These types of embeddings are\noften included in visual analytics systems as one of the main\nviews [35], [47], and are also used in application papers as\nstatic ﬁgures [56], [65]. However, viewing a 3D space on\na 2D medium (e.g., computer screen) may not be ideal for\ntasks like comparing exact distances.\nSince each reduced point corresponds to an original\ndata instance, another common approach is to retrieve the\noriginal image and place it at the reduced coordinate loca-\ntion. Although the image size must be greatly reduced to\nprevent excessive overlap, viewing all the images at once\ncan provide insight into what a deep learning model has\nlearned, as seen in the example in [77] where the authors\nvisualize ImageNet test data, or in [80] where the authors\ncreate many synthetic images from a single class and com-\npare the variance across many random initial starting seeds\nfor the generation algorithm. We have discussed the typical\ncase where each dot in the scatter plot is a data instance,\nbut some work has also visualized neurons in a layer as\nseparate data instances [58]. Another work studies closely\nhow data instances are transformed as their information is\npassed through the deep network, which in effect visualizes\nhow the neural network separates various classes along\napproximated decision boundaries [48]. It is also possible to\nuse time-dependent data and visualize how an embedding\nchanges over time, or in the case of deep learning, over\nepochs [81]. This can be useful for evaluating the quality\nof the embedding during the training phase.\nHowever, these scatter plots raise problems too. The\nquality of the embeddings greatly depends on the algorithm\nused to perform the reduction. Some works have studied\nhow PCA and t-SNE differ, mathematical and visually, and\nsuggest new reduction techniques to capture the semantic\nand syntactic qualities within word embeddings [82]. It\nhas also been shown that popular reduction techniques\nlike t-SNE are sensitive to changes in the hyperparameter\nspace. Wattenberg meticulously explores the hyperparame-\nter space for t-SNE, and offers lessons learned and practical\nadvice for those who wish to use dimensionality reduction\nmethods [78]. While these techniques are commonplace,\nthere are still iterative improvements that can be done using\nclever interaction design, such as ﬁnding instances similar\nto a target instance, i.e., those “near” the target in the\nprojected space, helping people build intuition for how data\nis spatially arranged [51].\n7.3\nLine Charts for Temporal Metrics\nModel developers track the progression of their deep learn-\ning models by monitoring and observing a number of\ndifferent metrics computed after each epoch, including the\nloss, accuracy, and different measure of errors. This can\nbe useful for diagnosing the long training process of deep\nlearning models., The most common visualization technique\nfor visualizing this data is by considering the metrics as time\nseries and plotting them in line charts [27]. This approach\nis widely used in deep learning visual analytics tools [35],\n[47]. After each epoch, a new entry in the time series\nis computed, therefore some tools, like TensorBoard, run\nalongside models as they train and update with the latest\nstatus [27]. TensorBoard focuses much of its screen real-\nestate to these types of charts and supports interactions\nfor plotting multiple metrics in small multiples, plotting\nmultiple models on top of one another, ﬁltering different\nmodels, providing tooltips for the exact metric values, and\nresizing charts for closer inspection. This technique appears\nin many visual analytics systems and has become a staple\nfor model training, comparison, and selection.\n7.4\nInstance-based Analysis & Exploration\nAnother technique to help interpret and debug deep learn-\ning models is testing speciﬁc data instances to understand\nhow they progress throughout a model. Many experts have\nbuilt up their own collection of data instances over time,\n12\nhaving developed deep knowledge about their expected be-\nhaviors in models while also knowing their ground truth la-\nbels [19], [39]. For example, an instance consisting of a single\nimage or a single text phrase is much easier to understand\nthan an entire image dataset or word embedding consisting\nof thousands of numerical features extracted from an end\nuser’s data. This is called instance-level observation, where\nintensive analysis and scrutiny is placed on a single data\ninstance’s transformation process throughout the network,\nand ultimately its ﬁnal output.\n7.4.1\nIdentifying & Analyzing Misclassiﬁed Instances\nOne application of instance-level analysis is using instances\nas unit tests for deep learning models. In the best case\nscenario, all the familiar instances are classiﬁed or predicted\ncorrectly; however, it is important to understand when a\nspeciﬁc instance can fail and how it fails. For example, in\nthe task of predicting population from satellite imagery,\nthe authors showcase three maps of areas with high errors\nby using a translucent heatmap overlaid on the satellite\nimagery [49]. Inspecting these instances reveals three ge-\nographic areas that contain high amounts of man-made\nfeatures and signs of activity but have no registered people\nliving in them: an army base, a national lab, and Walt\nDisney World. The visualization helps demonstrate that\nthe proposed model is indeed learning high-level features\nabout the input data. Another technique, HOGgles [83], uses\nan algorithm to visualize feature spaces by using object\ndetectors while inverting visual features back to natural\nimages. The authors ﬁnd that when visualizing the features\nof misclassiﬁed images, although the classiﬁcation is wrong\nin the image space, they look deceptively similar to the\ntrue positives in the feature space. Therefore, by visualizing\nfeature spaces of misclassiﬁed instances, we can gain a more\nintuitive understanding of recognition systems.\nFor textual data, a popular technique for analyzing\nparticular data instances is to use color as the primary\nencoding. For example, the background of particular char-\nacters in a phrase of words in a sentence would be colored\nusing a divergent color scheme according to some criteria,\noften their activation magnitudes [32], [36], [40]. This helps\nidentify particular data instances that may warrant deeper\ninspection (e.g., those misclassiﬁed) [19].\nWhen pre-deﬁned data instances are not unavailable\n(e.g., when analyzing a new dataset), how can we guide\nusers towards important and interesting instances? To\naddress this problem, a visual analytics system called\nBlocks [29] uses confusion matrices, a technique for sum-\nmarizing the performance of a classiﬁcation algorithm, and\nmatrix-level sorting interactions to reveal that class error\noften occurs in hierarchies. Blocks incorporates these tech-\nniques with a sample viewer in the user interface to show\nselected samples potentially worth exploring.\n7.4.2\nAnalyzing Groups of Instances\nInstead of using individual data instances for testing and\ndebugging a model, it is also common for experts to per-\nform similar similar tasks using groups of instances [19].\nWhile some detail may be lost when performing group-\nlevel analysis it allows experts to further test the model\nby evaluating its average and aggregate performance across\ndifferent groups.\nMuch of the work using this technique is done on text\ndata using LSTM models [52]. Some approaches compute\nthe saliency for groups of words across the model and\nvisualize the values as a matrix [41], while others use matrix\nvisualizations to show the activations of word groups when\nrepresented as feature vectors in word embeddings [50],\n[84]. One system, ActiVis [39], places instance group anal-\nysis at the focus of its interactive interface, allowing users to\ncompare preset and user-deﬁned groups of activations. Sim-\nilar to the matrix visualization that summarizes activations\nfor each class in CNNVis [14], ActiVis also uses a scrolling\nmatrix visualization to unify both instance-level and group-\nlevel analysis into a single view where users can compare\nthe activations of the user-deﬁned instances.\nHowever, sometimes it can be challenging to deﬁne\ngroups for images or text. For textual data, people often use\nwords to group documents and provide aggregated data.\nConceptVector [85] addresses the instance group generation\nproblem by providing an interactive interface to create in-\nteresting groups of concepts for model testing. Furthermore,\nthis system also suggests additional words to include in\nthe user-deﬁned groups, helping guide the user to create\nsemantically sound concepts.\n7.5\nInteractive Experimentation\nInteractive experimentation, another interesting area that\nintegrates deep learning visualization, makes heavy use of\ninteractions for users to experiment with models [86]. By\nusing direct manipulation for testing models, a user can\npose “what if?” questions and observe how the input data\naffects the results. Called explorable explanations [87], this\ntype of visual experimentation is popular for making sense\nof complex concepts and systems.\n7.5.1\nModels Responding to User-provided Input Data\nTo engage the user with the desired concepts to be taught,\nmany systems require the user to provide some kind of\ninput data into the system to obtain results. Some visual\nanalytics systems use a webcam to capture live videos,\nand visualize how the internals of neural network models\nrespond to these dynamic inputs [55]. Another example\nis a 3D visualization of a CNN trained on the classic\nMNIST dataset 1 that shows the convolution windows and\nactivations on images that the user draws by hand [37].\nFor example, drawing a “5” in the designated area passes\nthat example throughout the network and populates the\nvisualization with the corresponding activations using a\nnode-link diagram. A ﬁnal example using image data is\nShapeShop [38], a system that allows a user to select data\nfrom a bank of simple shapes to be classiﬁed. The system\nthen trains a neural network and using the class activation\nmaximization technique to generate visualizations of the\nlearned features of the model. This can be done in real-\ntime, therefore a user can quickly train multiple models with\ndifferent shapes to observe the effect of adding more diverse\ndata to improve the internal model representation.\n1. MNIST is a small, popular dataset consisting of thousands of\n28×28px images of handwritten digits (0 to 9). MNIST is commonly\nused as a benchmark for image classiﬁcation models.\n13\nFig. 4. TensorFlow Playground [16]: a web-based visual analytics tool\nfor exploring simple neural networks that uses direct manipulation rather\nthan programming to teach deep learning concepts and develop an\nintuition about how neural networks behave.\nAn example using textual data is the online, interactive\nDistill article for handwriting prediction [32], which allows\na user to write words on screen, and in real-time, the\nsystem draws multiple to-be-drawn curves predicting what\nthe user’s next stroke would be, while also visualizing the\nmodel’s activations. Another system uses GANs to inter-\nactively generate images based off of user’s sketches [59].\nBy sketching a few colored lines, the system presents the\nuser with multiple synthetic images using the sketch as\na guideline for what to generate. A ﬁnal example is the\nAdversarial Playground [44], a visual analytics system that\nenables users to compare adversarially-perturbed images, to\nhelp users understand why an adversarial example can fool\na CNN image classiﬁer. The user can select from one of the\nMNIST digits and adjust the strength of adversarial attack.\nThe system then compares the classiﬁcations scores in a\nbar chart to observe how simple perturbations can greatly\nimpact classiﬁcation accuracy.\n7.5.2\nHow Hyperparameters Affect Results\nWhile deep learning models automatically adjust their in-\nternal parameters, their hyperparameters still require ﬁne-\ntuning. These hyperparameters can have major impact on\nmodel performance and robustness. Some visual analytics\nsystems expose model hyperparameters to the user for inter-\nactive experimentation. One example previously mentioned\nis TensorFlow Playground [16], where users can use direct\nmanipulation to adjust the architecture of a simple, fully-\nconnected neural network, as well as the hyperparameters\nassociated with its training, such as the learning rate, activa-\ntion function, and regularization. Another example is a Dis-\ntill article that meticulously explores the hyperparaemters of\nthe t-SNE dimensionality reduction method [78]. This article\ntests dozens of synthetic datasets in different arrangements,\nwhile varying hyperparameters such as the t-SNE perplexity\nand the number of iterations to run the algorithm for.\n7.6\nAlgorithms for Attribution & Feature Visualization\nThe ﬁnal method for how to visualize deep learning hails\nfrom the AI and computer vision communities. These are\nalgorithmic techniques that entail image generation. Given\na trained a model, one can select a single image instance and\nuse one of the algorithmic techniques to generate a new im-\nage of the same size that either highlights important regions\nof the image (often called attribution) or is an entirely new\nimage that supposedly is representative of the same class\n(often called feature visualization) [46], [88]. In these works,\nit is common to see large, full-page ﬁgures consisting of\nhundreds of such images corresponding to multiple images\nclasses [89]. However, it is uncommon to see interactivity\nin these works, as the primary contribution is often about\nalgorithms, not interactive techniques or systems. Since the\nfocus of this interrogative survey is on visual analytics in\ndeep learning, we do not discuss in detail the various types\nof algorithmic techniques. Rather, we mention the most\nprominent techniques developed, since they are impactful\nto the growing ﬁeld of deep learning visualization and\ncould be incorporated into visual analytics systems in the\nfuture. For more details about these techniques, such as\ninput modiﬁcation, deconvolutional methods [10], and in-\nput reconstruction methods, we refer our readers to the tax-\nonomies [90] and literature surveys for visualizing learned\nfeatures in CNNs [22], [91], and a tutorial that presents\nthe theory behind many of these interpretation techniques\nand discusses tricks and recommendations to efﬁciently use\nthem on real data [61].\n7.6.1\nHeatmaps for Attribution, Attention, & Saliency\nOne research area generates translucent heatmaps that over-\nlay images to highlight important regions that contribute\ntowards classiﬁcation and their sensitivity [4], [69], [92], [93],\n[94]. One technique called visual backpropagation attempts\nto visualize which parts of an image have contributed to\nthe classiﬁcation, and can do so in real-time in a model\ndebugging tool for self-driving vehicles [30]. Another tech-\nnique is to invert representations, i.e., attempt to reconstruct\nan image using a feature vector to understand the what\na CNN has learned [77], [95], [96]. Prediction difference\nanalysis is a method that highlights features in an image to\nprovide evidence for or against a certain class [66]. Other\nwork hearkens back to more traditional computer vision\ntechniques by exploring how object detectors emerge in\nCNNs and attempts to give humans object detector vision\ncapabilities to better align humans and deep learning vision\nfor images [83], [97]. Visualizing CNN ﬁlters is also popular,\nand has famously shown to generate dream-like images,\nbecoming popular in artistic tasks [98], [99] . Some work\nfor interpreting visual question answering (VQA) models and\ntasks use these heatmaps to explain which parts of an image\na VQA model is looking at in unison with text activation\nmaps when answering the given textual questions [36].\nHowever, recent work has shown that some of these meth-\nods fail to provide correct results and argue that we should\ndevelop explanation methods that work on simpler models\nbefore extending them to the more complex ones [91].\n7.6.2\nFeature Visualization\nFor feature visualization, while some techniques have\nproven interesting [100], one of the most studied techniques,\nclass activation maximization, maximizes the activation of\na chosen, speciﬁc neuron using an optimization scheme,\nsuch as gradient ascent, and generates synthetic images that\nare representative of what the model has learned about the\n14\nchosen class [68]. This led to a number of works improving\nthe quality of the generated images. Some studies generated\nhundreds of these non-deterministic synthetic images and\nclustered them to see how variations in the class activation\nmaximization algorithm affects the output image [80]. In\nsome of their most recent work on this topic, Ngyuen\net al. [70] present hundreds of high-quality images using\na deep generator network to improve upon the state-of-\nthe-art, and include ﬁgures comparing their technique to\nmany of the existing and previous attempts to improve the\nquality of generated images. The techniques developed in\nthis research area have improved dramatically over the past\nfew years, where now it is possibly to synthetically generate\nphotorealistic images [101]. A recent comparison of feature\nvisualization techniques highlights their usefulness [88];\nhowever, the authors note that they remain skeptical of their\ntrustworthiness, e.g., do neurons have a consistent meaning\nacross different inputs, and if so, is that meaning accurately\nreiﬁed by feature visualization [46]?\n8\nWHEN TO VISUALIZE IN THE DEEP LEARNING\nPROCESS\nThis section describes when visualizing deep learning may\nbe most relevant and useful. Our discussion primarily cen-\nters around the training process: an iterative, foundational\nprocedure for using deep learning models. We identify two\ndistinct, non-mutually exclusive times for when to visualize:\nduring training and after training. Some works propose that\nvisualization be used both during and after training.\n8.1\nDuring Training\nArtiﬁcial neural networks learn higher-level features that\nare useful for class discrimination as training progress [102].\nBy using visualization during the training process, there\nis potential to monitor one’s model as it learns to closely\nobserve and track the model’s performance [48].\nMany of the systems in this category run in a separate\nweb-browser alongside the training process, and interface\nwith the underlying model to query the latest model status.\nThis way, users can visually explore and rigorously monitor\ntheir models in real time, while they are trained elsewhere.\nThe visualization systems dynamically update the charts\nwith metrics recomputed after every epoch, e.g., the loss,\naccuracy, and training time. Such metrics are important to\nmodel developers because they rely on them to determine\nif a model (1) has begun to learn anything at all; (2) is\nconverging and reaching the peak of its performance; or (3)\nhas potentially overﬁtted and memorized the training data.\nTherefore, many of the visual analytics systems used during\ntraining support and show these updating visualizations as\na primary view in the interface [16], [27], [34], [35], [42],\n[47]. One system, Deep View [58], visualizes model metrics\nduring the training process and uses its own deﬁned metrics\nfor monitoring (rather than the loss): a discriminability\nmetric, which evaluates neuron evolution, and a density\nmetric which evaluates the output feature maps. This way,\nfor detecting overﬁtting, the user does not need to wait long\nto view to infer overﬁtting; they simply observe the neuron\ndensity early in training phase.\nSimilarly, some systems help reduce development time\nand save computational resources by visualizing metrics\nthat indicate whether a model is successfully learning or\nnot, allowing a user to stop the training process early [16].\nBy using visualization during model training, users can save\ndevelopment time through model steering [35] and utilizing\nsuggestions for model improvement [34]. Lastly, another\nmodel development time minimization focuses on diagnos-\ning neurons and layers that are not training correctly or\nare misclassifying data instances. Examples include Deep-\nEyes [47], a system that identiﬁes stable and unstable layers\nand neurons so users may prune their models to speed up\ntraining; Blocks [29], a system that visualizes class confusion\nand reveals that confusion patterns follow a hierarchical\nstructure over the classes which can then be exploited to\ndesign hierarchy-aware architectures; and DGMTracker [42],\na system that proposes a credit assignment algorithm that\nindicates how other neurons contribute to the output of\nparticular failing neurons.\n8.2\nAfter Training\nWhile some works support neural network design during\nthe iterative model building process, there are other works\nthat focus their visualization efforts after a model has been\ntrained. In other words, these works assume a trained\nmodel as input to the system or visualization technique.\nNote that many, if not most, of the previously mentioned\nalgorithmic techniques developed in the AI ﬁelds, such as\nattribution and feature visualization, are performed after\ntraining. These techniques are discussed more in Section 7.6.\nThe Embedding Projector [51] specializes in visualizing\n2D and 3D embeddings produced by trained neural net-\nworks. While users can visualize typical high-dimensional\ndatasets in this tool, the Embedding Projector tailors the ex-\nperience towards embeddings commonly used deep learn-\ning. Once a neural network model has been trained, one can\ncompute the activations for a given test dataset and visual-\nize the activations in the Embedding Projector to visualize\nand explore the space that the network has learned. Instead\nof generating an overview embedding, another previously\ndiscussed system, the Deep Visualization Toolbox [55], uses\na trained model to visualize live activations in a large small-\nmultiples view to understand of what types of ﬁlters a\nconvolutional network has learned.\nMore traditional visual analytics systems have also been\ndeveloped to inspect a model after it has ﬁnished training.\nActiVis [39], a visual analytics system for neural network\ninterpretation deployed at Facebook reports that Facebook\nengineers and data scientists use visual analytics systems of-\nten in their normal workﬂow. Another system, RNNVis [43],\nvisualizes and compares different RNN models for various\nnatural language processing tasks. This system positions\nitself as a natural extension of TensorFlow; using multiple\nTensorFlow models as input, the system then analyzes the\ntrained models to extract learned representations in hidden\nstates, and further processes the evaluation results for visu-\nalization. Lastly, the LSTMVis [52] system, a visual analysis\ntool for RNN interpretability, separates model training from\nthe visualization. This system takes a model as input that\nmust be trained separately, and from the model, gathers the\n15\nrequired information to produce the interactive visualiza-\ntions to be rendered in a web-based front-end.\n9\nWHERE IS DEEP LEARNING VISUALIZATION\nFor the last question of the interrogative survey, we divide\nup “Where” into two subsections: where deep learning\nvisualization research has been applied, and where deep\nlearning visualization research has been conducted, describ-\ning the new and hybrid community. This division provides\na concise summary for practitioners who wish to investigate\nthe usage of the described techniques for their own work,\nand provides new researchers with the main venues for this\nresearch area to investigate existing literature.\n9.1\nApplication Domains & Models\nWhile many non-neural approaches are used for real-world\napplications, deep learning has successfully achieved state-\nof-the-art performance in several domains. Previously in\nSection 4.1.2, we presented works that apply neural net-\nworks to particular domains and use visualizations to lend\nqualitative support to their usual quantitative results to\nstrengthen users’ trust in their models. These domains in-\ncluded neural machine translation [65], reinforcement learn-\ning [56], social good [49], autonomous vehicles [30], medical\nimaging diagnostics [66], and urban planning [67].\nNext we summarize the types of models that have been\nused in deep learning visualization. Much of the exist-\ning work has used image-based data and models, namely\nCNNs, to generate attribution and feature visualization\nexplanations for what a model has learned from an image\ndataset. CNNs, while not exclusively used for images, have\nbecome popular in the computer vision community and are\noften used for image classiﬁcation and interactive, image-\nbased creative tasks [59], [103]. Besides images, sequential\ndata (e.g., text, time series data, and music) has also been\nstudied. This research stems from the natural language\nprocessing community, where researchers typically favor\nRNNs for learning representations of large text corpora.\nThese researchers make sense of large word embeddings\nby using interactive tools that support dimensionality re-\nduction techniques to solve problems such as sequence-\nto-sequence conversion, translation, and audio recognition.\nResearch combining both image and text data has also been\ndone, such as image captioning and visual question an-\nswering [104], [105]. Harder still are new types of networks\ncalled generative adversarial networks, or GANs for short, that\nhave produced remarkable results for data generation [106],\ne.g., producing real-looking yet fake images [107]. While\nGANs have only existed for a couple of years, they are now\nreceiving signiﬁcant research attention. To make sense of the\nlearned features and distributions from GANs, two visual\nanalytics systems, DGMTracker [42] and GANViz [53], focus\non understanding the training dynamics of GANs to help\nmodel developers better train these complex models, often\nconsisting of multiple dueling neural networks.\n9.2\nA Vibrant Research Community: Hybrid, Apace, &\nOpen-sourced\nAs seen from this survey, bringing together the visualization\ncommunities with the AI communities has led to the design\nand development of numerous tools and techniques for im-\nproving deep learning interpretability and democratization.\nThis hybrid research area has seen accelerated attention and\ninterest due to its widespread impact, as evidenced by the\nlarge number of works published in just a few years, as\nseen in Table 2. A consequence of this rapid progress is that\ndeep learning visualization research are being disseminated\nacross multiple related venues. In academia, the premiere\nvenues for deep learning visualization research consists of\ntwo main groups: the information visualization and visual\nanalytics communities; and the artiﬁcial intelligence and\ndeep learning communities. Furthermore, since this area is\nrelatively new, it has seen more attention at multiple work-\nshops at the previously mentioned academic conferences, as\ntabulated in Table 1.\nAnother consequence of this rapidly developing area is\nthat new work is immediately publicized and open-sourced,\nwithout waiting for it to be “ofﬁcially” published at confer-\nences, journals, etc. Many of these releases take the form of a\npreprint publication posted on arXiv, where a deep learning\npresence has thrived. Not only is it common for academic\nresearch labs and individuals to publish work on arXiv, but\ncompanies from industry are also publishing results, code,\nand tools. For example, the most popular libraries2 for im-\nplementing neural networks are open-source and have con-\nsistent contributions for improving all areas of the codebase,\ne.g., installation, computation, and deployment into speciﬁc\nprogramming languages’ open-source environments.\nSome works have a corresponding blog post on an\nindustry research blog3, which, while non-traditional, has\nlarge impact due to their prominent visibility and large\nreadership. While posting preprints may have its down-\nsides (e.g., little quality control) the communities have been\npromoting the good practices of open-sourcing developed\ncode and including direct links within the preprints; both\npractices are now the norm. Although it may be overwhelm-\ning to digest the amount of new research published daily,\nhaving access to the work with its code could encourage re-\nproducibility and allow the communities to progress faster.\nIn summary, given the increasing interest in deep learning\nvisualization research and its importance, we believe our\ncommunities will continue to thrive, and will positively\nimpact many domains for years to come.\n10\nRESEARCH DIRECTIONS & OPEN PROBLEMS\nNow we present research directions and open problems for\nfuture research distilled from the surveyed works.\n10.1\nFurthering Interpretability\nUnsurprisingly, with the amount of attention and impor-\ntance on interpretability and explainability in the deep\nlearning visualization literature, the ﬁrst area for future\nwork is continuing to create new interpretable methods\nfor deep learning models. For the information visualiza-\ntion and visual analytics communities, this could constitute\n2. Popular libraries include TensorFlow [27], Keras, Caffe, PyTorch,\nand Theano.\n3. High impact industry blogs include: Google Research Blog, Ope-\nnAI, Facebook Research Blog, the Apple Machine Learning Journal,\nNVIDIA Deep Learning AI, and Uber AI\n16\nFig. 5. Distill: The Building Blocks of Interpretability [46]: an interactive\nuser interface that combines feature visualization and attribution tech-\nniques to interpret neural networks.\ncreating new visual representations for the components\nin deep learning models, or developing new interaction\ntechniques to reveal deeper insights about one’s model. For\nthe AI communities, more insightful attribution and feature\nvisualization techniques for trained models that are fast\n(computationally cheap) could be incorporated into visu-\nalization systems. Combining visual representations, help-\nful interactions, and state-of-the-art attribution and feature\nvisualization techniques together into rich user interfaces\ncould lead to major breakthroughs for understanding neural\nnetworks [46].\n10.2\nSystem & Visual Scalability\nThroughout this survey, we have covered many visual an-\nalytics systems that facilitate interpretation and model un-\nderstanding. However, some systems suffer from scalability\nproblems. Visual scalability challenges arise when dealing\nwith large data, e.g., large number of hyperparameters\nand millions of parameters in deep neural networks. Some\nresearch has started to address this, by simplifying complex\ndataﬂow graphs and network weights for better model\nexplanations [14], [15], [52]. But, regarding activations and\nembeddings, dimensionality reduction techniques have a\nlimit to their usability when it comes to the number of points\nto visualize [48]. We think this is an important research\ndirection, especially given that the information visualization\ncommunities have developed techniques to visualize large,\nhigh-dimensional data that could potentially be applicable\nto deep learning [108].\nAside from visual scalability, some tools also suffer from\nsystem scalability. While some of these problems may be\nmore engineering-centric, we think that for visual analytics\nsystems to adopted, they need to handle state-of-the-art\ndeep models without penalizing performance or increasing\nmodel development time. Furthermore, these systems (often\nweb-based) will greatly beneﬁt from fast computations,\nsupporting real-time, rich user interactions [50]. This is espe-\ncially important for visual systems that need to perform pre-\ncomputation before rendering visualizations to the screen.\n10.3\nDesign Studies for Evaluation: Utility & Usability\nAn important facet of visualization research is the eval-\nuation of the utility and usefulness of the visual repre-\nsentation. Equally important is to evaluate the usability\nof deployed systems and their interactive visual analytics\ntechniques. It is encouraging to see many of the visual\nanalytics systems recognize this importance and report on\ndesign studies conducted with AI experts before building a\ntool to understand the users and their needs [14], [15], [19],\n[39], [43], [52]. It is common to see example use cases or\nillustrative usage scenarios that demonstrate the capabilities\nof the interactive systems. Some works go beyond these and\nconduct user studies to evaluate utility and usability [31].\nIn the AI communities, most works do not include user\nstudies. For those that do, they greatly beneﬁt from showing\nwhy their proposed methods are superior to the ones being\ntested against [69], [89], [109], [110]. Taking this idea to\nthe quantiﬁable extreme, a related avenue of evaluating\nthese techniques is the notion of quantifying interpretability,\nwhich has been recently studied [28], [111]. Other domains\nhave recognized that interpretable deep learning research\nmay require evaluation techniques for their interpretations,\nand argue that there is a large body of work from ﬁelds such\nas philosophy, cognitive science, and social psychology that\ncould be utilized [62], [112].\nWhen surveying the interfaces of deep learning visual\nanalytics tools, many of them contain multiple-coordinated\nviews with many visual representations. Displaying this\nmuch information at once can be overwhelming, and when\ninterpretability is the primary focus, it is critical for these\nsystems to have superior usability. Therefore, we think fu-\nture works could further beneﬁt from including more mem-\nbers of the human-computer interaction communities, in-\ncluding interface and user experience designers, that could\nhelp organize and prioritize interfaces using well-studied\nguidelines [86].\n10.4\nThe Human Role in Interpretability\n10.4.1\nHuman v. Machine Understanding of the World\nIn deep learning interpretability work, researchers are de-\nveloping methods to produce better explanations to “see\nthrough the black-box,” but unfortunately some of these\nmethods produce visualizations that, while visually inter-\nesting and thought-provoking [98], are not fully under-\nstandable by their human viewers. That is an important\nfacet of deep learning interpretability, namely, producing\nvisualizations and interpretations that are human under-\nstandable [88]. Some methods compare algorithmic results\nwith an empirically derived human baseline; this enables\ncomparison between machine and human generated re-\nsponses to objects in the world, particularly in images [113].\nUltimately, researchers seek to understand the commonal-\nities and differences between how humans and machines\nsee and decompose the world [114]. Some tools that we\nhave surveyed achieve this by using live-video to compare\nthe input images and the neural network’s activations and\nﬁlters in real time [55]. Other tools give users explicit control\nof an experiment by training multiple small models with\nonly a few exposed hyperparameters, automatically gener-\nating visualizations to then see the effect that the input data\n17\nhas on the learned representation [38]. These “what-if” tools\nand scenarios could potentially be extended to incorporate\nhuman feedback into the training or model steering process\nof neural network to better improve performance.\n10.4.2\nHuman-AI Pairing\nMuch of this survey is dedicated towards reviewing the\nstate-of-the-art in visual analytics for deep learning, with\na focus on interpretability. These works use visualization to\nexplain, explore, and debug models in order to choose the\nbest preforming model for a given task, often by placing\na human in the loop. However, a slight twist on this idea\nhearkening back to the original envisioning of the computer\nhas lead to the emergence of a new research area, one where\ntasks are not exclusively performed by humans or machines,\nbut one where the two complement each other. This area,\nrecently dubbed artiﬁcial intelligence augmentation describes\nthe use of AI systems to help develop new methods for\nintelligence augmentation [103]. Some related works we\nhave covered already propose artiﬁcial intelligence aug-\nmentation ideas, such as a system that suggests potentially\ninteresting directions to explore in a high-dimensional 3D\nembedding [51], predicting and showing where the next\nstroke of a word could be when handwriting text [32],\nautomatically generating images based off of user-provided\nsketches [59], and dynamically changing and steering a\nneural network model while it trains [35]. We believe this is\na rich, under-explored area for future research: using well-\ndesigned interfaces for humans to interact with machine\nlearning models, and for these machine learning models to\naugment creative human tasks.\n10.5\nSocial Good & Bias Detection\nThe aspirational pairing of humans and machines is a long-\nterm research endeavor. To quicken our pace, we must con-\ntinue to democratize artiﬁcial intelligence via educational\ntools, perhaps by using direct manipulation as an invitation\nfor people to engage with AI [16], [46], clear explanations\nfor model decision making, and robust tooling and libraries\nfor programming languages for people to develop such\nmodels [15], [27]. While doing this, we must also ensure\nthat AI applications remain ethical, fair, safe, transparent,\nand are beneﬁting society [63].\nAnother important consideration for future research is\ndetecting bias. This has been identiﬁed as a major problem\nin deep learning [115], [116], and a number of researchers\nare using visualization to understand why a model may\nbe biased [117]. One example that aims to detect data bias\nis Google’s Facets tool [118], a visual analytics system de-\nsigned speciﬁcally to preview and visualize machine learn-\ning datasets before training. This allows one to inspect large\ndatasets by exploring the different classes or data instances,\nto see if there are any high-level imbalances in the class or\ndata distribution.\nOther works have begun to explore if the mathematical\nalgorithms themselves can be biased towards particular\ndecisions. An example of this is an interactive article ti-\ntled “Attacking discrimination with smarter machine learn-\ning” [117], which explores how one can can create both fair\nand unfair threshold classiﬁers in an example task such as\nloan granting scenarios where a bank may grant or deny\na loan based on a single, automatically computed number\nsuch as a credit score. The article aims to highlight that\nequal opportunity [119] is not preserved by machine learning\nalgorithms, and that as AI-powered systems continue to\nmake important decisions across core social domains, it is\ncritical to ensure decisions are not discriminatory.\nFinally, aside from data and model bias, humans are\noften inherently biased decision makers. In response, there\nis a growing area of research into detecting and understand-\ning bias in visual analytics 4 and its affect on the decision\nmaking process [120]. Some work has developed metrics\nto detect types of bias to present to a user during data\nanalysis [120] which could also be applied to visual tools for\ndeep learning in the future. Some work has employed de-\nvelopmental and cognitive psychology analysis techniques\nto understand how humans learn, focusing on uncovering\nhow human bias is developed and inﬂuences learning, to\nultimately inﬂuence artiﬁcial neural network design [112].\n10.6\nProtecting Against Adversarial Attacks\nRegardless of the beneﬁts AI systems are bringing to society,\nwe would be remiss to immediately trust them; like most\ntechnologies, AI too has security faults. Identiﬁed and stud-\nied in seminal works, it has been shown that deep learning\nmodels such as image classiﬁers can be easily fooled by\nperturbing an input image [121], [122], [123]. Most alarming,\nsome perturbations are so subtle that they are untraceable\nby the human eye, yet would completely fool a model into\nmisclassiﬁcation [122]. This sparked great interest in the AI\ncommunities, and much work has been done to understand\nhow fragile deep neural network image classiﬁers are, iden-\ntify in what ways can they break, and explore methods\nfor protecting them. Norton et al. [44] demonstrate adding\nadversarial perturbations to images in an interactive tool,\nwhere users can tweak the type and intensity of the attack,\nand observe the resulting (mis)classiﬁcation. This is a great\nﬁrst start for using visualization to identify potential attacks,\nbut we think visualization can be majorly impactful in this\nresearch space, by not only showcasing how the attacks\nwork and detecting them, but also by taking action and pro-\ntecting AI systems from the attacks themselves. While some\nwork, primarily originating from the AI communities, has\nproposed computational techniques to protect AI from at-\ntacks, such as identifying adversarial examples before clas-\nsiﬁcation [124], modifying the network architecture [125],\nmodifying the training process [122], [126], and performing\npre-processing steps before classiﬁcation [127], [128], we\nthink visualization can have great impact for combating\nadversarial machine learning.\n11\nCONCLUSION\nWe presented a comprehensive, timely survey on visualiza-\ntion and visual analytics in deep learning research, using\na human-centered, interrogative framework. Our method\nhelps researchers and practitioners in visual analytics and\ndeep learning to quickly learn key aspects of this young\n4. The DECISIVe Workshop (http://decisive-workshop.dbvis.de/) at\nIEEE VIS is dedicated to understanding cognitive bias in visualization.\n18\nand rapidly growing body of research, whose impact\nspans a broad range of domains. Our survey goes beyond\nvisualization-focused venues to extend a wide scope that\nalso encompasses relevant works from top venues in AI,\nML, and computer vision. We highlighted visual analytics\nas an integral component in addressing pressing issues in\nmodern AI, helping to discover and communicate insight,\nfrom discerning model bias, understanding models, to pro-\nmoting AI safety. We concluded by highlighting impactful\nresearch directions and open problems.\nACKNOWLEDGMENTS\nThis work was supported by NSF grants IIS-1563816, CNS-\n1704701, and TWC-1526254; NIBIB grant U54EB020404; NSF\nGRFP DGE-1650044; NASA Space Technology Research Fel-\nlowship; and gifts from Intel, Google, Symantec.\nREFERENCES\n[1]\nW. S. McCulloch and W. Pitts, “A logical calculus of the ideas im-\nmanent in nervous activity,” The bulletin of mathematical biophysics,\nvol. 5, no. 4, 1943.\n[2]\nW. Rawat and Z. Wang, “Deep convolutional neural networks\nfor image classiﬁcation: A comprehensive review,” Neural compu-\ntation, vol. 29, no. 9, 2017.\n[3]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁ-\ncation with deep convolutional neural networks,” in NIPS, 2012.\n[4]\nK. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside con-\nvolutional networks: Visualising image classiﬁcation models and\nsaliency maps,” arXiv:1312.6034, 2013.\n[5]\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with\nconvolutions,” in CVPR, 2015.\n[6]\nA.\nKarpathy,\n“What\nI\nlearned\nfrom\ncompet-\ning\nagainst\na\nconvnet\non\nImageNet,”\n2014.\n[Online].\nAvailable:\nhttp://karpathy.github.io/2014/09/02/\nwhat-i-learned-from-eompeting-against-a-convnet-on-imagenet\n[7]\nK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016.\n[8]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n“ImageNet: A large-scale hierarchical image database,” in CVPR,\n2009.\n[9]\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge scale visual recognition challenge,” IJCV, vol. 115, no. 3,\n2015.\n[10]\nM. D. Zeiler and R. Fergus, “Visualizing and understanding\nconvolutional networks,” in ECCV.\nSpringer, 2014.\n[11]\nM. W. Craven and J. W. Shavlik, “Visualizing learning and\ncomputation in artiﬁcial neural networks,” International Journal\non Artiﬁcial Intelligence Tools, vol. 1, no. 03, 1992.\n[12]\nM. J. Streeter, M. O. Ward, and S. A. Alvarez, “NVIS: An in-\nteractive visualization tool for neural networks,” in Visual Data\nExploration and Analysis VIII, vol. 4302.\nInternational Society for\nOptics and Photonics, 2001.\n[13]\nF.-Y. Tzeng and K.-L. Ma, “Opening the black box: Data driven\nvisualization of neural networks,” in IEEE Visualization, 2005.\n[14]\nM. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu, “Towards better\nanalysis of deep convolutional neural networks,” IEEE TVCG,\nvol. 23, no. 1, 2017.\n[15]\nK. Wongsuphasawat, D. Smilkov, J. Wexler, J. Wilson, D. Man´e,\nD. Fritz, D. Krishnan, F. B. Vi´egas, and M. Wattenberg, “Visual-\nizing dataﬂow graphs of deep learning models in TensorFlow,”\nIEEE TVCG, vol. 24, no. 1, 2018.\n[16]\nD. Smilkov, S. Carter, D. Sculley, F. B. Viegas, and M. Wattenberg,\n“Direct-manipulation visualization of deep networks,” in ICML\nWorkshop on Vis for Deep Learning, 2016.\n[17]\nJ. Lu, W. Chen, Y. Ma, J. Ke, Z. Li, F. Zhang, and R. Maciejew-\nski, “Recent progress and trends in predictive visual analytics,”\nFrontiers of Computer Science, 2017.\n[18]\nY. Lu, R. Garcia, B. Hansen, M. Gleicher, and R. Maciejewski,\n“The state-of-the-art in predictive visual analytics,” in Computer\nGraphics Forum, vol. 36, no. 3.\nWiley Online Library, 2017.\n[19]\nD. Ren, S. Amershi, B. Lee, J. Suh, and J. D. Williams, “Squares:\nSupporting interactive performance analysis for multiclass clas-\nsiﬁers,” IEEE TVCG, vol. 23, no. 1, 2017.\n[20]\nS. Amershi, M. Cakmak, W. B. Knox, and T. Kulesza, “Power to\nthe people: The role of humans in interactive machine learning,”\nAI Magazine, vol. 35, no. 4, 2014.\n[21]\nD. Sacha, M. Sedlmair, L. Zhang, J. A. Lee, D. Weiskopf, S. North,\nand D. Keim, “Human-centered machine learning through inter-\nactive visualization,” in ESANN, 2016.\n[22]\nC. Seifert, A. Aamir, A. Balagopalan, D. Jain, A. Sharma, S. Grot-\ntel, and S. Gumhold, “Visualizations of deep neural networks in\ncomputer vision: A survey,” in Transparent Data Mining for Big\nand Small Data.\nSpringer, 2017.\n[23]\nH. Zeng, “Towards better understanding of deep learning with\nvisualization,” The Hong Kong University of Science and Technology,\n2016.\n[24]\nS. Liu, X. Wang, M. Liu, and J. Zhu, “Towards better analysis of\nmachine learning models: A visual analytics perspective,” Visual\nInformatics, vol. 1, no. 1, 2017.\n[25]\nJ. Choo and S. Liu, “Visual analytics for explainable deep learn-\ning,” IEEE Computer Graphics and Applications, 2018.\n[26]\nI. Goodfellow, Y. Bengio, and A. Courville, Deep Learning.\nMIT\nPress, 2016, http://www.deeplearningbook.org.\n[27]\nM. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean, M. Devin et al., “TensorFlow:\nLarge-scale machine learning on heterogeneous distributed sys-\ntems,” arXiv:1603.04467, 2016.\n[28]\nD. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, “Network\nDissection: Quantifying interpretability of deep visual represen-\ntations,” in CVPR, 2017.\n[29]\nA. Bilal, A. Jourabloo, M. Ye, X. Liu, and L. Ren, “Do convo-\nlutional neural networks learn class hierarchy?” IEEE TVCG,\nvol. 24, no. 1, pp. 152–162, 2018.\n[30]\nM. Bojarski, A. Choromanska, K. Choromanski, B. Firner,\nL. Jackel, U. Muller, and K. Zieba, “Visualbackprop: visualizing\ncnns for autonomous driving,” arXiv:1611.05418, 2016.\n[31]\nD. Bruckner, “Ml-o-scope: a diagnostic visualization system\nfor\ndeep\nmachine\nlearning\npipelines,”\nMaster’s\nthesis,\nEECS Department, University of California, Berkeley, May\n2014.\n[Online].\nAvailable:\nhttp://www2.eecs.berkeley.edu/\nPubs/TechRpts/2014/EECS-2014-99.html\n[32]\nS. Carter, D. Ha, I. Johnson, and C. Olah, “Experiments in\nhandwriting with a neural network,” Distill, 2016.\n[33]\nD. Cashman, G. Patterson, A. Mosca, and R. Chang, “RNNbow:\nVisualizing learning via backpropagation gradients in recurrent\nneural networks,” in Workshop on Visual Analytics for Deep Learn-\ning, 2017.\n[34]\nJ. Chae, S. Gao, A. Ramanthan, C. Steed, and G. D. Tourassi,\n“Visualization for classiﬁcation in deep neural networks,” in\nWorkshop on Visual Analytics for Deep Learning, 2017.\n[35]\nS. Chung, C. Park, S. Suh, K. Kang, J. Choo, and B. C. Kwon,\n“ReVACNN: Steering convolutional neural network via real-\ntime visual analytics,” in NIPS Workshop on Future of Interactive\nLearning Machines, 2016.\n[36]\nY. Goyal, A. Mohapatra, D. Parikh, and D. Batra, “Towards\ntransparent ai systems: Interpreting visual question answering\nmodels,” arXiv:1608.08974, 2016.\n[37]\nA. W. Harley, “An interactive node-link visualization of convolu-\ntional neural networks,” in ISVC, 2015, pp. 867–877.\n[38]\nF. Hohman, N. Hodas, and D. H. Chau, “ShapeShop: Towards\nunderstanding deep learning representations via interactive ex-\nperimentation,” in CHI, Extended Abstracts, 2017.\n[39]\nM. Kahng, P. Andrews, A. Kalro, and D. H. Chau, “ActiVis: Visual\nexploration of industry-scale deep neural network models,” IEEE\nTVCG, vol. 24, no. 1, 2018.\n[40]\nA. Karpathy, J. Johnson, and L. Fei-Fei, “Visualizing and under-\nstanding recurrent networks,” arXiv:1506.02078, 2015.\n[41]\nJ. Li, X. Chen, E. Hovy, and D. Jurafsky, “Visualizing and under-\nstanding neural models in nlp,” arXiv:1506.01066, 2015.\n[42]\nM. Liu, J. Shi, K. Cao, J. Zhu, and S. Liu, “Analyzing the training\nprocesses of deep generative models,” IEEE TVCG, vol. 24, no. 1,\n2018.\n[43]\nY. Ming, S. Cao, R. Zhang, Z. Li, and Y. Chen, “Understanding\nhidden memories of recurrent neural networks,” VAST, 2017.\n19\n[44]\nA. P. Norton and Y. Qi, “Adversarial-Playground: A visualization\nsuite showing how adversarial examples fool deep learning,” in\nVizSec.\nIEEE, 2017.\n[45]\nC.\nOlah,\n“Visualizing\nMNIST,”\nOlah’s\nBlog,\n2014.\n[Online].\nAvailable:\nhttp://colah.github.io/posts/\n2014-10-Visualizing-MNIST/\n[46]\nC. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye,\nand A. Mordvintsev, “The building blocks of interpretability,”\nDistill, 2018.\n[47]\nN. Pezzotti, T. H¨ollt, J. Van Gemert, B. P. Lelieveldt, E. Eisemann,\nand A. Vilanova, “DeepEyes: Progressive visual analytics for\ndesigning deep neural networks,” IEEE TVCG, vol. 24, no. 1,\n2018.\n[48]\nP. E. Rauber, S. G. Fadel, A. X. Falcao, and A. C. Telea, “Visualiz-\ning the hidden activity of artiﬁcial neural networks,” IEEE TVCG,\nvol. 23, no. 1, 2017.\n[49]\nC. Robinson, F. Hohman, and B. Dilkina, “A deep learning\napproach for population estimation from satellite imagery,” in\nSIGSPATIAL Workshop on Geospatial Humanities, 2017.\n[50]\nX. Rong and E. Adar, “Visual tools for debugging neural lan-\nguage models,” in ICML Workshop on Vis for Deep Learning, 2016.\n[51]\nD. Smilkov, N. Thorat, C. Nicholson, E. Reif, F. B. Vi´egas, and\nM. Wattenberg, “Embedding Projector: Interactive visualization\nand interpretation of embeddings,” in NIPS Workshop on Inter-\npretable ML in Complex Systems, 2016.\n[52]\nH. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush, “LSTMVis:\nA tool for visual analysis of hidden state dynamics in recurrent\nneural networks,” IEEE TVCG, vol. 24, no. 1, 2018.\n[53]\nJ. Wang, L. Gou, H. Yang, and H.-W. Shen, “GANViz: A visual\nanalytics approach to understand the adversarial game,” IEEE\nTVCG, 2018.\n[54]\nB.\nWebster,\n“Now\nanyone\ncan\nexplore\nmachine\nlearning,\nno\ncoding\nrequired,”\nGoogle\nOfﬁcial\nBlog,\n2017.\n[Online].\nAvailable: https://www.blog.google/topics/machine-learning/\nnow-anyone-can-explore-machine-learning-no-coding-required/\n[55]\nJ. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, “Under-\nstanding neural networks through deep visualization,” in ICML\nDeep Learning Workshop, 2015.\n[56]\nT. Zahavy, N. Ben-Zrihem, and S. Mannor, “Graying the black\nbox: Understanding DQNs,” in ICML, 2016.\n[57]\nH. Zeng, H. Haleem, X. Plantaz, N. Cao, and H. Qu, “CN-\nNComparator: Comparative analytics of convolutional neural\nnetworks,” in Workshop on Visual Analytics for Deep Learning, 2017.\n[58]\nW. Zhong, C. Xie, Y. Zhong, Y. Wang, W. Xu, S. Cheng, and\nK. Mueller, “Evolutionary visual analysis of deep neural net-\nworks,” in ICML Workshop on Vis for Deep Learning, 2017.\n[59]\nJ.-Y. Zhu, P. Kr¨ahenb¨uhl, E. Shechtman, and A. A. Efros, “Gen-\nerative visual manipulation on the natural image manifold,” in\nECCV.\nSpringer, 2016.\n[60]\nZ.\nC.\nLipton,\n“The\nmythos\nof\nmodel\ninterpretability,”\narXiv:1606.03490, 2016.\n[61]\nG. Montavon, W. Samek, and K.-R. M¨uller, “Methods for inter-\npreting and understanding deep neural networks,” Digital Signal\nProcessing, 2017.\n[62]\nT. Miller, “Explanation in artiﬁcial intelligence: Insights from the\nsocial sciences,” arXiv:1706.07269, 2017.\n[63]\nA. Weller, “Challenges for transparency,” ICML Workshop on\nHuman Interpretability in ML, 2017.\n[64]\nF. Offert, “”i know it when I see it”. visualization and intuitive\ninterpretability,” NIPS Symposium on Interpretable ML, 2017.\n[65]\nM. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen,\nN. Thorat, F. Vi´egas, M. Wattenberg, G. Corrado et al., “Google’s\nmultilingual neural machine translation system: enabling zero-\nshot translation,” arXiv:1611.04558, 2016.\n[66]\nL. M. Zintgraf, T. S. Cohen, T. Adel, and M. Welling, “Visualizing\ndeep neural network decisions: Prediction difference analysis,”\narXiv:1702.04595, 2017.\n[67]\nL. Li, J. Tompkin, P. Michalatos, and H. Pﬁster, “Hierarchical\nvisual feature analysis for city street view datasets,” in Workshop\non Visual Analytics for Deep Learning, 2017.\n[68]\nD. Erhan, Y. Bengio, A. Courville, and P. Vincent, “Visualizing\nhigher-layer features of a deep network,” University of Montreal,\nvol. 1341, 2009.\n[69]\nR. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh,\nand D. Batra, “Grad-CAM: Why did you say that? visual ex-\nplanations from deep networks via gradient-based localization,”\narXiv:1610.02391, 2016.\n[70]\nA. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune,\n“Synthesizing the preferred inputs for neurons in neural net-\nworks via deep generator networks,” in NIPS, 2016.\n[71]\nK. Patel, J. Fogarty, J. A. Landay, and B. Harrison, “Investigating\nstatistical machine learning as a tool for software development,”\nin CHI, 2008.\n[72]\nT. Kulesza, M. Burnett, W.-K. Wong, and S. Stumpf, “Principles\nof explanatory debugging to personalize interactive machine\nlearning,” in IUI, 2015.\n[73]\nB. Nushi, E. Kamar, E. Horvitz, and D. Kossmann, “On human\nintellect and machine failures: Troubleshooting integrative ma-\nchine learning systems,” in AAAI, 2017.\n[74]\nE. Alexander and M. Gleicher, “Task-driven comparison of topic\nmodels,” IEEE TVCG, vol. 22, no. 1, 2016.\n[75]\nH. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady,\nL. Nie, T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu,\nM. Wattenberg, A. M. Hrafnkelsson, T. Boulos, and J. Kubica, “Ad\nclick prediction: A view from the trenches,” in KDD, 2013.\n[76]\nM. Kahng, D. Fang, and D. H. P. Chau, “Visual exploration of\nmachine learning results using data cube analysis,” in SIGMOD\nWorkshop on Human-In-the-Loop Data Analytics, 2016.\n[77]\nW. Yu, K. Yang, Y. Bai, H. Yao, and Y. Rui, “Visualizing and com-\nparing convolutional neural networks,” arXiv:1412.6631, 2014.\n[78]\nM. Wattenberg, F. Vigas, and I. Johnson, “How to use t-SNE\neffectively,” Distill, 2016.\n[79]\nL. v. d. Maaten and G. Hinton, “Visualizing data using t-SNE,”\nJMLR, vol. 9, no. Nov, 2008.\n[80]\nA. Nguyen, J. Yosinski, and J. Clune, “Multifaceted feature visu-\nalization: Uncovering the different types of features learned by\neach neuron in deep neural networks,” in ICML Workshop on Vis\nfor Deep Learning, 2016.\n[81]\nP. E. Rauber, A. X. Falc˜ao, and A. C. Telea, “Visualizing time-\ndependent data using dynamic t-sne,” EuroVis, vol. 2, no. 5, 2016.\n[82]\nS. Liu, P.-T. Bremer, J. J. Thiagarajan, V. Srikumar, B. Wang,\nY. Livnat, and V. Pascucci, “Visual exploration of semantic re-\nlationships in neural word embeddings,” IEEE TVCG, vol. 24,\nno. 1, 2018.\n[83]\nC. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba, “Hog-\ngles: Visualizing object detection features,” in ICCV, 2013.\n[84]\nX.\nRong,\n“word2vec\nparameter\nlearning\nexplained,”\narXiv:1411.2738, 2014.\n[85]\nD. Park, S. Kim, J. Lee, J. Choo, N. Diakopoulos, and N. Elmqvist,\n“ConceptVector: text visual analytics via interactive lexicon\nbuilding using word embedding,” IEEE TVCG, vol. 24, no. 1,\n2018.\n[86]\nD. S. Weld and G. Bansal, “Intelligible artiﬁcial intelligence,”\narXiv:1803.04263, 2018.\n[87]\nC. Olah and S. Carter, “Research debt,” Distill, 2017.\n[88]\nC. Olah, A. Mordvintsev, and L. Schubert, “Feature visualiza-\ntion,” Distill, 2017.\n[89]\nA. Mahendran and A. Vedaldi, “Visualizing deep convolutional\nneural networks using natural pre-images,” IJCV, vol. 120, no. 3,\n2016.\n[90]\nF. Gr¨un, C. Rupprecht, N. Navab, and F. Tombari, “A taxonomy\nand library for visualizing learned features in convolutional\nneural networks,” ICML Workshop on Vis for Deep Learning, 2016.\n[91]\nP.-J. Kindermans, K. T. Sch¨utt, M. Alber, K.-R. M¨uller, and\nS. D¨ahne, “Learning how to explain neural networks: Patternnet\nand patternattribution,” arXiv:1705.05598, 2017.\n[92]\nH. Li, K. Mueller, and X. Chen, “Beyond saliency: understanding\nconvolutional neural networks from saliency prediction on layer-\nwise relevance propagation,” arXiv:1712.08268, 2017.\n[93]\nD. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wattenberg,\n“SmoothGrad: removing noise by adding noise,” in ICML Work-\nshop on Vis for Deep Learning, 2017.\n[94]\nB. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,\n“Learning deep features for discriminative localization,” in\nCVPR, 2016.\n[95]\nA. Dosovitskiy and T. Brox, “Inverting visual representations\nwith convolutional networks,” in CVPR, 2016.\n[96]\nA. Mahendran and A. Vedaldi, “Understanding deep image\nrepresentations by inverting them,” in CVPR, 2015.\n[97]\nB. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,\n“Object detectors emerge in deep scene CNNs,” arXiv:1412.6856,\n2014.\n[98]\nA. Mordvintsev, C. Olah, and M. Tyka, “Inceptionism: Going\ndeeper into neural networks,” Google Research Blog, 2015.\n20\n[99]\nJ. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-\nmiller, “Striving for simplicity: The all convolutional net,”\narXiv:1412.6806, 2014.\n[100] D. Wei, B. Zhou, A. Torrabla, and W. Freeman, “Understanding\nintra-class knowledge inside CNN,” arXiv:1507.02379, 2015.\n[101] A. Nguyen, J. Yosinski, Y. Bengio, A. Dosovitskiy, and J. Clune,\n“Plug & play generative networks: Conditional iterative genera-\ntion of images in latent space,” arXiv:1612.00005, 2016.\n[102] Y. Bengio et al., “Learning deep architectures for ai,” Foundations\nand trends in Machine Learning, vol. 2, no. 1, 2009.\n[103] S. Carter and M. Nielsen, “Using artiﬁcial intelligence to augment\nhuman intelligence,” Distill, 2017.\n[104] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A\nneural image caption generator,” in CVPR, 2015.\n[105] S.\nAntol,\nA.\nAgrawal,\nJ.\nLu,\nM.\nMitchell,\nD.\nBatra,\nC. Lawrence Zitnick, and D. Parikh, “Vqa: Visual question an-\nswering,” in ICCV, 2015.\n[106] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-\nsarial nets,” in NIPS, 2014.\n[107] I. Goodfellow, “NIPS 2016 tutorial: Generative adversarial net-\nworks,” arXiv:1701.00160, 2016.\n[108] S. Liu, D. Maljovec, B. Wang, P.-T. Bremer, and V. Pascucci, “Vi-\nsualizing high-dimensional data: Advances in the past decade,”\nIEEE TVCG, vol. 23, no. 3, 2017.\n[109] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K.-R.\nM¨uller, “Evaluating the visualization of what a deep neural\nnetwork has learned,” IEEE transactions on neural networks and\nlearning systems, 2017.\n[110] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should I trust\nyou?: Explaining the predictions of any classiﬁer,” in KDD, 2016.\n[111] C.-Y. Tsai and D. D. Cox, “Characterizing visual representations\nwithin convolutional neural networks: Toward a quantitative\napproach,” ICML Workshop on Vis for Deep Learning, 2016.\n[112] S. Ritter, D. G. Barrett, A. Santoro, and M. M. Botvinick, “Cog-\nnitive psychology for deep neural networks: A shape bias case\nstudy,” arXiv:1706.08606, 2017.\n[113] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human\nattention in visual question answering: Do humans and deep\nnetworks look at the same regions?” Computer Vision and Image\nUnderstanding, 2017.\n[114] G. K. Tam, V. Kothari, and M. Chen, “An analysis of machine-\nand human-analytics in classiﬁcation,” IEEE TVCG, vol. 23, no. 1,\n2017.\n[115] S. Barocas and A. D. Selbst, “Big data’s disparate impact,” Calif.\nL. Rev., vol. 104, pp. 671–769, 2016.\n[116] A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics de-\nrived automatically from language corpora contain human-like\nbiases,” Science, vol. 356, no. 6334, 2017.\n[117] M.\nWattenberg,\nF.\nViegas,\nand\nM.\nHardt,\n“Attacking\ndiscrimination with smarter machine learning,” Google Research\nWebsite, 2016. [Online]. Available: https://research.google.com/\nbigpicture/attacking-discrimination-in-ml/\n[118] “Facets,”\nGoogle\nPAIR,\n2017.\n[Online].\nAvailable:\nhttps://\npair-code.github.io/facets/\n[119] M. Hardt, E. Price, N. Srebro et al., “Equality of opportunity in\nsupervised learning,” in NIPS, 2016.\n[120] E. Wall, L. Blaha, L. Franklin, and A. Endert, “Warning, bias\nmay occur: A proposed approach to detecting cognitive bias in\ninteractive visual analytics,” VAST, 2017.\n[121] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Good-\nfellow, and R. Fergus, “Intriguing properties of neural networks,”\narXiv:1312.6199, 2013.\n[122] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and\nharnessing adversarial examples,” ICLR, 2014.\n[123] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecognizable\nimages,” in CVPR, 2015.\n[124] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, “On\ndetecting adversarial perturbations,” in ICLR, 2017.\n[125] S. Gu and L. Rigazio, “Towards deep neural network architec-\ntures robust to adversarial examples,” arXiv:1412.5068, 2014.\n[126] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distilla-\ntion as a defense to adversarial perturbations against deep neural\nnetworks,” in Security and Privacy, 2016.\n[127] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, S. Li, L. Chen,\nM. E. Kounavis, and D. H. Chau, “Shield: Fast, practical defense\nand vaccination for deep learning using jpeg compression,”\narXiv:1802.06816, 2018.\n[128] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples\nin the physical world,” arXiv:1607.02533, 2016.\nFred Hohman is a PhD student at Georgia\nTech’s College of Computing. His research com-\nbines HCI principles and ML techniques to\nimprove deep learning interpretability. He won\nthe NASA Space Technology Research Fellow-\nship. He received his B.S. in mathematics and\nphysics. He won SIGMOD’17 Best Demo, Hon-\norable Mention; Microsoft AI for Earth Award for\nusing AI to improve sustainability; and the Presi-\ndent’s Fellowship for top incoming PhD students.\nMinsuk Kahng is a computer science PhD stu-\ndent at Georgia Tech. His thesis research fo-\ncuses on building visual analytics tools for ex-\nploring, interpreting, and interacting with com-\nplex machine learning models and results, by\ncombining methods from information visualiza-\ntion, machine learning, and databases. He re-\nceived the Google PhD Fellowship and NSF\nGraduate Research Fellowship. His ActiVis deep\nlearning visualization system has been deployed\non Facebook’s machine learning platform.\nRobert Pienta is an industry researcher in ap-\nplied machine learning and visual analytics. He\nreceived his PhD degree in computational sci-\nence and engineering from Georgia Tech in\n2017. He was an NSF FLAMEL fellow and pres-\nidential scholar at Georgia Tech. His research\ninterests include visual analytics, graph analyt-\nics, and machine learning. In particular, the al-\ngorithms and design techniques for interactive\ngraph querying and exploration.\nDuen Horng (Polo) Chau is an Associate Pro-\nfessor at Georgia Tech. His research bridges\ndata mining and HCI to make sense of massive\ndatasets. His thesis won Carnegie Mellon’s CS\nDissertation Award, Honorable Mention. He re-\nceived awards from Intel, Google, Yahoo, Lex-\nisNexis, and Symantec; He won paper awards\nat SIGMOD, KDD and SDM. He is an ACM IUI\nsteering committee member, IUI15 co-chair, and\nIUI19 program co-chair. His research is deployed\nby Facebook, Symantec, and Yahoo.\n",
  "categories": [
    "cs.HC",
    "cs.AI",
    "cs.LG",
    "stat.ML",
    "H.5.2; I.5.1.d; I.6.9.c; I.6.9.f; I.2.6.g"
  ],
  "published": "2018-01-21",
  "updated": "2018-05-14"
}