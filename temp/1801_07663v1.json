{
  "id": "http://arxiv.org/abs/1801.07663v1",
  "title": "Inverse reinforcement learning in continuous time and space",
  "authors": [
    "Rushikesh Kamalapurkar"
  ],
  "abstract": "This paper develops a data-driven inverse reinforcement learning technique\nfor a class of linear systems to estimate the cost function of an agent online,\nusing input-output measurements. A simultaneous state and parameter estimator\nis utilized to facilitate output-feedback inverse reinforcement learning, and\ncost function estimation is achieved up to multiplication by a constant.",
  "text": "Inverse reinforcement learning in continuous time and space\nRushikesh Kamalapurkar\nAbstract— This paper develops a data-driven inverse rein-\nforcement learning technique for a class of linear systems to\nestimate the cost function of an agent online, using input-output\nmeasurements. A simultaneous state and parameter estimator\nis utilized to facilitate output-feedback inverse reinforcement\nlearning, and cost function estimation is achieved up to multi-\nplication by a constant.\nI. INTRODUCTION\nSeamless cooperation between humans and autonomous\nagents is a vital yet challenging aspect of modern robotic\nsystems. Effective cooperation between humans and au-\ntonomous systems can be achieved if the autonomous sys-\ntems are capable of learning to act by observing other cog-\nnitive entities. Based on the premise that a cost (or reward)\nfunction fully characterizes the intent of the demonstrator,\na method to learn the cost function from observations is\ndeveloped in this paper. The cost-estimation problem ﬁrst\nappears in [1] in a linear-quadratic regulation (LQR) setting,\nand a solution is provided in [2] via linear matrix inequalities.\nFor nonlinear systems and cost functions, computation of\nclosed-form solutions is generally intractable, and hence,\napproximate solutions are sought.\nIn [3]–[6], the cost function of a Markov decision process\n(MDP) is learned using inverse reinforcement learning (IRL).\nIt is demonstrated that the IRL problem is inherently ill-\nposed in the sense that it has multiple possible solutions,\nincluding the trivial ones. To overcome the degeneracy,\nthe cost function that differentiates the optimal behavior\nfrom the suboptimal behaviors by a margin is sought. In\n[7] the maximum entropy principle (cf. [8]) is utilized to\nsolve the ill-posed IRL problem for deterministic MDPs.\nIn [9] a causal version of the maximum entropy principle\nis developed and utilized to solve IRL problems in a fully\nstochastic setting. An IRL algorithm based on minimization\nof the Kullback-Leibler divergence between the empirical\ndistribution of trajectories obtained from a baseline policy\nand the trajectories obtained from the cost-based policy is\ndeveloped in [10].\nIn the past two decades, Bayesian [11], natural gradient\n[12], game theoretic [13], and feature construction based\nmethods [14] have also been developed for IRL. IRL is\nextended to problems with locally optimal demonstrations in\n[15] using likelihood optimization and to problems with non-\nlinear cost functions in [16] using Gaussian processes (GP).\nAnother GP-based IRL algorithm that increases the efﬁ-\nciency and applicability of IRL techniques by autonomously\nRushikesh Kamalapurkar is with the School of Mechanical and\nAerospace\nEngineering\nat\nthe\nOklahoma\nState\nUniversity.\nEmail:\nrushikesh.kamalapurkar@okstate.edu\nsegmenting the overall task into sub-goals is developed in\n[17]. Over the years, intent-based approaches such as IRL\nhave been successfully utilized to teach UxVs and humanoid\nrobots to perform speciﬁc maneuvers in an ofﬂine setting [4],\n[18], [19].\nOfﬂine approaches are ill suited for applications where\nteams of autonomous agents with varying levels of autonomy\nwork together to achieve evolving tasks. For example, con-\nsider a ﬂeet of unmanned air vehicles where only a few of the\nvehicles are remotely controlled by human operators and the\nrest are fully autonomous and capable of synthesizing their\nown control policies based on the task. If the tasks are subject\nto change and are known only to the human operators, the\nautonomous agents need the ability to identify the changing\nobjectives from observations in real-time.\nMotivated by recent progress in real-time reinforcement\nlearning (see, e.g., [20]–[24]), this paper develops an output-\nfeedback IRL technique for a class of linear systems to\nestimate the cost function online using input-output mea-\nsurements. The paper is organized as follows. Section II\ndetails the notation used throughout the paper. Section III\nformulates the problem. Section IV details the development\nof a simultaneous state and parameter estimator that facili-\ntates output-feedback cost estimation. Section V formulates\nthe error signal that is utilized in Section VI to achieve\nonline IRL. Section VII details the purging algorithm used\nto facilitate IRL in conjunction with the state and parameter\nestimator developed in Section IV. Section VIII analyzes\nthe convergence of the developed algorithm and Section X\nconcludes the paper.\nII. NOTATION\nThe n−dimensional Euclidean space is denoted by Rn.\nElements of Rn are interpreted as column vectors and (·)T\ndenotes the vector transpose operator. The set of positive\nintegers excluding 0 is denoted by N. For a ∈R, R≥a\ndenotes the interval [a, ∞) and R>a denotes the interval\n(a, ∞). Unless otherwise speciﬁed, an interval is assumed\nto be right-open. If a ∈Rm and b ∈Rn then [a; b] denotes\nthe concatenated vector\n\u0014a\nb\n\u0015\n∈Rm+n. The notations and In\nand 0n denote n × n identity matrix and the zero element\nof Rn, respectively. Whenever clear from the context, the\nsubscript n is suppressed.\nIII. PROBLEM FORMULATION\nConsider an agent under observation with linear dynamics\nof the form\n˙p = q,\n˙q = Ax + Bu,\n(1)\narXiv:1801.07663v1  [cs.SY]  23 Jan 2018\nwhere p : R≥0 →Rn denotes the generalized position, q :\nR≥0 →Rn denotes the generalized velocity, u : R≥0 →\nRm denotes the control input, x := [pT , qT ]T denotes the\nstate, and A ∈Rn×2n and B ∈Rn×m denote the unknown\nconstant system matrices. Assume that the pair (A′, B′) is\ncontrollable, where A′ :=\n\u00140n×n\nIn×n\nA\n\u0015\n, B′ :=\n\u0014\n0n×m\nB\n\u0015\n.\nThe agent under observation executes a policy that minimizes\nthe inﬁnite horizon cost\nJ (x0, u (·)) ≜\n∞\nˆ\n0\nr (x (τ; x0, u (·)) , u (τ)) dτ\n(2)\nwhere τ 7→x (τ; x0, u (·)) denotes the trajectory of (1) under\nthe control signal u (·) starting from the initial condition x0\nand r : R2n × Rm →R denotes the unknown instantaneous\ncost function deﬁned as r (x, u) = Q (x)+uT Ru, where R ∈\nRm×m is a constant positive deﬁnite matrix and Q : R2n →\nR2n is a positive deﬁnite function such that an optimal policy\nu∗: R2n →Rm exists. For ease of exposition, it is further\nassumed that R = diag {r1, · · · , rm}, and a basis σ : R2n →\nRL is known for Q such that for some W ∗\nQ ∈RL,\nQ (x) =\n\u0000W ∗\nQ\n\u0001T σQ (x) , ∀x ∈R2n.\n(3)\nThe objective of the observer is to estimate the cost\nfunction, r, using measurements of the generalized position\nand the control input. In the following, a model-based\nadaptive approximate dynamic programming based approach\nis developed to achieve the stated objective. To facilitate\nmodel-based approximate dynamic programming, the state,\nx, and the parameters, A and B, of the UxV are estimated\nfrom the input-output measurements using a simultaneous\nstate and parameter estimator. The state and the parameters\nare then utilized in an approximate dynamic programming\nscheme to estimate the cost.\nIV. SIMULTANEOUS STATE AND PARAMETER ESTIMATOR\nThe simultaneous state and parameter estimator developed\nby the authors in [25] is utilized in this result. This section\nprovides a brief overview of the same for completeness. For\nfurther details, the readers are directed to [25].\nTo facilitate parameter estimation, let A1, A2 ∈Rn×n be\nmatrices such that A =: [A1, A2]. The dynamics in (1) can\nbe rearranged to form the linear error system\nF (t) = G (t) θ, ∀t ∈R≥0.\n(4)\nIn (4), θ is a vector of unknown parameters, deﬁned as θ ≜\nh\nvec (A1)T\nvec (A2)T\nvec (B)T iT\n∈R2n2+mn, where\nvec (·) denotes the vectorization operator and the matrices\nF : R≥0 →Rn and G : R≥0 →Rn×(2n2+mn) are deﬁned\nas\nF (t) ≜\n\n\n\np (t−T2−T1)−p (t−T1)\n+p (t)−p (t−T2) ,\nt∈[T1+T2, ∞) ,\n0\nt < T1 + T2.\nG (t) ≜\nh\n(F (t) \u000f In)T\n(G (t) \u000f In)T\n(U (t) \u000f In)T i\n,\nwhere \u000f denotes the Kronecker product. The matrices F, G,\nand U are deﬁned as F (t) := Ip (t) ,\nG (t) := J p (t) −\nJ p (t −T1) ,\nU (t) := Iu (t), for t ∈[T1 + T2, ∞), and\nF (t) = G (t) = U (t) = 0, for t < T1+T2, where I := p 7→\n´ t\nt−T2\n´ σ\nσ−T1 p (τ) dτ dσ, and J := p 7→\n´ t\nt−T2 (p (σ)) dσ.\nFor ease of exposition, it is assumed that a history stack,\ni.e., a set of ordered pairs {(Fi, Gi)}M\ni=1 such that\nFi = Giθ, ∀i ∈{1, · · · , M} ,\n(5)\nis available a priori. A history stack {(Fi, Gi)}M\ni=1 is called\nfull rank if there exists a constant g ∈R such that\n0 < g < λmin {G } ,\n(6)\nwhere the matrix G ∈R(2n2+mn)×(2n2+mn) is deﬁned as\nG := PM\ni=1 GT\ni Gi. The concurrent learning update law to\nestimate the unknown parameters is then given by\n˙ˆθ (t) = kθΓ (t)\nM\nX\ni=1\nGT\ni\n\u0010\nFi −Giˆθ (t)\n\u0011\n,\n(7)\nwhere kθ ∈R>0 is a constant adaptation gain and Γ :\nR≥0 →R(2n2+mn)×(2n2+mn) is the least-squares gain\nupdated using the update law\n˙Γ (t) = β1Γ (t) −kθΓ (t)\nM\nX\ni=1\nGT\ni GiΓ (t) .\n(8)\nUsing arguments similar to [26, Corollary 4.3.2], it can be\nshown that provided λmin\n\b\nΓ−1 (0)\n\t\n> 0, the least squares\ngain matrix satisﬁes\nΓ I(2n2+mn) ≤Γ (t) ≤Γ I(2n2+mn),\n(9)\nwhere Γ and Γ are positive constants.\nTo facilitate parameter estimation based on a prediction\nerror, a state observer is developed in the following. To\nfacilitate the design, the dynamics in (1) are expressed in\nthe form ˙p (t) = q (t), ˙q (t) = Y (x (t) , u (t)) θ, where\nY : Rn × Rm →Rn×(2n2+mn) is deﬁned as\nY (x, u) =\nh\n(p \u000f In)T\n(q \u000f In)T\n(u \u000f In)T i\n.\nThe adaptive state observer is then designed as\n˙ˆp (t) = ˆq (t) ,\nˆp (0) = p (0) ,\n˙ˆq (t) = Y (x (t) , u (t)) ˆθ (t) + ν (t) ,\nˆq (0) = 0,\n(10)\nwhere ˆp : R≥0 →Rn, ˆq : R≥0 →Rn, ˆx : R≥0 →Rn, and\nˆθ : R≥0 →Rn are estimates of p, q, x, and θ, respectively,\nν is the feedback component of the identiﬁer, to be designed\nlater, and the prediction error ˜p : R≥0 →Rn is deﬁned as\n˜p (t) = p (t) −ˆp (t) .\nThe update law for the generalized velocity estimate\ndepends on the entire state x. However, using the structure of\nthe matrix Y and integrating by parts, the observer can be im-\nplemented without using generalized velocity measurements.\nUsing an integral form of (10), the update law in (10) can\nbe implemented without generalized velocity measurements\nas\nˆq (t) =\nt\nˆ\n0\n(u (τ) \u000f In)T vec\n\u0010\nˆB (τ)\n\u0011\ndτ +\nt\nˆ\n0\nν (τ) dτ\n+ ˆq (0) +\nt\nˆ\n0\n(p (τ) \u000f In)T\u0010\nvec\n\u0010\nˆA1 (τ)\n\u0011\n−vec\n\u0010 ˙ˆA2 (τ)\n\u0011\u0011\ndτ\n+(p (t)\u000fIn)Tvec\n\u0010\nˆA2 (t)\n\u0011\n−(p (0)\u000fIn)Tvec\n\u0010\nˆA2 (0)\n\u0011\n(11)\nTo facilitate the design of the feedback component ν, let\nr (t) = ˜q (t) + α˜p (t) + η (t) ,\n(12)\nwhere α > 0 is a constant observer gain and the signal\nη is added to compensate for the fact that the generalized\nvelocity state, q, is not measurable. Based on the subsequent\nstability analysis, the signal η is designed as the output of\nthe dynamic ﬁlter\n˙η (t) = −βη (t) −kr (t) −α˜q (t) ,\nη (0) = 0,\n(13)\nand the feedback component ν is designed as\nν (t) = ˜p (t) −(k + α + β) η (t) ,\n(14)\nwhere β > 0 and k > 0 are constant observer gains. The\ndesign of the signals η and ν to estimate the state from output\nmeasurements is inspired by the p−ﬁlter (cf. [27]). Similar\nto the update law for the generalized velocity, using the the\nfact that ˜p (0) = 0, the signal η can be implemented using\nthe integral form\nη (t) = −\nt\nˆ\n0\n(β + k) η (τ) dτ−\nt\nˆ\n0\nkα˜p (τ) dτ−(k + α) ˜p (t) .\n(15)\nUsing a Lyapunov-based analysis, it can be shown that the\ndeveloped parameter and state estimation results in exponen-\ntial convergence of the state and parameter estimation errors\nto zero. For a detailed analysis of the developed state and\nparameter estimator, see [25].\nV. INVERSE BELLMAN ERROR\nSince the agent under observation makes optimal deci-\nsions, and since the Hamiltonian H : R2n ×R2n ×Rm →R,\ndeﬁned as H (x, y, u) ≜yT (A′x + B′u)+r (x, u), is convex\nin u, the control signal, u (·), and the state, x (·), satisfy the\nHamilton-Jacobi-Bellman equation\nH\n\u0010\nx (t) , ∇x (V ∗(x (t)))T , u (t)\n\u0011\n= 0, ∀t ∈R≥0,\n(16)\nwhere V ∗: R2n →R denotes the unknown optimal value\nfunction. The objective of inverse reinforcement learning is\nto generate an estimate of the unknown cost function, r.\nTo facilitate estimation of the cost function, let ˆV : R2n ×\nRP →R,\n\u0010\nx, ˆWV\n\u0011\n7→ˆW T\nV σV (x) be a parametric estimate\nof the optimal value function, where ˆWV ∈RP are unknown\nparameters, and σV : R2n →RP are known continuously\ndifferentiable features. Assume that given any compact set\nχ ⊂R2n and a constant ϵ > 0, sufﬁciently many features\ncan be selected to ensure the existence of ideal parameters\nW ∗\nV ∈RP such that the error ϵ : R2n →R, deﬁned as\nϵ (x) := V (x) −ˆV (x, W ∗\nV ), satisﬁes supx∈χ |ϵ (x)| < ϵ and\nsupx∈χ |∇xϵ (x)| < ϵ. Using the estimates ˆA1, ˆA2, ˆB, ˆWV ,\nˆWQ, and ˆWR of the parameters A1, A2, B, W ∗\nV , W ∗\nQ, and\nWR := [r1, · · · , rm]T , respectively, and the estimate ˆx of the\nstate, x, in (16), the inverse Bellman error δ′ : R2n × Rm ×\nRL+P +m × R2n2+mn →R is obtained as\nδ′ \u0010\nˆx, u, ˆW, ˆθ\n\u0011\n= ˆW T\nV ∇xσV (ˆx)\n\u0010\nˆA′ˆx + ˆB′u\n\u0011\n+ ˆW T\nQσQ (ˆx)\n+ ˆW T\nR σu (u) ,\n(17)\nwhere σu (u) :=\n\u0002\nu2\n1, · · · , u2\nm\n\u0003\n, ˆA′ :=\n\u00140n×n\nIn×n\nˆA1\nˆ\nA2\n\u0015\n, and\nˆB′ :=\n\u00140n×m\nˆB\n\u0015\n. Rearranging,\nδ′ \u0010\nˆx, u, ˆW ′, ˆθ\n\u0011\n=\n\u0010\nˆW ′\u0011T\nσ′ \u0010\nˆx, u, ˆθ\n\u0011\n,\n(18)\nwhere\nˆW ′\n:=\nh\nˆWV ; ˆWQ; ˆWR\ni\n,\nσ′ \u0010\nˆx, u, ˆθ\n\u0011\n:=\nh\n∇xσV (ˆx)\n\u0010\nˆA′ˆx + ˆB′u\n\u0011\n; σQ (ˆx) ; σu (u)\ni\n. The following\nsection\ndetails\nthe\ndeveloped\nmodel-based\ninverse\nreinforcement learning algorithm.\nVI. INVERSE REINFORCEMENT LEARNING\nThe IRL problem can be solved by computing the esti-\nmates ˆW that minimize the inverse Bellman error in (18).\nTo facilitate the computation, the values of ˆx, u, and ˆθ\nare recorded at time instances {ti < t}N\ni=1 to generate the\nvalues {ˆσ′\nt (ti)}N\ni=1, where N ∈N, N >> L + P + m,\nand ˆσ′\nt (t) := σ′ \u0010\nˆx (t) , u (t) , ˆθ (t)\n\u0011\n. The data in the history\nstack can be collected in a matrix form to yield\n∆′ = ˆΣ′ ˆW ′,\n(19)\nwhere\n∆′\n:=\n[δ′\nt (t1) ; · · · ; δ′\nt (tN)],\nδ′\nt (t)\n:=\nδ′ \u0010\nˆx (t) , u (t) , ˆW ′, ˆθ (t)\n\u0011\n,\nand\nˆΣ′\n:=\nh\n(ˆσ′\nt)T (t1) ; · · · ; (ˆσ′\nt)T (tN)\ni\n.\nNote\nthat\nthe\nsolution\nˆW ′ = 0 trivially minimizes ∆′, which is to say that if the\ncost function is identically zero then every policy is optimal.\nHence, as stated, the IRL problem is clearly ill-posed. In\nfact, the cost functions r (x, u) and Kr (x, u), where K\nis a positive constant, result in identical optimal policies\nand state trajectories. Hence, even if the trivial solution is\ndiscarded, the cost function can only be identiﬁed up to\nmultiplication by a positive constant using the trajectories\nx (·) and u (·).\nTo remove the aforementioned ambiguity without loss of\ngenerality, the ﬁrst element of ˆWR is assumed to be known.\nThe inverse BE in (18) can then be expressed as\nδ′ \u0010\nˆx, u, ˆW, ˆθ\n\u0011\n= ˆW T σ′′ \u0010\nˆx, u, ˆθ\n\u0011\n+ r1σu1 (u) ,\n(20)\nwhere\nσui (u)\ndenotes\nthe\nith\nelement\nof\nthe\nvector\nσu (u),\nthe\nvector\nσ−\nu\ndenotes\nσu,\nwith\nthe\nﬁrst\nelement\nremoved,\nand\nσ′′ \u0010\nˆx, u, ˆθ\n\u0011\n:=\nh\n∇xσV (ˆx)\n\u0010\nˆA′ˆx + ˆB′u\n\u0011\n; σQ (ˆx) ; σ−\nu (u)\ni\n.\nThe closed-form optimal controller corresponding to (2)\nprovides the relationship\n−2Ru (t) = (B′)T ∇xσV (x (t)) W ∗\nV + (B′)T ∇xϵ (x (t)) ,\n(21)\nwhich can be expressed as\n−2r1u1 (t) + ∆u1 = σB1 ˆWV\n∆u−= σ−\nB ˆWV + 2 diag (u2, · · · , um) ˆW −\nR ,\nwhere σB1 and u1 denote the ﬁrst rows and σ−\nB and u−\ndenote all but the ﬁrst rows of σB := (B′)T ∇xσV (x) and u,\nrespectively, and R−:= diag ([r2, · · · , rm]). For notational\nbrevity let σ :=\n\u0014\nσ′′,\n\u0014\nσT\nB\nΘ\n\u0015\u0015\n, where\nΘ :=\n\u0014\n0m×2n,\n\u0014\n01×m−1\n2 diag ([u2, · · · , um])\n\u0015\u0015T\nThe history stack can then be utilized to generate the linear\nsystem\n−Σu1 = ˆΣ ˆW −∆′,\n(22)\nwhere\nˆW\n:=\nh\nˆWV ; ˆWQ; ˆW −\nR\ni\n,\nˆΣ\n:=\n\u0002\nˆσT\nt (t1) ; · · · ; ˆσT\nt (tN)\n\u0003\n,\nand\nΣu1\n:=\n[σ′\nu1 (u (t1)) ; · · · ; σ′\nu1 (u (tN))],\nwhere\nˆσt (τ)\n:=\nσ\n\u0010\nˆx (τ) , u (τ) , ˆθ (τ)\n\u0011\n, σ′\nu1 :=\n\u0002\nσu1; 2r1u1; 0(m−1)×1\n\u0003\n, and\nthe vector ˆW −\nR denotes ˆWR with the ﬁrst element removed.\nAt any time instant t, provided the history stack G (t)\nsatisﬁes\nrank\n\u0010\nˆΣ\n\u0011\n= L + P + m −1,\n(23)\nthen a least-squares estimate of the weights can be obtained\nas\nˆW (t) = −\n\u0010\nˆΣT ˆΣ\n\u0011−1 ˆΣT Σu1.\n(24)\nTo improve numerical stability of the least-squares solution,\nthe data recoded in the history stack is selected to maximize\nthe condition number of ˆΣ while ensuring that the vector Σu1\nremains nonzero. The data selection algorithm is detailed in\nFig. 1.\nVII. PURGING TO EXPLOIT IMPROVED STATE AND\nPARAMETER ESTIMATES\nSince the matrix Σ is a function of the state and parameter\nestimates, the accuracy of the least-squares solution in (24)\ndepends on the accuracy of the state and parameter estimates\nrecoded in G. The state and parameter estimates are likely to\nbe poor during the transient phase of the estimator dynamics.\nAs a result, a least-squares solution computed using data\nrecorded during the transient phase of the estimator may\nbe inaccurate. Based on the observation that the state and\nthe parameter estimates exponentially decay to the origin, a\npurging algorithm is developed in the following to remove\nerroneous state and parameter estimates from the history\nstack.\n1: if an observed, estimated or queried data point (x∗, u∗)\nis available at t = t∗then\n2:\nif the history stack is not full then\n3:\nadd the data point to the history stack\n4:\nelse if κ\n\u0012\u0010\nˆΣ (i ←∗)\n\u0011T\u0010\nˆΣ (i ←∗)\n\u0011\u0013\n<ξ1κ\n\u0010\nˆΣT ˆΣ\n\u0011\n,\nfor some i, and ∥Σu1 (i ←∗)∥≥ξ2 then\n5:\nadd the data point to the history stack\n6:\nϖ ←1\n7:\nelse\n8:\ndiscard the data point\n9:\nϖ ←0\n10:\nend if\n11: end if\nFig. 1.\nAlgorithm for selecting data for the history stack. The\nconstants\nξ1\n≥\n0\nand\nξ2\n>\n0\nare\ntunable\nthresholds.\nThe\noperator κ (·) denotes the condition number of a matrix. For the\nmatrix\nˆΣ\n=\n\u0002\nˆσT\nt (t1) ; · · · ; ˆσT\nt (ti) ; · · · ; ˆσT\nt (tN)\n\u0003\n, Σ (i ←∗)\n:=\n\u0002\nˆσT\nt (t1) ; · · · ; ˆσT\nt (t∗) ; · · · ; ˆσT\nt (tN)\n\u0003\nand\nfor\nthe\nvector\nΣu1\n=\n[σu1 (u (t1)) ; · · · ; σu1 (u (ti)) ; · · · ; σu1 (u (tN))],\nΣu1 (i ←∗)\n:=\n[σu1 (u (t1)) ; · · · ; σu1 (u (t∗)) ; · · · ; σu1 (u (tN))].\nNumerical differentiation is utilized to to gauge the quality\nof the state estimates. Given a constant T > 0, and the\nposition measurements over the interval [0, t], noncausal\nnumerical smoothing methods can be used to generate an ac-\ncurate estimate ˙p (t −T) of the velocity. The signal η1 (t) :=\nxT S1x is used as an indicator of the quality of the state\nestimates, where x :=\n\u0002\n˜p (t) ; ˙p (t −T)\n\u0003\n, and S1 ∈R2n×2n\nis a positive semideﬁnite matrix.\nTo gauge the quality of the parameter estimates, at each\ntime instance t, the model ˙p (τ) = q (τ) ,\n˙q (τ) = ˆAx (τ)+\nˆBu (τ) , is simulated over τ ∈[t −T, t], using the initial\ncondition x (t −T) =\n\u0002\np (t −T) ; ˙p (t −T)\n\u0003\n, to generate\nthe trajectory p : [t −T, t] →Rn. The signal η2 (t) :=\n´ t\nt−T (p (τ))T S2p (τ) dτ is used as an indicator of the\nquality of the parameter estimates, where S2 ∈Rn×n is\na positive semideﬁnite matrix. The composite signal η (t) :=\nη1 (t) + η2 (t) is used as an indicator of the quality of the\nsimultaneous state and parameter estimator.\nThe indicator developed above is used to purge and update\nthe history stack and to update the estimate ˆW according to\nthe algorithm detailed in Fig. 2. The algorithm begins with\nan empty history stack and an initial estimate of the weights\nW0. Values of ˆx, u, ˆθ, and η are recorded in the history\nstack using the algorithm detailed in Fig. 1, where η (t) is\nassumed to be inﬁnite for t < T. The estimate ˆW is held\nat the initial guess until the history stack is full. Then, it is\nupdated using (24) every time a new data point is added to\nthe history stack.\nCommunication with the entity under observation, wher-\never possible can be easily incorporated in the developed\nframework. In the query-based implementation of the de-\nveloped algorithm, the observed input-output trajectories\nare utilized to learn the dynamics of the UxV. Instead of\nusing the estimated state and control trajectories for cost\nestimation, control actions, ui of the entity under observation\n1: ˆW (0) ←W0, s ←0\n2: if κ\n\u0010\nˆΣT ˆΣ\n\u0011\n< κ1 and ϖ = 1 then\n3:\nˆW (t) ←−\n\u0010\nˆΣT ˆΣ\n\u0011−1 ˆΣT Σu1\n4: else\n5:\nHold ˆW at the previous value\n6: end if\n7: if κ\n\u0010\nˆΣT ˆΣ\n\u0011\n< κ2 and η (t) < η (t) then\n8:\nempty the history stack\n9:\ns ←s + 1\n10: end if\nFig. 2.\nAlgorithm for updating the weights and the history stack. The\nconstants κ1 > 0 and κ2 > 0 are tunable thresholds, the index s\ndenotes the number of times the history stack was purged, and η (t) :=\nmin {η (t1) , · · · , η (tM)}.\nin response to randomly selected states, xi, are queried. If\nthe queried state-input pair improves the condition number\nof the history stack then it is stored in the history stack and\nutilized for cost estimation.\nVIII. ANALYSIS\nA\ndetailed\nanalysis\nof\nthe\nsimultaneous\nstate\nand\nparameter estimator is excluded for brevity, and is available\nin [25]. To facilitate the analysis of the IRL algorithm, let\nΣ := [σ (x (t1) , u (t1) , θ) ; · · · ; σ (x (tM) , u (tM) , θ)] and\nlet ˆW ∗denote the least-squares solution of Σ ˆW = −Σu1.\nFurthermore,\nlet\nW\ndenote\nan\nappropriately\nscaled\nversion of the ideal weights, i.e, W :=\nW/r1. Provided\nthe\nrank\ncondition\nin\n(23)\nis\nsatisﬁed,\nthe\ninverse\nHJB equation in 16 implies that ΣW\n=\n−Σu1 −E,\nwhere\nE\n:=\n[∇xϵ (x (t1)) (Ax (t1) + Bu (t1));\n· · · ;\n∇xϵ (x (tM)) (Ax (tM) + Bu (tM))].\nThat\nis,\n\r\r\rW +\n\u0000ΣT Σ\n\u0001−1 ΣT Σu1\n\r\r\r\n≤\n\r\r\r\n\u0000ΣT Σ\n\u0001−1 ΣT E\n\r\r\r.\nSince\nˆW ∗\nis\na\nleast\nsquares\nsolution,\n\r\r\rW −ˆW ∗\r\r\r\n≤\n\r\r\r\n\u0000ΣT Σ\n\u0001−1 ΣT E\n\r\r\r.\nLet ˆΣs, Σu1s, and\nˆWs denote the regression matrices\nand the weight estimates corresponding to the sth history\nstack, respectively, and let Σs denote the ideal regres-\nsion matrix where ˆx (ti) and ˆθ (ti) in ˆΣs are replaced\nwith the corresponding ideal values x (ti) and θ. Let ˆW ∗\ns\ndenote the least-squares solution of Σs ˆW\n=\n−Σu1s.\nProvided ˆΣs satisﬁes the rank condition in (23), then\n\r\r\rW −ˆW ∗\ns\n\r\r\r ≤\n\r\r\r\n\u0000ΣT\ns Σs\n\u0001−1 ΣT\ns E\n\r\r\r. Furthermore,\nˆWs −\nˆW ∗\ns\n=\n\u0012\u0012\u0010\nˆΣT\ns ˆΣs\n\u0011−1 ˆΣT\ns\n\u0013\n−\n\u0010\u0000ΣT\ns Σs\n\u0001−1 ΣT\ns\n\u0011\u0013\nΣu1s\nSince the estimates ˆx and ˆθ exponentially converge to x\nand θ, respectively, the function (x, θ) 7→σ (x, u, θ) is\ncontinuous for all u, and under the rank condition in (23),\nthe function Σ 7→\n\u0000ΣT Σ\n\u0001−1 ΣT is continuous, it can be\nconcluded that ˆWs →ˆW ∗\ns as s →∞, and hence, the error\nbetween the estimates ˆWs and the ideal weights W is O (ϵ)\nas s →∞.\n0\n5\n10\n15\n20\nTime (s)\n-1.5\n-1\n-0.5\n0\n0.5\n1\nFig. 3.\nGeneralized position estimation error.\nIX. SIMULATION\nTo verify the performance of the developed method, a\nlinear quadratic optimal control problem is selected where\nA =\n\u00141\n1\n−1\n1\n5\n1\n1\n1\n\u0015\n,\nB =\n\u00141\n3\n0\n1\n\u0015\n.\nThe weighing matrices in the cost function are selected as\nQ = diag ([1, 2, 3, 6]) and R = [20, 10], where R (1, 1)\nis assumed to be known. The observed input-output trajec-\ntories, along with a prerecorded history stack are used to\nimplement the simultaneous state and parameter estimation\nalgorithm in Section IV. The design parameters in the system\nidentiﬁcation algorithm are selected using trial and error as\nM = 150, T1 = 1s, T2 = 0.8sk = 100, α = 20, β = 10,\nβ1 = 5, kθ = 0.3/M, and Γ (0) = 0.1 ∗IL+P +m−1.\nThe behavior of the system under the optimal controller\nu (t) = R−1 (B′)T Px (t) is observed, where P ∈R2n×2n is\nthe solution to the algebraic Riccati equation corresponding\nto (2). At each time step, a random state vector x∗is selected\nand the optimal action u∗corresponding to the random state\nvector is queried from the entity under observation. The\nqueried state-action pairs (x∗, u∗) are utilized in conjunc-\ntion with the estimated state-action pairs\n\u0010 ˆ\nx (t), u (t)\n\u0011\nto\nimplement the IRL algorithm developed in Section (VI).\nFigs. 3 and 4 demonstrate the performance of the devel-\noped state estimator and Fig. 5 illustrates the performance\nof the developed parameter estimator. The estimation errors\nin the generalized position, the generalized velocity, and the\nunknown plant parameters exponentially decay to the origin.\nFig. 6 indicates that the developed IRL technique can be\nsuccessfully utilized to estimate the cost function of an entity\nunder observation.\n0\n5\n10\n15\n20\nTime (s)\n-0.015\n-0.01\n-0.005\n0\n0.005\n0.01\n0.015\nFig. 4.\nGeneralized velocity estimation error.\n0\n5\n10\n15\n20\nTime (s)\n-2\n0\n2\n4\n6\nFig. 5.\nEstimation error for the unknown parameters in the system\ndynamics.\nX. CONCLUSION\nA data-driven inverse reinforcement learning technique\nis developed for a class of linear systems to estimate the\ncost function of an agent online, using input-output mea-\nsurements. A simultaneous state and parameter estimator is\nutilized to facilitate output-feedback inverse reinforcement\nlearning, and cost function estimation is achieved up to\nmultiplication by a constant. A purging algorithm is utilized\nto update the stored state and parameter estimates and bounds\non the cost estimation error are obtained.\nREFERENCES\n[1] R. E. Kalman, “When is a linear control system optimal?” J. Basic\nEng., vol. 86, no. 1, pp. 51–60, 1964.\n[2] S. Boyd, L. E. Ghaoui, E. Feron, and V. Balakrishnan, Linear matrix\ninequalities in system and control theory.\nSIAM, 1994.\n0\n5\n10\n15\n20\nTime (s)\n-100\n-50\n0\n50\n100\nFig. 6.\nEstimation error for the unknown parameters in the cost function.\n[3] A. Y. Ng and S. Russell, “Algorithms for inverse reinforcement\nlearning,” in Proc. Int. Conf. Mach. Learn. Morgan Kaufmann, 2000,\npp. 663–670.\n[4] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-\nforcement learning,” in Proc. Int. Conf. Mach. Learn., 2004.\n[5] ——, “Exploration and apprenticeship learning in reinforcement learn-\ning,” in Proc. Int. Conf. Mach. Learn.\nACM, 2005, pp. 1–8.\n[6] N. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich, “Maximum margin\nplanning,” in Proc. Int. Conf. Mach. Learn., 2006.\n[7] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum\nentropy inverse reinforcement learning,” in Proc. AAAI Conf. Artif.\nIntel., 2008, pp. 1433–1438.\n[8] E. T. Jaynes, “Information theory and statistical mechanics,” Phys.\nRev., vol. 106, no. 4, pp. 620–630, May 1957.\n[9] B. D. Ziebart, J. A. Bagnell, and A. K. Dey, “Modeling interaction via\nthe principle of maximum causal entropy,” in Proc. Int. Conf. Mach.\nLearn., Sep. 2010, pp. 1255–1262.\n[10] A. Boularias, J. Kober, and J. Peters, “Relative entropy inverse rein-\nforcement learning,” in Proc. Int. Conf. Artif. Intell. Stat., G. Gordon,\nD. Dunson, and M. Dud´ık, Eds., vol. 15.\nJMLR W&CP, 2011.\n[11] D. Ramachandran and E. Amir, “Bayesian inverse reinforcement\nlearning,” in Proc. Int. Joint Conf. Artif. Intell.\nSan Francisco, CA,\nUSA: Morgan Kaufmann Publishers Inc., 2007, pp. 2586–2591.\n[12] G. Neu and C. Szepesvari, “Apprenticeship learning using inverse\nreinforcement learning and gradient methods,” in Proc. Anu. Conf.\nUncertain. Artif. Intell.\nCorvallis, Oregon: AUAI Press, 2007, pp.\n295–302.\n[13] U. Syed and R. E. Schapire, “A game-theoretic approach to\napprenticeship\nlearning,”\nin\nAdvances\nin\nNeural\nInformation\nProcessing Systems 20, J. C. Platt, D. Koller, Y. Singer, and\nS. T. Roweis, Eds.\nCurran Associates, Inc., 2008, pp. 1449–1456.\n[14] S. Levine, Z. Popovic, and V. Koltun, “Feature construction for\ninverse reinforcement learning,” in Advances in Neural Information\nProcessing Systems 23, J. D. Lafferty, C. K. I. Williams, J. Shawe-\nTaylor, R. S. Zemel, and A. Culotta, Eds.\nCurran Associates, Inc.,\n2010, pp. 1342–1350.\n[15] S. Levine and V. Koltun, “Continuous inverse optimal control\nwith locally optimal examples,” in Proc. Int. Conf. Mach. Learn.,\nJ. Langford and J. Pineau, Eds.\nNew York, NY, USA: ACM, 2012,\npp. 41–48.\n[16] S. Levine, Z. Popovic, and V. Koltun, “Nonlinear inverse reinforcement\nlearning with gaussian processes,” in Advances in Neural Information\nProcessing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett,\nF. Pereira, and K. Q. Weinberger, Eds.\nCurran Associates, Inc.,\n2011, pp. 19–27.\n[17] B. Michini and J. P. How, “Bayesian nonparametric inverse reinforce-\nment learning,” in Machine Learning and Knowledge Discovery in\nDatabases, ser. Lecture Notes in Computer Science, P. A. Flach, T. D.\nBie, and N. Cristianini, Eds.\nSpringer Berlin Heidelberg, 2012, vol.\n7524, pp. 148–163.\n[18] K. Mombaur, A. Truong, and J.-P. Laumond, “From human to hu-\nmanoid locomotion—an inverse optimal control approach,” Auton.\nRobot., vol. 28, no. 3, pp. 369–383, 2010.\n[19] B. Michini, T. J. Walsh, A. A. Agha-Mohammadi, and J. P. How,\n“Bayesian nonparametric reward learning from demonstration,” IEEE\nTrans. Robot., vol. 31, no. 2, pp. 369–386, Apr. 2015.\n[20] K. Vamvoudakis and F. Lewis, “Online actor-critic algorithm to\nsolve the continuous-time inﬁnite horizon optimal control problem,”\nAutomatica, vol. 46, no. 5, pp. 878–888, 2010.\n[21] T. Bian, Y. Jiang, and Z.-P. Jiang, “Adaptive dynamic programming\nand optimal control of nonlinear nonafﬁne systems,” Automatica,\nvol. 50, no. 10, pp. 2624–2632, 2014.\n[22] H. Modares and F. L. Lewis, “Optimal tracking control of nonlinear\npartially-unknown constrained-input systems using integral reinforce-\nment learning,” Automatica, vol. 50, no. 7, pp. 1780–1792, 2014.\n[23] R.\nKamalapurkar,\nP.\nWalters,\nand\nW.\nE.\nDixon,\n“Model-\nbased reinforcement learning for approximate optimal regulation,”\nAutomatica, vol. 64, pp. 94–104, Feb. 2016.\n[24] D. Wang, D. Liu, H. Li, B. Luo, and H. Ma, “An approximate optimal\ncontrol approach for robust stabilization of a class of discrete-time\nnonlinear systems with uncertainties,” IEEE Trans. Syst. Man Cybern.\nSyst., vol. 46, no. 5, pp. 713–717, 2016.\n[25] R. Kamalapurkar, “Online output-feedback parameter and state\nestimation for second order linear systems,” in Proc. Am. Control\nConf., Seattle, WA, USA, May 2017, pp. 5672–5677.\n[26] P. Ioannou and J. Sun, Robust adaptive control.\nPrentice Hall, 1996.\n[27] B. Xian, M. S. de Queiroz, D. M. Dawson, and M. McIntyre, “A\ndiscontinuous output feedback controller and velocity observer for\nnonlinear mechanical systems,” Automatica, vol. 40, no. 4, pp. 695–\n700, 2004.\n",
  "categories": [
    "cs.SY",
    "math.OC"
  ],
  "published": "2018-01-23",
  "updated": "2018-01-23"
}