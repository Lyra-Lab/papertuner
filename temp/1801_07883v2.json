{
  "id": "http://arxiv.org/abs/1801.07883v2",
  "title": "Deep Learning for Sentiment Analysis : A Survey",
  "authors": [
    "Lei Zhang",
    "Shuai Wang",
    "Bing Liu"
  ],
  "abstract": "Deep learning has emerged as a powerful machine learning technique that\nlearns multiple layers of representations or features of the data and produces\nstate-of-the-art prediction results. Along with the success of deep learning in\nmany other application domains, deep learning is also popularly used in\nsentiment analysis in recent years. This paper first gives an overview of deep\nlearning and then provides a comprehensive survey of its current applications\nin sentiment analysis.",
  "text": "Deep Learning for Sentiment Analysis: A Survey \nLei Zhang, LinkedIn Corporation, lzhang32@gmail.com \nShuai Wang, University of Illinois at Chicago, shuaiwanghk@gmail.com \nBing Liu, University of Illinois at Chicago, liub@uic.edu \n \nAbstract \nDeep learning has emerged as a powerful machine learning technique that learns multiple layers of \nrepresentations or features of the data and produces state-of-the-art prediction results. Along with \nthe success of deep learning in many other application domains, deep learning is also popularly used \nin sentiment analysis in recent years. This paper first gives an overview of deep learning and then \nprovides a comprehensive survey of its current applications in sentiment analysis.  \nINTRODUCTION \nSentiment analysis or opinion mining is the computational study of people’s opinions, sentiments, \nemotions, appraisals, and attitudes towards entities such as products, services, organizations, \nindividuals, issues, events, topics, and their attributes.1 The inception and rapid growth of the field \ncoincide with those of the social media on the Web, for example, reviews, forum discussions, blogs, \nmicro-blogs, Twitter, and social networks, because for the first time in human history, we have a \nhuge volume of opinionated data recorded in digital forms. Since early 2000, sentiment analysis has \ngrown to be one of the most active research areas in natural language processing (NLP). It is also \nwidely studied in data mining, Web mining, text mining, and information retrieval. In fact, it has \nspread from computer science to management sciences and social sciences such as marketing, \nfinance, political science, communications, health science, and even history, due to its importance to \nbusiness and society as a whole. This proliferation is due to the fact that opinions are central to \nalmost all human activities and are key influencers of our behaviours. Our beliefs and perceptions of \nreality, and the choices we make, are, to a considerable degree, conditioned upon how others see \nand evaluate the world. For this reason, whenever we need to make a decision we often seek out \nthe opinions of others. This is not only true for individuals but also true for organizations.  \nNowadays, if one wants to buy a consumer product, one is no longer limited to asking one’s friends \nand family for opinions because there are many user reviews and discussions about the product in \npublic forums on the Web. For an organization, it may no longer be necessary to conduct surveys, \nopinion polls, and focus groups in order to gather public opinions because there is an abundance of \nsuch information publicly available. In recent years, we have witnessed that opinionated postings in \nsocial media have helped reshape businesses, and sway public sentiments and emotions, which have \nprofoundly impacted on our social and political systems. Such postings have also mobilized masses \nfor political changes such as those happened in some Arab countries in 2011. It has thus become a \nnecessity to collect and study opinions1. \nHowever, finding and monitoring opinion sites on the Web and distilling the information contained \nin them remains a formidable task because of the proliferation of diverse sites. Each site typically \ncontains a huge volume of opinion text that is not always easily deciphered in long blogs and forum \npostings. The average human reader will have difficulty identifying relevant sites and extracting and \nsummarizing the opinions in them. Automated sentiment analysis systems are thus needed. Because \nof this, there are many start-ups focusing on providing sentiment analysis services. Many big \ncorporations have also built their own in-house capabilities. These practical applications and \nindustrial interests have provided strong motivations for research in sentiment analysis. \nExisting research has produced numerous techniques for various tasks of sentiment analysis, which \ninclude both supervised and unsupervised methods. In the supervised setting, early papers used all \ntypes of supervised machine learning methods (such as Support Vector Machines (SVM), Maximum \nEntropy, Naïve Bayes, etc.) and feature combinations. Unsupervised methods include various \nmethods that exploit sentiment lexicons, grammatical analysis, and syntactic patterns. Several \nsurvey books and papers have been published, which cover those early methods and applications \nextensively.1,2,3  \nSince about a decade ago, deep learning has emerged as a powerful machine learning technique4 \nand produced state-of-the-art results in many application domains, ranging from computer vision \nand speech recognition to NLP. Applying deep learning to sentiment analysis has also become very \npopular recently. This paper first gives an overview of deep learning and then provides a \ncomprehensive survey of the sentiment analysis research based on deep learning. \nNEURAL NETWORKS  \nDeep learning is the application of artificial neural networks (neural networks for short) to learning \ntasks using networks of multiple layers. It can exploit much more learning (representation) power of \nneural networks, which once were deemed to be practical only with one or two layers and a small \namount of data.  \nInspired by the structure of the biological brain, neural networks consist of a large number of \ninformation processing units (called neurons) organized in layers, which work in unison. It can learn \nto perform tasks (e.g., classification) by adjusting the connection weights between neurons, \nresembling the learning process of a biological brain. \n \nFigure 1:  Feedforward neural network \nBased on network topologies, neural networks can generally be categorized into feedforward neural \nnetworks and recurrent/recursive neural networks, which can also be mixed and matched. We will \ndescribe recurrent/recursive neural networks later. A simple example of a feedforward neural \nnetwork is given in Figure 1, which consists of three layers 𝐿!, 𝐿! and 𝐿!. 𝐿! is the input layer, which \ncorresponds to the input vector (𝑥!, 𝑥!, 𝑥!) and intercept term +1. 𝐿! is the output layer, which \ncorresponds to the output vector (𝑠!). 𝐿! is the hidden layer, whose output is not visible as a \nnetwork output. A circle in 𝐿! represents an element in the input vector, while a circle in 𝐿! or 𝐿! \nrepresents a neuron, the basic computation element of a neural network. We also call it an \nactivation function. A line between two neurons represents a connection for the flow of information. \nEach connection is associated with a weight, a value controlling the signal between two neurons. \nThe learning of a neural network is achieved by adjusting the weights between neurons with the \ninformation flowing through them. Neurons read output from neurons in the previous layer, process \nthe information, and then generate output to neurons in the next layer. As in Figure 1, the neutral \nnetwork alters weights based on training examples (𝑥(!), 𝑦(!)). After the training process, it will \nobtain a complex form of hypotheses ℎ!,!(𝑥) that fits the data.  \nDiving into the hidden layer, we can see that each neuron in 𝐿! takes input 𝑥!, 𝑥!, 𝑥! and intercept \n+1 from 𝐿!, and outputs a value 𝑓(𝑊!𝑥) = 𝑓(\n𝑊!𝑥! + 𝑏)\n!\n!!!\n by the activation function 𝑓. 𝑊! are \nweights of the connections; 𝑏 is the intercept or bias; 𝑓 is normally non-linear. The common choices \nof 𝑓 are sigmoid function, hyperbolic tangent function (tanh), or rectified linear function (ReLU). \nTheir equations are as follows. \n                                                 𝑓𝑊!𝑥= 𝑠𝑖𝑔𝑚𝑜𝑖𝑑𝑊!𝑥=  \n!\n!!!\"# !!!!                         (1) \n                                                  𝑓𝑊!𝑥= tanh 𝑊!𝑥=  \n!!!!!!!!!!\n!!!!!!!!!!                                  (2)                                           \n                                                𝑓𝑊!𝑥= 𝑅𝑒𝐿𝑈𝑊!𝑥= max 0, 𝑊!𝑥                             (3)                                           \nThe sigmoid function takes a real-valued number and squashes it to a value in the range between 0 \nand 1. The function has been in frequent use historically due to its nice interpretation as the firing \nrate of a neuron: 0 for not firing or 1 for firing. But the non-linearity of the sigmoid has recently \nfallen out of favour because its activations can easily saturate at either tail of 0 or 1, where gradients \nare almost zero and the information flow would be cut. What is more is that its output is not zero-\ncentered, which could introduce undesirable zig-zagging dynamics in the gradient updates for the \nconnection weights in training. Thus, the tanh function is often more preferred in practice as its \noutput range is zero-centered, [-1, 1] instead of [0, 1]. The ReLU function has also become popular \nlately. Its activation is simply thresholded at zero when the input is less than 0. Compared with the \nsigmoid function and the tanh function, ReLU is easy to compute, fast to converge in training and \nyields equal or better performance in neural networks.5 \nIn 𝐿!, we can use the softmax function as the output neuron, which is a generalization of the logistic \nfunction that squashes a K-dimensional vector 𝑋 of arbitrary real values to a K-dimensional vector \n𝜎(𝑋) of real values in the range (0, 1) that add up to 1. The function definition is as follows.  \n                                                 𝜎𝑋! =\n!!!\n!!!\n!\n!!!\n             𝑓𝑜𝑟 𝑗= 1, … , 𝑘                              (4)                                           \nGenerally, softmax is used in the final layer of neural networks for final classification in feedforward \nneural networks.  \nBy connecting together all neurons, the neural network in Figure 1 has parameters (𝑊, 𝑏) =\n (𝑊! , 𝑏! , 𝑊! , 𝑏(!)), where 𝑊!\"\n(!) denotes the weight associated with the connection between \nneuron 𝑗 in layer 𝑙, and neuron 𝑖 in layer 𝑙+ 1. 𝑏!\n(!) is the bias associated with neuron 𝑖 in layer 𝑙+ 1.  \nTo train a neural network, stochastic gradient descent via backpropagation6 is usually employed to \nminimize the cross-entropy loss, which is a loss function for softmax output. Gradients of the loss \nfunction with respect to weights from the last hidden layer to the output layer are first calculated, \nand then gradients of the expressions with respect to weights between upper network layers are \ncalculated recursively by applying the chain rule in a backward manner. With those gradients, the \nweights between layers are adjusted accordingly. It is an iterative refinement process until certain \nstopping criteria are met. The pseudo code for training the neural network in Figure 1 is as follows. \n          Training algorithm: stochastic gradient descent via backpropagation \n         Initialize weights 𝑊 and biases 𝑏 of the neural network 𝑁 with random values \n           do                 \n               for each training example (𝑥! , 𝑦!)   \n                     𝑝!  = neural-network-prediction (𝑁, 𝑥! )       \n                     calculate gradients of loss function ( 𝑝! , 𝑦! )  with respect to 𝑤!  at  layer  𝐿!    \n                     get ∆𝑤! for all weights from hidden layer 𝐿! to output layer 𝐿!    \n                     calculate gradient with respect to 𝑤! by chain rule at layer 𝐿! \nget ∆𝑤! for all weights from input layer 𝐿!  to hidden layer 𝐿!        \n                     update ( 𝑤!, 𝑤! )   \n            until all training examples are classified correctly or other stopping criteria are met \n            return the trained neural network                              \nTable 1: Training the neural network in Figure 1. \nThe above algorithm can be extended to generic feedforward neural network training with multiple \nhidden layers. Note that stochastic gradient descent estimates the parameters for every training \nexample as opposed to the whole set of training examples in batch gradient descent. Therefore, the \nparameter updates have a high variance and cause the loss function to fluctuate to different \nintensities, which helps discover new and possibly better local minima.  \nDEEP LEARNING  \nThe research community lost interests in neural networks in late 1990s mainly because they were \nregarded as only practical for “shallow” neural networks (neural networks with one or two layers) as \ntraining a “deep” neural network (neural networks with more layers) is complicated and \ncomputationally very expensive. However, in the past 10 years, deep learning made breakthrough \nand produced state-of-the-art results in many application domains, starting from computer vision, \nthen speech recognition, and more recently, NLP.7,8 The renaissance of neural networks can be \nattributed to many factors. Most important ones include: (1) the availability of computing power \ndue to the advances in hardware (e.g., GPUs), (2) the availability of huge amounts of training data, \nand (3) the power and flexibility of learning intermediate representations.9  \nIn a nutshell, deep learning uses a cascade of multiple layers of nonlinear processing units for \nfeature extraction and transformation. The lower layers close to the data input learn simple features, \nwhile higher layers learn more complex features derived from lower layer features. The architecture \nforms a hierarchical and powerful feature representation. Figure 2 shows the feature hierarchy from \nthe left (a lower layer) to the right (a higher layer) learned by deep learning in face image \nclassification.10 We can see that the learned image features grow in complexity, starting from \nblobs/edges, then noses/eyes/cheeks, to faces. \n \n                                                   Figure 2:  Feature hierarchy by deep learning \nIn recent years, deep learning models have been extensively applied in the field of NLP and show \ngreat potentials. In the following several sections, we briefly describe the main deep learning \narchitectures and related techniques that have been applied to NLP tasks. \nWORD EMBEDDING  \nMany deep learning models in NLP need word embedding results as input features.7 Word \nembedding is a technique for language modelling and feature learning, which transforms words in a \nvocabulary \nto \nvectors \nof \ncontinuous \nreal \nnumbers \n(e.g., \n𝑤𝑜𝑟𝑑 \"ℎ𝑎𝑡\" →(… , 0.15, … , 0.23, … , 0.41, … ) ). The technique normally involves a mathematic \nembedding from a high-dimensional sparse vector space (e.g., one-hot encoding vector space, in \nwhich each word takes a dimension) to a lower-dimensional dense vector space. Each dimension of \nthe embedding vector represents a latent feature of a word. The vectors may encode linguistic \nregularities and patterns.  \nThe learning of word embeddings can be done using neural networks11-15 or matrix factorization.16,17 \nOne commonly used word embedding system is Word2Veci, which is essentially a computationally-\nefficient neural network prediction model that learns word embeddings from text. It contains \nContinuous Bag-of-Words model (CBOW)13, and Skip-Gram model (SG)14. The CBOW model predicts \nthe target word (e.g., “wearing”) from its context words (“the boy is _ a hat”, where “_” denotes the \ntarget word), while the SG model does the inverse, predicting the context words given the target \nword. Statistically, the CBOW model smoothens over a great deal of distributional information by \ntreating the entire context as one observation. It is effective for smaller datasets. However, the SG \nmodel treats each context-target pair as a new observation and is better for larger datasets.  \nAnother frequently used learning approach is Global Vectorii (GloVe)17, which is trained on the non-\nzero entries of a global word-word co-occurrence matrix. \nAUTOENCODER AND DENOISING AUTOENCODER  \nAutoencoder Neural Network is a three-layer neural network, which sets the target values to be \nequal to the input values. Figure 3 shows an example of an autoencoder architecture.  \n \nFigure 3:  Autoencoder neural network \n                                                             \ni Source code: https://code.google.com/archive/p/word2vec/ \nii Source code: https://github.com/stanfordnlp/GloVe \nGiven the input vector 𝑥∈ [0,1]! , the autoencoder first maps it to a hidden representation \n𝑦∈[0,1]!! by an encoder function ℎ(∙) (e.g., the sigmoid function). The latent representation 𝑦 is \nthen mapped back by a decoder function 𝑔(∙)  into a reconstruction 𝑟𝑥= 𝑔(ℎ𝑥) . The \nautoencoder is typically trained to minimize a form of reconstruction error 𝑙𝑜𝑠𝑠(𝑥, 𝑟𝑥). The \nobjective of the autoencoder is to learn a representation of the input, which is the activation of the \nhidden layer. Due to the nonlinear function ℎ(∙) and 𝑔(∙), the autoencoder is able to learn non-\nlinear representations, which give it much more expressive power than its linear counterparts, such \nas Principal Component Analysis (PCA) or Latent Semantic Analysis (LSA). \nOne often stacks autoencoders into layers. A higher level autoencoder uses the output of the lower \none as its training data. The stacked autoencoders18 along with Restricted Boltzmann Machines \n(RBMs)19 are earliest approaches to building deep neural networks. Once a stack of autoencoders \nhas been trained in an unsupervised fashion, their parameters describing multiple levels of \nrepresentations for 𝑥 (intermediate representations) can be used to initialize a supervised deep \nneural network, which has been shown empirically better than random parameter initialization.  \nThe Denoising Autoencoder (DAE)20 is an extension of autoencoder, in which the input vector 𝑥 is \nstochastically corrupted into a vector 𝑥. And the model is trained to denoise it, that is, to minimize a \ndenoising reconstruction error 𝑙𝑜𝑠𝑠(𝑥, 𝑟𝑥). The idea behind DAE is to force the hidden layer to \ndiscover more robust features and prevent it from simply learning the identity. A robust model \nshould be able to reconstruct the input well even in the presence of noises. For example, deleting or \nadding a few of words from or to a document should not change the semantic of the document.  \nCONVOLUTIONAL NEURAL NETWORK   \nConvolutional Neural Network (CNN) is a special type of feedforward neural network originally \nemployed in the field of computer vision. Its design is inspired by the human visual cortex, a visual \nmechanism in animal brain. The visual cortex contains a lot of cells that are responsible for detecting \nlight in small and overlapping sub-regions of the visual fields, which are called receptive fields. These \ncells act as local filters over the input space. CNN consists of multiple convolutional layers, each of \nwhich performs the function that is processed by the cells in the visual cortex.  \nFigure 4 shows a CNN for recognizing traffic signs.21 The input is a 32x32x1 pixel image (32 x 32 \nrepresents image width x height; 1 represents input channel). In this first stage, the filter (size 5x5x1) \nis used to scan the image. Each region in the input image that the filter projects on is a receptive \nfield. The filter is actually an array of numbers (called weights or parameters). As the filter is sliding \n(or convolving), it is multiplying its weight values with the original pixel values of the image (element \nwise multiplications). The multiplications are all summed up to a single number, which is a \nrepresentative of the receptive field. Every receptive field produces a number. After the filter \nfinishes scanning over the image, we can get an array (size 28x28x1), which is called the activation \nmap or feature map. In CNN, we need to use different filters to scan the input. In Figure 4, we apply \n108 kinds of filters and thus have 108 stacked feature maps in the first stage, which consists of the \nfirst convolutional layer. Following the convolutional layer, a subsampling (or pooling) layer is \nusually used to progressively reduce the spatial size of the representation, thus to reduce the \nnumber of features and the computational complexity of the network. For example, after \nsubsampling in the first stage, the convolutional layer reduces its dimensions to (14x14x108). Note \nthat while the dimensionality of each feature map is reduced, the subsampling step retains the most \nimportant information, with a commonly used subsampling operation being the max pooling. \nAfterwards, the output from the first stage becomes input to the second stage and the new filters \nare employed. The new filter size is 5x5x108, where 108 is the feature map size of the last layer. \nAfter the second stage, CNN uses a fully connected layer and then a softmax readout layer with \noutput classes for classification.  \nConvolutional layers in CNN play the role of feature extractor, which extracts local features as they \nrestrict the receptive fields of the hidden layers to be local. It means that CNN has a special spatially-\nlocal correlation by enforcing a local connectivity pattern between neurons of adjacent layers. Such \na characteristic is useful for classification in NLP, in which we expect to find strong local clues \nregarding class membership, but these clues can appear in different places in the input. For example, \nin a document classification task, a single key phrase (or an n-gram) can help in determining the \ntopic of the document. We would like to learn that certain sequences of words are good indicators \nof the topic, and do not necessarily care where they appear in the document. Convolutional and \npooling layers allow the CNN to learn to find such local indicators, regardless of their positions.8  \n \n \nFigure 4:  Convolutional neural network \nRECURRENT NEURAL NETWORK \nRecurrent Neural Network (RNN)22 is a class of neural networks whose connections between \nneurons form a directed cycle. Unlike feedforward neural networks, RNN can use its internal \n“memory” to process a sequence of inputs, which makes it popular for processing sequential \ninformation. The “memory” means that RNN performs the same task for every element of a \nsequence with each output being dependent on all previous computations, which is like \n“remembering” information about what has been processed so far.  \n \nFigure 5:  Recurrent neural network \nFigure 5 shows an example of a RNN. The left graph is an unfolded network with cycles, while the \nright graph is a folded sequence network with three time steps. The length of time steps is \ndetermined by the length of input. For example, if the word sequence to be processed is a sentence \nof six words, the RNN would be unfolded into a neural network with six time steps or layers. One \nlayer corresponds to a word.  \nIn Figure 5, 𝑥! is the input vector at time step 𝑡. ℎ! is the hidden state at time step 𝑡, which is \ncalculated based on the previous hidden state and the input at the current time step. \n                                                        ℎ! = 𝑓𝑤!!ℎ!!! + 𝑤!!𝑥!                                              (5) \nIn Equation (5), the activation function 𝑓 is usually the tanh function or the ReLU function. 𝑤!! is the \nweight matrix used to condition the input 𝑥!. 𝑤!! is the weight matrix used to condition the \nprevious hidden state ℎ!!!. \n𝑦! is the output probability distribution over the vocabulary at step t. For example, if we want to \npredict the next word in a sentence, it would be a vector of probabilities across the word vocabulary. \n                                                        𝑦! = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥𝑤!!ℎ!                                                     (6)  \nThe hidden state ℎ! is regarded as the memory of the network. It captures information about what \nhappened in all previous time steps. 𝑦! is calculated solely based on the memory ℎ! at time 𝑡 and the \ncorresponding weight matrix 𝑤!!. \nUnlike a feedforward neural network, which uses different parameters at each layer, RNN shares the \nsame parameters (𝑊!!, 𝑊!!, 𝑊!!) across all steps. This means that it performs the same task at \neach step, just with different inputs. This greatly reduces the total number of parameters needed to \nlearn.  \nTheoretically, RNN can make use of the information in arbitrarily long sequences, but in practice, the \nstandard RNN is limited to looking back only a few steps due to the vanishing gradient or exploding \ngradient problem.23  \n \nFigure 6:  Bidirectional RNN (left) and deep bidirectional RNN (right) \nResearchers have developed more sophisticated types of RNN to deal with the shortcomings of the \nstandard RNN model: Bidirectional RNN, Deep Bidirectional RNN and Long Short Term Memory \nnetwork. Bidirectional RNN is based on the idea that the output at each time may not only depend \non the previous elements in the sequence, but also depend on the next elements in the sequence. \nFor instance, to predict a missing word in a sequence, we may need to look at both the left and the \nright context. A bidirectional RNN24 consists of two RNNs, which are stacked on the top of each other. \nThe one that processes the input in its original order and the one that processes the reversed input \nsequence. The output is then computed based on the hidden state of both RNNs. Deep bidirectional \nRNN is similar to bidirectional RNN. The only difference is that it has multiple layers per time step, \nwhich provides higher learning capacity but needs a lot of training data. Figure 6 shows examples of \nbidirectional RNN and deep bidirectional RNN (with two layers) respectively. \nLSTM NETWORK \nLong Short Term Memory network (LSTM)25 is a special type of RNN, which is capable of learning \nlong-term dependencies.  \nAll RNNs have the form of a chain of repeating modules. In standard RNNs, this repeating module \nnormally has a simple structure. However, the repeating module for LSTM is more complicated. \nInstead of having a single neural network layer, there are four layers interacting in a special way. \nBesides, it has two states: hidden state and cell state.          \n \nFigure 7:  Long Short Term Memory network \nFigure 7 shows an example of LSTM. At time step 𝑡, LSTM first decides what information to dump \nfrom the cell state. This decision is made by a sigmoid function/layer 𝜎, called the “forget gate”.  The \nfunction takes ℎ!!! (output from the previous hidden layer) and 𝑥! (current input), and outputs a \nnumber in [0, 1], where 1 means “completely keep” and 0 means “completely dump” in Equation (7).  \n                                   𝑓! = 𝜎𝑊!𝑥! + 𝑈!ℎ!!!                                                                      (7) \nThen LSTM decides what new information to store in the cell state. This has two steps. First, a \nsigmoid function/layer, called the “input gate” as Equation (8), decides which values LSTM will \nupdate. Next, a tanh function/layer creates a vector of new candidate values 𝐶! , which will be \nadded to the cell state. LSTM combines these two to create an update to the state. \n                                   𝑖!  =  𝜎 𝑊!𝑥! + 𝑈!ℎ!!!                                                                     (8) \n                                  𝐶!  = tanh 𝑊!𝑥! + 𝑈!ℎ!!!                                                                 (9) \nIt is now time to update the old cell state 𝐶!!! into new cell state 𝐶! as Equation (10). Note that \nforget gate 𝑓! can control the gradient passes through it and allow for explicit “memory” deletes and \nupdates, which helps alleviate vanishing gradient or exploding gradient problem in standard RNN.   \n                                   𝐶! =  𝑓!  ∗ 𝐶!!!  + 𝑖!  ∗ 𝐶!                                                                 (10) \nFinally, LSTM decides the output, which is based on the cell state.  LSTM first runs a sigmoid layer, \nwhich decides which parts of the cell state to output in Equation (11), called “output gate”. Then, \nLSTM puts the cell state through the tanh function and multiplies it by the output of the sigmoid \ngate, so that LSTM only outputs the parts it decides to as Equation (12).  \n                                   𝑜!  =  𝜎𝑊!𝑥! + 𝑈!ℎ!!!                                                                      (11) \n                                   ℎ! =  𝑜!  ∗tanh 𝐶!                                                                                 (12) \nLSTM is commonly applied to sequential data but can also be used for tree-structured data. Tai et \nal.26 introduced a generalization of the standard LSTM to Tree-structured LSTM (Tree-LSTM) and \nshowed better performances for representing sentence meaning than a sequential LSTM.  \nA slight variation of LSTM is the Gated Recurrent Unit (GRU).27,28 It combines the “forget” and “input” \ngates into a single update gate. It also merges the cell state and hidden state, and makes some other \nchanges. The resulting model is simpler than the standard LSTM model, and has been growing in \npopularity. \nATTENTION MECHANISM WITH RECURRENT NEURAL NETWORK \nSupposedly, bidirectional RNN and LSTM should be able to deal with long-range dependencies in \ndata. But in practice, the long-range dependencies are still problematic to handle. Thus, a technique \ncalled the Attention Mechanism was proposed.   \nThe attention mechanism in neural networks is inspired by the visual attention mechanism found in \nhumans. That is, the human visual attention is able to focus on a certain region of an image with \n“high resolution” while perceiving the surrounding image in “low resolution” and then adjusting the \nfocal point over time. In NLP, the attention mechanism allows the model to learn what to attend to \nbased on the input text and what it has produced so far, rather than encoding the full source text \ninto a fixed-length vector like standard RNN and LSTM. \n \nFigure 8:  Attention mechanism in bidirectional recurrent neural network \nBahdanau et al.29 first utilized the attention mechanism for machine translation in NLP. They \nproposed an encoder-decoder framework where an attention mechanism is used to select reference \nwords in the original language for words in the target language before translation. Figure 8 \nillustrates the use of the attention mechanism in their bidirectional RNN. Note that each decoder \noutput word 𝑦! depends on a weighted combination of all the input states, not just the last state as \nin the normal case. 𝑎!,!  are weights that define in how much of each input state should be weighted \nfor each output. For example, if 𝑎!,! has a big value, it means that the decoder pays a lot of \nattention to the second state in the source sentence while producing the second word of the target \nsentence. The weights of 𝑎!,! sum to 1 normally.  \nMEMORY NETWORK \nWeston et al.30 introduced the concept of Memory Networks (MemNN) for the question answering \nproblem. It works with several inference components combined with a large long-term memory. The \ncomponents can be neural networks. The memory acts as a dynamic knowledge base. The four \nlearnable/inference components function as follows: I component coverts the incoming input to the \ninternal feature representation; G component updates old memories given the new input; O \ncomponent generates output (also in the feature representation space); R component converts the \noutput into a response format. For instance, given a list of sentences and a question for question \nanswering, MemNN finds evidences from those sentences and generates an answer. During \ninference, the I component reads one sentence at a time and encodes it into a vector representation. \nThen the G component updates a piece of memory based on the current sentence representation. \nAfter all sentences are processed, a memory matrix (each row representing a sentence) is generated, \nwhich stores the semantics of the sentences. For a question, MemNN encodes it into a vector \nrepresentation, then the O component uses the vector to select some related evidences from the \nmemory and generates an output vector. Finally, the R component takes the output vector as the \ninput and outputs a final response.  \nBased on MemNN, Sukhbaatar et al.31 proposed an End-to-End Memory Network (MemN2N), which \nis a neural network architecture with a recurrent attention mechanism over the long-term memory \ncomponent and it can be trained in an End-to-End manner through standard backpropagation. It \ndemonstrates that multiple computational layers (hops) in the O component can uncover more \nabstractive evidences than a single layer and yield improved results for question answering and \nlanguage modelling. It is worth noting that each computational layer can be a content-based \nattention model. Thus, MemN2N refines the attention mechanism to some extent. Note also a \nsimilar idea is the Neural Turing Machines reported by Graves et al.32 \nRECURSIVE NEURAL NETWORK \nRecursive Neural Network (RecNN) is a type of neural network that is usually used to learn a \ndirected acyclic graph structure (e.g., tree structure) from data. A recursive neural network can be \nseen as a generalization of the recurrent neural network. Given the structural representation of a \nsentence (e.g., a parse tree), RecNN recursively generates parent representations in a bottom-up \nfashion, by combining tokens to produce representations for phrases, eventually the whole sentence. \nThe sentence level representation can then be used to make a final classification (e.g., sentiment \nclassification) for a given input sentence. An example process of vector composition in RecNN is \nshown in Figure 9 33. The vector of node “very interesting” is composed from the vectors of the node \n“very” and the node “interesting”. Similarly, the node “is very interesting” is composed from the \nphrase node “very interesting” and the word node “is”. \n \nFigure 9:  Recursive Neural network \nSENTIMENT ANALYSIS TASKS \nWe are now ready to survey deep learning applications in sentiment analysis. But before doing that, \nwe first briefly introduce the main sentiment analysis tasks in this section. For additional details, \nplease refer to Liu’s book1 on sentiment analysis.  \nResearchers have mainly studied sentiment analysis at three levels of granularity: document level, \nsentence level, and aspect level. Document level sentiment classification classifies an opinionated \ndocument (e.g., a product review) as expressing an overall positive or negative opinion. It considers \nthe whole document as the basic information unit and assumes that the document is known to be \nopinionated and contain opinions about a single entity (e.g., a particular phone). Sentence level \nsentiment classification classifies individual sentences in a document. However, each sentence \ncannot be assumed to be opinionated. Traditionally, one often first classifies a sentence as \nopinionated or not opinionated, which is called subjectivity classification. Then the resulting \nopinionated sentences are classified as expressing positive or negative opinions. Sentence level \nsentiment classification can also be formulated as a three-class classification problem, that is, to \nclassify a sentence as neutral, positive or negative. Compared with document level and sentence \nlevel sentiment analysis, aspect level sentiment analysis or aspect-based sentiment analysis is more \nfine-grained. Its task is to extract and summarize people’s opinions expressed on entities and \naspects/features of entities, which are also called targets. For example, in a product review, it aims \nto summarize positive and negative opinions on different aspects of the product respectively, \nalthough the general sentiment on the product could be positive or negative. The whole task of \naspect-based sentiment analysis consists of several subtasks such as aspect extraction, entity \nextraction, and aspect sentiment classification. For example, from the sentence, “the voice quality \nof iPhone is great, but its battery sucks”, entity extraction should identify “iPhone” as the entity, and \naspect extraction should identify that “voice quality” and “battery” are two aspects. Aspect \nsentiment classification should classify the sentiment expressed on the voice quality of the iPhone as \npositive and on the battery of the iPhone as negative. Note that for simplicity, in most algorithms \naspect extraction and entity extraction are combined and are called aspect extraction or \nsentiment/opinion target extraction.  \nApart from these core tasks, sentiment analysis also studies emotion analysis, sarcasm detection, \nmultilingual sentiment analysis, etc. See Liu’s book1 for more details. In the following sections, we \nsurvey the deep learning applications in all these sentiment analysis tasks.  \nDOCUMENT LEVEL SENTIMENT CLASSIFICATION \nSentiment classification at the document level is to assign an overall sentiment orientation/polarity \nto an opinion document, i.e., to determine whether the document (e.g., a full online review) conveys \nan overall positive or negative opinion. In this setting, it is a binary classification task. It can also be \nformulated as a regression task, for example, to infer an overall rating score from 1 to 5 stars for the \nreview. Some researchers also treat this as a 5-class classification task.  \nSentiment classification is commonly regarded as a special case of document classification. In such a \nclassification, document representation plays an important role, which should reflect the original \ninformation conveyed by words or sentences in a document. Traditionally, the bag-of-words model \n(BoW) is used to generate text representations in NLP and text mining, by which a document is \nregarded as a bag of its words. Based on BoW, a document is transformed to a numeric feature \nvector with a fixed length, each element of which can be the word occurrence (absence or presence), \nword frequency, or TF-IDF score. Its dimension equals to the size of the vocabulary. A document \nvector from BoW is normally very sparse since a single document only contains a small number of \nwords in a vocabulary. Early neural networks adopted such feature settings.  \nDespite its popularity, BoW has some disadvantages. Firstly, the word order is ignored, which means \nthat two documents can have exactly the same representation as long as they share the same words. \nBag-of-N-Grams, an extension for BoW, can consider the word order in a short context (n-gram), but \nit also suffers from data sparsity and high dimensionality. Secondly, BoW can barely encode the \nsemantics of words. For example, the words “smart”, “clever” and “book” are of equal distance \nbetween them in BoW, but “smart” should be closer to “clever” than “book” semantically.  \nTo tackle the shortcomings of BoW, word embedding techniques based on neural networks \n(introduced in the aforementioned section) were proposed to generate dense vectors (or low-\ndimensional vectors) for word representation, which are, to some extent, able to encode some \nsemantic and syntactic properties of words. With word embeddings as input of words, document \nrepresentation as a dense vector (or called dense document vector) can be derived using neural \nnetworks. \nNotice that in addition to the above two approaches, i.e., using BoW and learning dense vectors for \ndocuments through word embeddings, one can also learn a dense document vector directly from \nBoW. We distinguish the different approaches used in related studies in Table 2. \nWhen documents are properly represented, sentiment classification can be conducted using a \nvariety of neural network models following the traditional supervised learning setting. In some cases, \nneural networks may only be used to extract text features/text representations, and these features \nare fed into some other non-neural classifiers (e.g., SVM) to obtain a final global optimum classifier. \nThe properties of neural networks and SVM complement each other in such a way that their \nadvantages are combined.    \nBesides sophisticated document/text representations, researchers also leveraged the characteristics \nof the data – product reviews, for sentiment classification. For product reviews, several researchers \nfound it beneficial to jointly model sentiment and some additional information (e.g., user \ninformation and product information) for classification. Additionally, since a document often \ncontains long dependency relations, the attention mechanism is also frequently used in document \nlevel sentiment classification. We summarize the existing techniques in Table 2.  \n \n \nResearch \nWork \nDocument/Text \nRepresentation \nNeural \nNetworks \nModel \nUse Attention \nMechanism \nJoint Modelling \nwith Sentiment \nMoraes et al.34 \nBoW \nANN \n(Artificial \nNeural \nNetwork) \nNo \n- \nLe \nand \nMikolov35 \nLearning dense vector at \nsentence, \nparagraph, \ndocument level  \nParagraph Vector \nNo \n- \nGlorot et al.36 \nBoW to dense document \nvector  \nSDA \n(Stacked \nDenoising \nAutoencoder) \nNo \nUnsupervised \ndata \nrepresentation from \ntarget domains (in \ntransfer \nlearning \nsettings) \nZhai \nand \nZhang37 \nBoW to dense document \nvector  \nDAE \n(Denoising \nAutoencoder) \nNo \n- \nJohnson \nand \nZhang38 \nBoW to dense document \nvector \nBoW-CNN and Seq-CNN \nNo \n- \nTang et al.39 \nWord \nembeddings \nto \ndense document vector  \nCNN/LSTM \n(to \nlearn \nsentence representation) + \nGRU (to learn document \nrepresentation) \nNo \n- \nTang et al.40 \nWord \nembeddings \nto \ndense document vector  \nUPNN (User Product Neutral \nNetwork) based on CNN \nNo \nUser information and \nproduct information \nChen et al.41 \nWord \nembeddings \nto \ndense document vector  \nUPA \n(User \nProduct \nAttention) based on LSTM \nYes \nUser information and \nproduct Information \nDou42 \nWord \nembeddings \nto \ndense document vector \nMemory Network \nYes \nUser information and \nproduct Information \nXu et al.43 \nWord \nembeddings \nto \ndense document vector \nLSTM \nNo \n- \nYang et al.44 \nWord \nembeddings \nto \ndense document vector \nGRU-based \nsequence \nencoder \nHierarchical \nattention \n- \nYin et al.45 \nWord \nembeddings \nto \ndense document vector \nInput encoder and LSTM \nHierarchical \nattention \nAspect/target \ninformation \nZhou et al.46 \nWord \nembeddings \nto \ndense document vector \nLSTM \nHierarchical \nattention \nCross-lingual \ninformation \nLi et al.47 \nWord \nembeddings \nto \ndense document vector \nMemory Network \nYes \nCross-domain \ninformation \nTable 2: Deep learning methods for document level sentiment classification \nBelow, we also give a brief description of these existing representative works.  \nMoraes et al.34 made an empirical comparison between Support Vector Machines (SVM) and \nArtificial Neural Networks (ANN) for document level sentiment classification, which demonstrated \nthat ANN produced competitive results to SVM’s in most cases. \nTo overcome the weakness of BoW, Le and Mikolov35 proposed Paragraph Vector, an unsupervised \nlearning algorithm that learns vector representations for variable-length texts such as sentences, \nparagraphs and documents. The vector representations are learned by predicting the surrounding \nwords in contexts sampled from the paragraph. \nGlorot et al.36 studied domain adaptation problem for sentiment classification. They proposed a \ndeep learning system based on Stacked Denoising Autoencoder with sparse rectifier units, which can \nperform an unsupervised text feature/representation extraction using both labeled and unlabeled \ndata. The features are highly beneficial for domain adaption of sentiment classifiers. \nZhai and Zhang37 introduced a semi-supervised autoencoder, which further considers the sentiment \ninformation in its learning stage in order to obtain better document vectors, for sentiment \nclassification. More specifically, the model learns a task-specific representation of the textual data \nby relaxing the loss function in the autoencoder to the Bregman Divergence and also deriving a \ndiscriminative loss function from the label information. \nJohnson and Zhang38 proposed a CNN variant named BoW-CNN that employs bag-of-word \nconversion in the convolution layer. They also designed a new model, called Seq-CNN, which keeps \nthe sequential information of words by concatenating the one-hot vector of multiple words. \nTang et al.39 proposed a neural network to learn document representation, with the consideration of \nsentence relationships. It first learns the sentence representation with CNN or LSTM from word \nembeddings. Then a GRU is utilized to adaptively encode semantics of sentences and their inherent \nrelations in document representations for sentiment classification.  \nTang et al.40 applied user representations and product representations in review classification. The \nidea is that those representations can capture important global clues such as individual preferences \nof users and overall qualities of products, which can provide better text representations.  \nChen et al.41 also incorporated user information and product information for classification but via \nword and sentence level attentions, which can take into account of the global user preference and \nproduct characteristics at both the word level and the semantic level. Likewise, Dou42 used a deep \nmemory network to capture user and product information. The proposed model can be divided into \ntwo separate parts. In the first part, LSTM is applied to learn a document representation. In the \nsecond part, a deep memory network consisting of multiple computational layers (hops) is used to \npredict the review rating for each document.  \nXu et al.43 proposed a cached LSTM model to capture the overall semantic information in a long text. \nThe memory in the model is divided into several groups with different forgetting rates. The intuition \nis to enable the memory groups with low forgetting rates to capture global semantic features and \nthe ones with high forgetting rates to learn local semantic features. \nYang et al.44 proposed a hierarchical attention network for document level sentiment rating \nprediction of reviews. The model includes two levels of attention mechanisms: one at the word level \nand the other at the sentence level, which allow the model to pay more or less attention to \nindividual words or sentences in constructing the representation of a document.  \nYin et al.45 formulated the document-level aspect-sentiment rating prediction task as a machine \ncomprehension problem and proposed a hierarchical interactive attention-based model. Specifically, \ndocuments and pseudo aspect-questions are interleaved to learn aspect-aware document \nrepresentation.  \nZhou et al.46 designed an attention-based LSTM network for cross-lingual sentiment classification at \nthe document level. The model consists of two attention-based LSTMs for bilingual representation, \nand each LSTM is also hierarchically structured. In this setting, it effectively adapts the sentiment \ninformation from a resource-rich language (English) to a resource-poor language (Chinese) and helps \nimprove the sentiment classification performance.  \nLi et al.47 proposed an adversarial memory network for cross-domain sentiment classification in a \ntransfer learning setting, where the data from the source and the target domain are modelled \ntogether. It jointly trains two networks for sentiment classification and domain classification (i.e., \nwhether a document is from the source or target domain). \nSENTENCE LEVEL SENTIMENT CLASSIFICATION \nSentence level sentiment classification is to determine the sentiment expressed in a single given \nsentence. As discussed earlier, the sentiment of a sentence can be inferred with subjectivity \nclassification48 and polarity classification, where the former classifies whether a sentence is \nsubjective or objective and the latter decides whether a subjective sentence expresses a negative or \npositive sentiment. In existing deep learning models, sentence sentiment classification is usually \nformulated as a joint three-way classification problem, namely, to predict a sentence as positive, \nneural, and negative.  \nSame as document level sentiment classification, sentence representation produced by neural \nnetworks is also important for sentence level sentiment classification. Additionally, since a sentence \nis usually short compared to a document, some syntactic and semantic information (e.g., parse \ntrees, opinion lexicons, and part-of-speech tags) may be used to help. Additional information such as \nreview ratings, social relationship, and cross-domain information can be considered too. For \nexample, social relationships have been exploited in discovering sentiments in social media data \nsuch as tweets. \nIn early research, parse trees (which provide some semantic and syntactic information) were used \ntogether with the original words as the input to neural models, so that the sentiment composition \ncan be better inferred. But lately, CNN and RNN become more popular, and they do not need parse \ntrees to extract features from sentences. Instead, CNN and RNN use word embeddings as input, \nwhich already encode some semantic and syntactic information. Moreover, the model architecture \nof CNN or RNN can help learn intrinsic relationships between words in a sentence too. The related \nworks are introduced in detail below.     \nSocher et al.49 first proposed a semi-supervised Recursive Autoencoders Network (RAE) for \nsentence level sentiment classification, which obtains a reduced dimensional vector representation \nfor a sentence. Later on, Socher et al.50 proposed a Matrix-vector Recursive Neural Network (MV-\nRNN), in which each word is additionally associated with a matrix representation (besides a vector \nrepresentation) in a tree structure. The tree structure is obtained from an external parser. In Socher \net al.51, the authors further introduced the Recursive Neural Tensor Network (RNTN), where tensor-\nbased compositional functions are used to better capture the interactions between elements. Qian \net al.33 proposed two more advanced models, Tag-guided Recursive Neural Network (TG-RNN), \nwhich chooses a composition function according to the part-of-speech tags of a phrase, and Tag-\nembedded Recursive Neural Network / Recursive Neural Tenser Network (TE-RNN/RNTN), which \nlearns tag embeddings and then combines tag and word embeddings together.  \nKalchbrenner et al.52 proposed a Dynamic CNN (called DCNN) for semantic modelling of sentences. \nDCNN uses the dynamic K-Max pooling operator as a non-linear subsampling function. The feature \ngraph induced by the network is able to capture word relations. Kim53 also proposed to use CNN for \nsentence-level sentiment classification and experimented with several variants, namely CNN-rand \n(where word embeddings are randomly initialized), CNN-static (where word embeddings are pre-\ntrained and fixed), CNN-non-static (where word embeddings are pre-trained and fine-tuned) and \nCNN-multichannel (where multiple sets of word embeddings are used).  \ndos Santos and Gatti54 proposed a Character to Sentence CNN (CharSCNN) model. CharSCNN uses \ntwo convolutional layers to extract relevant features from words and sentences of any size to \nperform sentiment analysis of short texts. Wang et al.55 utilized LSTM for Twitter sentiment \nclassification by simulating the interactions of words during the compositional process. \nMultiplicative operations between word embeddings through gate structures are used to provide \nmore flexibility and to produce better compositional results compared to the additive ones in simple \nrecurrent neural network. Similar to bidirectional RNN, the unidirectional LSTM can be extended to a \nbidirectional LSTM56 by allowing bidirectional connections in the hidden layer.  \nWang et al.57 proposed a regional CNN-LSTM model, which consists of two parts: regional CNN and \nLSTM, to predict the valence arousal ratings of text.  \nWang et al.58 described a joint CNN and RNN architecture for sentiment classification of short texts, \nwhich takes advantage of the coarse-grained local features generated by CNN and long-distance \ndependencies learned via RNN. \nGuggilla et al.59 presented a LSTM- and CNN-based deep neural network model, which utilizes \nword2vec and linguistic embeddings for claim classification (classifying sentences to be factual or \nfeeling). \nHuang et al.60 proposed to encode the syntactic knowledge (e.g., part-of-speech tags) in a tree-\nstructured LSTM to enhance phrase and sentence representation. \nAkhtar et al.61 proposed several multi-layer perceptron based ensemble models for fine-gained \nsentiment classification of financial microblogs and news. \nGuan et al.62 employed a weakly-supervised CNN for sentence (and also aspect) level sentiment \nclassification. It contains a two-step learning process: it first learns a sentence representation weakly \nsupervised by overall review ratings and then uses the sentence (and aspect) level labels for fine-\ntuning.  \nTeng et al.63 proposed a context-sensitive lexicon-based method for sentiment classification based \non a simple weighted-sum model, using bidirectional LSTM to learn the sentiment strength, \nintensification and negation of lexicon sentiments in composing the sentiment value of a sentence.  \nYu and Jiang64 studied the problem of learning generalized sentence embeddings for cross-domain \nsentence sentiment classification and designed a neural network model containing two separated \nCNNs that jointly learn two hidden feature representations from both the labeled and unlabeled \ndata.  \nZhao et al.65 introduced a recurrent random walk network learning approach for sentiment \nclassification of opinionated tweets by exploiting the deep semantic representation of both user \nposted tweets and their social relationships.  \nMishra et al.66 utilized CNN to automatically extract cognitive features from the eye-movement (or \ngaze) data of human readers reading the text and used them as enriched features along with textual \nfeatures for sentiment classification.  \nQian et al.67 presented a linguistically regularized LSTM for the task. The proposed model \nincorporates linguistic resources such as sentiment lexicon, negation words and intensity words into \nthe LSTM so as to capture the sentiment effect in sentences more accurately. \nASPECT LEVEL SENTIMENT CLASSIFICATION \nDifferent from the document level and the sentence level sentiment classification, aspect level \nsentiment classification considers both the sentiment and the target information, as a sentiment \nalways has a target. As mentioned earlier, a target is usually an entity or an entity aspect. For \nsimplicity, both entity and aspect are usually just called aspect. Given a sentence and a target aspect, \naspect level sentiment classification aims to infer the sentiment polarity/orientation of the sentence \ntoward the target aspect. For example, in the sentence “the screen is very clear but the battery life is \ntoo short.” the sentiment is positive if the target aspect is “screen” but negative if the target aspect \nis “battery life”. We will discuss automated aspect or target extraction in the next section.  \nAspect level sentiment classification is challenging because modelling the semantic relatedness of a \ntarget with its surrounding context words is difficult. Different context words have different \ninfluences on the sentiment polarity of a sentence towards the target. Therefore, it is necessary \ncapture semantic connections between the target word and the context words when building \nlearning models using neural networks.  \nThere are three important tasks in aspect level sentiment classification using neural networks. The \nfirst task is to represent the context of a target, where the context means the contextual words in a \nsentence or document. This issue can be similarly addressed using the text representation \napproaches mentioned in the above two sections. The second task is to generate a target \nrepresentation, which can properly interact with its context. A general solution is to learn a target \nembedding, which is similar to word embedding. The third task is to identify the important \nsentiment context (words) for the specified target. For example, in the sentence “the screen of \niPhone is clear but batter life is short”, “clear” is the important context word for “screen” and “short” \nis the important context for “battery life”. This task is recently addressed by the attention \nmechanism. Although many deep learning techniques have been proposed to deal with aspect level \nsentiment classification, to our knowledge, there are still no dominating techniques in the literature. \nRelated works and their main focuses are introduced below. \nDong et al.68 proposed an Adaptive Recursive Neural Network (AdaRNN) for target-dependent \ntwitter sentiment classification, which learns to propagate the sentiments of words towards the \ntarget depending on the context and syntactic structure. It uses the representation of the root node \nas the features, and feeds them into the softmax classifier to predict the distribution over classes.    \nVo and Zhang69 studied aspect-based Twitter sentiment classification by making use of rich \nautomatic features, which are additional features obtained using unsupervised learning methods. \nThe paper showed that multiple embeddings, multiple pooling functions, and sentiment lexicons can \noffer rich sources of feature information and help achieve performance gains. \nSince LSTM can capture semantic relations between the target and its context words in a more \nflexible way, Tang et al.70 proposed Target-dependent LSTM (TD-LSTM) and Target-connection LSTM \n(TC-LSTM) to extend LSTM by taking the target into consideration. They regarded the given target as \na feature and concatenated it with the context features for aspect sentiment classification.  \nRuder et al.71 proposed to use a hierarchical and bidirectional LSTM model for aspect level sentiment \nclassification, which is able to leverage both intra- and inter-sentence relations. The sole \ndependence on sentences and their structures within a review renders the proposed model \nlanguage-independent. Word embeddings are fed into a sentence-level bidirectional LSTM. Final \nstates of the forward and backward LSTM are concatenated together with the target embedding and \nfed into a bidirectional review-level LSTM. At every time step, the output of the forward and \nbackward LSTM is concatenated and fed into a final layer, which outputs a probability distribution \nover sentiments. \nConsidering the limitation of work by Dong et al.68 and Vo and Zhang69, Zhang et al.72 proposed a \nsentence level neural model to address the weakness of pooling functions, which do not explicitly \nmodel tweet-level semantics. To achieve that, two gated neural networks are presented. First, a bi-\ndirectional gated neural network is used to connect the words in a tweet so that pooling functions \ncan be applied over the hidden layer instead of words for better representing the target and its \ncontexts. Second, a three-way gated neural network structure is used to model the interaction \nbetween the target mention and its surrounding contexts, addressing the limitations by using gated \nneural network structures to model the syntax and semantics of the enclosing tweet, and the \ninteraction between the surrounding contexts and the target respectively. Gated neural networks \nhave been shown to reduce the bias of standard recurrent neural networks towards the ends of a \nsequence by better propagation of gradients.  \nWang et al.73 proposed an attention-based LSTM method with target embedding, which was proven \nto be an effective way to enforce the neural model to attend to the related part of a sentence. The \nattention mechanism is used to enforce the model to attend to the important part of a sentence, in \nresponse to a specific aspect. Likewise, Yang et al.74 proposed two attention-based bidirectional \nLSTMs to improve the classification performance. Liu and Zhang75 extended the attention modelling \nby differentiating the attention obtained from the left context and the right context of a given \ntarget/aspect. They further controlled their attention contribution by adding multiple gates.  \nTang et al.76 introduced an end-to-end memory network for aspect level sentiment classification, \nwhich employs an attention mechanism with an external memory to capture the importance of each \ncontext word with respect to the given target aspect. This approach explicitly captures the \nimportance of each context word when inferring the sentiment polarity of the aspect. Such \nimportance degree and text representation are calculated with multiple computational layers, each \nof which is a neural attention model over an external memory.    \nLei et al.77 proposed to use a neural network approach to extract pieces of input text as rationales \n(reasons) for review ratings. The model consists of a generator and a decoder. The generator \nspecifies a distribution over possible rationales (extracted text) and the encoder maps any such text \nto a task-specific target vector. For multi-aspect sentiment analysis, each coordinate of the target \nvector represents the response or rating pertaining to the associated aspect.    \nLi et al.78 integrated the target identification task into sentiment classification task to better model \naspect-sentiment interaction. They showed that sentiment identification can be solved with an end-\nto-end machine learning architecture, in which the two sub-tasks are interleaved by a deep memory \nnetwork. In this way, signals produced in target detection provide clues for polarity classification, \nand reversely, the predicted polarity provides feedback to the identification of targets.   \nMa et al.79 proposed an Interactive Attention Network (IAN) that considers both attentions on \ntarget and context. That is, it uses two attention networks to interactively detect the important \nwords of the target expression/description and the important words of its full context.  \nChen et al.80 proposed to utilize a recurrent attention network to better capture the sentiment of \ncomplicated contexts. To achieve that, their proposed model uses a recurrent/dynamic attention \nstructure and learns non-linear combination of the attention in GRUs.  \nTay et al.81 designed a Dyadic Memory Network (DyMemNN) that models dyadic interactions \nbetween aspect and context, by using either neural tensor compositions or holographic \ncompositions for memory selection operation. \nASPECT EXTRACTION AND CATEGORIZATION \nTo perform aspect level sentiment classification, one needs to have aspects (or targets), which can \nbe manually given or automatically extracted. In this section, we discuss existing work for automated \naspect extraction (or aspect term extraction) from a sentence or document using deep learning \nmodels. Let us use an example to state the problem. For example, in the sentence “the image is very \nclear” the word “image” is an aspect term (or sentiment target). The associated problem of aspect \ncategorization is to group the same aspect expressions into a category. For instance, the aspect \nterms “image”, “photo” and “picture” can be grouped into one aspect category named Image. In the \nreview below, we include the extraction of both aspect and entity that are associated with opinions.  \nOne reason why deep learning models can be helpful for this task is that, deep learning is essentially \ngood at learning (complicated) feature representations. When an aspect is properly characterized in \nsome feature space, for example, in one or some hidden layer(s), the semantics or correlation \nbetween an aspect and its context can be captured with the interplay between their corresponding \nfeature representations. In other words, deep learning provides a possible approach to automated \nfeature engineering without human involvement.  \nKatiyar and Cardie82 investigated the use of deep bidirectional LSTMs for joint extraction of opinion \nentities and the IS-FORM and IS-ABOUT relationships that connect the entities. Wang et al.83 further \nproposed a joint model integrating RNN and Conditional Random Fields (CRF) to co-extract aspects \nand opinion terms or expressions. The proposed model can learn high-level discriminative features \nand double-propagate information between aspect and opinion terms simultaneously. Wang et al.84 \nfurther proposed a Coupled Multi-Layer Attention Model (CMLA) for co-extracting of aspect and \nopinion terms. The model consists of an aspect attention and an opinion attention using GRU units. \nAn improved LSTM-based approach was reported by Li and Lam85, specifically for aspect term \nextraction. It consists of three LSTMs, of which two LSTMs are for capturing aspect and sentiment \ninteractions. The third LSTM is to use the sentiment polarity information as an additional guidance.  \nHe et al.86 proposed an attention-based model for unsupervised aspect extraction. The main \nintuition is to utilize the attention mechanism to focus more on aspect-related words while de-\nemphasizing aspect-irrelevant words during the learning of aspect embeddings, similar to the \nautoencoder framework. \nZhang et al.87 extended a CRF model using a neural network to jointly extract aspects and \ncorresponding sentiments. The proposed CRF variant replaces the original discrete features in CRF \nwith continuous word embeddings, and adds a neural layer between the input and output nodes.  \nZhou et al.88 proposed a semi-supervised word embedding learning method to obtain continuous \nword representations on a large set of reviews with noisy labels. With the word vectors learned, \ndeeper and hybrid features are learned by stacking on the word vectors through a neural network. \nFinally, a logistic regression classifier trained with the hybrid features is used to predict the aspect \ncategory. \nYin et al.89 first learned word embedding by considering the dependency path connecting words. \nThen they designed some embedding features that consider the linear context and dependency \ncontext information for CRF-based aspect term extraction. \nXiong et al.90 proposed an attention-based deep distance metric learning model to group aspect \nphrases. The attention-based model is to learn feature representation of contexts. Both aspect \nphrase embedding and context embedding are used to learn a deep feature subspace metric for K-\nmeans clustering. \nPoria et al.91 proposed to use CNN for aspect extraction. They developed a seven-layer deep \nconvolutional neural network to tag each word in opinionated sentences as either aspect or non-\naspect word. Some linguistic patterns are also integrated into the model for further improvement. \nYing et al.92 proposed two RNN-based models for cross-domain aspect extraction. They first used \nrule-based methods to generate an auxiliary label sequence for each sentence. They then trained \nthe models using both the true labels and auxiliary labels, which shows promising results.   \nOPINION EXPRESSION EXTRACTION \nIn this and the next few sections, we discuss deep learning applications to some other sentiment \nanalysis related tasks. This section focuses on the problem of opinion expression extraction (or \nopinion term extraction, or opinion identification), which aims to identify the expressions of \nsentiment in a sentence or a document.  \nSimilar to the aspect extraction, opinion expression extraction using deep learning models is \nworkable because their characteristics could be identified in some feature space as well. \nIrsoy and Cardie93 explored the application of deep bidirectional RNN for the task, which \noutperforms traditional shallow RNNs with the same number of parameters and also previous CRF \nmethods.94  \nLiu et al.95 presented a general class of discriminative models based on the RNN architecture and \nword embedding. The authors used pre-trained word embeddings from three external sources in \ndifferent RNN architectures including Elman-type, Jordan-type, LSTM and their variations.  \nWang et al.83 proposed a model integrating recursive neural networks and CRF to co-extract aspect \nand opinion terms. The aforementioned CMLA is also proposed for co-extraction of aspect and \nopinion terms.84  \nSENTIMENT COMPOSITION  \nSentiment composition claims that the sentiment orientation of an opinion expression is determined \nby the meaning of its constituents as well as the grammatical structure. Due to their particular tree-\nstructure design, RecNN is naturally suitable for this task.51 Irsoy and Cardie96 reported that the \nRecNN with a deep architecture can more accurately capture different aspects of compositionality in \nlanguage, which benefits sentiment compositionality. Zhu et al.97 proposed a neural network for \nintegrating the compositional and non-compositional sentiment in the process of sentiment \ncomposition. \nOPINION HOLDER EXTRACTION \nOpinion holder (or source) extraction is the task of recognizing who holds the opinion (or \nwhom/where the opinion is from).1 For example, in the sentence “John hates his car”, the opinion \nholder is “john”. This problem is commonly formulated as a sequence labelling problem like opinion \nexpression extraction or aspect extraction. Notice that opinion holder can be either explicit (from a \nnoun phrase in the sentence) or implicit (from the writer) as shown by Yang and Cardie98. Deng and \nWiebe99 proposed to use word embeddings of opinion expressions as features for recognizing \nsources of participant opinions and non-participant opinions, where a source can be the noun \nphrase or writer.  \nTEMPORAL OPINION MINING \nTime is also an important dimension in problem definition of sentiments analysis (see Liu’s book1). \nAs time passes by, people may maintain or change their mind, or even give new viewpoints. \nTherefore, predicting future opinion is important in sentiment analysis. Some research using neural \nnetworks has been reported recently to tackle this problem. \nChen et al.100 proposed a Content-based Social Influence Model (CIM) to make opinion behaviour \npredictions of twitter users. That is, it uses the past tweets to predict users’ future opinions. It is \nbased on a neural network framework to encode both the user content and social relation factor \n(one’s opinion about a target is influenced by one’s friends).   \nRashkin et al.101 used LSTMs for targeted sentiment forecast in the social media context. They \nintroduced multilingual connotation frames, which aim at forecasting implied sentiments among \nworld event participants engaged in a frame. \nSENTIMENT ANALYSIS WITH WORD EMBEDDING \nIt is clear that word embeddings played an important role in deep learning based sentiment analysis \nmodels. It is also shown that even without the use of deep learning models, word embeddings can \nbe used as features for non-neural learning models for various tasks. The section thus specifically \nhighlights word embeddings’ contribution to sentiment analysis.  \nWe first present the works of sentiment-encoded word embeddings. For sentiment analysis, directly \napplying regular word methods like CBOW or Skip-gram to learn word embeddings from context can \nencounter problems, because words with similar contexts but opposite sentiment polarities (e.g., \n“good” or “bad”) may be mapped to nearby vectors in the embedding space. Therefore, sentiment-\nencoded word embedding methods have been proposed. Mass el al.102 learned word embeddings \nthat can capture both semantic and sentiment information. Bespalov et al.103 showed that an n-gram \nmodel combined with latent representation would produce a more suitable embedding for \nsentiment classification. Labutov and Lipson104 re-embed existing word embeddings with logistic \nregression by regarding sentiment supervision of sentences as a regularization term.  \nLe and Mikolov35 proposed the concept of paragraph vector to first learn fixed-length representation \nfor variable-length pieces of texts, including sentences, paragraphs and documents. They \nexperimented on both sentence and document-level sentiment classification tasks and achieved \nperformance gains, which demonstrates the merit of paragraph vectors in capturing semantics to \nhelp sentiment classification. Tang et al.105,106 presented models to learn Sentiment-specific Word \nEmbeddings (SSWE), in which not only the semantic but also sentiment information is embedded in \nthe learned word vectors. Wang and Xia107 developed a neural architecture to train a sentiment-\nbearing word embedding by integrating the sentiment supervision at both the document and word \nlevels. Yu et al.108 adopted a refinement strategy to obtain joint semantic-sentiment bearing word \nvectors. \nFeature enrichment and multi-sense word embeddings are also investigated for sentiment analysis. \nVo and Zhang69 studied aspect-based Twitter sentiment classification by making use of rich \nautomatic features, which are additional features obtained using unsupervised learning techniques. \nLi and Jurafsky109 experimented with the utilization of multi-sense word embeddings on various NLP \ntasks. Experimental results show that while such embeddings do improve the performance of some \ntasks, they offer little help to sentiment classification tasks. Ren et al.110 proposed methods to learn \ntopic-enriched multi-prototype word embeddings for Twitter sentiment classification. \nMultilinguistic word embeddings have also been applied to sentiment analysis. Zhou et al.111 \nreported a Bilingual Sentiment Word Embedding (BSWE) model for cross-language sentiment \nclassification. It incorporates the sentiment information into English-Chinese bilingual embeddings \nby employing labeled corpora and their translation, instead of large-scale parallel corpora. Barnes et \nal.112 compared several types of bilingual word embeddings and neural machine translation \ntechniques for cross-lingual aspect-based sentiment classification. \nZhang et al.113 integrated word embeddings with matrix factorization for personalized review-based \nrating prediction. Specifically, the authors refine existing semantics-oriented word vectors (e.g., \nword2vec and GloVe) using sentiment lexicons. Sharma et al.114 proposed a semi-supervised \ntechnique to use sentiment bearing word embeddings for ranking sentiment intensity of adjectives. \nWord embedding techniques have also been utilized or improved to help address various sentiment \nanalysis tasks in many other recent studies.55,62,87,89,95 \nSARCASM ANALYSIS \nSarcasm is a form verbal irony and a closely related concept to sentiment analysis. Recently, there is \na growing interest in NLP communities in sarcasm detection. Researchers have attempted to solve it \nusing deep learning techniques due of their impressive success in many other NLP problems.  \nZhang et al.115 constructed a deep neural network model for tweet sarcasm detection. Their network \nfirst uses a bidirectional GRU model to capture the syntactic and semantic information over tweets \nlocally, and then uses a pooling neural network to extract contextual features automatically from \nhistory tweets for detecting sarcastic tweets.  \nJoshi et al.116 investigated word embeddings-based features for sarcasm detection. They \nexperimented four past algorithms for sarcasm detection with augmented word embeddings \nfeatures and showed promising results. \nPoria et al.117 developed a CNN-based model for sarcasm detection (sarcastic or non-sarcastic tweets \nclassification), by jointly modelling pre-trained emotion, sentiment and personality features, along \nwith the textual information in a tweet. \nPeled and Reichart118 proposed to interpret sarcasm tweets based on a RNN neural machine \ntranslation model.  \nGhosh and Veale119 proposed a CNN and bidirectional LSTM hybrid for sarcasm detection in tweets, \nwhich models both linguistic and psychological contexts.  \nMishra et al.66 utilized CNN to automatically extract cognitive features from the eye-movement (or \ngaze) data to enrich information for sarcasm detection. Word embeddings are also used for irony \nrecognition in English tweets120 and for controversial words identification in debates.121 \nEMOTION ANALYSIS \nEmotions are the subjective feelings and thoughts of human beings. The primary emotions include \nlove, joy, surprise, anger, sadness and fear. The concept of emotion is closely related to sentiment. \nFor example, the strength of a sentiment can be linked to the intensity of certain emotion like joy \nand anger. Thus, many deep learning models are also applied to emotion analysis following the way \nin sentiment analysis. \nWang et al. 122 built a bilingual attention network model for code-switched emotion prediction. A \nLSTM model is used to construct a document level representation of each post, and the attention \nmechanism is employed to capture the informative words from both the monolingual and bilingual \ncontexts. \nZhou et al. 123 proposed an emotional chatting machine to model the emotion influence in large-\nscale conversation generation based on GRU. The technique has also been applied in other papers. \n39,72,115 \nAbdul-Mageed and Ungar124 first built a large dataset for emotion detection automatically by using \ndistant supervision and then used a GRU network for fine-grained emotion detection.  \nFelbo et al. 125 used millions of emoji occurrences in social media for pretraining neural models in \norder to learn better representations of emotional contexts. \nA question-answering approach is proposed using a deep memory network for emotion cause \nextraction.126 Emotion cause extraction aims to identify the reasons behind a certain emotion \nexpressed in text. \nMULTIMODAL DATA FOR SENTIMENT ANALYSIS \nMultimodal data, such as the data carrying textual, visual, and acoustic information, has been used \nto help sentiment analysis as it provides additional sentiment signals to the traditional text features. \nSince deep learning models can map inputs to some latent space for feature representation, the \ninputs from multimodal data can also be projected simultaneously to learn multimodal data fusion, \nfor example, by using feature concatenation, joint latent space, or other more sophisticated fusion \napproaches. There is now a growing trend of using multimodal data with deep learning techniques. \nPoria et al.127 proposed a way of extracting features from short texts based on the activation values \nof an inner layer of CNN. The main novelty of the paper is the use of a deep CNN to extract features \nfrom text and the use of multiple kernel learning (MKL) to classify heterogeneous multimodal fused \nfeature vectors. \nBertero et al.128 described a CNN model for emotion and sentiment recognition in acoustic data from \ninteractive dialog systems.  \nFung et al.129 demonstrated a virtual interaction dialogue system that have incorporated sentiment, \nemotion and personality recognition capabilities trained by deep learning models.  \nWang et al.130 reported a CNN structured deep network, named Deep Coupled Adjective and Noun \n(DCAN) neural network, for visual sentiment classification. The key idea of DCAN is to harness the \nadjective and noun text descriptions, treating them as two (weak) supervision signals to learn two \nintermediate sentiment representations. Those learned representations are then concatenated and \nused for sentiment classification. \nYang et al.131 developed two algorithms based on a conditional probability neural network to analyse \nvisual sentiment in images. \nZhu et al.132 proposed a unified CNN-RNN model for visual emotion recognition. The architecture \nleverages CNN with multiple layers to extract different levels of features (e.g., colour, texture, object, \netc.) within a multi-task learning framework. And a bidirectional RNN is proposed to integrate the \nlearned features from different layers in the CNN model. \nYou et al.133 adopted the attention mechanism for visual sentiment analysis, which can jointly \ndiscover the relevant local image regions and build a sentiment classifier on top of these local \nregions.  \nPoria et al.134 proposed some a deep learning model for multi-modal sentiment analysis and emotion \nrecognition on video data. Particularly, a LSTM-based model is proposed for utterance-level \nsentiment analysis, which can capture contextual information from their surroundings in the same \nvideo.  \nTripathi et al.135 used deep and CNN-based models for emotion classification on a multimodal \ndataset DEAP, which contains electroencephalogram and peripheral physiological and video signals.  \nZadeh et al.136 formulated the problem of multimodal sentiment analysis as modelling intra-modality \nand inter-modality dynamics and introduced a new neural model named Tensor Fusion Network to \ntackle it.  \nLong et al.137 proposed an attention neural model trained with cognition grounded eye-tracking data \nfor sentence-level sentiment classification. A Cognition Based Attention (CBA) layer is built for \nneural sentiment analysis. \nWang et al. 138 proposed a Select-Additive Learning (SAL) approach to tackle the confounding factor \nproblem in multimodal sentiment analysis, which removes the individual specific latent \nrepresentations learned by neural networks (e.g., CNN). To achieve it, two learning phases are \ninvolved, namely, a selection phase for confounding factor identification and a removal phase for \nconfounding factor removal. \nRESOURCE-POOR LANGUAGE AND MULTILINGUAL SENTIMENT ANALYSIS \nRecently, sentiment analysis in resource-poor languages (compared to English) has also achieved \nsignificant progress due to the use of deep learning models. Additionally, multilingual features also \ncan help sentiment analysis just like multimodal data. In the same way, deep learning has been \napplied to the multilingual sentiment analysis setting. \nAkhtar et al.139 reported a CNN-based hybrid architecture for sentence and aspect level sentiment \nclassification in a resource-poor language, Hindi.  \nDahou et al.140 used word embeddings and a CNN-based model for Arabic sentiment classification at \nthe sentence level.  \nSinghal and Bhattacharyya141 designed a solution for multilingual sentiment classification at \nreview/sentence level and experimented with multiple languages, including Hindi, Marathi, Russian, \nDutch, French, Spanish, Italian, German, and Portuguese. The authors applied machine translation \ntools to translate these languages into English and then used English word embeddings, polarities \nfrom a sentiment lexicon and a CNN model for classification.  \nJoshi et al.142 introduced a sub-word level representation in a LSTM architecture for sentiment \nclassification of Hindi-English code-mixed sentences. \nOTHER RELATED TASKS \nThere are also applications of deep learning in some other sentiment analysis related tasks.  \nSentiment Intersubjectivity: Gui et al.143 tackled the intersubjectivity problem in sentiment analysis, \nwhere the problem is to study the gap between the surface form of a language and the \ncorresponding abstract concepts, and incorporate the modelling of intersubjectivity into a proposed \nCNN. \nLexicon Expansion: Wang et al.144 proposed a PU learning-based neural approach for opinion lexicon \nexpansion. \nFinancial Volatility Prediction: Rekabsaz et al.145 made volatility predictions using financial disclosure \nsentiment with word embedding-based information retrieval models, where word embeddings are \nused in similar word set expansion. \nOpinion Recommendation: Wang and Zhang146 introduced the task of opinion recommendation, \nwhich aims to generate a customized review score of a product that the particular user is likely to \ngive, as well as a customized review that the user would have written for the target product if the \nuser had reviewed the product. A multiple-attention memory network was proposed to tackle the \nproblem, which considers users’ reviews, product’s reviews, and users’ neighbours (similar users). \nStance Detection: Augenstein et al.147 proposed a bidirectional LSTMs with a conditional encoding \nmechanism for stance detection in political twitter data. Du et al.148 designed a target-specific neural \nattention model for stance classification. \nCONCLUSION \nApplying deep learning to sentiment analysis has become a popular research topic lately. In this \npaper, we introduced various deep learning architectures and their applications in sentiment \nanalysis. Many of these deep learning techniques have shown state-of-the-art results for various \nsentiment analysis tasks. With the advances of deep learning research and applications, we believe \nthat there will be more exciting research of deep learning for sentiment analysis in the near future. \n \nAcknowledgments \nBing Liu and Shuai Wang’s work was supported in part by National Science Foundation (NSF) under \ngrant no. IIS1407927 and IIS-1650900, and by Huawei Technologies Co. Ltd with a research gift.  \nReferences \n[1] Liu B. Sentiment analysis: mining opinions, sentiments, and emotions. The Cambridge University Press, \n2015.  \n[2] Liu B. Sentiment analysis and opinion mining (introduction and survey), Morgan & Claypool, May 2012. \n[3] Pang B and Lee L. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, \n2008. 2(1–2): pp. 1–135. \n[4] Goodfellow I, Bengio Y, Courville A. Deep learning. The MIT Press. 2016. \n[5] Glorot X, Bordes A, Bengio Y. Deep sparse rectifier neural networks. In Proceedings of the International \nConference on Artificial Intelligence and Statistics (AISTATS 2011), 2011. \n[6] Rumelhart D.E, Hinton G.E, Williams R.J. Learning representations by back-propagating errors. Cognitive \nmodelling, 1988. \n[7] Collobert R, Weston J, Bottou L, Karlen M, Kavukcuoglu K, and Kuksa P. Natural language processing (almost) \nfrom scratch. Journal of Machine Learning Research, 2011. \n[8] Goldberg Y. A primer on neural network models for natural language processing. Journal of Artificial \nIntelligence Research, 2016. \n[9] Bengio Y, Courville A, Vincent P. Representation learning:  a review and new perspectives. IEEE Transactions \non Pattern Analysis and Machine Intelligence, 2013.  \n[10] Lee H, Grosse R, Ranganath R, and Ng A.Y. Convolutional deep belief networks for scalable unsupervised \nlearning of hierarchical representations. In Proceedings of the International Conference on Machine Learning \n(ICML 2009), 2009. \n[11] Bengio Y, Ducharme R, Vincent P, and Jauvin C. A neural probabilistic language model. Journal of Machine \nLearning Research, 2003. \n[12] Morin F, Bengio Y. Hierarchical probabilistic neural network language model. In Proceedings of the \nInternational Workshop on Artificial Intelligence and Statistics, 2005.  \n[13] Mikolov T, Chen K, Corrado G, and Dean J. Efficient estimation of word representations in vector space. In \nProceedings of International Conference on Learning Representations (ICLR 2013), 2013. \n[14] Mikolov T, Sutskever I, Chen K, Corrado G, and Dean J. Distributed representations of words and phrases \nand their compositionality. In Proceedings of the Annual Conference on Advances in Neural Information \nProcessing Systems (NIPS 2013), 2013. \n[15] Mnih A, Kavukcuoglu K. Learning word embeddings efficiently with noise-contrastive estimation. In \nProceedings of the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2013), \n2013. \n[16] Huang E.H, Socher R, Manning C.D. and Ng A.Y. Improving word representations via global context and \nmultiple word prototypes. In Proceedings of the Annual Meeting of the Association for Computational \nLinguistics (ACL 2012), 2012. \n[17] Pennington J, Socher R, Manning C.D. GloVe: global vectors for word representation. In Proceedings of the \nConference on Empirical Methods on Natural Language Processing (EMNLP 2014), 2014. \n[18] Bengio Y, Lamblin P, Popovici D, and Larochelle H. Greedy layer-wise training of deep networks. In \nProceedings of the Annual Conference on Advances in Neural Information Processing Systems (NIPS 2006), \n2006. \n[19] Hinton G.E, Salakhutdinov R.R. Reducing the dimensionality of data with neural networks. Science, July \n2006. \n[20] Vincent P, Larochelle H, Bengio Y, and Manzagol P-A. Extracting and composing robust features with \ndenoising autoencoders. In Proceedings of the International Conference on Machine Learning (ICML 2008), \n2008.  \n[21] Sermanet P, LeCun Y. Traffic sign recognition with multi-scale convolutional networks. In Proceedings of \nthe International Joint Conference on Neural Networks (IJCNN 2011), 2011. \n[22] Elman J.L. Finding structure in time. Cognitive Science, 1990.  \n[23] Bengio Y, Simard P, Frasconi P. Learning long-term dependencies with gradient descent is difficult. IEEE \nTransactions on Neural Networks, 1994.  \n[24] Schuster M, Paliwal K.K. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, \n1997. \n[25] Hochreiter S, Schmidhuber J. Long short-term memory. Neural Computation, 9(8): 1735-1780, 1997. \n[26] Tai K.S, Socher R, Manning C. D. Improved semantic representations from tree-structured long short-term \nmemory networks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL \n2015), 2015. \n[27] Cho K, Bahdanau D, Bougares F, Schwenk H and Bengio Y. Learning phrase representations using RNN \nencoder-decoder for statistical machine translation. In Proceedings of the Conference on Empirical Methods in \nNatural Language Processing (EMNLP 2014), 2014. \n[28] Chung J, Gulcehre C, Cho K, Bengio Y. Empirical evaluation of gated recurrent neural networks on \nsequence modelling. arXiv preprint arXiv:1412.3555, 2014.  \n[29] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv \npreprint arXiv:1409.0473, 2014.   \n[30] Weston J, Chopra S, Bordes A. Memory networks. arXiv preprint arXiv:1410.3916. 2014. \n[31] Sukhbaatar S, Weston J, Fergus R. End-to-end memory networks. In Proceedings of the 29th Conference on \nNeural Information Processing Systems (NIPS 2015), 2015. \n[32] Graves A, Wayne G, Danihelka I. Neural Turing Machines. preprint arXiv:1410.5401. 2014.  \n[33] Qian Q, Tian B, Huang M, Liu Y, Zhu X and Zhu X. Learning tag embeddings and tag-specific composition \nfunctions in the recursive neural network. In Proceedings of the Annual Meeting of the Association for \nComputational Linguistics (ACL 2015), 2015. \n[34] Moraes R, Valiati J.F, Neto W.P. Document-level sentiment classification: an empirical comparison \nbetween SVM and ANN. Expert Systems with Applications. 2013.  \n[35] Le Q, Mikolov T. Distributed representations of sentences and documents. In Proceedings of the \nInternational Conference on Machine Learning (ICML 2014), 2014. \n[36] Glorot X, Bordes A, Bengio Y. Domain adaption for large-scale sentiment classification: a deep learning \napproach. In Proceedings of the International Conference on Machine Learning (ICML 2011), 2011. \n[37] Zhai S, Zhongfei (Mark) Zhang. Semisupervised autoencoder for sentiment analysis. In Proceedings of AAAI \nConference on Artificial Intelligence (AAAI 2016), 2016. \n[38] Johnson R, Zhang T. Effective use of word order for text categorization with convolutional neural networks.  \nIn Proceedings of the Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies (NAACL-HLT 2015), 2015. \n[39] Tang D, Qin B, Liu T. Document modelling with gated recurrent neural network for sentiment classification. \nIn Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), 2015.   \n[40] Tang D, Qin B, Liu T. Learning semantic representations of users and products for document level \nsentiment classification. In Proceedings of the Annual Meeting of the Association for Computational Linguistics \n(ACL 2015), 2015. \n[41] Chen H, Sun M, Tu C, Lin Y, and Liu Z. Neural sentiment classification with user and product attention. In \nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2016), 2016. \n[42] Dou ZY. Capturing user and product Information for document level sentiment analysis with deep memory \nnetwork. In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP \n2017), 2017. \n[43] Xu J, Chen D, Qiu X, and Huang X. Cached long short-term memory neural networks for document-level \nsentiment classification. In Proceedings of the Conference on Empirical Methods in Natural Language \nProcessing (EMNLP 2016), 2016.   \n[44] Yang Z, Yang D, Dyer C, He X, Smola AJ, and Hovy EH. Hierarchical attention networks for document \nclassification. In Proceedings of the Conference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies (NAACL-HLT 2016), 2016. \n[45] Yin Y, Song Y, Zhang M. Document-level multi-aspect sentiment classification as machine comprehension. \nIn Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2017), 2017.   \n[46] Zhou X, Wan X, Xiao J. Attention-based LSTM network for cross-lingual sentiment classification. In \nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2016), 2016.   \n[47] Li Z, Zhang Y, Wei Y, Wu Y, and Yang Q. End-to-end adversarial memory network for cross-domain \nsentiment classification. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI \n2017), 2017. \n[48] Wiebe J, Bruce R, and O’Hara T. Development and use of a gold standard data set for subjectivity \nclassifications. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL \n1999), 1999.  \n[49] Socher R, Pennington J, Huang E.H, Ng A.Y, and Manning C.D. Semi-supervised recursive autoencoders for \npredicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP 2011), 2011.   \n[50] Socher R, Huval B, Manning C.D, and Ng A.Y. Semantic compositionality through recursive matrix-vector \nspaces. In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP 2012), \n2012. \n[51] Socher R, Perelygin A, Wu J. Y, Chuang J, Manning C.D, Ng A. Y, and Potts C. Recursive deep models for \nsemantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods \non Natural Language Processing (EMNLP 2013), 2013. \n[52] Kalchbrenner N, Grefenstette E, Blunsom P. A convolutional neural network for modelling sentences. In \nProceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2014), 2014. \n[53] Kim Y. Convolutional neural networks for sentence classification. In Proceedings of the Annual Meeting of \nthe Association for Computational Linguistics (ACL 2014), 2014.                                                                                                               \n[54] dos Santos, C. N., Gatti M. Deep convolutional neural networks for sentiment analysis for short texts. In \nProceedings of the International Conference on Computational Linguistics (COLING 2014), 2014. \n[55] Wang X, Liu Y, Sun C, Wang B, and Wang X. Predicting polarities of tweets by composing word embeddings \nwith long short-term memory. In Proceedings of the Annual Meeting of the Association for Computational \nLinguistics (ACL 2015), 2015. \n[56] Graves A, Schmidhuber J. Framewise phoneme classification with bidirectional LSTM and other neural \nnetwork architectures. Neural Networks, 2005. \n[57] Wang J, Yu L-C, Lai R.K., and Zhang X. Dimensional sentiment analysis using a regional CNN-LSTM model.  \nIn Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2016), 2016. \n[58] Wang X, Jiang W, Luo Z. Combination of convolutional and recurrent neural network for sentiment \nanalysis of short texts. In Proceedings of the International Conference on Computational Linguistics (COLING \n2016), 2016. \n[59] Guggilla C, Miller T, Gurevych I. CNN-and LSTM-based claim classification in online user comments. In \nProceedings of the International Conference on Computational Linguistics (COLING 2016), 2016. \n[60] Huang M, Qian Q, Zhu X. Encoding syntactic knowledge in neural networks for sentiment classification. \nACM Transactions on Information Systems, 2017  \n[61] Akhtar MS, Kumar A, Ghosal D, Ekbal A, and Bhattacharyya P. A multilayer perceptron based ensemble \ntechnique for fine-grained financial sentiment analysis. In Proceedings of the Conference on Empirical Methods \non Natural Language Processing (EMNLP 2017), 2017. \n[62] Guan Z, Chen L, Zhao W, Zheng Y, Tan S, and Cai D. Weakly-supervised deep learning for customer review \nsentiment classification. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI \n2016), 2016.  \n[63] Teng Z, Vo D-T, and Zhang Y. Context-sensitive lexicon features for neural sentiment analysis. In \nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2016), 2016.   \n[64] Yu J, Jiang J. Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification. \nIn Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2016), 2016.   \n[65] Zhao Z, Lu H, Cai D, He X, Zhuang Y. Microblog sentiment classification via recurrent random walk network \nlearning. In Proceedings of the Internal Joint Conference on Artificial Intelligence (IJCAI 2017), 2017. \n[66] Mishra A, Dey K, Bhattacharyya P. Learning cognitive features from gaze data for sentiment and sarcasm \nclassification using convolutional neural network. In Proceedings of the Annual Meeting of the Association for \nComputational Linguistics (ACL 2017), 2017. \n[67] Qian Q, Huang M, Lei J, and Zhu X. Linguistically regularized LSTM for sentiment classification. In \nProceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2017), 2017. \n[68] Dong L, Wei F, Tan C, Tang D, Zhou M, and Xu K. Adaptive recursive neural network for target-dependent \nTwitter sentiment classification. In Proceedings of the Annual Meeting of the Association for Computational \nLinguistics (ACL 2014), 2014.  \n[69] Vo D-T, Zhang Y. Target-dependent twitter sentiment classification with rich automatic features. In \nProceedings of the Internal Joint Conference on Artificial Intelligence (IJCAI 2015), 2015.  \n[70] Tang D, Qin B, Feng X, and Liu T. Effective LSTMs for target-dependent sentiment classification. In \nProceedings of the International Conference on Computational Linguistics (COLING 2016), 2016. \n[71] Ruder S, Ghaffari P, Breslin J.G. A hierarchical model of reviews for aspect-based sentiment analysis. In \nProceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP 2016), 2016. \n[72] Zhang M, Zhang Y, Vo D-T. Gated neural networks for targeted sentiment analysis. In Proceedings of AAAI \nConference on Artificial Intelligence (AAAI 2016), 2016.   \n[73] Wang Y, Huang M, Zhu X, and Zhao L. Attention-based LSTM for aspect-level sentiment classification. In \nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2016), 2016.   \n[74] Yang M, Tu W, Wang J, Xu F, and Chen X. Attention-based LSTM for target-dependent sentiment \nclassification.  In Proceedings of AAAI Conference on Artificial Intelligence (AAAI 2017), 2017. \n[75] Liu J, Zhang Y. Attention modeling for targeted sentiment. In Proceedings of the Conference of the \nEuropean Chapter of the Association for Computational Linguistics (EACL 2017), 2017. \n[76] Tang D, Qin B, and Liu T. Aspect-level sentiment classification with deep memory network. arXiv preprint \narXiv:1605.08900, 2016. \n[77] Lei T, Barzilay R, Jaakkola T. Rationalizing neural predictions. In Proceedings of the Conference on Empirical \nMethods on Natural Language Processing (EMNLP 2016), 2016. \n[78] Li C, Guo X, Mei Q. Deep memory networks for attitude Identification. In Proceedings of the ACM \nInternational Conference on Web Search and Data Mining (WSDM 2017), 2017. \n[79] Ma D, Li S, Zhang X, Wang H. Interactive attention networks for aspect-Level sentiment classification. In \nProceedings of the Internal Joint Conference on Artificial Intelligence (IJCAI 2017), 2017.  \n[80] Chen P, Sun Z, Bing L, and Yang W. Recurrent attention network on memory for aspect sentiment analysis. \nIn Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[81] Tay Y, Tuan LA, Hui SC. Dyadic memory networks for aspect-based sentiment analysis. In Proceedings of \nthe International Conference on Information and Knowledge Management (CIKM 2017), 2017. \n[82] Katiyar A, Cardie C. Investigating LSTMs for joint extraction of opinion entities and relations. In \nProceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2016), 2016. \n[83] Wang W, Pan SJ, Dahlmeier D, and Xiao X. Recursive neural conditional random fields for aspect-based \nsentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing \n(EMNLP 2016), 2016.   \n[84] Wang W, Pan SJ, Dahlmeier D, and Xiao X. Coupled multi-Layer attentions for co-extraction of aspect and \nopinion terms. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI 2017), 2017. \n[85] Li X, Lam W. Deep multi-task learning for aspect term extraction with memory Interaction. In Proceedings \nof the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[86] He R, Lee WS, Ng HT, and Dahlmeier D. An unsupervised neural attention model for aspect extraction. In \nProceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2017), 2017. \n[87] Zhang M, Zhang Y, Vo D-T. Neural networks for open domain targeted sentiment. In Proceedings of the \nConference on Empirical Methods in Natural Language Processing (EMNLP 2015), 2015.   \n[88] Zhou X, Wan X, Xiao J. Representation learning for aspect category detection in online reviews. In \nProceeding of AAAI Conference on Artificial Intelligence (AAAI 2015), 2015. \n[89] Yin Y, Wei F, Dong L, Xu K, Zhang M, and Zhou M. Unsupervised word and dependency path embeddings \nfor aspect term extraction. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI \n2016), 2016. \n[90] Xiong S, Zhang Y, Ji D, and Lou Y. Distance metric learning for aspect phrase grouping. In Proceedings of \nthe International Conference on Computational Linguistics (COLING 2016), 2016. \n[91] Poria S, Cambria E, Gelbukh A. Aspect extraction for opinion mining with a deep convolutional neural \nnetwork. Journal of Knowledge-based Systems. 2016.  \n[92] Ying D, Yu J, Jiang J. Recurrent neural networks with auxiliary labels for cross-domain opinion target \nextraction. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI 2017), 2017 \n[93] Irsoy O, Cardie C. Opinion mining with deep recurrent neural networks. In Proceedings of the Conference \non Empirical Methods on Natural Language Processing (EMNLP 2014), 2014. \n[94] Yang B, Cardie C. Extracting opinion expressions with semi-markov conditional random fields. In \nProceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2012), 2012.   \n[95] Liu P, Joty S, Meng H. Fine-grained opinion mining with recurrent neural networks and word embeddings. \nIn Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), 2015. \n[96] Irsoy O, Cardie C. Deep recursive neural networks for compositionality in language. In Proceedings of the \nAnnual Conference on Advances in Neural Information Processing Systems (NIPS 2014), 2014. \n[97] Zhu X, Guo H, Sobhani P. Neural networks for integrating compositional and non-compositional sentiment \nin sentiment composition. In Proceedings of the Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies (NAACL-HLT 2015), 2015. \n[98] Yang B, Cardie C. Joint Inference for fine-grained opinion extraction. In Proceedings of the Annual Meeting \nof the Association for Computational Linguistics (ACL 2013), 2013. \n[99] Deng L, Wiebe J. Recognizing opinion sources based on a new categorization of opinion types. In \nProceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016), 2016. \n[100] Chen C, Wang Z, Lei Y, and Li W. Content-based influence modelling for opinion behaviour Prediction. In \nProceedings of the International Conference on Computational Linguistics (COLING 2016), 2016. \n[101] Rashkin H, Bell E, Choi Y, and Volkova S. Multilingual connotation frames: a case study on social media \nfor targeted sentiment analysis and forecast. In Proceedings of the Annual Meeting of the Association for \nComputational Linguistics (ACL 2017), 2017. \n[102] Mass A. L, Daly R. E, Pham P. T, Huang D, Ng A. Y. and Potts C. Learning word vectors for sentiment \nanalysis. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2011), \n2011.  \n[103] Bespalov D, Bai B, Qi Y, and Shokoufandeh A. Sentiment classification based on supervised latent n-gram \nanalysis. In Proceedings of the International Conference on Information and Knowledge Management (CIKM \n2011), 2011. \n[104] Labutov I, Lipson H. Re-embedding words. In Proceedings of the Annual Meeting of the Association for \nComputational Linguistics (ACL 2013), 2013. \n[105] Tang D, Wei F, Yang N, Zhou M, Liu T, and Qin B. Learning sentiment-specific word embedding for twitter \nsentiment classification. In Proceedings of the Annual Meeting of the Association for Computational Linguistics \n(ACL 2014), 2014. \n[106] Tang D, Wei F, Qin B, Yang N, Liu T, and Zhoug M. Sentiment embeddings with applications to sentiment \nanalysis. IEEE Transactions on Knowledge and Data Engineering, 2016. \n[107] Wang L, Xia R. Sentiment Lexicon construction with representation Learning based on hierarchical \nsentiment Supervision. In Proceedings of the Conference on Empirical Methods on Natural Language \nProcessing (EMNLP 2017), 2017. \n[108] Yu LC, Wang J, Lai KR, and Zhang X. Refining word embeddings for sentiment analysis. In Proceedings of \nthe Conference on Empirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[109] Li J, Jurafsky D. Do multi-sense embeddings improve natural language understanding? In Proceedings of \nthe Conference on Empirical Methods in Natural Language Processing (EMNLP 2015), 2015. \n[110] Ren Y, Zhang Y, Zhang, M and Ji D. Improving Twitter sentiment classification using topic-enriched multi-\nprototype word embeddings. In Proceeding of AAAI Conference on Artificial Intelligence (AAAI 2016), 2016. \n[111] Zhou H, Chen L, Shi F, Huang D. Learning bilingual sentiment word embeddings for cross-language \nsentiment classification. In Proceedings of the Annual Meeting of the Association for Computational Linguistics \n(ACL 2015), 2015. \n[112] Barnes J, Lambert P, Badia T. Exploring distributional representations and machine translation for aspect-\nbased cross-lingual sentiment classification. In Proceedings of the 27th International Conference on \nComputational Linguistics (COLING 2016), 2016. \n[113] Zhang W, Yuan Q, Han J, and Wang J. Collaborative multi-Level embedding learning from reviews for \nrating prediction. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016), \n2016. \n[114] Sharma R, Somani A, Kumar L, and Bhattacharyya P. Sentiment intensity ranking among adjectives using \nsentiment bearing word embeddings. In Proceedings of the Conference on Empirical Methods on Natural \nLanguage Processing (EMNLP 2017), 2017. \n[115] Zhang M, Zhang Y, Fu G. Tweet sarcasm detection using deep neural network. In Proceedings of the \nInternational Conference on Computational Linguistics (COLING 2016), 2016.  \n[116] Joshi A, Tripathi V, Patel K, Bhattacharyya P, and Carman M. Are word embedding-based features useful \nfor sarcasm detection? In Proceedings of the Conference on Empirical Methods on Natural Language \nProcessing (EMNLP 2016), 2016. \n[117] Poria S, Cambria E, Hazarika D, and Vij P. A deeper look into sarcastic tweets using deep convolutional \nneural networks. In Proceedings of the International Conference on Computational Linguistics (COLING 2016), \n2016. \n[118] Peled L, Reichart R. Sarcasm SIGN: Interpreting sarcasm with sentiment based monolingual machine \ntranslation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2017), \n2017. \n[119] Ghosh A, Veale T. Magnets for sarcasm: making sarcasm detection timely, contextual and very personal. \nIn Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[120] Van Hee C, Lefever E, Hoste V. Monday mornings are my fave:)# not exploring the automatic recognition \nof irony in english tweets. In Proceedings of the International Conference on Computational Linguistics (COLING \n2016), 2016. \n[121] Chen WF, Lin FY, Ku LW. WordForce: visualizing controversial words in debates. In Proceedings of the \nConference on Empirical Methods in Natural Language Processing (EMNLP 2016), 2016. \n[122] Wang Z, Zhang Y, Lee S, Li S, and Zhou G. A bilingual attention network for code-switched emotion \nprediction. In Proceedings of the International Conference on Computational Linguistics (COLING 2016), 2016. \n[123] Zhou H, Huang M, Zhang T, Zhu X and Liu B. Emotional chatting machine: emotional conversation \ngeneration with internal and external memory.  arXiv preprint. arXiv:1704.01074, 2017. \n[124] Abdul-Mageed M, Ungar L. EmoNet: fine-grained emotion detection with gated recurrent neural \nnetworks. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2017), \n2017. \n[125] Felbo B, Mislove A, Søgaard A, Rahwan I, and Lehmann S. Using millions of emoji occurrences to learn \nany-domain representations for detecting sentiment, emotion and sarcasm. In Proceedings of the Conference \non Empirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[126] Gui L, Hu J, He Y, Xu R, Lu Q, and Du J. A question answering approach to emotion cause extraction. In \nProceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[127] Poria S, Cambria E, Gelbukh A. Deep convolutional neural text features and multiple kernel learning for \nutterance-level multimodal sentiment analysis. In Proceedings of the Conference on Empirical Methods on \nNatural Language Processing (EMNLP 2015), 2015. \n[128] Bertero D, Siddique FB, Wu CS, Wan Y, Chan R.H, and Fung P. Real-time speech emotion and sentiment \nrecognition for interactive dialogue systems. In Proceedings of the Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP 2016), 2016. \n[129] Fung P, Dey A, Siddique FB, Lin R, Yang Y, Bertero D, Wan Y, Chan RH, and Wu CS. Zara: a virtual \ninteractive dialogue system incorporating emotion, sentiment and personality recognition. In Proceedings of \nthe International Conference on Computational Linguistics (COLING 2016), 2016. \n[130] Wang J, Fu J, Xu Y, and Mei T. Beyond object recognition: visual sentiment analysis with deep coupled \nadjective and noun neural networks. In Proceedings of the Internal Joint Conference on Artificial Intelligence \n(IJCAI 2016), 2016. \n[131] Yang J, Sun M, Sun X. Learning visual sentiment distributions via augmented conditional probability \nneural network. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI 2017), 2017. \n[132] Zhu X, Li L, Zhang W, Rao T, Xu M, Huang Q, and Xu D. Dependency exploitation: a unified CNN-RNN \napproach for visual emotion recognition. In Proceedings of the Internal Joint Conference on Artificial \nIntelligence (IJCAI 2017), 2017. \n[133] You Q, Jin H, Luo J. Visual sentiment analysis by attending on local image regions. In Proceedings of AAAI \nConference on Artificial Intelligence (AAAI 2017), 2017. \n[134] Poria S, Cambria E, Hazarika D, Majumder N, Zadeh A, and Morency LP. Context-dependent sentiment \nanalysis in user-generated videos. In Proceedings of the Annual Meeting of the Association for Computational \nLinguistics (ACL 2017), 2017. \n[135] Tripathi S, Acharya S, Sharma RD, Mittal S, and Bhattacharya S. Using deep and convolutional neural \nnetworks for accurate emotion classification on DEAP dataset. In Proceedings of AAAI Conference on Artificial \nIntelligence (AAAI 2017), 2017. \n[136] Zadeh A, Chen M, Poria S, Cambria E, and Morency LP. Tensor fusion network for multimodal sentiment \nanalysis. In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP \n2017), 2017. \n[137] Long Y, Qin L, Xiang R, Li M, and Huang CR. A cognition based attention model for sentiment analysis. In \nProceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[138] Wang H, Meghawat A, Morency LP, and Xing E.X.  Select-additive learning: improving generalization in \nmultimodal sentiment analysis. In Proceedings of the International Conference on Multimedia and Expo (ICME \n2017), 2017. \n[139] Akhtar MS, Kumar A, Ekbal A, and Bhattacharyya P. A hybrid deep learning architecture for sentiment \nanalysis. In Proceedings of the International Conference on Computational Linguistics (COLING 2016), 2016. \n[140] Dahou A, Xiong S, Zhou J, Haddoud MH, and Duan P. Word embeddings and convolutional neural \nnetwork for Arabic sentiment classification. In Proceedings of the International Conference on Computational \nLinguistics (COLING 2016), 2016. \n[141] Singhal P, Bhattacharyya P. Borrow a little from your rich cousin: using embeddings and polarities of \nenglish words for multilingual sentiment classification. In Proceedings of the International Conference on \nComputational Linguistics (COLING 2016), 2016. \n[142] Joshi A, Prabhu A, Shrivastava M, and Varma V. Towards sub-word level compositions for sentiment \nanalysis of Hindi-English code mixed text. In Proceedings of the International Conference on Computational \nLinguistics (COLING 2016), 2016. \n[143] Gui L, Xu R, He Y, Lu Q, and Wei Z. Intersubjectivity and sentiment: from language to knowledge. In \nProceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2016), 2016. \n[144] Wang Y, Zhang Y, Liu B. Sentiment lexicon expansion based on neural PU Learning, double dictionary \nlookup, and polarity association. In Proceedings of the Conference on Empirical Methods on Natural Language \nProcessing (EMNLP 2017), 2017. \n[145] Rekabsaz N, Lupu M, Baklanov A, Hanbury A, Dür A, and Anderson L. Volatility prediction using financial \ndisclosures sentiments with word embedding-based IR models. In Proceedings of the Annual Meeting of the \nAssociation for Computational Linguistics (ACL 2017), 2017. \n[146] Wang Z, Zhang Y. Opinion recommendation using a neural model. In Proceedings of the Conference on \nEmpirical Methods on Natural Language Processing (EMNLP 2017), 2017. \n[147] Augenstein I, Rocktäschel T, Vlachos A, Bontcheva K. Stance detection with bidirectional conditional \nencoding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \n2016), 2016.   \n[148] Du J, Xu R, He Y, Gui L. Stance classification with target-specific neural attention networks. In Proceedings \nof the Internal Joint Conference on Artificial Intelligence (IJCAI 2017), 2017. \n \n",
  "categories": [
    "cs.CL",
    "cs.IR",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-01-24",
  "updated": "2018-01-30"
}