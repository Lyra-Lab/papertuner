{
  "id": "http://arxiv.org/abs/1801.08360v1",
  "title": "Dual Asymmetric Deep Hashing Learning",
  "authors": [
    "Jinxing Li",
    "Bob Zhang",
    "Guangming Lu",
    "David Zhang"
  ],
  "abstract": "Due to the impressive learning power, deep learning has achieved a remarkable\nperformance in supervised hash function learning. In this paper, we propose a\nnovel asymmetric supervised deep hashing method to preserve the semantic\nstructure among different categories and generate the binary codes\nsimultaneously. Specifically, two asymmetric deep networks are constructed to\nreveal the similarity between each pair of images according to their semantic\nlabels. The deep hash functions are then learned through two networks by\nminimizing the gap between the learned features and discrete codes.\nFurthermore, since the binary codes in the Hamming space also should keep the\nsemantic affinity existing in the original space, another asymmetric pairwise\nloss is introduced to capture the similarity between the binary codes and\nreal-value features. This asymmetric loss not only improves the retrieval\nperformance, but also contributes to a quick convergence at the training phase.\nBy taking advantage of the two-stream deep structures and two types of\nasymmetric pairwise functions, an alternating algorithm is designed to optimize\nthe deep features and high-quality binary codes efficiently. Experimental\nresults on three real-world datasets substantiate the effectiveness and\nsuperiority of our approach as compared with state-of-the-art.",
  "text": "JOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n1\nDual Asymmetric Deep Hashing Learning\nJinxing Li, Bob Zhang, Member, IEEE, Guangming Lu, David Zhang*, Fellow, IEEE\nAbstract—Due to the impressive learning power, deep learning\nhas achieved a remarkable performance in supervised hash\nfunction learning. In this paper, we propose a novel asymmetric\nsupervised deep hashing method to preserve the semantic struc-\nture among different categories and generate the binary codes\nsimultaneously. Speciﬁcally, two asymmetric deep networks are\nconstructed to reveal the similarity between each pair of images\naccording to their semantic labels. The deep hash functions\nare then learned through two networks by minimizing the gap\nbetween the learned features and discrete codes. Furthermore,\nsince the binary codes in the Hamming space also should keep\nthe semantic afﬁnity existing in the original space, another\nasymmetric pairwise loss is introduced to capture the simi-\nlarity between the binary codes and real-value features. This\nasymmetric loss not only improves the retrieval performance,\nbut also contributes to a quick convergence at the training\nphase. By taking advantage of the two-stream deep structures\nand two types of asymmetric pairwise functions, an alternating\nalgorithm is designed to optimize the deep features and high-\nquality binary codes efﬁciently. Experimental results on three\nreal-world datasets substantiate the effectiveness and superiority\nof our approach as compared with state-of-the-art.\nIndex Terms—deep learning, hashing learning, image retrieval,\nsimilarity\nI. INTRODUCTION\nW\nITH the rapid growth of multimedia data in search\nengines and social networks, how to store these data\nand make a fast search when an novel one such as the image\nis given, plays a fundamental role in machine learning. Due\nto the low storage cost and fast retrieval speed, hashing tech-\nniques have attracted much attention and are widely applied\nin nearest neighbor search [1] for information retrieval on\nlarge scale datasets. Hashing learning aims to project the data\nfrom the original space into a Hamming space by generating\ncompact codes. These codes can not only dramatically reduce\nthe storage overhead and achieve a constant or sub-linear\ntime complexity in information search, but also preserve the\nsemantic afﬁnity existing in the original space.\nMany hashing methods have been studied [2] [3] [4] [5] [6]\n[7] [8] [9] [10]. Generally, these approaches can be roughly\nclassiﬁed into two categories: data-independent and data-\ndependent hashing methods. Locality Sensitive Hashing (LSH)\n[2] and its extension Kernelized LSH (KLSH) [3], as the most\ntypical data-independent hashing methods, were proposed to\nJ. Li is with the Department of Computing, Hong Kong Polytechnic\nUniversity, Hung Hom, Kowloon (e-mail: csjxli@comp.polyu.edu.hk).\nB. Zhang is with the Department of Computer and Information Sci-\nence, University of Macau, Avenida da Universidade, Taipa, Macau (e-mail:\nbobzhang@umac.mo).\nG. Lu is with Department of Computer Science, Harbin Institute\nof\nTechnology\nShenzhen\nGraduate\nSchool,\nShenzhen,\nChina\n(e-mail:\nluguangm@hit.edu.cn).\nD. Zhang is with the Department of Computing, Hong Kong Polytechnic\nUniversity, Hung Hom, Kowloon (e-mail: csdzhang@comp.polyu.edu.hk).\nobtain the hashing function by using random projections.\nAlthough the designation of these data-independent methods is\nquite simple, they often meet a performance degradation when\nthe length of the binary codes is relatively low. By contrary,\ninstead of randomly generating the hashing function like LSH\ndoes, data-dependent methods aims to learn a data-speciﬁc\nhashing function by using the training data, being capable of\ngenerating shorter binary codes but achieving more remark-\nable performance. Therefore, various data-dependent hashing\napproaches containing both unsupervised and supervised have\nbeen proposed. Unsupervised hashing, e.g. Spectral Hashing\n[6], Anchor Graph Hashing (AGH) [7], and Discrete Graph\nHashing (DGH) [8] etc., only try to utilize the data structure\nto learn compact binary codes to improve the performance. By\ntaking the label information into account, supervised hashing\nmethods attempt to map the original data into a compact\nHamming space to preserve the similarity between each pair\nsamples. Many representative works including Fast Supervised\nHashing (FastH) [5], Kernel Supervised Hashing (KSH) [9],\nand Supervised Discrete Hashing (SDH) [10] etc., demonstrate\nthat supervised hashing methods often obtain an outstanding\nperformance compared with unsupervised hashing methods.\nThus, we focus on studying the supervised hashing method in\nthis paper.\nAlthough some traditional supervised hashing methods\nachieve a good performance in some applications, most of\nthem only linearly map the original data into a Hamming space\nby using the hand-crafted features, limited their application\nfor large-scale datasets which have complex distributions.\nFortunately, due to the powerful capability of data repre-\nsentation, deep learning [11] [12] provides a promising way\nto jointly represent the data and learn hash codes. Some\nexisting deep learning based hashing methods have been\nstudied, such as Deep Supervised Hashing (DSH) [13] and\nDeep Pairwise Supervised Hashing (DPSH) [14], etc. These\napproaches demonstrate the effectiveness of the end-to-end\ndeep learning architecture for hashing learning.\nDespite the wide applications of deep neural network on\nhashing learning, most of them are symmetric structures in\nwhich the similarity between each pair points are estimated\nby the Hamming distance between the outputs of the same\nhash function [15]. As described in [15], a crucial problem is\nthat this symmetric scheme would result in the difﬁculty of op-\ntimizing the discrete constraint. Thus in this paper, we propose\na novel asymmetric hashing method to address aforementioned\nproblem. Note that a similar work was described by Shen et\nal. [15], named deep asymmetric pairwise hashing (DAPH).\nHowever, our study is quite distinctive from DAPH. Shen et\nal. tried to approximate the similarity afﬁnity by exploiting\ntwo different hashing functions, which can preserve more\nsimilarity information among the real-value features. However,\narXiv:1801.08360v1  [cs.CV]  25 Jan 2018\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n2\nDAPH only exploits a simple Euclidean distance, but ignores\nthe semantic structure between the learned real-value features\nand binary codes [16] [17]. One major deﬁciency is that it\nis difﬁcult to efﬁciently preserve the similarity in the learned\nhash functions and discrete codes. Furthermore, in DAPH, two\ndifferent types of discrete hash codes corresponding to two\nhash functions are estimated in the training time. However\nthis strategy would enlarge the gap between two schemes,\nresulting in a performance degradation. By contrast, we not\nonly propose a novel asymmetric structure to learn two dif-\nferent hash functions and one consistent binary code for each\nsample at the training phase, but also asymmetrically exploit\nreal-value and multiple integer values, which permits the better\npreservation of similarity between the learned features and\nhash codes. Experiments show that this novel asymmetric\nstructure can get a better performance in image retrieval and\nquicker convergence at the training stage.\nThe main contributions of the proposed method are shown\nas follows:\n(1) A novel asymmetric deep structure are proposed. Two\nstreams of deep neural networks are trained to asymmetrically\nlearn two different hash functions. The similarity between each\npair images are utilized through a pairwise loss according to\ntheir semantic/label information.\n(2) The similarity between the learned features and binary\ncodes are also revealed through an additional asymmetric loss.\nReal-value features and binary codes are bridged through an\ninner product, which alleviates the binary limitation, better\npreserves the similarity, and speeds up convergence at the\ntraining phase.\n(3) By taking advantage of these two asymmetric properties,\nan alternative algorithm is designed to efﬁciently optimize the\nreal values and discrete values.\n(4) Experimental results on three large-scale datasets sub-\nstantiate the effectiveness and superiority of our approach as\ncompared with some existing state-of-the-art hashing methods\nin image retrieval.\nThe rest of this paper is organized as follows. In Section\n2, the related works including data-independent and data-\ndependent hashing methods are brieﬂy reviewed. In Section\n3, the proposed Dual Asymmetric Deep Hashing Learning\n(DADH) is then analyzed, followed by its optimization. In\nSection 4, experiments are conducted on three real-world\ndatasets, and some comparisons, parameter sensitivity analysis\nand convergence analysis are discussed. This paper is ﬁnally\nconcluded in Section 5.\nII. RELATED WORKS\nAs mentioned before, the hashing method can be roughly\nseparated into data-independent and data-dependent hashing.\nLocality Sensitive Hashing (LSH) [2] aims to use several\nhash functions to randomly project the data into a Hamming\nspace, so as to ensure the probability of collision is much\nhigher for data points which are close to each other than for\nthose which are far apart. Consider the non-linearity existing in\nmany real-world datasets, LSH was generated to accommodate\narbitrary kernel functions (KLSH) in [3]. Some other priors,\nsuch as p-stable distributions [18] and shift-invariant kernels\n[19], are also embedded to extend LSH for performance\nimprovement.\nDifferent\nfrom\ndata-independent\nmethods,\nthe\ndata-\ndependent methods try to learn more compact codes from\na given dataset to achieve a satisfactory search accuracy.\nAccording to whether the label information is available, data-\ndependent hashing can also be classiﬁed into unsupervised and\nsupervised. Typical learning criteria for unsupervised hashing\nmethods contains graph learning [6] [7] [8] [20] and error\nminimization [4] [21] [22]. Graph Learning: Yair et al. [6]\nproved that ﬁnding a best code is associated with the problem\nof graph partitioning. Thus, a spectral hashing (SH) was pro-\nposed to learn the hash function. Another graph hashing named\nAnchor Graph Hashing (AGH) was presented by Liu et al.\n[7], which is capable of capturing the neighborhood structure\ninherent automatically. In order to avoid the high complexity\nof existing graph hashing methods, Jiang et al. [20] proposed\na scalable graph hashing (SGH) which can be effectively\napplied to the large-scale dataset search. Although SH, AGH\nand SGH achieve a satisfactory performance in some datasets,\nboth of them relax the optimization by discarding the discrete\nconstraints, which results in an accumulated quantization error.\nTo address this problem, a discrete graph hashing (DGH)\n[8] was proposed, which can ﬁnd the neighborhood structure\ninherent in a discrete code space. Error Minimization: A\ntypical method is the iterative quantization (ITQ) [4] which\naims to project the data to the vertices of a binary hypercube\nand minimize the quantization error. Additionally, the method\nin [21] makes a quantization by decomposing the input space\ninto a Cartesian product of low-dimensional subspaces, dra-\nmatically reducing the quantization noise. Different from most\nsingle-bit quantization, a double-bit quantization hashing [23]\nwas also studied by quantizing each dimension into double\nbits.\nIn contrast to unsupervised hashing methods, supervised\nhashing learning utilizes the label information to encourage the\nbinary codes in the Hamming space to preserve the semantic\nrelationship existing in the raw data. For instance, Mohammad\net al. [24] introduced a hinge-like loss function to exploit the\nsemantic information. Besides, Li et al. [25] projected the\nraw data into a latent subspace, and the label information is\nembedded on this subspace to preserve the semantic structure.\nThe Jensen Shannon Divergence is also utilized in [26] to\nlearn the binary codes within a probabilistic framework, in\nwhich an upper bound is derived for various hash functions.\nBeing similar to KLSH, Liu et al. [9] proposed a supervised\nhashing with kernels, in which the similar pairs are minimized\nwhile the dissimilar pairs are maximized. Consider the discrete\nconstraint, the supervised discrete hashing (SDH) [10] was\nproposed to not only preserve the semantic structure, but\nalso discretely learn the hash codes without any relaxation.\nHowever, this discrete optimization is time-consuming and\nunscalable. To tackle this problem, a novel method named\ncolumn sample based discrete supervised hashing (COSDISH)\nwas presented to directly obtain the binary codes from seman-\ntic information.\nAlthough various works mentioned above have been stud-\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n3\nied, they only project the data into the Hamming space by\nusing the hand-crafted features. The main limitation is that\nthey would meet a performance degradation if the distribution\nof a real-world dataset is complex. Fortunately, deep learning\nprovides a reasonable and promising solution. Liong et al.\n[27] used the deep structure to hierarchically and non-linearly\nlearn the hash codes. Convolutional neural network (CNN)\nwas ﬁrst applied by Xia and Yan et al. to the hashing learning\n(CNNH) [28], which simultaneously represents the image and\nlearns a hash function. A novel deep structure [29] was then\nproposed by modifying the fully-connected layer in CNNH\nto a divide-and-encode module, in which the hash codes can\nbe obtained bit by bit. Also, Can et al. [30] combined the\nquantization model with deep structure to gain a satisfactory\nperformance in image retrieval. Different from the triple loss\nused in some deep hashing methods, Li et al. [14] studied\na pairwise loss (DPSH) which can effectively preserve the\nsemantic information between each pair outputs. Due to the\npower of asymmetric structure, the asymmetric deep hashing\nwas also studied in recent years. For instance, Shen et al.\n[15] (DAPH) tried to learn hash functions in an asymmet-\nric network. However, DAPH only exploits two streams to\npreserve the pairwise label information between the deep\nneural network outputs, but ignores the similarity between\nthe real-value features and binary codes. Thus, in this paper,\nwe propose a novel deep hashing method to not only exploit\nthe label information between each two outputs through an\nasymmetric deep structure, but also semantically associated\nthe learned real-value features with the binary codes.\nIII. THE PROPOSED METHOD\nIn this section, we ﬁrst give some notations used in this\npaper, as well as the problem deﬁnition. The proposed Dual\nAsymmetric Deep Hashing Learning (DADH) is then de-\nscribed, followed by its optimization.\nA. Notation and Problem Deﬁnition\nIn\nthis\npaper,\nsince\nthere\nare\ntwo\nstreams\nin\nthe\nproposed\nmethod,\nwe\nuse\nthe\nuppercase\nletters\nX\n=\n{x1, · · · , xi, . . . , xN}\n∈\nRN×d1×d2×3\nand\nY\n=\n{y1, · · · , yi, . . . , yN}\n∈\nRN×d1×d2×3 to denote\nthe input images in the ﬁrst and second deep neural networks,\nrespectively, where N is the number of training samples,\nd1 and d2 are the length and width for each image. Note\nthat, although X and Y are represented with different\nsymbols, both of them denote the same training data. In our\nexperiments, we only alternatively use training samples X\nand Y in the ﬁrst and second networks. Since our method\nis supervised learning, the label information can be used.\nLet the uppercase letter S ∈{−1, +1} denote the similarity\nbetween X and Y and Si,j is the element in the i-th row and\nj-th column in S. Let Si,j = 1 if xi and yj share the same\nsemantic information or label, otherwise Si,j = −1.\nDenote the binary codes as B = [b1, · · · , bi, . . . , bN]T ∈\nRN×k and the k-bit binary code of the i-th sample as\nbi ∈{−1, +1}k×1. The purpose of our model is to learn\ntwo mapping functions F and G to project X and Y into the\nTABLE I\nTHE NETWORK STRUCTURE OF CNN-F.\nLayer\nStructure\nconv1\nf. 64 × 11 × 11; st. 4×4; pad. 0; LRN.; ×2 pool\nconv2\nf. 265 × 5 × 5; st. 1×1; pad. 2; LRN.; ×2 pool\nconv3\nf. 265 × 3 × 3; st. 1×1; pad. 1\nconv4\nf. 265 × 3 × 3; st. 1×1; pad. 1\nconv5\nf. 265 × 3 × 3; st. 1×1; pad. 1; ×2 pool\nfull6\n4096\nfull7\n4096\nfull8\n→k-bit hash code\nHamming space B. bi = sign(F(xi)) and bj = sign(G(yj)),\nwhere sign(·) is an element-wise sign function, and sign(x) =\n1 if x ≥0, otherwise sign(x) = -1. The Hamming distance\ndistH(bi, bj) between bi and bj should be as small as\npossible if sij = 1 and vice versa. Due to the power of\ndeep neural network in data representation, we apply the\nconvolution neural work to learn the hash functions. Speciﬁ-\ncally, the CNN-F structure [31] is adopted to perform feature\nlearning. In CNN-F model, there are eight layers including ﬁve\nconvolutional layers as well as three fully-connected layers.\nThe network structure is listed in Table I, where ”f.” means\nthe ﬁlter, ”st.” means the convolution stride, ”LRN” means the\nLocal Response Normalization [11]. In order to get the ﬁnal\nbinary code, we replace the last layer in CNN-F with a k-D\nvector and the k-bit binary codes are obtained through a sign\noperation on the output of the last layer. In this paper, CNN-F\nmodel is applied to both streams in our proposed asymmetric\nstructure.\nB. Dual Asymmetric Deep Hashing Learning\nThe main framework of the proposed method is shown\nin Fig.1. As we can see, there are two end-to-end neural\nnetworks to discriminatively represent the inputs. For a pair\nof outputs F and G in these two streams, their semantic\ninformation is exploit through a pairwise loss according to\ntheir predeﬁned similarity matrix. Since the purpose is to\nobtain hash functions through the deep networks, the binary\ncode B is also generated by minimizing its distance between\nF and G. Furthermore, in order to preserve the similarity\nbetween the learned binary codes and real-value features, and\nalleviate the binary limitation, another asymmetric pairwise\nloss is introduced by using the inner product of the hash codes\nB and learned features F (G).\nDenote f(xi, Wf) ∈Rk×1 as the output of the i-th sample\nin the last layer of the ﬁrst stream, where Wf is the parameter\nof the network. To simplify the notation, we use fi to replace\nf(xi, Wf). Similarly, we can obtain the output gj correspond-\ning to the j-th sample under the parameter Wg in the second\nnetwork. Thus, the features F = [f1, · · · , fi, fn]T ∈Rn×k and\nG = [g1, · · · , gi, gn]T ∈Rn×k corresponding to the ﬁrst and\nsecond networks are then gained.\nTo learn an accurate binary code, we set sign(fi) and\nsign(gi) to be close to their corresponding hash code bi. A\ngeneral way is to minimize the L2 loss between them.\nmin ∥sign(fi) −bi∥2\n2 + ∥sign(gi) −bi∥2\n2 ,\ns.t. bi ∈{−1, +1}\n(1)\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n4\n. . .\n1, 0,\n, 0,1\n0,1,\n, 0, 0\n1, 0,\n,1,1\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n. . .\n. . .\n. . .\nU\n. . .\n. . .\n. . .\nV\nS\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nB\nHash Code\nReal-valued \nData and \nBinary Codes \nSimilarity \nPreservation\nReal-valued \nData Similarity \nPreservation\nFeature Extraction with \ndifferent Weights\nFig. 1. The framework of the proposed method. Two streams with ﬁve convolution layers and two full-connected layers are used for feature extraction. For\nthe real-valued outputs from these two neural networks, their similarity is preserved by using a pairwise loss. Based on the outputs, a consistent hash code is\ngenerated. Furthermore, an asymmetric loss is introduced to exploit the semantic information between the binary code and real-valued data.\nHowever, it is difﬁcult to make a back-propagation for the\ngradient with respect to fi or gi in Eq.(1) since their gradients\nare zero anywhere. In this paper, we apply tanh(·) to softly\napproximate the sign(·) function. Thus, Eq.(1) is transformed\ninto\nmin ∥tanh(fi) −bi∥2\n2 + ∥tanh(gi) −bi∥2\n2 ,\ns.t. bi ∈{−1, +1}\n(2)\nFurthermore, to exploit the label information and keep\na consistent similarity between two outputs F and G, the\nnegative log likelihood of the dual-stream similarities with the\nlikelihood function is exploited.\np(Sij|tanh(fi), tanh(gj)) =\n\u001a\nσ(Θij)\nSij = 1\n1 −σ(Θij)\nSij = 0\n(3)\nwhere Θij =\n1\n2tanh(f T\ni )tanh(gj), and σ(Θij) =\n1\n1+e−Θij .\nTherefore, the pairwise loss for these two different outputs\nis shown as follows.\nmin −\nn\nX\ni,j=1\n(SijΘij −log(1 + eΘij)\n(4)\nAlthough Eq.(2) achieves to approximate discrete codes\nand Eq.(4) exploits the intra- and inter-class information, the\nsimilarity between the binary codes and real-value features is\nignored. To tackle this problem, another asymmetric pairwise\nloss is introduced.\nmin\n\r\rtanh(f T\ni )bj −kSij\n\r\r2\n2 +\n\r\rtanh(gT\ni )bj −kSij\n\r\r2\n2 ,\ns.t. bi ∈{−1, +1}\n(5)\nIn Eq.(5), the similarity between the real-valued data and\nbinary codes is measured by their inner product. It is easy to\nobserve that Eq.(5) not only encourages the tanh(fi) (tanh(gi))\nand bi to be consistent, but also preserve the similarity\nbetween them. Additionally, our experiments in Section 4 also\nprove that this kind of asymmetric inner product can quickly\nmake the network converge to a stable value for the real-valued\nfeatures and hash codes.\nJointly taking Eq.(2), Eq.(4) and Eq.(5) into account, the\nobjective function can be obtained as follows:\nmin\nF,G,B L =\n\r\rtanh(F)BT −kS\n\r\r2\nF +\n\r\rtanh(G)BT −kS\n\r\r2\nF\n−τ\nn\nX\ni,j=1\n(SijΘij −log(1 + eΘij)+\n+ γ(∥tanh(F) −B∥2\nF + ∥tanh(G) −B∥2\nF )\n+ η(\n\r\rtanh(F)T 1\n\r\r2\nF +\n\r\rtanh(G)T 1\n\r\r2\nF )\ns.t. bi ∈{−1, +1}\n(6)\nwhere τ, γ and η are the non-negative parameters to make a\ntrade-off among various terms. Note that the purpose of the\nforth term\n\r\rtanh(F)T 1\n\r\r2\nF +\n\r\rtanh(G)T 1\n\r\r2\nF in the objective\nfunction Eq.(6) is to maximize the information provided by\neach bit [32]. In detail, this term makes a balance for each bit,\nwhich encourages the number of -1 and +1 to be approximately\nsimilar among all training samples.\nC. Optimization\nFrom the objective function Eq.(6), we can see that the\nreal-valued features as well as the weights in two neural\nnetworks (F, Wf) / (G, Wg), and discrete codes B need\nto be optimized. Note that this NP-hard problem is highly\nnon-convex, and it is very difﬁcult to directly get the optimal\nsolutions. In this paper, we design an efﬁcient algorithm to\noptimize them alternatively. Specially, we update one variable\nby ﬁxing other variables.\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n5\n1) Update (F, Wf) with (G, Wg) and B ﬁxed: By ﬁxing\n(G, Wg) and B, the objective function Eq.(6) can be trans-\nformed to\nmin\nF\n\r\rtanh(F)BT −kS\n\r\r2\nF −τ\nn\nX\ni,j=1\n(SijΘij −log(1 + eΘij)+\n+ γ(∥tanh(F) −B∥2\nF + η\n\r\rtanh(F)T 1\n\r\r2\nF\n(7)\nThen the back-propagation is exploited to update (F, Wf).\nHere denote U = tanh(F) and V = tanh(G). The gradient of\nthe objective function with respect to fi is\n∂L\n∂fi\n= {\nn\nX\nj=1\n[2bj(bT\nj ui −kSij) + τ\n2(σ(Θij)vj −Sijvj)]\n+ 2γ(ui −bi) + 2ηUT 1} ⊙(1 −u2\ni )\n(8)\nwhere ⊙denotes the dot product. After getting the gradient\n∂L\n∂fi , the chain rule is used to obtain\n∂L\n∂Wf , and Wf is updated\nby using back-propagation.\n2) Update (G, Wg) with (F, Wf) and B ﬁxed: Similarly,\nby ﬁxing (F, Wf) and B, the back-propagation is exploited to\nupdate (G, Wg). The gradient of the objective function with\nrespect to gi is\n∂L\n∂gi\n= {\nn\nX\nj=1\n[2bj(bT\nj vi −kSij) + τ\n2(σ(Θij)uj −Sijuj)]\n+ 2γ(vi −bi) + 2ηVT 1} ⊙(1 −v2\ni )\n(9)\nAfter getting the gradient ∂L\n∂gi , the chain rule is used to obtain\n∂L\n∂Wg , and Wg is updated by using back-propagation.\n3) Update B with (F, Wf) and (G, Wg) ﬁxed: By ﬁxing\n(F, Wf) and (G, Wg), we can get the following formulation.\nmin\nB L(B) =\n\r\rUBT −kS\n\r\r2\nF +\n\r\rVBT −kS\n\r\r2\nF\n+ γ(∥U −B∥2\nF + ∥V −B∥2\nF ) s.t. bi ∈{−1, +1}\n(10)\nThen Eq.(10) can be rewrote as:\nmin\nB L(B) = −2Tr[B(k(UT S + VT S) + γ(UT + VT ))]\n+\n\r\rBUT \r\r2\nF +\n\r\rBVT \r\r2\nF + const s.t. bi ∈{−1, +1}\n(11)\nwhere ’const’ means a constant value without any association\nwith B. For the sake of simplicity, let Q = −2k(ST U +\nST V) −2γ(U + V). Eq.(11) can be simpliﬁed to\nmin\nB L(B) =\n\r\rBUT \r\r2\nF +\n\r\rBVT \r\r2\nF + Tr[BQT ] + const\ns.t. bi ∈{−1, +1}\n(12)\nAccording to Eq.(12) and [17], B can be updated bit by bit.\nIn other words, we update one column in B with remaining\ncolumns ﬁxed. Let B∗c be the c-th column and ˆBc be the\nremaining columns in B. So do U∗c, ˆUc, V∗c, ˆVc, Q∗c, and\nˆQc. Eq.(12) can then be rewrote as:\nmin\nB∗c Tr(B∗c[2(UT\n∗c ˆUc + VT\n∗c ˆVc)ˆBT\nc + QT\n∗c] + const\ns.t. B ∈{−1, +1}n×k\n(13)\nAlgorithm 1 Dual Asymmetric Deep Hashing Learning\n(DADH)\nInput: Training data X/Y; similarity matrix S; hash code\nlength k; predeﬁned parameters τ, γ and η.\nOutput: Hashing functions F and G.\nInitialization: Initialize weights of the ﬁrst seven layers by\nusing the pretrained ImageNet model; the last layer is\ninitialized randomly; B is set to be a matrix whose\nelements are zero.\n1: while not converged or not reach the maximum iteration\ndo\n2:\nUpdate (F, Wf):\nFix (G, Wg) and B and update (F, Wf) using back-\npropagation according to Eq.(8).\n3:\nUpdate (G, Wg):\nFix (F, Wf) and B and update (G, Wg) using back-\npropagation according to Eq.(9).\n4:\nUpdate B:\nFix (F, Wf) and (G, Wg) and update B according to\nEq.(14).\n5: end while\nObviously, the optimal solution for B∗c is\nB∗c = −sign(2ˆBc( ˆUT\nc U∗c + ˆVT\nc V∗c) + Q∗c)\n(14)\nAfter computing B∗c, we update B by replace the c-th column\nwith B∗c. Then we repeat Eq.(14) until all columns are\nupdated.\nOverall, the optimization of the proposed method is listed\nin Algorithm 1.\nD. Query\nWhen Wf and Wg are learned, the hash functions cor-\nresponding to the two neural networks are subsequently ob-\ntained. For a given testing image x∗, two kinds of binary\ncodes can be computed, which are b∗\nf = sign(f(x∗, Wf)) and\nb∗\ng = sign(f(x∗, Wg)), respectively. Note that since tanh will\nnot inﬂuence the sign of each element at the testing phase,\nwe do not apply tanh for the output. From the experiments\nwe ﬁnd that the performances computed through the ﬁrst and\nsecond networks are quite similar. To obtain a more robust\nresult, we use the average of two outputs as the ﬁnal result in\nour experiment.\nb∗= sign(0.5[f(x∗, Wf) + g(x∗, Wg)])\n(15)\nIV. EXPERIMENTS\nIn this section, experiments are conducted on three large-\nscale datasets to demonstrate the effectiveness of the proposed\nmethod compared with some state-of-the-art approaches. We\nﬁrst describe the datasets used in our experiments, followed\nby the description of baselines, evaluation protocol, and imple-\nmentation. We then make a comparison with other methods.\nThe parameter sensitivity as well as the convergence are\nsubsequently discussed.\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n6\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 2. The Precision-Recall curves computed by LSH, ITQ, DPLM, SDH, SGH, DPSH, ADSH, DAPH, and DADH on the IAPR TC-12 dataset. Figures\nfrom (a) to (f) are associated with the code length 8-bit, 12-bit, 16-bit, 24-bit, 36-bit and 48-bit.\nTABLE II\nTHE MAP SCORES OBTAINED BY DIFFERENT METHODS ON THE IAPR\nTC-12 DATASET.\nMethod\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\nLSH\n32.89\n33.39\n33.25\n34.33\n34.99\n35.15\nITQ\n35.83\n36.00\n36.21\n36.55\n36.66\n36.75\nDPLM\n36.38\n36.86\n37.35\n37.71\n38.64\n38.83\nSDH\n36.92\n37.81\n37.45\n38.12\n38.53\n38.27\nSGH\n34.48\n35.01\n35.32\n35.49\n35.84\n36.21\nDPSH\n45.19\n46.03\n46.82\n47.37\n47.97\n48.60\nADSH\n44.69\n46.98\n48.25\n49.06\n50.24\n50.59\nDAPH\n44.33\n44.48\n44.73\n45.12\n45.24\n45.52\nDADH\n46.54\n49.27\n50.83\n52.71\n54.47\n55.39\nA. Datasets\nThree datasets including IAPR TC-12 [33], MIRFLICKR-\n25K [34] and CIFAR-10 [35] are used in this paper.\nIAPR TC-12 [33] dataset consists of 20000 images associ-\nated with 255 categories. Since some samples have multiple\nlabels, we set Sij = 1 only if there is at least one same label\nfor the i-th and j-th sample. In this dataset, 2000 images are\nused for testing and 5000 samples selected from the remaining\n18000 (retrieval set) points are used for training to greatly\nreduce the training time.\nMIRFLICKR-25K dataset [34] is composed of 25000 im-\nages collected from the Flickr website. According to [32],\n20015 images associated with 24 categories are selected. Being\nsimilar to IAPR TC-12 dataset, some images has multiple\nlabels, we also deﬁne two images be a ground-truth neighbor if\nthey share at least one same label. Additionally, 2000 images\nare randomly selected as the testing data, and the rest is deﬁned\nas the retrieval data. Meanwhile, 5000 samples selected from\nthe retrieval data are used for training.\nCIFAR-10 dataset [35] contains 60000 32×32 color images\nwith ten categories. Each image belongs to one of these ten\nclasses. Two images will be regarded as semantic neighbor\nif they have the same label. Being similar to the setting in\n[17], we randomly select 1000 samples to be the testing data.\nIn order to reduce the training time, we also randomly select\n5000 images from the remaining 59000 images as the training\ndata. The rest is then regarded as the retrieval set.\nB. Baseline and Evaluation Protocol\nTo demonstrate the superiority of DADH, some existing\nhashing methods are used for comparison, including one\ndata-independent methods (LSH [2]), four traditional data-\ndependent methods (ITQ [4], DPLM [36], SDH [10], SGH\n[20]) and three deep learning based hashing methods (DPSH\n[14], ADSH [17], DAPH [15]). Since LSH, ITQ, DPLM,\nSDH, and SGH are not deep learning methods, features\nshould be extracted previously. For these three datasets, we\nhave extracted the 4096-D CNN feature 512-D GIST feature,\nrespectively. We have found that these ﬁve approaches often\nachieve a better performance on the GIST feature. Thus we use\nthe GIST feature as the input for LSH, ITQ, DPLM, SDH, and\nSGH. For DPSH, ADSH, and DAPH, the raw image is used\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n7\nTABLE III\nTHE TOP-500 MAP AND TOP-500 PRECISION SCORES OBTAINED BY DIFFERENT METHODS ON THE IAPR TC-12 DATASET.\nEvaluation\nMAP@Top500\nPrecision@Top500\nMethod\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\nLSH\n36.19\n37.47\n37.53\n39.83\n40.50\n41.52\n35.04\n36.10\n36.12\n38.10\n38.82\n39.55\nITQ\n41.16\n41.91\n42.76\n43.67\n44.07\n44.30\n39.73\n40.30\n40.88\n41.60\n41.86\n42.04\nDPLM\n42.21\n43.12\n43.89\n44.50\n45.86\n46.42\n40.84\n41.65\n42.33\n42.82\n44.02\n44.44\nSDH\n44.08\n45.31\n46.00\n47.30\n48.17\n48.06\n42.45\n43.54\n43.94\n45.00\n45.79\n45.49\nSGH\n40.04\n41.54\n42.09\n42.48\n43.07\n43.97\n38.67\n39.70\n40.08\n40.33\n40.79\n41.45\nDPSH\n57.07\n58.13\n59.94\n61.61\n63.05\n64.49\n55.03\n55.90\n57.73\n58.96\n60.15\n61.40\nADSH\n53.70\n58.12\n61.35\n63.25\n64.90\n65.59\n52.31\n56.47\n58.97\n60.29\n61.94\n62.50\nDAPH\n56.26\n57.98\n59.48\n61.27\n62.57\n63.94\n54.32\n55.44\n56.52\n57.92\n58.95\n60.03\nDADH\n57.12\n62.67\n65.15\n67.80\n70.11\n70.93\n55.31\n60.20\n62.52\n64.93\n67.13\n67.81\nas the input and all images are resized into 224×224×3. For\nall deep learning methods, the CNN-F is used as the network\nfor feature extraction and the parameters in DPSH and ADSH\nare set according to their descriptions in their publications.\nNote that, since the code for DAPH is not released, we\nimplement it with the deep learning toolbox MatConvNet [37]\nvery carefully. Additionally, the original network structure in\nDAPH is not CNN-F which means the parameters in [15] may\nbe not optimal. Thus, we try our best to tune the parameters\nin DAPH.\nTo quantatively measure the proposed method and other\ncomparison methods, two widely used metrics containing\nmean average precision (MAP) and precision-recall (PR) are\nadopted. The deﬁnitions of MAP criteria is demonstrated as\nfollows: Given a query, the average precision (AP) is ﬁrst\ncomputed by searching a set of R retrieved results.\nAP = 1\nT\nR\nX\nr=1\nP(r)δ(r)\n(16)\nwhere T is the total number of document set in retrieved set,\nP(r) is the precision of top r retrieved cases, and δ(r) denotes\nwhether the retrieved sample is relevant (if the instance is a\ntrue neighbor of the query, δ(r) = 1, otherwise δ(r) = 0).\nAdditionally, being similar to some existing methods [15] [32],\nTop-500 MAP and Top-500 Precision are also exploited to\nevaluate the superiority of the proposed method.\nC. Implementation\nWe implement DADH with the deep learning toolbox Mat-\nConvNet [37] on Titan X GPU. The pre-trained ImageNet\nmodel is used to initialize the ﬁrst seven layers in each stream\nand the weights in the last layer are initialized randomly.\nDuring the training time, we set the mini-batch size to be\n64 and divide the learning rate among [10−6, 10−4] into 150\niterations. In other words, the learning rate gradually reduces\nfrom 10−4 to 10−6 and the stochastic gradient descent is used\nto update the weights. Based on the cross-validation (a small\nset for validation is randomly selected from the training data),\nwe set γ = 100, η = 10, and τ = 10 in the three datasets. We\nwill further demonstrate the insensitivity of these parameters\nin the following subsection.\nTABLE IV\nTHE MAP SCORES OBTAINED BY DIFFERENT METHODS ON THE\nMIRFLICKR-25K DATASET.\nMethod\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\nLSH\n56.06\n56.10\n56.72\n56.82\n57.56\n57.35\nITQ\n57.59\n57.57\n57.70\n57.79\n57.84\n57.87\nDPLM\n60.42\n60.51\n60.53\n60.70\n60.91\n60.79\nSDH\n60.17\n60.27\n60.46\n60.67\n60.96\n61.59\nSGH\n57.35\n57.54\n57.57\n57.67\n57.80\n57.86\nDPSH\n73.48\n74.68\n75.58\n76.01\n76.09\n76.05\nADSH\n75.39\n76.41\n76.98\n76.59\n76.20\n74.53\nDAPH\n72.79\n74.70\n74.30\n74.14\n73.81\n73.41\nDADHL\n77.15\n78.16\n78.64\n79.44\n79.72\n79.26\nD. Comparison with Other Methods\n1) IAPR TC-12: The MAP scores obtained by different\nmethods on the IAPR TC-12 dataset are shown in Tab.II.\nIt is easy to observe that DADH achieves a remarkable\nimprovement in MAP scores compared with other approaches.\nIn contrast to the data-independent method LSH, DADH\nachieves more than 10%-20% percents higher in MAP scores.\nCompared with ITQ, DPLM, SDH and SGH, there is also\nan obvious enhancement. Speciﬁcally, our proposed method\nobtains at least 46.54% MAP score and reaches as high as\n55.39% when the bit length is 48, while the best result obtained\nby ITQ, DPLM, SDH and SGH is only 38.83%, being far\nbelow than our’s. Referring to DPSH, ADSH and DAPH,\nDADH also has more or less improvement in MAP scores.\nParticularly, compared with DAPH, the presented approach\ngains about or more than 5% enhancement when the bit length\nranges from 12 to 48. In comparison to DPSH and ADSH,\nthe performance obtained by DADH also has about 3%-5%\nimprovement when the length of hashing bit is 24, 36, and\n48, respectively.\nThe Top-500 MAP and Top-500 Precision scores on the\nIAPR TC-12 dataset are listed in Tab.III. From this table we\ncan see that the experimental results obtained by the deep\nlearning based methods including DPSH, ADSH, DAPH and\nDADH are remarkably better than that computed by other\ntraditional approaches. Speciﬁcally, there is about 10%-20%\nimprovement in MAP@Top500 and Precision@Top500 scores\nin most cases. In contrast to DPSH, ADSH and DAPH,\nthe performance achieved by the proposed method DADH\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n8\nTABLE V\nTHE TOP-500 MAP AND TOP-500 PRECISION SCORES OBTAINED BY DIFFERENT METHODS ON THE MIRFLICKR-25K DATASET.\nEvaluation\nMAP@Top500\nPrecision@Top500\nMethod\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\nLSH\n57.57\n58.09\n59.27\n59.55\n60.84\n60.67\n56.94\n57.25\n58.50\n58.69\n59.93\n59.84\nITQ\n61.00\n61.17\n61.32\n61.61\n61.64\n61.85\n59.98\n60.11\n60.29\n60.55\n60.56\n60.74\nDPLM\n63.08\n63.84\n64.05\n64.36\n64.73\n65.03\n62.36\n63.14\n63.31\n63.64\n64.05\n64.28\nSDH\n65.16\n64.75\n65.40\n65.34\n65.57\n66.46\n64.05\n63.93\n64.58\n64.44\n64.80\n65.60\nSGH\n60.74\n61.23\n61.48\n61.42\n61.91\n62.04\n59.81\n60.21\n60.46\n60.42\n60.85\n60.93\nDPSH\n82.88\n83.84\n84.34\n84.84\n85.77\n85.64\n81.85\n83.01\n83.58\n84.11\n84.97\n84.80\nADSH\n82.14\n83.80\n84.94\n84.90\n84.20\n82.06\n81.50\n83.18\n84.15\n84.03\n83.52\n81.31\nDAPH\n81.08\n84.20\n83.71\n84.45\n84.02\n84.07\n80.37\n83.24\n82.73\n83.40\n82.93\n82.91\nDADHL\n85.80\n86.83\n86.90\n87.42\n87.98\n87.58\n84.73\n85.78\n85.76\n86.68\n87.08\n86.80\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 3.\nThe Precision-Recall curves computed by LSH, ITQ, DPLM, SDH, SGH, DPSH, ADSH, DAPH, and DADH on the MIRFLICKR-25K dataset.\nFigures from (a) to (f) are associated with the code length 8-bit, 12-bit, 16-bit, 24-bit, 36-bit and 48-bit.\nstill reaches the best point. Except the case when the bit\nlength is 8, DADH always gain 4% or more enhancement in\nMAP@Top500 and Precision@Top500 scores, indicating the\neffectiveness of our method.\nThe Precision-Recall curves computed by different methods\non the IAPR TC-12 dataset are displayed in Fig.2, when the\nbit length changes from 8 to 48. We can easily observe that\ncovered areas gained by DADH are much larger than that\nobtained by other comparison methods. We can ﬁnd that the\nproposed method can dramatically outperform the traditional\ndata-independent and data-dependent strategies. Referring to\nDPSH, ADSH and DAPH, there is also a better achievement\nin all cases with different values of the code length.\n2) MIRFLICKR-25K: The MAP results of the experiment\nconducted on the MIRFLICKR-25K dataset are tabulated in\nTab.IV. We can see that DADH achieves the best performance\nin all cases with different values of the code length. Being\nsimilar to the results on IAPR TC-12 dataset, DPLM, SDH,\nDPSH, ADSH, DAPH and DADH can obtain higher values\nin MAP compared with LSH, ITQ and SGH. Referring to the\ncomparison between the traditional methods and deep learning\nmethods, DPSH, ADSH, DAPH and DADH dramatically\noutperform LSH, ITQ, DPLM, SDH and SGH. Make a com-\nparison between the proposed method with other deep hashing\napproaches, DADH still has a more or less improvement. For\nDADH, there is about 1.5%-3% enhancement on MAP scores\ncompared with these three deep hashing approaches.\nThe Top-500 MAP and Top-500 Precision scores on the\nMIRFLICKR-25K dataset are displayed in Tab.V. It is easy\nto observe that DADH obtains the best performance in both\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n9\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 4. The Precision-Recall curves computed by LSH, ITQ, DPLM, SDH, SGH, DPSH, ADSH, DAPH, and DADH on the CIFAR-10 dataset. Figures from\n(a) to (f) are associated with the code length 8-bit, 12-bit, 16-bit, 24-bit, 36-bit and 48-bit.\nTABLE VI\nTHE MAP SCORES OBTAINED BY DIFFERENT METHODS ON THE\nCIFAR-10 DATASET.\nMethod\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\nLSH\n14.19\n13.26\n13.13\n13.84\n14.90\n15.13\nITQ\n16.57\n17.12\n16.94\n17.08\n17.38\n17.58\nDPLM\n21.97\n22.76\n23.91\n25.89\n27.53\n28.85\nSDH\n30.93\n32.50\n33.59\n35.36\n35.59\n36.53\nSGH\n15.06\n15.43\n15.64\n16.07\n16.78\n16.88\nDPSH\n63.48\n66.97\n68.83\n73.45\n74.66\n75.02\nADSH\n56.67\n71.41\n76.50\n80.40\n82.73\n82.73\nDAPH\n59.09\n61.17\n68.15\n69.22\n70.74\n70.28\nDADHL\n71.86\n75.12\n80.33\n81.70\n83.16\n83.90\nMAP@Top500 and Precision@Top500, demonstrating the su-\nperiority compared with other existing strategies. With the\nchange of the code length, the MAP@Top500 and Preci-\nsion@Top500 increase from (85.80%, 84.73%) to (87.58%,\n86.80%), while the highest values obtained by LSH, ITQ and\nSGH are only (62.04%, 60.93%), being much lower than ours’.\nIn contrast to DPLM and SDH, our strategy still gains more\nthan 20% enhancement in most cases. Furthermore, results\ncomputed by DADH are much higher than that calculated by\nDPSH, ADSH and DAPH. Concretely, scores of Top-500 MAP\nand Top-500 Precision gained by DADH are almost always\nhigher than 85%, while these scores calculated by other deep\nhashing methods are below than 85% in most cases.\nFig.3 show the Precision-Recall curves computed by differ-\nent methods on the MIRFLICKR-25K dataset, when the bit\nlength changes from 8 to 48. Note that we do not depict the\nPrecision-Recall curve obtained by LSH in Fig.3(f), since its\nprecision scores is far below than that of others. From Fig.3\nwe can observe that DADH remarkably outperforms LSH,\nITQ, DPLM, SDH, SGH, ADSH, and DAPH. Referring the\ncomparison between DPSH and DADH, the proposed method\nis obviously superior to DPSH when the code length is 8 and\n12, respectively. Although DPSH covers more areas when the\nrecall value is smaller than 0.4 in Fig.3(c)-(f), it is inferior\nto DADH with the increase of the recall value. Overall, our\nmethod still outperforms DPSH when the code length is 16,\n24, 36 and 48.\n3) CIFAR-10: Tab.VI lists the MAP scores obtained by the\nproposed method and various comparison approaches on the\nCIFAR-10 dataset. With the change of the code length from 8\nto 48, the MAP scores computed by DADH rise from 71.86%\nto 83.90%, being much higher than that obtained by traditional\nhashing approaches, including LSH, ITQ, DPLM, SDH and\nSGH. In contrast to DPSH and DAPH, it is easy to observe\nthat the presented method can achieve a better performance\nunder the different code length. Except the cases when code\nlength is 8 and 12, the MAP scores gained by DADH are\nalways higher than 80%, while the best performance computed\nby DPSH and DAPH is only 75.02%. Also, ADSH is inferior\nto the proposed method, especially when the code length is\nsmall. This relatively indicates the effectiveness of our method\nno matter the code length is small or large.\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n10\nTABLE VII\nTHE TOP-500 MAP AND TOP-500 PRECISION SCORES OBTAINED BY DIFFERENT METHODS ON THE CIFAR-10 DATASET.\nEvaluation\nMAP@Top500\nPrecision@Top500\nMethod\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\n8-bit\n12-bit\n16-bit\n24-bit\n36-bit\n48-bit\nLSH\n20.29\n19.92\n19.81\n21.17\n23.07\n24.42\n14.54\n15.26\n16.19\n18.13\n20.45\n21.44\nITQ\n24.56\n27.72\n27.55\n28.65\n29.54\n30.24\n18.63\n21.88\n22.53\n24.59\n25.65\n26.40\nDPLM\n29.02\n34.07\n35.66\n38.86\n40.28\n41.67\n24.61\n29.72\n31.93\n35.62\n37.98\n39.84\nSDH\n28.34\n34.78\n37.92\n41.69\n42.77\n44.57\n27.91\n33.83\n37.31\n42.39\n43.78\n45.08\nSGH\n25.92\n25.11\n25.91\n27.13\n28.76\n29.46\n19.06\n20.10\n21.53\n23.46\n24.85\n25.42\nDPSH\n58.58\n66.85\n71.48\n75.74\n79.69\n80.62\n64.13\n70.80\n74.71\n78.34\n80.59\n81.55\nADSH\n58.92\n70.07\n74.95\n78.09\n78.85\n77.61\n61.09\n73.71\n78.85\n81.84\n83.45\n82.86\nDAPH\n50.88\n66.24\n72.43\n77.31\n79.21\n80.40\n54.22\n68.28\n74.42\n77.36\n78.80\n79.55\nDADHL\n67.11\n73.08\n78.42\n82.02\n83.51\n84.17\n72.95\n77.53\n82.68\n84.18\n85.23\n85.59\n(a)\n(b)\n(c)\nFig. 5. The MAP scores with the change of parameter τ, η and γ on three datasets, when the code length is 48.\n(a)\n(b)\n(c)\nFig. 6. The change of objective function values and MAP scores with the increase of iterations.\nThe Top-500 MAP and Top-500 Precision scores computed\nby different methods on the CIFAR-10 dataset are shown\nin Tab.VII under the various code length. Obviously, four\ndeep hashing methods always achieve dramatic experimen-\ntal results compared with rest traditional approaches. The\ncomparison between DADH and other deep hashing methods\nalso substantiates the superiority of the proposed strategy. In\ncontrast to DPSH, ADSH and DAPH, DADH has about 3%-\n4% enhancement in both Top-500 MAP and Top-500 Precision\nscores when the code length ranges from 12 to 48.\nThe Precision-Recall curves computed by different methods\non the CIFAR-10 dataset are depicted in Fig.4 under the bit\nlength ranging from 8 to 48. When the code length is 8, 12,\n16, and 24, the Precision-Recall curves obtained by DADH\ncovers the most areas. Although the performance computed\nby ADSH is competitive to ours’ when the code length is 36\nand 48, DADH is still much superior to LSH, ITQ, DPLM,\nSDH, SGH, DPSH, and DAPH.\nE. Parameter Sensitivity Analysis\nThe MAP scores under the changes of different values of\nτ, η and γ are shown in Fig.5. Note, we tune a parameter\nwith others ﬁxed. For instance, we tune τ in the range of\n[0.001, 1, 5, 10, 50, 100] by ﬁxing η = 10 and γ = 100,\nrespectively. Similarly, we set τ = 10, γ = 100 in η tuning\nand τ = 10, η = 10 in γ tuning. As we can see, our model is\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n11\n(a)\n(b)\n(c)\nFig. 7. The change of objective function values and MAP scores with the increase of iterations. Note that, the asymmetric terms\n\r\rtanh(F)BT −kS\n\r\r2\nF and\n\r\rtanh(G)BT −kS\n\r\r2\nF are removed from the objective function.\ninsensitive to parameters. Speciﬁcally, τ, η and γ have a wide\nrange [1,50], [1,50] and [1,300], respectively. Our method\nalways achieves a satisfactory performance when τ, η and γ\nare in these ranges. This relatively demonstrates the robustness\nand effectiveness of the proposed method.\nF. Convergence Analysis\nTo be honest, our proposed model can get a convergence\nwith a few of iterations. The change of the objective function\nvalues and MAP scores on three datasets are displayed in\nFig.6 when the code length is 48-bit. It is easy to observe\nthat DADH converges to a stable value after less than 30\niterations. In fact, we further ﬁnd that the asymmetric terms\n\r\rtanh(F)BT −kS\n\r\r2\nF and\n\r\rtanh(G)BT −kS\n\r\r2\nF greatly con-\ntribute to the quick convergence. We try to remove these\ntwo terms from our objective function to study the inﬂuence.\nNote that, if the asymmetric terms are removed, the binary\ncode B are updated through sign(γ[tanh(G) + tanh(F)]).\nAs shown in Fig.7, if we remove\n\r\rtanh(F)BT −kS\n\r\r2\nF and\n\r\rtanh(G)BT −kS\n\r\r2\nF from the objective function, not only\nthe MAP scores meet a degradation, but also our model\nconverges much slower compared with the original DADH,\nindicating the necessity and signiﬁcance of the asymmetric\nterms.\nV. CONCLUSION\nIn this paper, we propose a novel deep hashing method\nnamed dual asymmetric deep hashing learning (DADH) for\nimage retrieval. Speciﬁcally, two asymmetric networks are\ndesigned to integrate the feature representation and hash func-\ntion learning into the end-to-end framework. A pairwise loss\nis introduced to exploit the semantic structure between each\npair outputs. Furthermore, another pairwise loss is proposed\nto not only capture the similarity between the discrete binary\ncodes and learned real-value features, but also contribute to\na quick convergence at the training phase. Experiments are\nconducted on three large-scale datasets and the outstanding\nresults substantiate the superiority of the proposed method.\nACKNOWLEDGMENT\nThe work is partially supported by the GRF fund from\nthe HKSAR Government, the central fund from Hong Kong\nPolytechnic University, the NSFC fund (61332011, 61272292,\n61271344, 61602540), Shenzhen Fundamental Research fund\n(JCYJ20150403161923528, JCYJ20140508160910917), and\nthe Science and Technology Development Fund (FDCT) of\nMacau 124/2014/A3.\nREFERENCES\n[1] A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approxi-\nmate nearest neighbor in high dimensions,” in Foundations of Computer\nScience, 2006. FOCS’06. 47th Annual IEEE Symposium on. IEEE, 2006,\npp. 459–468.\n[2] A. Gionis, P. Indyk, R. Motwani et al., “Similarity search in high\ndimensions via hashing,” in VLDB, vol. 99, no. 6, 1999, pp. 518–529.\n[3] B. Kulis and K. Grauman, “Kernelized locality-sensitive hashing for\nscalable image search,” in Computer Vision, 2009 IEEE 12th International\nConference on.\nIEEE, 2009, pp. 2130–2137.\n[4] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin, “Iterative quantization:\nA procrustean approach to learning binary codes for large-scale image re-\ntrieval,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 35, no. 12, pp. 2916–2929, 2013.\n[5] G. Lin, C. Shen, Q. Shi, A. Van den Hengel, and D. Suter, “Fast\nsupervised hashing with decision trees for high-dimensional data,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2014, pp. 1963–1970.\n[6] Y. Weiss, A. Torralba, and R. Fergus, “Spectral hashing,” in Advances in\nneural information processing systems, 2009, pp. 1753–1760.\n[7] W. Liu, J. Wang, S. Kumar, and S.-F. Chang, “Hashing with graphs,”\nin Proceedings of the 28th international conference on machine learning\n(ICML-11).\nCiteseer, 2011, pp. 1–8.\n[8] W. Liu, C. Mu, S. Kumar, and S.-F. Chang, “Discrete graph hashing,”\nin Advances in Neural Information Processing Systems, 2014, pp. 3419–\n3427.\n[9] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang, “Supervised hashing\nwith kernels,” in Computer Vision and Pattern Recognition (CVPR), 2012\nIEEE Conference on.\nIEEE, 2012, pp. 2074–2081.\n[10] F. Shen, C. Shen, W. Liu, and H. Tao Shen, “Supervised discrete\nhashing,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2015, pp. 37–45.\n[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural infor-\nmation processing systems, 2012, pp. 1097–1105.\n[12] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[13] H. Liu, R. Wang, S. Shan, and X. Chen, “Deep supervised hashing for\nfast image retrieval,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2016, pp. 2064–2072.\nJOURNAL OF IEEE TRANSACTIONS ON XX, VOL. X, NO. X, XX 2018\n12\n[14] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep su-\npervised hashing with pairwise labels,” arXiv preprint arXiv:1511.03855,\n2015.\n[15] F. Shen, X. Gao, L. Liu, Y. Yang, and H. T. Shen, “Deep asymmetric\npairwise hashing,” 2017.\n[16] C. Da, S. Xu, K. Ding, G. Meng, S. Xiang, and C. Pan, “Amvh: Asym-\nmetric multi-valued hashing,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017, pp. 736–744.\n[17] Q.-Y. Jiang and W.-J. Li, “Asymmetric deep supervised hashing,” arXiv\npreprint arXiv:1707.08325, 2017.\n[18] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni, “Locality-sensitive\nhashing scheme based on p-stable distributions,” in Proceedings of the\ntwentieth annual symposium on Computational geometry.\nACM, 2004,\npp. 253–262.\n[19] M. Raginsky and S. Lazebnik, “Locality-sensitive binary codes from\nshift-invariant kernels,” in Advances in neural information processing\nsystems, 2009, pp. 1509–1517.\n[20] Q.-Y. Jiang and W.-J. Li, “Scalable graph hashing with feature transfor-\nmation.” in IJCAI, 2015, pp. 2248–2254.\n[21] H. Jegou, M. Douze, and C. Schmid, “Product quantization for nearest\nneighbor search,” IEEE transactions on pattern analysis and machine\nintelligence, vol. 33, no. 1, pp. 117–128, 2011.\n[22] F. Shen, C. Shen, Q. Shi, A. Van Den Hengel, and Z. Tang, “Inductive\nhashing on manifolds,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2013, pp. 1562–1569.\n[23] W. Kong and W.-J. Li, “Double-bit quantization for hashing.” in AAAI,\nvol. 1, no. 2, 2012, p. 5.\n[24] M. Norouzi and D. M. Blei, “Minimal loss hashing for compact binary\ncodes,” in Proceedings of the 28th international conference on machine\nlearning (ICML-11).\nCiteseer, 2011, pp. 353–360.\n[25] P. Zhang, W. Zhang, W.-J. Li, and M. Guo, “Supervised hashing with\nlatent factor models,” in Proceedings of the 37th international ACM SIGIR\nconference on Research & development in information retrieval.\nACM,\n2014, pp. 173–182.\n[26] L. Fan, “Supervised binary hash code learning with jensen shannon\ndivergence,” in Proceedings of the IEEE International Conference on\nComputer Vision, 2013, pp. 2616–2623.\n[27] V. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou, “Deep hashing for\ncompact binary codes learning,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2015, pp. 2475–2483.\n[28] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan, “Supervised hashing for\nimage retrieval via image representation learning.” in AAAI, vol. 1, 2014,\npp. 2156–2162.\n[29] H. Lai, Y. Pan, Y. Liu, and S. Yan, “Simultaneous feature learning and\nhash coding with deep neural networks,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2015, pp. 3270–\n3278.\n[30] Y. Cao, M. Long, J. Wang, and S. Liu, “Deep visual-semantic quanti-\nzation for efﬁcient image retrieval,” in CVPR, vol. 2, 2017, p. 6.\n[31] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return of the\ndevil in the details: Delving deep into convolutional nets,” arXiv preprint\narXiv:1405.3531, 2014.\n[32] Q.-Y. Jiang and W.-J. Li, “Deep cross-modal hashing,” arXiv preprint\narXiv:1602.02255, 2016.\n[33] H. J. Escalante, C. A. Hern´andez, J. A. Gonzalez, A. L´opez-L´opez,\nM. Montes, E. F. Morales, L. E. Sucar, L. Villase˜nor, and M. Grubinger,\n“The segmented and annotated iapr tc-12 benchmark,” Computer Vision\nand Image Understanding, vol. 114, no. 4, pp. 419–428, 2010.\n[34] M. J. Huiskes and M. S. Lew, “The mir ﬂickr retrieval evaluation,”\nin Proceedings of the 1st ACM international conference on Multimedia\ninformation retrieval.\nACM, 2008, pp. 39–43.\n[35] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from\ntiny images,” 2009.\n[36] F. Shen, X. Zhou, Y. Yang, J. Song, H. T. Shen, and D. Tao, “A fast\noptimization method for general binary code learning,” IEEE Transactions\non Image Processing, vol. 25, no. 12, pp. 5610–5621, 2016.\n[37] A. Vedaldi and K. Lenc, “Matconvnet: Convolutional neural networks\nfor matlab,” in Proceedings of the 23rd ACM international conference on\nMultimedia.\nACM, 2015, pp. 689–692.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-01-25",
  "updated": "2018-01-25"
}