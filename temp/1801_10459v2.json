{
  "id": "http://arxiv.org/abs/1801.10459v2",
  "title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With Expert Demonstrations",
  "authors": [
    "Xiaoqin Zhang",
    "Huimin Ma"
  ],
  "abstract": "Pretraining with expert demonstrations have been found useful in speeding up\nthe training process of deep reinforcement learning algorithms since less\nonline simulation data is required. Some people use supervised learning to\nspeed up the process of feature learning, others pretrain the policies by\nimitating expert demonstrations. However, these methods are unstable and not\nsuitable for actor-critic reinforcement learning algorithms. Also, some\nexisting methods rely on the global optimum assumption, which is not true in\nmost scenarios. In this paper, we employ expert demonstrations in a\nactor-critic reinforcement learning framework, and meanwhile ensure that the\nperformance is not affected by the fact that expert demonstrations are not\nglobal optimal. We theoretically derive a method for computing policy gradients\nand value estimators with only expert demonstrations. Our method is\ntheoretically plausible for actor-critic reinforcement learning algorithms that\npretrains both policy and value functions. We apply our method to two of the\ntypical actor-critic reinforcement learning algorithms, DDPG and ACER, and\ndemonstrate with experiments that our method not only outperforms the RL\nalgorithms without pretraining process, but also is more simulation efficient.",
  "text": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms\nWith Expert Demonstrations\nXiaoqin Zhang, Huimin Ma\nDept. of EE, Tsinghua University\nxiaoqin-15@mails.tsinghua.edu.cn, mhmpub@tsinghua.edu.cn\nAbstract\nPretraining with expert demonstrations have been\nfound useful in speeding up the training process of\ndeep reinforcement learning algorithms since less\nonline simulation data is required. Some people use\nsupervised learning to speed up the process of fea-\nture learning, others pretrain the policies by imitat-\ning expert demonstrations. However, these meth-\nods are unstable and not suitable for actor-critic\nreinforcement learning algorithms.\nAlso, some\nexisting methods rely on the global optimum as-\nsumption, which is not true in most scenarios. In\nthis paper, we employ expert demonstrations in a\nactor-critic reinforcement learning framework, and\nmeanwhile ensure that the performance is not af-\nfected by the fact that expert demonstrations are not\nglobal optimal. We theoretically derive a method\nfor computing policy gradients and value estima-\ntors with only expert demonstrations. Our method\nis theoretically plausible for actor-critic reinforce-\nment learning algorithms that pretrains both policy\nand value functions. We apply our method to two\nof the typical actor-critic reinforcement learning al-\ngorithms, DDPG and ACER, and demonstrate with\nexperiments that our method not only outperforms\nthe RL algorithms without pretraining process, but\nalso is more simulation efﬁcient.\n1\nIntroduction\nDeep reinforcement learning is a general method that have\nbeen successful in solving complex control problems. Mnih\net al. in [Mnih et al., 2015] combined Q learning with deep\nneural networks and proved to be successful in image based\nAtari games.\nPolicy gradient methods have been proved signiﬁcantly ef-\nﬁcient in both continuous control problems ([Sutton et al.,\n1999], [Silver et al., 2014], [Heess et al., 2015]) and discrete\ncontrol problems ([Silver et al., 2016], [Wang et al., 2016]).\nAmong policy gradient methods, actor-critic algorithms are at\nthe heart of many signiﬁcant advances in reinforcement learn-\ning ([Bhatnagar et al., 2009], [Degris et al., 2012], [Lillicrap\net al., 2015], [Mnih et al., 2016]). These algorithms estimate\nstate-action value functions independently, and proved to be\nefﬁcient in policy optimization.\nHowever, an enormous number of online simulation data is\nrequired for deep reinforcement learning. Hence we attempt\nto learn from expert demonstrations and decrease the amount\nof online data required in deep reinforcement learning algo-\nrithms.\nOne of the representative method of learning from ex-\npert demonstrations is inverse reinforcement learning.\nNg\net al. proposed the ﬁrst inverse reinforcement learning algo-\nrithm [Ng and Russell, 2000], which recovers reward function\nbased on the assumption that the expert policy is the global\noptimal policy. From recovered reward function, Abbeel et\nal. are able to propose apprenticeship learning ([Abbeel and\nNg, 2004]) to train a policy with expert demonstrations and a\nsimulation environment that does not output reward. Appren-\nticeship learning inspired many similar algorithms ([Syed and\nSchapire, 2008], [Syed et al., 2008], [Piot et al., 2014], [Ho\net al., 2016]), Ho et al. [Ho and Ermon, 2016a] proposed\na imitation learning method that merges inverse reinforce-\nment learning and reinforcement learning, hence imitate the\nexpert demonstrations with generative adversarial networks\n(GANs).\nThese algorithms proved successful in solving MDP\\R\n([Abbeel and Ng, 2004]). However, MDP\\R is different from\noriginal MDP since MDP\\R environments do not output task\nbased reward data. And for this reason, inverse reinforce-\nment based algorithms attempt to assume the expert demon-\nstrations to be global optimal and imitate the expert demon-\nstrations. In order to learn from expert demonstrations for\nMDP, alongside with state-of-the-art reinforcement learning\nalgorithms, different frameworks are required.\nThere are some prior work that attempt to make use of ex-\npert demonstrations for reinforcement learning algorithms.\nLakshminarayanan et al. [Lakshminarayanan et al., 2016]\nproposed a training method for DQN based on the assump-\ntion that expert demonstrations are global optimal, thus pre-\ntrain the state-action value function estimators.\nCruz Jr et al. [de la Cruz et al., 2017] focused on feature\nextracting for high dimensional, especially image based sim-\nulation environments, and proposed a framework for discrete\ncontrol problems that pretrains the neural networks with clas-\nsiﬁcation tasks using supervised learning. The purpose of this\npretraining process is to speed up the training process by try-\narXiv:1801.10459v2  [cs.AI]  9 Feb 2018\ning to extract features of high dimensional states. However,\nthis work is only suitable for image based, discrete action en-\nvironments, and ignored the fact that expert demonstrations\nperform better than current learned policies.\nThe ﬁrst published version of AlphaGo [Silver et al., 2016]\nis one of the most important work that pretrains the neural\nnetworks with human expert demonstrations. In this work, a\npolicy network and a value network is used. The value net-\nwork is trained with on-policy reinforcement learning, and\nthe policy network is pretrained with expert demonstrations\nusing supervised learning, then trained with policy gradient.\nThis work and [de la Cruz et al., 2017] are quite similar, the\nrole of expert demonstrations is to speed up the feature ex-\ntraction, and to give policy a warm start. The fact that ex-\npert demonstrations perform better is not fully used, and the\nframework is not extensive enough for other problems and\nother reinforcement learning algorithms.\nIn this paper, we propose an extensive framework that pre-\ntrains actor-critic reinforcement learning algorithms with ex-\npert demonstrations, and use expert demonstrations for both\npolicy functions and value estimators. We theoretically derive\na method for computing policy gradient and value estimators\nwith only expert demonstrations. Experiments show that our\nmethod improves the performance of baseline algorithms on\nboth continuous control environments and high-dimensional-\nstate discrete control environments.\n2\nBackground and Preliminaries\nIn this paper, we deal with an inﬁnite-horizon discounted\nMarkov Decision Process (MDP), which is deﬁned by the tu-\nple {S, A, P, r, ρ0, γ}. In the tuple, S is a ﬁnite set of states,\nA is a ﬁnite set of actions, P : S × A × S →R is the\ntransition probability distribution, r : S →R is the reward\nfunction, ρ0 : S →R is the probability distribution of initial\nstate S0, and γ ∈(0, 1) is the discount factor.\nA stochastic policy πs : S × A →R returns the probabil-\nity distribution of actions based on states, and a deterministic\npolicy πd : S →A returns the action based on states. In this\npaper, we deal with both stochastic policies and deterministic\npolicies, and a ∼π(s) means a ∼πs(a|s) or a = πd(s)\nrespectively. Thus the state-action value function Qπis:\nQπ(st, at) = Est+1,at+1,...\n\" ∞\nX\nτ=0\nγτr(st+τ)\n#\nThe deﬁnitions of the value function V π and the advantage\nfunction Aπ are:\nV π(st) = Eat,st+1,...\n\" ∞\nX\nτ=0\nγτr(st+τ)\n#\nAπ(st, at) = Qπ(st, at) −V π(st)\nAnd let η(π) denote the discounted reward of π:\nη(π) = Es0,a0,...\n\" ∞\nX\nt=0\nγtr(st)\n#\nFor future convenience, let dπ(s) denote the limiting dis-\ntribution of states:\ndπ(s) = lim\nt→∞Pr(st = s)\nwhere in all of the deﬁnitions above:\ns0 ∼ρ0(s0), at ∼π(st), st+1 ∼P(st+1|st, at)\nThe goal of actor-critic reinforcement learning algorithms\nis to maximize the discounted reward, η(π), to obtain the op-\ntimal policy, where we use a parameterized policy πθ. While\nestimating η(π) or ∇θη(πθ) based on simulated samples,\nmany algorithms use a state-action value estimator Qw, to\nestimate the state-value function Qπ for policy function πθ.\nOne typical deterministic actor-critic algorithm DDPG\n(Deep Deterministic Policy Gradient) [Lillicrap et al., 2015]\nuses estimator Qw\n=\nˆ\nQπ to estimate the gradient of\nan off-policy deterministic discounted reward ηβ(πθ)\n=\nP\ns∈S dβ(s)V π(s) [Degris et al., 2012], where β is the roll-\nout policy:\n∇θηβ(π) ≈Est∼dβ(s)\n\u0002\n∇θQw(s, a)|s=st,a∼πθ(st)\n\u0003\n= Est∼dβ(s)\n\u0002\n∇aQw(s, a)|s=st,a∼πθ(st)π′\nθ\n\u0003\nWhere Qw is updated with sampled data from π using Bell-\nman equation, π′\nθ = ∇θπθ(s)|s=st.\nAnother off-policy algorithm that has Qw as an estimator\nof policy πθ is ACER (Actor-Critic with Experience Replay)\n[Wang et al., 2016] that optimizes stochastic policy.\nThe\nalgorithm maximizes off-policy deterministic discounted re-\nward ηβ(πθ) as well, and modiﬁes the off-policy policy gra-\ndient ˆgacer = ∇θηβ(π) to:\nˆgacer = ¯ρt∇θ log πθ(at|st)\n\u0002\nQret(st, at) −V w(st)\n\u0003\n+ Ea∼πθ(st)\n \u0014ρt(a) −c\nρt(a)\n\u0015\n+\n∇θ log πθ(a|st)Aw(st, a)\n!\nWhere\nAw(st, a)\n=\nQw(st, a) −V w(st),\n¯ρt\n=\nmin\nn\nc, π(at,st)\nβ(at,st)\no\n; [x]+ = x if x > 0 and is zero otherwise;\nV w(st) = Ea∼πθ(st)(st, a); ρt(a) =\nπ(a,st)\nβ(a,st); st ∼dβ(st)\nand at ∼β(st); Qret is the Retrace estimator of Qπ [Munos\net al., 2016], which can be expressed recursively as follows:\nQret(st, at) = rt + γ¯ρt+1δQ(st+1, at+1) + γV w(s+1)\nwhere\nδQ(st+1, at+1) = Qret(st+1, at+1) −Qw(st+1, at+1)\nIn ACER, state-action value function is updated using Qret\nas target, with gradient gQ:\ngQ = (Qret(st, at) −Qw(st, at))∇wQw(st, at)\nIn this paper, we will apply our methods with expert\ndemonstrations to DDPG and ACER.\n3\nExpert Based Pretraining Methods\nSuppose there exists an expert policy π∗that performs better\nthan π. We deﬁne perform better with the following straight-\nforward constraint:\nη(π∗) ⩾η(π)\n(1)\nThe deﬁnition of perform better above is based on the fact\nthat the goal of actor-critic RL algorithms is to maximize\nη(π). Here the expert policy π∗is different from that of IRL\n[Ng and Russell, 2000], imitation learning [Ho and Ermon,\n2016b] or LfD [Hester et al., 2017], since π∗here is not the\noptimum policy of the MDPs.\nHere we deﬁne a demonstration of a policy π as a sequence\nof (s, a) pairs, {(st, at)}t=0,1,2,..., sampled from π.\nActor-critic RL algorithms tend to optimize η(πθ) as the\ntarget. Thus pretraining procedures for these algorithms need\nto estimate η(πθ) as the optimization target using expert\ndemonstrations. Also, from deﬁnition (1), we need to esti-\nmate η(π∗) as well.\nHowever, With only demonstrations of expert policy π∗\nand a black-box simulation environment, η(π∗) and η(πθ)\ncannot be directly estimated. Hence we introduce Theorem\n1 (see [Schulman et al., 2015] and [Kakade and Langford,\n2002]).\nTheorem 1. For two policies π and π∗:\nη(π∗) −η(π) = Es∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγtAπ(s∗\nt , a∗\nt )\n#\n(2)\nProof. (See also [Schulman et al., 2015] and [Kakade and\nLangford, 2002]) Note that\nAπ(s, a) = Es′∼P (s′|s,a) [r(s) + γV π(s′) −V π(s)]\nwe have:\nEs∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγtAπ(s∗\nt , a∗\nt )\n#\n=Es∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγt(r(st) + γV π(st+1) −V π(st))\n#\n= −Es∗\n0∼ρ0 [V π(s0)] + Es∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγtr(st)\n#\n= −η(π) + η(π∗)\nFor many actor-critic RL algorithms like DDPG and\nACER, policy optimization is based on accurate estima-\ntions of state-action value functions or value functions of the\nlearned policy πθ. Typically, those algorithms use data sam-\npled from πθ, {(st, at, rt)}t=0,1,2,..., to estimate Qπ and V π.\nThe estimating processes usually need a large amount of sim-\nulations to be accurate enough.\nCombine Theorem 1 with constraint (1), we have:\nEs∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγtAπ(s∗\nt , a∗\nt )\n#\n⩾0\n(3)\nThis result links state-action value functions with expert\ndemonstrations, allowing us to apply constraint (1) while\ntraining state-action value functions. This constraint is for\nvalue estimators, like Qw and V w. When value estimators\nare not accurate enough, constraint (3) would not be satis-\nﬁed. Hence if an algorithm update value estimators under\nconstraint (3), the estimators would be more accurate, and in\nresult improve the policy optimizing process.\nAnother pretraining process is policy optimization using\nexpert demonstrations. Like most actor-critic algorithms, we\nsuppose advantage function Aπ(s, a) is already known while\nconducting policy optimization. Then we can estimate the up-\ndate step with expert demonstrations and estimations of value\nfunctions.\nConsidering Theorem 1, we estimate he policy gradient as\nthe following:\n∇θη(πθ)\n=∇θ(η(πθ) −η(π∗))\n= −∇θEs∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγtAπ(s∗\nt , a∗\nt )\n#\n(4)\nEquation (4) provides an off-policy policy optimization\nprocedure with data only from expert demonstrations. It turns\nout that perform better is not a must in this procedure for ex-\npert policy π∗.\nRecently, people like to propose sample efﬁcient RL algo-\nrithms, like ACER and Q-Prop [Gu et al., 2017], since RL al-\ngorithms need a large amount of simulation time while train-\ning. With expert demonstrations, since there is no reward\ndata, we cannot conduct sample efﬁcient policy optimization\nprocesses. However, when we update policies with (4), no\nsimulation time is needed. We call the situation simulation ef-\nﬁcient, which means the algorithms may need a large amount\nof data, but need few simulation data while training.\nNote that sample efﬁcient algorithms are all simulation ef-\nﬁcient algorithms, all of these methods intend to decrease the\nsimulation time. In this paper, we evaluate our method by\nhow simulation efﬁcient it is.\nIn this section, we found two pretraining methods for actor-\ncritic RL algorithms, namely (3) and (4).\nBoth of them\nare based on Theorem 1. The theorem connects policy dis-\ncounted reward η(πθ) and expert demonstration data, requir-\ning no reward data from expert trajectories.\nEquation (3)\ngives a constraint of value function estimators based on the\ndeﬁnition of perform better, and equation (4) provides an off-\npolicy method to optimize policy function regardless of how\nexpert demonstrations perform.\n4\nAlgorithms with Expert Demonstrations\nTheorem 1 provides a way to satisfy constraint (1) and update\npolicies πθ with demonstrations of expert policy π∗, and does\nnot need reward data sampled from π∗. In this section, we\norganize the results in Section 3 in a more piratical way, then\nwe apply the pretraining methods to two of the typical actor-\ncritic RL algorithms, DDPG and ACER.\nThese actor-critic RL algorithms use neural networks\nQw(s, a) to estimate the state-action value functions of pol-\nicy, Qπ(s, a), where π is the is the current learned policy\nwhile training, which is a parameterized function, πθ, always\nin the form of artiﬁcial neural networks.\nFor pretraining processes based on Theorem 1, we need\nan estimator of advantage function for policy πθ, Aπ(s, a).\nBased on parameterized policy and state-action value func-\ntion estimator Qw, we obtain the advantage function estima-\ntor Aw,θ:\nAw,θ(s∗\nt , a∗\nt ) = Qw(s∗\nt , a∗\nt ) −V w,θ(s∗\nt )\n(5)\nV w,θ(s∗\nt ) = Ea∼πθ(s)Qw(s∗\nt , a)\n(6)\nConsidering the training processes of DDPG and ACER, at\nthe beginning of the processes the policies are nearly random\nand estimators Qw(s, a) are not accurate, since there is little\ndata from simulation. Therefore if there exist some expert\ndemonstrations that perform better than initial policies, we\ncan introduce the data using constraint (3), in order to obtain\na more accurate estimator Qw(s, a).\nIf constraint (3) is satisﬁed, then Qw(s, a) is accurate\nenough for the fact that π∗performs better. Hence we update\nthe estimator with expert demonstrations with the following\ngradient, in which [x]+ = x if x > 0, otherwise is zero:\ng∗\nQ = ∇w\n\"\nEs∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγtAw,θ(s∗\nt , a∗\nt )\n##\n+\n(7)\nFrom equation (4), we optimize policy with expert demon-\nstrations. Since expert demonstrations do not contain reward\ndata, we can update policy parameters with a simple policy\ngradient:\ng∗\nπ = −∇θEs∗\n0,a∗\n0,...∼π∗\n\" ∞\nX\nt=0\nγtAw,θ(s∗\nt , a∗\nt )\n#\n(8)\nFor the reason that π∗is not the optimal policy of the\nMDPs, we only train with expert demonstrations for a lim-\nited period of time at the beginning of the training process,\nto guarantee π∗performs better than πθ, hence we call the\nprocess pretraining.\nTo pretrain actor-critic RL algorithms like DDPG and\nACER, we add gradients g∗\nQ and g∗\nπ to the original gradients\nof the algorithms:\ngpre\nQ\n= gQ + λQg∗\nQ\n(9)\ngpre\nπ\n= gπ + λπg∗\nπ\n(10)\nWhere gQ and gπ are original gradients of baseline actor-\ncritic RL algorithms, andgpre\nQ\nand gpre\nπ\nare pretraining gradi-\nents for estimator Qw and parameterized policy function πθ\nrespectively while pretraining. We introduce expert demon-\nstrations to the base algorithms instead of replacing them,\nsince the state-action value functions are estimated with the\nbaseline algorithms and gradient g∗\nQ only makes Qw satisfy\nconstraint (1).\nFigure 1: Example screenshots of MuJoCo simulation environments\nthat we attend to experiment on with DDPG as baseline. The tasks\nare: HalfCheetah (left), Hopper (middle), and Walker2d (right).\n4.1\nPretraining DDPG\nDDPG is a representative off-policy actor-critic deterministic\nRL algorithm. The algorithm is for continuous action space\nMDPs, and optimizes the policy using off-policy policy gra-\ndient.\nTwo neural networks are used in DDPG at the same time.\nOne is named critic network, which is the state-action value\nfunction estimator Qw, and the other is named actor network,\nwhich is the parameterized policy πθ. Since it is an algorithm\nfor deterministic control, the input of the actor network is a\nstate of MDPs, and the output is the corresponding action.\nTwo neural networks are trained simultaneously, with gra-\ndients gQ and gπ respectively. gQ is based on Bellman equa-\ntion, and gπ is the off-policy policy gradient.\nIn order to introduce expert demonstrations for pretraining\ncritic network and actor network, we apply (9) and (10) to\npretrain the two neural networks.\nNote that for a deterministic policy πθ, equation (6) be-\ncomes V w,θ(s) = Qw(s, πθ(s)).\n4.2\nPretraining ACER\nACER is an off-policy actor-critic stochastic RL algorithm,\nwhich modiﬁes the policy gradient to make the process sam-\nple efﬁcient. ACER solves both discrete control problems and\ncontinuous control problems.\nFor discrete control problems, a double-output convolu-\ntional neural work (CNN) is used in ACER. One output is\na softmax policy πθ, and the other is Qw values. Although θ\nand w share most of the parameters, they are updated sepa-\nrately with different gradients.\nFor stochastic control problems, a new structure named\nStochastic Dueling Networks (SDNs) is used for value func-\ntion estimation. The network outputs a deterministic value es-\ntimation V w(s), and a stochastic state-action value estimation\nQw,θ(s, a) ∼V w(s) + Aw(s, a) −1\nn\nPn\ni=1 Aw(s, ˙a)|˙a∼πθ.\nHence equation (5) becomes Aw,θ(s∗\nt , a∗\nt ) = Qw,θ(s∗\nt , a∗\nt ) −\nV w(s∗\nt ).\nIn ACER, gradient gπ is the modiﬁed policy gradient, and\ngQ is based on Retrace. Both of the gradients are explained\nin Section 2.\nPolicy gradient is estimated using trust region in ACER,\nbut in this paper, we compute pretraining gradients g∗\nQ and\ng∗\nπ directly with expert demonstrations.\n0\n2\n4\n6\n8\n10\nTraining Steps /105\n0\n1000\n2000\n3000\n4000\n5000\n6000\nReward\nDDPG\nDDPG+Pretrain\n0\n2\n4\n6\n8\n10\nTraining Steps /105\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n0\n2\n4\n6\n8\n10\nTraining Steps /105\n0\n1000\n2000\n3000\n4000\nFigure 2: Results of pretraining based on DDPG. The ﬁgures each is a different task, and they are respectfully experimented on HalfCheetah\n(left), Hopper (middle) and Walker2d (right), The vertical dashed black lines represent the points when pretraining end, and the horizontal\ndashed brown lines represent the average episode reward of expert demonstrations. The transparent blue and red lines are original training\nresults, and the opaque lines are smoothed lines with sliding windows.\n5\nExperiments\nWe test our algorithms based on DDPG and ACER on various\nenvironments, in order to investigate how simulation efﬁcient\nthe pretraining methods are. The baselines are DDPG and\nACER without pretraining.\nBecause of the existence of [x]+, g∗\nQ deﬁned in (7) could\nbe inﬁnity sometimes. Hence we clip the gradient during pre-\ntraining. We set λQ and λπ = 1 in equations (9) and (10).\nThe expert policies that generate expert demonstrations are\npolicies trained with baseline algorithms, i.e.\nDDPG and\nACER.\nWith DDPG as baseline, we apply our algorithm to low\ndimensional simulation environments using the MuJoCo\nphysics engine [Todorov et al., 2012], and test on tasks with\naction dimensionality are: HalfCheetah (6D), Hopper (3D),\nand Walker2d (6D). These tasks are illustrated in Figure 1.\nAll the setups with DDPG as baseline share the same net-\nwork architecture that compute policies and estimate value\nfunctions referring to [Lillicrap et al., 2015]. Adam [Kingma\nand Ba, 2014] is used for learning parameters and the learn-\ning rate of actor network and critic network are respectively\n10−3 and 10−4. For critic network, L2 weight decay of 10−2\nis used with γ = 0.99. Both actor network and critic network\nhave 2 hidden layers with 400 and 300 units respectively.\nThe results of our pretraining method based on DDPG are\nillustrated in Figure 2. In the ﬁgures, the horizontal dashed\nbrown lines represent the average episode reward of expert\ndemonstrations. It is obvious that the expert demonstrations\nare not global optimal demonstrations, and in order to guar-\nantee the expert policies perform better than learned policies,\nthe pretraining process stops early with 30000 training steps\nand 60000 simulation steps.\nAs shown in Figure 2, it is obvious that DDPG with our\npretraining method outperforms initial DDPG. Results on\nHalfCheetah (Figure 2 left) is representative and clear, pre-\ntraining process gives training a warm start, and after pre-\ntraining stops, the performance drops because of the new\nlearning gradient. However, after pretraining, DDPG learns\nfaster than the baseline, hence it outperforms initial DDPG.\nAlthough the results of DDPG are unstable on Hopper (Fig-\nure 2 middle) and Walker2d (Figure 2 right), smoothed results\nFigure 3: Example screenshots of Atari simulation environments\nthat we attend to experiment on with ACER as baseline. The tasks\nfrom left to right are: AirRaid, Breakout, Carnival, CrazyClimber\nand Gopher.\nindicate that DDPG with pretraining processes learns faster\nthan DDPG.\nWith ACER as baseline, we apply our algorithm to image\nbased Atari games. We only tested on discrete control prob-\nlems with ACER, and the environments we tested on are: Air-\nRaid, Breakout, Carnival, CrazyClimber and Gopher. The en-\nvironments are illustrated in Figure 3.\nThe experiment settings are similar to [Wang et al., 2016],\nThe double-output network consists of a convolutional layer\nwith 32 8×8 kernels with stride 4, a convolutional layer with\n64 4 × 4 kernels with stride 2, a convolutional layer with 64\n3 × 3 kernels with stride 1, followed by a fully connected\nlayer with 512 units. The network outputs a softmax policy\nand state-action value Q for every action.\nBecause of the limitation of memory, each thread of ACER\nonly have a replay memory of 5000 frames, which is the only\ndifferent setting from [Wang et al., 2016]. Entropy regular-\nization with weight 0.001 is also adopted, and the discount\nfactor γ = 0.99, importance weight truncation c = 10. Trust\nregion updating is used as described in [Wang et al., 2016],\nand all the settings of trust region update remain the same.\nACER without trust region update is not tested in this paper.\nThe results of our pretraining method based on ACER with\ntrust region update is illustrated in Figure 4. All of the envi-\nronments are image based Atari games. All the lines have the\nsame meaning as Figure 2, and it is obvious that ACER with\npretraining process outperforms initial ACER.\nUnlike DDPG, the performance of learned policies does\nnot fall after pretraining process ends. This is because for\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Steps /107\n0\n20\n40\n60\n80\n100\nReward\nAirRaid\nACER+pretrain\nACER\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Steps /107\n0\n5\n10\n15\n20\n25\n30\nBreakout\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Steps /107\n10\n15\n20\n25\n30\n35\n40\nCarnival\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Steps /107\n0\n25\n50\n75\n100\n125\n150\n175\nReward\nCrazyClimber\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Steps /107\n0\n100\n200\n300\n400\n500\nGopher\nFigure 4: Results of pretraining based on ACER with trust region update. Similar to Figure 2, the vertical dashed black lines are the points\nwhen pretraining end, and the horizontal dashed brown lines are the average episode reward of expert demonstrations. The transparent red\nand blue lines represent the original training results, and the opaque ones are smoothed results with sliding windows.\nstochastic discrete control, a random policy and a random\nstate-action value estimator always satisﬁes constraint (1),\nhence g∗\nQ deﬁned in (7) is always zero, and g∗\nπ deﬁned in (8)\nis policy gradient based on expert demonstrations, similar to\noriginal gπ from baseline ACER, therefore the performance\nof learned policies does not fall after pretraining.\nNote that learning with expert demonstrations use the same\namount of simulation steps as baseline algorithms, our pre-\ntraining method is more simulation efﬁcient than baselines.\n6\nConclusion\nIn this work, we propose an extensive method that pretrains\nactor-critic reinforcement learning methods. Based on The-\norem 1, we design a method that takes advantage of expert\ndemonstrations. Our method does not rely on the global op-\ntimal assumption of expert demonstrations, which is one of\nthe key differences between our method and IRL algorithms.\nOur method pretrains policy function and state-action value\nestimators simultaneously with gradients (9) and (10). With\nexperiments based on DDPG and ACER, we demonstrate that\nour method outperforms the raw RL algorithms.\nOne limitation of our framework is that it has to estimate\nthe advantage function for expert demonstrations, and the\nframework is not suitable for algorithms like A3C [Mnih et\nal., 2016] and TRPO [Schulman et al., 2015] that only main-\ntain a value estimator V w(s). On the other hand, the fact\nthat expert demonstrations perform better is not considered\nduring pretraining of policies (Equation (8)). We left these\nextensions in our future work.\nAcknowledgments\nThis work was supported by National Key R&D Program of\nChina (No. 2016YFB0100901), and National Natural Sci-\nence Foundation of China (No. 61773231).\nReferences\n[Abbeel and Ng, 2004] Pieter Abbeel and Andrew Y Ng.\nApprenticeship learning via inverse reinforcement learn-\ning. In Proceedings of the twenty-ﬁrst international con-\nference on Machine learning. ACM, 2004.\n[Bhatnagar et al., 2009] Shalabh Bhatnagar, Richard Sutton,\nMohammad Ghavamzadeh, and Mark Lee. Natural actor-\ncritic algorithms. Automatica, 45(11), 2009.\n[de la Cruz et al., 2017] Gabriel V. de la Cruz, Jr., Yunshu\nDu, and Matthew E. Taylor. Pre-training neural networks\nwith human demonstrations for deep reinforcement learn-\ning. Technical report, September 2017.\n[Degris et al., 2012] Thomas Degris, Martha White, and\nRichard S Sutton. Off-Policy Actor-Critic.pdf. Icml, 2012.\n[Gu et al., 2017] Shixiang Gu, Timothy Lillicrap, Zoubin\nGhahramani, Richard E Turner, and Sergey Levine. Q-\nProp : Sample-Efﬁcient Policy Gradient with An Off -\nPolicy Critic. ICLR, 2017.\n[Heess et al., 2015] Nicolas Heess, Gregory Wayne, David\nSilver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learn-\ning continuous control policies by stochastic value gradi-\nents. In Advances in Neural Information Processing Sys-\ntems, pages 2944–2952, 2015.\n[Hester et al., 2017] Todd Hester, Matej Vecerik, Olivier\nPietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew\nSendonaris, Gabriel Dulac-Arnold, Ian Osband, John Aga-\npiou, et al. Learning from demonstrations for real world\nreinforcement learning. arXiv preprint arXiv:1704.03732,\n2017.\n[Ho and Ermon, 2016a] Jonathan Ho and Stefano Ermon.\nGenerative adversarial imitation learning.\nIn Advances\nin Neural Information Processing Systems, pages 4565–\n4573, 2016.\n[Ho and Ermon, 2016b] Jonathan Ho and Stefano Ermon.\nGenerative Adversarial Imitation Learning. In Nips, pages\n4565–4573, 2016.\n[Ho et al., 2016] Jonathan Ho, Jayesh Gupta, and Stefano\nErmon. Model-free imitation learning with policy opti-\nmization. In International Conference on Machine Learn-\ning, pages 2760–2769, 2016.\n[Kakade and Langford, 2002] Sham Kakade and John Lang-\nford.\nApproximately Optimal Approximate Reinforce-\nment Learning.\nProceedings of the 19th International\nConference on Machine Learning, pages 267–274, 2002.\n[Kingma and Ba, 2014] Diederik Kingma and Jimmy Ba.\nAdam:\nA method for stochastic optimization.\narXiv\npreprint arXiv:1412.6980, 2014.\n[Lakshminarayanan et al., 2016] Aravind\nS\nLakshmi-\nnarayanan, Sherjil Ozair, and Yoshua Bengio. Reinforce-\nment Learning with Few Expert Demonstrations. Neural\nInformation Processing Systems - Workshop on Deep\nLearning for Action and Interaction, 2016.\n[Lillicrap et al., 2015] Timothy P Lillicrap, Jonathan J Hunt,\nAlexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra.\nContinuous con-\ntrol with deep reinforcement learning.\narXiv preprint\narXiv:1509.02971, 2015.\n[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K Fidje-\nland, Georg Ostrovski, et al. Human-level control through\ndeep reinforcement learning. Nature, 518(7540):529–533,\n2015.\n[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech\nBadia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu.\nAsyn-\nchronous methods for deep reinforcement learning.\nIn\nInternational Conference on Machine Learning, pages\n1928–1937, 2016.\n[Munos et al., 2016] R´emi Munos, Tom Stepleton, Anna\nHarutyunyan, and Marc G. Bellemare. Safe and Efﬁcient\nOff-Policy Reinforcement Learning. arXiv, (Nips), 2016.\n[Ng and Russell, 2000] Andrew Ng and Stuart Russell. Al-\ngorithms for inverse reinforcement learning. Proceedings\nof the Seventeenth International Conference on Machine\nLearning, 0:663–670, 2000.\n[Piot et al., 2014] Bilal Piot, Matthieu Geist, and Olivier\nPietquin.\nBoosted bellman residual minimization han-\ndling expert demonstrations. In Joint European Confer-\nence on Machine Learning and Knowledge Discovery in\nDatabases, pages 549–564. Springer, 2014.\n[Schulman et al., 2015] John\nSchulman,\nSergey\nLevine,\nMichael Jordan, and Pieter Abbeel. Trust Region Policy\nOptimization. Icml-2015, page 16, 2015.\n[Silver et al., 2014] David Silver, Guy Lever, Nicolas Heess,\nThomas Degris, Daan Wierstra, and Martin Riedmiller.\nDeterministic policy gradient algorithms. In Proceedings\nof the 31st International Conference on Machine Learning\n(ICML-14), pages 387–395, 2014.\n[Silver et al., 2016] David Silver, Aja Huang, Chris J Maddi-\nson, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan-\nneershelvam, Marc Lanctot, et al. Mastering the game of\ngo with deep neural networks and tree search.\nNature,\n529(7587):484–489, 2016.\n[Sutton et al., 1999] Richard S. Sutton, David Mcallester,\nSatinder Singh, and Yishay Mansour.\nPolicy Gradient\nMethods for Reinforcement Learning with Function Ap-\nproximation. Advances in Neural Information Processing\nSystems 12, pages 1057–1063, 1999.\n[Syed and Schapire, 2008] Umar\nSyed\nand\nRobert\nE\nSchapire.\nA game-theoretic approach to apprenticeship\nlearning. In Advances in neural information processing\nsystems, pages 1449–1456, 2008.\n[Syed et al., 2008] Umar\nSyed,\nMichael\nBowling,\nand\nRobert E Schapire. Apprenticeship learning using linear\nprogramming.\nIn Proceedings of the 25th international\nconference on Machine learning, pages 1032–1039. ACM,\n2008.\n[Todorov et al., 2012] Emanuel Todorov, Tom Erez, and Yu-\nval Tassa.\nMujoco: A physics engine for model-based\ncontrol. In Intelligent Robots and Systems (IROS), 2012\nIEEE/RSJ International Conference on, pages 5026–5033.\nIEEE, 2012.\n[Wang et al., 2016] Ziyu Wang, Victor Bapst, Nicolas Heess,\nVolodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and\nNando de Freitas. Sample efﬁcient actor-critic with expe-\nrience replay. arXiv preprint arXiv:1611.01224, 2016.\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-01-31",
  "updated": "2018-02-09"
}