{
  "id": "http://arxiv.org/abs/1802.00810v4",
  "title": "Deep Learning for Genomics: A Concise Overview",
  "authors": [
    "Tianwei Yue",
    "Yuanxin Wang",
    "Longxiang Zhang",
    "Chunming Gu",
    "Haoru Xue",
    "Wenping Wang",
    "Qi Lyu",
    "Yujie Dun"
  ],
  "abstract": "Advancements in genomic research such as high-throughput sequencing\ntechniques have driven modern genomic studies into \"big data\" disciplines. This\ndata explosion is constantly challenging conventional methods used in genomics.\nIn parallel with the urgent demand for robust algorithms, deep learning has\nsucceeded in a variety of fields such as vision, speech, and text processing.\nYet genomics entails unique challenges to deep learning since we are expecting\nfrom deep learning a superhuman intelligence that explores beyond our knowledge\nto interpret the genome. A powerful deep learning model should rely on\ninsightful utilization of task-specific knowledge. In this paper, we briefly\ndiscuss the strengths of different deep learning models from a genomic\nperspective so as to fit each particular task with a proper deep architecture,\nand remark on practical considerations of developing modern deep learning\narchitectures for genomics. We also provide a concise review of deep learning\napplications in various aspects of genomic research, as well as pointing out\npotential opportunities and obstacles for future genomics applications.",
  "text": "arXiv:1802.00810v4  [q-bio.GN]  4 Oct 2023\nDeep Learning for Genomics: A Concise Overview\nDeep Learning for Genomics: A Concise Overview\nTianwei Yue†\ntianwei.vy.yue@gmail.com\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213, USA\nYuanxin Wang\nyuanxinw@alumni.cmu.edu\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213, USA\nLongxiang Zhang\nlongxiaz@alumni.cmu.edu\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213, USA\nChunming Gu\ncgu15@jhmi.edu\nDepartment of Biomedical Engineering\nSchool of Medicine\nJohns Hopkins University\nBaltimore, MD 21205, USA\nHaoru Xue\nhaorux@andrew.cmu.edu\nThe Robotics Institute\nCarnegie Mellon University\nPittsburgh, PA 15213, USA\nWenping Wang\nwenpingw@alumni.cmu.edu\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213, USA\nQi Lyu\nlyuqi1@msu.edu\nDepartment of Computational Mathematics, Science, and Engineering\nMichigan State University\nEast Lansing, MI 48824, USA\nYujie Dun\ndunyj@mail.xjtu.edu.cn\nSchool of Information and Communications Engineering\nXi’an Jiaotong University\nXi’an, Shaanxi 710049, China\nCorresponding Author: Tianwei Yue\n1\nYue et al.\nContents\n1\nIntroduction\n3\n2\nDeep Learning Architectures: A Genomic Perspective\n5\n2.1\nConvolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nRecurrent Neural Networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.3\nAutoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.4\nEmergent Deep Architectures . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.4.1\nBeyond Classic Models . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.4.2\nHybrid Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.5\nTransformer-based Large Language Models\n. . . . . . . . . . . . . . . . . .\n9\n3\nDeep Learning Architectures: Insights and Remarks\n10\n3.1\nModel Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nTransfer Learning and Multitask Learning . . . . . . . . . . . . . . . . . . .\n11\n3.3\nMulti-view Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4\nGenomic Applications\n12\n4.1\nGene expression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.1.1\nGene expression Characterization . . . . . . . . . . . . . . . . . . . .\n13\n4.1.2\nGene expression Prediction\n. . . . . . . . . . . . . . . . . . . . . . .\n14\n4.2\nRegulatory Genomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.2.1\nPromoters and Enhancers . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.2.2\nSplicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.2.3\nTranscription Factors and RNA-binding Proteins . . . . . . . . . . .\n18\n4.3\nFunctional Genomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.3.1\nMutations and Functional Activities . . . . . . . . . . . . . . . . . .\n19\n4.3.2\nSubcellular Localization . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.4\nStructural Genomics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.4.1\nStructural Classiﬁcation of Proteins\n. . . . . . . . . . . . . . . . . .\n20\n4.4.2\nProtein Secondary Structure\n. . . . . . . . . . . . . . . . . . . . . .\n21\n4.4.3\nContact Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.4.4\nProtein Tertiary Structure and Quality Assessment . . . . . . . . . .\n24\n5\nChallenges and Opportunities\n25\n5.1\nThe Nature of Data\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n5.1.1\nClass-Imbalanced Data . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.2\nVarious Data Types\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.3\nHeterogeneity and Confounding Correlations\n. . . . . . . . . . . . .\n27\n5.2\nFeature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5.2.1\nMathematical Feature Extraction . . . . . . . . . . . . . . . . . . . .\n28\n5.2.2\nFeature Representation\n. . . . . . . . . . . . . . . . . . . . . . . . .\n28\n6\nConclusion and Outlook\n28\n2\nDeep Learning for Genomics: A Concise Overview\nAbstract\nThis data explosion driven by advancements in genomic research, such as high-throughput\nsequencing techniques, is constantly challenging conventional methods used in genomics.\nIn parallel with the urgent demand for robust algorithms, deep learning has succeeded\nin various ﬁelds such as vision, speech, and text processing. Yet genomics entails unique\nchallenges to deep learning since we expect a superhuman intelligence that explores beyond\nour knowledge to interpret the genome from deep learning. A powerful deep learning model\nshould rely on insightful utilization of task-speciﬁc knowledge. In this paper, we brieﬂy\ndiscuss the strengths of diﬀerent deep learning models from a genomic perspective so as to ﬁt\neach particular task with proper deep architecture, and remark on practical considerations\nof developing deep learning architectures for genomics. We also provide a concise review\nof deep learning applications in various aspects of genomic research and point out current\nchallenges and potential research directions for future genomics applications.\n1. Introduction\nEven since Watson et al. (1953) ﬁrst interpreted DNA molecules as the physical medium\ncarrying genetic information, human beings have been striving to gather biological data\nand decipher the biological processes guided by genetic information. By the time of 2001,\nthe Human Genome Project launched in 1990 had drafted the raw information of a typ-\nical human genome (Lander et al., 2001). Many other genome projects, including FAN-\nTOM (Kawai et al., 2001), ENCODE (Consortium et al., 2012), Roadmap Epigenomics\n(Kundaje et al., 2015), were also launched in succession. These collaborative eﬀorts made\nan abundance of DNA data available and thus allowed a global perspective on the genome\nof diﬀerent species, leading to the prosperity of genomic research.\nGenomic research aims to understand the genomes of diﬀerent species. It studies the\nroles assumed by multiple genetic factors and the way they interact with the surrounding\nenvironment under diﬀerent conditions. In contrast to genetics which deals with a limited\nnumber of speciﬁc genes, genomics takes a global view that involves the entirety of genes\npossessed by an organism (JAX, 2018). For example, a study of homo sapiens involves\nsearching through approximately 3 billion units of DNA, containing protein-coding genes,\nRNA genes, cis-regulatory elements, long-range regulatory element, and transposable el-\nements (Bae et al., 2015). Additionally, genomics is becoming increasingly data-intensive\nwith the advancement in genomic research, such as the cost-eﬀective next-generation se-\nquencing technology that produces the entire readout of the DNA of an organism. This\nhigh-throughput technology is made available by more than 1,000 sequencing centers cata-\nloged by OmicsMaps (http://omicsmaps.com/) on nearly every continent (Stephens et al.,\n2015). The vast trove of information generated by genomic research provides a potential\nexhaustive resource for scientiﬁc study with statistical methods. These statistical meth-\nods can be used to identify diﬀerent types of genomic elements, such as exons, introns,\npromoters, enhancers, positioned nucleosomes, splice sites, untranslated region (UTR), etc.\nIn addition to recognizing these patterns in DNA sequences, models can take other ge-\nnetic and genomic information as input to build systems to help understand the biolog-\nical mechanisms of underlying genes.\nA large variety of data types are available, such\nas chromatin accessibility assays (e.g.\nMNase-seq, DNase-seq, FAIRE), genomic assays\n(e.g. microarray, RNA-seq expression), transcription factor (TF) binding ChIP-seq data,\n3\nYue et al.\ngene expression proﬁles, and histone modiﬁcations, etc (Libbrecht, 2016). Most of these\ndata are available through portals like GDC (https://portal.gdc.cancer.gov/), dbGaP\n(https://www.ncbi.nlm.nih.gov/gap), GEO (https://www.ncbi.nlm.nih.gov/geo/),\njust to name a few. A combination of various data can bring about deeper insights into\ngenes so as to help researchers to locate the information of interest.\nOn the other hand, the development of deep learning methods has granted the compu-\ntational power to resolve these complex research questions (LeCun et al., 2015; Wang et al.,\n2017d). Its success has already been demonstrated by the revolutionizing achievements in\nthe ﬁeld of artiﬁcial intelligence, e.g., image recognition, object detection, audio recognition,\nand natural language processing, etc. The boom of deep learning is supported by the suc-\ncessive introduction of a variety of deep architectures, including autoencoders (Fukushima,\n1975) and their variants, multilayer perceptron (MLP; Rumelhart et al., 1985; Svozil et al.,\n1997), restricted Boltzmann machines (RBMs; Hinton and Sejnowski, 1986), deep belief\nnetworks (DBNs; Hinton and Salakhutdinov, 2006), convolutional neural networks (CNN;\nFukushima and Miyake, 1982; LeCun et al., 1990), recurrent neural networks (RNN; Elman,\n1990), Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997), Transform-\ners (Vaswani et al., 2017), Large Language Models (Radford et al., 2018, 2019; Brown et al.,\n2020) , and other recently appearing architectures that will be introduced later in this ar-\nticle. The strong ﬂexibility and high accuracy of deep learning methods guarantee them\nsweeping superiority over other existing methods on these classical tasks.\nThe intersection of deep learning methods and genomic research may lead to a profound\nunderstanding of genomics that will beneﬁt multiple ﬁelds including precision medicine\n(Leung et al., 2016), pharmacy (i.e. drug design), and even agriculture, etc. Take medicine\nfor example, medical research and its applications such as gene therapies, molecular diag-\nnostics, and personalized medicine could be revolutionized by tailoring high-performance\ncomputing methods to analyzing available genomic datasets. Also, the process of developing\nnew drugs takes a long period and is usually very costly. To save time and cost, the general\napproach taken by pharmaceutical companies is to try to match the candidate protein iden-\ntiﬁed by researchers with their known drug molecules. (Mitchell, 2017). As we are facing\nlarger-scale and more complex medical demands, cutting-edge deep learning techniques such\nas large language models show emergent capabilities to eﬃciently and eﬀectively deal with\nunprecedented challenges such as Covid-19 Hammad et al. (2023); Zvyagin et al. (2022).\nAll these beneﬁts indicate the necessity of utilizing powerful and specially designed deep\nlearning methods to foster the development of the genomics industry. This article aims to\noﬀer a concise overview of the current deep learning applications in genomic research, and,\nif possible, point out promising directions for further applying deep learning in the genomic\nstudy.\nThe rest of this article is organized as follows: we ﬁrst brieﬂy introduce the genomic\nstudy powered by deep learning characterized by deep learning architectures in Section 2,\nwith additional discussions oﬀered in Section 3. Then we discuss the use of deep learning\nmethods on various topics in genomics in Section 4, followed by our summarization of the\ncurrent challenges and potential research directions in Section 5. Finally, conclusions are\ndrawn in Section 6.\n4\nDeep Learning for Genomics: A Concise Overview\n2. Deep Learning Architectures: A Genomic Perspective\nVarious deep learning algorithms have their own advantages to resolve particular types\nof problems in genomic applications. For example, CNNs that are famous for capturing\nfeatures in image classiﬁcation tasks have been widely adopted to automatically learn local\nand global characterization of genomic data.\nRNNs that succeed in speech recognition\nproblems are skillful at handling sequence data and thus were mostly used to deal with\nDNA sequences. Autoencoders are popular for both pre-training models and denoising or\npre-processing the input data. When designing deep learning models, researchers could take\nadvantage of these merits to eﬃciently extract reliable features and reasonably model the\nbiological process. This section will review some details on each type of deep architecture,\nfocusing on how each of their advantages can beneﬁt the speciﬁc genomic research questions.\nThis article will not cover the standard introduction of deep learning methods, readers can\nvisit classical textbooks (e.g. Goodfellow et al., 2016) or concise tutorials (e.g. Wang et al.,\n2017d) if necessary.\n2.1 Convolutional Neural Networks\nConvolutional neural networks (CNNs) are one of the most successful deep learning models\nfor image processing owing to their outstanding capacity to analyze spatial information.\nEarly applications of CNNs in genomics relied on the fundamental building blocks of CNNs\nin computer vision (Krizhevsky et al., 2012) to extract features. Zeng et al. (2016) described\nthe adaptation of CNNs from the ﬁeld of computer vision to genomics as accomplished by\ncomprehending a window of genome sequence as an image.\nThe highlight of CNNs is the dexterity of automatically performing adaptive feature ex-\ntraction during the training process. For instance, CNNs can be applied to discover mean-\ningful recurring patterns with small variances, such as genomic sequence motifs. This makes\nCNNs suitable for motif identiﬁcation and therefore binding classiﬁcation (Lanchantin et al.,\n2016a).\nRecently CNNs have been shown to take a lead among current algorithms for solv-\ning several sequence-based problems. Alipanahi et al. (2015, DeepBind) and (Zeng et al.,\n2016) successfully applied CNNs to model the sequence speciﬁcity of protein binding.\nZhou and Troyanskaya (2015, DeepSEA) developed a conventional three-layer CNN model\nto predict from only genomic sequence the eﬀects of non-coding variants. Kelley et al. (2016,\nBasset) adopted a similar architecture to study the functional activities of DNA sequence.\nThough multiple research has demonstrated the superiority of CNNs over other ex-\nisting methods, inappropriate structure design would still result in even poorer perfor-\nmance than conventional models Zeng et al. (2016).\nTherefore, what lies in the center\nfor researchers to master and optimize the ability of CNNs is to skillfully match a CNN\narchitecture to each particular given task.\nTo achieve this, researchers should have an\nin-depth understanding of CNN architectures as well as take into consideration of bio-\nlogical background.\nZeng et al. (2016) developed a parameterized convolutional neural\nnetwork to conduct a systematic exploration of CNNs on two classiﬁcation tasks, mo-\ntif discovery, and motif occupancy. They performed a hyper-parameter search using Mri\n(https://github.com/Mri-monitoring/Mri-docs/blob/master/mriapp.rst) and mainly\nexamined the performance of nine variants of CNNs, and concluded that CNNs are not nec-\n5\nYue et al.\nessary to be deep for motif discovery task as long as the structure is appropriately designed.\nWhen applying CNNs in genomic, since deep learning models are always over-parameterized,\nsimply changing the network depth would not account for much improvement in model\nperformance. On this direction, Xuan et al. (2019) designed a dual CNN with attention\nmechanisms to extract deeper and more complex feature representations of lncRNA (long\nnon-coding RNA genes); while Kelley et al. Kelley et al. (2018); Kelley (2020) took a diﬀer-\nent path in using dilated convolution instead of classical convolution to share information\nacross long distances without adding depth indeﬁnitely.\n2.2 Recurrent Neural Networks\nRecurrent neural networks (RNNs) raised a surge of interest owning to its impressive perfor-\nmance on sequential prediction problems such as language translation, summarization, and\nspeech recognition. RNNs outperform CNNs and other early deep neural networks (DNNs)\non sequential data thanks to their capability of processing long ordered sequences and mem-\norizing long-range information through recurrent loops. Speciﬁcally, RNNs scan the input\nsequences sequentially and feed both the previously hidden layer and current input segment\nas the model input so that the ﬁnal output implicitly integrates both current and previous\ninformation in the sequence. Schuster and Paliwal (1997) later proposed bidirectional RNN\n(BRNN) for use cases where both past and future contexts in the input matter.\nThe cyclic structure makes a seemingly shallow RNN over long-time prediction actually\nvery deep if unrolled in time. To resolve the problem of vanishing gradient rendered by this,\nHochreiter and Schmidhuber (1997) substituted the hidden units in RNNs with LSTM units\nto truncate the gradient propagation. Cho et al. (2014) introduced Gated Recurrent Units\n(GRUs) with a similar proposal.\nGenomics data are typically sequential and often considered languages of biological\nnature. Recurrent models are thus applicable in many scenarios. For example, Cao et al.\n(2017b, ProLanGO) built an LSTM-based Neural Machine Translation, which converts\nthe task of protein function prediction to a language translation problem by interpreting\nprotein sequences as the language of Gene Ontology terms. Boˇza et al. (2017) developed\nDeepNano for base calling, Quang and Xie (2016) proposed DanQ to quantify the function\nof non-coding DNA, Sønderby et al. (2015) devised a convolutional LSTM to predict protein\nsubcellular localization from protein sequences, and Busia et al. (2016) applied the idea of\nseq-to-seq learning to their model for protein secondary structure prediction conditioned\non previously predicted labels. Furthermore, sequence-to-sequence learning for genomics\nis boosted by attention mechanisms: Singh et al. (2017) introduced an attention-based\napproach where a hierarchy of multiple LSTM modules are used to encode input signals and\nmodel how various chromatin marks cooperate; similarly, Shen et al. (2020) used LSTM as\nfeature extractor and attention modules as importance scoring functions to identify regions\nof the RNA sequence that bind to proteins.\n2.3 Autoencoders\nAutoencoders, conventionally used as pre-processing tools to initialize the network weights,\nhave been extended to stacked autoencoders (SAEs; Bengio et al., 2007), denoising au-\ntoencoders (DAs; Vincent et al., 2008), contractive autoencoders (CAEs; Rifai et al., 2011),\n6\nDeep Learning for Genomics: A Concise Overview\netc. Now they have proved successful in feature extraction because of being able to learn\na compact representation of input through the encode-decode procedure.\nFor example,\nGupta et al. (2015) applied stacked denoising autoencoders (SDAs) for gene clustering tasks.\nThey extracted features from data by forcing the learned representation resistant to a partial\ncorruption of the raw input. More examples can be found in Section 4.1.1. Besides, au-\ntoencoders are also used for dimension reduction in gene expression, e.g. Tan et al. (2014,\n2016, 2017). When applying autoencoders, one should be aware that better reconstruc-\ntion accuracy does not necessarily lead to model improvement (Rampasek and Goldenberg,\n2017).\nVariational Autoencoders (VAEs), though named ”autoencoders”, was rather developed\nas an approximate-inference method to model latent variables. Based on the structure of\nautoencoders, Kingma and Welling (2013) added stochasticity to the encoded units and\nadded a penalty term encouraging the latent variables to produce a valid decoding. VAEs\naim to deal with the problems of which each data has a corresponding latent representation\nand are thus useful for genomic data, among which there are complex interdependencies.\nRampasek and Goldenberg (2017) presented a two-step VAE-based model for drug response\nprediction, which ﬁrst predicts the post- from the pre-treatment state in an unsupervised\nmanner, then extends it to the ﬁnal semi-supervised prediction. This model was based on\ndata from Genomics of Drug Sensitivity in Cancer (GDSC; Yang et al., 2013) and Cancer\nCell Line Encyclopedia (CCLE; Barretina et al., 2012). VAEs can also be used in many\nother genomic applications including cancer gene expression prediction Way and Greene\n(2017b,a), single cell feature extraction for unmasking tumor heterogeneity Rashid et al.\n(2021), metagenomic binning Nissen et al. (2021), DNA methylome dataset construction\nChoi and Chae (2020), etc.\n2.4 Emergent Deep Architectures\nAs deep learning constantly showing success in genomics, researchers are expecting deep\nlearning higher accuracy than simply outperforming statistical or machine learning methods.\nTo this end, the vast majority of work nowadays approached genomic problems from more\nadvanced models beyond classic deep architectures, or employing hybrid models. Here we\nreview some examples of recent appearing deep architectures by which skillfully modifying\nor combining classical deep learning models.\n2.4.1 Beyond Classic Models\nMost of these emergent architectures are of natural designs modiﬁed from classic deep learn-\ning models. Researchers began to leverage more genomic intuitions to ﬁt each particular\nproblem with a more advanced and suitable model.\nMotivated by the fact that protein folding is a progressive reﬁnement (Min et al., 2017)\nrather than an instantaneous process, Lena et al. (2012) designed DST-NNs for residue-\nresidue contact prediction. It consists of a 3D stack of neural networks in which topological\nstructures (same input, hidden, and output layer sizes) are identical in each stack. Each level\nof this stacked network can be regarded as a distinct contact predictor and can be trained in a\nsupervised matter to reﬁne the predictions of the previous level, hence addressing the typical\nproblem of vanishing gradients in deep architectures.\nThe spatial features in this deep\n7\nYue et al.\nspatiotemporal architecture refer to the original model inputs, while temporal features are\ngradually altered so as to progress to the upper layers. Angermueller et al. (2017, DeepCpG)\ntook advantage of two CNN sub-models and a fusion module to predict DNA methylation\nstates. The two CNN sub-models take diﬀerent inputs and thus focus on disparate purposes.\nThe CpG module accounts for correlations between CpG sites within and across cells, while\nthe DNA module detects informative sequence patterns (motifs). Then the fusion module\ncan integrate higher-level features derived from two low-level modules to make predictions.\nInstead of subtle modiﬁcations or combinations, some works focused on depth trying to\nimprove the model performance by designing even deeper architectures. Wang et al. (2017e)\ndeveloped an ultra-DNN consisting of two deep residual neural networks to predict protein\ncontacts from a sequence of amino acids.\nEach of the two residual nets in this model\nhas its particular function. A series of 1D convolutional transformations are designed for\nextracting sequential features (e.g., sequence proﬁle, predicted secondary structure, and\nsolvent accessibility). The 1D output is converted to a 2D matrix by an operation similar\nto the outer product and merged with pairwise features (e.g., pairwise contact, co-evolution\ninformation, and distance potential). Then they are together fed into the second residual\nnetwork, which consists of a series of 2D convolutional transformations. The combination\nof these two disparate residual nets makes possible a novel approach to integrate sequential\nfeatures and pairwise features in one model.\n2.4.2 Hybrid Architectures\nThe fact that each type of DNN has its own strength inspires researchers to develop hybrid\narchitectures that could well utilize the potential of multiple deep learning architectures.\nDanQ (Quang and Xie, 2016) is a hybrid convolutional and recurrent DNN for predicting\nthe function of non-coding DNA directly from sequence alone. A DNA sequence is input as\nthe one-hot representation of four bases to a simple convolutional neural network with the\npurpose of scanning motif sites. Motivated by the fact that the motifs can be determined to\nsome extent by the spatial arrangements and frequencies of combinations of DNA sequences\n(Quang and Xie, 2016), the purported motifs learned by CNN are then fed into a BLSTM.\nSimilar convolutional-recurrent designs were further discussed by Lanchantin et al. (2016b,\nDeep GDashboard). They demonstrated how to understand three deep architectures: con-\nvolutional, recurrent, and convolutional-recurrent networks, and veriﬁed the validity of the\nfeatures generated automatically by the model through visualization techniques. They ar-\ngued that a CNN-RNN architecture outperforms CNN or RNN alone based on their experi-\nmental results on a transcription factor binding site (TFBS) classiﬁcation task. The feature\nvisualization achieved by Deep GDashboard indicated that CNN-RNN architecture is able\nto model both motifs as well as dependencies among them. Sønderby et al. (2015) added a\nconvolutional layer between the raw data and LSTM input to address the problem of pro-\ntein sorting or subcellular localization. In total, there are three types of models proposed\nand compared in the paper, a vanilla LSTM, an LSTM with an attention model used in a\nhidden layer, and an ensemble of ten vanilla LSTMs. They achieved higher accuracy than\nprevious benchmark models in predicting the subcellular location of proteins from DNA\nsequences while no human-engineered features were involved. Almagro Armenteros et al.\n(2017) proposed a hybrid integration of RNN, BLSTM, attention mechanism, and a fully\n8\nDeep Learning for Genomics: A Concise Overview\nconnected layer for protein subcellular localization prediction; each of the four modules is\ndesigned for a speciﬁc purpose. These hybrid models are increasingly favored by recent\nresearch, e.g. Singh et al. (2016b).\n2.5 Transformer-based Large Language Models\nAs mentioned in Section 2.1 and 2.2, many prior deep-learning works utilized CNNs and\nRNNs to solve genomics tasks.\nHowever, there are several intrinsic limitations of these\ntwo architectures: (1) CNNs might fail to capture the global understanding of a long DNA\nsequence due to its limited receptive ﬁeld (2) RNNs could have diﬃculty in capturing\nuseful long-term dependencies because of vanishing gradients and suﬀer from low-eﬃciency\nproblem due to its non-parallel sequence processing nature (3) Both architectures need\nextensive labeled data to train. These limitations hinder them from coping with harder\ngenomics problems since these tasks usually require the model to (1) understand long-range\ninteractions (2) process very long sequences eﬃciently (3) perform well even for low-resource\ntraining labels.\nTransformer-based Vaswani et al. (2017) language models such as BERT Devlin et al.\n(2018) and GPT family Radford et al. (2018, 2019); Brown et al. (2020), then become\na natural ﬁt to overcome these limitations.\nTheir built-in attention mechanism learns\nbetter representations that can be generalized to data-scarce tasks via larger receptive\nﬁelds. Benegas et al. (2022) found that a pre-trained large DNA language model is able to\nmake accurate zero-shot predictions of non-coding variant eﬀects. Similarly, according to\nDalla-Torre et al. (2023), these language model architectures generate robust contextual-\nized embeddings on top of nucleotide sequences and achieve accurate molecular phenotype\nprediction even in low-data settings.\nInstead of processing input tokens one by one sequentially as RNNs, transformers process\nall input tokens more eﬃciently at the same time in parallel. However, simply increasing\nthe input context window inﬁnitely is infeasible since the computation time and memory\nscale quadratically with context length in the attention layers. Several improvements have\nbeen made from diﬀerent perspectives: Nguyen et al. (2023) uses the Hyena architecture\nPoli et al. (2023) and scales sub-quadratically in context length, while Zhou et al. (2023)\nreplace k-mer tokenization used in Ji et al. (2021) with Byte Pair Encoding (BPE) to achieve\na 3x eﬃciency improvement.\nIn light of dealing with extremely long-range interactions in DNA sequences, the En-\nformer model Avsec et al. (2021) employs transformer modules that scale a ﬁve times larger\nreceptive ﬁeld compared to previous CNN-based approaches Zhou et al. (2018); Kelley et al.\n(2018); Kelley (2020), and is capable of detecting sequence elements that are 100 kb away.\nMoreover, the recent success of ChatGPT Schulman et al. (2022) and GPT-4 OpenAI (2023)\nfurther illustrated the emergent capabilities of large language models (LLMs) to deal with\nsuch long DNA sequences. A typical transformer-based genomics foundational model can\nonly take 512 to 4k tokens as input context, which is less than 0.001% of the human genome.\nNguyen et al. (2023) proposed an LLM-based genomic model that expands the input con-\ntext length to 1 million tokens at the single nucleotide level, which is up to 500x increase\nover previous dense attention-based models.\n9\nYue et al.\n3. Deep Learning Architectures: Insights and Remarks\nApplications of deep learning in genomic problems have fully proven their power. Although\nthe pragmatism of deep learning is surprisingly successful, this method suﬀers from lacking\nthe physical transparency to be well interpreted so as to better assist the understanding of\ngenomic problems. What is auspicious in genomic research is that researchers have done lots\nof work to visualize and interpret their deep learning models. Besides, it is also constructive\nto take into additional considerations beyond the choice of deep learning architectures. In\nthis section, we review some visualization techniques that bring about insights into deep\nlearning architectures and add remarks on model design that might be conducive to real-\nworld applications.\n3.1 Model Interpretation\nPeople expect deep networks to succeed not only in predicting results but also in iden-\ntifying meaningful DNA sequence signals and giving further insights into the problems\nbeing solved. The interpretability of a model appears to be crucial when it comes to ap-\nplication. However, the technology of deep learning has exploded not only in prediction\naccuracy but also in complexity as well. Connections among network units so are convo-\nluted that the information is widespread throughout the network and thus perplexing to be\ncaptured (Castelvecchi, 2016). People are carrying out eﬀorts to remedy this pitfall since\nprediction accuracy alone does not guarantee that deep architectures are a better choice\nover traditional statistical or machine learning methods in applications. Diﬀerent visualiza-\ntions techniques are also being actively developed Min et al. (2016); Singh et al. (2016a);\nMikolov et al. (2013b); Sønderby et al. (2015); Riesselman et al. (2017).\nThe image classiﬁcation ﬁeld is where people started deciphering deep networks. Zeiler and Fergus\n(2014) gave insights into the function of intermediate features by mapping hidden layers back\nto the input through deconvolution, a technique described in that paper. Simonyan et al.\n(2013) linearly approximate the network by ﬁrst-order Taylor expansion and obtained\nSaliency Maps from a ConvNet by projecting back from the dense layers of the network. Peo-\nple also searched for an understanding of genes through deep networks. Denas and Taylor\n(2013) managed to pass the model knowledge back into the input space through the in-\nverse of the activation function, so that biologically-meaningful patterns can be highlighted.\nLanchantin et al. (2016b, Dashboard) adopted Saliency Maps to measure nucleotide impor-\ntance. Their work provided a series of visualization techniques to detect motifs, or sequence\npatterns from deep learning models, and went further to discuss the features extracted by\nCNNs and RNNs. Similarly, Alipanahi et al. (2015) visualized the sequence speciﬁcities\ndetermined by DeepBind through mutation maps that indicate the eﬀect of variations on\nbound sequences. Note that works conducted appropriately by classic models do not need\nadditional techniques to visualize features, e.g. P¨arnamaa and Parts (2017) trained an 11-\nlayer CNN for prediction protein subcellular localization from microscopy images, and easily\ninterpreted their model by features at diﬀerent layers.\nThe rise of attention-based models also opened up new avenues for interpretability in\ngenomics.\nSingh et al. (2017) argued that attention scores provide a better interpreta-\ntion than traditional feature visualization methods such as Saliency maps. According to\nChen et al. (2019), the visualization of attention weight changes can be used to understand\n10\nDeep Learning for Genomics: A Concise Overview\nwhen binding signal peaks move along the genomic sequence. Ghotra et al. (2021) further\nemphasized the importance of the convolutional layer(s) learning identiﬁable motifs for the\nattention maps to be interpretable.\n3.2 Transfer Learning and Multitask Learning\nThe concept of transfer learning is naturally motivated by human intelligence that peo-\nple can apply the knowledge that has already been acquired to address newly encountered\nproblems. Transfer learning is such a framework that allows deep learning to adapt the\npreviously-trained model to exploit a new but relevant problem more eﬀectively (Lu et al.,\n2015). It has been successfully applied to other ﬁelds, such as language processing (Cire¸san et al.,\n2012) or audio-visual recognition (Moon et al., 2014). Readers could ﬁnd surveys on trans-\nfer learning by Pan and Yang (2010) or Weiss et al. (2016). Additionally, multitask learning\nis an approach that inductively shares knowledge among multiple tasks. By learning related\ntasks in parallel while using shared architectures, what is learned by a single task can be\nauxiliary to those related. An overview of multitask learning, which especially focuses on\nneural networks, can be found in Ruder (2017). Widmer and R¨atsch (2012) brieﬂy discussed\nmultitask learning from a biological perspective.\nEarly adaptation of transfer learning in genomics was based on machine learning meth-\nods such as SVMs (Schweikert et al., 2009; Mei, 2013; Xu and Yang, 2011). Recent works\nhave also involved deep learning. For example, Zhang et al. (2016) developed a CNN model\nto analyze gene expression images for automatic controlled vocabulary (CV) term annota-\ntion. They pre-trained their model on ImageNet (http://www.image-net.org/) to extract\ngeneral features at diﬀerent scales, then ﬁne-tuned the model by multitask learning to cap-\nture CV term-speciﬁc discriminative information. Liu et al. (2016a) developed an iterative\nPEDLA to predict enhancers across multiple human cells and tissues. They ﬁrst pre-trained\nPEDLA on data derived from any cell type/tissue an unsupervised manner, then iteratively\nﬁne-tuned the model on a subsequent cell type/tissue supervisedly, using the trained model\nof the previous cell type/tissue as initialization. Cohn et al. (2018) transferred deep CNN\nparameters between networks trained on diﬀerent species/datasets for enhancer identiﬁca-\ntion. Qin and Feng (2017, TFImpute) adopted a CNN-based multi-task learning setting to\nborrow information across TFs and cell lines to predict cell-speciﬁc TF binding for TF-cell\nline combinations from only a small portion of available ChIP-seq data. They were able\nto predict TFs in new cell types by models trained unsupervisedly on TFs where ChIP-seq\ndata are available, which took the right step in the direction of developing a domain transfer\nmodel across cell types. Qi et al. (2010) proposed a semi-supervised multi-task framework\nfor protein-protein interaction (PPI) predictions. They applied the MLP classiﬁer trained\nsupervisedly to perform an auxiliary task that leverages partially labeled examples. The\nloss of the auxiliary task is added to MLP loss so that the two tasks can be jointly optimized.\nWang et al. (2017b) worked on the same problem by introducing a multi-task convolutional\nnetwork model for representation learning. Zhou and Troyanskaya (2015) incorporated mul-\ntitask approach for noncoding-variant eﬀects prediction on chromatin by jointly learning\nacross diverse chromatin factors.\nShao et al. (2020) proposed a task relationship learn-\ning framework to automatically investigate the inherent correlation between diagnosis and\nprognosis genomics tasks, while DeepND Beyreli et al. (2022) claimed to learn the shared\n11\nYue et al.\nand disorder-speciﬁc features using multitask learning setting where several tasks are solved\ntogether.\n3.3 Multi-view Learning\nAs the current technology has made available data from multi-platform or multi-view inputs\nwith heterogeneous feature sets, multi-view deep learning appears to be an encouraging di-\nrection for future deep learning research which exploits information across the datasets, cap-\nturing their high-level associations for prediction, clustering as well as handling incomplete\ndata. Readers can visit Li et al. (2016c) for a survey on multi-view methods if interested.\nIn many applications, we can approach the same problem from diﬀerent types of data, such\nas in computer visions when audio and video data are both available (Kidron et al., 2005;\nWang et al., 2017c). Genomics is an area where data of various types can be assimilated\nnaturally. For example, abundant types of genomic data (e.g., DNA methylation, gene ex-\npression, miRNA expression data) for the same set of tumor samples have been made avail-\nable by the state-of-the-art high-throughput sequencing technologies (Liang et al., 2015).\nTherefore, it is natural to think of leveraging multi-view information in genomics to achieve\na better prediction than that of a single view. Gligorijevi´c and Prˇzulj (2015) and Li et al.\n(2016b) reviewed some methods for multi-view biological data integration with instructive\nconsiderations.\nMulti-view learning can be achieved by, for example, concatenating features, ensem-\nble methods, or multi-modal learning (selecting speciﬁc deep networks, as sub-networks\nof the main model, for each view, then integrating them in higher layers), just to name\na few.\nThe previously mentioned ultra-DNN (Wang et al., 2017e) is a case in point,\nwhere it adopted 1D and 2D CNNs respectively for sequential features and spatial fea-\ntures. Liang et al. (2015) proposed a multi-modal DBN to integrate gene expression, DNA\nmethylation, miRNA, and drug response data to cluster cancer patients and deﬁne cancer\nsubtypes. Their stacked Gaussian-restricted Boltzmann machines (gRBM) are trained by\ncontrastive divergence, diﬀerent modalities are integrated via stacking hidden layers, and\ncommon features are eﬀused from inherent features derived from multiple single modalities.\nOn this direction, Zhang et al. (2015) utilized a multimodal DBN framework to integrate\nRNA primary sequence, predicted secondary, and tertiary structures, while a later work\nPan and Shen (2017) went a step further with a hybrid multimodal framework combining\nCNNs and DBNs to predict RNA-binding protein interaction sites and motifs on RNAs\nusing 5 diﬀerent modalities. Additionally, instead of dealing with diﬀerent modalities sepa-\nrately, some research started to explore multi-modal interactions: for example, Shao et al.\n(2020) performed integrative analysis on histopathological image and genomic data for can-\ncer diagnosis and prognosis, while Wang et al. (2021) explored both intra-modality and\ninter-modality feature modules for genomic data and pathological images.\n4. Genomic Applications\nIn this section, we review several aspects of genomic problems that can be approached from\ndeep learning methods and discuss how deep learning moves forward in these ﬁelds.\n12\nDeep Learning for Genomics: A Concise Overview\n4.1 Gene expression\nGene expression is a highly regulated process by which the genetic instructions in DNA are\nconverted into functional products such as proteins and other molecules, and also respond\nto the changing environment accordingly. Namely, genes encode protein synthesis, and self-\nregulate the functions of the cell by adjusting the amount and type of proteins it produces\n(Nature, 2010). Here we review some research that applied deep learning to analyze how\ngene expression is regulated.\n4.1.1 Gene expression Characterization\nAn increasing number of genome-wide gene expression assays for diﬀerent species have be-\ncome available in public databases, e.g. the Connectivity Map (CMap) project was launched\nto create a reference collection of gene expression proﬁles that can be used to identify func-\ntionally connected molecules (Lamb et al., 2006), these databases greatly facilitated the\ncomputational models for biological interpretation of these data. At the same time, recent\nworks have suggested better performance obtained by deep learning models on gene ex-\npression data; Urda et al. (2017) used a deep learning approach to outperform LASSO in\nanalyzing RNA-Seq gene expression proﬁle data.\nThe empirical results of early works that applied principal component analysis (PCA)\non gene expression data to capture cluster structure have shown that this mathemat-\nical tool was not eﬀective enough to allow some complicated biological considerations\n(Yeung and Ruzzo, 2001).\nAlso, since the reliability of the cross-experiment datasets is\nlimited by technical noise and unmatched experiment conditions (Tan et al., 2017), re-\nsearchers are considering the denoising and enhancement of the available data instead of\ndirectly ﬁnding principal components.\nDenoising autoencoders came in handy since they do not merely retain the information\nof raw data, but also generalize meaningful and important properties of the input distribu-\ntion across all input samples. Even shallow denoising autoencoders can be proven eﬀective\nin extracting biological insights. Danaee et al. (2017) adopted SDAs to detect functional\nfeatures in breast cancer from gene expression proﬁle data.\nTan et al. (2014, ADAGE)\npresented an unsupervised approach that eﬀectively applied SDA to capture key biological\nprinciples in breast cancer data.\nADAGE is an open-source project for extracting rele-\nvant patterns from large-scale gene expression datasets. Tan et al. (2016) further improved\nADAGE to successfully extract both clinical and molecular features. To build better sig-\nnatures that are more consistent with biological pathways and enhance model robustness,\nTan et al. (2017) developed an ensemble ADAGE (eADAGE) to integrate stable signatures\nacross models. These three similar works were all experimented on Pseudomonas aerugi-\nnosa gene expression data. Additionally, Gupta et al. (2015) demonstrated the eﬃcacy of\nusing the enhanced data by multi-layer denoising autoencoders to cluster yeast expression\nmicroarrays into known modules representing cell cycle processes. Motivated by the hi-\nerarchical organization of yeast transcriptomic machinery, Chen et al. (2016b) adopted a\nfour-layered autoencoder network with each layer accounting for a speciﬁc biological pro-\ncess in gene expression. This work also introduced sparsity into autoencoders. Edges of\ndenoising autoencoders over PCA and independent component analysis (ICA) were clearly\nillustrated in the aforementioned works.\n13\nYue et al.\nSome other works moved to variational inference in autoencoders, which is assumed\nto be more skillful to capture the internal dependencies among data.\nWay and Greene\n(2017a) trained VAE-based models to reveal the underlying patterns in the pathways of gene\nexpression, and compared their three VAE architectures to other dimensionality reduction\ntechniques, including the aforementioned ADAGE (Tan et al., 2014). Dincer et al. (2018)\nintroduced the DeepProﬁle, a framework featuring VAE, to extract latent variables that are\npredictive for acute myeloid leukemia from expression data. Shariﬁ-Noghabi et al. (2018)\nproposed Deep Genomic Signature (DGS), a pair of VAEs that are trained over unlabelled\nand labeled data separately from expression data for predicting metastasis.\nAnother thread for utilizing deep learning to characterize gene expression is to describe\nthe pairwise relationship. Wang et al. (2017b) showed that CNN can be seen as an eﬀective\nreplacement for the frequently used Pearson correlation applied to pair of genes, therefore\nthey built a multi-task CNN that can consider the information of GO semantics and in-\nteraction between genes together to extract higher level representations of gene pairs for\nthe further classiﬁcation task, which is further extended by two shared-parameter networks\n(Cao et al., 2017a).\nRecently LLMs have come into play for such pairwise relationship:\nCui et al. (2023) introduced a GPT-based foundational model and found a positive pair-\nwise correlation between the similarity of the gene embeddings and the number of common\npathways shared by these genes; similarly, Yang et al. (2022) utilized the attention weights\nin transformers to reﬂect the contribution of each gene and the interaction of gene pairs.\n4.1.2 Gene expression Prediction\nDeep learning approaches for gene expression prediction have outperformed other existing\nalgorithms. For example, Chen et al. (2016c) presented a three-layer feed-forward neural\nnetwork for gene expression prediction of selected landmark genes that achieved better\nperformance than linear regression. This model, D-GEX, is of the multi-task setting and\nwas tested on two types of expression data, the microarrays and RNA-Seqs.\nXie et al.\n(2017) showed that their deep model based on MLP and SDAs outperformed Lasso and\nRandom Forests in predicting gene expression quantiﬁcations from SNP genotypes.\nWhen making predictions from gene sequences, deep learning models have been shown\nfruitful in identifying the context-speciﬁc roles of local DNA-sequence elements, then the fur-\nther inferred regulatory rules can be used to predict expression patterns (Beer and Tavazoie,\n2004). Successful prediction usually relies much on the proper utilization of biological knowl-\nedge. Therefore, it could be more eﬃcient to pre-analyze the contextual information in DNA\nsequences than directly making predictions. Deep learning models could refer to two early\nmachine learning works that apply Bayesian networks to predict gene expression based on\ntheir learned motifs (Beer and Tavazoie, 2004; Yuan et al., 2007).\nIn most applications, the powerful of deep learning algorithms is paled by biological\nrestrictions.\nTherefore, instead of only using sequence information, combing epigenetic\ndata into the model might add to the explanatory power of the model. For example, the\ncorrelation between histone modiﬁcations and gene regulation was suggested experimen-\ntally in Lim et al. (2009), Cain et al. (2011) and Dong and Weng (2013), and has already\nbeen studied in some machine learning works before (Karli´c et al., 2010; Cheng et al., 2011;\nDong et al., 2012; Ho et al., 2015). Singh et al. (2016a) presented DeepChrome, a uniﬁed\n14\nDeep Learning for Genomics: A Concise Overview\ndiscriminative framework stacking an MLP on top of a CNN, and achieved an average\nAUC of 0.8 in binary classiﬁcation task that predicts high or low gene expression level.\nThe input was separated into bins so as to discover the combinatorial interactions among\ndiﬀerent histone modiﬁcation signals. The learned region representation is then fed into\nan MLP classiﬁer that maps to gene expression levels. Additionally, Singh et al. (2016a,\nDeepChrome) visualized high-order combinatorial to make the model interpretable. Other\nexamples of epigenetic information that can be utilized in gene expression prediction tasks\ninclude DNA methylation, miRNA, chromatin features, etc.\nGenerative models were also adopted due to the ability to capture high-order, latent cor-\nrelations. For example, to explore hypothetical gene expression proﬁles under various types\nof molecular and genetic perturbation, Way and Greene (2017b) trained a VAE on The\nCancer Genome Atlas (TCGA; Weinstein et al., 2013) pan-cancer RNA-seq data to cap-\nture biologically-relevant features. They have another previous work that evaluates VAEs\nof diﬀerent architectures, provided with comparison among VAEs, PCA, ICA, non-negative\nmatrix factorization (NMF), and aforementioned ADGAE (Way and Greene, 2017a). Hav-\ning the emergent capability to integrate long-range interactions in the genome, Generative\nLanguage Models such as Avsec et al. (2021) also claimed to improve gene expression pre-\ndiction accuracy from DNA sequences, leading to more accurate variant eﬀect predictions\non gene expression for both natural genetic variants and saturation mutagenesis measured\nby massively parallel reporter assays.\n4.2 Regulatory Genomics\nGene expression regulation is the cellular process that controls the expression level of gene\nproducts (RNA or protein) to be high or low. It increases the versatility of an organism\nso as to allow it to react towards and adapt to the surrounding environment. The under-\nlying interdependencies behind the sequences limit the ﬂexibility of conventional methods,\nbut deep networks that could model over-representation of sequence information have the\npotential to allow regulatory motifs to be identiﬁed according to their target sequences.\n4.2.1 Promoters and Enhancers\nThe most eﬃcient way of gene expression regulation for an organism is at the transcriptional\nlevel, which occurs at the early stage of gene regulation. Enhancers and promoters are two of\nthe most well-characterized types of functional elements in the regions of non-coding DNA,\nwhich belong to cis-regulatory elements (CREs). Readers can visit Wasserman and Sandelin\n(2004) and Li et al. (2015a) for a review of early approaches for the identiﬁcation of CREs.\nPromoters locate near the transcription start sites of genes and thereby initiate the\ntranscription of particular genes.\nConventional algorithms still perform poorly on pro-\nmoter prediction, while the prediction is always accompanied by a high false positive rate\n(Fickett and Hatzigeorgiou, 1997).\nThe compensation for sensitivity is usually achieved\nat the cost of speciﬁcity and renders the methods not accurate enough for applications.\nOne initial work by Horton and Kanehisa (1992) applied neural networks to predict E. coli\npromoter sites and provided a comparison of neural networks versus statistical methods.\nMatis et al. (1996) also applied neural networks to promoter recognition, albeit assisted\nwith some rules which use the gene context information predicted by GRAIL. These early\n15\nYue et al.\nworks of deep learning models were not noticeable enough to demonstrate a clear edge over\nthe weight matrix matching methods. One recent study by Umarov and Solovyev (2017)\nused a CNN with no more than three layers well demonstrated the superiority of CNN over\nconventional methods in promoter recognition of ﬁve distant organism.Their trained model\nhas been implemented as a web application called CNNProm. A more latest CNN-based\nmodel for enhancer prediction applied transfer learning setting on diﬀerent species/datasets\n(Cohn et al., 2018). Another highlight of their work lies in the design of adversarial training\ndata.\nPEDLA was developed by Liu et al. (2016a) as an algorithmic framework for enhancer\nprediction based on deep learning.\nIt is able to directly learn from heterogeneous and\nclass-imbalanced data an enhancer predictor that can be generalized across multiple cell\ntypes/tissues. The model has an embedded mechanism to handle class-imbalanced problems\nin which the prior probability of each class is directly approximated from the training data.\nPEDLA was ﬁrst trained on 9 types of data in H1 cells, then further extended with an\niterative scheme that manages to generalize the predictor across various cell types/tissues.\nPEDLA was also compared with and outperformed some of the most typical methods for\npredicting enhancers.\nMin et al. (2016, DeepEnhancer) adopted CNNs that surpass previous sequence-based\nSVM methods on the task of identifying enhancers from background genomic sequences.\nThey compared diﬀerent designs of CNNs and concluded the eﬀectiveness of max-pooling\nand batch normalization for improving classiﬁcation accuracy, while they also pointed out\nthat simply increasing the depth of deep architectures is not useful if being inappropriately\ndesigned. Their ﬁnal model has been ﬁne-tuned on ENCODE cell type-speciﬁc enhancer\ndatasets from the model trained on the FANTOM5 permissive enhancer dataset by applying\ntransfer learning.\nYang et al. (2017) showed the possibility of predicting enhancers with DNA sequence\nalone with the presentation of BiRen, a hybrid of CNN and RNN. While demonstrating the\npossibility, there seems to be room to improve BiRen with the techniques that enable deep\nlearning over heterogeneity data (e.g. see Section 5.1.3) since BiRen still exhibits weaker\npredictive performance in comparison to the methods that consider the cell-type-/tissue-\nspeciﬁc enhancer markers explicitly.\nDeep Feature Selection (DFS) is an attempt taken by Li et al. (2015b) to introduce\nsparsity to deep architectures.\nConventionally, the sparseness is achieved by adding a\nregularization term (e.g., LASSO, Elastic Net). Li et al. (2015b) took a novel approach\nby which they can automatically select an active subset of features at the input level to\nreduce the feature dimension. This is implemented as an additional sparse one-to-one (point-\nwise product) linear layer between the input data and the input layer of the main model.\nDFS is widely applicable to diﬀerent deep architectures.\nFor example, Li et al. (2015b)\ndemonstrated MLP-based DFS (shallow DFS), DNNs based DFS (Deep DFS), and pointed\nout that when back-propagation does not perform well for deep networks, people can resort\nto stacked contractive autoencoder (ScA) and DBN based DFS models that pre-trained\nlayer-wisely in a greedy way before ﬁne-tuned by back-propagation. The author developed\nan open-source package of DFS and illustrated the superiority of DFS over Elastic Net and\nRandom Forest in the identiﬁcation of enhancers and promoters. Li et al. (2016a) further\n16\nDeep Learning for Genomics: A Concise Overview\nimplemented a supervised deep learning package named DECRES, a feed-forward neural\nnetwork based on DFS, for genome-wide detection of regulatory regions.\nEnhancer-promoter interaction predictions are always based on non-sequence features\nfrom functional genomic signals. Singh et al. (2016b, SPEID) proposed the ﬁrst deep learn-\ning approach to infer enhancer-promoter interactions genome-wide from only sequence-\nbased features, as well as the locations of putative enhancers and promoters in a speciﬁc\ncell type. Their model was demonstrated to be superior to DeepFinder, which is based on\nmachine learning (Whalen et al., 2016). This hybrid model consists of two parts. The ﬁrst\npart accounts for the diﬀerences of underlying features that could be learned between en-\nhancers and promoters and thus treats enhancers and promoters separately at input by two\nbranches, where each branch is a one-layer CNN followed by a rectiﬁed linear unit (ReLU)\nactivation layer. The second part is an LSTM that is responsible for identifying informative\ncombinations of the extracted subsequence features. Their work provided insights into the\nlong-range gene regulation determined from the sequences.\nLLMs has made signiﬁcant progress in promoter and enhancer-related prediction tasks.\nJi et al. (2021) utilized a BERT architecture to eﬀectively predict proximal and core pro-\nmoter regions and the successor of this work Zhou et al. (2023) claimed to achieve optimal\nperformance in the Core Promoter Detection task.\nDalla-Torre et al. (2023) performed\ncomprehensive benchmarking on 17 datasets including predicting regulatory elements for\nenhancers and promoters for several transformer models.\n4.2.2 Splicing\nSplicing refers to the editing of pre-messenger RNA so as to produce a mature messenger\nRNA (mRNA) that can be translated into a protein. This process eﬀectively add up to the\ndiversity of protein isoforms. Predicting ”splicing code” aims to understand how splicing\nregulates and manifests the functional changes of proteins, and is crucial for understanding\ndiﬀerent ways of how proteins are produced.\nInitial machine learning attempts included naive Bayes model (Barash et al., 2010)\nand two-layer Bayesian neural network (Xiong et al., 2011) that utilized over a thousand\nsequence-based features.\nEarly applications of neural networks in regulatory genomics\nsimply replaced a classical machine learning approach with a deep model. For example,\nXiong et al. (2015) adopted a fully connected feed-forward neural network trained on exon-\nskipping events in the genome that can predict splicing regulation for any mRNA sequence.\nThey applied their model to analyze more than half a million mRNA splicing codes for\nthe human genome, and discovered many new disease-causing candidates while thousands\nof known disease-causing mutations were successfully identiﬁed. This is a case where high\nperformance mainly results from a proper data source rather than a descriptive model\ndesign. Lee and Yoon (2015) presented a DBN-based approach that is capable of dealing\nwith class-imbalanced data to predict splice sites while also identifying non-Canonical splice\nsites. They also proposed a new training method called boosted contrastive divergence with\ncategorical gradients and showed by their experiments its ability to improve prediction per-\nformance and shorten runtime compared to contrastive divergence or other methods.\nGao et al. (2021) developed an approach based on CNN and uses sequence signatures to\nidentify gene targets of a therapeutic for human splicing disorders.\n17\nYue et al.\nIn many cases happens the phenomenon of alternative splicing.\nThat is, a single\ngene might end up coding for multiple unique proteins by varying the exon composi-\ntion of the same mRNA during the splicing process.\nThis is a key post-transcriptional\nregulatory mechanism that aﬀects gene expression and contributes to proteomic diversity\n(Juan-Mateu et al., 2016). Leung et al. (2014) developed a DNN model containing three\nhidden layers to predict alternative splicing patterns in individual tissues, as well as across-\ntissue diﬀerences.The hidden variables of the model are designed to include cellular context\n(tissue types) information to extract genomic features. This is one of the initial works that\nadapt deep learning for splicing prediction. Work by Jha et al. (2017) based on previously\ndeveloped BNN (Xiong et al., 2011) and DNN (Leung et al., 2014) models to design an\nintegrative deep learning model for alternative splicing. They viewed previous work as the\nbaseline on their original dataset, and further developed these models by integrating addi-\ntional types of experimental data (e.g. tissue type) and proposed a new target function.\nTheir models are able to identify splicing regulators and their putative targets, as well as\ninfer the corresponding regulatory rules directly from the genomic sequence.\n4.2.3 Transcription Factors and RNA-binding Proteins\nTranscription factors (TFs) refer to proteins that bind to promoters and enhancers on DNA\nsequence, and RNA-binding proteins, as the name suggested, are both crucial regulatory\nelements in biological processes. Current high-throughput sequencing techniques for select-\ning candidate binding targets for certain TFs are restricted by the low eﬃciency and high\ncost Ching et al. (2017).\nResearchers seeking computational approaches for TF binding\nsites prediction on DNA sequences initially utilized consensus sequences or its alternative,\nposition weight matrices (Stormo, 2000). Later machine learning methods SVM using k-\nmer features (Ghandi et al., 2014), (Setty and Leslie, 2015) surpassed previous generative\nmodels.\nMany existing deep learning methods approach TFBS prediction tasks through con-\nvolutional kernels.\nAlipanahi et al. (2015, DeepBind) have shown successful using CNN\nmodels in large-scale problems of TFBS tasks.\nChen et al. (2017) combined the advan-\ntage of representation learning from CNN and the explicity of reproducing kernel Hilbert\nspace to introduce the Convolutional Kernel Networks to predict TFBS with interpretabil-\nity. Zeng et al. (2016) conducted a systematic analysis of CNN architectures for predicting\nDNA sequence binding sites based on large TF datasets. Lanchantin et al. (2016b) further\nexplored CNNs, RNNs, and the combination of the two in the task of TFBS with compre-\nhensive discussion and visualization techniques. Admittedly that CNNs can well capture\nmost sequential and spatial features in DNA sequences, but recurrent networks as well as\nbidirectional recurrent networks are useful when accounting for motifs in both directions of\nthe sequence. Motivated by the symmetry of double-stranded DNA, which means that iden-\ntical patterns may appear on one DNA strand and its reverse complement, Shrikumar et al.\n(2017) proposed a traditional convolution-based model which shares parameters of forward\nand reverse-complement versions of the same DNA sequences, and have shown robust on in\nvivo TFBS prediction tasks using chromatin ChIP-seq data. This is a novel work that tailors\nconventional neural networks to consider motifs through bidirectional characterizations.\n18\nDeep Learning for Genomics: A Concise Overview\nIn addition to convolutional neural networks, which proved powerful as long as being\nappropriately designed according to the speciﬁc problem, some other approaches deal with\ndiﬀerent feature extraction or multiple data sources. Cross-source data usually share com-\nmon knowledge at a higher abstraction level beyond the basic observation and thus need to\nbe further integrated by the model. Zhang et al. (2015) proposed a multi-modal deep belief\nnetwork that is capable of automatic extraction of structural features from RNA sequences;\nthey ﬁrst successfully introduce tertiary structural features of RNA sequences to improve\nthe prediction of RNA-binding proteins interaction sites. Another multi-modal deep learn-\ning model for the same purpose was developed by Pan and Shen (2017, iDeep). This model\nconsists of DBNs and CNNs to integrate lower-level representations extracted from diﬀerent\ndata sources. Cao and Zhang (2017, gkm-DNN) based on gapped k-mers frequency vectors\n(gkm-fvs) to extract informative features. The gkm-fvs after normalization are taken as\ninput for a multi-layer perceptron model trained by the standard error back-propagation\nalgorithm and mini-batch stochastic gradient descent. By taking advantage of both gapped\nk-mer methods and deep learning, gkm-DNN achieved overall better performance compared\nwith gkm-SVM. Qin and Feng (2017, TFImpute) proposed a CNN-based model that utilizes\ndomain adaptation methods, which are discussed in more detail in Section 3.2, to predict\nTFs in new cell types by models trained unsupervisedly on TFs where ChIP-seq data are\navailable. Ji et al. (2021) and Zhou et al. (2023) ﬁne-tuned BERT-based models to more\naccurately predict TFBSs, on both human and mouse genomic tracks.\n4.3 Functional Genomics\n4.3.1 Mutations and Functional Activities\nOne of the shortcomings of previous approaches for predicting the functional activities from\nDNA sequences is the insuﬃcient utilization of positional information. Though Ghandi et al.\n(2014) upgraded the k-mer method by introducing an alternative gapped k-mers method\n(gkm-SVM), the improvement is not remarkable since the DNA sequence is still simply\nrepresented as vectors of k-mer counts without considering the position of each segment in\nthe sequence. Though position-speciﬁc sequence kernels exist, they map the sequence into\nmuch higher dimension space and are thus not eﬃcient enough (Kelley et al., 2016).\nIn contrast to conventional methods, deep learning methods such as CNNs naturally\naccount for positional relationships between sequence signals and are computationally ef-\nﬁcient. Kelley et al. (2016, Basset) presented an open-source CNN-based package trained\non genomics data of 164 cell types, and remarkably improved the prediction for functional\nactivities of DNA sequences. Basset enables researchers to perform the single sequencing\nassay and annotate mutations in the genome with present chromatin accessibility learned at\nthe same time. Zhou and Troyanskaya (2015, DeepSEA) contributed another open-source\ndeep convolutional network for predicting from only genomic sequence the functional roles\nof non-coding variants on histone modiﬁcations, TFBS, and DNA accessibility of sequences\nwith high nucleotide resolution. Since CNN-based methods might require a certain amount\nof supervised training data, Benegas et al. (2022) utilized pre-trained DNA language mod-\nels to perform zero-shot non-coding variant eﬀects prediction and the results outperformed\nprevious approaches where vast amounts of functional genomics data are required for train-\ning.\n19\nYue et al.\nThe eﬀects of mutations are usually predicted by site-independent or pairwise models,\nbut these approaches do not suﬃciently model higher-order dependencies. Riesselman et al.\n(2017, DeepSequence) took a generative approach to tract mutation eﬀects that are beyond\npairwise by biologically-motivated Beyasian deep latent networks. They introduced latent\nvariables on which DNA depends and visualized model parameters to illustrate the struc-\ntural proximity and amino acid correlations captured by DeepSequence.\n4.3.2 Subcellular Localization\nSubcellular localization is to predict the subcellular compartment in a protein that resides\nin the cell from its biological sequence. In order to interact with each other, proteins need\nto at least temporarily inhabit physically adjacent compartments, therefore, the knowl-\nedge of protein location sheds light on where a protein might function as well as what\nother proteins it might interact with (Shatkay et al., 2007). Most previous methods rely\non SVMs and involve hand-generated features. For example, Shatkay et al. (2007, Sher-\nLoc) integrated diﬀerent sequence and text-based features, and Pierleoni et al. (2006, Ba-\nCelLo) developed a hierarchy of binary SVMs. Meinken et al. (2012) reported on previous\ntools and Wan and Mak (2015) covered the machine learning approaches for subcellular\nlocalization.\nM Some early deep learning works have shifted from SVMs to neural net-\nworks, such as Emanuelsson et al. (2000) and Hawkins and Bod´en (2006). Mooney et al.\n(2011) based on an N-to-1 neural network to develop a subcellular localization predictor\n(SCLpred). Sønderby et al. (2015) adopted LSTM to predict protein subcellular locations\nfrom only sequence information with high accuracy.\nThey further enhanced the model\nby adding convolutional ﬁlters before LSTM as a motif extractor and introducing the at-\ntention mechanism that forces the LSTM to focus on particular segments of the protein.\nThe validity of their convolutional ﬁlters and attention mechanisms were visualized in ex-\nperiments. Almagro Armenteros et al. (2017) proposed a similar integrative hybrid model\nDeepLoc consisting of four modules, including CNN, BLSTM, attention scheme and q fully\nconnected dense layer. Kobayashi et al. (2022) utilized vector quantized VAE architecture\nto encode high-resolution features of protein subcellular localization without the need for\nprior knowledge, categories or annotations.\nHigh-throughput microscopy images are a rich source of biological data that remains to\nbe better exploited. One of the important utilization of microscopy images is the automatic\ndetection of the cellular compartment.\nP¨arnamaa and Parts (2017, DeepYeast) devised\nan eleven-layer deep model for ﬂuorescent protein subcellular localization classiﬁcation in\nyeast cells, of which eight convolutional layers are succeeded by three fully connected layers.\nInternal outputs of the model are visualized and interpreted from the perspective of image\ncharacteristics. The author concluded that the low-level network functions as a basic image\nfeature extractor, while higher layers account for separating localization classes.\n4.4 Structural Genomics\n4.4.1 Structural Classification of Proteins\nProteins usually share structural similarities with other proteins, some of which have a\ncommon evolutionary origin (Lo Conte et al., 2000).\nClassiﬁcation of protein structure\ncan be traced back to the 1970s aiming to comprehend the process of protein folding and\n20\nDeep Learning for Genomics: A Concise Overview\nprotein structure evolution (Andreeva and Murzin, 2010). Grouping proteins into structural\nor functional categories also facilitates the understanding of an increasing number of the\nnewly sequenced genome.\nEarly methods for similarity measures mostly rely on sequence properties (i.e. alignment-\nbased), such as FASTA (Pearson and Lipman, 1988), BLAST (Altschul et al., 1990), or\nPSI-BLAST (Altschul et al., 1997), and were then upgraded by leveraging proﬁles derived\nfrom multiple sequence alignments and position-speciﬁc scoring matrices (PSSM) in addi-\ntion to raw sequences (Rangwala and Karypis, 2005), or discriminative models like SVM\n(Liao and Noble, 2003). For example, Cang et al. (2015) adopted SVM with a topological\napproach utilizing persistent homology to extract features for the classiﬁcation of protein\ndomains and superfamilies. Other top-performing deep learning works also rely on pro-\ntein homology detection (one can visit Chen et al. (2016a) for a review) to deduce the 3D\nstructure or function of a protein from its amino acid sequence. Hochreiter et al. (2007)\nsuggested a model-based approach that uses LSTM for homology detection. Their model\nmakes similarity measures such as BLOSUM or PAM matrices, not a priori ﬁxed, but in-\nstead suitably learned by LSTM with regard to each speciﬁc classiﬁcation task. Liu et al.\n(2017, ProDec-BLSTM) conducted a similar work on protein remote homology detection,\nand showed an improvement using BLSTM instead of LSTM (Hochreiter et al., 2007). One\ndrawback of homology-based approaches for fold recognition is the lack of a direct rela-\ntionship between the protein sequence and the fold since current methods substantially\nrely on the fold of known template protein to classify the fold of new proteins (Hou et al.,\n2017). Therefore, Hou et al. (2017, DeepSF) proposed a deep 1D CNN for fold classiﬁcation\ndirectly from protein sequences.\nThere are also some works based on available gene function annotation vocabularies\n(e.g. Gene Ontology (Park et al., 2005)) to perform protein classiﬁcation (Ashburner et al.,\n2000). By Similar motivation, BioVec (Asgari and Mofrad, 2015) was designed as a deep\nlearning method to compute a distributed representation of biological sequences with general\ngenomic applications such as protein family classiﬁcation. Each sequence is embedded in a\nhigh-dimension vector by BioVec, then the classiﬁcation of protein families can reduce to a\nsimple classiﬁcation task.\n4.4.2 Protein Secondary Structure\nProtein Secondary Structure (SS) refers to the local spatial structure formed by interac-\ntion between nearby stretches of a polypeptide chain. The protein SS encodes informa-\ntion for predicting biophysical properties of amino acid residues, higher-level protein struc-\nture (e.g., tertiary structure), protein function and evolution. It is traditionally described\nby either a 3-state model (Pauling et al., 1951), or an 8-state model by DSSP algorithm\n(Kabsch and Sander, 1983). The former labels each residue to be in one of three states:\nHelix, Strand, or Coil; while the 8-state model expands to eight diﬀerent states for a more\nﬁne-grained description of the spatial environment and chemical bonding of each amino\nacid. Q3 and Q8 accuracies are the widely adopted metrics to evaluate any model perfor-\nmance, which represents the percentage of correctly predicted secondary conformation of\namino acid residues. An alternative measure for 3-state prediction is the segment of overlap\n21\nYue et al.\n(SOV) score (Zemla et al., 1999).\nThe reasonable goal of SS prediction is suggested by\nRost et al. (1994) as a Q3 accuracy above 85%\nBefore deep learning became popular for protein SS prediction, machine learning ap-\nproaches including probabilistic graphical models (Schmidler et al., 2000; Maaten et al.,\n2011; Chu et al., 2004), hidden Markov models (Maaten et al., 2011) and SVMs (Hua and Sun,\n2001; Kim and Park, 2003; Ward et al., 2003) were widely adopted. At that nascent age\nof neural networks, one of the earliest applications developed a shallow feed-forward net-\nwork that predicts protein SS and homology from the amino acid sequences (Bohr et al.,\n1988).\nOther works for SS prediction adopted similar or slightly enhanced neural net-\nworks (Holley and Karplus, 1989; Kneller et al., 1990).\nQian and Sejnowski (1988) con-\nducted one of the inﬂuential works for 3-state prediction, reaching a Q3 accuracy of 64.3%.\nThey based on the fully connected neural networks to develop a cascaded architecture, tak-\ning as input window DNA sequences with orthogonal encoding. There was no signiﬁcant\nprogress for 3-state prediction accuracy by neural networks until being improved to 70.8%\nby Rost and Sander (1993a,b). Claimed of the marginal inﬂuence of free parameters in the\nmodel, Rost and Sander (1993a) accredited their improvement to leveraging evolutionary\ninformation encoded in the input proﬁles derived from multiple alignments. Riis and Krogh\n(1996) achieved a practically identical performance by a structured neural network. They\ndesigned speciﬁc networks for each SS class according to biological knowledge and the output\nprediction was made from ﬁltering and ensemble averaging. Based on the PSSM generated\nby PSI-BLAST, Jones (1999, PSIPRED) used a 2-stage neural network to obtain an average\nQ3 score of around 77%. Other popular deep learning methods such as bidirectional RNNs\nwere also widely applied for protein SS prediction (Baldi et al., 1999; Pollastri et al., 2002;\nMagnan and Baldi, 2014).\nEmergent deep architectures for protein SS prediction have been widely explored with\nmore prior knowledge and various features available. Faraggi et al. (2012, SPINE X) pro-\nposed an iterative six-step model, of which the neural network of each step follows a sim-\nilar structure and is designed for each speciﬁc purpose.\nSpencer et al. (2015) trained a\ndeep belief network model, in which an additional hidden layer is constructed to facilitate\nthe unsupervised layer-by-layer initialization of the Restricted Boltzmann Machine (RBM).\nLi and Yu (2016) designed a cascaded model, which leverages CNN to extract multi-scale\nlocal contextual features by diﬀerent kernel sizes, then added a BRNN accounting for long-\nrange dependencies in amino acid sequences to capture global contextual features.\nWang et al. (2016a, DeepCNF) took a large step in improving Q3 accuracy above 80%\nby extending conditional neural ﬁelds (CDFs) to include convolutional designs.\nDeep-\nCNF is able to capture both sequence-structure relationships and protein SS label cor-\nrelation among adjacent residues. They also achieved Q8 accuracy of around 72%, out-\nperforming Q8 accuracy of 66.4% obtained by a supervised generative stochastic network\n(Zhou and Troyanskaya, 2014). Busia et al. (2016) explored the model performance of 8-\nstated prediction from simple feed-forward networks to the adaptation of recent CNN archi-\ntectures (e.g. Inception, ReSNet, and DenseNet). They modiﬁed the convolution operators\nof diﬀerent scales and residual connections of successful CNN models in computer vision\nto suit the protein SS prediction task and also highlighted the diﬀerences compared to\nvision tasks. As opposed to the above-mentioned DeepCNF (Wang et al., 2016a) that in-\ncluded interdependencies between labels of adjacent residues by Conditional Random Field\n22\nDeep Learning for Genomics: A Concise Overview\n(CRF), Busia et al. (2016) conditions the current prediction on previously predicted labels\nby sequence-to-sequence modeling.\nA new class of Protein Language Model (ProtLM) has been proposed in recent years\nto utilize the power of large-scale, transformer-based language models. Rives et al. (2019)\nintroduced ESM-1b model, a BERT-like model trained on up to 250 million protein se-\nquence data from UniRef50 and UniRef100 datasets (Suzek et al., 2014) using the common\nmasked language model (MLM) objective, and achieved 70%+ Q8 accuracies at family, su-\nperfamily, and fold levels on a constructed test set derived from SCOPe database (Fox et al.,\n2013). The ProtTrans model family Elnaggar et al. (2020) introduced a series of transformer\nmodel architectures (Transformer-XL (Dai et al., 2019), BERT (Devlin et al., 2018), and\nT5 (Raﬀel et al., 2019)) pretrained on protein sequences from UniRef (Suzek et al., 2014)\nand Big Fantastic Database (Steinegger et al., 2018); they also showed the potential of\nProtLM completely independent of multiple sequence alignments (MSAs) features; their\nbest-performing model achieved a Q3 accuracy of 74.1% and Q8 accuracy of 60.7% on\nCASP14 (Kryshtafovych et al., 2021). ESM-2 model family (Lin et al., 2023) was later in-\ntroduce as an upgrade of the ESM-1b model in size (up to 15 billion parameters) and pushed\nthe Q3 accuracy to 76.8% and Q8 accuracy to 61.7% on CASP14. The latest eﬀort in scal-\ning up ProtLM has resulted in a 100-billion-parameter model, xTrimoPGLM (Chen et al.,\n2023), with a model backbone of General Language Model (GLM) (Du et al., 2021); how-\never, the authors also pointed out that the logarithmic increase of model performance on\nsize has already shown saturation on speciﬁc task such as the Q3 SS prediction.\n4.4.3 Contact Map\nA protein contact map is a binary 2D matrix denoting the spatial closeness of any two\nresidues in the folded 3D protein structure. Predicting residue-residue contact is thus crucial\nto protein structure prediction, and has been early studied by shallow neural networks\n(Torracinta and Campagne, 2016). Recent works proceeded to deeper networks. Lena et al.\n(2012) stacked together multiple standard three-layer feedforward networks sharing the same\ntopology, taking into consideration both spatial and temporal features to predict protein\nresidue–residue contact. Wang et al. (2017e) also developed an ultra-deep model to predict\nprotein contacts from amino acids sequence.\nTheir model consists of two deep residual\nneural networks that process 1D and 2D features separately and subsequently in order to\nconsider both sequential and pairwise features in the whole model. Zhang et al. (2017) and\nSchreiber et al. (2017) both contributed an open-source multi-modal CNN model for Hi-C\ncontact map prediction. Zhang et al. (2017, HiCPlus) ﬁrst interpolated the low-resolution\nHi-C matrix to the size of the high, then trained their model to predict high- from the\nlow-resolution matrix. The ﬁnal output was recombined into the entire Hi-C interaction\nmatrix. Schreiber et al. (2017, Rambutan) predicted Hi-C contacts at high resolution (1\nkb) from nucleotide sequences and DNaseI assay signal data.\nTheir model consists of\ntwo arms, with each arm processing one type of data independently. The learned feature\nmaps are then concatenated for further combination with genomic distance in the dense\nlayers.\nAdhikari et al. (2018) proposed a two-layer CNN network that consumes PSSM\nbased features as well as coevolutionary contact features to classify residue-residue contact\ninto ﬁve distance bins (6-10 ˚A).\n23\nYue et al.\nIn recent years, ResNet (He et al., 2016) has been extensively used in contact map pre-\ndiction. Yang et al. (2020) proposed trRossetta (based on Rossetta3 software Leaver-Fay et al.\n(2011)), which utilized known MSAs to guide the training of a 60-layer ResNet to clas-\nsify inter-residue distance as well as inter-residue orientational angles into discrete value\nbins. DeepDist (Wu et al., 2021) showcased the potential of ResNet for direct prediction of\nresidue-residue contact distance by combining outputs from four ResNet networks, each on\none type of sequential or co-evolutional features from the input, and was trained success-\nfully with a regression objective that minimizes the Mean-Squared Error (MSE) of predicted\ncontact distance in absolute value.\n4.4.4 Protein Tertiary Structure and Quality Assessment\nThe prediction of protein tertiary structure has proven crucial to human understanding\nof protein functions (Breda et al., 2007) and can be applied to, for instance, drug de-\nsigns (Jacobson and Sali, 2004). However, experimental methods for determining protein\nstructures, such as X-ray crystallography, are costly and sometimes impractical. Though\nthe number of experimentally solved protein structures included in the protein data bank\n(PDB)(https://www.rcsb.org/) keeps growing, it only accounts for a small proportion of\ncurrently sequenced proteins (Kryshtafovych and Fidelis, 2009). Thus, a potentially practi-\ncal approach to ﬁll the gap between the number of known protein sequences and the number\nof found protein structures is through computational modeling.\nTwo essential challenges in protein structure prediction include the sampling and the\nranking of protein structural models (Cao et al., 2015).\nQuality assessment (QA) is to\npredict the absolute or relative quality of the protein models before the native structure is\navailable so as to rank them. Some previous research, such as (Ray et al., 2012, ProQ2) and\n(Uziela et al., 2016, ProQ3), was conducted based on machine learning models. Recent deep\nlearning-based work from Uziela et al. (2017, ProQ3D) achieved substantial improvement by\nreplacing the SVMs in previous work with DNNs. As opposed to these existing methods that\nrely on energy or scoring functions, Nguyen et al. (2014) based solely on geometry to propose\na sparse stacked autoencoder classiﬁer that utilizes the contact map. Another research by\nCao et al. (2016) adopted a deep belief network protein structure prediction. Their model\ncould be used to evaluate the quality of any protein decoy. Local quality assessment remains\nto be substantially improved compared with global prediction (Shin et al., 2017). Liu et al.\n(2016b) introduced three models based on SDAs as a benchmark of deep learning methods\nfor assessing the quality of individual protein models.\nModern tertiary structure prediction systems typically pipeline functional modules that\n(i) query MSA for target protein sequence; (ii) predict contact map or residue-residue dis-\ntance, and (iii) reconstruct 3-D structure based on predicted contact map under energy\nand physical constraints. The quality of these MSA-based systems can depend sensitively\non the performance of the involved contact map prediction models (Section 4.4.3). MUL-\nTICOM (Hou et al., 2019) extended the use of DNCON2 as their contact map prediction\nmodule by incorporating 1-D structural features, such as residue-level secondary structure\nlabels, sequential features, and coevolutionary features; the system was upgraded in 2022\nto MULTICOM2 (Liu et al., 2022) as the authors incorporated more deep-learning based\nmodules, including using DeepDist in place of DNCON2 for contact map prediction, and\n24\nDeep Learning for Genomics: A Concise Overview\nachieved high system ranking (seventh out of 146 systems) in tertiary structure prediction in\nCASP14. ThreadAI (Zhang and Shen, 2020) also improved upon MULTICOM by adopting\ntrRossetta instead of DNCON2 in their contact map prediction.\nThe AlphaFold Senior et al. (2020) and AlphaFold2 models (Jumper et al., 2021) rev-\nolutionized the practicality of DNNs in predicting protein structure at atomic resolution:\nthe authors constructed a novel two-stage DNN relying heavily the attention mechanism\nto predict directly the 3D coordinates of all heavy atoms in a given protein: the ﬁrst stage\nencodes and combines both MSA and residue pairs features through a series of transformer-\nlike blocks with attention module; the second stage module then builds upon the learned\nrepresentations to reﬁne a hypothesized 3D structure subjecting to evolutionary, physical,\nand geometrical constraints. As of the writing of this review, AlphaFold2 remains the best\nmodel for protein structure prediction on CASP14.\nResearch since has revealed several\nlimitations of the AlphaFold2 model: its performance coud suﬀer from predicting intrinsi-\ncally disordered proteins Ruﬀand Pappu (2021); the performance on loop prediction is only\nhigh for short loops Stevens and He (2022); and signiﬁcant degradation in performance was\ndiscovered on target sequence with few homologous counterparts in existing databases or\nwhen the MSAs are of low depth Wang et al. (2022), although this problem was mitigated\nby MSA-Augmenter Zhang et al. (2023), which is a transformer model trained on known\nMSA sequences to generate artiﬁcial MSA sequences that are used to augment training data\nused by AlphaFold2.\nA more serious limitation with AlphaFold2 is the inability of predicting novel structures\ndue to its dependence on known MSA features. ProtLM models which are independent\nof MSA features overcome this limitation trivially and has shown great potential in novel\nstructure prediction. Lin et al. (2023) introduced ESMFold as an extension of ESM-2 model\nfamily with an added folding structure module; ESMFold depends only on embeddings\nlearned through ESM-2 and showed decent performance (80% of AlphaFold2) on CASP14.\nMore competitive performance is achieved by newer ProtLM such as OmegaFold (Wu et al.,\n2022) and EMBER3D (Weissenow et al., 2022). These models also showed much better in-\nference time than AlphaFold2, with OmegaFold attaining sub-second prediction on proteins\nwith sequence up to 1000 residues.\n5. Challenges and Opportunities\nWith the discussion of the successes of applications of deep learning in genomics, now we\nproceed to discuss some current challenges.\nAs deep learning models are usually over-\nparametrized, the performance can be conditional if the models are not appropriately de-\nsigned according to the problem. There are multiple worthwhile considerations and tech-\nniques involving model architectures, feature extraction, data limitation, etc., which help\ndeep learning models to better approach genomics. Here we brieﬂy discuss some current\nchallenges that deserve attention and several potential research directions that might shed\non light the future development of deep learning applications in genomic research.\n5.1 The Nature of Data\nAn inevitable challenge of transferring the success of deep learning in conventional vision\nor text data into genomics is raised due to the nature of the genomic data, such as the\n25\nYue et al.\nunavailability of true labels due to the lack of knowledge of the genetic process, the imbal-\nanced case and control samples due to the rarity of a certain disease, and the heterogeneity\nof data due to the expensiveness of large-scale data collection.\n5.1.1 Class-Imbalanced Data\nLarge-scale biological data gathered from assorted sources are usually inherently class-\nimbalanced. Take epigenetic datasets for example, there are in nature much fewer DNA\nmethylated regions (DMR) sites than non-DMR sites (Haque et al., 2014). It is also common\nin enhancer prediction problems where the number of non-enhancer classes overwhelmingly\nexceeds that of enhancer classes (Firpi et al., 2010; Kleftogiannis et al., 2014). Methods that\ndirectly over-samples minor classes or under-samples majority classes have been attempted\nand proven successful. ¨Oztornaci et al. (2023) found that multiple machine learning models\n(SVM, MLP, Random Forest) beniﬁts from Synthetic Minority Over-sampling Technique\n(SMOTE) in ﬁnding single nucleotide polymorphisms (SNPs). This data-imbalance issue\nhas also been encountered in machine learning methods (Yoon and Kwek, 2005; He and Ma,\n2013), while ensemble methods appear to be powerful (Haque et al., 2014). Sun et al. (2013)\napplied the undersampling method together with a majority vote to address the imbalanced\ndata distribution inherent in gene expression image annotation tasks. In deep learning ap-\nproaches, Al-Stouhi and Reddy (2016) based on boosting to propose an instance-transfer\nmodel to reduce the class-imbalanced inﬂuence while also improving the performance by\nleveraging data from an auxiliary domain. Combining conventional DNNs with a Deep De-\ncision Tree classiﬁer, R. et al. (2021) proposes a Hybrid DNN architecture which addresses\nclass-imbalance of certain RNA sequences by forcing the network to account for minor\nclasses in the decision tree’s hierarchical if-else cases. In addition to resorting to ensemble\napproaches, researchers can manage to resolve class-imbalanced problems through model\nparameters or training processes. For instance, Liu et al. (2016a, PEDLA) used an em-\nbedded mechanism utilizing the prior probability of each class directly estimated from the\ntraining data to compensate for the imbalance of classes. Lee and Yoon (2015) presented a\nmethod called boosted contrastive divergence with categorical gradients for training RBMs\nfor class imbalanced prediction of splice junctions. Singh et al. (2016b) performed data aug-\nmentation by slightly shifting each positive promoter or enhancer within the window since\nthe true label is not sensitive to these minimal changes. They also designed the training\nprocedure accordingly to avoid the high false positive rate resulting from the augmented\ndataset.\n5.1.2 Various Data Types\nIntuitively, integrating diverse types of data as discriminating features will lead to more\npredictive power of the models. For example, Liu et al. (2016a, PEDLA) trained their model\non nine types of data to identify enhancers, including chromatin accessibility (DNase-sseq),\nTFs and cofactors (ChIP-seq), histone modiﬁcations (ChIP-seq), transcription (RNA-Seq),\nDNA methylation (RRBS), sequence signatures, evolutionary conservation, CpG islands,\nand occupancy of TFBSs, resulting in better model performance in terms of multiple metrics\ncompared with existing popular methods. Angermueller et al. (2017, DeepCpG) predicted\n26\nDeep Learning for Genomics: A Concise Overview\nsingle-cell DNA methylation states by two disparate sub-networks designed accordingly for\nCpG sites and DNA sequences.\nIt pays oﬀto manage to utilize the data of multiple views; though merging the infor-\nmation from various data sources challenges the models that could well integrate them,\nthis eﬀort might provide more information with a great chance.\nA review of data rep-\nresentations in genomics, transcriptomics, proteomics, metabolomics and epigenomics for\ncomputer scientists can be found in Tsimenidis et al. (2022), in which the format, type, and\nencoding of data from these disciplines are presented, together with their common feature\nextraction techniques. For more discussions on encompassing diverse data sources, we refer\nto multi-view learning in Section 3.3.\n5.1.3 Heterogeneity and Confounding Correlations\nThe data in most genomic applications involving medical or clinical are heterogeneous due\nto population subgroups, or regional environments. One of the problems of integrating these\ndiﬀerent types of data is the underlying interdependencies among these heterogeneous data.\nCovariates are sometimes confounding, and render the model prediction inaccurate.\nThe Genome-Wide Association Study (GWAS) is an example where both population-\nbased confounders (population subgroups with diﬀerent ancestry) and individual related-\nnesses produce spurious correlations among SNPs to the trait of interest. Most existing\nstatistical methods estimate confounders before performing causal inference. These meth-\nods are based on linear regression (Yu et al., 2006; Astle et al., 2009), linear mixed model\n(LMM) (Kang et al., 2010; Yang et al., 2014), or others (Song et al., 2015). Wang et al.\n(2017a) tried to upgrade LMM and tested it on biological variable selection and prediction\ntasks. Though these LMM-based models (e.g. FaST-LMM, Lippert et al., 2011) are favored\nby some researchers and mathematically suﬃcient, their power pales when facing multiple\nnonlinear confounding correlations. The assumed Gaussian noise might overshadow true\nunderlying causals, and LMM also fails to literally model the variable correlations. A seem-\ningly more reliable approach is to through generative modeling, e.g.\nHao et al. (2015).\nTran and Blei (2017) and Louizos et al. (2017) are all based on variational inference to\npresent an implicit causal model for encoding complex, nonlinear causal relationships, with\nconsideration of latent confounders. Tran and Blei (2017) optimized their model iteratively\nto estimate confounders and SNPs, and their simulation study suggested a signiﬁcant im-\nprovement.\nFrom the methodology perspective, several deep learning methods that are not designed\nexclusively for confounder correction, such as the domain adversarial learning (Ganin et al.,\n2016), select-additive learning (Wang et al., 2017c), and confounder ﬁltering (Wu et al.,\n2018), can be re-used, once the identiﬁcation of confounder is presented.\n5.2 Feature Extraction\nDeep learning that performs automatic feature extraction saves great eﬀorts of choosing\nhand-engineered features, Torng and Altman (2017) also discussed the superiority of au-\ntomatically generated features over manually selected features.\nHowever, in practice, it\nis unfortunately time-consuming to directly learn features from genomic sequences when\ncomplex interdependences and long-range interactions are taken into consideration. Re-\n27\nYue et al.\nsearchers might still resort to task-speciﬁc feature extraction before automatic feature de-\ntection, which could strongly facilitate the model if skillfully designed.\n5.2.1 Mathematical Feature Extraction\nTechniques borrowed from mathematics have great potential to interpret the complex bio-\nlogical structures behind data that otherwise will hinder the generalization of deep learn-\ning.\nFor example, topology is a promising choice to untangle the geometric complexity\nunderlying the 3D biomolecular structure of protein (Cang and Wei, 2017), and homology\ndetection has been widely applied to protein classiﬁcation problems (Hochreiter et al., 2007;\nCang et al., 2015). DeepMethyl (Wang et al., 2016b) was developed as deep learning soft-\nware using features derived from 3D genome topology and DNA sequence patterns. It is\nbased on SDAs and is applied to predict methylation states of DNA CpG dinucleotides.\nCang and Wei (2017) introduced element-speciﬁc persistent homology (ESPH) into CNNs\nto predict protein-ligand binding aﬃnities and protein stability changes upon mutation,\nincluding globular protein mutation impacts and membrane protein mutations impact. Fi-\nnally, in making feature extraction techniques from popular literatures generally available\nto the genomics research community, Bonidia et al. (2021) made available a novel software\npackage MathFeature, which implements 20 mathematical descriptors and 17 convential\ndescriptors, used to numerically encode long gene sequences.\n5.2.2 Feature Representation\nBy the conceptual analogy of the fact that humans communicate through languages, biolog-\nical organisms convey information within and between cells through information encoded\nin biological sequences. To understand this language of life, Asgari and Mofrad (2015) de-\nsigned BioVec, an unsupervised data-driven feature representation method, which embeds\neach trigram of biological sequence in a 100-dimensional vector that characterizes biophys-\nical and biochemical properties of sequences.\nBioVec was trained by a variant of MLP\nadapted from word2vec (Mikolov et al., 2013b,a), a typical method in natural language\nprocessing. Ng (2017) further utilized shallow two-layer neural networks to compute the\nrepresentation of variable-length k-mers of DNA sequences that is consistent across diﬀerent\nlengths. In contrast to representation by BioVec for individual kmers, Kimothi et al. (2016)\nbased on doc2vec algorithm, an extension of word2vec, to proposed distributed represen-\ntation of complete protein sequence, and successfully applied to protein classiﬁcation fol-\nlowing the settings of Asgari and Mofrad (2015). Another feature representation technique\nwas proposed by Hao et al. (2023) on cancer survival prediction from genetic sequences.\nThe authors speciﬁcally represented the shared and unique features from DNA, mRNA\nand miRNA, and leveraged the consistent and complement information in these features to\nimprove prediction accuracy. These types of feature representation have the potential to\nfacilitate the work of genomics.\n6. Conclusion and Outlook\nGenomics is a challenging application area of deep learning that encounters unique chal-\nlenges compared to other ﬁelds such as vision, audio, and text processing, since we have\n28\nDeep Learning for Genomics: A Concise Overview\nlimited abilities to interpret genomic information but expect from deep learning a superhu-\nman intelligence that explores beyond our knowledge. Yet deep learning is undoubtedly an\nauspicious direction that has constantly rejuvenated and moved forward genomic research\nin recent years. As discussed in this review, recent breakthroughs of deep learning appli-\ncations in genomics have surpassed many previous state-of-the-art computational methods\nwith regard to predictive performance, though slightly lag behind traditional statistical\ninferences in terms of interpretation.\nCurrent applications, however, have not brought about a watershed revolution in ge-\nnomic research. The predictive performances in most problems have not reached the ex-\npectation for real-world applications, and neither have the interpretations of these abstruse\nmodels elucidate insightful knowledge. A plethora of new deep learning methods are con-\nstantly being proposed but await artful applications in genomics. By careful selection of\ndata sources and features or appropriate design of model structures, deep learning can be\ndriven towards a bright direction which produces a more accurate and interpretable pre-\ndiction. We need to bear in mind numerous challenges beyond simply improving predictive\naccuracy to seek essential advancements and revolutions in deep learning for genomics.\nAcknowledgement\nAcknowledgments\nWe would like to acknowledge the inspiration from a course instructed by Kundaje and Zou\n(2016), and two reviews contributed by Min et al. (2017) and Ching et al. (2017). A col-\nlaboratively written review paper on deep learning, genomics, and precision medicine, now\navailable at https://greenelab.github.io/deep-review/ .\n29\nYue et al.\nReferences\nBadri Adhikari, Jie Hou, and Jianlin Cheng. Dncon2: improved protein contact prediction\nusing two-level deep convolutional neural networks.\nBioinformatics, 34(9):1466–1472,\n2018.\nSamir Al-Stouhi and Chandan K Reddy. Transfer learning for class imbalance problems\nwith inadequate data. Knowledge and information systems, 48(1):201–228, 2016.\nBabak Alipanahi,\nAndrew Delong,\nMatthew T. Weirauch,\nand Brendan J. Frey.\nPredicting the sequence speciﬁcities\nof\ndna- and rna-binding proteins\nby deep\nlearning.\nNat\nBiotech,\n33(8):831–838,\nAug\n2015.\nISSN\n1087-0156.\nURL\nhttp://dx.doi.org/10.1038/nbt.3300. Computational Biology.\nJose Juan Almagro Armenteros, Casper Kaae Sønderby, Søren Kaae Sønderby, Henrik\nNielsen, and Ole Winther.\nDeeploc: prediction of protein subcellular localization us-\ning deep learning. Bioinformatics, 33(21):3387–3395, 2017.\nStephen F Altschul, Warren Gish, Webb Miller, Eugene W Myers, and David J Lipman.\nBasic local alignment search tool. Journal of molecular biology, 215(3):403–410, 1990.\nStephen F Altschul, Thomas L Madden, Alejandro A Sch¨aﬀer, Jinghui Zhang, Zheng Zhang,\nWebb Miller, and David J Lipman. Gapped blast and psi-blast: a new generation of\nprotein database search programs. Nucleic acids research, 25(17):3389–3402, 1997.\nAntonina Andreeva and Alexey G Murzin. Structural classiﬁcation of proteins and struc-\ntural genomics: new insights into protein folding and evolution. Acta Crystallograph-\nica Section F: Structural Biology and Crystallization Communications, 66(10):1190–1197,\n2010.\nChristof Angermueller, Heather J. Lee, Wolf Reik, and Oliver Stegle.\nDeepcpg: accu-\nrate prediction of single-cell dna methylation states using deep learning. Genome Bi-\nology, 18(1):67, Apr 2017.\nISSN 1474-760X.\ndoi: 10.1186/s13059-017-1189-z.\nURL\nhttps://doi.org/10.1186/s13059-017-1189-z.\nEhsaneddin Asgari and Mohammad RK Mofrad. Continuous distributed representation of\nbiological sequences for deep proteomics and genomics. PloS one, 10(11):e0141287, 2015.\nMichael Ashburner, Catherine A Ball, Judith A Blake, David Botstein, Heather Butler,\nJ Michael Cherry, Allan P Davis, Kara Dolinski, Selina S Dwight, Janan T Eppig, et al.\nGene ontology: tool for the uniﬁcation of biology. Nature genetics, 25(1):25–29, 2000.\nWilliam Astle, David J Balding, et al.\nPopulation structure and cryptic relatedness in\ngenetic association studies. Statistical Science, 24(4):451–471, 2009.\nˇZiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-\nBarwinska, Kyle R. Taylor,\nYannis Assael,\nJohn Jumper, Pushmeet Kohli, and\nDavid R. Kelley.\nEﬀective gene expression prediction from sequence by integrat-\ning long-range interactions.\nbioRxiv, 2021.\ndoi:\n10.1101/2021.04.07.438649.\nURL\nhttps://www.biorxiv.org/content/early/2021/04/08/2021.04.07.438649.\n30\nDeep Learning for Genomics: A Concise Overview\nByoung-Il Bae, Divya Jayaraman, and Christopher A Walsh. Genetic changes shaping the\nhuman brain. Developmental cell, 32(4):423–434, 2015.\nPierre Baldi, Søren Brunak, Paolo Frasconi, Giovanni Soda, and Gianluca Pollastri. Ex-\nploiting the past and the future in protein secondary structure prediction. Bioinformatics,\n15(11):937–946, 1999.\nYoseph Barash, John A Calarco, Weijun Gao, Qun Pan, Xinchen Wang, Ofer Shai, Ben-\njamin J Blencowe, and Brendan J Frey. Deciphering the splicing code. Nature, 465(7294):\n53–59, 2010.\nJordi Barretina, Giordano Caponigro, Nicolas Stransky, Kavitha Venkatesan, Adam A Mar-\ngolin, Sungjoon Kim, Christopher J Wilson, Joseph Leh´ar, Gregory V Kryukov, Dmitriy\nSonkin, et al. The cancer cell line encyclopedia enables predictive modelling of anticancer\ndrug sensitivity. Nature, 483(7391):603–607, 2012.\nMichael A Beer and Saeed Tavazoie. Predicting gene expression from sequence. Cell, 117\n(2):185–198, 2004.\nG Benegas, SS Batra, and YS Song. Dna language models are powerful zero-shot predictors\nof non-coding variant eﬀects. 2022.\nYoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise\ntraining of deep networks. In Advances in neural information processing systems, pages\n153–160, 2007.\nIlayda Beyreli, Oguzhan Karakahya, and A Ercument Cicek.\nDeepnd: Deep multitask\nlearning of gene risk for comorbid neurodevelopmental disorders. Patterns, 3(7), 2022.\nHenrik Bohr, Jakob Bohr, Søren Brunak, Rodney MJ Cotterill, Benny Lautrup, Leif\nNørskov, Ole H Olsen, and Steﬀen B Petersen.\nProtein secondary structure and ho-\nmology by neural networks the α-helices in rhodopsin. FEBS letters, 241(1-2):223–228,\n1988.\nRobson P Bonidia, Douglas S Domingues, Danilo S Sanches, and Andr´e C P L F\nde Carvalho.\nMathFeature:\nfeature extraction package for DNA, RNA and pro-\ntein sequences based on mathematical descriptors.\nBrieﬁngs in Bioinformatics,\n23(1):bbab434,\n11 2021.\nISSN 1477-4054.\ndoi:\n10.1093/bib/bbab434.\nURL\nhttps://doi.org/10.1093/bib/bbab434.\nVladim´ır Boˇza, Broˇna Brejov´a, and Tom´aˇs Vinaˇr. Deepnano: Deep recurrent neural net-\nworks for base calling in minion nanopore reads. PloS one, 12(6):e0178751, 2017.\nArdala Breda, Napoleao Fonseca Valadares, Osmar Norberto de Souza, and Richard Charles\nGarratt. Protein structure, modelling and applications. 2007.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural information processing systems,\n33:1877–1901, 2020.\n31\nYue et al.\nAkosua Busia, Jasmine Collins, and Navdeep Jaitly. Protein secondary structure prediction\nusing deep multi-scale convolutional neural networks and next-step conditioning. arXiv\npreprint arXiv:1611.01503, 2016.\nCarolyn E Cain, Ran Blekhman, John C Marioni, and Yoav Gilad. Gene expression diﬀer-\nences among primates are associated with changes in a histone epigenetic modiﬁcation.\nGenetics, 187(4):1225–1234, 2011.\nZixuan Cang and Guo-Wei Wei.\nTopologynet:\nTopology based deep convolutional\nand multi-task neural networks for biomolecular property predictions.\nPLOS Com-\nputational Biology, 13(7):1–27, 07 2017.\ndoi:\n10.1371/journal.pcbi.1005690.\nURL\nhttps://doi.org/10.1371/journal.pcbi.1005690.\nZixuan Cang, Lin Mu, Kedi Wu, Kristopher Opron, Kelin Xia, and Guo-Wei Wei.\nA\ntopological approach for protein classiﬁcation. Molecular Based Mathematical Biology, 3\n(1), 2015.\nJingjun Cao, Zhengli Wu, Wenting Ye, and Haohan Wang. Learning functional embedding\nof genes governed by pair-wised labels. In Computational Intelligence and Applications\n(ICCIA), 2017 2nd IEEE International Conference on, pages 397–401. IEEE, 2017a.\nRenzhi Cao, Debswapna Bhattacharya, Badri Adhikari, Jilong Li, and Jianlin Cheng. Large-\nscale model quality assessment for improving protein tertiary structure prediction. Bioin-\nformatics, 31(12):i116–i123, 2015.\nRenzhi Cao, Debswapna Bhattacharya, Jie Hou, and Jianlin Cheng. Deepqa: improving the\nestimation of single protein model quality with deep belief networks. BMC bioinformatics,\n17(1):495, 2016.\nRenzhi Cao, Colton Freitas, Leong Chan, Miao Sun, Haiqing Jiang, and Zhangxin Chen.\nProlango: Protein function prediction using neural machine translation based on a recur-\nrent neural network. Molecules, 22(10):1732, 2017b.\nZhen Cao and Shihua Zhang. gkm-dnn: eﬃcient prediction using gapped k-mer features\nand deep neural networks. bioRxiv, page 170761, 2017.\nDavide Castelvecchi.\nCan we open the black box of ai?\nNature, 2016.\nURL\nhttps://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731?goal=0_997ed6f472-4f7\nBo Chen, Xingyi Cheng, Yangli ao Geng, Shengyin Li, Xin Zeng, Bo Wang, Jing Gong,\nChiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, and Leo T. Song. xtrimopglm: Uniﬁed\n100b-scale pre-trained transformer for deciphering the language of protein. bioRxiv, 2023.\nURL https://api.semanticscholar.org/CorpusID:259502990.\nChen Chen, Jie Hou, Xiaowen Shi, Hua Yang, James A Birchler, and Jianlin Cheng. Inter-\npretable attention model in transcription factor binding site prediction with deep neural\nnetworks. bioRxiv, page 648691, 2019.\nDexiong Chen, Laurent Jacob, and Julien Mairal. Predicting transcription factor binding\nsites with convolutional kernel networks. bioRxiv, page 217257, 2017.\n32\nDeep Learning for Genomics: A Concise Overview\nJunjie Chen, Mingyue Guo, Xiaolong Wang, and Bin Liu. A comprehensive review and\ncomparison of diﬀerent computational methods for protein remote homology detection.\nBrieﬁngs in bioinformatics, page bbw108, 2016a.\nLujia Chen, Chunhui Cai, Vicky Chen, and Xinghua Lu.\nLearning a hierarchical rep-\nresentation of the yeast transcriptomic machinery using an autoencoder model. BMC\nbioinformatics, 17(1):S9, 2016b.\nYifei Chen, Yi Li, Rajiv Narayan, Aravind Subramanian, and Xiaohui Xie. Gene expression\ninference with deep learning. Bioinformatics, 32(12):1832–1839, 2016c.\nChao Cheng, Koon-Kiu Yan, Kevin Y Yip, Joel Rozowsky, Roger Alexander, Chong Shou,\nand Mark Gerstein. A statistical framework for modeling gene expression using chromatin\nfeatures and application to modencode datasets. Genome biology, 12(2):R15, 2011.\nTravers Ching, Daniel S Himmelstein, Brett K Beaulieu-Jones, Alexandr A Kalinin, Brian T\nDo, Gregory P Way, Enrico Ferrero, Paul-Michael Agapow, Wei Xie, Gail L Rosen, et al.\nOpportunities and obstacles for deep learning in biology and medicine. bioRxiv, page\n142760, 2017.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi\nBougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using\nrnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078,\n2014.\nJoungmin Choi and Heejoon Chae.\nmethcancer-gen: a dna methylome dataset genera-\ntor for user-speciﬁed cancer type based on conditional variational autoencoder. BMC\nBioinformatics, 21, 05 2020. doi: 10.1186/s12859-020-3516-8.\nWei Chu, Zoubin Ghahramani, and David L Wild.\nA graphical model for protein sec-\nondary structure prediction. In Proceedings of the twenty-ﬁrst international conference\non Machine learning, page 21. ACM, 2004.\nDan C Cire¸san, Ueli Meier, and J¨urgen Schmidhuber.\nTransfer learning for latin and\nchinese characters with deep neural networks. In Neural Networks (IJCNN), The 2012\nInternational Joint Conference on, pages 1–6. IEEE, 2012.\nDikla Cohn, Or Zuk, and Tommy Kaplan. Enhancer identiﬁcation using transfer and ad-\nversarial deep learning of dna sequences. bioRxiv, page 264200, 2018.\nENCODE Project Consortium et al. An integrated encyclopedia of dna elements in the\nhuman genome. Nature, 489(7414):57–74, 2012.\nHaotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, and Bo Wang.\nscgpt: Towards building a foundation model for single-cell multi-omics using generative\nai. bioRxiv, pages 2023–04, 2023.\nZihang\nDai,\nZhilin\nYang,\nYiming\nYang,\nJaime\nG.\nCarbonell,\nQuoc\nV.\nLe,\nand\nRuslan\nSalakhutdinov.\nTransformer-xl:\nAttentive\nlanguage\nmod-\nels\nbeyond\na\nﬁxed-length\ncontext.\nArXiv,\nabs/1901.02860,\n2019.\nURL\nhttps://api.semanticscholar.org/CorpusID:57759363.\n33\nYue et al.\nHugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza,\nAdam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan\nSirelkhatim, Guillaume Richard, et al. The nucleotide transformer: Building and evalu-\nating robust foundation models for human genomics. bioRxiv, pages 2023–01, 2023.\nPadideh Danaee, Reza Ghaeini, and David A Hendrix. A deep learning approach for cancer\ndetection and relevant gene identiﬁcation. In PACIFIC SYMPOSIUM ON BIOCOM-\nPUTING 2017, pages 219–229. World Scientiﬁc, 2017.\nOlgert Denas and James Taylor. Deep modeling of gene expression regulation in an ery-\nthropoiesis model. In Representation Learning, ICML Workshop, 2013.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert:\nPre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\nAyse Berceste Dincer, Saﬁye Celik, Naozumi Hiranuma, and Su-In Lee. Deepproﬁle: Deep\nlearning of patient molecular proﬁles for precision medicine in acute myeloid leukemia.\nbioRxiv, page 278739, 2018.\nXianjun Dong and Zhiping Weng. The correlation between histone modiﬁcations and gene\nexpression. 2013.\nXianjun Dong, Melissa C Greven, Anshul Kundaje, Sarah Djebali, James B Brown, Chao\nCheng, Thomas R Gingeras, Mark Gerstein, Roderic Guig´o, Ewan Birney, et al. Modeling\ngene expression using chromatin features in various cellular contexts. Genome biology,\n13(9):R53, 2012.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie\nTang.\nGlm:\nGeneral language model pretraining with autoregressive blank inﬁll-\ning. In Annual Meeting of the Association for Computational Linguistics, 2021. URL\nhttps://api.semanticscholar.org/CorpusID:247519241.\nJeﬀrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion\nJones, Tom Gibbs, Tamas B. Feh´er, Christoph Angerer, Martin Steinegger, Debsindhu\nBhowmik, and Burkhard Rost. Prottrans: Towards cracking the language of life’s code\nthrough self-supervised deep learning and high performance computing. bioRxiv, 2020.\nURL https://api.semanticscholar.org/CorpusID:220495861.\nOlof Emanuelsson, Henrik Nielsen, Søren Brunak, and Gunnar Von Heijne.\nPredicting\nsubcellular localization of proteins based on their n-terminal amino acid sequence. Journal\nof molecular biology, 300(4):1005–1016, 2000.\nEshel Faraggi, Tuo Zhang, Yuedong Yang, Lukasz Kurgan, and Yaoqi Zhou.\nSpine x:\nimproving protein secondary structure prediction by multistep learning coupled with\nprediction of solvent accessible surface area and backbone torsion angles.\nJournal of\ncomputational chemistry, 33(3):259–267, 2012.\n34\nDeep Learning for Genomics: A Concise Overview\nJames W Fickett and Artemis G Hatzigeorgiou. Eukaryotic promoter recognition. Genome\nresearch, 7(9):861–878, 1997.\nHiram A Firpi, Duygu Ucar, and Kai Tan. Discover regulatory dna elements using chromatin\nsignatures and artiﬁcial neural network. Bioinformatics, 26(13):1579–1586, 2010.\nNaomi Fox, Steven E. Brenner, and John-Marc Chandonia.\nScope:\nStructural clas-\nsiﬁcation of proteins—extended,\nintegrating scop and astral data and classiﬁca-\ntion of new structures.\nNucleic Acids Research, 42:D304 – D309, 2013.\nURL\nhttps://api.semanticscholar.org/CorpusID:14864309.\nKunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biological\ncybernetics, 20(3-4):121–136, 1975.\nKunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model\nfor a mechanism of visual pattern recognition. In Competition and cooperation in neural\nnets, pages 267–285. Springer, 1982.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle,\nFran¸cois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial train-\ning of neural networks. The Journal of Machine Learning Research, 17(1):2096–2030,\n2016.\nDadi Gao, Elisabetta Morini, Monica Salani, Aram J. Krauson, Anil Chekuri, Neeraj\nSharma, Ashok Ragavendran, Serkan Erdin, Emily M. Logan, Wencheng Li, Amal\nDakka, Jana Narasimhan, Xin Zhao, Nikolai Naryshkin, Christopher R. Trotta, Ker-\nstin A. Eﬀenberger, Matthew G. Woll, Vijayalakshmi Gabbeta, Gary Karp, Yong Yu,\nGraham Johnson, William D. Paquette, Garry R. Cutting, Michael E. Talkowski, and\nSusan A. Slaugenhaupt. A deep learning approach to identify gene targets of a ther-\napeutic for human splicing disorders. Nature Communications, 12(1):3332, 2021. doi:\n10.1038/s41467-021-23663-2. URL https://doi.org/10.1038/s41467-021-23663-2.\nMahmoud Ghandi, Dongwon Lee, Morteza Mohammad-Noori, and Michael A Beer. En-\nhanced regulatory sequence prediction using gapped k-mer features. PLoS computational\nbiology, 10(7):e1003711, 2014.\nRohan Ghotra, Nicholas Keone Lee, Rohit Tripathy, and Peter K Koo. Designing inter-\npretable convolution-based hybrid networks for genomics. bioRxiv, pages 2021–07, 2021.\nVladimir Gligorijevi´c and Nataˇsa Prˇzulj. Methods for biological data integration: perspec-\ntives and challenges. Journal of the Royal Society Interface, 12(112):20150571, 2015.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\nAman Gupta, Haohan Wang, and Madhavi Ganapathiraju.\nLearning structure in gene\nexpression data using deep architectures, with an application to gene clustering.\nIn\nBioinformatics and Biomedicine (BIBM), 2015 IEEE International Conference on, pages\n1328–1335. IEEE, 2015.\n35\nYue et al.\nMuhammed S. Hammad, Vidan F. Ghoneim, Mai S. Mabrouk, and Walid I. Al-atabany. A\nhybrid deep learning approach for COVID-19 detection based on genomic image process-\ning techniques. Scientiﬁc Reports, 13(1):4003, March 2023. ISSN 2045-2322. doi: 10.1038/\ns41598-023-30941-0. URL https://www.nature.com/articles/s41598-023-30941-0.\nWei Hao, Minsun Song, and John D Storey. Probabilistic models of genetic variation in\nstructured populations applied to global human studies. Bioinformatics, 32(5):713–721,\n2015.\nYaru Hao, Xiao-Yuan Jing, and Qixing Sun. Cancer survival prediction by learning com-\nprehensive deep feature representation for multiple types of genetic data - bmc bioinfor-\nmatics, Jun 2023. URL https://doi.org/10.1186/s12859-023-05392-z.\nM Muksitul Haque, Michael K Skinner, and Lawrence B Holder. Imbalanced class learning\nin epigenetics. Journal of Computational Biology, 21(7):492–507, 2014.\nJohn Hawkins and Mikael Bod´en. Detecting and sorting targeting peptides with neural\nnetworks and support vector machines.\nJournal of bioinformatics and computational\nbiology, 4(01):1–18, 2006.\nHaibo He and Yunqian Ma. Imbalanced learning: foundations, algorithms, and applications.\nJohn Wiley & Sons, 2013.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\nGeoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with\nneural networks. science, 313(5786):504–507, 2006.\nGeoﬀrey E Hinton and Terrence J Sejnowski. Learning and releaming in boltzmann ma-\nchines. Parallel Distrilmted Processing, 1, 1986.\nBich Hai Ho, Rania Mohammed Kotb Hassen, and Ngoc Tu Le. Combinatorial roles of dna\nmethylation and histone modiﬁcations on gene expression. In Some Current Advanced\nResearches on Information and Computer Science in Vietnam, pages 123–135. Springer,\n2015.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\nSepp Hochreiter, Martin Heusel, and Klaus Obermayer. Fast model-based protein homology\ndetection without alignment. Bioinformatics, 23(14):1728–1736, 2007.\nL Howard Holley and Martin Karplus. Protein secondary structure prediction with a neural\nnetwork. Proceedings of the National Academy of Sciences, 86(1):152–156, 1989.\nPaul B Horton and Minoru Kanehisa.\nAn assessment of neural network and statistical\napproaches for prediction of e. coli promoter sites. Nucleic Acids Research, 20(16):4331–\n4338, 1992.\n36\nDeep Learning for Genomics: A Concise Overview\nJie Hou, Badri Adhikari, and Jianlin Cheng. Deepsf: deep convolutional neural network for\nmapping protein sequences to folds. Bioinformatics, 2017.\nJie Hou, Tianqi Wu, Renzhi Cao, and Jianlin Cheng. Protein tertiary structure modeling\ndriven by deep learning and contact distance prediction in casp13. Proteins: Structure,\nFunction, and Bioinformatics, 87(12):1165–1178, 2019.\nSujun Hua and Zhirong Sun. A novel method of protein secondary structure prediction with\nhigh segment overlap measure: support vector machine approach. Journal of molecular\nbiology, 308(2):397–407, 2001.\nMatthew Jacobson and Andrej Sali. Comparative protein structure modeling and its appli-\ncations to drug discovery. 2004.\nThe\nJackson\nLaboratory\nJAX.\nGenetics\nvs.\ngenomics,\n2018.\nURL\nhttps://www.jax.org/personalized-medicine/precision-medicine-and-you/genetics-vs-genomics\nAnupama Jha, Matthew R Gazzara, and Yoseph Barash.\nIntegrative deep models for\nalternative splicing. bioRxiv, page 104869, 2017.\nYanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidi-\nrectional encoder representations from transformers model for dna-language in genome.\nBioinformatics, 37(15):2112–2120, 2021.\nDavid T Jones. Protein secondary structure prediction based on position-speciﬁc scoring\nmatrices. Journal of molecular biology, 292(2):195–202, 1999.\nJon`as Juan-Mateu, Olatz Villate, and D´ecio L Eizirik. Mechanisms in endocrinology: alter-\nnative splicing: the new frontier in diabetes research. European journal of endocrinology,\n174(5):R225–R238, 2016.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al.\nHighly accurate protein structure prediction with alphafold. Nature, 596(7873):583–589,\n2021.\nWolfgang Kabsch and Christian Sander. Dictionary of protein secondary structure: pattern\nrecognition of hydrogen-bonded and geometrical features. Biopolymers, 22(12):2577–2637,\n1983.\nHyun Min Kang, Jae Hoon Sul, Noah A Zaitlen, Sit-yee Kong, Nelson B Freimer, Chiara\nSabatti, Eleazar Eskin, et al. Variance component model to account for sample structure\nin genome-wide association studies. Nature genetics, 42(4):348–354, 2010.\nRosa Karli´c, Ho-Ryun Chung, Julia Lasserre, Kristian Vlahoviˇcek, and Martin Vingron.\nHistone modiﬁcation levels are predictive for gene expression. Proceedings of the National\nAcademy of Sciences, 107(7):2926–2931, 2010.\nJ Kawai, A Shinagawa, K Shibata, M Yoshino, M Itoh, Y Ishii, T Arakawa, A Hara,\nY Fukunishi, H Konno, et al. Functional annotation of a full-length mouse cdna collection.\nNature, 409(6821):685–690, 2001.\n37\nYue et al.\nDavid R Kelley. Cross-species regulatory sequence activity prediction. PLoS Comput. Biol.,\n16(7):e1008050, July 2020.\nDavid R. Kelley, Jasper Snoek, and John L. Rinn.\nBasset:\nlearning the regulatory\ncode of the accessible genome with deep convolutional neural networks.\nGenome\nRes, 26(7):990–999, Jul 2016.\nISSN 1088-9051.\ndoi:\n10.1101/gr.200535.115.\nURL\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC4937568/. 27197224[pmid].\nDavid R Kelley, Yakir A Reshef, Maxwell Bileschi, David Belanger, Cory Y McLean, and\nJasper Snoek. Sequential regulatory activity prediction across chromosomes with convo-\nlutional neural networks. Genome Res., 28(5):739–750, May 2018.\nEinat Kidron, Yoav Y Schechner, and Michael Elad.\nPixels that sound.\nIn Computer\nVision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference\non, volume 1, pages 88–95. IEEE, 2005.\nHyunsoo Kim and Haesun Park. Protein secondary structure prediction based on an im-\nproved support vector machines approach. Protein Engineering, 16(8):553–560, 2003.\nDhananjay Kimothi, Akshay Soni, Pravesh Biyani, and James M Hogan. Distributed rep-\nresentations for biological sequence analysis. arXiv preprint arXiv:1608.05949, 2016.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\nDimitrios Kleftogiannis, Panos Kalnis, and Vladimir B Bajic. Deep: a general computa-\ntional framework for predicting enhancers. Nucleic acids research, 43(1):e6–e6, 2014.\nDG Kneller, FE Cohen, and R Langridge. Improvements in protein secondary structure\nprediction by an enhanced neural network. Journal of molecular biology, 214(1):171–182,\n1990.\nHirofumi Kobayashi, Keith C. Cheveralls, Manuel D. Leonetti, and Loic A. Royer. Self-\nsupervised deep learning encodes high-resolution features of protein subcellular localiza-\ntion. Nature Methods, 19(8):995–1003, 2022.\ndoi: 10.1038/s41592-022-01541-z.\nURL\nhttps://doi.org/10.1038/s41592-022-01541-z.\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural information processing systems,\npages 1097–1105, 2012.\nAndriy Kryshtafovych and Krzysztof Fidelis. Protein structure prediction and model quality\nassessment. Drug discovery today, 14(7-8):386–393, 2009.\nAndriy Kryshtafovych, Torsten Schwede, Maya Topf, Krzysztof Fidelis, and John Moult.\nCritical assessment of methods of protein structure prediction (casp)—round xiv. Pro-\nteins: Structure, Function, and Bioinformatics, 89(12):1607–1617, 2021.\nAnshul Kundaje and James Zou.\nClass lecture, cs 273b:\nDeep learning in genomics\nand biomedicine.\nDepartment of Computer Science, Stanford University, 2016.\nURL\nhttps://canvas.stanford.edu/courses/51037.\n38\nDeep Learning for Genomics: A Concise Overview\nAnshul Kundaje, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza\nHeravi-Moussavi, Pouya Kheradpour, Zhizhuo Zhang, Jianrong Wang, Michael J Ziller,\net al. Integrative analysis of 111 reference human epigenomes. Nature, 518(7539):317–330,\n2015.\nJustin Lamb, Emily D Crawford, David Peck, Joshua W Modell, Irene C Blat, Matthew J\nWrobel, Jim Lerner, Jean-Philippe Brunet, Aravind Subramanian, Kenneth N Ross,\net al. The connectivity map: using gene-expression signatures to connect small molecules,\ngenes, and disease. science, 313(5795):1929–1935, 2006.\nJack Lanchantin, Ritambhara Singh, Zeming Lin, and Yanjun Qi. Deep motif: Visualizing\ngenomic sequence classiﬁcations. arXiv preprint arXiv:1605.01133, 2016a.\nJack Lanchantin, Ritambhara Singh, Beilun Wang, and Yanjun Qi.\nDeep gdashboard:\nVisualizing and understanding genomic sequences using deep neural networks. CoRR,\nabs/1608.03644, 2016b. URL http://arxiv.org/abs/1608.03644.\nEric S Lander, Lauren M Linton, Bruce Birren, Chad Nusbaum, Michael C Zody, Jen-\nnifer Baldwin, Keri Devon, Ken Dewar, Michael Doyle, William FitzHugh, et al. Initial\nsequencing and analysis of the human genome. Nature, 409(6822):860–921, 2001.\nAndrew Leaver-Fay, Michael Tyka, Steven M Lewis, Oliver F Lange, James Thompson,\nRon Jacak, Kristian W Kaufman, P Douglas Renfrew, Colin A Smith, Will Sheﬄer,\net al. Rosetta3: an object-oriented software suite for the simulation and design of macro-\nmolecules. In Methods in enzymology, volume 487, pages 545–574. Elsevier, 2011.\nYann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard,\nWayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-\npropagation network. In Advances in neural information processing systems, pages 396–\n404, 1990.\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\nDeep learning.\nNature, 521(7553):\n436–444, 2015.\nTaehoon Lee and Sungroh Yoon.\nBoosted categorical restricted boltzmann machine for\ncomputational prediction of splice junctions. In International Conference on Machine\nLearning, pages 2483–2492, 2015.\nPietro D Lena, Ken Nagata, and Pierre F Baldi. Deep spatio-temporal architectures and\nlearning for protein structure prediction. In Advances in neural information processing\nsystems, pages 512–520, 2012.\nMichael KK Leung, Hui Yuan Xiong, Leo J Lee, and Brendan J Frey. Deep learning of the\ntissue-regulated splicing code. Bioinformatics, 30(12):i121–i129, 2014.\nMichael KK Leung, Andrew Delong, Babak Alipanahi, and Brendan J Frey. Machine learn-\ning in genomic medicine: a review of computational problems and data sets. Proceedings\nof the IEEE, 104(1):176–197, 2016.\n39\nYue et al.\nYifeng Li, Chih-yu Chen, Alice M Kaye, and Wyeth W Wasserman. The identiﬁcation of\ncis-regulatory elements: A review from a machine learning perspective. Biosystems, 138:\n6–17, 2015a.\nYifeng Li, Chih-Yu Chen, and Wyeth W Wasserman. Deep feature selection: Theory and\napplication to identify enhancers and promoters. In RECOMB, pages 205–217, 2015b.\nYifeng Li, Wenqiang Shi, and Wyeth W Wasserman.\nGenome-wide prediction of cis-\nregulatory regions using supervised deep learning methods. bioRxiv, page 041616, 2016a.\nYifeng Li, Fang-Xiang Wu, and Alioune Ngom. A review on machine learning principles for\nmulti-view biological data integration. Brieﬁngs in bioinformatics, page bbw113, 2016b.\nYingming Li, Ming Yang, and Zhongfei Zhang.\nMulti-view representation learning: A\nsurvey from shallow methods to deep methods. arXiv preprint arXiv:1610.01206, 2016c.\nZhen Li and Yizhou Yu. Protein secondary structure prediction using cascaded convolutional\nand recurrent neural networks. arXiv preprint arXiv:1604.07176, 2016.\nMuxuan Liang, Zhizhong Li, Ting Chen, and Jianyang Zeng.\nIntegrative data analysis\nof multi-platform cancer data with a multimodal deep learning approach. IEEE/ACM\nTransactions on Computational Biology and Bioinformatics (TCBB), 12(4):928–937,\n2015.\nLi Liao and William Staﬀord Noble. Combining pairwise sequence similarity and support\nvector machines for detecting remote protein evolutionary and structural relationships.\nJournal of computational biology, 10(6):857–868, 2003.\nMaxwell Wing Libbrecht. Understanding human genome regulation through entropic graph-\nbased regularization and submodular optimization. PhD thesis, 2016.\nPek S Lim, Kristine Hardy, Karen L Bunting, Lina Ma, Kaiman Peng, Xinxin Chen, and\nMary F Shannon. Deﬁning the chromatin signature of inducible genes in t cells. Genome\nbiology, 10(10):R107, 2009.\nZeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita\nSmetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al.\nEvolutionary-scale pre-\ndiction of atomic-level protein structure with a language model.\nScience, 379(6637):\n1123–1130, 2023.\nChristoph Lippert, Jennifer Listgarten, Ying Liu, Carl M Kadie, Robert I Davidson, and\nDavid Heckerman. Fast linear mixed models for genome-wide association studies. Nature\nmethods, 8(10):833–835, 2011.\nBin Liu, Junjie Chen, and Shumin Li. Protein remote homology detection based on bidi-\nrectional long short-term memory. BMC bioinformatics, 18(1):443, 2017.\nFeng Liu, Hao Li, Chao Ren, Xiaochen Bo, and Wenjie Shu. Pedla: predicting enhancers\nwith a deep learning-based algorithmic framework. Scientiﬁc reports, 6:28517, 2016a.\n40\nDeep Learning for Genomics: A Concise Overview\nJian Liu, Tianqi Wu, Zhiye Guo, Jie Hou, and Jianlin Cheng. Improving protein tertiary\nstructure prediction by deep learning and distance prediction in casp14. Proteins: Struc-\nture, Function, and Bioinformatics, 90(1):58–72, 2022.\nTong Liu, Yiheng Wang, Jesse Eickholt, and Zheng Wang. Benchmarking deep networks\nfor predicting residue-speciﬁc quality of individual protein models in casp11. Scientiﬁc\nreports, 6:19301, 2016b.\nLoredana Lo Conte, Bart Ailey, Tim JP Hubbard, Steven E Brenner, Alexey G Murzin,\nand Cyrus Chothia. Scop: a structural classiﬁcation of proteins database. Nucleic acids\nresearch, 28(1):257–259, 2000.\nChristos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling.\nCausal eﬀect inference with deep latent-variable models. In Advances in Neural Informa-\ntion Processing Systems, pages 6449–6459, 2017.\nJie Lu, Vahid Behbood, Peng Hao, Hua Zuo, Shan Xue, and Guangquan Zhang. Transfer\nlearning using computational intelligence: a survey. Knowledge-Based Systems, 80:14–23,\n2015.\nLaurens Maaten, Max Welling, and Lawrence K Saul.\nHidden-unit conditional random\nﬁelds. In International Conference on Artiﬁcial Intelligence and Statistics, pages 479–\n488, 2011.\nChristophe N Magnan and Pierre Baldi. Sspro/accpro 5: almost perfect prediction of protein\nsecondary structure and relative solvent accessibility using proﬁles, machine learning and\nstructural similarity. Bioinformatics, 30(18):2592–2597, 2014.\nSherri Matis, Ying Xu, Manesh Shah, Xiaojun Guan, J Ralph Einstein, Richard Mural, and\nEdward Uberbacher. Detection of rna polymerase ii promoters and polyadenylation sites\nin human dna sequence. Computers & chemistry, 20(1):135–140, 1996.\nSuyu Mei. Probability weighted ensemble transfer learning for predicting interactions be-\ntween hiv-1 and human proteins. PLoS One, 8(11):e79606, 2013.\nJohn Meinken, Jack Min, et al. Computational prediction of protein subcellular locations\nin eukaryotes: an experience report. Computational Molecular Biology, 2(1), 2012.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013a.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and JeﬀDean. Distributed\nrepresentations of words and phrases and their compositionality. In Advances in neural\ninformation processing systems, pages 3111–3119, 2013b.\nSeonwoo Min, Byunghan Lee, and Sungroh Yoon. Deep learning in bioinformatics. Brieﬁngs\nin bioinformatics, 18(5):851–869, 2017.\nXu Min, Ning Chen, Ting Chen, and Rui Jiang. Deepenhancer: Predicting enhancers by\nconvolutional neural networks. In Bioinformatics and Biomedicine (BIBM), 2016 IEEE\nInternational Conference on, pages 637–644. IEEE, 2016.\n41\nYue et al.\nMarit\nMitchell.\nDeep\ngenomics\napplies\nmachine\nlearn-\ning\nto\ndevelop\nnew\ngenetic\nmedicines,\n2017.\nURL\nhttp://news.engineering.utoronto.ca/deep-genomics-applies-machine-learning-develop-new-ge\nSeungwhan Moon, Suyoun Kim, and Haohan Wang. Multimodal transfer deep learning\nwith applications in audio-visual recognition. arXiv preprint arXiv:1412.3121, 2014.\nCatherine Mooney, Yong-Hong Wang, and Gianluca Pollastri. Sclpred: protein subcellu-\nlar localization prediction by n-to-1 neural networks. Bioinformatics, 27(20):2812–2819,\n2011.\nNature.\nGene\nexpression.\nNature\nEducation,\n2010.\nURL\nhttps://www.nature.com/scitable/nated/topicpage/gene-expression-14121669.\nPatrick Ng. dna2vec: Consistent vector representations of variable-length k-mers. arXiv\npreprint arXiv:1701.06279, 2017.\nEric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael\nWornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano\nErmon, Stephen A. Baccus, and Chris R´e.\nHyenadna: Long-range genomic sequence\nmodeling at single nucleotide resolution, 2023.\nSon P Nguyen, Yi Shang, and Dong Xu. Dl-pro: A novel deep learning method for pro-\ntein model quality assessment. In Neural Networks (IJCNN), 2014 International Joint\nConference on, pages 2071–2078. IEEE, 2014.\nJakob Nybo Nissen, Joachim Johansen, Rosa Lundbye Allesøe, Casper Kaae Sønderby, Jose\nJuan Almagro Armenteros, Christopher Heje Grønbech, Lars Juhl Jensen, Henrik Bjørn\nNielsen, Thomas Nordahl Petersen, Ole Winther, et al. Improved metagenome binning\nand assembly using deep variational autoencoders. Nature biotechnology, 39(5):555–560,\n2021.\nR OpenAI. Gpt-4 technical report. arXiv, pages 2303–08774, 2023.\nR. Onur ¨Oztornaci, Hamzah Syed, Andrew P. Morris, and Bahar Ta¸sdelen. The use of\nclass imbalanced learning methods on ulsam data to predict the case-control status in\ngenome-wide association studies. bioRxiv, 2023. doi: 10.1101/2023.01.05.522884. URL\nhttps://www.biorxiv.org/content/early/2023/01/06/2023.01.05.522884.\nSinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on\nknowledge and data engineering, 22(10):1345–1359, 2010.\nXiaoyong Pan and Hong-Bin Shen. Rna-protein binding motifs mining with a new hybrid\ndeep learning based cross-domain knowledge integration approach. BMC bioinformatics,\n18(1):136, 2017.\nDae-Won Park, Hyoung-Sam Heo, Hyuk-Chul Kwon, and Hea-Young Chung. Protein func-\ntion classiﬁcation based on gene ontology. Information Retrieval Technology, pages 691–\n696, 2005.\n42\nDeep Learning for Genomics: A Concise Overview\nTanel P¨arnamaa and Leopold Parts. Accurate classiﬁcation of protein subcellular localiza-\ntion from high-throughput microscopy images using deep learning. G3: Genes, Genomes,\nGenetics, 7(5):1385–1392, 2017.\nLinus Pauling, Robert B Corey, and Herman R Branson. The structure of proteins: two\nhydrogen-bonded helical conﬁgurations of the polypeptide chain. Proceedings of the Na-\ntional Academy of Sciences, 37(4):205–211, 1951.\nWilliam R Pearson and David J Lipman. Improved tools for biological sequence comparison.\nProceedings of the National Academy of Sciences, 85(8):2444–2448, 1988.\nAndrea Pierleoni, Pier Luigi Martelli, Piero Fariselli, and Rita Casadio. Bacello: a balanced\nsubcellular localization predictor. Bioinformatics, 22(14):e408–e416, 2006.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus,\nYoshua Bengio, Stefano Ermon, and Christopher R´e. Hyena hierarchy: Towards larger\nconvolutional language models, 2023.\nGianluca Pollastri, Darisz Przybylski, Burkhard Rost, and Pierre Baldi.\nImproving the\nprediction of protein secondary structure in three and eight classes using recurrent neural\nnetworks and proﬁles. Proteins: Structure, Function, and Bioinformatics, 47(2):228–235,\n2002.\nYanjun Qi, Oznur Tastan, Jaime G Carbonell, Judith Klein-Seetharaman, and Jason We-\nston. Semi-supervised multi-task learning for predicting interactions between hiv-1 and\nhuman proteins. Bioinformatics, 26(18):i645–i652, 2010.\nNing Qian and Terrence J Sejnowski. Predicting the secondary structure of globular proteins\nusing neural network models. Journal of molecular biology, 202(4):865–884, 1988.\nQian Qin and Jianxing Feng. Imputation for transcription factor binding predictions based\non deep learning. PLoS computational biology, 13(2):e1005403, 2017.\nDaniel Quang and Xiaohui Xie.\nDanq:\na hybrid convolutional and recurrent deep\nneural network for quantifying the function of dna sequences.\nNucleic Acids Res,\n44(11):e107–e107, Jun 2016.\nISSN 0305-1048.\ndoi:\n10.1093/nar/gkw226.\nURL\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC4914104/. 27084946[pmid].\nElakkiya R., Deepak Kumar Jain, Ketan Kotecha, Sharnil Pandya, Sai Siddhartha Reddy,\nRajalakshmi E., Vijayakumar Varadarajan, Aniket Mahanti, and Subramaniyaswamy V.\nHybrid deep neural network for handling data imbalance in precursor microrna. Frontiers\nin Public Health, 9, 2021. doi: 10.3389/fpubh.2021.821410.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n43\nYue et al.\nColin Raﬀel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu.\nExploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer.\nArXiv, abs/1910.10683, 2019.\nURL\nhttps://api.semanticscholar.org/CorpusID:204838007.\nLadislav Rampasek and Anna Goldenberg. Dr. vae: Drug response variational autoencoder.\narXiv preprint arXiv:1706.08203, 2017.\nHuzefa Rangwala and George Karypis. Proﬁle-based direct kernels for remote homology\ndetection and fold recognition. Bioinformatics, 21(23):4239–4247, 2005.\nSabrina Rashid, Sohrab Shah, Ziv Bar-Joseph, and Ravi Pandya. Dhaka: variational au-\ntoencoder for unmasking tumor heterogeneity from single cell genomic data. Bioinfor-\nmatics, 37(11):1535–1543, 2021.\nArjun Ray, Erik Lindahl, and Bj¨orn Wallner. Improved model quality assessment using\nproq2. BMC bioinformatics, 13(1):224, 2012.\nAdam J Riesselman, John B Ingraham, and Debora S Marks. Deep generative models of\ngenetic variation capture mutation eﬀects. arXiv preprint arXiv:1712.06527, 2017.\nSalah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive\nauto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th\nInternational Conference on International Conference on Machine Learning, pages 833–\n840. Omnipress, 2011.\nSøren Kamaric Riis and Anders Krogh. Improving prediction of protein secondary structure\nusing structured neural networks and multiple sequence alignments. Journal of Compu-\ntational Biology, 3(1):163–183, 1996.\nAlexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C. Lawrence\nZitnick, Jerry Ma, and Rob Fergus.\nBiological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nProceedings of the\nNational Academy of Sciences of the United States of America, 118, 2019.\nURL\nhttps://api.semanticscholar.org/CorpusID:155162335.\nBurkhard Rost and Chris Sander. Prediction of protein secondary structure at better than\n70% accuracy. Journal of molecular biology, 232(2):584–599, 1993a.\nBurkhard Rost and Chris Sander. Improved prediction of protein secondary structure by\nuse of sequence proﬁles and neural networks. Proceedings of the National Academy of\nSciences, 90(16):7558–7562, 1993b.\nBurkhard Rost, Chris Sander, and Reinhard Schneider. Redeﬁning the goals of protein\nsecondary structure prediction. Journal of molecular biology, 235(1):13–26, 1994.\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098, 2017.\n44\nDeep Learning for Genomics: A Concise Overview\nKiersten M Ruﬀand Rohit V Pappu. Alphafold and implications for intrinsically disordered\nproteins. Journal of Molecular Biology, 433(20):167208, 2021.\nDavid E Rumelhart, Geoﬀrey E Hinton, and Ronald J Williams. Learning internal repre-\nsentations by error propagation. Technical report, California Univ San Diego La Jolla\nInst for Cognitive Science, 1985.\nScott C Schmidler, Jun S Liu, and Douglas L Brutlag. Bayesian segmentation of protein\nsecondary structure. Journal of computational biology, 7(1-2):233–248, 2000.\nJacob Schreiber, Maxwell Libbrecht, Jeﬀrey Bilmes, and William Noble. Nucleotide se-\nquence and dnasei sensitivity are predictive of 3d chromatin architecture. bioRxiv, page\n103614, 2017.\nJohn Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng,\nJuan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al.\nChatgpt:\nOptimizing language models for dialogue. OpenAI blog, 2022.\nMike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Trans-\nactions on Signal Processing, 45(11):2673–2681, 1997.\nGabriele Schweikert, Gunnar R¨atsch, Christian Widmer, and Bernhard Sch¨olkopf.\nAn\nempirical analysis of domain adaptation algorithms for genomic sequence analysis. In\nAdvances in Neural Information Processing Systems, pages 1433–1440, 2009.\nAndrew W. Senior, Richard Evans, John M. Jumper, James Kirkpatrick, L. Sifre, Tim\nGreen, Chongli Qin, Augustin Z´ıdek, Alexander W. R. Nelson, Alex Bridgland, Hugo\nPenedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T.\nJones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved protein struc-\nture prediction using potentials from deep learning. Nature, 577:706–710, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:210221987.\nManu Setty and Christina S Leslie.\nSeqgl identiﬁes context-dependent binding signals\nin genome-wide regulatory element maps. PLoS computational biology, 11(5):e1004271,\n2015.\nWei Shao,\nTongxin Wang,\nLiang Sun,\nTianhan Dong,\nZhi Han,\nZhi Huang,\nJie\nZhang, Daoqiang Zhang, and Kun Huang.\nMulti-task multi-modal learning for\njoint diagnosis and prognosis of human cancers.\nMedical Image Analysis, 65:101795,\n2020.\nISSN 1361-8415.\ndoi:\nhttps://doi.org/10.1016/j.media.2020.101795.\nURL\nhttps://www.sciencedirect.com/science/article/pii/S1361841520301596.\nHossein Shariﬁ-Noghabi, Yang Liu, Nicholas Erho, Raunak Shrestha, Mohammed Alsha-\nlalfa, Elai Davicioni, Colin C Collins, and Martin Ester. Deep genomic signature for early\nmetastasis prediction in prostate cancer. bioRxiv, page 276055, 2018.\nHagit\nShatkay,\nAnnette\nH¨oglund,\nScott\nBrady,\nTorsten\nBlum,\nPierre\nD¨onnes,\nand\nOliver\nKohlbacher.\nSherloc:\nhigh-accuracy\nprediction\nof\nprotein\nsubcel-\nlular\nlocalization\nby\nintegrating\ntext\nand\nprotein\nsequence\ndata.\nBioinfor-\n45\nYue et al.\nmatics,\n23(11):1410–1417,\n2007.\ndoi:\n10.1093/bioinformatics/btm115.\nURL\n+http://dx.doi.org/10.1093/bioinformatics/btm115.\nZhen Shen, Qinhu Zhang, Kyungsook Han, and De-Shuang Huang.\nA deep learn-\ning model for rna-protein binding preference prediction based on hierarchical lstm\nand attention network.\nIEEE/ACM Trans. Comput. Biol. Bioinformatics, 19(2):\n753–762,\njul 2020.\nISSN 1545-5963.\ndoi:\n10.1109/TCBB.2020.3007544.\nURL\nhttps://doi.org/10.1109/TCBB.2020.3007544.\nWoong-Hee Shin, Xuejiao Kang, Jian Zhang, and Daisuke Kihara. Prediction of local quality\nof protein structure models considering spatial neighbors in graphical models. Scientiﬁc\nreports, 7:40629, 2017.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Reverse-complement parameter\nsharing improves deep learning models for genomics. bioRxiv, page 103663, 2017.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman.\nDeep inside convolutional\nnetworks: Visualising image classiﬁcation models and saliency maps.\narXiv preprint\narXiv:1312.6034, 2013.\nRitambhara Singh, Jack Lanchantin, Gabriel Robins, and Yanjun Qi. Deepchrome: deep-\nlearning for predicting gene expression from histone modiﬁcations. Bioinformatics, 32\n(17):i639–i648, 2016a.\nRitambhara Singh, Jack Lanchantin, Arshdeep Sekhon, and Yanjun Qi. Attend and predict:\nUnderstanding gene regulation by selective attention on chromatin. Advances in neural\ninformation processing systems, 30, 2017.\nShashank Singh, Yang Yang, Barnabas Poczos, and Jian Ma. Predicting enhancer-promoter\ninteraction from genomic sequence with deep neural networks.\nbioRxiv, page 085241,\n2016b.\nSøren Kaae Sønderby, Casper Kaae Sønderby, Henrik Nielsen, and Ole Winther. Convolu-\ntional lstm networks for subcellular localization of proteins. In International Conference\non Algorithms for Computational Biology, pages 68–80. Springer, 2015.\nMinsun Song, Wei Hao, and John D Storey. Testing for genetic associations in arbitrarily\nstructured populations. Nature genetics, 47(5):550, 2015.\nMatt Spencer, Jesse Eickholt, and Jianlin Cheng. A deep learning network approach to ab\ninitio protein secondary structure prediction. IEEE/ACM transactions on computational\nbiology and bioinformatics, 12(1):103–112, 2015.\nMartin Steinegger, Milot Mirdita, and Johannes S¨oding. Protein-level assembly increases\nprotein sequence recovery from metagenomic samples manyfold. Nature Methods, pages\n1–4, 2018. URL https://api.semanticscholar.org/CorpusID:92596540.\nZachary D Stephens, Skylar Y Lee, Faraz Faghri, Roy H Campbell, Chengxiang Zhai,\nMiles J Efron, Ravishankar Iyer, Michael C Schatz, Saurabh Sinha, and Gene E Robinson.\nBig data: astronomical or genomical? PLoS biology, 13(7):e1002195, 2015.\n46\nDeep Learning for Genomics: A Concise Overview\nAmy O Stevens and Yi He. Benchmarking the accuracy of alphafold 2 in loop structure\nprediction. Biomolecules, 12(7):985, 2022.\nGary D Stormo. Dna binding sites: representation and discovery. Bioinformatics, 16(1):\n16–23, 2000.\nQian Sun, Sherin Muckatira, Lei Yuan, Shuiwang Ji, Stuart Newfeld, Sudhir Kumar, and\nJieping Ye. Image-level and group-level models for drosophila gene expression pattern\nannotation. BMC bioinformatics, 14(1):350, 2013.\nBaris E. Suzek, Yuqi Wang, Hongzhan Huang, Peter B. McGarvey, and Cathy H.\nWu.\nUniref\nclusters:\na\ncomprehensive\nand\nscalable\nalternative\nfor\nimprov-\ning sequence similarity\nsearches.\nBioinformatics,\n31:926\n– 932,\n2014.\nURL\nhttps://api.semanticscholar.org/CorpusID:12423917.\nDaniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal. Introduction to multi-layer feed-\nforward neural networks. Chemometrics and intelligent laboratory systems, 39(1):43–62,\n1997.\nJie Tan, Matthew Ung, Chao Cheng, and Casey S Greene. Unsupervised feature construc-\ntion and knowledge extraction from genome-wide assays of breast cancer with denoising\nautoencoders. In Paciﬁc Symposium on Biocomputing Co-Chairs, pages 132–143. World\nScientiﬁc, 2014.\nJie Tan, John H Hammond, Deborah A Hogan, and Casey S Greene. Adage-based integra-\ntion of publicly available pseudomonas aeruginosa gene expression data with denoising\nautoencoders illuminates microbe-host interactions. mSystems, 1(1):e00025–15, 2016.\nJie Tan, Georgia Doing, Kimberley A Lewis, Courtney E Price, Kathleen M Chen, Kyle C\nCady, Barret Perchuk, Michael T Laub, Deborah A Hogan, and Casey S Greene. Unsu-\npervised extraction of stable expression signatures from public compendia with eadage.\nbioRxiv, page 078659, 2017.\nWen\nTorng\nand\nRuss\nB.\nAltman.\n3d\ndeep\nconvolutional\nneural\nnetworks\nfor\namino\nacid\nenvironment\nsimilarity\nanalysis.\nBMC\nBioinformatics,\n18(1):\n302,\nJun\n2017.\nISSN\n1471-2105.\ndoi:\n10.1186/s12859-017-1702-0.\nURL\nhttps://doi.org/10.1186/s12859-017-1702-0.\nR´emi Torracinta and Fabien Campagne. Training genotype callers with neural networks.\nbioRxiv, page 097469, 2016.\nDustin Tran and David M Blei. Implicit causal models for genome-wide association studies.\narXiv preprint arXiv:1710.10742, 2017.\nStefanos Tsimenidis, Eleni Vrochidou, and George A. Papakostas. Omics Data and Data\nRepresentations for Deep Learning-Based Predictive Modeling.\nInternational Journal\nof Molecular Sciences, 23(20):12272, October 2022.\nISSN 1422-0067.\ndoi: 10.3390/\nijms232012272. URL https://www.mdpi.com/1422-0067/23/20/12272.\n47\nYue et al.\nRamzan Kh Umarov and Victor V Solovyev. Recognition of prokaryotic and eukaryotic\npromoters using convolutional deep learning neural networks. PloS one, 12(2):e0171410,\n2017.\nDaniel Urda, Julio Montes-Torres, Fernando Moreno, Leonardo Franco, and Jos´e M Jerez.\nDeep learning to analyze rna-seq gene expression data. In International Work-Conference\non Artiﬁcial Neural Networks, pages 50–59. Springer, 2017.\nKarolis Uziela, Nanjiang Shu, Bj¨orn Wallner, and Arne Elofsson. Proq3: Improved model\nquality assessments using rosetta energy terms. Scientiﬁc reports, 6:33509, 2016.\nKarolis Uziela, David Men´endez Hurtado, Nanjiang Shu, Bj¨orn Wallner, and Arne Elofsson.\nProq3d: improved model quality assessments using deep learning. Bioinformatics, 33(10):\n1578–1580, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nAdvances in\nneural information processing systems, 30, 2017.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting\nand composing robust features with denoising autoencoders. In Proceedings of the 25th\ninternational conference on Machine learning, pages 1096–1103. ACM, 2008.\nShibiao Wan and Man-Wai Mak. Machine learning for protein subcellular localization pre-\ndiction. Walter de Gruyter GmbH & Co KG, 2015.\nHaohan Wang, Bryon Aragam, and Eric P Xing.\nVariable selection in heterogeneous\ndatasets: A truncated-rank sparse linear mixed model with applications to genome-wide\nassociation studies. bioRxiv, page 228106, 2017a.\nHaohan Wang, Aman Gupta, and Ming Xu. Extracting compact representation of knowl-\nedge from gene expression data for protein-protein interaction. International Journal of\nData Mining and Bioinformatics, 17(4):279–292, 2017b.\nHaohan Wang, Aaksha Meghawat, Louis-Philippe Morency, and Eric P Xing. Select-additive\nlearning: Improving generalization in multimodal sentiment analysis. In Multimedia and\nExpo (ICME), 2017 IEEE International Conference on, pages 949–954. IEEE, 2017c.\nHaohan Wang, Bhiksha Raj, and Eric P Xing. On the origin of deep learning. arXiv preprint\narXiv:1702.07800, 2017d.\nQin Wang, Jiayang Chen, Yuzhe Zhou, Yu Li, Liangzhen Zheng, Sheng Wang, Zhen Li,\nand Shuguang Cui. Contact-distil: Boosting low homologous protein contact map predic-\ntion by self-supervised distillation. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 36, pages 4620–4627, 2022.\nSheng Wang, Jian Peng, Jianzhu Ma, and Jinbo Xu. Protein secondary structure prediction\nusing deep convolutional neural ﬁelds. Sci Rep, 6:18962, Jan 2016a. ISSN 2045-2322. doi:\n10.1038/srep18962. URL http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4707437/.\n26752681[pmid].\n48\nDeep Learning for Genomics: A Concise Overview\nSheng Wang, Siqi Sun, Zhen Li, Renyu Zhang, and Jinbo Xu. Accurate de novo prediction\nof protein contact map by ultra-deep learning model. PLoS computational biology, 13(1):\ne1005324, 2017e.\nYiheng Wang, Tong Liu, Dong Xu, Huidong Shi, Chaoyang Zhang, Yin-Yuan Mo,\nand Zheng Wang.\nPredicting dna methylation state of cpg dinucleotide using\ngenome topological features and deep networks.\n6:19598 EP –, Jan 2016b.\nURL\nhttp://dx.doi.org/10.1038/srep19598. Article.\nZhiqin Wang, Ruiqing Li, Minghui Wang, and Ao Li.\nGPDBN: deep bilinear network\nintegrating both genomic data and pathological images for breast cancer prognosis pre-\ndiction.\nBioinformatics, 37(18):2963–2970, 03 2021.\nISSN 1367-4803.\ndoi: 10.1093/\nbioinformatics/btab185. URL https://doi.org/10.1093/bioinformatics/btab185.\nJonathan J Ward, Liam J McGuﬃn, Bernard F. Buxton, and David T. Jones. Secondary\nstructure prediction with support vector machines. Bioinformatics, 19(13):1650–1655,\n2003.\nWyeth W Wasserman and Albin Sandelin. Applied bioinformatics for the identiﬁcation of\nregulatory elements. Nature Reviews Genetics, 5(4):276–287, 2004.\nJames D Watson, Francis HC Crick, et al. Molecular structure of nucleic acids. Nature, 171\n(4356):737–738, 1953.\nGregory P Way and Casey S Greene. Evaluating deep variational autoencoders trained on\npan-cancer gene expression. arXiv preprint arXiv:1711.04828, 2017a.\nGregory P Way and Casey S Greene. Extracting a biologically relevant latent space from\ncancer transcriptomes with variational autoencoders. bioRxiv, page 174474, 2017b.\nJohn N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Mills Shaw, Brad A Ozen-\nberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, Joshua M Stuart, Cancer Genome\nAtlas Research Network, et al. The cancer genome atlas pan-cancer analysis project.\nNature genetics, 45(10):1113–1120, 2013.\nKarl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning.\nJournal of Big Data, 3(1):9, 2016.\nKonstantin Weissenow, Michael Heinzinger, Martin Steinegger, and Burkhard Rost. Ultra-\nfast protein structure prediction to capture eﬀects of sequence variation in mutation\nmovies. bioRxiv, pages 2022–11, 2022.\nSean Whalen, Rebecca M Truty, and Katherine S Pollard. Enhancer–promoter interactions\nare encoded by complex genomic signatures on looping chromatin. Nature genetics, 48\n(5):488, 2016.\nChristian Widmer and Gunnar R¨atsch. Multitask learning in computational biology. In\nProceedings of ICML Workshop on Unsupervised and Transfer Learning, pages 207–216,\n2012.\n49\nYue et al.\nRuidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su,\nZuofan Wu, Qi Xie, Bonnie Berger, et al. High-resolution de novo structure prediction\nfrom primary sequence. BioRxiv, pages 2022–07, 2022.\nTianqi Wu, Zhiye Guo, Jie Hou, and Jianlin Cheng.\nDeepdist: real-value inter-residue\ndistance prediction with deep residual convolutional network. BMC bioinformatics, 22:\n1–17, 2021.\nZhenglin Wu, Haohan Wang, Mingze Cao, Yin Chen, and Eric P Xing. Fair deep learn-\ning prediction for healthcare applications with confounder ﬁltering.\narXiv preprint\narXiv:1803.07276, 2018.\nRui Xie, Jia Wen, Andrew Quitadamo, Jianlin Cheng, and Xinghua Shi. A deep auto-\nencoder model for gene expression prediction. BMC genomics, 18(9):845, 2017.\nHui Y. Xiong, Babak Alipanahi, Leo J. Lee, Hannes Bretschneider, Daniele Merico,\nRyan KC Yuen, Yimin Hua, Serge Gueroussov, Hamed S. Najafabadi, Timothy R.\nHughes, Quaid Morris, Yoseph Barash, Adrian R. Krainer, Nebojsa Jojic, Stephen W.\nScherer, Benjamin J. Blencowe, and Brendan J. Frey.\nThe human splicing code\nreveals new insights into the genetic determinants of disease.\nScience, 347(6218):\n1254806–1254806, Jan 2015.\nISSN 0036-8075.\ndoi:\n10.1126/science.1254806.\nURL\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC4362528/. 25525159[pmid].\nHui Yuan Xiong, Yoseph Barash, and Brendan J Frey.\nBayesian prediction of tissue-\nregulated splicing using rna sequence and cellular context. Bioinformatics, 27(18):2554–\n2562, 2011.\nQian Xu and Qiang Yang. A survey of transfer and multitask learning in bioinformatics.\nJournal of Computing Science and Engineering, 5(3):257–268, 2011.\nPing Xuan, Yangkun Cao, Tiangang Zhang, Rui Kong, and Zhaogong Zhang. Dual convo-\nlutional neural networks with attention mechanisms based method for predicting disease-\nrelated lncrna genes. Frontiers in genetics, 10:416, 2019.\nBite Yang, Feng Liu, Chao Ren, Zhangyi Ouyang, Ziwei Xie, Xiaochen Bo, and Wenjie Shu.\nBiren: predicting enhancers with a deep-learning-based model using the dna sequence\nalone. Bioinformatics, 33(13):1930–1936, 2017.\nFan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu,\nand Jianhua Yao. scbert as a large-scale pretrained deep language model for cell type\nannotation of single-cell rna-seq data. Nature Machine Intelligence, 4(10):852–866, 2022.\nJian Yang, Noah A Zaitlen, Michael E Goddard, Peter M Visscher, and Alkes L Price.\nAdvantages and pitfalls in the application of mixed-model association methods. Nature\ngenetics, 46(2):100–106, 2014.\nJianyi Yang, Ivan Anishchenko, Hahnbeom Park, Zhenling Peng, Sergey Ovchinnikov, and\nDavid Baker. Improved protein structure prediction using predicted interresidue orienta-\ntions. Proceedings of the National Academy of Sciences, 117(3):1496–1503, 2020.\n50\nDeep Learning for Genomics: A Concise Overview\nWanjuan Yang, Jorge Soares, Patricia Greninger, Elena J. Edelman, Howard Light-\nfoot, Simon Forbes, Nidhi Bindal, Dave Beare, James A. Smith, I. Richard Thomp-\nson, Sridhar Ramaswamy, P. Andrew Futreal, Daniel A. Haber, Michael R. Stratton,\nCyril Benes, Ultan McDermott, and Mathew J. Garnett.\nGenomics of drug sensitiv-\nity in cancer (gdsc):\na resource for therapeutic biomarker discovery in cancer cells.\nNucleic Acids Research, 41(D1):D955–D961, 2013.\ndoi:\n10.1093/nar/gks1111.\nURL\n+http://dx.doi.org/10.1093/nar/gks1111.\nKa Yee Yeung and Walter L. Ruzzo.\nPrincipal component analysis for clustering gene\nexpression data. Bioinformatics, 17(9):763–774, 2001.\nKihoon Yoon and Stephen Kwek.\nAn unsupervised learning approach to resolving the\ndata imbalanced issue in supervised learning problems in functional genomics. In Hybrid\nIntelligent Systems, 2005. HIS’05. Fifth International Conference on, pages 6–pp. IEEE,\n2005.\nJianming Yu, Gael Pressoir, William H Briggs, Irie Vroh Bi, Masanori Yamasaki, John F\nDoebley, Michael D McMullen, Brandon S Gaut, Dahlia M Nielsen, James B Holland,\net al. A uniﬁed mixed-model method for association mapping that accounts for multiple\nlevels of relatedness. Nature genetics, 38(2):203–208, 2006.\nYuan Yuan, Lei Guo, Lei Shen, and Jun S Liu. Predicting gene expression from sequence:\na reexamination. PLoS computational biology, 3(11):e243, 2007.\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.\nIn European conference on computer vision, pages 818–833. Springer, 2014.\nAdam Zemla, ˇCeslovas Venclovas, Krzysztof Fidelis, and Burkhard Rost. A modiﬁed deﬁ-\nnition of sov, a segment-based measure for protein secondary structure prediction assess-\nment. Proteins: Structure, Function, and Bioinformatics, 34(2):220–223, 1999.\nHaoyang Zeng, Matthew D Edwards, Ge Liu, and David K Giﬀord. Convolutional neural\nnetwork architectures for predicting dna–protein binding. Bioinformatics, 32(12):i121–\ni127, 2016.\nHaicang Zhang and Yufeng Shen. Template-based prediction of protein structure with deep\nlearning. BMC genomics, 21(11):1–9, 2020.\nLe Zhang, Jiayang Chen, Tao Shen, Yu Li, and Siqi Sun.\nEnhancing the protein ter-\ntiary structure prediction by multiple sequence alignment generation.\narXiv preprint\narXiv:2306.01824, 2023.\nSai Zhang, Jingtian Zhou, Hailin Hu, Haipeng Gong, Ligong Chen, Chao Cheng, and\nJianyang Zeng. A deep learning framework for modeling structural features of rna-binding\nprotein targets. Nucleic acids research, 44(4):e32–e32, 2015.\nWenlu Zhang, Rongjian Li, Tao Zeng, Qian Sun, Sudhir Kumar, Jieping Ye, and Shuiwang\nJi. Deep model based transfer and multi-task learning for biological image analysis. IEEE\nTransactions on Big Data, 2016.\n51\nYue et al.\nYan Zhang, Lin An, Ming Hu, Jijun Tang, and Feng Yue. Hicplus: Resolution enhancement\nof hi-c interaction heatmap. bioRxiv, page 112631, 2017.\nJian Zhou and Olga G Troyanskaya. Deep supervised and convolutional generative stochas-\ntic network for protein secondary structure prediction. In International Conference on\nMachine Learning, pages 745–753, 2014.\nJian Zhou and Olga G Troyanskaya. Predicting eﬀects of noncoding variants with deep\nlearning-based sequence model. Nature methods, 12(10):931–934, 2015.\nJian Zhou, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K. Wong, and\nOlga G. Troyanskaya. Deep learning sequence-based ab initio prediction of variant eﬀects\non expression and disease risk. Nature Genetics, 50(8):1171–1179, August 2018. ISSN\n1061-4036. doi: 10.1038/s41588-018-0160-6.\nZhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu.\nDnabert-2: Eﬃcient foundation model and benchmark for multi-species genome. arXiv\npreprint arXiv:2306.15006, 2023.\nMax T Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco\nBohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, et al. Genslms:\nGenome-scale language models reveal sars-cov-2 evolutionary dynamics. bioRxiv, 2022.\n52\n",
  "categories": [
    "q-bio.GN",
    "cs.LG"
  ],
  "published": "2018-02-02",
  "updated": "2023-10-04"
}