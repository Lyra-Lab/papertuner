{
  "id": "http://arxiv.org/abs/1802.01034v1",
  "title": "Multi-task Learning for Continuous Control",
  "authors": [
    "Himani Arora",
    "Rajath Kumar",
    "Jason Krone",
    "Chong Li"
  ],
  "abstract": "Reliable and effective multi-task learning is a prerequisite for the\ndevelopment of robotic agents that can quickly learn to accomplish related,\neveryday tasks. However, in the reinforcement learning domain, multi-task\nlearning has not exhibited the same level of success as in other domains, such\nas computer vision. In addition, most reinforcement learning research on\nmulti-task learning has been focused on discrete action spaces, which are not\nused for robotic control in the real-world. In this work, we apply multi-task\nlearning methods to continuous action spaces and benchmark their performance on\na series of simulated continuous control tasks. Most notably, we show that\nmulti-task learning outperforms our baselines and alternative knowledge sharing\nmethods.",
  "text": "Multi-task Learning for Continuous Control\nHimani Arora * 1 Rajath Kumar * 1 Jason Krone * 1 Chong Li 1\nAbstract\nReliable and effective multi-task learning is\na prerequisite for the development of robotic\nagents that can quickly learn to accomplish re-\nlated, everyday tasks. However, in the reinforce-\nment learning domain, multi-task learning has\nnot exhibited the same level of success as in other\ndomains, such as computer vision. In addition,\nmost reinforcement learning research on multi-\ntask learning has been focused on discrete ac-\ntion spaces, which are not used for robotic con-\ntrol in the real-world. In this work, we apply\nmulti-task learning methods to continuous action\nspaces and benchmark their performance on a se-\nries of simulated continuous control tasks. Most\nnotably, we show that multi-task learning outper-\nforms our baselines and alternative knowledge\nsharing methods.\n1. Introduction\nCurrently, reinforcement learning algorithms are sample\ninefﬁcient and learn from scratch through trail and error\nover millions of rollouts. This sample inefﬁciency is not\na problem when the goal is to maximize performance on\na single task in a simulated environment, where data is\ncheap and can be collected quickly. However, this inefﬁ-\nciency is not viable in real-world use cases when the goal is\nfor an agent to accomplish many tasks, which may change\nover time. In addition to developing more efﬁcient algo-\nrithms, one solution to this problem is to share knowledge\nbetween multiple tasks and develop ﬂexible representations\nthat can easily transfer to new tasks. In this work, we take a\nprerequisite step in this direction by evaluating the perfor-\nmance of the state-of-the-art multi-task learning methods\non continuous action spaces using an extended version of\nthe MuJoCo environment (Henderson et al., 2017). Ana-\nlyzing the success of various multi-task methods on con-\n*Equal contribution\n1Columbia University,\nUSA. Cor-\nrespondence\nto:\nJason\nKrone\n<jpk2151@columbia.edu>,\nHimani\nArora\n<ha2434@columbia.edu>,\nRajath\nKumar\n<rm3497@columbia.edu>.\ntinuous control tasks is an important contribution because\nmost research in this area has been on discrete action spaces\nin Atari environments.\n2. Related Work\nIn recent years, a number of works, most consistently from\nDeep Mind, have proposed methods for transfer learn-\ning and muli-task learning.\nThese works primarily fo-\ncus on two approaches: knowledge distillation and feature\nreuse. Knowledge distillation, originally proposed in (Bu-\ncila et al., 2006), serves as the foundation for the meth-\nods put forward in (Rusu et al., 2016a), (Parisotto et al.,\n2016), and (Teh et al., 2017). (Rusu et al., 2016a) extends\nthe distillation method, formulated in (Hinton et al., 2014),\nto Deep Q Networks trained on Atari environments and\ndemonstrates that policy distillation can act as a form of\nregularization for Deep Q Networks. In (Parisotto et al.,\n2016), the authors propose a novel loss function that in-\ncludes both a policy regression term as well as a feature\nregression term. This policy regression objective is tradi-\ntionally used in distillation, while the added regression ob-\njective encourages feature representations in the interme-\ndiate layers of the student network to match those of the\nexpert network. (Teh et al., 2017) applies distillation to\nthe multi-task setting by learning a common (distilled) pol-\nicy across a number of 3D environments. In addition, (Teh\net al., 2017) adds an entropy penalty as well as KL and\nentropy regularization coefﬁcients to the objective in order\nto trade off between exploration and exploitation. (Rusu\net al., 2016b) attack the problem of catastrophic forgetting,\nwhere a policy losses the ability to preform a pre-transfer\ntask after being transferred to a target task. Concretely, the\nauthors prevent catastrophic forgetting by maintaining task\nspeciﬁc representations within the policy network. The en-\nvironments we use in our experiments were introduced in\n(Henderson et al., 2017). These environments extend the\nMujoco continuous control tasks available in Open AI Gym\nand are designed to be a test bed for transfer learning and\nmulti-task learning. For a given simulated agent, these en-\nvironments provide minor structural variations such as the\nlength of an agent’s body parts.\narXiv:1802.01034v1  [cs.LG]  3 Feb 2018\nMulti-task Learning for Continuous Control\nEnvironment 1\nEnvironment 2\nValue\nPolicy\nValue\nPolicy\n(a) Vanilla Multi-task\nEnvironment 1\nEnvironment 2\nStudent\nPolicy\nTeacher\nPolicy\nKL Loss\nKL Loss\nTeacher\nPolicy\nStudent\nPolicy\n(b) Distillation Multi-Task\nFigure 1. Proposed approach for multi-task learning. (a) Vanilla Multi-task (b) Distillation Multi-task. Green– Trainable weights; Dark\nGrey– Fixed Weights; Blue– Shared Weights; Dark Blue– Expert network Weights\n3. Approach\n3.1. Preliminaries\n3.1.1. MARKOV DECISION PROCESS\nTo provide context we give a brief review of the reinforce-\nment learning problem. Reinforcement learning is the pro-\ncess of training an agent to maximize reward in an envi-\nronment. More technically, the aim is to learn the optimal\npolicy for selecting actions to take in a Markov Decision\nProcess (MDP). A MDP is deﬁned as ⟨S, A, P, r, ρ0, γ, T⟩\nwhere S is a set of states, A is a set of actions, P\n:\nS × A × S →R+ is a transition function, r : S ×\nA →[Rmin, Rmax] is a reward function, γ ∈[0, 1] is\na discount factor, and T is a time horizon.\nThe policy\nπ : S × A →R+ is trained to maximize the expected\ndiscounted return η(πθ) = Eτ∼πθ(τ)\nhPT\nt=0 γtr(st, at)\ni\n,\nwhere τ denotes a trajectory τ = (s0, a0, . . . ) sampled ac-\ncording to πθ with s0 ∼ρ0(s0), at ∼πθ(at | st), and\nst+1 ∼P(st+1 | st, at). Furthermore, we deﬁne the opti-\nmal approximate policy as πθ∗(a|s) where,\nθ∗= arg max\nθ\nEτ∼πθ(τ)\n\" T\nX\nt=0\nγtr(st, at)\n#\n(1)\nπθ(τ) = p(s1)\nT\nY\nt\nπθ(at|st)T(st+1|st, at)\n(2)\n3.1.2. ACTOR CRITIC ALGORITHM\nTo learn this optimal approximate policy we use the advan-\ntage actor-critic (A2C) algorithm, which is a synchronous\nimplementation of the A3C algorithm introduced in (Mnih\net al., 2016). A2C is an on-policy algorithm that operates\nin the forward view by sampling rollouts of current pol-\nicy to calculate n-step returns. The policy as well as the\nvalue function are updated after every tmax forward steps or\nwhen a terminal state is reached. This algorithm maintains\na policy πθ and a value function estimate VθV and performs\nupdates of the form ∇θπθ(at|st)Aθ,θV (st, at), where the\nadvantage Aθ,θV (st, at) is deﬁned as\nAθ,θV (st, at) =\nk−1\nX\ni=0\nγirt+i+γkVθV (st+k)−VθV (st) (3)\nWe discourage convergence to sub-optimal deterministic\npolicies can by adding a policy entropy term to the objec-\ntive function as originally proposed in (Williams & Peng,\n1991). To approximate πθ and VθV we use feed forward\nneural networks. Since our experiments deal with contin-\nuous environments, we select actions by sampling from a\nmultivariate Gaussian distribution with mean µ and diag-\nonal co-variance matrix Σ . Thus, the output layer of our\npolicy network consists of a real-valued mean and the log\nvariance for each dimension of the action space.\n3.1.3. KNOWLEDGE DISTILLATION\nThe goal of knowledge distillation is to transfer knowl-\nedge from a teacher model T to a student model S. In\nour experiments, T is a policy πθ trained from scratch\non a single environment using A2C, and S is a feed for-\nward network which has not been trained. S is trained on\nthe dataset D = (X, Y ), where X denotes features and\nY denotes targets. X contains state action pairs (si, ai)\ntaken from trajectories τ = (s0, a0, . . . ) of length tmax,\nwhich are sampled according to the student’s policy πθS\nwith probability ps and according to the teacher’s policy\nπθT with probability pt. Y contains the values µT , ΣT =\nπθT (si, ai) that parameterize the teacher’s policy for the\ngiven state, action pair. We train S on D using the KL\nDivergence DKL between the teacher policy and the stu-\ndent policy as the objective function.\nSpeciﬁcally, we\nuse L = DKL(N(µT , ΣT ), N(µS, ΣS)) since the actions\ntaken by T and S are drawn from multi-variate Gaussians.\nMulti-task Learning for Continuous Control\nDKL is deﬁned as follows:\nDKL(N(µT , ΣT ), N(µS, ΣS)) =\n1\n2\n\u0014\nlog |ΣT |\n|ΣS| −p + tr(Σ−1\nT ΣS)\n\u0015\n+\n1\n2(µT −µS)T Σ−1\nT (µT −µS)\n(4)\nWe choose to use KL Divergence as our loss function be-\ncause it was shown to perform well on discrete action\nspaces in (Rusu et al., 2016a)\n3.1.4. MULTI-TASK LEARNING\nTo goal of multi-task learning is to train a policy net-\nwork πθ that behaves optimally in n different environments\nE1, E2 · · · En. In our multi-task experiments, we approx-\nimate the optimal policy using an actor network that is es-\nsentially a feed forward network that contains two hidden\nlayers shared across all environments and n output layers\n(heads), where head hi produces the mean µi and covari-\nance Σi that parameterize the Gaussian policy for environ-\nment Ei. We experiment with two methods for training πθ\non multiple tasks:\n• Vanilla multi-task learning: Each head hi is trained\nusing A2C to maximize the expected discounted re-\nturn for environment Ei. The value network in this\ncase consists of one shared hidden layer and n output\nhead. During training, we sample an equal number\nof rollouts by cycling between environment E1 to en-\nvironment En. We provide an illustration of vanilla\nmulti-task in part (a) of ﬁgure 1.\n• Muti-task distillation: Each head hi is trained using\nknowledge distillation to match the output of a teacher\nnetwork Ti. The Multi-task distillation training pro-\ncess is identical to the knowledge distilation process\nexcept that a dataset Di is collected for each teacher,\nhead pair (Ti, hi) where the rollouts are sampled from\nthe student network.\nWe provide an illustration of\nmuti-task distillation in part (b) of ﬁgure 1.\nThe above 2 methods for multi-task learning are illustrated\nin ﬁgure 1.\n4. Experiments\nWe conducted our experiments using the half-cheetah\nagent\non\n6\nmorphologically\nmodiﬁed\nvariants\nof\nthe Open AI gym extensions described in (Hender-\nson\net\nal.,\n2017)\nnamely\nHalfCheetahSmallFoot-v0,\nHalfCheetahSmallLeg-v0,\nHalfCheetahSmallTorso-v0\nand HalfCheetahSmallThigh-v0 which reduce the size\nof the agent’s respective body part by 25% as well as\non HalfCheetahBigFoot-v0 and HalfCheetahBigTorso-v0\nwhich increase the size of the agent’s respective body\npart by 25%. We evaluate the performance of a trained\npolicy by reporting the mean and standard deviation of\nthe cumulative reward across 20 sample rollouts on each\ntarget environment as done in (Henderson et al., 2017).\nIn addition, we plot the learning curves for each method\nin order to determine the sample efﬁciency of these\napproaches, which we provide in the appendix.\n4.1. Implementation\nWe use PyTorch to implement all our models.\nOur ac-\ntor network and critic networks consist of 2 and 3 fully-\nconnected layers respectively, each of which have 64 hid-\nden units. Because the Mujoco environments we use are for\ncontinuous control each action taken by an agent is sampled\nfrom a Gaussian distribution parameterized by the mean\nand variance given by πθ. We use RMSprop with an initial\nlearning rate of 0.0007 to train our models. We set the A2C\nhyper parameter tmax = 5 for all of our experiments. In\naddition, we use an entropy penalty coefﬁcient of 0.01.\n4.2. Fine-tuning\nTraining\nSteps\nE1\nE2\nScratch\n5M\n1582.78±30.46\n1536.16±30.26\nE1 →E2\n1M\n1530.26±29.22\n1532.70 ± 23.35\nE1 →E2\n5M\n1500.76±30.36\n1568.66 ± 29.56\nE2 →E1\n1M\n1600.31 ± 26.32\n1498.45±19.76\nE2 →E1\n5M\n1493.02 ± 27.21\n1425.64±29.82\nTable 1. E1 and E2 refer to HalfCheetahSmallFoot-v0 and\nHalfCheetahSmallLeg-v0 respectively. E1 →E2 denotes ﬁne-\ntuning the weights of E1 on E2 and vice-versa for E2 →E1.\nAverage and standard deviation (µ ± σ) of reward is calculated\nacross a set of 20 sample rollouts. Steps is denoted in millions.\nOne of the simplest methods for multi-task learning is ﬁne-\ntuning. For this, we ﬁrst trained a policy with random ini-\ntialization of weights for 5M frames on each environment\nseparately. We then transferred this policy πθ to another\nenvironment by initializing its weights of a new network\nπ′\nθ to the weights used in πθ and then ﬁne-tuning the last\nlayer of the actor and critic networks in π′\nθ for another\n5M frames.\nWe conducted our ﬁne-tuning experiments\non HalfCheetahSmallFoot-v0 and HalfCheetahSmallLeg-\nv0 and evaluated the results both at the 1M and 5M mark.\nThe learning curves for all tasks are shown in Figure 2.The\nmean and standard deviation of the accumulated rewards\ncalculated on 20 rollouts of the policy are tabulated in Table\n1 for each combination of original and target environment.\nFrom the table it is clear that although ﬁne-tuning the\nMulti-task Learning for Continuous Control\nEnvironment\nScratch (3M)\nDistillation Multi-task (1M)\nVanilla Multi-task (1M)\nHalfCheetahSmallFoot-v0\n1326.30±21.15\n1348.30±16.65\n1480.89±30.92\nHalfCheetahBigFoot-v0\n37.80±441.49\n-63.07±0.93\n2188.11±21.28\nHalfCheetahSmallLeg-v0\n1420.77±21.66\n1413.55±17.73\n1525.09±25.15\nHalfCheetahSmallThigh-v0\n1444.21±24.75\n838.35±960.78\n1874.04±14.02\nHalfCheetahSmallTorso-v0\n2116.10±29.24\n2116.94±38.00\n2276.11±46.14\nHalfCheetahBigTorso-v0\n1312.50±24.75\n1323.61±29.64\n1428.97±7.06\nTable 2. Results on multi-task network on 6 environments\nweights of a pre-trained network on a new environment\nperforms better than training from scratch, its performance\ndegrades on the original environment. Thus it suffers from\ncatastrophic forgetting, which makes it a poor choice for\nmulti-task learning. For this reason, we did not explore all\ncombinations of original and target environments and in-\nstead focus on other types of multi-task learning, which we\ndiscuss below.\n4.3. Multi-task learning\n4.3.1. VANILLA MULTI-TASK LEARNING\nIn the vanilla multi-task experiment, we train a six head ac-\ntor network and a six head critic network on each environ-\nment. Initial hidden layers are shared across environments,\nwhile output layers are unique to each environment’s head\nas shown in Fig. 1. The training procedure is as follows,\nﬁrst sample rollouts from E1 are collected and the head\ncorresponding to the environment E1 is trained, similarly\nE2 and so on. the environments are continuously cycled in\nthis manner until each head is trained for 1M frames.\nAll results are tabulated in 2. As clearly visible, the vanilla\nmulti-task outperforms not only distillation multi-task but\nalso networks trained from scratch on a single environment.\nThis shows that sharing knowledge across multiple tasks\nhelps the network perform better on each individual task as\nwell re-afﬁrming our original motivation for this work. In\naddition, it also helps train the network faster and achieves\ncomparable performance in just 1M frames as compared to\nthe 3M frames.\n4.3.2. MULTI-TASK DISTILLATION\nFor the multi-task distillation experiment we ﬁrst trained\nteacher networks on all 6 tasks separately for 3M frames\neach. Our multi-task network then consisted only of an\nactor network with shared hidden layers and 6 output head\nlayers unique to each environment Fig. 1. The training pro-\ncedure was similar to the vanilla multi-task except we sam-\nple rollouts from the student policy and used the knowledge\ndistillation loss for training our network.\nThe motivation for exploring the use of distillation to train\neach head is two fold. Firstly, we hoped that distillation\nwould decrease training time by providing more stable tar-\ngets for the actor and critic networks. Secondly, we thought\nthat distillation had the potential to stabilize the training\nprocess by mimicking the behavior of the expert teacher\nnetwork. In the multi-task learning graphs provided in sec-\ntion B of our appendix, you can see that our ﬁrst assertion\nwas correct. Namely, the reward of the multi-task distil-\nlation agent reaches 1000 more quickly than the Vanilla\ndistillation agent.\nHowever, the variance in the reward\nof the trained distillation agent is not notably lower than\nthe variance of the vanilla distillation agent. This is evi-\ndenced by the fact that the standard deviation of the reward\nof the distillation agent on HalfCheetahBigTorso and\nHalfCheetahSmallThigh is considerably larger than\nthe standard deviation of the vanilla agent as shown in table\n2.\n5. Conclusion\nIn this paper, we experiment with two different methods for\nmulti-task learning for continuous control. We show that an\nagent that is trained simultaneously to perform on multiple\ntasks is not only able to generalize better on each individ-\nual task but also requires fewer training steps to achieve\ncomparable performance. This has a huge advantage since\nmost of the real-world environments are continuous and\nsampling large numbers of episodes from them can be dif-\nﬁcult. More sophisticated techniques could be developed\nthat use a reinforcement learning policy to select environ-\nments to sample episodes from, which are likely to help the\nnetwork focus on more difﬁcult tasks and train faster. We\nhope our methods can serve as benchmark for future work\nin this ﬁeld.\n6. Reproducibility\nWe have provided all the trained model weights and codes\nat https://github.com/jasonkrone/elen6885-ﬁnal-project\n7. Acknowledgments\nThe authors would like to thank Prof. Chong Li for shar-\ning his knowledge and the Teaching Assistants– Lingyu\nZhang, Chen-Yu Yen and Xing Yuan for their constant sup-\nport through out the course.\nMulti-task Learning for Continuous Control\nReferences\nBucila, Cristian, Caruana, Rich, and Niculescu-Mizil,\nAlexandru. Model compression. ACM, 2006.\nHenderson,\nPeter,\nChang,\nWei-Di,\nShkurti,\nFlorian,\nHansen, Johanna, Meger, David, and Dudek, Gregory.\nBenchmark environments for multitask learning in con-\ntinuous domains. arXiv preprint arXiv:1708.04352v1,\n2017.\nHinton, G., Vinyals, O., and Dean, J. Distilling the knowl-\nedge in a neural network. Deep Learning and Represen-\ntation Learning Workshop, NIPS, 2014.\nMnih, Volodymyr, Badia, Adri Puigdomnech, Mirza,\nMehdi, Graves, Alex, Harley, Tim, Lillicrap, Timothy P.,\nSilver, David, and Kavukcuoglu, Koray. Asynchronous\nmethods for deep reinforcement learning. JMLR, 2016.\nParisotto, Emilio, Ba, Jimmy, and Salakhutdinov, Ruslan.\nActor-mimic deep multitask and transfer reinforcement\nlearning. ICLR 2016, 2016.\nRusu, Andrei A., Colmenarejo, Sergio Gomez, Gulcehr,\nCaglar, Desjardins, Guillaume, Kirkpatrick, James, Pas-\ncanu, Razvan, Mnih, Volodymyr, Kavukcuoglu, Koray,\nand Hadsell, Raia.\nPolicy distilation.\narXiv preprint\narXiv:1511.06295v2, 2016a.\nRusu, Andrei A., Rabinowitz, Neil C., Desjardins, Guil-\nlaume, Soyer, Hubert, Kirkpatrick, James, Kavukcuoglu,\nKoray, Pascanu, Razvan, and Hadsell, Raia. Progressive\nneural networks.\narXiv preprint arXiv:1606.04671v3,\n2016b.\nTeh, Yee Whye, Bapst, Victor, Czarnecki, Wojciech Mar-\nian, Quan, John, Kirkpatrick, James, Hadsell, Raia,\nHeess, Nicolas, and Pascanu, Razvan.\nDistral: Ro-\nbust multitask reinforcement learning.\narXiv preprint\narXiv:1707.04175v1, 2017.\nWilliams, Ronald J and Peng, Jing. Function optimization\nusing connectionist reinforcement learning algorithms.\nConnection Science, 3(3):241–268, 1991.\nMulti-task Learning for Continuous Control\nA. Fine-tuning Learning curves\n(a)\n(b)\n(c)\n(d)\nFigure 2. Rewards computed across timesteps during training. (a) environment HalfCheetahSmallLeg-v0 trained from scratch, (b) en-\nvironment HalfCheetahSmallLeg-v0 ﬁne tuned with transferred weights from HalfCheetahSmallFoot-v0, (c) HalfCheetahSmallFoot-\nv0 environment trained from scratch, (d) environment HalfCheetahSmallFoot-v0 ﬁne tuned with transferred weights from\nHalfCheetahSmallLeg-v0\nB. Multi-Task graphs\n(a) Single-task training from scratch\n(b) 6-task training using distillation\n(c) 6-task training from scratch\nFigure 3. Learning curves for HalfCheetahSmallFoot-v0\nMulti-task Learning for Continuous Control\n(a) Single-task training from scratch\n(b) 6-task training using distillation\n(c) 6-task training from scratch\nFigure 4. Learning curves for HalfCheetahBigFoot-v0\n(a) Single-task training from scratch\n(b) 6-task training using distillation\n(c) 6-task training from scratch\nFigure 5. Learning curves for HalfCheetahSmallLeg-v0\n(a) Single-task training from scratch\n(b) 6-task training using distillation\n(c) 6-task training from scratch\nFigure 6. Learning curves for HalfCheetahSmallThigh-v0\n(a) Single-task training from scratch\n(b) 6-task training using distillation\n(c) 6-task training from scratch\nFigure 7. Learning curves for HalfCheetahSmallTorso-v0\nMulti-task Learning for Continuous Control\n(a) Single-task training from scratch\n(b) 6-task training using distillation\n(c) 6-task training from scratch\nFigure 8. Learning curves for HalfCheetahBigTorso-v0\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-02-03",
  "updated": "2018-02-03"
}