{
  "id": "http://arxiv.org/abs/1802.02277v2",
  "title": "From Game-theoretic Multi-agent Log Linear Learning to Reinforcement Learning",
  "authors": [
    "Mohammadhosein Hasanbeig",
    "Lacra Pavel"
  ],
  "abstract": "The main focus of this paper is on enhancement of two types of game-theoretic\nlearning algorithms: log-linear learning and reinforcement learning. The\nstandard analysis of log-linear learning needs a highly structured environment,\ni.e. strong assumptions about the game from an implementation perspective. In\nthis paper, we introduce a variant of log-linear learning that provides\nasymptotic guarantees while relaxing the structural assumptions to include\nsynchronous updates and limitations in information available to the players. On\nthe other hand, model-free reinforcement learning is able to perform even under\nweaker assumptions on players' knowledge about the environment and other\nplayers' strategies. We propose a reinforcement algorithm that uses a\ndouble-aggregation scheme in order to deepen players' insight about the\nenvironment and constant learning step-size which achieves a higher convergence\nrate. Numerical experiments are conducted to verify each algorithm's robustness\nand performance.",
  "text": "From Game-theoretic Multi-agent Log-Linear Learning to\nReinforcement Learning\nMohammadhosein Hasanbeig\nhosein.hasanbeig@cs.ox.ac.uk\nComputer Science Department\nUniversity of Oxford\nWolfson Building OX1 3QD, Oxford, UK\nLacra Pavel\npavel@ece.utoronto.ca\nElectrical and Computer Engineering Department\nUniversity of Toronto\n10 Kings College Road M5S 3G4, Toronto, Canada\nAbstract\nThe main focus of this paper is on enhancement of two types of game-theoretic learn-\ning algorithms: log-linear learning and reinforcement learning. The standard analysis of\nlog-linear learning needs a highly structured environment, i.e. strong assumptions about\nthe game from an implementation perspective. In this paper, we introduce a variant of\nlog-linear learning that provides asymptotic guarantees while relaxing the structural as-\nsumptions to include synchronous updates and limitations in information available to the\nplayers. On the other hand, model-free reinforcement learning is able to perform even un-\nder weaker assumptions on players’ knowledge about the environment and other players’\nstrategies. We propose a reinforcement algorithm that uses a double-aggregation scheme\nin order to deepen players’ insight about the environment and constant learning step-size\nwhich achieves a higher convergence rate. Numerical experiments are conducted to verify\neach algorithm’s robustness and performance.\n1. Introduction\nMost of the studies done on Multi-agent Systems (MASs) within the game theory framework\nare focused on a class of games called potential games (e.g. (Rahili and Ren (2014)), (Wang\nand Pavel (2014)), (Marden and Wierman (2008))).\nPotential games are an important\nclass of games that are most suitable for optimizing and modeling large-scale decentralized\nsystems. Both cooperative (e.g. (Li and Cassandras (2005))) and non-cooperative (e.g.\n(Rahili and Ren (2014))) games have been studied in MAS. A non-cooperative potential\ngame is a game in which there exists competition between players, while in a cooperative\npotential game players collaborate. In potential games, a relevant equilibrium solution is a\nNash equilibrium from which no player has any incentive to deviate unilaterally.\nThe concept of “learning” in potential games is an interesting notion by which a Nash\nequilibrium can be reached. Learning schemes assume that players eventually learn about\nthe environment (the space in which agents operate) and also about the behavior of other\nplayers (Kash et al. (2011)). A well-known game theoretic learning is Log-Linear Learning\n(LLL), originally introduced in (Blume (1993)). LLL has received signiﬁcant attention on\n1\narXiv:1802.02277v2  [cs.LG]  18 Sep 2018\nissues ranging from analyzing convergence rates (e.g. (Shah and Shin (2010))) to the ne-\ncessity of the structural requirements (e.g. (Al´os-Ferrer and Netzer (2010))). The standard\nanalysis of LLL relies on a number of explicit assumptions (Marden and Shamma (2012)):\n(1) Players’ utility functions establish a potential game. (2) Players update their strategies\none at a time, which is referred to as asynchrony. (3) A player is able to select any action\nin the action set, which is referred to as completeness assumption. LLL guarantees that\nonly the joint action proﬁles that maximize the potential function are stochastically stable.\nAs we see in Section 2.2, the asynchrony and completeness assumptions can be relaxed\nseparately which, results in two diﬀerent algorithms called Synchronous LLL (SLLL) and\nBinary LLL (BLLL) (Marden and Shamma (2012)) respectively.\nIn this work we combine the advantages of both algorithms and relax the asynchrony and\ncompleteness assumptions at the same time. We would like to emphasize that “synchronous\nlearning” used in (Marden and Shamma (2012)) does not necessary mean that the whole\nset of agents are allowed to learn, but rather means that the learning process is carried out\nby a group of agents. Although, this group can be the entire set of agents, we believe that\nthe phrase “partial-synchronous learning” is more accurate in reﬂecting what we mean by\nthis multi-agent learning and we use this expression in the rest of this paper.\nA diﬀerent type of learning, originally derived from behaviorist psychology and the no-\ntion of stimulus-response, is Reinforcement Learning (RL). The main idea in RL is that\nplayers tend to use strategies that worked well in the past. In RL, players keep an “aggre-\ngate” of their past interactions with their environment to respond in future situations and\nproduce the most favorable outcome. Towards this direction we propose a new RL-based\nalgorithm whose performance is much better than conventional RL methods in potential\ngames. The following is an overview of our contributions:\n• The work in (Marden and Shamma (2012)) studies LLL from the perspective of dis-\ntributed control theory. Classical LLL has useful convergence guarantees, but makes\nseveral assumptions that are unrealistic from the perspective of distributed control.\nNamely, it is assumed that agents always act asynchronously, i.e. one at a time, and\nthat they always have complete access to use any of their actions. Although, (Marden\nand Shamma (2012)) demonstrates that these assumptions can be relaxed separately,\nwe show that these relaxations can be combined, i.e. LLL can be employed without\nthe asynchrony assumption and without the completeness assumption, at the same\ntime. This, as we can see later, increases the convergence rate of the algorithm and\noptimizes the exploration process. Formal convergence analysis of the proposed algo-\nrithm is also presented. Note that, while in an asynchronous learning process only one\nplayer is allowed to learn at each iteration, in a partial-synchronous process a group\nof agents (including the whole set of agents) is able to take actions. However, in both\ncases, all players are aware of the process common clock.\n• We propose a modiﬁed Expectation Maximization (EM) algorithm that can be com-\nbined with LLL to build up a model-based LLL algorithm which further relaxes LLL’s\nassumptions on initial knowledge of utility function. In addition to this, our modiﬁed\nalgorithm relaxes the basic assumption of known component number of the classical\nEM, in order to make it more applicable to empirical examples. Through a numerical\n2\nexperiment, we show that by using this algorithm, both the convergence rate and the\nequilibrium are improved.\n• We ﬁnally propose a model-free RL algorithm which completely drops LLL’s assump-\ntions on players’ knowledge about their utility function and other players’ strategies.\nHowever, as we will see later this comes with the cost of slower convergence rate. The\nproposed RL employs a double-aggregation scheme in order to deepen players’ in-\nsight about the environment and uses constant learning step-size in order to achieve a\nhigher convergence rate. Convergence analysis of this algorithm is presented in detail.\nNumerical experiments are also provided to demonstrate the proposed algorithm’s\nimprovements.\nA short version of this work without proofs and generalization appears in (Hasanbeig and\nPavel (2017b)) and (Hasanbeig and Pavel (2017a)).\nThis paper discusses a number of\ngeneralizations and also proofs with necessary details.\n2. Background\nLet G(I,A,u) be a game where I denotes the set of players and |I|=N, A=×iAi denotes the\naction space, where Ai is the ﬁnite set of actions of player i, and ui:A→R is player i’s utility\nfunction. Player i’s (pure) action is denoted by αi∈Ai, with α−i=(α1,...,αi−1,αi+1,...,αN)\ndenoting action proﬁle for players other than i. With this notation, we may write a joint\naction proﬁle α=(α1,...,αN)∈A as α=(αi,α−i)∈A.\nIn this paper, t is the continuous time and n is the discrete time. In a repeated version\nof the game G, at each time t (or at every iteration n), each player i∈I selects an action\nαi(t)∈Ai (or αi(n)∈Ai) and receives a utility ui(α) which, in general, is a function of the\njoint action α. Each player i chooses action αi(t) (or αi(n)) according to the information\nand observations available to player i up to t (or iteration n) with the goal of maximizing\nits utility. Both the action selection process and the available information depend on the\nlearning process.\nIn a repeated game, a Best Response (BR) correspondence BR(α−i) is deﬁned as the set\nof optimal strategies for player i against the strategy proﬁle of its opponents, i.e. BR(α−i)=\nargmaxαi∈Aiui(αi,α−i) This notion is going to be used quite often in the rest of this paper.\n2.1 Potential Games\nThe concept of a potential game, ﬁrst introduced in (Monderer and Shapley (1996)), is a\nuseful tool to analyze equilibrium properties in games. In a potential game a change in each\nplayer’s strategy is expressed via a player-independent function, i.e. potential function. In\nother words, the potential function, speciﬁes players’ global preference over the outcome of\ntheir actions.\nDeﬁnition 1 Potential Game: A game G is a potential game if there exists a potential\nfunction Φ:A7→R such that for any agent i∈I, for every α−i∈A−i and any αi\n1,αi\n2∈Ai we\nhave Φ(αi\n2,α−i)−Φ(αi\n1,α−i)=ui(αi\n2,α−i)−ui(αi\n1,α−i), where ui:A7→R is the player i’s utility\nfunction (Marden and Shamma (2012)).\n3\nFrom Deﬁnition 1, when player i switches its action, the change in its utility equals the\nchange in the potential function. This means that for all possible deviations from all action\npairs, the utility function of each agent i is aligned with the potential function. Thus, in\npotential games, each player’s utility improvement is equal to the same improvement in the\npotential function.\nAn improvement path Ωin a potential game is deﬁned as a sequence of action proﬁles\nΩ={α1→α2→...→αm} such that in each sequence αk→αk+1 a player i makes a change in\nits action and receives a strictly higher utility, i.e. ui(αk+1)>ui(αk). An improvement path\nterminates at action proﬁle α∗if no further improvement can be obtained. A game G is\nsaid to have the ﬁnite improvement property if every improvement path in G is ﬁnite.\nTheorem 2 Every improvement path in a ﬁnite potential game is ﬁnite (Monderer and\nShapley (1996)).\nThis means that there exist a point α∗such that no player in I can improve its utility and\nthe global potential function by deviating from this point. In other words\nui(αi\n∗,α−i\n∗)≥ui(αi,α−i\n∗), ∀αi∈Ai, ∀i∈I.\n(1)\nThe strategy proﬁle α∗is called pure Nash equilibrium of the game.\nDeﬁnition 3 Nash Equilibrium: Given a game G, a strategy proﬁle α∗=(αi\n∗,α−i\n∗) is a\npure Nash equilibrium of G if and only if ui(αi\n∗,α−i\n∗)≥ui(αi,α−i\n∗), ∀αi∈Ai, ∀i∈I.\nAt a Nash equilibrium no player has a motivation to unilaterally deviate from its current\nstate (Nash (1951)).\nA mixed strategy for player i is deﬁned when player i randomly chooses between its ac-\ntions in Ai. Let xi\nαi be the probability that player i selects action αi∈Ai (the discrete version\nis denoted by Xi\nαi). Hence, player i’s mixed strategy is xi∈X i where xi=(xi\nαi\n1,...,xi\nαi\n|Ai|\n) and\nX i is a unit |Ai|-dimensional simplex X i={x∈R|Ai| s.t. P\nα∈Ai xα=1, xα≥0}. Likewise,\nwe denote the mixed-strategy proﬁle of all players by x=(x1,...,xN)∈X where the mixed\nstrategy space is denoted by X=×iX i.\nA mixed-strategy Nash equilibrium is an N-tuple such that each player’s mixed strategy\nmaximizes its expected payoﬀif the strategies of the others are held ﬁxed. Thus, each\nplayer’s strategy is optimal against his opponents’. Let the expected utility of player i be\ngiven as\nui(x)=\nX\nα∈A\n(\nY\ns∈I\nxs\nαs)ui(αi,α−i),\n(2)\nThen a mixed strategy proﬁle x∗∈X is a mixed strategy Nash equilibrium if for all players\nui(xi\n∗,x−i\n∗)≥ui(xi,x−i\n∗),\n∀xi∈X i,\n∀i∈I. Such a Nash equilibrium is a ﬁxed-point of the\nmixed-strategy best-response, or in other words, all players in a Nash equilibrium play their\nbest response xi\n∗∈BR(x−i\n∗),\n∀i∈I where BR(x−i)={xi\n∗∈X i|ui(xi\n∗,x−i)≥ui(xi,x−i), ∀xi∈\nX i}, is the best response set (Morgenstern and Von Neumann (1953)).\n4\n2.2 Learning in Games\nLearning in games tries to relax assumptions of classical game theory on players’ initial\nknowledge and belief about the game. In a game with learning, instead of immediately\nplaying the perfect action, players adapt their strategies based on the outcomes of their\npast actions. In the following, we review two classes of learning in games: (1) log-linear\nlearning and (2) reinforcement learning.\n2.2.1 Log-Linear Learning\nIn Log-Linear Learning (LLL), at each time step, only “one” random player, e.g. player i,\nis allowed to alter its action. According to its mixed strategy xi, player i chooses a trial\naction from its “entire” action set Ai. In LLL, player i’s mixed strategy or probability of\naction β∈Ai is updated by a Smooth Best Response (SBR) on ui:\nxi\nβ=\nexp(1/τ ui(β,α−i)\nP\nγ∈Aiexp(1/τ ui(γ,α−i).\n(3)\nwhere τ is often called the temperature parameter that controls the smoothness of the\nSBR. The greater the temperature, the closer xi is to the uniform distribution over player\ni’s action space.\nNote that, each player i in LLL needs to know the utility of all actions in Ai, including\nthose that are not played yet, and further actions of other players α−i. With these assump-\ntions, LLL can be modeled as a perturbed Markov process where the unperturbed Markov\nprocess is a best reply process.\nDeﬁnition 4 Stochastically Stable State: Let Prϵ\nβ be the frequency or the probability\nwith which action β is played in the associated perturbed Markov process, where ϵ>0 is the\nperturbation index. Action β is then a stochastically stable state if: limϵ→0Prϵ\nβ=Prβ where\nPrβ is the corresponding probability in the unperturbed Markov process (Young (1993)).\nSynchronous Learning:(relaxing asynchrony assumption)\nOne of the basic assumptions in standard LLL is that only one random player is allowed\nto alter its action at each step.\nIn (partial-) synchronous log-linear learning (SLLL) a\ngroup of players G⊂I is selected to update its action based on the probability distribution\nrp⊂∆(2I); rpG is deﬁned as the probability that group G will be chosen and rpi is deﬁned\nas the probability that player i∈I updates its action. The set of all groups with rpG>0\nis denoted by ¯G. In an independent revision process, each player independently decides\nwhether to revise his strategy by LLL rule.\nSLLL is proved to converge under certain\nassumptions (Marden and Shamma (2012)).\nConstrained Action Set:(relaxing completeness assumption)\nStandard LLL requires each player i to have access to all available actions in Ai. In the case\nwhen player i has no free access to every action in Ai, its action set is “constrained” and\nis denoted by Ai\nc:Ai→2Ai. With a constrained action set, players may be trapped in local\nsub-optimal equilibria since the entire Ai is not available to player i at each move. Thus,\nstochastically stable states may not be potential maximizers. Binary log-linear learning\n(BLLL) is a variant of standard LLL which provides a solution to this issue.\n5\nAssumption 1 For each player i and for any action pair αi(1),αi(m)∈Ai there exists\na sequence of actions αi(1) →αi(2) →... →αi(m) satisfying αi(k)∈Ai\nc(αi(k−1)), ∀k∈\n{2,...,m}.\nAssumption 2 For each player i and for any action pair αi(1),αi(2)∈Ai, αi(2)∈Ai\nc(αi(1))\n⇔αi(1)∈Ai\nc(αi(2)).\nThe following theorem studies the convergence of BLLL in potential games under Assump-\ntions 1 and 2.\nTheorem 5 In a ﬁnite N-player potential game satisfying Assumptions 1 and 2 and with\npotential function Φ:A7→R, if all players adhere to BLLL, then the stochastically stable\nstates are the set of potential maximizers (Marden and Shamma (2012)).\n2.2.2 Reinforcement Learning\nReinforcement Learning (RL) is another variant of learning algorithms that we consider\nin this paper.\nRL discusses how to map actions’ reward to players’ action so that the\naccumulated reward is maximized. Players are not told which actions to take but instead\nthey have to discover which actions yield the highest reward by “exploring” the environment\n(Sutton and Barto (2011)). RL only requires players to observe their own ongoing payoﬀs,\nso they do not need to monitor their opponents’ strategies or predict payoﬀs of actions\nthat they did not play. In the following we present a technical background on RL and its\napplication in MASs.\nAggregation:\nIn RL each player uses a score variable, as a memory, to store and track past events. We\ndenote player i’s score vector by pi∈Pi where Pi is player i’s score space. It is common\nto assume that the game rewards can be stochastic, i.e., an action proﬁle does not always\nresult in the same deterministic utility. Therefore, actions need to be sampled, i.e. aggre-\ngated, repeatedly or continuously. A common form of continuous aggregation rule is the\nexponential discounted model:\npi\nβ(t)=pi\nβ(0)λt+\nZ t\n0\nλt−sui(β,α−i)ds, (β,α−i)∈A,\n(4)\nwhere pi\nβ is action β’s aggregated score and λ>0 is the model’s discount rate. λ can be\nalternatively deﬁned via T=log(1/λ). The choice of T aﬀects the learning dynamics and the\nprocess of ﬁnding the estimated Nash equilibrium. The discount rate has a double role in\nRL: (1) It determines the weight that players give to their past observations. (2) T reﬂects\nthe rationality of the players in choosing their actions and consequently the accuracy of\nplayers’ stationary points in being the true Nash equilibrium. Additionally, discounting\nimplies that the score variable pi\nβ(t) will remain bounded which consequently prevent the\nagents’ mixed strategies from approaching the boundaries X (Coucheney et al. (2014)). By\ndiﬀerentiating (4), and assuming pi\nβ(0)=0, we obtain the following score dynamics\n˙pi\nβ=ui(β,α−i)−Tpi\nβ.\n(5)\n6\nBy applying the ﬁrst-order Euler discretization on (5) we obtain:\nP i\nβ(n+1)=P i\nβ(n)+µ(n)[ui(β,α−i)−TP i\nβ(n)],\n(6)\nwhere n is the iteration number, µ(n) is the discretization step size and P i\nβ(n) is the discrete\nequivalent of pi\nβ(t). A stochastic approximation of the discrete dynamics requires diminish-\ning step sizes such that P\nnµ(n)=∞and P\nnµ(n)2<∞(Bena¨ım (1999)).\nChoice Map:\nIn the action selection step players decide how to exploit the score variable to choose a\nstrategy against the environment, e.g. according to:\nSBR(pi)=arg max\nxi∈X i\nX\nβ∈Ai\n[xi\nβpi\nβ−hi(xi)].\n(7)\nThis choice model is often called “Smoothed Best Response (SBR) map” or “quantal re-\nsponse function” where the penalty function hi in (7) has to have the following properties:\n1. hi is ﬁnite except on the relative boundaries of X i,\n2. hi is continuous on X i, smooth on relative interior of X i and |dhi(xi)|→+∞as xi\napproaches to the boundaries of X i,\n3. hi is convex on X i and strongly convex on relative interior of X i.\nThe choice map (7) actually discourages player i from choosing an action from boundaries\nof X i, i.e. from choosing pure strategies. The most prominent SBR map is the “logit” map\nbased on using Gibbs entropy in (7),\nxi\nα=\nh\nSBR(pi)\ni\nα=\nexp(pi\nα)\nP\nβ∈Ai exp(pi\nβ).\n(8)\nIt is not always easy to write a closed form of the choice map and the Gibbs entropy is an\nexception (Coucheney et al. (2014)).\nIn order to discretize (7) we again apply ﬁrst-order Euler discretization:\nXi(n+1)=SBR(P i(n)),\n(9)\nwhere Xi is the discrete equivalent of xi.\nAt this point all the necessary background is presented and in the following we are going\nto discuss our proposed learning algorithms.\n3. Partial-Synchronous Binary Log-Linear Learning\nIn this section, we present a modiﬁed LLL algorithm in which both assumptions on asyn-\nchrony and complete action set are relaxed.\nThis means that in a Partial-Synchronous\nBinary Log-Linear Learning (P-SBLLL) scheme agents can learn simultaneously while their\navailable action sets are constrained. This simultaneous learning presumably increases the\nBLLL learning rate in the MAS problem.\n7\nIn P-SBLLL algorithm, we propose that at each time n, a set of players S(n)⊆I indepen-\ndently update their actions according to each player i’s revision probability rpi:Ai→(0,1).\nThe revision probability rpi is the probability with which agent i wakes up to update its\naction. All the other players I\\S(n) must repeat their current actions.\nEach player i∈S(n) selects one trial action αi\nT uniformly randomly from its constrained\naction set Ai\nc(αi(n)). Then, player i’s mixed strategy is\nXi\nαi(n)(n)=\nexp(1\nτ ui(α(n)))\nexp(1\nτ ui(α(n)))+exp(1\nτ ui(αT ))\n,\n(10)\nXi\nαi\nT (n)=\nexp(1\nτ ui(αT ))\nexp(1\nτ ui(α(n)))+exp(1\nτ ui(αT ))\n,\n(11)\nwhere αT is the action proﬁle for which each player i∈S(n) updates its action to αi\nT and all\nthe other players I\\S(n) repeat their actions. In the following, we analyze the convergence\nof the proposed algorithm.\n3.1 P-SBLLL’s Convergence Analysis\nFrom the theory of resistance trees, we use the useful relationship between stochastically\nstable states and potential maximizing states. We make the following assumption:\nAssumption 3 For each player i, and for each action αi, the revision probability rpi must\nbe bounded and 0<rpi(αi(n))<1.\nLemma 6 Under Assumption 3, P-SBLLL induces a perturbed Markov process where the\nresistance of any feasible transition α1∈A→α2∈A with deviating set of players S is\nR(α1→α2)=\nX\ni∈S\nmax{ui(α1),ui(α2)}−ui(α2),\n(12)\nwhere each deviating player i∈S selects its action based on Ai\nc.\nProof: Let Pϵ, ϵ>0 denote the perturbed transition matrix. The probability of transition\nfrom α1 to α2 is\nPϵ(α1→α2)=\nY\ni∈S\nrpi(αi\n1)\n|Aic(αi\n1)|\nY\nj∈I\\S\n(1−rpj(αj\n1))\nY\ni∈S\nϵ−ui(α2)\nϵ−ui(α1)+ϵ−ui(α2) ,\n(13)\nwhere ϵ:=e−1/τ. The ﬁrst term Q\ni∈S\nrpi(αi\n1)\n|Aic(αi\n1)| represents the probability that all the players\nin S wake up to change their actions from α1 to α2. The second term Q\nj∈I\\S(1−rpj(αj\n1))\nis the probability that the players in I\\S stay asleep. The last term Q\ni∈S\nϵ−ui(α2)\nϵ−ui(α1)+ϵ−ui(α2)\n8\nis the binary SBR over α1 and α2. Next deﬁne the maximum utility of player i for any two\naction proﬁles α1 and α2 as V i(α1,α2)=max{ui(α1),ui(α2)}. By multiplying the numerator\nand denominator of (13) by Q\ni∈S ϵV i(α1,α2) we obtain\nPϵ(α1→α2)=\nY\ni∈S\nrpi(αi\n1)\n|Aic(αi\n1)|\nY\nj∈I\\S\n(1−rpj(αj\n1))\nY\ni∈S\nϵV i(α1,α2)−ui(α2)\nϵV i(α1,α2)−ui(α1)+ϵV i(α1,α2)−ui(α2) .\n(14)\nDividing (14) by ϵ\nP\ni∈SV i(α1,α2)−ui(α2) yields\nPϵ(α1→α2)\nϵ\nP\ni∈SV i(α1,α2)−ui(α2) to be\nY\ni∈S\nrpi(αi\n1)\n|Aic(αi\n1)|\nY\nj∈I\\S\n(1−rpj(αj\n1))\nY\ni∈S\n1\nϵV i(α1,α2)−ui(α1)+ϵV i(α1,α2)−ui(α2) .\n(15)\nAccording to theory of resistance trees (Young (1993)), if\n0<lim\nϵ→0\nPϵ(α1→α2)\nϵ\nP\ni∈SV i(α1,α2)−ui(α2) <∞\nthen our claim about R(α1→α2) in (12) is true. Considering the deﬁnition of V i(α1,α2), we\nknow that for each player i, either V i(α1,α2)−ui(α1) or V i(α1,α2)−ui(α2) is zero and the\nother one is a positive real number. Thus, as ϵ→0, Q\ni∈S\n1\nϵV i(α1,α2)−ui(α1)+ϵV i(α1,α2)−ui(α2)\nin (15) approaches 1 and\nlim\nϵ→0\nPϵ(α1→α2)\nϵ\nP\ni∈SV i(α1,α2)−ui(α2) =\nY\ni∈S\nrpi(αi\n1)\n|Aic(αi\n1)|\nY\nj∈I\\S\n(1−rpj(αj\n1)).\n(16)\nFrom Assumption 3, Q\ni∈S\nrpi(αi\n1)\n|Aic(αi\n1)| and Q\nj∈I\\S(1−rpj(αj\n1)) are ﬁnite positive real num-\nbers. Hence, 0<limϵ→0\nPϵ(α1→α2)\nϵ\nP\ni∈Smax{ui(α1),ui(α2)}−ui(α2) <∞. Therefore, the process is a per-\nturbed Markov process where the resistance of the transition α1 to α2 is R(α1→α2)=\nP\ni∈Smax{ui(α1),ui(α2)}−ui(α2).\n□\nDeﬁnition 7 Separable Utility Function: Player i’s utility function ui is called sepa-\nrable if ui only depends on player i’s action αi.\nLemma 8 Consider any ﬁnite N-player potential game where all the players adhere to\nP-SBLLL and the potential function is deﬁned as Φ:A→R. Assume that players’ utility\nfunctions are separable.\nFor any feasible transition α1∈A→α2∈A with deviating set of\nplayers S, the following holds:\nR(α1→α2)−R(α2→α1)=Φ(α1)−Φ(α2).\n(17)\n9\nProof: From Lemma 6, R(α1→α2)=P\ni∈S12max{ui(α1),ui(α2)}−ui(α2), and R(α2→α1)=\nP\ni∈S21max{ui(α2),ui(α1)}−ui(α1), where S12 is the set of deviating players during the\ntransition α1→α2 and S21 is the set of deviating players in the transition α2→α1.\nBy\nAssumption 2, if the transition α1→α2 is possible then there exist a reverse transition\nα2→α1. Clearly, the same set of deviating players is needed for both α1→α2 and α2→α1,\ni.e. S12=S21=S. Therefore:\nR(α1→α2)−R(α2→α1)=\nh X\ni∈S12\nmax{ui(α1),ui(α2)}−ui(α2)\ni\n−\nh X\ni∈S21\nmax{ui(α2),ui(α1)}−ui(α1)\ni\n=\nX\ni∈S\nh\nmax{ui(α1),ui(α2)}−ui(α2)−max{ui(α2),ui(α1)}+ui(α1)\ni\n.\n(18)\nBy canceling identical terms max{ui(α2),ui(α1)}, R(α1→α2)−R(α2→α1)=P\ni∈Sui(α1)−\nui(α2). Since players’ utility functions are separable, for any player i, ui(α1)−ui(α2)=\nui(αi\n1,α−i\n1 )−ui(αi\n2,α−i\n1 ).\nFrom Deﬁnition 1 we have ui(αi\n1,α−i\n1 )−ui(αi\n2,α−i\n1 )=Φ(αi\n1,α−i\n1 )−\nΦ(αi\n2,α−i\n1 ) and it is easy to show that P\ni∈Sui(α1)−ui(α2)=Φ(α1)−Φ(α2).\n□\nNext, we prove that in separable games, the stable states of the algorithm maximize the\npotential function. In other words, the stable states are the optimal states.\nProposition 9 Consider any ﬁnite N-player potential game satisfying Assumptions 1, 2\nand 3.\nLet the potential function be Φ:A→R and assume that all players adhere to P-\nSBLLL. If the utility functions for all players are separable, then the stochastically stable\nstates are the set of potential maximizers.\nProof: We ﬁrst show that for any path Ω, deﬁned as a sequence of action proﬁles Ω:={α0→\nα1→...→αm} and its reverse path ΩR:={αm→αm−1→...→α0}, the resistance diﬀerence is\nR(Ω)−R(ΩR)=Φ(α0)−Φ(αm). where\nR(Ω):=\nm−1\nX\nk=0\nR(αk→αk+1), R(ΩR):=\nm−1\nX\nk=0\nR(αk+1→αk).\n(19)\nAssuming S as the set of deviating players for the edge αk→αk+1 in Ω, the expanded form of\nthis edge can be written as αk→αk+1={αk=αk0→αk1→αk2→...→αkq−1→αkq=αk+1}, where\nαki→αki+1 is a sub-edge in which only one player is deviating and q=|S|. From Lemma 8\nR(αki→αki+1)−R(αki+1→αki)=Φ(αki)−Φ(αki+1). Note that |S|=1 for each sub-edge. Con-\nsequently for each edge αk→αk+1={αk=αk0→αk1→αk2→...→αkq−1→αkq=αk+1} we obtain\nR(αk→αk+1)−R(αk+1→αk)=\nΦ(αk0)−Φ(αk1)+Φ(αk1)−Φ(αk2)+...+Φ(αkq−1)−Φ(αkq).\nBy canceling the identical terms,\nR(αk→αk+1)−R(αk+1→αk)=Φ(αk0)−Φ(αkq)=Φ(αk)−Φ(αk+1).\n(20)\nComparing (20) and (17) implies that if the utility functions of all players are separable,\nthe number of deviating players does not aﬀect the resistance change between forward and\n10\n(a) Tree T rooted at α\n(b) Ωp and its reverse path ΩR\np\n(c) Tree T ′ rooted at α∗\nFigure 1: Resistance Trees\nbackward transitions.\nFinally, we sum up the resistance diﬀerence in (20) for all pairs\n(αk,αk+1), k∈{0,1,...,m−1}: Pm−1\nk=0 R(αk→αk+1)−R(αk+1→αk)=Pm−1\nk=0 Φ(αk)−Φ(αk+1), or\nequivalently\nm−1\nX\nk=0\nR(αk→αk+1)−\nm−1\nX\nk=0\nR(αk+1→αk)=\nm−1\nX\nk=0\nΦ(αk)−Φ(αk+1).\n(21)\nFrom (19) we have Pm−1\nk=0 R(αk→αk+1) is the resistance over the path Ω, i.e. R(Ω) and\nPm−1\nk=0 R(αk+1→αk) is the resistance over the reverse path ΩR, i.e. R(ΩR). Furthermore,\nit is easy to show Pm−1\nk=0 Φ(αk)−Φ(αk+1)=Φ(α0)−Φ(αm). Consequently,\nR(Ω)−R(ΩR)=Φ(α0)−Φ(αm).\n(22)\nNow assume that an action proﬁle α is a stochastically stable state. Therefore, there exist\na tree T rooted at α (Fig. 1.a) for which the resistance is minimum among the trees rooted\nat other states. We use contradiction to prove our claim. As the contradiction assumption,\nsuppose the action proﬁle α does not maximize the potential and let α∗be the action proﬁle\nthat maximizes the potential function. Since T is rooted at α and from Assumption 1, there\nexist a path Ωp from α∗to α as Ωp={α∗→α1→...→α}. Consider the reverse path ΩR\np from\nα to α∗(Fig. 1.b) ΩR\np ={α→αm→...→α∗}. We can construct a new tree T ′ rooted at α∗by\nadding the edges of ΩR\np to T and removing the edges of Ωp (Fig. 1.c). The resistance of the\nnew tree T ′ is R(T ′)=R(T)+R(ΩR\np )−R(Ωp). By (22), R(T ′)=R(T)+Φ(α)−Φ(α∗). Recall\nthat we assumed Φ(α∗) is the maximum potential. Hence Φ(α)−Φ(α∗)<0 and consequently\nR(T ′)<R(T). Therefore T is not a minimum resistance tree among the trees rooted at\nother states, which is in contrast with the basic assumption about the tree T. Hence, the\nsupposition is false and α is the potential maximizer. We can use the above analysis to\nshow that all action proﬁles with maximum potential have the same stochastic potential\n11\n(ϕ) which means all the potential maximizers are stochastically stable.\n□\nIn the following we analyze the algorithm’s convergence for the general case of non-separable\nutilities under extra assumptions.\nAssumption 4 For any two player i and j, if Φ(αi\n2,α−i)≥Φ(αi\n1,α−i) then ui(αi\n2,α−i)≥\nuj(αi\n1,α−i).\nThe intuition behind Assumption 4 is that the set of agents has some level of homogeneity.\nThus, if the potential function is increased by agent i changing its action from αi\n1 to αi\n2 it\nis as if any other agent j had at most the same utility increase.\nAssumption 5 The potential function Φ is non-decreasing.\nNote that Assumption 5, does not require knowledge of Nash equilibrium. The intuition is\nthat when action proﬁle is changed such that we move towards the Nash equilibrium, the\nvalue of potential function will never decrease.\nTheorem 10 Consider a ﬁnite potential game in which all players adhere to P-SBLLL\nwith independent revision probability. Let Assumptions 1, 2, 3, 4 and 5 hold. If an action\nproﬁle is stochastically stable then it is potential function maximizer.\nProof: From the theory of resistance trees, the state α is stochastically stable if and only\nif there exists a minimum resistance tree T rooted at α (see Theorem 3.1 in (Marden and\nShamma (2012))).\nRecall that a minimum resistance tree rooted at α has a minimum\nresistance among the trees rooted at other states. We use contradiction to prove that α\nalso maximizes the potential function. As the contradiction assumption, suppose the action\nproﬁle α does not maximize the potential. Let α∗be any action proﬁle that maximizes the\npotential Φ. Since T is rooted at α, there exists a path from α∗to α: Ω={α∗→α}. We\ncan construct a new tree T ′, rooted at α∗by adding the edges of ΩR={α→α∗} to T and\nremoving the edges of Ω. Hence,\nR(T ′)=R(T)+R(ΩR)−R(Ω).\n(23)\nIf the deviator at each edge is only one player, then the algorithm reduces to BLLL. Sup-\npose there exist an edge αq→αq+1 in ΩR with multiple deviators. The set of deviators is\ndenoted by S. If we show R(ΩR)−R(Ω)≤0 then T is not the minimum resistance tree and\ntherefore, supposition is false and α is actually the potential maximizer. Note that since\nplayers’ utilities may not be separable, the utility of each player depends on the actions of\nother players.\nFrom Lemma 6, for any transition α1→α2 in ΩR, R(α1→α2)=P\ni∈S12max{ui(α1),ui(α2)}−\nui(α2), and R(α2→α1)=P\ni∈S21max{ui(α2),ui(α1)}−ui(α1), where S12 is the set of devi-\nating players during the transition α1→α2.\nBy Assumption 2, S12=S21=S.\nTherefore\nR(α1→α2)−R(α2→α1)=\nh X\ni∈S12\nmax{ui(α1),ui(α2)}−ui(α2)\ni\n−\nh X\ni∈S21\nmax{ui(α2),ui(α1)}−ui(α1)\ni\n=\nX\ni∈S\nh\nmax{ui(α1),ui(α2)}−ui(α2)−max{ui(α2),ui(α1)}+ui(α1)\ni\n.\n(24)\n12\nBy canceling identical terms max{ui(α2),ui(α1)},\nR(α1→α2)−R(α2→α1)=\nX\ni∈S\nui(α1)−ui(α2).\n(25)\nNow divide α1→α2 into a sequence of sub-edges in which only one agent is deviating.\nAssuming S={i,i+1,i+2,...,i+|S|} as the set of deviating players for this transition, the\nexpanded from of this edge can be written as\nα1→α2={α1=αk0→(αi\n2,α−i\n1 )=αk1→αk2→...→αkq−1→αkq=α2},\nwhere αkj→αkj+1 is a sub-edge in which only player i+jth is deviating. Note that q=|S|.\nWe can rewrite (25) as\nR(α1→α2)−R(α2→α1)=\nX\ni∈S\nui(α)−ui(αk1)+ui(αk1)−ui+1(αk2)+\nui+1(αk2)−...+ui+|S|−1(αkq−1)−ui(α).\n(26)\nBy Assumption 5 and by knowing that α1→α2 is in ΩR={α→α∗}: Φ(α1)=Φ(αk0)≤Φ(αk1)≤\nΦ(αk2)≤...≤Φ(αkq)=Φ(α2). Thus, by Assumption 4: ui(α)−ui(αk1)≤0,ui(αk1)−ui+1(αk2)≤\n0,...,ui+|S|−1(αkq−1)−ui(α)≤0.\nThis means that R(α1→α2)−R(α2→α1)≤0. Since α1→α2 is an arbitrary transition in\nΩR and α2→α1 is its reverse transition in Ω, it is obvious that R(ΩR)−R(Ω)≤0. Therefore,\nfrom (23), T is not a minimum resistance tree among the trees rooted at other states, which\nis in contrast with the basic assumption about T. Hence, the supposition is false and α is\nthe potential maximizer.\n□\n3.2 Model-based P-SBLLL\nNote that in P-SBLLL’s (10) and (11), player i needs to know the utility value of any action\nβ∈Ai. However, in the case of unknown environment, utility values of actions in Ai that\nare not played are not available to player i.\nIn a model-based learning algorithm, we assume that players can eventually form a\nbelief of the environment by constantly interacting with the environment. This belief is\ncalled model. By using the model, when the environment is unknown, players are able to\nestimate the payoﬀof the actions that they did not play. In a model-based learning, each\nplayer i possesses an unbiased and bounded estimate ˆui\nα for the payoﬀof each action α∈Ai\n(including the actions that he did not play) such that E[ˆui\nα(n+1)|Hn]=ui\nα(n), where Hn is\nthe history of the process up to the iteration n. Thus, in a model-based LLL scheme, utility\nvalues of unknown actions is replaced by player’s estimation model.\nThe model is usually a function that is ﬁtted over the utility values of the actions that are\nalready sampled. Therefore, one of the main assumptions in model-based learning is that\nplayers know the structure of the utility function so that they can ﬁt a function accordingly.\nWe conclude this section by emphasizing that P-SBLLL is a type of learning algorithm\nin which each agent’s action set is constrained (recall the completeness assumption). Fur-\nthermore, unlike BLLL in which only one random agent takes a trial action, all the agents\nare allowed to explore the environment. Another valuable feature of P-SBLLL is that each\n13\nagent’s exploration process can be precisely regulated by the revision probability. Thus,\neach agent can decide independently, and not randomly, whether it is the time for taking\na trial action or it is better to remain on the current state. More interestingly, the revi-\nsion probability can be conditioned on each agent’s situation. This will certainly reduce\nredundant explorations in P-SBLLL comparing to BLLL.\nDespite the above-mentioned improvements, P-SBLLL still relies on an estimation model\nof the environment. This means that players in P-SBLLL have a prior knowledge about\nthe structure of the utility distribution so they can apply an appropriate model on the\nenvironment. In practice, this may not be an appropriate assumption. In order to relax\nthis assumption, we turn to RL algorithms which do not require a model of the environment.\n4. Second Order Q-learning\nQ-learning, as a sub-class of RL algorithms, is widely used for policy synthesis problems.\nIn Q-learning algorithm the actions’ expected utilities can be compared without requiring a\nknowledge of the utility function. Thus, in situations where the utility structure is unknown\nand a model-based LLL can not be used, we may use Q-learning. Furthermore, Q-learning\ncan be used in processes with stochastic transitions, without requiring any adaptations.\nRecall that RL dynamics typically uses ﬁrst-order aggregation where actions’ payoﬀis\nused to determine the growth rate of the players’ score variable ˙pi\nβ=ui(β,α−i)−Tpi\nβ. We\nnow discuss what happens beyond ﬁrst-order, when using payoﬀs as higher-order forces of\nchange. To that end we ﬁrst introduce the notion of strictly dominated strategy and weakly\ndominated strategy:\nDeﬁnition 11 A strategy α∈Ai is strictly (weakly) dominated by strategy β∈Ai if for all\nx−i∈X −i we have ui(α,x−i)<u(β,x−i) (ui(α,x−i)≤u(β,x−i)).\nFor a pure strategy β∈Ai, extinction means that xi\nβ→0 as t→∞. Alternatively, a mixed\nstrategy qi∈X i becomes extinct along x(t) if Dkl(qi||xi)=P\nβqi\nβlog(qi\nβ/xi\nβ)→∞as t→∞.\nThe evolution of the system’s dynamic and the eﬃciency of players’ learning cru-\ncially depend on how players aggregate their past observation and update their score vari-\nable. With this in mind we discuss an n-fold, i.e. high-order, aggregation scheme in RL:\npi\nβ\n(n)(t)=ui(β,α−i) where pi\nβ\n(n) denotes nth-order derivative. Needless to say, an nth-order\naggregation scheme looks deeper into the past observations. Theorem 4.1 in (Laraki and\nMertikopoulos (2013)) proves that in a high-order continuous-time learning, dominated\nstrategies become extinct. Furthermore, Theorem 4.3 in (Laraki and Mertikopoulos (2013))\nshows that unlike the ﬁrst-order dynamics in which weakly dominated strategies may sur-\nvive, in high-order dynamics (n≥2) weakly dominated strategies become extinct.\nIn this section we propose a discrete-time Q-learning algorithm in which we incorpo-\nrated a second-order reinforcement in the aggregation step. Such high-order reinforcement\nprovides more momentum towards the strategies that tend to perform better. We call this\nalgorithm Second-Order Q-Learning (SOQL).\nRecall that in a RL algorithm, the score variable characterizes the relative utility of\na particular action. In the Q-learning algorithm, the score variable is represented by the\nQ-value. The Q-value corresponding to the selected action will be updated by the generated\n14\nreinforcement signal once that action is performed. In standard Q-learning, the update rule\nfor Q-value is\nQi\nβ(n+1)=Qi\nβ(n)+µ(n) [ui\nβ(n)−Qi\nβ(n)],\nwhere ui\nβ(n)=1{αi(n)=β} ui(n) and ui(n) is the player i’s realized utility at time step n,\nQi\nβ(n) is the player i’s Q-value corresponding to action β, and µ(n) is the step size assumed\nto be diminishing. In the action selection step, each player selects his action according to a\nQ-value based SBR:\nXi\nβ(n+1)=\nexp(1/τQi\nβ(n))\nP\nβ′∈Aiexp(1/τQi\nβ′(n)).\n(27)\nIn SOQL we propose a double aggregation process for updating Q-value. Furthermore,\nto increase SOQL’s convergence rate we also propose to use an update rule with constant\nstep size µ:\nP i\nβ(n+1)=P i\nβ(n)+µ [ui\nβ(n)−P i\nβ(n)],\nQi\nβ(n+1)=Qi\nβ(n)+µ [P i\nβ(n)−Qi\nβ(n)],\n(28)\nFor its mixed strategy, player i uses a greedy action selection similar to (Wang and Pavel\n(2014)):\nXi(n+1)=(1−ϑ)Xi(n)+ϑBRi(Qi(n)),\n(29)\nwhere the constant coeﬃcient ϑ is the action selection step size satisfying ϑ<µ<1 and\nplayer i’s best-response correspondence BRi(Qi(n)) is deﬁned as BRi(Qi(n))={ei\nαi∗| αi\n∗∈\nAi, Qi\nαi∗(n)=maxα∈Ai Qi\nα(n)} where ei\nαi∗is player i’s pure strategy or the unit vector cor-\nresponding to αi\n∗.\n4.1 SOQL’s Convergence Analysis\nIn the following we give conditions under which our proposed Q-learning algorithm converges\nto a pure strategy Nash equilibrium almost surely.\nProposition 12 If an action proﬁle α(n)=(β,α−i) is repeatedly played for m>0 iterations,\ni.e. α(n+c)=α(n) for all 1≤c<m, then:\nP i\nβ(n+m)=(1−µ)mP i\nβ(n)+(1−(1−µ)m)ui\nβ(n),\n(30)\nand\nQi\nβ(n+m)=m(1−µ)m−1Qi\nβ(n+1)−(m−1)(1−µ)mQi\nβ(n)\n+\n\u0002\n(m−1)(1−µ)m−m(1−µ)m−1+1\n\u0003\nui\nβ(n).\n(31)\nProof: From recursively using (28) it is easy to show that (30) and (31) hold.\n□\nCorollary 13 For suﬃciently large m if 0<µ<1 we have limm→∞Qi\nβ(n+m)=ui\nβ(n).\n15\nProof: Corollary 13 can be easily veriﬁed by taking the limit m→∞in (31), i.e. limm→∞m(1−\nµ)m−1=0, limm→∞(m−1)(1−µ)m=0, limm→∞(m−1)(1−µ)m−m(1−µ)m−1+1=1.\n□\nProposition 14 Let α∗=(β∗,α−i\n∗) be the action proﬁle that is played at time step n and\nn+1 where ei\nβ∗=BRi(Qi(n)). We show that if 0<µ<1 then at any following mth iteration,\nwith the probability of at least Qm\nc=1\n\u00001−(1−ϑ)c\u0001N,\nQi\nβ∗(n+m+1)=(m+1)(1−µ)mQi\nβ∗(n+1)−(m)(1−µ)m+1Qi\nβ∗(n)\n+\n\u0002\nm(1−µ)m+1−(m+1)(1−µ)m+1\n\u0003\nui\nβ∗(n),\n(32)\nand\nXi(n+m+1)=(1−ϑ)m+1 Xi(n)+(1−(1−ϑ)m+1)ei\nβ∗.\n(33)\nProof: Since β∗is BRi(Qi(n)) and also in Q-learning, Q-values are iteratively converging\nto ui\nβ∗, for every player i∈I and for every action β∈Ai, β̸=β∗we have Qi\nβ∗(n+1)>Qi\nβ∗(n)>\nQi\nβ(n) and ui\nβ∗(n+1)=ui\nβ∗(n)>Qi\nβ(n). We use induction to prove our claim. For m=1 since\nQi\nβ∗(n+1)>Qi\nβ(n)=Qi\nβ(n) it follows that at time step n+2, α∗is still the estimated best\nresponse. By (29) and considering the fact that BRi(Qi(n))=ei\nβ∗then at time n+2 we\nhave Xi(n+2)=(1−ϑ)Xi(n+1)+ϑei\nβ∗. Since ei\nβ∗is a unit vector, Xi\nβ∗(t)>ϑ and it is true\nfor all other players. Therefore, at time step n+2, the action proﬁle α∗is played with the\nprobability of at least ϑN. From the assumption that for player i, ui\nβ∗(n)>Qi\nβ∗(n), we have:\nµ2ui\nβ∗(n)>µ2Qi\nβ∗(n).\n(34)\nNow, consider the condition Qi\nβ∗(n+1)>Qi\nβ∗(n). For 0<µ<1 it is easy to show that\n2(1−µ)Qi\nβ∗(n+1)>2(1−µ)Qi\nβ∗(n).\n(35)\nBy combining (34) and (35)\n2(1−µ)Qi\nβ∗(n+1)+µ2ui\nβ∗(n)>2(1−µ)Qi\nβ∗(n)+µ2Qi\nβ∗(n).\nBy subtracting (1−µ)2Qi\nβ∗(n) from the both sides,\n2(1−µ)Qi\nβ∗(n+1)+µ2ui\nβ∗(n)−(1−µ)2Qi\nβ∗(n)>\n2(1−µ)Qi\nβ∗(n)+µ2Qi\nβ∗(n)−(1−µ)2Qi\nβ∗(n).\n(36)\nFrom (31), 2(1−µ)Qi\nβ∗(n+1)+µ2ui\nβ∗(n)−(1−µ)2Qi\nβ∗(n) is equivalent to Qi\nβ∗(n+2) and there-\nfore:\nQi\nβ∗(n+2)>Qi\nβ∗(n)>Qi\nβ(n)\n(37)\nRecall that other actions β∈Ai are not played during the m iterations, i.e. Qi\nβ(n)=Qi\nβ(n+\n1)=Qi\nβ(n+2). By substituting Qi\nβ(n) with Qi\nβ(n+2) in (37) we get Qi\nβ∗(n+2)>Qi\nβ(n+2)\nwhich means that β∗is still the estimated best response for player i at iteration n+2. By\n16\nrepeating the argument above for all players we show that at time step n+2, α∗remains\nthe estimated best response and the claim in (32) follows for m=1.\nNow assume, at every iteration c where 1≤c≤m−1, a∗is played with probability Qm−1\nc=1\n\u00001−\n(1−ϑ)c\u0001N. At time step m, Xi is updated as:\nXi(n+m+1)=(1−ϑ)m+1 Xi(n)+(1−(1−ϑ)m+1)ei\nβ∗.\n(38)\nBy Proposition 12,\nQi\nβ∗(n+m+1)=(m+1)(1−µ)mQi\nβ∗(n+1)−(m)(1−µ)m+1Qi\nβ∗(n)\n+\n\u0002\nm(1−µ)m+1−(m+1)(1−µ)m+1\n\u0003\nui\nβ∗(n),\n(39)\nSince for player i any action β̸=β∗is not played, it follows that Qi\nβ(n+m+1)=Qi\nβ(n).\nMoreover, Qi\nβ∗(n+m+1)>Qi\nβ∗(n+m)>Qi\nβ(n+m) and ui\nβ∗(n+m+1)>Qi\nβ(n+m). From (38),\nat time step n+m+2, α∗with probability of at least\n\u00001−(1−ϑ)n\u0001N and from Proposition\n12,\nQi\nβ(n+m+2)=2(1−µ)Qi\nβ(n+m+1)−(1−µ)2Qi\nβ(n+m)+µ2ui\nβ(n+m).\n(40)\nWith the same argument as for n=1, Qi\nβ∗(n+m+2)>Qi\nβ(n+m+2). Therefore, the estimated\nbest response for player i is not changed. In other words BRi(Qi(n+m+2))=BRi(Qi(n))=\nei\nβ∗. By substituting (38) into (29) we get Xi(n+m+2)=(1−ϑ)m+2Xi(t)+(1−(1−ϑ)m+2)ei\nβ∗,\nwhich completes the induction argument.\n□\nCorollary 15 If for suﬃciently large m>0 the conditions of Proposition 14 hold, then for\nevery player i the following holds with probability Q∞\nc=1[\n\u00001−(1−ϑ)c\u0001N:\nlim\nm→∞Qi\nβ∗(n+m)=ui(α∗), lim\nm→∞Xi(n+m)=ei\nβ∗.\n(41)\nTheorem 16 For suﬃciently large m, if the conditions in Proposition 14 hold, then X(n+\nm) converges to a neighborhood of α∗with probability one.\nProof: From Proposition 14 we know that α∗is played with probability Q∞\nc=1[\n\u00001−(1−\nϑ)c\u0001N. Inspired by Proposition 6.1 in Chasparis et al. (2011), we show that this probability\nis strictly positive. The product Q∞\nc=1(1−(1−ϑ)c) is non-zero if and only if P∞\nc=1log(1−(1−\nϑ)c)>−∞. Alternatively we can show\n−\n∞\nX\nc=1\nlog(1−(1−ϑ)c)<∞.\n(42)\nBy using the limit comparison test and knowing 0<1−ϑ<1:\nlim\nc→∞\n−log(1−(1−ϑ)c)\n(1−ϑ)c\n= lim\nc→∞\n1\n1−(1−ϑ)c)=1.\n17\nTherefore, (42) holds if and only if P∞\nc=1 (1−ϑ)c<∞. The latter holds since\n∞\nX\nc=1\n(1−ϑ)c=\n1\n1−(1−ϑ)= 1\nϑ < ∞.\nBy (42) we have limm→∞\nQm\nc=1(1−(1−ϑ)m)>0, so we can conclude that ∀η, 0<η<1, ∃M s.t. (1−\n(1−ϑ)m)≥η\n∀m≥M. In other words after M iterations X(n+M) enters a ball B1−η(α∗)\nwith probability QM\nc=1(1−(1−ϑ)c)N. As discussed in Wang and Pavel (2014), and following\nthe proof of Theorem 3.1 in Marden et al. (2009), for M≥log1−ϑ(1−η)>0, trajectories of\nX(n+M) enters a neighborhood of α∗with probability Q∞\nc=1(1−(1−ϑ)c)N; hence, converges\nto α∗almost surely.\n□\n4.1.1 Perturbation\nWe proved that the SOQL converges to a neighborhood of α∗, i.e. B1−η(α∗), almost surely.\nRecall that in a learning algorithm it is essential that the action space is well explored;\notherwise, it is likely that the estimated equilibrium does not converge to the true Nash\nequilibrium. Thus, the action selection procedure should allow players to explore new ac-\ntions even if the new actions are sub-optimal. In this section we discuss the conditions under\nwhich the estimated equilibrium can reach an actual Nash equilibrium. The perturbation\nfunctions are usually carefully designed to suit this need. A perfect perturbation function\nis decoupled from dynamics of the learning algorithm, adjustable and has minimum eﬀect\non perturbing the optimal solution (Wang and Pavel (2014)).\nIn the standard Q-learning scheme, the Boltzmann action selection map has already\nincorporated the exploration feature by using the temperature parameter τ (Leslie and\nCollins (2005)). In order to implement such exploration feature in the modiﬁed algorithm,\nwe use a perturbation function. The perturbed mixed strategy for player i is deﬁned as\n˜Xi\nj=(1−ρi(Xi,ξ))Xi\nj+ρi(Xi,ξ) 1i\nj/|Ai|.\n(43)\nwhere ρi(Xi,ξ) is the perturbation function. The perturbation function ρi:X i×[¯ϵ,1]→[0,1]\nis a continuously diﬀerentiable function where for some ξ∈(0,1) suﬃciently close to one, ρi\nsatisﬁes the following properties:\n• ρi(Xi,ξ)=0, ∀Xi such that ∀ξ≥¯ϵ: |Xi|∞<ζ,\n• lim|Xi|∞→1 ρi(Xi,ξ)=ξ,\n• lim|Xi|∞→1\n∂ρi(Xi,ξ)\n∂Xi\nj\n|ξ=0=0, ∀j∈Ai.\nAs in (43), each player i selects a random action with a small probability ρi and selects the\naction with highest Q value with the probability (1−ρi).\nAssumption 6 Step sizes in Q update rule are adjusted based on µi\nj(n)=(1−˜Xi\nj(n)).\nAssumption 7 When all the players enter the perturbation zone, i.e. |Xi|∞>ζ, ∀i∈I, no\nmore than one player chooses a sub-optimal action at each iteration (asynchronous pertur-\nbation).\n18\nTheorem 17 If the conditions in Proposition 14 hold for a suﬃciently large m, then un-\nder Assumption 6 and Assumption 7, the estimated equilibrium α∗converges to a Nash\nequilibrium almost surely.\nProof: From Corollary 15 we have limm→∞Qi\nβ∗(n+m)=ui(α∗), and\nlim\nm→∞Xi(n+m)=ei\nβ∗,\nwhere α∗=(β∗,α−i\n∗). Assume that perturbation becomes active at some large time step ¯n+1\nand player i chooses an action β̸=β∗. From (43) we know that such perturbation happens\nwith the probability of at least ξ/|Ai|. From (28) and Assumption 6, player i updates its\naction by\nQi\nβ(¯n+2)=2 ¯Xi\nβ(¯n)Qi\nβ(¯n+1)−¯Xi\nβ(¯n)2Qi\nβ(¯n)+(1−¯Xi\nβ(¯n))2ui(β(¯n),α−i\n∗(¯n)).\n(44)\nConsider the following two cases:\n• ui(β,α−i\n∗(¯n))<ui(α∗(¯n)): In this case player i failed to improve the utility and will\nstay at the estimated equilibrium α∗almost surely.\n• ui(β,α−i\n∗(¯n))=ui(α∗(¯n)): With ¯Xi\nβ suﬃciently close to zero the Q-values for the actions\nβ is updated to a value suﬃciently close to ui(β,α−i\n∗(¯n))=ui(α∗(¯n)). Hence, in the\nworst case, when α∗(¯n) is only played once, the Q-values for the actions β and β∗may\nbecome equal. Therefore, in the action selection stage, the set of best response may\ncontain two pure actions. Consequently, the mixed strategies for both actions β and\nβ∗is updated by (29) and player i chooses one of them randomly in the next time\nstep.\n• ui(β,α−i\n∗(¯n))>ui(α∗(¯n)): In this case player i found a response that is better than\nα∗. Since the action β has small probability ¯Xi\nβ, i.e., suﬃciently close to 0, then\n(1−¯Xi\nβ) is suﬃciently close to 1. Thus from (44), Qi\nβ is updated to a value suﬃciently\nclose to ui(β,α−i\n∗(¯n)) and the action proﬁle (β,α−i\n∗(¯n)) becomes the new estimated\nbest response, i.e., α∗(¯n+1):=(β,α−i\n∗(¯n)). Note that player’s utility is improved by\nui(α∗(¯n+1))−ui(α∗(¯n)), and the potential would also increase by the same amount.\nRecall that from the ﬁnite improvement property, such improvement in potential\nfunction is not limitless and will terminate at the potential maximum, i.e.\nNash\nequilibrium in potential games. Hence, the estimated equilibrium α∗will eventually\nconverge to an actual Nash equilibrium.\n□\nIn the following, through a number of numerical experiments, we compare the performances\nof P-SBLLL and SOQL with those of BLLL and standard QL.\n5. Case Study\nMulti-robot Coverage Control (MCC) algorithms are concerned with the design of rules for\ncoordinating a set of robots’ action. The main objective of agents in MCC is to eﬃciently\ncover an area. We assume that there is a set of hidden targets in the area and the robots\n19\nare assumed to ﬁnd them. Targets are distributed randomly in the area and each target\nemits a signal in a form of a symmetric Gaussian function which can be sensed by nearby\nrobots. The area is a limited, two-dimensional, rectangular plane over which a probabilistic\nfunction is deﬁned that represents the probability of targets’ existence (e.g. (Rahili and\nRen (2014)), (Li and Cassandras (2005)) and (Guestrin et al. (2005))).\nTherefore, the\nprobability of targets’ existence at each point is proportional to the cumulative strength\nof the signals detected at that point.\nHowever, there exist diﬀerent approaches in the\nliterature to model the search area. One approach is to model the environment as a two-\ndimensional polyhedron over which an event density function is deﬁned to determine targets’\ndistribution (e.g. (Li and Cassandras (2005))). Alternatively, the area can be divided into\nVoronoi regions where the objective of agents in this approach is to converge to the centroids\nof their assigned regions (e.g. (Cortes et al. (2002)), (Cortes et al. (2005)) and (Kwok and\nMartinez (2010))).\nDuring the search for the targets, the mobile sensors regularly collect data from the\nenvironment by sensing the signal strength at diﬀerent locations. The robots use the col-\nlected data to “reinforce” their knowledge about the area or to “model” the environment.\nThis knowledge will further support agents’ decision-making process. Clearly, the overall\nsystems’ performance is directly related to agents’ location in the environment. Thus, the\ncoverage control problem can be reduced to the problem of optimally locating the robots\nbased on data collected from the area. In the following we present the formulation of our\nMCC setup.\n5.1 Problem Formulation\nConsider a ﬁnite number of robots (i.e. agents) as a network of mobile sensors scattered in\na geographic area. The area is a two-dimensional plane, divided into a L×L square grid.\nEach cell on this grid is a 1×1 square and the coordinates of its center are l=(lx,ly)∈L,\nwhere L is deﬁned as the collection of the centroids of all lattices. The robots can only\nmove between these centroids.\nThe location of agent i∈I at iteration n is αi(n):=(li\nx(n),li\ny(n)) ∈L and this deﬁnes his\naction at time step n. Let the set of robot i’s neighbor cells be deﬁned as Ni\nR(α(n)):={l∈\nL| |αi(n)−l|≤R} where R is the neighborhood radius. The motion of agents is limited to\ntheir adjacent lattices and we use Ai\nc(n) to denote this constrained action set for agent i at\ntime step n.\nA number of targets that are distributed in the area (Fig. 2.a). Each target emits a\nsignal in the form of symmetric Gaussian function. Therefore, the signal distribution on\nthe area is basically a Gaussian mixture model.\nThe location of the targets is initially\nunknown for the robots. Thus, the worth of each cell is deﬁned as the probability that a\ntarget exists at the center of that cell and is denoted by f: L→[0,1]. By sensing targets’\nsignal, the agents can determine f(l) when they are at the coordinate l. Since there is a\ndirect correlation between the strength of the sensed signal and the worth value at a given\ncell, the worth distribution is also a Gaussian mixture model (Fig. 2.b). The area covered\nby agent i at time step n is Ci:Ai7→R and is deﬁned as Ci(αi(n)):=P\nl∈Ni\nδ(n)f(l) where δ\nis the covering range.\n20\n(a) Two dimensional environment and the\nset of targets\n(b) Worth distribution as a Gaussian mix-\nture model\nFigure 2: The set of targets and their associated worth distribution\nIn our setup, each agent i lays a ﬂag at each cell as he observes that cell; the trace of\nhis ﬂags is denoted by Υ i and is detectable by other agents from the maximum distance of\n2δ. This technique allows the agents to have access to the observed areas locally without\nany need to have access to each other’s observation vectors.\n5.1.1 Game Formulation\nAn appropriate utility function has to be deﬁned to specify the game’s objective and to\nproperly model the interactions between the agents.\nIn our case, we have to deﬁne a\nutility function that takes the reward of sensing worthwhile areas. Inspired by (Rahili and\nRen (2014)), we also consider the cost of movement of the robots: the movement energy\nconsumption of agent i from time step n−1 to n is denoted by Ei\nmove=Ki(|αi(n)−αi(n−1)|),\nwhere Ki>0 is a regulating coeﬃcient. The coverage problem can now be formulated as a\ngame by deﬁning the utility function:\nui(αi(n),αi(n−1))=ϱi[Ci(αi(n))−Ci\nn(αi(n))]−Ki(|αi(n)−αi(n−1)|),\n(45)\nwhere Ci\nn(αi(n))=P\nj∈I\\i\nP\nl∈Nδ\nj ∩Nδ\ni f(l) and ϱi is deﬁned as:\nϱi=\n\u001a 1\n:αi /∈Υ j,j̸=i\n0\n:otherwise\nParameter ϱi prevents players to pick their next actions from the areas that are previously\nobserved by other agents. As in (45), Ci(αi(n))−Ci\nn(αi(n)) is the worth of area that is only\ncovered by agent i. Hence, ui only depends on the actions of player i, i.e. ui is “separable”\nfrom the actions of other players.\nLemma 18 The coverage game G:=⟨I,A,Ucov⟩is a potential game where Ucov is the col-\nlection of all utilities Ucov={ui,i=1,...,N} and the potential function is Φ(α(n),α(n−1))=\nPN\nj=1uj(αj(n),αj(n−1)).\n21\nProof: We have to show that there exists a potential function Φ:A7→R such that for any\nagent i∈I, every α−i(n)∈A−i and any αi\n1(n),αi\n2(n)∈Ai:\nΦ(αi\n2(n),α−i(n),α(n−1))−Φ(αi\n1(n),α−i(n),α(n−1))=\nui(αi\n2(n),α−i(n),α(n−1))−ui(αi\n1(n),α−i(n),α(n−1)),\n(46)\nUsing Φ as in Lemma 18 we have:\nΦ(αi\n2(n),α−i(n),α(n−1))=[ΣN\nj=1,j̸=iuj(αj(n),αj(n−1))]+ui(αi\n2(n),αi(n−1))=\nh\nΣN\nj=1,j̸=iϱj[Cj(αj(n))−Cj\nn(αj(n))]−ΣN\nj=1,j̸=i[Ki(|αj(n)−αj(n−1)|)]\ni\n+\nh\nϱi[Ci(αi\n2(n))−Ci\nn(αi\n2(n))]−Ki(|αi\n2(n)−αi(n−1)|)\ni\n,\nand\nΦ(αi\n1(n),α−i(n),α(n−1))=[ΣN\nj=1,j̸=iuj(αj(n),αj(n−1))]+ui(αi\n1(n),αi(n−1))=\nh\nΣN\nj=1,j̸=iϱj[Cj(αj(n))−Cj\nn(αj(n))]−ΣN\nj=1,j̸=i[Ki(|αj(n)−αj(n−1)|)]\ni\n+\nh\nϱi[Ci(αi\n1(n))−Ci\nn(αi\n1(n))]−Ki(|αi\n1(n)−αi(n−1)|)\ni\n,\nHence,\nΦ(αi\n2(n),α−i(n),α(n−1))−Φ(αi\n1(n),α−i(n),α(n−1))=\nϱi[Ci(αi\n2(n))−Ci\nn(αi\n2(n))]−Ki(|αi\n2(n)−αi(n−1)|)−ϱi[Ci(αi\n1(n))−Ci\nn(αi\n1(n))]−\nKi(|αi\n1(n)−αi(n−1)|)=ui(αi\n2(n),α−i(n),α(n−1))−ui(αi\n1(n),α−i(n),α(n−1)),\nand (46) follows.\n□\n5.1.2 Gaussian Model\nTargets are assumed to emit a signal in a Gaussian form that decays proportionally to\nthe distance from the target; the mobile sensor should be close enough to sense the signal.\nThus, the function f(l) is basically a worth distribution in the form of Gaussian Mixture\nover the environment. A Gaussian Mixture Model (GMM) is a weighted sum of Gaussian\ncomponents (i.e. single Gaussian functions), i.e. GMM:=f(l)=PM\nj=1ωj g(l|µj,Σj), where\nM is the number of components (i.e., targets), ωj is the weight (signal strength) of jth\ncomponent and g(l|µj,Σj)=\n1\n2π|Σj|1/2 exp[−1\n2 (l−µj)T Σ−1\nj (l−µj)] is a two-variable Gaus-\nsian function where l is the location vector; µj is the mean vector (i.e. location of the target\nj); and Σj is the covariance matrix of component j. Note that each component represents\none target’s presence probability and the whole GMM is a mixture of these components.\nTo ensure that the Gaussian distribution is representing the probability map over the area,\nthe summation of all weight coeﬃcients must be equal to one (i.e. PM\nj=1ωj=1). GMM\nparameters can be summarized into the set λ:={ωj,µj,σj|j=1,...,M}.\n22\n5.2 Learning Process\nRecall that a learning scheme can eﬃciently reach a Nash equilibrium in a potential game\nbased on the game’s ﬁnite improvement property.\nThis section discusses P-SBLLL and\nSOQL as learning schemes to solve the MCC problem. We ﬁrst review P-SBLLL and then\nwe present SOQL’s results.\n5.2.1 P-SBLLL\nA 40×40 square area with 1×1 lattices has been considered as the environment.\nThe\nGaussian distribution in this area has been chosen randomly with diﬀerent but reasonable\nweight, mean and covariance values (Fig.2). The number of targets (Gaussian components)\nis between 1 to 5 and robots have no prior knowledge about this number.\nA group of ﬁve robots (N=5) are initialized randomly through this GMM to maximize\ntheir utility function. We assumed that a robot in our MCC setup is not able to move rapidly\nfrom its current lattice to any arbitrary lattice. In other words, the range of its movement\nis bounded, i.e., constrained. Recall that the P-SBLLL scheme is a modiﬁed version of\nstandard LLL which allows players to have a constrained action set in a potential game.\nFurthermore, we showed that in a potential game, the game will stochastically converge to\nthe potential maximizer if players adhere to P-SBLLL.\nHowever, in P-SBLLL each agent must have a posteriori knowledge about the utility dis-\ntribution, i.e. GMM, to be able to calculate their future utility function ui(αi\nT ,α−i(n),α(n))\nin (10) and (11). Hence, in the case of unknown environment, it is not possible to use P-\nSBLLL scheme, unless we replace ui(αi\nT ,α−i(n),α(n)) with an estimation from the model\nˆui(αi\nT ,α−i(n),α(n)). This raises the need for an environmental model in the P-SBLLL al-\ngorithm.\nUnknown Utility Model:\nIt is common in practice to assume that the game being played is unknown to the agents,\ni.e. agents do not have any prior information about the utility function and do not know\nthe expected reward that will result from playing a certain action (Now´e et al. (2012)). It\nis because the environment may be diﬃcult to assess, and the utility distribution may not\nbe completely known. In the following, inspired by (Rahili and Ren (2014)) we introduce\nan estimation which can provide a model of the environment in model-based learning.\nExpectation Maximization:\nAssume the environment is a GMM.\nAgent i’s estimation of the mixture parameters\nλ:={ωj,µj,σj|j=1,...,M} is denoted by ˆλi={ˆωi\nj,ˆµi\nj,ˆσi\nj|j=1,...,M}. The estimation model is\n\\\nGMM:= ˆf(l)=PM\nj=1ˆωj g(l|ˆµj, ˆΣj). During the searching task, the robots will keep their\nobservations of sensed regions in their memories. Let agent i’s observation vector at it-\neration n be a sequence of its sensed values from time step 1 to n and is denoted by\nOi={Oi\n1,Oi\n2,Oi\n3,...,Oi\nn} where Oi\nn is deﬁned as the corresponding coordinates of the sensed\nlattice by agent i at time step n. Expectation Maximization (EM) algorithm is an algorithm\nthat is used in the literature to ﬁnd maximum likelihood parameters of a statistical model\nwhen there are missing data points from the model. The EM algorithm can estimate the\n23\nGMM parameters ˆλ through an iterative algorithm (Dempster et al. (1977)):\nˆωi\nj= 1\nn\nn\nX\nτ=1\nP i(j|Oi\nτ,ˆλi),\nˆµi\nj=\nPn\nτ=1P i(j|Oi\nτ,ˆλi)Oi\nτ\nPn\nτ=1P i(j|Oiτ,ˆλi)\n,\nˆσi\nj\n2=\nPn\nτ=1P i(j|Oi\nτ,ˆλi)(Oi\nτ−ˆµi\nj)(Oi\nτ−ˆµi\nj)T\nPn\nτ=1P i(j|Oiτ,ˆλi)\n,\nP i(j|Oi\nτ,ˆλi)=\nˆωi\nj g(Oi\nτ| ˆµi\nj, ˆσi\nj)\nPM\nk=1 ˆωi\nk g(Oiτ| ˆµi\nk, ˆσi\nk)\n,\n(47)\nwhere P i(j|Oi\nτ,ˆλi) is often referred to as posteriori probability of the observation vector\nOi\nτ of agent i for the jth component of Gaussian distribution. Through (47), given the\ncurrent estimated parameters, the EM algorithm estimates the likelihood that each data\npoint belongs to each component. Next, the algorithm maximizes the likelihood to ﬁnd new\nparameters of the distribution.\nRecall that Oi is the sequence of agent i’s observed coordinates within the area. Thus,\n(47) only considers the coordinates of the observed lattice l and disregards its corresponding\nsignal strength f(l). This issue is pointed out in (Rahili and Ren (2014)) and the proposed\nsolution as introduced in (Lakshmanan and Kain (2010)) is to repeat the iterative algorithm\nfor m times in worthwhile areas and m is chosen as:\nm=\n\n\n\n1+V round( f(l)\nfmode\n)\n:f(l)≥fmode\n1\n:f(l)<fmode\nwhere f(l) is the worth at coordinate of l, fmode is the threshold above which the signal is\nconsidered worthwhile for EM repetition and V is the correction factor which regulates the\nnumber of algorithm repetitions for the worthy lattices. The algorithm will run once for\nareas with a worth value of less than the threshold.\nThe variation rate of the estimation parameters ˆλi is relatively low due to the rate of\nupdate of the observation vector and the nature of the EM algorithm. The case of slow vari-\nation of Gaussian distribution is investigated in (Lim and Shamma (2013)). With assuming\na slow changing distribution, (Lim and Shamma (2013)) showed that the agents’ estima-\ntion error |ˆλ(n)−λ(n)| is decreased by extending the observations vector; consequently, the\nagents will stochastically converge to a Nash equilibrium. If we assume that the number\nof the targets (M) is a known parameter, we can use the iterative algorithm discussed in\nthis section. However, we assumed the agents do not have any prior knowledge about the\nenvironment. This lack of knowledge includes the probability distribution function and also\nthe number of targets. Thus, the standard EM algorithm can not be used.\n5.2.2 EM with Split and Merge\nIn this section, we present a new modiﬁed EM algorithm that does not need the number of\ncomponents to calculate GMM’s parameters. To achieve this goal, we propose to have a\nmechanism that can estimate the number of the targets in parallel to parameters estimation.\n24\nThe Akaike information criterion (AIC) introduced in (Akaike (1974)) is considered as\nthe main veriﬁcation method to help agents select the best estimation for the number of\nthe targets.\nThe Akaike information criterion is a measure of the relative quality of a\ndistribution model for a set of data points. Given a set of models for the same data, AIC\nestimates the quality of each model, relative to other statistical models. The AIC value of\neach model is AIC=2k−2ln(L) where L is the maximized value of the likelihood and k is the\nnumber of estimated parameters in the model. Given a collection of statistical models, the\none with minimum AIC value is the best model (Akaike (1974)). Akaike criterion considers\nthe goodness of the ﬁt as the likelihood function L and penalty of complexity of the model\nas k (i.e. the number of model parameters).\nLet the estimated number of the targets at time step n each robot i be denoted by ˆ\nMi(n).\nIn our proposed algorithm, after each nAIC time steps, each agent i∈I randomly picks a\nnumber TAIC from the set M(nAIC)={ ˆ\nMi(nAIC)+1, ˆ\nMi(nAIC)−1}⊂N. If ˆ\nMi(nAIC)−1=\n0 then M(nAIC) reduces to { ˆ\nMi(nAIC)+1}. Next, player i decides between its current\nestimated Gaussian component number\nˆ\nMi(nAIC) and TAIC according to the following\nprobabilities:\nP i\nˆ\nMi(nAIC)=\nexp( 1\nτ IAIC( ˆ\nMi(nAIC)))\nexp( 1\nτ IAIC( ˆ\nMi(nAIC)))+exp( 1\nτ IAIC(TAIC))\n,\n(48)\nP i\nTAIC =\nexp( 1\nτ IAIC(TAIC))\nexp( 1\nτ IAIC( ˆ\nMi(nAIC)))+exp( 1\nτ IAIC(TAIC))\n,\n(49)\nwhere P i\nˆ\nMi(nAIC) is the probability that player i keeps its current estimation ˆ\nMi(nAIC) and\nP i\nTAIC is the probability that player i chooses TAIC as its estimation of the number of the\ncomponents. IAIC( ˆ\nMi(nAIC)) is the inverse AIC value for agent i’s estimation distribution\nwhen agent i’s estimated number of the components is ˆ\nMi(nAIC) and IAIC(TAIC) is the\ninverse AIC value when agent i’s estimation is TAIC.\nSince the number of the components are changing every nAIC iterations, the next step is\nto determine an appropriate method for merging and splitting Gaussian components. The\nmethod proposed in (Ueda et al. (2000)) incorporates the split and merge operations into\nthe EM algorithm for Gaussian mixture estimations. Moreover, eﬃcient criteria have been\nproposed in (Ueda et al. (2000)) to decide which components should be merged or split.\nDespite the fact that the number of Gaussian components are changing over time the shape\nof estimation distribution is not changed signiﬁcantly since the set of data points is not\nincreasing so fast. In the following the merge and split algorithms are brieﬂy discussed:\nMerge:\nIn order to reduce the number of components in a Gaussian distribution, some of the com-\nponents should be merged together. However, an eﬀective criterion is needed to pick the\noptimal pairs to merge.\na) Criterion: The posteriori probability of a data point gives a good estimation about which\nGaussian component that data point belongs. If for many data points, the posteriori prob-\n25\nabilities are almost equal for two diﬀerent components, it can be perceived that the compo-\nnents are mergeable. To mathematically implement this, the following criterion for jth and\nj′th Gaussian components is proposed in (Ueda et al. (2000)) Jmerge(j,j′;ˆλ)=Pj(ˆλ)T Pj′(ˆλ)\nwhere Pj(ˆλ)=(P(j|O1,ˆλ),P(j|O2,ˆλ),...,P(j|Ot,ˆλ))T is a N-dimensional vector consisting\nof posteriori probabilities of all data points for jth Gaussian component. The criterion\nJmerge(j,j′;ˆλ) must be calculated for all possible pairs and the pair with the largest value\nis a candidate for the merge.\nb) Merging Procedure: In order to merge two Gaussian components, the distribution model\nparameters must be re-estimated. A modiﬁed EM algorithm is proposed in (Ueda et al.\n(2000)) that re-estimates Gaussian parameters based on the former distribution parameters\n(ˆλ). If the merged Gaussian from the pair of j and j′ is denoted by j′′ then the initial param-\neters for the modiﬁed EM algorithm is ω0\nj′′=ωj+ωj′, µ0\nj′′=ωjµj+ωj′µj′\nωj+ωj′\n, Σ0\nj′′=ωjΣj+ωj′Σj′\nωj+ωj′\n.\nThe initial parameter values calculated by the latter are often poor. Hence, the newly gener-\nated Gaussians should be ﬁrst processed by ﬁxing the other Gaussians through the modiﬁed\nEM. An EM iterative algorithm then run to re-estimate the distribution parameters. The\nmain steps are the same as (47) except the posteriori probability:\nP(j′′|Oτ,ˆλ)=\nˆωj′′ g(Oτ|ˆµj′′,ˆσj′′) P\nk=j,j′P(k|Oτ,ˆλ)\nP\nk=j′′ ˆωk g(Oτ|ˆµk,ˆσk)\n.\n(50)\nBy using this modiﬁed EM algorithm, the parameters of j′′th Gaussian are re-estimated\nwithout aﬀecting the other Gaussian components. In our study, the merging algorithm\ncould be repeated for several times if more than one merge step was needed.\nSplit:\nIn case of a need to increase the number of components in the estimation distribution, we\nuse the split algorithm to split one or more Gaussians. As for the merging process, an\nappropriate criterion is necessary.\na) Criterion: As the split criterion of kth component, the local Kullback-Leibler divergence\nis proposed in (Ueda et al. (2000)): Jsplit(k;ˆλ)=\nR\npk(x,ˆλ)log(\npk(x,ˆλ)\ng(x| ˆµk, ˆ\nΣk)\n) dx, where pk(x,ˆλ)\nis the local data density around kth component and is deﬁned as\npk(x,ˆλ)=\nPt\nn=1 δ(x−xn)P(k|xn,ˆλ)\nPt\nn=1P(k|xn,ˆλ)\n. The split criterion Jsplit(k,ˆλ), which represents the distance between two Gaussian com-\nponents, must be applied over all candidates and the one with the largest value will be\nselected.\nb) Splitting Procedure: A modiﬁed EM algorithm is proposed in (Ueda et al. (2000)) to\nre-estimate the Gaussian parameters. If the split candidate is the kth Gaussian component\nand the two resulting Gaussians are denoted by j′ and k′ the initial conditions are calculated\nas follows:\nω0\nj′=ω0\nk′=1\n2 ωk, Σ0\nj′=Σ0\nk′=det(Σk)1/d Id,\n(51)\n26\n(a) The logic behind the revision probability\n(b) Revision probability func-\ntion\nFigure 3: Revision Probability\nwhere Id is the d-dimensional unit matrix and d is the dimension of Gaussian function\ng(x|µk,Σk). The mean vectors µ0\nj′ and µ0\nk′ are determined by applying random perturbation\nvector ϵm, m=1,2 on µk as µ0\nj′=µk+ϵ1 and µ0\nk′=µk+ϵ2 where ||ϵm||≪||µk|| and ϵ1̸=ϵ2. The\nparameters re-estimation for j′ and k′ can be done by a modiﬁed EM algorithm similar to\nthe merge EM algorithm where the modiﬁed posteriori probability is\nP(m′|Oτ,ˆλ)= ˆωm′ g(Oτ|ˆµm′,ˆσm′) P\nl=kP(l|Oτ,ˆλ)\nP\nl=j′,k′ ˆωl g(Oτ| ˆµl, ˆσl)\n,\n(52)\nwhere m′=j′,k′. The parameters of j′ and k′ are re-estimated without aﬀecting other Gaus-\nsians. Splitting algorithm will be repeated if more than one split was necessary according\nto the Akaike criterion. By means of AIC and split-merge technique, the agents are able\nto estimate the number of targets and estimate the GMM without knowing the number of\nagents.\nRevision Probability:\nRecall that the revision probability rpi in P-SBLLL is the probability with which agent i\nwakes up to update its action. We considered a two variable function for the revision prob-\nability of each agent. The corresponding probability of each player’s revision probability\ndepends on the player’s action, i.e. player’s location in our MCC example. Each player i\ncan determine the outcome of his action as the signal strength fi(l) of his location αi=l.\nFurthermore, we assume that each agent can sense a local gradient of the worth denoted\nby gi(l). At each iteration, fi(l) and gi(l) are normalized based on the player’s maximum\nobserved fi(l) and gi(l). For each agent i, let F i and Gi be the normalized versions of fi(l)\nand gi(l) respectively.\nWe desire a revision probability function (rpi(F i,Gi)) for which the probability that\nplayers wake up depends on each player’s situation.We consider three situations which a\nrobot can fall in (Fig.3.a):\n• Situation p1 where player’s received signal and gradient is almost zero: In such situ-\nation, player has to wake up to update its action, i.e. explore, and to move toward\nworthwhile areas. Let a1 be the probability that player in p1 wakes to update its\naction.\n27\n• Situation p2 where player’s received signal and gradient is relatively high: In this\nsituation, player has entered a worthwhile region. Since the player already entered a\nworthwhile area, the need for exploration reduces comparing to the player in p1, i.e.,\na2<a1 where a2 is the probability that player in p2 wakes up.\n• Situation p3 where player’s received signal is relatively high but the gradient is almost\nzero: This situation happens when player reaches a high local value. Clearly, player\nin p3 has to remain on its position, i.e. remain asleep. Hence, a3<a2<a1 where a3 is\nthe probability that player in p3 wakes up.\nTo match with the shape of Gaussian function, we prefer to have an exponential drop as\nplayer i’s normalized signal strength F i increases. Hence, when Gi=0, the form of revision\nprobability function is as follow:\nrpi(F i,Gi=0)=e−k(F i−c),\nwhere k is the drop rate and c:=ln(a1)\nk\n. For the sake of simplicity, we consider a linear\nchange in revision probability function as G increases. As G reaches 1, the value of the\nrevision probability function must be equal to a2. Therefore, for a constant F, the slope\nof the line from G=0 to G=1 is a2−e−k(F i−c). Furthermore, y-intercept for this line is\ne−k(F i−c). Thus the complete function is of the form\nrpi(F i,Gi)=(a2−e−k(F i−c))G+e−k(F i−c).\n(53)\nAs in Fig. 3.b, the function behavior is linear versus G and exponential versus F. With\nthis independent revision, each player decides based on its status whether it is a good time\nto wake up or not. We believe it is more eﬃcient than a random selection as in BLLL.\nFurthermore, it reduces the need for unnecessary trial and errors.\nSimulation Results:\nAssume that all agents adhere to P-SBLLL and Assumptions 2 and 3 hold for the MCC\nproblem. Players’ utility function is separable and from Proposition 9, we know that the\nstochastically stable states are the set of potential maximizers. The simulation parameters\nare chosen as Ki=3×10−5 for i=1,...,N, δ=1.5, Rcom=56, a1=1, a2=0.5 and a3=0.1.\nThe worth of covered area using P-SBLLL and BLLL is shown in Fig. 5. Clearly, the\ncovered worth is higher and the convergence rate is faster in P-SBLLL. Comparing to BLLL,\nin P-SBLLL each player autonomously decides, based on its situation, to update its action\nwhile in BLLL only one random player is allowed to do the trial and error. The algorithms\nran for 10 times with diﬀerent initial conditions and for each algorithm Fig. 5 presents a\nbound and a mean for the coverage worth. The ﬁnal conﬁguration of the agents are shown\nin Fig. 6. We can see that in P-SBLLL the agents found all the targets. Although three\nagents have the targets in their sensing range, the two other agents tried to optimize their\nconﬁguration with respect to the signal distribution to maximize the coverage worth.\n28\nset n=1\nfor each robot i∈I do\ninitialize αi(1)∈L randomly\nwhile the covered worth P\ni∈ICi(αi) is not in a steady state do\nif TAIC is a sub-multiple of n then\nprocess merge or split\nfor each robot i∈I do\ndetermine rpi (see (53))\nrobot i wakes up with probability rpi\n(let S(n) be the set of robots that are awake at n)\nfor each robot i∈S(n) do\nrobot i chooses a trial action αi\nT randomly from Ai\nc\ncalculate Xi\nαi(n) and Xi\nαi\nT by using both GMM and \\\nGMM (see (10) and (11))\nαi(n+1)←αi(n) with probability Xi\nαi(n)\nor\nαi(n+1)←αi\nT with probability Xi\nαi\nT\nif αi(n+1) is αi\nT then\nadd αi(n+1) to Oi\nrun EM and update \\\nGMM\nfor each robot j∈I\\S(n) do\nαj(n+1)←αj(n)\nn←n+1\nFigure 4: P-SBLLL’s pseudo code\nFigure 5: The real-time worth of the covered area\n5.2.3 SOQL\nIn SOQL, the robots sample the environment at each time to create a memory of the payoﬀ\nof the actions that they played. When the environment is explored enough, this memory\ncan help the robots to ﬁnd the game’s global optimum.\n29\nFigure 6: Final conﬁguration of agents in (a) BLLL and (b) P-SBLLL\nset n=1\nfor each robot i∈I do\ninitialize αi(1)∈L randomly\ninitialize Qi(0) and Qi(1)∈R|L|\ninitialize Xi∈R|L|\nwhile the covered worth P\ni∈ICi(αi) is not in a steady state do\nfor each player i∈I do\nperform an action αi(n)=βi from Ai\nc based on Xi(n)\nreceive ui(n)\nui\nβ(n)←1{αi(n)=β} ui(n)\nQi\nβ(n+1)←2(1−µi)Qi\nβ(n)−(1−µi)2Qi\nβ(n−1)+µi2ui\nβ(n)\nfor each β′∈Ai,β′̸=β do\nQi\nβ′(n+1)←Qi\nβ′(n)\nXi(n+1)←(1−ϑ)Xi(n)+ϑBRi(Qi(n))\nn←n+1\nFigure 7: SOQL’s pseudo code\nFigure 8: The real-time worth of the covered area\n30\nFigure 9: Final conﬁguration of agents in (a) standard Q-learning and (b) SOQL\nFigure 10: The real-time worth of the covered area\nThe simulation parameters are chosen as Ki=3×10−5 for i=1,...,N, δ=1.5, µ=0.97,\nϑ=0.5, ζ=0.9999 and ξ=0.01. The worth of covered area by all robots, using SOQL and\nﬁrst-order Q-learning, is shown in Fig. 8. It can be seen that the convergence rate is lower\nfor the SOQL algorithm comparing to the ﬁrst-order algorithm. However, the covered worth\nis higher comparing to the ﬁrst-order case. The algorithms ran for 5 times with diﬀerent\ninitial conditions and for each algorithm Fig.\n8 presents a bound and a mean for the\ncoverage worth.\nFig. 9 shows the ﬁnal conﬁguration of the robots in standard QL and SOQL. It can be\nseen that the robots that used SOQL as their learning algorithm, successfully found all the\ntargets. However, with the same initial locations the robots who used standard QL could\nnot ﬁnd all the targets.\n6. Conclusion\nBy relaxing both asynchrony and completeness assumptions P-SBLLL in Section 3 enhanced\nthe way the group of agents interact with their environment. We showed that comparing\nto BLLL, P-SBLLL’s performance is excellent in a model-based learning scheme. A higher\nconvergence rate in P-SBLLL, as it was expected, is because players learn in parallel. Fur-\n31\nthermore, because of this simultaneous learning in P-SBLLL, agents can widely explore\nthe environment. Another valuable feature of P-SBLLL is that each agent’s exploration is\ndependent on the agent’s situation in the environment. Thus, each agent can autonomously\ndecide whether it is a good time for exploration or it is better to remain on its current state.\nThis will certainly reduce redundant explorations in P-SBLLL comparing to BLLL.\nIn Section 4 we proposed SOQL, a second-order reinforcement to increase RL’s aggre-\ngation depth. Comparing to the model-based P-SBLLL and BLLL, the convergence rate of\nSOQL is lower (Fig.10) due to the need for a wide exploration in an RL scheme. However,\nin SOQL algorithm, the equilibrium worth is the same as P-SBLLL’s equilibrium worth,\nand can be employed when the utility distribution is not a GMM or more generally, when\nthe utility structure is unknown.\nAcknowledgments\nThis work was done while the ﬁrst author was at University of Toronto. We would like to\nacknowledge support for this project from the Natural Sciences and Engineering Research\nCouncil of Canada (NSERC).\nReferences\nAkaike, H. (1974). A new look at the statistical model identiﬁcation. IEEE transactions on\nautomatic control, 19(6):716–723.\nAl´os-Ferrer, C. and Netzer, N. (2010). The logit-response dynamics. Games and Economic\nBehavior, 68(2):413–427.\nBena¨ım, M. (1999). Dynamics of stochastic approximation algorithms. In Seminaire de\nprobabilites XXXIII, pages 1–68. Springer.\nBlume, L. E. (1993). The statistical mechanics of strategic interaction. Games and economic\nbehavior, 5(3):387–424.\nChasparis, G. C., Shamma, J. S., and Rantzer, A. (2011). Perturbed learning automata in\npotential games. In CDC, pages 2453–2458. IEEE.\nCortes, J., Martinez, S., and Bullo, F. (2005). Spatially-distributed coverage optimization\nand control with limited-range interactions. ESAIM: Control, Optimisation and Calculus\nof Variations, 11(4):691–719.\nCortes, J., Martinez, S., Karatas, T., and Bullo, F. (2002). Coverage control for mobile\nsensing networks. In ICRA, volume 2, pages 1327–1332. IEEE.\nCoucheney, P., Gaujal, B., and Mertikopoulos, P. (2014). Penalty-regulated dynamics and\nrobust learning procedures in games. Mathematics of Operations Research, 40(3):611–633.\nDempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from in-\ncomplete data via the EM algorithm. Journal of the royal statistical society. Series B\n(methodological), pages 1–38.\n32\nGuestrin, C., Krause, A., and Singh, A. P. (2005).\nNear-optimal sensor placements in\ngaussian processes. In ICML, pages 265–272. ACM.\nHasanbeig, M. and Pavel, L. (2017a). Distributed coverage control by robot networks in un-\nknown environments using a modiﬁed EM algorithm. International Journal of Computer,\nElectrical, Automation, Control and Information Engineering, 11(7):805–813.\nHasanbeig, M. and Pavel, L. (2017b). On synchronous binary log-linear learning and second\norder Q-learning. IFAC, 50(1):8987–8992.\nKash, I. A., Friedman, E. J., and Halpern, J. Y. (2011).\nMultiagent learning in large\nanonymous games. Journal of Artiﬁcial Intelligence Research, 40:571–598.\nKwok, A. and Martinez, S. (2010). Deployment algorithms for a power-constrained mobile\nsensor network. International Journal of Robust and Nonlinear Control, 20(7):745–763.\nLakshmanan, V. and Kain, J. S. (2010). A gaussian mixture model approach to forecast\nveriﬁcation. Weather and Forecasting, 25(3):908–920.\nLaraki, R. and Mertikopoulos, P. (2013). Higher order game dynamics. Journal of Economic\nTheory, 148(6):2666–2695.\nLeslie, D. and Collins, E. (2005).\nIndividual Q-learning in normal form games.\nSIAM\nJournal on Control and Optimization, 44(2):495–514.\nLi, W. and Cassandras, C. G. (2005). Distributed cooperative coverage control of sensor\nnetworks. In CDC, pages 2542–2547. IEEE.\nLim, Y. and Shamma, J. S. (2013). Robustness of stochastic stability in game theoretic\nlearning. In ACC, pages 6145–6150. IEEE.\nMarden, J. R., Arslan, G., and Shamma, J. S. (2009). Joint strategy ﬁctitious play with\ninertia for potential games. IEEE Transactions on Automatic Control, 54(2):208–220.\nMarden, J. R. and Shamma, J. S. (2012). Revisiting log-linear learning: Asynchrony, com-\npleteness and payoﬀ-based implementation. Games and Economic Behavior, 75(2):788–\n808.\nMarden, J. R. and Wierman, A. (2008). Distributed welfare games with applications to\nsensor coverage. In CDC, pages 1708–1713. IEEE.\nMonderer, D. and Shapley, L. S. (1996). Potential games. Games and economic behavior,\n14(1):124–143.\nMorgenstern, O. and Von Neumann, J. (1953). Theory of games and economic behavior.\nPrinceton university press.\nNash, J. (1951). Non-cooperative games. Annals of mathematics, pages 286–295.\nNow´e, A., Vrancx, P., and De Hauwere, Y.-M. (2012). Game theory and multi-agent rein-\nforcement learning. In Reinforcement Learning, pages 441–470. Springer.\n33\nRahili, S. and Ren, W. (2014). Game theory control solution for sensor coverage problem\nin unknown environment. In CDC, pages 1173–1178. IEEE.\nShah, D. and Shin, J. (2010). Dynamics in congestion games. ACM SIGMETRICS Perfor-\nmance Evaluation Review, 38(1):107–118.\nSutton, R. S. and Barto, A. G. (2011). Reinforcement learning: An introduction. Cambridge,\nMA: MIT Press.\nUeda, N., Nakano, R., Ghahramani, Z., and Hinton, G. E. (2000). Split and merge EM\nalgorithm for improving gaussian mixture density estimates.\nJournal of VLSI signal\nprocessing systems for signal, image and video technology, 26(1-2):133–140.\nWang, Y. and Pavel, L. (2014). A modiﬁed Q-learning algorithm for potential games. IFAC,\n47(3):8710–8718.\nYoung, H. P. (1993). The evolution of conventions. Econometrica: Journal of the Econo-\nmetric Society, pages 57–84.\n34\n",
  "categories": [
    "cs.LG",
    "cs.MA"
  ],
  "published": "2018-02-07",
  "updated": "2018-09-18"
}