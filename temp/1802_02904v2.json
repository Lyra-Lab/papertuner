{
  "id": "http://arxiv.org/abs/1802.02904v2",
  "title": "Deep Reinforcement Learning for Image Hashing",
  "authors": [
    "Yuxin Peng",
    "Jian Zhang",
    "Zhaoda Ye"
  ],
  "abstract": "Deep hashing methods have received much attention recently, which achieve\npromising results by taking advantage of the strong representation power of\ndeep networks. However, most existing deep hashing methods learn a whole set of\nhashing functions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accuracy greatly.\nInspired by the sequential decision ability of deep reinforcement learning, we\npropose a new Deep Reinforcement Learning approach for Image Hashing (DRLIH).\nOur proposed DRLIH approach models the hashing learning problem as a sequential\ndecision process, which learns each hashing function by correcting the errors\nimposed by previous ones and promotes retrieval accuracy. To the best of our\nknowledge, this is the first work to address hashing problem from deep\nreinforcement learning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose a deep\nreinforcement learning hashing network. In the proposed network, we utilize\nrecurrent neural network (RNN) as agents to model the hashing functions, which\ntake actions of projecting images into binary codes sequentially, so that the\ncurrent hashing function learning can take previous hashing functions' error\ninto account. (2) We propose a sequential learning strategy based on proposed\nDRLIH. We define the state as a tuple of internal features of RNN's hidden\nlayers and image features, which can reflect history decisions made by the\nagents. We also propose an action group method to enhance the correlation of\nhash functions in the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach.",
  "text": "1\nDeep Reinforcement Learning for Image Hashing\nYuxin Peng, Jian Zhang and Zhaoda Ye\nAbstract—Deep hashing methods have received much attention\nrecently, which achieve promising results by taking advantage\nof the strong representation power of deep networks. However,\nmost existing deep hashing methods learn a whole set of hashing\nfunctions independently, while ignore the correlations between\ndifferent hashing functions that can promote the retrieval accu-\nracy greatly. Inspired by the sequential decision ability of deep\nreinforcement learning, we propose a new Deep Reinforcement\nLearning approach for Image Hashing (DRLIH). Our proposed\nDRLIH approach models the hashing learning problem as a\nsequential decision process, which learns each hashing function\nby correcting the errors imposed by previous ones and promotes\nretrieval accuracy. To the best of our knowledge, this is the\nﬁrst work to address hashing problem from deep reinforcement\nlearning perspective. The main contributions of our proposed\nDRLIH approach can be summarized as follows: (1) We propose\na deep reinforcement learning hashing network. In the proposed\nnetwork, we utilize recurrent neural network (RNN) as agents\nto model the hashing functions, which take actions of projecting\nimages into binary codes sequentially, so that the current hashing\nfunction learning can take previous hashing functions’ error into\naccount. (2) We propose a sequential learning strategy based\non proposed DRLIH. We deﬁne the state as a tuple of internal\nfeatures of RNN’s hidden layers and image features, which can\nreﬂect history decisions made by the agents. We also propose an\naction group method to enhance the correlation of hash functions\nin the same group. Experiments on three widely-used datasets\ndemonstrate the effectiveness of our proposed DRLIH approach.\nIndex Terms—Deep reinforcement learning, image hashing,\nimage retrieval.\nI. INTRODUCTION\nW\nITH rapid growth of images on the web, the large\nscale image retrieval has attracted much attention.\nMany hashing methods have been proposed for the fast image\nretrieval [1]–[13]. Generally speaking, the goal of hashing\nmethods is to learn several mapping functions, so that the sim-\nilar images are mapped into similar binary codes. Traditional\nhashing methods use hand-crafted features (e.g. GIST [14],\nBag-of-Visual-Words [15]) as image representations, which\ncan not well represent the image content and limit the per-\nformance of image retrieval. Inspired by the recent success\nof deep networks on many computer vision tasks such as\nimage classiﬁcation and object detection [16], deep hashing\nmethods [3], [17]–[24] have been proposed to take advantage\nof feature representation power of deep neural networks.\nExisting deep hashing methods [3], [17], [25], [26] have\nachieved promising results on image retrieval. However, they\nlearn the whole set of hashing functions independently, which\nThis work was supported by National Natural Science Foundation of China\nunder Grant 61771025 and Grant 61532005.\nThe authors are with the Institute of Computer Science and Technology,\nPeking University, Beijing 100871, China. Corresponding author: Yuxin Peng\n(e-mail: pengyuxin@pku.edu.cn).\nignore the correlations between different hashing functions that\ncan promote the retrieval accuracy greatly. There exist several\ntraditional hashing methods [4], [27] that learn the hash code\nsequentially, but these methods require complicated optimiza-\ntion that cannot be directly adopted into deep networks.\nRecently, deep reinforcement learning has achieved some\nbreakthroughs. For example, deep reinforcement learning\nachieves\nhuman-level\nperformance\nin\nAtari\ngames\nand\nGO [28], [29]. A standard reinforcement learning model\nincludes an agent and an environment. The agent receives the\ninformation from the environment and chooses the actions to\nmaximize the sum of a reward function. It is noted that the\nhashing problem has native relationship to the reinforcement\nlearning, which is the motivation of this paper. 1) In the\nhashing problem, hashing functions can be regarded as agents,\nwhich take actions to project images into binary hash codes.\n2) The agents in the reinforcement learning framework choose\nactions to maximize the sum of reward in a task, where the\ndecisions should not be made independently. This property\nenlightens us that if we regard the hash code generation as\na sequential task, it is possible to learn hashing functions\ndependently within reinforcement learning framework.\nBased on the above analysis, in this paper, we propose a\nDeep Reinforcement Learning approach for Image Hashing\n(DRLIH). Instead of learning a whole set of hashing functions\nindependently, our proposed DRLIH approach models the\nhashing learning process as a sequential decision making\nprocess by the designed deep reinforcement learning network.\nThe main contributions of our proposed approach can be\nsummarized as follows:\n• A Deep reinforcement learning hashing network is\nproposed to learn hashing functions sequentially and\nprogressively. The proposed network consists of a feature\nrepresentation network and a policy network. The policy\nnetwork is composed by a RNN network, and it serves\nas the agent to sequentially project images into binary\ncodes. We design the policy network to generate the\nprobability of projecting images into hash code 1, and\ncalculate the probability of hash code 0. We also propose\ntwo hierarchical reward functions to drive the training of\nour proposed DRLIH network.\n• A sequential learning strategy is proposed to capture\nthe ranking errors caused by previous generated hash\ncodes. We deﬁne the states as tuples of the image\nfeatures and internal features of RNN, which reﬂect the\nhistory decisions, thus the agent can obtain the history\ninformation and capture the previous ranking errors to\nmake next decision. We also propose the action group as\nthe minimal step of the agent to enhance the relevance\nof hash functions in the same group and promote the\nretrieval accuracy.\narXiv:1802.02904v2  [cs.CV]  24 Feb 2019\n2\nExperiments on three widely-used datasets demonstrate the\neffectiveness of the proposed DRLIH approach. The rest of\nthis paper is organized as follows. Section II reviews some\nrepresentative related works. Section III presents our proposed\ndeep reinforcement learning approach for image hashing,\nsection IV shows the experiments on three widely-used image\ndatasets, and section V concludes this paper.\nII. RELATED WORK\nIn this section, we brieﬂy review some related works from\ntwo aspects: image hashing and deep reinforcement learning.\nA. Image Hashing\nThe goal of image hashing methods is to project images\ninto Hamming space, where similar images are mapped into\nsimilar hash codes to realize efﬁcient image retrieval. Existing\nhashing methods can be categorized into two classes: data-\nindependent and data-dependent. For the data-independent\nmethods, the most representative one is Locality Sensitive\nHashing (LSH) [2]. LSH uses random projections obtained\nfrom Gaussian distributions to map images into binary codes\nwhile preserving the cosine similarity. However, LSH needs\nto generate longer codes and multiple hashing tables to\nachieve satisfactory performance. Thus some works extend\nLSH framework to tackle this issue. Lv et al. [30] propose\nMulti-probe LSH, which uses a multi-probe strategy to avoid\ngenerating multiple hashing tables. Raginsky et al. [31] pro-\npose SIKH to extend LSH into kernel space.\nAccording to the utilization of side information, the data-\ndependent methods can be further classiﬁed into unsupervised\nmethods and supervised methods. Unsupervised methods do\nnot require label information to learn hashing functions. For\nexample, Weiss et al. [5] propose Spectral Hashing method,\nwhich generates balanced hash codes by solving a spectral\ngraph partitioning problem. Liu et al. [32] propose Anchor\nGraph Hashing (AGH) method, which learns hashing functions\nby exploiting the neighborhood structure of data samples\nby anchor graph. Gong and Lazebnik [33] present Iterative\nQuantization (ITQ) method, which simultaneously maximizes\nthe variance of each hash code and minimizes the quantization\nloss. Zhang et al. [34] propose Topology Preserving Hashing\n(TPH) method, which learns hashing functions by preserving\nnot only the neighborhood relationships but also the neigh-\nborhood rankings of data points. Irie et al. [35] propose LLH\nmethod to model the local linearity of manifolds by locality\nsensitive sparse coding, which tends to ﬁnd similar images\nlocated in the same manifold as the query. Supervised hashing\nmethods further exploit label information to learn hashing\nfunctions for better preserving the semantic similarity of image\ndata. For example, BRE [36] method is proposed to construct\nhashing functions to explicitly preserve the original distance\n(e.g. Euclidean distance) when mapping into Hamming space.\nWang et al. [4] propose SSH method, which learns hashing\nfunctions by minimizing the empirical error over labeled data\nand the information entropy of both labeled and unlabeled\ndata. Liu et al. [37] propose KSH method, which utilizes\nthe equivalence between optimizing the inner products and\nthe Hamming distances of hash codes, to minimize Hamming\ndistance between similar pairs of data while maximizing the\nHamming distance between dissimilar pairs. Shen et al. [38]\npresent SDH method, which expects the generated hash codes\nto be optimal for classiﬁcation. Besides pairwise labeled\ninformation, some methods also exploit ranking information\nprovided by labels, such as OPH [39] and RPH [40] methods.\nThe aforementioned learning based methods use hand-\ncrafted features to represent image contents, which limit\ntheir retrieval performance. Inspired by the successful appli-\ncation of deep networks in image classiﬁcation and object\ndetection [16], some deep hashing methods [3], [17]–[22]\nare proposed. Xia et al. [17] propose a two-stage CNNH\nmethod, which learns approximate hash codes by preserving\nthe pairwise semantic information in the ﬁrst hash code\nlearning stage, and then trains a deep hashing network by\nusing the learned hash codes as labels. However, the two-stage\nscheme causes that the deep networks cannot give feedback\nfor generating better hash codes in the ﬁrst stage, which\nlimits its retrieval performance. Lai et al. [3] propose NINH\nmethod to address this issue. NINH jointly learns the hashing\nfunctions and image feature representations simultaneously\nin Network in Network architecture [41]. NINH uses triplet\nranking loss [42]–[44] to model the semantic ranking infor-\nmation provided by labels, there are several works follow the\none-stage scheme of NINH. For example, BSDH [19] further\nlearns weights for each hashing function so that the length of\nhash codes can be determined. Zhu et al. [22] propose DHN\nmethod to further consider the quantization errors caused by\nhashing layer to promote retrieval accuracy. Yao et al. [18]\npropose DSRH to further consider the orthogonal constraints\nto make hash codes independent. Zhang et al. [23] propose\nSSDH method, which trains the deep hashing network in\na semi-supervised fashion to enhance the retrieval accuracy\nand generality of hashing functions. Although deep hashing\nmethods have achieved promising results, they usually learn\na whole set of hashing functions independently and directly,\nwhich ignore the correlations between different hashing func-\ntions that can promote the retrieval accuracy greatly. In this\npaper, we intend to address this issue from deep reinforcement\nlearning perspective.\nB. Deep Reinforcement Learning\nThe reinforcement learning is the problem faced by an agent\nthat must learn behavior through trial-and-error interactions\nwith a dynamic environment [45]. In the standard deep re-\ninforcement learning model, the agent receives the current\nstate of the environment as input, and chooses an action\nas output. The action changes the state of the environment,\nand the environment communicates to the agent through a\nscalar reinforcement signal named reward, which reﬂects the\nquality of the taken actions. The goal of the reinforcement\nlearning is to train the agent to choose actions that maximize\nthe sum of reward. Recently, deep reinforcement learning\nmethods have achieved some progresses in many domains.\nMnih et al. [28] utilize deep neural networks to learn an action-\nvalue function to play Atari games, which reaches human-\nlevel performance. Silver et al. [29] use policy network and\n3\nvalue network to play Go and beat the world-level professional\nplayer. Deep reinforcement learning has also been applied in\nvarious computer vision tasks. Caicedo et al. [46] propose a\ndeep reinforcement learning method for active object localiza-\ntion, where the agent is trained to deform the bounding box\nusing sample transformation actions. Zhou et al. [47] propose\na deep reinforcement learning based image caption model,\nwhich utilizes a “policy network” and a “value network” to\ncollaboratively generate captions. Zhao et al. [48] utilize the\ninformation entropy to guide a reinforcement learning agent to\nselect the key part of an image for better image classiﬁcation.\nRao et al. [49] propose attention-aware deep reinforcement\nlearning (ADRL) to discard the misleading and confounding\nframes and focus on the attentions in face videos for better\nperson recognition. Inspired by the recent advances of deep\nreinforcement learning, we think that hashing problem can be\nmodeled by deep reinforcement learning from two aspects:\n1) Hashing functions can be regarded as agents, which take\nactions to project images into binary hash codes. 2) The agents\nin the reinforcement learning framework can choose actions\nto maximize the sum of rewards in a task, which enlightens us\nthat if we regard the hash code generation as a sequential task,\nit is possible to learn hashing functions dependently within\nreinforcement learning framework.\nIII. DEEP REINFORCEMENT LEARNING HASHING\nDifferent from existing deep hashing methods, we model\nthe hashing problem as a Markov Decision Process (MDP),\nwhich provides a formal framework to model an agent that\nmakes a sequence of decisions. In our proposed DRLIH, we\nconsider a batch of images as the environment, and deﬁne\nthe states as image features combined with images’ internal\nfeatures of the policy network. The agent projects images into\nbinary codes based on the environment and its current states.\nIn the following part, we ﬁrst give the formal notations of\nhashing problems, then we introduce the deﬁnition of state,\naction and reward in our proposed deep reinforcement learning\nhashing network, then we introduce the network structure\nof our proposed DRLIH in detail, ﬁnally we introduce the\nsequential learning strategy of DRLIH network.\nA. Notations\nGiven a set of n images X ∈RD. The goal of hashing\nmethods is to learn a set of q hashing functions H =\n[h1, h2, · · · , hq], which encode an image x ∈X into a q-\ndimensional binary code H(x) in the Hamming space, while\npreserving the semantic similarity of images. Most deep hash-\ning methods learn H independently and directly, while ignore\nthe correlations between different hashing functions. In this\npaper we intend to learn the hashing functions H sequentially\nby deep reinforcement learning.\nB. Deﬁnition of the reinforcement hashing learning\n1) State: In our DRLIH approach, the current hash code\nsupposes to be generated in serialization which can capture the\nranking errors caused by the previous generated hash codes.\nSo the state has to reﬂect the history information of previous\nhash codes. We deﬁne the state as a tuple (h, i), where h\nis a history action vector of the generated hash codes, and i\ndenotes the image feature vector. The image feature vector is\nextracted from the original images using a pre-trained CNN\nmodel, and the history action vectors can be obtained from\nthe policy network.\n2) Action: Given the state tuple s = (h, i), the agent will\npredict the probability of the actions for current state. It is\nnoted that the hashing problem only has two possible actions\n(1 or 0) and the sum of the action probability equals to 1.\nDifferent from most of the reinforcement methods that predict\nthe probability distribution for every possible action, we take\nthe probability of the hash code 1 as the policy network output.\nThe overall probability distribution is formulated as follows:\nP(a|s, θ) =\n\u001a 1 −policy(s, θ)\na=0\npolicy(s, θ)\na=1\n(1)\nwhere the policy(s, θ) denotes the output of policy network\nwith input state s and parameters θ.\nConsidering that only one bit of hash code does not have\nenough ability to correct the history ranking errors, we propose\nan action group to address this problem. An action group is\ncomposed by k adjacent hash functions. Each action in the\ngroup shares the same reward which is designed to enhance\nthe ability of correcting the ranking errors. The action group\nis the minimal step for the agent which is the actual action\ndeﬁnition in the framework. The action probability πθ(si, Ai)\nis formulated as follows:\nAi = [ati+1, ati+2, ..ati+k]\nπθ(si, Ai) =\nk\nY\nj=1\nP(ati+j|ˆsi,j, θ)\n(2)\nwhere Ai is the i-th action group, ati+1..k is the element of\nthe action group, ˆsi,j is the input state of the ati+1 and will\nbe given more details in next subsection.\n3) Reward: After we obtain the generated hash codes, we\nuse a triplet ranking loss [42]–[44] as the reward function\nto measure the quality of generated codes. It is noted that the\nprobability of the hash code 1 can be regarded as relaxed hash\ncode, which is widely used in many hashing methods [3], [19],\n[22]. The higher probability of the action to project images\ninto hash code 1, the value of the probability is closer to 1 and\nvice versa. So we adopt the probability sequence of projecting\nimages into hash code 1 as the relaxed hash code to calculate\nthe reward.\nFor training images (X, Y), wehre Y denotes the corre-\nsponding labels. We sample a set of triplet tuples based on\nthe labels, T\n= {(xi, x+\ni , x−\ni )}t\ni=1, in which xi and x+\ni\nare two similar images with the same labels, while xi and\nx−\ni\nare two dissimilar images with different labels, and t is\nthe number of sampled triplet tuples. For the triplet tuples\n(xi, x+\ni , x−\ni ), i = 1 · · · t, the reward function is deﬁned as:\nJ (h(xi), h(x+\ni ), h(x−\ni )) =\nmax(0, mt + ∥h(xi) −h(x+\ni )∥2 −∥h(xi) −h(x−\ni )∥2) (3)\nwhere ∥· ∥2 denotes the Euclidean distance, h(·) denotes\nthe probability sequence of the corresponding image, and the\n4\nπ(a|s)\nEnvironment\nAgent\nState\nImages\n·\nRepresentation learning layers\nLabel\nImage feature\nAction\nReward\nReward \nFucntion\nPolicy\nNetwork\nHistory\nHash codes\nFig. 1: Overview of our proposed deep reinforcement learning hashing network, which consists of a representation learning\nnetwork, a policy network and a reward function. The policy network serves as the agent and makes decisions to project images\ninto binary codes, while the reward function drives the sequential training of the whole network.\nconstant parameter mt deﬁnes the margin between the relative\nsimilarities of the two pairs (xi, x+\ni ) and (xi, x−\ni ), that is to\nsay, we expect the distance of dissimilar pair (xi, x−\ni ) to be\nlarger than that of similar pair (xi, x+\ni ) by a margin of at\nleast mt. Triplet ranking loss based reward can reﬂect the\nsemantic ranking information, which well evaluates the quality\nof previous generated hash codes. It is noted that although\ntriplet ranking loss is based on semantic labels, our proposed\napproach is different from supervised hashing methods. Since\nwe only use triplet ranking loss to evaluate how well the\nlearned hash functions performs on the current environment,\nthen the next hashing function can make decisions to generate\nhash codes based on the calculated reward, thus our proposed\napproach ﬁts the reinforcement learning paradigm.\nThere are two hierarchical rewards to encourage the agent\nto ﬁnd the correct hash codes. The ﬁrst reward is the action\ngroup reward which mainly focuses on the hash code quality\nin the group level. The second reward is the global action\nreward which focus on the quality of the whole hash code.\nThe hierarchical rewards are deﬁned as:\nRg\ni,j = −J (hj(xi), hj(x+\ni ), hj(x−\ni ))\nRG\ni = −J (h(xi), h(x+\ni ), h(x−\ni ))\n(4)\nwhere Rg\ni,j denotes the group action reward of the i-th image\nin j-th action group, hj denotes the probability sequence of\nthe j-th action group, and RG\ni denotes the global action reward\nof the i −th image.\nC. Deep reinforcement learning hashing network\nThe overall framework of our proposed deep reinforcement\nlearning network is shown in Figure 1, which consists of\ntwo parts. The ﬁrst part is the environment including a\nrepresentation network and a reward function, which provide\nthe reward and state for the agent. The second part is a policy\nnetwork serves as the agent, which obtains the state from the\nenvironment and generates the hash codes.\n1) The representation network: The representation network\nserves as a feature extractor, it is a deep convolutional network\ncomposed of several convolutional layers and fully connected\nlayers. The representation network provides the image features\ni in the state tuple (h, i). We adopt the VGG-19 network [50]\nas the representation network. The ﬁrst 18 layers follow the\nexactly same settings in the VGG-19 network. We mainly\nfocus on the design of the policy network and the reward\nfunction.\n2) Policy network: The policy network is composed of a\nRNN layer and a policy layer. RNN layer transforms the image\nfeatures into an internal state, while the policy layer further\nmaps the internal state into a policy probability. The main idea\nbehind the RNN model is a built-in memory cell, which stores\ninformation over previous steps and implies the history actions.\nFigure 2 shows the details of the policy network, which maps\nthe state tuple (h, i) to the probability of an action group.\nThe memory cell aggregates information from two sources:\nthe previous cell memory unit, and the input vector in current\nstep. Formally, for a state tuple (h, i), the RNN layer maps the\ninput to an output sequence by computing activations of the\nunits in the network with the following equation recursively:\nct = tanh(Wxixt + bxi + Whict−1 + bhi)\n(5)\nwhere xt, ct are the input and hidden vectors, t denotes the\nt-th step, Wxi and Whi are the weight matrix from the input\nxt and hidden vectors to the new hidden state, bxi and bhi\nare the bias terms.\nWe initialize the RNN layer with c0 = h and x0 = i. But\nin the following step, we set the xi = ci−1 to emphasis the\nhistory action information in one action group. Although the\nimage feature is only the input of the ﬁrst step, the information\n5\nHistory\nπ(a|s)\nHistory \nRNN \ncell\nImage\n feature\nπ(a|s)\nAction group\nAction group \nreward\nπ(a|s)\nπ(a|s)\nHistory \nAction \ngroup\nImage\n feature\nImage\n feature\nGlobal reward\nImage\n feature\nFig. 2: The unrolled details of the policy network. During the training stage, the global reward ensures the generated codes to\npreserve the semantic ranking information, while the action group reward drives the network to correct the errors caused by\nprevious generated hash codes.\nof the image feature remains in the hidden state with the\nchanging history action information. Thus the hash codes are\ninﬂuenced by the history information and adjusted to correct\nthe errors caused by previous actions. The last hidden state ck\nwill be regarded as the history information h to synthesis the\nstate of next step.\nThe policy layer is a fully connect layer deﬁned as:\nht(x) = sigmoid(W T\nh ct + v)\n(6)\nwhere ct is the output extracted from RNN layer in step t,\nWh denotes the weights in the policy layer, and v is the bias\nparameter. Through the policy layer, the output of RNN at\nstep t is mapped into [0, 1]. We apply a threshold function to\nobtain the ﬁnal binary codes from the policy probability:\nbk(x) = g(h(x)) = sgn(hk(x) −0.5),\nk = 1, · · · , q\n(7)\nD. Agent training strategy\nFinally we introduce the agent training strategy. Firstly we\nunroll the policy network to better show the details, as shown\nin Figure 2, we use two reward functions to drive the training\nof policy network which are deﬁned in equation (4).\nWe use the Monte-Carlo Policy Gradient [51] to update the\nparameters to maximize the expected total reward of the action\ngroup:\nLg(θ) = EA∼π(A|s;θ)(\nX\ni\nRg\ni )\n≈\nX\ni\nlog[π(Ai|si; θ)]Rg\ni\n=\nX\ni\nX\nk\nlog[P(ai,k|ˆsi,k; θ)]Rg\ni\n(8)\nwhere Lg(θ) is the expected total reward of the action group.\nThe global action reward mainly focuses on the quality of\nthe whole set of hash codes, we adopt gradient decent method\nto optimize the global action reward. According to equation\n(3), we can compute the sub-gradient for each triplet tuple\n(xi, x+\ni , x−\ni ), with respect to h(xi), h(x+\ni ) and h(x−\ni ) as:\n∂RG\ni\n∂h(xi) = 2(h(x−\ni ) −h(x+\ni )) × Ic\n∂RG\ni\n∂h(x+\ni ) = 2(h(x+\ni ) −h(xi)) × Ic\n∂RG\ni\n∂h(x−\ni ) = 2(h(xi) −h(x−\ni )) × Ic\nIc = Imt+∥h(xi)−h(x+\ni )∥2−∥h(xi)−h(x−\ni )∥2>0\n(9)\nwhere Ic is an indicator function, Ic = 1 if c is true, otherwise\nIc = 0. Thus the global reward can ensure the generated codes\nto preserve the semantic ranking information.\nAt last, we explain how the policy network has the ability to\ncorrect the history ranking errors through the details of RNN\n6\ngradient. The gradient of RNN network is formulated as:\nδt\nh = θ′(ct)(δt\nkwh + δt+1\nh\nwhi)\nδt\nh\n△= ∂JL\n∂ct\nδt\nk\n△= ∂JL\n∂ht\n(10)\nwhere ct and ht are the hidden state and output of RNN\nrespectively, θ(ct) denotes the function which maps the ct to\nht. The gradient of hidden layer in the step t consists of two\nparts: the gradient from the output in t step , and the gradient\nfrom the hidden layer in t + 1 step. The latter is considered\nas the sequential reward which can correct the errors caused\nby previous generated hash codes. In the training stage, the\nprevious hash functions will receive the gradient information\nfrom current hash function and update the parameters to\nimprove the retrieval accuracy.\nIV. EXPERIMENTS\nIn this section, we will introduce the experiments conducted\non 3 widely-used datasets compared with 8 state-of-the-art\nmethods, including unsupervised methods LSH [2], SH [5],\nITQ [33] and supervised methods SDH [38], CNNH [17],\nNINH [3], DSH [25] and HashNet [26]. LSH, SH, SDH, and\nITQ are traditional hashing methods without deep networks,\nwhile CNNH, NINH, DSH, HashNet and our proposed DRLIH\nare deep hashing methods, which take the raw image pixels\nas input to conduct hashing function learning.\nA. Datasets\nWe conduct experiments on 3 widely-used image retrieval\ndatasets:\n• The CIFAR10 [52] dataset contains 60000 color images\nfrom 10 classes, the size of each image is 32 × 32.\nFollowing [3], [17], we randomly select 1000 images as\nquery set. For the compared unsupervised methods, all\nthe rest images are used as the training set, while for\nthe compared supervised methods, we further randomly\nselect 5000 images to form the training set.\n• The NUS-WIDE [53] dataset contains nearly 270000\nimages with associated labels from 81 semantic concepts.\nWe use the 21 most frequent concepts to conduct exper-\niment following [3]. We randomly select 2100 images as\nthe query set, 100 images per concept. All the rest images\nare used as training set for unsupervised methods, while\nwe further randomly select 10500 images to form training\nset for supervised methods.\n• The MIRFlickr [54] dataset consists of 25000 images\nobtained from Flickr website as well as associated tags.\nThese images are annotated with one or multiple labels\nof 38 semantic concepts. Similarly, we randomly choose\n1000 images as the query set, and use the rest images\nas the training set of unsupervised methods, and further\nselect 5000 images as training set of supervised methods.\nB. Experiment Settings and Evaluation Metrics\nWe implement the proposed approach based on the open-\nsource framework pytorch1. The parameters of the ﬁrst 18\nlayers in our network are initialized with the VGG-19 net-\nwork [50], which is pre-trained on the ImageNet dataset [55].\nSimilar initialization strategy has been used in other deep\nhashing methods [18], [22], [25]. The dimension of RNN’s\nhidden layer is set to be 4096 in the policy network. In all\nexperiments, our networks are trained with the initial learning\nrate of 0.001, we decrease the learning rate by a factor of 10\nevery 10000 steps. And the mini-batch size is 16, the weight\ndecay parameter is 0.0005. For the parameter in our proposed\nloss function, we set mt = 1 in all the experiments. For\nthe length of action group, we set it as 12 through out the\nexperiments.\nWe compare the proposed DRLIH approach with eight state-\nof-the-art methods, including unsupervised methods LSH, SH\nand ITQ, supervised methods SDH, CNNH, NINH, DSH and\nHashNet. The brief introductions of these methods are as\nfollows:\n• LSH [2] is a data independent unsupervised method,\nwhich uses randomly generated hash functions to map\nimage features into binary codes.\n• SH [5] is a data dependent unsupervised method, which\nlearns hash functions by making hash codes balanced and\nuncorrelated.\n• ITQ [33] is also a data dependent unsupervised method,\nwhich learns hash functions by minimizing the quanti-\nzation error of mapping data to the vertices of a binary\nhypercube.\n• SDH [38] is a supervised method, which leverages label\ninformation to obtain hash codes by integrating hash code\ngeneration and classiﬁer training.\n• CNNH [17] is a two-stage deep hashing method, which\nlearns hash codes for training images in ﬁrst stage, and\ntrains a deep hashing network in second stage.\n• NINH [3] is a one-stage deep hashing method, which\nlearns deep hashing network by a triplet loss function to\nmeasure the ranking information provided by labels.\n• DSH [25] is a one-stage deep hashing method, which de-\nsigns a loss function to maximize the discriminability of\nthe output space by encoding the supervised information\nfrom the input image pairs, and simultaneously imposing\nregularization on the real-valued outputs to approximate\nthe desired discrete values.\n• HashNet [26] directly learns the binary hash codes and\naddresses the ill-posed gradient and data imbalance prob-\nlems in an end-to-end framework of deep feature learning\nand binary hash encoding.\nFor our proposed DRLIH, and compared CNNH, NINH,\nDSH and HashNet methods, we use the raw image pixels as\ninput. The implementations of CNNH, DSH and HashNet are\nprovided by their authors, while NINH is our own implementa-\ntion. Since the representation learning layers of CNNH, NINH,\nDSH and HashNet are different from each other, for a fair\ncomparison, we use the same VGG-19 network as the base\n1http://pytorch.org\n7\nTABLE I: MAP scores with different length of hash codes on CIFAR10, NUS-WIDE and MIRFlickr datasets. MAP scores\nare calculated based on top 5000 returned images for NUS-WIDE dataset. The best results of each code length are shown in\nboldface.\nMethods\nCIFAR10\nNUS-WIDE\nMIRFlickr\n12bit\n24bit\n32bit\n48bit\n12bit\n24bit\n32bit\n48bit\n12bit\n24bit\n32bit\n48bit\nDRLIH (ours)\n0.816\n0.843\n0.855\n0.853\n0.823\n0.846\n0.845\n0.853\n0.796\n0.811\n0.810\n0.814\nHashNet∗\n0.765\n0.823\n0.840\n0.843\n0.812\n0.833\n0.830\n0.840\n0.777\n0.782\n0.785\n0.785\nDSH∗\n0.708\n0.712\n0.751\n0.720\n0.793\n0.804\n0.815\n0.800\n0.651\n0.681\n0.684\n0.686\nNINH∗\n0.792\n0.818\n0.832\n0.830\n0.808\n0.827\n0.827\n0.827\n0.772\n0.756\n0.760\n0.778\nCNNH∗\n0.683\n0.692\n0.667\n0.623\n0.768\n0.784\n0.790\n0.740\n0.763\n0.757\n0.758\n0.755\nSDH-VGG19\n0.430\n0.652\n0.653\n0.665\n0.730\n0.797\n0.819\n0.830\n0.732\n0.739\n0.737\n0.747\nITQ-VGG19\n0.339\n0.361\n0.368\n0.375\n0.777\n0.800\n0.806\n0.817\n0.686\n0.685\n0.687\n0.689\nSH-VGG19\n0.244\n0.213\n0.213\n0.209\n0.712\n0.697\n0.689\n0.682\n0.618\n0.604\n0.598\n0.595\nLSH-VGG19\n0.133\n0.171\n0.178\n0.198\n0.518\n0.567\n0.618\n0.651\n0.575\n0.584\n0.604\n0.614\nSDH\n0.255\n0.330\n0.344\n0.360\n0.460\n0.510\n0.519\n0.525\n0.595\n0.601\n0.608\n0.605\nITQ\n0.158\n0.163\n0.168\n0.169\n0.472\n0.478\n0.483\n0.476\n0.576\n0.579\n0.579\n0.580\nSH\n0.124\n0.125\n0.125\n0.126\n0.452\n0.445\n0.443\n0.437\n0.561\n0.562\n0.563\n0.562\nLSH\n0.116\n0.121\n0.124\n0.131\n0.436\n0.414\n0.432\n0.442\n0.557\n0.564\n0.562\n0.569\n10\n20\n30\n40\n50\nNumber of bits\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ hamm.dist.<=2\nCIFAR-10\n200\n400\n600\n800\n1000\nNumber of retrieved samples\n(b)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ 48 bits\nCIFAR-10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall @ 48 bits\n(c)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ 48 bits\nCIFAR-10\nDRLIH(ours)\nHashNet*\nDSH*\nNINH*\nCNNH*\nSDH-VGG19\nITQ-VGG19\nSH-VGG19\nLSH-VGG19\nSDH\nITQ\nSH\nLSH\nFig. 3: The comparison results on CIFAR10. (a) Precision within Hamming radius 2 using hash lookup; (b) Precision at top\nk returned results. (c) Precision-Recall curves of Hamming Ranking with 48bit.\nstructure for all the deep hashing methods. And the network\nparameters of all the deep hashing methods are initialized with\nthe same pre-trained VGG-19 model, thus we can perform a\nfair comparison between them. The results of CNNH, NINH,\nDSH and HashNet are referred as CNNH∗, DSH∗, NINH∗\nand HashNet∗respectively.\nFor other compared traditional methods without deep net-\nworks, we represent each image by hand-crafted features\nand deep features respectively. For hand-crafted features, we\nrepresent images in the CIFAR10 and MIRFlickr by 512-\ndimensional GIST features, and images in the NUS-WIDE by\n500-dimensional bag-of-words features. For a fair comparison\nbetween traditional methods and deep hashing methods, we\nalso conduct experiments on the traditional methods with deep\nfeatures, where we extract 4096-dimensional deep feature for\neach image from the same pre-trained VGG-19 network. We\ndenote the results of traditional methods using deep features\nby LSH-VGG19, SH-VGG19, ITQ-VGG19 and SDH-VGG19.\nThe results of SDH, SH and ITQ are obtained from the\nimplementations provided by their authors, while the results\nof LSH are from our own implementation.\nTo objectively and comprehensively evaluate the retrieval\naccuracy of the proposed approach and the compared methods,\nwe use four evaluation metrics: Mean Average Precision\n(MAP), precision at top k returned results, precision-recall\ncurves and precision within Hamming radius 2 using hash\nlookup. These four evaluation metrics are deﬁned as follows:\n• The MAP scores are computed as the mean of average\nprecision (AP) for all queries, and AP is computed as:\nAP = 1\nR\nn\nX\nk=1\nk\nRk\n× relk\n(11)\nwhere n is the size of database, R is the number of\nrelevant images in database, Rk is the number of relevant\nimages in the top k returns, and relk = 1 if the image\nranked at k-th position is relevant and 0 otherwise.\n• Precision at top k returned results (topK-precision): The\nprecision with respect to different numbers of retrieved\nsamples from the ranking list.\n• Precision within Hamming radius 2: Precision curve of\nreturned images with the Hamming distance smaller than\n2 using hash lookup.\nC. Experiment Results\n1) Experiment results on CIFAR10 dataset: Table I shows\nthe MAP scores with different length of hash codes on\nCIFAR10 dataset. Overall, the proposed DRLIH achieves the\n8\n10\n20\n30\n40\n50\nNumber of bits\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ hamm.dist.<=2\nNUS-WIDE\n200\n400\n600\n800\n1000\nNumber of retrieved samples\n(b)\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nPrecision @ 48 bits\nNUS-WIDE\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall @ 48 bits\n(c)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ 48 bits\nNUS-WIDE\nDRLIH(ours)\nHashNet*\nDSH*\nNINH*\nCNNH*\nSDH-VGG19\nITQ-VGG19\nSH-VGG19\nLSH-VGG19\nSDH\nITQ\nSH\nLSH\nFig. 4: The comparison results on NUS-WIDE. (a) Precision within Hamming radius 2 using hash lookup; (b) Precision at\ntop k returned results. (c) Precision-Recall curves of Hamming Ranking with 48bit.\n10\n20\n30\n40\n50\nNumber of bits\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ hamm.dist.<=2\nMIRFLICKR\n200\n400\n600\n800\n1000\nNumber of retrieved samples\n(b)\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nPrecision @ 48 bits\nMIRFLICKR\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall @ 48 bits\n(c)\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\nPrecision @ 48 bits\nMIRFLICKR\nDRLIH(ours)\nHashNet*\nDSH*\nNINH*\nCNNH*\nSDH-VGG19\nITQ-VGG19\nSH-VGG19\nLSH-VGG19\nSDH\nITQ\nSH\nLSH\nFig. 5: The comparison results on MIRFlickr. (a) Precision within Hamming radius 2 using hash lookup; (b) Precision at top\nk returned results. (c) Precision-Recall curves of Hamming Ranking with 48bit.\nhighest average MAP of 0.842, and consistently outperforms\nstate-of-the-art methods on all hash code lengths. More specif-\nically, the result tables are partitioned into three groups: deep\nhashing methods, traditional methods with deep features and\ntraditional methods with hand-crafted features. Compare with\nthe highest deep hashing methods HashNet∗, which achieves\naverage MAP of 0.818, the proposed DRLIH has an absolute\nimprovement of 0.024. Compare with the highest traditional\nmethods using deep features SDH-VGG19, which achieves an\naverage MAP of 0.600, the proposed method has an absolute\nimprovement of 0.242. While the highest traditional methods\nusing handcrafted features SDH achieves average MAP of\n0.322, the proposed approach has an absolute improvement\nof 0.520.\nFigure 3(a) shows the precisions within Hamming radius\n2 using hash lookup. The precision of proposed DRLIH\nconsistently outperforms state-of-the-art methods on all hash\ncode lengths. The precision of most traditional methods de-\ncrease when using longer hash codes. This is because the\nnumber of images sharing the same Hamming code decreases\nexponentially for longer hash codes (e.g. 48bit), which will\ncause some queries fail to return images within Hamming\nradius 2. While the proposed DRLIH achieves the highest\nprecision on 48bit code length, which shows the robustness of\nproposed method on longer hash codes. Figure 3(b) shows the\nprecision at top k returned results, we can also observe that\nour proposed DRLIH achieves the best precision compared\nwith state-of-the-art methods. Figure 3(c) demonstrates the\nprecision-recall curves using Hamming ranking with 48bit\ncodes. DRLIH still achieves the best accuracy on all recall\nlevels, which further demonstrates the effectiveness of the\nproposed approach.\n2) Experiment results on NUS-WIDE dataset:\nTable I\nshows the MAP scores with different length of hash codes on\nNUS-WIDE dataset. Following [17], we calculate the MAP\nscores based on top 5000 returned images. Similar results\ncan be observed, our proposed DRLIH still achieves the best\nMAP scores (average 0.842). DRLIH achieves an absolute\nimprovement of 0.013 on average MAP compared to the\nhighest deep methods HashNet∗(average 0.829). Compare\nwith the highest traditional method using deep features ITQ-\nVGG19, which achieves an average MAP of 0.800, DRLIH\nhas an absolute improvement of 0.042. It is also interesting\nto observe that with the deep features extracted from VGG-\n19 network, the traditional methods such as SDH and ITQ\nachieve comparable results with deep hashing methods.\n9\ntop10 returned samples\nquery\nquery\ntop10 returned samples\nDRLIH\nHashNet*\nNINH*\nDRLIH\nHashNet*\nNINH*\nFig. 6: Some retrieval results of NUS-WIDE dataset using Hamming ranking on 48bit hash codes. The blue rectangles denote\nthe query images. The red rectangles indicate wrong retrieval results. We can observe that DRLIH achieves the best results.\n10\n20\n30\n40\n50\nNumber of bits\n(a)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ hamm.dist.<=2\nCIFAR-10\n200\n400\n600\n800\n1000\nNumber of retrieved samples\n(b)\n0.82\n0.83\n0.84\n0.85\n0.86\n0.87\n0.88\n0.89\nPrecision @ 48 bits\nCIFAR-10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall @ 48 bits\n(c)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ 48 bits\nCIFAR-10\nDRLIH(Ours)\nBaseline_noseq\nBaseline_ng\nFig. 7: The comparison results of baseline methods on CIFAR10. (a) Precision within Hamming radius 2 using hash lookup;\n(b) Precision at top k returned results; (c) Precision-Recall curves of Hamming Ranking with 48bit.\nFigures 4(a), (b) and (c) demonstrate the precision within\nHamming radius 2 using hash lookup, precision at top k re-\nturned results and the precision-recall curves using Hamming\nranking with 48 bits. Similar trends can be observed on these\nthree evaluation metrics, the proposed DRLIH also achieves\npromising results on NUS-WIDE dataset on all hash code\nlengths, which further shows the effectiveness of our proposed\nDRLIH.\n3) Experiment results on MIRFLICKR dataset: Table I\nshows the MAP scores with different length of hash codes\non MIRFlickr dataset. We can also observe that our proposed\nDRLIH still achieves the best MAP scores (average 0.808).\nDRLIH achieves an absolute improvement of 0.026 on aver-\nage MAP compared to the highest deep methods HashNet∗\n(average 0.782). Compare with the highest traditional method\nusing deep features SDH-VGG19, which achieves an average\nMAP of 0.739, DRLIH has an absolute improvement of 0.069.\nCompare with the highest traditional method using hand-\ncrafted features SDH, which achieves an average MAP of\n0.602, DRLIH has an absolute improvement of 0.206.\nWe can observe from Figure 4(a) that the proposed DRLIH\nachieves the best precision within Hamming radius 2 using\nhash lookup. Figure 4(b) demonstrates the precision at top\nk returned results, our proposed DRLIH achieves the best\nprecision compared with state-of-the-art methods. Figure 4(c)\nshows the precision-recall curves using Hamming ranking\nwith 48 bits, and similar trends can be observed that the\nproposed DRLIH also achieves best precision on all recall\nlevels compared with state-of-the-art methods.\nFinally, we demonstrate the top 10 retrieval results of NUS-\n10\n10\n20\n30\n40\n50\nNumber of bits\n(a)\n0.72\n0.74\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\nPrecision @ hamm.dist.<=2\nNUS-WIDE\n200\n400\n600\n800\n1000\nNumber of retrieved samples\n(b)\n0.82\n0.83\n0.84\n0.85\n0.86\n0.87\n0.88\nPrecision @ 48 bits\nNUS-WIDE\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall @ 48 bits\n(c)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nPrecision @ 48 bits\nNUS-WIDE\nDRLIH(Ours)\nBaseline_noseq\nBaseline_ng\nFig. 8: The comparison results of baseline methods on NUS-WIDE. (a) Precision within Hamming radius 2 using hash lookup;\n(b) Precision at top k returned results; (c) Precision-Recall curves of Hamming Ranking with 48bit.\n10\n20\n30\n40\n50\nNumber of bits\n(a)\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nPrecision @ hamm.dist.<=2\nMIRFLICKR\n200\n400\n600\n800\n1000\nNumber of retrieved samples\n(b)\n0.84\n0.85\n0.86\n0.87\n0.88\n0.89\n0.9\nPrecision @ 48 bits\nMIRFLICKR\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall @ 48 bits\n(c)\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\nPrecision @ 48 bits\nMIRFLICKR\nDRLIH(Ours)\nBaseline_noseq\nBaseline_ng\nFig. 9: The comparison results of baseline methods on MIRFlickr. (a) Precision within Hamming radius 2 using hash lookup;\n(b) Precision at top k returned results; (c) Precision-Recall curves of Hamming Ranking with 48bit.\nTABLE II: Comparison between baseline methods and our proposed approach with different length of hash codes on CIFAR10,\nNUS-WIDE and MIRFlickr datasets.\nMethods\nCIFAR10\nNUS-WIDE\nMirFlickr\n12bit\n24bit\n32bit\n48bit\n12bit\n24bit\n32bit\n48bit\n12bit\n24bit\n32bit\n48bit\nDRLIH (ours)\n0.816\n0.843\n0.855\n0.853\n0.823\n0.846\n0.845\n0.853\n0.796\n0.811\n0.810\n0.814\nBaseline noseq\n0.759\n0.802\n0.811\n0.822\n0.803\n0.811\n0.820\n0.824\n0.764\n0.775\n0.776\n0.784\nBaseline ng\n0.805\n0.823\n0.820\n0.826\n0.811\n0.824\n0.829\n0.835\n0.774\n0.778\n0.784\n0.789\nWIDE using Hamming ranking on 48bit hash codes. As shown\nin Figure 6, our proposed DRLIH achieves the best results.\n4) Baseline experiments: To verify the effectiveness of\naction group and sequence learning strategy of deep rein-\nforcement learning, we also conduct two baseline experiments.\nMore speciﬁcally, we ﬁrst set the length of the action group\nas 1, which implies that we train the network without action\ngroup, we denote this method as Baseline ng. We also conduct\na baseline experiment without the sequence learning strategy,\nwhere we replace the agent with a fully-connected layer whose\ndimension is the same as the hash code length and train\nthe network by the triplet ranking loss. This layer serves as\nhashing layer that maps features of representation network\ninto binary codes directly and independently. We denote this\nmethod as Baseline noseq. Comparing the proposed DRLIH\napproach with Baseline ng, we can verify the effectiveness\nof action group. Comparing our DRLIH approach with Base-\nline noseq, we can verity the effectiveness of the sequence\nlearning strategy. The results are shown in table II, and we\ncan observe that compare with Baseline ng, our proposed\nDRLIH approach improves the average MAP score from\n0.819 to 0.842 on CIFAR10 dataset, from 0.825 to 0.842 on\nNUS-WIDE dataset and from 0.781 to 0.808 on MIRFlickr\ndataset. This demonstrates that the proposed action group\nmethod can promote the retrieval accuracy. Comparing with\nBaseline noseq, our proposed DRLIH approach improves the\naverage MAP score from 0.799 to 0.842 on CIFAR10 dataset,\nfrom 0.815 to 0.842 on NUS-WIDE dataset and from 0.775\nto 0.808 on MIRFlickr dataset. This demonstrates that we\ncan beneﬁt from sequence learning of deep reinforcement\nlearning framework to promote retrieval accuracy. We can\nalso observer that Baseline ng has better performance than\n11\nthe Baseline noseq, which further shows the effectiveness of\nthe sequential learning strategy. Figures 7, 8 and 9 show the\nprecision within hamming radius 2, precision at top k returned\nresults and precision recall curves on three datasets, we can\nobserve that the proposed DRLIH approach outperforms two\nbaseline methods on these three evaluation metrics.\nV. CONCLUSION\nIn this paper, we have proposed a Deep Reinforcement\nLearning approach for Image Hashing (DRLIH). First, we\npropose a policy based deep reinforcement learning network\nfor modeling hashing functions. We utilize recurrent neural\nnetwork (RNN) to model hashing functions as agents, which\ntake actions of projecting images into binary codes sequen-\ntially. While we regard hash codes and the image features\nas states, which provide history actions taken by agents.\nThe whole network is trained by optimizing two hierarchical\nreward functions. Second, we propose a sequential learning\nstrategy based on proposed DRLIH, which can iteratively\noptimize the overall accuracy by correcting the error generated\nby history actions. Experiments on three widely used datasets\ndemonstrate the effectiveness of our proposed approach.\nThe future work lies in two aspects: First, we will try to\ndeﬁne the sequential learning process explicitly, such that we\ncan better model the sequential learning. Second, we intend to\nexploit more advanced deep reinforcement learning framework\nto achieve better retrieval accuracy.\nREFERENCES\n[1] J. Wang, T. Zhang, N. Sebe, H. T. Shen et al., “A survey on learning to\nhash,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 40, no. 4, pp. 769–790, 2018.\n[2] A. Gionis, P. Indyk, R. Motwani et al., “Similarity search in high\ndimensions via hashing,” in International Conference on Very Large\nData Bases (VLDB), vol. 99, no. 6, 1999, pp. 518–529.\n[3] H. Lai, Y. Pan, Y. Liu, and S. Yan, “Simultaneous feature learning\nand hash coding with deep neural networks,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2015, pp. 3270–\n3278.\n[4] J. Wang, S. Kumar, and S.-F. Chang, “Sequential projection learning for\nhashing with compact codes,” in International Conference on Machine\nLearning (ICML), 2010, pp. 1127–1134.\n[5] Y. Weiss, A. Torralba, and R. Fergus, “Spectral hashing,” in Annual\nConference on Neural Information Processing Systems (NIPS), 2009,\npp. 1753–1760.\n[6] S. Zhang, J. Li, M. Jiang, and B. Zhang, “Scalable discrete supervised\nmultimedia hash learning with clustering,” IEEE Transactions on Cir-\ncuits and Systems for Video Technology (TCSVT), vol. PP, no. 99, pp.\n1–1, 2017.\n[7] Z. Chen, J. Lu, J. Feng, and J. Zhou, “Nonlinear discrete hashing,” IEEE\nTransactions on Multimedia (TMM), vol. 19, no. 1, pp. 123–135, 2017.\n[8] P. Li, M. Wang, J. Cheng, C. Xu, and H. Lu, “Spectral hashing with\nsemantically consistent graph for image indexing,” IEEE Transactions\non Multimedia (TMM), vol. 15, no. 1, pp. 141–152, 2013.\n[9] Y. Zhang, L. Zhang, and Q. Tian, “A prior-free weighting scheme for\nbinary code ranking,” IEEE Transactions on Multimedia (TMM), vol. 16,\nno. 4, pp. 1127–1139, 2014.\n[10] M. Kafai, K. Eshghi, and B. Bhanu, “Discrete cosine transform locality-\nsensitive hashes for face retrieval,” IEEE Transactions on Multimedia\n(TMM), vol. 16, no. 4, pp. 1090–1103, 2014.\n[11] K. Ding, B. Fan, C. Huo, S. Xiang, and C. Pan, “Cross-modal hashing\nvia rank-order preserving,” IEEE Transactions on Multimedia (TMM),\nvol. 19, no. 3, pp. 571–585, 2017.\n[12] V. E. Liong, J. Lu, Y.-P. Tan, and J. Zhou, “Deep video hashing,” IEEE\nTransactions on Multimedia (TMM), 2016.\n[13] Y. Hao, T. Mu, R. Hong, M. Wang, N. An, and J. Y. Goulermas,\n“Stochastic multiview hashing for large-scale near-duplicate video re-\ntrieval,” IEEE Transactions on Multimedia (TMM), vol. 19, no. 1, pp.\n1–14, 2017.\n[14] A. Oliva and A. Torralba, “Modeling the shape of the scene: A\nholistic representation of the spatial envelope,” International Journal\nof Computer Vision (IJCV), vol. 42, no. 3, pp. 145–175, 2001.\n[15] L. Fei-Fei and P. Perona, “A bayesian hierarchical model for learning\nnatural scene categories,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2005, pp. 524–531.\n[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Annual Conference on\nNeural Information Processing Systems (NIPS), 2012, pp. 1097–1105.\n[17] R. Xia, Y. Pan, H. Lai, C. Liu, and S. Yan, “Supervised hashing for\nimage retrieval via image representation learning,” in AAAI Conference\non Artiﬁcial Intelligence (AAAI), 2014, pp. 2156–2162.\n[18] T. Yao, F. Long, T. Mei, and Y. Rui, “Deep semantic-preserving\nand ranking-based hashing for image retrieval,” in International Joint\nConference on Artiﬁcial Intelligence (IJCAI), 2016, pp. 3931–3937.\n[19] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang, “Bit-scalable\ndeep hashing with regularized similarity learning for image retrieval\nand person re-identiﬁcation,” IEEE Transactions on Image Processing\n(TIP), vol. 24, no. 12, pp. 4766–4779, 2015.\n[20] F. Zhao, Y. Huang, L. Wang, and T. Tan, “Deep semantic ranking\nbased hashing for multi-label image retrieval,” in IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2015, pp. 1556–\n1564.\n[21] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep super-\nvised hashing with pairwise labels,” in International Joint Conference\non Artiﬁcial Intelligence (IJCAI), 2016, pp. 1711–1717.\n[22] H. Zhu, M. Long, J. Wang, and Y. Cao, “Deep hashing network for efﬁ-\ncient similarity retrieval,” in AAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2016, pp. 2415–2421.\n[23] J. Zhang and Y. Peng, “Ssdh: semi-supervised deep hashing for large\nscale image retrieval,” IEEE Transactions on Circuits and Systems for\nVideo Technology, 2017.\n[24] ——, “Query-adaptive image retrieval by deep weighted hashing,” IEEE\nTransactions on Multimedia, 2018.\n[25] H. Liu, R. Wang, S. Shan, and X. Chen, “Deep supervised hashing\nfor fast image retrieval,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2016, pp. 2064–2072.\n[26] Z. Cao, M. Long, J. Wang, and P. S. Yu, “Hashnet: Deep learning to hash\nby continuation,” in The IEEE International Conference on Computer\nVision (ICCV), Oct 2017.\n[27] J. Wang, S. Kumar, and S.-F. Chang, “Semi-supervised hashing for\nlarge-scale search,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 34, no. 12, pp. 2393–2406, 2012.\n[28] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, and G. Ostrovski,\n“Human-level control through deep reinforcement learning,” Nature, vol.\n518, no. 7540, p. 529, 2015.\n[29] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, and\nM. Lanctot, “Mastering the game of go with deep neural networks and\ntree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.\n[30] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and K. Li, “Multi-\nprobe lsh: efﬁcient indexing for high-dimensional similarity search,” in\nInternational conference on Very large data bases (VLDB), 2007, pp.\n950–961.\n[31] M. Raginsky and S. Lazebnik, “Locality-sensitive binary codes from\nshift-invariant kernels,” in Annual Conference on Neural Information\nProcessing Systems (NIPS), 2009, pp. 1509–1517.\n[32] W. Liu, J. Wang, S. Kumar, and S.-F. Chang, “Hashing with graphs,” in\nInternational Conference on Machine Learning (ICML), 2011, pp. 1–8.\n[33] Y. Gong and S. Lazebnik, “Iterative quantization: A procrustean ap-\nproach to learning binary codes,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2011, pp. 817–824.\n[34] L. Zhang, Y. Zhang, J. Tang, X. Gu, J. Li, and Q. Tian, “Topology pre-\nserving hashing for similarity search,” in ACM International Conference\non Multimedia (ACM-MM), 2013, pp. 123–132.\n[35] G. Irie, Z. Li, X.-M. Wu, and S.-F. Chang, “Locally linear hashing\nfor extracting non-linear manifolds,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2014, pp. 2115–2122.\n[36] B. Kulis and T. Darrell, “Learning to hash with binary reconstructive\nembeddings,” in Annual Conference on Neural Information Processing\nSystems (NIPS), 2009, pp. 1042–1050.\n12\n[37] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang, “Supervised\nhashing with kernels,” in IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2012, pp. 2074–2081.\n[38] F. Shen, C. Shen, W. Liu, and H. Tao Shen, “Supervised discrete hash-\ning,” in IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2015, pp. 37–45.\n[39] J. Wang, J. Wang, N. Yu, and S. Li, “Order preserving hashing for\napproximate nearest neighbor search,” in ACM International Conference\non Multimedia (ACM-MM), 2013, pp. 133–142.\n[40] Q. Wang, Z. Zhang, and L. Si, “Ranking preserving hashing for\nfast similarity search,” in International Joint Conference on Artiﬁcial\nIntelligence (IJCAI), 2015, pp. 3911–3917.\n[41] M. Lin, Q. Chen, and S. Yan, “Network in network,” arXiv preprint\narXiv:1312.4400, 2013.\n[42] M. Schultz and T. Joachims, “Learning a distance metric from relative\ncomparisons,” in Advances in Neural Information Processing Systems\n(NIPS), 2003, pp. 41–48.\n[43] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen,\nand Y. Wu, “Learning ﬁne-grained image similarity with deep ranking,”\nin IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2014, pp. 1386–1393.\n[44] G. Chechik, V. Sharma, U. Shalit, and S. Bengio, “Large scale online\nlearning of image similarity through ranking,” Journal of Machine\nLearning Research (JMLR), vol. 11, no. Mar, pp. 1109–1135, 2010.\n[45] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement\nlearning: A survey,” Journal of artiﬁcial intelligence research, vol. 4,\npp. 237–285, 1996.\n[46] J. C. Caicedo and S. Lazebnik, “Active object localization with deep\nreinforcement learning,” in Computer Vision (ICCV), 2015 IEEE Inter-\nnational Conference on.\nIEEE, 2015, pp. 2488–2496.\n[47] Z. Ren, X. Wang, N. Zhang, X. Lv, and L.-J. Li, “Deep reinforcement\nlearning-based image captioning with embedding reward,” in Computer\nVision and Pattern Recognition (CVPR), 2017 IEEE Conference on.\nIEEE, 2017, pp. 1151–1159.\n[48] D. Zhao, Y. Chen, and L. Lv, “Deep reinforcement learning with visual\nattention for vehicle classiﬁcation,” IEEE Transactions on Cognitive and\nDevelopmental Systems, 2016.\n[49] Y. Rao, J. Lu, and J. Zhou, “Attention-aware deep reinforcement learning\nfor video face recognition,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017, pp. 3931–3940.\n[50] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in International Conference on Learning\nRepresentations (ICLR), 2014.\n[51] R. J. Williams, Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning.\nSpringer, 1992, pp. 5–32.\n[52] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from\ntiny images,” 2009.\n[53] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng, “Nus-wide:\na real-world web image database from national university of singapore,”\nin ACM international conference on image and video retrieval (CIVR),\n2014, p. 48.\n[54] M. J. Huiskes and M. S. Lew, “The mir ﬂickr retrieval evaluation,”\nin ACM International Conference on Multimedia Information Retrieval\n(MIR), 2008, pp. 39–43.\n[55] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large\nscale visual recognition challenge,” International Journal of Computer\nVision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-02-07",
  "updated": "2019-02-24"
}