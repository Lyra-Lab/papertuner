{
  "id": "http://arxiv.org/abs/1802.03436v2",
  "title": "The language (and series) of Hammersley-type processes",
  "authors": [
    "Cosmin Bonchis",
    "Gabriel Istrate",
    "Vlad Rochian"
  ],
  "abstract": "We study languages and formal power series associated to (variants of)\nHammersley's process. We show that the ordinary Hammersley process yields a\nregular language and the Hammersley tree process yields deterministic\ncontext-free (but non-regular) languages. For the extension to intervals of the\nHammersley process we show that there are two relevant formal languages. One of\nthem leads to the same class of languages as the ordinary Hammersley tree\nprocess. The other one yields non-context-free languages. The results are\nmotivated by the problem of studying the analog of the famous Ulam-Hammersley\nproblem for heapable sequences. Towards this goal we also give an algorithm for\ncomputing formal power series associated to the variants of Hammersley's\nprocess. We employ these algorithms to settle the nature of the scaling\nconstant, conjectured in previous work to be the golden ratio. Our results\nprovide experimental support to this conjecture.",
  "text": "The language (and series) of Hammersley-type\nprocesses⋆\nCosmin Bonchis¸1,2, Gabriel Istrate1,2, and Vlad Rochian1\n1 Department of Computer Science, West University of Timis¸oara, Bd. V. Pˆarvan 4,\nTimis¸oara, Romania. Corresponding author’s email: gabrielistrate@acm.org\n2 e-Austria Research Institute, Bd. V. Pˆarvan 4, cam. 045 B, Timis¸oara, Romania.\nAbstract. We study languages and formal power series associated to (variants\nof) the Hammersley process. We show that the ordinary Hammersley process\nyields a regular language and the Hammersley tree process yields deterministic\ncontext-free (but non-regular) languages. For the Hammersley interval process\nwe show that there are two relevant variants of formal languages. One of them\nleads to the same language as the ordinary Hammersley tree process. The other\none yields non-context-free languages.\nThe results are motivated by the problem of studying the analog of the famous\nUlam-Hammersley problem for heapable sequences. Towards this goal we also\ngive an algorithm for computing formal power series associated to the Hammersley\nprocess. We employ this algorithm to settle the nature of the scaling constant, con-\njectured in previous work to be the golden ratio. Our results provide experimental\nsupport to this conjecture.\n1\nIntroduction\nThe Physics of Complex Systems and Theoretical Computing have a long and fruitful\nhistory of cooperation: for instance the celebrated Ising Model can be studied com-\nbinatorially, as some of its versions naturally relate to graph-theoretic concepts [20].\nMethods from formal language theory have been employed (even in papers published by\nphysicists, in physics venues) to the analysis of dynamical systems [13,21]. Sometimes\nthe cross-fertilization goes in the opposite direction: concepts from the theory of inter-\nacting particle systems [12] (e.g. the voter model) have been useful in the analysis of\ngossiping protocols. A relative of the famous TASEP process, the so-called Hammersley-\nAldous-Diaconis (HAD) process, has provided [1] the most illuminating solution to the\nfamous Ulam-Hammersley problem [17] concerning the scaling behavior of the longest\nincreasing subsequence of a random permutation.\nIn this paper we contribute to the literature on investigating physical models with\ndiscrete techniques by bringing methods based on formal language theory (and, possibly,\nnoncommutative formal power series) to the analysis of several variants of the HAD\nprocess: We deﬁne formal languages (and power series) encoding all possible trajectories\nof such processes, and completely determine (the complexity of) these languages.\n⋆This work was supported by a grant of Ministry of Research and Innovation, CNCS - UEFISCDI,\nproject number PN-III-P4-ID-PCE-2016-0842, within PNCDI III.\narXiv:1802.03436v2  [cs.FL]  9 Apr 2018\n2\nG. Istrate, C. Bonchis¸, and V. Rochian\nThe main process we are concerned with was deﬁned in combinatorially in [9], and in\nmore general form in [4], where it was dubbed the Hammersley tree process. It appeared\nnaturally in [9] as a tool to investigate a version of the Ulam-Hammersley problem\nthat employs the concept (due to Byers et al. [7]) of heapable sequence, an interesting\nvariation on the concept of increasing sequence. Informally, a sequence of integers is\nheapable if it can be successively inserted into the leaves of a (not necessarily complete)\nbinary tree satisfying the heap property. The Ulam-Hammesley problem for heapable\nsequences is open, the scaling behavior being the subject of an intriguing conjecture (see\nConjecture 19 below) involving the golden ratio [9]. Methods based on formal power\nseries can conceivably rigorously establish the true value of this constant. We also study\na (second) version of the Hammersley tree process, motivated by the analogue of the\nUlam-Hammersley problem for random intervals [3] (see Conjecture 20 below).\nThe outline of the paper is the following: In section 2 we precisely specify the the\nsystems we are interested in, and outline the results we obtain. In section 3 we discuss the\ncombinatorial and probability-theoretic motivations of the problems we are interested in.\nThis section is not needed to understand the technical details of our proofs. In Section 4\nwe prove our main result: we precisely identify the Hammersley language for every\nk ≥1. The language turns out to be regular for k = 1 and deterministic context-free but\nnon-regular for k ≥2. The result is then extended to (the analog of) the Hammersley\nprocess for intervals. In this case, it turns that there are two natural ways to deﬁne the\nassociated formal language. The ”effective” version yields the same language as in\nthe case of permutations. The ”more useful” one yields (as we show) non-context-free\nlanguages that can be explicitly characterized. We then proceed by presenting (Section 9)\nalgorithms for computing the power series associated to these systems. They are applied\nto the problem of determining true value of scaling constant (believed to be equal to the\ngolden ratio) in the Ulam-Hammersley problem for heapable sequences. In a nutshell,\nthe experimental results tend to conﬁrm the identity of this constant to the golden\nratio; however the convergence is slow, as the estimates based on the formal power series\ncomputations we undertake (based on small values of n) seem quite far from the true\nvalue. The paper concludes (Section 11) with several discussions and open problems.\n2\nMain deﬁnitions and results\nWe are interested in the following variant of the process in [4], totally adequate for the\npurpose of describing the heapability of random permutations, deﬁned as follows:\nDeﬁnition 1. In the process HADk, individuals appear at integer times t ≥1. Each\nindividual can be identiﬁed with a value Xt ∈R, and is initially endowed with k ”lives”.\nThe appearance of a new individual Xt+1 subtracts a life from the smallest individual\nXa > Xt+1 (if any) still alive at moment t.\nWe can describe combinatorially the evolution of process HADk in the following\nmanner: each state of the system at a certain moment n can be encoded by a word\nof length n over the alphabet Σk = {0, 1, . . . , k} obtained by discarding the value\ninformation from particles and only record the number of lives. Thus particles are\narranged in the increasing order of values, from the smallest to the largest\nThe language (and series) of Hammersley-type processes\n3\nExample 2. Consider the state sX of the system HAD2 after all particles with values\nX = [5, 1, 4, 2, 3] have arrived (in this order). Then in the state sX particle 5 has 0 lives,\nparticle 1 has two lives, particle 4 has 0 lives left, particle 2 has two lives, particle 3 has\ntwo lives left. Consequently, the word wX encoding sX is 22200.\nGiven this encoding, the dynamics of process HADk on random permutations can\nbe described in a completely equivalent manner as a process on words: given word\nwk encoding the state of the system at moment k, we choose a random position of\nwk, inserting a k there and subtract one from the ﬁrst nonzero digit to the right of\nthe insertion place, thus obtaining the word wk+1. This ﬁrst nonzero digit need not\nbe directly adjacent to the insertion place, but separated from it by a block of zeros.\nThese zeros will not be affected in the word wk+1. Figure 1 presents the snapshots of all\npossible trajectories of system HAD2 at moments t = 1, 2, 3.\nExample 3. If we run the process HAD2 on sequence X from Example 2, the outcome\nis a multiset of particles 1, 2 and 3, each with multiplicity 2, encoded by the word 22200.\n2\n21\n211\n220\n212\n22\n212\n221\n222\nFig. 1: Words in the Hammersley tree process (k = 2). Insertions are boldfaced. Positions that lost\na life at the current stage are underlined.\nWe are interested in the following formal power series that encodes the large-scale\nevolution of process HADk, and the associated formal language:\nDeﬁnition 4. Given k ≥1, the Hammersley power series of order k is the formal power\nseries Fk ∈N(< Σk >) deﬁned as follows: given word w ∈Σ∗\nk, deﬁne Fk(w) to be\nthe multiplicity of word w in the process HADk.\nThe Hammersley language of order k, Lk\nH, is deﬁned as the support of Fk, i.e. the\nset of words in Σ∗\nk s.t. there exists a trajectory of HADk that yields w.\nExample 5. F2(212) = 2, F2(220) = 1, hence 212, 220 ∈L2\nH. On the other hand\n200 ̸∈L2\nH, since F2(200) = 0.\nDeﬁnition 6. For w ∈Σ∗\nk and a ∈Σk, denote by |w|a the number of copies of a in w.\nGiven k ≥1, word w ∈Σk is called k-dominant if the following inequality holds for\nevery z ∈Pref(w): |z|k −Pk−2\ni=0 (k −i −1) · |z|i > 0. We call the left-hand side term\nthe structural difference of word z.\n4\nG. Istrate, C. Bonchis¸, and V. Rochian\nObservation 1 1-dominant words are precisely those that start with a 1. On the other\nhand, 2-dominant words are those that start with a 2 and have, in any preﬁx, strictly\nmore twos than zeros.\nOur main result completely characterizes the Hammersley language of order k:\nTheorem 7. For every k ≥1, Lk\nH = {w ∈Σ∗\nk | w is k-dominant.}.\nCorollary 8. Language L1\nH is regular. For k ≥2 languages Lk\nH are deterministic\none-counter languages but not regular.\nIn [3] we considered the extension of heapability to partial orders, including intervals.\nWe also noted that, just as in the case of random permutations, heapability of random\nintervals can be analyzed using the following version of the process HADk:\nDeﬁnition 9. The interval Hammersley process with k lives is the stochastic process\ndeﬁned as follows: The process starts with no particles. Particles arrive at integer\nmoments; they have a value in the interval (0, 1), and a number of lives. Given the state\nZn−1 of the process after step n −1, to obtain Zn we choose, independently, uniformly\nat random and with repetitions two random reals Xn, Yn ∈(0, 1). Then we perform the\nfollowing operations:\n- First a new particle with k lives and value min(Xn, Yn) is inserted.\n- Then the smallest (if any) live particle whose value is higher than max(Xn, Yn)\nloses one life, yielding state Zn.\nThe state of the process at a certain moment n comprises a record of all the real\nnumbers chosen along the trajectory:, (X0, Y0, . . . , Xn−1, Yn−1), even those that do\nnot correspond to a particle. Each number is endowed (in case it represented a new\nparticle) with an integer in the range 0 . . . k representing the number of lives the given\nparticle has left at moment n.\nJust as with process HADk, we can combinatorialize the previous deﬁnition as follows:\nDeﬁnition 10. Process HADk,INT is the stochastic process on (Σk ∪{⋄})∗deﬁned\nas follows: The process starts with the two-letter word Z1 = k⋄. Given the string\nrepresentation Zn−1 of the process after step n−1, we choose, independently, uniformly\nat random and with repetitions two positions Xn, Yn into string Zn−1. Xn, Yn may\nhappen to be the same position, in which we also choose randomly an ordering of\nXn, Yn. Then we perform (see Figure 2) the following operations:\n– First, a k is inserted into Zn−1 at position min(Xn, Yn).\n– Then a ⋄is introduced in position max(Xn, Yn) (immediately after the newly\nintroduced k, if Xn = Yn).\n– Then the smallest (if any) nonzero digit occurring after the position of the newly\ninserted ⋄loses one unit. This yields string Zn.\nIt turns out (see the discussion at the end of Section 3) that there are two languages\nmeaningfully associated to the process HADk,INT . The ﬁrst of them has the following\ndeﬁnition:\nThe language (and series) of Hammersley-type processes\n5\nXn\n·\n2\n·\nZn−1 =\n2\n·\n0\n·\n2\n·\n0\n·\nYn\nmin{Xn, Yn}\nmax{Xn, Yn}\n2\n2\n·\nZn =\n2\n·\n0\n⋄\n1\n·\n0\n·\nFig. 2: Insertion in process HAD2,INT . Insertion positions are marked with a dot. Positions\naffected by the insertion are in bold.\nDeﬁnition 11. Denote by Lk\nH,INT , called the language of the interval Hammersley\nprocess, the set of words (over alphabet Σk ∪{⋄}) generated by the process HADk,INT .\nThe second language associated to the interval Hammersley process is deﬁned as follows:\nDeﬁnition 12. The effective language of the interval Hammersley process, Lk,eff\nH,INT , is\nthe set of strings in Σ∗\nk obtained by deleting all diamonds from some string in Lk\nH,INT .\nDespite the fact that the dynamics of process HADk,INT is quite different from\nthat of the ordinary process HADk (a fact that is reﬂected in the coefﬁcients of the two\npower series), and the conjectured scaling behavior is not at all similar (for k ≥2), our\nnext result shows that this difference is not visible on the actual trajectories: the effective\nlanguage of the Interval Hammersley process coincides with that of the ”ordinary”\nHammersley tree process. Indeed, we have:\nTheorem 13. For every k ≥1, Lk,eff\nH,INT = Lk\nH = {w ∈Σ∗\nk | w is k-dominant.}.\nThe previous result contrasts with our next theorem:\nTheorem 14. For k ≥1 the language Lk\nH,INT is not context-free.\nIn fact we can give a complete characterization of Lk\nH,INT similar in spirit to the\none given for language Lk\nH in Theorem 7:\nTheorem 15. Given k ≥1, the language Lk\nH,INT is the set of words w over alphabet\nΣk ∪{⋄} that satisfy the following conditions:\n1. |w|⋄= |w|/2. In particular |w| must be even.\n2. For every preﬁx p of w, (a). |p|⋄≤|p|/2 and (b). s(p) + (k + 1)|p|⋄≥k|p|.\nFinally, we return to the power series perspective on the Ulam-Hammersley problem\nfor heapable sequences. We outline a simple algorithm (based on dynamic programming)\nfor computing the coefﬁcients of the Hammersley power series Fk.\nTheorem 16. Algorithm ComputeMultiplicity correctly computes series Fk.\nWe defer the presentation of the application of this result to Section 10.\n6\nG. Istrate, C. Bonchis¸, and V. Rochian\nInput: k ≥1, w ∈Σ∗\nk\nOutput: Fk(w)\nS := 0. w = w1w2 . . . wn\nif w ̸∈Lk\nH\nreturn 0\nif w == ‘k‘\nreturn 1\nfor i in 1:n-1\nif wi == k and wi+1 ̸= k\nlet r = min{l ≥1 : wi+l ̸= 0 or i + l = n + 1}\nfor j in 1:r-1\nlet z = w1 . . . wi−1wi+1 . . . wi+j−11wi+j+1 . . . wi+r . . . wn\nS := S + ComputeMultiplicity(k, z)\nif i + r ̸= n + 1 and wi+r ̸= k\nlet z = w1 . . . wi−1wi+1 . . . wi+r−1(wi+r + 1)wi+r+1 . . . wn\nS := S + ComputeMultiplicity(k, z)\nif wn == k\nlet Z = w1 . . . . . . wn−1\nS := S + ComputeMultiplicity(k, z)\nreturn S\nFig. 3: Algorithm ComputeMultiplicity(k,w)\n3\nMotivation and notations\nDeﬁne Σ∞= ∪k≥1Σk. Given x, y over Σ∞we use notation x ⊑y to denote the fact\nthat x is a preﬁx of y. The set of (non-empty) preﬁxes of x is be denoted by Pref(x).\nA k-ary (max)-heap is a k-ary tree, non necessary complete, whose nodes have\nlabels t[·] respecting the min-heap condition t[parent(x)] ≥t[x]. Let rgeq1, and let\na1, b1, . . . , ar > 0 and br ≥0 be integers. We will use notation [a0, b0, . . . , at, bt] as a\nshorthand for the word 1a00b0 . . . 1ar0br ∈Σ∗\n1 (where 00 = ϵ, the null word).\nThe following combinatorial concept was introduced (for k = 2) in [7] and further\nstudied in [9,15,10,4,5,3]:\nDeﬁnition 17. A sequence X = X0, . . . , Xn−1 is max k-heapable if there exists some\nk-ary tree T with nodes labeled by (exactly one of) the elements of X, such that for\nevery non-root node Xi and parent Xj, Xj ≥Xi and j < i. In particular a 2-heapable\nsequence will simply be called heapable [7]. Min heapability is deﬁned similarly.\nExample 18. X = [5, 1, 4, 2, 3] is max 2-heapable: A max 2-heap is displayed in Fig-\nure 4. On the other Y = [2, 4, 1, 3] is obviously not max 2-heapable, as 4 cannot be a\ndescendant of 2.\nHeapability can be viewed as a relaxation of the notion of decreasing sequence, thus\nit is natural to attempt to extend to heapable sequences the framework of the Ulam-\nHammersley problem [17], concerning the scaling behavior of the longest increasing\nsubsequence (LIS) of a random permutation. This extension can be performed in (at\nThe language (and series) of Hammersley-type processes\n7\n5\n1\n4\n2\n3\nFig. 4: Heap ordered tree for sequence X in Example 18.\nleast) two ways, equivalent for LIS but no longer equivalent for heapable sequences:\nthe ﬁrst way, that of studying the length of the longest heapable subsequence, was\ndealt with in [7], and is reasonably simple: with high probability the length of the\nlongest heapable subsequence of a random permutation is n −o(n). On the other\nhand, by Dilworth’s theorem [8] the length of the longest increasing subsequence of\nan arbitrary sequence is equal to the number of classes in a partition of the original\nsequence into decreasing subsequences. Thus it is natural to call the Hammersley-Ulam\nproblem for heapable sequences the investigation of the scaling behavior of the number\nof classes of the partition of a random permutation into a minimal number of (max)\nheapable subsequences. This was the approach we took in [9]. Unlike the case of LIS,\nfor heapable subsequences the relevant parameter (denoted in [9] by MHSk(π)) scales\nlogarithmically, and the following conjecture was proposed:\nConjecture 19. For every k ≥2 there exists λk > 0 s.t., as n →∞, E[MHSk(π)]\nln(n)\nconverges to λk. Moreover λ2 = 1+\n√\n5\n2\nis the golden ratio.\nThe problem was further investigated in [4,5], where the existence of the constant\nλk was proved. The equality of λ2 to the golden ratio is less clear: authors of [4] claim\nit is slightly less than φ. Some non-rigorous, ”physics-like” arguments, in favor of the\nidentity λ2 = φ was already outlined in [9], and is presented in [10], together with\nexperimental evidence. Here we bring more convincing such evidence.\nThe intuition for Conjecture 19 relies on the extension from the LIS problem to\nheapable sequences of a correspondence between LIS and an interactive particle system\n[1] called the Hammersley-Aldous-Diaconis (shortly, Hammersley or HAD) process. The\nvalidity of correspondence was noted, for heapable sequences, in [9]. The generalized\nprocess was further investigated in [4], where it was called the Hammersley tree process.\nTo recover the connection with random permutations we will assume from now on\nthat the Xi’s in process HADk are independent random numbers in (0, 1). The proposed\nvalue for λ2 arises from a conjectural identiﬁcation of the ”hydrodynamic limit” of the\nHammersley tree process (in the form of a compound Poisson process).\nAs n →∞a ”typical” sample word from the Hammersley process HAD2 will have\napproximately c0n zeros, c1n ones and ∼c2n twos, for some constants3 c0, c1, c2 > 0.\nMoreover, conditional on the number of zeros, ones, twos, in a typical word these digits\n3 Nonrigorous computations predict that c0 = c2 =\n√\n5−1\n2\n, c1 = 3+\n√\n5\n2\n.\n8\nG. Istrate, C. Bonchis¸, and V. Rochian\nare ”uniformly mixed” throughout the sequence. Experimental evidence presented in\n[10] seems to conﬁrm the accuracy of this heuristic description.\nA proof of the existence of constants c0, c1, c2 was attempted in [9] based on subad-\nditivity (Fekete’s lemma). However, part of the proof in [9] is incorrect. While it could\nperhaps be ﬁxed using more sophisticated tools (e.g. the subadditive ergodic theorem\n[19]) than those in [9], an alternate approach involves analyzing the asymptotic behavior\nof process HADk using (noncommutative) power series ([18,6]).\nUnderstanding and controlling the behavior of formal power series Fk may be the key\nto obtaining a rigorous analysis that conﬁrms the picture sketched above. Though that\nwe would very much want to accomplish this task, in this paper we resign ourselves to a\nsimpler, language-theoretic, version of this problem, that of computing the associated\nformal language.\nThe Ulam-Hammersley problem has also been studied [11] for sets of random\nintervals, generated as follows: to generate a new interval In ﬁrst we sample (inde-\npendently and uniformly) two random x, y from (0, 1). Then we let In be the interval\n[min(x, y), max(x, y)]. In fact the problem was settled in [11], where the scaling of\nLIS for sets of random intervals was determined to be limn→∞\nE[LIS(I1,...,In)]\n√n\n=\n2\n√π.\nSeveral results on the heapability of partial orders were proved in [3]; in partic-\nular, the greedy algorithm for partitioning a permutation into a minimal number of\nheapable subsequences extends to interval orders. This justiﬁes an extension of the\nUlam-Hammersley problem from increasing to heapable sequences of intervals. Indeed,\nin [3] we conjectured the following scaling law:\nConjecture 20. For every k ≥2 there exists ck > 0 such that, if Rn is a sequence of n\nrandom intervals then limn→∞\nE[#Heapsk(Rn)]\nn\n= ck. Moreover ck =\n1\nk+1.\nRemarkably, it was already noted in [3] that the connection between the Ulam-\nHammersley problem and particle systems extends to the interval setting as well. To\nprove a similar result for the interval Hammersley process we need to ”combinatorialize”\nthe process from Deﬁnition 9, that is, to replace that deﬁnition (which employs (random)\nreal values in (0, 1)) with an equivalent stochastic process on words.\nThe combinatorialization process has some technical complications with respect to\nthe case of permutations. Speciﬁcally, for permutations the state of the system could be\npreserved, with no real loss of information by a string representing only the number of\nlifelines of the given particles, but not their actual values. This enables (as we will see\nbelow in Section 9) an algorithm for computing the associated formal power series.\nTo accomplish a similar goal for random intervals we apparently need to take into\naccount the fact that at each step we choose two random numbers in Deﬁnition 9, even\nthough only one of them receives a particle, since the second one inﬂuences the state of\nthe system. Thus, the proper discretization requires an extra symbol ⋄(that marks the\npositions of real values that were generated but in which no particle was inserted), and is\naccomplished as described in Deﬁnition 10 and the language from Deﬁnition 11.\nA result that was easy for the process HADk but deserves some discussion in the\ncase of the interval process is the following:\nProposition 21. Consider the string wn ∈Σ∗\nk obtained by taking a random state of\nthe Hammersley interval process with k lifelines at stage n and then ”forgetting” the\nThe language (and series) of Hammersley-type processes\n9\nparticle value information (recording instead only the value in Σk ∪{⋄}). Then wn has\nthe same distribution as a sample from process HADk,INT at stage n.\nProof. The crux of the proof is the following\nLemma 22. The ordering of the values X0, Y0, X1, Y1, . . . , Xn−1Yn−1 inserted in the\nﬁrst n steps in the Hammersley interval process (disregarding their number of lifelines)\nis that of a random permutation with 2n elements.\nProof. Xi, Yi have the same distribution, both are random uniformly distributed variables\nin (0, 1). Thus to simulate HADk,INT for n steps one needs 2n random numbers in\n(0,1), which yields a random permutation of size 2n.\nThis discussion motivates the language-theoretic study of trajectories of the interval\nHammersley process HADk,INT as well. In that respect Deﬁnition 12 seems better\nmotivated than Deﬁnition 11. Indeed, due to the presence of diamonds, words in the\nDeﬁnition 12 are not ”physical”, as diamonds do not necessarily correspond to actual\nparticles. On the other hand one can easily obtain an algorithm (similar to the Com-\nputeMultiplicity algorithm presented above) that computes multiplicities for “extended\nwords” in the process HADk,INT such as those in the Deﬁnition 11. Hence the study of\nthis second language is motivated on pragmatic grounds, as a ﬁrst step to investigating\nFk,INT , the formal power series of multiplicities in the interval Hammersley process.\nWe defer this investigation to the journal version of the paper.\n4\nProof of the main result\nThe proof of Theorem 7 proceeds by double inclusion. Inclusion ”⊆” is proved with the\nhelp of several easy auxiliary results:\nLemma 23. Every word in Lk\nH starts with a k.\nProof. Follows easily by appealing to the particle view of the Hammersley process: the\nparticle with the smallest label x stays with k lives until the end of the process, as no\nother particle can arrive to its left.\nLemma 24. Lk\nH is closed under preﬁx.\nProof. Again we resort to the particle view of the Hammersley process: let w ∈Lk\nH be\na word and u = x0 . . . xn−1 be a trajectory in [0,1] yielding w. A non-empty preﬁx z of\nw corresponds to the restriction of u to some segment [0, l], 0 < l < 1. This restriction\nis a trajectory itself, that yields z.\nLemma 25. Every word in Lk\nH has a positive structural difference.\nProof. Let w ∈Lk\nH and let t be a corresponding trajectory in the particle process.\nLet λ be the number of times a particle arrives as a local maximum (without sub-\ntracting a lifeline from anyone). For i = 1, . . . , k let λi be the number of time the\n10\nG. Istrate, C. Bonchis¸, and V. Rochian\nnewly arrived particle subtracts a lifeline from a particle currently holding exactly i lives.\nλ, λ1, . . . , λk ≥0. Moreover, λ > 0, since the largest particle does not take any lifeline.\nBy counting the number of particles with i lives at the end of the process, we infer:\n|z|0 = λ1, |z|1 = λ2 −λ1, · · · |z|k−1 = λk −λk−1. Finally, |z|k = λ + Pi−2\ni=0 λi.(∗)\nSimple computations yield λi+1 = |z|0 + . . . + |z|i, for i = 0, . . . , k −1. Relation\n(*) and inequality λ > 0 yield the desired result.\nTogether, Claims 23, 24 and 25 establish the fact that any word from Lk\nH is k-\ndominant, thus proving inclusion ”⊆”. To proceed with the opposite inclusion, for every\nk-dominant word w we must construct a trajectory of the process HADk that acts as a\nwitness for w ∈Lk\nH.\nWe will further reduce the problem of constructing a trajectory Tz to the case when\nz further satisﬁes a certain simple property, explained below:\nDeﬁnition 26. k-dominant word u is called critical if |u|k −\nk−2\nP\ni=0\n(k −1 −i) · |u|i = 1.\nThe above-mentioned reduction has the following statement:\nLemma 27. Every k-dominant critical z is witnessed by some trajectory Tz.\nProof. By induction on |z|. The base case, |z| = 1, is trivial, as in this case z = k.\nInductive step: Assume the claim is true for all the critical words of length strictly\nsmaller than z’s. We claim that w1, the word obtained from z by deleting the last copy\nof k and increasing by 1 the value of the letter immediately to the right of the deleted\nletter, is critical.\nIndeed, it is easy to see that the structural difference of w1 is 1. Clearly the deleted\nletter could not have been the last one, otherwise deleting it would yield a preﬁx of z that\nhas structural constant equal to zero. Also clearly, the letter whose value was modiﬁed in\nthe previous constraint could not have been a k, by deﬁnition, and certainly is nonzero\nafter modiﬁcation. So w1’s construction is indeed correct. As |w1| = |z|−1, w1 satisﬁes\nthe conditions of the induction hypothesis.\nBy the induction hypothesis, w1 can be witnessed by some trajectory T. We can\nconstruct a trajectory for z by simply following T and then inserting the last k of z into\nw1 in its proper position (thus also making the next letter assume the correct value).\nWe now derive Theorem 7 from Lemma 27. The key observation is the following\nfact: every k-dominant word z is a preﬁx of a critical word, e.g. z′ = z(k −2)λ where\nλ = |z|k −Pk−2\ni=0 (k −i −1) · |z|i −1 ≥0.\nBy Lemma 27, z′ has a witnessing trajectory Tz′. Since the existence of a trajectory\nis closed under taking preﬁxes, Theorem 7 follows.\n5\nProof of Corollary 8\nProof. For k = 1 the result is trivial, as L1\nH = 1Σ∗\n1. The claim that Lk\nH is a deterministic\none-counter language for k ≥2 follows from Theorem 7, as one can construct a one-\ncounter pushdown automaton Pk for the language on k-dominant words.\nThe language (and series) of Hammersley-type processes\n11\nThe one-counter PDA has input alphabet 0, 1, 2, . . . , k. Its stack alphabet contains\ntwo special stack symbols, the bottom symbol Z and another ”counting” symbol ∗. The\ntransitions of Pk are informally deﬁned as follows:\n- Pk starts with the stack consisting of the symbol Z. If the ﬁrst letter is not a k, Pk\nimmediately rejects. Otherwise it pushes a ∗on the stack.\n- on reading any subsequent k, Pk pushes a ∗symbol on stack.\n- on reading any symbol i ∈1 . . . k −2, Pk attempts to pop k −i −1 stars from the\nstack. If this ever becomes impossible (by reaching Z), Pk immediately rejects.\n- Pk ignores all k −1 symbols, proceeding without changing the content of the stack.\n- If, while reaching the end of the word, the stack still contains a star, Pk accepts.\nTo prove that Lk\nH, k ≥2, is not regular is a simple exercise in formal languages.\nIt involves applying the pumping lemma for regular languages to words wk,n =\nkn(k−1)+10n ∈Lk\nH. We infer that for large enough n, wk,n = w1w2w3, with w2\nnonempty and consisting of k’s only, such that for every l ≥0, w1wl\n2w3 ∈Lk\nH. We\nobtain a contradiction by letting l = 0, thus obtaining a word z that cannot belong to\nLk\nH, since |z|k ≤(k −1)|z|0.\n⊓⊔\n6\nProof of Theorem 13\nIt is immediate that Lk\nH ⊆Lk,eff\nH,INT . Indeed, every trajectory of the process HADk is\na trajectory of the process HADk,INT as well: simply restrict at every stage the two\nparticles to choose the same slot.\nFor the opposite inclusion we prove, by induction on |t|, that the outcome w of every\ntrajectory t of the interval Hammersley process belongs to Lk\nH. The case |t| = 0 is trivial,\nsince w = k.\nDeﬁnition 28. Given a word w over Σk, word z is a left translate of w if z can be\nobtained from z by moving a k in w towards the beginning of w (we allow ”empty\nmoves”, i.e. z = w).\nLemma 29. Lk\nH is closed under left translates. That is, if w ∈Lk\nH and z is a left\ntranslate of w then z ∈Lk\nH.\nProof. By moving forward a k the structural constants of all preﬁxes of w can only\nincrease. Thus if these constants are positive for all preﬁxes of w then they are positive\nfor all preﬁxes of z as well.\nNow assume that the induction hypothesis is true for all trajectories of length less\nthan n. Let t be a trajectory of length n, let t′ be its preﬁx of length n −1, let w be the\nyield of t and z be the yield of t′. By the induction hypothesis z ∈Lk\nH. Let y be the\nword obtained by applying the Hammersley process to z, deleting a life from the same\nparticle as the interval Hammersley process does to z to obtain w. It is immediate that\nw is a left translate of y (that is because in the interval Hammersley process we insert\na particle to the left of the position where we would in HADk). Since y ∈Lk\nH, by the\nprevious lemma w ∈Lk\nH.\n12\nG. Istrate, C. Bonchis¸, and V. Rochian\n7\nProof of Theorem 14\nDeﬁne the language Sk = Lk\nH,INT ∩{k}∗⋄∗{k −1}∗⋄∗.\nLemma 30. Sk = {kc+d+e ⋄c+e (k −1)c ⋄c+d |c, d, e ≥0}.\nProof. The direct inclusion is fairly simple: let w ∈Sk. deﬁne c to be the number of\nletters (k −1) in w. Since there are no diamonds in between the (k −1)’s, all such letters\nmust have been produced by removing one lifeline each by some k’s. Thus the number\nof stars in between the k’s and (k −1)’s is c + e, with e being the number of pairs (k, ⋄)\nthat did not kill any particle that will eventually become a k −1.\nOn the other hand the number of k’s is obtained by tallying up c (for the c letters that\nbecome k −1, needing one copy of k each), e (for the pairs (k, ⋄) where ⋄belongs to\nthe ﬁrst set of diamonds) and d (for d pairs (k, ⋄) with ⋄in the second set of diamonds).\nFor the reverse implication we outline the following construction:\nFirst we derive ke⋄e. Then we repeat the following strategy c times:\n- We insert a k at the beginning of the k −1 block (initially at the end of the word)\nand the corresponding ⋄at the end of the word.\n- With one pair k, ⋄(with ⋄inserted in the ﬁrst block) we turn the k into a k −1.\nFinally we insert k pairs (k, ⋄), with ⋄in the second block.\nThe theorem now follows from the following\nLemma 31. Sk is not a context-free language.\nProof. An easy application of Ogden’s lemma: We take a string s ∈Sk,\ns = kc+d+e ⋄c+e (k −2)c⋄c+d\nwith c, d, e ≥p (where p is the parameter in Ogden’s Lemma. We mark all positions of\nk −1. Then s = uvwxy, with uviwxiy ∈Sk for all i ≥0. The ”pumping blocks” v, x\ncannot consist of more than one type of symbols, otherwise the pumped strings would\nfail to be a member of {k}∗⋄∗{k −1}∗⋄∗.\nTherefore no more than two blocks (of the four in s) get pumped. One that deﬁnitely\ngets pumped is the ﬁrst block of diamonds. Taking large enough i we obtain a contradic-\ntion, since the block that fails to get pumped will eventually have smaller length than the\n(pumped) ﬁrst block of diamonds.\n8\nProof of Theorem 15\nProof. The inclusion ⊆is easy: given w ∈Lk\nH,INT , conditions 1. and 2 (a). hold, as the\nprocess HADk\nINT inserts a digit (more precisely a k) before every diamond.\nAs for condition 2 (b)., each ⋄takes at most one life of a particle. The total number\nof lives particles in p are endowed with at their moments of birth is k(|p| −|p|⋄). These\nlives are either preserved (and are counted by s(p)), or they are lost, in a move which\n(also) introduces a ⋄in p. Thus k(|p| −|p|⋄) ≤|p|⋄+ s(p), which is equivalent to b.\nThe language (and series) of Hammersley-type processes\n13\nThe inclusion ⊇is proved by induction on |w|. What we have to prove is that every\nword that satisﬁes conditions 1-2 is an output of the process HADk,INT .\nThe case |w| = 2 is easy: the only word that satisﬁes conditions 1-2 is easily seen to\nbe w = k⋄, which can be generated in one move.\nAssume now that the induction hypothesis is true for all words of lengths strictly less\nthan 2n, and let w = w1 . . . w2n be a word of length 2n satisfying conditions 1-2.\nLemma 32. w2n = ⋄.\nProof. Let p = w1 . . . w2n−1. By condition 2(a)., |p|⋄≤(2n−1)/2, hence |p|⋄≤n−1.\nSince |w|⋄= n, the claim follows.\nLemma 33. w1 = k.\nProof. Let q = w1. Since |q|⋄≤1/2, w1 must be a digit. Since s(q) ≥k|q| = k, the\nclaim follows.\nLet now r be the largest index such that wr = k. Let s be the leftmost position s > r\nsuch that ws = ⋄. Let t be the leftmost position t > s such that wt ̸= ⋄, t = 2n + 1 if\nno such position exists.\nConsider the word b obtained from w by a. deleting positions wr and ws. b. increasing\nthe digit at position wt by one, if t ̸= 2n + 1. Note that, if t ̸= 2n + 1 then wt ̸= k, by\nthe deﬁnition of index r. Also, |b| = 2n −2 < 2n.\nw is easily obtained from b by inserting a k in position r and a diamond in position\ns, also deleting one lifeline from position t if t ̸= 2n + 1. To complete the proof we\nneed to argue that b satisﬁes conditions 1-2a.b. Then, by induction, b is an output of the\nprocess HADk,INT , hence so is w.\nCondition 1 is easy to check, since |b| = 2n −2, and b has exactly one ⋄less than w,\ni.e. n-1 ⋄’s. As for 2.(a)-(b), let p be a preﬁx of b. There are four cases:\n- Case 1: 1 ≤|p| < r: In this case p is also a preﬁx of w, and the result follows from\nthe inductive hypothesis.\n- Case 2: r ≤|p| < s −1: In this case p = w1 . . . wr−1wr+1 . . . w|p|+1. Let\nz1 = w1 . . . . . . w|p|+1 be the corresponding preﬁx of w.\nThe number of diamonds in p is equal to the number of diamonds in z1. Since z1\ndoes not end with a diamond (as |p| < s −1), the number of diamonds in z1 is\nequal to that of its preﬁx u of length |p|. By the induction hypothesis |p|⋄= |u|⋄≤\n|u|/2 = |p|/2. So condition 2 (a). holds. On the other hand s(p) + (k + 1)|p|⋄=\n(s(z1) −k) + (k + 1)|z1|⋄≥k|z1| −k = k|p|, so 2 (b). holds as well.\n- Case 3: s −1 ≤|p| < t −2: Thus p = w1 . . . wr−1wr+1 . . . ws−1ws+1 . . . w|p|+2.\nLet z2 = w1 . . . . . . w|p|+2 be the corresponding preﬁx of w of length |p| + 2 and z3\nthe preﬁx of w of length s −1.\nThe number of diamonds in p is equal to the number of diamonds in z2 minus\none. By the induction hypothesis, this is at most |z2|/2 −1, which is at most\n(|p| + 2)/2 −1 = |p|/2. Thus condition 2 (a). holds. Now s(p) + (k + 1)|p|⋄=\n(s(z2) −k) + (k + 1)(|z2|⋄−1) = (s(z3) −k) + (k + 1)(|z3|⋄+ |z2| −|z3| −1)\n≥k|z3| −k + (k + 1)(|p| + 2 −|z3| −1) = (k + 1)(|p| + 1) −|z3| −k =\n= (k + 1)|p| −|z3| + 1 > k|p| + 1 + (|p| −|z3|) > k|p|\n14\nG. Istrate, C. Bonchis¸, and V. Rochian\nso condition 2 (b). is established as well. In the previous chain of (in)equalities we\nused the fact (valid by the very deﬁnition of t) that for all s ≤i < t, wi = ⋄.\n- Case 4: t−2 ≤|p| ≤2n: ] In this case p = w1 . . . wr−1wr+1 . . . ws−1ws+1 . . . (wt+\n1) . . .. Furthermore, p ends with w|p|+2 (if |p| + 2 ̸= t) and with w|p|+2 + 1 (if\n|p| + 2 = t). Let z4 = w1 . . . . . . w|p|+2 be the preﬁx of w of length |p| + 2.\n- |p|⋄= |z4| −1 ≤|z4|/2 −1 = (|p| + 2)/2 −1 = |p|/2.\n- On the other hand s(p) + (k + 1)|p|⋄= (s(z4)) −k + 1) + (k + 1)(|z4| −1) ≥\nk|z4| −k + 1 −k −1 = k · (|p| + 2) −2k = k|p|.\nso conditions 2 (a)-(b). are proved in this last case as well.\n9\nProof of Theorem 16\nJustifying correctness of algorithm ComputeMultiplicity is simple: a string w can result\nfrom any string z by inserting a k and deleting one life from the closest non-zero letter of\nz to its right. After insertion, the new k will be the rightmost element of a maximal block\nof w of consecutive k’s. The letter it acts upon in z cannot be a k (in w), and cannot have\nany letters other than zero before it.\nThe candidates in w for the changed letter are those letters l succeeding the newly\ninserted k such that 0 ≤l ≤k −1 and the only values between k and l are zeros. Thus\nthese candidates are the following: (a)letters in w forming the maximal block B of zeros\nimmediately following k (if any), and (b).the ﬁrst letter after B, provided it has value\n0 to k −1. Since we are counting multiplicities and all these words lead to distinct\ncandidates, the correctness of the algorithm follows.\nFor k = 1 the algorithm ComputeMultiplicity simpliﬁes to a recurrence formula:\nIndeed, in this case there are no candidates of type (b). We derive:\nF1([a1, b1, . . . , as, bs]) = Ps\ni=1:\nai>1\nP\nj,l≥0\nj+1+l=bi\nF1([a1, . . . , ai−1, j, 1, l, ai+1, . . . , bs])+\nPs\ni=1:\nai=1\nP\nj,l≥0\nj+1+l=bi\nF1([a1, . . . , ai−1, bi−1 + j, 1, l, ai+1, . . . , bs]) if bs > 0, otherwise\nF1([a1, . . . , as, 0]) = Ps−1\ni=1:\nai>1\nP\nj,l≥0\nj+1+l=bi\nF1([a1, . . . , ai −1, j, 1, l, ai+1, . . . , as, 0]) +\nPs−1\ni=1:\nai=1\nP\nj,l≥0\nj+1+l=bi\nF1([a1, . . . , ai−1, bi−1 + j, 1, l, ai+1, . . . , bs]) + F1([a1, . . . , as −\n1, 0]).\nw\n1\n10\n11\n100\n101\nF1(w)\n1\n1\n1\n1\n2\nw\n110\n111 1000 1001 1010\nF1(w)\n2\n1\n1\n3\n5\nw\n1011 1100 1101 1110 1111\nF1(w)\n3\n5\n3\n3\n1\nw\n2\n21\n22\n211\n212\n220\n221\nF2(w)\n1\n1\n1\n1\n2\n1\n1\nw\n222 2111 2112 2120 2121 2122 2201\nF2(w)\n1\n1\n3\n2\n3\n3\n1\nw\n2202 2210 2211 2212 2220 2221 2222\nF2(w)\n3\n1\n1\n2\n2\n1\n1\nFig. 5: The leading coefﬁcients of formal power series (a). F1. (b). F2.\nIn spite of this, we weren’t able to solve the recurrence above and compute the gen-\nerating functions F1 or, more generally, Fk, for k ≥1. An inspection of the coefﬁcients\nThe language (and series) of Hammersley-type processes\n15\nobtained by the application of the algorithm is inconclusive: We tabulated the leading\ncoefﬁcients of series F1 and F2, computed using the Algorithm 3 in Figures 5 (a). and\n(b). The second listing is restricted to 2-dominant strings only. No apparent closed-form\nformula for the coefﬁcients of F1, F2 emerges by inspecting these values.\n10\nApplication: estimating the value of the scaling constant λ2.\nThe computation of series F2 allows us to tabulate (for small value of n) the values of\nthe distribution of increments, a structural parameter whose limiting behavior determines\nthe value of the constant λ2 (conjectured, remember, to be equal to 1+\n√\n5\n2\n).\nDeﬁnition 34. Let w be a word that is an outcome of the process HADk. An increment\nof w is a position p in w (among the |w| + 1 possible positions: at the beginning of w, at\nthe end of w or between two letters of w) such that no nonzero letters of w appear to the\nright of p. The number of increments of word w is denoted by #inck(w). It is nothing\nbut 1 plus the number of trailing zeros of w.\nLet L be an alphabet that contains Σk for some k ≥1. Given a word w ∈L∗we\ndenote by s(w) the sum of the digit characters of w.\nThe fact that increments are useful in computing λ2 is seen as follows: consider a\nword w of length n that is a sample from the HADk process. Increments of w are those\npositions where the insertion of a k does not remove any lifeline, thus increasing the\nnumber of heaps in the corresponding greedy ”patience heaping” algorithm [9] by 1. If\nw has t increment positions then the probability that the number of heaps will increase\nby one (given that the current state of the process is w) is t/(n + 1).\nWhat we need to show is that (as n →∞) the mean number of positions that are\nincrements in a random sample w of length n tends to λk. Therefore the probability\nthat a new position will increase the number of heaps by 1 is asymptotically equal to\nλk/(n + 1). The scaling of the expected number of heaps follows from this limit.\nIn Figure 6. we plot the exact probability distribution of the number of increments\n(from which we subtract one, to make the distribution start from zero) for k = 2 and\nseveral small values of n. They were computed exactly by employing Algorithm 3 to\nexactly compute the probability of each string w, and then computing #inc2(w). We\nperformed this computation for 2 ≤n ≤13. The corresponding expected values are\ntabulated (for all values n = 2, . . . , 10) in Figure 7.\nUnfortunately, as it turns out, the ability to exactly compute (for small values of n) the\ndistribution of increments does not give an accurate estimate of the asymptotic behavior\nof this distribution, as the convergence seems rather slow, and not at all captured by these\nsmall values of n. Indeed, to explore the distribution of increments for large values of\nn, as exact computation is no longer possible, we instead resorted to sampling from the\ndistribution, by generating 10000 independent random trajectories of length n from pro-\ncess HAD2, and then computing the distribution of increments of the sampled outcome\nstrings. The outcome is presented (for n = 100, 100000, 1000000, together with some\nof the cases of the exact distribution) in Figure 6. The distribution of increments seems to\nconverge (as n →∞) to a geometric distribution with parameter p =\n√\n5−1\n2\n∼0.618 · · · .\n16\nG. Istrate, C. Bonchis¸, and V. Rochian\nFig. 6: Probability distribution of increments, for k = 2, and n = 5, 9, 13, 1000000.\nn\n2\n3\n4\n5\n6\n7\nE[#inc2]\n1.0\n1.166 1.208 1.250 1.281 1.307\nn\n8\n9\n10\n100 100000 1 mil\nE[#inc2] 1.329 1.347 1.363 1.520 1.575 1.580\nFig. 7: The mean values of the distributions of increments.\nThat is, we predict that for all i ≥1, limn→∞Pr|w|=n[#inc2(w) = i] = p·(1−p)i−1.\nThe ﬁt between the (sampled) estimates for n = 1000000 and the predicted limit distri-\nbution is quite good: every coefﬁcient differs from its predicted value by no more than\n0.003, with the exception of the fourth coefﬁcient, whose difference is 0.007. Because of\nthe formula for computing averages, these small differences have, though, a cumulative\neffect in the discrepancy for the average E[#inc2(w)] for n = 10000000 accounting\nfor the 0.03 difference between the sampled value and the predicted limit: in fact most\nof the difference is due to the fourth coefﬁcient, as 4 × 0.007 = 0.028.\nConclusion 1 The increment data supports the conjectured value λ2 = 1 + p = 1+\n√\n5\n2\n.\nWe intend to present (in the journal version of this paper) a similar investigation of the\nvalue of constant ck in Conjecture 20.\n11\nOpen questions and future work\nThe major open problems raised by our work concerns the nature and asymptotic behavior\nof formal power series Fk, Fk,INT . An easy consequence of Corollary 8 is\nCorollary 35. For k ≥2 formal power series Fk, Fk,INT are not N-rational.\nOpen Problem 1 Are formal power series F1, F1,INT N-rational ? (We conjecture that\nthe answer is negative).\nNote that Reutenauer [16] extended the Chomsky-Sch¨utzenberger criterion for ratio-\nnality from formal languages to power series: a formal power series is rational if and\nThe language (and series) of Hammersley-type processes\n17\nonly if the so-called syntactic algebra associated to it has ﬁnite rank. We don’t know,\nthough, how to explicitly apply this result to the formal power series we investigate in\nthis paper. On the other hand, in the general case, the characterization of context-free\nlanguages as supports of N-algebraic series (e.g. Theorem 5 in [14] ), together with\nTheorem 14, establishes the fact that series Fk,INT is not N-algebraic.\nOpen Problem 2 Are series Fk N-algebraic ? (Conjecture: the answer is negative).\nReferences\n1. D. Aldous and P. Diaconis. Hammersley’s interacting particle process and longest increasing\nsubsequences. Probability theory and related ﬁelds, 103(2):199–213, 1995.\n2. D. Aldous and P. Diaconis. Longest increasing subsequences: from patience sorting to the\nBaik-Deift-Johansson theorem. Bull.of the A.M.S., 36(4):413–432, 1999.\n3. J. Balogh, C. Bonchis¸, D. Dinis¸, G. Istrate, and I. Todinc˜a. Heapability of partial orders. arXiv\npreprint arXiv:1706.01230, 2017.\n4. A.-L. Basdevant, L. Gerin, J.-B. Gou´er´e, and A. Singh. From Hammersley’s lines to Ham-\nmersley’s trees. Probability Theory and Related Fields, pages 1–51, 2016.\n5. A.-L. Basdevant and A. Singh. Almost-sure asymptotic for the number of heaps inside a\nrandom sequence. arXiv preprint arXiv:1702.06444, 2017.\n6. J. Berstel and C. Reutenauer. Noncommutative rational series with applications, volume 137.\nCambridge University Press, 2011.\n7. J. Byers, B. Heeringa, M. Mitzenmacher, and G. Zervas. Heapable sequences and subse-\nqeuences. In Proceedings of ANALCO’2011, pages 33–44. SIAM Press, 2011.\n8. R. P. Dilworth. A decomposition theorem for partially ordered sets. Annals of Mathematics,\npages 161–166, 1950.\n9. G. Istrate and C. Bonchis¸. Partition into heapable sequences, heap tableaux and a multiset\nextension of Hammersley’s process. In Proceedings of CPM’2015, volume 9133 of Lecture\nNotes in Computer Science, pages 261–271. Springer, 2015.\n10. G. Istrate and C. Bonchis¸. Heapability, interactive particle systems, partial orders: Results and\nopen problems. In Proceedings of DCFS’2016, volume 9777 of Lecture Notes in Computer\nScience, pages 18–28. Springer, 2016.\n11. J. Justicz, E. R. Scheinerman, and P. M. Winkler. Random intervals. Amer. Math. Monthly,\n97(10):881–889, 1990.\n12. T. Liggett. Interacting Particle Systems. Springer Verlag, 2004.\n13. C. Moore and P. Lakdawala. Queues, stacks and transcendentality at the transition to chaos.\nPhysica D, 135(1–2):24–40, 2000.\n14. I. Petre and A. Salomaa. Algebraic systems and pushdown automata. Handbook of Weighted\nAutomata, pages 257–289, 2009.\n15. J. Porﬁlio.\nA combinatorial characterization of heapability.\nMaster’s thesis, Williams\nCollege, May 2015. available from https://unbound.williams.edu/theses/\nislandora/object/studenttheses\\%3A907. Accessed: December 2017.\n16. C. Reutenauer. S´eries formelles et algebres syntactiques. J. Algebra, 66(2):448–483, 1980.\n17. D. Romik. The surprising mathematics of longest increasing subsequences. Cambridge\nUniversity Press, 2015.\n18. A. Salomaa and M. Soittola. Automata-theoretic aspects of formal power series. Springer\nScience & Business Media, 2012.\n19. W. Szpankowski. Average Case of Algorithms on Sequences. John Wiley & Sons, 2001.\n20. D. Welsh. Complexity: Knots, Colourings and Counting. Cambridge University Press, 1994.\n21. H. Xie. Grammatical Complexity and One-Dimensional Dynamical Systems. Directions in\nChaos vol. 6. World Scientiﬁc, 1996.\n",
  "categories": [
    "cs.FL",
    "cs.DM",
    "math.CO",
    "math.PR"
  ],
  "published": "2018-02-09",
  "updated": "2018-04-09"
}