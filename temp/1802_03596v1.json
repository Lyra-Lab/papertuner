{
  "id": "http://arxiv.org/abs/1802.03596v1",
  "title": "Deep Meta-Learning: Learning to Learn in the Concept Space",
  "authors": [
    "Fengwei Zhou",
    "Bin Wu",
    "Zhenguo Li"
  ],
  "abstract": "Few-shot learning remains challenging for meta-learning that learns a\nlearning algorithm (meta-learner) from many related tasks. In this work, we\nargue that this is due to the lack of a good representation for meta-learning,\nand propose deep meta-learning to integrate the representation power of deep\nlearning into meta-learning. The framework is composed of three modules, a\nconcept generator, a meta-learner, and a concept discriminator, which are\nlearned jointly. The concept generator, e.g. a deep residual net, extracts a\nrepresentation for each instance that captures its high-level concept, on which\nthe meta-learner performs few-shot learning, and the concept discriminator\nrecognizes the concepts. By learning to learn in the concept space rather than\nin the complicated instance space, deep meta-learning can substantially improve\nvanilla meta-learning, which is demonstrated on various few-shot image\nrecognition problems. For example, on 5-way-1-shot image recognition on\nCIFAR-100 and CUB-200, it improves Matching Nets from 50.53% and 56.53% to\n58.18% and 63.47%, improves MAML from 49.28% and 50.45% to 56.65% and 64.63%,\nand improves Meta-SGD from 53.83% and 53.34% to 61.62% and 66.95%,\nrespectively.",
  "text": "Deep Meta-Learning: Learning to Learn in the Concept Space\nFengwei Zhou\nBin Wu\nZhenguo Li\nHuawei Noah’s Ark Lab\n{zhou.fengwei, wu.bin1, li.zhenguo}@huawei.com\nAbstract\nFew-shot learning remains challenging for meta-\nlearning that learns a learning algorithm (meta-\nlearner) from many related tasks. In this work, we\nargue that this is due to the lack of a good represen-\ntation for meta-learning, and propose deep meta-\nlearning to integrate the representation power of\ndeep learning into meta-learning. The framework\nis composed of three modules, a concept genera-\ntor, a meta-learner, and a concept discriminator,\nwhich are learned jointly. The concept generator,\ne.g. a deep residual net, extracts a representa-\ntion for each instance that captures its high-level\nconcept, on which the meta-learner performs few-\nshot learning, and the concept discriminator rec-\nognizes the concepts. By learning to learn in the\nconcept space rather than in the complicated in-\nstance space, deep meta-learning can substantially\nimprove vanilla meta-learning, which is demon-\nstrated on various few-shot image recognition\nproblems. For example, on 5-way-1-shot image\nrecognition on CIFAR-100 and CUB-200, it im-\nproves Matching Nets from 50.53% and 56.53%\nto 58.18% and 63.47%, improves MAML from\n49.28% and 50.45% to 56.65% and 64.63%, and\nimproves Meta-SGD from 53.83% and 53.34% to\n61.62% and 66.95%, respectively.\n1. Introduction\nMany successes of machine learning today rely on enormous\namounts of labeled data, which is not practical for problems\nwith small data. For new applications such as autonomous\nvehicles, it is crucial to adapt instantly in a dynamic envi-\nronment. In either cases, learning algorithms are required\nto consume labeled data efﬁciently. As one remarkable ex-\nample, humans can learn new concepts rapidly from single\nimages by leveraging knowledge learned before (Lake et al.,\n2015). However, most learning algorithms, especially those\nof deep learning, are data-hungry and do not function other-\nwise. Recently, meta-learning, pioneered by (Schmidhuber,\n1987), draws renewed interest which learns on the level of\ntasks instead of instances, and learns task-agnostic learning\nalgorithms (e.g. SGD) instead of task-speciﬁc models (e.g.\nCNN). Remarkably, once trained, it can learn new tasks\nquickly from only a few examples (few-shot learning).\nIn meta-learning, one learns from a set of “labeled” tasks,\nas opposed to labeled instances, where each task consists of\na training set and a test set. The goal is to ﬁt to the tasks a\nlearning algorithm that generalizes well to related new tasks,\ni.e., it can learn from the training data a learner (a model)\nthat performs well on the test data. It involves learning\nat two levels – gradual learning performed across tasks to\ngain meta-level knowledge and rapid learning carried out\nfor a new task which is guided by the knowledge learned\nbefore. Meta-learning has been shown to signiﬁcantly out-\nperform conventional learning on various few-shot learning\nproblems, ranging from classiﬁcation (Santoro et al., 2016;\nVinyals et al., 2016; Ravi & Larochelle, 2017; Finn et al.,\n2017; Li et al., 2017; Snell et al., 2017), reinforcement learn-\ning (Wang et al., 2016; Duan et al., 2016; Nikhil Mishra,\n2018; Kevin Frans, 2018), regression (Santoro et al., 2016;\nFinn et al., 2017), machine translation (Kaiser et al., 2017),\nto object tracking (Park & Berg, 2018). However, its full\npotential is still far from reach. For example, the accuracy\nof 5-way 5-shot recognition of natural images is around\n60% despite humans surpass 90% with ease. We argue\nthat this is due to the lack of a good data representation for\nmeta-learning.\nFew-shot learning is inherently challenging in the compli-\ncated instance space, where a few examples are insufﬁcient\nto describe the intended high-level information such as cate-\ngories or concepts. Consider image recognition, where each\nclass corresponds to an abstract concept of objects such\nas “cat” or “dog”. As an object is subject to a number of\nvariations in scale, pose, translation, occlusion, illumination,\ndistortion, background, etc, the instance space can be highly\ncomplicated where two images of the same object can be\ndrastically different in appearance. This makes few-shot\nlearning almost intractable. While meta-learning alleviates\nthe issue by leveraging many related tasks, it does not solve\nthe problem. In this paper, we aim to show that few-shot\nlearning is much easier for meta-learning if done in the\nconcept space.\nConcepts more likely consist of rules rather than deﬁni-\ntions (Ahn & Brewer, 1993). Instead of designing the rules\nby hand, we intend to leverage the power of deep learning. It\narXiv:1802.03596v1  [cs.LG]  10 Feb 2018\nDeep Meta-Learning: Learning to Learn in the Concept Space\nFigure 1: Deep meta-learning: learning to learn in the concept space. The concept generator learns to extract concept-level\nrepresentations to ease meta-learning, while being enhanced by the concept discriminator that recognizes the concepts.\nis known that a deep convolutional neural network trained on\nlarge-scale image dataset can provide effective features for\ngeneric tasks (Donahue et al., 2014; Razavian et al., 2014;\nZeiler & Fergus, 2014). However, the meta-learner can\nhardly beneﬁt from directly attaching the concept generator\nto it, especially when the previous image recognition tasks\nare quite different from the meta-learning tasks at hand.\nIn our deep meta-learning framework, the key idea is to\ntrain a concept generator together with meta-learning tasks\nand large-scale image recognition tasks, which will ﬁnally\nimprove the performance of vanilla meta learning meth-\nods. Speciﬁcally, a concept generator and a meta-learner\nare trained on a series of related tasks, and the concept\ngenerator is enhanced through a concept discriminator by\nhandling image recognition tasks from an external dataset at\nthe same time. This joint learning process can balance the\nconcept learning from a large number of related few-shot im-\nage recognition tasks and from external large-scale datasets,\nwhich allows to incorporate both the external knowledge and\ntask-agnostic meta-level knowledge into the concept gener-\nator. Also, this new meta-learning framework is a life-long\nlearning system, where the concept generator can evolve\ncontinuously with the coming of fresh labeled samples.\nOur main contributions can be summarized as follows:\n• We propose deep meta-learning to integrate the power\nof deep learning into meta-learning, and show it im-\nproves vanilla meta-learning signiﬁcantly on the prob-\nlem of few-shot image recognition (see Figure 1). We\nbelieve this framework is general and applicable to a\nvariety of machine learning problems including rein-\nforcement learning and regression.\n• We propose to equip a meta-learner with a concept gen-\nerator to enable learning to learn in the concept space\nwhile employing a concept discriminator to enhance\nthe concept generator, and show that all three mod-\nules can be trained jointly in an end-to-end manner.\nSince the concept generator will continue to evolve\nwith coming labeled data, this framework could liter-\nally be implemented as a life-long learning system.\n• We instantiate the deep meta-learning framework on\ntop of three state-of-the-art meta-learners including\nMatching Nets (Vinyals et al., 2016), MAML (Finn\net al., 2017), and Meta-SGD (Li et al., 2017), and\nconduct extensive experiments to show that deep meta-\nlearning utilizes data more efﬁciently than all existing\nmethods, and provides signiﬁcantly better results on\nfew-shot image recognition.\n2. Related Work\nMeta learning. In (Ravi & Larochelle, 2017), an LSTM is\nlearned to train a learner such as CNN as it rolls out. (Finn\net al., 2017) learn how to initialize SGD while (Li et al.,\n2017) learn a full-stack SGD, including initialization, up-\ndate direction, and learning rate. In (Vinyals et al., 2016), a\nmatching network with a non-parametric, differential KNN-\nlike classiﬁer is learned. In these methods, the meta-learner\nperforms few-shot learning in the instance space, while in\nour proposed deep meta-learning, it is done in the concept\nspace. Memory-augmented neural networks show high ca-\npacity for meta-learning. (Santoro et al., 2016) train an\nLSTM as a controller for accessing (read and write) an addi-\ntional memory module, which is an extension of the internal\nmemory in LSTM. The training process is time-consuming\nsince the controller has to retrieve the entire memory at each\ntime step. The memory is task-speciﬁc and is emptied once\nthe task is ﬁnished. (Kaiser et al., 2017) also learn a match-\ning network like (Vinyals et al., 2016) but include a memory\nmodule that retains previously seen examples or their repre-\nsentatives. This method is designed to remember rare events,\nwhich is useful for machine translation. Like (Santoro et al.,\n2016), it is challenging to balance between the efﬁciency\nand accuracy of memory retrieval. Our concept generator\ncan be considered as a memory module which memorizes\nthe concept of each instance via a deep neural network, but\nit eliminates the need for exhaustive memory retrieval.\nDeep Meta-Learning: Learning to Learn in the Concept Space\nTransfer learning and multi-task learning.\nTransfer\nlearning aims to transfer knowledge obtained from one do-\nmain with plenty of labeled data to another domain with few\nlabeled data (Pan & Yang, 2010). Its performance depends\non how relevant of the previous large-scale image recogni-\ntion tasks to the tasks of interest (Yosinski et al., 2014).In the\ndeep learning regime, ﬁne-tuning is a popular technique to\nperform transfer learning (Yosinski et al., 2014). However,\nthe choices of frozen layers and learning rate should be man-\nually tailored to avoid over-ﬁtting and under-ﬁtting. Also,\nthis tedious labor work has to be done for every new task. In\ncontrast, meta-learner is capable of rapidly adapting to new\ntasks automatically. Multi-task learning has been well stud-\nied in the literature, where the central idea is to jointly learn\nmultiple related tasks via a shared representation (Caruana,\n1998). For example, Fast R-CNN (Girshick, 2015) trains im-\nage classiﬁers and bounding-box regressors simultaneously\nto perform object detection. Given 5-way-1-shot setting in\na dataset of 100 classes, the number of 5-way classiﬁcation\ntasks would incredibly reach to\n\u0000100\n5\n\u0001\n× 5!=9,034,502,400,\nand multi-task learning methods may fail in dealing with\ntoo many parameters induced by the large number of tasks\n(Argyriou et al., 2007; Ji & Ye, 2009). Meta-learning differs\nin that the meta-learner, once learned from many related\ntasks, can apply to any new task of the kind. Instead of\nmeasuring the similarity between tasks explicitly (Evgeniou\net al., 2005), meta-learning methods could learn it more\nintrinsically.\nFew-shot image recognition. Quite a few methods have\nbeen proposed for few-shot image recognition. (Li et al.,\n2006) present a Bayesian model for learning categories from\na few examples per category. (Salakhutdinov et al., 2012)\norganize seen categories into super-categories to derive hi-\nerarchically structured priors for new categories using a\nhierarchical Bayesian model. (Lake et al., 2011) develop\na generative model that composes pen strokes into charac-\nters for handwritten character recognition. (Wong & Yuille,\n2015) extend this idea to natural images without relying\non domain knowledge. Instead of following the typical\ngradient decent method, (Bertinetto et al., 2016) suggest a\nfeed-forward learner for learning deep neural networks to\naddress the overﬁtting for few-shot learning. (Hariharan\n& Girshick, 2016) generate dummy examples for the task\nof interest by using the geometric transformations inferred\nfrom existing examples of other categories. Recently, (Xu\net al., 2017) show a key-value memory networks for few-\nshot learning, which is not scalable due to the huge memory\nsize and the heavy cost in memory retrieval. Besides, the\nﬁxed feature extractor makes it difﬁcult to generalize to\nother domains. Other methods use metric learning to ease\npairwise comparison between examples (Fink, 2005; Koch,\n2015; Guillaumin et al., 2009; Schroff et al., 2015). Our\nmethod relies on meta-learning but learns to learn in the\nconcept space.\n3. Deep Meta-Learning\nIn this section, we propose a new meta-learning framework,\ncalled deep meta-learning (DEML), which integrates the\nrepresentation power of deep learning into meta-learning,\nand enables learning to learn in the concept space.\n3.1. Framework\nOur framework (Figure 1) is composed of three modules,\na concept generator G, a meta-learner M, and a concept\ndiscriminator D, which are learned jointly. On one hand, we\nexpect the concept generator G extract task-agnostic meta-\nlevel representations that capture the high-level concepts of\nthe instances from many related tasks, which can guide the\nmeta-learner M to perform task-speciﬁc few-shot learning\nquickly . On the other hand, we hope that the concept gener-\nator G can be enhanced through the concept discriminator D\nby handling concept discrimination tasks on external large-\nscale datasets D. After seeing a large number of instances\nand their concepts, the concept generator G gradually learns\nthe mapping from the raw instance space to the abstract con-\ncept space, and this high-capacity representation provider\nwill greatly ease the meta-learning process.\nMathematically, we formulate the following optimization\nproblem:\nmin\nθG,θM,θD ET ∼p(T ),(x,y)∼D[J(LT (θM, θG), L(x,y)(θD, θG))],\nwhere θG, θM and θD are the parameters of corresponding\nmodules. Meta-learning tasks T follow a distribution p(T )\nin a task space, and (x, y) represents a labeled instance\nsampled from an external dataset D. The objective is to\nminimize the expectation of the joint, denoted by J, of two\nlosses: the loss LT (θM, θG) on the meta-learning tasks and\nthe loss L(x,y)(θD, θG) on the concept discrimination tasks.\nThe overall deep meta-learning process is summarized in\nAlgorithm 1.\nAlgorithm 1 Deep Meta-Learning\nInput:\ntask distribution p(T ), labeled dataset D,\nlearning rate β\nOutput:θG, θD, θM\nInitialize θG, θD, θM\nwhile not done do\nSample task batch Bt;\nSample instance batch Bi;\nCompute meta-learning loss: LBt(θM, θG);\nCompute concept discrimination loss: LBi(θD, θG);\n(θG, θD, θM) ←(θG, θD, θM)\n−β∇\nh\nJ(LBt(θM, θG), LBi(θD, θG))\ni\n;\nend while\nDeep Meta-Learning: Learning to Learn in the Concept Space\n3.2. Modules\nFor the meta-learning pipeline M ◦G, we assume that there\nis a distribution p(T ) over the related task space, from which\nwe can sample tasks uniformly at random, and each task T\nconsists of a training set train(T ) and a testing set test(T ).\nFor the concept discrimination pipeline D ◦G, we assume\nthat there is a large-scale labeled dataset D from which we\ncan randomly sample labeled instances.\nConcept generator.\nThe concept generator G, param-\neterized by θG, is a deep neural network that could\nbe any popular convolutional neural network such as\nAlexNet (Krizhevsky et al., 2012), Inception (Szegedy et al.,\n2015), VGG (Simonyan & Zisserman, 2014), or ResNet (He\net al., 2016).\nConcept discriminator. The concept discriminator D, pa-\nrameterized by θD, is designed to predict labels for concepts\ngenerated by G. It could be implemented with any super-\nvised learning method, such as support vector machines,\nnearest neighbor classiﬁers, or neural networks.\nMeta-learner.\nThe meta-learner M, parameterized by\nθM, learns to learn a learner for each task T based on\ntrain(T ). Any existing meta-learner can be used in our\nframework (Vinyals et al., 2016; Ravi & Larochelle, 2017;\nFinn et al., 2017; Li et al., 2017; Snell et al., 2017). Match-\ning Nets (Vinyals et al., 2016) is a non-parametric model\nfor few-shot learning based on metric learning. It learns\nfrom many related tasks a meta-learner (attention mecha-\nnism) which can guide the learner to do sample embedding.\nMAML (Finn et al., 2017) is another model designed for\nfew-shot learning problems. It learns from many related\ntasks a meta-learner which can initialize a learner and update\nit by gradient descent with a ﬁxed learning rate for each task\nT based on train(T ). Meta-SGD (Li et al., 2017) suggests\nthe meta-learner should learn not only the initialization, but\nalso the update direction and learning rate of the learner.\nIn our deep meta-learning framework, the concept generator\nG, the concept discriminator D, and the meta-learner M\nare abstract modules which can be implemented using any\nproper models.\n3.3. Criterion\nThe two pipelines, concept discrimination and meta-\nlearning, are trained jointly in a learning-to-learn manner to\noptimize a combined loss. For the concept discrimination\npipeline D ◦G, the concept generator is trained to gener-\nate representations for samples that capture their concepts\nand the concept discriminator is trained to discriminate the\nconcepts. The goal is to minimize the expected loss on the\nconcept discrimination tasks:\nL(x,y)(θD, θG) = ℓ(D ◦G(x), y),\nwhere ℓcould be any loss function suitable for concept dis-\ncrimination. For the meta-learning pipeline M ◦G, the\nconcept generator is trained to extract meta-level representa-\ntions for samples and the meta-learner is learned to perform\nfew-shot learning in the high-level concept space. Given\nthe task T , we deﬁne the meta-learner M as a mapping:\nM : T →fT , i.e. M adapts a learner fT for a task T\nthrough parametric (Finn et al., 2017; Li et al., 2017) or\nnon-parametric (Vinyals et al., 2016) ways. The goal is\nto minimize the expected generalization loss on the meta-\nlearning tasks:\nLT (θM, θG) =\n1\n|test(T )|\nX\n(x,y)∈test(T )\nℓ(fT ◦G(x), y).\nIts computation depends on the meta-learning algorithms\nselected. For example, if we choose Matching Nets (Vinyals\net al., 2016) as our meta-learner, then fT ◦G(x) would be\nformalized as follows:\nfT ◦G(x) =\nX\n(x′,y′)∈train(T )\na(G(x), G(x′)) y′,\nwhere\na(G(x), G(x′)) =\nec(g(G(x)),g(G(x′)))\nP\n(x′,y′)∈train(T ) ec(g(G(x)),g(G(x′)))\nis the softmax over the cosine distance c and the embedding\nfunction g.\nIf MAML (Finn et al., 2017) is chosen as the meta-learner,\nthen\nfT ◦G(x) = fθM −α∇θMLtrain(T )(θM,θG)(G(x))\nwhere\nLtrain(T )(θM, θG) =\n1\n|train(T )|\nX\n(x,y)∈train(T )\nℓ(fθM(G(x)), y),\nand α is a ﬁxed learning rate.\nOr if we choose Meta-SGD (Li et al., 2017) as the meta-\nlearner, then the parameter of M contains two parts θM =\n{φ, α} and M updates the initialization φ of the learner in\nthe following way:\nφ′ = φ −α ◦∇φLtrain(T )(φ, θG).\nThe loss would be computed on test(T ) as follows:\nLtest(T )(φ′, θG) =\n1\n|test(T )|\nX\n(x,y)∈test(T )\nℓ(fφ′(G(x)), y).\n3.4. Algorithm\nAfter introducing the framework, modules, and criterion\nof deep meta-learning, we are ready to describe a com-\nplete algorithm of DEML. As a running case, we take\nDeep Meta-Learning: Learning to Learn in the Concept Space\nMeta-SGD (Li et al., 2017) as our meta-learner. Recall that\nthe meta-learning pipeline and the concept discrimination\npipeline are trained synergistically to optimize a combined\nloss. In this case, our deep meta-learning can be formulated\nas the following optimization problem:\nmin\nθG,θM={φ,α},θD ET ∼p(T ),(x,y)∼D[Ltest(T )(φ′, θG)\n+ λ ℓ(D(G(x)), y)],\nwhere φ′ = φ −α ◦∇φLtrain(T )(φ, θG) and λ is a hyper-\nparameter balancing meta-learning and concept discrimina-\ntion.\nThe stochastic gradient descent (SGD) algorithm can be\napplied to optimize the above objective. In our implemen-\ntation, we use the Adam (Kingma & Ba, 2014) method, a\nvariant of SGD. The detailed procedures are summarized\nin Algorithm 2, which is an instantiation of Algorithm 1 by\nthe use of Meta-SGD as the meta-learner.\n3.5. Comparison with Related Work\nMeta-learning. Vanilla meta-learning is done in the in-\nstance space, which can be challenging for the problem\nof few-shot learning because a few examples can hardly\ncapture the high-level concept they represent (e.g. dog).\nIn contrast, our framework executes meta-learning in the\nconcept space thanks to the concept generator, where the\nproblem of few-shot learning is much easier.\nTransfer learning. A well-pretrained concept generator\n(e.g. neural network) could provide concept-level represen-\ntation for a meta-learner, but it may beneﬁt little if the novel\ntasks are quite different from those the concept generator\nis trained on. Also, ﬁne-tuning techniques may result in\nforgetting the concepts learned before. In our deep meta-\nlearning framework, we propose a principled way to train\nthis concept generator to enhance the model, as well as a\nsystematic way to use it to guide the few-shot learning tasks\nat hand in a learning-to-learn manner.\nLife-long learning.\nInterestingly, this new deep meta-\nlearning framework could be easily extended to a life-long\nlearning system (Silver et al., 2013), which retains knowl-\nedge learned previously and adapts to new tasks over a\nlifelong time. With the increase of labeled data for concept\ndiscrimination tasks, the concept generator could effectively\nretain learned concepts and provide more effective represen-\ntations for the meta-learner. As a consequence, performance\non new tasks will be improved gradually over time.\n4. Experiments\nIn this section, we evaluate the proposed deep meta-learning\n(DEML) on a number of few-shot image recognition prob-\nlems, but note that it is applicable to classiﬁcation, reinforce-\nment learning, and regression in general.\n4.1. Datasets\nTwo different pipelines in DEML process two different for-\nmats of data. For concept discrimination tasks, we perform\nexperiments on a subset of ImageNet (Deng et al., 2009).\nFor meta-learning tasks, we perform experiments on Mini-\nImagenet (Vinyals et al., 2016), Caltech-256 (Grifﬁn et al.,\n2007), CIFAR-100 (Krizhevsky, 2009), and CUB-200 (Wah\net al., 2011).\n4.1.1. DATASETS FOR CONCEPT DISCRIMINATION\nImageNet-200.\nImageNet(Deng et al., 2009) contains\n14,197,122 images of 1,000 classes, including person, vehi-\ncle, plant, and so on. The whole dataset is too large, and so\nin our experiments we use a subset ImageNet-200 with 200\nclasses sampled from 900 classes (excluding the 100 classes\nused in MiniImagenet(Vinyals et al., 2016)). For images in\neach selected category, 90% examples are randomly chosen\nfor training, and the remaining images are used for testing.\n4.1.2. DATASETS FOR META-LEARNING\nMiniImagenet.\nThe MiniImagenet dataset, ﬁrst used\nin (Vinyals et al., 2016), consists of 60,000 color images of\n100 classes, with 600 examples per class. For our experi-\nments, we use the splits introduced by (Ravi & Larochelle,\n2017). Their splits use a different set of 100 classes, which\nare divided into three disjoint subsets: 64 classes for meta-\ntraining, 16 classes for meta-validation, and 20 classes for\nmeta-testing. Particularly, MiniImagenet and ImageNet-200\nare mutually exclusive at class level.\nCaltech-256. The Caltech-256 dataset (Grifﬁn et al., 2007)\nis a successor to the well-known dataset Caltech-101. It\nconsists of 30,607 color images of 256 classes. We use 150,\n56, and 50 classes for meta-training, meta-validation, and\nmeta-testing, respectively.\nCIFAR-100. The CIFAR-100 dataset (Krizhevsky, 2009)\ncontains 60,000 32 × 32 color images of 100 classes. We\nuse 64, 16, and 20 classes for meta-training, meta-validation,\nand meta-testing, respectively. In CIFAR-100, images are\nrescaled to 32 × 32. Consequently, the difﬁculty in recog-\nnizing different categories is greatly increased.\nCaltech-UCSD Birds-200-2011 (CUB-200). The CUB-\n200 dataset (Wah et al., 2011) contains 11,788 color im-\nages of 200 different bird species. We use 140 classes for\nmeta-training, 20 classes for meta-validation, and test on\nthe remaining 40 classes. In this ﬁne-grained dataset, sub-\ntle differences between very similar classes can hardly be\nrecognized even by humans.\nDeep Meta-Learning: Learning to Learn in the Concept Space\nAlgorithm 2 Deep Meta-Learning with Meta-SGD\n1: Input: task distribution p(T ), labeled dataset D, batch size n of tasks, batch size m of instances, learning rate β\n2: Output:θG, θD, θM = {φ, α}\n3: Initialize θG, θD, φ, α\n4: while not done do\n5:\nSample n tasks Ti ∼p(T ) and m instances (xj, yj) ∼D\n6:\nfor each Ti do\n7:\nLtrain(Ti)(φ, θG) ←\n1\n|train(Ti)|\nP\n(x,y)∈train(Ti)\nℓ(fφ(G(x)), y);\n8:\nφ′\ni ←φ −α ◦∇φLtrain(Ti)(φ, θG);\n9:\nLtest(Ti)(φ′\ni, θG) ←\n1\n|test(Ti)|\nP\n(x,y)∈test(Ti)\nℓ(fφ′\ni(G(x)), y);\n10:\nend for\n11:\n(θG, θD, φ, α) ←(θG, θD, φ, α) −β∇\nh\n1\nn\nnP\ni=1\nLtest(Ti)(φ′\ni, θG) +λ 1\nm\nm\nP\nj=1\nℓ(D(G(xj)), yj)\ni\n;\n12: end while\n4.2. Baselines\nTo compare DEML with existing meta-learning methods,\nwe evaluate existing meta-learning methods (Matching\nNets (Vinyals et al., 2016), MAML (Finn et al., 2017),\nand Meta-SGD (Li et al., 2017)) on meta-learning datasets\nas our baselines. We follow their original neural network\ndesigns to reproduce their results at ﬁrst. To show that\nthe improvements of DEML are not solely attributed to the\ndeeper neural network and rescaled images, we also evaluate\nthe previous approaches with exactly the same architecture\n(excluding the concept discriminator) and inputs as DEML.\nAccordingly, their deep version implementations are de-\nnoted by Deep Matching Nets, Deep MAML, and Deep\nMeta-SGD, respectively. Since DEML is a meta-learner-\nagnostic framework for meta-learning, we re-implement\nMatching Nets, MAML and Meta-SGD on DEML with\nthe following implementation conﬁgurations.\n4.3. Implementation\nAccording to the analysis of (Canziani et al., 2016), we\nchoose ResNet-50 (He et al., 2016) (excluding the last layer)\nas our concept generator in the experiments. For the con-\ncept discriminator, we use a shallow neural network with\none fully connected layer. The learner for few-shot learn-\ning tasks depends on the meta-learner we choose. When\nchoosing Matching Nets as the meta-learner, the learner is a\nneural network with an input layer of size 2048, followed by\none hidden layer of size 1024 with ReLU nonlinearities, and\nthen an output layer of size 512. When choosing MAML\nor Meta-SGD as the meta-learner, the learner is a neural\nnetwork with the same input layer, followed by two hidden\nlayers of size 1024 and 512 with ReLU nonlinearities, and\nthen an output layer of size 5. The architecture of our deep\nmeta-learning is provided in Figure 21. After introducing\n1Images with different input sizes will be rescaled to 224x224.\nFigure 2: Model conﬁguration in DEML for few-shot image\nrecognition.\nthe architecture of DEML in our experiments, more experi-\nment design details for meta-training and meta-testing are\nprovided below.\nMeta-training. For each iteration, a batch of examples from\nImageNet-200 is sampled for the image recognition pipeline\nD ◦G. The prediction loss is measured by the mean of cross-\nentropy over all the examples in this batch. For the meta-\nlearning pipeline M ◦G, a batch of tasks is sampled from\none speciﬁc meta-learning dataset. Here, each task contains\n5 classes of examples, each with K ∈{1, 5} examples\nfor training and 5 examples for testing. The meta-learner\nwill generate one learner based on the training set of each\ntask, and then is evaluated on the testing set, as stated in\nSection 3.3. The prediction loss is also measured by the\nmean of cross-entropy over all the examples in the testing\nsets of tasks in the batch. We update the whole model\nonce each iteration ﬁnishes according to Algorithm 2. The\nparameter λ is set to 1.0 in our experiments at ﬁrst, and\nfurther studies on different values will also be discussed later.\nThe batch size of examples for image recognition is set to\n64 and the batch size of tasks is set to 4 and 2 for 1-shot and\n5-shot recognition, respectively. The number of iterations is\n60,000 in the experiments on MiniImagenet, Caltech-256,\nDeep Meta-Learning: Learning to Learn in the Concept Space\nTable 1: Comparison between deep meta-learning and vanilla meta-learning.\nMethod\nMiniImagenet\nCaltech-256\nCIFAR-100\nCUB-200\n5-way-1-shot\n5-way-5-shot\n5-way-1-shot\n5-way-5-shot\n5-way-1-shot\n5-way-5-shot\n5-way-1-shot\n5-way-5-shot\nMatching Nets\n43.56 ± 0.84\n55.31 ± 0.73\n48.09 ± 0.83\n57.45 ± 0.74\n50.53 ± 0.87\n60.30 ± 0.82\n56.53 ± 0.99\n63.54 ± 0.85\nDEML+Matching Nets\n55.84 ± 0.94\n59.88 ± 0.73\n52.97 ± 0.99\n59.42 ± 0.75\n58.18 ± 1.09\n63.12 ± 0.85\n63.47 ± 1.10\n64.86 ± 0.87\nMAML\n48.70 ± 1.84\n63.11 ± 0.92\n45.59 ± 0.77\n54.61 ± 0.73\n49.28 ± 0.90\n58.30 ± 0.80\n50.45 ± 0.97\n59.60 ± 0.84\nDEML+MAML\n53.71 ± 0.89\n68.13 ± 0.77\n56.81 ± 1.01\n70.54 ± 0.73\n56.65 ± 1.09\n68.66 ± 0.85\n64.63 ± 1.08\n66.75 ± 0.89\nMeta-SGD\n50.47 ± 1.87\n64.03 ± 0.94\n48.65 ± 0.82\n64.74 ± 0.75\n53.83 ± 0.89\n70.40 ± 0.74\n53.34 ± 0.97\n67.59 ± 0.82\nDEML+Meta-SGD\n58.49 ± 0.91\n71.28 ± 0.69\n62.25 ± 1.00\n79.52 ± 0.63\n61.62 ± 1.01\n77.94 ± 0.74\n66.95 ± 1.06\n77.11 ± 0.78\nand CIFAR-100, and 20,000 in the experiments on CUB-\n200.\nMeta-testing. For performance evaluation, we randomly\nsample 600 tasks from the corresponding meta-learning\ndataset. Each task contains 5 classes of examples, each\nwith K ∈{1, 5} examples for training and 15 examples for\ntesting. The results averaged over the sampled 600 tasks\nwith 95% conﬁdence intervals are reported at Section 4.4.\nFor both MAML and Meta-SGD, the meta-learner uses one-\nstep adaptation during meta-training and meta-testing for\nfair, and the learning rate α for MAML is set to 0.01 in all\nexperiments.\n4.4. Results and Discussion\nDEML version vs. vanilla version. The comparison re-\nsults between DEML versions and vanilla versions of Match-\ning Nets, MAML, and Meta-SGD are summarized in Table\n1.\nOur results clearly indicate that DEML versions outperform\nvanilla versions by a wide margin. The effective repre-\nsentations provided by the concept generator can ease the\nmeta-learning process for different kinds of meta-learners.\nOur framework lifts the meta-learning from the complicated\ninstance space to the high-level concept space and achieves\nhigh accuracy.\nDEML version vs. vanilla deep version. To validate that\nthe improvements of DEML are not merely because of the\ndeeper neural network and rescaled images, we also evaluate\nthe deep versions of the previous approaches on MiniIm-\nagenet as mentioned in Section 4.2. We enlarge the meta-\ntraining dataset by merging together the original 64 classes\nof MiniImagenet and the 200 classes of ImageNet-200. The\nresults are summarized in Table 2. It can be seen that sim-\nply enlarging the network and training dataset can not lead\nto a higher accuracy. DEML leverages the power of deep\nlearning in a more principled way and achieves superior\nperformance.\nDEML vs. transfer learning.\nTo compare deep meta-\nlearning with transfer learning, we also evaluate some vari-\nTable 2: Comparison between deep meta-learning and\nvanilla meta-learning (deep version).\nMethod\nMiniImagenet\n5-way-1-shot\n5-way-5-shot\nDeep Matching Nets\n48.82 ± 0.89\n55.22 ± 0.70\nDEML+Matching Nets\n55.84 ± 0.94\n59.88 ± 0.73\nDeep MAML\n51.74 ± 0.94\n57.24 ± 0.74\nDEML+MAML\n53.71 ± 0.89\n68.13 ± 0.77\nDeep Meta-SGD\n51.62 ± 0.95\n64.50 ± 0.74\nDEML+Meta-SGD\n58.49 ± 0.91\n71.28 ± 0.69\nants of DEML. One simple baseline is Decaf+kNN, where\nwe merely train our concept generator and discriminator\nD ◦G on ImageNet-200 with 60000 episodes. For each task\nat meta-testing time, we compute a centroid for each class\nby averaging the features of the training examples, produced\nby the concept generator G, and label each testing exam-\nple with its nearest (Euclidean distance) centroid’s category.\nAnother simpliﬁed version of DEML is Decaf+Meta-SGD,\nwhere one pretrained generator G is attached to the meta-\nlearner to execute meta-training process. At meta-training\ntime, we exclude the image recognition pipeline and ﬁx the\nparameters of the concept generator. The results are shown\nin Table 3.\nIt is interesting to note that the baseline Decaf+kNN\nachieves the best performance on MiniImagenet and\nCaltech-256. Since the concept generator G is trained on\nImageNet-200, which is quite similar to MiniImagenet and\nCaltech-256 (Tommasi et al., 2017), representations pro-\nvided by it are so effective that the naive nearest-neighbor\nbaseline achieves the best performance. This shows that\nthe effective representations are quite crucial for few-shot\nlearning and if we have some prior knowledge of the few-\nshot learning tasks, we can incorporate it into the concept\ngenerator by choosing particular concept recognition dataset\nto ease the meta-learning process. On the contrary, the per-\nformance of Decaf+kNN drops a lot on CIFAR-100 and\nDeep Meta-Learning: Learning to Learn in the Concept Space\nTable 3: Comparison between deep meta-learning and transfer learning.\nMethod\nMiniImagenet\nCaltech-256\nCIFAR-100\nCUB-200\n5-way-1-shot\n5-way-5-shot\n5-way-1-shot\n5-way-5-shot\n5-way-1-shot\n5-way-5-shot\n5-way-1-shot\n5-way-5-shot\nDecaf+kNN\n61.81 ± 0.84\n79.88 ± 0.58\n63.08 ± 0.89\n80.70 ± 0.61\n47.04 ± 0.80\n65.96 ± 0.73\n45.58 ± 0.78\n65.57 ± 0.70\nDecaf+Meta-SGD\n58.06 ± 0.86\n71.30 ± 0.70\n60.47 ± 0.92\n74.91 ± 0.70\n54.10 ± 0.98\n68.30 ± 0.73\n56.85 ± 0.98\n66.29 ± 0.78\nDEML+Meta-SGD\n58.49 ± 0.91\n71.28 ± 0.69\n62.25 ± 1.00\n79.52 ± 0.63\n61.62 ± 1.01\n77.94 ± 0.74\n66.95 ± 1.06\n77.11 ± 0.78\nCUB-200, since these two datasets are quite different from\nImageNet-200. When the meta-learning dataset is quite\ndifferent from the dataset where the concept generator is\ntrained on, the meta-learner beneﬁts little by directly adding\nthis generator to it. In the joint learning process of DEML,\nthe concept generator extracts the task-agnostic meta-level\nconcepts of the data, as well as the external concepts. Com-\nbining the two sources of knowledge, the concept generator\nprovides effective representations for the meta-learner to do\nfew-shot learning.\nTo emphasize the necessity of our joint learning process, we\npropose the third version Decaf+Fine-Tune+Meta-SGD\nwhich is the same as Decaf+Meta-SGD except that the con-\ncept generator and the meta-learner are trained together\nduring meta-training process. The models are trained for\n60,000 and 20,000 iterations on CIFAR-100 and CUB-200,\nrespectively, and the results are shown in Figure 3, together\nwith the results of Decaf+Meta-SGD and DEML+Meta-\nSGD. DEML+Meta-SGD performs consistently better than\nDecaf+Meta-SGD and Decaf+Fine-Tune+Meta-SGD on all\ncases by a wide margin. Our joint learning process can bal-\nance the learning from a large number of related few-shot\nlearning tasks and the learning from external large-scale\ndataset. The concept generator being enhanced by the con-\ncept recognition pipeline as the meta-learning proceeds may\nlead to a higher generalization capability. The meta-learner\nand the concept generator evolve synergistically in this joint\nlearning process.\nFigure\n3:\nComparison\namong\nDecaf+Meta-SGD,\nDecaf+Fine-Tune+Meta-SGD, and DEML+Meta-SGD on\nCIFAR-100 (left) and CUB-200 (right).\nStudy of λ. In previous experiments, the hyperparameter λ\nis set to 1.0 as default. For different datasets, one intuition is\nthat we should incorporate external concepts at different lev-\nels. We verify it on CIFAR-100 with the 5-way-5-shot case\nwith DEML+Meta-SGD (Figure 4). It is obvious that as the\nvalue of λ increases, the accuracy of concept recognition\nincreases accordingly. However, the accuracy of few-shot\nlearning tasks increases ﬁrst and then decreases. This result\nshows that the meta-learner does beneﬁt from the concept\ngenerator enhanced by the external data, but placing too\nmuch emphasis on the external data can harm the perfor-\nmance of meta-learner. A balance between the external\nknowledge and the internal meta-level knowledge is useful\nin DEML.\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n 90\n0\n0.1\n1\n10\n100\nAccuracy(%)\nλ\nFew-shot accuracy \nClassification accuracy\nFigure 4: Few-shot (5-way-5-shot) learning accuracy and\nconcept recognition accuracy of DEML+Meta-SGD on\nCIFAR-100 with different values of λ.\n5. Conclusion and Future Work\nIn this paper, we propose deep meta-learning that integrates\nthe representation power of deep learning into meta-learning,\nand enables learning to learn in the concept space. A con-\ncept generator that can provide effective representations\nfor the meta-learner is trained on large-scale concept dis-\ncrimination and few-shot learning, simultaneously. This\nhigh-capacity generator captures the concept information of\nexamples from the external large-scale dataset as well as the\nmeta-level concepts of data from a large number of related\nfew-shot learning tasks, which lifts the meta-learning from\nthe raw instance space to the high-level concept space and\neases the meta-learning process. The joint learning pro-\ncess in this new framework allows the meta-learner and the\nconcept generator evolve synergistically, which leads to a\nhigher generalization capability. Extensive experiments on\nfew-shot image recognition show that this new framework\nimproves the vanilla meta-learning greatly.\nIn our experiments, we train the concept generator together\nwith the concept discriminator on a single dataset with 200\nclasses. It would be interesting to train this concept genera-\nDeep Meta-Learning: Learning to Learn in the Concept Space\ntor on multiple large-scale datasets with a large number of\ncategories, which can incorporate more external concepts\ninto the model. This requires more careful design for the\nconcept generator and more computing resources. We leave\nit for future work. Another future work is to implement\na life-long learning system that can evolve with new ex-\namples and new concepts. Indeed, our algorithm enables\nlife-long learning. The concept generator can be enhanced\nwith more training examples coming. A balance between\nthe concept learning from external datasets and from the\nfew-shot learning tasks at hand is crucial in this life-long\nlearning scenario. Forgetting problem is another concern.\nLearning new concepts should not result in the generator\nforgetting previously learned concepts. More studies are\nexpected to explore this problem.\nReferences\nAhn, Woo-Kyoung and Brewer, William F. Psychological studies\nof explanation—based learning. In Investigating explanation-\nbased learning, pp. 295–316. Springer, 1993.\nArgyriou, Andreas, Evgeniou, Theodoros, and Pontil, Massim-\niliano. Multi-task feature learning. In Advances in neural\ninformation processing systems, pp. 41–48, 2007.\nBertinetto, Luca, Henriques, João F., Valmadre, Jack, Torr, Philip\nH. S., and Vedaldi, Andrea. Learning feed-forward one-shot\nlearners. CoRR, abs/1606.05233, 2016.\nCanziani, Alfredo, Paszke, Adam, and Culurciello, Eugenio. An\nanalysis of deep neural network models for practical applica-\ntions. arXiv preprint arXiv:1605.07678, 2016.\nCaruana, Rich. Multitask learning. In Learning to Learn. Springer,\n1998.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR09, 2009.\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy,\nZhang, Ning, Tzeng, Eric, and Darrell, Trevor. DeCAF: A deep\nconvolutional activation feature for generic visual recognition.\nIn ICML, volume 32, 2014.\nDuan, Yan, Schulman, John, Chen, Xi, Bartlett, Peter L, Sutskever,\nIlya, and Abbeel, Pieter. RL2: Fast reinforcement learning via\nslow reinforcement learning. arXiv preprint arXiv:1611.02779,\n2016.\nEvgeniou, Theodoros, Micchelli, Charles A., and Pontil, Massim-\niliano. Learning multiple tasks with kernel methods. J. Mach.\nLearn. Res., 6:615–637, December 2005.\nFink, Michael. Object classiﬁcation from a single example utilizing\nclass relevance metrics. In Advances in neural information\nprocessing systems, pp. 449–456, 2005.\nFinn, Chelsea, Abbeel, Pieter, and Levine, Sergey. Model-agnostic\nmeta-learning for fast adaptation of deep networks.\narXiv\npreprint arXiv:1703.03400, 2017.\nGirshick, Ross. Fast r-cnn. arXiv preprint arXiv:1504.08083,\n2015.\nGrifﬁn, Gregory, Holub, Alex, and Perona, Pietro. Caltech-256\nobject category dataset. 2007.\nGuillaumin, M., Verbeek, J., and Schmid, C. Is that you? metric\nlearning approaches for face identiﬁcation. In 2009 IEEE 12th\nInternational Conference on Computer Vision, pp. 498–505,\nSept 2009.\nHariharan, Bharath and Girshick, Ross B. Low-shot visual object\nrecognition. CoRR, abs/1606.02819, 2016.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.\nDeep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recogni-\ntion, pp. 770–778, 2016.\nJi, Shuiwang and Ye, Jieping. An accelerated gradient method for\ntrace norm minimization. ICML ’09, pp. 457–464, 2009.\nKaiser, Łukasz, Nachum, Oﬁr, Roy, Aurko, and Bengio,\nSamy.\nLearning to remember rare events.\narXiv preprint\narXiv:1703.03129, 2017.\nKevin Frans, Jonathan Ho, Xi Chen Pieter Abbeel John Schul-\nman. Meta learning shared hierarchies. International Con-\nference on Learning Representations, 2018.\nURL https:\n//openreview.net/forum?id=SyX0IeWAW.\nKingma, Diederik and Ba, Jimmy. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKoch, Gregory.\nSiamese neural networks for one-shot image\nrecognition. PhD thesis, University of Toronto, 2015.\nKrizhevsky, Alex. Learning multiple layers of features from tiny\nimages. 2009.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Ima-\ngenet classiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pp. 1097–\n1105, 2012.\nLake, Brenden M., Salakhutdinov, Ruslan, Gross, Jason, and\nTenenbaum, Joshua B. One shot learning of simple visual\nconcepts. In CogSci, 2011.\nLake, Brenden M, Salakhutdinov, Ruslan, and Tenenbaum,\nJoshua B. Human-level concept learning through probabilistic\nprogram induction. Science, 350(6266), 2015.\nLi, Fei-Fei, Fergus, Robert, and Perona, Pietro. One-shot learning\nof object categories. IEEE Trans. Pattern Anal. Mach. Intell.,\n28(4):594–611, 2006.\nLi, Zhenguo, Zhou, Fengwei, Chen, Fei, and Li, Hang. Meta-\nSGD: Learning to Learn Quickly for Few Shot Learning. arXiv\npreprint arXiv:1707.09835, 2017.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen Pieter Abbeel.\nA simple neural attentive meta-learner.\nInternational Con-\nference on Learning Representations, 2018.\nURL https:\n//openreview.net/forum?id=B1DmUzWAW.\nPan, Sinno Jialin and Yang, Qiang. A survey on transfer learning.\nIEEE Transactions on knowledge and data engineering, 22(10):\n1345–1359, 2010.\nPark, Eunbyung and Berg, Alexander C. Meta-tracker: Fast and ro-\nbust online adaptation for visual object trackers. arXiv preprint\narXiv:1801.03049, 2018.\nRavi, Sachin and Larochelle, Hugo. Optimization as a model for\nfew-shot learning. In ICLR, 2017.\nRazavian, Ali Sharif, Azizpour, Hossein, Sullivan, Josephine, and\nDeep Meta-Learning: Learning to Learn in the Concept Space\nCarlsson, Stefan. Cnn features off-the-shelf: an astounding base-\nline for recognition. In Computer Vision and Pattern Recog-\nnition Workshops (CVPRW), 2014 IEEE Conference on, pp.\n512–519. IEEE, 2014.\nSalakhutdinov, Ruslan, Tenenbaum, Joshua, and Torralba, Antonio.\nOne-shot learning with a hierarchical nonparametric bayesian\nmodel. In Proceedings of ICML Workshop on Unsupervised\nand Transfer Learning, pp. 195–206, 2012.\nSantoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra,\nDaan, and Lillicrap, Timothy. Meta-learning with memory-\naugmented neural networks. In ICML, 2016.\nSchmidhuber, Jurgen. Evolutionary principles in self-referential\nlearning. On learning how to learn: The meta-meta-hook.)\nDiploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.\nSchroff, Florian, Kalenichenko, Dmitry, and Philbin, James.\nFacenet: A uniﬁed embedding for face recognition and clus-\ntering. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 815–823, 2015.\nSilver, Daniel L, Yang, Qiang, and Li, Lianghao. Lifelong machine\nlearning systems: Beyond learning algorithms. In AAAI Spring\nSymposium: Lifelong Machine Learning, volume 13, pp. 05,\n2013.\nSimonyan, Karen and Zisserman, Andrew. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\nSnell, Jake, Swersky, Kevin, and Zemel, Richard. Prototypical net-\nworks for few-shot learning. In Advances in Neural Information\nProcessing Systems, pp. 4080–4090, 2017.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre,\nReed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke,\nVincent, Rabinovich, Andrew, et al. Going deeper with convo-\nlutions. Cvpr, 2015.\nTommasi, Tatiana, Patricia, Novi, Caputo, Barbara, and Tuytelaars,\nTinne. A deeper look at dataset bias. In Domain Adaptation in\nComputer Vision Applications, pp. 37–55. Springer, 2017.\nVinyals, Oriol, Blundell, Charles, Lillicrap, Tim, and Wierstra,\nDaan. Matching networks for one shot learning. In NIPS, 2016.\nWah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S.\nThe Caltech-UCSD Birds-200-2011 Dataset. Technical report,\n2011.\nWang, Jane X, Kurth-Nelson, Zeb, Tirumala, Dhruva, Soyer, Hu-\nbert, Leibo, Joel Z, Munos, Remi, Blundell, Charles, Kumaran,\nDharshan, and Botvinick, Matt. Learning to reinforcement learn.\narXiv preprint arXiv:1611.05763, 2016.\nWong, A. and Yuille, A. One shot learning via compositions of\nmeaningful patches. In 2015 IEEE International Conference on\nComputer Vision (ICCV), pp. 1197–1205, 2015.\nXu, Zhongwen, Zhu, Linchao, and Yang, Yi.\nFew-shot ob-\nject recognition from machine-labeled web images. In The\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), July 2017.\nYosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod.\nHow transferable are features in deep neural networks? In NIPS,\n2014.\nZeiler, Matthew D and Fergus, Rob. Visualizing and understanding\nconvolutional networks. In European conference on computer\nvision, pp. 818–833. Springer, 2014.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2018-02-10",
  "updated": "2018-02-10"
}