{
  "id": "http://arxiv.org/abs/1802.05313v2",
  "title": "Reinforcement Learning from Imperfect Demonstrations",
  "authors": [
    "Yang Gao",
    "Huazhe Xu",
    "Ji Lin",
    "Fisher Yu",
    "Sergey Levine",
    "Trevor Darrell"
  ],
  "abstract": "Robust real-world learning should benefit from both demonstrations and\ninteractions with the environment. Current approaches to learning from\ndemonstration and reward perform supervised learning on expert demonstration\ndata and use reinforcement learning to further improve performance based on the\nreward received from the environment. These tasks have divergent losses which\nare difficult to jointly optimize and such methods can be very sensitive to\nnoisy demonstrations. We propose a unified reinforcement learning algorithm,\nNormalized Actor-Critic (NAC), that effectively normalizes the Q-function,\nreducing the Q-values of actions unseen in the demonstration data. NAC learns\nan initial policy network from demonstrations and refines the policy in the\nenvironment, surpassing the demonstrator's performance. Crucially, both\nlearning from demonstration and interactive refinement use the same objective,\nunlike prior approaches that combine distinct supervised and reinforcement\nlosses. This makes NAC robust to suboptimal demonstration data since the method\nis not forced to mimic all of the examples in the dataset. We show that our\nunified reinforcement learning algorithm can learn robustly and outperform\nexisting baselines when evaluated on several realistic driving games.",
  "text": "Reinforcement Learning from Imperfect Demonstrations\nYang Gao∗1 Huazhe(Harry) Xu∗1 Ji Lin 2 Fisher Yu 1 Sergey Levine 1 Trevor Darrell 1\nAbstract\nRobust real-world learning should beneﬁt from\nboth demonstrations and interactions with the en-\nvironment. Current approaches to learning from\ndemonstration and reward perform supervised\nlearning on expert demonstration data and use\nreinforcement learning to further improve perfor-\nmance based on the reward received from the\nenvironment. These tasks have divergent losses\nwhich are difﬁcult to jointly optimize and such\nmethods can be very sensitive to noisy demonstra-\ntions. We propose a uniﬁed reinforcement learn-\ning algorithm, Normalized Actor-Critic (NAC),\nthat effectively normalizes the Q-function, reduc-\ning the Q-values of actions unseen in the demon-\nstration data. NAC learns an initial policy network\nfrom demonstrations and reﬁnes the policy in the\nenvironment, surpassing the demonstrator’s per-\nformance. Crucially, both learning from demon-\nstration and interactive reﬁnement use the same\nobjective, unlike prior approaches that combine\ndistinct supervised and reinforcement losses. This\nmakes NAC robust to suboptimal demonstration\ndata, since the method is not forced to mimic all\nof the examples in the dataset. We show that\nour uniﬁed reinforcement learning algorithm can\nlearn robustly and outperform existing baselines\nwhen evaluated on several realistic driving games.\n1. Introduction\nDeep reinforcement learning (RL) has achieved signiﬁcant\nsuccess on many complex sequential decision-making prob-\nlems.\nHowever, RL algorithms usually require a large\namount of interactions with an environment to reach good\nperformance (Kakade et al., 2003); initial performance may\n*Equal contribution\n1Department of Electrical Engineering\nand Computer Science, UC Berkeley, CA, USA 2Department of\nElectrical Engineering, Tsinghua University, Beijing, China. Cor-\nrespondence to: Yang Gao <yg@eecs.berkeley.edu>, Huazhe Xu\n<huazhe xu@eecs.berkeley.edu>.\nProceedings of the 35 th International Conference on Machine\nLearning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018\nby the author(s).\nbe nearly random, clearly suboptimal, and often rather dan-\ngerous in real-world settings such as autonomous driving.\nLearning from demonstration is a well-known alternative,\nbut typically does not leverage reward, and presumes rela-\ntively small-scale noise-free demonstrations. We develop a\nnew robust algorithm that can learn value and policy func-\ntions from state, action and reward (s, a, r) signals that\neither come from imperfect demonstration data or the envi-\nronment.\nRecent efforts toward policy learning which does not suffer\nfrom a suboptimal initial performance generally leverage\nan initial phase of supervised learning and/or auxiliary task\nlearning. Several previous efforts have shown that demon-\nstrations can speed up RL by mimicking expert data with a\ntemporal difference regularizer (Hester et al., 2017) or via\ngradient-free optimization (Ebrahimi et al., 2017), yet these\nmethods presume near-optimal demonstrations. (Jaderberg\net al., 2016) and (Shelhamer et al., 2016) obtained better\ninitialization via auxiliary task losses (e.g., predicting en-\nvironment dynamics) in a self-supervised manner; policy\nperformance is still initially random with these approaches.\nA simple combination of several distinct losses can learn\nfrom demonstrations; however, it is more appealing to have a\nsingle principled loss that is applicable to learning both from\nthe demonstration and from the environment. Our approach,\nNormalized Actor-Critic (NAC), uses a uniﬁed loss func-\ntion to process both off-line demonstration data and on-line\nexperience based on the underlying maximum entropy rein-\nforcement learning framework (Toussaint, 2009; Haarnoja\net al., 2017b; Schulman et al., 2017). Our approach enables\nrobust learning from corrupted (or even partially adversarial)\ndemonstrations that contains (s, a, r), because no assump-\ntion on the optimality of the data is required. A normalized\nformulation of the soft Q-learning gradient enables the NAC\nmethod, which can also be regarded as a variant of the policy\ngradient.\nWe evaluate our approach in a toy Minecraft Game, as well\nas two realistic 3D simulated environments, Torcs and Grand\nTheft Auto V (GTA V), with either discrete states and tab-\nular Q functions, or raw image input and Q functions ap-\nproximated by neural networks. Our experimental results\noutperform previous approaches on driving tasks with only\na modest amount of demonstrations while tolerating signif-\nicant noise in the demonstrations, as our method utilizes\narXiv:1802.05313v2  [cs.AI]  30 May 2019\nReinforcement Learning from Imperfect Demonstrations\nrewards rather than simply imitates demonstrated behaviors.\nWe summarize the contributions in this paper as follows.\n• We propose the NAC method for learning from demon-\nstrations and discover its practical advantages on a\nvariety of environments.\n• To the best of our knowledge, we are the ﬁrst to pro-\npose a uniﬁed objective, capable of learning from both\ndemonstrations and environments, that outperforms\nmethods including the ones with an explicit supervised\nimitation loss.\n• Unlike other methods that utilize supervised learning\nto learn from demonstrations, our pure reinforcement\nlearning method is robust to noisy demonstrations.\n2. Preliminaries\nIn this section, we will brieﬂy review the reinforcement\nlearning techniques that our method is built on, including\nmaximum entropy reinforcement learning and the soft Q-\nlearning.\n2.1. Maximum Entropy Reinforcement Learning\nThe reinforcement learning problem we consider is deﬁned\nby a Markov decision process(MDP) (Thie, 1983). Speciﬁ-\ncally, the MDP is characterized by a tuple < S,A,R,T,γ >,\nwhere S is the set of states, A is the set of actions, R(s, a)\nis the reward function, T(s, a, s′) = P(s′|s, a) is the transi-\ntion function and γ is the reward discount factor. An agent\ninteracts with the environment by taking an action at a given\nstate, receiving the reward, and transiting to the next state.\nIn the standard reinforcement learning setting (Sutton &\nBarto, 1998), the goal of an agent is to learn a policy πstd,\nsuch that an agent maximizes the future discounted reward:\nπstd = argmax\nπ\nX\nt\nγt\nE\nst,at∼π[Rt]\n(1)\nMaximum entropy policy learning (Ziebart, 2010; Haarnoja\net al., 2017b) uses an entropy augmented reward. The op-\ntimal policy will not only optimize for discounted future\nrewards, but also maximize the discounted future entropy\nof the action distribution:\nπent = argmax\nπ\nX\nt\nγt\nE\nst,at∼π[Rt + αH(π(·|st))]\n(2)\nwhere α is a weighting term to balance the importance of the\nentropy. Unlike previous attempts that only adds the entropy\nterm at a single time step, maximum entropy policy learning\nmaximizes the discounted future entropy over the whole tra-\njectory. Maximum entropy reinforcement learning has many\nbeneﬁts, such as better exploration in multi-modal problems\nand connections between Q-learning and the actor-critic\nmethod (Haarnoja et al., 2017b; Schulman et al., 2017).\n2.2. Soft Value Functions\nSince the maximum entropy RL paradigm augments the\nreward with an entropy term, the deﬁnition of the value\nfunctions naturally changes to\nQπ(s, a) = R0 +\nE\n(st,at)∼π\n∞\nX\nt=1\nγt(Rt + αH(π(·|st)))\n(3)\nVπ(s) =\nE\n(st,at)∼π\n∞\nX\nt=0\nγt(Rt + αH(π(·|st)))\n(4)\nwhere π is some policy that value functions evaluate on.\nGiven the state-action value function Q∗(s, a) of the optimal\npolicy, (Ziebart, 2010) shows that the optimal state value\nfunction and the optimal policy could be expressed as:\nV ∗(s) = α log\nX\na\nexp(Q∗(s, a)/α)\n(5)\nπ∗(a|s) = exp((Q∗(s, a) −V ∗(s))/α)\n(6)\n2.3. Soft Q-Learning and Policy Gradient\nWith the entropy augmented reward, one can derive the soft\nversions of Q-learning (Haarnoja et al., 2017a) and policy\ngradient. The soft Q-learning gradient is given by\n∇θQθ(s, a)(Qθ(s, a) −ˆQ(s, a))\n(7)\nwhere ˆQ(s, a) is a bootstrapped Q-value estimate obtained\nby R(s, a) + γVQ(s′). Here, R(s, a) is the reward received\nfrom the environment, VQ is computed from Qθ(s, a) with\nEquation (5). We can also derive a policy gradient, which\nincludes the gradient of form:\nE\n\" ∞\nX\nt=0\n∇θ log πθ( ˆQπ −b(st)) + α∇θH(πθ(·|st))\n#\n(8)\nwhere b(st) is some arbitrary baseline (Schulman et al.,\n2017).\n3. Robust Learning from Demonstration and\nReward\nGiven a set of demonstrations that contains (s, a, r, s′) and\nthe corresponding environment, an agent should perform ap-\npropriate actions when it starts the interaction, and continue\nto improve. Although a number of off-policy RL algorithms\ncould in principle be used to learn directly from off-policy\nReinforcement Learning from Imperfect Demonstrations\nAlgorithm 1 Normalized Actor-Critic for Learning from\nDemonstration\nθ: parameters for the rapid Q network, θ′: param-\neters for the target Q network, D: demonstrations\ncollected by human or a trained policy network, T:\ntarget network update frequency, M: replay buffer,\nk: number of steps to train on the demonstrations\nfor step t ∈{1, 2, ...} do\nif t ≤k then\nSample a mini-batch of transitions from D\nelse\nStart from s, sample a from π, execute a, observe\n(s′,r) and store (s, a, r, s′) in M\nSample a mini-batch of transitions from M\nend if\nUpdate θ with gradient: ∇θJP G + ∇θJV\nif t mod T = 0 then\nθ′ ←θ\nend if\nend for\ndemonstration data, standard methods can suffer from ex-\ntremely poor performance when trained entirely on demon-\nstration data. This can happen when the demonstration set\nis a strongly biased sample of the environment transitions,\nwhich violates the assumptions of many off-policy RL meth-\nods. Although they are closely related, off-policy learning\nand learning from demonstrations are different problems.\nIn section 5, we show that Q-learning completely fails on\nthe demonstration data. The intuition behind this problem\nis that if the Q-function is trained only on good data, it has\nno way to understand why the action taken is appropriate: it\nwill assign a high Q-value, but will not necessarily assign a\nlow Q-value to other alternative actions.\nThe framework of soft optimality provides us with a nat-\nural mechanism to mitigate this problem by normalizing\nthe Q-function over the actions. Our approach, Normalized\nActor-Critic (NAC), utilizes soft policy gradient formula-\ntions described in Section 2.3 to obtain a Q-function gradient\nthat reduces the Q-values of actions that were not observed\nalong the demonstrations. In other words, without data to\nindicate otherwise, NAC will opt to follow the demonstra-\ntions. This method is a well-deﬁned RL algorithm without\nany auxiliary supervised loss and hence, it is able to learn\nwithout bias in the face of low-quality demonstration data.\nWe will ﬁrst describe our algorithm, and then discuss why it\nperforms well when trained on the demonstrations.\n3.1. Normalized Actor-Critic for Learning from\nDemonstration\nWe propose a uniﬁed learning from demonstration approach,\nwhich applies the normalized actor-critic updates to both\noff policy demonstrations and in-environment transitions.\nThe NAC method is derived from the soft policy gradient\nobjective with a Q function parametrization. Speciﬁcally,\nwe take gradient steps to maximize the future reward ob-\njective (Equation (2)), and parametrize π and V in terms of\nQ (Equation (6) and (5)). As derived in the appendix, the\nupdates for the actor and critic are:\n∇θJP G =\nE\ns,a∼πQ\nh\n(∇θQ(s, a) −∇θVQ(s))(Q(s, a) −ˆQ)\ni\n(9)\n∇θJV = E\ns\n\u0014\n∇θ\n1\n2(VQ(s) −ˆV (s))2\n\u0015\n(10)\nwhere VQ and πQ are deterministic functions of Q:\nVQ(s) = α log\nX\na\nexp(Q(s, a)/α)\n(11)\nπQ(a|s) = exp((Q(s, a) −VQ(s))/α)\n(12)\nˆQ(s, a), ˆV (s) are obtained by:\nˆQ(s, a) = R(s, a) + γVQ(s′)\n(13)\nˆV (s) =\nE\na∼πQ [R(s, a) + γVQ(s′)] + αH(πQ(·|s)) (14)\nThe difference is the ∇θV (s) term comparing NAC’s actor\nupdate term (Equation (9)) with the soft Q update (Equation\n(7)). We emphasize the normalization effect of this term:\nit avoids pushing up the Q-values of actions that are not\ndemonstrated. The mechanism is explained in Section 3.2.\nThe expectations in Eq. (9) and Eq. (10) are taken with\nrespect to πQ. In the demonstration set, we only have tran-\nsition samples from the behavioral policy µ(a|s). To have a\nproper policy gradient algorithm, we can employ importance\nsampling to correct the mismatch. To be speciﬁc, when esti-\nmating E(s,a)∼πQ [f(s, a)], we estimate E(s,a)∼µ [f(s, a)β]\n, where β = min\nn\nπQ(a|s)\nµ(a|s) , c\no\nand c is some constant that\nprevents the importance ratio from being too large. Al-\nthough the importance weights are needed to formalize our\nmethod as a proper policy gradient algorithm, we ﬁnd in\nour empirical evaluation that the inclusion of these weights\nconsistently reduces the performance of our method. We\nfound that omitting the weights results in better ﬁnal perfor-\nmance even when training entirely on demonstration data.\nFor this reason, our ﬁnal algorithm does not use importance\nsampling.\nWe summarize the proposed method in Algorithm 1. Our\nmethod uses samples from the demonstrations and the replay\nbuffer, rather than restricting the samples to be on policy\nas in standard actor-critic methods. Similar to DQN, we\nutilize a target network to compute ˆQ(s, a) and ˆV (s), which\nstabilizes the training process.\nReinforcement Learning from Imperfect Demonstrations\n3.2. Analysis of the Method\nWe provide an intuitive analysis in this section to explain\nwhy our method can learn from demonstrations while other\nreinforcement learning methods, such as Q-learning cannot.\nThe states and actions in the demonstrations generally have\nhigher Q-values than other states. Q-learning will push up\nQ(s, a) values in a sampled state s. However, if the values\nfor the bad actions are not observed, the Q-function has no\nway of knowing whether the action itself is good, or whether\nall actions in that state are good, so the demonstrated action\nwill not necessarily have a higher Q-value than other actions\nin the demonstrated state.\nComparing the actor update (Eq. (9)) of our method with\nthe soft Q-learning (Eq. (7)) update, our method includes an\nextra term in the gradient: −∇θVQ(s). This term falls out\nnaturally when we derive the update from a policy gradient\nalgorithm, rather than a Q-learning algorithm. Intuitively,\nthis term will decrease VQ(s) when increasing Q(s, a) and\nvice versa, since ∇θQ(s, a) and −∇θVQ(s) have different\nsigns. Because VQ(s) = α log P\na exp(Q(s, a)/α), de-\ncreasing VQ(s) will prevent Q(s, a) from increasing for the\nactions that are not in the demonstrations. That is why the\naforementioned normalization effect emerges with the extra\n∇θVQ(s) term.\nBesides having the normalizing behaviors, NAC is also less\nsensitive to noisy demonstrations. Since NAC is an RL\nalgorithm, it is naturally resistant to poor behaviors. One\ncould also see this with similar analysis as above. When\nthere is a negative reward in the demonstrations, Q(s, a)\ntends to decrease and VQ(s) tends to increase, hence having\nthe normalizing behavior in a reverse direction.\nMoreover, NAC provides a single and principled approach\nto learn from both demonstrations and environments. It\navoids the use of imitation learning. Therefore, besides its\nnatural robustness to imperfect demonstrations, it is a more\nuniﬁed approach comparing with other methods.\n4. Related Work\n4.1. Maximum entropy Reinforcement Learning\nMaximum entropy reinforcement learning has been explored\nin a number of prior works (Todorov, 2008; Toussaint, 2009;\nZiebart et al., 2008), including several recent works that ex-\ntend it into a deep reinforcement learning setting (Nachum\net al., 2017; Haarnoja et al., 2017b; Schulman et al., 2017).\nHowever, most of those works do not deal with the learn-\ning from demonstration settings. (Haarnoja et al., 2017b)\nand (Schulman et al., 2017) propose maximum entropy RL\nmethods to learn from environments. PCL (Nachum et al.,\n2017) is the only prior work that studies the learning from\ndemonstration task with the maximum entropy RL frame-\nFigure 1. Sample frames from Torcs (upper) and GTA (lower).\nFigure 2. The Toy Minecraft environment. We aim to learn a policy\nthat moves the agent to the goal. Two possible paths are shown,\nthe shorter optimal one (solid, brown) and the longer sub-optimal\none (dashed, blue). See text for details about the environment (Sec.\n5.1) and comparison between NAC and DQfD on this environment\n(Sec. 5.3).\nwork. Unlike their method where the loss is derived from\nan objective similar to Bellman error, our method is derived\nfrom policy gradient. Instead of minimizing Bellman er-\nrors, policy gradient directly optimizes future accumulated\nreward. As shown in Section 5, our method has large perfor-\nmance advantage compared with PCL, due to the different\nobjective function.\nOur method not only admits a uniﬁed objective on both\ndemonstrations and environments but also performs better\nthan alternative methods, such as PCL (Nachum et al., 2017)\nand DQfD (Hester et al., 2017). To the best of our knowl-\nedge, our proposed method is the ﬁrst uniﬁed method across\ndemonstrations and environments that outperforms methods\nincluding the ones with explicit supervised imitation loss\nsuch as DQfD.\nReinforcement Learning from Imperfect Demonstrations\n4.2. Learning from Demonstration\nMost of the prior learning from demonstration efforts as-\nsume the demonstrations are perfect, i.e. the ultimate goal\nis to copy the behaviors from the demonstrations. Imitation\nlearning is one of such approaches, examples including (Xu\net al., 2016; Bojarski et al., 2016). Extensions such as DAG-\nGER (Ross et al., 2011) are proposed to have the expert in\nthe loop, which further improves the agent’s performance.\nRecently, (Ho & Ermon, 2016; Wang et al., 2017; Ziebart\net al., 2008) explore an adversarial paradigm for the behav-\nior cloning method. Another popular paradigm is Inverse\nReinforcement Learning (IRL) (Ng et al., 2000; Abbeel &\nNg, 2004; Ziebart et al., 2008). IRL learns a reward model\nwhich explains the demonstrations as optimal behavior.\nInstead of assuming that the demonstrations are perfect,\nour pure RL method allows imperfect demonstrations. Our\nmethod learns which part of the demonstrations is good\nand which part is bad, unlike the methods that simply imi-\ntate the demonstrated behaviors. We follow the Reinforce-\nment Learning with Expert Demonstrations (RLED) frame-\nwork (Chemali & Lazaric, 2015; Kim et al., 2013; Piot\net al., 2014), where both rewards and actions are available\nin the demonstrations. The extra reward in the demonstra-\ntions allows our method to be aware of poor behaviors in\nthe demonstrations. DQfD (Hester et al., 2017) is a recent\nmethod that also uses rewards in the demonstrations. It\ncombines an imitation hinge loss with the Q-learning loss in\norder to learn from demonstrations and transfer to environ-\nments smoothly. Due to the use of the imitation loss, DQfD\nis more sensitive to noisy demonstrations, as we show in the\nexperiment section.\n4.3. Off-policy Learning\nIt is tempting to apply various off-policy methods to the\nproblem of learning from demonstration, such as policy gra-\ndient variants (Gu et al., 2017; 2016; Degris et al., 2012;\nWang et al., 2016), Q-learning (Watkins & Dayan, 1992)\nand Retrace (Munos et al., 2016). However, we emphasize\nthat off-policy learning and learning from demonstration are\ndifferent problems. For most of the off-policy methods, their\nconvergence relies on the assumption of visiting each (s, a)\npair inﬁnitely many times. In the learning from demonstra-\ntion setting, the samples are highly biased and off-policy\nmethod can fail to learn anything from the demonstrations,\nas we explained the Q-learning case in Section 3.\n5. Results\nOur experiments address several questions: (1) Can NAC\nbeneﬁt from both demonstrations and rewards? (2) Is NAC\nrobust to ill-behaved demonstrations? (3) Can NAC learn\nmeaningful behaviors with a limited amount of demon-\nstrations? We compare our algorithm with DQfD (Hester\net al., 2017), which has been shown to learn efﬁciently from\ndemonstrations and to preserve performance while acting\nin an environment. Other methods include supervised be-\nhavioral cloning method, Q-learning, soft Q-learning, the\nversion of our method with importance sampling weighting,\nPCL and Q-learning, soft Q-learning without demonstra-\ntions.\n5.1. Environments\nWe evaluate our result in a grid world, the toy Minecraft\n(Fig. 2), as well as two realistic 3D simulated environments,\nTorcs and Grand Theft Auto V (GTA V) shown in Figure 1.\nToy Minecraft: The toy Minecraft is a customized grid\nworld environment. As shown in Figure 2, the agent starts\nfrom the left and would like to reach the ﬁnal goal (marked\nas a heart). The agent can walk on the green grass and go\ninto the blue water ends the episode. The input to the agent\nis its current (x, y) location. At each step, the agent can\nmove Up, Down, Left or Right. It gets a reward of 1 when\nreaching the goal, 0 otherwise. For more details, please refer\nto the OpenAI gym Frozen-Lake environment (Brockman\net al., 2016).\nTorcs: Torcs is an open-source racing game that has been\nused widely as an experimental environment for driving.\nThe goal of the agent is to drive as fast as possible on the\ntrack while avoiding crashes. We use an oval two-lane\nracing venue in our experiments. The input to the agent is\nan 84×84 gray scale image. The agent controls the vehicle\nat 5Hz, and at each step, it chooses from a set of 9 actions\nwhich is a Cartesian product between {left, no-op, right}\nand {up, no-op, down}. We design a dense driving reward\nfunction that encourages the car to follow the lane and to\navoid collision with obstacles. 1\nGTA: Grand Theft Auto is an action-adventure video game\nwith goals similar in part to the Torcs game, but with a more\ndiverse and realistic surrounding environment, including\nthe presence of other vehicles, buildings, and bridges. The\nagent observes 84×84 RGB images from the environment.\nIt chooses from 7 possible actions from {left-up, up, right-\nup, left, no-op, right, down} at 6Hz. We use the same reward\nfunction as in Torcs.\n5.2. Comparisons\nWe compare our approach with the following methods:\n1reward = (1 −1damage)[(cos θ −sin θ −lane ratio)\n×speed] + 1damage [−10], where 1damage is an indicator func-\ntion of whether the vehicle is damaged at the current state.\nlane ratio is the ratio between distance to lane center and lane\nwidth. θ is the angle between the vehicle heading direction and the\nroad direction.\nReinforcement Learning from Imperfect Demonstrations\nFigure 3. Performances on the Torcs game. The x-axis shows the training iterations. The y-axis shows the average total rewards. Solid\nlines are average values over 10 random seeds. Shaded regions correspond to one standard deviation. The left ﬁgure shows the performance\nfor each agent when they only learn from demonstrations, while the right one shows the performance for each agent when they interact\nwith the environments after learning from demonstrations. Our method consistently outperforms other methods in both cases.\n• DQfD: the method proposed by (Hester et al., 2017). For\nthe learning from demonstration phase, DQfD combines\na hinge loss with a temporal difference (TD) loss. For\nthe ﬁnetuning-in-environment phase, DQfD combines\na hinge loss on demonstrations and a TD loss on both\nthe demonstrations and the policy-generated data. To\nalleviate over-ﬁtting issues, we also include weight decay\nfollowing the original paper.\n• Q-learning: the classic DQN method (Mnih et al., 2015).\nWe ﬁrst train DQN with the demonstrations in a replay\nbuffer and then ﬁnetune in the environment with regular\nQ-learning. Similar to DQfD, we use a constant explo-\nration ratio of 0.01 in the ﬁnetuning phase to preserve the\nperformance obtained from the demonstrations. We also\ntrain from scratch a baseline DQN in the environment,\nwithout any demonstration.\n• Soft Q-learning: similar to the Q-learning method, but\nwith an entropy regularized reward. This is the method\nproposed by (Haarnoja et al., 2017a; Schulman et al.,\n2017).\nWe also include the soft Q-learning trained\nwithout demonstration, as another baseline.\n• Behavior cloning with Q-learning: the naive way of\ncombining cross-entropy loss with Q-learning. First we\nperform behavior cloning with cross-entropy loss on the\ndemonstrations. Then we treat the logit activations prior\nthe softmax layer as an initialization of the Q function\nand ﬁnetune with regular Q-learning in the environment.\n• Normalized actor-critic with importance sampling:\nthe NAC method with the importance sampling weighting\nterm mentioned in Sec 3.1. The importance weighting\nterm is used to correct the action distribution mismatch\nbetween the demonstration and the current policy.\n• Path Consistency Learning (PCL): the PCL (Nachum\net al., 2017) method that minimizes the soft path consis-\ntency loss. The method proposed in the original paper\n(denoted as PCL-R) does not utilize a target network. We\nﬁnd that PCL-R does not work when it is trained from\nscratch in the visually complex environment. We stabilize\nit by adding a target network (denoted as PCL), similar\nto (Haarnoja et al., 2017a).\n5.3. Experiments on Toy Minecraft\nTo understand the basic properties of our proposed method,\nwe design the toy Minecraft environment. In this exper-\niment, the state is simply the location of the agent. We\nuse a tabular Q function. With those settings, we hope to\nreveal some differences between our NAC algorithm and\nalgorithms that incorporate supervised loss.\nAs shown in Figure 2, there are only two paths to reach the\ngoal. In terms of the discounted reward, the shorter path is\nmore favorable. To make the problem more interesting, we\nprovide the longer suboptimal path as the demonstrations.\nWe found that in the learning from demonstration phase,\nboth DQfD and NAC have learned the suboptimal path\nsince both methods do not have access to the environment\nand could not possibly ﬁgure out the optimal path. When\nthe two methods ﬁnetune their policies in the environment,\nNAC succeeds in ﬁnding the optimal path, while DQfD\nstucks with the suboptimal one. It is because DQfD has the\nimitation loss, thus preventing it from deviating from the\noriginal solution.\n5.4. Comparison to Other Methods\nWe compare our NAC method with other methods on 300k\ntransitions. The demonstrations are collected by a trained\nQ-learning expert policy. We execute the policy in the envi-\nronment to collect demonstrations. To avoid deterministic\nexecutions of the expert policy, we sample an action ran-\ndomly with probability 0.01.\nTo explicitly compare different methods, we show separate\nReinforcement Learning from Imperfect Demonstrations\n(a) The on-demonstration and in-environment performance of the\nNAC and DQfD methods on GTA. The vertical line separates the\nlearning from demonstration phase and ﬁnetuning in environment\nphase. Our method consistently outperforms DQfD in both phases.\n(b) Performances on the Torcs game with human demonstrations.\nDQfD performs well in the beginning, but overﬁts in the end. The\nbehavior cloning method is much worse than NAC and DQfD. Our\nNAC method performs best at convergence.\nFigure 4. Performance on GTA (left) and performance on Torcs with human demonstrations (right)\nﬁgures for performances on the demonstrations and inside\nthe environments. In Fig 3, we show that our method per-\nforms better than other methods on demonstrations. When\nwe start ﬁnetuning, the performance of our method contin-\nues to increase and reaches peak performance faster than\nother methods. DQfD (Hester et al., 2017) has similar be-\nhavior to ours but has lower performance. Behavior cloning\nlearns well on demonstrations, but it has a signiﬁcant per-\nformance drop while interacting with environments. All\nthe methods can ultimately learn by interacting with the\nenvironment but only our method and DQfD start from a\nrelatively high performance. Empirically, we found that the\nimportance weighted NAC method does not perform as well\nas NAC. The reason might be the decrease in the gradient\nbias is not offset sufﬁciently by the increase in the gradient\nvariance. Without the demonstration data, Q-learning (Q\nw/o demo) and soft Q-learning (soft-Q w/o demo) suffer\nfrom low performance during the initial interactions with\nthe environment. The original PCL-R method (PCL-R w/o\ndemo) fails to learn even when trained from scratch in the\nenvironments. The improved PCL method (PCL) is not\nable to learn on the demonstrations, but it can learn in the\nenvironment.\nWe also test our method on the challenging GTA environ-\nment, where both the visual input and the game logic are\nmore complex. Due to the limit of the environment execu-\ntion speed, we only compare our method with DQfD. As\nshown in Fig. 4a, our method outperforms DQfD both on\nthe demonstrations and inside the environment.\n5.5. Learning from Human Demonstrations\nFor many practical problems, such as autonomous driving,\nwe might have a large number of human demonstrations, but\nno demonstration available from a trained agent at all. In\ncontrast to a scripted agent, humans usually perform actions\ndiversely, both from multiple individuals (e.g. conservative\nplayers will slow down before a U-turn; aggressive players\nwill not) and a single individual (e.g. a player may randomly\nturn or go straight at an intersection). Many learning from\ndemonstration methods do not study this challenging case,\nsuch as (Ho & Ermon, 2016). We study how different meth-\nods perform with diverse demonstrations. To collect human\ndemonstrations, we asked 3 non-expert human players to\nplay TORCS for 3 hours each. Human players control the\ngame with the combination of four arrow keys, at 5Hz, the\nsame rate as the trained agent. In total, we collected around\n150k transitions.\nAmong them, 4.5k transitions are used\nas a validation set to monitor the Bellman error.\nCom-\nparing with data collected from a trained agent, the data is\nmore diverse and the quality of the demonstrations improves\nnaturally when the players get familiar with the game.\nIn Fig. 4b, we observe that the behavior cloning method\nperforms much worse than NAC and DQfD. DQfD initially\nis better than our method but later is surpassed by the NAC\nmethod quickly, which might be caused by the supervised\nhinge loss being harmful when demonstrations are subopti-\nmal. Similar to the policy generated demonstrations case,\nPCL, hard Q-learning and soft Q-learning do not perform\nwell.\n5.6. Effects of Imperfect Demonstrations\nIn the real world, collected demonstrations might be far\nfrom optimal. The human demonstrations above have al-\nready shown that imperfect demonstrations could have a\nlarge effect on performance. To study this phenomenon in\na principled manner, we collect a few versions of demon-\nstrations with varying degrees of noise. When collecting\nthe demonstrations with the trained Q agent, we corrupt a\ncertain percentage of the demonstrations by choosing non-\noptimal actions (argmina Q(s, a)).\nThe data corruption\nprocess is conducted while interacting with the environment;\ntherefore, the error will affect the collection of the follow-\nReinforcement Learning from Imperfect Demonstrations\nFigure 5. Left: Learning from imperfect data when the imperfectness is 30%. Our NAC method does not clone suboptimal behaviors and\nthus outperforms DQfD and behavior cloning. Right: Learning from a limit amount of demonstrations. Even with only 30 minutes (10k\ntransitions) of experience, our method could still learn a policy that is comparable with supervised learning methods. More results are\navailable in the appendix, including 50% and 80% imperfect data ablations, as well as 150k and 300k data amount studies.\ning steps. We get 3 sets of {30%, 50%, 80%} percentage\nof imperfect data. In the left of Fig. 5, we show that our\nmethod performs well compared with DQfD and behavior\ncloning methods. Supervised behavior cloning method is\nheavily inﬂuenced by the imperfect demonstrations. DQfD\nis also heavily affected, but not as severely as the behavior\ncloning. NAC is robust because it does not imitate the sub-\noptimal behaviors. The results for 50% and 80% percentage\nof imperfect data are similar, and they are available in the\nappendix.\n5.7. Effects of Demonstration Size\nIn this section, we show comparisons among our method and\nother methods with different amounts of demonstration data.\nWe use a trained agent to collect three sets of demonstrations\nwhich include 10k, 150k, and 300k transitions each. In the\nexperiments, we ﬁnd that our algorithm performs well when\nthe amount of data is large and is comparable to supervised\nmethods even with a limited amount of data. In Fig. 5\n(right), we show when there are extremely limited amounts\nof demonstration data (10k transitions or 30 minutes of\nexperience), our method performs on par with supervised\nmethods. In the appendix, we show the results for 150k and\n300k transitions: our method outperforms the baselines by a\nlarge margin with 300k transitions. In summary, our method\ncan learn from small amounts of demonstration data and\ndominates in terms of performance when there is a sufﬁcient\namount of data.\n5.8. Effects of Reward Choice\nIn the above experiments, we adopt a natural reward: it max-\nimizes speed along the lane, minimizes speed perpendicular\nto the lane and penalizes when the agent hits anything. How-\never, very informative rewards are not available under many\nconditions. In this section, we study whether our method is\nrobust to a less informative reward. We change the reward\nFigure 6. Similar to Figure 3 (left), we compare NAC with other\nmethods when only learning from the demonstrations, except that\nwe use a different reward: speed2. Our method still performs the\nbest.\nfunction to be the square of the speed of an agent, irrespec-\ntive of the speed’s direction. This reward encourages the\nagent to drive fast, however, it is difﬁcult to work with be-\ncause the agent has to learn by itself that driving off-road or\nhitting obstacles reduce its future speed. It is also hard be-\ncause speed2 has a large numerical range. Figure 6 shows\nthat NAC method still performs the best at convergence,\nwhile DQfD suffers from severe performance degeneration.\n6. Conclusion\nWe proposed a Normalized Actor-Critic algorithm for re-\ninforcement learning from demonstrations. Our algorithm\nprovides a uniﬁed approach for learning from reward and\ndemonstrations, and is robust to potentially suboptimal\ndemonstration data. An agent can be ﬁne-tuned with re-\nwards after training on demonstrations by simply continuing\nto perform the same algorithm on on-policy data. Our algo-\nrithm preserves and improves the behaviors learned from\ndemonstrations while receiving reward through interaction\nwith an environment.\nReinforcement Learning from Imperfect Demonstrations\nReferences\nAbbeel, Pieter and Ng, Andrew Y. Apprenticeship learn-\ning via inverse reinforcement learning. In Proceedings\nof the twenty-ﬁrst international conference on Machine\nlearning, pp. 1. ACM, 2004.\nBojarski, Mariusz, Del Testa, Davide, Dworakowski, Daniel,\nFirner, Bernhard, Flepp, Beat, Goyal, Prasoon, Jackel,\nLawrence D, Monfort, Mathew, Muller, Urs, Zhang, Ji-\nakai, et al. End to end learning for self-driving cars. arXiv\npreprint arXiv:1604.07316, 2016.\nBrockman, Greg, Cheung, Vicki, Pettersson, Ludwig,\nSchneider, Jonas, Schulman, John, Tang, Jie, and\nZaremba, Wojciech.\nOpenai gym.\narXiv preprint\narXiv:1606.01540, 2016.\nChemali, Jessica and Lazaric, Alessandro. Direct policy\niteration with demonstrations. In IJCAI, pp. 3380–3386,\n2015.\nDegris, Thomas, White, Martha, and Sutton, Richard S.\nOff-policy actor-critic. arXiv preprint arXiv:1205.4839,\n2012.\nEbrahimi, Sayna, Rohrbach, Anna, and Darrell, Trevor.\nGradient-free policy architecture search and adaptation.\nIn Levine, Sergey, Vanhoucke, Vincent, and Goldberg,\nKen (eds.), Proceedings of the 1st Annual Conference on\nRobot Learning, volume 78 of Proceedings of Machine\nLearning Research, pp. 505–514. PMLR, 13–15 Nov\n2017. URL http://proceedings.mlr.press/\nv78/ebrahimi17a.html.\nGu, Shixiang, Lillicrap, Timothy, Ghahramani, Zoubin,\nTurner, Richard E, and Levine, Sergey. Q-prop: Sample-\nefﬁcient policy gradient with an off-policy critic. arXiv\npreprint arXiv:1611.02247, 2016.\nGu, Shixiang, Lillicrap, Timothy, Ghahramani, Zoubin,\nTurner, Richard E, Sch¨olkopf, Bernhard, and Levine,\nSergey. Interpolated policy gradient: Merging on-policy\nand off-policy gradient estimation for deep reinforcement\nlearning. arXiv preprint arXiv:1706.00387, 2017.\nHaarnoja, Tuomas, Tang, Haoran, Abbeel, Pieter, and\nLevine, Sergey. Reinforcement learning with deep energy-\nbased policies. arXiv preprint arXiv:1702.08165, 2017a.\nHaarnoja, Tuomas, Tang, Haoran, Abbeel, Pieter, and\nLevine, Sergey. Reinforcement learning with deep energy-\nbased policies. In Precup, Doina and Teh, Yee Whye\n(eds.), Proceedings of the 34th International Confer-\nence on Machine Learning, volume 70 of Proceedings\nof Machine Learning Research, pp. 1352–1361, Interna-\ntional Convention Centre, Sydney, Australia, 06–11 Aug\n2017b. PMLR. URL http://proceedings.mlr.\npress/v70/haarnoja17a.html.\nHester, Todd, Vecerik, Matej, Pietquin, Olivier, Lanctot,\nMarc, Schaul, Tom, Piot, Bilal, Sendonaris, Andrew,\nDulac-Arnold, Gabriel, Osband, Ian, Agapiou, John, et al.\nLearning from demonstrations for real world reinforce-\nment learning. arXiv preprint arXiv:1704.03732, 2017.\nHo, Jonathan and Ermon, Stefano. Generative adversarial\nimitation learning. In Advances in Neural Information\nProcessing Systems, pp. 4565–4573, 2016.\nJaderberg, Max, Mnih, Volodymyr, Czarnecki, Woj-\nciech Marian, Schaul, Tom, Leibo, Joel Z, Silver,\nDavid, and Kavukcuoglu, Koray. Reinforcement learn-\ning with unsupervised auxiliary tasks. arXiv preprint\narXiv:1611.05397, 2016.\nKakade, Sham Machandranath et al. On the sample com-\nplexity of reinforcement learning. PhD thesis, University\nof London London, England, 2003.\nKim, Beomjoon, massoud Farahmand, Amir, Pineau, Joelle,\nand Precup, Doina. Learning from limited demonstra-\ntions. In Advances in Neural Information Processing\nSystems, pp. 2859–2867, 2013.\nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,\nRusu, Andrei A, Veness, Joel, Bellemare, Marc G, Graves,\nAlex, Riedmiller, Martin, Fidjeland, Andreas K, Ostro-\nvski, Georg, et al. Human-level control through deep re-\ninforcement learning. Nature, 518(7540):529–533, 2015.\nMunos, R´emi, Stepleton, Tom, Harutyunyan, Anna, and\nBellemare, Marc. Safe and efﬁcient off-policy reinforce-\nment learning. In Advances in Neural Information Pro-\ncessing Systems, pp. 1054–1062, 2016.\nNachum, Oﬁr, Norouzi, Mohammad, Xu, Kelvin, and Schu-\nurmans, Dale.\nBridging the gap between value and\npolicy based reinforcement learning.\narXiv preprint\narXiv:1702.08892, 2017.\nNg, Andrew Y, Russell, Stuart J, et al. Algorithms for\ninverse reinforcement learning. In Icml, pp. 663–670,\n2000.\nPiot, Bilal, Geist, Matthieu, and Pietquin, Olivier. Boosted\nbellman residual minimization handling expert demon-\nstrations. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases, pp.\n549–564. Springer, 2014.\nRoss, St´ephane, Gordon, Geoffrey J, and Bagnell, Drew. A\nreduction of imitation learning and structured prediction\nto no-regret online learning. In International Confer-\nence on Artiﬁcial Intelligence and Statistics, pp. 627–635,\n2011.\nReinforcement Learning from Imperfect Demonstrations\nSchulman, John, Abbeel, Pieter, and Chen, Xi. Equiva-\nlence between policy gradients and soft q-learning. arXiv\npreprint arXiv:1704.06440, 2017.\nShelhamer, Evan, Mahmoudieh, Parsa, Argus, Max,\nand Darrell, Trevor.\nLoss is its own reward: Self-\nsupervision for reinforcement learning. arXiv preprint\narXiv:1612.07307, 2016.\nSutton, Richard S and Barto, Andrew G. Reinforcement\nlearning: An introduction, volume 1. MIT press Cam-\nbridge, 1998.\nThie, Paul R. Markov decision processes. Comap, Incorpo-\nrated, 1983.\nTodorov, Emanuel. General duality between optimal control\nand estimation. In Decision and Control, 2008. CDC\n2008. 47th IEEE Conference on, pp. 4286–4292. IEEE,\n2008.\nToussaint, Marc. Robot trajectory optimization using ap-\nproximate inference. In Proceedings of the 26th annual\ninternational conference on machine learning, pp. 1049–\n1056. ACM, 2009.\nWang,\nZiyu,\nBapst,\nVictor,\nHeess,\nNicolas,\nMnih,\nVolodymyr, Munos, Remi, Kavukcuoglu, Koray, and\nde Freitas, Nando. Sample efﬁcient actor-critic with ex-\nperience replay. arXiv preprint arXiv:1611.01224, 2016.\nWang, Ziyu, Merel, Josh, Reed, Scott, Wayne, Greg, de Fre-\nitas, Nando, and Heess, Nicolas. Robust imitation of di-\nverse behaviors. arXiv preprint arXiv:1707.02747, 2017.\nWatkins, Christopher JCH and Dayan, Peter. Q-learning.\nMachine learning, 8(3-4):279–292, 1992.\nXu, Huazhe, Gao, Yang, Yu, Fisher, and Darrell, Trevor.\nEnd-to-end learning of driving models from large-scale\nvideo datasets. arXiv preprint arXiv:1612.01079, 2016.\nZiebart, Brian D. Modeling purposeful adaptive behavior\nwith the principle of maximum causal entropy. Carnegie\nMellon University, 2010.\nZiebart, Brian D, Maas, Andrew L, Bagnell, J Andrew, and\nDey, Anind K. Maximum entropy inverse reinforcement\nlearning. In AAAI, volume 8, pp. 1433–1438. Chicago,\nIL, USA, 2008.\nReinforcement Learning from Imperfect Demonstrations\n7. Appendix\n7.1. Normalized Actor-Critic with Q Parametrization\nUsually the actor-critic method parametrizes π(s, a) and\nV (s) with a neural network that has two heads. In this\nsection, we explore an alternative parametrization: Q-\nparametrization. Instead of outputting π and V directly,\nthe neural network computes Q(s, a). We parametrize π\nand V based on Q by specifying a ﬁxed mathematical trans-\nform:\nVQ(s) = α log\nX\na\nexp(Q(s, a)/α)\n(15)\nπQ(a|s) = exp((Q(s, a) −VQ(s))/α)\n(16)\nNote that the Q-parametrization we propose here can be\nseen as a speciﬁc design of the network architecture. In-\nstead of allowing the net to output arbitrary π(s, a) and V (s)\nvalues, we restrict the network to only output π(s, a) and\nV (s) pairs that satisfy the above relationship. This extra\nrestriction will not harm the network’s ability to learn since\nthe above relationship has to be satisﬁed at the optimal solu-\ntion(Schulman et al., 2017; Haarnoja et al., 2017b; Nachum\net al., 2017).\nBased on the Q-parametrization, we can derive the update of\nthe actor. Note that we assume the behavioral policy is πQ,\nand we sample one step out of a trajectory, thus dropping\nthe subscript t. The goal is to maximize expected future\nreward, thus taking gradient of it we get:\n∇Es,a∼πQ [R(s, a)]\n=Es,a∼πQ [R(s, a)∇logθ p(a, s|πQ)]\n≈Es,a∼πQ [R(s, a)∇θ log πQ(a|s)]\n(17)\nwhere the last step ignores the state distribution, thus an ap-\nproximation. By adding some baseline functions, it turns to\nthe following format, where ˆQ(s, a) = R(s, a) + γVQ(s′):\nEs,a\nh\n∇θ log πQ(a|s)( ˆQ(s, a) −b(s))\ni\n=Es\n\"X\na\nπQ(a|s)∇θ log πQ(a|s)( ˆQ(s, a) −b(s))\n#\n(18)\nAs in previous work, an entropy-regularized policy gradient\nsimply add the gradient of the entropy of the current policy\nwith a tunable parameter α in order to encourage exploration.\nThe entropy term is:\nEs [α∇θH(πQ(·|s))]\n=Es\n\"\nα∇θ\nX\na\n−πQ(a|s) log πQ(a|s)\n#\n=Es\n\"\nα\nX\na\n−∇θπQ(a|s) log πQ(a|s)\n−πQ(a|s)∇θ log πQ(a|s)]\n=Es\n\"\nα\nX\na\n−∇θπQ(a|s) log πQ(a|s)\n−πQ(a|s)\n1\nπQ(a|s)∇θπQ(a|s)\n\u0015\n=Es\n\"\nα\nX\na\n−∇θπQ(a|s) log πQ(a|s)\n#\n=Es\n\"\nα\nX\na\n−πQ(a|s)∇θ log πQ(a|s) log πQ(a|s)\n#\n(19)\nputting the two terms together and using the energy-based\npolicy formulation (Eq. (16)) :\nEs,a\nh\n∇θ log πQ(a|s)( ˆQ(s, a) −b(s)) + α∇θH(πQ(·|s))\ni\n=Es\n\"X\na\nπQ(s, a)∇θ log πQ(s, a)\n( ˆQ(s, a) −b(s) −(Q(s, a) −VQ(s)))\ni\nIf we let the baseline b(s) be VQ(s), we get the update:\nEs\n\"X\na\nπQ(s, a)∇θ log πQ(s, a)( ˆQ(s, a) −Q(s, a))\n#\n= 1\nαEs,a\nh\n(∇θQ(s, a) −∇θVQ(s))( ˆQ(s, a) −Q(s, a))\ni\n(20)\nwhere ˆQ(s, a) could be obtained through bootstrapping by\nR(s, a) + γVQ(s′). In practice VQ(s′) is computed from a\ntarget network. For the critic, the update is:\nEs\n\u0014\n∇θ\n1\n2(VQ(s) −ˆV (s))2\n\u0015\n=Es\nh\n∇θVQ(s)(VQ(s) −ˆV (s))\ni\n(21)\nwhere ˆV (s) could be similarly obtained by bootstrapping:\nˆV (s) = Ea [R(s, a) + γVQ(s′)] + αH(πQ(·|s)).\nReinforcement Learning from Imperfect Demonstrations\n7.2. Effects of Imperfect Demonstrations\nSee Figure 8 for more results for imperfect demonstrations\nwhen the amount of noise varies.\n7.3. Effects of Demonstration Amount\nSee Figure 7 for more results on the effect of the demonstra-\ntion amount.\n7.4. Experiment Details\nNetwork Architecture: We use the same architecture as\nin (Mnih et al., 2015) to parametrize Q(s, a). With this Q\nparametrization, we also output πQ(a|s) and VQ(s) based\non Eq. (16) and Eq. (15).\nHyper-parameters: We use a replay buffer with a capacity\nof 1 million steps and update the target network every 10K\nsteps. Initially, the learning rate is linearly annealed from 1e-\n4 to 5e-5 for the ﬁrst 1/10 of the training process and then\nit is kept as a constant (5e-5). Gradients are clipped at 10\nto reduce training variance. The reward discount factor γ is\nset to 0.99. We concatenate the 4 most recent frames as the\ninput to the neural network. For the methods with an entropy\nregularizer, we set α to 0.1, following (Schulman et al.,\n2017). We truncate the importance sampling weighting\nfactor β = min\nn\nπQ(a|s)\nµ(a|s) , c\no\nat 10, i.e., c = 10.\nReinforcement Learning from Imperfect Demonstrations\nFigure 7. More results when varying the amount of demonstrations. The left and right ﬁgures show when there are 150k and 300k\ntransitions respectively. Our NAC method achieves superior performance with a large amount of demonstrations and is comparable to\nsupervise methods with smaller amount of demonstrations.\nFigure 8. More results when introducing imperfect demonstrations. Left ﬁgure shows when there are 50% imperfect actions and the right\none shows the case for 80%. Our NAC method is highly robust to noisy demonstrations.\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-02-14",
  "updated": "2019-05-30"
}