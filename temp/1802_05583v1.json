{
  "id": "http://arxiv.org/abs/1802.05583v1",
  "title": "Tools and resources for Romanian text-to-speech and speech-to-text applications",
  "authors": [
    "Tiberiu Boros",
    "Stefan Daniel Dumitrescu",
    "Vasile Pais"
  ],
  "abstract": "In this paper we introduce a set of resources and tools aimed at providing\nsupport for natural language processing, text-to-speech synthesis and speech\nrecognition for Romanian. While the tools are general purpose and can be used\nfor any language (we successfully trained our system for more than 50 languages\nand participated in the Universal Dependencies Shared Task), the resources are\nonly relevant for Romanian language processing.",
  "text": "Tools and resources for Romanian text-to-speech and speech-to-text\napplications\nTiberiu Boros, Stefan Daniel Dumitrescu and Vasile Pais\nResearch Institute for Artiﬁcial Intelligence “Mihai Draganescu”, Romanian Academy\nCalea 13 Septembrie, nr. 13\n{tibi, sdumitrescu, pais}@racai.ro\nAbstract\nIn this paper we introduce a set of resources and tools aimed at providing support for natural language processing,\ntext-to-speech synthesis and speech recognition for Romanian. While the tools are general purpose and can be used for any\nlanguage (we successfully trained our system for more than 50 languages and participated in the Universal Dependencies\nShared Task), the resources are only relevant for Romanian language processing.\nKeywords: natural language processing, text-to-speech synthesis, Romanian language, multilingual, low-resourced\nenvironments, decision trees, neural networks, LSTMs\n1.\nIntroduction\nNatural language processing (NLP), text-to-speech\nsynthesis (TTS) and automatic speech recognition\n(ASR) are key components of modern applica-\ntions especially those that rely on human-computer-\ninteraction via voice input/output. As smart phones\nand gadgets are gaining ground on their competi-\ntors (laptops and desktops), they are the most likely\ncandidates to serve as front-ends in the Internet-\nof-Thing (IoT) landscape.\nHowever, these devices\nrely mostly on low-powered chips that need to run\nlive/responsive user interfaces. Our work is focused\nin providing support for NLP, TTS and ASR applica-\ntions in low-resourced environments by providing a\nset of tools that is designed to easily scale, depend-\ning on the available application and computational re-\nsources. Given the success of the widely spread and\nwell-known lightweight ASR system PocketSphinx\n(Huggins-Daines et al., 2006) we only address NLP\nand TTS with our tools. We do however, introduce a\nnewly created text-to-speech synthesis corpus that is\nintended to consolidate currently available speech re-\nsources for the Romanian language and a newly cre-\nated speech recognition corpus which is composed\nof a freely available sub-corpus and license-restricted\none. Though we are able to provide transcription and\nphoneme-level alignments for the non-free section of\nthe ASR corpus, obtaining the recorded speech is the\nsubject of a different process that involves a third\nparty (the RADOR1 press agency).\nAs we are cur-\nrently working on a neural-based speech recognition\nand keyword spotting tool for Romanian, providing\npre-trained models on the entire speech corpus will\nnot be a problem and will mitigate the license restric-\ntions. Also, we have to mention that the audio data\ncan be indirectly obtained by systematically using the\nOral Corpus Query Platform (OCQP) (later described\n1http://www.rador.ro/\nin section 2.3.) from the COROLA project2 based on\nour aligned data.\nThe paper is structured as follows:\n(a) the ﬁrst\npart introduces two ready-to-use frameworks that are\nfreely available for download with no license restric-\ntions; (b) the second part describes a freely available\nspeech corpus for Romanian, focusing on corpora-\ncomposition and annotations; (c) the third part dis-\ncusses our road-map for future developments and en-\nhancements.\n2.\nTools description\n2.1.\nNatural Language Processing\nMost natural language processing (NLP) tasks require\na certain level of text preprocessing aimed at segment-\ning the input into standard processing units (often\ninto sentences and words but, depending on the ap-\nplication, also syllables, phonemes etc.) and at en-\nriching these units with additional features designed\nto reduce the effect of data sparsity (lemmas, part-of-\nspeech tags, morphological attributes etc.). Because\nthis is a ground-zero requirement, the literature is\nabundant with methods and techniques for low-level\ntext processing, but multilingual text-processing is\nstill a challenging task. This has been proven by the\nwell-known shared task on Universal Dependencies\n(UD) parsing (Zeman et al., 2017). One very impor-\ntant conclusion is that while some algorithms have an\noverall better performance than others - and we draw\nthe attention to Stanford’s (Dozat et al., 2017) graph-\nbased parser, there is no “one size ﬁts all” algorithm\nthat is language and corpora-size independent.\nWhile accuracy carries a great weight in NLP ap-\nplications, there are two other factors that impact\nthe design of such systems:\ncomputational cost\nand memory footprint.\nWith this in mind we in-\ntroduced support for three very different machine\nlearning algorithms applied on the same set of\n2http://corola.racai.ro/#interogare\narXiv:1802.05583v1  [cs.CL]  15 Feb 2018\ntext-processing tasks: decision trees, linear models\nand neural networks (bidirectional long-short-term-\nmemory (LSTM) networks). We motivate our choice\nbased on the computational/memory requirements of\nthese algorithms:\n• Decision trees (DTs) require virtually no feature-\nengineering, provide a relatively small model\nfootprint, with a logarithmic computational\ncomplexity (O (log n)), where n is the number of\nunique features and low mathematical load;\n• Linear\nmodels\nrequire\nextensive\nfeature-\nengineering, yield models with large footprints,\nwith linear computational complexity (O (n))\nand a moderate mathematical load (commonly\nmultiplications and additions);\n• Neural networks are able to learn patterns and\nautomatically generate required non-linearities\nbetween the input features, yield small foot-\nprint models (even with compact feature embed-\ndings), but generate a high computational load,\nmainly because of the large number of operations\nand the use of complex mathematical functions\n(multiplications, additions, tanh or σ activation\nfunctions).\nFigure 1: Overview of the MLPLA architecture\nThe MLPLA architecture (Figure 1) (initially intro-\nduced in (Zaﬁu et al., 2015)) is composed of three\nmain layers: (a) input; (b) processing pipeline and\n(c) output. For overcoming language-dependent and\napproach-based limitations, our system is built using\na scalable plug-and-play methodology. The process-\ning units are built so that they implement one of three\ndifferent interfaces depending on the module under\nwhich they operate. The three interfaces are (a) the\ndata input processor - an implementation of this inter-\nface must be able to receive the input text as a charac-\nter sequence and perform any necessary preprocess-\ning in order to obtain a tokenized text; (b) the base\nprocessor interface – an implementation receives a se-\nquence of tokens, each token containing standard at-\ntributes (part-of-speech, lemma, phonetic transcrip-\ntion, syllables, accent, chunk and dependencies) but\nalso allowing the insertion of non-standard attributes\nthrough a key-value table - each processor is responsi-\nble for building its own feature set using all available\ndata, performing the NLP task it was designed for\n[Input]\nmlpla.language.preprocessing.BasicTokenizer\n[Pipeline]\nmlpla.language.baseprocessors.BasicTagger\nmlpla.language.baseprocessors.BasicLemmatizer\nmlpla.language.baseprocessors.BasicChunker\nmlpla.language.baseprocessors.BasicParser\nmlpla.language.baseprocessors.BasicSyllabiﬁer\nmlpla.language.baseprocessors.BasicLTS\nmlpla.language.baseprocessors.BasicStress\n[Output]\nmlpla.language.formats.TabFeatureOutput\nFigure 2: Excerpt from the MLPLA conﬁguration ﬁle\nand ﬁlling in either a value for a standard attribute or\nadding a custom attribute; (c) the feature-based out-\nput – an implementation must take a sequence of to-\nkens and convert it into a feature-based output, de-\npending on the application in which MLPLA is used.\nThe order in which the base processors are chained\nis controlled externally from a conﬁguration ﬁle (see\nFigure 2 for details).\nOur recent work has been centered on extending the\nexisting system and addressing multilingual text pro-\ncessing. Given that we are able to easily interchange\nbetween models/modules and classiﬁers, we focused\nour efforts into assessing what is the best trade-off be-\ntween speed/accuracy and model size because versa-\ntility is an important feature of our framework.\nNOTE: The performance of each model processing\nsteps are currently not the focus of this paper. How-\never, they can be looked up in Zaﬁu et al.\n(2015)\nfor Romanian speciﬁc data and in Dumitrescu et al.\n(2017) for the complete list of languages supported by\nUD. If this article is accepted as a long paper we plan\nto include more results using a stacked bidirectional\nLSTM model that we’ve worked on recently.\nWe note that the system supports tokenization,\nlemmatization,\nchunking,\npart-of-speech tagging,\nparsing, syllabiﬁcation, stress prediction on words\nand letter-to-sound (for text-to-speech purposes).\nEach of these processing tasks uses one or more of the\nalgorithms shortly described below; for example, tag-\nging can be done either with a linear model or with\nLSTMs. The conﬁguration ﬁle allows easy prototyp-\ning of solutions using our platform.\n2.2.\nText-to-speech synthesis\nThe speech synthesis tool is called SSLA which stands\nfor Speech Synthesis for Lightweight Applications.\nWe implemented statistical parametric speech synthe-\nsis because it offers constant quality and a small foot-\nprint in contrast to the concatenative (unit-selection)\napproach that might sound more natural at times\n(if enough data is available, otherwise worse than\nparametric synthesis) and a signiﬁcantly larger mem-\nory requirement. We use decision trees to indepen-\ndently model frame-by-frame speech parameters for\nthe spectral envelope, phone-state duration and voice\npitch.\nFor the effective speech synthesis process it can switch\nbetween the classical Mel-Log Spectral Approxima-\ntion (MLSA) ﬁlter (Imai et al., 1983) and Speech\nTransformation and Representation using Adaptive\nInterpolation of weiGHTed spectrum (STRAIGHT)\n(Kawahara et al., 1999). The reason for this is that\nwhile STRAIGHT results are superior in quality, it is\nway more computationally expensive that the MLSA\ncounterpart and, for some applications this can be a\nreally important bottleneck.\nThe input models are fully compatible with the HMM\nSpeech Synthesis (HTS) Toolkit (Zen et al., 2007),\nwhich in fact we use for training and compiling mod-\nels. Currently we only support multinomial training\nand we treat features as “bag-of-words”. There are no\nconstraints regarding the feature format, except that\nfeatures should not contain spaces or the special char-\nacter ‘/’ which is the feature delimiter.\nThe standard feature-set used by our speech synthesis\nback-end is:\n• Phonetic context: (a) current phoneme accompa-\nnied by two preceding phonemes and two suc-\nceeding phonemes; (b) articulatory information\nfor all phones inside the feature window;\n• Syllable information: (a) the identity of frequent\nsyllables3 is used as a feature along side with\n(b) information which is present regardless of the\nsyllables frequency such as: lexical accent, rela-\ntive syllable position inside the word and sen-\ntence, the total number of syllables in the sen-\ntence (which is actually a feature used for the\nglobal variance), distance from the previous and\ndistance to the next punctuation mark;\n• Global context information: the type of the sen-\ntence (declarative, interrogative or exclamation),\nthe identity of the previous and the next punctu-\nation marks and the total number of words inside\nthe utterance;\n• Local morphosyntactic information:\nprevious,\ncurrent and next part-of-speech together with the\nrelative position of the syllable and word inside\nsyntactic chunks as deﬁned in Ion (2007b)\n2.3.\nOral Corpus Query Platform\nThe Oral Corpus Query Platform is an on-line tool\ndesigned to help linguists in their study of spoken\nlanguage.\nIt enables one to query our oral corpus\nusing combinations of wordforms, lemmas and part-\nof-speech tags. It is currently part of the COROLA\nproject (Tuﬁs et al., 2015) and it is hosted in the RACAI\ncloud4, but, if desired we are willing to provide access\nto our code-base and help deploy the platform on-site.\n3In our experiments we set the threshold to 5 for frequent\nsyllables\n4http://korap.racai.ro/corola_sound_search/index.php\nThe data currently available on this platform con-\ntains the Romanian oral corpus which is described in\nthe next section, as well as additional speech corpora\nfrom the Institute of Computer Science of the Roma-\nnian Academy (IIT).\nIn order to fully support indexing and searching\nthrough the corpus we used the ﬂat start monophones\nprocedure of HTK (Young et al., 2002) in order to\nobtain phoneme-level alignments between the tran-\nscriptions and speech data. Because HTK only uses\nwords and their phonetic transcriptions we realigned\nthe raw text data with the phoneme-level information\nusing dynamic programming. Also, the raw text data\nwas tokenized, lemmatized and tagged using an ex-\nternal tool called TTL (Ion, 2007a).\nThe reason for\nnot using our own tool-chain was that COROLA re-\nquired consistent annotations over the entire corpora\nand the text-component was already processed using\nthis standard tool.\n3.\nSpeech corpus\nAs previously mentioned our speech data is com-\nposed of a section aimed at text-to-speech synthe-\nsis (composed of high-quality recordings) and an-\nother section which is intended to provide support for\nspeech recognition applications.\n3.1.\nThe text-to-speech synthesis corpus\n3.1.1.\nCorpora composition\nOne of the prerequisites in developing a TTS corpus\nstates that the corpus must provide a good cover-\nage over the target language and domain. In other\nwords this means that (a) the corpus must be pho-\nnetically balanced in terms of target speech units (i.e.\nphonemes, diphones etc.) and (b) a single unit must\nappear in multiple prosodic contexts in order to en-\nable the TTS system to learn the prosodic patterns\nthat relate to the language, the target domain and the\nspeaking style of the speaker himself.\nTaking into\nconsideration the above mentioned conditions we de-\ncided to construct a Romanian speech corpus com-\nposed of two sections:\n(a) The ﬁrst section (section A) is based on Wikipedia\n(for Romanian) and contains a number of sen-\ntences that were chosen using a greedy algorithm\n(that will be presented later in this paper) in or-\nder to ensure the completeness of the phonetic do-\nmain of the Romanian language. The sentences\nare treated as individual prompts (no larger con-\ntext is provided), thus the speaker must record\neach individual sentence “out of the blue” and he\nis forced to limit his narrative interpretation to the\nutterance itself.\n(b) The second section of the corpus (section B) is\ncomposed of the Romanian adaptation after Allen\nCarr’s book “Easy way to stop smoking”.\nThe\nbook contains a lot of motivational and persua-\nsive passages which are carefully crafted by the\nauthor to convince smokers quit their habit. Ad-\nditional to the prompts themselves, we also made\nuse of an existing audiobook. Originally, this au-\ndiobook was recorded by a male actor and has ap-\nproximately two and a half hours of high quality\nstudio recordings at 48KHz. This lead the actor\nto make use of highly prosodic rhetoric speech\nwith the purpose of (a) reshaping the cognitive\nstate of (b) and relying embedded messages to the\nlistener. Gaining access to the low-level prosodic\nparameters (F0, phone duration and pauses) that\nmake up such a speech is an asset to research in\nthe ﬁeld of natural TTS systems. The matching\nprompts (from the audiobook) were made avail-\nable to our speakers (one male and one female) in\norder to act as a baseline and a guide in their voice\nshaping process.\nThe second section of our corpus (the book section) is\nnot as well balanced as the ﬁrst section. The corpus\nfrom which section A sentences were extracted was\nthe full dump of the Romanian Wikipedia as of June\n2012, because, belonging to the encyclopedic genre, it\ncontains a wide range of domains and different word\ntypes.\nBecause the Wikipedia dump contains a lot of errors\nand is far from a clearly readable text, we had to em-\nploy a number of heuristic rules to remove and/or\ncorrect sentences. Below we enumerate the process-\ning steps applied:\n(a) Sentence-split the corpus and tokenize it, keeping\nonly the ones that were not longer than 20 words.\nUsing our in-house developed sentence splitter\n(based on a Maximum Entropy engine), we ob-\ntained over 5 million such sentences.\n(b) Remove all leading and trailing spaces or non-\nprintable characters.\n(c) Remove all lines that contain any of the follow-\ning characters: ‘½’, ‘G’, ‘¾’, ‘H’, brackets, slashes,\nquotes, etc. (several characters we manually in-\nput), as well as all the lines that contain abbrevi-\nations or tokens like : Sos., Cal., .ro., uk., www.,\netc. . All these rules were input manually because\nthere is a large number of sentences that contain\nthere tokens and are not suitable for recording.\nSome of the rules are regexes like a word hav-\ning Latin a-z characters; others were simple con-\nditions that a line should not have a certain sub-\nstring.\n(d) Remove all lines that contain numbers.\n(e) Remove all lines that are all caps (usually titles)\n(f) Remove all lines with less than three words with\nthe following exceptions: if the sentence length is\none, then that word should be in the Romanian\nLexicon, thus removing a signiﬁcant number of\nforeign sentences existing in Wikipedia.\n(g) Remove all lines that do not have at least 90%\nwords in the lexicon (excepting proper nouns).\nThis rule ensured that a lot of erroneous sentences\nwere removed because they contained words in\nforeign languages (even though we used the Ro-\nmanian Wikipedia dump, we still found a great\nnumber of sentences that are or at least contain\nwords in other languages).\n(h) Remove all lines that do not have at least 90%\nwords with diacritics, skipping the majority of ex-\nisting foreign sentences.\n(i) Correct the Romanian i-of-i (î) words to the cor-\nrect form of i-of-a (â). For example, the old word\nform “cîte” (meaning “how many”) was corrected\nto the new writing “câte”. While deterministic,\nthis process is not straightforward relying on a\nlexicon, backing off to a speciﬁc set of rules that\ninvolve word decomposition.\nStep by step, the number of sentences decreased to ap-\nprox. 252000 (only 19% of sentences passed the clean-\ning and correction phase). Interestingly, most lines\nthat were removed were because they had numbers\n(d) or did not contain the minimum percent of words\nin the lexicon (g). On this set of sentences we applied\nthe triphones balancing algorithm described next. To\nkeep the number of triphones from each type as bal-\nanced as possible (a perfect balancing is not possible\nbecause there are triphones that are intrinsically rare)\nwe have applied the following algorithm:\n1. Compile an initial frequency of triphones from\nthe whole corpus;\n2. If a sentence contained a rare triphones (with a\nfrequency below 100), keep it;\n3. If a sentence contained only very frequent tri-\nphones (with frequencies over the H index of the\ninitial distribution), discard it.\n4. Default action: keep the sentence.\n5. Finally, sort the sentences according to the least\ncommon triphones ﬁrst: this will ensure a bal-\nanced corpus from the start, no matter how many\nsentences we record out of the entire corpus.\n3.1.2.\nRecording details\nThe corpus was recorded in studio conditions by\ntwo professional speakers (male and female).\nThis\nspeech corpus is freely available for download and\nuse. It is composed of 6h:30m:23s (female speakers)\nand 6h:03m:46s (male speakers) and the archive con-\ntains the speech prompts (one ﬁle each), correspond-\ning audio ﬁles, phonetic transcription lexicon and\ntime-aligned phoneme sequences for each prompt-\naudio pair. Table 1 shows a quantitative evaluation\nof the speech corpus.\nFor a qualitative evaluation, we provide statistical\nparametric speech synthesis models that are com-\npatible with our platform, both for the STRAIGHT\nTable 1: Individual speaker statistics extracted from the phoneme-level aligned speech corpus\nSpeaker 1 (female)\nSpeaker 2 (female)\nSpeaker 3 (male)\nSpeaker 4 (male)\nPhoneme\nOcc\nTot. length\nOcc\nTot. length\nOcc\nTot. length\nOcc\nTot. length\n@\n3301\n225150\n3963\n278330\n4529\n277930\n2540\n177530\na\n10942\n951670\n14932\n1376420\n17279\n1384770\n5004\n409820\na@\n1754\n91750\n2097\n123630\n2376\n130300\n1094\n67540\nb\n992\n73270\n1490\n124170\n1682\n133220\n365\n26670\nch\n1539\n164230\n1965\n227920\n2206\n262139\n839\n72760\nd\n3624\n213640\n4897\n318940\n5652\n362660\n1946\n127410\ndz\n332\n32940\n634\n70390\n731\n78530\n125\n9740\ne\n11701\n770900\n14678\n976310\n16991\n1053870\n5390\n362330\ne@\n1042\n54360\n1364\n70680\n1598\n81770\n564\n32550\nf\n1482\n138980\n1734\n185380\n2012\n216239\n925\n85220\ng\n904\n60610\n1193\n98320\n1376\n102870\n436\n31750\nh\n280\n29320\n393\n46449\n488\n61060\n43\n3210\ni\n7596\n484880\n9737\n685550\n11240\n718580\n3436\n235340\nij\n1655\n76460\n2028\n111459\n2388\n140870\n693\n44200\nj\n2030\n133060\n2943\n204670\n3366\n260040\n1042\n72040\nk\n4418\n368310\n5473\n436100\n6373\n530070\n2128\n152020\nl\n5139\n277920\n7111\n400320\n8172\n476540\n1850\n101680\nm\n2985\n228550\n3800\n313770\n4382\n370190\n2057\n145020\nn\n6886\n378840\n9191\n482750\n10657\n655190\n3286\n187470\no\n4471\n354630\n5794\n450240\n6631\n442470\n1804\n146520\no@\n413\n27270\n491\n35310\n600\n33560\n227\n13200\np\n3394\n266380\n4246\n364239\n4894\n424840\n1481\n114340\npau\n1985\n526943\n2186\n3373752\n2490\n3883761\n1412\n1541130\nr\n7965\n351600\n10428\n483170\n12079\n569870\n3200\n137040\ns\n4351\n424560\n5672\n575170\n6525\n739510\n2096\n183390\nsh\n1145\n122640\n1543\n180770\n1848\n235920\n702\n64010\nsp\n5216\n243840\n3739\n736331\n5023\n1029306\n1700\n137580\nt\n7045\n498910\n8613\n619090\n9965\n729590\n3380\n231820\nts\n1365\n136910\n1640\n171210\n1895\n205560\n705\n59370\nu\n5855\n359180\n7680\n474569\n8876\n464020\n2822\n166780\nv\n1427\n97560\n1641\n143560\n1906\n139430\n711\n47320\nw\n703\n58710\n846\n74960\n1000\n87060\n113\n9610\nz\n922\n85960\n1107\n103760\n1242\n123720\n453\n39990\nzh\n410\n44910\n475\n56680\n551\n69340\n89\n8470\nTotal (h)\n2.32\n4\n4.58\n1.46\nOverall (h)\n12.36\nand MLSA vocoders.\nIn the near future we in-\ntend to extend our speech synthesis platform to sup-\nport WORLD (Morise et al., 2016) for real-time high-\nquality vocoding and we will include pre-trained\nmodels as well. Currently SSLA can be queried on-\nline5 for speech synthesis using one male and one fe-\nmale voice.\n3.2.\nThe speech recognition corpus\nAs earlier stated, the speech recognition corpus is\ncomposed of two subsections: the non-free sections\nwhich contains recordings from the RADOR agency\nand a collection of audio-books provided by IIT and\nthe free-section which was internally created based\non volunteers who recorded utterances from a prede-\nﬁned set of data. The quality of the recordings varies\nwithin the entire speech corpus, from sampling rate to\n5http://slp.racai.ro/index.php/ssla/\nnoise conditions. The lowest recording sample rate is\n16Khz and the highest is 48Khz. In terms of recording\nconditions we have studio recordings, semi-studio\nrecordings (high quality equipments but no hemiane-\nchoic room) and standard desktop/laptop/headset\nrecording equipment in noisy environments.\nThe corpus is sentence-split and each sentence is time-\naligned with the speech data at phoneme level. Also\nwe keep internal an internal-track of the source and\nrecording conditions for every sentence. However, in\nthis paper we will only provide quantitative informa-\ntion regarding the corpora composition divided be-\ntween the two sections: free and non-free.\nThe corpus construction is still an on-going work.\nAside from the data described in Table 2 we will en-\nhance the free section of the corpus with at least an-\nother 20 hours of speech data, which is currently be-\ning processed.\nTable 2: Phoneme distribution and duration for the two sections of the ASR corpus: free and non-free\nPhoneme\nNon-free\nFree\nOccurences\nTotal duration\nMean dur.\nOccurences\nTotal duration\nMean dur.\n@\n52117\n4108212\n78,83\n73516\n5580501\n75,91\na\n168665\n14646891\n86,84\n248419\n21268426\n85,62\na@\n25805\n2158373\n83,64\n36451\n2896279\n79,46\nb\n13971\n872960\n62,48\n20620\n1302030\n63,14\nch\n26430\n2431859\n92,01\n38491\n3522853\n91,52\nd\n58951\n3436241\n58,29\n85621\n5032471\n58,78\ndz\n4062\n332270\n81,80\n5985\n478950\n80,03\ne\n186060\n13077767\n70,29\n271792\n18861070\n69,40\ne@\n14495\n638588\n44,06\n21093\n934677\n44,31\nf\n17927\n1548020\n86,35\n26928\n2253210\n83,68\ng\n10674\n657900\n61,64\n15900\n991005\n62,33\nh\n1559\n117870\n75,61\n2259\n165015\n73,05\ni\n109493\n6623230\n60,49\n163033\n9928170\n60,90\nij\n30917\n1949659\n63,06\n44094\n2680152\n60,78\nj\n32366\n2138951\n66,09\n47568\n3135555\n65,92\nk\n64171\n4671150\n72,79\n92329\n6786150\n73,50\nl\n75989\n3479229\n45,79\n113536\n5122048\n45,11\nm\n52091\n3614139\n69,38\n74572\n5106568\n68,48\nn\n109934\n5741698\n52,23\n159700\n8360862\n52,35\no\n69089\n5166650\n74,78\n102393\n7529220\n73,53\no@\n6956\n370200\n53,22\n9781\n481665\n49,24\np\n53568\n3810840\n71,14\n78439\n5634345\n71,83\npau\n21672\n6198915\n286,03\n33459\n10481797\n313,27\nr\n119344\n5049519\n42,31\n176946\n7288335\n41,19\ns\n70045\n6348160\n90,63\n101028\n8991750\n89,00\nsh\n20623\n2118431\n102,72\n29284\n2957326\n100,99\nsp\n50862\n12363894\n243,09\n72142\n15154882\n210,07\nt\n109401\n7008668\n64,06\n160231\n10399795\n64,91\nts\n19940\n1811200\n90,83\n29230\n2653155\n90,77\nu\n88979\n5529020\n62,14\n130866\n7798155\n59,59\nv\n21825\n1309280\n59,99\n31038\n1862353\n60,00\nw\n11053\n881050\n79,71\n16323\n1225815\n75,10\nz\n15409\n1203250\n78,09\n23503\n1807170\n76,89\nzh\n3654\n335310\n91,77\n5407\n489360\n90,50\nTotal (hours)\n36.59\n52.54\nOverall (hours)\n89.14\nAs mentioned, the data varies in quality across the\nentire speech corpus.\nIn order to test if this cor-\npus is relevant at all for automatic speech recogni-\ntion we constructed a character-level (not phoneme-\nlevel) speech recognition system which uses Mel-\ngeneralized cepstral coefﬁcients extracted using a 5-\nms sliding window, which are fed into a two layer\nbidirectional LSTM (400 cells in each direction – to-\ntal 800 cells per layer) on top of which we use a\nsoftmax layer, trained using Connectionist Temporal\nClassiﬁcation (CTC) loss(Graves et al., 2013).\nThis\nsystem architecture combined with a RNN language\nmodel for word segmentation will be fully described\nin our future work. However, we must state that af-\nter 4 training epochs on the entire training dataset,\nwe obtained a character-level accuracy rate of 89.52%.\nTo our knowledge, this is the only character-level\nspeech recognition system for Romanian and the re-\nsults, which are consistent with those reported for\nother languages, show that this corpus can indeed be\nused to train ASR systems.\nWhereas we are unable to say anything about the ﬁ-\ndelity of the transcriptions for the non-free section,\nour speech data is carefully crafted and the error\ncount is surely low. Additionally, our transcriptions\ntake into account recoding and speech artifacts (noise,\nlaughter, caught etc.) as well as foreign words (which\nare transliterated) and regional accents (for which\nwe account by introducing the academic form of the\nword and the transliterated version that follows the\nactual pronunciation).\n4.\nFuture development plans\nThere are three main directions we want to proceed\nto in the near future: (a) extension of the tool-set; (b)\npre-training models and (c) creation of additional re-\nsources for Romanian.\nExtension of the tool-set: For the NLP module we\nseek to introduce a graph-based dependency parser\nwhich uses a complex network architecture composed\nof stacked Bidirectional LSTMs for feature extrac-\ntion and a multilayer perceptron for word-arc scor-\ning, similar to the approaches proposed in Dozat\nand Manning (2016) and Kiperwasser and Goldberg\n(2016). Also, based on the success of deep-learning\napplied to TTS (Oord et al., 2016) we plan to ex-\ntend our speech synthesis back-end to include neural\nspeech synthesis support;\nPre-trained models: Depending on the language and\ntraining corpora size and composition, all models\nrequire some ﬁne-tunning, weather we are talking\nabout model hyper-parameters for neural-networks\nor feature-combinations for linear models. As such,\nwe plan to provide pre-trained NLP models for all\nlanguages included in the Universal Dependencies\nTreebank (Nivre et al., 2017)\nText-to-speech corpora:\nDuring our subjective in-\nternal evaluation of the speech models we noticed\nthat the TTS system had a poor quality (in terms of\nprosody) when used in dialogue-style conversations.\nIntuitively, this is because neither the Wikipedia sec-\ntion nor the Audiobook section did include short dia-\nlogue sentences in our speech corpus. However, this\ntype of interaction is typical for assistive systems, thus\nour future development plans include the extension\nof the speech corpus and inclusion of short dialogue\nsentences.\nSpeech recognition corpora: As already mentioned,\nwe are still working on extending our speech recogni-\ntion corpus with new data.\n5.\nConclusions\nWe have presented two ready-to-use tools and a\nspeech resource that enable to construction and de-\nployment of NLP and TTS applications in low-\nresourced environments. Of course, every component\nis independent and can be used in a standalone sce-\nnario to provide functionality (NLP or TTS tool) or to\nbe used as input in training other systems.\nThe speech corpus is intended for Romanian, but the\ntools can be trained for any language.\nIn fact, our\ndemo shows how we trained NLP support for more\nthan 50 languages 6.\nThe tool set is available for download (code and bi-\nnaries)7 and was tested both on desktop/server envi-\nronments as well as on mobile devices (Android 5 and\n6).\nFurthermore, on request, we are happy to provide\nmore pre-trained TTS and NLP models that are not\ncurrently available on the website.\nAcknowledgments\nThe corpora construction work described in this pa-\nper was supported though the the Heimdallr project,\n6http://slp.racai.ro/mlpla-new\n7http://slp.racai.ro/\nwhich is funded by the Romanian Government\nthrough the Executive Agency for Higher Educa-\ntion, Research, Development and Innovation Fund-\ning (UEFISCDI), programme \"Experimental demon-\nstration project (PED) PED-2016\", project ID: PN-III-\nP2-2.1-PED-2016-1974, contract number 229PED.\nWe also want to thank all contributors and volunteers\nwho supported us through recordings performed on\nthe Romanian Anonymous Speech Corpus (RASC)8\nplatform.\n6.\nBibliographical References\nDozat, T. and Manning, C. D. (2016). Deep biafﬁne\nattention for neural dependency parsing.\narXiv\npreprint arXiv:1611.01734.\nDozat, T., Qi, P., and Manning, C. D. (2017). Stan-\nford’s graph-based neural dependency parser at the\nconll 2017 shared task. In Proceedings of the CoNLL\n2017 Shared Task: Multilingual Parsing from Raw Text\nto Universal Dependencies, pages 20–30, Vancouver,\nCanada, August. Association for Computational\nLinguistics.\nDumitrescu, S. D., Boro¸s, T., and Tuﬁ¸s, D. (2017).\nRacai’s natural language processing pipeline for\nuniversal dependencies. In Proceedings of the CoNLL\n2017 Shared Task: Multilingual Parsing from Raw Text\nto Universal Dependencies, pages 174–181, Vancou-\nver, Canada, August. Association for Computa-\ntional Linguistics.\nGraves, A., Mohamed, A.-r., and Hinton, G. (2013).\nSpeech recognition with deep recurrent neural net-\nworks.\nIn Acoustics, speech and signal processing\n(icassp), 2013 ieee international conference on, pages\n6645–6649. IEEE.\nHuggins-Daines, D., Kumar, M., Chan, A., Black,\nA. W., Ravishankar, M., and Rudnicky, A. I.\n(2006). Pocketsphinx: A free, real-time continu-\nous speech recognition system for hand-held de-\nvices.\nIn Acoustics, Speech and Signal Processing,\n2006. ICASSP 2006 Proceedings. 2006 IEEE Interna-\ntional Conference on, volume 1, pages I–I. IEEE.\nImai, S., Sumita, K., and Furuichi, C. (1983). Mel\nlog spectrum approximation (mlsa) ﬁlter for speech\nsynthesis. Electronics and Communications in Japan\n(Part I: Communications), 66(2):10–18.\nIon, R. (2007a). Ttl: A portable framework for tok-\nenization, tagging and lemmatization of large cor-\npora. Bucharest: Romanian Academy.\nIon, R. (2007b). Word sense disambiguation methods\napplied to english and romanian. PhD thesis. Roma-\nnian Academy, Bucharest.\nKawahara,\nH.,\nMasuda-Katsuse,\nI.,\nand\nDe Cheveigne, A. (1999). Restructuring speech rep-\nresentations using a pitch-adaptive time–frequency\nsmoothing and an instantaneous-frequency-based\nf0 extraction: Possible role of a repetitive structure\nin sounds. Speech communication, 27(3):187–207.\n8http://rasc.racai.ro/\nKiperwasser, E. and Goldberg, Y.\n(2016).\nSimple\nand accurate dependency parsing using bidirec-\ntional lstm feature representations. arXiv preprint\narXiv:1603.04351.\nMorise, M., Yokomori, F., and Ozawa, K.\n(2016).\nWorld:\nA\nvocoder-based\nhigh-quality\nspeech\nsynthesis system for real-time applications.\nIE-\nICE TRANSACTIONS on Information and Systems,\n99(7):1877–1884.\nNivre, J., Agi´c, Ž., Ahrenberg, L., et al. (2017). Uni-\nversal Dependencies 2.0. LINDAT/CLARIN digi-\ntal library at the Institute of Formal and Applied\nLinguistics, Charles University, Prague.\nOord, A. v. d., Dieleman, S., Zen, H., Simonyan,\nK., Vinyals, O., Graves, A., Kalchbrenner, N., Se-\nnior, A., and Kavukcuoglu, K. (2016). Wavenet:\nA generative model for raw audio. arXiv preprint\narXiv:1609.03499.\nTuﬁs, D., Mititelu, V. B., Irimia, E., Dumitrescu, S. D.,\nBoros, T., Teodorescu, H. N., Cristea, D., Scutel-\nnicu, A., Bolea, C., Moruz, A., et al. (2015). Corola\nstarts blooming–an update on the reference corpus\nof contemporary romanian language. Challenges in\nthe Management of Large Corpora (CMLC-3), page 5.\nYoung, S., Evermann, G., Gales, M., Hain, T., Ker-\nshaw, D., Liu, X., Moore, G., Odell, J., Ollason, D.,\nPovey, D., et al. (2002). The htk book. Cambridge\nuniversity engineering department, 3:175.\nZaﬁu,\nA.,\nDumitrescu,\nS.\nD.,\nand\nBoros,,\nT.\n(2015). Modular language processing framework\nfor lightweight applications (mlpla). In 7th Lan-\nguage & Technology Conference.\nZeman, D., Ginter, F., Hajiˇc, J., Nivre, J., Popel, M.,\nStraka, M., and et al. (2017). CoNLL 2017 Shared\nTask: Multilingual Parsing from Raw Text to Uni-\nversal Dependencies. In Proceedings of the CoNLL\n2017 Shared Task: Multilingual Parsing from Raw Text\nto Universal Dependencies, pages 1–20. Association\nfor Computational Linguistics.\nZen, H., Nose, T., Yamagishi, J., Sako, S., Masuko, T.,\nBlack, A. W., and Tokuda, K. (2007). The hmm-\nbased speech synthesis system (hts) version 2.0. In\nSSW, pages 294–299.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-02-15",
  "updated": "2018-02-15"
}