{
  "id": "http://arxiv.org/abs/1802.05944v2",
  "title": "Monte Carlo Q-learning for General Game Playing",
  "authors": [
    "Hui Wang",
    "Michael Emmerich",
    "Aske Plaat"
  ],
  "abstract": "After the recent groundbreaking results of AlphaGo, we have seen a strong\ninterest in reinforcement learning in game playing. General Game Playing (GGP)\nprovides a good testbed for reinforcement learning. In GGP, a specification of\ngames rules is given. GGP problems can be solved by reinforcement learning.\nQ-learning is one of the canonical reinforcement learning methods, and has been\nused by (Banerjee & Stone, IJCAI 2007) in GGP. In this paper we implement\nQ-learning in GGP for three small-board games (Tic-Tac-Toe, Connect Four, Hex),\nto allow comparison to Banerjee et al. As expected, Q-learning converges,\nalthough much slower than MCTS. Borrowing an idea from MCTS, we enhance\nQ-learning with Monte Carlo Search, to give QM-learning. This enhancement\nimproves the performance of pure Q-learning. We believe that QM-learning can\nalso be used to improve performance of reinforcement learning further for\nlarger games, something which we will test in future work.",
  "text": "Monte Carlo Q-learning for General Game\nPlaying\nHui Wang, Michael Emmerich, Aske Plaat\nLeiden Institute of Advanced Computer Science, Leiden University,\nLeiden, the Netherlands\nh.wang.13@liacs.leidenuniv.nl\nhttp://www.cs.leiden.edu\nAbstract. After the recent groundbreaking results of AlphaGo, we have\nseen a strong interest in reinforcement learning in game playing. General\nGame Playing (GGP) provides a good testbed for reinforcement learn-\ning. In GGP, a speciﬁcation of games rules is given. GGP problems can\nbe solved by reinforcement learning. Q-learning is one of the canoni-\ncal reinforcement learning methods, and has been used by (Banerjee &\nStone, IJCAI 2007) in GGP. In this paper we implement Q-learning in\nGGP for three small-board games (Tic-Tac-Toe, Connect Four, Hex), to\nallow comparison to Banerjee et al. As expected, Q-learning converges,\nalthough much slower than MCTS. Borrowing an idea from MCTS, we\nenhance Q-learning with Monte Carlo Search, to give QM-learning. This\nenhancement improves the performance of pure Q-learning. We believe\nthat QM-learning can also be used to improve performance of reinforce-\nment learning further for larger games, something which we will test in\nfuture work.\nKeywords: Reinforcement Learning, Q-learning, General Game Play-\ning, Monte Carlo Search\n1\nIntroduction\nTraditional game playing programs are written to play a single speciﬁc game,\nsuch as Chess, or Go. The aim of General Game Playing [1] (GGP) is to cre-\nate adaptive game playing programs; programs that can play more than one\ngame well. To this end, GGP applies a so-called Game Description Language\n(GDL) [2]. GDL-authors write game-descriptions that specify the rules of a game.\nThe challenge for GGP-authors is to write a GGP player that will play any game\nwell. GGP players should ensure that a wide range of GDL-games can be run\neﬃciently. Comprehensive tool-suites exist to help researchers write GGP and\nGDL programs, and an active research community exists [3,4,5].\nThe GGP model follows the state/action/result paradigm of reinforcement\nlearning [6], a paradigm that has yielded many successful problem solving al-\ngorithms. For example, the recent successes of AlphaGo are based on two rein-\nforcement learning algorithms, Monte Carlo Tree Search (MCTS) [7] and Deep\nQ-learning (DQN) [8,9]. MCTS, in particular, has been successful in GGP [10].\narXiv:1802.05944v2  [cs.AI]  21 May 2018\n2\nHui Wang et al.\nThe AlphaGo successes have also shown that for Q-learning much compute\npower is needed, something already noted in Banerjee [11], who reported slow\nconvergence for Q-learning. Following Banerjee, in this paper we address the\nconvergence speed of Q-learning. We use three 2-player zero-sum games: Tic-\nTac-Toe, Hex and Connect Four, and table-based Q-learning. Borrowing an idea\nfrom MCTS, we then create a new version of Q-learning,1 inserting Monte Carlo\nSearch (MCS) into the Q-learning loop.\nOur contributions can be summarized as follows:\n1. We evaluate the classical Q-learning algorithm, ﬁnding (1) that Q-learning\nworks in GGP, and (2) that classical Q-learning converges slowly in compar-\nison to MCTS.\n2. To improve performance, and in contrast to [11], we enhance Q-learning by\nadding a modest amount of Monte Carlo lookahead (QMPlayer) [12]. This\nimproves the rate of convergence of Q-learning.\nThe paper is organized as follows. Section 2 presents related work and re-\ncalls basic concepts of GGP and reinforcement learning. Section 3 provides the\ndesign of a player for single player games. We further discuss the player to play\ntwo-player zero-sum games, and implement such a player (QPlayer). Section 4\npresents the player, inserting MCS, into Q-learning (QMPlayer). Section 5 con-\ncludes the paper and discusses directions for future work.\n2\nRelated Work and Preliminaries\n2.1\nGGP\nA General Game Player must be able to accept formal GDL descriptions of\na game and play games eﬀectively without human intervention [4]. A Game\nDescription Language (GDL) has been deﬁned to describe the game rules [13].\nAn interpreter program [5] generates the legal moves (actions) for a speciﬁc board\n(state). Furthermore, a Game Manager (GM) is at the center of the software\necosystem. The GM interacts with game players through the TCP/IP protocol\nto control the match. The GM manages game descriptions and matches records\nand temporary states of matches while the game is running. The system also\ncontains a viewer interface for users who are interested in running matches and\na monitor to analyze the match process.\n2.2\nReinforcement Learning\nSince Watkins proposed Q-learning in 1989 [14], much progress has been made\nin reinforcement learning [15,16]. However, only few works report on the use\n1 Despite the success of deep-learning techniques in the ﬁeld of game playing, we\nconsider it to be valuable to develop more light-weight, table-based, machine learning\ntechniques for smaller board games. Such light-weight techniques would have the\nadvantage of being more accessible to theoretical analysis and of being more eﬃcient\nwith respect to computational resources.\nMonte Carlo Q-learning for General Game Playing\n3\nof Q-learning in GGP. In [11], Banerjee and Stone propose a method to create\na general game player to study knowledge transfer, combining Q-learning and\nGGP. Their aim is to improve the performance of Q-learning by transferring\nthe knowledge learned in one game to a new, but related, game. They found\nknowledge transfer with Q-learning to be expensive. In our work, instead, we\nuse Monte Carlo lookahead to get knowledge directly, in a single game.\nRecently, DeepMind published work on mastering Chess and Shogi by self-\nplay with a deep generalized reinforcement learning algorithm [18]. With a se-\nries of landmark publications from AlphaGo to AlphaZero [9,17,18], these works\nshowcase the promise of general reinforcement learning algorithms. However,\nsuch learning algorithms are very resource intensive and typically require spe-\ncial GPU hardware. Further more, the neural network-based approach is quite\ninaccessible to theoretical analysis. Therefore, in this paper we study perfor-\nmance of table-based Q-learning.\nIn General Game Playing, variants of MCTS [7] are used with great suc-\ncess [10]. M´ehat et al. combined UCT and nested MCS for single-player general\ngame playing [19]. Cazenave et al. further proposed a nested MCS for two-player\ngames [20]. Monte Carlo techniques have proved a viable approach for searching\nintractable game spaces and other optimization problems [21]. Therefore, in this\npaper we combine MCS to improve performance.\n2.3\nQ-learning\nA basic distinction of reinforcement learning methods is that of ”on-policy” and\n”oﬀ-policy” methods. On-policy methods attempt to evaluate or improve the\npolicy that is used to make decisions, whereas oﬀ-policy methods evaluate or\nimprove a policy diﬀerent from that used to make decisions [6]. Q-learning is\nan oﬀ-policy method. The reinforcement learning model consists of an agent, a\nset of states S, and a set of actions A of every state s, s ∈S [6]. In Q-learning,\nthe agent can move to the next state s′, s′ ∈S from state s after following\naction a, a ∈A, denoted as s\na−→s′. After ﬁnishing the action a, the agent gets\na reward, usually a numerical score, which is to be maximized (highest reward).\nIn order to achieve this goal, the agent must ﬁnd the optimal action for each\nstate. The reward of current state s by taking the action a, denoted as Q(s, a),\nis a weighted sum, calculated by the immediate reward R(s, a) of moving to the\nnext state and the maximum expected reward of all future states’ rewards:\nQ(s, a) = R(s, a) + γ(maxa′Q(s′, a′))\n(1)\nwhere a′ ∈A′, A′ is the set of actions under state s′. γ is the discount factor\nof maxQ(s′, a′) for next state s′. Q(s, a) can be updated by online interactions\nwith the environment using the following rule:\nQ(s, a) ←(1 −α)Q(s, a) + α(R(s, a) + γ(maxa′Q(s′, a′)))\n(2)\nwhere α ∈[0, 1] is the learning rate. The Q-values are guaranteed to converge\nby some schemas, such as exploring every (s, a), which should be ensured by a\nsuitable exploration and exploitation method (such as ϵ-greedy).\n4\nHui Wang et al.\n3\nClassical Q-learning\n3.1\nExploration/Exploitation: ϵ-greedy\nAs our baseline we use ϵ-greedy Q-learning [15] to balance exploration and ex-\nploitation. In order to ﬁnd a better baseline player, we create ϵ-greedy Q-learning\nplayers(α = 0.1, γ = 0.9) with ﬁxed ϵ=0.1, 0.2 and dynamically decreasing\nϵ ∈[0, 0.5] to play 30000 matches against Random player, respectively. During\nthese 30000 matches, dynamic ϵ decreases from 0.5 to 0, ﬁxed ϵ are 0.1, 0.2,\nrespectively. After 30000 matches, ﬁxed ϵ is also set to 0 to continue the compe-\ntition. Results in Fig.1 show that dynamically decreasing ϵ performs better. We\nsee that the ﬁnal win rate of dynamically decreasing ϵ is 4% higher than ﬁxed\nϵ=0.1 and 7% higher than ﬁxed ϵ=0.2.\n \nFig. 1. Win Rate of Fixed and Dynamic ϵ Player vs Random in Tic-Tac-Toe. In the\nwhite part, the player uses ϵ-greedy to learn; in the grey part, all players set ϵ=0 (stable\nperformance)\nTo enable comparison with previous work, we compare TD(λ), the baseline\nlearner of [11](α = 0.3, γ = 1.0, λ = 0.7, ϵ = 0.01), and our baseline learner(α =\n0.1, γ = 0.9, ϵ ∈[0, 0.5], Algorithm 1). For Tic-Tac-Toe, from Fig.2, we ﬁnd that\nalthough the TD(λ) player converges more quickly initially (win rate stays at\nabout 75.5% after 9000th match), our baseline performs better when the value\nof ϵ-greedy decreases dynamically with the learning process.\nMonte Carlo Q-learning for General Game Playing\n5\n \nFig. 2. Win Rate of ϵ-greedy Q-learning and [11] Baseline Player vs Random in Tic-\nTac-Toe. In the white part, the player uses ϵ-greedy to learn; in the grey part, all\nplayers set ϵ=0 (stable performance)\nIn our dynamic implementation, we use the function\nϵ(m) =\n(\na(cos( m\n2l π)) + b\nm ≤l\n0\nm > l\nfor ϵ, where m is the current match count, l is the total learning match, which\nwe set in advance. a and b can be set to limit the range of ϵ, where ϵ ∈[b, a + b],\na, b ≥0 and a + b ≤1. The player generates a random number num where\nnum ∈[0, 1]. If num < ϵ, the player will explore a new action randomly, else the\nplayer will choose a best action from the currently learnt Q(s, a) table.\nThe question is what to do when the player cannot get a record from Q(s, a)?\nIn classical Q-learning, it chooses an action randomly (Algorithm 2). While in\nthe enhanced algorithm, we insert MCS (Algorithm 3) inside Q-learning, giving\nAlgorithm 4. QMPlayer combines MCS and Random strategies in the ﬁrst part\nof the search, but after enough state-action pairs are learned, it performs just\nlike QPlayer.\n3.2\nQ-learning for Single Player Games\nWe start by introducing the simplest algorithm, playing single player games.\nSince games are played by only one player, we just need to build one Q(s, a) table\nfor the player to select the best action under the speciﬁc state, see Algorithm\n1 [6]:\n6\nHui Wang et al.\nAlgorithm 1 Basic Q-learning Player For Single Player Games\nInput:\n1: game state: S;\n2: legal actions:A\n3: learning rate: α\n4: discount factor: γ;\n5: reward: R(S, A);\n6: updating table: Q(S, A);\nOutput:\n7: selected action according to updating table: Q(S, A);\n8: function BasicQlearningSingle(S, A)\n9:\nfor each learning match do\n10:\nfor each game state during match do\n11:\nUpdate Q(s, a) ←(1 −α)Q(s, a) + α(R(s, a) + γmaxa′Q(s′, a′));\n12:\nend for\n13:\nend for\n14:\nselected = false;\n15:\nexpected score = 0;\n16:\nfor each q(s, a) in Q(S, A) do\n17:\nif(current game state equals s and expected score < q(s,a));\n18:\nexpected score = q(s,a);\n19:\nselected action = a;\n20:\nselected = true;\n21:\nend for\n22:\nif selected == false then\n23:\nselected action = Random();\n24:\nend if\n25:\nreturn selected action;\n26: end function\n3.3\nQ-learning for Two-Player Games\nNext, we consider more complex games played by two players. In GGP, the\nswitch-turn command allows every player to play the game roles by turn. The\nQ-learning player should build corresponding Q(s, a) tables for each role.\nSince our GGP games are two-player zero-sum games, we can use the same\nrule, see Algorithm 2 line 13, to create R(s, a) rather than to use a reward\ntable. In our experiments, we set R(s, a) = 0 for non-terminal states, and call\nthe getGoal() function for terminal states. In order to improve the learning\neﬀectiveness, we update the corresponding Q(s, a) table only at the end of the\nmatch.\nMonte Carlo Q-learning for General Game Playing\n7\nAlgorithm 2 ϵ-greedy Q-learning Player For Two-Player Zero-Sum Games\nInput:\n1: game state: S;\n2: legal actions:A;\n3: learning rate: α;\n4: discount factor: γ;\n5: corresponding updating tables: Qmyrole(S, A) for every role in the game;\nOutput:\n6: selected action according to updating table: Qmyrole(S, A);\n7: function epsilonGreedyQlearning(S, A)\n8:\nif ϵ-greedy is enabled then\n9:\nfor each learning match do\n10:\nrecord = getMatchRecord();\n11:\nfor each state from termination to the beginning in record do\n12:\nmyrole = getCurrentRole();\n13:\nR(s,a) = getReward(s,a);//s′ is terminal state? getGoal(s′,myrole):0\n14:\nUpdate\nQmyrole(s, a)\n←\n(1 −α)Qmyrole(s, a) + α(R(s, a) +\nγmaxa′Qmyrole(s′, a′));\n15:\nend for\n16:\nend for\n17:\nselected = false;\n18:\nexpected score = 0;\n19:\nfor each qmyrole(s, a) in Qmyrole(S, A) do\n20:\nif(current game state equals s and expected score < qmyrole(s, a));\n21:\nexpected score = qmyrole(s, a);\n22:\nselected action = a;\n23:\nselected = true;\n24:\nend for\n25:\nif selected == false then\n26:\nselectedaction = Random();\n27:\nend if\n28:\nelse\n29:\nselected action = Random()\n30:\nend if\n31:\nreturn selected action;\n32: end function\nExperiment 1 In our ﬁrst experiment we create QPlayer (see Algorithm 2) to\nplay Tic-Tac-Toe. We set parameters α = 0.1, γ = 0.9, ϵ ∈[0, 0.5], respectively.\nAs it learns to play Tic-Tac-Toe, we vary the total learning match in order to\nﬁnd how many matches it needs to learn to get convergence, and then make it\nplay the game with Random player for 1.5 × total learning match matches. We\nreport results averaged over 5 experiments. The results are shown in Fig.3.\n8\nHui Wang et al.\n \n(a) total learning match=5000\n \n(b) total learning match=10000\n \n(c) total learning match=20000\n \n(d) total learning match=30000\n \n(e) total learning match=40000\n \n(f) total learning match=50000\nFig. 3. Win Rate of QPlayer vs Random Player in Tic-Tac-Toe averaged over 5 ex-\nperiments. The winrate converges, and the variance is reduced as total learning match\nincreases\nFig.3(a) shows that QPlayer has the most unstable performance (the largest\nvariance in 5 experiments) and only wins around 55% matches after training\n5000 matches (i.e., 2500 matches trained for each role). Fig.3(b) illustrates that\nafter training 10000 matches QPlayer wins about 80% matches. However, during\nMonte Carlo Q-learning for General Game Playing\n9\nthe exploration period (the light part of the ﬁgure) the performance is still very\nunstable. Fig.3(c) shows that QPlayer wins about 86% of the matches while\nlearning 20000 matches still with high variance. Fig.3(d), Fig.3(e), Fig.3(f), show\nus that after training 30000, 40000, 50000 matches, QPlayer gets a similar win\nrate, which is nearly 86.5% with smaller and smaller variance.\nOverall, as the total learning match increases, the win rate of QPlayer be-\ncomes higher until leveling oﬀaround 86.5%. The variance becomes smaller and\nsmaller. More intuitively, the QPlayer performance during the full exploitation\nperiod (the convergence results in the dark part of Fig. 3) against diﬀerent total\nlearning match is shown in Fig.4.\n \nFig. 4. Convergence Win Rate of QPlayer vs Random in Tic-Tac-Toe. Win rate con-\nverges as total learning match increases\nFig.4 shows that QPlayer achieves, after convergence, a win rate of around\n86.5% with small variance. These experiments suggest indeed that Q-learning is\napplicable to a GGP system. However, beyond the basic applicability in a single\ngame, we need to show that it can do so (1) eﬃciently, and (2) in more than\none game. Thus, we further experiment with QPlayer to play Hex (learn 50000\nmatches) and Connect Four (learn 80000 matches) against the Random player.\nThe results of these experiments are given in Fig.5 and Fig.6.\n10\nHui Wang et al.\n \nFig. 5. Win Rate of QPlayer vs Random Player in 3×3 Hex, the win rate of Q-learning\nalso converges\n \nFig. 6. Win Rate of QPlayer vs Random Player in 4×4 ConnectFour, the win rate of\nQ-learning also converges\nIn order to limit excessive learning times, following [11], we play Hex on a\nvery small 3×3 board, and play ConnectFour on a 4×4 board. Fig.5 and Fig.6\nshow that QPlayer can also play these other games eﬀectively.\nMonte Carlo Q-learning for General Game Playing\n11\nHowever, there remains the problem that QPlayer should be able to learn to\nplay larger games. The complexity inﬂuences how many matches the QPlayer\nshould learn. We show results to demonstrate how QPlayer performs while play-\ning more complex games. We make QPlayer learn Tic-Tac-Toe 50000 matches\n(75000 for whole competition) in 3×3, 4×4, 5×5 boards respectively and show\nthe results in Fig.7:\n \nFig. 7. Win Rate of QPlayer vs Random in Tic-Tac-Toe on Diﬀerent Board Size. For\nlarger board sizes convergence slows down\nThe results show that with the increase of game board size, QPlayer performs\nworse and for larger boards does not achieve convergence.\n4\nMonte Carlo Q-learning\n4.1\nMonte Carlo Search\nThe main idea of MCS [12] is to make some lookahead probes from a non-\nterminal state to the end of the game by selecting random moves for the players\nto estimate the value of that state. The pseudo code of time limited MCS in\nGGP is shown in Algorithm 3.\n12\nHui Wang et al.\nAlgorithm 3 Time Limited Monte Carlo Search Algorithm\nInput:\n1: game state: S;\n2: legal actions:A;\n3: time limit of each searching t;\nOutput:\n4: The selected action sa, sa ∈A;\n5: function MonteCarloSearch(time limit)\n6:\nsa = A.get(0);//default value of sa is set as the ﬁrst action in A\n7:\nif A.size() > 1 then\n8:\nfor int i = 0; i < A.size(); i = (i+1)%A.size() do\n9:\nif time cost > time limit then\n10:\nbreak;\n11:\nend if\n12:\na = A.get(i);\n13:\nscore = getGoalByPerformingRandomActionsFromNextState(s,a);\n14:\nscore[i] += score;\n15:\nvisit[i] += 1;\n16:\nend for\n17:\nhighest score = 0;\n18:\nbest action index = 0;\n19:\nfor int i = 0; i < A.size(); i++ do\n20:\nexpected score[i] = score[i]/visit[i];\n21:\nif expected score[i] > highest score then\n22:\nhighest score = expected score[i];\n23:\nbest action index = i;\n24:\nend if\n25:\nend for\n26:\nsa = A.get(best action index)\n27:\nend if\n28:\nreturn sa;\n29: end function\n4.2\nInserting MCS inside Q-learning\nWe will now add Monte Carlo Search to Q-learning (Algorithm 4). Starting from\nplain Q-learning, in Algorithm 2 (line 26), we see that a random action is chosen\nwhen QPlayer can not ﬁnd an existing value in the Q(s, a) table. In this case,\nQPlayer acts like a random game player, which will lead to a low win rate and\nslow learning speed. In order to address this problem, we introduce a variant\nof Q-learning combined with MCS. MCS performs a time limited lookahead for\ngood moves. The more time it has, the better the action it ﬁnds will be. See\nAlgorithm 4 (line 26).\nBy adding MCS, we eﬀectively add a local version of the last two stages of\nMCTS to Q-learning: the playout and backup stage [7].\nMonte Carlo Q-learning for General Game Playing\n13\nAlgorithm 4 Monte Carlo Q-learning Player For Two-Player Zero-Sum Games\nInput:\n1: game state: S;\n2: legal actions:A;\n3: learning rate: α;\n4: discount factor: γ;\n5: corresponding updating tables: Qmyrole(S, A) for every role in the game;\nOutput:\n6: selected action according to updating table: Qmyrole(S, A);\n7: function epsilonGreedyMonteCarloQlearning(S, A)\n8:\nif ϵ-greedy is enabled then\n9:\nfor each learning match do\n10:\nrecord = getMatchRecord();\n11:\nfor each state from termination to the beginning in record do\n12:\nmyrole = getCurrentRole();\n13:\nR(s,a) = getReward(s,a);//s′ is terminal state? getGoal(s′,myrole):0\n14:\nUpdate\nQmyrole(s, a)\n←\n(1 −α)Qmyrole(s, a) + α(R(s, a) +\nγmaxa′Qmyrole(s′, a′));\n15:\nend for\n16:\nend for\n17:\nselected = false;\n18:\nexpected score = 0;\n19:\nfor each qmyrole(s, a) in Qmyrole(S, A) do\n20:\nif(current game state equals s and expected score < qmyrole(s, a));\n21:\nexpected score = qmyrole(s, a);\n22:\nselected action = a;\n23:\nselected = true;\n24:\nend for\n25:\nif selected == false then\n26:\nselected action = MonteCarloSearch(time limit); // Algorithm 3\n27:\nend if\n28:\nelse\n29:\nselected action = Random()\n30:\nend if\n31:\nreturn selected action;\n32: end function\nExperiment 2 We will now describe our second experiment. In this experiment,\nwith Monte Carlo-enhanced Q-learning, we use the QMPlayer. See Algorithm 4.\nWe set parameters α = 0.1, γ = 0.9, ϵ ∈[0, 0.5], time limit = 50ms respec-\ntively. For QMPlayer to learn to play Tic-Tac-Toe, we also set the total learning\nmatch=5000, 10000, 20000, 30000, 40000, 50000, respectively, and then make it\nplay the game with Random player for 1.5 × total learning match matches for 5\nrounds. The comparison with QPlayer is shown in Fig.8.\n14\nHui Wang et al.\n \n(a) total learning match=5000\n \n(b) total learning match=10000\n \n(c) total learning match=20000\n \n(d) total learning match=30000\n \n(e) total learning match=40000\n \n(f) total learning match=50000\nFig. 8. Win Rate of QMPlayer and QPlayer vs Random in Tic-Tac-Toe for 5 experi-\nments. Small Monte Carlo lookaheads improve the convergence of Q-learning, especially\nin the early part of the learning. QMPlayer always outperforms Qplayer\nIn Fig.8(a), QMPlayer gets a high win rate(about 67%) at the very beginning.\nThen the win rate decreases to 66% and 65%, and then increases from 65% to\naround 84% at the end of ϵ learning(match=5000). Finally, the win rate stays at\nMonte Carlo Q-learning for General Game Playing\n15\naround 85%. Also in the other sub ﬁgures, for QMPlayer, the trend of all curves\ndecreases ﬁrst and then increase until reaching a stable state. This is because\nat the very beginning, QMPlayer chooses more actions from MCS. Then as the\nlearning period moves forward, it chooses more actions from Q table and achieves\nconvergence.\nNote that in every sub ﬁgure, QMPlayer can always achieve a higher win\nrate than QPlayer, not only at the beginning but also at the end of the learning\nperiod. Overall, QMPlayer achieves a better performance than QPlayer with the\nhigher convergence win rate (at least 87.5% after training 50000 matches). To\ncompare the convergence speeds of QPlayer and QMPlayer, we summarize the\nconvergence win rates of diﬀerent total learning match according to Fig. 3 and\nFig. 8, in Fig.9.\n \nFig. 9. Convergence Win Rate of QMPlayer and QPlayer vs Random in Tic-Tac-Toe\nThese results show that combining MCS with Q-learning for GGP can im-\nprove the win rate both at the beginning and at the end of the learning period.\nThe main reason is that Monte Carlo-enhanced Q-learning allows the Q(s, a)\ntable to be ﬁlled quickly with good actions from MCS, achieving a quick and\ndirect learning rate. It is worth to note that, QMPlayer will spend slightly more\ntime (at most is search time limit× number of (state action) pairs) in training\nthan QPlayer.\n4.3\nComparison with MCTS\nIn order to evaluate the performance of both Q-learning players, we implemented\na basic MCTS player [10]. Referring to the basic MCTS algorithm in [7], we\npresent the pseudo code of basic time limited MCTS in GGP in Algorithm 5.\n16\nHui Wang et al.\nAlgorithm 5 Basic Time Limited Monte Carlo Tree Search Player Algorithm\nFor Two-Player Zero-Sum Games\nInput:\n1: game state: S;\n2: legal actions:A;\n3: empty game tree:tree;\n4: visited nodes: visited;\n5: current node:node;\nOutput:\n6: selected action according to updated game tree;\n7: function MCTS(S, A, time limit)\n8:\nif legal moves.size() == 1 then\n9:\nselected action = legal moves.get(0);\n10:\nelse\n11:\nwhile time cost ≤time limit do\n12:\nwhile !node.isLeaf() do\n13:\nnode = selectNodeByUCTMinMax();\n14:\nvisited.add(node);\n15:\nend while\n16:\nexpandGameTree(); //expand tree based on the number of all next states\n17:\nnode=selectNodeByUCTMinMax();\n18:\nvisited.add(node);\n19:\nbonus = playout(); //simulate from node to terminal state, get a score.\n20:\nbackUpdate(); //for every visited node, count+=1; value+=bonus;\n21:\nvisited.removeAll(visited);//erase visited list\n22:\nend while\n23:\nselected child=getChildWithMaxAverageValue(tree.get(0).children)\n24:\nselected action=getMoveFromParentToChild(selected child);\n25:\nend if\n26:\nreturn selected action;\n27: end function\n28: function selectNodeByUCTMinMax\n29:\nfor each child in node.children do\n30:\nﬂoat uct = child.totalvalue\nchild.visitcount +\nq\nln(node.visitcount+1)\nchild.visitcount\n;\n31:\nif is my turn according to node.game state then\n32:\nif max value < uct then\n33:\nmax value = uct;\n34:\nselected node = child;\n35:\nend if\n36:\nelse\n37:\nif min value > uct then\n38:\nmin value = uct;\n39:\nselected node = child;\n40:\nend if\n41:\nend if\n42:\nend for\n43:\nreturn selected node;\n44: end function\nMonte Carlo Q-learning for General Game Playing\n17\nFirst, we make QPlayer learn 50000 matches. Then we set time limit=10s for\nthe MCTS player to build and update the search tree. For MCS, we also allow 10\nseconds. With this long time limit, they reach perfect play on this small game.\nQPlayer and QM-player, in contrast, only get 50ms MCS time, and cannot reach\nperfect play in this short time period. QPlayer plays against the MCTS player in\nGGP by playing Tic-Tac-Toe for 100 matches. Then we pit other players against\neach other. The most relevant competition results of diﬀerent players mentioned\nin this paper are shown in Table 1. The cells contain the win rate of the column\nplayer against the row player.\nMCTS Random QPlayer QMPlayer MCS\nMCTS\n-\n0.5%\n0\n0\n35%\nRandom\n99.5% -\n86.5%\n87.5%\n100%\nQPlayer\n100%\n13.5%\n-\n-\n-\nQMPlayer 100%\n12.5%\n-\n-\n-\nMCS\n65%\n0\n-\n-\n-\nTable 1. Summary of Win Rate of Diﬀerent Players Against to Each Other. The\nstate space of Tic-Tac-Toe is too small for MCTS, it reaches perfect play. QMPlayer\nout-performs QPlayer\nIn Table 1, we ﬁnd that (1) the state space of Tic-Tac-Toe is too small for\nMCTS, which reaches perfect play (win rate of 100%). Tic-Tac-Toe is suitable\nfor showing the diﬀerence between QPlayer and QMPlayer. (2) MCTS wins 65%\nmatches against QMPlayer since MCTS can win in the ﬁrst hand matches and\nalways get a draw in the second hand matches while playing with MCS. (3) The\nconvergence win rate of QMPlayer(87.5%) against to Random is slightly higher\nthan QPlayer(86.5%).\n5\nConclusion\nThis paper examines the applicability of Q-learning, the canonical reinforce-\nment learning method, to create general algorithms for GGP programs. Firstly,\nwe show how good canonical implementations of Q-learning perform on GGP\ngames. The GGP system allows us to easily use three real games for our ex-\nperiments: Tic-Tac-Toe, Connect Four, and Hex. We ﬁnd that (1) Q-learning\nis indeed general enough to achieve convergence in GGP games. With Baner-\njee [11], however, we also ﬁnd that (2) convergence is slow. Compared against\nthe MCTS algorithm that is often used in GGP [10], performance of Q-learning\nis lacking: MCTS achieves perfect play in Tic-Tac-Toe, whereas Q-learning does\nnot.\nWe then enhance Q-learning with an MCS based lookahead. We ﬁnd that,\nespecially at the start of the learning, this speeds up convergence considerably.\nOur Q-learning is table-based, limiting it to small games. Even with the MCS\n18\nHui Wang et al.\nenhancement, convergence of QM-learning does not yet allow its direct use in\nlarger games. The QPlayer needs to learn a large number of matches to get\ngood performance in playing larger games. The results with the improved Monte\nCarlo algorithm (QM-learning) show a real improvement of the player’s win rate,\nand learn the most probable strategies to get high rewards faster than learning\ncompletely from scratch.\nA ﬁnal result is, that, where Banerjee et al. used a static value for ϵ, we ﬁnd\nthat a value for ϵ that changes with the learning phases gives better performance\n(start with more exploration, become more greedy later on).\nThe table-based implementation of Q-learning facilitates theoretical analysis,\nand comparison against some baselines [11]. However, it is only suitable for small\ngames. A neural network implementation facilitates the study of larger games,\nand allows meaningful comparison to DQN variants [8].\nOur use of Monte Carlo in QM-learning is diﬀerent from the AlphaGo archi-\ntecture, where MCTS is wrapped around Q-learning (DQN) [8]. In our approach,\nwe inserted Monte Carlo within the Q-learning loop. Future work should show\nif our QM-learning results transfer to AlphaGo-like uses of DQN inside MCTS,\nif QM-learning can achieve faster convergence, reducing the high computational\ndemands of AlphaGo [17]. Additionally, we plan to study nested MCS in Q-\nlearning [20]. Implementing Neural Network based players also allows the study\nof more complex GGP games.\nAcknowledgments. Hui Wang acknowledges ﬁnancial support from the China\nScholarship Council (CSC), CSC No.201706990015.\nReferences\n1. Genesereth M, Love N, Pell B: General game playing: Overview of the AAAI\ncompetition. AI magazine 26(2), 62–72 (2005)\n2. Love, Nathaniel and Hinrichs, Timothy and Haley, David and Schkufza, Eric and\nGenesereth, Michael: General game playing: Game description language speciﬁca-\ntion. Stanford Tech Report LG-2006-1 (2008)\n3. Kaiser D M: The Design and Implementation of a Successful General Game Play-\ning Agent. In: David Wilson, GeoﬀSutcliﬀe. International Florida Artiﬁcial In-\ntelligence Research Society Conference 2007, pp. 110–115. AAAI Press, California\n(2007)\n4. Genesereth M, Thielscher M: General game playing. Synthesis Lectures on Artiﬁ-\ncial Intelligence and Machine Learning 8(2), 1–229 (2014)\n5. ´Swiechowski M, Ma´ndziuk J: Fast interpreter for logical reasoning in general game\nplaying. Journal of Logic and Computation 26(5), 1697–1727 (2014)\n6. Sutton R S, Barto A G: Reinforcement learning: An introduction. 2nd edn. MIT\npress, Cambridge (1998)\n7. Browne C B, Powley E, Whitehouse D, et al: A survey of monte carlo tree search\nmethods. IEEE Transactions on Computational Intelligence and AI in games 4(1),\n1–43 (2012)\n8. Mnih V, Kavukcuoglu K, Silver D, et al: Human-level control through deep rein-\nforcement learning. Nature 518(7540), 529–533 (2015)\nMonte Carlo Q-learning for General Game Playing\n19\n9. Silver D, Huang A, Maddison C J, et al: Mastering the game of Go with deep\nneural networks and tree search. Nature 529(7587), 484–489 (2016)\n10. Mehat J, Cazenave T: Monte-carlo tree search for general game playing. Univ.\nParis 8, (2008)\n11. Banerjee B, Stone P: General Game Learning Using Knowledge Transfer. In:\nManuela M. Veloso. International Joint Conference on Artiﬁcial Intelligence 2007,\npp. 672–677. (2007)\n12. Robert C P: Monte carlo methods. John Wiley & Sons, New Jersey (2004)\n13. Thielscher M: The general game playing description language is universal. In: Toby\nWalsh. International Joint Conference on Artiﬁcial Intelligence 2011, vol. 22(1), pp.\n1107–1112. AAAI Press, California (2011)\n14. Watkins C J C H: Learning from delayed rewards. King’s College, Cambridge,\n(1989)\n15. Even-Dar E, Mansour Y: Convergence of optimistic and incremental Q-learning. In:\nThomas G.Dietterich, Suzanna Becker, Zoubin Ghahramani. Advances in neural\ninformation processing systems 2001, pp. 1499–1506. MIT press, Cambridge (2001)\n16. Hu J, Wellman M P: Nash Q-learning for general-sum stochastic games. Journal\nof machine learning research 4, 1039–1069 (2003)\n17. Silver D, Schrittwieser J, Simonyan K, et al: Mastering the game of go without\nhuman knowledge. Nature 550(7676), 354–359 (2017)\n18. Silver D, Hubert T, Schrittwieser J, et al: Mastering Chess and Shogi by\nSelf-Play with a General Reinforcement Learning Algorithm. arXiv preprint\narXiv:1712.01815, (2017).\n19. M´ehat J, Cazenave T: Combining UCT and nested Monte Carlo search for single-\nplayer general game playing. IEEE Transactions on Computational Intelligence\nand AI in Games 2(4), 271–277 (2010)\n20. Cazenave, T., Saﬃdine, A., Schoﬁeld, M. J., & Thielscher, M: Nested Monte Carlo\nSearch for Two-Player Games. In: Dale Schuurmans, Michael P.Wellman. AAAI\nConference on Artiﬁcial Intelligence 2016, vol. 16, pp. 687–693. AAAI Press, Cal-\nifornia (2016)\n21. B Ruijl, J Vermaseren, A Plaat, J Herik: Combining Simulated Annealing and\nMonte Carlo Tree Search for Expression Simpliﬁcation. In: B´eatrice Duval, H.\nJaap van den Herik, St´ephane Loiseau, Joaquim Filipe. Proceedings of the 6th\nInternational Conference on Agents and Artiﬁcial Intelligence 2014, vol. 1, pp.\n724–731. SciTePress, Set´ubal, Portugal (2014)\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2018-02-16",
  "updated": "2018-05-21"
}