{
  "id": "http://arxiv.org/abs/1802.07370v1",
  "title": "SufiSent - Universal Sentence Representations Using Suffix Encodings",
  "authors": [
    "Siddhartha Brahma"
  ],
  "abstract": "Computing universal distributed representations of sentences is a fundamental\ntask in natural language processing. We propose a method to learn such\nrepresentations by encoding the suffixes of word sequences in a sentence and\ntraining on the Stanford Natural Language Inference (SNLI) dataset. We\ndemonstrate the effectiveness of our approach by evaluating it on the SentEval\nbenchmark, improving on existing approaches on several transfer tasks.",
  "text": "Workshop track - ICLR 2018\nSUFISENT - UNIVERSAL SENTENCE REPRESENTA-\nTIONS USING SUFFIX ENCODINGS\nSiddhartha Brahma\nIBM Research, Almaden, USA\nbrahma@us.ibm.com\nABSTRACT\nComputing universal distributed representations of sentences is a fundamental task\nin natural language processing. We propose a method to learn such representa-\ntions by encoding the sufﬁxes of word sequences in a sentence and training on the\nStanford Natural Language Inference (SNLI) dataset. We demonstrate the effec-\ntiveness of our approach by evaluating it on the SentEval benchmark, improving\non existing approaches on several transfer tasks.\n1\nINTRODUCTION\nIn natural language processing, the use of distributed representations has become standard through\nthe effective use of word embeddings. In a wide range of NLP tasks, it is beneﬁcial to initialize the\nword embeddings with ones learnt from large text corpora like word2vec Mikolov et al. (2013) or\nGLoVe Pennington et al. (2014) and tune them as a part of a target task e.g. text classiﬁcation. It\nis therefore a natural question to ask whether such standardized representations of whole sentences\nthat can be widely used in downstream tasks, is possible.\nThere are two classes of approaches to this problem. Taking cue from word2vec, an unsupervised\nlearning approach is taken by SkipThought Kiros et al. (2015) and FastSent Hill et al. (2016). More\nrecently, the work of Conneau et al. (2017) takes a supervised learning approach. They train a\nsentence encoding model on the Stanford Natural Language Inference (SNLI) dataset Bowman et al.\n(2015) and show that the learnt encoding transfers well to to a set of transfer tasks encapsulated in\nthe SentEval benchmark. This is reminiscent of the approach taken by ImageNet Deng et al. (2009)\nin the computer vision community.\nOne of the most effective ways of encoding a sentence s is to pass it through a recurrent neural net-\nwork like a LSTM Hochreiter & Schmidhuber (1997) and use the last hidden state or a combination\nof the intermediate hidden states. Each intermediate state of a LSTM represents an encoding of a\npreﬁx of s. In a bidirectional LSTM, an additional network is used to encode the preﬁxes of the\nreversed sequence of words. Although this is equivalent to encoding the sufﬁxes of s, the sufﬁxes\nare encoded in a direction reverse of the preﬁxes.\nIn this paper, we argue that encoding the sufﬁxes of s in the forward direction can lead to better\nuniversal sentence representations. By Max-pooling the encodings of the preﬁxes and sufﬁxes, we\ndeﬁne a new sentence encoding that is trained on the SNLI dataset. We show through numerical ex-\nperiments that the learned encodings improve upon existing supervised approaches on the SentEval\nbenchmark. We call our sufﬁx based sentence encoding model SUFISENT.\n2\nSUFFIX BASED MODELS\nLet s be a sentence with n words. We will use s[i: j] to denote the sequence of words from s[i] to\ns[j], where i maybe less than j. Let\n⇀Lp represent a LSTM (or any other RNN) that encodes preﬁxes\nof s in the forward direction. For the i-th word, we have\n⇀\nhp,i =\n⇀Lp(s[1: i])\n(1)\nLet\n⇀Ls represent a LSTM that encodes sufﬁxes of s in the forward direction.\n⇀\nhs,i =\n⇀Ls(s[i: n])\n(2)\n1\narXiv:1802.07370v1  [cs.CL]  20 Feb 2018\nWorkshop track - ICLR 2018\n1\ni\nn\nhp,i(Lp)\n⇀\nhs,i(Ls)\n⇀\nhs,i(Ls)\n↼\nhp,i(Lp)\n↼\nPremise\nEncoding v\nHypothesis\nEncoding u\nu\nv\n|u −v|\nu ∗v\nFC\nLayer\n3-way\nSoftmax\n⇀\n⇀\n↼\n↼\n(a)\n(b)\nFigure 1: (a) Schematics of SUFISENT terms (b) Training architecture on SNLI dataset\nNote that the\n⇀\nhp,i can be computed in a single pass over s, while computing\n⇀\nhs,i needs a total of n\npasses over progressively smaller sufﬁxes of s. As in bidirectional LSTMs, we also consider\n↼Lp and\n↼Lp that encodes the preﬁxes and sufﬁxes of s in the backward direction.\n↼\nhs,i =\n↼Ls(s[i: 1]),\n↼\nhp,i =\n↼Lp(s[n: i]),\n(3)\nNote that\n⇀\nhp,i encodes the same subsequence as\n↼\nhs,i, but in different directions. See Fig.1(a) for\na schematic illustration. Let d be the dimension of the hidden state of each of\n⇀Lp,\n⇀Ls,\n↼Lp,\n↼Ls. We\nconsider the following sentence encodings.\n• SUFISENT - We max-pool the\n⇀\nhp,i over all i ∈[1 : n] to obtain\n⇀\nhp and max-pool the\n⇀\nhs,i to\nobtain\n⇀\nhs. Similarly, we obtain\n↼\nhp and\n↼\nhs by max-pooling over\n↼\nhp,i and\n↼\nhs,i respectively.\nThe ﬁnal encoding is a concatenation of max(\n⇀\nhp,\n⇀\nhs) and max(\n↼\nhp,\n↼\nhs). The sentence\nencoding is of size 2d. In contrast, a BiLSTM-Max model is a concatenation of\n⇀\nhp and\n↼\nhp.\n• SUFISENT-TIED - This is same as above, but the weights of\n⇀Lp and\n⇀Ls are shared or tied.\nSimilarly, the weights of\n↼Lp and\n↼Ls are tied. The sentence encoding is of size 2d.\n• SUFISENT-CAT - Similar to SUFISENT, we compute\n⇀\nhp,\n⇀\nhs,\n↼\nhp,\n↼\nhs. The sentence encod-\ning is the concatenation of these four vectors and is of size 4d.\n• SUFISENT-CAT-TIED - This is same as SUFISENT-CAT, except the weights of (\n⇀Lp,\n⇀Ls)\nand (\n↼Lp,\n↼Ls) are tied. The size of the encoding is 4d.\nWe use the SNLI dataset as the supervised dataset to train the encodings. SNLI is a large scale\nlabelled dataset consisting of pairs of sentences (premise and hypothesis) and each pair is labeled\nby one of three labels - entailment, contradiction and neutral. As shown in Fig. 1(b), for each of the\nSUFISENT models, the encodings of the premise and hypothesis sentences are computed as u and v.\nFollowing Mou et al. (2016), a feature vector consisting of u, v, |u −v| and u ∗v is fed into a fully\nconnected layer(s), before computing the 3-way softmax in the classiﬁcation layer.\n3\nTRAINING AND RESULTS\nThe encodings deﬁned by SUFISENT and SUFISENT-TIED are trained on the SNLI dataset for the\nLSTM hidden dimensions d ∈{256, 512, 1024, 2048}. The SUFISENT-CAT and SUFISENT-CAT-\nTIED encodings are trained for d ∈{128, 256, 512, 1024}. This corresponds to sentence encoding\ndimensions of 512, 1024, 2048, 4096 respectively. The FC layer has two layers of 512 dimensions\neach. For optimization, we use SGD with an initial learning rate of 0.1 which is decayed by 0.99\nafter every epoch or by 0.2 if there is a drop in the validation accuracy. Gradients are clipped to a\nmaximum norm of 5.0.\nWe evaluate the sentence encodings using the SentEval benchmark Conneau et al. (2017). This\nbenchmark consists of 6 text classiﬁcation tasks (MR, CR, SUBJ, MPQA, SST, TREC), one task\non paraphrase detection (MRPC) and one on entailment classiﬁcation (SICK-E). All these 8 tasks\nhave accuracy as their performance measure. There are two tasks (SICK-R and STS14) for which\nthe performance measure is Pearson and Pearson/Spearman correlation respectively. The trained\nencoding models are used to generate initial representations for the sentences in the transfer tasks,\nwhich are then tuned further. For more details, please refer to the above paper.\n2\nWorkshop track - ICLR 2018\nSNLI\nTransfer\nModel\ndim dev test micro macro\nBiLSTM-Mean\n4096 79.0 78.2\n83.1\n81.7\nInner-attention\n4096 82.3 82.5\n82.1\n81.0\nHConvNet\n4096 83.7 83.4\n82.0\n80.9\nBiLSTM-Max.\n4096 85.0 84.5\n85.2\n83.7\nSuﬁSent-Tied\n4096 84.7 84.6\n86.6\n85.1\nSuﬁSent\n4096 84.9 84.3\n86.5\n85.0\nSuﬁSent-Cat-Tied 4096 84.8 84.4\n86.2\n84.5\nSuﬁSent-Cat\n4096 84.6 84.2\n86.3\n84.3\nTable 1.\nPerformance of SUFISENT*\nmodels on the SNLI dataset and on the val-\nidation sets of transfer tasks with accuracy\nas performance. Numbers in ﬁrst four rows\nare taken from Conneau et al. (2017).\n512\n1024\n2048\n4096\n82\n83\n84\n85\n86\n87\nEncoding dimension\nMicro average\nSuﬁSent-Tied\nSuﬁSent\nSuﬁSent-Cat\nSuﬁsent-Cat-Tied\nFigure 2: Scaling of micro average of accu-\nracies on 8 tasks with encoding dimension.\nModel\nMR CR SUBJ MPQA SST TREC\nMRPC\nSICK-R SICK-E STS14\nUnsupervised Training - Ordered Sentences\nFastSent\n70.8 78.4\n88.7\n80.6\n-\n76.8\n72.2/80.3\n-\n-\n.63/.64\nSkipThought\n76.5 80.1\n93.6\n87.1\n82.0\n92.2\n73.0/82.0\n0.858\n82.3\n.29/.35\nSkipThought-LN\n79.4 83.1\n93.7\n89.3\n82.9\n88.4\n-\n0.858\n79.5\n.44/.45\nSupervised Training on SNLI\nInferSent\n79.9 84.6\n92.1\n89.8\n83.3\n88.7\n75.1/82.3\n0.885\n86.3\n.68/.65\nSuﬁSent\n80.3 84.7\n92.8\n90.1\n83.4\n88.0\n75.4/82.9\n0.886\n85.7\n.69/.66\nSuﬁSent-Tied\n80.6 85.4\n92.2\n90.3\n83.1\n88.4\n74.3/82.3\n0.887\n86.3\n.68/.66\nSuﬁSent-Cat\n80.3 84.4\n92.2\n90.2\n81.4\n85.2\n74.6/82.5\n0.883\n86.0\n.66/.63\nSuﬁSent-Cat-Tied 79.8 84.8\n92.3\n90.2\n82.3\n86.6\n74.5/82.5\n0.880\n85.8\n.64/.61\nSupervised Training on AllNLI\nInferSent\n81.1 86.3\n92.4\n90.2\n84.6\n88.2\n76.2/83.1\n0.884\n86.3\n.70/.67\nTable 2. Test set performance over the transfer tasks in SentEval. For MRPC, we report accuracy\nand F1 score. The dimension for the SUFISENT* and Infersent models is 4096. All numbers except\nfor our models are taken from Hill et al. (2016) and Conneau et al. (2017).\nAs can be seen from Table 1 and Fig. 2, among the models proposed in this paper, the SUFISENT-\nTIED model with dimension 4096 has the best test accuracy on the SNLI dataset and also the best\nmacro and micro average of the validation set accuracies in the 8 transfer tasks identiﬁed above. It\nalso performs signiﬁcantly better than the BiLSTM-Max (InferSent) of Conneau et al. (2017), which\nonly uses the max of the preﬁx encodings in both directions. The performance steadily improves\nwith increasing encoding dimension, as shown in Fig. 2. The test set performance of SUFISENT-\nTIED on SNLI improves on InferSent too. SUFISENT and SUFISENT-TIED are close, with the latter\nedging forward in higher dimensions.\nTable 2 compares the test set performance of the SUFISENT models with InferSent on each of the\ntransfer tasks for the encoding dimension of 4096. For the same training set (SNLI), both SUFISENT-\nTIED and SUFISENT improves or matches InferSent on 7 of the 10 tasks. The improvement is partic-\nularly signiﬁcant for MR, CR, SUBJ and MPQA. The performance of models trained on unlabeled\ndata and the InferSent model on the larger AllNLI dataset is also shown for comparison.\nTo conclude, we propose SUFISENT - a new universal sentence encoding that is computed by max-\npooling over the encodings of the sufﬁxes and preﬁxes of sentences, in both the forward and back-\nward directions. Preliminary results obtained by training on the SNLI dataset shows promise, im-\nproving over existing approaches on many transfer tasks in the SentEval benchmark. In future work,\nwe plan to train SUFISENT on the larger AllNLI dataset, explore its use in other NLP tasks as a basic\nrepresentation primitive and address computational efﬁciency issues.\n3\nWorkshop track - ICLR 2018\nREFERENCES\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno-\ntated corpus for learning natural language inference. In EMNLP, 2015.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc Barrault, and Antoine Bordes. Supervised\nlearning of universal sentence representations from natural language inference data. In EMNLP,\n2017.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\nfrom unlabelled data. In HLT-NAACL, 2016.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Skip-thought vectors. In NIPS, 2015.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representa-\ntions of words and phrases and their compositionality. In NIPS, 2013.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural\nnetworks in nlp applications? CoRR, abs/1603.06111, 2016.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word\nrepresentation. In EMNLP, 2014.\n4\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2018-02-20",
  "updated": "2018-02-20"
}