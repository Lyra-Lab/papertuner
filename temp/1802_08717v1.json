{
  "id": "http://arxiv.org/abs/1802.08717v1",
  "title": "Deep learning in radiology: an overview of the concepts and a survey of the state of the art",
  "authors": [
    "Maciej A. Mazurowski",
    "Mateusz Buda",
    "Ashirbani Saha",
    "Mustafa R. Bashir"
  ],
  "abstract": "Deep learning is a branch of artificial intelligence where networks of simple\ninterconnected units are used to extract patterns from data in order to solve\ncomplex problems. Deep learning algorithms have shown groundbreaking\nperformance in a variety of sophisticated tasks, especially those related to\nimages. They have often matched or exceeded human performance. Since the\nmedical field of radiology mostly relies on extracting useful information from\nimages, it is a very natural application area for deep learning, and research\nin this area has rapidly grown in recent years. In this article, we review the\nclinical reality of radiology and discuss the opportunities for application of\ndeep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the\nresearch in deep learning applied to radiology. We organize the studies by the\ntypes of specific tasks that they attempt to solve and review the broad range\nof utilized deep learning algorithms. Finally, we briefly discuss opportunities\nand challenges for incorporating deep learning in the radiology practice of the\nfuture.",
  "text": "Deep learning in radiology: an overview of the concepts\nand a survey of the state of the art\nMaciej A. Mazurowski1, 2, 3, Mateusz Buda1, Ashirbani Saha1, Mustafa R. Bashir1, 4\n1Department of Radiology, Duke University, Durham, NC\n2Department of Electrical and Computer Engineering, Duke University, Durham, NC\n3Duke Medical Physics Program, Duke University, Durham, NC\n4Center for Advanced Magnetic Resonance Development, Duke University, Durham, NC\nAbstract\nDeep learning is a branch of artiﬁcial intelligence where networks of simple interconnected\nunits are used to extract patterns from data in order to solve complex problems. Deep learning\nalgorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially\nthose related to images. They have often matched or exceeded human performance. Since the\nmedical ﬁeld of radiology mostly relies on extracting useful information from images, it is a very\nnatural application area for deep learning, and research in this area has rapidly grown in recent\nyears. In this article, we review the clinical reality of radiology and discuss the opportunities\nfor application of deep learning algorithms. We also introduce basic concepts of deep learning\nincluding convolutional neural networks. Then, we present a survey of the research in deep\nlearning applied to radiology. We organize the studies by the types of speciﬁc tasks that they\nattempt to solve and review the broad range of utilized deep learning algorithms. Finally, we\nbrieﬂy discuss opportunities and challenges for incorporating deep learning in the radiology\npractice of the future.\n1\nIntroduction\nThe ﬁeld of deep learning encompasses a group of artiﬁcial intelligence methods which employ a large\nnumber of simple interconnected units to perform complicated tasks. Deep learning algorithms,\nrather than using a set of pre-programmed instructions, are capable of learning from large amounts\nof data. The tasks solved by these algorithms include localizing and classifying objects in images,\nunderstanding language, playing games, and many others. While the ﬂagship of deep learning,\nconvolutional neural networks, were ﬁrst introduced decades ago, it is only the last 5 years that\nshowed an astonishing success of these algorithms elevating their status from an interesting but\nmostly impractical idea to the go-to algorithm in artiﬁcial intelligence. In recent years, not only\nhave deep learning algorithms been able to surpass performance of other methods in artiﬁcial\nintelligence [1] but in some tasks, they have shown performance superior to humans [2, 3, 4].\nArguably, the most well-known achievement of deep learning to date is its performance in the Im-\nageNet competition. ImageNet is a database of more than 14,000,000 annotated natural images con-\ntaining real world objects such as cars, animals, and buildings (image-net.org). One of the goals of\nthe competition, started in 2009, is to assign each image to one of 1 000 predeﬁned categories. When\na deep learning-based algorithm ﬁrst appeared in the competition in 2012, it dramatically improved\nthe error rate from 0.258 in the previous year (image-net.org/challenges/LSVRC/2011/results) to\n0.153 (image-net.org/challenges/LSVRC/2012/results). The performance of deep learning algo-\nrithms for image classiﬁcation has been improving since then and is now considered to be compa-\nrable to or better than human performance. Other areas relevant to the topic of this article, where\n1\narXiv:1802.08717v1  [cs.CV]  10 Feb 2018\ndeep learning algorithms have seen impressive results, is in the automatic generation of sophisti-\ncated captions for images that consist of full sentences [5] as well as localization and outlining of\nobjects in images [6, 7]. To illustrate the capacity of current detection network, Figure 1 shows the\nresult of a deep neural network approach applied to detect objects in an image.\nFigure 1: A picture of three of the authors of this article (Buda, Saha, Mazurowski) with detection labels\ngenerated by the YOLO detection network.\nThere are likely three reasons for the recent success of deep learning algorithms: availability of\ndata, increased processing power, and rapid development of algorithms. These are highly connected:\navailability of large datasets of images and computing power made it possible to demonstrate the\nstrength of the basic concepts of deep learning and, motivated the development of further datasets\nand algorithms. Increasing ease in applying algorithms and aﬀordable graphical processing units\nhave allowed for larger scientiﬁc and technical communities to get involved, and develop even more\npowerful algorithms which further advanced the ﬁeld.\nAs the primary strength of deep learning has been in image analysis, the potential applications\nin radiology have become very quickly apparent. The development of algorithms for radiology has\nshown some inertia due to the time needed for acquisition of the appropriate expertise in the medical\nimaging community as well as limited availability of large medical imaging datasets. However, the\nlast 2-3 years have seen remarkable productivity in the ﬁeld. It is now well recognized by both\nresearchers and clinicians that deep learning will play a signiﬁcant role in radiology.\nIn this paper, we begin with a general overview of radiology as the application domain and\nconsider where deep learning could have the most signiﬁcant impact.\nThen, we introduce the\ngeneral concepts of deep learning. This is followed by an overview of the recent work in the ﬁeld.\nThe article closes with remarks regarding the future of deep learning in radiology.\n2\nThe practice of radiology\nRadiology is a branch of medicine that focuses on using medical images for detection, diagnosis,\nand characterization of disease (diagnostic radiology) as well as guiding procedural interventions\n(interventional radiology). In the United States, a typical radiologist undergoes 14-15 years of edu-\ncation after high school including 4 years of college, 4 years of medical school, 1 year of internship,\n4 years of radiology residency, and generally 1-2 years of fellowship training. While medical image\n2\ninterpretation work is centered in radiology practices and academic departments, some of it is also\nperformed within other branches of medicine including cardiology, orthopedics, and surgery.\nIn diagnostic radiology, personal interaction between a radiologist with patients and other physi-\ncians is often limited. The primary duty of a radiologist (particularly outside of an academic in-\nstitution) is to view an image delivered to his/her reading station and generate a written report of\nﬁndings. This well-structured and isolated nature of the radiologist’s work makes is a particularly\nattractive application of artiﬁcial intelligence algorithms.\nDeep learning techniques (and artiﬁcial intelligence algorithms in general) have a tremendous\npotential to inﬂuence the practice of radiology. Unlike most other facets of medicine, all or nearly all\nof the primary data utilized in imaging is digital, lending itself to analysis by artiﬁcial intelligence\nalgorithms.\nIn this section, we describe some of the primary challenges that a radiologist faces in his/her\ndaily diagnostic radiology practice and brieﬂy point to opportunities for deep learning to address\nthem. While this is not intended to be an exhaustive description of every tasks that radiologists\nperform in their practice, it reﬂects majority of diagnostic radiology practice. We conclude this sec-\ntion with a description of some medical image interpretation tasks that are currently not performed\nby radiologists, but could be incorporated in radiology practice using deep learning.\n2.1\nDisease Detection\nOne of the most challenging tasks in the interpretation of imaging is the rapid diﬀerentiation of ab-\nnormalities from normal background anatomy. For example, in the interpretation of mammography,\neach radiograph contains thousands of individual focal densities, regional densities, and geometric\npoints and lines that must be interpreted to detect a small number of suspicious or abnormal ﬁnd-\nings. In most cases, the entire mammogram should be interpreted as normal or negative, adding\nfurther complexity to the interpretive task.\nIn order to be useful, a computer algorithm does not have to detect all objects of interest\n(e.g., abnormalities) and be perfectly speciﬁc (i.e., not mark any normal locations). For exam-\nple, in screening mammography, approximately 80% of screening mammograms should be read as\nnegative according to the ACR BI-RADS guideline, and of the 20% of examinations that trigger\nadditional evaluation, many will ultimately be categorized as negative or benign [8]. An algorithm\nthat could successfully categorize even half of screening mammograms as deﬁnitely negative would\ndramatically reduce the eﬀort required to interpret a large batch of examinations.\n2.2\nDisease Diagnosis and Management\nOnce an abnormality has been detected, the often-complex task of determining a diagnosis and\nthe disease management implications is undertaken. For focal masses generically, a large number\nof features must be integrated in order to decide how to appropriately manage the ﬁnding. These\nfeatures can include size, location, attenuation or signal intensity, borders, heterogeneity, change\nover time, and others. In some cases, simple criteria have been established and validated for the\nmanagement of focal ﬁndings. For example, most focal lesions in the kidney can be characterized as\neither simple or minimally complex cysts, which are almost uniformly benign. On the other hand,\nmost lesions in the kidney that are solid are considered to have high malignant potential. Finally,\na minority of focal kidney lesions is considered indeterminate and can be managed accordingly.\nWhile for some types of abnormalities making the diagnostic and disease management decision\nfollows straightforward guidelines, for other types of abnormalities, management algorithms are\nmuch more complex. In the BI-RADS guideline for assessing focal lesions in the breast, a mass\n3\nis categorized according to its shape (oval, round, or irregular), margin (circumscribed, obscured,\nmicrolobulated, indistinct, or spiculated), and its density (higher, equal to, or lower density than the\nglandular tissue, or fat-containing) [8]. Based on the constellation of features, the radiologist must\nthen decide whether a mass is likely benign or requires follow-up or biopsy. In the LI-RADS criteria\nfor assessing focal liver lesions in patients at risk for developing hepatocellular carcinoma, ﬁve major\nfeatures, and up to 21 ancillary features, are assessed to risk-stratify lesions and determine their\nmanagement [9].\nDeep learning algorithms have the potential to assess a large number of features, even those\npreviously not considered by radiologists, and arrive at a repeatable conclusion in a fraction of\nthe time required for a human interpreter. Perhaps most promisingly, these algorithms could be\nused to categorize large amounts of existing imaging data and correlate features with downstream\nhealth outcomes, a process that is currently extremely laborious and time-consuming when human\ninterpretation is required.\n2.3\nWorkﬂow\nWhile detection, diagnosis, and characterization of disease receive the primary attention among\nalgorithm developers, another important area where artiﬁcial intelligence could contribute is in\nfacilitating the workﬂow of the radiologists while interpreting images. With the widespread con-\nversion from printed ﬁlms to centralized Picture Archiving and Viewing Systems (PACS) as well\nas the availability of multi-planar, multi-contrast, and multi-phase imaging, radiologists have seen\nexponential growth in the size and complexity of image data to be analyzed. Additionally, inter-\npretations must often be rendered in the context of a multitude of prior examinations. The simple\ntask of ﬁnding and presenting these data is complex, and artiﬁcial intelligence systems may be\nwell-suited for this role.\nAn example of a highly complex workﬂow is that for many cancer patients.\nSuch patients\nare not uncommonly aﬄicted with more than one primary tumor, metastatic disease to numerous\nsites, and may have undergone a variety of biopsies, locoregional therapies, and systemic therapies\nwith varying results. In the simplest scenario, interpretation of a follow-up imaging examination\nrequires colocalization of all relevant sites of disease between the current and prior examinations.\nMeasurements of size are performed, and in some cases functional features, such as tumor perfusion\nor diﬀusion restriction, are assessed either subjectively or objectively. Most radiology practices\nutilize imaging equipment of diﬀerent types, generations, and often diﬀerent vendors, thus simply\nidentifying the appropriate image sets in prior examinations can be very challenging. After the\nappropriate images have been identiﬁed, the radiologist must colocalize disease sites and attempt\nto obtain precise repeated measurements in order to ensure that the values obtained from the\ncurrent and prior examinations can be compared.\nEach of the above tasks is time-consuming and does not necessarily require the full skill of\na radiologist. However, standard PACS systems are not able to reliably present all of the above\ndata for a variety of reasons, including the variability in labeling the types and components of\nimaging examinations, the variability in patient positioning and anatomy between examinations,\nthe variability in modalities used to image the same portion of the anatomy, as well as other\nfactors.\nIn principle, an artiﬁcial intelligence algorithm could assess a patient’s prior imaging,\nbring forward examinations that include the relevant body part(s), detect the image modality and\ncontrast type, and determine the location of the area of interest within the relevant anatomy to\nreduce the radiologist’s eﬀort in performing these relatively mundane tasks.\n4\n2.4\nImage interpretation tasks that radiologists do not perform but deep learn-\ning could\nIn addition to performing tasks that are a part of the current radiological practice, computer al-\ngorithms could perform medical image interpretation tasks that radiologists do not perform on a\nregular basis. The research toward this goal has been underway for some time, mostly using tradi-\ntional machine learning and image processing algorithms. One example is radiogenomics [10], which\naims to ﬁnd relationships between imaging features of tumors and their genomic characteristics.\nExamples can be found in breast cancer [11], glioblastoma [12], low grade glioma [13], and kidney\ncancer [14]. Radiogenomics is not a part of the typical clinical practice of a radiologist. Another\nexample is prediction of outcomes of cancer patients with applications in glioblastoma [12, 15],\nlower grade glioma, [13], and breast cancer [16]. While imaging features have a potential to be\ninformative of patient outcomes, very few are currently used to guide oncological treatment. Deep\nlearning could facilitate the process of incorporating more of the information available from imaging\ninto the oncology practice.\n3\nAn introduction to deep learning\n3.1\nTerminology\nTo understand deep learning, it is helpful to ﬁrst understand the related concepts of artiﬁcial\nintelligence and machine learning. Artiﬁcial intelligence is a set of computer algorithms that are\nable to perform complicated tasks or tasks that require intelligence when conducted by humans.\nMachine learning is a subset of artiﬁcial intelligence algorithms which, to perform these complicated\ntasks, are able to learn from provided data and do not require pre-deﬁned rules of reasoning.\nThe ﬁeld of machine learning is very diverse and has already had notable applications in medical\nimaging [17]. Deep learning is a sub-discipline of machine learning that relies on networks of simple\ninterconnected units. In deep learning models, these units are connected to form multiple layers\nthat are capable of generating increasingly high level representations of the provided input (e.g.,\nimages). Below, in order to explain the architecture of deep learning models, we introduce the\nartiﬁcial neural network in general and one speciﬁc type: the convolutional neural network. Then,\nwe detail the ”learning” process of these networks, which is the process of incorporating the patterns\nextracted from data into deep neural networks.\n3.2\nArtiﬁcial Neural Networks\nArtiﬁcial neural networks (ANNs) are machine learning models based on basic concepts dating\nas far as 1940s, signiﬁcant development in 1970s and 1980s and a period notable popularity in\n1990s and 2000s followed by a period of being overshadowed by other machine learning algorithms.\nANNs consist of a multitude of interconnected processing units, called neurons, usually organized in\nlayers. A traditional ANN typically used in the practice of machine learning contains 2 to 3 layers of\nneurons. Each neuron performs a very simple operation. While many neuron models were proposed,\na typical neuron simply multiplies each input by a certain weight, then adds all the products for\nall the inputs and applies a simple nondecreasing function at the end. Even though each neuron\nperforms a very rudimentary calculation, the interconnected nature of the network allows for the\nperformance of very sophisticated calculations and implementation of very complicated functions.\n5\n3.3\nConvolutional Neural Networks\nDeep neural networks are a special type of an ANN. The most common type of a deep neural\nnetwork is a deep convolutional neural network (CNN). A deep convolutional neural network, while\ninheriting the properties of a generic ANN, has also its own speciﬁc features. First, it is deep. A\ntypical number of layers is 10-30 but in extreme cases it could exceed 1 000. Second, the neurons\nare connected such that multiple neurons share weights. This eﬀectively allows the network to\nperform convolutions (or template matching) of the input image with the ﬁlters (deﬁned by the\nweights) within the CNN. Other special feature of CNNs is that between some layers, they perform\npooling which makes the network invariant to small shifts of the images. Finally, CNNs typically\nuse a diﬀerent activation function of the neurons as compared to traditional ANNs.\nFigure 2: A diagram illustrating a typical architecture of a convolutional neural network.\nFigure 2 shows an example of a small architecture for a typical CNN. One can see that the ﬁrst\nlayers are the convolutional ones which serve the role of generating useful features for classiﬁcation.\nThose layers can be thought of as implementing image ﬁlters, ranging from simple ﬁlters that\nmatch edges to those that eventually match much more complicated shapes such as eyes, or tumors.\nFurther from the network input are so called fully connected layers (similar to traditional ANNs)\nwhich utilize the features extracted by the convolutional layers to generate a decision (e.g., assign a\nlabel). A variety of deep learning architectures have been proposed, often driven by characteristics\nof the task at hand (e.g., fully convolutional neural networks for image segmentation). Some of\nthese are described in more detail in the section of this paper that reviews the current state of the\nart.\n3.4\nThe learning process in convolutional neural networks\nAbove, we described general characteristics of traditional neural networks and deep learning’s\nﬂagship: the convolutional neural network. Next, we will explore how to make those networks\nperform useful tasks. This is accomplished in the process referred to as learning or training. The\nlearning process of a convolutional neural network simply consists of changing the weights of the\nindividual neurons in response to the provided data. In the most popular type of learning process,\ncalled supervised learning, a training example contains an object of interest (e.g., an image of\na tumor) and a label (e.g., the tumor’s pathology: benign or malignant). In our example, the\nimage is presented to the network’s input, and the calculation is carried out within the network to\nproduce a prediction based on the current weights of the network. Then, the network’s prediction is\ncompared to the actual label of the object and an error is calculated. This error is then propagated\n6\nthrough the network to change the values of the network’s weights such that the next time the\nnetwork analyzes this example, the error decreases.\nIn practice, the adaptation of the weights\nis performed after a group of examples (a batch) are presented to the network. This process is\ncalled error backpropagation or stochastic gradient descent. Various modiﬁcations of stochastic\ngradient descent algorithm have been developed [18]. In principle, this iterative process consists\nof calculations of error between the output of the model and the desired output and adjusting the\nweights in the direction where the error decreases.\nFigure 3: An illustration of diﬀerent ways of training in deep neural networks.\nThe most straightforward way of training is to start with a random set of weights and train\nusing available data speciﬁc to the problem being solved (training from scratch). However, given the\nlarge number of parameters (weights) in a network, often above 10 million, and a limited amount of\ntraining data (common in medical imaging), a network may overﬁt to the available data, resulting\n7\nin poor performance on test data. Two training methods have been developed to address this issue:\ntransfer learning [19] and oﬀ-the-shelf features (a.k.a. deep features) [20]. A diagram comparing\ntraining from scratch with transfer learning and oﬀ-the-shelf deep features is shown in Figure 3.\nIn the transfer learning approach, the network is ﬁrst trained using a diﬀerent dataset, for\nexample an ImageNet collection. Then, the network is ”ﬁne-tuned” through additional training with\ndata speciﬁc to the problem to be addressed. The idea behind this approach is that solving diﬀerent\nvisual tasks shares a certain level of processing such as recognition of edges or simple shapes. This\napproach has been shown successful in, for example, prediction of survival time from brain MRI in\npatients with glioblastoma tumor [21] or in skin lesion classiﬁcation [22]. Another approach that\naddresses the issue of limited training data is the deep ”oﬀ-the-shelf” features approach which uses\nconvolutional neural networks which have been trained on a diﬀerent dataset to extract features\nfrom the images. This is done by extracting outputs of layers prior to the network’s ﬁnal layer.\nThose layers typically have hundreds or thousands of outputs. Then, these outputs are used as\ninputs to ”traditional” classiﬁers such as linear discriminant analysis, support vector machines, or\ndecision trees. This is similar to transfer learning (and is sometimes considered a part of transfer\nlearning) with the diﬀerence being that the last layers of a CNN are replaced by a traditional\nclassiﬁer and the early layers are not additionally trained.\n3.5\nDeep learning vs ”traditional” machine learning\nIncreasingly often we hear a distinction between deep learning and ”traditional” machine learning\n(see Figure 4). The diﬀerence is very important, particularly in the context of medical imaging. In\ntraditional machine learning, the typical ﬁrst step is feature extraction. This means that to classify\nan object, one must decide which characteristics of an object will be important and implement\nalgorithms that are able to capture these characteristics. A number of sophisticated algorithms in\nthe ﬁeld of computer vision have been proposed for this purpose and a variety of size, shape, texture,\nand other features were extracted. This process is to a large extent arbitrary since the machine\nlearning researcher or practitioner often must guess which features will be of use for a particular task\nand runs the risk of including useless and redundant features and, more importantly, not including\ntruly useful features. In deep learning, the process of feature extraction and decision making are\nmerged and trainable, and therefore no choices need to be made regarding which features should\nbe extracted; this is decided by the network in the training process. However, the cost of allowing\nthe neural network to select its own features is a requirement for much larger training data sets.\nFigure 4: An illustration of diﬀerence between “traditional” machine learning and deep learning.\n8\n4\nDeep learning in radiology: state of the art\nIn this section, we give an overview of applications of deep learning in radiology. We organized\nthis section by the tasks that the deep learning algorithms perform. Within each subsection, we\ndescribe diﬀerent methods applied, and when possible, we systematically discuss the evolution of\nthese methods in the recent years.\n4.1\nClassiﬁcation\nIn a classiﬁcation task, an object is assigned to one of the predeﬁned classes. A number of diﬀerent\nclassiﬁcation tasks can be found in the domain of radiology such as: classiﬁcation of an image or an\nexamination to determine the presence or an absence of an abnormality; classiﬁcation of abnormal-\nities as benign or malignant; classiﬁcation of cancerous lesions according to their histopathological\nand genomic features; prognostication; and classiﬁcation for the purpose of organization radiological\ndata.\nDeep learning is becoming the methodology of choice for classifying radiological data.\nThe\nmajority of the available deep learning classiﬁers use convolutional neural networks with a varying\nnumber of convolutional layers followed by fully connected layers. The availability of radiological\ndata is limited as compared to the natural image datasets which drove the development of deep\nlearning techniques in the last 5 years. Therefore, many applications of deep learning in medical\nimage classiﬁcation have resorted to techniques meant to alleviate this issue: oﬀ-the-shelf features\nand transfer learning [23] discussed in the previous section of this article. Oﬀ-the-shelf features\nhave performed well in a variety of domains [20], and this technique has been successfully applied\nto medical imaging [24, 25]. In [24], the authors combined the deep oﬀ-the shelf features extracted\nfrom a pre-trained VGG19 network with hand-crafted features for determining malignancy of breast\nlesions in mammography, ultrasound, and MRI. In [25], long-term and short term survival was\npredicted for patients with lung carcinoma. The transfer learning strategy, which involves ﬁne\ntuning of a network pre-trained on a diﬀerent dataset, has been applied to a variety of tasks such\nas classiﬁcation of prostate MR images to distinguish patients with prostate cancer from patients\nwith benign prostate conditions [26], identiﬁcation of CT images with pulmonary tuberculosis [27],\nand classiﬁcation of radiographs to identify hip osteoarthritis [28]. Most of the studies which apply\nthe transfer learning strategy replace and retrain the deepest layer of a network, whereas shallow\nlayers are ﬁxed after the initial training. A variation of the transfer learning strategy combines\nﬁne-tuning and deep features approach. It ﬁne-tunes a pre-trained network on a new dataset to\nobtain more task-speciﬁc deep feature representations. An example of this is the study [29], which\nperformed ultrasound imaging-based thyroid nodule classiﬁcation using features extracted from a\nﬁne-tuned pre-trained GoogLeNet. An ensemble of ﬁne-tuned CNN classiﬁers was shown to predict\nradiological image modality in the study [30]. A comparison of approaches using deep features and\ntransfer learning with ﬁne tuning was shown in the study [31] identifying radiogenomic relationships\nin breast cancer MR images and in [32] for predicting the upstaging of ductal carcinoma in situ\nto invasive breast cancer from breast cancer MR images. In both of these problems, deep features\nperformed better than transfer learning with the ﬁne tuning approach. However, both of these\nstudies faced the issue of a small size of the training set.\nWhen suﬃcient data are available, an entire deep neural network can be trained from a random\ninitialization (training from scratch). The size of the network to be trained depends on task and\ndataset characteristics. However, the commonly used architecture in medical imaging is based on\nAlexNet [1] and VGG [33] with modiﬁcations that have fewer layers and weights. Examples of\ntraining from scratch can also be found in various studies such as: assessing for the presence of\n9\nAlzheimer’s disease based on brain MRI using deep learning [34, 35], glioma grading in MRI [36],\nand disease staging and prognosis in chest CT of smokers [37]. Recent advances in the design\nof CNN architectures has made networks easier to train and more eﬃcient.\nThey have more\nlayers and perform better while having fewer trainable parameters [38] which reduces the likelihood\nof overtraining. The most notable examples include Residual Networks (ResNets) [39] and the\nInception architecture [40, 41]. A shift to these more powerful networks has also taken place in\napplications of deep learning to radiology both for transfer learning and training from scratch. Three\ndiﬀerent ResNets were used to predict methylation of the O6-methylguanine methyltransferase gene\nstatus from pre-surgical brain tumor MRI [42]. In [43], the InceptionV3 network was ﬁne-tuned\nand served as a feature extractor instead of previously used GoogLeNet. In another work using\nchest X-ray images [4], the authors ﬁne-tuned a DenseNet with 121 layers for the classiﬁcation\nof miscellaneous pathologies, achieving radiologist-level classiﬁcation performance for identifying\npneumonia.\nIn another approach, auto-encoder (AE) [44] or stacked auto-encoder (SAE) [45, 46] networks\nhave been trained from scratch, layer by layer in unsupervised way. A stacked denoising auto-\nencoder with backpropagation was used in [47] to determine the presence of Alzheimer’s disease.\nAEs and SAEs can also be used to extract feature representations (similarly to the deep features\napproach) from hidden layers for further classiﬁcation. Such feature representation has been used\nin the classiﬁcation of lung nodules into benign and malignant classes in CT [48], and in the\nidentiﬁcation of multiple sclerosis lesions in MRI [49].\nApart from the classiﬁcation of radiological images, analysis of radiological text reports plays\na signiﬁcant role [50]. The most prominent approach in this type of classiﬁcation is deep learning-\nbased natural language processing (NLP) [51], which is based on the seminal work for obtaining\nvector representation of phrases using an unsupervised neural model [52]. An example of application\nof this architecture can be found in [53] where the authors classiﬁed CT radiology reports as\nrepresenting presence or absence of pulmonary embolism (PE), as well as type (chronic or acute)\nand location (central or subsegmental) of PE when present.\nThey showed an improvement as\ncompared to a non-deep learning algorithm. The same architecture was used in [54] for classifying\nhead CT reports of ICU patients with altered mental status as having diﬀerent degrees of severity\naccording to each of ﬁve criteria: severity of study, acute intracranial bleed, acute mass eﬀect,\nacute stroke, acute hydrocephalus. Radiology reports using the International Coding of Diseases\n(ICD) were auto-encoded in [55] using a publicly available dataset. A third application of the\nsame architecture can be found in [55] where radiology reports were classiﬁed according to the\nInternational Coding of Diseases9 (ICD9) using a publicly available dataset.\n4.2\nSegmentation\nIn an image segmentation task, an image is divided into diﬀerent regions in order to separate distinct\nparts or objects. In radiology, the common applications are segmentation of organs, substructures,\nor lesions, often as a preprocessing step for feature extraction and classiﬁcation [34, 56]. Below,\nwe discuss diﬀerent types of deep learning approaches used in segmentation tasks in a variety of\nradiological images.\nThe most straightforward and still widely used method for image segmentation is classiﬁcation\nof individual pixels based on small image patches (both 2-dimensional and 3-dimensional) extracted\naround the classiﬁed pixel. This approach has found usage in diﬀerent types of segmentation tasks\nin MRI, for example brain tumor segmentation in [57, 58, 59], white matter segmentation in multiple\nsclerosis patients [60], segmentation of 25 diﬀerent structures in brain [61], and for rectal cancer\nsegmentation in pelvis MRI [62]. It allows for using the same network architectures and solutions\n10\nthat are known to work well for classiﬁcation, however, there are some shortcomings of this method.\nThe primary issue is that it is computationally ineﬃcient, since it processes overlapping parts of\nimages multiple times. Another drawback is that each pixel is segmented based on a limited-size\ncontext window and ignores the wider context. In some cases, a piece of global information, e.g.\npixel location or relative position to other image parts, may be needed to correctly assign its label.\nOne approach that addresses the shortcomings of the pixel-based segmentation is a fully con-\nvolutional neural network (fCNN) [63]. Networks of this type process the entire image (or large\nportions of it) at the same time and output a 2-dimensional map of labels (i.e., a segmentation map)\ninstead of a label for a single pixel. Example architectures that were successfully used in both nat-\nural images and radiology applications are encoder-decoder architectures such as U-Net [64, 65, 66]\nor Fully Convolutional DenseNet [67, 68, 69]. Various adjustments to these types of architectures\nhave been developed that mainly focus on connections between the encoder and decoder parts of\nthe networks, called skip connections. Applications of fCNNs in radiology include prostate gland\nsegmentation in MRI [70], segmentation of multiple sclerosis lesions and gliomas in MRI [71], and\nultrasound-based nerve segmentation [72]. Moreover, loss functions have been explored that ac-\ncount for class imbalance (diﬀerences in the number of examples in each class), which is typical in\nmedical datasets, e.g. weighted cross entropy was used in [73] for brain structure segmentation in\nMRI or Dice coeﬃcient-based loss for brain tumor segmentation in MRI [74].\nIn order to segment 3-dimensional data, it is common to process data as 2-dimensional slices\nand then combine the 2-dimensional segmentation maps into a 3-dimensiaonal map since 3D fC-\nNNs are signiﬁcantly larger in terms of trainable parameters and as a result require signiﬁcantly\nlarger amounts of data. Nevertheless, these obstacles can be overcome, and there are successful\napplications of 3D fCNNs in radiology, e.g. V-Net for prostate segmentation from MRI [75] and\n3D U-Net [76] for segmentation of the proximal femur in MRI [77] and tumor segmentation in\nmultimodal brain MRI [78].\nFinally, a deep learning approach that has found some application in medical imaging segmen-\ntation is recurrent neural networks (RNNs). In [79], the authors used a Boundary Completion\nRNN for prostate segmentation in ultrasound images. Another notable application is in [80], where\nthe authors applied a recurrent fully convolutional neural network for left-ventricle segmentation\nin multi-slice cardiac MRI to leverage inter-slice spatial dependencies. Similarly, [81] used Long\nShort-Term Memory (LSTM) [82] type of RNN trained end-to-end together with fCNN to take\nadvantage of 3D contextual information for pancreas segmentation in CT and MR images.\nIn\naddition, they proposed a novel loss function that directly optimizes a widely used segmentation\nmetric, the Jaccard Index [83].\n4.3\nDetection\nDetection is a task of localizing and pointing out (e.g., using a rectangular box) an object in an\nimage. In radiology, detection is often an important step in the diagnostic process which identiﬁes\nan abnormality (such as a mass or a nodule), an organ, an anatomical structure, or a region\nof interest for further classiﬁcation or segmentation [84, 85, 86]. Here, we discuss the common\narchitectures used for various detection tasks in radiology along with example speciﬁc applications.\nThe most common approach to detection for 2-dimensional data is a 2-phase process that re-\nquires training of 2 models. The ﬁrst phase identiﬁes all suspicious regions that may contain the\nobject of interest. The requirement for this phase is high sensitivity [87] and therefore it usually\nproduces many false positives.\nA typical deep learning approach for this phase is a regression\nnetwork for bounding box coordinates based on architectures used for classiﬁcation [88, 89]. The\nsecond phase is simply classiﬁcation of sub-images extracted in the previous step. In some ap-\n11\nplications, only one of the two steps uses deep learning. The classiﬁcation step, when utilizing\ndeep learning, is usually performed using transfer learning. The models are often pre-trained using\nnatural images, for example for thoraco-abdominal lymph node detection in [90] and pulmonary\nembolism detection in CT pulmonary angiogram images [23]. In other applications, the models are\npre-trained using other medical imaging dataset to detect masses in digital breast tomosynthesis\nimages [91]. The same network architectures can be used for the second phase as in a regular\nclassiﬁcation task (e.g., VGG [33], GoogLeNet [92], Inception [40], ResNet [39]) depending on the\nneeds of a particular application.\nWhile in the 2-phase detection process the models are trained separately for each phase, in the\nend-to-end approach one model encompassing both phases is trained. An end-to-end architecture\nthat has proved to be successful in object detection in natural images, and was recently applied\nto medical imaging, is the Faster Region-based Convolutional Neural Network (R-CNN) [7]. It\nuses a CNN to obtain a feature map which is shared between region proposal network that outputs\nbounding box candidates, and a classiﬁcation network which predicts the category of each candidate.\nIt was recently applied for intervertebral disc detection in X-ray images [93] and detection of colitis\non CT images [94]. A domain speciﬁc modiﬁcation that uses additional preprocessing before the\nregion proposal step was used by [95] for detection of architectural distortions in mammograms.\nAnother approach to detection is a single-phase detector that eliminates the ﬁrst phase of\nregion proposals. Examples of popular methods that were ﬁrst developed for detection in natural\nimages and rely on this approach are You Only Look Once (YOLO) [96], Single Shot MultiBox\nDetector (SSD) [97] and RetinaNet [6]. In the context of radiology, a YOLO-based network called\nBC-DROID has been developed by [98] for region of interest detection in breast mammograms.\nSSD has been employed, for example in [99], for breast tumor detection in ultrasound images,\noutperforming other evaluated deep learning methods that were available at the time. The authors\nof [100] applied the same network for detection of pulmonary lung nodules in CT images.\nIn the examples above, 2-dimensional data was used. For 3-dimensional imaging volumes, com-\nmon in medical imaging, results obtained from 2-dimensional processing are combined to produce\nthe ultimate 3-dimensional bounding box. As an example, in [101] the authors performed detection\nof 3D anatomy in chest CT images by processing data slice by slice in one direction. Combining\noutput from diﬀerent planes was performed in several studies. Most of the them [102, 103, 104]\nused orthogonal planes of MRI and CT images performing detection in each direction separately.\nThe results can then be combined in diﬀerent ways, e.g. by an algorithm based on output prob-\nabilities [101] or using another machine learning method like random forest [100]. An alternative\nmethod for 3D detection has been proposed for automatic detection of lymph nodes using CT\nimages by concatenating coronal, sagittal and axial views as a single 3-channel image in [87].\n4.4\nOther Tasks in Radiology\nWhile the majority of the applications of deep learning in radiology have been in classiﬁcation,\nsegmentation, and detection, other medical imaging-related problems have found some solutions in\ndeep learning. Due the variety of those problems, there is no unifying methodological framework\nfor these solutions. Therefore below, we organize the examples according to the problem that they\nattempt to address.\nImage Registration:\nIn this task two or more images (often 3D volumes), typically of diﬀerent\ntypes (e.g., T1-weighted and T2-weighted MRIs) must be spatially aligned such that the same loca-\ntion in each image represents the same physical location in the depicted organ. Several approaches\ncan be taken to address the problem. In one approach, it is necessary to calculate similarity mea-\n12\nsures between image patches taken from the images of interest to register them. The authors of [105]\nused deep learning to learn a similarity measure from T1-T2 MRI image pairs of adult brain and\ntested it to register T1-T2 MRI interpatient images of the neonatal brain. This similarity measure\nperformed better than the standard measure, called mutual information, which is widely used in\nregistration [106]. In another deep learning-based approach to image registration, the deformation\nparameters between image pairs are directly learned using misaligned image pairs. In [107], the\nauthors trained a CNN-based model to learn the sequence of movements that resulted in the mis-\nalignment of the image pairs of CT and cone-beam CT examinations of the abdominal spine and\nheart. In another study [108], chest CT follow-up examinations were registered by training a CNN\nto predict three-dimensional displacement vector ﬁelds between the ﬁxed and moving image pairs.\nA CNN-based network was trained to correct respiratory motion in 3D abdominal MR images by\npredicting spatial transforms [109]. All of these techniques are supervised regression techniques\nas they were trained using ground truth deformation information.\nIn another approach [110],\nwhich was unsupervised, a CNN was trained end-to-end to generate a spatial transformation which\nminimized dissimilarity between misaligned image pairs.\nImage generation:\nAcquisition parameters of a radiological image strongly aﬀect the visual\nquality and detail of the images obtained using the same modality. First, we discuss the applications\nthat synthesize images generated using diﬀerent acquisition parameters within the same modality.\nIn [111], 7T like images were generated from 3T MR images by training a CNN with patches centered\naround voxels in the 3T MR images. Undersampled (in k-space) cardiac MRIs were reconstructed\nusing a deep cascade of CNNs in [112]. A real-time method to reconstruct compressed sensed MRI\nusing GAN was proposed by [113]. In another approach [114] in order to synthesize brain MRI\nimages based on other MRI sequences in the same patient, convolutional encoders were built to\ngenerate a latent representation of images. Then, based on that representation a sequence of interest\nwas generated. Reconstruction of ”normal-dose” CT images from low-dose CT images (which are\ndegraded in comparison to normal-dose images) has been performed using patch-by-patch mapping\nof low-dose images to high-dose images using a shallow CNN [115]. In contrast, a deep CNN has\nbeen trained with low-dose abdominal CT images for reconstruction of normal-dose CT [116]. In\nanother study, CT images were reconstructed from a lower number of views using an U-Net inspired\narchitecture [117].\nDeep learning has also been applied to synthesizing images of diﬀerent modalities. For example,\nCT images have been generated using MRIs by adopting an FCN to learn an end-to-end non-linear\nmapping between pelvic CTs and MRIs [118]. Synthetic CT images of brain were generated from\none T1-weighted MRI sequence in [119]. In another application to aid a classiﬁcation framework\nfor Alzheimer’s disease diagnosis with missing PET scans, PET patterns were predicted from MRI\nusing CNN [120].\nImage enhancement:\nImage enhancement aims to improve diﬀerent characteristics of the im-\nage such as resolution, signal-to-noise-ratio, and necessary anatomical structures (by suppressing\nunnecessary information) through various approaches such as super-resolution and denoising.\nSuper-resolution of images is important speciﬁcally in cardiac and lung imaging. Three dimen-\nsional near–isotropic cardiac and lung images often require long scan times in comparison to the\ntime the subject can hold his or her breath. Thus, multiple 2D slices are acquired instead and the\nsuper-resolution methodology is applied to improve the resolution of the images. An example of\nusing deep learning in super-resolution in cardiac MRI can be found in [121], where the authors\ndeveloped diﬀerent models for single image super-resolution and for generating high resolution\n13\nthree-dimensional image volumes from two-dimensional image stacks. In another study using CT,\na single image super-resolution approach based on CNN was applied in a publicly available chest CT\nimage dataset to generate high-resolution CT images, which are preferred for interstitial lung dis-\nease detection [122]. In this study, upscaled bicubic-interpolated images were ﬁrst passed through\none convolutional layer to generate low-resolution features. Then, a non-linear transformation of\nthose features was mapped to generate high resolution image features for the reconstruction.\nAn example of an application of deep learning in denoising can be found in [123] where the\nauthors performed denoising of DCE-MRI images of a brain (for stroke and brain tumors) by\ntraining an ensemble of deep auto-encoders using synthesized data. Removal of Rician noise in\nMR images using a deep convolutional neural network aided with residual learning was performed\nin [124].\nIn an attempt to enhance the visual details of lung structure in chest radiographs, the eﬀect\nof bone structures (ribs and clavicles) were suppressed.\nBone structure has been estimated by\nconditional random ﬁeld based fusion of the outputs of a cascaded architecture of CNNs at mul-\ntiple scales [125]. Metal artifacts (caused by prosthetics, dental procedures etc.) have also been\nsuppressed by using a trained CNN model to generate metal-free images using CT [126].\nContent-based image retrieval:\nIn the most typical version of this task, the algorithm, given\na query image, ﬁnds the most similar images in a given database. To accomplish this task, in [127],\na deep CNN was ﬁrst trained to distinguish between diﬀerent organs. Then, features from the\nthree fully connected layers in the network were extracted for the images in the set from which the\nimages were retrieved (evaluation dataset). The same features were then extracted from the query\nimage and compared with those of the evaluation dataset to retrieve the image. In another study,\na method was developed to retrieve, arrange, and learn the relationships between lesions in CT\nimages [128].\nObjective image quality assessment:\nObjective quality assessment measures of medical im-\nages aim to classify an image to be of satisfactory or unsatisfactory quality for the subsequent tasks.\nObjective quality measures of medical images are important to improve diagnosis and aid in better\ntreatment [129]. Image quality of fetal ultrasound was predicted using CNN in a recent study [130].\nAnother study [131], attempted to reduce the data acquisition variability in echocardiograms us-\ning a CNN trained on the quality scores assigned by an expert radiologist. Using a simple CNN\narchitecture, T2-weighted liver MR images were classiﬁed as diagnostic or non-diagnostic quality\nby CNN in [132].\n5\nFuture of deep learning in radiology\nThere is a general agreement that deep learning will play a role in the future practice of radiology.\nSome predict that it will conduct mundane tasks leaving radiologists with more time to focus on\nintellectually demanding challenges. Some believe that radiologists and deep learning algorithms\nwill work hand-in-hand to deliver performance superior to either of them alone.\nFinally, some\npredict that deep learning algorithms will replace radiologists (at least in their image interpretation\ncapacity) altogether.\nIncorporation of deep learning in radiology will be associated with multiple challenges. First,\nand currently foremost, is the technological challenge. While deep learning has shown extraordinary\npromise in other image-related tasks, the results in radiology are far from showing that deep learning\nalgorithms will replace a radiologist in the entire scope of their diagnostic work.\nSome recent\n14\nstudies [133, 134, 135, 136, 137, 138, 139, 4] indicate performance of these algorithms comparable\nto expert humans, but these results are only applicable to a very small minority of the tasks\nthat radiologists perform. This is likely to change in upcoming years given the rapid progress in\nimplementing the deep learning algorithms in the realm of radiology.\nImplementation of deep learning in radiology practice also poses legal and ethical challenges.\nPrimarily: who will be responsible for the mistakes that a computer will make? While this is a\ndiﬃcult question, similar questions have been posed and resolved when other technologies were\nintroduced including elevators and cars.\nSince artiﬁcial intelligence penetrates various areas of\nhuman activity, questions of this type will likely be studied and answers proposed in the coming\nyears.\nOther challenges will include patient acceptance or non-acceptance of a radiologist’s not being\ninvolved in the process of interpreting their images (regardless of the performance) as well as regu-\nlatory issues. Finally, an important practical issue is how to incorporate deep learning algorithms\ninto the radiology workﬂow in order to improve, rather than disrupt the radiology practice.\n6\nConclusion\nIn summary, in this paper we have discussed the principles of deep learning as well as the current\npractice of radiology to elucidate how these new algorithms can be incorporated into radiology\nworkﬂow. We have discussed the progress and state of art in the ﬁeld. Finally, we have discussed\nsome challenges and questions related to implementation of deep learning in the current practice\nof medicine. All signs show that deep learning will play a signiﬁcant role in radiology. The next\n5 years will be a very exciting time in the ﬁeld that may see many questions stated in this article\nanswered through a collaboration of machine learning scientists and radiologists.\nAcknowledgments: The authors would like to acknowledge funding from the National Institutes\nof Biomedical Imaging and Bioengineering grant 5 R01 EB021360. The authors would like to thank\nGemini Janas for reviewing and editing this article.\nReferences\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in neural information processing systems, pages\n1097–1105, 2012.\n[2] Samuel Dodge and Lina Karam. A Study and Comparison of Human and Deep Learning\nRecognition Performance Under Visual Distortions. arXiv preprint arXiv:1705.02498, 2017.\n[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE\ninternational conference on computer vision, pages 1026–1034, 2015.\n[4] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan,\nDaisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, and Others.\nCheXNet:\nRadiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning. arXiv preprint\narXiv:1711.05225, 2017.\n15\n[5] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image de-\nscriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 3128–3137, 2015.\n[6] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense\nobject detection. arXiv preprint arXiv:1708.02002, 2017.\n[7] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time\nobject detection with region proposal networks. In Advances in neural information processing\nsystems, pages 91–99, 2015.\n[8] Sujata V Ghate, Mary Scott Soo, Jay A Baker, Ruth Walsh, Edgardo I Gimenez, and Eric L\nRosen. Comparison of recall and cancer detection rates for immediate versus batch interpre-\ntation of screening mammograms. Radiology, 235(1):31–35, 2005. ISSN 0033-8419.\n[9] Khaled M Elsayes, Jonathan C Hooker, Michelle M Agrons, Ania Z Kielar, An Tang,\nKathryn J Fowler, Victoria Chernyak, Mustafa R Bashir, Yuko Kono, and Richard K Do. 2017\nversion of LI-RADS for CT and MR imaging: An update. RadioGraphics, 37(7):1994–2017,\n2017. ISSN 0271-5333.\n[10] Maciej A Mazurowski. Radiogenomics: what it is and why it is important. Journal of the\nAmerican College of Radiology, 12(8):862–866, 2015.\n[11] M A Mazurowski, J Zhang, L J Grimm, S C Yoon, and J I Silber. Radiogenomic analysis\nof breast cancer: Luminal B molecular subtype is associated with enhancement dynamics at\nMR imaging. Radiology, 273(2):365–372, 2014. doi: 10.1148/radiol.14132641.\n[12] David A Gutman, L A Cooper, Scott N Hwang, Chad A Holder, JingJing Gao, Tarun D\nAurora, William D Dunn Jr, Lisa Scarpace, Tom Mikkelsen, and Rajan Jain. MR imaging\npredictors of molecular proﬁle and survival: multi-institutional study of the TCGA glioblas-\ntoma data set. Radiology, 267(2):560–569, 2013. ISSN 0033-8419.\n[13] Maciej A Mazurowski, Kal Clark, Nicholas M Czarnek, Parisa Shamsesfandabadi, Kather-\nine B Peters, and Ashirbani Saha. Radiogenomics of lower-grade glioma: algorithmically-\nassessed tumor shape is associated with tumor genomic subtypes and patient outcomes in a\nmulti-institutional study with The Cancer Genome Atlas data. Journal of Neuro-Oncology,\npages 1–9, 2017.\n[14] Christoph A Karlo, Pier Luigi Di Paolo, Joshua Chaim, A Ari Hakimi, Irina Ostrovnaya,\nPaul Russo, Hedvig Hricak, Robert Motzer, James J Hsieh, and Oguz Akin. Radiogenomics\nof clear cell renal cell carcinoma: associations between CT imaging features and mutations.\nRadiology, 270(2):464–471, 2014. ISSN 0033-8419.\n[15] Maciej A Mazurowski, Annick Desjardins, and Jordan Milton Malof. Imaging descriptors\nimprove the predictive power of survival models for glioblastoma patients. Neuro-oncology,\n15(10):1389–94, oct 2013. ISSN 1523-5866. doi: 10.1093/neuonc/nos335.\n[16] Maciej A Mazurowski, Lars J Grimm, Jing Zhang, P Kelly Marcom, Sora C Yoon, Connie\nKim, Sujata V Ghate, and Karen S Johnson. Recurrence-free survival in breast cancer is\nassociated with MRI tumor enhancement dynamics quantiﬁed using computer algorithms.\nEuropean journal of radiology, 84(11):2117–2122, nov 2015. doi: 10.1016/j.ejrad.2015.07.012.\n16\n[17] Bradley J Erickson, Panagiotis Korﬁatis, Zeynettin Akkus, and Timothy L Kline. Machine\nLearning for Medical Imaging. RadioGraphics, 37(2):505–515, 2017. ISSN 0271-5333.\n[18] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint\narXiv:1609.04747, 2016.\n[19] Jason Yosinski, JeﬀClune, Yoshua Bengio, and Hod Lipson. How transferable are features\nin deep neural networks? In Advances in neural information processing systems, pages 3320–\n3328, 2014.\n[20] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features\noﬀ-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition workshops, pages 806–813, 2014.\n[21] Kaoutar B Ahmed, Lawrence O Hall, Dmitry B Goldgof, Renhao Liu, and Robert A Gatenby.\nFine-tuning convolutional deep features for MRI based brain tumor classiﬁcation. In Med-\nical Imaging 2017: Computer-Aided Diagnosis, volume 10134, page 101342E. International\nSociety for Optics and Photonics, 2017.\n[22] A Esteva, B Kuprel, RA Novoa, J Ko, and SM Swetter. Dermatologist-level classiﬁcation of\nskin cancer with deep neural networks. Nature, 2017.\n[23] Nima Tajbakhsh, Jae Y Shin, Suryakanth R Gurudu, R Todd Hurst, Christopher B Kendall,\nMichael B Gotway, and Jianming Liang. Convolutional neural networks for medical image\nanalysis: Full training or ﬁne tuning?\nIEEE transactions on medical imaging, 35(5):1299–\n1312, 2016.\n[24] Natalia Antropova, Benjamin Q Huynh, and Maryellen L Giger.\nA deep feature fusion\nmethodology for breast cancer diagnosis demonstrated on three imaging modality datasets.\nMedical physics, 2017.\n[25] R Paul, SH Hawkins, and Y Balagurunathan. Deep Feature Transfer Learning in Combination\nwith Traditional Features Predicts Survival Among Patients with Lung Adenocarcinoma.\nTomography, 2(4):388–395, 2016. ISSN 2379-1381. doi: 10.18383/j.tom.2016.00211.\n[26] Xinggang Wang, Wei Yang, Jeﬀrey Weinreb, Juan Han, Qiubai Li, Xiangchuang Kong,\nYongluan Yan, Zan Ke, Bo Luo, Tao Liu, and Liang Wang.\nSearching for prostate can-\ncer by fully automated magnetic resonance imaging classiﬁcation:\nDeep learning versus\nnon-deep learning.\nScientiﬁc Reports, 7(1):15415, 2017.\nISSN 20452322.\ndoi: 10.1038/\ns41598-017-15720-y.\n[27] Paras Lakhani and Baskaran Sundaram. Deep Learning at Chest Radiography: Automated\nClassiﬁcation of Pulmonary Tuberculosis by Using Convolutional Neural Networks. Radiology,\n284(2):574–582, aug 2017. ISSN 0033-8419. doi: 10.1148/radiol.2017162326.\n[28] Yanping Xue, Rongguo Zhang, Yufeng Deng, Kuan Chen, and Tao Jiang. A preliminary\nexamination of the diagnostic value of deep learning in hip osteoarthritis. PLOS ONE, 12(6):\ne0178992, jun 2017. ISSN 1932-6203. doi: 10.1371/journal.pone.0178992.\n[29] Jianning Chi, Ekta Walia, Paul Babyn, Jimmy Wang, Gary Groot, and Mark Eramian. Thy-\nroid Nodule Classiﬁcation in Ultrasound Images by Fine-Tuning Deep Convolutional Neu-\nral Network. Journal of Digital Imaging, 30(4):477–486, aug 2017. ISSN 1618727X. doi:\n10.1007/s10278-017-9997-y.\n17\n[30] A. Kumar, J. Kim, D. Lyndon, M. Fulham, and D. Feng.\nAn Ensemble of Fine-Tuned\nConvolutional Neural Networks for Medical Image Classiﬁcation. IEEE Journal of Biomedical\nand Health Informatics, 21(1):31–40, 2017. ISSN 2168-2194. doi: 10.1109/JBHI.2016.2635663.\n[31] Zhe Zhu, Ehab Albadawy, Ashirbani Saha, Jun Zhang, Michael R. Harowicz, and Maciej A.\nMazurowski. Deep Learning for identifying radiogenomic associations in breast cancer. arXiv\npreprint arXiv:1711.11097, 2017.\n[32] Zhe Zhu, Michael R. Harowicz, Jun Zhang, Ashirbani Saha, Lars J. Grimm, E. Shelley Hwang,\nand Maciej A. Mazurowski. Deep learning analysis of breast MRIs for prediction of occult\ninvasive disease in ductal carcinoma in situ. arXiv:1711.10577, 2017.\n[33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\n[34] Zeju Li, Yuanyuan Wang, Jinhua Yu, Yi Guo, and Wei Cao. Deep Learning based Radiomics\n(DLR) and its usage in noninvasive IDH1 prediction for low grade glioma. Scientiﬁc Reports,\n7(1), 2017. ISSN 20452322. doi: 10.1038/s41598-017-05848-2.\n[35] Heung-Il Suk, Seong-Whan Lee, and Dinggang Shen. Deep ensemble learning of sparse re-\ngression models for brain disease diagnosis. Medical Image Analysis, 37:101–113, 2017. ISSN\n13618415. doi: 10.1016/j.media.2017.01.008.\n[36] S Khawaldeh, U Pervaiz, A Raﬁq, and RS Alkhawaldeh Sciences. Noninvasive Grading of\nGlioma Tumor Using Magnetic Resonance Imaging with Convolutional Neural Networks.\nmdpi.com.\n[37] Germ´an Gonz´alez, Samuel Y Ash, Gonzalo Vegas Sanchez-Ferrero, Jorge Onieva Onieva,\nFarbod N Rahaghi, James C Ross, Alejandro D´ıaz, Ra´ul San Jos´e Est´epar, George R Washko,\nCOPDGene and ECLIPSE investigators, Vadose Zone, and J Accepted Paper. Disease Staging\nand Prognosis in Smokers using Deep Learning in Chest Computed Tomography. American\nJournal of Respiratory and Critical Care Medicine, pages 1–49, sep 2017. ISSN 1073-449X.\ndoi: 10.2136/vzj2016.06.0055.\n[38] Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network\nmodels for practical applications. arXiv preprint arXiv:1605.07678, 2016.\n[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[40] Christian Szegedy, Sergey Ioﬀe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4,\nInception-ResNet and the Impact of Residual Connections on Learning.\nIn AAAI, pages\n4278–4284, 2017.\n[41] Christian Szegedy, Vincent Vanhoucke, Sergey Ioﬀe, Jon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, pages 2818–2826, 2016.\n[42] Panagiotis Korﬁatis, Timothy L. Kline, Daniel H. Lachance, Ian F. Parney, Jan C. Buckner,\nand Bradley J. Erickson.\nResidual Deep Convolutional Neural Network Predicts MGMT\nMethylation Status. Journal of Digital Imaging, 30(5):622–628, oct 2017. ISSN 1618727X.\ndoi: 10.1007/s10278-017-0009-z.\n18\n[43] D H Kim and T Mackinnon. Artiﬁcial intelligence in fracture detection: transfer learning\nfrom deep convolutional neural networks. Elsevier, 2017. ISSN 1365229X. doi: 10.1016/j.\ncrad.2017.11.015.\n[44] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with\nneural networks. science, 313(5786):504–507, 2006.\n[45] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.\nGreedy layer-wise\ntraining of deep networks. In Advances in neural information processing systems, pages 153–\n160, 2007.\n[46] Christopher Poultney, Sumit Chopra, Yann L Cun, and Others. Eﬃcient learning of sparse\nrepresentations with an energy-based model. In Advances in neural information processing\nsystems, pages 1137–1144, 2007.\n[47] Andr´es Ortiz, Jorge Munilla, Francisco J. Mart´ınez-Murcia, Juan M. G´orriz, and Javier\nRam´ırez. Learning longitudinal MRI patterns by SICE and deep learning: Assessing the\nAlzheimer’s disease progression. In Communications in Computer and Information Science,\nvolume 723, pages 413–424, 2017. ISBN 9783319609638. doi: 10.1007/978-3-319-60964-5 36.\n[48] Devinder Kumar, Alexander Wong, and David A. Clausi. Lung Nodule Classiﬁcation Using\nDeep Features in CT Images. In 2015 12th Conference on Computer and Robot Vision, pages\n133–138, 2015. ISBN 978-1-4799-1986-4. doi: 10.1109/CRV.2015.25.\n[49] Youngjin Yoo, Lisa Y.W. Tang, Tom Brosch, David K.B. Li, Shannon Kolind, Irene Vavasour,\nAlexander Rauscher, Alex L. MacKay, Anthony Traboulsee, and Roger C. Tam. Deep learning\nof joint myelin and T1w MRI features in normal-appearing brain tissue to distinguish between\nmultiple sclerosis patients and healthy controls.\nNeuroImage: Clinical, 17:169–178, 2018.\nISSN 22131582. doi: 10.1016/j.nicl.2017.10.015.\n[50] Shijun Wang and Ronald M Summers.\nMachine learning and radiology.\nMedical Image\nAnalysis, 16(5):933–951, 2012. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2012.\n02.005.\n[51] Yoon Kim. Convolutional Neural Networks for Sentence Classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha,\nQatar, October 25–29, 2014. Stroudsburg, pages 1746–1751. Citeseer, 2014.\n[52] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and JeﬀDean.\nDistributed\nrepresentations of words and phrases and their compositionality.\nIn Advances in neural\ninformation processing systems, pages 3111–3119, 2013.\n[53] Matthew C. Chen, Robyn L. Ball, Lingyao Yang, Nathaniel Moradzadeh, Brian E. Chapman,\nDavid B. Larson, Curtis P. Langlotz, Timothy J. Amrhein, and Matthew P. Lungren. Deep\nLearning to Classify Radiology Free-Text Reports. Radiology, page 171115, nov 2017. ISSN\n0033-8419. doi: 10.1148/radiol.2017171115.\n[54] Bonggun Shin, Falgun H. Chokshi, Timothy Lee, and Jinho D. Choi. Classiﬁcation of radiol-\nogy reports using neural attention models. In Proceedings of the International Joint Confer-\nence on Neural Networks, volume 2017-May, pages 4363–4370, 2017. ISBN 9781509061815.\ndoi: 10.1109/IJCNN.2017.7966408.\n19\n[55] Sarvnaz Karimi, Xiang Dai, Hamedh Hassanzadeh, and Anthony Nguyen. Automatic Di-\nagnosis Coding of Radiology Reports: A Comparison of Deep Learning and Conventional\nClassiﬁcation Methods. BioNLP, pages 328–332, 2017.\n[56] Zeynettin Akkus, Issa Ali, Jiˇr´ı Sedl´aˇr, Jay P. Agrawal, Ian F. Parney, Caterina Giannini,\nand Bradley J. Erickson. Predicting Deletion of Chromosomal Arms 1p/19q in Low-Grade\nGliomas from MR Images Using Machine Intelligence. Journal of Digital Imaging, 30(4):\n469–476, aug 2017. ISSN 0897-1889. doi: 10.1007/s10278-017-9984-3.\n[57] Mohammad Havaei, Axel Davy, David Warde-Farley, Antoine Biard, Aaron Courville, Yoshua\nBengio, Chris Pal, Pierre-Marc Jodoin, and Hugo Larochelle. Brain Tumor Segmentation with\nDeep Neural Networks. arXiv preprint arXiv:1505.03540, 2015.\n[58] Saddam Hussain, Syed Muhammad Anwar, and Muhammad Majid. Brain tumor segmen-\ntation using cascaded deep convolutional neural network. In Engineering in Medicine and\nBiology Society (EMBC), 2017 39th Annual International Conference of the IEEE, pages\n1998–2001. IEEE, 2017.\n[59] Fausto Milletari, Seyed-Ahmad Ahmadi, Christine Kroll, Annika Plate, Verena Rozanski,\nJuliana Maiostre, Johannes Levin, Olaf Dietrich, Birgit Ertl-Wagner, Kai B¨otzel, and Others.\nHough-CNN: Deep learning for segmentation of deep brain regions in MRI and ultrasound.\nComputer Vision and Image Understanding, 2017.\n[60] Sergi Valverde, Mariano Cabezas, Eloy Roura, Sandra Gonz´alez-Vill`a, Deborah Pareto,\nJoan C Vilanova, Llu\\’\\is Rami´o-Torrent`a, `Alex Rovira, Arnau Oliver, and Xavier Llad´o.\nImproving automated multiple sclerosis lesion segmentation with a cascaded 3D convolu-\ntional neural network approach. NeuroImage, 155:159–168, 2017.\n[61] Christian Wachinger, Martin Reuter, and Tassilo Klein. DeepNAT: Deep convolutional neural\nnetwork for segmenting neuroanatomy. NeuroImage, 2017.\n[62] Stefano Trebeschi, Joost J M van Griethuysen, Doenja M J Lambregts, Max J Lahaye,\nChintan Parmer, Frans C H Bakers, Nicky H G M Peters, Regina G H Beets-Tan, and Hugo\nJ W L Aerts. Deep learning for fully-automated localization and segmentation of rectal cancer\non multiparametric MR. Scientiﬁc reports, 7(1):5301, 2017.\n[63] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for seman-\ntic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3431–3440, 2015.\n[64] Patrick Ferdinand Christ, Florian Ettlinger, Felix Gr¨un, Mohamed Ezzeldin A Elshaera, Jana\nLipkova, Sebastian Schlecht, Freba Ahmaddy, Sunil Tatavarty, Marc Bickel, Patrick Bilic, and\nOthers. Automatic Liver and Tumor Segmentation of CT and MRI Volumes using Cascaded\nFully Convolutional Neural Networks. arXiv preprint arXiv:1702.05970, 2017.\n[65] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention, pages 234–241. Springer, 2015.\n[66] Seyed Sadegh Mohseni Salehi, Seyed Raein Hashemi, Clemente Velasco-Annis, Abdelhakim\nOuaalam, Judy A Estroﬀ, Deniz Erdogmus, Simon K Warﬁeld, and Ali Gholipour. Real-\nTime Automatic Fetal Brain Extraction in Fetal MRI by Deep Learning.\narXiv preprint\narXiv:1710.09338, 2017.\n20\n[67] Simon J´egou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua Bengio. The\none hundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In\nComputer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference\non, pages 1175–1183. IEEE, 2017.\n[68] Lele Chen, Yue Wu, Adora M DSouza, Anas Z Abidin, Chenliang Xu, and Axel Wism¨uller.\nMRI tumor segmentation with densely connected 3D CNN.\n[69] Xiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing Fu, and Pheng Ann Heng. H-\nDenseUNet: Hybrid densely connected UNet for liver and liver tumor segmentation from CT\nvolumes. arXiv preprint arXiv:1709.07330, 2017.\n[70] Tyler Clark, Alexander Wong, Masoom A Haider, and Farzad Khalvati. Fully deep con-\nvolutional neural networks for segmentation of the prostate gland in diﬀusion-weighted MR\nimages. In International Conference Image Analysis and Recognition, pages 97–104. Springer,\n2017.\n[71] Richard McKinley, Rik Wepfer, Tom Gundersen, Franca Wagner, Andrew Chan, Roland\nWiest, and Mauricio Reyes. Nabla-net: A Deep Dag-Like Convolutional Architecture for\nBiomedical Image Segmentation. In International Workshop on Brainlesion: Glioma, Multi-\nple Sclerosis, Stroke and Traumatic Brain Injuries, pages 119–128. Springer, 2016.\n[72] Qiao Zhang, Zhipeng Cui, Xiaoguang Niu, Shijie Geng, and Yu Qiao. Image Segmentation\nwith Pyramid Dilated Convolution Based on ResNet and U-Net. In International Conference\non Neural Information Processing, pages 364–372. Springer, 2017.\n[73] Raghav Mehta and Jayanthi Sivaswamy. M-net: A Convolutional Neural Network for deep\nbrain structure segmentation. In Biomedical Imaging (ISBI 2017), 2017 IEEE 14th Interna-\ntional Symposium on, pages 437–440. IEEE, 2017.\n[74] Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M Jorge Cardoso.\nGeneralised Dice overlap as a deep learning loss function for highly unbalanced segmentations.\nIn Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision\nSupport, pages 240–248. Springer, 2017.\n[75] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation.\nIn 3D Vision (3DV), 2016 Fourth\nInternational Conference on, pages 565–571. IEEE, 2016.\n[76] ¨Ozg¨un C¸i¸cek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger.\n3D U-Net: learning dense volumetric segmentation from sparse annotation. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 424–\n432. Springer, 2016.\n[77] Cem M Deniz, Spencer Hallyburton, Arakua Welbeck, Stephen Honig, Kyunghyun Cho,\nand Gregory Chang.\nSegmentation of the Proximal Femur from MR Images using Deep\nConvolutional Neural Networks. arXiv preprint arXiv:1704.06176, 2017.\n[78] Liyue Shen and Timothy Anderson. Multimodal Brain MRI Tumor Segmentation via Con-\nvolutional Neural Networks.\n21\n[79] Xin Yang, Lequan Yu, Lingyun Wu, Yi Wang, Dong Ni, Jing Qin, and Pheng-Ann Heng.\nFine-Grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound\nImages. In AAAI, pages 1633–1639, 2017.\n[80] Rudra P K Poudel, Pablo Lamata, and Giovanni Montana. Recurrent fully convolutional\nneural networks for multi-slice mri cardiac segmentation.\nIn International Workshop on\nReconstruction and Analysis of Moving Body Organs, pages 83–94. Springer, 2016.\n[81] Jinzheng Cai, Le Lu, Yuanpu Xie, Fuyong Xing, and Lin Yang. Improving deep pancreas\nsegmentation in ct and mri images via recurrent neural contextual learning and direct loss\nfunction. arXiv preprint arXiv:1707.04912, 2017.\n[82] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9\n(8):1735–1780, 1997.\n[83] Paul Jaccard. The distribution of the ﬂora in the alpine zone. New phytologist, 11(2):37–50,\n1912.\n[84] Yousef Al-Kofahi, Wiem Lassoued, William Lee, and Badrinath Roysam. Improved automatic\ndetection and segmentation of cell nuclei in histopathology images. IEEE Transactions on\nBiomedical Engineering, 57(4):841–852, 2010.\n[85] Arnau Oliver, Jordi Freixenet, Joan Marti, Elsa P´erez, Josep Pont, Erika R E Denton, and\nReyer Zwiggelaar. A review of automatic mass detection and segmentation in mammographic\nimages. Medical image analysis, 14(2):87–110, 2010.\n[86] David Rey, G´erard Subsol, Herv´e Delingette, and Nicholas Ayache. Automatic detection and\nsegmentation of evolving processes in 3D medical images: Application to multiple sclerosis.\nMedical image analysis, 6(2):163–179, 2002.\n[87] Holger R Roth, Le Lu, Ari Seﬀ, Kevin M Cherry, Joanne Hoﬀman, Shijun Wang, Jiamin\nLiu, Evrim Turkbey, and Ronald M Summers. A new 2.5 D representation for lymph node\ndetection using random sets of deep convolutional neural network observations. In Interna-\ntional Conference on Medical Image Computing and Computer-Assisted Intervention, pages\n520–527. Springer, 2014.\n[88] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. Scalable\nobject detection using deep neural networks.\nIn Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 2147–2154, 2014.\n[89] Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov, and Sergey Ioﬀe. Scal-\nable, high-quality object detection. arXiv preprint arXiv:1412.1441, 2014.\n[90] Hoo-Chang Shin, Holger R Roth, Mingchen Gao, Le Lu, Ziyue Xu, Isabella Nogues, Jian-\nhua Yao, Daniel Mollura, and Ronald M Summers. Deep convolutional neural networks for\ncomputer-aided detection: CNN architectures, dataset characteristics and transfer learning.\nIEEE transactions on medical imaging, 35(5):1285–1298, 2016.\n[91] Ravi K Samala, Heang-Ping Chan, Lubomir Hadjiiski, Mark A Helvie, Jun Wei, and Kenny\nCha. Mass detection in digital breast tomosynthesis: Deep convolutional neural network with\ntransfer learning from mammography. Medical physics, 43(12):6654–6666, 2016.\n22\n[92] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolu-\ntions. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 1–9, 2015.\n[93] Ruhan Sa, William Owens, Raymond Wiegand, Mark Studin, Donald Capoferri, Kenneth\nBarooha, Alexander Greaux, Robert Rattray, Adam Hutton, John Cintineo, and Others.\nIntervertebral disc detection in X-ray images using faster R-CNN. In Engineering in Medicine\nand Biology Society (EMBC), 2017 39th Annual International Conference of the IEEE, pages\n564–567. IEEE, 2017.\n[94] Jiamin Liu, David Wang, Le Lu, Zhuoshi Wei, Lauren Kim, Evrim B Turkbey, Berkman\nSahiner, Nicholas Petrick, and Ronald M Summers. detection and diagnosis of colitis on\ncomputed tomography using deep convolutional neural networks. Medical Physics, 2017.\n[95] Rami Ben-Ari, Ayelet Akselrod-Ballin, Leonid Karlinsky, and Sharbell Hashoul.\nDomain\nspeciﬁc convolutional neural nets for detection of architectural distortion in mammograms.\nIn Biomedical Imaging (ISBI 2017), 2017 IEEE 14th International Symposium on, pages\n552–556. IEEE, 2017.\n[96] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.\nYou only look once:\nUniﬁed, real-time object detection. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 779–788, 2016.\n[97] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang\nFu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on\ncomputer vision, pages 21–37. Springer, 2016.\n[98] Richard Platania, Shayan Shams, Seungwon Yang, Jian Zhang, Kisung Lee, and Seung-Jong\nPark.\nAutomated Breast Cancer Diagnosis Using Deep Learning and Region of Interest\nDetection (BC-DROID). In Proceedings of the 8th ACM International Conference on Bioin-\nformatics, Computational Biology, and Health Informatics, pages 536–543. ACM, 2017.\n[99] Zhantao Cao, Lixin Duan, Guowu Yang, Ting Yue, Qin Chen, Huazhu Fu, and Yanwu Xu.\nBreast Tumor Detection in Ultrasound Images Using Deep Learning. In International Work-\nshop on Patch-based Techniques in Medical Imaging, pages 121–128. Springer, 2017.\n[100] Ning Li, Haopeng Liu, Bin Qiu, Wei Guo, Shijun Zhao, Kungang Li, and Jie He. Detection\nand Attention: Diagnosing Pulmonary Lung Cancer from CT by Imitating Physicians. arXiv\npreprint arXiv:1712.05114, 2017.\n[101] Bob D de Vos, Jelmer M Wolterink, Pim A de Jong, Max A Viergever, and Ivana Isgum.\n2D image classiﬁcation for 3D anatomy localization: employing deep convolutional neural\nnetworks. In Medical Imaging: Image Processing, page 97841Y, 2016.\n[102] Adhish Prasoon, Kersten Petersen, Christian Igel, Fran¸cois Lauze, Erik Dam, and Mads\nNielsen. Deep feature learning for knee cartilage segmentation using a triplanar convolutional\nneural network.\nIn International conference on medical image computing and computer-\nassisted intervention, pages 246–253. Springer, 2013.\n[103] Holger R Roth, Yinong Wang, Jianhua Yao, Le Lu, Joseph E Burns, and Ronald M Summers.\nDeep convolutional networks for automated detection of posterior-element fractures on spine\nCT. arXiv preprint arXiv:1602.00020, 2016.\n23\n[104] Holger R Roth, Le Lu, Jiamin Liu, Jianhua Yao, Ari Seﬀ, Kevin Cherry, Lauren Kim, and\nRonald M Summers. Improving computer-aided detection using convolutional neural networks\nand random view aggregation. IEEE transactions on medical imaging, 35(5):1170–1181, 2016.\n[105] Martin Simonovsky, Benjam´ın Guti´errez-Becker, Diana Mateus, Nassir Navab, and Nikos\nKomodakis.\nA deep metric for multimodal registration.\nIn International Conference on\nMedical Image Computing and Computer-Assisted Intervention, pages 10–18. Springer, 2016.\n[106] Frederik Maes, Andre Collignon, Dirk Vandermeulen, Guy Marchal, and Paul Suetens. Multi-\nmodality image registration by maximization of mutual information. Medical Imaging, IEEE\nTransactions on, 16(2):187–198, 1997. ISSN 0278-0062.\n[107] Rui Liao, Shun Miao, Pierre de Tournemire, Sasa Grbic, Ali Kamen, Tommaso Mansi, and\nDorin Comaniciu. An Artiﬁcial Agent for Robust Image Registration. In AAAI, pages 4168–\n4175, 2017.\n[108] Hessam Sokooti, Bob de Vos, Floris Berendsen, Boudewijn P. F. Lelieveldt, Ivana Isgum,\nand Marius Staring. Nonrigid Image Registration Using Multi-scale 3D Convolutional Neural\nNetworks. In Medical Image Computing and Computer Assisted Intervention, pages 232–239.\n2017. ISBN 978-3-319-66182-7. doi: 10.1007/978-3-319-66182-7 27.\n[109] Jun Lv, Ming Yang, Jue Zhang, and Xiaoying Wang. Respiratory motion correction for free-\nbreathing 3D abdominal MRI using CNN based image registration: a feasibility study. The\nBritish Journal of Radiology, page 20170788, dec 2017. ISSN 0007-1285. doi: 10.1259/bjr.\n20170788.\n[110] Bob D de Vos, Floris F Berendsen, Max A Viergever, Marius Staring, and Ivana Iˇsgum. End-\nto-End Unsupervised Deformable Image Registration with a Convolutional Neural Network\nBT - Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision\nSupport : Third International Workshop, DLMIA 2017, and 7th International. pages 204–\n212. Springer International Publishing, Cham, 2017. ISBN 978-3-319-67558-9. doi: 10.1007/\n978-3-319-67558-9 24.\n[111] Khosro Bahrami, Feng Shi, Xiaopeng Zong, Hae Won Shin, Hongyu An, and Dinggang Shen.\nReconstruction of 7T-Like Images from 3T MRI. IEEE Transactions on Medical Imaging, 35\n(9):2085–2097, 2016. ISSN 1558254X. doi: 10.1109/TMI.2016.2549918.\n[112] J Schlemper, J Caballero, J V Hajnal, A Price, and D Rueckert. A Deep Cascade of Con-\nvolutional Neural Networks for Dynamic MR Image Reconstruction. IEEE Transactions on\nMedical Imaging, PP(99):1, 2017. ISSN 0278-0062 VO - PP. doi: 10.1109/TMI.2017.2760978.\n[113] Guang Yang, Simiao Yu, Hao Dong, Greg Slabaugh, Pier Luigi Dragotti, Xujiong Ye, Fangde\nLiu, Simon Arridge, Jennifer Keegan, Yike Guo, and David Firmin.\nDAGAN: Deep De-\nAliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction.\nIEEE Transactions on Medical Imaging, pages 1–1, 2017. ISSN 0278-0062. doi: 10.1109/\nTMI.2017.2785879.\n[114] Agisilaos Chartsias, Thomas Joyce, Mario Valerio Giuﬀrida, and Sotirios A. Tsaftaris. Mul-\ntimodal MR Synthesis via Modality-Invariant Latent Representation.\nIEEE Transactions\non Medical Imaging, 0062(JANUARY):1–1, 2017. ISSN 0278-0062. doi: 10.1109/TMI.2017.\n2764326.\n24\n[115] Hu Chen, Yi Zhang, Weihua Zhang, Peixi Liao, Ke Li, Jiliu Zhou, and Ge Wang. Low-dose\nCT via convolutional neural network. Biomedical optics express, 8(2):679–694, 2017. ISSN\n2156-7085.\n[116] Eunhee Kang, Junhong Min, and Jong Chul Ye. A deep convolutional neural network using\ndirectional wavelets for low-dose X-ray CT reconstruction. Medical Physics, 44(10):e360–e375,\noct 2017. ISSN 00942405. doi: 10.1002/mp.12344.\n[117] Kyong Hwan Jin, Michael T. McCann, Emmanuel Froustey, and Michael Unser. Deep Con-\nvolutional Neural Network for Inverse Problems in Imaging. IEEE Transactions on Image\nProcessing, 26(9):4509–4522, 2017. ISSN 10577149. doi: 10.1109/TIP.2017.2713099.\n[118] Dong Nie, Xiaohuan Cao, Yaozong Gao, Li Wang, and Dinggang Shen.\nEstimating CT\nImage from MRI Data Using 3D Fully Convolutional Networks BT - Deep Learning and\nData Labeling for Medical Applications: First International Workshop, LABELS 2016, and\nSecond International Workshop, DLMIA 2016, Held in Conjunction with MICC. pages 170–\n178. Springer International Publishing, Cham, 2016. ISBN 978-3-319-46976-8. doi: 10.1007/\n978-3-319-46976-8 18.\n[119] Xiao Han. MR-based synthetic CT generation using a deep convolutional neural network\nmethod. Medical Physics, 44(4):1408–1419, apr 2017. ISSN 00942405. doi: 10.1002/mp.12155.\n[120] Rongjian Li, Wenlu Zhang, Heung-Il Suk, Li Wang, Jiang Li, Dinggang Shen, and Shuiwang\nJi. Deep Learning Based Imaging Data Completion for Improved Brain Disease Diagnosis.\nMedical image computing and computer-assisted intervention : MICCAI ... International\nConference on Medical Image Computing and Computer-Assisted Intervention, 17(0 3):305–\n312, 2014.\n[121] Ozan Oktay, Wenjia Bai, Matthew Lee, Ricardo Guerrero, Konstantinos Kamnitsas, Jose\nCaballero, Antonio de Marvao, Stuart Cook, Declan O’Regan, and Daniel Rueckert. Multi-\ninput Cardiac Image Super-Resolution Using Convolutional Neural Networks.\nIn Medical\nImage Computing and Computer-Assisted Intervention – MICCAI 2016: 19th International\nConference, Athens, Greece, October 17-21, 2016, Proceedings, Part III, pages 246–254, 2016.\nISBN 3642236286. doi: 10.1007/978-3-319-46726-9 29.\n[122] Kensuke Umehara, Junko Ota, and Takayuki Ishida. Application of Super-Resolution Con-\nvolutional Neural Network for Enhancing Image Resolution in Chest CT, oct 2017. ISSN\n1618727X.\n[123] A Benou, R Veksler, A Friedman, and T Riklin Raviv.\nEnsemble of expert deep neural\nnetworks for spatio-temporal denoising of contrast-enhanced MRI sequences. Medical Image\nAnalysis, 42:145–159, 2017. doi: 10.1016/j.media.2017.07.006.\n[124] Dongsheng Jiang, Weiqiang Dou, Luc Vosters, Xiayu Xu, Yue Sun, and Tao Tan. Denoising\nof 3D magnetic resonance images with multi-channel residual learning of convolutional neural\nnetwork. arxiv.org, 2017.\n[125] W Yang, Y Chen, Y Liu, L Zhong, G Qin, Z Lu Medical, and Undeﬁned 2017. Cascade\nof multi-scale convolutional neural networks for bone suppression of chest radiographs in\ngradient domain. Medical Image Analysis, 35:421–433, 2017.\n25\n[126] Yanbo Zhang, Ying Chu, and Hengyong Yu. Reduction of metal artifacts in x-ray CT images\nusing a convolutional neural network. volume 10391, pages 103910V–10391–11, 2017.\n[127] Adnan Qayyum, Syed Muhammad Anwar, Muhammad Awais, and Muhammad Majid. Medi-\ncal image retrieval using deep convolutional neural network. Neurocomputing, 266(Supplement\nC):8–20, 2017. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2017.05.025.\n[128] Ke Yan, Xiaosong Wang, Le Lu, Ling Zhang, Adam Harrison, Mohammadhad Bagheri, and\nRonald Summers. Deep lesion graphs in the wild: Relationship learning and organization of\nsigniﬁcant radiology image ﬁndings in a diverse large-scale lesion database. arXiv preprint\narXiv:1711.10535, 2017.\n[129] Li Sze Chow and Raveendran Paramesran.\nReview of medical image quality assessment.\nBiomedical Signal Processing and Control, 27:145–154, may 2016.\nISSN 1746-8094.\ndoi:\n10.1016/J.BSPC.2016.02.006.\n[130] L Wu, JZ Cheng, S Li, B Lei, and T Wang. FUIQA: Fetal ultrasound image quality assessment\nwith deep convolutional networks. IEEE Trans Cybernetics, 45(5):1336 – 1349, 2017.\n[131] AH Abdi, C Luong, T Tsang, and G Allan Imaging.\nAutomatic Quality Assessment of\nEchocardiograms Using Convolutional Neural Networks:\nFeasibility on the Apical Four-\nChamber View. IEEE Trans Med Imaging, 36(6):1221–1230, 2017.\n[132] Steven J. Esses, Xiaoguang Lu, Tiejun Zhao, Krishna Shanbhogue, Bari Dane, Mary Bruno,\nand Hersh Chandarana. Automated image quality evaluation of T2-weighted liver MRI uti-\nlizing deep learning architecture. Journal of Magnetic Resonance Imaging, jun 2017. ISSN\n10531807. doi: 10.1002/jmri.25779.\n[133] William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P Bradley, and Lyle J\nPalmer. Detecting hip fractures with radiologist-level performance using deep neural net-\nworks. arXiv preprint arXiv:1711.06504, 2017.\n[134] Monika Grewal, Muktabh Mayank Srivastava, Pulkit Kumar, and Srikrishna Varadarajan.\nRADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE detection\nin CT Scans. arXiv preprint arXiv:1710.04934, 2017.\n[135] Amir Jamaludin, Meelis Lootus, Timor Kadir, Andrew Zisserman, Jill Urban, Michele C\nBatti´e, Jeremy Fairbank, and Iain McCall. Automation of reading of radiological features\nfrom magnetic resonance images (MRIs) of the lumbar spine without human intervention is\ncomparable with an expert radiologist. European Spine Journal, 26(5):1374–1383, 2017. ISSN\n1432-0932. doi: 10.1007/s00586-017-4956-3.\n[136] Thijs Kooi, Geert Litjens, Bram van Ginneken, Albert Gubern-M´erida, Clara I S´anchez,\nRitse Mann, Ard den Heeten, and Nico Karssemeijer. Large scale deep learning for computer\naided detection of mammographic lesions. Medical image analysis, 35:303–312, 2017. ISSN\n1361-8415.\n[137] David B Larson, Matthew C Chen, Matthew P Lungren, Safwan S Halabi, Nicholas V Stence,\nand Curtis P Langlotz. Performance of a Deep-Learning Neural Network Model in Assessing\nSkeletal Maturity on Pediatric Hand Radiographs. Radiology, page 170236, nov 2017. ISSN\n0033-8419. doi: 10.1148/radiol.2017170236.\n26\n[138] Jameson Merkow, Robert Luftkin, Kim Nguyen, Stefano Soatto, Zhuowen Tu, and Andrea\nVedaldi.\nDeepRadiologyNet: Radiologist Level Pathology Detection in CT Head Images.\narXiv preprint arXiv:1711.09313, 2017.\n[139] Jakub Olczak, Niklas Fahlberg, Atsuto Maki, Ali Sharif Razavian, Anthony Jilert, Andr´e\nStark, Olof Sk¨oldenberg, and Max Gordon. Artiﬁcial intelligence for analyzing orthopedic\ntrauma radiographs.\nActa Orthopaedica, 88(6):581–586, nov 2017.\nISSN 1745-3674.\ndoi:\n10.1080/17453674.2017.1344459.\n27\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "stat.AP",
    "stat.ML"
  ],
  "published": "2018-02-10",
  "updated": "2018-02-10"
}