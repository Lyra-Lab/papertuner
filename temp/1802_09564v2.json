{
  "id": "http://arxiv.org/abs/1802.09564v2",
  "title": "Reinforcement and Imitation Learning for Diverse Visuomotor Skills",
  "authors": [
    "Yuke Zhu",
    "Ziyu Wang",
    "Josh Merel",
    "Andrei Rusu",
    "Tom Erez",
    "Serkan Cabi",
    "Saran Tunyasuvunakool",
    "János Kramár",
    "Raia Hadsell",
    "Nando de Freitas",
    "Nicolas Heess"
  ],
  "abstract": "We propose a model-free deep reinforcement learning method that leverages a\nsmall amount of demonstration data to assist a reinforcement learning agent. We\napply this approach to robotic manipulation tasks and train end-to-end\nvisuomotor policies that map directly from RGB camera inputs to joint\nvelocities. We demonstrate that our approach can solve a wide variety of\nvisuomotor tasks, for which engineering a scripted controller would be\nlaborious. In experiments, our reinforcement and imitation agent achieves\nsignificantly better performances than agents trained with reinforcement\nlearning or imitation learning alone. We also illustrate that these policies,\ntrained with large visual and dynamics variations, can achieve preliminary\nsuccesses in zero-shot sim2real transfer. A brief visual description of this\nwork can be viewed in https://youtu.be/EDl8SQUNjj0",
  "text": "Reinforcement and Imitation Learning\nfor Diverse Visuomotor Skills\nYuke Zhu†\nZiyu Wang‡\nJosh Merel‡\nAndrei Rusu‡\nTom Erez‡\nSerkan Cabi‡\nSaran Tunyasuvunakool‡\nJ´anos Kram´ar‡\nRaia Hadsell‡\nNando de Freitas‡\nNicolas Heess‡\n†Computer Science Department, Stanford University, USA\n‡DeepMind, London, UK\nAbstract—We propose a model-free deep reinforcement learn-\ning method that leverages a small amount of demonstration data\nto assist a reinforcement learning agent. We apply this approach\nto robotic manipulation tasks and train end-to-end visuomotor\npolicies that map directly from RGB camera inputs to joint\nvelocities. We demonstrate that our approach can solve a wide\nvariety of visuomotor tasks, for which engineering a scripted\ncontroller would be laborious. In experiments, our reinforcement\nand imitation agent achieves signiﬁcantly better performances\nthan agents trained with reinforcement learning or imitation\nlearning alone. We also illustrate that these policies, trained with\nlarge visual and dynamics variations, can achieve preliminary\nsuccesses in zero-shot sim2real transfer. A brief visual description\nof this work can be viewed in this video.\nI. INTRODUCTION\nRecent advances in deep reinforcement learning (RL) have\nperformed very well in several challenging domains such\nas video games [29] and Go [46]. For robotics, RL in\ncombination with powerful function approximators such as\nneural networks provides a general framework for designing\nsophisticated controllers that would be hard to handcraft oth-\nerwise. Reinforcement learning methods have a long history\nin robotics control but have typically been used with low-\ndimensional movement representations [4, 20]. The last few\nyears have seen a growing number of successful demonstra-\ntions of deep RL for robotic manipulation using model-based\n(e.g. Levine et al. [23], Yahya et al. [52], Levine et al. [24])\nand model-free techniques (e.g. Chebotar et al. [3], Gu et al.\n[9], Popov et al. [35]), both in simulation and on real hardware.\nNevertheless, end-to-end learning of visuomotor controllers for\nlong-horizon and multi-stage manipulation tasks using model-\nfree RL techniques remains a challenging problem.\nDeveloping RL agents for robotics requires overcoming sev-\neral signiﬁcant challenges. Policies for robotics must transform\nmulti-modal and partial observations from noisy sensors, such\nas cameras, into coordinated activity of many degrees of free-\ndom. At the same time, realistic tasks often come with contact-\nrich dynamics and vary along multiple dimensions (visual\nappearance, position, shapes, etc.), posing signiﬁcant general-\nization challenges. Model-based methods can have difﬁculties\nhandling such complex dynamics and large variations. Directly\ntraining model-free methods on real robotics hardware can be\ndaunting due to the high sample complexity. The difﬁculty of\n* This work was done when Yuke Zhu (yukez@cs.stanford.edu)\nworked as a summer intern at DeepMind.\n3D motion controller\nCollecting\ndemonstrations\nphysics engine\nTraining in\nsimulation\nreal environment\nRunning on\nreal robot\nFig. 1: Our proposal of a principled robot learning pipeline. We\nused 3D motion controllers to collect human demonstrations\nof a task. Our reinforcement and imitation learning model\nleveraged these demonstrations to facilitate learning in a sim-\nulated physical engine. We then performed sim2real transfer\nto deploy the learned visuomotor policy to a real robot.\nreal-world RL training is compounded by safety considerations\nas well as the difﬁculty of accessing information about the\nstate of the environment (e.g. the position of an object) to\ndeﬁne a reward function. Finally, even in simulation when\nperfect state information and large amounts of training data are\navailable, exploration can be a signiﬁcant challenge, especially\nfor on-policy methods. This is partly due to the often high-\ndimensional and continuous action space, but also due to the\ndifﬁculty of designing suitable reward functions.\nIn this paper, we present a model-free deep RL method\nthat can solve a variety of robotic manipulation tasks directly\nfrom pixel input. Our key insights are 1) to reduce the\ndifﬁculty of exploration in continuous domains by leveraging\na handful of human demonstrations; 2) to leverage several new\ntechniques that exploit privileged and task-speciﬁc information\nduring training only which can accelerate and stabilize the\nlearning of visuomotor policies in multi-stage tasks; and 3)\nto improve generalization by increasing the diversity of the\ntraining conditions. As a result, the policies work well under\nsigniﬁcant variations of system dynamics, object appearances,\ntask lengths, etc. Furthermore, we demonstrate promising\npreliminary results for two tasks, where the policies trained\nin simulation achieve zero-shot transfer to a real robot.\nWe evaluate our method on six manipulation tasks, includ-\ning stacking, pouring, etc. The set of tasks includes multi-\nstage and long-horizon tasks, and they require full 9-DoF joint\narXiv:1802.09564v2  [cs.RO]  27 May 2018\nvelocity control directly from pixels. The controllers need to\nbe able to handle signiﬁcant shape and appearance variations.\nTo address these challenges, our method combines imitation\nlearning with reinforcement learning into a uniﬁed training\nframework. Our approach utilizes demonstration data in two\nways: ﬁrst, it uses a hybrid reward that combines the task\nreward with an imitation reward based on Generative Ad-\nversarial Imitation Learning [15]. This aids with exploration\nwhile still allowing the ﬁnal controller to outperform the\nhuman demonstrator on the task. Second, it uses demonstration\ntrajectories to construct a curriculum of states along which\nto initialize the episodes during training. This enables the\nagent to learn about later stages of the task earlier in training,\nfacilitating the solving of long tasks. As a result, our approach\nsolves all six tasks, which neither the reinforcement learning\nnor imitation learning baselines can solve alone.\nTo sidestep the constraints of training on real hardware\nwe embrace the sim2real paradigm which has recently shown\npromising results [17, 39, 47]. Through the use of a physics\nengine and high-throughput RL algorithms, we can simulate\nparallel copies of a robot arm to perform millions of com-\nplex physical interactions in a contact-rich environment while\neliminating the practical concerns of robot safety and system\nreset. Furthermore, we can, during training, exploit privileged\nand task-speciﬁc information about the true system state with\nseveral new techniques, including learning policy and value in\nseparate modalities, an object-centric GAIL discriminator, and\nauxiliary tasks for visual modules. These techniques stabilize\nand speed up policy learning, without imposing any constraints\non the system at test time.\nFinally, we diversify training conditions such as visual\nappearance, object geometry, and system dynamics. This\nimproves both generalization with respect to different task\nconditions as well as transfer from simulation to reality.\nWe use the same model and the same algorithm with only\nsmall task-speciﬁc modiﬁcations of the training setup to learn\nvisuomotor controllers for six diverse robot arm manipulation\ntasks. As illustrated in Fig. 1 this instantiates a visuomotor\nlearning pipeline going from collecting human demonstration\nto learning in simulation, and back to real-world deployment\nvia sim2real policy transfer.\nII. RELATED WORK\nReinforcement learning methods have been extensively used\nwith low-dimensional policy representations such as move-\nment primitives to solve a variety of control problems both in\nsimulation and in reality. Three classes of RL algorithms are\ncurrently dominant for continuous control problems: guided\npolicy search methods (GPS; Levine and Koltun [22]), value-\nbased methods such as the deterministic policy gradient (DPG;\nSilver et al. [45], Lillicrap et al. [26], Heess et al. [12])\nor the normalized advantage function (NAF; Gu et al. [10])\nalgorithm, and trust-region based policy gradient algorithms\nsuch as trust region policy optimization (TRPO) and proximal\npolicy optimization (PPO). TRPO [42] and PPO [43] hold ap-\npeal due to their robustness to hyperparameter settings as well\nas their scalability [14] but the lack of sample efﬁciency makes\nthem unsuitable for training directly on robotics hardware.\nGPS [22] has been used e.g. by Levine et al. [23], Yahya\net al. [52] and Chebotar et al. [3] to learn visuomotor policies\ndirectly on a real robotics hardware after a network pretraining\nphase. Gupta et al. [11] and Kumar et al. [21] use GPS\nfor learning controllers for robotic hand models. Value-based\nmethods have been employed, e.g. by Gu et al. [9] who use\nNAF to learn a door opening task directly on a robot while\nPopov et al. [35] demonstrate how to solve a stacking problem\nefﬁciently using a distributed variant of DPG.\nThe idea of using large-scale data collection for training\nvisuomotor controllers has been the focus of Levine et al. [24]\nand Pinto and Gupta [33] who train a convolutional network\nto predict grasp success for diverse sets of objects using a\nlarge dataset with 10s or 100s of thousands of grasp attempts\ncollected from multiple robots in a self-supervised setting.\nAn alternative strategy for dealing with the data demand\nis to train in simulation and transfer the learned controller to\nreal hardware, or to augment real-world training with synthetic\ndata. Rusu et al. [40] learn simple visuomotor policies for\na Jaco robot arm and transfer to reality using progressive\nnetworks [39]. Viereck et al. [50] minimize the reality gap\nby relying on depth. Tobin et al. [47] use visual variations to\nlearn robust object detectors that can transfer to reality; James\net al. [17] combine randomization with supervised learning.\nBousmalis et al. [2] augment the training with simulated data\nto learn grasp prediction of diverse shapes.\nSuitable cost functions and exploration strategies for control\nproblems are challenging to design, so demonstrations have\nlong played an important role. Demonstrations can be used\nto initialize policies, design cost functions, guide exploration,\naugment the training data, or a combination of these. Cost\nfunctions can be derived from demonstrations either via\ntracking objectives (e.g. Gupta et al. [11]) or via inverse\nRL (e.g. Boularias et al. [1], Finn et al. [6]), or, as in our\ncase, via adversarial learning [15]. When expert actions or\nexpert policies are available, behavioral cloning can be used\n(Rahmatizadeh et al. [36], James et al. [17], Duan et al. [5]).\nAlternatively, expert trajectories can be used as additional\ntraining data for off-policy algorithms such as DPG (e.g.\nVecerik et al. [49]). Most of these methods require observation\nand/or action spaces to be aligned between the robot and\ndemonstrations. Recently, methods for third person imitation\nhave been proposed (e.g. Sermanet et al. [44], Liu et al.\n[27], Finn et al. [7]).\nConcurrently with our work several papers have presented\nresults on manipulation tasks. Rajeswaran et al. [37], Nair\net al. [30] both use human demonstrations to aid exploration.\nNair et al. [30] extends the DDPGfD algorithm [49] to learn a\nblock stacking task on a position-controlled arm in simulation.\nRajeswaran et al. [37] use the demonstrations with a form\nof behavioral cloning and data augmentation to learn several\ncomplex manipulation tasks. In both cases, controllers observe\na low-level state feature and these methods inherently require\naligned state and action spaces with the demonstrations. In\npixel\nobservation\nproprioceptive\nfeature\nobject-centric\nfeature\nCNN\nMLP\nLSTM\nLSTM\njoint \nvelocity\nvalue\nfunction\nstate prediction\nauxiliary tasks\nMLP\ndeep visuomotor policy\nMLP\ndiscriminator\nscore\nGAIL\nDiscriminator\n(MLP)\nD (s, a)\n⇡✓(a|s)\nVφ(s)\nFig. 2: Model overview. The core of our model is the deep visuomotor policy, which takes the camera observation and the\nproprioceptive feature as input and produces the next joint velocities.\ncontrast, our method learns end-to-end visuomotor policies\nwithout reliance on demonstrator actions. Thus, it can utilize\ndemonstrations when raw demonstrator actions are unknown\nor generated by a different body. Pinto et al. [34] and Peng\net al. [32] address the transfer from simulation to reality,\nfocusing on randomizing visual appearance and robot dynam-\nics respectively. Peng et al. transfer a block-pushing policy\noperating from state features to a 7-DoF position controlled\nFetch robotics arm. Pinto et al. consider different tasks using\nvisual input with end-effector position control. These concur-\nrent works have each introduced a subset of techniques that\nour model employs. This work, developed independently from\nconcurrent works, integrates several new techniques into one\ncoherent method. Our experimental results demonstrate that\ngood performances come from the synergy of these combined\ntechniques.\nIII. MODEL\nOur goal is to learn a visuomotor policy with deep neu-\nral networks for robot manipulation tasks. The policy takes\nboth an RGB camera observation and a proprioceptive fea-\nture vector that describes the joint positions and angular\nvelocities. These two sensory modalities are also available\non the real robot, allowing us to train in simulation and\nsubsequently transfer the learned policy to the robot without\nmodiﬁcations. Fig. 2 provides an overview of our model. The\ndeep visuomotor policy encodes the pixel observation with a\nconvolutional network (CNN) and the proprioceptive feature\nwith a multilayer perceptron (MLP). The features from these\ntwo modules are concatenated and passed to a recurrent long\nshort term memory (LSTM) layer before producing the joint\nvelocities (control commands). The whole network is trained\nend-to-end. We start with a brief review of the basics of\ngenerative adversarial imitation learning (GAIL) and proximal\npolicy optimization (PPO). Our model extends upon these two\nmethods for visuomotor skills.\nA. Background: GAIL and PPO\nImitation learning (IL) is the problem of learning a behavior\npolicy by mimicking a set of demonstrations. Here we assume\nthat human demonstrations are provided as a dataset of state-\naction pairs D = {(si, ai)}i=1...N. Some IL methods cast the\nproblem as one of supervised learning, i.e., behavior cloning.\nThese methods use maximum likelihood to train a parameter-\nized policy πθ : S →A, where S is the state space and A is\nthe action space, such that θ∗= arg maxθ\nP\nN log πθ(ai|si).\nThe behavior cloning approach works effectively when demon-\nstrations are abundant [38]. However, as robot demonstrations\ncan be costly and time-consuming to collect, we aim for\na method that can learn from a handful of demonstrations.\nGAIL [15] uses demonstration data efﬁciently by allowing\nthe agent to interact with the environment and learn from its\nown experiences. Similar to Generative Adversarial Networks\n(GANs) [8], GAIL employs two networks, a policy network\nπθ : S →A and a discriminator network Dψ : S×A →[0, 1].\nIt uses a min-max objective function similar to that of GANs:\nmin\nθ\nmax\nψ\nEπE[log Dψ(s, a)] + Eπθ[log(1 −Dψ(s, a))],\n(1)\nwhere πE denotes the expert policy that generated the demon-\nstration trajectories. This objective encourages the policy πθ to\nhave an occupancy measure close to that of the expert policy.\nIn this work we train πθ with policy gradient methods\nto maximize the discounted sum of the reward function\nrgail(st, at) = −log(1 −Dψ(st, at)), clipped at a max value\nof 10. In continuous domains, trust region methods greatly\nstabilize policy training. GAIL was originally presented in\ncombination with TRPO [42] for updating the policy. Recently,\nPPO [43] has been proposed as a simple and scalable ap-\nproximation to TRPO. PPO only relies on ﬁrst-order gradients\nand can be easily implemented with recurrent networks in a\ndistributed setting [14]. PPO implements an approximate trust\nregion that limits the change in the policy per iteration. This\nis achieved via a regularization term based on the Kullback-\nLeibler (KL) divergence, the strength of which is adjusted\ndynamically depending on actual change in the policy in past\niterations.\nB. Reinforcement and Imitation Learning Model\n1) Hybrid IL/RL Reward: Shaping rewards are a popular\nmeans of facilitating exploration. Although reward shaping can\nbe very effective it can also lead to suboptimal solutions [31].\nHence, we design the task rewards as sparse piecewise constant\nfunctions based on the different stages of the respective tasks.\nFor example, we deﬁne three stages for the block stacking task,\nincluding reaching, lifting, and stacking. Reward change only\noccurs when the task transits from one stage to another. In\npractice, we ﬁnd deﬁning such a sparse multi-stage reward\neasier than handcrafting a dense shaping reward and less\nprone to producing suboptimal behaviors. Training agents in\ncontinuous domains with sparse or piecewise constant rewards\nis challenging. Inspired by reward augmentation as described\nin Li et al. [25] and Merel et al. [28], we provide additional\nguidance via a hybrid reward function that combines the\nimitation reward rgail with the task reward rtask:\nr(st, at) = λrgail(st, at)+(1−λ)rtask(st, at) λ ∈[0, 1]. (2)\nMaximizing this hybrid reward can be interpreted as si-\nmultaneous reinforcement and imitation learning, where the\nimitation reward encourages the policy to generate trajectories\ncloser to demonstration trajectories, and the task reward en-\ncourages the policy to achieve high returns on the task. Setting\nλ to either 0 or 1 reduces this method to the standard RL or\nGAIL setups. In our experiments, with a balanced contribution\nof these two rewards the agents can solve tasks that neither\nGAIL nor RL can solve alone. Further, the ﬁnal agents achieve\nhigher returns than the human demonstrations owing to the\nexposure to task rewards.\n2) Leveraging Physical States in Simulation: The physics\nsimulator we employ for training exposes the full state of\nthe system. Even though such privileged information is un-\navailable on a real system, we can take advantage of it when\ntraining the policy in simulation. We propose four techniques\nfor leveraging the physical states in simulation to stabilize and\naccelerate learning (1) the use of a curriculum derived from\ndemonstration states, (2) the use of privileged information for\nthe value function (baseline), (3) the use of object-centric\nfeatures in the discriminator, and (4) auxiliary tasks. We\nelaborate these four techniques as follows:\n1. Demonstration as a curriculum. The problem of\nexploration in continuous domains is exacerbated by the\nlong duration of realistic tasks. Previous work indicates that\nshaping the distribution of start states towards states where\nthe optimal policy tends to visit can greatly improve policy\nlearning [18, 35]. We alter the start state distribution with\ndemonstration states. We build a curriculum that contains\nclusters of states in different stages of a task. For instance, we\ndeﬁne three clusters for the pouring task, including reaching\nthe mug, grasping the mug, and pouring. During training, with\nprobability ϵ, we then start an episode from a random initial\nstate, and with probability 1 −ϵ we uniformly select a cluster\nand initialize the episode with a demonstration state from that\ncluster. This is possible since our simulated system is fully\ncharacterized by the physical states.\n2. Learning value functions from states. PPO uses a learn-\nable value function Vφ to estimate the advantage required to\ncompute the policy gradient. During training, each PPO worker\nexecutes the policy for K steps and uses the discounted sum\nof rewards and the value as an advantage function estimator\nˆAt = PK\ni=1 γi−1rt+i + γK−1Vφ(st+K) −Vφ(st), where γ\nis the discount factor. As the policy gradient relies on the\nvalue function to reduce variance, it is beneﬁcial to accelerate\nlearning of the value function. Rather than using pixels as\ninputs similar to the policy network, we take advantage of the\nlow-level physical states (e.g., the position and velocity of the\n3D objects and the robot arm) to train the value Vφ with a\nsmaller multilayer perceptron. We ﬁnd that training the policy\nand value in two different modalities stabilizes training and\nreduces oscillation of the agent’s performance. This technique\nhas also been been proposed concurrently by Pinto et al. [34].\n3. Object-centric discriminator. As for the value func-\ntion, we exploit the availability of the physical states for\nthe GAIL discriminator and provide task speciﬁc features\nas input. We ﬁnd that object-centric representations (e.g.,\nabsolute and relative positions of the objects) provide the\nsalient and relevant signals to the discriminator. The states\nof the robot arm in contrast lead the discriminator to focus\non irrelevant aspects of the behavior of the controller and are\ndetrimental for training of the policy. Inspired by information\nhiding strategies used in locomotion domains [13, 28], our\ndiscriminator only takes the object-centric features as input\nwhile masking out arm-related information. The construction\nof the object-centric representation requires a certain amount\nof domain knowledge of the tasks. We ﬁnd that the relative\npositions of objects and displacements from the gripper to the\nobjects usually provide the most informative characterization\nof a task. Empirically, we ﬁnd that our model is not very\nsensitive to the particular choices of object-centric features,\nas long as they carry sufﬁcient task-speciﬁc information. We\nprovide detailed descriptions in Appendix C.\n4. State prediction auxiliary tasks. Auxiliary tasks have\nbeen shown to be effective in improving the learning efﬁciency\nand the ﬁnal performance of deep RL methods [16]. To fa-\ncilitate learning visuomotor policies we add a state prediction\nlayer on the top of the CNN module to predict the locations of\nobjects from the camera observation. We use a fully-connected\nlayer to regress the 3D coordinates of objects in the task,\nminimizing the ℓ2 loss between the predicted and ground-truth\nobject locations. The auxiliary tasks are not required for our\nmodel to learn good visuomotor policies; however, adding the\nadditional supervision can often accelerate the training of the\nCNN module.\n3) Sim2Real Policy Transfer: We perform policy transfer\nexperiments on a real-world Kinova Jaco robot arm. The simu-\nlation was manually adjusted to roughly match the appearance\nand dynamics of the laboratory setup: a Kinect camera was\nvisually calibrated to match the position and orientation of the\npouring\nliquid\nblock\nlifting\norder\nfulfillment\nclearing \ntable with\na box\nblock\nstacking\nclearing\ntable with\nblocks\nblock\nlifting\n(real)\nblock\nstacking\n(real)\nFig. 3: Visualizations of the six manipulation tasks in our experiments. The left column shows RGB images of all six tasks\nin the simulated environments. These images correspond to the actual pixel observations as input to the visuomotor policies.\nThe right column shows the two tasks with color blocks on the real robot.\nsimulated camera, and the simulation’s dynamics parameters\nwere manually adjusted to match the dynamics of the real\narm. Instead of using professional calibration equipment, our\napproach to sim2real policy transfer relies on domain ran-\ndomization of camera position and orientation [17, 47]. In\ncontrast to some previous works our trained policies do not\nrely on any object position information or intermediate goals\nbut rather learn a mapping end-to-end from raw pixel input\njoint velocities. In addition, to improve the robustness of our\ncontrollers to latency effects on the real robot, we also ﬁne-\ntune our policies while subjecting them to action dropping. A\ndetailed description is available in Appendix B.\nIV. EXPERIMENTS\nHere we demonstrate that our approach offers a ﬂexible\nframework to visuomotor policy learning. To this end we eval-\nuate its performance on the six manipulation tasks illustrated in\nFig. 3. We provide additional qualitative results in this video.\nA. Environment Setup\nWe use a Kinova Jaco arm that has 9 degrees of freedom: six\narm joints and three actuated ﬁngers. The robot arm interacts\nwith a diverse set of objects on a tabletop. The visuomotor\npolicy controls the robot by setting the joint velocity com-\nmands, producing 9-dimensional continuous velocities in the\nrange of [−1, 1] at 20Hz. The proprioceptive features consist\nof the positions and angular velocities of the arm joints and the\nﬁngers. Visual observations of the table-top scene are provided\nvia a suitably positioned real-time RGB camera. The propri-\noceptive features and the camera observations are available\nin both simulation and real environments thus enabling policy\ntransfer. The physical environment is simulated in the MuJoCo\nphysics simulator [48].\nWe use a large variety of objects, ranging from basic\ngeometric shapes to procedurally generated 3D objects built\nfrom ensembles of primitive shapes. We increase the diver-\nsity of objects by randomizing various physical properties,\nincluding dimension, color, mass, friction, etc. We collect\ndemonstrations using a SpaceNavigator 3D motion controller,\nwhich allows us to operate the robot arm with a position con-\ntroller, and gather 30 episodes of demonstration for each task\nincluding observations, actions, and physical states. As each\nepisode takes less than a minute to complete, demonstrating\neach task can be done within half an hour.\nB. Robot Arm Manipulation Tasks\nFig. 3 shows the six manipulation tasks in our experiments.\nThe ﬁrst column shows the six tasks in simulated environ-\nments, and the second column shows the real-world setup of\nthe block lifting and stacking tasks. We see obvious visual\ndiscrepancies of the same task in simulation and reality. These\nsix tasks exhibit learning challenges to varying degrees. The\nﬁrst three tasks use simple colored blocks, which makes it\neasy to replicate a similar setup on the real robot. We study\nsim2real policy transfer with the block lifting and stacking\ntasks in Sec. IV-D.\nBlock lifting. The goal is to grasp and lift a randomized\nblock, allowing us to evaluate the model’s robustness. We\nvary several random factors, including the robot arm dynamics\n(friction and armature), lighting conditions, camera poses,\nbackground colors, as well as the properties of the block. Each\nepisode starts with a new conﬁguration with these random\nfactors uniformly drawn from a preset range.\nBlock stacking. The goal is to stack one block on top of\nthe other block. Together with the block lifting task, this is\nevaluated in sim2real transfer experiments.\nClearing table with blocks. This task requires lifting two\nblocks off the tabletop. One solution is to stack the blocks and\nlift them both together. This task requires longer time and a\nmore dexterous controller, introducing a signiﬁcant challenge\nfor exploration.\nThe next three tasks involve a large variety of procedurally\ngenerated 3D shapes, making them difﬁcult to recreate in real\n(a) Block lifting\n(b) Block stacking\n(c) Clearing table with blocks\n(d) Clearing table with a box\n(e) Pouring liquid\n(f) Order fulﬁllment\nFig. 4: Learning efﬁciency of our reinforcement and imitation model against baselines. The plots are averaged over 5 runs\nwith different random seeds. All the policies use the same network architecture and the same hyperparameters (except λ).\nenvironments. We use them to examine the model’s ability to\ngeneralize across object variations in long and complex tasks.\nClearing table with a box. The goal is to clear the tabletop\nthat has a box and a toy car. One strategy is to grasp the toy,\nput it into the box, and lift the box. Both the box and the toy\ncar are randomly generated for each episode.\nPouring liquid. Modeling and reasoning about deformable\nobjects and ﬂuids is a long-standing challenge in the robotics\ncommunity [41]. We design a pouring task where we use\nmany small spheres to simulate liquid. The goal is to pour\nthe “liquid” from one mug to the other container. This task\nis particularly challenging due to the dexterity required. Even\nhumans struggled to demonstrate the task with our 3D motion\ncontroller after extensive practice.\nOrder fulﬁllment. In this task we randomly place a variable\nnumber of procedurally generated toy planes and cars on the\ntable. The goal is to place all the planes into the green box and\nall the cars into the red box. This task requires the policy to\ngeneralize at an abstract level. It needs to recognize the object\ncategories, perform successful grasps on diverse shapes, and\nhandle tasks with variable lengths.\nC. Quantitative Evaluation\nOur full model can solve all six tasks, with only occasional\nfailures, using the same policy network, the same training\nalgorithm, and a ﬁxed set of hyperparameters. On the contrary,\nneither reinforcement nor imitation alone can solve all tasks.\nWe compare the full model with three baselines which cor-\nrespond to pure RL, pure GAIL, and RL w/o demonstration\ncurriculum. These baselines use the same setup as the full\nmodel, except that we set λ = 0 for RL and λ = 1 for GAIL,\nwhile our model uses a balanced contribution of the hybrid\nreward, where λ = 0.5. In the third baseline, all training\nepisodes start from random initial states rather than resetting\nto demonstration states. This is a standard RL setup.\nWe report the mean episode returns as a function of the\nnumber of training iterations in Fig. 4. Our full model achieves\nthe highest returns in all six tasks. The only case where the\nbaseline model is on par with the full model is the block\nlifting task, in which both the RL baseline and the full model\nachieved similar levels of performance. We hypothesize that\nthis is due to the short length of the lifting task, where\nrandom exploration can provide a sufﬁcient learning signal\nwithout the aid of demonstrations. In the other ﬁve tasks, the\nfull model outperforms both the reinforcement learning and\nimitation learning baselines by a large margin, demonstrating\nthe effectiveness of combining reinforcement and imitation for\nlearning complex tasks. Comparing the two variants of RL\nwith and without using demonstration as a curriculum, we\nsee a pronounced effect of altering the start state distribution.\nWe see that RL from scratch leads to very slow learning\nprogress; while initiating episodes along demonstration tra-\njectories enables the agent to train on states from different\nstages of a task. As a result, it greatly reduces the burden\nof exploration and improves the learning efﬁciency. We also\nreport the mean episode returns of human demonstrations in\n0.0\n0.5\n1.0\n1.5\n2.0\niteration (in millions)\n100\n200\n300\n400\n500\n600\naverage episode return\nfull model\nno auxiliary task\nno RNN policy\nno action to discriminator\nno discriminator mask\nGAIL w/ demo curriculum\nRL w/ demo curriculum\nlearning value on pixels\n(a) Ablation study of model components\n(b) Model sensitivity to λ values\nFig. 5: Model analysis in the stacking task. On the left we investigate the impact on performance by removing each individual\ncomponent from the full model. On the right we investigate the model’s sensitivity to the hyperparameter λ that moderates the\ncontribution of reinforcement and imitation.\nthese ﬁgures. Demonstrations with the 3D motion controller\nare imperfect, especially for pouring (see video), and the\ntrained agents exceed the performance of the human operator.\nTwo ﬁndings are noteworthy. First, the RL agent learns\nfaster than the full model in the clearing blocks task, but the\nfull model eventually outperforms. This is because the full\nmodel discovers a novel strategy, different from the strategy\nemployed by human operators (see video). In this case, imi-\ntation gave contradictory signals but eventually, reinforcement\nlearning guided the policy towards a better strategy. Second,\npouring liquid is the only task where GAIL outperforms its\nRL counterpart. Imitation can effectively shape the agent’s\nbehaviors towards the demonstration trajectories [51]. This\nis a viable solution for the pouring task, where a controller\nthat generates similar-looking behaviors can complete the task.\nIn contact-rich domains with sufﬁcient variation, however, a\ncontroller trained only from a small number of demonstrations\nwill struggle to handle the complex dynamics and to generalize\nappropriately to novel instances of the task. We hypothesize\nthat this is why the baseline RL agent outperforms the GAIL\nagent in the other ﬁve tasks.\nWe further perform an ablation study on the block stacking\ntask to understand the impact of different components of our\nmodel. In Fig. 5a, we trained our agents with a number of\nconﬁgurations, each with a single modiﬁcation to the full\nmodel. We see that these ablations cluster into two groups:\nagents that learn to stack (with average returns greater than\n400) and agents that only learn to lift (with average returns\nbetween 200 and 300). These results indicate that the hybrid\nRL/IL reward, learning value function from states, and object-\ncentred features for the discriminator play an integral role in\nlearning good policies. Using only the RL or GAIL reward,\nlearning the value function from pixels, or providing the full\narm state as discriminator input (no discriminator mask) all\nresult in inferior performance. In contrast, the optional com-\nponents include the recurrent policy core (LSTM), the use of\nstate prediction auxiliary tasks, and whether to include actions\nin discriminator input. This result suggests that our model\ncan learn end-to-end visuomotor policies without a pretraining\nphase or the need of auxiliary tasks, as opposed to previous\nwork on visuomotor learning [3, 23, 52]. Furthermore, it can\nwork when the GAIL discriminator only has access to the\ndemonstration states without the accompanying demonstrator\nactions. Therefore, it can potentially use demonstrations col-\nlected with a different body where the underlying controls\nare unknown or different from the robot’s actuators. We then\nexamine the model’s sensitivity to the λ values in Eq. 2. We\nsee in Fig. 5b that, our model works well with a broad range\nof λ values from 0.3 to 0.7 that provide a balanced mix of the\nRL and GAIL rewards.\nD. Sim2Real Policy Transfer Results\nTo assess the robustness of the simulation-trained policy, we\nevaluate zero-shot transfer (no additional training) on a real\nJaco arm. The real-world setup was roughly matched to the\nsimulation environment (including camera positions and robot\nkinematics and approximate object size and color). We execute\nthe trained policy network on the robot and count the number\nof successful trials for both the lifting and stacking tasks. The\narm position is randomly initialized and the target block(s)\nare placed in a number of repeatable start conﬁgurations for\neach task. The zero-shot transfer of the lifting policy has a\nsuccess rate of 64% over 25 trials (split between 5 block\nconﬁgurations). The stacking policy has a success rate of\n35% over 20 trials (split between 2 block conﬁgurations).\n80% of the stacking trajectories, however, contain success-\nful lifting behavior, and 100% contains successful reaching\nbehavior. It is impractical to conduct a fair comparison with\nprevious work [17, 47, 50] that implemented different tasks\nand different conﬁgurations. The state-of-the-art sim2real work\nclosest to our setup is progressive network [39, 40], which has\ndemonstrated block reaching behaviors with a pixel-to-action\nRL policy on a Jaco arm. Their work did not demonstrate any\nlifting or stacking behavior, while our method has achieved\nreaching behaviors with a 100% success rate. Qualitatively,\nthe policies are notably robust even on failed attempts. The\nstacking policy repeatedly chases the block to get a successful\ngrasp before trying to stack (see video). For more detailed\ndescriptions of the sim2real results, refer to Appendix B.\nSeveral aspects of system mismatch have constrained the\npolicies from attaining a better performance on the real robot.\nAlthough the sim and real domains are similar, there is still a\nsizable reality gap that makes zero-shot transfer challenging.\nFor example, while the simulated blocks are rigid the objects\nemployed in the real-world setup are non-rigid foam blocks\nwhich deform and bounce unpredictably. Furthermore, neural\nnetwork policies are sensitive to subtle discrepancies between\nsimulated rendering and the real camera frame. Nonetheless,\nthe preliminary successes achieved by these policies offer a\ngood starting point for future work to leverage a small amount\nof real-world experience to enable better transfer.\nV. DISCUSSION\nIn this paper, we have described a general model-free\ndeep reinforcement learning method for end-to-end learning\nof policies that operate from RGB camera images and per-\nform manipulation using joint velocity control. Our method\ncombines the use of demonstrations via generative adversarial\nimitation learning [15] with model-free RL to achieve both\neffective learning of difﬁcult tasks and robust generalization.\nThe approach only requires a small number of demonstration\ntrajectories (30 per task in the experiments). Additionally, this\napproach works from state trajectories (without demonstrator\nactions) combined with the use of only partial/featurized\ndemonstrations being seen by the discriminator – this can\nsimplify and increase the ﬂexibility during data collection\nand facilitate generalization beyond conditions seen in the\ndemonstrations (e.g. demonstrations could potentially be col-\nlected with a different body, such as a human demonstrator\nvia motion capture). Demonstrations were collected via tele-\noperation of the simulated arm in less than thirty minutes per\ntask. Our method integrates several new techniques to leverage\nthe ﬂexibility and scalability afforded by simulation, such as\naccess to privileged information and the use of large-scale\nRL algorithms. The experimental results have demonstrated\nits effectiveness in complex manipulation tasks in simulation\nand achieved preliminary successes of zero-shot transfer to\nreal hardware. We trained all the policies with the same\npolicy network, the same training algorithm, and the same\nhyperparameters. The approach makes some use of task-\nspeciﬁc information especially in the choice of the object-\ncentric features for the discriminator and the RL reward. In\npractice we have found the speciﬁcation of these features\nintuitive, and our method was reasonably robust to speciﬁc\nchoices, thus striking favorable balance between the need for\n(limited) prior knowledge and the generality of the solutions\nthat can be learned for complex tasks.\nIn order to fulﬁll the potential of deep RL in robotics, it\nis essential to confront the full variability of the real-world,\nincluding diversity of object appearances, system dynamics,\ntask semantics, etc. We have therefore focused on learning\ncontrollers that could handle signiﬁcant task variations along\nmultiple dimensions. To improve a policy’s ability to gener-\nalize, we have increased the diversity of training conditions\nwith parameterized, procedurally generated 3D objects and\nrandomized system dynamics. This has resulted in policies that\nexhibit robustness to large variations in simulation as well as\nagainst some of the domain discrepancy between simulation\nand the real world.\nSimulation is at the center of our method. Training in simu-\nlation circumvents several practical challenges of deep RL for\nrobotics, such as access to state information for reward spec-\niﬁcation, high sample complexity, and safety considerations.\nTraining in simulation also allows us to use the simulation\nstate to facilitate and stabilize training (i.e. by providing state\ninformation to the value function), which in our experiments\nhas been important for learning good visuomotor policies.\nHowever, even though our method utilizes such privileged\ninformation during training it ultimately produces policies that\nonly rely on vision and proprioceptive information of the arm\nand that can thus be deployed on real hardware.\nExecuting the policies on the real robot reveals that there\nremains a sizable domain gap between simulation and real\nhardware. Transfer is affected by visual discrepancies as well\nas by differences in the arm dynamics and in the physical\nproperties of the environment. This leads to a certain level\nof performance degradation when running a simulation policy\non the real robot. Still, our real-world experiments have\nexempliﬁed that zero-shot sim2real transfer can achieve initial\nsuccess with RL trained policies performing pixel-to-joint-\nvelocity control.\nVI. CONCLUSION\nWe have shown that combining reinforcement and imitation\nlearning considerably improves our ability to train systems\ncapable of solving challenging dexterous manipulation tasks\nfrom pixels. Our method implements all three stages of a\npipeline for robot skill learning: ﬁrst, we collected a small\namount of demonstration data to simplify the exploration\nproblem; second, we relied on physical simulation to perform\nlarge-scale distributed robot training; and third, we performed\nsim2real transfer for real-world deployment. In future work,\nwe seek to improve the sample efﬁciency of the learning\nmethod and to leverage real-world experience to close the\nreality gap for policy transfer.\nACKNOWLEDGMENT\nThe authors would like to thank Yuval Tassa, Jonathan\nScholz, Thomas Roth¨orl, Jonathan Hunt, and many other col-\nleagues at DeepMind for the helpful discussion and feedback.\nREFERENCES\n[1] Abdeslam Boularias, Jens Kober, and Jan Peters. Relative\nentropy inverse reinforcement learning.\nIn AISTATS,\npages 182–189, 2011.\n[2] Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart,\nYunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura\nDowns, Julian Ibarz, Peter Pastor, Kurt Konolige, Sergey\nLevine, and Vincent Vanhoucke.\nUsing Simulation\nand Domain Adaptation to Improve Efﬁciency of Deep\nRobotic Grasping.\narXiv preprint arXiv:1709.07857,\n2017.\n[3] Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya,\nAdrian Li, Stefan Schaal, and Sergey Levine.\nPath\nintegral guided policy search. In ICRA, 2017.\n[4] Marc Peter Deisenroth, Gerhard Neumann, Jan Peters,\net al. A survey on policy search for robotics. Foundations\nand Trends in Robotics, 2(1-2):1–142, 2013.\n[5] Yan Duan, Marcin Andrychowicz, Bradly C. Stadie,\nJonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter\nAbbeel, and Wojciech Zaremba.\nOne-shot imitation\nlearning. arXiv preprint arXiv:1703.07326, 2017.\n[6] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided\ncost learning: Deep inverse optimal control via policy\noptimization. In ICML, pages 49–58, 2016.\n[7] Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel,\nand Sergey Levine. One-Shot Visual Imitation Learning\nvia Meta-Learning.\narXiv preprint arXiv:1709.04905,\n2017.\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. In NIPS,\npages 2672–2680, 2014.\n[9] Shixiang Gu, Ethan Holly, Timothy P. Lillicrap, and\nSergey Levine. Deep reinforcement learning for robotic\nmanipulation. arXiv preprint arXiv:1610.00633, 2016.\n[10] Shixiang Gu, Tim Lillicrap, Ilya Sutskever, and Sergey\nLevine. Continuous deep Q-learning with model-based\nacceleration. In ICML, 2016.\n[11] Abhishek Gupta, Clemens Eppner, Sergey Levine, and\nPieter Abbeel. Learning dexterous manipulation for a soft\nrobotic hand from human demonstration. arXiv preprint\narXiv:1603.06348, 2016.\n[12] Nicolas Heess, Gregory Wayne, David Silver, Tim Lill-\nicrap, Tom Erez, and Yuval Tassa. Learning continuous\ncontrol policies by stochastic value gradients. In NIPS,\npages 2926–2934, 2015.\n[13] Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lil-\nlicrap, Martin Riedmiller, and David Silver.\nLearning\nand transfer of modulated locomotor controllers. arXiv\npreprint arXiv:1610.05182, 2016.\n[14] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh\nMerel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang,\nAli Eslami, Martin Riedmiller, et al. Emergence of loco-\nmotion behaviours in rich environments. arXiv preprint\narXiv:1707.02286, 2017.\n[15] Jonathan Ho and Stefano Ermon. Generative adversarial\nimitation learning. In NIPS, pages 4565–4573, 2016.\n[16] Max Jaderberg, Volodymyr Mnih, Wojciech Marian\nCzarnecki, Tom Schaul, Joel Z Leibo, David Sil-\nver, and Koray Kavukcuoglu.\nReinforcement learn-\ning with unsupervised auxiliary tasks.\narXiv preprint\narXiv:1611.05397, 2016.\n[17] Stephen James, Andrew J. Davison, and Edward Johns.\nTransferring end-to-end visuomotor control from simu-\nlation to real world for a multi-stage task. arXiv preprint\narXiv:1707.02267, 2017.\n[18] Sham Kakade and John Langford. Approximately opti-\nmal approximate reinforcement learning. In ICML, 2002.\n[19] Diederik Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n[20] Jens Kober and Jan Peters. Reinforcement learning in\nrobotics: A survey.\nIn Reinforcement Learning, pages\n579–610. Springer, 2012.\n[21] Vikash Kumar, Abhishek Gupta, Emanuel Todorov, and\nSergey Levine.\nLearning dexterous manipulation poli-\ncies from experience and imitation.\narXiv preprint\narXiv:1611.05095, 2016.\n[22] Sergey Levine and Vladlen Koltun. Guided policy search.\nIn ICML, pages 1–9, 2013.\n[23] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter\nAbbeel. End-to-end training of deep visuomotor policies.\narXiv preprint arXiv:1504.00702, 2015.\n[24] Sergey Levine, Peter Pastor, Alex Krizhevsky, and\nDeirdre Quillen.\nLearning hand-eye coordination for\nrobotic grasping with deep learning and large-scale data\ncollection. arXiv preprint arXiv:1603.02199, 2016.\n[25] Yunzhu Li, Jiaming Song, and Stefano Ermon. Inferring\nthe latent structure of human decision-making from raw\nvisual inputs. arXiv preprint arXiv:1703.08840, 2017.\n[26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,\nNicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. Continuous control with deep reinforce-\nment learning. ICLR, 2016.\n[27] Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey\nLevine. Imitation from observation: Learning to imitate\nbehaviors from raw video via context translation. arXiv\npreprint arXiv:1707.03374, 2017.\n[28] Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srini-\nvasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and\nNicolas Heess.\nLearning human behaviors from mo-\ntion capture by adversarial imitation.\narXiv preprint\narXiv:1707.02201, 2017.\n[29] Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAndrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. Human-level control through deep rein-\nforcement learning. Nature, 518(7540):529–533, 2015.\n[30] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wo-\njciech Zaremba, and Pieter Abbeel.\nOvercoming ex-\nploration in reinforcement learning with demonstrations.\narXiv preprint arXiv:1709.10089, 2017.\n[31] Andrew Y Ng, Daishi Harada, and Stuart J Russell.\nPolicy invariance under reward transformations: Theory\nand application to reward shaping. In ICML, pages 278–\n287, 1999.\n[32] Xue\nBin\nPeng,\nMarcin\nAndrychowicz,\nWojciech\nZaremba, and Pieter Abbeel.\nSim-to-Real Transfer of\nRobotic Control with Dynamics Randomization. arXiv\npreprint arXiv:1710.06537, October 2017.\n[33] Lerrel Pinto and Abhinav Gupta.\nSupersizing self-\nsupervision: Learning to grasp from 50k tries and 700\nrobot hours. arXiv preprint arXiv:1509.06825, 2015.\n[34] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wo-\njciech Zaremba, and Pieter Abbeel. Asymmetric Actor\nCritic for Image-Based Robot Learning. ArXiv e-prints,\n2017.\n[35] Ivaylo Popov, Nicolas Heess, Timothy P. Lillicrap,\nRoland Hafner, Gabriel Barth-Maron, Matej Vecerik,\nThomas Lampe, Yuval Tassa, Tom Erez, and Mar-\ntin A. Riedmiller.\nData-efﬁcient deep reinforcement\nlearning for dexterous manipulation.\narXiv preprint\narXiv:1704.03073, 2017.\n[36] Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladis-\nlau B¨ol¨oni, and Sergey Levine.\nVision-based multi-\ntask manipulation for inexpensive robots using end-\nto-end learning from demonstration.\narXiv preprint\narXiv:1707.02920, 2017.\n[37] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta,\nJohn Schulman, Emanuel Todorov, and Sergey Levine.\nLearning complex dexterous manipulation with deep re-\ninforcement learning and demonstrations. arXiv preprint\narXiv:1709.10087, 2017.\n[38] St´ephane Ross, Geoffrey J Gordon, and Drew Bagnell. A\nreduction of imitation learning and structured prediction\nto no-regret online learning. In AISTATS, pages 627–635,\n2011.\n[39] Andrei Rusu, Neil Rabinowitz, Guillaume Desjardins,\nHubert Soyer, James Kirkpatrick, Koray Kavukcuoglu,\nRazvan Pascanu, and Raia Hadsell. Progressive neural\nnetworks. arXiv preprint arXiv:1606.04671, 2016.\n[40] Andrei Rusu, Matej Vecerik, Thomas Roth¨orl, Nicolas\nHeess, Razvan Pascanu, and Raia Hadsell. Sim-to-real\nrobot learning from pixels with progressive nets. arXiv\npreprint arXiv:1610.04286, 2016.\n[41] Connor Schenck and Dieter Fox.\nReasoning about\nliquids via closed-loop simulation.\narXiv preprint\narXiv:1703.01656, 2017.\n[42] John Schulman, Sergey Levine, Pieter Abbeel, Michael\nJordan, and Philipp Moritz. Trust region policy optimiza-\ntion. In ICML, pages 1889–1897, 2015.\n[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[44] Pierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey\nLevine.\nTime-contrastive networks: Self-supervised\nlearning from multi-view observation.\narXiv preprint\narXiv:1704.06888, 2017.\n[45] David Silver, Guy Lever, Nicolas Heess, Thomas Degris,\nDaan Wierstra, and Martin Riedmiller.\nDeterministic\npolicy gradient algorithms. In ICML, 2014.\n[46] David Silver, Aja Huang, Chris J Maddison, Arthur\nGuez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershel-\nvam, Marc Lanctot, et al.\nMastering the game of go\nwith deep neural networks and tree search. Nature, 529\n(7587):484–489, 2016.\n[47] Joshua Tobin, Rachel Fong, Alex Ray, Jonas Schneider,\nWojciech Zaremba, and Pieter Abbeel. Domain random-\nization for transferring deep neural networks from simu-\nlation to the real world. arXiv preprint arXiv:1703.06907,\n2017.\n[48] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco:\nA physics engine for model-based control.\nIn IROS,\npages 5026–5033, 2012.\n[49] Matej Vecerik, Todd Hester, Jonathan Scholz, Fumin\nWang, Olivier Pietquin, Bilal Piot, Nicolas Heess,\nThomas Roth¨orl, Thomas Lampe, and Martin A. Ried-\nmiller.\nLeveraging demonstrations for deep reinforce-\nment learning on robotics problems with sparse rewards.\narXiv preprint arXiv:1707.08817, 2017.\n[50] Ulrich Viereck, Andreas ten Pas, Kate Saenko, and\nRobert Platt.\nLearning a visuomotor controller for\nreal world robotic grasping using easily simulated depth\nimages. arXiv preprint arXiv:1706.04652, 2017.\n[51] Ziyu Wang, Josh Merel, Scott E. Reed, Greg Wayne,\nNando de Freitas, and Nicolas Heess. Robust imitation\nof diverse behaviors. NIPS, 2017.\n[52] Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen\nChebotar, and Sergey Levine.\nCollective robot rein-\nforcement learning with distributed asynchronous guided\npolicy search. arXiv preprint arXiv:1610.00673, 2016.\nAPPENDIX A\nEXPERIMENT DETAILS\nThe policy network takes the pixel observation and the\nproprioceptive feature as input. The pixel observation is an\nRGB image of size 64 × 64 × 3. We used the Kinect for\nXbox One camera1 in the real environment. The proprioceptive\nfeature describes the joint positions and velocities of the\nKinova Jaco arm.2 Each joint position is represented as the\nsin and cos of the angle of the joint in joint coordinates. Each\njoint velocity is represented as the scalar angular velocity.\nThis results in a 24-dimensional proprioceptive feature that\ncontains the positions (12-d) and velocities (6-d) of the six\narm joints and the positions (6-d) of the three ﬁngers. We\nexclude the ﬁnger velocities due to the noisy sensory readings\non the real robot. When collecting demonstrations, we use a 6-\nDoF SpaceNavigator motion controller3 to command the end\neffector to complete the tasks.\nWe used Adam [19] to train the neural network parameters.\nWe set the learning rate of policy and value to 10−4 and\n10−3 respectively, and 10−4 for both the discriminator and the\nauxiliary tasks. The pixel observation is encoded by a two-\nlayer convolutional network. We use 2 convolutional layers\nfollowed by a fully-connected layer with 128 hidden units.\nThe ﬁrst convolutional layer has 16 8 × 8 ﬁlters with stride 4\nand the second 32 4×4 ﬁlters with stride 2. We add a recurrent\nlayer of 100 LSTM units before the policy and value outputs.\nThe policy output is the mean and the standard deviation\nof a conditional Gaussian distribution over the 9-dimensional\njoint velocities. The initial policy standard deviation is set to\nexp(−3) for the clearing table with blocks task and exp(−1)\nfor the other ﬁve tasks. The auxiliary head of the policy\ncontains a separate three-layer MLP sitting on top of the\nconvolutional network. The ﬁrst two layers of the MLP has\n200 and 100 hidden units respectively, while the third layer\npredicts the auxiliary outputs. Finally, the discriminator is a\nsimple three-layer MLP of 100 and 64 hidden units for the ﬁrst\ntwo layers with the third layer producing log probabilities. The\nnetworks use tanh nonlinearities.\nWe trained the visuomotor policies using the distributed\nPPO algorithm [14] with synchronous gradient updates from\n256 CPU workers. Each worker runs the policy to complete\nan entire episode before the parameter updates are computed.\nWe set a constant episode length for each task based on its\ndifﬁculty, with the longest being 1000 time steps (50 seconds)\nfor the clearing table with blocks and order fulﬁllment tasks.\nWe set K = 50 as the number of time steps for computing\nK-step returns and truncated backpropagation through time to\ntrain the LSTM units. After a worker collects a batch of data\npoints, it performs 50 parameter updates for the policy and\nvalue networks, 5 for the discriminator and 5 for the auxiliary\nprediction network.\n1https://www.xbox.com/en-US/xbox-one/accessories/kinect\n2http://www.kinovarobotics.com\n3https://www.3dconnexion.com/spacemouse compact\nTABLE I: Block lifting success rate from different positions\n(LL, LR, UL, UR, and C represent the positions of lower left,\nlower right, upper left, upper right, and center respectively).\nLL\nLR\nUL\nUR\nC\nAll\nNo Action Dropping\n2/5\n2/5\n1/5\n3/5\n4/5\n12/25\nAction Dropping\n4/5\n4/5\n4/5\n0/5\n4/5\n16/25\nTABLE II: Success rate of the block stacking agent (with\naction dropping) from different starting positions (Left and\nRight indicate the positions of the support block upon initial-\nization).\nLeft\nRight\nAll\nStacking Success Rate\n5/10\n2/10\n7/20\nLifting Success Rate\n9/10\n7/10\n16/20\nAPPENDIX B\nSIM2REAL DETAILS\nTo better facilitate sim2real transfer, we lower the frequency\nat which we sample the observations. Pixel observations are\nonly observed at the rate of 5Hz despite the fact that our\ncontroller runs at 20Hz. Similarly, the proprioceptive features\nare observed at a rate of 10Hz. In addition to observation\ndelays, we also apply domain variations. Gaussian noise (of\nstandard deviation 0.01) are added proprioceptive features.\nUniform integers noise in the range of [−5, 5] are added to\neach pixel independently. Pixels of values outside the range\nof [0, 255] are clipped. We also vary randomly the shade of\ngrey on the Jaco arm, the color of the table top, as well as the\nlocation and orientation of the light source (see Fig. 6).\nIn the case of block lifting, we vary in addition the dynamics\nof the arm. Speciﬁcally, we dynamically change the friction,\ndamping, armature, and gain parameters of the robot arm in\nsimulation to further enhance the agent’s robustness.\nA. Action Dropping\nOur analysis indicates that, on the real robot, there is often\na delay in the execution of actions. The amount of delay\nalso varies signiﬁcantly. This has an adverse effect on the\nperformance of our agent on the physical robot since our\nagents’ performance depends on the timely execution of their\nactions. To better facilitate the transfer to the real robot, we\nﬁne-tune our trained agent in simulation while subjecting\nthem to a random chance of dropping actions. Speciﬁcally,\neach action emitted by the agent has a 50% chance of being\nexecuted immediately in which case the action is ﬂagged as\nthe last executed action. If the current action is not executed,\nthe last executed action will then be executed. Using the above\nprocedure, we ﬁne-tune our agents on both block lifting and\nblock stacking for a further 2 million iterations.\nTo demonstrate the effectiveness of action dropping, we\ncompare our agent on the real robot over the task of block\nlifting. Without action dropping, the baseline agent lifts 48%\npercent of the time. After ﬁne-tuning using action dropping,\nour agent succeeded 64% percent of the time. For the complete\nset of results, please see Table I and Table II.\nFig. 6: Tiles show the representative range of diversity seen in the domain-randomized variations of the colors, lighting,\nbackground, etc.\nAPPENDIX C\nTASK DETAILS\nWe use a ﬁxed episode length for each task, which is deter-\nmined by the amount of time a skilled human demonstrator can\ncomplete the task. An episode terminates when a maximum\nnumber of agent steps are performed. The robot arm operates\nat a control frequency of 20Hz, which means each time step\ntakes 0.05 second.\nWe segment into a sequence of stages that represent an\nagent’s progress in a task. For instance, the block stacking\ntask can be characterized by three stages, including reaching\nthe block, lifting the block and stacking the block. We deﬁne\nfunctions on the underlying physical state to determine the\nstage of a state. This way, we can cluster demonstration states\naccording to their corresponding stages. These clusters are\nused to reset training episodes in our demonstration as a\ncurriculum technique proposed in Sec. III-B2. The deﬁnition\nof stages also gives rise to a convenient way of specifying the\nreward functions without hand-engineering a shaping reward.\nWe deﬁne a piecewise constant reward function for each task,\nwhere we assign the same constant reward to all the states\nthat belong to the same stage. We detail the stages, reward\nfunctions, auxiliary tasks, and object-centric features for the\nsix tasks in our experiments.\nBlock lifting. Each episode lasts 100 time steps. We deﬁne\nthree stages and their rewards (in parentheses) to be initial\n(0), reaching the block (0.125) and lifting the block (1.0).\nThe auxiliary task is to predict the 3D coordinates of the\ncolor block. The object-centric feature consists of the relative\nposition between the gripper and the block.\nBlock stacking. Each episode lasts 500 time steps. We\ndeﬁne four stages and their rewards to be initial (0), reaching\nthe orange block (0.125), lifting the orange block (0.25),\nand stacking the orange block onto the pink block (1.0).\nThe auxiliary task is to predict the 3D coordinates of the\ntwo blocks. The object-centric feature consists of the relative\npositions between the gripper and the two blocks respectively.\nClearing table with blocks. Each episode lasts 1000 time\nsteps. We deﬁne ﬁve stages and their rewards to be initial (0),\nreaching the orange block (0.125), lifting the orange block\n(0.25), stacking the orange block onto the pink block (1.0),\nand lifting both blocks off the ground (2.0). The auxiliary\ntask is to predict the 3D coordinates of the two blocks. The\nobject-centric feature consists of the 3D positions of the two\nblocks as well as the relative positions between the gripper\nand the two blocks respectively.\nClearing table with a box. Each episode lasts 500 time\nsteps. We deﬁne ﬁve stages and their rewards to be initial (0),\nreaching the toy (0.125), grasping the toy (0.25), putting the\ntoy into the box (1.0), and lifting the box (2.0). The auxiliary\ntask is to predict the 3D coordinates of the toy and the box.\nThe object-centric feature consists of the 3D positions of the\ntoy and the box as well as the relative positions between the\ngripper and these two objects respectively.\nPouring liquid. Each episode lasts 500 time steps. We\ndeﬁne three stages and their rewards to be initial (0), grasping\nthe mug (0.05), pouring (0.1N), where N is the number of\nsmall spheres in the other container. The auxiliary task is\nto predict the 3D coordinates of the mug. The object-centric\nfeature consists of the 3D positions of the mug, the relative\nposition between the gripper and the mug, and the relative\nposition between the mug and the container.\nOrder fulﬁllment. Each episode lasts 1000 time steps. The\nnumber of objects varies from 1 to 4 across episodes. We\ndeﬁne ﬁve stages that correspond to the number of toys in the\nboxes. The immediate reward corresponds to the number of\ntoys placed in the correct boxes (number of toy planes in the\ngreen box and toy cars in the red box). To handle the variable\nnumber of objects, we only represent the objects nearest\nto the gripper for the auxiliary task and the object-centric\nfeature. The auxiliary task is to predict the 3D coordinates\nof the nearest plane and the nearest car to the gripper. The\nobject-centric feature consists of the relative positions from\nthe gripper to these two nearest objects.\n",
  "categories": [
    "cs.RO",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2018-02-26",
  "updated": "2018-05-27"
}