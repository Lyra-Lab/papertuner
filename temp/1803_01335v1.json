{
  "id": "http://arxiv.org/abs/1803.01335v1",
  "title": "CAESAR: Context Awareness Enabled Summary-Attentive Reader",
  "authors": [
    "Long-Huei Chen",
    "Kshitiz Tripathi"
  ],
  "abstract": "Comprehending meaning from natural language is a primary objective of Natural\nLanguage Processing (NLP), and text comprehension is the cornerstone for\nachieving this objective upon which all other problems like chat bots, language\ntranslation and others can be achieved. We report a Summary-Attentive Reader we\ndesigned to better emulate the human reading process, along with a\ndictiontary-based solution regarding out-of-vocabulary (OOV) words in the data,\nto generate answer based on machine comprehension of reading passages and\nquestion from the SQuAD benchmark. Our implementation of these features with\ntwo popular models (Match LSTM and Dynamic Coattention) was able to reach close\nto matching the results obtained from humans.",
  "text": "CAESAR: Context Awareness Enabled\nSummary-Attentive Reader\nKshitiz Tripathi ∗\nYahoo Inc.\nkshitiz@stanford.edu\nLong-Huei Chen ∗†\nGraduate School of Interdisciplinary Information Studies\nThe University of Tokyo\nlonghuei@g.ecc.u-tokyo.ac.jp\nAbstract\nComprehending meaning from natural language is a primary objective of Natu-\nral Language Processing (NLP), and text comprehension is the cornerstone for\nachieving this objective upon which all other problems like chat bots, language\ntranslation and others can be achieved. We report a Summary-Attentive Reader we\ndesigned to better emulate the human reading process, along with a dictiontary-\nbased solution regarding out-of-vocabulary (OOV) words in the data, to generate\nanswer based on machine comprehension of reading passages and question from\nthe SQuAD benchmark. Our implementation of these features with two popu-\nlar models (Match LSTM and Dynamic Coattention) was able to reach close to\nmatching the results obtained from humans.\n1\nIntroduction\nEndowing machines with the ability to understand and comprehend meaning is one of the ultimate\ngoals of language processing, one that holds promise to revolutionize the way people interact with\nand retrieve information from machines [1]. The goal was outlined by Richardson et al. with the\nMCTest dataset they proposed [2], in which questions are provided for which the answer can only\nbe found in the associated passage text. To perform well on such task, machine comprehension\nmodels are expected to possess some sort of semantic interpretation of text along with probabilistic\ninference to determine the most probable answer.\nHere we propose a novel approach to better question-answering by preprocessing document text be-\nfore answer selection from the passage. This is achieved by a summarization engine that is applied\nto truncate document length to enable narrower focus on the correct answer span. The summariza-\ntion takes into account the nature of the question, and therefore can help eliminate unrelated answer\ncandidates in the passage that are likely to be erroneously selected. The summarization engine also\nassists in machine efﬁciency, since the encoder is more likely to focus directly around the answer\nspan. The summarized document is then passed through a coattention-based encoder, and the en-\ncoded message is passed along to an answer pointer decoder to predict the answer.\n2\nRelated Work\n2.1\nMachine Comprehension and Question-Answering\nWe begin with an overview of the features adopted by currently high-performing models evaluated\non the SQuAD question-answering dataset. Instead of summarizing and repeating similar function-\n∗The authors contributed equally.\n†Work done while author was a student at Computer Science Department, Stanford University, CA, USA.\n1\narXiv:1803.01335v1  [cs.CL]  4 Mar 2018\nality shared among the implementations, we list characteristics that are unique to the models, from\nwhich we drew inspiration to develop our CAESAR question-answering engine.\n• Multi-Perspective Context Matching : During preprocessing, character-level embedding\nare created from a LSTM RNN to assist with words with which pre-existing word em-\nbeddings are not available [3]. Then the document is ﬁltered based on word-level cosine\nsimilarity between question and document. The key to the model is the MPCM layer,\nwhich compares the document with question in multiple perspectives that are applicable to\ndifferent kinds of answer spans as seen in passage.\n• Match-LSTM: In this model, bi-directional match-LSTM RNN created from concatena-\ntion of document and attention-weighted question is applied to predict the answer span [4].\nThe answer-pointer layer predict answer from the 2 directions of the LSTM bi-directional\nRNN hidden state. Sentinel vector is appended at the end of document to facilitate deter-\nmination of answer span ends.\n• Dyanmic Coattention Network : In the dynamic coattention network, the coattention\nmechanism uniquely attends to both the question and the document simultaneously [5]. In\naddition to the conventional attention of question in light of document words and the at-\ntention of the document in light of the question, the authors also calculated the summaries\nof attention in light of each word of the document. They also applied an iterative dynamic\npointer decoder to predict on the answer span. Because the model alternates between pre-\ndicting the start and the end point, recovery from local maxima is more likely.\n• Bi-directional Attention Flow : The model also employed character-level embeddings\nwith Convolutional Neural Networks [6], which are then combined with word embeddings\nvia a Highway Network. Interestingly, in this model the attention ﬂow is not summarized\ninto single feature vectors, but is allowed to ﬂow along with contextual embeddings through\nto subsequent modeling.\n• RaSoR : To address lexical overlap between question and document, for each word in doc-\nument they created both passage-aligned and passage-independent question representation\n[7]. For passage-aligned representation, ﬁxed-length representation of the question is cre-\nated based on soft-alignments with the single passage word computed via neural attention.\nFor passage-independent representation, the question word is compared to the universally\nlearned embeddings instead of any particular word.\n• ReasoNet : The ReasoNet speciﬁcally mimics the inference process of a human reader by\nreading a document repeatedly, with attention on different parts each time until a satisﬁed\nanswer is found [8]. The authors present an elegant approach for ReasoNet to dynamically\ndetermine whether to continue the comprehension process after digesting intermediate re-\nsults, or to terminate reading when it concludes that existing information is adequate to\nproduce an answer. Since termination state is a discrete variable and non-trainable by\ncanonical back-propogation, they use Reinforcement Learning to solve that.\n• Dynamic Chunk Reader : After obtaining attention vectors between each word in the\nquestion and the document, the dynamic chuck reader creates chunk representation that\nencodes the contextual information of candidate chunks selected from the document [9].\nThen cosine similarities is evaluated between the chuck repreeation and the question, and\nthe highest-scoring chuck is taken as the answer.\n• Fine-grained Gating: While other methods typically use word-level representation, or\ncharacter-level representation, or both [10], in ﬁne-grained gating a gate is applied to dy-\nnamically choose between the word or character-level representations for each document\nword. Word features such as named entity tags, part-of-speech tags, binned document fre-\nquency vectors, and the word-level representations all form the feature vector of the gate.\n3\nData Source\nBuilding on the basis of similar evaluation metrics of machine comprehension, Rajpurkar et al.\nreleased the Stanford Question Answering dataset (SQuAD) [11], which has quickly become the\nde facto standard of comparison among machine comprehension models [12]. The reason of the\ndataset’s wide applicability is several-fold: ﬁrst, the dataset is at least 2-orders of magnitude larger\n2\nDoc-\nu-\nment\nImperialism is a type of advocacy of empire. Its name originated from the Latin word\n”imperium”, which means to rule over large territories. Imperialism is ”a policy of\nextending a country’s power and inﬂuence through colonization, use of military force,\nor other means”. Imperialism has greatly shaped the contemporary world. It has also\nallowed for the rapid spread of technologies and ideas. The term imperialism has been\napplied to Western (and Japanese) political and dominance especially in Asia and Africa\nin the 19th and 20th centuries. Its precise meaning continues to be debated by scholars.\nSome writers, such as Edward Said, use the term more broadly to describe any system\nof domination and subordination organised with an imperial center and a periphery.\nQues-\ntion\nThe term imperialism has been applied to western countries, and which eastern county?\nAn-\nswer\nJapan\nTable 1: Example from the SQuAD dataset. Provided with a paragraph of text, we built an automated\nsystem to generate response based on a query by taking a sentence/phrase from the paragraph.\nfrom other similar releases. Secondly, the questions are human-curated and are realistic approxi-\nmation of real-life needs. Thirdly, the corresponding answers to each question are of a variety of\nnature and can test the generalizability of machine comprehension. Finally, the answer can be an\narbitrary span instead of one of the pre-determined choices. SQuAD is comprised of around 100K\nquestion-answer pairs, along with a context paragraph. The context paragraphs were extracted from\na set of articles from Wikipedia. Humans generated questions using that paragraph as a context, and\nselected a span from the same paragraph as the target answer.\n4\nMatch-LSTM with Answer Pointer Decoder Baseline\nIn Match LSTM model, the authors present a layered network architecture consisting of three layers.\nWe begin with an LSTM layer that preprocesses the passage and the question using LSTMs. A\npreprocessing layer uses standard one dimension LSTM to process the embedded passage and the\nquestion and collect the entire sequence of hidden states to generate Document and Question repre-\nsentations, Hp and Hq, respectively.\nHp =\n⃗\nLSTM(P), Hq =\n⃗\nLSTM(Q)\nThe LSTM preprocessing helps incorporate context from the respective passages, and enable the\nencoded hidden state to include information contained words that are adjacent to the word in ques-\ntion. Also note that we shared a single LSTM unit for both the question and the document passage, a\ndecision made for the purpose of shared parameters training, and is unlike the original Match-LSTM\npaper.\nAfterwhich, we add a match-LSTM layer that tries to match the passage against the question. This\nis the core of the network where attention vector αi is computed by globally attending to the hidden\nrepresentation of each word of the passage for the entire question.\n⃗Gi = tanh(W qHq + (W php\ni + bp) ⊗eQ)\n⃗αi = softmax(wT ⃗Gi + b ⊗eQ)\nwhere W p, W q, W r ∈RlXl bp, w ∈Rl and b ∈R. hi is the hidden state for the one-directional\nmatch-LSTM which receives the the concatenation of passage representation and attention weighted\nquestion representation. A similar LSTM is used for representation in reverse direction. Omitting\nthe detail as they exactly match the Match-LSTM paper. Finally, the hidden states from both the\nLSTM are concatenated which is passed to the next layer for inference.\nAn Answer Pointer (Ans-Ptr) is located at the top layer is motivated by the Pointer Net introduced\nby Vinyals et al. [13]. The authors mentions two variants for this layer: predicting the entire answer\nSequence vs predicting only the answer span. The initial testing showed us that boundary model\nwas better than the sequence model, which was also inline with the results mentioned in the paper,\nso we continued with the boundary model which was also simpler to implement.\n3\nFigure 1: Overall Network Architecture\n5\nDynamic Coattention\nAfter setting up a baseline with the Match LSTM model, we started to implement Dynamic Coat-\ntention Model [5] which had an advanced attention mechanism and a novel iterative approach for\nﬁnding the answer span. Similar to Match-LSTM, this model also consisted of layers describe be-\nlow.\n5.1\nDocument and Question Encode Layer\nThis layer is similar to the preprocessing layer of Match-LSTM model but with two modiﬁcations:\nthe question and context tokens are fed to the same LSTM unit to obtain their encoding. And, there\nis an additional linear layer on top op the question encoding which results in general scoring function\n[14] instead of simple dot product between question and document encoding for attention weights\ncalculation. After this is completed, we obtain two matrices D ∈Rm×l and Q ∈Rn×l. Each of\nthe same dimension as the previous layer but now with contextual information. Also a nonlinear\nlayer is placed upon the question encoding to produce Q′ from Q, to introduce variation between\nthe question and document encoding space [4].\n5.1.1\nUnknown Word Lookup\nA major bottleneck of the model in case of test/dev prediction was the presence of many new words\nin the test/dev dataset which were missing in the vocabulary constituted purely of training dataset.\nSince our network performed better with constant embeddings, we had the ﬂexibility of enhancing\nthe vocabulary before test/dev prediction. And upon including the Unknown Word Look-up module,\nwe got a lift of 6-9% in each F1 and EM score which was very crucial for the success of our model.\nWe used an efﬁcient hash join technique for the look-up and on test servers, it took just 60-100sec for\nlooking up the missing word in the 2.19M Glove 840B 100D dataset which gave us more iterations\nto test.\n4\n5.2\nCoattention Encoder Layer\nThe coattention mechanism attends to the question and document simultaneously, and ﬁnally fuses\nboth attention contexts [5], which proceeds as follows:\nFirst, the afﬁnity matrix calculates scores corresponding to all pairs of document words and question\nwords\nL = Q′D′T ∈Rn×o\nthen attention weights are produced by normalizing the afﬁnity matrix. Then the afﬁnity matrix is\nnormalized column-wise to produce the attention weights across the document for each word in the\nquestion\nAQ = softmax (L) ∈Rn×o\nand normalized row-wise to produce the attention weights across the question for each word in the\ndocument,\nAD = softmax\n\u0000LT \u0001\n∈Ro×n\nWe then obtain summaries, or attention contexts, of the document in light of each word of the\nquestion [6].\nCQ = AQD′ ∈Rn×l\nADQ is summaries of the question in light of each word of the documents, and ADCQ is the\nsummaries of the previous attention contexts in light of each word of the document. These two\noperations are done in parallel, the latter can be interpreted as the mapping of question encoding\ninto space of document encoding.\nCD = AD[Q′, CQ] ∈Ro×2l\nWe deﬁne CDas a co-dependent representation of the question and document, as the coattention con-\ntext. The last step is the fusion of temporal information to the coattention context via a bidirectional-\nLSTM.\nU = Bi−LSTM\n\u0000[D′, CD]\n\u0001\n∈Ro×2l\n5.3\nSummary-Attentive Reader\nFigure 2: The summarization engine ﬁrst selects the key sentence in the document, which has the\nhighest similarity to the question. Then the document is truncated around the key sentence to obtain\nthe summary.\nTo facilitate better answer prediction, we created a summarization engine that can preprocess the\ndocument to generate a shortened summary that is more likely to contain the answer. This is moti-\nvated by the human readers’ approach to question answering: when we are looking for an answer to\n5\nsome question from a paragraph, we would rarely read the article word-by-word in its entirety before\nselecting an answer. Instead we would often rapidly scan the document passage, identify sentences\ncontaining similar wording to the question, then select the answer based on our understanding of the\ntext.\nIn an effort to emulate the aforementioned human question-answering process, we designed the\nsummarization engine to truncate the document into a shorter paragraph before further processing.\nThis is carried out through the following steps (Fig. 2):\n1. A sentence-tokenizer is applied to the document to identify span indices of the sentences.\n2. Max-pooling or mean-pooling is applied across all word embeddings of each sentence in\nthe document, in order to obtain sentence-level representations of all sentences.\n3. Similar pooling techniques is applied to the question.\n4. The sentence-level representation of each sentence in document is compared with the ques-\ntion, in order to identify the sentence that has the highest cosine similarity as the key sen-\ntence.\n5. The document is truncated around the key sentence to the pre-speciﬁed summary length.\nAfter being shortened, the summarized document is allowed to ﬂow through to the subsequent layers.\nOur summarized method ensures that similar attention mechanisms and prediction methods can be\napplied regardless of whether the document is full-length or has being shortened, since the word\nordering and sentence meaning are preserved in this summarization engine.\n5.4\nDecoder Layer\nThe Pointer Network represents an effort in selecting speciﬁc locations within the source text as an\nanswer produced in the output. The idea is actually quite elegant: rather than producing an attention\nvector based on weights calculated from a nonlinear transformation of the decoder hidden states\nalong with all the encoder hidden states, we simply use the weight as a probability that the particular\nlocation in the encoder would be chosen. Even though the idea is, at ﬁrst glance, simple, it has been\nsuccessfully applied to a variety of tasks such as ﬁnding convex hulls, Delaunay triangulations, and\nthe Travelling Salesman Problem [13].\nIn our speciﬁc use case, the attention mechanism is used again to obtain an attention weight vector\nB ∈R2×o, where b1,p is the probability that the p-th word from the document is the start word of\nthe answer, and b2,q is the probability that the q-th word is the end word.\n6\nExperiment\n6.1\nMatch-LSTM Baseline\nAfter many iterations, we achieved a signiﬁcant score of ( F1=53%, EM=39%) (complete score in\n6.4 ) and settled for that as a baseline score. Although there was huge scope for improvement, but\nwe wanted to explore other advanced models.\n6.2\nSummary-attentive reader\nTo evaluate the summarization engine, we truncate each document in the train set to different word\nlength. The success of the engine is measured by the ground truth answer retain rate, which is the\npercentage of ground truth that are wholly contained in the summary we selected. This can serve as\na metric of how well the engine is doing as we can expect the same summarization, when applied to\ndocument with unknown answer from the test set, will also retain the actual answer for subsequent\nlayer prediction.\nEven though the maximum document size of our train set is 600 words, the summarization engine\nis able to retain more than 98.31% of the ground truth answer when we truncate the document to\n200 words or more (Fig. 3). Even when we truncate the document down to 150 words, 92.17% of\nanswers are contained in the summary. We also see that max-pooling performs marginally well than\n6\nmean-pooling in the averaging across word embedding when obtaining sentence-level representa-\ntion.\nFigure 3: Ground truth answer retain rate with respect to the truncated document summary length.\nThe original document has a maximum length of 600, and the summarization engine is able to retain\nmore than 92% of ground truth answers in the summary when we truncate the document to length\n150.\n6.3\nExperiment Details\nAftering experimenting with hyperparameters and various preprocessing settings, we settle on the\nfollowing experiment details which gave the optimal result.\nWe apply pretrained GloVe word embeddings trained on common crawl with 840B tokens and 300\ndimensions [15]. The Stanford CoreNLP toolkit served as tokenizer [16], while the document are\ntruncated to a length of 500 while questions are similar truncated to 35 words as questions/passages\nlonger than this length are few. We also adopt the following hyperparameters during training:\n• Learning rate = 0.001\n• Global norm gradient clipping threshold = 5\n• Adam optimizer with β1 = 0.9, β2 = 0.999\n• Dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n• Hidden state size = 200\nWe also experimented with trainable vs. constant embeddings and found the latter to give better\nvalidation score (1-2% lift) due to the relatively small vocabulary in the dataset.\n6.4\nResults\nSystem\nval EM/F1\ndev EM/F1\ntest EM/F1\nSeq2seq Attetion Baseline\nMatch LSTM Baseline\n38.8/ 53.8\n42.1/55.4\nCoattention + feedforward decoder\n48.6/64.0\n46.4/60.8\n46.1/60.1\nCoattention + ans-ptr\n49.7/65.2\nCoattention + ans-ptr + dropout +word lookup\n56.0/69.8\n61.6/72.3\n61.9/72.8\n7\n7\nDiscussion\nEven though the summarization engine did not effectively raise prediction accuracy in the current\niteration in our model, we believe that the underlying idea is of great potential. As the current\nmachine-based question-answering models do not perform well enough to challenge human readers,\nour attempt to draw inspiration from the human reading process is well-founded. Interestingly, other\nmodels performing the Q&A task has developed similar summarization features to ours [1], though\ntheir selection scheme is more sophisticated and thus presumably of higher quality.\nAs an immediate next step, we’re planning to continue optimizing the summarization engine by (1)\nno longer limiting the summary to consists of consecutive sentences (2) develop better boundaries of\nselection in addition the natural sentence spans, and ﬁnally (3) incorporate semantic understanding\nof the text based on matching information ﬂow to the question to better faciliate summarizing.\nReferences\n[1] Wenpeng Yin, Sebastian Ebert, and Hinrich Sch¨utze. Attention-based convolutional neural network for\nmachine comprehension. arXiv preprint arXiv:1602.04341, 2016.\n[2] Matthew Richardson, Christopher JC Burges, and Erin Renshaw. Mctest: A challenge dataset for the\nopen-domain machine comprehension of text. In EMNLP, volume 3, page 4, 2013.\n[3] Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu Florian. Multi-perspective context matching for ma-\nchine comprehension. arXiv preprint arXiv:1612.04211, 2016.\n[4] Shuohang Wang and Jing Jiang. Machine comprehension using match-lstm and answer pointer. arXiv\npreprint arXiv:1608.07905, 2016.\n[5] Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question answer-\ning. arXiv preprint arXiv:1611.01604, 2016.\n[6] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention ﬂow\nfor machine comprehension. arXiv preprint arXiv:1611.01603, 2016.\n[7] Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Dipanjan Das. Learning recurrent span representations\nfor extractive question answering. arXiv preprint arXiv:1611.01436, 2016.\n[8] Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in\nmachine comprehension. arXiv preprint arXiv:1609.05284, 2016.\n[9] Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, and Bowen Zhou. End-to-end answer chunk\nextraction and ranking for reading comprehension. arXiv preprint arXiv:1610.09996, 2016.\n[10] Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W Cohen, and Ruslan Salakhutdinov. Words\nor characters? ﬁne-grained gating for reading comprehension. arXiv preprint arXiv:1611.01724, 2016.\n[11] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[12] Arvind Neelakantan, Quoc V Le, Martin Abadi, Andrew McCallum, and Dario Amodei. Learning a\nnatural language interface with neural programmer. arXiv preprint arXiv:1611.08945, 2016.\n[13] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In C. Cortes, N. D. Lawrence,\nD. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems\n28, pages 2692–2700. Curran Associates, Inc., 2015.\n[14] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based\nneural machine translation. CoRR, abs/1508.04025, 2015.\n[15] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word repre-\nsentation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, 2014.\n[16] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky.\nThe stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the\nassociation for computational linguistics: system demonstrations, pages 55–60, 2014.\n8\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-03-04",
  "updated": "2018-03-04"
}