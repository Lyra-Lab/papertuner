{
  "id": "http://arxiv.org/abs/1803.02323v1",
  "title": "Deep Super Learner: A Deep Ensemble for Classification Problems",
  "authors": [
    "Steven Young",
    "Tamer Abdou",
    "Ayse Bener"
  ],
  "abstract": "Deep learning has become very popular for tasks such as predictive modeling\nand pattern recognition in handling big data. Deep learning is a powerful\nmachine learning method that extracts lower level features and feeds them\nforward for the next layer to identify higher level features that improve\nperformance. However, deep neural networks have drawbacks, which include many\nhyper-parameters and infinite architectures, opaqueness into results, and\nrelatively slower convergence on smaller datasets. While traditional machine\nlearning algorithms can address these drawbacks, they are not typically capable\nof the performance levels achieved by deep neural networks. To improve\nperformance, ensemble methods are used to combine multiple base learners. Super\nlearning is an ensemble that finds the optimal combination of diverse learning\nalgorithms. This paper proposes deep super learning as an approach which\nachieves log loss and accuracy results competitive to deep neural networks\nwhile employing traditional machine learning algorithms in a hierarchical\nstructure. The deep super learner is flexible, adaptable, and easy to train\nwith good performance across different tasks using identical hyper-parameter\nvalues. Using traditional machine learning requires fewer hyper-parameters,\nallows transparency into results, and has relatively fast convergence on\nsmaller datasets. Experimental results show that the deep super learner has\nsuperior performance compared to the individual base learners, single-layer\nensembles, and in some cases deep neural networks. Performance of the deep\nsuper learner may further be improved with task-specific tuning.",
  "text": "Deep Super Learner:\nA Deep Ensemble for Classiﬁcation Problems\nSteven Young, Tamer Abdou, and Ayse Bener\nData Science Laboratory, Ryerson University, Toronto ON M5B 2K3, Canada,\n{steven.young, tamer.abdou, ayse.bener}@ryerson.ca\nAbstract. Deep learning has become very popular for tasks such as\npredictive modeling and pattern recognition in handling big data. Deep\nlearning is a powerful machine learning method that extracts lower level\nfeatures and feeds them forward for the next layer to identify higher\nlevel features that improve performance. However, deep neural networks\nhave drawbacks, which include many hyper-parameters and inﬁnite ar-\nchitectures, opaqueness into results, and relatively slower convergence on\nsmaller datasets. While traditional machine learning algorithms can ad-\ndress these drawbacks, they are not typically capable of the performance\nlevels achieved by deep neural networks. To improve performance, ensem-\nble methods are used to combine multiple base learners. Super learning\nis an ensemble that ﬁnds the optimal combination of diverse learning\nalgorithms. This paper proposes deep super learning as an approach\nwhich achieves log loss and accuracy results competitive to deep neural\nnetworks while employing traditional machine learning algorithms in a\nhierarchical structure. The deep super learner is ﬂexible, adaptable, and\neasy to train with good performance across diﬀerent tasks using identi-\ncal hyper-parameter values. Using traditional machine learning requires\nfewer hyper-parameters, allows transparency into results, and has rela-\ntively fast convergence on smaller datasets. Experimental results show\nthat the deep super learner has superior performance compared to the\nindividual base learners, single-layer ensembles, and in some cases deep\nneural networks. Performance of the deep super learner may further be\nimproved with task-speciﬁc tuning.\nKeywords: Deep Learning, Neural Network, Ensemble Learning\n1\nIntroduction\nDeep learning is a machine learning method that uses layers of processing units\nwhere the output of a layer cascades to be the input of the next layer and\ncan be applied to either supervised or unsupervised learning problems [1] [2].\nDeep neural networks (DNN) is an architecture of deep learning that typically\n1 This paper was written as part of the Certiﬁcate in Data Analytics, Big Data, and\nPredictive Analytics at Ryerson University\narXiv:1803.02323v1  [cs.LG]  6 Mar 2018\nhas many connected units arranged in layers of varying sizes with information\nbeing fed forward through the network. DNN have been successfully applied to\nﬁelds such as computer vision and natural language processing, having achieved\naccuracy rates similar or superior to humans in classiﬁcation [3]. For example,\nCiresan et al. using DNN achieved an error rate half the rate of humans in\nrecognizing traﬃc signs. The multiple layers of a DNN allow for varying levels of\nabstraction and the cascade between the layers enables the extraction of features\nfrom lower to higher level layers to improve performance [4]. However, DNN also\nhave drawbacks, listed below:\n– DNN have many hyper-parameters, which are parameters where their val-\nues are set prior to training as opposed to parameter values that are set via\ntraining, that interact with each other in their relation to performance. Nu-\nmerous hyper-parameters, together with inﬁnite architectures, makes tuning\nof hyper-parameter and architecture diﬃcult [5].\n– With a large number of processing units, tracing through a DNN to un-\nderstand the reasoning for classiﬁcations is diﬃcult, leading to DNN being\ntreated as black boxes [6].\n– DNN typically require very large amounts of data to train and do not con-\nverge as fast, with respect to sample size, as traditional machine learning\nalgorithms [7].\nTraditional machine learning algorithms, on the other hand, are relatively simple\nto tune and their output may provide interpretable results leading to a deeper\nunderstanding of the problem, though they tend to underperform DNN in terms\nof accuracy.\nThe remainder of this paper is organized as follows: section 1 introduces\nthe motivation and background for this paper, section 2 presents the overall\nprocedure of the DSL approach, section 3 describes the methodology of the\nexperiment, section 4 presents the results of a comparison of the performance of\nthe DSL to the individual base learners and a selection of ensembles and DNN\non various problems, and section 5 concludes and describes future work.\n1.1\nMotivation\nGiven the drawbacks of DNN and the poor performance of traditional machine\nlearning algorithms in some domains and/ or prediction tasks, this paper inves-\ntigates whether traditional machine learning algorithms can be used to address\nthe drawbacks of DNN and achieve levels of performance comparable to DNN.\nA new ensemble method, named here as Deep Super Learner (DSL), seeks to\nhave simplicity in setup, interpretability of results, fast convergence on small\nand large datasets with the power of deep learning.\n1.2\nEnsemble Methods\nEnsemble methods are techniques that train multiple learning algorithms, which\nin combination yields signiﬁcantly higher accuracy results than a single learner [8].\nCommon methods include boosting, bagging, stacking, and a combination of base\nlearners. Each of these methods are tested for performance in this paper. Boost-\ning takes a model trained on data and incrementally constructs new models\nthat focus on the errors in classifying made by the previous model. An example\nis XGBoost, which is an eﬃcient implementation of gradient boosting decision\ntrees [9]. Bagging involves training models on random subsamples and then each\nmodel votes with equal weight on the classiﬁcation. Random forest uses a bag-\nging approach to enable the selection of a random set of features at each internal\nnode [10]. Stacking takes the output of a set of models and feeds them into an-\nother algorithm that combines them to make the ﬁnal predictions. Any arbitrary\nset of base learners and combiner algorithm can be used. Combination takes the\npredictions of the models and combines them with a simple or weighted average.\nSuper learner is a combination method that ﬁnds optimal weights to use when\ncalculating the ﬁnal prediction [11].\nSuper learning is an ensemble method proposed by Van der Laan et al. that\noptimizes the weights of the base component learners by minimizing a loss func-\ntion given cross-validated output of the learners. Super learning ﬁnds the optimal\nset of weights for the learners and guarantees that performance will be at least as\ngood as the best base learner [11]. The proposed algorithm, DSL, is an extension\nof the super learner ensemble.\nWhen constructing an ensemble, having diversity among the component\nlearners is essential for performance and a strong generalization ability [12].\nThe super learner adapts to various problems given a set of diverse base learners\nsince the weights of the components are optimized for the problem as diﬀerent\nlearners perform diﬀerently on diﬀerent problems. There is also ﬂexibility in the\nset of base learners to use depending on requirements or constraints as dictated\nby the problem or computational resources.\n1.3\nRelated Work\nVery little research has been conducted on using traditional machine learning in\na deep learning architecture. Zhou and Feng describe a tree-based deep learning\narchitecture [5]. However, the use of only decision tree based algorithms and the\nuse of a simple average to combine the results of the base learners may limit\nthe ultimate performance of this approach and its adaptability to diverse set\nof problems. Farrelly tested an architecture using traditional learners arranged\nin three hidden layers [7]. It is unclear if the implementation allowed iteration\nto continue the deep learning. Accuracy results from this architecture did not\noutperform the super learner.\n2\nThe Proposed Deep Super Learner Approach\nDeep learning consists of a layer by layer processing of features with a cascading\nhierarchy structure where the information processed by a layer is fed to the next\nlayer for further processing. The deep super learner essentially uses a super learn-\ning ensemble for each layer. The overall training process and hyper-parameter\nvalues used are described below (see Figure 1 and Algorithm 1). Let there be j\nclasses, k folds, l features, m base learners, and n records in the training set.\nFig. 1. Overall procedure of DSL with j classes, k folds, l features, m learners, n records\n1. Cross-validation is used to generate out-of-sample predictions for the entire\ntraining set. Split the training set into k equal size, n/k, mutually exclusive\ngroups to be used as validation sets. For each of the k validation sets form\nk folds where all the n −n/k training records not in the validation set are\nused for training for that validation set. The number of folds can impact the\ndegree of under and over-ﬁtting of the algorithm as well as runtime since with\na higher number of folds each fold contains a larger portion of the training\ndata to train on leading to a better ﬁt, all else being equal. However, with\nmore folds there is a greater overlap in the training data between the folds\nleading to potential over-ﬁtting and more runtime. Three folds are used in\nthis paper as experimentation showed three folds to be a good balance of ﬁt\nand runtime.\n2. Build and train each of the m base learning algorithms on each fold. Each\nmodel outputs class probabilities for each fold. There are k ∗m trained\nmodels. Bring the predictions on each validation set together for each learner\nto obtain predictions for the entire training set. This paper uses ﬁve base\nlearners: logistic regression, k-nearest neighbors, random forest, extremely\nrandomized trees, and XGBoost. Logistic regression, k-nearest neighbors,\nand random forest are used as they represent three diﬀerent classiﬁcation\nmodels with diﬀerent philosophies and performance on diﬀerent datasets [13].\nExtremely randomized trees and XGBoost are used for additional diversity\nwithin tree-based approaches. The hyper-parameters of these learners are\ndescribed below.\n3. Optimize a loss function to ﬁnd the linear combination of predictions from\neach of the m learners to minimize the objective against the true values\nfor the training data. Save the value of the loss function and the optimized\nweights. Log loss, which is a convex function and therefore convex optimiza-\ntion is suﬃcient, is used here.\n4. With the optimized weights, calculate the weighted average of the predictions\nacross learners to obtain overall predictions for each record.\n5. (Optional) Re-train each of the models on the entire training set to get m\ntrained models. Super learning as described by Van der Laan et al. requires\nthis step [11]. However, with a suﬃcient number of cross-validation folds, as\ndescribed above, the trained models will be trained on a suﬃcient portion of\nthe training data where additional training data does not improve goodness\nof ﬁt. Re-training may also be computationally expensive. This step is rec-\nommended when the number of folds is low or when making predictions is\nmore computationally expensive than training, such as for k-nearest neigh-\nbors. This step was not performed for this paper as experimentation showed\nno signiﬁcant diﬀerence in performance on the tested datasets.\n6. Append the overall predictions to the original training data. For example, if\nthere are j classes, step 4 produces a j-dimensional vector containing class\nprobabilities for each record. These vectors are concatenated as additional\nfeatures to the original l-dimensional feature vectors for the records, resulting\nin a total of l + j features.\n7. Feed the training data augmented with the predictions through the steps\nabove. Repeat this process until the optimized loss value no longer decreases\nwith each iteration. Save the number of iterations after which the training\nprocess ends.\nTo make predictions on unseen test data, pass the data in its entirety through\na similar process using each of the models trained and weights optimized at each\niteration. If the models are trained on the entire training set, use these m models\nfor each iteration. If the models are trained on the k folds, use each model trained\non each fold to make predictions on all the unseen data and average across the\nk models to get predictions for each of the m learners. Using the optimum\nweights for the m learners found during training for the iteration, calculate the\noverall weighted average predictions for the iteration. Append the predictions\nto the original test data as additional features. Repeat the process for the same\nnumber of iterations used in training.\n3\nMethodology\nThe hyper-parameters and architectures for the DSL, base learners, benchmark\nensembles, and benchmark DNN described below are kept constant between\ndatasets. When necessary, adjustments are made for the diﬀerent dimensionality\nof the datasets.\nfor iteration in 1 to max iterations do\nSplit data into k folds each with train and validate sets;\nfor each fold in k folds do\nfor each learner in ensemble do\nTrain learner on train set in fold;\nGet class probabilities from learner on validate set in fold;\nBuild predictions matrix of class probabilities;\nend\nend\nGet weights to minimize loss function with predictions and true labels;\nGet average probabilities across learners by multiplying predictions with\nweights;\nGet loss value of loss function with average probabilities and true labels;\nif loss value is less than loss value from previous iteration then\nAppend average probabilities to data;\nelse\nSave iteration;\nBreak for;\nend\nend\nAlgorithm 1: A Pseudo-code of the Proposed Approach, DSL\n3.1\nBase Learners and Benchmark Ensembles\nThe same ﬁve base learners used in DSL are also tested individually and in\nthe benchmark ensembles using identical hyper-parameter values. If a hyper-\nparameter of a learner is not listed below, in Table 1, default values of the\nimplementation of the algorithm are used.\nTable 1. Summary of hyper-parameters for base learning algorithms\nBase learner\nHyper-parameters\nLogistic regression\nN/A\nk-Nearest neighbors\nNeighbors: 11\nRandom forest\nTrees: 200; Depth: unlimited\nExtremely randomized trees\nTrees: 200; Depth: unlimited;\nMax features when splitting: 1\nXGBoost\nTrees: 200; Max depth: 3;\nRow subsampling: 0;\nColumn subsampling: 0; Learning rate: 1\nSince random forest, extremely randomized trees, and XGBoost are them-\nselves ensembles, three additional ensembles are tested for comparison: a simple\nequal weighted average of the base learners, a stacked ensemble where the output\nof the base learners is fed into XGBoost, and a single-layer super learner.\n3.2\nBenchmark Deep Neural Networks\nDNN are used to establish benchmarks for performance. Some hyper-parameter\ntuning through experimentation is performed to achieve performance indicative\nof the capabilities of DNN. In Table 2, two DNN architectures are tested and\ndescribed : a multi-layer perceptron (MLP) and a convolutional neural network\n(CNN)\nTable 2. Summary of architecture and hyper-parameter values used for benchmark\ndeep neural networks.\nArchitecture\nHyper-parameters\n(Multi-layer perceptron)\nHyper-parameters\n(Convolutional neural network)\nConvolutional layer\nN/A\nFilters: 32; Kernel size: 5 or (5, 5);\nActivation: RELU;\nWeight constraint: 4\nMax pooling layer\nN/A\nPool size: 2 or (2, 2)\nConvolutional layer\nN/A\nFilters: 16; Kernel size: 3 or (3, 3);\nActivation: RELU;\nWeight constraint: 4\nMax pooling layer\nN/A\nPool size: 2 or (2, 2)\nDropout regularization N/A\nDrop rate: 0.2\nDense layer\nNodes: 128; Activation: RELU;\nWeight constraint: 4\nNodes: 128; Activation: RELU;\nWeight constraint: 4\nDense layer\nNodes: 64; Activation: RELU;\nWeight constraint: 4\nNodes: 64; Activation: RELU;\nWeight constraint: 4\nOutput layer\nNodes: number of classes;\nActivation: Softmax\nNodes: number of classes;\nActivation: Softmax\nOptimizer: Adam\nLearning rate: 0.001;\nLearning rate decay:\nLearning rate/√Max epochs\nLearning rate: 0.001;\nLearning rate decay:\nLearning rate/√Max epochs\nBatch size\n200\n200\nMax epochs\n50\n50\nValidation split\n0.2\n0.2\nEarly stopping patience 3\n3\n3.3\nDatasets\nSentiment Classiﬁcation\nThe IMDB Movie reviews sentiment classiﬁcation dataset contains 25,000 re-\nviews for training and 25,000 for testing. The reviews have been labelled as\npositive or negative [14]. The 2,000 most frequent words in the set are used to\ncalculate the term frequency-inverse document frequency (TF-IDF) matrix.\nImage Categorization\nThe MNIST database of handwritten digits is a commonly used dataset to test\nthe performance of computer vision algorithms. It includes a training set of\n60,000 images and a test set of 10,000 images of handwritten digits 0 to 9. The\nimages are 28 pixels by 28 pixels in greyscale [15].\n3.4\nPerformance Measures\nTwo metrics are used to evaluate the performance of the learning algorithms. One\nis Accuracy, which is the proportion of correctly classiﬁed records, and the other\nis LogLoss. Both Accuracy and LogLoss formulas are shown in Equations 1\nand 2, respectively.\nAccuracy =\nPn\nx=1\nPj\ny=1 f(x, y)C(x, y)\nn\n=\nTP + TN\nTP + FP + TN + FN\n(1)\nWhere n denotes the number of instances, j the number of classes, f(x, y) the\nactual probability of instance x to be of class y. C(x, y) is one if and only if y\nis the predicted class of x, otherwise C(x, y) is zero. Accuracy is equivalently\ndeﬁned in terms of the confusion matrix, where TP is true positives, TN is true\nnegatives, FP is false positives, and FN is false negatives.\nLogLoss =\n−Pj\ny=1\nPn\nx=1 f(x, y)log(p(x, y))\nn\n(2)\nWhere f(x, y) is deﬁned as above and p(x, y) is the estimated probability of\ninstance x is class y. Minimizing LogLoss, also known as cross entropy, is equiv-\nalent to maximizing the log likelihood of observing the data from the model. Both\nAccuracy and LogLoss are commonly used performance measures in machine\nlearning [16].\n4\nResults\n4.1\nSentiment Classiﬁcation\nLog loss and accuracy results of DSL, base learners, benchmark ensembles, and\nbenchmark DNN on the IMDB sentiment classiﬁcation dataset are shown in\nTable 3.\nThe DSL achieved statistically signiﬁcantly lower loss and higher accuracy\nthan all other algorithms. Since the TF-IDF matrix does not convey spatial\nor sequential relationships, DNN architectures CNN may not be expected to\nperform as well on this test. The MLP, like DSL here, is set up to be more\ngeneral purpose yet is outperformed by DSL. DSL outperforming a single-layer\nsuper learner indicates adding depth to the algorithm improves performance.\nFigure 2 shows the performance of DSL on the IMDB test data by iteration.\nTable 3. Comparison of log loss and accuracy on IMDB test data\nTest\nLog loss\nAccuracy\nDeep Super Learner (DSL)\n0.28\n88.22%\nMulti-layer perceptron\n0.29\n87.53%\nSuper learner\n0.30\n86.59%\nXGBoost stack ensemble\n0.31\n87.19%\nConvolutional neural network\n0.31\n86.40%\nLogistic regression\n0.32\n87.78%\nXGBoost\n0.39\n84.45%\nSimple average ensemble\n0.42\n86.57%\nRandom forest\n0.46\n84.27%\nExtremely randomized trees\n0.60\n79.20%\nk-Nearest neighbors\n0.72\n68.22%\nFig. 2. Log loss and accuracy by iteration of the DSL on IMDB test data.\n4.2\nImage Categorization\nLog loss and accuracy results of DSL, base learners, benchmark ensembles, and\nbenchmark DNN on the MNIST handwritten digits dataset are shown in Table 4.\nThe DSL achieved statistically signiﬁcantly lower loss and higher accuracy\nthan all algorithms except for CNN. The design of CNN make them well suited\nto image processing. Again, DSL outperformed MLP and super learner showing\nthe advantages of diversity in learners and depth. The order of the base learners\nby performance diﬀers between the two datasets showing the importance of\nincluding a diverse set of learners when addressing various problems and the\nvalue of optimizing the component weights. Figure 3 shows the performance of\nDSL on the MNIST test data by iteration.\nTable 4. Comparison of log loss and accuracy on MNIST test data.\nTest\nLog loss\nAccuracy\nConvolutional neural network\n0.03\n99.17%\nDeep Super Learner (DSL)\n0.06\n98.42%\nSuper learner\n0.07\n97.82%\nMulti-layer perceptron\n0.07\n97.85%\nXGBoost stack ensemble\n0.08\n98.24%\nXGBoost\n0.08\n97.65%\nSimple average ensemble\n0.18\n97.65%\nRandom forest\n0.24\n97.00%\nk-Nearest neighbors\n0.26\n96.68%\nLogistic regression\n0.27\n92.55%\nExtremely randomized trees\n0.43\n95.87%\nFig. 3. Log loss and accuracy by iteration of the DSL on MNIST test data.\n4.3\nRuntime\nAll algorithms are implemented in Python using the scikit-learn library for lo-\ngistic regression, k-nearest neighbors (KNN), random forest, and extremely ran-\ndomized trees, XGBoost library for XGBoost, SciPy for the convex optimizer,\nand Keras with a TensorFlow backend for MLP and CNN. Experiments are run\non a desktop with an Intel Core i7-7700 with 16 GB of RAM. DSL on IMDB\nconverged after three iterations running for a total of 50 minutes, 46 of which\nare spent in the prediction phase of KNN. MLP on IMDB converged after two\nepochs running for one minute. CNN on IMDB converged after six epochs run-\nning for a total of seven minutes. DSL on MNIST converged after ﬁve iterations,\nrunning for a total of 86 minutes, 70 of which are spent in the prediction phase\nof KNN. MLP on MNIST converged in 12 epochs running for two minutes. CNN\non MNIST converged in 12 epochs running for a total of 12 minutes. DSL is in-\nherently parallel across component learners. With optimized parallel processing\nand selection of base learners, the runtime of DSL can be dramatically reduced.\n4.4\nThreats to Validity\nThreats to internal validity include errors in the experiments and implementa-\ntions. While the experiments and implementations were carefully double checked,\nerrors are possible. Threats to external validity include whether the results in\nthis paper generalize to other datasets, sets of base learners and architectures.\nWhile the tests include two rather diﬀerent datasets, only one set of base learn-\ners and deep architecture were tested. Applying the methods described here to\nadditional datasets as well as varying base learners and architecture will re-\nduce this threat. Threats to construct validity include the appropriateness of\nthe benchmark algorithms and evaluation metrics. For the most part, bench-\nmark ensembles outperformed their component learners and DNN outperformed\nensembles of base learners on the same tasks as expected. The use of log loss and\naccuracy are common in machine learning studies to evaluate the performance\nof prediction algorithms.\n5\nConclusion\nResults for the deep super learner are encouraging. Using a weighted average\nof the base learners optimized to minimize log loss yields results superior to\nany individual base learner. Using a cascade of multiple layers of base learners\nwhere each successive layer uses the output from the previous layer as augmented\nfeatures for input to add depth to the learning improved performance further.\nWhile still shy of the performance levels obtained by CNN on image data, the\ndeep super learner using traditional machine learning algorithms outperformed\nMLP on image data and outperformed MLP and CNN on classiﬁcation from a\nTF-IDF matrix while also having fewer hyper-parameters and providing inter-\npretable and transparent results. Though still in the early stages of development\nof a deep super learning ensemble, particularly compared to DNN, further devel-\nopment of the architecture, for example to better capture spatial or sequential\nrelationships, should be conducted.\nReferences\n1. Bengio, Y., Courville, A., Vincent, P.: Representation Learning: A Review and New\nPerspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence\n35(8) (8 2013) 1798–1828\n2. L¨angkvist, M., Karlsson, L., Loutﬁ, A.: A Review of Unsupervised Feature Learn-\ning and Deep Learning for Time-series Modeling. Pattern Recognition Letters 42\n(2014) 11–24\n3. Schmidhuber, J.: Multi-column Deep Neural Networks for Image Classiﬁcation.\nIn: Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR’12), Washington, DC, USA, IEEE Computer Society (2012)\n3642–3649\n4. Bengio, Y.: Learning Deep Architectures for AI. Foundations and Trends R\n⃝in\nMachine Learning 2(1) (1 2009) 1–127\n5. Zhou, Z.H., Feng, J.: Deep forest: Towards an Alternative to Deep Neural Net-\nworks. In: Proceedings of the 26th International Joint Conference on Artiﬁcial\nIntelligence (IJCAI ’17), Melbourne, Australia (2017) 3553–3559\n6. Sussillo, D., Barak, O.: Opening the Black Box: Low-Dimensional Dynamics in\nHigh-Dimensional Recurrent Neural Networks. Neural Computation 25(3) (2013)\n626–649\n7. Farrelly, C.M.: Deep vs. Diverse Architectures for Classiﬁcation Problems. (2017)\n8. Seni, G., Elder, J.F.:\nEnsemble Methods in Data Mining: Improving Accuracy\nThrough Combining Predictions.\nIn Grossman, R., ed.: Synthesis Lectures on\nData Mining and Knowledge Discovery. Morgan & Claypool (2010)\n9. Chen, T., Guestrin, C.: XGBoost: A Scalable Tree Boosting System. In: Proceed-\nings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, San Francisco, California, USA, ACM (2016) 785–794\n10. Xie, J., Rojkova, V., Pal, S., Coggeshall, S.:\nA Combination of Boosting and\nBagging for KDD Cup 2009 - Fast Scoring on a Large Database. The Journal of\nMachine Learning Research (JMLR) 7 (2009) 35–43\n11. van der Laan, M.J., Polley, E.C., Hubbard, A.E.: Super Learner. Statistical Ap-\nplications in Genetics and Molecular Biology 6(1) (1 2007)\n12. Zhou, Z.H.: Ensemble Methods: Foundations and Algorithms. illustrate edn. Chap-\nman & Hall/CRC. Machine Learning & Pattern Recognition Series, Boca Raton,\nFL (2012)\n13. Lessmann, S., Baesens, B., Mues, C., Pietsch, S.:\nBenchmarking Classiﬁcation\nModels for Software Defect Prediction: A Proposed Framework and Novel Findings.\nIEEE Transactions on Software Engineering 34(4) (7 2008) 485–496\n14. Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C.: Learning\nWord Vectors for Sentiment Analysis. In: Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies.\nHLT ’11, Portland, Oregon (2011) 142–150\n15. Lecun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based Learning Applied\nto Document Recognition. Proceedings of the IEEE 86(11) (1998) 2278–2324\n16. Ferri, C., Hern´andez-Orallo, J., Modroiu, R.:\nAn Experimental Comparison of\nPerformance Measures for Classiﬁcation. Pattern Recognition Letters 30(1) (2009)\n27–38\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-03-06",
  "updated": "2018-03-06"
}