{
  "id": "http://arxiv.org/abs/1803.03772v2",
  "title": "Generalization and Expressivity for Deep Nets",
  "authors": [
    "Shao-Bo Lin"
  ],
  "abstract": "Along with the rapid development of deep learning in practice, the\ntheoretical explanations for its success become urgent. Generalization and\nexpressivity are two widely used measurements to quantify theoretical behaviors\nof deep learning. The expressivity focuses on finding functions expressible by\ndeep nets but cannot be approximated by shallow nets with the similar number of\nneurons. It usually implies the large capacity. The generalization aims at\nderiving fast learning rate for deep nets. It usually requires small capacity\nto reduce the variance. Different from previous studies on deep learning,\npursuing either expressivity or generalization, we take both factors into\naccount to explore the theoretical advantages of deep nets. For this purpose,\nwe construct a deep net with two hidden layers possessing excellent\nexpressivity in terms of localized and sparse approximation. Then, utilizing\nthe well known covering number to measure the capacity, we find that deep nets\npossess excellent expressive power (measured by localized and sparse\napproximation) without enlarging the capacity of shallow nets. As a\nconsequence, we derive near optimal learning rates for implementing empirical\nrisk minimization (ERM) on the constructed deep nets. These results\ntheoretically exhibit the advantage of deep nets from learning theory\nviewpoints.",
  "text": "arXiv:1803.03772v2  [cs.LG]  23 Mar 2018\n1\nGeneralization and Expressivity for Deep Nets\nShao-Bo Lin\nAbstract—Along with the rapid development of deep learn-\ning in practice, theoretical explanations for its success become\nurgent. Generalization and expressivity are two widely used\nmeasurements to quantify theoretical behaviors of deep learning.\nThe expressivity focuses on ﬁnding functions expressible by\ndeep nets but cannot be approximated by shallow nets with\nthe similar number of neurons. It usually implies the large\ncapacity. The generalization aims at deriving fast learning rate\nfor deep nets. It usually requires small capacity to reduce\nthe variance. Different from previous studies on deep learning,\npursuing either expressivity or generalization, we take both\nfactors into account to explore theoretical advantages of deep\nnets. For this purpose, we construct a deep net with two hidden\nlayers possessing excellent expressivity in terms of localized and\nsparse approximation. Then, utilizing the well known covering\nnumber to measure the capacity, we ﬁnd that deep nets possess\nexcellent expressive power (measured by localized and sparse\napproximation) without essentially enlarging the capacity of\nshallow nets. As a consequence, we derive near optimal learning\nrates for implementing empirical risk minimization (ERM) on\nthe constructed deep nets. These results theoretically exhibit the\nadvantage of deep nets from learning theory viewpoints.\nIndex Terms—Deep learning, learning theory, generalization,\nexpressivity, localized approximation\nI. INTRODUCTION\nTechnological innovations on data mining bring massive\ndata in diverse areas of modern scientiﬁc research [48]. Deep\nlearning [15], [2] is recognized to be a state-of-the-art scheme\nto take advantage of massive data, due to their unreasonable\neffective empirical evidence. Theoretical veriﬁcations for such\neffectiveness of deep learning is a hot topic in recent years’\nstatistical and machine learning [13].\nOne of the most important reasons for the success of deep\nlearning is the utilization of deep nets, a.k.a., neural networks\nwith more than one hidden layers. In the classical neural\nnetwork approximation literature [38], deep nets were shown\nto outperform shallow nets, i.e., neural networks with one\nhidden layer, in terms of providing localized approximation\nand breaking through some lower bounds for shallow nets\napproximation. Besides these classical assertions, recent focus\n[18], [12], [43], [35], [26] on deep nets approximation is\nto provide various functions expressible for deep nets but\ncannot be approximated by shallow nets with similar number\nof neurons. All these results present theoretical veriﬁcations\nfor the necessity of deep nets from the approximation theory\nviewpoint.\nSince deep nets can approximate more functions than\nshallow nets, the capacity of deep nets seems to be larger\nthan that of shallow nets with similar number of neurons.\nS. Lin is with the Department of Mathematics, Wenzhou University, Wen-\nzhou, China and also with the Department of Mathematics, City University\nof Hong Kong, Kowloon, Hong Kong, China.\nThis argument was recently veriﬁed under some speciﬁed\ncomplexity measurements such as the number of linear regions\n[37], Betti numbers [3], number of monomials [11] and so on\n[39]. The large capacity of deep nets inevitably comes with the\ndownside of increased overﬁtting risk according to the bias and\nvariance trade-off principle [10]. For example, deep nets with\nﬁnitely many neurons were proved in [29] to be capable of\napproximating arbitrary continuous function within arbitrary\naccuracy, but the pseudo-dimension [28] for such deep nets is\ninﬁnite, which usually leads to extremely large variance in the\nlearning process. Thus the existing necessity of deep nets in\nthe approximation theory community cannot be used directly\nto explain the feasibility of deep nets in machine learning.\nIn this paper, we aim at studying the learning performance\nfor implementing empirical risk minimization (ERM) on some\nspeciﬁed deep nets. Our analysis starts with the localized\napproximation property as well as the sparse approximation\nability of deep nets to show their expressive power. We then\nconduct a reﬁned estimate for the covering number [46] of\ndeep nets, which is closely connected to learning theory [10],\nto measure the capacity. The result shows that, although deep\nnets possess localized and sparse approximation while shallow\nnets fail, their capacities measured by the covering number\nare similar, provided there are comparable number of neurons\nin both nets. As a consequence, we derive almost optimal\nlearning rates for the proposed ERM algorithms on deep\nnets when the so-called regression function [10] is Liptchiz\ncontinuous. Furthermore, we prove that deep nets can reﬂect\nthe sparse property of the regression functions via breaking\nthrough the established almost optimal learning rates. All\nthese results show that learning schemes based on deep nets\ncan learn more (complicated) functions than those based on\nshallow nets.\nThe rest of this paper is organized as follows. In the\nnext section, we present some results on the expressivity and\ncapacity of deep nets. These properties were utilized in Section\nIII to show outperformance of deep nets in the machine\nlearning community. In Section IV, we present some related\nwork and comparisons. In the last section, we draw a simple\nconclusion of our work.\nII. EXPRESSIVITY AND CAPACITY\nExpressivity [39] of deep nets usually means that deep nets\ncan represent some functions that cannot be approximated\nby shallow nets with similar number of neurons. Generally\nspeaking, expressivity implies the large capacity of deep nets.\nIn this section, we ﬁrstly show the expressivity of deep nets\nin terms of localized and sparse approximation, and then\nprove that the capacity measured by covering number is\nnot essentially enlarged when the number of hidden layer\nincreases.\n2\nFig. 1.\nLocalized approximation:realizing the location of inputs\nA. Localized approximation for deep nets\nLet\nSσ,n =\n\n\n\nn\nX\nj=1\ncjσ(wjx + θj) : cj, θj ∈R, wj ∈Rd\n\n\n\nbe the set of shallow nets with activation function σ and n\nneurons. Denote by Dσ1,σ2,n1,n2 the set of deep nets with\ntwo hidden layers\ng(x) =\nn2\nX\nk=1\nckσ2\n\n\nn1\nX\nj=1\nck,jσ1(wk,jx + θk,j) + θk\n\n\nwhere ck, ck,j, θk, θk,j ∈R, wk,j ∈Rd. The aim of this\nsubsection is to show the outperformance of Dσ1,σ2,n1,n2 over\nSσ,n to verify the necessity of depth in providing localized\napproximation.\nThe localized approximation of a neural network [7] shows\nthat if the target function is modiﬁed only on a small subset of\nthe Euclidean space, then only a few neurons, rather than the\nentire network, need to be retrained. As shown in Figure 1, a\nneural network with localized approximation should recognize\nthe location of the input in a small region. Mathematically\nspeaking, localized approximation means that for arbitrary\nhypercube Q ⊂X and arbitrary n ∈N, it is capable of ﬁnding\na neural network h such that χQ = h, where X is the input\nspace and χQ denotes the indicator function of the set Q, i.e.,\nχQ(x) = 1 when x ∈Q and χQ(x) = 0 when x /∈Q.\nLet d ≥2 and σ0 be the heaviside function, i.e. σ0(t) = 1,\nwhen t ≥0 and σ0(t) = 0 when t < 0. It can be found in\n[4, Theorem 5] (see also [7], [38]) that Sσ0,n cannot provide\nlocalized approximation, implying that functions in Sσ0,n with\nﬁnite number of neurons cannot catch the position information\nof the input. However, in the following, we will construct a\ndeep net in Dσ0,σ,2d,1 with some activation function σ and\ntotally 2d + 1 neurons to recognize the location of the input.\nLet σ : R →R be a sigmoidal function, i.e.,\nlim\nt→+∞σ(t) = 1,\nlim\nt→−∞σ(t) = 0.\nThen, for arbitrary ε > 0, there exists a Kε := K(ε, σ) > 0\ndepending only on σ and ε such that\n\u001a |σ(t) −1| < ε,\nif t ≥Kε,\n|σ(t)| < ε,\nif t ≤−Kε.\n(1)\nLet Id := [0, 1]d. Denote by {An,j}j∈Ndn the cubic partition of\nId with centers {ξj}j∈Ndn and side length 1\nn, where we write\narbitrary vector a ∈Rd as a = (a(1), . . . , a(d))T and Nd\nn =\n{1, 2, . . ., n}d . Then, for K > 0 and arbitrary j ∈Nd\nn, we\nconstruct a deep net Dσ0,σ,2d,1 by\nN ∗\nn,j,K(x) := σ\n(\n2K\n\" d\nX\nℓ=1\nσ0\n\u0014 1\n2n + x(ℓ) −ξ(ℓ)\nj\n\u0015\n+\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n −x(ℓ) + ξ(ℓ)\nj\n\u0015\n−2d + 1\n2\n#)\n.\n(2)\nIn the following proposition proved in Appendix A, we show\nthat deep nets possess totally different property from shallow\nnets in localized approximation.\nProposition 1. For arbitrary ε > 0, if N ∗\nn,j,Kε is deﬁned\nby (2) with Kε satisfying (1) and σ being a non-decreasing\nsigmoidal function, then\n(a) For arbitrary x /∈An,j, there holds N ∗\nn,j,Kε(x) < ε.\n(b) For arbitrary x ∈An,j, there holds 1 −N ∗\nn,j,Kε(x) ≤ε.\nIf we set ε →0, Proposition 1 shows that N ∗\nn,j,Kε is\nan indicator function for An,j, and consequently provides\nlocalized approximation. Furthermore, as n →∞, it follows\nfrom Proposition 1 that N ∗\nn,j,Kε can recognize the location of\nx in an arbitrarily small region. In the prominent paper [7], the\nlocalized approximation property of deep nets with two hidden\nlayers and sigmoidal activation functions was established in\na weaker sense. The difference between Proposition 1 and\nresults in [7] is that we adopt the heaviside activation function\nin the ﬁrst hidden layer to guarantee the equivalence of N ∗\nn,j,Kε\nand χAn,j. In the second hidden layer, it will be shown in\nSection II.C that some smoothness assumptions should be\nimposed on the activation function to derive a tight bound\nof the covering number. Thus, we do not recommend the use\nof heaviside activation. In short, we require different activation\nfunctions in different hidden layers to show excellent expres-\nsivity and small capacity of deep nets.\nCompared with shallow nets in Sσ0,n, the constructed deep\nnet N ∗\nn,j,K introduces a second hidden layer to act as a\njudger to discriminate the location of inputs. Figure 2 below\nnumerically exhibits the localized approximation of N ∗\nn,j,K\nwith n = 4, d = 2, K\n= 10000, ξj being the center\nof the yellow zone in Figure 1 and σ being the logistic\nfunction, i.e., σ(t) =\n1\n1+e−t . As shown in Figure 2, we can\nconstruct deep net that control a small region of the input\nspace but is independent of other regions. Thus, if the target\nfunction changes only on a small region, then it is sufﬁcient\nto tune a few neurons, rather than retrain the entire network.\nSince the locality of the data abound in sparse coding [36],\nstatistical physics [27] and image processing [44], the localized\napproximation makes deep nets be effective and efﬁcient in the\nrelated applications.\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFig. 2.\nLocalized approximation for the constructed deep net in (2)\nFig. 3.\nSparseness in the spacial domain: an example of 4-sparse in 16\npartitions function\nB. Sparse approximation for deep nets\nThe localized approximation property of deep nets shows\ntheir power to recognize functions deﬁned on small regions.\nA direct consequence is that deep nets can reﬂect the sparse\nproperty of the target functions in the spacial domain. In this\npart, based on the localized approximation property established\nin Proposition 1, we focus on developing a deep net with\nsparse approximation property in the spacial domain.\nSparseness in the spacial domain means that the response\nof some actions happens only on several small regions in the\ninput space, just as sparse coding [36] purports to show. As\nshown in Figure 3, sparseness studied in this paper means the\nresponse (or function) vanishes in a large number of regions\nand requires neural networks to recognize where the response\ndoes not vanish.\nMathematically speaking, denote by {BN,k}k∈Nd\nN the cubic\npartitions of Id with center ζk and side length\n1\nN . For s ∈N\nwith s ≤N d, deﬁne\nΛs :=\n\b\nkℓ: kℓ∈Nd\nN, 1 ≤ℓ≤s\n\t\n(3)\nand\nS := ∪k∈ΛsBN,k.\n(4)\nIt is easy to see that S contains arbitrary regions consisting at\nmost s sub-cubes (such as the yellow zones in Figure 3 with\ns = 4). We then say S is a sparse subset of Id of sparseness\ns. For some function f deﬁned on Id, if the support of f is\nS, we then say that f is s-sparse in N d partitions.\nAs discussed above, the sparseness depends on the localized\napproximation property. We thus can construct a deep net to\nembody the spareness by the help of the constructed deep net\nin (2). For arbitrary ε > 0 and η := {ηj}j∈Nd\nn with ηj ∈An,j,\ndeﬁne\nNn,η,Kε(x) :=\nX\nj∈Ndn\nf(ηj)N ∗\nn,j,Kε(x),\n(5)\nwhere {An,j}j∈Ndn is the cubic partition deﬁned in the previous\nsubsection. Obviously, we have Nn,η,Kε ∈Dσ0,σ,2d,nd which\npossesses nd(2d + 1) neurons. In the following Proposition 2,\nwe will show that Nn,η,Kε can embody the sparseness of the\ntarget function by exhibiting a fast approximation rate which\nbreaks through the bottleneck of shallow nets.\nFor this purpose, we should at ﬁrst introduce some a-priori\ninformation on the target function. We say a function f : Id →\nR is (r, c0)-Lipschitz if f satisﬁes\n|f(x) −f(x′)| ≤c0∥x −x′∥r,\n∀x, x′ ∈Id,\n(6)\nwhere r, c0 > 0 and ∥x∥denotes the Euclidean norm of x.\nDenote by Lip(r,c0) the family of (r, c0)-Lipschitz functions\nsatisfying (6). The Lipschitz property describes the smoothness\ninformation of f and has been adopted in vast literature [7],\n[28], [38], [22], [9] to quantify the approximation ability of\nneural networks. Denote by Lip(N,s,r,c0) the set of all f ∈\nLip(r,c0) which is s-sparse in N d partitions. It is easy to check\nthat Lip(N,s,r,c0) quantiﬁes both smoothness information and\nsparseness in the spacial domain of the target function.\nThen, we introduce the support set of Nn,η,Kε. Note that the\nnumber of neurons of Nn,η,Kε controls the side length of the\ncubic partition {An,j}j∈Nd\nn, while f is supported on s cubes\nin {BN,k}k∈Nd\nN. Since {BN,k}k∈Nd\nN is ﬁxed, we need to tune\nn such that the constructed deep net Nn,η,Kε can recognize\neach BN,k with k ∈Nd\nN. Under this circumstance, we take\nn ≥4N and for each k ∈Nd\nN, deﬁne\nΛk := {j ∈Nd\nn : An,j ∩BN,k ̸= ∅}.\n(7)\nThe set S\nk∈Λs Λk corresponds to the family of cubes An,j\nwhere f is not vanished. Since each An,j can be recognized by\n2d+1 neuron of Nn,η,Kε as given in Proposition 1, S\nk∈Λs Λk\nactually describes the support of Nn,η,Kε. With these helps,\nwe exhibit in the following proposition that Nn,η,Kε possesses\nthe spare approximation ability, whose proof will be presented\nin Appendix A.\nProposition 2. Let ε > 0 and Nn,η,Kε be deﬁned by (5). If\nf ∈Lip(N,s,r,c0) with N, s ∈N, 0 < r ≤1 and c0 > 0,\nKε satisﬁes (1), σ is a non-decreasing sigmoidal function and\nη = {ηj}j∈Ndn with ηj ∈An,j, then for arbitrary x ∈Id, there\nholds\n|f(x) −Nn,η,Kε(x)| ≤2r/2c0n−r + ∥f∥L∞(Id)ndε.\n(8)\nFurthermore, if n ≥4N, we have\n|Nn,η,Kε(x)| ≤∥f∥L∞(Id)ndε,\n∀x ∈Id\\\n[\nk∈Λs\nΛk. (9)\n4\nIt can be derived from (8) with S = Id and ε ≤n−d−r\nthat the deep net constructed in (5) satisﬁes the well known\nJackson-type inequality [22] for multivariate functions. This\nproperty shows that in approximating Lipschitz functions,\ndeep nets perform at least not worse than shallow nets [38].\nIf additional sparseness information is presented, i.e. f ∈\nLip(N,s,r,c0) with s < N d, by setting ε →0, (9) illustrates\nthat for every x ∈Id\\ S\nk∈Λs Λk,\nNn,η,Kε(x) →0,\nimplying the sparseness of Nn,η,Kε in the spacial domain. It\nshould be highlighted that for each k ∈Λs the cardinality of\nΛk, denoted by |Λk|, satisﬁes\n|Λk| ≤\n\u0010 n\nN + 2\n\u0011d\n≤2dnd\nN d ,\n∀n ≥4N.\n(10)\nTherefore, there are at least\n(2d + 1)nd −(2d + 1)s2dnd\nN d\n= (2d + 1)nd N d −2ds\nN d\nneurons satisfying (9), which is large when s is small with re-\nspect to N d. The aforementioned sparse approximation ability\nreduces the complexity of deep nets in approximating sparse\nfunctions, which makes deep-net-based learning breaks though\nsome limitations of shallow-net-based learning, as shown in\nSection III.\nC. Covering number of deep nets\nProposition 1 and Proposition 2 show the expressive power\nof deep nets. In this subsection, we exhibit that the capacity\nof deep nets, measured by the well known covering number,\nis similar as that of shallow nets, implying that deep nets can\napproximate more functions than shallow nets but do not bring\nadditional costs.\nLet B be a Banach space and V be a compact set in B.\nDenote by N(ε, V, B) the covering number [46] of V under\nthe metric of B, which is the number of elements in least ε-net\nof V . If B = C(Id), the space of continuous functions, we\ndenote N(ε, V ) := N(ε, V, C(Id)) for brevity. The estimate\nof covering number of shallow nets is a classical research\ntopic in approximation and learning theory [32], [17], [14],\n[30], [31]. Our purpose is to present a reﬁned estimate for\nthe covering number of deep nets to show whether there are\nadditional costs required by deep nets to embody the localized\nand sparse approximation.\nTo this end, we focus on a special subset of Dσ1,σ2,n1,n2\nwhich consists the deep nets satisfying Propositions 1 and 2.\nLet g be a deep net with two hidden layers deﬁned by\ng(x)\n=\nnd\nX\nj=1\ncjσ\n d\nX\nℓ=1\nαj,ℓσ0\n\u0010\nx(ℓ) + βj,ℓ\n\u0011\n+\nd\nX\nℓ=1\nα′\nj,ℓσ0\n\u0010\nx(ℓ) + γj,ℓ\n\u0011\n+ bj\n!\n,\nwhere cj, bj, αj,ℓ, βj,ℓ, γj,ℓ∈R. Deﬁne Φn,2d be the family\nof such deep nets whose parameters are bounded, i.e.,\nΦn,2d\n:=\n\b\ng : |cj| ≤Cn, |bj| ≤Bn, |αj,ℓ|,\n|α′\nj,ℓ| ≤Ξn, βj,ℓ, γj,ℓ∈R\n\t\n,\n(11)\n−8\n−6\n−4\n−2\n0\n2\n4\n6\n8\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n \n \nLogisitc\n(a) σ(t) =\n1\n1+e−t ,\n−8\n−6\n−4\n−2\n0\n2\n4\n6\n8\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n \n \nHyperbolic tangent sigmoidal\n(b) σ(t) = 1\n2(tanh(t) + 1)\n−8\n−6\n−4\n−2\n0\n2\n4\n6\n8\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n \n \narctan sigmoidal\n(c) σ(t) = 1\nπ arctan(t) + 1\n2\n−8\n−6\n−4\n−2\n0\n2\n4\n6\n8\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n \n \nGompertz\n(d) σ(t) = e−e−t\nFig. 4.\nFour widely used activation functions: (a) logistic function; (b)\nhyperbolic tangent function; (c) arctan sigmoidal function; (d) Gompertz\nfunction\nwhere Bn, Cn and Ξn, are positive numbers. We can see\nNn,η,Kε ∈Φn,2d ⊂Dσ0,σ,2d,nd for sufﬁcient large Bn, Cn\nand Ξn. To present the covering number of Φn,2d, we need\nthe following smoothness assumption on σ.\nAssumption 1. σ is a non-decreasing sigmoidal function\nsatisfying\n|σ(t) −σ(t′)| ≤Cσ|t −t′|.\n(12)\nAssumption 1 has already been adopted in [17, Theorem\n5.1] and [32, Lemma 2] to quantify the covering number\nof some shallow nets. It should be mentioned that there are\nnumerous functions satisfying Assumption 1, including the\nwidely used functions presented in Figure 4. With these helps,\nwe present a tight estimate for the covering number of Φn,2d\nin the following proposition, whose proof will be given in\nAppendix B.\nProposition 3. Let Φn,2d be deﬁned by (11). Under Assump-\ntion 1, there holds\nlog N(ε, Φn,2d) ≤4dnd log log 3e(2d + 1)CnCσndΞn\nε\n+\nnd log 4Bn(24e2)2d(2d + 1)6dC6d+2\nn\nΞ6d\nn C6d+1\nσ\nn6d2+2d\nε6d+2\n.\nIn [32], [17], a bound of the covering number for the set\nF := {f = σ(wx + b) : w ∈Rd, b ∈R, ∥f∥∗≤1}\nwith ∥·∥∗denoting some norm including the uniform norm and\nσ satisfying Assumption 1 was derived. It is obvious that F is\na shallow net of only one neurons. Based on this interesting\nresult, [14, Chap.16] and [31] presented a tight estimate for\nN(ε, S∗\nσ,n) as\nN(ε, S∗\nσ,n) = O\n\u0012\nnd log Γn\nε\n\u0013\n,\n(13)\n5\nwhere\nS∗\nσ,n :=\n\n\nf =\nn\nX\nj=1\ncjσ(wjx + θj) : |cj| ≤Γn, wj, θj ∈R\n\n\n\nand Γn > 0 and σ satisﬁes (1). Here, we should highlight\nthat the bounded assumption |cj| ≤Γn for the outer weights\nare necessary, without which the capacity should be inﬁnity\naccording to theory of [29], [31].\nIf Bn, Cn, Ξn and Γn are not very large, i.e., do not\ngrow exponentially with respect to n, then it follows from\nProposition 3 that\nlog N(ε, Φn,2d) = O\n\u0010\nnd log n\nε\n\u0011\n,\n(14)\nwhich is the same as (13). Comparing Φn,2d with S∗\nσ,n, we\nﬁnd that adding a layer with bounded parameters does not\nenlarge the covering number. Thus, Proposition 3 together\nwith Proposition 1 yields that deep nets can approximate more\nfunctions than shallow nets without increasing the covering\nnumber of shallow nets. Proposition 3 and Proposition 2 show\nthat deep nets can approximate sparse function better than\nshallow nets within the same price.\nIII. LEARNING RATE ANALYSIS\nIn this section, we present the ERM algorithm on deep\nnets and provide its near optimal learning rates in learning\nLipschitz functions and sparse functions in the framework of\nlearning theory [10].\nA. Algorithm and assumptions\nIn learning theory [10], samples Dm = (xi, yi)m\ni=1 are\nassumed to be drawn independently according to ρ, a Borel\nprobability measure on Z = X × Y with X\n= Id and\nY ⊆[−M, M] for some M > 0. The primary objective is\nthe regression function deﬁned by\nfρ(x) =\nZ\nY\nydρ(y|x),\nx ∈X\nwhich minimizes the generalization error\nE(f) :=\nZ\nZ\n(f(x) −y)2dρ,\nwhere ρ(y|x) denotes the conditional distribution at x induced\nby ρ. Let ρX be the marginal distribution of ρ on X and\n(L2\nρX , ∥· ∥ρ) be the Hilbert space of ρX square integrable\nfunctions on X. Then for arbitrary f ∈L2\nρX, there holds [10]\nE(f) −E(fρ) = ∥f −fρ∥2\nρ.\n(15)\nWe devote to deriving learning rate for the following ERM\nalgorithm\nfD,n := arg\nmin\nf∈Φn,2d\n1\nm\nm\nX\ni=1\n[f(xi) −yi]2,\n(16)\nwhere Φn,2d is the set of deep nets deﬁned by (11). Before\npresenting the main results, we should introduce some assump-\ntions.\nAssumption 2. We assume fρ ∈Lip(r,c0).\nAssumption 2 is the r-Lipschitz continuous condition for\nthe regression function, which is standard in learning theory\n[14], [16], [30], [10], [23], [26]. To show the advantage of\ndeep nets learning, we should add the sparseness assumption\non fρ.\nAssumption 3. We assume fρ ∈Lip(N,s,r,c0).\nAssumption 3 shows that fρ is s-sparse in N d partitions.\nThe additional sparseness assumption is natural in applications\nlike image processing [44] and computer vision [6].\nAssumption 4. There exists some constant c1 > 0 such that\n∥f∥ρ ≤c1∥f∥L2(Id).\nAssumption 4 concerns the distortion of the marginal dis-\ntribution ρX. It has been utilized in [47] and [42] to quantify\nthe learning rates of support vector machines and kernel\nlasso. It is obvious that this assumption holds for the uniform\ndistribution. If fρ is supported on S but ρX is supported on\nId\\S, it is impossible to derive a satisfactory learning rate.\nThus, Assumption 4 is important and necessary to show the\nsparseness of fρ in the spacial domain. Let M be the class of\nall Borel measures ρ on Z satisfying Assumption 2. Let Gm\nbe the set of all estimators derived from the samples Dm.\nDeﬁne\nem(Θ) :=\ninf\nfD∈Gm sup\nρ∈Θ\nE\n\b\n∥fρ −fD∥2\nρ\n\t\n.\nThen it can be found in [14, Theorem 3.2] that\nem(M) ≥˜Cm−\n2r\n2r+d , m = 1, 2, . . . ,\n(17)\nwhere ˜C is a constant depending only on c0, c1, M, r and d.\nAssumption 5. Let Ξn ≥2L, Bn ≥2d and Cn ≥M, where\nL satisﬁes\n(\n|σ(t) −1| < n−r−d \u0000 s\nN d\n\u0001 1\n2 ,\nif t ≥L,\n|σ(t)| < n−r−d \u0000 s\nN d\n\u0001 1\n2 ,\nif t ≤−L.\n(18)\nIt is obvious that L depends only on σ, s, N and n.\nAssumption 5 is technical and describes the capacity of Φn,2d.\nIt guarantees that the space Φn,2d is large enough to contain\nN ∗\nn,j,L. Furthermore, the solvability of (16) depends heavily\non the concrete values Bn, Cn and Ξn [14].\nB. Learning rate analysis\nSince |y| ≤M almost everywhere, we have |fρ(x)| ≤M.\nIt is natural for us to project an output function f : X →R\nonto the interval [−M, M] by the projection operator\nπMf(x) :=\n\n\n\nf(x),\nif −M ≤f(x) ≤M,\nM,\nif f(x) > M,\n−M,\nif f(x) < −M.\nThus, the estimate we studied in this paper is πMfD,n.\nThe main results of this paper are the following two learning\nrate estimates. In the ﬁrst one, we present the learning rate\nfor algorithm (16) when the smoothness information of the\nregression function is given.\n6\nTheorem 1. Let 0 < δ < 1 and fD,n be deﬁned by (16).\nUnder Assumptions 1, 2 and 5, if n =\nj\nm\nd\n2s+d\nk\n, then with\nconﬁdence at least 1 −δ, there holds\nE(πMfD,n) −E(fρ) ≤Cm\n−2r\n2r+d log (BnCnΞnm) log 2\nδ , (19)\nwhere C is a constant independent of δ, n or m.\nFrom Theorem 1, we can derive the following corollary,\nwhich states the near optimality of the derived learning rate\nfor πMfD,n.\nCorollary 1. Under Assumptions 1, 2 and 5, if n =\nj\nm\nd\n2r+d\nk\n,\nthen\n˜Cm−\n2r\n2r+d ≤\nmax\nfρ∈Lip(r,C0) E [E(πMfD,n) −E(fρ)]\n≤\n(2 + log 2)Cm−\n2r\n2r+d log(BnCnΞnm).\nThe proofs of Theorem 1 and Corollary 1 will be postponed\nto Appendix C. It is shown in Theorem 1 and Corollary 1\nthat implementing ERM on Φn,2d can reach the near optimal\nlearning rates (up to a logarithmic factor) provided Ξn, Cn\nand Bn are not very large. In fact, neglecting the solvability\nof algorithm (16), we can set Bn = 2d, Cn = M and\nΞn = 2L. Due to (18), the concrete value of L depends\non σ. Taking the logistic function for example, we can set\nL = (r+d) log(nN d/s). Theorem 1 and Corollary 1 yield that\nfor some easy learning task (exploring only the smoothness\ninformation of fρ), deep nets perform at least not worse than\nshallow nets and can reach the almost optimal learning rates\nfor all learning schemes.\nIn the following theorem, we show that for some difﬁcult\nlearning task (exploring sparseness and smoothness informa-\ntion of fρ), deep nets learning can break through the bottleneck\nof shallow nets learning via establishing a learning rate much\nfaster than (17).\nTheorem 2. Let 0 < δ < 1 and fD,n be deﬁned by (16).\nUnder Assumptions 1, 3, 4 and 5, if n =\nj\u0000 ms\nN d\n\u0001\nd\n2r+d k\nand\nm ≥42r+dN 2r+2d\ns\n, then with conﬁdence at least 1 −δ, there\nholds\nE(πMfD,n) −E(fρ)\n≤\nC′m−\n2r\n2r+d log (BnCnΞnm)\n\u0010 s\nN d\n\u0011\nd\n2r+d log 2\nδ , (20)\nwhere C′ is a constant independent of N, s, δ, n or m.\nSimilarly, we can obtain the following corollary, which\nexhibits the derived learning rate in expectation.\nCorollary 2. Under Assumptions 1, 3, 4 and 5, if n =\nj\u0000 ms\nN d\n\u0001\nd\n2s+d k\nand m ≥42r+dN 2r+2d\ns\n, then\nE [E(πMfD,n) −E(fρ)]\n≤\n(2 + log 2)C′m\n−2r\n2r+d log(BnCnΞnm)\n\u0010 s\nN d\n\u0011\nd\n2r+d .\nTheorem 2 and Corollary 2, whose proofs will be given in\nAppendix C, show that if the additional sparseness information\nis imposed, then ERM based on deep nets can break through\nthe optimal learning rates in (17) for shallow nets. To be\ndetailed, if fρ is 1-sparse in m\n1\n2r+2d partitions, then we can\ntake σ be the logistic function and Bn = 2d, Cn = M and\nΞn = 2(r + d) log(nN d) to get a learning rate of order\nm−\n2r\n2r+d −\nd\n2r+2d ≪m−\n2r\n2r+d . This shows the advantage of deep\nnets in learning sparse functions.\nIV. RELATED WORK AND DISCUSSIONS\nStimulated by the great success of deep learning in appli-\ncations understanding deep learning as well as its theoretical\nveriﬁcation becomes a hot topic in approximation and statisti-\ncal learning theory. Roughly speaking, the studies of deep net\napproximation can be divided into two categories: deducing\nthe limitations of shallow nets and pursuing the advantages of\ndeep nets.\nLimitations of the approximation capabilities of shallow nets\nwere ﬁrstly proposed in [4] in terms of their incapability of\nlocalized approximation. Five years later, [8] described their\nlimitations via providing lower bounds of approximation of\nsmooth functions in the minimax sense, which was recently\nhighlighted by [25] via showing that there exists a probabilistic\nmeasure, under which, all smooth functions cannot be approx-\nimated by shallow nets very well with high conﬁdence. In [1],\nBengio et al. also pointed out the limitations of some shallow\nnets in terms of the so-called “curse of dimensionality”. In\nsome recent interesting papers [20], [20], [21], limitations of\nshallow nets were presented in terms of establishing lower\nbound of approximating functions with different variation\nrestrictions.\nStudying advantages of deep nets is also a classical topic\nin neural networks approximation. It can date back to 1994,\nwhere Chui et al. [7] deduced the localized approximation\nproperty of deep nets which is far beyond the capability of\nshallow nets [4]. Recently, more and more advantages of\ndeep nets were theoretical veriﬁed in the approximation theory\ncommunity. In particular, [35] showed the power of depth of\nneural network in approximating hierarchical functions; [40]\ndemonstrated that deep nets can improve the approximation\ncapability of shallow nets when the data are located on a\nmanifold; [27] presented the necessity of deep nets in physical\nproblems which possess symmetry, locality or sparsity; [33]\nexhibited the outperformance of deep nets in approximating\nradial functions and so on. Compared with these results,\nwe focus on show the good performance of deep nets in\napproximation sparse functions in the spacial domain and also\nstudy the cost for the approximation, just as Propositions 2 and\n3 exhibited.\nIn the learning theory community, learning rates for ERM on\nshallow nets with certain activation functions were studied in\n[30]. Under Assumption 2, [30] derived a near optimal learning\nrate of order m\n−2r\n2r+d log2 m. The novelty of our Theorem 1 is\nthat we focus on learning rates of ERM on deep nets rather\nthan shallow nets, since deep nets studied in this paper can\nprovide localized approximation. Our result together with [30]\ndemonstrates that deep nets can learn more functions (such as\nthe indicator function) than shallow nets without sacriﬁcing the\ngeneralization capability of shallow nets. However, since deep\n7\nnets possess the sparse approximation property, it is stated\nin Theorem 2 that if additional a-priori information is given,\nthen deep nets can breakthrough the optimal learning rate for\nshallow nets, showing the power of depth in neural networks\nlearning. Learning rates for shallow nets equipped with a so-\ncalled complexity penalization strategy were presented in [14,\nChapter 16]. However, only variance estimate rather than the\nlearning rate were established in [14]. More importantly, their\nalgorithms and network architectures are different from our\npaper.\nIn the recent work [24], a neural network with two hidden\nlayers was developed for the learning purpose and the optimal\nlearning rates of order m\n−2r\n2r+d were presented. It should be\nnoticed that the main idea of the construction in [24] is the\nlocal average argument rather than any optimization strategy\nsuch as (16). Furthermore, [24]’s network architecture is a\nhybrid of feed-forward neural network (second hidden layer)\nand radial basis function networks (ﬁrst hidden layer). The\nconstructed network in the present paper is a standard deep\nnet possessing the same network architectures in both hidden\nlayers.\nIn our previous work [9], we constructed a deep net with\nthree hidden layers when X is in a d∗< d dimensional sub-\nmanifold and provided a learning rate of order m−\n2r\n2r+d∗. The\nconstruction in [9] were based on the local average argument\n[14]. The main difference between the present paper and [9]\nis that we used the optimization strategy in determining the\nparameters of deep nets rather than construct them directly. In\nparticular, the main tool in this paper is a reﬁned estimate for\nthe covering number.\nAnother related work is [16], which provided error analysis\nof a complexity regularization scheme whose hypothesis space\nis deep nets with two hidden layers proposed in [34]. They\nderived a learning rate of O(m−2r/(2r+D)(log m)4r/(2r+d))\nunder Assumption 2, which is the same as the rate in Theorem\n1 up to a logarithmic factor. Neglecting the algorithmic factor,\nthe main novelty of our work is that our analysis combines\nthe expressivity (localized approximation) and generalization\ncapability, while [16]’s result concerns only the generalization\ncapability. We refer the readers to [5], [7] for some advantages\nof localized approximation and sparse approximation in the\nspacial domain.\nTo ﬁnalize the discussion, we mention that the present paper\nonly compares deep nets with two hidden layers with shallow\nnets and demonstrates the advantage of the former architecture\nfrom approximation learning theory viewpoints. As far as the\noptimal learning rate is concerned, to theoretically provide the\npower of depth, more restrictions on the regression function\nshould be imposed. For example, shallow nets are capable\nof exploring the smoothness information [30], deep nets with\ntwo hidden layers can tackle both sparseness and smoothness\ninformation (Theorem 2 in this paper), and deep nets with\nmore hidden layers succeed in handling sparseness informa-\ntion, smoothness information and manifold features of the\ninput space (combining Theorem 2 in this paper with Theorem\n1 in [9]). In a word, deep nets with more hidden layers can\nembody more information for the learning task. It is interesting\nto study the power of depth along such ﬂavor and determine\nwhich information can (or cannot) be explored by deepening\nthe networks.\nV. CONCLUSION\nIn this paper, we analyzed the expressivity and generaliza-\ntion of deep nets. Our results showed that without essentially\nenlarging the capacity of shallow nets, deep nets possess\nexcellent expressive power in terms of providing localized\napproximation and sparse approximation. Consequently, we\nproved that for some difﬁcult learning tasks (exploring both\nsparsity and smoothness), deep nets could break though the\noptimal learning rates established for shallow nets. All these\nresults showed the power of depth from the learning theory\nviewpoint.\nAPPENDIX A: PROOFS OF PROPOSITIONS 1 AND 2\nIn this Appendix, we present the proofs of Propositions 1\nand 2. The basic idea of our proof was motivated by [7] and\nthe property (1) of sigmoidal functions.\nProof of Proposition 1: When x /∈An,j, there exists an\nℓ0 such that |x(ℓ0) −ξ(ℓ0)\nj\n| >\n1\n2n. If x(ℓ0) −ξ(ℓ0)\nj\n< −1/(2n),\nthen\n1/(2n) + x(ℓ0) −ξ(ℓ0)\nj\n< 0.\nIf x(ℓ0) −ξ(ℓ0)\nj\n> 1/(2n), then\n1/(2n) −x(ℓ0) + ξ(ℓ0)\nj\n< 0.\nThe above assertions together with the deﬁnition of σ0 yield\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n + x(ℓ) −ξ(ℓ)\nj\n\u0015\n+\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n −x(ℓ) + ξ(ℓ)\nj\n\u0015\n< 2d−1.\nThus,\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n + x(ℓ) −ξ(ℓ)\nj\n\u0015\n+\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n −x(ℓ) + ξ(ℓ)\nj\n\u0015\n−2d + 1/2 < −1/2,\nwhich together with (1) and (2) yields\n|N ∗\nn,j,Kε(x)| < ε.\nThis ﬁnishes the proof of part (a). We turn to prove assertion\n(b) in Proposition 1. Since x ∈An,j, for all 1 ≤ℓ≤d, there\nholds |x(ℓ) −ξ(ℓ)\nj | ≤\n1\n2n. Thus, for all ξ ∈An,j, there holds\n1\n2n ± (x(ℓ) −ξ(ℓ)\nj ) ≥0.\nIt follows from the deﬁnition of σ0 that\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n + x(ℓ) −ξ(ℓ)\nj\n\u0015\n+\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n −x(ℓ) + ξ(ℓ)\nj\n\u0015\n= 2d.\nThat is,\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n + x(ℓ) −ξ(ℓ)\nj\n\u0015\n+\nd\nX\nℓ=1\nσ0\n\u0014 1\n2n −x(ℓ) + ξ(ℓ)\nj\n\u0015\n−2d + 1/2 = 1/2.\n8\nHence, (1) implies\n|N ∗\nn,j,Kε(x) −1| < ε.\nSince σ is non-decreasing, we have N ∗\nn,j,K(x) ≤1 for all\nx ∈[0, 1]d. The proof of Proposition 1 is ﬁnished.\nProof of Proposition 2: Since Id = S\nj∈Ndn An,j, for each\nx ∈Id, there exists a jx such that x ∈An,jx. Here, if x lies\non the boundary of some An,j, we denote by jx an arbitrary\nbut ﬁxed j satisfying An,j ∋x. Then, it follows from (5) that\nf(x) −Nn,η,Kε(x)\n=\nf(x) −f(ηjx) −\nX\nj̸=jx\nf(ηj)N ∗\nn,j,Kε(x)\n+\nf(ηjx)[1 −N ∗\nn,jx,Kε(x)].\n(21)\nWe get from (21), (6), x, ηjx ∈An,jx and Proposition 1 that\n|f(x) −Nn,η,Kε(x)|\n≤\nc0∥x −ηjx∥r + (nd −1)∥f∥L∞(Id)ε + ∥f∥L∞(Id)ε\n≤\n2r/2c0n−r + nd∥f∥L∞(Id)ε.\nThis proves (8). If x /∈S\nk∈Λs Λk, then An,jx ∩S = ∅.\nThus, for arbitrary ηjx satisfying ηjx ∈An,jx, we have from\nfρ ∈Lip(N,s,r,c0), Proposition 1 and (5) that\n|Nn,η,Kε(x)|\n=\nX\nj̸=jx\nf(ηj)N ∗\nn,j,Kε(x) + f(ηjx)N ∗\nn,jx,Kε(x)\n≤\n∥f∥L∞(Id)\nX\nj̸=jx\nN ∗\nn,j,Kε(x) ≤∥f∥L∞(Id)ndε.\nThis proves (9) and completes the proof of Proposition 2.\nAPPENDIX B: PROOFS OF PROPOSITION 3\nThe aim of this appendix is to prove Proposition 3. Our\nmain idea is to decouple different hidden layers by using\nAssumption 1 and the deﬁnition of the covering number. For\nthis purpose, we need the following ﬁve lemmas. the ﬁrst two\ncan be found in [14, Lemma 16.3] and [14, Theorem 9.5],\nrespectively. The third one can be easily deuced from [14,\nLemma 9.2], [14, Theorem 9.4] with p = 1 and the fact\nN(ε, F) ≤N(ε, F, L1(X)). The last two are well known,\nand we present their proofs for the sake of completeness.\nLemma 1. Let F be a family of real functions and let h :\nR →R be a ﬁxed nondecreasing function. Deﬁne the class\nG = {h ◦f : f ∈F}. Then\nVG+ ≤VF +,\nwhere\nH+ :=\n\b\n{(z, t) ∈Rd × R; t ≤h(z)} : h ∈H\n\t\nfor some set of functions H and VU denotes the VC dimension\n[14] of the set U over X.\nLemma 2. Let G be an r-dimensional vector space of real\nfunctions on Rd, and set\nA = {{z : g(z) ≥0} : g ∈G} .\nThen\nVA ≤r.\nLemma 3. Let F be a class of functions f : Rd →[0, M ∗]\nwith VF + ≥2. Let 0 < ε < M ∗/4, we have\nN(ε, F) ≤3\n\u00122eM ∗\nε\nlog 3eM ∗\nε\n\u0013V +\nF\n.\nLemma 4. Let F and G be two families of real functions. If\nF ⊕G denotes the set of functions {f + g : f ∈F, g ∈G},\nthen for any ε, ν > 0, we have\nN(ε + ν, F ⊕G) ≤N(ε, F)N(ν, G).\nProof: Let {f1, . . . , fN} and {g1, . . . , gL} be an ε-cover\nand a ν-cover of F and G, respectively. Then, for every f ∈F\nand g ∈G, there exist k ∈{1, . . . , N} and ℓ∈{1, . . . , L}\nsuch that\n∥f −fk∥∞< ε,\n∥g −gℓ∥∞< ν.\nDue to the triangle inequality, we have\n∥f + g −fk −gℓ∥∞≤∥f −fk∥∞+ ∥g −gℓ∥∞≤ε + ν,\nwhich shows that {fk + gℓ: 1 ≤k ≤N, 1 ≤ℓ≤L} is an\n(ε + ν)-cover of F ⊕G. The deﬁnition of covering number\nthen yields\nN(ε + ν, F ⊕G) ≤N(ε, F)N(ν, G).\nThis ﬁnishes the proof of Lemma 4.\nLemma 5. Let F and G be two families of real functions\nuniformly bounded by M1 and M2 respectively. If F ⊙G\ndenotes the set of functions {f · g : f ∈F, g ∈G}, then\nfor any ε, ν > 0, we have\nN(ε + ν, F ⊙G) ≤N(ε/M2, F)N(ν/M1, G).\nProof: Let {f1, . . . , fN} and {g1, . . . , gL} be an ε/M2-\ncover and a ν/M1-cover of F and G, respectively. Then, for\nevery f ∈F and g ∈G, there exist k ∈{1, . . . , N} and\nℓ∈{1, . . ., L} such that ∥fk∥∞≤M1, ∥gℓ∥∞≤M2, and\n∥f −fk∥∞< ε/M2,\n∥g −gℓ∥∞< ν/M1.\nIt then follows from the triangle inequality that\n∥fg −fkgℓ∥∞≤∥fg −fgℓ∥∞+ ∥fgℓ−fkgℓ∥∞\n≤\nM1∥g −gℓ∥∞+ ∥gℓ∥∥f −fk∥∞≤ν + ε,\nwhich implies that {fkgℓ: 1 ≤k ≤N, 1 ≤ℓ≤L} is an\n(ε + ν)-cover of F ⊙G. This together with the deﬁnition of\ncovering number ﬁnishes the proof of Lemma 5.\nBy the help of previous lemmas, we are in a position to\nprove Proposition 3.\nProof of Proposition 3: According to Lemma 4, we have\nN(ε, Φn,2d) ≤\n\u0012\nmax\n1≤j≤nd N(ε/nd, G1,j)\n\u0013nd\n,\n(22)\nwhere\nG1,j :=\n\b\ngj\n:\n|cj| ≤Cn, |bj| ≤Bn, |αj,ℓ|,\n|α′\nj,ℓ| ≤Ξn, βj,ℓ, γj,ℓ∈R\n\t\n,\n9\nand\ngj(x)\n:=\ncjσ\n d\nX\nℓ=1\nαj,ℓσ0\n\u0010\nx(ℓ) + βj,ℓ\n\u0011\n+\nd\nX\nℓ=1\nα′\nj,ℓσ0\n\u0010\nx(ℓ) + γj,ℓ\n\u0011\n+ bj\n!\n.\nSince |cj| ≤Cn for all 1 ≤j ≤nd and ∥σ∥∞≤1, we obtain\nfrom Lemma 5 that for arbitrary 1 ≤j ≤nd, there holds\nN(ε/nd, G1,j) ≤N(ε/nd, {cj : |cj| ≤Cn})N(ε/(Cnnd), G2,j),\n(23)\nwhere\nG2,j :=\n\b\nhj : |bj| ≤Bn, |αj,ℓ|, |α′\nj,ℓ| ≤Ξn, βj,ℓ, γj,ℓ∈R\n\t\n,\nand\nhj(x)\n:=\nσ\n d\nX\nℓ=1\nαj,ℓσ0\n\u0010\nx(ℓ) + βj,ℓ\n\u0011\n+\nd\nX\nℓ=1\nα′\nj,ℓσ0\n\u0010\nx(ℓ) + γj,ℓ\n\u0011\n+ bj\n!\n.\nFrom the deﬁnition of the covering number, we can deduce\nN(ε/nd, {cj : |cj| ≤Cn}) ≤2Cn\nε/nd = 2Cnnd\nε\n.\n(24)\nDue to (12), we get\n∥σ(f1(·)) −σ(f2(·))∥∞≤Cσ∥f1 −f2∥∞,\nwhich implies\nN(ε/(Cnnd), G2,j) ≤N(ε/(CσCnnd), G3,j),\n(25)\nwhere\nG3,j :=\n\b\npj : |bj| ≤Bn, |αj,ℓ|, |α′\nj,ℓ| ≤Ξn, βj,ℓ, γj,ℓ∈R\n\t\n,\nand\npj :=\nd\nX\nℓ=1\nαj,ℓσ0\n\u0010\nx(ℓ) + βj,ℓ\n\u0011\n+\nd\nX\nℓ=1\nα′\nj,ℓσ0\n\u0010\nx(ℓ) + γj,ℓ\n\u0011\n+bj.\nLemma 4 then implies\nN\n\u0012\nε\nCσCnnd , G3,j\n\u0013\n≤\nN\n\u0012\nε\n(2d + 1)CσCnnd , {bj : |bj| ≤Bn}\n\u0013\n×\n\u0014\nmax\n1≤ℓ≤d N\n\u0012\nε\n(2d + 1)CσCnnd , G4,j,ℓ\n\u0013\u0015d\n×\n\u0014\nmax\n1≤ℓ≤d N\n\u0012\nε\n(2d + 1)CσCnnd , G′\n4,j,ℓ\n\u0013\u0015d\n,\n(26)\nwhere\nG4,j,ℓ:=\nn\nαj,ℓσ0\n\u0010\nx(ℓ) + βj,ℓ\n\u0011\n: |αj,ℓ| ≤Ξn, γj,ℓ∈R\no\n,\nand\nG′\n4,j,ℓ:=\nn\nα′\nj,ℓσ0\n\u0010\nx(ℓ) + γj,ℓ\n\u0011\n: |α′\nj,ℓ| ≤Ξn, βj,ℓ∈R\no\n.\nFrom the deﬁnition of the covering number again, we can\ndeduce\nN\n\u0012\nε\n(2d + 1)CσCnnd , {bj : |bj| ≤Bn}\n\u0013\n≤\n2Bn(2d + 1)CσCnnd\nε\n.\n(27)\nFurthermore, it follows from Lemma 5 and |αj,ℓ|, |α′\nj,ℓ| ≤Ξn,\n∥σ0∥∞≤1 that\nN\n\u0012\nε\n(2d + 1)CσCnnd , G4,j,ℓ\n\u0013\n≤\nN\n\u0012\nε\n(2d + 1)CσCnnd , {αj,ℓ: |αj,ℓ| ≤Ξn}\n\u0013\n×\nN\n\u0012\nε\n(2d + 1)CnCσndΞn\n, G5,j,ℓ\n\u0013\n,\n(28)\nand\nN\n\u0012\nε\n(2d + 1)CσCnnd , G′\n4,j,ℓ\n\u0013\n≤\nN\n\u0012\nε\n(2d + 1)CσCnnd , {αj,ℓ: |αj,ℓ| ≤Ξn}\n\u0013\n×\nN\n\u0012\nε\n(2d + 1)CnCσndΞn\n, G′\n5,j,ℓ\n\u0013\n,\n(29)\nwhere\nG5,j,ℓ:=\nn\nσ0\n\u0010\nx(ℓ) + βj,ℓ\n\u0011\n: βj,ℓ∈R\no\n,\nand\nG′\n5,j,ℓ:=\nn\nσ0\n\u0010\nx(ℓ) + γj,ℓ\n\u0011\n: γj,ℓ∈R\no\n.\nSimilarly, it is easy to see\nN\n\u0012\nε\n(2d + 1)CσCnndΞn\n, {αj,ℓ: |αj,ℓ| ≤Ξn}\n\u0013\n≤\n2Ξn(2d + 1)CσCnnd\nε\n.\n(30)\nTo bound the covering numbers of G5,j,ℓand G′\n5,j,ℓ, we notice\nthat σ0 is a non-decreasing function. Then, it follows from\nLemma 1 that VG+\n5,j,ℓ≤VG+\n6,j,ℓ, where\nG6,j,ℓ:=\nn\nx(ℓ) + βj,ℓ: βj,ℓ∈R\no\n.\nNoting G6,j,ℓis in a one-dimensional linear space, the deﬁni-\ntion of G+\n6,j,ℓimplies\nG+\n6,j,ℓ⊆{{(z, t) ∈R × R : αt + g(z) ≥0} : g ∈G6,j,ℓ, α ∈R}\nand thus G+\n6,j,ℓis in a two-dimensional linear space. Therefore,\nit follows from Lemma 2 that VG+\n6,j,ℓ≤2, which implies\nVG+\n5,j,ℓ≤2. Therefore, it follows from Lemma 3 with M ∗= 1\nthat\nN\n\u0012\nε\n(2d + 1)CnCσndΞn\n, G5,j,ℓ\n\u0013\n(31)\n≤\n3\n\u00122e(2d + 1)CnCσndΞn\nε\nlog 3e(2d + 1)CnCσndΞn\nε\n\u00132\n.\n10\nThe same method also yields\nN\n\u0012\nε\n(2d + 1)CnCσndΞn\n, G′\n5,j,ℓ\n\u0013\n(32)\n≤\n3\n\u00122e(2d + 1)CnCσndΞn\nε\nlog 3e(2d + 1)CnCσndΞn\nε\n\u00132\n.\nPlugging (31) and (30) into (28) and inserting (32) and (30)\ninto (29), we obtain\nN\n\u0012\nε\n(2d + 1)CσCnnd , G4,j,ℓ\n\u0013\n≤\n6Ξn(2d + 1)CσCnnd\nε\n×\n\u00122eCnCσndΞn\nε\nlog 3e(2d + 1)CnCσndΞn\nε\n\u00132\n=\n24e2(2d + 1)3C3\nnC3\nσn3dΞ3\nn\nε3\n×\n\u0012\nlog 3e(2d + 1)CnCσndΞn\nε\n\u00132\nand\nN\n\u0012\nε\n(2d + 1)CσCnnd , G′\n4,j,ℓ\n\u0013\n≤\n24e2(2d + 1)3C3\nnC3\nσn3dΞ3\nn\nε3\n×\n\u0012\nlog 3e(2d + 1)CnCσndΞn\nε\n\u00132\n.\nInserting the above two estimates and (27) into (26), we then\nget\nN\n\u0012\nε\nCσCnnd , G3,j\n\u0013\n≤2Bn(2d + 1)CσCnnd\nε\n×\n\u001424e2(2d + 1)3C3\nnC3\nσn3dΞ3\nn\nε3\n×\n\u0012\nlog 3e(2d + 1)CnCσndΞn\nε\n\u00132#2d\n.\nThis together with (25), (24) and (23) yields\nN\n\u0010 ε\nnd , G1,j\n\u0011\n≤\n\u0012\nlog 3e(2d + 1)CnCσndΞn\nε\n\u00134d\n4Bn(24e2)2d(2d + 1)6d+1C6d+2\nn\nΞ6d\nn C6d+1\nσ\nn6d2+2d\nε6d+2\nPlugging the above inequality into (22), we get\nlog N(ε, Φn,2d) ≤4dnd log log 3e(2d + 1)CnCσndΞn\nε\n+\nnd log 4Bn(24e2)2d(2d + 1)6dC6d+2\nn\nΞ6d\nn C6d+1\nσ\nn6d2+2d\nε6d+2\n.\nThis ﬁnishes the proof of Proposition 3.\nAPPENDIX C: DERIVING LEARNING RATES\nIn this appendix, we aim at proving results in Section III.\nOur main idea is motivated by the classical error decompo-\nsition strategy proposed [45] that divides the generalization\nerror into the approximation error and sample error. The\napproximation error can be estimated by using Propositions 1\nand 2, while the sample error is estimated by using Proposition\n3 and some concentration inequality in statistics.\nA. Error decomposition\nDeﬁne\nNn,2d,L(x) =\nX\nj∈Ndn\nfρ(ξj)N ∗\nn,j,L(x)\n(33)\nwith L being deﬁned by (18). Since |yi| ≤M almost surely, it\nfollows from Assumption 5 that Nn,2d,L ∈Φn,2d. The follow\nlemma presents the error decomposition for our analysis.\nLemma 6. Let fD,n and Nn,2d,L be deﬁned by (16) and (33),\nrespectively. Then, we have\nE(πMfD,n) −E(fρ) ≤E(Nn,2d,L) −E(fρ)\n+\nE(πMfD,n) −ED(πMfD,n)\n+\nED(Nn,2d,L) −E(Nn,2d,L),\nwhere ED(f) = 1\nm\nPm\ni=1(f(xi) −yi)2.\nProof: It is obvious that\nE(πMfD,n) −E(fρ) ≤E(Nn,2d,L) −E(fρ)\n+\nE(πMfD,n) −ED(πMfD,n) + ED(Nn,2d,L)\n−\nE(Nn,2d,L) + ED(πMfD,n) −ED(Nn,2d,L).\nDue to the deﬁnition of πM, it follows from (16) and\nNn,2d,L ∈Φn,2d that\nED(πMfD,n)−ED(Nn,2d,L) ≤ED(fD,n)−ED(Nn,2d,L) ≤0.\nThis ﬁnishes the proof of Lemma 6.\nSetting Dn := E(Nn,2d,L) −E(fρ), S1 := S1,n,D :=\nED(Nn,2d,L)−E(Nn,2d,L) and S2 := S2,n,D := E(πMfD,n)−\nED(πMfD,n), we get from Lemma 6 that\nE(πMfD,n) −E(fρ) ≤Dn + S1 + S2.\n(34)\nB. Approximation error estimate\nThe main tool to present the approximation error estimate\nis Proposition 2. Indeed, we can deduce the following tight\nbounds for Dn.\nProposition 4. Under Assumptions 1, 2, 5, there holds\nDn ≤(2rc2\n0 + M 2)n−2r.\n(35)\nUnder Assumptions 1, 3, 4, 5, there holds\nDn ≤(c12r+dc2\n0 + (1 + c1)M 2)n−2r s\nN d .\n(36)\nProof: Due to (15) and ∥· ∥ρ ≤∥· ∥L∞(I)d, we have\nDn = ∥fρ −Nn,2d,L∥2\nρ ≤∥fρ −Nn,2d,L∥2\nL∞(I)d.\nThen, it follows from ∥fρ∥L∞(I)d\n≤M, (8) with η =\n{ξj}j∈Ndn, Kε = L and ε = n−r−d that\nDn ≤(2rc2\n0 + M 2)n−2r,\nwhich proves (35).\n11\nNow, we turn to bound (36). It is easy to check that\nDn =\nZ\nX\n|fρ(x) −Nn,2d,L(x)|2dρX\n≤\nX\nk∈Λs\nX\nj∈Λk\nZ\nAn,j\n|fρ(x) −Nn,2d,L(x)|2dρX\n+\nX\nk∈Λs\nX\nj/∈Λk\nZ\nAn,j\n|fρ(x) −Nn,2d,L(x)|2dρX\n=:\nJ1 + J2.\n(37)\nFrom (10) and (8) Assumption 4 and Assumption 5, we get\nJ1\n≤\n(2rc2\n0 + M 2)n−2r X\nk∈Λs\nX\nj/∈Λk\nZ\nAn,j\ndρX\n≤\nc1(2r+dc2\n0 + M 2)n−2r s\nN d .\nSince n ≥4N, we get from Assumption 4, Assumption (9)\nwith ε = that\nJ2 ≤M 2n2dε2 ≤M 2n−2r s\nN d .\nPlugging the above two estimates into (37), we get\nDn ≤(c12r+dc2\n0 + (1 + c1)M 2)n−2r s\nN d .\nThis completes the proof of Proposition 4.\nC. Sample error estimate\nTo bound S1, we need the following two Lemmas. The ﬁrst\nis the Bernstein inequality, which was proved in [41].\nLemma 7. Let ξ be a random variable on a probability space\nZ with variance σ2 satisfying |ξ−Eξ| ≤Mξ for some constant\nMξ. Then for any 0 < δ < 1, with conﬁdence 1 −δ, we have\n1\nm\nm\nX\ni=1\nξ(zi) −Eξ ≤2Mξ log 1\nδ\n3m\n+\ns\n2σ2 log 1\nδ\nm\n.\nThe second lemma presents a bound for the summation of\nN ∗\nn,j,L.\nLemma 8. Let N ∗\nn,j,L be deﬁned by (2) with L satisfying (18).\nUnder Assumption 1, there holds\nX\nj∈Ndn\n\f\fN ∗\nn,j,L(x)\n\f\f ≤2d + 1,\n∀x ∈[0, 1]d.\nProof: Due to the deﬁnition of An,j, we have [0, 1]d =\nS\nj∈Ndn An,j. Furthermore, it is easy to see that for arbitrary\nx ∈[0, 1]d, there are at most 2d j’s denoted by j1, . . . , j2d\nsuch that x ∈An,jk, k = 1, . . . , 2d. Then it follows from\nProposition 1 that\nX\nj∈Nd\nn\n\f\fN ∗\nn,j,L(x)\n\f\f =\n2d\nX\nk=1\n\f\fN ∗\nn,jk,L(x)\n\f\f\n+\nX\nj̸=j1,...,j2d\n\f\fN ∗\nn,j,L(x)\n\f\f ≤2d + ndn−s−d ≤2d + 1.\nThis ﬁnishes the proof of Lemmas 8.\nBy the help of the above lemma, we obtain the following\nProposition 5.\nProposition 5. For any 0 < δ < 1, with conﬁdence 1 −δ\n2,\nS1 ≤7M 2(2d + 4)2 log 2\nδ\n3m\n+ 1\n2Dn.\nProof: Let the random variable ξ on Z be deﬁned by\nξ(z) = (y −Nn,2d,L(x))2 −(y −fρ(x))2\nz = (x, y) ∈Z.\nSince |fρ(x)| ≤M almost everywhere, it follows from Lemma\n8 that\n|ξ(z)|\n=\n|(fρ(x) −Nn,2d,L(x))(2y −Nn,2d(x) −fρ(x))|\n≤\nM 2(2d + 2)(2d + 4) ≤Mξ := M 2(2d + 4)2\nand almost surely\n|ξ −Eξ| ≤2Mξ.\nMoreover, we have\nE(ξ2)\n=\nZ\nZ\n(Nn,2d,L(x) + fρ(x) −2y)2(Nn,2d,L −fρ(x))2dρ\n≤\nMξ∥fρ −Nn,2d,L∥2\nρ,\nwhich implies that the variance σ2 of ξ can be bounded\nas σ2 ≤E(ξ2) ≤MξDn. Now applying Lemma 7, with\nconﬁdence 1 −δ\n2, we have\nS1\n=\n1\nm\nm\nX\ni=1\nξ(zi) −Eξ ≤4Mξ log 2\nδ\n3m\n+\ns\n2MξD(n) log 2\nδ\nm\n≤\n7M 2(2d + 4)2 log 2\nδ\n3m\n+ 1\n2Dn.\nThis ﬁnishes the proof of Proposition 5.\nTo bound S2, we need the following ratio probability\ninequality which is a standard result in learning theory [45].\nLemma 9. Let G be a set of functions on Z such that, for some\nc ≥0, |g−E(g)| ≤B0 almost everywhere and E(g2) ≤cE(g)\nfor each g ∈G. Then, for every ε > 0,\nP\n(\nsup\nf∈G\nE(g) −1\nm\nPm\ni=1 g(zi)\np\nE(g) + ε\n≥√ε\n)\n≤\nN(ε, G) exp\n(\n−\nmε\n2c + 2B0\n3\n)\n.\nUsing the above lemma and Proposition 3, we can deduce\nthe following estimate for S2.\nProposition 6. Let 0 < δ < 1. With conﬁdence at least 1 −δ\n2,\nthere holds\nS2 ≤1\n2[E(πMfD,n) −E(fρ)] + m\n−2s\n2s+d 428(6d + 2)M 2\n×\nlog\n\u0002\n192e2(2d + 1)MCσBnCnΞnm\n\u0003\nlog 2\nδ .\nProof: Set\nFn := {(πMf(x) −y)2 −(fρ(x) −y)2 : f ∈Φn,2d}.\n12\nThen for g ∈Fn, there exists f ∈Φn,2d such that g(z) =\n(πMf(x) −y)2 −(fρ(x) −y)2. Therefore,\nE(g) = E(πMf) −E(fρ) ≥0,\nand\n1\nm\nm\nX\ni=1\ng(zi) = ED(πMf) −ED(fρ).\nSince |πMf| ≤M and |fρ(x)| ≤M almost everywhere, we\nﬁnd that\n|g(z)| = |(πMf(x)−fρ(x))((πM f(x)−y)+(fρ(x)−y))| ≤8M 2,\nwhich together with (15) follows |g(z) −E(g)| ≤16M 2\nalmost everywhere and\nE(g2) ≤16M 2∥πMf −fρ∥2\nL2ρ = 16M 2E(g).\nNow we apply Lemma 9 with B0 = c = 16M 2 to the set of\nfunctions Fn and obtain that\nsup\nf∈Φn,2d\n{E(πMf) −E(fρ)} −{ED(πMf) −ED(fρ)}\np\n{E(πMf) −E(fρ)} + ε\n≤√ε (38)\nwith conﬁdence at least\n1 −N(ε, Fn)exp\n\u001a\n−3mε\n128M 2\n\u001b\n.\nObserve that for g1, g2 ∈Fn there exist f1, f2 ∈Φn,2d such\nthat\ngj(z) = (πMfj(x) −y)2 −(fρ(x) −y)2, j = 1, 2.\nThen\n|g1(z) −g2(z)| = |(πMf1(x) −y)2 −(πMf2(x) −y)2|\n≤\n4M∥πMf1 −πMf2∥∞≤4M∥f1 −f2∥∞.\nWe see that for any ε > 0, an\n\u0000 ε\n4M\n\u0001\n-covering of Φn,2d\nprovides an ε-covering of Fn. Therefore\nN(ε, Fn) ≤N\n\u0010 ε\n4M , Φn,2d\n\u0011\n.\nThen the conﬁdence is\n1 −N(ε, Fn) exp\n\u001a\n−3mε\n128M 2\n\u001b\n≥\n1 −N\n\u0010 ε\n4M , Φn,2d\n\u0011\nexp\n\u001a\n−3mε\n128M 2\n\u001b\n.\nAccording to Proposition 3, we have\nlog N(ε/4M, Φn,2d)\n≤\n4dnd log log 12Me(2d + 1)CσCnndΞn\nε\n+\n(6d + 2)nd log\n\u0002M(4Bn)\n1\n6d+2 (24e2)\n2d\n6d+2 (2d + 1)\n6d\n6d+2\nε\n×\nCnΞ\n6d\n6d+2\nn\nC\n6d+1\n6d+2\nσ\nnd\u0003\n.\nThus it follows from the above estimate and (38) that, with\nconﬁdence at least\n1 −exp\n\b\n4dnd log log 12Me(2d + 1)CσCnndΞn\nε\n−\n3mε\n128M 2 + (6d + 2)nd log\n\u0002M(4Bn)\n1\n6d+2 (24e2)\n2d\n6d+2\nε\n×\n(2d + 1)\n6d\n6d+2 CnΞ\n6d\n6d+2\nn\nC\n6d+1\n6d+2\nσ\nnd\u0003\t\n(39)\nthere holds\n{E(πMfD,n) −E(fρ)} −{ED(πMfD,n) −ED(fρ)}\np\n{E(πMfD,n) −E(fρ)} + ε\n≤\nsup\nf∈Φn,2d\n{E(πMf) −E(fρ)} −{ED(πMf) −ED(fρ)}\np\n{E(πMf) −E(fρ)} + ε\n≤\n√ε.\nThat is,\nS2 ≤1\n2[E(πMfD,n) −E(fρ)] + ε.\n(40)\nDeﬁne\nh(η) := 4dnd log log 12Me(2d + 1)CσCnndΞn\nε\n−\n3mε\n128M 2 + (6d + 2)nd log\n\u0002M(4Bn)\n1\n6d+2 (24e2)\n2d\n6d+2\nε\n×\n(2d + 1)\n6d\n6d+2 CnΞ\n6d\n6d+2\nn\nC\n6d+1\n6d+2\nσ\nnd\u0003\n.\nChoose η∗to be the positive solution to the equation\nh(η) = log δ\n2.\nThe function h : R+ →R is strictly decreasing. Hence η∗≤η\nif h(η) ≤h(η∗) = log δ\n2. Let n =\nj\nm\n1\n2s+d\nk\n. For arbitrary\nη ≥m−2s/(2s+d), we have\nh(η) ≤4dm\nd\n2s+d log log [12Me(2d + 1)CσCnmΞn]\n−\n3mη\n128M 2 + m\nd\n2s+d (6d + 2) log\n\u0014\nM(4Bn)\n1\n6d+2 (24e2)\n2d\n6d+2 (2d + 1)\n6d\n6d+2 CnΞ\n6d\n6d+2\nn\nC\n6d+1\n6d+2\nσ\nm\n\u0015\n.\nTake η1 to be a positive number satisfying\nlog δ\n2 = 4dm\nd\n2s+d log log [12Me(2d + 1)CσCnmΞn]\n−\n3mη\n128M 2 + m\nd\n2s+d (6d + 2) log\n\u0002\nM(4Bn)\n1\n6d+2\n(24e2)\n2d\n6d+2 (2d + 1)\n6d\n6d+2 CnΞ\n6d\n6d+2\nn\nC\n6d+1\n6d+2\nσ\nm\n\u0003\n.\nThen we have h(η1) ≤h(η∗) = log δ\n2, provided η1 ≥\nm−2s/(2r+s). Direct computation yields\nη1 = 512dM 2\n3\nm\n−2s\n2r+s log log [12Me(2d + 1)CσCnmΞn]\n+\n128M 2\n3m\nlog 2\nδ + 128M 2\n3\nm\n−2s\n2r+s (6d + 2) log\n\u0002\nM\n(4Bn)\n1\n6d+2 (24e2)\n2d\n6d+2 (2d + 1)\n6d\n6d+2 CnΞ\n6d\n6d+2\nn\nC\n6d+1\n6d+2\nσ\nm\n\u0003\n.\nIt is obvious that η1 ≥m−2s/(2s+d). Then we obtain\nη∗≤η1 ≤43M 2m−1 log 2\nδ + m\n−2s\n2s+d 214(6d + 2)M 2\n×\nlog\n\u0002\n192e2(2d + 1)MCσBnCnΞnm\n\u0003\n.\nHence, it follows from (39) and (40) that with conﬁdence at\nleast 1 −δ\n2, there holds\nS2 ≤1\n2[E(πMfD,n) −E(fρ)] + m\n−2s\n2s+d 428(6d + 2)M 2\n×\nlog\n\u0002\n192e2(2d + 1)MCσBnCnΞnm\n\u0003\nlog 2\nδ .\nThis ﬁnishes the proof of Proposition 6.\n13\nD. Learning rate analysis\nIn this part, we prove results in Section III by using the error\ndecomposition, approximation error estimate and sample error\nestimate presented in the previous three subsections.\nProof of Theorem 1: Due to (34) and Proposition 6, there\nexists a subset Zm\nδ,1 of Zm with measure at least 1−δ/2 such\nthat for every Dm ∈Zm\n1,δ, there holds\nE(πMfD,n) −E(fρ) ≤2Dn + 2S1 + 856(6d + 2)M 2\n×\nlog\n\u0002\n192e2(2d + 1)MCσBnCnΞnm\n\u0003\nm\n−2s\n2s+d log 2\nδ .(41)\nFurthermore, it follows from Proposition 5 that there exists a\nsubset Zm\nδ,2 of Zm with measure at least 1−δ/2 such that for\nevery Dm ∈Zm\n2,δ, there holds\n2S1 ≤14M 2(2d + 4)2 log 2\nδ\n3m\n+ D(n).\n(42)\nPlugging the above estimate and (35) into (41), and noting\nn = ⌊m1/(2s+d)⌋, we have for every Dm ∈Zm\nδ,1 ∩Zm\nδ,2, there\nholds\nE(πMfD,n) −E(fρ) ≤2(2rc2\n0 + M 2)n−2rm−\n2r\n2r+d\n+\n14M 2(2d + 4)2 log 2\nδ\n3m\n+ 856(6d + 2)M 2m\n−2r\n2r+d log 2\nδ\n×\nlog\n\u0002\n192e2(2d + 1)MCσBnCnΞnm\n\u0003\n.\nHence, with conﬁdence at least 1 −δ, there holds\nE(πMfD,n) −E(fρ) ≤C log [BnCnΞnm] m\n−2s\n2s+d log 2\nδ ,\nwhere\nC\n:=\n2(2rc2\n0 + M 2) + 14M 2(2d + 4)2\n3\n+\n856(12d + 4)M 2 log(192e2Cσ).\nThis ﬁnishes the proof of Theorem 1.\nProof of Corollary 1: From the conﬁdence-based error\nbound (19), we obtain that the nonnegative random variable\nξ = E(πMfD,n) −E(fρ) satisﬁes\nP [ξ > t] ≤2 exp\nn\n−C−1tm\n2s\n2s+d log−1(BnCnΞnm)\no\nfor any t ≥C log 2m\n−2s\n2s+d log [BnCnΞnm] . Applying this\nbound to the formula\nE[ξ] =\nZ ∞\n0\nP[ξ > t]dt\nfor nonnegative random variables, we obtain\nE [E(πMfD,n) −E(fρ)] ≤Cm\n−2s\n2s+d log [BnCnΞnm] log 2\n+2\nZ ∞\n0\nexp\nn\n−C−1tm\n2s\n2s+d log−1(BnCnΞnm)\no\ndt.\nBy a change of variable, we see that the above integration\nequals\nCm\n−2s\n2s+d log(BnCnΞnΛnm)\nZ ∞\n0\nexp {−u} du\n=\nCm\n−2s\n2s+d log(BnCnΞnm).\nHence\nE [E(πMfD,n) −E(fρ)] ≤(2 + log 2) Cm\n−2s\n2s+d log(BnCnΞnm).\nThis together with (17) completes the proof of Corollary 1.\nProof of Theorem 2: Plugging (41), (42) and (36) into\n(41), and noting n =\nj\u0000 ms\nN d\n\u0001\nd\n2r+d k\n, we have for every Dm ∈\nZm\nδ,1 ∩Zm\nδ,2, there holds\nE(πMfD,n) −E(fρ)\n≤\n2(2rc2\n0 + 2M 2)m−\n2r\n2r+d\n\u0010 s\nN d\n\u0011\nd\n2r+d\n+\n14M 2(2d + 4)2 log 2\nδ\n3m\n+\n856(6d + 2)M 2m\n−2r\n2r+d\n\u0010 s\nN d\n\u0011\nd\n2r+d log 2\nδ\n×\nlog\n\u0002\n192e2(2d + 1)MCσBnCnΞnm\n\u0003\n.\nHence, with conﬁdence at least 1 −δ, there holds\nE(πMfD,n) −E(fρ)\n≤\nC′ log [BnCnΞnm] m\n−2s\n2s+d\n\u0010 s\nN d\n\u0011\nd\n2r+d log 2\nδ ,\nwhere\nC′\n:=\n2(c12rc2\n0 + (1 + c1)M 2) + 14M 2(2d + 4)2\n3\n+\n856(12d + 4)M 2 log(192e2Cσ).\nThis ﬁnishes the proof of Theorem 2.\nProof of Corollary 2: The bound can be deduced from\nthe conﬁdence-based error bound in Theorem 2 by the same\nmethod as that in the proof of Corollary 1.\nACKNOWLEDGEMENT\nThe research was supported by the National Natural Science\nFoundation of China (Grant Nos. 61502342, 11771012). The\nauthor would like to thank two anonymous referees for their\nconstructive suggestions. The author also would like to thank\nProfessor Jinshan Zeng for his helpful suggestions in revising\nthe paper.\nREFERENCES\n[1] Y. Bengio, O. Delalleau and N. L. Roux. “ The curse of highly variable\nfunctions for local kenrel machines”, Nips, 2006: 107-114.\n[2] Y. Bengio. “Learning deep architectures for AI”, Found. Trends Mach.\nLearn., vol. 2, pp. 1-127, 2009.\n[3] M. Bianchini and F. Scarselli. “On the complexity of neural network\nclassiﬁers: a comparison between shallow and deep architectures”, IEEE.\nTrans. Neural Netw. & Learn. Sys., vol. 25, pp. 1553-1565, 2014.\n[4] E. Blum and L. Li. “Approximation theory and neural networks”, Neural\nNetworks, vol. 4, pp. 511-515, 1991.\n[5] L. Bottou and V. Vapnik. “Local learning algorithms”, Neural Comput.,\nvol. 4, pp. 888-900, 1992.\n[6] G. Bradski and A. Kaehler. “Learning OpenCV: Computer vision with\nthe OpenCV library”, O’Reilly Media, Inc., 2008.\n[7] C. K. Chui, X. Li and H. N. Mhaskar. “Neural networks for lozalized\napproximation”, Math. Comput., vol. 63, pp. 607-623, 1994.\n[8] C. K. Chui, X. Li and H. N. Mhaskar. “Limitations of the approximation\ncapabilities of neural networks with one hidden layer”, Adv. Comput.\nMath., vol. 5, pp. 233-243, 1996.\n[9] C. K. Chui, S. B. Lin and D. X. Zhou. “Construction of neural\nnetworks for realization of localized deep learning”, arXiv preprint\narXiv: 1803.03503, 2018.\n14\n[10] F. Cucker and D. X. Zhou. “Learning Theory: An Approximation Theory\nViewpoint”, Cambridge University Press, Cambridge, 2007.\n[11] O. Delalleau and Y. Bengio. “Shallow vs. deep sum-product networks”,\nNIPs, 666-674, 2011.\n[12] R. Eldan and O. Shamir. “The power of depth for feedforward neural\nnetworks”, arXiv preprint arXiv:1512.03965, 2015.\n[13] I. Goodfellow, Y. Bengio and A. Courville. “Deep Learning”, MIT, 2016.\n[14] L. Gy¨orfy, M. Kohler, A. Krzyzak and H. Walk. “A Distribution-Free\nTheory of Nonparametric Regression”, Springer, Berlin, 2002.\n[15] G. E. Hinton, S. Osindero and Y. W. Teh. “A fast learning algorithm for\ndeep belief netws”, Neural Comput., vol. 18, pp. 1527-1554, 2006.\n[16] M. Kohler and A. Krzy˙zak. “Adaptive regression estimation with mul-\ntilayer feedforward neural networks”, J. Nonparametric Statis., vol. 17,\npp. 891-913, 2005.\n[17] V. K˚urkov´a and M. Sanguineti. “Estimates of covering numbers of\nconvex sets with slowly decaying orthogonal subsets”, Discrete Appl.\nMath., vol. 155, pp. 1930-1942, 2007.\n[18] V. K˚urkov´a and M. Sanguineti. “Can two hidden layers make a differ-\nence?”, in Adaptive and Natural Computing Algorithms (Lecture Notes\nin Computer Science) Vol 7823, M. Tomassini, A. Antonioni, F. Daolio\nand P. Buesser, Eds. New York, Ny, USA: Springer-Verlag, 2013, pp.\n30-39.\n[19] V. K˚urkov´a and M. Sanguineti. “Model complexities of shallow net-\nworks representing highly-varying functions”, Neurocomputing, vol.\n171, pp. 598-604, 2016.\n[20] V. K˚urkov´a, M. Sanguineti, Probabilistic lower bounds for approxima-\ntion by shallow perceptron networks, Neural Networks, 91 (2017), 34-\n41.\n[21] V. K˚urkov´a. Constructive lower bounds on model complexity of shallow\nperceptron networks, Neural Comput. Appl., Doi: 10.1007/s00521-017-\n2965-0.\n[22] S. Lin, Y. Rong and Z. Xu. “Multivariate Jackson-type inequality for a\nnew type neural network approximation”. Appl. Math. Model., vol. 38,\npp. 6031-6037, 2014.\n[23] S. Lin, X. Liu, J. Fang and Z. Xu. “Is extreme learning machine feasible?\nA theoretical assessment ( PART II)”, IEEE Trans. Neural Netw. Learn.\nSyst., vol. 26, pp. 21-34, 2015.\n[24] S. Lin, J. Zeng and X. Zhang. “Constructive neural network learning”,\nIEEE Trans. Cyber., DOI:10.1109/TCYB.2017.2771463, 2017.\n[25] S. B. Lin. Limitations of shallow nets approximation, Neural Networks,\nvol. 94, pp. 96-102, 2017.\n[26] S. B. Lin and D. X. Zhou. “Distributed kernel-based gradient descent\nalgorithms”, Constr. Approx., 47: 249-276, 2018.\n[27] H. W. Lin, M. Tegmark and D. Rolnick. “Why does deep and cheap\nlearning works so well?”, J. Stat. Phys., vol. 168, pp. 1223-1247, 2017.\n[28] V. Maiorov and J. Ratsaby. “On the degree of approximation by\nmanifolds of ﬁnite pseudo-dimension”, Constr. Approx., vol. 15, pp.\n291-300, 1999.\n[29] V. Maiorov and A. Pinkus. “Lower bounds for approximation by MLP\nneural networks”, Neurocomputing, vol. 25, pp. 81-91, 1999.\n[30] V. Maiorov. “Approximation by neural networks and learning theory”,\nJ. Complex., vol. 22, pp. 102-117, 2006.\n[31] V. Maiorov. “Pseudo-dimension and entropy of manifolds formed by\nafﬁne-invariant dictionary”, Adv. Comput. Math., 2006, 25(4): 435-450.\n[32] Y. Makovoz. “Random approximants and neural networks”, J. Approx.\nTheory, vol. 85, pp. 98-109, 1996.\n[33] B. McCane and L. Szymanski. “Deep radial kernel networks: approxi-\nmating radially symmetric functions with deep networks”, arXiv preprint\narXiv:1703.03470, 2017.\n[34] H. N. Mhaskar. “Approximation properties of a multilayered feedforward\nartiﬁcial neural network”, Adv. Comput. Math., vol. 1, pp. 61-80, 1993.\n[35] H. N. Mhaskar and T. Poggio. “Deep vs. shallow networks: An approx-\nimation theory perspective”, Anal. Appl., vol. 14, pp. 829-848, 2016\n[36] B. A. Olshausen and D. J. Field. “Sparse coding with an overcomplete\nbasis set: A strategy employed by V1?”, Vision Res., vol. 37(23), pp.\n3311-3325, 1997.\n[37] G. Mont´ufar, R. pascanu, K. Cho and Y. Bengio. “On the number of\nlinear regions of deep nerual networks”, Nips, 2014, 2924-2932.\n[38] A. Pinkus. “Approximation theory of the MLP model in neural net-\nworks”, Acta Numerica, vol. 8, pp. 143-195, 1999.\n[39] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli and J. Sohl-Dickstein. “On\nthe expressive power of deep neural networks”, arXiv preprint arXiv:\n1606.05336.\n[40] U. Shaham, A. Cloninger and R. R. Coifman. “Provable approximation\nproperties for deep neural networks”, Appl. Comput. Harmon. Anal.,\nvol. 44, pp. 537-557, 2018.\n[41] L. Shi, Y. L. Feng and D. X. Zhou. “Concentration estimates for\nlearning with l1-regularizer and data dependent hypothesis spaces”,\nAppl. Comput. Harmon. Anal., vol. 31, pp. 286-302, 2011.\n[42] L. Shi. “Learning theory estimates for coefﬁcient-based regularized\nregression”, Appl. Comput. Harmon. Anal., vol. 34, pp. 252-265, 2013.\n[43] M. Telgarsky. “Beneﬁts of depth in neural networks”, arXiv reprint\narXiv:1602.04485, 2016.\n[44] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli. “Image quality\nassessment: from error visibility to structural similarity”, IEEE Trans.\nImage Process., vol. 13, pp. 600-612, 2004.\n[45] Q. Wu, Y. M. Ying and D. X. Zhou. “Learning rates of least square\nregularized regression”, Found. Comput. Math., vol. 6, 171-192, 2006.\n[46] D. X. Zhou. “Capacity of reproducing kernel spaces in learning theory”,\nIEEE Trans. Inf. Theory, vol. 49, pp. 1743-1752, 2003.\n[47] D. X. Zhou and K. Jetter. “Approximation with polynomial kernels and\nSVM classiﬁers,” Adv. Comput. Math., vol. 25, pp. 323-344, 2006.\n[48] Z. H. Zhou, N. V. Chawla, Y. Jin and G. J. Williams. “Big data oppor-\ntunities and challenges: Discussions from data analytics perspectives”,\nIEEE Comput. Intel. Mag., vol. 9, pp. 62-74, 2014.\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2018-03-10",
  "updated": "2018-03-23"
}