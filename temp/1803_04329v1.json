{
  "id": "http://arxiv.org/abs/1803.04329v1",
  "title": "Semantic Parsing Natural Language into SPARQL: Improving Target Language Representation with Neural Attention",
  "authors": [
    "Fabiano Ferreira Luz",
    "Marcelo Finger"
  ],
  "abstract": "Semantic parsing is the process of mapping a natural language sentence into a\nformal representation of its meaning. In this work we use the neural network\napproach to transform natural language sentence into a query to an ontology\ndatabase in the SPARQL language. This method does not rely on handcraft-rules,\nhigh-quality lexicons, manually-built templates or other handmade complex\nstructures. Our approach is based on vector space model and neural networks.\nThe proposed model is based in two learning steps. The first step generates a\nvector representation for the sentence in natural language and SPARQL query.\nThe second step uses this vector representation as input to a neural network\n(LSTM with attention mechanism) to generate a model able to encode natural\nlanguage and decode SPARQL.",
  "text": "Semantic Parsing Natural Language into SPARQL: Improving\nTarget Language Representation with Neural Attention\nFabiano Ferreira Luz1, Marcelo Finger1\n1Universidade de S˜ao Paulo - USP\nInstituto de Matem´atica e Estat´ıstica - IME\nDepartamento de Ciˆencia da Computac¸˜ao\nRua do Mat˜ao, 1010 - Cidade Universit´aria\nS˜ao Paulo - SP - Brasil - CEP 05508-090\n{fluz,mfinger}@ime.usp.br\nAbstract. Semantic parsing is the process of mapping a natural language sen-\ntence into a formal representation of its meaning. In this work we use the neu-\nral network approach to transform natural language sentence into a query to\nan ontology database in the SPARQL language. This method does not rely on\nhandcraft-rules, high-quality lexicons, manually-built templates or other hand-\nmade complex structures. Our approach is based on vector space model and\nneural networks. The proposed model is based in two learning steps. The ﬁrst\nstep generates a vector representation for the sentence in natural language and\nSPARQL query. The second step uses this vector representation as input to a\nneural network (LSTM with attention mechanism) to generate a model able to\nencode natural language and decode SPARQL.\n1. Introduction\nSemantic parsing can be deﬁned as the process of mapping natural language sentences\ninto a machine interpreted, formal representation of its meaning. Currently, there are\nmany efforts aimed at transforming human language into a computational representa-\ntion [Zettlemoyer and Collins 2012, Alshawi et al. 2014, Bowman et al. 2014].\nIn this work, we are concerned with semantic parsing as the task of transforming\nnatural language into SPARQL queries. In recent years this task has received a lot of\nattention [Wang et al. 2007, Lehmann and B¨uhmann 2011, Ferr´e 2012, Ferr´e 2017]. This\nis mainly due to the increase of RDF-based documents available on the web, with pub-\nlic repositories such as DBPedia [Auer et al. 2007], which already provides support for\nSPARQL queries in its database.\nTraditional approaches to translating natural language into SPARQL rely on high-\nquality lexicons, manually-built templates, and linguistic features which are either domain\nor representation-speciﬁc [Dong and Lapata 2016]. This is a problem since adapting such\nmodels to a new domain can be a very laborious task. Because of this, we propose a\nmodel based on artiﬁcial neural networks that among other advantages, does not rely\non handcraft rules, high-quality lexicons, manually-built templates or other handmade\ncomplex structures.\nIn our work, we used an LSTM encoder-decoder model capable of encoding nat-\nural language (English) and decoding query language (SPARQL). The ﬁrst part of our\narXiv:1803.04329v1  [cs.CL]  12 Mar 2018\nwork is dedicated to ﬁnding suitable vector representations for sentences both in natural\nlanguage and in SPARQL. On the natural language side, we use the vector-based Glove\nmodel [Pennington et al. 2014]. For the SPARQL part, we propose a composition of\nmethods to generate the vector representation. In addition, we propose our own way of\ngenerating this representation. In the second part of this work, we implement and conﬁg-\nure an LSTM encoder-decoder model tailored to the task of translating natural language\nqueries to SPARQL.\nThe contributions of this paper are the following.\n• The creation of a version of the Geo880 dataset with SPARQL queries like a target\nlanguage.\n• The creation of an OWL ontology of the Geo880 domain.\n• The development of a novel vector representation for the target language lexicon.\n• The adaptation of the encoder-decoder model with neural attention to transform\nnatural language into SPARQL queries.\nThis work is organized as follows. In Section 2, we provide a background of the\nelements needed to understand how our model works. In Section 3 we detail our model\nand the lexical generation procedure. Section 4 is dedicated to discussing experiments\nand results. In Section 5 we talk more about related work and in the last section we\ncomment on our contribution and future work.\n2. Background\nIn the following sections we describe the neural network architecture encoder-decoder\nand also discuss the concept of neural attention.\n2.1. Recurrent Neural Network\nA Recurrent Neural Network (RNN) is a type of artiﬁcial neural network where connec-\ntions between units form a directed cycle. This cycle represents an internal state of the\nnetwork which allows it to exhibit dynamic temporal behavior. RNNs can use their inter-\nnal memory to process arbitrary sequences of inputs. An output of a hidden layer ht of an\nRNN can be deﬁned as\nht = f(ht−1, xt),\n(1)\nwhere ht−1, is the value of the hidden layer at time t−1, xt is the input feature vector and\nf(.) is a nonlinear function.\nIt has been noted by [Bengio et al. 1994] that RNNs suffer from the vanishing\ngradient problem, which consists of the exponential decrease that the value of ht′ has\ninﬂuence over the value of ht, t′ < t, leading to a very short the temporal memory in\nthe network. One solution to this problem was a change in the neuron’s nucleus called\nLong Short Term Memory (LSTM) [Hochreiter and Schmidhuber 1997]. RNN-LSTM\nhas been used successfully in language modeling problems because, so now it can handle\nlong sequences quite well.\n2.2. Encoder-Decoder Model\nThe Encoder-Decoder model, proposed by [Cho et al. 2014], is a neural network archi-\ntecture that learns the conditional distribution of a conditioning variable-length sequence\nx in another variable-length sequence y. It performs this task by learning how to encode a\nvariable-length sequence x1, ..., xT into a ﬁxed-length vector representation c and then to\ndecode a given ﬁxed-length vector representation c back into a variable-length sequence\ny1, ..., yS. The function may be interpreted as the distribution p(y1, ..., yS|x1, ..., xT); the\ninput sequence length T and output one S can be different.\nThe encoder is an RNN that reads each word of an input sequence x sequentially.\nAs it reads each symbol, the hidden state of the RNN is updated according to equation (1).\nAfter reading the end of the sequence (marked with an end sequence symbol), the hidden\nstate of the RNN is summarized. We call this summary c. In order to simplify we can\ndeﬁne c as the output h.\nThe decoder is another RNN which is trained to generate the output sequence\nby predicting the next symbol yt given the hidden state ht. However, unlike the RNN\ndescribed previously, here both yt and ht are conditioned to yt−1 and the summary c\nof the input sequence. Thus, the hidden state of the decoder at time t is computed by:\nht = f(ht−1, yt−1, c), and likewise, we can deﬁne the conditional distribution of the next\nsymbol by the following equation:\nP(yt|yt−1, ..., y1, c) = g(ht−1, yt, c).\n(2)\nThe activation function g produces valid probabilities by, for example, computing the\nsoftmax function. Figure 1 presents an overview of the encoder and decoder scheme.\nFigure 1. Encoder-Decoder scheme.\nThe combination of the two described components (Encoder and Decoder) make\nup the training of the proposed model to maximize the conditional log-likelihood and can\nbe represented by the equation:\nmax\nθ\n1\nN\nN\nX\nn=1\nlog pθ(y|x),\n(3)\nwhere θ is the set of the model parameters and each pair (x, y) is, respectively, an input\nand output sequence. In our case, we use the vector representation of questions in natural\nlanguage as input and the SPARQL query as an output. Since the output of the decoder,\nstarting from the input, is differentiable, we can use a gradient-based algorithm to estimate\nthe model parameters.\nAfter training the encoder-decoder RNN, the model can be used in two distinct\nways. In the ﬁrst case, we can use the model to generate a target sequence, once the input\nsequence is provided computing the most probable output given the input. In the second\none, the model can be used to evaluate a given pair of input and output sequences by\ncalculating a score (e.g. the probability pθ(y|x)).\nAlthough sequential models present good results for sequence transformation,\nthey still present a distortion in the sentence mapping task because relationships among\nsentences are largely organized in terms of latent nested syntactic/semantic structures\nrather than sequential surface order [Lees and Chomsky 1957, Dyer et al. 2016]. One\nway to deal with these linguistic properties is adding the mechanism of neural attention,\nas used, for example, in soft alignment [Bahdanau et al. 2015].\n2.3. Attention Mechanism in Neural Networks\nThe model proposed by [Bahdanau et al. 2015] differs from the basic model of encoder-\ndecoder by not attempting to encode a full entry into a ﬁxed-size vector. Instead, it en-\ncodes entries into a sequence of vectors and selects a subset of them adaptively while\ndecoding the translation. With this modiﬁcation, the neural network no longer has the\nchallenge of compressing information of an entire sequence into a ﬁxed-size vector. The\nnew architecture consists of a bidirectional RNN as an encoder and a decoder that emu-\nlates searching through a source sentence while decoding a translation.\nNew Encoder:\nThe proposed encoder in [Bahdanau et al. 2015] does not use a standard\nRNN described in equation (1), which reads an input sequence x starting from the ﬁrst el-\nement x1 to the last xTx. However, in the proposed scheme, the encoder does not compute\nonly a single summary of the previous words. Instead, for each input word it computes\nan annotation representing both a summary of the previous words and also one for the\nfollowing ones. Then, an appropriate model to obtain such annotation is a bidirectional\nRNN [Schuster and Paliwal 1997], which has recently been used successfully in speech\nrecognition [Graves et al. 2013].\nA bidirectional RNN is composed of a forward RNN −→f that reads the input se-\nquence as it is ordered from x1 to xTx and calculates a sequence of forward hidden states\n(−→h 1, ..., −→h Tx) and the backward RNN ←−f which reads the sequence in reverse order, from\nxTx to x1, resulting in a sequence of backward hidden states (←−h 1, ..., ←−h Tx).\nThus, an annotation is obtained for each word xj by concatenating the forward\nhidden state −→\nhj and the backward one ←−\nhj,\nhj =\nh−→h T\nj ; ←−h T\nj\niT\n.\n(4)\nThe annotation hj encodes both preceding and following words. Due to the ten-\ndency of RNNs to better represent recent inputs, the annotation hj is focused on the words\naround xj. This sequence of annotations is used by the decoder and the alignment model\nlater to compute the context vector.\nNew Decoder:\nIn the new model, each conditional probability is deﬁned by:\np(yi|y1, ..., yi−1, x) = g(yi, si, ci), where si is an RNN hidden state for time i, computed\nby:\nsi = f(si−1, yi−1, ci)\n(5)\nUnlike the basic encoder-decoder approach in (2), here the probability is conditioned on\na distinct context vector ci for each target word yi. The context vector ci depends on a\nsequence of annotations (h1, ..., hTx) in which an encoder maps the input sentence. Each\nannotation hi consists of information about the whole input sequence with a strong focus\non the parts surrounding the i-th word of the input sequence.\nThe context vector ci is then computed as a weighted sum of these annotations hi\nci =\nTx\nX\nj=1\nαijhj\n(6)\nThe weight αij of each annotation hj is computed by\nαij =\nexp(eij)\nPTx\nk=1 exp(eik)\n,\n(7)\nwhere eij = a(si−1, hj) is an alignment model that scores how well the input around\nposition j and the output at position i match. The score is based on the RNN hidden state\nsi−1 of (5) and the j-th annotation hj of the input sentence.\nThe alignment model a is parametrized as a feedforward neural network which is\nconcomitantly trained with all the other components of the proposed system. The align-\nment model directly computes a soft alignment, which allows the gradient of the cost\nfunction to be backpropagated. This gradient can be used to train the alignment model as\nwell as the whole translation model at the same time.\nThe use of a weighted sum of all annotations can be interpreted as calculating\nan expected annotation, where the expectation is over possible alignments. Let αij be\na probability that the target word yi is aligned to, or translated from, a source word xj.\nThen, the i-th context vector ci is the expected annotation over all the annotations with\nprobabilities αij.\nFor visual reasons, the alignment matrix of Figure 2 was plotted with colors\ninstead of numerical values. The matrix is an example of the alignment obtained during\nthe training of our model. The darker the cell of the matrix the bigger the correlation\nbetween the terms row and column.\nFigure 2. Neural attention matrix example\n3. Our Approach\nThe model described here is based on two approaches in the neural network literature.\nFirst, a neural probabilistic language model similar to that of [Pennington et al. 2014]\nis used to learn a word vector representation, and a LSTM neural network similar to\nthat of [Dong and Lapata 2016] is used to encode natural language sentences and decode\nSPARQL query. We provided the necessary background on these two components in\nSection 2.\nThere are at least two important contributions in our work. The ﬁrst is the fact\nthat we use the model in question to translate from natural language to SPARQL. Another\nimportant contribution is the representation of the lexicon we are proposing. For this\nrepresentation we use the neural attention mechanism to generate the table that will be\nused in matching. The present section is divided into three parts where, ﬁrst, we give\nan overview of the developing model; then we describe the concept of matching using\nneural attention; and in the end we detail how we generate the lexical representation for\nthe target language.\n3.1. General overview of the approach\nOur approach consists of two learning phases, which can be observed in Figure 3:\nFigure 3. Overview\nIn the following text, we have a detailed description of each step:\n1. The ﬁrst step is to ﬁnd a good vector representation for the target language\nlexicon.\nTo represent the source natural language, we employ the method in\n[Pennington et al. 2014].\n2. In the second part, we are concerned with implementing and ﬁnding the settings\nso that our architecture can translate from natural language to SPARQL.\nFor the training of our model, a dataset of paired sentences is necessary where,\non the one hand we have questions in natural language and the other their SPARQL. This\ndataset is described in Section 4.\n3.2. Vocabulary matching using neural attention\nIn this work we propose a kind of dictionary where for each word of the target language\nwe have another one in the source language that has the greatest correlation. Let T be\ntarget language vocabulary and S be source language vocabulary. Λ is the total alignment\nmatrix of dimension |S| × |T| where Λij contains the value of correlation between words\nsi and tj. We can deﬁne the greatest correlation of a word tj from the target language as\nthe:\nmax\ni=1..|S|(Λij)\nand the word wi∗, i∗= argmax(Λij), is the word from the source language that possesses\nthe greatest correlation with tj.\nWe can exemplify the matching procedure with Figure 2 from the previous section.\nNote that the target word p:has city has the corresponding word cities. We can observe\nthis because the darker cell connects the two words. That is, the word p:has city has\ngreater correlation with the word cities. Based on this table, we have built a translation\ndictionary from target to source. This dictionary could have several practical uses, for\nexample, the lexical representation of the target. It will be better explained in the next\nsection.\n3.3. Lexical representation\nTo represent the lexicon of natural language, our source language, we use the model\ndeveloped in [Pennington et al. 2014]. However, in the case of the SPARQL terms we\ndid several experiments to analyze different representations. Next we describe the vector\nrepresentation generation methods for the lexicon we use.\nRANDOM:\nThis representation was generated randomly using a normal distribution\nas a kernel with values from -1 to 1. This approach was chosen in order to kick off the\nrepresentation like a baseline and even if it could not capture the relationship between\nterms, its performance was not so low when compared to more sophisticated approaches.\nTF-IDF/PCA:\nWe use the target language sentences and generate a term document\nmatrix. In this matrix, we consider each query a document and each word in the tar-\nget language a term.\nAfter generating the term-document matrix we apply TF-IDF\n[Aizawa 2003]. At the end we did a size reduction using Principal Component Analy-\nsis (PCA) [Jolliffe 2002]. Our reduction was from 880 to 300 dimensions.\nW2V10:\nThis approach is based on work by [Mikolov et al. 2010], which proposed the\ngeneration of a vector representation of words based on recurrent neural networks, using\nonly text as input. We use the target language sentences as input text to generate a vector\nrepresentation.\nOUR-APP:\nOur approach is focused on using the same set of vectors to represent both\nsource and target language vocabulary. What we do is a match between the terms of\nthe source language and the target language seeking a match between the two vocabular-\nies. The matching process is possible with the alignment table generated by the neural\nattention mechanism.\nThen, ﬁrst using a random version to represent the target’s lexicon, we train and\ngenerate an alignment table, Figure 2. Then, we use the Matching (see section 3.2) to\nassociate the lexicon of the source language with the target language. We use a heuristic\nto associate all elements of one vocabulary with another. SPARQL words are similar to\nEnglish words, for example, “SELECT”, “FILTER”, “ORDER”, etc were directly asso-\nciated with their respective correspondents in English. The other terms such as, subject,\npredicate and object of the triples, are solved with the matching procedure. Our heuristics\nproved to be good according to the results. A full description of the heuristics used and\nhow to deal with the generation of non-grammatical (incorrect) SPARQL expressions will\nbe dealt with in future work.\nNote that in this work we do not deal with the “out of vocabulary” situation (OOV),\nnor with lexical disambiguation problem. In fact, the OOV problem is avoided by the\nword2vec pre-processing, and every word is assumed to be associated to a vector of\nfeatures, which is comparable to having a controlled vocabulary. Similarly, the lexical\nambiguities are assumed to be codiﬁed inside the attribute vectors in the pre-processing;\nfurthermore, approximately 1% of the words in the vocabulary are lexically ambiguous,\nallowing us to safely ignore such effects.\n4. Experimental Evaluation\nWe compared our methodology with related work using the Geo880 dataset. We use only\none dataset because we needed to create a SPARQL corpus for each domain and it is very\nlaborious. The test with different datasets remained as future work.\nIn this section, ﬁrst, we deﬁne our metrics and then talk about an adaptation of the\ndataset to the SPARQL query. We also talk about syntactic errors, neural network settings\nand ﬁnally comment on comparisons of our work with other different approaches. Here\nwe use two metrics to evaluate our approach:\nAccuracy = # of correctly translated queries\n# total of queries\nSyntactical Errors = # of queries with syntactical errors\n# total of queries\n4.1. Dataset\nOur experiments were conducted using one traditional dataset: Geo880, a set of 880\nqueries to a database of U.S. geography. The data were originally annotated with Pro-\nlog style semantics which we manually converted to equivalent statements in SPARQL\nqueries. On the ofﬁcial web page of Geo880 dataset of the University of Austin in Texas1\nwe found some ﬁles. Using two ﬁles, the geobase with assertions of the dataset and\nthe geoquery880 ﬁle, containing questions directed at this domain. First we created an\nOWL ontology based on the geodata ﬁle and then, for each query in natural language on\ngeoquery880 ﬁle, we wrote a corresponding SPARQL query. Both the ontology and the\nset of questions can be found in our repository https://github.com/mllovers/\ngeo880-sparql .\n4.2. Syntactical Errors\nWe call a syntactical error when a generated SPARQL statement cannot be processed by\nProt´eg´e 2 due to syntactical formation3. In general, there are several syntactical errors that\ncan be generated, such as not closing any brackets or even trying to apply a function to a\nvariable of the wrong type.\n4.3. Settings\nFinding better parameters and hyper-parameters for neural networks is always a very\ncostly task. The ﬁrst rounds with the neural network, the pre-tests, served to ﬁnd the\nbest parameters for the neural network. In all, the pre-tests lasted more than two months.\n• Learning rate: During the pre-test, we use two different learning rates. The ﬁrst,\nwe maintained the learning rate at 0.5. The second was started learning rate at 0.9\nand decreasing 0.9 of it in each iteration (epoch). The best results obtained are\nused in the second case;\n• Epoch: In the tests we chose to use 100 epochs, as this was the best result found\nin our pre-tests for hyperparameters;\n• Hidden dimension: With regard to the number of hidden layers, we used a series\nof pre-tests with 100, 200 and 400 hidden layers. We continued testing with the 4\ndifferent dimensions;\n• Input dimension: We used three different input dimensions, 100, 200 and 300 but\nthe best results were obtained with vectors of size 300.\n4.4. Results\nIn Table 1 we show the experiments performed without the mechanism of neural attention.\nAll experiments were performed with the Geo880 dataset. We used 10-fold crossvalida-\ntion.\nLSTM Encoder-decoder - Geo880 Dataset\nHidden Dim.\nMethod\nSynt. error\nAccuracy\n100\nRANDOM\n14.77\n30.68\nTF-IDF/PCA\n13.64\n31.89\nW2V10\n14.77\n28.40\nOUR-APP\n11.36\n38.63\n200\nRANDOM\n11.36\n36.36\nTF-IDF/PCA\n10.23\n38.67\nW2V10\n11.36\n32.95\nOUR-APP\n07.95\n54.55\n400\nRANDOM\n09.09\n51.14\nTF-IDF/PCA\n07.95\n54.55\nW2V10\n10.23\n42.04\nOUR-APP\n06.81\n64.77\nTable 1. Without neural at-\ntention\nLSTM Encoder-Decoder - Geo880 Dataset\nHidden Dim.\nMethod\nSynt. Error\nAccuracy\n100\nRANDOM\n12.50\n37.50\nTF-IDF/PCA\n11.36\n40.91\nW2V10\n12.50\n35.23\nOUR-APP\n0\n54.55\n200\nRANDOM\n11.36\n47.73\nTF-IDF/PCA\n10.22\n51.14\nW2V10\n11.36\n45.45\nOUR-APP\n09.09\n67.04\n400\nRANDOM\n07.95\n62.50\nTF-IDF/PCA\n06.81\n64.77\nW2V10\n09.09\n60.23\nOUR-APP\n05.68\n78.40\nTable 2. With neural atten-\ntion\n1http://www.cs.utexas.edu/users/ml/nldata/geoquery.html\n2http://protege.stanford.edu/\n3Another option would be to use a grammar that describes SPARQL, then use it as parameter of the\nparser algorithm. However a rejection by the SPARQL interpreter is a cheaper option found.\nIn Table 2 the tests were run on the LSTM encoder-decoder architecture with\nattention mechanism. In next table we show examples of inputs and outputs generated by\nour model.\nInput\nOutput (Without IRI Preﬁx to simplify)\nhow many rivers are there in idaho ?\nSELECT (COUNT(?A) AS ?QTD) { ?idaho p:river ?A\nFILTER (regex(str(?idaho), ”idaho”, ”i”)) . }\nshow major cities in colorado ?\nSELECT ?A { ?colorado p:city ?A FILTER (regex(str(?colorado), ”colorado”, ”i”)) .\n?A r:type p:Major . }\nwhat are the cities of the state\nSELECT ?B { ?A p:city ?B . { SELECT ?A { ?A r:type p:State . ?A p:highest point ?B .\nwith the highest point ?\n?B p:height ?height . } ORDER BY DESC(?height) LIMIT 1 } }\nRegarding the task of transforming the natural language into SPARQL, we com-\npared our work with [AlAgha 2015] and [Kaufmann et al. 2006]. In the ﬁrst paper, the\nauthors also use the Geo880 dataset and through linguistic analysis identify elements of\nnatural language and generate RDF triples. In the second paper, the main strategy of the\nauthors was to try to associate triples of natural language with RDF triples. As can be\nseen in the Table 3, we obtained better results in the tests with dataset Geo880.\nAccuracy\n[AlAgha 2015]\n58.61\nQuerix [Kaufmann et al. 2006]\n77.67\nOur method\n78.40\nTable 3. Natural Language to SPARQL comparisons\nAlthough we mention the work [Wang et al. 2007], that also makes use of the\nGeo880 dataset, we do not make the comparison with it because it does not use the original\nset of Geo880 queries in their tests. For the same reason, we also disregard one of the\nresults of Querix [Kaufmann et al. 2006]. With respect to the syntactical errors, we can\nsee in our tests that the better the model in general, the lower the error rate of syntax. We\nalso propose as future work to develop a model that the generated sentence has syntactic\nguarantee.\n5. Related Work\nIn this section we discuss three related works that are related to different aspects of\nour model. The ﬁrst, [Dong and Lapata 2016], is related to the task of mapping nat-\nural language sentences to the logical form.\nThe second [AlAgha 2015] and third\n[Kaufmann et al. 2006], are related with the task of translating natural language for\nSPARQL.\n[Dong and Lapata 2016] present a general method based on an attention-enhanced\nsequence- to-sequence model. They encode input sentences into vector representations\nusing recurrent neural networks, and generate their logical forms by conditioning the\noutput on the encoding vectors. The model is trained in an end-to-end fashion to maximize\nthe likelihood of target logical forms given the natural language inputs. Although they do\nnot deal with SPARQL, their approach uses a neural network attention-based structure\nsimilar to ours.\nThe work in [AlAgha 2015] associates phrases in natural language with RDF\ntriples, as in our approach. Through a linguistic analysis, their model extracts relations\nand associates them to triples. They also generate SPARQL scripts, however using ﬁrst\nan intermediate format. Then, with the help of an ontology, a SPARQL query is generated\nafter identifying the targets and modiﬁers of the query. That is developed using Arabic as\nthe source language, however a comparison with ours can be made as both works also use\ninformation extraction from a syntactic tree.\nThe Querix [Kaufmann et al. 2006] employs a statistical approach. Given a query,\nthe system consists of parsing, removing important elements and then looking for triples\nthat are related to the elements of the query. Querix works as a component and can be\nadapted in any application. It is based on clariﬁcation of dialogues, so when there is\nambiguity the system asks the user to decide. All these works were evaluated using the\nGEO 880 dataset.\n6. Discussion and Future Work\nThe purpose of this work was to explore artiﬁcial neural network resources in the de-\nvelopment of a model that may be able to translate from natural language to SPARQL.\nThe choice of the OWL Ontology and the SPARQL language as the target language is\ndue to the fact that we are interested in practical applications. Among the advantages of\nusing artiﬁcial neural networks, we can highlight the fact that we do not need linguistic\nknowledge nor do we depend on the development of complex grammars.\nIn addition to dealing with SPARQL, we propose in this work a representation\nof the target language lexicon that according to the results was a good approach. This\nrepresentation is only possible because we use the concept of Matching of terms oriented\nby the alignment table that is generated by the mechanism of neural attention. Moreover,\nwe can highlight as main future works: To perform tests with different datasets and to\nguarantee of correct syntax in query generation.\nAcknowledgment\nThis work was developed with the support of the National Council for Scientiﬁc and\nTechnological Development (CNPq). Marcelo Finger was partly supported by Fapesp\nprojects 2015/21880-4 and 2014/12236-1 and CNPq grant PQ 306582/2014-7.\nReferences\n[Aizawa 2003] Aizawa, A. (2003). An information-theoretic perspective of tf–idf measures.\nInformation Processing & Management, 39(1):45–65.\n[AlAgha 2015] AlAgha, I. (2015). Using linguistic analysis to translate arabic natural lan-\nguage queries to SPARQL. CoRR, abs/1508.01447.\n[Alshawi et al. 2014] Alshawi, H., Chang, P.-C., and Ringgaard, M. (2014). Deterministic\nstatistical mapping of sentences to underspeciﬁed semantics. In Computing Meaning,\npages 13–25. Springer.\n[Auer et al. 2007] Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., and Ives,\nZ. (2007). Dbpedia: A nucleus for a web of open data. The semantic web, pages\n722–735.\n[Bahdanau et al. 2015] Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine\ntranslation by jointly learning to align and translate. In Proceedings of the ICLR.\n[Bengio et al. 1994] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term\ndependencies with gradient descent is difﬁcult. IEEE transactions on neural networks,\n5(2):157–166.\n[Bowman et al. 2014] Bowman, S. R., Potts, C., and Manning, C. D. (2014). Recursive\nneural networks for learning logical semantics. CoRR, abs/1406.1827, 5.\n[Cho et al. 2014] Cho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares,\nF., Schwenk, H., and Bengio, Y. (2014).\nLearning phrase representations using\nrnn encoder-decoder for statistical machine translation. In Proceedings of the 2014\nEMNLP, pages 1724–1734.\n[Dong and Lapata 2016] Dong, L. and Lapata, M. (2016). Language to logical form with\nneural attention. CoRR, abs/1601.01280.\n[Dyer et al. 2016] Dyer, C., Kuncoro, A., Ballesteros, M., and Smith, N. A. (2016). Recur-\nrent neural network grammars. CoRR, abs/1602.07776.\n[Ferr´e 2012] Ferr´e, S. (2012). Squall: A controlled natural language for querying and up-\ndating rdf graphs. In International Workshop on Controlled Natural Language, pages\n11–25. Springer.\n[Ferr´e 2017] Ferr´e, S. (2017). Sparklis: an expressive query builder for sparql endpoints\nwith guidance in natural language. Semantic Web, 8(3):405–418.\n[Graves et al. 2013] Graves, A., Jaitly, N., and Mohamed, A.-r. (2013).\nHybrid speech\nrecognition with deep bidirectional lstm. In Automatic Speech Recognition and Un-\nderstanding (ASRU), 2013 IEEE Workshop on, pages 273–278. IEEE.\n[Hochreiter and Schmidhuber 1997] Hochreiter, S. and Schmidhuber, J. (1997). Long short-\nterm memory. Neural computation, 9(8):1735–1780.\n[Jolliffe 2002] Jolliffe, I. (2002). Principal component analysis. Wiley Online Library.\n[Kaufmann et al. 2006] Kaufmann, E., Bernstein, A., and Zumstein, R. (2006). Querix: A\nnatural language interface to query ontologies based on clariﬁcation dialogs. In 5th\nInternational Semantic Web Conference (ISWC 2006), pages 980–981. Springer.\n[Lees and Chomsky 1957] Lees, R. B. and Chomsky, N. (1957). Syntactic structures. Lan-\nguage, 33(3 Part 1):375–408.\n[Lehmann and B¨uhmann 2011] Lehmann, J. and B¨uhmann, L. (2011).\nAutosparql: Let\nusers query your knowledge base. The Semantic Web: Research and Applications,\npages 63–79.\n[Mikolov et al. 2010] Mikolov, T., Karaﬁ´at, M., Burget, L., Cernock`y, J., and Khudanpur,\nS. (2010). Recurrent neural network based language model. In Interspeech, volume 2,\npage 3.\n[Pennington et al. 2014] Pennington, J., Socher, R., and Manning, C. D. (2014). Glove:\nGlobal vectors for word representation. In EMNLP, volume 14, pages 1532–1543.\n[Schuster and Paliwal 1997] Schuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent\nneural networks. Signal Processing, IEEE Transactions on, 45(11):2673–2681.\n[Wang et al. 2007] Wang, C., Xiong, M., Zhou, Q., and Yu, Y. (2007). Panto: A portable\nnatural language interface to ontologies. In European Semantic Web Conference, pages\n473–487. Springer.\n[Zettlemoyer and Collins 2012] Zettlemoyer, L. S. and Collins, M. (2012). Learning to map\nsentences to logical form: Structured classiﬁcation with probabilistic categorial gram-\nmars. CoRR, abs/1207.1420.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-03-12",
  "updated": "2018-03-12"
}