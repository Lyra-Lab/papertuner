{
  "id": "http://arxiv.org/abs/1803.07067v1",
  "title": "Setting up a Reinforcement Learning Task with a Real-World Robot",
  "authors": [
    "A. Rupam Mahmood",
    "Dmytro Korenkevych",
    "Brent J. Komer",
    "James Bergstra"
  ],
  "abstract": "Reinforcement learning is a promising approach to developing hard-to-engineer\nadaptive solutions for complex and diverse robotic tasks. However, learning\nwith real-world robots is often unreliable and difficult, which resulted in\ntheir low adoption in reinforcement learning research. This difficulty is\nworsened by the lack of guidelines for setting up learning tasks with robots.\nIn this work, we develop a learning task with a UR5 robotic arm to bring to\nlight some key elements of a task setup and study their contributions to the\nchallenges with robots. We find that learning performance can be highly\nsensitive to the setup, and thus oversights and omissions in setup details can\nmake effective learning, reproducibility, and fair comparison hard. Our study\nsuggests some mitigating steps to help future experimenters avoid difficulties\nand pitfalls. We show that highly reliable and repeatable experiments can be\nperformed in our setup, indicating the possibility of reinforcement learning\nresearch extensively based on real-world robots.",
  "text": "Setting up a Reinforcement Learning Task with a Real-World Robot\nA. Rupam Mahmood1, Dmytro Korenkevych1, Brent J. Komer2, and James Bergstra1\n{rupam, dmytro.korenkevych}@kindred.ai, bjkomer@uwaterloo.ca, james@kindred.ai\nAbstract— Reinforcement learning is a promising approach to\ndeveloping hard-to-engineer adaptive solutions for complex and\ndiverse robotic tasks. However, learning with real-world robots\nis often unreliable and difﬁcult, which resulted in their low\nadoption in reinforcement learning research. This difﬁculty is\nworsened by the lack of guidelines for setting up learning tasks\nwith robots. In this work, we develop a learning task with a UR5\nrobotic arm to bring to light some key elements of a task setup\nand study their contributions to the challenges with robots. We\nﬁnd that learning performance can be highly sensitive to the\nsetup, and thus oversights and omissions in setup details can\nmake effective learning, reproducibility, and fair comparison\nhard. Our study suggests some mitigating steps to help future\nexperimenters avoid difﬁculties and pitfalls. We show that\nhighly reliable and repeatable experiments can be performed in\nour setup, indicating the possibility of reinforcement learning\nresearch extensively based on real-world robots.\nI. INTRODUCTION\nDespite some recent successes (e.g., Levine et al. 2016,\nGu et al. 2017), real-world robots are under-utilized in the\nquest for general reinforcement learning (RL) agents, which\nat this time is primarily conﬁned to simulation. This under-\nutilization is largely due to frustrations around unreliable\nand poor learning performance with robots. Although sev-\neral RL methods are recently shown to be highly effective\nin simulations (Duan et al. 2016), they often yield poor\nperformance when applied off-the-shelf to real-world tasks.\nSuch ineffectiveness is sometimes attributed to some of the\nintegral aspects of the real world including slow rate of\ndata collection, partial observability, noisy sensors, safety,\nand frailty of physical devices. This barrier contributed to a\nreliance on indirect approaches such as simulation-to-reality\ntransfer (Rusu et al. 2017) and collective learning (Yahya et\nal. 2017, Gu et al. 2017), which sometimes compensate for\nfailures to learn from a single stream of real experience.\nOne oft-ignored shortcoming in real-world RL research is\nthe lack of benchmark learning tasks or standards for setting\nup experiments with robots. Experiments with simulated\nrobots are typically done on benchmark tasks with easily\navailable simulators and standardized interfaces, relieving\nexperimenters of many task-setup details such as the action\nspace, the action cycle time and system delays. On the other\nhand, setting up a learning task with real-world robots is far\nfrom obvious. An experimenter has to put a lot of work into\nestablishing a device-speciﬁc sensorimotor interface between\nthe learning agent and the robot as well as determining all the\naspects of the environment that deﬁne the learning task. Such\n1The authors are with Kindred Inc.\n2The author contributed during his internship at Kindred Inc.\nFingertip\nJoint-2\nJoint-1\nTarget\nFig. 1.\nWe use a UR5 robotic arm to deﬁne reinforcement learning tasks\nbased on two joints (shown above) and six joints. The reward is the negative\ndistance between the ﬁngertip (blue) and a random target location (red).\nchoices can be crucial for effective and reproducible learning\nperformance. Unfortunately, RL research works with real-\nworld robots typically do not describe many of these details,\nlet alone study their effects in a controlled manner, although\nsome notable exceptions exist (e.g., Schuitema et al. 2010,\nDegris et al. 2012, Hester & Stone 2013).\nIn this work, we address the question of how to set up a\nreal-world robotic task so that an off-the-shelf implementa-\ntion of a standard RL method can perform effectively and\nreliably. We address this by developing a Reacher task for\nthe UR5 robotic arm (see Figure 1), in which an agent learns\nto reach arbitrary target positions with low-level actuations\nof a robotic arm using trial and error. This task is easy to\nsolve in a simulation but can be difﬁcult with real-world\nrobots (Gu et al. 2017). For the learning method, we use\nthe rllab implementation of TRPO (Schulman et al. 2015,\nDuan et al. 2016), a popular learning method with robust\nperformance with respect to its hyper-parameters.\nAs we set up UR5 Reacher, we describe the steps and\nelements of setting up real-world RL tasks including the\nmedium of data transmission, concurrency, ordering and\ndelays of computations, low-level actuation types, and fre-\nquencies of operating them. By exploring different variations\nof these elements, we study their individual contributions to\nthe difﬁculty with robot learning. We ﬁnd that variability in\ntime delays and choosing an action representation that is non-\ntrivially processed before actuation can be highly detrimental\nto learning. By accounting for these effects, we show that it is\npossible to achieve not only effective performance with real-\nworld robots but also repeatability of learning from scratch\nin a highly reliable manner even when the repeats are run\nfor hours on different times using different physical robots.\narXiv:1803.07067v1  [cs.LG]  19 Mar 2018\nII. REINFORCEMENT LEARNING TASK FORMULATION\nA reinforcement learning (RL) task is composed of an\nagent and an environment interacting with each other (Sutton\n& Barto 2017), modeled formally as a Markov decision\nprocess (MDP). In an MDP, an agent interacts with its\nenvironment at discrete time steps t = 1, 2, 3, · · · , where\nat each step t, the agent receives the environment’s state\ninformation St ∈S and a scalar reward signal Rt ∈\nR. The agent uses a stochastic policy π governed by a\nprobability distribution π(a|s)\ndef\n== Pr {At = a|St = s} to\nselect an action At ∈A. The environment transitions to\na new state St+1 and produces a new reward Rt+1 at the\nnext time step using a transition probability distribution:\np(s′, r|s, a)\ndef\n== Pr {St+1 = s′, Rt+1 = r|St = s, At = a}.\nThe agent’s performance is typically evaluated in terms of\nits future accumulated rewards, known as a return Gt\ndef\n==\nP\nk=t γk−tRk+1, where γ ∈[0, 1] is a discount factor.\nThe goal of the agent is typically to ﬁnd a policy that\nmaximizes the expected return. Such policies are often\nlearned by estimating action values as in Q-learning or by\ndirectly parameterizing the policy and optimizing the policy\nparameters as in TRPO. In practice, the agent does not\nreceive the environment’s full state information but rather\nobserves it partially through a real-valued observation vector\not. Under this framework, an RL task is described primarily\nusing three elements: the observation space, the action space,\nand the reward function.\nIII. THE UR5 REACHER TASK\nIn this section, we design a Reacher task with the UR5\nrobot, which we call UR5 Reacher. We design it to be\nsimilar to OpenAI-Gym Reacher (Brockman et al. 2016),\nwhere an agent learns to reach arbitrary target positions\nwith direct torque control of a simulated two-joint robotic\narm. By parameterizing the policy nonlinearly with a neural\nnetwork, a policy-search method such as TRPO can solve\nGym Reacher reasonably well in a few thousand time steps.\nDesigning the task based on Gym Reacher allows us to set\na reasonable expectation of the learning time, utilize the\nchoices already made, and isolate challenges that emerge\nfrom design decisions in the hardware interface. In the\nfollowing, we describe the interface of the UR5 robot and\nthe details of the UR5 Reacher task.\nThe UR5 is a lightweight, ﬂexible industrial robot with\nsix joints manufactured by Universal Robots. The low-\nlevel robot controller of UR5, called URControl, can be\nprogrammed by communicating over a TCP/IP connection.\nThe robot can be controlled at the script-level using a\nprogramming language called URScript. After establishing\na connection, we can send URScript programs from a com-\nputer to URControl as strings over the socket. URScript pro-\ngrams run on URControl in real-time, which streams status\npackets every 8ms. Each packet from the URControl contains\nthe sensorimotor information of the robot including angular\npositions, velocities, target accelerations, and currents for all\njoints. The robot can be controlled with URScript by sending\nlow-level actuation commands on the same 8ms clock. The\nURScript servoj command offers a position control interface,\nand speedj offers a velocity control interface. Unlike Gym\nReacher, there is no torque control interface.\nIn UR5 Reacher, we actuate the second and the third\njoints from the base. We also extend it to a task with\nall six joints actuated, which we called UR5 Reacher 6D.\nThe observation vector includes the joint angles, the joint\nvelocities, and the vector difference between the target and\nthe ﬁngertip coordinates. Unlike Gym Reacher, we do not\ninclude the sines or cosines of joint angles or the target-\nposition coordinates to simplify and reduce the observation\nspace without losing essential information. We include the\nprevious action as part of the observation vector, which can\nbe helpful for learning in systems with delays (Katsikopoulos\n& Engelbrecht 2003). In Gym Reacher, the reward function\nis deﬁned as: Rt = −dt −pt−1, where dt is the Euclidean\ndistance between the target and the ﬁngertip positions, and\npt is the L2-norm of At to penalize large torques. We use the\nsame reward function but simplify it by dropping the penalty\nterm. UR5 Reacher consists of episodes of interactions,\nwhere each episode is 4 seconds long to allow adequate\nexploration. The ﬁngertip of UR5 is conﬁned within a 2-\ndimensional 0.7m × 0.5m boundary in UR5 Reacher and\nwithin a 3-dimensional 0.7m × 0.5m × 0.4m boundary in\nUR5 Reacher 6D. At each episode, the target position is\nchosen randomly within the boundary, and the arm starts\nfrom the middle of the boundary. In addition to constraining\nthe ﬁngertip within a boundary, the robot is also constrained\nwithin a joint-angular boundary to avoid self-collision.\nThere are several other crucial aspects of a real-world task\nthat are rarely studied in simulations, such as the action cycle\ntime, the medium of connection, the choice of actuation type,\nand concurrency and delays in computation. These aspects\nare the main focus of the current work.\nIV. ELEMENTS OF A REAL-WORLD TASK SETUP\nIn this section, we describe the key elements of setting up\na real-world learning task, different choices for each element,\nand the choice we make for our baseline UR5-Reacher setup.\nA. Concurrency, ordering and delays of computations\nIn simulated tasks, it is natural to perform agent and\nenvironment-related computations synchronously, which may\nnot be desirable in real-world tasks. Figure 2(a) shows\nthe computational steps that are executed sequentially in a\ntypical simulated experiment at each episode. The ﬁrst four\n(1-4) computational steps are environment related whereas\nthe last two (5, 6) are agent related. The simulated world\nadvances discretely in time during Step 2 and does not\nchange during the rest. This way, simulated tasks can comply\nwith the MDP framework, in which time does not advance\nbetween observing and acting.\nIn real-world tasks, time marches on during each agent\nand environment-related computations. Therefore, the agent\nalways operates on delayed sensorimotor information. The\nAt each step t\n1. Compute the control variable \nof the simulator using At\n2. Advance simulation by constant time \nand set t ← t + 1\n3. Extract sensorimotor information\n4. Construct ot and Rt \n5. Make a learning update (optionally)\n6. Update At based on ot \na) Sequential computations \nin simulated tasks\nb) Concurrent computations in real-world tasks\nRobot communication process\nForever:\nI. Wait for a sensory packet\nII. Extract sensorimotor \ninformation (3)\nSensor thread\nForever:\nI. Wait for a sensory packet\nII. Send actuation to robot (1)\nActuator thread\nForever:\nI. Update the action based on \nthe observation vector (6)\nII. Make a learning update \n(optionally) (5)\nIII. Wait to complete the \naction cycle (2)\nForever:\nI. Construct the observation \nvector and reward (4)\nII. Check safety constraints\nIII. Update the actuation \ncommand using action (1)\nIV. Wait for a minuscule delay\nEnvironment thread\nAgent thread\nSensorimotor\ninformation\nActuation \ncommand\nReinforcement learning process\nFig. 2.\na) In typical simulated learning tasks, the world stands still during computations, and they can be executed sequentially without any consequence\nin learning performance. b) The real world moves on during all computations and executing them concurrently can be desirable for minimizing delays.\noverall latency can be further ampliﬁed by misplaced syn-\nchronization and ordering of computations, which may result\nin a more difﬁcult learning problem and reduced potential for\nresponsive control. Therefore, a design objective in setting\nup a learning task is to manage and minimize delays.\nDifferent approaches are proposed to alleviate this issue\n(Katsikopoulos & Engelbrecht 2003, Walsh et al. 2009), such\nas augmenting the state space with actions or predicting\nthe future state of action execution. These approaches do\nnot minimize the delay but compensate for it from the\nperspective of learning agents. A seldom discussed aspect\nof this issue is that different orderings or concurrencies of\ntask computations may have different overall latencies.\nIn UR5 Reacher, we implemented the computational steps\nin Python and distributed them into two asynchronous pro-\ncesses: the robot communication process and the reinforce-\nment learning (RL) process. They exchange sensorimotor\ninformation and actuation commands. Figure 2(b) depicts\nthe computational model of UR5 Reacher, which may also\nserve as a computational model for other real-world tasks.\nSome of the computational steps in 2(b) end with step\nnumbers from 2(a) when they are directly relevant. The robot\ncommunication process is a device driver which collects\nsensorimotor data from URControl in a sensor thread at\n8ms cycle time and sends actuation commands to it in\na separate actuator thread. The RL process contains an\nenvironment thread that checks spatial boundaries, computes\nthe observation vector and the reward function based on UR5\nsensorimotor packets and updates the actuation command\nfor the actuator thread based on actions in a fast loop. The\nagent thread in the RL process deﬁnes task time steps and\ndetermines the action cycle time. It makes learning updates\nand computes actions using the agent’s policy pass.\nFor the learning agent, we use the rllab implementation\nof TRPO. It performs computationally expensive learning\nupdates infrequently, once every few episodes. We scheduled\nthese updates between episodes to ensure that they do not\ninterfere with the normal course of the agent’s sensorimotor\nexperience. Thus, learning updates of TRPO occur in the\nagent thread but not every action cycle or time step.\nOur computational model of real-world tasks in Figure\n5(b) suggests concurrency and certain ordering of compu-\ntations to avoid unnecessary system delays. For example,\nsplitting the robot communication process into two threads\nallows asynchronous communication with physical devices.\nSplitting the RL process into two threads allows checking\nsafety constraints faster than and concurrently with action\nupdates. Moreover, we suggest making learning updates after\nupdating the action, unlike Step 5 and 6 of simulated tasks\n(Figure 5a) where they are computed in the opposite order.\nThis helps to dispatch actions as soon as they are computed\ninstead of waiting for learning updates, which may increase\nobservation-to-action delays. This computational model also\nextends to robotic tasks comprising multiple devices by\nhaving one robot communication process per device, which\nallows the agent to access sensorimotor information fast.\nB. The medium of data transmission\nIt is natural to consider pairing a mobile robot with limited\nonboard computing power with a more computationally\npowerful base station via Wi-Fi or Bluetooth rather than USB\nor Ethernet. Wi-Fi commonly introduces variability in the\ninter-arrival time of streamed packets. In UR5 Reacher, the\nrobot communication process communicates with URControl\nover a TCP/IP connection. We use an Ethernet connection for\nour baseline setup as communicating over Ethernet allows\ntighter control of system latency. However, we also test\nthe effect of using a Wi-Fi connection. Figure 3 shows the\nvariability in packet inter-arrival times for both wired and\nwireless connections measured using 10,000 packets. Packets\nare sent once every 8ms by URControl. The Inter-arrival time\nis consistently around 8ms for the wired connection with\nall times between [7.8, 8.6] ms. For the wireless connection,\nthere is much more variability with the complete range\nvarying between [0.2, 127] ms.\nC. The rate of sending actuation commands to the Robot\nDifferent robotic devices operate in different ways. Some\nrobots allow external computers to write directly in its\ncontrol table. The robot controller controls the actuators\nbased on the control table and does not wait for instructions\n5\n6\n7\n8\n9\n10\nPacket inter-arrival times (ms)\nWireless\nWired\nFig. 3.\nPacket inter-arrival times for the wired and the wireless connections\nused with UR5. The box plots show different percentiles (5, 25, 50, 75, 95).\nfrom the external computer. Some other robots such as\nUR5 provide an interface where the controller controls the\nactuators based on actuation commands repeatedly sent by\nan external computer. We refer to the transmission of these\ncommands from an external computer to the robot as robot\nactuations. In UR5 Reacher, we choose the robot-actuation\ncycle time to be the default of 8ms.\nD. The action cycle time\nThe action cycle time, also known as the time-step du-\nration, is the time between two subsequent action updates\nby the agent’s policy. Choosing a cycle time for a particular\ntask is not obvious, and the literature lacks guidelines or\ninvestigations of this task-setup element. Shorter cycle times\nmay include superior policies with ﬁner control. However,\nif changes in subsequent observation vectors with too short\ncycle times are not perceptible to the agent, the result is\na learning problem that is hard or impossible for existing\nlearning methods. Long cycle times may limit the set of\npossible policies and the precision of control but may also\nmake the learning problem easier. If the cycle time is\nprolonged too much, it may also start to impede learning\nrate by slowing down the data-collection rate.\nIn our concurrent computational model, it is possible to\nchoose action cycle times that are different than the robot-\nactuation cycle time. When the action cycle time is longer,\nthe actuator thread repeats sending the same command to\nthe robot until a new command is computed based on a new\naction. This may fortuitously beneﬁt agents with long action\ncycle times. For example, to reach the target quickly, an arm\nmay gain momentum by repeating similar robot actuations\nmany times. Agents with a short cycle time must learn to\ngain such a momentum, while agents with a long cycle time\nget it for free by design. We choose 40ms action cycle time\nin our baseline setup and compare the effects of both shorter\nand longer cycle times.\nE. The action space: position vs velocity control\nChoosing the action space can be difﬁcult for real-world\ntasks as physical-robot controllers are usually not designed\nwith the learning of low-level control in mind. A control vari-\nable which has a strong cause-and-effect relationship with\nthe robot’s state in the immediate future is an appropriate\nchoice for actions. Torques or accelerations are often chosen\n0\n2\n4\n6\n8\nLag (packets)\n0.0\n0.2\n0.4\n0.6\n0.8\nCross-\ncorrelation\nTarget torque\nVelocity\nPosition\n0\n2\n4\n6\n8\nLag (packets)\n0.0\n0.2\n0.4\n0.6\n0.8\nTarget acceleration\n0\n2\n4\n6\n8\nLag (packets)\n0.0\n0.2\n0.4\n0.6\n0.8\nMeasured current\nCorrelations between actions (velocity and position controls) and\nsubsequent motor signals (target torque, target acceleration and measured current)\nFig. 4.\nWe compare actions based on velocity and position controls by\nshowing their cross-correlations with three motor signals of UR5: target\ntorque (left), target acceleration (middle), and measured current (right).\nas actions in simulated robot tasks. We are interested in\nusing a low-level actuation command for controlling UR5\nto make the task similar to Gym Reacher. UR5 allows\nboth position and velocity controls by sending commands\nevery 8ms. We were able to use velocity control directly as\nactions, but using position control directly was not feasible.\nThe randomly-generated initial policies with direct position\ncontrol generated sequences of angular position commands\nthat caused violent, abrupt movements and emergency stops.\nWe choose direct velocity control for our baseline setup\nand compare with a smoothed form of position control. To\navoid abrupt robot movements with direct velocity control,\nwe only needed to restrict the angular speeds between\n[−0.3, +0.3] rad/s and the leading-axis acceleration to a\nmaximum of 1.4 rad/s2. With position control, we needed to\napply smoothing twice, where actions become a proxy to the\nsecond derivative of desired positions; applying smoothing\nonce could not avoid abrupt movements. Our smoothing\ntechnique can be described as follows:\nyt = clipymax\nymin (yt−1 + τzt),\n(1)\nqdes\nt\n= clipqmax\nqmin (qt + τyt).\n(2)\nHere t is the agent time step, z is the action vector, q is the\nmeasured joint positions, qdes is the desired joint position\nsent as the position-control command, and y is the ﬁrst\nderivative variable. The clipb\na operator clips a value between\n[a, b]. We set τ to be the action cycle time, ymax = −ymin =\n1, and (qmin, qmax) according to the angle safety boundary.\nWe choose the default value for the gain of the position-\ncontrol command according to the URScript API.\nURControl further modulates both position and velocity\ncommands before moving the robot. Figure 4 shows the\ncross-correlation between both action types and three differ-\nent motor signals based on data collected by a random agent\nwith 8ms action cycle time. These motor signals are target\nacceleration, target torque and measured current, which are\nmore closely related to subsequent motor events than other\nsignals in UR5. Both actions had their highest correlations\nwith motor signals after two packets for target acceleration\nand torque, and after three packets for measured current.\nThis observation has driven the choice of our baseline cycle\nbaseline (wired)\naction-update delay \n(80ms mean)\nwireless\nrobot-actuation delay (2ms mean)\nsmoothed \nposition control\nvelocity control\n4 runs with \nsame seed\n4 runs with \nanother seed\n80ms\n160ms\n8ms\n40ms\n8ms\n40ms\n40ms\n8ms\nFig. 5.\nLearning performance in different UR5 Reacher setups. a) Our baseline setup allowed highly repeatable experiments with effective learning\nperformance, where independent runs with same seeds provided similar learning curves. b) Using a wireless connection or applying different sources of\nartiﬁcial delays were detrimental to learning in different degrees. c) Using too short or long action cycle times resulted in worse learning performance\ncompared to our baseline setup (40ms). d) Velocity control outperformed smoothed position control signiﬁcantly with two different action cycle times.\ntime to be longer than 8ms. Velocity control had a higher\ncorrelation with motor signals than position control except\nfor target torque. All the correlations for velocity control\nwere concentrated in a single time shift whereas correlations\nof position control linger for multiple consecutive time shifts,\nindicating a more indirect relationship.\nV. IMPACTS OF DIFFERENT TASK-SETUP ELEMENTS\nWe make variations to our baseline task setup to investi-\ngate the impact of different elements. In each variation of\ntask setups, we used TRPO with the same hyper-parameters:\na discount factor of 0.995, a batch-size of 20 episodes, and\na step size of 0.04, which had the best overall performance\non ﬁve different robotic Gym tasks. The policy is deﬁned\nby a normal distribution where the mean and the standard\ndeviation are represented by neural networks. Both policy\nand critic networks use two hidden layers of 64 nodes each.\nFor each experiment, we run ﬁve independent trials and\nobserve average returns over time.\nNeural networks are notorious for their dependence of\nperformance on initial weights. Recently, Henderson et al.\n(2017) reminded how easily wrong conclusions could be\ndrawn from experiments in which deep reinforcement learn-\ning methods were not applied carefully. It was exempliﬁed by\nshowing that the same algorithm may appear to achieve sig-\nniﬁcantly different performance if the experiment is repeated\nusing different sets of randomization seeds. Therefore, two\ndifferent methods or setups can seem signiﬁcantly different\nsimply due to random chance ensuing from different pseudo-\nrandom number sequences between them. We took extra\ncaution in setting up our experiments to ensure that in each\ntask-setup variation the same set of ﬁve initial networks and\nﬁve sequences of target positions were used.\nTo validate the correctness of our experimental setup, we\nrepeated the baseline experiment four times, as shown in\nFigure 5(a) for two different seeds. Each trial consists of\n150,000 time steps or 100 minutes of agent-experience time\nand about three hours of total real-time including resets.\nOver time each learning curve improved signiﬁcantly, and\nthe agent achieved higher average returns resulting in an\neffective and consistent reaching behavior, as shown in the\ncompanion video1. Notably, all learning curves were quite\nsimilar to each other for the same seed even though they\nwere generated by running each trial for multiple hours on\ndifferent days and physical UR5 units. This is a testament\nto the precision of UR5, the stability of TRPO, and the\nreliability of our experimental and task setups.\nFigure 5(b) shows the impact of using a wireless con-\nnection. The solid lines are average returns, and the shaded\nregions are standard errors. The wireless connection re-\nsulted in signiﬁcant deterioration of performance compared\nto our baseline setup with a wired connection, which can\nbe ascribed to variabilities and delays in the arrival of\nboth sensorimotor packets to the computer and actuation\ncommands to URControl. To study their impacts we injected\nartiﬁcial exponential random delays, which crudely modeled\nWi-Fi transmission delays, separately in action updates and\nthe sending of actuation commands to URControl. Action-\nupdate delays are in effect similar to observation delays (Kat-\nsikopoulos & Engelbrecht 2003) and can also be caused by\ninefﬁcient implementations. Both delays can make a learning\nproblem difﬁcult by adding uncertainty in how actions affect\nsubsequent observations whereas delaying robot actuations\nmay additionally affect the robot’s operation. Figure 5(b)\nshows that a random action-update delay of mean 80ms\ncaused signiﬁcant deterioration in learning performance. On\nthe other hand, adding a small random robot-actuation delay\nof mean 2ms devastated learning completely.\nIn Figure 5(c), we show the impact of choosing different\naction cycle times. The performance deteriorated when the\ncycle time was decreased to 8ms. On the other hand, the\nperformance improved when it was increased to 80ms,\nbut deteriorated signiﬁcantly from there when increased\nto 160ms. In Figure 5(d), we show learning performance\nof both direct velocity and smoothed position controls for\ntwo different action cycle times: 8ms and 40ms. Smoothed\nposition control performed signiﬁcantly worse than velocity\ncontrol in both cases.\nFinally, we investigated whether our baseline setup re-\nmained effective for six-joint control by applying it to UR5\nReacher 6D. To accommodate the higher complexity of the\nproblem, we explored policy and critic networks with larger\n1 https://youtu.be/ZVIxt2rt1_4\nHidden sizes\n(64, 64)\nHidden sizes\n(256, 256)\nFig. 6.\nLearning in UR5 Reacher 6D with all six joints actuated.\nhidden layers. Figure 6 shows noticeable learning progress\nin 200 minutes, which continued to improve over time. The\nvideo shows effective behavior after running longer.\nVI. DISCUSSION AND CONCLUSIONS\nIn this work, we designed and developed a learning task\nstep-by-step with a UR5 robot to discuss the key elements of\nreal-world task setups. Our discussion is summarized in the\nfollowing hypotheses for real-world robotic learning tasks,\nwhich we used as a guideline for our baseline setup:\n1) System delays occurring in different computational stages\nare generally detrimental to learning. Consequently, wired\ncommunications are preferable to the wireless ones.\n2) Too small action cycle times make learning harder. Too\nlong action cycle times also impede performance as they\nreduce precision and cause slow data collection.\n3) Choosing action spaces where actions are applied more\ndirectly to the robot makes learning easier by having more\ndirect relationships with future observations.\n4) Due to reduced delays, some concurrent computations\nare preferable to sequential computations of conventional\nsimulated tasks.\nWe studied the validity of the ﬁrst three hypotheses by cre-\nating variations to our baseline setup, and our study largely\nsupported them. We demonstrated that learning performance\ncould be highly sensitive to some setup elements, speciﬁcally,\nsystem delays and the choice of action spaces. The perfor-\nmance was less sensitive to different action cycle times in\ncomparison. Our results suggest that mitigating delays from\nany source is likely beneﬁcial, indicating the prospect of the\nlast hypothesis. Our study comprises only a small step toward\na comprehensive understanding of real-world learning tasks,\nwhich requires more thorough investigations and validations\nusing different tasks, learning methods and sets of hyper-\nparameters. Our baseline setup allowed us to conduct highly\nreliable and repeatable real-world experiments using an off-\nthe-shelf RL method. This served as a strong testament to the\nviability of extensive RL experimentations and research with\nreal-world robots despite barriers and frustrations around\nrobots and reproducibility of deep RL research, in general.\nACKNOWLEDGEMENTS\nWe thank Richard Sutton, Lavi Shpigelman, Joseph\nModayil and Cindy Yeh for their thoughtful suggestions and\nfeedback that helped improve the manuscript. We also thank\nWilliam Ma and Gautham Vasan for recording and editing\nthe companion video.\nREFERENCES\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., Zaremba, W. (2016). Openai gym.\narXiv preprint arXiv:1606.01540.\nDegris, T., Pilarski, P. M., Sutton, R. S. (2012). Model-free\nreinforcement learning with continuous action in practice.\nIn American Control Conference, pp: 2177–2182.\nDuan, Y., Chen, X., Houthooft, R., Schulman, J., Abbeel,\nP. (2016). Benchmarking deep reinforcement learning for\ncontinuous control. In International Conference on Machine\nLearning, pp: 1329–1338.\nGu, S., Holly, E., Lillicrap, T., Levine, S. (2017). Deep\nreinforcement learning for robotic manipulation with asyn-\nchronous off-policy updates. In IEEE International Confer-\nence on Robotics and Automation, pp:3389–3396.\nHenderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D.,\nMeger, D. (2017). Deep reinforcement learning that matters.\narXiv preprint arXiv:1709.06560.\nHester, T., Stone, P. (2013). TEXPLORE: real-time sample-\nefﬁcient reinforcement learning for robots. Machine learn-\ning 90(3): 385–429.\nKatsikopoulos, K. V., Engelbrecht, S. E.(2003). Markov deci-\nsion processeswith delays and asynchronous cost collection.\nIEEE transactions on automatic control 48(4): 568–574.\nLevine, S., Finn, C., Darrell, T., Abbeel, P. (2016). End-to-\nend training of deep visuomotor policies. The Journal of\nMachine Learning Research 17(1): 1334–1373.\nRusu, A. A., Ve˘cer´ık, M., Roth¨orl, T., Heess, N., Pascanu, R.,\nHadsell, R. (2017). Sim-to-Real robot learning from pixels\nwith progressive nets. In Conference on Robot Learning,\npp: 262–270.\nSchuitema, E., Wisse, M., Ramakers, T., Jonker, P. (2010).\nThe design of LEO: a 2D bipedal walking robot for online\nautonomous reinforcement learning. In 2010 IEEE/RSJ In-\nternational Conference on Intelligent Robots and Systems,\npp: 3238–3243.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P.\n(2015). Trust region policy optimization. In International\nConference on Machine Learning, pp:1889–1897.\nSutton, R. S., Barto, A. G. (2017). Reinforcement Learning:\nAn Introduction (2nd Ed., in preparation). MIT Press.\nWalsh, T. J., Nouri, A., Li, L., Littman, M. L. (2009). Learn-\ning and planning in environments with delayed feedback.\nAutonomous Agents and Multi-Agent Systems 18.\nYahya, A., Li, A., Kalakrishnan, M., Chebotar, Y., Levine,\nS. (2017). Collective robot reinforcement learning with\ndistributed asynchronous guided policy search. In 2017\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems, pp: 79–86.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2018-03-19",
  "updated": "2018-03-19"
}