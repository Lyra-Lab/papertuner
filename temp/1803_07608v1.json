{
  "id": "http://arxiv.org/abs/1803.07608v1",
  "title": "A Survey of Deep Learning Techniques for Mobile Robot Applications",
  "authors": [
    "Jahanzaib Shabbir",
    "Tarique Anwer"
  ],
  "abstract": "Advancements in deep learning over the years have attracted research into how\ndeep artificial neural networks can be used in robotic systems. This research\nsurvey will present a summarization of the current research with a specific\nfocus on the gains and obstacles for deep learning to be applied to mobile\nrobotics.",
  "text": "arXiv:1803.07608v1  [cs.CV]  20 Mar 2018\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nA Survey of Deep Learning Techniques for\nMobile Robot Applications\nJahanzaib Shabbir, and Tarique Anwer\nAbstract—Advancements in deep learning over the years have attracted research into how deep artiﬁcial neural networks can be used\nin robotic systems. It is on this basis that the following research survey will present a discussion of the applications, gains, and\nobstacles to deep learning in comparison to physical robotic systems while using modern research as examples. The research survey\nwill present a summarization of the current research with speciﬁc focus on the gains and obstacles in comparison to robotics. This will\nbe followed by a primer on discussing how notable deep learning structures can be used in robotics with relevant examples. The next\nsection will show the practical considerations robotics researchers desire to use in regard to deep learning neural networks. Finally, the\nresearch survey will show the shortcomings and solutions to mitigate them in addition to discussion of the future trends. The intention\nof this research is to show how recent advancements in the broader robotics ﬁeld can inspire additional research in applying deep\nlearning in robotics.\nIndex Terms—Deep learning, robotic vision, navigation, autonomous driving, deep reinforcement learning, algorithms for robotic\nperception, Semi-supervised and self-supervised learning, Deep learning architectures, multimodal, decision making and control.\n✦\n1\nINTRODUCTION\n1.1\nDeﬁning Deep Learning in the Context of Robotic\nSystems\nDeep learning is deﬁned as the ﬁeld of science that involves\ntraining extensive artiﬁcial neural networks using complex\nfunctions, for example, nonlinear dynamics to change data\nfrom a raw, high-dimension, multimodal state to that which\ncan be understood by a robotic system [1]. However, deep\nlearning entails certain shortcomings which affect physical\nrobotic systems whereby generation of training data in\noverall is costly and therefore sub-optimal performance in\nthe course of training poses a risk in certain applications.\nYet, even with such difﬁculties, robotics researchers are\nsearching for creative options, for instance, leveraging train-\ning data through digital manipulation, automated training\nand using multiple deep neural networks to improve the\nperformance and lower the time for training [2]. The idea\nof using machine learning to control robots needs humans\nto show the willingness to lose a certain measure of con-\ntrol. This is seemingly counterintuitive in the beginning\nalthough the gain for doing this is to allow the system\nto begin learning on its own [3]. This makes the systems\ncapable of adaptation such that the potential of ultimately\nimproving their direction is that originating from human\ncontrol. This makes deep neural networks well suited to be\nused with robots since they are ﬂexible and can be used\nin frameworks that cannot be supported by other machine\nlearning models [4]. For a long time, the most notable\nmethod for optimization in neural networks is known as the\nstochastic gradient descent. However, improved techniques,\nfor instance, RMSProp, as well as Adam of recent, have\ngained widespread use. Each of the many types of deep\nlearning models is made through the stacking of several\nlayers of regression models [5]. Within these models, distinct\ntypes of layers have undergone evolution for many aims.\n1.2\nForms of Deep Learning Applied to Mobile Robotic\nSystems\nOne type of layer that demands speciﬁc mention is con-\nvolutional layers. Unlike traditional layers that are fully\nconnected, convolutional layers apply the same weights in\norder to operate in all the input space. This brings about a\nsigniﬁcant reduction of the overall number of weights in the\nneural network which is speciﬁcally vital with images that\nnormally compose of hundreds of thousands and millions\nof pixels that require processing [6]. It should be noted\nthat processing these kinds of images which have fully con-\nnected layers would need over 100K2 to 1M2 weights which\nconnect to each layer which makes it entirely impractical.\nThe inspiration of convolutional layers came from cortical\nneurons within the visual cortex which only respond to\nstimuli in a receptive environment. Since convolution es-\ntimates such behavior, convolutional layers can be expected\nto excel at image processing assignments [7]. The pioneer-\ning research in neural networks using convolutional layers\nuses image recognition tasks which we built on the ad-\nvancements of ImageNet recognition competitions around\n2012. The lessons learned in this period gained widespread\ninterest in convolutional layers being able to gain super-\nhuman recognition of images [8]. Currently, convolutional\nneural networks have been come well known and highly\neffective as a deep learning model for many image-based\napplications. These applications comprise of semantic image\nsegmentation, scaling images using super-resolution, scene\nrecognition, object localization with images, human gesture\nrecognition and facial recognition [9] [10]. Images are not the\nonly form of a signal which illustrates the excellence of con-\nvolutional neural networks. Their capability is also effective\nin any form of a signal which demonstrates spatiotemporal\nproximity for example speech recognition as well as speech\nand audio synthesis [11]. Naturally, these have also started\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\nto be dominative in the domain of signal processing and\nheavily used in robotics, for instance, pedestrian detection\nwith the use of LIDAR, mico-Doppler signatures, as well\nas depth-map, estimating [12] [13]. Recent projects have\neven started to integrate signals from several modalities and\ncombine them for uniﬁed recognition and perception [14].\nUltimately, the philosophy that underlies and prevails in\nthe deep learning community is that every component of\na complex system can be taught to ”learn.” Therefore, the\nactual power of deep learning does not come from applying\njust one of the described structures in the previous section\nas a part in robotics systems by in connecting components\nof all these structures to form a complete system that learns\nentirely [15]. This is the point where deep learning starts to\nmake its impact such that each component of the system is\ncapable of learning as a whole and is capable of adapting\nto sophisticated methods. For example, neuroscientists have\neven started recognizing the many patterns evolving in the\ndeep learning community and in all artiﬁcial intelligence\nare starting to mirror patterns previously evolved in the\nhuman brain [16]. In the process of learning complex, high-\ndimensional as well as novel dynamics, the analysis of\nderivatives within these complex dynamics needs human\nexpertise. However, this process normally consumes a lot of\ntime and can bring about a trade-off between the dimension-\nality and tractability of states [17]. Therefore making these\nmodels robust to unforeseen impact is challenging and in\nmost cases, full state information is normally unknown. In\nthis case, systems that are able to rapidly and autonomously\nadapt to modern dynamics are required to solve prob-\nlems for instance moving over services with unknown or\nuncertain attributes, managing interaction in a new envi-\nronment or adapting or degrading robot subsystems [18].\nTherefore, we need methods that are able to accomplish\npossession of hundreds or thousands of degrees of freedom\nand demonstrate high measures of uncertainty which are\nonly available in a state of partial information. On the other\nhand, the process of learning control policies in dynamic\nenvironments and dynamic control systems is able to ac-\ncommodate high measures of freedom for applications such\nas swarm robotics, anthropomorphic hands, robot vision,\nautonomous robot driving and robotic arm manipulation\n[19]. However, despite the advancements gained over the\nyears in active research, robust and overall solutions for\ntasks, for instance, moving in deformed surfaces or navigat-\ning complex geometries with the use of tools and actuator\nsystems have remained elusive more so in novel scenarios.\nThis shortcoming is inclusive of kinematic and path plan-\nning tasks which are inherent in advanced movement [20].\nOn the other hand, in terms of advanced object recognition,\ndeep neural networks have proved to be increasingly adapt\nto the recognition and classiﬁcation of objects. Examples\nof advanced application include recognition of deformed\nobjects and estimation of their state and pose for movement,\nsemantic task, and path speciﬁcation, for example, moving\naround the table [20]. In addition, it includes recognizing the\nattributes of an object and surface whereby for instance a\nsharp object could present a danger to human collaborators\nin certain environments such as rough terrain. In the face\nof such difﬁculties, deep learning models can be used in the\napproximation of functions from sample input-output pairs.\nThese can be the most general purpose deep learning struc-\ntures since there are several distinct functions in robotics\nwhich researchers can use in approximating from sample\nobservations [21]. Certain examples of these observations\nentail mapping from actions to corresponding changes in\nthe stage, mapping these changes in state to actions that\ncan cause it or mapping from force to motion. While in\ncertain cases particular physical equations for such func-\ntions may already be deﬁned, there are several situations\nwhere the environment is highly complex for such equa-\ntions to generate acceptable accuracy [22]. However, in such\nscenarios learning approximation of functions from sample\nobservations can yield accuracy that is signiﬁcantly better. In\nother words, approximated functions do not need to be con-\ntinuous. However, function approximation models are also\nexcellent at classiﬁcation tasks, for instance, determining the\ntype of obstacles before a robot, the overall path planning\nstrategy well suited for present environments or the state of\na certain complex object which the object is interacting with\n[23]. Furthermore, function approximation deep learning\narchitecture using rectiﬁers can model the high coupled\ndynamics of an autonomous mobile robot to solve the\nanalytic derivatives and challenging system identiﬁcation\nproblems. Deep neural networks have superseded other\nmodels in detection and perception since they are capable\nof engaging in direct operation with highly-dimensional\ninput rather than needing feature vectors based on hand-\nengineered designs by humans [24]. This lowers the depen-\ndence on humans such that additional training time can\nbe partially offset by lowering initial engineering efforts\n[25]. Extraction of meaning from video or still imagery is\nanother application where deep learning has gained im-\npressive progression. This process demands simultaneously\naddressing using the four independent factors of object\ndetection and a single deep neural network. These factors\ninclude feature extraction, motion handling, classiﬁcation,\narticulation and occlusion handling [26]. Uniﬁed systems\nlimit suboptimal interactions between normally separate\nsystems by predicting the physical results of dynamic scenes\nusing vision alone. This is based on the actions of humans to\nbe able to predict the results of a dynamic scene from visual\ninformation, for example, a rock falling down and impacting\nanother rock [27]. It is therefore on this premise that deep\nlearning has been identiﬁed as being effective in managing\nmultimodal data generation in robotic sensor applications.\nThese applications include integration of vision and haptic\nsensor data, incorporating depth data and image informa-\ntion from RGB-D camera data. Due to the extensive number\nof meta-parameters, deep neural networks have evolved\nsomewhat a reputation of being challenging for non-experts\nto be used effectively [28]. However, such parameters also\navail signiﬁcant ﬂexibility which is a vital factor in their\ngeneral success. Therefore, training deep neural networks\nneeds the user to be able to develop at least an elementary\nlevel of familiarization with many concepts. Speciﬁcally,\napplying these techniques will help in tacking advanced\nobject recognition challenges and reduced the extent of the\nentire changes as well [29].\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\n2\nDEEP LEARNING FOR ROBOTIC PERCEPTION\n2.1\nCurrent Robotic Perception Trends\nAlthough current trends are more leaning to deep and\nbig models, a simpliﬁed neural network with just a single\nhidden layer and a basic sigmoid shaped activation function\nwill train faster and provide a baseline that is used to give\nmeaning to any deeper model improvements. When we\nuse deeper models, Leaky Rectiﬁers are able to normally\npromote faster training by lowering the impact of the dimin-\nishing gradient challenge and improving accuracy through\nusing simpliﬁed monotonic derivatives [30]. Furthermore,\nsince models with additional weights have increased ﬂex-\nibility to over ﬁtting training data, regularization is a vi-\ntal technique in training the best model. In addition, an\nelastic net is a combination of well-established regulariza-\ntion methods used in promoting robustness against weight\nsaturation and also promotion sparsity in weights [31].\nHowever, newer regularization methods inclusive of drop-\nout and drop-connect has attained even better empirical\noutcomes. Furthermore, many regularization methods are\nalso in existence in speciﬁcally improving the robustness\nof autoencoders. In this case, special-purpose layers can\nalso make a signiﬁcant distinction with deep neural net-\nworks [32]. It is a common method to alternate between\nconvolutional and max pool layers. These pool layers can\nlower the general number of weights in the network and\nalso allow the model to be able to recognize objects inde-\npendent of where they are placed in the visual ﬁeld [33].\nOn the other hand, batch normalization can provide us\nwith signiﬁcant improvements in rating the convergence\nby ensuring the gradient in range affects the weights of\nall neurons. In addition, residual layers can allow a deeper\nand consequently more ﬂexible model to be trained [34]. To\nmake effective use of deep learning models, it is vital to\ntrain one or many General Purpose Graphical Processing\nUnits since the other methods of parallelization of deep\nneural networks have been tried but none of them have\nyet provided gains in beneﬁcial performance of General\nPurpose Graphical Processing Units [35]. For a long time\nuntil in recent years, robots have long been used in in-\ndustrial environments. In industrial environments, robotic\nsystems are pre-programmed with repetitive assignments\nwhich lack the capability of autonomy and as such operate\non the basis of a structured approach. Such an environment\ncannot be adaptive for a mobile robot since it eliminates the\nneed for autonomy. On the other hand, mobile robots need\nless structured environments such that they can be able to\nmake their own decisions such as navigate paths, determine\nwhether objects are obstacles, recognize images and audio\nas well as map their environments. As such, surviving and\nadapting in the real world is more complex for any robotic\nsystem in comparison to the industrial setting since the\nrisk of failure, system error, external factors, obstacles, cor-\nrupt data, human error and unrecognizable environments is\nmore prevalent [15].\n2.2\nMachine Learning Usage in Training Robotic Sys-\ntem Perception\nThe difference between deep learning and machine learn-\ning is that deep learning place emphasis on the subset of\nmachine learning resources and method and uses them to\nsolve any difﬁculties that need ”thought” whether human\nor artiﬁcial. Deep learning is also introduced as a means of\nmaking sense of data with the use of multiple abstraction\nlayers. In the course of the training process, deep neural\nnetworks are able to learn the means of discovering useful\npatterns to digitally represent data such as sounds and\nimages. This is speciﬁcally why we observe more advances\nin the areas of image recognition and natural language\nprocessing originating from deep learning [36]. It is on this\nbackdrop that deep learning has taken the forefront position\nin helping researchers develop breakthrough methods to\nthe perception capabilities of robotics systems [16]. In more\nsimpliﬁed language, perception refers to the functionality of\nrobots being able to detect its surroundings. It is therefore\nheavily reliant on multiple sources of sensory information.\nHowever, with traditional robot technology extracting data\nfrom raw sensor by using rudimentary constructed sensors\ntheseold methods were limited by constraints of adapting\nto generic settings [17]. In situations where these robotic\nsystems faced dynamic environments, they operated in an\nunstructured manner by combining hybrid and autonomous\nfunctionality to process information about their surround-\nings [18]. As such, with deep learning came the introduction\nof new methods of processing data from robotic sensorsof\na robotic systems surroundings using a feature known as\nperception. These methods comprise of robot motion, per-\nception, human-interaction, manipulation and grasping, au-\ntomation, self-supervision, self training and learning as well\nas robot vision [19]. Deep learning models utilize automated\nactions technology at only half the cost by using supervised\nlearning to attain their goals [20]. For example, in order\nto perfect an image recognition application, a neural net\nwill be required to be trained with a collection of labeled\ndata. On the other hand, unsupervised learning is how\ndeep learning operates and allows for the discovery of new\npatterns and insights by tackling problems with little or\nno insight on what the results should be perceived by a\nrobot [21]. The method by which a mobile robot is able\nto detect its environment with the use perception is by\nusing deﬁnitive decision-making policies [20]. For instance,\nmobile robots using deep learning are able to navigate with\nrationality by using motion and track precision sensors\nwhich are driven machine learning algorithms [21]. How-\never, in difﬁcult environments such as a congested room, the\nlevel of accurately perceiving their environment is limited.\nTherefore, deep learning based solutions can tackle this\nchallenge by using artiﬁcial intelligence, high computational\nhardware and processing layers known as deep convolution\nneural networks to solve this dilemma by successfully de-\nciphering intricate environment perception difﬁculties [22].\nThe various obstacles that exist in a robots environment\nare an indication of the immense high-dimensional data\nprocessing capabilitiesrequiredby a robotic system to be able\nto perceive its surroundings [23]. By using self-supervised,\nsemi-supervised and full supervised training coupled with\nlearning, robotic systems are able to utilize their machine\nlearning and pattern recognition capabilities to process raw\ndata such as images, objects, and semantics, audio as well\nas process natural language in the real time. This segment of\ndeep learning is the best since it uses feed-forward artiﬁcial\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nneural networks to successfully analyze visual imagery. It\nalso uses multiple multilayer perceptrons based on designs\nthat need low preprocessing [22]. In comparison to other im-\nage classiﬁcation algorithms, it uses minimal pre-processing\nwhich means that the network is able to learn ﬁlters using\nautomated procedures unlike traditional algorithms that are\nmanually engineered. Therefore, by not relying on past\nknowledge and human efforts in the design of features\nmakes it a major advantage [24]. As such in general, this\nmakes deep learning capable of the extraction of multi-\nlevel attributes from raw sensor data in a direct manner\nwith no need for human assisted robotic control [24]. This\npresents researchers with the implication that deep learning\nprogramming librariessuch as TensorﬂowTheano of Python,\nCaffe of C++, darch in R, CNTK, Convent.js of Javascript,\nand Deeplearning4j derived from C++ and Java among\nothers are extremely of use in providing robotic systems\nwith a platform to develop their sensory data analysis\nand environmentlearningby using deep learning algorithms\n[25]. Robotic system perception concerns auxiliary functions\nwithin mobile robotic are vital in interactingwith a robots\nenvironment.Sensing and intelligent perception are some of\nthe applications which are vital since they determine the\nperformance of a robotic system. These performances are\nlargely dependent on how robot sensors perform. Modern\nsensors and their functionality can provide impressive robot\nperception which is the foundation of self-adaptive as well\nas robotic artiﬁcial intelligence [26]. The process of changing\nfrom sensory input to control output using sensory-motor\ncontrol structures presentsa big difﬁcultyin robotic percep-\ntion [27]. Some of the vital mobile robot components include\nthe manipulator comprising of many joints and connections,\na locomotion device, sensors, a controller and an endeffec-\ntor [37]. Mobile robots are automated systems capable of\nmoving. They also have the ability to move around their\nsurroundings and are not ﬁxed to a single physical location.\nWith these features mobile robots are able to perceive their\nenvironments usingsensory data andautonomous control\ncommands [28].\n2.3\nShortcomings of Deep Learning in Robotic Percep-\ntion\nHowever, certain challenges remain unresolved in these\nrobotic systems particularly the areas of perception and\nintelligent control. Some of these challenges are reﬂected\nin the process needing a lot of data to be able to train and\nteach algorithms progressively. Large datasets are required\nto ensure machines deliver the desired outcomes. In the\nsame way as the human brain needs rich experience to\nlearn and deduce information, artiﬁcial neural networks\nalso need abundant amounts of data. This means for more\npowerful abstractions, more parameters are required and\nhence more data. Another challenge is the tendency of over\nﬁtting in neural networks whereby in certain cases, there\nis a sharp distinction between an error within a training\nset and that encountered into new untrained datasets. This\nproblem arises when many models make the relative num-\nber of parameters fail to reliably perform. Therefore, the\nmodel only memorizes training examples and fails to learn\ngeneralization of new situations and new datasets.\n2.4\nHow Mobile Robotic Perception Models Can Be\nUsed to Attain Complete Situational Awareness\nThese capabilities will be surveyed within the research\nsurvey through deep learning perception model algorithms\nthat are ableto determine how a robot responds to the\ndynamic changes within an environment [29]. The basis\nof these models revolves around control theory afﬁliated\nparadigms for instance system stability, control as well as\nobservation [30]. This theory states that a robotic system\nis able to perceive its environment by using hierarchical\nextensions or enhancements of learning by maximizing the\nrange of its sensor capabilities while using path planning\nalgorithms to maneuver around obstacles or paths [31].\nMost real-time map algorithms are concerned with the\nacquisition of compact 3D mapping within indoor settings\nwith the use of range as well as imaging sensory capabil-\nities [32]. The process of developing models of a robot’s\nenvironment is a vital problem to deal with more so in\nregard to managing its workspace especially when it is\nshared with other machinery [33]. A mobile robot interacts\nwith the environment by using control systems which deﬁne\nstructures or obstacles as geometrical areas so as to be able\nto cover all the likely conﬁgurations on the robot. Objects or\nstructures are deﬁned according to parallelepipeds, spheres,\nplanes, and cylinders. With such a simpliﬁed model, the\nmobile robot system is able to deﬁne many geometrical\nareas of this nature to cover nearly all objects within its\nsurrounding, for example, moving objects, stationary items\nsuch as furniture and machinery. Therefore, we propose\nelementary geometrical volumes as a means of modeling\na mobile robot’s perception capabilities of its environment\n[34]. This method will allow the robot to be able to move\nwithin an environment with the certainty of not colliding\nwith regions that are forbidden; these regions must already\nbe deﬁned, declared and activated so as to be able to\ncorrectly work [35].\nThe geometrical perception algorithm will check if the\nrobot end effector is within the controlled area or warning\nzone. This checking is done through the use of already\nstored geometrical areas already deﬁned by the user [38].\nSimilarly, in this context, the position of the dynamic ob-\nject avails the perception system with the likelihood of\nconnecting the geometrical regions with arbitrary moving\npoints [39]. These points can be read from external sensors\nsuch as encoders. Control of speed is undertaken with the\nuser of geometrical area blocks able to detect the shape\ntypology and choose the correct movement law to be used\nso as to modify the robot override and avoid collisions with\nuser-deﬁned zones [40]. The speed override is transformed\nsmoothly when the robot end effector collides with a spher-\nical zone in accordance with the perception law in (1) [3].\nV = v0.d −r\nδ\n, r ≤d ≤r + δ,\nV = v0, d > r + δ, and\nV = 0, d < r\n(1)\nIn this case, v is represented as the robot end effector ac-\ntual speed override, v0 as the old override, d as the distance\nbetween the robot end-effector and the main spherical area,\nthe thickness of the warning zone is represented as δ while r\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nis the sphere radius [41]. At thestage when the robot meets a\ncylindrical zone, the speed override will be subjected to the\nperception law in (2) [3].\nV = v0.d1 −r\nR −r , 0 ≤z ≤h,\nV = v0.d2\nδ , h ≤z ≤h + δ, −δ ≤z < 0, p ∈cyl, and\nV = v0.d3\nδ , h ≤z ≤h + δ, −δ ≤z < 0, p ∈cyl\n(2)\nThe perception law above represents h as the cylinder\nheight, the position of the robot end effector is denoted as\np, the distance between the center of the cylinder and the\nposition of the robot is denoted as d1 while the d2 represents\nthe distance between the top or bottom base of the cylinder\nas well as the position of the robot [42].\nIn addition, the least position between the position of\nthe robot and the top/bottom circumference points of the\ncylinder base is denoted as d3. Furthermore, the robot speed\noverride coincides with the past speed override at the stage\nwhen the robot end effector is outside the warning zone [43].\n3\nDEEP LEARNING FOR ROBOTIC CONTROL AND\nEXPLORATION\n3.1\nHow Autonomous Robotic Systems Use Deep\nLearning to Control and Explore Their Enivornments\nRealizing the beneﬁts of autonomous robot exploration\npresents robotics researchers with many applications of\nconsiderable community and ﬁnancial impact [44]. Robotics\nresearch relies on perfect knowledge and control of the\nenvironment [45]. The problems related to unstructured\nenvironments are an outcome of the high-dimensional state\nspace as well as the inherent likelihood in mapping sensory\nperceptions on particular states. It should be noted that the\nhigh dimensionality of the state space is representative of\nthe most basic difﬁculty since robots leave highly controlled\nenvironments of a laboratory and enter into unstructured\nsurroundings. For example autonomous unmanned aerial\nvehicles used deep learning to classify terrain and solve any\nexploration shortcomings by generating control commands\nfor its human operator so as to adapt to a certain trade-\noff [46]. The major hypothesis of this approach is therefore\nfor mobile robots to succeed in unstructured surroundings\nsuch that they can carefully choose assignment speciﬁc\nattributes and identify the relevant real-time structures to\nlower their state space without impacting the performance\nof their exploration objectives [47]. Robots perform assign-\nments by exploring their surroundings. As such, given our\nfocus on autonomous mobile exploration, we shall direct\nmost attention to exploration in service of movement, that\nis to say collision-free movement for end-effector placement\n[48]. The challenge of generating such movement is an\nexample of the problem faced in motion planning. Motion\nplanning for robotic systems with many levels of freedom is\ncomputationally challenging even in environments that are\nhighly structured due to the increased-dimensional conﬁg-\nuration space [49]. In addition, unstructured environments\nare associated with the imposition of added difﬁculties in\nmotion generation in comparison to the traditional motion\nplanning process. Furthermore, in environments that are un-\nstructured, a robot is only able to possess limited knowledge\nof its environment, objects are able to change their unknown\nto a known state for the robot by manipulating assignments\nmay using the end effector [50]. However, this capability can\nbe challenged by to a constrained trajectory such that the\nmobile robot is unable to reach a particular location with\nease [51]. Each of these problems makes the challenge of\nmotion generation more complex. The explicit coordination\nof planning and sensing necessary to manage dynamic\nenvironments increases the dimension of the state space. In\naddition, robotic assignment requirements impose stringent\nterms in form of high-frequency feedback [52]. Therefore,\nexisting motion planners tend to make assumptions that\nare highly restrictive for environments that are unstructured\nsince they are highly computationally difﬁcult to satisfy\nin terms of gaining operator-to-robot feedback [53]. These\nassumptions, as well as the computational difﬁculty, are an\noutcome of the foundation of motion planning with its high-\ndimensional conﬁguration spacing which makes it highly\nunsuited in solving space problems. Planners instead can\nbe able to solve this paradigm by solely using workspace\ninformation for collision avoidance. Nearly all real-world\nsurroundings, however, comprised of a considerable mea-\nsure of structure [54]. For instance, buildings are segmented\ninto hallways, doors, and rooms; outdoor environments\ncomprise of paths, streets, and intersections while objects\nfor example tables, shelves and chairs have more favorable\napproach directions. This information, however, is neglected\nwhen robot exploration planners exclusively operate on the\nbasis of a conﬁguration space [55]. Therefore, the outcome\nfor most robotic exploration planners is to operate by the\nassumption that any environment is perfectly deﬁned and\nsustained as static in the course of planning [56].\n4\nDEEP LEARNING IN ROBOTIC ROBOTIC NAVI-\nGATION AND AUTONOMOUS DRIVING\nUsing deep learning to attain autonomous driving assign-\nment is not a perfectly controlled and modeled task as most\npeople think. Instead, it needs optimal perceptual capabili-\nties [57]. The process of perception of a robots environment\nand interpreting the information it acquires allows it to\nunderstand the condition of its surroundings, devise plans\nto change the state and observe how its actions impact\nits environment. In unstructured environments, recognition\nof objects has been proven to be highly challenging. With\nimmense volumes of sensor data and increased variation\nof objects within similar object categories, for example, a\npaved and unpaved road as well as recognizing objects\n[58]. Deep learning uses machine learning motion capturing\nabilities as well as optimal perceptual functionalities. This\nis a prerequisite of several vital applications for robots\nfor instance ﬂexible manufacturing, planetary exploration,\ncollaboration with human experts and elder care [59]. The\nchallenge of driving in an environment includes problems\nof movement of the robot in navigating varied obstacles\nby pushing and pulling. Even in structured environments,\nautomated driving is difﬁcult due to the complexities of\nthe related state space [60]. This state space comprises of\nappearance, dimension, position as well as the weight of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nobjects within the scene. It also comprises of several other\nrelevant attributes which provide indications of where to\npull, push or grasp as well as the level of force to apply [61].\nDeep learning and associated machine learning collective\nactions improved the performance of robots in making the\ndecision by embedding the capability within these systems\nto choose between possible actions and determine the re-\nquired parameters for their controllers.\n4.1\nHow Autonomous Robotic Systems Use Deep\nLearning to Navigate Their Enivornments\nAutonomous driving in unstructured environments faces\nmany challenges which do not exist in structured envi-\nronments. In unstructured environments, object attributes\nneeded for driving cannot be deﬁned as priori. Informa-\ntion concerning objects has to be gained through sensors\neven though these are normally ambiguous and therefore\nintroduce uncertainty and avail information that is redun-\ndant. Furthermore, autonomous driving in unstructured\nand dynamic surroundings normally needs responding in\na fashion that is timely to a rapidly transforming environ-\nment [62]. Problems of autonomous driving can be made\nsimple through the exploitation of the structure which is\ninherent to human surroundings. Most objects in the real\nworld are based on designs to perform certain functions\nwith the intention of being utilized by humans [44]. As\nan outcome, several real-world objects can share common\nattributes which allude to their intended usage. By plac-\ning emphasis on these assignment-related object attributes,\nthe complexity of autonomous driving is lowered. For ex-\nample, visual data can be analyzed for the identiﬁcation\nof few points which correspond to positive locations at\nwhich a robot can maneuver. Furthermore, since movement\nfeatures are similar across many objects, robots could be\ntrained to identify them. As a consequence, the state space\nthat requires exploration in order to move is signiﬁcantly\nlowered [63]. In this case, researchers normally make as-\nsumptions to lower the complexity of autonomous driving\nin unstructured environments. For instance, it is normally\nassumed that full models of objects in the environment can\nbe availed a priori or it can be gained through sensors\nwhile the environment remains the same in the process of\ninteraction [64]. However, in practical terms, it is impossible\nto avail autonomous driving with full priori models in the\nactual world. However, models that are perfect are not a\nprerequisite for successful autonomous driving [65]. Robot\nmobile driving can be guided using existing structures in\nthe world and in most cases those which are easy to per-\nceive. As such, by leveraging this structure, the complexity\nof autonomous driving in unstructured environments is\nlowered signiﬁcantly. Similarly, understanding the intrinsic\nmeasures of freedom of objects in an environment is also\nable to lower the complexity of autonomous driving in\nunstructured surroundings [45].\n5\nSEMI-SUPERVISED\nAND\nSELF-SUPERVISED\nLEARNING FOR ROBOTICS\nImitation based learning is a promising approach to tackling\nthe difﬁcult robotic assignments, for instance, autonomous\nnavigation. However, it needs human supervision to over-\nsee the process of training and sending correct control\ncommands to robots without feedback. It is this form of\nprocedure that is prone to failure and high cost [66]. There-\nfore, in order to lower human involvement and limit manual\ndata labeling of autonomous robotic navigation using imi-\ntation learning, the techniques of semi-supervised and self-\nsupervised learning can be introduced. It should be noted\nhowever that these techniques need to operate according\nto a multi-sensory design approach. The solution should\ncomprise of a suboptimal sensor policy founded on sensor\nfusion and automatic labeling of states the robot could\nencounter [67]. This is also aimed at eliminating human\nsupervision in the course of the learning process [68] [69].\nFurthermore, a recording policy needs to be developed to\nprovide throttling of the adversarial impact of too much\ndata being learned from the suboptimal sensor policy. As\nsuch, this solution will equip the robot with the capability\nof achieving near-human performance to a large extent in\nmost of its assignments [70]. It is also capable of surpassing\nhuman performance in situations of unexpected outcomes\nfor instance hardware failure or human operator error. Fur-\nthermore, the semi-supervised method can be considered\nas a solution to the problem of track classiﬁcation in con-\ngested environments such as a room. This problem entails\nobject classiﬁcation undergoing segmenting and tracking\nwithout using class models [71]. Therefore, we introduce\nsemi-supervised learning as a technique capable of solving\nthis problem by iteratively training a classiﬁer and extract-\ning vital training examples from the data in its unlabeled\nstate. This is achieved by exploiting the tracking data. In\naddition, the process also involves evaluating large multi-\nclass difﬁculties presented by data sourced from congested\nartiﬁcial natural environments such as a street [72]. As\nsuch, when provided with manually labeled training tracks\nof individual object classes, then semi-supervised learning\nperformance in comparison to self-supervised learning is\nable to use thousands of training tracks [73]. In addition,\nwhen also provided with augmented unlabeled data, semi-\nsupervised learning has demonstrated the capability of\noutperforming the self-supervised learning method. In this\ncase, semi-supervised learning presents itself as the most\nsimpliﬁed algorithmic approach to speeding up incremental\nupdating of booster classiﬁers by lowering the learning time\nfactor by three [74].\n6\nMULTIMODAL DEEP LEARNING METHODS FOR\nROBOTIC VISION AND CONTROL\nPerforming tasks in an imperfect controlled and modeled\nsurrounding means robots need to have optimal multimodal\ncapabilities. The procedure of perceiving the environment\nand interpreting the gained information allows robots to\nperceive the state of the environment, devise methods to\nalter the state and observe the impacts of their actions on\nthe environment\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\n6.1\nHow Autonomous Robotic Systems Use Semi-\nSupervised and Self-Supervised Learning to Learn Their\nEnivornments\nThe environment of a robot can be controlled too many\nlevels. In principle, less constrained environments are more\ndifﬁcult to perceive. In the real-world and its unstructured\nand dynamic surroundings such as vegetation landscape\nand terrain, the perception of a mobile robot needs to be\ncapable of navigating this unknown environment by using\nsensor modalities. More so, even without the introduction of\nuncertainty, sensors in themselves are ambiguous [75]. For\nexample, a lemon and a soccer ball can look similar from\na certain perspective. In addition, a cup could be invisible\nin case the cupboard is shut and it can be challenging\nto tell the difference between a remote control and cell\nphone is they are both facing down. These factors are all\ncontributive to the challenges of perceiving the state of the\nenvironment. Furthermore, for example, advances in face\nrecognition normally operate under the assumption con-\ncerning the position and orientation of the individual in the\nimage. The outcomes of object segmentation are normally\nfounded on the capability of telling the difference between\nan object and background on the basis of differences in color\n[76]. In addition, object recognition is normally reduced to\nsimilarities in computing to a limited collection of given\nobjects. On the other hand, in an unstructured environ-\nment, the position and orientation are uncontrollable since\nassumptions concerning color and shades and problematic\nto justify. Furthermore, the range and likely objects the robot\ncould encounter are intractable [77].\n6.2\nHow Autonomous Robotic Systems Use Multi-\nmodal and Deep Learning Methods to Percieve Their\nEnivornments\nTherefore, in order to tackle perception in unstructured\nsurroundings, robots need to be able to lower the state\nspace that requires being analyzed. to provide facilitation of\ncertain perceptual assignments by limiting uncertainty and\nas such lowering the dimensionality of the state space. For\ninstance, in order to compute the distance of objects in an en-\nvironment, robots need to relate depth to visual information\n[78]. This is normally done with the use of a stereo vision\nsystem and solving the correspondence challenge between\ntwo static 2D images. In addressing the correspondence\nproblem, however, it is complicated due to noise, many\nlikely matches and the uncertainty in calibrating the camera\n[79]. On the other hand, in a system capable of the capture of\nat least three view angles in one image, this lowers the state\nspace by reducing a multi-sensor system to a single sensor.\nFurthermore, in an unstructured environment, recognition\nof objects has proven to be highly challenging [57]. This\nis due to large volumes of sensor data and an increased\nvariation within objects of a similar category. In this case,\nobject recognition is an increased dimensional challenge.\nHowever, even in the face of these challenges, objects in\nthe similar category do share similar attributes. As such, by\napplying this insight, robots are able to place emphasis to\nonly a minimal subset of the state space that comprises of\nthe most relevant characteristics for classiﬁcation [58].\n7\nAPPLICATION OF DEEP MODELS TO PROBLEMS\nIN VISION AND ROBOTICS\nThe preceding overview of machine learning applications\nin robotics will highlight ﬁve major areas where consid-\nerable impacts have been made by robotic technologies\ncurrently and in the development levels for long-term use.\nHowever, by no means inclusive, the aim of this summa-\nrization is to provide the reader with a preview of the\nform of machine learning applications in existence within\nrobotics and motivate the desire for extended research in\nsuch and other ﬁelds [80]. The growth of big data which\nis to day visual information provided on the internet with\nthe inclusion of annotated images and video has pushed\nforward advancements in computer vision which has in\nturn assisted in extending machine-based learning systems\nto prediction learning methods such as those presented by\nresearch at Carnegie Mellon [81]. This presentation involved\nunveiling offshoot examples such as the anomaly detection\nusing supervised learning have been applied in building\nstructures with the capability of searching and assessing\ndamages in silicon wafers with the use of convolutional\nneural networks [81]. In addition, extrasensory technologies\nfor instance lidar, ultrasound, and radar such as those\ndeveloped by Nvidia as also propelling the creation of 360-\ndegree vision-based systems for autonomous vehicles and\nUAVs [82]. Imitation learning which is closely associated\nwith observational learning is also a ﬁeld categorized by\nreinforcement learning or the difﬁculties of gaining an\nagent to act towards maximizing rewards. One example\nis Bayesian or probabilistic models which stand out as a\ncommon machine learning method used an integral compo-\nnent of ﬁeld robotics where attributes of mobility in ﬁelds\nsuch as construction, rescue, and inverse optimal control\nmethods have been utilized in humanoid robotics, off-\nroad terrain navigation, and legged locomotion [83]. Self-\nsupervised learning is another method that allows robots\nto generate personalized training instances so as to reﬁne\nperformance. This has been integrated robots and optical\ndevices for instance in detection and rejection of objects\nsuch as dust and snow, identiﬁcation of obstacles, vehicle\ndynamics modeling, and 3D-scene analysis [84]. Assistive\nand medical technologies are another application where\nassistive robots entail devices capable of sensing, processing\nsensory data and undertaking actions that gain individuals\nwith disabilities. Even as smart assistive technologies are\nexistent for the overall population, for instance, driver assis-\ntance resources, movement therapy robots avail diagnostic\nor therapeutic gains. Furthermore, multi-agent learning con-\ncerning coordination and negotiation are vital components\ninvolving machine learning based robots also known as\nagents. This method is broadly utilized in games that are\ncapable of adapting to a transforming landscape of other\nrobots or agents and searching for equilibrium strategies\n[76].\nThe interdisciplinary arena of computer vision con-\ncerned with how computers are able to be developed for at-\ntaining an increased measure of perception from the digital\nimagery of video. Computer vision assignments comprise\nof techniques for acquisition, processing, analysis, perceiv-\ning digital imagery as well as extracting high dimensional\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\ninformation from the actual world so as to yield numerical\nor symbolic data for example in the format of decisions [85].\nArtiﬁcial intelligence areas concerned with autonomous\nplanning or deliberated robotic systems navigation demand\na thorough perception of such settings since information\nconcerning the environment can be availed by a computer\nvision system in action as a vision sensor [57]. Therefore,\nartiﬁcial intelligence, as well as computer vision, share\nother ﬁelds, for instance, pattern recognition and learning\nmethods. The consequence is that in certain cases, computer\nvision is viewed as a component of artiﬁcial intelligence.\nAnother application of computer vision is solid state physics\nsince a large portion of computer vision systems are reliant\non imagery sensors that provide detection of electromag-\nnetic radiation which normally takes a form that is either\nvisible or infra-red lighting [81] [86]. These sensors operate\naccording to quantum physics designs with the procedure\nby which light interacts with the surface better explained by\noptics behaviors. Such intricate inner working demonstrates\nhow even complex image sensors even need quantum me-\nchanics to avail a full understanding of the process of image\nformation. Furthermore, another application of computer\nvision is the multiple measurement challenges in physics\nwhich can be tackled by utilizing computer vision for in-\nstance ﬂuids motion [39].\n8\nBENEFITS AND DRAWBACKS OF DEEP LEARN-\nING TO BE APPLIED IN MOBILE ROBOTS\n8.1\nBeneﬁts of Deep Learning in the Context of Mobile\nRobots\nThe gains of deep learning as a component of the wider\nfamily of machine learning techniques founded on repre-\nsentations of learning data in opposition to assignment-\nparticular algorithms through supervised, unsupervised\nand semi-supervised learning allows for structured on the\ninterpretation of information processing and patterns of\ncommunications which can be viewed as trials at deﬁning\na relation between multiple stimuli and related neuronal\nresponses [87]. Deep learning architectures for instance deep\nneural, deep beliefs as well as recurrent neural networks\nhave been utilized in arenas inclusive of computer vision,\nnatural language processing, social network ﬁltering, speech\nrecognition, bioinformatics and audio recognition. In these\nmentioned ﬁelds, deep learning architecture has produced\noutcomes in comparison to an in certain case more advanced\nto human expertise. Furthermore, deep learning algorithms\nutilize a cascade of many nonlinear processing unit layers\nfor extraction of features and transformation with particular\nlayers applying the output from the past layer as input [88].\nDeep reinforcement learning proposes a simpliﬁed con-\nceptual light framework that utilizes asynchronous gradient\ndescent to cater for deep neural network controller opti-\nmization. The presented asynchronous variants in standard\nreinforcement learning algorithms reveal that parallel actor\nlearner holds a stabilizing inﬂuence on training which al-\nlows for the successful training of the neural network con-\ntroller [19]. It is on this premise that asynchronous variants\nare presented as the most appealing Deep reinforcement\nlearning approach.\n8.2\nDrawbacks of Deep Learning in the Context of Mo-\nbile Robots\nThe drawbacks of deep learning in applied robotics is that\nthe storage of the agent data using replay memory does not\nallow for re-batching or sampling at randomly from varied\ntime-stages. As such, memory aggregation in this approach\nlowers non-stationary and eroded updates while in simulta-\nneously limiting the techniques to off-policy reinforcement\nlearning algorithms [23].\n9\nCONCLUSION\nDeep learning is set to transform the arena of artiﬁcial\nintelligence as well as represent a measure in the direction\nof developing autonomous systems with an increased scope\nof perceiving the visual world. Presently, deep learning\nis allowing scaling of challenges that were traditionally\nintractable for instance learning to directly play video games\nfor pixels. Furthermore, deep learning algorithms are also\nutilized in robotics to foster the capability of control func-\ntionality for robots indirectly learning from cameral inputs\nin the actual world. It is on this premise that the sur-\nvey above illustrates the major advances and approaches\nof reinforcement learning in regard to the main streams\nof value and policy-driven methods as well as associated\ncoverage of central algorithms in deep learning for instance\ndeep network, asynchronous advantage actor-critic as well\nas trust region policy optimization. Furthermore, the re-\nsearch survey has highlighted the gains of deep neural\nnetworking with emphasis on visual perception through\ndeep learning. One of the core objectives of the discipline\nof artiﬁcial intelligence it the production of completely au-\ntonomous agents that are able to interact with their settings\nto learn optimal behavior and demonstrate improvements\nwith time by trial and error regiments. It is therefore on\nthis premise that creating artiﬁcial intelligence systems that\nare responsive and with the capability of learning has long\nbeen an elusive challenge. However, hope is found in the\nprincipled mathematical framework of deep learning with\nutilizes experience driven autonomous learning to apply a\nfunctional approximation to represent learning attributes of\ndeep neural networks to overcome these challenges.\nREFERENCES\n[1]\nY. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.\n521, no. 7553, p. 436, 2015.\n[2]\nT. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep rein-\nforcement learning,” arXiv preprint arXiv:1509.02971, 2015.\n[3]\nJ. S. Esteves, A. Carvalho, and C. Couto, “Generalized geo-\nmetric triangulation algorithm for mobile robot absolute self-\nlocalization,” in Industrial Electronics, 2003. ISIE’03. 2003 IEEE\nInternational Symposium on, vol. 1.\nIEEE, 2003, pp. 346–351.\n[4]\nA. Vedaldi and K. Lenc, “Matconvnet: Convolutional neural net-\nworks for matlab,” in Proceedings of the 23rd ACM international\nconference on Multimedia.\nACM, 2015, pp. 689–692.\n[5]\nA. Eitel, J. T. Springenberg, L. Spinello, M. Riedmiller, and W. Bur-\ngard, “Multimodal deep learning for robust rgb-d object recog-\nnition,” in Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ\nInternational Conference on.\nIEEE, 2015, pp. 681–687.\n[6]\nY. Yang and Y. Li, “Robot learning manipulation action plans by”\nwatching” unconstrained videos from the world wide web.” 2015.\n[7]\nD. C. Cires¸an, U. Meier, L. M. Gambardella, and J. Schmidhuber,\n“Deep, big, simple neural nets for handwritten digit recognition,”\nNeural computation, vol. 22, no. 12, pp. 3207–3220, 2010.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\n[8]\nJ. Lu, V. Behbood, P. Hao, H. Zuo, S. Xue, and G. Zhang, “Transfer\nlearning using computational intelligence: a survey,” Knowledge-\nBased Systems, vol. 80, pp. 14–23, 2015.\n[9]\nM. Turan, J. Shabbir, H. Araujo, E. Konukoglu, and M. Sitti,\n“A deep learning based fusion of rgb camera information and\nmagnetic localization information for endoscopic capsule robots,”\nInternational journal of intelligent robotics and applications, vol. 1,\nno. 4, pp. 442–450, 2017.\n[10] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, per-\nspectives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260,\n2015.\n[11] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti,\n“Deep endovo: A recurrent convolutional neural network (rcnn)\nbased visual odometry approach for endoscopic capsule robots,”\nNeurocomputing, vol. 275, pp. 1861–1870, 2018.\n[12] N. S¨underhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell,\nB. Upcroft, and M. Milford, “Place recognition with convnet land-\nmarks: Viewpoint-robust, condition-robust, training-free,” Proceed-\nings of Robotics: Science and Systems XII, 2015.\n[13] M. Turan, Y. Y. Pilavci, R. Jamiruddin, H. Araujo, E. Konukoglu,\nand M. Sitti, “A fully dense and globally consistent 3d map recon-\nstruction approach for gi tract to enhance therapeutic relevance\nof the endoscopic capsule robot,” arXiv preprint arXiv:1705.06524,\n2017.\n[14] M. Turan, Y. Y. Pilavci, I. Ganiyusufoglu, H. Araujo, E. Konukoglu,\nand M. Sitti, “Sparse-then-dense alignment-based 3d map recon-\nstruction method for endoscopic capsule robots,” Machine Vision\nand Applications, vol. 29, no. 2, pp. 345–359, 2018.\n[15] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, “Human-\nlevel concept learning through probabilistic program induction,”\nScience, vol. 350, no. 6266, pp. 1332–1338, 2015.\n[16] S. Marsland, “Machine learning, an algorithmic perspective, chap-\nman & hall/crc machine learning & pattern recognition,” CRC,\nBoca Raton, Fla, 2009.\n[17] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detect-\ning robotic grasps,” The International Journal of Robotics Research,\nvol. 34, no. 4-5, pp. 705–724, 2015.\n[18] Z. Ghahramani, “Probabilistic machine learning and artiﬁcial in-\ntelligence,” Nature, vol. 521, no. 7553, p. 452, 2015.\n[19] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel,\n“Benchmarking deep reinforcement learning for continuous con-\ntrol,” in International Conference on Machine Learning, 2016, pp.\n1329–1338.\n[20] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen, “Learning\nhand-eye coordination for robotic grasping with large-scale data\ncollection,” in International Symposium on Experimental Robotics.\nSpringer, 2016, pp. 173–184.\n[21] Y. Yang, C. Fermuller, Y. Li, and Y. Aloimonos, “Grasp type\nrevisited: A modern perspective on a classical feature for vision,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015, pp. 400–408.\n[22] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution\nusing deep convolutional networks,” IEEE transactions on pattern\nanalysis and machine intelligence, vol. 38, no. 2, pp. 295–307, 2016.\n[23] A. Gongal, S. Amatya, M. Karkee, Q. Zhang, and K. Lewis, “Sen-\nsors and systems for fruit detection and localization: A review,”\nComputers and Electronics in Agriculture, vol. 116, pp. 8–19, 2015.\n[24] A. M. Nguyen, J. Yosinski, and J. Clune, “Innovation engines:\nAutomated creativity and improved stochastic optimization via\ndeep learning,” in Proceedings of the 2015 Annual Conference on\nGenetic and Evolutionary Computation.\nACM, 2015, pp. 959–966.\n[25] Y. Du, W. Wang, and L. Wang, “Hierarchical recurrent neural\nnetwork for skeleton based action recognition,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2015,\npp. 1110–1118.\n[26] Y. Tang, “Deep learning using linear support vector machines,”\narXiv preprint arXiv:1306.0239, 2013.\n[27] N. S¨underhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford,\n“On the performance of convnet features for place recognition,”\nin Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International\nConference on.\nIEEE, 2015, pp. 4297–4304.\n[28] C. Chen, A. Seff, A. Kornhauser, and J. Xiao, “Deepdriving:\nLearning affordance for direct perception in autonomous driving,”\nin Computer Vision (ICCV), 2015 IEEE International Conference on.\nIEEE, 2015, pp. 2722–2730.\n[29] J. Schmidhuber, “Deep learning in neural networks: An overview,”\nNeural networks, vol. 61, pp. 85–117, 2015.\n[30] H.\nBrighton\nand\nH.\nSelina,\nIntroducing\nArtiﬁ-\ncial\nIntelligence:\nA\nGraphic\nGuide,\nser.\nIntroduc-\ning...\nIcon\nBooks\nLimited,\n2015.\n[Online].\nAvailable:\nhttps://books.google.com.pk/books?id=4GxGCgAAQBAJ\n[31] J. Bai, Y. Wu, J. Zhang, and F. Chen, “Subset based deep learning\nfor rgb-d object recognition,” Neurocomputing, vol. 165, pp. 280–\n292, 2015.\n[32] V. Veeriah, N. Zhuang, and G.-J. Qi, “Differential recurrent neural\nnetworks for action recognition,” in Computer Vision (ICCV), 2015\nIEEE International Conference on.\nIEEE, 2015, pp. 4041–4049.\n[33] R. Xu, C. Xiong, W. Chen, and J. J. Corso, “Jointly modeling deep\nvideo and compositional text to bridge vision and language in a\nuniﬁed framework.” 2015.\n[34] K. Narasimhan, T. Kulkarni, and R. Barzilay, “Language under-\nstanding for text-based games using deep reinforcement learning,”\narXiv preprint arXiv:1506.08941, 2015.\n[35] J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum, “Galileo:\nPerceiving physical object properties by integrating a physics\nengine with deep learning,” in Advances in neural information\nprocessing systems, 2015, pp. 127–135.\n[36] M. Turan, E. P. Ornek, N. Ibrahimli, C. Giracoglu, Y. Almali-\noglu, M. F. Yanik, and M. Sitti, “Unsupervised odometry and\ndepth learning for endoscopic capsule robots,” arXiv preprint\narXiv:1803.01047, 2018.\n[37] M. Turan, Y. Almalioglu, H. Araujo, T. Cemgil, and M. Sitti,\n“Endosensorfusion: Particle ﬁltering-based multi-sensory data fu-\nsion with switching state-space model for endoscopic capsule\nrobots using recurrent neural network kinematics,” arXiv preprint\narXiv:1709.03401, 2017.\n[38] R. Lun and W. Zhao, “A survey of applications and human motion\nrecognition with microsoft kinect,” International Journal of Pattern\nRecognition and Artiﬁcial Intelligence, vol. 29, no. 05, p. 1555008,\n2015.\n[39] J. Kuen, K. M. Lim, and C. P. Lee, “Self-taught learning of a deep\ninvariant representation for visual tracking via temporal slowness\nprinciple,” Pattern recognition, vol. 48, no. 10, pp. 2964–2982, 2015.\n[40] Y. Hou, H. Zhang, and S. Zhou, “Convolutional neural network-\nbased image representation for visual loop closure detection,” in\nInformation and Automation, 2015 IEEE International Conference on.\nIEEE, 2015, pp. 2238–2245.\n[41] Y. Qian, J. Dong, W. Wang, and T. Tan, “Deep learning for steganal-\nysis via convolutional neural networks,” in Media Watermarking,\nSecurity, and Forensics 2015, vol. 9409.\nInternational Society for\nOptics and Photonics, 2015, p. 94090J.\n[42] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, “Simultaneous\ndeep transfer across domains and tasks,” in Computer Vision\n(ICCV), 2015 IEEE International Conference on.\nIEEE, 2015, pp.\n4068–4076.\n[43] L. Pinto, D. Gandhi, Y. Han, Y.-L. Park, and A. Gupta, “The curious\nrobot: Learning visual representations via physical interactions,”\nin European Conference on Computer Vision.\nSpringer, 2016, pp.\n3–18.\n[44] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman,\n“Building machines that learn and think like people,” Behavioral\nand Brain Sciences, vol. 40, 2017.\n[45] G. Chen, D. Clarke, M. Giuliani, A. Gaschler, and A. Knoll, “Com-\nbining unsupervised learning and discrimination for 3d action\nrecognition,” Signal Processing, vol. 110, pp. 67–81, 2015.\n[46] J. Wulff and M. J. Black, “Efﬁcient sparse-to-dense optical ﬂow\nestimation using a learned basis and layers,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2015,\npp. 120–130.\n[47] J.-R. Ruiz-Sarmiento, C. Galindo, and J. Gonzalez-Jimenez, “Scene\nobject recognition for mobile robots through semantic knowledge\nand probabilistic graphical models,” Expert Systems with Applica-\ntions, vol. 42, no. 22, pp. 8805–8816, 2015.\n[48] N. Das, E. Ohn-Bar, and M. M. Trivedi, “On performance evalua-\ntion of driver hand detection algorithms: Challenges, dataset, and\nmetrics,” in Intelligent Transportation Systems (ITSC), 2015 IEEE 18th\nInternational Conference on.\nIEEE, 2015, pp. 2953–2958.\n[49] R. Salakhutdinov, “Learning deep generative models,” Annual\nReview of Statistics and Its Application, vol. 2, pp. 361–385, 2015.\n[50] J. Schmidhuber, “On learning to think: Algorithmic informa-\ntion theory for novel combinations of reinforcement learning\ncontrollers and recurrent neural world models,” arXiv preprint\narXiv:1511.09249, 2015.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\n[51] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti,\n“A non-rigid map fusion-based direct slam method for endoscopic\ncapsule robots,” International journal of intelligent robotics and appli-\ncations, vol. 1, no. 4, pp. 399–409, 2017.\n[52] T. Chen, Z. Chen, Q. Shi, and X. Huang, “Road marking detection\nand classiﬁcation using machine learning algorithms,” in Intelli-\ngent Vehicles Symposium (IV), 2015 IEEE.\nIEEE, 2015, pp. 617–621.\n[53] M. Vrigkas, C. Nikou, and I. A. Kakadiaris, “A review of human\nactivity recognition methods,” Frontiers in Robotics and AI, vol. 2,\np. 28, 2015.\n[54] O. K. Oyedotun and A. Khashman, “Deep learning in vision-\nbased static hand gesture recognition,” Neural Computing and\nApplications, vol. 28, no. 12, pp. 3941–3951, 2017.\n[55] R. K. Moore, “From talking and listening robots to intelligent\ncommunicative machines.”\n[56] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algo-\nrithm for deep belief nets,” Neural computation, vol. 18, no. 7, pp.\n1527–1554, 2006.\n[57] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei,\nand A. Farhadi, “Target-driven visual navigation in indoor scenes\nusing deep reinforcement learning,” in Robotics and Automation\n(ICRA), 2017 IEEE International Conference on.\nIEEE, 2017, pp.\n3357–3364.\n[58] F. Cruz, J. Twiefel, S. Magg, C. Weber, and S. Wermter, “Interactive\nreinforcement learning through speech guidance in a domestic\nscenario,” in Neural Networks (IJCNN), 2015 International Joint\nConference on.\nIEEE, 2015, pp. 1–8.\n[59] A. Vinciarelli, A. Esposito, E. Andr´e, F. Bonin, M. Chetouani, J. F.\nCohn, M. Cristani, F. Fuhrmann, E. Gilmartin, Z. Hammal et al.,\n“Open challenges in modelling, analysis and synthesis of human\nbehaviour in human–human and human–machine interactions,”\nCognitive Computation, vol. 7, no. 4, pp. 397–413, 2015.\n[60] J. Doshi, Z. Kira, and A. Wagner, “From deep learning to episodic\nmemories: Creating categories of visual experiences,” in Proceed-\nings of the third annual conference on advances in cognitive systems\nACS, 2015, p. 15.\n[61] X. Wang, D. Fouhey, and A. Gupta, “Designing deep networks for\nsurface normal estimation,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2015, pp. 539–547.\n[62] H. Cuay´ahuitl, S. Keizer, and O. Lemon, “Strategic dialogue\nmanagement via deep reinforcement learning,” arXiv preprint\narXiv:1511.08099, 2015.\n[63] E. Ohn-Bar and M. M. Trivedi, “Looking at humans in the age of\nself-driving and highly automated vehicles,” IEEE Transactions on\nIntelligent Vehicles, vol. 1, no. 1, pp. 90–104, 2016.\n[64] J. Wei, H. Liu, G. Yan, and F. Sun, “Robotic grasping recognition\nusing multi-modal deep extreme learning machine,” Multidimen-\nsional Systems and Signal Processing, vol. 28, no. 3, pp. 817–833,\n2017.\n[65] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale\nvideo prediction beyond mean square error,” arXiv preprint\narXiv:1511.05440, 2015.\n[66] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n“Rethinking the inception architecture for computer vision,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 2818–2826.\n[67] M. Turan, Y. Almalioglu, E. Konukoglu, and M. Sitti, “A deep\nlearning based 6 degree-of-freedom localization method for endo-\nscopic capsule robots,” arXiv preprint arXiv:1705.05435, 2017.\n[68] J. Tang, C. Deng, and G.-B. Huang, “Extreme learning machine for\nmultilayer perceptron,” IEEE transactions on neural networks and\nlearning systems, vol. 27, no. 4, pp. 809–821, 2016.\n[69] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti,\n“A non-rigid map fusion-based rgb-depth slam method for endo-\nscopic capsule robots,” arXiv preprint arXiv:1705.05444, 2017.\n[70] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.\nYuille, “Deeplab: Semantic image segmentation with deep convo-\nlutional nets, atrous convolution, and fully connected crfs,” arXiv\npreprint arXiv:1606.00915, 2016.\n[71] J.-C. Chen, V. M. Patel, and R. Chellappa, “Unconstrained face\nveriﬁcation using deep cnn features,” in Applications of Computer\nVision (WACV), 2016 IEEE Winter Conference on.\nIEEE, 2016, pp.\n1–9.\n[72] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,\nand A. Swami, “The limitations of deep learning in adversarial\nsettings,” in Security and Privacy (EuroS&P), 2016 IEEE European\nSymposium on.\nIEEE, 2016, pp. 372–387.\n[73] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training\nof deep visuomotor policies,” The Journal of Machine Learning\nResearch, vol. 17, no. 1, pp. 1334–1373, 2016.\n[74] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep\nnetworks with stochastic depth,” in European Conference on Com-\nputer Vision.\nSpringer, 2016, pp. 646–661.\n[75] S. Niekum, S. Osentoski, G. Konidaris, S. Chitta, B. Marthi, and\nA. G. Barto, “Learning grounded ﬁnite-state representations from\nunstructured demonstrations,” The International Journal of Robotics\nResearch, vol. 34, no. 2, pp. 131–157, 2015.\n[76] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learning\nmodular neural network policies for multi-task and multi-robot\ntransfer,” in Robotics and Automation (ICRA), 2017 IEEE Interna-\ntional Conference on.\nIEEE, 2017, pp. 2169–2176.\n[77] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel,\n“Deep spatial autoencoders for visuomotor learning,” in Robotics\nand Automation (ICRA), 2016 IEEE International Conference on.\nIEEE, 2016, pp. 512–519.\n[78] A. A. Rusu, M. Vecerik, T. Roth¨orl, N. Heess, R. Pascanu, and\nR. Hadsell, “Sim-to-real robot learning from pixels with progres-\nsive nets,” arXiv preprint arXiv:1610.04286, 2016.\n[79] S. Mohamed and D. J. Rezende, “Variational information max-\nimisation for intrinsically motivated reinforcement learning,” in\nAdvances in neural information processing systems, 2015, pp. 2125–\n2133.\n[80] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural\nnetwork for real-time object recognition,” in Intelligent Robots and\nSystems (IROS), 2015 IEEE/RSJ International Conference on.\nIEEE,\n2015, pp. 922–928.\n[81] Y. Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, “Improving\nobject detection with deep convolutional networks via bayesian\noptimization and structured prediction,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2015, pp.\n249–258.\n[82] M. Turan, Y. Almalioglu, E. P. Ornek, H. Araujo, M. F. Yanik, and\nM. Sitti, “Magnetic-visual sensor fusion-based dense 3d recon-\nstruction and localization for endoscopic capsule robots,” arXiv\npreprint arXiv:1803.01048, 2018.\n[83] C. Finn and S. Levine, “Deep visual foresight for planning robot\nmotion,” in Robotics and Automation (ICRA), 2017 IEEE International\nConference on.\nIEEE, 2017, pp. 2786–2793.\n[84] V. Campos, A. Salvador, X. Giro-i Nieto, and B. Jou, “Diving deep\ninto sentiment: Understanding ﬁne-tuned cnns for visual senti-\nment prediction,” in Proceedings of the 1st International Workshop on\nAffect & Sentiment in Multimedia.\nACM, 2015, pp. 57–62.\n[85] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement\nlearning for robotic manipulation with asynchronous off-policy\nupdates,” in Robotics and Automation (ICRA), 2017 IEEE Interna-\ntional Conference on.\nIEEE, 2017, pp. 3389–3396.\n[86] M. Turan, Y. Almalioglu, H. Gilbert, A. E. Sari, U. Soylu,\nand M. Sitti, “Endo-vmfusenet: deep visual-magnetic sensor fu-\nsion approach for uncalibrated, unsynchronized and asymmet-\nric endoscopic capsule robot localization data,” arXiv preprint\narXiv:1709.06041, 2017.\n[87] J. Sanchez-Riera, K.-L. Hua, Y.-S. Hsiao, T. Lim, S. C. Hidayati,\nand W.-H. Cheng, “A comparative study of data fusion for rgb-d\nbased visual recognition,” Pattern Recognition Letters, vol. 73, pp.\n1–6, 2016.\n[88] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorﬂow:\nLarge-scale machine learning on heterogeneous distributed sys-\ntems,” arXiv preprint arXiv:1603.04467, 2016.\n",
  "categories": [
    "cs.CV",
    "cs.RO"
  ],
  "published": "2018-03-20",
  "updated": "2018-03-20"
}