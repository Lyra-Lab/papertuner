{
  "id": "http://arxiv.org/abs/1803.08416v1",
  "title": "Demystifying Deep Learning: A Geometric Approach to Iterative Projections",
  "authors": [
    "Ashkan Panahi",
    "Hamid Krim",
    "Liyi Dai"
  ],
  "abstract": "Parametric approaches to Learning, such as deep learning (DL), are highly\npopular in nonlinear regression, in spite of their extremely difficult training\nwith their increasing complexity (e.g. number of layers in DL). In this paper,\nwe present an alternative semi-parametric framework which foregoes the\nordinarily required feedback, by introducing the novel idea of geometric\nregularization. We show that certain deep learning techniques such as residual\nnetwork (ResNet) architecture are closely related to our approach. Hence, our\ntechnique can be used to analyze these types of deep learning. Moreover, we\npresent preliminary results which confirm that our approach can be easily\ntrained to obtain complex structures.",
  "text": "DEMYSTIFYING DEEP LEARNING: A GEOMETRIC APPROACH TO ITERATIVE\nPROJECTIONS\nAshkan Panahi†, Hamid Krim†, Liyi Dai†#∗\n† Department of Electrical and Computer Engineering, North Carolina State University,\nRaleigh, NC, 27606\n# The U.S. Army Research Ofﬁce, Durham, NC 27709\nABSTRACT\nParametric approaches to Learning, such as deep learning\n(DL), are highly popular in nonlinear regression, in spite\nof their extremely difﬁcult training with their increasing\ncomplexity (e.g.\nnumber of layers in DL). In this paper,\nwe present an alternative semi-parametric framework which\nforegoes the ordinarily required feedback, by introducing the\nnovel idea of geometric regularization. We show that certain\ndeep learning techniques such as residual network (ResNet)\narchitecture are closely related to our approach. Hence, our\ntechnique can be used to analyze these types of deep learn-\ning. Moreover, we present preliminary results which conﬁrm\nthat our approach can be easily trained to obtain complex\nstructures.\nIndex Terms— supervised learning, back propagation,\ngeometric approaches\n1. INTRODUCTION\nLearning a nonlinear function through a ﬁnite number of\ninput-output observations is a fundamental problem of super-\nvised machine learning, and has wide applications in science\nand engineering. From a statistical vantage point, this prob-\nlem entails a regression procedure which, depending on the\nnature of the underlying function, may be linear or nonlinear.\nIn the past few decades, there has been a ﬂurry of advances in\nthe area of nonlinear regression [1]. Deep learning is perhaps\none of the most well-known approaches with a promising and\nremarkable performance in great many applications.\nDeep learning has a number of distinctive advantages: 1.\nIt relies on a parametric description of functions that are eas-\nily computable. Once the parameters (weights) of a deep net-\nwork are set, the output can be rapidly computed in a feed\nforward fashion by a few iterations of afﬁne and elementwise\nnonlinear operations; 2. it can avoid over-parametrization by\nadjusting the architecture (number of parameters) of the net-\nwork, hence providing control over the generalization power\nof deep learning. Finally, deep networks have been observed\n∗This work is partially supported by the U.S. Army Research Ofﬁce under\nagreement W911NF-16-2-0005\nto be highly ﬂexible in expressing complex and highly non-\nlinear functions [2, 3]. There are, however, a number of chal-\nlenges associated with deep learning, chief among them is that\nof obtaining the exact assessment of their expressive power\nwhich remains to this day, an open problem. An important\nexception is the single-layer network for which the so called\nuniversal approximation property (UAP) has been established\nfor some time[4, 5], and which is clearly a highly desirable\nproperty. Another practical difﬁculty with deep learning is\nthat the output becomes unproportionally sensitive to the pa-\nrameters of different layers, making it, from an optimization\nperspective, extremely difﬁcult to train [6]. A recent solution\nis the so-called residual network (ResNet) learning, which in-\ntroduces bridging branches to the conventional deep learning\narchitecture [7]. In this paper, we address the above issues by\nproposing a different perspective on learning with a substan-\ntially different architecture, which totally forgoes any feed-\nback. Speciﬁcally, we propose an interative foward projec-\ntion in lieu of back propagation to update parameters. As\nsuch, this may rapidly yield an over-parametrized system, we\nrestrict each layer to perform an ”incremental” update on the\ndata, as approximately captured by the realization of a dif-\nferential equation, we refer to as geometric regularization, as\ndiscussed in Section 2.1. The formulation of this geometric\nregularization allows us to tie the analysis of deep networks\nto differential geometry. The study in [8] notices this relation,\nbut adopts a different approach. In particular, we conjecture\na converse of the celebrated Frobenius integrability theorem,\nwhich potentially proves a universal approximation property\nfor a family of modiﬁed deep ResNets. We also present pre-\nliminary results in Section 5, and show that foregoing back\npropagation in a neural network does not greatly limit the ex-\npressive power of deep networks, and in fact potentially de-\ncreases their training effort, dramatically.\narXiv:1803.08416v1  [cs.LG]  22 Mar 2018\n2. MMSE ESTIMATION BY GEOMETRIC\nREGULARIZATION\nFor the sake of generality, we consider a C1 Banach man-\nifold1 F of functions f : Rn →Rm, where n, m are the\ndimensions of the data and label vectors, respectively, and\neach element f ∈F represents a candidate model between\nthe data and the labels. The arbitrary choice of F allows one\nto impose structural properties on the models. Due to space\nlimitation and for clarity sake, we just focus on the simpler\ncase of F = L2, i.e. the space of square integrable functions,\nand defer further generalizations to a later publication. More-\nover, consider a probability space (Ω, Σ, µ), and two random\nvectors x : Ω→Rn and y : Ω→Rm representing sta-\ntistical information about the data. As samples (xt, yt) for\nt = 1, 2, . . . , T of x, y are often provided, in which case their\nempirical distribution is used.\nWe consider the supervised learning problem by minimiz-\ning the following mean square error (MSE),\nL(f) = E\n\u0002\n∥f(x) −y∥2\n2\n\u0003\n,\n(1)\nwhere E[. ] denotes expectation.\nFor observed samples\n(xt, yt), this criterion simpliﬁes to\nmin\nf∈F\n1\nT\nT\nX\nt=1\n∥f(xt) −yt∥2\n2.\n(2)\nIn practice, the statistical assumptions in Eq. (2) are highly\nunderdetermined and minimization of MSE (MMSE) leads to\nundesired solutions. To cope with this, additional constraints\nare considered to tame the problem by way of regulariza-\ntion. For example the set F can be restricted to a (ﬁnite-\ndimensional) smooth sub-manifold. This is an implicit fact\nin parametric approaches, such as deep neural networks.\n2.1. Geometric Regularization\nWe introduce a more general type of regularization, which\nalso includes parametric restriction. Our generalization is in-\nspired by the observation that standard (smooth) optimization\ntechniques such as gradient descent, are based on a realization\nof a differential equation of the following form,\ndfτ\ndτ = φ(fτ),\n(3)\nwhere φ(f) ∈Tf is a tangent vector of F at f. The resulting\nsolution is typically in an iterative form, as follows,\nft+1 = ft + µtφ(ft),\n(4)\nwhere µt is the step size at iteration t = 0, 1, 2, . . .. The\ntangent vector φ(fτ) is often selected as a descent direction,\nwhere according to Eq. (1), dL/dτ < 0.\n1A Banach manifold is an inﬁnite-dimensional generalization of a con-\nventional differentiable manifold [9].\nFor geometric regularization, we restrict the choice of the\ntangent vector to a closed cone Cf ⊆Tf in the tangent space.\nIn the case of function estimation, where F and hence the\ntangent space Tf, is inﬁnite dimensional, we adopt a para-\nmetric deﬁnition of Cf by restricting the tangent vector to a\nﬁnite dimensional space. However, this might not restrict the\nfunction to a ﬁnite dimensional submanifold. A particularly\nimportant case, where geometric regularization simpliﬁes to\na parametric (ﬁnite dimensional) manifold restriction is given\nby the Frobenius integrability theorem [10, 11]:\nTheorem 1 (Frobenius theorem) Suppose that Cf is an n-\ndimensional linear subspace of Tf. For any choice of φ(f) ∈\nCf, the solution of Eq. (3) remains on an n-dimensional sub-\nmanifold of F only, depending on the initial point f0, iff Cf\nis involutive, i.e. for any two vector ﬁelds φ(f), ψ(f) in Cf\nwe have that\n[φ(f), ψ(f)] ∈Cf,\nwhere [. , . ] denotes a Lie bracket [11].\nA simple example of an involutive regularization is when\nCf =\n(\nW0f +\nr\nX\nk=1\nWkf k + b | Wk ∈Rm×m, b ∈Rm\n)\nwhere f k are ﬁxed functions. It is clear that the solution ft\nremains in Cf0 from an initial f0. Hence, this case corre-\nsponds to a linear regression. Selecting a nonlinear function\ng : R →R, we can write a more general form of the geomet-\nric regularization discussed here, as follows,\nCf =\n\u001a\nΓ(f)\n\u0014\nW0f +\nrP\nk=1\nWkf k + b\n\u0015\n| Wk ∈Rd×dk, b ∈Rd\n\u001b\n,(5)\nwhere f k are arbitrary ﬁxed functions and Γ(a) for a =\n(a1, a2, . . . , ad) ∈Rd is a diagonal matrix with diagonals\nΓii = dg/dx(ai).\n3. ALGORITHMIC SOLUTION\nThe solution to the differential equation in Eq. (3) with the\ngeometric regularization in Eq. (5) requires a speciﬁcation of\nthe tangent vectors φ(f) ∈Cf. To preserve a good control\non the computations, and much like for the DNN architecture,\nwe deﬁne f : Rn →Rd, where the reduced dimension d < n\nis a design parameter. Then, the desired function is calculated\nas Dfτ + c where D ∈Rm×d and c ∈Rm are ﬁxed. Letting\nf0(x) = Ux where U ∈Rd×n is a constant dimensionality\nreduction matrix, we rewrite the MSE objective in Eq. (1) as\nL(f) = E\nh\n∥y −Df(x) −c∥2\n2\ni\n.\nWe subsequently apply the steepest descent principle to yield\nthe following optimization:\nφf = arg\nmin\nφ∈Cf |∥φ∥2≤1\ndL\ndτ .\n(6)\nWe next observe that under mild assumptions,\ndL\ndτ = −E\n\"*\nz , DΓ(f)\n\"\nW0f +\nr\nX\nk=1\nWkf k + b\n#+#\n,\nwhere z = y −Df(x) −c and W0, Wk, b are to be decided\nbased on the optimization in Eq. (6). After some manipula-\ntions, this leads to\nφf = Γ(f)\n\"\nW0,ff +\nr\nX\nk=1\nWk,ff k + bf\n#\n,\nwhere\nW0,f = E\n\u0002\nΓ(f(x))DT zf T (x)\n\u0003\n,\nWk,f = E\nh\nΓ(f(x))DT z\n\u0000f k\u0001T (x)\ni\n, k = 1, 2, . . . ,\nbf = E\n\u0002\nΓ(f(x))DT z\n\u0003\n(7)\nare specialized values of W0, Wk, b, respectively.\n3.1. Initialization\nAn efﬁcient execution of the above procedure requires us to\njudiciously select the parameters U, D, c.\nWe select U as\nthe collection of basis vectors of the ﬁrst d principal com-\nponents of x, i.e. U = P T\n1 where E[xxT ] = PΣP T is the\nEigen-representation (SVD) of the correlation matrix, P =\n[p1 p2 . . . pn] and P1 = [p1 p2 . . . pd]. The matrices U, c are\nselected by minimizing the MSE objective with f = f0. This\nyields,\nD = E\n\u0002\nyf T\n0 (x)\n\u0003\nE\n\u0002\nf0(x)f T\n0 (x)\n\u0003−1 ,\nc = E [y] −DE [f0(x)] .\nThis also affords us to update these matrices in the course of\nthe optimization,\nD ←E\n\u0002\nyf T\nt (x)\n\u0003\nE\n\u0002\nft(x)f T\nt (x)\n\u0003−1 ,\nc ←E [y] −DE [ft(x)] .\n3.2. Momentum Method\nMomentum methods are popular in machine learning and lead\nto considerable improvement in both performance and con-\nvergence speed [12, 13]. Since the originally formulated do\nnot conform to our geometric regularization framework, we\nproposed an alternative approach to effectively mix the learn-\ning direction φf at each iteration with its preceding iterates\nto better control any associated rapid changes over iterations\n(low-pass ﬁltering). Here to keep the geometric regularization\nstructure, we instead mix the parameters Wk, b. This leads to\nthe following modiﬁcation in the original algorithm in Eq. (4):\nft+1 = ft + µtΓ(f)\n\u0014\nV0,tft +\nrP\nk=1\nVk,tf k + et\n\u0015\n,\nVk,t+1 = αkVk,t + Wk,ft, k = 0, 1, . . . , r,\net+1 = βet + bft,\n(8)\nwhere Wk,ft, bft are given in Eq. (7).\n3.3. Learning Parameter Selection\nTo select the remaining parameters αk, β and µt, we manually\ntune parameters αk, β, speciﬁcally utilize two strategies when\ntuning µt: a) ﬁxing µt = µ and b) using line search. The\nsecond method, we obtain by simple computations as\nµt = E[zT\nt Dψt]\nE[∥Dψt∥2\n2],\nwhere zt = y −Dft(x) −c, and\nψt = Γ(ft)\n\"\nV0,tft +\nr\nX\nk=1\nVk,tf k + et\n#\n.\n3.4. Incorporating Shift Invariance\nIn the context of deep learning, especially for image process-\ning, convolutional networks are popular. They differ from the\nregular deep networks in attempting to induce shift invariance\nin the linear operations of some layers, by way of convolution\n(Toeplitz matrix). We may adopt the same strategy in geo-\nmetric regularization by further assuming that W0 in Eq. (5)\nrepresents a convolution. We skip the derivations, for not only\nspace limitation reasons, but also for their similarity to those\nleading to Eq. (7). The resulting algorithm with the momen-\ntum method is similar to Eq. (8) where W0,f is replaced by\nWconv,f, deﬁned as\nWconv,f = arg max\nW ⟨W, W0,f⟩,\nwhere the optimization is over unit-norm convolution (Toeplitz)\nmatrices. It turns out that since Toeplitz matrices form a vec-\ntor space, Wconv,f is a linear function of W0,f and can be\nquickly calculated [14]. Due to space limitation, we defer the\ndetails to [15].\n4. THEORETICAL DISCUSSION\n4.1. Relation to Deep Residual Networks\nThe proposed geometric regularization for nonlinear regres-\nsion in Eq. (5) is inspired by the advances in the ﬁeld of neural\nnetworks and deep learning. Recall that a generic deep artiﬁ-\ncial neural network (DNN) represents a sequence of functions\n(hidden layers) f0(x), f1(x), . . . , fT (x) where f0(x) = x\nand ft for t = 0, 1, 2, . . ., is dt-dimensional, where dt is the\nnetwork width in the tth layer. The relation of these func-\ntions is plotted in Figure 1 (a). The so-called residual net-\nwork (ResNet) architecture modiﬁes DNNs by introducing\nbridging branches (edges) as shown by Figure 1 (b). We ob-\nserve that the geometric regularization in Eq. (5) corresponds\n௡\n \n \n௡ \n௡ାଵ \n௡\n \n \n௡ \n௡ାଵ \n \n௡ \n(b) \n(c) \n௡\n \n \n௡ \n௡ାଵ \n \n௡\n௡ \n(a) \nFig. 1.\nSchematic scheme of a single layer in a)ANN\nb)ResNet and c)modiﬁed ResNet Architectures.\nMethod\nPerformance\nplain iterations\n97.4%\nconvolutional iterations\n98.0%\n2-stage learning\n98.7%\nFig. 2. Performance of different learning strategies\nto a modiﬁed version of ResNets, as depicted in Figure 1 (c),\nwhich can be written as\nft+1 = g(Wtft + bt) −g(ft) + ft.\n(9)\nMore concretely, when Wt, bt are respectively near identity\nand near zero, i.e. Wt = I+ϵ ¯Wt and bt = ϵ¯bt for small values\nof ϵ, we observe by Taylor expansion of Eq. (9) with respect to\nϵ that the differential equation in Eq. (3) with geometric reg-\nularization in Eq. (5) provides the limit of the above modiﬁed\nResNet architecture. This profound relation provides a novel\napproach for analyzing deep networks, which is deferred to\n[15].\n5. NUMERICAL RESULTS\nAs a preliminary validation, we examine geometric regular-\nization on the MNIST handwritten digits database including\n50, 000 28 × 28 black and white images of handwritten dig-\nits for training and 10, 000 more for testing [16, 17]. We note\nthat state-of-the-art techniques already achieve an accuracy as\nhigh as 99.7%, thus justifying our validation study merely as\na proof of concept. We use a single ﬁxed function f 1(x) = x.\nIn all experiments, we set α0 = β = 0.98 and let α1 vary.\nWe have performed extensive numerical studies with dif-\nferent strategies (ﬁxed or variable D, c, different step size se-\nlection methods and convolutional/plain layers), but can only\nfocus on some key results due to space limitation. A more\ncomprehensive comparison between these strategies is also\ninsightful [15]. A summary of the best achieved performances\nis listed in Figure 2, where plain (non-convolutional) itera-\ntions are applied with ﬁxed step size µ = 0.06 and α1 = 0.99,\nd = 400 and ﬁxed D, c. The convolutional iterations also\ninclude 2-D convolutional (Toeplitz) matrices with window\nFig. 3. Performance of different step size selection strategies.\nlength 5, ﬁxed step size µ = 1 and α1 = 0.95, and D, c\nupdated at each iteration. We also consider a 2-stage proce-\ndure, where in the ﬁrst 50 iterations, convolutional matrices\nare considered, and plain iterations are subsequently applied.\nIn both stages, the step size is ﬁxed to µ = 3 and α = 0.95,\nwhile the matrices D, c are updated at each iteration.\nFigure 3 compares different strategies for step size se-\nlection with convolutional iterations by their associated per-\nformance, i.e. the fraction of correctly classiﬁed images in\ndifferent iterations. The best asymptotic performance is ob-\ntained by ﬁxing µ = 1. Faster convergence may be obtained\nby larger step sizes at the expense of a decreased asymp-\ntotic performance. For example for µ = 6, the algorithms\nreaches 96% accuracy in only 10 iterations and is 97% cor-\nrect at 30. However, the process becomes substantially slower\nafterwards, which suggests a multi-stage procedure to boost\nperformance. Using adaptive step size with line search shows\na slightly degraded (higher than µ = 6) performance, but dra-\nmatically decreases the convergence rate.\n6. CONCLUSION\nWe proposed a supervised learning technique, which en-\njoys many common properties with deep learning, such as\nsuccessive application of linear and non-linear operators, mo-\nmentum method of implementation and convolutional layers.\nIn contrast to deep learning, our method abandons the need\nfor back propagation to hence improve the computational\nburden. Our method is semi-parametric as it essentially ex-\nploits a large number of weight parameters, yet avoiding\nover-parametrization. Another advantage of our technique is\nthat it can theoretically be analyzed by tools in differential\ngeometry as brieﬂy discussed earlier. A conprehensive devel-\nopment is in [15]. The performance on the data sets we have\nthus far achieved, promises a great and unexplored potential\nwaiting to be unveiled.\n7. REFERENCES\n[1] T. Hastie, R. Tibshirani, and J. Friedman, “Overview\nof supervised learning,” in The elements of statistical\nlearning.\nSpringer, 2009, pp. 9–41.\n[2] S. S. Haykin, Neural networks: a comprehensive foun-\ndation.\nTsinghua University Press, 2001.\n[3] M. H. Hassoun, Fundamentals of artiﬁcial neural net-\nworks.\nMIT press, 1995.\n[4] G. Gybenko, “Approximation by superposition of sig-\nmoidal functions,” Mathematics of Control, Signals and\nSystems, vol. 2, no. 4, pp. 303–314, 1989.\n[5] K. Hornik, “Approximation capabilities of multilayer\nfeedforward networks,” Neural networks, vol. 4, no. 2,\npp. 251–257, 1991.\n[6] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,”\nNature, vol. 521, no. 7553, pp. 436–444, 2015.\n[7] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image recognition,” in Proceedings of the\nIEEE conference on computer vision and pattern recog-\nnition, 2016, pp. 770–778.\n[8] M. Hauser and A. Ray, “Principles of riemannian geom-\netry in neural networks,” in Advances in Neural Infor-\nmation Processing Systems, 2017, pp. 2804–2813.\n[9] S. Lang, Differential manifolds.\nSpringer, 1972, vol.\n212.\n[10] C. Camacho and A. L. Neto, Geometric theory of folia-\ntions.\nSpringer Science & Business Media, 2013.\n[11] H. K. Khalil, “Noninear systems,” Prentice-Hall, New\nJersey, vol. 2, no. 5, pp. 5–1, 1996.\n[12] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On\nthe importance of initialization and momentum in deep\nlearning,” in International conference on machine learn-\ning, 2013, pp. 1139–1147.\n[13] D. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” arXiv preprint arXiv:1412.6980, 2014.\n[14] A. B¨ottcher and S. M. Grudsky, Toeplitz matrices,\nasymptotic linear algebra, and functional analysis.\nBirkh¨auser, 2012.\n[15] A. Panahi and H. Krim, “a geometric view on represen-\ntation power of deep networks,” under preparation.\n[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner,\n“Gradient-based learning applied to document recog-\nnition,” Proceedings of the IEEE, vol. 86, no. 11, pp.\n2278–2324, 1998.\n[17] D. Ciregan, U. Meier, and J. Schmidhuber, “Multi-\ncolumn deep neural networks for image classiﬁcation,”\nin Computer Vision and Pattern Recognition (CVPR),\n2012 IEEE Conference on.\nIEEE, 2012, pp. 3642–\n3649.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-03-22",
  "updated": "2018-03-22"
}