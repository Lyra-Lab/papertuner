{
  "id": "http://arxiv.org/abs/1803.08793v1",
  "title": "Exploring the Naturalness of Buggy Code with Recurrent Neural Networks",
  "authors": [
    "Jack Lanchantin",
    "Ji Gao"
  ],
  "abstract": "Statistical language models are powerful tools which have been used for many\ntasks within natural language processing. Recently, they have been used for\nother sequential data such as source code.(Ray et al., 2015) showed that it is\npossible train an n-gram source code language mode, and use it to predict buggy\nlines in code by determining \"unnatural\" lines via entropy with respect to the\nlanguage model. In this work, we propose using a more advanced language\nmodeling technique, Long Short-term Memory recurrent neural networks, to model\nsource code and classify buggy lines based on entropy. We show that our method\nslightly outperforms an n-gram model in the buggy line classification task\nusing AUC.",
  "text": "Exploring the Naturalness of Buggy Code with Recurrent Neural Networks\nJack Lanchantin and Ji Gao\njjl5sw@virginia.edu, jg6yd@virginia.edu\nDepartment of Computer Science\nUniversity of Virginia\nAbstract\nStatistical language models are powerful tools\nwhich have been used for many tasks within nat-\nural language processing. Recently, they have been\nused for other sequential data such as source code.\n(Ray et al., 2015) showed that it is possible train an\nn-gram source code language mode, and use it to\npredict buggy lines in code by determining “unnat-\nural” lines via entropy with respect to the language\nmodel. In this work, we propose using a more ad-\nvanced language modeling technique, Long Short-\nterm Memory recurrent neural networks, to model\nsource code and classify buggy lines based on en-\ntropy. We show that our method slightly outper-\nforms an n-gram model in the buggy line classiﬁca-\ntion task using AUC.\n1. Introduction\nNatural language is inherently very well understood by hu-\nmans. There are certain linguistics and structures associated\nwith natural language which make it ﬂuid and efﬁcient. These\nrepetitive and predictive properties of natural language make\nit easy to exploit via statistical language models. Although\nthe actual semantics are very much different, source code is\nalso repetitive and predictive. Some of this is constrained by\nwhat the compiler expects, and some of it is due to the way\nthat humans construct the code. Regardless of why it is pre-\ndictable, it has been shown that code is accommodating to the\nsame kinds of language modeling as natural language (Hindle\net al., 2012).\nThe language modeling task is deﬁned as estimating the prob-\nability of a sequence of words (or tokens). Formally, given a\nsequence of tokens S, a language model attempts to estimate\nthe probability of S occurring in the language via the follow-\ning equation:\nP(S) = P(s1)\nN\nY\ni=2\nP(st|s1, s2, ..., st−1)\n(1)\nWhere the conditional probabilities P(st|s1, s2, ..., st−1)\nmodel the probability of token st occurring given all previ-\nous tokens s1, s2, ..., st−1.\nFrom a distribution such as a language model, we can mea-\nsure the entropy (see section 3.4), or amount of uncertainty of\na character given all previous characters. Using this metric,\nwe can determine particular sequences which are “unnatural”\nwith respect to the language.\nRecently, (Ray et al., 2015) showed that it is possible to pre-\ndict buggy lines of code based on the entropy of the line with\nrespect to a code language model. In this work, the authors\nproposed a cache language model, which is an extension of\nan n-gram language model to handle local regularities in a\npiece of code which is being examined for bugs. They com-\nbine both a global and cache language model to measure the\nentropy of a certain line. They provide extensive experimen-\ntation to show that entropy of code can successfully be used\nto determine buggy lines similar to, and in some cases better\nthan previous state-of-the-art bug localization tasks.\nThe main drawback of their paper is that they use a typical n-\ngram language model, which struggles to handle long term\ndependencies due to the computation cost of a large n. Re-\ncent work has shown that recurrent neural network (RNN)\nmodels are able to model languages with long term depen-\ndencies much better than previous techniques such as n-grams\n(Graves, 2013; Sutskever et al., 2014; Sundermeyer et al.;\nKarpathy et al., 2015).\nIn this work, we propose using a recurrent neural network, and\nmore speciﬁcally, a Long short-term Memory (Hochreiter &\nSchmidhuber, 1997) network in order to create a source code\nlanuage model. At the cost of a harder optimization problem,\nLSTMs are able to model the full conditional distribution of\na sequence. We then use this language model to compare our\nresults with (Ray et al., 2015) in classifying buggy lines based\non entropy.\n1.1. Motivation for Using LSTMs for Modeling Code\nUsing LSTMs in modeling source code is arguably more im-\nportant than in the natural language modeling case. It has\nbeen shown that although computationally expensive, modest\nsized n-gram models can model languages and generate text\nwhich is comparable to human writing. However, this is due\nto the fact that natural language is fairly local. Most words\n(or characters) do not heavily depend on words farther than\narXiv:1803.08793v1  [cs.SE]  21 Mar 2018\nExploring the Naturalness of Buggy Code with Recurrent Neural Networks\nabout 20 back. In the source code case, it is drastically dif-\nferent. For example, a function could be 20 lines (and thus >\nabout 200 tokens) long. The characters at the end of the func-\ntion are heavily dependent on the characters at the beginning\nof the function, and possibly even lines before the function.\nFurthermore, there are multiple dependencies within source\ncode in order to model the sequence (e.g. syntax, indenting,\nvariable names). Although it is hard to properly model all as-\npects of code in a mode, it has been argued that a deep stack\nof non-linear layers in between the input and the output unit\nof a neural network are a way for the model to encode a non-\nlocal generalization prior over the input space (Bengio, 2009;\nSzegedy et al., 2013). In other words, it is assumed that is\npossible for the language model to model regions of the input\nspace (sequences) that contain no training examples, but are\ninterpolated versions in the vicinity of training samples. This\nis important because code can be highly variable in terms of\nthe actual tokens, but the underlying mechanisms and struc-\nture is consistent.\nRecently, it was shown by (Karpathy et al., 2015) that we\ncan easily generate source code, which appears to be writ-\nten by a human, from a source code language model. This is\nimpressive because in the examples shown, the code is well\nindented, the braces and brackets are correctly nested, and\neven commented correctly. This is not something that can\nbe achieved by looking at the previous n characters. They\nfound certain LSTM “cells” which were responsible for code\nsemantics such as indentation, if statements, and comments.\nIn this sense, we hypothesize that LSTMs are better suited to\nmodel source code, and thus will give higher accuracy bug\npredictions.\n2. Related Work\n2.1. Bug Detection in Software Engineering\nFor software bug detection, there are two main areas of re-\nsearch: bug prediction, and bug localization.\n(1) Bug prediction, or statistical defect prediction, which is\nconcerned with being able to predict whether or not there is\na bug in a certain piece of code, has been widely studied in\nrecent years (Catal & Diri, 2009). With the vast amount of\narchived repositories in websites containing bug reports, there\nare many opportunities for ﬁnding bugs in code.\n(2) Bug localization is concerned with exactly locating or\nclassifying speciﬁc lines as buggy or non-buggy. Static bug\nﬁnders (SBFs) automatically ﬁnd where in code a bug is lo-\ncated. SBFs use properties of code to indicate locations of\nbugs. There has been a wide array of recent work in this area\n(Rahman et al., 2014), which use many pattern recognition\ntechniques to ﬁnd bugs. As noted in (Ray et al., 2015), us-\ning SBFs and using the entropy of language models are two\nvery different approaches to achieve the same goal. The main\ngoal of their work is to compare the effectiveness of language\nmodels vs SBFs for the same task of classifying buggy lines.\n2.2. Natural Language Processing\nThere have been many works in NLP which use lan-\nguage models for sequential tasks such as word completion\n(Mikolov et al., 2013), machine translation (Bahdanau et al.,\n2014), and many others. There are also a variety of models\nwhich use word embeddings (Mikolov et al., 2013) which are\nlearned from language models in order to perform other tasks\nsuch as sentiment analysis.\nSeparately, there are a variety of models which do not use\nlanguage models, but do sequence classiﬁcations (e.g. senti-\nment analysis, document classiﬁcation) using pattern recogni-\ntion techniques (Zhang et al., 2015; Yang et al., 2016). These\nwould be similar to SBF techniques in software engineering.\nHowever, to the best of our knowledge, there have not been\nany works in NLP which use entropy from neural language\nmodels to classify sequences.\n3. Methods\n3.1. Recurrent Neural Network Language Models\nRecurrent neural networks (RNNs) are models which are par-\nticularly well suited for modeling sequential data. At each\ntime step t, an RNN takes an input vector xt ∈Rn and a\nhidden state vector ht−1 ∈Rm and produces the next hidden\nstate ht by applying the following recursive operation:\nht = f(Wxt + Uht−1 + b)\n(2)\nThe output prediction yt ∈Rn is made via the following\noperation:\nyt = Cht\n(3)\nWhere W ∈Rm×n, U ∈Rm×m, C ∈Rm×n, b ∈Rm are\nthe learnable parameters of the model, and f is an element-\nwise nonlinearity. The parameters of the model are learnt via\nbackpropagation through time (Werbos, 1990). Due to their\nrecursive nature, RNNs can model the full conditional distri-\nbution of any sequential distribution. However, RNNs suffer\nfrom what is referred to as the “vanishing gradient” problem,\nwhere the gradients of time steps far away either vanish to-\nward 0, or explode toward inﬁnity, thus making the optimiza-\ntion of the parameters difﬁcult.\nTo handle the vanishing gradients problem, (Hochreiter &\nSchmidhuber, 1997) proposed Long Short-term Memory\n(LSTM), which can handle long term dependencies by us-\ning “gating” functions which can control when information\nis written to, read from, and forgotten. Speciﬁcally, LSTM\n“modules” take inputs xt, ht−1, and ct−1, and produce ht,\nExploring the Naturalness of Buggy Code with Recurrent Neural Networks\nFigure 1. An LSTM Module\nand ct in the following way:\nit = σ(Wixt + Uiht−1 + bi)\nft = σ(Wfxt + Ufht−1 + bf)\not = σ(Woxt + Uoht−1 + bo)\ngt = tanh(Wgxt + Ught−1 + bg)\n(4)\nct = ft ⊙ct−1 + it ⊙gt\nht = ot ⊙tanh(ct)\nWhere σ() and tanh() are element-wise sigmoid and hyper-\nbolic tangent functions. ⊙represents an element-wise multi-\nplication. it, ft, and ot are referred to as the input, forget, and\noutput gates, respectively. An overview of an LSTM module\ncan be seen in Figure 1. It is the gating mechanisms that al-\nlow the LSTM to remember long term dependencies, which\nare especially important in code where a certain line may de-\npend on code many lines back.\n3.2. Handling the Global and Local Representations of\nCode\nOne signiﬁcant difference between natural language and\nsource code is that source code has specialized local depen-\ndencies such as variable names.\nIt is hard to capture ele-\nments such as variable names with global source code lan-\nguage models. To handle this, we propose a model similar\nto (Ray et al., 2015), where we construct both a “global” and\n“local” (i.e. cache) language model, each being an LSTM.\nDetails of how the global and local models are trained is ex-\nplained in section 4.\n3.3. Model Details\nAn overview of our model can be seen in Figure 2. The LSTM\nmodules are as described in section 3.1. In this ﬁgure, we\nshow the “unrolled” version of the model, which shows what\nhappens at each time step for 4 different input tokens. Each\nLSTM module has 128 cells, or units. Although it is shown\nwith only one in the ﬁgure, our implementation uses two lay-\ners of LSTM modules. That is, there is a second LSTM on\ntop of the LSTM shown in the ﬁgure. This stack of LSTMs\nallows for a better representation of the input to output signal.\nWx, Wh, and Wy, are the learned input, hidden, and output\nFigure 2. Model Overview\nembeddings of the model, where Wh encapsulates all learned\nparameters of the LSTM as explained in 3.1. There are mul-\ntiple ways to represent the input embedding Wx. It could be\ndone using a “one hot” embedding, but we use learned char-\nacter level embeddings (Mikolov et al., 2013) which can learn\nrepresentations among characters. Our character embeddings\nare vectors of length 64. Although we do not do so in this\nwork, these embeddings could be used to do other tasks such\nas classiﬁcation directly from input sequences.\nWe use the Adam method of optimization (Kingma & Ba,\n2014), using mini-batches of 128 sequences at a time. Dur-\ning training, we optimize over only 50 characters at a time,\nbut it has been shown that LSTMs can still learn dependen-\ncies longer than the training sequence length (Karpathy et al.,\n2015).\nWe implement our model using the Torch7 framework\n(Collobert et al., 2011).\nOur code is available at\nhttps://github.com/jacklanchantin/sclm.\n3.4. Entropy\nFor any probability distribution, the entropy is deﬁned as the\namount of uncertainty in the distribution. Formally, the en-\ntropy H is:\nH = −\nX\nx∈X\np(x)logp(x),\n(5)\nwhere the summation is over all values of the random variable\nX. In our case, we are interested in the amount of uncertainty\nof predicting the next character given all previous characters\nbased on our source code language model. In other words, the\ndistribution is over all possible characters given the previous\ncharacters, where X is the dictionary of characters. If the out-\nput probability of the next character is close to uniform, then\nthe entropy will be high, meaning that the predicted character\nis “unnatural” with regard to the language model. In order to\ndetermine the entropy of a line of code, we take the average\nExploring the Naturalness of Buggy Code with Recurrent Neural Networks\nSnapshots\nJava ﬁles\nLines\nGlobal Training\n50\n118164\n16502732\nLocal Training\n18\n59180\n9437902\nTotal\n68\n177344\n25940634\nTable 1. Statistics of the datasets. We use the ﬁrst 50 snapshots as a\nglobal training set, and the other 18 snapshots as a local testing set.\nWe test on 10,902 buggy and 10,902 non-buggy lines from the local\ntraining set (which are excluded in the actual training).\nentropy of each character in the line.\nTo handle the global and local language models, we formulate\nthe total entropy as:\nHtotal = λHglobal + λHlocal\n(6)\nWhere λ is a weighting term. We use λ = 0.5 in our experi-\nments. The main contribution of this work is to augment (Ray\net al., 2015), which showed that it is possible to use this metric\nof uncertainty in a line to predict a bug. We simply implement\na more advanced language modeling technique, thus the only\ndifference is the distribution p.\n4. Experiments and Results\n4.1. Dataset\nTo train and test an LSTM language model which could be\nused to distinguish buggy and non-buggy code, we choose the\nElasticSearch Project on Github, which has revision histories\ncontaining comments and commits containing bug line indi-\ncations.\nThe ElasticSearch project is a distributed search engine built\nfor the cloud, written in Java.\nWe use 68 snapshots of\nthe project, which in total contain 177,344 Java ﬁles and\n25,940,634 lines of code. Detailed information of the data\nset is listed in Table 1.\n4.2. Experimental Design\nTo show our model can detect buggy code in a real environ-\nment, we separate the dataset into two parts: a global training\nset and a local testing set. By separating the training data and\ntesting data, we aim to show that our model could be used to\npredict buggy codes in the development phase of a project.\nTo construct our experimental setting, the ﬁrst 50 snapshots\nare used to form the global training set. The local training\nset contains other 18 snapshots from the project, excluding\nthe testing lines. To form the test set, we choose all 10,902\nbuggy lines and an matching number (10,902) of non-buggy\nlines from the total 9,437,902 lines in the local training set.\nThis ensures that the tested lines have the local semantics of\nthe local language model, but also the global semantics of the\nglobal model. An overview of the data is shown in Figure 3.\nFigure 3. Experimental conﬁguration: We use the ﬁrst 50 snapshots\n(green) to train a global model. The remaining 18 snapshots (blue)\nexcluding the testing lines are used as the local training set. We select\n10,492 buggy and 10,492 non-buggy lines from the training set (pink\nlines).\nFigure 4. Average entropy difference (average entropy of buggy\ncode - average entropy of non-buggy code). The blue bar is the result\nfrom Global model, and the red bar is the result from Global+Local\nmodel.\n4.3. Model design and Metrics\nTo model the code, we train a separate LSTM on the Java\nﬁles in the global and local training sets. From these trained\nmodels, we seek to show their modeling capacity by predict-\ning bugs based on the entropy of lines in the testing set with\nrespect to the conditional distribution of the language model.\nOur baseline model is the n-gram language model from (Ray\net al., 2015), which is also used to predict whether a line of\ncode is buggy or not. To compare our model with theirs, we\nuse the average entropy of each line (see section 3.4), and\nevaluate the positive and negative buggy lines with entropy\nusing area under the ROC curve (AUC). AUC is a traditional\nnon-parametric metric to evaluate the performance of a binary\nclassiﬁcation result. A larger AUC value suggests that the\nclassiﬁcation result is well-ordered and less mistaken. AUC\nis a valid metric because the actual prediction values (i.e. en-\ntropy), don’t matter, but rather the ordering of entropy taking\ninto account the labels.\nExploring the Naturalness of Buggy Code with Recurrent Neural Networks\nTable 2. Average entropy value of the 10,492 buggy lines and 10,492\nnon-buggy lines from our 2 models.\nModel\nBuggy Lines\nNon-Buggy Lines\nGlobal LSTM\n1.6498\n1.5925\nGlobal + Local LSTM\n1.5842\n1.523\nFigure 5. Comparing the AUC value of three tested models. Both\nour models (left 2) achieve a higher AUC than the baseline.\n4.4. Results\n4.4.1. ENTROPY\nSince entropy is relative, we can’t directly compare our en-\ntropy result with the entropy result from the baseline model.\nBut by showing the difference between the entropy of buggy\nlines and non-buggy lines, we can justify the validity of our\nmodel. This is shown in Table 2 and Fig 4.\nFrom Table 2 we can see that buggy lines generally have a\nlarger entropy value, which accords our assumption. Also,\nfrom Figure 4, we can see that the mixed global + local model\nhas a larger average entropy difference than just the global.\nThis proves that the local model can help in capturing some\nof the local regularities in the project.\n4.4.2. AUC\nFigure 5 shows a comparison of AUC values for the three\nmodels. We can see from the graph that both our global LSTM\nand global + local LSTM achieve a slightly larger AUC value\nthan the baseline language model, which illustrates that our\nmodel does a better job capturing the true semantics of the\nnon-buggy lines.\n5. Threats to Validity\nThe main threat to the validity of our experiments are the\ndatasets used. We believe that the training set and testing set\nare too similar, leading to an over-ﬁtted language model. Sim-\nilarly, the local vs global datasets are too similar, not allowing\nfor a better opportunity to capture local regularities. Also,\nthere are some buggy lines in the training set, but we believe\nthat since there is only a very small percentage of them, they\nare essentially disregarded in the language model. See sec-\ntion 6 for our proposal of future work which we believe could\nalleviate the issues proposed.\nThe second threat is that we use Area Under ROC Curve\n(AUC) scores to evaluate our predictions, where it has been\nshown that the Area Under the Cost-Effectiveness Curve\n(AUCEC) is the optimal metric for evaluating bug localiza-\ntion (Arisholm et al., 2010). For future work, we would like\nto evaluate our method using AUCEC.\nIt is difﬁcult to directly compare our results vs the results in\n(Ray et al., 2015) due to the fact that entropy is relative, so the\nvalues do not matter. We attempted to handle this issue using\nAUC.\nLastly, it is hard to say that this technique is generalizable\nsince we only used one small dataset. However, the main goal\nof this work was to show that more advanced language mod-\neling techniques are applicable for such software engineering\ntasks. We hope to further validate our approach by applying\nour method on a greater number of datasets.\n6. Conclusion and Future Work\nIn this work, we show that a more advanced language model,\nnamely a Long Short-term Memory (LSTM) recurrent neu-\nral network, can outperform a simpler n-gram model for the\ntask of buggy line classﬁciation. We hypothesize that LSTMs\nare able to capture better regularities in the source code than\nprevious techniques.\nFor future work, we would like to expand on this idea and im-\nplement our techniques on a new dataset. We believe that the\nbest way to evaluate this model and technique is to implement\nthe following two steps: (1) train a language model on many\ndifferent projects of the same language (e.g. Java). This will\ncreate a better “true” global language model which captures\nthe regularities of the language semantics itself. (2) Train a\nseparate language model on just one project that you are in-\nterested in predicting bugs, regardless of snapshot time. This\ncaptures a better version of the local semantics. We would\nalso like to explore different parameters of the global vs local\nweighting term λ.\nReferences\nArisholm, Erik, Briand, Lionel C, and Johannessen, Eivind B. A systematic and com-\nprehensive investigation of methods to build and evaluate fault prediction models.\nJournal of Systems and Software, 83(1):2–17, 2010.\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation\nby jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\nBengio, Yoshua. Learning deep architectures for ai. Foundations and trends R\n⃝in Ma-\nchine Learning, 2(1):1–127, 2009.\nCatal, Cagatay and Diri, Banu. A systematic review of software fault prediction studies.\nExpert systems with applications, 36(4):7346–7354, 2009.\nCollobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement. Torch7: A matlab-like\nenvironment for machine learning. In BigLearn, NIPS Workshop, number EPFL-\nCONF-192376, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\nHindle, Abram, Barr, Earl T, Su, Zhendong, Gabel, Mark, and Devanbu, Premkumar. On\nthe naturalness of software. In Software Engineering (ICSE), 2012 34th International\nConference on, pp. 837–847. IEEE, 2012.\nHochreiter, Sepp and Schmidhuber, J¨urgen. Long short-term memory. Neural computa-\ntion, 9(8):1735–1780, 1997.\nKarpathy, Andrej, Johnson, Justin, and Li, Fei-Fei. Visualizing and understanding re-\ncurrent networks. arXiv preprint arXiv:1506.02078, 2015.\nExploring the Naturalness of Buggy Code with Recurrent Neural Networks\nKingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\nMikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg S, and Dean, Jeff. Dis-\ntributed representations of words and phrases and their compositionality.\nIn Ad-\nvances in neural information processing systems, pp. 3111–3119, 2013.\nRahman, Foyzur, Khatri, Sameer, Barr, Earl T, and Devanbu, Premkumar. Comparing\nstatic bug ﬁnders and statistical prediction. In Proceedings of the 36th International\nConference on Software Engineering, pp. 424–434. ACM, 2014.\nRay, Baishakhi, Hellendoorn, Vincent, Tu, Zhaopeng, Nguyen, Connie, Godhane, Sa-\nheel, Bacchelli, Alberto, and Devanbu, Premkumar. On the” naturalness” of buggy\ncode. arXiv preprint arXiv:1506.01159, 2015.\nSundermeyer, Martin, Schl¨uter, Ralf, and Ney, Hermann. Lstm neural networks for\nlanguage modeling.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with\nneural networks. In Advances in neural information processing systems, pp. 3104–\n3112, 2014.\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru,\nGoodfellow, Ian, and Fergus, Rob. Intriguing properties of neural networks. arXiv\npreprint arXiv:1312.6199, 2013.\nWerbos, Paul J. Backpropagation through time: what it does and how to do it. Proceed-\nings of the IEEE, 78(10):1550–1560, 1990.\nYang, Zichao, Yang, Diyi, Dyer, Chris, He, Xiaodong, Smola, Alex, and Hovy, Eduard.\nHierarchical attention networks for document classiﬁcation. In Proceedings of the\n2016 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 2016.\nZhang, Xiang, Zhao, Junbo, and LeCun, Yann. Character-level convolutional networks\nfor text classiﬁcation. In Advances in Neural Information Processing Systems, pp.\n649–657, 2015.\n",
  "categories": [
    "cs.SE",
    "cs.CL",
    "cs.LG"
  ],
  "published": "2018-03-21",
  "updated": "2018-03-21"
}