{
  "id": "http://arxiv.org/abs/1803.08869v2",
  "title": "On the difficulty of a distributional semantics of spoken language",
  "authors": [
    "Grzegorz Chrupała",
    "Lieke Gelderloos",
    "Ákos Kádár",
    "Afra Alishahi"
  ],
  "abstract": "In the domain of unsupervised learning most work on speech has focused on\ndiscovering low-level constructs such as phoneme inventories or word-like\nunits. In contrast, for written language, where there is a large body of work\non unsupervised induction of semantic representations of words, whole sentences\nand longer texts. In this study we examine the challenges of adapting these\napproaches from written to spoken language. We conjecture that unsupervised\nlearning of the semantics of spoken language becomes feasible if we abstract\nfrom the surface variability. We simulate this setting with a dataset of\nutterances spoken by a realistic but uniform synthetic voice. We evaluate two\nsimple unsupervised models which, to varying degrees of success, learn semantic\nrepresentations of speech fragments. Finally we present inconclusive results on\nhuman speech, and discuss the challenges inherent in learning distributional\nsemantic representations on unrestricted natural spoken language.",
  "text": "On the difﬁculty of a distributional semantics of spoken language\nGrzegorz Chrupała\nTilburg University\ng.chrupala@uvt.nl\nLieke Gelderloos\nTilburg University\nl.j.gelderloos@uvt.nl\n´Akos K´ad´ar\nTilburg University\na.kadar@uvt.nl\nAfra Alishahi\nTilburg University\na.alishahi@uvt.nl\nAbstract\nIn the domain of unsupervised learning most\nwork on speech has focused on discovering\nlow-level constructs such as phoneme inven-\ntories or word-like units. In contrast, for writ-\nten language, where there is a large body of\nwork on unsupervised induction of semantic\nrepresentations of words, whole sentences and\nlonger texts.\nIn this study we examine the\nchallenges of adapting these approaches from\nwritten to spoken language.\nWe conjecture\nthat unsupervised learning of the semantics of\nspoken language becomes feasible if we ab-\nstract from the surface variability. We simulate\nthis setting with a dataset of utterances spo-\nken by a realistic but uniform synthetic voice.\nWe evaluate two simple unsupervised models\nwhich, to varying degrees of success, learn\nsemantic representations of speech fragments.\nFinally we present inconclusive results on hu-\nman speech, and discuss the challenges inher-\nent in learning distributional semantic repre-\nsentations on unrestricted natural spoken lan-\nguage.\n1\nIntroduction\nIn the realm of NLP for written language, unsuper-\nvised approaches to inducing semantic representa-\ntions of words have a long pedigree and a history\nof substantial success (Landauer et al., 1998; Blei\net al., 2003; Mikolov et al., 2013b). The core idea\nbehind these models is to build word representa-\ntions that can predict their surrounding context.\nIn search for similarly generic and versatile rep-\nresentations of whole sentences, various composi-\ntion operators have been applied on word repre-\nsentations (e.g. Socher et al., 2013; Kalchbrenner\net al., 2014; Kim, 2014; Zhao et al., 2015). Alter-\nnatively, sentence representations are induced via\nthe objective to predict the surrounding sentences\n(e.g. Le and Mikolov, 2014; Kiros et al., 2015;\nArora et al., 2016; Jernite et al., 2017; Logeswaran\nand Lee, 2018). Such representations capture as-\npects of the meaning of the encoded sentences,\nwhich can be used in a variety of tasks such as\nsemantic entailment or text understanding.\nIn the case of spoken language, unsupervised\nmethods usually focus on discovering relatively\nlow-level constructs such as phoneme inventories\nor word-like units. This is mainly due to the fact\nthat the key insight from distributional semantics\nthat “you shall know the word by the company it\nkeeps” (Firth, 1957) is hopelessly confounded in\nthe case of spoken language. In text two words are\nconsidered semantically similar if they co-occur\nwith similar neighbors.\nHowever, speech seg-\nments which occur in the same utterance or situ-\nation often have many other features in addition to\nsimilar meaning, such as being uttered by the same\nspeaker or accompanied by similar ambient noise.\nIn this study we show that if we can abstract\naway from speaker and background noise, we can\neffectively capture semantic characteristics of spo-\nken utterances in an unsupervised way. We present\nSegMatch, a model trained to match segments of\nthe same utterance. SegMatch utterance encod-\nings are compared to those in Audio2Vec, which\nis trained to decode the context that surrounds an\nutterance.\nTo investigate whether our represen-\ntations capture semantics, we evaluate on speech\nand vision datasets where photographic images are\npaired with spoken descriptions. Our experiments\nshow that for a single synthetic voice, a simple\nmodel trained only on image captions can capture\npairwise similarities that correlate with those in\nthe visual space.\narXiv:1803.08869v2  [cs.CL]  26 Oct 2018\nFurthermore we discuss the factors preventing\neffective learning in datasets with multiple human\nspeakers: these include confounds between se-\nmantic and situational factors as well as artifacts\nin the datasets.\n2\nRelated work\nStudies of unsupervised learning from speech typ-\nically aim to discover the phonemic or lexical\nbuilding blocks of the language signal. Park and\nGlass (2008) show that words and phrase units in\ncontinuous speech can be discovered using algo-\nrithms based on dynamic time warping. van den\nOord et al. (2017) introduce a Vector Quantised-\nVariational AutoEncoder model, in which a con-\nvolutional encoder trained on raw audio data\ngives discrete encodings that are closely related to\nphonemes. Recently several unsupervised speech\nrecognition methods were proposed that segment\nspeech and cluster the resulting word-like seg-\nments (Kamper et al., 2017a) or encode them into\nsegment embeddings containing phonetic infor-\nmation (Wang et al., 2018).\nScharenborg et al.\n(2018) show that word and phrase units arise as a\nby-product in end-to-end tasks such as speech-to-\nspeech translation. In the current work, the aim is\nto directly extract semantic, rather than word form\ninformation from speech.\nSemantic information encoded in speech is used\nin studies that ground speech to the visual context.\nDatasets of images paired with spoken captions\ncan be used to train multimodal models that extract\nvisually salient semantic information from speech,\nwithout access to textual information (Harwath\nand Glass, 2015; Harwath et al., 2016; Kamper\net al., 2017b; Chrupała et al., 2017; Alishahi et al.,\n2017; Harwath and Glass, 2017). This form of se-\nmantic supervision, through contextual informa-\ntion from another modality, has its limits: it can\nonly help to learn to understand speech describing\nthe here and now.\nOn the other hand, the success of word embed-\ndings derived by distributional semantic princi-\nples has shown how rich the semantic information\nwithin the structure of language itself is. Semantic\nrepresentations of words obtained through Latent\nSemantic Analysis have proven to closely resem-\nble human semantic knowledge (Blei et al., 2003;\nLandauer et al., 1998). Word2vec models produce\nsemantically rich word embeddings by learning\nto predict the surrounding words in text (Mikolov\net al., 2013a,b) and this principle is extended to\nsentences in the Skip-thought model (Kiros et al.,\n2015) and several subsequent works (Arora et al.,\n2016; Jernite et al., 2017; Logeswaran and Lee,\n2018).\nIn the realm of spoken language, in Chung\nand Glass (2017) the sequence-to-sequence Au-\ndio2vec model learns semantic embeddings for\naudio segments corresponding to words, by pre-\ndicting the audio segments around it.\nChung\nand Glass (2018) further experiment with this\nmodel and rename it to Speech2vec. Chen et al.\n(2018) train semantic word embeddings from\nword-segmented speech as part of their method of\ntraining an ASR system from non-aligned speech\nand text. These works are closely related to our\ncurrent study, but crucially, unlike them we do not\nassume that speech is already segmented into dis-\ncrete words.\n3\nModels\n3.1\nEncoder\nAll the models in this section use the same encoder\narchitecture. The encoder is loosely based on the\narchitecture of Chrupała et al. (2017), i.e. it con-\nsists of a 1-dimensional convolutional layer which\nsubsamples the input, followed by a stack of recur-\nrent layers, followed by a self-attention operator.\nUnlike Chrupała et al. (2017) we use GRU layers\n(Chung et al., 2014) instead of RHN layers (Zilly\net al., 2017), and do not implement residual con-\nnections. These modiﬁcations are made in order\nto exploit the fast native CUDNN implementation\nof a GRU stack and thus speed up experimenta-\ntion in this exploratory stage of our research. The\nencoder Enc is deﬁned as follows:\nEnc(x) = unit(Attn(GRUℓ(Convs,d,z(x))))\n(1)\nwhere Conv is a convolutional layer with length\ns, d channels, and stride z, GRUℓis a stack of ℓ\nGRU layers, Attn is self-attention and unit is L2-\nnormalization. The self-attention operator com-\nputes a weighted sum of the RNN activations at\nall timesteps:\nAttn(x) =\nX\nt\nαtxt\n(2)\nwhere the weights αt are determined by an MLP\nwith learned parameters U and W, and passed\nthrough the timewise softmax function:\nαt =\nexp(U tanh(Wxt))\nP\nt′ exp(U tanh(Wxt′))\n(3)\n3.2\nAudio2vec\nFirstly we deﬁne a model inspired by Chung and\nGlass (2017) which uses the multilayer GRU en-\ncoder described above, and a single-layer GRU de-\ncoder, conditioned on the output of the encoder.\nThe model of Chung and Glass (2017) works\non word-segmented speech: the encoder encodes\nthe middle word of a ﬁve word sequence, and the\ndecoder decodes each of the surrounding words.\nSimilarly, the Skip-thought model of (Kiros et al.,\n2015) works with a sequence of three sentences,\nencoding the middle one and decoding the previ-\nous and next one. In our fully unsupervised setup\nwe do not have access to word segmentation, and\nthus our Audio2vec models work with arbitrary\nspeech segments.\nWe split each utterance into\nthree equal sized chunks: the model encodes the\nmiddle one, and decodes the ﬁrst and third one.\nThe decoder predicts the MFCC features at time\nt + 1 based on the state of the hidden layer at time\nt. From reading Chung and Glass (2017) it is not\nclear whether in addition to the hidden state their\ndecoder also receives the MFCC frame at t as in-\nput. We thus implemented two versions, one with\nand one without this input.\nAudio2vec-C The decoder receives the output of\nthe encoder as the initial state of the hidden layer,\nand the frame at t as input as it predicts the next\nframe at t + 1.\nˆxﬁrst\nt+1 = Fht\n(4)\nht = gru(ht−1, xﬁrst\nt\n)\n(5)\nh0 = Enc\n\u0000xmiddle\u0001\n(6)\nwhere xﬁrst\nt\nare the MFCC features of the previous\nchunk at time t, ˆxﬁrst\nt+1 are the predicted features at\nthe next time step, F is a learned projection matrix,\ngru(·, ·) is a single step of the GRU recurrence,\nand xmiddle is the sequence of the MFCC features\nof the input. The decoder for the third chunk xthird\nis deﬁned in the same way.\nAudio2vec-U The decoder receives the output of\nthe encoder as the input at each time step, but does\nnot have access to the frame at t.\nˆxﬁrst\nt+1 = Fht\n(7)\nht = gru(ht−1, Enc(xmiddle))\n(8)\nIn this version h0 is a learned parameter. There\nare two separate decoders: i.e. the weights of the\ndecoder for the ﬁrst chunk and for the third chunk\nare not shared.\nFor both versions of Audio2vec the loss func-\ntion is the Mean Squared Error.\n3.3\nSegMatch\nThis model works with segments of utterances\nalso: we split each utterance approximately in\nhalf, while erasing a short portion in the center\nin order to prevent the model from ﬁnding triv-\nial solutions based on matching local patterns at\nthe edges of the segments. The encoder is as de-\nscribed above. After encoding the segments, we\nproject the initial and ﬁnal segments via separate\nlearned projection matrices:\nb = BEnc(x0:m)\n(9)\ne = EEnc(xm+k:n)\n(10)\nwhere x0:n is the sequence of MFCC frames for\nan utterance, k is the size of the erased segment,\nEnc(·) is the encoder and B and E are the projec-\ntion matrices for the beginning and end segment\nrespectively. That is, there is a single shared en-\ncoder for both types of speech segments (begin-\nning and end), but the projections are separate.\nThere is no decoding, but rather the model learns\nto match encoded segments from the same utter-\nance and distinguish them from encoded segments\nfrom different utterances within the same mini-\nbatch. The loss function is similar to the one for\nmatching spoken utterances to images in Chrupała\net al. (2017), with the difference that here we are\nmatching utterance segments to each other:\nL =\nX\nb,e\n X\nb′\nmax[0, α + d(b, e) −d(b′, e)]\n+\nX\ne′\nmax[0, α + d(b, e) −d(b, e′)]\n!\n(11)\nwhere (b, e) are beginning and end segments from\nthe same utterance, and (b′, e) and (b, e′) are be-\nginning and end segments from two different utter-\nances within a batch, while d(·, ·) is the cosine dis-\ntance between encoded segments. The loss func-\ntion thus attempts to make the cosine distance be-\ntween encodings of matching segments less than\nthe distance between encodings of mismatching\nsegment pairs, by a margin.\nNote that the speciﬁc way we segment speech is\nnot a crucial component of either of the models: it\nis mostly driven by the fact that we run our experi-\nments on speech and vision datasets, where speech\nconsists of isolated utterances. For data consisting\nof longer narratives, or dialogs, we could use dif-\nferent segmentation schemes.\n4\nExperimental setup\n4.1\nDatasets\nIn order to facilitate evaluation of the semantic as-\npect of the learned representations, we work with\nspeech and vision datasets, which couple pho-\ntographic images with their spoken descriptions.\nThanks to the structure of these data we can use\nthe evaluation metrics detailed in section 4.2.\nSynthetically spoken COCO This dataset was\ncreated by Chrupała et al. (2017), based on the\noriginal COCO dataset (Lin et al., 2014), using the\nGoogle TTS API. The captions are spoken by a\nsingle synthetic voice, which is realistic but sim-\npler than human speakers, lacking variability and\nambient noise. There are 300,000 images, each\nwith ﬁve captions. Five thousand images each are\nheld out for validation and test.\nFlickr8k Audio Caption Corpus\nThis dataset\n(Harwath and Glass, 2015) contains the captions\nin the original Flickr8K corpus (Hodosh et al.,\n2013) read aloud by crowdworkers.\nThere are\n8,000 images, each image with ﬁve descriptions.\nOne thousand images are held out for validation,\nand another one thousand for the test set.\nPlaces Audio Caption Corpus This dataset was\ncollected by (Harwath et al., 2016) using crowd-\nworkers. Here each image is described by a single\nspontaneously spoken caption. There are 214,585\ntraining images, and 1000 validation images (there\nare no separate test data).\n4.2\nEvaluation metrics\nWe evaluate the quality of the learned semantic\nspeech representations according to the following\ncriteria.\nParaphrase retrieval For the Synthetically Spo-\nken COCO dataset as well as for the Flickr8k Au-\ndio Caption Corpus each image is described via\nﬁve independent spoken captions. Thus captions\ndescribing the same image are effectively para-\nphrases of each other. This structure of the data\nallows us to use a paraphrasing retrieval task as\na measure of the semantic quality of the learned\nspeech embeddings. We encode each of the spo-\nken utterances in the validation data, and rank\nthe others according to the cosine similarity. We\nthen measure: (a) Median rank of the top-ranked\nparaphrase; and (b) recall@K: the proportion of\nparaphrases among K top-ranked utterances, for\nK ∈{1, 5, 10}.\nRepresentational\nsimilarity\nto\nimage\nspace\nRepresentational\nsimilarity\nanalysis\n(RSA) is a way of evaluating how pairwise\nsimilarities between objects are correlated in\ntwo object representation spaces (Kriegeskorte\net al., 2008). Here we compare cosine similarities\namong encoded utterances versus cosine simi-\nlarities among vector representations of images.\nSpeciﬁcally, we create two pairwise N × N\nsimilarity matrices: (a) among encoded utterances\nfrom the validation data, and (b) among images\ncorresponding to each utterance in (a). Note that\nsince there are ﬁve descriptions per image, each\nimage is replicated ﬁve times in matrix (b). We\nthen take the upper triangulars of these matrices\n(excluding the diagonal) and compute Pearson’s\ncorrelation coefﬁcient between them. The image\nfeatures for this evaluation are obtained from the\nﬁnal fully connected layer of VGG-16 (Simonyan\nand Zisserman, 2014) pre-trained on Imagenet\n(Russakovsky et al., 2014) and consist of 4096\ndimensions.\n4.3\nSettings\nWe preprocess the audio by extracting 12-\ndimensional mel-frequency cepstral coefﬁcients\n(MFCC) plus log of the total energy. We use 25\nmilisecond windows, sampled every 10 milisec-\nonds.\nAudio2vec and SegMatch models are\ntrained for a maximum of 15 epochs with Adam,\nwith learning rate 0.0002, and gradient clipping at\n2.0. SegMatch uses margin α = 0.2. The en-\ncoder GRU has 5 layers of 512 units. The con-\nvolutional layer has 64 channels, size of 6 and\nstride 3. The hidden layer of the attention MLP\nis 512. The GRU of the Audio2vec decoder has\n512 hidden units; the size of the output of the pro-\njections B and E in SegMatch is also 512 units.\nFor SegMatch the size of the erased center portion\nof the utterance is 30 frames. We apply early stop-\nping and report all the results of each model after\nthe epoch for which it scored best on recall@10.\nWhen applying SegMatch on human data, each\nmini-batch includes utterances spoken only by one\nspeaker: this is in order to discourage the model\nfrom encoding speaker-speciﬁc features.\n5\nResults\n5.1\nSynthetic speech\nTable 1 shows the evaluation results on synthetic\nspeech.\nRepresentations learned by Audio2vec\nand SegMatch are compared to the performance of\nrandom vectors, mean MFCC vectors, as well as\nvisually supervised representations (VGS, model\nfrom Chrupała et al. (2017)). Audio2vec works\nbetter than chance and mean MFCC on paraphrase\nretrieval, but does not correlate with the visual\nspace.\nSegMatch works much better than Au-\ndio2vec according to both criteria.\nIt does not\ncome close to VGS on paraphrase retrieval, but it\ndoes correlate with the visual modality even better.\n5.2\nHuman speech\nPlaces\nThis dataset only features a single cap-\ntion per image and thus we only evaluate accord-\ning to RSA: with both SegMatch and Audio2vec\nwe found the correlations to be zero.\nFlickr8K\nInitial\nexperiments\nwith\nFlickr8K\nwere similarly unsuccessful.\nAnalysis of the\nlearned SegMatch representations revealed that in\nspite of partitioning the data by speaker for train-\ning, speaker identity can be decoded from them.\nEnforcing speaker invariance\nWe thus imple-\nmented a version of SegMatch where an auxiliary\nspeaker classiﬁer is connected to the encoder via a\ngradient reversal operator (Ganin and Lempitsky,\n2015). This architecture optimizes the main loss,\nwhile at the same time pushing the encoder to re-\nmove information about speaker identity from the\nrepresentation it outputs. In preliminary experi-\nments we saw that this addition was able to prevent\nspeaker identity from being encoded in the rep-\nresentations during the ﬁrst few epochs of train-\ning. Evaluating this speaker-invariant representa-\ntion gave contradictory results, shown in Table 2:\nvery good scores on paraphrase retrieval, but zero\ncorrelation with visual space.\nFurther analysis showed that there seems to be\nan artifact in the Flickr8K data where spoken cap-\ntions belonging to consecutively numbered images\nshare some characteristics, even though the im-\nages do not.\nAs a side effect, this causes cap-\ntions belonging to the same image to also share\nsome features, independent of their semantic con-\ntent, leading to high paraphrasing scores. The arti-\nfact may be due to changes in data collection pro-\ncedure which affected some aspect of the captions\nin ways which correlate with their sequential or-\ndering in the dataset.\nIf we treat the image ID number as a regression\ntarget, and the ﬁrst two principal components of\nthe SegMatch representation of one of its captions\nas the predictors, we can account for about 12%\nof the holdout variance in IDs using a non-linear\nmodel (using either K-Nearest Neighbors or Ran-\ndom Forest). This effect disappears if we arbitrar-\nily relabel images.\n6\nConclusion\nFor synthetic speech the SegMatch approach\nto inducing utterance embeddings shows very\npromising performance. Likewise, previous work\nhas shown some success with word-segmented\nspeech.\nThere remain challenges in carrying\nover these results to natural, unsegmented speech.\nWord segmentation is a highly non-trivial research\nproblem in itself and the variability of spoken lan-\nguage is a serious and intractable confounding fac-\ntor.\nEven when controlling for speaker identity there\nare still superﬁcial features of the speech signal\nwhich make it easy for the model to ignore the se-\nmantic content. Some of these may be due to arti-\nfacts in datasets and thus care is needed when eval-\nuating unsupervised models of spoken language:\nfor example use of multiple evaluation criteria\nmay help spot spurious results. In spite of these\nchallenges, in future we want to further explore\nthe effectiveness of enforcing desired invariances\nvia auxiliary classiﬁers with gradient reversal.\nReferences\nAfra Alishahi, Marie Barking, and Grzegorz Chrupała.\n2017. Encoding of phonology in a recurrent neu-\nral model of grounded speech. In Proceedings of\nthe 21st Conference on Computational Natural Lan-\nguage Learning (CoNLL 2017), pages 368–378. As-\nsociation for Computational Linguistics.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In ICLR.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of ma-\nchine Learning research, 3(Jan):993–1022.\nRecall@10 (%)\nMedian rank\nRSAimage\nVGS\n27\n6\n0.4\nSegMatch\n10\n37\n0.5\nAudio2vec-U\n5\n105\n0.0\nAudio2vec-C\n2\n647\n0.0\nMean MFCC\n1\n1,414\n0.0\nChance\n0\n3,955\n0.0\nTable 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from\nChrupała et al. (2017).\nRecall@10 (%)\nMedian rank\nRSAimage\nVGS\n15\n17\n0.2\nSegMatch\n12\n17\n0.0\nMean MFCC\n0\n711\n0.0\nTable 2: Results on Flickr8K. The row labeled VGS is the visually supervised model from Chrupała et al. (2017).\nYi-Chen Chen, Chia-Hao Shen, Sung-Feng Huang,\nand Hung-yi Lee. 2018.\nTowards unsuper-\nvised automatic speech recognition trained by un-\naligned speech and text only.\narXiv preprint\narXiv:1803.10952.\nGrzegorz Chrupała, Lieke Gelderloos, and Afra Al-\nishahi. 2017.\nRepresentations of language in a\nmodel of visually grounded speech signal. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. In NIPS 2014 Deep Learning and Representa-\ntion Learning Workshop.\nYu-An Chung and James Glass. 2017. Learning word\nembeddings from speech. In NIPS ML4Audio Work-\nshop.\nYu-An Chung and James Glass. 2018.\nSpeech2vec:\nA sequence-to-sequence framework for learning\nword embeddings from speech.\narXiv preprint\narXiv:1803.08976.\nJohn Rupert Firth. 1957. A synopsis of linguistic the-\nory 1930-1955, volume 1952-59. The Philological\nSociety.\nYaroslav Ganin and Victor Lempitsky. 2015.\nUnsu-\npervised domain adaptation by backpropagation. In\nProceedings of the 32nd International Conference\non Machine Learning, volume 37 of Proceedings\nof Machine Learning Research, pages 1180–1189,\nLille, France. PMLR.\nDavid Harwath and James Glass. 2015. Deep multi-\nmodal semantic embeddings for speech and images.\nIn IEEE Automatic Speech Recognition and Under-\nstanding Workshop.\nDavid Harwath and James R Glass. 2017.\nLearn-\ning word-like units from joint audio-visual analysis.\narXiv preprint arXiv:1701.07481.\nDavid Harwath, Antonio Torralba, and James Glass.\n2016.\nUnsupervised learning of spoken language\nwith visual context. In Advances in Neural Infor-\nmation Processing Systems, pages 1858–1866.\nMicah Hodosh, Peter Young, and Julia Hockenmaier.\n2013. Framing image description as a ranking task:\nData, models and evaluation metrics. Journal of Ar-\ntiﬁcial Intelligence Research, 47:853–899.\nYacine Jernite, Samuel R Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. arXiv\npreprint arXiv:1705.00557.\nNal Kalchbrenner, Edward Grefenstette, and Phil\nBlunsom. 2014.\nA convolutional neural net-\nwork for modelling sentences.\narXiv preprint\narXiv:1404.2188.\nHerman Kamper, Aren Jansen, and Sharon Goldwa-\nter. 2017a.\nA segmental framework for fully-\nunsupervised large-vocabulary speech recognition.\nComputer Speech & Language, 46:154–174.\nHerman\nKamper,\nShane\nSettle,\nGregory\nShakhnarovich,\nand\nKaren\nLivescu.\n2017b.\nVisually grounded learning of keyword prediction\nfrom untranscribed speech.\nIn Proc. Interspeech\n2017, pages 3677–3681.\nYoon\nKim.\n2014.\nConvolutional\nneural\nnet-\nworks for sentence classiﬁcation.\narXiv preprint\narXiv:1408.5882.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In Ad-\nvances in Neural Information Processing Systems,\npages 3276–3284.\nNikolaus Kriegeskorte, Marieke Mur, and Peter A Ban-\ndettini. 2008. Representational similarity analysis-\nconnecting the branches of systems neuroscience.\nFrontiers in systems neuroscience, 2:4.\nThomas K Landauer, Peter W Foltz, and Darrell La-\nham. 1998. An introduction to latent semantic anal-\nysis. Discourse processes, 25(2-3):259–284.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188–1196.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. In Computer Vision–\nECCV 2014, pages 740–755. Springer.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nefﬁcient framework for learning sentence represen-\ntations. arXiv preprint arXiv:1803.02893.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a.\nEfﬁcient estimation of word\nrepresentations in vector space.\narXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, pages 3111–3119.\nA¨aron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. 2017. Neural discrete representation\nlearning. CoRR, abs/1711.00937.\nAlex S Park and James R Glass. 2008.\nUnsuper-\nvised pattern discovery in speech. IEEE Transac-\ntions on Audio, Speech, and Language Processing,\n16(1):186–197.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, An-\ndrej Karpathy, Aditya Khosla, Michael Bernstein,\nAlexander C. Berg, and Li Fei-Fei. 2014. ImageNet\nlarge scale visual recognition challenge.\nOdette Scharenborg, Laurent Besacier, Alan Black,\nMark Hasegawa-Johnson, Florian Metze, Graham\nNeubig, Sebastian St¨uker, Pierre Godard, Markus\nM¨uller, Lucas Ondel, et al. 2018.\nLinguistic unit\ndiscovery from multi-modal inputs in unwritten lan-\nguages: Summary of the Speaking Rosetta JSALT\n2017 workshop. arXiv preprint arXiv:1802.05092.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. CoRR, abs/1409.1556.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank.\nIn Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nYu-Hsuan Wang, Hung-yi Lee, and Lin-shan Lee.\n2018. Segmental audio word2vec: Representing ut-\nterances as sequences of vectors with applications in\nspoken term detection. In 2018 IEEE International\nConference on Acoustics, Speech, and Signal Pro-\ncessing. Proceedings.\nHan Zhao, Zhengdong Lu, and Pascal Poupart. 2015.\nSelf-adaptive hierarchical sentence model. In IJCAI,\npages 4069–4076.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent\nhighway networks. In Proceedings of the 34th In-\nternational Conference on Machine Learning, vol-\nume 70 of Proceedings of Machine Learning Re-\nsearch, pages 4189–4198, International Convention\nCentre, Sydney, Australia. PMLR.\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "cs.SD",
    "eess.AS"
  ],
  "published": "2018-03-23",
  "updated": "2018-10-26"
}