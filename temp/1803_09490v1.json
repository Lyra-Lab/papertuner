{
  "id": "http://arxiv.org/abs/1803.09490v1",
  "title": "Unsupervised Learning and Segmentation of Complex Activities from Video",
  "authors": [
    "Fadime Sener",
    "Angela Yao"
  ],
  "abstract": "This paper presents a new method for unsupervised segmentation of complex\nactivities from video into multiple steps, or sub-activities, without any\ntextual input. We propose an iterative discriminative-generative approach which\nalternates between discriminatively learning the appearance of sub-activities\nfrom the videos' visual features to sub-activity labels and generatively\nmodelling the temporal structure of sub-activities using a Generalized Mallows\nModel. In addition, we introduce a model for background to account for frames\nunrelated to the actual activities. Our approach is validated on the\nchallenging Breakfast Actions and Inria Instructional Videos datasets and\noutperforms both unsupervised and weakly-supervised state of the art.",
  "text": "arXiv:1803.09490v1  [cs.CV]  26 Mar 2018\nUnsupervised Learning and Segmentation of Complex Activities from Video\nFadime Sener, Angela Yao\nUniversity of Bonn, Germany\n{sener,yao}@cs.uni-bonn.de\nAbstract\nThis paper presents a new method for unsupervised seg-\nmentation of complex activities from video into multiple\nsteps, or sub-activities, without any textual input. We pro-\npose an iterative discriminative-generative approach which\nalternates between discriminatively learning the appear-\nance of sub-activities from the videos’ visual features to\nsub-activity labels and generatively modelling the tempo-\nral structure of sub-activities using a Generalized Mallows\nModel. In addition, we introduce a model for background\nto account for frames unrelated to the actual activities. Our\napproach is validated on the challenging Breakfast Actions\nand Inria Instructional Videos datasets and outperforms\nboth unsupervised and weakly-supervised state of the art.\n1. Introduction\nWe address the problem of understanding complex ac-\ntivities from video sequences. A complex activity is a pro-\ncedural task with multiple steps or sub-activities that follow\nsome loose ordering. Complex activities can be found in in-\nstructional videos; YouTube hosts hundreds of thousands of\nsuch videos on activities as common as ‘making coffee’ to\nthe more obscure ‘weaving banana ﬁbre cloths’. Similarly,\nin assistive robotics, a robot that can understand and parse\nthe steps of a household task such as ‘doing laundry’ can\nanticipate and support upcoming steps or sub-activities.\nComplex activity understanding has received little at-\ntention in the computer vision community compared to\nthe more popular simple action recognition task. In sim-\nple action recognition, short, trimmed clips are classiﬁed\nwith single labels, e.g. of sports, playing musical instru-\nments [10, 27], and so on. Performance on simple action\nrecognition has seen a remarkable boost with the use of\ndeep architectures [10, 25, 29]. Such methods however are\nrarely applicable for temporally localizing and/or classify-\ning actions from longer, untrimmed video sequences, usu-\nally due to the lack of temporal consideration. Even works\nwhich do incorporate some modelling of temporal struc-\nture [4, 24, 28, 29] do little more than capturing frame-to-\nframe changes, which is why the state of the art still relies\non either optical ﬂow [25] or dense trajectories [29, 30].\nMoving towards understanding complex activities then be-\ncomes even more challenging, as it requires not only pars-\ning long video sequences into semantically meaningful sub-\nactivities, but also capturing the temporal relationships that\noccur between these sub-activities.\nWe aim to discover and segment the steps of a complex\nactivity from collections of video in an unsupervised way\nbased purely on visual inputs.\nWithin the same activity\nclass, it is likely that videos share common steps and follow\na similar temporal ordering. To date, works in a similar vein\nof unsupervised learning all require inputs from narration;\nthe sub-activities and sequence information are extracted ei-\nther entirely from [17, 1], or rely heavily [23] on text. Such\nworks assume that the text is well-aligned with the visual\ninformation of the video so that visual representations of\nthe sub-activity are learned from within the text’s temporal\nbounds. This is not always the case for instructional videos,\nas it is far more natural for the human narrator to ﬁrst speak\nabout what will be done, and then carry out the action. Fi-\nnally, reliably parsing spoken natural language into scripts1\nis an unsolved and open research topic in itself. As such, it\nis in our interest to rely only on visual inputs.\nIn this work, we propose an iterative model which al-\nternates between learning a discriminative representation of\na video’s visual features to sub-activities and a generative\nmodel of the sub-activities’ temporal structure. By com-\nbining the sub-activity representations with the temporal\nmodel, we arrive at a segmentation of the video sequence,\nwhich is then used to update the visual representations (see\nFig. 1a). We represent sub-activities by learning linear map-\npings from visual features to a low dimensional embedding\nspace with a ranking loss. The mappings are optimized such\nthat visual features from the same sub-activity are pushed\ntogether, while different sub-activities are pulled apart.\nTemporally, we treat a complex activity as a sequence\nof permutable sub-activities and model the distribution\nover permutations with a Generalized Mallows Model\n(GMM) [5]. GMMs have been successfully used in the NLP\n1Here, we refer to the NLP deﬁnition of script as “a predetermined,\nstereotyped sequence of actions that deﬁne a well-known situation” [22].\n1\nsegmented video collection\n...\n...\n...\n...\n...\n...\nsub-activity assigment\nfor all videos\n1\n2\n3\n4\n5\norder\nsub-activity counts\n1\n2\n3\n4\n5\n4  4\n4  4\n4\nbackground indicator \n(a) Overview\nRequire: K, Q, F, θ0, α, β, ρ0, ν0\nEnsure: z and ρ\n1: initialize a, b, π and construct z\n2: randomly initialize W\n3: for each iteration do\n4:\nlearn W with z\n5:\nfor k = 1 →K do\n6:\nlearn {ωk, µk, Σk}\n7:\nend for\n8:\nfor i = 1 →M do\n9:\nfor j = 1 →Ji do\n10:\nfor k = 1 →K do\n11:\nP (bij=0, aij=k| . . . ) ←Eq. 17\n12:\nend for\n13:\nP (bij=1, aij| . . . ) ←Eq. 18\n14:\nend for\n15:\n{aij, bij}←draw from P (bij, aij| . . . )\n16:\nfor k = 1 →K −1 do\n17:\nP (vik| . . . ) ←Eq. 14\n18:\nend for\n19:\nvij ←draw from P (vik| . . . )\n20:\nend for\n21:\nfor k = 1 →K −1 do\n22:\nP (ρk| . . . ) ←Eq. 3\n23:\nend for\n24:\nconstruct z with new a, b, π\n25: end for\n(b) Algorithm\nFigure 1: (a) Our iterative model alternates between learning visual appearance and temporal structure of sub-activities. We\ncombine visual appearance with a temporal model to obtain a segmentation of video sequences which is then used to update\nthe visual appearance representation for the next iteration. (b) Algorithm for our model. (Figure best viewed in color.)\ncommunity to model document structures [3] and script\nknowledge [7]. In our method, the GMM assumes that a\ncanonical sequence ordering is shared among videos of the\nsame complex activity. There are several advantages of us-\ning the GMM for modelling temporal structure. First and\nforemost, the canonical ordering enforces a global ordering\nconstraint over the activity – something not possible with\nMarkovian models [12, 19, 23] and recurrent neural net-\nworks (RNNs) [32]. Secondly, considering temporal struc-\nture as a permutation offers ﬂexibility and richness in mod-\nelling. We can allow for missing steps and deviations, all\nof which are characteristic of complex activities, but cannot\nbe accounted for with works which enforce a strict order-\ning [1]. Finally, the GMM is compact – parameters grow\nlinearly with the number of sub-activities, versus quadratic\ngrowth in pairwise relationships, e.g. in HMMs.\nWithin a video, it is unlikely that every frame corre-\nsponds to a speciﬁed sub-activity; they may be interspersed\nwith unrelated segments of actors talking or highlighting\nprevious or subsequent sub-activities. Depending on how\nthe video is made, such segments can occur arbitrarily. It\nbecomes difﬁcult to maintain a consistent temporal model\nunder these uncertainties, which in turn affects the qual-\nity of visual representations. In this paper we extend our\nsegmentation method to explicitly learn about and repre-\nsent such “background frames” so that we can exclude them\nfrom the temporal model. To summarize our contributions:\n• We are the ﬁrst to explore a fully unsupervised method\nfor temporal understanding of complex activities in\nvideo without requiring any text. We design a discrim-\ninative appearance learning model to enable the use of\nGMMs on state-of-the-art visual features [21, 29, 30].\n• We verify our method on real-world videos of complex\nactivities which do not follow strict orderings and are\nheavily interspersed with background frames.\n• We demonstrate that our method achieves competitive\nresults comparable to or better than the state of the art\non two challenging complex activity datasets, Break-\nfast Actions [12] and Inria Instructional Videos [1].\n2. Related Work\nModelling temporal structures in activities has been fo-\ncused predominantly at a frame-wise level [4, 24, 28, 29].\nExisting works on complex activity understanding typically\nrequire fully annotated video sequences with start and end\npoints of each sub-activity [12, 18, 20]. Annotating every\nframe in videos is expensive and makes it difﬁcult to work at\na large scale. Instead of annotations, a second line of work\ntries to use cues from accompanying narrations [1, 17, 23].\nThese works assume that the narrative text is well-aligned\nwith the visual data, with performance governed largely by\nthe quality of the alignment. For example, in the work of\nAlayrac et al. [1], instruction narrations are used as tempo-\nral boundaries of sub-activities for discriminative cluster-\ning. Sener et al. [23], represent every frame as a concate-\nnated histogram of text and visual words, which are used\nas input to a probabilistic model. The applicability of these\n2\nmethods is limited because neither the existence of accom-\npanying text, nor their proper alignment to the visual data\ncan be taken for granted.\nMore recent works focus on developing weakly-\nsupervised solutions, i.e. where the orderings of the sub-\nactivities are provided either only during training [9, 19]\nor testing as well [2].\nThese methods try to align the\nframes to the given ordered sub-activities. Similar to us,\nthe work of Bojanowski et al. [2] includes a “background”\nclass. However, they assume that the background appears\nonly once between every consecutive pair of sub-activities,\nwhile our model does not force any constraints on the oc-\ncurrence of background. Others [9, 19] borrow temporal\nmodelling methods from speech recognition such as con-\nnectionist temporal classiﬁcation, RNNs and HMMs.\nIn the bigger scope of temporal sequences, several pre-\nvious works have also addressed unsupervised segmenta-\ntion [6, 11, 33]. Similar to us in spirit is the work of Fox\net al. [6], which proposes a Bayesian nonparametric ap-\nproach to model multiple sets of time series data concur-\nrently. However, it has been applied only to motion capture\ndata. Since skeleton poses are lower-dimensional and ex-\nhibit much less variance than video, it is unlikely for such\na model to be directly applicable to video without a strong\ndiscriminative appearance model. To our knowledge, we\nare the ﬁrst to tackle the problem of complex activity seg-\nmentation working solely with visual data without any su-\npervision.\n3. The Generalized Mallows Model (GMM)\nThe GMM models distributions over orderings or per-\nmutations. In the standard Mallows model [16], the proba-\nbility of observing some ordering π is deﬁned by a disper-\nsion parameter ρ and a canonical ordering σ,\nPMM(π|σ, ρ) = e−ρ·d(π,σ)\nψ(ρ)\n,\n(1)\nwhere any distance metric for rankings or orderings can be\nused for d(·, ·). The extent to which the probability de-\ncreases as π differs from σ is controlled by a dispersion\nparameter ρ > 0; ψ(ρ) serves as a normalization constant.\nThe GMM, ﬁrst introduced by Fligner and Verducci [5],\nextends the standard Mallows model by introducing a set\nof dispersion parameters ρ = [ρ1, ..., ρK−1], to allow indi-\nvidual parameterization of the K elements in the ordering.\nThe GMM represents permutations as a vector of inversion\ncounts v = [v1, ..., vK−1] with respect to an identity permu-\ntation (1, ..., K), where element vk corresponds to the total\nnumber of elements in (k + 1, . . . , K) that are ranked be-\nfore k in the ordering π2. If we assume that σ is the identity\n2Only K −1 elements are needed since vK is 0 by deﬁnition as there\ncannot be any elements greater than K.\npermutation, then a natural distance d(π, σ) can be deﬁned\nas P\nk ρkvk, leading to\nPGMM(v|ρ) = e−P\nk ρkvk\nψk(ρ)\n=\nK−1\nY\nk\ne−ρkvk\nψk(ρk),\n(2)\nwith ψk(ρk)= 1−e−(K−k+1)ρk\n1−e−ρk\nas the normalization.\nAs the GMM is an exponential distribution, the natural\nprior for each element ρk is the conjugate:\nPGMM0(ρk|vk,0, ν0) ∝e−ρkvk,0−log(ψk(ρk))ν0,\n(3)\nwith hyper-parameters vk,0 and ν0. Intuitively, the prior\nstates that over ν0 previous trials, ν0 · vk,0 inversions will\nbe observed [3]. For simplicity, we do not set multiple pri-\nors for each k and use a common prior ρ0 as per [3], such\nthat\nvk,0 =\n1\neρ0−1 −\nK −k + 1\ne(K−k+1)ρ0 −1.\n(4)\n4. Proposed Model\nAssume we are given a collection of M videos, all of\nthe same complex activity, and that each video is composed\nof an ordered sequence of multiple sub-activities. A single\nvideo i with Ji frames can be represented by a design ma-\ntrix of features Fi ∈RJi×D, where D is the feature dimen-\nsion. We further deﬁne F as the concatenated design matrix\nof features from all M videos and F\\i as the features ex-\ncluding video i. We ﬁrst describe how we discriminatively\nlearn the features F in Sec. 4.1 before describing the stan-\ndard temporal model in Sec. 4.2 and the full model which\nmodels background frames in Sec. 4.3.\n4.1. Sub-Activity Visual Features\nWithin a video collection of a complex activity there\nmay be huge variations in visual appearance, even with\nstate-of-the-art visual feature descriptors [21, 29, 30]. Sup-\npose for frame j of video i we have video features Xij with\ndimensionality V . These features, if clustered naively, are\nmost likely to group together according to video rather than\nsub-activity. To cluster the features more discriminantly, we\nlearn a linear mapping of these features into a latent embed-\nding space, i.e. Φf(Xij) : RV →RE. We also deﬁne in\nthe latent space K anchor points, with locations determined\nby a second mapping Φa(k) : {1, . . . , K} →RE. More\nspeciﬁcally,\nΦf(Xij) = WfXij,\nWf ∈RE×V\n(5)\nΦa(k) = Wa(k),\nWa ∈RE×K\n(6)\nwhere Wf and Wa are the learned embedding weights and\nE is the dimensionality of the joint latent space.\nHere,\nWa(k) is the k-th column of Wa, which corresponds to\nthe location of anchor k in the latent space. Together, Wf\nand Wa make up the parameter W. We use the similarity\n3\nof the video feature with respect to these anchor points as a\nvisual feature descriptor, i.e.\nFij = Wa\n⊺WfXij,\n(7)\nwhere Fij = [f 1, ..., f K]ij. Each element f k\nij is inversely\nproportional to the distance between Xij and anchor point\nk in the latent space. By using K anchor points, this implies\nthat D = K.\nOur objective in learning the embeddings is to cluster the\nvideo features discriminatively. We achieve this by encour-\naging the Xij belonging to the same sub-activity to cluster\nclosely around a single anchor point while being far away\nfrom the other anchor points. If we assign each anchor point\nto a given sub-activity, then we can learn W by minimizing\na pair-wise ranking loss L, where\nL =\nM,Ji\nX\ni,j\nK\nX\nk=1,k̸=k∗\nmax[0, f k\nij −f k∗\nij + ∆] + γ||W||2\n2. (8)\nIn this loss, k∗is the anchor point associated with the true\nsub-activity label for Fij, ∆is a margin parameter and γ is\nthe regularization constant for the l2 regularizer of W. The\nloss in Eq. 8 encourages the distance of Xij in the latent\nspace to be closer to the anchor point k∗associated with the\ntrue sub-activity than any other anchor point by a margin ∆.\nThe above formulation assumes that the right anchor\npoint k∗, i.e. the true sub-activity label, is known. This is\nnot the case in an unsupervised scenario so we follow an\niterative approach where we learn W at each iteration from\nan assumed sub-activity based on the segmentation of the\nprevious iteration. More details are given in Sec. 4.4.\n4.2. Standard Temporal Model\nGiven a collection of M videos of the same complex\nactivity, we would like to infer the sub-activity assignments\nz = {zi}, i ∈{1, . . . , M}. For video i, zi = {zij}, j ∈\n{1, . . ., Ji}, zij ∈{1, . . ., K} can be assigned to one of\nK possible sub-activities3. We introduce ai, a bag of sub-\nactivity labels for video i, i.e. the collection of elements in\nzi but without consideration for the temporal frame order-\ning. The ordering is then described by πi. ai is expressed\nas a vector of counts of the K possible sub-activities, while\nπi is expressed as an ordered list. Together, ai and πi de-\ntermine the sub-activity label assignments zi to the frames\nof video i. (a, π) are redundant to z; the extra set of vari-\nables gives us the ﬂexibility to separately model the sub-\nactivities’ visual appearance (based on a) from the tempo-\nral ordering (based on π). We model a as a multinomial,\nwith parameter θ and a Dirichlet prior with hyperparameter\nθ0. For the ordering π, we use a GMM with the exponential\nprior from Eq. 3 and hyperparameters ρ0 and ν0. The joint\ndistribution of the model factorizes as follows:\n3For convenience, we overload the use of K for both the number of ele-\nments in the ordering for the GMM as well as the number of sub-activities,\nas the two are equal when applying the GMM.\nF\nz\na\nθ\nθ0\nπ\nv\nρ\nρ0, ν0\nJi\nM\n(a) Standard model\nF\nz\na\nθ\nθ0\nπ\nv\nρ\nρ0, ν0\nb\nλ\nα, β\nJi\nM\n(b) Full model with background\nFigure 2: Plate diagrams of our models. Shaded nodes:\nobserved variables, rectangles:\nﬁxed hyper-parameters,\ndashed arrows: deterministically constructed variables.\nP(z, θ,ρ, F|θ0, ρ0, ν0)\n=P(F|z)P(a|θ)P(π|ρ)P(θ|θ0)P(ρ|ρ0, ν0)\n=\nh M,Ji\nY\ni,j=1\nP(Fij|zij)\nih M\nY\ni=1\nP(ai|θ)P(πi|ρ)\ni\nh K\nY\nk=1\nP(θk|θ0)\nih K−1\nY\nk=1\nP(ρk|ρ0, ν0)\ni\n,\n(9)\nbased on the assumption that each frame of each video as\nwell as each video are all independent observations.\nWe show a diagram of the model in Fig. 2a.\nWhen\nusing the GMM, performing MLE to ﬁnd a consensus or\ncanonical ordering over a set of observed orderings is an\nNP hard problem, though several approximations have been\nproposed. Our case is the reverse, in which we assume that\na canonical ordering is already given and we would like\nto ﬁnd a (latent) set of orderings. Our interest is to infer\nthe posterior P(z, ρ|F, θ0, ρ0, ν0) for the entire video cor-\npus. Directly working with this posterior is intractable, so\nwe make MCMC sampling-based approximations. Speciﬁ-\ncally, we use slice sampling for ρ and collapsed Gibbs sam-\npling [8] for z. Since z is fully speciﬁed by a and π, it is\nequivalent to sample a and π. Before elaborating on the\nsampling equations, we ﬁrst detail how we model the video\nlikelihood P(Fi|zi).\nVideo likelihood P(Fi|zi) can be broken down into the\nproduct of frame likelihoods, since each frame is condition-\nally independent given the frame’s sub-activity, i.e.\nP(Fi|zi, F\\i, z\\i) =\nJi\nY\nj=1\nP(Fij|zij, F\\i, z\\i).\n(10)\nSince our temporal model is generative, we need to make\nsome assumptions about the generating process behind the\nvideo features. We directly model the frame likelihoods and\nuse K mixtures of Gaussians, one for each sub-activity k.\nEach mixture has Q components with weights ωk, means\n4\nµk and covariances Σk, with likelihood scores for each\nmixture selected according to the assignments zij:\nP(Fij|zij = k, F\\i, z\\i) ∼\nQ\nX\nq=1\nωq\nk · N(µq\nk, Σk).\n(11)\nSampling sub-activity ai is done with collapsed Gibbs\nsampling. Recall that a is modelled as a multinomial with\nK outcomes parameterized by θ. We sample aij, the j-th\nframe for video i, from the posterior conditioned on all\nother variables. Without the redundant terms, this posterior\nis expressed as\nP(aij = k| . . . ) ∝P(aij = k|a\\ij, θ0) · P(Fi|zi, F\\i, z\\i),\n(12)\nwhere the second term is the video likelihood from Eq. 10.\nThe ﬁrst term is a prior over the sub-activities, and can be\nestimated by integrating over θ. The integration is done\nvia the collapsed Gibbs sampling, and, as we assumed that\nθ ∼Dirichlet(θ0), this results in\nP(aij = k|a\\ij, θ0) =\nNk + θ0\nPK\nk=1 Nk + Kθ0\n,\n(13)\nwhere Nk is the total number of times the sub-activity k\nis observed in the all sequences and PK\nk=1 Nk is the total\nnumber of sub-activity assignments.\nNote that sampling aij does not correspond to the sub-\nactivity assignment to the j-th frame. The assignment is\ngiven by zij, which can only be computed after sampling\naij for all Ji frames of video i and then re-ordering the bag\nof frames according to πi.\nSampling ordering πi\nis done via regular Gibbs sam-\npling. Recall that the ordering follows a GMM as described\nin Sec. 3 and is parameterized for elements in the order-\ning individually via inversion count vector vi. As such, we\nsample a value for each position in the inversion count vec-\ntor from k = 1 to K −1 independently according to\nP(vik = c|z, ρ, F) ∝P(vik = c|ρk), ·P(Fi|zi, F\\i, z\\i), (14)\nwhere c indicates the inversion count assignment to vik.\nAgain, the second term is the video likelihood from Eq. 10,\nwhile the ﬁrst term corresponds to PGMM(vik = c; ρk), and\nis computed according to Eq. 2. We estimate the probabil-\nity of every possible value of vik, which ranges from 0 to\nK −k, and sample a new inversion count value c based on\nthese probabilities.\nSampling GMM dispersion parameter ρk:\nThis is done\nfor each sub-activity k = 1 to K −1 independently. We\ndraw ρk using slice sampling [15] from the conjugate prior\ndistribution PGMM0 according to Eq. 3.\n4.3. Background Modeling\nTo consider background, we extend the label assign-\nment vector z with a binary indicator variable bij ∈{0, 1}\nfor each frame.\nThe indicator bij follows a Bernoulli\nvariable parameterized by λ, with a beta prior, i.e. λ ∼\nBeta(α, β).\nIn this setting, zi is determined by the bag\nof sub-activities ai, the ordering πi, and background vec-\ntor bi = {bij}, where bi indicates the frames to be ex-\ncluded from sub-activity consideration. For example, for\nvideo i, given ai = [6 3 5], πi = [2 3 1] and bi =\n[11100111001100011110011], the sub-activity assignment\nis zi =[22200333003300011110011].\nWe show a diagram of the model in Fig. 2b. The joint\ndistribution of the model can be expressed as\nP(z,θ, ρ, F|θ0, α, β, ρ0, ν0, ) = P(a|θ, θ0)\n·P(π|ρ, ρ0, ν0) · P(b|λ, α, β) · P(F|a, π, b).\n(15)\nDrawing samples from this full model requires a small mod-\niﬁcation to the sub-activity sampling ai. More speciﬁcally,\nwe need a blocked collapsed Gibbs sampler that samples\naij and bij jointly while integrating over θ and λ.\nSampling background bi is done from the joint condi-\ntional\nP(bij, aij| . . . ) ∝P(bij|α, β) · P(aij|a\\ij, θ0) · P(Fi|zi, F\\i, z\\i). (16)\nThis is equivalent to the following for a sub-activity frame:\nP(bij =0, aij =k| . . . ) ∝\nNf + α\nNf + Nb + α + β\n·\nNk + θ0\nPK\nk=1 Nk + Kθ0\n· P(Fi|bij =0, aij =k, F\\i, z\\ij),\n(17)\nwhere Nf and Nb are the total number of sub-activity\nframes and background frames in the corpus respectively.\nFor a background frame, the joint conditional is equal to\nP(bij =1, aij| . . . ) ∝\nNb+α\nNf+Nb+α+β · P(Fi|bij =1, aij, F\\i, z\\ij). (18)\nThe video likelihood in Eqs. 17 and 18 are computed in a\nsimilar way as deﬁned in Eqs. 10 and 11, with the exception\nthat we now iterate over the joint states of background and\nsub-activity labels for the frame likelihoods. Note that this\nonly adds one extra probability in being computed, i.e. b =\n1, since the state of aij is then irrelevant. The rest of the\nGibbs sampling remains the same.\n4.4. Inference Procedure\nOur model’s inputs are the frames X, the number of sub-\nactivities K and the number of Gaussian mixtures Q. We\niterate between solving for F and sampling z and ρ from\nthe posterior P(z, ρ|F, θ0, α, β, ρ0, ν0). To initialize zi for\neach video i, the sub-activity counts ai are split uniformly\nover K sub-activities; πi is set to the canonical ordering; bi\nis set with every other frame being background (see Fig. 1a).\nUsing the current assignments z, we ﬁrst learn W of the\nlatent embeddings to solve for F and then for each sub-\nactivity k, the Gaussian mixture components {ωk, µk, Σk}.\nFor each video i, we then proceed to re-sample {ai, bi}, πi,\nin that order, using Gibbs sampling to construct zi. After\n5\nvideo3\nGT\nINIT\n...\n...\n...\nFINAL\nsubactivity\nbackground\nvideo13\nbackground\nsubactivity\nvideo14\nsubactivity\nbackground\nFigure 3: Segmentation outputs on three ‘making coffee’ examples from Inria Instructional Videos Dataset [1]. Colors\nindicate different sub-activities, black the background frames. Since our algorithm is fully unsupervised, we established\none-to-one color mappings between the ground truth and our outputs for visualization purposes. The ﬁrst row (GT) is the\nground truth; the remaining rows show the progression from the initialization (INIT) over some iterations to the (FINAL)\nsegmentation. Our method performs well when the appearance of the sub-activities is discriminative, e.g. for video 3, oc-\ncurrence of a hand during a sub-activity vs. none during the background frames, or people talking for video 13. We fail in\ndetecting background when there are also interactions with objects of interest, e.g. in video 14. Our model does not enforce\ncontinuity over the background frames and may result in fragmentation, but as shown, with good appearance modelling, the\nbackground clusters naturally. Furthermore, the ﬁnal segmentations may contain a different number of sub-activities while\nstill maintaining a global order, e.g. the orange sub-activity tends to appear last and follows the grey one.\nrepeating for each video, we can then re-sample the disper-\nsion parameter ρ. From the new z and ρ, we then repeat.\nThis process is summarized in the algorithm in Fig. 1b.\nTo optimize Eq. 8 for learning W, we use Stochastic\nGradient Descent (SGD) with mini-batches of 200 and mo-\nmentum of 0.9.\nWe set the hyper-parameters ρ0 = 1,\nα = 0.2, β = 0.2, ν0 = 0.1, θ0 = 0.1.\n5. Experimentation\n5.1. Datasets & Evaluation Metrics\nWe analyze our model’s performance on two challeng-\ning datasets, Breakfast Actions [12] and Inria Instructional\nVideos [1].\nBreakfast Actions has 1,712 videos of 52\nparticipants performing 10 breakfast preparation activities.\nThere are 48 sub-activities, and videos vary according to the\nparticipants’ preference of preparation style and orderings.\nWe use the visual features from [13] based on improved\ndense trajectories [31]. This dataset has no background.\nInria Instructional Videos contains 150 narrated videos\nof 5 complex activities collected from YouTube. The videos\nare on average 2 minutes long with 47 sub-activities. We\nuse the visual features provided by [1]: improved dense\ntrajectories and VGG-16 [26] conv5 layer responses taken\nover multiple windows per frame. The trajectory and CNN\nfeatures are each encoded with bag-of-words and concate-\nnated for each frame. The videos are labelled, including the\nbackground, i.e. frames in which the sub-activity is not vi-\nsually discernible, usually when the person stops to explain\npast, current or upcoming steps. As such, the sub-activities\nare separated by hundreds of background frames (73% of\nall frames). We evaluate our standard model without back-\nground modelling by removing these frames from the se-\nquence as well as our full model on the original sequences.\nTo evaluate our segmentations in the fully unsupervised\nsetting, we need one-to-one mappings between the segment\nand ground truth labels. In line with [1, 23], we use the\nHungarian method to ﬁnd the mapping that maximizes the\nevaluation scores and then evaluate with three metrics: The\nmean over frames (Mof) evaluates temporal localization of\nsub-activities and indicates the percentage of frames cor-\nrectly labelled. The Jaccard index, computed as intersection\nover detections, as well as the F1 score quantify differences\nbetween ground truth and predicted segmentations. With all\nthree measures, higher values indicate better performance.\nWe also show a partly supervised baseline in which we\nuse ground truth sub-activity labels for learning F but learn\nthe temporal alignments unsupervised. This can be thought\nof as an upper bound on performance for our fully unsu-\npervised version, in which we iteratively learn the temporal\nalignment and discover the visual appearance of the sub-\nactivities. We refer to these to as “ours GT” and “ours\niterated” respectively in the experimental results.\n5.2. Sub-Activity Visual Appearance Modelling\nBy projecting the frames’ visual features and the sub-\nactivity labels into a joint feature space, we learn a visual\nappearance model for the sub-activities. We ﬁrst consider\nthe standard model on Inria Instructional Videos with the\nbackground frames removed. The plot in Fig. 4(a) tells\nus that the appearance model can be learned successfully\nin an iterative fashion and begins to stabilize after approx-\nimately 5 iterations between learning the sub-activity ap-\npearance and the GMM. Our model’s performance depend-\n6\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\niterations\n0\n0.2\n0.4\n0.6\nMean accuracy (Mof)\n(a) Convergence\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n# components, Q\n0\n0.1\n0.2\n0.3\n0.4\nAccuracy (Mof)\n(b) vs. # mixture components\n1\n5\n10\n15\nEpochs\n0.2\n0.3\n0.4\n0.5\n0.6\nAccuracy (Mof)\n100\n200\n500\n800\n1000\n(c) Dimensionality\nFigure 4: Inﬂuence of our model’s parameters are tested on the Instructional Videos Dataset [1] without background frames.\nWe set K to the ground truth sub-activity number of all ﬁve activities. Our method’s performance over iterations is shown in\n(a), using different numbers of Gaussian mixture components in (b) and dimensionality of embedding space in (c).\ning on the the number of Gaussian mixture components Q\nis shown in Fig. 4(b). The resulting sub-activity representa-\ntions are very low-dimensional and highly separable so that\nwe achieve higher Mof with a few number of components.\nWe use Q = 3 mixture components for our iterative and\nQ = 1 for the ground truth experiments. In Fig. 4(c), we\nuse our iterated method to show the Mof for different val-\nues of E, our embedding dimensionality, over the training\nepochs. We ﬁnd only small differences in Mof for differ-\nent E values. We ﬁx the embedding size E = 200 with\n12 epochs of training and 5 iterations of sub-activity rep-\nresentation and GMM learning for subsequent experiments\non both datasets. The run time of a single iteration of our al-\ngorithm is proportional to the number of frames Ji in each\nvideo and the assumed number of sub-actvities K. On a\ncomputer with an Intel Core i7 3.30 GHz CPU, our model,\nfor a single iteration, takes approximately 115 seconds (109\nfor learning the sub-activity appearance model and 6 sec-\nonds for estimating the temporal structure).\n7 \n10\n12\n15\nK\n0\n0.2\n0.4\n0.6\n0.8\nAccuracy (Mof)\n(a) ours GT\n7 \n10\n12\n15\nK\n0\n0.2\n0.4\n0.6\n0.8\nAccuracy (Mof)\nChanging tire (11)\nMaking coffee (10)\nCPR (7)\nJump car (12)\nRepot plant (8)\n(b) ours iterated\nFigure 5: Results (Mof) on Instructional Videos [1] without\nbackground frames with varying K. The legend gives the\nground truth K for each subactivity in braces.\n5.3. Temporal Structure Modelling\nThe GMM models temporal ordering – without it, one\ncan only classify each frame’s sub-activity label based on\nthe visual appearance. Even if these appearance models\nare trained on ground truth, the segmentation results would\nbe very poor. On Inria Instructional Videos without back-\nground, the average MoF over actions is 0.322 without ver-\nsus 0.692 with the GMM (see Fig. 5).\nThe only GMM parameter is K, the number of assumed\nsub-activities. We again consider Inria Instructional Videos\nwithout background and show the Mof as a function of K,\nonce partially unsupervised (sub-activity appearance model\nfrom ground truth) and once fully unsupervised in Fig. 5(a)\nand (b) respectively. As can be expected, the Mof drops\nwhen moving from the partially to the fully unsupervised\ncase. This drop can be attributed to the fact that the Instruc-\ntional Videos Dataset is extremely difﬁcult, and exhibits a\nlot of variation across the videos. In both partially and fully\nunsupervised cases, however, the Mof remains stable with\nrespect to K, demonstrating that our method is quite ro-\nbust with respect to varying K. This is also the case once\nbackground is considered in the full model with the original\nsequences (see Fig. 6).\n5.4. Background Modelling\nIn Fig. 7a, we demonstrate the effectiveness of our full\nmodel in capturing the background in the original sequences\nin Inria Instructional Videos. Fig. 7a shows the improve-\nment in Mof once the background is accounted for in the\nmodel; there are improvements on every activity, with the\nmost signiﬁcant being a three-fold increase for ‘jump car’\ndespite the sequences being 83% background. In Fig. 3,\nwe show qualitative examples of how our model copes with\nbackground, where it succeeds, where it fails.\n5.5. Comparison to State of the Art\nInria Instructional Videos\nWe compare our full model\nto [1] in Fig. 6, 7b. The method of [1] outputs a single repre-\nsentative frame for each sub-activity and reports an F1 score\non this single frame. To make a valid comparison, since our\nwork is aimed at ﬁnding entire segments, we randomly se-\nlect a frame from each segment and then ﬁnd a one-to-one\nmapping based on [14]. Our performance across the ﬁve ac-\ntivities is consistent and varies much less than [1]. We have\nstronger performance in three out of ﬁve activities, while we\nare worse on ‘perform cpr’ and ‘changing tire’. The GMM\nis a distribution on permutations and orderings; it is by def-\ninition unable to account for repeating sub-activities but in\n‘perform CPR’, ‘give breath’ and ‘do compression’ are re-\npeated multiple times and account for more than 50% of\nthe sequence frames. In general, we attribute our stronger\nperformance to the fact that the GMM can model ﬂexible\nsub-activity orderings, while [1] enforces a strict ordering.\n7\nChanging tire (11)\n7\n10\n12\n15\nK = \n0\n0.1\n0.2\n0.3\n0.4\nF1 score\nMaking coffee (10)\n7\n10\n12\n15\nCPR (7)\n7\n10\n12\n15\nJump car (12)\n7\n10\n12\n15\nRepot plant (8)\n7\n10\n12\n15\nours\nAlayrac et al.\nUniform\nFigure 6: Comparison of our method with Alayrac et al. [1] on the Instructional Videos Dataset [1]. To be compatible to the\nmain step detection of Alayrac et al. we report the mean over 15 randomly selected frames from each segment.\nMof\nJaccard\nFully Supervised\nSVM [9]\n15.8\n-\nHTK [12]\n19.7\n-\nWeakly Supervised\nOCDC [2]\n8.9\n23.4\nECTC [9]\n27.7\n-\nFine2Coarse [18]\n33.3\n47.3\nUnsupervised\nours iterated\n34.6\n47.1\nTable 1: Comparisons on Breakfast Actions [12]. Methods\nare evaluated according to Mof and Jaccard index. For both,\na higher result indicates better performance.\nThe GMM parameter ρ has a prior with hyper-parameter ρo\n(Sec. 3). A smaller ρ0 allows more ﬂexible orderings, while\na larger ρ0 encourages the ordering π to remain similar to\nthe canonical ordering σ. In all of our reported results, we\nﬁxed ρ0 =1. We ﬁnd that for an activity such as ‘change\ntire’, which follows a strict ordering, a larger ρ0 is more\nappropriate; with ρ0=5 we are comparable to [1] (0.41 vs.\n0.42 F1 score). For ‘jump car’ our method outperforms [1],\nhowever our overall performance is the lowest as our model\nstruggles with separating the visually very similar ‘remove\ncable A’ and ‘remove cable B’.\nChanging tire\nMaking coffee\nCPR\nJump car\nRepot plant\n0\n0.2\n0.4\nAccuracy (Mof)\nbackground model\nstandard model\n(a) on [1]\nChanging tire\nMaking coffee\nCPR\nJump car\nRepot plant\n0\n0.2\n0.4\n0.6\n0.8\nF1 score\nours GT\nAlayrac et al.\n(b) vs. Alayrac et al. [1]\nFigure 7: (a) Our standard model vs. background model\non original Inria Instructional Videos sequences. The frac-\ntions of background are changing tire (0.46), making cof-\nfee (0.71), perform CPR (0.56), jump car (0.83) and re-\npot plant (0.66). (b) Comparison of our supervised set-\nting against Alayrac et al.’s supervised method on the In-\nstructional Videos Dataset [1]. Here, our model learns the\nsub-activity appearance from the ground truth annotations.\nAlayrac et al. use the ground truth annotations as constraints\nfor their discriminative clustering based algorithm.\nBreakfast Actions\nThis dataset has no background labels\nso we apply our standard model and compare with other\nfully supervised and semi-supervised approaches in Table 1.\nOf the supervised methods, the SVM method [9] classiﬁes\neach frame individually without any temporal consideration\nand achieves an Mof of 15.8%. This shows the strength (and\nnecessity) of temporal information. “Ours iterated” is the\nonly fully unsupervised method; we only set K based on\nground truth. In comparison, the weakly supervised meth-\nods [9, 18, 2] require both K as well as an ordered list of\nsub-activities as input. ECTC [9] is based on discriminative\nclustering, while OCDC [2] and Fine2Coarse [18] are both\nRNN-based methods. We ﬁnd that our fully unsupervised\napproach has performance that is state of the art.\n6. Conclusion\nIn this paper we present an unsupervised method for par-\ntitioning complex activity videos into coherent segments of\nsub-activities. We learn a function assigning sub-activity\nscores to a video frame’s visual features, we model the\ndistribution over sub-activity permutations by a General-\nized Mallows Model (GMM). Furthermore, we account for\nbackground frames not contributing to the actual activity.\nWe successfully test our method on two datasets of this\nchallenging problem and are either comparable to or out-\nperform the state of the art, even though our method is com-\npletely unsupervised, in contrast to the existing work. Our\nmethod is able to produce coherent segments, at the same\ntime being ﬂexible enough to allow missing steps and vari-\nations in ordering. Performance drops slightly for complex\nactivities including repetitive sub-activities, as the GMM\ndoes not allow for such repeating structures. In the future\nwe plan to investigate approaching this problem in a hier-\narchical manner to handle repeating blocks as a single step,\nwhich can then be further subdivided. Finally, the GMM\nis unimodal – only one canonical ordering for the set is\nassumed. This is a valid assumption for activities such as\ncooking and simple procedural tasks, but we will consider\nfor future work applying multi-modal extensions.\nAcknowledgments\nResearch in this paper was supported\nby the DFG project YA 447/2-1 (DFG Research Unit FOR\n2535 Anticipating Human Behavior).\n8\nReferences\n[1] J.-B. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev,\nand S. Lacoste-Julien. Unsupervised learning from narrated\ninstruction videos. In CVPR, 2016.\n[2] P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce,\nC. Schmid, and J. Sivic. Weakly supervised action labeling\nin videos under ordering constraints. In ECCV, 2014.\n[3] H. Chen, S. Branavan, R. Barzilay, D. R. Karger, et al. Con-\ntent modeling using latent permutations. Journal of Artiﬁcial\nIntelligence Research, 36(1):129–163, 2009.\n[4] B. Fernando, E. Gavves, J. M. Oramas, A. Ghodrati, and\nT. Tuytelaars. Modeling video evolution for action recogni-\ntion. In CVPR, 2015.\n[5] M. A. Fligner and J. S. Verducci. Distance based ranking\nmodels. Journal of the Royal Statistical Society. Series B\n(Methodological), pages 359–369, 1986.\n[6] E. B. Fox, M. C. Hughes, E. B. Sudderth, M. I. Jordan, et al.\nJoint modeling of multiple time series via the beta process\nwith application to motion capture segmentation. The Annals\nof Applied Statistics, 8(3):1281–1313, 2014.\n[7] L. Frermann, I. Titov, and M. Pinkal. A hierarchical bayesian\nmodel for unsupervised induction of script knowledge. In\nEACL, 2014.\n[8] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics.\nPNAS, 101(suppl 1):5228–5235, 2004.\n[9] D.-A. Huang, L. Fei-Fei, and J. C. Niebles. Connectionist\ntemporal modeling for weakly supervised action labeling. In\nECCV, 2016.\n[10] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\nand L. Fei-Fei. Large-scale video classiﬁcation with convo-\nlutional neural networks. In CVPR, 2014.\n[11] B. Kr¨uger, A. V¨ogele, T. Willig, A. Yao, R. Klein, and\nA. Weber.\nEfﬁcient unsupervised temporal segmentation\nof motion data. IEEE Transactions on Multimedia (TMM),\n19(4):797–812, 2017.\n[12] H. Kuehne, A. Arslan, and T. Serre. The language of actions:\nRecovering the syntax and semantics of goal-directed human\nactivities. In CVPR, 2014.\n[13] H. Kuehne, J. Gall, and T. Serre.\nAn end-to-end genera-\ntive framework for video segmentation and recognition. In\nWACV, 2016.\n[14] T. W. Liao. Clustering of time series dataa survey. Pattern\nrecognition, 38(11):1857–1874, 2005.\n[15] D. J. MacKay. Information theory, inference and learning\nalgorithms. Cambridge university press, 2003.\n[16] C. L. Mallows.\nNon-null ranking models. i.\nBiometrika,\n44(1/2):114–130, 1957.\n[17] J. Malmaud, J. Huang, V. Rathod, N. Johnston, A. Rabi-\nnovich, and K. Murphy. What’s cookin’? Interpreting cook-\ning videos using text, speech and vision. In NAACL HLT\n2015, 2015.\n[18] A. Richard and J. Gall. Temporal action detection using a\nstatistical language model. In CVPR, 2016.\n[19] A. Richard, H. Kuehne, and J. Gall.\nWeakly supervised\naction learning with rnn based ﬁne-to-coarse modeling. In\nCVPR, 2017.\n[20] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A\ndatabase for ﬁne grained activity detection of cooking activ-\nities. In CVPR, pages 1194–1201, 2012.\n[21] J. S´anchez, F. Perronnin, T. Mensink, and J. Verbeek. Im-\nage classiﬁcation with the ﬁsher vector: Theory and practice.\nIJCV, 105(3):222–245, 2013.\n[22] R. C. Schank and R. P. Abelson. Scripts, plans, and knowl-\nedge. In IJCAI, 1975.\n[23] O. Sener, A. R. Zamir, S. Savarese, and A. Saxena. Unsuper-\nvised semantic parsing of video collections. In ICCV, 2015.\n[24] S. Sharma, R. Kiros, and R. Salakhutdinov. Action recogni-\ntion using visual attention. arXiv preprint arXiv:1511.04119,\n2015.\n[25] K. Simonyan and A. Zisserman. Two-stream convolutional\nnetworks for action recognition in videos. In NIPS, 2014.\n[26] K. Simonyan and A. Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\nCoRR,\nabs/1409.1556, 2014.\n[27] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset\nof 101 human actions classes from videos in the wild. arXiv\npreprint arXiv:1212.0402, 2012.\n[28] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsu-\npervised learning of video representations using lstms. In\nICML, 2015.\n[29] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri.\nLearning spatiotemporal features with 3d convolutional net-\nworks. In ICCV, 2015.\n[30] H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Dense tra-\njectories and motion boundary descriptors for action recog-\nnition. IJCV, 103(1):60–79, 2013.\n[31] H. Wang and C. Schmid. Action recognition with improved\ntrajectories. In ICCV, 2013.\n[32] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-\nto-end learning of action detection from frame glimpses in\nvideos. In CVPR, 2016.\n[33] F. Zhou, F. De la Torre, and J. K. Hodgins.\nHierarchical\naligned cluster analysis for temporal clustering of human\nmotion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI), 35(3):582–596, 2013.\n9\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-03-26",
  "updated": "2018-03-26"
}