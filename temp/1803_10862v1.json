{
  "id": "http://arxiv.org/abs/1803.10862v1",
  "title": "A Survey on Deep Learning Methods for Robot Vision",
  "authors": [
    "Javier Ruiz-del-Solar",
    "Patricio Loncomilla",
    "Naiomi Soto"
  ],
  "abstract": "Deep learning has allowed a paradigm shift in pattern recognition, from using\nhand-crafted features together with statistical classifiers to using\ngeneral-purpose learning procedures for learning data-driven representations,\nfeatures, and classifiers together. The application of this new paradigm has\nbeen particularly successful in computer vision, in which the development of\ndeep learning methods for vision applications has become a hot research topic.\nGiven that deep learning has already attracted the attention of the robot\nvision community, the main purpose of this survey is to address the use of deep\nlearning in robot vision. To achieve this, a comprehensive overview of deep\nlearning and its usage in computer vision is given, that includes a description\nof the most frequently used neural models and their main application areas.\nThen, the standard methodology and tools used for designing deep-learning based\nvision systems are presented. Afterwards, a review of the principal work using\ndeep learning in robot vision is presented, as well as current and future\ntrends related to the use of deep learning in robotics. This survey is intended\nto be a guide for the developers of robot vision systems.",
  "text": " \n1 \nA Survey on Deep Learning Methods for Robot Vision \nJavier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto \nAdvanced Mining Technology Center & Dept. of Electrical Engineering \nUniversidad de Chile, Chile  \n{jruizd@ing.uchile.cl, ploncomi@ing.uchile.cl, c.naiomi.soto@gmail.com} \nAbstract. Deep learning has allowed a paradigm shift in pattern recognition, from using hand-crafted features \ntogether with statistical classifiers to using general-purpose learning procedures for learning data-driven \nrepresentations, features, and classifiers together. The application of this new paradigm has been particularly \nsuccessful in computer vision, in which the development of deep learning methods for vision applications has \nbecome a hot research topic. Given that deep learning has already attracted the attention of the robot vision \ncommunity, the main purpose of this survey is to address the use of deep learning in robot vision. To achieve this, a \ncomprehensive overview of deep learning and its usage in computer vision is given, that includes a description of \nthe most frequently used neural models and their main application areas. Then, the standard methodology and tools \nused for designing deep-learning based vision systems are presented. Afterwards, a review of the principal work \nusing deep learning in robot vision is presented, as well as current and future trends related to the use of deep \nlearning in robotics. This survey is intended to be a guide for the developers of robot vision systems. \nKeywords: Deep neural networks, Deep learning, Convolutional neural networks, Robot vision.  \n1   Introduction \nDeep learning1 is a hot topic in the pattern recognition, machine learning, and computer vision research communities. This can \nbe seen clearly in the large number of reviews, surveys, special issues, and workshops that are being presented, and the special \nsessions in conferences that address this topic (e.g. [9][10][11][12][13][14][16][55][56][57]). Indeed, the explosive growth of \ncomputational power and training datasets, as well as technical improvements in the training of neural networks, has allowed a \nparadigm shift in pattern recognition, from using hand-crafted features (e.g. HOG, SIFT, and LBP) together with statistical \nclassifiers to the use of data-driven representations, in which features and classifiers are learned together. Thus, the success of this \nnew paradigm is that “it requires very little engineering by hand” [12], because most of the parameters are learned from the data, \nusing general-purpose learning procedures. Moreover, the existence of public repositories with source code and parameters of \ntrained deep neural networks, as well as the existence of specific deep learning frameworks/tools such as Caffe [60], has promoted \nincreasing the use of deep learning methods. \nThe use of the deep learning paradigm has facilitated addressing several computer vision problems in a more successful way \nthan with traditional approaches. In fact, in several computer vision benchmarks, such as the ones addressing image classification, \nobject detection and recognition, semantic segmentation, and action recognition, just to name a few, most of the competitive \nmethods are now based on the use of deep learning techniques (see Table 2). In addition, most of the recent presentations at the \n                                                 \n1 The term deep is used to denote artificial neural networks with several layers, as opposed to traditional artificial neural networks with few \nlayers that are  therefore called shallow networks. \n \n2 \nflagship conferences in this area (e.g. CVPR, ECCV, ICCV) use deep learning methods or hybrid approaches that incorporate deep \nlearning.  \nDeep learning has already attracted the attention of the robot vision community [56][57]. However, given that new methods and \nalgorithms are usually developed within the computer vision community and then transferred to the robot vision community, the \nquestion is whether or not new deep learning solutions to computer vision and recognition problems can be directly transferred to \nrobot vision applications. We believe that this transfer is not straightforward considering the multiple requirements of current deep \nlearning solutions in terms of memory and computational resources, which in many cases include the use of GPUs. Furthermore, \nfollowing [54], we believe that this transfer must consider that robot vision applications have different requirements from standard \ncomputer vision applications, such as real-time operation with limited on-board computational resources, and the constraining \nobservational conditions derived from the robot geometry, limited camera resolution, and sensor/object relative pose. \nCurrently, there are several reviews and surveys related to deep learning [9][10][11][12][13][14][16]. However, they are not \nfocused on robotic vision applications, and they do not consider their specific requirements. In this context, the main motivation of \nour survey is to address the use of deep learning in robot vision. In order to achieve this, first, an overview of deep learning and its \nuse in computer vision is given, which includes a description of the most frequently used neural models and their main application \nareas. Then, the standard methodology and tools used for designing deep learning-based vision systems are presented. Afterwards, \na review of the main current work using deep learning in robot vision is presented, as well as current and future trends related to \nthe use of deep learning in robotics.  \nThis survey is intended to be a guide for developers of robot vision systems; therefore the focus is on the practical aspects of the \nuse of deep neural networks rather than on theoretical issues. It is also important to note that as Convolutional Neural Networks \n[16][17] are the most commonly used deep neural networks in vision applications, we focus our analysis on them. \nThis paper is organized as follows: in Section 2, the use of deep neural networks in computer vision is described. The historical \ndevelopment of the discipline is presented first; then some basic definitions are given; and finally the most important deep neural \nmodels are described, as well as their applications in computer vision. In Section 3, the design process of deep-learning based \nvision applications, including the case of robot vision systems, is discussed. In Section 4, a review of the recent application of \ndeep learning in robot vision systems is presented. In Section 5 current and future challenges of deep learning in robotic \napplications are discussed. Finally, in Section 6, conclusions related to the use of deep learning in robotics are drawn. \nThe reader interested in having a practical guide for the use/application of deep learning in robot vision, and not in the basic \naspects of deep learning, is referred to Sections 3-6. \n \n2    Deep Neural Networks in Computer Vision \nDeep learning is based on the use of artificial neural networks (ANNs) with several hidden layers. The basic aspects of deep \nneural networks (DNNs) and their usage in computer vision applications will be described in this section. In Section 2.1, we will \nreview the historical development of DNNs, and in Section 2.2, we will present some basic definitions needed to understand the \ndifferent DNN models. Then, in Sections 2.3-2.6, we will present various DNN models used in computer vision applications, and \nin Section 2.7 the Deep Reinforcement Learning paradigm. Finally, in Section 2.8, we will present techniques used in order to \nreduce the memory and computational requirements of DNNs. \n \n3 \n2.1. Historical Development \nANNs were first proposed in 1943 as electrical circuits by Warren McCulloch, and a learning algorithm was proposed by \nDonald Hebb in 1949 [185]. Marvin Minsky built the first ANN in hardware in 1951, and Frank Rosenblatt in 1958 built the first \nANN that was able to learn. These ANNs were shallow networks, i.e., typically, they were formed by three or four layers of \nneurons, with only one or two hidden layers. In [186], Poggio and Girosi demonstrated that a regularized learning algorithm was \nequivalent to an interpolation using one Radial Basis Function (RBF) per each datum in the input, which can be reinterpreted as an \nANN. Funahashi [187] was able to show that a three-layer, shallow neural network is able to represent any continuous function \nwith arbitrary precision. This result can be extended to networks with more layers in a straightforward way, by induction. \nHowever, this work was not able to guarantee that a training procedure can achieve the theoretical optimal configuration. \nIn 1980, Fukushima proposed the NeoCognitron [4], a multilayered neural network inspired by the seminal studies of the \nmammalian visual system of Hubel and Wiesel [3]. The NeoCognitron is composed of a sequence of two-dimensional layers of \nneurons, with each neuron being connected to near neurons from the previous layer. Each layer can be formed by “S-cells” \n(extract visual features), “C-cells” (pool local responses), or “V-cells” (average values). Similar 2D neuron layers are grouped into \n3D cell planes. The NeoCognitron has no supervised training algorithm; it has to be trained by specifying the type of expected \nresponse for each layer separately, i.e., the weights for the neurons are not computed in a fully automatic way. \nInspired by the work of Fukushima, LeCun et al. [17][18][19] developed the first Convolutional Neural Network (CNN) in the \nlate 1980s. It included the concept of feature maps produced by layers of neurons with local and heavily restricted connections \n[17] (similar to the receptive fields of biological neurons), and whose weights were adapted using the backpropagation algorithm \n[70]. Although the network was successful in the recognition of handwritten characters, its further development was delayed by \nthe limited computational capabilities available at that time, and by some algorithmic difficulties when training deep networks. In \nparticular, gradient vanishing [1][6] in deep networks was one of the most important issues. Also, the result that a shallow network \ncan approximate any function [187] and the lack of strong theoretical warranties of global convergence [12] when compared to \nalternative methods like SVM [2] were important factors that hindered the acceptance of deep learning methods within the \nmachine learning community. Thus, the use of handcrafted features used together with statistical classifiers (sometimes \nimplemented as shallow networks) continued to be the dominant paradigm in pattern recognition and machine learning. \nDuring the 1990s and the beginning of the 21st Century, deep feedforward networks (e.g. CNN) were largely forsaken by the \nmachine-learning community and ignored by the computer vision community [12] (e.g. a 2002 review on image processing with \nneural networks [61] makes almost no mention of DNNs). However, interest in these networks was reborn in mid-2000 (e.g. \n[71][72][73]). But it was not until the proposal of the AlexNet [20] that the boom of deep neural networks started in the computer \nvision community. AlexNet was able to largely outperform previous feature-based methods in the ILSVRC 2012 challenge \n(AlexNet brought down the error rate by about 40% compared to hand-engineering approaches), a well-known image \nclassification benchmark. The main factors explaining the success of AlexNet are the use of specific technical solutions such as \nReLU activation functions, local response normalization, dropout and stochastic gradient descent, in addition to the use of GPUs \nfor training and testing the network, using large amounts of data (the ImageNet dataset [69] in the case of AlexNet). All these \nfactors have enabled CNNs to be in the core of the most current state-of-the-art applications for computer vision. \nTable 1 shows a summary of relevant work in deep learning for computer vision. All the models and algorithms mentioned will \nbe described in the following sections. \n \nTable 1. Summary of Relevant Work in Deep Learning for Computer Vision \n \n4 \nPaper\nContribution\nModel/Algorithm\nFukushima, 1980 [4] \nThe Neocognitron network is proposed.\nNeocognitron\nLeCun et al., 1998 [19]\nLeNet-5, the first widely known convolutional neural \nnetwork, is proposed. Previous versions of this network \nwere proposed in 1989-1990. [17][18]. \nLeNet-5\nHochreiter & Schmidhuber, 1997 [68]\nLSTM recurrent networks are introduced.\nLSTM (Long Short-Term Memory)\nNair & Hinton, 2010 [5]\nThe paper introduced the ReLU activation functions.\nReLU (Rectified linear unit)\nGlorot, Bordes, & Bengio, 2010 [7]\nThe paper demonstrated that the training of a network is \nmuch faster when ReLU activation functions are used. \nReLU (Rectified linear unit)\nBottou, 2010 [67]\nThe Stochastic Gradient Descent is proposed as a learning \nalgorithm for large-scale networks. \nStochastic Gradient Descent\nHinton et al. 2012 [62]\nDropout, a simple and effective method to prevent \noverfitting, is proposed. \nDropout\nKrizhevsky et al., 2012 [20]\nAlexNet is proposed and, thanks to its outstanding \nperformance in the ILSVRC 2012 benchmark [69], the \nboom of deep learning in the computer vision community \nstarted. AlexNet has 8 layers. \nAlexNet\nSimonyan & Zisserman, 2014 [21]\nThe VGG network is proposed. It has 16/18 layers.\nVGG\nGirshick et al., 2014 [27]\nThe Regions with CNN features network is proposed.\nR-CNN\nSzegedy et al., 2015 [22]\nThe GoogleNet network is proposed. It has 22 layers.\nGoogleNet\nHe et al., 2015 [23][24]\nThe ResNet network is proposed. It is based on the use of \nresiduals and is able to use up to 1001 layers. \nResNet\nBadrinarayanan et al., 2015 [31]\nSegNet, a fully convolutional network for image \nsegmentation applications, is proposed. \nSegNet\n \n5 \nVan de Sande et al., 2016 [235] \nThe DenseNet network is proposed. It includes skip \nconnections between every layer and its previous layers. \nDenseNet\nHu et al [221]\nThe Squeeze-and-excitation network is proposed. It \nintroduces squeeze-and-excitation blocks used for \nrepresenting contextual information. \nSENet\n \n2.2. Basic Definitions \nAn artificial neuron takes on an input vector and outputs a scalar value. The neuron is parameterized by a set of weights. Each \nweight is used as a multiplier for a scalar input. The output of the neuron is the result of applying a nonlinear activation function \non the sum of the weighted inputs. Thus, a neuron with weights w , inputs x , output y , and non-linear activation function φ  is \nrepresented as: \n \n\n\n\n\n\n\n⋅\n=\n∑\n=\nn\ni\ni\ni x\nw\ny\n0\nφ\n \n \nwith n  the size of x , and \n0\nw representing a bias for the neuron input if \n0x  is set to 1. \nNon-linear activation functions \n)\n(x\nφ\n are used for improving the expressive power of the network. They are valid as long as \nthey are continuous, bounded, and monotonically increasing [65]. An additional requirement of the learning algorithms over \n)\n(x\nφ\n \nis differentiability. Thus, the activation functions used most often include the sigmoid logistic function \n)\nexp(\n1\n1\n)\n(\nx\nx\n+\n=\nφ\n, the \nsigmoid hyperbolic tangent function \n)\ntanh(\n)\n(\nx\nx =\nφ\n, the rectified linear unit (ReLU) function \n)\n,0\nmax(\n)\n(\nx\nx =\nφ\n [5], and the \nparametric rectified linear unit (PReLU) \n)\n,0\nmin(\n)\n,0\nmax(\n)\n(\nx\nx\nx\nα\nφ\n+\n=\n [8]. In addition, activation functions that output a vector \nlike softmax \n∑\n=\n=\nm\nk\nk\nj\nj\nx\nx\nx\n1\n)\nexp(\n)\nexp(\n)\n(\nφ\n, whose outputs add up to 1 and enable them to represent multi-class output probabilities [5], are \nselected for neurons used as multi-class classifiers. \nAn artificial neural network (ANN) consists of a set of connected neurons. Typically, neurons are grouped in layers. Each layer \ntakes a set of inputs for computing a set of outputs. The input-output relation is determined by the weights in the layer. A shallow \nnetwork contains a small number of hidden neuron layers, and one output layer. Shallow feedforward networks are able to \nrepresent any function with an arbitrary number of hidden neurons [65]. However, training processes are not guaranteed to achieve \nthe global optimum. DNNs are artificial neural networks containing several hidden layers. They are also able to represent any \nfunction with an arbitrarily high number of hidden neurons. Moreover, under certain conditions, feedforward deep networks are \nable to learn functions using fewer neurons than shallow networks [77][78]. \nA DNN, and in general any ANN, is specified by both hyper-parameters and parameters. Hyper-parameters are related to the \narchitecture of the network (number of layers, number of neurons per layer, activation function, connectivity of the neurons, etc.) \n \n6 \nand to the parameters of the learning process. The parameters of a network correspond to the set of weights of all the neurons. The \nnetwork can be trained for selecting optimal weights (parameters) by using numerical optimization methods. The optimal hyper-\nparameters cannot be learned directly from the data, so several network architectures must be trained and tested for selecting the \nbest hyper-parameters. Thus, in order to be able to adapt the parameters of the network, to select its hyper-parameters, and to \ncharacterize its performance, the available data must be divided into three non-overlapping datasets: training, validation, and test. \nThe training dataset is used for learning the model parameters (weights) of the network. The validation dataset is used for selecting \nthe hyper-parameters by evaluating the performance of the network under different hyper-parameter configurations. The test \ndataset is used for characterizing the trained network by estimating its generalization error. \nDNNs are usually trained using supervised learning. The objective of the training procedure is to find the network parameters \nthat minimize a loss function (e.g. a function of the global error between the obtained and the desired output of the network). Two \nkinds of iterative optimization procedures are commonly used: first order optimization (gradient based), and second order \noptimization (hessian based). When the loss function is convex, second order optimization is commonly used because it converges \nfaster to the optimal solution. However, in non-convex problems, second order algorithms can be attracted to saddle points or local \nminima [79][80]. Then, gradient-based methods are normally used for training deep neural networks. As the datasets used for \ntraining deep neural networks are generally large, only a small subset of the data (a mini-batch) is used at each optimization \niteration for training the network. This procedure is called Stochastic Gradient Descent (SGD) [67]. The term stochastic is used \nbecause each mini-batch gives a noisy estimation of the gradient over all the data, which allows finding a good set of weights very \nquickly [12]. “The stochastic gradient descent directly optimizes the expected risk, since the examples are randomly drawn from \nthe ground truth distribution.” [67]. The use of mini-batches improves the generalization ability of the network and enables fast \ntraining. Additional mechanisms are the decrease of the learning rate over time, and the use of momentum [66]. The generation of \nspecialized second order algorithms for deep learning is a current research topic [79]. \nIn several cases, minimizing the original loss function can cause misbehavior of the optimization process. For example, in some \ncases the learned weights can be too large because of overfitting. When the training procedure generates ill-posed networks, \nregularization can be used. Regularization corresponds to a set of techniques used to reduce overfitting by improving robustness in \nthe parameter estimation process [80]. Some of these techniques can be implemented by adding a regularization term to the loss \nfunction. This extra term can be used for limiting the size of the weights, and also for improving invariance against small changes \nin the inputs. As an example, L2 regularization [80] consists of adding a penalization term to the original loss function: \n2\n*\n)\n,\n(\n)\n,\n(\nw\nw\nx\nL\nw\nx\nL\noriginal\nregul\nα\n+\n=\n. \nDropout [62][63] is a technique that consists of using only a random subset of the neurons in each layer (along with its \nconnections) during training episodes in order to reduce overfitting. This procedure is able to improve the generalization error of \nthe networks, since its effect is similar to that of using an ensemble of networks. Using dropout can be interpreted as an alternative \nform of regularization. Once the network has been trained, weights are decreased proportionally in order to account for the smaller \nnumber of neurons used when training. Also, dropout can be used for generating multiple possible outputs for the same input [32] \nwhen activating different units. In this way, a confidence level for the output can be computed. An alternative to Dropout is \nDropConnect [64], which, instead of dropping the activations of some neurons, drops their weights. In this way each unit receives \ninput from a random subset of units on the previous layer. \nBatch normalization consists of normalizing the input layers at each mini-batch iteration [177]. This enables the training \nprocedure to use higher learning rates, and to be more robust with respect to initial weights in the network. Batch normalization \n \n7 \nalso acts as a regularizer, eliminating the need for dropout. State-of-the-art ResNet networks, described in  Section 2.3, do not use \ndropout; they use batch normalization [23]. \n2.3 Convolutional Neural Networks \nConvolutional Neural Networks (CNNs) correspond to a family of DNN architectures whose main goal is analyzing natural \nsignals by taking advantage of their properties. According to [12], the main properties of CNNs are local connections, shared \nweights, pooling, and the use of many layers. The main reason for this is that the structure of CNNs is inspired by the processing \npipeline of the mammalian visual system [3], where (i) neurons have local connectivity, (ii) several layers of neurons exist \nbetween the fovea and the cortical visual areas, (iii) neurons on a given layer perform the same kind of processing (a non-linear \nactivation function is applied over its weighted inputs), and (iv) the size of the receptive fields of the neurons (i.e. the region of the \nsensory space whose signals affect the operation of a given neuron) increases from one layer to the next one. Hence, the behavior \nof a given layer of CNN neurons can be modeled as a linear filtering operation, i.e. a convolution, followed by a non-linear \noperation (activation function). After the convolution layer, the application of a pooling operation permits modeling the increase \nof the receptive size of the neurons. After several convolutions and pooling operations are applied (the network has several layers), \na decision needs to be made about the input stimuli. This is implemented using fully connected layers of neurons. Thus, CNNs are \ncomposed of three kinds of processing layers: convolutional layers, pooling layers, and fully-connected layers, as is shown in \nFigure 1.  \nThe number of layers in a CNN and their type will depend on the specific network structure, but normally the number of \nconvolution and pooling layers is much larger than the number of fully-connected layers. A very important side effect of this is \nthat a CNN can have a smaller number of weights to be adapted than a shallow network, given that the number of free parameters \nin the convolutional layers is very small, and proportional to the size of the implemented filters. For instance, in a CNN whose \nfirst convolutional layer has 224x224x3 input neurons (i.e., 3 channels), a filter size of 11x11, and C output channels, the number \nof free parameters, i.e. the weights, to be adapted is 11x11x3xC, while in the case of a fully-connected network, the number of \nweights would be 224*224*3*C. Then, the number of free parameters increases linearly if the input has several channels (i.e., a \nRGB input image) and/or if several output channels must be computed. One important aspect to highlight is that most of the \noperations that need to be computed in a CNN are local, and therefore can be mapped easily to GPUs. \nThe designing of a convolutional layer basically consists of selecting the number of filters that a given layer will have, and its \nsize. The parameters of the filters, i.e. the weights of the neurons, will be adapted during training. In the case of the design of a \npooling layer, the pooling operation needs to be selected. The most frequently used pooling operations are average pooling and \nmax pooling; however, several other alternatives have been studied and used [9] (e.g. stochastic pooling, spatial pyramid pooling, \ndef-pooling). \nThe fully-connected layers are the ones in charge of converting the 2D feature maps into a 1D feature vector, in addition to \nbeing in charge of the classification. The fully-connected layers have a similar structure to a traditional shallow network, and they \nnormally contain a large percentage of the parameters of a CNN (about 90% according to [9]). In some cases the fully-connected \nlayers in charge of the final classification are replaced by a softmax unit, or a different statistical classifier (e.g. an SVM [127]). \nSoftmax has been shown to achieve better accuracy than SVM [28], because it can be trained end-to-end inside the network.  \n \n \n8 \n \n \n(a) \n \n(b) \n \n \n(c) \nFigure 1. (a) CNN Architecture. (b) The Operation of a Convolutional layer. (c) The Operation of a Pooling/Resampling layer.  \nAdapted from [9]. \n \nDifferent CNN architectures implement different network topologies (e.g. number of convolutional layers, filter size in each \nlayer, number of fully connected layers), and operations (e.g. activation function, pooling operation, dropout). AlexNet, one of the \nmost popular CNNs and the one responsible for the DNN boom thanks to its outstanding performance in ILSVRC2012, has 5 \nconvolutional layers and 3 fully connected layers. The convolutional layers 1, 2, and 5 include max pooling, and the convolutional \nlayers 1 and 2 include local response normalization [20]. In addition, they use ReLU activation functions, dropout, and stochastic \ngradient descent for training. AlexNet has a fixed input size (224x224x3) and is well suited for image classification tasks. The \nnetwork generates a probability vector of size 1000 from the whole input image, each component representing the probability of \npresence for one object category. \nSome CNN architectures keep the same working strategy as AlexNet, but they go deeper. For instance, VGG [21], which got \nthe second place at ILSVRC 2014, has 16 (18) layers, 13 (15) convolutional and 3 fully-connected, but it reduces the filter size to \n3x3. GoogleNet [22], which won first place at ILSVRC 2014, has 22 layers, and introduced a more complex architecture based on \nthe use of several parallel sub-layers inside each layer. However, in practice, the use of 30 or more layers generates an increase in \nthe error. This problem is solved by ResNet [23] by using residual layers that work by adding the output of a previous layer to the \ncurrent one. Then, the network processes residuals instead of the original data layers. ResNet is able to use up to 1001 layers [24], \nconsistently decreasing the error when adding more layers. While ResNet uses only skip connections between pairs of layers, \nDenseNet [235] uses skip connections between every layer and its previous layers. DenseNet has been shown to perform better \nthan ResNet, while requiring fewer parameters to learn. Finally squeeze-and-excitation networks, (SENet) ,[221] were introduced \nInput\nFilter Mask \nOutput \nPooling \noperation\n \n9 \nin 2017. As each layer has a reduced local receptive field, channel descriptors are computed for exploiting contextual information \noutside of this region. SENet was the winner of the ILSVRC 2017 classification challenge, and is currently the state-of-the-art for \nimage classification. \nOn the other hand, some approaches focus on using multiple CNNs that work independently, and whose results are combined. \nOne of the most interesting approaches is the Multi-Column DNN (MCDNN) [93], which uses several DNNs in parallel and   \naverages their predictions. MCDNN-based approaches obtain state-of-the-art results on some image classification benchmarks (see \nTable 2). \n \nTable 2. Performance of the Best Performing DNNs in some Selected Computer Vision Benchmarks. The absolute ranking corresponds to the \nranking of the network in the benchmark, while the relative one corresponds to the ranking when considering only DNN models. \n \nTask\nBenchmark\nPaper\nRelative \n(Absolute) \nRanking \nDNN Model\nImage Classification\nMNIST\nWan et al., 2013 [173]\n1(1)\nMCDNN+ \nDropConnect \nImage Classification\nCIFAR-10\nGraham, 2015 [174]\n1(1)\nDeepCNet (Adapted \nMCDNN) + Fractional \nMax Pooling\nImage Classification\nCIFAR-100\nClevert et al., 2016 [175]\n1(1)\nELU-Network\nImage Classification\nILSVRC 2017\nHu et al [221]\n1(1)\nSqueeze-and-\nExcitation networks \nObject Detection and \nRecognition \nPascal VOC2012  \ncomp 4 (20 classes) \n \nDai et al, 2016 [218] \n2(2)\nRegion-based fully \nconvolutional network \nObject Detection and \nRecognition \nKITTI (October 11, 2017) \n(Car/Pedestrian/Cycle) \nRen et al., 2017 [219]\n7/3/32(7/3/3)\nRecurrent Rolling \nConvolution \nObject Detection and \nRecognition \nCOCO 2016 \n(91 Object Types) \nHuang et al., 2017 [220]\n 1(1)\nFaster R-CNN + \nResNet \nObject Detection and \nRecognition \nILSVRC 2017 \nTask1a \nShuai et al, 2017 \n[23][222][223][224][225][226][227] \n1(1)\nResidual Attention \nNetwork + Feature \nPyramid Networks \nSemantic \nSegmentation \nCityscapes\nDeepLabv3 2017 [208]\n12(1)\nDeepLab v3\nSemantic \nSegmentation \nPascal VOC2012 comp6 \nDeepLabv3-JFT 2017 [208]\n1(1)\nDeepLab v3\n                                                 \n2 The methods obtaining the best performance in this benchmark are not described. It is unknown whether they are based on deep learning or not. \n \n10 \nAction Recognition\nMPII\nChen et al. 2017 [228]\n1(1)\nAdversarial PoseNet\nAction Recognition\nPascal VOC2012 comp10\nGkioxari et al., 2016 [168]\n2(2)\nR*CNN\n \nAs mentioned, standard CNN architectures generate outputs for the whole input image. However, in object \ndetection/recognition tasks, where the object of interest covers a small area in the input image, the CNN will not be able to detect \nthe object. In that case, sliding windows can be used for defining the areas of the image to be analyzed. However, this approach, \nwhich requires analyzing all possible sliding windows using a CNN, is unable to run in near real-time because of the large number \nof windows to be processed. Hence, instead of using a sliding window before applying CNNs, region proposals can be computed. \nA region proposal is a window inside the input image that is likely to contain some object. Region proposal algorithms are class-\nagnostic object detectors that can be computed very quickly. \nThe R-CNN algorithm [27] is based on using region proposals that are fed into a CNN. The proposals are computed by using \nthe Selective Search algorithm [26]. The R-CNN algorithm is able to run faster than a naïve CNN using sliding windows, but the \nCNN features need to be recomputed for each object proposal instance. Fast R-CNN [28] also employs object proposals from \nSelective Search, but the features to be used for the final detection are computed just once, using shared convolutional layers for \neach input image. Each proposal defines a region of interest in the last feature map, and the selected region is used for carrying out \nthe classification, using fully connected layers. Faster R-CNN [29] implements a Region Proposal Network (RPN) that is \ncomputed by applying a per-pixel fully connected network over the last shared convolutional layer (see Figure 2). The RPN \nnetwork is able to generate region proposals, which are then used by a Fast R-CNN for the detection of the objects. The Faster R-\nCNN has also been used with the ResNet architecture [23][24], and it is the state-of-the-art for general-purpose object detection \n(see Table 2). \n                             \n \nFigure 2: Left: Faster R-CNN is based on computing a set of shared convolutional layers that are used for computing region proposals and then \nfor clasifying them. Right: Region proposal network (RPN). Figures taken from [29] with permission of the authors. \n2.4    Fully Convolutional Networks \nFully convolutional networks (FCNs) are a special kind of CNN that includes only convolutional layers (no fully connected \nunits). The output of an FCN is a feature map from the last convolutional layer. Given that FCN inputs and outputs are two-\n \n11 \ndimensional, they are trained end-to-end, pixel-to-pixel, and can be used for dimensionality reduction, regression, or semantic \nsegmentation. \nAuto-encoders [81] are artificial neural networks trained to predict their own inputs. FCN based auto-encoders are formed by \ntwo smaller FCNs: an encoder and a decoder. The encoder is an FCN that decreases the size of the input image through its layers, \ngenerating a compressed representation of the image. The decoder is an FCN that maps the compressed representation onto a final \nimage. When the auto-encoder is trained, its encoder is able to compress images, while its decoder can reconstruct them. \nDenoising auto-encoders [82] are FCN regression models that are able to denoise images. They are trained with noisy images as \ninput, and their correspondent clean images as output. Also, FCNs can be used for performing semantic segmentation \n[30][31][32], sometimes in conjunction with Conditional Random Fields (CRFs) [83]. Semantic segmentation FCNs are trained to \nmap RGB images onto images containing a pixel-wise semantic segmentation of the environment. \nThe first successful use of FCNs for semantic segmentation was implemented in the work of Long, Shelhamer and Darrell [30]. \nThey used a set of convolutional layers with decreasing size whose output corresponds to the segmented image with a lower \nresolution than the original one. They used dropout during the training stage for regularization. SegNet [31] is also based on an \nencoder and a decoder. The encoder corresponds to a set of convolutional layers with decreasing spatial size. The decoder takes \nthe output data of the encoder as input, and applies a set of convolutional layers that scale up the data to become the same size as \nthe original input image. By using the decoder, the segmentation obtained by SegNet has higher resolution than that of Long et al. \n[30]. The SegNet architecture is shown in Figure 3. Bayesian SegNet [32] uses dropout both for training and for normal execution. \nSince dropout selects random subsets of the neurons when using the network, various network outputs can be obtained by applying \ndropout several times. Then, a set of semantic, segmented images is available. The images can be analyzed for computing the \nuncertainty associated with the labels of each pixel. Then, if a pixel has the same label in all of the images, the uncertainty is low, \nbut if the pixel has different labels in the images, their uncertainty is high. \nFCNs are based on simple pixel-wise CNNs, without any explicit modeling of the image structure. Thus, current research on \nsemantic segmentation is focused on modifying the baseline FCNs for using contextual information. Dilation10 [83] and \nDeepLabv2-CRF [85] use dilated/up-sampled filters for covering a greater receptive field without using more parameters. LLR-4x \n[84] uses a multi-scale representation similar to a Laplace pyramid for collecting semantically and spatially resolved data.  \nDeepLabv2-CRF [85] and Adelaide-Context [86] also integrate CRFs into the FCN process. \n \nFigure 3: SegNet, an Example of fully Convolutional Architecture. Figure taken from [31] with permission of the authors. \n \n12 \n2.5   Long Short-Term Memory \nRecurrent Neural Networks (RNNs) are networks with at least one feedback connection that allows them to have internal \nmemory. RNNs are useful for learning information from temporal or spatial sequences with variable lengths, which cannot be \nhandled properly by traditional CNNs. A common approach is to self-feed a neural network with its output in the last time frame. \nThen, back propagation through time (BPTT) [150] can be used for learning the network parameters. However, the use of BPTT \nsuffers from vanishing and exploding gradients [1] because the past network inputs tend to vanish exponentially, or to cause \nnetwork divergence over time. \n A successful recurrent neural network architecture is the Long Short-Term Memory (LSTM) [36][37], which is based on using \nmemory cells. Each memory cell is able to learn simple input-output relations among sequences, even when the relation involves \nlong delays. At each time frame, the memory cell has an input \ntx , an output \nth , and a hidden state \ntc . The input \ntx  can modify \nthe hidden state \ntc  (writing) or be ignored. The output \nth  can be related to the hidden state \ntc  (reading) or be zero. Also, the \nvalue of \ntc  can be preserved in the next frame or set to zero (forget). The amount of reading level \nto , writing level ti , and \nforgetting level \ntf  at each iteration are computed by using three independent sigmoidal neural layers inside each memory cell. \nThen, when training the network, the cell learns to store, to output, or to forget values depending on contextual information from \nthe sequences. The architecture of a memory cell does not suffer from exponential vanishing or explosion of the input data. Also, \nwhen training the cells by using BPTT, the network state \ntc  does not require further back propagation. \nLSTMs are useful for solving sequence-related problems, such as speech recognition, handwriting recognition, and polyphonic \nmusic modeling [128]. By stacking CNN and LSTM based models, systems have been developed that are able to anticipate driver \nactions [110], generate captions from images [94], answer questions that require image inspection [95], or even align video and \ntext descriptions [96]. \n2.6   Restricted Boltzmann Machines \nRestricted Boltzmann Machines (RBMs) are stochastic neural networks composed of stochastic neuron-like binary units. RBMs \nconsist of one layer of visible units, one layer of hidden units, and an extra bias unit. Each visible unit is connected to all hidden \nunits and vice versa. The bias unit is also connected to all hidden and visible units. All of the connections are undirected. RBMs \nare able to learn the data distribution of the input data (visible units) and to generate samples from that distribution (generative \nlearning) in an unsupervised fashion. The network is able to approximate any distribution as the number of hidden units increases. \nGiven an initial visible input, the network is able to compute binary activations for the hidden units by using binary sampling. \nAlso, given the values of the hidden units, the network can compute activation values for the visible units. By repeating these two \nprocesses, the underlying distribution represented by the RBM can be sampled. \nThe parameters of the network are obtained by minimizing an energy function that depends on the training input vectors (no \nsupervision is required). In practice, parameters are adapted for diminishing the divergence between the distribution sampled from \nthe RBM and the ground truth distribution. Also, several layers of hidden units can be stacked for creating a Deep Boltzmann \nMachine (DBM), but its training procedure requires high computational resources. Another form of stacking RBMs is by using \nDeep Belief Networks (DBNs) that include directed connections between layers, enabling the use of greedy layer-wise training \nalgorithms. \n \n13 \nA more recent approach for training unsupervised stochastic networks is the Deep Energy Model (DEM) [151]. This \narchitecture is composed of a set of deterministic feed forward layers and a final stochastic layer. This system can also be \ndescribed as a deep feed-forward network followed by an RBM, trained jointly. Then, the feed-forward network learns a \nrepresentation that is easily modeled by the final RBM. The system can also be interpreted as a normal RBM with a modified \nenergy function that is dependent on the parameters of the feed-forward network. The training of DEMs is faster than the training \nof DBMs and DBNs, because only one stochastic layer is used. \n \nDBMs/DBNs/DEMs can be used as a pre-training step for initializing the weights of feed-forward networks. However, \nalthough state-of-the-art feed-forward implementations still do not use RBM-derived methods for pre-training [23][80], this \ntendency could change in the future.  \n2.7   Deep Reinforcement Learning \nReinforcement Learning consists of a set of techniques designed to maximize the cumulative reward obtained by an agent when \ninteracting with its environment. The agent has an internal state, and is able to observe its environment partially, to act on it, and to \nobtain an immediate reward. This problem can be modeled by using a Markov Decision Process (MDP), which is defined by: (i) \nS , a set of possible states, (ii) \n)\n(s\nA\n, a set of actions possible in state S , (iii) \n)\n,'\n,\n(\na\ns\ns\nP\n, the probability of transition from s  to \n's  given an action, a , (iv) \n)\n,'\n,\n(\na\ns\ns\nR\n, the expected reward on transition from s  to 's  given an action, a , and (v) the discount \nrate, γ , for delayed reward in discrete time which trades off the importance of immediate and future rewards. Also, a policy, π , \nindicates the actions that the agent takes, i.e., it maps each state to an action. The state-action value function, \n)\n,\n(\na\ns\nQπ\n, of each \nstate-action pair is the expected sum of the discounted rewards for an agent starting at state s , taking action a  and then following \na policy, π , thereafter. The objective of MDP is to find the optimal policy \n*\nπ  that maximizes the value function. The optimal \nvalue function satisfies the Bellman Equation, which has a recursive nature and relates value functions from consecutive time \nframes. If the optimal state-value function \n)\n,\n(\n*\na\ns\nQπ\n is known, the best next action can be computed trivially without needing to \nstore the policy \n*\nπ explicitly. When transition probabilities \n)\n,'\n,\n(\na\ns\ns\nP\n and reward function \n)\n,'\n,\n(\na\ns\ns\nR\nare known, the optimal \npolicy can be computed by using dynamic programming. However, when both are not known a priori, and can only be estimated \nby interacting with an environment, other approaches must be used. Classical reinforcement learning, in particular Q-learning, \nuses a discrete state space (usually by partitioning a continuous one) and learns the value function Q  by performing several \nexperimental trials, and exploiting the recursivity property for learning from successful ones. Also, approaches that use a linear \nparameterized function approximation, \nθ\nφ\nθ\nT\na\ns\na\ns\nQ\n)\n,\n(\n)\n;\n,\n(\n=\n, with a number of parameters smaller than the number of states, \nhas been used [194]. Deep reinforcement learning (DRL) is able to learn the optimal mapping, \n*\nQ , directly from states \n(represented as sets of consecutive raw observations) and actions related to the different outcomes for each action, by using a \nconvolutional neural network, called a Deep Q Network (DQN). This technique has been applied successfully for agents to learn \nby playing several Atari games with only raw gray scale images and the scores in the game [190], achieving human-level playing \nability. As neural networks learn from new data, a technique named experience replay is used. It consists of selecting random \nsamples from both old and new trials for training the network. Then, the learning process does not get biased by the last trials, and \ncontext information from old trials can be preserved and used [190]. Policy gradient methods and actor/critic algorithms have been \n \n14 \nproposed recently [195] for enabling end-to-end training and faster convergence and evaluation times with respect to DQNs. The \nuse of supervised learning for policy networks as initialization for reinforcement learning has proven to be useful, and has enabled \ncomputers to defeat human professional players in the game of Go [196]. Deep reinforcement learning is a very hot research topic \nas it has the potential to solve real-world problems with a human-level performance, using end-to-end formulations.  \n2.8   Deep Compression and Quantization \nState-of-the-art vision systems based on CNNs require large memory and computational resources, such as those provided by \nhigh-end GPUs. For this reason CNN-based methods are unable to run on devices with low resources, such as smartphones or \nmobile robots, limiting their use in real-world applications. Thus, the development of mechanisms that allow CNNs to work using \nless memory and fewer computational resources, such as compression and quantization of DNNs, is an important area to be \naddressed. \nMathieu et al. [183] propose computing convolutions by using FFTs. This technique enables reusing transformed filters with \nmultiple images. FFT is efficient when computing responses for shared convolutional layers. Also, inverse FFT is required only \nonce for each output channel. However, the use of FFTs requires additional memory for storing the layers in the frequency \ndomain. Libraries such as CuDNN [153] are able to use FFTs when training and testing the networks. FFT is more efficient than \nnormal convolutions when filter masks are large. However, the state-of-the-art ResNet [23] uses small 3x3 masks. \nThe use of sparse representations is a useful approach for network compression. In [87], kernels are factorized into a set of \ncolumn and row filters, by using an approach based on separable convolutions. A 2.5x speedup is obtained with no loss of \naccuracy, and a 4.5x speedup with a loss in accuracy of only 1%. A procedure for reducing the redundancy in a CNN by using \nsparse decomposition is proposed in [88]. Both the images and the kernels are transformed onto a different space by using matrix \nmultiplications, where convolutions can be computed by using sparse matrix multiplication. This procedure is able to make more \nthan 90% of the parameters zero in an AlexNet [20], with a drop in accuracy that is less than 1%. \nIn [89], a methodology for compressing CNNs is proposed, which is based on a three-stage pipeline: pruning, trained \nquantization, and Huffman coding. In the first stage, the original network is pruned by learning only the important connections, \nand a compressed sparse row/column format is used for storing the non-zero components. In the second stage, weights are \nquantized by using a 256-word dictionary for convolutional layers, and a 32-word dictionary for fully connected layers, enabling \nweight sharing. In the third stage, the binary representation of the network is compressed by using Huffman coding. By using the \nproposed approach, AlexNet can be compressed from 240MB to 6.9MB, achieving a 35x compression rate, and VGG can be \ncompressed from 552MB to 11.3MB, achieving a 49x compression rate, without loss of accuracy. In [90], an EIE (Efficient \nInference Engine) is proposed for processing images while using the compressed network representation. EIE is 189x faster, and \n13x faster when compared to CPU and GPU implementations of AlexNet without compression, without loss of accuracy.  \nTwo binary-based network architectures are proposed in [91]: Binary-Weight-Networks and XNOR-Networks. In Binary-\nWeight-Networks, the filters are approximated with binary values in closed form, resulting in a 32x memory saving. In XNOR-\nNetworks, both the filters and the input to convolutional layers are binary, but non-binary non-linearities like ReLU can still be \nused. This results in 58x faster convolutional operations on a CPU, by using XNOR-Bitcounting operations. The classification \naccuracy with a Binary-Weight-Network version of AlexNet is only 2.9% less than the full-precision AlexNet (in top-1 \nmeasure),;while XNOR-Networks have a larger, 12.4%, drop in accuracy. \n \n15 \nThe compression and quantization of DNNs is an area under development. It is expected that easier ways for reducing the size \nof DNNs will be developed in the near future, and that deep learning frameworks and tools will make them available for use by \ndevelopers of DNN based applications. \n3 Designing Deep-learning based Vision Applications  \nFor designers of a deep-learning based vision system, it is important to know which learning frameworks and databases are \navailable to be used in the design process, which DNN models are best suited for their application, whether an already trained \nDNN can be adapted to the new application, and how to approach the learning process. All these issues are addressed in this \nsection. These guidelines can be used in the design process of any kind of vision system, including robot vision systems. \n \n3.1 Developing Tools: Learning Frameworks and Databases \n \nGiven the complexity in the design and training of DNNs, special learning frameworks/tools are used for these tasks. There is a \nwide variety of frameworks available for training and using deep neural networks (see a list in [164]), as well as several public \ndatabases suited to various applications and used for training (see a list in [165]). Table 3 presents some of these frameworks, \nshowing the DNN models included in each case. Some of the most popular learning frameworks are described in the following \nparagraphs. These frameworks were selected by considering the availability of pre-trained models, training/testing speed, and \nspecific advantages such as the inclusion of RBMs in Torch, or multi GPU optimization in CNTK.  \nCaffe [60] is a deep learning framework written in C++, able to use CUDA [152] and CUDNN [153] for GPU computing. It \ncan also be used on systems without GPUs. Caffe is designed to be used in computer vision applications. The framework can be \nused through Python and MATLAB, and it has a model zoo [178], which is a collection of several state-of-the-art pre-trained \nmodels for several computer vision tasks. Pre-trained models from the Caffe Zoo can be downloaded and used in a straightforward \nway. The model zoo includes AlexNet, CaffeNet (an AlexNet variant used across tutorials), R-CNN, GoogleNet, VGG, and \nSegNet, among other pre-trained models. Caffe has several dependencies, and its installation process is not straightforward. \nTorch [130] is a scientific computing framework with wide support for machine learning. It is written in C, and able to use \nCUDA [152] and CUDNN [153]. It can be accessed by using a scripting language called LuaJIT. As it is a general machine \nlearning framework, it is able to train LSTMs and energy-based models such as DBMs. It is also able to load some models from \nthe Caffe Model Zoo. It is embeddable, with ports to iOS, Android, and FPGA backends. \nTheano [131][132] is a deep learning framework written in Python, that is able to use CUDA [152] and CUDNN [153]. It can \nrun as fast as C frameworks with large datasets and GPUs. It enables the use of symbolic differentiation, which speeds \ndevelopment. It can also generate customized C code for many mathematical operations and its installation procedure is \nstraightforward. There are other frameworks built on Theano, such as Pylearn2 [133], Theano Lights [134], Blocks [135] and \nLasagne [136]. Pylearn includes algorithms such as CNNs, LSTMs, and auto-encoders. Theano is able to read Caffe models by \nusing Lasagne. \nTensorflow [137] is an open source library for numerical computation using data flow graphs. Nodes represent operations, \nwhile edges represent multidimensional data arrays (tensors). Data flow graphs can be shown using a graphical interface that \nenables monitoring the system in real-time. It is able to make computations using several CPUs and GPUs, by using CUDA [152] \n \n16 \nand CUDNN [153]. The graphs can be implemented in Python and C++. Also, Tensorflow is able to generate stand-alone code for \nPython and C++. Tensorflow is able to load models from Caffe Zoo by installing an extension. \nCNTK [141] is the Computational Network Toolkit by Microsoft Research. It describes deep networks as a series of \ncomputational steps via directed graphs. Leaf nodes represent inputs, and other nodes represent matrix operations. It enables easy \nuse of feed-forward DNNs, CNNs, and LSTMs. CNTK internal implementation is based on C++. It is able to make computations \nusing several CPUs and GPUs, by using CUDA [152] and CUDNN [153]. CNTK is able to outperform other frameworks in \ntraining speed when multiple GPUs are used. \nFor the designer, it is very important to select the framework best suited to the needs and resources of the final application. In \n[170] five popular frameworks are compared (Caffe, Neon, Tensorflow, Theano, and Torch). The main conclusions are: (i) Caffe \nis the easiest to use tool when using standard DNNs, (ii) Theano and Torch are the most extensible tools, (iii) Theano/Torch is \nfastest for smaller/larger networks for GPU-based training of CNN and fully connected networks; in the case of deployment of \nCNN and fully connected networks, Torch obtains the best performance followed by Theano, (iv) Theano results in the best \nperformance for GPU-based training and deployment of recurrent networks, and (v) Torch performs the best followed by Theano \nfor CPU-based training and deployment of any DNN.  \n \nTable 3. Tools for Designing and Developing deep learning based Vision Applications. \nTool\nDescription \nIncluded DNN models \nCaffe [60]\nDeep learning framework, written in C++ and \nable to use CUDA/CuDNN for multi-GPU. \nSupport for Python and MATLAB. \nCaffe Model Zoo 3  includes pre-trained reference models of \nCaffeNet, AlexNet, R-CNN, GoogLeNet, NiN, VGG, Places-CNN, \nFCN, ParseNet, SegNet, among others. \nTorch [130]\nScientific computing framework, written in C \nand able to use CUDA/CuDNN for multi-\nGPU. Support for LuaJIT. \nTorch Model Zoo includes pre-trained models of OverFeat, \nDeepCompare, and models loaded from Caffe into Torch. \nTheano [131][132]\nPython library for large-scale computing, able \nto \nuse \nCUDA/CuDNN. \nIt \nhas \nstarted \nexperimental multi-GPU support. \nAuto-encoders, RBMs (through Pylearn2), CNN, LSTM (through \nTheano Lights), models from Caffe Zoo (through Lasagne) \nTensorFlow [137]\nFramework for computation using data flow \ngraphs, written in C++ with Python APIs. \nAble to use CUDA/CuDNN for multi-\nGPU/multi-machine. \nIt has examples of small CNNs and RNN with LSTM in its tutorial. \nCan load models from Caffe Zoo. \nMXNet [138]\nDeep learning framework. It is portable, \nallows multi-GPU/multi-machine use, and has \nbindings for Python, R, C++ and Julia. \nIncludes three pre-trained models: Inception-BN Network, \nInception-V3 Network, and Full ImageNet Network. \n                                                 \n3 Model Zoo refers to a repository of pre-trained models. \n \n17 \nDeeplearning4j [139]\nDeep learning library written in Java and \nScala. Has multi-GPU/multi-machine support. \nExamples of RBM, DBM, LSTM. Its Model Zoo includes AlexNet, \nLeNet, VGGNetA, VGGNetD. \nChainer [140]\nDeep Learning Framework written in Python. \nImplements CuPy for multi-GPU support. \nAlexNet, GoogLeNet, NiN, MLP. Can import some pre-trained \nmodels from the Caffe Zoo. \nCNTK [141]\nComputational \nNetwork \nToolkit. \nAllows \nefficient multi-GPU/multi-machine use. \nIt has no pre-trained models.\nOverFeat [142]\nFeature extractor and classifier based on \nCNNs. No support for GPUs. \nOverFeat Network (pre-trained on ImageNet)\nSINGA [143][144]\nDistributed deep learning platform written in \nC++, Has support for multi-GPU/multi-\nmachine. \nIt has no pre-trained models.\nConvNetJS [145]\nJavascript library for training deep learning \nmodels entirely in a web browser. No support \nfor GPUs. \nBrowser demos of CNNs\nCuda-convnet2 [146]\nA \nfast \nC++/CUDA \nimplementation \nof \nconvolutional neural networks. Has support \nfor multi-GPU. \nIt has no pre-trained models.\nMatConvNet [147]\nMATLAB toolbox implementing CNNs. Able \nto use CUDA/CuDNN on multi-GPU. \nPre-trained models of VGG-Face, FCNs, ResNet, GoogLeNet, \nVGG-VD [21], VGG-S,M,F [74] CaffeNet, AlexNet. \nNeon [148]\nPython based Deep Learning framework. Able \nto use CUDA for multi-GPU. \nPre-trained models of VGG, Reinforcement learning, ResNet, \nImage Captioning, Sentiment analysis, and more. \nVeles [149]\nDistributed \nplatform, \nable \nto \nuse \nCUDA/CuDNN on multi-GPU/multi-machine. \nWritten in Python. \nAlexNet, FCNs, CNNs, auto-encoders.\n \n \nThe availability of training data is also a crucial issue. There are several datasets available for popular computer vision tasks \nsuch as image classification, object detection and recognition, semantic segmentation, and action recognition (see Table 4). \nHowever, in most of the cases  there is no specific database for the particular problem being solved, and pre-training and fine-\ntuning can be applied (see Section 3.2). \n \nTable 4. Training Databases used in some selected Computer Vision Applications. \nApplication\nSelected databases\n \n18 \nImage classification \nMNIST (70,000 images, handwritten digits) [171], CIFAR-10 (60,000 tiny images) [172], CIFAR-\n100 (60,000 tiny images) [172], ImageNet [163] (14M+ images), SUN (scene classification, 130,519 \nimages) [162] \nObject detection and \nrecognition \nPascal VOC 2012 (11,540 train/val images) [154], KITTI (7,481 train images, car/pedestrian/cyclist) \n[157], MS COCO (300,000+ images) [158], LabelMe (2,920 train images) [159] \nSemantic segmentation\nCityscapes (5,000 fine + 20,000 coarse images) [160], Pascal VOC 2012 (2,913 images) [154], \nNYU2 (RGBD, 1,449 images) [155] \nAction recognition\nMPII (25,000+ images) [156], Pascal VOC 2012 (5,888 images) [154]\n \n \n3.2 Pre-training and Fine-tuning \n \nWhen developing a new application, it is very common to use a pre-trained DNN instead of training a network from scratch. \nThe main reason for this is that a large dataset is needed for training a DNN, and usually such a dataset is not available. Pre-\ntraining basically consists of using the features of a DNN trained by using a large dataset and applying it to another, usually \nsmaller dataset, after a fine-tuning process. For instance, consider a DNN that has been trained for recognizing a given set of \nobjects using a large dataset containing those objects, and that has to be used for recognizing a different set of objects. The dataset \nfor this second set of objects is smaller. Considering that the two datasets are different, the DNN needs to be fine-tuned. In the \ncase of using a CNN, this problem is normally addressed using a pre-trained CNN, whose last layers (the ones that make the final \nclassification) are replaced and whose first layers are re-used. The rationale behind the procedure is that the same features can be \nused for solving the two different problems, and that only the classifiers need to be trained again. The training of the new network, \nincluding both reused and new layers, is called fine-tuning. The new layers must always be trained by using the second set of \nobjects. Also, the reused layers can be trained further with a smaller learning rate depending on data availability. \nAn important issue that needs to be analyzed carefully when features of one network are used in a second one is the \ntransferability gap, which grows when the distance between the tasks/problems to be addressed increases [76]. Useful guidelines \nfor the process of using features of one network in a second one can be found in [76]. \nIn the case of robotics applications, pre-training and fine-tuning are relevant since it is difficult to collect large databases, \nrequired to train networks, from scratch. Also, the pre-trained networks can be run on the real robotic platforms for estimating the \nruntime of the corresponding fine-tuned networks. The runtime measured on the robotic platform considers all the restrictions,  \nsuch as available memory, presence of GPU, CPU power, etc. That measurement can be useful for selecting candidate network \narchitectures without needing to train all the options beforehand. \n \n3.3. Selection of the DNN Model \n \nThe selection of the network model to be used must consider a trade-off between classification performance, computational \nrequirements, and processing speed. \n \n19 \nWhen selecting a DNN model, there are two main options: using a pre-existing model, or using a novel one. But, the use of a \nfully novel DNN model is not recommended because of the difficulties in predicting its behavior, unless the purpose of the \ndeveloper is to propose a new optimized neural model. When using a pre-existing model there are three options: (i) using a pre-\ntrained neural model for solving the task directly, (ii) fine-tuning the parameters of a pre-trained model for adapting it to the new \ntask, or (iii) training the model from scratch. In cases (ii) and (iii), the dimensionality of the network output can be different from \nthe dimensionality required by the task. In that case, the last fully-connected layers of the network in charge of the classification \ncan be replaced by new ones, or by statistical classifiers such as SVMs or random forests.  \nTable 5 shows popular DNNs, the number of layers and parameters used in each case, the main applications in which the DNNs \nhave been used, and the sizes of the datasets that have been used for training. Table 2 shows the best performing DNNs for each \ntask/application. \nIt is important to stress that in robotics applications, hardware capabilities like memory and availability of GPU must be \nconsidered when selecting the DNN model, since in many cases the runtime requirements of the best performing models cannot be \nsatisfied. Therefore, in cases where the best available models shown in Table 2 cannot be run on the available hardware platforms, \nor when the runtime requirements cannot be fulfilled, smaller DNN architectures must be considered. For instance, in [198] two \nsmaller CNN based robot detectors that are able to run in real-time, in NAO robots while playing soccer, were presented. Each \ndetector is able to process a robot object-proposal in ~1ms, with an average number of 1.5 proposals per frame. The obtained \ndetection rate was ~97%. \n \nTable 5. Popular CNN-based Architectures used in Computer-vision Applications. The dataset size does not consider data augmentation. \nName\nLayers\nParameters\nApplication\nDataset size used for \ntraining (# images) \nLe-Net5\n2 conv, 2 fc, 1 Gaussian\n60 K\nImage classification\n70 K\nAlexNet [20]\n5 conv, 3 fc\n60 M\nImage classification\n1.2 M\nVGG-Fast/Medium/Slow\n5 conv, 3 fc\n77M / 102M / 102M\nImage classification\n1.2 M\nVGG16\n13 conv, 3 fc\n138 M\nImage classification\n1.2 M\nVGG19\n16 conv, 3 fc\n144 M\nImage classification\n1.2 M\nGoogleNet\n22 layers\n7 M\nImage classification\n1.2 M\nResNet-50 [23]\n49 res-conv + 1 fc\n0.75 M4\nImage classification (also \nused in other applications)  \n1.2 M\nDenseNet [235]\n1 conv + 1 pooling + N \n0.8M – 27.2M\nImage classification\n1.2 M\n                                                 \n4 Trained on CIFAR small-image dataset. \n \n20 \ndense blocks + 1 fc\nSENet [221]\n1 conv + 1 pooling + N \nsqueeze-and-excitation \nblocks + 1 fc \n103MB – 137MB\nImage Classification\n1.2 M\nFaster R-CNN (ZF) [29]\n5 conv shared, 2 conv \nRPN, 1 fc reg, 1 fc cls \n54 M\nObject detection and \nrecognition \n200 K\nFaster R-CNN (VGG16) [29]\n13 conv shared, 2 conv \nRPN, 1 fc reg, 1 fc cls \n138 M\nObject detection and \nrecognition \n200 K\nFaster R-CNN (ResNet-50) \n[23] \n49 res-conv shared, 2 \nconv RPN, 1 fc reg, 1 fc \ncls \n0.75 M4\nObject detection and \nrecognition \n200 K\nSegNet [31]\n13 conv encoder, 13 conv \ndecoder \n29.45 M\nSemantic segmentation\n200 K\nAdelaide_context [166]\nVGG-16 based, unary \nand pairwise nets \nNot available\nSemantic segmentation\n200 K\nDeepLabv3 [208]\nResNet based, atrous \nconvolution \nNot available\nSemantic segmentation\n200 K\nPyramid Scene Parsing  \nnetwork [209] \nPyramid pooling modules\n188 MB\nSemantic segmentation\n200K\nStacked hourglass networks \n[167] \n42 layers, multiscale \nskipping connections \n166 MB\nAction Recognition\n24 K\nR*CNN [168]\n5 conv shared, 2 fc for \nmain object, 2 fc for \nsecondary object  \n500 MB\nAction Recognition\n24 K\n \n3.4 Training \n \nTraining, validation and test dataset definitions. The available dataset must be divided into three non-overlapping groups: \ntraining, validation, and test. The training dataset is used for learning the model parameters (weights) of the network. The \n \n21 \nvalidation dataset is used for selecting the hyper-parameters by evaluating the performance of the network under different hyper-\nparameter configurations. The test dataset is used for characterizing the trained network by estimating its generalization error. \nData augmentation is a mechanism used to increase the size of the training datasets by applying some \ntransformations/perturbations to the data in order to generate additional examples. Data augmentation has been used in deep \nlearning (reported, e.g., in [20][74]), but also in traditional object learning methods (e.g. [75]). The most frequently used \ntransformations are geometric ones such as in-plane rotations, flipping, random cropping, and photometric transformations, such \nas RGB jittering [16], color casting, vignetting, and lens distortion [9]. As shown in [74], the use of data augmentation can \nincrease the performance by about 3%. \nWeights Initialization. When a network is trained from scratch, a set of initial weights must be specified. Initialization of neural \nnetworks is an important topic because, if the initialization is not adequate, the network can converge to bad local minima. Initial \nweights need to have diversity for breaking symmetries: if all the initial weights are the same, they will converge to the same value \nper layer, and will not be able to represent the input-output relation. Also, when using sigmoid functions, weights with high values \ncan cause saturation on the neuron’s outputs, generating very small gradients that are not useful for learning. \nA common approach is to use a uniform distribution for weight initialization. For example, a normalized initialization is \nproposed in [6]: \n\n\n\n\n\n\n\n\n+\n+\n−\nout\nin\nout\nin\nn\nn\nn\nn\nU\nW\n6\n,\n6\n~\n \n(2) \nwhere \nin\nn  is the number of inputs for the neuron, and \nout\nn\n is the number of neurons in the next layer connected to the current \none. \n \nFor the particular case of networks based on ReLU activation function, the following initialization procedure is proposed: \n[\n]\nin\nin\nn\nn\nU\nW\n/\n2\n,\n/\n2\n~\n−\n \n(3) \nAlso, when using ReLU units, a small bias near 0.01 can be added to account for the asymmetry of this activation function. \nHyper-parameters for the learning process. Once the architecture and initial weights are selected, a tuning of the network to the \ntask dataset must be performed. The learning procedure (gradient descent with momentum) is controlled by several hyper-\nparameters. Recommended hyperparameters for CNNs [20][74] are: momentum 0.9, weight decay 5*10-4, and initial learning rate \n10-2. Learning rate is decreased by a factor of 10 when the validation error stops decreasing. \nLearning is computationally expensive when compared to final DNN use, and requires large amounts of time, memory, and \ncomputing power. In most of the problems, learning can be performed offline in high-end computers, and robotic platforms can be \nused only for collecting data. Systems based on reinforcement learning, that require interaction of an agent with its environment, \nhave been applied in simulated environments like computer games [190], in which runtime of the full system can be controlled \nand several trials can be performed in parallel. Furthermore, systems able to learn from interaction with their environment have \nbeen developed for some specific tasks like learning eye-hand coordination [48], in which several grippers were used for \ncollecting data, large computer capabilities were available, and supervised learning was adapted for the task. However, online \nlearning on limited hardware platforms is a problem that is far from being solved technically, and is an open research area (see \nSection 5). \n \n22 \nFinally, it is important to stress that the final performance of a DNN depends largely on the implementation details [74], such as \nthe selection of the hyper parameters, fine-tuning, data augmentation, etc. Therefore, this aspect needs to be addressed properly. \n4   Deep Neural Networks in Robot Vision  \nDeep learning has already attracted the attention of the robot vision community, and during the last couple of years studies \naddressing the use of deep learning in robots have been published in robotics conferences. Table 6 shows some of the recently \npublished papers presented at flagship robotics conferences and symposia. The papers have been categorized in four application \nareas: Object Detection and Categorization, Object Grasping and Manipulation, Scene Representation and Classification, and \nSpatiotemporal Vision. The most representative work and the observed tendencies related to the use of deep learning for \naddressing robot vision problems in each area are described in the next sections. It is important to note that much of the work \ndescribed has not been validated in real robots operating under real-world conditions, but only in databases corresponding to robot \napplications. Therefore, in these cases it is not clear whether or not the proposed methods can work in real-time using a real robot \nplatform. \nTable 6. Selected Papers on Robot Vision Applications based on DNN. \n \nPaper \nDNN Model and Distinctive Methods\nApplication\nPasquale et al., 2015 [38]\nUse of CaffeNet in humanoid robots for recognizing objects.\nObject Detection and Categorization\nBogun et al., 2015 [39]\nDNN with LSTM for recognizing objects in videos.\nObject Detection and Categorization\nHosang at al., 2015 [40]\nAlexNet-based R-CNN for pedestrian detection with \nSquaresChnFtrs as person proposal. \nObject \nDetection \nand \nCategorization \n(Pedestrian detection) \nTome et al., 2016 [42]\nCNN (Alexnet v/s GoogleNet) for pedestrian detection with \nLCDF as person proposal. \nObject \nDetection \nand \nCategorization \n(Pedestrian detection) \nLenz at al., 2013 [45]\nCNN trained on hand-labeled data, two-stage with two hidden \nlayers each. \nObject Grasping and Manipulation\nRedmon et al., 2015 [46]\nCNN based on AlexNet trained on hand-labeled data, \nrectangle regression. \nObject Grasping and Manipulation\nSung et al., 2016 [47]\nTransfer manipulation strategy in embedding space by using a \ndeep neural network. \nObject Grasping and Manipulation\nLevine at al., 2016 [48]\nLearn hand-eye coordination independently of camera \ncalibration or robot pose, visual/motor DNN. \nObject Grasping and Manipulation\nZhou 2014 et al., [50]\nCNNs for place recognition based on CaffeNet.\nScene \nRepresentation \nand \nClassification \n(Place recognition) \nGomez-Ojeda et al., 2015 [49]\nCNN for appearance-invariant place recognition, based on \nCaffeNet. \nScene \nRepresentation \nand \nClassification \n(Place recognition) \n \n23 \nHou et al., 2015 [51]\nCNN for loop closing, based on CaffeNet.\nScene \nRepresentation \nand \nClassification \n(Place recognition) \nSundehauf et al., 2015 [52]\nPlace categorization and semantic mapping, based on \nCaffeNet. \nScene \nRepresentation \nand \nClassification \n(Scene categorization) \nYe et al., 2016 [53]\nR-CNN for functional scene understanding, based on \nSelective Search and VGG. \nScene \nRepresentation \nand \nClassification \n(Scene categorization) \nCadena et al., 2016 [97]\nMulti-modal auto-encoders for semantic segmentation. The \ninputs are RGB-D, LIDAR and stereo data. It uses inverse \ndepth parametrization. \nScene \nRepresentation \nand \nClassification \n(Semantic \nsegmentation, \nScene \nDepth \nEstimation) \nLi et al., 2016 [98]\nFCN for vehicle detection. Input is a point map generated \nusing a LIDAR. \nObject Detection and Categorization\nAlcantarilla et al., 2016 [99]\nDeconvolutional Networks for Street-View Change \nDetection. \nScene \nRepresentation \nand \nClassification  \n(Street-View Change Detection) \nSunderhauf et al., 2015 [100]\nR-CNN used for creating region landmarks for describing an \nimage. AlexNet (up to conv3) as feature extractor. \nScene \nRepresentation \nand \nClassification \n(Place Recognition) \nAlbani et al., 2016 [101]\nCNN as a validating step for humanoid robot detection.\nObject \nDetection \nand \nCategorization \n(humanoid soccer robot detection) \nSpeck et al., 2016 [102]\nCNNs for ball localization in robotics soccer. \nObject \nDetection \nand \nCategorization \n(humanoid soccer ball detection) \nFinn et al., 2016 [103]\nDeep Spatial Auto-encoder for learning state representation. \nUsed for reinforcement learning. \nObject Grasping and Manipulation\nGao et al., 2016 [104]\nVisual CNN and Haptic CNN combined for haptic \nclassification. \nSpatiotemporal Vision (Object Understanding)\nHusain et al., 2016 [105]\nTemporal concatenation of the output of pre-trained VGG-16 \ninto a 3D convolutional layer. \nSpatiotemporal Vision (Action Recognition)\nOliveira et al., 2016 [106]\nFCN-based architecture for human body part segmentation. \nObject Detection and Categorization\nZaki et al. 2016 [107]\nConvolutional Hypercube Pyramid based on VGG-f as feature \nextractor for RGBD. \nObject Detection and Categorization\nMendes et al., 2016 [108]\nNetwork-in-Network converted into FCN for road \nsegmentation. \nScene Representation and Categorization \n(Semantic Segmentation) \nPinto et al., 2016 [109]\nAlexNet based architecture to predict grasp location and angle \nfrom image patches, based on self-supervision. \nObject Grasping and Manipulation\n \n24 \nJain et al., 2016 [110]\nFusion RNN based on LSTM units fed by visual features. \nSpatiotemporal \nVision \n(Human \nAction \nPrediction). \nSchlosser et al., 2016 [111]\nR-CNN for pedestrian detection by using RGB-D from \ncamera and LIDAR. The depth represented using HHA. \nRGB-D deformable parts model as object proposals. \nObject Detection and Recognition (Pedestrian \ndetection) \nCostante et al., 2016 [112]\nCNNs for learning feature representation and frame to frame \nmotion estimation from optical flow. \nSpatiotemporal Vision (Visual Odometry)\nLiao et al., 2016 [113]\nAlexNet-based scene classifier with a semantic segmentation \nbranch. \nScene \nRepresentation \nand \nClassification \n(Place \nClassification \n/ \nSemantic \nSegmentation) \nMahler et al., 2016 [114]\nMulti-View Convolutional Neural Networks with Pre-trained \nAlexNet as CNNs for computing shape descriptors for 3D \nobjects. Used for selecting object grasps. \nObject Grasping and Manipulation\nGuo et al., 2016 [115]\nAlexNet-based CNN for detecting object and grasp by \nregression on an image patch. \nObject Grasping and Manipulation\nYin et al., 2016 [116]\nDeep Autoencoder for nonlinear time alignment of human \nskeleton representations.  \nSpatiotemporal \nvision \n(Human \nAction \nRecognition) \nGiusti et al., 2016 [117]\nCNN that receives a trail image as input and classifies it as \nthe kind of motion needed for remaining on the trail. \nScene Representation and Classification (Trail \nDirection Classification) \nHeld et al., 2016 [118]\nCaffeNet, pre-trained on ImageNet, fine-tuned for viewpoint \ninvariance. \nObject Detection and Categorization (Single-\nview Object Recognition) \nYang et al., 2016 [119]\nFCN and DSN based Network with AlexNet as basis. Used \nwith CRF for estimating 3D scene layout from monocular \ncamera. \nScene \nRepresentation \nand \nClassification \n(Semantic \nSegmentation, \nScene \nDepth \nEstimation) \nUršic et al., 2016 [120]\nR-CNN used for generating histograms of part-based models. \nPlaces-CNN (CaffeNet trained on Places 205) as region \nfeature extractor. \nScene \nRepresentation \nand \nClassification  \n(Place Classification) \nBunel et al., 2016 [121]\nCNN architecture of 4 convolutional layers with PReLU as \nactivation function and 2 FC layers. Used for detecting \npedestrians at far distance. \nObject \nDetection \nand \nCategorization \n(Pedestrian Detection) \nMurali et al., 2016 [122]\nVGG architecture as feature extractor for unsupervised \nsegmentation of image sequences. \nSpatiotemporal \nVision \n(Segmentation \nof \ntrajectories in robot-assisted surgery). \n \n25 \nKendall et al., 2016 [123]\nBayesian PoseNet (Modified GoogLeNet) with pose \nregression, use dropout for estimating uncertainty. \nScene \nRepresentation \nand \nClassification  \n(Camera re-localization) \nHusain et al., 2016 [124]\nCNN using layers from OverFeat Network with multiple \npooling sizes. RGB-D inputs. Use of HHA and distance-from-\nwall for depth. \nScene \nRepresentation \nand \nClassification \n(Semantic Segmentation) \nHoffman et al., 2016 [125]\nR-CNN using RGB-D data, based on AlexNet and HHA. \nProposals are based on RGB-D Selective Search. \nObject Detection and Categorization\nSunderhauf et al., 2016 [126]\nPlaces-CNN as image classifier for building 2D grid semantic \nmaps. LIDAR used for SLAM. Bayesian filtering over class \nlabels.  \nScene \nRepresentation \nand \nClassification  \n(Place Classification) \nSaxena et al., 2017 [189]\nCNN used for image-based visual servoing. Inputs are \nmonocular images from current and desired poses. Outputs \nare velocity commands for reaching the desired pose. \nObject Grasping and Manipulation (Visual \nservoing) \nLei et al., [191] \nRobot exploration by using a CNN, trained first by supervised \nlearning, later by using deep reinforcement learning. Tested \non simulated and real experiments. \nScene \nRepresentation \nand \nClassification \n(Scene Exploration)  \nZhu et al., [192] \nRobot navigation by learning a scene-dependent Siamese \nnetwork, which receives images from two places as input, and \ngenerates motion commands for travelling between them. \nScene \nRepresentation \nand \nClassification \n(Visual Navigation) \nMirowski et al., [193]\nRobot navigation in complex maze-like environments from \nraw monocular images and inertial information. Use of deep \nreinforcement learning on a network composed of a CNN \nfollowed by two LSTM layers. Multi-task loss considering \nreward prediction and depth prediction improves learning. \nScene \nRepresentation \nand \nClassification \n(Visual Navigation) \n \n \n4.1 Object Detection and Categorization \n \nObject detection and categorization is a fundamental ability in robotics. It enables a robot to execute tasks that require \ninteraction with object instances in the real-world. Deep learning is already being used for general-purpose object detection and \ncategorization, as well as for pedestrian detection, and for detecting objects in robotics soccer. State-of-the-art methods used for \nobject detection and categorization are based on generating object proposals, and then classifying them using a DNN, enabling \nsystems to detect thousands of different object categories. As   will be shown, one of the main challenges for the application of \nDNNs for object detection and characterization in robotics is real-time operation. It must be stressed that obtaining the required \n \n26 \nobject proposals for feeding the DNNs is not real-time in the general case, and that, on the other hand, general-purpose object \ndetection and categorization DNN based methods are not able to run in real-time in most robotics platforms. These challenges are \naddressed by using task-dependent methods for generating few, fast, and high quality proposals for a limited number of possible \nobject categories. These methods are based on using other information sources for segmenting the objects (depth information, \nmotion, color, etc.), and/or by using object specific, non general-purpose, weak detectors, for generating the required proposals. \nAlso, smaller DNN architectures can be used when dealing with a limited number of object categories.  \nIt is worth mentioning that most of the reported studies do not indicate the frame rate needed for full object \ndetection/categorization, or they show frame rates that are far from being real-time. In generic object detection methods, \ncomputation of proposals using methods like Selective Search or EdgeBoxes takes most of the time [29]. Systems like Faster R-\nCNN that compute proposals using CNNs are able to obtain higher frame rates, but require the use of high-end GPUs for working. \nThe use of task-specific knowledge based detectors on depth [33][34][111], motion [38], color segmentation [101], or weak object \ndetectors [42] can be useful for generating fewer, faster proposals, which is the key for achieving high frame rates on CPU-based \nsystems. Finally, methods based on FCNs cannot achieve high frame rates on robotic platforms with low processing capabilities \n(no GPU available) because they process images with larger resolutions than normal CNNs. Then, FCNs cannot be used trivially \nfor real time robotics on these kinds of platforms. \nFirst, we will analyze the use of complementary information sources for segmenting the objects. Robotic platforms usually have \nsensors, such as Kinects/Asus or LIDAR sensors, that are able to extract depth information. Depth information can be used for \nboosting the object segmentation. Methods that use RGBD data for detecting objects include those presented in \n[33][34][92][107][125], while methods that use LIDAR data include those of [98][111]. These methods are able to generate \nproposals by using tabletop segmentation or edge/gradient information. Also, they generate colorized images from depth for use  \nin a standard CNN pipeline for object recognition. These studies use CNNs and custom depth-based proposals, and then their \nspeed is limited by the CNN model. For instance, in [197], a system is described that runs at 405 fps on a Titan X GPU, 67 fps on \na Tegra X1 GPU, and 62 fps on a Core i7 6700K CPU [197]. It must be noted that, while TitanX GPU and Core i7 CPU \nprocessors are designed for  desktop computers, Tegra X1 is a mobile GPU processor for embedded systems aimed at low power \nconsumption, and so it can be used on robotic platforms. \nSecond, we will present two applications that use specific detectors for generating the object proposals: pedestrian detection, \nand detection of objects in robotics soccer. Detecting pedestrians in real time with high reliability is an important ability in many \nrobotics applications, especially for autonomous driving. Large differences in illumination and variable, cluttered backgrounds \noutdoors are hard issues to be addressed. Methods that use CNN-based methods for pedestrian detection include [40], [41], [42], \n[121], [199], [200], [201], [202], [203]. Person detectors, such as LDCF [43] or ACF [44], are used for generating pedestrian \nproposals in some methods. As indicated in [42], the system based on AlexNet [20] requires only 3 ms for processing a region, \nand proposal computation runs at 2 fps when using LDCF, and at 21.7 fps when using ACF in an NVIDIA GTX980 GPU, with  \nimages  of size 640x480. The lightweight pedestrian detector is able to run at 2.4 fps in an NVIDIA Jetson TK1, and beats \nclassical methods by a large margin [42]. A second approach is to use a FCN for detecting parts of persons [106]. This method is \ntested on a NVIDIA Titan X GPU, and   is able to run at 4 fps when processing images of size 300x300. KITTY pedestrians is a \nbenchmark used by state-of-the-art methods. Current leading methods whose algorithms are described (many of the best \nperforming methods do not describe their algorithms) are shown in Table 7. Note that, as the objective of the benchmark is to \nevaluate accuracy, most of the methods cannot run in real-time (~30 fps). This is an aspect that needs further research. \n \n \n27 \nTable 7. Selected Methods for Pedestrian Detection from KITTY Benchmark. Note that methods not describing  their algorithms are not  \nincluded. \nMethod \nModerate \nEasy \nHard \nRuntime \nComputing Environment \nRRC [199] (code available) \n75.33% \n84.14 % \n70.39 % \n3.6 s \nGPU @ 2.5 Ghz (Python + C/C++) \nMS-CNN [200] (code available) \n73.62% \n83.70% \n68.28% \n0.4 s \nGPU @ 2.5 Ghz (C/C++) \nSubCNN [201] \n71.34 % \n83.17 % \n66.36 % \n2 s \nGPU @ 3.5 Ghz (Python + C/C++) \nIVA [202] (code available) \n70.63 % \n83.03 % \n64.68 % \n0.4 s \nGPU @ 2.5 Ghz (C/C++) \nSDP+RPN [203] \n70.20 % \n79.98 % \n64.84 % \n0.4 s \nGPU @ 2.5 Ghz (Python + C/C++) \n \nRobotics soccer is an interesting area because it requires real-time vision algorithms able to work with very limited \ncomputational resources. Deep learning techniques developed in robot soccer can therefore be transferred to other resource-\nconstrained platforms, such as smartphones and UAVs. Current applications for robotics soccer include soccer player detection \n[101][198], and ball detection [102]. State-of-the-art soccer player detection [198] uses a model based on SqueezeNet using only 2 \nconvolutional layers, an extended fire layer and a fully-connected layer, while processing regions  of size 28x28. This method \nrequires only 1 ms on an Nao robot for processing a region, using only an Intel Atom CPU. Another application is ball detection, \nwhich is addressed in [102]. This method uses an architecture based on 3 conv layers and 2 fully-connected layers. However, its \nruntime inside a Nao robot is not reported. Currently, CNNs have been used for solving very specific tasks in robotics soccer \nsuccessfully, and it is expected that new algorithms (and hardware) will enable the use of deep learning in more global, high-level \ntasks. \n \n4.2. Object Grasping and Manipulation \nThe ability to grasp objects is of paramount importance for autonomous robots. Classical algorithms for grasping and \nmanipulating objects require detecting both the gripper and the object in the image, and using graph-based or Jacobian-based \nalgorithms for computing the movements for the gripper. Deep learning has generated a paradigm change since tasks like \ngrasping, manipulation and visual servoing can be solved directly, without the need of hand-crafting intermediate processes like \ndetecting features, detecting objects, detecting the gripper, or performing graph-based optimizations. As problems are solved as a \nwhole from raw data, the performance limit imposed by inaccuracies of intermediate processes can be surpassed, and, therefore, \nnovel approaches for solving tasks related to grasping, manipulation, and visual servoing are being developed. Also, the new \nalgorithms are able to be generalized to novel objects and situations. The performance of this new generation of methods is \ncurrently limited by the size of the training datasets and the computing capabilities, which becomes relevant when using hardware-\nlimited robotic platforms. \nGrasping objects directly, without the need of detecting specific objects, is a difficult problem because it must generalize over \ndifferent kinds of objects, including those not available when training the system. This problem has recently been addressed \nsuccessfully by using deep learning. Studies that are based on selecting grasp locations include [45], [46], [115] and [109], which \nuses self-supervision during 700 hours of trial and error in a Baxter robot (see Figure 4). That work has been extended for learning \neye-hand  coordination in [48], which is the state-of-the-art. The latter system is trained without supervision by using over 800,000 \n \n28 \ngrasp attempts collected over the course of two months, using between 6 and 14 manipulators at any given time. The architecture \nis based on a concatenation of two inputs: an image (size 512x512), and a motor command. The image network has 7 \nconvolutional layers, and its output is concatenated with the motor command. The concatenation layer is further processed by an \nadditional 8 convolutional layers and two fully-connected layers. The system achieves effective real-time control; the robot can \ngrasp novel objects successfully and correct mistakes by continuous servoing, and lowers the failure rate from 35% (hand \ndesigned) to 20%. However, learned eye-hand coordination depends on the particular robot used for training the system. The full \ncomputing hardware used in this work is not fully described. It must be noted that these kinds of solutions are able to run \nsuccessfully in real-time using GPUs. \n \nFigure 4. A Baxter Robot learning to Grasp Objects by Trial and Error, by using Self-supervised Learning. Figure taken from [109] with \npermission of the authors. \n \nSystems able to infer manipulation trajectories for object-task pairs using DNNs have been developed recently. This task is \ndifficult because generalization over different objects and manipulators is needed. They include Robobarista [47], and [103]. \nThese systems are able to learn from manipulation demonstrations and generalize over new objects. In Robobarista [47], the input \nof the system is composed of a point cloud, a trajectory, and a natural language space, which is mapped to a joint space. Once \ntrained, the trajectory generating the best similarity to a new point cloud and natural language instruction is selected. \nComputational hardware (GPU) and runtime are not reported in this work. In [103] the system is able to learn new tasks by using \nimage-based reinforcement learning.  That system is composed by an encoder and a decoder. The encoder is formed by 3 \nconvolutional layers and 2 fully-connected layers. Computational hardware and runtime are not reported in this work. \nVisual servoing systems have also benefited from the use of deep learning techniques. In Saxena et al. [189], a system that is \nable to perform image-based visual servoing by feeding raw data into convolutional networks is presented. The system does not \nrequire 3D geometry of the scene or intrinsic parameters from the camera. The CNN takes a pair of monocular images as input, \nrepresenting the current and desired pose, and processes them by using 4 convolutional layers and a fully-connected layer. The \nsystem is trained for computing the transform in 3D space (position and orientation) needed for moving the camera from its \ncurrent pose to the desired one. The estimated relative motion is used for generating linear and angular speed commands for \nmoving the camera to the desired pose. The system is trained  on synthetic data, and tested both in simulated environments and \n \n29 \nusing a real quadrotor. The system takes 20 msec for processing each frame when using an NVIDIA Titan X GPU, and 65 msec \nwhen using a Quadro M2000 GPU. A wifi connection was used for transferring data between the drone and a host PC. \nMultimodal data delivers valuable information for object grasping. In [104], a haptic object classifier that is fed by visual and \nhaptic data is proposed. The outputs of a visual CNN and a haptic CNN are fused into a fusion layer, followed by a loss function \nfor haptic adjectives. The visual CNN is based on the (5a) layer of GoogleNet, while the haptic CNN is based on 3 one-\ndimensional convolutional layers followed by a fully-connected layer. Examples of haptic adjectives are “absorbent,” “bumpy,” \nand “compressible”. The system, after being trained, is able to estimate haptic adjectives of objects by using only visual data, and \nis shown to improve over systems based on classical features. Computing hardware and runtime of the method is not reported. \nIn summary, deep learning has enabled the generation of novel approaches to solving tasks related to object grasping, object \nmanipulation, and visual servoing. Real-time processing is needed for hand-eye coordination and visual servoing to work, which \nimposes constraints on hardware capabilities. However, real-time processing is not needed for open-loop object grasping \ndetection. Direct methods that do not require detecting objects to work are robust, but also dependent on the robot used for \ncollecting data. GPUs are needed for deep learning based methods that require closed loop control, which limits its usefulness \nwhen working with hardware-limited robotic platforms. Grasping and manipulation algorithms will continue to benefit from the \nuse of deep reinforcement learning methods in the near future. \n \n4.3 Scene Representation and Classification \n \nScene representation is an important topic since it is required by mobile robots for performing a variety of tasks that  depend on \nsemantic and/or geometric information from the current scene. In particular, visual navigation is an important ability that is needed \nby autonomous robots for performing more general tasks that involve moving autonomously. Classical approaches for \nrepresenting scenes from monocular images are based on extraction of hand-crafted features, while visual navigation requires the \nrobot to have previously had a metric or topological map of its environment as well as an estimation of the depth of the \nobstacles/objects in the current frame. Deep learning has enabled solving   a variety of scene-related tasks directly, ranging from \nencoding scene information to performing navigation from raw monocular data, without the need of handcrafting sub-processes \nlike feature extraction, map estimation, or localization. These novel task-oriented algorithms are able to solve difficult problems \nsuccessfully that were not directly affordable before. In the following paragraphs the use of deep learning in applications such as \nplace recognition, semantic segmentation, visual navigation, and 3D geometry estimation is analyzed. \n \nPlace recognition is a task that has been solved successfully by using deep learning-based approaches. Studies related to this \nproblem are [49], [50], [51], [52], [113], [120] and [100]. These methods are able to recognize places under severe appearance \nchanges caused  by weather, illumination, and changes in viewpoint. Also, in [126], data from LIDAR is used alongside a CNN \nfor building a tridimensional semantic map. The CNN used is based on AlexNet and requires 30 ms to run on an Nvidia Quadro \nK4000 GPU. There are several datasets for testing algorithms, such as Sun [204], and Places [50][205], and also several variants \nderived from them. As there is not a dominant benchmark for place recognition, different studies use different datasets, sometimes \ncustomized, and thus most methodologies cannot be compared directly. Also, the best performing methods in ILSVRC, scene \nclassification challenge [216], and Scene2 challenges [217], do not describe their algorithms in detail, but achieve an impressive \n0.0901 top-5 classification error. Computing hardware and runtime related to challenges [216] and [217] are not reported. \n \n30 \nNevertheless, the related datasets used in scene classification, and the best performing methods in each case, are described in \nTable 8. \n \nTable 8 Datasets used for Scene Classification. \nDataset \nResults \nPlaces [50] \n50.0% [50] (CaffeNet), 14.56 ms @ Pascal Titan X \n85.16% [113] (AlexNet for scene classification, 8 conv for semantic segmentation)  \nSun [204][206] \n38% [204] (non deep) \n 66.2% [50] (AlexNet) , 14.56 ms @ Pascal Titan X \nSun-RGBD[113] \n41.3% [113] (input 81x81, 5 conv + 3 fc classification, 8 conv semantic segmentation) \n \nSemantic segmentation consists of classifying each pixel in the image, enabling  the solving of tasks that require a pixel-wise \nlevel of precision. Studies related to semantic segmentation of scenes are [98], [108], [113], [208], [209], [210], [211], [212], \n[213], [214] and [215]. Also, multimodal information is used in [97], [124] and [126]. These studies are useful for tasks that \nrequire pixel-wise precision like road segmentation and other tasks in autonomous driving. However, runtimes of methods based \non semantic segmentation are usually not reported. PASCAL VOC and Cityscapes are two benchmarks used for semantic \nsegmentation. While images from PASCAL VOC are general-purpose, Cityscapes is aimed at autonomous driving. Best \nperforming methods in both databases are reported in Tables 9 and 10. It can be noted that DeepLabv3 [208] and PSPNet [209] \nachieve good average precision at both benchmarks, and also code from [209] is available. Thus, [209] is recommended. Also, the \nincreasing performance in average precision has a deep impact on autonomous driving, as this application requires   confident \nsegmentation for working because of the risks involved in it.  \n \nTable 9: PASCAL VOC Segmentation Benchmark. Only entries with reported methods are considered. Repeated entries are not considered. \n \nName \nAP \nRuntime (msec/frame) \nDeepLabv3-JFT [208] \n86.9 \nn/a \nDIS [213] \n86.8 \n140 msec (gpu model not reported) \nCASIA_IVA_SDN [214] \n86.6 \nn/a \nIDW-CNN [215] \n86.3 \nn/a \nPSPNet [209] (code available) \n85.3 \nn/a \n \nTable 10: Cityscapes Benchmark. Only entries with reported methods are considered. Repeated entries are not considered. \n \n \n31 \nName \nIoU class \nRuntime (ms/frame) \nDeeplabv3 [208] \n81.3 \nn/a \nPSPNet [209] \n81.2 \nn/a \nResNet-38 [210] \n80.6 \n6 msec (minibatch size 10) @ GTX 980 gpu \nTuSimple_Coarse [211] (code available) \n80.1 \nn/a \nSAC-multiple [212] \n78.2 \nn/a \n \nVisual navigation using deep learning techniques is an active research topic since new methodologies are able to solve \nnavigation directly, without the need of detecting objects or roads. Examples of studies related to visual navigation are [117], [93], \n[191], [192] and [193]. Their   methods are able to perform navigation directly over raw images captured by RGB cameras. For \ninstance, the forest trail follower in [117] has an architecture composed of 7 convolutional layers and 1 fully-connected layer. It is \nable to run at 15 fps on an Odroid-U3 platform, enabling it to run in a real UAV. \nThe use of CNN methodologies for estimating 3D geometry is another active research topic in robotics. Studies related to 3D \ngeometry estimation from RGB images include [112], [119], [123], Also, depth information is added in [97] and [124]. These \nmethods provide a new approach to dealing with 3D geometry, and are shown to overcome the classical structure-from-motion \napproaches, as they are able to infer depth even from only a single image. \nA functional area is a zone in the real-world that can be manipulated by a robot. Functional areas are classified by the kind of \nmanipulation action the robot can realize on them. In [53] a system for localizing and recognizing functional areas by using deep \nlearning is presented. The system enables an autonomous robot to have a functional understanding of an arbitrary indoor scene. An \nontology considering 11 possible end categories is set. Functional areas are detected by using selective-search region proposals, \nand then by applying a VGG-based CNN trained on the functionality categories. The system is able to generalize onto novel \nscenes. Neither hardware nor runtime are reported. However, its architecture is similar to RCNN [27], which requires 18 seconds \nfor processing a frame on a K20x GPU. \nIn summary, deep learning has given rise to a new generation of task-oriented algorithms that do not require explicit subtask \nengineering. In general, methods for representing, recognizing, or classifying scenes as a whole (not pixel-wise) do not require \ncomputing object proposals; the image is processed directly by a CNN, and then, depending on the computing capabilities of the \nrobotic platform, near real time processing can be achieved. However, pixel-wise methods based on FCNs (like semantic \nsegmentation) require a GPU for running in near real time, because a larger resolution is needed by the intermediate convolutional \nlayers. When using robotic platforms with low computing capabilities (only CPU), processing each frame in an FCN can take \nseveral seconds. Considering that CPUs are also needed to execute other tasks concurrently, the use of FCN-based methods on \nCPUs is discouraged for real time applications in most robotic platforms. \n \n4.4 Spatiotemporal Vision \n \nThe processing of video and other spatiotemporal sources of information is of paramount importance in robotics. Classical \nmethods related to spatiotemporal vision require engineering subtasks such as motion detection, optical flow, local feature \nextraction, or image segmentation. Deep learning has the potential to change the current paradigm of spatiotemporal vision from \n \n32 \nhand-engineering complex subtasks to solving tasks as a whole. The increment in both video datasets and computational power \nwill enable the analysis of spatiotemporal data at near real-time, but only in the future. Then, spatiotemporal vision will be a goal \nof future robot vision as well as computer vision research.  \nThe use of short video sequences for object recognition is an alternative for improving current image-based techniques. In [39], \na convolutional LSTM is proposed for processing short video sequences, those containing about four frames, for robotic \nperception. The system is shown to improve on the baseline (smoothing CNN results from individual image frames), and   is able \nto run at 0.87 fps when using a GPU. \nHuman action recognition is an area that can benefit greatly from using video sequences instead of isolated images. Reports of \nresearch dealing with recognition of human actions include [229], [230], [231], [232], [233], [234], [105], [116], and [110], in \nwhich driver activity anticipation is performed. In various studies, information from multiple frames is integrated by using several \nCNNs for each frame, by using LSTM or by using Dynamic Time Warping. The C3D method [234] is able to run at 313 fps on a \nK40 GPU, processing videos with resolution 320x240. Thus, those kinds of methods have the potential for running in real time. \nHowever, current state-of-the-art methods are not real time, or their runtimes are not reported. The best performing methods on the \nUCF-101 dataset are reported in Table 11. Note that both the accuracy and runtime of C3D [234] are lower than Wu et al. [232], \nand so a tradeoff between accuracy and runtime is present. For selecting a method, it must be considered if real time is needed in \nthe specific task to be solved. \n \nTable 11: Current Results on the UCF-101 Video Dataset for Action Recognition [230] \n \nName \nAccuracy \nRuntime (msec/frame) \nC3D (1 net) + linear SVM [234] \n82.3% \n3.2 msec/frame @ Tesla K40 GPU \nVGG-3D + C3D [105] \n86.7% \nn/a  \nNg et al. [231] \n88.6% \nn/a \nWu et al. [232] \n92.6% \n1730 msec/frame @ Tesla K80 GPU \nGuo et al. [230] \n93.3% \nn/a \nLev et al. [233] \n94.0% \nn/a \n \nTransfer of video-based computer vision techniques to medical robotics is an active area of research, especially in surgery-\nrelated applications. The use of deep learning in this application area has already begun. For instance, in [122], a system that is \nable to segment video and kinematic data for performing unsupervised trajectory segmentation of multi-modal surgical \ndemonstrations is implemented. The system is able to segment video-kinematic descriptions of surgical demonstrations \nsuccessfully corresponding to stages such as “position,” “push needle,” “pull needle,” and “hand-off.” The system is based on a \nswitching linear dynamical system which considers both continuous and discrete states composed of kinematic and visual features. \nVisual features are extracted from the fifth convolutional layer of a VGG CNN and then reduced to 100 dimensions by using PCA. \nClustering on the state space is applied by the system and it is able to learn transitions between clusters, enabling trajectory \n \n33 \nsegmentation. The system uses a video source with 10 frames per second; however, the frame rate of the full method is not \nreported. \nSpatiotemporal vision applications need real-time processing capability to be useful in robotics. However, current \ninvestigations using deep learning are experimental, and the frame rates for most of the methods are not reported. Platforms with \nlow computing capability (only CPU) are not able to run most of the methods at a useful frame rate. Availability of specialized \nhardware for computing CNNs [12] would enable the use of deep spatiotemporal vision applications in the future. \n5. Discussion: Current and Future Challenges of Deep Learning in Robotic Applications  \nAs already mentioned, the increase in the use of deep learning in robotics will depend on the possibility that DNN methods will \nbe able to adapt to the requirements of robotics applications, such as real-time operation with limited on-board computational \nresources, and the ability to deal with constrained observational conditions derived from the robot geometry, limited camera \nresolution, and sensor/object relative pose. \n \nThe current requirements of large computational power and memory needed for most CNN models used in vision applications \nare relevant barriers to its adoption in resource-constrained robotic applications such as UAVs, humanoid biped robots, or \nautonomous vehicles. Efforts are already being made to develop CNN architectures that can be compressed and quantized for use \nin resource-constrained platforms [89][90] (see Section 2.8). For instance, in [198] a CNN based robot detector is presented that is \nable to run in real-time, in NAO robots while playing soccer. In addition, companies such as Intel, NVIDIA, and Samsung, just to \nname a few, are developing CNN chips that will enable real-time vision applications [12]. For instance, mobile GPU processors \nlike NVIDIA Tegra K1 enable efficient implementation of deep learning algorithms with low power consumption, which is \nrelevant for mobile robotics [197]. It is expected that these methodologies will consolidate in the next few years, and will then be \navailable to the developers of robot vision applications. \nThe ability of deep learning models to manage spatial invariance to the input data in a computationally and efficient manner is \nanother important issue. In order to address this, the Spatial Transformer Module (STM) was recently introduced [179]. This \ncorresponds to a self-contained module that can be incorporated into DNNs, and that is able to perform explicit spatial \ntransformation of the features. The most important characteristic of STM is that its parameters (i.e. the transformations of the \nspatial transformation) can be learned together with the other parameters of the network using the back propagation of the loss. \nThe further development of the STM concept will allow addressing the required invariance to the observational conditions derived \nfrom the robot geometry, limited camera resolution, and sensor/object relative pose. STMs are able to deal with rotations, \ntranslations, and scaling [179], and STMs are already being extended to deal with 3D transformations [180]. Further development \nof STM-inspired techniques is expected in the next few years. \nUnsupervised learning is another relevant area of future development for deep-learning based vision systems. Biological \nlearning is largely unsupervised; animals and humans discover the structure of the world by exploring and observing it. Therefore, \none would expect that similar mechanisms could be used in robotics and other computer-based systems. Until now, the success of \npurely supervised learning, which is based largely on the availability of massive labeled data, has overshadowed the application of \nunsupervised learning in the development of vision systems. However, when considering the increasing availability of non-labeled \ndigital data, and the increasing number of vision applications that need to be addressed, one would expect further development of \nunsupervised learning strategies for DNN models [12][16]. In the case of robotics applications, a natural way of addressing this \n \n34 \nissue is by using deep learning and reinforcement learning together. This strategy has already been applied for the learning of \ngames (Atari, Go, etc.) [190][196], for solving simple tasks like pushing a box [103], and for solving more complex tasks such as \nnavigating on simulated environments [192][193], but not for robots learning in the wild. The visual system of most animals is \nactive, i.e. animals decide where to look, based on a combination of internal and external stimuli. In a robotics system having a \nsimilar active vision approach, reinforcement learning can be used for deciding where to look according to the results of the \ninteraction of the robot with the environment. \n A related relevant issue is that of open-world learning [181], i.e. to learn to detect new classes incrementally, or to learn to \ndistinguish among subclasses incrementally after the “main” one has been learned. If this can be done without supervision, new \nclassifiers can be built based on those that already exist, greatly reducing the effort required to learn new object classes. Note that \nhumans are continuously inventing new objects, fashion changes, etc., and therefore robot vision systems will need to be updated \ncontinuously, adding new classes, and/or updating existing ones [181]. Some recent work has addressed these issues, based mainly \non the joint use of deep-learning and transfer-learning methods [182][183]. \nClassification using a very large number of classes is another important challenge to address. AlexNet,  like most CNN \nnetworks, was developed for solving problems which contain ~1,000 classes (e.g., ILSVRC 2012 challenge). It uses fully \nconnected units and a final softmax unit for classification. However, when working  on problems with a very large number of \nclasses (10,000+) in which dense data sampling is available, nearest neighbors could work better than other classifiers [188]. Also, \nthe use of a hierarchy-aware cost function (based on WordNet) could enable providing more detailed information when the classes \nhave redundancy [188]. However, in current object recognition benchmarks (like ILSVRC 2016), the number of classes has not \nincreased, and no hierarchical information has been used.  The ability of learning systems to deal with a high number of classes is \nan important challenge that needs to be addressed for performing open-world learning. \nAnother important area of research is the combination of new deep learning based methods with classical vision methods based \non geometry, which has been very successful in robotics (e.g. visual SLAM). This topic has been addressed at recent workshops \n[58] and conferences [59]. There are several ways in which these important and complementary paradigms can be combined. On \nthe one hand, geometry-based methods such as Structure from Motion and Visual SLAM can be used for the training of deep-\nlearning based vision systems; geometry-based methods can extract and model the structure of the environment, and this structure \ncan be used for assisting the learning process of a DNN. On the other hand, DNNs can also be used for learning to compute the \nvisual odometry automatically, and eventually to learn the parameters of a visual SLAM system. In a very recent study, STMs \nwere extended to 3D spatial transformations [180], allowing end-to-end learning of DNNs for computing visual odometry. It is \nexpected that STM-based techniques will allow end-to-end training for optical flow, depth estimation, place recognition with \ngeometric invariance, small-scale bundle adjustment, etc. [180]. \nA final relevant research topic is the direct analysis of video sequences in robotics applications. The analysis of video \nsequences by using current deep-learning based techniques is very demanding of computational resources. It is expected that the \nincrease of computing capabilities will enable the analysis of video sequences in real-time in the near future. Also, the availability \nof labeled video datasets for learning tasks such as action recognition will enable the improvement of robot perception tasks, \nwhich are currently addressed by using independent CNNs for individual video frames. \n \n35 \n6. Conclusions  \nIn light of the paradigm shift that deep-learning has produced in pattern recognition and machine learning, this survey has \naddressed the use of DNNs in robot vision. After providing a comprehensive overview of DNNs, which includes presenting the \nhistorical development of the discipline, basic definitions, popular DNN models, and application areas in computer vision, the \ndesign process of DNN-based vision systems has been presented, followed by a review of the application of deep-learning in robot \nvision. Research tendencies are shown and the main studies are grouped together in four application areas: Object Detection and \nCategorization, Object Grasping and Manipulation, Scene Representation and Classification, and Spatiotemporal Vision. Then, a \ndiscussion about the future of deep learning in robotic vision has been presented. \nWe believe that this survey provides a valuable guide for the developers of robot vision systems, since it promotes \nunderstanding of the basic concepts behind the application of deep-learning in vision applications, explains the tools and \nframeworks used in the development process of vision systems, and shows current tendencies in the use of DNN models in robot \nvision. \nIt is expected that the use of deep learning in robot vision will increase in the next few years, thanks to a better adaptation of \nDNN-based solutions to the requirements of robotics applications. Areas in which advances will be observed in the coming years \nare DNN models with fewer computational and memory requirements, and DNN models that are invariant to different \ntransformations in the input data. Moreover, given that in many applications robots need to learn as they interact with the real-\nworld, more research will be done in unsupervised learning, open-world learning, and in the joint use of deep and reinforcement \nlearning. Finally, it is expected that joint use of geometry-based and deep-based methods will allow developing state-of-the-art \nvision systems that will allow increasing the autonomy of robotic systems, and also the automatic training of DNNs. \n \nAcknowledgments \nThis work was partially funded by FONDECYT grant 1161500. \n \nReferences \n \n1. \nY. Bengio, P. Simard, P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural \nNetworks  (Volume:5 ,  Issue: 2 ), pages 157 - 166, Mar 1994. \n2. \nCorinna Cortes , Vladimir Vapnik. Support-Vector Networks. Machine Learning, vol 20, number 3, pages 273-297 (1995) \n3. \nD. H. Hubel and T. N. Wiesel, Receptive fields and functional architecture of monkey striate cortex, The Journal of physiology, 1968. \n4. \nK. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in \nposition. Biological cybernetics, 36(4):193–202, 1980. \n5. \nV. Nair, G.E. Hinton, Rectified Linear Units Improve Restricted Boltzmann Machines, Proc. of the ICML 2010. \n6. \nX. Glorot, Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the International \nConference on Artificial Intelligence and Statistics (AISTATS’10). Society for Artificial Intelligence and Statistics. 2010. \n7. \nX. Glorot, A. Bordes & Y. Bengio. Deep sparse rectifier neural networks. In Proc. 14th International Conference on Artificial Intelligence \nand Statistics 315–323 (2011). \n8. \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing Human-Level Performance on \nImageNet Classification. IEEE International Conference on Computer Vision (ICCV), 2015. \n9. \nYanming Guo, Yu Liu, Ard Oerlemans, Songyang Lao, Song Wu, Michael S. Lew. Deep learning for visual understanding: A review. \nNeurocomputing, 187 (2016), pp. 27-48. \n10. Soren Goyal, Paul Benjamin. Object Recognition Using Deep Neural Networks: A Survey. http://arxiv.org/abs/1412.3684. Dec 2014. \n11. Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang. Recent \nAdvances in Convolutional Neural Networks. http://arxiv.org/abs/1512.07108. Dec 2015. \n12. Yann LeCun, Yoshua Bengio, Geoffrey Hinton. Deep learning. Nature 521, 436–444 (28 May 2015), doi:10.1038/nature14539 \n \n36 \n13. Li Deng. A Tutorial Survey of Architectures, Algorithms, and Applications for Deep Learning. APSIPA Transactions on Signal and \nInformation Processing. \n14. J. Schmidhuber. Deep Learning in Neural Networks: An Overview. Neural Networks, Volume 61, January 2015, Pages 85-117 (DOI: \n10.1016/j.neunet.2014.09.003), published online in 2014. \n15. Seyed-Mahdi Khaligh-Razavi. What you need to know about the state-of-the-art computational models of object-vision: A tour through the \nmodels. ArXiv:1407.2776 \n16. Suraj Srinivas, Ravi Kiran Sarvadevabhatla, Konda Reddy Mopuri, Nikita Prabhu, Srinivas S. S. Kruthiventi and R. Venkatesh Babu. A \nTaxonomy \nof \nDeep \nConvolutional \nNeural \nNets \nfor \nComputer \nVision. \nFront. \nRobot. \nAI, \n11 \nJanuary \n2016 \n| \nhttp://dx.doi.org/10.3389/frobt.2015.00036 \n17. Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code \nrecognition. Neural Comp., 1989. \n18. Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel, D. Henderson. Handwritten digit recognition with a back-\npropagation network. In Proc. Advances in Neural Information Processing Systems 396–404 (1990). \n19. Y. Lecun, L. Bottou, Y. Bengio, P. Haffner. Gradient-based learning applied to document recognition. Proc. of the IEEE  (Volume:86 , \nIssue: 11 ). Pages 2278 - 2324. Nov 1998. \n20. Alex Krizhevsky and Sutskever, Ilya and Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. \nAdvances in Neural Information Processing Systems 25. Pages 1097—1105. 2012. \n21. Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. ILSVRC-2014. \nhttp://arxiv.org/pdf/1409.1556. \n22. Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and \nVincent Vanhoucke and Andrew Rabinovich. Going Deeper with Convolutions. CVPR 2015. \n23. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. Microsoft Research, 2015. \nArXiv:1512.03385. \n24. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Identity Mappings in Deep Residual Networks. arXiv:1603.05027 \n25. https://github.com/KaimingHe/deep-residual-networks \n26. Koen E.A.Van de Sande, Jasper R.R. Uijlings, Theo Gevers, Arnold W.M. Smeulders. Segmentation As Selective Search for Object \nRecognition. Proceedings of the 2011 International Conference on Computer Vision, pages 1879--1886, 2011. \n27. R. Girshick, J. Donahue, T. Darrell, J. Malik. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. IEEE \nConference on Computer Vision and Pattern Recognition (CVPR), 2014. \n28. R. Girshick. Fast R-CNN. Proceedings of the 2015 IEEE International Conference on Computer Vision, 1440-1448 \n29. Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal \nNetworks. arXiv:1506.01497v3, 2015. \n30. Jon Long, Evan Shelhamer, Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation. CVPR 2015 (best paper honorable \nmention) \n31. Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla. SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image \nSegmentation, 2015. arXiv:1511.00561. \n32. Alex Kendall, Vijay Badrinarayanan, Roberto Cipolla. Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder \nArchitectures for Scene Understanding. arXiv:1511.02680. \n33. Max Schwarz, Hannes Schulz, and Sven Behnke. RGB-D Object Recognition and Pose Estimation based on Pre-trained Convolutional \nNeural Network Features. ICRA 2015. \n34. Saurabh Gupta, Ross Girshick, Pablo Arbeláez, Jitendra Malik. Learning Rich Features from RGB-D Images for Object Detection and \nSegmentation.  ArXiv:1407.5736 (ECCV 2014) \n35. Asako Kanezaki. RotationNet: Learning Object Classification Using Unsupervised Viewpoint Estimation. arXiv:1603.06208v1 [cs.CV] 20 \nMar 2016 \n36. Sepp Hochreiter, Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, Volume 9 Issue 8, November 15, 1997. Pages \n1735-1780. \n37. A. Gers F., J. Schmidhuber, F. Cummins. Learning to Forget: Continual Prediction with LSTM. Istituto Dalle Molle Di Studi Sull \nIntelligenza Artificiale. 1999. \n38. Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco, Lorenzo Natale. Real-world Object Recognition with Off-the-shelf \nDeep Conv Nets: How Many Objects can iCub Learn? arXiv:1504.03154v2 \n39. Ivan Bogun, Anelia Angelova, Navdeep Jaitly. Object Recognition from Short Videos for Robotic Perception. arXiv:1509.01602v1 [cs.CV] \n4 Sep 2015 \n40. J. Hosang, M. Omran, R. Benenson, B. Schiele. Taking a Deeper Look at Pedestrians. CVPR 2015. \n \n37 \n41. Rodrigo Benenson, Markus Mathias, Tinne Tuytelaars, Luc Van Gool. Seeking the Strongest Rigid Detector. Computer Vision and Pattern \nRecognition (CVPR), 2013 IEEE Conference on. \n42. D. Tomè, F. Monti, L. Baroffio, L. Bondi, M. Tagliasacchi, S. Tubaro. Deep Convolutional neural networks for pedestrian detection. \narXiv:1510.03608v5 [cs.CV] 7 Mar 2016 \n43. Woonhyun Nam, Piotr Dollár, and Joon Hee Han. Local Decorrelation For Improved Pedestrian Detection. NIPS 2014, Montreal, Quebec. \n44. P. Dollar, R. Appel, S. Belongie, P. Perona. Fast feature pyramids for object detection. IEEE Trans. Pattern Anal. Mach. Intell., vol 36, \nnumber 8. August 2014. \n45. Ian Lenz, Honglak Lee, Ashutosh Saxena. Deep Learning for Detecting Robotic Grasps. ArXiv:1301.3592. \n46. Joseph Redmon, Anelia Angelova. Real-Time Grasp Detection Using Convolutional Neural Networks. arXiv:1412.3128v2 [cs.RO] 28 Feb \n2015 \n47. Jaeyong Sung, Seok Hyun Jin, Ian Lenz, and Ashutosh Saxena. Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal \nEmbedding. arXiv:1601.02705v1 [cs.RO] 12 Jan 2016 \n48. Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen. Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning \nand Large-Scale Data Collection. rXiv:1603.02199v2 [cs.LG] 24 Mar 2016 \n49. Ruben Gomez-Ojeda, Manuel Lopez-Antequera, Nicolai Petkov, Javier Gonzales-Jimenez. Training a Convolutional Neural Network for \nAppearance-Invariant Place Recognition. arXiv:1505.07428v1 [cs.CV] 27 May 2015 \n50. Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning Deep Features for Scene Recognition using \nPlaces Database. Advances in Neural Information Processing Systems (NIPS) 27, 2014. \n51. Yi Hou, Hong Zhang, Shilin Zhou. Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection. \narXiv:1504.05241v1 [cs.RO] 20 Apr 2015 \n52. Niko Sunderhauf, Feras Dayoub, Sean McMahon, Ben Talbot, Ruth Schulz, Peter Corke, Gordon Wyeth, Ben Upcroft, and Michael \nMilford. Place Categorization and Semantic Mapping on a Mobile Robot. arXiv:1507.02428v1 [cs.RO] 9 Jul 2015 \n53. Chengxi Ye, Yezhou Yang, Cornelia Fermüller, Yiannis Aloimonos. What Can I Do Around Here? Deep Functional Scene Understanding \nfor Cognitive Robots. ArXiv:1602.00032 , Feb 2016 \n54. P. Loncomilla, J. Ruiz-del-Solar and L. Martínez, Object Recognition using Local Invariant Features for Robotic Applications: A Survey, \nPattern Recognition, Vol. 60, Dec. 2016, pp. 499-514. \n55. Workshop Geometry Meets Deep Learning, ECCV 2016. Available (July 2016) in https://sites.google.com/site/deepgeometry/ \n56. Workshop \nDeep \nLearning \nfor \nAutonomous \nRobots, \nRSS \n2016. \nAvailable \n(July \n2016) \nin \nhttp://www.umiacs.umd.edu/~yzyang/deeprobotics.html \n57. Workshop Are the Skeptics Right? Limits and Potentials of Deep Learning in Robotics, RSS 2016. Available (July 2016) in \nhttp://juxi.net/workshop/deep-learning-rss-2016/ \n58. Workshop The Future of Real-Time SLAM: Sensors, Processors, Representations, and Algorithms, https://wp.doc.ic.ac.uk/thefutureofslam/ \n59. Keynote talk “Deep Grasping: Can large datasets and reinforcement learning bridge the dexterity gap?”, Ken Goldberg at ICRA 2016 \n60. Caffe deep learning framework. Available (July 2016) in http://caffe.berkeleyvision.org/ \n61. M. Egmont-Petersen, D. de Ridder, H. Handels, Image processing with neural networks—a review, Pattern Recognition, Vol. 35, Issue 10, \nOct. 2002, pp. 2279–2301. \n62. G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Improving neural networks by preventing co-adaptation of \nfeature detectors, arXiv preprint, arXiv: 1207.0580, 2012. \n63. N. Srivastava, G.E. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Dropout: A Simple Way to Prevent Neural Networks from \nOverfitting, Journal of Machine Learning Research 15 (2014) 1929-1958. \n64. L. Wan, M. Zeiler, S. Zhang, Y. LeCun, R. Fergus. Regularization of neural networks using DropConnect, Proc. of the ICML, 2013. \n65. K. Hornik, Approximation capabilities of multilayer feedfoward networks, Neural Networks, Vol. 4, pp. 251-257, 1991. \n66. B.T. Polyak, Some methods of speeding up the convergence of iteration methods, USSR Computational Mathematics and Mathematical \nPhysics 4 (5), pp. 1-17, 1964. \n67. L. Bottou, Large-Scale Machine Learning with Stochastic Gradient Descent, Proc. COM PSTAT 2010. \n68. S. Hochreiter, and J. Schmidhuber. Long short-term memory. Neural Comput. 9, pp. 1735–1780, 1997. \n69. Large \nScale \nVisual \nRecognition \nChallenge \n(ILSVRC) \nOfficial \nwebsite. \nAvailable \n(July \n2016) \nin: \nhttp://www.image-\nnet.org/challenges/LSVRC/ \n70. P. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. PhD thesis, Harvard Univ. (1974). \n71. G.E. Hinton, S. Osindero & Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Comp. 18, 1527–1554 (2006). \n72. Y. Bengi, P. Lamblin, D. Popovici & H. Larochelle. Greedy layer-wise training of deep networks. In Proc. Advances in Neural Information \nProcessing Systems 19, 153–160 (2006). \n73. M. Ranzato, C. Poultney, S. Chopra & Y. LeCun. Efficient learning of sparse representations with an energy-based model. In Proc. \nAdvances in Neural Information Processing Systems 19 1137–1144 (2006). \n \n38 \n74. K Chatfield, K Simonyan, A Vedaldi, A Zisserman. Return of the devil in the details: Delving deep into convolutional nets, in Proc. of the \nBritish Machine Vision Conference 2014. \n75. R. Verschae, J. Ruiz-del-Solar, M. Correa (2008). A Unified Learning Framework for object Detection and Classification using Nested \nCascades of Boosted Classifiers. Machine Vision and Applications, Vol. 19, No. 2, pp. 85-103, 2008. \n76. J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in Neural Information \nProcessing Systems 27 (NIPS ’14), NIPS Foundation, 2014. \n77. Olivier Delalleau and Yoshua Bengio. Shallow vs. Deep Sum-Product Networks. Advances in Neural Information Processing Systems 24. \nPages 666-674, 2012 \n78. M. Bianchini and F. Scarselli. On the Complexity of Neural Network Classifiers: A Comparison Between Shallow and Deep Architectures. \nIn IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 8, pp. 1553-1565, Aug. 2014. \n79. Y.N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, Y. Bengio. Identifying and attacking the saddle point problem in high-\ndimensional non-convex optimization. Advances in neural information processing systems 2014, 2933-2941. \n80. Ian Goodfellow, Yoshua Bengio and Aaron Courville. Deep Learning. Book in preparation for MIT Press. 2016. \nhttp://www.deeplearningbook.org. \n81. G.E. Hinton, & R.R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786), 504-507, year 2006. \n82. P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio & P.A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a \ndeep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec), 3371-3408, year 2010. \n83. F. Yu, V. Koltun. Multi-Scale Context Aggregation by Dilated Convolutions. ICLR 2016. \n84. G. Ghiasi, C. Fowlkes. Laplacian Reconstruction and Refinement for Semantic Segmentation. arXiv:1605.02264. \n85. L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, A.L. Yuille. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, \nAtrous Convolution, and Fully Connected CRFs. arXiv:1606.00915 \n86. G. Lin, C. Shen, A. Van Dan Hengel, I. Reid. Efficient piecewise training of deep structured models for semantic segmentation. IEEE Conf. \nComputer Vision and Pattern Recognition (CVPR) 2016. \n87. M. Jaderberg, A. Vedaldi, A. Zisserman. Speeding up Convolutional Neural Networks with Low Rank Expansions. arXiv:1405.3866 \n[cs.CV] \n88. B. Liu, M. Wang, H. Foroosh, M. Tappen and M. Penksy. Sparse Convolutional Neural Networks, 2015 IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), Boston, MA, 2015, pp. 806-814. \n89. S. Han, H. Mao, W.J. Dally: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman \nCoding (ICLR'16, best paper award) \n90. S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M.A. Horowitz. and W.J. Dally. EIE: Efficient Inference Engine on Compressed Deep Neural \nNetwork. International Conference on Computer Architecture (ISCA), 2016. \n91. M. Rastegari, V. Ordonez, J. Redmon, & A. Farhadi. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. \narXiv preprint arXiv:1603.05279., year 2016. \n92. A. Eitel, J.T. Springenberg, L. Spinello, M. Riedmiller, W. Burgard. Multimodal Deep Learning for Robust RGB-D Object Recognition. \nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Hamburg, Germany, 2015. \n93. D. Ciresan, U. Meier, J. Schmidhuber, Multi-column deep neural networks for image classification, in: Proceedings of the CVPR 2012. \n94. O. Vinyals, A. Toshev, S. Bengio, D. Erhan. Show and tell: A neural image caption generator. Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition 2015. \n95. M. Malinowski, M. Rohrbach and M. Fritz. Ask Your Neurons: A Neural-based Approach to Answering Questions about Images. ICCV \n2015. \n96. Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler. Aligning Books and Movies: Towards Story-Like Visual \nExplanations by Watching Movies and Reading Books. The IEEE International Conference on Computer Vision (ICCV), 2015, pp. 19-27. \n97. Cesar Cadena. Anthony Dick and Ian D. Reid. Multi-modal Auto-Encoders as Joint Estimators for Robotics Scene Understanding. \nProceedings of Robotics: Science and System Proceedings. June 2016 \n98. Bo Li, Tianlei Zhang and Tian Xia. Vehicle Detection from 3D Lidar Using Fully Convolutional Network. Proceedings of Robotics: \nScience and System Proceedings. June 2016 \n99. Pablo F. Alcantarilla, Simon Stent, German Ros, Roberto Arroyo and Riccardo Gherardi. Street-View Change Detection with \nDeconvolutional Networks. Proceedings of Robotics: Science and System Proceedings. June 2016 \n100. Niko Sunderhauf, Sareh Shirazi, Adam Jacobson, Feras Dayoub, Edward Pepperell, Ben Upcroft, and Michael Milford. Place Recognition \nwith ConvNet Landmarks: Viewpoint-Robust, Condition-Robust, Training-Free. Proceedings of Robotics: Science and System Proceedings. \nJuly 2015 \n101. D. Albani, A. Youssef, V. Suriani, D. Nardi, and D.D. Bloisi. A Deep Learning Approach for Object Recognition with NAO Soccer Robots. \nRoboCup International Symposium. July 2016. \n \n39 \n102. Daniel Speck, Pablo Barros, Cornelius Weber and Stefan Wermter. Ball Localization for Robocup Soccer using Convolutional Neural \nNetworks. RoboCup International Symposium. July 2016. \n103. Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine and Pieter Abbeel. Deep Spatial Autoencoders for Visuomotor \nLearning. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n104. Yang Gao, Lisa Anne Hendricks, Katherine J. Kuchenbecker and Trevor Darrell. Deep Learning for Tactile Understanding from Visual and \nHaptic Data. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n105. Farzad Husain, Babette Dellen and Carme Torra. Action Recognition based on Efficient Deep Feature Learning in the Spatio-Temporal \nDomain. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n106. Gabriel L. Oliveira, Abhinav Valada, Claas Bollen, Wolfram Burgard and Thomas Brox. Deep Learning for Human Part Discovery in \nImages. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n107. Hasan F. M. Zaki, Faisal Shafait and Ajmal Mian. Convolutional Hypercube Pyramid for Accurate RGB-D Object Category and Instance \nRecognition.  IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n108. Caio Cesar Teodoro Mendes, Vincent Fremont and Denis Fernando Wolf. Exploiting Fully Convolutional Neural Networks for Fast Road \nDetection. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n109. Lerrel Pinto and Abhinav Gupta. Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours. IEEE International \nConference on Robotics and Automation (ICRA). May 2016. \n110. Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh, and Ashutosh Saxena. Recurrent Neural Networks for Driver Activity Anticipation \nvia Sensory-Fusion Architecture. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n111. Joel Schlosser, Christopher K. Chow, and Zsolt Kira. Fusing LIDAR and Images for Pedestrian Detection using Convolutional Neural \nNetworks. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n112. Gabriele Costante, Michele Mancini, Paolo Valigi and Thomas A. Ciarfuglia. Exploring Representation Learning with CNNs for Frame to \nFrame Ego-Motion Estimation. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n113.  Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi and Yong Liu. Understand Scene Categories by Objects: A Semantic Regularized Scene \nClassifier Using Convolutional Neural Networks.  IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n114. Jeffrey Mahler, Florian T. Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kroger, James \nKuffner and Ken Goldberg. Dex-Net 1.0: A Cloud-Based Network of 3D Objects for Robust Grasp Planning Using a Multi-Armed Bandit \nModel with Correlated Rewards. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n115. Di Guo, Tao Kong, Fuchun Sun and Huaping Liu. Object Discovery and Grasp Detection with a Shared Convolutional Neural Network. \nIEEE International Conference on Robotics and Automation (ICRA). May 2016. \n116. Xiaochuan Yin and Qijun Chen. Deep Metric Learning Autoencoder for Nonlinear Temporal Alignment of Human Motion. IEEE \nInternational Conference on Robotics and Automation (ICRA). May 2016.  \n117. Alessandro Giusti, Jérôme Guzzi Dan C. Ciresan, Fang-Lin He, Juan P. Rodríguez, Flavio Fontana, Matthias Faessler, Christian Forster, \nJürgen Schmidhuber, Gianni Di Caro, Davide Scaramuzza and Luca M. Gambardella. A Machine Learning Approach to Visual Perception \nof Forest Trails for Mobile Robots. IEEE International Conference on Robotics and Automation (ICRA). May 2016.  \n118. David Held, Sebastian Thrun and Silvio Savarese. Robust Single-View Instance Recognition.  IEEE International Conference on Robotics \nand Automation (ICRA). May 2016. \n119. Shichao Yang, Daniel Maturana and Sebastian Scherer. Real-time 3D Scene Layout from a Single Image Using Convolutional Neural \nNetworks.  IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n120.  Peter Uršic, Rok Mandeljc, Aleš Leonardis and Matej Kristan. Part-Based Room Categorization for Household Service Robots.  IEEE \nInternational Conference on Robotics and Automation (ICRA). May 2016. \n121.  Rudy Bunel, Franck Davoine and Philippe Xu. Detection of Pedestrians at Far Distance. IEEE International Conference on Robotics and \nAutomation (ICRA). May 2016.  \n122. Adithyavairavan Murali, Animesh Garg, Sanjay Krishnan, Florian T. Pokorny, Pieter Abbeel, Trevor Darrell and Ken Goldberg. TSC-DL: \nUnsupervised Trajectory Segmentation of Multi-Modal Surgical Demonstrations with Deep Learning. IEEE International Conference on \nRobotics and Automation (ICRA). May 2016. \n123. Alex Kendall and Roberto Cipolla. Modelling Uncertainty in Deep Learning for Camera Relocalization. IEEE International Conference on \nRobotics and Automation (ICRA). May 2016. \n124. Farzad Husain, Hannes Schulz, Babette Dellen, Carme Torras and Sven Behnke. Combining Semantic and Geometric Features for Object \nClass Segmentation of Indoor Scenes. IEEE International Conference on Robotics and Automation (ICRA). May 2016. \n125. Judy Hoffman, Saurabh Gupta, Jian Leong, Sergio Guadarrama and Trevor Darrell. Cross-Modal Adaptation for RGB-D Detection. IEEE \nInternational Conference on Robotics and Automation (ICRA). May 2016. \n126. Niko Sunderhauf, Feras Dayoub, Sean McMahon, Ben Talbot, Ruth Schulz, Peter Corke, Gordon Wyeth, Ben Upcroft, and Michael \nMilford. Place Categorization and Semantic Mapping on a Mobile Robot. IEEE International Conference on Robotics and Automation \n(ICRA). May 2016.  \n \n40 \n127. M. Szarvas, A. Yoshizawa, M. Yamamoto and J. Ogata, \"Pedestrian detection with convolutional neural networks,\" IEEE Proceedings. \nIntelligent Vehicles Symposium, 2005., 2005, pp. 224-229. doi: 10.1109/IVS.2005.1505106 \n128. Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, Jürgen Schmidhuber. LSTM: A Search Space Odyssey. \narXiv:1503.04069. \n129. Robocup Standard Platform League: http://www.robocup.org/robocup-soccer/standard-platform/ \n130. Torch: A scientific computing framework for luaJIT. Official website. Available (July 2016) in: http://torch.ch/ \n131. Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv:1605.02688v1. May \n2016. \n132. James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-\nFarley and Yoshua Bengio. Theano: A CPU and GPU Math Compiler in Python. Proc. Of The 9th Python In Science Conf. (SCIPY 2010). \n133. Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Frédéric \nBastien, and Yoshua Bengio. \"Pylearn2: a machine learning research library\". arXiv preprint arXiv:1308.4214. \n134. Theano Lights. GitHub Repository. Available (July 2016) in: https://github.com/Ivaylo-Popov/Theano-Lights. \n135. Bart van Merriënboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, and Yoshua Bengio, \n\"Blocks and Fuel: Frameworks for deep learning,\" arXiv preprint arXiv:1506.00619 [cs.LG], 2015. \n136. Lasagne Library. GitHub Repository. Available (July 2016) in: https://github.com/Lasagne/Lasagne  \n137. TensorFlow. Official Website. Available (July 2016) in: https://www.tensorflow.org/ \n138. Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. MXNet: \nA Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. In Neural Information Processing Systems, \nWorkshop on Machine Learning Systems, 2015 \n139. Deeplearning4j. Official website. Available (July 2016) in: http://deeplearning4j.org/ \n140. S. Tokui, K. Oono, S. Hido and J. Clayton, Chainer: a Next-Generation Open Source Framework for Deep Learning,Proceedings of \nWorkshop on Machine Learning Systems(LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing \nSystems (NIPS), (2015)  \n141. Amit Agarwal, Eldar Akchurin, Chris Basoglu, Guoguo Chen, Scott Cyphers, Jasha Droppo, Adam Eversole, Brian Guenter, Mark \nHillebrand, T. Ryan Hoens, Xuedong Huang, Zhiheng Huang, Vladimir Ivanov, Alexey Kamenev, Philipp Kranen, Oleksii Kuchaiev, \nWolfgang Manousek, Avner May, Bhaskar Mitra, Olivier Nano, Gaizka Navarro, Alexey Orlov, Hari Parthasarathi, Baolin Peng, Marko \nRadmilac, Alexey Reznichenko, Frank Seide, Michael L. Seltzer, Malcolm Slaney, Andreas Stolcke, Huaming Wang, Yongqiang Wang, \nKaisheng Yao, Dong Yu, Yu Zhang, Geoffrey Zweig (in alphabetical order), \"An Introduction to Computational Networks and the \nComputational Network Toolkit\", Microsoft Technical Report MSR-TR-2014-112, 2014. \n142. Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun. OverFeat: Integrated Recognition, Localization \nand Detection using Convolutional Networks.  arXiv:1312.6229. Feb 2014. \n143. B. C. Ooi, K.-L. Tan, S. Wang, W. Wang, Q. Cai, G. Chen, J. Gao, Z. Luo, A. K. H. Tung, Y. Wang, Z. Xie, M. Zhang, and K. Zheng. \nSINGA: A distributed deep learning platform. ACM Multimedia (Open Source Software Competition) 2015. \n144. W. Wang, G. Chen, T. T. A. Dinh, B. C. Ooi, K.-L.Tan, J. Gao, and S. Wang. SINGA: putting deep learning in the hands of multimedia \nusers. ACM Multimedia 2015. \n145. ConvNetJS: Deep Learning in your browser. Official Website. Available (July 2017) in: http://cs.stanford.edu/people/karpathy/convnetjs/. \n146. Cuda-convnet2. GitHub Repository. Available (July 2016) in: https://github.com/akrizhevsky/cuda-convnet2 \n147. MatConvNet: CNNs for MATLAB. Official Website. Available (July 2016) in: http://www.vlfeat.org/matconvnet/ \n148. Neon: Nervana ’s Python-based deep learning library. Official Website. Available (July 2016) in: http://neon.nervanasys.com/ \n149. Veles: Distributed platform for rapid Deep learning application development. Official Website. Avalaible (July 2016) in:  \nhttps://velesnet.ml/ \n150. R.J. Williams, J. Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural Computation, \nVolume 2, Issue 4, Winter 1990, Pages 490-501 \n151. J. Ngiam, Z. Chen, P.W. Koh , A.Y. Ng. Learning Deep Energy Models. in: Proceedings of the ICML, 2011 \n152. NVIDIA CUDA toolkit documentation: http://docs.nvidia.com/cuda/ \n153. NVIDIA CUDNN GPU Accelerated Deep Learning: https://developer.nvidia.com/cudnn \n154. PASCAL VOC 2012: http://host.robots.ox.ac.uk:8080/leaderboard/main_bootstrap.php \n155. NYU2: http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html \n156. MPII Human Pose Dataset: http://human-pose.mpi-inf.mpg.de/#results \n157. KITTI: http://www.cvlibs.net/datasets/kitti/eval_object.php \n158. MS COCO: http://mscoco.org/dataset/#detections-leaderboard \n159. LabelMe: http://labelme.csail.mit.edu/Release3.0/browserTools/php/dataset.php \n160. Cityscapes: https://www.cityscapes-dataset.com/benchmarks/ \n \n41 \n161. MNIST - CIFAR 10 - CIFAR 100: http://rodrigob.github.io/are_we_there_yet/build/#datasets \n162. SUN dataset: http://vision.princeton.edu/projects/2010/SUN/ \n163. ImageNet: http://www.image-net.org/ \n164. Software links for Deep Learning. Available (July 2017) in: http://deeplearning.net/software_links/ \n165. Datasets for Deep Learning. Available (July 2017) in: http://deeplearning.net/datasets/ \n166. Guosheng Lin, Chunhua Shen, Anton van den Hengel and Ian Reid. Efficient Piecewise Training of Deep Structured Models for Semantic \nSegmentation. arXiv:1504.01013v4 2016 \n167. Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked Hourglass Networks for Human Pose Estimation.. arXiv:1603.06937v1. 2016. \n168. Georgia Gkioxari, Ross Girshick and Jitendra Malik. Contextual Action Recognition with R*CNN. arXiv:1505.01197v3 2016 \n169. Google Translate Application: https://play.google.com/store/apps/details?id=com.google.android.apps.translate \n170. Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, Mohak Shah. Comparative Study of Deep Learning Software Frameworks. \narXiv:1511.06435v3 2016. \n171. MNIST: http://yann.lecun.com/exdb/mnist/ \n172. CIFAR-10 and CIFAR-100: https://www.cs.toronto.edu/~kriz/cifar.html \n173. Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, Rob Fergus. Regularization of Neural Network using DropConnect. International \nConference on Machine Learning 2013 \n174. Benjamin Graham. Fractional Max-Pooling. arXiv:1412.6071v4. 2015 \n175. Djork-Arne Clevert, Thomas Unterthiner and Sepp Hochreiter. Fast and Accurate Deep Network Learning by Exponential Linear Units \n(ELUs). arXiv:1511.07289v5. 2016 \n176. Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Subcategory-aware Convolutional Neural Networks for Object Proposals and \nDetection. arXiv:1604.04693v1 2016 \n177. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015 \n178. Caffe Model Zoo: http://caffe.berkeleyvision.org/model_zoo.html \n179. M. Jaderberg, K. Simonyan, A. Zisserman, K. Kavukcuoglu: Spatial transformer networks. In: NIPS. (2015) \n180. A. Handa, M. Bloesch, V. Patraucean, S. Stent, J. McCormac, A. Davison, gvnn: Neural Network Library for Geometric Computer Vision. \nIn: ECCV 2016. \n181. R. Verschae, J. Ruiz-del-Solar. Object Detection: Current and Future Directions, Frontiers in Robotics and AI, Article 29, Vol. 2, Nov. \n2015. \n182. Y. Bengio. Deep learning of representations for unsupervised and transfer learning,” in ICML Unsupervised and Transfer Learning, Volume \n27 of JMLR Proceedings, eds I. Guyon, G. Dror, V. Lemaire, G. W. Taylor, and D. L. Silver (Bellevue: JMLR.Org), 17–36, year 2012. \n183. D. Kotzias, M. Denil, P. Blunsom,and N. de Freitas. Deep Multi-Instance Transfer Learning. CoRR, abs/1411.3128, year 2014. \n184. M. Mathieu, M. Henaff and Y. LeCun. Fast training of convolutional networks through ffts, arXiv preprint arXiv:1312.5851, 2013 \n185. D.O. Hebb. The Organization of Behavior. New York: Wiley & Sons, 1949 \n186. T. Poggio, F. Girosi. Regularization Algorithms for Learning that are Equivalent to Multilayer Networks. Science, Volume 247, Issue 4945, \npp. 978-982, 1990 \n187. K. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural Networks, Volume 2 Issue 3, 1989. Pages \n183-192 \n188. Jia Deng, Alexander C. Berg, Kai Li, and Li Fei-Fei. 2010. What does classifying more than 10,000 image categories tell us?. In \nProceedings of the 11th European conference on Computer vision: Part V (ECCV'10), Kostas Daniilidis, Petros Maragos, and Nikos \nParagios (Eds.). Springer-Verlag, Berlin, Heidelberg, 71-84 \n189. A. Saxena, H. Pandya, G. Kumar, K. M. Krishna. Exploring Convolutional Networks for End-to-End Visual Servoing. \nhttp://robotics.iiit.ac.in/people/harit.pandya/servonets/ \n190. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, \nAndreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, \nDaan Wierstra, Shane Legg, Demis Hassabis. Human-level Control through Deep Reinforcement Learning. In Nature, 518: 529–533, 2015. \n191. T Lei, L Ming. A robot exploration strategy based on q-learning network. Real-time Computing and Robotics (RCAR), IEEE International \nConference on, 6-10 June 2016. \n192. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi. Target-driven Visual Navigation in \nIndoor Scenes using Deep Reinforcement Learning. arXiv preprint arXiv:1609.05143. \n \n42 \n193. Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent \nSifre, Koray Kavukcuoglu, Dharshan Kumaran, Raia Hadsell. Learning to Navigate in Complex Environments. Under review as a \nconference paper at ICLR 2017. \n194. Alborz Geramifard, Thomas J. Walsh, Stefanie Tellex, Girish Chowdhary, Nicholas Roy, and Jonathan P. How. 2013. A Tutorial on Linear \nFunction Approximators for Dynamic Programming and Reinforcement Learning. Found. Trends Mach. Learn. 6, 4 (December 2013), 375-\n451. DOI=http://dx.doi.org/10.1561/2200000042 \n195. Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando de Freitas. Sample Efficient Actor-\nCritic with Experience Replay. arXiv:1611.01224 [cs.LG]. 2016 \n196. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis \nAntonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, \nTimothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis. Mastering the game of Go with deep neural \nnetworks and tree search. Nature, Vol. 529, No. 7587. (27 January 2016), pp. 484-489, doi:10.1038/nature16961. \n197. GPU-Based \nDeep \nLearning \nInference: \nA \nPerformance \nand \nPower \nAnalysis. \nNVIDIA \nwhitepaper. \nhttps://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf \n198. Nicolás Cruz, Kenzo Lobos-Tsunekawa, Javier Ruiz-del-Solar. Using Convolutional Neural Networks in Robots with Limited \nComputational Resources: Detecting NAO Robots while Playing Soccer. arXiv:1706.06702 (2017) \n199. Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan, Yu-Wing Tai, and Li Xu. Accurate Single Stage Detector \nUsing Recurrent Rolling Convolution. arXiv preprint arXiv:1704.05776 (2017). \n200. Zhaowei Cai, Quanfu Fan, Rogerio S. Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object \ndetection. In European Conference on Computer Vision, pp. 354-370. Springer International Publishing, 2016. \n201. Y. Xiang, W. Choi, Y. Lin and S. Savarese: Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection. IEEE \nWinter Conference on Applications of Computer Vision (WACV) 2017. \n202. Yousong Zhu, Jinqiao Wang, Chaoyang Zhao, Haiyun Guo, and Hanqing Lu. Scale-Adaptive Deconvolutional Regression Network for \nPedestrian Detection. In Asian Conference on Computer Vision, pp. 416-430. Springer, Cham, 2016. \n203. F. Yang, W. Choi and Y. Lin: Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded \nRejection Classifiers. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition 2016. \n204. Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey \nto zoo. In Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pp. 3485-3492. IEEE, 2010. \n205. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene \nrecognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2017). \n206. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, \n2010. \n207. Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proceedings of the \nIEEE conference on computer vision and pattern recognition, pp. 567-576. 2015. \n208. Liang-Chieh, Chen George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image \nsegmentation. arXiv preprint arXiv:1706.05587 (2017). \n209. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. arXiv preprint \narXiv:1612.01105 (2016). \n210. Zifeng, Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet model for visual recognition. arXiv preprint \narXiv:1611.10080 (2016). \n211. Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi Hou, and Garrison Cottrell. Understanding convolution for semantic \nsegmentation. arXiv preprint arXiv:1702.08502 (2017). \n212. Rui Zhang, Sheng Tang, Yongdong Zhang, Jintao Li, and Shuicheng Yan. Scale-Adaptive Convolutions for Scene Parsing. In Proceedings \nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2031-2039. 2017. \n \n43 \n213. Ping Luo, Guangrun Wang, Liang Lin, and Xiaogang Wang. Deep Dual Learning for Semantic Image Segmentation. In Proceedings of the \nIEEE Conference on Computer Vision and Pattern Recognition, pp. 2718-2726. 2017. \n214. Jun Fu, Jing Liu, Yuhang Wang, and Hanqing Lu. Stacked Deconvolutional Network for Semantic Segmentation. arXiv preprint \narXiv:1708.04943 (2017). \n215. Guangrun Wang, Ping Luo, Liang Lin, and Xiaogang Wang. Learning object interactions and descriptions for semantic image \nsegmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5859-5867. 2017. \n216. ILSVRC 2016 scene classification. http://image-net.org/challenges/LSVRC/2016/results \n217. Places Challenge 2016: http://places2.csail.mit.edu/results2016.html \n218. Jifeng Dai, Yi Li, Kaiming He, Jian Sun. R-FCN: Object Detection via Region-based Fully Convolutional Networks. arXiv:1605.06409 \n219. Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan, Yu-Wing Tai, Li Xu. Accurate Single Stage Detector Using \nRecurrent Rolling Convolution. CVPR 2017. \n220. Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, \nSergio Guadarrama, Kevin Murphy. Speed/accuracy trade-offs for modern convolutional object detectors. arXiv:1611.10012. 2017 \n221. Jie Hu, Li Shen, Gang Sun. Squeeze-and-Excitation Networks. arXiv:1709.01507 \n222. Wang F, Jiang M, Qian C, et al. Residual Attention Network for Image Classification. arXiv preprint arXiv:1704.06904, 2017. \n223. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. Inception-v4, Inception-ResNet and the Impact of Residual Connections \non Learning. AAAI. 2017: 4278-4284. \n224. Olaf Ronneberger, Philipp Fischer, Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. arXiv preprint \narXiv:1505.04597, 2015. \n225. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie. Feature pyramid networks for object detection. \narXiv preprint arXiv:1612.03144, 2016. \n226. Abhinav Shrivastava, Rahul Sukthankar, Jitendra Malik, Abhinav Gupta. Beyond skip connections: Top-down modulation for object \ndetection. arXiv preprint arXiv:1612.06851, 2016. \n227. Xingyu Zeng, Wanli Ouyang, Junjie Yan, Hongsheng Li, Tong Xiao, Kun Wang, Yu Liu, Yucong Zhou, Bin Yang, Zhe Wang, Hui Zhou, \nXiaogang Wang. Crafting GBD-Net for Object Detection. arXiv preprint arXiv:1610.02579, 2016. \n228. Yu Chen, Chunhua Shen, Xiu-Shen Wei, Lingqiao Liu, Jian Yang. Adversarial PoseNet: A Structure-aware Convolutional Network for \nHuman Pose Estimation. arXiv:1705.00389 \n229. J. Wang, Z. Liu, Y. Wu, J. Yuan. Learning actionlet ensemble for 3d human action recognition, IEEE Transactions on Pattern Analysis and \nMachine Intelligence, vol. 36, no. 5, pp. 914-927, 2014. \n230. Huiwen Guo, Xinyu Wu, Wei Fengab. Multi-stream deep networks for human action classification with sequential tensor decomposition. \nSignal Processing, Volume 140, November 2017, Pages 198-206 \n231. Y.H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, G. Toderici. Beyond short snippets: deep networks for video \nclassification. IEEE Conference on Computer Vision and Pattern Recognition(CVPR) (2015), pp. 4694-4702 \n232. Z. Wu, Y.G. Jiang, X. Wang, H. Ye, X. Xue. Multi-stream multi-class fusion of deep networks for video classification. ACM on \nMultimedia Conference (2016), pp. 791-800 \n233. G. Lev, G. Sadeh, B. Klein, L. Wolf. RNN fisher vectors for action recognition and image annotation. European Conference on Computer \nVision(ECCV) (2016), pp. 833-850 \n234. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri. Learning Spatiotemporal Features with 3D Convolutional \nNetworks. arXiv:1412.0767 (2014) \n235. Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten. Densely Connected Convolutional Networks. arXiv:1608.06993, \n2016. \n",
  "categories": [
    "cs.CV",
    "68T45"
  ],
  "published": "2018-03-28",
  "updated": "2018-03-28"
}