{
  "id": "http://arxiv.org/abs/1803.10910v1",
  "title": "Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective",
  "authors": [
    "Jing Zhang",
    "Tong Zhang",
    "Yuchao Dai",
    "Mehrtash Harandi",
    "Richard Hartley"
  ],
  "abstract": "The success of current deep saliency detection methods heavily depends on the\navailability of large-scale supervision in the form of per-pixel labeling. Such\nsupervision, while labor-intensive and not always possible, tends to hinder the\ngeneralization ability of the learned models. By contrast, traditional\nhandcrafted features based unsupervised saliency detection methods, even though\nhave been surpassed by the deep supervised methods, are generally\ndataset-independent and could be applied in the wild. This raises a natural\nquestion that \"Is it possible to learn saliency maps without using labeled data\nwhile improving the generalization ability?\". To this end, we present a novel\nperspective to unsupervised saliency detection through learning from multiple\nnoisy labeling generated by \"weak\" and \"noisy\" unsupervised handcrafted\nsaliency methods. Our end-to-end deep learning framework for unsupervised\nsaliency detection consists of a latent saliency prediction module and a noise\nmodeling module that work collaboratively and are optimized jointly. Explicit\nnoise modeling enables us to deal with noisy saliency maps in a probabilistic\nway. Extensive experimental results on various benchmarking datasets show that\nour model not only outperforms all the unsupervised saliency methods with a\nlarge margin but also achieves comparable performance with the recent\nstate-of-the-art supervised deep saliency methods.",
  "text": "Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective\nJing Zhang∗1,2, Tong Zhang∗2,3, Yuchao Dai†1, Mehrtash Harandi2,3, and Richard Hartley2\n1Northwestern Polytechnical University, Xi’an, China\n2Australian National University, Canberra, Australia\n3DATA61,CSIRO, Canberra, Australia\nAbstract\nThe success of current deep saliency detection meth-\nods heavily depends on the availability of large-scale su-\npervision in the form of per-pixel labeling. Such supervi-\nsion, while labor-intensive and not always possible, tends\nto hinder the generalization ability of the learned mod-\nels. By contrast, traditional handcrafted features based un-\nsupervised saliency detection methods, even though have\nbeen surpassed by the deep supervised methods, are gener-\nally dataset-independent and could be applied in the wild.\nThis raises a natural question that “Is it possible to learn\nsaliency maps without using labeled data while improving\nthe generalization ability?”. To this end, we present a novel\nperspective to unsupervised 1 saliency detection through\nlearning from multiple noisy labeling generated by “weak”\nand “noisy” unsupervised handcrafted saliency methods.\nOur end-to-end deep learning framework for unsupervised\nsaliency detection consists of a latent saliency prediction\nmodule and a noise modeling module that work collabo-\nratively and are optimized jointly. Explicit noise modeling\nenables us to deal with noisy saliency maps in a probabilis-\ntic way. Extensive experimental results on various bench-\nmarking datasets show that our model not only outperforms\nall the unsupervised saliency methods with a large margin\nbut also achieves comparable performance with the recent\nstate-of-the-art supervised deep saliency methods.\n1. Introduction\nSaliency detection aims at identifying the visually inter-\nesting objects in images that are consistent with human per-\nception, which is intrinsic to various vision tasks such as\n∗These authors contributed equally in this work.\n†Y. Dai (daiyuchao@nwpu.edu.cn) is the corresponding author.\n1There could be multiple deﬁnitions for unsupervised learning, in this\npaper, we refer unsupervised learning as learning without task-speciﬁc hu-\nman annotations, e.g. dense saliency maps in our task.\nFigure 1. Unsupervised saliency learning from weak “noisy”\nsaliency maps. Given an input image xi and its corresponding\nunsupervised saliency maps yj\ni , our framework learns the latent\nsaliency map ¯yi by jointly optimizing the saliency prediction mod-\nule and the noise modeling module. Compared with SBF [35]\nwhich also learns from unsupervised saliency but with different\nstrategy, our model achieves better performance.\ncontext-aware image editing [36], image caption generation\n[31]. Depending on whether human annotations have been\nused, saliency detection methods can be roughly divided as:\nunsupervised methods and supervised methods. The former\nones compute saliency directly based on various priors (e.g.,\ncenter prior [9], global contrast prior [6], background con-\nnectivity prior [43] and etc.), which are summarized and de-\nscribed with human knowledge. The later ones learn direct\nmapping from color images to saliency maps by exploiting\nthe availability of large-scale human annotated database.\nBuilding upon the powerful learning capacity of convo-\nlutional neural network (CNN), deep supervised saliency\ndetection methods [42, 11, 40] achieve state-of-the-art per-\nformances, outperforming the unsupervised methods by a\nwide margin. The success of these deep saliency methods\nstrongly depend on the availability of large-scale training\ndataset with pixel-level human annotations, which is not\nonly labor-intensive but also could hinder the generaliza-\ntion ability of the learned network models. By contrast, the\nunsupervised saliency methods, even though have been out-\nperformed by the deep supervised methods, are generally\ndataset-independent and could be applied in the wild.\n1\narXiv:1803.10910v1  [cs.CV]  29 Mar 2018\nIn this paper, we present a novel end-to-end deep learn-\ning framework for saliency detection that is free from hu-\nman annotations, thus “unsupervised” (see Fig. 1 for a vi-\nsualization). Our framework is built upon existing efﬁcient\nand effective unsupervised saliency methods and the pow-\nerful capacity of deep neural network. The unsupervised\nsaliency methods are formulated with human knowledge\nand different unsupervised saliency methods exploit differ-\nent human designed priors for saliency detection.\nThey\nare noisy (compared with ground truth human annotations)\nand could have method-speciﬁc bias in predicting saliency\nmaps. By utilizing existing unsupervised saliency maps, we\nare able to remove the need of labor-intensive human anno-\ntations, also by jointly learn different priors from multiple\nunsupervised saliency methods, we are able to get comple-\nmentary information of those unsupervised saliency.\nTo effectively leverage these noisy but informative\nsaliency maps, we propose a novel perspective to the prob-\nlem: Instead of removing the noise in saliency labeling from\nunsupervised saliency methods with different fusion strate-\ngies [35], we explicitly model the noise in saliency maps.\nAs illustrated in Fig. 2, our framework consists of two\nconsecutive modules, namely a saliency prediction mod-\nule that learns the mapping from a color image to the “la-\ntent” saliency map based on current noise estimation and the\nnoisy saliency maps, and a noise modeling module that ﬁts\nthe noise in noisy saliency maps and updates the noise esti-\nmation in different saliency maps based on updated saliency\nprediction and the noisy saliency maps. In this way, our\nmethod takes advantages of both probabilistic methods and\ndeterministic methods, where the latent saliency prediction\nmodule works in a deterministic way while the noise model-\ning module ﬁts the noise distribution in a probabilistic man-\nner. Experiments suggest that our strategy is very effective\nand it only takes several rounds 2 till convergence.\nTo the best of our knowledge, the idea of considering\nunsupervised saliency maps as learning from multiple noisy\nlabels is brand new and different from existing unsupervised\ndeep saliency methods (e.g., [35]). Our main contributions\ncan be summarized as:\n1) We present a novel perspective to unsupervised deep\nsaliency detection, and learn saliency maps from mul-\ntiple noisy unsupervised saliency methods. We formu-\nlate the problem as joint optimization of a latent saliency\nprediction module and a noise modeling module.\n2) Our deep saliency model is trained in an end-to-end\nmanner without using any human annotations, leading\nto an extremely cheap solution.\n3) Extensive performance evaluation on seven benchmark-\ning datasets show that our framework outperforms ex-\n2In our paper, an epoch means a complete pass through all the training\ndata, an iteration means a complete pass through a batch, and a round\nmeans an update on noise module.\nisting unsupervised methods with a wide margin while\nachieving comparable results with state-of-the-art deep\nsupervised saliency detection methods [11, 40].\n2. Related Work\nDepending on whether human annotations are used or\nnot, saliency detection techniques can be roughly grouped\nas unsupervised and supervised methods. Deep learning\nbased methods are particular examples of the latter one. We\nwill also discuss learning with multiple noisy labels.\n2.1. Unsupervised Saliency Detection\nPrior to the deep learning revolution, saliency methods\nmainly relied on different priors and handcrafted features\n[43, 7, 6, 9]. We refer interested readers to [2] and [3] for\nsurveys and benchmark comparisons. Color contrast prior\nhas been exploited at superpixel level in [6]. Shen and Wu\n[27] formulated saliency detection as a low-rank matrix de-\ncomposition problem by exploiting the sparsity prior for\nsalient objects. Objectness, which highlights the object-like\nregions, has also been used in [15] to mark the regions that\nhave higher possibilities of being an object. Zhu et al. [43]\npresented a robust background measure, namely “bound-\nary connectivity” along with an optimization framework to\nmeasure backgroundness of each superpixel. Building upon\nthe center prior, [9] detects the image regions that represent\nthe scene, especially those that are near image center.\n2.2. Supervised Saliency Detection\nConventional supervised techniques, such as [14, 17],\nformulate saliency detection as a regression problem, and a\nclassiﬁer is trained to assign saliency at pixel or superpixel\nlevel. Recently, deep neural networks have been adopted\nsuccessfully for saliency detection [40, 26, 41, 29, 11, 22,\n42, 19, 28, 20, 38, 39, 37]. Deep networks can encode high-\nlevel semantic features and hence capture saliency more ef-\nfectively than both unsupervised saliency methods and non-\ndeep supervised methods. Deep saliency detection meth-\nods generally train a deep neural network to assign saliency\nto each pixel or superpixel. Li and Yu [19] used learned\nfeatures from an existing CNN model to replace the hand-\ncrafted features. Recently, Cheng et al. [11] proposed a\ndeep supervised framework with multi-branch short con-\nnections embed both high- and low-level features for ac-\ncurate saliency detection. With the same purpose, a multi-\nlevel deep feature aggregation framework is proposed in\n[40]. A top-down strategy and a loss function which pe-\nnalizes errors on the edge is presented in [26].\n2.3. Learning with Noisy Labels\nThough deep techniques are methods of choice in\nsaliency detection, very few studies have explicitly ad-\ndressed the problem of saliency learning with unreliable\n2\nFigure 2.\nConceptual illustration of our saliency detection framework, which consists of a “latent” saliency prediction module and a\nnoise modeling module. Given an input image, noisy saliency maps are generated by handcrafted feature based unsupervised saliency\ndetection methods. Our framework jointly optimizes both modules under a uniﬁed loss function. The saliency prediction module targets at\nlearning latent saliency maps based on current noise estimation and the noisy saliency maps. The noise modeling module updates the noise\nestimation in different saliency maps based on updated saliency prediction and the noisy saliency maps. In our experiments, the overall\noptimization converges in several rounds.\nand noisy labels [35]. Learning with noisy labels is mainly\nabout learning classiﬁcation models in the presence of inac-\ncurate class labels. Whitehill et al. [30] solved the problem\nof picking the correct label based on the labels provided\nby many labelers with different expertise. Jindal et al. [16]\nproposed a dropout-regularized noise model by augmenting\nexisting deep network with a noise model that accounts for\nlabel noise. Yao et al. [34] proposed a quality embedding\nmodel to infer the trustworthiness of noisy labels. Different\nfrom the above supervised learning with noisy labels meth-\nods, Lu et al. [25] proposed a weakly supervised semantic\nsegmentation framework to deal with noisy labels.\nTo the best of our knowledge, [35] is the ﬁrst and only\ndeep method that learns saliency without human anno-\ntations, where saliency maps from unsupervised saliency\nmethods are fused with manually designed rules in combin-\ning “intra-image” fusion stream and “inter-image” fusion\nstream to generate the learning curriculum. The method\niteratively replaces inter-image saliency map of low relia-\nbility with its corresponding saliency map. Their recursive\noptimization depends on dedicated design and is computa-\ntionally expensive. Different from [35], we formulate unsu-\npervised saliency learning as the joint optimization of latent\nsaliency and noise modeling. Our method is not only sim-\npler and easier to implement, but also outperforms [35] and\nexisting unsupervised saliency methods. Furthermore, our\nmethod produces competitive performances as compared to\nthe most recent deep supervised saliency detection methods.\n3. Our Framework\nTargeting at achieving deep saliency detection without\nhuman annotations, we propose an end-to-end noise model\nintegrated deep framework, which builds upon existing efﬁ-\ncient and effective unsupervised saliency detection methods\nand the powerful capacity of deep neural networks.\nGiven a color image xi, we would like to learn a bet-\nter saliency map from its M noisy saliency maps yj\ni , j =\n1, · · · , M using different unsupervised saliency methods\n[32, 13, 21, 43]. A trivial and direct solution would be us-\ning the noisy saliency maps as “proxy” human annotations\nand train a deep model with these noisy saliency maps as\nsupervision. However, it is well-known that the network\ntraining is highly prone to the noise in supervision signals.\nA simple fusion of the multiple labels (training with averag-\ning, treating as multiple labels) will also not work due to the\nstrong inconsistency between labels. While there could be\nmany other potentials in utilizing the noisy saliency maps,\nthey are all based on human-designed pipelines, thus cannot\neffectively exploit the best manner. Instead, we propose a\nprincipled way to infer the saliency maps from using multi-\nple noisy labels and simultaneously estimate the noise.\n3.1. Joint Saliency Prediction and Noise Modeling\nBy contrast to existing manually designed procedures\nand deep learning based pipeline [35], we propose a new\nperspective toward the problem of learning from unsuper-\nvised saliency. As illustrated in Fig. 2, our framework con-\nsists of two consecutive modules, namely a saliency pre-\ndiction module that learns the mapping from a color image\nto the “latent” saliency map, and a noise modeling module\nthat ﬁts the noise. These two modules work collaboratively\ntoward ﬁtting the noisy saliency maps. By explicitly mod-\neling noise, we are able to train a deep saliency prediction\nmodel without any human annotations and thus achieve un-\nsupervised deep saliency detection.\n3\n3.2. Loss Function\nWe start with a set of training images, denoted as X =\n{xi, i = 1, . . . , N} and a set of M different saliency maps\nof these images, denoted as Y = {yj\ni , i = 1, . . . , N; j =\n1, . . . , M}, where N is number of training images. These\nare precomputed by applying M different handcrafted “la-\nbellers”. Throughout this discussion, i indexes the training\nimage and j indexes the handcrafted labeller. We propose\na neural network with parameter Θ for saliency detection,\nwhich computes a saliency map ¯yi = f(xi, Θ) of each im-\nage. Our idea is to model each of the handcrafted labellers\nas the sum of ¯yi plus noise: yj\ni = ¯yi + nj\ni, where nj\ni is a\nsample chosen from some probability (“noise”) distribution\nqi, which is to be estimated. For simplicity in this work,\nit is assumed that the distribution q depends on xi, and not\non the labeller j3. We assume a simple model for the noise\ndistributions qi, namely that it a zero-mean Gaussian, inde-\npendent for each pixel of each image xi. Thus, the total\ndistribution q = {q1, q2, . . . , qN} is assumed independent\nfor all i and pixel (m, n), and is parametrized by a param-\neter set Σ = {σi\nmn}, where i indexes the training image\nand (m, n) are pixel coordinates. Sometimes, distribution\nq will be denoted as q(Σ) to emphasize the role of the pa-\nrameters Σ. With this simple parameterization it is easy to\ngenerate noise samples nj\ni for any i and j.\nGiven Θ, Σ, and an input image xi, one generates\nsaliency map ˆyj\ni according to:\nˆyj\ni = f(xi; Θ) + nj\ni = ¯yi + nj\ni,\n(1)\nwhere each nj\ni is a sample drawn from distribution qi(Σ). In\nthe training process, the parameters Θ of the network and Σ\nof the noise model are updated to minimize an appropriate\nloss function. The loss function has two parts:\nL(Θ, Σ) = Lpred(Θ, Σ) + λLnoise(Θ, Σ),\n(2)\nwhere λ is the regularizer to balance these two terms. Un-\nder our optimization framework, increasing the variance in\nnoise modeling will make the prediction loss Lpred large\nand decrease the Lnoise.\nMeanwhile, keeping the vari-\nance lower will decrease the cross-entropy loss Lpred but\nincrease Lnoise. Thus our model balances between these\ntwo losses and converges to the state minimizing the overall\nloss. These two losses are described below:\nSaliency Prediction:\nFor the latent saliency prediction\nmodule, we use a fully convolutional neural network (FCN)\ndue to its superior capability in feature learning and fea-\nture representation. We use the conventional cross-entropy\nloss and compute the loss function element-wisely across\nthe whole training images.\n3Assuming that distribution q is also dependent on the labeller j was\nobserved not to improve results\nThe predictive loss LPred is designed to measure the\nagreement of the predicted labellings ˆyj\ni with handcrafted\nlabellings yj\ni . Cross-entropy loss is used for this purpose,\nand the cross-entropy loss between modeled value ˆy and\n“ground truth” value y (noisy label) is given by:\nLCE = −(y log(ˆy) + (1 −y) log(1 −ˆy)).\n(3)\nThis is applied to all pixel (m, n), all labellers j and all the\ntest images xi to give the total prediction loss.\nLpred(Θ, Σ) =\nN\nX\ni=1\nM\nX\nj=1\nX\nm,n\nLCE(yj\ni,mn, ˆyj\ni,mn),\n(4)\nwhere ˆyj\ni,mn is our noisy saliency map prediction at pixel\n(m, n) which can be easily computed by (1) element-wise,\nand ˆyj\ni,mn is truncated to lie in the range of [0, 1].\nNoise Modeling\nTo effectively handle noisy saliency\nmaps from different unsupervised saliency map labelers, we\nbuild a probabilistic model to approximate the noise, and\nconnect it with our deterministic part (latent saliency pre-\ndiction model as shown in Fig. 2). In this way, our entire\nmodel can be trained in an end-to-end manner to minimize\nthe overall loss function Eq. (2).\nThe noise loss Lnoise measures (for each training image\nxi) the agreement of the noise distribution qi(Σ) with the\nempirical variance of the measurements yj\ni with respect to\nthe output ¯yi = f(xi; Θ) of the network. More precisely,\ngiven an input xi, deﬁne ˆnj\ni = yj\ni −¯yi, the empirical error\nof each yj\ni with respect to the network prediction. For each\npixel location (m, n), this provides M samples from a zero-\nmean Gaussian probability distribution pi, and its variance\non every pixel can be written as ˆσi,mn. The complete set of\nparameters for pi is denoted as ˆΣ = {ˆσi,mn}.\nSince it is intractable to estimate the true posterior distri-\nbution of ˆnj\ni, thus we propose to approximate it by sequen-\ntially optimizing the parameters of prior. We assume that\nthe noise is generated by some random process, involving\nan unobserved continuous random variable set Σ. From an\nencoder perspective, the unobserved variable n can be in-\nterpreted as a latent representation. Here, we model ˆyj\ni as a\nprobabilistic encoder, since given an image xi and network\nparameters Θ it produces a distribution (e.g. a Gaussian)\nover possible values of the code n. The process consists of\ntwo steps: (1) a noise map ni is generated from some prior\ndistribution q(Σ∗); (2) a noise map ˆnj\ni is produced and es-\ntimating the corresponding parameter ˆσi\nThe corresponding noise loss is deﬁned to be the KL di-\nvergence between distribution pi and qi.\nLnoise(Θ, Σ) =\nN\nX\ni\nKL(q(Σi)∥p( ˆΣi)).\n(5)\n4\nSince we employ the Gaussian distribution as the prior\ndistribution for our noise model, the KL divergence has a\nclosed-form solution as:\nKL(q(σ)∥p(ˆσ)) = log(ˆσ/σ) + σ2 + (µ −ˆµ)2\n2ˆσ2\n−1\n2, (6)\nBased on this equation, we can update σ2\ni for every coordi-\nnate (m, n) as\n(σt+1\ni\n)2 = (σt\ni)2 + α((ˆσt\ni)2 −(σt\ni)2),\n(7)\nby differentiating Eq. (6) with respect to σ2\ni,mn, where α is\nthe step size, and we set α = 0.01 in this paper.\nFor different images we have the corresponding noise\nmaps, which follows i.i.d. Gaussian distributions with dif-\nferent variance. Thus, it is hard to converge if simultane-\nously optimizing the FCN parameters Θ and noise param-\neters Σ. In order to train the whole network smoothly, we\nupdate the parameters of noise module after the prediction\nloss converges. Noise maps of a given image are sampled\nfrom the same distribution in a round, but they are updated\nin every round. At the ﬁrst round, we initialize noise vari-\nance to be zero, and train the FCN until it converges. Based\non the variance of the saliency prediction and noisy labels,\nwe then update the noise variance for each image and retrain\nthe network. Through minimizing the loss function Eq. (2)\nwith this procedure, We can train the network and estimate\nthe corresponding noise maps.\n3.3. Deep Noise Model based Saliency Detector\nNetwork Architecture We build our latent saliency pre-\ndiction module upon the DeepLab network [4], where a\ndeep CNN (ResNet-101 [10] in particular) originally de-\nsigned for image classiﬁcation is re-purposed by 1) trans-\nforming all fully connected layers to convolutional layers\nand 2) increasing feature resolution through dilated convo-\nlution [4]. Figure 2 shows the whole structure of our frame-\nwork. Speciﬁcally, our model takes a rescaled image xi of\n425 × 425 as input. For training, the noise model is used to\niteratively update saliency prediction ˆyj\ni , and it’s excluded\nin testing stage, where the latent saliency prediction output\n¯yi in Fig. 2 is our predicted saliency map.\nImplementation details: We trained our model using\nCaffe [12] with maximum epoch of 20. We initialized our\nmodel by using the Deep Residual Model trained for image\nclassiﬁcation [10]. We used the stochastic gradient descent\nmethod with momentum 0.9 and decreased learning rate\n90% when the training loss did not decrease. Base learn-\ning rate is initialized as 1e-3 with the “poly” decay policy\n[12]. For validation, we set “test iter” as 500 (test batch\nsize 1) to cover the full 500 validation images. The train-\ning took 4 hours for one round with training batch size 1\nand “iter size” 20 on a PC with an NVIDIA Quadro M4000\nGPU.\n4. Experimental Results\nIn this section, we report experimental results on various\nsaliency detection benchmarking datasets.\n4.1. Setup\nDataset: We evaluated performance of our proposed\nmodel on 7 saliency benchmarking datasets. 3,000 images\nfrom the MSRA-B dataset[24] are used to get the noisy la-\nbels (where 2,500 images for training and 500 images for\nvalidation) and the remaining 2,000 images are kept for\ntesting. Most of the images in MSRA-B dataset only have\none salient object. The ECSSD dataset [32] contains 1,000\nimages of semantically meaningful but structurally com-\nplex images. The DUT dataset [33] contains 5,168 images.\nThe SOD saliency dataset [14] contains 300 images, where\nmany images contain multiple salient objects with low con-\ntrast. The SED2 [1] dataset contains 100 images with each\nimage contains two salient objects. The PASCAL-S [23]\ndataset is generated from the PASCAL VOC [8] dataset\nand contains 850 images. The THUR dataset [5] contains\n6,232 images of ﬁve classes, namely “butterﬂy”,“coffee\nmug”,“dog jump”,“giraffe” and “plane”.\nUnsupervised Saliency Methods: In this paper, we\nlearn unsupervised saliency from existing unsupervised\nsaliency detection methods. In our experiment, we choose\nRBD [43], DSR [21], MC [13] and HS [32] due to their\neffectiveness and efﬁciency as illustrated in [3].\nCompeting methods: We compared our method against\n10 state-of-the-art deep saliency detection methods (with\nclean labels): DSS [11], NLDF [26], Amulet [40], UCF\n[41], SRM [35], DMT [22], RFCN [28], DeepMC [42],\nMDF [19] and DC [20], 5 conventional handcrafted feature\nbased saliency detection methods: DRFI [14], RBD [43],\nDSR [21], MC [13], and HS [32], which were proven in\n[3] as the state-of-the-art methods before the deep learning\nrevolution, and the very recent unsupervised deep saliency\ndetection method SBF [35].\nEvaluation metrics: We use 3 evaluation metrics, in-\ncluding the mean absolute error (MAE), F-measure, as well\nas the Precision-Recall (PR) curve. MAE can provide a bet-\nter estimate of the dissimilarity between the estimated and\nground truth saliency map. It is the average per-pixel differ-\nence between the ground truth and the estimated saliency\nmap, normalized to [0, 1], which is deﬁned as:\nMAE =\n1\nW × H\nW\nX\nx=1\nH\nX\ny=1\n|S(x, y) −GT(x, y)|,\n(8)\nwhere W and H are the width and height of the respective\nsaliency map S, GT is the ground truth saliency map.\nThe F-measure (Fβ) is deﬁned as the weighted harmonic\n5\nTable 1. Performance of mean F-measure (Fβ) and MAE for different methods including ours on seven benchmark datasets.\nMSRA-B\nECSSD\nDUT\nSED2\nPASCALS\nTHUR\nSOD\nMethods\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nBL1\n.7905\n.0936\n.7205\n.1444\n.5825\n.1369\n.7773\n.1112\n.6714\n.2206\n.5953\n.1339\n.6306\n.1870\nBL2\n.6909\n.1710\n.6542\n.2170\n.4552\n.2951\n.7232\n.1406\n.6776\n.2409\n.5119\n.2545\n.5928\n.2566\nBL3\n.8879\n.0587\n.8717\n.0772\n.7253\n.0772\n.8520\n.0819\n.8264\n.1525\n.7368\n.0749\n.7922\n.1231\nOURS\n.8770\n.0560\n.8783\n.0704\n.7156\n.0860\n.8380\n.0881\n.8422\n.1391\n.7322\n.0811\n.7976\n.1182\nTable 2. Performance of mean F-measure (Fβ) and MAE for different methods including ours on seven benchmark datasets (Best ones in\nbold). From DSS to DC are deep learning based supervised methods, from DRFI to HS are the handcrafted feature based unsupervised\nmethods, SBF and OURS are deep learning based unsupervised saliency detection methods.\nMSRA-B\nECSSD\nDUT\nSED2\nPASCALS\nTHUR\nSOD\nMethods\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nFβ\nMAE\nDSS [11]\n.8941\n.0474\n.8796\n.0699\n.7290\n.0760\n.8236\n.1014\n.8243\n.1546\n.7081\n.1142\n.8048\n.1118\nNLDF [26]\n.8970\n.0478\n.8908\n.0655\n.7360\n.0796\n-\n-\n.8391\n.1454\n-\n-\n.8235\n.1030\nAmulet [40]\n-\n-\n.8825\n.0607\n.6932\n.0976\n.8745\n.0629\n.8371\n.1292\n.7115\n.0937\n.7729\n.1248\nUCF [41]\n-\n-\n.8521\n.0797\n.6595\n.1321\n.8444\n.0742\n.8060\n.1492\n.6920\n.1119\n.7429\n.1527\nSRM [29]\n.8506\n.0665\n.8260\n.0922\n.6722\n.0846\n.7447\n.1164\n.7766\n.1696\n.6894\n.0871\n.7246\n.1369\nDMT [22]\n-\n-\n.7589\n.1601\n.6045\n.0758\n.7778\n.1074\n.6657\n.2103\n.6254\n.0854\n.6978\n.1503\nRFCN [28]\n-\n-\n.8426\n.0973\n.6918\n.0945\n.7616\n.1140\n.8064\n.1662\n.7062\n.1003\n.7531\n.1394\nDeepMC [42]\n.8966\n.0491\n.8061\n.1019\n.6715\n.0885\n.7660\n.1162\n.7327\n.1928\n.6549\n.1025\n.6862\n.1557\nMDF [19]\n.7780\n.1040\n.8097\n.1081\n.6768\n.0916\n.7658\n.1171\n.7425\n.2069\n.6670\n.1029\n.6377\n.1669\nDC [20]\n.8973\n.0467\n.8315\n.0906\n.6902\n.0971\n.7840\n.1014\n.7861\n.1614\n.6940\n.0959\n.7603\n.1208\nDRFI [14]\n.7282\n.1229\n.6440\n.1719\n.5525\n.1496\n.7252\n.1373\n.5745\n.2556\n.5613\n.1471\n.5440\n.2046\nRBD [43]\n.7508\n.1171\n.6518\n.1832\n.5100\n.2011\n.7939\n.1096\n.6581\n.2418\n.5221\n.1936\n.5927\n.2181\nDSR [21]\n.7227\n.1207\n.6387\n.1742\n.5583\n.1374\n.7053\n.1452\n.5785\n.2600\n.5498\n.1408\n.5500\n.2133\nMC [13]\n.7165\n.1441\n.6114\n.2037\n.5289\n.1863\n.6619\n.1848\n.5742\n.2719\n.5149\n.1838\n.5332\n.2435\nHS [44]\n.7129\n.1609\n.6234\n.2283\n.5205\n.2274\n.7168\n.1869\n.5948\n.2860\n.5157\n.2178\n.5383\n.2729\nSBF [35]\n-\n-\n.7870\n.0850\n.5830\n.1350\n-\n-\n.7780\n.1669\n-\n-\n.6760\n.1400\nOURS\n.8770\n.0560\n.8783\n.0704\n.7156\n.0860\n.8380\n.0881\n.8422\n.1391\n.7322\n.0811\n.7976\n.1182\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nDUT\nSBF\nSRM\nAmulet\nUCF\nNLDF\nDSS\nDMT\nRFCN\nDeepMC\nMDF\nDC\nDRFI\nRBD\nDSR\nMC\nHS\nOURS\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nECSSD\nSBF\nSRM\nAmulet\nUCF\nNLDF\nDSS\nDMT\nRFCN\nDeepMC\nMDF\nDC\nDRFI\nRBD\nDSR\nMC\nHS\nOURS\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPASCAL-S\nSBF\nSRM\nAmulet\nUCF\nNLDF\nDSS\nDMT\nRFCN\nDeepMC\nMDF\nDC\nDRFI\nRBD\nDSR\nMC\nHS\nOURS\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSOD\nSRM\nAmulet\nUCF\nNLDF\nDSS\nDMT\nRFCN\nDeepMC\nMDF\nDC\nDRFI\nRBD\nDSR\nMC\nHS\nOURS\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nMSRA-B\nSRM\nNLDF\nDSS\nDMT\nDeepMC\nMDF\nDC\nDRFI\nRBD\nDSR\nMC\nHS\nOURS\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTHUR\nSRM\nAmulet\nUCF\nDSS\nRFCN\nDeepMC\nMDF\nDC\nDRFI\nRBD\nDSR\nMC\nHS\nOURS\nFigure 3. PR curves on six benchmark datasets (DUT, ECSSD, PASCAL-S, SOD, MSRA-B, THUR). Best Viewed on Screen.\nmean of precision and recall:\nFβ = (1 + β2) Precision × Recall\nβ2Precision + Recall,\n(9)\nwhere β2 = 0.3, Precision corresponds to the percentage\nof salient pixels being correctly detected, Recall is the frac-\ntion of detected salient pixels in relation to the ground truth\n6\nnumber of salient pixels. The PR curves are obtained by\nthresholding the saliency map in the range of [0, 255].\n4.2. Baseline Experiments\nAs there could be different ways to utilize the multiple\nnoisy saliency maps, and for fair comparisons with straight-\nforward solutions for our task, we run the following three\nbaseline methods and the results are reported in Table 1.\nBaseline 1 using noisy unsupervised saliency pseudo\nground truth: For a given input image xi and its M hand-\ncrafted feature based saliency map yj\ni , j = 1, ..., M, we\nget M image pairs with noisy label {xi,yj\ni , j = 1, ..., M).\nThen we train a deep model [10] based on those noisy labels\ndirectly, and the results are shown as “BL1” in Table 1.\nBaseline 2: using averaged unsupervised saliency as\npseudo ground truth: Instead of using all the four unsuper-\nvised saliency as ground truth, we use the averaged saliency\nmap of those unsupervised saliency as pseudo ground truth,\nand trained another baseline model “BL2” in Table 1.\nBaseline 3: supervised learning with ground truth\nsupervision:\nOur proposed framework consists of the\nsaliency prediction module and the noise modeling module\nto effectively leverage the noisy saliency maps. To illustrate\nthe best performance our model can achieve as well as to\nprovide a baseline comparison for our framework, we train\nour latent saliency module directly with clean labels, which\nnaturally gives an upper bound of the saliency detection per-\nformance. The results “BL3” are reported in Table 1.\nAnalysis: In Table 1, we compare our unsupervised\nsaliency method with the above baseline conﬁgurations.\nOur method clearly outperforms both BL1 and BL2 with\na wide margin, demonstrating the superiority of our end-\nto-end learning framework. As illustrated in Table 1, the\nperformance of BL1 is better than the performance of BL2.\nThis is because: 1) For BL1, we have 12,000 training im-\nage pairs (four unsupervised saliency methods), while for\nBL2, we have 3,000 averaged noisy labels; 2) as those un-\nsupervised saliency methods tend to prefer different priors\nfor saliency detection, and their saliency maps can be com-\nplementary or controversial to some extent.\nSimply av-\neraging those saliency maps results in even worse proxy\nsaliency map supervision. Compared with BL3, which is\ntrained with ground truth clean labels and without noise,\nour unsupervised method achieves highly comparable re-\nsults. This demonstrates that by jointly learning the latent\nsaliency maps and modeling the noise in a uniﬁed frame-\nwork, we are able to learn the desired reliable saliency maps\neven without any human annotations.\n4.3. Comparison with the State-of-the-art\nQuantitative Comparison We compared our method\nwith eleven most resent deep saliency methods and ﬁve\nconventional methods.\nResults are reported in Table 2\nand Fig. 3, where “OURS” represents the results of our\nmodel.\nTable 2 shows that on those seven benchmark\ndatasets, deep supervised methods signiﬁcantly outperform\ntraditional methods with 2%-12% decrease in MAE, which\nfurther proves the superiority of deep saliency detection.\nMSRA-B is a relatively simple dataset, where most\nsalient objects dominate the whole image. The most re-\ncent deep supervised saliency methods [11] [26] [40] can\nachieve the highest mean F-measure of 0.8970, and our un-\nsupervised method without human annotations can achieve\na mean F-measure of 0.8770, which is only a slight worse.\nThe DUT dataset has more than 25% of images with\nsaliency occupation less than 4%. Small salient object de-\ntection is quite challenging which increase the difﬁculty of\nthis dataset. We achieve the third highest mean F-measure\ncompared with all the competing methods.\nThe THUR\ndataset is the largest dataset we used in this paper, and most\nof the images have complex background. The state-of-the-\nart competing method achieves a mean F-measure/MAE as\n0.7115/0.0854, while our method achieves the best mean F-\nmeasure and MAE as 0.7322/0.0811. SBF [35] uses inter-\nand intra-image conﬁdence map as pseudo ground truth to\ntrain an unsupervised deep model based on unsupervised\nsaliency, which is quite different from our formulation of\npredicting saliency from unsupervised saliency as learn-\ning from noisy labels. Table 2 shows that our framework\nleads to better performance, with 10% mean F-measure im-\nprovement and 3% decrease of MAE on average. Fig. 3\nshows comparison between PR curves of our method and\nthe competing methods on four benchmarking datasets. For\nthe PASCAL-S and THUR dataset, our method ranks al-\nmost the 1st, and for the other three datasets, our method\nachieves competitive performance compared with the com-\npeting deep supervised methods. These experiments alto-\ngether proves the effectiveness our proposed unsupervised\nsaliency detection framework.\nQualitative Comparison Figure 4 demonstrates several\nvisual comparisons, where our method consistently outper-\nforms the competing methods, especially those four unsu-\npervised saliency we used to train our model. The ﬁrst im-\nage is a simple scenario, and most of the competing meth-\nods can achieve good results, while our method achieves the\nbest result with most of the background region suppressed.\nBackground of the third image is very complex, and all the\ncompeting methods fail to detect salient object. With proper\nnoisy labels, we achieve the best results compared with both\nunsupervised saliency methods and deep saliency methods.\nThe fourth image is in very low-contrast, where most of the\ncompeting methods failed to capture the whole salient ob-\njects with the last penguin mis-detected, especially for those\nunsupervised saliency methods. Our method captures all\nthe three penguins properly. The salient objects in the last\nrow are quite small, and the competing methods failed to\n7\nFigure 4. Visual comparison between our method and other competing methods.\ncapture salient regions, while our method capture the whole\nsalient region with high precision.\nAblation Studies: In this paper, we propose to iter-\natively update the noise modeling module and the latent\nsaliency prediction model to achieve accurate saliency de-\ntection. As the two modules work collaboratively to opti-\nmize the overall loss function, it is interesting to see how\nthe saliency prediction results evolves with respect to the\nincrease of updating round. In Fig. 5, we illustrate both the\nperformance metric (MAE) with respect to updating round\nand an example saliency detection results. Starting with the\nzero noise initialization, our method consistently improves\nthe performance of saliency detection with the updating of\nnoise modeling. Also, only after several updating rounds,\nour method convergences to desired state as shown in Fig. 5.\n5. Conclusions\nIn this paper, we propose an end-to-end saliency learning\nframework without the need of human annotated saliency\nmaps in network training.\nWe represent unsupervised\nsaliency learning as learning from multiple noisy saliency\nmaps generated by various efﬁcient and effective con-\nventional unsupervised saliency detection methods.\nOur\nframework consists of a latent saliency prediction mod-\nule and an explicit noise modeling models, which work\ncollaboratively. Extensive experimental results on various\nbenchmarking datasets prove the superiority of our method,\nwhich not only outperforms traditional unsupervised meth-\nods with a wide margin but also achieves highly comparable\nperformance with current state-of-the-art deep supervised\nsaliency detection methods. In the future, we plan to in-\nvestigate the challenging scenarios of multiple saliency ob-\nject detection and small salient object detection under our\nMSRA-B ECSSD\nDUT\nSED2\nPASCAL\nTHUR\nSOD\n0\n0.05\n0.1\n0.15\n0.2\nMean Absolute Error\n(a) MAE of each round on 7 datasets\n(b) Input\n(c) GT\n(d) 1st\n(e) 2nd\n(f) 3rd\n(g) 4th\nFigure 5. Performance of each round. Top: MAE of each dataset.\nBottom: an example image, ground-truth and intermedia results\ngenerated by each updating round.\nframework. Extending our framework to dense prediction\ntasks such as semantic segmentation [25] and monocular\ndepth estimation [18] could be interesting directions.\nAcknowledgement. J. Zhang would like to thank Prof. Mingyi\nHe for his immeasurable support and encouragement. T. Zhang\nwas supported by the Australian Research Council (ARC) Discov-\nery Projects funding scheme (project DP150104645). Y. Dai was\nsupported in part by National 1000 Young Talents Plan of China,\nNatural Science Foundation of China (61420106007, 61671387),\nand ARC grant (DE140100180).\n8\nReferences\n[1] S. Alpert, M. Galun, A. Brandt, and R. Basri. Image segmen-\ntation by probabilistic bottom-up aggregation and cue inte-\ngration. IEEE Trans. Pattern Anal. Mach. Intell., 34(2):315–\n327, Feb 2012. 5\n[2] A. Borji, M. Cheng, Q. Hou, H. Jiang, and J. Li. Salient\nobject detection: A survey. CoRR, abs/1411.5878, 2014. 2\n[3] A. Borji, M. Cheng, H. Jiang, and J. Li. Salient object detec-\ntion: A benchmark. IEEE Trans. Image Proc., 24(12):5706–\n5722, 2015. 2, 5\n[4] L. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and\nA. L. Yuille.\nDeeplab:\nSemantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully\nconnected crfs.\nIEEE Trans. Pattern Anal. Mach. Intell.,\nPP(99):1–1, 2017. 5\n[5] M. Cheng, N. J. Mitra, X. Huang, and S. Hu. Salientshape:\ngroup saliency in image collections. The Visual Computer,\n30(4):443–453, 2014. 5\n[6] M. Cheng, G. Zhang, N. Mitra, X. Huang, and S.-M. Hu.\nGlobal contrast based salient region detection. In Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., pages 409–416, 2011. 1, 2\n[7] M.-M. Cheng, J. Warrell, W.-Y. Lin, S. Zheng, V. Vineet,\nand N. Crook. Efﬁcient salient region detection with soft im-\nage abstraction. In Proc. IEEE Int. Conf. Comp. Vis., pages\n1529–1536, 2013. 2\n[8] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\nject classes challenge: A retrospective. Int. J. Comp. Vis.,\n111(1):98–136, 2015. 5\n[9] S. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware\nsaliency detection. IEEE Trans. Pattern Anal. Mach. Intell.,\n34(10):1915–1926, Oct 2012. 1, 2\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., pages 770–778, June 2016. 5, 7\n[11] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. H. S.\nTorr. Deeply supervised salient object detection with short\nconnections. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\npages 3203–3212, July 2017. 1, 2, 5, 6, 7\n[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In Proc. ACM Int.\nConf. Multimedia, pages 675–678, 2014. 5\n[13] B. Jiang, L. Zhang, H. Lu, C. Yang, and M. Yang. Saliency\ndetection via absorbing markov chain. In Proc. IEEE Int.\nConf. Comp. Vis., pages 1665–1672, 2013. 3, 5, 6\n[14] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li.\nSalient object detection: A discriminative regional feature\nintegration approach. In Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., pages 2083–2090, 2013. 2, 5, 6\n[15] P. Jiang, H. Ling, J. Yu, and J. Peng. Salient region detection\nby UFO: Uniqueness, focusness and objectness.\nIn Proc.\nIEEE Int. Conf. Comp. Vis., pages 1976–1983, 2013. 2\n[16] I. Jindal, M. Nokleby, and X. Chen. Learning deep networks\nfrom noisy labels with dropout regularization. In Proc. IEEE\nInt. Conf. Data Mining., pages 967–972, Dec 2016. 3\n[17] J. Kim, D. Han, Y.-W. Tai, and J. Kim. Salient region de-\ntection via high-dimensional color transform. In Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., pages 883–890, 2014. 2\n[18] B. Li, C. Shen, Y. Dai, A. van den Hengel, and M. He.\nDepth and surface normal estimation from monocular im-\nages using regression on deep features and hierarchical crfs.\nIn Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1119–\n1127, June 2015. 8\n[19] G. Li and Y. Yu. Visual saliency based on multiscale deep\nfeatures.\nIn Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\npages 5455–5463, June 2015. 2, 5, 6\n[20] G. Li and Y. Yu. Deep contrast learning for salient object\ndetection. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\npages 478–487, June 2016. 2, 5, 6\n[21] X. Li, H. Lu, L. Zhang, X. Ruan, and M. Yang. Saliency\ndetection via dense and sparse reconstruction. In Proc. IEEE\nInt. Conf. Comp. Vis., pages 2976–2983, Dec 2013. 3, 5, 6\n[22] X. Li, L. Zhao, L. Wei, M. H. Yang, F. Wu, Y. Zhuang,\nH. Ling, and J. Wang. Deepsaliency: Multi-task deep neu-\nral network model for salient object detection. IEEE Trans.\nImage Proc., 25(8):3919–3930, Aug 2016. 2, 5, 6\n[23] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille. The\nsecrets of salient object segmentation. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., pages 280–287, 2014. 5\n[24] T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum.\nLearning to detect a salient object.\nIn Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., pages 1–8, 2007. 5\n[25] Z. Lu, Z. Fu, T. Xiang, P. Han, L. Wang, and X. Gao. Learn-\ning from weak and noisy labels for semantic segmentation.\nIEEE Trans. Pattern Anal. Mach. Intell., 39(3):486–500, Mar\n2017. 3, 8\n[26] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M.\nJodoin. Non-local deep features for salient object detection.\nIn Proc. IEEE Conf. Comp. Vis. Patt. Recogn., July 2017. 2,\n5, 6, 7\n[27] X. Shen and Y. Wu. A uniﬁed approach to salient object\ndetection via low rank matrix recovery. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., pages 853–860, 2012. 2\n[28] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan. Saliency\ndetection with recurrent fully convolutional networks.\nIn\nProc. Eur. Conf. Comp. Vis., pages 825–841, 2016.\n2, 5,\n6\n[29] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu. A stage-\nwise reﬁnement model for detecting salient objects in im-\nages. In Proc. IEEE Int. Conf. Comp. Vis., 2017. 2, 6\n[30] J. Whitehill, T. fan Wu, J. Bergsma, J. R. Movellan, and P. L.\nRuvolo. Whose vote should count more: Optimal integration\nof labels from labelers of unknown expertise. In Proc. Adv.\nNeural Inf. Process. Syst., pages 2035–2043. 2009. 3\n[31] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-\nnov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention. In Proc. Int.\nConf. Mach. Learn., volume 37, pages 2048–2057, 2015. 1\n[32] Q. Yan, L. Xu, J. Shi, and J. Jia. Hierarchical saliency de-\ntection. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages\n1155–1162, 2013. 3, 5\n9\n[33] C. Yang, L. Zhang, H. Lu, X. Ruan, and M. Yang. Saliency\ndetection via graph-based manifold ranking. In Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., pages 3166–3173, 2013. 5\n[34] J. Yao, J. Wang, I. Tsang, Y. Zhang, J. Sun, C. Zhang, and\nR. Zhang. Deep Learning from Noisy Image Labels with\nQuality Embedding. ArXiv e-prints, Nov. 2017. 3\n[35] D. Zhang, J. Han, and Y. Zhang. Supervision by fusion: To-\nwards unsupervised learning of deep salient object detector.\nIn Proc. IEEE Int. Conf. Comp. Vis., Oct 2017. 1, 2, 3, 5, 6,\n7\n[36] G.-X. Zhang, M.-M. Cheng, S.-M. Hu, and R. R. Martin.\nA shape-preserving approach to image resizing. Computer\nGraphics Forum, 28(7):1897–1906, 2009. 1\n[37] J. Zhang, Y. Dai, B. Li, and M. He. Attention to the scale:\nDeep multi-scale salient object detection. In Proc. Int. Conf.\non Digital Image Computing: Techniques and Applications,\npages 1–7, Nov 2017. 2\n[38] J. Zhang, Y. Dai, and F. Porikli. Deep salient object detec-\ntion by integrating multi-level cues. In Proc. IEEE Winter\nConference on Applications of Computer Vision, pages 1–10,\nMarch 2017. 2\n[39] J. Zhang, B. Li, Y. Dai, F. Porikli, and M. He. Integrated\ndeep and shallow networks for salient object detection. In\nProc. IEEE Int. Conf. Image Process., pages 1537–1541,\nSept 2017. 2\n[40] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan. Amulet:\nAggregating multi-level convolutional features for salient\nobject detection. In Proc. IEEE Int. Conf. Comp. Vis., Oct\n2017. 1, 2, 5, 6, 7\n[41] P. Zhang, D. Wang, H. Lu, H. Wang, and B. Yin. Learning\nuncertain convolutional features for accurate saliency detec-\ntion. In Proc. IEEE Int. Conf. Comp. Vis., Oct 2017. 2, 5,\n6\n[42] R. Zhao, W. Ouyang, H. Li, and X. Wang. Saliency detection\nby multi-context deep learning. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., pages 1265–1274, 2015. 1, 2, 5, 6\n[43] W. Zhu, S. Liang, Y. Wei, and J. Sun. Saliency optimiza-\ntion from robust background detection. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., pages 2814–2821, 2014. 1, 2, 3, 5,\n6\n[44] W. Zou and N. Komodakis. Harf: Hierarchy-associated rich\nfeatures for salient object detection. In Proc. IEEE Int. Conf.\nComp. Vis., pages 406–414, Dec 2015. 6\n10\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-03-29",
  "updated": "2018-03-29"
}