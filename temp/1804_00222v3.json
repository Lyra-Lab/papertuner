{
  "id": "http://arxiv.org/abs/1804.00222v3",
  "title": "Meta-Learning Update Rules for Unsupervised Representation Learning",
  "authors": [
    "Luke Metz",
    "Niru Maheswaranathan",
    "Brian Cheung",
    "Jascha Sohl-Dickstein"
  ],
  "abstract": "A major goal of unsupervised learning is to discover data representations\nthat are useful for subsequent tasks, without access to supervised labels\nduring training. Typically, this involves minimizing a surrogate objective,\nsuch as the negative log likelihood of a generative model, with the hope that\nrepresentations useful for subsequent tasks will arise as a side effect. In\nthis work, we propose instead to directly target later desired tasks by\nmeta-learning an unsupervised learning rule which leads to representations\nuseful for those tasks. Specifically, we target semi-supervised classification\nperformance, and we meta-learn an algorithm -- an unsupervised weight update\nrule -- that produces representations useful for this task. Additionally, we\nconstrain our unsupervised update rule to a be a biologically-motivated,\nneuron-local function, which enables it to generalize to different neural\nnetwork architectures, datasets, and data modalities. We show that the\nmeta-learned update rule produces useful features and sometimes outperforms\nexisting unsupervised learning techniques. We further show that the\nmeta-learned unsupervised update rule generalizes to train networks with\ndifferent widths, depths, and nonlinearities. It also generalizes to train on\ndata with randomly permuted input dimensions and even generalizes from image\ndatasets to a text task.",
  "text": "Published as a conference paper at ICLR 2019\nMETA-LEARNING\nUPDATE\nRULES\nFOR\nUNSUPER-\nVISED REPRESENTATION LEARNING\nLuke Metz\nGoogle Brain\nlmetz@google.com\nNiru Maheswaranathan\nGoogle Brain\nnirum@google.com\nBrian Cheung\nUniversity of California, Berkeley\nbcheung@berkeley.edu\nJascha Sohl-Dickstein\nGoogle Brain\njaschasd@google.com\nABSTRACT\nA major goal of unsupervised learning is to discover data representations that are\nuseful for subsequent tasks, without access to supervised labels during training.\nTypically, this involves minimizing a surrogate objective, such as the negative log\nlikelihood of a generative model, with the hope that representations useful for\nsubsequent tasks will arise as a side effect. In this work, we propose instead to\ndirectly target later desired tasks by meta-learning an unsupervised learning rule\nwhich leads to representations useful for those tasks.\nSpeciﬁcally, we target semi-supervised classiﬁcation performance, and we meta-\nlearn an algorithm – an unsupervised weight update rule – that produces represen-\ntations useful for this task. Additionally, we constrain our unsupervised update\nrule to a be a biologically-motivated, neuron-local function, which enables it to\ngeneralize to different neural network architectures, datasets, and data modalities.\nWe show that the meta-learned update rule produces useful features and sometimes\noutperforms existing unsupervised learning techniques. We further show that the\nmeta-learned unsupervised update rule generalizes to train networks with different\nwidths, depths, and nonlinearities. It also generalizes to train on data with randomly\npermuted input dimensions and even generalizes from image datasets to a text task.\n1\nINTRODUCTION\nSupervised learning has proven extremely effective for many problems where large amounts of\nlabeled training data are available. There is a common hope that unsupervised learning will prove\nsimilarly powerful in situations where labels are expensive, impractical to collect, or where the\nprediction target is unknown during training. Unsupervised learning however has yet to fulﬁll this\npromise. One explanation for this failure is that unsupervised representation learning algorithms\nare typically mismatched to the target task. Ideally, learned representations should linearly expose\nhigh level attributes of data (e.g. object identity) and perform well in semi-supervised settings.\nMany current unsupervised objectives, however, optimize for objectives such as log-likelihood of a\ngenerative model or reconstruction error, producing useful representations only as a side effect.\nUnsupervised representation learning seems uniquely suited for meta-learning (Hochreiter et al.,\n2001; Schmidhuber, 1995). Unlike most tasks where meta-learning is applied, unsupervised learning\ndoes not deﬁne an explicit objective, which makes it impossible to phrase the task as a standard\noptimization problem. It is possible, however, to directly express a meta-objective that captures the\nquality of representations produced by an unsupervised update rule by evaluating the usefulness\nof the representation for candidate tasks. In this work, we propose to meta-learn an unsupervised\nupdate rule by meta-training on a meta-objective that directly optimizes the utility of the unsupervised\nrepresentation. Unlike hand-designed unsupervised learning rules, this meta-objective directly targets\nthe usefulness of a representation generated from unlabeled data for later supervised tasks.\nBy recasting unsupervised representation learning as meta-learning, we treat the creation of the\nunsupervised update rule as a transfer learning problem. Instead of learning transferable features,\n1\narXiv:1804.00222v3  [cs.LG]  26 Feb 2019\nPublished as a conference paper at ICLR 2019\nwe learn a transferable learning rule which does not require access to labels and generalizes across\nboth data domains and neural network architectures. Although we focus on the meta-objective\nof semi-supervised classiﬁcation here, in principle a learning rule could be optimized to generate\nrepresentations for any subsequent task.\n2\nRELATED WORK\n2.1\nUNSUPERVISED REPRESENTATION LEARNING\nUnsupervised learning is a topic of broad and diverse interest. Here we brieﬂy review several\ntechniques that can lead to a useful latent representation of a dataset. In contrast to our work,\neach method imposes a manually deﬁned training algorithm or loss function whereas we learn the\nalgorithm that creates useful representations as determined by a meta-objective.\nAutoencoders (Hinton and Salakhutdinov, 2006) work by ﬁrst compressing and optimizing recon-\nstruction loss. Extensions have been made to de-noise data (Vincent et al., 2008; 2010), as well as\ncompress information in an information theoretic way (Kingma and Welling, 2013). Le et al. (2011)\nfurther explored scaling up these unsupervised methods to large image datasets.\nGenerative adversarial networks (Goodfellow et al., 2014) take another approach to unsupervised\nfeature learning. Instead of a loss function, an explicit min-max optimization is deﬁned to learn a\ngenerative model of a data distribution. Recent work has shown that this training procedure can\nlearn unsupervised features useful for few shot learning (Radford et al., 2015; Donahue et al., 2016;\nDumoulin et al., 2016).\nOther techniques rely on self-supervision where labels are easily generated to create a non-trivial\n‘supervised’ loss. Domain knowledge of the input is often necessary to deﬁne these losses. Noroozi\nand Favaro (2016) use unscrambling jigsaw-like crops of an image. Techniques used by Misra et al.\n(2016) and Sermanet et al. (2017) rely on using temporal ordering from videos.\nAnother approach to unsupervised learning relies on feature space design such as clustering. Coates\nand Ng (2012) showed that k-means can be used for feature learning. Xie et al. (2016) jointly\nlearn features and cluster assignments. Bojanowski and Joulin (2017) develop a scalable technique\nto cluster by predicting noise. Other techniques such as Schmidhuber (1992), Hochreiter and\nSchmidhuber (1999), and Olshausen and Field (1997) deﬁne various desirable properties about\nthe latent representation of the input, such as predictability, complexity of encoding mapping,\nindependence, or sparsity, and optimize to achieve these properties.\n2.2\nMETA LEARNING\nMost meta-learning algorithms consist of two levels of learning, or ‘loops’ of computation: an inner\nloop, where some form of learning occurs (e.g. an optimization process), and an outer loop or meta-\ntraining loop, which optimizes some aspect of the inner loop, parameterized by meta-parameters.\nThe performance of the inner loop computation for a given set of meta-parameters is quantiﬁed by\na meta-objective. Meta-training is then the process of adjusting the meta-parameters so that the\ninner loop performs well on this meta-objective. Meta-learning approaches differ by the computation\nperformed in the inner loop, the domain, the choice of meta-parameters, and the method of optimizing\nthe outer loop.\nSome of the earliest work in meta-learning includes work by Schmidhuber (1987), which explores\na variety of meta-learning and self-referential algorithms. Similarly to our algorithm, Bengio et al.\n(1990; 1992) propose to learn a neuron local learning rule, though their approach differs in task and\nproblem formulation. Additionally, Runarsson and Jonsson (2000) meta-learn supervised learning\nrules which mix local and global network information. A number of papers propose meta-learning\nfor few shot learning (Vinyals et al., 2016; Ravi and Larochelle, 2016; Mishra et al., 2017; Finn et al.,\n2017; Snell et al., 2017), though these do not take advantage of unlabeled data. Others make use\nof both labeled and unlabeld data (Ren et al., 2018). Hsu et al. (2018) uses a task created with no\nsupervision to then train few-shot detectors. Garg (2018) use meta-learning for unsupervised learning,\nprimarily in the context of clustering and with a small number of meta-parameters.\n2\nPublished as a conference paper at ICLR 2019\nFigure 1: Left: Schematic for meta-learning an unsupervised learning algorithm. The inner loop\ncomputation consists of iteratively applying the UnsupervisedUpdate to a base model. During meta-\ntraining the UnsupervisedUpdate (parameterized by θ) is itself updated by gradient descent on the\nMetaObjective. Right: Schematic of the base model and UnsupervisedUpdate. Unlabeled input data,\nx0, is passed through the base model, which is parameterised by W and colored green. The goal of\nthe UnsupervisedUpdate is to modify W to achieve a top layer representation xL which performs\nwell at few-shot learning. In order to train the base model, information is propagated backwards\nby the UnsupervisedUpdate in a manner analogous to backprop. Unlike in backprop however, the\nbackward weights V are decoupled from the forward weights W. Additionally, unlike backprop,\nthere is no explicit error signal as there is no loss. Instead at each layer, and for each neuron, a\nlearning signal is injected by a meta-learned MLP parameterized by θ, with hidden state h. Weight\nupdates are again analogous to those in backprop, and depend on the hidden state of the pre- and post-\nsynaptic neurons for each weight.\nTo allow easy comparison against other existing approaches, we present a more extensive survey of\nprevious work in meta-learning in table form in Table 1, highlighting differences in choice of task,\nstructure of the meta-learning problem, choice of meta-architecture, and choice of domain.\nTo our knowledge, we are the ﬁrst meta-learning approach to tackle the problem of unsupervised\nrepresentation learning, where the inner loop consists of unsupervised learning. This contrasts with\ntransfer learning, where a neural network is instead trained on a similar dataset, and then ﬁne tuned or\notherwise post-processed on the target dataset. We additionally believe we are the ﬁrst representation\nmeta-learning approach to generalize across input data modalities as well as datasets, the ﬁrst to\ngeneralize across permutation of the input dimensions, and the ﬁrst to generalize across neural\nnetwork architectures (e.g. layer width, network depth, activation function).\n3\nMODEL DESIGN\nWe consider a multilayer perceptron (MLP) with parameters φt as the base model. The inner loop of\nour meta-learning process trains this base model via iterative application of our learned update rule.\nSee Figure 1 for a schematic illustration and Appendix A for a more detailed diagram.\nIn standard supervised learning, the ‘learned’ optimizer is stochastic gradient descent (SGD). A\nsupervised loss l (x, y) is associated with this model, where x is a minibatch of inputs, and y\nare the corresponding labels. The parameters φt of the base model are then updated iteratively\nby performing SGD using the gradient ∂l(x,y)\n∂φt . This supervised update rule can be written as\nφt+1 = SupervisedUpdate(φt, xt, yt; θ), where t denotes the inner-loop iteration or step. Here θ are\nthe meta-parameters of the optimizer, which consist of hyper-parameters such as learning rate and\nmomentum.\nIn this work, our learned update is a parametric function which does not depend on label information,\nφt+1 = UnsupervisedUpdate(φt, xt; θ). This form of the update rule is general, it encompasses\nmany unsupervised learning algorithms and all methods in Section 2.1.\nIn traditional learning algorithms, expert knowledge or a simple hyper-parameter search determines\nθ, which consists of a handful of meta-parameters such as learning rate and regularization constants.\nIn contrast, our update rule will have orders of magnitude more meta-parameters, including the\n3\nPublished as a conference paper at ICLR 2019\nMethod\nInner loop updates\nOuter loop updates, meta-\nGeneralizes to\nparameters\nobjective\noptimizer\nHyper parameter optimization\nJones (2001); Snoek et al. (2012);\nBergstra et al. (2011); Bergstra\nand Bengio (2012)\nmany steps of optimization\noptimization\nhyper-\nparameters\ntraining or\nvalidation\nset loss\nBaysian\nmethods,\nrandom\nsearch, etc\ntest data from a\nﬁxed dataset\nNeural architecture search Stanley\nand Miikkulainen (2002); Zoph\nand Le (2017); Baker et al. (2017);\nZoph et al. (2018); Real et al.\n(2017)\nsupervised SGD training\nusing meta-learned\narchitecture\narchitecture\nvalidation\nset loss\nRL or\nevolution\ntest loss within\nsimilar datasets\nTask-speciﬁc optimizer (eg for\nquadratic function identiﬁcation)\n(Hochreiter et al., 2001)\nadjustment of model weights\nby an LSTM\nLSTM weights\ntask loss\nSGD\nsimilar domain\ntasks\nLearned optimizers Jones (2001);\nMaclaurin et al. (2015);\nAndrychowicz et al. (2016); Chen\net al. (2016); Li and Malik (2017);\nWichrowska et al. (2017); Bello\net al. (2017)\nmany steps of optimization of\na ﬁxed loss function\nparametric\noptimizer\naverage or\nﬁnal loss\nSGD or\nRL\nnew loss\nfunctions\n(mixed success)\nPrototypical networks Snell et al.\n(2017)\napply a feature extractor to a\nbatch of data and use soft\nnearest neighbors to compute\nclass probabilities\nweights of the\nfeature\nextractor\nfew shot\nperformance\nSGD\nnew image\nclasses within\nsimilar dataset\nMAML Finn et al. (2017)\none step of SGD on training\nloss starting from a\nmeta-learned network\ninitial weights\nof neural\nnetwork\nreward or\ntraining loss\nSGD\nnew goals,\nsimilar task\nregimes with\nsame input\ndomain\nEvolved Policy Gradient\nHouthooft et al. (2018)\nperforming gradient descent\non a learned loss\nparameters of a\nlearned loss\nfunction\nreward\nEvolutionary\nStrategies\nnew\nenvironment\nconﬁgurations,\nboth in and not\nin meta-training\ndistribution.\nFew shot learning (Vinyals et al.,\n2016; Ravi and Larochelle, 2016;\nMishra et al., 2017)\napplication of a recurrent\nmodel, e.g. LSTM, Wavenet.\nrecurrent model\nweights\ntest loss on\ntraining\ntasks\nSGD\nnew image\nclasses within\nsimilar dataset.\nMeta-unsupervised learning for\nclustering Garg (2018)\nrun clustering algorithm or\nevaluate binary similarity\nfunction\nclustering\nalgorithm + hy-\nperparameters,\nbinary\nsimilarity\nfunction\nempirical\nrisk mini-\nmization\nvaried\nnew clustering\nor similarity\nmeasurement\ntasks\nLearning synaptic learning rules\n(Bengio et al., 1990; 1992)\nrun a synapse-local learning\nrule\nparametric\nlearning rule\nsupervised\nloss, or\nsimilarity to\nbiologically-\nmotivated\nnetwork\ngradient\ndescent,\nsimulated\nannealing,\ngenetic\nalgorithms\nsimilar domain\ntasks\nOur work — metalearning for\nunsupervised representation\nlearning\nmany applications of an\nunsupervised update rule\nparametric\nupdate rule\nfew shot\nclassiﬁca-\ntion after\nunsuper-\nvised\npre-\ntraining\nSGD\nnew base\nmodels (width,\ndepth,\nnonlinearity),\nnew datasets,\nnew data\nmodalities\nTable 1: A comparison of published meta-learning approaches.\n4\nPublished as a conference paper at ICLR 2019\nweights of a neural network. We train these meta-parameters by performing SGD on the sum of the\nMetaObjective over the course of (inner loop) training in order to ﬁnd optimal parameters θ∗,\nθ∗= argmin\nθ\nEtask\n\"X\nt\nMetaObjective(φt)\n#\n,\n(1)\nthat minimize the meta-objective over a distribution of training tasks. Note that φt is a function of θ\nsince θ affects the optimization trajectory.\nIn the following sections, we brieﬂy review the main components of this model: the base model,\nthe UnsupervisedUpdate, and the MetaObjective. See the Appendix for a complete speciﬁcation.\nAdditionally, code and meta-trained parameters θ for our meta-learned UnsupervisedUpdate is\navailable1.\n3.1\nBASE MODEL\nOur base model consists of a standard fully connected multi-layer perceptron (MLP), with batch\nnormalization (Ioffe and Szegedy, 2015), and ReLU nonlinearities. We chose this as opposed to a\nconvolutional model to limit the inductive bias of convolutions in favor of learned behavior from\nthe UnsupervisedUpdate. We call the pre-nonlinearity activations z1, · · · , zL, and post-nonlinearity\nactivations x0, · · · , xL, where L is the total number of layers, and x0 ≡x is the network input (raw\ndata). The parameters are φ =\n\b\nW 1, b1, V 1, · · · , W L, bL, V L\t\n, where W l and bl are the weights\nand biases (applied after batch norm) for layer l, and V l are the corresponding weights used in the\nbackward pass.\n3.2\nLEARNED UPDATE RULE\nWe wish for our update rule to generalize across architectures with different widths, depths, or\neven network topologies. To achieve this, we design our update rule to be neuron-local, so that\nupdates are a function of pre- and post- synaptic neurons in the base model, and are deﬁned for\nany base model architecture. This has the added beneﬁt that it makes the weight updates more\nsimilar to synaptic updates in biological neurons, which depend almost exclusively on local pre- and\npost-synaptic neuronal activity (Whittington and Bogacz, 2017). In practice, we relax this constraint\nand incorporate some cross neuron information to decorrelate neurons (see Appendix G.5 for more\ninformation).\nTo build these updates, each neuron i in every layer l in the base model has an MLP, referred to\nas an update network, associated with it, with output hl\nbi = MLP\n\u0000xl\nbi, zbil, V l+1, δl+1; θ\n\u0001\nwhere\nb indexes the training minibatch. The inputs to the MLP are the feedforward activations (xl & zl)\ndeﬁned above, and feedback weights and an error signal (V l and δl, respectively) which are deﬁned\nbelow.\nAll update networks share meta-parameters θ. Evaluating the statistics of unit activation over a batch\nof data has proven helpful in supervised learning (Ioffe and Szegedy, 2015). It has similarly proven\nhelpful in hand-designed unsupervised learning rules, such as sparse coding and clustering. We\ntherefore allow hl\nbi to accumulate statistics across examples in each training minibatch.\nDuring an unsupervised training step, the base model is ﬁrst run in a standard feed-forward fashion,\npopulating xl\nbi, zl\nbi. As in supervised learning, an error signal δl\nbi is then propagated backwards\nthrough the network. Unlike in supervised backprop, however, this error signal is generated by the\ncorresponding update network for each unit. It is read out by linear projection of the per-neuron\nhidden state h, δl\nbi = lin\n\u0000hl\nbi\n\u0001\n, and propogated backward using a set of learned ‘backward weights’\n(V l)T , rather than the transpose of the forward weights (W l)T as would be the case in backprop\n(diagrammed in Figure 1). This is done to be more biologically plausible (Lillicrap et al., 2016).\nAgain as in supervised learning, the weight updates (∆W l) are a product of pre- and post-synaptic sig-\nnals. Unlike in supervised learning however, these signals are generated using the per-neuron update\n1https://github.com/tensorflow/models/tree/master/research/learning_\nunsupervised_learning\n5\nPublished as a conference paper at ICLR 2019\nnetworks: ∆W l\nij = func\n\u0010\nhl\nbi, hl−1\nbj , Wij\n\u0011\n. The full weight update (which involves normalization\nand decorrelation across neurons) is deﬁned in Appendix G.5.\n3.3\nMETA-OBJECTIVE\nThe meta-objective determines the quality of the unsupervised representations. In order to meta-train\nvia SGD, this loss must be differentiable. The meta-objective we use in this work is based on ﬁtting a\nlinear regression to labeled examples with a small number of data points. In order to encourage the\nlearning of features that generalize well, we estimate the linear regression weights on one minibatch\n{xa, ya} of K data points, and evaluate the classiﬁcation performance on a second minibatch {xb, yb}\nalso with K datapoints,\nˆv = argmin\nv\n\u0010\r\rya −vT xL\na\n\r\r2 + λ ∥v∥2\u0011\n,\nMetaObjective(·; φ) = CosDist\n\u0000yb, ˆvT xL\nb\n\u0001\n,\n(2)\nwhere xL\na , xL\nb are features extracted from the base model on data xa, xb, respectively. The target\nlabels ya, yb consist of one hot encoded labels and potentially also regression targets from data\naugmentation (e.g. rotation angle, see Section 4.2). We found that using a cosine distance, CosDist,\nrather than unnormalized squared error improved stability. Note this meta-objective is only used\nduring meta-training and not used when applying the learned update rule. The inner loop computation\nis performed without labels via the UnsupervisedUpdate.\n4\nTRAINING THE UPDATE RULE\n4.1\nAPPROXIMATE GRADIENT BASED TRAINING\nWe choose to meta-optimize via SGD as opposed to reinforcement learning or other black box\nmethods, due to the superior convergence properties of SGD in high dimensions, and the high\ndimensional nature of θ. Training and computing derivatives through long recurrent computation\nof this form is notoriously difﬁcult (Pascanu et al., 2013). To improve stability and reduce the\ncomputational cost we approximate the gradients ∂[MetaObjective]\n∂θ\nvia truncated backprop through time\n(Shaban et al., 2018). Many additional design choices were also crucial to achieving stability and\nconvergence in meta-learning, including the use of batch norm, and restricting the norm of the\nUnsupervisedUpdate update step (a full discussion of these and other choices is in Appendix B).\n4.2\nMETA-TRAINING DISTRIBUTION AND GENERALIZATION\nGeneralization in our learned optimizer comes from both the form of the UnsupervisedUpdate\n(Section 3.2), and from the meta-training distribution. Our meta-training distribution is composed of\nboth datasets and base model architectures.\nWe construct a set of training tasks consisting of CIFAR10 (Krizhevsky and Hinton, 2009) and\nmulti-class classiﬁcation from subsets of classes from Imagenet (Russakovsky et al., 2015) as well\nas from a dataset consisting of rendered fonts (Appendix H.1.1). We ﬁnd that increased training\ndataset variation actually improves the meta-optimization process. To reduce computation we restrict\nthe input data to 16x16 pixels or less during meta-training, and resize all datasets accordingly.\nFor evaluation, we use MNIST (LeCun et al., 1998), Fashion MNIST (Xiao et al., 2017), IMDB\n(Maas et al., 2011), and a hold-out set of Imagenet classes. We additionally sample the base model\narchitecture. We sample number of layers uniformly between 2-5 and the number of units per layer\nlogarithmically between 64 to 512.\nAs part of preprocessing, we permute all inputs along the feature dimension, so that the\nUnsupervisedUpdate must learn a permutation invariant learning rule. Unlike other work, we focus\nexplicitly on learning a learning algorithm as opposed to the discovery of ﬁxed feature extractors that\ngeneralize across similar tasks. This makes the learning task much harder, as the UnsupervisedUpdate\nhas to discover the relationship between pixels based solely on their joint statistics, and cannot “cheat”\nand memorize pixel identity. To provide further dataset variation, we additionally augment the data\nwith shifts, rotations, and noise. We add these augmentation coefﬁcients as additional regression\ntargets for the meta-objective–e.g. rotate the image and predict the rotation angle as well as the image\nclass. For additional details, see Appendix H.1.1.\n6\nPublished as a conference paper at ICLR 2019\n4.3\nDISTRIBUTED IMPLEMENTATION\nWe implement the above models in distributed TensorFlow (Abadi et al., 2016). Training uses 512\nworkers, each of which performs a sequence of partial unrolls of the inner loop UnsupervisedUpdate,\nand computes gradients of the meta-objective asynchronously. Training takes ∼8 days, and consists\nof ∼200 thousand updates to θ with minibatch size 256. Additional details are in Appendix C.\n5\nEXPERIMENTAL RESULTS\nFirst, we examine limitations of existing unsupervised and meta learning methods. Then, we show\nmeta-training and generalization properties of our learned optimizer and ﬁnally we conclude by\nvisualizing how our learned update rule works. For details of the experimental setup, see Appendix\nH.\n5.1\nOBJECTIVE FUNCTION MISMATCH AND EXISTING META-LEARNING METHODS\nTo illustrate the negative consequences of objective function mismatch in unsupervised learnin al-\ngorithms, we train a variational autoencoder on 16x16 CIFAR10. Over the course of training we\nevaluate classiﬁcation performance from few shot classiﬁcation using the learned latent representa-\ntions. Training curves can be seen in Figure 2. Despite continuing to improve the VAE objective\nthroughout training (not shown here), the classiﬁcation accuracy decreases sharply later in training.\nTo demonstrate the reduced generalization that results from learning transferable features rather than\nan update algorithm, we train a prototypical network (Snell et al., 2017) with and without the input\nshufﬂing described in Section 4.2. As the prototypical network primarily learns transferrable features,\nperformance is signiﬁcantly hampered by input shufﬂing. Results are in Figure 2.\nFigure 2: Left: Standard unsupervised learning approaches suffer from objective function missmatch.\nContinuing to optimize a variational auto-encoder (VAE) hurts few-shot accuracy after some number\nof steps (dashed line). Right: Prototypical networks transfer features rather than a learning algorithm,\nand perform poorly if tasks don’t have consistent data structure. Training a prototypical network with\na fully connected architecture (same as our base model) on a MiniImagenet 10-way classiﬁcation task\nwith either intact inputs (light purple) or by permuting the pixels before every training and testing task\n(dark purple). Performance with permuted inputs is greatly reduced (gray line). Our performance is\ninvariant to pixel permutation.\n5.2\nMETA-OPTIMIZATION\nWhile training, we monitor a rolling average of the meta-objective averaged across all datasets, model\narchitectures, and the number of unrolling steps performed. In Figure 3 the training loss is continuing\nto decrease after 200 hours of training, which suggests that the approximate training techniques still\nproduce effective learning. In addition to this global number, we measure performance obtained by\nrolling out the UnsupervisedUpdate on various meta-training and meta-testing datasets. We see that\non held out image datasets, such as MNIST and Fashion Mnist, the evaluation loss is still decreasing.\nHowever, for datasets in a different domain, such as IMDB sentiment prediction (Maas et al., 2011),\nwe start to see meta-overﬁtting. For all remaining experimental results, unless otherwise stated, we\nuse meta-parameters, θ, for the UnsupervisedUpdate resulting from 200 hours of meta-training.\n7\nPublished as a conference paper at ICLR 2019\nFigure 3: Training curves for the training and evaluation task distributions. Our train set consists\nof MiniImagenet, Alphabet, and MiniCIFAR. Our test sets are Mini Imagenet Test, Tiny Fashion\nMNIST, Tiny MNIST and IMDB. Error bars denote standard deviation of evaluations with a ﬁxed\nwindow of samples evaluated from a single model. Dashed line at 200 hours indicates model used for\nremaining experiments unless otherwise stated. For a bigger version of this ﬁgure, see Appendix E.\n5.3\nGENERALIZATION\nThe goal of this work is to learn a general purpose unsupervised representation learning algorithm.\nAs such, this algorithm must be able to generalize across a wide range of scenarios, including tasks\nthat are not sampled i.i.d. from the meta-training distribution. In the following sections, we explore a\nsubset of the factors we seek to generalize over.\nGeneralizing over datasets and domains\nIn Figure 4, we compare performance on few shot classiﬁcation with 10 examples per class. We\nevaluate test performance on holdout datasets of MNIST and Fashion MNIST at 2 resolutions: 14×14\nand 28×28 (larger than any dataset experienced in meta-training). On the same base model archi-\ntecture, our learned UnsupervisedUpdate leads to performance better than a variational autoencoder,\nsupervised learning on the labeled examples, and random initialization with trained readout layer.\nTinyMnist\nMnist\nTinyFashionMnist\nFashionMnist\n0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy\nInitialization\nSupervised\nVAE\nLearned (ours)\nFigure 4:\nLeft: The learned UnsupervisedUpdate generalizes to unseen datasets. Our learned\nupdate rule produces representations more suitable for few shot classiﬁcation than those from random\ninitialization or a variational autoecoder and outperforms fully supervised learning on the same\nlabeled examples. Error bars show standard error. Right: Early in meta-training (purple), the\nUnsupervisedUpdate is able to learn useful features on a 2 way text classiﬁcation data set, IMDB,\ndespite being meta-trained only from image datasets. Later in meta-training (red) performance drops\ndue to the domain mismatch. We show inner-loop training, consisting of 5k applications of the\nUnsupervisedUpdate evaluating the MetaObjective each iteration. Error bars show standard error\nacross 10 runs.\nTo further explore generalization limits, we test our learned optimizer on data from a vastly different\ndomain. We train on a binary text classiﬁcation dataset: IMDB movie reviews (Maas et al., 2011),\nencoded by computing a bag of words with 1K words. We evaluate using a model 30 hours and 200\nhours into meta-training (see Figure 4). Despite being trained exclusively on image datasets, the 30\nhour learned optimizer improves upon the random initialization by almost 10%. When meta-training\nfor longer, however, the learned optimizer “meta-overﬁts” to the image domain resulting in poor\nperformance. This performance is quite low in an absolute sense, for this task. Nevertheless, we ﬁnd\n8\nPublished as a conference paper at ICLR 2019\nthis result very exciting as we are unaware of any work showing this kind of transfer of learned rules\nfrom images to text.\nGeneralizing over network architectures\nWe train models of varying depths and unit counts with our learned optimizer and compare results\nat different points in training (Figure 5). We ﬁnd that despite only training on networks with 2 to 5\nlayers and 64 to 512 units per layer, the learned rule generalizes to 11 layers and 10,000 units per\nlayer.\nReLU\nLeaky ReLU\nELU\nSoftplus\nSwish\nTanh\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\nAccuracy\n0 steps\n100 steps\n1000 steps\n10000 steps\nFigure 5: Left: The learned UnsupervisedUpdate is capable of optimizing base models with hidden\nsizes and depths outside the meta-training regime. As we increase the number of units per layer, the\nlearned model can make use of this additional capacity despite never having experienced it during\nmeta-training. Right: The learned UnsupervisedUpdate generalizes across many different activation\nfunctions not seen in training. We show accuracy over the course of training on 14x14 MNIST.\nNext we look at generalization over different activation functions. We apply our learned optimizer\non base models with a variety of different activation functions. Performance evaluated at different\npoints in training (Figure 5). Despite training only on ReLU activations, our learned optimizer is able\nto improve on random initializations in all cases. For certain activations, leaky ReLU (Maas et al.,\n2013) and Swish (Ramachandran et al., 2017), there is little to no decrease in performance. Another\ninteresting case is the step activation function. These activations are traditionally challenging to train\nas there is no useful gradient signal. Despite this, our learned UnsupervisedUpdate is capable of\noptimizing as it does not use base model gradients, and achieves performance double that of random\ninitialization.\n5.4\nHOW IT LEARNS AND HOW IT LEARNS TO LEARN\nTo analyze how our learned optimizer functions, we analyze the ﬁrst layer ﬁlters over the course\nof meta-training. Despite the permutation invariant nature of our data (enforced by shufﬂing input\nimage pixels before each unsupervised training run), the base model learns features such as those\nshown in Figure 6, which appear template-like for MNIST, and local-feature-like for CIFAR10.\nEarly in training, there are coarse features, and a lot of noise. As the meta-training progresses, more\ninteresting and local features emerge.\nIn an effort to understand what our algorithm learns to do, we fed it data from the two moons dataset.\nWe ﬁnd that despite being a 2D dataset, dissimilar from the image datasets used in meta-training, the\nlearned model is still capable of manipulating and partially separating the data manifold in a purely\nunsupervised manner (Figure 6). We also ﬁnd that almost all the variance in the embedding space is\ndominated by a few dimensions. As a comparison, we do the same analysis on MNIST. In this setting,\nthe explained variance is spread out over more of the principal components. This makes sense as the\ngenerative process contains many more latent dimensions – at least enough to express the 10 digits.\n9\nPublished as a conference paper at ICLR 2019\nFigure 6:\nLeft: From left to right we show ﬁrst layer base model receptive ﬁelds produced by\nour learned UnsupervisedUpdate rule over the course of meta-training. Each pane consists of ﬁrst\nlayer ﬁlters extracted from φ after 10k applications of UnsupervisedUpdate on MNIST (top) and\nCIFAR10 (bottom). For MNIST, the optimizer learns image-template-like features. For CIFAR10,\nlow frequency features evolve into higher frequency and more spatially localized features. For\nmore ﬁlters, see Appendix D. Center: Visualization of learned representations before (left) and\nafter (right) training a base model with our learned UnsupervisedUpdate for two moons (top) and\nMNIST (bottom). The UnsupervisedUpdate is capable of manipulating the data manifold, without\naccess to labels, to separate the data classes. Visualization shows a projection of the 32-dimensional\nrepresentation of the base network onto the top three principal components. Right: Cumulative\nvariance explained using principal components analysis (PCA) on the learned representations. The\nrepresentation for two moons data (red) is much lower dimensional than MNIST (blue), although\nboth occupy a fraction of the full 32-dimensional space.\n6\nDISCUSSION\nIn this work we meta-learn an unsupervised representation learning update rule. We show performance\nthat matches or exceeds existing unsupervised learning on held out tasks. Additionally, the update rule\ncan train models of varying widths, depths, and activation functions. More broadly, we demonstrate an\napplication of meta-learning for learning complex optimization tasks where no objective is explicitly\ndeﬁned. Analogously to how increased data and compute have powered supervised learning, we\nbelieve this work is a proof of principle that the same can be done with algorithm design–replacing\nhand designed techniques with architectures designed for learning and learned from data via meta-\nlearning.\nACKNOWLEDGMENTS\nWe would like to thank Samy Bengio, David Dohan, Keren Gu, Gamaleldin Elsayed, C. Daniel\nFreeman, Sam Greydanus, Nando de Freitas, Ross Goroshin, Ishaan Gulrajani, Eric Jang, Hugo\nLarochelle, Jeremy Nixon, Esteban Real, Suharsh Sivakumar, Pavel Sountsov, Alex Toshev, George\nTucker, Hoang Trieu Trinh, Olga Wichrowska, Lechao Xiao, Zongheng Yang, Jiaqi Zhai and the rest\nof the Google Brain team for extremely helpful conversations and feedback on this work.\n10\nPublished as a conference paper at ICLR 2019\nREFERENCES\nSepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In\nInternational Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001.\nJuergen Schmidhuber. On learning how to learn learning strategies. 1995.\nGeoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks.\nscience, 313(5786):504–507, 2006.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing\nrobust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine\nlearning, pages 1096–1103. ACM, 2008.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked\ndenoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.\nJournal of Machine Learning Research, 11(Dec):3371–3408, 2010.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nQuoc V Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S Corrado, Jeff Dean,\nand Andrew Y Ng. Building high-level features using large scale unsupervised learning. arXiv preprint\narXiv:1112.6209, 2011.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing\nsystems, pages 2672–2680, 2014.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional\ngenerative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nJeff Donahue, Philipp Krähenbühl, and Trevor Darrell.\nAdversarial feature learning.\narXiv preprint\narXiv:1605.09782, 2016.\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron\nCourville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In\nEuropean Conference on Computer Vision, pages 69–84. Springer, 2016.\nIshan Misra, C Lawrence Zitnick, and Martial Hebert. Shufﬂe and learn: unsupervised learning using temporal\norder veriﬁcation. In European Conference on Computer Vision, pages 527–544. Springer, 2016.\nPierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey Levine. Time-contrastive networks: Self-supervised\nlearning from multi-view observation. arXiv preprint arXiv:1704.06888, 2017.\nAdam Coates and Andrew Y Ng. Learning feature representations with k-means. In Neural networks: Tricks of\nthe trade, pages 561–580. Springer, 2012.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In\nInternational conference on machine learning, pages 478–487, 2016.\nPiotr Bojanowski and Armand Joulin.\nUnsupervised learning by predicting noise.\narXiv preprint\narXiv:1704.05310, 2017.\nJürgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):\n863–879, 1992.\nSepp Hochreiter and Jürgen Schmidhuber. Feature extraction through lococode. Neural Computation, 11(3):\n679–714, 1999.\nBruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by\nv1? Vision research, 37(23):3311–3325, 1997.\nJürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the\nmeta-meta-... hook. PhD thesis, Technische Universität München, 1987.\nYoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Université de Montréal,\nDépartement d’informatique et de recherche opérationnelle, 1990.\n11\nPublished as a conference paper at ICLR 2019\nSamy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule.\nIn Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks, pages 6–8. Univ. of Texas, 1992.\nThomas Philip Runarsson and Magnus Thor Jonsson. Evolution and design of distributed learning rules. In\nCombinations of Evolutionary Computation and Neural Networks, 2000 IEEE Symposium on, pages 59–63.\nIEEE, 2000.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for\none shot learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 29, pages 3630–3638. Curran Associates, Inc., 2016. URL http://\npapers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference on\nLearning Representations, 2016.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal convolutions.\narXiv preprint arXiv:1707.03141, 2017.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on\nMachine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126–1135, International\nConvention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr.\npress/v70/finn17a.html.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In Advances in\nNeural Information Processing Systems, pages 4080–4090, 2017.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo\nLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. arXiv preprint\narXiv:1803.00676, 2018.\nKyle Hsu, Sergey Levine, and Chelsea Finn.\nUnsupervised learning via meta-learning.\narXiv preprint\narXiv:1810.02334, 2018.\nVikas Garg. Supervising unsupervised learning. In Advances in Neural Information Processing Systems, pages\n4996–5006, 2018.\nDonald R Jones. A taxonomy of global optimization methods based on response surfaces. Journal of global\noptimization, 21(4):345–383, 2001.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning\nalgorithms. In Advances in neural information processing systems, pages 2951–2959, 2012.\nJames S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization.\nIn Advances in neural information processing systems, pages 2546–2554, 2011.\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine\nLearning Research, 13(Feb):281–305, 2012.\nKenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolu-\ntionary computation, 10(2):99–127, 2002.\nBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. International Conference\non Learning Representations, 2017. URL https://arxiv.org/abs/1611.01578.\nBowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using\nreinforcement learning. International Conference on Learning Representations, 2017.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable\nimage recognition. Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.\nEsteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc Le, and Alex\nKurakin. Large-scale evolution of image classiﬁers. arXiv preprint arXiv:1703.01041, 2017.\nDougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through\nreversible learning. In International Conference on Machine Learning, pages 2113–2122, 2015.\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando\nde Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information\nProcessing Systems, pages 3981–3989, 2016.\n12\nPublished as a conference paper at ICLR 2019\nYutian Chen, Matthew W Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P Lillicrap, Matt\nBotvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient descent. arXiv\npreprint arXiv:1611.03824, 2016.\nKe Li and Jitendra Malik. Learning to optimize. International Conference on Learning Representations, 2017.\nOlga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio Gomez Colmenarejo, Misha Denil,\nNando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and generalize. International\nConference on Machine Learning, 2017.\nIrwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc Le. Neural optimizer search with reinforcement learning.\n2017. URL https://arxiv.org/pdf/1709.07417.pdf.\nRein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie, Filip Wolski, Jonathan Ho, and Pieter Abbeel.\nEvolved policy gradients. arXiv preprint arXiv:1802.04821, 2018.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In Francis Bach and David Blei, editors, Proceedings of the 32nd International\nConference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 448–456,\nLille, France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/ioffe15.\nhtml.\nJames CR Whittington and Rafal Bogacz. An approximation of the error backpropagation algorithm in a\npredictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5):1229–1262,\n2017.\nTimothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random synaptic feedback\nweights support error backpropagation for deep learning. Nature communications, 7:13276, 2016.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural networks. In\nInternational Conference on Machine Learning, pages 1310–1318, 2013.\nAmirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation for bilevel\noptimization. arXiv preprint arXiv:1810.10667, 2018.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi:\n10.1007/s11263-015-0816-y.\nYann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine\nlearning algorithms, 2017.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA,\nJune 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/\nP11-1015.\nMartín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale machine learning. In\nOSDI, volume 16, pages 265–283, 2016.\nAndrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network acoustic\nmodels. In Proc. icml, volume 30, page 3, 2013.\nPrajit Ramachandran, Barret Zoph, and Quoc Le. Searching for activation functions. 2017.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. CoRR,\nabs/1211.5063, 2012.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n13\nPublished as a conference paper at ICLR 2019\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\nMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\nBarak Pearlmutter. An investigation of the gradient descent process in neural networks. PhD thesis, Carnegie\nMellon University Pittsburgh, PA, 1996.\nSamuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation.\narXiv preprint arXiv:1611.01232, 2016.\n14\nPublished as a conference paper at ICLR 2019\nA\nMORE DETAILED SYSTEM DIAGRAM\n∂[MetaObjective]\n∂θ\n_\n+\nUnlabeled\n Data\nForward\nPass\nBackward \nPass\nΦt\nΦt+1\n+\nΔΦ\nx0\nx1\nx2\nδ0\nh0\nx3\nh1\nh2\nh3\nδ1\nδ2\nΔΦt\n1\nΔΦt\n2\nΔΦt\n3\nΦt\n1\nΦt\n2\nΦt\n3\nBackward \n(θ)\nWeight\nUpdate\n(θ)\nx1\nConvolutional \nNeural Network\nθ0\nθ1\nθt\nθt+1\nΦ0\nΦ1\nΦt\nMetaObjective(Φ, Labeled Data)\nSGD\nd.) Forward and Backward Pass Details\nActivations\nδ1\nΦt+1\nθt\nθt+1\nUnsupervisedUpdate( · ; θt)\nSGD\nΦT\nUpper Error\nSignal\nh2\nh1\nΔΦt\n2\nTo lower layer\nc.) Inner Loop Training – No Labels Used\na.) Meta-Training / Outer Loop\ne.) Parameterization of the Weight Updates\nb.) Meta-Objective Calculation\nδ2\nOther\nFrom upper layer\nFigure App.1: Schematic for meta-learning an unsupervised learning algorithm. We show the hi-\nerarchical nature of both the meta-training procedure and update rule. a) Meta-training, where\nthe meta-parameters, θ, are updated via our meta-optimizer (SGD). b) The gradients of the\nMetaObjective with respect to θ are computed by backpropagation through the unrolled application\nof the UnsupervisedUpdate. c) UnsupervisedUpdate updates the base model parameters (φ) using\na minibatch of unlabeled data. d) Each application of UnsupervisedUpdate involves computing a\nforward and “backward” pass through the base model. The base model itself is a fully connected\nnetwork producing hidden states xl for each layer l. The “backward” pass through the base model\nuses an error signal from the layer above, δ, which is generated by a meta-learned function. e.) The\nweight updates ∆φ are computed using a convolutional network, using δ and x from the pre- and\npost-synaptic neurons, along with several other terms discussed in the text.\nB\nSTABILIZING GRADIENT BASED META-LEARNING TRAINING\nTraining and computing derivatives through recurrent computation of this form is notoriously difﬁcult\nPascanu et al. (2013). Training parameters of recurrent systems in general can lead to chaos. We\nused the usual techniques such as gradient clipping (Pascanu et al., 2012), small learning rates, and\nadaptive learning rate methods (in our case Adam (Kingma and Ba, 2014)), but in practice this\n15\nPublished as a conference paper at ICLR 2019\nwas not enough to train most UnsupervisedUpdate architectures. In this section we address other\ntechniques needed for stable convergence.\nWhen training with truncated backprop the problem shifts from pure optimization to something\nmore like optimizing on a Markov Decision Process where the state space is the base-model weights,\nφ, and the ‘policy’ is the learned optimizer. While traversing these states, the policy is constantly\nmeta-optimized and changing, thus changing the distribution of states the optimizer reaches. This\ntype of non-i.i.d training has been discussed at great length with respect to on and off-policy RL\ntraining algorithms (Mnih et al., 2013). Other works cast optimizer meta-learning as RL (Li and\nMalik, 2017) for this very reason, at a large cost in terms of gradient variance. In this work, we\npartially address this issue by training a large number of workers in parallel, to always maintain a\ndiverse set of states when computing gradients.\nFor similar reasons, the number of steps per truncation, and the total number of unsupervised training\nsteps, are both sampled in an attempt to limit the bias introduced by truncation.\nWe found restricting the maximum inner loop step size to be crucial for stability. Pearlmutter (1996)\nstudied the effect of learning rates with respect to the stability of optimization and showed that as the\nlearning rate increases gradients become chaotic. This effect was later demonstrated with respect to\nneural network training in Maclaurin et al. (2015). If learning rates are not constrained, we found that\nthey rapidly grew and entered this chaotic regime.\nAnother technique we found useful in addressing these problems is the use of batch norm in both\nthe base model and in the UnsupervisedUpdate rule. Multi-layer perceptron training traditionally\nrequires very precise weight initialization for learning to occur. Poorly scaled initialization can make\nlearning impossible (Schoenholz et al., 2016). When applying a learned optimizer, especially early in\nmeta-training of the learned optimizer, it is very easy for the learned optimizer to cause high variance\nweights in the base model, after which recovery is difﬁcult. Batch norm helps solve this issues by\nmaking more of the weight space usable.\nC\nDISTRIBUTED IMPLEMENTATION\nWe implement the described models in distributed Tensorﬂow (Abadi et al., 2016). We construct a\ncluster of 512 workers, each of which computes gradients of the meta-objective asynchronously. Each\nworker trains on one task by ﬁrst sampling a dataset, architecture, and a number of training steps.\nNext, each worker samples k unrolling steps, does k applications of the UnsupervisedUpdate(·; θ),\ncomputes the MetaObjective on each new state, computes ∂[MetaObjective]\n∂θ\nand sends this gradient to a\nparameter server. The ﬁnal base-model state, φ, is then used as the starting point for the next unroll\nuntil the speciﬁed number of steps is reached. These gradients from different workers are batched\nand θ is updated with asynchronous SGD. By batching gradients as workers complete unrolls, we\neliminate most gradient staleness while retaining the compute efﬁciency of asynchronous workers,\nespecially given heterogeneous workloads which arise from dataset and model size variation. An\noverview of our training can be seen in algorithm F. Due to the small base models and the sequential\nnature of our compute workloads, we use multi core CPUs as opposed to GPUs. Training occurs over\nthe course of ∼8 days with ∼200 thousand updates to θ with minibatch size 256.\n16\nPublished as a conference paper at ICLR 2019\nD\nMORE FILTERS OVER META-TRAINING\nFigure App.2: More ﬁlters extracted over the course of meta-training. Note, due to implementation\nreasons, the columns do not represent the same point / same iteration of θ. Each ﬁlter is extracted\nafter 10k inner loop optimization steps, at φ10k. From top to bottom we show: MNIST, Tiny MNIST,\nAlphabet, CIFAR10, and Mini CIFAR10. Filters shift from noise at initialization to more local\nfeatures later in meta-training.\n17\nPublished as a conference paper at ICLR 2019\nE\nMETA-TRAINING CURVES\nFigure App.3: Training curves for the meta-training and meta-evaluation task distributions. Our\nmeta-train set consists of Mini Imagenet, Alphabet, and MiniCIFAR10. Our meta-test sets are Mini\nImagenet Test, Tiny Fashion MNIST, Tiny MNIST and IMDB. Error bars denote standard deviation\nof evaluations with a ﬁxed window of samples evaluated from a single model.\nF\nLEARNING ALGORITHM\nInitialize UnsupervisedUpdates parameters, θ0.\nInitialize meta-training step count v ←0.\nInitialize shared gradient state S\nInitialize R to be max meta-training steps.\nwhile r < R on 512 workers in parallel do\nSample supervised task, D\nSample base model f(·; φ) and initialize φ0 randomly\nSample K truncated iterations\nInitialize learner iteration count: t ←0\nfor k = 1 to K do\nSample U unroll iterations\nfor u = 0 to U do\nSample data, x, from D\nφt+u+1 = UnsupervisedUpdate(x, φt+u; θr)\nend for\nInitialize MetaObjective accumulator J ←0\nfor u = 1 to U do\nfor m = 1 to M (number of MetaObjective evaluations per iteration) do\nSample 2 batches of data, x,x′, and labels, y,y′ from D for both training and testing.\nJ = J +\n1\nUM MetaObjective(x, y, x′, y′, φt+u)\nend for\nend for\nCompute ∂J\n∂θr for the last U steps and store in S\nUpdate current unroll iteration: t ←t + U\nif size of S > meta-batch-size then\nTake averaged batch of gradients G from S\nθr+1 = θr −AdamUpdate(G)\nr ←r + 1\nend if\nend for\nend while\nAlgorithm 1: Distributed Training Algorithm\n18\nPublished as a conference paper at ICLR 2019\nG\nMODEL SPECIFICATION\nIn this section we describe the details of our base model and learned optimizer. First, we describe the\ninner loop, and then we describe the meta-training procedure in more depth.\nIn the following sections, λname denotes a hyper-parameter for the learned update rule.\nThe design goals of this system are stated in the text body. At the time of writing, we were unaware of\nany similar systems, so as such the space of possible architectures was massive. Future work consists\nof simplifying and improving abstractions around algorithmic components.\nAn open source implementation of the UnsupervisedUpdate can be found at https:\n//github.com/tensorflow/models/tree/master/research/learning_\nunsupervised_learning.\nG.1\nINNER LOOP\nThe inner loop computation consists of iterative application of the UnsupervisedUpdate on a Base\nModel parameterized by φ,\nφt+1 = UnsupervisedUpdate(·, φt; θ),\n(App.1)\nwhere φ consists of forward weights, W l, biases, bl, parameterizing a multi-layer perceptron as well\nas backward weights, V l used by UnsupervisedUpdate when inner-loop training.\nThis computation can be broken down further as a forward pass on an unlabeled batch of data,\nx0 ∼D\n(App.2)\n{x1..xL, z1..zL} = f(x0; φt),\n(App.3)\nwhere zl, and xl are the pre- and post-activations on layer l, and x0 is input data. We then compute\nweight updates:\n{(∆W 1···L)t, (∆b1···L)t, (∆V 1···L)t} = ComputeDeltaWeight(x0 · · · xL, z1 · · · zL, φt; θ)\n(App.4)\nFinally, the next set of φ forward weights (W l), biases (bl), and backward weights (V l), are computed.\nWe use an SGD like update but with the addition of a decay term. Equivalently, this can be seen as\nsetting the weights and biases to an exponential moving average of the ∆W l, ∆V l, and ∆bl terms.\nW l\nt+1 = W l\nt(1 −λφlr) + ∆W lλφlr\n(App.5)\nV l\nt+1 = V l\nt (1 −λφlr) + ∆V lλφlr\n(App.6)\nbl\nt+1 = bl\nt(1 −λφlr) + ∆blλφlr\n(App.7)\nWe use λφlr = 3e −4 in our work. ∆W l, ∆V l, ∆bl are computed via meta-learned functions\n(parameterized by θ).\nIn the following sections, we describe the functional form of the base model, f, as well as the\nfunctional form of ComputeDeltaWeight (·; θ).\nG.2\nBASE MODEL\nThe base model, the model our learned update rule is training, is an L layer multi layer perception\nwith batch norm. We deﬁne φ as this model’s parameters, consisting of weights (W l) and biases\n(bl) as well as the backward weights (V l) used only during inner-loop training (applications of\nUnsupervisedUpdate). We deﬁne N 1..N L to be the sizes of the layers of the base model and N 0 to\nbe the size of the input data.\nφ = {W 1..W L, V 1..V L, b1..bL},\n(App.8)\n19\nPublished as a conference paper at ICLR 2019\nwhere\nW l ∈RNl−1,N l\n(App.9)\nV l ∈RNl−1,N l\n(App.10)\nbl ∈RNl\n(App.11)\nwhere N l is the hidden size of the network, N 0 is the input size of data, and N L is the size of the\noutput embedding. In this work, we ﬁx the output layer: N L = 32 and vary the remaining the\nintermediate N 1..(L−1) hidden sizes.\nThe forward computation parameterized by φ, consumes batches of unlabeled data from a dataset D:\nx0 ∼D, x0 ∈RB,N 0\n(App.12)\nzl = BatchNorm\n\u0000xl−1W l\u0001\n+ bl\n(App.13)\nxl = ReLU(zl)\n(App.14)\nfor l = 1..L.\nWe deﬁne f(x; φ) as a function that returns the set of internal pre- and post-activation function hidden\nstates as well as ˆf(x; φ) as the function returning the ﬁnal hidden state:\nf(x, φ) = {z1...zL, x0..xL, φ}\n(App.15)\nˆf(x, φ) = xL\n(App.16)\nG.3\nMETAOBJECTIVE\nWe deﬁne the MetaObjective to be a few shot linear regression. To increase stability and avoid\nundesirable loss landscapes, we additionally center, as well as normalize the predicted target before\ndoing the loss computation. The full computation is as follows:\nMetaObjective(x, y, x′, y′, φ) :: (RB,N 0, RB,N classes, RB,N 0, RB,N classes, Φ) →R1, (App.17)\nwhere N classes is the number of classes, and y, y′ are one hot encoded labels.\nFirst, the inputs are converted to embeddings with the base model,\nxL = ˆf(x; φ)\n(App.18)\nx′L = ˆf(x′; φ)\n(App.19)\nNext, we center and normalize the prediction targets. We show this for y, but y′ is processed\nidentically.\n¯y =\n1\nBN classes\nB\nX\ni\nN classes\nX\nj\nyij\n(App.20)\nˆyij =\nyij −¯y\nq\n1\nNclasses\nPNclasses\na\n∥¯yia∥2\n2\n(App.21)\nWe then solve for the linear regression weights in closed form with features: xL and targets: ˆy. We\naccount for a bias by concatenating a 1’s vector to the features.\nA = [xL; 1]\n(App.22)\nC =\n\u0000(AtA) + Iλridge\n\u0001−1 AT ˆy\n(App.23)\nWe then use these inferred regression weights C to make a prediction on the second batch of data,\nnormalize the resulting prediction, and compute a ﬁnal loss,\np = C[x′L; 1]\n(App.24)\nˆpbi =\npbi\n∥pb∥2\n(App.25)\nMetaObjective(·) = 1\nB\nB\nX\nb\n∥ˆpb −ˆyb∥2\n2.\n(App.26)\n20\nPublished as a conference paper at ICLR 2019\nNote that due to the normalization of predictions and targets, this corresponds to a cosine distance\n(up to an offset and multiplicative factor).\nG.4\nUNSUPERVISEDUPDATE\nThe learned update rule is parameterized by θ. In the following section, we denote all instances of θ\nwith a subscript to be separate named learnable parameters of the UnsupervisedUpdate. θ is shared\nacross all instantiations of the unsupervised learning rule (shared across layers). It is this weight\nsharing that lets us generalize across different network architectures.\nThe computation is split into a few main components: ﬁrst, there is the forward pass, deﬁned in\nf(x; φ). Next, there is a “backward” pass, which operates on the hidden states in a reverse order to\npropagate an error signal δl back down the network. In this process, a hidden state hl is created for\neach layer. These h are tensors with a batch, neuron and feature index: hl ∈RB,N l,λhdims where\nλhdims = 64. Weight updates to W l and bl are then readout from these hl and other signals found\nboth locally, and in aggregate along both batch and neuron.\nG.4.1\nBACKWARD ERROR PROPAGATION\nIn backprop, there exists a single scalar error signal that gets backpropagated back down the network.\nIn our work, this error signal does not exist as there is no loss being optimized. Instead we have a\nlearned top-down signal, dL, at the top of the network. Because we are no longer restricted to the\nform of backprop, we make this quantity a vector for each unit in the network, rather than a scalar,\nx0 ∼D, x0 ∈RB,Nx\n(App.27)\n{z1...zL, x0..xL, φ} = f(x; φ)\n(App.28)\ndL = TopD(xL; θtopD)\n(App.29)\n(App.30)\nwhere dl ∈RB,N l,λdeltadims. In this work we set λdeltadims = 32. The architecture of TopD is a\nneural network that operates along every dimension of xL. The speciﬁcation can be found in G.7.\nWe structure our error propagation similarly to the structure of the backpropagated error signal in a\nstandard MLP with contributions to the error at every layer in the network,\nδl\nijd = dl\nijd ⊙σ(zl\nijd) +\nλhdims\nX\nk\n(θerrorP ropW )kd hl\nijk + (θerrorP ropB)d\n(App.31)\nwhere δl has the same shape as dl, and θerrorP ropW ∈Rλhdims,λdeltadims and θerrorP ropB ∈\nRλdeltadims. Note both θerrorP ropW and θerrorP ropB are shared for all layers l.\nIn a similar way to backprop, we move this signal down the network via multiplying by a backward\nweight matrix (V l). We do not use the previous weight matrix transpose as done in backprop, instead\nwe learn a separate set of weights that are not tied to the forward weights and updated along with\nthe forward weights as described in G.5. Additionally, we normalize the signal to have ﬁxed second\nmoment,\n˜dl\nimd =\nNl+1\nX\nj\nδl+1\nijd (V l+1)mj\n(App.32)\ndl\nimd = ˆdl\nimd\n \n1\nλdeltadims\nλdeltadims\nX\na\n˜dl\nima\n!−1\n2\n(App.33)\nThe internal hl ∈RB,N l,λhdims vectors are computed via:\nhl = ComputeH\n\u0000dl, xl, zl; θcomputeH\n\u0001\n(App.34)\nThe architecture of ComputeH is a neural network that operates on every dimension of all the inputs.\nIt can be found in G.8. These deﬁnitions are recursive, and are computed in order: hL, hL−1 · · · h1, h0.\nWith these computed, weight updates can be read out (Section G.5). When the corresponding symbols\nare not deﬁned (e.g. z0) a zeros tensor with the correct shape is used instead.\n21\nPublished as a conference paper at ICLR 2019\nG.5\nWEIGHT UPDATES\nThe following is the implementation of ComputeDeltaWeight(x0 · · · xL, z1 · · · zL, φ; θ)\nFor a given layer, l, our weight updates are a mixture of multiple low rank readouts from hl and hl−1.\nThese terms are then added together with a learnable weight, in θ to form a ﬁnal update. The ﬁnal\nupdate is then normalized mixed with the previous weights. We update both the forward, W l, and the\nbackward, V l, using the same update rule parameters θ. We show the forward weight update rule\nhere, and drop the backward for brevity.\nFor convenience, we deﬁne a low rank readout function LowRR that takes hl like tensors, and outputs\na single lower rank tensor.\nLowRR\n\u0000ha, hb; Θ\n\u0001\n::\n(App.35)\n\u0010\nRB,N a,λhdims, RB,N b,λhdims\n\u0011\n→RN”a,N b\n(App.36)\nHere, Θ, is a placehoder for the parameters of the given readout. LowRR is deﬁned as:\nΘ = {P a, P b}\nwhere P a ∈Rλhdims,λgradc and P b ∈Rλhdims,λgradc\n(App.37)\nra\nijp =\nX\nk\nha\nijkP a\nkp\n(App.38)\nrb\nijp =\nX\nk\nhb\nijkP b\nkp\n(App.39)\nLowRR (·)jp =\nB\nX\ni\nλgradc\nX\nk\nrb\nijkra\nipk\n1\nBλhdims\n(App.40)\nwhere λgradc = 4 and is the rank of the readout matrix (per batch element).\nG.5.1\nLOCAL TERMS\nThis sequence of terms allow the weight to be adjusted as a function of state in the pre- and post-\nsynaptic neurons. They should be viewed as a basis function representation of the way in which\nthe weight changes as a function of pre- and post-synaptic neurons, and the current weight value.\nWe express each of these contributions to the weight update as a sequence of weight update planes,\nwith the ith plane written ∆W l\ni ∈RNl−1×Nl. Each of these planes will be linearly summed, with\ncoefﬁcients generated as described in Equation App.57, in order to generate the eventual weight\nupdate.\nˆW l =\nW l\nq\n1\nNl−1\nPN l−1\ni\n(W l)2\ni\n(App.41)\n∆W l\n1 = ˆW l\n(App.42)\n∆W l\n2 = ( ˆW l)2sign( ˆW l)\n(App.43)\n∆W l\n3 = LowRR(hl−1, hl; θzero)\n(App.44)\n∆W l\n4 = exp(−( ˆW l\nb)2) ⊙LowRR(hl−1, hl; θrbf)\n(App.45)\n∆W l\n5 = W l\nb ⊙LowRR(hl−1, hl; θfirst)\n(App.46)\n∆W l\n6 = 1\nB\nB\nX\nb\n \nxl−1\nb\n−1\nB\nB\nX\nb′\nxl−1\nb′\n!T  \nxl\nb −1\nB\nB\nX\nb′\nxl\nb′\n!T\n(App.47)\nG.5.2\nDECORRELATION TERMS\nAdditional weight update planes are designed to aid units in remaining decorrelated from each other’s\nactivity, and in decorrelating their receptive ﬁelds. Without terms like this, a common failure mode\n22\nPublished as a conference paper at ICLR 2019\nis for many units in a layer to develop near-identical representations. Here, Sl\ni indicates a scratch\nmatrix associated with weight update plane i and layer l.\nSl\n7 =\n1\n√\nN l−1 LowRR(hl−1, hl−1, θlinLowerSymm)\n(App.48)\n∆W l\n7 =\n1\n√\n2\nh\u0010\nSl\n7 +\n\u0000Sl\n7\n\u0001T \u0011\n⊙(1 −I)\ni\nW l\n(App.49)\nSl\n8 =\n1\n√\nN l−1 LowRR(hl−1, hl−1, θsqrLowerSymm)\n(App.50)\n∆W l\n8 =\n1\n√\n2\nh\u0010\nSl\n8 +\n\u0000Sl\n8\n\u0001T \u0011\n⊙(1 −I)\ni \u0012q\n1 + (W l)2 −1\n\u0013\n(App.51)\nSl\n9 =\n1\n√\nN l LowRR(hl, hl, θlinUpperSymm)\n(App.52)\n∆W l\n9 =\n1\n√\n2W l h\u0010\nSl\n9 +\n\u0000Sl\n9\n\u0001T \u0011\n⊙(1 −I)\ni\n(App.53)\nSl\n10 =\n1\n√\nN l LowRR(hl, hl, θsqrUpperSymm)\n(App.54)\n∆W l\n10 =\n1\n√\n2\n\u0012q\n1 + (W l)2 −1\n\u0013 h\u0010\nSl\n10 +\n\u0000Sl\n10\n\u0001T \u0011\n⊙(1 −I)\ni\n(App.55)\n(App.56)\nG.6\nAPPLICATION OF THE WEIGHT TERMS IN THE OPTIMIZER\nWe then normalize, re-weight, and merge each of these weight update planes into a single term, which\nwill be used in the weight update step,\n˜\n∆W l\ni =\n∆W l\ni\nq\n1 +\n1\nNl−1Nl\nPNl−1\nm\nPNl\nn\n\u0000∆W l\nimn\n\u00012\n(App.57)\n(∆W l\nmerge)jk = 1\nB\nB\nX\ni\n(θmergeW )i\n˜\n∆W l\nijk,\n(App.58)\nwhere θmergeW ∈R10 (as we have 10 input planes).\nTo prevent pathologies during training, we perform two post processing steps to prevent the learned\noptimizer from cheating, and increasing its effective learning rate, leading to instability. We only\nallow updates which do not decrease the weight matrix magnitude,\n∆W l\north = ∆W l\nmerge −ˆW l ReLU(∆Wmerge · ˆW l),\n(App.59)\nwhere ˆW l is W l scaled to have unit norm, and we normalize the length of the update,\n∆W l\nfinal =\n∆W l\north\nq\n1 +\n1\nNl−1Nl\nPN l−1\nm\nPNl\nn\n\u0000∆W l\north\n\u00012\nmn\n(App.60)\nTo compute changes in the biases, we do a readout from hl. We put some constraints on this update to\nprevent the biases from pushing all units into the linear regime, and minimizing learning. We found\nthis to be a possible pathology.\n∆bl\nbase = 1\nB\nB\nX\ni\nλhdims\nX\nk\n(θBreadout)khl\nijk\n(App.61)\n∆bl\nconstrained = ∆bl\nbase −ReLU\n\n−1\nN l\nNl\nX\ni\n\u0000∆bl\nbase\n\u0001\ni\n\n,\n(App.62)\n23\nPublished as a conference paper at ICLR 2019\nwhere θBreadout ∈Rλhdims.\nWe then normalize the update via the second moment:\n∆bl\nfinal =\n∆bl\nconstrained\n1\nN l\nPN l\ni (∆bl\nconstrained)2\ni\n(App.63)\nFinally, we deﬁne ComputeDeltaWeight as all layer’s forward weight updates, and backward weight\nupdates, and bias updates.\nComputeDeltaWeight(x0 · · · xL, z1 · · · zL, φt; θ) = (∆W 1..L\nfinal, ∆b1..L\nfinal, ∆V 1..L\nfinal)\n(App.64)\nG.7\nTopD\nThis function performs various convolutions over the batch dimension and data dimension. For ease\nof notation, we use m as an intermediate variable. Additionally, we drop all convolution and batch\nnorm parameterizations. They are all separate elements of θtopD. We deﬁne two 1D convolution\noperators that act on rank 3 tensors: ConvBatch which performs convolutions over the zeroth index,\nand ConvUnit which performs convolutions along the ﬁrs index. We deﬁne the S argument to be\nsize of hidden units, and the K argument to be the kernel size of the 1D convolutions. Additionally,\nwe set the second argument of BatchNorm to be the axis normalized over.\nTopD\n\u0000xL; θtopD\n\u0001\n:: RB,N L →RB,N L,λdeltadims\n(App.65)\nFirst, we reshape to add another dimension to the end of xL, in pseudocode:\nm0 = [xL]\n(App.66)\nNext, a convolution is performed on the batch dimension with a batch norm and a ReLU non linearity.\nThis starts to pull information from around the batch into these channels.\nm1 = ConvBatch (m0, S = λtopdeltasize, K = 5)\n(App.67)\nm2 = ReLU (BatchNorm (m1, [0, 1]))\n(App.68)\n(App.69)\nWe set λtopdeltasize = 64. Next, a series of unit convolutions (convolutions over the last dimension)\nare performed. These act as compute over information composed from the nearby elements of the\nbatch. These unit dimensions effectively rewrite the batch dimension. This restricts the model to\noperate on a ﬁxed size batch.\nm3 = ConvUnit (m2, S = B, K = 3)\n(App.70)\nm4 = ReLU (BatchNorm (m3, [0, 1]))\n(App.71)\nm5 = ConvUnit (m4, S = B, K = 3)\n(App.72)\nm6 = ReLU (BatchNorm (m5, [0, 1]))\n(App.73)\nNext a series of 1D convolutions are performed over the batch dimension for more compute capacity.\nm7 = ConvBatch (m6, S = λtopdeltasize, K = 3)\n(App.74)\nm8 = ReLU (BatchNorm (m7, [0, 1]))\n(App.75)\nm9 = ConvBatch (m8, S = λtopdeltasize, K = 3)\n(App.76)\nm10 = ReLU (BatchNorm (m9, [0, 1]))\n(App.77)\nFinally, we convert the representations to the desired dimensions and output.\nm11 = ConvBatch (m10, S = λdeltadims, K = 3)\n(App.78)\nTopD\n\u0000xL; θtopD\n\u0001\n= m11\n(App.79)\n24\nPublished as a conference paper at ICLR 2019\nG.8\nComputeH\nThis is the main computation performed while transfering signals down the network and the output is\ndirectly used for weight updates. It is deﬁned as:\nComputeH\n\u0000dl, xl, zl, W l, W l+1, bl; θcomputeH\n\u0001\n::\n\u0010\nRB,N l,λdeltadims, RB,N l, RB,N l, RNl−1,N l, RN1,N l+1, RNl\u0011\n→\n\u0010\nRB,N l,λhdims\n\u0011\n(App.80)\nThe outputs of the base model, (xl, zl), plus an additional positional embeddings are stacked then\nconcatenated with d, to form a tensor in RB,N l,(4+λdeltadims):\n(p0)ij = sin(2jπ\nN l )\n(App.81)\n(p1)ij = cos(2jπ\nN l )\n(App.82)\nm0 = [xl, zl, p0, p1] where m0 ∈RB,N l,4\n(App.83)\nm1 = [m0; dl]\n(App.84)\nStatistics across the batch and unit dimensions, 0 and 1, are computed. We deﬁne a Statsi function\nbellow. We have 2 instances for both the zeroth and the ﬁrst index, shown bellow is the zeroth index\nand the ﬁrst is omitted.\nStats0 (w) :: RK0,K1 →RK1,4\n(App.85)\n(sl1)j =\n1\nK0\nK0\nX\ni\nabs(wij)\n(App.86)\n(sl2)j =\nv\nu\nu\nt 1\nK0\nK0\nX\ni\n(wij)2\n(App.87)\n(sµ)j =\n1\nK0\nK0\nX\ni\nwij\n(App.88)\n(sσ)j =\nv\nu\nu\nt 1\nK0\nK0\nX\ni\n((sµ)i −wij)2\n(App.89)\nStats0 (w) = [sl1, sl2, sµ, sσ]\n(App.90)\nWe the compute statistics of the weight matrix below, and above. We tile the statistics to the\nappropriate dimensions and concatenate with normalized inputs as well as with the bias (also tiled\nappropriately).\n(s0)ijk = (Stats0\n\u0000W l, 0\n\u0001\n)j\n(App.91)\n(s1)ijk = (Stats1\n\u0000W l+1, 1\n\u0001\n)i\n(App.92)\nm2 = BatchNorm(m1, [0, 1])\n(App.93)\nˆbijk = bl\nj where ˆb ∈RB,N l,1\n(App.94)\nm3 = [s0; s1; m2;ˆb] where m3 ∈RB,N l,4+4+(4+λdeltadims)+1\n(App.95)\n25\nPublished as a conference paper at ICLR 2019\nWith the inputs prepared, we next perform a series of convolutions on batch and unit dimensions, (0,\n1).\nm4 = ConvBatch (m3, S = λcomputehsize, K = 3)\n(App.96)\nm5 = ReLU (BatchNorm (m4, [0, 1]))\n(App.97)\nm6 = ConvUnit (m5, S = λcomputehsize, K = 3)\n(App.98)\nm7 = ReLU (BatchNorm (m6, [0, 1]))\n(App.99)\nm8 = ConvBatch (m7, S = λcomputehsize, K = 3)\n(App.100)\nm9 = ReLU (BatchNorm (m8, [0, 1]))\n(App.101)\nm10 = ConvUnit (m9, S = λcomputehsize, K = 3)\n(App.102)\nm11 = ReLU (BatchNorm (m10, [0, 1]))\n(App.103)\nThe result is then output.\nComputeH (·; θcomputeH) = m11\n(App.104)\nWe set λcomputehsize = 64 which is the inner computation size.\nH\nEXPERIMENTAL DETAILS\nH.1\nMETA TRAINING\nH.1.1\nTRAINING DATA DISTRIBUTION\nWe trained on a data distribution consisting of tasks sampled uniformly over the following datasets.\nHalf of our training tasks where constructed off of a dataset consisting of 1000 font rendered\ncharacters. We resized these to 14x14 black and white images. We call this the glyph dataset.\n\"Alphabet\" is an example of such a dataset consisting of alphabet characters. We used a mixture\n10, 13, 14, 17, 20, and 30 way classiﬁcation problems randomly sampled, as well as sampling from\nthree 10-way classiﬁcation problems sampled from speciﬁc types of images: letters of the alphabet,\nmath symbols, and currency symbols. For half of the random sampling and all of the specialized\nselection we apply additional augmentation. This augmentation consists of random rotations (up to\n360 degrees) and shifts up to +-5 pixels in the x and y directions. The parameters of the augmentation\nwere inserted into the regression target of the MetaObjective as a curriculum of sorts and to provide\ndiverse training signal.\nIn addition to the the glyph set, we additionally used Cifar10, resized to 16x16, as well as 10, 15,\n20, and 25 way classiﬁcation problems from imagenet. Once again we resized to 16x16 for compute\nreasons.\nWith a dataset selected, we apply additional augmentation with some probability consisting of the\nfollowing augmentations. A per task dropout mask (ﬁxed mask across all images in that task). A per\nexample dropout mask (a random mask per image). A permutation sampled from a ﬁxed number of\npre created permutations per class. A per image random shift in the x direction each of image. All of\nthese additional augmentations help with larger domain transfer.\nH.2\nMETA-OPTIMIZATION\nWe employ Adam (Kingma and Ba, 2014) as our meta-optimizer. We use a learning rate schedule of\n3e-4 for the ﬁrst 100k steps, then 1e-4 for next 50k steps, then 2e-5 for remainder of meta-training.\nWe use gradient clipping of norm 5 on minibatchs of size 256.\nWe compute our meta-objective by averaging 5 evaluation of the linear regression. We use a ridge\npenalty of 0.1 for all this work.\nWhen computing truncated gradients, we initially sample the number of unrolled applications of\nthe UnsupervisedUpdate in a uniform distribution of [2,4]. This is the number of steps gradients\nare backpropogated through. Over the course of 50k meta-training steps we uniformly increase this\nto [8,15]. This increases meta-training speed and stability as large unrolls early in training can be\nunstable and don’t seem to provide any value.\n26\nPublished as a conference paper at ICLR 2019\nFor sampling the number of truncated steps (number of times the above unrolls are performed), we\nuse a shifted normal distribution – a normal distribution with the same mean and standard deviation.\nWe chose this based on the expected distribution of the training step, φ iteration number, across the\ncluster of workers. We initially set the standard deviation low, 20, but slowly increased it over the\ncourse of 5000 steps to 20k steps. This slow increase also improved stability and training speed.\nH.3\nEXPERIMENTAL SETUP\nFor each experimental ﬁgure, we document the details.\nH.3.1\nOBJECTIVE FUNCTION MISMATCH\nThe VAE we used consists of 3 layers, size 128, with ReLU activations and batch norm between\neach layer. We then learn a projection to mean and log std of size 32. We sample, and use the\ninverse architecture to decode back to images. We use a quantized normal distribution (once again\nparameterized as mean and log std) as a posterior. We train with Adam with a learning rate of 1e-4.\nTo isolate the effects of objective function mismatch and overﬁtting, we both train on the unlabeled\ntraining set and evaluate on the labeled training set instead of a validation set.\nH.3.2\nGENERALIZATION: DATASET AND DOMAIN\nWe use a 4 layer, size 128 unit architecture with a 32 layer embedding for all models. We select\nperformance at 100k training steps for the VAE, and 3k for our learned optimizer.\nOur supervised learning baseline consists of the same architecture for the base model but with an\nadditional layer that outputs log probabilities. We train with cross entropy loss on a dataset consisting\nof only 10 examples per class (to match the other numbers). Surprisingly, the noise from batch norm\nacts as a regularizer, and allowing us to avoid needing a complex early stopping scheme as test set\nperformance simply plateaus over the course of 10k steps. We train with Adam with a learning\nrate of 3e-3, selected via a grid over learning rate on test set performance. In this setting, having a\ntrue validation set would dramatically lower the amount of labeled data available (only 100 labeled\nexamples) and using the test set only aids in the this baseline’s performance.\nFor the IMDB experiments, we tokenized and selected the top 1K words with an additional set of\ntokens for unknown, sentence start, and sentence end. Our encoding consisted of a 1 if the word\nis present, otherwise 0. We used the same 4 layer, 128 hidden unit MLP with an addition layer\noutputting a 32 dimensional embedding.\nH.3.3\nGENERALIZATION: NETWORK ARCHITECTURE\nWe used ReLU activations and 4 layers of size 128 with an additional layer to 32 units unless\notherwise speciﬁed by the speciﬁc experiment.\nH.3.4\nEXISTING META LEARNING MODELS\nWe trained prototypical networks (Snell et al., 2017) on either intact or shufﬂed mini-Imagenet\nimages. For shufﬂed images, we generated a ﬁxed random permutation for the inputs independently\nfor every instantiation of the base network (or ’episode’ in the meta-learning literature (Vinyals et al.,\n2016; Ravi and Larochelle, 2016)). The purpose of shufﬂing was to demonstrate the inductive bias of\nthis type of meta-learning, namely that they do not generalize across data domain. Note that the base\nnetwork trained was the same fully connected architecture like that used in this paper (3 layers, size\n128, with ReLU activations and batch normalization between layers). Though the original paper used\na convolutional architecture, here we swapped it with the fully connected architecture because the\ntied weights in a convolutional model do not make sense with shufﬂed pixels.\n27\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-03-31",
  "updated": "2019-02-26"
}