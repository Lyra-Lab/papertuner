{
  "id": "http://arxiv.org/abs/1804.01653v2",
  "title": "Review of Deep Learning",
  "authors": [
    "Rong Zhang",
    "Weiping Li",
    "Tong Mo"
  ],
  "abstract": "In recent years, China, the United States and other countries, Google and\nother high-tech companies have increased investment in artificial intelligence.\nDeep learning is one of the current artificial intelligence research's key\nareas. This paper analyzes and summarizes the latest progress and future\nresearch directions of deep learning. Firstly, three basic models of deep\nlearning are outlined, including multilayer perceptrons, convolutional neural\nnetworks, and recurrent neural networks. On this basis, we further analyze the\nemerging new models of convolution neural networks and recurrent neural\nnetworks. This paper then summarizes deep learning's applications in many areas\nof artificial intelligence, including speech processing, computer vision,\nnatural language processing and so on. Finally, this paper discusses the\nexisting problems of deep learning and gives the corresponding possible\nsolutions.",
  "text": "信息与控制　　２０１８年　第４７卷　第４期：３８５～３９７\nＤＯＩ：１０．１３９７６／ｊ．ｃｎｋｉ．ｘｋ．２０１８．８０９１\n文章编号：１００２－０４１１（２０１８）－０４－０３８５－１３\n深度学习研究综述\n张　荣，李伟平，莫　同\n北京大学软件与微电子学院，北京　１００８７１\n基金项目：国家重点研发计划项目（２０１７ＹＦＣ０８０３６０９，２０１７ＹＦＢ１４００４００）；河南省交通运输厅科技项目（２０１６Ｇ５）\n通信作者：李伟平，ｗｐｌｉ＠ｓｓ．ｐｋｕ．ｅｄｕ．ｃｎ　　收稿／录用／修回：２０１８－０３－０１／２０１８－０５－０２／２０１８－０５－１４\n摘要\n近年来，中美等国家、谷歌等高科技公司纷纷加大对人工智能的投入，深度\n学习是目前人工智能的重点研究领域之一，本文对深度学习最新进展及未来研究\n方向进行了分析和总结．首先概述了三类深度学习基本模型，包括多层感知器、\n卷积神经网络和循环神经网络．在此基础上，进一步分析了不断涌现出来的新型\n卷积神经网络和循环神经网络．然后本文总结了深度学习在人工智能众多领域中\n的应用，包括语音处理、计算机视觉和自然语言处理等．最后探讨了深度学习目\n前存在的问题并给出了相应的可能解决方法．\n关键词\n深度学习\n神经网络\n机器学习\n人工智能\n卷积神经网络\n循环神经网络\n中图法分类号：ＴＰ３０１．６\n文献标识码：Ａ\nＲｅｖｉｅｗｏｆＤｅｅｐＬｅａｒｎｉｎｇ\nＺＨＡＮＧＲｏｎｇ，ＬＩＷｅｉｐｉｎｇ，ＭＯＴｏｎｇ\nＳｃｈｏｏｌｏｆＳｏｆｔｗａｒｅａｎｄＭｉｃｒｏｅｌｅｃｔｒｏｎｉｃｓ，ＰｅｋｉｎｇＵｎｉｖｅｒｓｉｔｙ，Ｂｅｉｊｉｎｇ１００８７１，Ｃｈｉｎａ\nＡｂｓｔｒａｃｔ\nＩｎｒｅｃｅｎｔｙｅａｒｓ，ｓｅｖｅｒａｌｃｏｕｎｔｒｉｅｓ，ｓｕｃｈａｓＣｈｉｎａａｎｄｔｈｅＵｎｉｔｅｄＳｔａｔｅｓ，ａｎｄｈｉｇｈｔｅｃｈｃｏｍｐａｎｉｅｓ，ｓｕｃｈ\nａｓＧｏｏｇｌｅ，ｈａｖｅｉｎｃｒｅａｓｅｄｉｎｖｅｓｔｍｅｎｔｉｎａｒｔｉｆｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ．Ｄｅｅｐｌｅａｒｎｉｎｇｉｓｏｎｅｏｆｔｈｅｃｕｒｒｅｎｔａｒｔｉｆｉｃｉａｌｉｎ\nｔｅｌｌｉｇｅｎｃｅｒｅｓｅａｒｃｈｋｅｙａｒｅａｓ．Ｗｅａｎａｌｙｚｅａｎｄｓｕｍｍａｒｉｚｅｔｈｅｌａｔｅｓｔｐｒｏｇｒｅｓｓａｎｄｆｕｔｕｒｅｒｅｓｅａｒｃｈｄｉｒｅｃｔｉｏｎｓｏｆ\nｄｅｅｐｌｅａｒｎｉｎｇ．Ｆｉｒｓｔ，ｗｅｏｕｔｌｉｎｅｔｈｒｅｅｂａｓｉｃｍｏｄｅｌｓｏｆｄｅｅｐｌｅａｒｎｉｎｇ，ｗｈｉｃｈａｒｅｍｕｌｔｉｌａｙｅｒｐｅｒｃｅｐｔｒｏｎｓ，ｃｏｎｖ\nｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ，ａｎｄｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ．Ｏｎｔｈｉｓｂａｓｉｓ，ｗｅｆｕｒｔｈｅｒａｎａｌｙｚｅｔｈｅｅｍｅｒｇｉｎｇｎｅｗ\nｍｏｄｅｌｓｏｆｃｏｎｖｏｌｕｔｉｏｎｎｅｕｒａｌｎｅｔｗｏｒｋｓａｎｄｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ．Ｆｕｒｔｈｅｒｍｏｒｅ，ｗｅｓｕｍｍａｒｉｚｅｔｈｅａｐｐｌｉ\nｃａｔｉｏｎｓｏｆｄｅｅｐｌｅａｒｎｉｎｇｉｎｍａｎｙａｒｅａｓｏｆａｒｔｉｆｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ，ｉｎｃｌｕｄｉｎｇｓｐｅｅｃｈｐｒｏｃｅｓｓｉｎｇ，ｃｏｍｐｕｔｅｒｖｉｓｉｏｎ，\nａｎｄｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ．Ｆｉｎａｌｌｙ，ｗｅｄｉｓｃｕｓｓｔｈｅｅｘｉｓｔｉｎｇｐｒｏｂｌｅｍｓｏｆｄｅｅｐｌｅａｒｎｉｎｇａｎｄｐｒｏｖｉｄｅｔｈｅ\nｃｏｒｒｅｓｐｏｎｄｉｎｇｐｏｓｓｉｂｌｅｓｏｌｕｔｉｏｎｓ．\nＫｅｙｗｏｒｄｓ\nｄｅｅｐｌｅａｒｎｉｎｇ；\nｎｅｕｒａｌｎｅｔｗｏｒｋ；\nｍａｃｈｉｎｅｌｅａｒｎｉｎｇ；\nａｒｔｉｆｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ；\nｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋ；\nｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋ\n０　引言\n２０１６年３月，“人工智能”一词被写入中国“十三五”\n规划纲要，２０１６年１０月美国政府发布《美国国家人工智能\n研究与发展战略规划》文件．Ｇｏｏｇｌｅ、Ｍｉｃｒｏｓｏｆｔ、Ｆａｃｅｂｏｏｋ、\n百度、腾讯、阿里巴巴等各大互联网公司也纷纷加大对人\n工智能的投入．各类人工智能创业公司层出不穷，各种人\n工智能应用逐渐改变人类的生活．深度学习是目前人工智\n能的重点研究领域之一，应用于人工智能的众多领域，包\n括语音处理、计算机视觉、自然语言处理等．\n１９４３年，ＭｃＣｕｌｌｏｃｈ和Ｐｉｔｔｓ\n［１］提出ＭＰ神经元数学模\n型．１９５８年，第一代神经网络单层感知器由Ｒｏｓｅｎｂｌａｔｔ\n［２］\n提出，第一代神经网络能够区分三角形、正方形等基本形\n状，让人类觉得有可能发明出真正能感知、学习、记忆的\n智能机器．但是第一代神经网络基本原理的限制打破了人\n类的梦想，１９６９年，Ｍｉｎｓｋｙ\n［３］发表感知器专著：单层感知\n器无法解决异或ＸＯＲ问题；神经网络的特征层是固定的，\n是由人类设计的，此与真正智能机器的定义不相符．１９８６年，\nＨｉｎｔｏｎ等［４］提出第二代神经网络，将原始单一固定的特征\n层替换成多个隐藏层，激活函数采用Ｓｉｇｍｏｉｄ函数，利用\n误差的反向传播算法来训练模型，能有效解决非线性分类\n问题．１９８９年，Ｃｙｂｅｎｋｏ和Ｈｏｒｎｉｋ等［５－６］证明了万能逼近\n定理（ｕｎｉｖｅｒｓａｌａｐｐｒｏｘｉｍａｔｉｏｎｔｈｅｏｒｅｍ）：任何函数都可以被\n三层神经网络以任意精度逼近．同年，ＬｅＣｕｎ等［７－８］发明\n了卷积神经网络用来识别手写体，当时需要３天来训练模\n型．１９９１年，反向传播算法被指出存在梯度消失问题．此\n后十多年，各种浅层机器学习模型相继被提出，包括１９９５年\nＣｏｒｔｅｓ与Ｖａｐｎｉｋ［９］发明的支持向量机，神经网络的研究被\n搁置．２００６年，Ｈｉｎｔｏｎ等探讨大脑中的图模型［１０］，提出自\n编码器（ａｕｔｏｅｎｃｏｄｅｒ）来降低数据的维度［１１］，并提出用预训\n练的方式快速训练深度信念网［１２］，来抑制梯度消失问题．\nＢｅｎｇｉｏ等［１３］证明预训练的方法还适用于自编码器等无监\n督学习，Ｐｏｕｌｔｎｅｙ等［１４］用基于能量的模型来有效学习稀疏\n表示．这些论文奠定了深度学习的基础，从此深度学习进\n入快速发展期．２０１０年，美国国防部ＤＡＲＰＡ计划首次资\n助深度学习项目．２０１１年，Ｇｌｏｒｏｔ等［１５］提出ＲｅＬＵ激活函\n数，能有效抑制梯度消失问题．深度学习在语音识别上最\n先取得重大突破，微软和谷歌［１６－１７］先后采用深度学习将\n语音识别错误率降低至２０％～３０％，是该领域１０年来最大\n突破．２０１２年，Ｈｉｎｔｏｎ和他的学生将ＩｍａｇｅＮｅｔ\n［１８］图片分类\n问题的Ｔｏｐ５错误率由２６％降低至１５％［１９］，从此深度学习\n进入爆发期．Ｄａｕｐｈｉｎ等［２０］在２０１４年，Ｃｈｏｒｏｍａｎｓｋａ等［２１］\n在２０１５年分别证明局部极小值问题通常来说不是严重的问\n题，消除了笼罩在神经网络上的局部极值阴霾．深度学习发\n展历史如图１所示．图１中的空心圆圈表示深度学习热度\n上升与下降的关键转折点，实心圈圈的大小表示深度学习\n在这一年的突破大小．斜向上的直线表示深度学习热度正\n处于上升期，斜向下的直线表示深度学习热度处于下降期．\n图１　深度学习发展历史\nＦｉｇ．１　Ｔｈｅｈｉｓｔｏｒｙｏｆｄｅｅｐｌｅａｒｎｉｎｇ\n　　深度学习其实是机器学习的一部分，机器学习经历了\n从浅层机器学习到深度学习两次浪潮［２２］．深度学习模型\n与浅层机器学习模型之间存在重要区别．浅层机器学习模\n型不使用分布式表示（ｄｉｓｔｒｉｂｕｔｅｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｓ）［２３］，而且\n需要人为提取特征，模型本身只是根据特征进行分类或预\n测，人为提取的特征好坏很大程度上决定了整个系统的好\n坏．特征提取需要专业的领域知识，而且特征提取、特征\n工程需要花费大量时间．深度学习是一种表示学习［２４］，能\n够学到数据更高层次的抽象表示，能够自动从数据中提取\n特征［２５－２６］．而且深度学习里的隐藏层相当于是输入特征\n的线性组合，隐藏层与输入层之间的权重相当于输入特征\n在线性组合中的权重［２７］．另外，深度学习的模型能力会随\n着深度的增加而呈指数增长［２８］．\n１　基本网络结构\n１．１　多层感知器\n多层感知器（ｍｕｌｔｉｌａｙｅｒｐｅｒｃｅｐｔｉｏｎ，ＭＬＰ）［２］也叫前向\n传播网络、深度前馈网络，是最基本的深度学习网络结\n构．ＭＬＰ由若干层组成，每一层包含若干个神经元．激活\n函数采用径向基函数的多层感知器被称为径向基网络（ｒａ\nｄｉａｌｂａｓｉｓｆｕｎｃｔｉｏｎｎｅｔｗｏｒｋ）．多层感知器的前向传播如图２\n所示．\n图２　多层感知器的前向传播\nＦｉｇ．２　ＴｈｅｆｏｒｗａｒｄｐｒｏｐａｇａｔｉｏｎｏｆＭＬＰ\n６\n８\n３\n信息与控制　　　　　　　　　　　　　　　　　　４７卷\nＭＬＰ的前向传播公式如式（１）、式（２）所示：\nｚ\nｌ＋１\nｉ\n＝∑\nｊ\nＷｌ\nｊｉｙ\nｌ\nｊ＋ｂ\nｌ\nｉ　\n（１）\nｙ\nｌ＋１\nｉ\n＝ｆ（ｚ\nｌ＋１\nｉ）\n（２）\n其中，ｙ\nｌ\nｊ是第ｌ层的第ｊ个神经元的输出，ｚ\nｌ＋１\nｉ\n是第ｌ＋１层\n的第ｉ个神经元被激活函数作用之前的值，Ｗｌ\nｊｉ是第ｌ层的\n第ｊ个神经元与第ｌ＋１层的第ｉ个神经元之间的权重，ｂ\nｌ\nｉ\n是偏置，ｆ（·）是非线性激活函数，常见的有径向基函数、\nＲｅＬＵ、ＰＲｅＬＵ、Ｔａｎｈ、Ｓｉｇｍｏｉｄ等．\n如果采用均方误差（ｍｅａｎｓｑｕａｒｅｄｅｒｒｏｒ），则损失函数为\nＪ＝１\n２∑\nｉ\n（ｙ\nＬ\nｉ－ｙ\nｉ）２　\n（３）\n其中，ｙ\nＬ\nｉ是神经网络最后一层第ｉ个神经元的输出，ｙ\nｉ是\n第ｉ个神经元的真实值．神经网络训练的目标是最小化损\n失函数，优化方法通常采用批梯度下降法．\n１．２　卷积神经网络\n卷积神经网络（ｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋ，ＣＮＮ）［２９］\n适合处理空间数据，在计算机视觉领域应用广泛．一维卷\n积神经网络也被称为时间延迟神经网络（ｔｉｍｅｄｅｌａｙｎｅｕｒａｌ\nｎｅｔｗｏｒｋ），可以用来处理一维数据．ＣＮＮ的设计思想受到\n了视觉神经科学的启发，主要由卷积层（ｃｏｎｖｏｌｕｔｉｏｎａｌｌａｙ\nｅｒ）和池化层（ｐｏｏｌｉｎｇｌａｙｅｒ）组成．卷积层能够保持图像的\n空间连续性，能将图像的局部特征提取出来．池化层可以\n采用最大池化（ｍａｘｐｏｏｌｉｎｇ）或平均池化（ｍｅａｎｐｏｏｌｉｎｇ），\n池化层能降低中间隐藏层的维度，减少接下来各层的运算\n量，并提供了旋转不变性．卷积与池化操作示意图如图３\n所示，图中采用３×３的卷积核和２×２的ｐｏｏｌｉｎｇ．\n最早期的卷积神经网络模型是ＬｅＣｕｎ等［２９］在１９９８年\n提出的ＬｅＮｅｔ５，其结构图如图４所示．输入的ＭＮＩＳＴ图\n片大小为３２×３２，经过卷积操作，卷积核大小为５×５，得\n到２８×２８的图片，经过池化操作，得到１４×１４的图片，\n然后再卷积再池化，最后得到５×５的图片．接着依次有\n１２０、８４、１０个神经元的全连接层，最后经过Ｓｏｆｔｍａｘ函数\n作用，得到数字０～９的概率，取概率最大的作为神经网络\n的预测结果．随着卷积和池化操作，网络越高层，图片大\n小越小，但图片数量越多．\n图３　卷积与池化操作示意图\nＦｉｇ．３　Ｔｈｅｉｌｌｕｓｔｒａｔｉｏｎｆｏｒｃｏｎｖｏｌｕｔｉｏｎａｎｄｐｏｏｌｉｎｇ\n图４　ＬｅＮｅｔ５结构图\nＦｉｇ．４　ＴｈｅｓｔｒｕｃｔｕｒｅｏｆＬｅＮｅｔ５\n　　ＣＮＮ提供了视觉数据的分层表示，ＣＮＮ每层的权重实\n际上学到了图像的某些成分，越高层，成分越具体．ＣＮＮ将\n原始信号经过逐层的处理，依次识别出部分到整体．可以对\nＣＮＮ进行可视化来理解ＣＮＮ［３０］：ＣＮＮ的第二层能识别出\n拐角、边和颜色；第三层能识别出纹理、文字等更复杂的不\n变性；第四层能识别出狗的脸、鸟的腿等具体部位；第五层\n能识别出键盘、狗等具体物体．比如说人脸识别，ＣＮＮ先是\n识别出点、边、颜色、拐角，再是眼角、嘴唇、鼻子，再是整\n张脸．ＣＮＮ容易在ＦＰＧＡ等硬件上实现并获得加速［３１］；\nＣＮＮ同一卷积层内权值共享，都为卷积核的权重．ＣＮＮ的局\n部连接、权值共享、池化操作等特性减少了模型参数［３２］，降\n低了网络复杂性，也提供了平移、扭曲、旋转、缩放不变性．\n１．３　循环神经网络\n循环神经网络（ｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ，ＲＮＮ）［４］适合\n７\n８\n３\n４期\n张荣，等：深度学习研究综述\n处理时序数据，在语音处理、自然语言处理领域应用广\n泛，人类的语音和语言天生具有时序性．ＲＮＮ及其展开图\n如图５所示．\n图５　ＲＮＮ及其展开图\nＦｉｇ．５　ＲＮＮａｎｄｉｔｓｕｎｆｏｌｄｉｎｇ\nＲＮＮ的前向传播公式如式（４）～（６）所示：\nｚ\nｔ\nｈ＝∑\nＩ\nｉ＝１\nｗｉｈｘ\nｔ\nｉ＋∑\nＨ\nｈ′＝１\nｗｈ′ｈａｔ－１\nｈ′ 　\n（４）\nａｔ\nｈ＝ｆ\nｈ（ｚ\nｔ\nｈ）\n（５）\nｙ\nｔ\nｋ＝∑\nＨ\nｈ＝１\nｗｈｋａｔ\nｈ\n（６）\n其中，ｘ\nｔ\nｉ是ｔ时刻输入层的第ｉ个神经元，ａｔ－１\nｈ′ 是ｔ－１时刻\n隐藏层的第ｈ′个神经元，ｚ\nｔ\nｈ是ｔ时刻隐藏层第ｈ个神经元\n被激活函数作用之前的值，ｙ\nｔ\nｋ是ｔ时刻输出层的第ｋ个神\n经元，ｗｉｈ、ｗｈ′ｈ、ｗｈｋ分别是输入层与隐藏层、隐藏层与隐\n藏层、隐藏层与输出层之间的权重，ｆ\nｈ（·）是非线性激活\n函数．\nＲＮＮ将上一时刻隐藏层的输出也作为这一时刻隐藏\n层的输入，能够利用过去时刻的信息，即ＲＮＮ具有记忆\n性．ＲＮＮ在各个时间上共享权重，大幅减少了模型参数．\n但ＲＮＮ训练难度依然较大，因此Ｓｕｔｓｋｅｖｅｒ等［３３］和Ｐａｓｃａ\nｎｕ等［３４］都对ＲＮＮ的训练方法进行了改进．\n２　网络结构改进\n２．１　卷积神经网络改进\nＩｍａｇｅＮｅｔ\n［１８］比赛（ＩｍａｇｅＮｅｔｌａｒｇｅｓｃａｌｅｖｉｓｕａｌｒｅｃｏｇｎｉｔｉｏｎ\nｃｏｍｐｅｔｉｔｉｏｎ，ＩＬＳＶＲＣ）极大促进了卷积神经网络的发展，不\n断有新发明的卷积神经网络刷新了ＩｍａｇｅＮｅｔ成绩．从\n２０１２年的ＡｌｅｘＮｅｔ\n［１９］，到２０１３年的ＺＦＮｅｔ\n［３０］，２０１４年的\nＶＧＧＮｅｔ\n［３５］、ＧｏｏｇＬｅＮｅｔ\n［３６］，再到２０１５年的ＲｅｓＮｅｔ\n［３７］，网\n络层数不断增加，模型能力也不断增强．ＡｌｅｘＮｅｔ第一次展\n现了深度学习的强大能力，ＺＦＮｅｔ是可视化理解卷积神经\n网络的结果，ＶＧＧＮｅｔ表明网络深度能显著提高深度学习\n的效果，ＧｏｏｇＬｅＮｅｔ第一次打破了卷积层池化层堆叠的模\n式，ＲｅｓＮｅｔ首次成功训练了深度达到１５２层的神经网络．\nＣＮＮ应用于物体检测的主流方法是ＲＣＮＮ［３８］及其之后的\n改进ＦａｓｔＲＣＮＮ［３９］、ＦａｓｔｅｒＲＣＮＮ［４０］、ＭａｓｋＲＣＮＮ［４１］．\n改进的过程其实是用深度学习模型来替代浅层机器学习模\n型的过程，实现端到端的训练，速度也越来越快．另外，\n网中网结构［４２］创新性地提出在网络里嵌套网络的想法；\n空间变换网络［４３］说明模型效果的提升不一定需要改变网\n络结构，还可以通过对输入数据进行变换．\n１）ＡｌｅｘＮｅｔ．Ｈｉｎｔｏｎ为了验证深度学习的有效性，２０１２年\n参加ＩＬＳＶＲＣ并取得第一名，所用到的神经网络模型被称为\nＡｌｅｘＮｅｔ．ＡｌｅｘＮｅｔ网络包含５层卷积层、ｍａｘｐｏｏｌｉｎｇ层和\nｄｒｏｐｏｕｔ层，接着连接３层全连接层，最后输出层有１０００个\n神经元，对应１０００个分类，经过Ｓｏｆｔｍａｘ函数作用后得到\n每一类的概率．ＡｌｅｘＮｅｔ采用平移、翻转、截取图片一部分\n等方式来增加训练数据，用ｄｒｏｐｏｕｔ来防止过拟合，用带有\n动量和权重衰减的批梯度下降方法来训练模型．ＡｌｅｘＮｅｔ\n用两块ＧＰＵ并行训练了６天，而且采用ＲｅＬＵ作为激活函\n数比用Ｔａｎｈ训练时间缩短了６倍．ＡｌｅｘＮｅｔ所采用的这一\n系列技术现在仍然被广泛使用．\n２）ＺＦＮｅｔ．ＺＦＮｅｔ是ＩＬＳＶＲＣ２０１３冠军，错误率为\n１１．２％，ＺＦＮｅｔ可以认为是ＡｌｅｘＮｅｔ的微调，网络层数仍为\n８．Ｚｅｉｌｅｒ和Ｆｅｒｇｕｓ利用反卷积网络对ＣＮＮ进行可视化来\n理解ＣＮＮ每一层的作用，可视化帮助找到了比ＡｌｅｘＮｅｔ效\n果更好的网络结构ＺＦＮｅｔ．ＺＦＮｅｔ所需的训练数据更少，\nＡｌｅｘＮｅｔ用１５００万张图片来训练模型，而ＺＦＮｅｔ只用了\n１３０万张图片．ＡｌｅｘＮｅｔ第一层卷积核为１１×１１，而ＺＦＮｅｔ\n为７×７，卷积核变小使得ＺＦＮｅｔ在第一层能保留更多的\n相关信息．\n３）ＶＧＧＮｅｔ．Ｓｉｍｏｎｙａｎ等逐次在ＡｌｅｘＮｅｔ中增加卷积\n层，比较６种不同深度的网络，研究网络深度的影响．结\n果表明神经网络越深，效果越好，当增加到１６、１９层时，\n效果提升明显，１９层的网络被称为ＶＧＧ１９．ＶＧＧＮｅｔ严格\n采用３×３的卷积核，步长（ｓｔｒｉｄｅ）和填补（ｐａｄｄｉｎｇ）都为１；\n采用２×２的ｍａｘｐｏｏｌｉｎｇ，步长为２．相比于ＺＦＮｅｔ７×７的\n卷积核，ＶＧＧＮｅｔ卷积核大小只有３×３，使得模型参数更\n８\n８\n３\n信息与控制　　　　　　　　　　　　　　　　　　４７卷\n少，而且连续两层的卷积层使其有７×７卷积核的效果，之\n后人们通常也使用３×３的卷积核．ＶＧＧＮｅｔ模型用Ｃａｆｆｅ\n来实现，利用图片抖动来增加训练数据，在图片分类和物\n体定位任务方面都有很好的效果．\n４）ＧｏｏｇＬｅＮｅｔ．ＧｏｏｇＬｅＮｅｔ是ＩＬＳＶＲＣ２０１４冠军，ｔｏｐ５错\n误率为６．７％，其网络层数为２２层．ＧｏｏｇＬｅＮｅｔ表明ＣＮＮ\n不一定是要将卷积层、池化层依次堆叠起来．ＧｏｏｇＬｅＮｅｔ\n采用Ｉｎｃｅｐｔｉｏｎ模块，模块里的卷积层、池化层是并行的，\n所以不用选择这一层是用卷积层还是池化层．在Ｉｎｃｅｐｔｉｏｎ\n模块的最后不直接将所有神经元“拉直”排成一排，而是采\n用池化将７×７×１０２４变成１×１×１０２４，参数量减少到１／４９，\nＧｏｏｇＬｅＮｅｔ总的参数量只有ＡｌｅｘＮｅｔ的１／１２．使用训练好\n的模型对图片进行分类时，对同一张图片的多张变形图片\n输出Ｓｏｆｔｍａｘ概率后求平均作为此图片的概率．\n５）深度残差网络（ＲｅｓＮｅｔ）．ＲｅｓＮｅｔ是ＩＬＳＶＲＣ２０１５\n冠军，同一网络赢得图片分类、物体定位、物体检测三项\n任务冠军，图像分类任务错误率为３．５７％，超过人类错误\n率５．１％．ＲｅｓＮｅｔ网络层数达到１５２层，甚至１０００层．深\n层网络有梯度消失的问题，ＲｅｓＮｅｔ在两层或多层之间直接\n加上线性连通通路，即构成了残差模块，保证梯度能通过\n线性通路传到底层，也使得输入层的信息能直接保留到后\n面网络层．\n６）ＲＣＮＮ．Ｇｉｒｓｈｉｃｋ等提出ＲＣＮＮ用于完成计算机视\n觉中的物体检测任务．物体检测目标是将图片中所有物体\n用方框框出来，此任务可以分成两个子任务，首先是生成方\n框将物体框出来，然后对框出来的物体进行分类判断是具\n体哪个物体．ＲＣＮＮ采用选择性搜索（ｓｅｌｅｃｔｉｖｅｓｅａｒｃｈ）方法\n生成大约２０００个方框，用已训练好的ＣＮＮ比如ＡｌｅｘＮｅｔ\n对每一个方框内的图片提取出特征，再将特征放进ＳＶＭ\n进行分类，同时将特征放入回归器中得到更精确的候选\n方框．\n７）ＦａｓｔＲＣＮＮ．ＦａｓｔＲＣＮＮ将ＲＣＮＮ中ＣＮＮ提取特\n征、ＳＶＭ分类、回归这三个过程放在一起，形成端到端整\n体的模型，速度和准确率都得到提升．ＦａｓｔＲＣＮＮ的输入\n数据是整张图片和若干方框．首先用若干卷积层、池化层\n处理整张图片得到特征图（ｆｅａｔｕｒｅｍａｐ）；用兴趣区域池化\n层（ｒｅｇｉｏｎｏｆｉｎｔｅｒｅｓｔｐｏｏｌｉｎｇｌａｙｅｒ）处理每个方框得到固定大\n小的特征图．然后接若干全连接层，最后同时输出是某个\n类别的概率、确定每个类的方框的４个值．\n８）ＦａｓｔｅｒＲＣＮＮ．ＦａｓｔｅｒＲＣＮＮ首先用卷积层、池化\n层处理整张图片得到特征图，在此特征图上用ｒｅｇｉｏｎｐｒｏ\nｐｏｓａｌｎｅｔｗｏｒｋ来生成方框，其它操作跟ＦａｓｔＲＣＮＮ一样．\n即ＦａｓｔｅｒＲＣＮＮ将生成方框的方法也换成了深度学习模\n型，并由原来在整张图上生成改成在更小的特征图上生\n成，使得模型训练速度进一步加快．\n９）ＭａｓｋＲＣＮＮ．ＭａｓｋＲＣＮＮ在ＦａｓｔｅｒＲＣＮＮ基础上\n增加语义分割的并行分支，在原来生成方框、分类、回归\n任务基础上增加分割任务，能同时实现物体检测和语义分\n割．ＭａｓｋＲＣＮＮ的基础网络使用ＲｅｓＮｅＸｔ１０１和ＦＰＮ（ｆｅａ\nｔｕｒｅｐｙｒａｍｉｄｎｅｔｗｏｒｋ）［４４］．语义分割任务的误差由基于单像\n素Ｓｏｆｔｍａｘ多项式交叉熵变成了基于单像素Ｓｉｇｍｏｉｄ二值交\n叉熵．ＭａｓｋＲＣＮＮ加入了ＲｏＩＡｌｉｇｎ层，相当于对特征图\n进行插值．\n１０）网中网结构（ｎｅｔｗｏｒｋｉｎｎｅｔｗｏｒｋ，ＮＩＮ）．网中网结\n构用微型神经网络比如多层感知器，来代替ＣＮＮ中的卷\n积核，形成了神经网络里嵌套着微型神经网络的结构．因\n为已经用微型网络进行了复杂的局部建模，所以ＣＮＮ中\n最后的全连接层可以由全局ｍｅａｎｐｏｏｌｉｎｇ来代替．这使得\n模型参数大大减少，防止了过拟合，也增加了可解释性，\nＮＩＮ的参数有２９００万个，是ＡｌｅｘＮｅｔ的１／１０．\n１１）空间变换网络（ｓｐａｔｉａｌｔｒａｎｓｆｏｒｍｅｒｎｅｔｗｏｒｋｓ，ＳＴＮｓ）．\n空间变换网络通过变换输入的图片来提升准确率，而不是\n通过改变网络结构．ＳＴＮｓ里主要包含空间变换模块，其又\n由本地化网络（ｌｏｃａｌｉｚａｔｉｏｎｎｅｔｗｏｒｋ）、网格生成器（ｇｒｉｄｇｅｎ\nｅｒａｔｏｒ）、采样器（ｓａｍｐｌｅｒ）三部分组成．ＳＴＮｓ对于输入的图\n片，先用本地化网络来预测需要进行的变换，然后网格生\n成器和采样器对图片实施变换，变换得到的图片被放到\nＣＮＮ中进行分类．ＳＴＮｓ的鲁棒性很好，具有平移、伸缩、\n旋转、扰动、弯曲等空间不变性．\n１２）其它卷积神经网络改进．此外，还有其它卷积神\n经网络改进，包括ｄｅｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ\n［４５］、ｓｔａｃｋｅｄｃｏｎ\nｖｏｌｕｔｉｏｎａｌａｕｔｏｅｎｃｏｄｅｒｓ\n［４６］、ＳＲＣＮＮ［４７］、ＯｖｅｒＦｅａｔ\n［４８］、Ｆｌｏｗ\nＮｅｔ\n［４９］、ＭｒＣＮＮ［５０］、ＦＶＣＮＮ［５１］、ＤｅｅｐＥｄｇｅ\n［５２］、ＤｅｅｐＣｏｎ\nｔｏｕｒ\n［５３］、ｄｅｅｐｐａｒｓｉｎｇｎｅｔｗｏｒｋ［５４］、ＢｏｘＳｕｐ［５５］、ＴＣＮＮ［５６］、\n３维ＣＮＮ［５７］等．\n２．２　循环神经网络改进\n循环神经网络存在梯度消失或者梯度爆炸问题［５８］，\n无法利用过去长时间的信息，例如当激活函数是Ｓｉｇｍｏｉｄ\n函数时，其导数是个小于１的数，多个小于１的导数连乘\n就会导致梯度消失问题．ＬＳＴＭ［５９］、分层ＲＮＮ［６０－６１］都是针\n对这个问题的解决方案．ＲＮＮ一般只能处理时间序列等一\n维数据，多维ＲＮＮ［６２］被提出来处理图像等多维数据．针\n对很多自然语言处理任务都需要利用上下文信息，双向\nＲＮＮ［６３］通过双向处理同一个序列来利用上下文信息．\nＲＮＮ存在训练算法复杂、计算量大的问题，回声状态网\n络［６４］不需要反复计算梯度就能达到很高的精度，为ＲＮＮ\n的训练提供了新思路．循环神经网络缺乏推理功能，无法\n完成需要推理的任务，神经图灵机［６５］与记忆网络［６６］通过\n增加记忆模块来解决此问题．\n１）长短时记忆网络（ｌｏｎｇｓｈｏｒｔｔｅｒｍｍｅｍｏｒｙ，ＬＳＴＭ）．\n长短时记忆网络用ＬＳＴＭ单元替代ＲＮＮ中的神经元，在输\n入、输出、忘记过去信息上分别加了输入门、输出门、遗\n忘门来控制允许多少信息通过．ＬＳＴＭ有单元状态（ｃｅｌｌ\nｓｔａｔｅ）和隐藏状态（ｈｉｄｄｅｎｓｔａｔｅ）两个传输状态，单元状态\n随时刻改变缓慢，而不同时刻的隐藏状态可能会很不同．\nＬＳＴＭ建立了门机制来达到旧时刻输入与新时刻输入之间\n的权衡，本质是根据训练目标，调整出记忆的重点然后进\n行整串编码．ＬＳＴＭ能够记住需要长时间记忆的，忘记不\n重要的，能缓解梯度消失、梯度爆炸问题，在较长的序列\n上比ＲＮＮ有更好的表现．\n９\n８\n３\n４期\n张荣，等：深度学习研究综述\n２）ＧＲＵ（ｇａｔｅｄｒｅｃｕｒｒｅｎｔｕｎｉｔ）．ＧＲＵ［６７］是ＬＳＴＭ的轻\n量级变体，只有两个门：更新门和重置门．更新门决定保\n留过去多少信息，以及从输入层输入多少信息；重置门与\nＬＳＴＭ里的遗忘门类似．ＧＲＵ没有输出门，所以总是输出\n完整状态．ＧＲＵ在所有的地方使用了更少的连接，参数更\n少，所以训练速度更容易更快．\n３）分层ＲＮＮ（ｈｉｅｒａｒｃｈｉｃａｌＲＮＮ）．分层ＲＮＮ利用了时间\n依赖是分层结构化的先验知识，长时间依赖能够由一个变量\n来表示．分层多尺度ＲＮＮ（ｈｉｅｒａｒｃｈｉｃａｌｍｕｌｔｉｓｃａｌｅＲＮＮ）［６１］\n通过编码不同时间尺度的时间依赖来捕捉序列中潜在的分\n层结构．分层ＲＮＮ、分层多尺度ＲＮＮ都能缓解梯度消失\n问题．\n４）双向ＲＮＮ（ｂｉｄｉｒｅｃｔｉｏｎａｌＲＮＮ）．双向ＲＮＮ向前、向\n后双向处理同一个序列，相当于有两个ＲＮＮ，然后接着同\n一个输出层．向前和向后隐藏层之间没有连接，避免形成信\n息回路．比如对于自然语言处理任务，单词的上文跟下文都\n会对单词有影响，双向ＲＮＮ就能利用单词的上下文信息．\n双向ＲＮＮ能同时捕捉到前后的信息，双向ＬＳＴＭ［６８－６９］、双\n向ＧＲＵ［７０］也被证明非常有效．\n５）多维ＲＮＮ（ｍｕｌｔｉｄｉｍｅｎｓｉｏｎａｌＲＮＮ）．ＲＮＮ适合处理\n时间序列等一维数据，多维ＲＮＮ能够处理多维数据，包括\n图像、视频、医学成像等．多维ＲＮＮ的思想是将多维数据\n按照一定顺序排列成一维数据，进而能用ＲＮＮ处理，当然\n这种排列顺序需要保持多维数据的空间连续性．\n６）回声状态网络（ｅｃｈｏｓｔａｔｅｎｅｔｗｏｒｋ，ＥＳＮ）．ＥＳＮ将\nＲＮＮ中的隐藏层变成了存储池，存储池内的神经元之间随\n机连接，存储池即隐藏层不是整齐划一的神经元网络层．\n输入会回荡在存储池中，就像有回声一样，故此网络被称\n为回声状态网络．ＥＳＮ输入层到隐藏层、隐藏层到隐藏层\n的权重随机初始化后固定不变．模型训练的时候，只更新\n隐藏层到输出层的权重，即变成了线性回归问题，所以训\n练速度快．\n７）神经图灵机（ｎｅｕｒａｌＴｕｒｉｎｇｍａｃｈｉｎｅｓ）．神经图灵机\n本质是使用外部存储矩阵来进行交互的ＲＮＮ，整体系统与\n图灵机类似，用神经网络的方法建立了“输入纸带到输出\n纸带的映射”．控制器网络（ｃｏｎｔｒｏｌｌｅｒｎｅｔｗｏｒｋ）通过并行的\n读写头改变外部存储矩阵的值来读取或写入记忆．控制器\n网络会接收ＲＮＮ的输入，也会输出给ＲＮＮ．\n８）记忆网络（ｍｅｍｏｒｙｎｅｔｗｏｒｋｓ）．记忆网络主要包含记\n忆单元和输入、生成、输出、应答四个模块．记忆单元其实\n是一个数组，数组的每一个元素保存一句话的记忆．记忆网\n络可以回答需要复杂推理的问题［７１］，实现自动问答．输入\n的文本被输入模块编码成特征向量，生成模块根据此向量\n对记忆单元进行读写操作，对记忆进行更新．输出模块会将\n记忆按照与问题的相关程度加权组合得到输出向量，最终\n应答模块根据输出向量编码生成问题的答案．神经图灵机\n与记忆网络都适合完成需要推理与符号处理的任务．\n９）其它循环神经网络改进．其它循环神经网络改进\n也主要是两个方向，一个是改进网络结构，另一个是额外\n加入记忆模块．结构方面有：ＧＦＲＮＮ［７２］、ｓｔａｃｋｅｄＲＮＮ［７３］、\nＴｒｅｅＳｔｒｕｃｔｕｒｅｄＬＳＴＭ［７４］、ｇｒｉｄＬＳＴＭ［７５］、ｓｅｇｍｅｎｔａｌＲＮＮ［７６］、\nｓｅｑ２ｓｅｑｆｏｒｓｅｔｓ\n［７７］等．记忆方面有：ｎｅｕｒａｌＧＰＵ［７８］、ｐｏｉｎｔｅｒ\nｎｅｔｗｏｒｋ［７９］、ｄｅｅｐａｔｔｅｎｔｉｏｎｒｅｃｕｒｒｅｎｔＱＮｅｔｗｏｒｋ［８０］、ｄｙｎａｍｉｃ\nｍｅｍｏｒｙｎｅｔｗｏｒｋｓ\n［８１］等．\n２．３　其它改进\n有些创新能同时改进卷积神经网络和循环神经网络，\n比如批标准化与ａｔｔｅｎｔｉｏｎ等．另外有些有意思的应用同时\n用到了卷积神经网络与循环神经网络．\n１）批标准化（ｂａｔｃｈｎｏｒｍａｌｉｚａｔｉｏｎ）．训练过程权重更新\n会改变网络层输入的分布，导致需要更低的学习率与精细\n的权重初始化，训练时间也会变长．对每一网络层的输入\n进行批标准化能保证输入分布的统一，并在一定程度上能\n替代ｄｒｏｐｏｕｔ．批标准化首先被应用于卷积神经网络［８２］，\n而后也被应用于循环神经网络［８３］中．\n２）基于ａｔｔｅｎｔｉｏｎ的模型［８４］．人在看东西时目光会集\n中于感兴趣的部分，当翻译文章时我们关注当前翻译的那\n部分，而不是整篇文章，这些都说明人类系统中存在注意\n力（ａｔｔｅｎｔｉｏｎ）．类似地，ａｔｔｅｎｔｉｏｎ机制使得深度学习模型能\n够只集中关注输入数据中最为重要的一部分，相应的模型\n有基于ａｔｔｅｎｔｉｏｎ的ＣＮＮ［８５］、基于ａｔｔｅｎｔｉｏｎ的ＲＮＮ［８６］和基\n于ａｔｔｅｎｔｉｏｎ的ＬＳＴＭ［８７］．Ａｔｔｅｎｔｉｏｎ其实是在网络中加入了关\n注区域的移动、缩放、旋转机制，此采用强化学习来实现．\n３）卷积神经网络与循环神经网络结合．卷积神经网\n络与循环神经网络结合可以做出很有意思的应用，比如根\n据图片生成描述文字［８８］等．根据图片生成描述文字，首先\n是用卷积神经网络对图片信息进行编码，得到其固定长度\n的向量表示．然后用循环神经网络对此向量进行解码，生\n成图片对应的描述文字．\n３　深度学习典型应用\n３．１　语音处理\n深度学习最先在语音处理领域取得突破性进展［１６］，\n无论是在标准小数据集上［８９］还是大数据集上［１７］．语音处\n理领域主要有两大任务：语音识别和语音合成．深度学习\n广泛应用于语音识别［９０］中，Ｇｏｏｇｌｅ\n［９１］推出端到端的语音识\n别系统、百度［９２］推出语音识别系统ＤｅｅｐＳｐｅｅｃｈ２．２０１６年，\n微软［９３］在日常对话数据上的语音识别准确率达到５．９％，\n首次达到人类水平．各大公司也都用深度学习来实现语音\n合成，包括Ｇｏｏｇｌｅ\n［９４］、Ａｐｐｌｅ\n［９５］、科大讯飞［９６］等．Ｇｏｏｇｌｅ\nＤｅｅｐＭｉｎｄ［９７］提出并行化ＷａｖｅＮｅｔ模型来进行语音合成，\n百度［９８］推出产品级别的实时语音合成系统ＤｅｅｐＶｏｉｃｅ３．\n３．２　计算机视觉\n深度学习被广泛应用于计算机视觉各种任务，包括交\n通标志检测和分类［９９］、人脸识别［１００］、人脸检测［１０１］、图像\n分类［３７］、多尺度变换融合图像［１０２］、物体检测［４１］、图像语\n义分割［１０３］、实时多人姿态估计［１０４］、行人检测［１０５］、场景\n识别［１０６］、物体跟踪［５６］、端到端的视频分类［１０７］、视频里的\n人体动作识别［１０８］等．另外，还有一些很有意思的应用，如\n给黑白照片自动着彩色［１０９］、将涂鸦变成艺术画［１１０］、艺术\n风格转移［１１１］、去掉图片里的马赛克［１１２］等．牛津大学和\n０\n９\n３\n信息与控制　　　　　　　　　　　　　　　　　　４７卷\nＧｏｏｇｌｅＤｅｅｐＭｉｎｄ［１１３］还共同提出了ＬｉｐＮｅｔ来读唇语，准确\n率达到９３％，远超人类５２％的平均水平．\n３．３　自然语言处理\nＮＥＣＬａｂｓＡｍｅｒｉｃａ\n［１１４］最早将深度学习应用于自然语\n言处理领域．目前处理自然语言时通常先用ｗｏｒｄ２ｖｅｃ\n［１１５］\n将单词转化成词向量，其可以作为单词的特征［４］．自然语\n言处理领域各种任务广泛用到了深度学习技术，包括词性\n标注［８１］、依存关系语法分析［１１６］、命名体识别［１１７］、语义角\n色标注［１１８］、只用字母的分布式表示来学习语言模型［１１９］、\n用字母级别的输入来预测单词级别的输出［１１９］、Ｔｗｉｔｔｅｒ情\n感分析［１２０］、中文微博情感分析［１２１］、文章分类［１２２］、机器翻\n译［１２３］、阅读理解［１２４］、自动问答［７１－８１］、对话系统［１２５］等．\n３．４　其它\n在生物信息学方面，深度学习能够被用来预测药物分\n子的活动［１２６］，预测人眼停留部位［１２７］，预测非编码ＤＮＡ\n基因突变对基因表达与疾病的影响［１２８］．金融行业积累了\n大量的数据，故深度学习在金融方面也有众多应用，包括\n金融市场预测［１２９］、证券投资组合［１３０］、保险流失预测［１３１］\n等，也相应涌现出一批金融科技创业公司．另外，基于深\n度学习的实时发电调度算法［１３２］能在满足实时发电任务的\n前提下，使机组总污染物排放量降低，达到节能减排的目\n的．深度学习还可以诊断电动潜油柱塞泵的故障［１３３］，避\n免故障事故的发生，有效延长检泵周期．深度学习应用于\n强非线性、复杂的化工过程软测量建模［１３４］中也能获得很\n好的精度．\n４　深度学习目前问题及进一步研究方向\n虽然深度学习使得诸多领域取得突破性进展，但是深\n度学习仍然存在一些问题，攻克解决这些问题是学者们的\n进一步研究方向，本文针对这些问题也探讨了相应的可能\n解决思路．问题被分为训练问题、落地问题、功能问题和\n领域问题，训练问题指的是深度学习训练时间太长等问\n题，落地问题指的是限制了深度学习实际落地应用的问\n题，功能问题指的是深度学习目前还不能很好完成的任\n务，领域问题指的是针对计算机视觉和自然语言处理特定\n领域的问题．\n４．１　训练问题\n１）训练时间太长，需要大量训练计算资源．在ＷＭＴ′１４\n英语到法语的数据集上，Ｇｏｏｇｌｅ\n［１３５］用９６块ＮＶＩＤＩＡＫ８０ＧＰＵ\n需要训练６天来得到基本的机器翻译模型，另外还需要\n３天来微调进一步改进模型．这还只是训练一次模型，再\n加上需要调各种超参数，总的训练时间非常长，而且９６块\nＫ８０ＧＰＵ成本也非常高．深度学习模型需要的计算资源太\n多了，训练时间也太长了，需要全新的硬件、算法、系统\n设计来加速模型的训练．例如硬件方面不只采用具有通用\n性的ＧＰＵ，还可以采用高性能低功耗的可编程可配置型\nＦＰＧＡ芯片、为了某种特定需求而专门定制的ＡＳＩＣ芯片．\nＧｏｏｇｌｅ为ＴｅｎｓｏｒＦｌｏｗ深度学习框架专门设计了ＡＳＩＣ芯片\nＴＰＵ及二代ＣｌｏｕｄＴＰＵ．自动驾驶领域也倾向于采用ＡＳＩＣ\n芯片，Ｎｖｉｄｉａ发布针对Ｌ５级全自动驾驶的ＡＩ处理器Ｄｒｉｖｅ\nＰＸＰｅｇａｓｕｓ．\n２）梯度消失问题，训练难度大．深层模型的训练难度\n非常大，网络层数太多的模型存在梯度消失问题．激活函\n数例如Ｓｉｇｍｏｉｄ函数的导数是个很小的数，多个很小的数\n连乘之后几乎为０，则梯度无法从输出层传到输入层．可\n以从训练方法、技巧、网络结构等方面来缓解梯度消失问\n题．训练方法上：Ｈｉｎｔｏｎ等［１２］提出先预训练后微调的训练\n方法，先无监督逐层预训练，再用反向传播算法对整个网\n络进行微调训练，预训练每次只训练一层隐藏层避免了梯\n度消失问题．技巧上：用ＲｅＬＵ［１５］来代替Ｓｉｇｍｏｉｄ作为激活\n函数、使用ｄｒｏｐｏｕｔ、批标准化（ｂａｔｃｈｎｏｒｍａｌｉｚａｔｉｏｎ）［８２］，这\n些技巧都能缓解梯度消失问题．网络结构上：ＬＳＴＭ采用\n门机制，加了输入门、输出门、遗忘门来控制允许多少信\n息通过；ｈｉｇｈｗａｙｎｅｔｗｏｒｋｓ\n［１３６］加入携带门（ｃａｒｒｙｇａｔｅ）和转\n化门（ｔｒａｎｓｆｏｒｍｇａｔｅ）使得输出由直接输入和转化后的输入\n两部分组成；ＲｅｓＮｅｔ\n［３７］在在两层或多层之间直接加上线性\n连通通路，保证梯度能通过线性通路传到底层．\n３）依赖大规模带标签训练数据．深度学习严重依赖\n大规模带标签数据来训练模型．人工数据标注耗时耗力，\n代价高昂；某些特定领域如疑难杂症等，几乎不可能收集\n到足够多的带标签数据．因此无监督学习是深度学习接下\n来的一个重要研究方向，目前已经有一些成果，包括\nＧｏｏｄｆｅｌｌｏｗ等［１３７］提出的生成对抗网络（ｇｅｎｅｒａｔｉｖｅａｄｖｅｒｓａｒｉ\nａｌｎｅｔｓ，ＧＡＮｓ）、微软［１３８］提出的新的学习范式对偶学习．\n４）分布式训练问题．深度学习模型在单机上训练时\n间过长，尤其是当训练数据太多时，而且规模过大的模型\n也无法放入一台机器里．所以需要进行大规模分布式训练\n深度学习模型．分布式并行训练可以分为数据并行、模型\n并行和混合并行．混合并行里同时使用了数据并行和模型\n并行，例如可以在不同机器间使用数据并行，同一台机器\n上使用模型并行．数据并行是目前多数分布式系统的首\n选，各种数据并行方法的区别在于是参数平均法还是更新\n式方法、是同步更新还是异步更新、是中心化同步还是分\n布式同步．参数平均法是传输各节点的参数到参数服务\n器，然后所有参数求平均得到全局参数，更新式方法传输\n的不是参数，而是参数的更新量．微软［１３９］提出延迟补偿\n的异步更新方法，在同步更新与异步更新中寻找合适的平\n衡，并开源了参数服务器框架Ｍｕｌｔｉｖｅｒｓｏ．Ｓｔｒｏｍ［１４０］去掉中\n心的参数服务器，平等地在各节点间传输参数更新量，高\n度压缩节点间的更新使得网络通信减少了３个数量级．\n４．２　落地问题\n１）对抗性样本攻击．Ｓｚｅｇｅｄｙ等［１４１］发现深度学习会\n被对抗性样本攻击，即本来正确分类的图片加上小的扰动\n能使深度学习模型误判成别的类别．知道神经网络结构的\n攻击被称为白盒攻击（ｗｈｉｔｅｂｏｘａｔｔａｃｋ）；对抗性样本还可\n以迁移，一个模型产生的对抗性样本也会被另一模型错误\n分类，此为黑盒攻击（ｂｌａｃｋｂｏｘａｔｔａｃｋ）．对抗性样本攻击\n说明深度学习并没有那么可靠，Ｇｏｏｄｆｅｌｌｏｗ等［１４２］认为这是\n由于深度学习模型在高维空间中的线性性质导致的．为了\n防止对抗性样本攻击，可以进行对抗性训练，将对抗性样\n１\n９\n３\n４期\n张荣，等：深度学习研究综述\n本跟普通样本都作为训练数据来训练模型，使用对抗目标\n函数，能使模型更加正则化．\n２）鲁棒性差．即使深度学习的平均准确率很高，但在\n某些测试用例上的预测效果可能很差．高可靠性系统如无\n人车，远程外科手术，卫星发射等，不允许出现某个很离\n谱的结果．故需要提高深度学习的鲁棒性，保证坏的时候\n不至于太坏，使得深度学习的应用领域更广泛．\n３）太多的超参数．针对实际问题，如何设计一个最适\n合的深度模型？深度学习有太多的超参数，包括如何进行\n数据采集、生成、选择、划分；神经网络结构是用ＭＬＰ、\nＣＮＮ还是ＲＮＮ；神经网络层数、每层的神经元数量；权重\n初始化方法；正则项系数（ｗｅｉｇｈｔｄｅｃａｙ）；动量（ｍｏｍｅｎ\nｔｕｍ）、学习率、ｌｅａｒｎｉｎｇｒａｔｅｄｅｃａｙ、ｄｒｏｐｏｕｔ；迭代次数、\nｂａｔｃｈ；模型更新规则是用ＳＧＤ还是Ａｄａｍ；分布式训练结\n果如何聚合等等．可以用另一个神经网络来学习这些超参\n数，让神经网络设计变得自动化．ＧｏｏｇｌｅＤｅｅｐＭｉｎｄ［１４３］采\n用Ｌｅａｒｎｉｎｇｔｏｌｅａｒｎ算法，用另一个网络来调整学习率，使\n得收敛更快．Ｇｏｏｇｌｅ发布了能自动搜索最优网络结构的\nＣｌｏｕｄＡｕｔｏＭＬ．韩红桂等［１４４］提出利用竞争机制来动态增\n加或删减隐藏层神经元，动态优化神经网络结构．张昭昭\n等［１４５］提出多层自适应模块化神经网络结构设计方法．\n４）可解释性差．深度学习模型的可解释性差，是典型\n的黑箱算法，模型复杂，通常包含上亿个参数．线上应用\n模型后，如果对某个用户造成严重影响，无法确定是哪个\n参数出了问题，从而无法针对性地调整某个参数来解决此\n用户的问题．而且模型的可解释性问题限制了其在医学、\n自动驾驶、军事、航天等重要领域的应用．可以尝试将深\n度学习与符号学习、可解释性算法进行结合，使其既有深\n度学习强大的表达能力，又有一定的可解释性．微软用图\n学习机（ＧｒａｐｈＥｎｇｉｎｅ）来统一机器学习与知识图谱．Ｓｔａｎ\nｆｏｒｄ博士生Ｗｕ等［１４６］将深度学习与可解释性决策树算法\n结合起来，用树正则化方法来提高深度学习的可解释性．\n５）模型太大．深度学习模型本身非常大，不方便放在\nＧＰＵ中，更不方便在移动端使用．特别对于语言模型来\n说，词表非常大，输出神经元很多，导致模型非常大．所\n以需要在保证准确率的前提下，进行模型压缩，将模型变\n小．模型压缩方法主要有参数修剪和共享［１４７］、低秩分\n解［１４８］、压缩卷积滤波器［１４９］、知识精炼［１５０］四类［１５１］．参数\n修剪和共享是去除对准确率没有提升的冗余参数，根据减\n少信息冗余或参数空间冗余的方式，参数修剪和共享又可\n以细分为量化和二进制、剪枝和共享、设计结构化矩阵三\n类．低秩分解是用矩阵分解或张量分解来评估最具信息量\n的参数；压缩卷积滤波器是设计特殊结构的卷积滤波器来\n减少存储和计算的复杂度；知识精炼是提取大型模型里的\n知识来训练更小更紧凑的模型．\n４．３　功能问题\n１）不能像人类一样进行小样本学习．深度学习需要\n大规模的训练数据，样本利用率不高，但是人类的学习只\n需要极少几个样本．其实小孩在学习过程中，利用了大人\n传授的知识．目前还缺乏统一的框架向深度学习模型提供\n领域先验知识．要像人类一样进行小样本学习，可以尝试\n将深度学习与知识图谱、逻辑推理、符号学习等结合在一\n起，同时利用好数据与知识．\n２）不能很好完成动态决策性任务．决策性任务会随\n着时间而改变，是动态的；而且往往与环境有复杂的交\n互，各种因素互相影响．金融股票预测是个典型的动态决\n策性任务，同一股票同一价格在不同的时间可能就需要进\n行相反的买卖操作；一只股票的涨跌会对其他股票产生影\n响．动态决策性任务内部之间互相博弈，可以尝试将深度\n学习与博弈论结合来完成动态决策性任务．\n３）不能很好完成逻辑推理任务．深度学习目前只是\n在图像识别、语音识别等感知层面的任务有较好的表现，\n缺乏逻辑推理能力，无法很好地完成需要逻辑推理的任\n务．可以考虑将深度学习与擅长逻辑推理的符号学习、存\n储了知识的知识图谱结合；另外还可以给深度学习模型增\n加记忆模块，如神经图灵机［６５］与记忆网络［６６］等．\n４）不能很好处理已有特征的小数据问题．深度学习\n由于能够自动提取特征，所以在图像识别等人为很难提取\n特征的任务上表现很好．但是一些任务如保险用户流失预\n测等，人工能够很好地提取有效特征，而且训练数据较\n少．深度学习在这些已有特征的小数据问题上效果还不如\nＧＢＤＴ、ＸＧＢｏｏｓｔ、ＬｉｇｈｔＧＢＭ［１５２］等集成学习算法，甚至不如\n普通的浅层机器学习算法［１５３］．\n５）无法同时处理多任务．人脑是多才多艺的，能同时\n识别语音，识别图像，理解文字等．而目前深度学习模型\n都是针对某一特定任务用特定数据集训练的，训练得到的\n模型也只能完成这一特定任务．为了能完成多任务，进一\n步实现通用人工智能，可以尝试将不同功能的神经网络以\n某种方式连接成更大的神经网络．Ｇｏｏｇｌｅ\n［１５４］通过稀疏门\n矩阵将多个多层感知器子网络组合成超大网络，用反向传\n播同时训练所有子网络．\n６）终极算法．机器学习有五大流派：符号主义、连接\n主义、进化主义、贝叶斯主义、分析主义，深度学习属于\n连接主义．深度学习的扩展性很强，可以尝试以深度学习\n为基础，将其他流派包含进来，形成集成了五大流派的终\n极算法（ｍａｓｔｅｒａｌｇｏｒｉｔｈｍ）［１５５］．ＯｐｅｎＡＩ\n［１５６］发现用进化主义\n的遗传算法替代反向传播算法来训练深度强化学习能更快\n收敛．Ｇｏｏｇｌｅ\n［１５７］开源了概率建模推理库Ｅｄｗａｒｄ，清华大\n学［１５８］开源了贝叶斯深度学习的概率编程库ＺｈｕＳｕａｎ，Ｕｂｅｒ\n和Ｓｔａｎｆｏｒｄ开源了深度概率编程库Ｐｙｒｏ，这些都证明了贝\n叶斯主义与深度学习可以结合在一起．另外，深度学习还\n可以跟逻辑回归等广义线性模型结合［１３１］．\n４．４　领域问题\n１）图像理解问题．深度学习目前在图像识别等感知\n任务有较好的表现，但在图像理解如视觉关系理解、图片\n内容问答、视觉注意点预测等方面成果还并不多．视觉关\n系理解首先需要检测出关键对象，然后预测对象之间的关\n系．图片内容问答是根据给定的图片，回答相应的问题．\n视觉注意点预测是对于给定的图片，预测人最感兴趣图片\n的哪一部分．这些都需要对图像内容有很好的理解，图像\n２\n９\n３\n信息与控制　　　　　　　　　　　　　　　　　　４７卷\n理解问题需要学者们的进一步探索．\n２）自然语言处理问题．语言其实是比语音、图像更高\n级的非自然信号，是完全由人脑产生和处理的符号系统．\n深度学习在自然语言处理上的效果还不像语音、图像那么\n显著，但是深度学习是受人脑启发得到的算法，相信深度\n学习接下来会在自然语言处理领域有更多的成果．\n参考文献\n［１］ＭｃＣｕｌｌｏｃｈＷＳ，ＰｉｔｔｓＷＨ．Ａｌｏｇｉｃａｌｃａｌｃｕｌｕｓｏｆｔｈｅｉｄｅａｓｉｍｍａｎｅｎｔｉｎｎｅｒｖｏｕｓａｃｔｉｖｉｔｙ［Ｊ］．ＢｕｌｌｅｔｉｎｏｆＭａｔｈｅｍａｔｉｃａｌＢｉｏｌｏｇｙ，１９４３，５（４）：\n１１５－１３３．\n［２］ＲｏｓｅｎｂｌａｔｔＦ．Ｔｈｅｐｅｒｃｅｐｔｒｏｎ：Ａｐｒｏｂａｂｉｌｉｓｔｉｃｍｏｄｅｌｆｏｒｉｎｆｏｒｍａｔｉｏｎｓｔｏｒａｇｅａｎｄｏｒｇａｎｉｚａｔｉｏｎｉｎｔｈｅｂｒａｉｎ［Ｊ］．ＰｓｙｃｈｏｌｏｇｉｃａｌＲｅｖｉｅｗ，１９５８，\n６５（６）：３８６－３９２．\n［３］ＭｉｎｓｋｙＭＬ，ＰａｐｅｒｔＳＡ．Ｐｅｒｃｅｐｔｒｏｎｓ：Ａｎｉｎｔｒｏｄｕｃｔｉｏｎｔｏｃｏｍｐｕｔａｔｉｏｎａｌｇｅｏｍｅｔｒｙ［Ｍ］．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，１９６９．\n［４］ＲｕｍｅｌｈａｒｔＤＥ，ＨｉｎｔｏｎＧＥ，ＷｉｌｌｉａｍｓＲＪ．Ｌｅａｒｎｉｎｇｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｂｙｂａｃｋｐｒｏｐａｇａｔｉｎｇｅｒｒｏｒｓ［Ｊ］．Ｎａｔｕｒｅ，１９８６，３２３（６０８８）：５３３－\n５３６．\n［５］ＣｙｂｅｎｋｏＧ．Ａｐｐｒｏｘｉｍａｔｉｏｎｂｙｓｕｐｅｒｐｏｓｉｔｉｏｎｓｏｆａｓｉｇｍｏｉｄａｌｆｕｎｃｔｉｏｎ［Ｊ］．ＭａｔｈｅｍａｔｉｃｓｏｆＣｏｎｔｒｏｌＳｉｇｎａｌｓａｎｄＳｙｓｔｅｍｓ，１９８９，２（３）：１７－２８．\n［６］ＨｏｒｎｉｋＫ，ＳｔｉｎｃｈｃｏｍｂｅＭ，ＷｈｉｔｅＨ．Ｍｕｌｔｉｌａｙｅｒｆｅｅｄｆｏｒｗａｒｄｎｅｔｗｏｒｋｓａｒｅｕｎｉｖｅｒｓａｌａｐｐｒｏｘｉｍａｔｏｒｓ［Ｊ］．ＮｅｕｒａｌＮｅｔｗｏｒｋｓ，１９８９，２（５）：\n３５９－３６６．\n［７］ＬｅｃｕｎＹ，ＢｏｓｅｒＢ，ＤｅｎｋｅｒＪＳ，ｅｔａｌ．Ｂａｃｋｐｒｏｐａｇａｔｉｏｎａｐｐｌｉｅｄｔｏｈａｎｄｗｒｉｔｔｅｎｚｉｐｃｏｄｅｒｅｃｏｇｎｉｔｉｏｎ［Ｊ］．ＮｅｕｒａｌＣｏｍｐｕｔａｔｉｏｎ，１９８９，１（４）：\n５４１－５５１．\n［８］ＬｅｃｕｎＹ，ＢｏｓｅｒＢ，ＤｅｎｋｅｒＪＳ，ｅｔａｌ．Ｈａｎｄｗｒｉｔｔｅｎｄｉｇｉｔｒｅｃｏｇｎｉｔｉｏｎｗｉｔｈａｂａｃｋｐｒｏｐａｇａｔｉｏｎｎｅｔｗｏｒｋ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎ\nｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，１９８９：３９６－４０４．\n［９］ＣｏｒｔｅｓＣ，ＶａｐｎｉｋＶ．Ｓｕｐｐｏｒｔｖｅｃｔｏｒｎｅｔｗｏｒｋｓ［Ｊ］．ＭａｃｈｉｎｅＬｅａｒｎｉｎｇ，１９９５，２０（３）：２７３－２９７．\n［１０］ＨｉｎｔｏｎＧＥ．Ｗｈａｔｋｉｎｄｏｆｇｒａｐｈｉｃａｌｍｏｄｅｌｉｓｔｈｅｂｒａｉｎ？［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＪｏｉｎｔＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ．Ｂｕｒｌｉｎｇｔｏｎ，ＵＳＡ：\nＭｏｒｇａｎＫａｕｆｍａｎｎ，２００５：１７６５－１７７５．\n［１１］ＨｉｎｔｏｎＧＥ，ＳａｌａｋｈｕｔｄｉｎｏｖＲＲ．Ｒｅｄｕｃｉｎｇｔｈｅｄｉｍｅｎｓｉｏｎａｌｉｔｙｏｆｄａｔａｗｉｔｈｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｊ］．Ｓｃｉｅｎｃｅ，２００６，３１３（５７８６）：５０４－５０７．\n［１２］ＨｉｎｔｏｎＧＥ，ＯｓｉｎｄｅｒｏＳ，ＴｅｈＹＷ．Ａｆａｓｔｌｅａｒｎｉｎｇａｌｇｏｒｉｔｈｍｆｏｒｄｅｅｐｂｅｌｉｅｆｎｅｔｓ［Ｊ］．ＮｅｕｒａｌＣｏｍｐｕｔａｔｉｏｎ，２００６，１８（７）：１５２７－１５５４．\n［１３］ＢｅｎｇｉｏＹ，ＬａｍｂｌｉｎＰ，ＰｏｐｏｖｉｃｉＤ，ｅｔａｌ．Ｇｒｅｅｄｙｌａｙｅｒｗｉｓｅｔｒａｉｎｉｎｇｏｆｄｅｅｐｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏ\nｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２００６：１５３－１６０．\n［１４］ＰｏｕｌｔｎｅｙＣ，ＣｈｏｐｒａＳ，ＬｅｃｕｎＹ．Ｅｆｆｉｃｉｅｎｔｌｅａｒｎｉｎｇｏｆｓｐａｒｓｅｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｗｉｔｈａｎｅｎｅｒｇｙｂａｓｅｄｍｏｄｅｌ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌ\nＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２００６：１１３７－１１４４．\n［１５］ＧｌｏｒｏｔＸ，ＢｏｒｄｅｓＡ，ＢｅｎｇｉｏＹ．Ｄｅｅｐｓｐａｒｓｅｒｅｃｔｉｆｉｅｒｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅａｎｄＳｔａｔｉｓｔｉｃｓ．\nＰｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１１：３１５－３２３．\n［１６］ＨｉｎｔｏｎＧＥ，ＤｅｎｇＬ，ＹｕＤ，ｅｔａｌ．Ｄｅｅｐｎｅｕｒａｌｎｅｔｗｏｒｋｓｆｏｒａｃｏｕｓｔｉｃｍｏｄｅｌｉｎｇｉｎｓｐｅｅｃｈｒｅｃｏｇｎｉｔｉｏｎ：Ｔｈｅｓｈａｒｅｄｖｉｅｗｓｏｆｆｏｕｒｒｅｓｅａｒｃｈ\nｇｒｏｕｐｓ［Ｊ］．ＩＥＥＥＳｉｇｎａｌＰｒｏｃｅｓｓｉｎｇＭａｇａｚｉｎｅ，２０１２，２９（６）：８２－９７．\n［１７］ＤａｈｌＧＥ，ＹｕＤ，ＤｅｎｇＬ，ｅｔａｌ．Ｃｏｎｔｅｘｔｄｅｐｅｎｄｅｎｔｐｒｅｔｒａｉｎｅｄｄｅｅｐｎｅｕｒａｌｎｅｔｗｏｒｋｓｆｏｒｌａｒｇｅｖｏｃａｂｕｌａｒｙｓｐｅｅｃｈｒｅｃｏｇｎｉｔｉｏｎ［Ｊ］．ＩＥＥＥ\nＴｒａｎｓａｃｔｉｏｎｓｏｎＡｕｄｉｏＳｐｅｅｃｈ＆ＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，２０１１，２０（１）：３０－４２．\n［１８］ＤｅｎｇＪ，ＤｏｎｇＷ，ＳｏｃｈｅｒＲ，ｅｔａｌ．ＩｍａｇｅＮｅｔ：Ａｌａｒｇｅｓｃａｌｅｈｉｅｒａｒｃｈｉｃａｌｉｍａｇｅｄａｔａｂａｓｅ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔ\nｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２００９：２４８－２５５．\n［１９］ＫｒｉｚｈｅｖｓｋｙＡ，ＳｕｔｓｋｅｖｅｒＩ，ＨｉｎｔｏｎＧＥ．ＩｍａｇｅＮｅｔｃｌａｓｓｉｆｉｃａｔｉｏｎｗｉｔｈｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌ\nＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１２：１０９７－１１０５．\n［２０］ＤａｕｐｈｉｎＹＮ，ＰａｓｃａｎｕＲ，ＧｕｌｃｅｈｒｅＣ，ｅｔａｌ．Ｉｄｅｎｔｉｆｙｉｎｇａｎｄａｔｔａｃｋｉｎｇｔｈｅｓａｄｄｌｅｐｏｉｎｔｐｒｏｂｌｅｍｉｎｈｉｇｈｄｉｍｅｎｓｉｏｎａｌｎｏｎｃｏｎｖｅｘｏｐｔｉｍｉｚａｔｉｏｎ\n［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１４：２９３３－２９４１．\n［２１］ＣｈｏｒｏｍａｎｓｋａＡ，ＨｅｎａｆｆＭ，ＭａｔｈｉｅｕＭ，ｅｔａｌ．Ｔｈｅｌｏｓｓｓｕｒｆａｃｅｓｏｆｍｕｌｔｉｌａｙｅｒｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉ\nｇｅｎｃｅａｎｄＳｔａｔｉｓｔｉｃｓ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：１９２－２０４．\n［２２］余凯，贾磊，陈雨强，等．深度学习的昨天，今天和明天［Ｊ］．计算机研究与发展，２０１３，５０（９）：１７９９－１８０４．\nＹｕＫ，ＪｉａＬ，ＣｈｅｎＹＱ，ｅｔａｌ．Ｄｅｅｐｌｅａｒｎｉｎｇ：Ｙｅｓｔｅｒｄａｙ，ｔｏｄａｙ，ａｎｄｔｏｍｏｒｒｏｗ［Ｊ］．ＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒＲｅｓｅａｒｃｈａｎｄＤｅｖｅｌｏｐｍｅｎｔ，２０１３，\n５０（９）：１７９９－１８０４．\n［２３］ＢｅｎｇｉｏＹ，ＤｅｌａｌｌｅａｕＯ，ＲｏｕｘＮＬ．Ｔｈｅｃｕｒｓｅｏｆｈｉｇｈｌｙｖａｒｉａｂｌｅｆｕｎｃｔｉｏｎｓｆｏｒｌｏｃａｌｋｅｒｎｅｌｍａｃｈｉｎｅｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒ\nｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２００６：１０７－１１４．\n［２４］ＢｅｎｇｉｏＹ，ＣｏｕｒｖｉｌｌｅＡ，ＶｉｎｃｅｎｔＰ．Ｒｅｐｒｅｓｅｎｔａｔｉｏｎｌｅａｒｎｉｎｇ：Ａｒｅｖｉｅｗａｎｄｎｅｗｐｅｒｓｐｅｃｔｉｖｅｓ［Ｊ］．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＰａｔｔｅｒｎＡｎａｌｙｓｉｓａｎｄ\nＭａｃｈｉｎｅＩｎｔｅｌｌｉｇｅｎｃｅ，２０１３，３５（８）：１７９８－１８２８．\n［２５］ＬｅＣｕｎＹ，ＢｅｎｇｉｏＹ，ＨｉｎｔｏｎＧ．Ｄｅｅｐｌｅａｒｎｉｎｇ［Ｊ］．Ｎａｔｕｒｅ，２０１５，５２１（７５５３）：４３６－４４４．\n［２６］ＧｏｏｄｆｅｌｌｏｗＩ，ＢｅｎｇｉｏＹ，ＣｏｕｒｖｉｌｌｅＡ．Ｄｅｅｐｌｅａｒｎｉｎｇ［Ｍ］．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１６．\n［２７］ＢｅｎｇｉｏＹ．ＬｅａｒｎｉｎｇｄｅｅｐａｒｃｈｉｔｅｃｔｕｒｅｓｆｏｒＡＩ［Ｊ］．Ｆｏｕｎｄａｔｉｏｎｓ＆ＴｒｅｎｄｓｉｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ，２００９，２（１）：１－１２７．\n［２８］ＭｏｎｔｕｆａｒＧＦ，ＰａｓｃａｎｕＲ，ＣｈｏＫ，ｅｔａｌ．Ｏｎｔｈｅｎｕｍｂｅｒｏｆｌｉｎｅａｒｒｅｇｉｏｎｓｏｆｄｅｅｐｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒ\nｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１４：２９２４－２９３２．\n３\n９\n３\n４期\n张荣，等：深度学习研究综述\n［２９］ＬｅＣｕｎＹ，ＢｏｔｔｏｕＬ，ＢｅｎｇｉｏＹ，ｅｔａｌ．Ｇｒａｄｉｅｎｔｂａｓｅｄｌｅａｒｎｉｎｇａｐｐｌｉｅｄｔｏｄｏｃｕｍｅｎｔｒｅｃｏｇｎｉｔｉｏｎ［Ｊ］．ＰｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅＩＥＥＥ，１９９８，８６\n（１１）：２２７８－２３２４．\n［３０］ＺｅｉｌｅｒＭＤ，ＦｅｒｇｕｓＲ．Ｖｉｓｕａｌｉｚｉｎｇａｎｄｕｎｄｅｒｓｔａｎｄｉｎｇｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｃｈａｍ，Ｓｗｉｔｚ\nｅｒｌａｎｄ：ＳｐｒｉｎｇｅｒＩｎｔｅｒｎａｔｉｏｎａｌＰｕｂｌｉｓｈｉｎｇＡＧ，２０１４：８１８－８３３．\n［３１］ＤｉｃｅｃｃｏＲ，ＬａｃｅｙＧ，ＶａｓｉｌｊｅｖｉｃＪ，ｅｔａｌ．ＣａｆｆｅｉｎａｔｅｄＦＰＧＡｓ：ＦＰＧＡｆｒａｍｅｗｏｒｋｆｏｒｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒ\nｅｎｃｅｏｎＦｉｅｌｄＰｒｏｇｒａｍｍａｂｌｅＴｅｃｈｎｏｌｏｇｙ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１７：２６５－２６８．\n［３２］周飞燕，金林鹏，董军．卷积神经网络研究综述［Ｊ］．计算机学报，２０１７，４０（６）：１２２９－１２５１．\nＺｈｏｕＦＹ，ＪｉｎＬＰ，ＤｏｎｇＪ．Ｒｅｖｉｅｗｏｆｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋ［Ｊ］．ＣｈｉｎｅｓｅＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒｓ，２０１７，４０（６）：１２２９－１２５１．\n［３３］ＳｕｔｓｋｅｖｅｒＩ．Ｔｒａｉｎｉｎｇｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｄ］．Ｔｏｒｏｎｔｏ，Ｃａｎａｄａ：ＵｎｉｖｅｒｓｉｔｙｏｆＴｏｒｏｎｔｏ，２０１３．\n［３４］ＰａｓｃａｎｕＲ，ＭｉｋｏｌｏｖＴ，ＢｅｎｇｉｏＹ．Ｏｎｔｈｅｄｉｆｆｉｃｕｌｔｙｏｆｔｒａｉｎｉｎｇｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎ\nｉｎｇ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１３：１３１０－１３１８．\n［３５］ＳｉｍｏｎｙａｎＫ，ＺｉｓｓｅｒｍａｎＡ．Ｖｅｒｙｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｌａｒｇｅｓｃａｌｅｉｍａｇｅｒｅｃｏｇｎｉｔｉｏｎ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１４．\n［３６］ＳｚｅｇｅｄｙＣ，ＬｉｕＷ，ＪｉａＹ，ｅｔａｌ．Ｇｏｉｎｇｄｅｅｐｅｒｗｉｔｈｃｏｎｖｏｌｕｔｉｏｎｓ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔ\nａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：１－９．\n［３７］ＨｅＫ，ＺｈａｎｇＸ，ＲｅｎＳ，ｅｔａｌ．Ｄｅｅｐｒｅｓｉｄｕａｌｌｅａｒｎｉｎｇｆｏｒｉｍａｇｅｒｅｃｏｇｎｉｔｉｏｎ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉ\nｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１６：７７０－７７８．\n［３８］ＧｉｒｓｈｉｃｋＲ，ＤｏｎａｈｕｅＪ，ＤａｒｒｅｌｌＴ，ｅｔａｌ．Ｒｉｃｈｆｅａｔｕｒｅｈｉｅｒａｒｃｈｉｅｓｆｏｒａｃｃｕｒａｔｅｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎａｎｄｓｅｍａｎｔｉｃｓｅｇｍｅｎｔａｔｉｏｎ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒ\nｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１４：５８０－５８７．\n［３９］ＧｉｒｓｈｉｃｋＲ．ＦａｓｔＲＣＮＮ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：１４４０－１４４８．\n［４０］ＲｅｎＳ，ＨｅＫ，ＧｉｒｓｈｉｃｋＲ，ｅｔａｌ．ＦａｓｔｅｒＲＣＮＮ：Ｔｏｗａｒｄｓｒｅａｌｔｉｍｅｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎｗｉｔｈｒｅｇｉｏｎｐｒｏｐｏｓａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅ\nｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１５：９１－９９．\n［４１］ＨｅＫ，ＧｋｉｏｘａｒｉＧ，ＤｏｌｌｒＰ，ｅｔａｌ．ＭａｓｋＲＣＮＮ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１７：\n２９８０－２９８８．\n［４２］ＬｉｎＭ，ＣｈｅｎＱ，ＹａｎＳ．Ｎｅｔｗｏｒｋｉｎｎｅｔｗｏｒｋ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１３．\n［４３］ＪａｄｅｒｂｅｒｇＭ，ＳｉｍｏｎｙａｎＫ，ＺｉｓｓｅｒｍａｎＡ．Ｓｐａｔｉａｌｔｒａｎｓｆｏｒｍｅｒｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．\nＣａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１５：２０１７－２０２５．\n［４４］ＬｉｎＴＹ，ＤｏｌｌｒＰ，ＧｉｒｓｈｉｃｋＲ，ｅｔａｌ．Ｆｅａｔｕｒｅｐｙｒａｍｉｄｎｅｔｗｏｒｋｓｆｏｒｏｂｊｅｃｔｄｅｔｅｃｔｉｏｎ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［４５］ＺｅｉｌｅｒＭＤ，ＫｒｉｓｈｎａｎＤ，ＴａｙｌｏｒＧＷ，ｅｔａｌ．Ｄｅｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．\nＰｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１０：２５２８－２５３５．\n［４６］ＭａｓｃｉＪ，ＭｅｉｅｒＵ，Ｃｉｒｅ?ａｎＤ，ｅｔａｌ．Ｓｔａｃｋｅｄｃｏｎｖｏｌｕｔｉｏｎａｌａｕｔｏｅｎｃｏｄｅｒｓｆｏｒｈｉｅｒａｒｃｈｉｃａｌｆｅａｔｕｒｅｅｘｔｒａｃｔｉｏｎ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎ\nＡｒｔｉｆｉｃｉａｌＮｅｕｒａｌＮｅｔｗｏｒｋｓ．Ｂｅｒｌｉｎ，Ｇｅｒｍａｎｙ：Ｓｐｒｉｎｇｅｒ，２０１１：５２－５９．\n［４７］ＤｏｎｇＣ，ＬｏｙＣＣ，ＨｅＫ，ｅｔａｌ．Ｌｅａｒｎｉｎｇａｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｆｏｒｉｍａｇｅｓｕｐｅｒｒｅｓｏｌｕｔｉｏｎ［Ｃ］／／ＥｕｒｏｐｅａｎＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒ\nＶｉｓｉｏｎ．Ｃｈａｍ，Ｓｗｉｔｚｅｒｌａｎｄ：ＳｐｒｉｎｇｅｒＩｎｔｅｒｎａｔｉｏｎａｌＰｕｂｌｉｓｈｉｎｇＡＧ，２０１４：１８４－１９９．\n［４８］ＳｅｒｍａｎｅｔＰ，ＥｉｇｅｎＤ，ＺｈａｎｇＸ，ｅｔａｌ．ＯｖｅｒＦｅａｔ：Ｉｎｔｅｇｒａｔｅｄｒｅｃｏｇｎｉｔｉｏｎ，ｌｏｃａｌｉｚａｔｉｏｎａｎｄｄｅｔｅｃｔｉｏｎｕｓｉｎｇｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ［Ｃ］／／Ｉｎｔｅｒ\nｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬｅａｒｎｉｎｇＲｅｐｒｅｓｅｎｔａｔｉｏｎｓ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１４．\n［４９］ＤｏｓｏｖｉｔｓｋｉｙＡ，ＦｉｓｃｈｅｒＰ，ＩｌｇＥ，ｅｔａｌ．ＦｌｏｗＮｅｔ：Ｌｅａｒｎｉｎｇｏｐｔｉｃａｌｆｌｏｗｗｉｔｈｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔ\nｅｒＶｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：２７５８－２７６６．\n［５０］ＬｉｕＮ，ＨａｎＪ，ＺｈａｎｇＤ，ｅｔａｌ．Ｐｒｅｄｉｃｔｉｎｇｅｙｅｆｉｘａｔｉｏｎｓｕｓｉｎｇｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ\nＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：３６２－３７０．\n［５１］ＣｉｍｐｏｉＭ，ＭａｊｉＳ，ＶｅｄａｌｄｉＡ．Ｄｅｅｐｆｉｌｔｅｒｂａｎｋｓｆｏｒｔｅｘｔｕｒｅｒｅｃｏｇｎｉｔｉｏｎａｎｄｓｅｇｍｅｎｔａｔｉｏｎ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ\nＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：３８２８－３８３６．\n［５２］ＢｅｒｔａｓｉｕｓＧ，ＳｈｉＪ，ＴｏｒｒｅｓａｎｉＬ．ＤｅｅｐＥｄｇｅ：Ａｍｕｌｔｉｓｃａｌｅｂｉｆｕｒｃａｔｅｄｄｅｅｐｎｅｔｗｏｒｋｆｏｒｔｏｐｄｏｗｎｃｏｎｔｏｕｒｄｅｔｅｃｔｉｏｎ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎ\nＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：４３８０－４３８９．\n［５３］ＳｈｅｎＷ，ＷａｎｇＸ，ＷａｎｇＹ，ｅｔａｌ．ＤｅｅｐＣｏｎｔｏｕｒ：Ａｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｆｅａｔｕｒｅｌｅａｒｎｅｄｂｙｐｏｓｉｔｉｖｅｓｈａｒｉｎｇｌｏｓｓｆｏｒｃｏｎｔｏｕｒｄｅｔｅｃｔｉｏｎ［Ｃ］／／\nＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：３９８２－３９９１．\n［５４］ＬｉｕＺ，ＬｉＸ，ＬｕｏＰ，ｅｔａｌ．Ｓｅｍａｎｔｉｃｉｍａｇｅｓｅｇｍｅｎｔａｔｉｏｎｖｉａｄｅｅｐｐａｒｓｉｎｇｎｅｔｗｏｒｋ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａ\nｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：１３７７－１３８５．\n［５５］ＤａｉＪ，ＨｅＫ，ＳｕｎＪ．ＢｏｘＳｕｐ：Ｅｘｐｌｏｉｔｉｎｇｂｏｕｎｄｉｎｇｂｏｘｅｓｔｏｓｕｐｅｒｖｉｓｅｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｓｅｍａｎｔｉｃｓｅｇｍｅｎｔａｔｉｏｎ［Ｃ］／／Ｉｎｔｅｒｎａｔｉｏｎａｌ\nＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：１６３５－１６４３．\n［５６］ＮａｍＨ，ＢａｅｋＭ，ＨａｎＢ．Ｍｏｄｅｌｉｎｇａｎｄｐｒｏｐａｇａｔｉｎｇｃｎｎｓｉｎａｔｒｅｅｓｔｒｕｃｔｕｒｅｆｏｒｖｉｓｕａｌｔｒａｃｋｉｎｇ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［５７］ＴｒａｎＤ，ＢｏｕｒｄｅｖＬ，ＦｅｒｇｕｓＲ，ｅｔａｌ．Ｌｅａｒｎｉｎｇｓｐａｔｉｏｔｅｍｐｏｒａｌｆｅａｔｕｒｅｓｗｉｔｈ３Ｄｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍ\nｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：４４８９－４４９７．\n［５８］ＢｅｎｇｉｏＹ，ＳｉｍａｒｄＰ，ＦｒａｓｃｏｎｉＰ．Ｌｅａｒｎｉｎｇｌｏｎｇｔｅｒｍｄｅｐｅｎｄｅｎｃｉｅｓｗｉｔｈｇｒａｄｉｅｎｔｄｅｓｃｅｎｔｉｓｄｉｆｆｉｃｕｌｔ［Ｊ］．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＮｅｕｒａｌＮｅｔ\nｗｏｒｋｓ，１９９４，５（２）：１５７－１６６．\n４\n９\n３\n信息与控制　　　　　　　　　　　　　　　　　　４７卷\n［５９］ＨｏｃｈｒｅｉｔｅｒＳ，ＳｃｈｍｉｄｈｕｂｅｒＪ．Ｌｏｎｇｓｈｏｒｔｔｅｒｍｍｅｍｏｒｙ［Ｊ］．ＮｅｕｒａｌＣｏｍｐｕｔａｔｉｏｎ，１９９７，９（８）：１７３５－１７８０．\n［６０］ＨｉｈｉＳＥ，ＢｅｎｇｉｏＹ．Ｈｉｅｒａｒｃｈｉｃａｌｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓｆｏｒｌｏｎｇｔｅｒｍｄｅｐｅｎｄｅｎｃｉｅｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏ\nｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，１９９５：４９３－４９９．\n［６１］ＣｈｕｎｇＪ，ＡｈｎＳ，ＢｅｎｇｉｏＹ．Ｈｉｅｒａｒｃｈｉｃａｌｍｕｌｔｉｓｃａｌｅｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［６２］ＧｒａｖｅｓＡ，ＦｅｒｎｎｄｅｚＳ，ＳｃｈｍｉｄｈｕｂｅｒＪ．Ｍｕｌｔｉｄｉｍｅｎｓｉｏｎａｌｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＮｅｕｒａｌＮｅｔ\nｗｏｒｋｓ．Ｂｅｒｌｉｎ，Ｇｅｒｍａｎｙ：Ｓｐｒｉｎｇｅｒ，２００７：５４９－５５８．\n［６３］ＳｃｈｕｓｔｅｒＭ，ＰａｌｉｗａｌＫＫ．Ｂｉｄｉｒｅｃｔｉｏｎａｌｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｊ］．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＳｉｇｎａｌＰｒｏｃｅｓｓｉｎｇ，２００２，４５（１１）：２６７３－\n２６８１．\n［６４］ＪａｅｇｅｒＨ．Ｔｈｅ“ｅｃｈｏｓｔａｔｅ”ａｐｐｒｏａｃｈｔｏａｎａｌｙｓｉｎｇａｎｄｔｒａｉｎｉｎｇｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓｗｉｔｈａｎｅｒｒａｔｕｍｎｏｔｅ［Ｒ］．Ｂｏｎｎ，Ｇｅｒｍａｎｙ：Ｇｅｒｍａｎ\nＮａｔｉｏｎａｌＲｅｓｅａｒｃｈＣｅｎｔｅｒｆｏｒＩｎｆｏｒｍａｔｉｏｎＴｅｃｈｎｏｌｏｇｙＧＭＤＴｅｃｈｎｉｃａｌＲｅｐｏｒｔ，２００１．\n［６５］ＧｒａｖｅｓＡ，ＷａｙｎｅＧ，ＤａｎｉｈｅｌｋａＩ．Ｎｅｕｒａｌｔｕｒｉｎｇｍａｃｈｉｎｅｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１４．\n［６６］ＷｅｓｔｏｎＪ，ＣｈｏｐｒａＳ，ＢｏｒｄｅｓＡ．Ｍｅｍｏｒｙｎｅｔｗｏｒｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１４．\n［６７］ＣｈｕｎｇＪ，ＧｕｌｃｅｈｒｅＣ，ＣｈｏＫ，ｅｔａｌ．Ｅｍｐｉｒｉｃａｌｅｖａｌｕａｔｉｏｎｏｆｇａｔｅｄｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓｏｎｓｅｑｕｅｎｃｅｍｏｄｅｌｉｎｇ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１４．\n［６８］ＧｒａｖｅｓＡ，ＳｃｈｍｉｄｈｕｂｅｒＪ．ＦｒａｍｅｗｉｓｅｐｈｏｎｅｍｅｃｌａｓｓｉｆｉｃａｔｉｏｎｗｉｔｈｂｉｄｉｒｅｃｔｉｏｎａｌＬＳＴＭａｎｄｏｔｈｅｒｎｅｕｒａｌｎｅｔｗｏｒｋａｒｃｈｉｔｅｃｔｕｒｅｓ［Ｊ］．ＮｅｕｒａｌＮｅｔ\nｗｏｒｋｓｔｈｅＯｆｆｉｃｉａｌＪｏｕｒｎａｌｏｆｔｈｅＩｎｔｅｒｎａｔｉｏｎａｌＮｅｕｒａｌＮｅｔｗｏｒｋＳｏｃｉｅｔｙ，２００５，１８（５／６）：６０２－６１０．\n［６９］ＴｈｉｒｅｏｕＴ，ＲｅｃｚｋｏＭ．Ｂｉｄｉｒｅｃｔｉｏｎａｌｌｏｎｇｓｈｏｒｔｔｅｒｍｍｅｍｏｒｙｎｅｔｗｏｒｋｓｆｏｒｐｒｅｄｉｃｔｉｎｇｔｈｅｓｕｂｃｅｌｌｕｌａｒｌｏｃａｌｉｚａｔｉｏｎｏｆｅｕｋａｒｙｏｔｉｃｐｒｏｔｅｉｎｓ［Ｊ］．\nＩＥＥＥ／ＡＣＭＴｒａｎｓａｃｔｉｏｎｓｏｎＣｏｍｐｕｔａｔｉｏｎａｌＢｉｏｌｏｇｙ＆Ｂｉｏｉｎｆｏｒｍａｔｉｃｓ，２００７，４（３）：４４１－４４６．\n［７０］ＶｕｋｏｔｉｃＶ，ＲａｙｍｏｎｄＣ，ＧｒａｖｉｅｒＧ．ＡｓｔｅｐｂｅｙｏｎｄｌｏｃａｌｏｂｓｅｒｖａｔｉｏｎｓｗｉｔｈａｄｉａｌｏｇａｗａｒｅｂｉｄｉｒｅｃｔｉｏｎａｌＧＲＵｎｅｔｗｏｒｋｆｏｒｓｐｏｋｅｎｌａｎｇｕａｇｅｕｎ\nｄｅｒｓｔａｎｄｉｎｇ［Ｃ］／／ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＩｎｔｅｒｎａｔｉｏｎａｌＳｐｅｅｃｈＣｏｍｍｕｎｉｃａｔｉｏｎＡｓｓｏｃｉａｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＵＳＡ：ＩＥＥＥ，２０１６：３２４１－３２４４．\n［７１］ＷｅｓｔｏｎＪ，ＢｏｒｄｅｓＡ，ＣｈｏｐｒａＳ，ｅｔａｌ．Ｔｏｗａｒｄｓａｉｃｏｍｐｌｅｔｅｑｕｅｓｔｉｏｎａｎｓｗｅｒｉｎｇ：Ａｓｅｔｏｆｐｒｅｒｅｑｕｉｓｉｔｅｔｏｙｔａｓｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［７２］ＣｈｕｎｇＪ，ＧｕｌｃｅｈｒｅＣ，ＣｈｏＫ，ｅｔａｌ．Ｇａｔｅｄｆｅｅｄｂａｃｋｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ．Ｎｅｗ\nＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１５：２０６７－２０７５．\n［７３］ＨｅｒｍａｎｓＭ，ＳｃｈｒａｕｗｅｎＢ．Ｔｒａｉｎｉｎｇａｎｄａｎａｌｙｓｉｎｇｄｅｅｐｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇ\nＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１３：１９０－１９８．\n［７４］ＴａｉＫＳ，ＳｏｃｈｅｒＲ，ＭａｎｎｉｎｇＣＤ．Ｉｍｐｒｏｖｅｄｓｅｍａｎｔｉｃｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｆｒｏｍｔｒｅｅｓｔｒｕｃｔｕｒｅｄｌｏｎｇｓｈｏｒｔｔｅｒｍｍｅｍｏｒｙｎｅｔｗｏｒｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘ\nｉｖ，２０１５．\n［７５］ＫａｌｃｈｂｒｅｎｎｅｒＮ，ＤａｎｉｈｅｌｋａＩ，ＧｒａｖｅｓＡ．Ｇｒｉｄｌｏｎｇｓｈｏｒｔｔｅｒｍｍｅｍｏｒｙ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［７６］ＫｏｎｇＬ，ＤｙｅｒＣ，ＳｍｉｔｈＮＡ．Ｓｅｇｍｅｎｔａｌｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［７７］ＶｉｎｙａｌｓＯ，ＢｅｎｇｉｏＳ，ＫｕｄｌｕｒＭ．Ｏｒｄｅｒｍａｔｔｅｒｓ：Ｓｅｑｕｅｎｃｅｔｏｓｅｑｕｅｎｃｅｆｏｒｓｅｔｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［７８］Ｋａｉｓｅｒ，ＳｕｔｓｋｅｖｅｒＩ．Ｎｅｕｒａｌｇｐｕｓｌｅａｒｎａｌｇｏｒｉｔｈｍｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［７９］ＶｉｎｙａｌｓＯ，ＦｏｒｔｕｎａｔｏＭ，ＪａｉｔｌｙＮ．Ｐｏｉｎｔｅｒｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：\nＭＩＴＰｒｅｓｓ，２０１５：２６９２－２７００．\n［８０］ＳｏｒｏｋｉｎＩ，ＳｅｌｅｚｎｅｖＡ，ＰａｖｌｏｖＭ，ｅｔａｌ．ＤｅｅｐａｔｔｅｎｔｉｏｎｒｅｃｕｒｒｅｎｔＱｎｅｔｗｏｒｋ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［８１］ＫｕｍａｒＡ，ＩｒｓｏｙＯ，ＯｎｄｒｕｓｋａＰ，ｅｔａｌ．Ａｓｋｍｅａｎｙｔｈｉｎｇ：Ｄｙｎａｍｉｃｍｅｍｏｒｙｎｅｔｗｏｒｋｓｆｏｒｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎ\nｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１６：１３７８－１３８７．\n［８２］ＩｏｆｆｅＳ，ＳｚｅｇｅｄｙＣ．Ｂａｔｃｈｎｏｒｍａｌｉｚａｔｉｏｎ：Ａｃｃｅｌｅｒａｔｉｎｇｄｅｅｐｎｅｔｗｏｒｋｔｒａｉｎｉｎｇｂｙｒｅｄｕｃｉｎｇｉｎｔｅｒｎａｌｃｏｖａｒｉａｔｅｓｈｉｆｔ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒ\nｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１５：４４８－４５６．\n［８３］ＬａｕｒｅｎｔＣ，ＰｅｒｅｙｒａＧ，ＢｒａｋｅｌＰ，ｅｔａｌ．Ｂａｔｃｈｎｏｒｍａｌｉｚｅｄｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡｃｏｕｓｔｉｃｓ，\nＳｐｅｅｃｈａｎｄＳＰ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１６：２６５７－２６６１．\n［８４］ＶａｓｗａｎｉＡ，ＳｈａｚｅｅｒＮ，ＰａｒｍａｒＮ，ｅｔａｌ．Ａｔｔｅｎｔｉｏｎｉｓａｌｌｙｏｕｎｅｅｄ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍ\nｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１７：６０００－６０１０．\n［８５］ＹｉｎＷ，ＳｃｈüｔｚｅＨ，ＸｉａｎｇＢ，ｅｔａｌ．ＡＢＣＮＮ：Ａｔｔｅｎｔｉｏｎｂａｓｅｄｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｆｏｒｍｏｄｅｌｉｎｇｓｅｎｔｅｎｃｅｐａｉｒｓ［Ｊ］．Ｔｒａｎｓａｃｔｉｏｎｓｏｆ\nｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ，２０１６，４（１）：２５９－２７２．\n［８６］ＬｉｕＢ，ＬａｎｅＩ．Ａｔｔｅｎｔｉｏｎｂａｓｅｄｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｍｏｄｅｌｓｆｏｒｊｏｉｎｔｉｎｔｅｎｔｄｅｔｅｃｔｉｏｎａｎｄｓｌｏｔｆｉｌｌｉｎｇ［Ｃ］／／ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＩｎｔｅｒｎａｔｉｏｎａｌ\nＳｐｅｅｃｈＣｏｍｍｕｎｉｃａｔｉｏｎＡｓｓｏｃｉａｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＵＳＡ：ＩＥＥＥ，２０１６：６８５－６８９．\n［８７］ＹａｎｇＭ，ＴｕＷ，ＷａｎｇＪ，ｅｔａｌ．ＡｔｔｅｎｔｉｏｎｂａｓｅｄＬＳＴＭｆｏｒｔａｒｇｅｔｄｅｐｅｎｄｅｎｔｓｅｎｔｉｍｅｎｔｃｌａｓｓｉｆｉｃａｔｉｏｎ［Ｃ］／／ＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌ\nｌｉｇｅｎｃｅ．Ｋｅｙｓｔｏｎｅ，ＵＳＡ：ＡＡＡＩ，２０１７：５０１３－５０１４．\n［８８］ＶｉｎｙａｌｓＯ，ＴｏｓｈｅｖＡ，ＢｅｎｇｉｏＳ，ｅｔａｌ．Ｓｈｏｗａｎｄｔｅｌｌ：Ａｎｅｕｒａｌｉｍａｇｅｃａｐｔｉｏｎｇｅｎｅｒａｔｏｒ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔ\nｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：３１５６－３１６４．\n［８９］ＭｏｈａｍｅｄＡＲ，ＤａｈｌＧＥ，ＨｉｎｔｏｎＧ．Ａｃｏｕｓｔｉｃｍｏｄｅｌｉｎｇｕｓｉｎｇｄｅｅｐｂｅｌｉｅｆｎｅｔｗｏｒｋｓ［Ｊ］．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＡｕｄｉｏ，Ｓｐｅｅｃｈ，ａｎｄＬａｎ\nｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ，２０１２，２０（１）：１４－２２．\n［９０］ＧｒａｖｅｓＡ，ＭｏｈａｍｅｄＡＲ，ＨｉｎｔｏｎＧ．Ｓｐｅｅｃｈｒｅｃｏｇｎｉｔｉｏｎｗｉｔｈｄｅｅｐｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡｃｏｕｓ\nｔｉｃｓ，ＳｐｅｅｃｈａｎｄＳＰ．Ｐｉｓｃａｔａｗａｙ，ＵＳＡ：ＩＥＥＥ，２０１３：６６４５－６６４９．\n［９１］ＣｈｉｕＣＣ，ＳａｉｎａｔｈＴＮ，ＷｕＹ，ｅｔａｌ．Ｓｔａｔｅｏｆｔｈｅａｒｔｓｐｅｅｃｈｒｅｃｏｇｎｉｔｉｏｎｗｉｔｈｓｅｑｕｅｎｃｅｔｏｓｅｑｕｅｎｃｅｍｏｄｅｌｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n５\n９\n３\n４期\n张荣，等：深度学习研究综述\n［９２］ＡｍｏｄｅｉＤ，ＡｎａｎｔｈａｎａｒａｙａｎａｎＳ，ＡｎｕｂｈａｉＲ，ｅｔａｌ．Ｄｅｅｐｓｐｅｅｃｈ２：Ｅｎｄｔｏｅｎｄｓｐｅｅｃｈｒｅｃｏｇｎｉｔｉｏｎｉｎｅｎｇｌｉｓｈａｎｄｍａｎｄａｒｉｎ［Ｃ］／／Ｉｎｔｅｒｎａ\nｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１６：１７３－１８２．\n［９３］ＸｉｏｎｇＷ，ＤｒｏｐｐｏＪ，ＨｕａｎｇＸ，ｅｔａｌ．Ａｃｈｉｅｖｉｎｇｈｕｍａｎｐａｒｉｔｙｉｎｃｏｎｖｅｒｓａｔｉｏｎａｌｓｐｅｅｃｈｒｅｃｏｇｎｉｔｉｏｎ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［９４］ＺｅＨ，ＳｅｎｉｏｒＡ，ＳｃｈｕｓｔｅｒＭ．Ｓｔａｔｉｓｔｉｃａｌｐａｒａｍｅｔｒｉｃｓｐｅｅｃｈｓｙｎｔｈｅｓｉｓｕｓｉｎｇｄｅｅｐｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩＥＥＥＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＡ\nｃｏｕｓｔｉｃｓ，ＳｐｅｅｃｈａｎｄＳＰ．Ｐｉｓｃａｔａｗａｙ，ＵＳＡ：ＩＥＥＥ，２０１３：７９６２－７９６６．\n［９５］ＣａｐｅｓＴ，ＣｏｌｅｓＰ，ＣｏｎｋｉｅＡ，ｅｔａｌ．Ｓｉｒｉｏｎｄｅｖｉｃｅｄｅｅｐｌｅａｒｎｉｎｇｇｕｉｄｅｄｕｎｉｔｓｅｌｅｃｔｉｏｎｔｅｘｔｔｏｓｐｅｅｃｈｓｙｓｔｅｍ［Ｃ］／／ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＩｎｔｅｒｎａ\nｔｉｏｎａｌＳｐｅｅｃｈＣｏｍｍｕｎｉｃａｔｉｏｎＡｓｓｏｃｉａｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＵＳＡ：ＩＥＥＥ，２０１７：４０１１－４０１５．\n［９６］ＬｉｕＬＪ，ＣｈｕａｎｇＤｉｎｇＹＪ，ＺｈｏｕＭ，ｅｔａｌ．ＴｈｅＩＦＬＹＴＥＫｓｙｓｔｅｍｆｏｒｂｌｉｚｚａｒｄｃｈａｌｌｅｎｇｅ［Ｒ］．２０１７．\n［９７］ＯｏｒｄＡＶ，ＬｉＹ，ＢａｂｕｓｃｈｋｉｎＩ，ｅｔａｌ．ＰａｒａｌｌｅｌＷａｖｅＮｅｔ：Ｆａｓｔｈｉｇｈｆｉｄｅｌｉｔｙｓｐｅｅｃｈｓｙｎｔｈｅｓｉｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n［９８］ＰｉｎｇＷ，ＰｅｎｇＫ，ＧｉｂｉａｎｓｋｙＡ，ｅｔａｌ．Ｄｅｅｐｖｏｉｃｅ３：２０００ｓｐｅａｋｅｒｎｅｕｒａｌｔｅｘｔｔｏｓｐｅｅｃｈ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n［９９］ＺｈｕＺ，ＬｉａｎｇＤ，ＺｈａｎｇＳ，ｅｔａｌ．Ｔｒａｆｆｉｃｓｉｇｎｄｅｔｅｃｔｉｏｎａｎｄｃｌａｓｓｉｆｉｃａｔｉｏｎｉｎｔｈｅｗｉｌｄ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔｔｅｒｎ\nＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１６：２１１０－２１１８．\n［１００］ＨｕＧ，ＹａｎｇＹ，ＹｉＤ，ｅｔａｌ．Ｗｈｅｎｆａｃｅｒｅｃｏｇｎｉｔｉｏｎｍｅｅｔｓｗｉｔｈｄｅｅｐｌｅａｒｎｉｎｇ：Ａｎｅｖａｌｕａｔｉｏｎｏｆｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓｆｏｒｆａｃｅｒｅｃｏｇｎｉ\nｔｉｏｎ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：１４２－１５０．\n［１０１］ＹａｎｇＳ，ＬｕｏＰ，ＬｏｙＣＣ，ｅｔａｌ．ＦａｃｅｎｅｓｓＮｅｔ：Ｆａｃｅｄｅｔｅｃｔｉｏｎｔｈｒｏｕｇｈｄｅｅｐｆａｃｉａｌｐａｒｔｒｅｓｐｏｎｓｅｓ［Ｊ］．ＩＥＥＥＴｒａｎｓｏｎＰａｔｔｅｒｎＡｎａｌｙｓｉｓａｎｄ\nＭａｃｈｉｎｅＩｎｔｅｌｌｉｇｅｎｃｅ，２０１７，ＰＰ（９９）：１－１４．\n［１０２］蔺素珍，韩泽．基于深度堆叠卷积神经网络的图像融合［Ｊ］．计算机学报，２０１７，４０（１１）：２５０６－２５１８．\nＬｉｎＳＺ，ＨａｎＺ．Ｉｍａｇｅｓｆｕｓｉｏｎｂａｓｅｄｏｎｄｅｅｐｓｔａｃｋｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋ［Ｊ］．ＣｈｉｎｅｓｅＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒｓ，２０１７，４０（１１）：２５０６－\n２５１８．\n［１０３］ＬｏｎｇＪ，ＳｈｅｌｈａｍｅｒＥ，ＤａｒｒｅｌｌＴ．Ｆｕｌｌｙｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｓｅｍａｎｔｉｃｓｅｇｍｅｎｔａｔｉｏｎ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔ\nｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：３４３１－３４４０．\n［１０４］ＣａｏＺ，ＳｉｍｏｎＴ，ＷｅｉＳＥ，ｅｔａｌ．Ｒｅａｌｔｉｍｅｍｕｌｔｉｐｅｒｓｏｎ２Ｄｐｏｓｅｅｓｔｉｍａｔｉｏｎｕｓｉｎｇｐａｒｔａｆｆｉｎｉｔｙｆｉｅｌｄｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１０５］ＴｉａｎＹ，ＬｕｏＰ，ＷａｎｇＸ，ｅｔａｌ．Ｄｅｅｐｌｅａｒｎｉｎｇｓｔｒｏｎｇｐａｒｔｓｆｏｒｐｅｄｅｓｔｒｉａｎｄｅｔｅｃｔｉｏｎ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａ\nｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：１９０４－１９１２．\n［１０６］ＺｈｏｕＢ，ＬａｐｅｄｒｉｚａＡ，ＸｉａｏＪ，ｅｔａｌ．Ｌｅａｒｎｉｎｇｄｅｅｐｆｅａｔｕｒｅｓｆｏｒｓｃｅｎｅｒｅｃｏｇｎｉｔｉｏｎｕｓｉｎｇｐｌａｃｅｓｄａｔａｂａｓｅ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌ\nＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１４：４８７－４９５．\n［１０７］ＦｅｒｎａｎｄｏＢ，ＧｏｕｌｄＳ．Ｌｅａｒｎｉｎｇｅｎｄｔｏｅｎｄｖｉｄｅｏｃｌａｓｓｉｆｉｃａｔｉｏｎｗｉｔｈｒａｎｋｐｏｏｌｉｎｇ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ．Ｎｅｗ\nＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１６：１１８７－１１９６．\n［１０８］ＬａｎＺ，ＺｈｕＹ，ＨａｕｐｔｍａｎｎＡＧ，ｅｔａｌ．Ｄｅｅｐｌｏｃａｌｖｉｄｅｏｆｅａｔｕｒｅｆｏｒａｃｔｉｏｎｒｅｃｏｇｎｉｔｉｏｎ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄＰａｔ\nｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１７：１２１９－１２２５．\n［１０９］ＣｈｅｎｇＺ，ＹａｎｇＱ，ＳｈｅｎｇＢ．Ｄｅｅｐｃｏｌｏｒｉｚａｔｉｏｎ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：\n４１５－４２３．\n［１１０］ＣｈａｍｐａｎｄａｒｄＡＪ．Ｓｅｍａｎｔｉｃｓｔｙｌｅｔｒａｎｓｆｅｒａｎｄｔｕｒｎｉｎｇｔｗｏｂｉｔｄｏｏｄｌｅｓｉｎｔｏｆｉｎｅａｒｔｗｏｒｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１１１］ＧａｔｙｓＬＡ，ＥｃｋｅｒＡＳ，ＢｅｔｈｇｅＭ．Ｉｍａｇｅｓｔｙｌｅｔｒａｎｓｆｅｒｕｓｉｎｇｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ\nＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１６：２４１４－２４２３．\n［１１２］ＤａｈｌＲ，ＮｏｒｏｕｚｉＭ，ＳｈｌｅｎｓＪ．Ｐｉｘｅｌｒｅｃｕｒｓｉｖｅｓｕｐｅｒｒｅｓｏｌｕｔｉｏｎ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n［１１３］ＡｓｓａｅｌＹＭ，ＳｈｉｌｌｉｎｇｆｏｒｄＢ，ＷｈｉｔｅｓｏｎＳ，ｅｔａｌ．ＬｉｐＮｅｔ：Ｅｎｄｔｏｅｎｄｓｅｎｔｅｎｃｅｌｅｖｅｌｌｉｐｒｅａｄｉｎｇ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１１４］ＣｏｌｌｏｂｅｒｔＲ，ＷｅｓｔｏｎＪ，ＢｏｔｔｏｕＬ，ｅｔａｌ．Ｎａｔｕｒａｌｌａｎｇｕａｇｅｐｒｏｃｅｓｓｉｎｇ（ａｌｍｏｓｔ）ｆｒｏｍｓｃｒａｔｃｈ［Ｊ］．ＪｏｕｒｎａｌｏｆＭａｃｈｉｎｅＬｅａｒｎｉｎｇＲｅｓｅａｒｃｈ，\n２０１１，１２（１）：２４９３－２５３７．\n［１１５］ＭｉｋｏｌｏｖＴ，ＳｕｔｓｋｅｖｅｒＩ，ＣｈｅｎＫ，ｅｔａｌ．Ｄｉｓｔｒｉｂｕｔｅｄｒｅｐｒｅｓｅｎｔａｔｉｏｎｓｏｆｗｏｒｄｓａｎｄｐｈｒａｓｅｓａｎｄｔｈｅｉｒｃｏｍｐｏｓｉｔｉｏｎａｌｉｔｙ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅ\nｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１３：３１１１－３１１９．\n［１１６］ＷｅｉｓｓＤ，ＡｌｂｅｒｔｉＣ，ＣｏｌｌｉｎｓＭ，ｅｔａｌ．Ｓｔｒｕｃｔｕｒｅｄｔｒａｉｎｉｎｇｆｏｒｎｅｕｒａｌｎｅｔｗｏｒｋｔｒａｎｓｉｔｉｏｎｂａｓｅｄｐａｒｓｉｎｇ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［１１７］ＬａｍｐｌｅＧ，ＢａｌｌｅｓｔｅｒｏｓＭ，ＳｕｂｒａｍａｎｉａｎＳ，ｅｔａｌ．Ｎｅｕｒａｌａｒｃｈｉｔｅｃｔｕｒｅｓｆｏｒｎａｍｅｄｅｎｔｉｔｙｒｅｃｏｇｎｉｔｉｏｎ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１１８］ＨｅＬ，ＬｅｅＫ，ＬｅｗｉｓＭ，ｅｔａｌ．Ｄｅｅｐｓｅｍａｎｔｉｃｒｏｌｅｌａｂｅｌｉｎｇ：Ｗｈａｔｗｏｒｋｓａｎｄｗｈａｔ′ｓｎｅｘｔ［Ｃ］／／ＡｎｎｕａｌＭｅｅｔｉｎｇｏｆｔｈｅＡｓｓｏｃｉａｔｉｏｎｆｏｒＣｏｍｐｕ\nｔａｔｉｏｎａｌＬｉｎｇｕｉｓｔｉｃｓ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１７：４７３－４８３．\n［１１９］ＫｉｍＹ，ＪｅｒｎｉｔｅＹ，ＳｏｎｔａｇＤ，ｅｔａｌ．Ｃｈａｒａｃｔｅｒａｗａｒｅｎｅｕｒａｌｌａｎｇｕａｇｅｍｏｄｅｌｓ［Ｃ］／／ＡＡＡＩＣｏｎｆｅｒｅｎｃｅｏｎＡｒｔｉｆｉｃｉａｌＩｎｔｅｌｌｉｇｅｎｃｅ．Ｋｅｙｓｔｏｎｅ，\nＵＳＡ：ＡＡＡＩ，２０１６：２７４１－２７４９．\n［１２０］ＳｅｖｅｒｙｎＡ，ＭｏｓｃｈｉｔｔｉＡ．Ｔｗｉｔｔｅｒｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓｗｉｔｈｄｅｅｐｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＲｅｓｅａｒｃｈｏｎ\nＤｅｖｅｌｏｐｍｅｎｔｉｎＩｎｆｏｒｍａｔｉｏｎＲｅｔｒｉｅｖａｌ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１５：９５９－９６２．\n［１２１］何炎祥，孙松涛，牛菲菲，等．用于微博情感分析的一种情感语义增强的深度学习模型［Ｊ］．计算机学报，２０１７，４０（４）：７７３－７９０．\nＨｅＹＸ，ＳｕｎＳＴ，ＮｉｕＦＦ，ｅｔａｌ．Ａｄｅｅｐｌｅａｒｎｉｎｇｍｏｄｅｌｅｎｈａｎｃｅｄｗｉｔｈｅｍｏｔｉｏｎｓｅｍａｎｔｉｃｓｆｏｒｍｉｃｒｏｂｌｏｇｓｅｎｔｉｍｅｎｔａｎａｌｙｓｉｓ［Ｊ］．Ｃｈｉｎｅｓｅ\nＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒｓ，２０１７，４０（４）：７７３－７９０．\n［１２２］ＪｏｕｌｉｎＡ，ＧｒａｖｅＥ，ＢｏｊａｎｏｗｓｋｉＰ，ｅｔａｌ．Ｂａｇｏｆｔｒｉｃｋｓｆｏｒｅｆｆｉｃｉｅｎｔｔｅｘｔｃｌａｓｓｉｆｉｃａｔｉｏｎ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１２３］ＧｅｈｒｉｎｇＪ，ＡｕｌｉＭ，ＧｒａｎｇｉｅｒＤ，ｅｔａｌ．Ｃｏｎｖｏｌｕｔｉｏｎａｌｓｅｑｕｅｎｃｅｔｏｓｅｑｕｅｎｃｅｌｅａｒｎｉｎｇ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n６\n９\n３\n信息与控制　　　　　　　　　　　　　　　　　　４７卷\n［１２４］ＨｅｒｍａｎｎＫＭ，ＫｏｃｉｓｋｙＴ，ＧｒｅｆｅｎｓｔｅｔｔｅＥ，ｅｔａｌ．Ｔｅａｃｈｉｎｇｍａｃｈｉｎｅｓｔｏｒｅａｄａｎｄｃｏｍｐｒｅｈｅｎｄ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎ\nＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１５：１６９３－１７０１．\n［１２５］ＺｈｏｕＸ，ＤｏｎｇＤ，ＷｕＨ，ｅｔａｌ．Ｍｕｌｔｉｖｉｅｗｒｅｓｐｏｎｓｅｓｅｌｅｃｔｉｏｎｆｏｒｈｕｍａｎｃｏｍｐｕｔｅｒｃｏｎｖｅｒｓａｔｉｏｎ［Ｃ］／／ＣｏｎｆｅｒｅｎｃｅｏｎＥｍｐｉｒｉｃａｌＭｅｔｈｏｄｓｉｎ\nＮａｔｕｒａｌＬａｎｇｕａｇｅＰｒｏｃｅｓｓｉｎｇ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１６：３７２－３８１．\n［１２６］ＭａＪ，ＳｈｅｒｉｄａｎＲＰ，ＬｉａｗＡ，ｅｔａｌ．Ｄｅｅｐｎｅｕｒａｌｎｅｔｓａｓａｍｅｔｈｏｄｆｏｒｑｕａｎｔｉｔａｔｉｖｅｓｔｒｕｃｔｕｒｅａｃｔｉｖｉｔｙｒｅｌａｔｉｏｎｓｈｉｐｓ［Ｊ］．ＪｏｕｒｎａｌｏｆＣｈｅｍｉｃａｌＩｎ\nｆｏｒｍａｔｉｏｎ＆Ｍｏｄｅｌｉｎｇ，２０１５，５５（２）：２６３－２７４．\n［１２７］ＬｉｕＮ，ＨａｎＪ，ＺｈａｎｇＤ，ｅｔａｌ．Ｐｒｅｄｉｃｔｉｎｇｅｙｅｆｉｘａｔｉｏｎｓｕｓｉｎｇｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩＥＥＥＣｏｎｆｅｒｅｎｃｅｏｎＣｏｍｐｕｔｅｒＶｉｓｉｏｎａｎｄ\nＰａｔｔｅｒｎＲｅｃｏｇｎｉｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１５：３６２－３７０．\n［１２８］ＬｅｕｎｇＭＫ，ＸｉｏｎｇＨＹ，ＬｅｅＬＪ，ｅｔａｌ．Ｄｅｅｐｌｅａｒｎｉｎｇｏｆｔｈｅｔｉｓｓｕｅｒｅｇｕｌａｔｅｄｓｐｌｉｃｉｎｇｃｏｄｅ［Ｊ］．Ｂｉｏｉｎｆｏｒｍａｔｉｃｓ，２０１４，３０（１２）：ｉ１２１－\nｉ１２９．\n［１２９］ＤｉｘｏｎＭ，ＫｌａｂｊａｎＤ，ＢａｎｇＪＨ．Ｃｌａｓｓｉｆｉｃａｔｉｏｎｂａｓｅｄｆｉｎａｎｃｉａｌｍａｒｋｅｔｓｐｒｅｄｉｃｔｉｏｎｕｓｉｎｇｄｅｅｐｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１３０］ＨｅａｔｏｎＪＢ，ＰｏｌｓｏｎＮＧ，ＷｉｔｔｅＪＨ．Ｄｅｅｐｌｅａｒｎｉｎｇｆｏｒｆｉｎａｎｃｅ：Ｄｅｅｐｐｏｒｔｆｏｌｉｏｓ［Ｊ］．ＡｐｐｌｉｅｄＳｔｏｃｈａｓｔｉｃＭｏｄｅｌｓｉｎＢｕｓｉｎｅｓｓ＆Ｉｎｄｕｓｔｒｙ，\n２０１６，３３（１）：３－１２．\n［１３１］ＺｈａｎｇＲ，ＬｉＷ，ＴａｎＷ，ｅｔａｌ．Ｄｅｅｐａｎｄｓｈａｌｌｏｗｍｏｄｅｌｆｏｒｉｎｓｕｒａｎｃｅｃｈｕｒｎｐｒｅｄｉｃｔｉｏｎｓｅｒｖｉｃｅ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＳｅｒｖｉｃｅｓ\nＣｏｍｐｕｔｉｎｇ．Ｐｉｓｃａｔａｗａｙ，ＮＪ，ＵＳＡ：ＩＥＥＥ，２０１７：３４６－３５３．\n［１３２］柯余洋，杨训政，熊焰，等．基于递归神经网络和蚁群优化算法的发电环保调度［Ｊ］．信息与控制，２０１７，４６（４）：４１５－４２１．\nＫｅＹＹ，ＹａｎｇＸＺ，ＸｉｏｎｇＹ，ｅｔａｌ．Ｐｏｗｅｒｇｅｎｅｒａｔｉｏｎｄｉｓｐａｔｃｈｉｎｇｆｏｒｅｎｖｉｒｏｎｍｅｎｔａｌｐｒｏｔｅｃｔｉｏｎｂａｓｅｄｏｎｒｅｃｕｒｓｉｖｅｎｅｕｒａｌｎｅｔｗｏｒｋａｎｄａｎｔｃｏｌｏ\nｎｙｏｐｔｉｍｉｚａｔｉｏｎａｌｇｏｒｉｔｈｍ［Ｊ］．ＩｎｆｏｒｍａｔｉｏｎａｎｄＣｏｎｔｒｏｌ，２０１７，４６（４）：４１５－４２１．\n［１３３］于德亮，李妍美，丁宝，等．基于思维进化算法和ＢＰ神经网络的电动潜油柱塞泵故障诊断方法［Ｊ］．信息与控制，２０１７，４６（６）：\n６９８－７０５．\nＹｕＤＬ，ＬｉＹＭ，ＤｉｎｇＢ，ｅｔａｌ．Ｆａｉｌｕｒｅｄｉａｇｎｏｓｉｓｍｅｔｈｏｄｆｏｒｅｌｅｃｔｒｉｃｓｕｂｍｅｒｓｉｂｌｅｐｌｕｎｇｅｒｐｕｍｐｂａｓｅｄｏｎｍｉｎｄｅｖｏｌｕｔｉｏｎａｒｙａｌｇｏｒｉｔｈｍａｎｄ\nｂａｃｋｐｒｏｐａｇａｔｉｏｎｎｅｕｒａｌｎｅｔｗｏｒｋ［Ｊ］．ＩｎｆｏｒｍａｔｉｏｎａｎｄＣｏｎｔｒｏｌ，２０１７，４６（６）：６９８－７０５．\n［１３４］李军，桑桦．基于ＳＣＫＦ的Ｅｌｍａｎ递归神经网络在软测量建模中的应用［Ｊ］．信息与控制，２０１７，４６（３）：３４２－３４９．\nＬｉＪ，ＳａｎｇＨ．ＥｌｍａｎｒｅｃｕｒｒｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｍｅｔｈｏｄｂａｓｅｄｏｎＳＣＫＦａｌｇｏｒｉｔｈｍａｎｄｉｔｓａｐｐｌｉｃａｔｉｏｎｔｏｓｏｆｔｓｅｎｓｏｒｍｏｄｅｌｉｎｇ［Ｊ］．Ｉｎｆｏｒｍａｔｉｏｎ\nａｎｄＣｏｎｔｒｏｌ，２０１７，４６（３）：３４２－３４９．\n［１３５］ＷｕＹ，ＳｃｈｕｓｔｅｒＭ，ＣｈｅｎＺ，ｅｔａｌ．Ｇｏｏｇｌｅ′ｓｎｅｕｒａｌｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎｓｙｓｔｅｍ：Ｂｒｉｄｇｉｎｇｔｈｅｇａｐｂｅｔｗｅｅｎｈｕｍａｎａｎｄｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎ\n［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１３６］ＳｒｉｖａｓｔａｖａＲＫ，ＧｒｅｆｆＫ，ＳｃｈｍｉｄｈｕｂｅｒＪ．Ｔｒａｉｎｉｎｇｖｅｒｙｄｅｅｐｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．\nＣａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１５：２３７７－２３８５．\n［１３７］ＧｏｏｄｆｅｌｌｏｗＩ，ＰｏｕｇｅｔＡｂａｄｉｅＪ，ＭｉｒｚａＭ，ｅｔａｌ．Ｇｅｎｅｒａｔｉｖｅａｄｖｅｒｓａｒｉａｌｎｅｔｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓ\nｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１４：２６７２－２６８０．\n［１３８］ＨｅＤ，ＸｉａＹ，ＱｉｎＴ，ｅｔａｌ．Ｄｕａｌｌｅａｒｎｉｎｇｆｏｒｍａｃｈｉｎｅｔｒａｎｓｌａｔｉｏｎ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍ\nｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１６：８２０－８２８．\n［１３９］ＺｈｅｎｇＳ，ＭｅｎｇＱ，ＷａｎｇＴ，ｅｔａｌ．Ａｓｙｎｃｈｒｏｎｏｕｓｓｔｏｃｈａｓｔｉｃｇｒａｄｉｅｎｔｄｅｓｃｅｎｔｗｉｔｈｄｅｌａｙｃｏｍｐｅｎｓａｔｉｏｎ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａ\nｃｈｉｎｅＬｅａｒｎｉｎｇ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：ＡＣＭ，２０１７：４１２０－４１２９．\n［１４０］ＳｔｒｏｍＮ．Ｓｃａｌａｂｌｅｄｉｓｔｒｉｂｕｔｅｄｄｎｎｔｒａｉｎｉｎｇｕｓｉｎｇｃｏｍｍｏｄｉｔｙｇｐｕｃｌｏｕｄｃｏｍｐｕｔｉｎｇ［Ｃ］／／ＣｏｎｆｅｒｅｎｃｅｏｆｔｈｅＩｎｔｅｒｎａｔｉｏｎａｌＳｐｅｅｃｈＣｏｍｍｕｎｉｃａｔｉｏｎ\nＡｓｓｏｃｉａｔｉｏｎ．Ｐｉｓｃａｔａｗａｙ，ＵＳＡ：ＩＥＥＥ，２０１５．\n［１４１］ＳｚｅｇｅｄｙＣ，ＺａｒｅｍｂａＷ，ＳｕｔｓｋｅｖｅｒＩ，ｅｔａｌ．Ｉｎｔｒｉｇｕｉｎｇｐｒｏｐｅｒｔｉｅｓｏｆｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＬｅａｒｎｉｎｇＲｅｐｒｅｓｅｎｔａ\nｔｉｏｎｓ．Ｐｉｓｃａｔａｗａｙ，ＵＳＡ：ＩＥＥＥ，２０１４．\n［１４２］ＧｏｏｄｆｅｌｌｏｗＩＪ，ＳｈｌｅｎｓＪ，ＳｚｅｇｅｄｙＣ．Ｅｘｐｌａｉｎｉｎｇａｎｄｈａｒｎｅｓｓｉｎｇａｄｖｅｒｓａｒｉａｌｅｘａｍｐｌｅｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１４．\n［１４３］ＡｎｄｒｙｃｈｏｗｉｃｚＭ，ＤｅｎｉｌＭ，ＧｏｍｅｚＳ，ｅｔａｌ．Ｌｅａｒｎｉｎｇｔｏｌｅａｒｎｂｙｇｒａｄｉｅｎｔｄｅｓｃｅｎｔｂｙｇｒａｄｉｅｎｔｄｅｓｃｅｎｔ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎ\nｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１６：３９８１－３９８９．\n［１４４］韩红桂，甄博然，乔俊飞．动态结构优化神经网络及其在溶解氧控制中的应用［Ｊ］．信息与控制，２０１０，３９（３）：３５４－３６０．\nＨａｎＨＧ，ＺｈｅｎＢＲ，ＱｉａｏＪＦ．Ｄｙｎａｍｉｃｓｔｒｕｃｔｕｒｅｏｐｔｉｍｉｚａｔｉｏｎｎｅｕｒａｌｎｅｔｗｏｒｋａｎｄｉｔｓａｐｐｌｉｃａｔｉｏｎｓｔｏｄｉｓｓｏｌｖｅｄｏｘｙｇｅｎｉｃ（ＤＯ）ｃｏｎｔｒｏｌ．Ｉｎｆｏｒ\nｍａｔｉｏｎａｎｄＣｏｎｔｒｏｌ，２０１０，３９（３）：３５４－３６０．\n［１４５］张昭昭，乔俊飞，余文．多层自适应模块化神经网络结构设计［Ｊ］．计算机学报，２０１７，４０（１２）：２８２７－２８３８．\nＺｈａｎｇＺＺ，ＱｉａｏＪＦ，ＹｕＷ．Ｓｔｒｕｃｔｕｒｅｄｅｓｉｇｎｏｆｈｉｅｒａｒｃｈｉｃａｌａｄａｐｔｉｖｅｍｏｄｕｌａｒｎｅｕｒａｌｎｅｔｗｏｒｋ［Ｊ］．ＣｈｉｎｅｓｅＪｏｕｒｎａｌｏｆＣｏｍｐｕｔｅｒｓ，２０１７，４０\n（１２）：２８２７－２８３８．\n［１４６］ＷｕＭ，ＨｕｇｈｅｓＭＣ，ＰａｒｂｈｏｏＳ，ｅｔａｌ．Ｂｅｙｏｎｄｓｐａｒｓｉｔｙ：Ｔｒｅｅｒｅｇｕｌａｒｉｚａｔｉｏｎｏｆｄｅｅｐｍｏｄｅｌｓｆｏｒｉｎｔｅｒｐｒｅｔａｂｉｌｉｔｙ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n［１４７］ＴｒａｎＪ，ＴｒａｎＪ，ＴｒａｎＪ，ｅｔａｌ．Ｌｅａｒｎｉｎｇｂｏｔｈｗｅｉｇｈｔｓａｎｄｃｏｎｎｅｃｔｉｏｎｓｆｏｒｅｆｆｉｃｉｅｎｔｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒ\nｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１５：１１３５－１１４３．\n［１４８］ＤｅｎｔｏｎＥＬ，ＺａｒｅｍｂａＷ，ＢｒｕｎａＪ，ｅｔａｌ．Ｅｘｐｌｏｉｔｉｎｇｌｉｎｅａｒｓｔｒｕｃｔｕｒｅｗｉｔｈｉｎｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓｆｏｒｅｆｆｉｃｉｅｎｔｅｖａｌｕａｔｉｏｎ［Ｃ］／／ＡｎｎｕａｌＣｏｎ\nｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１４：１２６９－１２７７．\n（下转第４１０页）\n７\n９\n３\n４期\n张荣，等：深度学习研究综述\n２７５－２８１．\n［８９］ＺｈａｏＪ，ＣｈｅｖａｌｉｅｒＦ，ＣｏｌｌｉｎｓＣ，ｅｔａｌ．Ｆａｃｉｌｉｔａｔｉｎｇｄｉｓｃｏｕｒｓｅａｎａｌｙｓｉｓｗｉｔｈｉｎｔｅｒａｃｔｉｖｅｖｉｓｕａｌｉｚａｔｉｏｎ［Ｊ］．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＶｉｓｕａｌｉｚａｔｉｏｎａｎｄ\nＣｏｍｐｕｔｅｒＧｒａｐｈｉｃｓ，２０１２，１８（１２）：２６３９－２６４８．\n［９０］ＣｏｌｌｉｎｓＣ，ＣａｒｐｅｎｄａｌｅＳ，ＰｅｎｎＧ．ＤｏｃｕＢｕｒｓｔ：Ｖｉｓｕａｌｉｚｉｎｇｄｏｃｕｍｅｎｔｃｏｎｔｅｎｔｕｓｉｎｇｌａｎｇｕａｇｅｓｔｒｕｃｔｕｒｅ［Ｊ］．ＣｏｍｐｕｔｅｒＧｒａｐｈｉｃｓＦｏｒｕｍ，２００９，\n２８（３）：１０３９－１０４６．\n［９１］ＨｅｒｍａｎＩ，ＭｅｌａｎｃｏｎＧ，ＭａｒｓｈａｌｌＭＳ．Ｇｒａｐｈｖｉｓｕａｌｉｚａｔｉｏｎａｎｄｎａｖｉｇａｔｉｏｎｉｎｉｎｆｏｒｍａｔｉｏｎｖｉｓｕａｌｉｚａｔｉｏｎ：Ａｓｕｒｖｅｙ［Ｊ］．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎ\nＶｉｓｕａｌｉｚａｔｉｏｎａｎｄＣｏｍｐｕｔｅｒＧｒａｐｈｉｃｓ，２０００，６（１）：２４－４３．\n［９２］ＳｈｎｅｉｄｅｒｍａｎＢ．Ｔｒｅｅｖｉｓｕａｌｉｚａｔｉｏｎｗｉｔｈｔｒｅｅｍａｐｓ：２ｎｄｓｐａｃｉｎｇｆｉｌｌｉｎｇａｐｐｒｏａｃｈ［Ｊ］．ＡＣＭＴｒａｎｓａｃｔｉｏｎｓｏｎＧｒａｐｈｉｃｓ，１９９２，１１（１）：９２－９９．\n［９３］ＨａｌｅｖｉＧ，ＭｏｅｄＨ．Ｔｈｅｅｖｏｌｕｔｉｏｎｏｆｂｉｇｄａｔａａｓａｒｅｓｅａｒｃｈａｎｄｓｃｉｅｎｔｉｆｉｃｔｏｐｉｃ：Ｏｖｅｒｖｉｅｗｏｆｔｈｅｌｉｔｅｒａｔｕｒｅ［Ｊ］．ＲｅｓｅａｒｃｈＴｒｅｎｄｓ，２０１２，３０\n（１）：３－６．\n［９４］ＨｅｙＴ，ＧａｎｎｏｎＤ，ＰｉｎｋｅｌｍａｎＪ．Ｔｈｅｆｕｔｕｒｅｏｆｄａｔａｉｎｔｅｎｓｉｖｅｓｃｉｅｎｃｅ［Ｊ］．Ｃｏｍｐｕｔｅｒ，２０１２，４５（５）：８１－８２．\n［９５］ＫｅｉｍＤＡ，ＫｒｉｅｇｅｌＨＰ．Ｖｉｓｕａｌｉｚａｔｉｏｎｔｅｃｈｎｉｑｕｅｓｆｏｒｍｉｎｉｎｇｌａｒｇｅｄａｔａｂａｓｅｓ：Ａｃｏｍｐａｒｉｓｏｎ［Ｊ］．ＩＥＥＥＴｒａｎｓａｃｔｉｏｎｓｏｎＫｎｏｗｌｅｄｇｅａｎｄＤａｔａ\nＥｎｇｉｎｅｅｒｉｎｇ，１９９６，８（６）：９２３－９３８．\n［９６］ＡｈｌｂｅｒｇＣ，ＳｈｎｅｉｄｅｒｍａｎＢ．Ｖｉｓｕａｌｉｎｆｏｒｍａｔｉｏｎｓｅｅｋｉｎｇ：Ｔｉｇｈｔｃｏｕｐｌｉｎｇｏｆｄｙｎａｍｉｃｑｕｅｒｙｆｉｌｔｅｒｓｗｉｔｈｓｔａｒｆｉｅｌｄｄｉｓｐｌａｙｓ［Ｃ］／／Ｐｒｏｃｅｅｄｉｎｇｓｏｆｔｈｅ\nＳＩＧＣＨＩＣｏｎｆｅｒｅｎｃｅｏｎＨｕｍａｎＦａｃｔｏｒｓｉｎＣｏｍｐｕｔｉｎｇＳｙｓｔｅｍｓ．ＮｅｗＹｏｒｋ，ＵＳＡ：ＡＣＭ，１９９４：３１３－３１７．\n［９７］胡琴琴．基于Ｈａｄｏｏｐ的数据可视化技术研究与应用［Ｄ］．北京：北方工业大学，２０１６．\nＨｕＱＱ．ＲｅｓｅａｒｃｈａｎｄａｐｐｌｉｃａｔｉｏｎｏｆｄａｔａｖｉｓｕａｌｉｚａｔｉｏｎｔｅｃｈｎｏｌｏｇｙｂａｓｅｄｏｎＨａｄｏｏｐ［Ｄ］．Ｂｅｉｊｉｎｇ：ＮｏｒｔｈＣｈｉｎａＵｎｉｖｅｒｓｉｔｙｏｆＴｅｃｈｎｏｌｏｇｙ，\n２０１６．\n［９８］张向宇．工业炉温度场可视化与辐射特性参数解耦重建研究［Ｄ］．武汉：华中科技大学，２０１１．\nＺｈａｎｇＸＹ．Ｒｅｓｅａｒｃｈｏｎｖｉｓｕａｌｉｚａｔｉｏｎｏｆｔｅｍｐｅｒａｔｕｒｅｆｉｅｌｄｏｆｉｎｄｕｓｔｒｉａｌｆｕｒｎａｃｅａｎｄｄｅｃｏｕｐｌｉｎｇｒｅｃｏｎｓｔｒｕｃｔｉｏｎｏｆｒａｄｉａｔｉｏｎｃｈａｒａｃｔｅｒｉｓｔｉｃｐａｒａｍｅ\nｔｅｒｓ［Ｄ］．Ｗｕｈａｎ：ＨｕａｚｈｏｎｇＵｎｉｖｅｒｓｉｔｙｏｆＳｃｉｅｎｃｅａｎｄＴｅｃｈｎｏｌｏｇｙ，２０１１．\n作者简介\n何文韬（１９９４－），男，硕士生．研究领域为复杂工业过程建模与集成优化控制理论．\n邵　诚（１９５８－），男，博士，教授，博士生导师．研究领域为复杂工业过程建模与集成优化控制理论．\n（上接第３９７页）\n［１４９］ＣｏｈｅｎＴ，ＷｅｌｌｉｎｇＭ．Ｇｒｏｕｐｅｑｕｉｖａｒｉａｎｔｃｏｎｖｏｌｕｔｉｏｎａｌｎｅｔｗｏｒｋｓ［Ｃ］／／ＩｎｔｅｒｎａｔｉｏｎａｌＣｏｎｆｅｒｅｎｃｅｏｎＭａｃｈｉｎｅＬｅａｒｎｉｎｇ．ＮｅｗＹｏｒｋ，ＮＪ，ＵＳＡ：\nＡＣＭ，２０１６：２９９０－２９９９．\n［１５０］ＨｉｎｔｏｎＧ，ＶｉｎｙａｌｓＯ，ＤｅａｎＪ．Ｄｉｓｔｉｌｌｉｎｇｔｈｅｋｎｏｗｌｅｄｇｅｉｎａｎｅｕｒａｌｎｅｔｗｏｒｋ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１５．\n［１５１］ＣｈｅｎｇＹ，ＷａｎｇＤ，ＺｈｏｕＰ，ｅｔａｌ．Ａｓｕｒｖｅｙｏｆｍｏｄｅｌｃｏｍｐｒｅｓｓｉｏｎａｎｄａｃｃｅｌｅｒａｔｉｏｎｆｏｒｄｅｅｐｎｅｕｒａｌｎｅｔｗｏｒｋｓ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n［１５２］ＫｅＧ，ＭｅｎｇＱ，ＦｉｎｌｅｙＴ，ｅｔａｌ．ＬｉｇｈｔＧＢＭ：Ａｈｉｇｈｌｙｅｆｆｉｃｉｅｎｔｇｒａｄｉｅｎｔｂｏｏｓｔｉｎｇｄｅｃｉｓｉｏｎｔｒｅｅ［Ｃ］／／ＡｎｎｕａｌＣｏｎｆｅｒｅｎｃｅｏｎＮｅｕｒａｌＩｎｆｏｒｍａｔｉｏｎ\nＰｒｏｃｅｓｓｉｎｇＳｙｓｔｅｍｓ．Ｃａｍｂｒｉｄｇｅ，ＵＳＡ：ＭＩＴＰｒｅｓｓ，２０１７：３１４９－３１５７．\n［１５３］ＭｏＴ，ＺｈａｎｇＲ，ＬｉＷ，ｅｔａｌ．Ａｎｉｎｆｌｕｅｎｃｅｂａｓｅｄｆａｓｔｐｒｅｃｅｄｉｎｇｑｕｅｓｔｉｏｎｎａｉｒｅｍｏｄｅｌｆｏｒｅｌｄｅｒｌｙａｓｓｅｓｓｍｅｎｔｓ［Ｊ］．ＩｎｔｅｌｌｉｇｅｎｔＤａｔａＡｎａｌｙｓｉｓ，\n２０１８，２２（２）：４０７－４３７．\n［１５４］ＳｈａｚｅｅｒＮ，ＭｉｒｈｏｓｅｉｎｉＡ，ＭａｚｉａｒｚＫ，ｅｔａｌ．Ｏｕｔｒａｇｅｏｕｓｌｙｌａｒｇｅｎｅｕｒａｌｎｅｔｗｏｒｋｓ：Ｔｈｅｓｐａｒｓｅｌｙｇａｔｅｄｍｉｘｔｕｒｅｏｆｅｘｐｅｒｔｓｌａｙｅｒ［Ｊ］．ＥｐｒｉｎｔＡｒｘ\nｉｖ，２０１７．\n［１５５］ＤｏｍｉｎｇｏｓＰ．Ｔｈｅｍａｓｔｅｒａｌｇｏｒｉｔｈｍ：Ｈｏｗｔｈｅｑｕｅｓｔｆｏｒｔｈｅｕｌｔｉｍａｔｅｌｅａｒｎｉｎｇｍａｃｈｉｎｅｗｉｌｌｒｅｍａｋｅｏｕｒｗｏｒｌｄ［Ｍ］．ＮｅｗＹｏｒｋ，ＵＳＡ：Ｂａｓｉｃ\nＢｏｏｋｓ，ａｍｅｍｂｅｒｏｆｔｈｅＰｅｒｓｅｕｓＢｏｏｋｓＧｒｏｕｐ，２０１５．\n［１５６］ＳａｌｉｍａｎｓＴ，ＨｏＪ，ＣｈｅｎＸ，ｅｔａｌ．Ｅｖｏｌｕｔｉｏｎｓｔｒａｔｅｇｉｅｓａｓａｓｃａｌａｂｌｅａｌｔｅｒｎａｔｉｖｅｔｏｒｅｉｎｆｏｒｃｅｍｅｎｔｌｅａｒｎｉｎｇ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n［１５７］ＴｒａｎＤ，ＫｕｃｕｋｅｌｂｉｒＡ，ＤｉｅｎｇＡＢ，ｅｔａｌ．Ｅｄｗａｒｄ：Ａｌｉｂｒａｒｙｆｏｒｐｒｏｂａｂｉｌｉｓｔｉｃｍｏｄｅｌｉｎｇ，ｉｎｆｅｒｅｎｃｅ，ａｎｄｃｒｉｔｉｃｉｓｍ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１６．\n［１５８］ＳｈｉＪ，ＣｈｅｎＪ，ＺｈｕＪ，ｅｔａｌ．ＺｈｕＳｕａｎ：ＡｌｉｂｒａｒｙｆｏｒＢａｙｅｓｉａｎｄｅｅｐｌｅａｒｎｉｎｇ［Ｊ］．ＥｐｒｉｎｔＡｒｘｉｖ，２０１７．\n作者简介\n张　荣（１９９２－），男，博士生．研究领域为深度学习．\n李伟平（１９７３－），男，博士，教授，博士生导师．研究领域为服务计算，机器学习．\n莫　同（１９８１－），男，博士，副教授，硕士生导师．研究领域为数据挖掘．\n０\n１\n４\n信息与控制　　　　　　　　　　　　　　　　　　４７卷\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-04-05",
  "updated": "2018-08-28"
}