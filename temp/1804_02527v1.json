{
  "id": "http://arxiv.org/abs/1804.02527v1",
  "title": "Visual Analytics for Explainable Deep Learning",
  "authors": [
    "Jaegul Choo",
    "Shixia Liu"
  ],
  "abstract": "Recently, deep learning has been advancing the state of the art in artificial\nintelligence to a new level, and humans rely on artificial intelligence\ntechniques more than ever. However, even with such unprecedented advancements,\nthe lack of explanation regarding the decisions made by deep learning models\nand absence of control over their internal processes act as major drawbacks in\ncritical decision-making processes, such as precision medicine and law\nenforcement. In response, efforts are being made to make deep learning\ninterpretable and controllable by humans. In this paper, we review visual\nanalytics, information visualization, and machine learning perspectives\nrelevant to this aim, and discuss potential challenges and future research\ndirections.",
  "text": " \n \nDEPARTMENT: Visualization Viewpoints                             Editor: Theresa-Marie Rhyne \nVisual Analytics for \nExplainable Deep Learning \nRecently, deep learning has been advancing the state of the art in \nartificial intelligence to a new level, and humans rely on artificial \nintelligence techniques more than ever. However, even with such \nunprecedented advancements, the lack of explanation regarding \nthe decisions made by deep learning models and absence of control over their internal processes act as \nmajor drawbacks in critical decision-making processes, such as precision medicine and law \nenforcement. In response, efforts are being made to make deep learning interpretable and controllable \nby humans. In this paper, we review visual analytics, information visualization, and machine learning \nperspectives relevant to this aim, and discuss potential challenges and future research directions. \nWHY EXPLAINABLE DEEP LEARNING? \nDeep learning has had a considerable impact on various long-running artificial intelligence prob-\nlems, including computer vision, speech recognition and synthesis, and natural language under-\nstanding and generation [1]. As humans rely on artificial intelligence techniques, the \ninterpretability of their decisions and control over their internal processes are becoming a serious \nconcern for various high-impact tasks, such as precision medicine, law enforcement, and finan-\ncial investment. Gender and racial biases learnt by artificial intelligence programs recently \nemerged as a serious issue.1 In April 2016, the European Union legislated the human right to \nrequest an explanation regarding machine-generated decisions.2  \nInterpretation is the process of generating human-understandable explanations on why a particu-\nlar decision is made by a deep learning model. However, the end-to-end learning paradigm hides \nthe entire decision process behind the complicated inner-workings of deep learning models, mak-\ning it difficult to obtain interpretations and explanations.  \nRecently, considerable effort has been invested to tackle this issue. For instance, Defense Ad-\nvanced Research Projects Agency (DARPA) of the United States is launching a large initiative \n                                                                 \n1 https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women/ \n2 https://www.wired.com/2016/07/artificial-intelligence-setting-internet-huge-clash-europe/ \nJaegul Choo \nKorea University \nShixia Liu \nTsinghua University \n  \n \n \nMAGAZINE NAME HERE \ncalled Explainable Artificial Intelligence (XAI). 3 Machine learning and artificial intelligence \ncommunities have conducted many relevant workshops and meetings.4  \nInteractive visualization plays a critical role in enhancing the interpretability of deep learning \nmodels, and it is emerging as a promising research field. Recently, in premier venues in this field \nsuch as IEEE VIS, a growing number of papers concerning interactive visualization and visual \nanalytics for deep learning have been published, and the best paper in IEEE VIS’17 has been \nawarded to a visual analytics system that supports advanced interactive visual capabilities in \nTensorBoard, a visualization tool for Google’s TensorFlow [2].  \nGiven the deluge of deep learning techniques and their applications, this article provides a com-\nprehensive overview of recent research concerning interpretability and explainability of deep \nlearning using basic toolkits, advanced algorithmic techniques, and intuitive and interac-\ntive visual interfaces. Based on a systematic analysis of the aforementioned research, we de-\nscribe current research challenges and promising future directions. \n \nFigure 1. Overview of explainable deep learning \nOVERVIEW OF EXPLAINABLE DEEP LEARNING \nAs shown in Figure 1, explainable deep learning encompasses three major research directions: \nmodel understanding, debugging, and refinement/steering. Model understanding aims to ex-\nplain the rationale behind model predictions and the inner workings of deep learning models, and \nit attempts to make these complex models at least partly understandable. Model debugging is the \nprocess of identifying and addressing defects or issues within a deep learning model that fails to \nconverge or does not achieve an acceptable performance [9]. Model refinement/steering is a \nmethod to interactively incorporate expert knowledge and expertise into the improvement and \nrefinement process of a deep learning model, through a set of rich user interactions, in addition \nto semi-supervised learning or active learning. In the following, we will discuss recent approach-\nes to explainable deep learning in these directions.  \nEDUCATIONAL USE AND INTUITIVE UNDERSTANDING \nWITH INTERACTIVE VISUALIZATION \nInteractive visualization has played an important role in providing an in-depth understanding of \nhow deep learning models work. Tensorflow Playground5 is an effective system for education and \n                                                                 \n3 https://www.darpa.mil/program/explainable-artificial-intelligence \n4 A partial list of workshops can be found at http://icmlviz.github.io/reference/ \n \n \n \nSECTION TITLE HERE \nintuitive understanding, where users can play with simple neural networks by changing various \nconfigurations in terms of the numbers of layers and nodes, and the types of nonlinear units. It \nadopts two-dimensional toy data sets for classification and regression tasks. The manner in which \neach node in the network is activated across different input data values is fully visualized as a \nheatmap in a two-dimensional space. \nAnother web-based deep learning library called ConvNetJS6 features easy access to deep learn-\ning techniques by simply using a web browser, together with a rich set of visualization modules, \nwhich can be effectively used for education and understanding. DeepVis toolbox7 dynamically \nvisualizes the activation map of various filters in a user-selected layer of convolutional neural \nnetworks (CNNs), given a webcam video input in real time.  \nThese tools and systems provide effective interactive visualization for interpreting deep learning \nmodels, but most of them are limited to simple models and basic applications, and thus their \napplicability remains far from real-world problems.  \n \nFigure 2. Embedding projector based on 2D/3D embedding techniques, such as principal component \nanalysis and t-distributed stochastic embedding. In this example, MNIST handwritten images are \nvisualized as rectangles colored by their associated digit labels so that those images with high \nsimilarity in their original feature space are placed close to each other in the 2D/3D space. In this \nmanner, one can easily identify which digit clusters are similar (and thus confusing from a classifier's \nperspective) and which digit images are outliers (and thus confusing as another digit).  \nMODEL DEBUGGING THROUGH VISUALIZATION \nTOOLKITS \nMost deep learning libraries are often accompanied by basic visualization toolkits that allow \nusers to debug current models and improve performance. For example, TensorFlow’s Tensor-\nBoard visualizes the structure of a given computational graph that a user creates and provides \n                                                                                                                                                                               \n5 http://playground.tensorflow.org \n6 https://cs.stanford.edu/people/karpathy/convnetjs/ \n7 http://yosinski.com/deepvis \n  \n \n \nMAGAZINE NAME HERE \nbasic line graphs and histograms of user-selected statistics, such as the loss value and activation \nand/or gradient value of a particular node. Recently, as shown in Figure 2, TensorFlow has been \nequipped with a new visualization module called Embedding Projector.8 This module provides a \n2D/3D embedding view using principal component analysis and t-distributed stochastic neighbor \nembedding, which reveals the relationships between data points with respect to their multi-\ndimensional representations in a given layer.  \nVisdom9 is a web-based interactive visualization toolkit that is easy to use with deep learning \nlibraries such as PyTorch. Deeplearning4j UI10 is another visual user interface, which allows \nusers to monitor the training process with several basic visualization components. \nAlthough these visualization toolkits offer an intuitive presentation of the low-level information \ndirectly provided by deep learning models, it remains difficult for humans to understand the \nbehaviors of these models at a semantically meaningful level.  \nCOMPUTATIONAL METHODS FOR INTERPRETATION AND \nEXPLANATION \nIn the academic field of machine learning, the interpretation of a deep learning model typically \nrefers to the task of identifying the feature importance score, e.g., which part of the input feature \nof a given data item is responsible for the prediction result at the output layer and/or the high \nactivation of an internal layer/node.  \nThe machine learning and artificial intelligence communities have been developing new tech-\nniques to solve this problem. Perturbation experiments [3] and saliency map-based methods [4] \nhave shown their capabilities in revealing which part of the input image is most responsible for \nthe final prediction of a model. A recently proposed technique called LIME [5] approximately \nconstructs a locally linear model from a complex model, in order to allow the interpretation of \nlinear combination weights as the feature importance score.  \nFurthermore, the training data items most responsible for a particular prediction output have \nbeen identified in an efficient manner by adopting a classical technique called influence func-\ntions [6]. The corresponding paper was selected for the best paper award in ICML 2017.  \nIn fact, the integration of these advanced computational methods with an interactive visualization can bring \nsignificant potentials towards explainable deep learning and remains a major challenge in real-world applica-\ntions.  \nVISUAL ANALYTICS FOR IN-DEPTH UNDERSTANDING \nAND MODEL REFINEMENT \nThe visual analytics and information visualization communities have recently developed intui-\ntive and interactive user interfaces with advanced visualization modules. Such visual analytics \nsystems provide users with an in-depth understanding and clues concerning how to troubleshoot \nand further improve a given model [7,8].  \n                                                                 \n8 http://projector.tensorflow.org/ \n9 https://github.com/facebookresearch/visdom \n10 https://deeplearning4j.org/visualization \n \n \n \nSECTION TITLE HERE \n \nFigure 3. Overview of CNNVis [8]. The system employs a DAG to visually illustrate the inner \nworkings of a CNN training process. It also performs bi-clustering to visualize the strength \nof interactions between filters of adjacent layers, along with their most activated images. \nCNNVis [8] is a representative example of a visual analytics system for model understanding \nand the diagnosis of CNNs. As shown in Figure 3, by leveraging the DAG layout and examining \nthe learned representations, CNNVis investigates how the depth and width of neural networks \ninfluence the features detected by the neurons and the model classification performance. It also \nhelps machine learning experts to diagnose a failed training process. ActiVis [10] provides a \nvisual exploratory analysis of a given deep learning model via multiple coordinated views, such \nas a matrix view and an embedding view.  \n \nFigure 4. Overview of LSTMVis [10]. Given a sequential input, e.g., words in a document, the line graphs \nin the top panel visualize the activation patterns of individual hidden nodes over the sequence. Once a \nuser selects a sub-sequence containing multiple words, e.g., the noun phrase \"a little prince,\" and \nspecifies a threshold, the system identifies those hidden nodes exhibiting activation values greater than \nthe threshold and finds other sub-sequence examples for which the same hidden nodes have high \nactivation values. These sub-sequences allow users to interactively explore the learnt behavior of \nhidden nodes.  \n  \n \n \nMAGAZINE NAME HERE \nSome initial effort has been made to understand recurrent neural networks (RNNs) and their \nwidely employed architecture called long short-term memory (LSTM). For example, in order to \nprovide an idea of the semantic meanings of cells in language modeling applications, LSTMVis \n[11] presents the activation patterns of individual cells over time steps or sequences as line \ngraphs (Figure 4). RNNVis [12] performs clustering on hidden-state nodes with similar activa-\ntion patterns and visualizes them as grid-style heatmaps, along with their most strongly associat-\ned keywords.  \nThere also exist visual analytics systems that visualize the training processes of deep learning \nmodels in real time and facilitate the debugging/improvement of model accuracy and computa-\ntional time. ReVACNN [13] is one such visual analytics system for CNNs, which provides real-\ntime model steering capabilities during training, such as dynamically removing/adding nodes and \ninteractively selecting data items for a subsequent mini-batch in the training process. In addition, \nDeepEyes [14] is capable of real-time monitoring and interactive model steering of deep learning \nmodels, for example by highlighting stable nodes and layers. DeepEyes has also made some \ninitial effort towards model refinement, by allowing users to remove filters with very low activa-\ntion values. DGMTracker employs the blue noise sampling algorithm and credit assignment \nalgorithm to detect which portions of the input images cause a training failure for a particular \nimage set in deep generative models, such as generative adversarial networks and variational \nautoencoders [15].  \nAlthough recent visual analytics systems provide sophisticated visualization and interaction ca-\npabilities, research issues on how to effectively loop human into the analysis process and how to \nincrease applicability of explainable deep learning techniques have not been fully investigated. \nFor example, widening user interaction capabilities with deep learning models based on various \nuser needs, tightly integrating the current data-driven learning process with knowledge-driven \nanalysis processes, evaluating and improving the robustness of deep learning against out-of-\nsample data, as well as explaining other types of popular deep learning models, still pose consid-\nerable challenges for explainable deep learning. \nRESEARCH GAPS AND OPPORTUNITIES \nThe development of a highly accurate and efficient deep learning model is an iterative and pro-\ngressive process of training, evaluation, and refinement, which typically relies on a time-\nconsuming trial-and-error procedure where the parameters and the model structures are adjusted \nbased on user expertise. Typically, machine learning researchers tend to build a new learning \nprocess to trace a model’s prediction [5]. For example, model predictions can be explained by \nhighlighted image regions. Visualization researchers are making initial attempts to visually illus-\ntrate intuitive model behaviors and debug the training processes of widely-used deep learning \nmodels such as CNNs and RNNs [7,11]. However, little effort has been made in tightly integrat-\ning state-of-the-art deep learning models/methods with interactive visualizations to maximize the \nvalue of both. Based on this gap and our understanding of current practices, we identify the fol-\nlowing research opportunities. \nOpportunity 1. Injecting external human knowledge \nCurrently, most deep learning models constitute data-driven methods, whereas knowledge-\ndriven perspectives have received comparatively little attention. In this sense, an open research \nopportunity is to combine human expert knowledge and deep learning techniques through inter-\nactive visualization. To be specific, potential research topics include domain knowledge repre-\nsentation and interpretation, expert knowledge prorogation, and knowledge-based visual \nexplanation. In addition, visual analytics could be utilized to intuitively verify that a model cor-\nrectly follows human-injected knowledge and rules, which is a crucial step in ensuring the proper \nbehavior of critical deep learning applications. For example, when training an autonomous driv-\ning model with video camera inputs, one can impose the rule of never hitting a person recog-\nnized in the scene.  \n \n \n \nSECTION TITLE HERE \nOpportunity 2. Progressive visual analytics of deep learning \nMost of the existing explainable deep learning approaches mainly focus on understanding and \nanalyzing model predictions or the training process offline after the model training is complete. \nAs the training of many deep learning models is time-consuming (requiring from hours to days \nof computation), progressive visual analytics techniques are needed to incorporate experts into \nthe analysis loop. To this end, deep learning models are expected to produce semantically mean-\ningful partial results during the training process. Experts can then leverage interactive visualiza-\ntions to explore these partial results, examine newly incoming results, and perform new rounds \nof exploratory analysis without having to wait for the entire training process to be completed.  \nOpportunity 3. User-driven generative models \nTraditional classification or regression problems associate each data item with a unique answer \nin a simple form, and thus the prediction output of deep learning models has little room for hu-\nman intervention. However, generative models, which perform the generation and translation of, \nsay, images, sentences, and speech signals, can have multiple possible answers, which leaves \nample room for the interactive steering of deep learning outputs.  \nFor instance, users can steer the colorization process of a given gray-scale image [16]. One can \nalso transform a given facial image by interactively changing its attributes such as gender and \nfacial expression [17]. Promising directions would be to develop (1) new deep learning models \nthat can flexibly take various user inputs and reflect them in the generative output and (2) novel \nvisualization-based interfaces that allow users to effectively interact with deep learning.  \nOpportunity 4. Improving the robustness of deep learning for \nsecure artificial intelligence \nDeep learning models are generally vulnerable to adversarial perturbations, where adversarial \nexamples are maliciously generated to mislead the model to output wrong predictions. An adver-\nsarial example is modified very slightly, and thus in many cases these modifications can be so \nsubtle that a human observer cannot even notice the modification at all, yet the model still makes \na mistake. These adversarial examples are often used to attack a deep learning model.  \nIn this respect, maintaining the robustness of a deep learning model is critical in real-world ap-\nplications. Accordingly, one research opportunity concerning explainable deep learning is to \nincorporate human knowledge to improve the robustness of deep learning models.  \nOpportunity 5. Reducing the size of the required training set \nTypically, a deep learning model contains hundreds of parameters. To fully train a learning mod-\nel with such a large number of parameters, thousands of training examples are required. In real-\nworld applications, a method is impractical if each specific task requires its own separate large-\nscale collection of training examples. To close the gap between academic research outputs and \nreal-world requirements, it is necessary to reduce the sizes of required training sets by leveraging \nprior knowledge obtained from previously trained models in similar categories, as well as human \nexpertise.  \nOne-shot learning or zero-shot learning [18], which are major unsolved problems in the current \npractice of training deep learning models, provide a possibility to incorporate prior knowledge \non objects into a “prior” probability density function. That is, those models trained using given \ndata and their labels can usually solve only pre-defined problems for which they were originally \ntrained. For example, a deep learning model that detects a cat cannot in principle detect a tiger \nwithout sufficient training data with tiger labels. The injection of a small amount of user inputs \ncan potentially solve these problems through a visual analytics framework.  \nAs a result, an interesting direction for future research would be to study how to combine visual \nanalytics with learning capabilities such as one-shot learning or zero-shot learning to incorporate \nexternal human knowledge and reduce the number of training samples needed.  \n  \n \n \nMAGAZINE NAME HERE \nOpportunity 6. Visual analytics for advanced deep learning \narchitectures \nSo far, researchers have mostly developed visual analytic approaches for basic deep learning \narchitectures, such as CNNs and RNNs. Attention-based models have exhibited their advantages \nin various applications. Many other advanced architectures have recently been proposed as effec-\ntive alternatives, and some of these are being effectively employed for state-of-the-art perfor-\nmance in existing tasks (ResNet and DenseNet for image recognition), as well as for new \nchallenging tasks (memory networks and co-attention models for natural-language question an-\nswering).  \nIn general, these models are usually not only composed of numerous layers (e.g., several hun-\ndreds or thousands in ResNet and DenseNet), but also involve complicated network designs for \nindividual layers, as well as a heavily connected structure between them. Such complexity of \nadvanced deep learning architectures poses unprecedented interpretation and interaction chal-\nlenges in the visual analytic and information visualization communities.  \nAccordingly, future research should address the aforementioned issues by developing effective \nand efficient visualization techniques and the intuitive summarization of such large-scale net-\nworks in terms of the number of nodes, layers, and their connectivity. Furthermore, novel inter-\naction techniques should be developed to enhance their interpretability and model-steering \ncapabilities.  \nCONCLUSION \nDeep learning and artificial intelligence have had a significant impact on our lives, and as we \nrely heavily on them, explainability and a deep understanding regarding their decisions and in-\nternal processes are becoming crucial. In this paper, we reviewed recent efforts from visual ana-\nlytics, information visualization, and machine learning perspectives in both academia and \nindustry, including basic toolkits, advanced computational techniques, and intuitive and interac-\ntive visual analytics systems.  \nFinally, we discussed the gaps in research, and proposed potential research opportunities such as \nhuman-in-the-loop visual analytics integrating human knowledge and data-driven learning ap-\nproach, progressive visual analytics for deep learning, user-driven generative models, and visual \nanalytics for secure deep learning.  \nWe hope that these proposed directions will inspire new research that can improve the current \nstate of the art in deep learning toward accurate, interpretable, efficient, and secure artificial \nintelligence.  \nACKNOWLEDGMENTS \nWe greatly appreciate the feedback from anonymous reviewers. This work was partially \nsupported by the Basic Science Research Program through the National Research Founda-\ntion of Korea (NRF) grant funded by the Korea government (MSIP) (No. NRF-\n2016R1C1B2015924) and National NSF of China (No. 61672308). Any opinions, findings, \nand conclusions or recommendations expressed here are those of the authors and do not \nnecessarily reflect the views of funding agencies.  \nREFERENCES \n1. \nK. Xu, S. Guo, N. Cao, D. Gotz, A. Xu, H. Qu, Z. Yao, Y. Chen. “ECGLens: Interac-\ntive Visual Exploration of Large Scale ECG Data for Arrhythmia Detection,” Proc. the \nACM CHI Conference on Human Factors in Computing Systems, 2018. \n \n \n \nSECTION TITLE HERE \n2. \nK. Wongsuphasawat, D. Smilkov, J. Wexler, J. Wilson, D. Mane, D. Fritz, D. Krish-\nnan, F. B. Viegas, and M. Wattenberg, “Visualizing Dataflow Graphs of Deep Learn-\ning Models in TensorFlow,” IEEE Trans. Visualization and Computer Graphics, vol. \n24, no. 1, pp. 1–12, 2018.  \n3. \nM. D. Zeiler and R. Fergus, “Visualizing and Understanding Convolutional Net-\nworks,” Proc. the European Conference on Computer Vision, pp. 818–833, 2014.  \n4. \nK. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside Convolutional Networks: \nVisualising Image Classification Models and Saliency Maps,” Proc. the International \nConference on Learning Representations, 2014. \n5. \nM. T. Ribeiro, S. Singh, and C. Guestrin, “\"Why Should I Trust You?\": Explaining the \nPredictions of Any Classifier,” Proc. the ACM SIGKDD Conference on Knowledge \nDiscovery and Data Mining, pp. 1135–1144, 2016.  \n6. \nP. W. Koh and P. Liang, “Understanding Black-Box Predictions via Influence Func-\ntions,” Proc. the International Conference on Machine Learning, pp. 1885–1894, \n2017.  \n7. \nP.E. Rauber, S.G. Fadel, A.X. Falcao, and A.C. Telea, “Visualizing the Hidden Activi-\nty of Artificial Neural Networks,” IEEE Trans. Visualization and Computer Graphics, \nvol. 23, no. 1, pp. 101–110, 2017.  \n8. \nM. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu, “Towards Better Analysis of Deep Con-\nvolutional Neural Networks,” IEEE Trans. Visualization and Computer Graphics, vol. \n23, no. 1, pp. 91–100, 2017.  \n9. \nS Liu, X Wang, M Liu, and J Zhu, “Towards Better Analysis of Machine Learning \nModels: A Visual Analytics Perspective,” Visual Informatics, vol. 1, no 1, pp.48–56, \n2017. \n10. M. Kahng, P. Y. Andrews, A. Kalro, and D. H. Chau, “ActiVis: Visual Exploration of \nIndustry-Scale Deep Neural Network Models,” IEEE Trans. Visualization and Com-\nputer Graphics, vol. 24, no. 1, pp. 88–97, 2018.  \n11. H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush, “LSTMVis: A Tool for Visual \nAnalysis of Hidden State Dynamics in Recurrent Neural Networks,” IEEE Trans. Vis-\nualization and Computer Graphics, vol. 24, no. 1, pp. 667–676, 2018.  \n12. Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu, “Understanding Hid-\nden Memories of Recurrent Neural Networks,” Proc. the IEEE Conference on Visual \nAnalytics Science and Technology, 2017. \n13. S. Chung, S. Suh, C. Park, K. Kang, J. Choo, and B. C. Kwon, “ReVACNN: Real-\nTime Visual Analytics for Convolutional Neural Network.” KDD’16 Workshop on In-\nteractive Data Exploration and Analytics, 2016. \n14. N. Pezzotti, T. Höllt, J. V. Gemert, B. P. F. Lelieveldt, E. Eisemann, A. Vilanova, \n“DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks,” \nIEEE Trans. Visualization and Computer Graphics, vol. 24, no. 1, pp. 98–108, 2018.  \n15. M. Liu, J. Shi, K. Cao, J. Zhu, and S. Liu, “Analyzing the Training Processes of Deep \nGenerative Models,” IEEE Trans. Visualization and Computer Graphics, vol. 24, no. \n1, pp. 77–87, 2018.  \n16. R. Zhang, J. Y. Zhu, P. Isola, P. Geng, A. S. Lin, T. Yu, A. A. Efros, “Real-Time Us-\ner-Guided Image Colorization with Learned Deep Priors,” ACM Transactions on \nGraphics, vol. 36, no. 4, pp. 119:1–119:11, 2017.  \n17. Y. Choi, M. Choi, M. Kim, J. Ha, S. Kim, J. Choo, “StarGAN: Unified Generative \nAdversarial Networks for Multi-Domain Image-to-Image Translation,” ArXiv, 2017.  \n18. F Li, R. Fergus and P. Perona, \"A Bayesian Approach to Unsupervised One-shot \nLearning of Object Categories,\" Proc. the IEEE International Conference on Comput-\ner Vision, pp. 1134–1141, 2003.  \nAUTHORS BIOS \nJaegul Choo is an assistant professor in the Dept. of Computer Science and Engineering at \nKorea University. He has been a research scientist at Georgia Tech from 2011 to 2015, \nwhere he also received an M.S in 2009 and Ph.D in 2013. His research focuses on visual \nanalytics for machine learning and deep learning. He earned the Best Student Paper at \nICDM’16, the Outstanding Research Scientist Award at Georgia Tech in 2015, and the Best \nPoster Award at IEEE VAST (as part of IEEE VIS) in 2014, and he was nominated as one \n  \n \n \nMAGAZINE NAME HERE \nof the four finalists for the IEEE Visualization Pioneers Group dissertation award in 2013. \nHe can be contacted at jchoo@korea.ac.kr. \nShixia Liu Shixia Liu is an associate professor in the School of Software, Tsinghua Univer-\nsity. Her research interests include visual text analytics, visual social analytics, visual model \nanalytics, and text mining. Shixia is the associate editor of IEEE Transactions on Visualiza-\ntion and Computer Graphics and on the editorial board of Information Visualization. She \nwas the papers co-chair of IEEE VAST 2016 and 2017 and was the program co-chair of \nPacifcVis 2014 and VINCI 2012. She can be contacted at shixia@tsinghua.edu.cn.  \nContact department editor Theresa-Marie Rhyne at theresamarierhyne@gmail.com. \n",
  "categories": [
    "cs.HC",
    "cs.LG",
    "stat.ML",
    "I.6.9.c"
  ],
  "published": "2018-04-07",
  "updated": "2018-04-07"
}