{
  "id": "http://arxiv.org/abs/1804.03830v1",
  "title": "Unsupervised Segmentation of 3D Medical Images Based on Clustering and Deep Representation Learning",
  "authors": [
    "Takayasu Moriya",
    "Holger R. Roth",
    "Shota Nakamura",
    "Hirohisa Oda",
    "Kai Nagara",
    "Masahiro Oda",
    "Kensaku Mori"
  ],
  "abstract": "This paper presents a novel unsupervised segmentation method for 3D medical\nimages. Convolutional neural networks (CNNs) have brought significant advances\nin image segmentation. However, most of the recent methods rely on supervised\nlearning, which requires large amounts of manually annotated data. Thus, it is\nchallenging for these methods to cope with the growing amount of medical\nimages. This paper proposes a unified approach to unsupervised deep\nrepresentation learning and clustering for segmentation. Our proposed method\nconsists of two phases. In the first phase, we learn deep feature\nrepresentations of training patches from a target image using joint\nunsupervised learning (JULE) that alternately clusters representations\ngenerated by a CNN and updates the CNN parameters using cluster labels as\nsupervisory signals. We extend JULE to 3D medical images by utilizing 3D\nconvolutions throughout the CNN architecture. In the second phase, we apply\nk-means to the deep representations from the trained CNN and then project\ncluster labels to the target image in order to obtain the fully segmented\nimage. We evaluated our methods on three images of lung cancer specimens\nscanned with micro-computed tomography (micro-CT). The automatic segmentation\nof pathological regions in micro-CT could further contribute to the\npathological examination process. Hence, we aim to automatically divide each\nimage into the regions of invasive carcinoma, noninvasive carcinoma, and normal\ntissue. Our experiments show the potential abilities of unsupervised deep\nrepresentation learning for medical image segmentation.",
  "text": "Unsupervised Segmentation of 3D Medical Images\nBased on Clustering and Deep Representation Learning\nTakayasu Moriyaa, Holger R. Rotha, Shota Nakamurab, Hirohisa Odac, Kai Nagarac,\nMasahiro Odaa, and Kensaku Moria\naGraduate School of Informatics, Nagoya University\nbNagoya University Graduate School of Medicine\ncGraduate School of Information Science, Nagoya University\nABSTRACT\nThis paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural\nnetworks (CNNs) have brought signiﬁcant advances in image segmentation. However, most of the recent methods\nrely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for\nthese methods to cope with the growing amount of medical images. This paper proposes a uniﬁed approach to\nunsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two\nphases. In the ﬁrst phase, we learn deep feature representations of training patches from a target image using\njoint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates\nthe CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by\nutilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep\nrepresentations from the trained CNN and then project cluster labels to the target image in order to obtain\nthe fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with\nmicro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could\nfurther contribute to the pathological examination process. Hence, we aim to automatically divide each image\ninto the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the\npotential abilities of unsupervised deep representation learning for medical image segmentation.\nKeywords: Segmentation, Micro-CT, Representation Learning, Unsupervised Learning, Deep Learning\n1. PURPOSE\nThe purpose of our study is to develop an unsupervised segmentation method of 3D medical images. Most of\nthe recent segmentation methods using convolutional neural networks (CNNs) rely on supervised learning that\nrequires large amounts of manually annotated data.1 Therefore, it is challenging for these methods to cope with\nmedical images due to the diﬃculty of obtaining manual annotations. Thus, research into unsupervised learning,\nespecially for 3D medical images, is very promising. Many previous unsupervised segmentation methods for 3D\nmedical images are based on clustering.2 However, most unsupervised work in medical imaging was limited to\nhand-crafted features that were then used with traditional clustering methods to provide segmentation.\nIn our study, we investigated whether representations learned by unsupervised deep learning aid in the clus-\ntering and segmentation of 3D medical images.\nAs an unsupervised deep representation learning, we adopt\njoint unsupervised learning (JULE)3 based on a framework that progressively clusters images and learns deep\nrepresentations via a CNN. Our main contribution is to combine JULE with k-means4 for medical image seg-\nmentation.\nTo our knowledge, our methods are the ﬁrst to employ JULE for unsupervised medical image\nsegmentation. Moreover, our work is the ﬁrst to conduct automatic segmentation for pathological diagnosis of\nmicro-CT images. This work demonstrates that deep representations can be useful for unsupervised medical\nimage segmentation.\nThere are two reasons why we chose JULE for our proposed method. The ﬁrst reason is that JULE is robust\nagainst data variation (e. g., image type, image size, and sample size) and thus can cope with a dataset composed\nof 3D patches cropped out of medical images. Moreover, the range of intensities is diﬀerent for each medical\nimage. Thus, we need a learning method that works well with various datasets. The second reason is that JULE\narXiv:1804.03830v1  [cs.CV]  11 Apr 2018\ncan learn representations that work well with many clustering algorithms. This advantage allows us to learn\nrepresentations on a subset of possible patches from a target image and then apply a faster clustering algorithm\nto the representations of all patches for segmentation.\n2. METHOD\nThe proposed segmentation method has two phases: (1) learning feature representations using JULE and (2)\nclustering deep representations for segmentation. In phase (1), we conduct JULE in order to learn the repre-\nsentations of image patches randomly extracted from an unlabeled image. For use with 3D medical images, we\nextend JULE to use 3D convolutions. The purpose of this phase is to obtain a trained CNN that can transform\nimage patches to discriminative feature representations. In phase (2), we use k-means to assign labels to learned\nrepresentations generated by the trained CNN.\n2.1 Deep Representation Learning\nThe main idea behind JULE is that meaningful cluster labels could become supervisory signals for representation\nlearning and discriminative representations help to obtain meaningful clusters.\nGiven a set of ns unlabeled\nimage patches I = {I1, . . . , Ins}, cluster labels for all image patches y = {y1, . . . , yns}, and the parameters for\nrepresentations θ, the objective function of JULE is formulated as\n(ˆy, ˆθ) = arg min\ny,θ\nL(y, θ|I)\n(1)\nwhere L is a loss function. JULE tries to ﬁnd optimal ˆy in the forward pass and optimal ˆθ in the backward\npass to minimize L. By iterating the forward pass and the backward pass, we can obtain more discriminative\nrepresentations and therefore better image clusters. In the forward pass, we conduct image clustering to merge\nclusters using agglomerative clustering.5 In the backward pass, we conduct representation learning via a 3D\nCNN using cluster labels as supervisory signals. JULE can be interpreted as a recurrent framework because it\niterates merging clusters and learning representations over multiple timesteps until it obtains the desired number\nof clusters C. Fig. 1 shows an overview of a recurrent process at the time of step t.\n2.2 Extension to 3D Medical Images\nWe conducted two extensions of JULE. One is the extension of the recurrent process for the CNN training in the\nbackward pass. Originally, JULE aims to obtain the ﬁnal clusters and ﬁnishes when it obtains a desired number\nof clusters in the forward pass.3 In contrast, our purpose is to obtain a well-trained CNN. If we terminate the\nrecurrent process in the ﬁnal forward pass, we lose a chance to train the CNN with the ﬁnal cluster labels.\nTherefore, we extended the recurrent process to train the CNN using the ﬁnal cluster label in the backward\npass. The intuitive reason is that the ﬁnal clusters are the most precise of the entire process and representations\nlearned with them become more discriminative.\nThe other is the extension of CNN to support 3D medical\nimages. Originally, JULE is a representation learning and clustering method for 2D images. We, however, aim\nto learn representations using 3D image patches. Thus, we extended the CNN architecture of the original JULE3\nto use 3D convolutions throughout the network.\n2.3 Patch Extraction\nPrior to learning representations, we need to prepare training data composed of small 3D image patches. These\npatches are extracted from the unlabeled image, which is our target for segmentation, by randomly cropping ns\nsub-volumes of w × w × w voxels. In many cases of medical image segmentation, we need to exclude the outside\nof a scanned object from the training data. We choose a certain threshold that can divide the scanned target\nregion from the background and include only patches whose center voxel intensity is within the threshold. After\nextracting training patches, we centralize them by subtracting out the mean of all intensities and dividing by\nthe standard deviation, following Yang et al.3\nFigure 1: Illustration of a recurrent process at the time of step t. First, we extract representations Xt from\ntraining patches I via a CNN with parameter θt. Next, we merge them and assign new labels yt to Xt. Finally,\nwe input I into the CNN again and update the CNN parameters θt to θt+1 through back propagation from a\nloss calculated using yt as supervisory signals. Note that the CNN is initialized with random weights.\nFigure 2: Our CNN architecture has 3 convolutional, 1 max pooling, and 2 fully-connected layers.\nAll 3D\nconvolutional kernels are 5 × 5 × 5 with stride 1. Number of kernels are denoted in each box. Pooling kernels are\n2 × 2 × 2 with stride 2. The ﬁrst fully-connected layer has 1350 neurons, and the second one has 160 neurons.\n2.4 CNN Architecture\nOur CNN consists of three convolutional layers, one max pooling layer, and two fully-connected layers. The\nkernels of the second and third convolutional layers are connected to all kernel maps in the previous layer. The\nneurons in the fully-connected layers are connected to all neurons in the previous layer. The max pooling layer\nfollows the ﬁrst convolutional layers. Batch normalization is applied to the output of each convolutional layer.\nA rectiﬁed linear unit (ReLU) is used as the nonlinearity after batch normalization. The second fully-connected\nlayer is followed by the L2-normalization layer. All of the convolutional layers use 50 kernels of 5 × 5 × 5 voxels\nwith a stride of 1 voxel. The Max pooling layer has a kernel of 2 × 2 × 2 voxels with a stride of 2 voxels. The\ninput to the network are image patches of 27 × 27 × 27 voxels. The ﬁrst fully-connected layer has 1350 neurons\nand the second has 160 neurons. Other parameters for the CNN training, such as learning rate, are the same as\nproposed in the original JULE.3 The CNN architecture is presented in Fig. 2.\nFigure 3: Our segmentation process. We ﬁrst obtain feature representations from a trained CNN and then apply\nconventional k-means to them. Finally, we assign labels to the patches based on the clustering results. (For\nsimpliﬁcation, we have drawn the ﬁgure with a stride equal to w.)\n2.5 Segmentation\nIn the segmentation phase, we ﬁrst extract a possible number of patches of w × w × w voxels from the target\nimage separated by s voxels each. Note that stride s is not larger than w. As with extracting training patches,\nwe select only voxels within the scanned sample by thresholding. The trained CNN transforms each patch into\na feature representation. We then divide the feature representations into K clusters by k-means. After applying\nk-means, each representation is assigned a label l(1 ≤l ≤K) and we need to project these labels onto the\noriginal image. We consider subpatches of s × s × s voxels centered in each extracted patch. Each subpatch is\nassigned the same label as the closest cluster representation using Euclidean distance. This segmentation process\nis illustrated in Fig. 3.\n3. EXPERIMENTS AND RESULTS\n3.1 Datasets\nWe chose three lung cancer specimen images scanned with a micro-CT scanner (inspeXio SMX-90CT Plus,\nShimadzu Corporation, Kyoto, Japan) to evaluate our proposed method. The lung cancer specimens from the\nrespective patients were scanned with similar resolutions. We aimed to divide each image into three histopatho-\nlogical regions: (a) invasive carcinoma; (b) noninvasive carcinoma; and (c) normal tissue. We selected these\nimages because segmenting the regions on micro-CT images based on histopathological features could contribute\nto the pathological examination.6,7 Detailed information for each image is shown in Table 1.\n3.2 Parameter Settings\nFor JULE, we randomly extracted 10,000 patches of 27×27×27 voxels from a target image. We set the number of\nﬁnal clusters C to 100 for lung-A and lung-C, to 10 for lung-B, which are the stopping conditions of agglomerative\nclustering. Other parameters are the same as in the original JULE.3 After representation learning, we extracted\nTable 1: Images used in our experiments\nImage\nImage Size (voxel)\nResolution (µm)\nThreshold (intensity)\nlung-A\n756×520×545\n27.1×27.1×27.1\n4000\nlung-B\n594×602×624\n29.63×29.63×29.63\n2820\nlung-C\n477×454×971\n29.51×29.51×29.51\n4700\n(a) NMI comparison on lung-A\n(b) NMI comparison on lung-B\n(c) NMI comparison on lung-C\n(d) Average NMI comparison\nFigure 4: NMI comparison on three datasets. Our method outperformed traditional unsupervised methods.\npatches of 27 × 27 × 27 voxels with a stride of ﬁve voxels and processed them by the trained CNN to obtain a\n160 dimensional representation for each patch. For segmentation, we applied the conventional k-means to the\nfeature representations, setting K to 3.\n3.3 Evaluations\nWe used normalized mutual information (NMI)8 to measure segmentation accuracy. A larger NMI value means\nmore precise segmentation results. We used seven manually annotated slices for evaluation. We compared the\nproposed method with traditional k-means segmentation and multithreshold Otsu method.9 We also evaluated\nthe average NMI of each method across the datasets. The results are shown in Fig. 4. In each ﬁgure, the best\nperformance NMI for each K is in bold. As shown in all of the ﬁgures, JULE-based segmentation outperformed\ntraditional unsupervised methods. While the NMI scores of our methods are not high, qualitative evaluation\nshows promising results of our proposed method (see Fig. 5). The qualitative examples show that JULE-based\nsegmentation divided normal tissue region from the cancer region, including invasive carcinoma and noninvasive\ncarcinoma, well.\n4. DISCUSSIONS\nQualitative evaluations demonstrate that JULE can learn features that divide higher intensity regions from lower\nintensity regions. This is because, generally, regions of invasive and noninvasive carcinoma have substantially\nhigh intensities, whereas normal tissue regions have low intensities. Moreover, for lung-A and lung-B, JULE\ndivided invasive carcinoma from noninvasive carcinoma. This results shows the potential ability to learn features\nthat reﬂect variation in intensity. This is because, seemingly, invasive carcinoma and normal tissue typically\nhave a small variation of intensities, whereas noninvasive carcinoma has a large variation of intensities.\nFigure 5: Segmentation results of lung-A (top line), lung-B (middle line), and lung-C (bottom line). In the ground\ntruth, the red, green, and blue regions correspond to the region of invasive carcinoma, noninvasive carcinoma,\nand normal tissue, respectively. In the results of the JULE-based segmentation, colors indicate the same cluster,\nbut are assigned at random.\n5. CONCLUSION\nWe proposed an unsupervised segmentation using JULE that alternately learns deep representations and im-\nage clusters. We demonstrated the potential abilities of unsupervised medical image segmentation using deep\nrepresentations. Our segmentation method could be applicable to many other applications in medical imaging.\nACKNOWLEDGMENTS\nThis research was supported by the Kakenhi by MEXT and JSPS (26108006, 17K20099) and the JSPS Bilateral\nInternational Collaboration Grants.\nREFERENCES\n[1] Long, J., Shelhamer, E., and Darrell, T., “Fully convolutional networks for semantic segmentation,” in [IEEE\nCVPR], 3431–3440 (2015).\n[2] Garc´ıa-Lorenzo, D., Francis, S., Narayanan, S., Arnold, D. L., and Collins, D. L., “Review of automatic\nsegmentation methods of multiple sclerosis white matter lesions on conventional magnetic resonance imaging,”\nMedical Image Analysis 17, 1–18 (2013).\n[3] Yang, J., Parikh, D., and Batra, D., “Joint unsupervised learning of deep representations and image clusters,”\nin [IEEE CVPR], 5147–5156 (2016).\n[4] MacQueen, J. et al., “Some methods for classiﬁcation and analysis of multivariate observations,” in [Proceed-\nings of the ﬁfth Berkeley Symposium on Mathematical Statistics and Probability], 1, 281–297 (1967).\n[5] Zhang, W., Wang, X., Zhao, D., and Tang, X., “Graph degree linkage: Agglomerative clustering on a directed\ngraph,” in [ECCV], 7572, 428–441 (2012).\n[6] Mori, K., “From macro-scale to micro-scale computational anatomy: a perspective on the next 20 years,”\nMedical Image Analysis 33, 159–164 (2016).\n[7] Nakamura, S., Mori, K., Okasaka, T., Kawaguchi, K., Fukui, T., Fukumoto, K., and Yokoi, K., “Micro-\ncomputed tomography of the lung: Imaging of alveolar duct and alveolus in human lung,” in [D55. LAB\nMETHODOLOGY AND BIOENGINEERING: JUST DO IT], A7411–A7411, American Thoracic Society\n(2016).\n[8] Strehl, A. and Ghosh, J., “Cluster ensembles—a knowledge reuse framework for combining multiple parti-\ntions,” Journal of machine learning research 3(Dec), 583–617 (2002).\n[9] Otsu, N., “A threshold selection method from gray-level histograms,” IEEE transactions on systems, man,\nand cybernetics 9(1), 62–66 (1979).\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-04-11",
  "updated": "2018-04-11"
}