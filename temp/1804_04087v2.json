{
  "id": "http://arxiv.org/abs/1804.04087v2",
  "title": "Natural Language Statistical Features of LSTM-generated Texts",
  "authors": [
    "Marco Lippi",
    "Marcelo A Montemurro",
    "Mirko Degli Esposti",
    "Giampaolo Cristadoro"
  ],
  "abstract": "Long Short-Term Memory (LSTM) networks have recently shown remarkable\nperformance in several tasks dealing with natural language generation, such as\nimage captioning or poetry composition. Yet, only few works have analyzed text\ngenerated by LSTMs in order to quantitatively evaluate to which extent such\nartificial texts resemble those generated by humans. We compared the\nstatistical structure of LSTM-generated language to that of written natural\nlanguage, and to those produced by Markov models of various orders. In\nparticular, we characterized the statistical structure of language by assessing\nword-frequency statistics, long-range correlations, and entropy measures. Our\nmain finding is that while both LSTM and Markov-generated texts can exhibit\nfeatures similar to real ones in their word-frequency statistics and entropy\nmeasures, LSTM-texts are shown to reproduce long-range correlations at scales\ncomparable to those found in natural language. Moreover, for LSTM networks a\ntemperature-like parameter controlling the generation process shows an optimal\nvalue---for which the produced texts are closest to real language---consistent\nacross all the different statistical features investigated.",
  "text": "Natural Language Statistical Features of\nLSTM-generated Texts\nMarco Lippi∗, Marcelo A Montemurro∗, Mirko Degli Esposti, and Giampaolo Cristadoro\nAbstract—Long Short-Term Memory (LSTM) networks have\nrecently shown remarkable performance in several tasks dealing\nwith natural language generation, such as image captioning\nor poetry composition. Yet, only few works have analyzed\ntext generated by LSTMs in order to quantitatively evaluate\nto which extent such artiﬁcial texts resemble those generated\nby humans. We compared the statistical structure of LSTM-\ngenerated language to that of written natural language, and\nto those produced by Markov models of various orders. In\nparticular, we characterized the statistical structure of language\nby assessing word-frequency statistics, long-range correlations,\nand entropy measures. Our main ﬁnding is that while both LSTM\nand Markov-generated texts can exhibit features similar to real\nones in their word-frequency statistics and entropy measures,\nLSTM-texts are shown to reproduce long-range correlations at\nscales comparable to those found in natural language. Moreover,\nfor LSTM networks a temperature-like parameter controlling\nthe generation process shows an optimal value—for which the\nproduced texts are closest to real language—consistent across the\ndifferent statistical features investigated.\nIndex Terms—Long Short-Term Memory Networks, Natural\nLanguage Generation, Long-Range Correlations, Entropy, Au-\nthorship Attribution.\nI. INTRODUCTION\nBuilding artiﬁcial systems capable of mimicking human\ncreativity has long been one of the aims of Artiﬁcial In-\ntelligence (AI) [1]. In this work, we focus on the problem\nof Natural Language Generation (NLG), which encompasses\nthe capability of machines to synthesize text in a way that\nresembles spoken or written language typically employed by\nhumans [2]. This research ﬁeld has recently known a period\nof great excitement, mostly due to the huge development in\nthe area of deep learning [3], whose methods and algorithms\nhave certainly contributed to move signiﬁcant steps forward.\nDeep learning techniques have in fact produced stunning\nresults in a variety of different research ﬁelds and application\ndomains, and one of the major successes has been that of\ngenerative models [4]. In the area of NLG, many studies\nhave been dedicated to speciﬁc, focused applications, such\nas image and video captioning [5], [6], poem synthesis [7]\nor lyric generation [8]. In all these cases, the considered\ngenerated texts are relatively short (captions, poems, lyrics)\n∗These two authors contributed equally. Marco Lippi is with the Department\nof Sciences and Methods for Engineering, University of Modena and Reggio\nEmilia, email: marco.lippi@unimore.it. Marcelo A Montemurro is with the\nDivision of Neuroscience and Experimental Psychology, University of Manch-\nester, Manchester, UK, email: m.montemurro@manchester.ac.uk. Mirko Degli\nEsposti is with the Department of Computer Science and Engineering, Univer-\nsity of Bologna, email: mirko.degliesposti@unibo.it. Giampaolo Cristadoro is\nwith the Department of Mathematics and Applications, University of Milano\nBicocca, email: giampaolo.cristadoro@unimib.it.\nand correlations between words rarely span across several\nsentences. The scenario totally changes when we consider\nlonger texts, such as novels. Natural language has been widely\nstudied within this context, and notoriously it shows statistical\nproperties in the distribution of terms, as well as long-range\ncorrelations between words [9], [10], [11]. In comparison to\nshort texts such as captions, this is a much more challenging\nsetting to imitate for machines.\nIn this work, we aim to provide an extensive empirical\nevaluation of texts generated with Long-Short Term Memory\n(LSTM) networks, one of the most widely used deep learning\nmodels for NLG. Our goal is to quantitatively assess whether\nLSTM texts do share some similarities with natural language\nthat is commonly produced by humans. To this aim, we trained\nan LSTM network with a corpus that consists of a collection of\nnovels by Charles Dickens. Such network is trained to predict\nthe next character of a given text, and thus it can be employed\nto iteratively generate a document of any desired length. The\nsetting was adopted in several works (e.g., see [12], [13] and\ncitations therein).\nIn our experimental framework we evaluated several dif-\nferent aspects of machine-generated texts, comparing them\nagainst the statistics of real language and Markov-generated\nsamples. First, we analyzed fundamental linguistic properties\ntypically shown by texts, such as Zipf’s [14] and Heaps’ [15]\nlaws for words. Second, we studied whether the generated\ntexts presented long-range correlations, which are commonly\nencountered in human-generated texts, but difﬁcult to repro-\nduce for machines. As a third point, we compared the entropy\nof the generated texts with the one of the original corpus.\nWe then moved our analysis to a higher level, by carrying\nout a preliminary study looking at characteristics dealing with\nthe style and quality of the generated texts: in particular, we\nanalyzed the degree of creativity and plagiarism of the artiﬁcial\ntexts with respect to the dataset on which the LSTM was\ntrained, by looking at longest common subsequences. We also\nassessed whether an authorship attribution algorithm would\ncapture some analogy between the generated text and the\noriginal one, in terms of author’s style.\nSurprisingly, very few studies have been dedicated to a\nthorough analysis and to a quantitative evaluation of the sim-\nilarities between texts created by machines and texts created\nby humans. Karpathy et al. [13] also provide an experimental\nanalysis of LSTM networks trained for character-by-character\ntext generation, but they focused their study on a qualitative\nevaluation of the cell activations within the neural architecture:\nfor example, they looked for open and closed parentheses or\nquotes, that typically span a few tens or hundreds of characters.\nc⃝2019 IEEE. Paper published in IEEE Transactions on Neural Networks and Learning Systems (DOI: 10.1109/TNNLS.2019.2890970). Personal use of this material is permitted.\nPermission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes,\ncreating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\narXiv:1804.04087v2  [cs.CL]  15 Apr 2019\nTheir claim that the LSTM model is capable of capturing long-\nrange dependencies is thus only supported by such qualitative\nevidence, without giving a deep insight in the characteristics\nof the generated documents. Lin and Tegmark [16] compared\nnatural language texts with those generated by Markov models\nand LSTMs, exploiting metrics coming from information the-\nory. Their analysis shows that LSTMs are capable of capturing\ncorrelations that Markov models instead fail to represent, yet\nthe range of correlations they consider is still quite limited\n(up to 1,000 characters). Conversely, Takahashi and Tanaka-\nIshii [17] reported that LSTM language models have limitation\nin reproducing such long-range correlations if measured with\na method based on clustering properties of rare words; note\nhowever that their analysis is still limited to a range of\n∼1,000 words, and the corpus they employ for training is\nmuch smaller than the one used in our experiments. Ghodsi\nand DeNero [18] instead, analyzed some statistical properties\nof text generated by a Recurrent Neural Network Language\nModel (RNNLM), in particular focusing on the length of\nsentences, the vocabulary distribution, and the distribution\nof speciﬁc grammatical elements, such as pronouns. In a\ncomplementary study, Lake and Baroni [19] focused on short-\nscale structures instead of the overall statistical properties of\npseudo-texts. They showed that RNNs are able to exploit\nsystematic compositionality, and thus also to reproduce, for\nexample, abstract grammatical generalizations.\nThe remainder of the paper is organized as follows. Sec-\ntion II will describe the LSTM model used in our experiments.\nSection III will present the statistical methods employed for\nthe quantitative evaluation of the artiﬁcial text properties.\nSection IV will describe the corpora used to train our model,\nand Section V will report and discuss the experimental results.\nSection VI will ﬁnally conclude the paper, also pointing for\nfuture research directions.\nII. LONG SHORT-TERM MEMORY NETWORKS\nLong Short-Term Memory networks (LSTMs) are recurrent\nneural networks (RNNs) that have been ﬁrst developed at the\nend of the 90s, achieving remarkable results in applications\ndealing with input sequences [20]. Such model was speciﬁcally\ndesigned to address the issue of vanishing gradients, that\ngreatly limited the applicability of standard RNNs [21]. Within\nthe “deep learning revolution” that Artiﬁcial Intelligence has\nbeen undergoing in the last decade, LSTMs have regained\npopularity, being now widely used in a huge number of\nresearch and industrial applications, including automatic ma-\nchine translation, speech recognition, text-to-speech generation\n(e.g., see [22] and references therein).\nA. General framework\nRNNs allow to process sequences of arbitrary lengths, by\nexploiting L hidden layers hℓ\nt, with ℓ= {1, . . . , L} whose\ncells are functions not only of the layer input xt, but also of\nthe hidden layer at the previous time step: hℓ\nt = f(xt, hℓ\nt−1).\nRNNs are typically trained with Backpropagation Through\nTime (BPTT) [23], by unfolding the recurrent structure into\nFig. 1. Depiction of LSTM cell. σf, σi, σo are typically the sigmoid function.\nRecurrent connections ht and Ct propagate information through time.\na sort of temporal chain through which the gradient is propa-\ngated, up to a certain number K of time steps. Unfortunately,\nthis method suffers from the well-known problem of vanishing\nor exploding gradients [20], [21], which makes plain RNNs\nscarcely used in practice. LSTMs overcome this issue by\nexploiting a more complex hidden cell, namely a memory cell,\nand non-linear gating units, that control the information ﬂows\ninto and out of the cell.\nBasically, the LSTM cells are capable of maintaining their\nstate over time, of forgetting what they have learned, and also\nof allowing novel information in. An example of such a cell\nis depicted in Figure 1. The model is based on the concept\nof cell status at time t, namely Ct, which depends on three\ngates: an input gate it that can let new information into the cell\nstate, a forget gate ft that can modulate how much information\nis forgotten from the previous state, and an output gate ot\nthat controls how much information is transferred to the upper\nlayers. The following equations describe the behaviour of an\nLSTM layer (we drop the layer index ℓin order to simplify\nthe notation):\nft = σf(Wfxt + Ufht−1 + bf)\n(1)\nit = σi(Wixt + Uiht−1 + bi)\n(2)\not = σo(Woxt + Uoht−1 + bo)\n(3)\nCt = ft ⊙Ct−1 + it ⊙tanh(Wcxt + Ucht−1 + bc)\n(4)\nht = ot ⊙tanh(Ct)\n(5)\nwhere all σf, σi, σo are typically the sigmoid function, ⊙\nindicates the Hadamard (or element-wise) product, and W,\nU, b represent the model parameters that have to be learned.\nAs a form of regularization, dropout is nowadays typically\nemployed in deep neural networks [24]. Dropout simply con-\nsists in randomly dropping a percentage (1−p) of connections\nbetween neurons during training, while multiplying by p every\nweight in the network at testing time. In recurrent architec-\ntures, though, this general framework does not work well in\npractice, but dropout can still be successfully employed, if\napplied only to inter-layer connections and not to recurrent\nones [25]. This is how we employed dropout in our model.\nB. LSTM for text generation\nThe most widely employed LSTM architecture for text\ngeneration is based on character-level sentence modeling.\nBasically, the input of the network consists in M characters,\nthat correspond to a ﬁxed-size portion of text, whereas the\nnumber of output neurons is the total number S of possible\nsymbols in the text, each neuron corresponding to one of such\ncharacters. The output layer consists in a softmax layer, so\nthat each symbol has an associated probability, and all such\nprobabilities sum up to 1. A hard way of generating texts is\nto pick the character with the highest probability as the next\none in the sequence, and to feed it back into the network\ninput. A soft alternative (which is the one used in practice)\nallows to sample the next character in the sequence from the\nprobability distribution of the output cells: in this way the\noutput of the network is not deterministic, given the same\ninitial input sequence. With such an iterated procedure, texts\nof any length can be generated. The hidden states of the cells\nkeep track of the “memory” of the network, so as to exploit\nalso information not directly encoded in the input any more.\nThis generation phase can be controlled by a parameter T,\nusually named temperature. Different temperature values can\nbe tuned in order to obtain a smoother or sharper softmax\ndistribution from which characters are generated: in particular,\nthe ﬁnal softmax layer in the LSTM computes the probability\nof each symbol j as follows:\nPj =\nexp( yj\nT )\nPS\ni=1 exp( yi\nT )\n(6)\nwhere yi are the output values for each symbol that are fed into\nsoftmax. Large T values lead towards a uniform distribution\nof symbol probabilities, whereas when T tends to zero, the\ndistribution is skewed towards the most probable symbol.\nSuch a model is trained in a classic supervised learning\nsetting, where the input training corpus is fed to the network,\nusing as target the true (known) next character, as it appears in\nthe corpus. If the cell states are not reset as subsequent input\nwindows are presented to the network, long-range dependen-\ncies can in principle be captured by the model.\nNote that, in principle, the same task could be modeled at a\nword level, thus training the LSTM network to predict the next\nword in the text, rather than the next character. Although this\nsolution may appear a more appropriate way for modeling the\nproblem, it has two main limitations: (i) the number of possible\noutput classes of the network would become huge, being the\ntotal number of distinct words in the input corpus, leading to a\nmuch more difﬁcult optimization problem, which would likely\nrequire a larger number of training examples; (ii) the set of\npossible output words would be limited to those appearing in\nthe training corpus, which could be enough in many cases, but\nwould limit the creativity of the network.\nIII. METHODS\nWe now present the methods we employ in order to quanti-\ntatively evaluate the characteristics of the artiﬁcially generated\ntexts with respect to the original, human-generated texts.\nA. Zipf’s and Heaps’ laws for words\nNatural languages show remarkable statistical properties in\ntheir word statistics. The two best known examples are Zipf’s\n[14] and Heaps’ [15] laws in language, which refer to universal\nfeatures related to word frequencies.\nZipf’s law states that if the word frequencies of any suf-\nﬁciently long text are arranged in decreasing order, there\nis a power-law relationship between the frequency and the\ncorresponding ranking order of each word. More explicitly, if\nwe denote the rank of a word by r, the Zipf’s law states the\nfollowing relationship between the rank and the frequency of\na word at that rank position f(r), as follows:\nf(r) = Ar−β.\n(7)\nThis relationship is roughly the same for all human lan-\nguages, the exponent β taking values close to 1.\nHeaps’ law states that the number of the different words (i.e\nthe size of the vocabulary) after seeing t consecutive words in\na text, obeys approximately the following relationship:\nn(t) = Btν\n(8)\nwith exponent ν typically taking values smaller than 1 [26].\nB. Long-Range Correlations\nLinguistic laws on the scaling of word frequencies, like\nZipf’s and Heaps’, do not reveal any statistical structure\nthat depends directly on word order. Zipf’s rank-frequency\ndistribution would be unaltered after a random text shufﬂing,\nand similarly for the average behaviour predicted by Heaps’\nlaw. Statistical measures that capture structure at the sequence\nlevel in texts involve correlations and spectral analysis [27].\nCorrelations in language are known to be of the power-law\ntype [9], [10], [11], decaying as C(τ) ∝τ −γ, where τ is\nthe distance between symbols – e.g. words or characters. It is\npossible to characterize the structure of long-range correlations\nusing the method of Detrended Fluctuation Analysis (DFA)\n[28], [29]. The ﬁrst step in the method involves the mapping\nof the symbol sequence onto a numerical time-series, by\nassigning a number to each basic symbol in the sequence.\nIn order to preserve the maximum of structure from the\ntext sources, we performed the mapping at character level—\nincluding all punctuation signs, numbers, capital letters, and\naccented forms. The procedure to assign a number to each\ncharacter followed similar lines to the method employed in\n[10], where in the present case each character is replaced by\nits rank. Thus, the most frequent character is assigned the\nnumber 1, the second most frequent the number 2, and so on.\nFor a sequence of length N, the character at position t in the\ntime series, with t ∈N, can be represented by the number\nx(t), and the following random-walk can be constructed:\nX(t) =\nt\nX\ni=1\n(x(i) −⟨x⟩)\n(9)\nwhere ⟨x⟩represents the mean of the times series. The time\nseries X(t) is then split into windows of length L, and in each\nof those windows the corresponding stretch of the series is\nﬁtted by a straight line by means of least squares. These linear\nﬁts represent the local trend within each of the windows of\nlength L. The sequence of length-L trends can be concatenated\nin a piecewise manner deﬁning a piecewise linear function\nYL(t). Then we compute the average ﬂuctuations at scale L,\nthat is the deviations from the trend, deﬁned as follows:\nF(L) =\n \n1\nN\nN\nX\nt=1\n(X(t) −YL(t))2\n! 1\n2\n.\n(10)\nThe nature of the correlations present in the original time\nseries can be evaluated by observing the dependence of F(L)\non L. In particular, the growth of ﬂuctuation with the scale L\nwill be given as F(L) ∼Lα, with 0 < α < 1. In the case\nof an uncorrelated or short-range correlated time series xt, we\nhave F(L) ∼L\n1\n2 . However, in the presence of long-range\ncorrelations of the power-law type in the original time series\nxt, the ﬂuctuation exponent α will differ from 1/2 [29], with\nα > 1/2 for persistent (positive) correlations and α < 1/2 for\nanti-persistent (negative) correlations.\nC. Entropy and KL-divergence estimation\nThe entropy of a symbolic sequence can be interpreted as a\nquantiﬁcation of the degree of predictability in the sequence. A\nhigh level of predictability of consecutive values in a sequence\nimplies a low level of surprise about future symbols, which is\nlinked to a low entropy. On the other hand, a sequence with\na high degree of randomness will be characterized by a high\nlevel of surprise in the identity of future symbols, and result\nin a high value for the entropy. Therefore, the determination\nof the entropy of language serves as a quantiﬁcation of its\ndegree of order. Early attempts to determine the entropy of\nlanguage were based on the close link between entropy and\npredictability [30]. However, the estimation of the entropy\nfrom long sequences of written text, requires the estimation\nof block probabilities, which poses a serious computational\nchallenge, due to the presence of long-range correlations in\nlanguage. The required sample size needed for an accurate\nestimation of the block probabilities grows exponentially with\nthe length of the block, thus quickly rendering insufﬁcient\nany available amount of text. This difﬁculty can be overcome\nthrough the link between entropy and predictability mentioned\nabove. The degree of predictability in a sequence determines\nhow much it can be compressed by a lossless compression\nmethod. Sequences with high predictability can be compressed\nmore than sequences with a higher degree of randomness. In\nparticular, it can be shown that under general assumptions of\nstationarity and ergodicity, the entropy rate of a stochastic\nsource is a lower bound to the length per symbol of any\nencoding of it [31]. Hence, the entropy of symbolic sequences\ncan be estimated by means of efﬁcient lossless compression\nalgorithms [32], [33], [34]. We estimated the entropy of long\ncharacter sequences using an implementation of the algorithm\nproposed by Lempel and Ziv [32], [35], [36], which relies on\nthe estimation of redundancy by looking for matches between\nfuture and past substrings in a symbolic sequence. Imple-\nmentations of entropy estimation algorithms based on these\nmethods have proved to work well for symbolic sequences\neven in the presence of long range correlations as those found\nin language [34], [37].\nThe Kullback-Leibler (KL) divergence D(P|Q) is a mea-\nsure of relative entropy between two probability distributions\nP and Q [31]. When P ≡Q then the KL-divergence is\nzero, but it takes positive values when P ̸= Q. It can be\nshown that the D(P|Q) is a measure of the extra numbers\nof bits that are required to encode typical sequences with the\ndistribution P, when using a code based on Q [31]. This inter-\npretation suggests that D(P|Q) can also be estimated using\ncompression algorithms in which one signal is compressed\nusing past sequences in the second signal. More speciﬁcally,\nthe KL divergence D(P|Q) can be written as [31]\nD(P|Q) = HP (Q) −H(P)\n(11)\nwhere H(P) is the entropy of the distribution P and HP (Q)\nis the cross-entropy between P and Q. Let us assume that\ntwo text sequences produced by the stochastic source with\nprobability distribution P are represented by X = {xt}N\nt=1,\nand those produced by Q as Z = {zt}N\nt=1. Then, for notational\nsuccinctness let us write the information quantities explicitly\nin terms of the generated sequences, therefore representing the\nKL-divergence between P and Q as D(X|Z). With this no-\ntation, we have D(X|Z) = HX(Z) −H(X). A compression-\nbased algorithm proposed in [38] permits to compute the cross-\nentropy HX(Z) based on the symbolic sequences X and Z.\nThen, the KL-divergence is obtained by subtracting the entropy\nof the sequence X from the cross-entropy HX(Y ). The KL\ndivergence is a non-symmetric quantity and in order to have\na distance-like measure between character sequences X and\nY , we deﬁned a symmetrized divergence as Ds(X, Y ) =\n(D(X|Y ) + D(Y |X))/2.\nAnother measure that is strictly related to entropy, and\nthat is widely used for the evaluation of artiﬁcial texts, is\nperplexity [39], which can be computed as the geometric\nmean of the inverse probability for each predicted word in\nthe a document [40], [41], where probabilities are typically\nestimated on a language model trained on a larger corpus.\nD. Creativity and Authorship Attribution\nProviding a quantitative method able to address the cre-\nativity of a given algorithm for artiﬁcial texts generation is\na complex task. In this paper we consider two distinct yet\nstrictly intertwined aspects: we aim to measure at what extent\nthe algorithm is capable of capturing the stylistic traits of a\ngiven author, while, at the same time, avoiding to perform just\na plagiarism of the training corpus.\nMeasuring the Longest Common Subsequence (LCS) is one\nof the simplest way to implement the idea of quantitatively\nmeasuring plagiarism: given the k-th character xk of the\nartiﬁcial text, we denote by Lk the length of the longest\nsubsequence starting at xk that is also contained in (thus,\nplagiarized from) the training corpus. Different statistics of the\nset of all Lk can be used to quantify how various algorithms\nare able to reproduce some stylist traits of a given corpus while\ngenerating innovative texts, not written before. Here we adopt\nthe simplest one and consider the maximum over the whole\nartiﬁcial text: ¯L = maxk Lk (see [42] and [43]).\nWe now move our analysis to a higher level, by exploring\nhow artiﬁcial texts resemble the style of the training author.\nStylistic traits are supposed to reﬂect subtle choices of the\nauthor in terms of vocabulary, syntactic constructions and\nstructural composition, to mention a few. As such, a compre-\nhensive quantiﬁcation of the style of an author is out of reach.\nOn the other hand, a very simple feature such as the frequency\ndistribution of n-gram of letters has been successfully selected\nas a key ingredient in some of the most effective approaches\nto authorship attribution [44], [45].\nWe use one of the state-of-the-art algorithms to test the\nautomatic attribution of the author of our artiﬁcial texts. The\nimplemented method is in fact one of the two methods that\nhave been succesfully used for the attribution of Antonio\nGramsci’s papers [44]. Essentially, each method deﬁnes a\nkind of similarity distance between texts. Let us very brieﬂy\ndescribe just the ﬁrst method used here, referring to [45] for\nfurther details. The method is based on (characters) n-grams\nand it is probably one of the simplest possible measures on\na text: after a ﬁrst experiment based on bigram frequencies\npresented in 1976 by Bennett [46], Ke˘selj et al [47] published\nin 2003 a paper in which n-gram frequencies were used to\ndeﬁne a similarity distance between texts (see also [48]). The\nsimilarity distance was introduced and discussed in [44]: we\ncall ω an arbitrary n-gram, and we denote by fX(ω) and\nfY (ω) the relative frequencies with which ω occurs in text\nX and Y . Dn(X) is the n-gram dictionary of text X, that is,\nthe set of all n-grams which have non-zero frequency in X\n(similarly for Y ) and we deﬁne what we will call the n-gram\ndistance between text X and text Y as1:\ndn(X, Y ) :=\n1\n|Z|\nX\nω∈Dn(X)∪Dn(Y )\n\u0012fX(ω) −fY (ω)\nfX(ω) + fY (ω)\n\u00132\n(12)\nwhere the denominator |Z| = |Dn(X)| + |Dn(Y )| is the sum\nof the number of different n-grams in the two dictionaries,\nrespectively, while the inner sum is taken over all the different\nn-grams occurring in the two texts.\nSuppose the goal is to decide whether a given document X\nhas been authored by author A or not. The approach adopted\nin [44] consists in ﬁrst collecting m documents from author\nA and m documents from an author B (or, in general, from\nmore authors). The distance of the candidate text X to these\ndocuments is then used, with the help of a simple probabilistic\nmethod, to produce a similarity index I(X), −1 ≤I(X) ≤1\n(see [44] for details on the method). Values of the index close\nto 1 (respectively, −1) reveal a very strong attribution to author\nA (respectively, B), while values close to 0 indicate a very\nweak attribution (see [44], [45] for more details).\nIV. CORPORA\nOur aim was to train an LSTM network on a large corpus\nobtained from a single author, in order to perform also the\n1To be more precise, dn is a pseudo-distance, since it does not satisfy the\ntriangle inequality and it is not even positive deﬁnite: two texts X, Y can be\nat distance dn(X, Y ) = 0 without being the same.\nexperiments on authorship attribution and to assess whether\nthe model was capable of capturing some stylistic features of\nits “teacher”. We employed the works by Charles Dickens,\nsince he was a very proliﬁc author whose bibliography is\nfreely available in plain text format at ProjectGutenberg.2 We\ncollected a total of eighteen works by Charles Dickens, which\nresulted in a corpus of over 24 millions characters.3\nFor the authorship attribution experiments, we also col-\nlected a smaller corpus of texts, some still from Charles\nDickens, and some others from a different author. Clearly,\nthese additional texts from Dickens needed to be disjoint\nfrom those in the larger corpus, on which the LSTM network\nhad to be trained.4 Regarding non-Dickens documents, we\ncollected texts by Robert Louis Stevenson, who was also a\nproliﬁc author of the XIX century.5 For this second corpus, we\ncollected 30 documents both for Dickens and for Stevenson,\neach consisting of 10,000 characters.\nV. EXPERIMENTS\nThe\nexperiments\nwith\nLSTMs\nwere\nrun\nusing\nthe\ntorch-rnn package.6 We trained an LSTM network with\ntwo layers and 1,024 cells in each layer. As customary in\ntext generation experiments with LSTMs [13], the training set\nwas split into chunks of length equal to K characters: in this\nway, gradients in truncated backpropagation are propagated\nup to K steps back, but the status of each LSTM cell is\nnot reset after each example, so that longer-range correlations\ncan in principle be learnt. We ﬁrstly present results obtained\nusing K = 100, later investigating the impact of such hyper-\nparameter on the considered evaluation metrics. To avoid\noverﬁtting, a dropout equal to 0.7 was applied, and a validation\nset (4% of the whole corpus) was exploited to monitor the loss\nfunction during training.\nTo compare the statistics of the LSTM texts with that of\nother non-trivial models, we used a plain m-order Markov\nprocess as a baseline. A family of m-order Markov models\nwere trained from our corpus, with m taking values 2, 4, 6, 8,\n10, and 12. The models were used to generate artiﬁcial texts\nstarting from a seed taken from the original corpus.\nTables I and II show some examples of texts generated\nwith different temperatures from the LSTM, and by different\nMarkov models, respectively. The whole set of generated\nsamples is publicly available at http://agentgroup.unimore.it/\nLippi/generated samples Dickens.zip.\n2http://gutenberg.org\n3We used the following works: A tale of two cities, David Copperﬁeld,\nOliver Twist, Bleak house, Great expectations, The life and adventures of\nNicholas Nickleby, The old curiosity shop, The Pickwick papers, Dombey\nand son, Little Dorrit, Life and adventures of Martin Chuzzlewit, Our mutual\nfriend, Barnaby Rudge, A Christmas carol, The uncommercial traveller, Hard\ntimes, Letters, A child’s history of England.\n4We used excerpts from the following works: Signal-man, A Christmas tree,\nThe poor relation’s story, The schoolboy’s story, Hunted down, Pictures from\nItaly, The chimes, The haunted man and the ghost’s bargain, Tom Tiddler’s\nground, The wreck of the Golden Mary.\n5We used excerpts from the following works: Treasure island, The strange\ncase of Doctor Jekyll and Mister Hyde, Kidnapped, The Black Arrow.\n6https://github.com/jcjohnson/torch-rnn\nTABLE I\nSOME SAMPLES GENERATED BY LSTM, AS A FUNCTION OF THE TEMPERATURE T .\nT\nLSTM generated text\n0.1\nI had no doubt that I had no doubt I had no sooner said that I had no sooner said\nthat I had no sooner said that I was a stranger to me to see him\n0.3\nI will not see him as I have been the family of my heart and expense,\nand I should have seen him so soon.\n0.5\n’I am sure I think it is,’ said the doctor, looking at him at his heart on the window,\nand set him there for a short time, ’that I shall ﬁnd the girl there.\n0.6\nThe old man entered the other side, and then ascended the key on his shoulder.\n’I think I have no doubt, sir,’ replied the woman.\n0.7\n’I can refer to the world,’ said Mr. Tupman, suspiciously,\n’that the same brother was on the provoked passage.’\n0.8\nYou look at me, you know I dont think it should be what I have understood.\nI know what she has of no conﬁdence, and I have ﬁrst cheered you.\n0.9\nAnd so the position and paper escaped you by the Major, with the neck,\nby my own evening, that there was a shadow of it.\n1.0\nThis interference of point that was always large in the evening, which was,\nand used to save the crooked way, and could leave the undertakers upon it.\n1.2\nSoftly. They rose and who faithfully as stalled off, and his distrust\nin the tip of which power it was.\n1.5\nAs if us, she Immoodnished Mrs Jipe Town horsemaking,\nlike the nights foldans with mid-yoUge false. Half-up?\n2.0\nConnursing, visibly; brassbling, on what cohn; orPertixwerkliss issuin’p).\nhaf-pihy; and he-carse masls anycori; nod me: full, nor cur your two yellmoteg’\n3.0\n’Siceday; Quaky ok, ,-GNIRRZVRIIMoklheHw, eab-mo,’\nventvedes.r.’\nEgg iglazzro!wM. ’Sav nam. Ebb- Edjaevi.”\nTABLE II\nSOME SAMPLES GENERATED WITH MARKOV MODELS OF INCREASING ORDER.\norder\nMarkov generated text\n2\n’And Mr Butentime ther foreweemair Masperf torto lit, ’It make to to yesee!”\nshis thed to goin blie, thave com of a dess at’s mand havestroult frot own: ady. Saint\n4\nBut the Nation, and you in looking at all,’ said Mrs. Kenwigs’s hospite as I have gover with now,\n’and the eyes. Perhaps, and even to help, I am sure loving her,\n6\nSam eyed Oliver, with some to her motion of the ladies, which at no immense\nmob gathering which the after all the adorned to nod it, Sir!’ said Fledgeby\n8\nThe foot of the medical manner, ”Jenny saw, asserting to the large majority,\njust outside as he had hardly achieved to be induced the room.\n10\nI endeavour to get so precious contents, handed downstairs with pleasure the great Constitutions.\nIt happened to him like a man, and the fallacy of human being accepted lover of the tide\n12\nMr Witherden the notary, too, regarded him; with what seemed to bear reference to the friar,\ntaking this, as it served to divert his attention was diverted by any artiﬁce.\nA. Zipf’s and Heaps’ Laws\nAs a ﬁrst test comparing the statistics of the LSTM-\ngenerated texts to other models capable of rendering stochastic\nversions of texts, we looked and the distribution of words\nfrequencies. We ﬁrst evaluated the Zipf’s law, by measuring\nthe relationship between the rank in the set of words, ordered\nby frequency, and word frequency itself. Figure 2A shows\nthe rank-frequency distributions of words in the LSTM texts\nfor temperatures in the range 0.1-3.0. The plot also shows in\nblack the result for the original Dickens corpus. We ﬁtted the\nvalue of the exponent within the region between ranks 102 and\n103 to determine more clearly the behavior with temperature,\nwhich showed a clear power-law behaviour consistently across\nall but the two extreme temperatures. Figure 2B shows the\nresulting value of the Zipf’s exponent β as a function of tem-\nperature in the range 0.3-2.0. LSTM texts generated with low\ntemperatures have a frequency rank distribution which decays\nfaster with rank. On the contrary, for higher temperatures the\ndistribution ﬂattens, showing a smaller exponent β. The dashed\nline in the ﬁgure shows the value of the exponent estimated\nfrom the Dickens’ corpus, which intercepts the LSTM results\nat a temperature between 0.8 and 0.9, approximately.\nA similar analysis was performed on the Markov-generated\ntexts. Figure 2C shows the rank-frequency distribution for texts\ngenerated with Markov models. Interestingly, there is little\nvariation of the distribution as a function of Markov order.\nThis is also corroborated by the estimation of the exponent\n(Figure 2D), which shows the exponent of Markov texts with\na value slightly below that of the original source.\nIn order to assess the validity of Heaps’ law for the artiﬁcial\nlanguage, we computed the statistics of the vocabulary growth\nfor the LSTM and Markov texts. Figure 3A shows the results\nfor the LSTM texts for a range of temperatures from 0.1 to\n3.0. For comparison, the curve for the original text is shown\nin black. Figure 2B represents the results of ﬁtting the power-\nlaw behavior in the region for t > 1000, using the same range\nof temperatures shown for the β exponent in Figure 2B. The\ndashed line, which represents results for the original text, cuts\nthe LSTM data at a point between temperatures 0.8 and 0.9.\nA)\n100\n101\n102\n103\n104\n105\nr\n100\n101\n102\n103\n104\nf(r)\nT=0.1\nT=0.3\nT=0.4\nT=0.5\nT=0.6\nT=0.7\nT=0.8\nT=0.9\nT=1.0\nT=1.2\nT=1.5\nT=2.0\nT=3.0\nOriginal\nB)\n0.0\n0.5\n1.0\n1.5\n2.0\nT\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nLSTM\nOriginal\nC)\n100\n101\n102\n103\n104\nr\n100\n101\n102\n103\n104\nf(r)\nm=2\nm=4\nm=6\nm=8\nm=10\nm=12\nOriginal\nD)\n2\n4\n6\n8\n10\n12\nm\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nMarkov\nOriginal\nFig. 2. Zipf’s law. A-C) Zipf’s rank-frequency distribution in LSTM (respectively, Markov) texts, with temperature T in the range 0.1-3.0 (respectively, order\nm in the range 2-12). B-D) Exponent β measured in the region between ranks 102 and 103, as a function of T (LSTM) or m (Markov), with dashed line\ncorresponding to the exponent measured in the original corpus. For readability, panels A-C do not show markers for every point. Figure best seen in colors.\nB. Long-Range Correlations\nBeyond the statistics of word frequencies, natural texts show\ncorrelations that span hundreds or thousands of characters,\nshowing power-law decay. While a direct measure of these\ncorrelations is possible in principle, more efﬁcient methods\nare based on spectral [27] or ﬂuctuation [28], [29] properties\nof the sequences. In particular, we tested the LSTM generated\ntexts using Detrended Fluctuation Analysis (DFA) [28], [29]\napplied to linguistic character sequences. By means of DFA it\nis possible to estimate such exponent α, by ﬁtting a power law\nto the the dependence of the ﬂuctuations F(L) with the scale\nL. We estimated the dependence of F(L) with the scale L for\nLSTM texts generated at different temperatures and for the\nMarkov-generated texts for different orders. Figure 4A shows\nthe normalised ﬂuctuations Fn(L) as a function of the scale\nL for LSTM-generated texts together with the results for the\noriginal Dickens’ corpus and a shufﬂed realisation of it. In all\ncases there are signs of power-law behavior for a wide range\nof scales, with the exception of the two extreme temperatures\n(0.1 and 3.0). Although the data corresponding to the shufﬂed\ntext seems to have a less steep slope than the LSTM samples, a\nmore quantitative analysis is required to compare the extent of\ncorrelations. Figure 4B shows the estimations of the exponent\nα obtained by ﬁtting a power-law to the data in panel A in the\nregion spanned by scales L = 102 . . . 104. For comparison,\nthe full black line corresponds to the result for the original\nDickens’ text while the dashed line is the result obtained from\nthe shufﬂed text. The full black circles are the average of\nthe estimation of the exponent from the 10 available samples,\nwhereas small squares show the result for each sample. It is\nin the region between of temperatures between 0.5 and 1.0\nthat the exponent α es closet to the value for the original\ntext. Yet, while the dispersion of individual sample measures\nare very broadly spread for temperatures close to 0.5 they\nare tightly clustered around the mean for T = 1.0. A similar\nanalysis was done on the Markov-generated texts. Figure 4C\nshows the result of the DFA for Markov texts of orders 2–12,\nand a comparison to the original and shufﬂed texts: clearly,\nall the Markov-generated texts have a structural correlation\nthat resembles more the shufﬂed text than the original one.\nThis trend is corroborated in Figure 4D, showing the estimated\nexponent α as a function of Markov order. In all cases, such\nvalue is very close to that obtained by the shufﬂed text.\nA)\n100\n101\n102\n103\n104\n105\nt\n100\n101\n102\n103\n104\n105\nn\nT=0.1\nT=0.3\nT=0.4\nT=0.5\nT=0.6\nT=0.7\nT=0.8\nT=0.9\nT=1.0\nT=1.2\nT=1.5\nT=2.0\nT=3.0\nOriginal\nB)\n0.0\n0.5\n1.0\n1.5\n2.0\nT\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nLSTM\nOriginal\nC)\n100\n101\n102\n103\n104\n105\nt\n100\n101\n102\n103\n104\nn\nm=2\nm=4\nm=6\nm=8\nm=10\nm=12\nOriginal\nD)\n2\n4\n6\n8\n10\n12\nm\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nMarkov\nOriginal\nFig. 3. Heaps’ law. A-C) Heaps’ growth curve for the LSTM (respectively, Markov) texts, for temperatures in the range 0.1-3.0 (respectively, order in range\n2-12). B-D) Exponent ν measured in the t > 1000 region of the Heaps’s curve for LSTM (respectively, Markov) texts as a function of the temperature\n(respectively, order). In panels B and D the dashed line corresponds to the exponent for the Dickens’ corpus. Figure best seen in colors.\nC. Entropy\nOur ﬁnal test to probe the statistical structure of the LSTM\ntexts consisted in the estimation of entropy measures. The ﬁrst\nestimation corresponded to a symmetrized KL-divergence (see\nSection III) between the LSTM for different temperatures and\noriginal text, estimated by means of compression algorithms\nthat are sensitive to the long-range structure of the signal.\nFigure 5A shows the divergence as a function of T. There\nis a well-deﬁned minimum at T ∼1, indicating that at that\ntemperature the structure of the LSTM text is closest to the\noriginal. Figure 5C shows a similar plot for Markov texts,\nwhich also approximate the structure of the original texts\nasymptotically for larger orders. While the previous analyses\nshowed the behavior of a distance-like quantity, the entropy is\na more direct estimation of the statistical structure of a signal.\nPanels B and D in Figure 5 show the value of the entropy for\nthe LSTM and Markov texts as a function of the temperature\nand order respectively. In both cases the solid line corresponds\nto the value of the entropy of the original text. For LSTM,\nlower T values produce texts with small entropy, thus easier\nto compress, since they show repetitive patterns. As T grows,\nentropy also grows, with the texts becoming the most similar\nto the original around T ∼0.9 (in line with our analyses) and\nfar more disordered for larger T values. Markov texts, instead,\nshow a monotonic approximation to the entropy of the original\ntext, slightly above even for higher orders.\nTo complete the analysis, we also computed the perplexity\nof LSTM-generated texts, using the KenLM library7. To do\nthis, we learned a bigram language model on the original\nDickens corpus and then we computed the perplexity both for\nthe artiﬁcial texts, and for the off-sample Dickens documents\nin the test corpus used for authorship attribution. Results are\nperfectly in line with those obtained with entropy computation\nvia compression, with values of temperature around 0.9-1.0\nproducing texts that are the most similar to the original.\nD. Creativity and Authorship Attribution\nTo assess the degree of plagiarism (and, thus, of creativ-\nity) of our models, we measured the length of the Longest\nCommon Subsequence (LCS) between the artiﬁcial texts and\nthe training corpus. As for Markov texts, not surprisingly the\n7https://github.com/kpu/kenlm\nA)\n101\n102\n103\n104\n105\nL [chars]\n100\n101\n102\n103\nFn(L)\nT=0.1\nT=0.3\nT=0.4\nT=0.5\nT=0.6\nT=0.7\nT=0.8\nT=0.9\nT=1.0\nT=1.2\nT=1.5\nT=2.0\nT=3.0\nOriginal\nShuffled\nB)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nT\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nLSTM\nOriginal\nShuffled\nC)\n101\n102\n103\n104\n105\nL [chars]\n100\n101\n102\nFn(L)\nm=2\nm=4\nm=6\nm=8\nm=10\nm=12\nOriginal\nShuffled\nD)\n2\n4\n6\n8\n10\n12\nm\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nMarkov\nOriginal\nShuffled\nFig. 4. Long-range correlations. A-C) Normalised ﬂuctuations Fn(L) as a function of the scale L for LSTM (respectively, Markov) texts. B-D) Fluctuation\nexponent α obtained by ﬁtting a power law the LSTM and Markov data in panels A and C, in the range L = [102, 104]. Black circles represent the mean\nover 10 samples, while small empty squares are the estimations for individual samples. The full (dashed) black line represents the real (shufﬂed) text.\nlength of the LCS rapidly grows with the order of the model,\nfrom a value of 19 for order 2, up to 73 and 125 for order 10\nand 12, respectively. For LSTMs, the length of the LCS instead\nresults to be lower, with values comprised between 40 and 60\nwhen T is in the range between 0.4 and 1.2. For smaller and\nlarger temperatures, the LCS is clearly shorter, as the generated\ntexts have a less realistic structure, thus it is less probable to\nencounter patterns identical to the original. As a comparison\nwith what we could call a sort of self-plagiarism, encountered\nin a subset of the training corpus, we also computed the length\nof the LCS of the novel “Oliver Twist” with respect to ﬁve\nother novels by Dickens (David Copperﬁeld, A Tale of Two\nCities, Bleak House, Great Expectations, Hard Times): the\nLCS in this case resulted to be 45 characters long.\nFor the experiments on authorship attribution, we used\nthe Dickens and Stevenson corpora described in Section IV.\nFor a ﬁxed temperature of the generated text, attribution is\nperformed by averaging over 10 samples of artiﬁcial texts,\nusing n-grams from n = 1 up to n = 12. To assess the validity\nof the attribution algorithm, we compare the attribution of the\ngenerated texts with that of real texts. In particular, for each\nof the 30 Dickens and Stevenson documents, we employ the\nremaining 29 documents of each group to perform attribution\nto either author. Figure 6 shows the value of the index I(X)\ndeﬁned in Section III-D as a function of n-grams (on the x-\naxis) and either temperature for LSTM (panel A) or order for\nMarkov texts (panel B), respectively. The attribution of real\ntexts performs extremely well, with a maximum for n = 6\nand n = 10 for Dickens and Stevenson, respectively. For\nLSTM, while the texts are indeed always correctly attributed\nto Dickens, it is interesting to notice how the best results\nare again achieved for values of T around 0.8-1.0, while for\nlower temperatures the attribution with larger n-grams drops,\nas the generated texts tend to reproduce periodic (hence, less\nrealistic) patterns. Not surprisingly, Markov texts with m ≥6\nare also well attributed to Dickens, due to the high degree of\nplagiarism described in Section V-D.\nE. Impact of K hyper-parameter\nThe results presented so far consistently show that there is\na narrow range of temperatures around T = 1 for which the\ntexts generated by LSTM are the most similar to the original,\nin terms of all the considered metrics. We ﬁnally investigated\nA)\n0\n1\n2\n3\nT\n0.1\n0.2\n0.5\n1\n2\nsymmetric KL divergence [bits]\nB)\n0\n1\n2\n3\nT\n1\n2\n3\n4\nentropy [bits]\nLSTM\nOriginal\nC)\n2\n4\n6\n8\n10\n12\nm\n0.1\n0.2\n0.5\n1\n2\nsymmetric KL divergence [bits]\nD)\n2\n4\n6\n8\n10\n12\nm\n0\n1\n2\n3\n4\nentropy [bits]\nMarkov\nOriginal\nFig. 5. Divergence and entropy. Symmetrised KL-divergence and entropy, as a function of temperature for the LSTM texts (panels A-B), and as a function\nof the order for the Markov-generated texts (panels C-D). In panels B and D the solid line represents the entropy of the original text.\nA)\n2\n4\n6\n8\n10\n12\nn\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nI(n)\nT=0.3\nT=0.4\nT=0.5\nT=0.6\nT=0.7\nT=0.8\nT=0.9\nT=1.0\nDickens\nStevenson\nB)\n2\n4\n6\n8\n10\n12\nn\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\nI(n)\norder=2\norder=4\norder=6\norder=8\norder=10\norder=12\nDickens\nStevenson\nFig. 6. Authorship attribution for LSTM (A) and Markov texts (B). We show the attribution index I(X) for a text X as a function of n-grams and as a\nfunction of T (for LSTM) and m (for Markov models). Indices closer to 1 (respectively, -1) indicate stronger attributions to Dickens (respectively, Stevenson).\nthe impact of K hyper-parameter on the generated texts.\nGiven that K is the number of backward gradient propagation\nsteps in the BPTT algorithm, it directly affects the way in\nwhich long-range information is propagated through the cells.\nTherefore, we considered two additional LSTM networks,\ntrained with K = 10 and K = 1, 000, respectively, and\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nT\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nK=10\nK=100\nK=1000\nOriginal\nShuffled\nFig. 7. Exponent of DFA as a function of T, for different values of K.\nidentical to the ﬁrst network for all the other settings. For\nall the considered metrics apart from long-range correlations,\nresults are not signiﬁcantly different from those obtained\nfor the network trained with K = 100 (see supplementary\nmaterial for details). Long-range correlations are instead more\nsigniﬁcantly affected by the variation in K. From Figure 7\nwe observe two different phenomena: (i) a peak for low-level\ntemperatures, likely due to spurious, periodic patterns, that\nis visible also for K = 100 (also notice the large variance\nacross samples shown in Figure 4); (ii) a dramatic drop in the\nexponent α for K = 10 starting from T = 0.7, whereas for\nK = 1, 000 the exponent is much closer to that of real text.\nVI. CONCLUSIONS\nIn this paper, we investigated at what degree texts generated\nby an LSTM network resemble texts generated by humans. To\nthis aim, we presented an extensive experimental evaluation\nwhere we compared several characteristics of artiﬁcial and\noriginal texts, starting from statistical properties typically\nshown by natural language, such as the distribution of word\nfrequencies and long-range correlations, up to higher-level\nanalyses, such as the attribution of authorship. Our study\nshows that LSTM-generated texts share key statistical features\nwith natural language. In particular, the experimental results\nhighlight the crucial role of the temperature parameter in\nproducing texts that resemble those created by humans in their\nstatistical structure, with an optimal range of temperatures,\naround T = 1, that induce the highest degree of similarity.\nInterestingly, we also illustrated how a network trained on a\nsingle-author corpus can produce texts that are attributed to\nthat author, according to authorship attribution algorithms.\nThe presented study suggests many interesting research\ndirections, which we plan to investigate in future works.\nFirst, we aim to compare the semantic information of\noriginal and artiﬁcial texts. It is clear from the samples shown\nin the experimental section, that LSTM texts are still far from\nhuman-generated texts in terms of meaningfulness, although\nshowing similar statistical properties. It is thus possible that\ncertain correlates of semantic information, like burstiness and\nclustering of keywords, are reﬂected in LSTM texts. Given that\neven the origin of long-range correlations in natural language\nis still debated, our work paves the way to deeper future\ninvestigations in this direction.\nSecondly, we aim to extend the analysis of these statistical\nproperties of the LSTM texts to different languages, in order\nto assess whether there are some languages that are easier or\nmore difﬁcult to reproduce for a machine.\nFinally, we aim to extend the study on authorship attribution\nand plagiarism, for which in this paper we only presented\npreliminary results. For that purpose we plan to employ a\nlarger corpus, in order to compare several authors, genres\nand languages, and to test different algorithms, as for instance\nthose based on trainable machine learning systems [49], [50].\nREFERENCES\n[1] M. A. Boden, “Creativity and Artiﬁcial Intelligence,” Artif. Intell., vol.\n103, no. 1-2, pp. 347–356, 1998.\n[2] E. Reiter, R. Dale, and Z. Feng, Building natural language generation\nsystems.\nMIT Press, 2000, vol. 33.\n[3] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[4] Y. Bengio et al., “Learning deep architectures for AI,” Foundations and\ntrends R⃝in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.\n[5] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for\ngenerating image descriptions,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2015, pp. 3128–3137.\n[6] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and\nK. Saenko, “Sequence to sequence-video to text,” in Proceedings of the\nIEEE Int. Conference on Computer Vision, 2015, pp. 4534–4542.\n[7] X. Zhang and M. Lapata, “Chinese Poetry Generation with Recurrent\nNeural Networks.” in EMNLP, 2014, pp. 670–680.\n[8] P. Potash, A. Romanov, and A. Rumshisky, “GhostWriter: Using an\nLSTM for Automatic Rap Lyric Generation.” in EMNLP, 2015, pp.\n1919–1924.\n[9] W. Ebeling and A. Neiman, “Long-Range Correlations between Letters\nand Sentences in Texts,” Physica A, vol. 215, no. 3, pp. 233–241, 1995.\n[10] M. A. Montemurro and P. Pury, “Long-range fractals correlations in\nliterary corpora,” Fractals, vol. 10, p. 451, 2002.\n[11] E. G. Altmann, G. Cristadoro, and M. Degli Esposti, “On the origin of\nlong-range correlations in texts,” PNAS, vol. 109, no. 29, pp. 11 582–\n11 587, 2012.\n[12] A. Graves, “Generating sequences with recurrent neural networks,”\nCoRR, vol. abs/1308.0850, 2013.\n[13] A. Karpathy, J. Johnson, and F. Li, “Visualizing and Understanding\nRecurrent Networks,” CoRR, vol. abs/1506.02078, 2015.\n[14] G. K. Zipf, The psycho-biology of language: an introduction to dynamic\nphilology.\nBoston,: Houghton Mifﬂin Company, 1935.\n[15] H. S. Heaps, Information retrieval, computational and theoretical as-\npects.\nNew York: Academic Press, 1978.\n[16] H. W. Lin and M. Tegmark, “Critical behavior in physics and proba-\nbilistic formal languages,” Entropy, vol. 19, no. 7, p. 299, 2017.\n[17] S. Takahashi and K. Tanaka-Ishii, “Do neural nets learn statistical laws\nbehind natural language?” PloS one, vol. 12, no. 12, p. e0189326, 2017.\n[18] A. Ghodsi and J. DeNero, “An Analysis of the Ability of Statistical\nLanguage Models to Capture the Structural Properties of Language,” in\nThe 9th Int. Natural Language Generation Conference, 2016, p. 227.\n[19] B. M. Lake and M. Baroni, “Still not systematic after all these years: On\nthe compositional skills of sequence-to-sequence recurrent networks,”\narXiv preprint arXiv:1711.00350, 2017.\n[20] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[21] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies\nwith gradient descent is difﬁcult,” Neural Networks, IEEE Transactions\non, vol. 5, no. 2, pp. 157–166, 1994.\n[22] K. Greff, R. K. Srivastava, J. Koutn´ık, B. R. Steunebrink, and J. Schmid-\nhuber, “LSTM: A Search Space Odyssey,” IEEE Transactions on Neural\nNetworks and Learning Systems, vol. PP, no. 99, pp. 1–11, 2017.\n[23] P. J. Werbos, “Backpropagation through time: what it does and how to\ndo it,” Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–1560, 1990.\n[24] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: A simple way to prevent neural networks from over-\nﬁtting,” Journ. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958, 2014.\n[25] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent Neural Network\nRegularization,” CoRR, vol. abs/1409.2329, 2014.\n[26] M. Gerlach and E. G. Altmann, “Stochastic model for the vocabulary\ngrowth in natural languages,” Phys. Rev. X, vol. 3, no. 2, 2013.\n[27] R. F. Voss and J. Clarke, “1/f noise in music and speech,” Nature, vol.\n258, no. 5533, pp. 317–318, 1975.\n[28] C. K. Peng, S. V. Buldyrev, A. L. Goldberger, S. Havlin, F. Sciortino,\nM. Simons, and H. E. Stanley, “Long-range correlations in nucleotide\nsequences,” Nature, vol. 356, no. 6365, pp. 168–70, 1992.\n[29] S. V. Buldyrev, A. L. Goldberger, S. Havlin, R. N. Mantegna, M. E.\nMatsa, C. K. Peng, M. Simons, and H. E. Stanley, “Long-range cor-\nrelation properties of coding and noncoding DNA sequences: GenBank\nanalysis,” Phys Rev E Stat Phys Plasmas Fluids Relat Interdiscip Topics,\nvol. 51, no. 5, pp. 5084–91, 1995.\n[30] C. E. Shannon, “Prediction and Entropy of Printed English,” Bell System\nTechnical Journal, vol. 30, no. 1, pp. 50–64, 1951.\n[31] T. M. Cover and J. A. Thomas, Elements of information theory, 2nd ed.\nHoboken, N.J.: Wiley-Interscience, 2006.\n[32] A. Lempel and J. Ziv, “On the complexity of ﬁnite sequences,” Infor-\nmation Theory, IEEE Transactions on, vol. 22, no. 1, pp. 75–81, 1976.\n[33] A. D. Wyner and J. Ziv, “Some Asymptotic Properties of the En-\ntropy of a Stationary Ergodic Data Source with Applications to Data-\nCompression,” IEEE Trans. on Information Theory, vol. 35, no. 6, pp.\n1250–1258, 1989.\n[34] T. Schurmann and P. Grassberger, “Entropy estimation of symbol\nsequences,” Chaos, vol. 6, no. 3, pp. 414–427, 1996.\n[35] J. Ziv and A. Lempel, “Universal Algorithm for Sequential Data\nCompression,” IEEE Trans. on Information Theory, vol. 23, no. 3, pp.\n337–343, 1977.\n[36] ——, “Compression of Individual Sequences Via Variable-Rate Coding,”\nIEEE Trans. on Information Theory, vol. 24, no. 5, pp. 530–536, 1978.\n[37] I. Kontoyiannis, P. H. Algoet, Y. M. Suhov, and A. J. Wyner, “Nonpara-\nmetric entropy estimation for stationary processes and random ﬁelds,\nwith applications to English text,” IEEE Transactions on Information\nTheory, vol. 44, no. 3, pp. 1319–1327, 1998.\n[38] J. Ziv and N. Merhav, “A measure of relative entropy between individual\nsequences with application to universal classiﬁcation,” IEEE transac-\ntions on information theory, vol. 39, no. 4, pp. 1270–1279, 1993.\n[39] F. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker, “Perplexity - a\nmeasure of the difﬁculty of speech recognition tasks,” The Journal of\nthe Acoustical Society of America, vol. 62, no. S1, pp. S63–S63, 1977.\n[40] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\nbilistic language model,” Journal of machine learning research, vol. 3,\nno. Feb, pp. 1137–1155, 2003.\n[41] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural\nimage caption generator,” in Computer Vision and Pattern Recognition\n(CVPR), 2015 IEEE Conference on.\nIEEE, 2015, pp. 3156–3164.\n[42] A. Papadopoulos, F. Pachet, and P. Roy, “Generating non-plagiaristic\nmarkov sequences with max order sampling,” in Creativity and Univer-\nsality in Language.\nSpringer, 2016, pp. 85–103.\n[43] F. R. P. Pachet and A. Papadopoulos, “Constrained Markov Sequence\nGeneration, Applications to Music and Text,” ser. Computational Syn-\nthesis and Creative Systems.\nSpringer, 2018.\n[44] C. Basile, D. Benedetto, E. Caglioti, and M. Degli Esposti, “An ex-\nample of mathematical authorship attribution,” Journal of Mathematical\nPhysics, vol. 49, no. 12, p. 125211, 2008.\n[45] D. Benedetto, M. Degli Esposti, and G. Maspero, “The puzzle of Basil’s\nEpistula 38: a mathematical approach to a philological problem,” Journal\nof Quantitative Linguistics, vol. 20, no. 4, pp. 267–287, 2013.\n[46] W. R. Bennett, Scientiﬁc and engineering problem-solving with the\ncomputer.\nPrentice Hall PTR, 1976.\n[47] V. Keˇselj, F. Peng, N. Cercone, and C. Thomas, “N-gram-based author\nproﬁles for authorship attribution,” in PACLING ’03, Halifax, Canada,\nAugust 2003, pp. 255–264.\n[48] R. Clement and D. Sharp, “Ngram and Bayesian classiﬁcation of\ndocuments for topic and authorship,” Literary and linguistic computing,\nvol. 18, no. 4, pp. 423–447, 2003.\n[49] A. Neme, J. R. Gutierrez-Pulido, A. Mu˜noz, S. Hern´andez, and T. Dey,\n“Stylistics analysis and authorship attribution algorithms based on self-\norganizing maps,” Neurocomputing, vol. 147, pp. 147–159, 2015.\n[50] M. L. Jockers and D. M. Witten, “A comparative study of machine\nlearning methods for authorship attribution,” Literary and Linguistic\nComputing, vol. 25, no. 2, pp. 215–223, 2010.\nMarco Lippi received the PhD in Computer and\nAutomation Engineering from the University of Flo-\nrence in 2010. Currently, he is Assistant Professor at\nthe Department of Sciences and Methods for Engi-\nneering, University of Modena and Reggio Emilia.\nHe previously held positions at the Universities of\nFlorence, Siena and Bologna, and he was visiting\nscholar at UPMC, Paris. His work focuses on ma-\nchine learning and artiﬁcial intelligence, with appli-\ncations in bioinformatics, law, game playing, natural\nlanguage processing, and argumentation mining.\nMarcelo A. Montemurro obtained a PhD in Theo-\nretical Physics in 2002 at the National University\nof C´ordoba (Argentina). He then moved to Italy\nwith a fellowship from UNESCO to work at the\nInternational Centre for Theoretical Physics in Tri-\neste (Italy). In 2004 he moved to the University\nof Manchester (UK) where he held a number of\nfellowships before being appointed Lecturer. His\nresearch interests include the statistical physics of\ncomplex systems and computational neuroscience.\nMirko Degli Esposti is Full Professor in Mathemat-\nical Physics in the Computer Science Department\nat University of Bologna. He holds a Degree in\nPhysics and a PhD in Mathematics (Caltech and\nPennState, USA). Lately he turned his attention to\napplications of the theory of dynamical systems and\nof information theory to life science and human\nscience, in particular human language.\nGiampaolo Cristadoro is Associate Professor in\nMathematical Physics at the Department of Math-\nematics and Applications, University of Milano-\nBicocca. He received the PhD in Theoretical Physics\nfrom the University of Insubria, Como and the Uni-\nversity Paul-Sabatier, Toulouse. He was postdoctoral\nfellow at the Max Planck Institute for the Physics\nof Complex Systems in Dresden, at the Center for\nNonlinear and Complex Systems in Como, and at\nthe Department of Mathematics at University of\nBologna, where he became Assistant and Associate\nProfessor. His research interests include dynamical systems, probability and\ninformation theory, with applications to life science and natural language.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2018-04-10",
  "updated": "2019-04-15"
}