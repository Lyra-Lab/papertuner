{
  "id": "http://arxiv.org/abs/1804.05499v1",
  "title": "Community Member Retrieval on Social Media using Textual Information",
  "authors": [
    "Aaron Jaech",
    "Shobhit Hathi",
    "Mari Ostendorf"
  ],
  "abstract": "This paper addresses the problem of community membership detection using only\ntext features in a scenario where a small number of positive labeled examples\ndefines the community. The solution introduces an unsupervised proxy task for\nlearning user embeddings: user re-identification. Experiments with 16 different\ncommunities show that the resulting embeddings are more effective for community\nmembership identification than common unsupervised representations.",
  "text": "arXiv:1804.05499v1  [cs.CL]  16 Apr 2018\nCommunity Member Retrieval on Social Media using Textual Information\nAaron Jaech, Shobhit Hathi, Mari Ostendorf\nUniversity of Washington\n{ajaech, shathi, ostendor}uw.edu\nAbstract\nThis paper addresses the problem of commu-\nnity membership detection using only text fea-\ntures in a scenario where a small number of\npositive labeled examples deﬁnes the commu-\nnity. The solution introduces an unsupervised\nproxy task for learning user embeddings: user\nre-identiﬁcation. Experiments with 16 differ-\nent communities show that the resulting em-\nbeddings are more effective for community\nmembership identiﬁcation than common unsu-\npervised representations.\n1\nIntroduction\nActive users of social media often like identifying\nother users with common interests and values. Or,\na user may want to ﬁnd other users that share char-\nacteristics with speciﬁc accounts that they follow,\ne.g. cartoonists or local food trucks. Members of\nsuch communities of interest are often identiﬁable\nvia their social network connections, and shared\nsocial connections are clearly important in recom-\nmendations. However, shared connections often\nreﬂect a subset of a person’s interests, and there\nmay be users of interest where any shared connec-\ntions are distant. In addition, there may be scenar-\nios where there is no explicit social graph, or the\nfull graph is expensive to obtain. In such cases, the\nlanguage of tweets, blogs, etc. is helpful in identi-\nfying users with particular interests.\nIn this paper, we represent users in terms of\nthe text in their communications and introduce a\nscenario where a user can deﬁne a “community”\nby providing a small number of example accounts\nthat are used to train a system for retrieving sim-\nilar users. Note that our use of the term “com-\nmunity” differs from other online contexts, where\nmembers explicitly self-identify with a commu-\nnity (e.g. by joining a discussion forum or using\na speciﬁc hashtag). The community is in the eye\nof the user issuing the query.\nWe frame the task of community membership\ndetection as a retrieval problem. A small set of\nrepresentative accounts selected by the user forms\nthe query, and the system retrieves additional\ncommunity members from a large index of\naccounts.\nThe task is loosely related to entity\nset expansion (Pantel et al., 2009). We make no\nassumptions about the type of communities that\ncan be handled, and no labeled data is available\nother than the query.\nBecause the training set\n(query) is minimal,\nunsupervised learning is\nuseful for the text representation.\nWe propose\nthe proxy task of person re-identiﬁcation for\nlearning a user embedding, where the goal is for\ntwo embeddings from the same user to be closer\nto each other than to the embedding of a random\nuser. The hypothesis is that a representation useful\nfor detecting similarities between posts from the\nsame person made at different times will also do\nwell at identifying similarities between people in\nthe same community. This hypothesis stems from\nobservations that people with shared interests\noften talk about topics related to these interests,\nand that they tend to have shared jargon and other\nsimilarities in language use (Nguyen and Ros´e,\n2011;\nDanescu-Niculescu-Mizil et al.,\n2013;\nTran and Ostendorf, 2016).\nIn this paper, we demonstrate experimentally\nthat the re-identiﬁcation proxy task is useful with\nsimple models that are suited to the retrieval sce-\nnario, and present analyses showing that the ap-\nproach learns to emphasize words associated with\nindividual interests and polarizing issues.\n2\nModel\nThe model for community detection includes: i) a\nmapping from a user’s text (a collection of tweets)\nto a k-dimensional embedding, and ii) a binary\nclassiﬁer for detecting whether a candidate user\nbelongs to the target community. The novel con-\ntribution of the work is the proxy re-identiﬁcation\ntask for learning the user embedding.\nUser Embedding Model.\nThe mapping from\ntext\nto\nan\nembedding\ncould\nleverage\nany\ndocument-level representation.\nWe focus on a\nsimple weighted bag-of-words neural model for\ndirect comparison to other popular methods, mo-\ntivated by the fact that many virtual communities\nform around shared interests in particular topics.\nSpeciﬁcally, let cp,i denote the number of times\nperson p uses word vi ∈V , where V is the vocab-\nulary, and wp,i = log(cp,i + 1) be the log-scaled\nword count. Then the user embedding is\nup =\nwT\np E\n||wTp E||\n(1)\nwhere wp = [wp,1 · · · wp,|V |] and E ∈ℜ|V |×k is\nthe matrix of word embeddings.\nPerson Re-identiﬁcation Learning.\nThe em-\nbedding matrix E is learned using a person re-\nidentiﬁcation objective that encourages embed-\ndings from the same person to be closer than\nembeddings from different people. We build on\nthe triplet loss function taken from Schroff et al.\n(2015) used to train a face recognition system.\nSpeciﬁcally:\nE = argmin\nE\nX\np1,p2∈P\ncost(p1, p2),\n(2)\ncost(p1, p2) = (1 + d(up1\n1, up2\n1) −d(up1\n1, up1\n2))+,\nwhere d(x, y) is the cosine distance between x and\ny. up1\n1 and up2\n1 are embeddings made from dis-\ntinct subsets of a single person’s Tweets, and up1\n2\nis an embedding made from a subset of another\nperson’s Tweets. In practice, we estimate the loss\nfunction randomly sampling triplets (p1\n1, p2\n1, p1\n2)\nfrom a large training set.\nClassiﬁer.\nA logistic regression model with L2\nregularization is used for the classiﬁer, because it\nis simple but powerful and our scenario has lit-\ntle training data. Simplicity is important because\nthe classiﬁer should be trainable in real-time after\nreceiving the query. The classiﬁer objective is to\ndiscriminate the embeddings from the users in the\nquery from a set of user embeddings from the gen-\neral collection. For the i-th user, let yi ∈{0, 1}\nbe the binary label indicating whether the user be-\nlongs to a particular community and ui be the user\nembedding. The logistic regression model com-\nputes the probability that the user belongs to the\ncommunity according to:\np(yi = 1|ui) = σ(wT ui + b),\n(3)\nwhere σ(x) = 1/(1 −e−x). During evaluation,\nthe users in the index are ranked according to the\nmaximum log probability ratio\nargmax\ni\nlog p(yi = 1|ui)\np(yi = 0|ui) = argmax\ni\nwT ui. (4)\nBecause the classiﬁer is linear, we can quickly\nretrieve the top matching users from the in-\ndex using approximate nearest-neighbor search\n(Kushilevitz et al., 2000). The technique is scal-\nable up to hundreds of millions of users and be-\nyond.\n3\nData\nAll data was collected using the Twitter API.1 We\nused 1,035 randomly selected items from the list\nof trending topics in the USA during the period\nApril-June 2017 to query for users and collected\ntheir most recent 2,000 tweets. Example trend-\ning topics are #Quantico, RonaldoCristiano, and\n#MayDay2017. (The full list is available with the\ndata.) Each user had at least one Tweet that men-\ntioned a trending topic but their other Tweets could\nbe on any topic.\nWe refer to this collection as the “general pop-\nulation,” because it was not targeted towards any\nparticular community.\nIn total, we collected\naround 80,000 such users and used roughly 36,000\nfor learning user embeddings, 1,000 for learning\nthe community classiﬁers, and 43,000 for evalua-\ntion. The text is mostly in English, but some of it\nis in Spanish, French, or other languages. A list of\nthe tweet IDs is available.2\nTo support evaluation with the community de-\ntection task, we conducted a second collection\n(contemporaneous with the ﬁrst) targeting mem-\nbers that we had identiﬁed as belonging to one of\n16 communities (Table 2). To deﬁne a “commu-\nnity,” volunteers manually selected a set of users\nthat ﬁt with a theme that they had familiarity with.\nThus, the speciﬁc 16 communities were deter-\nmined based on themes of interest to the authors\nand their friends and colleagues, where we could\n1http://developer.twitter.com/en/docs/api-reference-index\n2http://github.com/ajaech/twittercommunities\nbe reasonably conﬁdent about membership deci-\nsions. In addition, we tried to avoid themes that\nmight be biased towards well-known celebrities,\nand we made an effort to have diversity in the char-\nacteristics of the communities. The communities\nwere selected to span a range of topics, sizes (6-\n130 accounts), individuals vs. organizations, and\nother characteristics. A few of the communities\nare comprised of organizations rather than indi-\nviduals such as the high school drama departments\nand the Pittsburgh food truck communities. (The\ncommunity names are invented by the authors for\npurposes of describing the data in this paper; they\nare not part of the retrieval task.)\nThe text is lower-cased and some punctuation\nis removed using regular expressions. Words are\nformed by splitting on white space.\nWhile this\nstrategy will not work for languages that do not\ndelimit words by spaces, these make up a neg-\nligible portion of the data.\nA 174k vocabulary\nwas created by extracting the unique types that\nwere seen in the tweets from the general popula-\ntion, as well as selected bigrams extracted using\nthe open source Gensim library using a point-wise\nmutual information criteria ( ˇReh˚uˇrek and Sojka,\n2010). The vocabulary included roughly 49k bi-\ngrams, 36k usernames and 17k hashtags. User-\nnames, hashtags, and URLs are not treated spe-\ncially and can be part of the vocabulary just like\nany other word if they occur frequently enough.\n4\nExperiments\n4.1\nExperiment Conﬁguration\nThe experiments involved comparing different\nmethods of learning user embeddings, all with a\nweighted bag-of-words modeling assumption:\n• Weighted word2vec (W2V) using default3\nskip-gram training (Mikolov et al., 2013);\n• Latent Dirichlet allocation (LDA) (Blei et al.,\n2003), using default settings from the Scikit\nLearn library (Pedregosa et al., 2011);\n• Person re-identiﬁcation with random initial-\nization (RE-ID); and\n• Person re-identiﬁcation with W2V initializa-\ntion (RE-ID, W2V init).\nBoth count-weighted W2V and LDA have been\nused as unsupervised representations in Twitter\n3The default conﬁguration uses a window of ±7 words.\nWe also tried using a window of 50 words, which roughly\nmatches the context used in other methods, but community\ndetection performance was signiﬁcantly worse.\nclassiﬁcation tasks, as noted in Section 5.\nDe-\nfault conﬁgurations are used because there is in-\nsufﬁcient data to have a separate validation set.\nFor all methods, the same vocabulary, ﬁnal di-\nmension (128), unit vector normalization strategy,\nand logistic regression model training were used.\nThe embeddings are trained on the 36k user gen-\neral data, randomly sampling pairs of users p1 and\np2 and then sampling 50 tweets at a time with-\nout replacement to create up1\n1, up2\n1, and up1\n2. The\nlogistic regression models are trained on the 1K\nuser general training pool, using the 50 most re-\ncent tweets for each user. Because there are so\nfew labeled examples for most communities, train-\ning and evaluation is done using a leave-one-out\nstrategy with the positive samples but including all\nof the 1K negative samples. For each of the N\nclassiﬁers (corresponding to N labeled samples),\nthe test set is the left-out positive example and the\n43K general user test pool. Also because of train-\ning limitations, there is no tuning of the regular-\nization weight; the default weight of 1.0 is used.\nTuning may be useful given a collection of train-\ning and testing communities. Performance is aver-\naged over the N classiﬁers (corresponding to the\nN labeled samples). Two evaluation criteria are\nused: a retrieval metric (inverse mean reciprocal\nrank or 1/MRR) (Voorhees et al., 1999) and a de-\ntection metric (area under the curve or AUC).\n4.2\nResults\nTable 1 shows retrieval results averaged across\nall communities. The RE-ID model outperforms\nthe W2V and LDA baselines for both criteria,\nwith substantial gains in 1/MRR (lower is bet-\nter). Further, the version of RE-ID initialized with\nword2vec did better than the one that was initial-\nized randomly even though the randomly initial-\nized version was trained for twice as long.\nStrategy\nAUC\n1/MRR\nW2V\n93.9\n846\nLDA\n95.0\n501\nRE-ID (rand. init)\n98.0\n24\nRE-ID (W2V init)\n98.5\n12\nTable 1: Performance of different model variants.\nA breakdown of the best model performance\nby community is given in Table 2. Sample size\ndoes not seem to be a good indicator of perfor-\nmance: the two smallest communities (Cartoon-\nists, Fresno City Council) had the worst and one\nof the best results, respectively. Anecdotally, we\nobserved that the sample of cartoonists were more\nlikely to Tweet about topics outside their main in-\nterest (e.g., politics or sports). We hypothesize that\nthe diversity of interests of the members of a com-\nmunity affects the difﬁculty of the retrieval task,\nbut our test set is too small to conﬁrm this hypoth-\nesis.\nCommunity\nSize\n1/MRR\nCartoonists\n8\n58.1\nChess Stars\n14\n5.4\nConan Show Writers\n12\n4.7\nFashion Commentators\n11\n8.3\nFresno City Council\n6\n3.0\nHedge Fund Managers\n11\n25.7\nH.S Drama Departments\n18\n2.3\nMathematicians\n11\n32.6\nNLP Researchers\n50\n4.9\nPittsburgh Food Trucks\n15\n3.3\nPolice Dogs\n16\n2.7\nProfessional Economists\n11\n3.6\nSCOTUS Reporters4\n16\n1.9\nThe Stranger Reporters5\n11\n8.3\nUltimate Frisbee Players\n130\n6.7\nUltramarathon Runners\n28\n14.6\nTable 2: W2V+RE-ID results by community\nThese results may underestimate performance,\nbecause there is a chance that some users in the\ngeneral population test data may actually belong\nto one or more of our test communities, i.e. there\ncould be mislabeled data. To assess the potential\nimpact, we manually checked the top ten false pos-\nitives for each community for mislabeled users.\nWe did discover some mislabeled examples for\nthe economist, hedge fund manager, and ultrama-\nrathon runner communities.\nFor the most part,\nthe top ranked users from the general population\ntended to be people from related communities. For\nexample, the top false ultimate frisbee users con-\ntained people who wrote about their participation\nin tournaments for other sports such as soccer.\n4.3\nAnalysis\nThe ﬁnding that the W2V-initialized RE-ID model\nis signiﬁcantly better than W2V raises the ques-\ntion: how do the embeddings learned by the re-\nidentiﬁcation task differ from the ones learned by\n4People who write news articles about the Supreme Court\nof the United States.\n5The Stranger is a small weekly newspaper.\nthe word2vec objective? To investigate this, we\nlooked at the 1,000 words in the RE-ID model with\nembeddings that were farthest (in Euclidean dis-\ntance) from its word2vec initialization. These top\nwords disproportionately contain Twitter user han-\ndles, so some social network structure is captured.\nUsing agglomerative clustering, we found groups\nof words that centered around frequent words used\nin particular regions (foreign words, dialects) or\ncultures (sociolects), associated with hobbies or\ninterests (speciﬁc sports, music genres, gaming),\nor polarizing topics (political parties, controversial\nissues). At least one of the top tokens was the user-\nname of an account later identiﬁed as being spon-\nsored by the Russian government to spread propa-\nganda during the United States presidential elec-\ntion, e.g., “ten gop” in Table 4 of the Appendix.\nWe also looked at which communities are clos-\nest in the embedding space. We represent a com-\nmunity with the average of the member embed-\ndings and use a normalized cosine distance for\nsimilarity. The two nearest neighbors are Math-\nematicians and NLP researchers, which are also\nclose to the next two nearest neighbors, Hedge\nFund Managers and Professional Economists.\nTo interpret what the model as a whole cap-\ntured, we found the top scoring tweets for each\nheld-out user (creating an embedding for a single\ntweet) according to the logistic regression model.\nRepresentative examples include “recurrent neu-\nral network grammars simpliﬁed and analyzed”\nfor NLP Researchers, and “we’re looking forward\nto seeing you opening night may 24th love the cast\nof high school musical” for High School Drama\nclubs. Examples for additional communities are\nincluded in the appendix. The results provide in-\nsight into the community member identiﬁcation\ndecision.\n5\nRelated Work\nOne notion of community detection involves dis-\ncovering different communities within a collection\nof users (Chen et al., 2009; Di, 2011; Fani et al.,\n2017).\nA related task is making recommenda-\ntions of friends or people to follow (Gupta et al.,\n2013; Yu et al., 2016). In contrast, our task in-\nvolves identifying other members of a community,\nwhich is speciﬁed in terms of a set of example\nusers. These tasks use different learning frame-\nworks (our work uses supervised learning), but\nthe features (social network and/or text cues) are\nrelevant across tasks. Our task is perhaps more\nsimilar to using social media text to predict author\ncharacteristics such as personality (Golbeck et al.,\n2011), gang membership (Wijeratne et al., 2016),\ngeolocation (Han et al., 2014), political afﬁlia-\ntion (Makazhanov et al., 2014), occupational class\n(Preot¸iuc-Pietro et al., 2015), and more. Again, a\ncommonality across tasks is the frequent use of\nunsupervised representations of textual features.\nIn representing text, a common assumption\nis\nthat\ncommunity\nlanguage\nreﬂects\ntopical\ninterests,\nso\nrepresentations\naimed\nat\ntopic\nmodeling\nhave\nbeen\nused,\nincluding\nLDA\n(Pennacchiotti and Popescu,\n2011)\nand\ntf-idf\nweighted\nword2vec\nembeddings\n(Boom et al.,\n2016; Wijeratne et al., 2016).\nYu et al. (2016)\ncompute a user embedding by averaging tweet\nembeddings.\nOther work investigates methods\nfor learning embeddings that integrate text and\nsocial network (graph or text-based) features\n(Benton et al., 2016).\nThe work closest to ours is by Fani et al. (2017),\nwhich learns embeddings that are close for like-\nminded users, where like-minded pairs are iden-\ntiﬁed by a deterministic algorithm that leverages\ntiming of related posts. Our approach requires no\nadditional heuristics for deﬁning user similarity,\nbut instead relies on an objective that maximizes\nself-similarity and minimizes similarity to other\nusers randomly sampled from a large general pool.\nOur person re-identiﬁcation proxy task makes\nuse of the triplet loss used to learn person embed-\ndings for face recognition (Schroff et al., 2015). In\nimage processing, person re-identiﬁcation refers\nto the task of tracking people who have left the\nﬁeld of view of one camera and are later seen by\nanother camera (Bedagkar-Gala and Shah, 2014).\nIt is different from our proxy task and the methods\nare not the same.\n6\nConclusion\nIn summary, this paper deﬁnes a task of com-\nmunity member retrieval based on their tweets,\nintroduces a person re-identiﬁcation task to al-\nlow community deﬁnition with a small number of\nexamples, and shows that that the method gives\nvery good results compared to word2vec and LDA\nbaselines.\nAnalyses show that the user embed-\ndings learned efﬁciently represent user interests.\nThe text embeddings are largely complementary\nto the social network features used in other stud-\nies, so performance gains can be expected from\nfeature combination.\nWhile our experiments use a bag-of-words rep-\nresentation, as in most related work, the re-\nidentiﬁcation training objective proposed here can\neasily be used with other methods for deriv-\ning document embeddings, e.g. (Le and Mikolov,\n2014; Kim, 2014).\nAcknowledgements\nThe authors thank the anonymous reviewers for\ntheir feedback and helpful suggestions.\nReferences\nApurva Bedagkar-Gala and Shishir Shah. 2014.\nA\nsurvey of approaches and trends in person re-\nidentiﬁcation.\nImage and Vision Computing,\n32(4):270–216.\nAdrian Benton, Raman Arora, and Mark Dredze. 2016.\nLearning multiview embeddings of Twitter users. In\nProc. ACL, volume 2, pages 14–19.\nDavid Blei, Andrew Ng, and Michael Jordan. 2003.\nLatent dirichlet allocation.\nJournal of Machine\nLearning Research, 3:993–1022.\nCedric De Boom, Steven Van Canneyt, Thomas De-\nmeester, and Bart Dhoedt. 2016.\nRepresentation\nlearning for very short texts using weighted word\nembedding aggregation.\nPattern Recognition Let-\nters, 80:150–156.\nJiyang Chen, Osmar R. Za¨ıane, and Randy Goebel.\n2009. Local community identiﬁcation in social net-\nworks.\nIn International Conference on Advances\nin Social Network Analysis and Mining, pages 237–\n242.\nCristian Danescu-Niculescu-Mizil, Robert West, Dan\nJurafsky, Jure Leskovec, and Christopher Potts.\n2013. No country for old members: User lifecy-\ncle and linguistic change in online communities. In\nProc. WWW.\nYing Di. 2011. Community detection: topological vs.\ntopical. Journal of Informatics, 5(4):489–514.\nHossein Fani, Ebrahim Bagheri, and Weichang Du.\n2017.\nTemporally like-minded user community\nidentiﬁcation through neural embeddings. In Proc.\nACM Conference on Information and Knowledge\nManagement, pages 577–586.\nJennifer Golbeck, Cristina Robles, Michon Edmond-\nson, and Karen Turner. 2011.\nPredicting person-\nality from twitter.\nIn Privacy, Security, Risk and\nTrust (PASSAT) and 2011 IEEE Third Inernational\nConference on Social Computing (SocialCom), 2011\nIEEE Third International Conference on, pages\n149–156. IEEE.\nPankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh\nSharma, Dong Wang, and Reza Zadeh. 2013. Wtf:\nThe who to follow service at Twitter.\nIn Proc.\nWWW, pages 505–514. ACM.\nBo Han, Paul Cook, and Timothy Baldwin. 2014. Text-\nbased Twitter user geolocation prediction. Journal\nof Artiﬁcial Intelligence Research, 49:451–500.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation.\nIn Proc. EMNLP, pages\n1746–1751.\nEyal Kushilevitz, Rafail Ostrovsky, and Yuval Ra-\nbani. 2000. Efﬁcient search for approximate nearest\nneighbor in high dimensional spaces. SIAM Journal\non Computing, 30(2):457–474.\nQuo Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Proc.\nICML, pages 3104–3112.\nAibek Makazhanov, Davood Raﬁei, and Muhammad\nWaqar. 2014. Predicting political preference of twit-\nter users.\nSocial Network Analysis and Mining,\n4(1):1–15.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Proc. NIPS, pages 3111–3119.\nDong Nguyen and Carolyn P. Ros´e. 2011. Language\nuse as a reﬂection of socialization in online com-\nmunities. In Proceedings of the Workshop on Lan-\nguages in Social Media, LSM ’11, pages 76–85. As-\nsociation for Computational Linguistics.\nPatrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-\nMaria Popescu, and Vishnu Vyas. 2009. Web-scale\ndistributional similarity and entity set expansion.\nIn Proc. EMNLP, pages 938–947. Association for\nComputational Linguistics.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-\nsos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. 2011. Scikit-learn: Machine learning\nin Python. Journal of Machine Learning Research,\n12:2825–2830.\nMarco Pennacchiotti and Ana-Maria Popescu. 2011. A\nmachine learning approach to Twitter user classiﬁ-\ncation. In ICWSM.\nDaniel Preot¸iuc-Pietro, Vasileios Lampos, and Niko-\nlaos Aletras. 2015. An analysis of the user occupa-\ntional class through Twitter content. In Proc. ACL-\nIJCNLP, pages 1754–1764.\nRadim ˇReh˚uˇrek and Petr Sojka. 2010. Software frame-\nwork for topic modelling with large corpora. In Pro-\nceedings of the LREC 2010 Workshop on New Chal-\nlenges for NLP Frameworks, pages 45–50, Valletta,\nMalta. ELRA.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015. Facenet: A uniﬁed embedding for\nface recognition and clustering.\nIn Proc. CVPR,\npages 815–823.\nTrang Tran and Mari Ostendorf. 2016. Characterizing\nthe language of online communities and its relation\nto community reception. In Proc. EMNLP.\nEllen M Voorhees et al. 1999. The TREC-8 question\nanswering track report. In Trec, volume 99, pages\n77–82.\nSanjaya Wijeratne, Lakshika Balasuriya, Derek Doran,\nand Amit Sheth. 2016.\nWord embeddings to en-\nhance Twitter gang member proﬁle identiﬁcation. In\nProc. IJCAI Workshop on Semantic Machine Learn-\ning.\nYang Yu, Xiaojun Wan, and Xinjie Zhu. 2016. User\nembedding for scholarly microblag recommenda-\ntion. In Proc. ACL, pages 449–453.\nAppendix\nSupplementary materials include examples of rep-\nresentative tweets for each community (Table 3)\nand lists of the words that change the most be-\ntween Word2Vec and the person re-identiﬁcation\ntask (Table 4).\nCommunity\nSelected Tweet\nChess Stars\n@chesscom yep karpov well done twittersphere\nProfessional Economists\n#china real estate as long as liquidity remains ample this will continue\nFashion Commentators\nrihanna’s fenty corp creative director jahleel weaver styles the collection\non 3 muses\nFresno City Council\ngr8 resource developed by our local @citdfresno on how to export @city-\noffresno @fresnocountyedc lee ann eager\nHigh School Drama\nwe’re looking forward to seeing you opening night may 24th love the cast\nof high school musical\nMathematicians\nforms of knowledge of advanced mathematics for teaching (i wrote a thing\n)\nNLP Researchers\nrecurrent neural network grammars simpliﬁed and analyzed\nPolice Dogs\nwhen a trained police dog is placed with another handler they complete a\nre handling course to be licensed normally 2 weeks\nSCOTUS Reporters\nas supreme court throws out two gop-drawn congressional districts as un-\nconstitutional racial gerrymanders\nUltramarathon Runners\nwe’re covering the lake sonoma 50 mile live on saturday tell your friends\nspread the word and get ready\nTable 3: Top tweets for selected communities. Underscore is used to join bigrams.\nInterpretation\nTop Words\nLanguages &\n• `a, c¸a, j’ai, quand, c’est, avec, sur, dans le\nDialects\n• ´e, n˜ao, melhor, tem, mesmo, s´o, mais, hoje, uma, t´a, j´a\n• es un, m´as, jugar, en el, maduro, jajajaja\n• bruh, dawg, @iamakademiks, black women, @chancetherapper, lmaooo, y’all,\ntryna\nSports\n• @mlb, baseball, bullpen, @angels, mets, mlb\n• arsenal, mate, liverpool, @manutd, mourinho, #mufc\n• @nhl, hockey, nhl, leafs, @nhlblackhawks, @nhlonnbcsports\n• xd, @playoverwatch, #ps4share, anime, @keemstar, overwatch, twitch, @nin-\ntendoamerica, gaming\nMusic\n• @niallofﬁcial, @harry styles, @louis tomlinson, @ashton5sos, @shawnmendes,\n@ethandolan, @graysondolan, @michael5sos, @danisnotonﬁre\nPolitical\n• @indivisibleteam #resist, #trumpcare, @ezlusztig, @kurteichenwald, @george-\ntakei, @sarahkendzior, @repadamschiff, @malcolmnance, @lawrence\n• @mitchellvii, @prisonplanet, @realjameswoods, @jackposobiec, @bfraser747,\n@cernovich, @ten gop, #maga\nOther\n• tories, labour, corbyn, #auspol, tory, mum, nhs, lads scotland\nTable 4: Clusters of words which change the most between Word2Vec and the person re-identiﬁcation task.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-04-16",
  "updated": "2018-04-16"
}