{
  "id": "http://arxiv.org/abs/1804.05806v1",
  "title": "Deep Embedding Kernel",
  "authors": [
    "Linh Le",
    "Ying Xie"
  ],
  "abstract": "In this paper, we propose a novel supervised learning method that is called\nDeep Embedding Kernel (DEK). DEK combines the advantages of deep learning and\nkernel methods in a unified framework. More specifically, DEK is a learnable\nkernel represented by a newly designed deep architecture. Compared with\npre-defined kernels, this kernel can be explicitly trained to map data to an\noptimized high-level feature space where data may have favorable features\ntoward the application. Compared with typical deep learning using SoftMax or\nlogistic regression as the top layer, DEK is expected to be more generalizable\nto new data. Experimental results show that DEK has superior performance than\ntypical machine learning methods in identity detection, classification,\nregression, dimension reduction, and transfer learning.",
  "text": "Deep Embedding Kernel\nLinh Le 1 Ying Xie 2\nAbstract\nIn this paper, we propose a novel supervised learn-\ning method that is called Deep Embedding Kernel\n(DEK). DEK combines the advantages of deep\nlearning and kernel methods in a uniﬁed frame-\nwork. More speciﬁcally, DEK is a learnable ker-\nnel represented by a newly designed deep archi-\ntecture. Compared with pre-deﬁned kernels, this\nkernel can be explicitly trained to map data to\nan optimized high-level feature space where data\nmay have favorable features toward the applica-\ntion. Compared with typical deep learning using\nSoftMax or logistic regression as the top layer,\nDEK is expected to be more generalizable to new\ndata. Experimental results show that DEK has su-\nperior performance than typical machine learning\nmethods in identity detection, classiﬁcation, re-\ngression, dimension reduction, and transfer learn-\ning.\n1. Introduction\nWe consider two major branches of machine learning, kernel\nmethods (Hofmann et al., 2008) and deep learning (Schmid-\nhuber, 2015). Kernel methods center around the kernel trick\n(Hofmann et al., 2008) – using a pre-deﬁned kernel function\nto implicitly map data to a new feature space. However,\nthis implicit mapping is rather heuristic in that there is no\nguarantee that the pre-deﬁned kernel can lead to a more\nfavorable feature space where data has better distribution\ntowards the application. Hyper-parameter tuning algorithms\nlike grid-search may improve the model performance (i.e.\nless prediction errors), but this brutal-force strategy does\nnot fundamentally solve the problem of using pre-deﬁned\nkernels.\nDeep learning, on the other hand, utilizes a high number\nof parameters structured by layers of neural networks to\n1Institute of Analytics and Data Science, Kennesaw State\nUniversity, Georgia, USA\n2Department of Information Tech-\nnology, Kennesaw State University, Georgia, USA . Correspon-\ndence to: Linh Le <lle12@students.kennesaw.edu>, Ying Xie\n<yxie2@kennesaw.edu>.\nmap the data to an explicit feature space with speciﬁed di-\nmensionality (Schmidhuber, 2015). The parameters of the\nnetwork that determines the mapping are typically tuned\nbased on an explicit learning objective. In other words, by\ndeep learning, the mapping of data into high-level repre-\nsentations is directly guided by the given learning objective\nthrough some top-down learning processes such as gradient\ndescend. Therefore, learning objectives play critical roles in\nthe quality of mapping. Frequently used learning objectives\ntry to minimize training errors, which may not have the\ndesired generalization ability according statistical learning\ntheory (Vapnik, 1999). The work in (Tang, 2013) tries to\nimprove generalization ability of deep learning by using\nlinear SVM at the top layer, but the computational complex-\nity of integrating SVM to deep learning is high. Another\nrestriction of deep learning is that the dimensionality of\nthe mapped feature space is pre-speciﬁed, instead of being\nlearned.\nIn this paper, we try to address the problems of both kernel\nmachines and deep learning by proposing a new supervised\nlearning method called Deep Embedding Kernel (DEK) that\nis able to utilize the strengths of each method to address the\nweakness of the other in a uniﬁed framework. First of all,\nDEK does not explicitly map data to a feature space with pre-\nspeciﬁed dimensionality, nor implicitly map data through\na pre-deﬁned kernel; instead, DEK uses a newly designed\ndeep architecture to represent a learnable kernel. In other\nwords, DEK utilizes the learning power of deep learning to\ntrain a kernel, which in turn implicitly maps data to a high\ndimensional feature space. The learning objective of DEK\nspeciﬁes a desired relationship of data in the mapped feature\nspace. Then the kernel represented by DEK trained by the\nlearning objective is expected to implicitly map data to such\na feature space. Therefore, the whole mapped feature space,\nincluding its dimensionality, is learned via deep learning.\nUsing deep architectures to learn a kernel, instead of directly\nlearn the feature space also has the advantages of ﬂexibility\nin that the learned kernel can be applied to a wide range\nof supervised learning tasks including identity detection,\ngeneral classiﬁcation, dimension reduction, regression, and\nother kernel based machine learning applications.\nThe architecture of DEK integrates two learning networks,\nnamely kernel network and embedding network. The\nkernel network directly represents the parameterized kernel\narXiv:1804.05806v1  [stat.ML]  16 Apr 2018\nDeep Embedding Kernel\ntrained from data, while the embedding network tries to\nlearn optimized data representations to feed into the kernel\nnetwork. The training of both networks is done in a single\ngradient descent process with the same learning objective\nthat speciﬁes an optimized relationship of data in the desired\nfeature space.\nDEK can be easily extended to work on unstructured data by\nlaying itself on top of deep architectures designed for certain\ntype of unstructured data, such as Convolutional Neural\nNetwork (CNN) for image data, Recurrent Neural Network\n(RNN) for sequential data, or the combination of CNN\nand RNN for video data. By this extension, the particular\ndeep architecture used on unstructured data will learn vector\nembedding from unstructured data in the same learning\nprocess where embedding network and kernel network of\nDEK are trained via gradient descent. Moreover, DEK can\nbe used to boost the learning power of transfer learning by\nbeing laid over a trained deep network that outputs vector\nembedding.\nIn this paper, we will demonstrate that DEK has superior\nperformance over other typical supervised learning meth-\nods, such as Kernel Support Vector Machines, Gradient\nBoosting Trees, Random Forests, and Neural Networks on\nmultiple learning tasks, including identity detection, general\nclassiﬁcation, regression, dimension reduction, and transfer\nlearning.\n2. Related Works\nVarious attempts were made to stack kernels to form deep ar-\nchitectures in (Zhuang et al., 2011), (Strobl & Visweswaran,\n2013), (Jose et al., 2013), (Jiu & Sahbi, 2017), and (Sahbi,\n2017). The output of this type of deep architecture is typi-\ncally a highly nonlinear combination of input kernels. The\nlearning process of stacking kernels involves jointly training\na SVM classiﬁer and modifying network weights as well\nas kernel parameters using gradient descent. Some limita-\ntions of these works include 1) using pre-deﬁned kernels\n(such as RBF kernel) as input neurons limits the ﬂexibility\nand capacity of learning by the deep architecture; 2) using\nSVM optimization as the learning objective for training the\ndeep architecture is computationally expensive. The pro-\nposed DEK tries to maximize the learning by ﬁrst learning\nan optimal high-level representation of data, followed by\nlearning a highly non-linear kernel, which is in turn based\non dimension-wise relationships of the high-level represen-\ntations. In other words, DEK forms the kernel based on\nmuch ﬁner granularity of relationship between data instead\nof starting with pre-deﬁned kernel functions on the whole\nset or subsets of dimensions of data. Furthermore, the learn-\ning objective of DEK can be evaluated online by each pair\nof data without the need of quadratic programming on at\nleast a batch of data.\nSimilarly, stacking SVMs to deepen the model architecture\nwas discussed in (Wiering & Schomaker, 2014). The authors\nof this work use different SVMs to extract latent features\nin different subsets of dimensions in the data. A global\nSVM is then used to aggregate all SVMs to form a ﬁnal\ndecision layer. However, because of computational expenses\nof SVMs, it is not practical to form a deep architecture by\nsimply stacking SVMs. Therefore, the extent to which\nthis type of stacking takes advantages of deep learning is\nrather limited. On the contrary, DEK can fully embrace the\nlearning power of deep learning, given that DEK itself is\na true deep architecture without any add-on restriction on\ndepths of the network. Instead of stacking SVMs, the work\nin (Tang, 2013) tried to improve generalization ability of\ndeep learning by using linear SVM classiﬁer at the top layer\nto deﬁne the learning objective. But this architecture strictly\nties with classiﬁcation tasks and training a SVM at the top\nlayer is still non-trivial as it requires quadratic programming\non a batch of data.\nThere were works computing similarity of data using deep\narchitectures on image data in (Zbontar & LeCun, 2015) and\n(Zagoruyko & Komodakis, 2015). However, their similarity\ncomputing is specialized on a particular task and unable\nto be generalized to other learning tasks. Moreover, their\noutput similarities do not necessarily possess the character\nof symmetricity, therefore cannot be used as kernels.\nGoogles FaceNet uses a cost function that is called triplet\nloss on facial identiﬁcation (Schroff et al., 2015). Each\nevaluation of triplet loss involves selecting three instances\nxi, x+\ni , x−\ni that satisﬁes the following criteria: xi is an an-\nchor point, x+\ni is another data point with the same class as\nxi, x−\ni is a data point with a different class than xi, and the\nfollowing inequality holds.\n∥xi −x+\ni ∥2\n2 > ∥xi −x−\ni ∥2\n2\n(1)\nThe deep network is then trying to learn an mapping f(·)\nsuch that\n∥f(xi) −f(x+\ni )∥2\n2 < ∥f(xi) −f(x−\ni )∥2\n2\n∀i\n(2)\nTherefore, the learning objective of the deep learning can\nbe expressed as minimize the following cost function:\nL =\nN\nX\ni=1\n(∥f(xi)−f(x+\ni )∥2\n2−∥f(xi)−f(x−\ni )∥2\n2+α) (3)\nwith α being a margin parameter. The Triplet Loss function\nwas extended to other identity detection tasks such as voice\nDeep Embedding Kernel\nrecognition (Bredin, 2017). An issue with triplet loss based\ncost function, according to (Hermans et al., 2017), is that\nthe training of the network requires a large training data\nthat contains a sufﬁcient amount of triplets that satisﬁes\nthe described criteria. In contrast, DEK can evaluate the\nlearning object online by using every pair from the training\ndata (though it is not necessary to use every pair if the\ntraining data is large enough). From another perspective,\nDEK may even be able to solve the ”Small Training Data”\nproblem by forming n2 training pairs from just n instances.\nLastly, we would like to mention transfer learning. In the\ncontext of deep learning, transfer learning aims to reuse a\ndeep network that is trained for one application to another\nrelevant task (Pan & Yang, 2010) and (Bengio, 2012). A\npopular way of doing transfer learning is to replace the\ndecision layer(s) of the trained deep network with a new one\nfor the new task. DEK can work as a general decision layers\nto be laid on top of a pre-trained network. Experimental\nresults demonstrate that DEK has better performance than\nMulti-layer Perceptron with triplet loss for being used as\nthe decision layers in transfer learning.\n3. Methodology\nThe goal of our methodology is to learn an optimized feature\nspace of data with desired features for the application. This\noptimized space is determined by DEK, a learnable kernel\nthat is represented by a deep architecture. When we design\nDEK, we consider the following factors. First, since it\nrepresents a kernel, DEK takes a pair of data instances as\ninput and output their similarity. Similarity of data can\nbe computed based on different representations of data at\ndifferent abstraction levels. We want the DEK to be able to\nlearn data similarity based on optimized data representations.\nThen based on the given data representation, we want the\nDEK to be able to learn a similarity function that is complex\nenough to map data to an optimized space with desired\ndata distributions. Therefore, DEK is designed to have\ntwo learning components, namely embedding network and\nkernel network, integrated in a uniﬁed deep architecture.\nThese two learning components will be trained using the\nsame learning objective in a single learning process. The\noverall architecture of DEK is shown in Figure 1.\n3.1. Kernel Network\nAs shown in Figure 1, the input of the kernel network is\ndenoted as U, which is formed by the outputs of the two\nbranches of the embedding network, which are oi and oj\nrespectively. More speciﬁcally, U can be expressed as the\nfollowing function of oi and oj\nU =\n\u001a oi1 ∗oj1, oi2 ∗oj2, . . . oid ∗ojd,\n|oi1 −oj1|, . . . |oid −ojd|\n\u001b\n(4)\nFigure 1. The Structure of DEK\nWhere oik denotes the kth dimension of oi, and d is the\ndimensionality of oi and oj . In other words, each neuron in\nthe input layer of the kernel network represents a symmetric\nrelationship of oi and oj on a single dimension. The use of\nﬁne granularity of relationship on each individual dimension\nas input provides more room for learning, compared with\ndirectly using different pre-deﬁned kernel functions on oi\nand oj as inputs. Essentially, this design of inputs allows the\nkernel network to learn a kernel that is a highly nonlinear\ncombination of angles and distances of the data pairs in the\nspace that is learned by the underneath embedding network.\nFurthermore, this design of inputs guarantees the output\nsimilarity is symmetric.\nThe output of the kernel network is the probability that\nsample i and j belong to the same class. Formally, given\nsample i and j, the output can be expressed as\nK(oi, oj) = P(y(i) = y(j)|x(i), x(j))\n= sigmoid(W (K)\nout · H(K)\nout + b(K)\nout )\n(5)\nWith W (K)\nout and b(K)\nout being the parameters of the output\nlayer, and H(K)\nout being the input, of the kernel network.\nObviously we have K(oi, oj) = K(oj, oi) > 0. Therefore,\nK(·) is a kernel function.\nTo train the kernel network (as well as the whole DEK), we\nlabel each pair in the following way.\n(\nY (i,j) = 1 ⇐⇒y(i) = y(j)\nY (i,j) = 0 ⇐⇒y(i) ̸= y(j)\n(6)\nThat is, if instance i and j belong to the same class, the label\nfor the pair of i and j is 1, otherwise it is 0. Then we deﬁne\nDeep Embedding Kernel\nthe learning objective of training DEK (including kernel\nnetwork) is to minimize the following cost function.\nL =\nX\ndata\n(Y (i,j) log(K(oi, oj)+\n(1 −Y (i,j)) log(1 −K(oi, oj)))\n(7)\n3.2. Embedding Network\nThe purpose of the embedding network is to learn optimized\nhigh-level representations of data to feed into the kernel\nnetwork as inputs. Let the mapping made by the embedding\nnetwork be E(·) then the high-level representation of sam-\nple x(i) can be represented as oi = E(x(i)). The goal of\ndesigning the embedding network is to increase the learning\ncapacity of the ﬁnal kernel. Experimental results demon-\nstrate that the embedding network positively contributes to\nthe performance of DEK.\nThe training of embedding network is in the same gradient\ndescent process using the same cost function as in Equation\n(7).\n3.3. Overall Design\nSuppose the embedding network has k1 hidden layers\nH(e)\n1\n. . . H(e)\nk1 and the kernel network has k2 hidden lay-\ners H(K)\n1\n. . . H(K)\nk2 . Also suppose the input layer of the\nembedding network is H(e)\n0\nand of the kernel network is\nH(K)\n0\n, and the weights and bias of layer i of network j are\nW (j)\ni\nand b(j)\ni . The computational ﬂow from a sample pair\n(x(i), x(j)) can be expressed as\n• The embedding of x(i):\nH(e)\n0 (i) = x(i)\nH(e)\n1 (i) = σ(W (e)\n0\n· H(e)\n0 (i) + b(e)\n0 )\n. . .\noi = H(e)\nk1 (i) = σ(W (e)\nk1−1H(e)\nk1−1(i) + b(e)\nk1−1)\n• The embedding of x(j):\nH(e)\n0 (j) = x(j)\nH(e)\n1 (j) = σ(W (e)\n0\n· H(e)\n0 (j) + b(e)\n0 )\n. . .\noj = H(e)\nk1 (j) = σ(W (e)\nk1−1H(e)\nk1−1(j) + b(e)\nk1−1)\n• Input to the kernel network:\nU = oi • oj = H(K)\n0\nH(K)\n1\n= σ(W (K)\n0\n· H(K)\n0\n+ b(K)\n0\n)\n...\nH(K)\nk2\n= σ(W (K)\nk2−1 · H(K)\nk2−1 + b(K)\nk2−1)\nK(x(i), x(j)) = s(W (K)\nk2\n· H(K)\nk2\n+ b(K)\nk2 )\nwith σ(·) being the activation function, s(·) being the output\nfunction, and • being the dimension-wise similarity operator\nas discussed:\noi • oj =\n\u001a oi1 ∗oj1, oi2 ∗oj2, . . . oid ∗ojd,\n|oi1 −oj1|, . . . |oid −ojd|\n\u001b\nLayers in both component network are updated with gradi-\nent descent:\nW (j)\ni\n←W (j)\ni\n−α\n∂L\n∂W (j)\ni\nb(j)\ni\n←b(j)\ni\n−α ∂L\n∂b(j)\ni\n(8)\nA uniﬁed structure is currently being employed on all layers\nto simplify the training process. In detail, all embedding\nlayers have k hidden neurons, and all kernel layers have 2k\nneurons, where k = αd with d being the dimensionality of\nthe original data and α being an integer factor (typically, we\nuse α ∈{1, 2, 3, 4}).\n4. DEK for Unstructured Data\nIf data is not in the form of structured records (such as\nimages, sequential data, or text), we can lay DEK on top of\nCNN, RNN, or other deep architectures that are suitable for\nthe given unstructured data to form a uniﬁed deep neural\nnetwork for supervised learning. The deep neural network\nwith DEK on top for both image data and sequential data\nare shown in Figure 2.\nIn this type of deep architecture, there are three integrated\nlearning components that will be trained in the same learn-\ning process. The ﬁrst is to learn an optimized vector em-\nbedding of the unstructured data; the second is to learn an\noptimized high-level embedding based on the bottom vector\nembedding; and the last is to learn a complex similarity func-\ntion based on the high-level embedding of the data. Again,\nall these components will be trained in the same learning\nprocess with the same learning objective.\nThe deep architecture, shown in 2, can be viewed as a frame-\nwork for transfer learning. In other words, the bottom com-\nponent of vector embedding can be replaced by a network\nthat is trained from data with similar natures.\nDeep Embedding Kernel\nFigure 2. The DEK Architecture for Unstructured Data\n5. DEK for Different Types of Supervised\nLearning\nIn this section, we describe different applications of DEK on\nsupervised learning including identity detection, classiﬁca-\ntion, regression, dimension reduction, and transfer learning.\nAll experiments for each of the above tasks are conducted in\nPython version 2.7.12. Deep models are implemented using\nthe package Theano (Bergstra et al., 2010), other machine\nlearning models (including the regular MLP) are from the\nSci-Kit Learn (Pedregosa et al., 2011) package. Visualiza-\ntions are generated using the Matplotlib library (Hunter,\n2007).\n5.1. Identity Detection\nThe problem of identity detection can be deﬁned as assign-\ning an identity to a query sample (e.g. a speech segment\nor a facial image). A common supervised learning strategy\nto solve this problem is to assign the identity to the query\nsample based on its nearest neighbors in the training set.\nIdentity detection with DEK feeds the query sample and\neach of the training sample into the trained deep network\nand ﬁnds the nearest neighbors of the query sample using\nthe outputted kernel values. In our experimental studies, we\napply DEK to both speaker identiﬁcation and facial recog-\nnition. Since both tasks are based on unstructured data (i.e.\nspeech segments and facial images), we use the extended\nDEK framework discussed in section 4. In other words,\nFigure 3. Accuracy Rates of Speaker Identiﬁcation Models\nwe lay DEK on top of deep architectures that are proper to\nunderneath unstructured data.\nSpeaker Identiﬁcation\nMost speaker identiﬁcation models work by ﬁrst extracting\nfeatures from the speech segments. We choose spectro-\ngrams as the feature set to be modeled in this task. In short,\nspectrograms are representations of audio segments in the\ntime/frequency space and have characteristics similar to im-\nages. Therefore, CNN is a proper deep architecture to model\nspectrograms. In other words, we lay DEK on the top of\nCNN to model the similarities of the speech segments.\nIn our experiment on speaker identiﬁcation, we use the Char-\nacterizing Individual Speakers (CHAINS) dataset - (Cum-\nmins et al., 2006). The data consists of speech segments\nfrom 36 persons in various speaking conditions. In our study,\nwe use only segments recorded in studio where the speakers\nread scripts in normal talking speed. In the preprocessing\nphase, we ﬁrst split the speech segments into syllables using\nsilent gaps; then pad each of them to be 1.5-second-long;\nand ﬁnally transform them into spectrograms. We compare\ntwo models to identify the speakers, one is a CNN using\nTriplet Loss cost function (CNN/TL), and the other is ex-\ntended DEK laid on top of the same CNN. The preprocessed\ndata is split into 75% training and 25% testing.\nThe accuracy rates of the two models by number of nearest\nneighbors from 1 to 55 is shown in Figure 3. It can be seen\nthat the DEK provides a signiﬁcant lift in accuracy rate (over\n2%) over the CNN/TL model.\nFacial Recognition\nWe study the performance of DEK on transfer learning\non facial recognition. The data we use is Indian Movie\nFace Database (IMFDB) – (Setty et al., 2013). This dataset\ncontains facial images of Indian movie actors and actresses.\nDeep Embedding Kernel\nFigure 4. Performance of Transfer Learning Models with DEK and\nMLP/TL on Facial Recognition\nWe build two transfer learning models based on a pretrained\nGoogle FaceNet (available from https://github.com/\ndavidsandberg/facenet). This version of FaceNet was\ntrained from about 500,000 facial images. We build two\ntransfer learning models based on the pretrained Facenet.\nOne model lays a Multi-layer Perceptron using Triplet Loss\n(MLP/TL) on top of the pretrained FaceNet, the other lays\nDEK on the pretrained Facenet. Both models are trained\nand tested on the same subsets from the IMFDB data (75%\ntraining, 25% testing). The trained MLP/TL outputs vector\nembedding based on which we can compute the pairwise\ndistances among images. The trained DEK outputs kernel\nvalues that can be interpreted as similarities.\nTo evaluate the two models, each image in the testing set is\nused as a query image to rank all images in the training set\nin the ascending order of their distances outputted by the\nMLP/TL model, and in the descending order of similarities\noutputted by DEK. We then plot the average precision-recall\ncurve for these two rankings. We also plot the precision-\nrecall curve generated by the pretrained FaceNet without\ntransfer learning as the baseline. As shown in Figure 4, both\ntransfer learning models make substantial improvements\nover the pretrained FaceNet. DEK makes further improve-\nment over MLP/TL at almost every recall level. Given\nMLP/TL has already achieved near-perfect precisions, the\nfurther improvement made by DEK is signiﬁcant. Therefore,\nDEK can be used as the desired solution to facial recogni-\ntion in critical applications where very high accuracy is\ndemanded.\nWe further study the contribution of the embedding net-\nwork of DEK towards the performance in this experiment.\nMore speciﬁcally, we build a transfer learning model by\nonly laying the kernel network component of DEK on the\ntop the pretrained FaceNet. We denote this model as DEK-\nEN. Both DEK and DEK-EN are trained independently on\nIMFDB. The precision-recall curves of both models are plot-\nted in Figure 5. It can be seen that the embedding network\nFigure 5. Contributions of Embedding Layers to DEK Perfor-\nmance\nof DEK contribute signiﬁcantly towards the performance.\nThis experimental result re-enforces our hypothesis that the\nincorporating of the embedding network in DEK increases\nthe learning capacity of the model.\n5.2. General Classiﬁcation\nThe learning objective of DEK (described in section 3.1)\nnaturally ﬁts into identity detection problems, in that the\ndesired similarity of two samples belonging to the same\nidentity is 1 and the desired similarity of two samples be-\nlonging to different identities is 0. However, for general\nclassiﬁcation problems, this learning objective may be over-\nstrict, given that two samples belong to the same class may\nnot necessarily have the same level of similarity as two be-\nlonging to the same identity. Therefore, to adapt DEK to\ngeneral classiﬁcation problems, a local pairing strategy is\nproposed and added to the learning process of DEK. In de-\ntails, we use local pairing strategy to generate training pairs\nat certain interval of iterations. For example, local pairing\nstrategy is applied to generate training pairs at the 1st, 51st,\n101st, 151st, ... iterations. Other iterations between the in-\nterval use the same training pairs generated most recently.\nThe local pairing strategy works as follows. First, all pairs\nof data are fed into DEK; each sample is used as reference to\nrank all other samples in descending order of kernel values\noutputted by DEK. A certain recall level (e.g., 0.1) is then\nused to determine the neighborhood of the reference sample.\nWithin the neighborhood, we form positive pairs between\nthe reference sample and the samples of the same class, and\nform negative pairs between the reference sample and the\nsamples of different classes. The local pairing strategy is\nillustrated in Figure 6. By using local pairing strategy, we\navoid to force the similarity of distant samples of the same\nclass to be close to 1.\nTo study the performance of DEK with local pairing strat-\negy on general classiﬁcation, we compare SVM using DEK\nDeep Embedding Kernel\nTable 1. Accuracy Rates of Models in Experiments for Classiﬁcation\nDATASET\nSVM/DEK\nKNN/DEK\nSVM/RBF\nGB\nRF\nMLP\nSEGMENT (ZHANG, 1992)\n0.9691\n0.9678\n0.9647\n0.9604\n0.9610\n0.9593\nCARDIOTOCOGRAPHY (AYRES-DE CAMPOS ET AL., 2000)\n0.9893\n0.9899\n0.9879\n0.9825\n0.9846\n0.9850\nMESSIDOR FEATURES (DECENCI`ERE ET AL., 2014)\n0.7803\n0.7746\n0.7543\n0.7110\n0.7168\n0.7222\nWAVEFORM (BREIMAN ET AL., 1984)\n0.8696\n0.8704\n0.8684\n0.8488\n0.8456\n0.8672\nPIMA DIABETE (SMITH ET AL., 1988)\n0.7839\n0.7865\n0.7708\n0.7396\n0.7604\n0.7630\nTable 2. R2 of Models in Experiments for Regression\nDATASET\nSVR/DEK\nKNN/DEK\nSVR/RBF\nGB\nRF\nMLP\nCONCRETE (YEH, 1998)\n0.8651\n0.8980\n0.8702\n0.9067\n0.8751\n0.8119\nAIRFOIL (BROOKS ET AL., 1989)\n0.8242\n0.9195\n0.8371\n0.8840\n0.9047\n0.8568\nENERGY EFFICIENCY (TSANAS & XIFARA, 2012)\n0.9685\n0.9783\n0.9621\n0.9775\n0.9756\n0.9470\nFigure 6. Illustration of the Local Pairing Strategy\n(SVM/DEK) and KNN using DEK (KNN/DEK) with other\nclassiﬁcation models including SVM using RBF kernel\n(SVM/RBF), Gradient Boosting Trees (GB) (Friedman,\n2002), Random Forest (RF) (Liaw et al., 2002), and MLP\non ﬁve datasets. The datasets are split into 50% training\ndata and 50% testing data. The hyper-parameters used by\nRBF kernel used by SVM are optimized via grid-search on\nthe trained dataset. Reported accuracy rates are computed\nin the testing set. Table 1 shows the test accuracy rates for\nall the models.\nAs can be seen, DEK-based SVM and DEK-based KNN\nachieve the best results in all datasets. The improvement\nis from 0.2% in the Cardiotocography data (comparing to\nSVM/RBF) to about 7% in the Messidor Features data (com-\nparing to GB).\n5.3. Regression\nUnlike identity detection or classiﬁcation models, determin-\ning the similarity of a sample pair in regression is not that\nobvious since the target value is now continuous. When\napplying DEK to regression, we model the similarity of\nsample pairs based on the similarity between their target\nvalues. In other words, let K′(·) be a similarity function\ndeﬁned on a pair of target values, then the network is trained\nso that K(x(i), x(j)) approximates K′(y(i), y(j)):\nK(x(i), x(j)) ≈K′(y(i), y(j))\n(9)\nWe deﬁne K′(y(i), y(j)) = exp (−γ|y(i) −y(j)|) with γ\nbeing a scale parameter. Accordingly, the output layer and\ncost function of the regression DEK are\nK(x(i), x(j)) = ReLU(W (K)\nk2\n· H(K)\nk2\n+ b(K)\nk2 )\n(10)\nL = 1\nN\nX\ni,j∈Data\n(K(x(i), x(j)) −K′(y(i), y(j)))2\n(11)\n(with ReLU(x) = max (0, x))\nTo study the performance of DEK on regression, we com-\npare Support Vector Regressor using DEK (SVR/DEK) and\nKNN using DEK (KNN/DEK) with other regression mod-\nels including SVR using RBF kernel (hyper-parameters\noptimized via grid-search) (SVR/RBF), Gradient Boosting\nTrees Regressor (GB), Random Forest Regressor (RF), and\nMLP Regressor, on three datasets. A ratio of 50% training\ndata and 50% testing data is also used. R2 is used as the\nmeasurement to compare the models. Table 2 shows the\nperformances of tested models in the regression task.\nAs can be seen, KNN/DEK achieves the best performance\nin two out of three datasets while being slightly behind the\nGB model in the Concrete dataset.\n5.4. Dimension Reduction\nAs being a kernel function, a trained DEK can be used\nwith kernel Principal Component Analysis (kPCA) to per-\nDeep Embedding Kernel\nFigure 7. Visualization of Four Datasets in 3D Space\nform dimension reduction on labeled data. We compare the\nperformance of dimension reduction by kPCA with DEK\nand kPCA with RBF kernel (hyper-parameter optimized by\ngrid-search with SVM) on two classiﬁcation datasets (Car-\ndiotocography and Waveform) and two regression datasets\n(Concrete and Airfoil). Figure 7 illustrates the four (testing)\nsets projected into 3D space by kPCA with a trained DEK\nand RBF kernel.\nWe can observe that, for the two classiﬁcation datasets,\nDEK maps the data to a space where classes (represented\nby nodes colors) better match the geographical clusters. For\nthe two regression datasets, the function patterns are clearer\nin the space mapped by DEK than RBF (the target values\nare represented by the shades of the nodes – darker nodes\nindicate higher target values).\n6. Conclusion\nIn this paper, we propose a new learnable kernel that is\ncalled DEK to automatically learn an optimized feature\nspace from training data. DEK is represented by a deep\nneural network that consists of two components: a deep\nembedding network and a deep kernel network. The inte-\ngration of these two components in a uniﬁed framework is\nto maximize the learning power of the deep architecture.\nThe deep embedding network is designed to learn high-level\nrepresentations; while the deep kernel network is designed\nto further learn non-linear similarities. Besides the learn-\ning capabilities presented by the embedding network and\nthe kernel network, DEK can also integrate deep architec-\ntures for embedding learning on unstructured data. DEK\ncan act as a general-purpose kernel function applicable in\nmost supervised learning tasks including identity detection,\nclassiﬁcation, regression, and dimension reduction. DEK\nalso achieves superior performance on transfer learning for\nfacial recognition compared with frequently used fully con-\nnected layers built on pretrained deep networks. We plan\nto contribute DEK as an open source package on GitHub to\npromote its usages on different application domains.\nReferences\nAyres-de Campos, Diogo, Bernardes, Joao, Garrido, An-\ntonio, Marques-de Sa, Joaquim, and Pereira-Leite, Luis.\nSisporto 2.0: a program for automated analysis of car-\ndiotocograms. Journal of Maternal-Fetal Medicine, 9(5):\n311–318, 2000.\nBengio, Yoshua. Deep learning of representations for unsu-\npervised and transfer learning. In Proceedings of ICML\nWorkshop on Unsupervised and Transfer Learning, pp.\n17–36, 2012.\nBergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lam-\nblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume,\nTurian, Joseph, Warde-Farley, David, and Bengio, Yoshua.\nTheano: A cpu and gpu math compiler in python. In Proc.\n9th Python in Science Conf, pp. 1–7, 2010.\nBredin, Herv´e. Tristounet: triplet loss for speaker turn\nembedding. In Acoustics, Speech and Signal Processing\nDeep Embedding Kernel\n(ICASSP), 2017 IEEE International Conference on, pp.\n5430–5434. IEEE, 2017.\nBreiman, Leo, Friedman, Jerome, Stone, Charles J, and\nOlshen, Richard A. Classiﬁcation and regression trees.\nCRC press, 1984.\nBrooks, Thomas F, Pope, D Stuart, and Marcolini,\nMichael A. Airfoil self-noise and prediction. 1989.\nCummins, Fred, Grimaldi, Marco, Leonard, Thomas, and\nSimko, Juraj. The chains corpus: Characterizing indi-\nvidual speakers. In Proc of SPECOM, volume 6, pp.\n431–435. Citeseer, 2006.\nDecenci`ere, Etienne, Zhang, Xiwei, Cazuguel, Guy, Lay,\nBruno, Cochener, B´eatrice, Trone, Caroline, Gain,\nPhilippe, Ordonez, Richard, Massin, Pascale, Erginay,\nAli, et al.\nFeedback on a publicly distributed image\ndatabase: the messidor database. Image Analysis & Stere-\nology, 33(3):231–234, 2014.\nFriedman, Jerome H. Stochastic gradient boosting. Com-\nputational Statistics & Data Analysis, 38(4):367–378,\n2002.\nHermans, Alexander, Beyer, Lucas, and Leibe, Bastian. In\ndefense of the triplet loss for person re-identiﬁcation.\narXiv preprint arXiv:1703.07737, 2017.\nHofmann, Thomas, Sch¨olkopf, Bernhard, and Smola,\nAlexander J. Kernel methods in machine learning. The\nannals of statistics, pp. 1171–1220, 2008.\nHunter, John D. Matplotlib: A 2d graphics environment.\nComputing in science & engineering, 9(3):90–95, 2007.\nJiu, Mingyuan and Sahbi, Hichem. Nonlinear deep kernel\nlearning for image annotation. IEEE Transactions on\nImage Processing, 26(4):1820–1832, 2017.\nJose, Cijo, Goyal, Prasoon, Aggrwal, Parv, and Varma,\nManik. Local deep kernel learning for efﬁcient non-linear\nsvm prediction. In International Conference on Machine\nLearning, pp. 486–494, 2013.\nLiaw, Andy, Wiener, Matthew, et al. Classiﬁcation and\nregression by randomforest. R news, 2(3):18–22, 2002.\nPan, Sinno Jialin and Yang, Qiang. A survey on transfer\nlearning. IEEE Transactions on knowledge and data\nengineering, 22(10):1345–1359, 2010.\nPedregosa, Fabian, Varoquaux, Ga¨el, Gramfort, Alexandre,\nMichel, Vincent, Thirion, Bertrand, Grisel, Olivier, Blon-\ndel, Mathieu, Prettenhofer, Peter, Weiss, Ron, Dubourg,\nVincent, et al. Scikit-learn: Machine learning in python.\nJournal of machine learning research, 12(Oct):2825–\n2830, 2011.\nSahbi, Hichem. Coarse-to-ﬁne deep kernel networks. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 1131–1139, 2017.\nSchmidhuber, J¨urgen. Deep learning in neural networks:\nAn overview. Neural networks, 61:85–117, 2015.\nSchroff, Florian, Kalenichenko, Dmitry, and Philbin, James.\nFacenet: A uniﬁed embedding for face recognition and\nclustering. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 815–823,\n2015.\nSetty, Shankar, Husain, Moula, Beham, Parisa, Gu-\ndavalli, Jyothi, Kandasamy, Menaka, Vaddi, Radhesyam,\nHemadri, Vidyagouri, Karure, JC, Raju, Raja, Rajan, B,\net al. Indian movie face database: a benchmark for face\nrecognition under wide variations. In Computer Vision,\nPattern Recognition, Image Processing and Graphics\n(NCVPRIPG), 2013 Fourth National Conference on, pp.\n1–5. IEEE, 2013.\nSmith, Jack W, Everhart, JE, Dickson, WC, Knowler, WC,\nand Johannes, RS. Using the adap learning algorithm to\nforecast the onset of diabetes mellitus. In Proceedings\nof the Annual Symposium on Computer Application in\nMedical Care, pp. 261. American Medical Informatics\nAssociation, 1988.\nStrobl, Eric V and Visweswaran, Shyam. Deep multiple\nkernel learning. In Machine Learning and Applications\n(ICMLA), 2013 12th International Conference on, vol-\nume 1, pp. 414–417. IEEE, 2013.\nTang, Yichuan. Deep learning using linear support vector\nmachines. arXiv preprint arXiv:1306.0239, 2013.\nTsanas, Athanasios and Xifara, Angeliki. Accurate quan-\ntitative estimation of energy performance of residential\nbuildings using statistical machine learning tools. Energy\nand Buildings, 49:560–567, 2012.\nVapnik, Vladimir Naumovich. An overview of statistical\nlearning theory. IEEE transactions on neural networks,\n10(5):988–999, 1999.\nWiering, Marco A and Schomaker, Lambert RB. Multi-layer\nsupport vector machines. Regularization, optimization,\nkernels, and support vector machines, pp. 457–476, 2014.\nYeh, I-C. Modeling of strength of high-performance con-\ncrete using artiﬁcial neural networks. Cement and Con-\ncrete research, 28(12):1797–1808, 1998.\nZagoruyko, Sergey and Komodakis, Nikos. Learning to\ncompare image patches via convolutional neural networks.\nIn Computer Vision and Pattern Recognition (CVPR),\n2015 IEEE Conference on, pp. 4353–4361. IEEE, 2015.\nDeep Embedding Kernel\nZbontar, Jure and LeCun, Yann. Computing the stereo\nmatching cost with a convolutional neural network. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 1592–1599, 2015.\nZhang, Jianping. Selecting typical instances in instance-\nbased learning. In Machine Learning Proceedings 1992,\npp. 470–479. Elsevier, 1992.\nZhuang, Jinfeng, Tsang, Ivor W, and Hoi, Steven CH. Two-\nlayer multiple kernel learning. In Proceedings of the\nFourteenth International Conference on Artiﬁcial Intelli-\ngence and Statistics, pp. 909–917, 2011.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-04-16",
  "updated": "2018-04-16"
}