{
  "id": "http://arxiv.org/abs/1804.06458v1",
  "title": "Deep Probabilistic Programming Languages: A Qualitative Study",
  "authors": [
    "Guillaume Baudart",
    "Martin Hirzel",
    "Louis Mandel"
  ],
  "abstract": "Deep probabilistic programming languages try to combine the advantages of\ndeep learning with those of probabilistic programming languages. If successful,\nthis would be a big step forward in machine learning and programming languages.\nUnfortunately, as of now, this new crop of languages is hard to use and\nunderstand. This paper addresses this problem directly by explaining deep\nprobabilistic programming languages and indirectly by characterizing their\ncurrent strengths and weaknesses.",
  "text": "Deep Probabilistic Programming Languages: A Qualitative Study\nGuillaume Baudart\nIBM Research\nguillaume.baudart@ibm.com\nMartin Hirzel\nIBM Research\nhirzel@us.ibm.com\nLouis Mandel\nIBM Research\nlmandel@us.ibm.com\nABSTRACT\nDeep probabilistic programming languages try to combine the ad-\nvantages of deep learning with those of probabilistic programming\nlanguages. If successful, this would be a big step forward in ma-\nchine learning and programming languages. Unfortunately, as of\nnow, this new crop of languages is hard to use and understand. This\npaper addresses this problem directly by explaining deep proba-\nbilistic programming languages and indirectly by characterizing\ntheir current strengths and weaknesses.\nCCS CONCEPTS\n• Theory of computation →Probabilistic computation;\n• Computing methodologies →Neural networks;\n• Software and its engineering →Domain specific languages;\nKEYWORDS\nDL, PPL, DSL\n1\nINTRODUCTION\nA deep probabilistic programming language (PPL) is a language\nfor specifying both deep neural networks and probabilistic models.\nIn other words, a deep PPL draws upon programming languages,\nBayesian statistics, and deep learning to ease the development of\npowerful machine-learning applications.\nFor decades, scientists have developed probabilistic models in\nvarious fields of exploration without the benefit of either dedicated\nprogramming languages or deep neural networks [12]. But since\nthese models involve Bayesian inference with often intractable\nintegrals, they sap the productivity of experts and are beyond the\nreach of non-experts. PPLs address this issue by letting users express\na probabilistic model as a program [15]. The program specifies how\nto generate output data by sampling latent probability distributions.\nThe compiler checks this program for type errors and translates it\nto a form suitable for an inference procedure, which uses observed\noutput data to fit the latent distributions. Probabilistic models show\ngreat promise: they overtly represent uncertainty [6] and they have\nbeen demonstrated to enable explainable machine learning even in\nthe important but difficult case of small training data [21, 26, 30].\nOver the last few years, machine learning with deep neural net-\nworks (deep learning, DL) has become enormously popular. This is\nbecause in several domains, DL solves what was previously a vexing\nproblem [10], namely manual feature engineering. Each layer of\na neural network can be viewed as learning increasingly higher-\nlevel features. In other words, the essence of DL is automatic hier-\narchical representation learning [22]. Hence, DL powered recent\nbreakthrough results in accurate supervised large-data tasks such\nas image recognition [20] and natural language translation [33].\nToday, most DL is based on frameworks that are well-supported,\nefficient, and expressive, such as TensorFlow [1] and PyTorch [11].\nThese frameworks provide automatic differentiation (users need not\nmanually calculate gradients for gradient descent), GPU support\n(to efficiently execute vectorized computations), and Python-based\nembedded domain-specific languages [18].\nDeep PPLs, which have emerged just recently [29–32], aim to\ncombine the benefits of PPLs and DL. Ideally, programs in deep\nPPLs would overtly represent uncertainty, yield explainable models,\nand require only a small amount of training data; be easy to write\nin a well-designed programming language; and match the break-\nthrough accuracy and fast training times of DL. Realizing all of\nthese promises would yield tremendous advantages. Unfortunately,\nthis is hard to achieve. Some of the strengths of PPLs and DL are\nseemingly at odds, such as explainability vs. automated feature\nengineering, or learning from small data vs. optimizing for large\ndata. Furthermore, the barrier to entry for work in deep PPLs is\nhigh, since it requires non-trivial background in fields as diverse\nas statistics, programming languages, and deep learning. To tackle\nthis problem, this paper characterizes deep PPLs, thus lowering the\nbarrier to entry, providing a programming-languages perspective\nearly when it can make a difference, and shining a light on gaps\nthat the community should try to address.\nThis paper uses the Stan PPL as a representative of the state of\nthe art in regular (not deep) PPLs [9]. Stan is a main-stream, mature,\nand widely-used PPL: it is maintained by a large group of developers,\nhas a yearly StanCon conference, and has an active forum. Stan is\nTuring complete and has its own stand-alone syntax and semantics,\nbut provides bindings for several languages including Python.\nMost importantly, this paper uses Edward [31] and Pyro [32] as\nrepresentatives of the state of the art in deep PPLs. Edward is based\non TensorFlow and Pyro is based on PyTorch. Edward was first\nreleased in mid-2016 and has a single main maintainer, who is fo-\ncusing on a new version. Pyro is a much newer framework (released\nlate 2017), but seems to be very responsive to community questions.\nThis paper characterizes deep PPLs by explaining them (Sec-\ntions 2, 3, and 4), comparing them to each other and to regular\nPPLs and DL frameworks (Section 5), and envisioning next steps\n(Section 6). Additionally, the paper serves as a comparative tutorial\nto both Edward and Pyro. To this end, it presents examples of in-\ncreasing complexity written in both languages, using deliberately\nuniform terminology and presentation style. By writing this paper,\nwe hope to help the research community contribute to the exciting\nnew field of deep PPLs, and ultimately, combine the strengths of\nboth DL and PPLs.\n2\nPROBABILISTIC MODEL EXAMPLE\nThis section explains PPLs using an example that is probabilistic\nbut not deep. The example, adapted from Section 9.1 of [3], is about\nlearning the bias of a coin. We picked this example because it is\nsimple, lets us introduce basic concepts, and shows how different\nPPLs represent these concepts.\narXiv:1804.06458v1  [cs.AI]  17 Apr 2018\nθ\nxi\nN\nFigure 1: Graphical model for biased coin tosses. Circles rep-\nresent random variables. The white circle forθ indicates that\nit is latent and the gray circle for xi indicates that it is ob-\nserved. The arrow represents dependency. The rounded rec-\ntangle is a plate, representing N distributions that are IID.\nWe write xi = 1 if the result of the ith coin toss is head and\nxi = 0 if it is tail. We assume that individual coin tosses are inde-\npendent and identically distributed (IID) and that each toss follows\na Bernoulli distribution with parameter θ: p(xi = 1 | θ) = θ and\np(xi = 0 | θ) = 1 −θ. The latent (i.e., unobserved) variable θ is the\nbias of the coin. The task is to infer θ given the results of previously\nobserved coin tosses, that is, p(θ | x1,x2, . . . ,xN ). Figure 1 shows\nthe corresponding graphical model. The model is generative: once\nthe distribution of the latent variable has been inferred, one can\ndraw samples to generate data points similar to the observed data.\nWe now present this simple example in Stan, Edward, and Pyro.\nIn all these languages we follow a Bayesian approach: the program-\nmer first defines a probabilistic model of the problem. Assumptions\nare encoded with prior distributions over the variables of the model.\nThen the programmer launches an inference procedure to automat-\nically compute the posterior distributions of the parameters of the\nmodel based on observed data. In other words, inference adjusts\nthe prior distribution using the observed data to give a more pre-\ncise model. Compared to other machine-learning models such as\ndeep neural networks, the result of a probabilistic program is a\nprobability distribution, which allows the programmer to explicitly\nvisualize and manipulate the uncertainty associated with a result.\nThis overt uncertainty is an advantage of PPLs. Furthermore, a\nprobabilistic model has the advantage that it directly describes the\ncorresponding world based on the programmer’s knowledge. Such\ndescriptive models are more explainable than deep neural networks,\nwhose representation is big and does not overtly resemble the world\nthey model.\nFigure 2(a) solves the biased-coin task in Stan, a well-established\nPPL [9]. This example uses PyStan, the Python interface for Stan.\nLines 2-13 contain code in Stan syntax in a multi-line Python string.\nThe data block introduces observed random variables, which are\nplaceholders for concrete data to be provided as input to the in-\nference procedure, whereas the parameters block introduces latent\nrandom variables, which will not be provided and must be inferred.\nLine 4 declares x as a vector of ten discrete random variables, con-\nstrained to only take on values from a finite set, in this case, either\n0 or 1. Line 7 declares θ as a continuous random variable, which can\ntake on values from an infinite set, in this case, real numbers be-\ntween 0 and 1. Stan uses the tilde operator (~) for sampling. Line 10\nsamples θ from a uniform distribution (same probability for all val-\nues) between 0 and 1. Since θ is a latent variable, this distribution is\nmerely a prior belief, which inference will replace by a posterior dis-\ntribution. Line 12 samples the xi from a Bernoulli distribution with\nparameter θ. Since the xi are observed variables, this sampling is\nreally a check for how well the model corresponds to data provided\nat inference time. One can think of sampling an observed variable\nlike an assertion in verification [15]. Line 15 specifies the data and\nLines 17-18 run the inference using the model and the data. By\ndefault, Stan uses a form of Monte-Carlo sampling for inference [9].\nLines 20-22 extract and print the mean and standard deviation of\nthe posterior distribution of θ.\nFigure 2(b) solves the same task in Edward [31]. Line 2 samples θ\nfrom the prior distribution, and Line 3 samples a vector of random\nvariables from a Bernoulli distribution of parameter θ, one for each\ncoin toss. Line 5 specifies the data. Lines 7-8 define a placeholder\nthat will be used by the inference procedure to compute the poste-\nrior distribution of θ. The shape and size of the placeholder depends\non the inference procedure. Here we use the Hamiltonian Monte-\nCarlo inference HMC, the posterior distribution is thus computed\nbased on a set of random samples and follows an empirical dis-\ntribution. The size of the placeholder corresponds to the number\nof random samples computed during inference. Lines 9-11 launch\nthe inference. The inference takes as parameter the prior:posterior\npair theta:qtheta and links the data to the variable x. Lines 13-16\nextract and print the mean and standard deviation of the posterior\ndistribution of θ.\nFigure 2(c) solves the same task in Pyro [32]. Lines 2-7 define\nthe model as a function coin. Lines 3-5 sample θ from the prior\ndistribution, and Lines 6-7 sample a vector of random variable x\nfrom a Bernoulli distribution of parameter θ. Pyro stores random\nvariables in a dictionary keyed by the first argument of the func-\ntion pyro.sample. Lines 9-10 define the data as a dictionary. Line 12\nconditions the model using the data by matching the value of the\ndata dictionary with the random variables defined in the model.\nLines 13-15 apply inference to this conditioned model, using im-\nportance sampling. Compared to Stan and Edward, we first define\na conditioned model with the observed data before running the\ninference instead of passing the data as an argument to the infer-\nence. The inference returns a probabilistic model, post, that can be\nsampled to extract the mean and standard deviation of the posterior\ndistribution of θ in Lines 17-19.\nThe deep PPLs Edward and Pyro are built on top of two popular\ndeep learning frameworks TensorFlow [1] and PyTorch [11]. They\nbenefit from efficient computations over large datasets, automatic\ndifferentiation, and optimizers provided by these frameworks that\ncan be used to efficiently implement inference procedures. As we\nwill see in the next sections, this design choice also reduces the gap\nbetween DL and probabilistic models, allowing the programmer\nto combine the two. On the other hand, this choice leads to piling\nup abstractions (Edward/TensorFlow/Numpy/Python or Pyro/Py-\nTorch/Numpy/Python) that can complicate the code. We defer a\ndiscussion of these towers of abstraction to Section 5.\nVariational Inference\nInference, for Bayesian models, computes the posterior distribu-\ntion of the latent parameters θ given a set of observations x, that\nis, p(θ | x). For complex models, computing the exact posterior\ndistribution can be costly or even intractable. Variational inference\nturns the inference problem into an optimization problem and tends\n2\n1 # Model\n2 coin_code = \"\"\"\n3\ndata {\n4\nint<lower=0,upper=1> x[10];\n5\n}\n6\nparameters {\n7\nreal<lower=0,upper=1> theta;\n8\n}\n9\nmodel {\n10\ntheta ~ uniform(0,1);\n11\nfor (i in 1:10)\n12\nx[i] ~ bernoulli(theta);\n13\n}\"\"\"\n14 # Data\n15 data = {'x': [0, 1, 0, 0, 0, 0, 0, 0, 0, 1]}\n16 # Inference\n17 fit = pystan.stan(model_code=coin_code,\n18\ndata=data, iter=1000)\n19 # Results\n20 samples = fit.extract()['theta']\n21 print(\"Posterior mean:\", np.mean(samples))\n22 print(\"Posterior stddev:\", np.std(samples))\n1 # Model\n2 theta = Uniform(0.0, 1.0)\n3 x = Bernoulli(probs=theta, sample_shape=10)\n4 # Data\n5 data = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n6 # Inference\n7 qtheta = Empirical(\n8\ntf.Variable(tf.ones(1000) ∗0.5))\n9 inference = ed.HMC({theta: qtheta},\n10\ndata={x: data})\n11 inference.run()\n12 # Results\n13 mean, stddev = ed.get_session().run(\n14\n[qtheta.mean(),qtheta.stddev()])\n15 print(\"Posterior mean:\", mean)\n16 print(\"Posterior stddev:\", stddev)\n1 # Model\n2 def coin():\n3\ntheta = pyro.sample(\"theta\", Uniform(\n4\nVariable(torch.Tensor([0])),\n5\nVariable(torch.Tensor([1])))\n6\npyro.sample(\"x\", Bernoulli(\n7\ntheta ∗Variable(torch.ones(10)))\n8 # Data\n9 data = {\"x\": Variable(torch.Tensor(\n10\n[0, 1, 0, 0, 0, 0, 0, 0, 0, 1]))}\n11 # Inference\n12 cond = pyro.condition(coin, data=data)\n13 sampler = pyro.infer.Importance(cond,\n14\nnum_samples=1000)\n15 post = pyro.infer.Marginal(sampler, sites=[\"theta\"])\n16 # Result\n17 samples = [post()[\"theta\"].data[0] for _ in range(1000)]\n18 print(\"Posterior mean:\", np.mean(samples))\n19 print(\"Posterior stddev:\", np.std(samples))\n(a) Stan\n(b) Edward\n(c) Pyro\nFigure 2: Probabilistic model for learning the bias of a coin.\n1 # Inference Guide\n2 qalpha = tf.Variable(1.0)\n3 qbeta = tf.Variable(1.0)\n4 qtheta = Beta(qalpha, qbeta)\n5 # Inference\n6 inference = ed.KLqp({theta: qtheta}, {x: data})\n7 inference.run()\n1 # Inference Guide\n2 def guide():\n3\nqalpha = pyro.param(\"qalpha\", Variable(torch.Tensor([1.0]), requires_grad=True))\n4\nqbeta = pyro.param(\"qbeta\", Variable(torch.Tensor([1.0]), requires_grad=True))\n5\npyro.sample(\"theta\", Beta(qalpha, qbeta))\n6 # Inference\n7 svi = SVI(cond, guide, Adam({}), loss=\"ELBO\", num_particles=7)\n8 for step in range(1000):\n9\nsvi.step()\n(a) Edward\n(b) Pyro\nFigure 3: Variational Inference for learning the bias of a coin.\nto be faster and more adapted to large datasets than sampling-based\nMonte-Carlo methods [4].\nVariational infence tries to find the member q∗(θ) of a family Q\nof simpler distribution over the latent variables that is the closest\nto the true posterior p(θ | x). The fitness of a candidate is measured\nusing the Kullback-Leibler (KL) divergence from the true posterior,\na similarity measure between probability distributions.\nq∗(θ) = argmin\nq(θ)∈Q\nKL(q(θ) || p(θ | x))\nIt is up to the programmer to choose a family of candidates, or\nguides, that is sufficiently expressive to capture a close approxima-\ntion of the true posterior, but simple enough to make the optimiza-\ntion problem tractable.\nBoth Edward and Pyro support variational inference. Figure 3\nshows how to adapt Figure 2 to use it. In Edward (Figure 3(a)), the\nprogrammer defines the family of guides by changing the shape\nof the placeholder used in the inference. Lines 2-4 use a beta dis-\ntribution with unknown parameters α and β that will be com-\nputed during inference. Lines 6-7 do variational inference using the\nKullback-Leibler divergence. In Pyro (Figure 3(b)), this is done by\ndefining a guide function. Lines 2-5 also define a beta distribution\nwith parameters α and β. Lines 7-9 do inference using Stochastic\nVariational Inference, an optimized algorithm for variational infer-\nence. Both Edward and Pyro rely on the underlying framework to\nsolve the optimization problem. Probabilistic inference thus closely\nfollows the scheme used for training procedures of DL models.\nThis section gave a high-level introduction to PPLs and intro-\nduced basic concepts (generative models, sampling, prior and pos-\nterior, latent and observed, discrete and continuous). Next, we turn\nour attention to deep learning.\n3\nh\nw00\n*\nw01\n*\nb0\n+\nw10\n*\nw11\n*\nb1\n+\nw20\n*\nw21\n*\nb2\n+\nReLU\nReLU\nReLU\nw00\n*\nw01\n*\nb0\n+\nw02\n*\nw10 w11\n*\nb1\n+\nw12\n*\ny1\n*\ninput layer\n(nx = 2)\nhidden layer\n(nh = 3)\noutput layer\n(ny = 2)\nh\nh\nh\nh\nh\nh\nh\nh\ny\ny\ny\ny\ny\ny\ny\ny\nsoftmax\nl\np0\np1\nargmax\nh0\nh1\nh2\nx0\nx1\ny0\nMLP\n(a) Computational graph for non-probabilistic MLP.\nq\nl\nN\nx\np\nMLP\n(b) Graphical model for probabilistic MLP.\nFigure 4: Multi-layer perceptron (MLP) for classifying\nimages. Circles and squares are probabilistic and non-\nprobabilistic variables. Black rectangles are pure functions.\nArrows represent dependencies and forward data flow.\n3\nPROBABILISTIC MODELS IN DL\nThis section explains DL using an example of a deep neural network\nand shows how to make that probabilistic. The task is multiclass\nclassification: given input features x, e.g., an image of a handwritten\ndigit [23] comprising nx = 28 · 28 pixels, predict a label l, e.g., one\nof ny = 10 digits. Before we explain how to solve this task using DL,\nlet us clarify some terminology. In cases where DL uses different\nterminology from PPLs, this paper favors the PPL terminology. So\nwe say inference for what DL calls training; predicting for what DL\ncalls inferencing; and observed variables for what DL calls train-\ning data. For instance, the observed variables for inference in the\nclassifier task are the image x and label l.\nAmong the many neural network architectures suitable for this\ntask, we chose a simple one: a multi-layer perceptron (MLP [28]).\nWe start with the non-probabilistic version. Figure 4(a) shows an\nMLP with a 2-feature input layer, a 3-feature hidden layer, and a\n2-feature output layer; of course, this generalizes to wider (more\nfeatures) and deeper (more layers) MLPs. From left to right, there is\na fully-connected subnet where each input feature xi contributes to\neach hidden feature hj, multiplied with a weight wji and offset with\na bias bj. The weights and biases are latent variables. Treating the\ninput, biases, and weights as vectors and a matrix lets us cast this\nsubnet in linear algebra xW h + bh, which can run efficiently via\nvector instructions on CPUs or GPUs. Next, a rectified linear unit\nReLU(z) = max(0,z) computes the hidden feature vector h. The\nReLU lets the MLP discriminate input spaces that are not linearly\nseparable. The hidden layer exhibits both the advantage and dis-\nadvantage of deep learning: automatically learned features that\nneed not be hand-engineered but would require non-trivial reverse\nengineering to explain in real-world terms. Next, another fully-\nconnected subnet hW y + by computes the output layer y. Then,\nthe softmax computes a vector π of scores that add up to one. The\nhigher the value of πl, the more strongly the classifier predicts\nlabel l. Using the output of the MLP, the argmax extracts the label l\nwith highest score.\nTraditional methods to train such a neural network incrementally\nupdate the latent parameters of the network to optimize a loss func-\ntion via gradient descent [28]. In the case of hand-written digits, the\nloss function is a distance metrics between observed and predicted\nlabels. The variables and computations are non-probabilistic, in\nthat they use concrete values rather than probability distributions.\nDeep PPLs can express Bayesian neural networks with probabilis-\ntic weights and biases [5]. One way to visualize this is by replacing\nrectangles with circles for latent variables in Figure 4(a) to indicate\nthat they hold probability distributions. Figure 4(b) shows the cor-\nresponding graphical model, where the latent variable θ denotes\nall the parameters of the MLP: W h, bh, W y, by.\nBayesian inference starts from prior beliefs about the parameters\nand learns distributions that fit observed data (such as, images and\nlabels). We can then sample concrete weights and biases to obtain\na concrete MLP. In fact, we do not need to stop at a single MLP:\nwe can sample an ensemble of as many MLPs as we like. Then,\nwe can feed a concrete image to all the sampled MLPs to get their\npredictions, followed by a vote.\nFigure 5(a) shows the probabilistic MLP example in Edward.\nLines 3-4 are placeholders for observed variables (i.e., batches of\nimagesx and labelsl). Lines 5-9 defines the MLP parameterized byθ,\na dictionary containing all the network parameters. Lines 10-14\nsample the parameters from the prior distributions. Line 15 defines\nthe output of the network: a categorical distribution over all possible\nlabel values parameterized by the output of the MLP. Line 17-23\ndefine the guides for the latent variables, initialized with random\nnumbers. Later, the variational inference will update these during\noptimization, so they will ultimately hold an approximation of\nthe posterior distribution after inference. Lines 25-29 set up the\ninference with one prior:posterior pair for each parameter of the\nnetwork and link the output of the network to the observed data.\nFigure 5(b) shows the same example in Pyro. Lines 2-11 contain\nthe basic neural network, where torch.nn.Linear wraps the low-level\nlinear algebra. Lines 3-6 declare the structure of the net, that is, the\ntype and dimension of each layer. Lines 7-10 combine the layers\nto define the network. It is possible to use equivalent high-level\nTensorFlow APIs for this in Edward as well, but we refrained from\ndoing so to illustrate the transition of the parameters to random\nvariables. Lines 14-20 define the model. Lines 15-18 sample priors\nfor the parameters, associating them with object properties created\nby torch.nn.Linear (i.e., the weight and bias of each layer). Line 19\nlifts the MLP definition from concrete to probabilistic. We thus\nobtain a MLP where all parameters are treated as random variables.\nLine 20 conditions the model using a categorical distribution over\nall possible label values. Lines 26-31 define the guide for the latent\nvariables, initialized with random numbers, just like in the Edward\nversion. Line 33 sets up the inference.\nAfter the inference, Figure 6 shows how to use the posterior\ndistribution of the MLP parameters to classify unknown data. In\nEdward (Figure 6(a)), Lines 2-4 draw several samples of the param-\neters from the posterior distribution. Then, Lines 5-6 execute the\n4\n1 batch_size, nx, nh, ny = 128, 28 ∗28, 1024, 10\n2 # Model\n3 x = tf.placeholder(tf.float32, [batch_size, nx])\n4 l = tf.placeholder(tf.int32, [batch_size])\n5 def mlp(theta, x):\n6\nh = tf.nn.relu(tf.matmul(x, theta[\"Wh\"]) + theta[\"bh\"])\n7\nyhat = tf.matmul(h, theta[\"Wy\"]) + theta[\"by\"]\n8\nlog_pi = tf.nn.log_softmax(yhat)\n9\nreturn log_pi\n10 theta = {\n11\n'Wh': Normal(loc=tf.zeros([nx, nh]), scale=tf.ones([nx, nh])),\n12\n'bh': Normal(loc=tf.zeros(nh), scale=tf.ones(nh)),\n13\n'Wy': Normal(loc=tf.zeros([nh, ny]), scale=tf.ones([nh, ny])),\n14\n'by': Normal(loc=tf.zeros(ny), scale=tf.ones(ny)) }\n15 lhat = Categorical(logits=mlp(theta, x))\n16 # Inference Guide\n17 def vr(∗shape):\n18\nreturn tf.Variable(tf.random_normal(shape))\n19 qtheta = {\n20\n'Wh': Normal(loc=vr(nx, nh), scale=tf.nn.softplus(vr(nx, nh))),\n21\n'bh': Normal(loc=vr(nh), scale=tf.nn.softplus(vr(nh))),\n22\n'Wy': Normal(loc=vr(nh, ny), scale=tf.nn.softplus(vr(nh, ny))),\n23\n'by': Normal(loc=vr(ny), scale=tf.nn.softplus(vr(ny))) }\n24 # Inference\n25 inference = ed.KLqp({ theta[\"Wh\"]: qtheta[\"Wh\"],\n26\ntheta[\"bh\"]: qtheta[\"bh\"],\n27\ntheta[\"Wy\"]: qtheta[\"Wy\"],\n28\ntheta[\"by\"]: qtheta[\"by\"] },\n29\ndata={lhat: l})\n1 # Model\n2 class MLP(nn.Module):\n3\ndef __init__(self):\n4\nsuper(MLP, self).__init__()\n5\nself.lh = torch.nn.Linear(nx, nh)\n6\nself.ly = torch.nn.Linear(nh, ny)\n7\ndef forward(self, x):\n8\nh = F.relu(self.lh(x.view((−1, nx))))\n9\nlog_pi = F.log_softmax(self.ly(h), dim=−1)\n10\nreturn log_pi\n11 mlp = MLP()\n12 def v0s(∗shape): return Variable(torch.zeros(∗shape))\n13 def v1s(∗shape): return Variable(torch.ones(∗shape))\n14 def model(x, l):\n15\ntheta = { 'lh.weight': Normal(v0s(nh, nx), v1s(nh, nx)),\n16\n'lh.bias': Normal(v0s(nh), v1s(nh)),\n17\n'ly.weight': Normal(v0s(ny, nh), v1s(ny, nh)),\n18\n'ly.bias': Normal(v0s(ny), v1s(ny)) }\n19\nlifted_mlp = pyro.random_module(\"mlp\", mlp, theta)()\n20\npyro.observe(\"obs\", Categorical(logits=lifted_mlp(x)), one_hot(l))\n21 # Inference Guide\n22 def vr(name, ∗shape):\n23\nreturn pyro.param(name,\n24\nVariable(torch.randn(∗shape), requires_grad=True))\n25 def guide(x, l):\n26\nqtheta = {\n27\n'lh.weight': Normal(vr(\"Wh_m\", nh, nx), F.softplus(vr(\"Wh_s\", nh, nx))),\n28\n'lh.bias': Normal(vr(\"bh_m\", nh), F.softplus(vr(\"bh_s\", nh))),\n29\n'ly.weight': Normal(vr(\"Wy_m\", ny, nh), F.softplus(vr(\"Wy_s\", ny, nh))),\n30\n'ly.bias': Normal(vr(\"by_m\", ny), F.softplus(vr(\"by_s\", ny))) }\n31\nreturn pyro.random_module(\"mlp\", mlp, qtheta)()\n32 # Inference\n33 inference = SVI(model, guide, Adam({\"lr\": 0.01}), loss=\"ELBO\")\n(a) Edward\n(b) Pyro\nFigure 5: Probabilistic multilayer perceptron for classifying images.\n1 def predict(x):\n2\ntheta_samples = [ { \"Wh\": qtheta[\"Wh\"].sample(), \"bh\": qtheta[\"bh\"].sample(),\n3\n\"Wy\": qtheta[\"Wy\"].sample(), \"by\": qtheta[\"by\"].sample() }\n4\nfor _ in range(args.num_samples) ]\n5\nyhats = [ mlp(theta_samp, x).eval()\n6\nfor theta_samp in theta_samples ]\n7\nmean = np.mean(yhats, axis=0)\n8\nreturn np.argmax(mean, axis=1)\n1 def predict(x):\n2\nsampled_models = [ guide(None)\n3\nfor _ in range (args.num_samples) ]\n4\nyhats = [ model(Variable(x)).data\n5\nfor model in sampled_models ]\n6\nmean = torch.mean(torch.stack(yhats), 0)\n7\nreturn np.argmax(mean, axis=1)\n(a) Edward\n(b) Pyro\nFigure 6: Predictions by the probabilistic multilayer perceptron.\nMLP with each concrete model. Line 7 computes the score of a label\nas the average of the scores returned by the MLPs. Finally, Line 8\npredicts the label with the highest score. In Pyro (Figure 6(b)), the\nprediction is done similarly but we obtain multiple versions of the\nMLP by sampling the guide (Line 2-3), not the parameters.\nThis section showed how to use probabilistic variables as build-\ning blocks for a DL model. Compared to non-probabilistic DL, this\napproach has the advantage of reduced overfitting and accurately\nquantified uncertainty [5]. On the other hand, this approach re-\nquires inference techniques, like variational inference, that are\nmore advanced than classic back-propagation. The next section\nwill present the dual approach, showing how to use neural net-\nworks as building blocks for a probabilistic model.\n5\n4\nDL IN PROBABILISTIC MODELS\nThis section explains how deep PPLs can use non-probabilistic\ndeep neural networks as components in probabilistic models. The\nexample task is learning a vector-space representation. Such a rep-\nresentation reduces the number of input dimensions to make other\nmachine-learning tasks more manageable by counter-acting the\ncurse of dimensionality [10]. The observed random variable is x, for\ninstance, an image of a hand-written digit with nx = 28 · 28 pixels.\nThe latent random variable is z, the vector-space representation,\nfor instance, with nz = 4 features. Learning a vector-space repre-\nsentation is an unsupervised problem, requiring only images but\nno labels. While not too useful on its own, such a representation is\nan essential building block. For instance, it can help in other image\ngeneration tasks, e.g., to generate an image for a given writing\nstyle [30]. Furthermore, it can help learning with small data, e.g.,\nvia a K-nearest neighbors approach in vector space [2].\nEach image x depends on the latent representation z in a com-\nplex non-linear way (i.e., via a deep neural network). The task is to\nlearn this dependency between x and z. The top half of Figure 7(a)\nshows the corresponding graphical model. The output of the neural\nnetwork, named decoder, is a vector µ that parameterizes a Bernoulli\ndistribution over each pixel in the image x. Each pixel is thus asso-\nciated to a probability of being present in the image. Similarly to\nFigure 4(b) the parameter θ of the decoder is global (i.e., shared by\nall data points) and is thus drawn outside the plate. Compared to\nSection 3 the network here is not probabilistic, hence the square\naround θ.\nThe main idea of the VAE [19, 27] is to use variational inference to\nlearn the latent representation. As for the examples presented in the\nprevious sections, we need to define a guide for the inference. The\nguide maps each x to a latent variable z via another neural network.\nThe bottom half of Figure 7(a) shows the graphical model of the\nguide. The network, named encoder, returns, for each image x, the\nparameters µz and σz of a Gaussian distribution in the latent space.\nAgain the parameter ϕ of the network is global and not probabilistic.\nThen inference tries to learn good values for parameter θ and ϕ,\nsimultaneously training the decoder and the encoder, according to\nthe data and the prior beliefs on the latent variables (e.g., Gaussian\ndistribution).\nAfter the inference, we can generate a latent representation of an\nimage with the encoder and reconstruct the image with the decoder.\nThe similarity of the two images gives an indication of the success\nof the inference. The model and the guide together can thus be seen\nas an autoencoder, hence the term variational autoencoder.\nFigure 7(b) shows the VAE examples in Edward. Lines 4-12 define\nthe decoder: a simple 2-layers neural network similar to the one\npresented in Section 3. The parameter θ is initialized with random\nnoise. Line 13 samples the priors for the latent variable z from a\nGaussian distribution. Lines 14-15 define the dependency betweenx\nand z, as a Bernoulli distribution parameterized by the output of\nthe decoder. Lines 17-29 define the encoder: a neural network with\none hidden layer and two distinct output layers for µz and σz.\nThe parameter ϕ is also initialized with random noise. Lines 30-31\ndefine the inference guide for the latent variable, that is, a Gaussian\ndistribution parameterized by the outputs of the encoder. Line 33\nsets up the inference matching the prior:posterior pair for the latent\nvariable and linking the data with the output of the decoder.\nFigure 7(c) shows the VAE example in Pyro. Lines 2-12 define\nthe decoder. Lines 13-19 define the model. Lines 14-16 sample the\npriors for the latent variable z. Lines 18-19 define the dependency\nbetween x and z via the decoder. In contrast to Figure 5(b), the de-\ncoder is not probabilistic, so there is no need for lifting the network.\nLines 34-37 define the guide as in Edward linking z and x via the\ndecoder defined Lines 21-33. Line 39 sets up the inference.\nThis example illustrates that we can embed non-probabilistic DL\nmodels inside probabilistic programs and learn the parameters of\nthe DL models during inference. Sections 2, 3, and 4 were about\nexplaining deep PPLs with examples. The next section is about\ncomparing deep PPLs with each other and with their potential.\n5\nCHARACTERIZATION\nThis section attempts to answer the following research question:\nAt this point in time, how well do deep PPLs live up to their poten-\ntial? Deep PPLs combine probabilistic models, deep learning, and\nprogramming languages in an effort to combine their advantages.\nThis section explores those advantages grouped by pedigree and\nuses them to characterize Edward and Pyro.\nBefore we dive in, some disclaimers are in order. First, both\nEdward and Pyro are young, not mature projects with years of\nimprovements based on user experiences, and they enable new\napplications that are still under active research. We should keep\nthis in mind when we criticize them. On the positive side, early\ncriticism can be a positive influence. Second, since getting even\na limited number of example programs to support direct side-by-\nside comparisons was non-trivial, we kept our study qualitative.\nA respectable quantitative study would require more programs\nand data sets. On the positive side, all of the code shown in this\npaper actually runs. Third, doing an outside-in study risks missing\nsubtleties that the designers of Edward and Pyro may be more\nexpert in. On the positive side, the outside-in view resembles what\nnew users experience.\n5.1\nAdvantages from Probabilistic Models\nProbabilistic models support overt uncertainty: they give not just a\nprediction but also a meaningful probability. This is useful to avoid\nuncertainty bugs [6], track compounding effects of uncertainty,\nand even make better exploration decisions in reinforcement learn-\ning [5]. Both Edward and Pyro support overt uncertainty well, see\ne.g. the lines under the comment “# Results” in Figure 2.\nProbabilistic models give users a choice of inference procedures:\nthe user has the flexibility to pick and configure different approaches.\nDeep PPLs support two primary families of inference procedures:\nthose based on Monte-Carlo sampling and those based on varia-\ntional inference. Edward supports both and furthermore flexible\ncompositions, where different inference procedures are applied to\ndifferent parts of the model. Pyro supports primarily variational\ninference and focuses less on Monte-Carlo sampling. In comparison,\nStan makes a form of Monte-Carlo sampling the default, focusing\non making it easy-to-tune in practice [8].\nProbabilistic models can help with small data: even when in-\nference uses only small amount of labeled data, there have been\n6\nx\nN\nz\nµ\ndecoder\nx\nN\nz\nµz\nencoder\nsz\nf\nq\nmodel pq (x | z)\nguide qf (z | x)\n(a) Graphical models.\n1 batch_size, nx, nh, nz = 256, 28 ∗28, 1024, 4\n2 def vr(∗shape): return tf.Variable(0.01 ∗tf.random_normal(shape))\n3 # Model\n4 X = tf.placeholder(tf.int32, [batch_size, nx])\n5 def decoder(theta, z):\n6\nhidden = tf.nn.relu(tf.matmul(z, theta['Wh']) + theta['bh'])\n7\nmu = tf.matmul(hidden, theta['Wy']) + theta['by']\n8\nreturn mu\n9 theta = { 'Wh': vr(nz, nh),\n10\n'bh': vr(nh),\n11\n'Wy': vr(nh, nx),\n12\n'by': vr(nx) }\n13 z = Normal(loc=tf.zeros([batch_size, nz]), scale=tf.ones([batch_size, nz]))\n14 logits = decoder(theta, z)\n15 x = Bernoulli(logits=logits)\n16 # Inference Guide\n17 def encoder(phi, x):\n18\nx = tf.cast(x, tf.float32)\n19\nhidden = tf.nn.relu(tf.matmul(x, phi['Wh']) + phi['bh'])\n20\nz_mu = tf.matmul(hidden, phi['Wy_mu']) + phi['by_mu']\n21\nz_sigma = tf.nn.softplus(\n22\ntf.matmul(hidden, phi['Wy_sigma']) + phi['by_sigma'])\n23\nreturn z_mu, z_sigma\n24 phi = { 'Wh': vr(nx, nh),\n25\n'bh': vr(nh),\n26\n'Wy_mu': vr(nh, nz),\n27\n'by_mu': vr(nz),\n28\n'Wy_sigma': vr(nh, nz),\n29\n'by_sigma': vr(nz) }\n30 loc, scale = encoder(phi, X)\n31 qz = Normal(loc=loc, scale=scale)\n32 # Inference\n33 inference = ed.KLqp({z: qz}, data={x: X})\n1 # Model\n2 class Decoder(nn.Module):\n3\ndef __init__(self):\n4\nsuper(Decoder, self).__init__()\n5\nself.lh = nn.Linear(nz, nh)\n6\nself.lx = nn.Linear(nh, nx)\n7\nself.relu = nn.ReLU()\n8\ndef forward(self, z):\n9\nhidden = self.relu(self.lh(z))\n10\nmu = self.lx(hidden)\n11\nreturn mu\n12 decoder = Decoder()\n13 def model(x):\n14\nz_mu = Variable(torch.zeros(x.size(0), nz))\n15\nz_sigma = Variable(torch.ones(x.size(0), nz))\n16\nz = pyro.sample(\"z\", dist.Normal(z_mu, z_sigma))\n17\npyro.module(\"decoder\", decoder)\n18\nmu = decoder.forward(z)\n19\npyro.sample(\"xhat\", dist.Bernoulli(mu), obs=x.view(−1, nx))\n20 # Inference Guide\n21 class Encoder(nn.Module):\n22\ndef __init__(self):\n23\nsuper(Encoder, self).__init__()\n24\nself.lh = torch.nn.Linear(nx, nh)\n25\nself.lz_mu = torch.nn.Linear(nh, nz)\n26\nself.lz_sigma = torch.nn.Linear(nh, nz)\n27\nself.softplus = nn.Softplus()\n28\ndef forward(self, x):\n29\nhidden = F.relu(self.lh(x.view((−1, nx))))\n30\nz_mu = self.lz_mu(hidden)\n31\nz_sigma = self.softplus(self.lz_sigma(hidden))\n32\nreturn z_mu, z_sigma\n33 encoder = Encoder()\n34 def guide(x):\n35\npyro.module(\"encoder\", encoder)\n36\nz_mu, z_sigma = encoder.forward(x)\n37\npyro.sample(\"z\", dist.Normal(z_mu, z_sigma))\n38 # Inference\n39 inference = SVI(model, guide, Adam({\"lr\": 0.01}), loss=\"ELBO\")\n(b) Edward\n(c) Pyro\nFigure 7: Variational autoencoder for encoding and decoding images.\nhigh-profile cases where probabilistic models still make accurate\npredictions [21]. Working with small data is useful to avoid the cost\nof hand-labeling, to improve privacy, to build personalized mod-\nels, and to do well on underrepresented corners of a big-data task.\nThe intuition for how probabilistic models help is that they can\nmake up for lacking labeled data for a task by domain knowledge\nincorporated in the model, by unlabeled data, or by labeled data\nfor other tasks. There are some promising initial successes of using\ndeep probabilistic programming on small data [26, 30]; at the same\ntime, this remains an active area of research.\nProbabilistic models can support explainability: when the com-\nponents of a probabilistic model correspond directly to concepts\nof a real-world domain being modeled, predictions can include an\nexplanation in terms of those concepts. Explainability is useful\nwhen predictions are consulted for high-stakes decisions, as well\nas for transparency around bias [7]. Unfortunately, the parameters\n7\nof a deep neural network are just as opaque with as without proba-\nbilistic programming. There is cause for hope though. For instance,\nSiddharth et al. advocate disentangled representations that help\nexplainability [30]. Overall, the jury is still out on the extent to\nwhich deep PPLs can leverage this advantage from PPLs.\n5.2\nAdvantages from Deep Learning\nDeep learning is automatic hierarchical representation learning [22]:\neach unit in a deep neural network can be viewed as an automati-\ncally learned feature. Learning features automatically is useful to\navoid the cost of engineering features by hand. Fortunately, this DL\nadvantage remains true in the context of a deep PPL. In fact, a deep\nPPL makes the trade-off between automated and hand-crafted fea-\ntures more flexible than most other machine-learning approaches.\nDeep learning can accomplish high accuracy: for various tasks,\nthere have been high-profile cases where deep neural networks\nbeat out earlier approaches with years of research behind them.\nArguably, the victory of DL at the ImageNet competition in 2012\nushered in the latest craze around DL [20]. Record-breaking ac-\ncuracy is useful not just for publicity but also to cross thresholds\nwhere practical deployments become desirable, for instance, in\nmachine translation [33]. Since a deep PPL can use deep neural\nnetworks, in principle, it inherits this advantage from DL [31].\nHowever, even non-probabilistic DL requires tuning, and in our\nexperience with the example programs in this paper, the tuning\nburden is exacerbated with variational inference.\nDeep learning supports fast inference: even for large models and\na large data set, the wall-clock time for a batch job to infer pos-\nteriors is short. The fast-inference advantage is the result of the\nback-propagation algorithm [28], novel techniques for paralleliza-\ntion [25] and data representation [17], and massive investments in\nthe efficiency of DL frameworks such as TensorFlow and PyTorch,\nwith vectorization on CPU and GPU. Fast inference is useful for it-\nerating more quickly on ideas, trying more hyperparameter during,\nand wasting fewer resources. Tran et al. measured the efficiency of\nthe Edward deep PPL, demonstrating that it does benefit from the\nefficiency advantage of the underlying DL framework [31].\n5.3\nAdvantages from Programming Languages\nProgramming language design is essential for composability: bigger\nmodels can be composed from smaller components. Composabil-\nity is useful for testing, team-work, and reuse. Conceptually, both\ngraphical probabilistic models and deep neural networks compose\nwell. On the other hand, some PPLs impose structure in a way that\nreduces composability; fortunately, this can be mitigated [16]. Both\nEdward and Pyro are embedded in Python, and, as our example\nprograms demonstrate, work with Python functions and classes.\nFor instance, users are not limited to declaring all latent variables\nin one place; instead, they can compose models, such as MLPs, with\nseparately declared latent variables. Edward and Pyro also work\nwith higher-level DL framework modules such as tf.layers.dense or\ntorch.nn.Linear, and Pyro even supports automatically lifting those\nto make them probabilistic. Edward and Pyro also do not prevent\nusers from composing probabilistic models with non-probabilistic\ncode, but doing so requires care. For instance, when Monte-Carlo\nsampling runs the same code multiple times, it is up to the pro-\ngrammer to watch out for unwanted side-effects. One area where\nmore work is needed is the extensibility of Edward or Pyro itself [8].\nFinally, in addition to composing models, Edward also emphasizes\ncomposing inference procedures.\nNot all PPLs have the same expressiveness: some are Turing com-\nplete, others not [15]. For instance, BUGS is not Turing complete,\nbut has nevertheless been highly successful [13]. The field of deep\nprobabilistic programming is too young to judge which levels of\nexpressiveness are how useful. Edward and Pyro are both Turing\ncomplete. However, Edward makes it harder to express while-loops\nand conditionals than Pyro. Since Edward builds on TensorFlow,\nthe user must use special APIs to incorporate dynamic control con-\nstructs into the computational graph. In contrast, since Pyro builds\non PyTorch, it can use native Python control constructs, one of the\nadvantages of dynamic DL frameworks [24].\nProgramming language design affects conciseness: it determines\nwhether a model can be expressed in few lines of code. Conciseness\nis useful to make models easier to write and, when used in good\ntaste, easier to read. In our code examples, Edward is more concise\nthan Pyro. Pyro arguably trades conciseness for structure, making\nheavier use of Python classes and functions. Wrapping the model\nand guide into functions allows compiling them into co-routines,\nan ingredient for implementing efficient inference procedures [14].\nIn both Edward and Pyro, conciseness is hampered by the Bayesian\nrequirement for explicit priors and by the variational-inference\nrequirement for explicit guides.\nProgramming languages can offer watertight abstractions: they\ncan abstract away lower-level concepts and prevent them from\nleaking out, for instance, using types and constraints [8]. Consider\nthe expression xW [1] + b[1] from Section 3. At face value, this looks\nlike eager arithmetic on concrete scalars, running just once in the\nforward direction. But actually, it may be lazy (building a computa-\ntional graph) arithmetic on probability distributions (not concrete\nvalues) of tensors (not scalars), running several times (for different\nMonte-Carlo samples or data batches), possibly in the backward di-\nrection (for back-propagation of gradients). Abstractions are useful\nto reduce the cognitive burden on the programmer, but only if they\nare watertight. Unfortunately, abstractions in deep PPLs are leaky.\nOur code examples directly invoke features from several layers of\nthe technology stack (Edward or Pyro, on TensorFlow or PyTorch,\non NumPy, on Python). Furthermore, we found that error mes-\nsages rarely refer to source code constructs. For instance, names of\nPython variables are erased from the computational graph, making\nit hard to debug tensor dimensions, a common cause for mistakes.\nIt does not have to be that way. For instance, DL frameworks are\ngood at hiding the abstraction of back-propagation. More work is\nrequired to make deep PPL abstractions more watertight.\n6\nCONCLUSION AND OUTLOOK\nThis paper is a study of two deep PPLs, Edward and Pyro. The study\nis qualitative and driven by code examples. This paper explains how\nto solve common tasks, contributing side-by-side comparisons of\nEdward and Pyro. The potential of deep PPLs is to combine the ad-\nvantages of probabilistic models, deep learning, and programming\nlanguages. In addition to comparing Edward and Pyro to each other,\n8\nthis paper also compares them to that potential. A quantitative\nstudy is left to future work. Based on our experience, we confirm\nthat Edward and Pyro combine three advantages out-of-the-box:\nthe overt uncertainty of probabilistic models; the hierarchical rep-\nresentation learning of DL; and the composability of programming\nlanguages.\nFollowing are possible next steps in deep PPL research.\n• Choice of inference procedures: Especially Pyro should support\nMonte-Carlo methods at the same level as variational inference.\n• Small data: While possible in theory, this has yet to be demon-\nstrated on Edward and Pyro, with interesting data sets.\n• High accuracy: Edward and Pyro need to be improved to simplify\nthe tuning required to improve model accuracy.\n• Expressiveness: While Turing complete in theory, Edward should\nadopt recent dynamic TensorFlow features for usability.\n• Conciseness: Both Edward and Pyro would benefit from reducing\nthe repetitive idioms of priors and guides.\n• Watertight abstractions: Both Edward and Pyro fall short on this\ngoal, necessitating more careful language design.\n• Explainability: This is inherently hard with deep PPLs, necessi-\ntating more machine-learning innovation.\nIn summary, deep PPLs show great promises and remain an active\nfield with many research opportunities.\nREFERENCES\n[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\nDean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Man-\njunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,\nBenoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan\nYu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Machine\nLearning. In Operating Systems Design and Implementation (OSDI). 265–283. https:\n//www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi\n[2] Petr Babkin, Md. Faisal Mahbub Chowdhury, Alfio Gliozzo, Martin Hirzel, and\nAvraham Shinnar. 2017. Bootstrapping Chatbots for Novel Domains. In Workshop\nat NIPS on Learning with Limited Labeled Data (LLD).\nhttps://lld-workshop.\ngithub.io/papers/LLD_2017_paper_10.pdf\n[3] David Barber. 2012. Bayesian Reasoning and Machine Learning. Cambridge\nUniversity Press. http://www.cs.ucl.ac.uk/staff/d.barber/brml/\n[4] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. 2017. Variational inference:\nA review for statisticians. J. Amer. Statist. Assoc. 112, 518 (2017), 859–877. https:\n//arxiv.org/abs/1601.00670\n[5] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015.\nWeight Uncertainty in Neural Network. In International Conference on Machine\nLearning (ICML). 1613–1622. http://proceedings.mlr.press/v37/blundell15.html\n[6] James Bornholt, Todd Mytkowicz, and Kathryn S. McKinley. 2014. Uncertain<T>:\nA First-order Type for Uncertain Data. In Conference on Architectural Support\nfor Programming Languages and Operating Systems (ASPLOS). 51–66.\nhttps:\n//doi.org/10.1145/2541940.2541958\n[7] Flavio P. Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Nate-\nsan\nRamamurthy,\nand\nKush\nR.\nVarshney.\n2017.\nOptimized\nPre-\nProcessing\nfor\nDiscrimination\nPrevention.\nIn\nNeural\nInformation\nProcessing\nSystems\n(NIPS).\n3995–4004.\nhttp://papers.nips.cc/paper/\n6988-optimized-pre-processing-for-discrimination-prevention.pdf\n[8] Bob Carpenter. 2017. Hello, world! Stan, PyMC3, and Edward. (2017). http://\nandrewgelman.com/2017/05/31/compare-stan-pymc3-edward-hello-world/ (Re-\ntrieved February 2018).\n[9] Bob Carpenter, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich,\nMichael Betancourt, Michael A. Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell.\n2017. Stan: A probabilistic programming language. Journal of Statistical Software\n76, 1 (2017), 1–37. https://www.jstatsoft.org/article/view/v076i01\n[10] Pedro Domingos. 2012. A Few Useful Things to Know About Machine Learning.\nCommunications of the ACM (CACM) 55, 10 (Oct. 2012), 78–87. https://doi.org/\n10.1145/2347736.2347755\n[11] Facebook. 2016. PyTorch. (2016). http://pytorch.org/ (Retrieved February 2018).\n[12] Zoubin Ghahramani. 2015. Probabilistic machine learning and artificial intelli-\ngence. Nature 521, 7553 (May 2015), 452–459. https://www.nature.com/articles/\nnature14541\n[13] W. R. Gilks, A. Thomas, and D. J. Spiegelhalter. 1994. A Language and Program\nfor Complex Bayesian Modelling. The Statistician 43, 1 (Jan. 1994), 169–177.\n[14] Noah D. Goodman and Andreas Stuhlmüller. 2014. The Design and Implementa-\ntion of Probabilistic Programming Languages. (2014). http://dippl.org (Retrieved\nFebruary 2018).\n[15] Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sriram K. Rajamani.\n2014. Probabilistic Programming. In ICSE track on Future of Software Engineering\n(FOSE). 167–181. https://doi.org/10.1145/2593882.2593900\n[16] Maria I. Gorinova, Andrew D. Gordon, and Charles Sutton. 2018. SlicStan: Im-\nproving Probabilistic Programming using Information Flow Analysis. In Work-\nshop on Probabilistic Programming Languages, Semantics, and Systems (PPS).\nhttps://pps2018.soic.indiana.edu/files/2017/12/SlicStanPPS.pdf\n[17] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.\n2015. Deep learning with limited numerical precision. In International Confer-\nence on Machine Learning (ICML). 1737–1746. http://proceedings.mlr.press/v37/\ngupta15.pdf\n[18] Paul Hudak. 1998. Modular domain specific languages and tools. In International\nConference on Software Reuse (ICSR). 134–142. https://doi.org/10.1109/ICSR.1998.\n685738\n[19] Diederik P. Kingma and Max Welling. 2013. Auto-encoding variational Bayes.\n(2013). https://arxiv.org/abs/1312.6114\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012.\nIma-\ngeNet Classification with Deep Convolutional Networks. In Advances in\nNeural Information Processing Systems (NIPS).\nhttp://papers.nips.cc/paper/\n4824-imagenet-classification-with-deep-convolutional-neural-networks\n[21] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. 2015. Human-\nlevel concept learning through probabilistic program induction. Science 350 (Dec.\n2015), 1332–1338. Issue 6266. http://science.sciencemag.org/content/350/6266/\n1332\n[22] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature\n521, 7553 (May 2015), 436–444. https://www.nature.com/articles/nature14539\n[23] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. 1998. The MNIST\nDatabase of Handwritten Digits. (1998).\nhttp://yann.lecun.com/exdb/mnist/\n(Retrieved February 2018).\n[24] Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar,\nAntonios Anastasopoulos, Miguel Ballesteros, David Chiang, Daniel Clothiaux,\nTrevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng\nJi, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul\nMichel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta,\nand Pengcheng Yin. 2017. DyNet: The Dynamic Neural Network Toolkit. (2017).\n[25] Feng Niu, Benjamin Recht, Christopher Ré, and Stephen J. Wright.\n2011.\nHogwild:\nA\nLock-Free\nApproach\nto\nParallelizing\nStochas-\ntic\nGradient\nDescent.\nIn\nConference\non\nNeural\nInformation\nPro-\ncessing\nSystems\n(NIPS).\n693–701.\nhttp://papers.nips.cc/paper/\n4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent\n[26] Danilo J. Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan\nWierstra. 2016. One-shot Generalization in Deep Generative Models. In Interna-\ntional Conference on Machine Learning (ICML). 1521–1529. http://proceedings.\nmlr.press/v48/rezende16.html\n[27] Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Back-\npropagation and Approximate Inference in Deep Generative Models. In Interna-\ntional Conference on Machine Learning (ICML). 1278–1286. http://proceedings.\nmlr.press/v32/rezende14.html\n[28] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning\nrepresentations by back-propagating errors. Nature 323 (Oct. 1986), 533–536.\nhttps://doi.org/doi:10.1038/323533a0\n[29] John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. Probabilis-\ntic programming in Python using PyMC3. PeerJ Computer Science 2 (April 2016),\ne55. https://doi.org/10.7717/peerj-cs.55\n[30] N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmai-\nson, Noah D. Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr.\n2017.\nLearning Disentangled Representations with Semi-Supervised\nDeep\nGenerative\nModels.\nIn\nAdvances\nin\nNeural\nInformation\nPro-\ncessing\nSystems\n(NIPS).\n5927–5937.\nhttp://papers.nips.cc/paper/\n7174-learning-disentangled-representations-with-semi-supervised-deep\\\n-generative-models.pdf\n[31] Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin Murphy,\nand David M. Blei. 2017. Deep Probabilistic Programming. In International\nConference on Learning Representations (ICLR). https://arxiv.org/abs/1701.03757\n[32] Uber. 2017. Pyro. (2017). http://pyro.ai/ (Retrieved February 2018).\n[33] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian,\nNishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,\nOriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s\nneural machine translation system: Bridging the gap between human and ma-\nchine translation. (2016). https://arxiv.org/abs/1609.08144\n9\n",
  "categories": [
    "cs.AI",
    "cs.PL"
  ],
  "published": "2018-04-17",
  "updated": "2018-04-17"
}