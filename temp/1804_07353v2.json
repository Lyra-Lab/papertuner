{
  "id": "http://arxiv.org/abs/1804.07353v2",
  "title": "Unsupervised Representation Adversarial Learning Network: from Reconstruction to Generation",
  "authors": [
    "Yuqian Zhou",
    "Kuangxiao Gu",
    "Thomas Huang"
  ],
  "abstract": "A good representation for arbitrarily complicated data should have the\ncapability of semantic generation, clustering and reconstruction. Previous\nresearch has already achieved impressive performance on either one. This paper\naims at learning a disentangled representation effective for all of them in an\nunsupervised way. To achieve all the three tasks together, we learn the forward\nand inverse mapping between data and representation on the basis of a symmetric\nadversarial process. In theory, we minimize the upper bound of the two\nconditional entropy loss between the latent variables and the observations\ntogether to achieve the cycle consistency. The newly proposed RepGAN is tested\non MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised\nclassification, generation and reconstruction tasks. The result demonstrates\nthat RepGAN is able to learn a useful and competitive representation. To the\nauthor's knowledge, our work is the first one to achieve both a high\nunsupervised classification accuracy and low reconstruction error on MNIST.\nCodes are available at https://github.com/yzhouas/RepGAN-tensorflow.",
  "text": "Unsupervised Representation Adversarial Learning Network: from\nReconstruction to Generation\nYuqian Zhou*, Kuangxiao Gu*, Thomas Huang\nIFP, Beckman Institute, UIUC\n{yuqian2, kgu3}@illinois.edu, huang@ifp.uiuc.edu\nAbstract— A good representation for arbitrarily complicated\ndata should have the capability of semantic generation, cluster-\ning and reconstruction. Previous research has already achieved\nimpressive performance on either one. This paper aims at\nlearning a disentangled representation effective for all of them\nin an unsupervised way. To achieve all the three tasks together,\nwe learn the forward and inverse mapping between data and\nrepresentation on the basis of a symmetric adversarial process.\nIn theory, we minimize the upper bound of the two conditional\nentropy loss between the latent variables and the observations\ntogether to achieve the cycle consistency. The newly proposed\nRepGAN is tested on MNIST, fashionMNIST, CelebA, and\nSVHN datasets to perform unsupervised classiﬁcation, gener-\nation and reconstruction tasks. The result demonstrates that\nRepGAN is able to learn a useful and competitive representa-\ntion. To the author’s knowledge, our work is the ﬁrst one to\nachieve both a high unsupervised classiﬁcation accuracy and\nlow reconstruction error on MNIST. Codes are available at\nhttps://github.com/yzhouas/RepGAN-tensorﬂow.\nI. INTRODUCTION\nLearning a good representation from complex data distri-\nbution can be resolved by deep directed generative models.\nAmong them, Generative Adversarial Network (GAN)[1] is\nproposed to generate complicated data space by sampling\nfrom a simple pre-deﬁned latent space. Speciﬁcally, a gener-\nator is modeled to map the latent samples to real data, and\na discriminator is applied to differentiate real samples from\ngenerated ones. However, the original GAN only learns the\nforward mapping from a entangled latent space to data space.\nGiven the complicated data, it lacks the inverse inference\nnetwork to map the data back to the interpretable latent space.\nEfforts have been put on learning the bidirectional mapping\nin an adversarial way. InfoGAN [2] is proposed to address\nthe problem of uninformative latent space of GAN, by\ndisentangling the latent variables, and maximizing the mutual\ninformation between a subset of the variables and the\nobservations. InfoGAN is able to learn a representation with\nsemantic meaning in a fully unsupervised way. However,\nfaithful reconstruction cannot be achieved by InfoGAN.\nAnother model named Adversarial Autoencoder (AAE)[3]\nperforms variational inference by matching the aggregated\nposterior distribution with the prior distribution using an\nadversarial loss. The autoencoder-like structure guarantees a\ngood reconstruction performance, but the generation using\nthe sampled latent variables is not faithful enough. BiGAN[4]\nand ALI[5] both propose an encoder (inference network)\n*Authors have equal contribution\nFig. 1.\nStructure of the proposed Representation GAN (RepGAN). The\nlatent representation is split to a categorical slot c, continuous slot s, and\nnoise n. RepGAN jointly trains two types of structure, which are x-Rep-x\nand Rep-x-Rep, to emphasis on the cycle consistency. Thus the bijection\ncan be achieved between the latent and data space.\nand decoder (generative network), and seek to match the\njoint distributions of latent variables and data from the two\nnetworks. However, the objective functions do not constraint\non the relationship between the latent variables and the\nobservations, which results in an unsatisﬁed reconstruction\nperformance. ALICE [6] resolves this non-identiﬁability\nissues by additionally optimizing the conditional entropy.\nBut it does not learn a disentangled latent space for semantic\ninterpretation and knowledge discovery.\nBi-directional mapping is also addressed in some applica-\ntions like image domain transformation or image semantic\nediting. In BiCycleGAN[7], the authors differentiated two\nmodels cVAE-GAN and cLR-GAN and explained the hybrid\nmodel in an intuitive way (regarding to real or fake sampling).\nIt does not encode interpretable information into the latent\nvector, but directly concatenates the vector with the images\nfrom another domain. crVAE [8] could only demonstrate the\nsemantic meaning of latent vector from visual inspection.\nIAN[9] proposed a hybridization of VAE and GAN to\nsolve the semantic photo editing problem by improving the\nrepresentation capability of latent space without increasing its\ndimensionality. The decoder of VAE is used as generator of\nGAN, and hidden layer outputs of discriminator are used to\nquantify reconstruction loss, which was showed to improve\nthe reconstruction quality. However no cycle consistency was\nenforced and the latent space was not disentangled. DTN[10]\napplied a similar structure for image domain transfer, while\narXiv:1804.07353v2  [cs.CV]  6 Apr 2019\nthe latent space is not constrained to a regularized distribution,\nthus random generation tasks were not performed.\nIn this paper, we seek to learn a generic interpretable\nrepresentation and bidirectional network which is capable\nof reconstruction, generation and clustering at the same\ntime. A model supporting all these capabilities is important\nfor data analysis and transmission. Reconstruction ability\nwill help data compression while transmitting. Clustering\nand generation ability will beneﬁt the natural analysis of\ncomplicated data without human prior knowledge.\nwe ﬁrst perform a theoretical analysis of two types of\nstructures, which are x-Rep-x and Rep-x-Rep. We identify\ntheir advantages and disadvantages respectively by studying\nthe loss functions they try to minimize, and relate it to the\nmutual information and conditional entropy in the information\ntheory [11]. Then we propose a novel model involving the\nconcept of cycle consistency [12], [13], [14] to combine\nthose two structures, which is able to achieve a better overall\nperformance in terms of unsupervised classiﬁcation accuracy,\ndata reconstruction and generation by learning a useful generic\ndisentangled latent space. Finally, we show and analyze the\neffectiveness of this new model on the image datasets like\nMNIST, FashionMNIST, CelebA and SVHN.\nII. RELATED WORK\nIn this section, we review the objectives of the two general\nstructures utilized by representation learning, which are x-\nRep-x and Rep-x-Rep, as shown in ﬁgure 1. Here we denote\nthe parameter of the generator (from z to x) as θ, and that\nof the encoder (from x to z) as φ.\nA. x-Rep-x Structure\nThe x-Rep-x structure is well known as autoencoder, and\nthe most popular instance is Variational Autoencoder (VAE)\n[15], [16], [17]. Recall that in the VAE, the variational lower\nbound it optimizes is,\nL(θ, φ; x) = KL(qφ(z|x)||pθ(z)) −Eqφ(z|x)[log pθ(x|z)]\n(1)\nThe ﬁrst term is identiﬁed as the regularization term which\ntries to match qφ(z|x), the posterior of z conditional on x,\nto a target distribution pθ(z) using the KL divergence. The\nsecond term represents the reconstruction loss, namely given\nthe data x, generating the latent representation z, and then\nusing this z to reconstruct the data. Notice that the loss term\nabove is for a speciﬁc data point x. To get the loss inside a\ntraining batch, we need to average it over x, namely\nLV AE = Epdata(x)[L(θ, φ; x)]\n= Epdata(x)[KL(qφ(z|x)||pθ(z))]\n−Epdata(x)[Eqφ(z|x)[log pθ(x|z)]]\n(2)\nAdversarial Autoencoder[3] is a related work of the VAE.\nThe loss function of AAE is similar to VAE, except for the\nregularization term KL(qφ(z|x)||pθ(z)) is replaced by an\nadversarial learning process (represented by JS Divergence) on\nthe aggregated posterior distribution. Therefore, the objective\nfunction for AAE becomes,\nLAAE = JS(qφ(z)||pθ(z)) −Eqφ(z,x)[log pθ(x|z)]\n(3)\nFig. 2.\nMapping relations of x-Rep-x ,Rep-x-Rep and proposed RepGAN.\nCycle consistency is involved for a deterministic bijection better for both\nreconstruction and generation.\nInfoVAE[11] generalizes the regularization term of AAE\nto a divergence family, and justify the richer information\nit provides in the latent code sampled from the aggregated\nposterior distribution. The author also proved that the latent\nspace learned by InfoVAE (or the variants AAE) does not\nsuffer from exploding problem and uninformative latent mod-\neling. However, unsupervised generation with disentangled\nlatent vector was not reported in the original VAE, AAE or\nInfoVAE paper.\nB. Rep-x-Rep Structure\nThe Rep-x-Rep structure is derived from the vanilla GAN\n[1] , by making the discriminator output not only the real\nor fake label, but also the semantic representation [2], [18],\n[19], [20].\nOne example of this structure utilized for unsupervised\nlearning is InfoGAN (ﬁgure 1), which basically adds an\ninformation maximizing term on top of the vanilla GAN [1] so\nthat the generator is forced to use all the information contained\nin the input when generating sample data points. In the\noriginal InfoGAN paper, the latent vector is disentangled into\ncategorical, continuous and noise parts, and the discriminator\nwill output the categorical and continuous parts to achieve\nthe mutual information maximization. The loss function of\nsuch Rep-x-Rep structure can be written as,\nLInfoGAN =JS(pθ(x)||pdata(x))\n−Epθ(z)[Epθ(x|z)[log qφ(z|x)]\n(4)\nInfoGAN achieves a fully unsupervised representation\nlearning with disentangled semantic latent space. Both the\ngeneration and clustering performance is impressive, but the\nreconstruction quality of input images was not satisﬁed to\nreport.\nIII. REPGAN\nIn this section, we derive the correlation between the\nloss function of previous models and conditional entropy,\nand compare their inﬂuence on the learned representation.\nThen we integrate the loss and propose a novel model called\nRepresentation GAN to learn a useful disentangled latent\nspace.\nA. Conditional Entropy\nThe conditional entropy measures the uncertainty of one\nrandom variable given the other. For example, H(X|Z)\nquantiﬁes the uncertainty of the observation space X given\nthe latent space Z, which can be formulated as,\nH(X|Z) = −Epdata(x)[Eqφ(z|x)[log qφ(x|z)]]\n= −Epdata(x)[Eqφ(z|x)[log pθ(x|z)]]\n−Ep(z)[KL(qφ(x|z)||pθ(x|z))]\n≤−Epdata(x)[Eqφ(z|x)[log pθ(x|z)]]\n(5)\nComparing the upper bound of conditional entropy with\nthe second term in VAE/AAE loss (equation 2), we can see\nthat they are exactly the same expression. As a result, the\nupper bound of the conditional entropy H(X|Z) is equivalent\nto the reconstruction term of VAE/AAE objective function.\nMinimizing the upper bound of the conditional entropy also\nturns out to be maximizing the mutual information I(X, Z) =\nH(X)−H(X|Z), if the entropy of the data distribution H(X)\nis assumed to be ﬁxed.\nAgain the formulation of conditional entropy H(Z|X) is,\nH(Z|X) = −Epθ(z)[Epθ(x|z)[log pθ(z|x)]]\n= −Epθ(z)[Epθ(x|z)[log qφ(z|x)]]\n−Epdata(x)[KL(pθ(z|x)||qφ(z|x))]\n≤−Epθ(z)[Epθ(x|z)[log qφ(z|x)]]\n(6)\nWe see that the reconstruction loss term in equation 4\nis exactly the same as the upper bound of the conditional\nentropy H(Z|X). This loss function tries to minimize the\nconditional entropy H(Z|X) and consequently maximizing\nthe mutual information I(X, Z) = H(Z) −H(Z|X).\nB. Comparing x-Rep-x and Rep-x-Rep\nNow we study the property of the loss function of x-Rep-\nx and Rep-x-Rep. Both the two structures try to maximize\nthe mutual information I(X, Z) by minimizing conditional\nentropies. However since conditional entropy is not symmetric,\nthose two structures show different focuses.x-Rep-x structure\nminimizes the conditional entropy H(X|Z). It is trained to\ndecrease the uncertainty of x given z. As shown in ﬁgure\n2, it demonstrates a stochastic mapping from x to the latent\nrepresentation z. Comparing equation 4 to equation 2, we can\nsee that they are symmetric to each other. As a result, we can\nuse the same argument as in the x-Rep-x case to conclude\nthat Rep-x-Rep actually maps multiple data points x back to\nthe same latent representation z as shown in ﬁgure 2.\nIn conclusion, we show that x-Rep-x maps multiple points\nin latent space to a single point in data space, whereas Rep-\nx-Rep maps multiple points in data space to a single point\nin latent space. Therefore, x-Rep-x is good at reconstruction\n(when the latent space is large enough). The classiﬁcation\nperformance of it is not guaranteed though. On the other hand,\nx-Rep-x is good at classiﬁcation, because different digits with\nsubtle differences can be put into the same category, which\nmakes the classiﬁer robust to noises and small style changes.\nBut the reconstruction performance is not guaranteed.\nActually, if we follow the design of the latent vector in\nthe original InfoGAN paper, the reconstruction of InfoGAN\ncannot be good because the noise at the input of InfoGAN\nis not present at the output, which means subtle information\ndescribing the details of the image is discarded during\nreconstruction. In our experiments, we ﬁnd out that the\nnoise actually changes the generated image greatly, as shown\nin ﬁgure 10 and 11. If the noise is simply discarded, the\nreconstructed images will be almost not the same.\nTo further understand the mapping relation of x-Rep-x and\nRep-x-Rep, suppose in the discrete case, we notice that the\nsecond term in equation 2 is minimized to zero when pθ(x|z)\nequals one for all z, whereas those z are actually the output\nfrom the encoder of the AAE. Thus in the optimal case,\nthe probability mass function qφ(z|x) should have disjoint\nsupport for different given x [11], but it is not optimized to\n1 for a speciﬁc z. That is the reason why one data point x\ncan be mapped to different z, and multiple z can be used\nto reconstruct the same x. Similar explanations apply to the\nmapping property of Rep-x-Rep.\nAnother problem is the dimension of the latent vector.\nLower latent dimension will suffer from insufﬁcient repre-\nsentation ability, while higher latent dimension will increase\nthe difﬁculty of distribution regularization using adversarial\nlearning or KL divergence. The drawbacks are shown in\nﬁgure 4 and 5.\nC. Model Structure\nIn order to combine the strength of both two structures,\nwe propose to train x-Rep-x and Rep-x-Rep together with\nshared parameters elegantly, so that the new model can\nachieve good classiﬁcation and reconstruction performance\nat the same time. The network architecture is illustrated\nin ﬁgure 1. Speciﬁcally, the encoder (x2Rep) of x-Rep-\nand Rep-x-Rep are the same module sharing parameters.\nLikewise, the decoder (Rep2X) of the two structures are also\nthe same module sharing parameters. During the training,\nwe train the model alternatively between x-Rep-x-fashion\nand Rep-x-Rep-fashion so that the classiﬁcation accuracy\ncan be improved by Rep-x-Rep training while reconstruction\nperformance can be improved by x-Rep-x training. The\ntraining of Rep-x-Rep is emphasized with ﬁve times\niterations, which experimentally gives better result. The\nlatent vector is split into three subsets, a categorical variable\nc, a continuous variable s and a noise n. Continuous and\nnoise variable could be sampled from Gaussian distribution.\nThe full objective function for RepGAN is,\nLRepGAN =JS(qφ(c)||p(c))\n+ JS(qφ(s)||p(s))\n+ JS(qφ(n)||p(n))\n−Eqφ(z,x)(log pθ(x|z))\n+ JS(pθ(x)||pdata(x))\n−Ep(c)[Epθ(x|c)(log qφ(c|x))]\n−Ep(s)[Epθ(x|s)(log qφ(s|x))]\n(7)\nTABLE I\nNETWORK STRUCTURE OF REPGAN\nencoder\ndecoder\nIn 28x28x1\nIn 32x1\n4x4x64 conv, LReLU,BN\nFC1024 ReLU,BN\n4x4x128 conv,LReLU, BN\nFC7x7x128 ReLU,BN\nFC 1024 LReLU, BN\n4x4x64 deConv,ReLU,BN\nc: FC 10 softmax, BN\n4x4x1 deConv, Sigmoid\ns mean: FC 2 LRelu, BN\ns sigma: FC 2 LRelu, BN, exp()\nn: FC 20 LRelu, BN\nDz\nDx\nIn c/s/n\nIn 28x28x1\nFC3000 LReLU\n4x4x64 conv, LReLU\nFC3000 LReLU\n4x4x128 conv, LReLU, BN\nFC1 raw output (WGAN)\nFC1024 LRelu, BN\nFC 1 sigmoid\nFig. 3.\nReconstruction visualization of x-Rep-x, Rep-x-Rep and RepGAN\nwith Gaussian latent space. x-Rep-x and RepGAN achieves identically good\nreconstruction, but Rep-x-Rep cannot recover the original input image good\nenough. The reconstructed images are blur when the latent dimension of\nx-Rep-x is low, but better with RepGAN.\nIn practical implementation, we rewrite the objective\nfunction as,\nLRepGAN =LAdvC + LAdvS + LAdvN + LAdvX\n+ LRecX + LRecC + LRecS\n(8)\nwhere LRecX is computed as L2 norm for image reconstruc-\ntion, LRecC represents cross-entropy loss, and LRecS is nega-\ntive log-likelihood for Gaussian loss with re-parameterization\ntricks as InfoGAN. The model structure for experiment is\nsummarized in table I.The stride for each convolution layer is\nalways 2, and we refer the structure design and optimization\ntricks as WGAN[21], [22]. For learning rate, we used 5e-4,\n1e-3, 2e-4 for the generators, x-Rep-x discriminator and Rep-\nx-Rep discriminator on MNIST dataset. For fashionMNIST,\nwe used 5e-5, 1e-3, 2e-5. For SVHN, we used 1e-4, 1e-3,\n2e-5.\nIV. EXPERIMENTS\nWe tested the three models x-Rep-x, Rep-x-Rep and\nRepGAN on MNIST[23], FashionMNIST[24], and SVHN[25]\ndataset. We conducted two sets of experiments with different\ntypes of the latent variable z. First, z is not disentangled\nbut directly sampled from an isotropic Gaussian distribution.\nFig. 4.\nGenerated samples and the randomly selected learned latent\ndistribution of all the three models.The target latent space is zero-mean\nGaussian with 0.5 variance.\nFig. 5.\nReconstruction error and latent vector error curve for the three\nmodels across the latent dimensions. Left: Data reconstruction error in terms\nof MSE. Right: Illustrating the ability of generation by showing latent\nReconstruction error in MSE.\nIn this experiment, we investigate the theory of mapping\ndiscussed in last section. Second, like the original InfoGAN,\nwe split the latent vector z into three slots: a one-hot vector c\nsampled from a categorical distribution, a continuous vector s\nsampled from Gaussian, and a random noise n. In additional\nto reconstruction and generation performance, we demonstrate\nthe unsupervised clustering performance of RepGAN, and\nnoise importance.\nA. Gaussian Latent space\nWe ﬁrst implement the x-Rep-x, Rep-x-Rep, and RepGAN\nwith a single entangled latent space using a latent vector\nsampled from isotropic Gaussian distribution with zero mean\nand 0.5 variance. We vary the dimension of the latent vector\nto 2, 8, 16, 32 and 64, and compare the reconstruction\nperformance of image or latent space of all the three models.\nTraining and testing is conducted on MNIST dataset.\n1) Image Reconstruction: The image reconstruction result\nis computed by organizing the structure like x-Rep-x after\ntraining each model, and feeding the real data sample to the\ninput. The visualization is shown in ﬁgure 3. x-Rep-x achieves\na better reconstruction ability than Rep-x-Rep, and RepGAN\nis almost as good as x-Rep-x. As shown in the ﬁgure 3 and 5,\nRep-x-Rep has a bad ability of reconstruction, and for all the\nlatent dimensions, the error keeps the highest among the three\nmodels. That is because the loss deﬁnition of Rep-x-Rep does\nnot put constrains on the image reconstruction.\n2) Latent Reconstruction and Generation: For latent\nreconstruction evaluation, we follow the structure of Rep-x-\nTABLE II\nTESTING ACCURACY FOR UNSUPERVISED CLASSIFICATION\nModel\nMNIST\nMNIST\nFMNIST\nFMNIST\n(Acc)\n(MSE)\n(Acc)\n(MSE)\nx-Rep-x\n86.92%\n0.007\n57.30%\n0.015\nRep-x-Rep\n95%\n0.07\n53.81%\n0.098\nVADE\n94.46%\nNone\nNone\nNone\nDEC\n84.30%\nNone\nNone\nNone\nRepGAN\n96.74%\n0.02\n58.64%\n0.013\nFig. 6.\nVarying the categorical variable, the RepGAN can cluster different\ntypes of images in MNIST and FashionMNIST in a fully unsupervised way.\nThe categorical variable varies along the column, and the continuous variable\nvaries along the row.\nRep for testing. After training all the models, we reorganize\nthe network structure, and feed a sampled latent vector z\ninto the network. we plotted the MSE of the latent code to\nexamine the ability of latent regularization and the exist of\nmode collapse of the models in ﬁgure 5. For x-Rep-x trained\none, the error becomes large when the latent dimension is\nhigh because of (1) heavy mode collapse: given different\nz, the model generates identical x. This is illustrated in the\nobjective, and (2) unsatisﬁed latent regularizing like VAE. In\nthis case, when we sample from a true prior distribution, the\nx-Rep-x model cannot generate good-quality images. That is\nbecause the high-quality manifold shifted.As shown in ﬁgure\n4, autoencoder cannot generate high-quality samples when\nthe latent dimension is too large, and cannot generate sharp\nsamples when the dimension is small. The model fails to\nlearn a good latent distribution when the latent dimension is\nlarger or equal to 16.\nHowever, Rep-x-Rep trained structure and RepGAN\nachieve an identical good performance for latent space\nmodeling and new sample generation. The generated images\nare also sharp and clear. All the images in ﬁgure 4 are\nrandomly sampled from the generation results. Compared\nwith x-Rep-x and Rep-x-Rep which can only guarantee\neither recognition and generation, the proposed RepGAN can\nsimultaneously achieve the two capabilities by constraining\non two conditional entropy, and the mapping between the\nlatent variable and real data shrinks to a bijection.\nFig. 7.\nComparison of different attributes generation ability of RepGAN\nand InfoGAN. RepGAN generates better-quality images than infoGAN.\nFig. 8.\nComparison of generated SVHN samples between x-Rep-x (AAE)\nand RepGAN. RepGAN generates sharper images than x-Rep-x trained\nstructure.\nB. Disentangled Latent space\nIn this experiment, we disentangle the latent space and\nfollow the original structure of InfoGAN. c, s and n\nhave dimension 10, 2, and 20 respectively. In addition to\nreconstruction and generation, we compare the unsupervised\nclustering performance. We also investigate the importance\nof the noise.\n1) Unsupervised Learning: When evaluating the unsuper-\nvised clustering accuracy, we set the continuous and noise\nvector to zero, and generate the cluster head of each clusters.\nThen we searched in the training set to ﬁnd the closest sample\nwith the cluster head, and assigned the label of that sample\nto the whole cluster. Finally, we computed the accuracy\nbased on the assigned cluster labels. Table II shows the\nclassiﬁcation accuracies of comparable models like VADE[26]\nand DEC[27] on MNIST and FashionMNIST. The Rep-x-Rep\nand RepGAN are able to achieve an average accuracy of 95%\nor 96%, which is much higher than the x-Rep-x, which only\nachieves 87%. For FashionMNIST, the classiﬁcation accuracy\nis low due to the high similarity of images assigned by\ndifferent category labels. This experiment result is consistent\nwith our theoretical analysis, which is Rep-x-Rep is better\nfor classiﬁcation than x-Rep-x. RepGAN, being the elegant\ncombination of the two structures, successfully preserved\nFig. 9.\nReconstruction performance of x-Rep-x, Rep-x-Rep and RepGAN with disentangled latent space. The ﬁrst row of each group of images are the\ninput, and the second row is the reconstruction. Rep-x-Rep cannot achieve a faithful reconstruction, and x-Rep-x only recovers blurred images if the latent\ndimension is not big enough. RepGAN achieves sharper and more clear reconstruction.\nFig. 10.\nFirst row: traversing on continuous variable s with zero noise, and then controlling s and adding noise n. Using the same noise batch for different\nclusters, but it shows different variants. Thus noise variable is cluster-dependent. Second row: traversing on the ﬁrst two dimensions of noise vector with\nzero s. Tiny changes of samples can be visualized. Then we add same random continuous variable value for distinct clusters, and identical changes are\nillustrated. s is cluster-independent and corresponding to commonly shared attributes: slant and thickness.\nthe ability of Rep-x-Rep for clustering and generation, and\nx-Rep-x for reconstruction.\nThe qualitative evaluation of reconstruction and generation\nability of RepGAN is shown in ﬁgure 6 and 9. By ﬁxing\nthe categorical code, the model is able to generate any\nsamples belonging to this cluster. And by changing the\ncontinuous value, the model learns the manifold of the styles.\nWhile reconstructing, RepGAN achieves a more faithful\nreconstruction than Rep-x-Rep, and sharper images than x-\nRep-x.\nWe also compare our generated image on CelebA with\ninfoGAN in ﬁgure 7. By using the same latent space\nconﬁguration as infoGAN, namely 10 categorical variables\nwhere each one is 10-dim OneHot vector, we are able to\nachieve a better image quality while showing attribute change\nat the same time. In addition, we compare the generation\nquality on SVHN between x-Rep-x and RepGAN in ﬁgure 8\nand shows that RepGAN generates sharper images than an\nautoencoder. In summary, RepGAN currently does well in all\nthe three tasks: reconstruction, generation, and unsupervised\nclustering.\n2) Effectiveness of Noise : The noise variable is interpreted\nas representation incompressible information in InfoGAN. We\ntunnel the noise for intact and plausible image reconstruction\nduring training the x-Rep-x part, since categorical and\ncontinuous variable may not be expressive enough for intact\nreconstruction. The difference between continuous and noise\nvariable is that: the lower-dimensional continuous variable\nis used to encode the most salient attributes (or largest data\nvariance direction) commonly shared by all the samples\n(it is enhanced by LRecS), while noise is used to encode\nincompressible or entangled information (it is enhanced by\nLRecX).\nIn ﬁgure 10 and 11, we demonstrate the effect of continuous\nv.s. noise variable on generated samples. Speciﬁcally, on the\nﬁrst row, we interpolate on the continuous code, and set\nFig. 11.\nGenerated samples on FashionMNIST dataset. Similar to MNIST, we can see a smooth style transition when noise is set to zero. Compared to\nMNIST dataset, noise variables have a larger impact on generated images on fashionMNIST. In addition, since the RepGAN on fashionMNIST is trained\nunsupervised, different categories with similar appearance may get classiﬁed as same category, as shown on the bottom right sub-ﬁgure where sandals and\nsneakers are confused.\nthe noise variable to zero. While varying the continuous\nvariable, the style changes explicitly and smoothly. After\nadding random noise, in addition to uniformly changed style,\nmore variants are generated.\nOn the second row, we interpolate the ﬁrst two dimension\nof the noise code, and set the continuous variable to zero. We\ncan see tiny changes of the generated images when traversing\non the ﬁrst two dimensions of noise, and the changes are\nslightly different for distinct clusters. It demonstrated the\ninformation encoded in noise is actually cluster-dependent.\nIf we randomly sample the continuous variable and keep\nit the same for all the clusters, we can visualize identical\nchanges of the image attributes across clusters (slant and\nthickness degree). It demonstrated the information encoded\nin s is actually cluster-independent or cluster-shared.\nV. CONCLUSION AND DISCUSSION\nIn this paper, we analyzed the advantage and disadvantage\nof two unsupervised machine learning strictures: x-Rep-x and\nRep-x-Rep. We showed both theoretically and experimentally\nthat Rep-x-Rep is able to achieve a higher classiﬁcation ac-\ncuracy, whereas x-Rep-x is able to get a better reconstruction\nquality. After that, we combined those structures elegantly in\nan attempt to take their advantages. We showed on MNIST,\nFashionMNIST and SVHN dataset that the new model, named\nRepGAN, is able to achieve both a high classiﬁcation accuracy\nand a good reconstruction quality in both the original input\nspace and the latent space. By performing well in both\nclassiﬁcation and reconstruction, RepGAN is able to learn\na good bidirectional mapping between the input space and\nthe latent space, which is a desired property of unsupervised\nrepresentation learning model. It will be inspiring if it can be\nutilized for arbitrarily complicated data discovery with more\ncomplicated network structures and larger latent dimension,\nwhich is left for future work.\nREFERENCES\n[1] I. Goodfellow, “Nips 2016 tutorial: Generative adversarial networks,”\narXiv preprint arXiv:1701.00160, 2016.\n[2] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and\nP. Abbeel, “Infogan: Interpretable representation learning by infor-\nmation maximizing generative adversarial nets,” in Advances in Neural\nInformation Processing Systems, pp. 2172–2180, 2016.\n[3] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey,\n“Adversarial autoencoders,” arXiv preprint arXiv:1511.05644, 2015.\n[4] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell, “Adversarial feature learning,”\narXiv preprint arXiv:1605.09782, 2016.\n[5] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mas-\ntropietro, and A. Courville, “Adversarially learned inference,” arXiv\npreprint arXiv:1606.00704, 2016.\n[6] C. Li, H. Liu, C. Chen, Y. Pu, L. Chen, R. Henao, and L. Carin,\n“Alice: Towards understanding adversarial learning for joint distribution\nmatching,” in Advances in Neural Information Processing Systems,\npp. 5501–5509, 2017.\n[7] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang,\nand E. Shechtman, “Toward multimodal image-to-image translation,”\nin Advances in Neural Information Processing Systems, pp. 465–476,\n2017.\n[8] W. Shang, K. Sohn, Z. Akata, and Y. Tian, “Channel-recurrent\nvariational autoencoders,” arXiv preprint arXiv:1706.03729, 2017.\n[9] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, “Neural photo\nediting with introspective adversarial networks,” arXiv preprint\narXiv:1609.07093, 2016.\n[10] Y. Taigman, A. Polyak, and L. Wolf, “Unsupervised cross-domain\nimage generation,” arXiv preprint arXiv:1611.02200, 2016.\n[11] S. Zhao, J. Song, and S. Ermon, “Infovae: Information maximizing\nvariational autoencoders,” arXiv preprint arXiv:1706.02262, 2017.\n[12] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks,” arXiv preprint\narXiv:1703.10593, 2017.\n[13] Z. Yi, H. Zhang, P. T. Gong, et al., “Dualgan: Unsupervised dual learn-\ning for image-to-image translation,” arXiv preprint arXiv:1704.02510,\n2017.\n[14] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim, “Learning to discover\ncross-domain relations with generative adversarial networks,” arXiv\npreprint arXiv:1703.05192, 2017.\n[15] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[16] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backpropa-\ngation and approximate inference in deep generative models,” arXiv\npreprint arXiv:1401.4082, 2014.\n[17] C. Doersch, “Tutorial on variational autoencoders,” arXiv preprint\narXiv:1606.05908, 2016.\n[18] A. Odena, C. Olah, and J. Shlens, “Conditional image synthesis with\nauxiliary classiﬁer gans,” arXiv preprint arXiv:1610.09585, 2016.\n[19] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and\nX. Chen, “Improved techniques for training gans,” in Advances in\nNeural Information Processing Systems, pp. 2234–2242, 2016.\n[20] J. T. Springenberg, “Unsupervised and semi-supervised learning\nwith categorical generative adversarial networks,” arXiv preprint\narXiv:1511.06390, 2015.\n[21] M. Arjovsky and L. Bottou, “Towards principled methods for training\ngenerative adversarial networks,” arXiv preprint, 2017.\n[22] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” arXiv\npreprint arXiv:1701.07875, 2017.\n[23] Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,”\nAT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist,\nvol. 2, 2010.\n[24] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image\ndataset for benchmarking machine learning algorithms,” arXiv preprint\narXiv:1708.07747, 2017.\n[25] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,\n“Reading digits in natural images with unsupervised feature learning,”\nin NIPS workshop on deep learning and unsupervised feature learning,\nvol. 2011, p. 5, 2011.\n[26] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou, “Variational deep\nembedding: An unsupervised and generative approach to clustering,”\narXiv preprint arXiv:1611.05148, 2016.\n[27] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for\nclustering analysis,” in International conference on machine learning,\npp. 478–487, 2016.\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-04-19",
  "updated": "2019-04-06"
}