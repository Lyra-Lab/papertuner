{
  "id": "http://arxiv.org/abs/1804.08597v1",
  "title": "Towards Symbolic Reinforcement Learning with Common Sense",
  "authors": [
    "Artur d'Avila Garcez",
    "Aimore Resende Riquetti Dutra",
    "Eduardo Alonso"
  ],
  "abstract": "Deep Reinforcement Learning (deep RL) has made several breakthroughs in\nrecent years in applications ranging from complex control tasks in unmanned\nvehicles to game playing. Despite their success, deep RL still lacks several\nimportant capacities of human intelligence, such as transfer learning,\nabstraction and interpretability. Deep Symbolic Reinforcement Learning (DSRL)\nseeks to incorporate such capacities to deep Q-networks (DQN) by learning a\nrelevant symbolic representation prior to using Q-learning. In this paper, we\npropose a novel extension of DSRL, which we call Symbolic Reinforcement\nLearning with Common Sense (SRL+CS), offering a better balance between\ngeneralization and specialization, inspired by principles of common sense when\nassigning rewards and aggregating Q-values. Experiments reported in this paper\nshow that SRL+CS learns consistently faster than Q-learning and DSRL, achieving\nalso a higher accuracy. In the hardest case, where agents were trained in a\ndeterministic environment and tested in a random environment, SRL+CS achieves\nnearly 100% average accuracy compared to DSRL's 70% and DQN's 50% accuracy. To\nthe best of our knowledge, this is the first case of near perfect zero-shot\ntransfer learning using Reinforcement Learning.",
  "text": "Towards Symbolic Reinforcement Learning\nwith Common Sense\nArtur d’Avila Garcez1\nAimore Resende Riquetti Dutra2\nEduardo Alonso3\nDepartment of Computer Science\nCity, University of London\nLondon EC1V 0HB, U.K.\na.garcez@city.ac.uk1, aimorerrd@hotmail.com2, e.alonso@city.ac.uk3\nAbstract\nDeep Reinforcement Learning (deep RL) has made several breakthroughs in recent\nyears in applications ranging from complex control tasks in unmanned vehicles to\ngame playing. Despite their success, deep RL still lacks several important capaci-\nties of human intelligence, such as transfer learning, abstraction and interpretability.\nDeep Symbolic Reinforcement Learning (DSRL) seeks to incorporate such capac-\nities to deep Q-networks (DQN) by learning a relevant symbolic representation\nprior to using Q-learning. In this paper, we propose a novel extension of DSRL,\nwhich we call Symbolic Reinforcement Learning with Common Sense (SRL+CS),\noffering a better balance between generalization and specialization, inspired by\nprinciples of common sense when assigning rewards and aggregating Q-values.\nExperiments reported in this paper show that SRL+CS learns consistently faster\nthan Q-learning and DSRL, achieving also a higher accuracy. In the hardest case,\nwhere agents were trained in a deterministic environment and tested in a random en-\nvironment, SRL+CS achieves nearly 100% average accuracy compared to DSRL’s\n70% and DQN’s 50% accuracy. To the best of our knowledge, this is the ﬁrst case\nof near perfect zero-shot transfer learning using Reinforcement Learning.\n1\nIntroduction\nThe combination of classical Reinforcement Learning (RL) with deep neural networks has achieved\nhuman-level competence at solving some difﬁcult problems, especially with the use of Deep Q-\nNetworks (DQN) at solving games[15, 17, 20]. There is no doubt that Deep Reinforcement Learning\n(DRL) has offered new perspectives to the areas of automation and Artiﬁcial Intelligence (AI), but\ndespite their success, DRL seems unable to tackle a number of problems that are considered relatively\nsimple for humans to solve. DRL requires large training data sets, thus learning slowly. A trained\ndeep network performing very well in one task will often perform poorly in another, analogous\n(sometimes very similar) task. Finally, DRL lacks interpretability, being criticized frequently for\nbeing a “black-box” model.\nSome authors have tried to address the above shortcomings by adding prior knowledge to neural\nnetworks in general, and more recently to RL [4, 8, 11, 24]. In [5], it is argued that combining\naspects of symbolic AI with neural networks and Reinforcement Learning could solve at once all\nof the above shortcomings. Such ideas are then instantiated in a computational system called Deep\nSymbolic Reinforcement Learning (DSRL). DSRL uses convolutional neural networks which, applied\nto navigation tasks typically consisting of negotiating obstacles (objects) and ﬁnding optimal paths or\ncollecting other objects, can learn relevant object types from images, thus building a more abstract\nsymbolic representation of the state-space, which is followed by a Q-learning algorithm for learning\na policy on this state-space. Despite the fact that DSRL does not learn as effectively as DQN in\narXiv:1804.08597v1  [cs.LG]  23 Apr 2018\na deterministic environment, DSRL was shown to outperform DQN when the policy learned in a\ndeterministic environment is transferred to a random environment, thus indicating the value of using\na more abstract, symbolic representation.\nIn this paper, we seek to reproduce the DSRL system with some simpliﬁcations. We have implemented\nthis simpliﬁed, but also more generic system, which we refer to as Symbolic Reinforcement Learning\n(SRL). For the sake of analysis, it is desirable to separate within SRL, learning and decision-making\nfrom the object recognition part of the system, this latter part carried out in DSRL by convolutional\nneural networks. Such a separation allows us to focus the analysis of performance on the learning and\ndecision making by providing the symbols and positions of the objects directly to the SRL system.\nAspects of type transition and symbol interaction dealt with by DSRL are not analyzed in this paper.\nWe also propose an extension of SRL called Symbolic Reinforcement Learning with Common\nSense (SRL+CS). SRL+CS makes one modiﬁcation to the learning of SRL and one modiﬁcation to\nthe decision-making. This is then shown empirically to be sufﬁcient for producing a considerable\nimprovement in performance by comparison with various other algorithms. Differently from SRL, in\nSRL+CS only the states containing objects with which an agent interacts have their Q-values updated\nwhen rewards are non-zero (let us call this principle one), and the relative position of an object with\nrespect to an agent is considered in the decision of which action to take (call this principle two).\nMore speciﬁcally, principle one is implemented through Equation ( 3) by making speciﬁc updates\nto the Q-values according to the speciﬁc interactions that an agent has with the (objects in the)\nenvironment; principle two, which seeks to give more importance to the objects that are nearer to the\nagent, is implemented through Equation (4) by simply dividing the Q-values by a factor proportional\nto the relative position of the agent w.r.t. the objects for the purpose of choosing an action. The\nimplementation of these two principles within a standard RL set-up such as SRL+CS is only made\npossible by the adoption of a more abstract symbolic representation of the state-space, as put forward\nby the DSRL approach.\nFor the sake of comparison between the various RL systems already mentioned, we chose to reproduce\na benchmark game used by DSRL in [5]. In this game, SRL+CS achieves 100% accuracy when tested\nin a random environment, compared to DQN’s 50% and DSRL’s 70% accuracy. In general, SRL+CS\nlearns faster and more effectively than SRL by simply incorporating the above principles one and two\nto Q-learning.\nThe remainder of the paper is organized as follows: In Section 2, the Q-Learning, DQN and DSRL\nalgorithms are reviewed brieﬂy. In Section 3, the Symbolic Reinforcement Learning (SRL) algorithm\nis explained in three parts: state-space representation, learning, and decision-making. In Section 4,\nthe Symbolic Reinforcement Learning with Common Sense algorithm is introduced through simple\nmodiﬁcations to the SRL algorithm’s learning and decision making. The setup of the experimental\nenvironment is introduced in Section 5 using several variants of the benchmark game where an agent\nis expected to collect an object type while avoiding another. Experimental results are also presented\nin Section 5. We evaluate how Q-learning, DQN, DSRL, SRL and SRL+CS generalize and scale\nwith changes to the size of the environment, as well as how the systems perform at zero-shot transfer\nlearning when trained and tested in deterministic or random setups. SRL+CS outperforms all the\nother systems in all but one of these experiments. We then conclude and discuss directions for future\nwork.\n2\nBackground\nIn this section, we brieﬂy recall the algorithms of Q-learning, Deep Q-Networks (DQN) and Deep\nSymbolic Reinforcement Learning (DSRL).\n2.1\nQ-Learning\nQ-learning is a model-free RL algorithm (although model-based variations have been proposed). In\nQ-learning, an agent’s goal is to maximize by trial and error its total future reward thus learning\nwhich action is optimal in a given state [26]. By learning a function Q(s, a) mapping states to\nactions, Q-learning can under certain conditions ﬁnd an optimal action-selection policy for any given\nﬁnite Markov Decision Process (MDP). Q(s, a) denotes the quality of a state-action pair, which\nultimately gives the expected utility of an action a in a given state s. The Q-learning algorithm can\n2\nbe summarized as follows (below, an ϵ-greedy policy is the most commonly used policy, which with\nprobability ϵ, selects an action randomly, and with probability 1 −ϵ, selects an action that gives the\nmaximum reward in the current state):\nStarting at state:\ns0,\nFor\nt = 1, 2, 3, . . .\nChoose an action at using e.g. an ϵ-greedy policy w.r.t. Q(s, a);\nExecute action at;\nUpdate Q, as follows:\nQ(st, at) = Q(st, at) + αt [R(st, at) + γ maxa Q(st+1, a) −Q(st, at)],\nwhere αt is the learning rate at time t,\nR is the reward observed for the current state and choice of action,\nγ is a temporal discount factor, and\nmaxa Q(st+1, a) is an estimate of the optimal future value.\nAt each step, the Q-Learning algorithm updates the Q-value function considering the current state\n(st) and the action performed (at). After learning, the policy Π dictating the action to be taken at\nany given state will be given by: Π(s) = arg maxa Q(s, a). A full introduction to Q-learning can be\nfound in [23].\n2.2\nDeep Q-Networks (DQNs)\nDeep Q-Networks (DQNs) were introduced in [17] and ignited the ﬁeld of deep Reinforcement\nLearning for three main reasons: ﬁrst, DQNs were designed as an end-to-end RL approach when\napplied to games having only the pixel values and the game score as inputs, thus requiring minimal\ndomain knowledge; second, it used deep convolutional neural networks (CNN) and experience replay\nto learn a Q-value approximation function successfully; third, it showed very good performance in\na wide range of games - in some of them achieving performance higher than human-level - with\nthe use of the same hyper-parameters [13]. In a nutshell, DQNs are composed of a convolutional\nneural network to reduce the state space and seek to generalize states, and a Q-learning algorithm for\nmapping states to actions. Despite their success, DQN training can be computationally inefﬁcient,\nlacking explainability and hierarchical reasoning.\n2.3\nDeep Symbolic Reinforcement Learning (DSRL)\nDSRL was introduced in [5]. In DSRL, ﬁrst, object types are classiﬁed with objects having their\nlocations detected by a low-level symbol generator which uses a convolutional auto-encoder and a\nspectrum comparison technique. Second, a spatio-temporal representation is built of all the objects,\nusing the relative positions of the objects at two consecutive time frames. Third, this spatio-temporal\nrepresentation is provided to a group of Q-learning functions, one for each pair of type of objects.\nThe update rule for the interaction between objects of types i and j is given by:\nQij(sij\nt , at) ←Qij(sij\nt , at) + α\nh\nrt+1 + γ max\na\nQij(sij\nt+1, a) −Qij(sij\nt , at)\ni\n(1)\nwhere α is the learning rate, γ is a temporal discount factor, and each state sij\nt represents an interaction\nbetween object types i and j at time t. After learning, the values of the Qij functions are added, and\nthe action with the largest sum is chosen as shown in Equation 2.\nat+1 = argmaxa\nX\nQ\n(Q(st+1, a))\n(2)\n3\nSymbolic Reinforcement Learning (SRL)\nSRL is our implementation of the DSRL model with some simpliﬁcations. For ease of analysis, in\nSRL we separate the Reinforcement Learning problem into three parts: State-Space Representation,\nLearning, and Decision-Making, as detailed in what follows.\n3.1\nState-Space Representation\nNaive approach: The simplest way of representing the state space of an RL environment derived\nfrom an image is to associate each possible set of pixel values with a state. A black and white image\n3\nis composed of a matrix of pixels that can assume values from 0 to 255. In a black and white image\nwhere n is the number of rows and m is the number of columns of pixels in the image, the number of\npossible states will then be 256n∗m. Such a very large state space presents obvious difﬁculties for\nlearning. In this setting, each combination of assignments of values to the pixels in the image can be\nconsidered a relevant state for the purpose of RL.\nCNN approach: In order to reduce the number of states in an image, the use of a convolutional neural\nnetwork (CNN) was proposed in [16], leading to Deep Q-networks. DQNs seek to generalize the\nstates provided by pixel values using learned ﬁlters and a fully-connected feedforward neural network\nas a regressor for the Q-function value approximation. However, the states generalized by the CNN\nare by construction dependent on all the pixels in the raw image, which can hinder generalization\nacross different images. In [21], it is shown for example, that varying one pixel value can cause\nthe classiﬁcation of a CNN to change, or in the case of DQN, the state to change. Furthermore, the\nstates are not fully translation-, rotation-, and size-invariant, requiring additional manipulation to\nbecome more robust, as shown in [7]. States generalized by CNNs do not carry explicit information\nabout the objects’ relative positions, and are in general restricted by the ﬁxed dimensions of the input\nimage. Experiments reported in [5] and reproduced here will highlight this problem in the context of\nQ-learning.\nSymbolic approach: Another way to compress and generalize the number of states in an image that\ndoes not incur the above loss of information is to recognize such objects (possibly using a CNN) and\nrepresent them by symbols with their respective locations. In the DSRL approach of [5] this is done by\nusing a low-level symbol generation technique, denoted by the letter C in Figure 1, which illustrates\nthe process. An advantage of DSRL, therefore, is that now the state-space can be represented not by\nthe combination of all pixel values or learned ﬁlters but by object types (e.g. ‘+’ and ‘-’ as in Figure\n1) and their positions (X and Y numerical values in Figure 1), represented in symbolic form.\nThe use of a symbolic approach allows one to separate the detection and classiﬁcation of objects\nin an image, which have been shown to work well with the use of CNNs and other deep learning\nand hybrid approaches [1, 5, 6, 12, 19], from the ways that an agent may learn and reason about its\nenvironment. In our current implementation of SRL, used to obtain the results reported in this paper,\nthe object detection and classiﬁcation part is not included. Instead, we focus on the agent’s learning\nand reasoning. Each object’s type and position is provided directly to the system in symbolic form\nas shown on the right-hand side of Figure 2, where we use the letter A to denote abstraction. Each\nbox in Figure 2 can be seen as a sub-state, which can in turn be composed with other sub-states in\ndifferent ways depending on the choice of abstraction.\nSub-states that are represented only by a single object often do not carry enough relevant information\nwhen it comes to reasoning about the properties of the problem at hand such as size invariance. In\nthe experiments carried out in this paper, which use the same game as used in [5] where a single\nagent represented by a star is expected to collect all the positive objects in the environment while\navoiding any negative objects, the four objects shown in Figure 1 are combined to create the six\nabstract sub-states shown on the right hand side of Figure 2.\nWe have observed empirically that the choice of abstraction can facilitate learning considerably by\nrepresenting the relevant parts of the environment in which an agent is expected to focus. Although it\nis likely that the best abstraction will depend on the problem at hand, and therefore on background\nknowledge about the problem domain, in what follows we identify some generally desirable (common-\nsense) abstractions, such as the relative distance between objects, which allows learning to scale well\nto larger environments, and which can be implemented still by a model-free variation of Q-learning.\nBoth in DSRL and SRL, abstraction is carried out by the combination two by two of sub-states. In\nDSRL, this is done as shown in Figure 2 by taking the relative position of each object w.r.t. every\nother object. In SRL, the same is done but then, in addition, any state where an agent (the star-shaped\nobject) is not included is removed (states s′4 to s′6 in the example of Figure 2). This is simply because\nany state without an agent is obviously irrelevant to our game. In general, ideally, the system would\nknow how to select only sub-states which are relevant to solving the problem.\nAn alternative choice of abstraction would essentially dictate how general or speciﬁc the resulting\nstate-space should be in relation to the learning task at hand. We leave such an analysis of generality\nvs. speciﬁcity as future work. Notice that the outcome of abstraction in SRL or DSRL is different\nfrom the outcome of a neural network-based approach such as DQN. While a CNN is capable of\n4\ndistinguishing one state from another, (symbolic) abstraction is capable of creating entirely new\nstates from the original input. It is likely however that such abstraction can be achieved by stacking\nend-to-end differentiable neural networks, although this too is left as future work.\nFigure 1: The letter C represents the tasks of object recognition and object type assignment typically\ncarried out with the use of a convolutional neural network [5]. In our approach, such object types and\ntheir location are seen as sub-states of the state of an image.\nFigure 2: Each sub-state can be combined with certain other sub-states to form a richer, and hopefully\nmore meaningful symbolic representation for the learning task at hand. We call this process an\nabstraction, here denoted by the letter A. In this particular example, A simply calculates the relative\nposition of each object w.r.t. every other object. The boxes drawn using dotted lines indicate that the\npositions are relative, while the boxes drawn using solid lines indicate that the positions are absolute.\nIn summary, several advantages can be found in the symbolic way of representing the world:\n(a) It simpliﬁes the problem by reducing the state-space and limiting the number of relevant objects\nin each sub-state;\n(b) It is compositional, allowing the state-space to be represented in different ways which may\nfacilitate learning;\n(c) Sub-states can reappear at different regions of an image thus contributing to better generalization;\n(d) It is independent of the size of the images; once a task is learned it should scale to larger\nenvironments;\n(e) It should be possible at least in principle to learn relations among objects.\n3.2\nLearning\nLearning with multiple sub-states: A potential drawback of having sub-states is that, instead of\nsolving one Markov Decision Process (MDP), our learning algorithm now has to solve N Partially\nObservable Markov Decision Processes (POMDPs), where N is the number of abstract sub-states\n(s’). In DSRL and SRL, Q-Learning was chosen for this task, in which case there are two options:\n5\nuse a single Q-value function which embeds the type and position of objects in its sub-states (s’), or\nuse one Q-value function for each pair of object types, embedding only their position in the sub-states.\nThe alternatives are illustrated in Figure 3. In DSRL and SRL, the second option was chosen, since\nhaving separate Q-value functions should make learning from each speciﬁc combination of object\ntypes easier.\nFigure 3: Two options for encoding abstract sub-states into a Q-value function: use a single Q-value\nfunction or use as many as the number of pairs of object types. By choosing the latter, another choice\nwill be required on how to use the Q-value functions for decision making.\nIn summary, when abstractions (s’) are used in the place of entire images, an agent is required to\nlearn in a partially observable environment. In such cases, no guarantees exist of ﬁnding an optimal\npolicy, as in the case of a fully-observable environment. Nevertheless, it should still be posible for\nthe learning algorithm to ﬁnd optimal policies for sub-tasks which may be described fully within the\nsub-spaces. These sub-tasks often have shorter horizons and may appear more than once, making\nlearning in such cases converge faster, and hopefully helping an agent to succeed in its main task.1\nRewarding multiple sub-states: In Q-learning, a reward is obtained when an action takes place in a\ngiven state. An alternative is to associate rewards with the resulting state that an action may produce,\nor even with reaching a state independently of the action. In SRL with the use of multiple Q-learning\nfunctions, therefore, most of the time that a reward is obtained it will be impossible to know to which\nsub-states this reward corresponds. An agent will perform poorly if rewards are associated with\nsub-states that are not relevant, or are not the cause of the reward.2\nIn SRL, the same rewards go into the updates of all the Q-value functions. As discussed, this is\nundesirable because certain sub-states may be rewarded for an unrelated action. We have conﬁrmed\nthis empirically by creating an example where rewards are assigned incorrectly, leading to low\nperformance at test time. SRL+CS will address this problem by limiting reward updates to speciﬁc\nsub-states based on valid object interactions, as detailed in the next section.\n3.3\nDecision Making\nAnother issue that arises when considering multiple sub-spaces is how to aggregate the Q-value\nfunctions learned in order to make a decision, i.e. choose a single action at a given state.\n1An example would be playing the Atari game of Breakout by focusing ﬁrst only on how the paddle moves\nleft or right as a result of the corresponding key strokes, then focusing on certain objects only, such as the paddle\nand the ball, and how to hit the ball every time, and so on. At some point, e.g. to end the game with a win, an\nagent could choose to combine all sub-states in its abstraction to form the full image and solve the problem as an\nMDP. That would require changing the focus from paddle and ball only to the entire state space and probably\nconsidering longer horizons, although at this point fewer objects will have been left, and therefore the state space\nwill be smaller than at the start of the game.\n2For example, in the game of Chess it may be counter-productive to reward for wining the game the positions\nof the pieces that do not take any part in the King’s checkmate.\n6\nDSRL selects the highest value among the sum of each Q-value function, as shown in Equation 2. In\nSRL, we use the same approach. However, this may not work in all situations because the actions\nassociated with the Q-value functions may be contradictory, especially as the number of Q-value\nfunctions (i.e. objects) increases. Simply adding the Q-values seems insufﬁcient to guarantee that\nargmax will maximize the accumulated future reward.\nAnother way to aggregate the Q-value functions would be to simply take the argmax among all\nthe Q-value function values. This does not work well either because it ignores Q-value functions\nwith lower values, which may be relevant nevertheless (e.g. for learning from multiple sub-states).\nSRL+CS will address this problem by assigning a degree of importance to the different Q-functions,\nas detailed in the next section.\n4\nSymbolic Reinforcement Learning with Common Sense (SRL+CS)\nSymbolic Reinforcement Learning with Common Sense is based on SRL with two modiﬁcations,\none in the learning and another in the decision-making part, with the objective of solving the two\nissues identiﬁed in the previous section: assigning rewards and aggregating Q-values adequately. In\nwhat follows, ﬁrst, we formalize the state-space representation, then we present the two modiﬁcations\nmade in the learning and decision-making algorithms.\nSTATE-SPACE REPRESENTATION:\nSRL+CS creates a state-space exactly as done by SRL. Formally, it builds a representation of the\nworld (abstraction) by creating sub-states sk which consist of the relative position of an agent\nw.r.t. an object. Each sub-state represented by this pair (agent, object) is denoted by k. Let agent\nm and object n have absolute positions (xm, ym) and (xn, yn), respectively. Then, sk is a tuple\n(xm −xn, ym −yn).\nLEARNING:\nIn order to assign rewards adequately, SRL+CS restricts the Q-value function updates to speciﬁc\nsub-states, based on the interactions between an agent and the objects. Speciﬁcally, when a reward\nthat is non-zero is received, only the Q-value function with a sub-state sk with a value of (0,0) is\nupdated. Recall that a value of (0,0) indicates that an agent is in direct contact with the object.\nEquation 3 shows the Q-value function update:\nQij(sk\nt , at) ←Qij(sk\nt , at) + α\nh\nrt+1 + γ max\nA Qij(sk\nt+1, A) −Qij(sk\nt , at)\ni\n(3)\nwhere α is the learning rate, γ is the temporal discount factor, and A is the set of all possible actions\nfor that state sk\nt+1. Notice that more than one update to the Q-value function with indices i and j can\ntake place (i.e. for different sub-states) at each time point t due to the presence of multiple objects\nof the same type in the image. Equation 3 is very similar to the standard Q-learning approach, with\nthe difference that now usually the update does not occur once, but for all sub-states (k). When the\nreward is different from zero, an update occurs only for the sub-state with value (0,0). Intuitively, this\nis expected to indicate where a reward has been coming from, given a set-up consisting of multiple\nsub-states.\nDECISION MAKING:\nIn order to aggregate Q-values adequately, SRL+CS assigns a degree of importance to each Q-value\nfunction, based on how far the objects are from an agent. The calculation of the distances from an\nagent to the objects is straightforward given the above choice of symbolic state-space representation.\nThis allows one to give a priority to the decisions that are spatially close and therefore generally more\nrelevant. It should also help an agent concentrate on a relevant subtask; in the case of the game used\nin the experiments that will follow, a relevant sub-task may be to collect the positive objects which\nare above or below. This idea is in line with that of a common sense core whereby our actions are\ninﬂuenced mostly by our surroundings [10], giving more importance in general to the objects that are\nnearer or can be seen.\nFormally, the next action is chosen by Equation (4), where dk\nt is the Euclidean distance between the\nagent and the object’s position at time t.\n7\nat+1 = arg max\nA\n\" k\nX Qij(sk\nt , A)\n(dk\nt )2\n#\n(4)\nNotice the similarity with Equation 2. As in SRL, Q-values are summed and the action associated\nwith the function with the largest value is taken. It is worth pointing out that, as before, this is not\nguaranteed to work in all situations, although it works very well in most situations of the game used\nin our experiments, reported in the next section.\n5\nExperimental Results\nWe have implemented a game, also used in [5], to evaluate and compare the performances of the\nfollowing RL algorithms: Q-Learning, DQN, DSRL, SRL and SRL+CS, as introduced and discussed\nin the previous sections. In this game, an agent (a star-shaped object) can move up, down, left or\nright to collect as many positive objects as possible, denoted by a plus sign, while avoiding as many\nnegative objects as possible, denoted by a minus sign. The objects are at ﬁxed positions. Every\ntime that the position of the star-shaped agent coincides with that of an object the object disappears,\nyielding a reward of 1 in the case of positive objects and -1 in the case of negative objects. A set of\nbricks restricts the passage of the agent, as shown in Figure 4. The game ﬁnishes when all positive\nobjects are collected or after 100 movements of the agent.\nThe environment is fully-observable (the agent can see all the objects), sequential, static, discrete and\nﬁnite, with no model of the world provided in advance.\nAll algorithms (Q-Learning, DQN, DSRL, SRL and SRL+CS) used an ϵ-greedy exploration strategy\nwith a 10% chance of choosing a different action at random during training. In addition, actions were\nchosen randomly when two or more actions shared the same Q-value, i.e. no predeﬁned preference\nwas assigned to any action. The learning rate (alpha) was set to 1.0 since the state transitions were\ndeterministic, and the discount-factor rate was set to 0.9 as a standard value.\nThe metric chosen for measuring the performance of the star-shaped agent was a score calculated as\nthe sum of accumulated rewards. This tells us how much an agent has learned to move correctly by\ncollecting positive objects while avoiding negative ones.\n5.1\nTraining in Larger State-spaces\nExperiment 1 - Increasing the Size of the Environment.\nIn this ﬁrst experiment, agents were trained on the three grids shown in Figure 4 of increasing sizes.\nStarting from the center of the grid, an agent has to collect a positive object placed at a random\nlocation within the grid. No negative objects were used in this experiment.\n(a) Grid 3x3\n(b) Grid 5x5\n(c) Grid 7x7\nFigure 4: Environments of increasing sizes used in Experiment 1 with positive objects only.\nIn SRL (and DSRL and SRL+CS), an abstraction is responsible for building a state space that takes\ninto consideration the relative positions of the objects. As a result, an increase in the size of the\nenvironment should not imply the same increase in the size of the state-space. If the relative position\nbetween agent and object is the same in both the small and large grids, the agent’s performance\n8\nshould be roughly the same regardless of the size of the environment. This is not normally the case\nwith the use of Q-learning.\nFigure 5 shows, in each plot, the accumulated scores obtained over 10 runs of 1000 games each,\nusing both SRL+CS and Q-learning. It can be seen that, as the size of the grid increases, Q-learning\nrequires many more training steps (i.e. movements of the agent within the grid) than SRL+CS. The\nresults conﬁrm those reported in [5].\n(a) Results of grid 3x3\n(b) Results of grid 5x5\n(c) Results of grid 7x7\nFigure 5: Experiment 1 - These three plots, each containing 10 runs of 1000 games (each game is also\nreferred to as an episode; each step is one movement of the agent in the grid) show that the symbolic\napproach (in this case SRL+CS) learns to collect the positive objects much faster than Q-learning\nas the size of the grid increases; SRL obtained very similar results to SRL+CS, but these were not\nplotted to avoid occlusion.\n5.2\nTraining and Testing in Deterministic and Random Conﬁgurations\nWe now run three experiments using the two conﬁgurations shown in Figure 6 where the environment\nis initialized at each episode either deterministically or at random with the same number of both\npositive and negative objects. These conﬁgurations are the same as used in the experiments reported\nin [5].\n(a) Deterministic Conﬁguration\n(b) Random Conﬁguration\nFigure 6: Two examples of initial conﬁgurations of the environment used in Experiments 2, 3 and 4,\nwhere each episode starts with eight (a) and seven (b) objects of each type.\nIn addition to the accumulated scores metric already used to measure agent performance, here we\nalso use the percentage of positive objects collected over the total number of objects collected: a\nvalue of 100% shows that an agent has collected only positive objects, but not necessarily all positive\nobjects.\nIn [5], for some experiments, only this second metric is used. We argue that it should be used in\nconnection with the accummulated scores, as done here, as otherwise it may boast the performance\nof a very conservative agent, one which collects very few positive objects.\n9\nExperiment 2 - Training in a Deterministic Conﬁguration.\nIn this experiment, starting from the center of the grid, an agent has to collect positive objects while\navoiding negative ones, all positioned in the grid by following a deterministic pattern as illustrated in\nFigure 6a.\nThe results of this experiment, shown in Figure 7, indicate that SRL+CS learns a policy which is better\nthan that chosen by any of the other algorithms, faster than any of the other algorithms. SRL+CS\ncollects an average rate of more than 90% of positive objects. DQN did not achieve a reasonably good\nperformance in our experiments, probably because it requires a large number of hyper-parameter ﬁne\ntunning, which was not done extensively. In this same task, results reported in [5] show that DQN\ncan achieve a rate of almost 100% of positive objects collected, and that DSRL can achieve a rate of\n70% of positive objects collected, although accumulated scores were not reported.\n(a) Accumulated Scores for 10 runs of 1000 episodes\neach for each algorithm.\n(b) Rolling mean (using a window of ten episodes)\nof the average percentage of positive objects col-\nlected over 10 runs.\nFigure 7: Experiment 2 - Fig. 7(a) shows that SRL+CS learns faster to collect positive objects and\navoid negative objects. Some Q-learning curves start to show the same slope as SRL+CS but much\nlater. Fig. 7(b) shows that the rate of positive objects collected using SRL+CS is more than 90%.\nNotice that SRL+CS cannot reach 100% as an average due to the effect of 10% random exploration\nbuilt into the learning algorithm.\nExperiment 3 - Training in a Random Conﬁguration.\nIn this experiment, the positions of the objects to be placed on the grid are chosen randomly at\nthe start of each episode, except for the agent’s position, which is always the center of the grid, as\nexempliﬁed in Figure 6b.\nThe results presented in Figure 8 show that only SRL+CS is capable of learning an effective policy\nin this case. It suggests that SRL+CS can learn certain sub-tasks, despite the randomness of the\nenvironment, unlike the other algorithms. SRL+CS must have learned sub-tasks independently of\nwhere the objects were located, for example, move up if a positive object is above, even though\nSRL+CS is not provided with any explicit knowledge.\nExperiment 4 - Training in a Deterministic Conﬁguration and Testing in a Random Conﬁgu-\nration (Zero-shot Transfer Learning).\nIn this experiment, we evaluate the system’s ability to re-use knowledge learned in a situation (Fig.\n6a) in a different yet related situation (Fig. 6b). All systems were trained on the deterministic\nconﬁguration and tested on the random conﬁguration. During testing, there was no further learning\ninvolved; hence, there was no exploration either.\nThe results of this experiment (Figure 9) show that only SRL+CS is able to transfer its learning\ncorrectly to a similar situation; SRL+CS collects almost 100% of the positive objects in the random\nenvironment. Such results are impressive and deserve further investigation. Figure 9b shows that\nSRL collects only 15% of positive objects, but SRL achieves a higher score than Q-Learning (c.f.\nFigure 9a). This is because in many of the runs SRL did not collect any object at all, making the\naverage of the percentage of positive objects collected lower than a random walk (where the same\n10\n(a) Accumulated Scores for 10 runs of 1000 episodes\neach for each algorithm.\n(b) Rolling mean (using a window of ten episodes)\nof the average percentage of positive objects col-\nlected over 10 runs.\nFigure 8: Experiment 3 - Figs. 8(a) and 8(b) show that only SRL+CS learns an effective policy for\nthe random environment. This indicates that, independently of the conﬁguration of the environment,\nSRL+CS learns how to interact with the objects and to move correctly in order to collect as many\npositive objects as possible avoiding negative objects.\namount of negative and positive objects are collected). DQN and Q-learning were unable to transfer\nknowledge learned in the deterministic environment to the new situation. Additionally, DSRL was\nable to collect only 70% of positive objects.\n(a) Accumulated Scores for 10 runs of 1000 episodes\neach, for each algorithm. (*) Results for DQN and\nDSRL are not available.\n(b) Rolling mean (using a window of ten episodes)\nof the average percentage of positive objects col-\nlected over 10 runs. The results reported for DSRL\nand DQN are obtained from [5].\nFigure 9: Experiment 4 - In this zero-shot transfer learning experiment, SRL+CS does not receive\nany training in the random environment task, but is the only algorithm to solve the task.\nWe also ran other experiments training and testing SRL+CS in a number of different game conﬁgura-\ntions, achieving similar results as reported here (apart from one case where the possible movements\nof the agent are restricted by the environment, e.g. in a 3x2 environment with one positive and one\nnegative object). This is discussed next.\n5.3\nFurther Transfer Learning Experiments\nFinally, we run two transfer learning experiments using the three conﬁgurations shown in Figure 10.\nFor the purpose of highlighting in the plots if a negative object is collected, in these experiments a\ndifferent reward function is chosen. If a negative object is collected, a reward of -10 is obtained, and\nif a positive object is collected, a reward of +1 is obtained. We use 10 steps, instead of 100, as the\nmaximum number of steps in each episode, since the environments are much smaller now.\n11\nFigure 10 shows the three conﬁgurations used. The systems were trained in conﬁguration (a) and\ntested in conﬁgurations (b) and (c). Figure 11 shows the algorithms’ training performances in\nconﬁguration (a). Out of the 10 runs for each algorithm, the best models (the ones achieving the\nhighest scores) were selected for testing in conﬁgurations (b) and (c). Testing in these conﬁgurations\nwas done once per algorithm since the conﬁgurations were deterministic and there was no further\nlearning.\n(a) Train\n(b) Test\n(c) Test\nFigure 10: Initial conﬁgurations used in Experiments 5 and 6.\nFigure 11: Scores for training in conﬁguration (a), showing 10 runs for each algorithm, each run\ncontaining 1000 episodes. For all algorithms, apart from DQN, agents have learned to collect the\npositive object and to avoid the negative object. DQN, however, is quite inconsistent, although the\nbest DQN run, whose model is selected for the rest of the experiments, achieves performance similar\nto the other algorithms.\nExperiment 5 - Transferring Undefeasible Knowledge.\nIn this experiment, after training in conﬁguration (a) the agents were tested in conﬁguration (b).\nFigure 12 shows that DQN is unable to collect the positive objects, Q-learning shows a random\nbehavior, and the symbolic algorithms were able to generalize to the new situation, thus avoiding\nthe negative object while collecting the positive object. By inspecting conﬁgurations (a) and (b), a\nreasonable assumption is that the symbolic approaches can learn to move up and to the right, although\nno explicit knowledge is used (recall that Equations 3 and 4 are the only changes made to Q-learning).\nComparing conﬁgurations (a) and (c) though, it is clear that knowledge learned in (a) needs to be\nrevised in (c), as discussed next.\nExperiment 6 - Negative Transfer.\nIn this experiment, following training in conﬁguration (a), the systems are tested in conﬁguration (c).\nFigure 13 shows the results. Now, DQN does not collect any object, as before, Q-Learning walks\n12\nFigure 12: Experiment 5 - The symbolic algorithms are able to transfer learning correctly from\nenvironment (a) to environment (b), while Q-learning behaves randomly, and DQN never collects any\nobject.\nrandomly, as before, but SRL and SRL+CS collect the negative object before collecting the positive\nobject in every episode.\nExperiment 6 was the only case where SRL+CS failed to generalize. By inspecting conﬁgurations (a)\nand (c), it is clear that if an agent learns to move up and to the right then it shall collect the negative\nobject always in conﬁguration (c). This is a case of negative transfer, where the system has no means\nof revising the knowledge learned. Notice that the problem is not in the decision-making, which\nshould distinguish positive and negative objects, but in the learning phase. Symbolic RL algorithms\nuse Q-learning, which learns from (state, action) pairs. This may not be desirable, as seen here,\nleading to poor generalization. In other words, Q-learning cannot generalize over states only. Because\nof this, an agent will not learn to simply avoid a negative object (recall that during learning this agent\ndid not encounter a situation where it was below a negative object and executed the action of moving\nup). In many real-world situations, it should not matter how a state is reached, and the simple fact\nof being in a state can be sufﬁcient to make a decision on a relevant action. This limitation of the\ngeneralization capacity of Q-learning is well-known.\n6\nConclusion and Future Work\nIn this paper, we have presented a symbolic RL algorithm with features of common sense (SRL+CS)\nwhich can outperform Q-learning, DSRL and DQN in environments with deterministic and random\nconﬁgurations, and at transfer learning. Building on the knowledge representation proposed by deep\nsymbolic RL, SRL+CS can make two simple modiﬁcations to the standard Q-learning algorithm,\nimplementing common sense features such that attention can be focused normally on the objects that\nare of most relevance to an agent.\nSRL+CS is model free: the choice of an appropriate abstraction for the representation of the state\nspace allows for simple changes to learning and decision making which enhance the generalization\ncapacity of RL. The experiments reported in this paper were also run with only one of the two\nmodiﬁcations made at a time. Although each modiﬁcation has led to an improvement in performance,\nonly the combination of the two has produced the high levels of performance seen in all the tasks.\nThe results reported reiterate the value of the symbolic approach to RL, which through the use of\nsub-state abstractions can reduce the size of the state space considerably. However, we have shown\nthat two important issues arise: the assignment of rewards to sub-states during learning and the\naggregation of Q-values for decision making. SRL+CS addresses both issues adequately.\n13\nFigure 13: Experiment 6 - It this case of negative transfer, the symbolic algorithms fail, while\nQ-Learning behaves randomly, performing better than SRL and SRL+CS as a result, and DQN never\ncollects any object. In this experiment, SRL and SRL+CS share exactly the same behaviour of\ncollecting the negative object before collecting the positive object.\nVarious features of common sense have been argued to be relevant for achieving an adequate balance\nbetween generalization and specialization [2, 14]. Symbolic RL offers the possibility of adding to\nstandard RL features of common sense which go beyond those considered in this paper. Despite\nthe impressive results of SRL+CS at zero-shot learning, the experiments reported also highlight the\nlimitations of Q-learning at generalizing from state-action representations. As future work, therefore,\nwe aim to build an end-to-end SRL+CS architecture capable of learning common sense domain\nknowledge and applying it to solving more complex problems. A promising direction for further\ninvestigation is the interplay between SRL+CS and related work on the integration of planning and\nRL [9, 22, 25] and Relational RL [3, 18].\nReferences\n[1] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:\nInterpretable representation learning by information maximizing generative adversarial nets. In\nAdvances in Neural Information Processing Systems, pages 2172–2180, 2016.\n[2] Ernest Davis and Gary Marcus. Commonsense reasoning and commonsense knowledge in\nartiﬁcial intelligence. Communications of the ACM, 58(9):92–103, 2015.\n[3] Saso Dzeroski, Luc De Raedt, and Kurt Driessens. Relational reinforcement learning. Machine\nlearning, 43(1-2):7–52, 2001.\n[4] Artur S. d’Avila Garcez, Lus C. Lamb, and Dov M. Gabbay. Neural-Symbolic Cognitive\nReasoning. Springer Publishing Company, Incorporated, 1 edition, 2008.\n[5] Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards deep symbolic reinforcement\nlearning. arXiv preprint arXiv:1609.05518, 2016.\n[6] Artur d’Avila Garcez Ivan Donadello, Luciano Seraﬁni. Logic tensor networks for semantic\nimage interpretation. In Proceedings of the Twenty-Sixth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI-17, pages 1596–1602, 2017.\n[7] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In\nAdvances in Neural Information Processing Systems, pages 2017–2025, 2015.\n[8] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A\nsurvey. Journal of artiﬁcial intelligence research, 4:237–285, 1996.\n14\n[9] Ken Kansky, Tom Silver, David A Mély, Mohamed Eldawy, Miguel Lázaro-Gredilla, Xinghua\nLou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George.\nSchema net-\nworks: Zero-shot transfer with a generative causal model of intuitive physics. arXiv preprint\narXiv:1706.04317, 2017.\n[10] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building\nmachines that learn and think like people. Behavioral and Brain Sciences, 40, 2017.\n[11] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,\n2015.\n[12] Guiying Li, Junlong Liu, Chunhui Jiang, and Ke Tang. Relief impression image detection:\nUnsupervised extracting objects directly from feature arrangements of deep cnn. arXiv preprint\narXiv:1601.06719, 2016.\n[13] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.\n[14] John McCarthy. Programs with common sense. RLE and MIT Computation Center, 1960.\n[15] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n[16] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,\nabs/1312.5602, 2013.\n[17] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n[18] Matthias Nickles. Integrating relational reinforcement learning with reasoning about actions\nand change. In International Conference on Inductive Logic Programming, pages 255–269.\nSpringer, 2011.\n[19] Amaia Salvador, Xavier Giró-i Nieto, Ferran Marqués, and Shin’ichi Satoh. Faster r-cnn features\nfor instance search. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops, pages 9–16, 2016.\n[20] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of\ngo without human knowledge. Nature, 550(7676):354–359, 2017.\n[21] Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. One pixel attack for fooling deep\nneural networks. arXiv preprint arXiv:1710.08864, 2017.\n[22] Richard S Sutton. First results with dyna, an integrated architecture for learning, planning and\nreacting. Neural Networks for Control, pages 179–189, 1990.\n[23] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1.\nMIT press Cambridge, 1998.\n[24] S. N. Tran and A. S. d’Avila Garcez. Deep logic networks: Inserting and extracting knowledge\nfrom deep belief networks. IEEE Transactions on Neural Networks and Learning Systems,\nPP(99):1–13, 2016.\n[25] Harm Vanseijen and Rich Sutton. A deeper look at planning as learning from replay. In\nInternational Conference on Machine Learning, pages 2314–2322, 2015.\n[26] Christopher Watkins and Peter Dayan Q-learning. In Machine Learning 8:279-292, 1992.\n15\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML",
    "I.2.6"
  ],
  "published": "2018-04-23",
  "updated": "2018-04-23"
}