{
  "id": "http://arxiv.org/abs/1804.08881v1",
  "title": "Assessing Language Models with Scaling Properties",
  "authors": [
    "Shuntaro Takahashi",
    "Kumiko Tanaka-Ishii"
  ],
  "abstract": "Language models have primarily been evaluated with perplexity. While\nperplexity quantifies the most comprehensible prediction performance, it does\nnot provide qualitative information on the success or failure of models.\nAnother approach for evaluating language models is thus proposed, using the\nscaling properties of natural language. Five such tests are considered, with\nthe first two accounting for the vocabulary population and the other three for\nthe long memory of natural language. The following models were evaluated with\nthese tests: n-grams, probabilistic context-free grammar (PCFG), Simon and\nPitman-Yor (PY) processes, hierarchical PY, and neural language models. Only\nthe neural language models exhibit the long memory properties of natural\nlanguage, but to a limited degree. The effectiveness of every test of these\nmodels is also discussed.",
  "text": "Assessing Language Models with Scaling Properties\nShuntaro Takahashi1†, Kumiko Tanaka-Ishii2‡*\n1† The University of Tokyo, Graduate School of Frontier Sciences\n2‡* The University of Tokyo, Research Center for Advanced Science and Technology\nApril 25, 2018\nAbstract\nLanguage models have primarily been evaluated with perplexity. While perplexity quantiﬁes\nthe most comprehensible prediction performance, it does not provide qualitative information on the\nsuccess or failure of models. Another approach for evaluating language models is thus proposed,\nusing the scaling properties of natural language.\nFive such tests are considered, with the ﬁrst\ntwo accounting for the vocabulary population and the other three for the long memory of natural\nlanguage. The following models were evaluated with these tests: n-grams, probabilistic context-free\ngrammar (PCFG), Simon and Pitman-Yor (PY) processes, hierarchical PY, and neural language\nmodels. Only the neural language models exhibit the long memory properties of natural language,\nbut to a limited degree. The eﬀectiveness of every test of these models is also discussed.\n1\nIntroduction\nThe performance of language models has generally been evaluated with perplexity (Manning and\nSchutze 1999), which quantiﬁes the predictive power of such models. Since the development of n-\ngram models, state-of-the-art neural language models have radically reduced perplexity. Although\nperplexity is easy to compute and its signiﬁcation is comprehensible, we consider it to have two main\ndrawbacks. First, perplexity does not provide information on how a model is limited with respect\nto the important linguistic aspects, such as generalization of word concepts, syntactic structure, and\nlong-term dependency. Although perplexity indicates the overall performance across various aspects,\nwe want to know more speciﬁcally which aspects are challenging for the language models. Second,\nwe cannot obtain a lower bound of perplexity in a dataset and thus cannot quantitatively recognize\nthe diﬀerence of a model from natural language or an ideal language model that perfectly reproduces\nnatural language.\nTo compensate for these drawbacks of perplexity, we propose to assess language models with a\nset of scaling properties exhibited by natural language. The assessment is conducted by investigating\nwhether the text generated by a language model would exhibit these scaling properties. We presented\nthis scheme in (Takahashi and Tanaka-Ishii 2017) in which the reproducibility of the character level\nneural language models are investigated. This assessment has two advantages over the perplexity\nmeasure. First, since the scaling properties are designed to measure some aspect underlying a sequence,\nthey can provide information on how limited a model is in terms of that aspect. Second, since the\nscaling properties quantify the behavior of a data set in terms of exponents, we can evaluate how much\nthe text generated from a model diﬀers from natural language.\nThe scaling properties in this article are roughly categorized into two types: those related to word\nfrequency distribution and those related to long memory. For the ﬁrst category, we consider the well-\nknown Zipf’s law and Heaps’ law. For the second category, we consider properties that quantify the\nmemory underlying a sequence. We then test whether texts generated from various language models\nexhibit every property.\n1\narXiv:1804.08881v1  [cs.CL]  24 Apr 2018\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Corr.\nFigure 1: Five scaling properties for wikitext-2 (WT2).\n2\nRelated Work\nMany studies of language models have considered Zipf’s and Heaps’ laws. The laws concern the word\nfrequency distribution, characterizing how a large part of vocabulary consists of rare words, causing\nproblems of sparseness and unknown words. The Chinese restaurant process, as a kind of PY process\n(Pitman 2006), was introduced partly because it satisﬁes Zipf’s and Heaps’ law (Goldwater et al. 2011;\nTeh 2006). In the recent works on neural language models, application of the two laws was considered\nto improve the architecture. Jozefowicz et al. (2016) evaluated models in terms of word frequencies\nand concluded that a neural model is more capable of predicting rare words than is a Kneser-Ney\nlanguage model (Kneser and Ney 1995). Merity et al. (2016) constructed a new dataset, wikitext-2\n(WT2) for processing rare words with respect to the word frequency distribution, and it has become\na standard dataset.\nLong-range dependency has mainly been explored in studying the architecture of neural networks.\nAfter the ﬁrst neural language model was proposed (Bengio et al. 2003), Mikolov et al. (2010) ﬁrst\nintroduced recurrent neural networks (RNNs) to language modeling, with the potential to maintain\nlong-term dependency. In more recent works, Merity et al. (2016) introduced pointer networks (Vinyals\net al. 2015) to the task and demonstrated that they improve perplexity on the regular long short-term\nmemory (LSTM) language model. Grave et al. (2017) integrated a cache model designed to sample\nfrom context to model the observation of the clustering behavior of certain words.\nMerity et al.\n(2018) investigated the eﬀect of a cache model in terms of the diﬀerence in log perplexity of each\nword. These analysis methodologies for long-term dependency, however, have not been formalized\nand instead rely on case studies. Lin and Tegmark (2017) investigated how an LSTM-like architecture\ncould reproduce the power decay of mutual information. Although their argument is theoretically\nvalid, natural language does not exhibit power decay of mutual information. Our interest is therefore\nto introduce better measures of the long-range dependency of natural language.\n2\n3\nScaling Properties\nThis section provides a summary of the scaling properties of natural language and deﬁnes how to\nevaluate word or character sequences with them. Nine scaling laws of natural language are acknowl-\nedged (Altmann and Gerlach 2017). Some of them deal with word formation and network structure\nand do not directly relate to language modeling. This leaves ﬁve other scaling properties that are\nstill mathematically closely related. We can roughly categorize them into those based on vocabulary\npopulation and long memory. The following subsections proceed through the example of applying\nthese scaling properties with WT2 (Merity et al. 2016), as shown in Figure 1.\nWe test language models by generating text from them and evaluating the scaling properties. In\nthis article, there are two levels of judgment for these properties.\nQ1 Does the scaling property hold qualitatively?\nQ2 How diﬀerent is the exponent?\nAs revealed in the following subsections, many language models fail to satisfy even the ﬁrst criterion for\nsome properties. For models that satisfy Q1, their exponents are estimated and compared with those\nof the original text. Consider a power law y ∝zκ for points (y1,z1),. . .,(yN,zN). The exponent κ is esti-\nmated by the least-squares method, i.e., ˆκ = arg minκ ε(κ), where ε(κ) ≡\nqPN\ni=1(log yi −log zκ\ni )2/N.\nThe error reported here is the average error per point, i.e., ε(ˆκ).\n3.1\nZipf’s Law and Heaps’ Law\nGiven a text, let r be the rank of a particular word type and f(r) be its frequency. The well-known\nZipf’s law formulates a power-law relation between the rank and frequency:\nf(r) ∝r−α,\nα ≈1.0.\n(1)\nIn fact, this scaling generally holds not only for unigrams but also for n-grams, with smaller α. The\nﬁrst left graph in Figure 1 shows Zipf distributions for WT2, with unigrams in red and bigrams in blue.\nBecause WT2 replaces rare words having frequencies under a certain threshold with <unk>, the tail of\nthe unigram distribution disappears. The Zipf distributions for unigrams and bigrams typically cross\nin the middle. In reality, plots for real natural language texts are often not aligned linearly, making\nthe exponent diﬃcult to estimate. Previous works have dealt with this problem, but it is beyond the\nscope of this article. In this work, we therefore do not estimate α either.\nHeaps’ law is another scaling property and shows how vocabulary grows with text size, forming a\npower-law distribution. Let n be the length of a text and v(n) be its vocabulary size. Then Heaps’\nlaw is formulated as the following relation:\nv(n) ∝nβ,\n0 < β < 1.\n(2)\nThe second graph in Figure 1 shows the result for WT2. The exponent here is 0.75, which is smaller\nthan 1.0 (dashed black line), with ε = 0.13. Because of the replacement of unknown words in WT2, the\nplot exhibits convex growth. Also, note that Heaps’ law can be deduced from Zipf’s law (BaezaYates\nand Navarro 2000; van Leijenhorst and van der Weide 2005; L¨u et al. 2010).\n3.2\nLong Memory\nThe statistical mechanics domain has introduced two directions for considering long memory: ﬂuctu-\nation analysis and long-range correlation. Here, we introduce two ﬂuctuation analysis methods, one\nfor characters and one for words, and one long-range correlation method, applied to words. Although\n3\nthese methods are related analytically for a well-formed time series, for real phenomena with ﬁnite-size\nsamples, the relations are non-trivial. The generated texts could behave similar to a random sequence\nor contrarily exhibit better long memory than a natural language text.\n3.2.1\nFluctuation Analysis\nFluctuation analysis quantiﬁes the strength of memory and the degree of symbol occurrence burstiness\nunderlying a text.\nEbeling’s Method\nBurstiness has been known to occur in various natural and social domains.\nFluctuation analysis\noriginated in (Hurst 1951), motivated by the need to quantify the degree of burstiness of Nile River\nﬂoods. The method applies only for numerical series, so (Montemurro and Pury 2002) applied it for\ntexts by transforming a word sequence into a rank sequence, which obscures the results.\nOne work (Ebeling and P¨oschel 1994) applied a simple ﬂuctuation analysis method on text. That\nwork showed how the variance of characters grows by a power law with respect to the text length (i.e.,\ntime span). Given a set of elements |W| (characters in this method), let y(k, l) be the number of the\nck ∈W within text length l. Then,\nm(l) =\n|W|\nX\nk=1\nm2(k, l) ∝lη,\n(3)\nwhere m2(k, l) is the variance of y(k, l):\nm2(k, l) =< y2(k, l) > −(< y(k, l) >)2.\n(4)\nTheoretically, if the time series is independent and identically distributed (i.i.d.), then η = 1.0. (Ebel-\ning and P¨oschel 1994) showed that the Bible has η = 1.69, thus exhibiting larger ﬂuctuation than an\ni.i.d. sequence. This article successfully demonstrates that a character sequence has long memory.\nThe third graph in Figure 1 shows m(l) for WT2. The exponent is 1.32 with ε = 0.10.\nTaylor’s Law\nWhile Ebeling’s method considers the growth of variance with respect to a subsequence of length l,\nTaylor’s method considers that growth with respect to the mean within length l. Because the mean of\na subsequence linearly correlates with l, the two methods are closely related. While Ebeling’s method\nsums the variance over all elements of W, Taylor’s method estimates the scaling exponent from the\npoints of all words. Because of this diﬀerence, Ebeling’s method is not applicable for detecting long\nmemory underlying a word sequence: the exponent becomes 1.0 in this case, like an i.i.d. sequence.\nIn general, Taylor’s law is more robust and is applicable to words and also to characters, if for a large\nset size.\nTaylor’s law was originally reported in two pioneering works (Taylor 1961; Smith 1938) and has\nbeen applied in various domains as reported in (Eisler et al. 2007). Its application for language data,\nhowever, has been scarce. The only such study so far is (Gerlach and Altmann 2014). However the\nwork considered vocabulary size rather than word occurrence, and its diﬀerences from the original\nTaylor analysis make theoretical interpretation inapplicable. We introduce another method proposed\nby Kobayashi and Tanaka-Ishii (2018) with which we can quantify the long range dependence of\nnatural language and symbolic time-series in general and is highly interpretable.\nGiven a text produced from a set of words, W, for a given segment size l, the number of occurrences\nof a particular word wk ∈W is counted and the mean µk and standard deviation σk are calculated\n4\nfor wk. Doing this for all elements of W gives the distribution of σ with respect to µ. Taylor’s law\nthen holds when σ and µ are correlated by a power law:\nσ ∝µζ.\n(5)\nExperimentally, the Taylor exponent ζ is known to range over 0.5 ≤ζ ≤1.0.\nThe advantage of Taylor’s law over the other analysis methods for long memory is the interpretabil-\nity of the exponent ζ. The two limit values ζ = 0.5, 1.0 provide a clear interpretation. For an i.i.d.\nprocess, it is proved that ζ = 0.5. For a sequence in which all segments of length l contain the same\nproportions of the elements of W, ζ = 1.0. For example, given W = {a, b}, suppose that b always\noccurs twice as often as a in all segments (e.g., one segment with three a and six b, another segment\nwith one a and two b, etc.). Then, both the mean and standard deviation for b are twice those for a,\nand therefore ζ = 1.0. The Taylor exponent thus quantiﬁes how consistently words co-occur in texts.\nFor this reason, a set of tweets from the same source or CHILDES texts would typically exhibit a\nhigher exponent ζ.\nThe fourth graph in Figure 1 shows the result for WT2 with l = 5620 (l can be any value larger\nthan one). The plot exhibits a power law, although some deviation from the regression line is also\nvisible. The Taylor exponent is ζ = 0.62, with ε = 0.15.\n3.2.2\nLong-Range Correlation\nBurstiness of word occurrence leads to a related observation: how subsequences are similar. Such\nthinking resulted in another genre of analysis methods, called long-range correlation analysis, which\nmeasures the self-similarity within two subsequences of a time series.\nA time series is said to be long-range correlated if the correlation function c(s) for two subsequences\nseparated by distance s follows a power law:\nc(s) ∝s−ξ,\ns > 0, 0 < ξ < 1.\n(6)\nA widely used choice for c(s) is the autocorrelation function (ACF). By using the ACF, the value of\nc(s) ranges between -1 and 1. When a sequence is long-range correlated, c(s) takes positive values for\ns until about 1/100 of the length (Lennartz and Bunde 2009), whereas c(s) ﬂuctuates around zero for\na sequence without temporal correlation.\nSince the ACF is applicable only for numerical time series, application of this method for natural\nlanguage requires transforming a natural language text into a numerical time series. Among recent\nreports (Altmann et al. 2009: 2012; Tanaka-Ishii and Bunde 2016), we apply the most recent, a rare\nword clustering method with parameter Q = 16 (Tanaka-Ishii and Bunde 2016).\nApplication of long-range correlation analysis to word sequences in WT2 produced the last graph\nin Figure 1. As noted in the legend, ξ = 0.33 and ε = 0.04 with c(s) is all positive up to 1/100 of the\nsequence length. Throughout this article, for ε of this metric only, it is measured for s ≤100.\n4\nLanguage Models\nThis section explains the language models used in this article.\nEach model deﬁnes a probability\ndistribution P(xt+1), given Xt\n1 = x1, x2, . . . , xt. We probabilistically generated texts following the\noutput distributions of these models. The all graphs for the scaling properties of the language models\nare available as supplementary material.\n5\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Corr.\nFigure 2: Scaling properties of HPYLM trained with WT2.\n4.1\nN-gram Model\nAn n-gram language model is the most basic model, as it is a Markov model of order n −1. This\narticle considers a 3-gram model and 5-gram model with back-oﬀ. Note that the perplexity of the\nmodels here should tend to be lower than in previous reports, as we did not adopt the <BOS> and\n<EOS> tags to maintain inter-sentence structure.\n4.2\nGrammatical Model\nProbabilistic context-free grammar (PCFG) is the most basic grammatical model. We constructed this\ngrammar model with the annotated Penn Tree Bank (PTB) dataset and used the Natural Language\nToolkit (NLTK) to generate sentences according to the probabilities assigned to productions. Unlike\nan n-gram model, a PCFG ensures the grammatical correctness of all productions.\n4.3\nSimon/Pitman-Yor Models\nThe Simon and Pitman-Yor (PY) processes are important for our perspective, because they are capable\nof reproducing Zipf’s law and Heaps’ law with simple formulations. It is thus interesting to see whether\nthey satisfy the other scaling properties.\nThese are generative models, and a sequence is formulated over time, either through (1) introduc-\ntion of new words or (2) sampling from the past sequence. Let K(Xt\n1) be the number of word types\nexisting in Xt\n1, and let nk(Xt\n1) be the frequency of the kth word type in Xt\n1. The sequence starts with\nK(X0) = 1 and X0 = x0 at t = 0.\nFor t ≥1, given a constant a with 0 < a < 1, the Simon process (Simon 1955) introduces a new\n6\n(a) Zipf’s Law\n(b) Heaps’ Law\n(c) Ebeling’s Method\n(d) Taylor’s Law\n(e) Long-Range Corr.\nFigure 3: Scaling properties of AWD-LSTM-Cache model trained with WT2.\nword with probability a, or a word is sampled from Xt\n1 with probability 1 −a:\nP(xt+1 = wk) =\n\u001a\n(1 −a)\nnk(Xt\n1)\nt\n1 ≤k ≤K(Xt\n1)\na\nk = K(Xt\n1) + 1 .\nThe Simon process strictly follows both Zipf’s law and Heaps’ law with an exponent of 1.0. The\nPY process copes with this problem by decreasing the introduction rate of new words in proportion\nto K(Xt\n1) via another parameter b, with 0 ≤a < 1 and 0 ≤b:\nP(xt+1 = wk) =\n\u001a\nnk(Xt\n1)−a\nt+b\n1 ≤k ≤K(Xt\n1)\naK(Xt\n1)+b\nt+b\nk = K(Xt\n1) + 1 .\nThese two parameters would serve to produce Zipf’s law with slightly convex behavior (Goldwater\net al. 2011). The basic models introduced thus far deﬁne nothing about how to introduce words:\nwe would simply generate random sequences and examine their scaling properties, because the basic\nformulation thus far governs the nature of the language model elaborated from these models. By\nmapping a word to the elements produced, however, we would generate a language model, like the\ntwo-stage model proposed in (Goldwater et al. 2011).\nHere, we consider a more advanced model\nproposed as the hierarchical Pitman-Yor language model (HPYLM)(Teh 2006), which integrates the\nPitman-Yor process into an n-gram model.\n4.4\nNeural Language Models\nThe predictive performance of state-of-the-art language models improved radically with neural lan-\nguage models. The majority of promising neural language models (Mikolov and Zweig 2012; Melis\net al. 2018; Merity et al. 2018; Yang et al. 2018) adopt recurrent neural networks (RNNs). The RNNs\ncompute a hidden state ht from the input xt and the previous hidden state ht−1 to incorporate past\n7\ninformation eﬀectively:\nht = Φ(xt, ht−1).\n(7)\nThe function Φ depends on the recurrent architecture of the network. This article focuses on LSTM\n(Hochreiter and Schmidhuber 1997), because the diﬀerence in performance is insigniﬁcant among\narchitectures such as the gated recurrent unit (GRU) (Cho et al. 2014) and other LSTM variants\n(Chung et al. 2014; Greﬀet al. 2017; Melis et al. 2018). A total of six neural language models are\nconsidered.\nThe ﬁrst model is an LSTM language model that is not trained with regularization techniques.\nThe predictive performance is equivalent or better than with n-gram models but signiﬁcantly worse\nthan with state-of-the-art models. This model therefore could be considered as a baseline to verify\nwhether further regularization techniques contribute to reproducing the scaling properties.\nFor the advanced models, we adopted averaged stochastic gradient descent (ASGD) weight-dropped\nLSTM, or simply AWD-LSTM (Merity et al. 2018), because it consists of a standard architecture\n(embedding + LSTM + softmax) yet outperforms many other models in terms of perplexity.\nIn\naddition to the standard AWD-LSTM model, two other architectures are considered: continuous\ncache (Grave et al. 2017) and mixture of softmax (MoS) (Yang et al. 2018).\nContinuous cache is a memory augmentation architecture that computes a cache probability pcache\nfrom context wt\nt−l, where l is a window size parameter. It computes the similarity between ht and\nhi to estimate the reappearance of wi at t + 1. The output probability of the model with continuous\ncache, denoted as the AWD-LSTM-Cache model, is a linear interpolation of the AWD-LSTM and the\ncache probability. We also considered a model incorporating the Simon process, denoted as the AWD-\nLSTM-Simon model. It behaves as a uniform sampling from the past generated sequence and is a\nspecial case of AWD-LSTM-Cache. MoS reformulates the language model task as matrix factorization\nand is a state-of-the-art language model integrated with AWD-LSTM as the AWD-LSTM-MoS model.\nWe also considered a combination of all these architectures, the AWD-LSTM-MoS-Cache model.\nIn our experiments, all of the language models are trained to minimize the negative log-likelihood\nof the training data by stochastic gradient algorithms. The window size l for the AWD-LSTM-Simon\nmodel is set to 10,000 to balance a large window size with computation eﬃciency.\n5\nScaling Properties of Language Models\nWe chose two standard datasets, WT2 and PTB, for training language models. Table 1 and Table 2\nlist the overall results for WT2 and PTB, respectively.\nEach table contains ﬁve blocks, with the\nﬁrst indicating the properties of the original datasets with and without preprocessing. The remaining\nblocks indicate the results for the language models. Every row gives the results for a generated text\nof 1 million words.\nThe two rows for the Simon and PY processes appear only in the table for\nWT2, because they represent non-linguistic random sequences obtained by the deﬁnitions given in\n§4.3, and no real data learning is involved. All the other rows show the results of the corresponding\nlanguage model learning either the WT2 or PTB dataset. The datasets were preprocessed to reduce\nthe vocabulary size. Infrequent words were replaced with <unk>, and numbers were replaced with N\nin PTB.\nThe ﬁrst column lists the perplexity measured by the model. For some models, perplexity does\nnot apply, as indicated by the symbol “-”. The perplexity grows with the Simon and PY models, and\nthat of the PCFG cannot be compared under the same criteria.\nFor the Zipf’s law column, when the law holds, this is indicated by “Yes”. The reason is that,\nas mentioned in §3.1, the exponent is often diﬃcult to estimate. Ebeling’s method is not applicable\nto the Simon and PY processes without a word generation model. For the other models, when an\nexponent was obtained, we checked whether the value was larger than 1.0 (the i.i.d. case). Similarly,\n8\nTable 1: Summary of scaling properties of language models with WT2\nPerplexity\nVocabulary Population\nLong Memory\nZipf’s\nHeaps’\nEbeling’s\nTaylor’s\nLong Range\nLaw\nLaw\nMethod\nLaw\nCorrelation\nf(r) ∝r−α\nv(n) ∝nβ\nm(l) ∝lη\nσ ∝µζ\nc(s) ∝s−ξ\nOriginal Dataset\nWikitext-2\n-\nYes\n0.75 (0.13)\n1.32 (0.10)\n0.62 (0.15)\n0.33 (0.04)\nWikitext-2-raw\n-\nYes\n0.78 (0.09)\n1.33 (0.10)\n0.65 (0.11)\n0.32 (0.03)\nN-gram Language Model\n3-gram\n195.54\nYes\n0.78 (0.13)\n1.01 (0.01)\n0.50 (0.02)\nNo\n5-gram with back-oﬀ\n181.75\nYes\n0.78 (0.13)\n1.00 (0.01)\n0.50 (0.02)\nNo\nGrammatical Model\nPCFG(PTB)\n-\nYes\n0.73 (0.19)\n1.00 (0.00)\n0.50 (0.02)\nNo\nSimon and Pitman-Yor Family\nSimon (a = 0.1)\n-\nYes\n0.95 (0.15)\n-\n0.50 (0.01)\n0.09 (0.03)\nPitman-Yor (a = 0.8,b = 1.0)\n-\nYes\n0.78 (0.09)\n-\n0.50 (0.01)\nNo\nHPYLM\n187.10\nYes\n0.78 (0.13)\n1.00 (0.00)\n0.50 (0.02)\nNo\nNeural Language Model\nLSTM (no regularization)\n113.18\nYes\n0.78 (0.12)\n1.10 (0.03)\n0.52 (0.03)\n0.43 (0.15)\nAWD-LSTM\n64.27\nYes\n0.76 (0.13)\n1.30 (0.15)\n0.58 (0.06)\n0.05 (0.01)\nAWD-LSTM-Simon\n61.59\nYes\n0.77 (0.10)\n1.25 (0.15)\n0.55 (0.05)\n0.03 (0.01)\nAWD-LSTM-MoS\n62.44\nYes\n0.78 (0.12)\n1.16 (0.07)\n0.54 (0.04)\n0.33 (0.07)\nAWD-LSTM-MoS-Cache\n59.21\nYes\n0.78 (0.11)\n1.20 (0.07)\n0.57 (0.07)\n0.29 (0.05)\nAWD-LSTM-Cache\n50.39\nYes\n0.78 (0.11)\n1.25 (0.10)\n0.59 (0.07)\n0.14 (0.04)\nTable 2: Summary of scaling properties of language models with PTB\nPerplexity\nVocabulary Population\nLong Memory\nZipf’s\nHeaps’s\nEbeling’s\nTaylor’s\nLong Range\nLaw\nLaw\nMethod\nLaw\nCorrelation\nf(r) ∝r−α\nv(n) ∝nβ\nm(l) ∝lη\nσ ∝µζ\nc(s) ∝s−ξ\nOriginal Dataset\nPenn Tree Bank\n-\nYes\n0.70 (0.16)\n1.23 (0.06)\n0.56 (0.14)\n0.81 (0.24)\nPenn Tree Bank-raw\n-\nYes\n0.83 (0.07)\n1.20 (0.05)\n0.57 (0.06)\n0.60 (0.16)\nN-gram Language Model\n3-gram\n126.71\nYes\n0.71 (0.19)\n1.00 (0.00)\n0.50 (0.02)\nNo\n5-gram with back-oﬀ\n112.87\nYes\n0.71 (0.19)\n1.01 (0.01)\n0.50 (0.02)\nNo\nGrammatical Model\nPCFG\n-\nYes\n0.73 (0.19)\n1.00 (0.00)\n0.50 (0.02)\nNo\nSimon and Pitman-Yor Family\nHPYLM\n144.41\nYes\n0.73 (0.21)\n1.00 (0.00)\n0.50(0.02)\nNo\nNeural Language Model\nLSTM (no regularization)\n111.79\nYes\n0.71 (0.19)\n1.04 (0.01)\n0.51 (0.02)\n0.84 (Weak)\nAWD-LSTM\n56.40\nYes\n0.71 (0.18)\n1.06 (0.02)\n0.51 (0.03)\n0.69 (Weak)\nAWD-LSTM-Simon\n57.85\nYes\n0.72 (0.16)\n1.04 (0.01)\n0.51 (0.03)\nNo\nAWD-LSTM-MoS\n54.77\nYes\n0.71 (0.18)\n1.10 (0.03)\n0.52 (0.04)\n0.77 (Weak)\nAWD-LSTM-MoS-Cache\n54.03\nYes\n0.71 (0.18)\n1.13 (0.04)\n0.55 (0.06)\n0.61 (Weak)\nAWD-LSTM-Cache\n52.51\nYes\n0.72 (0.17)\n1.07 (0.02)\n0.53 (0.05)\n0.57 (Weak)\nfor Taylor’s law, we checked whether the exponent was larger than 0.5. If the text is not long-range\ncorrelated, this is mentioned by “No” or “Weak”: “No” if more than one value was negative for s ≤10,\nor “Weak” for s ≤100.\nThe two tables do not indicate qualitative diﬀerences except for the cells in the last block of the\nlast column: for other cells, when a scaling property held for one, it also held for the other. The main\ndiﬀerence occurred because of a diﬀerence in dataset quality: PTB (The Wall Street Journal) consists\nof short articles, whereas WT2 (Wikipedia) consists of longer articles written coherently with careful\n9\nFigure 4: Values of the Taylor exponent and perplexity for all the language models and the real data of\nWT2. When the perplexity is unmeasurable (for the original WT2 data and some language models),\nno bar appears. The results show that the two measures are correlated, but not totally among the\nmost advanced models.\npreprocessing. WT2 therefore has stronger long memory than PTB, which is apparent from their long\nmemory properties.\nThe language models listed above the neural language models in the tables failed to reproduce\nall of the scaling properties for long memory. Figure 2 shows a set of graphs for the HPLYM: apart\nfrom Zipf’s and Heaps’ laws, this model did not exhibit any long memory. The n-gram models and\nPCFG had similar tendencies.\nThe sole exception was the Simon model, which presented strong\nlong-range correlation, even stronger than that of the original WT2. Its Taylor exponent, however,\nwas 0.5, suggesting its essential diﬀerence from a natural language text. This is obvious, in a way,\nbecause a sequence produced by the Simon model has a diﬀerent mathematical nature with respect to\nvocabulary population for small t and large t, which is not the case for natural language. Curiously,\nthis long memory was destroyed with the PY models and HPLYM. Overall, the Simon- and PY-based\nmodeling might not be adequate for natural language.\nIn contrast, for WT2, the neural language models were able to reproduce all of the scaling prop-\nerties. Figure 3 shows a set of graphs for the AWD-LSTM-Cache model, for WT2. The ﬁgure shows\nhow well the model captures the nature of the original text in terms of every property.\nThe analysis, however, suggests the possibility of further improvement. Even though the AWD-\nLSTM-Cache model performed the best for WT2 with respect to perplexity, its Taylor exponent ζ\nstill remained smaller (0.59) than that of the original text (0.62). Veriﬁcation of the actual generated\n10\ntext showed that this model had a tendency to repeat words locally. Moreover, the neural language\nmodels failed to reproduce long memory for the PTB dataset. Although the long memory underlying\nthe original PTB is weaker than that of WT2, PTB still has long memory, and yet the models could\nonly reproduce “weak” long-range correlation. This and all the above ﬁndings suggest directions for\nimproving the neural language models.\nAt the same time, the discussion highlights the importance of the dataset used for evaluation:\nPTB is less suitable for evaluating long memory, while WT2 is recommended for this purpose.\n6\nEﬀectiveness of Scaling Properties for Examining Language Mod-\nels\nThe results thus far also enable discussion from another viewpoint, of considering which scaling prop-\nerty would most eﬀectively evaluate language models. Most important is the question in the article’s\ntitle: “Is perplexity suﬃcient?”. We might ask whether model quality correlates with proximity to\nthe exponent for a dataset, and which scaling property is the most eﬀective for evaluation. We thus\nexamine every property from this viewpoint.\nFirst, Zipf’s law does not add exploitable information on the quality of a language model, because\nits exponent is diﬃcult to estimate, as discussed in §3.1, and because all the models exhibited behavior\nlike Zipf’s law.\nStill, the Zipf’s law distribution for WT2 in Figure 1 showed a drop at the tail,\nindicating that something unnatural occurred (i.e., replacement of rare words with <unk>). Therefore,\nZipf’s law could be applied just to verify whether a vocabulary population is normal. Since Heaps’\nlaw is derived from Zipf’s law, it also does not contribute much to evaluating language models.\nTurning to long memory, the three properties roughly have positive correlation: when the original\ndataset had weaker long memory (PTB as compared with WT2), the degree of memory exhibited by\nthe neural language models was also weaker. Ebeling’s method is limited in that it only applies for\ncharacters. The long-range correlation also has a limited capacity, because the Simon model exhibited\nstrong long-range correlation but had a Taylor exponent of 0.5. Taylor’s law therefore seems more\ncredible for evaluating model quality. Figure 4 shows the values of the Taylor exponent and perplexity\nfor every language model.\nThe ﬁgure indicates that they correlated well with diﬀerences for the\nmost advanced neural language models. It highlights some aspects of the model characteristics. For\nexample, it is likely that the long memory quality for WT2 was degraded by MoS, even though it\nimproved the perplexity from AWD-LSTM. These results demonstrate that evaluation of language\nmodels from multiple viewpoints would contribute to better understanding of the nature of learning\ntechniques, and scaling properties can provide these diﬀerent viewpoints.\n7\nConclusion\nWe explored the performance of language models with respect to the scaling properties of natural\nlanguage and proposed evaluation methods other than perplexity. We listed ﬁve such tests, two for\nvocabulary population and three for long memory. For vocabulary population, all models considered\nhere presented the scaling property. In contrast, many models did not exhibit long memory, but the\nstate-of-the-art neural language models were able to produce a sequence with long memory. This\nconstitutes solid evidence of the diﬀerence between n-gram and neural language models. Moreover,\nthe diﬀerence between the exponents from a model and from the original dataset shows the possibility\nof improvement.\nFurther veriﬁcation along the same line using raw data and other datasets remains for our future\nwork. We also intend to investigate other kinds of metrics for model assessment.\n11\nReferences\nAltmann, E.G. and Gerlach, M. (2017). Statistical laws in linguistics. Creativity and Universality in\nLanguage, pages 7–26.\nAltmann, E.G. , Pierrehumbert, J.B. , and Motter, E.A. (2009). Beyond word frequency: Bursts,\nlulls, and scaling in the temporal distributions of words. PLoS ONE, 4(11), e7678.\nAltmann, E.G. , Cristadoro, G. , and Esposti, M.D. (2012). On the origin of long-range correlations\nin texts. Proceedings of the National Academy of Sciences, 109(29), 11582–11587.\nBaezaYates, R. and Navarro, G. (2000).\nBlock addressing indices for approximate text retrieval.\nJournal of the American Society for Information Science, 51(1), 6982.\nBengio, J. , Ducharme, R. , Vincent, P. , and Jauvin, C. (2003). A neural probabilistic language\nmodel. Journal of Machine Learning Research, (3), 1137–1155.\nCho, K. , van Merri¨enboer, B. , G¨ul¸cehre, C¸. , Bahdanau, D. , Bougares, F. , Schwenk, H. , and Ben-\ngio, Y. (2014). Learning phrase representations using rnn encoder–decoder for statistical machine\ntranslation. In Empirical Methods in Natural Language Processing, pages 1724–1734.\nChung, C. , G¨ul¸cehre, C¸. , Cho, K. , and Bengio, Y. (2014). Empirical evaluation of gated recurrent\nneural networks on sequence modeling. In Neural Information Processing Systems Workshop.\nEbeling, W. and P¨oschel, T. (1994). Entropy and long-range correlations in literary english. Euro-\nphysics Letters, 26(4), 241–246.\nEisler, Z. , Bartos, I. , and Kert’esz, J. (2007). Fluctuation scaling in complex systems: Taylor’s law\nand beyond. Advances in Physics, 57(1), 89–142.\nGerlach, M. and Altmann, E.G. (2014). Scaling laws and ﬂuctuations in the statistics of word fre-\nquencies. New Journal of Physics, 16(11), 113010.\nGoldwater, S. , Griﬃths, Thomas L. , and Johnson, M. (2011). Producing power-law distributions and\ndamping word frequencies with two-stage language models. Journal of Machine Learning Research,\n12, 2335–2382.\nGrave, E. , Joulin, A. , and Usunier, N. (2017). Improving neural language models with a continuous\ncache. In International Conference on Learning Representations.\nGreﬀ, K. , Srivastava, R. K. , Koutnk, J. , Steunebrink, B. R. , and Schmidhuber, J. (2017). LSTM:\nA Search Space Odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10),\n2222–2232.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8),\n1735–1780.\nHurst, H.E. (1951). Long-term storage capacity of reservoirs. Transactions of the American Society\nof Civil Engineers, 116(1), 770–808.\nJozefowicz, R. , Vinyals, O. , Schuster, M. , Shazeer, N. , and Wu, Y. (2016). Exploring the limits of\nlanguage modeling. arXiv preprint, arXiv:1602.02410.\nKneser, R. and Ney, H. (1995).\nImproved backing-oﬀfor m-gram language modeling.\nIn IEEE\nConference on Acoustics, Speech and Signal Processing, volume 1, pages 181–184.\n12\nKobayashi, T. and Tanaka-Ishii, K. (2018). Taylor’s law for human linguistic sequences. arXiv preprint,\narXiv:1804.07893.\nLennartz, S. and Bunde, A. (2009).\nEliminating ﬁnite-size eﬀects and detecting the amount of\nwhitenoise in short records with long-term memory. Physical Review E, 79(6), 066101.\nLin, H. W. and Tegmark, M. (2017). Critical behavior in physics and probabilistic formal languages.\nEntropy, 19(7), 299.\nL¨u, L. , Zhang, Z.K. , and Zhou, T. (2010). Zipfs law leads to heaps’ law: Analyzing their relation in\nﬁnite-size systems. PLoS ONE, 5(12), e14139.\nManning, C.D. and Schutze, H. (1999). Foundations of Statistical Natural Language Processing. MIT\nPress.\nMelis, G. , Dyer, C. , and Blunsom, P. (2018). On the state of the art of evaluation in neural language\nmodels. In International Conference on Learning Representations.\nMerity, S. , Xiong, C. , Bradbury, J. , and Socher, R. (2016). Pointer sentinel mixture models. In\nInternational Conference on Learning Representations.\nMerity, S. , Keskar, N.S. , and Socher, R. (2018). Regularizing and optimizing LSTM language models.\nIn International Conference on Learning Representations.\nMikolov, T. and Zweig, G. (2012). Context dependent recurrent neural network language model. In\nIEEE Spoken Language Technology Workshop, pages 234–239.\nMikolov, T. , Karaﬁt, M. , Burget, L. , Cernocky, J.H. , and Khudanpur, S. (2010). Recurrent neural\nnetwork based language model. In Annual Conference of the International Speech Communication\nAssociation, page 3.\nMontemurro, M. and Pury, P.A. (2002). Long-range fractal correlations in literary corpora. Fractals,\n10(4), 451–461.\nPitman, J. (2006). Combinatorial Stochastic Processes. Springer.\nSimon, H.A. (1955). On a class of skew distribution functions. Biometrika, 42(3/4), 425–440.\nSmith, H.F. (1938). An empirical law describing hetero-geneity in the yields of agricultural crops.\nJournal of Agriculture Science, 28(1), 1–23.\nTakahashi, S. and Tanaka-Ishii, K. (2017). Do neural nets learn statistical laws behind natural lan-\nguage? PLoS ONE, 12(12), e0189326.\nTanaka-Ishii, K. and Bunde, A. (2016).\nLong-range memory in literary texts: On the universal\nclustering of the rare words. PLoS ONE, 11(11), e0164658.\nTaylor, L.R. (1961). Aggregation, variance and the mean. Nature, 189(4766), 732–735.\nTeh, Y.W. (2006). A hierarchical bayesian language model based on pitman-yor processes. In Annual\nConference on Computational Linguistics, pages 985–992.\nvan Leijenhorst, D.C. and van der Weide, Th.P. (2005). A formal derivation of heaps law. Information\nSciences, 170(2-4), 263–272.\n13\nVinyals, O. , Fortunato, M. , and Jaitly, N. (2015). Pointer networks. In Advances in Neural Infor-\nmation Processing Systems, pages 2692–2700.\nYang, Z. , Dai, Z. , Salakhutdinov, R. , and Cohen, W.V. (2018). Breaking the softmax bottleneck:\nA high-rank RNN language model. In International Conference on Learning Representations.\n14\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-04-24",
  "updated": "2018-04-24"
}