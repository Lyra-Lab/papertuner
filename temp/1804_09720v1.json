{
  "id": "http://arxiv.org/abs/1804.09720v1",
  "title": "JUNIPR: a Framework for Unsupervised Machine Learning in Particle Physics",
  "authors": [
    "Anders Andreassen",
    "Ilya Feige",
    "Christopher Frye",
    "Matthew D. Schwartz"
  ],
  "abstract": "In applications of machine learning to particle physics, a persistent\nchallenge is how to go beyond discrimination to learn about the underlying\nphysics. To this end, a powerful tool would be a framework for unsupervised\nlearning, where the machine learns the intricate high-dimensional contours of\nthe data upon which it is trained, without reference to pre-established labels.\nIn order to approach such a complex task, an unsupervised network must be\nstructured intelligently, based on a qualitative understanding of the data. In\nthis paper, we scaffold the neural network's architecture around a\nleading-order model of the physics underlying the data. In addition to making\nunsupervised learning tractable, this design actually alleviates existing\ntensions between performance and interpretability. We call the framework\nJUNIPR: \"Jets from UNsupervised Interpretable PRobabilistic models\". In this\napproach, the set of particle momenta composing a jet are clustered into a\nbinary tree that the neural network examines sequentially. Training is\nunsupervised and unrestricted: the network could decide that the data bears\nlittle correspondence to the chosen tree structure. However, when there is a\ncorrespondence, the network's output along the tree has a direct physical\ninterpretation. JUNIPR models can perform discrimination tasks, through the\nstatistically optimal likelihood-ratio test, and they permit visualizations of\ndiscrimination power at each branching in a jet's tree. Additionally, JUNIPR\nmodels provide a probability distribution from which events can be drawn,\nproviding a data-driven Monte Carlo generator. As a third application, JUNIPR\nmodels can reweight events from one (e.g. simulated) data set to agree with\ndistributions from another (e.g. experimental) data set.",
  "text": "Prepared for submission to JHEP\nJUNIPR: a Framework for Unsupervised Machine\nLearning in Particle Physics\nAnders Andreassen,a Ilya Feige,b Christopher Frye,a Matthew D. Schwartza\naDepartment of Physics, Harvard University, Cambridge, MA 02138\nbASI Data Science, 54 Welbeck Street, London, W1G 9XS\nE-mail: anders@physics.harvard.edu, ilya@asidatascience.com,\nfrye@physics.harvard.edu, schwartz@physics.harvard.edu\nAbstract:\nIn applications of machine learning to particle physics, a persistent challenge\nis how to go beyond discrimination to learn about the underlying physics. To this end, a\npowerful tool would be a framework for unsupervised learning, where the machine learns the\nintricate high-dimensional contours of the data upon which it is trained, without reference to\npre-established labels. In order to approach such a complex task, an unsupervised network\nmust be structured intelligently, based on a qualitative understanding of the data. In this\npaper, we scaﬀold the neural network’s architecture around a leading-order model of the\nphysics underlying the data.\nIn addition to making unsupervised learning tractable, this\ndesign actually alleviates existing tensions between performance and interpretability. We call\nthe framework Junipr: “Jets from UNsupervised Interpretable PRobabilistic models”. In this\napproach, the set of particle momenta composing a jet are clustered into a binary tree that the\nneural network examines sequentially. Training is unsupervised and unrestricted: the network\ncould decide that the data bears little correspondence to the chosen tree structure. However,\nwhen there is a correspondence, the network’s output along the tree has a direct physical\ninterpretation. Junipr models can perform discrimination tasks, through the statistically\noptimal likelihood-ratio test, and they permit visualizations of discrimination power at each\nbranching in a jet’s tree.\nAdditionally, Junipr models provide a probability distribution\nfrom which events can be drawn, providing a data-driven Monte Carlo generator. As a third\napplication, Junipr models can reweight events from one (e.g. simulated) data set to agree\nwith distributions from another (e.g. experimental) data set.\narXiv:1804.09720v1  [hep-ph]  25 Apr 2018\nContents\n1\nIntroduction\n2\n2\nUnsupervised Learning in Jet Physics\n6\n2.1\nGeneral Probabilistic Model\n7\n2.2\nNeural Network Implementation\n9\n3\nTraining and Validation\n13\n3.1\nTraining Data\n13\n3.2\nApproach to Training\n15\n3.3\nValidation of Model Components\n15\n3.4\nIncreasing the Branching Function Resolution\n17\n4\nApplications and Results\n20\n4.1\nLikelihood Ratio Discrimination\n21\n4.2\nGeneration from JUNIPR\n25\n4.3\nReweighting Monte Carlo Events\n27\n5\nFactorization and JUNIPR\n29\n5.1\nThe Encoding of Global Information\n31\n5.2\nClustering Algorithm Independence\n31\n5.3\nAnti-kt Shower Generator\n34\n6\nConclusions and Outlook\n36\n– 1 –\n1\nIntroduction\nMachine learning models based on deep neural networks have revolutionized information\nprocessing over the last decade. Such models can recognize objects in images [1–3], perform\nlanguage translation [4, 5], transcribe spoken language [6], and even speak written text [7] at\napproaching human level. The truly revolutionary aspect of this progress is the generality of\ndeep neural networks: a broad diversity of network architectures can be created from basic\nbuilding blocks that allow for eﬃcient calculation of gradients via back propagation, and thus\neﬃcient optimization through stochastic gradient descent [8]. These methods are arbitrarily\nexpressive and can model extremely high dimensional data.\nThe architecture of a neural network should be designed to process information eﬃciently,\nfrom the input data all the way through to the network’s ﬁnal output. Indeed, it empirically\nseems to be the case that networks that process information evenly layer-by-layer perform\nvery well. One example of this empirical result is that deep convolutional networks for image\nprocessing seem to perform sequentially more abstract operations as a function of depth\n[1]. Similarly, recurrent networks perform well on time series data, as their recurrent layers\nnaturally describe step-by-step evolution in time [9].\nThe power and generality of deep neural networks has been leveraged across the sciences,\nand in particular in particle physics. The simplest architecture explored has been the fully-\nconnected network, which has successfully been applied in a wide variety of contexts, such\nas in identifying and splitting clusters from multiple particles in the pixel detector [10], in\nb-tagging [11], and in τ-identiﬁcation [12]. In these basic applications, the neural network\noptimizes its use of some ﬁnite number of relevant physical observables for the task at hand.1\nOne drawback of such an approach is that the neural network is limited by the observables it\nis given. In fact, for these applications, other multivariate methods such as boosted decision\ntrees often have comparable performance using the same inputs, but train faster and can be\nless sensitive to noise [17, 18].\nAs an alternative to feeding a neural network a set of motivated observables, one can\nfeed it raw information. By doing so, one allows the network to take advantage of useful\nfeatures that physicists have yet to discover. One way of preprocessing the raw data in a\nfairly unbiased way is through the use of jet images, which contain as pixel intensities the\nenergy deposited by jet constituents in calorimeter cells [19]. Jet images invite the use of\ntechniques from image recognition to discriminate jets of diﬀerent origins. In [19], the pixel\nintensities in the two-dimensional jet image were combined into a vector, and a Fisher linear\ndiscriminant was then used to ﬁnd a plane in the high-dimensional space that maximally\nseparates two diﬀerent jet classes. Treating a 2-dimensional jet image as an unstructured\ncollection of pixel intensities, however, ignores the spatial locality of the problem, i.e. that\nneighboring pixels should have related intensities. Convolutional neural networks (CNNs),\nwhich boast reduced complexity by leveraging this spatially local structure, have since been\n1 For recent work on constructing a basis for neural network inputs, see [13–15], and see [16] for a linear\napproach that does not require neural network methods.\n– 2 –\nadopted instead, and they generally outperform fully-connected networks due to their eﬃcient\nfeature detection. In the ﬁrst applications of CNNs to jet images, on boosted W detection [20]\nand quark/gluon discrimination [21], it was indeed found that simple CNNs could generally\noutperform previous techniques. Since then, a number of studies have aimed to optimize\nvarious discrimination tasks using CNNs [22–27].\nWhile the two-dimensional detector image acts as a natural representation of a jet, espe-\ncially from an experimental standpoint, the 4-momenta of individual jet constituents provide\na more fundamental representation for the input to a neural network. One complication in\ntransitioning from the jet image to its list of momenta is that, while the image is a ﬁxed-size\nrepresentation, the list of momenta will have diﬀerent sizes for diﬀerent jets. To avoid this\nproblem, one could truncate the list of momenta in the jet to a ﬁxed size, and zero-pad jets\nsmaller than this size [28]. Alternatively, there are network architectures, namely recursive\n(RecNNs) and recurrent neural networks (RNNs), that handle variable length inputs natu-\nrally. With such methods, one also has the freedom to choose the order in which constituent\nmomenta are fed into the network. In [29], a RecNN was used to build a ﬁxed-size represen-\ntation of the jet, and the authors explored various ways of ordering the momenta as input to\nthe network: by jet clustering algorithms, by transverse momentum, and randomly. The re-\nsulting representation of the jet was then fed to a fully-connected neural network for boosted\nW tagging. RecNNs and RNNs have also been used in similar ways for quark/gluon discrim-\nination [30], top tagging [31], and jet charge [32]. See also [33, 34] for jet ﬂavor classiﬁcation\nusing tracks.\nTo date, the majority of applications of machine learning to particle physics employ\nsupervised machine learning techniques. Supervised learning is the optimization of a model\nto map input to output based on labeled input-output pairs in the training data. These\ntraining examples are typically simulated by Monte Carlo generators, in which case the labels\ncome from the underlying physical processes being generated.\nMost of the classiﬁcation\nstudies mentioned above employ this style of supervised learning, and similar techniques\nhave also been utilized for regression tasks such as pileup subtraction [22]. Alternatively,\ntraining data can be organized in mixed samples, each containing diﬀerent proportions of\nthe diﬀerent underlying processes. In this case, labels correspond to the mixed samples, and\nlearning is referred to as weakly supervised. While full and weak supervision are very similar\nas computational techniques, the distinction is exceptionally important in particle physics,\nwhere the underlying physical processes are unobservable in real collider data. Early studies\nof weakly supervised learning in particle physics show very promising results: performance\ncomparable to fully supervised methods was found both with low-dimensional inputs [35, 36]\n(a few physical observables) and with very high-dimensional inputs [37] (jet images).\nWith supervised learning, there is a notion of absolute accuracy: since every training\nexample is labeled with the desired output, the network predicts this output either correctly\nor incorrectly. This is in contrast to unsupervised learning, where the machine learns\nunderlying structure that is unlabeled in the training data. Without output-labeled training\nexamples, there is no notion of absolute accuracy.\nSeveral recent studies have employed\n– 3 –\nunsupervised learning techniques in particle physics. In [38], borrowing concepts from topic\nmodelling in text documents, the authors extract observable distributions of underlying quark\nand gluon jets from two mixed samples. In [39–41], generative adversarial networks (GANs)\nare used to eﬃciently generate realistic jet images and calorimeter showers.\nIn this work, we explore another approach to unsupervised machine learning in particle\nphysics, in which a deep neural network learns to compute the relative diﬀerential cross\nsection of each data point under consideration, or equivalently, the probability distribution\ngenerating the data. The power of having access to the probability distribution underlying\nthe data should not be underestimated. For example, likelihood ratios would provide optimal\ndiscriminants [42], and sampling from the probability distribution would provide completely\ndata-driven simulations.\nIn this paper, we introduce a framework named Junipr: “Jets from UNsupervised Inter-\npretable PRobabilistic models”. We also present a basic implementation of this framework\nusing a deep neural network. This network directly computes the general probability distri-\nbution underlying particle collider data using unsupervised learning.\nThe task of learning the probability distribution underlying collider data comes with chal-\nlenges due to the complexity of the data. Some past studies have aimed to process collider\ninformation eﬃciently by using neural network architectures inspired by physics techniques\nalready in use [29–33, 43]. In this paper, we take this idea one step further. We scaﬀold\nthe neural network architecture around a leading-order description of the physics underly-\ning the data, from ﬁrst input all the way to ﬁnal output. Speciﬁcally, we base the Junipr\nframework on algorithmic jet clustering trees. The tree structure is used, both in processing\ninput information, and in decomposing the network’s output. In particular, Junipr’s output\nis organized into meaningful probabilities attached to individual nodes in a jet’s clustering\ntree. In addition to reducing the complexity and increasing the eﬃciency of the correspond-\ning neural network, this approach also forces the machine to speak a language familiar to\nphysicists, thus enabling its users to interpret the underlying physics it has learned. Indeed,\none common downside associated with machine learning techniques in physics is that, though\nthey provide powerful methods to accomplish the tasks learned in training, they do little\nto clarify the underlying physics that underpins their success. Our approach minimizes this\ndownside.\nLet us elaborate on the tree-based architecture used for Junipr’s implementation. In\nparticle physics, events at colliders are dominated by the production of collimated collections\nof particles known as jets. The origin of jets and many of their properties can be understood\nthrough the fundamental theory of strong interactions, quantum chromodynamics (QCD).\nOne insight from QCD is that jets have an inherently fractal structure, inherited from the\napproximate scale invariance of the fundamental theory. The fractal structure is made precise\nthrough the notion of factorization, which states that the dynamics in QCD stratify according\nto soft, collinear, and hard physics [44–48], with each sector being separately scale invariant.\nTo capture this structure eﬃciently in Junipr, we use a kind of factorized architecture, with\na dense network to describe local branchings (well-suited for collinear factorization), and a\n– 4 –\nglobal RNN superstructure general enough to encode soft coherence and any factorization-\nviolating eﬀects.\nOne might naively expect this setup to require knowledge of the sequence of splittings\nthat created the jet. Although there is a sequence of splittings in parton-shower simulations,\nthe splittings are only a semi-classical approximation used to model the intensely complex\nand essentially incalculable distribution of ﬁnal state particles. Real data is not labelled with\nany such sequence. In fact, there are many possible sequences which could produce the same\nevent, and the cross section for the event is given by the square of the quantum mechanical\nsum of all such amplitudes, including eﬀects of virtual particles. A proxy for this ﬁctitious\nsplitting history is a clustering history that can be constructed in a deterministic way using\na jet-clustering algorithm, such as the kt algorithm [49, 50] or the Cambridge/Aachen (C/A)\nalgorithm [51, 52]. There is no correct algorithm: each is just a diﬀerent way to process the\nmomenta in an event. Indeed, there seems to be useful information in the multiple diﬀerent\nways that the same event can be clustered [53–55]. Any of these algorithms, or any algorithm\nat all that encodes the momenta of an event into a binary tree, can be used to scaﬀold a\nneural network in the Junipr approach.\nFor practical purposes, Junipr is implemented with respect to a ﬁxed jet clustering al-\ngorithm. Without a ﬁxed algorithm, the probability of the ﬁnal-state particles constructed\nthrough 1 →2 branchings would require marginalization over all possible clustering histo-\nries — an extremely onerous computational task. In principle, ﬁxing the algorithm used to\nimplement Junipr should be inconsequential for its output, namely the probability distri-\nbution over ﬁnal-state momenta, as these momenta are independent of clustering algorithm.\nTo reiterate, the Junipr approach does not require the chosen clustering algorithm to agree\nwith the underlying data-generation process; this is demonstrated in Secs. 5.2 and 5.3 below.\nOn the other hand, the sequence of probabilities assigned to each branching in a clustering\ntree certainly depends on the algorithm used to deﬁne the tree. For example, the same ﬁnal\nprobability P = 10−22 could be reached with one clustering algorithm through the sequence\nP = 10−5 ·10−6 ·10−8 ·10−3, or with another algorithm through P = 10−15 ·10−2 ·10−1 ·10−4.\nThe key idea is that, if an algorithm is chosen which does correspond to a semi-classical\nparton shower, the resulting sequence of probabilities may be understandable. This provides\navenues for users to interpret what physics the machine learns, and we expect that dissecting\nJunipr will be useful in such cases. We will demonstrate this throughout the paper.\nIt is worth emphasizing one fundamental aspect of our approach for clarity. The Junipr\nframework yields a probabilistic model, not a generative model. The probabilistic model\nallows us to directly compute the probability density of an individual jet, as deﬁned by its\nset of constituent particle momenta. To be precise, this is the probability density for those\nparticular momenta to arise in an event, conditioned on the event selection criteria used\nto select the training data.\nAs a complementary example of this, shower deconstruction\n[56, 57] provides a theory-driven approach to probabilistic modeling in particle physics, in\nwhich probabilities are calculated using QCD rather than a neural network. In contrast, a\ngenerative model would output an example jet, taking random noise as input to seed the\n– 5 –\ngeneration process. Given a distribution of input seeds, the jets output from a generative\nmodel should follow the same distribution as the training data. While this means that the\nprobability distribution underlying the data is internally encoded in a generative model, this\nunderlying distribution is hidden from the user. Examples of generative models in particle\nphysics include Monte Carlo event generators and, more recently, GANs used to generate jet\nimages and detector simulations [39–41].\nThe direct access to the probability distribution that is enabled by a probabilistic model\ncomes with several advantages. If two diﬀerent probabilistic models are trained on two dif-\nferent samples of jets, they can be used to compute likelihood ratios that distinguish between\nthe two samples. Likelihood ratios provide theoretically optimal discriminants [42], which is\nindeed a major motivation for Junipr’s probabilistic approach. One can also sample from a\nprobabilistic model in order to generate events, though generative models are better-suited\nfor this application [39–41]. In addition, one can use a probabilistic model to reweight events\ngenerated by an imperfect simulator, so that the reweighted events properly agree with data.\nIn this paper, as a proof-of-concept, we use simulated e+e−data to train a basic imple-\nmentation of the Junipr framework described above. We have not yet attempted to optimize\nall of this implementation’s hyperparameters; however, we do ﬁnd that a very simple archi-\ntecture with no ﬁne tuning is adequate. This is conﬁrmed by its impressive discrimination\npower and its eﬀective predictivity for a broad class of observables, but more rigorous testing\nis needed to determine whether this approach can provide state-of-the-art results on the most\npressing physics problems.\nThe general probabilistic model, its motivation, and a speciﬁc neural network imple-\nmentation of it are discussed in Sec. 2. A comprehensive discussion of training the model,\nincluding the data used and potential subtleties in extending the model are covered in Sec. 3.\nResults on discrimination, generation, and reweighting are presented in Sec. 4. We provide\nrobustness tests and some conceptually interesting results related to factorization in Sec. 5,\nincluding the counterintuitive anti-kt shower generator. There are many ways to generalize\nour approach, as well as many applications that we do not fully explore in this work. We\nleave a discussion of some of these possible extensions to Sec. 6, where we conclude.\n2\nUnsupervised Learning in Jet Physics\nTo establish the framework clearly and generally, Sec. 2.1 begins by describing Junipr as a\ngeneral probabilistic model, independent of the speciﬁc parametric form taken by the various\nfunctions it involves. From this perspective, such a probabilistic model could be implemented\nin many diﬀerent ways. Sec. 2.2 then describes the particular neural network implementation\nof Junipr used in this paper, which has a simple but QCD-customized architecture and\nminimal hyperparameter tuning.\n– 6 –\n=⇒\nCmMBT`\np1\npn\nXXX\np2\nPD2i({p1 . . . pn})\nmMQ#b2`p\u001c#H2 Z*. 2pQHmiBQM\nFigure 1: Junipr predicts the probability density Pjet({p1, · · · , pn}) of ﬁnding a given set\nof momenta {p1, . . . , pn} in a jet, conditioned on the jet selection criteria used to select the\ntraining data. No assumptions are made about the underlying quantum-mechanical processes\nthat generated the jet.\n2.1\nGeneral Probabilistic Model\nConsider a set of ﬁnal-state 4-momenta p1, . . . , pn that we hereafter refer to as “the jet”.\nJunipr computes the probability density Pjet({p1, . . . , pn}) of this set of momenta arising\nin an event, assuming the event selection criteria used to select the training data.\nThis\nprobability distribution is normalized so that, abstractly,\n∞\nX\nn=1\nZ\nd4p1 · · · d4pn Pjet({p1, . . . pn}) = 1 ,\n(2.1)\nwhere the integral extends over the physical region of phase space. (In practice, in implement-\ning Junipr we discretized the phase space into cells and assigned a measure of unity to each\ndiscrete cell. This results in Pjet being a discrete cell-size-dependent probability distribution,\nbut this choice is conceptually unimportant here.) A high-level schematic of Junipr is shown\nin Fig. 1, which emphasizes that the model does not attempt to learn the quantum-mechanical\nevolution that created the jet, but only meaningfully predicts the likelihood of its ﬁnal-state\nmomenta.\nAn unstructured model of the above form would ignore the fact that we know jet evo-\nlution is well-described by a semi-classical sequence of 1 →2 splittings, due to factorization\ntheorems [44–48]. A model that ignores factorization would be much more opaque to inter-\npretation, and have many more parameters than needed due to its unnecessary neutrality.\nThus, we propose a model that describes a given conﬁguration of ﬁnal-state momenta using\nsequential 1 →2 splittings. Such a sequence is deﬁned by a jet clustering algorithm, which\nassigns a clustering tree to any set of ﬁnal-state momenta, so that a sequential decomposition\nof the probability distribution can be performed without loss of generality. We imagine ﬁxing\na speciﬁc algorithm to deﬁne the trees, so that there is no need to marginalize over all possible\ntrees in computing a probability, a computation that would be intractable. While a determin-\nistic clustering algorithm cannot directly describe the underlying quantum-mechanical parton\nevolution, that is not the goal for this model. With the algorithm set, the model as shown in\nFig. 1 becomes that shown in Fig. 2.\n– 7 –\n=⇒\nCmMBT`\nt = 1\nt = 2\nt = 3\nt = 4\nPD2i({p1 . . . pn}) = Pt=1 · · · Pt=n\nt = 5\nU2M/V\nFigure 2: With any ﬁxed clustering algorithm, the probability distribution over ﬁnal-state\nmomenta can be decomposed into a product of distributions. Each factor in the product\ncorresponds to a diﬀerent step in the clustering tree. Subsequent probabilities are conditioned\non the outcomes from previous steps, so this decomposition entails no loss of generality.\nWe will now formalize this discussion into explicit equations. For the rest of this section\nwe assume that the clustering tree is determined by a ﬁxed jet algorithm (e.g. any of the\ngeneralized kt algorithms [58, 59]). The particular algorithm chosen is theoretically inconse-\nquential to the model, as the same probability distribution over ﬁnal states will be learned\nfor any choice. Practically speaking, however, certain algorithms may have advantages over\nothers. We will discuss the choice of clustering algorithm further in Secs. 5.2 and 5.3.\nThe application of a clustering algorithm on the jet constituents p1, . . . , pn deﬁnes a\nsequence of “intermediate states” k(t)\n1 , . . . , k(t)\nt . Here the superscript t = 1, . . . , n labels the\nintermediate state after the (t −1)th branching in the tree (where counting starts at 1) and\nthe subscript i = 1, . . . , n enumerates momenta in that state. To be explicit,\n• the “initial state” consists of a single momentum: k(1)\n1\n= p1 + · · · + pn;\n• at subsequent steps {k(t)\n1 , . . . , k(t)\nt } is gotten from {k(t−1)\n1\n, . . . , k(t−1)\nt−1 } by a single momentum-\nconserving 1 →2 branching;\n• after the ﬁnal branching, the state is the physical jet: {k(n)\n1 , . . . , k(n)\nn } = {p1, . . . , pn}.\nIn this notation, the probability of the jet (as shown in Fig. 2) can be written as\nPjet({p1, . . . pn}) =\n\" n−1\nY\nt=1\nPt\n\u0000k(t+1)\n1\n, . . . , k(t+1)\nt+1\n\f\fk(t)\n1 , . . . , k(t)\nt\n\u0001\n#\n(2.2)\n× Pn\n\u0000end\n\f\fk(n)\n1 , . . . , k(n)\nn\n\u0001\n.\nEq. (2.2) allows for a natural, sequential description of the jet. However, it obscures\nthe factorization of QCD which predicts an approximately self-similar splitting evolution.\nThus we decompose the model further, so that each Pt in Eq. (2.2) is described by a 1 →2\nbranching function that only indirectly receives information about the rest of the jet. The\nlatter is achieved via an unobserved representation vector h(t) of the global state of the jet at\nstep t. To be explicit, let k(t)\nm →k(t+1)\nd1\nk(t+1)\nd2\ndenote the branching of a mother into daughters\nthat achieves the transition from k(t)\n1 , . . . , k(t)\nt\nto k(t+1)\n1\n, . . . , k(t+1)\nt+1\nin the clustering tree. Then\n– 8 –\nwe can write\nPt\n\u0000k(t+1)\n1\n, . . .\n\f\fk(t)\n1 , . . .\n\u0001\n= Pend\n\u00000\n\f\fh(t)\u0001\nPmother\n\u0000m(t)\f\fh(t)\u0001\nPbranch\n\u0000k(t+1)\nd1\n, k(t+1)\nd2\n\f\fk(t)\nm , h(t)\u0001\nPn\n\u0000end\n\f\fk(n)\n1 , . . .\n\u0001\n= Pend\n\u00001\n\f\fh(n)\u0001\n(2.3)\nwhere m(t) is the mother’s discrete index in the tth intermediate state.\nWe thus have a\nsequential model that at each t step predicts\n• Pend\n\u00000\n\f\fh(t)\u0001\n: probability over binary values for whether or not the tree ends;\n• Pmother\n\u0000m(t)\f\fh(t)\u0001\n: probability over m ∈{1, . . . , t} indexing candidate mother momenta;\n• Pbranch\n\u0000k(t+1)\nd1\n, k(t+1)\nd2\n\f\fk(t)\nm , h(t)\u0001\n: probability over possible km →kd1, kd2 branchings.\nNote that we have left the conditioning on end = 0 implicit in Pmother and Pbranch, since\nwe will never need to use these functions when end = 1. In the product of Eq. (2.3), each\nsubsequent factor is thus conditioned on the outcomes of previous factors, so that breaking\nup Pjet in this way is without loss of generality. In particular, no assumption has been made\nabout the underlying physical processes that generate the data.\nWith these choices, we force the hidden representation h(t) to encode all global informa-\ntion about the tree, since it must predict whether the tree ends, which momentum branches\nnext, and the branching pattern. In fact, providing Pbranch with the momenta that directly\nparticipate in the 1 →2 branching means that h(t) only needs to encode global information.\nWe show that the global structure stored in h(t) is crucial for the model to predict the correct\nbranching patterns in Sec. 5.1.\n2.2\nNeural Network Implementation\nFor a neural network based implementation of the model deﬁned by Eqs. (2.2) and (2.3),\nwe use an RNN with hidden state h(t) augmented by dense neural networks for each of the\nthree probability distributions in Eq. (2.3). The recurrent structure of this implementation\nis shown in Fig. 3, which emphasizes how the RNN’s hidden representation h(t) keeps track\nof the global state of the jet, by sequentially reading in the momenta that branched most\nrecently.\nThe fact that h(t) learns and remembers the full jet, despite only being shown the two\nnew momenta at step t, is ensured by the tasks for which h(t) is responsible. These are shown\nin the detailed network diagram of Fig. 4. There one can see that h(t) is the only input\ninto the components of the model that predict when the tree ends and which momentum is\nnext to branch. The domains of the three probability functions in Eq. (2.3) are shown in\nFig. 4 as well: Pend is deﬁned over the binary set Z2 corresponding to “end” or “not”; Pmother\nis multinomial over the set Zt of candidate mothers; and Pbranch is deﬁned on the space of\npossible 1 →2 branchings, which is (a subset of) R4 by momentum conservation. At each\nstep, the model outputs the full probability distributions, which in mathematical notation\nare Pend(Z2|h(t)), Pmother(Zt|h(t)), and Pbranch(R4|k(t)\nm , h(t)).\n– 9 –\n=⇒\nh(t−1)\nh(t)\nh(t+1)\n· · ·\n· · ·\nb?Q`i?\u001cM/\nh(t−1)\n_LL, h(t)\n/\u001cm;?i2`b\nk(t)\nd1 k(t)\nd2\n/\u001cm;?i2`b\nk(t)\nd1 k(t)\nd2\n/\u001cm;?i2`b\nk(t−1)\nd1\nk(t−1)\nd2\n/\u001cm;?i2`b\nk(t+1)\nd1\nk(t+1)\nd2\nFigure 3: Information about the clustering tree is embedded in the hidden state h(t) of the\nRNN. For brevity, this recurrent structure is simpliﬁed on the right using a shaded box to\nindicate stepping from t −1 to t. At each step, the next two daughter momenta emerging in\nthe tree and the previous hidden state h(t−1) are inputs to the updated hidden state h(t).\nFig. 3 and Fig. 4 show how Junipr provides a probability distribution at each step t given\nthe momenta emerging from the preceding branching. For clarity, Fig. 5 separately shows\nhow Junipr is used to evaluate the full probability density Pjet({p1, . . . , pn}) over ﬁnal-state\nmomenta in a jet. At each step t, the point in Z2 representing whether the tree ends, the\npoint in Zt representing which mother momentum branches, and the point in R4 representing\nits daughters are plugged into the probability distributions to obtain the probabilities that\nshould be assigned to the jet under consideration. The product of these three probabilities,\ntaken over all t steps, leads to Pjet({p1, . . . , pn}).\nLet us now go into detail about the neural network architecture used. We use basic RNN\ncells [60] with tanh activation,\nh(t) = tanh\n\u0000W · (k(t)\nd1 , k(t)\nd2 ) + V · h(t−1) + b\n\u0001\n,\n(2.4)\nand found that a hidden representation vector h(t) of generic size 100 was suﬃcient for our\nneeds. We found GRU [61] and LSTM [62] cells to be unnecessarily complex and high-capacity\nfor the tasks carried out in this paper. This is in contrast to language modelling, for which\nbasic RNN cells are underpowered. To see why this might heuristically be expected, note\nthat a sentence containing 20 words is much more complex than a jet containing 20 momenta,\nbecause the words in the sentence are ordered, whereas the momenta in the jet are not. This\nintroduces an additional factor of 20! ∼1018 to the complexity of language modelling. It is\nthus reasonable to expect that jet physics will not require all the high-powered tools designed\nfor natural language processing.\nFor Pend we use a fully-connected network with h(t) as input, a single hidden layer of size\n100 with ReLU activation, and a sigmoid output layer. We use the same setup for Pmother, the\nonly diﬀerence being that the output layer is a softmax over the t candidate mother momenta,\n– 10 –\n7mHHv\n+QMM2+i2/\nM2irQ`F\n7mHHv\n+QMM2+i2/\nM2irQ`F\n7mHHv\n+QMM2+i2/\nM2irQ`F\n2ti2`M\u001cH BMTmi\n!\n\"#\n$\n!\n\"#\n$\nM2irQ`F QmiTmi, T`Q#\u001c#BHBiv /Bbi`B#miBQMb\nh(t−1)\nP2M/(Z2)\nPKQi?2`(Zt)\nP#`\u001cM+?(R4)\n_LL, h(t)\nKQi?2`\nk(t)\nm\n/\u001cm;?i2`b\nk(t)\nd1 k(t)\nd2\n≡CmMBT`\nFigure 4: Neural network implementation of the general probabilistic model proposed in\nEqs. (2.2) and (2.3). The network takes as external inputs two daughter momenta and one\nmother momentum. The global RNN then passes only its representation vector h(t) to each\nof the dense networks shown. The networks output three full probability distributions, which\npredict the end of the tree, the next mother to branch, and its daughter momenta.\nordered by energy. These choices are generic and not highly tuned. We found that Junipr\nworks well for a very general set of architectures and sizes, so we stick with this simple setup.\nFor the branching function Pbranch we must describe the probability distribution over\nall possible conﬁgurations of daughter momenta k(t+1)\nd1\n, k(t+1)\nd2\nconsistent with the mother\nmomentum k(t)\nm . For this system, we use coordinates x = (z, θ, φ, δ) centered around the\nmother, where z is the energy fraction of the softer daughter, θ (δ) is the opening angle of the\nsofter (harder) daughter, and φ speciﬁes the plane in which the branching occurs. See Fig. 6\nfor a visualization of these coordinates.\nThere are two separate approaches one could take to model the branching function Pbranch.\nFirstly, the variables x could be treated as discrete, with Pbranch outputting a softmax prob-\nability over discrete cells representing diﬀerent x values. Secondly, one could treat x as a\ncontinuous variable and use an “energy model” of the form Pbranch ∼eE(x)/Z , where Z is a\nnormalizing partition function. In this work we predominantly adopt the former approach, as\n– 11 –\n!\n\"#\n$\nQmiTmi \u001ci bi2T t, T`Q#\u001c#BHBiB2b\n!\n\"#\n$\nBMTmi \u001ci\nbi2T t,\nt 2pQHmiBQM\nP#`\u001cM+?\n!\nk(t+1)\nd1\n, k(t+1)\nd2\n\"\nPKQi?2`(m(t))\nP2M/(0)\nT`2pBQmb\n/\u001cm;?i2`b\nk(t)\nd1 k(t)\nd2\nKQi?2`\nk(t)\nm\n2M/ = 0\n\u001ci bi2T t\nKQi?2`\nBM/2t\nm(t)\nM2ti\n/\u001cm;?i2`b\nk(t+1)\nd1\nk(t+1)\nd2\nCmMBT`\nFigure 5: Using Junipr to evaluate the probability density over ﬁnal-state momenta in a\njet. For a given jet and its particular clustering tree, the values associated with the tree\nending, which momenta branch, and the emerging daughters are all known and plugged into\nthe probability distributions directly. The probability density of the jet is then the product\nover the three distributions, over all splitting steps t.\nit is much faster, and most distributions are insensitive to the discretization of x. However,\nwe do train an energy model to show that models with continuous x are possible, which we\ndiscuss in Sec. 3.4.\nIn the discrete case, we bin the possible values of x into a 4-dimensional grid with 10 bins\nper dimension, so that the entire grid has 104 cells. For a given value of x, we place a 1 in\nthe bin corresponding to that value, and we place 0’s everywhere else. This 1-hot encoding\nof the possible values of x allows us to use a softmax function at the top layer of the neural\nnetwork describing Pbranch (see Fig. 4). Furthermore, we use a dense network with a single\nhidden layer of size 100 and ReLU activation for Pbranch, just as we did for Pend and Pmother.\nThe hidden units in this network receive h(t) as input, as well as the mother momentum k(t)\nm .\n– 12 –\nφ\n✓\nδ\n?\u001c`/ /\u001cm;?i2`,\n(1 −z) Em\nbQ7i /\u001cm;?i2`,\nz Em\nKQi?2`,\nEm\nFigure 6: Local coordinates x = (z, θ, φ, δ) that parameterize the momentum-conserving\n1 →2 branching at each step in the clustering tree of a jet.\nThus we have a neural network implementation of Eqs. (2.2) and (2.3), with a repre-\nsentation of the evolving global jet state stored in h(t), and with fully-connected networks\ndescribing Pend, Pmother, and Pbranch. As deﬁned above, the model has a single 106 parameter\nmatrix, mapping the branching function’s 100 dimensional hidden layer to its 104 dimensional\noutput layer, and has 6 × 104 parameters elsewhere. One might refer to this implementation\nas Junipr0/, as one can imagine many alternative implementations within the Junipr frame-\nwork that may prove useful in future applications. We will continue to use the term Junipr\nfor brevity, to refer both to the framework and to the basic implementation described here.\n3\nTraining and Validation\nWe now describe how to train the model outlined in Sec. 2.2. We begin by discussing the\ntraining data used, followed by our general approach to training and validation. Finally we\ndiscuss an alternative model choice that allows higher resolution on the particle momenta.\n3.1\nTraining Data\nTo enable proof-of-concept demonstrations of Junipr’s various applications, we train the\nimplementation described in Sec. 2.2 using jets simulated in Pythia v8.226 [63, 64] and clus-\ntered using FastJet v3.2.2 [59]. We simulated 600k hemisphere jets in Pythia using the\nprocess e+e−→q¯q at a center-of-mass energy of 1 TeV, with hemispheres deﬁned in Fast-\nJet using the exclusive kt algorithm [49, 50], and with an energy window of 450–550 GeV\nimposed on the jets. To create the deterministic trees that Junipr requires, we reclustered\nthe jets using the C/A clustering algorithm [51, 52], with Esub = 1 GeV and Rsub = 0.1. The\nnonzero values of Esub and Rsub make the input to Junipr formally infrared-and-collinear\nsafe, but this is by no means necessary. Furthermore, our approach is formally independent\nof the reclustering algorithm chosen. We demonstrate this by showing results using an ab-\nsurd reclustering algorithm inspired by a 2D printer in Sec. 5.2, as well as for anti-kt [58]\nreclustering in Sec. 5.3.\n– 13 –\nThus we have 600k quark jets with Ejet ∼500 GeV and Rjet ∼π/2. We use 500k of\nthese jets for training, with 10k set aside as a test set to monitor overﬁtting, and we use the\nremaining validation set of 100k jets to make the plots in this paper.\nIn the applications of Sec. 4, we also make use of several other data sets produced accord-\ning to the above speciﬁcations, with small but important changes. We list these modiﬁcations\nhere for completeness. In one case, quark jets from e+e−→q¯q were required to lie in a very\ntight mass window of 90.7–91.7 GeV. A sample of boosted Z jets from e+e−→ZZ events\nwas also produced with the same mass cut. And ﬁnally, another sample of quark jets was\nproduced, as detailed above, but with the value of αs(mZ) in the ﬁnal state shower changed\nfrom Pythia’s default value of 0.1365 to 0.11.\nBefore being fed to Junipr, jets in these data sets must be clustered, so that each jet\nbecomes a tree of 1 →2 branchings ending in the n ﬁnal-state momenta of the jet:\n(t = 1)\n(t = 2)\n. . .\n(t = n −1)\n(t = n)\n\n\n\n\n\n\n\n\n\n\n\n\np1\np2\n. . .\np3\nXXX\npn\nk(1)\n1\nk(2)\n1\nk(2)\n2\nk(n−1)\n1\nk(n−1)\n2 XXX\nk(n−1)\nn−1\n+Hmbi2`BM;\n\u001cH;Q`Bi?K\nD2i =\n\n\n\n\n\n\n\n\n\n\n\n\np1\np2\np3\nXXX\npn\n−−−−−−→\n(3.1)\nwhere the momenta in one column are equal to those of the next column except for a single\n1 →2 branching. At each step t, only the momenta associated with this 1 →2 branching are\nfed into Junipr, as detailed in Sec. 2. With this setup, Junipr requires minimal parameters;\nit learns to update h(t) as the tree evolves by focusing only on the step-by-step changes to\nthe jet. Note also that jets of arbitrary length can be considered.\nNote that in implementing Junipr, we do not directly evaluate the branching function\nPbranch\n\u0000k(t+1)\nd1\n, k(t+1)\nd2\n\f\fk(t)\nm , h(t)\u0001\non the momenta k(t+1)\nd1\n, k(t+1)\nd2\nbut instead use the parameter-\nization x = (z, θ, φ, δ) shown in Fig. 6. In fact, we use a nonlinear transformation of this\nparameterization:\n˜z =\nlog z −log Esub\nEjet\nlog 1\n2 −log Esub\nEjet\n˜θ =\nlog θ −log Rsub\n2\nlog Rjet −log Rsub\n2\n˜φ = φ\n2π\n˜δ =\nlog δ −log EsubRsub\nEjet\nlog Rjet\n2\n−log EsubRsub\nEjet\n(3.2)\nThis invertible transformation simply maps the range of each coordinate onto [0, 1], which\nreduces the amount of global parametric shift required in optimization. Similarly, we perform\na transformation on the components of k(t)\nd1 , k(t)\nd2 before feeding them into the update rule for\nh(t) in Eq. (2.4); we do the same for k(t)\nm , the input to the branching function Pbranch. This is\na technical point that is not conceptually important.\n– 14 –\n3.2\nApproach to Training\nTo train Junipr, we maximize the log likelihood over the full set of training data:\nlog likelihood =\nX\njet i in data\nlog Pjet({p(i)\n1 , . . . , p(i)\nn }) .\n(3.3)\nFor a particular jet with ﬁnal-state momenta p1, . . . , pn we use Eqs. (2.2) and (2.3) to compute\nlog Pjet({p1, . . . , pn}) =\nn−1\nX\nt=1\n\u0014\nlog Pend\n\u00000\n\f\fh(t)\u0001\n(3.4)\n+ log Pmother\n\u0000m(t)\f\fh(t)\u0001\n+ log Pbranch\n\u0000k(t+1)\nd1\n, k(t+1)\nd2\n\f\fk(t)\nm , h(t)\u0001\u0015\n+ log Pend\n\u00001\n\f\fh(n)\u0001\nwhere m(t) is the index of the mother momentum at step t in the training example and\nk(t+1)\nd1\n, k(t+1)\nd2\nare its daughters. Maximizing the log likelihood in this way allows the model to\nlearn each t step in parallel, providing computational eﬃciency and stability.\nFor all models presented in this paper, we use basic stochastic gradient descent with the\nfollowing learning rate and batch size schedule, where training proceeds from left to right:\nSchedule\n5 epochs\n5 epochs\n5 epochs\n5 epochs\n5 epochs\n5 epochs\nlearning rate\n10−2\n10−3\n10−4\n10−3\n10−4\n10−5\nbatch size\n10\n10\n10\n100\n100\n100\nWe follow such a schedule to slowly increase the resolution and decrease the stochasticity\nof gradient descent throughout training. Decreasing the learning rate reduces the step size,\nthereby allowing ﬁner details of the cost surface to be resolved. Increasing the batch size\nreduces the stochasticity by improving the sample estimates of the true gradients.\nWe wrote Junipr in Theano [65] and trained it on 16-core CPU servers using the Sher-\nlockML technical data science platform. Training Junipr on 500k jets according to the above\nschedule took an average of 4 days.\n3.3\nValidation of Model Components\nJunipr is constructed as a probabilistic model for jet physics by expanding Pjet as a prod-\nuct over steps t in the jet’s clustering tree, as shown in Eq. (2.2). Each step involves three\ncomponents: the probability Pend that the tree will end, the probability Pmother that a given\nmomentum will be the next mother to branch, and the probability Pbranch over the daughter\nmomenta of the branching, as shown in Eq. (2.3). We now validate each of Junipr’s com-\nponents using our validation set of 100k previously unseen Pythia jets. In this section, we\npresent histograms of actual outcomes in the Pythia validation set (i.e. frequency distribu-\ntions) as well as Junipr’s probabilistic output when evaluated on the jets in this data set\n(i.e. marginalized probability distributions) to check for agreement.\n– 15 –\n0\n5\n10\n15\n20\n25\n30\nintermediate state length\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nprobability to end\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\n0.0\n0.5\n1.0\n1.5\nmax angle(jet, particle) in state\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability to end\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\nFigure 7: Validation of Pend, the probability that the tree should end. Comparison is made\nbetween actual outcomes in the validation set of Pythia jets and Junipr’s probabilistic\npredictions for these jets. (Left) Pend as a function of intermediate state length. (Right) Pend\nas a function of the maximum angle between the jet axis and momenta in the intermediate\nstate.\nIn Fig. 7 we show the probability Pend that the tree should end, as a function of both in-\ntermediate state length and maximum particle-to-jet-axis angle. In both cases we see excellent\nagreement with the validation data, demonstrating a good model ﬁt with low underﬁtting and\nno overﬁtting. Note that Fig. 7 (left) is in one-to-one correspondence with the jet constituent\nmultiplicity, and that the shape of Fig. 7 (right) is a direct consequence of C/A clustering\nwith Rsub = 0.1 . Indeed, if an opening angle near Rsub already exists in an angular-ordered\ntree, then there are likely no remaining branchings in the clustering tree.\nIn Fig. 8 we show the probability Pmother that a given candidate will be the next mother\nto branch in the clustering tree, as a function of both the candidate’s index (which is sorted\nto be decreasing in energy) and the candidate’s angle from the jet axis. The ﬁrst of these\nresults is shown in particular for the t = 10th step in the clustering trees. We observe again\nthat the model ﬁts the validation data well. Note from Fig. 8 (left) that the highest energy\nbranches of the clustering tree are most likely to undergo subsequent branchings, in line with\nthe expectation at leading logarithmic accuracy. Fig. 8 (right) shows consistent predictions,\nsince the highest energy branches also lie at the narrowest angles to the jet axis.\nIn Fig. 9 we show the branching function Pbranch, the component of the model that\npredicts how a mother momentum should split into a pair of daughter momenta. We show\nthe branching function results for z and θ (i.e. with Pbranch marginalized over the variables\nnot shown) at the ﬁrst step in the jet evolution t = 1, as well as at a later step t = 10. (See\nFig. 6 for deﬁnitions of z and θ and Eq. (3.2) for their ranges in the data.) This shows the\ndependency of the branching function on the evolving jet representation h(t), which we will\ndiscuss in detail in Sec. 5.1. We see that for these direct predictions, Junipr ﬁts the validation\n– 16 –\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nindex in energy sorted state (t=10)\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability to branch\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\n0.0\n0.5\n1.0\n1.5\nangle(jet, candidate)\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability to branch\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\nFigure 8: Validation of Pmother, the probability that a given candidate will branch next in\nthe clustering tree. Comparison is made between actual outcomes in the validation set of\nPythia jets and Junipr’s probabilistic predictions for these jets. (Left) Pmother at t = 10,\nas a function of a candidate’s index in the energy ordered intermediate state. (Right) Pmother\naveraged over all t’s, as a function of a candidate’s angle relative to the jet axis.\ndata almost perfectly. Note that in Fig. 9 (top) soft wide-angle emissions are the norm at\nthe earliest t steps, as expected with the C/A clustering algorithm. In Fig. 9 (bottom) one\ncan see that later in the clustering trees, harder more-collinear branchings are commonplace.\nIt bears repeating that these trends are highly dependent on the chosen clustering algorithm\nand have no precise connection to the underlying physical processes generating the data.\n3.4\nIncreasing the Branching Function Resolution\nIn this section, we discuss increasing the resolution of the branching function\nP(x) ≡Pbranch\n\u0000k(t+1)\nd1\n, k(t+1)\nd2\n\f\fk(t)\nm , h(t)\u0001\n(3.5)\nincluding the case where P(x) is an energy model over continuous x = (z, θ, φ, δ). (The x\ncoordinates were deﬁned in Fig. 6.) This technical section can easily be skipped without loss\nof the logical ﬂow of the paper.\nWe begin by brieﬂy discussing increasing the resolution of the branching function over\ndiscrete x, the case described in Sec. 2.2. The ﬁrst thing to note is that with a softmax over\n4-dimensional x, the size of the matrix multiplication required in a dense network is quartic\nin the number of bins used for each dimension. We generically use 10 bins for each of z, θ, φ, δ\nresulting in an output size of 104. (In fact we use 10 linearly spaced bins in the transformed\ncoordinates of Eq. (3.2), and this can be seen on the logarithmic axes of Fig. 9, but this detail\nis not conceptually important.) Given this quartic scaling, simply increasing the number of\ndiscrete x cells quickly becomes prohibitively computationally expensive. Potential solutions\nto this problem include: (i) using a hierarchical softmax [66, 67], and (ii) simply interpolating\nbetween the discrete bins of the model.\n– 17 –\n0.002\n0.01\n0.1\n0.5\nz(t=1)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nprobability\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\n0.05\n0.1\n0.3\n1\nπ/2\nθ(t=1)\n0.00\n0.05\n0.10\n0.15\n0.20\nprobability\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\n0.002\n0.01\n0.1\n0.5\nz(t=10)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nprobability\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\n0.05\n0.1\n0.3\n1\nπ/2\nθ(t=10)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nprobability\nPythia e+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr prob.\nFigure 9: Validation of Pbranch, the 4-dimensional probability distribution over 1 →2 branch-\nings. Comparison is made between actual outcomes in the validation set of Pythia jets and\nJunipr’s probabilistic predictions for these jets. Results are shown for energy fraction z (left)\nand branching angle θ (right) as deﬁned in Fig. 6. Evolution step t = 1 is shown (top) where\nsoft wide-angle emissions are the norm, as expected in the C/A tree. Evolution step t = 10\n(bottom) gives rise to harder more-collinear branchings.\nIn a hierarchical softmax, a low-resolution probability is predicted ﬁrst, say with 54\ncells, then another 54-celled distribution is predicted inside the chosen low-resolution cell.\nIn principle, this gives 254 resolution at only twice the computational time required for 54\nresolution. We brieﬂy implemented the hierarchical softmax, and preliminary tests found it\nto work eﬃciently, but perhaps with a decrease in training stability. We chose not to pursue\nthe hierarchical softmax further in this work, primarily because we have not seen the need\nfor resolution much higher than 104 discrete x cells.\nDue to its ease of use, we do employ linear interpolation between the discrete bins in our\nbaseline model with resolution 104. This comes at no extra training cost, and removes most\nof the eﬀects of discretization on the observable distributions generated by sampling from\n– 18 –\nJunipr; see Sec. 4.2.\nWe now turn to the continuous version of Junipr in which the branching function P(x)\nis given by an undirected energy model:\nP(x) = eE(x)\nZ\n,\nwhere\nZ =\nZ\ndx eE(x).\n(3.6)\nTo model E(x), we again use a fully-connected network with hidden layer of size 100, as used\neverywhere else, except here the output layer is left to be linear. We perform the integral\nover Z using importance sampling:\nZ =\nZ\ndx q(x) eE(x)\nq(x) =\n\u001ceE(x)\nq(x)\n\u001d\nq\n≈1\n|S|\nX\nxs∈S\neE(xs)\nq(xs) = bZ(S)\n(3.7)\nwhere S is the set of xs’s sampled from the importance distribution q.\nUnlike the discrete-x version of Junipr, where training is relatively straightforward, the\ncontinuous-x version requires a non-standard technique in training the branching function\nP(x). This is because, although Eq. (3.7) provides an unbiased approximation to Z,\n⟨bZ⟩S∼q = Z,\n(3.8)\nthis leads to a biased estimate of the log likelihood, since\n\nlog bZ\n\u000b\nS∼q < log\n\n bZ\n\u000b\nS∼q = log Z\n(3.9)\nby Jensen’s inequality. Thus, every gradient step taken is systematically diﬀerent from the\ntrue gradient, and this bias derails training, especially near convergence when the true gra-\ndient becomes small.\nTo overcome this problem, we start by computing the sample variance on our estimate\nbZ(S), which is\nσ( bZ)2 =\n1\n|S| −1\nX\nxs∈S\n\u0012eE(xs)\nq(xs) −bZ(S)\n\u00132\n.\n(3.10)\nThen the percent-error ∆in our biased estimate of the gradient is approximately\n∆=\n1\np\n|S|\nσ( bZ)\nbZ\n.\n(3.11)\nThis error propagates into the log likelihood, causing the bias in Eq. (3.9). To mitigate this,\nwe adopt a policy of monitoring ∆during training, and whenever ∆increases above some\nvalue ∆threshold (a hyperparameter that we set to 2%) we double the sample size |S| used to\ncompute bZ(S). This slows down training considerably, but it eﬀectively reduces the bias in\nour gradient estimates. Note that while generic importance sampling typically fails in higher\ndimensions, our branching function lives in only 4 dimensions, so this approach is robust using\n– 19 –\n0.002\n0.01\n0.1\n0.5\nz (all t’s)\n0.00\n0.02\n0.04\n0.06\n0.08\nprobability\nPythia e+e−→q¯q\nC/A clustering\nJunipr cont. prob.\nPythia freq.\nFigure 10: Branching function modelled by a deep undirected energy model over continuous\nvariables z, θ, φ, δ that parameterize the branching. Shown is the marginalized distribution\nover z, averaged over all t steps. Comparison is made between actual outcomes in the vali-\ndation set of Pythia jets and Junipr’s probabilistic predictions for these jets.\nany reasonable importance distribution q. Indeed, we found that a uniform distribution over\nthe transformed coordinates of Eq. (3.2) is a ﬁne choice for q.\nIn Fig. 10 we show results for Junipr trained with the continuous branching function as\ndescribed above. In this case, we can use arbitrarily high-resolution binning, as Junipr has\nlearned a fully continuous probability density. Fig. 10 can be roughly compared to Fig. 9,\nwhere we were required to use 10 bins for each dimension of x.\nTo close this section, we note that in most cases, we expect the discretized branching\nfunction with 10 bins per dimension of x to be suﬃcient, especially if one performs a linear\ninterpolation on the output cells. This simple case is certainly faster to train and does not\nrequire the technique described here to avoid biased gradient estimates.\n4\nApplications and Results\nWith Junipr trained and validated, we turn to some of the most interesting results it enables.\nGiven a jet, Junipr can compute the probability density associated with the momenta inside\nthe jet, conditioned on the criteria used to select the training data. To visualize this, we show\na C/A-clustered Pythia jet in Fig. 11 with the Junipr-computed probability associated\nwith each branching written near that node in the tree. Note that these are small discretized\nprobabilities due to the discretized implementation of Junipr’s branching function described\nin Sec. 2. This is shown primarily to conceptualize the model, which is constructed to be quite\ninterpretable as it is broken down to compute the probability of each step in the clustering\nhistory of a jet.\nA direct and powerful application of the Junipr framework, enabled by having access to\nseparate probabilistic models of diﬀerent data sources, is in discrimination based on likelihood\n– 20 –\n−2.9\n−2.4\n−3.4\n−3.3\n−3.0\n−3.0\n−2.4\n−3.6\n−2.0\n−2.9\n−2.4\n−3.2\n−2.7\n−3.2\n−2.9\n−2.9\n−2.6\n−2.8\nPend · Pmother · Pbranch\nPt=18 = (10−0.7)(10−0.1)(10−2.0)\nSvi?B\u001c [m\u001c`F D2i\n*f\u001b +Hmbi2`BM;\n8yy :2o\nky :2o\nR :2o\n=⇒\nPjet = 10−51.6\nFigure 11: Junipr-computed probability asigned to example Pythia jet and sequentially\ndecomposed along its C/A clustering tree.\nNodes are labeled with log10 Pt, where Pt =\nPend · Pmother · Pbranch includes the product of all three components of the probability at step\nt, as shown in Eq. (2.3).\nColor corresponds to energy and opening angle corresponds to\n3-dimensional branching angle. Probabilities are small and discrete due to the discretized\nbranching function used in Junipr’s implementation.\nratios. We discuss discrimination in Sec. 4.1, along with a highly intuitive way of visualizing\nit.\nIn contrast, an instinctive but indirect use of Junipr as a probabilistic model is in\nsampling new jets from it. We discuss the observable distributions generated through sampling\nin Sec. 4.2. However, sampling from a probabilistic model is often ineﬃcient (e.g. slower\nthan Pythia) compared to evaluating probabilities of jets directly. In Sec. 4.3 we discuss\nreweighting samples from one simulator to match those of another distribution. In principle,\nthis could be used to tweak Pythia samples to match observed collider data simply by\nreweighting.\n4.1\nLikelihood Ratio Discrimination\nWe expect that one of the most exciting applications of Junipr will be in discriminating the\nunderlying physics that could have created a jet.2 For example, suppose we had two sets of\njets, one set corresponding to decays of a boosted Z boson, the other set simply high-energy\nquarks. We could then train one copy of Junipr on just the boosted Z sample, giving the\nprobability distribution PZ, and another copy of Junipr on just the quark jets, giving Pq.\nFinally, for any new jet we could determine whether the jet was initiated by a boosted Z or\nby a high-energy quark by looking at the likelihood ratio:\nPZ(jet)\nPq(jet) > threshold\n=⇒\njet is boosted Z\n(4.1)\n2We thank Kyle Cranmer for an early discussion on this topic.\n– 21 –\nwhere the threshold is set according to the location on the ROC (receiver operating charac-\nteristic) curve desired for the discrimination task at hand. In contrast to approaches that try\nto compute likelihood ratios like this using QCD [56, 57], the Junipr approach can learn the\nseparate probability distributions directly from samples of training data.\nDiscrimination based on the likelihood ratio theoretically provides the most statistically\npowerful discriminant between two hypotheses [42]. Moreover, our setup takes into account\nall the momenta that deﬁne a speciﬁc type of jet. Note also that for the task of pairwise dis-\ncrimination between N jet types, this unsupervised approach requires training N probabilistic\nmodels, whereas a supervised learning approach would require training N(N −1)/2 classiﬁers.\nThus, we expect likelihood-ratio discrimination using Junipr to provide a powerful tool.\nWe note further that we do not even require pure samples of the two underlying processes\nbetween which we would like to discriminate [35]. Thus, it would be feasible to discriminate\nbased solely on real collider data. In our Z/quark example above, we would simply train\none copy of Junipr on a sample of predominantly boosted-Z jets, and train another copy\non predominantly quark jets, and the likelihood ratio of those two models would still be\ntheoretically optimal for Z/quark discrimination.\nIn order to get a ﬁrst look at the potential of likelihood-ratio discrimination using Junipr,\nwe continue with the Z/quark example discussed above. We use Pythia to simulate e+e−→\nq¯q and e+e−→ZZ events at a center-of-mass energy of 1 TeV. We impose a very tight mass\nwindow, 90.7 – 91.7 GeV, on the jets in each data set, so that no discrimination power can\nbe gleaned from the jet mass. More details on the generation of the data sets were given in\nSec. 3.1. We admit that a more compelling example of discrimination power would be for\nquark and gluon jets at hadron colliders, but we leave a proper treatment of that important\ncase to future work. The toy scenario studied here serves both to prove that the probabilities\noutput by Junipr are meaningful, and that likelihood ratio discrimination using unsupervised\nprobabilistic models is a promising application of the Junipr framework.\nIn Fig. 12 we show the Z/quark separation power achieved by Junipr, both in terms\nof full likelihood ratio distributions for validation sets of Z and quark jets, as well as the\nresulting ROC curve. For comparison, in Fig. 12 we also show the ROC curve achieved us-\ning a 2D likelihood ratio discriminant based on 2-subjettiness [68] and multiplicity. Junipr’s\nlikelihood-ratio discrimination is clearly superior to that based on combining the most natural\nobservables: 2-subjettiness, multiplicity, (and keep in mind the tight mass cut). Of course,\nthese observables do not provide state-of-the-art discrimination power even in this toy sce-\nnario, but we include the comparison in this proof-of-concept to provide a sense of scale on\nthe plot.\nBy design, Junipr naturally processes the information in jets via a recurrent mechanism\nthat tracks the evolution of their clustering trees, and this allows users to peer inside at this\nstructure and access the probabilities at each branching. In particular, we can consider the\nlikelihood ratio at each step in the clustering trees to understand which branchings give rise\nto the greatest discrimination power. We show this in Fig. 13, where it is clear that Junipr\ncan extract useful discriminatory information at most branchings.\n– 22 –\n−10\n−5\n0\n5\nlog10(likelihood ratio)\n0\n1000\n2000\n3000\n4000\n5000\njets per bin\nPythia jets\nC/A clustering\ne+e−→q¯q\ne+e−→ZZ\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nZ acceptance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nquark rejection\nPythia e+e−→q¯q vs. ZZ\nC/A clustering\nJunipr likelihood ratio\nτ21 + multiplicity\nFigure 12: (Left) Likelihood ratio PZ(jet)/Pq(jet) evaluated on Pythia jets in the validation\nset. (Right) ROC curve for discrimination based on Junipr’s likelihood ratio, in comparison\nto the empirical 2D distribution using 2-subjettiness and constituent multiplicity. All jets\nused in this study have masses between 90.7 and 91.7 GeV.\nIndeed, visualizing jets as in Fig. 13 can provide a number of insights. Unsurprisingly,\nwe see for the quark jet (on the top) that the likelihood ratio of the ﬁrst branching is rather\nextreme, at 10−3.7, since it is unlike the energy-balanced ﬁrst branching associated with\nboosted-Z jets. However, we also see that almost all subsequent branchings are also unlike\nthose expected in boosted-Z jets, and they combine to provide comparable discrimination\npower to the ﬁrst branching alone.\nMany eﬀects probably contribute to this separation\npower at later branchings, including that quark jets often gain their mass throughout their\nevolution instead of solely at the ﬁrst branching, and that the quark jet is color-connected to\nother objects in the global event. Such eﬀects have proven to be useful for discrimination in\nother contexts [69].\nSimilarly, considering the boosted-Z jet on the bottom of Fig. 13 shows that signiﬁ-\ncant discrimination power comes not only from the ﬁrst branching, but also from subsequent\nsplittings, as the boosted-Z jet evolves as a color-singlet q¯q pair. Note the presence of the\npredictive secondary emissions sent from one quark-subjet toward the other. This is reminis-\ncent of the pull observable, which has proven useful for discrimination in other contexts [70].\nMore generally, the importance of the energy distribution, opening angles, multiplicity, and\nbranching pattern in high-performance discrimination can be understood from such pictures.\nWe are very excited by the prospect of visualizing Junipr’s discrimination power on\njets, based on the likelihood ratio it assigns at each branching in their clustering trees, as in\nFig. 13. Such visualizations could provide intuition that leads to the development of new,\nhuman-interpretable, perhaps calculable observables for discrimination in important contexts.\nWe would like to make one side note about discrimination, before moving on to the next\napplication of Junipr. The statement that likelihood-ratio discrimination is optimal of course\n– 23 –\n−3.7\n−1.4\n−0.1\n−0.0\n−0.1\n−0.5\n0.0\n−0.3\n−0.4\n2.4\n0.3\n0.0\n−0.1\n0.2\n−0.3\n0.7\nR :2o\nky :2o\n8yy :2o\nSvi?B\u001c D2ib\n*f\u001b +Hmbi2`BM;\nUiQTV [m\u001c`F D2i\nU#QiiQKV #QQbi2/ w\nFigure 13: Junipr trees for visualization of discrimination power at individual nodes in\nthe clustering history.\nEach node is labeled with the component of log10 PZ(jet)\n\u000e\nPq(jet)\nassociated with that t step. Colors represent energies, and opening angles represent physical\n3-dimensional branching angles. The top ﬁgure is a quark jet generated using Pythia, with\nmass between between 90.7 and 91.7 GeV; the bottom ﬁgure is a boosted-Z jet. The role\nthat the energy distribution, opening angles, multiplicity, and branching pattern play in high-\nperformance discrimination can be understood from such pictures.\nonly applies in the limit of perfect models. Since this limit is never fully realized, one may\nworry that discrimination with Junipr may in fact be suboptimal. Since the two probabilistic\nmodels we use for discrimination are each trained individually to replicate a certain type of\njet, they are not conditioned to focus on the diﬀerences between the two jet types, which may\nbe very subtle in the case of a diﬃcult discrimination task. In the realistic case of slightly\nimperfect models, it may be advantageous for discrimination purposes to instead train the\ntwo models to focus on the diﬀerences. To be speciﬁc, one could train the two models on\nthe two data sets simultaneously, with the goal being to maximize the likelihood ratio on one\ndata set and minimize it on the other. Following this method in the particular example of\nZ/quark discrimination used above, one would train the PZ and Pq models on data sets DZ\nand Dq to maximize the following quantity:\nX\njet∈DZ\nlog PZ(jet)\nPq(jet) −\nX\njet∈Dq\nlog PZ(jet)\nPq(jet) .\n(4.2)\nCompare this to the approach we have taken above, namley training PZ and Pq to separately\nmaximize the log likelihood of Eq. (3.3) on their corresponding sets of training data. This\n– 24 –\nBMTmi \u001ci\nbi2T t,\nT\u001c`iB\u001cH i`22\nQmiTmi \u001ci\nbi2T t,\nmT/\u001ci2/ i`22\n;2M2`\u001ciBQM\nP2M/(Z2)\nPKQi?2`(Zt)\nP#`\u001cM+?(R4)\n/`\u001crM\nKQi?2`\nk(t)\nm\nH\u001ci2bi\n/\u001cm;?i2`b\nk(t)\nd1 k(t)\nd2\n/`\u001cr\n/\u001cm;?i2`b\n/`\u001cr\nKQi?2`\n/`\u001cr\n2M/fMQi\nCmMBT`\nFigure 14: Sampling from Junipr to generate jets. Draws from low-dimensional distribu-\ntions at each step t are fed forward to subsequent steps to ultimately generate a full jet.\nalternative training method would correspond to optimizing Junipr for the application of\ndiscrimination, leaving intact our ability to visualize discrimination power in clustering trees,\nbut sacriﬁcing the probabilistic interpretation of the model’s output. We have not tested\ntraining with Eq. (4.2), and thus cannot attest to its practicality, but we suspect an approach\nalong these lines may be useful in certain contexts.\n4.2\nGeneration from JUNIPR\nWe now turn to a more familiar approach to jet physics, but a somewhat less appropriate\nusage of Junipr models: sampling new jets from the learned probability distribution to\ngenerate traditional observable distributions. We include this application here, not only to\ndemonstrate this capability, but also to further validate the distribution learned by Junipr\nduring unsupervised training.\nSampling from Junipr is relatively eﬃcient; one simply samples from the low dimensional\ndistributions at each step t and feeds those samples forward as input to subsequent steps. In\nthis way, one generates a full jet in many steps, as detailed in Fig. 14.\nWe used the baseline implementation of Junipr trained on quark jets, as described in\nSec. 3, to generate 100k jets in this way. The resulting jet mass and constituent multiplicity\n– 25 –\n−5\n−4\n−3\n−2\n−1\n0\nlog10(m2/E2)\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability density\ne+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr gen.\n0\n10\n20\n30\n40\nconstituent multiplicity\n0.00\n0.02\n0.04\n0.06\n0.08\nprobability\ne+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr gen.\nFigure 15: Jet mass (left) and constituent multiplicity (right) distributions computed on\njets sampled from Junipr and compared against Pythia jets in the validation set.\n−5\n−4\n−3\n−2\n−1\n0\nlog10(m2/E2)\n0\n10\n20\n30\n40\nconstituent multiplicity\nPythia freq.\ne+e−→q¯q\nC/A clustering\n0.02\n0.04\n0.06\n0.08\n−5\n−4\n−3\n−2\n−1\n0\nlog10(m2/E2)\n0\n10\n20\n30\n40\nconstituent multiplicity\nJunipr gen.\ne+e−→q¯q\nC/A clustering\n0.02\n0.04\n0.06\n0.08\nFigure 16: 2-dimensional probability distributions with respect to jet mass and constituent\nmultiplicity. (Left) Distribution computed using validation set of Pythia jets. (Right) Dis-\ntribution computed using jets sampled from Junipr.\ndistributions are plotted in Fig. 15 where both distributions sampled from Junipr match\nthose created from our validation set of 100k Pythia jets withheld from training. Reasonable\nagreement can also be seen in the 2D distributions of Fig. 16.\nHowever, there are two reasons why we do not consider Junipr to be built for generation.\n(These drawbacks could be avoided with a generative model; see [39–41].) The ﬁrst is simply\nthat sampling from probability distributions is generally diﬃcult. As we just showed, it turns\nout that Junipr is relatively easy to sample from, due to its sequential structure and the\nfact that distributions are low-dimensional at each t step. Despite this, sampling jets from\nJunipr is still much slower than generation with, for example, Pythia.\nThe second reason is more fundamental. With a sequential model structured as Junipr is,\n– 26 –\n0.0\n0.2\n0.4\n0.6\n0.8\nτ21\n0\n1\n2\n3\nprobability density\ne+e−→q¯q\nC/A clustering\nPythia freq.\nJunipr gen.\nFigure 17: 2-subjettiness ratio observable computed on jets sampled from Junipr. Dis-\nagreement with the distribution on Pythia jets, due to the feedback involved in sampling\nfrom Junipr, is visible. This disagreement is amended in Fig. 18.\nprobability distributions at late t steps in generation are highly sensitive to the draws made at\nearlier t steps. Very small defects in the probability distributions at early steps cause feedback\nin the model that ampliﬁes those errors. Furthermore, as a partially generated jet becomes\nmore misrepresentative of the training data, the resulting probability distributions used at\nlater steps are less trained, which can result in a run-away eﬀect. All of this is to say that,\nfor the purpose of generating jets, Junipr’s accuracy at early t steps is disproportionately\nimportant. This is in tension with the training method undertaken in Sec. 3.2, namely the\nmaximization of the log-likelihood, which prioritizes all branchings equally. Thus, we should\nexpect that some observable distributions generated by sampling jets from Junipr might\nagree worse with the validation set of Pythia data than otherwise expected. We mention in\npassing that this second drawback could be mitigated by reweighting jets after generation, as\ndetailed in Sec. 4.3 below.\nIn fact, we have found empirically that the N-subjettiness ratio observables computed\nby sampling from Junipr do not match the held-out Pythia data perfectly. This can be\nseen in Fig. 17 with the 2-subjettiness distribution, where the diﬀerence between the two\ndistributions is more signiﬁcant.\nWe consider this disagreement to be both expected and non-diminishing of Junipr’s\npotential. Indeed, in the next section we will show how to overcome this issue, by generating\nsamples consistent with Junipr’s learned probabilistic model, without ever sampling from it.\nIn particular, the disagreement in Fig. 17 will be rectiﬁed in Fig. 18.\n4.3\nReweighting Monte Carlo Events\nAnother application of the Junipr framework is to reweight events. For example, suppose we\ntrained Junipr on data from the Large Hadron Collider (LHC) to yield a probabilistic model\nPLHC. Then one could generate a sample of new events using a relatively accurate Monte\n– 27 –\nCarlo simulator, train another instance of Junipr on that sample to yield Psim, and ﬁnally\nreweight the simulated events by PLHC/Psim evaluated on an event-by-event basis.\nThis\nprocess yields a sample of events that is theoretically equivalent to the LHC data used in\ntraining PLHC. The advantage of such an approach is that Junipr can correct the simulated\nevents on diﬀerent levels, for example using the data reclustered in Rsub = 0.1 subjets as\nwe have done in this paper.\nHowever, the full simulated event has the complete hadron\ndistributions and can thereby be interfaced with a detector simulation. This is in many ways\na simpler approach than trying to improve the simulation directly through the dark art of\nMonte-Carlo tuning.\nThis reweighting is identical to importance sampling from a proposal distribution given\nby the simulated data distribution Psim. For example, suppose one wanted to measure the\ndistribution of an observable O(jet) at the LHC, which is given by\nP(O) =\nZ\nd[jet] PLHC(jet) δ(O −O(jet))\n≈1\nN\nX\njet∼PLHC\nδ(O −O(jet))\n(4.3)\nwhere the last approximation is associated with collecting a ﬁnite amount N of LHC data\nin order to measure the distribution. (The reader can substitute discretized delta functions\nappropriate for histogramming if averse to the singular notation used in these equations.)\nInstead of using real data, if say a public version of PLHC were available, then anyone could\ncalculate this observable distribution using only simulated data sampled from Psim as follows:\nP(O) =\nZ\nd[jet] Psim(jet) δ(O −O(jet)) PLHC(jet)\nPsim(jet)\n≈1\nN\nX\njet∼Psim\nδ(O −O(jet)) PLHC(jet)\nPsim(jet) .\n(4.4)\nIn this way, one could eﬃciently obtain samples of arbitrary size from PLHC by reweighting\nsamples generated by an eﬃcient simulator. The only limitation to this process is that the\nsimulated data must be similar to the actual target data, so that they have overlapping\nregions of support (formal requirement) and the weights are not too far from unity (eﬃciency\nrequirement).\nAs with the likelihood-ratio discrimination in Sec. 4.1, here we will show results in a toy\nscenario as a proof-of-principle. Ideally a model trained on LHC data, with all related compli-\ncations, would be used to reweight Monte Carlo jets to make the simulated data indiscernible\nfrom LHC data; we leave a proper study of this to future work.\nInstead, here we use two samples of jets generated using two diﬀerent versions of Pythia.\nWe reweight jets from one of the samples and demonstrate their agreement with the other\nsample. In particular, we use our baseline Junipr model trained on Pythia-generated quark\njets as our “true distribution”. For the moment, we will refer to this model as Pαs=0.1365,\n– 28 –\nsince its training data was generated using Pythia’s default value of αs(mZ) = 0.1365 in the\nﬁnal state shower. As our “simulated distribution” we will use Pαs=0.11, which was trained on\nquark jets generated with coupling parameter changed to αs(mZ) = 0.11 in Pythia’s ﬁnal-\nstate shower. (See Sec. 3.1 for a more in-depth description of the training data used.) Our goal\nis to show that reweighting jets from the “simulated distribution” according to the likelihood\nratio Pαs=0.1365/Pαs=0.11 leads to observables in agreement with the “true distribution”.\nIn Fig. 18 we demonstrate that this is indeed the case. We check this for both the 2-\nsubjettiness and 3-subjettiness ratio observables, as well as the jet shape observable. On\nthe left side of Fig. 18, one can see that in all cases, the αs = 0.11 distribution is clearly\ndiﬀerent from the αs = 0.1365 distribution. On the right side of Fig. 18, one ﬁnds that the\ntwo distributions come into relatively good agreement once the αs = 0.11 jets are reweighted\nby Pαs=0.1365/Pαs=0.11. This also provides further conﬁrmation that Junipr learns subtle\ncorrelations between constituent momenta inside jets.\nNote that it was the 2-subjettiness ratio observable that Junipr struggled to predict\nwell through direct sampling (see Fig. 17), whereas when reweighting another set of samples,\nJunipr matches the data well on this observable (see top-right of Fig. 18). This corroborates\nthe discussion in Sec. 4.2 concerning the diﬃculties in sampling directly from Junipr.\nBefore closing this section, let us reiterate one point mentioned above. For the procedure\nof reweighting events to be practical, the weights used should not be radically diﬀerent from\nunity, meaning that the two distributions generating the two samples should not be too\ndiﬀerent. If this condition is not satisﬁed, then away from the limit of inﬁnite statistics, a\nfew events with very large weights could vastly overpower the rest of the events, leading to\na choppy reweighted distribution with large statistical uncertainties. To avoid this problem\nin the toy scenario explored in this section, we found it necessary to discard roughly 0.1% of\nthe jets in the αs = 0.11 sample which were outliers with Pαs=0.1365/Pαs=0.11 > 100. These\noutliers were uncorrelated with the observables shown, and we believe they resulted from\nimperfections in the trained model. It is clear that much more needs to be understood about\nthe application of reweighting, but this would perhaps be more eﬀectively done in the context\nof a speciﬁc task of interest involving LHC data.\n5\nFactorization and JUNIPR\nIn the previous section, we showed some preliminary but very exciting results for likelihood-\nratio discrimination and for the generation of observables by reweighting simulated jets. Both\nof these applications require access to an unsupervised probabilistic model. Next we discuss\nsome of the more subtle internal workings of Junipr, which are intimately related to the\nunderlying physics of factorization.\nIn particular, we show that the hidden representation h(t) indeed stores important global\ninformation about intermediate states of jets in Sec. 5.1. We then discuss the clustering-\nalgorithm independence of Junipr by considering two distinct clustering algorithms: a “printer”\nalgorithm in Sec. 5.2, where momenta are processed left-to-right and top-to-bottom as if by\n– 29 –\n0.0\n0.2\n0.4\n0.6\n0.8\nτ21\n0\n1\n2\n3\nprobability density\nPythia e+e−→q¯q\nC/A clustering\nαs =0.1365\nαs =0.11\n0.0\n0.2\n0.4\n0.6\n0.8\nτ21\n0\n1\n2\n3\nprobability density\nPythia e+e−→q¯q\nC/A clustering\nαs =0.1365\nαs =0.11 wtd.\n0.0\n0.2\n0.4\n0.6\n0.8\nτ32\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nprobability density\nPythia e+e−→q¯q\nC/A clustering\nαs =0.1365\nαs =0.11\n0.0\n0.2\n0.4\n0.6\n0.8\nτ32\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nprobability density\nPythia e+e−→q¯q\nC/A clustering\nαs =0.1365\nαs =0.11 wtd.\n10−4\n10−3\n10−2\n10−1\n100\nangle\n0.00\n0.25\n0.50\n0.75\n1.00\ncumulative energy fraction\nPythia e+e−→q¯q\nC/A clustering\nαs =0.1365\nαs =0.11\n10−4\n10−3\n10−2\n10−1\n100\nangle\n0.00\n0.25\n0.50\n0.75\n1.00\ncumulative energy fraction\nPythia e+e−→q¯q\nC/A clustering\nαs =0.1365\nαs =0.11 wtd.\nFigure 18: (Left) Disagreement in observable distributions for two Pythia tunes of αs.\nObservables are the 2-subjettiness and 3-subjettiness ratio observables and the jet shape, from\ntop to bottom. (Right) Upon reweighting the αs = 0.11 jets by the ratio Pαs=0.1365/Pαs=0.11 of\nlearned underlying probability distributions, observable distributions exhibit good agreement.\n– 30 –\nan inkjet printer; and the anti-kt algorithm in Sec. 5.3, which allows us to present another\ncounterintuitive result, the anti-kt shower generator.\n5.1\nThe Encoding of Global Information\nWe have constructed Junipr so that all global information about the jet is contained in\nthe RNN’s hidden state h(t). Only the branching function Pbranch receives the local 1 →2\nbranching information in addition to h(t).\nThis forces h(t) to contain all the information\nneeded to predict when the shower should end, Pend, to predict which momentum should\nbranch next, Pmother, and to inform the branching function Pbranch of the relevant global\nstructure. As the primary feature vector for all three of these distinct tasks, h(t) must learn\nan eﬀective representation of the jet at evolution step t.\nTo explicitly show that h(t) stores important global information about the intermediate\njet state at step t, we train a new model on our baseline quark jet data (see Sec. 3.1) with the\ndiﬀerence that we remove h(t) as an input to the branching function Pbranch. We expect that\nsuch a “local” branching model will not evolve correctly as the global jet structure evolves,\nsince all global information is being withheld. This is indeed what we ﬁnd, as can be seen in\nFig. 19. On the left side of that ﬁgure, the evolution of the θ distribution (deﬁned in Fig. 6)\nfrom t = 1 to t = 2 is shown using 100k Pythia jets from our held-out set of validation data.\nThere we see the gradual decrease in angle as expected for C/A trees. On the right side of\nFig. 19, the evolution of the branching function is shown for the “local” branching model, and\nthe disagreement between this damaged model and Pythia is clear. Note that this prediction\nof incorrect distributions at intermediate branchings in the C/A tree will inevitably lead to\nan incorrect probability distribution Pjet({p1, . . . , pn}) over ﬁnal-state momenta.\nWhile we do not show the corresponding results from our baseline (global) model in Fig. 19\nto avoid clutter, the agreement with Pythia is essentially perfect, as one would expect from\nthe similar check performed in Fig. 9. This conﬁrms the success of the jet representation\nh(t) in supplying the branching function Pbranch with important information about the global\nstructure.\n5.2\nClustering Algorithm Independence\nAnother subtle aspect of Junipr is its theoretical clustering algorithm independence.\nIn\nprinciple, the model as described in Sec. 2.1 is indeed independent of the chosen algorithm,\nwhich is ﬁxed simply to avoid a sum over all possible trees consistent with the ﬁnal-state\nmomenta. That is, for each clustering procedure chosen by the user, a diﬀerent model is\nlearned, but one that describes the same probability distribution over ﬁnal-state momenta,\nat least formally.\nHowever, it is not guaranteed that a given neural-network implementation of Junipr\nwill work well for every clustering algorithm. We have chosen an architecture that stores the\nglobal jet physics in the RNN’s hidden state h(t) and the local 1 →2 branching physics in\nthe branching function Pbranch. This architecture is motivated by the factorizing structure\nof QCD, and thus Junipr will most easily learn jet trees that are most similar to QCD —\n– 31 –\n0.05\n0.1\n0.3\n1\nπ/2\nθ\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability\nPythia e+e−→q¯q\nC/A clustering\nPythia (t=1)\nPythia (t=2)\n0.05\n0.1\n0.3\n1\nπ/2\nθ\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability\nPythia e+e−→q¯q\nC/A clustering\nJunipr local (t=1)\nJunipr local (t=2)\nFigure 19: (Left) Evolution of the θ distribution from t = 1 to t = 2 in the validation set\nof Pythia jets. (Right) Corresponding evolution of the branching function as predicted by\na “local” branching model without access to the hidden representation h(t). Disagreement\nbetween Pythia and this local model is clear. Not shown is the result using our baseline\n(global) model, which agrees perfectly with Pythia, as expected from Fig. 9.\nour primary reason for predominantly using the C/A algorithm. Consequently, though the\nmodel described in Sec. 2.1 is formally independent of clustering algorithm, the particular\nimplementation adopted in Sec. 2.2 may weakly depend on the chosen algorithm by virtue of\nthe ease with which it can learn the data.\nTo put this to the test, we have introduced a jet clustering algorithm that is nothing like\nQCD, but more like a 2D printer.3 The “printer” clustering algorithm scans the 2D jet image\n(i.e. the cross sectional image perpendicular to the jet axis) from right-to-left and bottom-to-\ntop, clustering particles as it encounters them. Run in reverse (i.e. as a shower) particles are\nemitted from the jet core from left-to-right and top-to-bottom; this is how a jet image would\nbe printed by an inkjet printer with a single printing tip. In Fig. 20 we show a single Pythia\njet clustered using the printer algorithm. As can be seen in the jet image on the right side\nof Fig. 20, momenta are indeed emitted top-to-bottom. On the left side of Fig. 20, we see\nthat any collinear branching structure is completely absent from the clustering tree; instead,\nparticles are steadily emitted up-and-to-the-left.\nThough Junipr’s neural network architecture is not optimized for the informational\nstructure of the printer algorithm, it is still able to learn the structure, by relying much more\nheavily on the the jet representation h(t). We demonstrate this by training Junipr on our\ndata set of Pythia-generated quark jets (see Sec. 3.1) clustered with the printer algorithm,\nthus yielding the probabilistic model Pprinter. Indeed, in Fig. 21 one can see a jet sampled\nfrom Pprinter, which correctly follows the printer structure.\nAs expected, however, the distributions sampled from Pprinter are not quite as good\n3We thank Eric Metodiev for this suggestion.\n– 32 –\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n8yy :2o\nky :2o\nR :2o\nSvi?B\u001c [m\u001c`F D2i\nT`BMi2` +Hmbi2`BM;\nFigure 20: A single Pythia jet clustered using the printer algorithm. Shown are its clus-\ntering tree (left) and jet image (right) in which colors correspond to energies and polar\ncoordinates correspond to the θ and φ values of the momenta. Each momentum is labelled\nby its corresponding step t in the clustering tree.\n1\n2\n4\n3\n6\n7\n8\n9\n10\n5\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n8yy :2o\nky :2o\nR :2o\nCmMBT` [m\u001c`F D2i\nT`BMi2` +Hmbi2`BM;\nFigure 21: A single jet sampled from Junipr, which was trained on Pythia-generated\nquark jets that were clustered using the printer algorithm. The sampled jet emits with the\ncorrect printer structure, as can be seen by its emission tree (left) and jet image (right). Each\nmomentum is labelled by the step t at which it was emitted during generation from Junipr.\nas our C/A results.\nOn the left side of Fig. 22 we show the 2-dimensional distribution\nover jet mass and constituent multiplicity generated using 100k jets sampled directly from\nPprinter. Comparing to the distribution generated by Pythia (see the left side of Fig. 16) this\ndistribution matches well. However, for the 2-subjettiness ratio observable on the right side\nof Fig. 22 we get a somewhat worse match to the Pythia validation data; compare this to\nthe results of the C/A model in Fig. 17. Of course, we discussed in Sec. 4.2 why we do not\nexpect direct sampling from Junipr to be perfectly reliable (and we discussed a way around\nthis in Sec. 4.3), but it is still clear that such distributions are comparably worse when using\n– 33 –\n−5\n−4\n−3\n−2\n−1\n0\nlog10(m2/E2)\n0\n10\n20\n30\n40\nconstituent multiplicity\nJunipr gen.\ne+e−→q¯q\nprinter clustering\n0.02\n0.04\n0.06\n0.0\n0.2\n0.4\n0.6\n0.8\nτ21\n0\n1\n2\n3\nprobability density\ne+e−→q¯q\nprinter clustering\nPythia freq.\nJunipr gen.\nFigure 22: (Left) 2-dimensional distribution with respect to jet mass and constituent mul-\ntiplicity, calculated by sampling jets directly from Pprinter, an instance of Junipr trained on\njets clustered with the printer algorithm. (Right) 2-subjettiness ratio observable distribution\ngenerated using Pprinter and compared to the corresponding distribution on Pythia jets in\nthe validation set.\nthe printer clustering algorithm, instead of the more natural C/A algorithm.\n5.3\nAnti-kt Shower Generator\nReassured by the results of the previous section, we next consider Junipr trained on Pythia\njets reclustered with anti-kt [58]. Like the printer algorithm, anti-kt does not approximate\nthe natural collinear structure of QCD. Unlike the printer algorithm, however, anti-kt is a\nvery commonly used tool. For the latter reason we explore anti-kt jets here.\nPerhaps the most interesting result associated with an anti-kt version of Junipr is that\nit provides access to an anti-kt shower generator. Generating an anti-kt shower is counter-\nintuitive, because the anti-kt algorithm generally clusters soft emissions one-by-one with the\nhard jet core. Thus, a generator must remember where previous emissions landed in order to\nsend subsequent emissions nearby. This is required to reproduce the correct collinear struc-\nture in the distribution of ﬁnal-state of momenta. Said in another way, since the collinear\nfactorization of QCD is not built into the anti-kt clustering algorithm, a local (or factorized)\nanti-kt generator could not produce emissions with the correct collinear distribution. Thus,\nwe should expect that, in an anti-kt version of Junipr, higher demands will be placed on the\njet representation h(t) to monitor all the radiation in the jet. This is certainly possible, but\nnot the task for which our neural network architecture is optimized.\nTo see to what extent an anti-kt implementation of Junipr relies on the global information\nstored in h(t), we trained two models on Pythia-generated quark jets clustered with anti-kt\n(see Sec. 3.1 for more details on the training data used). One model, Panti, has the baseline\narchitecture outlined in Sec. 2. The other, Panti-local, is a local branching model like the one\n– 34 –\n8yy :2o\nky :2o\nR :2o\nCmMBT` [m\u001c`F D2ib\n\u001cMiB@kt +Hmbi2`BM;\nUiQTV rBi?Qmi h(t)\nU#QiiQKV rBi? h(t)\nFigure 23: (Top) Shower sampled from an anti-kt version of Junipr, but one in which\nthe global representation h(t) is withheld from the branching function. Correlation between\nemission directions is absent in this case. (Bottom) Shower sampled from an anti-kt version\nof Junipr, using the standard architecture complete with h(t). Strong coherence in emission\ndirections is clearly evident.\nused in Sec. 5.1, in which the global representation h(t) is withheld as input to the branching\nfunction.\nIn Fig. 23 (bottom) we show a jet sampled from Panti. In this case, though the tree\nitself does not properly guide the collinear structure of emissions, one can see that the emis-\nsion directions are highly correlated with one another, demonstrating the success of the jet\nrepresentation h(t) in tracking the global branching pattern. In Fig. 23 (top) we show for\ncomparison a jet sampled from Panti-local, in which the branching function does not receive\nh(t). In the latter case, all correlation between the emission directions is lost. This shows\nthat the global representation h(t) is crucial for a successful anti-kt branching model.\nIn Fig. 24 we show the 2-dimensional distribution over jet mass and constituent multi-\nplicity, as well as the 2-subjettiness distribution, generated with Panti. One can see that the\nformer distribution is consistent with the distribution generated by Pythia in Fig. 16. Mild\ndisagreement between Panti’s 2-subjettiness distribution and Pythia’s can be seen on the\nright side of Fig. 24. This is on par with the agreement obtained by sampling from the C/A\nmodel in Fig. 17.\nIn Sec. 5.1 we saw that the RNN’s hidden state h(t) manages the global information in\nJunipr’s neural network architecture. This is an eﬃcient and natural way to characterize\nQCD-like jets, and therefore also C/A clustering trees. Though Junipr is formally indepen-\ndent of jet algorithm (i.e. in the inﬁnite-capacity and perfect-training limit), we might expect\nJunipr’s performance to degrade somewhat when paired with clustering algorithms that re-\nquire signiﬁcantly more information to be stored in h(t). This was explored in Secs. 5.2 and\n5.3 using two separate non-QCD-like clustering algorithms, namely the “printer” and anti-kt\n– 35 –\n−5\n−4\n−3\n−2\n−1\n0\nlog10(m2/E2)\n0\n10\n20\n30\n40\nconstituent multiplicity\nJunipr gen.\ne+e−→q¯q\nanti-kt clustering\n0.02\n0.04\n0.06\n0.08\n0.0\n0.2\n0.4\n0.6\n0.8\nτ21\n0\n1\n2\n3\nprobability density\ne+e−→q¯q\nanti-kt clustering\nPythia freq.\nJunipr gen.\nFigure 24: (Left) 2-dimensional distribution over jet mass and constituent multiplicity,\ncalculated by sampling jets directly from an anti-kt implementation of Junipr.\n(Right)\n2-subjettiness ratio observable distribution sampled from this model and compared to the\ndistribution on Pythia jets in the validation set.\nalgorithms. Despite these clustering algorithms being unnatural choices for Junipr, we were\nable to demonstrate conceptually interesting and novel results, such as the anti-kt shower\ngenerator. This further demonstrates that Junipr can continue to function well, even when\nthe clustering algorithm chosen for implementation bears little resemblance to the underlying\nphysical processes that generate the data.\n6\nConclusions and Outlook\nIn this paper, we have introduced Junipr as a framework for unsupervised machine learn-\ning in particle physics. The framework calls for a neural network architecture designed to\neﬃciently describe the leading-order physics of 1 →2 splittings, alongside a representation\nof the global jet physics. This requires the momenta in a jet to be clustered into a binary\ntree. The choice of clustering algorithm is not essential to Junipr’s performance, but choos-\ning an algorithm that has some correspondence with an underlying physical model, such as\nthe angular-ordered parton shower in quantum chromodynamics, gives improved performance\nand allows for intrerpretability of the network. At Junipr’s core is a recurrent neural network\nwith three interconnected components. It moves along the jet’s clustering tree, evaluating\nthe likelihood of each branching. More generally, Junipr is a function that acts on a set of\n4-momenta in an event to compute their relative diﬀerential cross section, i.e. the probability\ndensity for this event to occur, given the event selection criteria used to select the training\nsample. One of the appealing features of Junipr is its interpretability: it provides a de-\nsconstruction of the probability density into contributions from each point in the clustering\nhistory.\n– 36 –\nThere are many promising applications of Junipr, and we have only been able to touch on\na few proof-of-concept tests in this introductory work. One exciting use case is discrimination.\nIn contrast to supervised models which directly learn to discriminate between two samples,\nJunipr learns the features of the samples separately. It then discriminates by comparing the\nlikelihood of a given event with respect to alternative models of the underlying physics. The\nresulting likelihood ratio provides theoretically optimal statistical power. As an example, we\nshowed that Junipr can discriminate between boosted Z bosons and quark jets (in a very\ntight mass window around mZ) in e+e−events when trained on the two samples separately.\nWith Junipr, it is not only possible to perform powerful discrimination using unsupervised\nlearning, but the discrimination power can be visualized over the entire clustering tree of each\njet, as in Fig. 13. This opens new avenues for physicists to gain intuition about the physics\nunderlying high-performance discrimination. Such studies might even inspire the construction\nof new calculable observables.\nAnother exciting potential application of Junipr is the reweighting of Monte Carlo events,\nin order to improve agreement with real collider data. A proof-of-concept of this idea was\ngiven in Fig. 18, where jets generated with one Pythia tune were reweighted to match jets\ngenerated with another. The reason this application is important is that current Monte Carlo\nevent generators do an excellent job of simulating events on average, but are limited by the\nmodels and parameters within them. It may be easier to correct for systematic bias in event\ngeneration by a small reweighting factor appropriate for a particular data sample, rather than\nby trying to isolate and improve faulty components of the model. In this context, Junipr\ncan be thought of as providing small but highly granular tweaks to simulations in order to\nimprove agreement with data.\nThe Junipr framework was used here to compute the likelihood that a given set of\nparticle momenta will arise inside a jet. One can also imagine more general models that act\non all the momenta in an entire event, including particle identiﬁcation tags, or even low-level\ndetector data. A particularly interesting direction would be to consider applying Junipr\nto heavy ion collisions, in which the medium where the jets are produced and evolve is not\nyet well understood. In this case, comparing the probabilities in data to those of simulation\ncould give insights into how to improve the simulations, or more optimistically, to improve\nour understanding of the underlying physics.\nAcknowledgments\nWe beneﬁted from interesting discussions with D. Barber, E. Bernton, A. Botev, Y.-T. Chien,\nK. Cranmer, R. Elayavalli, M. Freytsis, B. Gaujac, R. Habib, P. Komiske, E. Metodiev,\nB. Nachman, and J. Thaler. AA, CF, and MDS are supported in part by the Department of\nEnergy under contract DE-SC0013607. Support for AA and CF was provided in part by the\nHarvard Data Science Initiative.\n– 37 –\nReferences\n[1] A. Krizhevsky, I. Sutskever and G. E. Hinton, Imagenet classiﬁcation with deep convolutional\nneural networks, in Advances in neural information processing systems, pp. 1097–1105, 2012.\n[2] K. He, X. Zhang, S. Ren and J. Sun, Deep residual learning for image recognition, pp. 770–778,\n2016. 1512.03385.\n[3] G. Huang, Z. Liu and K. Q. Weinberger, Densely connected convolutional networks, 2017.\n1608.06993.\n[4] D. Bahdanau, K. Cho and Y. Bengio, Neural machine translation by jointly learning to align\nand translate, 2014. 1409.0473.\n[5] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey et al., Google’s neural\nmachine translation system: Bridging the gap between human and machine translation,\n1609.08144.\n[6] A. Graves and N. Jaitly, Towards end-to-end speech recognition with recurrent neural networks,\n2014.\n[7] A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves et al., Wavenet: A\ngenerative model for raw audio, 2016. 1609.03499.\n[8] D. E. Rumelhart, G. E. Hinton and R. J. Williams, Learning representations by\nback-propagating errors, nature 323 (1986) 533.\n[9] T. Mikolov, M. Karaﬁ´at, L. Burget, J. ˇCernock`y and S. Khudanpur, Recurrent neural network\nbased language model, in Eleventh Annual Conference of the International Speech\nCommunication Association, 2010.\n[10] ATLAS collaboration, G. Aad et al., A neural network clustering algorithm for the ATLAS\nsilicon pixel detector, JINST 9 (2014) P09009, [1406.7690].\n[11] ATLAS collaboration, G. Aad et al., Performance of b-Jet Identiﬁcation in the ATLAS\nExperiment, JINST 11 (2016) P04008, [1512.01094].\n[12] CMS collaboration, S. Chatrchyan et al., Performance of tau-lepton reconstruction and\nidentiﬁcation in CMS, JINST 7 (2012) P01001, [1109.6034].\n[13] K. Datta and A. Larkoski, How Much Information is in a Jet?, JHEP 06 (2017) 073,\n[1704.08249].\n[14] K. Datta and A. J. Larkoski, Novel Jet Observables from Machine Learning, JHEP 03 (2018)\n086, [1710.01305].\n[15] H. Luo, M.-x. Luo, K. Wang, T. Xu and G. Zhu, Quark jet versus gluon jet: deep neural\nnetworks with high-level features, 1712.03634.\n[16] P. T. Komiske, E. M. Metodiev and J. Thaler, Energy ﬂow polynomials: A complete linear basis\nfor jet substructure, 1712.07124.\n[17] J. Gallicchio, J. Huth, M. Kagan, M. D. Schwartz, K. Black and B. Tweedie, Multivariate\ndiscrimination and the Higgs + W/Z search, JHEP 04 (2011) 069, [1010.3698].\n[18] ATLAS Collaboration collaboration, Identiﬁcation of Hadronically-Decaying W Bosons and\nTop Quarks Using High-Level Features as Input to Boosted Decision Trees and Deep Neural\n– 38 –\nNetworks in ATLAS at √s = 13 TeV, Tech. Rep. ATL-PHYS-PUB-2017-004, CERN, Geneva,\nApr, 2017.\n[19] J. Cogan, M. Kagan, E. Strauss and A. Schwarztman, Jet-Images: Computer Vision Inspired\nTechniques for Jet Tagging, JHEP 02 (2015) 118, [1407.5675].\n[20] L. de Oliveira, M. Kagan, L. Mackey, B. Nachman and A. Schwartzman, Jet-images deep\nlearning edition, JHEP 07 (2016) 069, [1511.05190].\n[21] P. T. Komiske, E. M. Metodiev and M. D. Schwartz, Deep learning in color: towards automated\nquark/gluon jet discrimination, JHEP 01 (2017) 110, [1612.01551].\n[22] P. T. Komiske, E. M. Metodiev, B. Nachman and M. D. Schwartz, Pileup Mitigation with\nMachine Learning (PUMML), JHEP 12 (2017) 051, [1707.08600].\n[23] G. Kasieczka, T. Plehn, M. Russell and T. Schell, Deep-learning Top Taggers or The End of\nQCD?, JHEP 05 (2017) 006, [1701.08784].\n[24] W. Bhimji, S. A. Farrell, T. Kurth, M. Paganini, Prabhat and E. Racah, Deep Neural Networks\nfor Physics Analysis on low-level whole-detector data at the LHC, in 18th International\nWorkshop on Advanced Computing and Analysis Techniques in Physics Research (ACAT 2017)\nSeattle, WA, USA, August 21-25, 2017, 2017. 1711.03573.\n[25] ATLAS Collaboration collaboration, Quark versus Gluon Jet Tagging Using Jet Images\nwith the ATLAS Detector, Tech. Rep. ATL-PHYS-PUB-2017-017, CERN, Geneva, Jul, 2017.\n[26] S. Macaluso and D. Shih, Pulling Out All the Tops with Computer Vision and Deep Learning,\n1803.00107.\n[27] Y.-T. Chien and R. Kunnawalkam Elayavalli, Probing heavy ion collisions using quark and\ngluon jet substructure, 1803.03589.\n[28] J. Pearkes, W. Fedorko, A. Lister and C. Gay, Jet Constituents for Deep Neural Network Based\nTop Quark Tagging, 1704.02124.\n[29] G. Louppe, K. Cho, C. Becot and K. Cranmer, QCD-Aware Recursive Neural Networks for Jet\nPhysics, 1702.00748.\n[30] T. Cheng, Recursive Neural Networks in Quark/Gluon Tagging, 1711.02633.\n[31] S. Egan, W. Fedorko, A. Lister, J. Pearkes and C. Gay, Long Short-Term Memory (LSTM)\nnetworks with jet constituents for boosted top tagging at the LHC, 1711.09059.\n[32] K. Fraser and M. D. Schwartz, Jet Charge and Machine Learning, 1803.08066.\n[33] D. Guest, J. Collado, P. Baldi, S.-C. Hsu, G. Urban and D. Whiteson, Jet Flavor Classiﬁcation\nin High-Energy Physics with Deep Neural Networks, Phys. Rev. D94 (2016) 112002,\n[1607.08633].\n[34] ATLAS Collaboration collaboration, Identiﬁcation of Jets Containing b-Hadrons with\nRecurrent Neural Networks at the ATLAS Experiment, Tech. Rep. ATL-PHYS-PUB-2017-003,\nCERN, Geneva, Mar, 2017.\n[35] E. M. Metodiev, B. Nachman and J. Thaler, Classiﬁcation without labels: Learning from mixed\nsamples in high energy physics, JHEP 10 (2017) 174, [1708.02949].\n– 39 –\n[36] T. Cohen, M. Freytsis and B. Ostdiek, (Machine) Learning to Do More with Less, JHEP 02\n(2018) 034, [1706.09451].\n[37] P. T. Komiske, E. M. Metodiev, B. Nachman and M. D. Schwartz, Learning to Classify from\nImpure Samples, 1801.10158.\n[38] E. M. Metodiev and J. Thaler, On the Topic of Jets, 1802.00008.\n[39] L. de Oliveira, M. Paganini and B. Nachman, Learning Particle Physics by Example:\nLocation-Aware Generative Adversarial Networks for Physics Synthesis, Comput. Softw. Big\nSci. 1 (2017) 4, [1701.05927].\n[40] M. Paganini, L. de Oliveira and B. Nachman, Accelerating Science with Generative Adversarial\nNetworks: An Application to 3D Particle Showers in Multilayer Calorimeters, Phys. Rev. Lett.\n120 (2018) 042003, [1705.02355].\n[41] M. Paganini, L. de Oliveira and B. Nachman, CaloGAN : Simulating 3D high energy particle\nshowers in multilayer electromagnetic calorimeters with generative adversarial networks, Phys.\nRev. D97 (2018) 014021, [1712.10321].\n[42] J. Neyman and E. S. Pearson, Ix. on the problem of the most eﬃcient tests of statistical\nhypotheses, Philosophical Transactions of the Royal Society of London A: Mathematical,\nPhysical and Engineering Sciences 231 (1933) 289–337,\n[http://rsta.royalsocietypublishing.org/content/231/694-706/289.full.pdf].\n[43] A. Butter, G. Kasieczka, T. Plehn and M. Russell, Deep-learned Top Tagging with a Lorentz\nLayer, 1707.08966.\n[44] S. Coleman and R. Norton, Singularities in the physical region, Nuovo Cim. 38 (1965) 438–442.\n[45] J. C. Collins, D. E. Soper and G. F. Sterman, Factorization for Short Distance Hadron -\nHadron Scattering, Nucl.Phys. B261 (1985) 104.\n[46] J. C. Collins, D. E. Soper and G. F. Sterman, Soft Gluons and Factorization, Nucl.Phys. B308\n(1988) 833.\n[47] I. Feige and M. D. Schwartz, Hard-soft-collinear factorization to all orders, Physical Review D\n90 (2014) 105020.\n[48] I. Feige and M. D. Schwartz, An on-shell approach to factorization, Physical Review D 88\n(2013) 065021.\n[49] S. Catani, Y. L. Dokshitzer, M. H. Seymour and B. R. Webber, Longitudinally invariant Kt\nclustering algorithms for hadron hadron collisions, Nucl. Phys. B406 (1993) 187–224.\n[50] S. D. Ellis and D. E. Soper, Successive combination jet algorithm for hadron collisions, Phys.\nRev. D48 (1993) 3160–3166, [hep-ph/9305266].\n[51] Y. L. Dokshitzer, G. D. Leder, S. Moretti and B. R. Webber, Better jet clustering algorithms,\nJHEP 08 (1997) 001, [hep-ph/9707323].\n[52] M. Wobisch and T. Wengler, Hadronization corrections to jet cross-sections in deep inelastic\nscattering, in Monte Carlo generators for HERA physics. Proceedings, Workshop, Hamburg,\nGermany, 1998-1999, pp. 270–279, 1998. hep-ph/9907280.\n[53] S. D. Ellis, A. Hornig, T. S. Roy, D. Krohn and M. D. Schwartz, Qjets: A Non-Deterministic\nApproach to Tree-Based Jet Substructure, Phys. Rev. Lett. 108 (2012) 182003, [1201.1914].\n– 40 –\n[54] D. Kahawala, D. Krohn and M. D. Schwartz, Jet Sampling: Improving Event Reconstruction\nthrough Multiple Interpretations, JHEP 06 (2013) 006, [1304.2394].\n[55] L. Mackey, B. Nachman, A. Schwartzman and C. Stansbury, Fuzzy Jets, JHEP 06 (2016) 010,\n[1509.02216].\n[56] D. E. Soper and M. Spannowsky, Finding physics signals with shower deconstruction, Phys.\nRev. D84 (2011) 074002, [1102.3480].\n[57] D. E. Soper and M. Spannowsky, Finding physics signals with event deconstruction, Phys. Rev.\nD89 (2014) 094005, [1402.1189].\n[58] M. Cacciari, G. P. Salam and G. Soyez, The Anti-k(t) jet clustering algorithm, JHEP 04 (2008)\n063, [0802.1189].\n[59] M. Cacciari, G. P. Salam and G. Soyez, FastJet User Manual, Eur. Phys. J. C72 (2012) 1896,\n[1111.6097].\n[60] I. Goodfellow, Y. Bengio and A. Courville, Deep Learning. MIT Press, 2016.\n[61] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk et al.,\nLearning phrase representations using rnn encoder–decoder for statistical machine translation,\nin Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pp. 1724–1734, Association for Computational Linguistics, 2014. DOI.\n[62] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural Comput. 9 (Nov., 1997)\n1735–1780.\n[63] T. Sjostrand, S. Mrenna and P. Z. Skands, PYTHIA 6.4 Physics and Manual, JHEP 05 (2006)\n026, [hep-ph/0603175].\n[64] T. Sjstrand, S. Ask, J. R. Christiansen, R. Corke, N. Desai, P. Ilten et al., An Introduction to\nPYTHIA 8.2, Comput. Phys. Commun. 191 (2015) 159–177, [1410.3012].\n[65] Theano Development Team collaboration, R. Al-Rfou, G. Alain, A. Almahairi,\nC. Angermueller, D. Bahdanau, N. Ballas et al., Theano: A Python framework for fast\ncomputation of mathematical expressions, arXiv e-prints abs/1605.02688 (May, 2016) .\n[66] F. Morin and Y. Bengio, Hierarchical probabilistic neural network language model, in\nAISTATS05, pp. 246–252, 2005.\n[67] A. Mnih and G. E. Hinton, A scalable hierarchical distributed language model, in Advances in\nNeural Information Processing Systems 21 (D. Koller, D. Schuurmans, Y. Bengio and\nL. Bottou, eds.), pp. 1081–1088. Curran Associates, Inc., 2009.\n[68] J. Thaler and K. Van Tilburg, Identifying Boosted Objects with N-subjettiness, JHEP 03 (2011)\n015, [1011.2268].\n[69] Y.-T. Chien, A. Emerman, S.-C. Hsu, S. Meehan and Z. Montague, Telescoping jet\nsubstructure, 1711.11041.\n[70] J. Gallicchio and M. D. Schwartz, Seeing in Color: Jet Superstructure, Phys. Rev. Lett. 105\n(2010) 022001, [1001.5027].\n– 41 –\n",
  "categories": [
    "hep-ph",
    "stat.ML"
  ],
  "published": "2018-04-25",
  "updated": "2018-04-25"
}