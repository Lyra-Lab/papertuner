{
  "id": "http://arxiv.org/abs/1804.09812v2",
  "title": "Improved Classification Based on Deep Belief Networks",
  "authors": [
    "Jaehoon Koo",
    "Diego Klabjan"
  ],
  "abstract": "For better classification generative models are used to initialize the model\nand model features before training a classifier. Typically it is needed to\nsolve separate unsupervised and supervised learning problems. Generative\nrestricted Boltzmann machines and deep belief networks are widely used for\nunsupervised learning. We developed several supervised models based on DBN in\norder to improve this two-phase strategy. Modifying the loss function to\naccount for expectation with respect to the underlying generative model,\nintroducing weight bounds, and multi-level programming are applied in model\ndevelopment. The proposed models capture both unsupervised and supervised\nobjectives effectively. The computational study verifies that our models\nperform better than the two-phase training approach.",
  "text": "arXiv:1804.09812v2  [stat.ML]  12 Aug 2019\nImproved Classiﬁcation Based on Deep Belief\nNetworks\nJaehoon Koo\nNorthwestern University\nEvanston, IL, USA\njaehoonkoo2018@u.northwestern.edu\nDiego Klabjan\nNorthwestern University\nEvanston, IL, USA\nd-klabjan@northwestern.edu\nAbstract—For better classiﬁcation generative models are used\nto initialize the model and model features before training a\nclassiﬁer. Typically it is needed to solve separate unsupervised and\nsupervised learning problems. Generative restricted Boltzmann\nmachines and deep belief networks are widely used for unsuper-\nvised learning. We developed several supervised models based\non DBN in order to improve this two-phase strategy. Modifying\nthe loss function to account for expectation with respect to\nthe underlying generative model, introducing weight bounds,\nand multi-level programming are applied in model development.\nThe proposed models capture both unsupervised and supervised\nobjectives effectively. The computational study veriﬁes that our\nmodels perform better than the two-phase training approach.\nIndex Terms—deep learning, neural networks, classiﬁcation\nI. INTRODUCTION\nRestricted Boltzmann machine (RBM), an energy-based\nmodel to deﬁne an input distribution, is widely used to\nextract latent features before classiﬁcation. Such an approach\ncombines unsupervised learning for feature modeling and\nsupervised learning for classiﬁcation. Two training steps are\nneeded. The ﬁrst step, called pre-training, is to model features\nused for classiﬁcation. This can be done by training RBM that\ncaptures the distribution of input. The second step, called ﬁne-\ntuning, is to train a separate classiﬁer based on the features\nfrom the ﬁrst step [1]. This two-phase training approach for\nclassiﬁcation is also used for deep networks. Deep belief\nnetworks (DBN) are built with stacked RBMs, and trained in\na layer-wise manner [2]. Two-phase training based on a deep\nnetwork consists of DBN and a classiﬁer on top of it.\nThe two-phase training strategy has three possible problems.\n1) It requires two training processes; one for training RBMs\nand one for training a classiﬁer. 2) It is not guaranteed\nthat the modeled features in the ﬁrst step are useful in the\nclassiﬁcation phase since they are obtained independently of\nthe classiﬁcation task. 3) It is an effort to decide which\nclassiﬁer is the best for each problem. Therefore, there is a\nneed for a method that can conduct feature modeling and\nclassiﬁcation concurrently [1].\nTo resolve these problems, recent papers suggest to trans-\nform RBM to a model that can deal with both unsupervised\nand supervised learning. Since RBM calculate the joint and\nconditional probabilities, the suggested prior models combine\na generative and discriminative RBM. Consequently, this\nhybrid discriminative RBM is trained concurrently for both\nobjectives by summing the two contributions [1], [3]. In a sim-\nilar way a self-contained RBM for classiﬁcation is developed\nby applying the free-energy function based approximation to\nRBM, which was used for a supervised learning method,\nreinforcement learning [4]. However, these approaches are\nlimited to transforming RBM that is a shallow network.\nIn this study, we developed alternative models to solve a\nclassiﬁcation problem based on DBN. Viewing the two-phase\ntraining as two separate optimization problems, we applied\noptimization modeling techniques in developing our models.\nOur ﬁrst approach is to design new objective functions. We\ndesign an expected loss function based on p(h|x) built by DBN\nand the loss function of the classiﬁer. Second, we introduce\nconstraints that bound the DBN weights in the feed-forward\nphase. The constraints keep a good representation of input\nas well as regularize the weights during updates. Third, we\napplied bilevel programming to the two-phase training method.\nThe bilevel model has a loss function of the classiﬁer in\nits objective function but it constrains the DBN values to\nthe optimal to phase-1. This model searches possible optimal\nsolutions for the classiﬁcation objective only where DBN\nobjective solutions are optimal.\nOur main contributions are several classiﬁcation models\ncombining DBN and a loss function in a coherent way. In\nthe computational study we verify that the suggested models\nperform better than the two-phase method.\nII. LITERATURE REVIEW\nThe two-phase training strategy has been applied to many\nclassiﬁcation tasks on different types of data. Two-phase train-\ning with RBM and support vector machine (SVM) has been\nexplored in classiﬁcation tasks on images, documents, and\nnetwork intrusion data\n[5], [6], [7], [8]. Logistic regression\nreplacing SVM has been explored [9], [10]. Gehler et al. [11]\nused the 1-nearest neighborhood classiﬁer with RBM to solve\na document classiﬁcation task. Hinton and Salakhutdinov [2]\nsuggested DBN consisting of stacked RBMs that is trained in\na layer-wise manner. Two-phase method using DBN and deep\nneural network has been studied to solve various classiﬁcation\nproblems such as image and text recognition [2], [12], [13].\nRecently, this approach has been applied to motor imagery\nclassiﬁcation in the area of brain–computer interface [14],\nbiomedical research, classiﬁcation of Cytochrome P450 1A2\ninhibitors and non-inhibitors [15], and web spam classiﬁcation\nthat detects web pages deliberately created to manipulate\nsearch rankings\n[16]. All these papers rely on two distinct\nphases, while our models assume a holistic view of both\naspects.\nMany studies have been conducted to improve the problems\nof two-phase training. Most of the research has been focused\non transforming RBM so that the modiﬁed model can achieve\ngenerative and discriminative objectives at the same time.\nSchmah et al. [17] proposed a discriminative RBM method,\nand subsequently classiﬁcation is done in the manner of a\nBayes classiﬁer. However, this method cannot capture the\nrelationship between the classes since the RBM of each class\nis trained separately. Larochelle et al. [1], [3] proposed a self-\ncontained discriminative RBM framework where the objective\nfunction consists of the generative learning objective p(x, y),\nand the discriminative learning objective, p(y|x). Both dis-\ntributions are derived from RBM. Similarly, a self-contained\ndiscriminative RBM method for classiﬁcation is proposed [4].\nThe free-energy function based approximation is applied in\nthe development of this method, which is initially suggested\nfor reinforcement learning. This prior paper relying on RBM\nconditional probability while we handle general loss functions.\nOur models also hinge on completely different principles.\nIII. BACKGROUND\na) Restricted Boltzmann Machines: RBM is an energy-\nbased probabilistic model, which is a restricted version of\nBoltzmann machines (BM) that is a log-linear Markov Ran-\ndom Field. It has visible nodes x corresponding to input\nand hidden nodes h matching the latent features. The joint\ndistribution of the visible nodes x ∈RJ and hidden variable\nh ∈RI is deﬁned as\np(x, h) = 1\nZ e−E(x,h), E(x, h) = −hWx −ch −bx\nwhere W ∈RI×J, b ∈RJ, and c ∈RI are the model\nparameters, and Z is the partition function. Since units in a\nlayer are independent in RBM, we have the following form of\nconditional distributions:\np(h|x) =\nIY\ni=1\np(hi|x), p(x|h) =\nJ\nY\nj=1\np(xj|h).\nFor binary units where x ∈{0, 1}J and h ∈{0, 1}I, we\ncan write p(hi = 1|h) = σ(ci + Wix) and p(xj = 1|h) =\nσ(bj +Wjx) where σ() is the sigmoid function. In this manner\nRBM with binary units is an unsupervised neural network\nwith a sigmoid activation function. The model calibration\nof RBM can be done by minimizing negative log-likelihood\nthrough gradient descent. RBM takes advantage of having the\nabove conditional probabilities which enable to obtain model\nsamples easier through a Gibbs sampling method. Contrastive\ndivergence (CD) makes Gibbs sampling even simpler: 1) start\na Markov chain with training samples, and 2) stop to obtain\nsamples after k steps. It is shown that CD with a few steps\nperforms effectively [18], [19].\nb) Deep Belief Networks: DBN is a generative graphical\nmodel consisting of stacked RBMs. Based on its deep structure\nDBN can capture a hierarchical representation of input data.\nHinton et al. (2006) introduced DBN with a training algorithm\nthat greedily trains one layer at a time. Given visible unit x\nand ℓhidden layers the joint distribution is deﬁned as [18],\n[20]\np(x, h1, · · · , hℓ) = p(hℓ−1, hℓ)\n ℓ−2\nY\nk=1\np(hk|hk+1)\n!\np(x|h1).\nSince each layer of DBN is constructed as RBM, training each\nlayer of DBN is the same as training a RBM.\nClassiﬁcation is conducted by initializing a network through\nDBN training\n[12], [20]. A two-phase training can be\ndone sequentially by: 1) pre-training, unsupervised learning\nof stacked RBM in a layer-wise manner, and 2) ﬁne-tuning,\nsupervised learning with a classiﬁer. Each phase requires\nsolving an optimization problem. Given training dataset D =\n{(x(1), y(1)), . . . , (x(|D|), y(|D|))} with input x and label y, the\npre-training phase solves the following optimization problem\nat each layer k\nmin\nθk\n1\n|D|\n|D|\nX\ni=1\nh\n−log p(x(i)\nk ; θk)\ni\nwhere θk = (Wk, bk, ck) is the RBM model parameter that\ndenotes weights, visible bias, and hidden bias in the energy\nfunction, and x(i)\nk\nis visible input to layer k corresponding\nto input x(i). Note that in layer-wise updating manner we\nneed to solve ℓof the problems from the bottom to the top\nhidden layer. For the ﬁne-tuning phase we solve the following\noptimization problem\nmin\nφ\n1\n|D|\n|D|\nX\ni=1\nh\nL(φ; y(i), h(x(i)))\ni\n(1)\nwhere L() is a loss function, h denotes the ﬁnal hidden\nfeatures at layer ℓ, and φ denotes the parameters of the\nclassiﬁer. Here for simplicity we write h(x(i)) = h(x(i)\nℓ).\nWhen combining DBN and a feed-forward neural networks\n(FFN) with sigmoid activation, all the weights and hidden bias\nparameters among input and hidden layers are shared for both\ntraining phases. Therefore, in this case we initialize FFN by\ntraining DBN.\nIV. PROPOSED MODELS\nWe model an expected loss function for classiﬁcation.\nConsidering classiﬁcation of two phase method is conducted\non hidden space, the probability distribution of the hidden\nvariables obtained by DBN is used in the proposed models.\nThe two-phase method provides information about modeling\nparameters after each phase is trained. Constraints based on\nthe information are suggested to prevent the model param-\neters from deviating far from good representation of input.\nOptimal solution set for unsupervised objective of the two-\nphase method is good candidate solutions for the second phase.\nBilevel model has the set to ﬁnd optimal solutions for the\nphase-2 objective so that it conducts the two-phase training at\none-shot.\na) DBN Fitting Plus Loss Model: We start with a naive\nmodel of summing pre-trainning and ﬁne-tuning objectives.\nThis model conducts the two-phase training strategy simul-\ntaneously; however, we need to add one more hyperparam-\neter ρ to balance the impact of both objectives. The model\n(DBN+loss) is deﬁned as\nmin\nθL,θDBN Ey,x[L(θL; y, h(x))] + ρ Ex[−log p(x; θDBN)]\nand empirically based on training samples D,\nmin\nθL,θDBN\n1\n|D|\n|D|\nX\ni=1\nh\nL(θL; y(i), h(x(i))) −ρ log p(x(i); θDBN)\ni\n(2)\nwhere θL, θDBN are the underlying parameters. Note that\nθL = φ from (1) and θDBN = (θk)k=1. This model has\nalready been proposed if the classiﬁcation loss function is\nbased on the RBM conditional distribution [1], [3].\nb) Expected Loss Model with DBN Boxing: We ﬁrst de-\nsign an expected loss model based on conditional distribution\np(h|x) obtained by DBN. This model conducts classiﬁcation\non the hidden space. Since it minimizes the expected loss, it\nshould be more robust and thus it should yield better accuracy\non data not observed. The mathematical model that minimizes\nthe expected loss function is deﬁned as\nmin\nθL,θDBN\nEy,h|x[L(θL; y, h(θDBN; x))]\nand empirically based on training samples D,\nmin\nθL,θDBN\n1\n|D|\n|D|\nX\ni=1\n\"X\nh\np(h|x(i))L(θL; y(i), h(θDBN; x(i)))\n#\n.\nWith notation h(θDBN; x(i)) = h(x(i)) we explicitly show\nthe dependency of h on θDBN. We modify the expected\nloss model by introducing a constraint that sets bounds on\nDBN related parameters with respect to their optimal values.\nThis model has two beneﬁts. First, the model keeps a good\nrepresentation of input by constraining parameters ﬁtted in\nthe unsupervised manner. Also, the constraint regularizes the\nmodel parameters by preventing them from blowing up while\nbeing updated. Given training samples D the mathematical\nform of the model (EL-DBN) reads\nmin\nθL,θDBN\n1\n|D|\n|D|\nX\ni=1\n\"X\nh\np(h|x(i))L(θL; y(i), h(θDBN; x(i)))\n#\ns.t.\n|θDBN −θ∗\nDBN| ≤δ\nwhere θ∗\nDBN are the optimal DBN parameters and δ is a\nhyperparameter. This model needs a pre-training phase to\nobtain the DBN ﬁtted parameters.\nc) Expected Loss Model with DBN Classiﬁcation Boxing:\nSimilar to the DBN boxing model, this expected loss model\nhas a constraint that the DBN parameters are bounded by\ntheir optimal values at the end of both phases. This model\nregularizes parameters with those that are ﬁtted in both the\nunsupervised and supervised manner. Therefore, it can achieve\nbetter accuracy even though we need an additional training to\nthe two-phase trainings. Given training samples D the model\n(EL-DBNOPT) reads\nmin\nθL,θDBN\n1\n|D|\n|D|\nX\ni=1\n\"X\nh\np(h|x(i))L(θL; y(i), h(θDBN; x(i)))\n#\ns.t.\n|θDBN −θ∗\nDBN−OP T | ≤δ\n(3)\nwhere θ∗\nDBN−OP T are the optimal values of DBN parameters\nafter two-phase training and δ is a hyperparameter.\nd) Feed-forward Network with DBN Boxing: We also\npropose a model based on boxing constraints where FFN is\nconstrained by DBN output. The mathematical model (FFN-\nDBN) based on training samples D is\nmin\nθL,θDBN\n1\n|D|\n|D|\nX\ni=1\nh\nL(θL; y(i), h(θDBN; x(i)))\ni\ns.t.\n|θDBN −θ∗\nDBN| ≤δ.\n(4)\ne) Feed-forward Network with DBN Classiﬁcation Box-\ning: Given training samples D this model (FFN-DBNOPT),\nwhich is a mixture of (3) and (4), reads\nmin\nθL,θDBN\n1\n|D|\n|D|\nX\ni=1\nh\nL(θL; y(i), h(θDBN; x(i)))\ni\ns.t.\n|θDBN −θ∗\nDBN−OP T | ≤δ.\nf) Bilevel Model: We also apply bilevel programming to\nthe two-phase training method. This model searches optimal\nsolutions to minimize the loss function of the classiﬁer only\nwhere DBN objective solutions are optimal. Possible candi-\ndates for optimal solutions of the ﬁrst level objective function\nare optimal solutions of the second level objective function.\nThis model (BL) reads\nmin\nθL,θ∗\nDBN\nEy,x[L(θL; y, h(θ∗\nDBN; x))]\ns.t.\nθ∗\nDBN = arg min\nθDBN\nEx[−log p(x; θDBN)]\nand empirically based on training samples,\nmin\nθL,θ∗\nDBN\n1\n|D|\n|D|\nX\ni=1\nh\nL(θL; y(i), h(θ∗\nDBN; x(i)))\ni\ns.t.\nθ∗\nDBN = arg min\nθDBN\n1\n|D|\n|D|\nX\ni=1\nh\n−log p(x(i); θDBN)\ni\n.\nOne of the solution approaches to bilevel programming is to\napply Karush–Kuhn–Tucker (KKT) conditions to the lower\nlevel problem. After applying KKT to the lower level, we\nobtain\nmin\nθL,θ∗\nDBN\nEy,x[L(θL; y, h(θ∗\nDBN; x))]\ns.t.\n∇θDBN Ex[−log p(x; θDBN)|θ∗\nDBN ] = 0.\nFurthermore, we transform this constrained problem to an\nunconstrained problem with a quadratic penalty function:\nmin\nθL,θ∗\nDBN\nEy,x[L(θL; y, h(θ∗\nDBN; x))]+\nµ\n2 ||∇θDBN Ex[−log p(x; θDBN)]|θ∗\nDBN ||2\n(5)\nwhere µ is a hyperparameter. The gradient of the objective\nfunction is derived in the appendix.\nV. COMPUTATIONAL STUDY\nTo evaluate the proposed models classiﬁcation tasks on three\ndatasets were conducted: the MNIST hand-written images 1,\nthe KDD’99 network intrusion dataset (NI)2, and the isolated\nletter speech recognition dataset (ISOLET) 3. The experimen-\ntal results of the proposed models on these datasets were\ncompared to those of the two-phase method.\nIn FFN, the sigmoid functions in the hidden layers and\nthe softmax function in the output layer were chosen with\nnegative log-likelihood as a loss function of the classiﬁers.\nWe selected the hyperparameters based on the settings used\nin [21], which were ﬁne-tuned. We ﬁrst implemented the two-\nphase method with DBNs of 1, 2, 3 and 4 hidden layers to\nﬁnd the best conﬁguration for each dataset, and then applied\nthe best conﬁguration to the proposed models.\nImplementations were done in Theano using GeForce GTX\nTITAN X. The mini-batch gradient descent algorithm was used\nto solve the optimization problems of each model. To calculate\nthe gradients of each objective function of the models Theano’s\nbuilt-in functions, ’theano.tensor.grad’, was used. We denote\nby DBN-FFN the two-phase approach.\nA. MNIST\nThe task on the MNIST is to classify ten digits from 0 to\n9 given by 28 × 28 pixel hand-written images. The dataset\nis divided in 60,000 samples for training and validation, and\n10,000 samples for testing. The hyperparameters are set as:\nthere are 1,000 hidden units in each layer; the number of pre-\ntraining epochs per layer is 100 with the learning rate of 0.01;\nthe number of ﬁne-tuning epochs is 300 with the learning rate\nof 0.1; the batch size is 10; and ρ in the DBN+loss and µ\nin the BL model are diminishing during epochs. Note that\nDBN+Loss and BL do not require pre-training.\nDBN-FFN with three-hidden layers of size, 784-1000-1000-\n1000-10, was the best, and subsequently we compared it to the\nproposed models with the same size of the network. We com-\nputed the means of the classiﬁcation errors and their standard\ndeviations for each model averaged over 5 random runs. In\neach table, we stressed in bold the best three models with\n1http://yann.lecun.com/exdb/mnist/\n2http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n3https://archive.ics.uci.edu/ml/datasets/ISOLET\nMean\nSd.\nDBN-FFN\n1.33%\n0.03%\nDBN+loss\n1.84%\n0.14%\nEL-DBN\n1.46%\n0.05%\nEL-DBNOPT\n1.33%\n0.04%\nFFN-DBN\n1.34%\n0.04%\nFFN-DBNOPT\n1.32%\n0.03%\nBL\n1.85%\n0.07%\nTABLE I: Classiﬁcation errors with respect to the best DBN\nstructure for the MNIST.\nties broken by deviation. In Table 1, the best mean test error\nrate was achieved by FFN-DBNOPT, 1.32%. Furthermore, the\nmodels with the DBN classiﬁcation constraints, EL-DBNOPT\nand FFN-DBNOPT, perform similar to, or better than the\ntwo-phase method. This shows that DBN classiﬁcation boxing\nconstraints regularize the model parameters by keeping a good\nrepresentation of input.\nB. Network Intrusion\nThe classiﬁcation task on NI is to distinguish between nor-\nmal and bad connections given the related network connection\ninformation. The preprocessed dataset consists of 41 input\nfeatures and 5 classes, and 4,898,431 examples for training and\n311,029 examples for testing. The experiments were conducted\non 20%, 30%, and 40% subsets of the whole training set,\nwhich were obtained by stratiﬁed random sampling. The\nhyperparameters are set as: there are 15 hidden units in each\nlayer; the number of pre-training epochs per layer is 100 with\nthe learning rate of 0.01; the number of ﬁne-tuning epochs is\n500 with the learning rate of 0.1; the batch size is 1,000; and\nρ in the DBN+loss and µ in the BL model are diminishing\nduring epochs.\nOn NI the best structure of DBN-FFN was 41-15-15-5 for\nall three datasets. Table 2 shows the experimental results of the\nproposed models with the same network as the best DBN-FFN.\nBL performed the best in all datasets, achieving the lowest\nmean classiﬁcation error without the pre-training step. The\ndifference in the classiﬁcation error between our best model,\nBL, and DBN-FFN is statistically signiﬁcant since the p-values\nare 0.03, 0.01, and 0.03 for 20%, 30%, and 40% datasets,\nrespectively. This showed that the model being trained con-\ncurrently for unsupervised and supervised purpose can achieve\nbetter accuracy than the two-phase method. Furthermore, both\nEL-DBNOPT and FFN-DBNOPT yield similar to, or lower\nmean error rates than DBN-FFN in all of the three subsets.\nC. ISOLET\nThe classiﬁcation on ISOLET is to predict which letter-\nname was spoken among the 26 English alphabets given 617\ninput features of the related signal processing information. The\ndataset consists of 5,600 for training, 638 for validation, and\n1,559 examples for testing. The hyperparameters are set as:\nthere are 1,000 hidden units in each layer; the number of pre-\ntraining epochs per layer is 100 with the learning rate of 0.005;\n20% dataset\n30% dataset\nMean\nSd.\nMean\nSd.\nDBN-FFN\n8.14%\n0.12%\n8.18%\n0.12%\nDBN+loss\n8.07%\n0.06%\n8.13%\n0.09%\nEL-DBN\n8.30%\n0.09%\n8.27%\n0.07%\nEL-DBNOPT\n8.14%\n0.14%\n8.15%\n0.15%\nFFN-DBN\n8.17%\n0.09%\n8.20%\n0.08%\nFFN-DBNOPT\n8.07%\n0.12%\n8.12%\n0.11%\nBL\n7.93%\n0.09%\n7.90%\n0.11%\n40% dataset\nMean\nSd.\nDBN-FFN\n8.06%\n0.02%\nDBN+loss\n8.05%\n0.05%\nEL-DBN\n8.29%\n0.14%\nEL-DBNOPT\n8.08%\n0.10%\nFFN-DBN\n8.07%\n0.11%\nFFN-DBNOPT\n7.95%\n0.11%\nBL\n7.89%\n0.10%\nTABLE II: Classiﬁcation errors with respect to the best DBN\nstructure for NI\nMean\nSd.\nDBN-FFN\n3.94%\n0.22%\nDBN+loss\n4.38%\n0.20%\nEL-DBN\n3.91%\n0.18%\nEL-DBNOPT\n3.75%\n0.14%\nFFN-DBN\n3.94%\n0.19%\nFFN-DBNOPT\n3.75%\n0.13%\nBL\n4.43%\n0.18%\nTABLE III: Classiﬁcation errors with respect to the best DBN\nstructure for ISOLET.\nthe number of ﬁne-tuning epochs is 300 with the learning rate\nof 0.1; the batch size is 20; and ρ in the DBN+loss and µ in\nthe BL model are diminishing during epochs.\nIn this experiment the shallow network performed better\nthan the deep network; 617-1000-26 was the best structure for\nDBN-FFN. One possible reason for this is its small size of\ntraining samples. EL models performed great for this instance.\nEL-DBNOPT achieved the best mean classiﬁcation error, tied\nwith FFN-DBNOPT. With the same training effort, EL-DBN\nachieved a lower mean classiﬁcation error and smaller standard\ndeviation than the two-phase method, DBN-FFN. Considering\na relatively small sample size of ISOLET, EL shows that it\nyields better accuracy on unseen data as it minimizes the\nexpected loss, i.e., it generalizes better. In this data set, p-value\nis 0.07 for the difference in the classiﬁcation error between our\nbest model, FFN-DBNOPT, and DBN-FFN.\nVI. CONCLUSIONS\nDBN+loss performs better than two-phase training DBN-\nFFN only in one instance. Aggregating two unsupervised\nand supervised objectives without a speciﬁc treatment is not\neffective. Second, the models with DBN boxing, EL-DBN and\nFFN-DBN, do not perform better than DBN-FFN in almost all\ndatasets. Regularizing the model parameters with unsupervised\nlearning is not so effective in solving a supervised learning\nproblem. Third, the models with DBN classiﬁcation boxing,\nEL-DBNOPT and FFN-DBNOPT, perform better than DBN-\nFFN in almost all of the experiments. FFN-DBNOPT is\nconsistently one of the best three performers in all instances.\nThis shows that classiﬁcation accuracy can be improved by\nregularizing the model parameters with the values trained for\nunsupervised and supervised purpose. One drawback of this\napproach is that one more training phase to the two-phase ap-\nproach is necessary. Last, BL shows that one-step training can\nachieve a better performance than two-phase training. Even\nthough it worked in one instance, improvements to current\nBL can be made such as applying different solution search\nalgorithms, supervised learning regularization techniques, or\ndifferent initialization strategies.\nREFERENCES\n[1] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, “Learning\nalgorithms for the classiﬁcation restricted Boltzmann machine,” Journal\nof Machine Learning Research, vol. 13, pp. 643–669, 2012.\n[2] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\n2006.\n[3] H. Larochelle and Y. Bengio, “Classiﬁcation using discriminative re-\nstricted Boltzmann machines,” in International Conference on Machine\nLearning (ICML) 25, (Helsinki, Finland), pp. 536–543, 2008.\n[4] S. Elfwing, E. Uchibe, and K. Doya, “Expected energy-based restricted\nBoltzmann machine for classiﬁcation,” Neural Networks, vol. 64, pp. 29–\n38, 2015.\n[5] E. P. Xing, R. Yan, and A. G. Hauptmann, “Mining associated text and\nimages with dual-wing Harmoniums,” in Conference on Uncertainty in\nArtiﬁcial Intelligence (UAI), vol. 21, (Edinburgh, Scotland), pp. 633–\n641, 2005.\n[6] M. Norouzi, M. Ranjbar, and G. Mori, “Stacks of convolutional restricted\nBoltzmann machines for shift-invariant feature learning,” in IEEE Com-\nputer Society Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPR), (Miami, FL, USA), pp. 2735–2742, 2009.\n[7] M. A. Salama, H. F. Eid, R. A. Ramadan, A. Darwish, and A. E.\nHassanien, “Hybrid intelligent intrusion detection scheme,” Advances\nin Intelligent and Soft Computing, pp. 293–303, 2011.\n[8] G. E. Dahl, R. P. Adams, and H. Larochelle, “Training restricted\nBoltzmann machines on word observations,” in International Conference\non Machine Learning (ICML) 29, vol. 29, (Edinburgh, Scotland, UK),\npp. 679–686, 2012.\n[9] A. Mccallum, C. Pal, G. Druck, and X. Wang, “Multi-conditional\nlearning : generative / discriminative training for clustering and classiﬁ-\ncation,” in National Conference on Artiﬁcial Intelligence (AAAI), vol. 21,\npp. 433–439, 2006.\n[10] K. Cho, A. Ilin, and T. Raiko, “Improved learning algorithms for\nrestricted Boltzmann machines,” in Artiﬁcial Neural Networks and\nMachine Learning (ICANN), vol. 6791, Springer, Berlin, Heidelberg,\n2011.\n[11] P. V. Gehler, A. D. Holub, and MaxWelling, “The rate adapting Poisson\n(RAP) model for information retrieval and object recognition,” in\nInternational Conference on Machine Learning (ICML) 23, vol. 23,\n(Pittsburgh, PA, USA), pp. 337–344, 2006.\n[12] Y. Bengio and P. Lamblin, “Greedy layer-wise training of deep net-\nworks,” in Advances in Neural Information Processing Systems (NIPS)\n19, vol. 20, pp. 153–160, MIT Press, 2007.\n[13] R. Sarikaya, G. E. Hinton, and A. Deoras, “Application of deep belief\nnetworks for natural language understanding,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 778–\n784, 2014.\n[14] N. Lu, T. Li, X. Ren, and H. Miao, “A deep learning scheme for motor\nimagery classiﬁcation based on restricted Boltzmann machines,” IEEE\nTransactions on Neural Systems and Rehabilitation Engineering, vol. 25,\npp. 566–576, 2017.\n[15] L. Yu, X. Shi, and T. Shengwei, “Classiﬁcation of Cytochrome P450\n1A2 of inhibitors and noninhibitors based on deep belief network,”\nInternational Journal of Computational Intelligence and Applications,\nvol. 16, p. 1750002, 2017.\n[16] Y. Li, X. Nie, and R. Huang, “Web spam classiﬁcation method based on\ndeep belief networks,” Expert Systems With Applications, vol. 96, no. 1,\npp. 261–270, 2018.\n[17] T. Schmah, G. E. Hinton, R. S. Zemel, S. L. Small, and S. Strother,\n“Generative versus discriminative training of RBMs for classiﬁcation of\nfMRI images,” in Advances in Neural Information Processing Systems\n(NIPS) 21, vol. 21, pp. 1409–1416, Curran Associates, Inc., 2009.\n[18] Y. Bengio, “Learning deep architectures for AI,” Foundations and Trends\nin Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.\n[19] G. E. Hinton, “Training products of experts by minimizing contrastive\ndivergence.,” Neural computation, vol. 14, no. 8, pp. 1771–1800, 2002.\n[20] G. E. Hinton, S. Osindero, and Y. W. Teh, “A fast learning algorithm\nfor deep belief nets.,” Neural computation, vol. 18, no. 7, pp. 1527–54,\n2006.\n[21] B. Wang and D. Klabjan, “Regularization for unsupervised deep neural\nnets,” in National Conference on Artiﬁcial Intelligence (AAAI), vol. 31,\npp. 2681–2687, 2017.\n[22] A. Fischer and C. Igel, “An introduction to restricted Boltzmann\nmachines,” Progress in Pattern Recognition, Image Analysis, Computer\nVision, and Applications, vol. 7441, pp. 14–36, 2012.\nAPPENDIX\nDBN deﬁnes the joint distribution of the visible unit x and\nthe ℓhidden layers, h1, h2, · · · , hℓas\np(x, h1, · · · , hℓ) = p(hℓ−1, hℓ)\n ℓ−2\nY\nk=0\np(hk|hk+1)\n!\nwith h0 = x.\na) DBN Fitting Plus Loss Model: From Eq. (2), p(x) in\nthe second term of the objective function is approximated as\np(x; θDBN) =\nX\nh1,h2,··· ,hℓ\np(x, h1, · · · , hℓ) ≈\nX\nh1\np(x, h1).\nb) Expected Loss Models: p(h|x) in the objective func-\ntion is approximated as\np(hℓ|x) ≈p(hℓ|x, h1, · · · , hℓ)\n=\np(hℓ, hℓ−1, · · · , h1, x)\np(hℓ−1, hℓ−2, · · · , h1, x)\n=\np(hℓ−1, hℓ)\n\u0010Qℓ−2\nk=0 p(hk|hk+1)\n\u0011\np(hℓ−2, hℓ−1)\n\u0010Qℓ−3\nk=0 p(hk|hk+1)\n\u0011\n= p(hℓ−1, hℓ)p(hℓ−2|hℓ−1)\np(hℓ−2, hℓ−1)\n= p(hℓ−1, hℓ)p(hℓ−2, hℓ−1)\np(hℓ−2, hℓ−1)p(hℓ−1)\n= p(hℓ|hℓ−1).\nc) Bilevel Model: From Eq. (5), ∇θDBNlog p(x) in the\nobjective function is approximated for i = 0, 1, · · · , ℓas\n[∇θDBNlog p(x)]i = ∂log p(x)\n∂θi\nDBN\n=\n∂log\n\u0010P\nh1,h2,··· ,hℓp(x, h1, h2, · · · , hℓ)\n\u0011\n∂θi\nDBN\n≈∂log (P\nhi+1 p(hi, hi+1))\n∂θi\nDBN\n(6)\nwhere θDBN = (θ0\nDBN, θ2\nDBN, · · · , θi\nDBN, · · · θℓ\nDBN). The\ngradient of this approximated quantity is then the Hessian\nmatrix of the underlying RBM.\nWe write the approximated ||∇θDBN −log p(x)||2 at the\nlayer i as\n||[∇θDBN −log p(x)]i||2 ≈||∂−log (P\nhi+1 p(hi, hi+1))\n∂θi\nDBN\n||2\n=\n\" \u0012∂−log p(hi)\n∂θi\n11\n\u00132\n+\n\u0012∂−log p(hi)\n∂θi\n12\n\u00132\n+\n· · · +\n\u0012∂−log p(hi)\n∂θinm\n\u00132 #\nwhere m and n denote dimensions of hi and hi+1 and θi\npq\ndenotes the pth and qth component of the θi\nDBN. The gradient\nof the approximated ||∇θDBN −log p(x)||2 at the layer i is\n∂\nθipq\n X\np,q\n\u0012∂−log p(hi)\n∂θipq\n\u00132!\n= 2\n\" \u0012∂−log p(hi)\n∂θi\n11\n\u0013 \u0012∂2 −log p(hi)\n∂θi\n11θipq\n\u0013\n+\n\u0012∂−log p(hi)\n∂θi\n12\n\u0013 \u0012∂2 −log p(hi)\n∂θi\n12∂θipq\n\u0013\n+\n· · · +\n\u0012∂−log p(hi)\n∂θipq\n\u0013 \u0012∂2 −log p(hi)\n∂θipq∂θipq\n\u0013\n+\n· · · +\n\u0012∂−log p(hi)\n∂θinm\n\u0013 \u0012∂2 −log p(hi)\n∂θinmθipq\n\u0013 #\nfor p = 1, ...n, q = 1, ...m. This shows that the gradient of\nthe approximated ||∇θDBN −log p(x)||2 in (5) is then the\nHessian matrix times the gradient of the underlying RBM. The\nstochastic gradient of −log p(x) of RBM with binary input x\nand hidden unit h with respect to θDBNwpq is\n∂RBM\n∂wpq\n= p(hp = 1|x)xq −\nX\nx\np(x)p(hp = 1|x)xq\nwhere RBM denotes −log p(x) [22]. We derive the Hessian\nmatrix with respect to wpq as\n∂2RBM\n∂w2pq\n=\n∂\nwpq\n[p(hp = 1|x)xq)] −\nX\nx\n∂\nwpq\n[p(x)p(hp = 1|x)xq)]\n= σ( g\nnetp)(1 −σ( g\nnetp))x2\nq −\nX\nx\n[∂p(x)\n∂wpq\np(hp = 1|x)xq+\np(x)σ( g\nnetp)(1 −σ( g\nnetp))x2\nq],\n∂2RBM\n∂wpk∂wpq\n=\n∂\nwpk\n[p(hp = 1|x)xq)] −\n∂\nwpk\n[\nX\nx\np(x)p(hp = 1|x)xq)]\n= σ( g\nnetp)(1 −σ( g\nnetp))xqxk −\nX\nx\n[∂p(x)\n∂wpk\np(hp = 1|x)xq+\np(x)σ( g\nnetp)(1 −σ( g\nnetp))xqxk],\n∂2RBM\n∂wkq∂wpq\n=\n∂\nwkq\n[p(hp = 1|x)xq)] −\n∂\nwkq\n[\nX\nx\np(x)p(hp = 1|x)xq]\n= −\nX\nx\n[∂p(x)\n∂wkq\np(hp = 1|x)xq + p(x)\n∂\n∂wkq\n[p(hp = 1|x)xq]],\n∂2RBM\n∂wkp∂wpq\n= −\nX\nx\n[∂p(x)\n∂wkp\np(hp = 1|x)xq + p(x)]\nwhere σ() is the sigmoid function, g\nnetp is P\nq wpqxq+cp, and\ncp is the hidden bias. Based on what we derive above we can\ncalculate the gradient of approximated ||[∇θDBN −logp(x)]i||2.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-04-25",
  "updated": "2019-08-12"
}