{
  "id": "http://arxiv.org/abs/1804.10765v1",
  "title": "Specifying and Verbalising Answer Set Programs in Controlled Natural Language",
  "authors": [
    "Rolf Schwitter"
  ],
  "abstract": "We show how a bi-directional grammar can be used to specify and verbalise\nanswer set programs in controlled natural language. We start from a program\nspecification in controlled natural language and translate this specification\nautomatically into an executable answer set program. The resulting answer set\nprogram can be modified following certain naming conventions and the revised\nversion of the program can then be verbalised in the same subset of natural\nlanguage that was used as specification language. The bi-directional grammar is\nparametrised for processing and generation, deals with referring expressions,\nand exploits symmetries in the data structure of the grammar rules whenever\nthese grammar rules need to be duplicated. We demonstrate that verbalisation\nrequires sentence planning in order to aggregate similar structures with the\naim to improve the readability of the generated specification. Without\nmodifications, the generated specification is always semantically equivalent to\nthe original one; our bi-directional grammar is the first one that allows for\nsemantic round-tripping in the context of controlled natural language\nprocessing. This paper is under consideration for acceptance in TPLP.",
  "text": "arXiv:1804.10765v1  [cs.AI]  28 Apr 2018\nUnder consideration for publication in Theory and Practice of Logic Programming\n1\nSpecifying and Verbalising Answer Set Programs in\nControlled Natural Language\nROLF SCHWITTER\nDepartment of Computing, Macquarie University, Sydney, NSW 2109, Australia\n(e-mail: Rolf.Schwitter@mq.edu.au)\nsubmitted 12 February 2018; revised 5 April 2018; accepted Date\nAbstract\nWe show how a bi-directional grammar can be used to specify and verbalise answer set programs in con-\ntrolled natural language. We start from a program speciﬁcation in controlled natural language and translate\nthis speciﬁcation automatically into an executable answer set program. The resulting answer set program\ncan be modiﬁed following certain naming conventions and the revised version of the program can then be\nverbalised in the same subset of natural language that was used as speciﬁcation language. The bi-directional\ngrammar is parametrised for processing and generation, deals with referring expressions, and exploits sym-\nmetries in the data structure of the grammar rules whenever these grammar rules need to be duplicated. We\ndemonstrate that verbalisation requires sentence planning in order to aggregate similar structures with the\naim to improve the readability of the generated speciﬁcation. Without modiﬁcations, the generated speciﬁ-\ncation is always semantically equivalent to the original one; our bi-directional grammar is the ﬁrst one that\nallows for semantic round-tripping in the context of controlled natural language processing. This paper is\nunder consideration for acceptance in TPLP.\nKEYWORDS: Bi-directional grammars, controlled natural languages, answer set programming, sentence\nplanning, executable speciﬁcations\n1 Introduction\nThere exist a number of controlled natural languages (Sowa 2004, Clark et al. 2005, Fuchs et\nal. 2008, Guy and Schwitter 2017) that have been designed as high-level interface languages\nto knowledge systems. However, none of the underlying grammars of these controlled natural\nlanguages is bi-directional in the sense that a speciﬁcation can be written in controlled natural\nlanguage, translated into a formal target representation, and then – after potential modiﬁcations\nof that target representation – back again into the same subset of natural language. This form\nof round-tripping under modiﬁcation is difﬁcult to achieve, in particular with a single grammar\nthat can be used for processing as well as for generation. In this paper, we show how such a\nbi-directional grammar can be built in form of a logic program for a controlled natural language.\nWe discuss the implementation of a deﬁnite clause grammar that translates a controlled natural\nlanguage speciﬁcation into an executable answer set program. The resulting answer set program\ncan be modiﬁed and then verbalised in controlled natural language using the same grammar\nexploiting symmetries in the feature structures of the grammar rules. Without modiﬁcation of\nthe original answer set program, this leads to a form of round-tripping that preserves semantic\nequivalence but not syntactic equivalence. This work has certain similarities with our previous\n2\nRolf Schwitter\nwork (Schwitter 2008) where a bi-directional grammar is used to translate an experimental con-\ntrolled natural language into syntactically annotated TPTP1 formulas; these annotated formulas\nare then stored and serve as templates to answer questions in controlled natural language. The\nnovelty of the presented work is that no such annotated formulas are required; instead a sen-\ntence planner is used that applies different generation strategies. The deﬁnite clause grammar\nuses an internal format for the representation of clauses for the answer set program. This inter-\nnal format is designed in such a way that it can be used for processing and generation purposes\nand it respects at the same time accessibility constraints of antecedents for referring expressions.\nThe grammar of the controlled natural language for answer set programs is more restricted than\na corresponding grammar for ﬁrst-order formulas, since some linguistic constructions can only\noccur at certain positions in particular sentences. However, in contrast to a controlled natural lan-\nguage that is equivalent to ﬁrst-order logic, a controlled language for answer set programs allows\nus to distinguish, for example, between strong and weak negation using two separate linguistic\nconstructions for these forms of negation.\nThe rest of this paper is structured as follows: In Section 2, we recall linguistic preliminaries\nand sketch the controlled natural language to be used. In Section 3, we brieﬂy review answer set\nprogramming and show how it has been employed for natural language processing, with a par-\nticular focus on controlled natural language processing. In Section 4, we present a speciﬁcation\nwritten in controlled natural language, discuss the resulting answer set program and its verbali-\nsation. In Section 5, we present the bi-directional grammar in detail and explain how symmetries\nin the feature structures of the grammar rules can be exploited to process and generate sentences.\nIn Section 6, we introduce the sentence planner and a more complex speciﬁcation, and discuss\nthe planning strategies that are applied to the internal format for answer set programs before the\nbi-directional grammar is used to verbalise an answer set program. In Section 7, we summarise\nthe key points of our approach, highlight its beneﬁts, and conclude.\n2 Linguistic Preliminaries\nThe controlled natural language that we discuss here is based on the one used in the PENGASP\nsystem (Guy and Schwitter 2017). This system translates speciﬁcations written in controlled lan-\nguage into answer set programs but not back again. This is because the grammar of the language\nPENGASP has not been designed with bi-directionality in mind. The language PENGASP distin-\nguishes between simple and complex sentences. We focus in the following introduction mainly\non those linguistic constructions that occur also in the subsequent example speciﬁcations.\n2.1 Simple Sentences\nSimple sentences in our controlled natural language consist of a subject and a predicate, and\ninclude zero or more complements depending on the verbal predicate. For example, the following\nare simple sentences:\n1. Bob is a student.\n2. Sue Miller works.\n3. Tom studies at Macquarie University.\n1 http://www.cs.miami.edu/˜tptp/\nSpecifying and Verbalising Answer Set Programs in Controlled Natural Language\n3\n4. The Biology book is expensive.\n5. There is a student.\nSentence (1) consists of a proper name (Bob) in subject position, followed by a linking verb\n(is), and a subject complement (a student). Sentence (2) contains an intransitive verb (works) but\nno complement. Sentence (3) contains a transitive prepositional verb (studies at) with a comple-\nment in form of a proper name (Macquarie University) in object position. Sentence (4) consists\nof a deﬁnite noun phrase (The Biology book) in subject position followed by a linking verb and an\nadjective (expensive) as subject complement. Sentence (5) shows that it is possible to introduce\nnew entities with an expletive there-construction where there acts as ﬁller for the subject.\n2.2 Complex Sentences\nComplex sentences are built from simpler sentences through coordination, subordination, quan-\ntiﬁcation, and negation:\n6. Tom studies at Macquarie University and is enrolled in Information Technology.\n7. Bob who is enrolled in Linguistics studies or parties.\n8. Tom is enrolled in COMP329, COMP330 and COMP332.\n9. Every student who works is successful.\n10. It is not the case that a student who is enrolled in Information Technology parties.\n11. If a student does not provably work then the student does not work.\nIn sentence (6), two verb phrases are coordinated with the help of a conjunction (and). In sen-\ntence (7), a subordinate relative clause (who is enrolled in Linguistics) is used in subject position\nand two verb phrases are coordinated with the help of a disjunction (or). Sentence (8) contains\na coordination in form of an enumeration (COMP329, COMP330 and COMP332) in object po-\nsition. Sentence (9) uses a universally quantiﬁed noun phrase (Every student) that contains a\nrelative clause (who works) in subject positions. In (10), the entire complex sentence is negated\nwith the help of the predeﬁned construction It is not the case that; and ﬁnally in (11), weak\nnegation (does not provably work) is used in the condition of a conditional sentence and strong\nnegation (does not work) in the consequent of that sentence.\n3 Answer Set Programming and Natural Language Processing\nAnswer set programming is a modern approach to declarative programming and has its roots in\nlogic programming, logic-based knowledge representation and reasoning, deductive databases,\nand constraint solving (Gelfond and Lifschitz 1988, Baral 2003, Lifschitz 2008, Gebser et al.\n2017). The main idea behind answer set programming is to represent a computational problem as\na (non-monotonic)logic program whose resulting models (= answer sets) correspond to solutions\nof the problem. Answer set programs consist of clauses that look similar to clauses in Prolog;\nhowever, in answer set programming an entirely different computational mechanism is used that\ncomputes answer sets in a bottom-up fashion. An answer set program consists of a set of clauses\n(rules) of the form:\nL1 ; ... ; Lk :- Lk+1, ..., Lm, not Lm+1, ..., not Ln.\nwhere all Li are literals or strongly negated literals. The connective “:-” stands for if and separates\nthe head from the body of the rule. The connective “;” in the head of the rule stands for an\n4\nRolf Schwitter\nepistemic disjunction (at least one literal is believed to be true). The connective “,” in the body\nof the rule stands for and and the connective “not” for negation as failure (also called weak\nnegation). A rule with an empty body and without an if-connective is called a fact, and a rule\nwithout a head is called a constraint. Answer set programming is supported by powerful tools; for\nexample, the clingo system (Gebser et al. 2017) combines a grounder and a solver. The grounder\nconverts an answer set program that contains variables into an equivalent ground (variable-free)\npropositional program and the solver computes the answer sets of the propositional program.\nAnswers to questions can then be extracted directly from the resulting answer sets.\nAnswer set programming is an interesting formalism for knowledge representation in the con-\ntext of natural language processing because of its ability to combine strong and weak nega-\ntion which offers support for non-monotonic reasoning. Answer set programming has been used\nfor syntactic parsing in the context of the Combinatory Categorial Grammar formalism (Lieler\nand Sch¨uller 2012, Sch¨uller 2013), for semantic parsing and representation of textual informa-\ntion (Baral et al. 2011, Nguyen et al. 2015), and for natural language understanding in the con-\ntext of question answering (Baral and Tari 2006) and the Winograd Schema Challenge (Sch¨uller\n2014, Bailey et al. 2015). In the context of controlled natural language processing, answer set\nprogramming has been used as a prototype of a rule system in the Attempto project (Kuhn\n2007, Fuchs et al. 2008), as a target language for biomedical queries related to drug discov-\nery (Erdem and Yeniterzi 2009), as a source language for generating explanations for biomedical\nqueries (Erdem and ¨Oztok 2015), as a framework for human-robot interaction (Demirel et al.\n2016), and as a target language for writing executable speciﬁcations (Guy and Schwitter 2017).\n4 From Controlled Natural Language to Answer Set Programs and Back Again\nBefore we discuss the implementation of our bi-directional grammar for answer set programs in\nSection 5, we ﬁrst illustrate how our controlled natural language can be used for specifying and\nverbalising answer set programs, and thereby motivate its usefulness.\nEvery student who works is successful.\n%\nC1\nEvery student who studies at Macquarie University works or parties.\n%\nC2\nIt is not the case that a student who is enrolled in Information\n%\nC3\nTechnology parties.\nTom is a student.\n%\nC4\nTom studies at Macquarie University and is enrolled in Information\n%\nC5\nTechnology.\nBob is a student.\n%\nC6\nBob studies at Macquarie University and does not work.\n%\nC7\nWho is successful?\n%\nC8\nFig. 1: Successful Student in Controlled Natural Language\nFigure 1 shows a speciﬁcation written in controlled natural language. This speciﬁcation con-\nsists of two universally quantiﬁed sentences (C1 and C2), a constraint (C3), four declarative sen-\ntences (C4-C7), followed by a wh-question (C8). Note that the two universally quantiﬁed sentences\ncontain an embedded relative clause in subject position; the second one of these universally quan-\ntiﬁed sentences (C2) contains a disjunctive verb phrase. The two declarative sentences (C5 and C7)\nSpecifying and Verbalising Answer Set Programs in Controlled Natural Language\n5\nare complex and contain coordinated verb phrases. Note also that the proper name Macquarie Uni-\nversity in C5 and C7 is used anaphorically and links back to the proper name Macquarie University\nintroduced in C2. This speciﬁcation is automatically translated into the answer set program dis-\nplayed in Figure 2.\nsuccessful(A) :- student(A), work(A).\n%\nA1\nwork(B) ; party(B) :- student(B), study_at(B,macquarie_university).\n%\nA2\n:- student(C), enrolled_in(C,information_technology), party(C).\n%\nA3\nstudent(tom).\n%\nA4\nstudy_at(tom,macquarie_university).\n%\nA5\nenrolled_in(tom,information_technology).\n%\nA6\nstudent(bob).\n%\nA7\nstudy_at(bob,macquarie_university).\n%\nA8\n-work(bob).\n%\nA9\nanswer(D) :- successful(D).\n% A10\nFig. 2: Successful Student as Answer Set Program\nNote that the translation of sentence C1 results in a normal clause (A1) with a single literal\nin its head. The translation of sentence C2 leads to a clause with a disjunction in its head (A2),\nand the translation of sentence C3 to a constraint (A3). The translation of the two declarative\nsentences C4 and C6 results in a single fact (A4+A7) for both sentences, and the translation of the\ntwo coordinated sentences C5 and C7 leads to two facts each (A5+A6 and A8+A9). In the case of C7\none of these facts is strongly negated (A9). Finally, the translation of the wh-question C8 results\nin a rule with a speciﬁc answer literal (answer(D)) in its head (A10). The user can now modify\nthis answer set program as illustrated in Figure 3 – as long as they stick to the existing naming\nconvention.\nstressed(A) :- student(A), work(A).\n%\nA1'\nwork(B) ; party(B) :- student(B), study_at(B,macquarie_university).\n%\nA2'\n:- student(C), enrolled_in(C,information_technology), party(C).\n%\nA3'\nstudent(tom).\n%\nA4'\nstudy_at(tom,macquarie_university).\n%\nA5'\nenrolled_in(tom,information_technology).\n%\nA6'\nstudent(bob).\n%\nA7'\nstudy_at(bob,macquarie_university).\n%\nA8'\nenrolled_in(bob,linguistics).\n%\nA9'\nparty(bob).\n% A10'\nanswer(D) :- stressed(D).\n% A11'\nFig. 3: Modiﬁed Answer Set Program\nThis modiﬁed answer set program contains the following changes: the literal successful(A)\nin the head of the clause A1 has been replaced by the literal stressed(A) on line A1'; the new fact\nenrolled in(bob,linguistics) has been added to the program on line A9'; the negated literal\n-work(bob) on line A9 has been replaced by the positive literal party(bob) on line A10'; and the\n6\nRolf Schwitter\nliteral successful(D) in the body of the clause that answers the question on line A10 has been\nreplaced by the literal stressed(D) on line A11'. This modiﬁed answer set program can now be\nautomatically verbalised in controlled natural language as illustrated in Figure 4.\nEvery student who works is stressed.\n%\nC1'\nEvery student who studies at Macquarie University works or parties.\n%\nC2'\nIt is not the case that a student who is enrolled in Information\n%\nC3'\nTechnology parties.\nTom is a student.\n%\nC4'\nTom studies at Macquarie University and is enrolled in Information\n%\nC5'\nTechnology.\nBob is a student.\n%\nC6'\nBob studies at Macquarie University and is enrolled in Linguistics.\n%\nC7'\nBob parties.\n%\nC8'\nWho is stressed?\n%\nC9'\nFig. 4: Verbalisation of the Modiﬁed Answer Set Program in Controlled Natural Language\nWe observe that the clauses on line A5' and A6' and similarly those on line A8' and A9' of the\nmodiﬁed answer set program have been used to generate a coordinated verb phrase as shown on\nline C5' and C7' of Figure 4. This is a form of aggregation that has been executed by the sentence\nplanner and combines similar structures that fall under the same subject based on the proximity\nof the clauses. Alternatively, the sentence planner could use another aggregation strategy and\ncombine, for example, the clauses on line A4' and A5' or those on line A4' and A5' and A6' into\na verb phrase (more on this in Section 6).\n5 Implementation of the Bi-directional Grammar\nTo implement our bi-directional grammar, we use the deﬁnite clause grammar (DCG) formal-\nism (Pereira and Warren 1980, Pereira and Shieber 1987). This formalism gives us a conve-\nnient notation to express feature structures and the resulting grammar can easily be transformed\ninto another notation via term expansion, if an alternative parsing strategy is required (Guy and\nSchwitter 2017). Our bi-directional grammar implements a syntactic-semantic interface where\nthe meaning of a textual speciﬁcation and its linguistic constituents is established in a composi-\ntional way during the translation of the speciﬁcation into an answer set program. Uniﬁcation is\nused to map the syntactic constituents of the controlled language on terms and (partial) clauses\nof the corresponding answer set program. In our context, a textual speciﬁcation A and its verbal-\nisation A′ can be shown to be semantically equivalent, if they both generate the same answer set\nprogram that entails the same solutions.\nOur implementation of the grammar relies on four other components that are necessary to\nbuild an operational system: (1) a tokeniser that splits a speciﬁcation written in controlled natural\nlanguage into a list of tokens for each sentence; (2) a writer that translates the internal format\nconstructed during the parsing process into the ﬁnal answer set program; (3) a reader that reads\na (potentially modiﬁed) answer set program and converts this program into the internal format;\nand (4) a sentence planner that tries to apply a number of aggregation strategies, reorganises the\ninternal format for this purpose, before it is used by the grammar for the generation process.\nSpecifying and Verbalising Answer Set Programs in Controlled Natural Language\n7\n5.1 Internal Format for Answer Set Programs\nThe bi-directional deﬁnite clause grammar uses an internal format for answer set programs and\na special feature structure that distinguishes between a processing and a generation mode in the\ngrammar rules. Depending on this mode, the same difference list (Sterling and Shapiro 1994) is\nused as a data structure to either construct the internal format for an input text or to deconstruct\nthe internal format to generate a verbalisation. The internal format is based on a ﬂat notation that\nuses a small number of typed predicates (e.g. class, pred, prop, named) that are associated with\nlinguistic categories (e.g. noun, verb, adjective, proper name) in the lexicon. This ﬂat notation\nallows us to abstract over individual predicate names and makes the processing for the anaphora\nresolution algorithm and sentence planner easier. Let us brieﬂy illustrate the form and structure\nof this internal format with the help of the following two example sentences:\n12. Tom is a student and works.\n13. If a student works then the student is successful.\nIn the case of processing, the tokeniser sends a list of tokens for these two sentences to the\nbi-directional grammar that builds up the following internal format during the parsing process\nwith the help of the information that is available in the lexicon for these tokens:\n[['.', [pred(C,work), class(C,student)], '-:', [prop(C,successful)],\n'.', pred(A,work), class(B,student), pred(A,B,isa), named(A,tom)]]\nAs this example shows, the internal format is built up in reverse order; the literals derived from\nthe ﬁrst sentence (12) follow the literals derived from the second sentence (13). As a consequence\nof this ordering, the arrow operator for the rule (-:) points from the body on the left side of the\nrule to the head on the right side. Note that this internal format retains the full stops in order\nto separate the representation of individual clauses and uses, in our example, two sublists as\npart of the second clause. These sublists constrain the accessibility of antecedents for referring\nexpressions – in a similar way as postulated in discourse representation theory (Kamp and Reyle\n1993, Geurts et al. 2015). This internal format is then automatically translated by the writer into\nthe ﬁnal answer set program. In order to achieve this, the writer reverses the order of the clauses,\nremoves auxiliary literals, and turns the typed representation of literals into an untyped one:\nstudent(tom). work(tom).\nsuccessful(C) :- student(C), work(C).\nIn the case of generation, the clauses of the answer set program are ﬁrst read by the reader that\nreconstructs the ﬂat internal format. Since we use the same data structure (difference list) in the\ngrammar for processing and generation, the internal format for clauses is now built up in normal\norder and not in reverse order as before. This has – among other things – the effect that the arrow\noperator (:-) of the rule now points from the body of the rule on the right side to the head on the\nleft side:\n[[named(A,tom), pred(A,B,isa), class(B,student), pred(B,work), '.',\n[prop(C,successful)], ':-', [class(C,student), pred(C,work)], '.']]\nThis reconstructed internal format is then sent to the sentence planner that tries to apply a\nnumber of strategies to aggregate the information in order to generate, for example, coordinated\nsentences or to take care of enumerations in sentences. As we will see in Section 6, sentence\nplanning requires reorganising the internal format to achieve the intended result.\n8\nRolf Schwitter\n5.2 Overview of Grammar Rules\nIn this section we introduce the format of the bi-directional deﬁnite clause grammar rules. Since\nour grammar is a grammar for a controlled natural natural, speciﬁc restrictions apply to the\ngrammar rules. We distinguish between grammar rules for declarative sentences, conditional\nsentences, constraints, and questions. This distinction is important since only certain syntactic\nconstructions can occur at certain positions in these sentences. For example, a noun phrase in\nsubject position of a declarative sentence can only be coordinated if this noun phrase is followed\nby a linking verb or a noun phrase that occurs in the subject position in the consequent of a\nconditional sentence can only have the form of a deﬁnite noun phrase. On the interface level,\nthese syntactic constraints are enforced by a lookahead text editor that guides the writing process\nof the user and uses the grammar as starting point to extract the lookahead information (for details\nabout this interface technology see (Guy and Schwitter 2017)).\nWhen a speciﬁcation is processed, the text is split up by the tokeniser into a list of lists where\neach list contains those tokens that represent a sentence. At the beginning the difference list\nthat constructs the internal representation for an answer set program has the following form:\n[[]]-C, where the incoming part consists of a list that contains an empty list and the outgoing\npart consists of a variable; the same applies to the difference list that collects all accessible\nantecedents during the parsing process. When an answer set program is verbalised, the incoming\npart of the difference list consists of an embedded list of elements that stand for the internal\nformat of the answer set program; of course, the lists for the tokens and accessible antecedents\nare empty at this point. It is important to note that these difference lists are processed recursively\nfor each sentence or for each clause of an answer set program and that they establish a context\nfor the actual interpretation.\nThe following is a grammar rule in deﬁnite clause grammar format; it takes as input either a\nlist of tokens of a declarative sentence and updates the answer set program under construction or\nit takes as input a (partial) answer set program and produces a list of tokens that correspond to a\ndeclarative sentence:\ns(mode:M, ct:fact, cl:C1-C4, ante:A1-A3) -->\nnp(mode:M, ct:fact, crd:'-', fcn:subj, num:N, arg:X, cl:C1-C2, ante:A1-A2),\nvp(mode:M, ct:fact, crd:'+', num:N, arg:X, cl:C2-C3, ante:A2-A3),\nfs(mode:M, cl:C3-C4).\nThis grammar rule states that a declarative sentence (s) consists of a noun phrase (np) and a\nverb phrase (vp), followed by a full stop (fs). This grammar rule contains additional arguments\nthat implement feature structures in the form of attribute:value pairs that can be directly pro-\ncessed via uniﬁcation. The feature structure mode:M distinguishes between the processing mode\n(proc) and generation mode (gen). The feature structure ct:fact stands for the clause type and\nspeciﬁes that the noun phrase as well as the verb phrase of this rule are used to generate or\nprocess facts. The feature structures cl:C1-C4, cl:C1-C2, cl:C2-C3, and cl:C3-C4 implement a\ndifference list and are used to construct the internal format for an answer set program or to decon-\nstruct the internal format of an answer set program to drive the generation of a verbalisation. The\nfeature structures ante:A1-A3, ante:A1-A2, and ante:A2-A3 implement a second difference list\nthat stores all accessible antecedents. These accessible literals are continuously updated by the\nanaphora resolution algorithm during the parsing process. The feature structure crd:'-' speci-\nﬁes that the noun phrase of this rule cannot be coordinated and the feature structure crd:'+' that\nthe verb phrase of this rule can be coordinated. The feature structure num:N deals with number\nSpecifying and Verbalising Answer Set Programs in Controlled Natural Language\n9\nagreement between noun phrase and verb phrase, while the feature structure arg:X makes the\nvariable for the entity denoted by the noun phrase in subject position (fcn:subj) available for the\nliteral described by the verb phrase. Our implementation uses additional feature structures that\nbuild up a syntax tree and a paraphrase during the parsing process but these feature structures are\nnot shown in our example to keep the presentation clear and compact.\nThe following grammar rule is a preterminal rule that is used in the processing mode (proc)\nand adds the literal named(X,Name) for a proper name to the outgoing part of the difference list\nthat constructs the internal format and to the difference list that stores all accessible antecedents:\npname(mode:proc, fcn:F, num:N, arg:X,\ncl:[C1|C2]-[[named(X,Name)|C1]|C2],\nante:[A1|A2]-[[named(X,Name)|A1]|A2]) -->\n{ lexicon(cat:pname, mode:proc, wform:WForm, fcn:F, num:N, arg:X,\nlit:named(X,Name)) },\nWForm.\nNote that the outgoing list contains all information of the incoming list plus the added literal.\nAt this point, this addition is only preliminary and the anaphora resolution algorithm will decide\nif this literal is new or not. The grammar rule that is used in the generation mode (gen) for proper\nnames looks very similar to the grammar rule in the processing mode, but now a literal for the\nproper name is removed from the incoming list of clauses instead of added to the outgoing list:\npname(mode:gen, fcn:F, num:N, arg:X,\ncl:[[named(X,Name)|C1]|C2]-[C1|C2],\nante:[A1|A2]-[[named(X,Name)|A1]|A2]) -->\n{ lexicon(cat:pname, mode:gen, wform:WForm, fcn:F, num:N, arg:X,\nlit:named(X,Name)) },\nWForm.\nThe grammar rule below translates conditional sentences into the internal format for rules and\nthe internal format for rules into conditional sentences:\ns(mode:M, cl:C1-C3, ante:A1-A3) -->\nprep(mode:M, wform:['If'], head:H1-H2, body:B1-B2, cl:C1-C2),\ns(mode:M, loc:body, cl:B1-B2, ante:A1-A2),\nadv(mode:M, wform:[then]),\ns(mode:M, loc:head, cl:H1-H2, ante:A2-A3),\nfs(mode:M, cl:C2-C3).\nThe feature structures cl:C1-C3, cl:C1-C2, and cl:C2-C3 implement again a difference list\nand are used to construct or deconstruct the internal format of a rule. The two feature struc-\ntures cl:H1-H2 and cl:B1-B2 implement another difference list that provides the relevant data\nstructures for the head and body of the rule. The feature structures ante:A1-A3, ante:A1-A2, and\nante:A2-A3 are used as before to store those antecedents that are currently accessible. The over-\nall structure for the rule as well as the structure of the head and the body for the rule become\navailable via the following preterminal grammar rules:\nprep(mode:proc, wform:WForm,\nhead:[[]|C2]-[Head, Body, C3|C4],\nbody:[[]|C1]-C2,\ncl:C1-[[Body, '-:', Head|C3]|C4]) -->\n{ lexicon(cat:prep, wform:WForm) },\nWForm.\n10\nRolf Schwitter\nprep(mode:gen, wform:WForm,\nhead:C2-[[]|C1],\nbody:[Body, Head, C3|C4]-[[]|C2],\ncl:[[Head, ':-', Body|C3]|C4]-C1) -->\n{ lexicon(cat:prep, wform:WForm) },\nWForm.\nThe ﬁrst rule is used for processing and the second one for generation. In the case of processing\na conditional sentence, ﬁrst the body of a rule is constructed and then the head. Constructing the\nbody of a rule starts with an empty list ([]) and takes the already processed context in list C1\ninto consideration for anaphora resolution. During the parsing process literals are added to the\nempty list, and the resulting outgoing list (C2) then builds the new context for processing the\nhead of the rule. The processing of the head of the rule starts again with an empty list and\nresults in an outgoing list that consists of a list (Head) that contains the head literals and a list\n(Body) that contains the body literals. Note that the variable C3 stands for a list that contains the\npreviously processed context. These elements are then combined in a rule in the outgoing part\nof the difference list ([[Body, '-:', Head|C3]|C4]) that build up the clauses for the answer set\nprogram. It is interesting to see that we end up in the case of generation with feature structures\nthat are symmetric to processing. In the case of generation, a rule is deconstructed into a body\n(Body) and a head (Head), and then the body is ﬁrst processed followed by the head. Note that the\nsuccessful processing of the body results in an empty list ([]) in the outgoing difference list, the\nsame happens after the processing of the head. We observe that all grammar rules for preterminal\nsymbols are symmetric, including grammar rules that process cardinality quantiﬁers (cqnt) such\nas exactly Number Noun, at least Number Noun, and at most Number Noun:\ncqnt(mode:proc, wform:WForm, num:N,\nlit:[[]|C]-[Lit|C],\ncond:[[]|C]-[Cs|C],\ncl:[[]|C]-[H|C]) -->\n{ lexicon(cat:cqnt, mode:proc, wform:WForm, num:N, lit:Lit, cond:Cs, cl:H) },\nWForm.\ncqnt(mode:gen, wform:WForm, num:N,\nlit:[Lit|C]-[[]|C],\ncond:[Cs|C]-[[]|C],\ncl:[H|C]-[[]|C]) -->\n{ lexicon(cat:cqnt, mode:gen, wform:WForm, num:N, lit:Lit, cond:Cs, cl:H) },\nWForm.\nThe two rules above take part in the processing and generation of clauses with a cardinality\nconstraint in the head: Lower1 { Lit:Ds } Upper2 :- BODY. Here the expression Lit:Ds is a\nconditional literal consisting of a basic literal (Lit) and one or more domain predicates (Ds).\n5.3 Anaphora Resolution\nProper names and deﬁnite noun phrases can be used as referring expressions in the controlled\nnatural language PENGASP. In our case the list structure of the internal representation constrains\nthe accessibility of antecedents for referring expressions. In particular, a referring expression can\nonly refer to an expression in the current list or to an expression in a list superordinate to that\nlist. A list that encloses the current list is superordinate to it; and in addition, a list in the body of\na rule is superordinate to the list in the head of the rule, but not vice versa.\nSpecifying and Verbalising Answer Set Programs in Controlled Natural Language\n11\nThe following grammar rule processes a deﬁnite noun phrase and checks whether this deﬁnite\nnoun phrase introduces a new entity or whether it is used anaphorically. After processing the\ndeterminer (det) and the noun (noun), the grammar rule calls the anaphora resolution algorithm\nfor deﬁnite noun phrases (def) and updates the internal format:\nnp(mode:M, loc:body, crd:'-', fcn:subj, num:N, arg:X,\ncl:C1-C4, ante:A1-A3) -->\ndet(mode:M, num:N, def:'+', cl:C1-C2),\nnoun(mode:M, num:N, arg:X, cl:C2-C3, ante:A1-A2),\n{ anaphora_resolution(def, M, X, C1, C3, C4, A1, A2, A3) }.\nThat means after processing of a deﬁnite noun phrase and temporarily adding the correspond-\ning literal (for example, class(X,student)) to the outgoing part (C3) of the difference list, the\nanaphora resolution algorithm checks, whether an accessible literal already exists in the data\nstructure C1 or not. If one exists, then the variable (X) of the newly added literal is uniﬁed with\nthe variable of the closest accessible antecedent and the temporarily added literal is removed\nfrom the outgoing part of the difference list C3; resulting in C4. If no accessible antecedent exists,\nthen the deﬁnite noun phrase is treated as an indeﬁnite one and the new literal is not removed.\nThe anaphora resolution algorithm also updates the list of accessible antecedents (A3) at the same\ntime that can then be used to generate a list of referring expressions for the user interface.\nThe same grammar rule is used in the case of generation to decide whether a deﬁnite noun\nphrase can be generated or not. Since the internal data structure is reduced during the generation\nprocess, the anaphora resolution algorithm uses the list of accessible antecedents (A1) that is built\nup in parallel to check if a deﬁnite noun phrase can be generated or not.\n6 Sentence Planner\nIn the case of generation, the reader ﬁrst reads the answer set program and reconstructs the inter-\nnal format. The sentence planner then tries to apply a number of aggregation strategies (Reiter\nand Dale 2000, Horacek 2015). Aggregation requires to reorganise the internal format of the\nanswer set program. Remember that the internal format relies on a ﬂat notation. In order to re-\nconstruct this internal format for the modiﬁed answer set program in Figure 3, the reader has to\nadd literals of the form named(Var,Name) to the internal representation for each constant (proper\nname) that occurs in the answer set program. After this reconstruction process, the result looks,\nfor example, as follows for A5' and A6':\n[[..., named(F,tom),\npred(F,C,study_at),\nnamed(C,macquarie_university), '.',\nnamed(F,tom),\nprop(F,E,enrolled_in),\nnamed(E,information_technology), '.', ...]]\nIn the next step, the sentence planner tries to apply a suitable aggregation strategy using this\nreconstructed representation as a starting point. One possible strategy is to identify identical\nliterals that can occur in the subject position (for example, named(F,tom)) and literals with the\nsame number of arguments (for example, pred(F,C,study at) and prop(F,E,enrolled in)) that\ncan occur in the predicate position, and aggregate these literals in order to generate a coordinated\nverb phrase. In our case, the sentence planner generates the following internal structure for verb\nphrase coordination:\n12\nRolf Schwitter\n[[..., named(F,tom),\npred(F,C,study_at),\nnamed(C,macquarie_university),\nprop(F,E,enrolled_in),\nnamed(E,information_technology), '.', ...]]\nThe sentence planner only aggregates up to three literals in predicate position and takes the\nnumber of their arguments and their proximity into consideration. That means no more than three\nverbs/relational adjectives with the same number of complements are currently coordinated in\norder to achieve good readability of the resulting speciﬁcation.\nIn many cases the reconstruction of the internal structure is more complex and the sentence\nplanner needs to select among the available strategies. We illustrate why this is the case with the\nhelp of the graph colouring problem (Gebser et al. 2012). The graph colouring problem deals\nwith the assignment of different colours to nodes of a graph under certain constraints as speciﬁed\nin controlled natural language in Figure 5:\nThe node 1 is connected to the nodes 2, 3 and 4.\n%\nC1\nThe node 2 is connected to the nodes 4, 5 and 6.\n%\nC2\nThe node 3 is connected to the nodes 1, 4 and 5.\n%\nC3\nThe node 4 is connected to the nodes 1 and 2.\n%\nC4\nThe node 5 is connected to the nodes 3, 4 and 6.\n%\nC5\nThe node 6 is connected to the nodes 2, 3 and 5.\n%\nC6\nRed is a colour.\n%\nC7\nBlue is a colour.\n%\nC8\nGreen is a colour.\n%\nC9\nEvery node is assigned to exactly one colour.\n% C10\nIt is not the case that a node X is assigned to a colour\n% C11\nand a node Y is assigned to the colour\nand the node X is connected to the node Y.\nFig. 5: Graph Colouring Problem in Controlled Natural Language\nThis speciﬁcation shows the use of enumerations in object position (C1-C6). It also uses explicit\nvariables (X and Y) in C11 to distinguish noun phrases that have the same form but denote different\nentities. The translation of this speciﬁcation results in the answer set program in Figure 6. This\nprogram does not keep all the information that is available in the internal format. In the internal\nformat, integers (1-6) are represented with the help of literals of the form integer(Var,Integer)\nbut result in integer arguments (for example, node(1)) in the answer set program and variables\n(X and Y) are represented via literals of the form variable(Var,VariableName) which are not\nnecessary anymore in the answer set program after anaphora resolution.\nGenerating a verbalisation that starts from the answer set program in Figure 6 requires again\nﬁrst the explicit reconstruction of the internal representation by the reader. For example, this\nreconstruction looks as follows for A4':\n[[..., class(D,node), integer(D,4), prop(D,A,connected_to), class(A,node),\ninteger(A,1), '.',\nclass(D,node), integer(D,4), prop(D,B,connected_to), class(B,node),\ninteger(B,2), '.', ...]]\nSpecifying and Verbalising Answer Set Programs in Controlled Natural Language\n13\nnode(1). connected_to(1,2). node(2). connected_to(1,3). node(3).\n%\nA1'\nconnected_to(1,4). node(4).\nconnected_to(2,4). connected_to(2,5). node(5). connected_to(2,6).\n%\nA2'\nnode(6).\nconnected_to(3,1). connected_to(3,4). connected_to(3,5).\n%\nA3'\nconnected_to(4,1). connected_to(4,2).\n%\nA4'\nconnected_to(5,3). connected_to(5,4). connected_to(5,6).\n%\nA5'\nconnected_to(6,2). connected_to(6,3). connected_to(6,5).\n%\nA6'\ncolour(red).\n%\nA7'\ncolour(blue).\n%\nA8'\ncolour(green).\n%\nA9'\n1 { assigned_to(A,B) : colour(B) } 1 :- node(A).\n% A10'\n:- node(C), assigned_to(C,D), colour(D), node(E), assigned_to(E,D),\n% A11'\nconnected_to(C,E).\nFig. 6: Graph Colouring Problem as Answer Set Program\nThe sentence planner then reorganises this internal format in order to prepare for enumerating\nthe integers (1, 2) of those nodes in object position that have the same subject and the same\nproperty name. In our case, this results in the following internal representation:\n[[..., class(D,node),\ninteger(D,4),\nprop(D,A,connected_to),\nclass(A,node),\ninteger(A,1),\nprop(D,B,connected_to),\nclass(B,node),\ninteger(B,2), '.', ...]]\nThe grammar rules are ordered in such a way that priority is given to enumeration for this\nstructure and not to verb phrase coordination. In our case, the original sentence C4, here repeated\nas (a), is generated and not the sentence (b):\na. The node 4 is connected to the nodes 1 and 2.\nb. The node 4 is connected to the node 1 and is connected to the node 2.\nIn the case of A11', the reconstruction process is more complex since the clause in the answer\nset program contains two literals (node(C) and node(E)) with the same name. These literals need\nto be distinguished on the level of the controlled natural language with the help of variable names.\nThat means literals of the form variable(Var,VariableName) that represent variable names need\nto be added to the internal representation:\n[[..., ':-', [class(O,node), variable(O,'X'),\nprop(O,P,assigned_to),\nclass(P,colour),\nclass(Q,node), variable(Q,'Y'),\nprop(Q,P,assigned_to),\nclass(P,colour),\nclass(O,node), variable(O,'X'),\nprop(O,Q,connected_to),\nclass(Q,node), variable(Q,'Y')], '.', ...]]\n14\nRolf Schwitter\nIn order to generate a verbalisation that distinguishes between indeﬁnite and deﬁnite noun\nphrases in the same way as illustrated in C11 of Figure 5, the ﬁrst occurrence of the literals that\ngenerate an indeﬁnite noun phrase (for example, class(O,node), variable(O,'X')) are added\nto the outgoing difference list that maintains all accessible antecedents, so that this information\ncan be looked up later from that list to decide whether a deﬁnite noun phrase can be generated or\nnot if the same literals occur a second time in the internal representation.\n7 Conclusion\nIn this paper, we presented a bi-directional grammar for the controlled natural language PENGASP.\nThis grammar uses the deﬁnite clause grammar formalism and allows us to translate a speciﬁ-\ncation written in controlled natural language into an answer set program using a difference list\nas main data structure. This answer set program can then be translated (after potential modiﬁca-\ntions) back again into a controlled language speciﬁcation. The same grammar rules can be used\nfor processing and for generation; only those grammar rules that process preterminal symbols\nand add literals to or remove literals from the difference list need to be duplicated. However,\nthese grammar rules have a nice symmetric structure that reﬂects the processing and generation\ntask. The grammar also deals with referring expressions and resolves these expressions during\nthe parsing process. The generation process requires sentence planning and allows us to generate\nsemantically equivalent speciﬁcations if the answer set program has not been modiﬁed. If the\nprogram has been modiﬁed, then round-tripping is still possible as long as we stick to the same\nnaming convention. The presented approach allows domain specialists who may not be familiar\nwith answer set programming to specify and inspect an answer set program on the level of a\ncontrolled natural language. To the best of our knowledge, this is the ﬁrst bi-directional grammar\nof a controlled natural language that can be used to process and verbalise answer set programs.\nReferences\nDaniel Bailey, Amelia Harrison, Yuliya Lierler, Vladimir Lifschitz, and Julian Michael. The Winograd\nSchema Challenge and Reasoning about Correlation. In: Commensense, 2015.\nChitta Baral. 2003. In: Knowledge Representation, Reasoning and Declarative Problem Solving, Cambridge\nUniversity Press.\nChitta Baral and Luis Tari. Using AnsProlog with Link Grammar and WordNet for QA with deep reasoning.\nIn: International Conference on Information Technology, 2006.\nChitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez, and Jiayu Zhou. Using Inverse λ and Generalization\nto Translate English to Formal Languages. In: Proceedings of the Ninth International Conference in\nComputational Semantics, pp. 35-44, 2011.\nPeter Clark, Phil Harrison, Tom Jenkins, John Thompson, and Rick Wojcik. Acquiring and Using World\nKnowledge using a Restricted Subset of English. In: The 18th International FLAIRS Conference\n(FLAIRS’05), pp. 506-511, 2005.\nEzgi Demirel, Kamil Doruk Gur, and Esra Erdem. Human-Robot Interaction in a Shopping Mall: A CNL\nApproach. In: CNL 2016, pp. 111-122, 2016.\nEsra Erdem and Reyyan Yeniterzi. Transforming controlled natural language biomedical queries into an-\nswer set programs. In: Proceedings of the Workshop on BioNLP, pp. 117-124, 2009.\nEsra Erdem and Umut ¨Oztok. Generating explanations for biomedical queries. In: Theory and Practice of\nLogic Programming, Volume 15, Issue 1, pp. 35-78, 2015.\nNorbert E. Fuchs, Kaarel Kaljurand, and Tobias Kuhn. Attempto Controlled English for Knowledge Rep-\nresentation. In: Reasoning Web, Fourth International Summer School 2008, LNCS 5224, pp. 104-124,\n2008.\nSpecifying and Verbalising Answer Set Programs in Controlled Natural Language\n15\nMartin Gebser, Roland Kaminski, Benjamin Kaufmann, Torsten Schaub. Answer Set Solving in Practice,\nSynthesis Lectures on Artiﬁcial Intelligence and Machine Learning, Morgan & Claypool, 2012.\nMartin Gebser, Roland Kaminski, Benjamin Kaufmann, Marius Lindauer, Max Ostrowski, Javier\nRomero Torsten Schaub, and Sven Thiele. Potassco User Guide, Version 2.1.0, available at:\nhttps://github.com/potassco/guide/releases/, October 2017.\nMichael Gelfond and Vladimir Lifschitz. The Stable Model Semantics for Logic Programming. In: Pro-\nceedings of the Fifth International Conference on Logic Programming (ICLP), pp. 1070-1080, 1988.\nBart Geurts, David I. Beaver, and Emar Maier. Discourse Representation Theory. In: Stanford\nEncyclopedia of Philosophy, available at: https://plato.stanford.edu/entries/discourse-\nrepresentation-theory/, 24th December, 2015.\nStephen Guy and Rolf Schwitter. The PENGASP System: Architecture, Language and Authoring Tool. In:\nJournal of Language Resources and Evaluation, Special Issue: Controlled Natural Language, Vol. 51,\nIssue 1, pp. 67-92, 2017.\nHelmut Horacek. New Concepts in Natural Language Generation: Planning, Realization and Systems. Lin-\nguistic Communication in Artiﬁcial Intelligence, Bloomsbury, 2015.\nHans Kamp and Uwe Reyle. From Discourse to Logic. Kluwer, Dordrecht, 1993.\nTobias Kuhn. AceRules: Executing Rules in Controlled Natural Language. In: Proceedings of the First\nInternational Conference on Web Reasoning and Rule Systems (RR2007), LNCS, pp. 299-308, 2007.\nYuliya Lierler and Peter Sch¨uller. Parsing Combinatory Categorial Grammar via Planning in Answer Set\nProgramming. In: Correct Reasoning, Springer, pp. 436-453, 2012.\nVladimir Lifschitz. What is Answer Set Programming? In: Proceedings of AAAI, pp. 1594-1597, 2008.\nHa Vo Nguyen, Arindam Mitra, and Chitta Baral. The NL2KR Platform for building Natural Language\nTranslation Systems. In Proceedings of ACL, 2015.\nFernando C. N. Pereira and David H. D. Warren. Deﬁnite Clause Grammars for Language Analysis – A Sur-\nvey of the Formalism and a Comparison with Augmented Transition Networks. In: Artiﬁcial Intelligence,\nNo. 13, pp. 231-278, 1980.\nFernando C. N. Pereira and Stuart M. Shieber. Prolog and Natural-Language Analysis. Cambridge Univer-\nsity Press, 1987.\nEhud Reiter and Robert Dale. Building Natural Language Generation Systems. Studies in Natural Language\nProcessing. Cambridge University Press, 2000.\nPeter Sch¨uller. Flexible Combinatory Categorial Grammar using the CYK Algorithm and Answer Set Pro-\ngramming. In: LPNMR, pp. 499-511, 2013.\nPeter Sch¨uller. Tackling Winograd Schemas by Formalizing Relevance Theory in Knowledge Graphs. In:\nInternational Conference on the Principles of Knowledge Representation and Reasoning (KR), pp. 358-\n367, 2014.\nRolf Schwitter. Working for Two: a Bidirectional Grammar for a Controlled Natural Language. In: Pro-\nceedings of AI 2008, LNAI 5360, pp. 168-179, 2008.\nJohn F. Sowa. Common Logic Controlled English. Draft, http://www.jfsowa.com/clce/specs.htm,\n24 February 2004.\nLeon S. Sterling and Ehud Y. Shapiro. The Art of Prolog, Advanced Programming Techniques. Second\nEdition, MIT Press, 1994.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2018-04-28",
  "updated": "2018-04-28"
}