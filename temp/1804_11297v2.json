{
  "id": "http://arxiv.org/abs/1804.11297v2",
  "title": "Sampling strategies in Siamese Networks for unsupervised speech representation learning",
  "authors": [
    "Rachid Riad",
    "Corentin Dancette",
    "Julien Karadayi",
    "Neil Zeghidour",
    "Thomas Schatz",
    "Emmanuel Dupoux"
  ],
  "abstract": "Recent studies have investigated siamese network architectures for learning\ninvariant speech representations using same-different side information at the\nword level. Here we investigate systematically an often ignored component of\nsiamese networks: the sampling procedure (how pairs of same vs. different\ntokens are selected). We show that sampling strategies taking into account\nZipf's Law, the distribution of speakers and the proportions of same and\ndifferent pairs of words significantly impact the performance of the network.\nIn particular, we show that word frequency compression improves learning across\na large range of variations in number of training pairs. This effect does not\napply to the same extent to the fully unsupervised setting, where the pairs of\nsame-different words are obtained by spoken term discovery. We apply these\nresults to pairs of words discovered using an unsupervised algorithm and show\nan improvement on state-of-the-art in unsupervised representation learning\nusing siamese networks.",
  "text": "Sampling strategies in Siamese Networks for unsupervised speech\nrepresentation learning\nRachid Riad1,2, Corentin Dancette1, Julien Karadayi1, Neil Zeghidour 1,3, Thomas Schatz1,4,5,\nEmmanuel Dupoux1,3\n1 CoML/ENS/CNRS/EHESS/INRIA/PSL Research University, Paris, France\n2NPI/ENS/INSERM/UPEC, Cr´eteil, France, 3Facebook A.I. Research, Paris, France\n4Department of Linguistics & UMIACS, University of Maryland, College Park MD, USA\n5Department of Linguistics, Massachusetts Institute of Technology, Cambridge MA, USA\n{emmanuel.dupoux,riadrachid3,julien.karadayi,corentin.dancette, neil.zeghidour,\nthomas.schatz.1986}@gmail.com\nAbstract\nRecent studies have investigated siamese network architec-\ntures for learning invariant speech representations using same-\ndifferent side information at the word level. Here we investi-\ngate systematically an often ignored component of siamese net-\nworks: the sampling procedure (how pairs of same vs. different\ntokens are selected). We show that sampling strategies taking\ninto account Zipf’s Law, the distribution of speakers and the\nproportions of same and different pairs of words signiﬁcantly\nimpact the performance of the network. In particular, we show\nthat word frequency compression improves learning across a\nlarge range of variations in number of training pairs. This ef-\nfect does not apply to the same extent to the fully unsupervised\nsetting, where the pairs of same-different words are obtained\nby spoken term discovery. We apply these results to pairs of\nwords discovered using an unsupervised algorithm and show an\nimprovement on state-of-the-art in unsupervised representation\nlearning using siamese networks.\nIndex Terms: language acquisition, speech recognition, sam-\npling, Zipf’s law, weakly supervised learning, unsupervised\nlearning, Siamese network, speech embeddings, ABX, zero re-\nsource speech technology\n1. Introduction\nCurrent speech and language technologies based on Deep Neu-\nral Networks (DNNs) [1] require large quantities of transcribed\ndata and additional linguistic resources (phonetic dictionary,\ntranscribed data). Yet, for many languages in the world, such\nresources are not available and gathering them would be very\ndifﬁcult due to a lack of stable and widespread orthography [2].\nThe goal of Zero-resource technologies is to build speech\nand language systems in an unknown language by using only\nraw speech data [3]. The Zero Resource challenges (2015 and\n2017) focused on discovering invariant sub-word representa-\ntions (Track 1) and audio terms (Track 2) in an unsupervised\nfashion. Several teams have proposed to use terms discovered\nin Track 2 to provide DNNs with pairs of same versus differ-\nent words as a form of weak or self supervision for Track 1:\ncorrespondence auto-encoders [4, 5], siamese networks [6, 7].\nThis paper extends and complements the ABnet Siamese\nnetwork architecture proposed by [8, 6] for the sub-word mod-\nelling task. DNN contributions typically focus on novel archi-\ntectures or objective functions. Here, we study an often over-\nlooked component of Siamese networks: the sampling proce-\ndure which chooses the set of pairs of same versus different to-\nkens. To assess how each parameter contributes to the algorithm\nperformance, we conduct a comprehensive set of experiments\nwith a large range of variations in one parameter, holding con-\nstant the quantity of available data and the other parameters. We\nﬁnd that frequency compression of the word types has a partic-\nularly important effect. This is congruent with other frequency\ncompression techniques used in NLP, for instance in the com-\nputation of word embeddings (word2vec [9]). Besides, Levy et\nal. [10] reveals that the performance differences between word-\nembedding algorithms are due more to the choice of the hyper-\nparameters, than to the embedding algorithms themselves.\nIn this study, we ﬁrst show that, using gold word-level an-\nnotations on the Buckeye corpus, a ﬂattened frequency range\ngives the best results on phonetic learning in a Siamese network.\nThen, we show that the hyper-parameters that worked best with\ngold annotations yield improvements in the zero-resource sce-\nnario (unsupervised pairs) as well. Speciﬁcally, they improve\non the state-of-the-art obtained with siamese and auto-encoder\narchitectures.\n2. Methods\nWe developed a new package abnet31 using the pytorch frame-\nwork [11]. The code is open-sourced (BSD 3-clause) and avail-\nable on github, as is the code for the experiments for this paper2.\n2.1. Data preparation\nFor the weakly-supervised study, we use 4 subsets of the Buck-\neye [12] dataset from the ZeroSpeech 2015 challenge [3] with,\nrespectively, 1%, 10%, 50%, and 100% of the original data (see\nTable 1). The original dataset is composed of American En-\nglish casual conversations recorded in the laboratory, with no\noverlap, no speech noises, separated in two splits: 12 speakers\nfor training and 2 speakers for test. A Voice Activity Detec-\ntion ﬁle indicates the onset and offset of each utterance and en-\nables to discard silence portions of each ﬁle. We use the ortho-\ngraphic transcription from word-level annotations to determine\nsame and different pairs to train the siamese networks.\nIn the fully unsupervised setting, we obtain pairs of same\nand different words from the Track 2 baseline of the 2015 Ze-\nroSpeech challenge [3]: the Spoken Term Discovery system\nfrom [13]. We use both the original ﬁles from the baseline,\nand a rerun of the algorithm with systematic variations on its\n1https://github.com/bootphon/abnet3\n2https://github.com/Rachine/sampling_siamese2018\narXiv:1804.11297v2  [cs.CL]  23 Aug 2018\nDuration\n#tokens\n#words\n#possible pairs\n1%\n3.0 min\n1006\n355\n∼5.105\n10%\n29.9 min\n7189\n1297\n∼2.107\n50%\n149.5 min\n34912\n3112\n∼6.108\n100%\n299.1 min\n69543\n4538\n≥2.109\nTable 1: Statistics for the 4 Buckeye splits used for the weakly\nsupervised training, the duration in minutes expressed the total\namount of speech for training\nsimilarity threshold parameter.\nFor the speech signal pre-processing, frames are taken ev-\nery 10ms and each one is encoded by a 40 log-energy Mel-\nscale ﬁlterbank representing 25ms of speech (Hamming win-\ndowed), without deltas or delta-delta coefﬁcients. The input to\nthe Siamese network is a stack of 7 successive ﬁlterbank frames.\nThe features are mean-variance normalized per ﬁle, using the\nVAD information.\n2.2. ABnet\nA Siamese network is a type of neural network architecture\nthat is used for representation learning, initially introduced for\nsignature veriﬁcation [14].\nIt contains 2 subnetworks shar-\ning the same architecture and weights.\nIn our case, to ob-\ntain the training information, we use the lexicon of words to\nlearn an embedding of speech sounds which is more represen-\ntative of the linguistic properties of the signal at the sub-word\nlevel (phoneme structure) and invariant to non-linguistic ones\n(speaker ID, channel, etc). A token t is from a speciﬁc word\ntype w (ex: “the”,“process” etc.)\npronounced by a speciﬁc\nspeaker s. The input to the network during training is a pair\nof stacked frames of ﬁlterbank features x1 and x2 and we use\nas label y = 1({w1 = w2}). For pairs of identical words, we\nrealign them at the frame level using the Dynamic Time Warp-\ning (DTW) algorithm [15]. Based on the alignment paths from\nthe DTW algorithm, the sequences of the stacked frames are\nthen presented as the entries of the siamese network. Dissimilar\npairs are aligned along the shortest word, e.g. the longest word\nis trimmed. With these notions of similarity, we can learn a rep-\nresentation where the distance between the two outputs of the\nsiamese network e(x1) and e(x2) try to respect as much as pos-\nsible the local constraints between x1 and x2. To do so, ABnet\nis trained with the margin cosine loss function:\nlγ(x1, x2, y) =\n\u001a\n−cos(e(x1), e(x2)),\nif y = 1\nmax(0, cos(e(x1), e(x2)) −γ),\notherwise\nFor a clear and fair comparison between the sampling pro-\ncedures we ﬁxed the network architecture and loss function as\nin [6]. The subnetwork is composed of 2 hidden layers with\n500 units, with the Sigmoid as non-linearity and a ﬁnal embed-\nding layer of 100 units. For regularization, we use the Batch\nNormalization technique [16], with a loss margin γ = 0.5. All\nthe experiments are carried using the Adam training procedure\n[17] and early-stopping on a held-out validation set of 30% of\nspoken words. We sample the validation set in the same way as\nthe training set.\n2.3. Sampling\nThe sampling strategy refers to the way pairs of tokens are fed to\nthe Siamese network. Sampling every possible pairs of tokens\nbecomes quickly intractable as the dataset grows (cf. Table 1).\nThere are four different possible conﬁgurations for a pair of\nword tokens (t1, t2) : whether, or not, the tokens are from the\nsame word type, w1 = w2. and whether, or not, the tokens are\npronounced by the same speaker, s1 = s2.\nEach speciﬁc word type w is characterized by the total num-\nber of occurrences nw it has been spoken in the whole corpus.\nThen, is deduced the frequency of appearances fw ∝nw, and\nrw its frequency rank in the given corpus. We want to sample\na pair of word tokens, in our framework we sample indepen-\ndently these 2 tokens. We deﬁne the probability to sample a\nspeciﬁc token word type w as a function of nw. We introduce\nthe function φ as the sampling compression function:\nP(w) =\nφ(nw)\nP\n∀w′ φ(nw′)\n(1)\nWhen a speciﬁc word type w is selected according to these\nprobabilities, a token t is selected randomly from the speciﬁc\nword type w. The usual strategy to select pairs to train siamese\nnetworks is to randomly pick two tokens from the whole list\nof training tokens examples [14, 18, 6]. In this framework, the\nsampling function corresponds φ : n →n. Yet, there is a puz-\nzling phenomenon in human language, there exists an empiri-\ncal law for the distribution of words, also known as the Zipf’s\nlaw [19]. Words types appear following a power law relation-\nship between the frequency fw and the corresponding rank rw:\na few very high-frequency types account for almost all tokens\nin a natural corpus (most of them are function words such as\n“the”,“a”,“it”, etc.) and there are many word types with a low\nfrequency of appearances (“magret”,“duck”,“hectagon”). The\nfrequency ft of type t scales with its corresponding rt follow-\ning a power law, with a parameter α depending on the language:\nfw ∝1\nrα\nw\n, α ≈1\nOne main effect on the training is the oversampling of word\ntypes with high frequency, and this is accentuated with the sam-\npling of two tokens for the siamese. These frequent, usually\nmonosyllabic, word types do not carry the necessary phonetic\ndiversity to learn an embedding robust to rarer co-articulations,\nand rarer phones. To study and minimize this empirical linguis-\ntic trend, we will examine 4 other possibilities for the φ function\nthat compress the word frequency type:\nφ : n →\n2√n,\nφ : n →\n3√n\nφ : n →log(1 + n),\nφ : n →1\nThe ﬁrst two options minimize the effect of the Zipf’s Law\non the frequency, but the power law is kept. The log option re-\nmoves the power law distribution, yet it keeps a linear weight-\ning as a function of the rank of the types. Finally with the last\nconﬁguration, the word types are sampled uniformly.\nAnother important variation factor in speech realizations\nis the speaker identity. We expect that the learning of speech\nrepresentations to take advantage of word pairs from different\nspeakers, to generalize better to new ones, and improve the ABX\nperformance.\nP s\n−= #Sampled pairs pronounced by different speakers\n#Sampled pairs\nGiven the natural statistics of the dataset, the number of\npossible ”different” pairs exceeds by a large margin the num-\nber of possible ”same” pairs (∼1% of all token pairs for the\nGold 1%\nGold 10%\nGold 50%\nGold 100%\nBuckeye Splits\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\n30.0\n35.0\n33.5\n40.0\nerror rate (%)\nABX across-talker\nFilterbanks\nn\nn\n3 n\nlog(1 + n)\n1\nFigure 1: ABX across-speaker error rates on test set with vari-\nous sampling compression functions φ for the 4 different Buck-\neye splits used for weakly supervised training. Here, the pro-\nportions of pairs with different speakers P s\n−and with different\nword types P w\n−are kept ﬁxed: P s\n−= 0.5, P w\n−= 0.5\nBuckeye-100%). The siamese loss is such that ”Same” pairs are\nbrought together in embedding space, and ”Different” pairs are\npulled apart. Should we reﬂect this statistic during the training,\nor eliminate it by presenting same and different pairs equally?\nWe manipulate systematically the proportion of pairs from dif-\nferent word types fed to the network:\nP w\n−= #Sampled pairs with non-matching word types\n#Sampled pairs\n2.4. Evaluation with ABX tasks\nTo test if the learned representations can separate phonetic cate-\ngories, we use a minimal pair ABX discrimination task [20, 21].\nIt only requires to deﬁne a dissimilarity function d between\nspeech tokens, no external training algorithm is needed. We\ndeﬁne the ABX-discriminability of category x from category y\nas the probability that A and X are further apart than B and\nX when A and X are from category x and B is from cate-\ngory y, according to a dissimilarity function d. Here, we focus\non phone triplet minimal pairs: sequences of 3 phonemes that\ndiffer only in the central one (“beg”-“bag”, “api”-“ati”, etc.).\nFor the within-speaker task, all the phones triplets belong to the\nsame speaker (e.g. A = begT1, B = bagT1, X = bag′\nT1) Fi-\nnally the scores for every pair of central phones are averaged\nand subtracted from 1 to yield the reported within-talker ABX\nerror rate. For the across-speaker task, A and B belong to the\nsame speaker, and X to a different one (e.g. A = begT1, B =\nbagT1, X = bag′\nT2). The scores for a given minimal pair are\nﬁrst averaged across all of the pairs of speakers for which this\ncontrast can be made. As above, the resulting scores are aver-\naged over all contexts over all pairs of central phones and con-\nverted to an error rate.\n3. Results\n3.1. Weakly supervised Learning\n3.1.1. Sampling function φ\nWe ﬁrst analyze the results for the sampling compression func-\ntion φ Figure 1. For all training datasets, we observe a similar\npattern for the performances on both tasks: the word frequency\ncompression improves the learning and generalization. The re-\nsult show that, compared to the raw ﬁlterbank features base-\nline, all the trained ABnet networks improve the scores on the\nphoneme discrimination tasks, even in the 1% scenario. Yet,\nthe improvement with the usual sampling scenario φ : n →n\nis small in all 4 training datasets. The optimal function for the\nwithin and across speaker task on all training conﬁguration is\nthe uniform function φ : n →1. It yields substantial improve-\nments over the raw ﬁlterbanks for ABX task across-speaker (\n5.6 absolute points and 16.8% relative improvement for the\n1%-Buckeye training). The addition of data for these experi-\nments improves the performance of the network, but not in a\nsubstantial way: the improvements from 1%-Buckeye to 100%-\nBuckeye, for φ : n →1, is 1.9 absolute points and 7.9% rel-\native. These results show that using frequency compression is\nclearly beneﬁcial, and surprisingly adding more data is still ad-\nvantageous but not as much as the choice of φ. Renshaw et al.\n[5], found similar results with a correspondence auto-encoder,\ntraining with more training data did not yield improvements for\ntheir system.\n3.1.2. Proportion of pairs from different speakers P s\n−\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPs Proportion of pairs with non-matching speakers\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\n30.0\n35.0\n33.5\n40.0\n45.0\n50.0\nerror rate (%)\nABX across-talker\nGold 1%\nGold 100%\nFilterbanks\nFigure 2: Average ABX error rates across-speaker with various\nproportion pairs of different speakers P s\n−, with φ : n →1 and\nP w\n−= 0.5.\nWe now look at the effect on the ABX performances of the\nproportion of pairs of words pronounced by two different speak-\ners Figure 2. We start from our best sampling function conﬁg-\nuration so far φ : n →1. We report on the graph only the\ntwo extreme training settings. The variations for the 4 different\ntraining splits are similar, and still witness a positive effect with\nadditional data on the siamese network performances. Counter-\nintuitively, the performances on the ABX tasks does not take\nadvantage of pairs from different speakers. It even shows a ten-\ndency to increase the ABX error rate: for the 100%-Buckeye\nwe witness an augmentation of the ABX error-rate (2.9 points\nand 11.6% relative) between P s\n−= 0 and P s\n−= 1. One of our\nhypothesis on this surprising effect, might be the poor perfor-\nmance of the DTW alignment algorithm directly on raw ﬁlter-\nbanks features of tokens from 2 different speakers.\n3.1.3. Proportion of pairs with different word types P w\n−\nWe next study the inﬂuence of the proportion of pairs from dif-\nferent word types P w\n−Figure 3. In all training scenarios, to\nprivilege either only the positive or the negative examples is not\nthe solution. For the different training splits, the optimal num-\nber for P w\n−is either 0.7 or 0.8 in the within and across speaker\nABX task. We do not observe a symmetric inﬂuence of the\npositive and negative examples, but it is necessary to keep the\nsame and different pairs. The results collapsed, if the siamese\nnetwork is provided only with positive labels to match: the net-\nwork will tend to map all speech tokens to the same vector point\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPw Proportion of pairs with non-matching word types\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\n30.0\n35.0\n33.5\n40.0\n45.0\n50.0\nerror rate (%)\nABX across-talker\nGold 1%\nGold 100%\nFilterbanks\nFigure 3: Average ABX error rates across-speaker with various\nproportion pairs with different word types P w\n−, where φ : n →\n1 and P s\n−= 0.5\nand the discriminability is at chance level.\n3.2. Applications to fully unsupervised setting\n3.2.1. ZeroSpeech 2015 challenge\nNow, we transfer the ﬁndings about sampling from the weakly\nsupervised setting, to the fully unsupervised setting. We report\nin Table 2 our results for the two ZeroSpeech 2015[3] corpus:\nthe same subset of the Buckeye Corpus as earlier and a subset of\nthe NCHLT corpus of Xitsonga [22]. To train our siamese net-\nworks, we use as [6], the top-down information from the base-\nline for the Track 2 (Spoken Term Discovery) of the ZeroSpeech\n2015 challenge from [13]. The resulting clusters are not perfect,\nwhereas we had perfect clusters in our previous analysis.\nModels\nEnglish\nXitsonga\nwithin\nacross\nwithin\nacross\nbaseline (MFCC)\n15.6\n28.1\n19.1\n33.8\nsupervised topline (HMM-GMM)\n12.1\n16.0\n04.5\n03.5\nOur ABnet with P w\n−= 0.7, P s\n−= 0, φ : n →1\n10.4\n17.2\n9.4\n15.2\nCAE, Renshaw et al. [5]\n13.5\n21.1\n11.9\n19.3\nABnet, Thioli`ere et al. [6]\n12.0\n17.9\n11.7\n16.6\nScatABnet, Zeghidour et al. [7]\n11.0\n17\n12.0\n15.8\nDPGMM Chen et al. [23]\n10.8\n16.3\n9.6\n17.2\nDPGMM+PLP+bestLDA+DPGMM Heck et al. [24]\n10.6\n16.0\n8.0\n12.6\nTable 2: ABX discriminability results for the ZeroSpeech2015\ndatasets. The best error rates for each conditions for siamese\narchitectures are in bold. The best error rates for each condi-\ntions overall are underlined.\nIn Thioli`ere et al. [6] the sampling is done with : P w\n−=\nP s\n−= 0.5, and φ = n →n. This gives us a baseline to\ncompare our sampling method improvements with our own im-\nplementation of siamese networks.\nFirst, the “discovered” clusters – obtained from spoken\nterm discovery system – don’t follow the Zipf’s law like the\ngold clusters. This difference of distributions diminishes the\nimpact of the sampling compression function φ.\nWe matched state-of-the-art for this challenge only on the\nABX task within-speaker for the Buckeye, otherwise the modi-\nﬁed DPGMM algorithm proposed by Heck et al. stays the best\nsubmissions for the 2015 ZeroSpeech challenge.\n3.2.2. Spoken Term discovery - DTW-threshold δ\nFinally, we study the inﬂuence of the DTW-threshold δ used in\nthe spoken discovery system on the phonetic discriminability\nof siamese networks. We start again from our best ﬁnding from\nweakly supervised learning. The clusters found by the Jansen et\nal. [13] system are very sensitive to this parameter with a trade-\noff between the Coverage and the Normalized Edit Distance\n(NED) introduced by [25].\nδ\n#clusters\nNED\nCoverage\nABX across\n0.82\n27,770\n0.792\n0.541\n18.2\n0.83\n27,758\n0.792\n0.541\n18.1\n0.84\n27,600\n0.789\n0.541\n18.4\n0.85\n26,466\n0.76\n0.54\n18.4\n0.86\n22,627\n0.711\n0.527\n18.2\n0.87\n16,108\n0.569\n0.485\n18.2\n0.88\n9,853\n0.442\n0.394\n17.7\n0.89\n5,481\n0.309\n0.282\n17.6\n0.90\n2,846\n0.228\n0.182\n17.9\n0.91\n1,286\n0.179\n0.109\n18.6\n0.92\n468\n0.179\n0.058\n19.2\nTable 3: Number of found clusters, NED, Coverage, ABX dis-\ncriminability results with our ABnet with P w\n−= 0.7, P s\n−=\n0, φ : n →1, for the ZeroSpeech2015 Buckeye for various\nDTW-thresholds δ in the Jansen et al. [13] STD system. The\nbest results for each metric are in bold.\nWe ﬁnd that ABnet is getting good results across the various\noutputs of the STD system shown in Table 3 and improves over\nthe ﬁlterbanks results in all cases. Obtaining more data with\nthe STD system involves a loss in words quality. In contrast\nwith the weakly supervised setting, there is an optimal trade-\noff between the amount and quality of discovered words for the\nsub-word modelling task with siamese networks.\n4. Conclusions and Future work\nWe presented a systematic study of the sampling component\nin siamese networks.\nIn the weakly-supervised setting, we\nestablished that the word frequency compression had an im-\nportant impact on the discriminability performances. We also\nfound that optimal proportions of pairs with different types and\nspeakers are not the ones usually used in siamese networks.\nWe transferred the best parameters to the unsupervised setting\nto compare our results to the 2015 Zero Resource challenge\nsubmissions. It lead to improvements over the previous neu-\nral networks architectures, yet the Gaussian mixture methods\n(DPGMM) remain the state-of-the-art in the phonetic discrim-\ninability task. In the future, we will study in the same system-\natic way the inﬂuence of sampling in the fully unsupervised set-\nting. We will then try to leverage the better discriminability of\nour representations obtained with ABnet to improve the spo-\nken term discovery, which relies on frame-level discrimination\nto ﬁnd pairs of similar words. Besides, power law distributions\nare endemic in natural language tasks. It would be interesting\nto extend this principle to other tasks (for instance, language\nmodeling).\n5. Acknowledgements\nThe team’s project is funded by the European Research Council\n(ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale\npour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-\n0001-02 PSL* ), Almerys (industrial chair Data Science and\nSecurity), Facebook AI Research (Doctoral research contract),\nMicrosoft Research (joint MSR-INRIA center) and a Google\nAward Grant.\n6. References\n[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., “Deep\nneural networks for acoustic modeling in speech recognition: The\nshared views of four research groups,” IEEE Signal Processing\nMagazine, vol. 29, no. 6, pp. 82–97, 2012.\n[2] G. Adda, S. St¨uker, M. Adda-Decker, O. Ambouroue, L. Be-\nsacier, D. Blachon, H. Bonneau-Maynard, P. Godard, F. Ham-\nlaoui, D. Idiatov et al., “Breaking the unwritten language barrier:\nThe bulb project,” Procedia Computer Science, vol. 81, pp. 8–14,\n2016.\n[3] M. Versteegh, R. Thiolliere, T. Schatz, X. N. Cao, X. Anguera,\nA. Jansen, and E. Dupoux, “The zero resource speech challenge\n2015,” in Sixteenth Annual Conference of the International Speech\nCommunication Association, 2015.\n[4] H. Kamper, M. Elsner, A. Jansen, and S. Goldwater, “Unsuper-\nvised neural network based feature extraction using weak top-\ndown constraints,” in Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on. IEEE, 2015,\npp. 5818–5822.\n[5] D. Renshaw, H. Kamper, A. Jansen, and S. Goldwater, “A com-\nparison of neural network methods for unsupervised representa-\ntion learning on the zero resource speech challenge,” in Sixteenth\nAnnual Conference of the International Speech Communication\nAssociation, 2015.\n[6] R. Thiolliere, E. Dunbar, G. Synnaeve, M. Versteegh, and\nE. Dupoux, “A hybrid dynamic time warping-deep neural network\narchitecture for unsupervised acoustic modeling,” in Sixteenth An-\nnual Conference of the International Speech Communication As-\nsociation, 2015.\n[7] N. Zeghidour, G. Synnaeve, M. Versteegh, and E. Dupoux, “A\ndeep scattering spectrumdeep siamese network pipeline for un-\nsupervised acoustic modeling,” in Acoustics, Speech and Signal\nProcessing (ICASSP), 2016 IEEE International Conference on.\nIEEE, 2016, pp. 4965–4969.\n[8] G. Synnaeve, T. Schatz, and E. Dupoux, “Phonetics embedding\nlearning with side information,” in Spoken Language Technology\nWorkshop (SLT), 2014 IEEE.\nIEEE, 2014, pp. 106–111.\n[9] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their com-\npositionality,” in Advances in neural information processing sys-\ntems, 2013, pp. 3111–3119.\n[10] O. Levy, Y. Goldberg, and I. Dagan, “Improving distributional\nsimilarity with lessons learned from word embeddings,” Transac-\ntions of the Association for Computational Linguistics, vol. 3, pp.\n211–225, 2015.\n[11] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,\nZ. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differ-\nentiation in pytorch,” in NIPS-W, 2017.\n[12] M. A. Pitt, K. Johnson, E. Hume, S. Kiesling, and W. Raymond,\n“The buckeye corpus of conversational speech: labeling conven-\ntions and a test of transcriber reliability,” Speech Communication,\nvol. 45, no. 1, pp. 89–95, 2005.\n[13] A. Jansen, K. Church, and H. Hermansky, “Towards spoken term\ndiscovery at scale with zero resources,” in Eleventh Annual Con-\nference of the International Speech Communication Association,\n2010.\n[14] J. Bromley, I. Guyon, Y. LeCun, E. S¨ackinger, and R. Shah, “Sig-\nnature veriﬁcation using a” siamese” time delay neural network,”\nin Advances in Neural Information Processing Systems, 1994, pp.\n737–744.\n[15] H. Sakoe and S. Chiba, “Dynamic programming algorithm op-\ntimization for spoken word recognition,” IEEE transactions on\nacoustics, speech, and signal processing, vol. 26, no. 1, pp. 43–\n49, 1978.\n[16] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in Interna-\ntional conference on machine learning, 2015, pp. 448–456.\n[17] D. P. Kingma and J. Ba, “Adam:\nA method for stochastic\noptimization,”\nCoRR,\nvol.\nabs/1412.6980,\n2014.\n[Online].\nAvailable: http://arxiv.org/abs/1412.6980\n[18] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity met-\nric discriminatively, with application to face veriﬁcation,” in Com-\nputer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE\nComputer Society Conference on, vol. 1.\nIEEE, 2005, pp. 539–\n546.\n[19] G. K. Zipf, “The psycho-biology of language.” 1935.\n[20] T. Schatz, V. Peddinti, F. Bach, A. Jansen, H. Hermansky, and\nE. Dupoux, “Evaluating speech features with the minimal-pair\nabx task: Analysis of the classical mfc/plp pipeline,” in INTER-\nSPEECH 2013: 14th Annual Conference of the International\nSpeech Communication Association, 2013, pp. 1–5.\n[21] T. Schatz, V. Peddinti, X.-N. Cao, F. Bach, H. Hermansky, and\nE. Dupoux, “Evaluating speech features with the minimal-pair\nabx task (ii): Resistance to noise,” in Fifteenth Annual Conference\nof the International Speech Communication Association, 2014.\n[22] N. J. De Vries, M. H. Davel, J. Badenhorst, W. D. Basson,\nF. De Wet, E. Barnard, and A. De Waal, “A smartphone-based\nasr data collection tool for under-resourced languages,” Speech\ncommunication, vol. 56, pp. 119–131, 2014.\n[23] H. Chen, C.-C. Leung, L. Xie, B. Ma, and H. Li, “Parallel in-\nference of dirichlet process gaussian mixture models for unsuper-\nvised acoustic modeling: A feasibility study,” in Sixteenth Annual\nConference of the International Speech Communication Associa-\ntion, 2015.\n[24] M. Heck, S. Sakti, and S. Nakamura, “Unsupervised linear dis-\ncriminant analysis for supporting dpgmm clustering in the zero\nresource scenario,” Procedia Computer Science, vol. 81, pp. 73–\n79, 2016.\n[25] B. Ludusan, M. Versteegh, A. Jansen, G. Gravier, X.-N. Cao,\nM. Johnson, E. Dupoux et al., “Bridging the gap between speech\ntechnology and natural language processing: an evaluation tool-\nbox for term discovery systems,” 2014.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2018-04-30",
  "updated": "2018-08-23"
}