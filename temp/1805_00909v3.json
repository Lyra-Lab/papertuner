{
  "id": "http://arxiv.org/abs/1805.00909v3",
  "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review",
  "authors": [
    "Sergey Levine"
  ],
  "abstract": "The framework of reinforcement learning or optimal control provides a\nmathematical formalization of intelligent decision making that is powerful and\nbroadly applicable. While the general form of the reinforcement learning\nproblem enables effective reasoning about uncertainty, the connection between\nreinforcement learning and inference in probabilistic models is not immediately\nobvious. However, such a connection has considerable value when it comes to\nalgorithm design: formalizing a problem as probabilistic inference in principle\nallows us to bring to bear a wide array of approximate inference tools, extend\nthe model in flexible and powerful ways, and reason about compositionality and\npartial observability. In this article, we will discuss how a generalization of\nthe reinforcement learning or optimal control problem, which is sometimes\ntermed maximum entropy reinforcement learning, is equivalent to exact\nprobabilistic inference in the case of deterministic dynamics, and variational\ninference in the case of stochastic dynamics. We will present a detailed\nderivation of this framework, overview prior work that has drawn on this and\nrelated ideas to propose new reinforcement learning and control algorithms, and\ndescribe perspectives on future research.",
  "text": "arXiv:1805.00909v3  [cs.LG]  20 May 2018\nReinforcement Learning and Control as Probabilistic\nInference: Tutorial and Review\nSergey Levine\nUC Berkeley\nsvlevine@eecs.berkeley.edu\nAbstract\nThe framework of reinforcement learning or optimal control provides a mathe-\nmatical formalization of intelligent decision making that is powerful and broadly\napplicable. While the general form of the reinforcement learning problem enables\neffective reasoning about uncertainty, the connection between reinforcement learn-\ning and inference in probabilistic models is not immediately obvious. However,\nsuch a connection has considerable value when it comes to algorithm design: for-\nmalizing a problem as probabilistic inference in principle allows us to bring to\nbear a wide array of approximate inference tools, extend the model in ﬂexible and\npowerful ways, and reason about compositionality and partial observability. In\nthis article, we will discuss how a generalization of the reinforcement learning\nor optimal control problem, which is sometimes termed maximum entropy rein-\nforcement learning, is equivalent to exact probabilistic inference in the case of\ndeterministic dynamics, and variational inference in the case of stochastic dynam-\nics. We will present a detailed derivation of this framework, overview prior work\nthat has drawn on this and related ideas to propose new reinforcement learning\nand control algorithms, and describe perspectives on future research.\n1\nIntroduction\nProbabilistic graphical models (PGMs) offer a broadly applicable and useful toolbox for the machine\nlearning researcher (Koller and Friedman, 2009): by couching the entirety of the learning problem\nin the parlance of probability theory, they provide a consistent and ﬂexible framework to devise prin-\ncipled objectives, set up models that reﬂect the causal structure in the world, and allow a common\nset of inference methods to be deployed against a broad range of problem domains. Indeed, if a\nparticular learning problem can be set up as a probabilistic graphical model, this can often serve as\nthe ﬁrst and most important step to solving it. Crucially, in the framework of PGMs, it is sufﬁcient\nto write down the model and pose the question, and the objectives for learning and inference emerge\nautomatically.\nConventionally, decision making problems formalized as reinforcement learning or optimal control\nhave been cast into a framework that aims to generalize probabilistic models by augmenting them\nwith utilities or rewards, where the reward function is viewed as an extrinsic signal. In this view,\ndetermining an optimal course of action (a plan) or an optimal decision-making strategy (a policy)\nis a fundamentally distinct type of problem than probabilistic inference, although the underlying\ndynamical system might still be described by a probabilistic graphical model. In this article, we\ninstead derive an alterate view of decision making, reinforcement learning, and optimal control,\nwhere the decision making problem is simply an inference problem in a particular type of graphical\nmodel. Formalizing decision making as inference in probabilistic graphical models can in principle\nallow us to to bring to bear a wide array of approximate inference tools, extend the model in ﬂexible\nand powerful ways, and reason about compositionality and partial observability.\nSpeciﬁcally, we will discuss how a generalization of the reinforcement learning or optimal control\nproblem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to ex-\nact probabilistic inference in the case of deterministic dynamics, and variational inference in the\ncase of stochastic dynamics. This observation is not a new one, and the connection between proba-\nbilistic inference and control has been explored in the literature under a variety of names, including\nthe Kalman duality (Todorov, 2008), maximum entropy reinforcement learning (Ziebart, 2010), KL-\ndivergence control (Kappen et al., 2012; Kappen, 2011), and stochastic optimal control (Toussaint,\n2009). While the speciﬁc derivations the differ, the basic underlying framework and optimization\nobjective are the same. All of these methods involve formulating control or reinforcement learning\nas a PGM, either explicitly or implicitly, and then deploying learning and inference methods from\nthe PGM literature to solve the resulting inference and learning problems.\nFormulating reinforcement learning and decision making as inference provides a number of other\nappealing tools: a natural exploration strategy based on entropy maximization, effective tools for\ninverse reinforcement learning, and the ability to deploy powerful approximate inference algorithms\nto solve reinforcement learning problems. Furthermore, the connection between probabilistic infer-\nence and control provides an appealing probabilistic interpretation for the meaning of the reward\nfunction, and its effect on the optimal policy. The design of reward or cost functions in reinforce-\nment learning is oftentimes as much art as science, and the choice of reward often blurs the line\nbetween algorithm and objective, with task-speciﬁc heuristics and task objectives combined into a\nsingle reward. In the control as inference framework, the reward induces a distribution over random\nvariables, and the optimal policy aims to explicitly match a probability distribution deﬁned by the\nreward and system dynamics, which may in future work suggest a way to systematize reward design.\nThis article will present the probabilistic model that can be used to embed a maximum entropy gener-\nalization of control or reinforcement learning into the framework of PGMs, describe how to perform\ninference in this model – exactly in the case of deterministic dynamics, or via structured variational\ninference in the case of stochastic dynamics, – and discuss how approximate methods based on\nfunction approximation ﬁt within this framework. Although the particular variational inference in-\nterpretation of control differs somewhat from the presentation in prior work, the goal of this article is\nnot to propose a fundamentally novel way of viewing the connection between control and inference.\nRather, it is to provide a uniﬁed treatment of the topic in a self-contained and accessible tutorial for-\nmat, and to connect this framework to recent research in reinforcement learning, including recently\nproposed deep reinforcement learning algorithms. In addition, this article presents a review of the\nrecent reinforcement learning literature that relates to this view of control as probabilistic inference,\nand offers some perspectives on future research directions.\nThe basic graphical model for control will be presented in Section 2, variational inference for\nstochastic dynamics will be discussed in Section 3, approximate methods based on function ap-\nproximation, including deep reinforcement learning, will be discussed in Section 4, and a survey\nand review of recent literature will be presented in Section 5. Finally, we will discuss perspectives\non future research directions in Section 6.\n2\nA Graphical Model for Control as Inference\nIn this section, we will present the basic graphical model that allows us to embed control into the\nframework of PGMs, and discuss how this framework can be used to derive variants of several\nstandard reinforcement learning and dynamic programming approaches. The PGM presented in this\nsection corresponds to a generalization of the standard reinforcement learning problem, where the\nRL objective is augmented with an entropy term. The magnitude of the reward function trades off\nbetween reward maximization and entropy maximization, allowing the original RL problem to be\nrecovered in the limit of inﬁnitely large rewards. We will begin by deﬁning notation, then deﬁning\nthe graphical model, and then presenting several inference methods and describing how they relate to\nstandard algorithms in reinforcement learning and dynamic programming. Finally, we will discuss\na few limitations of this method and motivate the variational approach in Section 3.\n2.1\nThe Decision Making Problem and Terminology\nFirst, we will introduce the notation we will use for the standard optimal control or reinforcement\nlearning formulation. We will use s ∈S to denote states and a ∈A to denote actions, which may\neach be discrete or continuous. States evolve according to the stochastic dynamics p(st+1|st, at),\nwhich are in general unknown. We will follow a discrete-time ﬁnite-horizon derivation, with horizon\nT , and omit discount factors for now. A discount γ can be readily incorporated into this framework\nsimply by modifying the transition dynamics, such that any action produces a transition into an\nabsorbing state with probability 1 −γ, and all standard transition probabilities are multiplied by γ.\n2\na1\na2\na3\na4\ns1\ns2\ns3\ns4\n(a) graphical model with states and actions\na1\na2\na3\na4\ns1\ns2\ns3\ns4\nO1\nO2\nO3\nO4\n(b) graphical model with optimality variables\nFigure 1: The graphical model for control as inference. We begin by laying out the states and actions, which\nform the backbone of the model (a). In order to embed a control problem into this model, we need to add\nnodes that depend on the reward (b). These “optimality variables” correspond to observations in a HMM-\nstyle framework: we condition on the optimality variables being true, and then infer the most probable action\nsequence or most probable action distributions.\nA task in this framework can be deﬁned by a reward function r(st, at). Solving a task typically\ninvolves recovering a policy p(at|st, θ), which speciﬁes a distribution over actions conditioned on\nthe state parameterized by some parameter vector θ. A standard reinforcement learning policy search\nproblem is then given by the following maximization:\nθ⋆= arg max\nθ\nT\nX\nt=1\nE(st,at)∼p(st,at|θ)[r(st, at)].\n(1)\nThis optimization problem aims to ﬁnd a vector of policy parameters θ that maximize the total\nexpected reward P\nt r(st, at) of the policy. The expectation is taken under the policy’s trajectory\ndistribution p(τ), given by\np(τ) = p(s1, at, . . . , sT, aT |θ) = p(s1)\nT\nY\nt=1\np(at|st, θ)p(st+1|st, at).\n(2)\nFor conciseness, it is common to denote the action conditional p(at|st, θ) as πθ(at|st), to emphasize\nthat it is given by a parameterized policy with parameters θ. These parameters might correspond,\nfor example, to the weights in a neural network. However, we could just as well embed a standard\nplanning problem in this formulation, by letting θ denote a sequence of actions in an open-loop plan.\nHaving formulated the decision making problem in this way, the next question we have to ask to\nderive the control as inference framework is: how can we formulate a probabilistic graphical model\nsuch that the most probable trajectory corresponds to the trajectory from the optimal policy? Or,\nequivalently, how can we formulate a probabilistic graphical model such that inferring the posterior\naction conditional p(at|st, θ) gives us the optimal policy?\n2.2\nThe Graphical Model\nTo embed the control problem into a graphical model, we can begin simply by modeling the rela-\ntionship between states, actions, and next states. This relationship is simple, and corresponds to a\ngraphical model with factors of the form p(st+1|st, at), as shown in Figure 1 (a). However, this\ngraphical model is insufﬁcient for solving control problems, because it has no notion of rewards or\ncosts. We therefore have to introduce an additional variable into this model, which we will denote\nOt. This additional variable is a binary random variable, where Ot = 1 denotes that time step t is\noptimal, and Ot = 0 denotes that it is not optimal. We will choose the distribution over this variable\nto be given by the following equation:\np(Ot = 1|st, at) = exp(r(st, at)).\n(3)\nThe graphical model with these additional variables is summarized in Figure 1 (b). While this might\nat ﬁrst seem like a peculiar and arbitrary choice, it leads to a very natural posterior distribution over\n3\nactions when we condition on Ot = 1 for all t ∈{1, . . . , T }:\np(τ|o1:T ) ∝p(τ, o1:T ) = p(s1)\nT\nY\nt=1\np(Ot = 1|st, at)p(st+1|st, at)\n= p(s1)\nT\nY\nt=1\nexp(r(st, at))p(st+1|st, at)\n=\n\"\np(s1)\nT\nY\nt=1\np(st+1|st, at)\n#\nexp\n T\nX\nt=1\nr(st, at)\n!\n.\n(4)\nThat is, the probability of observing a given trajectory is given by the product between its probability\nto occur according to the dynamics (the term in square brackets on the last line), and the exponential\nof the total reward along that trajectory. It is most straightforward to understand this equation in\nsystems with deterministic dynamics, where the ﬁrst term is a constant for all trajectories that are\ndynamically feasible. In this case, the trajectory with the highest reward has the highest probability,\nand trajectories with lower reward have exponentially lower probability. If we would like to plan\nfor an optimal action sequence starting from some initial state s1, we can condition on o1:T and\nchoose p(s1) = δ(s1), in which case maximum a posteriori inference corresponds to a kind of\nplanning problem. It is easy to see that this exactly corresponds to standard planning or trajectory\noptimization in the case where the dynamics are deterministic, in which case Equation (4) reduces\nto\np(τ|o1:T ) ∝1[p(τ) ̸= 0] exp\n T\nX\nt=1\nr(st, at)\n!\n.\n(5)\nHere, the indicator function simply indicates that the trajectory τ is dynamically consistent (meaning\nthat p(st+1|st, at) ̸= 0) and the initial state is correct. The case of stochastic dynamics poses some\nchallenges, and will be discussed in detail in Section 3. However, even under deterministic dynamics,\nwe are often interested in recovering a policy rather than a plan. In this PGM, the optimal policy\ncan be written as p(at|st, Ot:T = 1) (we will drop = 1 in the remainder of the derivation for\nconciseness). This distribution is somewhat analogous to p(at|st, θ⋆) in the previous section, with\ntwo major differences: ﬁrst, it is independent of the parameterization θ, and second, we will see\nlater that it optimizes an objective that is slightly different from the standard reinforcement learning\nobjective in Equation (1).\n2.3\nPolicy Search as Probabilistic Inference\nWe can recover the optimal policy p(at|st, Ot:T ) using a standard sum-product inference algorithm,\nanalogously to inference in HMM-style dynamic Bayesian networks. As we will see in this section,\nit is sufﬁcient to compute backward messages of the form\nβt(st, at) = p(Ot:T |st, at).\nThese messages have a natural interpretation: they denote the probability that a trajectory can be\noptimal for time steps from t to T if it begins in state st with the action at.1 Slightly overloading\nthe notation, we will also introduce the message\nβt(st) = p(Ot:T |st).\nThese messages denote the probability that the trajectory from t to T is optimal if it begins in state st.\nWe can recover the state-only message from the state-action message by integrating out the action:\nβt(st) = p(Ot:T |st) =\nZ\nA\np(Ot:T |st, at)p(at|st)dat =\nZ\nA\nβt(st, at)p(at|st)dat.\nThe factor p(at|st) is the action prior. Note that it is not conditioned on O1:T in any way: it does\nnot denote the probability of an optimal action, but simply the prior probability of actions. The\nPGM in Figure 1 doesn’t actually contain this factor, and we can assume that p(at|st) =\n1\n|A| for\nsimplicity – that is, it is a constant corresponding to a uniform distribution over the set of actions.\n1Note that βt(st, at) is not a probability density over st, at, but rather the probability of Ot:T = 1.\n4\nWe will see later that this assumption does not actually introduce any loss of generality, because any\nnon-uniform p(at|st) can be incorporated instead into p(Ot|st, at) via the reward function.\nThe recursive message passing algorithm for computing βt(st, at) proceeds from the last time step\nt = T backward through time to t = 1. In the base case, we note that p(OT |sT , aT ) is simply\nproportional to exp(r(sT , aT )), since there is only one factor to consider. The recursive case is then\ngiven as following:\nβt(st, at) = p(Ot:T |st, at) =\nZ\nS\nβt+1(st+1)p(st+1|st, at)p(Ot|st, at)dst+1.\n(6)\nFrom these backward messages, we can then derive the optimal policy p(at|st, O1:T ). First, note\nthat O1:(t−1) is conditionally independent of at given st, which means that p(at|st, O1:T ) =\np(at|st, Ot:T ), and we can disregard the past when considering the current action distribution. This\nmakes intuitive sense: in a Markovian system, the optimal action does not depend on the past. From\nthis, we can easily recover the optimal action distribution using the two backward messages:\np(at|st, Ot:T ) = p(st, at|Ot:T )\np(st|Ot:T )\n= p(Ot:T |st, at)p(at|st)p(st)\np(Ot:T |st)p(st)\n∝p(Ot:T |st, at)\np(Ot:T |st)\n= βt(st, at)\nβt(st)\n,\nwhere the order of conditioning in the third step is ﬂipped by using Bayes’ rule, and cancelling the\nfactor of p(Ot:T ) that appears in both the numerator and denominator. The term p(at|st) disappears,\nsince we previously assumed it was a uniform distribution.\nThis derivation provides us with a solution, but perhaps not as much of the intuition. The intuition\ncan be recovered by considering what these equations are doing in log space. To that end, we will\nintroduce the log-space messages as\nQ(st, at) = log βt(st, at)\nV (st) = log βt(st).\nThe use of Q and V here is not accidental: the log-space messages correspond to “soft” variants\nof the state and state-action value functions. First, consider the marginalization over actions in\nlog-space:\nV (st) = log\nZ\nA\nexp(Q(st, at))dat.\nWhen the values of Q(st, at) are large, the above equation resembles a hard maximum over at. That\nis, for large Q(st, at),\nV (st) = log\nZ\nA\nexp(Q(st, at))dat ≈max\nat Q(st, at).\nFor smaller values of Q(st, at), the maximum is soft. Hence, we can refer to V and Q as soft\nvalue functions and Q-functions, respectively. We can also consider the backup in Equation (6) in\nlog-space. In the case of deterministic dynamics, this backup is given by\nQ(st, at) = r(st, at) + V (st+1),\nwhich exactly corresponds to the Bellman backup. However, when the dynamics are stochastic, the\nbackup is given by\nQ(st, at) = r(st, at) + log Est+1∼p(st+1|st,at)[exp(V (st+1))].\n(7)\nThis backup is peculiar, since it does not consider the expected value at the next state, but a “soft\nmax” over the next expected value. Intuitively, this produces Q-functions that are optimistic: if\namong the possible outcomes for the next state there is one outcome with a very high value, it\nwill dominate the backup, even when there are other possible states that might be likely and have\nextremely low value. This creates risk seeking behavior: if an agent behaves according to this Q-\nfunction, it might take actions that have extremely high risk, so long as they have some non-zero\nprobability of a high reward. Clearly, this behavior is not desirable in many cases, and the standard\nPGM described in this section is often not well suited to stochastic dynamics. In Section 3, we will\ndescribe a simple modiﬁcation that makes the backup correspond to the soft Bellman backup in the\ncase of stochastic dynamics also, by using the framework of variational inference.\n5\n2.4\nWhich Objective does This Inference Procedure Optimize?\nIn the previous section, we derived an inference procedure that can be used to obtain the distribution\nover actions conditioned on all of the optimality variables, p(at|st, O1:T ). But which objective does\nthis policy actually optimize? Recall that the overall distribution is given by\np(τ) =\n\"\np(s1)\nT\nY\nt=1\np(st+1|st, at)\n#\nexp\n T\nX\nt=1\nr(st, at)\n!\n,\n(8)\nwhich we can simplify in the case of deterministic dynamics into Equation (5). In this case, the\nconditional distributions p(at|st, O1:T ) are simply obtained by marginalizing the full trajectory dis-\ntribution and conditioning the policy at each time step on st. We can adopt an optimization-based\napproximate inference approach to this problem, in which case the goal is to ﬁt an approximation\nπ(at|st) such that the trajectory distribution\nˆp(τ) ∝1[p(τ) ̸= 0]\nT\nY\nt=1\nπ(at|st)\nmatches the distribution in Equation (5). In the case of exact inference, as derived in the previous sec-\ntion, the match is exact, which means that DKL(ˆp(τ)∥p(τ)) = 0, where DKL is the KL-divergence.\nWe can therefore view the inference process as minimizing DKL(ˆp(τ)∥p(τ)), which is given by\nDKL(ˆp(τ)∥p(τ)) = −Eτ∼ˆp(τ)[log p(τ) −log ˆp(τ)].\nNegating both sides and substituting in the equations for p(τ) and ˆp(τ), we get\n−DKL(ˆp(τ)∥p(τ)) = Eτ∼ˆp(τ)\n\"\nlog p(s1) +\nT\nX\nt=1\n(log p(st+1|st, at) + r(st, at)) −\nlog p(s1) −\nT\nX\nt=1\n(log p(st+1|st, at) + log π(at|st))\n#\n= Eτ∼ˆp(τ)\n\" T\nX\nt=1\nr(st, at) −log π(at|st)\n#\n=\nT\nX\nt=1\nE(st,at)∼ˆp(st,at))[r(st, at) −log π(at|st)]\n=\nT\nX\nt=1\nE(st,at)∼ˆp(st,at))[r(st, at)] + Est∼ˆp(st)[H(π(at|st))].\nTherefore, minimizing the KL-divergence corresponds to maximizing the expected reward and the\nexpected conditional entropy, in contrast to the standard control objective in Equation (1), which\nonly maximizes reward. Hence, this type of control objective is sometimes referred to as maximum\nentropy reinforcement learning or maximum entropy control.\nHowever, that in the case of stochastic dynamics, the solution is not quite so simple. Under stochastic\ndynamics, the optimized distribution is given by\nˆp(τ) = p(s1|O1:T )\nT\nY\nt=1\np(st+1|st, at, O1:T )p(at|st, O1:T ),\n(9)\nwhere the initial state distribution and the dynamics are also conditioned on optimality. As a result\nof this, the dynamics and initial state terms in the KL-divergence do not cancel, and the objective\ndoes not have the simple entropy maximizing form derived above.2 We can still fall back on the\noriginal KL-divergence minimization at the trajectory level, and write the objective as\n−DKL(ˆp(τ)∥p(τ)) = Eτ∼ˆp(τ)\n\"\nlog p(s1) +\nT\nX\nt=1\nr(st, at) + log p(st+1|st, at)\n#\n+ H(ˆp(τ)). (10)\n2In the deterministic case, we know that p(st+1|st, at, O1:T ) = p(st+1|st, at), since exactly one transition\nis ever possible.\n6\nHowever, because of the log p(st+1|st, at) terms, this objective is difﬁcult to optimize in a model-\nfree setting. As discussed in the previous section, it also results in an optimistic policy that assumes a\ndegree of control over the dynamics that is unrealistic in most control problems. In Section 3, we will\nderive a variational inference procedure that does reduce to the convenient objective in Equation (9)\neven in the case of stochastic dynamics, and in the process also addresses the risk-seeking behavior\ndiscussed in Section 2.3.\n2.5\nAlternative Model Formulations\nIt’s worth pointing out that the deﬁnition of p(Ot = 1|st, at) in Equation (3) requires an additional\nassumption, which is that the rewards r(st, at) are always negative.3 Otherwise, we end up with a\nnegative probability for p(Ot = 0|st, at). However, this assumption is not actually required: it’s\nquite possible to instead deﬁne the graphical model with an undirected factor on (st, at, Ot), with an\nunnormalized potential given by Φt(st, at, Ot) = 1Ot=1 exp(r(st, at)). The potential for Ot = 0\ndoesn’t matter, since we always condition on Ot = 1. This leads to the same exact inference proce-\ndure as the one we described above, but without the negative reward assumption. Once we are con-\ntent to working with undirected graphical models, we can even remove the variables Ot completely,\nand simply add an undirected factor on (st, at) with the potential Φt(st, at) = exp(r(st, at)),\nwhich is mathematically equivalent. This is the conditional random ﬁeld formulation described\nby Ziebart (Ziebart, 2010). The analysis and inference methods in this model are identical to the\nones for the directed model with explicit optimality variables Ot, and the particular choice of model\nis simply a notational convenience. We will use the variables Ot in this article for clarity of deriva-\ntion and stay within the directed graphical model framework, but all derivations are straightforward\nto reproduce in the conditional random ﬁeld formulation.\nAnother common modiﬁcation to this framework is to incorporate an explicit temperature α into the\nCPD for Ot, such that p(Ot|st, bat) = exp( 1\nαr(st, at)). The corresponding maximum entropy ob-\njective can then be written equivalently as the expectation of the (original) reward, with an additional\nmultiplier of α on the entropy term. This provides a natural mechanism to interpolate between en-\ntropy maximization and standard optimal control or RL: as α →0, the optimal solution approaches\nthe standard optimal control solution. Note that this does not actually increase the generality of the\nmethod, since the constant 1\nα can always be multiplied into the reward, but making this temperature\nconstant explicit can help to illuminate the connection between standard and entropy maximizing\noptimal control.\nFinally, it is worth remarking again on the role of discount factors: it is very common in reinforce-\nment learning to use a Bellman backup of the form\nQ(st, at) ←r(st, at) + γEst+1∼p(st+1|st,at)[V (st+1)],\nwhere γ ∈(0, 1] is a discount factor. This allows for learning value functions in inﬁnite-horizon\nsettings, where the backup would otherwise be non-convergent for γ = 1, and reduces variance\nfor Monte Carlo advantage estimators in policy gradient algorithms (Schulman et al., 2016). The\ndiscount factor can be viewed a simple redeﬁnition of the system dynamics. If the initial dynamics\nare given by p(st+1|st, at), adding a discount factor is equivalent to undiscounted value ﬁtting under\nthe modiﬁed dynamics ¯p(st+1|st, at) = γp(st+1|st, at), where there is an additional transition with\nprobability 1 −γ, regardless of action, into an absorbing state with reward zero. We will omit γ\nfrom the derivations in this article, but it can be inserted trivially in all cases simply by modifying\nthe (soft) Bellman backups in any place where the expectation over p(st+1|st, at) occurs, such as\nEquation (7) previously or Equation (15) in the next section.\n3\nVariational Inference and Stochastic Dynamics\nThe problematic nature of the maximum entropy framework in the case of stochastic dynamics, dis-\ncussed in Section 2.3 and Section 2.4, in essence amounts to an assumption that the agent is allowed\nto control both its actions and the dynamics of the system in order to produce optimal trajectories,\nbut its authority over the dynamics is penalized based on deviation from the true dynamics. Hence,\nthe log p(st+1|st, at) terms in Equation (10) can be factored out of the equations, producing additive\n3This assumption is not actually very strong: if we assume the reward is bounded above, we can always\nconstruct an exactly equivalent reward simply by subtracting the maximum reward.\n7\nterms that corresponds to the cross-entropy between the posterior dynamics p(st+1|st, at, O1:T ) and\nthe true dynamics p(st+1|st, at). This explains the risk-seeking nature of the method discussed in\nSection 2.3: if the agent is allowed to inﬂuence its dynamics, even a little bit, it would reasonably\nchoose to remove unlikely but extremely bad outcomes of risky actions.\nOf course, in practical reinforcement learning and control problems, such manipulation of system\ndynamics is not possible, and the resulting policies can lead to disastrously bad outcomes. We can\ncorrect this issue by modifying the inference procedure. In this section, we will derive this correction\nby freezing the system dynamics, writing down the corresponding maximum entropy objective, and\nderiving a dynamic programming procedure for optimizing it. Then we will show that this procedure\namounts to a direct application of structured variational inference.\n3.1\nMaximum Entropy Reinforcement Learning with Fixed Dynamics\nThe issue discussed in Section 2.4 for stochastic dynamics can brieﬂy be summarized as following:\nsince the posterior dynamics distribution p(st+1|st, at, O1:T ) does not necessarily match the true\ndynamics p(st+1|st, at), the agent assumes that it can inﬂuence the dynamics to a limited extent.\nA simple ﬁx to this issue is to explicitly disallow this control, by forcing the posterior dynamics\nand initial state distributions to match p(st+1|st, at) and p(s1), respectively. Then, the optimized\ntrajectory distribution is given simply by\nˆp(τ) = p(s1)\nT\nY\nt=1\np(st+1|st, at)π(at|st),\nand the same derivation as the one presented in Section 2.4 for the deterministic case results in the\nfollowing objective:\n−DKL(ˆp(τ)∥p(τ)) =\nT\nX\nt=1\nE(st,at)∼ˆp(st,at))[r(st, at) + H(π(at|st))].\n(11)\nThat is, the objective is still to maximize reward and entropy, but now under stochastic transition\ndynamics. To optimize this objective, we can compute backward messages like we did in Section 2.3.\nHowever, since we are now starting from the maximization of the objective in Equation (11), we have\nto derive these backward messages from an optimization perspective as a dynamic programming\nalgorithm. As before, we will begin with the base case of optimizing π(aT |sT ), which maximizes\nE(sT ,aT )∼ˆp(sT ,aT )[r(sT , aT ) −log π(aT |sT )] =\nEsT ∼ˆp(sT )\n\u0014\n−DKL\n\u0012\nπ(aT |sT )∥\n1\nexp(V (sT )) exp(r(sT , aT ))\n\u0013\n+ V (sT )\n\u0015\n,\n(12)\nwhere the equality holds from the deﬁnition of KL-divergence, and exp(V (sT )) is the normalizing\nconstant for exp(r(sT , aT )) with respect to aT where V (sT ) = log\nR\nA exp(r(sT , aT ))daT , which\nis the same soft maximization as in Section 2.3. Since we know that the KL-divergence is minimized\nwhen the two arguments represent the same distribution, the optimal policy is given by\nπ(aT |sT ) = exp (r(sT , aT ) −V (sT )) ,\n(13)\nThe recursive case can then computed as following: for a given time step t, π(at|st) must maximize\ntwo terms:\nE(st,at)∼ˆp(st,at)[r(st, at) −log π(at|st)] + E(st,at)∼ˆp(st,at)[Est+1∼p(st+1|st,at)[V (st+1)]].\n(14)\nThe ﬁrst term follows directly from the objective in Equation (11), while the second term represents\nthe contribution of π(at|st) to the expectations of all subsequent time steps. The second term de-\nserves a more in-depth derivation. First, consider the base case: given the equation for π(aT |sT )\nin Equation (13), we can evaluate the objective for the policy by directly substituting this equation\ninto Equation (12). Since the KL-divergence then evaluates to zero, we are left only with the V (sT )\nterm. In the recursive case, we note that we can rewrite the objective in Equation (14) as\nE(st,at)∼ˆp(st,at)[r(st, at) −log π(at|st)] + E(st,at)∼ˆp(st,at)[Est+1∼p(st+1|st,at)[V (st+1)]] =\nEst∼ˆp(st)\n\u0014\n−DKL\n\u0012\nπ(at|st)∥\n1\nexp(V (st)) exp(Q(st, at))\n\u0013\n+ V (st)\n\u0015\n,\n8\nwhere we now deﬁne\nQ(st, at) = r(st, at) + Est+1∼p(st+1|st,at)[V (st+1)]\n(15)\nV (st) = log\nZ\nA\nexp(Q(st, at))dat,\nwhich corresponds to a standard Bellman backup with a soft maximization for the value function.\nChoosing\nπ(at|st) = exp (Q(st, at) −V (st)) ,\n(16)\nwe again see that the KL-divergence evaluates to zero, leaving Est∼ˆp(st)[V (st)] as the only remain-\ning term in the objective for time step t, just like in the base case of t = T . This means that, if we ﬁx\nthe dynamics and initial state distribution, and only allow the policy to change, we recover a Bellman\nbackup operator that uses the expected value of the next state, rather than the optimistic estimate we\nsaw in Section 2.3 (compare Equation (15) to Equation (7)). While this provides a solution to the\npractical problem of risk-seeking policies, it is perhaps a bit unsatisfying in its divergence from the\nconvenient framework of probabilistic graphical models. In the next section, we will discuss how\nthis procedure amounts to a direct application of structured variational inference.\n3.2\nConnection to Structured Variational Inference\nOne way to interpret the optimization procedure in Section 3.1 is as a particular type of structured\nvariational inference. In structured variational inference, our goal is to approximate some distribu-\ntion p(y) with another, potentially simpler distribution q(y). Typically, q(y) is taken to be some\ntractable factorized distribution, such as a product of conditional distributions connected in a chain\nor tree, which lends itself to tractable exact inference. In our case, we aim to approximate p(τ),\ngiven by\np(τ) =\n\"\np(s1)\nT\nY\nt=1\np(st+1|st, at)\n#\nexp\n T\nX\nt=1\nr(st, at)\n!\n,\n(17)\nvia the distribution\nq(τ) = q(s1)\nT\nY\nt=1\nq(st+1|st, at)q(at|st).\n(18)\nIf we ﬁx q(s1) = p(s1) and q(st+1|st, at) = p(st+1|st, at), then q(τ) is exactly the distribution\nˆp(τ) from Section 3.1, which we’ve renamed here to q(τ) to emphasize the connection to structured\nvariational inference. Note that we’ve also renamed π(at|st) to q(at|st) for the same reason. In\nstructured variational inference, approximate inference is performed by optimizing the variational\nlower bound (also called the evidence lower bound). Recall that our evidence here is that Ot = 1\nfor all t ∈{1, . . ., T }, and the posterior is conditioned on the initial state s1. The variational lower\nbound is given by\nlog p(O1:T ) = log\nZ Z\np(O1:T , s1:T , a1:T )ds1:T da1:T\n= log\nZ Z\np(O1:T , s1:T , a1:T )q(s1:T , a1:T )\nq(s1:T , a1:T )ds1:T da1:T\n= log E(s1:T ,a1:T )∼q(s1:T ,a1:T )\n\u0014p(O1:T , s1:T , a1:T )\nq(s1:T , a1:T )\n\u0015\n≥E(s1:T ,a1:T )∼q(s1:T ,a1:T ) [log p(O1:T , s1:T , a1:T ) −log q(s1:T , a1:T )] ,\nwhere the inequality on the last line is obtained via Jensen’s inequality. Substituting the deﬁnitions\nof p(τ) and q(τ) from Equations (17) and (18), and noting the cancellation due to q(st+1|st, at) =\np(st+1|st, at), the bound reduces to\nlog p(O1:T ) ≥E(s1:T ,a1:T )∼q(s1:T ,a1:T )\n\" T\nX\nt=1\nr(st, at) −log q(at|st)\n#\n,\n(19)\nup to an additive constant. Optimizing this objective with respect to the policy q(at|st) corresponds\nexactly to the objective in Equation (11). Intuitively, this means that this objective attempts to ﬁnd\n9\nthe closest match to the maximum entropy trajectory distribution, subject to the constraint that the\nagent is only allowed to modify the policy, and not the dynamics. Note that this framework can\nalso easily accommodate any other structural constraints on the policy, including restriction to a\nparticular distribution class (e.g., conditional Gaussian, or a categorical distribution parameterized\nby a neural network), or restriction to partial observability, where the entire state st is not available\nas an input, but rather the policy only has access to some non-invertible function of the state.\n4\nApproximate Inference with Function Approximation\nWe saw in the discussion above that a dynamic programming backward algorithm with updates that\nresemble Bellman backups can recover “soft” analogues of the value function and Q-function in\nthe maximum entropy reinforcement learning framework, and the stochastic optimal policy can be\nrecovered from the Q-function and value function. In this section, we will discuss how practical al-\ngorithms for high-dimensional or continuous reinforcement learning problems can be derived from\nthis theoretical framework, with the use of function approximation. This will give rise to several pro-\ntotypical methods that mirror corresponding techniques in standard reinforcement learning: policy\ngradients, actor-critic algorithms, and Q-learning.\n4.1\nMaximum Entropy Policy Gradients\nOne approach to performing structured variational inference is to directly optimize the evidence\nlower bound with respect to the variational distribution (Koller and Friedman, 2009). This approach\ncan be directly applied to maximum entropy reinforcement learning. Note that the variational distri-\nbution consists of three terms: q(s1), q(st+1|st, at), and q(at|st). The ﬁrst two terms are ﬁxed to\np(s1) and p(st+1|st, at), respectively, leaving only q(at|st) to vary. We can parameterize this distri-\nbution with any expressive conditional, with parameters θ, and will therefore denote it as qθ(at|st).\nThe parameters could correspond, for example, to the weights in a deep neural network, which takes\nst as input and outputs the parameters of some distribution class. In the case of discrete actions, the\nnetwork could directly output the parameters of a categorical distribution (e.g., via a soft max oper-\nator). In the case of continuous actions, the network could output the parameters of an exponential\nfamily distribution, such as a Gaussian. In all cases, we can directly optimize the objective in Equa-\ntion (11) by estimating its gradient using samples. This gradient has a form that is nearly identical\nto the standard policy gradient (Williams, 1992), which we summarize here for completeness. First,\nlet us restate the objective as following:\nJ(θ) =\nT\nX\nt=1\nE(st,at)∼q(st,at) [r(st, at) −H(qθ(at|st))] .\nThe gradient is then given by\n∇θJ(θ) =\nT\nX\nt=1\n∇θE(st,at)∼q(st,at) [r(st, at) + H(qθ(at|st))]\n=\nT\nX\nt=1\nE(st,at)∼q(st,at)\n\"\n∇θ log qθ(at|st)\n T\nX\nt′=t\nr(st′, at′) −log qθ(at′|st′) −1\n!#\n=\nT\nX\nt=1\nE(st,at)∼q(st,at)\n\"\n∇θ log qθ(at|st)\n T\nX\nt′=t\nr(st′, at′) −log qθ(at′|st′) −b(st′)\n!#\n,\nwhere the second line follows from applying the likelihood ratio trick (Williams, 1992) and the\ndeﬁnition of entropy to obtain the log qθ(at′|st′) term. The −1 comes from the derivative of the\nentropy term. The last line follows by noting that the gradient estimator is invariant to additive state-\ndependent constants, and replacing −1 with a state-dependent baseline b(st′). The resulting policy\ngradient estimator exactly matches a standard policy gradient estimator, with the only modiﬁcation\nbeing the addition of the −log qθ(at′|st′) term to the reward at each time step t′. Intuitively, the\nreward of each action is modiﬁed by subtracting the log-probability of that action under the current\npolicy, which causes the policy to maximize entropy. This gradient estimator can be written more\n10\ncompactly as\n∇θJ(θ) =\nT\nX\nt=1\nE(st,at)∼q(st,at)\nh\n∇θ log qθ(at|st) ˆA(st, at)\ni\n,\nwhere ˆA(st, at) is an advantage estimator. Any standard advantage estimator, such as the GAE\nestimator (Schulman et al., 2016), can be used in place of the standard baselined Monte Carlo return\nabove. Again, the only necessary modiﬁcation is to add −log qθ(at′|st′) to the reward at each time\nstep t′. As with standard policy gradients, a practical implementation of this method estimates the\nexpectation by sampling trajectories from the current policy, and may be improved by following the\nnatural gradient direction.\n4.2\nMaximum Entropy Actor-Critic Algorithms\nInstead of directly differentiating the variational lower bound, we can adopt a message passing\napproach which, as we will see later, can produce lower-variance gradient estimates. First, note that\nwe can write down the following equation for the optimal target distribution for q(at|st):\nq⋆(at|st) = 1\nZ exp\n \nEq(s(t+1):T ,a(t+1):T |st,at)\n\" T\nX\nt′=t\nlog p(Ot′|st′, at′) −\nT\nX\nt′=t+1\nlog q(at′|st′)\n#!\n.\nThis is because conditioning on st makes the action at completely independent of all past states, but\nthe action still depends on all future states and actions. Note that the dynamics terms p(st+1|st, at)\nand q(st+1|st, at) do not appear in the above equation, since they perfectly cancel. We can simplify\nthe expectation above as follows:\nEq(s(t+1):T ,a(t+1):T |st,at)[log p(Ot:T |st:T , at:T )] =\nlog p(Ot|st, at) + Eq(st+1|st,at)\n\"\nE\n\"\nT\nX\nt′=t+1\nlog p(Ot′|st′, at′) −log q(at′|st′)\n##\n.\nIn this case, note that the inner expectation does not contain st or at, and therefore makes for a\nnatural representation for a message that can be sent from future states. We will denote this message\nV (st+1), since it will correspond to a soft value function:\nV (st) = E\n\"\nT\nX\nt′=t+1\nlog p(Ot′|st′, at′) −log q(at′|st′)\n#\n= Eq(at|st)[log p(Ot|st, at) −log q(at|st) + Eq(st+1|st,at[V (st+1)]].\nFor convenience, we can also deﬁne a Q-function as\nQ(st, at) = log p(Ot|st, at) + Eq(st+1|st,at)[V (st+1)],\nsuch that V (st) = Eq(at|st)[Q(st, at) −log q(at|st)], and the optimal policy is\nq⋆(at|st) =\nexp(Q(st, at))\nlog R\nA exp(Q(st, at))dat\n.\n(20)\nNote that, in this case, the value function and Q-function correspond to the values of the current\npolicy q(at|st), rather than the optimal value function and Q-function, as in the case of dynamic\nprogramming. However, at convergence, when q(at|st) = q⋆(at|st) for each t, we have\nV (st) = Eq(at|st)[Q(st, at) −log q(at|st)]\n= Eq(at|st)\n\u0014\nQ(st, at) −Q(st, at) + log\nZ\nA\nexp(Q(st, at))dat\n\u0015\n= log\nZ\nA\nexp(Q(st, at))dat,\n(21)\nwhich is the familiar soft maximum from Section 2.3. We now see that the optimal variational\ndistribution for q(at|st) can be computed by passing messages backward through time, and the\nmessages are given by V (st) and Q(st, at).\n11\nSo far, this derivation assumes that the policy and messages can be represented exactly. We can relax\nthe ﬁrst assumption in the same way as in the preceding section. We ﬁrst write down the variational\nlower bound for a single factor q(at|st) as following:\nmax\nq(at|st) Est∼q(st)\n\u0002\nEat∼q(at|st)[Q(st, at) −log q(at|st)]\n\u0003\n.\n(22)\nIt’s straightforward to show that this objective is simply the full variational lower bound, which is\ngiven by Eq(τ)[log p(τ) −log q(τ)], restricted to just the terms that include q(at|st). If we restrict\nthe class of policies q(at|st) so that they cannot represent q⋆(at|st) exactly, we can still optimize\nthe objective in Equation (22) by computing its gradient, which is given by\nEst∼q(st)\n\u0002\nEat∼q(at|st)[∇log q(at|st)(Q(st, at) −log q(at|st) −b(st))]\n\u0003\n,\nwhere b(st) is any state-dependent baseline. This gradient can computed using samples from q(τ)\nand, like the policy gradient in the previous section, is directly analogous to a classic likelihood ratio\npolicy gradient. The modiﬁcation lies in the use of the backward message Q(st, at) in place of the\nMonte Carlo advantage estimate. The algorithm therefore corresponds to an actor-critic algorithm,\nwhich generally provides lower variance gradient estimates.\nIn order to turn this into a practical algorithm, we must also be able to approximate the backward\nmessage Q(st, at) and V (st). A simple and straightforward approach is to represent them with pa-\nrameterized functions Qφ(st, at) and Vψ(st), with parameters φ and ψ, and optimize the parameters\nto minimize a squared error objectives\nE(φ) = E(st,at)∼q(st,at)\nh\u0000r(st, at) + Eq(st+1|st,at)[Vψ(st+1)] −Qφ(st, at)\n\u00012i\n(23)\nE(ψ) = Est∼q(st)\nh\u0000Eat∼q(at|st)[Qφ(st, at) −log q(at|st)] −Vψ(st, at)\n\u00012i\n.\nThis interpretation gives rise to a few interesting possibilities for maximum entropy actor-critic and\npolicy iteration algorithms. First, it suggests that it may be beneﬁcial to keep track of both V (st) and\nQ(st, at) networks. This is perfectly reasonable in a message passing framework, and in practice\nmight have many of the same beneﬁts as the use of a target network, where the updates to Q and\nV can be staggered or damped for stability. Second, it suggests that policy iteration or actor-critic\nmethods might be preferred (over, for example, direct Q-learning), since they explicitly handle both\napproximate messages and approximate factors in the structured variational approximation. This is\nprecisely the scheme employed by the soft actor-critic algorithm (Haarnoja et al., 2018b).\n4.3\nSoft Q-Learning\nWe can derive an alternative form for a reinforcement learning algorithm without using an explicit\npolicy parameterization, ﬁtting only the messages Qφ(st, at). In this case, we assume an implicit\nparameterization for both the value function V (st) and policy q(at|st), where\nV (st) = log\nZ\nA\nexp(Q(st, at))dat,\nas in Equation (21), and\nq(at|st) = exp(Q(st, at) −V (st)),\nwhich corresponds directly to Equation (20). In this case, no further parameterization is needed\nbeyond Qφ(st, at), which can be learned by minimizing the error in Equation (23), substituting the\nimplicit equation for V (st) in place of Vψ(st). We can write the resulting gradient update as\nφ ←φ −αE\n\u0014dQφ\ndφ (st, at)\n\u0012\nQφ(st, at) −\n\u0012\nr(st, at) + log\nZ\nA\nexp(Q(st+1, at+1))dat+1\n\u0013\u0013\u0015\n.\nIt is worth pointing out the similarity to the standard Q-learning update:\nφ ←φ −αE\n\u0014dQφ\ndφ (st, at)\n\u0012\nQφ(st, at) −\n\u0012\nr(st, at) + max\nat+1 Qφ(st+1, at+1))\n\u0013\u0013\u0015\n.\nWhere the standard Q-learning update has a max over at+1, the soft Q-learning update has a “soft”\nmax. As the magnitude of the reward increases, the soft update comes to resemble the hard update.\n12\nIn the case of discrete actions, this update is straightforward to implement, since the integral is\nreplaced with a summation, and the policy can be extracted simply by normalizing the Q-function.\nIn the case of continuous actions, a further level of approximation is needed to evaluate the integral\nusing samples. Sampling from the implicit policy is also non-trivial, and requires an approximate\ninference procedure, as discussed by Haarnoja et al. (Haarnoja et al., 2017).\nWe can further use this framework to illustrate an interesting connection between soft Q-learning\nand policy gradients. According to the deﬁnition of the policy in Equation (20), which is deﬁned\nentirely in terms of Qφ(st, at), we can derive an alternative gradient with respect to φ starting from\nthe policy gradient. This derivation represents a connection between policy gradient and Q-learning\nthat is not apparent in the standard framework, but becomes apparent in the maximum entropy\nframework. The full derivation is provided by Haarnoja et al. (Haarnoja et al., 2017) (Appendix B).\nThe ﬁnal gradient corresponds to\n∇φJ(φ) =\nT\nX\nt=1\nE(st,at)∼q(st,at)\nh\n(∇φQ(st, at) −∇φV (st)) ˆA(st, at)\ni\n.\nThe soft Q-learning gradient can equivalently be written as\n∇φJ(φ) =\nT\nX\nt=1\nE(st,at)∼q(st,at)\nh\n∇φQ(st, at) ˆA(st, at)\ni\n,\n(24)\nwhere we substitute the target value r(st, at) + V (st+1) for ˆA(st, at), taking advantage of the fact\nthat we can use any state-dependent baseline. Although these gradients are not exactly equal, the\nextra term −∇φV (st) simply accounts for the fact that the policy gradient alone is insufﬁcient to re-\nsolve one extra degree of freedom in Q(st, at): the addition or subtraction of an action-independent\nconstant. We can eliminate this term if we add the policy gradient with respect to φ together with\nBellman error minimization for V (st), which has the gradient\n∇φV (st)Eat∼q(at|st)\n\u0002\nr(st, at) + Est+1∼q(st+1|st,at)[V (st+1)]\n\u0003\n= ∇φV (st)Eat∼q(at|st)[ ˆQ(st, at)].\nNoting that ˆQ(st, at) is simply a (non-baselined) return estimate, we can show that the sum of\nthe policy gradient and value gradient exactly matches Equation (24) for a particular choice of\nstate-dependent baseline, since the term ∇φV (st)Eat∼q(at|st)[ ˆQ(st, at)] cancels against the term\n−∇φV (st) ˆA(st, at) in expectation over at when ˆA(st, at) = ˆQ(st, at) (that is, when we use a\nbaseline of zero). This completes the proof of a general equivalence between soft Q-learning and\npolicy gradients.\n5\nReview of Prior Work\nIn this section, we will discuss a variety of prior works that have sought to explore the connection\nbetween inference and control, make use of this connection to devise more effective learning algo-\nrithms, and extend it into other applications, such as intent inference and human behavior forecasting.\nWe will ﬁrst discuss the variety of frameworks proposed in prior work that are either equivalent to the\napproach presented in this article, or special cases (or generalization) thereof (Section 5.1). We will\nthen discuss alternative formulations that, though similar, differ in some critical way (Section 5.2).\nWe will then discuss speciﬁc reinforcement learning algorithms that build on the maximum entropy\nframework (Section 5.3), and conclude with a discussion of applications of the maximum entropy\nframework in other areas, such as intent inference, human behavior modeling, and forecasting (Sec-\ntion 5.4).\n5.1\nFrameworks for Control as Inference\nFraming control, decision making, and reinforcement learning as a probabilistic inference and learn-\ning problem has a long history, going back to original work by Rudolf Kalman (Kalman, 1960),\nwho described how the Kalman smoothing algorithm can also be used to solve control problems\nwith linear dynamics and quadratic costs (the “linear-quadratic regulator” or LQR setting). It is\nworth noting that, in the case of linear-quadratic systems, the maximum entropy solution is a linear-\nGaussian policy where the mean corresponds exactly to the optimal deterministic policy. This some-\ntimes referred to as the Kalman duality (Todorov, 2008). Unfortunately, this elegant duality does\n13\nnot in general hold for non-LQR systems: the maximum entropy framework for control as inference\ngeneralizes standard optimal control, and the optimal stochastic policy for a non-zero temperature\n(see Section 2.5) does not in general have the optimal deterministic policy as its mean.\nSubsequent work expanded further on the connection between control and inference. Attias (2003)\nproposed to implement a planning algorithm by means of the Baum-Welch-like method in HMM-\nstyle models.\nTodorov (2006) formulated a class of reinforcement learning problems termed\n“linearly-solvable” MDPs (LMDPs). LMDPs correspond to the graphical model described in Sec-\ntion 2, but immediately marginalize out the actions or, equivalently, posit that actions are equivalent\nto the next state, allowing the entire framework to operate entirely on states rather than actions. This\ngives rise to a simple and elegant framework that is especially tractable in the tabular setting. In the\ndomain of optimal control, Kappen (2011) formulated a class of path integral control problems that\nalso correspond to the graphical model discussed in Section 2, but derived starting from a continuous\ntime formulation and formulated as a diffusion process. This continuous time generalization arrives\nat the same solution in discrete time, but requires considerably more stochastic processes machinery,\nso is not discussed in detail in this article. Similar work by Toussaint and colleagues (Toussaint,\n2009; Rawlik et al., 2013) formulated graphical models for solving decision making problems, indi-\nrectly arriving at the same framework as in the previously discussed works. In the case of Toussaint\n(2009), expectation propagation (Minka, 2001) was adapted for approximate message passing dur-\ning planning. Ziebart (2010) formulated learning in PGMs corresponding to the control or reinforce-\nment learning problem as a problem of learning reward functions, and used this connection to derive\nmaximum entropy inverse reinforcement learning algorithms, which are discussed in more detail in\nSection 5.4. Ziebart’s derivation of the relationship between decision making, conditional random\nﬁelds, and PGMs also provides a thorough exploration of the foundational theory in this ﬁeld, and\nis a highly recommend compendium to accompany this article for readers seeking a more in-depth\ntheoretical discussion and connections to maximum entropy models (Ziebart, 2010). The particular\nPGM studied by Ziebart is discussed in Section 2.5: although Ziebart frames the model as a condi-\ntional random ﬁeld, without the auxiliary optimality variables Ot, this formulation is equivalent to\nthe one discussed here. Furthermore, the maximum causal entropy method discussed by Ziebart can\nbe shown to be equivalent to the variational inference formulation presented in Section 3.1.\n5.2\nRelated but Distinct Approaches\nAll of the methods discussed in the previous section are either special cases or generalizations of the\ncontrol as inference framework presented in this article. A number of other works have presented\nrelated approaches that also aim to unify control and inference, but do so in somewhat different ways.\nWe survey some of these prior techniques in this section, and describe their technical and practical\ndifferences from the presented formulation.\nBoltzmann Exploration.\nThe form of the optimal policy in the maximum entropy framework\n(e.g., Equation (20)) suggests a very natural exploration strategy: actions that have large Q-value\nshould be taken more often, while actions that have low Q-value should be taken less often, and the\nstochastic exploration strategy has the form of a Boltzmann-like distribution, with the Q-function\nacting as the negative energy. A large number of prior methods (Sutton, 1990; Kaelbling et al., 1996)\nhave proposed to use such a policy distribution as an exploration strategy, but in the context of a rein-\nforcement learning algorithm where the Q-function is learned via the standard (“hard”) max operator,\ncorresponding to a temperature (see Section 2.5) of zero. Boltzmann exploration therefore does not\noptimize the maximum entropy objective, but rather serves as a heuristic modiﬁcation to enable im-\nproved exploration. A closely related idea is presented in the work on energy-based reinforcement\nlearning (Sallans and Hinton, 2004; Heess et al., 2013), where the free energy of an energy-based\nmodel (in that case, a restricted Boltzmann machine) is adjusted based on a reinforcement learning\nupdate rule, such that the energy corresponds to the negative Q-function. Interestingly, energy-based\nreinforcement learning can optimize either the maximum entropy objective or the standard objective\n(with Boltzmann exploration), based on the type of update rule that is used. When used with an\non-policy SARSA update rule, as proposed by Sallans and Hinton (2004), the method actually does\noptimize the maximum entropy objective, since the policy uses the Boltzmann distribution. How-\never, when updated using an off-policy Q-learning objective with a hard max, the method reduces to\nBoltzmann exploration and optimizes the standard RL objective.\n14\nEntropy Regularization.\nIn the context of policy gradient and actor-critic methods, a com-\nmonly used technique is to use “entropy regularization,” where an entropy maximization term\nis added to the policy objective to prevent the policy from becoming too deterministic pre-\nmaturely.\nThis technique was proposed as early as the ﬁrst work on the REINFORCE algo-\nrithm (Williams and Peng, 1991; Williams, 1992), and is often used in recent methods (see, e.g.,\ndiscussion by O’Donoghue et al. (2017)). While the particular technique for incorporating this en-\ntropy regularizer varies, typically the simplest way is to simply add the gradient of the policy entropy\nat each sampled state to a standard policy gradient estimate, which itself may use a critic. Note that\nthis is not, in general, equivalent to the maximum entropy objective, which not only optimizes for\na policy with maximum entropy, but also optimizes the policy itself to visit states where it has high\nentropy. Put another way, the maximum entropy objective optimizes the expectation of the entropy\nwith respect to the policy’s state distribution, while entropy regularization only optimizes the policy\nentropy at the states that are visited, without actually trying to modify the policy itself to visit high-\nentropy states (see, e.g., Equation (2) in O’Donoghue et al. (2017)). While this does correspond to\na well-deﬁned objective, that objective is rather involved to write out and generally not mentioned\nin work that uses entropy regularization. The technique is typically presented as a heuristic modiﬁ-\ncation to the policy gradient. Interestingly, it is actually easier to perform proper maximum entropy\nRL than entropy regularization: maximum entropy RL with a policy gradient or actor-critic method\nonly requires subtracting log π(a|s) from the reward function, while heuristic entropy maximization\ntypically uses an explicit entropy gradient.\nVariational Policy Search and Expectation Maximization.\nAnother formulation of the rein-\nforcement learning problem with strong connections to probabilistic inference is the formulation\nof policy search in an expectation-maximization style algorithm. One common way to accomplish\nthis is to directly treat rewards as a probability density, and then use a “pseudo-likelihood” written\nas\nJ(θ) =\nZ\nr(τ)p(τ|θ)dτ,\nwhere r(τ) is the total reward along a trajectory, and p(τ|θ) is the probability of observing\na trajectory τ given a policy parameter vector θ.\nAssuming r(τ) is positive and bounded\nand applying Jensen’s inequality results in a variety of algorithms (Peters and Schaal, 2007;\nHachiya et al., 2009; Neumann, 2011; Abdolmaleki et al., 2018), including reward-weighted regres-\nsion (Peters and Schaal, 2007), that all follow the following general recipe: samples are weighted\naccording to some function of their return (potentially with importance weights), and the policy is\nthen updated by optimizing a regression objective to match the sample actions, weighted by these\nweights. The result is that samples with higher return are matched more closely, while those with\nlow return are ignored. Variational policy search methods also fall into this category (Neumann,\n2011; Levine and Koltun, 2013b), sometimes with the modiﬁcation of using explicit trajectory op-\ntimization rather than reweighting to construct the target actions (Levine and Koltun, 2013b), and\nsometimes using an exponential transformation on r(τ) to ensure positivity. Unlike the approach\ndiscussed in this article, the use of the reward as a “pseudo-likelihood” does not correspond directly\nto a well-deﬁned probabilistic model, though the application of Jensen’s inequality can still be mo-\ntivated simply from the standpoint of deriving a bound for the RL optimization problem. A more\nserious disadvantage of this class of methods is that, by regressing onto the reweighted samples, the\nmethod loses the ability to properly handle risk for stochastic dynamics and policies. Consider, for\nexample, a setting where we aim to ﬁt a unimodal policy for a stateless problem with a 1D action.\nIf we have a high reward for the action −1 and +1, and a low reward for the action 0, the optimal\nﬁt will still place all of the probability mass in the middle, at the action 0, which is the worst pos-\nsible option. Mathematically, this problem steps from the fact that supervised learning matches a\ntarget distribution by minimizing a KL-divergence of the form DKL(ptgt∥pθ), where ptgt is the tar-\nget distribution (e.g., the reward or exponentiated reward). RL instead minimizes a KL-divergence\nof the form DKL(pθ∥ptgt), which prioritizes ﬁnding a mode of the target distribution rather than\nmatching its moments. This issue is discussed in more detail in Section 5.3.5 of (Levine, 2014). In\ngeneral, the issue manifests itself as risk-seeking behavior, though distinct in nature from the risk-\nseeking behavior discussed in Section 2.4. Note that Toussaint and Storkey (2006) also propose an\nexpectation-maximization based algorithm for control as inference, but in a framework that does in\nfact yield maximum expected reward solutions, with a similar formulation to the one in this article.\n15\nKL-Divergence Constraints for Policy Search.\nPolicy search methods frequently employ a con-\nstraint between the new policy and the old policy at each iteration, in order to bound the change in the\npolicy distribution and thereby ensure smooth, stable convergence. Since policies are distributions,\na natural choice for the form of this constraint is a bound on the KL-divergence between the new\npolicy and the old one (Bagnell and Schneider, 2003; Peters et al., 2010; Levine and Abbeel, 2014;\nSchulman et al., 2015; Abdolmaleki et al., 2018). When we write out the Lagrangian of the resulting\noptimization problem, we typically end up with a maximum entropy optimization problem similar\nto the one in Equation (11), where instead of taking the KL-divergence between the new policy and\nexponentiated reward, we instead have a KL-divergence between the new policy and the old one.\nThis corresponds to a maximum entropy optimization where the reward is r(s, a) + λ log ¯π(a|s),\nwhere λ is the Lagrange multiplier and ¯π is the old policy, and the entropy term has a weight of\nλ. This is equivalent to a maximum entropy optimization where the entropy has a weight of one,\nand the reward is scaled by 1\nλ. Thus, although none of these methods actually aim to optimize the\nmaximum entropy objective in the end, each step of the policy update involves solving a maximum\nentropy problem. A similar approach is proposed by Rawlik et al. (2013), where a sequence of maxi-\nmum entropy problems is solved in a Q-learning style framework to eventually arrive at the standard\nmaximum reward solution.\n5.3\nReinforcement Learning Algorithms\nMaximum entropy reinforcement learning algorithms have been proposed in a range of frame-\nworks and with a wide variety of assumptions.\nThe path integral framework has been used\nto derive algorithms for both optimal control and planning (Kappen, 2011) and policy search\nvia reinforcement learning (Theodorou et al., 2010). The framework of linearly solvable MDPs\nhas been used to derive policy search algorithms (Todorov, 2010), value function based algo-\nrithms (Todorov, 2006), and inverse reinforcement learning algorithms (Dvijotham and Todorov,\n2010).\nMore recently, entropy maximization has been used as a component in algorithms\nbased on model-free policy search with importance sampling and its variants (Levine and Koltun,\n2013a; Nachum et al., 2017a,b), model-based algorithms based on the guided policy search frame-\nwork (Levine and Abbeel, 2014; Levine and Koltun, 2014; Levine et al., 2016), and a variety of\nmethods based on soft Q-learning (Haarnoja et al., 2017; Schulman et al., 2017) and soft actor-critic\nalgorithms (Haarnoja et al., 2018b; Hausman et al., 2018).\nThe particular reasons for the use of the control as inference framework differ between each of\nthese algorithms. The motivation for linearly solvable MDPs is typically based on computationally\ntractable exact solutions for tabular settings (Todorov, 2010), which are enabled essentially by dis-\npensing with the non-linear maximization operator in the standard RL framework. Although the\nmaximum entropy dynamic programming equations are still not linear in terms of value functions\nand Q-functions, they are linear under an exponential transformation. The reason for this is quite\nnatural: since these methods implement sum-product message passing, the only operations in the\noriginal probability space are summations and multiplications. However, for larger problems where\ntabular representations are impractical, these beneﬁts are not apparent.\nIn the case of path consistency methods (Nachum et al., 2017a,b), the maximum entropy frame-\nwork offers an appealing mechanism for off-policy learning. In the case of guided policy search,\nit provides a natural method for matching distributions between model-based local policies and a\nglobal policy that uniﬁes the local policy into a single globally coherent strategy (Levine and Koltun,\n2014; Levine and Abbeel, 2014; Levine et al., 2016). For the more recent model-free maximum en-\ntropy algorithms, such as soft Q-learning (Haarnoja et al., 2017) and soft-actor critic (Haarnoja et al.,\n2018b), as well the work of Hausman et al. (2018), the beneﬁts are improved stability and model-\nfree RL performance, improved exploration, and the ability to pre-train policies for diverse and\nunder-speciﬁed goals. For example, Haarnoja et al. (2017) present a quadrupedal robot locomotion\ntask where the reward depends only on the speed of the robot’s motion, regardless of direction. In a\nstandard RL framework, this results in a policy that runs in an arbitrary direction. Under the maxi-\nmum entropy framework, the optimal policy runs in all directions with equal probability. This makes\nit well-suited for pretraining general-purpose policies that can then be ﬁnetuned for more narrowly\ntailored tasks. More recently, Haarnoja et al. also showed that maximum entropy policies can be\ncomposed simply by adding their Q-functions, resulting in a Q-function with bounded difference\nagainst the optimal Q-function for the corresponding composed reward (Haarnjoa et al., 2018).\n16\nRecently, a number of papers have explored how the control as inference or maximum entropy re-\ninforcement learning framework can be extended to add additional latent variables to the model,\nsuch that the policy is given by π(a|s, z), where z is a latent variable.\nIn one class of meth-\nods (Hausman et al., 2018; Gupta et al., 2018), these variables are held constant over the duration of\nthe episode, providing for a time-correlated exploration signal that can enable a single policy to cap-\nture multiple skills and rapidly explore plausible behaviors for new tasks by searching in the space\nof values for z. In another class of methods (Haarnoja et al., 2017, 2018a), the latent variable z is\nselected independently at each time step, and the policy π(a|s, z) has some simple unimodal form\n(e.g., a Gaussian distribution) conditioned on z, but a complex multimodal form when z is integrated\nout. This enables the policy to represent very complex mulitmodal distributions, which can be use-\nful, for example, for capturing the true maximum entropy distribution for an underspeciﬁed reward\nfunction (e.g., run in all possible directions). It also makes it possible to learn a higher-level policy\nthat uses z as its action space (Haarnoja et al., 2018a), effectively driving the lower level policy and\nusing it as a distribution over skills. This leads to a natural probabilistic hierarchical reinforcement\nlearning formulation.\n5.4\nModeling, Intent Inference, and Forecasting\nAside from devising more effective reinforcement learning and optimal control algorithms, maxi-\nmum entropy reinforcement learning has also been used extensively in the inverse reinforcement\nlearning setting, where the goal is to infer intent, acquire reward functions from data, and predict the\nbehavior of agents (e.g., humans) in the world from observation. Indeed, the use of the term “maxi-\nmum entropy reinforcement learning” in this article is based on the work of Ziebart and colleagues,\nwho proposed the maximum entropy inverse reinforcement learning algorithm (Ziebart et al., 2008)\nfor inferring reward functions and modeling human behavior.\nWhile maximum entropy reinforcement learning corresponds to inference in the graphical model\nover the variables st, at, and Ot, inverse reinforcement learning corresponds to a learning problem,\nwhere the goal is to learn the CPD p(Ot|st, at), given example sequences {s1:T,i, a1:T,i, O1:T,i},\nwhere Ot is always true, indicating that the data consists of demonstrations of optimal trajectories.\nAs with all graphical model learning problems, inference takes place in the inner loop of an itera-\ntive learning procedure. Exact inference via dynamic programming results in an algorithm where,\nat each iteration, we solve for the optimal soft value function, compute the corresponding policy,\nand then use this policy to compute the gradient of the likelihood of the data with respect to the\nparameters of the CPD p(Ot|st, at). For example, if we use a linear reward representation, such that\np(Ot|st, at) = exp(φT f(st, at)), the learning problem can be expressed as\nφ⋆= arg max\nφ\nX\ni\nX\nt\nlog p(at,i|st,i, O1:T , φ),\nwhere computing log p(at,i|st,i, O1:T , φ) and its gradient requires solving for the optimal policy\nunder the current reward parameters φ.\nThe same optimism issue discussed in Section 2 occurs in the inverse reinforcement learning set-\nting, where exact inference in the graphical model produces an “optimistic” policy that assumes\nsome degree of control over the system dynamics. For this reason, Ziebart and colleagues proposed\nthe maximum causal entropy framework for inverse reinforcement learning under stochastic dynam-\nics (Ziebart et al., 2010). Although this framework is derived starting from a causal reformulation\nof the maximum entropy principle, the resulting algorithm is exactly identical to the variational in-\nference algorithm presented in Section 3, and the corresponding learning procedure corresponds to\noptimizing the variational lower bound with respect to the reward parameters φ.\nSubsequent work in inverse reinforcement learning has studied settings where the reward func-\ntion has a more complex, non-linear representation (Levine et al., 2011; Wulfmeier et al., 2015),\nand extensions to approximate inference via the Laplace approximation (under known dynam-\nics) (Levine and Koltun, 2012; Dragan et al., 2013) and approximate reinforcement learning (un-\nder unknown dynamics) (Finn et al., 2016b; Fu et al., 2018). Aside from inferring reward func-\ntions from demonstrations for the purpose of imitation learning, prior work has also sought to\nleverage the framework of maximum entropy inverse reinforcement learning for inferring the in-\ntent of humans, for applications such as robotic assistance (Dragan et al., 2013), brain-computer\ninterfaces (Javdani et al., 2015), and forecasting of human behavior (Huang and Kitani, 2014;\nHuang et al., 2015).\n17\nRecent\nwork\nhas\nalso\ndrawn\nconnections\nbetween\ngenerative\nadversarial\nnetworks\n(GANs) (Goodfellow et al., 2014) and maximum entropy inverse reinforcement learning (Finn et al.,\n2016b,a; Fu et al., 2018; Ho and Ermon, 2016). This connection is quite natural since, just like\ngenerative adversarial networks, the graphical model in the maximum entropy reinforcement\nlearning framework is a generative model, in this case of trajectories. GANs avoid the need for\nexplicit estimation of the partition function by noting that, given a model ˆp(x) for some true\ndistribution p(x), the optimal classiﬁer for discriminating whether a sample x came from the model\nor from the data corresponds to the odds ratio\nD(x) =\np(x)\np(x) + ˆp(x).\nAlthough ˆp(x) is unknown, ﬁtting this “discriminator” and using its gradients with respect to x\nto modify p(x) allows for effective training of the generative model. In the inverse reinforcement\nlearning setting, the discriminator takes the form of the reward function. The reward function is\nlearned so as to maximize the reward of the demonstration data and minimize the reward of samples\nfrom the current policy, while the policy is updated via the maximum entropy objective to maxi-\nmize the expectation of the reward and maximize entropy. As discussed by Finn et al., this process\ncorresponds to a generative adversarial network over trajectories, and also corresponds exactly to\nmaximum entropy inverse reinforcement learning (Finn et al., 2016a). A recent extension of this\nframework also provides for an effective inverse reinforcement learning algorithm in a model-free\ndeep RL context, as well as a mechanism for recovering robust and transferable rewards in ambigu-\nous settings (Fu et al., 2018). A simpliﬁcation on this setup known as generative adversarial imita-\ntion learning (GAIL) (Ho and Ermon, 2016) dispenses with the goal of recovering reward functions,\nand simply aims to clone the demonstrated policy. In this setup, the algorithm learns the advantage\nfunction directly, rather than the reward, which corresponds roughly to an adversarial version of the\nOptV algorithm (Dvijotham and Todorov, 2010).\nA number of prior works have also sought to incorporate probabilistic inference into a model\nof biological decision making and control (Solway and Botvinick, 2012; Botvinick and An, 2009;\nBotvinick and Toussaint, 2012; Friston, 2009). The particular frameworks employed in these ap-\nproaches differ: the formulation proposed by Friston (2009) is similar to the maximum entropy ap-\nproach outlined in this survey, and also employs the formalism of approximate variational inference.\nThe formulation described by Botvinick and Toussaint (2012) does not use the exponential reward\ntransformation, and corresponds more closely to the “pseudo-likelihood” formulation outlined in\nSection 5.2.\n6\nPerspectives and Future Directions\nIn this article, we discussed how the maximization of a reward function in Markov decision process\ncan be formulated as an inference problem in a particular graphical model, and how a set of update\nequations similar to the well-known value function dynamic programming solution can be recovered\nas the direct consequence of applying structured variational inference to this graphical model. The\nclassical maximum expected reward formulation emerges as a limiting case of this framework, while\nthe general case corresponds to a maximum entropy variant of reinforcement learning or optimal\ncontrol, where the optimal policy not only aims to maximize the expected reward, but also aims to\nmaintain high entropy.\nThe framework of maximum entropy reinforcement learning has already been employed in a range\nof contexts, as discussed in the previous section, from devising more effective and powerful for-\nward reinforcement learning algorithms, to developing probabilistic algorithms for modeling and\nreasoning about observed goal-driven behavior. A particularly exciting recent development is the\nintersection of maximum entropy reinforcement learning and latent variable models, where the\ngraphical model for control as inference is augmented with additional variables for modeling time-\ncorrelated stochasticity for exploration (Hausman et al., 2018; Gupta et al., 2018) or higher-level\ncontrol through learned latent action spaces (Haarnoja et al., 2017, 2018a). The extensibility and\ncompositionality of graphical models can likely be leveraged to produce more sophisticated re-\ninforcement learning methods, and the framework of probabilistic inference can offer a powerful\ntoolkit for deriving effective and convergent learning algorithms for the corresponding models.\n18\nLess explored in the recent literature is the connection between maximum entropy reinforcement\nlearning and robust control. Although some work has hinted at this connection (Ziebart, 2010),\nthe potential for maximum entropy reinforcement learning to produce policies that are robust to\nmodeling errors and distributional shift has not been explored in detail. In principle, a policy that is\ntrained to achieve high expected reward under the highest possible amount of injected noise (highest\nentropy) should be robust to unexpected perturbations at test time. Indeed, recent work in robotics\nhas illustrated that policies trained with maximum entropy reinforcementlearning methods (e.g., soft\nQ-learning) do indeed exhibit a very high degree of robustness (Haarnjoa et al., 2018). However, a\ndetailed theoretical exploration of this phenomenon has so far been lacking, and it is likely that it\ncan be applied more broadly to a range of challenging problems involving domain shift, unexpected\nperturbations, and model errors.\nFinally, the relationship between probabilistic inference and control can shed some light on the de-\nsign of reward functions and objectives in reinforcement learning. This is an often-neglected topic\nthat has tremendous practical implications: reinforcement learning algorithms typically assume that\nthe reward function is an extrinsic and unchanging signal that is provided as part of the problem\ndeﬁnition. However, in practice, the design of the reward function requires considerable care, and\nthe success of a reinforcement learning application is in large part determined by the ability of the\nuser to design a suitable reward function. The control as inference framework suggests a probabilis-\ntic interpretation of rewards as log probability of some discrete event variable Ot, and exploring\nhow this interpretation can lead to more interpretable, more effective, and easier to specify reward\nfunctions could lead to substantially more practical reinforcement learning methods in the future.\nAcknowledgements\nI would like to thank Emanuel Todorov, Karol Hausman, Nicolas Heess, and Shixiang Gu for sug-\ngestions on the writing, presentation, and prior work, as well as Vitchyr Pong, Rowan McAllister,\nTuomas Haarnoja, and Justin Fu for feedback on earlier drafts of this tutorial and all the students\nand post-docs in the UC Berkeley Robotic AI & Learning Lab for helpful discussion.\nReferences\nAbdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M. (2018).\nMaximum a posteriori policy optimisation. In International Conference on Learning Representa-\ntions (ICLR).\nAttias, H. (2003). Planning by probabilistic inference. In Proceedings of the 9th International\nWorkshop on Artiﬁcial Intelligence and Statistics.\nBagnell, J. A. and Schneider, J. (2003). Covariant policy search. In International Joint Conference\non Artiﬁcal Intelligence (IJCAI).\nBotvinick, M. and An, J. (2009). Goal-directed decision making in prefrontal cortex: a computa-\ntional framework. In Advances in Neural Information Processing Systems (NIPS).\nBotvinick, M. and Toussaint, M. (2012). Planning as inference. Trends in Cognitive Sciences,\n16(10):485–488.\nDragan, A. D., Lee, K. C. T., and Srinivasa, S. S. (2013). Legibility and predictability of robot\nmotion. In International Conference on Human-Robot Interaction (HRI).\nDvijotham, K. and Todorov, E. (2010). Inverse optimal control with linearly-solvable mdps. In\nInternational Conference on International Conference on Machine Learning (ICML).\nFinn, C., Christiano, P., Abbeel, P., and Levine, S. (2016a).\nA connection between genera-\ntive adversarial networks, inverse reinforcement learning, and energy-based models.\nCoRR,\nabs/1611.03852.\nFinn, C., Levine, S., and Abbeel, P. (2016b). Guided cost learning: Deep inverse optimal control via\npolicy optimization. In International Conference on Machine Learning (ICML).\n19\nFriston, K. (2009). The free-energy principle: A rough guide to the brain?\nTrends in Cognitive\nSciences, 13(7):293–301.\nFu, J., Luo, K., and Levine, S. (2018). Learning robust rewards with adversarial inverse reinforce-\nment learning. In International Conference on Learning Representations (ICLR).\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,\nand Bengio, Y. (2014). Generative adversarial nets. In Neural Information Processing Systems\n(NIPS).\nGupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. (2018). Meta-reinforcement learning\nof structured exploration strategies. CoRR, abs/1802.07245.\nHaarnjoa, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and Levine, S. (2018). Composable deep\nreinforcement learning for robotic manipulation. In International Conference on Robotics and\nAutomation (ICRA).\nHaarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a). Latent space policies for hierar-\nchical reinforcement learning. CoRR, abs/1804.02808.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-\nbased policies. In International Conference on Machine Learning (ICML).\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018b). Soft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic actor. In arXiv.\nHachiya, H., Peters, J., and Sugiyama, M. (2009). Efﬁcient sample reuse in em-based policy search.\nIn European Conference on Machine Learning (ECML).\nHausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. (2018). Learning an\nembedding space for transferable robot skills. In International Conference on Learning Represen-\ntations (ICLR).\nHeess, N., Silver, D., and Teh, Y. W. (2013). Actor-critic reinforcement learning with energy-based\npolicies. In European Workshop on Reinforcement Learning (EWRL).\nHo, J. and Ermon, S. (2016). Generative adversarial imitation learning. In Neural Information\nProcessing Systems (NIPS).\nHuang, D., Farahmand, A., Kitani, K. M., and Bagnell, J. A. (2015). Approximate MaxEnt in-\nverse optimal control and its application for mental simulation of human interactions. In AAAI\nConference on Artiﬁcial Intelligence (AAAI).\nHuang, D. and Kitani, K. M. (2014). Action-reaction: Forecasting the dynamics of human interac-\ntion. In European Conference on Computer Vision (ECCV).\nJavdani, S., Srinivasa, S., and Bagnell, J. A. (2015). Shared autonomy via hindsight optimization.\nIn Robotics: Science and Systems (RSS).\nKaelbling, L. P., Littman, M. L., and Moore, A. P. (1996). Reinforcement learning: A survey.\nJournal of Artiﬁcial Intelligence Research, 4:237–285.\nKalman, R. (1960). A new approach to linear ﬁltering and prediction problems. ASME Transactions\njournal of basic engineering, 82(1):35–45.\nKappen, H. J. (2011).\nOptimal control theory and the linear bellman equation.\nInference and\nLearning in Dynamic Models, pages 363–387.\nKappen, H. J., Gómez, V., and Opper, M. (2012). Optimal control as a graphical model inference\nproblem. Machine Learning, 87(2):159–182.\nKoller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques.\nThe MIT Press.\n20\nLevine, S. (2014). Motor skill learning with local trajectory methods. PhD thesis, Stanford Univer-\nsity.\nLevine, S. and Abbeel, P. (2014). Learning neural network policies with guided policy search under\nunknown dynamics. In Neural Information Processing Systems (NIPS).\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-end training of deep visuomotor\npolicies. Journal of Machine Learning Research, 17(1).\nLevine, S. and Koltun, V. (2012). Continuous inverse optimal control with locally optimal examples.\nIn International Conference on Machine Learning (ICML).\nLevine, S. and Koltun, V. (2013a). Guided policy search. In International Conference on Interna-\ntional Conference on Machine Learning (ICML).\nLevine, S. and Koltun, V. (2013b). Variational policy search via trajectory optimization. In Advances\nin Neural Information Processing Systems (NIPS).\nLevine, S. and Koltun, V. (2014). Learning complex neural network policies with trajectory opti-\nmization. In International Conference on Machine Learning (ICML).\nLevine, S., Popovi´c, Z., and Koltun, V. (2011). Nonlinear inverse reinforcement learning with\ngaussian processes. In Neural Information Processing Systems (NIPS).\nMinka, T. P. (2001). Expectation propagation for approximate bayesian inference. In Uncertainty\nin Artiﬁcial Intelligence (UAI).\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017a). Bridging the gap between value\nand policy based reinforcement learning. In arXiv.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017b). Trust-pcl: An off-policy trust\nregion method for continuous control. CoRR, abs/1707.01891.\nNeumann, G. (2011). Variational inference for policy search in changing situations. In International\nConference on Machine Learning (ICML).\nO’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2017). Pgq: Combining policy\ngradient and q-learning. In International Conference on Learning Representations (ICLR).\nPeters, J., Mülling, K., and Altün, Y. (2010). Relative entropy policy search. In AAAI Conference\non Artiﬁcial Intelligence (AAAI).\nPeters, J. and Schaal, S. (2007). Reinforcement learning by reward-weighted regression for opera-\ntional space control. In International Conference on Machine Learning (ICML).\nRawlik, K., Toussaint, M., and Vijayakumar, S. (2013). On stochastic optimal control and reinforce-\nment learning by approximate inference. In Robotics: Science and Systems (RSS).\nSallans, B. and Hinton, G. E. (2004). Reinforcement learning with factored states and actions.\nJournal of Machine Learning Research, 5.\nSchulman, J., Chen, X., and Abbeel, P. (2017). Equivalence between policy gradients and soft\nq-learning. In arXiv.\nSchulman, J., Levine, S., Moritz, P., Jordan, M. I., and Abbeel, P. (2015). Trust region policy\noptimization. In International Conference on Machine Learning (ICML).\nSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2016). High-dimensional contin-\nuous control using generalized advantage estimation. In International Conference on Learning\nRepresentations (ICLR).\nSolway, A. and Botvinick, M. (2012). Goal-directed decision making as probabilistic inference: a\ncomputational framework and potential neural correlates. Psychol Rev., 119(1):120–154.\nSutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on approxi-\nmating dynamic programming. In International Conference on Machine Learning (ICML).\n21\nTheodorou, E. A., Buchli, J., and Schaal, S. (2010). Learning policy improvements with path inte-\ngrals. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2010).\nTodorov, E. (2006). Linearly-solvable markov decision problems. In Advances in Neural Informa-\ntion Processing Systems (NIPS).\nTodorov, E. (2008). General duality between optimal control and estimation. In Conference on\nDecision and Control (CDC).\nTodorov, E. (2010). Policy gradients in linearly-solvable mdps. In Neural Information Processing\nSystems (NIPS).\nToussaint, M. (2009). Robot trajectory optimization using approximate inference. In International\nConference on Machine Learning (ICML).\nToussaint, M. and Storkey, A. (2006). Probabilistic inference for solving discrete and continuous\nstate markov decision processes. In International Conference on Machine Learning (ICML).\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Machine Learning, 8(3-4):229–256.\nWilliams, R. J. and Peng, J. (1991). Function optimization using connectionist reinforcement learn-\ning algorithms. Connection Science, 3(3):241–268.\nWulfmeier, M., Ondruska, P., and Posner, I. (2015). Maximum entropy deep inverse reinforcement\nlearning. In Neural Information Processing Systems Conference, Deep Reinforcement Learning\nWorkshop.\nZiebart, B. (2010). Modeling purposeful adaptive behavior with the principle of maximum causal\nentropy. PhD thesis, Carnegie Mellon University.\nZiebart, B. D., Bagnell, J. A., and Dey, A. K. (2010). Modeling interaction via the principle of\nmaximum causal entropy. In International Conference on Machine Learning (ICML).\nZiebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. (2008). Maximum entropy inverse reinforce-\nment learning. In International Conference on Artiﬁcial Intelligence (AAAI).\n22\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2018-05-02",
  "updated": "2018-05-20"
}