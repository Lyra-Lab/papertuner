{
  "id": "http://arxiv.org/abs/1805.01542v1",
  "title": "Fast and Scalable Expansion of Natural Language Understanding Functionality for Intelligent Agents",
  "authors": [
    "Anuj Goyal",
    "Angeliki Metallinou",
    "Spyros Matsoukas"
  ],
  "abstract": "Fast expansion of natural language functionality of intelligent virtual\nagents is critical for achieving engaging and informative interactions.\nHowever, developing accurate models for new natural language domains is a time\nand data intensive process. We propose efficient deep neural network\narchitectures that maximally re-use available resources through transfer\nlearning. Our methods are applied for expanding the understanding capabilities\nof a popular commercial agent and are evaluated on hundreds of new domains,\ndesigned by internal or external developers. We demonstrate that our proposed\nmethods significantly increase accuracy in low resource settings and enable\nrapid development of accurate models with less data.",
  "text": "Fast and Scalable Expansion of Natural Language Understanding\nFunctionality for Intelligent Agents\nAnuj Goyal, Angeliki Metallinou, Spyros Matsoukas\nAmazon Alexa\n{anujgoya, ametalli, matsouka}@amazon.com\nAbstract\nFast expansion of natural language functional-\nity of intelligent virtual agents is critical for\nachieving engaging and informative interac-\ntions. However, developing accurate models\nfor new natural language domains is a time\nand data intensive process.\nWe propose ef-\nﬁcient deep neural network architectures that\nmaximally re-use available resources through\ntransfer learning.\nOur methods are applied\nfor expanding the understanding capabilities\nof a popular commercial agent and are eval-\nuated on hundreds of new domains, designed\nby internal or external developers. We demon-\nstrate that our proposed methods signiﬁcantly\nincrease accuracy in low resource settings and\nenable rapid development of accurate models\nwith less data.\n1\nIntroduction\nVoice powered artiﬁcial agents have become\nwidespread among consumer devices, with agents\nlike Amazon Alexa, Google Now and Apple Siri\nbeing popular and widely used. Their success re-\nlies not only on accurately recognizing user re-\nquests, but also on continuously expanding the\nrange of requests that they can understand. An\never growing set of functionalities is critical for\ncreating an agent that is engaging, useful and\nhuman-like.\nThis presents signiﬁcant scalability challenges\nregarding rapidly developing the models at the\nheart of the natural language understanding (NLU)\nengines of such agents. Building accurate mod-\nels for new functionality typically requires collec-\ntion and manual annotation of new data resources,\nan expensive and lengthy process, often requir-\ning highly skilled teams.\nIn addition, data col-\nlected from real user interactions is very valuable\nfor developing accurate models but without an ac-\ncurate model already in place, the agent will not\nenjoy widespread use thereby hindering collection\nof high quality data.\nPresented with this challenge, our goal is to\nspeed up the natural language expansion process\nfor Amazon Alexa, a popular commercial artiﬁcial\nagent, through methods that maximize re-usability\nof resources across areas of functionality. Each\narea of Alexa’s functionality, e.g., Music, Calen-\ndar, is called a domain. Our focus is to a) increase\naccuracy of low resource domains b) rapidly build\nnew domains such that the functionality can be\nmade available to Alexa’s users as soon as possi-\nble, and thus start beneﬁting from user interaction\ndata. To achieve this, we adapt recent ideas at the\nintersection of deep learning and transfer learning\nthat enable us to leverage available user interaction\ndata from other areas of functionality.\nTo summarize our contributions, we describe\ndata efﬁcient deep learning architectures for NLU\nthat facilitate knowledge transfer from similar\ntasks. We evaluate our methods at a much larger\nscale than related transfer learning work in NLU,\nfor fast and scalable expansion of hundreds of\nnew natural language domains of Amazon Alexa,\na commercial artiﬁcial agent. We show that our\nmethods achieve signiﬁcant performance gains in\nlow resource settings and enable building accurate\nfunctionality faster during early stages of model\ndevelopment by reducing reliance on large anno-\ntated datasets.\n2\nRelated Work\nDeep learning models, including Long-Short term\nmemory networks (LSTM)\n(Gers et al., 1999),\nare state of the art for many natural language\nprocessing tasks (NLP), such as sequence label-\ning (Chung et al., 2014), named entity recogni-\ntion (NER)\n(Chiu and Nichols, 2015) and part\nof speech (POS) tagging\n(Huang et al., 2015).\narXiv:1805.01542v1  [cs.CL]  3 May 2018\nMultitask learning is also widely applied in NLP,\nwhere a network is jointly trained for multiple re-\nlated tasks. Multitask architectures have been suc-\ncesfully applied for joint learning of NER, POS,\nchunking and supertagging tasks, as in (Collobert\net al., 2011; Collobert and Weston, 2008; Søgaard\nand Goldberg, 2016).\nSimilarly, transfer learning addresses the trans-\nfer of knowledge from data-rich source tasks\nto under-resourced target tasks.\nNeural transfer\nlearning has been successfully applied in com-\nputer vision tasks where lower layers of a net-\nwork learn generic features that are transferred\nwell to different tasks (Zeiler and Fergus, 2014;\nKrizhevsky et al., 2012). Such methods led to im-\npressive results for image classiﬁcation and object\ndetection (Donahue et al., 2014; Sharif Razavian\net al., 2014; Girshick et al., 2014) In NLP, trans-\nferring neural features across tasks with disparate\nlabel spaces is relatively less common. In (Mou\net al., 2016), authors conclude that network trans-\nferability depends on the semantic relatedness of\nthe source and target tasks.\nIn cross-language\ntransfer learning, (Buys and Botha, 2016) use\nweak supervision to project morphology tags to\na common label set, while (Kim et al., 2017a)\ntransfer lower layer representations across lan-\nguages for POS tagging. Other related work ad-\ndresses transfer learning where source and target\nshare the same label space, while feature and la-\nbel distributions differ, including deep learning\nmethods (Glorot et al., 2011; Kim et al., 2017b),\nand earlier domain adaptation methods such as\nEasyAdapt (Daum´e III, 2007), instance weight-\ning (Jiang and Zhai, 2007) and structural corre-\nspondence learning (Blitzer et al., 2006).\nFast functionality expansion is critical in indus-\ntry settings. Related work has focused on scalabil-\nity and ability to learn from few resources when\ndeveloping a new domain, and includes zero-shot\nlearning (Chen et al., 2016; Ferreira et al., 2015),\ndomain attention (Kim et al., 2017c), and scal-\nable, modular classiﬁers (Li et al., 2014). There\nis a multitude of commercial tools for develop-\ners to build their own custom natural language ap-\nplications, including Amazon Alexa ASK (Kumar\net al., 2017), DialogFlow by Google (DialogFlow)\nand LUIS by Microsoft (LUIS). Along these lines,\nwe propose scalable methods that can be applied\nfor rapid development of hundreds of low resource\ndomains across disparate label spaces.\n3\nNLU Functionality Expansion\nWe focus on Amazon Alexa, an intelligent conver-\nsational agent that interacts with the user through\nvoice commands and is able to process requests on\na range of natural language domains, e.g., playing\nmusic, asking for weather information and editing\na calendar. In addition to this built-in functionality\nthat is designed and built by internal developers,\nthe Alexa Skills Kit (ASK) (Kumar et al., 2017)\nenables external developers to build their own cus-\ntom functionality which they can share with other\nusers, effectively allowing for unlimited new capa-\nbilities. Below, we describe the development pro-\ncess and challenges associated with natural lan-\nguage domain expansion.\nFor each new domain, the internal or exter-\nnal developers deﬁne a set of intents and slots\nfor the target functionality. Intents correspond to\nuser intention, e.g., ‘FindRecipeIntent’, and slots\ncorrespond to domain-speciﬁc entities of interest\ne.g.,‘FoodItem’. Developers also deﬁne a set of\ncommonly used utterances that cover the core use\ncases of the functionality, e.g., ‘ﬁnd a recipe for\nchicken’. We call those core utterances. In addi-\ntion, developers need to create gazetteers for their\ndomain, which are lists of slot values. For exam-\nple, a gazetteer for ‘FoodItem’ will contain differ-\nent food names like ‘chicken’. We have devel-\noped infrastructure to allow internal and external\nteams to deﬁne their domain, and create or ex-\npand linguistic resources such as core utterances\nand gazetteers. We have also built tools that en-\nable extracting carrier phrases from the example\nutterances by abstracting the utterance slot values,\nsuch as ‘ﬁnd a recipe for {FoodItem}’. The col-\nlection of carrier phrases and gazetteers for a do-\nmain is called a grammar. Grammars can be sam-\npled to generate synthetic data for model training.\nFor example, we can generate the utterance ‘ﬁnd\na recipe for pasta’ if the latter dish is contained in\nthe ‘FoodItem’ gazetteer.\nNext, developers enrich the linguistic resources\navailable for a new domain, to cover more linguis-\ntic variations for intents and slots. This includes\ncreating bootstrap data for model development, in-\ncluding collecting utterances that cover the new\nfunctionality, manually writing variations of ex-\nample utterances, and expanding the gazetteer val-\nues. In general, this is a time and data intensive\nprocess.\nExternal developers can also continu-\nously enrich the data they provide for their cus-\ntom domain. However, external developers typi-\ncally lack the time, resources or expertise to pro-\nvide rich datasets, therefore in practice custom do-\nmains are signiﬁcantly under-resourced compared\nto built-in domains.\nOnce the new domain model is bootstrapped\nusing the collected datasets, it becomes part\nof Alexa’s natural language functionality and is\navailable for user interactions. The data from such\nuser interactions can be sampled and annotated in\norder to provide additional targeted training data\nfor improving the accuracy of the domain. A good\nbootstrap model accuracy will lead to higher user\nengagement with the new functionality and hence\nto a larger opportunity to learn from user interac-\ntion data.\nConsidering these challenges, our goal is to re-\nduce our reliance on large annotated datasets for\na new domain by re-using resources from existing\ndomains. Speciﬁcally, we aim to achieve higher\nmodel accuracy in low resource settings and accel-\nerate new domain development by building good\nquality bootstrap models faster.\n4\nMethodology\nIn this section, we describe transfer learning meth-\nods for efﬁcient data re-use.\nTransfer learning\nrefers to transferring the knowledge gained while\nperforming a task in a source domain Ds to ben-\neﬁt a related task in a target domain Dt. Typi-\ncally, we have a large dataset for Ds, while Dt is\nan under-resourced new task. Here, the target do-\nmain is the new built-in or custom domain, while\nthe source domain contains functionality that we\nhave released, for which we have large amounts of\ndata. The tasks of interest in both Ds and Dt are\nthe same, namely slot tagging and intent classiﬁ-\ncation. However Ds and Dt have different label\nspaces Ys and Yt, because a new domain will con-\ntain new intent and slot labels compared to previ-\nously released domains.\n4.1\nDNN-based natural language engine\nWe ﬁrst present our NLU system where we per-\nform slot tagging (ST) and intent classiﬁcation\n(IC) for a given input user utterance.\nWe are\ninspired by the neural architecture of (Søgaard\nand Goldberg, 2016), where a multi-task learn-\ning architecture is used with deep bi-directional\nRecurrent Neural Networks (RNNs). Supervision\nfor the different tasks happens at different lay-\ners.\nOur neural network contains three layers\nFigure 1: Multitask stacked bi-LSTM architecture for\nST and IC, with a shared bottom layer, two separate top\nlayers for ST and IC. Gazetteer features can be added\nas optional input to the ST and IC layers during the\nﬁne-tuning stage. (see also Sec. 4.2)\nof bi-directional Long Short Term Memory net-\nworks (LSTMs) (Graves and Schmidhuber, 2005;\nHochreiter and Schmidhuber, 1997). The two top\nlayers are optimized separately for the ST and\nIC tasks, while the common bottom layer is op-\ntimized for both tasks, as shown in Figure 1.\nSpeciﬁcally let rc\nt denote the common represen-\ntation computed by the bottommost bi-LSTM for\neach word input at time t. The ST forward LSTM\nlayer learns a representation rST,f\nt\n= φ(rc\nt, rST\nt−1),\nwhere φ denotes the LSTM operation.\nThe IC\nforward LSTM layer learns rIC,f\nt\n= φ(rc\nt, rIC\nt−1).\nSimilarly, the backward LSTM layers learn rST,b\nt\nand rIC,b\nt\n. To obtain the slot tagging decision, we\nfeed the ST bi-LSTM layer’s output per step into a\nsoftmax, and produce a slot label at each time step\n(e.g., at each input word). For the intent decision,\nwe concatenate the last time step from the forward\nLSTM with the ﬁrst step of the backward LSTM,\nand feed it into a softmax for classiﬁcation:\nrslot\nt\n= rST,f\nt\n⊕rST,b\nt\n, rintent = rIC,f\nT\n⊕rIC,b\n1\nˆSt = softmax(Wsrslot\nt\n+ bs)\nˆI = softmax(WIrintent + bI)\nwhere ⊕denotes concatenation.\nWs, WI, bs, bI\nare the weights and biases for the slot and intent\nsoftmax layers respectively.\nˆSt is the predicted\nslot tag per time step (per input word), and ˆI is\nthe predicted intent label for the sentence.\nThe overall objective function for the multi-\ntask network combines the IC and ST objectives.\nTherefore we jointly learn a shared representation\nrc\nt that leverages the correlations between the re-\nlated IC and ST tasks, and shares beneﬁcial knowl-\nedge across tasks. Empirically, we have observed\nthat this multitask architecture achieves better re-\nsults than separately training intent and slot mod-\nels, with the added advantage of having a single\nmodel, and a smaller total parameter size.\nIn our setup, each input word is embedded into\na 300-dimensional embedding, where the embed-\ndings are estimated from our data. We also use\npre-trained word embeddings as a separate input,\nthat allows incorporating unsupervised word infor-\nmation from much larger corpora (FastText (Bo-\njanowski et al., 2016)). We encode slot spans us-\ning the IOB tagging scheme (Ramshaw and Mar-\ncus, 1995). When we have available gazetteers rel-\nevant to the ST task, we use gazetteer features as\nan additional input. Such features are binary indi-\ncators of the presence of an n-gram in a gazetteer,\nand are common for ST tasks (Radford et al.,\n2015; Nadeau and Sekine, 2007).\n4.2\nTransfer learning for the DNN engine\nTypically, a new domain Dt contains little avail-\nable data for training the multitask DNN architec-\nture of Sec 4.1. We propose to leverage existing\ndata from mature released domains (source Ds) to\nbuild generic models, which are then adapted to\nthe new tasks (target Dt).\nWe train our DNN engine using labeled data\nfrom Ds in a supervised way. The source slot tags\nspace Y slot\ns\nand intent label space Y intent\ns\ncon-\ntain labels from previously released slots and in-\ntents respectively. We refer to this stage as pre-\ntraining, where the stacked layers in the network\nlearn to generate features which are useful for the\nST and IC tasks of Ds. Our hypothesis is that\nsuch features will also be useful for Dt.\nAfter\npre-training is complete, we replace the top-most\nafﬁne transform and softmax layers for IC and ST\nwith layer dimensions that correspond to the tar-\nget label space for intents and slots respectively,\ni.e., Y intent\nt\nand Y slot\nt\n. The network is then trained\nagain using the available target labeled data for IC\nand ST. We refer to this stage as ﬁne-tuning of the\nDNN parameters for adapting to Dt.\nA network can be pre-trained on large datasets\nfrom Ds and later ﬁne tuned separately for many\nlow resource new domains Dt.\nIn some cases,\nwhen developing a new domain Dt, new domain-\nspeciﬁc information becomes available, such as\ndomain gazetteers (which were not available at\npre-training). To incorporate this information dur-\ning ﬁne-tuning, we add gazetteer features as an\nextra input to the two top-most ST and IC layers,\nas shown in Figure 1. We found that adding new\nfeatures during ﬁne-tuning signiﬁcantly changes\nthe upper layer distributions. Therefore, in such\ncases, it is better to train the ST and IC layers from\nscratch and only transfer and ﬁne-tune weights\nfrom the common representation rc of the bottom\nlayer. However, when no gazetteers are available,\nit is beneﬁcial to pre-train all stacked Bi-LSTM\nlayers (common, IC and ST), except from the task-\nspeciﬁc afﬁne transform leading to the softmax.\n4.3\nBaseline natural language engine\nWhile DNNs are strong models for both ST and\nIC, they typically need large amounts of training\ndata. As we focus on under-resourced function-\nality, we examine an alternative baseline that re-\nlies on simpler models; namely a Maximum En-\ntropy (MaxEnt) (Berger et al., 1996) model for in-\ntent classiﬁcation and a Conditional Random Field\n(CRF) (Lafferty et al., 2001) model for slot tag-\nging. MaxEnt models are regularized log-linear\nmodels that have been shown to be effective for\ntext classiﬁcation tasks (Berger et al., 1996). Sim-\nilarly, CRFs have been popular tagging models\nin the NLP literature (Nadeau and Sekine, 2007)\nprior to the recent growth in deep learning.\nIn\nour experience, these models require less data to\ntrain well and represent strong baselines for low\nresource classiﬁcation and tagging tasks.\n5\nExperiments and Results\nWe evaluate the transfer learning methods of Sec-\ntion 4.2 for both custom and built-in domains, and\ncompare with baselines that do not beneﬁt from\nknowledge transfer (Sections 4.1, 4.3). We exper-\niment with around 200 developer deﬁned custom\ndomains, whose statistics are presented in Table\n1. Looking at the median numbers, which are less\ninﬂuenced by a few large custom domains com-\npared to mean values, we note that typically devel-\nopers provide just a few tens of example phrases\nand few tens of values per gazetteer (slot gazetteer\nsize). Therefore, most custom domains are sig-\nniﬁcantly under-resourced. We also select three\nnew built-in domains, and evaluate them at various\nearly stages of domain development. Here, we as-\nsume that variable amounts of training data grad-\nually become available, including bootstrap and\nuser interaction data.\nWe pre-train DNN models using millions of an-\nnotated utterances from existing mature built-in\ndomains. Each annotated utterance has an associ-\nated domain label, which we use to make sure that\nthe pre-training data does not contain utterances\nlabeled as any of the custom or built-in target do-\nmains.\nAfter excluding the target domains, the\npre-training data is randomly selected from a vari-\nety of mature Alexa domains covering hundreds\nof intents and slots across a wide range of nat-\nural language functionality. For all experiments,\nwe use L1 and L2 to regularize our DNN, CRF\nand MaxEnt models, while DNNs are additionally\nregularized with dropout.\nThe test sets contain user data, annotated for\neach custom or built-in domain. For custom do-\nmains, test set size is a few hundred utterances per\ndomain, while for built-in domains it is a few thou-\nsand utterances per domain. Our metrics include\nstandard F1 scores for the SC and IC tasks, and\na sentence error rate (SER) deﬁned as the ratio of\nutterances with at least one IC or ST error over all\nutterances. The latter metric combines IC and ST\nerrors per utterance and reﬂects how many utter-\nances we could not understand correctly.\nData type\nMean\nMedian\nnumber of intents\n8.02\n3\nnumber of slots\n2.07\n1\nslot gazetteer size\n441.35\n11\nnumber of example phrases\n268.11\n42\nTable 1: Statistics of data for around 200 developer de-\nﬁned custom domains\n5.1\nResults for custom developer domains\nFor the custom domain experiments, we focus on\na low resource experimental setup, where we as-\nsume that our only target training data is the data\nprovided by the external developer. We report re-\nsults for around 200 custom domains, which is a\nsubset of all domains we support. We compare\nthe proposed transfer learning method, denoted\nas DNN Pretrained, with the two baseline meth-\nods described in sections 4.1 and 4.3, denoted as\nDNN Baseline and CRF/MaxEnt Baseline, respec-\ntively. For training the baselines, we use the avail-\nable data provided by the developer for each do-\nmain, e.g., example phrases and gazetteers. From\nthese resources, we create grammars and we sam-\nple them to generate 50K training utterances per\ndomain, using the process described in Section\n3.\nThis training data size was selected empiri-\ncally based on baseline model accuracy. The gen-\nerated utterances may contain repetitions for do-\nmains where the external developer provided a\nsmall amount of example phrases and few slot val-\nues per gazetteer. For the proposed method, we\npre-train a DNN model on 4 million utterances and\nﬁne tune it per domain using the 50K grammar ut-\nterances of that domain and any available gazetteer\ninformation (for extracting gazetteer features). In\nTable 2, we show the mean and median across cus-\ntom domains for F1slot, F1intent and SER.\nTable 2 shows that the CRF and MaxEnt mod-\nels present a strong baseline and generally outper-\nform the DNN model without pretraining, which\nhas a larger number of parameters. This suggests\nthat the baseline DNN models (without pretrain-\ning) cannot be trained robustly without large avail-\nable training data. The proposed pre-trained DNN\nsigniﬁcantly outperforms both baselines across all\nmetrics (paired t-test, p < .01). Median SER re-\nduces by around 14% relative when we use trans-\nfer learning compared to both baselines. We are\nable to harness the knowledge obtained from data\nof multiple mature source domains Ds and trans-\nfer it to our under-resourced target domains Dt,\nacross disparate label spaces.\nTo investigate the effect of semantic similarity\nacross source and target domains we selected a\nsubset of 30 custom domains with high seman-\ntic similarity with the source tasks.\nSemantic\nsimilarity was computed by comparing the sen-\ntence representations computed by the common\nbi-LSTM layer across source and target sentences,\nand selecting target custom domains with sen-\ntences close to at least one of the source tasks.\nFor these 30 domains, we observed higher gains of\naround 19% relative median SER reduction. This\ncorroborates observations of (Mou et al., 2016),\nthat neural feature transferability for NLP depends\non the semantic similarity between source and tar-\nget. In our low resource tasks, we see a beneﬁt\nfrom transfer learning and this beneﬁt increases as\nwe select more semantically similar data.\nOur approach is scalable and is does not rely\non manual domain-speciﬁc annotations, besides\ndeveloper provided data. Also, pretrained DNN\nmodels are about ﬁve times faster to train dur-\ning the ﬁne-tuning stage, compared to training\nthe model from scratch for each custom domain,\nApproach\nF1Intent\nF1Slot\nSER\nMean\nMedian\nMean\nMedian\nMean\nMedian\nBaseline CRF/MaxEnt\n94.6\n96.6\n80.0\n91.5\n14.5\n9.2\nBaseline DNN\n91.9\n95.9\n85.1\n92.9\n14.7\n9.2\nProposed Pretrained DNN *\n95.2\n97.2\n88.6\n93.0\n13.1\n7.9\nTable 2: Results for around 200 custom developer domains. For F1, higher values are better, while for SER lower\nvalues are better. * denotes statistically signiﬁcant SER difference compared to both baselines.\nwhich speeds up model turn-around time.\n5.2\nResults for built-in domains\nWe evaluate our methods on three new built-in do-\nmains referred here as domain A (5 intents, 36 slot\ntypes), domain B (2 intents, 17 slot types) and do-\nmain C (22 intents, 43 slot types). Table 3 shows\nresults for domains A, B and C across experimen-\ntal early stages of domain development, where dif-\nferent data types and amounts of data per data type\ngradually become available. Core data refers to\ncore example utterances, bootstrap data refers to\ndomain data collection and generation of synthetic\n(grammar) utterances, and user data refers to user\ninteractions with our agent. As described in Sec-\ntion 3, the collection and annotation of these data\nsources is a lengthy process. Here we evaluate\nwhether we can accelerate the development pro-\ncess by achieving accuracy gains in early, low re-\nsource stages, and bootstrap a model faster.\nFor each data setting and size, we compare our\nproposed pretrained DNN models with the base-\nline CRF/MaxEnt baseline, which is the better\nperforming baseline of Section 5.1. Results for\nthe non pre-trained DNN baseline are similar, and\nomitted for lack of space.\nOur proposed DNN\nmodels are pre-trained on 4 million data from ma-\nture domains and then ﬁne tuned on the available\ntarget data. The baseline CRF/MaxEnt models are\ntrained on the available target data. Note that the\ndatasets of Table 3 represent early stages of model\ndevelopment and do not reﬂect ﬁnal training size\nor model performance. The types of target data\nslightly differ across domains according to domain\ndevelopment characteristics. For example, for do-\nmain B there was very small amount of core data\navailable and it was combined with the bootstrap\ndata for experiments.\nOverall, we notice that our proposed DNN pre-\ntraining method improves performance over the\nCRF/MaxEnt baseline, for almost all data settings.\nAs we would expect, we see the largest gains for\nthe most low resource data settings. For example,\nfor domain A, we observe a 7% and 5% relative\nTrain Set\nSize\nMethod\nF1intent\nF1slot\nSER\nDomain A (5 intents, 36 slots)\nCore*\n500\nBaseline\n85.0\n63.9\n51.9\ndata\nProposed\n86.6\n66.6\n48.2\nBootstrap\n18K\nBaseline\n86.1\n72.8\n49.6\ndata*\nProposed\n86.9\n73.8\n47.0\nCore +\n3.5K\nBaseline\n90.4\n74.3\n40.5\nuser data*\nProposed\n90.1\n75.8\n37.9\nCore +\n43K\nBaseline\n92.1\n80.6\n33.4\nbootstrap +\nProposed\n91.9\n80.8\n32.8\nuser data\nDomain B (2 intents, 17 slots)\nBootstrap\n2K\nBaseline\n97.0\n94.7\n10.1\ndata*\nProposed\n97.8\n95.3\n6.3\nUser data\n2.5K\nBaseline\n97.0\n94.7\n8.2\nProposed\n97.1\n96.4\n7.1\nBootstrap +\n52K\nBaseline\n96.7\n95.2\n8.2\nuser data*\nProposed\n97.0\n96.6\n6.4\nDomain C (22 intents, 43 slots)\nCore*\n300\nBaseline\n77.9\n47.8\n64.2\ndata\nProposed\n85.6\n46.6\n51.8\nBootstrap\n26K\nBaseline\n46.1\n65.8\n64.0\ndata*\nProposed\n49.1\n68.9\n62.8\nCore +\n126K\nBaseline\n92.3\n78.3\n28.1\nbootstrap. +\nProposed\n92.7\n72.7\n31.9\nuser data*\nTable 3:\nResults on domains A, B and C for the\nproposed pretrained DNN method and the baseline\nCRF/MaxEnt method during experimental early stages\nof domain development. * denotes statistically signiﬁ-\ncant SER difference between proposed and baseline\nSER improvement on core and bootstrap data set-\ntings respectively. The performance gain we ob-\ntain on those early stages of development brings us\ncloser to our goal of rapidly bootstrapping models\nwith less data. From domains A and C, we also\nnotice that we achieve the highest performance in\nsettings that leverage user data, which highlights\nthe importance of such data. Note that the drop in\nFintent for domain C between core and bootstrap\ndata is because the available bootstrap data did not\ncontain data for all of the 22 intents of domain C.\nFinally, we notice that the gain from transfer learn-\ning diminishes in some larger data settings, and we\nmay see degradation (domain C, 126K data set-\nting). We hypothesize that as larger training data\nbecomes available it may be better to not pre-train\nor pre-train with source data that are semantically\nsimilar to the target. We will investigate this as\npart of future work.\n6\nConclusions and Future Work\nWe have described the process and challenges as-\nsociated with large scale natural language func-\ntionality expansion for built-in and custom do-\nmains for Amazon Alexa, a popular commercial\nintelligent agent. To address scalability and data\ncollection bottlenecks, we have proposed data efﬁ-\ncient deep learning architectures that beneﬁt from\ntransfer learning from resource-rich functionality\ndomains. Our models are pre-trained on existing\nresources and then adapted to hundreds of new,\nlow resource tasks, allowing for rapid and accurate\nexpansion of NLU functionality. In the future, we\nplan to explore unsupervised methods for transfer\nlearning and the effect of semantic similarity be-\ntween source and target tasks.\nReferences\nAdam L Berger, Vincent J Della Pietra, and Stephen\nA Della Pietra. 1996. A maximum entropy approach\nto natural language processing. Computational lin-\nguistics, 22(1):39–71.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120–128. Association for Computa-\ntional Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin,\nand Tomas Mikolov. 2016.\nEnriching word vec-\ntors with subword information.\narXiv preprint\narXiv:1607.04606.\nJan Buys and Jan A. Botha. 2016. Cross-lingual mor-\nphological tagging for low-resource languages. In\nProceedings of the Association for Computational\nLinguistics (ACL). Association for Computational\nLinguistics.\nYun-Nung Chen, Dilek Hakkani-T¨ur, and Xiaodong\nHe. 2016.\nZero-shot learning of intent embed-\ndings for expansion by convolutional deep struc-\ntured semantic models.\nIn Acoustics, Speech and\nSignal Processing (ICASSP), 2016 IEEE Interna-\ntional Conference on, pages 6045–6049. IEEE.\nJason PC Chiu and Eric Nichols. 2015.\nNamed en-\ntity recognition with bidirectional lstm-cnns. arXiv\npreprint arXiv:1511.08308.\nJ. Chung, C. Gulcehre, K. Cho, and Y. Bengio. 2014.\nEmpirical evaluation of gated recurrent neural net-\nworks on sequence modeling. In NIPS 2014 Work-\nshop on Deep Learning,.\nR. Collobert and J. Weston. 2008. A uniﬁed architec-\nture for natural language processing: Deep neural\nnetworks with multitask learning. In Proc. of Inter-\nnational Conference of Machine Learning (ICML)\n2008.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011.\nNatural language processing (almost) from\nscratch.\nJournal of Machine Learning Research,\n12(Aug):2493–2537.\nHal Daum´e III. 2007. Frustratingly easy domain adap-\ntation. In Proceedings of the Association for Com-\nputational Linguistics (ACL). Association for Com-\nputational Linguistics.\nDialogFlow. https://dialogﬂow.com.\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-\nman, Ning Zhang, Eric Tzeng, and Trevor Darrell.\n2014. Decaf: A deep convolutional activation fea-\nture for generic visual recognition. In International\nconference on machine learning, pages 647–655.\nEmmanuel Ferreira, Bassam Jabaian, and Fabrice\nLef`evre. 2015. Zero-shot semantic parser for spoken\nlanguage understanding. In Sixteenth Annual Con-\nference of the International Speech Communication\nAssociation.\nFelix A Gers, J¨urgen Schmidhuber, and Fred Cummins.\n1999. Learning to forget: Continual prediction with\nlstm.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Ji-\ntendra Malik. 2014. Rich feature hierarchies for ac-\ncurate object detection and semantic segmentation.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 580–587.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio.\n2011. Domain adaptation for large-scale sentiment\nclassiﬁcation: A deep learning approach. In Pro-\nceedings of the 28th international conference on ma-\nchine learning (ICML-11), pages 513–520.\nAlex Graves and J¨urgen Schmidhuber. 2005. Frame-\nwise phoneme classiﬁcation with bidirectional lstm\nand other neural network architectures. Neural Net-\nworks, 18(5):602–610.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\ntional lstm-crf models for sequence tagging. arXiv\npreprint arXiv:1508.01991.\nJing Jiang and ChengXiang Zhai. 2007.\nInstance\nweighting for domain adaptation in nlp.\nIn ACL,\nvolume 7, pages 264–271.\nJ.-K. Kim, Y.-B. Kim, R. Sarikaya, and E. Fosler-\nLussier. 2017a. Cross-lingual transfer learning for\npos tagging without cross-lingual resources.\nIn\nProc. of the 2017 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n2832–2838.\nYoung-Bum Kim, Karl Stratos, and Dongchan Kim.\n2017b. Adversarial adaptation of synthetic or stale\ndata. In ACL.\nYoung-Bum Kim, Karl Stratos, and Dongchan Kim.\n2017c. Domain attention with an ensemble of ex-\nperts.\nIn Annual Meeting of the Association for\nComputational Linguistics.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. Imagenet classiﬁcation with deep con-\nvolutional neural networks. In Advances in neural\ninformation processing systems, pages 1097–1105.\nAnjishnu Kumar, Arpit Gupta, Julian Chan, Sam\nTucker,\nBj¨orn\nHoffmeister,\nMarkus\nDreyer,\nStanislav Peshterliev, Ankur Gandhe, Denis Filimi-\nnov, Ariya Rastrow, Christian Monson, and Agnika\nKumar. 2017. Just ASK: building an architecture for\nextensible self-service spoken language understand-\ning.\nIn NIPS 2017 Workshop on Conversational\nAI.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001.\nConditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data.\nQ. Li, G. Tur, D. Hakkani-Tur, X. Li, T. Paek,\nA. Gunawardana, and C. Quirk. 2014. Distributed\nopen-domain conversational understanding frame-\nwork with domain independent extractors. In Spo-\nken Language Technology Workshop (SLT) 2014.\nLUIS. The Microsoft Language Understanding Intelli-\ngent Service (LUIS), https://www.luis.ai.\nL. Mou, Z. Meng, R. Yan, G. Li, Y. Xu, L. Zhang, and\nZ. Jin. 2016. How transferable are neural networks\nin nlp applications. In Proceedings of the 2016 con-\nference on empirical methods in natural language\nprocessing, pages 479–489.\nD. Nadeau and S. Sekine. 2007. A survey of named en-\ntity recognition and classiﬁcation. Linguisticae In-\nvestigationes,, volume 1. John Benjamins Publish-\ning Company.\nW. Radford, X. Carreras, and J. Henderson. 2015.\nNamed entity recognition with document-speciﬁc kb\ntag gazetteers. In Proc. of the 2015 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 512–517.\nLance Ramshaw and Mitch Marcus. 1995. Text chunk-\ning using transformation-based learning. In Third\nWorkshop on Very Large Corpora.\nAli Sharif Razavian, Hossein Azizpour, Josephine Sul-\nlivan, and Stefan Carlsson. 2014. Cnn features off-\nthe-shelf: an astounding baseline for recognition. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition workshops, pages 806–\n813.\nAnders Søgaard and Yoav Goldberg. 2016.\nDeep\nmulti-task learning with low level tasks supervised\nat lower layers. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, volume 2, pages 231–235.\nMatthew D Zeiler and Rob Fergus. 2014. Visualizing\nand understanding convolutional networks. In Eu-\nropean conference on computer vision, pages 818–\n833. Springer.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-05-03",
  "updated": "2018-05-03"
}