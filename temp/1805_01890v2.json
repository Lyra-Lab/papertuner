{
  "id": "http://arxiv.org/abs/1805.01890v2",
  "title": "RMDL: Random Multimodel Deep Learning for Classification",
  "authors": [
    "Kamran Kowsari",
    "Mojtaba Heidarysafa",
    "Donald E. Brown",
    "Kiana Jafari Meimandi",
    "Laura E. Barnes"
  ],
  "abstract": "The continually increasing number of complex datasets each year necessitates\never improving machine learning methods for robust and accurate categorization\nof these data. This paper introduces Random Multimodel Deep Learning (RMDL): a\nnew ensemble, deep learning approach for classification. Deep learning models\nhave achieved state-of-the-art results across many domains. RMDL solves the\nproblem of finding the best deep learning structure and architecture while\nsimultaneously improving robustness and accuracy through ensembles of deep\nlearning architectures. RDML can accept as input a variety data to include\ntext, video, images, and symbolic. This paper describes RMDL and shows test\nresults for image and text data including MNIST, CIFAR-10, WOS, Reuters, IMDB,\nand 20newsgroup. These test results show that RDML produces consistently better\nperformance than standard methods over a broad range of data types and\nclassification problems.",
  "text": "RMDL: Random Multimodel Deep Learning for Classification\nKamran Kowsari∗\nDepartment of System and\nInformation Engineering,\nUniversity of Virginia\nCharlottesville, VA, USA\nkk7nc@virginia.edu\nMojtaba Heidarysafa\nDepartment of System and\nInformation Engineering,\nUniversity of Virginia\nCharlottesville, VA, USA\nmh4pk@virginia.edu\nDonald E. Brown†‡\nDepartment of System and\nInformation Engineering,\nUniversity of Virginia\nCharlottesville, VA, USA\ndeb@virginia.edu\nKiana Jafari Meimandi\nDepartment of System and\nInformation Engineering,\nUniversity of Virginia\nCharlottesville, VA, USA\nkj6vd@virginia.edu\nLaura E. Barnes∗†\nDepartment of System and\nInformation Engineering,\nUniversity of Virginia\nCharlottesville, VA, USA\nlb3dp@virginia.edu\nABSTRACT\nThe continually increasing number of complex datasets each year\nnecessitates ever improving machine learning methods for robust\nand accurate categorization of these data. This paper introduces\nRandom Multimodel Deep Learning (RMDL): a new ensemble, deep\nlearning approach for classification. Deep learning models have\nachieved state-of-the-art results across many domains. RMDL solves\nthe problem of finding the best deep learning structure and archi-\ntecture while simultaneously improving robustness and accuracy\nthrough ensembles of deep learning architectures. RDML can accept\nas input a variety data to include text, video, images, and symbolic.\nThis paper describes RMDL and shows test results for image and\ntext data including MNIST, CIFAR-10, WOS, Reuters, IMDB, and\n20newsgroup. These test results show that RDML produces con-\nsistently better performance than standard methods over a broad\nrange of data types and classification problems.1\nCCS CONCEPTS\n• Information systems →Decision support systems, Data\nmining;\nKEYWORDS\nData Mining, Text Classification, Image Classification, Deep Neural\nNetworks, Deep Learning, Supervised Learning\n∗Sensing Systems for Health Lab, University of Virginia, Charlottesville, VA USA\n†Data Science Institute, University of Virginia Charlottesville, VA USA\n‡Predictive Technology Laboratory, University of Virginia, Charlottesville, VA USA\n1Code is shared as an open source tool at https://github.com/kk7nc/RMDL\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to the\nAssociation for Computing Machinery.\nACM ISBN 978-1-4503-6354-9/18/04...$15.00\nhttps://doi.org/10.1145/3206098.3206111\nACM Reference Format:\nKamran Kowsari, Mojtaba Heidarysafa, Donald E. Brown, Kiana Jafari\nMeimandi, and Laura E. Barnes. 2018. RMDL: Random Multimodel Deep\nLearning for Classification. In ICISDM ’18: 2018 2nd International Conference\non Information System and Data Mining ICISDM ’18, April 9–11, 2018, Lake-\nland, FL, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/\n3206098.3206111\n1\nINTRODUCTION\nCategorization and classification with complex data such as images,\ndocuments, and video are central challenges in the data science\ncommunity. Recently, there has been an increasing body of work\nusing deep learning structures and architectures for such problems.\nHowever, the majority of these deep architectures are designed\nfor a specific type of data or domain. There is a need to develop\nmore general information processing methods for classification and\ncategorization across a broad range of data types.\nWhile many researchers have successfully used deep learning\nfor classification problems (e.g., see [9, 23, 28, 30, 51]), the central\nproblem remains as to which deep learning architecture (DNN,\nCNN, or RNN) and structure (how many nodes (units) and hidden\nlayers) is more efficient for different types of data and applications.\nThe favored approach to this problem is trial and error for the\nspecific application and dataset.\nThis paper describes an approach to this challenge using ensem-\nbles of deep learning architectures. This approach, called Random\nMultimodel Deep Learning (RMDL), uses three different deep learn-\ning architectures: Deep Neural Networks (DNN), Convolutional\nNeural Netwroks (CNN), and Recurrent Neural Networks (RNN).\nTest results with a variety of data types demonstrate that this new\napproach is highly accurate, robust and efficient.\nThe three basic deep learning architectures use different feature\nspace methods as input layers. For instance, for feature extrac-\ntion from text, DNN uses term frequency-inverse document fre-\nquency (TF-IDF) [43]. RDML searches across randomly generated\nhyperparameters for the number of hidden layers and nodes (desity)\nin each hidden layer in the DNN. CNN has been well designed for\nimage classification. RMDL finds choices for hyperparameters in\narXiv:1805.01890v2  [cs.LG]  31 May 2018\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nK. Kowsari et al.\nCNN using random feature maps and random numbers of hidden\nlayers. CNN can be used for more than image data. The structures\nfor CNN used by RMDL are 1D convolutional layer for text, 2D\nfor images and 3D for video processings. RNN architectures are\nused primarily for text classification. RMDL uses two specific RNN\nstructures: Gated Recurrent Units (GRUs) and Long Short-Term\nMemory (LSTM). The number of GRU or LSTM units and hidden\nlayers used by the RDML are also the results of search over ran-\ndomly generated hyperparameters.\nThe main contributions of this work are as follows: I) Description\nof an ensemble approach to deep learning which makes the final\nmodel more robust and accurate. II) Use of different optimization\ntechniques in training the models to stabilize the classification\ntask. III) Different feature extraction approaches for each Random\nDeep Leaning (RDL) model in order to better understand the feature\nspace (specially for text and video data). IV) Use of dropout in each\nindividual RDL to address over-fitting. V) Use of majority voting\namong the n RDL models. This majority vote from the ensemble\nof RDL models improves the accuracy and robustness of results.\nSpecifically, if k number of RDL models produce inaccuracies or\noverfit classifications and n > k, the overall system is robust and\naccurate VI) Finally, the RMDL has ability to process a variety of\ndata types such as text, images and videos.\nThe rest of this paper is organized as follows: Section 2 gives\nrelated work for feature extraction, other classification techniques,\nand deep learning for classification task; Section 3 describes current\ntechniques for classification tasks which are used as our baseline;\nSection 4 describes Random Multimodel Deep Learning methods\nand the architecture for RMDL including Section 4.1 shows feature\nextraction in RMDL, Section 4.2 talks about overall view of RMDL;\nSection 4.3 addresses the deep learning structure used in this model,\nSection 4.4 discusses optimization problem; Section 5.1 talks about\nevaluation of these techniques; Section 5 shows the experimental\nresults which includes the accuracy and performance of RMDL;\nand finally, Section 6 presents discussion and conclusions of our\nwork.\n2\nRELATED WORK\nResearchers from a variety of disciplines have produced work\nrelevant to the approach described in this paper. We have orga-\nnized this work into three areas: I) Feature extraction; II) Classifica-\ntion methods and techniques (baseline and other related methods);\nand III) Deep learning for classification.\nFeature Extraction: Feature extraction is a significant part of\nmachine learning especially for text, image, and video data. Text\nand many biomedical datasets are mostly unstructured data from\nwhich we need to generate a meaningful and structures for use by\nmachine learning algorithms. As an early example, L. Krueger et. al.\nin 1979 [26] introduced an effective method for feature extraction\nfor text categorization. This feature extraction method is based on\nword counting to create a structure for statistical learning. Even ear-\nlier work by H. Luhn [34] introduced weighted values for each word\nand then G. Salton et. al. in 1988 [44] modified the weights of words\nby frequency counts called term frequency-inverse document fre-\nquency (TF-IDF). The TF-IDF vectors measure the number of times\na word appears in the document weighted by the inverse frequency\nof the commonality of the word across documents. Although, the\nTF-IDF and word counting are simple and intuitive feature extrac-\ntion methods, they do not capture relationships between words as\nsequences. Recently, T. Mikolov et. al. [36] introduced an improved\ntechnique for feature extraction from text using the concept of\nembedding or placing the word into a vector space based on con-\ntext. This approach to word embedding, called Word2Vec, solves the\nproblem of representing contextual word relationships in a com-\nputable feature space. Building on these ideas, J. Pennington et. al.\nin 2014 [41] developed a learning vector space representation of the\nwords called Glove and deployed it in Stanford NLP lab. The RMDL\napproach described in this paper uses Glove for feature extraction\nfrom textual data.\nClassification Methods and Techniques: Over the last 50\nyears, many supervised learning classification techniques have\nbeen developed and implemented in software to accurately label\ndata. For example, the researchers, K. Murphy in 2006 [38] and I.\nRish in 2001 [42] introduced the Naïve Bayes Classifier (NBC) as a\nsimple approach to the more general respresentation of the super-\nvised learning classification problem. This approach has provided\na useful technique for text classification and information retrieval\napplications. As with most supervised learning classification tech-\nniques, NBC takes an input vector of numeric or categorical data\nvalues and produce the probability for each possible output labels.\nThis approach is fast and efficient for text classification, but NBC\nhas important limitations. Namely, the order of the sequences in\ntext is not reflected on the output probability because for text analy-\nsis, naïve bayes uses a bag of words approach for feature extraction.\nBecause of its popularity, this paper uses NBC as one of the baseline\nmethods for comparison with RMDL. Another popular classification\ntechnique is Support Vector Machines (SVM), which has proven\nquite accurate over a wide variety of data. This technique con-\nstructs a set of hyper-planes in a transformed feature space. This\ntransformation is not performed explicitly but rather through the\nkernal trick which allows the SVM classifier to perform well with\nhighly nonlinear relationships between the predictor and response\nvariables in the data. A variety of approaches have been developed\nto further extend the basic methodology and obtain greater accu-\nracy. C. Yu et. al. in 2009 [54] introduced latent variables into the\ndiscriminative model as a new structure for SVM, and S. Tong et. al.\nin 2001 [50] added active learning using SVM for text classification.\nFor a large volume of data and datasets with a huge number of\nfeatures (such as text), SVM implementations are computationally\ncomplex. Another technique that helps mediate the computational\ncomplexity of the SVM for classification tasks is stochastic gradient\ndescent classifier (SGDClassifier) [18] which has been widely used\nin both text and image classification. SGDClassifier is an iterative\nmodel for large datasets. The model is trained based on the SGD\noptimizer iteratively.\nDeep Learning: Neural networks derive their architecture as\na relatively simply representation of the neurons in the human’s\nbrain. They are essentially weighte combinations of inputs the\npass through multiple non-linear functions. Neural networks use\nan iterative learning method known as back-propagation and an\noptimizer (such as stochastic gradient descent (SGD)).\nDeep Neural Networks (DNN) are based on simple neural net-\nworks architectures but they contain multiple hidden layers. These\nnetworks have been widely used for classification. For example, D.\nRMDL: Random Multimodel Deep Learning for Classification\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nCireşAn et. al. in 2012 [10] used multi-column deep neural networks\nfor classification tasks, where multi-column deep neural networks\nuse DNN architectures. Convolutional Neural Networks (CNN)\nprovide a different architectural approach to learning with neural\nnetworks. The main idea of CNN is to use feed-forward networks\nwith convolutional layers that include local and global pooling lay-\ners. A. Krizhevsky in 2012 [25] used CNN, but they have used 2D\nconvolutional layers combined with the 2D feature space of the\nimage. Another example of CNN in [28] showed excellent accu-\nracy for image classification. This architecture can also be used\nfor text classification as shown in the work of [21]. For text and\nsequences, 1D convolutional layers are used with word embeddings\nas the input feature space. The final type of deep learning architec-\nture is Recurrent Neural Networks (RNN) where outputs from the\nneurons are fed back into the network as inputs for the next step.\nSome recent extensions to this architecture uses Gated Recurrent\nUnits (GRUs) [9] or Long Short-Term Memory (LSTM) units [15].\nThese new units help control for instability problems in the original\nnetwork architecure. RNN have been successfully used for natural\nlanguage processing [37]. Recently, Z. Yang et. al. in 2016 [53] devel-\noped hierarchical attention networks for document classification.\nThese networks have two important characteristics: hierarchical\nstructure and an attention mechanism at word and sentence level.\nNew work has combined these three basic models of the deep\nlearning structure and developed a novel technique for enhancing\naccuracy and robustness. The work of M. Turan et. al. in 2017 [51]\nand M. Liang et. al.in 2015 [33] implemented innovative combina-\ntions of CNN and RNN called A Recurrent Convolutional Neural\nNetwork (RCNN). K. Kowsari et. al. in 2017 [23] introduced hier-\narchical deep learning for text classification (HDLTex) which is\na combination of all deep learning techniques in a hierarchical\nstructure for document classification has improved accuracy over\ntraditional methods. The work in this paper builds on these ideas,\nspcifically the work of [23] to provide a more general approach to\nsupervised learning for classification.\n3\nBASELINE\nIn this paper, we use both contemporary and traditional techniques\nof document and image classification as our baselines. The baselines\nof image and text classification are different due to feature extrac-\ntion and structure of model; thus, text and image classification’s\nbaselines are described separately in the following section.\n3.1\nText Classification Baselines\nText classification techniques which are used as our baselines\nto evaluate our model are as follows: regular deep models such\nas Recurrent Neural Networks (RNN), Convolutional Neural Net-\nworks (CNN), and Deep Neural Networks (DNN). Also, we have\nused two different techniques of Support Vector Machine (SVM),\nnaïve bayes classification (NBC), and finally Hierarchical Deep\nLearning for Text Classification (HDLTex) [23].\n3.1.1\nDeep Learning. The baseline, we used in this paper is Deep\nLearning without hierarchical levels. An example of hierarchical\nlevels’ structure is [53] that has been used as one of our baselines\nfor text classification. In our methods’ Section 4, we will explain\nthe basic models of deep learning such as DNN, CNN, and RNN\nwhich are used as part of RMDL model.\n3.1.2\nSupport Vector Machine (SVM). The original version of\nSVM was introduced by Vapnik, VN and Chervonenkis, A Ya [6] in\n1963. The early 1990s, nonlinear version was addressed in [3].\nMulti-class SVM. The original version of SVM is used for binary\nclassification, so for multi class we need to generate Multimodel or\nMSVM. One-Vs-One is a technique for multi-class SVM and needs\nto build N(N-1) classifiers.\nThe natural way to solve k-class problem is to construct a de-\ncision function of all k classes at once [5, 52]. Another technique\nof multi-class classification using SVM is All-against-One. In SVM,\nmany different methods are available for feature extraction such\nas word sequences feature extracting [55], and Term frequency-\ninverse document frequency (TF-IDF).\nString Kernel. The basic idea of String Kernel (SK) is using Φ(.)\nfor mapping string in the feature space; therefore, the only different\nbetween the three techniques are the way they map the string into\nfeature space. For many applications such as text, DNA, and protein\nclassification, Spectrum Kernel (SP) is addressed [13, 32]. The basic\nidea of SP is counting number of time a word appears in string xi\nas feature map where defining feature maps from x →Rlk\nMismatch Kernel is the other stable way to map the string into\nfeature space. The key idea is using k which stands for k −mer or\nsize of the word and allow to havem mismatch in feature space [31].\nThe main problem of SVM for string sequences is time complexity\nof these models. S. Ritambhara et. al. in 2017 [47] addressed the\nproblem of time for gap k-mers kernel called GaKCo which is used\nonly for protein and DNA sequences.\n3.1.3\nStacking Support Vector Machine (SVM). Stacking SVMs\nis used as another baseline method for comparison with RMDL, but\nthis technique is used only for hierarchical labeled datasets. The\nstacking SVM provides an ensemble of individual SVM classifiers\nand generally produces more accurate results than single-SVM\nmodels [46, 48].\n3.1.4\nNaïve Bayes Classification (NBC). This technique has been\nused in industry and academia for a long time, and it is the most\ntraditional method of text categorization which is widely used in\nInformation Retrieval [35]. If the number of n documents, fit into k\ncategories, the predicted class as output is c ∈C. Naïve bayes is a\nsimple algorithm using naïve bayes rule described as follows:\nP(c | d) = P(d | c)P(c)\nP(d)\n(1)\nwhere d is document, c indicates classes.\nCMAP = arд max\nc ∈C P(d | c)P(c)\n= arд max\nc ∈C P(x1,x2, ...,xn | c)p(c)\n(2)\nThe baseline of this paper is word level of NBC [20] as follows:\nP(cj | di; ˆθ) = P(cj | ˆθ)P(di | cj; ˆθj)\nP(di | ˆθ)\n(3)\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nK. Kowsari et al.\n3.1.5\nHierarchical Deep Learning for Text Classification (HDL-\nTex). This technique is used as one of our baselines for hierarchical\nlabeled datasets. When documents are organized hierarchically,\nmulti-class approaches are difficult to apply using traditional su-\npervised learning methods. The HDLTex [23] introduced a new\napproach to hierarchical document classification that combines\nmultiple deep learning approaches to produce hierarchical classifi-\ncation. The primary contribution of HDLTex research is hierarchical\nclassification of documents. A traditional multi-class classification\ntechnique can work well for a limited number of classes, but per-\nformance drops with increasing number of classes, as is present in\nhierarchically organized documents. HDLTex solved this problem\nby creating architectures that specialize deep learning approaches\nfor their level of the document hierarchy.\n3.2\nImage Classification Baselines\nFor image classification, we have five baselines as follows: Deep\nL2-SVM [49], Maxout Network [14], BinaryConnect [11], PCANet-\n1 [4], and gcForest [56].\nDeep L2-SVM: This technique is known as deep learning using linear\nsupport vector machines which simply softmax is replaced with\nlinear SVMs [49].\nMaxout Network: I. Goodfellow et. al. in 2013 [14] defined a simple\nnovel model called maxout (named because its outputs’ layer is a\nset of max of inputs’ layer, and it is a natural companion to dropout).\nTheir design both facilitates optimization by using dropout, and\nalso improves the accuracy of dropout’s model.\nBinaryConnect: M. Courbariaux et. al. in 2015 [11] worked on train-\ning Deep Neural Networks (DNN) with binary weights during prop-\nagations. They have introduced a binarization scheme for binary\nweights during forward and backward propagations (BinaryConnect)\nwhich is mainly used for image classification. BinaryConnect is\nused as our baseline for RMDL on image classification.\nPCANet: I. Chan et. al. in 2015 [4] is simple way of deep learning\nfor image classification which uses CNN structure. Their technique\nis one of the basic and efficient methods of deep learning. The CNN\nstructure they’ve used, is part of RMDL with significant differences\nthat they use: I) cascaded principal component analysis (PCA); II)\nbinary hashing; and III) blockwise histograms, and also number of\nhidden layers and nodes in RMDL is selected automatically.\ngcForest (Deep Forest): Z. Zhou et. al. in 2017 [56] introduced a\ndecision tree ensemble approach with high performance as an al-\nternative to deep neural networks. Deep forest creates multi level\nof forests as decision trees.\n4\nMETHOD\nThe novelty of this work is in using multi random deep learning\nmodels including DNN, RNN, and CNN techniques for text and\nimage classification. The method section of this paper is organized\nas follows: first we describe RMDL and we discuss three techniques\nof deep learning architectures (DNN, RNN, and CNN) which are\ntrained in parallel. Next, we talk about multi optimizer techniques\nthat are used in different random models.\n4.1\nFeature Extraction and Data Pre-processing\nThe feature extraction is divided into two main parts for RMDL (Text\nand image). Text and sequential datasets are unstructured data,\nwhile the feature space is structured for image datasets.\n4.1.1\nImage and 3D Object Feature Extraction. Image features\nare the followings: h × w × c where h denotes the height of the\nimage, w represents the width of image, and c is the color that has 3\ndimensions (RGB). For gray scale datasets such as MNIST dataset,\nthe feature space is h × w. A 3D object in space contains n cloud\npoints in space and each cloud point has 6 features which are (x, y, z,\nR, G, and B). The 3D object is unstructured due to number of cloud\npoints since one object could be different with others. However,\nwe could use simple instance down/up sampling to generate the\nstructured datasets.\n4.1.2\nText and Sequences Feature Extraction. In this paper we\nuse several techniques of text feature extraction which are word\nembedding (GloVe and Word2vec) and also TF-IDF. In this paper,\nwe use word vectorization techniques [16] for extracting features;\nBesides, we also can use N-gram representation as features for\nneural deep learning [12, 19]. For example, feature extraction in\nthis model for the string \"In this paper we introduced this technique\"\nwould be composed of the following:\n• Feature count(1) { (In 1) , (this 2), (paper 1), (we 1), (introduced\n1), (technique 1) }\n• Feature count(2) { (In 1) , (this 2), (paper 1), (we 1), (introduced\n1), (technique 1), (In this 1), (This Paper 1), ( paper we 1), (\nwe introduced 1), (introduced this 1), ( this technique 1) }\nDocuments enter our models via features extracted from the text.\nWe employed different feature extraction approaches for the deep\nlearning architectures we built. For CNN and RNN, we used the\ntext vector-space models using 200 dimensions as described in\nGloVe [41]. A vector-space model is a mathematical mapping of the\nword space, defined as follows:\ndj = (w1,j,w2,j, ...,wi,j...,wlj,j)\n(4)\nwhere lj is the length of the document j, and wi,j is the GloVe word\nembedding vectorization of word i in document j.\n4.2\nRandom Multimodel Deep Learning\nRandom Multimodel Deep Learning is a novel technique that we\ncan use in any kind of dataset for classification. An overview of this\ntechnique is shown in Figure 2 which contains multi Deep Neural\nNetworks (DNN), Deep Convolutional Neural Networks (CNN),\nand Deep Recurrent Neural Networks (RNN). The number of layers\nand nodes for all of these Deep learning multi models are gener-\nated randomly (e.g. 9 Random Models in RMDL constructed of 3\nCNNs, 3 RNNs, and 3 DNNs, all of them are unique due to randomly\ncreation).\nM(yi1,yi2, ...,yin) =\n\u0016 1\n2 +\n(Ín\nj=1 yij) −1\n2\nn\n\u0017\n(5)\nWhere n is the number of random models, and yij is the output\nprediction of model for data point i in model j (Equation 5 is used\nfor binary classification, k ∈{0 or 1}). Output space uses majority\nRMDL: Random Multimodel Deep Learning for Classification\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nRDL 1\nRDL 2\nRDL 3\nRDL n\nC=1\nC=2\nC=k\nC=1\nC=2\nC=k\nC=1\nC=2\nC=k\nC=1\nC=2\nC=k\n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n𝑥0\n𝑥1\n𝑥𝑓−1 𝑥𝑓 \n1\n2\n3\nk\n1\n2\n3\nk\n1 2 3\nk\n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \nVoting for 𝑋1  \nVoting for 𝑋2  \nVoting for 𝑋𝑚  \nInput\nRMDL\nMajority Voting \nBased on n RDLs\n. . . \nFigure 1: Overview of RDML: Random Multimodel Deep Learning for classification that includes n Random models which are\nd random model of DNN classifiers, c models of CNN classifiers, and r RNN classifiers where r + c + d = n.\nvote for final ˆyi. Therefore, ˆyi is given as follows:\nˆyi =\n\u0002 ˆyi1 . . . ˆyij . . . ˆyin\n\u0003T\n(6)\nWhere n is number of random model, and ˆyij shows the predic-\ntion of label of document or data point of Di ∈{xi,yi } for model j\nand ˆyi,j is defined as follows:\nˆyi,j = arд max\nk [sof tmax(y∗\ni,j)]\n(7)\nAfter all RDL models (RMDL) are trained, the final prediction is\ncalculated using majority vote of these models.\n4.3\nDeep Learning in RMDL\nThe RMDL model structure (section 4.2) includes three basic archi-\ntectures of deep learning in parallel. We describe each individual\nmodel separately. The final model contains d random DNNs (Sec-\ntion 4.3.1),r RNNs (Section 4.3.2), andc CNNs models (Section 4.3.3).\n4.3.1\nDeep Neural Networks. Deep Neural Networks’ structure\nis designed to learn by multi connection of layers that each layer\nonly receives connection from previous and provides connections\nonly to the next layer in hidden part. The input is a connection of\nfeature space with first hidden layer for all random models. The\noutput layer is number of classes for multi-class classification and\nonly one output for binary classification. But our main contribution\nof this paper is that we have many training DNN for different\npurposes. In our techniques, we have multi-classes DNNs where\neach learning models is generated randomly (number of nodes\nin each layer and also number of layers are completely random\nassigned). Our implementation of Deep Neural Networks (DNN) is\ndiscriminative trained model that uses standard back-propagation\nalgorithm using sigmoid (equation 8), ReLU [39] (equation 9) as\nactivation function. The output layer for multi-class classification,\nshould use Sof tmax equation 10.\nf (x) =\n1\n1 + e−x ∈(0, 1)\n(8)\nf (x) = max(0,x)\n(9)\nσ(z)j =\nezj\nÍK\nk=1 ezk\n(10)\n∀j ∈{1, . . . ,K}\nGiven a set of example pairs (x,y),x ∈X,y ∈Y, the goal is to\nlearn from these input and target space using hidden layers. In text\nclassification, the input is string which is generated by vectorization\nof text. In Figure 2 the left model shows how DNN contribute in\nRMDL.\n4.3.2\nRecurrent Neural Networks (RNN). Another neural net-\nwork architecture that contributes in RMDL is Recurrent Neural\nNetworks (RNN). RNN assigns more weights to the previous data\npoints of sequence. Therefore, this technique is a powerful method\nfor text, string and sequential data classification but also could be\nused for image classification as we did in this work. In RNN the\nneural net considers the information of previous nodes in a very\nsophisticated method which allows for better semantic analysis of\nstructures of dataset. General formulation of this concept is given\nin Equation 11 where xt is the state at time t and ut refers to the\ninput at step t.\nxt = F(xt−1,ut ,θ)\n(11)\nMore specifically, we can use weights to formulate the Equa-\ntion 11 with specified parameters in Equation 12\nxt = Wrecσ(xt−1) + Winut + b\n(12)\nWhere Wrec refers to recurrent matrix weight, Win refers to input\nweights, b is the bias and σ denotes an element-wise function.\nAgain, we have modified the basic architecture for use RMDL.\nFigure 2 left side shows this extended RNN architecture. Several\nproblems arise from RNN when the error of the gradient descent\nalgorithm is back propagated through the network: vanishing gra-\ndient and exploding gradient [2].\nLong Short-Term Memory (LSTM): To deal with these problems\nLong Short-Term Memory (LSTM) is a special type of RNN that\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nK. Kowsari et al.\n...\n...\n...\n...\n...\n...\n...\n...\n...\n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n𝑥0\n𝑥1\n𝑥𝑖\n𝑥𝑓−1 𝑥𝑓 \n𝑦0\n𝑦1\n𝑦𝑖\n𝑦𝑚−1 𝑦𝑚 \n. . . \n. . . \n𝑦0\n𝑦1\n𝑦𝑚−1 𝑦𝑚 \n. . . \n𝑥0\n𝑥1\n𝑥𝑓−1 𝑥𝑓 \nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\nLSTM\nGRU\n...\n...\n...\n. . . \n. . . \n. . . \n𝑦0\n𝑦1\n𝑦𝑚−1 𝑦𝑚 \n. . . \n𝑥0\n𝑥1\n𝑥𝑓 \n. . . \n𝑦0\n𝑦1\n𝑦𝑚−1 𝑦𝑚 \n. . . \n𝑥0\n𝑥1\n𝑥𝑓−1 𝑥𝑓 \n. . . \nInput Layer\nHidden Layer\nOutput Layer\nn×k representation of \nsentence or image\nConvolutional layer \nwith multi filter\nMax-over-time\npooling\nHidden Layer of LSTM or GRU\nFigure 2: Random Multimodel Deep Learning (RDML) architecture for classification which includes 3 Random models, a DNN\nclassifier at left, a Deep CNN classifier at middle, and a Deep RNN classifier at right (each unit could be LSTM or GRU).\npreserve long term dependency in a more effective way in com-\nparison to the basic RNN. This is particularly useful to overcome\nvanishing gradient problem [40]. Although LSTM has a chain-like\nstructure similar to RNN, LSTM uses multiple gates to carefully\nregulate the amount of information that will be allowed into each\nnode state. Figure 3 shows the basic cell of a LSTM model. A step\nby step explanation of a LSTM cell is as following:\nit =σ(Wi[xt,ht−1] + bi),\n(13)\n˜Ct = tanh(Wc[xt,ht−1] + bc),\n(14)\nft =σ(Wf [xt,ht−1] + bf ),\n(15)\nCt =it ∗˜Ct + ftCt−1,\n(16)\not =σ(Wo[xt,ht−1] + bo),\n(17)\nht =ot tanh(Ct ),\n(18)\nWhere equation 13 is input gate, Equation 14 shows candid memory\ncell value, Equation 15 is forget gate activation, Equation 16 is new\nmemory cell value, and Equation 17 and 18 show output gate value.\nIn the above description all b represents bias vectors and all W rep-\nresent weight matrices and xt is used as input to the memory cell\nat time t. Also, i,c, f ,o indices refer to input, cell memory, forget\nand output gates respectively. Figure 3 shows the structure of these\ngates with a graphical representation.\nAn RNN can be biased when later words are more influential than\nthe earlier ones. To overcome this bias Convolutional Neural Net-\nwork (CNN) models (discussed in Subsection 4.3.3 were introduced\nwhich deploys a max-pooling layer to determine discriminative\nphrases in a text [27].\nGated Recurrent Unit (GRU): Gated Recurrent Unit (GRU) is\na gating mechanism for RNN which was introduced by [9] and [7].\nGRU is a simplified variant of the LSTM architecture, but there\nare differences as follows: GRU contains two gates, a GRU does\nnot possess internal memory (the Ct−1 in Figure 3); and finally, a\nsecond non-linearity is not applied (tanh in Figure 3). A step by\nstep explanation of a GRU cell is as following:\nzt = σд(Wzxt + Uzht−1 + bz),\n(19)\nWhere zt refers to update gate vector of t, xt stands for input vec-\ntor, W , U and b are parameter matrices and vector, σд is activation\nfunction that could be sigmoid or ReLU.\n˜rt = σд(Wrxt + Urht−1 + br ),\n(20)\nht = zt ◦ht−1 + (1 −zt ) ◦σh(Whxt + Uh(rt ◦ht−1) + bh)\n(21)\nWhere ht is output vector of t, rt stands for reset gate vector of t,\nzt is update gate vector of t, σh indicates the hyperbolic tangent\nfunction.\n4.3.3\nConvolutional Neural Networks (CNN). The final deep\nlearning approach which contributes in RMDL is Convolutional\nNeural Networks (CNN) that is employed for document or image\nclassification. Although originally built for image processing with\narchitecture similar to the visual cortex, CNN have also been effec-\ntively used for text classification [29]; thus, in RMDL, this technique\nis used in all datasets.\nIn the basic CNN for image processing an image tensor is convolved\nwith a set of kernels of size d × d. These convolution layers are\ncalled feature maps and can be stacked to provide multiple filters\non the input. To reduce the computational complexity CNN use\npooling which reduces the size of the output from one layer to\nRMDL: Random Multimodel Deep Learning for Classification\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nthe next in the network. Different pooling techniques are used to\nreduce outputs while preserving important features [45]. The most\ncommon pooling method is max pooling where the maximum ele-\nment is selected in the pooling window.\nIn order to feed the pooled output from stacked featured maps to\nthe final layer, the maps are flattened into one column. The final\nlayers in a CNN are typically fully connected.\nIn general, during the back propagation step of a convolutional neu-\nral network not only the weights are adjusted but also the feature\ndetector filters. A potential problem of CNN used for text is the num-\nber of ’channels’, Σ (size of the feature space). This might be very\nlarge (e.g. 50K), for text but for images this is less of a problem (e.g.\nonly 3 channels of RGB) [17]. This means the dimensionality of the\nCNN for text is very high.\n4.4\nOptimization\nIn this paper we use two types of stochastic gradient optimizer\nin our neural networks implementation which are RMSProp and\nAdam optimizer:\n4.4.1\nStochastic Gradient Descent (SGD) Optimizer. SGD has\nbeen used as one of our optimizer that is shown in equation 22. It\nuses a momentum on re-scaled gradient which is shown in equa-\ntion 23 for updating parameters. The other technique of optimizer\nthat is used is RMSProp which does not do bias correction. This\nwill be a significant problem while dealing with sparse gradient.\nθ ←θ −α∇θ J(θ,xi,yi)\n(22)\nθ ←θ −\u0000γθ + α∇θ J(θ,xi,yi)\u0001\n(23)\n4.4.2\nAdam Optimizer. Adam is another stochastic gradient\noptimizer which uses only the first two moments of gradient (v\nand m that are shown in equation 24, 25, 26, and 27) and average\nover them. It can handle non-stationary of objective function as in\nRMSProp while overcoming the sparse gradient issue that was a\ntanh\n𝜎 \n𝜎 \n𝑥𝑡 \nℎ𝑡−1\nℎ𝑡\nℎ𝑡 \ntanh\ntanh\n𝜎 \n𝜎 \n𝜎 \n𝑥𝑡 \n𝐶𝑡−1\nℎ𝑡−1\nℎ𝑡\n𝐶𝑡 \nℎ𝑡 \n1-\nFigure 3: Top Figure is a cell of GRU, and bottom Figure is a\ncell of LSTM\nFigure 4: This figure Shows multi SGD optimizer\ndrawback in RMSProp [22].\nθ ←θ −\nα\n√\nˆv + ϵ\nˆm\n(24)\nдi,t = ∇θ J(θi,xi,yi)\n(25)\nmt = β1mt−1 + (1 −β1)дi,t\n(26)\nmt = β2vt−1 + (1 −β2)д2\ni,t\n(27)\nWhere mt is the first moment and vt indicates second moment that\nboth are estimated. ˆ\nmt =\nmt\n1−β t\n1 and ˆvt =\nvt\n1−β t\n2\n4.4.3\nMulti Optimization rule. The main idea of using multi\nmodel with different optimizers is that if one optimizer does not\nprovide a good fit for a specific datasets, the RMDL model with n\nrandom models (some of them might use different optimizers) could\nignore k models which are not efficient if and only if n > k. The\nFigure 4 provides a visual insight on how three optimizers work\nbetter in the concept of majority voting. Using multi techniques of\noptimizers such as SGD, adam, RMSProp, Adagrad, Adamax, and so\non helps the RMDL model to be more stable for any type of datasets.\nIn this research, we only used two optimizers (Adam and RMSProp)\nfor evaluating our model, but the RMDL model has the capability\nto use any kind of optimizer.\n5\nEXPERIMENTAL RESULTS\nIn this section, experimental results are discussed including evalua-\ntion of method, experimental setup, and datasets. Also, we discuss\nthe hardware and frameworks which are used in RMDL; finally, a\ncomparison between our empirical results and the baselines has\nbeen presented. Moreover, losses and accuracies of this model for\neach individual RDL (in each epoch) is shown in Figure 5.\n5.1\nEvaluation\nIn this work, we report accuracy and Micro F1-Score which are given\nas follows:\nPrecisionmicro =\nÍL\nl=1TPl\nÍL\nl=1TPl + FPl\n(28)\nRecallmicro =\nÍL\nl=1TPl\nÍL\nl=1TPl + FNl\n(29)\nF1 −Scoremicro =\nÍL\nl=1 2TPl\nÍL\nl=1 2TPl + FPl + FNl\n(30)\nHowever, the performance of our model is evaluated only in terms\nof F1-score for evaluation as in Tables 1 and 3. Formally, given I =\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nK. Kowsari et al.\n{1, 2, · · · ,k} a set of indices, we define the ith class as Ci. If we de-\nnote l = |I | and for TPi-true positive of Ci, FPi-false positive, FNi-\nfalse negative, and TNi-true negative counts respectively then the\nabove definitions apply for our multi-class classification problem.\n5.2\nExperimental Setup\nTwo types of datasets (text and image) has been used to test and\nevaluate our approach performance. However, in theory the model\nhas capability to solve classification problems with a variety of data\nincluding video, text, and images.\n5.2.1\nText Datasets. For text classification, we used 4 different\ndatasets, namely, WOS , Reuters, IMDB, and 20newsдroups.\nWeb Of Science (WOS) dataset [24] is a collection of academic arti-\ncles’ abstracts which contains three corpora (5736, 11967, and 46985\ndocuments) for (11, 34, and 134 topics).\nThe Reuters-21578 news dataset contains 10, 788 documents which\nare divided into 7, 769 documents for training and 3, 019 for testing\nwith total of 90 classes.\nIMDB dataset contains 50, 000 reviews that is splitted into a set\nof 25, 000 highly popular movie reviews for training, and 25, 000\nfor testing.\n20NewsGroup dataset includes 19, 997 documents with maximum\nlength of 1, 000 words. In this dataset, we have 15, 997 for training\nand 4, 000 samples are used for validation.\n5.2.2\nImage datasets. For image classification, two traditional\nand ground truth datasets are used, namely, MNIST hand writing\ndataset and CIFAR.\nMNIST: this dataset contains handwritten number k ∈{0, 1, ..., 9}\nand input feature space is in 28 × 28 × 1 format. The training\nand the test set contains 60, 000 and 10, 000 data point examples\nrespectively.\nCIFAR: This dataset consists of 60, 000 images with 32×32×3 format\nassigned in 10 classes, with 6, 000 images per class that is splitted\ninto 50, 000 training and 10, 000 test images. Classes are airplane,\nautomobile, bird, cat, deer, dog, frog, horse, ship, and truck.\nTable\n1:\nAccuracy\ncomparison\nfor\ntext\nclassification.\nW.1 (WOS-5736) refers to Web of Science dataset, W.2\nrepresents W-11967, W.3 is WOS-46985, and R stands for\nReuters-21578\nModel\nDataset\nW.1\nW.2\nW.3\nR\nBaseline\nDNN\n86.15\n80.02\n66.95\n85.3\nCNN [53]\n88.68\n83.29\n70.46\n86.3\nRNN [53]\n89.46\n83.96\n72.12\n88.4\nNBC\n78.14\n68.8\n46.2\n83.6\nSVM [55]\n85.54\n80.65\n67.56\n86.9\nSVM (TF-IDF) [5]\n88.24\n83.16\n70.22\n88.93\nStacking SVM [48]\n85.68\n79.45\n71.81\nNA\nHDLTex [23]\n90.42\n86.07\n76.58\nNA\nRMDL\n3 RDLs\n90.86\n87.39\n78.39\n89.10\n9 RDLs\n92.60\n90.65\n81.92\n90.36\n15 RDLs\n92.66\n91.01\n81.86\n89.91\n30 RDLs\n93.57\n91.59\n82.42\n90.69\nTable\n2:\nError\nrate\ncomparison\nfor\nImage\nclassifica-\ntion (MNIST and CIFAR-10 datasets)\nMethods\nMNIST\nCIFAR-10\nBaseline\nDeep L2-SVM [49]\n0.87\n11.9\nMaxout Network [14]\n0.94\n11.68\nBinaryConnect [11]\n1.29\n9.90\nPCANet-1 [4]\n0.62\n21.33\ngcForest [56]\n0.74\n31.00\nRMDL\n3 RDLs\n0.51\n9.89\n9 RDLs\n0.41\n9.1\n15 RDLs\n0.21\n8.74\n30 RDLs\n0.18\n8.79\n5.3\nHardware\nAll of the results shown in this paper are performed on Central Pro-\ncess Units (CPU) and Graphical Process Units (GPU). Also, RMDL\ncan be implemented using only GPU, CPU, or both. The process-\ning units that has been used through this experiment was intel on\nXeon E5-2640 (2.6 GHz) with 12 cores and 64 GB memory (DDR3).\nAlso, we have used three graphical cards on our machine which\nare two Nvidia GeForce GTX 1080 Ti and Nvidia Tesla K20c.\n5.4\nFramework\nThis work is implemented in Python using Compute Unified Device\nArchitecture (CUDA) which is a parallel computing platform and\nApplication Programming Interface (API) model created by Nvidia.\nWe used TensorFelow and Keras library for creating the neural\nnetworks [1, 8].\n5.5\nEmpirical Results\n5.5.1\nImage classification. Table 2 shows the error rate of RMDL\nfor image classification. The comparison between the RMDL with\nbaselines (as described in Section 3.2), shows that the error rate\nof the RMDL for MNIST dataset has been improved to 0.51, 0.41,\nand 0.21 for 3, 9 and 15 random models respectively. For the CIFAR-\n10 datasets, the error rate has been decreased for RMDL to 9.89, 9.1,\n8.74, and 8.79,using 3, 9, 15, and 30 RDL respectively.\n5.5.2\nDocument categorization. Table 1 shows that for four\nground truth datasets, RMDL improved the accuracy in comparison\nto the baselines. In Table 1, we evaluated our empirical results by\nfour different RMDL models (using 3, 9, 15, and 30 RDLs). For Web of\nScience (WOS-5,736) the accuracy is improved to 90.86, 92.60, 92.66,\nand 93.57 respectively. For Web of Science (WOS-11,967), the ac-\ncuracy is increased to 87.39, 90.65, 91.01, and 91.59 respectively,\nand for Web of Science (WOS-46,985) the accuracy has increased\nto 78.39, 81.92, 81.86, and 82.42 respectively. The accuracy of Reuters-\n21578 is 88.95, 90.29, 89.91, and 90.69 respectively. We report results\nfor other ground truth datasets such as Large Movie Review Dataset\n(IMDB) and 20NewsGroups. As it is mentioned in Table 3, for two\nground truth datasets, RMDL improves the accuracy. In Table 3,\nwe evaluated our empirical results of two datasets (IMDB reviewer\nand 20NewsGroups).The accuracy of IMDB dataset is 89.91, 90.13,\nand 90.79 for 3, 9, and 15 RDLs respectively, whereas the accuracy\nof DNN is 88.55%, CNN [53] is 87.44%, RNN [53] is 88.59%, Naïve\nBayes Classifier is 83.19%, SVM [55] is 87.97%, and SVM [5] using\nRMDL: Random Multimodel Deep Learning for Classification\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nepoch\nepoch\nloss\nTest\nTrain\nRDL1\nRDL2\nRDL3\nRDL4\nRDL5\nRDL6\nRDL7\nRDL8\nRDL9\nRDL10\nRDL11\nRDL12\nRDL13\nRDL14\nRDL15\nCIFAR\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.05\n0\n20\n40\n60\n80\n100\n120\nepoch\nRDL1\nRDL2\nRDL3\nRDL4\nRDL5\nRDL6\nRDL7\nRDL8\nRDL9\nRDL10\nRDL11\nRDL12\nRDL13\nRDL14\nRDL15\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n0.05\n0\n20\n40\n60\n80\n100\n120\nepoch\nloss\nMNIST\n(a) This sub-figure indicates MNIST and CIFAR-10 loss function for\n15 Random Deep Learning (RDL) model. The MNNST shown as 120\nepoch and CIFAR has 200 epoch\nepoch\nTest\nTrain\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n1.0\n0.8\n0.6\n0.4\n0.2\n0   \n      20\n     40             60            80\n        100 \nepoch\n0   \n      20\n     40             60            80\n        100 \nAccuracy\n0   \n      20\n     40             60            80\n        100 \n1.0\n0.9\n0.8\n0.7\n0.6\n0.4\n0.9\n0.8\n0.7\n0.6\n0.4\nAccuracy\n0   \n      20\n     40             60            80\n        100 \nWOS-5736\nReuters-21578\nepoch\nepoch\n(b) This sub-figure indicates WOS-5736 (Web Of Science dataset with 11\ncategories and 5736 documents) accuracy function for 9 Random Deep\nLearning (RDL) model, and bottom figure indicates Reuters-21578 ac-\ncuracy function for 9 Random Deep Learning (RDL) model\nFigure 5: This figure shows results of individual RDLs (accuracy and loss) for each epoch as part of RMDL.\nTF-IDF is equal to 88.45%. The accuracy of 20NewsGroup dataset\nis 86.73%, 87.62%, and 87.91% for 3, 9, and 15 random models respec-\ntively, whereas the accuracy of DNN is 86.50%, CNN [53] is 82.91%,\nRNN [53] is 83.75%, Naïve Bayes Classifier is 81.67%, SVM [55]\nis 84.57%, and SVM [5] using TF-IDF is equal to 86.00%.\nFigure 5 indicates accuracies and losses of RMDL which are\nshown with 9 (RDLs) for text classification and 15 RDLs for image\nclassification. As shown in Figure 5a, 4 RDLs’ loss of MNIST dataset\nare increasing over each epoch (RDL 6, RDL 9, RDL 14 and RDL 15)\nafter 40 epochs, but RMDL model contains 15 RDL models; thus,\nthe accuracy of the majority votes for these models as presented in\nTable 2 is competing with our baselines.\nIn Figure 5a, for CIFAR dataset, the models do not have overfitting\nproblem, but for MNIST datasets at least 4 models’ losses are in-\ncreasing over each epoch after 40 iterations (RDL 4, RDL 5, RDL 6,\nand RDL 9); although the accuracy and F1-measure of these 4 mod-\nels will drop after 40 epochs, the majority votes’ accuracy is robust\nTable 3: Accuracy comparison for text classification on\nIMDB and 20NewsGroup datasets\nModel\nDataset\nIMDB\n20NewsGroup\nBaseline\nDNN\n88.55\n86.50\nCNN [53]\n87.44\n82.91\nRNN [53]\n88.59\n83.75\nNaïve Bayes Classifier\n83.19\n81.67\nSVM [55]\n87.97\n84.57\nSVM(TF-IDF) [5]\n88.45\n86.00\nRMDL\n3 RDLs\n89.91\n86.73\n9 RDLs\n90.13\n87.62\n15 RDLs\n90.79\n87.91\nand efficient which means RMDL will ignore them due to major-\nity votes between 15 models. The Figure 5a shows the loss value\nover each epoch of two ground truth datasets, CIFAR and IMDB\nfor 15 random deep learning models (RDL). Figure 5b presents the\naccuracy of 15 random models for Reuters-21578 respectively. In\nFigure 5b, the accuracy of Random Deep Learning (RDLs) model is\naddressed over each epoch for WOS-5736 (Web Of Science dataset\nwith 17 categories and 5, 736 documents), the majority votes of\nthese models as shown in Table 1 is competing with our baselines.\n6\nDISCUSSION AND CONCLUSION\nThe classification task is an important problem to address in ma-\nchine learning, given the growing number and size of datasets\nthat need sophisticated classification. We propose a novel tech-\nnique to solve the problem of choosing best technique and method\nout of many possible structures and architectures in deep learn-\ning. This paper introduces a new approach called RMDL (Random\nMultimodel Deep Learning) for the classification that combines\nmulti deep learning approaches to produce random classification\nmodels. Our evaluation on datasets obtained from the Web of Sci-\nence (WOS), Reuters, MNIST, CIFAR, IMDB, and 20NewsGroups\nshows that combinations of DNNs, RNNs and CNNs with the par-\nallel learning architecture, has consistently higher accuracy than\nthose obtained by conventional approaches using naïve Bayes, SVM,\nor single deep learning model. These results show that deep learn-\ning methods can provide improvements for classification and that\nthey provide flexibility to classify datasets by using majority vote.\nThe proposed approach has the ability to improve accuracy and\nefficiency of models and can be use across a wide range of data\ntypes and applications.\nICISDM ’18, April 9–11, 2018, Lakeland, FL, USA\nK. Kowsari et al.\nREFERENCES\n[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig\nCitro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, and others.\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\nsystems. arXiv preprint arXiv:1603.04467 (2016).\n[2] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term\ndependencies with gradient descent is difficult. IEEE transactions on neural\nnetworks 5, 2 (1994), 157–166.\n[3] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. 1992. A training\nalgorithm for optimal margin classifiers. In COLT92. ACM, 144–152.\n[4] Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma.\n2015. PCANet: A simple deep learning baseline for image classification? IEEE\nTransactions on Image Processing 24, 12 (2015), 5017–5032.\n[5] Kewen Chen, Zuping Zhang, Jun Long, and Hao Zhang. 2016. Turning from\nTF-IDF to TF-IGM for term weighting in text classification. Expert Systems with\nApplications 66 (2016), 245–260.\n[6] Alexey Ya Chervonenkis. 2013. Early history of support vector machines. In\nEmpirical Inference. Springer, 13–20.\n[7] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine translation.\narXiv preprint arXiv:1406.1078 (2014).\n[8] François Chollet and others. 2015. Keras: Deep learning library for theano and\ntensorflow. URL: https://keras. io/k (2015).\n[9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.\nEmpirical evaluation of gated recurrent neural networks on sequence modeling.\narXiv preprint arXiv:1412.3555 (2014).\n[10] Dan CireşAn, Ueli Meier, Jonathan Masci, and Jürgen Schmidhuber. 2012. Multi-\ncolumn deep neural network for traffic sign classification. Neural Networks 32\n(2012), 333–338.\n[11] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binarycon-\nnect: Training deep neural networks with binary weights during propagations.\nIn Advances in Neural Information Processing Systems. 3123–3131.\n[12] Kushal Dave, Steve Lawrence, and David M Pennock. 2003. Mining the peanut\ngallery: Opinion extraction and semantic classification of product reviews. In\nWWW. ACM, 519–528.\n[13] Eleazar Eskin, Jason Weston, William S Noble, and Christina S Leslie. 2002.\nMismatch string kernels for SVM protein classification. In Advances in neural\ninformation processing systems. 1417–1424.\n[14] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua\nBengio. 2013. Maxout networks. arXiv preprint arXiv:1302.4389 (2013).\n[15] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural\ncomputation 9, 8 (1997), 1735–1780.\n[16] Hajime Hotta, Masanobu Kittaka, and Masafumi Hagiwara. 2010. Word vector-\nization using relations among words for neural network. IEEE Transactions on\nElectronics, Information and Systems 130, 1 (2010), 75–82.\n[17] Rie Johnson and Tong Zhang. 2014. Effective use of word order for text catego-\nrization with convolutional neural networks. preprint arXiv:1412.1058 (2014).\n[18] Fasihul Kabir, Sabbir Siddique, Mohammed Rokibul Alam Kotwal, and Moham-\nmad Nurul Huda. 2015. Bangla text document categorization using stochastic\ngradient descent (sgd) classifier. In CCIP. IEEE, 1–4.\n[19] Vlado Kešelj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003. N-gram-\nbased author profiles for authorship attribution. In PACLING, Vol. 3. 255–264.\n[20] Sang-Bum Kim, Kyoung-Soo Han, Hae-Chang Rim, and Sung Hyon Myaeng. 2006.\nSome effective techniques for naive bayes text classification. IEEE transactions\non knowledge and data engineering 18, 11 (2006), 1457–1466.\n[21] Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv\npreprint arXiv:1408.5882 (2014).\n[22] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza-\ntion. arXiv preprint arXiv:1412.6980 (2014).\n[23] Kamran Kowsari, Donald E Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi,\nMatthew S Gerber, and Laura E Barnes. 2017. HDLTex: Hierarchical Deep Learn-\ning for Text Classification. In 2017 16th IEEE International Conference on Machine\nLearning and Applications (ICMLA). 364–371. DOI:https://doi.org/10.1109/ICMLA.\n2017.0-134\n[24] Kamran Kowsari, Donald E Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi,\nMatthew S Gerber, and Laura E Barnes. 2018. Web of Science Dataset. DOI:\nhttps://doi.org/10.17632/9rw3vkcfy4.6\n[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica-\ntion with deep convolutional neural networks. In Advances in neural information\nprocessing systems. 1097–1105.\n[26] Lester E Krueger and Ronald G Shapiro. 1979. Letter detection with rapid serial\nvisual presentation: Evidence against word superiority at feature extraction.\nJournal of Experimental Psychology: Human Perception and Performance 5, 4\n(1979), 657.\n[27] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent Convolutional\nNeural Networks for Text Classification.. In AAAI, Vol. 333. 2267–2273.\n[28] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature\n521, 7553 (2015), 436–444.\n[29] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-\nbased learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–\n2324.\n[30] Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. 2009. Convo-\nlutional deep belief networks for scalable unsupervised learning of hierarchical\nrepresentations. In Proceedings of the 26th annual international conference on\nmachine learning. ACM, 609–616.\n[31] Christina S Leslie, Eleazar Eskin, Adiel Cohen, Jason Weston, and William Stafford\nNoble. 2004. Mismatch string kernels for discriminative protein classification.\nBioinformatics 20, 4 (2004), 467–476.\n[32] Christina S Leslie, Eleazar Eskin, and William Stafford Noble. 2002. The spectrum\nkernel: A string kernel for SVM protein classification.. In Pacific symposium on\nbiocomputing, Vol. 7. 566–575.\n[33] Ming Liang and Xiaolin Hu. 2015. Recurrent convolutional neural network for\nobject recognition. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 3367–3375.\n[34] Hans Peter Luhn. 1957. A statistical approach to mechanized encoding and\nsearching of literary information. IBM Journal of research and development 1, 4\n(1957), 309–317.\n[35] Christopher D Manning, Prabhakar Raghavan, Hinrich Schütze, and others. 2008.\nIntroduction to information retrieval. Vol. 1. Cambridge university press Cam-\nbridge.\n[36] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nestimation of word representations in vector space. arXiv:1301.3781 (2013).\n[37] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock`y, and Sanjeev Khu-\ndanpur. 2010. Recurrent neural network based language model.. In Interspeech,\nVol. 2. 3.\n[38] Kevin P Murphy. 2006. Naive bayes classifiers. University of British Columbia\n(2006).\n[39] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve re-\nstricted boltzmann machines. In Proceedings of the 27th international conference\non machine learning (ICML-10). 807–814.\n[40] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of\ntraining recurrent neural networks. ICML (3) 28 (2013), 1310–1318.\n[41] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of the 2014 conference on\nempirical methods in natural language processing (EMNLP). 1532–1543.\n[42] Irina Rish. 2001. An empirical study of the naive Bayes classifier. In IJCAI 2001\nworkshop on empirical methods in artificial intelligence, Vol. 3. IBM, 41–46.\n[43] Stephen Robertson. 2004. Understanding inverse document frequency: on theo-\nretical arguments for IDF. Journal of documentation 60, 5 (2004), 503–520.\n[44] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in\nautomatic text retrieval. Information processing & management 24, 5 (1988),\n513–523.\n[45] Dominik Scherer, Andreas Müller, and Sven Behnke. 2010. Evaluation of pooling\noperations in convolutional architectures for object recognition. Artificial Neural\nNetworks–ICANN 2010 (2010), 92–101.\n[46] Fabrizio Sebastiani. 2002. Machine learning in automated text categorization.\nACM computing surveys (CSUR) 34, 1 (2002), 1–47.\n[47] Ritambhara Singh, Arshdeep Sekhon, Kamran Kowsari, Jack Lanchantin, Beilun\nWang, and Yanjun Qi. 2017. Gakco: a fast gapped k-mer string kernel using\ncounting. In Joint European Conference on Machine Learning and Knowledge\nDiscovery in Databases. Springer, 356–373.\n[48] Aixin Sun and Ee-Peng Lim. 2001. Hierarchical text classification and evaluation.\nIn ICDM. IEEE, 521–528.\n[49] Yichuan Tang. 2013. Deep learning using linear support vector machines. arXiv\npreprint arXiv:1306.0239 (2013).\n[50] Simon Tong and Daphne Koller. 2001. Support vector machine active learning\nwith applications to text classification. Journal of machine learning research 2,\nNov (2001), 45–66.\n[51] Mehmet Turan, Yasin Almalioglu, Helder Araujo, Ender Konukoglu, and Metin\nSitti. 2017. Deep EndoVO: A Recurrent Convolutional Neural Network (RCNN)\nbased Visual Odometry Approach for Endoscopic Capsule Robots. arXiv preprint\narXiv:1708.06822 (2017).\n[52] Jason Weston and Chris Watkins. 1998. Multi-class support vector machines.\nTechnical Report. Department of Computer Science, University of London.\n[53] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Ed-\nuard H Hovy. 2016. Hierarchical Attention Networks for Document Classification..\nIn HLT-NAACL. 1480–1489.\n[54] Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural svms with\nlatent variables. In ICML. ACM, 1169–1176.\n[55] Wen Zhang, Taketoshi Yoshida, and Xijin Tang. 2008. Text classification based on\nmulti-word with support vector machine. Knowledge-Based Systems 21, 8 (2008),\n879–886.\n[56] Zhi-Hua Zhou and Ji Feng. 2017. Deep forest: Towards an alternative to deep\nneural networks. arXiv preprint arXiv:1702.08835 (2017).\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-05-03",
  "updated": "2018-05-31"
}