{
  "id": "http://arxiv.org/abs/1805.01891v1",
  "title": "Power Law in Sparsified Deep Neural Networks",
  "authors": [
    "Lu Hou",
    "James T. Kwok"
  ],
  "abstract": "The power law has been observed in the degree distributions of many\nbiological neural networks. Sparse deep neural networks, which learn an\neconomical representation from the data, resemble biological neural networks in\nmany ways. In this paper, we study if these artificial networks also exhibit\nproperties of the power law. Experimental results on two popular deep learning\nmodels, namely, multilayer perceptrons and convolutional neural networks, are\naffirmative. The power law is also naturally related to preferential\nattachment. To study the dynamical properties of deep networks in continual\nlearning, we propose an internal preferential attachment model to explain how\nthe network topology evolves. Experimental results show that with the arrival\nof a new task, the new connections made follow this preferential attachment\nprocess.",
  "text": "Power Law in Sparsiﬁed Deep Neural Networks\nLu Hou\nJames T. Kwok\nDepartment of Computer Science and Engineering\nHong Kong University of Science and Technology\nHong Kong\nAbstract—The power law has been observed in the degree\ndistributions of many biological neural networks. Sparse deep\nneural networks, which learn an economical representation\nfrom the data, resemble biological neural networks in many\nways. In this paper, we study if these artiﬁcial networks also ex-\nhibit properties of the power law. Experimental results on two\npopular deep learning models, namely, multilayer perceptrons\nand convolutional neural networks, are afﬁrmative. The power\nlaw is also naturally related to preferential attachment. To\nstudy the dynamical properties of deep networks in continual\nlearning, we propose an internal preferential attachment model\nto explain how the network topology evolves. Experimental\nresults show that with the arrival of a new task, the new\nconnections made follow this preferential attachment process.\n1. Introduction\nThe power law distribution has been commonly used\nto describe the underlying mechanisms of a wide variety\nof physical, biological and man-made networks [1]. Its\nprobability density function is of the form:\nf(x) ∝x−α,\n(1)\nwhere x is the measurement, and α > 1 is the exponent.\nIt is well-known that the power law can originate in an\nevolving network via preferential attachment [1], in which\nnew connections are preferentially made to the more highly\nconnected nodes. Networks exhibiting a power law degree\ndistribution are also called scale-free.\nIn the context of biological neural networks, the power\nlaw and its variants have been commonly observed. For\nexample, Monteiro et al. [2] showed that the mean learning\ncurves of scale-free networks resemble that of the biolog-\nical neural network of the worm Caenorhabditis Elegans.\nMoreover, these learning curves are better than those gen-\nerated from random and small-world networks. Eguiluz et\nal. [3] studied the functional networks connecting correlated\nhuman brain sites, and showed that the distribution of func-\ntional connections also follows the power law.\nIn practice, few empirical phenomena obey the power\nlaw exactly [4]. Often, the degree distribution has a power\nlaw regime followed by a fall-off. This may result from the\nﬁnite size of the data, temporal limitations of the collected\ndata or constraints imposed by the underlying physics [5].\nSuch networks are sometimes called broad-scale or trun-\ncated scale-free networks [6], and have been observed for\nexample in the human brain anatomical network [7]. To\nmodel this upper truncation effect, extensions of the power\nlaw have been proposed [5], [8]. In this paper, we will focus\non the truncated power law (TPL) distribution proposed in\n[8], which explicitly includes a lower and upper threshold.\nRecently, deep neural networks have achieved state-of-\nthe-art performance in various tasks such as speech recogni-\ntion, visual object recognition, and image classiﬁcation [9].\nHowever, connectivity of the network, and subsequently its\ndegree distribution, are ﬁxed by design. Moreover, many\nof its connections are redundant. Recent studies show that\nthese deep networks can often be signiﬁcantly sparsiﬁed.\nIn particular, by pruning unimportant connections and then\nretraining the remaining connections, the resultant sparse\nnetwork often suffers no performance degradation [10],\n[11]. This sparsiﬁcation process is also analogous to how\nlearning works in the mammalian brain [12]. For example,\nthe pruning (resp. retraining) in artiﬁcial neural networks\nresembles the weakening (resp. strengthening) of functional\nconnections in brain maturity.\nAnother similarity between biological and deep neural\nnetworks can be seen in the context of continual learning,\nin which the network learns progressively with the arrival of\nnew tasks [13]. Biological neural networks are able to learn\ncontinually as they evolve over a lifetime [14]. Continual\nlearning by deep networks mimics this biological learning\nprocess in which new connections are made without loss\nof established functionalities in neural circuits [15]. In both\nbiological and artiﬁcial neural networks, sparsity works as\na regularizer and allows a more economical representation\nof the learning experience to be obtained.\nIn general, a network has both static and dynamical\nproperties [16]. Static properties describe the topology of the\nnetwork, while dynamical properties describe the dynamics\ngoverning network evolution and explain how the topology\nis formed. In this paper, we study if the sparsiﬁed deep\nneural networks also exhibit properties of the power law as\nobserved in their biological counterparts.\nThe rest of this paper is organized as follows. Section 2\nﬁrst reviews the power law and preferential attachment. Sec-\ntion 3 studies the static properties of sparsiﬁed deep neural\nnetworks. In particular, we examine the degree distributions\nof two popular deep learning models, namely, multilayer\narXiv:1805.01891v1  [cs.LG]  4 May 2018\nperceptrons and convolutional neural networks, and show\nthat they follow the truncated power law. Section 4 studies\nthe dynamics behind this power law behavior. We propose a\npreferential attachment model for deep neural networks, and\nverify that new connections added to the artiﬁcial network\nin a continual learning setting follow this model. Finally,\nthe last section gives some concluding remarks.\n2. Related Work\n2.1. Truncated Power Law\nIn practice, few empirical phenomena obey the power\nlaw in (1) exactly for the whole range of observations\n[4]. Often, the power law applies only for values greater\nthan some minimum [4]. There may also be a maximum\nvalue above which the power law is no longer valid. Such\nan upper truncation is often observed in natural systems\nlike forest ﬁre areas, hydrocarbon volumes, fault lengths,\nand oil and gas ﬁeld sizes [5]. This may result from the\nﬁnite size of the data, temporal limitations of the collected\ndata, or constraints imposed by the underlying physics [5].\nFor example, the size of forest ﬁres is naturally limited\nby availability of fuel and climate, and the upper-bounded\npower law ﬁts the data better [17].\nThe truncated power law (TPL) distribution, with lower\nthreshold xmin and upper threshold xmax, captures the above\neffects [8]. Its probability density function (PDF) and com-\nplementary cumulative distribution function (CCDF)1 are\ngiven by:\np(x) =\n1 −α\nx1−α\nmax −x1−α\nmin\nx−α,\nS(x) = x1−α −x1−α\nmax\nx1−α\nmin −x1−α\nmax\n. (2)\nIt is well-known that the log-log CCDF plot for the\nstandard power law distribution is a line. However, from\n(2), the log-log CCDF for a TPL is\nlog(S(x)) = log(x1−α −x1−α\nmax) −log(x1−α\nmin −x1−α\nmax). (3)\nAs limx→xmax log(S(x)) = −∞, the log-log CCDF plot for\nTPL has a fall-off near xmax. Moreover, when xmax is large,\nlog(S(x)) ≃(1−α) log(x)−(1−α) log(xmin), and the log-\nlog CCDF plot reduces to a line. When xmax gets smaller,\nthe linear region shrinks and the fall-off starts earlier.\nWhen x only takes integer values (instead of a range of\ncontinuous values), the probability function of TPL distri-\nbution becomes [18]\nf(x) =\nx−α\nζ(α, xmin, xmax),\n(4)\nwhere ζ(α, xmin, xmax) = Pxmax\nk=xmin k−α. The correspond-\ning CCDF is: S(x) =\nζ(α,x,xmax)\nζ(α,xmin,xmax).\n1. The CCDF is deﬁned as one minus the cumulative distribution func-\ntion, i.e., 1 −\nR x\nxmin f(x) dx =\nR xmax\nx\nf(x) dx.\n2.2. Preferential attachment\nThe power law distributions can originate from the\nprocess of preferential attachment [1], which can be either\nexternal or internal [16]. External preferential attachment\nrefers to that when a new node is added to the network, it is\nmore likely to connect to an existing node with high degree;\nwhile internal preferential attachment means that existing\nnodes with high degrees are more likely to connect to each\nother. In this paper (as will be explained in Section 4), we\nwill focus on internal preferential attachment.\nLet N be the number of nodes in the network, and a be\nthe number of new internal connections created in unit time\nper existing node. For two nodes with degrees d1 and d2,\nthe expected number of new connections created between\nthem per unit time is:\n∆(d1, d2) = 2Na\nd1d2\nP\ns,m̸=s dsdm\n,\n(5)\nwhere s and m are the indices to all the nodes in the\nnetwork.\n3. Power Law in Neural Networks\nIn this section, we study whether the degree distributions\nin artiﬁcial neural networks follow the power law. However,\nconnectivity of the network is often ﬁxed by design and not\nlearned from data. Moreover, many of the network connec-\ntions are redundant. Hence, we will study networks that have\nbeen sparsiﬁed, in which only important connections are\nkept. Speciﬁcally, we use sparse networks produced by the\nstate-of-the-art three-step network pruning method in [12],\nand also pre-trained sparse convolutional neural networks.\nFor the three-step network pruning method, a dense network\nis ﬁrst trained. For each layer, a fraction of s ∈(0, 1)\nconnections with the smallest magnitudes are pruned. The\nunpruned connections are then retrained. To avoid potential\nperformance degradation, connections to the output layer is\nalways left unpruned, as is common in network pruning [11]\nand continual learning [15].\nObviously, neural networks are of ﬁnite size and the con-\nnections each node can make is limited. This suggests upper\ntruncation and the TPL is more appropriate for modeling\nconnectivity. In particular, the discrete TPL is more suitable\nas the degree is integer-valued. Moreover, as the number of\nnodes, the maximum number of connections each node can\nmake, and the nature of features extracted at different layers\nare different, it is more appropriate to study the connectivity\nin a layer-wise manner.\nWe follow the method in [4] to estimate xmin, xmax and\nα in (4). Speciﬁcally, xmin and xmax are chosen by min-\nimizing the difference between the probability distribution\nof the observed data and the best-ﬁt power-law model as\nmeasured by the Kolmogorov-Smirnov (KS) statistic:\nD = maxx∈{xmin,xmin+1,...,xmax−1,xmax}|S(x) −P(x)|,\nwhere S(x) and P(x) are the CCDF’s of the observed\ndata and ﬁtted power-law model for x ∈{xmin, xmin +\n1, . . . , xmax −1, xmax}. To reduce the search space, we\nsearch xmin in the ⌊(n × k%)⌋smallest degree values,\nwhere n is the number of nodes in that layer, and xmax in the\n⌊(n × k%)⌋largest degree values, respectively. Empirically,\nk = 30 is used. For each (xmin, xmax) pair, we estimate α\nusing the method of maximum likelihood as in [4].\n3.1. Multilayer Perceptron (MLP) on MNIST\nIn this section, we perform experiment on the MNIST\ndata set2, which contains 28×28 gray images from 10 digit\nclasses. 50, 000 images are used for training, another 10, 000\nfor validation, and the remaining 10, 000 for testing.\nFollowing [12], we ﬁrst train a dense MLP (with two\nhidden layers) which uses the cross-entropy loss in the\nLasagne package3:\ninput −1024FC −1024FC −10softmax.\nHere, 1024FC denotes a fully connected layer with 1024\nunits and 10softmax is a softmax layer for the 10 classes.\nThe optimizer is stochastic gradient descent (SGD) with\nmomentum. All hyperparameters are the same as in the\nLasagne package. The maximum number of epochs is 200.\nAfter training, a fraction of s = 0.9 connections are pruned\nin each layer. The testing accuracies of the dense and\n(retrained) pruned networks are comparable (98.09% and\n98.21%).\nFor each node, we obtain its degree by counting the total\nnumber of connections (after pruning) to nodes in its upper\nand lower layers.4 Figure 1 shows the CCDF plot and TPL\nﬁt for each layer in the MLP. As can be seen, the TPL ﬁts\nthe degree distributions well.\n(a) input.\n(b) fc1.\n(c) fc2.\nFigure 1.\nLog-log CCDF plot and TPL ﬁt (red) for the MLP layers on\nMNIST. “fc1” and “fc2” denote the ﬁrst and second FC layer, respectively.\nThe softmax layer, which is not pruned, is not shown.\n3.2. Convolutional Neural Network on MNIST\nIn this section, we use the convolutional neural network\n(CNN) in the Lasagne package. It is similar to LeNet-5\n[19], and has two convolutional layers followed by 2 fully\nconnected layers:\ninput −32C5 −MP2 −32C5 −MP2 −256FC −10softmax\n2. http://yann.lecun.com/exdb/mnist/\n3. https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\n4. For a node in the input (resp. last pruned) layer, we only count its\nconnections to nodes in the upper (resp. lower) layer.\nHere, 32C5 denotes a ReLU convolution layer with 32 5×5\nﬁlters, and MP2 is a 2 × 2 max-pooling layer. We use SGD\nwith momentum as the optimizer. The other hyperparameters\nare the same as in the Lasagne package. The maximum\nnumber of epochs is 200. The (dense) CNN has a testing\naccuracy of 98.85%. This is then pruned using the method\nin [12], with s = 0.7. After retraining, the sparse CNN has\na testing accuracy of 98.78%, and is comparable with the\ndense network.\nFor a convolutional layer, we consider each feature map\nat the convolutional layer as a node. As in Section 3.1, the\nnode degree is obtained by counting the total number of\nconnections (after pruning) to its upper and lower layers.\nAs an example, consider a feature map (node) in the ﬁrst\nconvolutional layer (conv1). As the input image is of size\n28×28, each such feature map is of size 24×24. To illustrate\nthe counting more easily, we consider the unpruned network.\nWe ﬁrst count its connections to the input layer. Recall that\nin conv1, (i) the ﬁlter size is 5 × 5; (ii) each ﬁlter weight\nis used 24 × 24 = 576 times; and (iii) there is only one\nchannel in the grayscale MNIST image. Thus, each node\nhas 25 × 576 × 1 = 14, 400 connections to the input layer.\nSimilarly, for connections to the conv2 layer, (i) the conv2\nﬁlter size is 5 × 5; (ii) each ﬁlter weight is used 8 × 8 = 64\ntimes (size of each conv2 feature map); and (iii) there are\n32 feature maps in conv2. Thus, each conv1 node has 25 ×\n64 × 32 = 51, 200 connections to the conv2 layer. Hence,\nthe degree of each conv1 node (in an unpruned network) is\n14, 400+51, 200 = 65, 600. Note that the pooling layers do\nnot have learnable connections. They are never sparsiﬁed,\nand we do not need to study their degree distributions.\nFigure 2 shows the CCDF plot and TPL ﬁt for each\nCNN layer. Again, the TPL ﬁts the distributions well.\n(a) conv1.\n(b) conv2.\n(c) fc3.\nFigure 2.\nLog-log CCDF plot and TPL ﬁt (red) for the CNN layers on\nMNIST. Here, “conv1” and “conv2” denote the ﬁrst and second convolu-\ntional layers, and “fc3” is the 3rd (FC) layer.\n3.3. CNN on CIFAR-10\nIn this section, we perform experiments on the CIFAR-\n10 data set5, which contains 32 × 32 color images from ten\nobject classes. We use 45, 000 images for training, another\n5, 000 for validation, and the remaining 10, 000 for testing.\nThe following CNN from [20] is used6:\ninput −32C3 −32C3 −MP2 −64C3 −64C3 −MP2\n−512FC −10softmax.\n5. https://www.cs.toronto.edu/∼kriz/cifar.html\n6. https://github.com/fchollet/keras/blob/master/examples/cifar10 cnn.\npy\nWe use RMSprop as the optimizer, and the maximum\nnumber of epochs is 100. The (dense) CNN has a testing\naccuracy of 81.90%. This is then pruned using the method\nin [12], with s = 0.7. After retraining, the testing accuracy\nof the sparse CNN is 80.46%, and is comparable with the\noriginal network. Figure 3 shows the CCDF plot and TPL\nﬁt for each layer. Again, the TPL ﬁts the distributions well.\n(a) conv1.\n(b) conv2.\n(c) conv3.\n(d) conv4.\n(e) fc5.\nFigure 3.\nLog-log CCDF plot and TPL ﬁt (red) for the CNN layers on\nCIFAR-10.\n3.4. AlexNet on ImageNet\nIn this section, we perform experiments on the ImageNet\ndata set [21], which has 1,000 categories, over 1.2 million\ntraining images, 50,000 validation images, and 100,000 test\nimages. We use the sparsiﬁed AlexNet7 (with 5 convo-\nlutional layers and 3 fully connected layers) from [10].\nFigure 4 shows the CCDF plot and TPL ﬁt for the various\nAlexNet layers.\n3.5. VGG-16 on ImageNet\nIn this section, we use the (dense) VGG-16 model8\n(with 13 convolutional layers and 3 fully connected layers)\nobtained by the dense-sparse-dense (DSD) procedure of\n[22]. We then prune this using the method in [22] with\ns = 0.3. The top-1 and top-5 testing accuracies for the\noriginal dense CNN are 68.50% and 88.68%, respectively,\nwhile those of the sparse CNN are 71.81% and 90.77%.\nFigure 5 shows the CCDF plot and TPL ﬁt for various layers.\n4. Internal Preferential Attachment in NN\nTo study the dynamical properties of neural network\nconnectivity, we consider the continual learning setting in\nwhich consecutive tasks are learned [13]. Speciﬁcally, at\ntime t = 0, an initial sparse network is trained to learn the\nﬁrst task. At each following timestep t = 1, 2, . . . , a new\ntask is encountered, and the network is re-trained on the\n7. https://github.com/songhan/Deep-Compression-AlexNet\n8. https://github.com/songhan/DSD/tree/master/VGG16\nnew task. We assume that only new connections, but not\nnew nodes, can be added. Hence, preferential attachment, if\nexists, can only be internal but not external.\n4.1. Evolution of the Degree Distribution\nIn this section, we assume that only nodes in adjacent\nlayers can be connected, as is common in feedforward neural\nnetworks. Consider a pair of nodes, one from layer l with\ndegree d1 and the other from layer (l + 1) with degree\nd2. If internal preferential attachment exists, the number of\nconnections between this node pair grows proportional to the\nproduct d1d2. Analogous to (5), the number of connections\ncreated per unit time between this node pair at time t is:9\n∆l\nt(d1, d2) = N lal\nd1d2\nP\ns,m dsdm\n,\n(6)\nwhere s and m are indices to all the nodes in the two layers\ninvolved, N l is the number of nodes in layer l, and al is\nthe number of new internal connections created in unit time\nfrom a node in layer l to layer (l + 1).\nNext, we study how the degree of a node evolves. For\nnode i at layer l, let di(t) be its degree at time t. Out of\nthese di(t) connections, let d↑\ni (t) be connected to the upper\nlayer, and d↓\ni (t) to the lower layer10. Using (6), the increase\nof its degree due to new connections to the upper layer is:\ndd↑\ni (t)\ndt\n=\nX\nm\n∆l\nt(di(t), dm(t))\n=\nN laldi(t)\nP\nm dm(t)\n(P\ns ds(t))(P\nm dm(t))\n=\nN laldi(t)\nP\ns ds(t) ,\nwhere m and s are indices to all the nodes in layer (l + 1)\nand layer l, respectively. Similarly, the increase of degree\ndue to new connections to the lower layer is:\ndd↓\ni (t)\ndt\n=\nX\nr\n∆l−1\nt\n(dr(t), di(t)) = N l−1al−1di(t)\nP\ns ds(t)\n,\nwhere r and s are indices to all the nodes in layer (l−1) and\nlayer l, respectively. The total number of new connections\nfor nodes in layer l can be obtained as:\nX\ns\nds(t)\n=\nX\ns\nds(0) +\nZ t\n0\n(N lal + N l−1al−1)dt\n=\nX\ns\nds(0) + (N lal + N l−1al−1)t.\nCombining all these, we have\nddi(t)\ndt\n=\ndd↑\ni (t)\ndt\n+ dd↓\ni (t)\ndt\n=\n(N lal + N l−1al−1)di(t)\nP\ns ds(0) + (N lal + N l−1al−1)t.\n(7)\n9. Unlike (5), note that there is no factor of 2 here.\n10. For simplicity, we only consider hidden layers here. Analysis for the\nother layers can be easily modiﬁed and are not detailed here.\n(a) conv1.\n(b) conv2.\n(c) conv3.\n(d) conv4.\n(e) conv5.\n(f) fc6.\n(g) fc7.\n(h) fc8.\nFigure 4. Log-log CCDF plot and TPL ﬁt (red) for the sparse AlexNet layers. Naming of the layers follows that deﬁned in Caffe.\n(a) conv1 1.\n(b) conv1 2.\n(c) conv2 1.\n(d) conv2 2.\n(e) conv3 1.\n(f) conv3 2.\n(g) conv3 3.\n(h) conv4 1.\n(i) conv4 2.\n(j) conv4 3.\n(k) conv5 1.\n(l) conv5 2.\n(m) conv5 3.\n(n) fc6.\n(o) fc7.\n(p) fc8.\nFigure 5. Log-log CCDF plot and TPL ﬁt (red) for the VGG-16 layers. Naming of the layers follows that deﬁned in Caffe.\nAfter integration and simpliﬁcation, we obtain\ndi(t) = di(0)cl(t),\n(8)\nwhere cl(t) = 1 + (N lal+Nl−1al−1)t\nP\ns dls(0)\n. Hence, di(t) is linear\nw.r.t. the node’s initial degree di(0). In particular, assume\nthat the degree distribution of layer l at t = 0 (denoted\npl\n0) follows the power law (standard or TPL), i.e., pl\n0(d) =\nAd−α for some A > 0 and α > 1. Then, from (8), its degree\ndistribution at time t is:\npl\nt(d) = pl\n0\n\u0012 d\ncl(t)\n\u0013\n= A\n\u0012 d\ncl(t)\n\u0013−α\n=(cl(t))αAd−α,\n(9)\nwhich follows the same power law as pl\n0, but scaled by the\nfactor (cl(t))α.\nRemark 4.1. Note from (8) that as cl(t) increases with t,\nhence di(t) increases with t. However, as we assume\nthat new nodes cannot be added, the degree cannot grow\ninﬁnitely and (8) will not hold for large t.\n4.2. Experiments\nWe consider a simple continual learning setting with\nonly two tasks. Experiments are performed on the MNIST\ndata set, with the same setup in Section 3.1. The two tasks\nare from [13], [23]. Task A uses the original images, while\ntask B uses images in which pixels inside a central P × P\nsquare are permuted. As in [13]), we use P = 8 and 26.\nNote that as the size of MNIST image is 28 × 28, when\nP = 26, the task B (permuted) images are very different\nfrom the task A (original) images.\nWe ﬁrst train a dense network on task A (Figure 6).\nA fraction of s1 = 0.9 connections are pruned from each\nlayer11 as in Section 3. The remaining connections are\nretrained on task A, and then frozen. Next, we reinitialize\nthe pruned connections and train a dense network on task B.\nAnother fraction of s2 = 0.8 connections from each layer\nare again pruned and the remaining connections retrained.\nRecall that connections learned on task A are frozen, and\nso they will not be pruned or retrained. Table 1 shows the\ntesting accuracies of the four networks in Figure 6. As can\nbe seen, the sparse network has good performance on both\ntasks.\nFigure 6. The continual learning setup. Connections learned for task A are\nin black, and those for task B are in red. Left: A dense network is trained\non task A; Middle left: Network pruned and retrained on task A; Middle\nright: The pruned connections are reinitialized and retrained on task B;\nRight: Connections are pruned and the remaining connections retrained on\ntask B.\nTABLE 1. TESTING ACCURACIES (%) FOR NETWORKS IN THE\nCONTINUAL LEARNING EXPERIMENT.\ntask A\ntask B\nP\ndense\nsparse\ndense\nsparse\n8\n98.09\n98.21\n98.17\n98.09\n26\n98.09\n98.21\n98.04\n98.16\n11. As mentioned at the beginning of Section 3, connections to the last\nsoftmax layer are not pruned.\n4.2.1. Existence of Internal Preferential Attachment.\nEmpirically, ∆l\nt(d1, d2) in (6) can be estimated by counting\nthe number of new connections created at time (t + 1)\nbetween all the involved node pairs (i.e., one from layer l\nwith degree d1 and the other from layer (l + 1) with degree\nd2 at time t), and then divide it by the number of such node\npairs. Figure 7 shows this empirical estimate at time t = 0\n(denoted ˆ∆l\n0(d1, d2)) versus the degrees d1 and d2. As can\nbe seen, ˆ∆l\n0(d1d2) increases with d1 and d2, indicating the\npresence of internal preferential attachment.\n(a) input-to-fc1.\n(b) fc1-to-fc2.\n(c) input-to-fc1.\n(d) fc1-to-fc2.\nFigure 7. ˆ∆l\n0(d1, d2) vs d1 and d2. Top: P = 8; Bottom: P = 26.\n4.2.2. Number of New Connections versus Degree. From\n(7), for a node with degree di(0) at t = 0, the number of\nconnections added to it at t = 1 is equal to\nddi(t)\ndt\n\f\f\f\f\nt=0\n= di(0)N lal + N l−1al−1\nP\ns ds(0)\n.\n(10)\nLet di(0) = d, an empirical estimate of ddi(t)\ndt\n\f\f\f\nt=0 (denoted\nˆΩl\n0(d)) can be obtained by counting the number of new\nconnections that all nodes (from layer l) with degree d made\nat time (t+1), and then divide it by the number of degree-d\nnodes (in layer l) at time t.\nFigure 8 shows ˆΩl\n0(d) versus the node degree d at time\nt = 0. As can be seen, when the two tasks are similar\n(P = 8), the relationship is roughly linear for all layers,\nwhich agrees with (10). However, when the tasks are much\nless similar (P = 26), the network must learn to associate\nnew collections from pixels to penstrokes, and connections\nfrom the input layer to the ﬁrst hidden layer have to be\nsigniﬁcantly modiﬁed. Hence, preferential attachment is no\nlonger useful. As can be seen, there is no linear relationship\nfor the input layer, and the linear relationship in the fc1\nlayer12 is noisier than that for P = 8. On the other hand, as\n12. Recall that the fc1 layer also counts the connections to the input\nlayer.\ndiscussed in [23], once the input layer has established new\nassociations to map from pixels to penstrokes, the higher-\nlevel feature extractors (i.e., fc2) are only dependent on these\npenstroke features (extracted at fc1) and thus less affected\nby the input permutation. As can be seen, the fc2 layer still\nshows a linear relationship.\n(a) input.\n(b) fc1.\n(c) fc2.\n(d) input.\n(e) fc1.\n(f) fc2.\nFigure 8. Number of new connections ˆΩl\n0(d) vs degree d. Top: P = 8;\nBottom: P = 26.\n4.2.3. Degree Distribution. Recall from Section 3.1 that\nthe degree distribution of the sparse MLP trained on task\nA (t = 0) follows the TPL. From (9), we thus expect that\nits degree distribution after training on task B (t = 1) also\nfollows the TPL. Figure 9 shows the CCDF plot and TPL\nﬁt for each layer of the sparse network at t = 1. As can be\nseen, the p-values of the ﬁrst layer are relatively low. This\nis because that, due to permutation, the input layer has to\nestablish new associations to map from pixels to penstrokes.\n(a) input.\n(b) fc1.\n(c) fc2.\n(d) input.\n(e) fc1.\n(f) fc2.\nFigure 9.\nLog-log CCDF plot and TPL ﬁt (red) for the MLP layers on\nMNIST at t = 1. Top: P = 8; Bottom: P = 26.\n5. Conclusion\nIn this paper, we showed that a number of sparse deep\nlearning models exhibit the (truncated) power law behavior.\nWe also proposed an internal preferential attachment model\nto explain how the network topology evolves, and verify in\na continual learning setting that new connections added to\nthe network indeed have this preferential bias.\nIn biological neural networks with limited capacities,\nreuse of neural circuits is essential for learning multiple\ntasks, and scale-free networks can learn faster than random\nand small-world networks [2]. In the future, we will use the\ndynamic behavior observed on artiﬁcial neural networks to\ndesign faster continual learning algorithms. Moreover, this\npaper only studies feedforward neural networks. We will\nalso study existence of the power law in recurrent neural\nnetworks.\nReferences\n[1]\nA. L. Barab´asi and R. Albert, “Emergence of scaling in random\nnetworks,” Science, vol. 286, no. 5439, pp. 509–512, 1999.\n[2]\nR. L. S. Monteiro, T. K. G. Carneiro, J. R. A. Fontoura, V. L. da Silva,\nM. A. Moret, and H. B. de Barros Pereira, “A model for improving\nthe learning curves of artiﬁcial neural networks,” PloS One, vol. 11,\nno. 2, p. e0149874, 2016.\n[3]\nV. M. Eguiluz, D. R. Chialvo, G. A. Cecchi, M. Baliki, and A. V.\nApkarian, “Scale-free brain functional networks,” Physical Review\nLetters, vol. 94, no. 1, p. 018102, 2005.\n[4]\nA. Clauset, C. R. Shalizi, and M. E. J. Newman, “Power-law distri-\nbutions in empirical data,” SIAM Review, vol. 51, no. 4, pp. 661–703,\n2009.\n[5]\nS. M. Burroughs and S. F. Tebbens, “Upper-truncated power laws in\nnatural systems,” Pure and Applied Geophysics, vol. 158, no. 4, pp.\n741–757, 2001.\n[6]\nL. A. N. Amaral, A. Scala, M. Barthelemy, and H. E. Stanley, “Classes\nof small-world networks,” Proceedings of the National Academy of\nSciences, vol. 97, no. 21, pp. 11 149–11 152, 2000.\n[7]\nY. Iturria-Medina, R. C. Sotero, E. J. Canales-Rodr´ıguez, Y. Alem´an-\nG´omez, and L. Melie-Garc´ıa, “Studying the human brain anatomical\nnetwork via diffusion-weighted mri and graph theory,” Neuroimage,\nvol. 40, no. 3, pp. 1064–1076, 2008.\n[8]\nD. Kolyukhin and A. Torabi, “Power-law testing for fault attributes\ndistributions,” Pure and Applied Geophysics, vol. 170, no. 12, pp.\n2173–2183, 2013.\n[9]\nY. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.\n521, no. 7553, pp. 436–444, 2015.\n[10] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing\ndeep neural networks with pruning, trained quantization and huffman\ncoding,” in International Conference of Learning Representations,\n2016.\n[11] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning\nﬁlters for efﬁcient convnets,” in International Conference of Learning\nRepresentations, 2017.\n[12] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and\nconnections for efﬁcient neural network,” in Advances in Neural\nInformation Processing Systems, 2015, pp. 1135–1143.\n[13] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,\nA. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska\net al., “Overcoming catastrophic forgetting in neural networks,” Pro-\nceedings of the National Academy of Sciences, vol. 114, no. 13, pp.\n3521–3526, 2017.\n[14] M. L. Anderson, “Neural reuse: A fundamental organizational prin-\nciple of the brain,” Behavioral and Brain Sciences, vol. 33, no. 4, pp.\n245–266, 2010.\n[15] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu,\nA. Pritzel, and D. Wierstra, “Pathnet: Evolution channels gradient\ndescent in super neural networks,” Preprint arXiv:1701.08734, 2017.\n[16] A. Barabˆasi, H. Jeong, Z. N´eda, E. Ravasz, A. Schubert, and T. Vic-\nsek, “Evolution of the social network of scientiﬁc collaborations,”\nPhysica A: Statistical Mechanics and its Applications, vol. 311, no. 3,\npp. 590–614, 2002.\n[17] B. D. Malamud, G. Morein, and D. L. Turcotte, “Forest ﬁres: An\nexample of self-organized critical behavior,” Science, vol. 281, no.\n5384, pp. 1840–1842, 1998.\n[18] R. Hanel, B. Corominas-Murtra, B. Liu, and S. Thurner, “Fitting\npower-laws in empirical data with estimators that work for all ex-\nponents,” PloS one, vol. 12, no. 2, p. e0170920, 2017.\n[19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based\nlearning applied to document recognition,” Proceedings of the IEEE,\nvol. 86, no. 11, pp. 2278–2324, 1998.\n[20] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through\nsynaptic intelligence,” in International Conference on Machine Learn-\ning, 2017, pp. 3987–3995.\n[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg, and F.-F.\nLi, “ImageNet large scale visual recognition challenge,” International\nJournal of Computer Vision, vol. 115, no. 3, pp. 211–252, 2015.\n[22] S. Han, J. Pool, S. Narang, H. Mao, S. Tang, E. Elsen, B. Catanzaro,\nJ. Tran, and W. J. Dally, “DSD: Regularizing deep neural networks\nwith dense-sparse-dense training ﬂow,” in International Conference\nof Learning Representations, 2017.\n[23] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, “An\nempirical investigation of catastrophic forgetting in gradient-based\nneural networks,” Preprint arXiv:1312.6211, 2013.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-05-04",
  "updated": "2018-05-04"
}