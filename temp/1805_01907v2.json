{
  "id": "http://arxiv.org/abs/1805.01907v2",
  "title": "Exploration by Distributional Reinforcement Learning",
  "authors": [
    "Yunhao Tang",
    "Shipra Agrawal"
  ],
  "abstract": "We propose a framework based on distributional reinforcement learning and\nrecent attempts to combine Bayesian parameter updates with deep reinforcement\nlearning. We show that our proposed framework conceptually unifies multiple\nprevious methods in exploration. We also derive a practical algorithm that\nachieves efficient exploration on challenging control tasks.",
  "text": "Exploration by Distributional Reinforcement Learning\nYunhao Tang, Shipra Agrawal\nColumbia University IEOR\nyt2541@columbia.edu, sa3305@columbia.edu\nAbstract\nWe propose a framework based on distributional re-\ninforcement learning and recent attempts to com-\nbine Bayesian parameter updates with deep rein-\nforcement learning.\nWe show that our proposed\nframework conceptually uniﬁes multiple previous\nmethods in exploration. We also derive a practi-\ncal algorithm that achieves efﬁcient exploration on\nchallenging control tasks.\n1\nIntroduction\nDeep reinforcement learning (RL) has enjoyed numerous re-\ncent successes in various domains such as video games and\nrobotics control [Schulman et al., 2015; Duan et al., 2016;\nLevine et al., 2016]. Deep RL algorithms typically apply\nnaive exploration strategies such as ϵ−greedy [Mnih et al.,\n2013; Lillicrap et al., 2016]. However, such myopic strate-\ngies cannot lead to systematic exploration in hard environ-\nments [Osband et al., 2017].\nWe provide an exploration algorithm based on distribu-\ntional RL [Bellemare et al., 2017] and recent attempts to\ncombine Bayesian parameter updates with deep reinforce-\nment learning. We show that the proposed algorithm pro-\nvides a conceptual uniﬁcation of multiple previous methods\non exploration in deep reinforcement learning setting. We\nalso show that the algorithm achieves efﬁcient exploration in\nchallenging environments.\n2\nBackground\n2.1\nMarkov Decision Process and Value Based\nReinforcement Learning\nIn a Markov Decision Process (MDP), at time step t ≥0, an\nagent is in state st ∈S, takes action at ∈A, receives reward\nrt and gets transitioned to next state st+1 ∼p(st+1|st, at).\nAt time t = 0 the agent’s state distribution follows s0 ∼\nρ(s0). A policy is a mapping from a state to a distribution\nover action at ∼π(·|st). The objective is to ﬁnd a policy π\nto maximize the discounted cumulative reward\nJ = Es0∼ρ,at∼π(·|st)\n\u0002 ∞\nX\nt=0\nrtγt\u0003\n,\n(1)\nwhere γ ∈(0, 1] is a discount factor. In state s, the action-\nvalue function Qπ(s, a) is deﬁned as the expected cumulative\nreward that could be received by ﬁrst taking action a and fol-\nlowing policy π thereafter\nQπ(s, a) = Eat∼π(·|st)\n\u0002 ∞\nX\nt=0\nrtγt|s0 = s, a0 = a\n\u0003\n.\nFrom the above deﬁnition, it can be shown that Qπ(st, at)\nsatisﬁes the Bellman equation\nQπ(st, at) = E[rt + γQπ(st+1, at+1)], ∀(st, at).\nLet π∗= arg maxπ J be the optimal policy and Q∗(s, a) its\naction value function. Q∗(s, a) satisﬁes the following Bell-\nman equation\nQ∗(st, at) = E\n\u0002\nrt + γ max\na\nQ∗(st+1, a)\n\u0003\n, ∀(st, at).\nThe above equations illustrate the temporal consistency of the\naction value functions that allows for the design of learning\nalgorithms. Deﬁne Bellman operator\nT ∗Q(st, at) := E[rt + γ max\na′ Q(st+1, a′)].\nWhen γ ∈(0, 1), starting from any Q(0)(s, a), iteratively\napplying the operator Q(t+1)(s, a) ←T ∗Q(t)(s, a) leads to\nconvergence Q(t)(s, a) →Q∗(s, a) as t →∞.\nIn high dimensional cases, it is critical to use function ap-\nproximation as a compact representation of action values. Let\nQθ(s, a) be such a function with parameter θ that approxi-\nmates a table of action values with entry (s, a). The aim is\nto ﬁnd θ such that Qθ(s, a) ≈Q∗(s, a). Let Π be the op-\nerator that projects arbitrary vector Q(s, a) ∈R|S|×|A| to\nthe subspace spanned by function Qθ(s, a). Since the up-\ndate of action values can now only take place in the sub-\nspace spanned by function Qθ(s, a), the iterate Q(t)(s, a) is\nupdated as Q(t+1)(s, a) ←ΠT ∗Q(t)(s, a). In cases where\nQθ(s, a) is linear, the above procedure can be shown to con-\nverge [Tsitsiklis and Van Roy, 1996].\nHowever, in cases\nwhere Qθ(s, a) is nonlinear (neural network), the function\napproximation becomes more expressive at the cost of no\nconvergence guarantee. Many deep RL algorithms are de-\nsigned following the above formulation, such as Deep Q Net-\nwork (DQN) [Mnih et al., 2013].\narXiv:1805.01907v2  [cs.LG]  21 Jun 2018\n2.2\nDistributional Reinforcement Learning\nFollowing [Bellemare et al., 2017], instead of considering\naction value Qπ(s, a) under policy π, which is itself an ex-\npectation, consider the random return at (st, at) by follow-\ning policy π, Rπ(st, at) = P\nt′≥t rt′γt′−t. It follows that\nQπ(s, a) = E[Rπ(s, a)]. Let Zπ(s, a) be the distribution of\nRπ(s, a). The Bellman equation for random return is similar\nto that of the action value functions\nZπ(st, at) =D rt + γZπ(st+1, π(st+1)),\nwhere both sides are distributions and =D denotes equality\nin distribution.1 Deﬁne distributional Bellman operator Hπ\nunder policy π as\nHπZ(st, at) := rt + γZ(st+1, π(st+1)).\nNotice that Hπ operates on distributions. Deﬁne H∗as fol-\nlows\nH∗Z := Hπ∗Z, for some optimal policy π∗.\nWhen γ ∈(0, 1), starting from any distribution Z(0)(s, a),\napplying the operator as Z(t+1)(s, a) ←H∗Z(t)(s, a) leads\nto convergence in expectation E[Z(t)(s, a)] →Q∗(s, a).\nHowever, the distribution Z(t)(s, a) itself may not weakly\nconverge.\nTo design a practical algorithm, one must use a parametric\nfamily of distribution Zθ(s, a) to approximate Zπ(s, a), with\nparameter θ. Let D(Z1, Z2) be a discrepancy measure be-\ntween distribution Z1 and Z2. Deﬁne the projection operator\nΠ as follows\nΠZ := arg min\nZθ D(Z, Zθ).\nIn other words, Π projects a distribution Z into another\ndistribution Zθ in the parametric family with smallest dis-\ncrepancy from Z. Hence the distribution Z is updated as\nZ(t+1)(s, a) ←ΠH∗Z(t)(s, a). In practice, the operator is\napplied to different entries (s, a) asynchronously. For a given\npair (st, at), one ﬁrst selects a greedy action for next state\na′ = arg max\na\nE[Zθ(st+1, a)],\nthen updates the distribution Zθ(st, at) to match the target\ndistribution by minimizing the discrepancy\nmin\nθ\nD(rt + γZθ(st+1, a′), Zθ(st, at)).\n(2)\nWhen only samples xi ∼rt + γZθ(st+1, a′), 1 ≤i ≤N are\navailable, let the empirical distribution be ˆZ = PN\ni=1\n1\nN δ(x−\nxi) 2, then (2) reduces to minimizing D( ˆZ, Zθ).\n3\nRelated Work\nIn reinforcement learning (RL), naive explorations such as\nϵ−greedy [Mnih et al., 2013; Lillicrap et al., 2016] do not\nexplore well because local perturbations of actions break the\n1In future notations, we replace =D by = for simplicity.\n2δ(x −xi) is the Dirac distribution that assigns point mass of\nprobability 1 at x = xi.\nconsistency between consecutive steps [Osband and Van Roy,\n2015]. A number of prior works apply randomization to pa-\nrameter space [Fortunato et al., 2017; Plappert et al., 2016]\nto preserve the consistency in exploration, but their formula-\ntions are built on heuristics. Posterior sampling is a principled\nexploration strategy in the bandit setting [Thompson, 1933;\nRusso, 2017], yet its extension to RL [Osband et al., 2013] is\nhard to scale to large problems. More recent prior works have\nformulated the exploration strategy as sampling randomized\nvalue functions and interpreted the algorithm as approximate\nposterior sampling [Osband et al., 2016; Osband et al., 2017].\nInstead of modeling value functions, our formulation is built\non modeling return distributions which reduces to exact pos-\nterior sampling in the bandit setting.\nFollowing similar ideas of randomized value function,\nmultiple recent works have combined approximate Bayesian\ninference [Ranganath et al., 2014; Blei et al., 2017] with Q\nlearning and justiﬁed the efﬁciency of exploration by relat-\ning to posterior sampling [Lipton et al., 2016; Tang and Ku-\ncukelbir, 2017; Azizzadenesheli et al., 2017; Moerland et al.,\n2017]. Though their formulations are based on randomized\nvalue functions, we offer an alternate interpretation by mod-\neling return distribution and provide a conceptual framework\nthat uniﬁes these previous methods (Section 5). We will also\nprovide a potential approach that extends the current frame-\nwork to policy based methods as in [Henderson et al., 2017].\nModeling return distribution dated back to early work of\n[Dearden et al., 1998; Morimura et al., 2010; Morimura et\nal., 2012], where learning a return distribution instead of only\nits expectation presents a more statistically challenging task\nbut provides more information during control. More recently,\n[Bellemare et al., 2017] applies a histogram to learn the return\ndistribution and displays big performance gains over DQN\n[Mnih et al., 2013]. Based on [Bellemare et al., 2017], we\nprovide a more general distributional learning paradigm that\ncombines return distribution learning and exploration based\non approximate posterior sampling.\n4\nExploration by Distributional\nReinforcement Learning\n4.1\nFormulation\nRecall that Z(s, a) is the return distribution for state action\npair (s, a). In practice, we approximate such distribution by a\nparametric distribution Zθ(s, a) with parameter θ. Following\n[Bellemare et al., 2017], we take the discrepancy to be KL\ndivergence. Recall ˆZ is the empirical distribution of samples\nˆZ = PN\ni=1\n1\nN δ(x −xi), hence the KL divergence reduces to\nKL[ ˆZ||Zθ] =\nN\nX\ni=1\n1\nN log\n1\nN\nZθ(xi) = −1\nN\nN\nX\ni=1\nlog Zθ(xi),\n(3)\nwhere we have dropped a constant −log N in the last equal-\nity. Let θ follow a given distribution θ ∼qφ(θ) with parame-\nter φ. We propose to minimize the following objective\nmin\nφ Eθ∼qφ(θ)[−\nN\nX\ni=1\nlog Zθ(xi)] −H(qφ(θ)),\n(4)\nwhere H(qφ(θ)) is the entropy of qφ(θ). Note that (3) cor-\nresponds to the projection step ΠH∗deﬁned in (2), and the\nﬁrst term of (4) takes an expectation of projection discrep-\nancy over the distribution θ ∼qφ(θ). The intuition behind\n(4) is that by the ﬁrst term, the objective encourages low ex-\npected discrepancy (which is equivalent to Bellman error) to\nlearn optimal policies; the second term serves as an explo-\nration bonus to encourage a dispersed distribution over θ for\nbetter exploration during learning.\nWe now draw the connection between (4) and approximate\nBayesian inference. First assign an improper uniform prior on\nθ, i.e. p(θ) ∝1. The posterior is deﬁned by Bayes rule given\nthe data {xi}N\ni=1 as p(θ|{xi}N\ni=1) ∝p(θ)p({xi}N\ni=1|θ) where\np({xi}N\ni=1|θ) = Πip(xi|θ) 3. Since by deﬁnition p(xi|θ) =\nZθ(xi), (4) is equivalent to\nmin\nφ KL[qφ(θ)||p(θ|{xi}N\ni=1)].\n(5)\nHence to minimize the objective (4) is to search for a\nparametric distribution qφ(θ) to approximate the posterior\np(θ|{xi}N\ni=1).\nFrom (5) we can see that the posterior\np(θ|{xi}N\ni=1) is the minimizer policy of (4), which achieves\nthe optimal balance between minimizing low discrepancy and\nbeing as random as possible. The close resemblance between\nour formulation and posterior sampling partially justiﬁes the\npotential strength of our exploration strategy.\n4.2\nGeneric Algorithm\nA generic algorithm Algorithm 1 can be derived from (5).\nWe start with a proposed distribution qφ(θ) over parameter θ\nand a distribution model Zθ(s, a). During control, in state st,\nwe sample a parameter from θ ∼qφ(θ) and choose action\nat = arg maxa E[Zθ(st, a)]. This is equivalent to taking an\naction based on the approximate posterior probability that it is\noptimal. During training, we sample from one-step lookahead\ndistribution of the greedy action, and update parameter by\noptimizing (4).\nAlgorithm 1 Exploration by Distributional RL: Generic\n1: INPUT: generic return distribution Zθ(s, a) with param-\neter θ, parameter distribution qφ(θ) with parameter φ.\n2: while not converged do\n3:\n// Control\n4:\nSample θ ∼qφ(θ).\n5:\nIn state st, choose at = arg maxa E[Zθ(st, a)], get\ntransition st+1 and reward rt.\n6:\n// Training\n7:\nGiven state action pair st, at, choose greedy one-\nstep lookahead distribution a′ = arg maxa E[rt +\nγZθ(st+1, a)].\n8:\nSample from the distribution rt + γZθ(st+1, a′) and\nlet ˆZ be the empirical distribution of samples, update\nparameter φ by minimizing objective (4).\n9: end while\n3We assume samples drawn from the next state distributions are\ni.i.d. as in [Bellemare et al., 2017].\n4.3\nPractical Algorithm: Gaussian Assumption\nWe turn Algorithm 1 into a practical algorithm by impos-\ning assumption on Zθ(s, a). [Dearden et al., 1998] assumes\nZθ(s, a) to be Gaussian based on the assumption that the\nchain is ergodic and γ close to 1. We make this assumption\nhere and let Zθ(s, a) be a Gaussian with parametrized mean\nQθ(s, a) and ﬁxed standard error σ. The objective (3) reduces\nto\nmin\nθ\n1\nN\nN\nX\ni=1\n(Qθ(s, a) −xi)2\n2σ2\n.\n(6)\nWe now have an analytical form E[Zθ(s, a)] = Qθ(s, a). The\nobjective (4) reduces to\nmin\nφ Eθ∼qφ(θ)[\nN\nX\ni=1\n(Qθ(s, a) −xi)2\n2σ2\n] −H(qφ(θ)).\n(7)\nAlgorithm 2 Exploration by Distributional RL: Gaussian\n1: INPUT: target parameter update period τ ; learning rate\nα; Gaussian distribution parameter σ2.\n2: INITIALIZE: parameters φ, φ−; replay buffer B ←{};\nstep counter counter ←0.\n3: for e = 1, 2, 3...E do\n4:\nwhile episode not terminated do\n5:\ncounter ←counter + 1.\n6:\nSample θ ∼qφ(θ).\n7:\nIn state st, choose at = arg maxa Qθ(st, a), get\ntransition st+1 and reward rt.\n8:\nSave experience tuple {st, at, rt, st+1} to buffer B.\n9:\nSample N parameters θ−\nj\n∼qφ−(θ−) and sample\nN tuples D = {sj, aj, rj, s′\nj} from B.\n10:\nSample target xj ∼rj + γZθ−\nj (s′\nj, a′) for jth tuple\nin D where a′ is greedy w.r.t. Qθ−(s′\nj, a).\n11:\nTake gradient ∆φ of the KL divergence in (7).\n12:\nφ ←φ −α∆φ.\n13:\nif counter mod τ = 0 then\n14:\nUpdate target parameter φ−←φ.\n15:\nend if\n16:\nend while\n17: end for\nParallel to the principal network qφ(θ) with parameter φ,\nwe maintain a target network qφ−(θ−) with parameter φ−to\nstabilize learning [Mnih et al., 2013]. Samples for updates\nare generated by target network θ−∼qφ−(θ−). We also\nmaintain a replay buffer B to store off-policy data.\n4.4\nRandomized Value Function as Randomized\nCritic for Policy Gradient\nIn off-policy optimization algorithm like Deep Determinis-\ntic Policy Gradient (DDPG) [Lillicrap et al., 2016], a policy\nπθp(s) with parameter θp and a critic Qθ(s, a) with parameter\nθ are trained at the same time. The policy gradient of reward\nobjective (1) is\n∇θpJ = E[\nX\nt\n∇aQπ(s, a)|a=πθp(st)∇θpπθp(st)]\n≈E[\nX\nt\n∇aQθ(s, a)|a=πθp(st)∇θpπθp(st)],\n(8)\nwhere replacing true Qπ(s, a) by a critic Qθ(s, a) introduces\nbias but largely reduces variance [Lillicrap et al., 2016].\nTo extend the formulation of Algorithm 2 to policy based\nmethods, we can interpret Qθ(s, a) as a randomized critic\nwith a distribution induced by θ ∼qφ(θ). At each update\nwe sample a parameter ˆθ ∼qφ(θ) and compute the policy\ngradient (8) through the sampled critic Qˆθ(s, a) to update θp.\nThe distributional parameters φ are updated as in Algorithm\n2 with the greedy actions replaced by actions produced by the\npolicy πθp.\nPolicy gradients computed from randomized critic may\nlead to better exploration directly in the policy space as in\n[Plappert et al., 2016], since the uncertainties in the value\nfunction can be propagated into the policy via gradients\nthrough the uncertain value functions.\n5\nConnections with Previous Methods\nWe now argue that the above formulation provides a concep-\ntual uniﬁcation to multiple previous methods. We can recover\nthe same objective functions as previous methods by properly\nchoosing the parametric form of return distribution Zθ(s, a),\nthe distribution over model parameter qφ(θ) and the algorithm\nto optimize the objective (5).\n5.1\nPosterior Sampling for Bandits\nIn the bandit setting, we only have a set of actions a ∈A.\nAssume the underlying reward for each action a is Gaus-\nsian distributed. To model the return distribution of action\na, we set Zθ(a) to be Gaussian with unknown mean param-\neters µa, i.e. Zθ = N(µa, σ2). We assume the distribu-\ntion over parameter qφ(µ) to be Gaussian as well. Due to the\nconjugacy between improper uniform prior p(µ) (assumed in\nSection 4.1) and likelihood Zθ(a), the posterior p(µ|{xi})\nis still Gaussian.\nWe can minimize (5) exactly by setting\nqφ(µ) = p(µ|{xi}). During control, Algorithm 1 selects\naction at = arg maxa µ(a) with sampled µ ∼qφ(µ) =\np(µ|{xi}), which is exact posterior sampling. This shows\nthat our proposed algorithm reduces to exact posterior sam-\npling for bandits. For general RL cases, the equivalence is not\nexact but this connection partially justiﬁes that our algorithm\ncan achieve very efﬁcient exploration.\n5.2\nDeep Q Network with Bayesian Updates\nDespite minor algorithmic differences, Algorithm 2 has very\nsimilar objective as Variational DQN [Tang and Kucukel-\nbir, 2017], BBQ Network [Lipton et al., 2016] and Bayesian\nDQN [Azizzadenesheli et al., 2017], i.e. all three algorithms\ncan be interpreted as having Gaussian assumption over re-\nturn distribution Zθ(s, a) ∼N(Qθ(s, a), σ2) and proposing\nGaussian distribution over parameters qφ(θ). However, it is\nworth recalling that Algorithm 2 is formulated by modeling\nreturn distributions, while previous methods are formulated\nby randomizing value functions.\nIf we are to interpret these three algorithms as instantia-\ntions of Algorithm 2, the difference lies in how they opti-\nmize (7). Variational DQN and BBQ apply variational infer-\nence to minimize the divergence between qφ(θ) and poste-\nrior p(θ|{xi}), while Bayesian DQN applies exact analytical\nupdates (exact minimization of (7)), by using the conjugacy\nof prior and likelihood distributions as discussed above. Al-\ngorithm 1 generalizes these variants of DQN with Bayesian\nupdates by allowing for other parametric likelihood models\nZθ(s, a), though in practice Gaussian distribution is very pop-\nular due to its simple analytical form.\nTo recover NoisyNet [Fortunato et al., 2017] from (7), we\ncan properly scale the objective (by multiplying (7) by σ2)\nand let σ →0. This implies that NoisyNet makes less strict\nassumption on return distribution (Gauss parameter σ does\nnot appear in objective) but does not explicitly encourage\nexploration by adding entropy bonus, hence the exploration\npurely relies on the randomization of parameter θ. To fur-\nther recover the objective of DQN [Mnih et al., 2013], we set\nqφ(θ) = δ(θ −φ) to be the Dirac distribution. Finally, since\nDQN has no randomness in the parameter θ, its exploration\nrelies on greedy action perturbations.\n5.3\nDistributional RL\nDistributional RL [Bellemare et al., 2017] models return dis-\ntribution using categorical distribution and does not introduce\nparameter uncertainties. Since there is no distribution over\nparameter θ, Algorithm 1 recovers the exact objective of dis-\ntributional RL from (4) by setting qφ(θ) = δ(θ −φ) and\nletting Zθ(s, a) be categorical distributions. As the number\nof atoms in the categorical distribution increases, the model-\ning becomes increasingly close to non-parametric estimation.\nThough having more atoms makes the parametric distribu-\ntion more expressive, it also poses a bigger statistical chal-\nlenge during learning due to a larger number of parameters.\nAs with general Zθ(s, a), choosing a parametric form with\nappropriate representation power is critical for learning.\n6\nExperiments\nIn all experiments, we implement Algorithm 2 and refer to it\nas GE (Gauss exploration) in the following. We aim to answer\nthe following questions,\n• In environments that require consistent exploration, does\nGE achieve more efﬁcient exploration than conventional\nnaive exploration strategies like ϵ−greedy in DQN and\ndirect parameter randomization in NoisyNet?\n• When a deterministic critic in an off-policy algorithm\nlike DDPG [Lillicrap et al., 2016] is replaced by a ran-\ndomized critic, does the algorithm achieve better explo-\nration?\n6.1\nTesting Environment\nChain MDP.\nThe chain MDP [Osband et al., 2016] (Fig-\nure 1) serves as a benchmark to test if an algorithm entails\nconsistent exploration. The environment consists of N states\nand each episode lasts N + 9 time steps. The agent has two\nFigure 1: Chain MDP with N states\nactions {left, right} at each state si, 1 ≤i ≤N, while state\ns1, sN are both absorbing. The transition is deterministic. At\nstate s1 the agent receives reward r =\n1\n1000, at state sN the\nagent receives reward r = 1 and no reward anywhere else.\nThe initial state is always s2, making it hard for the agent\nto escape local optimality at s1. If the agent explores uni-\nformly randomly, the expected number of time steps required\nto reach sN is 2N−2. For large N, it is almost not possible for\nthe randomly exploring agent to reach sN in a single episode,\nand the optimal strategy to reach sN will never be learned.\nSparse Reward Environments.\nAll RL agents require re-\nward signals to learn good policies. In sparse reward envi-\nronment, agents with naive exploration strategies randomly\nstumble around for most of the time and require many\nmore samples to learn good policies than agents that ex-\nplore consistently. We modify the reward signals in OpenAI\ngym [Brockman et al., 2016] and MuJoCo benchmark tasks\n[Todorov et al., 2012] to be sparse as follows.\n• MountainCar, Acrobot: r = 1 when the episode termi-\nnates and r = 0 otherwise.\n• CartPole, InvertedPendulum, InvertedDoublePendulum:\nr = −1 when the episode terminates and r = 0 other-\nwise.\n6.2\nExperiment Results\nExploration in Chain MDP.\nIn Figure 2 (a) - (c) we com-\npare DQN vs NoisyNet vs GE in Chain MDP environments\nwith different number of states N. When N = 10, all three\nalgorithms can solve the task. When N = 50, DQN can-\nnot explore properly and cannot make progress, GE explores\nmore efﬁciently and converges to optimal policy faster than\nNoisyNet. When N = 100, both NoisyNet and DQN get\nstuck while GE makes progress more consistently. Compared\nto Bootstrapped DQN (BDQN)[Osband et al., 2016], GE has\na higher variance when N = 100. This might be because\nBDQN represents the distribution using multiple heads and\ncan approximate more complex distributions, enabling better\nexploration on this particular task. In general, however, our\nalgorithm is much more computationally feasible than BDQN\nyet still achieves very efﬁcient exploration.\nFigure 2 (d) plots the state visit frequency for GE vs. DQN\nwithin the ﬁrst 10 episodes of training. DQN mostly visits\nstates near s2 (the initial state), while GE visits a much wider\nrange of states. Such active exploration allows the agent to\nconsistently visit sN and learns the optimal policy within a\nsmall number of iterations.\nExploration in Sparse Reward Environments.\nIn Figure\n3 (a) - (c) we present the comparison of three algorithms in\n(a) Chain MDP N = 10\n(b) Chain MDP N = 50\n(c) Chain MDP N = 100\n(d) State visit frequency\nFigure 2: Comparison of DQN vs NoisyNet vs GE on Chain MDP\nenvironments with (a) N = 10 (b) N = 50 and (c) N = 100 states.\nFigure 2 (d) plots state visit frequency within the ﬁrst iteration in\ntraining for Gauss vs. DQN in Chain MDP N = 128. For state si,\nset ci = 1 if si is ever visited in one episode and ci = 0 otherwise.\nThe moving average of ci across multiple episodes computes the\nstate visit frequency. Each iteration consists of 20 episodes.\nsparse reward environments. For each environment, we plot\nthe rewards at a different scale. In CartPole, the plotted cu-\nmulative reward is the episode length; in MountainCar, the\nplotted cumulative reward is 1 for reaching the target within\none episode and 0 otherwise; in Acrobot, the plotted cumu-\nlative reward is the negative of the episode length.\nIn all\nsparse reward tasks, GE entails much faster progress than the\nother two algorithms. For example, in Sparse MountainCar,\nwithin the given number of iterations, DQN and NoisyNet\nhave never (or very rarely) reached the target, hence they\nmake no (little) progress in cumulative reward. On the other\nhand, GE reaches the targets more frequently since early stage\nof the training, and makes progress more steadily.\nIn Figure 3 (d) we plot the state visit trajectories of GE\nvs. DQN in Sparse MountainCar. The vertical and horizon-\ntal axes of the plot correspond to two coordinates of the state\nspace. Two panels of (d) correspond to training after 10 and\n30 iterations respectively. As the training proceeds, the state\nvisits of DQN increasingly cluster on a small region in state\nspace and fail to efﬁciently explore. On the contrary, GE\nmaintains a widespread distribution over states and can ex-\nplore more systematically.\nRandomized Critic for Exploration.\nWe evaluate the per-\nformance of DDPG with different critics. When DQN is used\nas a critic, the agent explores by injecting noise into actions\nproduced by the policy [Lillicrap et al., 2016]. When crit-\nics are NoisyNet or randomized DQN with GE, the agent ex-\nplores by updating its parameters using policy gradients com-\nputed through randomized critics, effectively injecting noise\ninto the parameter space. In conventional continuous control\n(a) Sparse CartPole\n(b) Sparse MountainCar\n(c) Sparse Acrobot\n(d) State visit trajectories\nFigure 3: Comparison of DQN vs NoisyNet vs GE on sparse re-\nward environments (a) Sparse CartPole (b) Sparse MountainCar (c)\nSparse Acrobot. Each iteration corresponds to 20 episodes. Rewards\nare plotted using moving windows of 20 episodes. Figure 3 (d) plots\nstate visit trajectories for Gauss vs. DQN in Sparse MountainCar.\nLeft panel of (d) is training after 10 iterations and the right panel is\nafter 30 iterations. The vertical and horizontal axes correspond to\ntwo coordinates of the state space.\ntasks (Figure 4 (a) and (b)), randomized critics do not enjoy\nmuch advantage: for example, in simple control task like In-\nvertedPendulum, where exploration is not important, DDPG\nwith action noise injection makes progress much faster (Fig-\nure 4 (a)), though DDPG with randomized critics seem to\nmake progress in a steadier manner. In sparse reward envi-\nronments (Figure 4 (c) and (d)), however, DDPG with ran-\ndomized critics tend to make progress at a slightly higher rate\nthan action noise injection.\nHyper-parameter.\nIn all experiments, we set qφ(θ) to be\nfactorized Gaussian. In GE, as in NoisyNet [Fortunato et al.,\n2017], each parameter θ in a fully connected layer (weight\nand bias) has two distributional parameters: the mean µθ and\nstandard error σθ. Set σθ = log(1 + exp(−ρθ)) and let ρθ be\nthe actual hyper-parameter to tune. If ρθ is large, the distribu-\ntion over θ is widespread and the agent can execute a larger\nrange of policies before committing to a solution. For both\nNoisyNet and GE, we require all ρθ to be the same, denoted\nas ρ, and set the range ρ ∈[−1, −10] for grid search. A\nsecond hyper-parameter for GE is the Gauss parameter σ2 to\ndetermine the balance between expected Bellman error and\nentropy in (7). In our experiments, we tune σ on the log scale\nlog10 σ ∈[−1, −8].\nWe empirically ﬁnd that both ρ, σ are critical to the perfor-\nmance of GE. For each algorithm, We use a fairly exhaustive\ngrid search to obtain the best hyper-parameters. Each experi-\nment is performed multiple times and the reward plots in Fig-\nure 2,3,4 are averaged over ﬁve different seeds. In Figure 5,\nwe plot the performance of GE under different ρ and σ on\n(a) InvertedPendulum\n(b) InvertedDoublePendulum\n(c) Sparse InvertedPendulum\n(d) Sparse DoublePendulum\nFigure 4: Comparison of original Q function (DQN) vs NoisyNet\nvs GE as baselines for DDPG on sparse reward environments (a)\nInvertedPendulum (b) InvertedDoublePendulum (c) Sparse Invert-\nedPendulum (d) Sparse InvertedDoublePendulum.\n(a) Hyper-parameter ρ\n(b) Hyper-parameter σ\nFigure 5: Hyper-parameter for Gauss Exploration (GE)\nSparse CartPole. From Figure 5 we see that the performance\nis not monotonic in ρ, σ: large σ (small ρ) generally leads to\nmore active exploration but may hinder fast convergence, and\nvice versa. One must strike a proper balance between explo-\nration and exploitation to obtain good performance. In DQN,\nwe set the exploration constant to be ϵ = 0.1. In all experi-\nments, we tune the learning rate α ∈{10−3, 10−4, 10−5}.\n7\nConclusion\nWe have provided a framework based on distributional RL\nthat uniﬁes multiple previous methods on exploration in re-\ninforcement learning, including posterior sampling for ban-\ndits as well as recent efforts in Bayesian updates of DQN pa-\nrameters. We have also derived a practical algorithm based\non the Gaussian assumption of return distribution, which al-\nlows for efﬁcient control and parameter updates. We have ob-\nserved that the proposed algorithm obtains good performance\non challenging tasks that require consistent exploration. A\nfurther extension of our current algorithm is to relax the Gaus-\nsian assumption on return distributions. We leave it be future\nwork if more ﬂexible assumption can lead to better perfor-\nmance and whether it can be combined with model-based RL.\nReferences\n[Azizzadenesheli et al., 2017] Kamyar\nAzizzadenesheli,\nEmma Brunskill, and Animashree Anandkumar.\nEf-\nﬁcient exploration through bayesian deep q networks.\nSymposium on Deep Reinforcement Learning, NIPS,\n2017.\n[Bellemare et al., 2017] Marc G. Bellemare, Will Dabney,\nand Remi Munos.\nA distributional perspective on rein-\nforcement learning. International Conference on Machine\nLearning, 2017.\n[Blei et al., 2017] David M. Blei,\nAlp Kucukelbir,\nand\nJon D. McAuliffe.\nVariational inference: A review for\nstatisticians. Journal of the American Statistical Associ-\nation, Volume 112 - Issue 518, 2017.\n[Brockman et al., 2016] Greg Brockman,\nVicki Cheung,\nLudwig Pettersson, Jonas Schneider, John Schulman, Jie\nTang, and Wojciech Zaremba.\nOpenai gym.\nArxiv:\n1606.01540, 2016.\n[Dearden et al., 1998] Richard Dearden, Nir Friedman, and\nStuart Russel. Bayesian q learning. American Association\nfor Artiﬁcial Intelligence (AAAI), 1998.\n[Duan et al., 2016] Yan Duan, Xi Chen, Rein Houthooft,\nJohn Schulman, and Pieter Abbeel. Benchmarking deep\nreinforcement learning for continuous control.\nInterna-\ntional Conference on Machine Learning, 2016.\n[Fortunato et al., 2017] Meire\nFortunato,\nMoham-\nmad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian\nOsband, Alex Graves, Vlad Mnih, Remi Munos, Demis\nHassabis, Ilivier Pietquin, Charles Blundell, and Shane\nLegg. Noisy network for exploration. arXiv:1706.10295,\n2017.\n[Henderson et al., 2017] Peter Henderson, Thang Doan, Ri-\nashat Islam, and David Meger. Bayesian policy gradients\nvia alpha divergence dropout inference. 2nd Workshop on\nBayesian Deep Learning, NIPS, 2017.\n[Levine et al., 2016] Sergey Levine, Chelsea Finn, Trevor\nDarrell, and Pieter Abbeel. End to end training of deep\nvisuomotor policies.\nJournal of Machine Learning Re-\nsearch, 2016.\n[Lillicrap et al., 2016] Timothy P. Lillicrap,\nJonathan J.\nHunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yu-\nval Tassa, David Silver, and Daan Wierstra. Continuous\ncontrol with deep reinforcement learning. International\nConference on Learning Representations, 2016.\n[Lipton et al., 2016] Zachary C. Lipton, Xiujun Li, Jianfeng\nGao, Lihong Li, Faisal Ahmed, and Li Deng.\nEfﬁ-\ncient dialogue policy learning with bbq-networks. ArXiv:\n1608.05081, 2016.\n[Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep\nreinforcement learning. NIPS workshop in Deep Learning,\n2013.\n[Moerland et al., 2017] Thomas\nM.\nMoerland,\nJoost\nBroekens, and Catholijn M. Jonker. Efﬁcient exploration\nwith double uncertain value networks.\nSymposium on\nDeep Reinforcement Learning, NIPS, 2017.\n[Morimura et al., 2010] Tetsuro\nMorimura,\nMasashi\nSugiyama, Hisashi Kashima, Hirotaka Hachiya, and\nToshiyuki Tanaka.\nNonparametric return distribution\napproximation for reinforcement learning. ICML, 2010.\n[Morimura et al., 2012] Tetsuro\nMorimura,\nMasashi\nSugiyama, Hisashi Kashima, Hirotaka Hachiya, and\nToshiyuki Tanaka.\nParametric return density estimation\nfor reinforcement learning. UAI, 2012.\n[Osband and Van Roy, 2015] Ian Osband and Benjamin Van\nRoy. Bootstrapped thompson sampling and deep explo-\nration. arXiv:1507:00300, 2015.\n[Osband et al., 2013] Ian Osband, Daniel Russo, and Ben-\njamin Van Roy. (more) efﬁcient reinforcement learning\nvia posterior sampling. Arxiv: 1306.0940, 2013.\n[Osband et al., 2016] Ian Osband, Charles Blundell, Alexan-\nder Pritzel, and Benjamin Van Roy. Deep exploration via\nbootstrapped dqn. arXiv:1602.04621, 2016.\n[Osband et al., 2017] Ian Osband, daniel Russo, Zheng Wen,\nand Benjamin Van Roy. Deep exploration via randomized\nvalue functions. arXiv: 1703.07608, 2017.\n[Plappert et al., 2016] Matthias Plappert, Rein Houthooft,\nPrafulla Dhariwal, Szymon Sidor, Richard Y. Chen,\nXi Chen, Tamim Asfour, Pieter Abbeel, and Marcin\nAndrychowicz. Parameter space noise for exploration. In-\nternational Conference on Learning Representation, 2016.\n[Ranganath et al., 2014] Rejesh Ranganath, Sean Gerrish,\nand David M. Blei. Black box variational inference. Pro-\nceedings of the 17th International Conference on Artiﬁcial\nIntelligence and Statistics (AISTATS), 2014.\n[Russo, 2017] Daniel Russo.\nTutorial on thompson sam-\npling. arxiv, 2017.\n[Schulman et al., 2015] John\nSchulman,\nSergey\nLevine,\nPhilipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust\nregion policy optimization. International Conference on\nMachine Learning, 2015.\n[Tang and Kucukelbir, 2017] Yunhao Tang and Alp Ku-\ncukelbir. Variational deep q network. 2nd Workshop on\nBayesian Deep Learning, NIPS, 2017.\n[Thompson, 1933] William R. Thompson. On the likelihood\nthat one unknown probability exceeds another in view of\nthe evidence of two samples. Biometrika, Vol. 25, No. 3/4,\n1933.\n[Todorov et al., 2012] Emanuel Todorov, Tom Erez, and Yu-\nval Tassa. Mujoco: A physics engine for model-based con-\ntrol. International Conference on Intelligent Robots, 2012.\n[Tsitsiklis and Van Roy, 1996] John N. Tsitsiklis and Ben-\njamin Van Roy.\nFeature based methods for large scale\ndynamic programming. Machine Learning, 1996.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-05-04",
  "updated": "2018-06-21"
}