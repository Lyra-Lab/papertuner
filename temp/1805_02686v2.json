{
  "id": "http://arxiv.org/abs/1805.02686v2",
  "title": "Holarchic Structures for Decentralized Deep Learning - A Performance Analysis",
  "authors": [
    "Evangelos Pournaras",
    "Srivatsan Yadhunathan",
    "Ada Diaconescu"
  ],
  "abstract": "Structure plays a key role in learning performance. In centralized\ncomputational systems, hyperparameter optimization and regularization\ntechniques such as dropout are computational means to enhance learning\nperformance by adjusting the deep hierarchical structure. However, in\ndecentralized deep learning by the Internet of Things, the structure is an\nactual network of autonomous interconnected devices such as smart phones that\ninteract via complex network protocols. Self-adaptation of the learning\nstructure is a challenge. Uncertainties such as network latency, node and link\nfailures or even bottlenecks by limited processing capacity and energy\navailability can signif- icantly downgrade learning performance. Network\nself-organization and self-management is complex, while it requires additional\ncomputational and network resources that hinder the feasibility of\ndecentralized deep learning. In contrast, this paper introduces a self-adaptive\nlearning approach based on holarchic learning structures for exploring,\nmitigating and boosting learning performance in distributed environments with\nuncertainties. A large-scale performance analysis with 864000 experiments fed\nwith synthetic and real-world data from smart grid and smart city pilot\nprojects confirm the cost-effectiveness of holarchic structures for\ndecentralized deep learning.",
  "text": "Noname manuscript No.\n(will be inserted by the editor)\nHolarchic Structures for Decentralized Deep\nLearning – A Performance Analysis\nEvangelos Pournaras · Srivatsan\nYadhunathan · Ada Diaconescu\nReceived: date / Accepted: date\nAbstract Structure plays a key role in learning performance. In centralized\ncomputational systems, hyperparameter optimization and regularization tech-\nniques such as dropout are computational means to enhance learning perfor-\nmance by adjusting the deep hierarchical structure. However, in decentralized\ndeep learning by the Internet of Things, the structure is an actual network\nof autonomous interconnected devices such as smart phones that interact via\ncomplex network protocols. Self-adaptation of the learning structure is a chal-\nlenge. Uncertainties such as network latency, node and link failures or even\nbottlenecks by limited processing capacity and energy availability can signif-\nicantly downgrade learning performance. Network self-organization and self-\nmanagement is complex, while it requires additional computational and net-\nwork resources that hinder the feasibility of decentralized deep learning. In\ncontrast, this paper introduces a self-adaptive learning approach based on ho-\nlarchic learning structures for exploring, mitigating and boosting learning per-\nformance in distributed environments with uncertainties. A large-scale perfor-\nmance analysis with 864000 experiments fed with synthetic and real-world data\nfrom smart grid and smart city pilot projects conﬁrm the cost-eﬀectiveness of\nholarchic structures for decentralized deep learning.\nKeywords deep learning · optimization · self-adaptation · holarchy · Internet\nof Things · dropout · multi-agent system · distributed system · network ·\nSmart City · Smart Grid\nEvangelos Pournaras · Srivatsan Yadhunathan\nProfessorship of Computational Social Science\nETH Zurich, Zurich, Switzerland\nTel.: +41446320458\nE-mail: {epournaras,ysrivatsan}@ethz.ch\nAda Diaconescu\nTelecom ParisTech\nParis-Saclay University, Paris, France\nTel.: +33145818072\nE-mail: ada.diaconescu@telecom-paristech.fr\narXiv:1805.02686v2  [cs.LG]  17 Sep 2018\n2\nEvangelos Pournaras et al.\n1 Introduction\nSmart citizens’ devices with increasing processing power and high energy au-\ntonomy are becoming pervasive and ubiquitous in everyday life. The Internet\nof Things empowers a high level of interconnectivity between smart phones,\nsensors and wearable devices. These technological developments provide un-\nprecedented opportunities to rethink about the future of machine learning\nand artiﬁcial intelligence: Centralized computational intelligence can be of-\nten used for privacy-intrusive and discriminatory services that create ‘ﬁlter\nbubbles’ and undermine citizens’ autonomy by nudging [12,31,17]. In con-\ntrast, this paper envisions a more socially responsible design for digital society\nbased on decentralized learning and collective intelligence formed by bottom-\nup planetary-scale networks run by citizens [38,18].\nIn this context, the structural elements of decentralized deep learning pro-\ncesses play a key role. The eﬀectiveness of several classiﬁcation and prediction\noperations often relies heavily on hyperparameter optimization [27,53] and on\nthe learning structure, for instance, the number of layers in a neural network,\nthe interconnectivity of the neurons, the activation or deactivation of certain\npathways i.e. dropout regularization [51], can enhance learning performance.\nControlling and adjusting a deep hierarchical structure in a centralized com-\nputing system is straightforward in the sense that all meta information for\nmodel generation is locally available. However, in decentralized learning the\nchallenge of optimizing the learning structure is not anymore exclusively a\ncomputational problem. Other challenges such as network latency, node and\nlink failures as well as the overall complexity of building and maintaining an\noverlay network [36] in a distributed environment perplex the feasibility of\ndecentralized learning.\nThis paper introduces the concept of holarchy in deep hierarchical struc-\ntures as the means to adapt to the aforementioned uncertainties of distributed\nenvironments. A learning process can be localized and performed over a ho-\nlarchic structure in a recursive way without changing the core learning logic\nand without employing additional mechanisms to reconﬁgure the network.\nThis is the proposed self-adaption approach to decentralized learning that is\nby design highly reactive and cost-eﬀective as it maximizes the utilization of\nthe available communication and computational resources, in contrast to a\ncomplementary and more proactive self-organization approach that requires\nadditional interactions between agents and therefore can increase communica-\ntion and computational cost. By using holarchies for learning, forward prop-\nagation and backpropagation become recursive in nested levels over the deep\nhierarchical structure as the means to (i) explore improving solutions, (ii) mit-\nigate learning performance in case part of the network is disconnected or even\n(iii) boost learning performance after the default learning process completes.\nThese three scenarios are formalized by three holarchic schemes applied to\na multi-agent system for decentralized deep learning in combinatorial opti-\nmization problems: I-EPOS, the Iterative Economic Planning and Optimized\nSelections [38].\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n3\nA large-scale performance analysis with 864000 experiments is performed\nusing synthetic and real-world data from pilot projects such as bike sharing,\nenergy demand and electric vehicles. Several dimensions are studied, for in-\nstance, topological properties of the deep hierarchical structure, constraints by\nthe agents’ preferences and the scale of the holarchic structures, i.e. number of\nnested layers. Results conﬁrm the cost-eﬀectiveness of the holarchic learning\nschemes for exploration, mitigation and boosting of learning performance in\ndynamic distributed environments. Nevertheless, in stable environments the\nlocalization of the learning process within holarchic structures may result in\nlower performance compared to a learning applied system-wide.\nIn summary, the contributions of this paper are the following:\n– A novel self-adaptation approach to decentralized deep learning as the\nmeans to decrease or avoid the communication and computational cost of\nself-organization in distributed environments with uncertainties.\n– The concept of holarchy as a self-adaptation approach for decentralized\ndeep learning.\n– The introduction of three holarchic schemes as self-adaptation means to\nexplore, mitigate and boost deep learning performance.\n– The applicability and extension of I-EPOS with the three holarchic schemes\nto perform collective decision-making in decentralized combinatorial opti-\nmization problems.\n– An empirical performance analysis of 864000 benchmark experiments gen-\nerated with synthetic and real-world data from Smart Grid and Smart City\npilot projects.\nThis paper is organized as follows: Section 2 introduces the concept of\nholarchy in decentralized learning as well as three holarchic schemes to ex-\nplore, mitigate and boost deep learning performance under uncertainties of\ndistributed environments. Section 3 illustrates a case study of a decentral-\nized deep learning system to which the three holarchic schemes are applied:\nI-EPOS. Section 4 shows the experimental methodology followed to conduct\na large-scale performance analysis of the three holarchic schemes. Section 5\nillustrates the results of the experimental evaluation. Section 6 summarizes\nand discusses the main ﬁndings. Section 7 positions and compares this paper\nwith related work. Finally, Section 8 concludes this paper and outlines future\nwork.\n2 Holarchic Structures for Decentralized Deep Learning\nThis paper studies decentralized deep learning processes in which the deep\nhierarchical structure is fully distributed and self-organized by remote au-\ntonomous (software) agents that interact over a communication network. In\nother words, the learning process is crowd-sourced to citizens, who participate\nby contributing their computational resources, for instance their personal com-\nputers or smart phones. The agents reside on these devices and collectively run\n4\nEvangelos Pournaras et al.\nthe decentralized learning process. For example, in contrast to a conventional\nmodel of a centralized neural network, which performs training by locally ac-\ncessing all citizens’ data, a collective neural network consists of neurons that\nare remote citizens’ devices interacting over the Internet to form an overlay\npeer-to-peer network [36]. It is this overlay network that represents the hier-\narchical neural network structure.\nMethods for hyperparameter optimization of the hierarchical structure to\nimprove the learning performance of a model are usually designed for cen-\ntralized systems in which all information, including input data, the network\nstructure and the learning model itself are locally available. This provides a\nlarge spectrum of ﬂexibility to change the deep hierarchical structure oﬄine\nor even online [16,42] and determine via hyperparameter optimization the set-\ntings that maximize the learning performance. In contrast to learning based on\ncentralized computational systems, in decentralized deep learning the structure\ncannot arbitrary change without paying for some computational and commu-\nnication cost. Network uncertainties such as node and link failures, latency\nas well as limited resources in terms of bandwidth, computational capacity or\neven energy in case of sensors and smart phones can limit performance, inter-\nrupt the learning process and increase the design complexity of decentralized\nhyperparameter optimization.\nThe aforementioned uncertainties of distributed environments introduce\nendogenous constraints in parts of the deep hierarchical structure: the learning\nprocess is interrupted and becomes localized within branches of the hierarchi-\ncal structure. For instance, node and link failures or suspension of the learning\nprocesses due to conservation of resources [3,52] in nodes are scenarios under\nwhich learning can be localized. This paper poses here the following ques-\ntion: How to preserve the learning capacity of a decentralized system, whose\nlearning process is interrupted and localized by aforementioned uncertianties\nin distributed environments? On the one hand, it is known that localization\nand greedy optimization can underperform with search becoming trapped to\nlocally optimum solutions that have a signiﬁcant divergence from global op-\ntimality [4]. On the other hand, limiting the learning units of hierarchical\nstructures can also increase learning performance by preventing overﬁtting as\nknown by the dropout concept in neural networks [51]. This paper sheds light\non the role of localization in decentralized learning.\nIn this context, the management of the learning performance of an algo-\nrithm is no longer entirely a computational challenge but rather a multifaceted\nself-adaptation process: Other aspects such as performance exploration, miti-\ngation and boosting come to the analysis foreground. Exploration adapts the\nlearning process and the search space to improve learning performance under\nlocalization. Mitigation is the maintenance of a high learning performance un-\nder a localization of the learning process. Finally, the feasibility of boosting\nthe learning performance under localization is subject of this study as well.\nThis paper introduces the concept of holarchy in deep learning hierarchi-\ncal structures to study the performance exploration, mitigation and boosting\npotential under the aforementioned uncertainties of distributed environments.\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n5\nA holarchy is a recursive hierarchical network of holons that represent part of\nthe deep hierarchical structure as well as the whole structure. In the case of a\ntree topology, every possible branch (part) in the whole tree topology is also a\ntree topology (whole). When an agent (parent) connects two branches, it forms\nanother nested holon that is the next level of the holarchic structure. This re-\ncursive process starts from the parents of the leaves in the tree and progresses\nup to the root as shown in Figure 1. Learning iterations can be independently\nexecuted in every holonic branch before the process progresses to the next\nlevel of the holarchic structure, in which a new series of learning iterations\nare executed. The top holonic branch is actually the whole tree topology and\ntherefore the execution of learning iterations at this top level corresponds to\nthe learning iterations without a holarchic structure. In other words, the con-\ncept of holarchy introduces multiple localized, nested and incremental learning\nprocesses.\n(a)\nLearning\niterations\nbetween\nthe\nleaves\nand\ntheir parents.\n(b) Progress to the next\nlevel. Learning iterations\nare performed within tree\nbranches.\n(c)\nHolarchic\nlearning\ncompletes\nwith\nlearning\niterations performed over\nthe whole tree structure.\nFig. 1 The concept of holarchic learning. Learning iterations are performed in nested\nbranches, the holons. Figure 1c actually depicts the default baseline learning strategy, while\nFigure 1a and 1b show the earlier learning iterations performed within the holons.\nThe nested learning processes of a holarchic system enable the management\nof each level independently from the higher and lower levels, which further\nallows the management separation between the agents at each level. This is\na divide-and-conquer approach to complex system management and hence\nimproves localization, parallelism, reusablity and diversity with a potential\nin improving system performance and robustness while decreasing costs [8].\nFrom a software engineering perspective, the following key properties of a\nholonic design are identiﬁed [1,8]: (i) Bottom-up abstraction that represents\nthe aggregation of information from lower levels. (ii) Partial isolation within\nand among levels – ensuring an agent’s decision results from the aggregation\nof agents’ decisions at a lower level and results in the agent’s decision at a\n6\nEvangelos Pournaras et al.\nhigher level, hence other agents do not inﬂuence. (iii) Inter-level time tuning –\nensuring that the relative execution times at diﬀerent holarchic levels are set\nin order to avoid cross-level oscillations and divergence.\nThis paper studies whether learning processes structured within holarchies\nare cost-eﬀective countermeasures to adapt to the uncertainties of distributed\nenvironments. Holarchies can provide the following operational ﬂexibility and\nadaptation: (i) The learning process can be limited to a targeted part of the\nnetwork to prevent a network-wide use of computational and communication\nresources, i.e. agents can participate in the learning process on-demand. Any\nfailure to serve the participation does not disrupt the learning process that can\ncontinue within part of the network. (ii) The mapping and deployment of hol-\narchies on the network can be designed according to the network heterogeneity,\ni.e. varying latency, computational and battery capacities. For instance, higher\nperforming nodes can be placed at the bottom of a holarchy to serve their more\nfrequent recurrent use in bottom-up learning interactions.\nThis paper studies three self-adaptation scenarios of the holarchic con-\ncept in decentralized deep learning each designed for performance exploration,\nmitigation and boosting respectively: (i) holarchic initialization, (ii) holarchic\nruntime and (iii) holarchic termination. Assume a baseline scheme that in-\nvolves a tree structure with agents interacting in a (i) bottom-up phase and\na (ii) top-down phase that both complete a learning iteration. The former\nphase may represent a ﬁt forward learning process starting from the leaves\nand completing to the root while the latter phase a backpropagation start-\ning from the root and reaching back the leaves. Without loss of generality,\nan exact decentralized learning algorithm realizing these concepts is presented\nin Section 3. Learning iterations repeat to decrease a cost function. Learning\nconverges when a certain number of iterations is performed or when the cost\nfunction cannot be decreased further. Figure 2a illustrates the baseline scheme.\nFigure 1c depicts one baseline learning iteration, while within each nested\nholon formed during the bottom-up phase several learning iterations are per-\nformed. This process is common in all holarchic schemes. Holarchic initializa-\ntion is applied before baseline to perform an exploration of the search space.\nSeveral learning iterations can be performed before switching to baseline as il-\nlustrated in Figure 2b. In contrast, holarchic runtime applies holarchic learning\nthroughout runtime without switching to baseline as shown in Figure 2c. This\nscheme is applicable in self-adaptation scenarios of failures or conservation of\nresources to mitigate losses of learning performance. Finally, holarchic termi-\nnation is applied after the baseline convergence as the means to further boost\nthe baseline performance. This self-adaptation scheme is shown in Figure 2d.\n3 Applicability of Holarchic Schemes\nThis section illustrates a case study for the applicability of holarchic struc-\ntures in decentralized deep learning for combinatorial optimization problems:\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n7\n(a) Baseline: Throughout runtime a ﬁxed number of learning iterations is performed\nduring which convergence is potentially achieved. The bottom-up phase starts from the\nleaves and progresses level-by-level up to the root, while the reverse process (backpropaga-\ntion) is performed in the top-down phase. In both phases, the parent-children interactions\nalways progress to the next level, in contrast to the holarchic strategies in which learning\niterations are performed in nested tree branches as shown below.\n(b) Holarchic initialization: Before the baseline execution, holarchic learning is per-\nformed.\n(c) Holarchic runtime: Holarchic learning is performed throughout the runtime. Note\nthat one learning iteration corresponds to multiple holarchic iterations performed in each\nholon.\n(d) Holarchic termination: Holarchic learning is activated after convergence is reached\nwith baseline to potentially discover improved solutions.\nFig. 2 The four learning schemes studied and compared in this paper.\nI-EPOS1, the Iterative Economic Planning and Optimized Selections [38,35].\nI-EPOS consists of agents that autonomously plan resources they consume and\nproduce. Planning is a process of resource scheduling or resource allocation.\nFor instance agents may represent smart phone apps (personal assistants),\ncyber-physical controllers or smart home information systems with the capa-\n1 Available at epos-net.org (last accessed: September 2018)\n8\nEvangelos Pournaras et al.\nbility to plan the energy consumption of residential appliances, the charging\nof electric vehicles or the choices of bike sharing stations. Planning serves the\nlocal resource requirements of users as well as system-wide objectives, for in-\nstance, the decrease of demand oscillations in the whole power grid to prevent\nblackouts [40], the synchronization of power demand with the availability of\nrenewables [2] or the load-balancing of bike sharing stations to decrease the\noperational costs of manual bike relocations [50,38].\nI-EPOS introduces the computational model of locally generated possible\nplans that represent users’ operational ﬂexibility on how resources can be con-\nsumed or produced. For instance, a user may turn on a laundry machine earlier\nor later in time, can choose among two or more stations to return a shared\nbike, etc. Computationally, plans are vectors of real values and agents need to\ncollectively choose one and only one of these plans to execute so that the sum-\nmation of all these selected plans satisﬁes a system-wide objective measured\nby a global cost function. This paper focuses on the computational problem of\nthe variance minimization that is a quadrtic cost function, which cannot be\nlocally minimized: coordination and collective decision-making is required [43].\nChoosing the optimum combination of plans, whose summation minimizes a\nquadratic cost function is a non-convex combinatorial optimization problem\nknown to be NP-hard [41].\nAmong the global cost that captures system-wide objectives, agents can\nalso assess a local cost of their possible plans that may represent a notion\nof discomfort or inconvenience [39]. For instance, the longer the time period\na user shifts the energy consumption earlier or later in time to prevent a\npower peak (global cost reduction) the higher the level of disruption is in the\nregular residential activities of a user (local cost increase). The agents’ choices\ncan be autonomously parameterized to increase or decrease the priority of\nminimizing the local cost over the global cost. This trade-oﬀis regulated by\nthe λ parameter for each agent. A λ = 0 results in choices that exclusively\nminimize the global cost, i.e. variance, and ignores the local cost. In contrast,\na λ > 0 biases the agents’ choices to favor plans with a lower local cost.\nLearning is performed as follows: agents self-organize [36] in a tree network\ntopology over which collective decision-making is performed – a plan is chosen\nby taking into account the following aggregate information (element-wise sum-\nmation of vectors): (i) the aggregate plan choices of the agents in the branch\nunderneath made available during the bottom-up phase and (ii) the aggregate\nplan choices of all agents at the previous learning iteration made available dur-\ning the top-down phase. Note that decision-making remains highly localized\nand decentralized as the planning information of the other agents is always\nat an aggregate level, i.e. the possible plans of other agents are not explicitly\nrequired.\nSeveral of the following factors inﬂuence the learning performance deﬁned\nby the level of the global cost reduction and the number of learning iterations\nrequired to minimize variance: (i) the positioning of the agents in the tree\nthat determines the order of the collective choices made, (ii) the λ parameter\nthat regulates the trade-oﬀof global versus local cost and (iii) the overall\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n9\ntopological structure and speciﬁcally in this paper the number of children c in\nbalanced trees is studied.\nImproving the learning performance by repositioning the agents in the tree\nor adapting the topology is complex and costly in distributed environments as\nthe aforementioned self-organization methodologies are based on supplemen-\ntary distributed protocols that consume resources, i.e. exchange of messages,\ncomputational power, energy, etc. Moreover, any node or link failure limits the\ncomputation of the aggregate plans at a branch level and therefore the col-\nlective decision-making cannot be anymore performed over all participating\nagents. The applicability of the holarchic schemes does not require any change\nin the logic of I-EPOS. The algorithm is localized and applied within multiple\nnested and connected branches even when the network becomes disconnected\ndue to node and link failures: A disconnected agent triggers adaptation by chil-\ndren that turn to roots of holons and initiate the top-down phase of I-EPOS.\nThe fact that a number of agents is isolated and does not participate in the\nlearning process is the self-adaptation means to traverse the optimization space\nvia alternative pathways. This has the potential to explore improving solutions,\nmitigate performance loss compared to a total interruption of I-EPOS, or even\nboost the reduction of the global cost that cannot be decreased anymore with\nI-EPOS.\nNote that I-EPOS is selected to evaluate the applicability of holarchic\nschemes as it is a fully decentralized and hierarchical learning algorithm. The\ngoal of learning in terms of whether it is designed for classiﬁcation, prediction,\npattern recognition, etc. or whether textual or image data are used, do not\ninﬂuence the applicability of holarchic schemes. The rest of this paper illus-\ntrates an empirical performance analysis of the three holarchic schemes and\ntheir applicability to I-EPOS.\n4 Experimental Methodology\nThis section illustrates the system prototyping, the varying dimensions in the\nexperiments and the experimental settings. It also illustrates the evaluation\nmetrics used to assess the holarchic learning schemes.\n4.1 Prototyping and test runs\nAn open-source implementation of I-EPOS2 is used for the prototyping of\nthe holarchic schemes3. The software is implemented using the Protopeer dis-\ntributed prototyping toolkit [13] and is designed to run in a simulation and live\nmode. A software artifact4 of EPOS for the broader scientiﬁc community is\n2 Available at https://github.com/epournaras/EPOS (last accessed: September 2018).\n3 Available at https://github.com/ysrivatsan/EPOS/tree/Srivatsan (last accessed:\nSeptember 2018).\n4 Available at http://epos-net.org/software/exemplar/ (last accessed: September\n2018).\n10\nEvangelos Pournaras et al.\navailable for further research and evaluations [38,35]. The actual experiments\nare deployed and parallelized in the Euler5 cluster computing infrastructure\nof ETH Zurich.\n4.2 Varying dimensions and performed experiments\nThe following system dimensions are studied: (i) application scenarios, (ii)\nholarchic schemes, (iii) scale of holarchy, (iv) number of children c in the tree\nnetwork, (v) diﬀerent agent preferences λ.\nSynthetic and empirical plans are generated for 1000 agents using data\nfrom real-world pilot projects. These four application scenarios are referred to\nas follows: (i) synthetic, (ii) bike sharing, (iii) energy demand and (iv) electric\nvehicles.\nThe synthetic dataset consists of 16 possible plans of size 100 generated\nfrom a standard normal distribution with a mean of 0 and a standard deviation\nof 1. A random local cost is assigned to the plans, i.e. the index of the plan\nrepresents its cost.\nThe bike sharing dataset6 of the Hubway bike sharing system7 in Paris is\nused to generate a varying number of plans of size 98 for each agent based on\nthe unique historic trips performed by each user. Therefore, the plans represent\nthe trip proﬁles of the users and they contain the number of incoming/outgoing\nbike changes made by each user in every station [37]. The local cost of each\nplan is deﬁned by the likelihood of a user to not perform a trip instructed in\nthe plan [38]. For instance, if three plans are chosen 4, 5 and 1 days during\nthe measured time period respectively, the local cost for these plans is 0.6, 0.5\nand 0.9 respectively.\nThe energy demand dataset8 is generated via disaggregation of aggregate\nload obtained from the Paciﬁc Northwest Smart Grid Demonstration Project\n(PNW) by Battelle9. The disaggregation algorithm and the raw data are il-\nlustrated in earlier work [40]. The generated dataset contains 5600 agents\nrepresenting residential consumers. Every agent has 10 possible plans, each\nwith length 144 containing electricity consumption records for every 5 min-\nutes. The ﬁrst plan corresponds to the raw data. The next three possible plans\n5 Available at https://scicomp.ethz.ch/wiki/Euler (last accessed: September 2018).\n6 Available\nat\nhttp://epos-net.org/shared/datasets/EPOS-BICYCLES.zip\n(last\nac-\ncessed: September 2018).\n7 The plans are generated using the dataset made available in the context of the Hubway\nData Visualization Challenge: http://hubwaydatachallenge.org/ (last accessed: September\n2018). Although this dataset does not contain personalized records, user trips are extracted\nfrom user information: zip-code, year of birth and gender. All trips that have common values\nin these ﬁelds are assumed to be made by the same user. The timeslot is chosen from 8:00\nam to 10:00 am. All historic unique trips a user did in the deﬁned timeslot of a week day\nare considered as the possible plans for that day.\n8 Available at http://epos-net.org/shared/datasets/EPOS-ENERGY-SUBSET.zip (last\naccessed: September 2018).\n9 Available upon request at http://www.pnwsmartgrid.org/participants.asp (last ac-\ncessed: September 2018)\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n11\nare obtained via the SHUFFLE generation scheme [40] that randomly permutes\nthe values of the ﬁrst plan. The next three plans are generated with the SWAP-\n15 generation scheme [40] that randomly picks a pair of values from the ﬁrst\nplan and swaps their values. The process repeats 15 times. Respectively, the\nlast three plans are generated by SWAP-30 [40] that applies the same process 30\ntimes. The local cost of each plan represents the level of perturbation intro-\nduced on the ﬁrst plan, i.e. on the disaggregated data, by the plan generation\nscheme. It is measured by the standard deviation of the diﬀerence between the\nraw and the perturbed plan values, element-wise: σ(x1 −y1, ..., x144 −y144).\nThe plans10 for the electric vehicles are generated using data11 from the\nHousehold Travel Survey of the California Department of Transportation dur-\ning 2010–2012. The plans concern the energy consumption of the electric vehi-\ncles by charging from the power grid. Four plans per agent with size 1440 are\ngenerated by extracting the vehicle utilization using the historical data and\nthen computing the state of charge by redistributing the charging times over\ndiﬀerent time slots. The methodology is outlined in detail in earlier work [37].\nThe local cost of each plan is measured by the likelihood of the vehicle utiliza-\ntion during the selected charging times.\nThe learning schemes studied are the baseline that is default I-EPOS and\nthe three holarchic schemes: Holarchic initialization is used as an exploration\nstrategy to evaluate its likelihood to improve the learning capacity. Holarchic\nruntime is used as a mitigation strategy to evaluate the maintenance of learn-\ning capacity in distributed environments under uncertainties. Finally, holarchic\ntermination is used as a boosting strategy to evaluate the likelihood of improv-\ning the learning capacity. In each holon of the holarchic schemes within one\nmain learning iteration, τ = 5 holarchic iterations are executed.\nTwo holarchic scales are evaluated: (i) full and (ii) partial. The full scale\nuses all levels of the baseline tree network to apply a holarchic scheme as also\nshown in Figure 1. In contrast, partial scale is applied in one branch under the\nroot.\nThe inﬂuence of the topological tree structure and agent preferences on the\nlearning capacity is studied by varying respectively the number of children as\nc = 2, .., 5 and the λ parameter as λ = 0, 0.25, 0.5, 0.75. Table 1 summarizes\nthe dimensions and their variations in the performed experiments.\nThe total experiments performed are calculated as follows: For the dimen-\nsion of application scenarios, 4 variations are counted and 3 variations for the\nlearning schemes given that the baseline and the holarchic termination can be\ngenerated within one experiment. The partial scale uses all possible branch\ncombinations: 2 + 3 + 4 + 5 = 14 variations plus 4 variations for the full scale\nresult in 18 total variations for the two dimensions of holarchic scale and num-\nber of children. Finally, 4 variations are counted for the dimension of agent\npreferences. The total number of 4 ∗3 ∗18 ∗4 = 864 variation combinations is\n10 Available\nat\nhttp://epos-net.org/shared/datasets/EPOS-ELECTRIC-VEHICLES.zip\n(last accessed: September 2018).\n11 Available at www.nrel.gov/tsdc (last accessed: September 2018). Electric vehicles\nequipped with GPS are selected.\n12\nEvangelos Pournaras et al.\nTable 1 Dimensions and their variations in the total of 864000 experiments.\nDimension\nVariation 1\nVariation 2\nVariation 3\nVariation 4\nApplication scenario\nSynthetic\nBike sharing\nEnergy\ndemand\nElectric vehi-\ncles\nLearning scheme\nBaseline\nHolarchic\ninitialization\nHolarchic\nruntime\nHolarchic\ntermination\nHolarchic scale\nFull\nPartial\n-\n-\nNumber of children (c)\nc = 2\nc = 3\nc = 4\nc = 5\nAgent preferences (λ)\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\nTotal experiments:\n864000\nthe total number of experiments performed. Each experimental combination is\nrepeated 1000 times by (i) 1000 samples of possible plans in the synthetic sce-\nnario and (ii) 1000 random assignments of the agents in the tree network in the\nother three application scenarios. Therefore, the total number of experiments\nperformed is 864 ∗1000 = 864000.\n4.3 Evaluation metrics\nPerformance is evaluated with the following metrics: (i) standardized global\ncost, (ii) improvement index and (iii) communication cost.\nThe standardized global cost is the variance of the global plan at conver-\ngence time. The minimization of the variance is the optimization objective and\ntherefore the variance is used as the criterion of the learning capacity. Stan-\ndardization12 is applied on the variance so that the learning capacity among\ndiﬀerent datasets can be compared.\nThe improvement index I measures the reduction or increase of the global\ncost at convergence time for the holarchic schemes compared to the baseline.\nPositive values indicate an improvement of the baseline, while negative values\nshow a deterioration. The improvement index is measured as follows:\nI = Cb −Ch\nCb + Ch\n(1)\nwhere Cb is the global cost for the baseline and Ch is the global cost for a hol-\narchic scheme, both at convergence time. The improvement index is calculated\nbased on the principle of the symmetric mean absolute error, which compared\nto the mean absolute error can handle single zero values, it is bound to [−1, 1]\nand eliminates very large values originated by low denominators [21].\n12 Standardization transforms the global cost values to have zero mean and units of vari-\nance as follows: x−µ\nσ\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n13\nThe communication cost measures the number of messages exchanged to\ncomplete a learning iteration and can be distinguished to total and synchro-\nnized. The total communication cost counts all exchanged messages between\nthe agents during a learning iteration and it is calculated for the baseline Mb\nas follows:\nMb = 2(c0 + ... + cl −1)\n(2)\nwhere c is the number of children in a balanced tree. Equation 2 sums up the\nnumber of agents in each level of the tree and substracts 1 to count the number\nof links. Multiplication by 2 counts both bottom-up and top-down phases. The\ntotal communication cost of a holarchic scheme can be measured as follows:\nMt = 2τ\nl\nX\nj=0\ncl−j(c0 + ... + cj −1)\n(3)\nwhere c is here again the number of children in a balanced tree/holarchy and τ\nis the number of holarchic iterations. The summation starts from leaves (level\nj = 0), and progresses to the root of the holarchy (at level j = l). Equation 3\nmultiplies the number of agents cl−j at each level l−j with 2τ times (τ bottom-\nup and τ top-down holarchic iterations) the number of agents in the branches\nunderneath: c0 + ... + cj −1. For example, j = 0 corresponds to Figure 1a\nwith a communication cost of 2τ22(20 + 21 −1) = 16τ. j = 1 corresponds\nto Figure 1b with a communication cost of 2τ21(20 + 21 + 22 −1) = 24τ\nand respectively, for j = 2 and Figure 1c communication cost is calculated\nas 2τ20(20 + 21 + 22 + 23 −1) = 28τ. These nested calculations for the full\nholarchy sum up to Mt = (16 + 24 + 28)τ = 68τ messages.\nThe synchronized communication cost counts the number of messages ex-\nchanged within holons, while counting this number only once for holons at\nthe same level which can exchange messages in parallel. For instance, Fig-\nure 1a illustrates four parallel holons with a total communication cost of\n2τ22(20 + 21 −1) = 16τ messages. Instead, the synchronized communication\ncost counts for 2τ(20+21−1) = 4τ messages. The synchronized communication\ncost of a full holarchy can be measured as follows:\nMs = 2τ\nl\nX\nj=0\n(c0 + ... + cj −1)\n(4)\nwhere c is the number of children and τ the number of holarchic iterations.\nNote that the synchronized communication cost considers holons performing in\nparallel and not individual agents, that is why within each holon all messages\nexchanged are counted. For the same reason, the synchronized communication\ncost for baseline is not deﬁned for a fairer comparison with holarchic schemes.\n14\nEvangelos Pournaras et al.\n4.4 Computational challenge\nTo better understand the computational challenge and context in which the\nperformance of the holarchic schemes is studied, the performance character-\nistics of the baseline are illustrated in this section. Figure 3 illustrates the\nlearning curves of the baseline in the four application scenarios.\n0\n1\n2\n3\n4\n5\n0\n5\n10\n15\n20\n25\n30\nIteration\nStandardized Global Cost\nBike Sharing\nEnergy Demand\nElectric Vehicles\nSynthetic\nFig. 3 Learning curves of I-EPOS for the four benchmark application scenarios with 1000\nagents generating 4 plans and choosing a plan with λ = 0.\nThe learning performance of I-EPOS shows the following behavior: Global\ncost decreases dramatically in very few iterations in all application scenarios,\nwhile the decrease is monotonous. Therefore, I-EPOS has a superior eﬃciency\nto learn fast combinations of plans that minimize the variance by executing\n10-15 iterations. Convergence does not necessarily mean that the globally op-\ntimum solution is found. The evaluation of the global optimality in a system\nwith 1000 agents and more than 4 plans per agents is computationally infeasi-\nble given the exponential complexity of the combinatorial space: 41000. For this\nreason, the global optimality is evaluated using brute force search in a small-\nscale system of 10 agents with 4 plans per agent and λ = 0. Therefore the total\nnumber of solutions is 410. Each experiment is repeated 10 times by shuﬄing\nthe agents over a binary tree. Figure 4 illustrates the global optimality results\nfor each application scenario.\nI−EPOS Convergence\n5\n10\n15\n20\n0\n25\n50\n75\n100\nSolution Index [ 104]\nGlobal Cost\n(a) Synthetic\nI−EPOS Convergence\n0.4\n0.6\n0.8\n1.0\n0\n25\n50\n75\n100\nSolution Index [ 104]\nGlobal Cost\n(b) Bike sharing\nI−EPOS Convergence\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n25\n50\n75\n100\nSolution Index [ 104]\nGlobal Cost\n(c)\nEnergy\nde-\nmand\nI−EPOS Convergence\n14\n16\n18\n20\n22\n0\n25\n50\n75\n100\nSolution Index [ 104]\nGlobal Cost\n(d) Electric\nvehi-\ncles\nFig. 4 Global optimality of I-EPOS for the four benchmark applicaiton scenarios with 10\nagents generating 4 plans and choosing a plan with λ = 0.\nI-EPOS ﬁnds the 0.007%, 0%, 0.017% and 0.153% top solution in each of\nthe application scenarios of Figure 4. Note that such a signiﬁcant optimality\nmay not be achieved for systems with thousands of agents and several plans.\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n15\nNevertheless, designing a new learning scheme to overpass this performance\nlevel, without introducing additional complexity and resources is a challenge\nand potentially not an endeavor worth pursuing. Instead this paper studies\nholarchic structures as learning strategies to explore, mitigate and boost the\ncost-eﬀectiveness of I-EPOS in distributed environments and in this sense the\nnotion of holarchy is the means for decentralized learning to tolerate their\nuncertainties.\n5 Experimental Evaluation\nThis section illustrates the learning capacity of the three holarchic schemes\nfollowed by the trade-oﬀs and cost-eﬀectiveness of the holarchic runtime. All\nmain experimental ﬁndings are covered in this section and an appendix pro-\nvides supplementary results that cover the broader range of the varying pa-\nrameters. The results in the appendix are included for the sake of completeness\nof the paper and future reference.\n5.1 Learning capacity\nFigure 5 illustrates the learning curves for the four application scenarios and\nlearning schemes. The partial scale with λ = 0 and c = 2 is illustrated. The\nlearning curves for the full scale, λ = 0.5 and c = 5 are illustrated in Figure 12\nof Appendix A.\nBaseline Convergence\nBefore Convergence After Convergence\n2\n4\n6\n8 10 12\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(a) Synthetic\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(b) Bike sharing\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20\n30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(c)\nEnergy\nde-\nmand\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10 15 20 25 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(d) Electric\nvehi-\ncles\nFig. 5 Learning curves. Dimensions: learning schemes, application scenarios. Settings: par-\ntial scale, λ = 0, c = 2.\nThe following observations can be made in Figure 5: Holarchic runtime\nachieves the fastest convergence speed given the several multi-level holarchic\niterations performed within a main learning iteration. However, a performance\nsacriﬁce in global cost is observed, which is though low and observable for\nthe scenario of electric vehicles in which the global cost is 10% higher than\nthe baseline. Moreover, within 7-8 iterations all holarchic schemes achieve the\n16\nEvangelos Pournaras et al.\nglobal cost reduction of the baseline. The convergence speed of the holarchic\ninitialization is 1-2 iterations slower than the baseline, though this insigniﬁcant\ndiﬀerence is not anymore observable in the scenario of electric vehicles.\nFigure 6 illustrates the improvement index of the holarchic schemes for\ndiﬀerent application scenarios and diﬀerent λ values. The exploration poten-\ntial of the holarchic initialization is indicated by the error bars above the\nmean. Moreover, holarchic initialization does not inﬂuence signiﬁcantly the\nglobal cost reduction, however, for higher λ values, i.e. λ = 0.75, an average\nincrease of 0.5% is observed in the improvement index. This means that in\nmore constrained optimization settings, the exploration strategy of the hol-\narchic initialization contributes a low performance improvement. In contrast,\nthe mitigation strategy of the holarchic runtime manages to preserve the base-\nline performance with an improvement index of values close to 0. An average\nperformance boosting of 1.65% is observed via the holarchic termination in\nthe bike sharing scenario that has sparse data, while in the other scenarios no\nsigniﬁcant improvement is observed.\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(a) Synthetic\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(b) Bike sharing\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(c)\nEnergy\nde-\nmand\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(d) Electric\nvehi-\ncles\nFig. 6 Improvement index. Dimensions: holarchic schemes, application scenarios, diﬀerent\nλ values. Settings: partial scale, c = 2.\nFigure 7 illustrates the improvement index of the holarchic schemes for\ndiﬀerent application scenarios and diﬀerent c values. Holarchic initialization\nretains an average improvement index of -0.008 while it can scale up the im-\nprovement index to values of 0.225 on average. The number of children does not\ninﬂuence the performance of this holarchic scheme. In contrast, the holarchic\nruntime shows an average increase of 4.1% in the improvement index by in-\ncreasing c from 2 to 5, while this holarchic scheme serves well its performance\nmitigation role: an average improvement index of -0.041. Finally, holarchic\ntermination boosts performance by 1.1% in the bike sharing scenario.\nFigure 8 demonstrates the higher performance that the partial scale shows\ncompared to full scale: 0.25% higher improvement index for holarchic initial-\nization and 6.5% for holarchic runtime.\nFigure 13, 14 and 15 of Appendix A show the probability density of the\nimprovement index for diﬀerent λ, c values and holarchic scales respectively.\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n17\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(a) Synthetic\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(b) Bike sharing\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(c)\nEnergy\nde-\nmand\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(d) Electric\nvehi-\ncles\nFig. 7 Improvement index. Dimensions: holarchic schemes, application scenarios, varying\nnumber of children. Settings: partial scale, λ = 0.\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(a) Synthetic\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(b) Bike sharing\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(c)\nEnergy\nde-\nmand\n−1.0\n−0.5\n0.0\n0.5\n1.0\nInitialization Runtime Termination\nHolarchic Scheme\nImprovement Index\n(d) Electric\nvehi-\ncles\nFig. 8 Improvement index. Dimensions: holarchic schemes, application scenarios, partial\nversus full scale. Settings: λ = 0, c = 2.\nIn these ﬁgures one can study in more detail the density of the improvement\nindex values behind the error bars of the respective Figure 6, 7 and 8.\nThe rest of this section studies the trade-oﬀs and the cost-eﬀectiveness of\nthe holarchic runtime designed for the mitigation of the learning performance.\n5.2 Trade-oﬀs and cost-eﬀectiveness\nFigure 9a illustrates the communication cost per iteration of the baseline ver-\nsus the total and synchronized communication cost of the holarchic runtime.\nThis is a worse case scenario as applying the holarchy to a smaller branch\nor for a fewer than 5 holarchic iterations can make the communication cost\nequivalent13 to the one of the baseline. In Figure 9a, the communication cost\nof the holarchic runtime decreases as the number of children increases given\nthat the recursion of the holarchy is limited to a lower number of levels in the\ntree, i.e. fewer holons are formed. The synchronized communication cost is on\naverage 45% lower than the total communication cost.\n13 The communication cost of the holarchic runtime can even become lower than baseline\nassuming a holarchy at partial scale with the agents that do not belong to the holarchy\nbeing disconnected.\n18\nEvangelos Pournaras et al.\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\n2000\n4000\n6000\n8000\n1\n2\n3\n4\nChildren\nCommunication Cost\nG\nG\nG\nBaseline\nSynchronous Communication\nTotal Communication\n(a)\nTotal\nversus\nsynchronized\ncom-\nmunication\nfor\ndiﬀerent c.\n0\n5\n10\n15\n20\n25\n30\n0\n5\n10\n15\n20\n25\n30\nIteration\nCommunication Cost\nc = 2\nc = 5\nBaseline\nCase 50000\nCase 30000\n(b) Total commu-\nnication\n0\n5\n10\n15\n20\n25\n30\n0\n5\n10\n15\n20\n25\n30\nIteration\nCommunication Cost\nc = 2\nc = 5\nBaseline\nCase 50000\nCase 30000\n(c)\nSynchronized\ncommunication\nFig. 9 Communication cost. Settings: partial scale, λ = 0 (a) Dimensions: varying number\nof children, total versus synchronized communication cost, baseline versus holarchic runtime.\n(b) and (c) Sampling case 30000 and case 50000 under total versus synchronized communi-\ncation cost. Dimensions: c = 2 versus c = 5, number of iterations, baseline versus holarchic\nruntime.\nThe cost-eﬀectiveness of the holarchic runtime is studied by ﬁxing the com-\nmunication cost for both baseline and holarching runtime and looking into the\nglobal cost reduction achieved for the same number of messages exchanged.\nFigure 9b and 9c illustrate this process for total and synchronous communica-\ntion cost. Two cases are determined: (i) case 30000 and (ii) case 50000. Each\ncase runs for a given number of iterations that is determined by the intersec-\ntion with the horizontal dashed lines for each of the c = 2 and c = 5. Then,\nthe global costs can be compared for the same number of exchanged messages\nas shown in Figure 10.\nThe following observations can be made in Figure 10a to 10d for the to-\ntal communication cost: When c = 5, the holarchic runtime achieves a highly\nequivalent performance with the baseline. This also holds for c = 2 in the sce-\nnarios of synthetic and electric vehicles. The performance mitigation becomes\n13.64% more signiﬁcant when the synchronized communication cost is counted\nin Figure 10e to 10h. This is also shown by the shifted probability densities of\nthe improvement index in Figure 16 of Appendix A.\nThese ﬁndings can be generalized further for the broader range of com-\nmunication cost as shown in Figure 11 for c = 2. The key observation that\nconﬁrms the mitigation capability of the holarchic runtime is the comparable\nglobal cost achieved using the communication cost required for baseline to\nconverge. This mitigation potential is also demonstrated by the probability\ndensity of the relative global cost between baseline and holarchic runtime in\nFigure 17 of Appendix A.\nFigure 11 can also be compared with Figure 18 of Appendix A that shows\nthe cost-eﬀectiveness under c = 5. The performance mitigation is even higher\nwith this setting.\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n19\n7\n3\n15\n15\n11\n5\n25\n25\n0.000\n0.025\n0.050\n0.075\n0.100\nCase 30000\nCase 50000\nStandardized Global Cost\n(a) Synthetic, total\ncommunication\n7\n3\n15\n15\n11\n5\n25\n25\n0.0\n0.1\n0.2\n0.3\n0.4\nCase 30000\nCase 50000\nStandardized Global Cost\n(b) Bike\nsharing,\ntotal\ncommunica-\ntion\n7\n3\n15\n15\n11\n5\n25\n25\n0.000\n0.025\n0.050\n0.075\n0.100\nCase 30000\nCase 50000\nStandardized Global Cost\n(c)\nEnergy\nde-\nmand,\ntotal\ncom-\nmunication\n7\n3\n15\n15\n11\n5\n25\n25\n0.0\n0.5\n1.0\n1.5\nCase 30000\nCase 50000\nStandardized Global Cost\n(d) Electric\nvehi-\ncles total communi-\ncation\n10\n7\n15\n15\n16\n12\n25\n25\n0.000\n0.025\n0.050\n0.075\n0.100\nCase 30000\nCase 50000\nStandardized Global Cost\n(e) Synthetic, syn-\nchronized\ncommu-\nnication\n10\n7\n15\n15\n16\n12\n25\n25\n0.0\n0.1\n0.2\n0.3\n0.4\nCase 30000\nCase 50000\nStandardized Global Cost\n(f) Bike\nsharing,\nsynchronized\ncom-\nmunication\n10\n7\n15\n15\n16\n12\n25\n25\n0.000\n0.025\n0.050\n0.075\n0.100\nCase 30000\nCase 50000\nStandardized Global Cost\n(g)\nEnergy\nde-\nmand,\nsynchro-\nnized\ncommunica-\ntion\n10\n7\n15\n15\n16\n12\n25\n25\n0.0\n0.5\n1.0\n1.5\nCase 30000\nCase 50000\nStandardized Global Cost\n(h) Electric\nvehi-\ncles,\nsynchronized\ncommunication\nFig. 10 Global cost. Dimensions: baseline versus holarchic runtime, total versus synchro-\nnized communication cost, application scenarios, c = 2 versus c = 5, number of iterations\ngiven a communication cost, case 30000 versus case 50000. Settings: partial scale, λ = 0.\n6 Summary of Results and Discussion\nThe following key observations can be made about the learning capacity of\nholarchic structures: (i) The limitation to contribute improvements in terms\nof global cost as motivated in Figure 3 and 4 is conﬁrmed in the performed\nexperiments. (ii) The performance exploration, mitigation and boosting for\nwhich the holarchic schemes are designed are conﬁrmed: A low and sporadic\nperformance improvement is observed by holarchic initialization, especially in\nconstrained environments of high λ values that justiﬁes the exploration poten-\ntial of this scheme. Similarly, the mitigation potential of the holarchic runtime\nis also conﬁrmed by the fast convergence and preservation of the performance,\nespecially for trees with a higher number of children c. Performance boost-\ning by holarchic termination is scarce but observable in the bike sharing sce-\nnario that has sparser data. (ii) Strikingly, holarchies applied at a partial scale\ndemonstrate higher cost-eﬀectiveness than full scale in all holarchic schemes,\ni.e. lower cost due to the higher localization that limits communication cost\nand higher eﬀectiveness in terms of higher improvement index.\nIn the trade-oﬀs of cost-eﬀectiveness, the synchronized communication cost\nis almost half of the total communication cost via a parallel execution of the\nlearning process using holarchic structures. Moreover, for the same communi-\ncation cost in baseline and holarchic runtime, the global costs become equiva-\n20\nEvangelos Pournaras et al.\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(a) Synthetic, total\ncommunication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(b) Bike\nsharing,\ntotal\ncommunica-\ntion\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(c)\nEnergy\nde-\nmand,\ntotal\ncom-\nmunication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(d) Electric\nvehi-\ncles, total commu-\nnication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(e) Synthetic, syn-\nchronized\ncommu-\nnication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(f) Bike\nsharing,\nsynchronized\ncom-\nmunication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(g)\nEnergy\nde-\nmand,\nsynchro-\nnized\ncommunica-\ntion\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(h) Electric\nvehi-\ncles,\nsynchronized\ncommunication\nFig. 11 Cost-eﬀectiveness. Dimensions: baseline versus holarchic runtime, application sce-\nnarios, total versus synchronized communication cost. Settings: partial scale, λ = 0, c = 2.\nlent, especially in terms of the synchronized communication cost and for trees\nwith a higher number of children c.\nOverall the results suggest that several parallel and small-scale holarchic\nstructures decrease the probability of trapping to local optima, require a lower\nparallelizable communication cost, while they better serve the purpose they are\ndesigned for: make decentralized deep learning more resilient to the uncertain-\nties of distributed environments. Because of the high eﬃciency of I-EPOS to\nwhich holarchic structures are applied, conclusions cannot be reached whether\nthe learning capacity of other mechanisms can be enhanced using holarchic\nstructures. Nevertheless, another promising use case of holarchies is the multi-\nlevel optimization of complex techno-socio-economic systems formed by agents\nwith diﬀerent goals. In other words, the localization of the learning process in\npart of the deep hierarchical structure can represent the collective eﬀort of a\ncommunity to meet its goal that can diﬀer or even oppose the goal of another\ncommunity, i.e. another part of the deep hierarchical structure.\n7 Positioning and Comparison with Related Work\nEarlier work [48,49] identiﬁes the key role that hierarchical design plays in\ndealing with the complexity [9] of large-scale and highly dynamic systems.\nMore speciﬁcally, it is studied how such hierarchies can address the challenge of\nlimited rationality, which stems from the combinatorial explosion of alternative\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n21\nsystem compositions. In the context of this paper, the combinatorial explosion\nresults from the multitude of resource planning alternatives. Such hierarchies\nare later on introduced as ‘holarchies’ by emphasizing their recursive and self-\nencapsulated nature [22] that represent both an entire system (whole) and a\nmere subsystem (part).\nGeneral-purpose holonic designs that integrate the above principles are\nproposed via holonic multi-agent system platforms [45,44]; multi-level mod-\neling and simulation approaches [14]; and hierarchical problem solving [24].\nCustomised holonic designs are been applied to various domains, including\nhierarchical planning [33], traﬃc control [10], manufacturing [7,15] and smart\ngrids [25,47,11]. The success of these applications motivate the adoption of\nsimilar holonic principles for designing deep learning systems for large-scale\ndistributed environments.\nThe design of I-EPOS [38,35], used in this paper as case study and per-\nformance baseline, adopts a hierarchical approach, featuring: (i) bottom-up\nabstraction – via the aggregation of plans from the lower level; (ii) partial iso-\nlation – as diﬀerent tree branches operate in parallel; and (iii) time tuning – by\nsynchronizing the execution of hierarchical levels. However, the learning pro-\ncess in I-EPOS executes over the entire hierarchy – sequentially, level-by-level\n– rather than being nested within multiple holarchic levels.\nVarious deep learning techniques employ a hierarchical structure for dif-\nferent purposes. This depends on the nature of the learning problem, e.g.\nclassiﬁcation or prediction, and on the context within which the learning pro-\ncess executes, i.e. amount of input data and availability of computational re-\nsources. Hierarchical models are employed to deal with complex processing of\ndata, by performing learning tasks progressively within incremental modelling\nlevels, e.g. image classiﬁcation [46,20] and text categorisation [23]. Hierarchi-\ncal structures are employed to process online large amounts of distributed\ninput data, hence scaling up machine learning techniques [34,28]. Such learn-\ning approaches rely on data partitions – pre-existing or artiﬁcially created –\nover which they distribute the learning process. The partial results from each\npartition are then collected and aggregated into an overall learning outcome.\nThe work of this paper is positioned and compared below with some of these\napproaches.\nAn earlier survey [34] compares distributed learning approaches with re-\nspect to their capability to (i) combine learning outcomes among heteroge-\nneous representations of data partitions and (ii) deal with privacy constraints.\nThe studied approaches feature a two-layer hierarchy: one layer for distributed\nlearning across data partitions and a second layer for collecting and aggregat-\ning the results. In contrast, the holarchic learning design proposed in this\npaper introduces multiple levels to further to enhance the learning capacity in\ndistributed environments under uncertainties. The localized holons operating\nin parallel reuse their learning outcomes even when the topology is clustered\nby node or link failures.\nMLNet [28] introduces a special-purpose communication layer for distributed\nmachine learning. It uses tree-based overlay networks to aggregate progres-\n22\nEvangelos Pournaras et al.\nsively partial learning results in order to reduce network traﬃc. This approach\ndraws parallels with the design proposed here, while it is more applicable at\nthe lower communication layers.\nIn dynamic environments, rescheduling, or reoptimization [29] becomes a\ncritical function for dealing with unpredictable disturbances. It usually raises\nthe additional constraints of minimizing reoptimization time as well as the dis-\ntance between the initial optimization solution and the one of reoptimization,\ne.g. dynamic rescheduling in manufacturing systems [26] or shift reschedul-\ning [29]. For instance, a two-level holarchy is earlier adopted to combine global\noptimization scheduling with fast rescheduling when dynamic system distur-\nbances occur [29]. In contrast, the holarchic schemes of this paper do not\nundermine the learning performance, while adding resilience by localizing the\nlearning process without reinitiating it.\nAnother line of relevant related work concerns the initial selection and\nmaintenance of the topology over which the distributed learning process oper-\nates. Multi-agent approaches often rely on self-organization by changing their\ninteractions and system structure at runtime. A holonic multi-agent approach\nfor optimizing facility location problems is earlier introduced [32]. Facilities in-\nclude distribution of bus stops, hospitals or schools within a geographical area.\nThe agents react to mutual attraction and repulsion forces to self-organize into\na holarchy. Stable solutions represent optimal facility localization distributions.\nThis self-organization process has a considerable computational and commu-\nnication cost, in case of remote agents’ interactions. In contrast, the holarchic\nschemes studied in this paper preserve the agents’ organizational structure,\nwhile the decentralized learning process used for system optimization is self-\nadapted by localizing the learning span within part of the tree network.\nThe experimental work of this paper shows how diﬀerent topological con-\nﬁgurations, i.e. agents’ positioning and number of children, inﬂuence learning\nperformance. The key role that such hyperparameters play in ensuring the\neﬀectiveness of learning approaches is also conﬁrmed by related work on the\noptimization of the hierarchical structure and its conﬁguration variables, for\ninstance grid search, i.e. exhaustive search of all possibilities, (which, however,\nsuﬀers from exponential combinatorics), random search, Bayesian optimiza-\ntion, e.g. in neural networks and deep belief systems [5,6] as well as gradient-\nbased optimization [27]. This is also relevant for deep learning applications\nvia unsupervised pre-training [53] and evolutionary algorithms, e.g. in deep\nlearning neural networks [30,54]. In the context of this work, such approaches\ncan be used to determine the most eﬀective holarchic structures and hyperpa-\nrameter conﬁgurations.\n8 Conclusion and Future Work\nThis paper concludes that holarchic structures for decentralized deep learn-\ning can be a highly cost-eﬀective organizational artifact for managing learning\nperformance under uncertainties of distributed environments. The communi-\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n23\ncation cost of self-organization can be eliminated by self-adapting the span of\nthe learning process at a more localized level within the hierarchical structure\nas the means to cope with failures, latency and constrained computational\nresources. An extensive experimental evaluation with more than 864000 ex-\nperiments fed with synthetic and real-world data from pilot projects conﬁrm\nthe potential to explore, mitigate and boost the learning performance using\nthree respective holarchic schemes applied to the I-EPOS decentralized deep\nlearning system for solving combinatorial optimization problems.\nResults show that the exploration of improving solutions is feasible and\nmore likely to happen under stricter agents’ constraints, while performance\nmitigation is more eﬀective in balanced tree topologies with higher number of\nchildren. Boosting the learning performance via holarchic structures is chal-\nlenging yet consistently observed under computational problems with sparse\ndata, i.e. the bike sharing application scenario. The partial scale of the hol-\narchic structures is more cost-eﬀective than the full scale. Nevertheless, when\nthe uncertainties of distributed environments are not anymore a constraint, ho-\nlarchic schemes cannot outperform the cost-eﬀectiveness of learning systems\nthat make use of the whole hierarchical structure, which is a ﬁnding consistent\nwith earlier work on greedy optimization and suboptimum heuristics trapped\nin local optima [4].\nMechanisms for an automated activation and deactivation of holarchic\nschemes as well as the applicability of these schemes in other more complex hi-\nerarchical structures than tree topologies are subject of future work. Applying\nthe concept of holarchy in non-hierarchical structures such as the unstruc-\ntured learning network of COHDA [19] can provide new means to control high\ncommunication costs, while preserving a high learning performance.\nAcknowledgements This work is supported by the European Communitys H2020 Pro-\ngram under the scheme ‘INFRAIA-1-2014-2015: Research Infrastructures, grant agreement\n#654024 ‘SoBigData: Social Mining & Big Data Ecosystem (http://www.sobigdata.eu) and\nthe European Communitys H2020 Program under the scheme ‘ICT-10-2015 RIA’, grant\nagreement #688364 ‘ASSET: Instant Gratiﬁcation for Collective Awareness and Sustain-\nable Consumerism’.\nReferences\n1. Ada Diaconescu, S.T., M´uller-Schloer, C.: Holonic cellular automata: Modelling multi-\nlevel self-organisation of structure and behaviour. In: Proceedings of the 2018 Conference\non Artiﬁcial Life (2018)\n2. Aghaei, J., Alizadeh, M.I.: Demand response in smart electricity grids equipped with\nrenewable energy sources: A review. Renewable and Sustainable Energy Reviews 18,\n64–72 (2013)\n3. Aziz, A.A., Sekercioglu, Y.A., Fitzpatrick, P., Ivanovich, M.: A survey on distributed\ntopology control techniques for extending the lifetime of battery powered wireless sensor\nnetworks. IEEE communications surveys & tutorials 15(1), 121–144 (2013)\n4. Bang-Jensen, J., Gutin, G., Yeo, A.: When the greedy algorithm fails. Discrete Opti-\nmization 1(2), 121–127 (2004)\n5. Bergstra, J., Bengio, Y.: Algorithms for hyper-parameter optimization. In: In NIPS,\npp. 2546–2554 (2011)\n24\nEvangelos Pournaras et al.\n6. Bergstra, J., Bengio, Y.: Random search for hyper-parameter optimization. J. Mach.\nLearn. Res. 13, 281–305 (2012)\n7. Christensen, J.H.: HMS/FB Architecture and its Implementation, pp. 53–87. Springer\nBerlin Heidelberg, Berlin, Heidelberg (2003)\n8. Diaconescu, A.: Goal-oriented holonic systems. In: C. M¨uller-Schloer, S. Tomforde (eds.)\nOrganic Computing – Technical Systems for Survival in the Real World, pp. 209–258.\nSpringer International Publishing (2017)\n9. Diaconescu, A.: Organising complexity: Hierarchies and holarchies.\nIn: C. M¨uller-\nSchloer, S. Tomforde (eds.) Organic Computing – Technical Systems for Survival in\nthe Real World, pp. 89–105. Springer International Publishing (2017)\n10. Fischer, K.: Holonic multiagent systems\ntheory and applications . In: P. Barahona,\nJ. Alferes (eds.) Progress in Artiﬁcial Intelligence, LNCS, vol. 1695, pp. 34–48. Springer\nBerlin Heidelberg (1999)\n11. Frey, S., Diaconescu, A., Menga, D., Demeure, I.: A generic holonic control architecture\nfor heterogeneous multiscale and multiobjective smart microgrids. ACM Trans. Auton.\nAdapt. Syst. 10(2), 9:1–9:21 (2015)\n12. Friedman, A., Knijnenburg, B.P., Vanhecke, K., Martens, L., Berkovsky, S.: Privacy\naspects of recommender systems. In: Recommender Systems Handbook, pp. 649–688.\nSpringer (2015)\n13. Galuba, W., Aberer, K., Despotovic, Z., Kellerer, W.: Protopeer: a p2p toolkit bridging\nthe gap between simulation and live deployement. In: Proceedings of the 2nd Inter-\nnational Conference on Simulation Tools and Techniques, p. 60. ICST (Institute for\nComputer Sciences, Social-Informatics and Telecommunications Engineering) (2009)\n14. Gaud, N.: A.: Towards a multilevel simulation approach based on holonic multiagent\nsystems.\nIn: In: UKSIM 08: Proceedings of the Tenth International Conference on\nComputer Modeling and Simulation, pp. 180–185 (2008)\n15. Giret, A., Botti, V.: Engineering holonic manufacturing systems. Comput. Ind. 60(6),\n428–440 (2009)\n16. Grande, R.C., Chowdhary, G., How, J.P.: Nonparametric adaptive control using gaus-\nsian processes with online hyperparameter estimation. In: Decision and Control (CDC),\n2013 IEEE 52nd Annual Conference on, pp. 861–867. IEEE (2013)\n17. Heidelberger, N., Karpinnen, K., D’Acunto, L.: Exposure diversity as a design principle\nfor recommender systems. Information, Communication & Society, 2, 21, 191-207 (2018)\n18. Helbing, D., Frey, B.S., Gigerenzer, G., Hafen, E., Hagner, M., Hofstetter, Y., van den\nHoven, J., Zicari, R.V., Zwitter, A.: Will democracy survive big data and artiﬁcial\nintelligence. Scientiﬁc American 25 (2017)\n19. Hinrichs, C., Lehnhoﬀ, S., Sonnenschein, M.: A decentralized heuristic for multiple-\nchoice combinatorial optimization problems. In: Operations Research Proceedings 2012,\npp. 297–302. Springer (2014)\n20. Huang, G.B., Lee, H., Learned-Miller, E.: Learning hierarchical representations for face\nveriﬁcation with convolutional deep belief networks.\nIn: 2012 IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 2518–2525 (2012)\n21. Hyndman, R.J., Koehler, A.B.: Another look at measures of forecast accuracy. Inter-\nnational journal of forecasting 22(4), 679–688 (2006)\n22. Koestler, A.: The Ghost in the Machine, 1 edn. GATEWAY EDITIONS, Henry Regnery\nCo. (1967)\n23. Kowsari, K., Brown, D., Heidarysafa, M., Jafari Meimandi, K., Gerber, M., Barnes,\nL.: Hdltex: Hierarchical deep learning for text classiﬁcation.\nIn: IEEE International\nConference on Machine Learning and Applications (2017)\n24. Landauer, C., Bellman, K.L.: New architectures for constructed complex systems. Appl.\nMath. Comput. 120(1-3), 149–163 (2001)\n25. L¨assig, J., Satzger, B., Kramer, O.: Hierarchically structured energy markets as novel\nsmart grid control approach. In: J. Bach, S. Edelkamp (eds.) KI 2011: Advances in\nArtiﬁcial Intelligence, pp. 179–190. Springer Berlin Heidelberg, Berlin, Heidelberg (2011)\n26. Leit˜ao, P., Restivo, F.: A holonic approach to dynamic manufacturing scheduling.\nRobotics and Computer-Integrated Manufacturing 24(5), 625–634 (2008)\n27. Maclaurin, D., Duvenaud, D., Adams, R.P.: Gradient-based hyperparameter optimiza-\ntion through reversible learning. In: Proceedings of the 32nd International Conference\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n25\non International Conference on Machine Learning - Volume 37, ICML’15, pp. 2113–2122.\nJMLR.org (2015)\n28. Mai, L., Hong, C., Costa, P.: Optimizing network performance in distributed machine\nlearning. In: Proceedings of the 7th USENIX Conference on Hot Topics in Cloud Com-\nputing, HotCloud’15. USENIX Association, Berkeley, CA, USA (2015)\n29. Meignan, D.: A heuristic approach to schedule reoptimization in the context of interac-\ntive optimization (2014)\n30. Miikkulainen, R., Liang, J.Z., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju,\nB., Shahrzad, H., Navruzyan, A., Duﬀy, N., Hodjat, B.: Evolving deep neural networks.\nCoRR abs/1703.00548 (2017)\n31. Mik, E.: The erosion of autonomy in online consumer transactions. Law, Innovation\nand Technology 8(1), 1–38 (2016)\n32. Moujahed, S., Gaud, N., Meignan, D.: A self-organizing and holonic model for optimiza-\ntion in multi-level location problems (2007)\n33. Nolle, L., Wong, K., Hopgood, A.: Darbs: A distributed blackboard system.\nIn:\nM. Bramer, F. Coenen, A. Preece (eds.) Research and Development in Intelligent Sys-\ntems XVIII, pp. 161–170. Springer London (2002)\n34. Peteiro-Barral Diegoand Guijarro-Berdi˜nas, B.: A survey of methods for distributed\nmachine learning. Progress in Artiﬁcial Intelligence 2(1), 1–11 (2013)\n35. Pilgerstorfer, P., Pournaras, E.: Self-adaptive learning in decentralized combinatorial\noptimization: a design paradigm for sharing economies.\nIn: Proceedings of the 12th\nInternational Symposium on Software Engineering for Adaptive and Self-Managing Sys-\ntems, pp. 54–64. IEEE Press (2017)\n36. Pournaras, E.: Multi-level reconﬁgurable self-organization in overlay services.\nPh.D.\nthesis, TU Delft, Delft University of Technology (2013)\n37. Pournaras, E., Jung, S., Yadhunathan, S., Zhang, H., Fang, X.: Socio-technical smart\ngrid optimization via decentralized charge control of electric vehicles. arXiv preprint\narXiv:1701.06811 (2017)\n38. Pournaras, E., Pilgerstorfer, P., Asikis, T.: Decentralized collective learning for self-\nmanaged sharing economies. ACM Transactions of Autonomous and Adaptive Systems\n(2018)\n39. Pournaras, E., Vasirani, M., Kooij, R.E., Aberer, K.: Measuring and controlling unfair-\nness in decentralized planning of energy demand. In: Energy Conference (ENERGY-\nCON), 2014 IEEE International, pp. 1255–1262. IEEE (2014)\n40. Pournaras, E., Yao, M., Helbing, D.: Self-regulating supply–demand systems. Future\nGeneration Computer Systems 76, 73–91 (2017)\n41. Puchinger, J., Raidl, G.R.: Combining metaheuristics and exact algorithms in combina-\ntorial optimization: A survey and classiﬁcation. In: International Work-Conference on\nthe Interplay Between Natural and Artiﬁcial Computation, pp. 41–53. Springer (2005)\n42. Reymann, C., Renzaglia, A., Lamraoui, F., Bronz, M., Lacroix, S.: Adaptive sampling\nof cumulus clouds with uavs. Autonomous robots 42(2), 491–512 (2018)\n43. Rockafellar, R.T., Uryasev, S.: Optimization of conditional value-at-risk. Journal of risk\n2, 21–42 (2000)\n44. Rodriguez, S., Gaud, N., Galland, S.: SARL: a general-purpose agent-oriented program-\nming language. In: the 2014 IEEE/WIC/ACM International Conference on Intelligent\nAgent Technology. IEEE Computer Society Press, Warsaw, Poland (2014)\n45. Rodriguez, S., Hilaire, V., Gaud, N., Galland, S., Koukam, A.: Holonic Multi-Agent\nSystems, ﬁrst edn., chap. 11, pp. 238–263. Self-Organising Software From Natural to\nArtiﬁcial Adaptation - Natural Computing. Springer (2011)\n46. Salakhutdinov, R., Tenenbaum, J.B., Torralba, A.: Learning with hierarchical-deep\nmodels. IEEE Trans. Pattern Anal. Mach. Intell. 35(8), 1958–1971 (2013)\n47. Schiendorfer, A., Stegh¨ofer, J.P., Reif, W.: Synthesis and abstraction of constraint mod-\nels for hierarchical resource allocation problems. Proc. of the 6th International Confer-\nence on Agents and Artiﬁcial Intelligence (ICAART) 2 (2014)\n48. Simon, H.A.: The architecture of complexity.\nAmerican Philosophical Society 106\n(1962)\n49. Simon, H.A.: The Sciences of the Artiﬁcial. MIT Press (1996)\n50. Singla, A., Santoni, M., Bart´ok, G., Mukerji, P., Meenen, M., Krause, A.: Incentivizing\nusers for balancing bike sharing systems. In: AAAI, pp. 723–729 (2015)\n26\nEvangelos Pournaras et al.\n51. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout:\nA simple way to prevent neural networks from overﬁtting.\nThe Journal of Machine\nLearning Research 15(1), 1929–1958 (2014)\n52. Sterbenz, J.P., Hutchison, D., C¸etinkaya, E.K., Jabbar, A., Rohrer, J.P., Sch¨oller, M.,\nSmith, P.: Resilience and survivability in communication networks: Strategies, princi-\nples, and survey of disciplines. Computer Networks 54(8), 1245–1265 (2010)\n53. Yao, C., Cai, D., Bu, J., Chen, G.: Pre-training the deep generative models with adaptive\nhyperparameter optimization. Neurocomputing 247, 144 – 155 (2017)\n54. Young, S.R., Rose, D.C., Johnston, T., Heller, W.T., Karnowski, T.P., Potok, T.E.,\nPatton, R.M., Perdue, G., Miller, J.: Evolving deep networks using hpc. In: Proceedings\nof the Machine Learning on HPC Environments, MLHPC’17, pp. 7:1–7:7. ACM, New\nYork, NY, USA (2017)\nA Detailed Experimental Results\nFigure 12 compares with Figure 5 by varying partial scale to full (Figure 12a-12d), λ = 0 to\nλ = 0.5 (Figure 12e-12h) and c = 0 to c = 5 (Figure 12i-12l).\nFigure 13 elaborates on the Figure 6. It illustrates the probability density function of\nthe improvement index by ﬁxing the holarchic scale to partial, c = 2 and varying all other\ndimensions.\nFigure 14 elaborates on the Figure 7. It illustrates the probability density function of\nthe improvement index by ﬁxing the holarchic scale to partial, λ = 0 and varying all other\ndimensions.\nFigure 15 elaborates on the Figure 8. It illustrates the probability density function of\nthe improvement index by ﬁxing λ = 0, c = 2 and varying all other dimensions.\nFigure 16 contrasts the improvement index of the holarchic runtime on the basis of total\nversus synchronized communication cost. Results show an average increase of the improve-\nment index by 9.5%, indicated by the shift to the right for synchronized communication.\nThe contrast of total versus synchronized communication cost is illustrated in Figure 17\nvia the relative performance that is deﬁned as follows:\nP = C(1)\nh\n−C(T )\nh\nC(1)\nb\n−C(T )\nb\n(5)\nwhere C(1)\nh\n, C(1)\nb\nis the global cost at the ﬁrst iteration t = 1 for holarchic runtime and\nbaseline respectively, while C(T )\nh\n, C(T )\nb\nis the global cost at convergence t = T. This metric\nencodes the additional information of the improvement extent during the learning iterations.\nFor instance, while Figure 16f shows a density with improvement index values around -0.7,\nthe relative performance values in Figure 17f spread very close to 100%, which means that\na performance peak is achieved.\nFigure 18 compares with Figure 11 by varying c = 2 to c = 5. In this case, the global\ncost of the baseline and holarchic runtime is equivalent for the required communication cost\nto converge. The global cost of the holarchic runtime requires fewer messages to drop for\nc = 5 compared to c = 2 as indicated by the respective line shifted to the left.\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n27\nBaseline Convergence\nBefore Convergence After Convergence\n2\n4\n6\n8 10 12\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(a) Synthetic, full\nscale, λ = 0, c = 2\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(b) Bike\nsharing,\nfull scale, λ = 0,\nc = 2\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20\n30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(c)\nEnergy\nde-\nmand,\nfull\nscale,\nλ = 0, c = 2\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10 15 20 25 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(d) Electric\nvehi-\ncles, full scale, λ =\n0, c = 2\nBaseline Convergence\nBefore Convergence After Convergence\n2\n4\n6\n8 10 12 20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(e) Synthetic, par-\ntial scale, λ = 0.5,\nc = 2\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(f) Bike\nsharing,\npartial scale, λ =\n0.5, c = 2\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(g)\nEnergy\nde-\nmand, partial scale,\nλ = 0.5, c = 2\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20\n30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(h) Electric\nvehi-\ncles, partial scale,\nλ = 0.5, c = 2\nBaseline Convergence\nBefore Convergence After Convergence\n2\n4\n6\n8 10 12 20 30 40 50 60\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(i) Synthetic, par-\ntial scale, λ = 0,\nc = 5\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(j)\nBike\nsharing,\npartial scale, λ = 0,\nc = 5\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20 30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(k)\nEnergy\nde-\nmand, partial scale,\nλ = 0, c = 5\nBaseline Convergence\nBefore Convergence After Convergence\n5\n10\n15\n20\n30 40 50 60 70\n0\n2\n4\n6\n8\nIteration\nStandardized Global Cost\n(l)\nElectric\nvehi-\ncles, partial scale,\nλ = 0, c = 5\nFig. 12 Learning curves for comparison with Figure 5. Dimensions: holarchic schemes,\napplication scenarios, full versus partial scale, diﬀerent λ values, varying number of children.\n28\nEvangelos Pournaras et al.\n0\n1\n2\n3\n4\n5\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(a) Synthetic, hol-\narchic initialization\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(b) Bike\nsharing,\nholarchic initializa-\ntion\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(c)\nEnergy\nde-\nmand,\nholarchic\ninitialization\n0\n100\n200\n300\n400\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(d) Electric\nvehi-\ncles, holarchic ini-\ntialization\n0\n2\n4\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(e) Synthetic, hol-\narchic runtime\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(f) Bike\nsharing,\nholarchic runtime\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(g)\nEnergy\nde-\nmand,\nholarchic\nruntime\n0\n5\n10\n15\n20\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(h) Electric\nvehi-\ncles, holarchic run-\ntime\n0\n200\n400\n600\n800\n1000\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(i) Synthetic, hol-\narchic termination\n0\n5000\n10000\n15000\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(j)\nBike\nsharing,\nholarchic\ntermina-\ntion\n0\n200\n400\n600\n800\n1000\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(k)\nEnergy\nde-\nmand,\nholarchic\ntermination\n0\n200\n400\n600\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nλ = 0\nλ = 0.25\nλ = 0.5\nλ = 0.75\n(l)\nElectric\nve-\nhicles,\nholarchic\ntermination\nFig. 13 Probability density of the improvement index that elaborates on Figure 6. Dimen-\nsions: holarchic schemes, application scenarios, diﬀerent λ values. Settings: partial scale,\nc = 2.\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n29\n0\n1\n2\n3\n4\n5\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(a) Synthetic, hol-\narchic initialization\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(b) Bike\nsharing,\nholarchic initializa-\ntion\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(c)\nEnergy\nde-\nmand,\nholarchic\ninitialization\n0\n100\n200\n300\n400\n500\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(d) Electric\nvehi-\ncles, holarchic ini-\ntialization\n0\n2\n4\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(e) Synthetic, hol-\narchic runtime\n0\n1\n2\n3\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(f) Bike\nsharing,\nholarchic runtime\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(g)\nEnergy\nde-\nmand,\nholarchic\nruntime\n0\n50\n100\n150\n200\n250\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(h) Electric\nvehi-\ncles, holarchic run-\ntime\n0\n200\n400\n600\n800\n1000\n1200\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(i) Synthetic, hol-\narchic termination\n0\n5000\n10000\n15000\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(j)\nBike\nsharing,\nholarchic\ntermina-\ntion\n0\n200\n400\n600\n800\n1000\n1200\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(k)\nEnergy\nde-\nmand,\nholarchic\ntermination\n0.0e+00\n5.0e+08\n1.0e+09\n1.5e+09\n2.0e+09\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(l)\nElectric\nve-\nhicles,\nholarchic\ntermination\nFig. 14 Probability density of the improvement index that elaborates on Figure 7. Dimen-\nsions: holarchic schemes, application scenarios, varying number of children. Settings: partial\nscale, λ = 0.\n30\nEvangelos Pournaras et al.\n0\n1\n2\n3\n4\n5\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(a) Synthetic, hol-\narchic initialization\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(b) Bike\nsharing,\nholarchic initializa-\ntion\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(c)\nEnergy\nde-\nmand,\nholarchic\ninitialization\n0\n100\n200\n300\n400\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(d) Electric\nvehi-\ncles, holarchic ini-\ntialization\n0\n2\n4\n6\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(e) Synthetic, hol-\narchic runtime\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(f) Bike\nsharing,\nholarchic runtime\n0\n1\n2\n3\n4\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(g)\nEnergy\nde-\nmand,\nholarchic\nruntime\n0\n10\n20\n30\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(h) Electric\nvehi-\ncles, holarchic run-\ntime\n0\n200\n400\n600\n800\n1000\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(i) Synthetic, hol-\narchic termination\n0\n5000\n10000\n15000\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(j)\nBike\nsharing,\nholarchic\ntermina-\ntion\n0\n200\n400\n600\n800\n1000\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(k)\nEnergy\nde-\nmand,\nholarchic\ntermination\n0\n200\n400\n600\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nFull\nPartial\n(l)\nElectric\nve-\nhicles,\nholarchic\ntermination\nFig. 15 Probability density of the improvement index that elaborates on Figure 8. Dimen-\nsions: holarchic schemes, application scenarios, partial versus full scale. Settings: λ = 0,\nc = 2.\nHolarchic Structures for Decentralized Deep Learning – A Performance Analysis\n31\n0\n5\n10\n15\n20\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(a) Synthetic, total\ncommunication\n0\n2\n4\n6\n8\n10\n12\n14\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(b) Bike\nsharing,\ntotal\ncommunica-\ntion\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(c)\nEnergy\nde-\nmand,\ntotal\ncom-\nmunication\n0\n20\n40\n60\n80\n100\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(d) Electric\nvehi-\ncles, total commu-\nnication\n0\n5\n10\n15\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(e) Synthetic, syn-\nchronized\ncommu-\nnication\n0\n2\n4\n6\n8\n10\n12\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(f) Bike\nsharing,\nsynchronized\ncom-\nmunication\n0\n2\n4\n6\n8\n10\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(g)\nEnergy\nde-\nmand,\nsynchro-\nnized\ncommunica-\ntion\n0\n20\n40\n60\n80\n100\n−1.0\n−0.5\n0.0\n0.5\n1.0\nImprovement Index\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(h) Electric\nvehi-\ncles,\nsynchronized\ncommunication\nFig. 16 Probability density of the improvement index. Dimensions: total versus synchro-\nnized communication cost, application scenarios, varying number of children. Settings: hol-\narchic runtime, partial scale, λ = 0.\n0\n2\n4\n6\n8\n10\n12\n14\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(a) Synthetic, total\ncommunication\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(b) Bike\nsharing,\ntotal\ncommunica-\ntion\n0\n5\n10\n15\n20\n25\n30\n35\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(c)\nEnergy\nde-\nmand,\ntotal\ncom-\nmunication\n0.0\n0.5\n1.0\n1.5\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(d) Electric\nvehi-\ncles, total commu-\nnication\n0\n2\n4\n6\n8\n10\n12\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(e) Synthetic, syn-\nchronized\ncommu-\nnication\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(f) Bike\nsharing,\nsynchronized\ncom-\nmunication\n0\n5\n10\n15\n20\n25\n30\n35\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(g)\nEnergy\nde-\nmand,\nsynchro-\nnized\ncommunica-\ntion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n96\n98\n100\n102\n104\nRelative Performance [%]\nProbability Density\nc = 2\nc = 3\nc = 4\nc = 5\n(h) Electric\nvehi-\ncles,\nsynchronized\ncommunication\nFig. 17 Probability density of the relative global cost reduction. Dimensions: total ver-\nsus synchronized communication cost, application scenarios, varying number of children.\nSettings: holarchic runtime, partial scale, λ = 0.\n32\nEvangelos Pournaras et al.\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(a) Synthetic, total\ncommunication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(b) Bike\nsharing,\ntotal\ncommunica-\ntion\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(c)\nEnergy\nde-\nmand,\ntotal\ncom-\nmunication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(d) Electric\nvehi-\ncles, total commu-\nnication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(e) Synthetic, syn-\nchronized\ncommu-\nnication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(f) Bike\nsharing,\nsynchronized\ncom-\nmunication\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(g)\nEnergy\nde-\nmand,\nsynchro-\nnized\ncommunica-\ntion\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\nBaseline \n    Convergence\n0\n2\n4\n6\n0\n25000\n50000\n75000\n10000\nCommunication Cost\nStandardized Global Cost\n(h) Electric\nvehi-\ncles,\nsynchronized\ncommunication\nFig. 18 Cost eﬀectiveness for comparison with Figure 11. Dimensions: baseline versus ho-\nlarchic runtime, application scenarios, total versus synchronized communication cost. Set-\ntings: partial scale, λ = 0, c = 5.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-05-07",
  "updated": "2018-09-17"
}