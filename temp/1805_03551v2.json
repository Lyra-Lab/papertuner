{
  "id": "http://arxiv.org/abs/1805.03551v2",
  "title": "A Unified Framework of Deep Neural Networks by Capsules",
  "authors": [
    "Yujian Li",
    "Chuanhui Shan"
  ],
  "abstract": "With the growth of deep learning, how to describe deep neural networks\nunifiedly is becoming an important issue. We first formalize neural networks\nmathematically with their directed graph representations, and prove a\ngeneration theorem about the induced networks of connected directed acyclic\ngraphs. Then, we set up a unified framework for deep learning with capsule\nnetworks. This capsule framework could simplify the description of existing\ndeep neural networks, and provide a theoretical basis of graphic designing and\nprogramming techniques for deep learning models, thus would be of great\nsignificance to the advancement of deep learning.",
  "text": "A Uniﬁed Framework of Deep Neural Networks by\nCapsules\nYujian Li\nCollege of Computer Science\nFaculty of Information Technology\nBeijing University of Technology\nBeijing, China 100124\nliyujian@bjut.edu.cn\nChuanhui Shan\nCollege of Computer Science\nFaculty of Information Technology\nBeijing University of Technology\nBeijing, China 100124\nchuanhuishan@emails.bjut.edu.cn\nAbstract\nWith the growth of deep learning, how to describe deep neural networks uniﬁedly\nis becoming an important issue. We ﬁrst formalize neural networks mathematically\nwith their directed graph representations, and prove a generation theorem about the\ninduced networks of connected directed acyclic graphs. Then, we set up a uniﬁed\nframework for deep learning with capsule networks. This capsule framework could\nsimplify the description of existing deep neural networks, and provide a theoretical\nbasis of graphic designing and programming techniques for deep learning models,\nthus would be of great signiﬁcance to the advancement of deep learning.\n1\nIntroduction\nDeep learning has made a great deal of success in processing images, audios, and natural languages\n[1-3], inﬂuencing academia and industry dramatically. It is essentially a collection of various methods\nfor effectively training neural networks with deep structures. A neural network is usually regarded as\na hierarchical system composed of many nonlinear computing units (or neurons, nodes). The most\npopular neural network was once multilayer perceptron (MLP) [4]. A MLP consists of an input layer,\na number of hidden layers and an output layer, as shown in Figure 1. The depth of it is the number of\nlayers excluding the input layer. If the depth is greater than 2, a neural network is now called “deep”.\nFor training MLPs, backpropagation (BP) is certainly the most well-known algorithm in common\nuse [4], but it seemed to work only for shallow networks. In 1991, Hochreiter indicated that typical\ndeep neural networks (DNNs) suffer from the problem of vanishing or exploding gradients [5]. To\novercome training difﬁculties in DNNs, Hinton et al. started the new ﬁeld of deep learning in 2006\n[6, 7].\nBesides deep MLPs, DNNs also include convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs). Here, we omit RNNs for saving space. Theoretically, a CNN can be regarded\nas a special MLP or feedforward neural network. It generally consists of an input layer, alternating\nconvolutional and pooling layers, a fully connected layer, and an output layer, as shown in Figure\n2. Note that “convolutional layers” are also called \"detection layers\", and “pooling layers\" are also\ncalled “downsampling layers”. There have been a large number of CNN variants, for example, LeNet\n[8], AlexNet [1], VGGNet [9], GoogLeNet [10], ResNet [11], Faster R-CNN [12], DenseNet [13],\nMask R-CNN [14], YOLO [15], SSD [16], and so on. They not only take the lead in competitions of\nimage classiﬁcation and recognition as well as object localization and detection [9-12], but also in\nother applications such as deep Q-networks [17], AlphaGo [18], speech recognition [2], and machine\ntranslation [3]. To cope with the disadvantages of CNNs, in 2017 Hinton et al. further proposed a\ncapsule network [19], which is more convincing from the neurobiological point of view. So many\ndeep models are dazzling with different structures. Some of them have added shortcut connections,\nPreprint. Work in progress.\narXiv:1805.03551v2  [cs.LG]  10 May 2018\nFigure 1: The structure of a MLP.\nFigure 2: The structure of a CNN.\nparallel connections, and even nested structures to traditional layered structures. How to establish a\nuniﬁed framework for DNNs is becoming a progressively important issue in theory. We are motivated\nto address it.\nThis paper is organized as follows. In Section 2, we propose a mathematical deﬁnition to formalize\nneural networks, give their directed graph representations, and prove a generation theorem about the\ninduced networks of connected directed acyclic graphs. In Section 3, we use the concept of capsule\nto extend neural networks, deﬁne an induced model for capsule networks, and establish a uniﬁed\nframework for deep learning with a universal backpropagation algorithm. Finally, in Section 4 we\nmake a few conclusions to summarize the signiﬁcance of the capsule framework to advance deep\nlearning in theory and application.\n2\nFormalization of Neural networks\n2.1\nMathematical deﬁnition\nA neural network is a computational model composed of nodes and connections. Nodes are di-\nvided into input nodes and neuron nodes. Input nodes can be represented by real variables, e.g.\nx1, x2, · · · , xn . The set of input nodes is denoted as X = {x1, x2, · · · , xn} . A neuron node can\nreceive signals through connections both from input nodes and the outputs of other neuron nodes,\nand perform a weighted sum of these signals for a nonlinear transformation. Note that the weight\nmeasures the strength of a connection, and the nonlinear transformation is the effect of an activation\nfunction. Let F be a set of activation functions, such as sigmoid, tanh, ReLU, and so on.\nOn X and F, a neural network can be formally deﬁned as a 4-tuple net = (S, H, W, Y ), where S is\na set of input nodes, H is a set of neuron nodes, W is a set of weighting connections, and Y is a set\nof outputs. The neural network is recursively generated by four basic rules as follows:\n1) Rule of variable. For any z ∈X, let yz = z. If S = {z}, H = ∅, W = ∅, Y = {yz}, then the\n4-tuple net = (S, H, W, Y ) is a neural network.\n2) Rule of neuron. For any nonempty subset S ⊆X, ∀f ∈F, ∀b ∈R, construct a node\nh ̸∈X that depends on (f, b) and select a set of weighting connections wxi→h(xi ∈S). Let\nyh = f(P\nxi∈S wxi→hxi + b) be the output of node h. If H = {h}, W = {wxi→h|xi ∈S}, and\nY = {yh}, then net = (S, H, W, Y ) is a neural network.\n3) Rule of growth. Suppose net = (S, H, W, Y ) is a neural network. For any nonempty subset\nN ⊆S ∪H, ∀f ∈F, ∀b ∈R, construct a node h ̸∈S ∪H that depends on (f, b) and select a\n2\nFigure 3: (a)A trivial network; (b)A 1-input-1-neuron network.\nFigure 4: Three 1-input-2-neuron networks.\nset of weighting connections wzj→h(zj ∈N). Let yh = f(P\nzj∈N wzj→hyzj + b) be the output\nof node h. If S′ = S, H′ = H ∪{h}, W ′ = W ∪{wzj→h|zj ∈N}, and Y ′ = Y ∪{yh}, then\nnet′ = (S′, H′, W ′, Y ′) is also a neural network.\n4) Rule of convergence. Suppose netk = (Sk, Hk, Wk, Yk)(1 ≤k ≤K) are K neural networks,\nsatisfying that ∀1 ≤i ̸= j ≤K, (Si ∪Hi) ∩(Sj ∪Hj) = ∅. For any nonempty subsets Ak ⊆\nSk ∪Hk(1 ≤k ≤K), N = SK\nk=1 Ak, ∀f ∈F, ∀b ∈R, construct a node h ̸∈SK\nk=1(Sk ∪Hk) that\ndepends on (f, b), select a set of weighting connections wz→h(z ∈N). Let yh = f(P\nz∈N wz→hyz+\nb) be the output of the node h. If S = SK\nk=1 Sk, H = (SK\nk=1 Hk) ∪{h}, W = (SK\nk=1 Wk) ∪\n{wz→h|z ∈N}, and Y = (SK\nk=1 Yk) ∪{yh}, then net = (S, H, W, Y ) is also a neural network.\nAmong the four generation rules, it should be noted that the rule of neuron is not independent. This\nrule can be derived from the rule of variable and the rule of convergence. Moreover, the weighting\nconnection wz→h should be taken as a combination of the weight and the connection, rather than just\nthe weight. Additionally, if a node h depends on (f, b), f is called the activation function of h, and b\nis called the bias of h.\n2.2\nDirected graph representation\nLet X be a set of real variables and F be a set of activation functions. For any neural network\nnet = (S, H, W, Y ) on X and F, a directed acyclic graph Gnet = (V, E) can be constructed with\nthe vertex set V = S ∪H and the directed edge set E = {z →h|wz→h ∈W}. Gnet = (V, E)\nis called the directed graph representation of net = (S, H, W, Y ). Two cases of the representation\ngeneration are discussed in the following.\n1)The case of X = {x1}\nUsing the rule of variable, for x1 ∈X, let yx1 = x1. If S = {x1}, H = ∅, W = ∅, and Y = {yx1},\nthen net = (S, H, W, Y ) is a neural network. Since this network has only one input node without any\nfunction for nonlinear transformation, it is also called a trivial network, as shown in Figure 3(a). Using\nthe rule of neuron, for a nonempty subset S = {x1} ⊆X, ∀f ∈F, ∀b ∈R, construct a node h1 ̸∈S\nthat depends on (f, b), select a weighting connection wx1→h1, and let yh1 = f(wx1→h1x1 + b). If\nH = {h1}, W = {wx1→h1}, and Y = {yh1}, then net = (S, H, W, Y ) is a neural network, which\nhas one input and one neuron. It is also called a 1-input-1-neuron network, as shown in Figure 3(b).\nUsing the rule of growth on the network, three new neural networks with different structures can be\ngenerated, as shown in Figures 4(a-c). Likewise, they are called 1-input-2-neuron networks. Using\nthe rule of growth on the three networks, twenty-one new neural networks with different structures\ncan be totally generated further. Seven out of them for Figure 4(a) are displayed in Figures 5(a-g).\nThey are called 1-input-3-neuron networks.\n2)The case of X = {x1, x2}\nUsing the rule of variable, for x1, x2 ∈X, let yx1 = x1 and yx2 = x2. If S1 = {x1}, S2 = {x2},\nH1 = H2 = ∅, W1 = W2 = ∅, Y1 = {yx1}, and Y2 = {yx2}, then net1 = ({x1}, ∅, ∅, {yx1}) and\nnet2 = ({x2}, ∅, ∅, {yx2}) are neural networks. Obviously, both of them are trivial networks. Using\n3\nFigure 5: Seven 1-input-3-neuron networks.\nFigure 6: A 2-input-1-neuron network.\nthe rule of neuron, for a nonempty subset S ⊆X, if S = {x1} or S = {x2}, the neural network can\nbe similarly constructed with the case of X = {x1}.\nIf X = {x1, x2}, ∀f ∈F, ∀b ∈R, construct a node h1 ̸∈S that depends on (f, b), select a set\nof weighting connections wxi→h1(xi ∈S) and let yh1 = f(P\nxi∈S wxi→h1xi + b). If H = {h1},\nW = {wx1→h1, wx2→h1}, and Y = {yh1}, then net = (S, H, W, Y ) is a neural network. This is a\n2-input-1-neuron network, as depicted in Figure 6. Using the rule of growth on this network, seven\n2-input-2-neuron networks with different structures can be generated, as shown in Figures 7(a-g).\nFinally, the rule of convergence is necessary. In fact, it cannot generate all neural networks only using\nthe three rules of variable, neuron and growth. For example, the network in Figure 8(c) cannot be\ngenerated without using the rule of convergence on the two in Figures 8(a-b).\n2.3\nInduced network and its generation theorem\nSuppose G = (V, E) is a connected directed acyclic graph, where V denotes the vertex set and E\ndenotes the directed edge set. For any vertex h ∈V , let INh = {z|z ∈V, z →h ∈E} be the set of\nvertices each with a directed edge to h, and OUTh = {z|z ∈V, h →z ∈E} be the set of vertices\nfor h to have directed edges each to. If INh = ∅, then h is called an input node of G. If OUTh = ∅,\nthen h is called an output node of G. Otherwise, h is called a hidden node of G. Let X stand for\nFigure 7: Seven 2-input-2-neuron networks.\n4\nFigure 8: A necessary explanation for the rule of convergence.\nthe set of all input nodes, O for the set of all output nodes, and M for the set of all hidden nodes.\nObviously, V = X ∪M ∪O, and M = V −X ∪O.\nFurthermore, let yh be the output of node h, and wz→h be the weighting connection from z to h.\nThen, a computational model of graph G can be deﬁned as follows:\n1) ∀z ∈X, yz = z.\n2) ∀h ∈M ∪O, select f ∈F and b ∈R to compute yh = f(P\nz∈INh wz→hyz + b).\nIf S = X, H = M ∪O, W = {wz→h|z →h ∈E}, and Y = {yh|h ∈V }, then netG =\n(S, H, W, Y ) is called an induced network of graph G. The following generation theorem holds on\nthe induced network.\nGeneration Theorem: For any connected directed acyclic graph G = (V, E), its induced network\nnetG is a neural network that can be recursively generated by the rules of variable, neuron, growth,\nand convergence.\nProof: By induction on |V | (i.e. number of vertices), we prove the theorem as follows.\n1) When |V | = 1, we have |X| = 1 and |O| = 0, so the induced network netG is a neural network\nthat can be generated directly by the rule of variable.\n2) When |V | = 2, we have |X| = 1 and |O| = 1, so the induced network netG is a neural network\nthat can be generated directly by the rule of growth.\n3) Assume that the theorem holds for |V | ≤n. When |V | = n + 1 ≥3, the induced network netG\nhas at least one output node h ∈O. Let Eh = {z →h ∈E} denote the set of edges heading\nto the node h. Moreover, let V ′ = V −{h} and E′ = E −Eh. Based on the connectedness of\nG′ = (V ′, E′), we have two cases to discuss in the following:\ni) If G′ = (V ′, E′) is connected, then applying the induction assumption for |V ′| ≤n, the\ninduced network netG′ = (S′, H′, W ′, Y ′) can be recursively generated by the rules of\nvariable, neuron, growth, and convergence. Let N = INh. In netG = (S, H, W, Y ),\nwe use f ∈F and b ∈R to stand for the activation function and bias of node h, and\nwz→h(z ∈N) for the weighting connection from node z to the node h. Then, netG can\nbe obtained by using the rule of growth on netG′, to generate the node h and its output\nyh = f(P\nz∈N wz→hyz + b).\nii) Otherwise, G′ comprises a number of disjoint connected components Gk = (Vk, Ek)(1 ≤\nk ≤K). Using the induction assumption for |Vk| ≤n(1 ≤k ≤K), the induced\nnetwork netGk = (Sk, Hk, Wk, Yk) can be recursively generated by the rules of variable,\nneuron, growth, and convergence. Let Ak = (Sk ∪Hk) ∩INh, and N = SK\nk=1 Ak. In\nnetG = (S, H, W, Y ), we use f ∈F and b ∈R to stand for the activation function and\nbias of the node h, and wz→h(z ∈N) for the weighting connection from node z to node\nh. Then, netG can be obtained by using the rule of convergence on netGk(1 ≤k ≤K), to\ngenerate the node h and its output yh = f(P\nz∈N wz→hyz + b).\nAs a result, the theorem always holds.\n3\nCapsule framework of Deep learning\n3.1\nMathematical deﬁnition of capsules\nIn 2017, Hinton et al. pioneered the idea of capsules and considered a nonlinear “squashing” capsule\n[19]. From the viewpoint of mathematical models, a capsule is essentially an extension of the\n5\nFigure 9: Mathematical model of a general capsule.\nFigure 10: The capsule structure of a MLP.\ntraditional activation function. It is primarily deﬁned as an activation function with a vector input and\na vector output. More generally, a capsule can be an activation function with a tensor input and a\ntensor output.\nAs shown in Figure 9, a general capsule may have n input tensors X1, X2, · · · , Xn, n weight tensors\nW1, W2, · · · , Wn, and a capsule bias B, and n weighting operations ⊗1, ⊗2, · · · , ⊗n. Note that a\nweighting operation may be taken as an identity transfer, a scalar multiplication, a vector dot product,\na matrix multiplication, a convolution operation, and so on. Meantime, Wi ⊗i Xi(1 ≤i ≤n) and B\nmust be tensors with the same dimension. The total input of the capsule is U = P\ni Wi ⊗i Xi + B,\nand the output Y is a tensor computed by a nonlinear capsule function cap, namely,\nY = cap(U) = cap(\nX\ni\nWi ⊗i Xi + B).\n(1)\nFor convenience, we use F to stand for a nonempty set of capsule functions, and T for the set of all\ntensors.\n3.2\nCapsule Networks\nSuppose G = (V, E) is a connected directed acyclic graph, where V denotes the vertex set and E\ndenotes the directed edge set. For any vertex H ∈V, let INH be the set of vertices each with a\ndirected edge to H, and OUTH be the set of vertices for H to have a directed edge each to. If\nINH = ∅, then H is called an input node of G. If OUTH = ∅, then H is called an output node of\nG. Otherwise, H is called a hidden node of G. Let X stand for the set of all input nodes, O for the\nset of all output nodes, and M for the set of all hidden nodes. Obviously, V = X ∪M ∪O, and\nM = V −X ∪O.\nFurthermore, let YH be the output of node H, and (WZ→H, ⊗Z→H) be the tensor-weighting con-\nnection from Z to H. If ∀H ∈M ∪O, ∀Z ∈INH, WZ→H ⊗Z→H YZ and B are tensors with the\nsame dimension, then a tensor-computational model of graph G can be deﬁned as follows:\n1) ∀Z ∈X, YZ = Z.\n2)\n∀H\n∈\nM ∪O,\nselect\ncap\n∈\nF\nand\nB\n∈\nT\nto\ncompute\nYH\n=\ncap(P\nZ∈INH WZ→H ⊗Z→H YZ + B).\nIf S = X, H = M ∪O, W = {(WZ→H, ⊗Z→H)|Z →H ∈E}, and Y = {YH|H ∈V}, then\nnetG = (S, H, W, Y) is called a tensor-induced network of graph G. This network is also called a\ncapsule network.\nUsing a capsule network, a MLP can be simpliﬁed as a directed acyclic path of capsules. For\nexample, the MLP in Figure 1 has ﬁve layers: an input layer, three hidden layers, and an output\nlayer. On the whole, each layer could be thought of as a capsule. Let X = (x1, x2, · · · , x5)T\nstand for the input capsule node, Hi = (capi, Bi)(i = 1, 2, 3) for the hidden capsule nodes, and\nO = (cap4, B4) for the output capsule node. Note that capsule function capi and capsule bias Bi are\ndeﬁned by the elementwise activation function and the bias vector respectively of the correspond-\ning layer in the MLP. If the weighting operations ⊗X→H1, ⊗H1→H2, ⊗H2→H3, and ⊗H3→O are\n6\nFigure 11: The Capsule structure of a CNN, with “∗” standing for convolution, “→” for identity\ntransfer, “◁” for tensor reshaping, and “×” for matrix multiplication.\nall taken as matrix multiplication “×”, then we have (WX→H1, ⊗X→H1) = ((wX→H1\nm,n\n)7×5, ×),\n(WH1→H2, ⊗H1→H2) = ((wH1→H2\nm,n\n)7×7, ×), (WH2→H3, ⊗H2→H3) = ((wH2→H3\nm,n\n)7×7, ×) and\n(WH3→O, ⊗H3→O) = ((wH3→O\nm,n\n)4×7, ×), which are the tensor-weighting connections from X\nto H1, H1 to H2, H2 to H3 and H3 to O. Finally, let YHi(i = 1, 2, 3) stand for the output vec-\ntor of Hi, and YO for the output vector of O. Setting YHO = X and YH4 = YO, we obtain\nYHi = capi(WHi−1→Hi × YHi−1 + Bi). Therefore, the capsule structure of the MLP is a directed\nacyclic path, as displayed in Figure 10.\nBesides MLPs, capsule networks can also be used to simplify the structures of other DNNs. Let\nus consider the CNN in Figure 2. This CNN has 7 layers: one input layer, two convolutional lay-\ners, two downsampling (pooling) layers, one fully connected layer, and one output layer. On the\nwhole, each of the layers could be thought of as a capsule. Let X stand for the input capsule node,\nHi = (capi, Bi)(i = 1, · · · , 5) for the hidden capsule nodes, and O = (cap6, B6) for the output cap-\nsule node. Note that cap1 and cap3 are capsule functions deﬁned by elementwise ReLUs. cap2 and\ncap4 are capsule functions deﬁned by downsampling “↓”. cap5 is an identity function. cap6 is a cap-\nsule function deﬁned by softmax. In addition, Bi(i = 1, · · · , 6) are capsule biases each deﬁned by the\nbias tensor of the corresponding layer in the CNN. Let both ⊗X→H1 and ⊗H2→H3 be the convolution\noperation “∗”, both ⊗H1→H2 and ⊗H3→H4 be the identity transfer “→”, ⊗H4→H5 be the tensor-\nreshaping operation “◁”, and ⊗H5→O be the matrix multiplication “×”. Then, (WX→H1, ⊗X→H1) =\n(WX→H1, ∗), (WH1→H2, ⊗H1→H2)\n=\n(””, →), (WH2→H3, ⊗H2→H3)\n=\n(WH2→H3, ∗),\n(WH3→H4, ⊗H3→H4) = (””, →), (WH4→H5, ⊗H4→H5) = (””, ◁), and (WH5→O, ⊗H5→O) =\n(WH5→O, ×), which are the tensor-weighting connections from X to H1, H1 to H2, H2 to H3, H3\nto H4, H4 to H5, and H5 to O. Finally, let YHi(i = 1, 2, 3, 4, 5) stand for the output tensor of Hi,\nand YO for the output tensor of O. This leads to the following computations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYH1 = cap1(WX→H1 ∗X + B1) = ReLU(WX→H1 ∗X + B1),\nYH2 = cap2(WH1→H2 ⊗H1→H2 YH1 + B2) = cap2(→YH1 + B2) =↓YH1 + B2,\nYH3 = cap3(WH2→H3 ∗X + B3) = ReLU(WH2→H3 ∗YH2 + B3),\nYH4 = cap4(WH3→H4 ⊗H3→H4 YH3 + B4) = cap4(→YH3 + B4) =↓YH3 + B4,\nYH5 = cap5(◁YH4 + B5) = ◁YH4,\nYO = cap6(WH5→O × YH5 + B6) = softmax(WH5→O × YH5 + B6).\n(2)\nTherefore, the capsule structure of the CNN is also a directed acyclic path, as depicted in Figure 11.\nBesides simplifying the description of existing DNNs, the capsule networks can also be used to\ngraphically design a variety of new structures for complex DNNs, such as displayed in Figure 12.\n3.3\nUniversal backpropagation of capsule networks\nSuppose G = (V, E) is a connected directed acyclic graph. Let X = {X1, X2, · · · , Xn} stand\nfor the set of all input nodes, O = {O1, O2, · · · , Om} for the set of all output nodes, and M =\nV −X ∪O = {H1, H2, · · · , Hl} for the set of all hidden nodes. netG = (S, H, W, Y) is an tensor-\ninduced network of graph G. This is also a capsule network. If the number of nodes |S ∪H| ≥2,\nthen for ∀H ∈H,\n\u001aUH = P\nZ∈INH WZ→H ⊗Z→H YZ + BH,\nYH = capH(UH) = capH(P\nZ∈INH WZ→H ⊗Z→H YZ + BH).\n(3)\nFor any output node H ∈O, let YH and TH be its actual output and expected output for input X,\nrespectively. The loss function between them is deﬁned as LH = Loss(YH, TH). Accordingly, we\n7\nFigure 12: Structure of a general capsule network.\nAlgorithm 1: One iteration of the universal backpropagation algorithm.\n1) Select a learning rate η > 0,\n2) ∀H ∈M ∪O, ∀Z ∈INH, initialize WZ→H and BH,\n3) ∀H ∈O, compute δH = ∂Loss(YH,TH)\n∂YH\n· ∂capH\n∂UH ,\n4) ∀H ∈M, compute δH = P\nP ∈OUTH δP · ∂UP\n∂YH · ∂capH\n∂UH ,\n5) Compute ∆WZ→H = δH ·\n∂UH\n∂WZ→H and ∆BH = δH,\n6) Update WZ→H ←WZ→H −η · ∆WZ→H,BH ←BH −η · ∆BH.\nhave the total loss function L = P\nH∈O LH. Let δH =\n∂L\n∂UH denote the backpropagated error signal\n(or sensitivity) for capsule node H. By the chain rule, we further obtain:\n∀H ∈O,\n\n\n\n\n\nδH\n=\n∂L\n∂UH = ∂Loss(YH,TH)\n∂YH\n· ∂capH\n∂UH ,\n∂L\n∂BH\n=\n∂L\n∂UH · ∂UH\n∂BH = δH,\n∂L\n∂WZ→H\n=\n∂L\n∂UH ·\n∂UH\n∂WZ→H = δH ·\n∂UH\n∂WZ→H .\n(4)\n∀H ∈M,\n\n\n\n\n\n\n\n\n\nδH\n=\n∂L\n∂UH = P\nP ∈OUTH\n∂L\n∂UP · ∂UP\n∂YH · ∂YH\n∂UH\n= P\nP ∈OUTH δP · ∂UP\n∂YH · ∂capH\n∂UH ,\n∂L\n∂BH\n=\n∂L\n∂UH · ∂UH\n∂BH = δH,\n∂L\n∂WZ→H\n=\n∂L\n∂UH ·\n∂UH\n∂WZ→H = δH ·\n∂UH\n∂WZ→H .\n(5)\nNote that in formulae (4)-(5), ∂capH\n∂UH\ndepends on the speciﬁc form of capsule function capH. For\nexample, when capH is an elementwise sigmoid function, the result is ∂capH\n∂UH\n= sigmoid(UH)(1 −\nsigmoid(UH)). Meanwhile,\n∂UH\n∂WZ→H and ∂UP\n∂YH also depend on the speciﬁc choice of the weighting\noperation ⊗Z→H.\nBased on formulae (4)-(5), a universal backpropagation algorithm can be designed theoretically for\ncapsule networks, with one iteration detailed in Algorithm 1. In practice, this algorithm should be\nchanged to one of many variants with training data [20].\n4\nConclusions\nBased on the formalization of neural networks, we have developed capsule networks to establish a\nuniﬁed framework for deep learning. This capsule framework could not only simplify the description\nof existing DNNs, but also provide a theoretical basis of graphical designing and programming for\nnew deep learning models. As future work, we will try to deﬁne an industrial standard and implement\na graphic platform for the advancement of deep learning with capsule networks, and even with a\nsimilar extension to recurrent neural networks.\n8\nReferences\n[1] Krizhevsky, A., Sutskever, I. & Hinton, G.E. (2012) Imagenet classiﬁcation with deep convolutional neural\nnetworks. In F. Pereira, C.J.C. Burges, L. Bottou and K.Q. Weinberger (eds.), Advances in neural information\nprocessing systems 25, pp. 1097–1105. Cambridge, MA: MIT Press.\n[2] Amodei, D., Ananthanarayanan, S. & Anubhai, R. et al. (2016) Deep speech 2: End-to-end speech recognition\nin English and Mandarin. International Conference on Machine Learning, pp. 173–182.\n[3] Wu, Y., Schuster, M. & Chen, Z. et al. (2016) Google’s Neural Machine Translation System: Bridging the\nGap between Human and Machine Translation. arXiv preprint arXiv:1609.08144.\n[4] Rumellhart, D.E. (1986) Learning internal representations by error propagation. Parallel distributed\nprocessing: Explorations in the microstructure of cognition 1:319-362.\n[5] Schmidhuber, J. (2014) Deep learning in neural networks: An overview. Neural Network 61:85-117.\n[6] Hinton, G.E. & Salakhutdinov, R.R. (2006) Reducing the dimensionality of data with neural networks.\nScience 313(5786):504-507.\n[7] Hinton, G.E., Osindero, S. & Teh, Y.W. (2006) A fast learning algorithm for deep belief nets. Neural\ncomputation 18(7):1527-1554.\n[8] LeCun, Y., Bottou, L. & Bengio Y, et al. (1998) Gradient-based learning applied to document recognition.\nProceedings of the IEEE 86(11):2278-2324.\n[9] Simonyan, K. & Zisserman, A. (2014) Very Deep Convolutional Networks for Large-Scale Image Recognition.\nComputer Science.\n[10] Szegedy, C. Liu, W. & Jia, Y. et al. (2015) Going deeper with convolutions. IEEE Conference on Computer\nVision and Pattern Recognition.\n[11] He, K. Zhang, X. & Ren, S. et al. (2016) Deep residual learning for image recognition. Proceedings of the\nIEEE conference on computer vision and pattern recognition, pp. 770–778\n[12] Ren, S., He, K. & Girshick, R. et al. (2015) Faster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information processing systems, pp. 91–99.\n[13] Huang, G., Liu, Z. & Weinberger, K.Q. et al. (2017) Densely connected convolutional networks. Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 1(2):3.\n[14] He, K., Gkioxari, G. & Dollár, P. et al. (2017) Mask r-cnn. Computer Vision (ICCV), 2017 IEEE International\nConference on. IEEE, pp. 2980–2988.\n[15] Redmon, J., Divvala, S. & Girshick, R. et al. (2016) You only look once: Uniﬁed, real-time object detection.\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 779–788.\n[16] Liu, W., Anguelov, D. & Erhan, D. et al. (2016) Ssd: Single shot multibox detector. European conference\non computer vision. Springer, Cham, pp. 21–37.\n[17] Mnih, V., Kavukcuoglu, K. & Silver, D. et al. (2015) Human-level control through deep reinforcement\nlearning. Nature 518(7540):529.\n[18] Silver, D., Schrittwieser, J. & Simonyan, K. et al. (2017) Mastering the game of Go without human\nknowledge. Nature 550(7676):354-359.\n[19] Sabour, S., Frosst, N. & Hinton, G.E. (2017) Dynamic routing between capsules. In I. Guyon, U.V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R. Garnett (eds.), Advances in Neural Information\nProcessing Systems 30, pp. 3859-3869. Cambridge, MA: MIT Press.\n[20] Ruder, S. (2016) An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.\n9\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-05-09",
  "updated": "2018-05-10"
}