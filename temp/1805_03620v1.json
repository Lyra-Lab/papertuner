{
  "id": "http://arxiv.org/abs/1805.03620v1",
  "title": "On the Limitations of Unsupervised Bilingual Dictionary Induction",
  "authors": [
    "Anders Søgaard",
    "Sebastian Ruder",
    "Ivan Vulić"
  ],
  "abstract": "Unsupervised machine translation---i.e., not assuming any cross-lingual\nsupervision signal, whether a dictionary, translations, or comparable\ncorpora---seems impossible, but nevertheless, Lample et al. (2018) recently\nproposed a fully unsupervised machine translation (MT) model. The model relies\nheavily on an adversarial, unsupervised alignment of word embedding spaces for\nbilingual dictionary induction (Conneau et al., 2018), which we examine here.\nOur results identify the limitations of current unsupervised MT: unsupervised\nbilingual dictionary induction performs much worse on morphologically rich\nlanguages that are not dependent marking, when monolingual corpora from\ndifferent domains or different embedding algorithms are used. We show that a\nsimple trick, exploiting a weak supervision signal from identical words,\nenables more robust induction, and establish a near-perfect correlation between\nunsupervised bilingual dictionary induction performance and a previously\nunexplored graph similarity metric.",
  "text": "On the Limitations of Unsupervised Bilingual Dictionary Induction\nAnders Søgaard♥\nSebastian Ruder♠♣\nIvan Vuli´c3\n♥University of Copenhagen, Copenhagen, Denmark\n♠Insight Research Centre, National University of Ireland, Galway, Ireland\n♣Aylien Ltd., Dublin, Ireland\n3Language Technology Lab, University of Cambridge, UK\nsoegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk\nAbstract\nUnsupervised machine translation—i.e.,\nnot assuming any cross-lingual supervi-\nsion signal, whether a dictionary, transla-\ntions, or comparable corpora—seems im-\npossible, but nevertheless, Lample et al.\n(2018a) recently proposed a fully unsu-\npervised machine translation (MT) model.\nThe model relies heavily on an adversar-\nial, unsupervised alignment of word em-\nbedding spaces for bilingual dictionary in-\nduction (Conneau et al., 2018), which we\nexamine here. Our results identify the lim-\nitations of current unsupervised MT: un-\nsupervised bilingual dictionary induction\nperforms much worse on morphologically\nrich languages that are not dependent mark-\ning, when monolingual corpora from dif-\nferent domains or different embedding al-\ngorithms are used. We show that a simple\ntrick, exploiting a weak supervision sig-\nnal from identical words, enables more ro-\nbust induction, and establish a near-perfect\ncorrelation between unsupervised bilingual\ndictionary induction performance and a pre-\nviously unexplored graph similarity metric.\n1\nIntroduction\nCross-lingual word representations enable us to\nreason about word meaning in multilingual con-\ntexts and facilitate cross-lingual transfer (Ruder\net al., 2018). Early cross-lingual word embedding\nmodels relied on large amounts of parallel data\n(Klementiev et al., 2012; Mikolov et al., 2013a),\nbut more recent approaches have tried to minimize\nthe amount of supervision necessary (Vuli´c and\nKorhonen, 2016; Levy et al., 2017; Artetxe et al.,\n2017). Some researchers have even presented un-\nsupervised methods that do not rely on any form\nof cross-lingual supervision at all (Barone, 2016;\nConneau et al., 2018; Zhang et al., 2017).\nUnsupervised cross-lingual word embeddings\nhold promise to induce bilingual lexicons and ma-\nchine translation models in the absence of dictio-\nnaries and translations (Barone, 2016; Zhang et al.,\n2017; Lample et al., 2018a), and would therefore\nbe a major step toward machine translation to, from,\nor even between low-resource languages.\nUnsupervised approaches to learning cross-\nlingual word embeddings are based on the assump-\ntion that monolingual word embedding graphs are\napproximately isomorphic, that is, after removing a\nsmall set of vertices (words) (Mikolov et al., 2013b;\nBarone, 2016; Zhang et al., 2017; Conneau et al.,\n2018). In the words of Barone (2016):\n...we hypothesize that, if languages are used to\nconvey thematically similar information in similar\ncontexts, these random processes should be approx-\nimately isomorphic between languages, and that\nthis isomorphism can be learned from the statistics\nof the realizations of these processes, the mono-\nlingual corpora, in principle without any form of\nexplicit alignment.\nOur results indicate this assumption is not true in\ngeneral, and that approaches based on this assump-\ntion have important limitations.\nContributions\nWe focus on the recent state-\nof-the-art unsupervised model of Conneau et al.\n(2018).1 Our contributions are: (a) In §2, we show\nthat the monolingual word embeddings used in\nConneau et al. (2018) are not approximately iso-\nmorphic, using the VF2 algorithm (Cordella et al.,\n2001) and we therefore introduce a metric for quan-\ntifying the similarity of word embeddings, based\non Laplacian eigenvalues. (b) In §3, we identify cir-\ncumstances under which the unsupervised bilingual\n1Our motivation for this is that Artetxe et al. (2017) use\nsmall dictionary seeds for supervision, and Barone (2016)\nseems to obtain worse performance than Conneau et al. (2018).\nOur results should extend to Barone (2016) and Zhang et al.\n(2017), which rely on very similar methodology.\narXiv:1805.03620v1  [cs.CL]  9 May 2018\n(a) Top 10 most frequent\nEnglish words\n(b) German translations\nof top 10 most frequent\nEnglish words\n(c) Top 10 most frequent\nEnglish nouns\n(d) German translations\nof top 10 most frequent\nEnglish nouns\nFigure 1: Nearest neighbor graphs.\ndictionary induction (BDI) algorithm proposed in\nConneau et al. (2018) does not lead to good perfor-\nmance. (c) We show that a simple trick, exploiting\na weak supervision signal from words that are iden-\ntical across languages, makes the algorithm much\nmore robust. Our main ﬁnding is that the perfor-\nmance of unsupervised BDI depends heavily on all\nthree factors: the language pair, the comparability\nof the monolingual corpora, and the parameters of\nthe word embedding algorithms.\n2\nHow similar are embeddings across\nlanguages?\nAs mentioned, recent work focused on unsuper-\nvised BDI assumes that monolingual word embed-\nding spaces (or at least the subgraphs formed by\nthe most frequent words) are approximately isomor-\nphic. In this section, we show, by investigating the\nnearest neighbor graphs of word embedding spaces,\nthat word embeddings are far from isomorphic. We\ntherefore introduce a method for computing the\nsimilarity of non-isomorphic graphs. In §4.7, we\ncorrelate our similarity metric with performance on\nunsupervised BDI.\nIsomorphism\nTo motivate our study, we ﬁrst\nestablish that word embeddings are far from\ngraph isomorphic2—even for two closely re-\n2Two graphs that contain the same number of graph ver-\ntices connected in the same way are said to be isomorphic. In\nthe context of weighted graphs such as word embeddings, a\nlated languages, English and German, and us-\ning embeddings induced from comparable corpora\n(Wikipedia) with the same hyper-parameters.\nIf we take the top k most frequent words in En-\nglish, and the top k most frequent words in German,\nand build nearest neighbor graphs for English and\nGerman using the monolingual word embeddings\nused in Conneau et al. (2018), the graphs are of\ncourse very different. This is, among other things,\ndue to German case and the fact that the translates\ninto der, die, and das, but unsupervised alignment\ndoes not have access to this kind of information.\nEven if we consider the top k most frequent En-\nglish words and their translations into German, the\nnearest neighbor graphs are not isomorphic. Fig-\nure 1a-b shows the nearest neighbor graphs of the\ntop 10 most frequent English words on Wikipedia,\nand their German translations.\nWord embeddings are particularly good at cap-\nturing relations between nouns, but even if we con-\nsider the top k most frequent English nouns and\ntheir translations, the graphs are not isomorphic;\nsee Figure 1c-d. We take this as evidence that\nword embeddings are not approximately isomor-\nphic across languages. We also ran graph isomor-\nphism checks on 10 random samples of frequent\nEnglish nouns and their translations into Spanish,\nand only in 1/10 of the samples were the corre-\nsponding nearest neighbor graphs isomorphic.\nEigenvector similarity\nSince the nearest neigh-\nbor graphs are not isomorphic, even for frequent\ntranslation pairs in neighboring languages, we want\nto quantify the potential for unsupervised BDI us-\ning a metric that captures varying degrees of graph\nsimilarity. Eigenvalues are compact representations\nof global properties of graphs, and we introduce\na spectral metric based on Laplacian eigenvalues\n(Shigehalli and Shettar, 2011) that quantiﬁes the\nextent to which the nearest neighbor graphs are\nisospectral. Note that (approximately) isospectral\ngraphs need not be (approximately) isomorphic,\nbut (approximately) isomorphic graphs are always\n(approximately) isospectral (Gordon et al., 1992).\nLet A1 and A2 be the adjacency matrices of the\nnearest neighbor graphs G1 and G2 of our two\nword embeddings, respectively. Let L1 = D1 −A1\nand L2 = D2 −A2 be the Laplacians of the nearest\nneighbor graphs, where D1 and D2 are the corre-\nsponding diagonal matrices of degrees. We now\nweak version of this is to require that the underlying nearest\nneighbor graphs for the most frequent k words are isomorphic.\ncompute the eigensimilarity of the Laplacians of\nthe nearest neighbor graphs, L1 and L2. For each\ngraph, we ﬁnd the smallest k such that the sum of\nthe k largest Laplacian eigenvalues is <90% of the\nLaplacian eigenvalues. We take the smallest k of\nthe two, and use the sum of the squared differences\nbetween the largest k Laplacian eigenvalues ∆as\nour similarity metric.\n∆=\nk\nX\ni=1\n(λ1i −λ2i)2\nwhere k is chosen s.t.\nmin\nj {\nPk\ni=1 λji\nPn\ni=1 λji\n> 0.9}\nNote that ∆= 0 means the graphs are isospec-\ntral, and the metric goes to inﬁnite. Thus, the higher\n∆is, the less similar the graphs (i.e., their Lapla-\ncian spectra). We discuss the correlation between\nunsupervised BDI performance and approximate\nisospectrality or eigenvector similarity in §4.7.\n3\nUnsupervised cross-lingual learning\n3.1\nLearning scenarios\nUnsupervised neural machine translation relies on\nBDI using cross-lingual embeddings (Lample et al.,\n2018a; Artetxe et al., 2018), which in turn relies\non the assumption that word embedding graphs are\napproximately isomorphic. The work of Conneau\net al. (2018), which we focus on here, also makes\nseveral implicit assumptions that may or may not be\nnecessary to achieve such isomorphism, and which\nmay or may not scale to low-resource languages.\nThe algorithms are not intended to be limited to\nlearning scenarios where these assumptions hold,\nbut since they do in the reported experiments, it is\nimportant to see to what extent these assumptions\nare necessary for the algorithms to produce useful\nembeddings or dictionaries.\nWe focus on the work of Conneau et al. (2018),\nwho present a fully unsupervised approach to align-\ning monolingual word embeddings, induced using\nfastText (Bojanowski et al., 2017). We describe the\nlearning algorithm in §3.2. Conneau et al. (2018)\nconsider a speciﬁc set of learning scenarios:\n(a) The authors work with the following lan-\nguages: English-{French, German, Chinese,\nRussian, Spanish}. These languages, except\nFrench, are dependent marking (Table 1).3 We\nevaluate Conneau et al. (2018) on (English to)\nEstonian (ET), Finnish (FI), Greek (EL), Hun-\ngarian (HU), Polish (PL), and Turkish (TR) in\n§4.2, to test whether the selection of languages\nin the original study introduces a bias.\n(b) The monolingual corpora in their experiments\nare comparable; Wikipedia corpora are used,\nexcept for an experiment in which they in-\nclude Google Gigawords. We evaluate across\ndifferent domains, i.e., on all combinations of\nWikipedia, EuroParl, and the EMEA medical\ncorpus, in §4.3. We believe such scenarios are\nmore realistic for low-resource languages.\n(c) The monolingual embedding models are in-\nduced using the same algorithms with the\nsame hyper-parameters. We evaluate Con-\nneau et al. (2018) on pairs of embeddings\ninduced with different hyper-parameters in\n§4.4. While keeping hyper-parameters ﬁxed\nis always possible, it is of practical interest to\nknow whether the unsupervised methods work\non any set of pre-trained word embeddings.\nWe also investigate the sensitivity of unsuper-\nvised BDI to the dimensionality of the monolin-\ngual word embeddings in §4.5. The motivation for\nthis is that dimensionality reduction will alter the\ngeometric shape and remove characteristics of the\nembedding graphs that are important for alignment;\nbut on the other hand, lower dimensionality intro-\nduces regularization, which will make the graphs\nmore similar. Finally, in §4.6, we investigate the\nimpact of different types of query test words on\nperformance, including how performance varies\nacross part-of-speech word classes and on shared\nvocabulary items.\n3.2\nSummary of Conneau et al. (2018)\nWe now introduce the method of Conneau et al.\n(2018).4 The approach builds on existing work on\nlearning a mapping between monolingual word em-\nbeddings (Mikolov et al., 2013b; Xing et al., 2015)\nand consists of the following steps: 1) Monolin-\ngual word embeddings: An off-the-shelf word\nembedding algorithm (Bojanowski et al., 2017) is\nused to learn source and target language spaces X\n3A dependent-marking language marks agreement and\ncase more commonly on dependents than on heads.\n4https://github.com/facebookresearch/\nMUSE\nand Y . 2) Adversarial mapping: A translation\nmatrix W is learned between the spaces X and Y\nusing adversarial techniques (Ganin et al., 2016).\nA discriminator is trained to discriminate samples\nfrom the translated source space WX from the tar-\nget space Y , while W is trained to prevent this.\nThis, again, is motivated by the assumption that\nsource and target language word embeddings are\napproximately isomorphic. 3) Reﬁnement (Pro-\ncrustes analysis): W is used to build a small bilin-\ngual dictionary of frequent words, which is pruned\nsuch that only bidirectional translations are kept\n(Vuli´c and Korhonen, 2016). A new translation\nmatrix W that translates between the spaces X and\nY of these frequent word pairs is then induced by\nsolving the Orthogonal Procrustes problem:\nW ∗= argminW ∥WX −Y ∥F = UV ⊤\ns.t. UΣV ⊤= SVD(Y X⊤)\n(1)\nThis step can be used iteratively by using the new\nmatrix W to create new seed translation pairs. It\nrequires frequent words to serve as reliable anchors\nfor learning a translation matrix. In the experiments\nin Conneau et al. (2018), as well as in ours, the iter-\native Procrustes reﬁnement improves performance\nacross the board. 4) Cross-domain similarity lo-\ncal scaling (CSLS) is used to expand high-density\nareas and condense low-density ones, for more ac-\ncurate nearest neighbor calculation, CSLS reduces\nthe hubness problem in high-dimensional spaces\n(Radovanovi´c et al., 2010; Dinu et al., 2015). It\nrelies on the mean similarity of a source language\nembedding x to its K target language nearest neigh-\nbours (K = 10 suggested) nn1, . . . , nnK:\nmnnT (x) = 1\nK\nK\nX\ni=1\ncos(x, nni)\n(2)\nwhere cos is the cosine similarity. mnnS(y) is\ndeﬁned in an analogous manner for any target lan-\nguage embedding y. CSLS(x, y) is then calcu-\nlated as follows:\n2cos(x, y) −mnnT (x) −mnnS(y)\n(3)\n3.3\nA simple supervised method\nInstead of learning cross-lingual embeddings com-\npletely without supervision, we can extract inex-\npensive supervision signals by harvesting identi-\ncally spelled words as in, e.g. (Artetxe et al., 2017;\nSmith et al., 2017). Speciﬁcally, we use identi-\ncally spelled words that occur in the vocabularies\nof both languages as bilingual seeds, without em-\nploying any additional transliteration or lemma-\ntization/normalization methods. Using this seed\ndictionary, we then run the reﬁnement step using\nProcrustes analysis of Conneau et al. (2018).\n4\nExperiments\nIn the following experiments, we investigate the\nrobustness of unsupervised cross-lingual word\nembedding learning, varying the language pairs,\nmonolingual corpora, hyper-parameters, etc., to\nobtain a better understanding of when and why\nunsupervised BDI works.\nTask: Bilingual dictionary induction\nAfter the\nshared cross-lingual space is induced, given a list\nof N source language words xu,1, . . . , xu,N, the\ntask is to ﬁnd a target language word t for each\nquery word xu relying on the representations in\nthe space. ti is the target language word closest\nto the source language word xu,i in the induced\ncross-lingual space, also known as the cross-lingual\nnearest neighbor. The set of learned N (xu,i, ti)\npairs is then run against a gold standard dictionary.\nWe use bilingual dictionaries compiled by Con-\nneau et al. (2018) as gold standard, and adopt their\nevaluation procedure: each test set in each language\nconsists of 1500 gold translation pairs. We rely on\nCSLS for retrieving the nearest neighbors, as it con-\nsistently outperformed the cosine similarity in all\nour experiments. Following a standard evaluation\npractice (Vuli´c and Moens, 2013; Mikolov et al.,\n2013b; Conneau et al., 2018), we report Precision\nat 1 scores (P@1): how many times one of the\ncorrect translations of a source word w is retrieved\nas the nearest neighbor of w in the target language.\n4.1\nExperimental setup\nOur default experimental setup closely follows the\nsetup of Conneau et al. (2018). For each language\nwe induce monolingual word embeddings for all\nlanguages from their respective tokenized and low-\nercased Polyglot Wikipedias (Al-Rfou et al., 2013)\nusing fastText (Bojanowski et al., 2017). Only\nwords with more than 5 occurrences are retained\nfor training. Our fastText setup relies on skip-gram\nwith negative sampling (Mikolov et al., 2013a) with\nstandard hyper-parameters: bag-of-words contexts\nwith the window size 2, 15 negative samples, sub-\nsampling rate 10−4, and character n-gram length\nMarking\nType\n# Cases\nEnglish (EN)\ndependent\nisolating\nNone\nFrench (FR)\nmixed\nfusional\nNone\nGerman (DE)\ndependent\nfusional\n4\nChinese (ZH)\ndependent\nisolating\nNone\nRussian (RU)\ndependent\nfusional\n6–7\nSpanish (ES)\ndependent\nfusional\nNone\nEstonian (ET)\nmixed\nagglutinative\n10+\nFinnish (FI)\nmixed\nagglutinative\n10+\nGreek (EL)\ndouble\nfusional\n3\nHungarian (HU)\ndependent\nagglutinative\n10+\nPolish (PL)\ndependent\nfusional\n6–7\nTurkish (TR)\ndependent\nagglutinative\n6–7\nTable 1: Languages in Conneau et al. (2018) and\nin our experiments (lower half)\nUnsupervised\nSupervised\nSimilarity\n(Adversarial)\n(Identical)\n(Eigenvectors)\nEN-ES\n81.89\n82.62\n2.07\nEN-ET\n00.00\n31.45\n6.61\nEN-FI\n00.09\n28.01\n7.33\nEN-EL\n00.07\n42.96\n5.01\nEN-HU\n45.06\n46.56\n3.27\nEN-PL\n46.83\n52.63\n2.56\nEN-TR\n32.71\n39.22\n3.14\nET-FI\n29.62\n24.35\n3.98\nTable 2: Bilingual dictionary induction scores\n(P@1×100%) using a) the unsupervised method\nwith adversarial training; b) the supervised method\nwith a bilingual seed dictionary consisting of iden-\ntical words (shared between the two languages).\nThe third columns lists eigenvector similarities be-\ntween 10 randomly sampled source language near-\nest neighbor subgraphs of 10 nodes and the sub-\ngraphs of their translations, all from the benchmark\ndictionaries in Conneau et al. (2018).\n3-6. All embeddings are 300-dimensional.\nAs we analyze the impact of various modeling\nassumptions in the following sections (e.g., domain\ndifferences, algorithm choices, hyper-parameters),\nwe also train monolingual word embeddings us-\ning other corpora and different hyper-parameter\nchoices. Quick summaries of each experimental\nsetup are provided in the respective subsections.\n4.2\nImpact of language similarity\nConneau et al. (2018) present results for several\ntarget languages: Spanish, French, German, Rus-\nsian, Chinese, and Esperanto. All languages but Es-\nperanto are isolating or exclusively concatenating\nlanguages from a morphological point of view. All\nlanguages but French are dependent-marking. Ta-\nble 1 lists three important morphological properties\nof the languages involved in their/our experiments.\nAgglutinative languages with mixed or double\nmarking show more morphological variance with\ncontent words, and we speculate whether unsuper-\nvised BDI is challenged by this kind of morpholog-\nical complexity. To evaluate this, we experiment\nwith Estonian and Finnish, and we include Greek,\nHungarian, Polish, and Turkish to see how their\napproach fares on combinations of these two mor-\nphological traits.\nWe show results in the left column of Table 2.\nThe results are quite dramatic.\nThe approach\nachieves impressive performance for Spanish, one\nof the languages Conneau et al. (2018) include in\ntheir paper. For the languages we add here, perfor-\nmance is less impressive. For the languages with\ndependent marking (Hungarian, Polish, and Turk-\nish), P@1 scores are still reasonable, with Turkish\nbeing slightly lower (0.327) than the others. How-\never, for Estonian and Finnish, the method fails\ncompletely. Only in less than 1/1000 cases does a\nnearest neighbor search in the induced embeddings\nreturn a correct translation of a query word.5\nThe sizes of Wikipedias naturally vary across\nlanguages: e.g., fastText trains on approximately\n16M sentences and 363M word tokens for Spanish,\nwhile it trains on 1M sentences and 12M words for\nFinnish. However, the difference in performance\ncannot be explained by the difference in training\ndata sizes. To verify that near-zero performance in\nFinnish is not a result of insufﬁcient training data,\nwe have conducted another experiment using the\nlarge Finnish WaC corpus (Ljubeši´c et al., 2016)\ncontaining 1.7B words in total (this is similar in\nsize to the English Polyglot Wikipedia). However,\neven with this large Finnish corpus, the model does\nnot induce anything useful: P@1 equals 0.0.\nWe note that while languages with mixed mark-\ning may be harder to align, it seems unsupervised\nBDI is possible between similar, mixed marking\nlanguages. So while unsupervised learning fails\nfor English-Finnish and English-Estonian, perfor-\nmance is reasonable and stable for the more similar\nEstonian-Finnish pair (Table 2). In general, un-\nsupervised BDI, using the approach in Conneau\net al. (2018), seems challenged when pairing En-\n5We note, though, that varying our random seed, perfor-\nmance for Estonian, Finnish, and Greek is sometimes (approx-\nimately 1 out of 10 runs) on par with Turkish. Detecting main\ncauses and remedies for the inherent instability of adversarial\ntraining is one the most important avenues for future research.\nglish with languages that are not isolating and do\nnot have dependent marking.6\nThe promise of zero-supervision models is that\nwe can learn cross-lingual embeddings even for\nlow-resource languages. On the other hand, a simi-\nlar distribution of embeddings requires languages\nto be similar. This raises the question whether we\nneed fully unsupervised methods at all. In fact, our\nsupervised method that relies on very naive supervi-\nsion in the form of identically spelled words leads\nto competitive performance for similar language\npairs and better results for dissimilar pairs. The fact\nthat we can reach competitive and more robust per-\nformance with such a simple heuristic questions the\ntrue applicability of fully unsupervised approaches\nand suggests that it might often be better to rely on\navailable weak supervision.\n4.3\nImpact of domain differences\nMonolingual word embeddings used in Conneau\net al. (2018) are induced from Wikipedia, a near-\nparallel corpus. In order to assess the sensitivity of\nunsupervised BDI to the comparability and domain\nsimilarity of the monolingual corpora, we repli-\ncate the experiments in Conneau et al. (2018) using\ncombinations of word embeddings extracted from\nthree different domains: 1) parliamentary proceed-\nings from EuroParl.v7 (Koehn, 2005), 2) Wikipedia\n(Al-Rfou et al., 2013), and 3) the EMEA corpus in\nthe medical domain (Tiedemann, 2009). We report\nexperiments with three language pairs: English-\n{Spanish, Finnish, Hungarian}.\nTo control for the corpus size, we restrict each\ncorpus in each language to 1.1M sentences in to-\ntal (i.e., the number of sentences in the smallest,\nEMEA corpus). 300-dim fastText vectors are in-\nduced as in §4.1, retaining all words with more than\n5 occurrences in the training data. For each pair\nof monolingual corpora, we compute their domain\n(dis)similarity by calculating the Jensen-Shannon\ndivergence (El-Gamal, 1991), based on term distri-\nbutions.7 The domain similarities are displayed in\nFigures 2a–c.8\nWe show the results of unsupervised BDI in Fig-\nures 2g–i. For Spanish, we see good performance\nin all three cases where the English and Spanish\n6One exception here is French, which they include in their\npaper, but French arguably has a relatively simple morphology.\n7In order to get comparable term distributions, we translate\nthe source language to the target language using the bilingual\ndictionaries provided by Conneau et al. (2018).\n8We also computed A-distances (Blitzer et al., 2007) and\nconﬁrmed that trends were similar.\ncorpora are from the same domain. When the two\ncorpora are from different domains, performance\nis close to zero. For Finnish and Hungarian, perfor-\nmance is always poor, suggesting that more data\nis needed, even when domains are similar. This is\nin sharp contrast with the results of our minimally\nsupervised approach (Figures 2d–f) based on iden-\ntical words, which achieves decent performance in\nmany set-ups.\nWe also observe a strong decrease in P@1 for\nEnglish-Spanish (from 81.19% to 46.52%) when\nusing the smaller Wikipedia corpora. This result\nindicates the importance of procuring large mono-\nlingual corpora from similar domains in order to\nenable unsupervised dictionary induction. How-\never, resource-lean languages, for which the unsu-\npervised method was designed in the ﬁrst place,\ncannot be guaranteed to have as large monolingual\ntraining corpora as available for English, Spanish\nor other major resource-rich languages.\n4.4\nImpact of hyper-parameters\nConneau et al. (2018) use the same hyper-\nparameters for inducing embeddings for all lan-\nguages. This is of course always practically possi-\nble, but we are interested in seeing whether their ap-\nproach works on pre-trained embeddings induced\nwith possibly very different hyper-parameters. We\nfocus on two hyper-parameters: context window-\nsize (win) and the parameter controlling the num-\nber of n-gram features in the fastText model (chn),\nwhile at the same time varying the underlying algo-\nrithm: skip-gram vs. cbow. The results for English-\nSpanish are listed in Table 3.\nThe small variations in the hyper-parameters\nwith the same underlying algorithm (i.e., using skip-\ngram or cbow for both EN and ES) yield only slight\ndrops in the ﬁnal scores. Still, the best scores are\nobtained with the same conﬁguration on both sides.\nOur main ﬁnding here is that unsupervised BDI\nfails (even) for EN-ES when the two monolingual\nembedding spaces are induced by two different al-\ngorithms (see the results of the entire Spanish cbow\ncolumn).9 In sum, this means that the unsuper-\nvised approach is unlikely to work on pre-trained\nword embeddings unless they are induced on same-\n9We also checked if this result might be due to a lower-\nquality monolingual ES space. However, monolingual word\nsimilarity scores on available datasets in Spanish show perfor-\nmance comparable to that of Spanish skip-gram vectors: e.g.,\nSpearman’s ρ correlation is ≈0.7 on the ES evaluation set\nfrom SemEval-2017 Task 2 (Camacho-Collados et al., 2017).\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nJensen-Shannon Similarity\nEP\nWiki\nEMEA\n(a) en-es: domain similarity\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nJensen-Shannon Similarity\nEP\nWiki\nEMEA\n(b) en-ﬁ: domain similarity\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nJensen-Shannon Similarity\nEP\nWiki\nEMEA\n(c) en-hu: domain similarity\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0\n10\n20\n30\n40\n50\n60\nBLI: P@1\n64.09\n25.48\n4.84\n25.17\n46.52\n6.63\n9.42\n9.63\n49.24\n(d) en-es: identical words\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0\n10\n20\n30\n40\n50\n60\nBLI: P@1\n28.63\n10.14\n2.31\n5.84\n11.08\n2.27\n4.97\n5.96\n8.11\n(e) en-ﬁ: identical words\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0\n10\n20\n30\n40\n50\n60\nBLI: P@1\n26.99\n9.22\n2.26\n7.07\n14.79\n1.58\n3.74\n3.45\n15.56\n(f) en-hu: identical words\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0\n10\n20\n30\n40\n50\n60\nBLI: P@1\n61.01\n0.13\n0.0\n0.11\n41.38\n0.0\n0.0\n0.08\n49.43\n(g) en-es: fully unsupervised BLI\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0\n10\n20\n30\n40\n50\n60\nBLI: P@1\n0.72\n0.10\n0.0\n0.0\n0.0\n0.16\n0.0\n0.0\n0.96\n(h) en-ﬁ: fully unsupervised BLI\nEN:EP\nEN:Wiki\nEN:EMEA\nTraining Corpus (English)\n0\n10\n20\n30\n40\n50\n60\nBLI: P@1\n0.24\n0.11\n0.0\n0.11\n6.68\n0.0\n0.0\n0.0\n0.45\n(i) en-hu: fully unsupervised BLI\nFigure 2: Inﬂuence of language-pair and domain similarity on BLI performance, with three language pairs\n(en-es/ﬁ/hu). Top row, (a)-(c): Domain similarity (higher is more similar) computed as dsim = 1 −JS,\nwhere JS is Jensen-Shannon divergence; Middle row, (d)-(f): baseline BLI model which learns a linear\nmapping between two monolingual spaces based on a set of identical (i.e., shared) words; Bottom row,\n(g)-(i): fully unsupervised BLI model relying on the distribution-level alignment and adversarial training.\nBoth BLI models apply the Procrustes analysis and use CSLS to retrieve nearest neighbours.\nor comparable-domain, reasonably-sized training\ndata using the same underlying algorithm.\n4.5\nImpact of dimensionality\nWe also perform an experiment on 40-dimensional\nmonolingual word embeddings. This leads to re-\nduced expressivity, and can potentially make the\ngeometric shapes of embedding spaces harder to\nalign; on the other hand, reduced dimensionality\nmay also lead to less overﬁtting. We generally\nsee worse performance (P@1 is 50.33 for Spanish,\n21.81 for Hungarian, 20.11 for Polish, and 22.03\nfor Turkish) – but, very interestingly, we obtain\nbetter performance for Estonian (13.53), Finnish\n(15.33), and Greek (24.17) than we did with 300 di-\nmensions. We hypothesize this indicates monolin-\ngual word embedding algorithms over-ﬁt to some\nof the rarer peculiarities of these languages.\nEnglish\n(skipgram, win=2, chn=3-6)\nSpanish\nSpanish\n(skipgram)\n(cbow)\n==\n81.89\n00.00\n̸= win=10\n81.28\n00.07\n̸= chn=2-7\n80.74\n00.00\n̸= win=10, chn=2-7\n80.15\n00.13\nTable 3: Varying the underlying fastText algorithm\nand hyper-parameters. The ﬁrst column lists differ-\nences in training conﬁgurations between English\nand Spanish monolingual embeddings.\nen-es\nen-hu\nen-ﬁ\nNoun\n80.94\n26.87\n00.00\nVerb\n66.05\n25.44\n00.00\nAdjective\n85.53\n53.28\n00.00\nAdverb\n80.00\n51.57\n00.00\nOther\n73.00\n53.40\n00.00\nTable 4: P@1 × 100% scores for query words with\ndifferent parts-of-speech.\n4.6\nImpact of evaluation procedure\nBDI models are evaluated on a held-out set of query\nwords. Here, we analyze the performance of the\nunsupervised approach across different parts-of-\nspeech, frequency bins, and with respect to query\nwords that have orthographically identical coun-\nterparts in the target language with the same or a\ndifferent meaning.\nPart-of-speech\nWe show the impact of the part-\nof-speech of the query words in Table 4; again on a\nrepresentative subset of our languages. The results\nindicate that performance on verbs is lowest across\nthe board. This is consistent with research on dis-\ntributional semantics and verb meaning (Schwartz\net al., 2015; Gerz et al., 2016).\nFrequency\nWe also investigate the impact of the\nfrequency of query words. We calculate the word\nfrequency of English words based on Google’s Tril-\nlion Word Corpus: query words are divided in\ngroups based on their rank – i.e., the ﬁrst group\ncontains the top 100 most frequent words, the sec-\nond one contains the 101th-1000th most frequent\nwords, etc. – and plot performance (P@1) relative\nto rank in Figure 3. For EN-FI, P@1 was 0 across\nall frequency ranks. The plot shows sensitivity to\nfrequency for HU, but less so for ES.\nHomographs\nSince we use identical word forms\n(homographs) for supervision, we investigated\n20\n40\n60\n80\n100\n1000\n10000\nP@1×100%\nWord frequency rank\nen-es\nen-hu\nFigure 3: P@1 scores for EN-ES and EN-HU for\nqueries with different frequency ranks.\nSpelling\nMeaning\nen-es\nen-hu\nen-ﬁ\nSame\nSame\n45.94\n18.07\n00.00\nSame\nDiff\n39.66\n29.97\n00.00\nDiff\nDiff\n62.42\n34.45\n00.00\nTable 5: Scores (P@1 × 100%) for query words\nwith same and different spellings and meanings.\nwhether these are representative or harder to align\nthan other words. Table 5 lists performance for\nthree sets of query words: (a) source words that\nhave homographs (words that are spelled the same\nway) with the same meaning (homonyms) in the\ntarget language, e.g., many proper names; (b)\nsource words that have homographs that are not\nhomonyms in the target language, e.g., many short\nwords; and (c) other words. Somewhat surpris-\ningly, words which have translations that are ho-\nmographs, are associated with lower precision than\nother words. This is probably due to loan words\nand proper names, but note that using homographs\nas supervision for alignment, we achieve high pre-\ncision for this part of the vocabulary for free.\n4.7\nEvaluating eigenvector similarity\nFinally, in order to get a better understanding of\nthe limitations of unsupervised BDI, we correlate\nthe graph similarity metric described in §2 (right\ncolumn of Table 2) with performance across lan-\nguages (left column). Since we already established\nthat the monolingual word embeddings are far from\nisomorphic—in contrast with the intuitions motivat-\ning previous work (Mikolov et al., 2013b; Barone,\n2016; Zhang et al., 2017; Conneau et al., 2018)—\nwe would like to establish another diagnostic met-\nric that identiﬁes embedding spaces for which the\napproach in Conneau et al. (2018) is likely to work.\nDifferences in morphology, domain, or embedding\nparameters seem to be predictive of poor perfor-\nmance, but a metric that is independent of linguistic\nFigure 4: Strong correlation (ρ = 0.89) between\nBDI performance (x) and graph similarity (y)\ncategorizations and the characteristics of the mono-\nlingual corpora would be more widely applicable.\nWe plot the values in Table 2 in Figure 4. Recall\nthat our graph similarity metric returns a value in\nthe half-open interval [0, ∞). The correlation be-\ntween BDI performance and graph similarity is\nstrong (ρ ∼0.89).\n5\nRelated work\nCross-lingual word embeddings\nCross-lingual\nword embedding models typically, unlike Conneau\net al. (2018), require aligned words, sentences, or\ndocuments (Levy et al., 2017). Most approaches\nbased on word alignments learn an explicit map-\nping between the two embedding spaces (Mikolov\net al., 2013b; Xing et al., 2015). Recent approaches\ntry to minimize the amount of supervision needed\n(Vuli´c and Korhonen, 2016; Artetxe et al., 2017;\nSmith et al., 2017). See Upadhyay et al. (2016) and\nRuder et al. (2018) for surveys.\nUnsupervised cross-lingual learning\nHaghighi\net al. (2008) were ﬁrst to explore unsupervised\nBDI, using features such as context counts and or-\nthographic substrings, and canonical correlation\nanalysis. Recent approaches use adversarial learn-\ning (Goodfellow et al., 2014) and employ a discrim-\ninator, trained to distinguish between the translated\nsource and the target language space, and a gener-\nator learning a translation matrix (Barone, 2016).\nZhang et al. (2017), in addition, use different forms\nof regularization for convergence, while Conneau\net al. (2018) uses additional steps to reﬁne the in-\nduced embedding space.\nUnsupervised machine translation\nResearch\non unsupervised machine translation (Lample et al.,\n2018a; Artetxe et al., 2018; Lample et al., 2018b)\nhas generated a lot of interest recently with a\npromise to support the construction of MT systems\nfor and between resource-poor languages. All unsu-\npervised NMT methods critically rely on accurate\nunsupervised BDI and back-translation. Models\nare trained to reconstruct a corrupted version of\nthe source sentence and to translate its translated\nversion back to the source language. Since the cru-\ncial input to these systems are indeed cross-lingual\nword embedding spaces induced in an unsupervised\nfashion, in this paper we also implicitly investigate\none core limitation of such unsupervised MT tech-\nniques.\n6\nConclusion\nWe investigated when unsupervised BDI (Conneau\net al., 2018) is possible and found that differences\nin morphology, domains or word embedding algo-\nrithms may challenge this approach. Further, we\nfound eigenvector similarity of sampled nearest\nneighbor subgraphs to be predictive of unsuper-\nvised BDI performance. We hope that this work\nwill guide further developments in this new and\nexciting ﬁeld.\nAcknowledgments\nWe thank the anonymous reviewers, as well as Hin-\nrich Schütze and Yova Kementchedjhieva, for their\nvaluable feedback. Anders is supported by the\nERC Starting Grant LOWLANDS No. 313695 and\na Google Focused Research Award. Sebastian is\nsupported by Irish Research Council Grant Num-\nber EBPPG/2014/30 and Science Foundation Ire-\nland Grant Number SFI/12/RC/2289. Ivan is sup-\nported by the ERC Consolidator Grant LEXICAL\nNo. 648909.\nReferences\nRami Al-Rfou, Bryan Perozzi, and Steven Skiena.\n2013.\nPolyglot: Distributed word representations\nfor multilingual NLP.\nIn Proceedings of CoNLL,\npages 183–192.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data.\nIn Proceedings of ACL, pages\n451–462.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018.\nUnsupervised neural ma-\nchine translation. In Proceedings of ICLR (Confer-\nence Track).\nAntonio Valerio Miceli Barone. 2016. Towards cross-\nlingual distributed representations without parallel\ntext trained with adversarial autoencoders. Proceed-\nings of the 1st Workshop on Representation Learning\nfor NLP, pages 121–126.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiﬁcation. In\nProceedings of ACL, 1, pages 440–447.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:125–136.\nJose Camacho-Collados, Mohammad Taher Pilehvar,\nNigel Collier, and Roberto Navigli. 2017. SemEval-\n2017 Task 2: Multilingual and cross-lingual seman-\ntic word similarity. In Proceedings of SEMEVAL,\npages 15–26.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. Proceedings\nof ICLR.\nL. P. Cordella, P. Foggia, C. Sansone, and M. Vento.\n2001.\nAn improved algorithm for matching large\ngraphs. Proceedings of the 3rd IAPR TC-15 Work-\nshop on Graphbased Representations in Pattern\nRecognition, 17:1–35.\nGeorgiana Dinu, Angeliki Lazaridou, and Marco Ba-\nroni. 2015. Improving zero-shot learning by mitigat-\ning the hubness problem. In Proceedings of ICLR\n(Workshop Papers).\nM. A El-Gamal. 1991.\nThe role of priors in active\nBayesian learning in the sequential statistical deci-\nsion framework. In Maximum Entropy and Bayesian\nMethods, pages 33–38. Springer Netherlands.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Francois Lavi-\nolette, Mario Marchand, and Victor Lempitsky.\n2016.\nDomain-adversarial training of neural net-\nworks.\nJournal of Machine Learning Research,\n17:1–35.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and\nAnna Korhonen. 2016.\nSimVerb-3500: A large-\nscale evaluation set of verb similarity. In Proceed-\nings of EMNLP, pages 2173–2182.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Proceedings of NIPS, pages 2672–\n2680.\nCarolyn Gordon, David L. Webb, and Scott Wolpert.\n1992. One cannot hear the shape of a drum. Bulletin\nof the American Mathematical Society.\nAria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,\nand Dan Klein. 2008. Learning bilingual lexicons\nfrom monolingual corpora. In Proceedings of ACL,\npages 771–779.\nAlexandre Klementiev, Ivan Titov, and Binod Bhattarai.\n2012. Inducing crosslingual distributed representa-\ntions of words. In Proceedings of COLING, pages\n1459–1474.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of the\n10th Machine Translation Summit (MT SUMMIT),\npages 79–86.\nGuillaume\nLample,\nLudovic\nDenoyer,\nand\nMarc’Aurelio Ranzato. 2018a.\nUnsupervised\nmachine translation using monolingual corpora\nonly. In Proceedings of ICLR (Conference Papers).\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. CoRR, abs/1804.07755.\nOmer Levy, Anders Søgaard, and Yoav Goldberg. 2017.\nA strong baseline for learning cross-lingual word\nembeddings from sentence alignments. In Proceed-\nings of EACL, pages 765–774.\nNikola Ljubeši´c, Tommi Pirinen, and Antonio Toral.\n2016. Finnish Web corpus ﬁWaC 1.0. Slovenian\nlanguage resource repository CLARIN.SI.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Distributed representations of words\nand phrases and their compositionality. In Proceed-\nings of NIPS, pages 3111–3119.\nTomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b.\nExploiting similarities among languages for ma-\nchine translation.\nMilos Radovanovi´c, Alexandros Nanopoulos, and Mir-\njana Ivanovic. 2010. Hubs in space: Popular nearest\nneighbors in high-dimensional data. Journal of Ma-\nchine Learning Research, 11:2487–2531.\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard.\n2018. A survey of cross-lingual word embedding\nmodels. Journal of Artiﬁcial Intelligence Research.\nRoy Schwartz, Roi Reichart, and Ari Rappoport. 2015.\nSymmetric pattern based word embeddings for im-\nproved word similarity prediction. In Proceedings\nof CoNLL, pages 258–267.\nVijayalaxmi Shigehalli and Vidya Shettar. 2011. Spec-\ntral technique using normalized adjacency matrices\nfor graph matching. International Journal of Com-\nputational Science and Mathematics, 3:371–378.\nSamuel L. Smith, David H. P. Turban, Steven Hamblin,\nand Nils Y. Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In Proceedings of ICLR (Conference Pa-\npers).\nJörg Tiedemann. 2009. News from OPUS - A collec-\ntion of multilingual parallel corpora with tools and\ninterfaces. In Proceedings of RANLP, pages 237–\n248.\nShyam Upadhyay, Manaal Faruqui, Chris Dyer, and\nDan Roth. 2016. Cross-lingual models of word em-\nbeddings: An empirical comparison. In Proceedings\nof ACL, pages 1661–1670.\nIvan Vuli´c and Anna Korhonen. 2016. On the role of\nseed lexicons in learning bilingual word embeddings.\nIn Proceedings of ACL, pages 247–257.\nIvan Vuli´c and Marie-Francine Moens. 2013. A study\non bootstrapping bilingual vector spaces from non-\nparallel data (and nothing else). In Proceedings of\nEMNLP, pages 1613–1624.\nChao Xing, Chao Liu, Dong Wang, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. In Proceedings\nof NAACL-HLT, pages 1005–1010.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017.\nAdversarial training for unsupervised\nbilingual lexicon induction. In Proceedings of ACL,\npages 1959–1970.\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-05-09",
  "updated": "2018-05-09"
}