{
  "id": "http://arxiv.org/abs/1805.04419v1",
  "title": "Deep Hierarchical Reinforcement Learning Algorithm in Partially Observable Markov Decision Processes",
  "authors": [
    "Le Pham Tuyen",
    "Ngo Anh Vien",
    "Abu Layek",
    "TaeChoong Chung"
  ],
  "abstract": "In recent years, reinforcement learning has achieved many remarkable\nsuccesses due to the growing adoption of deep learning techniques and the rapid\ngrowth in computing power. Nevertheless, it is well-known that flat\nreinforcement learning algorithms are often not able to learn well and\ndata-efficient in tasks having hierarchical structures, e.g. consisting of\nmultiple subtasks. Hierarchical reinforcement learning is a principled approach\nthat is able to tackle these challenging tasks. On the other hand, many\nreal-world tasks usually have only partial observability in which state\nmeasurements are often imperfect and partially observable. The problems of RL\nin such settings can be formulated as a partially observable Markov decision\nprocess (POMDP). In this paper, we study hierarchical RL in POMDP in which the\ntasks have only partial observability and possess hierarchical properties. We\npropose a hierarchical deep reinforcement learning approach for learning in\nhierarchical POMDP. The deep hierarchical RL algorithm is proposed to apply to\nboth MDP and POMDP learning. We evaluate the proposed algorithm on various\nchallenging hierarchical POMDP.",
  "text": "1\nDeep Hierarchical Reinforcement Learning\nAlgorithm in Partially Observable Markov Decision\nProcesses)\nLe Pham Tuyen∗, Ngo Anh Vien†, Md. Abu Layek∗, TaeChoong Chung ∗\u0015\n∗Artiﬁcial Intelligence Lab, Computer Science and Engineering Department, Kyung Hee University, Yongin,\nGyeonggi 446-701, South Korea\n{tuyenple, layek, tcchung}@khu.ac.kr\n†EEECS/ECIT, Queens University Belfast, Belfast, UK\nv.ngo@qub.ac.uk\nAbstract—In recent years, reinforcement learning has achieved\nmany remarkable successes due to the growing adoption of deep\nlearning techniques and the rapid growth in computing power.\nNevertheless, it is well-known that ﬂat reinforcement learning\nalgorithms are often not able to learn well and data-efﬁcient in\ntasks having hierarchical structures, e.g. consisting of multiple\nsubtasks. Hierarchical reinforcement learning is a principled\napproach that is able to tackle these challenging tasks. On the\nother hand, many real-world tasks usually have only partial\nobservability in which state measurements are often imperfect\nand partially observable. The problems of RL in such settings\ncan be formulated as a partially observable Markov decision\nprocess (POMDP). In this paper, we study hierarchical RL in\nPOMDP in which the tasks have only partial observability and\npossess hierarchical properties. We propose a hierarchical deep\nreinforcement learning approach for learning in hierarchical\nPOMDP. The deep hierarchical RL algorithm is proposed to\napply to both MDP and POMDP learning. We evaluate the\nproposed algorithm on various challenging hierarchical POMDP.\nIndex Terms—Hierarchical Deep Reinforcement Learning,\nPartially Observable MDP (POMDP), Semi-MDP, Partially Ob-\nservable Semi-MDP (POSMDP)\nI. INTRODUCTION\nReinforcement Learning (RL) [1] is a subﬁeld of machine\nlearning focused on learning a policy in order to maximize\ntotal cumulative reward in an unknown environment. RL has\nbeen applied to many areas such as robotics [2][3][4][5],\neconomics [6][7][8], computer games [9][10][11] and other\napplications [12][13][14]. RL is divided into two approaches:\nvalue-based approach and policy-based approach [15]. A typ-\nical value-based approach tries to obtain an optimal policy\nby ﬁnding optimal value functions. The value functions are\nupdated using the immediate reward and the discounted value\nof the next state. Some methods based on this approach\nare Q-learning, SARSA, and TD-learning [1]. In contrast,\nthe policy-based approach directly learns a parameterized\npolicy that maximizes the cumulative discounted reward. Some\ntechniques used for searching optimal parameters of the policy\n\u0015 Corresponding author\n) This work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may no\nlonger be accessible.\nare policy gradient [16], expectation maximization [17], and\nevolutionary algorithm [18]. In addition, a hybrid approach\nof value-based approach and policy-based approach is called\nactor-critic [19]. Recently, RL algorithms integrated with a\ndeep neural network (DNN) [10] achieved good performance,\neven better than human performance at some tasks such as\nAtari games [10] and Go game [11]. However, obtaining good\nperformance on a task consisting of multiple subtasks, such as\nMazebase [20] and Montezuma’s Revenge [9], is still a major\nchallenge for ﬂat RL algorithms. Hierarchical reinforcement\nlearning (HRL) [21] was developed to tackle these domains.\nHRL decomposes a RL problem into a hierarchy of subprob-\nlems (subtasks) and each of which can itself be a RL problem.\nIdentical subtasks can be gathered into one group and are con-\ntrolled by the same policy. As a result, hierarchical decomposi-\ntion represents the problem in a compact form and reduces the\ncomputational complexity. Various approaches to decompose\nthe RL problem have been proposed, such as options [21],\nHAMs [22][23], MAXQ [24], Bayesian HRL [25, 26, 27] and\nsome other advanced techniques [28]. All approaches com-\nmonly consider semi-Markov decision process [21] as a base\ntheory. Recently, many studies have combined HRL with deep\nneural networks in different ways to improve performance\non hierarchical tasks [29][30][31][32][33][34][35]. Bacon et\nal. [30] proposed an option-critic architecture, which has a\nﬁxed number of intra-options, each of which is followed by a\n“deep” policy. At each time step, only one option is activated\nand is selected by another policy that is called “policy over\noptions”. DeepMind lab [33] also proposed a deep hierarchical\nframework inspired by a classical framework called feudal\nreinforcement learning [36]. Similarly, Kulkarni et al. [37]\nproposed a deep hierarchical framework of two levels in which\nthe high-level controller produces a subgoal and the low-level\ncontroller performs primitive actions to obtain the subgoal.\nThis framework is useful to solve a problem with multiple\nsubgoals such as Montezuma’s Revenge [9] and games in\nMazebase [20]. Other studies have tried to tackle more chal-\nlenging problems in HRL such as discovering subgoals [38]\nand adaptively ﬁnding a number of options [39].\nThough many studies have made great efforts in this topic,\nmost of them rely on an assumption of full observability, where\narXiv:1805.04419v1  [cs.AI]  11 May 2018\n2\na learning agent can observe environment states fully. In other\nwords, the environment is represented as a Markov decision\nprocess (MDP). This assumption does not reﬂect the nature\nof real-world applications in which the agent only observes a\npartial information of the environment states. Therefore, the\nenvironment, in this case, is represented as a POMDP. In order\nfor the agent to learn in such a POMDP environment, more\nadvanced techniques are required in order to have a good\nprediction over environment responses, such as maintaining a\nbelief distribution over unobservable states; alternatively using\na recurrent neural network (RNN) [40] to summarize an ob-\nservable history. Recently, there have been some studies using\ndeep RNNs to deal with learning in POMDP environment\n[41][42].\nIn this paper, we develop a deep HRL algorithm for POMDP\nproblems inspired by the deep HRL framework [37]. The\nagent in this framework makes decisions through a hierarchical\npolicy of two levels. The top policy determines the subgoal to\nbe achieved, while the lower-level policy performs primitive\nactions to achieve the selected subgoal. To learn in POMDP,\nwe represent all policies using RNNs. The RNN in lower-\nlevel policies constructs an internal state to remember the\nwhole states observed during the course of interaction with the\nenvironment. The top policy is a RNN to encode a sequence\nof observations during the execution of a selected subgoal.\nWe highlight our contributions as follows. First, we exploit\nthe advantages of RNNs to integrate with hierarchical RL in\norder to handle learning on challenging and complex tasks in\nPOMDP. Second, this integration leads to a new hierarchical\nPOMDP learning framework that is more scalable and data-\nefﬁcient.\nThe rest of the paper is organized as follows. Section\nII reviews the underlying knowledge such as semi-Markov\ndecision process, partially observable Markov decision pro-\ncess and deep reinforcement learning. Our contributions are\ndescribed in Section III, which consists of two parts. The\ndeep model part describes all elements in our algorithm and\nthe learning algorithm part summarizes the algorithm in a\ngeneralized form. The usefulness of the proposed algorithms\nis demonstrated through POMDP benchmarks in Section IV.\nFinally, the conclusion is established in Section V.\nII. BACKGROUND\nIn this section, we brieﬂy review all underlying theories\nthat the content of this paper relies on: Markov decision\nprocess, semi-Markov decision process for hierarchical RL,\npartially observable Markov decision process for RL, and deep\nreinforcement learning.\nA. Markov Decision Process\nA discrete MDP models a sequence of decisions of a learn-\ning agent interacting with an environment at some discrete\ntime scale, t = 1, 2, . . . . Formally, an MDP consists of a\ntuple of ﬁve elements {S, A, P, r, γ}, where S is a discrete\nstate space, A is a discrete action space , P(st+1, st, at) =\np(st+1|st, at) is a transition function that measures the proba-\nbility of obtaining a next state st+1 given a current state-action\npair (st, at), r (st, at) deﬁnes an immediate reward achieved\nat each state-action pair, and γ ∈(0, 1) denotes a discount\nfactor. MDP relies on the Markov property that a next state\nonly depends on the current state-action pair:\np(st+1|{s1, a1, s2, a2, ..., st, at}) = p(st+1|st, at).\nA policy of a RL algorithm is denoted by π which is the\nprobability of taking an action a given a state s: π = P(a|s),\nthe goal of a RL algorithm is to ﬁnd an optimal policy π∗\nin order to maximize the expected total discounted reward as\nfollows\nJ(π) = E\n\u0014 ∞\nX\nt=0\nγtr(st, at)\n\u0015\n.\n(1)\nB. Semi-Markov Decision Process for Hierarchical Reinforce-\nment Learning\nLearning over different levels of policy is the main challenge\nfor hierarchical tasks. The semi-Markov decision process\n(SMDP) [21], which is as an extension of MDP, was devel-\noped to deal with this challenge. In this theory, the concept\n“options” is introduced as a type of temporal abstraction. An\n“option” ξ ∈Ξ is deﬁned by three elements: an option’s policy\nπ, a termination condition β, and an initiation set I ⊆S\ndenoted as the set of states in the option. In addition, a policy\nover options µ(ξ|s) is introduced to select options. An option\nis executed as follows. First, when an option’s policy πξ is\ntaken, the action at is selected based on πξ. The environment\nthen transitions to state st+1. The option either terminates\nor continues according to a termination probability βξ(st+1).\nWhen the option terminates, the agent can select a next option\nbased on µ(ξ|st+1). The total discounted reward received by\nexecuting option ξ is deﬁned as\nRξ\ns = E\n\u0014 t+k−1\nX\ni=t\nγi−trξ(si, ai)\n\u0015\n.\n(2)\nThe multi-time state transition model of option ξ [43], which\nis initiated in state s and terminate at state s′ after k steps,\nhas the formula\nPξ\nss′ =\n∞\nX\nk=1\nPξ(s′, k|s, ξ)γk.\n(3)\nwhere Pξ(s′, k|s, ξ) is the joint probability of ending up at s′\nafter k steps if taking option ξ at state s. Given this, we can\nwrite the Bellman equation for the value function of a policy\nµ over options:\nVµ(s) =\nX\nξ∈Ξ\nµ(ξ|s)\n\u0014\nRξ\ns +\nX\ns′\nPξ\nss′Vµ(s′)\n\u0015\n(4)\nand the option-value function:\nQµ(s, ξ) = Rξ\ns +\nX\ns′\nPξ\nss′\nX\nξ′∈Ξ\nµ(ξ′|s′)Qµ(s′, ξ′).\n(5)\nSimilarly, the corresponding optimal Bellman equations are as\nfollows:\nV∗(s) = max\nξ∈Ξ\n\u0014\nRξ\ns +\nX\ns′\nPξ\nss′V∗(s′)\n\u0015\n(6)\n3\nQ∗(s, ξ) = Rξ\ns +\nX\ns′\nPξ\nss′ max\nξ′∈Ξ Q∗(s′, ξ′).\n(7)\nThe optimal Bellman equation can be computed using syn-\nchronous value iteration (SVI) [21], which iterates the follow-\ning steps for every state:\nV(s) = max\nξ∈Ξ\n\u0014\nRξ\ns +\nX\ns′\nPξ\nss′V(s′, ξ′)\n\u0015\n.\n(8)\nWhen the option model is unknown, Qt(s, ξ) can be\nestimated using a Q-learning algorithm with an estimation\nformula:\nQ(s, ξ) ←Q(s, ξ) + α\n\u0014\nr(s, ξ(s)) + γk max\nξ′∈Ξ Q(s′, ξ′) −Q(s, ξ)\n\u0015\n,\n(9)\nwhere α denotes the learning rate, k denotes the number\nof time steps elapsing between s and s′ and r denotes an\nintermediate reward if ξ(s) is a primitive action, otherwise r\nis the total reward when executing option ξ(s).\nC. Partially observable Markov Decision Process in Rein-\nforcement Learning\nIn many real-world tasks, the agent might not have full\nobservability over the environment. In principle, those tasks\ncan in principle be formulated as a POMDP which is deﬁned as\na tuple of six components {S, A, P, r, Ω, Z}, where S, A, P, r\nare the state space, action space, transition function and reward\nfunction, respectively as in a Markov decision process. Ωand\nZ are observation space and observation model, respectively.\nIf the POMDP model is known, the optimal approach is to\nmaintain a hidden state bt called belief state. The belief deﬁnes\nthe probability of being in state s according to its history of\nactions and observations. Given a new action and observation,\nbelief updates are performed using the Bayes rule [44] and\ndeﬁned as follows:\nb′(s′) ∝Z(s′, a, o)\nX\ns∈S\nP(s, a, s′)b(s).\n(10)\nHowever, exact belief updates require a high computational\ncost and expensive memory [40]. Another approach is using a\nQ-learning agent with function approximation, which uses Q-\nlearning algorithm for updating the policy. Because updating\nthe Q-value from an observation can be less accurate than\nestimating from a complete state, a better way would be that\na POMDP agent using Q-Learning uses the last k observations\nas input to the policy. Nevertheless, the problem with using\na ﬁnite number of observations is that key-event observations\nfar in the past would be neglected in future decisions. For\nthis reason, a RNN is used to maintain a long-term state,\nas in [41]. Our model using RNNs at different levels of the\nhierarchical policies is expected to take advantage in POMDP\nenvironments.\nD. Deep Reinforcement Learning\nRecent advances in deep learning [45] are widely applied\nto reinforcement learning to form deep reinforcement learning.\nA few years ago, reinforcement learning still used “shallow”\npolicies such as tabular, linear, radial basis network, or neural\nnetworks of few layers. The shallow policies contain many\nlimitations, especially in representing highly complex behav-\niors and the computational cost of updating parameters. In\ncontrast, deep neural networks in deep reinforcement learning\ncan extract more information from the raw inputs by pushing\nthem through multiple neural layers such as multilayer percep-\ntron layer (MLP) and convolutional layer (CONV). Multiple\nlayers in DNNs can have a lot of parameters allowing them to\nrepresent highly non-linear problems. Deep Q-Network (DQN)\nhas been proposed recently by Google Deepmind [10], that\nopens a new era of deep reinforcement learning and inﬂuenced\nmost later studies in deep reinforcement learning. In term of\narchitecture, a Q-network parameterized by θ, e.g., Q(s, a|θ) is\nbuilt on a convolutional neural network (CNN) which receives\nan input of four images of size 84 × 84 and is processed\nby three hidden CONV layers. The ﬁnal hidden layer is a\nMLP layer with 512 rectiﬁer units. The output layer is a MLP\nlayer, which has the number of outputs equal to the number\nof actions. In term of the learning algorithm, DQN learns Q-\nvalue functions iteratively by updating the Q-value estimation\nvia the temporal difference error:\nQ(s, a) := Q(s, a) + α(r + γ max\na′ Q(s′, a′) −Q(s, a)).\n(11)\nIn addition, the stability of DQN also relies on two mech-\nanisms. The ﬁrst mechanism is experience replay memory,\nwhich stores transition data in the form of tuples {s, a, s′, r}.\nIt allows the agent to uniformly sample from and train on\nprevious data (off-policy) in batch, thus reducing the variance\nof learning updates and breaking the temporal correlation\namong data samples. The second mechanism is the target Q-\nnetwork, which is parameterized by θ′, e.g., ˆQ(s, a|θ′), and is\na copy of the main Q-network. The target Q-network is used\nto estimate the loss function as follows:\nL = E\n\u0014\n(y −Q(s, a|θ))2\n\u0015\n,\n(12)\nwhere y = r + γ max\na′\nˆQ(s′, a′|θ′). Initially, the parameters of\nthe target Q-network are the same as in the main Q-network.\nHowever, during the learning iteration, they are only updated\nat a speciﬁed time step. This update rule causes the target Q-\nnetwork to decouple from the main Q-network and improves\nthe stability of the learning process.\nMany other models based on DQNs have been developed\nsuch as Double DQNs [46], Dueling DQN [47], and Priority\nExperiment Replay [48]. On the other hand, deep neural\nnetworks can be integrated into other methods rather than\nestimating Q-values, such as representing the policy in policy\nsearch algorithms [5], estimating advantage function [49], or\nmixed actor network and critic network [50].\nRecently, researchers have used RNNs in reinforcement\nlearning to deal with POMDP domains. RNNs have been\nsuccessfully applied to domains, such as natural language\nprocessing and speech recognition, and are expected to be\nadvantageous in the POMDP domain, which needs to process\na sequence of observation rather than a single input. Our\n4\nproposed method uses RNNs not only for solving the POMDP\ndomain but also for solving these domains in a hierarchical\nform of reinforcement learning.\nIII. HIERARCHICAL DEEP RECURRENT Q-LEARNING IN\nPARTIALLY OBSERVABLE SEMI-MARKOV DECISION\nPROCESS\nIn this section, we describe hierarchical recurrent Q learn-\ning algorithm (hDRQN), our proposed framework. First, we\ndescribe the model of hDRQN and explain the way to learn\nthis model. Second, we describe some components of our\nalgorithm such as sampling strategies, subgoal deﬁnition,\nextrinsic and intrinsic reward functions. We rely on partially\nobservable semi-Markov decision process (POSMDP) settings,\nwhere the agent follows a hierarchical policy to solve POMDP\ndomains. The setting of POSMDP [43, 26] is described as\nfollows. The domain is decomposed into multiple subdomains.\nEach subdomain is equivalent to an option ξ in the SMDP\nframework and has a subgoal g ∈Ω, that needs to be achieved\nbefore switching to another option. Within an option, the agent\nobserves a partial state o ∈Ω, takes an action a ∈A, receives\nan extrinsic reward rex ∈R, and transits to another state\ns′ (but the agent only observes a part of the state o′ ∈Ω).\nThe agent executes option ξ until it is terminated (either the\nsubgoal g is obtained or the termination signal is raised).\nAfterward, the agent selects and executes another option. The\nsequence of options is repeated until the agent reaches the\nﬁnal goal. In addition, for obtaining subgoal g in each option,\nan intrinsic reward function rin ∈R is maintained. While\nthe objective of a subdomain is to maximize the cumulative\ndiscounted intrinsic reward, the objective of the whole domain\nis to maximize the cumulative discounted extrinsic reward.\nSpeciﬁcally, the belief update given a taken option ξ,\nobservation o and current belief b is deﬁned as\nb′(s′) ∝\n∞\nX\nk=1\nγk−1 X\ns\nP(s′, o, k|s, ξ)b(s)\nwhere P(s′, o, k|s, ξ) is a joint transition and observation func-\ntion of the underlying POSMDP model on the environment.\nWe adopt a similar notation from the two frameworks MAX-\nQ [24] and Options [21], to describe our problem. We denote\nQM(ot, ξ|θM) as the Q-value function of the meta-controller\nat state ot, θM (in which we use a RNN to encode past obser-\nvation that has been encoded in its weights θM) and option\n(macro-action) ξ (assuming ξ has a corresponding subgoal gt).\nWe note that the pair {ot, θM} represents the belief or history\nat time t, bt (we will use them inter-changeably: QM(b, ξ) or\nQM(o, ξ|θM)).\nThe multi-time observation model of option ξ [43], which\nis initiated in belief b and observe o, has a formula as follows\nP(o|b, ξ) =\n∞\nX\nk=1\nX\ns′\nX\ns\nγkP(s′, o, k|s, ξ)b(s)\n(13)\nGiven above, we can write the Bellman equation for the\nvalue function of the meta-controller policy π over options\nas follows\nVM(b) =\nX\nξ\nπ(ξ|b)\n\"\nRξ\nb +\nX\no′\nP(o′|b, ξ)VM(b′)\n#\n(14)\nand the option-value function,\nQM(b, ξ) = Rξ\nb +\nX\no′\nP(o′|b, ξ)\nX\nξ′∈Ξ\nµ(ξ′|b′)QM(b′, ξ′)\n(15)\nSimilarly to the MAX-Q framework, the reward term Rξ\nb\nis the total reward collected by executing the option ξ and\ndeﬁned as V ξ(b). Its corresponding Q-value function is deﬁned\nas QS(b, a) (the value function for sub-controllers). In the\nuse of RNN, we also denote QS(b, a) as QS({ot, gt}, a|θS)\nin which θS is the weights of the sub-controller network\nthat encodes previous observations, and {ot, gt} denote the\nobservation input to the sub-controller.\nOur hierarchical frameworks are illustrated in Fig. 1. The\nframework in Fig. 1a is inspired by a related idea in [37]. The\ndifference is that our framework is built on two deep recurrent\nneural policies: a meta-controller and a sub-controller. The\nmeta-controller is equivalent to a “policy over subgoals” that\nreceives the current observation ot and determine new subgoal\ngt. The sub-controller is equivalent to the option’s policy,\nwhich directly interacts with the environment by performing\naction at. The sub-controller receives both gt and ot as inputs\nto the deep neural network. Each controller contains an arrow\n(Fig. 1) pointed to itself that indicates that the controller\nemploys recurrent neural networks. In addition, an internal\ncomponent called “critic” is used to determine whether the\nagent has achieved subgoal or not and to produce an intrinsic\nreward that is used to learn the sub-controller. In contrast to\nthe framework in Fig. 1a, the framework in Fig. 1b does not\nuse the current observation to determine the subgoal in the\nmeta-controller but instead uses the last hidden state hS\nt of\nthe sub-controller. The last hidden state that is inferred from\na sequence of observations of the sub-controller is expected\nto contribute to the meta-controller to correctly determine the\nnext subgoal.\n(a) Framework 1\n(b) Framework 2\nFig. 1: Hierarchical deep recurrent Q-network frameworks\n5\nAs mentioned in the previous section, RNNs is used in our\nframework in order to enable learning in POMDP. Particularly,\nCNNs is used to learn a low-dimensional representation for\nimage inputs, while RNN is used in order to capture a\ntemporal relation of observation data. Without using a recur-\nrent layer as in RNNs, CNNs cannot accurately approximate\nthe state feature from observations in POMDP domain. The\nprocedure of the agent is clearly illustrated in Fig. 2. The\nmeta-controller and sub-controller use the Deep Recurrent\nQ Network (DRQN), as described in [41]. Particularly, at\nstep t, the meta-controller takes an observation ot from the\nenvironment (framework 1) or the last hidden state of sub-\ncontroller generated by the previous sequence of observations\n(framework 2), extracts state features through some deep\nneural layers, internally constructs a hidden state hM\nt , and\nproduces the Q subgoal values QM(ot, gt|θM). The Q subgoal\nvalues then are used to determine the next subgoal gt+k.\nSimilarly, the sub-controller receives both observation ot and\nsubgoal gt, extracts their feature, constructs a hidden state hS\nt ,\nand produces Q action values QS({ot, gt}, at|θS), which are\nused to determine the next action at+1. Those explanations\nare formalized into the following equations:\nΦM =\n(\nf extract(ot)\nframework 1\nf extract(hS)\nframework 2\n(16)\nhM\nt , QM(ot, gt|θM) = f M(ΦM, hM\nt−1)\n(17)\nΦS = f extract(ot, gt)\n(18)\nhS\nt , QS({ot, gt}, at|θS) = f S(ΦS, hS\nt−1),\n(19)\nwhere ΦM and ΦS are the features after the extraction process\nof the meta-controller and sub-controller, respectively. f M and\nf S are the respective recurrent networks of the meta-controller\nand sub-controller. They receive the state features and a hidden\nstate, and then provide the next hidden state and Q value\nfunction.\nThe networks for controllers are illustrated in Fig. 3. For\nframework 1, we use the networks demonstrated in Fig. 3a\nfor both the meta-controller and sub-controller. A sequence\nof four CONV layers and ReLU layers interleaved together\nis used to extract information from raw observations. A RNN\nlayer, especially LSTM, is employed in front of the feature to\nmemorize information from previous observations. The output\nof the RNN layer is split into Advantage stream A and Value\nstream V before being uniﬁed to the Q-value. This architecture\ninherits from Dueling architecture [47], effectively learning\nstates without having to learn the effect of an action on that\nstate. For framework 2, we use the network in Fig. 3a for the\nsub-controller and use the network in Fig. 3b to the meta-\ncontroller. The meta-controller in framework 2 uses the state\nthat is the internal hidden state from the sub-controller. By\npassing through four fully connected layers and ReLU layers,\nfeatures of the meta-controller are extracted. The rest of the\nnetwork is the same as the network for framework 1.\nA. Learning Model\nWe use a state-of-the-art Double DQN to learn the param-\neters of the network. Particularly, the controllers estimate the\nfollowing Q value function:\nQM(ˆot, gt) = QM(ˆot, gt)+\nα(rex + γk max\ngt+k QM(ˆot+k, gt+k) −QM(ˆot, gt))\n(20)\nand\nQS({ot, gt}, at) = QS({ot, gt}, at)+\nα(rin + γ max\nat+1 QS({ot+1, gt}, at+1) −QS({ot, gt}, at))\n(21)\nwhere ˆo can be a direct observation or an internal hidden\nstate generated by the sub-controller. Let θM and θS be\nparameters (weights and bias) of the deep neural networks, that\nparameterize the networks QM(ˆot, gt) and QS({ot, gt}, at),\ncorrespondingly, e.g. QM(ˆot, gt)\n=\nQM(ˆot, gt|θM) and\nQS({ot, gt}, at) = QS({ot, gt}, at|θS). Then, QM and QS\nare trained by minimizing the loss function LM and LS,\ncorrespondingly. LM can be formulated as:\nLM = E(o,g,o′,g′,rex)∼MM\n\u0002\nyM\ni\n−QM(o, g|θM)\n\u0003\n,\n(22)\nwhere E denotes the expectation over a batch of data which\nis uniformly sampled from an experience replay M M, i is the\niteration number in the batch of samples and\nyM\ni\n= rex + γQM ′(o′, argmax\ng′\nQM(o′, g′|θM)|θM ′).\n(23)\nSimilarly, the formula of LS is\nLS = E(o,g,a,rin)∼MS\n\u0002\nyS\ni −QS({o, g}, a|θS)\n\u0003\n,\n(24)\nwhere\nyS\ni = rin + γQS′({o′, g}, argmax\na′\nQS({o′, g}, a′|θS)|θS′).\n(25)\nMS is experience replay that stores the transition data from\nsub-controller, and QS′ is the target network of QS. Intuitively,\nin contrast to DQN which uses the maximum operator for both\nselecting and evaluating an action, Double DQN separately\nuses the main Q network to greedily estimate the next action\nand the target Q network to estimate the value function. This\nmethod has been shown to achieve better performance than\nDQN on Atari games [46].\nB. Minibatch sampling strategy\nFor updating RNNs in our model, we need to pass a\nsequence of samples. Particularly, some episodes from the\nexperience replay are uniformly sampled and are processed\nfrom the beginning of the episode to the end of the episode.\nThis strategy called Bootstrapped Sequential Updates [41],\nis an ideal method to update RNNs because their hidden\nstate can carry all information through the whole episode.\nHowever, this strategy is computationally expensive in a\nlong episode, which can contain many time steps. Another\napproach proposed in [41] has been evaluated to achieve the\nsame performance as Bootstrapped Sequential Updates, but\nreduce the computational complexity. The strategy is called\nBootstrapped Random Updates and has a procedure as follows.\n6\nMETA\nCONTROLLER\nSUB\nCONTROLLER\n...\n( ,g|\n)\nQM ot\nθM\ngt\not\not\n({ ,\n},a|\n)\nQS\not gt\nθS\nat\not+1\n({\n,\n},a|\n)\nQS\not+1 gt\nθS\nat+1\not+k\n({\n,\n},a|\n)\nQS\not+k gt\nθS\nat+k\n............\nMETA\nCONTROLLER\n(\n,g|\n)\nQM ot+k\nθM\ngt+k\not+k\n...\nSUB\nCONTROLLER\nSUB\nCONTROLLER\nhSt\nhS\nt+1\nhS\nt+k\nhM\nt\nhM\nt+k\n(a) Framework 1\nMETA\nCONTROLLER\nSUB\nCONTROLLER\n...\n(\n,g|\n)\nQM hM\nt\nθM\ngt\nhS\nt−1\not\n({ ,\n},a|\n)\nQS\not gt\nθS\nat\not+1\n({\n,\n},a|\n)\nQS\not+1 gt\nθS\nat+1\not+k\n({\n,\n},a|\n)\nQS\not+k gt\nθS\nat+k\n............\nMETA\nCONTROLLER\n(\n,g|\n)\nQM hM\nt+k\nθM\ngt+k\n...\nSUB\nCONTROLLER\nSUB\nCONTROLLER\nhSt\nhS\nt+1\nhS\nt+k\nhM\nt\nhM\nt+k\n(b) Framework 2\nFig. 2: Procedure of Hierarchical Deep Recurrent Q Learning (hDRQN). The framework is based on [37]\n(a) The network for framework 1\n(b) Meta-controller network in framework 2\nFig. 3: Network models\nThis strategy also randomly selects a batch of episodes from\nthe experience replay. Then, for each episode, we begin at a\nrandom transition and select a sequence of n transitions. The\nvalue of n that affects to the performance of our algorithm\nis analyzed in section IV. We apply the same procedure of\nBootstrapped Random Updates to our algorithm.\nIn addition, the mechanism explained in [51] is also applied.\nThat study explains a problem when updating DRQN: using\nthe ﬁrst observations in a sequence of transitions to update the\nQ value function might be inaccurate. Thus, the solution is to\nuse the last observations to update DRQN. Particularly, our\nmethod uses the last n\n2 transitions to update the Q-value.\nC. Subgoal Deﬁnition\nOur model is based on the “option” framework. Learning an\noption is using ﬂat deep RL algorithms to achieve a subgoal\nof that option. However, discovering subgoals among existing\nstates in the environment is still one of the challenges in\nhierarchical reinforcement learning. For simplifying the model,\nwe assume that a set of pre-deﬁned subgoals is provided in\nadvance. The pre-deﬁned subgoals based on object-oriented\nMDPs [52], where entities or objects in the environment are\ndecoded as subgoals.\n7\nD. Intrinsic and Extrinsic Reward Functions\nTraditional RL accumulates all reward and penalty in a\nreward function, which is hard to learn in a speciﬁed task\nin a complex domain. In contrast, hierarchical RL introduces\nthe notions of intrinsic reward function and an extrinsic reward\nfunction. Initially, intrinsic motivation is based on psychology,\ncognitive science, and neuroscience [53] and has been applied\nto hierarchical RL [54][55][56][57][58][59]. Our framework\nfollows the model of intrinsic motivation in [55]. Particularly,\nwithin an option (or skill), the agent needs to learn an option’s\npolicy (sub-controller in our framework) to obtain a subgoal\n(a salient event) under reinforcing of an intrinsic reward while\nfor the overall task, a policy over options (meta-controller) is\nlearned to generate a sequence of subgoals while reinforcing\nan extrinsic reward. Deﬁning “good” intrinsic reward and\nextrinsic reward functions is still an open question in rein-\nforcement learning, and it is difﬁcult to ﬁnd a reward function\nmodel that is generalized to all domains. To demonstrate some\nnotions above, Fig. 4 describes the domain of multiple goals\nin four-rooms which is used to evaluate our algorithm in\nSection IV. The four-rooms contain a number of objects: an\nagent (in black), two obstacles (in red) and two goals (in blue\nand green). These objects are randomly located on the map.\nAt each time step, the agent has to follow one of the four\nactions: top, down, left or right, and has to move to the goal\nlocation in a proper order: the blue goal ﬁrst and then the\ngreen goal. If the agent obtains all goals in the right order,\nit will receive a big reward; otherwise, it will only receive a\nsmall reward. In addition, the agent has to learn to avoid the\nobstacles if it does not want to be penalized. For this example,\nthe salient event is equivalent to reaching the subgoal or hitting\nthe obstacle. In addition, there are two skills the agent should\nlearn. One is moving to the goals while correctly avoiding the\nobstacles, and the second is selecting the goal it should reach\nﬁrst. The intrinsic reward for each skill is generated based on\nthe salient events encounters while exploring the environment.\nParticularly, for reaching the goal, the intrinsic reward includes\nthe reward for reaching goal successful and the penalty if the\nagent encounters an obstacle. For reaching the goals in order,\nthe intrinsic reward includes a big reward if the agent reaches\nthe goals in a proper order and a small reward if the agent\nreaches the goal in an improper order. A detailed explanation\nof intrinsic and extrinsic rewards for this domain is included\nin Section IV.\nE. Learning Algorithm\nIn this section, our contributions are summarized through\npseudo-code Algorithm 1. The algorithm learns four neural\nnetworks: two networks for meta-controller (QM and QM ′)\nand two networks for sub-controller (QS and QS′). They are\nparameterized by θM, θM ′, θS and θS′. The architectures of\nthe networks are described in Section III. In addition, the al-\ngorithm separately maintains two experience replay memories\nM M and M S to store transition data from meta-controller and\nsub-controller, respectively. Before starting the algorithm, the\nparameters of the main networks are randomly initialized and\nare copied to the target networks. ϵM and ϵS are annealed from\nAlgorithm 1 hDRQN in POMDP\nRequire:\n1: POMDP M = {S, A, P, r, Ω, Z}\n2: Meta-controller with the network QM (main) and QM ′\n(target) parameterized by θM and θM ′, respectively\n3: Sub-controller with the network QS (main) and QS′\n(target) parameterized by θS and θS′, respectively\n4: Exploration rate ϵM for meta-controller and ϵS for sub-\ncontroller\n5: Experience replay memories M M and M S of meta-\ncontroller and sub-controller respectively\n6: A pre-deﬁned set of subgoals G\n7: f M and f S are recurrent networks of meta-controller and\nsub-controller respectively\nEnsure:\n8: Initialize:\n• Experiences replay memories M M and M S\n• Randomly initialize θM and θS\n• Assign value to the target networks θM ′ ←θM and\nθS′ ←θS\n• ϵM ←1.0 and decreasing to 0.1\n• ϵS ←1.0 and decreasing to 0.1\n9: for k = 1, 2, . . . K do\n10:\nInitialize: the environment and get the start observation\no\n11:\nInitialize: hidden states hM ←0\n12:\nwhile o is not terminal do\n13:\nInitialize: hidden states hS ←0\n14:\nInitialize: start observations o0 ←ˆo where ˆo can be\nobservation o or hidden state hS\n15:\nDetermine subgoal: g, hM ←\nEPS GREEDY (ˆo, hM, G, ϵM, QM, f M)\n16:\nwhile o is not terminal and g is not reached do\n17:\nDetermine action: a, hS ←\nEPS GREEDY ({o, g}, hS, A, ϵS, QS, f S)\n18:\nExecute action a, receive reward r, extrinsic re-\nward rex, intrinsic reward rin, and obtain the next\nstate s′\n19:\nStore transition {{o, g}, a, rin, {o′, g′}} in M S\n20:\nUpdate sub-controller\nSUB UPDATE(M S, QS, QS′)\n21:\nUpdate meta-controller\nMETA UPDATE(M M, QM, QM ′)\n22:\nTransition to next observation o ←o′\n23:\nend while\n24:\nStore transition {o0, g, rex\ntotal, ˆo′} in M S where ˆo′\ncan be observation o′ or the last hidden state hS\n25:\nend while\n26:\nAnneal ϵM and ϵS\n27: end for\n8\nFig. 4: Example domain for illustrating the notions of intrinsic\nand extrinsic motivation\n1.0 to 0.1, which gradually increase control of controllers. The\nalgorithm loops through a speciﬁed number of episodes (Line\n9) and each episode is executed until the agent reaches the\nterminal state. To start an episode, ﬁrst, a starting observation\no0 is obtained (Line 10). Next, hidden states, which are inputs\nto RNNs, must be initialized with zero values (Line 11 and\nLine 13) and are updated during the episode (Line 15 and\nLine 17). Each subgoal is determined by passing observation\no or hidden state hS (depending on the framework) to the\nmeta-controller (Line 15). By following a greedy ϵ fashion,\na subgoal will be selected from the meta-controller if it is a\nrandom number greater than ϵ. Otherwise, a random subgoal\nwill be selected (Algorithm 2). The sub-controller is taught\nto reach the subgoal; when the subgoal is reached, a new\nsubgoal will be selected. The process is repeated until the\nﬁnal goal is obtained. Intrinsic reward is evaluated by the critic\nmodule and is stored in M S (Line 19) for updating the sub-\ncontroller. Meanwhile, the extrinsic reward is directly received\nfrom the environment and is stored in M M for updating the\nmeta-controller (Line 24) . Updating controllers at Line 20\nand 21 is described in Section III-A and is summarized in\nAlgorithm 3 and Algorithm 4.\nAlgorithm 2 EPS GREEDY (x, h, B, ϵ, Q, f)\nRequire:\n1: x: input of the Q network\n2: h: internal hidden states\n3: B: a set of outputs\n4: ϵ: exploration rate\n5: Q network and recurrent function f\nEnsure:\n6: h ←f(x, h)\n7: if random() < ϵ then\n8:\no ←An element from the set of output B\n9: else\n10:\no = argmaxm∈B Q(x, m)\n11: end if\n12: Return o, h\nAlgorithm 3 META UPDATE(M M, QM, QM ′)\nRequire:\n1: M M: experience replay memory of meta-controller\nEnsure:\n2: Sample a mini-batch of {o, g, rex, o′} from M M as the\nstrategy explained at III-B\n3: Update the network by minimizing the loss function:\nLM = E(o,g,o′,g′,rex)∼MM\n\u0002\nyM\ni\n−QM(o, g|θM)\n\u0003\nwhere\nyM\ni\n= rex + γQM ′(o′, argmax\ng′\nQM(o′, g′|θM)|θM ′)\n4: Update the target network: θM ′ ←τθM + (1 −τ)θM ′\nAlgorithm 4 SUB UPDATE(M S, QS, QS′)\nRequire:\n1: M S: experience replay memory of meta-controller\n2: Sample a mini-batch of {{o, g}, a, rin, {o′, g′}} from M S\nas the strategy explained at III-B\n3: Update the main network by minimizing the loss func-\ntion:\nLS = E(o,g,a,rin)∼MS\n\u0002\nyS\ni −QS({o, g}, a|θS)\n\u0003\n,\nwhere\nyS\ni = rin+γQS′({o′, g}, argmax\na′\nQS({o′, g}, a′|θS′)|θS′)\n4: Update the target network: θS′ ←τθS + (1 −τ)θS′\nIV. EXPERIMENTS\nIn this section, we evaluate two versions of the hierarchical\ndeep recurrent network algorithm. hDRQNv1 is the algorithm\nformed by framework 1, and hDRQNv2 is the algorithm\nformed by framework 2. We compare them with ﬂat algo-\nrithms (DQN, DRQN) and the state-of-the-art hierarchical RL\nalgorithm (hDQN). The comparisons are performed on three\ndomains. The domain of multiple goals in a gridworld is used\nto evaluate many aspects of the proposed algorithms. Mean-\nwhile, the harder domain called multiple goals in four-rooms\nis used to benchmark the proposed algorithm. Finally, the most\nchallenging game in ATARI 2600 [9] called Montezuma’s\nRevenge, is used to conﬁrm the efﬁciency of our proposed\nframework.\nA. Implementation Details\nWe use Tensorﬂow [60] to implement our algorithms. The\nsettings for each domain are different, but they have some\ncommonalities as follows. For a hDRQNv1 algorithm, the\ninputs to the meta-controller and sub-controller are an image\nof size 44×44×3 (a color image). The input image is resized\nfrom an observation which is observed around the agent (either\n3 × 3 unit or 5 × 5 unit). The image feature of 256 values\nextracted through four CNNs and ReLUs is put into a LSTM\nlayer of 256 states to generate 256 output values, and an\n9\ninternal hidden state of 256 values is also constructed. For\na hDRQNv2 algorithm, a hidden state of 256 values is put\ninto the network of the meta-controller. The state is passed\nthrough four fully connected layers and ReLU layers instead of\nfour CONV layers. The output is a feature of 256 values. The\nalgorithm uses ADAM [61] for learning the neural network\nparameters with the learning rate 0.001 for both the meta-\ncontroller network and sub-controller. For updating the target\nnetwork, a τ value of 0.001 is applied. The algorithm uses a\ndiscount factor of 0.99. The capacity of M S and M M is 1E5\nand 5E4, respectively.\nB. Multiple Goals in a Gridworld\nThe domain of multiple goals in a gridworld is a simple\nform of multiple goals in four-rooms which is described in\nSection III-D. In this domain, a gridworld map of size 11×11\nunit is instead of four-rooms map. At each time step, the agent\nonly observes a part of the surrounding environment, either\n3 × 3 unit (Fig. 5b) or 5 × 5 unit (Fig. 5c). The agent is\nallowed to choose one of four actions (top, left, right, bottom)\nthat are deterministic. The agent can not move if the action\nleads it to the wall. The rewards for the agent are deﬁned as\nfollows. If the agent hits an obstacle, it will receive a penalty\nof minus one. If the agent reaches two goals in proper order,\nit will receive a reward of one for each goal. Otherwise, it\nonly receives 0.01. For applying to an hDRQN algorithm, an\nintrinsic reward function and an extrinsic reward function are\ndeﬁned as follows\nrin =\n(\n1\nobtain the goal\n−1\nhit the obstacle\n(26)\nand\nrex =\n(\n1\nreach goals in order\n0.01\notherwise\n(27)\n(a) Multiple goals in gridworld\n(b) 3 × 3 unit\n(c) 5 × 5 unit\nFig. 5: Multiple goals in gridworld\nThe ﬁrst evaluation reported in Fig. 6 is a comparison of\ndifferent lengths of selected transitions discussed in section\nIII-B). The agent in this evaluation can observe an area of\n5 × 5 unit. We report the performance through three runs of\n20000 episodes and each episode has 50 steps. The number of\nsteps for each episode assures that the agent can explore any\nlocation on the map. In the ﬁgures on the left (hDRQNv1)\nand in the middle (hDRQNv2), we use a ﬁxed length of\nmeta-transitions (nM = 1) and compare different lengths\nof sub-transitions. Otherwise, the ﬁgures on the right show\nthe performance of the algorithm using a ﬁxed length of\nsub-transitions (nS = 8) and compare different lengths of\nmeta-transitions. With a ﬁxed length of meta-transition, the\nalgorithm performs well with a long length of sub-transition\n(nS = 8 or nS = 12); the performance decreases when the\nlength of sub-transitions is decreased. Intuitively, the RNN\nneeds a sequence of transitions that is long enough to increase\nthe probability that the agent will reach the subgoal within\nthat sequence. Another observation is that, with nS = 8\nor nS\n= 12, there is a little difference in performance.\nThis is reasonable because only eight transitions are needed\nfor the agent to reach the subgoals. For a ﬁxed length of\nsub-transitions (nS = 8), with a hDRQNv1 algorithm, the\nsetting with nM = 2 has low performance and high variance\ncompared to the setting with nM = 1. The reason is that while\nthe sub-controller for two settings has the same performance\n(Fig. 6f) the meta-controller with nM = 1 performs better\nthan the meta-controller with nM = 2. Meanwhile, with a\nhDRQNv2 algorithm, the performance is the same at both\nsettings nM = 1 and nM = 2. This means that the hidden\nstate from the sub-controller is a better input to determine the\nsubgoal rather than a raw observation as it causes the algorithm\nto not depend on the length of the meta-transition. The number\nof steps to obtain two goals in order is around 22.\nThe next evaluation is a comparison at different levels\nof observation. Fig. 7 shows the performance of hDRQN\nalgorithms with a 3 × 3 observable agent compared with a\n5×5 observable agent and a fully observable agent. It is clear\nthat a fully observable agent can have more information around\nit than a 5 × 5 observable agent and a 3 × 3 observable agent;\nthus, the agent with a larger observation area can quickly\nexplore and localize the environment completely. As a result,\nthe performance of the agent with a larger observation area is\nbetter than the agent with a smaller observing ability. From\nthe ﬁgure, the performance of a 5 × 5 observable agent using\nhDRQNv2 seems to converge faster than a fully observable\nagent. However, the performance of the fully observable agent\nsurpasses the performance of 5×5 observable agent at the end.\nIn the last evaluation of this domain, we compare the\nperformance of the proposed algorithms with the well-known\nalgorithms DQN, DRQN, and hDQN [37]. All algorithms\nassume that the agent only observes an area of 5 units around\nit. The results are shown in Fig. 8. For both domains of\ntwo goals and three goals, hDRQN algorithms outperform\nother algorithms and hDRQNv2 has the best performance. The\nhDQN algorithm, which can operate in a hierarchical domain,\nis better than ﬂat algorithms but not better than hDRQN\nalgorithms. It might be that hDQN algorithm is only for\nfully observed domains and has poor performance in partially\nobservable domains.\n10\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nReward\nsub-2\nsub-4\nsub-8\nsub-12\n(a) hDRQNv1: reward\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nReward\nsub-2\nsub-4\nsub-8\nsub-12\n(b) hDRQNv1: intrinsic\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nReward\nsub-2\nsub-4\nsub-8\nsub-12\n(c) hDRQNv1: extrinsic\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n20\n25\n30\n35\n40\n45\n50\nSteps\nsub-2\nsub-4\nsub-8\nsub-12\n(d) hDRQNv1: steps\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.5\n0.0\n0.5\n1.0\n1.5\nReward\nsub-2\nsub-4\nsub-8\nsub-12\n(e) hDRQNv2: reward\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nReward\nsub-2\nsub-4\nsub-8\nsub-12\n(f) hDRQNv2: intrinsic\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nReward\nsub-2\nsub-4\nsub-8\nsub-12\n(g) hDRQNv2: extrinsic\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n20\n25\n30\n35\n40\n45\n50\nSteps\nsub-2\nsub-4\nsub-8\nsub-12\n(h) hDRQNv2: steps\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.5\n0.0\n0.5\n1.0\n1.5\nReward\nhDRQNv1 meta-1\nhDRQNv2 meta-1\nhDRQNv1 meta-2\nhDRQNv2 meta-2\n(i) Reward\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nReward\nhDRQNv1 meta-1\nhDRQNv2 meta-1\nhDRQNv1 meta-2\nhDRQNv2 meta-2\n(j) Intrinsic\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nReward\nhDRQNv1 meta-1\nhDRQNv2 meta-1\nhDRQNv1 meta-2\nhDRQNv2 meta-2\n(k) Extrinsic\n0\n25\n50\n75\n100\n125\n150\n175\nTimesteps*100\n20\n25\n30\n35\n40\n45\n50\nSteps\nhDRQNv1 meta-1\nhDRQNv2 meta-1\nhDRQNv1 meta-2\nhDRQNv2 meta-2\n(l) Steps\nFig. 6: Evaluation of different lengths of transitions. In the left-side ﬁgures, we use a ﬁxed length of meta-transitions (nM = 1).\nThe ﬁgures on the right use a ﬁxed length of sub-transitions (nS = 8)\n.\n0\n20\n40\n60\n80\nTimesteps*1000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nReward\nhDRQNv1 obs-3\nhDRQNv2 obs-3\nhDRQNv1 obs-5\nhDRQNv2 obs-5\nhDRQNv1 obs-full\nhDRQNv2 obs-full\nFig. 7: Evaluation on different levels of observation\nC. Multiple Goals in a Four-rooms Domain\nIn this domain, we apply the multiple goals domain to a\ncomplex map called four-rooms (Fig. 9). The dynamics of the\nenvironment in this domain is similar to that of the task in\nIV-B. The agent in this domain must usually pass through\nhallways to obtain goals that are randomly located in four\nrooms. Originally, the four-rooms domain was an environment\nfor testing a hierarchical reinforcement learning algorithm\n[21].\nThe performance is shown in Fig. 10a is averaged through\nthree runs of 50000 episodes and each episode has 50 time\nsteps. Meanwhile, the performance is shown in Fig. 10b is\naveraged through three runs of 100000 episodes, and each\nepisode has 100 time steps. Similarly, the proposed algorithms\noutperform other algorithms, especially, the hDRQNv2 algo-\n11\n0\n50\n100\n150\nTimesteps*100\n0.5\n0.0\n0.5\n1.0\n1.5\nReward\nhDRQNv1\nhDRQNv2\nhDQN\nDQN\nDRQN\n(a) Two goals in gridworld\n0\n100\n200\n300\n400\nTimesteps*100\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nReward\nhDRQNv1\nhDRQNv2\nhDQN\nDQN\nDRQN\n(b) Three goals in gridworld\nFig. 8: Comparing hDRQN algorithms with some baseline\nalgorithms\n(a) The map of multiple goals in the\nfour-rooms domain\n(b) 3 × 3 unit\n(c) 5 × 5 unit\nFig. 9: Multiple goals in the four-rooms domain\nrithm.\nD. Montezuma’s Revenge game in ATARI 2600\nMontezumas Revenge is the hardest game in ATARI 2600,\nwhere the DQN algorithm [10] can only achieve a score of\nzero. We use OpenAI Gym to simulate this domain [62]. The\ngame is hard because the agent must execute a long sequence\nof actions until a state with non-zero reward (delayed reward)\ncan be visited. In addition, in order to obtain a state with\nlarger rewards, the agent needs to reach a special state in\n0\n100\n200\n300\n400\nTimesteps*100\n0.0\n0.5\n1.0\n1.5\nReward\nhDRQNv1\nhDRQNv2\nhDQN\nDQN\nDRQN\n(a) Two goals in four-rooms\n0\n20\n40\n60\n80\nTimesteps*1000\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nReward\nhDRQNv1\nhDRQNv2\nhDQN\nDQN\nDRQN\n(b) Three goals in four-rooms\nFig. 10: Comparing hDRQN algorithms with some baseline\nalgorithms\nadvance. This paper evaluates our proposed algorithms on the\nﬁrst screen of the game (Fig. 11). Particularly, the agent, which\nonly observes a part of the environment (Fig. 11b), needs to\npass through doors (the yellow line in the top left and top\nright corners of the ﬁgure) to explore other screens. However,\nto pass through the doors, ﬁrst, the agent needs to pick up the\nkey on the left side of the screen. Thus, the agent must learn\nto navigate to the key’s location and then navigate back to the\ndoor and open the next screens. The agent will earn 100 after\nit obtains the key and 300 after it reaches any door. Totally,\nthe agent can receive 400 for this screen.\nThe intrinsic reward function is deﬁned to motivate the\nagent to explore the whole environment. Particularly, the agent\nwill be received an intrinsic value of 1 if it could reach a\nsubgoal from other subgoals. The set of subgoals is pre-deﬁned\nin Fig. 11a (the white rectangles). In contrast to intrinsic\nreward function, the extrinsic reward function is deﬁned as\na reward value of 1 when the agent obtains the key or\nopen the doors. Because learning the meta-controller and the\nsub-controllers simultaneously is highly complex and time-\nconsuming, we separate the learning process into two phases.\nIn the ﬁrst phase, we learn the sub-controllers completely such\nthat the agent can explore the whole environment by moving\nbetween subgoals. In the second phase, we learn the meta-\ncontroller and sub-controllers altogether. The architecture of\nthe meta-controller and the sub-controllers is described in\n12\n(a) The ﬁrst screen of Montezuma’s\nRevenge game\n(b)\nObservable\narea\nFig. 11: Montezuma’s Revenge game in ATARI 2600\nsection IV-A. The length of sub-transition and meta-transition\nis 8 and 16 correspondingly. In this domain, the agent can\nobserve an area of 70 × 70 pixels. Then, the observation area\nis resized to 44 × 44 to ﬁt the input of a controller network.\nThe performance of proposed algorithms compared to baseline\nalgorithms is shown in Fig. 12. DQN reported a score of\nzero which is similar to the result from [10]. DRQN which\ncan perform well in a partially observable environment also\nachieves a score of zero because of the highly hierarchical\ncomplexity of the domain. Meanwhile, hDQN can achieve\na high score on this domain. However, it cannot perform\nwell in a partial observability setting. The performance of\nhDQN in a full observability setting can be found in the paper\nof Kulkarni [37]. Our proposed algorithms can adapt to the\npartial observability setting and hierarchical domains as well.\nThe hDRQNv2 algorithm shows a better performance than\nhDRQNv1. It seems that the difference in the architecture\nof two frameworks (described in section III) has affected\ntheir performance. Particularly, using internal states of a sub-\ncontroller as the input to the meta-controller can give more\ninformation for prediction than using only a raw observation.\nTo evaluate the effectiveness of the two algorithms, we report\nthe success ratio for reaching the goal “key” in Fig. 13 and\nthe number of time steps the agent explores each subgoal\nin Fig. 14. In Fig. 13, the agent using hDRQNv2 algorithm\nalmost picks up the “key” at the end of the learning process.\nMoreover, Fig. 14 shows that hDRQNv2 tends to explore more\noften on subgoals that are on the way reaching the “key” (E.g.\ntop-right-ladder, bottom-right-ladder, and bottom-left-ladder)\nwhile exploring less often on other subgoals such as the left\ndoor and right door.\nV. CONCLUSION\nWe introduced a new hierarchical deep reinforcement learn-\ning algorithm that is a learning framework for both full\nobservability (MDP) and partial observability (POMDP). The\nalgorithm takes advantages of deep neural networks (DNN,\nCNN, LSTM) to produce hierarchical policies that can solve\n0\n20\n40\n60\n80\n100\nEpisodes*100\n0\n50\n100\n150\n200\n250\n300\n350\nReward\nhDRQNv1\nhDRQNv2\nhDQN\nDQN\nDRQN\nFig. 12: Comparing hDRQN algorithms with some baseline\nalgorithms on Montezuma’s Revenge game\n0\n20\n40\n60\n80\n100\nEpisodes*100\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nReward\nFig. 13: Success ratio for reaching the goal “key”\n4000\n8000\n12000\nEpisodes\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTimes\nleft-door\nright-door\ntop-right-ladder\nbottom-right-ladder\nbottom-left-ladder\nkey\nFig. 14: Statistic about the number of times the agent visits\nsubgoals\n13\ndomains with a highly hierarchical nonlinearity. We showed\nthat the framework has performed well when learning in hi-\nerarchical POMDP environments. Nevertheless, our approach\ncontains some limitations as follows. First, our framework is\nbuilt on two levels of a hierarchy which does not ﬁt into the\ndomain with multiple levels of hierarchy. Second, in order\nto simplify the learning problem in hierarchical POMDP, we\nassume that the set of subgoals is predeﬁned and ﬁxed because\nthe problem of discovering a set of subgoals in POMDP is\nstill a hard problem. In the future, we plan to improve our\nframework by tackling those problems. In addition, we can\napply DRQN to multi-agent problems where the environment\nis partially observable and the task is hierarchical.\nACKNOWLEDGMENT\nThe authors are grateful to the Basic Science Research\nProgram through the National Research Foundation of Korea\n(NRF-2017R1D1A1B04036354).\nREFERENCES\n[1] R. S. Sutton and A. G. Barto, Introduction to Reinforce-\nment Learning, 1st ed.\nCambridge, MA, USA: MIT\nPress, 1998.\n[2] M. P. Deisenroth, G. Neumann, J. Peters et al., “A survey\non policy search for robotics,” Foundations and Trends R⃝\nin Robotics, vol. 2, no. 1–2, pp. 1–142, 2013.\n[3] J. Peters and S. Schaal, “Policy gradient methods\nfor robotics,” in Intelligent Robots and Systems, 2006\nIEEE/RSJ International Conference on.\nIEEE, 2006,\npp. 2219–2225.\n[4] ——, “Natural actor-critic,” Neurocomputing, vol. 71,\nno. 7, pp. 1180 – 1190, 2008, progress in Modeling,\nTheory,\nand\nApplication\nof\nComputational\nIntelli-\ngenc. [Online]. Available: http://www.sciencedirect.com/\nscience/article/pii/S0925231208000532\n[5] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and\nP. Moritz, “Trust region policy optimization,” in Proceed-\nings of the 32nd International Conference on Machine\nLearning (ICML-15), 2015, pp. 1889–1897.\n[6] J. W. Lee, “Stock price prediction using reinforcement\nlearning,” in Industrial Electronics, 2001. Proceedings.\nISIE 2001. IEEE International Symposium on, vol. 1.\nIEEE, 2001, pp. 690–695.\n[7] J. Moody and M. Saffell, “Learning to trade via direct\nreinforcement,” IEEE transactions on neural Networks,\nvol. 12, no. 4, pp. 875–889, 2001.\n[8] Y. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai, “Deep\ndirect reinforcement learning for ﬁnancial signal rep-\nresentation and trading,” IEEE transactions on neural\nnetworks and learning systems, vol. 28, no. 3, pp. 653–\n664, 2017.\n[9] V.\nMnih,\nK.\nKavukcuoglu,\nD.\nSilver,\nA.\nGraves,\nI. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing\natari with deep reinforcement learning,” arXiv preprint\narXiv:1312.5602, 2013.\n[10] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-\nness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski et al., “Human-level control\nthrough deep reinforcement learning,” Nature, vol. 518,\nno. 7540, pp. 529–533, 2015.\n[11] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre,\nG. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot et al., “Mastering the\ngame of go with deep neural networks and tree search,”\nNature, vol. 529, no. 7587, pp. 484–489, 2016.\n[12] G. Tesauro, D. Gondek, J. Lenchner, J. Fan, and J. M.\nPrager, “Simulation, learning, and optimization tech-\nniques in watson’s game strategies,” IBM Journal of\nResearch and Development, vol. 56, no. 3.4, pp. 16–1,\n2012.\n[13] G. Tesauro, “Td-gammon: A self-teaching backgammon\nprogram,” in Applications of Neural Networks. Springer,\n1995, pp. 267–285.\n[14] D. Ernst, M. Glavic, and L. Wehenkel, “Power systems\nstability control: Reinforcement learning framework,”\nIEEE transactions on power systems, vol. 19, no. 1, pp.\n427–435, 2004.\n[15] J.\nKober\nand\nJ.\nPeters,\n“Reinforcement\nlearning\nin robotics: A survey,” in Reinforcement Learning.\nSpringer, 2012, pp. 579–610.\n[16] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Man-\nsour, “Policy gradient methods for reinforcement learning\nwith function approximation,” in Advances in neural\ninformation processing systems, 2000, pp. 1057–1063.\n[17] P.\nDayan\nand\nG.\nE.\nHinton,\n“Using\nexpectation-\nmaximization for reinforcement learning,” Neural Com-\nputation, vol. 9, no. 2, pp. 271–278, 1997.\n[18] D. E. Moriarty, A. C. Schultz, and J. J. Grefenstette,\n“Evolutionary algorithms for reinforcement learning,”\n1999.\n[19] V. R. Konda and J. N. Tsitsiklis, “Actor-critic algo-\nrithms,” in Advances in neural information processing\nsystems, 2000, pp. 1008–1014.\n[20] S. Sukhbaatar, A. Szlam, G. Synnaeve, S. Chintala, and\nR. Fergus, “Mazebase: A sandbox for learning from\ngames,” arXiv preprint arXiv:1511.07401, 2015.\n[21] R. S. Sutton, D. Precup, and S. Singh, “Between mdps\nand semi-mdps: A framework for temporal abstraction in\nreinforcement learning,” Artiﬁcial intelligence, vol. 112,\nno. 1-2, pp. 181–211, 1999.\n[22] R. E. Parr, Hierarchical control and learning for Markov\ndecision processes.\nUniversity of California, Berkeley\nBerkeley, CA, 1998.\n[23] R. Parr and S. J. Russell, “Reinforcement learning with\nhierarchies of machines,” in Advances in neural informa-\ntion processing systems, 1998, pp. 1043–1049.\n[24] T. G. Dietterich, “Hierarchical reinforcement learning\nwith the maxq value function decomposition,” Journal of\nArtiﬁcial Intelligence Research, vol. 13, no. 1, pp. 227–\n303, 2000.\n[25] N. A. Vien, H. Q. Ngo, S. Lee, and T. Chung, “Approx-\nimate planning for bayesian hierarchical reinforcement\nlearning,” Appl. Intell., vol. 41, no. 3, pp. 808–819, 2014.\n[26] N. A. Vien and M. Toussaint, “Hierarchical monte-carlo\nplanning,” in Proceedings of the Twenty-Ninth AAAI Con-\n14\nference on Artiﬁcial Intelligence, January 25-30, 2015,\nAustin, Texas, USA., 2015, pp. 3613–3619.\n[27] N. A. Vien, S. Lee, and T. Chung, “Bayes-adaptive\nhierarchical mdps,” Appl. Intell., vol. 45, no. 1, pp. 112–\n126, 2016.\n[28] A. G. Barto and S. Mahadevan, “Recent advances in\nhierarchical reinforcement learning,” Discrete Event Dy-\nnamic Systems, vol. 13, no. 4, pp. 341–379, 2003.\n[29] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul,\nD. Saxton, and R. Munos, “Unifying count-based explo-\nration and intrinsic motivation,” in Advances in Neural\nInformation Processing Systems, 2016, pp. 1471–1479.\n[30] P.-L. Bacon, J. Harb, and D. Precup, “The option-critic\narchitecture.” 2017.\n[31] R. Fox, S. Krishnan, I. Stoica, and K. Goldberg,\n“Multi-level discovery of deep options,” arXiv preprint\narXiv:1703.08294, 2017.\n[32] S. Lee, S.-W. Lee, J. Choi, D.-H. Kwak, and B.-T.\nZhang, “Micro-objective learning: Accelerating deep re-\ninforcement learning through the discovery of continuous\nsubgoals,” arXiv preprint arXiv:1703.03933, 2017.\n[33] A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess,\nM. Jaderberg, D. Silver, and K. Kavukcuoglu, “Feudal\nnetworks for hierarchical reinforcement learning,” arXiv\npreprint arXiv:1703.01161, 2017.\n[34] I. P. Durugkar, C. Rosenbaum, S. Dernbach, and S. Ma-\nhadevan, “Deep reinforcement learning with macro-\nactions,” arXiv preprint arXiv:1606.04615, 2016.\n[35] K. Arulkumaran, N. Dilokthanakul, M. Shanahan, and\nA. A. Bharath, “Classifying options for deep reinforce-\nment learning,” arXiv preprint arXiv:1604.08153, 2016.\n[36] P. Dayan and G. E. Hinton, “Feudal reinforcement\nlearning,” in Advances in neural information processing\nsystems, 1993, pp. 271–278.\n[37] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenen-\nbaum, “Hierarchical deep reinforcement learning: Inte-\ngrating temporal abstraction and intrinsic motivation,”\nin Advances in Neural Information Processing Systems,\n2016, pp. 3675–3683.\n[38] C.-C. Chiu and V.-W. Soo, “Subgoal identiﬁcations in\nreinforcement learning: A survey,” in Advances in Rein-\nforcement Learning.\nInTech, 2011.\n[39] M. Stolle, “Automated discovery of options in reinforce-\nment learning,” Ph.D. dissertation, McGill University,\n2004.\n[40] K. P. Murphy, “A survey of pomdp solution techniques,”\nenvironment, vol. 2, p. X3, 2000.\n[41] M. Hausknecht and P. Stone, “Deep recurrent q-learning\nfor partially observable mdps,” 2015.\n[42] M. Egorov, “Deep reinforcement learning with pomdps,”\n2015.\n[43] C. C. White, “Procedures for the solution of a ﬁnite-\nhorizon, partially observed, semi-markov optimization\nproblem,” Operations Research, vol. 24, no. 2, pp. 348–\n358, 1976.\n[44] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra,\n“Planning and acting in partially observable stochastic\ndomains,” Artiﬁcial intelligence, vol. 101, no. 1, pp. 99–\n134, 1998.\n[45] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learn-\ning.\nMIT Press, 2016.\n[46] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforce-\nment learning with double q-learning.” 2016.\n[47] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanc-\ntot, and N. De Freitas, “Dueling network architec-\ntures for deep reinforcement learning,” arXiv preprint\narXiv:1511.06581, 2015.\n[48] T.\nSchaul,\nJ.\nQuan,\nI.\nAntonoglou,\nand\nD.\nSil-\nver,\n“Prioritized\nexperience\nreplay,”\narXiv\npreprint\narXiv:1511.05952, 2015.\n[49] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Con-\ntinuous deep q-learning with model-based acceleration,”\nin International Conference on Machine Learning, 2016,\npp. 2829–2838.\n[50] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,\nY. Tassa, D. Silver, and D. Wierstra, “Continuous con-\ntrol with deep reinforcement learning,” arXiv preprint\narXiv:1509.02971, 2015.\n[51] G. Lample and D. S. Chaplot, “Playing fps games with\ndeep reinforcement learning.” 2017.\n[52] C. Diuk, A. Cohen, and M. L. Littman, “An object-\noriented representation for efﬁcient reinforcement learn-\ning,” in Proceedings of the 25th international conference\non Machine learning.\nACM, 2008, pp. 240–247.\n[53] R. M. Ryan and E. L. Deci, “Intrinsic and extrinsic\nmotivations: Classic deﬁnitions and new directions,”\nContemporary educational psychology, vol. 25, no. 1, pp.\n54–67, 2000.\n[54] A. Stout, G. D. Konidaris, and A. G. Barto, “Intrin-\nsically motivated reinforcement learning: A promising\nframework for developmental robot learning,” MAS-\nSACHUSETTS UNIV AMHERST DEPT OF COM-\nPUTER SCIENCE, Tech. Rep., 2005.\n[55] A. G. Barto, “Intrinsically motivated learning of hierar-\nchical collections of skills,” 2004, pp. 112–119.\n[56] S. Singh, R. L. Lewis, A. G. Barto, and J. Sorg, “Intrinsi-\ncally motivated reinforcement learning: An evolutionary\nperspective,” IEEE Transactions on Autonomous Mental\nDevelopment, vol. 2, no. 2, pp. 70–82, 2010.\n[57] M. Frank, J. Leitner, M. Stollenga, A. F¨orster, and\nJ. Schmidhuber, “Curiosity driven reinforcement learning\nfor motion planning on humanoids,” Frontiers in neuro-\nrobotics, vol. 7, p. 25, 2014.\n[58] S. Mohamed and D. J. Rezende, “Variational information\nmaximisation for intrinsically motivated reinforcement\nlearning,” in Advances in neural information processing\nsystems, 2015, pp. 2125–2133.\n[59] J. Schmidhuber, “Formal theory of creativity, fun, and\nintrinsic motivation (1990–2010),” IEEE Transactions on\nAutonomous Mental Development, vol. 2, no. 3, pp. 230–\n247, 2010.\n[60] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-\nard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-\nberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,\n15\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-\nwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas,\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,\nand X. Zheng, “TensorFlow: Large-scale machine learn-\ning on heterogeneous systems,” 2015, software available\nfrom tensorﬂow.org.\n[61] D. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” arXiv preprint arXiv:1412.6980, 2014.\n[62] G. Brockman, V. Cheung, L. Pettersson, J. Schneider,\nJ. Schulman, J. Tang, and W. Zaremba, “Openai gym,”\n2016.\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2018-05-11",
  "updated": "2018-05-11"
}