{
  "id": "http://arxiv.org/abs/1805.07008v1",
  "title": "Hierarchical Reinforcement Learning with Deep Nested Agents",
  "authors": [
    "Marc Brittain",
    "Peng Wei"
  ],
  "abstract": "Deep hierarchical reinforcement learning has gained a lot of attention in\nrecent years due to its ability to produce state-of-the-art results in\nchallenging environments where non-hierarchical frameworks fail to learn useful\npolicies. However, as problem domains become more complex, deep hierarchical\nreinforcement learning can become inefficient, leading to longer convergence\ntimes and poor performance. We introduce the Deep Nested Agent framework, which\nis a variant of deep hierarchical reinforcement learning where information from\nthe main agent is propagated to the low level $nested$ agent by incorporating\nthis information into the nested agent's state. We demonstrate the\neffectiveness and performance of the Deep Nested Agent framework by applying it\nto three scenarios in Minecraft with comparisons to a deep non-hierarchical\nsingle agent framework, as well as, a deep hierarchical framework.",
  "text": "Hierarchical Reinforcement Learning with Deep\nNested Agents\nMarc W. Brittain∗\nDepartment of Aerospace Engineering\nIowa State University\nAmes, IA 50011\nmwb@iastate.edu\nPeng Wei†\nDepartment of Aerospace Engineering\nIowa State University\nAmes, IA 50011\npwei@iastate.edu\nAbstract\nDeep hierarchical reinforcement learning has gained a lot of attention in recent years\ndue to its ability to produce state-of-the-art results in challenging environments\nwhere non-hierarchical frameworks fail to learn useful policies. However, as\nproblem domains become more complex, deep hierarchical reinforcement learning\ncan become inefﬁcient, leading to longer convergence times and poor performance.\nWe introduce the Deep Nested Agent framework, which is a variant of deep\nhierarchical reinforcement learning where information from the main agent is\npropagated to the low level nested agent by incorporating this information into\nthe nested agent’s state. We demonstrate the effectiveness and performance of the\nDeep Nested Agent framework by applying it to three scenarios in Minecraft with\ncomparisons to a deep non-hierarchical single agent framework, as well as, a deep\nhierarchical framework.\n1\nIntroduction\nDeep reinforcement learning has recently attracted a large community due to recent successes like\nDeepmind’s agent AlphaGO [1]. AlphaGO was able to defeat the world champion Ke Jie in three\nmatches of GO! in May 2017, which shows the capability that deep reinforcement learning has for\nsolving challenging problems. A problem arises, however, in domains where the typical assumptions\nof reinforcement learning no longer hold. In environments where the Markovian assumption fails,\nactions taken early on can have long term effects that are unknown until the environment progresses\nfor some time which lead to sub-optimal convergence in these domains. To alleviate this problem, a\nhierarchical formulation of reinforcement learning can be applied to create an Markovian environment\nconsisting of a top-level and low-level agent (although more complex hierarchies consisting of many\nlow-level agents are also possible).\nThe framework we propose draws inspiration from deep hierarchical reinforcement learning formula-\ntions, where there is a main agent as well as a low-level agent known as the nested agent. The main\nagent is able to pass the information down to the nested agent in the form of state inclusion, where\nthe information from the main agent can be a policy, task, objective, etc.\nIn this paper we utilize three non-markovian scenarios in Minecraft to demonstrate the efﬁciency and\nperformance of the Deep Nested Agent framework, when compared to hierarchical reinforcement\nlearning and non-hierarchical single agent reinforcement learning. Minecraft is a popular open-world\ngenerated environment that has recently attracted the attention of AI researchers due to the ability\nto create controlled scenarios. In this environment, we create our own controlled scenarios in what\n∗Website: marcbrittain.github.io (Marc Brittain)\n†Website: http://www.aere.iastate.edu/∼pwei/ (Peng Wei)\nPreprint. Work in progress.\narXiv:1805.07008v1  [cs.AI]  18 May 2018\nwe call the arena to test our Deep Nested Agent framework on. Figure 1 shows a screen-shot of the\nMinecraft arena that we created.\nThe structure of this paper is as follows: in Section II, the background of reinforcement learning and\ndeep Q-networks will be introduced. In Section III, a review of the current state-of-the-art hierarchical\nreinforcement learning approaches will be introduced. Section IV presents our proposed Deep Nested\nAgent framework to solve this problem. The numerical experiments as well as the results are shown\nin Section V, with Section VI concluding this paper.\nFigure 1: Arena created within the Minecraft environment.\n2\nBackground\n2.1\nReinforcement Learning\nReinforcement learning is one type of sequential decision making where the goal is to learn how to\nact optimally in a given environment with unknown dynamics. A reinforcement learning problem\ninvolves an environment, an agent, and different actions the agent can take in this environment. The\nagent is unique to the environment and we assume the agent is only interacting with one environment.\nLet t represent the current time, then the components that make up a reinforcement learning problem\nare as follows:\n• S - The state space S is a set of all possible states in the environment\n• A - The action space A is a set of all actions the agent can take in the environment\n• r(st, at, st+1) - The reward function determines how much reward the agent is able to\nacquire for a given (st, at, st+1) transition\n• γ ∈[0, 1] - A discount factor determines how far in the future to look for rewards. As\nγ →0, only immediate rewards are considered, whereas, when γ →1, future rewards are\nprioritized.\nS contains all information about the environment and each element st can be considered a snapshot\nof the environment at time t. The agent accepts st and with this, the agent then determines an action,\nat. By taking action at, the state is now updated to st+1 and there is an associated reward from\nmaking the transition from st →st+1. How the state evolves from st →st+1 given action at is\ndependent upon the dynamics of the system, which is unknown. The reward function is user deﬁned,\nbut needs to be carefully designed to reﬂect the goal of the agent. Figure 1 shows the progression of a\nreinforcement learning problem.\nFrom this framework, the agent is able to learn the optimal decisions in each state of the environment\nby maximizing a cumulative reward function. We call the sequential actions the agent makes in\n2\nAgent\ns\n′\nr\nAgent\ns\na\ns\n′\nFigure 2: Progression of a reinforcement learning problem within an environment.\nthe environment a policy. Let π represent some policy and T represent the total time for a given\nenvironment, then the optimal policy can be deﬁned as:\nπ∗= arg max\nπ\nE[\nT\nX\nt=0\n(r(st, at, st+1)| π)].\n(1)\nIf we deﬁne the reward for actions we deem \"optimal\" very high, then by maximizing the total reward,\nwe have found the optimal solution to the problem.\n2.2\nQ-Learning\nOne of the most fundamental reinforcement learning algorithms is known as Q-learning. This popular\nlearning algorithm was introduced by Watkins [2] and the goal is to maximize a cumulative reward by\nselecting an appropriate action in each state. The idea of Q-learning is to estimate a value Q for each\nstate and action pair (s, a) in an environment that directly reﬂects the future reward associated with\ntaking such an action from this state. By doing this, we can extract the policy that reﬂects the optimal\nactions for an agent to take. The policy can be thought of as a mapping or a look-up table, where\nat each state, the policy tells the agent which action is the best one to take. During each learning\niteration, the Q-values are updated as follows:\nQ(st, at) ←Q(st, at) + α(r + γ max\nat+1 Q(st+1, at+1) −Q(st, at)).\n(2)\nIn equation (2), α represents the learning rate, r represents the reward for a given state and action,\nand γ represents the discount factor. One can see that in the maxat+1 Q(st+1, at+1) term, the idea is\nto determine the best possible future reward by taking this action.\n2.3\nDeep Q-Network (DQN)\nWhile Q-learning performs well in environments where the state-space is small, as the state-space\nbegins to increase, Q-leaning becomes intractable. It is because there is now a need for more\nexperience (more game episodes to be played) in the environment to allow convergence of the\nQ-values. To obtain Q-value estimates in environments where the state-space is large, the agent\nmust now generalize from limited experience to states that may have not been visited [3]. One of\nthe most widely used function approximation techniques for Q-learning is deep Q-networks (DQN),\nwhich involves using a neural network to approximate the Q-values for all the states. With standard\nQ-learning, the Q-value was a function of Q(s, a), but with DQN the Q-value is now a function of\nQ(s, a, θ), where θ is the parameters of the neural network. Given an n-dimensional state-space\nwith an m-dimensional action space, the neural network creates a map Rn →Rm. As mentioned\nby Van Hasselt et al. [4], incorporating a target network and experience replay are the two main\ningredients for DQN [5]. The target network with parameters θ−, is equivalent to the on-line network,\nbut the weights (θ−) are updated every τ time steps. The target used by DQN can then be written as:\nY DQN\nt\n= rt+1 + γ max\nat+1 Q(st+1, at+1; θ−\nt ).\n(3)\nThe idea of experience replay is that for a certain amount of time, observed transitions are stored\nand then sampled uniformly to update the network. By incorporating the target network, as well as,\nexperience replay, this can drastically improve the performance of the algorithm [5].\n3\n2.4\nDouble Deep Q-Network (DDQN)\nIn Q-learning and DQN there is the use of a max operator to select which action results in the largest\npotential future reward. Van Hasselt et al. [4] showed that due to this max operation, the network\nis more likely to overestimate the values, resulting in overoptimistic Q value estimations. The idea\nintroduced by Van Hasselt [6] was to decouple the max operation to prevent this overestimation\nto create what is called double deep Q-network (DDQN). To decouple the max operator, a second\nvalue function must be introduced, including a second network with weights θ′. During each training\niteration, one set of weights determines the greedy policy and the other then determine the Q-value\nassociated. Formulating equation (2) as a DDQN problem:\nY DDQN\nt\n= rt+1 + γQ(st+1, arg max\nat+1\nQ(st+1, at+1; θt); θ\n′\nt).\n(4)\nIn equation (3), it can be seen that the max operator has been removed and we are now including an\narg maxat+1 Q(st+1, at+1; θt) function to determine the best action due to the on-line weights. We\nthen use that action, along with the second set of weights to determine the estimated Q-value.\n3\nRelated Work\nLearning in complex, hierarchical environments is a challenging task and has attracted a lot of\nattention in recent publications [7–21]. One popular method for constructing hierarchical agents is\nbased off of work by [7, 8], known as the options framework. In this framework, a low-level agent,\nknown as the option, can be thought of as a sub-policy generated by the top-level agent that operates\nin an environment until a given termination criteria is met. A top-level agent picks an option, given\nits own policy, thus creating a hierarchical structure. It was mentioned in work by Vezhnevets et al.\n[21] that the options are typically learned using sub-goals and ’pseudo-rewards’ that are explicitly\ndeﬁned [7, 10, 9]. In recent publications, it was demonstrated that learning a selection rule among\npredeﬁned options using deep neural networks (DNN) delivers promising results in challenging,\ncomplex environments like Minecraft and Atari [22–24]; in addition, other research has shown that it\nis possible to learn options jointly with a policy-over-options end-to-end [21, 20]. In the hierarchical\nformulation by Vezhnevets et al. [21], their framework used a top-level agent which produces a\nmeaningful and explicit goal for the bottom level to achieve. In this framework, the authors were\nable to create sub-goals that emerge as directions in the latent space and are naturally diverse, which\ndiffers from the options framework.\nA key difference between our approach and the aforementioned approaches is that in our proposal\nthe main agent propagates information to the low-level nested agent that becomes included in the\nstate space of the nested agent. This decreases training time in complex environments and leads to\nsigniﬁcantly better performance (see section 5).\n4\nMethods\nWe now introduce the Deep Nested Agent framework, a variant of deep hierarchical reinforcement\nlearning where the information from the top level agent is propagated to the nested agent in the form\nof state augmentation. In this section we discuss how the Deep Nested Agent algorithm is formulated\nand provide pseudo-code for the algorithm (see Algorithm 1).\n4.1\nNested Agents\nNested Agents thrive in environments where there is explicit hierarchical structure. In these hierarchi-\ncal environments, there are typically different sets of actions that can be decoupled into one main\nagent action set and one nested agent action set (or many nested agent action sets) that operate at\ndifferent granularities of time or sequence. By doing this, the main agent can propagate information\nto the nested agent by adding an extra dimension to the state of the nested agent. For example,\nconsider a two-level hierarchical environment with two actions sets: A1, A2. If we assume that the\nactions in A1 operate at a slower time granularity, then we can assign A1 to the main agent, and A2\nto the nested agent as follows:\n4\naM ∈A1\naN ∈A2,\nwhere the subscript M corresponds to the main agent and N corresponds to the nested agent. Once\nthe action set is deﬁned, we can then construct the progression of information from the main agent to\nthe nested agent. Consider state sM for the main agent. Given an action aM, we can propagate the\ninformation to the nested agent as follows:\nsN = [sM, aM].\nThis framework is also applicable with inputs of images, where the nested agent will consist of one\nextra dimension as compared to the main agent. By avoiding the creation of many agents for main\nagent to choose from, there is much less training required for the nested agent as compared to current\ndeep hierarchical methods since we are only adding one dimension to the state of the nested agent. In\nmany of the current hierarchical formulations, a top-level agent chooses an action a ∈[1, ...n] which\nselects a speciﬁc agent, agenta. By only adding an extra dimension, the nested agent is able to learn\nmore efﬁciently in a given environment, as well as, achieve superior performance without having\nto add more agents, which leads to reduced memory consumption. Figure 3 shows the progress of\ninformation from the main agent to the nested agent and Algorithm 1 provides pseudo-code for the\nDeep Nested Agent framework. We can see the difference in time-granularity for the main agent\nand the nested agent in Algorithm 1 from the main agent choosing an action before entering into a\niterative loop that only involves the nested agent. We assumed that the main agent only takes one\ndecision before passing the information down to the nested agent, but in general, the main agent\ncould take more than one action before passing the information down the the nested agent.\nMain Agent\nsN = [sM, a]\nNested Agent\nsM\na\nsN\nFigure 3: Progression from main agent to nested agent.\n4.2\nExploration Vs. Exploitation\nIn our experiments using the Deep Nested Agent framework, we implemented the ϵ-greedy search\nstrategy. The problem with ϵ-greedy in a hierarchical reinforcement learning formulation is that if ϵ\nfor the main agent decays at the same rate as ϵ for the nested agent, the convergence will be much\nslower since the main agent operates at a different time granularity. By allowing ϵ to decay faster\nfor the main agent, this allows the main agent to take more greedy actions sooner, ensuring that the\nnested agent has more time to explore with the state containing the greedy main agent action. We also\nset the lower bound on ϵ to be greater for the main agent than that of the nested agent so that there is\nstill a probability of exploring the other actions of the main agent and their effect on the nested agent.\nAlgorithm 1 Deep Nested Agent\nInitialize: Main Agent\nInitialize: Nested Agent\nInitialize: sM\nreward = 0\nnumber of episodes = n\nfor i = 1 to n do\naM = ChooseAction(Main Agent)\nsN = [sM, aM]\nrepeat\naN = ChooseAction(Nested Agent)\ns′, rN = SimulateEnvironment\nreceive main agent reward = rM(s′)\nreward = reward + r\nupdate(Nested Agent)\nuntil Terminal\nupdate(Main Agent)\nend for\n5\n5\nExperiments\nWe experimented with the Nested Agent framework on three non-Markovian scenarios within the\nMinecraft environment that consisted of constructing different designs of varying difﬁculty: a vertical\nline, a zigzag, and a diamond (see Figure 4). In each scenario, the agent is only able to look in one\ndirection: forward. This increases the complexity of the problem, because the agent can not look in a\ndifferent direction and place a block, the agent can only place a block in front of where the agent is\nlocated. This is because we are currently using the Java version of Minecraft and are not using an\nAPI to interact with Minecraft. The blocks can be composed of two materials; wood or stone, and\nthere is an associated penalty reward for selected either of the two. The non-Markovian element of\nthe environment arises from the fact that the agent can only select to change material at the beginning\nof the episode and once the material is selected, it cannot be changed. Therefore, a decision early\non in the environment (changing material) will have an effect later on that is not noticeable in the\nbeginning. Our arena was composed of a 15 by 15 grid where each block in the grid corresponds to a\nblock in Minecraft.\n(a) Scenario 1: Constructing a vertical line.\n(b) Scenario 2: Constructing a zigzag.\n(c) Scenario 3: Constructing a diamond.\nFigure 4: Designs for the different scenarios.\n5.1\nImplementation\n5.1.1\nState-Space\nA state contains all the information the agent needs to make decisions. In our framework, the\nstate-space of the main agent is different from the state of the nested agent. For the main agent,\nthe information included in the state was the agent’s position (xi, yi) and the number of available\nmaterials remaining (b). The reason for including the number of available materials is because when\nall materials were used, the episode terminated and a new one began. In each scenario there was an\nupper-bound on the number of available materials that was equal to the number of required materials\nto build the design. For the nested agent, the state space included the same information but added on\nthe action of the main agent (aM).\n6\n5.1.2\nAction-Space\nAt each time-step, the main agent and the nested agent can make a decision to choose what material to\nuse, as well as, move and drop blocks, respectively. The only difference is the the decision time-step\nfor the main agent and nested agent. The main agent takes one action every episode, where an episode\nis deﬁned as one entire evolution of the environment. The nested agent takes actions throughout the\nepisode once the main agent has selected their action. The action-space for the main agent can be\ndeﬁned as follows:\nAM = [WoodBlocks, StoneBlocks],\nalong with the action-space for the nested agent:\nAN = [F, B, L, R, F + D, L + D, R + D, B + D],\nwhere F is move forward, B backward, L left, R right, and the addition of D means to place a block\ndown.\n5.1.3\nReward Function\nThe reward function for the main agent and nested agent needed to be designed to reﬂect the goal\nof the scenarios: constructing speciﬁc designs. We were able to capture our goals in the following\nreward functions for the main agent and the nested agent:\nMain Agent\nrM =\n15\nX\ni=1\n15\nX\nj=1\nI(xi, yj) + p,\nNested Agent\nrN(xi, yj) = I(xi, yj),\nwhere I is an indicator function deﬁned as:\nI(xi, yj) :=\n\u001a\n1;\nif K(xi, yj) = Shape(xi, yj)\n0;\notherwise\nand p is a constant that depends on the action of the main agent. If the main agent chooses the\nwood material, p = −5 and if the main agent chooses the stone material, p = 10. We also deﬁned\ntwo (15x15) matrices K(xi, yj) and Shape(xi, yj), where K contains the locations that the agent\nplaced a block down and Shape contains the locations of where the blocks should be placed from the\nscenario design (see Figure 4).\n5.2\nNested Agent Performance\nOur deep nested agent framework outperformed the deep hierarchical framework, as well as, the\ndeep non-hierarchical single agent framework in terms of training stability, converged score, and\ncomputational expense. A detailed explanation of the deep non-hierarchical single agent framework\nalong with the deep hierarchical framework are provided in the Appendix. The performance averaged\nover 10 trials through training is shown in Figure 5, where we evaluated the model after every 30\nepisodes. In all scenarios we allowed the agent to learn for 3,000 episodes. We can see how the\ndeep nested agent framework was able to achieve much better performance early on on in scenario\n1 and was ultimately able to achieve a greater converged score. In scenario 2 and scenario 3, the\nperformance of the deep nested agent and hierarchical agent were comparable, but with the deep\nnested agent algorithm there was a beneﬁt of not having to train an additional neural network. Both\nthe deep nested agent framework and the deep hierarchical framework outperformed the deep non-\nhierarchical single agent framework in all scenarios. By using this nested agent framework, we\navoided having to train separate neural networks for each action of the main agent, which was less\ncomputationally expensive.\n7\n(a) Scenario 1.\n(b) Scenario 2.\n(c) Scenario 3.\nFigure 5: Score throughout training on the scenarios.\n6\nConclusion and Future Work\nWe introduced the deep nested agent framework, a variant of deep hierarchical reinforcement learning\nwhere the action of the main agent is included in the state of the nested agent. We found that the\nperformance of the deep nested agent framework outperforms the deep hierarchical framework, as\nwell as, the non-hierarchical single agent framework in environments that exhibit non-Markovian\nproperties based on the stability of learning, the computational complexity, and the converged score\nin the scenarios. We also found that we reduced the computational demand for this problem by\neliminating the need for training additional neural network models for each action of the main agent.\nBy adding the action of the main agent to the state of the nested agent, we only had to train two\nDDQN models, compared with three for the deep hierarchical framework (this value increases for\neach unique main agent action). As future work, we are applying this framework to multi-agent\nscenarios where there are many nested agents that are cooperating with each other. We are particularly\ninterested in this problem since the convergence in multi-agent scenarios is not guaranteed and we\nbelieve the deep nested agent framework will be able to improve the current state-of-the-art results.\nIn addition, we plan to apply this framework to more complex environments where the main agent\nhas many actions to take. In these environments, as the number of main agent actions increases, we\nbelieve the performance will increase as well, when comparing to the deep hierarchical framework\nand the deep non-hierarchical single agent framework. We envision our deep nested agent framework\nwill soon be able to solve more complex problems, as well as, achieve better performance while\nreducing the required computational resources.\n8\nReferences\n[1] Deepmind. Alphago at the future of go summit, 23-27 may 2017, 2017. URL https:\n//deepmind.com/research/alphago/alphago-china/.\n[2] Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis,\nKing’s College, Cambridge, 1989.\n[3] Mykel J. Kochenderfer, Christopher Amato, Girish Chowdhary, Jonathan P. How, Hayley\nJ. Davison Reynolds, Jason R. Thornton, Pedro A. Torres-Carrasquillo, N. Kemal Üre, and\nJohn Vian. Decision Making Under Uncertainty: Theory and Application. The MIT Press, 1st\nedition, 2015. ISBN 0262029251, 9780262029254.\n[4] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double\nq-learning. In AAAI, volume 16, pages 2094–2100, 2016.\n[5] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n[6] Hado Van Hasselt. Double q-learning. In Advances in Neural Information Processing Systems,\npages 2613–2621, 2010.\n[7] Richard S. Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A\nframework for temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):\n181–211, 1999.\n[8] Doina Precup. Temporal Abstraction in Reinforcement Learning. University of Massachusetts\nAmherst, 2000.\n[9] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural\ninformation processing systems, pages 271–278, 1993.\n[10] Thomas G Dietterich. Hierarchical reinforcement learning with the maxq value function\ndecomposition. J. Artif. Intell. Res.(JAIR), 13(1):227–303, 2000.\n[11] Craig Boutilier, Ronen I Brafman, and Christopher Geib. Prioritized goal decomposition of\nmarkov decision processes: Toward a synthesis of classical and decision theoretic planning. In\nIJCAI, pages 1156–1162, 1997.\n[12] Peter Dayan. Improving generalization for temporal difference learning: The successor repre-\nsentation. Neural Computation, 5(4):613–624, 1993.\n[13] Leslie Pack Kaelbling. Hierarchical learning in stochastic domains: Preliminary results. In\nProceedings of the tenth international conference on machine learning, volume 951, pages\n167–173, 1993.\n[14] Ronald Parr and Stuart J Russell. Reinforcement learning with hierarchies of machines. In\nAdvances in neural information processing systems, pages 1043–1049, 1998.\n[15] Doina Precup, Richard S Sutton, and Satinder P Singh. Planning with closed-loop macro actions.\nIn Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems,\npages 70–76, 1997.\n[16] Jürgen Schmidhuber. Neural sequence chunkers. 1991.\n[17] Richard S. Sutton. Td models: Modeling the world at a mixture of time scales. In Machine\nLearning Proceedings 1995, pages 531–539. Elsevier, 1995.\n[18] Marco Wiering and Jürgen Schmidhuber. Hq-learning. Adaptive Behavior, 6(2):219–246, 1997.\n[19] Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John\nAgapiou, et al. Strategic attentive writer for learning macro-actions. In Advances in neural\ninformation processing systems, pages 3486–3494, 2016.\n9\n[20] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pages\n1726–1734, 2017.\n[21] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg,\nDavid Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning.\narXiv preprint arXiv:1703.01161, 2017.\n[22] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. A deep\nhierarchical approach to lifelong learning in minecraft. In AAAI, volume 3, page 6, 2017.\n[23] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical\ndeep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In\nAdvances in neural information processing systems, pages 3675–3683, 2016.\n[24] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization\nwith multi-task deep reinforcement learning. arXiv preprint arXiv:1706.05064, 2017.\n10\n7\nAppendix\n7.1\nExperiment Setup and Results\nAll code for the experiments were run in python 3.6 using keras with tensorﬂow compiled from source.\nThe experiments were run on a Ubuntu workstation with a single 12gb memory 1080x TI GPU with\n32gb of RAM. We used the JAVA version of Minecraft, so all code had to be designed to know where\nto click on the game screen to start, with all actions being programmed as time-interval key strokes.\nAs mentioned in the paper, we ran each model (non-hierarchical single agent, hierarchical agent, and\nnested agent) for 3,000 episodes and repeated the training for 10 trials to average the results. This\nled to a more interpretable result instead of reporting one single trial. I can be seen that the models\nwere unable to obtain the optimal solution in scenario 2 and scenario 3. We believe this is due to the\nnumber of episodes being set to 3,000 and with more training the deep nested agent model and the\ndeep hierarchical agent model should be able to obtain the optimal solution.\n7.1.1\nDeep Non-Hierarchical Single Agent\nTo construct the deep non-hierarchical single agent, we utilized the DDQN concept that was mentioned\nearlier in the paper. Since the agent did not have any hierarchical properties, the action set of the\nmain agent and nested agent were combined into one action set for the single agent. Because the\naction of choosing material was time-sensitive, if the single agent did not choose the material ﬁrst,\nthen the episode would terminate.\n7.1.2\nDeep Hierarchical Agent\nIn the deep hierarchical agent algorithm, we had three total agents: one top-level agent and two\nlow-level agents. The top-level agent’s action was to select the low-level agent to construct the\nscenario’s design. Each low-level agent was assigned a material to build, so one agent could build\nwith stone and the other agent could build with wood. No information of the top-level agent was\nincluded in the state of the low-level agent and in this scenario we had to train an additional neural\nnetwork since we have two low-level agents instead of one like in the deep nested agent algorithm.\n7.2\nHyper-parameters\nWe used an experience replay memory length of one-million with no priority, but in the future we will\ncompare our results using prioritized experience replay. Our neural network model was composed\nof a simple 4-layer network with two hidden layers. Each hidden layer had 32 nodes and we used a\nbatch size of 32 as well. In all of the experiments we used the TanH activation function in training\nour neural network. Each agent was trained using the DDQN architecture that was introduced in\nSection 2 with the addition of the deep nested agent framework. We also used the ϵ −greedy search\nstrategy with ϵ decaying faster for the main agent and ϵ decaying slower for the nested agent. ϵ was\nlinearly decayed from 1.0 →0.001 in all experiments for the main agent and nested agent.\n11\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2018-05-18",
  "updated": "2018-05-18"
}