{
  "id": "http://arxiv.org/abs/1805.07504v2",
  "title": "Deep Loopy Neural Network Model for Graph Structured Data Representation Learning",
  "authors": [
    "Jiawei Zhang"
  ],
  "abstract": "Existing deep learning models may encounter great challenges in handling\ngraph structured data. In this paper, we introduce a new deep learning model\nfor graph data specifically, namely the deep loopy neural network.\nSignificantly different from the previous deep models, inside the deep loopy\nneural network, there exist a large number of loops created by the extensive\nconnections among nodes in the input graph data, which makes model learning an\ninfeasible task. To resolve such a problem, in this paper, we will introduce a\nnew learning algorithm for the deep loopy neural network specifically. Instead\nof learning the model variables based on the original model, in the proposed\nlearning algorithm, errors will be back-propagated through the edges in a group\nof extracted spanning trees. Extensive numerical experiments have been done on\nseveral real-world graph datasets, and the experimental results demonstrate the\neffectiveness of both the proposed model and the learning algorithm in handling\ngraph data.",
  "text": "Deep Loopy Neural Network Model for Graph\nStructured Data Representation Learning\nJiawei Zhang\n⋆IFM Lab, Florida State University, FL, USA\njzhang@cs.fsu.edu\nAbstract\nExisting deep learning models may encounter great challenges in handling graph\nstructured data. In this paper, we introduce a new deep learning model for graph\ndata speciﬁcally, namely the deep loopy neural network. Signiﬁcantly different\nfrom the previous deep models, inside the deep loopy neural network, there exist\na large number of loops created by the extensive connections among nodes in the\ninput graph data, which makes model learning an infeasible task. To resolve such\na problem, in this paper, we will introduce a new learning algorithm for the deep\nloopy neural network speciﬁcally. Instead of learning the model variables based\non the original model, in the proposed learning algorithm, errors will be back-\npropagated through the edges in a group of extracted spanning trees. Extensive\nnumerical experiments have been done on several real-world graph datasets, and\nthe experimental results demonstrate the effectiveness of both the proposed model\nand the learning algorithm in handling graph data.\n1\nIntroduction\nFormally, a loopy neural network denotes a neural network model involving loops among neurons in\nits architecture. Deep loopy neural network is a novel learning model proposed for graph structured\ndata speciﬁcally in this paper. Given a graph data G = (V, E) (where V and E denote the node and\nedge sets respectively), the architecture of deep neural nets constructed for G can be described as\nfollows:\nDEFINITION 1 (Deep Loopy Neural Network): Formally, we can represent a deep loopy neural\nnetwork model constructed for network G = (V, E) as G = (N, L), where N covers the set of\nneurons and L includes the connections among the neurons deﬁned based on graph G.\nMore speciﬁcally, the neurons covered in set N can be categorized into several layers: (1) input\nlayer X, (2) hidden layers H = Sk\nl=1 H(l), and (3) output layer Y, where X = {xi}vi∈V, H(l) =\n{h(l)\ni }vi∈V, ∀l ∈{1, 2, · · · , k}, Y = {yi}vi∈V and k denotes the hidden layer depth. Vector xi ∈\nRn, h(l)\ni\n∈Rm(l) and yi ∈Rd denote the feature, hidden state (at layer l) and label vectors of\nnode vi ∈V in the input graph respectively. Meanwhile, the connections among the neurons in L\ncan be divided into (1) intra-layer neuron connections La (which connect neurons within the same\nlayers), and (2) inter-layer neuron connections Le (which connect neurons across layers), where\nLa = Sk\nl=1{(h(l)\ni , h(l)\nj )}(vi,vj)∈E and Le = Lx,h1 ∪Lh1,h2 ∪· · · ∪Lhk,y. In the notation, Lx,h1\ncovers the connections between neurons in the input layer and the ﬁrst hidden layer, and so forth for\nthe other neuron connection sets.\nIn\nFigure\n1,\nwe\nshow\nan\nexample\nof\na\ndeep\nloopy\nneural\nnetwork\nmodel\ncon-\nstructed for the input graph as shown in the left plot.\nIn the example,\nthe input\nPreprint. Work in progress.\narXiv:1805.07504v2  [cs.LG]  5 Sep 2019\nx1\nx2\nx3\nx4\nx5\nx6\ny6\ny5\ny4\ny3\ny2\ny1\nh1\nh2\nh3\nh4\nh5\nh6\nh1\nh2\nh3\nh4\nh5\nh6\nOutput \nLayer\nInput \nLayer\nHidden \nLayer 1\nHidden \nLayer K\n1\n1\n1\n1\n1\n1\nK\nK\nK\nK\nK\nK\n…\n…\n…\n…\n…\n…\n…\nv1\nv2\nv3\nv4\nv5\nv6\nInput Graph\nFigure 1: Overall Architecture of Deep Loopy Neural Network Model.\ngraph can be represented as G\n=\n(V, E),\nwhere V\n=\n{v1, v2, · · · , v6} and E\n=\n{(v1, v2), (v1, v3), (v1, v4), (v2, v3), (v3, v4), (v4, v5), (v5, v6)}. The constructed deep loopy neu-\nral network model involves k layers and the intra-layer neuron connections mainly exist in these\nhidden layers respectively.\nFormally, given the node input feature vector xi for node vi, we can represents its hidden states at\ndifferent hidden layers and the output label as follows:\n\n\n\n\n\n\n\n\n\nh(1)\ni\n= σ\n\u0000Wxxi + bx + P\nvj∈Γ(vi)(Wh1hj + bh1)\n\u0001\n,\n· · ·\nh(k)\ni\n= σ\n\u0000Whk−1,hkh(k−1)\ni\n+ bhk−1,hk + P\nvj∈Γ(vi)(Whkhj + bhk)\n\u0001\n,\nyi\n= σ(Wyh(k)\ni\n+ by),\n(1)\nwhere set Γ(vi) = {vj|vj ∈V ∧(vi, vj) ∈E} denotes the neighbors of node vi in the input graph.\nCompared against the true label vector of nodes in the network, e.g., ˆyi for vi ∈V, we can represent\nthe introduced loss by the model on the input graph as\nE(G) =\nX\nvi∈V\nE(vi) =\nX\nvi∈V\nloss(ˆyi, yi),\n(2)\nwhere different loss functions can be adopted here, e.g., mean square loss or cross-entropy loss.\nBy minimizing the loss function, we will be able to learn the variables involved in the model. By\nthis context so far, most of the deep neural network model training is based on the error back prop-\nagation algorithm. However, applying the error back propagation algorithm to train the deep loopy\nneural network model will encounter great challenges due to the extensive variable dependence\nrelationships created by the loops, which will be illustrated in great detail later.\nThe following part of this paper is organized as follows. In Section 2, we will introduce the existing\nrelated works to this paper. We will analyze the challenges in learning deep loop neural networks\nwith error back-propagation algorithm in Section 3, and a new learning algorithm will be introduced\nin Section 4. Extensive numerical experiments will be provided to evaluate the model performance\nin Section 5, and ﬁnally we will conclude this paper in Section 6.\n2\nRelated Works\nTwo research topics are closely related to this paper, including deep learning and network represen-\ntation learning, and we will provide a brief overview of the existing papers published on these two\ntopics as follows.\nDeep Learning Research and Applications: The essence of deep learning is to compute hierar-\nchical features or representations of the observational data [8, 16]. With the surge of deep learning\n2\nresearch and applications in recent years, lots of research works have appeared to apply the deep\nlearning methods, like deep belief network [12], deep Boltzmann machine [24], deep neural net-\nwork [13,15] and deep autoencoder model [26], in various applications, like speech and audio pro-\ncessing [7, 11], language modeling and processing [1, 19], information retrieval [10, 24], objective\nrecognition and computer vision [16], as well as multimodal and multi-task learning [29,30].\nNetwork Embedding: Network embedding has become a very hot research problem recently, which\ncan project a graph-structured data to the feature vector representations. In graphs, the relation can\nbe treated as a translation of the entities, and many translation based embedding models have been\nproposed, like TransE [2], TransH [28] and TransR [17]. In recent years, many network embedding\nworks based on random walk model and deep learning models have been introduced, like Deepwalk\n[21], LINE [25], node2vec [9], HNE [4] and DNE [27]. Perozzi et al. extends the word2vec model\n[18] to the network scenario and introduce the Deepwalk algorithm [21]. Tang et al. [25] propose\nto embed the networks with LINE algorithm, which can preserve both the local and global network\nstructures. Grover et al. [9] introduce a ﬂexible notion of a node’s network neighborhood and design\na biased random walk procedure to sample the neighbors. Chang et al. [4] learn the embedding of\nnetworks involving text and image information. Chen et al. [6] introduce a task guided embedding\nmodel to learn the representations for the author identiﬁcation problem.\n3\nLearning Challenges Analysis of Deep Loopy Neural Network\nIn this part, we will analyze the challenges in training the deep loop neural network with traditional\nerror back propagation algorithm. To simplify the settings, we assume the deep loop neural network\nhas only one hidden layer, i.e., k = 1. According to the deﬁnition of the deep loopy neural network\nmodel provided in Section 1, we can represent the inferred labels for graph nodes, e.g., vi ∈V, as\nvector yi = [yi,1, yi,2, · · · , yi,d]⊤, and its jth entry yi(j) (or yi,j) can be denoted as\n(\nyi(j)= σ\u0000 Pm\ne=1 Wy(e, j) · hi(e) + by(j)\u0001\n,\nhi(e)= σ\u0000 Pn\nc=1 Wx(c, e) · xi(c) + bx(e) + P\nvp∈Γ(vi)\nPm\nf=1 Wh(f, e) · hp(f) + bh(e)\u0001\n.\n(3)\nHere, we will use the mean square error as an example of the loss function, and the loss introduced\nby the model for node vi ∈V compared with the ground truth label ˆyi can be represented as\nE(vi) = 1\n2 ∥yi −ˆyi∥2\n2 = 1\n2\nd\nX\nj=1\n(yi(j) −ˆyi(j))2 .\n(4)\n3.1\nLearning Output Layer Variables\nThe variables involved in the deep loopy neural network model can be learned by error back propa-\ngation algorithm. For instance, here we will use SGD as the optimization algorithm. Given the node\nvi and its introduced error function E(vi), we can represent the updating equation for the variables\nin the output layer, e.g., Wy(e, j) and by(j), as follows:\n\n\n\nWy\nτ(e, j)\n= Wy\nτ−1(e, j) −ηw\nτ ·\n∂E(vi)\n∂Wy\nτ−1(e,j),\nby\nτ(j)\n= by\nτ−1(j) −ηb\nτ ·\n∂E(vi)\n∂by\nτ−1(j).\n(5)\nwhere ηw\nτ and ηb\nτ denote the learning rates in updating Wy and by at iteration τ respectively.\nAccording to the derivative chain rule, we can represent the partial derivative terms\n∂E(vi)\n∂Wy\nτ−1(e,j) and\n∂E(vi)\n∂by\n( j) as follows respectively:\n\n\n\n\n\n∂E(vi)\n∂Wy(e,j)\n= ∂E(vi)\n∂yi(j) · ∂yi(j)\n∂zh\ni (j) ·\n∂zh\ni (j)\n∂Wy(e,j) =\n\u0000yi(j) −ˆyi(j)\n\u0001\n· yi(j)\n\u00001 −yi(j)\n\u0001\n· hi(e),\n∂E(vi)\n∂by(j)\n= ∂E(vi)\n∂yi(j) · ∂yi(j)\n∂zh\ni (j) · ∂zh\ni (j)\n∂by(j) =\n\u0000yi(j) −ˆyi(j)\n\u0001\n· yi(j)\n\u00001 −yi(j)\n\u0001\n· 1,\n(6)\nwhere term zh\ni (j) = Pm\ne=1 Wy(e, j) · hi(e) + by(j).\n3\n3.2\nLearning Hidden Layer and Input Layer Variables\nMeanwhile, when updating the variables in the hidden and output layers, we will encounter great\nchallenges in computing the partial derivatives of the error function regarding these variables. Given\ntwo connected nodes vi, vj ∈V, where (vi, vj) ∈E, from the graph, we have the representation of\ntheir hidden state vectors hi and hj as follows:\nhi = σ\n\u0000Wxxi + bx + Whhj + bh +\nX\nvk∈Γ(vi)\\{vj}\n(Whhk + bh)\n\u0001\n,\n(7)\nhj = σ\n\u0000Wxxj + bx + Whhi + bh +\nX\nv′\nk∈Γ(vj)\\{vi}\n(Whhk′ + bh)\n\u0001\n.\n(8)\nWe can observe that hi and hj co-rely on each other in the computation, whose representation will be\ninvolved in an inﬁnite recursive deﬁnition. When we compute the partial derivative of error function\nregarding variable Wx, bx or Wh, bh, we will have an inﬁnite partial derivative sequence involving\nhi and hj according to the chair rule. The problem will be much more serious for connected graphs,\nas illustrated by the following theorem.\nTHEOREM 1 Let G denote the input graph. If graph G is connected (i.e., there exist a path con-\nnecting any pairs of nodes in the graph), for any node vi in the graph, its hidden state vector hi will\nbe involved in the hidden state representation of all the other nodes in the graph.\nPROOF 1 The Theorem can be proved via contradiction.\nHere, we assume hidden state vector hi is not involved in the hidden representation of a certain\nnode vj, i.e., hj. Formally, given the neighbor set Γ(vj) of node vj in the network, we know that\nvector hj is deﬁned based on the hidden state vectors {hk}vk∈Γ(vi). Therefore, we can assert that\nvector hi should also not be involved in the representation of all the nodes in Γ(vj). Viewed in this\nperspective, we can also show that hi is not involved in the hidden state representations of the 2-hop\nneighbors of node vj, as well as the 3-hop, and so forth.\nConsidering that graph G is connected, we know there should exist a path of length p connecting vj\nwith vi, i.e., vi will be a p-hop neighbor of node vj, which will contract the claim hi is not involved\nin the hidden state representations of the p-hop neighbors of node vj. Therefore, the assumption we\nmake at the beginning doesn’t hold and we will prove the theorem.\nAccording to the above theorem, when we try to compute the derivative of the error function regard-\ning variable Wx, we will have\n∂E(vi)\n∂Wx = ∂E(vi)\n∂yi\n· ∂yi\n∂zh\ni\n· ∂zh\ni\n∂hi\n·\n\u0012 ∂hi\n∂Wx +\nX\nvj∈V\n∂hi\n∂hj\n·\n\u0010 ∂hj\n∂Wx +\nX\nvk∈V\n∂hj\n∂hk\n·\n\u0000· · ·\n\u0001\u0011\u0013\n(9)\nDue to the recursive deﬁnition of the hidden state vector {hi}vi∈V in the network, the partial deriva-\ntive of term ∂E(vi)\n∂Wx will be impossible to compute mathematically. The partial derivative sequence\nwill extend to an inﬁnite length according to Theorem 1. Similar phenomena can be observed when\ncomputing the partial derivative of the error function regarding variables bx or Wh, bh.\n4\nProposed Method\nTo resolve the challenges introduced in the previous section, in this part, we will propose an ap-\nproximation algorithm to learn the deep loopy neural network model. Given the complete model\narchitecture (which is also in a graph shape), to differentiate it from the input graph data, we will\nname it as the model graph formally, the neuron vectored involved in which are called the neuron\nnodes by default. From the model graph, we propose to extract a set of tree structured model dia-\ngrams rooted at certain neuron states in the model graph. Model learning will be mainly performed\non these extracted rooted trees instead.\n4.1\ng-Hop Model Subgraph and Rooted Spanning Tree Extraction\nGiven the deep loopy neural network model graph, involving the input, output and hidden state\nvariables and the projections parameterized by the variables to be learned, we propose to extract a\n4\nx1\nx2\nx3\nx4\ny4\ny3\ny2\ny1\nh1\nh2\nh3\nh4\nh5\nx1\nx2\nx3\nx4\nh1\nh2\nh3\nh4\nh5\nh1\nh3\nh2 h4\nh1\nh3\nh1\nx1\nx2\nx3\nx4\ny4\ny3\ny2\ny1\nh1\nh2\nh3\nh4\nh5\nh6\nx6\nx5\ny5\ny6\n3-hop subgraph\n3-hop rooted tree\ny1\nx1\nx1\nx1\nx3\nx3\nx2 x4\nx5\nFigure 2: Structure of 3-Hop Subgraph and Rooted Spanning Tree at y1. (The bottom feature nodes\nin the gray component attached to the tree leaves are append for model learning purposes.)\nset of g-hop subgraph (g-subgraph) around certain target neuron node from it, where all the neuron\nnodes involved are within g hops from the target neuron node.\nDEFINITION 2 (g-hop subgraph): Given the deep loopy neural network model graph G = (N, L)\nand a target neuron node, e.g., n ∈N, we can represent the extracted g-subgraph rooted at n as\nGn = (Nn, Ln), where Nn ⊂N and Ln ⊂L. For all the nodes in set Nn, formed by links in Lyi,\nthere exist a directed path of length less than g connecting the neuron node n with them.\nFor instance, from the deep loopy neural network model graph (with 1 hidden layer) as shown in the\nleft plot of Figure 2, we can extract a 3-subgraph rooted at neuron node y1 as shown in the central\nplot. In the sub-graph, it contains the feature, label and hidden state vectors of nodes v2, v3, v4 as\nwell as the hidden state vector of v5, whose feature and label vectors are not included since they are\n4-hops away from h1.\nIn the model graph, the link direction denote the forward propagation direction. In the model learn-\ning process, the error information will propagate from the output layer, i.e., the label neuron nodes,\nbackward to the hidden state and input neuron nodes along the reversed direction of these links. For\ninstance, given the target node y1, its error can be propagated to h1, h2, · · · , h5 and x1, x2, x3, x4,\nbut cannot reach y2, y3 and y4. Meanwhile, to resolve the recursive partial derivative problem in-\ntroduced in the previous section, in this paper, we propose to further extract a g-hop rooted spanning\ntree (g-tree) from the g-subgraph. The extracted g-tree will be acyclic and all the involved links are\nfrom the children nodes to the parent nodes only.\nDEFINITION 3 (g-hop rooted spanning tree): Given the extracted g-subgraph around the target\nneuron node n, i.e., Gn = (Nn, Ln), we can represent its g-tree as Tn = (Sn, Rn, n), where n is\nthe root, sets Sn ⊂Nn and Rn ⊂Ln. All the links in Pn are pointing from the children nodes to\nthe parent nodes.\nFor instance, in the right plot of Figure 2, we display an example of the extracted 3-tree rooted at\nneuron node y1. From the plot, we observe that all the label vectors are removed, since there exist\nno directed edges from them to the root node. Among all the nodes, h1 is 1-hop away from y1,\nx1, h2, h3 and h4 are 2-hop away from y1, and all the remaining nodes are 3 hops away from h1.\nGiven any two pair of connected nodes in the 3-tree, e.g., x1 and y1 (or h4 and h5), the edges\nconnecting them clearly indicate the variables to be learned via error back propagation across them.\nWhen learning the loopy neural network, instead of using the whole network, we propose to back\npropagate the errors from the root, e.g., y1, to the remaining nodes in the extracted 3-tree.\nTHEOREM 2 Given g = ∞, the learning process based on g-tree will be identical as learning based\non the original network. Meanwhile, in the case when g is a ﬁnite number and g ≥diameter(G), the\ng-tree of any nodes will actually cover all the neuron nodes and variables to be learned.\nThe proof to the above theorem will not be introduced here due to the limited space. Generally,\nlarger g can preserve more complete network structure information. However, on the other hand, as\n5\ng increases, the paths involved in the g-tree from the leaf node to the root node will also be longer,\nwhich may lead to the gradient vanishing/exploding problem as well [20].\n4.2\nRooted Spanning Tree based Learning Algorithm for Loopy Neural Network Model\nBased on the extracted g-tree, the deep loopy neural network model can be effectively trained,\nand in this section we will introduce the general learning algorithm in detail.\nFormally, let\nTn = (Sn, Rn, n) denote an extracted spanning tree rooted at neuron node n. Based on the er-\nrors computed on node n (if n is in the output layer) or the errors propagated to n, we can further\nback propagate the errors to the remaining nodes in Sn \\ {n}.\nFormally, from the spanning tree Tn = (Sn, Rn, n), we can deﬁne the set of variables involved as\nW, which can be indicated by the links in set Rn. For instance, given the 3-tree as shown in Figure 2,\nwe know that there exist three types of variables involved in the tree diagram, where the variable set\nW = {Wx, bx, Wy, by, Wh, bh}. For the spanning trees extracted from deeper neural network\nmodels, the variable type set will be much larger. Meanwhile, given a random node m ∈Sn, we\nwill use notation Tn(m) to denote a subtree of Tn rooted at m, and notation Γ(m) to represent the\nchildren neuron nodes of m in Tn. Furthermore, for simplicity, we will use notation W ∈T to\ndenote that variable W ∈W is involved in the tree or sub-tree T .\nGiven the spanning tree Tn together with the errors E(n) computed at n (or propagated to n),\nregarding a variable W ∈W, we can ﬁrst deﬁne a basic learning operation between neuron nodes\nx, y ∈Sn (where y ∈Γ(x)) as follows:\nPROP(x, y; W) =\n(\n1\n\u0000W ∈Tn(m)\n\u0001 ∂x\n∂W,\nif y is a leaf node;\n1\n\u0000W ∈Tn(m)\n\u0001 ∂x\n∂y ·\n\u0010 P\nz∈Γ(y) PROP(y, z; W)\n\u0011\n,\notherwise.\n(10)\nBased on the above operation, we can represent the partial derivative of the error function E(n)\nregarding variable W as:\n∂E(n)\n∂W\n= ∂E(n)\n∂n\n·\n\u0000X\nm∈Γ(n)\nPROP(n, m; W)\n\u0001\n(11)\nTHEOREM 3 Formally, given a g-hop rooted spanning tree Tn, the PROP(x, y; W) operation will\nbe called at most (dmax+1)k+1−dmax−1\ndmax\ntimes in computing the partial derivative term ∂E(n)\n∂W , where\ndmax denotes the largest node degree in the original input graph data G.\nPROOF 2 Operation PROP(x, y; W) will be called for each node in the spanning tree Tn except\nthe root node. Formally, given a max node degree dmax in the original graph, the largest number of\nchildren node connected to a random neuron node in the spanning tree will be dmax+1 (+1 because\nof the connection from the lower layer neuron node). The maximum number of nodes (except the\nroot node) in the spanning tree of depth g can be denoted as the sum\n(dmax + 1) + (dmax + 1)2 + (dmax + 1)3 + · · · + (dmax + 1)g,\n(12)\nwhich equals to (dmax+1)g+1−dmax−1\ndmax\nas indicated in the theorem.\nConsidering that in the model, the hidden neuron nodes are computed based on the lower-level neu-\nrons and they don’t have input representations. To make the model learnable, as indicated by the\nbottom gray layer below the g-tree in Figure 2, we propose to append the input feature representa-\ntions to the g-tree leaf nodes, based on which we will be able to compute the node hidden states. The\nlearning process will involve several epochs until convergence, where each epoch will enumerate all\nthe nodes in the graph once. To further boost the convergence rate, several latest optimization algo-\nrithms, e.g., Adam [14], can be adopted to replace traditional SGD in updating the model variables.\n5\nNumerical Experiments\nTo test the effectiveness of the proposed deep loopy neural network and the learning algorithm, ex-\ntensive numerical experiments have been done on several frequently used graph benchmark datasets,\n6\nTable 1: Experimental Results on the Douban Graph Dataset.\nDouban\nmethods\nMSE\nMAE\nLRS\navg. rank\nLNN\n0.035±0.0 (1)\n0.067±0.001 (1)\n0.102±0.002 (1)\n(1)\nDEEPWALK [21]\n0.05±0.0 (7)\n0.097±0.0 (7)\n0.129±0.003 (8)\n(7)\nWALKLETS [22]\n0.046±0.001 (3)\n0.087±0.001 (3)\n0.108±0.001 (3)\n(2)\nLINE [25]\n0.048±0.0 (6)\n0.091±0.001 (6)\n0.12±0.003 (6)\n(6)\nHPE [5]\n0.046±0.001 (3)\n0.077±0.001 (2)\n0.111±0.002 (4)\n(2)\nAPP [31]\n0.05±0.0 (8)\n0.099±0.0 (8)\n0.127±0.002 (7)\n(8)\nMF [3]\n0.045±0.0 (2)\n0.089±0.001 (5)\n0.102±0.001 (1)\n(4)\nBPR [23]\n0.046±0.0 (3)\n0.089±0.0 (4)\n0.114±0.002 (5)\n(5)\nTable 2: Experimental Results on the IMDB Graph Dataset.\nIMDB\nmethods\nMSE\nMAE\nLRS\navg. rank\nLNN\n0.046±0.0 (1)\n0.090±0.001 (1)\n0.116±0.002 (1)\n(1)\nDEEPWALK [21]\n0.065±0.0 (7)\n0.128±0.001 (8)\n0.187±0.002 (7)\n(7)\nWALKLETS [22]\n0.057±0.0 (4)\n0.112±0.001 (3)\n0.139±0.003 (4)\n(3)\nLINE [25]\n0.061±0.0 (6)\n0.121±0.001 (7)\n0.169±0.004 (6)\n(6)\nHPE [5]\n0.055±0.0 (2)\n0.107±0.001 (2)\n0.127±0.003 (2)\n(2)\nAPP [31]\n0.066±0.0 (8)\n0.13±0.001 (6)\n0.193±0.002 (8)\n(7)\nMF [3]\n0.055±0.0 (2)\n0.112±0.001 (3)\n0.129±0.003 (3)\n(3)\nBPR [23]\n0.058±0.0 (5)\n0.117±0.0 (5)\n0.157±0.002 (5)\n(5)\nincluding two social networks: Foursquare and Twitter, as well as two knowledge graphs: Douban\nand IMDB, which will be introduced in this section.\n5.1\nExperimental Settings\nThe basic statistics information about the graph datasets used in the experiments are as follows:\n• Douban: Number of Nodes: 11, 297; Number of Links: 753, 940.\n• IMDB: Number of Nodes: 13, 896; Number of Links: 1, 404, 202.\n• Foursquare: Number of Nodes: 5, 392; Number of Links: 111, 852.\n• Twitter: Number of Nodes: 5, 120; Number of Links: 261, 151.\nBoth Foursquare and Twitter are online social networks. The nodes and links involved in Foursquare\nand Twitter denote the users and their friendships respectively. Douban and IMDB are two movie\nknowledge graphs, where the nodes denote the movies. Based on the movie cast information, we\nconstruct the connections among the movies, where two movies can be linked if they share a com-\nmon cast member. Besides the network structure information, a group of node attributes can also\nbe collected for users and movies, which cover the user and movie basic proﬁle information respec-\ntively. Based on the attribute information, a set of features can be extracted as the node input feature\ninformation. In the experiments, we use the movie genre and user hometown state as the labels.\nThese labeled nodes are partitioned into training and testing sets via 5-fold cross validation: 4-fold\nfor training, and 1-fold for testing.\nMeanwhile, to demonstrate the advantages of the learned deep loopy neural network against the\nother existing network embedding models, many baseline methods are compared with deep loopy\nneural network in the experiments. The baseline methods cover the state-of-the-art methods on\ngraph data representation learning published in recent years, which include DEEPWALK [21],\nWALKLETS [22], LINE (Large-scale Information Network Embedding) [25], HPE (Heterogeneous\nPreference Embedding) [5], APP (Asymmetric Proximity Preserving graph embedding) [31], MF\n(Matrix Factorization) [3] and BPR (Bayesian Personalized Ranking) [23]. For these network rep-\nresentation baseline methods, based on the learned representation features, we will further train a\n7\nTable 3: Experimental Results on the Foursquare Social Network Dataset.\nFoursquare Network\nmethods\nMSE\nMAE\nLRS\navg. rank\nLNN\n0.002±0.001 (3)\n0.003±0.001 (1)\n0.121±0.001 (1)\n(1)\nDEEPWALK [21]\n0.002±0.001 (3)\n0.004±0.001 (4)\n0.149±0.007 (3)\n(3)\nWALKLETS [22]\n0.001±0.001 (1)\n0.004±0.001 (4)\n0.216±0.0016 (5)\n(3)\nLINE [25]\n0.002±0.001 (3)\n0.003±0.001 (1)\n0.309±0.0017 (8)\n(6)\nHPE [5]\n0.001±0.001 (1)\n0.003±0.001 (1)\n0.246±0.0016 (7)\n(2)\nAPP [31]\n0.002±0.001 (3)\n0.012±0.001 (6)\n0.145±0.001 (2)\n(5)\nMF [3]\n0.002±0.001 (3)\n0.013±0.001(7)\n0.173±0.006 (4)\n(7)\nBPR [23]\n0.002±0.001 (3)\n0.021±0.001 (8)\n0.235±0.023 (6)\n(8)\nTable 4: Experimental Results on the Twitter Social Network Dataset.\nTwitter Network\nmethods\nMSE\nMAE\nLRS\navg. rank\nLNN\n0.001±0.001 (1)\n0.002±0.001 (1)\n0.276±0.0013 (1)\n(1)\nDEEPWALK [21]\n0.001±0.001 (1)\n0.003±0.001 (2)\n0.31±0.028 (4)\n(3)\nWALKLETS [22]\n0.001±0.001(1)\n0.003±0.001 (2)\n0.309±0.006 (3)\n(2)\nLINE [25]\n0.001±0.001(1)\n0.003±0.001 (2)\n0.455±0.008 (8)\n(6)\nHPE [5]\n0.001±0.001(1)\n0.003±0.001 (2)\n0.351±0.001 (6)\n(4)\nAPP [31]\n0.001±0.0(1)\n0.015±0.0 (7)\n0.308±0.0018 (2)\n(5)\nMF [3]\n0.001±0.001(1)\n0.013±0.001 (6)\n0.341±0.0017 (5)\n(7)\nBPR [23]\n0.002±0.001 (8)\n0.023±0.001 (8)\n0.374±0.0015 (7)\n(8)\nMLP (multiple layer perceptron) as the classiﬁer. By comparing the inference results by the models\non the testing set with the ground truth label vectors, we will use MSE (mean square error), MAE\n(mean absolute error) and LRS (label ranking loss) as the evaluation metrics. Detailed experimental\nresults will be provided in the following subsection.\n5.2\nExperimental Results\nThe results achieved by the comparison methods on these 4 different graph datasets are provided\nin Tables 1-4. The best results are presented in a bolded font, and the blue numbers in the table\ndenote the relative rankings of the methods regarding certain evaluation metrics. Compared with\nthe baseline methods, deep loopy neural network can achieve the best performance than the baseline\nmethods. For instance in Table 1, the MSE obtained by deep loopy neural network is 0.035, which\nabout 7.8% lower than the MSE obtained by the 2nd best method, i.e., MF; and 30% lower than\nthat of APP. Similar results can be observed for the MAE and LRS metrics. At the right hand\nside of the tables, we illustrate the average ranking positions achieved by the methods based on\nthese 3 different evaluation metrics. According to the avg. rank shown in Tables 1, 2 and 4, LNN\ncan achieve the best performance consistently for all the metrics based on the Douban, IMDB,\nFoursquare and Twitter datasets. Although in Table 3, the deep loopy neural network model loses to\nHPE and BPR regarding the MSE metric, but for the other two metrics, it still outperforms all the\nbaseline methods with signiﬁcant advantages according to the results.\n6\nConclusion\nIn this paper, we have introduced the deep loopy neural network model, which is a new deep learning\nmodel proposed for the graph structured data speciﬁcally. Due to the extensive connections among\nnodes in the input graph data, the constructed deep loopy neural network model is very challenging\nto learn with the traditional error back-propagation algorithm. To resolve such a problem, we pro-\npose to extract a set of k-hop subgraph and k-hop rooted spanning tree from the model architecture,\nvia which the errors can be effectively propagated throughout the model architecture. Extensive\nexperiments have been done on several different categories of graph datasets, and the numerical\nexperiment results demonstrate the effectiveness of both the proposed deep loopy neural network\nmodel and the introduced learning algorithm.\n8\nReferences\n[1] E. Arisoy, T. Sainath, B. Kingsbury, and B. Ramabhadran. Deep neural network language\nmodels. In WLM, 2012.\n[2] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko. Translating embed-\ndings for modeling multi-relational data. In NIPS. 2013.\n[3] D. Cai, X. He, J. Han, and T. Huang. Graph regularized nonnegative matrix factorization for\ndata representation. IEEE Trans. Pattern Anal. Mach. Intell., 2011.\n[4] S. Chang, W. Han, J. Tang, G. Qi, C. Aggarwal, and T. Huang. Heterogeneous network em-\nbedding via deep architectures. In KDD, 2015.\n[5] C. Chen, M. Tsai, Y. Lin, and Y. Yang. Query-based music recommendations via preference\nembedding. In RecSys, 2016.\n[6] T. Chen and Y. Sun. Task-guided and path-augmented heterogeneous network embedding for\nauthor identiﬁcation. CoRR, abs/1612.02814, 2016.\n[7] L. Deng, G. Hinton, and B. Kingsbury. New types of deep neural network learning for speech\nrecognition and related applications: An overview. In ICASSP, 2013.\n[8] I. Goodfellow, Y. Bengio, and A. Courville.\nDeep Learning.\nMIT Press, 2016.\nhttp:\n//www.deeplearningbook.org.\n[9] A. Grover and J. Leskovec. Node2vec: Scalable feature learning for networks. In KDD, 2016.\n[10] G. Hinton. A practical guide to training restricted boltzmann machines. In Neural Networks:\nTricks of the Trade (2nd ed.). 2012.\n[11] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke,\nP. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in\nspeech recognition. IEEE Signal Processing Magazine, 2012.\n[12] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural\nComput., 2006.\n[13] H. Jaeger. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and\nthe “echo state network” approach. Technical report, Fraunhofer Institute for Autonomous\nIntelligent Systems (AIS), 2002.\n[14] D. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,\n2014.\n[15] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In NIPS, 2012.\n[16] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521, 2015. http://dx.doi.\norg/10.1038/nature14539.\n[17] Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu. Learning entity and relation embeddings for\nknowledge graph completion. In AAAI, 2015.\n[18] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of\nwords and phrases and their compositionality. In NIPS, 2013.\n[19] A. Mnih and G. Hinton. A scalable hierarchical distributed language model. In NIPS. 2009.\n[20] R. Pascanu, T. Mikolov, and Y. Bengio. On the difﬁculty of training recurrent neural networks.\nIn ICML, 2013.\n[21] B. Perozzi, R. Al-Rfou, and S. Skiena. Deepwalk: Online learning of social representations.\nIn KDD, 2014.\n[22] B. Perozzi, V. Kulkarni, and S. Skiena. Walklets: Multiscale graph embeddings for inter-\npretable network classiﬁcation. CoRR, abs/1605.02115, 2016.\n[23] S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. Bpr: Bayesian personalized\nranking from implicit feedback. In UAI, 2009.\n[24] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\nReasoning, 2009.\n9\n[25] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. Line: Large-scale information\nnetwork embedding. In WWW, 2015.\n[26] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. Stacked denoising autoen-\ncoders: Learning useful representations in a deep network with a local denoising criterion.\nJournal of Machine Learning Research, 2010.\n[27] D. Wang, P. Cui, and W. Zhu. Structural deep network embedding. In KDD, 2016.\n[28] Z. Wang, J. Zhang, J. Feng, and Z. Chen. Knowledge graph embedding by translating on\nhyperplanes. In AAAI, 2014.\n[29] J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to rank with\njoint word-image embeddings. Journal of Machine Learning, 2010.\n[30] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large vocabulary image annota-\ntion. In IJCAI, 2011.\n[31] C. Zhou, Y. Liu, X. Liu, Z. Liu, and J. Gao. Scalable graph embedding for asymmetric prox-\nimity. In AAAI, 2017.\n10\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-05-19",
  "updated": "2019-09-05"
}