{
  "id": "http://arxiv.org/abs/1805.08355v1",
  "title": "Opening the black box of deep learning",
  "authors": [
    "Dian Lei",
    "Xiaoxiao Chen",
    "Jianfei Zhao"
  ],
  "abstract": "The great success of deep learning shows that its technology contains\nprofound truth, and understanding its internal mechanism not only has important\nimplications for the development of its technology and effective application in\nvarious fields, but also provides meaningful insights into the understanding of\nhuman brain mechanism. At present, most of the theoretical research on deep\nlearning is based on mathematics. This dissertation proposes that the neural\nnetwork of deep learning is a physical system, examines deep learning from\nthree different perspectives: microscopic, macroscopic, and physical world\nviews, answers multiple theoretical puzzles in deep learning by using physics\nprinciples. For example, from the perspective of quantum mechanics and\nstatistical physics, this dissertation presents the calculation methods for\nconvolution calculation, pooling, normalization, and Restricted Boltzmann\nMachine, as well as the selection of cost functions, explains why deep learning\nmust be deep, what characteristics are learned in deep learning, why\nConvolutional Neural Networks do not have to be trained layer by layer, and the\nlimitations of deep learning, etc., and proposes the theoretical direction and\nbasis for the further development of deep learning now and in the future. The\nbrilliance of physics flashes in deep learning, we try to establish the deep\nlearning technology based on the scientific theory of physics.",
  "text": "This is a research paper titled \"Opening the Black Box of Deep Learning\".  The authors propose that deep learning neural networks should be understood as physical systems, not just mathematical ones.  The paper argues that understanding deep learning from a physics perspective can provide deeper insights into its workings and guide future development.\n\nThe paper explores this idea from three perspectives:\n\n* **Microscopic:**  This section analyzes deep learning at the level of individual neurons, proposing a model where neurons function as scattering sources of quasi-particles.  They draw analogies to the double-slit experiment, suggesting that the interference patterns created by these quasi-particles are analogous to the features learned by deep learning models.  The authors use this perspective to explain various aspects of Convolutional Neural Networks (CNNs), including the meaning of convolution operations and pooling layers.\n\n* **Macroscopic:**  This section examines deep learning from the perspective of statistical physics and thermodynamics.  The authors relate the success of deep learning to the maximization of entropy, arguing that the models find the state with the maximum number of microscopic configurations.  They discuss the roles of pure and hybrid ensembles of neural networks in the context of statistical mechanics.\n\n* **Physical World View:**  This section explores deep learning through the lens of fundamental physical concepts like locality, symmetry, and conjugacy.  It examines how these concepts manifest in the structure and behavior of deep learning models, arguing that deep learning's success is linked to its ability to capture these fundamental physical properties.\n\nThe authors also discuss the limitations and vulnerabilities of current deep learning models and offer suggestions for future research directions based on their physics-based approach.  They also cover topics such as the importance of normalization in neural networks, the role of energy considerations, and the connection between deep learning and renormalization group theory.  The paper concludes by emphasizing the potential for a deeper understanding of deep learning through the integration of physics and machine learning.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-05-22",
  "updated": "2018-05-22"
}