{
  "id": "http://arxiv.org/abs/1805.08776v1",
  "title": "Scalable Centralized Deep Multi-Agent Reinforcement Learning via Policy Gradients",
  "authors": [
    "Arbaaz Khan",
    "Clark Zhang",
    "Daniel D. Lee",
    "Vijay Kumar",
    "Alejandro Ribeiro"
  ],
  "abstract": "In this paper, we explore using deep reinforcement learning for problems with\nmultiple agents. Most existing methods for deep multi-agent reinforcement\nlearning consider only a small number of agents. When the number of agents\nincreases, the dimensionality of the input and control spaces increase as well,\nand these methods do not scale well. To address this, we propose casting the\nmulti-agent reinforcement learning problem as a distributed optimization\nproblem. Our algorithm assumes that for multi-agent settings, policies of\nindividual agents in a given population live close to each other in parameter\nspace and can be approximated by a single policy. With this simple assumption,\nwe show our algorithm to be extremely effective for reinforcement learning in\nmulti-agent settings. We demonstrate its effectiveness against existing\ncomparable approaches on co-operative and competitive tasks.",
  "text": "Scalable Centralized Deep Multi-Agent\nReinforcement Learning via Policy Gradients\nArbaaz Khan, Clark Zhang, Daniel D. Lee, Vijay Kumar, Alejandro Ribeiro\nGRASP Laboratory\nUniversity of Pennsylvania\nAbstract\nIn this paper, we explore using deep reinforcement learning for problems with\nmultiple agents. Most existing methods for deep multi-agent reinforcement learning\nconsider only a small number of agents. When the number of agents increases, the\ndimensionality of the input and control spaces increase as well, and these methods\ndo not scale well. To address this, we propose casting the multi-agent reinforcement\nlearning problem as a distributed optimization problem. Our algorithm assumes\nthat for multi-agent settings, policies of individual agents in a given population live\nclose to each other in parameter space and can be approximated by a single policy.\nWith this simple assumption, we show our algorithm to be extremely effective for\nreinforcement learning in multi-agent settings. We demonstrate its effectiveness\nagainst existing comparable approaches on co-operative and competitive tasks.\n1\nIntroduction\nLeveraging the power of deep neural networks in reinforcement learning (RL) has emerged as a\nsuccessful approach to designing policies that map sensor inputs to control outputs for complex tasks.\nThese include, but are not limited to, learning to play video games [1, 2], learning complex control\npolicies for robot tasks [3] and learning to plan with only sensory information [4–6]. While these\nresults are impressive, most of these methods consider only single agent settings.\nIn the real world, many applications, especially in ﬁelds like robotics and communications, require\nmultiple agents to interact with each other in co-operative or competitive settings. Examples include\nwarehouse management with teams of robots [7], multi-robot furniture assembly [8], and concurrent\ncontrol and communication for teams of robots [9]. Traditionally, these problems were solved by\nminimizing a carefully set up optimization problem constrained by robot and environment dynamics.\nOften, these become intractable when adding simple constraints to the problem or by simply increasing\nthe number of agents [10]. In this paper, we attempt to solve multi-agent problems by framing them\nas multi-agent reinforcement learning (MARL) problems and leverage the power of deep neural\nnetworks. In MARL, the environment from the perspective on an agent appears non-stationary.\nThis is because the other agents are also changing their policies (due to learning). Traditional RL\nparadigms such as Q-learning are ill suited for such non-stationary environments.\nSeveral recent works have proposed using decentralized actor-centralized critic models [11, 12].\nThese have been shown to work well when the number of agents being considered is small. Setting\nup a large number of actor networks is not computationally resource efﬁcient. Further, the input space\nof the critic network grows quickly with the number of agents. Also, in decentralized frameworks,\nevery agent must estimate and track the other agents [13, 14]. Most deep RL algorithms are sample\ninefﬁcient even with only a single agent. Attempting to learn individual policies for multiple agents\nin a decentralized framework becomes highly inefﬁcient, as we will demonstrate. Thus, attempting to\nlearn multiple policies with limited interaction using decentralized frameworks is often infeasible.\nPreprint. Work in progress.\narXiv:1805.08776v1  [cs.LG]  22 May 2018\nFigure 1: Multi-Agent framework for Distributed Learning: Each agent n (Agn) starts under\npolicy parametrized by θ and uses it to collect experience τ θ\nn. τ θ\nn is used to minimize agent Agn’s loss\nfunction Ln and adapt its policy from θ to θn. Now, Agn uses policy parametrized by θn assuming\nother agents policies remain θ. The trajectory generated in this case is denoted by τ θ,θn\nn\nand is used to\nimprove Agn’s policy by taking gradients w.r.t this intermediate policy. Finally, using this improved\npolicy, we collect another new trajectory τ θ,θn\nn\n. These new trajectories are used to update θ.\nInstead, we propose the use of a centralized model. Here, all agents become aware of the actions of\nother agents, which mitigates the non-stationarity. To use a centralized framework for MARL, one\nmust collect experiences from individual agents and then learn to combine these to output actions for\nall agents. One option is to use high-capacity models like neural networks to learn policies that can\nmap the joint observations of all agents to the joint actions of all agents. This simple approach works\nwhen the number of agents is small but suffers from the curse of dimensionality when the number of\nagents increases. Another possibility is to learn a policy for one agent and ﬁne tune it across all agents\nbut this also turns out to be impractical. To mitigate the problems of scale and limited interaction, we\npropose using a distributed optimization framework for the MARL problem. The key idea is to learn\none policy for all agents that exhibits emergent behaviors when multiple agents interact. This type of\npolicy has been shown to be used in nature [15] as well as in swarm robotics [16]. In this paper, the\ngoal is to learn these policies from raw observations and rewards with reinforcement learning.\nOptimizing one policy across all agents is difﬁcult and sometimes intractable (especially when number\nof agents are large). Instead, we take a distributed approach where each agent improves the central\npolicy with their local observations. Then, a central controller combines these improvements in a way\nthat reﬁnes the overall policy. This can be seen as recasting the original problem of optimizing one\npolicy to optimizing several policies subject to the constraint that they are identical. After training,\nthere will only be a single policy for all agents to use. This is a optimization technique that has seen\nsuccess in distributed settings before [17]. Thus the main contributions of this paper are :\n1. A novel algorithm for solving MARL problems using distributed optimization.\n2. The policy gradient formulation when using distributed optimization for MARL\n2\nRelated Work\nMulti-Agent Reinforcement Learning (MARL) has been an actively explored area of research in\nthe ﬁeld of reinforcement learning [18, 19]. Many initial approaches have been focused on tabular\nmethods to compute Q-values for general sum Markov games [20]. Another approach in the past has\nbeen to remove the non-stationarity in MARL by treating each episode as an iterative game, where\nthe other agent is held constant during its turn. In such a game, the proposed algorithm searches for a\nNash equilibrium [21]. Naturally, for complex competitive or collaborative tasks with many agents,\nﬁnding a Nash equilibrium is non-trivial. Building on the recent success of methods for deep RL,\nthere has been a renewed interest in using high capacity models such as neural networks for solving\nMARL problems. However, this is not very straightforward and is hard to extend to games where the\nnumber of agents is more than two [22].\nWhen using deep neural networks for MARL, one method that has worked well in the past is the\nuse of decentralized actors for each agent and a centralized critic with parameter sharing among the\nagents [11, 12]. While this works well for a small number of agents, it is sample inefﬁcient and very\noften, the training becomes unstable when the number of agents in the environment increases.\nIn our work, we derive the policy gradient derivation for multiple agents. This derivation is very\nsimilar to that for policy gradients in meta-learning from [23, 24], where the authors use meta-\n2\nlearning to solve continuous task adaptation. In [23] the authors propose a meta-learning algorithm\nthat attempts to mitigate the non-stationarity by treating it as a sequence of stationary tasks and train\nagents to exploit the dependencies between consecutive tasks such that they can handle similar non\nstationaries at execution time. This is in contrast to our work where we are focused on the MARL\nproblem. In MARL there are often very few inter-task (in the MARL setting this corresponds to\ninter-agent) dependencies that can be exploited. Instead, we focus on using distributed learning to\nlearn a policy.\n3\nCollaborative Reinforcement Learning in Markov Teams\nWe consider policy learning problems in a collaborative Markov team [19]. The team is composed\nof N agents generically indexed by n which at any given point in time t are described by a state\nsnt ∈S and an action ant ∈A. Observe that we are assuming all agents to have common state\nspace S and common action space A. Individual states and actions of each agent are collected in\nthe vectors st := [s1; . . . ; sN] ∈SN and at := [a1; . . . ; aN] ∈AN. Since the team is assumed to\nbe Markov, the probability distribution of the state at time t + 1 is completely determined by the\nconditional transition probability p\n\u0000st+1\n\f\f st, at\n\u0001\n. We further assume here that agents are statistically\nidentical in that the probability transition kernel is invariant to permutations.\nAt any point in time t, the agents can communicate their states to each other and agents utilize this\ninformation to select their actions. This means that each agent executes a policy πn : SN →A with\nthe action executed by agent n at time t being ant = πn(st). As agents operate in their environment,\nthey collect individual rewards rn(st, ant) which depend on the state of the team st and their own\nindividual action ant. The quantity of interest to agent n is not this instantaneous reward but rather\nthe long term reward accumulated over a time horizon T as discounted by a factor γ,\nRn :=\nT\nX\nt=0\nγtrn(st, ant).\n(1)\nThe reward Rn in (1) is stochastic as it depends on the trajectory’s realization. In conventional\nRL problems, agent n would deﬁne the cost ˜Ln(πn) := Eπn(Rn) and search for a policy πn that\nmaximizes this long term expected reward. This expectation, however, neglects the effect of other\nagents, which we can incorporate competitively or collaboratively. In a competitive formulation\nagent n considers the loss Ln(Π) := EΠ(Rn) that is integrated not only with respect to its own\npolicy but with respect to the policies of all agents Π := [π1; . . . ; πN]. In the collaborative problems\nwe consider here, agent n takes the rewards of other agents into consideration. Thus, the reward of\ninterest to agent n is the expected reward accumulated over time and across all agents,\nL(Π) = EΠ\n\u0014\nN\nX\nn=1\nRn\n\u0015\n=\nN\nX\nn=1\nEπn,π−n[Rn] =\nN\nX\nn=1\nLn(Π) =\nN\nX\nn=1\nLn(πn, π−n).\n(2)\nwhere, we recall, Π = [π1; . . . ; πN] denotes the joint policy of the team and we have further deﬁned\nπ−n = [πm]m̸=n to group the policies of all agents except n.\nThe goal in a collaborative reinforcement learning problem is to ﬁnd a policies πn that optimize the\naggregate expected reward in (2). We can write these optimal policies as Π† = argmaxΠ(L(Π)).\nThe drawback with this problem formulation is that it requires learning separate policies for each\nindividual agent. This is intractable when N is large, which motivates a restriction in which all agents\nare required to execute a common policy. This leads to the optimization problem\nπ∗:= argmax L(πn, π−n),\ns. t. πn = πm, for all n ̸= m.\n(3)\nWe reformulate into the more tractable problem\nπ∗= argmax L(πn, π),\ns. t. πn = π for all n\n(4)\nIn the next section, we present a distributed algorithm to solve this optimization problem.\n4\nDistributed Optimization for MARL using Policy Gradients\nLet us reiterate the problem in Eqn 3 in terms of the parameterization of the policy and trajectories\ndrawn from the policy. Eqn 3 can be interpreted as a problem where we aim to solve is to ﬁnd the\n3\nbest set of parameters θ∗that parameterizes a policy πθ to maximize the sum of rewards Ri for all\nagents over some time horizon T. Speciﬁcally, the optimization problem in Eqn 3 can be written as:\nθ∗= max\nθ\nN\nX\nn=1\nEτ θ\nn∼Pn(τ θ\nn|θ)\nh\nRn\ni\n= max\nθ\nN\nX\nn=1\nLn(θ)\n(5)\nwhere τ θ\nn are trajectories of agent n\nτ θ\nn =\nn\n[st0,θ\nn\n, at0,θ\nn\n, at0,θ\n1,...N̸=n, rt0\nn ], [st1,θ\nn\n, at1,θ\nn\n, at1,θ\n1,...N̸=n, rt1\nn ] . . . , [stT ,θ\nn\n, atT ,θ\nn\n, atT ,θ\n1,...N̸=n, rtT\nn ]\no\n(6)\nsampled from the distribution of trajectories Pn(τ θ\nn|θ) induced by the policy πθ. However, as stated\nabove this problem can be intractable for large N. Rewriting the parametrized version of the more\ntractable optimization in Eqn 4 we get:\nmax\nθ,{θi}\nN\nX\nn=1\nEτ θ,θn\nn\n∼Pn(τ θ,θn\nn\n|θ,θn)\nh\nRn\ni\n= max\nθ\nN\nX\nn=1\nLn(θ, θn)\nsubject to\nθn = θ, for all n\n(7)\nwhere we deﬁne the trajectories τ θ,θn\nn\nto be those obtained when agent n follows policy πθn and all\nother agents follow policy πθ. 1\nτ θ,θn\nn\n=\nn\n[st0,θn\nn\n, at0,θn\nn\n, at0,θ\n1,...N̸=n, rt0\nn ], [st1,θn\nn\n, at1,θn\nn\n, at1,θ\n1,...N̸=n, rt1\nn ],\n. . . , [stT ,θn\nn\n, atT ,θn\nn\n, atT ,θ\n1,...N̸=n, rtT\nn ]\no\n(8)\nThe difference between Eqn 5 and Eqn 7 is that we have formed N copies of θ labeled θn and put a\nconstraint that θ = θn. This approach allows us to look at the problem in a different light. Similar to\nother distributed optimization problems such as ADMM [17], we can decouple the optimization over\nθn from that of θ. The general approach is an iterative process where\n1. For each agent n, optimize the corresponding θn\n2. Consolidate the θn into θ\nThis is often realized as a projected gradient descent where for each agent n, we apply the gradients\nθn ←θn + α1∇θnL(θ, θn) as well as applying a gradient θ ←θ + α2∇θ\nPN\nn=1 L(θ, θn). Then,\nin the next iteration all agents start at θn where θn is realized by taking a projection step such that\nθn = θ ←\n1\nN+1(θ + PN\nn=1 θn) is taken to satisfy the constraint in problem 7. However, when\ncomputing this projected gradient step, we need to keep track of all θn to compute the average. This is\ninfeasible if this is done for a large number of agents. Instead a simple approximation to the projected\ngradient is used by setting θn ←θ. In the next subsection, we present our algorithm Distributed\nMulti Agent Policy Gradient or DiMA-PG and its practical implementation.\n4.1\nDistributed Multi-Agent Policy Gradients (DIMA-PG)\nIn this section, we propose the Distributed Multi Agent Policy Gradient (DiMA-PG) algorithm\nwhich learns a centralized policy that can be deployed across all agents. Consider a population Pop\nfrom which N statistically identical agents are sampled according to a distribution P(Pop). The\nparameters θn of this agent-speciﬁc policy are updated by taking the gradient w.r.t θ at the speciﬁc\nvalue of θ = θ0 (where θ0 is your current central policy):\nθn ←θ0 + α1∇θnLn(θ, θn)|θ=θ0,θn=θ0\n(9)\nwhere α is step size hyperparameter and L(θ, θn) is as deﬁned in Eqn 7. Note that L(θ0, θ0) uses\ntrajectories τ θ0,θ0\nn\ngenerated when all agents follow policies πθ0 while L(θ0, θn) uses trajectories\n1This optimization problem is the same as the one in Eqn 4. The difference being that, we have now written\nthe optimization in terms of the parametrization of the policies and trajectories drawn from the policies.\n4\nτ θ0,θn\nn\nwhen agent n follows πθn while all other agents follow πθ0.We do this because, when the\nenvironment is held constant w.r.t agent, then the problem for agent n reduces to a MDP [25].\nIn practice, we can take k gradient steps instead of just one as presented in Eqn 9. This can be done\nwith the following inductive steps\nθ[0]\nn = θ0\nθ[k]\nn = θ[k−1]\nn\n+ α2∇θnLn(θ, θn)|θ=θ0,θn=θ[k−1]\nn\nθn = θ[k]\nn\n(10)\nFinally, we update θ:\nθ ←θ + ϵ∇θ\nN\nX\nn=1\nLn(θ, θn)\n(11)\nNumerically, we approximate ∇θnLn(θ, θn) by drawing l trajectories where agent n uses policy\nπθn while all other agents uses policy πθ and averaging over the policy gradients [26, 25] that each\ntrajectory provides. Recall that the trajectories τ θ\nn and τ θ,θn\nn\nare random variables with distributions\nPn(τ θ\nn|θ) and Pn(τ θ,θn\nn\n|θ, θn) respectively. The individual agent policy parameters, θn are also\nrandom variables with distribution Pn(θn|θ). The overall optimization can be written as:\nmax\nθ\nEn∼P (P op)\nh\nEτ θ\nn∼Pn(τ θ\nn|θ)\nh\nEτ θ,θn\nn\n∼Pn(τ θ,θn\nn\n|θ,θn)[Ln(θ, θn)|(τ θ\nn, θ)]\nii\n(12)\nAssuming, we sample N agents, Eqn. 12 can be rewritten as:\nmax\nθ\n1\nN\nN\nX\nn=1\nh\nEτ θ\nn∼Pn(τ θ\nn|θ)\nh\nEτ θ,θn\nn\n∼Pn(τ θ,θn\nn\n|θ,θn)[Ln(θ, θn)|(τ θ\nn, θ)]\nii\n(13)\nTo learn θ, we use policy gradient methods [26, 25] which operate by taking the gradient of Eqn. 13.\nOne can also use recently proposed state of the art methods for policy gradient methods [27, 28].\nThe gradient for each agent in Eqn 13 (the quantity inside the sum) w.r.t θ can be written as:\n∇θLn(θ, θn) =\nE\nτ θ\nn∼Pn(.|θ),τ θ,θn\nn\n∼Pn(.|θ,θn)\n\"\nLn(θ, θn)∇θ log πθn(τ θ,θn\nn\n) + Ln(θ, θn)∇θ log πθ(τ θ\nn)\n#\n(14)\nThe policy gradient for each agent consists of two policy gradient terms, one over the trajectories\nτ θ,θn\nn\nsampled using (θ, θn) and another term over the trajectories τ θ\nn sampled using θ. It may be\nnoted that the terms from the agent speciﬁc policy improvement when the other agents are held\nstationary (Eqn 10) do not appear in the ﬁnal term. We show that it is possible to marginalize these\nterms out in the derivation for the gradient and point the reader to the appendix for a full derivation of\nthe policy gradient. The full algorithm for DiMA-PG is presented in Algorithm 1.\nAlgorithm 1: Distributed Multi Agent with Policy Gradients (DIMA-PG)\nRequire: Initial random central policy θ, step-size hyperparameters α1, α2, ϵ and distribution over\nagent population P(Pop)\n1: while True do\n2:\nSample N agents ∼P(Pop)\n3:\nfor all agents do\n4:\nCollect trajectory τ θ\nn as given in Eqn 6 and evaluate agent loss Ln(θ, θn)|θ=θ0,θn=θ0\n5:\nCompute agent speciﬁc policy θi according to Eqn 9\n6:\nUsing θ and θn compute trajectory τθ,θn according to Eqn 8\n7:\nend for\n8:\nCompute policy gradient ∇θLn(θ, θn) for every agent according to Eqn 14\n9:\nUpdate central policy θ ←θ + ϵ∇θ\nPN\nn=1 Ln(θ, θn) (Eqn 11)\n10: end while\n5\n5\nExperiments\n5.1\nEnvironments\nTo test the effectiveness of DIMAPG, we perform experiments on both collaborative and competitive\ntasks. The environments from [12] and the many-agent (MAgent) environment from [29] are adapted\nfor our experiments. We setup the following experiments to test out our algorithm :\nCooperative Navigation This task consists of N agents and N goals. All agents are identical, and\neach agent observes the position of the goals and the other agents relative to its own position. The\nagents are collectively rewarded based on the how far any agent is from each goal. Further, the agents\nget negative reward for colliding with other agents. This can be seen as a coverage task where all\nagents must learn to cover all goals without colliding into each other. We test increasing the number\nof agents and goal regions and report the minimum reward across all agents.\nPredator Prey This task environment consists of two populations - predators and preys. Prey are\nfaster than the predators. The environment is also populated with static obstacles that the agents must\nlearn to avoid or use to their advantage. All agents observe relative positions and velocities of other\nagents and the positions of the static obstacles. Predators are rewarded positively when they collide\nwith the preys and the preys are rewarded are negatively.\nSurvival This task consists of a large number of agents operating in an environment with limited\nresources or food. Agents get reward for eating food but also get reward for killing other agents\n(reward for eating food is higher). Agents must either rush to get reward from eating food or\nmonopolize the food by killing other agents. However, when the agents kill other agents they incur a\nsmall negative reward. Each agent’s observations consists of a spatial local view component and a\nnon spatial component. The local view component encodes information about other agents within a\nrange while the non spatial component encodes features such as the agents ID, last action executed,\nlast reward and the relative position of the agent in the environment.\n5.2\nExperimental Results\nFor all experiments, we use a neural network policy that consists of two hidden layers with 100\nunits each and uses ReLU nonlinearity. For the Cooperative Navigation task, we use the vanilla\npolicy gradient or REINFORCE [26] to compute updates (θn) and TRPO [28] to compute θ. For the\nPredator Prey and Survival tasks we switch to using REINFORCE for both θ and θn. To establish\nbaselines, we compare against both centralized and decentralized deep MARL approaches. For\ndecentralized learning, we use MADDPG from [12] using the online implementation open sourced\nby the authors. Since the authors in [12] already show MADDPG agents work better than other\nmethods where individual agents are trained by DDPG, REINFORCE, Actor-Critic, TRPO, DQN,\nwe do not re implement those algorithms. Instead, we implement a centralized A3C (Actor-Critic) [2]\nand centralized TRPO that take in as input the joint space of all agents observations and output\nactions over the joint space of all agents. We call this the Kitchensink approach. Details about the\npolicy architecture for A3C_Kitchenshink and TRPO_Kitchensink are provided in the appendix. Our\nexperiments are designed using the rllab benchmark suite [30] and use Tensorﬂow [31] to setup the\ncomputation graph for the neural network and compute gradients.\nFigure 2: Multi-agent environments for testing: We consider both collaborative as well as com-\npetitive environments. Left: Cooperative Navigation (with 3 agents) Center Left: Cooperative\nNavigation for 10 agents. Center Right: Predator-Prey Right: Survival with many (630) agents\n6\nFigure 3: Min reward vs. number of episodes for Cooperative Navigation: DIMAPG converges\nquickly in both scenarios. The protocol followed in the plots involves 5 independent runs for each\nalgorithm with different seeds, darker line represents the mean and the shaded lighter region represents\nthe variance.\n5.2.1\nCooperative Navigation\nWe setup co-operative navigation as described in Section 5.1. Agents are rewarded for being close to\nthe goals (negative square of distance to the goals) and get negatively rewarded for colliding into each\nother or when they step out of the environment boundary. We also observe that in order to stabilize\ntraining, we need to clip our rewards in the range [-1,1]. We use a horizon T = 200 after which\nepisodes are terminated. Additional hyper parameters are provided in the Appendix.\nn=3\nn=10\nUsing θ\n-34.8\n-8\nUsing θ′\ni\n-37.19\n-8.5\nFine Tune\n-44.17\n-56.3\nTable 1: Min. reward across\nall agents after training (avg.\nover 100 episodes)\nWe run our proposed algorithm and baselines on this environment\nwhen number of agents n = 3 and n = 10. Since the baselines\nA3C_Kitchenshink and TRPO_Kitchensink operate over the joint\nspace, they are setup to maximize the minimum reward across all\nagents. The training curve for our tasks can be seen in Fig 3. We\nnotice that for the simple case, A3C_Kitchenshink performs very\nwell and quickly converges. This is expected since the number of\nagents is low and the dimensionality of the input space is not large.\nTRPO_Kitchenshink and MADDPG perform worse and while they\nconverge, the convergence is only seen after 300-400k episodes.\nWhen n is increased to ten, we observe that only DIMAPG is able\nto quickly learn policies for all agents.\nIn our initial hypothesis, we sought to use θ across all agents since we assumed that the policies for\nall agents in a given population live close to each other in parameter space. We observe from Table\n1 that after training using θ or θ′\ni (after k-shot adaptation from θ) yields almost similar results thus,\nverifying our hypothesis. We also consider the case where we train only 1 agent and then run the\nsame policy across all agents. We observe that this yields poor results.\n5.3\nPredator Prey\nThe goal of this experiment is to compare the effectiveness of DIMAPG on competitive tasks. In\nthis task, there exist 2 populations of agents; predators and preys. Extending our hypothesis to this\ntask, we would like to learn a single policy for all predators and a single policy for all preys. It is\nimportant to note that even though, the policies are different, they are trained in parallel which in the\ncentralized setup enables us to condition each agents trajectory on the actions of other agents even if\nthey are in a different population. We experiment with two scenarios; 12vs1 and 3vs1 predator prey\ngames where the prey are faster than the predator. The horizon used is T = 200.\nOur results are presented in Fig 4. We observe that DIMAPG is able to effectively learn better policies\nthan both MADDPG and the centralized Kitchensink methods on this competitive task. Similar\nresults with DIMAPG are achieved even when the number of predators and preys are increased.\n7\nFigure 4: Results on Predator Prey. Left, Center: Average predator reward collected over 100\nepisodes after training different policies for predators and preys. In the 3 Predators vs 1 Prey game,\nthe prey is 30% faster than the predators. In the 12 Predators vs 4 Prey, the prey is 50% faster than\nthe predators. Right: Avg predator reward vs episodes during training for 3vs1 game.\n5.4\nSurvival\nThe goal of this experiment is to demonstrate the effectiveness of DIMAPG on environments with\na large number of agents. The environment is populated with agents and food (the food is static\nparticles at the center). Agents must learn to survive by eating food. To do so they can either rush to\ngather food and get reward or monopolize the food by ﬁrst killing other agents (killing other agents\nresults in a small negative reward). We use DIMAPG to learn the central policy that is deployed\nacross all agents by randomly sampling N agents from the population. We roll out each episode for a\nhorizon of T = 200. Each environment is populated with 160 food particles (eating one food particle\nyields a reward of +5). For this task, it is infeasible to train the other baselines and hence we do not\nbenchmark for this experiment.\nStatistics\nN=230\nN=630\nFood Left\n0\n0\nSurvivors\n227\n490\nAverage Reward\n946\n674\nTable 2: Statistics on Survival collected over over 100 games using DIMAPG, after training.\nInitial average reward for N = 630 is -3800 and for N = 230 it is -1530.\nWe gauge the performance of DIMAPG on this task by evaluating the number of surviving agents and\nthe food left at the end of the episode as well as the average reward over agents per episode.(Table 2).\nIt is observed in the case when N = 225, the agents do not kill each other and instead learn to gather\nfood. When the number of agents is increased to N = 630 agents close to the food rush in to gather\nfood while those further away start killing other agents.\n6\nConclusion and Outlook\nThus, in this work we have proposed a distributed optimization setup for multi-agent reinforcement\nlearning that learns to combine information from all agents into a single policy that works well for\nlarge populations. We show that our proposed algorithm performs better than other state of the art\ndeep multi agent reinforcement learning algorithms when the number of agents are increased.\nOne bottleneck in our work is the signiﬁcant computation cost involved in computing the second\nderivatives for the gradient updates. Due to this, in practice we make approximations for the second\nderivative and are restricted to simple feedforward neural networks. On more challenging tasks,\nit might be a good idea to try recurrent neural networks and investigate methods such as the one\npresented in [32] to compute fast gradients. We leave this for future work.\n8\nReferences\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing\natari with deep reinforcement learning,” arXiv preprint arXiv:1312.5602, 2013.\n[2] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asyn-\nchronous methods for deep reinforcement learning,” in International Conference on Machine Learning,\npp. 1928–1937, 2016.\n[3] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of deep visuomotor policies,” arXiv\npreprint arXiv:1504.00702, 2015.\n[4] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven exploration by self-supervised\nprediction,” arXiv preprint arXiv:1705.05363, 2017.\n[5] A. Khan, C. Zhang, N. Atanasov, K. Karydis, V. Kumar, and D. D. Lee, “Memory augmented control\nnetworks,” in International Conference on Learning Representations, 2018.\n[6] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, “Cognitive mapping and planning for visual\nnavigation,” arXiv preprint arXiv:1702.03920, 2017.\n[7] J. Enright and P. R. Wurman, “Optimization and coordinated autonomy in mobile fulﬁllment systems.,”\n2011.\n[8] R. A. Knepper, T. Layton, J. Romanishin, and D. Rus, “Ikeabot: An autonomous multi-robot coordinated\nfurniture assembly system,” in Robotics and Automation (ICRA), 2013 IEEE International Conference on,\npp. 855–862, IEEE, 2013.\n[9] J. Stephan, J. Fink, V. Kumar, and A. Ribeiro, “Concurrent control of mobility and communication in\nmultirobot systems,” IEEE Transactions on Robotics, vol. 33, pp. 1248–1254, October 2017.\n[10] K. Solovey and D. Halperin, “On the hardness of unlabeled multi-robot motion planning,” The International\nJournal of Robotics Research, vol. 35, no. 14, pp. 1750–1759, 2016.\n[11] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy\ngradients,” arXiv preprint arXiv:1705.08926, 2017.\n[12] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch, “Multi-agent actor-critic for mixed\ncooperative-competitive environments,” in Advances in Neural Information Processing Systems, pp. 6382–\n6393, 2017.\n[13] B. C. Da Silva, E. W. Basso, A. L. Bazzan, and P. M. Engel, “Dealing with non-stationary environments\nusing context detection,” in Proceedings of the 23rd international conference on Machine learning,\npp. 217–224, ACM, 2006.\n[14] R. S. Sutton, A. Koop, and D. Silver, “On the role of tracking in stationary environments,” in Proceedings\nof the 24th international conference on Machine learning, pp. 871–878, ACM, 2007.\n[15] K. A. Potter, H. Arthur Woods, and S. Pincebourde, “Microclimatic challenges in global change biology,”\nGlobal change biology, vol. 19, no. 10, pp. 2932–2939, 2013.\n[16] M. Rubenstein, C. Ahler, and R. Nagpal, “Kilobot: A low cost scalable robot system for collective\nbehaviors,” in Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 3293–3298,\nIEEE, 2012.\n[17] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, et al., “Distributed optimization and statistical learning\nvia the alternating direction method of multipliers,” Foundations and Trends R⃝in Machine learning, vol. 3,\nno. 1, pp. 1–122, 2011.\n[18] L. Busoniu, R. Babuska, and B. De Schutter, “Multi-agent reinforcement learning: A survey,” in Control,\nAutomation, Robotics and Vision, 2006. ICARCV’06. 9th International Conference on, pp. 1–6, IEEE,\n2006.\n[19] M. L. Littman, “Markov games as a framework for multi-agent reinforcement learning,” in Machine\nLearning Proceedings 1994, pp. 157–163, Elsevier, 1994.\n[20] J. Hu and M. P. Wellman, “Nash q-learning for general-sum stochastic games,” Journal of machine learning\nresearch, vol. 4, no. Nov, pp. 1039–1069, 2003.\n9\n[21] V. Conitzer and T. Sandholm, “Awesome: A general multiagent learning algorithm that converges in\nself-play and learns a best response against stationary opponents,” Machine Learning, vol. 67, no. 1-2,\npp. 23–43, 2007.\n[22] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente, “Multiagent\ncooperation and competition with deep reinforcement learning,” PloS one, vol. 12, no. 4, p. e0172395,\n2017.\n[23] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and P. Abbeel, “Continuous adaptation via\nmeta-learning in nonstationary and competitive environments,” arXiv preprint arXiv:1710.03641, 2017.\n[24] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,”\narXiv preprint arXiv:1703.03400, 2017.\n[25] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, vol. 1. MIT press Cambridge,\n1998.\n[26] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,”\nMachine learning, vol. 8, no. 3-4, pp. 229–256, 1992.\n[27] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-dimensional continuous control using\ngeneralized advantage estimation,” arXiv preprint arXiv:1506.02438, 2015.\n[28] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in\nInternational Conference on Machine Learning, pp. 1889–1897, 2015.\n[29] L. Zheng, J. Yang, H. Cai, W. Zhang, J. Wang, and Y. Yu, “Magent: A many-agent reinforcement learning\nplatform for artiﬁcial collective intelligence,” arXiv preprint arXiv:1712.00600, 2017.\n[30] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Benchmarking deep reinforcement learning\nfor continuous control,” in International Conference on Machine Learning, pp. 1329–1338, 2016.\n[31] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,\nM. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,\nB. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. War-\nden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: Large-scale machine learning on\nheterogeneous systems,” 2015. Software available from tensorﬂow.org.\n[32] J. Martens, J. Ba, and M. Johnson, “Kronecker-factored curvature approximations for recurrent neural\nnetworks,” in International Conference on Learning Representations, 2018.\n10\nAPPENDIX\nA\nDerivation for Multi-Agent Policy Gradient\nFollowing Section 4.1, the overall optimization problem for distributed meta-learning was given as :\nmax\nθ\nEn∼P (P op)\nh\nEτθ\nn∼Pn(τθ\nn|θ)\nh\nEτθ,θn\nn\n∼Pn(τθ,θn\nn\n|θ,θn)[Ln(θ, θn)|(τ θ\nn, θ)]\nii\n(15)\nwhere trajectories τ θ\nn and τ θ,θn\nn\nare random variables with distributions Pn(τ θ\nn|θ) and Pn(τ θ,θn\nn\n|θ, θn) respec-\ntively. Assuming, we sample N agents, the above Eqn 15 can be rewritten as:\nmax\nθ\n1\nN\nN\nX\nn=1\nh\nEτθ\nn∼Pn(τθ\nn|θ)\nh\nEτθ,θn\nn\n∼Pn(τθ,θn\nn\n|θ,θn)[Ln(θ, θn)|(τ θ\nn, θ)]\nii\n(16)\nLet :\nLn(θ, θn) =\nh\nEτθ\nn∼Pn(τθ\nn|θ)\nh\nEτθ,θn\nn\n∼Pn(τθ,θn\nn\n|θ,θn)[Ln(θ, θn)|(τ θ\nn, θ)]\nii\n(17)\nSince it is required that we maximize only over theta, we are interested in marginalizing θn. Expanding all\nexpectations we can write:\nLn(θ, θn) =\nZ Z Z\nLn(θ, θn)Pn(τ θ,θn\nn\n|(θ, θn))Pn(θn|θ, τ θ\nn)Pn(τ θ\nn|θ)dτ θ\nndτ θ,θn\nn\ndθn\n(18)\nAssuming, we use the k gradient steps instead of just one as presented in Eqn 10 in the main paper, this can be\nrewritten as :\nLn(θ, θn) =\nZ\nLn(θ, θn)Pn(τ θ,θn\nn\n|(θ, θn))Pn(θ[k]\nn |θ[k−1]\nn\n, τ θ[k−1]\nn\nn\n)Pn(θ[k−1]\nn\n|θ[k−2]\nn\n, τ θ[k−2]\nn\nn\n) . . .\nPn(θ[1]\nn |θ[0]\nn , τ θ[0]\nn\nn\n)Pn(τ θ\nn|θ)dτ θ\nndτ θ,θ[0]\nn\nn\ndτ θ,θ[1]\nn\nn\n. . . dτ θ,θ[k]\nn\nn\ndθn\n(19)\nThe term Pn(θn|θ, τ θ\nn)dθn in the above Eqn 18 can be integrated out if we assume a delta distribution for\nPn(θn|θ, τ θ\nn):\nPn(θn|θ, τ θ\nn) = δ\n\u0012\nθ0 + α1∇θnLn(θ, θn)|θ=θ0,θn=θ0\n\u0013\n(20)\nA similar observation can be made for the intermediate terms Pn(θ[1]\nn |θ[0]\nn , τ θ[0]\nn\nn\n), Pn(θ[2]\nn |θ[1]\nn , τ θ[1]\nn\nn\n),\n. . ., Pn(θ[k]\nn |θ[k−1]\nn\n, τ θ[k−1]\nn\nn\n) in the above Eqn 19. Thus after integrating these terms out (in the above Eqn 18\nor 19, we are left with:\nLn(θ, θn) =\nZ Z\nLn(θ, θn)Pn(τ θ,θn\nn\n|(θ, θn))Pn(τ θ\nn|θ)dτ θ\nndτ θ,θn\nn\n(21)\nTaking the gradient of this above equation 21 and rewriting it as an expectation form we get:\n∇θLn(θ, θn) =\nE\nτθ\nn∼Pn(.|θ),τθ,θn\nn\n∼Pn(.|θ,θn)\n\"\nLn(θ, θn)∇θ log πθn(τ θ,θn\nn\n) + Ln(θ, θn)∇θ log πθ(τ θ\nn)\n#\n(22)\nB\nConnection to Meta-Learning\nWe observe that there exists a natural connection between our proposed distributed learning and gradient based\nmeta-learning techniques such as the one used in [23,24]. We brieﬂy introduce gradient based meta-learning\nhere and draw connections from our work to that of meta-learning.\nB.1\nModel-Agnostic Meta Learning (MAML)\nConsider a series of RL tasks Ti that one would like to learn. Each task can be thought of as a Markov\nDecision Process (MDP) M(S, A, R, P′) consisting of observations s ∈S, actions a ∈A, a state transition\nfunction P′(st+1|st, at) and a reward function R(st, at). To solve the MDP (for each task), one would like\nto learn a policy π : s →a that maximizes the expected sum of rewards over a ﬁnite time horizon H,\nmaxπ[PH\nt=1 Rt(st, at)]. Let the policy be represented by some function fθ where θ is the initial parameters of\nthe function.\nIn MAML [24] the authors show that, it is possible to learn a policy πθ which can be used on a task Ti to collect\na limited number of trajectories τθ or experience D and quickly adapt to a task speciﬁc policy πθ′\ni that minimizes\n11\nthe task speciﬁc loss LTi(τθ) = −Est,at∼τθ[PH\nt=1 Rt(st, at)]. MAML learns task speciﬁc policy πθ′\ni by\ntaking the gradient of LTi(τθ) w.r.t θ. This is then followed by collecting new trajectories τθ′\ni or experience set\nD′\ni using πθ′\ni in task Ti. θ is then updated by taking the gradient of LTi(τθ′\ni) w.r.t θ over all tasks. The update\nequations for θ′ and θ are given as:\nθ′\ni := θ −α∇θLTi(τθ),\nθ := θ −β∇θ\nX\nTi\nLTi(τθ′\ni)\n(23)\nwhere α and β are the hyperparameters for step size. Authors in [23] extend MAML to show that one can think\nabout MAML from a probabilistic perspective where all tasks, trajectories and policies can be thought as random\nvariables and θ′ is generated from some conditional distribution P(θ′|θ, τθ).\nB.2\nDistributed Optimization for Multi Agent systems\nWe observe the meta-policy πθ that MAML attempts to learn and uses as an initialization point for the different\ntasks is similar in spirit to the central policy θ DIMAPG attempts to learn and execute on all agents. In both,\napproaches θ captures information across multiple tasks or multiple agents. An important difference between\nour work and MAML or meta-learning is that during execution (post training) we execute θ while MAML uses\nθ to do a 1-shot adaptation for task Ti and then executes θ′\ni on Ti.\nAnother interesting point to note here is the difference in the trajectories τθ′\ni that is used by MAML and the\ntrajectory τ θ,θn\nn\nthat is used by DIMAPG to update task or agent speciﬁc policy θ′\ni or θn. In the distributed\noptimization for multi-agent setting, due to the non-stationarity, it is absolutely necessary that we ensure the\nother agents are held constant (to θ) while agent n is optimizing its task speciﬁc policy θn. MAML has no such\nrequirement.\nC\nExperimental Details\nC.1\nA3C KitchenSink and TRPO KitchenSink\nFor A3C KitchenSink, we input the agents observation and reshape it into a n × m matrix. This is then fed into\na 2D convolution layer with 16 outputs, Elu activation and a kernel size of 2, stride of 1. The output from this\nlayer is fed into another 2D convolution layer with 32 outputs,Elu activation and a kernel size of 2, stride of 1.\nThe output from this layer is ﬂattened and fed into a fully connected layer with 256 outputs and Elu activation.\nThis is followed by feeding into a LSTM layer with 256 hidden units. The output from the LSTM is then fed\ninto two separate fully connected layers to get the policy estimate and the value function estimate. Actor-critic\nloss is setup and minimzied using Adam with learning rate 1e-4. For TRPO Kitchensink, we setup similar policy\nlayer and value function layer.\nC.2\nDIMAPG\nFor this task, we used a neural network policy with two hidden layers with 100 units each. The network\nuses a ReLU non-linearity. Depending on the experiment we compute agent speciﬁc gradient updates using\nREINFORCE and TRPO for the central policy gradient updates. The baseline is ﬁtted separately at each iteration\nfor all agents sampled from the population. We use the standard linear feature baseline. The learning rate for\nagent speciﬁc policy updates α1=α2=0.01. Learning rate for central policy updates ϵ = 0.05. In practice, to\nadapt θ to θn we do multiple gradient steps. We observe k=3 (number of gradient steps) is a good choice for\nmost tasks. For both θ and θn updates, we collect 25 trajectories.\nC.3\nSurvivor\nIn this experiment, the environment is populated with agents and food particles. The agents must learn to survive\nby eating food. To do so they can either rush to gather food and get reward or monopolize the food by ﬁrst\nkilling other agents (killing other agents results in a small negative reward). Each agent in this environment also\nhas orientation. The agents can either chose to one of 12 neighboring cells or stay as is, or chose to attack any\nagent or entity in 8 neighboring cells. Finally the agent can also choose to turn right or left. At every step, the\nagents receive a \"step reward\" of -0.01. If the agent dies, its given a reward of -1. If the agent attacks another\nagent, it receives a penalty of -0.1. However, if it chooses to attack another agent by forming a group it receives\nan award of 1. The agent also gets a reward of +5 for eating food.\nAs stated in the main paper, it is observed that in the case when N = 225, the agents do not kill each other and\ninstead learn to gather food. When the number of agents is increased to N = 630 agents close to the food rush\nin to gather food while those further away start killing other agents. We present a snapshot of the learned policy\nin Figure 1 and Figure 2.\n12\nFigure 5: Learned policy on Survivor(N=230) When the number of agents is small, agents prefer\nto eat food instead of killing each other. Most agents survive in this setting.\nFigure 6: Learned policy on Survivor(N=630) When the number of agents is much larger than the\namount of food in the environment, the agents closer to the food rush in to gather food. We observe\nthat the agents further away (near the walls) form teams and try to take down other agents thus\nmaximizing reward for the group. This can also be interpreted as follows: Agents who can observe\nthe food within their sensing range choose to rush in food. Agents who do not observe food within\ntheir sensing range choose to form groups to take down other agents.\n13\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "stat.ML"
  ],
  "published": "2018-05-22",
  "updated": "2018-05-22"
}