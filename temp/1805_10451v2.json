{
  "id": "http://arxiv.org/abs/1805.10451v2",
  "title": "Geometric Understanding of Deep Learning",
  "authors": [
    "Na Lei",
    "Zhongxuan Luo",
    "Shing-Tung Yau",
    "David Xianfeng Gu"
  ],
  "abstract": "Deep learning is the mainstream technique for many machine learning tasks,\nincluding image recognition, machine translation, speech recognition, and so\non. It has outperformed conventional methods in various fields and achieved\ngreat successes. Unfortunately, the understanding on how it works remains\nunclear. It has the central importance to lay down the theoretic foundation for\ndeep learning.\n  In this work, we give a geometric view to understand deep learning: we show\nthat the fundamental principle attributing to the success is the manifold\nstructure in data, namely natural high dimensional data concentrates close to a\nlow-dimensional manifold, deep learning learns the manifold and the probability\ndistribution on it.\n  We further introduce the concepts of rectified linear complexity for deep\nneural network measuring its learning capability, rectified linear complexity\nof an embedding manifold describing the difficulty to be learned. Then we show\nfor any deep neural network with fixed architecture, there exists a manifold\nthat cannot be learned by the network. Finally, we propose to apply optimal\nmass transportation theory to control the probability distribution in the\nlatent space.",
  "text": "Geometric Understanding of Deep Learning\nNa Lei ∗\nZhongxuan Luo †\nShing-Tung Yau ‡\nDavid Xianfeng Gu §\nAbstract\nDeep learning is the mainstream technique for many machine learning tasks, including image recog-\nnition, machine translation, speech recognition, and so on. It has outperformed conventional methods in\nvarious ﬁelds and achieved great successes. Unfortunately, the understanding on how it works remains\nunclear. It has the central importance to lay down the theoretic foundation for deep learning.\nIn this work, we give a geometric view to understand deep learning: we show that the fundamental\nprinciple attributing to the success is the manifold structure in data, namely natural high dimensional data\nconcentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability\ndistribution on it.\nWe further introduce the concepts of rectiﬁed linear complexity for deep neural network measuring\nits learning capability, rectiﬁed linear complexity of an embedding manifold describing the difﬁculty to\nbe learned. Then we show for any deep neural network with ﬁxed architecture, there exists a manifold\nthat cannot be learned by the network. Finally, we propose to apply optimal mass transportation theory\nto control the probability distribution in the latent space.\n∗Dalian University of Technology, Dalian, China. Email: nalei@dlut.edu.cn\n†Dalian University of Technology, Dalian, China. Email: zxluo@dlut.edu.cn\n‡Harvard University, Boston, US. Email: yau@math.harvard.edu\n§Harvard University, Boston, US. Email: gu@cmsa.fas.harvard.edu.\n1\narXiv:1805.10451v2  [cs.LG]  31 May 2018\n1\nIntroduction\nDeep learning is the mainstream technique for many machine learning tasks, including image recognition,\nmachine translation, speech recognition, and so on [12]. It has outperformed conventional methods in\nvarious ﬁelds and achieved great successes. Unfortunately, the understanding on how it works remains\nunclear. It has the central importance to lay down the theoretic foundation for deep learning.\nWe believe that the main fundamental principle to explain the success of deep learning is the manifold\nstructure in the data, there exists a well accepted manifold assumption: natural high dimensional data\nconcentrates close to a non-linear low-dimensional manifold.\nManifold Representation\nThe main focus of various deep learn methods is to learn the manifold structure\nfrom the real data and obtain a parametric representation of the manifold. In general, there is a probability\ndistribution µ in the ambient space X, the support of µ is a low dimensional manifold Σ ⊂X. For example,\nan autoencoder learns the encoding map ϕθ : X →F and the decoding map ψθ : F →X, where F\nis the latent space. The parametric representation of the input manifold Σ is given by the decoding map\nψθ. The reconstructed manifold ˜Σ = ψθ ◦ϕθ(Σ) approximates input manifold. Furthermore, the DNN\nalso learns and controls the distribution induced by the encoder (ϕθ)∗µ deﬁned on the latent space. Once\nthe parametric manifold structure is obtained, it can be applied for various application, such as randomly\ngenerating a sample on ˜Σ as a generative model. Image denoising can be reinterpreted geometrically as\nprojecting a noisy sample onto ˜Σ representing the clean image manifold, the closest point on ˜Σ gives the\ndenoised image.\nLearning Capability\nAn autoencoder implemented by a ReLU DNN offers a piecewise functional space,\nthe manifold structure can be learned by optimizing special loss functions. We introduce the concept of\nRectiﬁed Linear Complexity of a DNN, which represents the upper bound of the number of pieces of all\nthe functions representable by the DNN, and gives a measurement for the learning capability of the DNN.\nOn the other hand, the piecewise linear encoding map ϕθ deﬁned on the ambient space is required to be\nhomemorphic from Σ to a domain on F. This requirement induces strong topological constraints of the\ninput manifold Σ. We introduce another concept Rectiﬁed linear Complexity of an embedded manifold\n(Σ, X), which describes the minimal number of pieces for a PL encoding map, and measures the difﬁculty\nto be encoded by a DNN. By comparing the complexities of the DNN and the manifold, we can verify if the\nDNN can learn the manifold in principle. Furthermore, we show for any DNN with ﬁxed architecture, there\nexists an embedding manifold that can not be encoded by the DNN.\nLatent Probability Distribution Control\nThe distribution (ϕθ)∗µ induced by the encoding map can be\ncontrolled by designing special loss functions to modify the encoding map ϕθ. We also propose to use\noptimal mass transportation theory to ﬁnd the optimal transportation map deﬁned on the latent space, which\ntransforms simple distributions, such as Gaussian or uniform, to (ϕθ)∗µ. Comparing to the conventional\nWGAN model, this method replaces the blackbox by explicit mathematical construction, and avoids the\ncompetition between the generator and the discriminator.\n1.1\nContributions\nThis work proposes a geometric framework to understand autoencoder and general deep neural networks and\nexplains the main theoretic reason for the great success of deep learning - the manifold structure hidden in\ndata. The work introduces the concept of rectiﬁed linear complexity of a ReLU DNN to measure the learning\ncapability, and rectiﬁed linear complexity of an embedded manifold to describe the encoding difﬁculty.\nBy applying the concept of complexities, it is shown that for any DNN with ﬁxed architecture, there is a\n2\nmanifold too complicated to be encoded by the DNN. Finally, the work proposes to apply optimal mass\ntransportation map to control the distribution on the latent space.\n1.2\nOrganization\nThe current work is organized in the following way: section 2 brieﬂy reviews the literature of autoencoders;\nsection 3 explains the manifold representation; section 4 quantiﬁes the learning capability of a DNN and\nthe learning difﬁculty for a manifold; section 5 proposes to control the probability measure induced by the\nencoder using optimal mass transportation theory. Experimental results are demonstrated in the appendix 6.\n2\nPrevious Works\nThe literature of autoencoders is vast, in the following we only brieﬂy review the most related ones as\nrepresentatives.\nTraditional Autoencoders (AE)\nThe traditional autoencoder (AE) framework ﬁrst appeared in [2], which\nwas initially proposed to achieve dimensionality reduction. [2] use linear autoencoder to compare with\nPCA. With the same purpose, [14] proposed a deep autoencoder architecture, where the encoder and the\ndecoder are multi-layer deep networks. Due to non-convexity of deep networks, they are easy to converge\nto poor local optima with random initialized weights. To solve this problem, [14] used restricted Boltzmann\nmachines (RBMs) to pre-train the model layer by layer before ﬁne-tuning. Later [4] used traditional AEs to\npre-train each layer and got similar results.\nSparse Encoders\nThe traditional AE uses bottleneck structure, the width of the middle later is less than\nthat of the input layer. The sparse autoencoder (SAE) was introduced in [10], which uses over-complete\nlatent space, that is the middle layer is wider than the input layer. Sparse autoencoders [19, 21, 20] were\nproposed.\nExtra regularizations for sparsity was added in the object function, such as the KL divergence between\nthe bottle neck layer output distribution and the desired distribution [18]. SAEs are used in a lot of classiﬁ-\ncation tasks [31, 26], and feature tranfer learning [8].\nDenoising Autoencoder (DAE)\n[30, 29] proposed denoising autoencoder (DAE) in order to improve the\nrobustness from the corrupted input. DAEs add regularizations on inputs to reconstruct a “repaired” input\nfrom a corrupted version. Stacked denoising autoencoders (SDAEs) is constructed by stacking multiple\nlayers of DAEs, where each layer is pre-trained by DAEs. The DAE/SDAE is suitable for denosing purposes,\nsuch as speech recognition [9, 9], and removing musics from speeches [33], medical image denoising [11]\nand super-resolutions [7].\nContractive Autoencoders (CAEs)\n[24] proposed contractive autoencoders (CAEs) to achieve robustness\nby minimizing the ﬁrst order variation, the Jacobian. The concept of contraction ratio is introduced, which\nis similar to the Lipschitz constants. In order to learn the low-dimensional structure of the input data, the\npanelty of construction error encourages the contraction ratios on the tangential directions of the manifold\nto be close to 1, and on the orthogonal directions to the manifold close to 0. Their experiments showed that\nthe learned representations performed as good as DAEs on classiﬁcation problems and showed that their\ncontraction properties are similar. Following this work, [23] proposed the higher-order CAE which adds an\nadditional penalty on all higher derivatives.\n3\nGenerative Model\nAutoencoders can be transformed into a generative model by sampling in the latent\nspace and then decode the samples to obtain new data. [30] used Bernoulli sampling to AEs and DAEs to\nﬁrst implement this idea. [5] used Gibbs sampling to alternatively sample between the input space and the\nlatent space, and transfered DAEs into generative models. They also proved that the generated distribution\nis consistent with the distribution of the dataset. [22] proposed a generative model by sampling from CADs.\nThey used the information of the Jacobian to sample around the latent space.\nThe Variational autoencoder (VAE) [15] use probability perspective to interprete autoencoders. Suppose\nthe real data distribution is µ in X, the encoding map ϕθ : X →F pushes µ forward to a distribution in the\nlatent space (ϕθ)∗µ. VAE optimizes ϕθ, such that (ϕθ)∗µ is normal distributed (ϕθ)∗µ ∼N(0, 1) in the\nlatent space.\nFollowed by the big success of GANs, [17] proposed adversarial autoencoders (AAEs), which use GANs\nto minimize the discrepancy between the push forward distribution (ϕθ)∗µ and the desired distribution in\nthe latent space.\n3\nManifold Structure\nDeep learning is the mainstream technique for many machine learning tasks, including image recognition,\nmachine translation, speech recognition, and so on [12]. It has outperformed conventional methods in\nvarious ﬁelds and achieved great successes. Unfortunately, the understanding on how it works remains\nunclear. It has the central importance to lay down the theoretic foundation for deep learning.\nWe believe that the main fundamental principle to explain the success of deep learning is the mani-\nfold structure in the data, namely natural high dimensional data concentrates close to a non-linear low-\ndimensional manifold.\nThe goal of deep learning is to learn the manifold structure in data and the probability distribution\nassociated with the manifold.\n3.1\nConcepts and Notations\nThe concepts related to manifold are from differential geometry, and have been translated to the machine\nlearning language.\nϕβ\nϕα\nϕαβ\nUα\nUβ\nX\nF\nΣ(µ)\nFigure 1: A manifold structure in the data.\n4\nDeﬁnition 3.1 (Manifold). An n-dimensional manifold Σ is a topological space, covered by a set of open\nsets Σ ⊂S\nα Uα. For each open set Uα, there is a homeomorphism ϕα : Uα →Rn, the pair (Uα, ϕα) form\na chart. The union of charts form an atlas A = {(Uα, ϕα)}. If Uα ∩Uβ ̸= ∅, then the chart transition map\nis given by ϕαβ : ϕα(Uα ∩Uβ) →ϕβ(Uα ∩Uβ),\nϕαβ := ϕβ ◦ϕ−1\nα .\na. Input manifold\nb. latent representation\nc. reconstructed manifold\nM ⊂X\nD = ϕθ(M)\n˜\nM = ψθ(D)\nFigure 2: Auto-encoder pipeline.\nAs shown in Fig. 1, suppose X is the ambient space, µ is a probability distribution deﬁned on X,\nrepresented as a density function µ : X →R≥0. The support of µ,\nΣ(µ) := {x ∈X|µ(x) > 0}\nis a low-dimensional manifold. (Uα, ϕβ) is a local chart, ϕα : Uα →F is called an encoding map, the\nparameter domain F is called the latent space or feature space. A point x ∈Σ is called a sample, its\nparameter ϕα(x) is called the code or feature of x. The inverse map ψα := ϕ−1\nα\n: F →Σ is called the\ndecoding map. Locally, ψα : F →Σ gives a local parametric representation of the manifold.\nFurthermore, the encoding map ϕα : Uα →F induces a push-forward probability measure (ϕα)∗µ\ndeﬁned on the latent space F: for any measurable set B ⊂F,\n(ϕα)∗µ(B) := µ(ϕα(B)).\nThe goal for deep learning is to learn the encoding map ϕα, decoding map ψα, the parametric represen-\ntation of the manifold ψα : F →Σ, furthermore the push-forward probability (ϕα)∗µ and so on. In the\nfollowing, we explain how an autoencoder learns the manifold and the distribution.\n3.2\nManifold Learned by an Autoencoder\nAutoencoders are commonly used for unsupervised learning [3], they have been applied for compression,\ndenoising, pre-training and so on. In abstract level, autoencoder learns the low-dimensional structure of data\nand represent it as a parametric polyhedral manifold, namely a piecewise linear (PL) map from latent space\n(parameter domain) to the ambient space, the image of the PL mapping is a manifold. Then autoencoder\nutilizes the polyhedral manifold as the approximation of the manifold in data for various applications. In\n5\nimplementation level, an autoencoder partition the manifold into pieces (by decomposing the ambient space\ninto cells) and approximate each piece by a hyperplane as shown in Fig. 2.\nArchitecturally, an autoencoder is a feedforward, non-recurrent neural network with the output layer\nhaving the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs.\nIn general, a bottleneck layer is added for the purpose of dimensionality reduction. The input space X is the\nambient space, the output space is also the ambient space. The output space of the bottle neck layer F is the\nlatent space.\n{(X, x), µ, Σ} ϕ- {(F, z), D}\n{(X, ˜x), ˜Σ}\nψ\n?\nψ ◦ϕ\n-\nAn autoencoder always consists of two parts, the encoder and the decoder. The encoder takes a sample\nx ∈X and maps it to z ∈F, z = ϕ(x), the image z is usually referred to as latent representation of\nx. The encoder ϕ : X →F maps Σ to its latent representation D = ϕ(Σ) homemorphically. After that,\nthe decoder ψ : F →X maps z to the reconstruction ˜x of the same shape as x, ˜x = ψ(z) = ψ ◦ϕ(x).\nAutoencoders are also trained to minimise reconstruction errors:\nϕ, ψ = argminϕ,ψ\nZ\nX\nL(x, ψ ◦ϕ(x))dµ(x),\nwhere L(·, ·) is the loss function, such as squared errors. The reconstructed manifold ˜Σ = ψ ◦ϕ(Σ) is used\nas an approximation of Σ.\nIn practice, both encoder and decoder are implemented as ReLU DNNs, parameterized by θ. Let X =\n{x(1), x(2), . . . , xk} be the training data set, X ⊂Σ, the autoencoder optimizes the following loss function:\nmin\nθ\nL(θ) = min\nθ\n1\nk\nk\nX\ni=1\n∥x(i) −ψθ ◦ϕθ(x(i))∥2.\nBoth the encoder ϕθ and the decoder ψθ are piecewise linear mappings. The encoder ϕθ induces a cell\ndecomposition D(ϕθ) of the ambient space\nD(ϕθ) : X =\n[\nα\nUα\nθ ,\nwhere Uα\nθ is a convex polyhedron, the restriction of ϕθ on it is an afﬁne map. Similarly, the piecewise\nlinear map ψθ ◦ϕθ induces a polyhedral cell decomposition D(ψθ, ϕθ), which is a reﬁnement (subdivision)\nof D(ϕθ). The reconstructed polyhedral manifold has a parametric representation ψθ : F →X, which\napproximates the manifold M in the data.\nFig. 2 shows an example to demonstrate the learning results of an autoencoder. The ambient space X is\nR3, the manifold Σ is the buddha surface as shown in frame (a). The latent space is R2, the encoding map\nϕθ : X →D parameterizes the input manifold to a domain on D ⊂F as shown in frame (b). The decoding\nmap ψθ : D →X reconstructs the surface into a piecewise linear surface ˜Σ = ψθ ◦ϕθ(Σ), as shown in\nframe (c). In ideal situation, the composition of the encoder and decoder ψθ ◦ϕθ ∼id should equal to the\nidentity map, the reconstruction ˜Σ should coincide with the input Σ. In reality, the reconstruction ˜Σ is only\na piecewise linear approximation of Σ.\nFig. 3 shows the cell decompositions induced by the encoding map D(ϕθ) and that by the reconstruction\nmap D(ψθ ◦ϕθ) for another autoencoder. It is obvious that D(ψθ ◦ϕθ) subdivides D(ϕθ).\n6\nd. cell decomposition\ne. cut view of\nf. cell decomposition\nD(ϕθ)\nD(ϕθ)\nD(ψθ ◦ϕθ)\nFigure 3: Cell decomposition induced by the encoding, decoding maps.\n3.3\nDirect Applications\nOnce the neural network has learned a manifold Σ, it can be utilized for many applications.\nGenerative Model\nSuppose X is the space of all n × n color images, where each point represents an\nimage. We can deﬁne a probability measure µ, which represents the probability for an image to represent\na human face. The shape of a human face is determined by a ﬁnite number of genes. The facial photo is\ndetermined by the geometry of the face, the lightings, the camera parameters and so on. Therefore, it is\nsensible to assume all the human facial photos are concentrated around a ﬁnite dimensional manifold, we\ncall it as human facial photo manifold Σ.\nBy using many real human facial photos, we can train an autoendoer to learn the human facial photo\nmanifold. The learning process produces a decoding map ψθ : F →˜Σ, namely a parametric representation\nof the reconstructed manifold. We randomly generate a parameter z ∈F (white noise), ϕθ(z) ∈˜Σ gives a\nhuman facial image. This can be applied as a generative model for generating human facial photos.\nDenoising\nTradition image denoising performs Fourier transformation of the input noisy image, then ﬁl-\ntering out the high frequency components, inverse Fourier transformation to get the denoised image. This\nmethod is general and independent of the content of the image.\nIn deep learning, image denoising can be re-interpreted as geometric projection as shown in Fig. 4.\nSuppose we perform human facial image denoising. The clean human facial photo manifold is Σ, the noisy\nfacial image ˜p is not in Σ but close to Σ. We project ˜p to Σ, the closest point to ˜p on Σ is p, then p is the\ndenoised image.\nIn practice, suppose an noisy facial image is given x, we train an autoencoder to obtain a manifold of\nclean facial images represented as ψθ : F →X and an encoding map ϕθ : X →F, then we encode the\nnoisy image z = ϕ(x), then maps z to the reconstructed manifold ˜x = ψθ(z). The result ˜x is the denoised\nimage. Fig. 5 shows the projection of several outliers onto the buddha surface using an autoencoder.\nWe apply this method for human facial image denoising as shown in Fig. 6, in frame (a) we project the\nnoisy image to the human facial image manifold and obtain good denoising result; in frame (b) we use the\ncat facial image manifold, the results are meaningless. This shows deep learning method heavily depends\non the underlying manifold, which is speciﬁc to the problem. Hence the deep learning based method is not\n7\nΣ\nRn\np\n˜p\nFigure 4: Geometric interpretation of image denoising.\na. input manifold\nb. reconstructed manifold\nFigure 5: Geometric projection.\nas universal as the conventional ones.\n4\nLearning Capability\n4.1\nMain Ideas\nFig. 7 shows another example, an Archimedean spiral curve embedded in R2, the curve equation is given\nby ρ(θ) = (a + bθ)eiwθ, a, b, w > 0 are constants, θ ∈(0, T]. For relatively small range T, the encoder\nsuccessfully maps it onto a straight line segment, and the decoder reconstructs a piecewise linear curve with\ngood approximation quality. When we extend the spiral curve by enlarging T, then at some threshold, the\nautoencoder with the same architecture fails to encode it.\nThe central problems we want to answer are as follows:\n1. How to decide the bound of the encoding or representation capability for an autoencoder with a ﬁxed\nReLU DNN architecture?\n2. How to describe and compute the complexity of a manifold embedded in the ambient space to be\nencoded ?\n8\n(a) project to the human facial\n(b) project to the cat facial\nimage manifold\nimage manifold\nFigure 6: Image denoising.\na. Input manifold\nb. latent representation\nc. reconstructed manifold\nM ⊂X\nD = ϕθ(M)\n˜\nM = ψθ(D)\nd. cell decomposition\ne. cell decomposition\nf. level set\nD(ϕθ)\nD(ψθ ◦ϕθ)\nFigure 7: Encode/decode a spiral curve.\n3. How to verify whether a embedded manifold can be encoded by a ReLU DNN autoencoder?\n9\nFor the ﬁrst problem, our solutions are based on the geometric intuition of the piecewise linear nature of\nencoder/decoder maps. By examining ﬁg. 3 and ﬁg. 7, we can see the mapping ϕθ and ψθ induces polyhedral\ncell decompositions of the ambient space X, D(ϕθ) and D(ψθ ◦ϕθ) respectively. The number of cells offers\na measurement to describing the representation capabilities of these maps, the upper bound of the number\nof cells maxθ |D(ϕθ)| describes the limit of the encoding capability of ϕθ. We call this upper bound as\nthe rectiﬁed linear complexity of the autoencoder. The rectiﬁed linear complexity can be deduced from the\narchitecture of the encoder network, as claimed in our theorem 4.5.\nFor the second problem, we introduce the similar concept to the embedded manifold. The encoder map\nϕθ has a very strong geometric requirement: suppose Uk is a cell in D(ϕθ), then ϕθ : Uk →F is an afﬁne\nmap to the latent space, its restriction on Uk ∩Σ is a homeomorphism ϕθ : Uk ∩Σ →ϕθ(Uk ∩Σ). In order\nto satisfy the two stringent requirements for the encoding map: the piecewise ambient linearity and the local\nhomeomorphism, the number of cells of the decomposition of Σ (and of X) must be greater than a lower\nbound. Similarly, we call this lower bound the rectiﬁed linear complexity of the pair of the manifold and\nthe ambient space (X, Σ). The rectiﬁed linear complexity can be derived from the geometry of Σ and its\nembedding in X. Our theorem 4.12 gives a criteria to verify if a manifold can be rectiﬁed by a linear map.\nFor the third problem, we can compare the rectiﬁed linear complexity of the manifold and the autoen-\ncoder. If the RL complexity of the autoencoder is less than that of the manifold, then the autoencoder can\nnot encode the manifold. Speciﬁcally, we show that for any autoencoder with a ﬁxed architecture, there\nexists an embedded manifold, which can not be encoded by it.\n4.2\nReLU Deep Neuron Networks\nWe extend the ReLU activation function to vectors x ∈Rn through entry-wise operation:\nσ(x) = (max{0, x1}, max{0, x2}, . . . , max{0, xn}).\nFor any (m, n) ∈N, let An\nm and Ln\nm denote the class of afﬁne and linear transformations from Rm →Rn,\nrespectively.\nDeﬁnition 4.1 (ReLU DNN). For any number of hidden layers k ∈N, input and output dimensions\nw0, wk+1 ∈N, a Rw0 →Rwk+1 ReLU DNN is given by specifying a sequence of k natural numbers\nw1, w2, . . . , wk representing widths of the hidden layers, a set of k afﬁne transformations Ti : Rwi−1 →Rwi\nfor i = 1, . . . , k and a linear transformation Tk+1 : Rwk →Rwk+1 corresponding to weights of hidden lay-\ners. Such a ReLU DNN is called a (k + 1)-layer ReLU DNN, and is said to have k hidden layers, denoted\nas N(w0, w1, . . . , wk, wk+1).\nThe mapping ϕθ : Rw0 →Rwk+1 represented by this ReLU DNN is\nϕθ = Tk+1 ◦σ ◦Tk ◦· · · ◦T2 ◦σ ◦T1,\n(1)\nwhere ◦denotes mapping composition, θ represent all the weight and bias parameters. The depth of the\nReLU DNN is k + 1, the width is max{w1, . . . , wk}, the size w1 + w2 + · · · + wk.\nDeﬁnition 4.2 (PL Mapping). A mapping ϕ : Rn →Rm is a piecewise linear mapping if there exists a ﬁnite\nset of polyhedra whose union is Rn, and ϕ is afﬁne linear over each polyhedron. The number of pieces of ϕ\nis the number of maximal connected subsets of Rn over which ϕ is afﬁne linear, denoted as N(ϕ). We call\nN(ϕ) as the rectiﬁed linear complexity of ϕ.\nDeﬁnition 4.3 (Rectiﬁed Linear Complexity of a ReLU DNN). Given a ReLU DNN N(w0, . . . , wk+1),\nits rectiﬁed linear complexity is the upper bound of the rectiﬁed linear complexities of all PL functions ϕθ\nrepresented by N,\nN(N) := max\nθ\nN(ϕθ).\n10\nLemma 4.4. The maximum number of parts one can get when cutting d-dimensional space Rd with n\nhyperplanes is denoted as C(d, n), then\nC(d, n) =\n\u0012 n\n0\n\u0013\n+\n\u0012 n\n1\n\u0013\n+\n\u0012 n\n2\n\u0013\n+ · · · +\n\u0012 n\nd\n\u0013\n.\n(2)\nProof. Suppose n hyperplanes cut Rd into C(d, n) cells, each cell is a convex polyhedron. The (n + 1)-th\nhyperplane is π, then the ﬁrst n hyperplanes intersection π and partition π into C(d −1, n) cells, each cell\non π partitions a polyhedron in Rd into 2 cells, hence we get the formula\nC(d, n + 1) = C(d, n) + C(d −1, n).\nIt is obvious that C(2, 1) = 2, the formula (2) can be easily obtained by induction.\nTheorem 4.5 (Rectiﬁed Linear Complexity of a ReLU DNN). Given a ReLU DNN N(w0, . . . , wk+1),\nrepresenting PL mappings ϕθ : Rw0 →Rwk+1 with k hidden layers of widths {wi}k\ni=1, then the linear\nrectiﬁed complexity of N has an upper bound,\nN(N) ≤Πk+1\ni=1 C(wi−1, wi).\n(3)\nProof. The i-th hidden layer computes the mapping Ti : Rwi−1 →Rwi. Each neuron represents a hyper-\nplane in Rwi−1, the wi hyperplanes partition the whole space into C(wi−1, wi) polyhedra.\nThe ﬁrst layer partitions Rw0 into at most C(w0, w1) cells; the second layer further subdivides the cell\ndecomposition, each cell is at most subdivides into C(w1, w2) polyhedra, hence two layers partition the\nsource space into at most C(w0, w1)C(w1, w2). By induction, one can obtain the upper bound of N(N) as\ndescribed by the inequality (2).\n4.3\nCell Decomposition\nThe PL mappings induces cell decompositions of both the ambient space X and the latent space F. The\nnumber of cells is closely related to the rectiﬁed linear complexity.\nFix the encoding map ϕθ , let the set of all neurons in the network is denoted as S, all the subsets is\ndenoted as 2S.\nDeﬁnition 4.6 (Activated Path). Given a point x ∈X, the activated path of x consists all the activated\nneurons when ϕθ(x) is evaluated, and denoted as ρ(x). Then the activated path deﬁnes a set-valued function\nρ : X →2S.\nDeﬁnition 4.7 (Cell Decomposition). Fix an encoding map ϕθ represented by a ReLU RNN, two data points\nx1, x2 ∈X are equivalent, denoted as x1 ∼x2, if they share the same activated path, ρ(x1) = ρ(x2). Then\neach equivalence relation partitions the ambient space X into cells,\nD(ϕθ) : X =\n[\nα\nUα,\neach equivalence class corresponds to a cell: x1, x2 ∈Uα if and only if x1 ∼x2. D(ϕθ) is called the cell\ndecomposition induced by the encoding map ϕθ.\nFurthermore, ϕθ maps the cell decomposition in the ambient space D(ϕθ) to a cell decomposition in the\nlatent space. Similarly, the composition of the encoding and decoding maps also produces a cell decompo-\nsition, denoted as D(ψθ ◦ϕθ), which subdivises D(ϕθ). Fig. 2 bottom row shows these cell decompositions.\n11\n4.4\nLearning Difﬁculty\nDeﬁnition 4.8 (Linear Rectiﬁable Manifold). Suppose Σ is a m-dimensional manifold, embedded in Rn,\nwe say Σ is linear rectiﬁable, if there exists an afﬁne map ϕ : Rn →Rm, such that the restriction of ϕ on\nΣ, ϕ|Σ : Σ →ϕ(Σ) ⊂Rm, is homeomorphic. ϕ is called the corresponding rectiﬁed linear map of M.\nDeﬁnition 4.9 (Linear Rectiﬁable Atlas). Suppose Σ is a m-dimensional manifold, embedded in Rn, A =\n{(Uα, ϕα} is an atlas of M. If each chart (Uα, ϕα) is linear rectiﬁable, ϕα : Uα →Rm is the rectiﬁed\nlinear map of Uα, then the atlas is called a linear rectiﬁable atlas of Σ.\nGiven a compact manifold Σ and its atlas A, one can select a ﬁnite number of local charts ˜\nA =\n{(Ui, ϕi)}n\ni=1, ˜\nA still covers Σ. The number of charts of an atlas A is denoted as |A|.\nDeﬁnition 4.10 (Rectiﬁed Linear Complexity of a Manifold). Suppose Σ is a m-dimensional manifold\nembedded in Rn, the rectiﬁed linear complexity of Σ is denoted as N(Rn, Σ) and deﬁned as,\nN(Rn, Σ) := min {|A| |A is a linear rectiﬁable altas of Σ} .\n(4)\n4.5\nLearnable Condition\nDeﬁnition 4.11 (Encoding Map). Suppose M is a m-dimensional manifold, embedded in Rn, a continuous\nmapping ϕ : Rn →Rm is called an encoding map of (Rn, Σ), if restricted on Σ, ϕ|Σ : Σ →ϕ(Σ) ⊂Rm is\nhomeomorphic.\nTheorem 4.12. Suppose a ReLU DNN N(w0, . . . , wk+1) represents a PL mapping ϕθ : Rn →Rm, Σ is a\nm-dimensional manifold embedded in Rn. If ϕθ is an encoding mapping of (Rn, Σ), then the rectiﬁed linear\ncomplexity of N is no less that the rectiﬁed linear complexity of (Rn, Σ),\nN(Rn, Σ) ≤N(ϕθ) ≤N(N).\nProof. The ReLU DNN computes the PL mapping ϕθ, suppose the corresponding cell decomposition of Rn\nis\nD(ϕθ) : Rn =\nk[\ni=1\nUi,\nwhere each Ui is a convex polyhedron, k ≤N(ϕθ). If ϕθ is an encoding map of Σ, then\nA := {(Di, ϕθ|Di)|Di ∩Σ ̸= ∅}\nform a linear rectiﬁable atlas of Σ. Hence from the deﬁnition of rectiﬁed linear complexity of an ReLU\nDNN and the manifold, we obtain\nN(Rn, Σ) ≤N(ϕθ) ≤N(ϕ).\nThe encoding map ϕθ : X →F is required to be homeomorphic, this adds strong topological constraints\nto the manifold Σ. For example, if Σ is a surface, F is R2, then Σ must be a genus zero surface with\nboundaries. In general, assume ϕθ(Σ) is a simply connected domain in F = Rm, then Σ must be a m-\ndimensional topological disk. The topological constraint implies that autoencoder can only learn manifolds\nwith simple topologies, or a local chart of the whole manifold.\nOn the other hand, the geometry and the embedding of Σ determines the linear rectiﬁability of (Σ, Rn).\n12\nLemma 4.13. Suppose a n dimensional manifold Σ is embedded in Rn+1,\nM\nG\n- Sn\np - RPn\nwhere G : Σ →Sn is the Gauss map, RPn is the real projective space, the projection p : Sn →RPn maps\nantipodal points to the same point, if p ◦G(Σ) covers the whole RPn, then Σ is not linear rectiﬁable.\nProof. Given any unit vector v ∈Rn+1, all the unit vectors orthogonal to v form a sphere Sn−2(v), then\np(Sn−2(v))∩RPn ̸= ∅, therefore there is a point q ∈Σ, v is in the tangent space at q. Line q+tv is tangent\nto Σ, by shifting the line by an inﬁnitesimal amount, the line intersects Σ at two points. This shows there is\nno linear mapping, which projects Σ onto Rn along v. Because v is arbitrary, Σ is not linear rectiﬁable.\na. linear rectiﬁable\nb. non-linear-rectiﬁable\nc. C1 Peano curve\nd. C2 Peano curve\nFigure 8: Linear rectiﬁable and non-linear-rectiﬁable curves.\nTheorem 4.14. Given any ReLU deep neural network N(w0, w1, . . . , wk, wk+1), there is a manifold Σ\nembedded in Rw0, such that Σ can not be encoded by N.\nProof. First, we prove the simplest case. When (w0, wk+1) = (2, 1), we can construct space ﬁlling Peano\ncurves, as shown in Fig. 8. Suppose C1 is shown in the left frame, we make 4 copies of C1, by translation,\nrotation, reconnection and scaling to construct C2, as shown in the right frame. Similarly, we can construct\nall Ck’s. The red square shows one unit, C1 has 16 units, Cn has 4n+1 units. Each unit is not rectiﬁable,\ntherefore\nN(R2, Cn) ≥4n+1.\nWe can choose n big enough, such that 4n+1 > N(N), then Cn can not be encoded by N.\nSimilarly, for any w0 and wk+1 = 1, we can construct Peano curves to ﬁll Rw0, which can not be\nencoded by N. The Peano curve construction can be generalized to higher dimensional manifolds by direct\nproduct with unit intervals.\n5\nControl Induced Measure\nIn generative models, such as VAE [15] or GAN [1], the probability measure in the latent space induced by\nthe encoding mapping (ϕθ)∗µ is controlled to be simple distributions, such as Gaussian or uniform, then in\nthe generating process, we can sample from the simple distribution in the latent space, and use the decoding\nmap to produce a sample in the ambient space.\nThe buddha surface Σ is conformally mapped onto the planar unit disk ϕ : Σ →D using the Ricci ﬂow\nmethod [32], the uniform distribution on the parameter domain induces a non-uniform distribution on the\n13\nFigure 9: Control distributions by optimal mass transportation.\nsurface, as shown in the top row of Fig. 9. Then by composing with an optimal mass transportation map\nψ : D →D using the algorithm in [25], one obtain an area-preserving mapping ψ ◦ϕ : Σ →D, the image is\nshown in the bottom row of Fig. 9 left frame. Then we uniformly sample the planar disk to get the samples\nZ = {z1, . . . , zk}, then pull them back on to Σ by ψ ◦ϕ, X = {x1, . . . , xk}, xi = (ψ ◦ϕ)−1(zi). Because\nψ ◦ϕ is area-preserving, Z is uniformly distributed on the disk, X is uniformly distributed on Σ as shown\nin the bottom row of Fig. 9 right frame.\nOptimal Mass Transportation\nThe optimal transportation theory can be found in Villani’s classical books\n[27][28]. Suppose ν = (ϕθ)∗µ is the induced probability in the latent space with a convex support Ω⊂F, ζ\nis the simple distribution, e.g. the uniform distribution on Ω. A mapping T : Ω→Ωis measure-preserving\nif T∗ν = ζ. Given the transportation cost between two points c : Ω× Ω→R, the transportation cost of T\nis deﬁned as\nE(T) :=\nZ\nΩ\nc(x, T(x))dν(x).\nThe Wasserstein distance between ν and ζ is deﬁned as\nW(ν, ζ) :=\ninf\nT∗ν=ζ E(T).\n14\nThe measure-preserving map T that minimizes the transportation cost is called the optimal mass transporta-\ntion map.\nKantorovich proved that the Wasserstein distance can be represented as\nW(ν, ζ) := max\nf\nZ\nΩ\nfdν +\nZ\nΩ\nfcdζ\nwhere f : Ω→R is called the Kontarovhich potential, its c-transform\nfc(y) := inf\nx∈Ωc(x, y) −f(x).\nIn WGAN, the discriminator computes the generator computes the decoding map ψθ : F →X, the\ndiscriminator computes the Wasserstein distance between (ψθ)∗ζ and µ. If the cost function is chosen to be\nthe L1 norm, c(x, y) = |x −y|, f is 1-Lipsitz, then fc = −f, the discriminator computes the Kontarovich\npotential, the generator computes the optimal mass transportation map, hence WGAN can be modeled as an\noptimization\nmin\nθ\nmax\nf\nZ\nΩ\nf ◦ψθ(z)dζ(z) −\nZ\nX\nf(x)dµ(x).\nThe competition between the discriminator and the generator leads to the solution.\nIf we choose the cost function to be the L2 norm, c(x, y) = 1\n2|x −y|2, then the computation can be\ngreatly simpliﬁed. Briener’s theorem [6] claims that there exists a convex function u : Ω→R, the so-called\nBrenier’s potential, such that its gradient map ∇u : Ω→Ωgives the optimal mass transportation map. The\nBrenier’s potential satisﬁes the Monge-Ampere equation\ndet\n\u0012\n∂2u\n∂xi∂xj\n\u0013\n=\nν(x)\nζ(∇u(x)).\nGeometrically, the Monge-Ampere equation can be understood as solving Alexandroff problem: ﬁnding a\nconvex surface with prescribed Gaussian curvature. A practical algorithm based on variational principle can\nbe found in [13]. The Brenier’s potential and the Kontarovich’s potential are related by the closed form\nu(x) = 1\n2|x|2 −f(x).\n(5)\nEqn.(5) shows that: the generator computes the optimal transportation map ∇u, the discriminator computes\nthe Wasserstein distance by ﬁnding Kontarovich’s potential f; u and f can be converted to each other, hence\nthe competition between the generator and the discriminator is unnecessary, the two deep neural networks\nfor the generator and the discriminator are redundant.\nAutoencoder-OMT model\nAs shown in Fig. 10, we can use autoencoder to realize encoder ϕθ : X →F\nand decoder ψθ : F →X, use OMT in the latent space to realize probability transformation T : F →F,\nsuch that\nT∗ζ = (ϕθ)∗µ.\nWe call this model as OMT-autoencoder.\nFig. 5 shows the experiments on the MNIST data set. The digits generates by OMT-AE have better\nqualities than those generated by VAE and WGAN. Fig.(5) shows the human facial images on CelebA data\nset. The images generated by OMT-AE look better than those produced by VAE.\n15\nΣ\nX\nF\nF\nζ\nϕθ\nµ\nT\nν = (ϕθ)∗µ\nψθ\nFigure 10: Autoencoder combined with a optimal transportation map.\n(a) real digits\n(b) VAE\n(c) WGAN\n(d) AE-OMT\n16\n(a) VAE\n(d) AE-OMT\n6\nConclusion\nThis work gives a geometric understanding of autoencoders and general deep neural networks. The underly-\ning principle is the manifold structure hidden in data, which attributes to the great success of deep learning.\nThe autoencoders learn the manifold structure and construct a parametric representation. The concepts of\nrectiﬁed linear complexities are introduced to both DNN and manifold, which describes the fundamental\nlearning limitation of the DNN and the difﬁculty to be learned of the manifold. By applying the concept of\ncomplexities, it is shown that for any DNN with ﬁxed architecture, there is a manifold too complicated to\nbe encoded by the DNN. Experiments on surfaces show the approximation accuracy can be improved. By\napplying L2 optimal mass transportation theory, the probability distribution in the latent space can be fully\ncontrolled in a more understandable and more efﬁcient way.\nIn the future, we will develop reﬁner estimates for the complexities of the deep neural networks and the\nembedding manifolds, generalize the geometric framework to other deep learning models.\nAppendix\nHere we illustrate some examples and explain the implementation details.\nFacial Surface\nFig. 11 shows a human facial surface Σ is encoded/decoded by an autoencoder. From\nthe image, it can be seen that the encoding/decoding maps are homeomorphic. The Hausdorff distance\nbetween the input surface and the reconstructed surface is relatively small, but the normal deviation is big.\nThe geometric details around the mouth area are lost during the process. There are a lot of local curvature\nﬂuctuations. Furthermore, the shape of the encoding image (parameter image) in the latent space is highly\nirregular, this creates difﬁculty for generating random samples on the reconstructed manifold.\nBuddha Model\nFig. 12 shows the buddha model, the top row shows the three views of the input surface,\nthe bottom row shows the reconstructed surface. The encoder network architecture is {3, 768, 384, 192, 96, 48, 2},\nthe decoder network is {2, 48, 96, 192, 384, 768, 3}. The input and output spaces are R3, the latent space is\n17\ninput manifold\nlatent representation\nreconstructed manifold\nreconstructed manifold\nFigure 11: A human facial surface is encoded/decoded by an autoencoder.\na. front view\nb. left view\nc. back view\nd. right view\ne. front view\nf. back view\nFigure 12: top row: input manifold;bottom row, reconstructed manifold.\nR2. We use ReLU as the activation function in hidden layers except the latent space layer. The loss function\nis the mean squared error between the input and the target. Adam optimizer is used in this autoencoder and\nthe weight decay is set to 0 in the optimizer. From the ﬁgure, we can see the reconstruction approximates\nthe original surface with high accuracy, all the subtle geometric features are well preserved. We uniformly\n18\nsample the surface, there are 235, 771 samples in total. The number of cells in the cell decomposition in-\nduced by the reconstruction map is 230051. We see that the autoencoder produces a highly reﬁned cell\ndecomposition to capture all the geometric details of the input surface. The source code and the data set can\nbe found in [34]. If we reduce the number of neurons and add regularize the output surface, then the recon-\nstructed surface loses geometric details, and preserves the major shape as shown in Fig. 13. Furthermore,\nthe mapping is not homeomorphic either, near the mouth and ﬁnger areas, the mapping is degenerated.\na. right view\nb. front view\nback view\nFigure 13: Reconstructed manifold with cell decomposition produced by an autoencoder with half of the\nneurons.\nAcknowledgement\nThe authors thank our students: Yang Guo, Dongsheng An, Jingyao Ke, Huidong Liu for all the experimental\nresults, also thank our collaborators: Feng Luo, Kefeng Liu, Dimitris Samaras for the helpful discussions.\n19\nReferences\n[1] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.\nInternational Conference on Machine Learning, pages 214–223, 2017.\n[2] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples\nwithout local minima. Neural Netw., 2(1):53–58, January 1989.\n[3] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798–1828, August 2013.\n[4] Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Leon Bottou, Olivier\nChapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines. MIT Press, 2007.\n[5] Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders\nas generative models. In Proceedings of the 26th International Conference on Neural Information\nProcessing Systems - Volume 1, NIPS’13, pages 899–907, USA, 2013. Curran Associates Inc.\n[6] Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Comm.\nPure Appl. Math., 44(4):375–417, 1991.\n[7] Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, Christoph Schied, Marco Salvi, Aaron Lefohn,\nDerek Nowrouzezahrai, and Timo Aila. Interactive reconstruction of monte carlo image sequences\nusing a recurrent denoising autoencoder. ACM Trans. Graph., 36(4):98:1–98:12, July 2017.\n[8] J. Deng, Z. Zhang, E. Marchi, and B. Schuller. Sparse autoencoder-based feature transfer learning for\nspeech emotion recognition. In 2013 Humaine Association Conference on Affective Computing and\nIntelligent Interaction, pages 511–516, Sept 2013.\n[9] X. Feng, Y. Zhang, and J. Glass. Speech feature denoising and dereverberation via deep autoencoders\nfor noisy reverberant speech recognition. In 2014 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 1759–1763, May 2014.\n[10] Peter F¨oldi´ak and Malcolm P. Young. The handbook of brain theory and neural networks. chapter\nSparse Coding in the Primate Cortex, pages 895–898. MIT Press, Cambridge, MA, USA, 1998.\n[11] L. Gondara. Medical image denoising using convolutional denoising autoencoders. In 2016 IEEE 16th\nInternational Conference on Data Mining Workshops (ICDMW), pages 241–246, Dec 2016.\n[12] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n[13] Xianfeng Gu, Feng Luo, Jian Sun, and Shing-Tung Yau. Variational principles for minkowski type\nproblems, discrete optimal transport, and discrete monge-ampere equations. Asian Journal of Mathe-\nmatics (AJM), 20(2):383 C 398, 2016.\n[14] Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks.\nScience, 313(5786):504 – 507, 2006.\n[15] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.\n[16] B. L´evy, S. Petitjean, N. Ray, and J. Maillot. Least squares conformal maps for automatic texture\ngeneration. ACM Trans. on Graphics (SIGGRAPH), 21(2):362–371, 2002.\n20\n[17] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders.\nIn International Conference on Learning Representations, 2016.\n[18] Andrew Ng. Sparse autoencoder. CS294A Lecture Notes, December 2011.\n[19] Bruno A. Olshausen and David J. Field. Sparse coding with an overcomplete basis set: A strategy\nemployed by v1? Vision Research, 37(23):3311 – 3325, 1997.\n[20] Marc’ Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sparse feature learning for deep belief\nnetworks. In Proceedings of the 20th International Conference on Neural Information Processing\nSystems, NIPS’07, pages 1185–1192, USA, 2007. Curran Associates Inc.\n[21] Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efﬁcient learning of\nsparse representations with an energy-based model. In Proceedings of the 19th International Confer-\nence on Neural Information Processing Systems, NIPS’06, pages 1137–1144, Cambridge, MA, USA,\n2006. MIT Press.\n[22] Salah Rifai, Yoshua Bengio, Yann N. Dauphin, and Pascal Vincent. A generative process for sam-\npling contractive auto-encoders. In Proceedings of the 29th International Coference on International\nConference on Machine Learning, ICML’12, pages 1811–1818, USA, 2012. Omnipress.\n[23] Salah Rifai, Gr´egoire Mesnil, Pascal Vincent, Xavier Muller, Yoshua Bengio, Yann Dauphin, and\nXavier Glorot. Higher order contractive auto-encoder. In Proceedings of the 2011 European Confer-\nence on Machine Learning and Knowledge Discovery in Databases - Volume Part II, ECML PKDD’11,\npages 645–660, Berlin, Heidelberg, 2011. Springer-Verlag.\n[24] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-\nencoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Con-\nference on International Conference on Machine Learning, ICML’11, pages 833–840, USA, 2011.\nOmnipress.\n[25] Zhengyu Su, Yalin Wang, Rui Shi, Wei Zeng, Jian Sun, Feng Luo, and Xianfeng Gu. Optimal mass\ntransport for shape matching and comparison. IEEE Trans. Pattern Anal. Mach. Intell., 37(11):2246–\n2259, 2015.\n[26] C. Tao, H. Pan, Y. Li, and Z. Zou. Unsupervised spectral spatial feature learning with stacked sparse\nautoencoder for hyperspectral imagery classiﬁcation. IEEE Geoscience and Remote Sensing Letters,\n12(12):2438–2442, Dec 2015.\n[27] C´edric Villani. Topics in optimal transportation. Number 58 in Graduate Studies in Mathematics.\nAmerican Mathematical Society, Providence, RI, 2003.\n[28] C´edric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,\n2008.\n[29] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and com-\nposing robust features with denoising autoencoders. In Proceedings of the 25th International Confer-\nence on Machine Learning, ICML ’08, pages 1096–1103, New York, NY, USA, 2008. ACM.\n[30] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local de-\nnoising criterion. J. Mach. Learn. Res., 11:3371–3408, December 2010.\n21\n[31] Jun Xu, Lei Xiang, Qingshan Liu, Hannah Gilmore, Jianzhong Wu, Jinghai Tang, and Anant Madab-\nhushi. Stacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images.\nIEEE Transactions on Medical Imaging, 35(1):119–130, 1 2016.\n[32] Wei Zeng, Dimitris Samaras, and Xianfeng David Gu. Ricci ﬂow for 3d shape analysis. IEEE Trans.\nPattern Anal. Mach. Intell., 32(4):662–677, 2010.\n[33] M. Zhao, D. Wang, Z. Zhang, and X. Zhang. Music removal by convolutional denoising autoencoder\nin speech recognition. In 2015 Asia-Paciﬁc Signal and Information Processing Association Annual\nSummit and Conference (APSIPA), pages 338–341, Dec 2015.\n[34] https://github.com/sskqgfnnh/reconstruct pointcloud ae\n22\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-05-26",
  "updated": "2018-05-31"
}