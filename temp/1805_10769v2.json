{
  "id": "http://arxiv.org/abs/1805.10769v2",
  "title": "Universality of Deep Convolutional Neural Networks",
  "authors": [
    "Ding-Xuan Zhou"
  ],
  "abstract": "Deep learning has been widely applied and brought breakthroughs in speech\nrecognition, computer vision, and many other domains. The involved deep neural\nnetwork architectures and computational issues have been well studied in\nmachine learning. But there lacks a theoretical foundation for understanding\nthe approximation or generalization ability of deep learning methods generated\nby the network architectures such as deep convolutional neural networks having\nconvolutional structures. Here we show that a deep convolutional neural network\n(CNN) is universal, meaning that it can be used to approximate any continuous\nfunction to an arbitrary accuracy when the depth of the neural network is large\nenough. This answers an open question in learning theory. Our quantitative\nestimate, given tightly in terms of the number of free parameters to be\ncomputed, verifies the efficiency of deep CNNs in dealing with large\ndimensional data. Our study also demonstrates the role of convolutions in deep\nCNNs.",
  "text": "arXiv:1805.10769v2  [cs.LG]  20 Jul 2018\nUniversality of Deep Convolutional Neural\nNetworks\nDing-Xuan Zhou\nDepartment of Mathematics, City University of Hong Kong\nKowloon, Hong Kong\nEmail: mazhou@cityu.edu.hk\nAbstract\nDeep learning has been widely applied and brought breakthroughs\nin speech recognition, computer vision, and many other domains.\nDeep neural network architectures and computational issues have been\nwell studied in machine learning. But there lacks a theoretical foun-\ndation for understanding the approximation or generalization ability\nof deep learning methods generated by the network architectures such\nas deep convolutional neural networks. Here we show that a deep con-\nvolutional neural network (CNN) is universal, meaning that it can be\nused to approximate any continuous function to an arbitrary accuracy\nwhen the depth of the neural network is large enough. This answers\nan open question in learning theory. Our quantitative estimate, given\ntightly in terms of the number of free parameters to be computed,\nveriﬁes the eﬃciency of deep CNNs in dealing with large dimensional\ndata. Our study also demonstrates the role of convolutions in deep\nCNNs.\nKeywords: Deep learning, Convolutional neural network, Universality, Ap-\nproximation theory\n1\nIntroduction and Main Results\nDeep learning provides various models and algorithms to process data as ef-\nﬁciently as biological nervous systems or neuronal responses in the human\nbrain [1, 2, 3, 4, 5]. It is based on deep neural network architectures and those\nstructures bring essential tools for obtaining data features and function rep-\nresentations in practical applications. A main concern about deep learning\n1\nwhich has attracted much scientiﬁc attention and some criticism is its lack of\ntheories supporting its practical eﬃciency caused by its network structures,\nthough there have been some theoretical attempts from approximation the-\nory viewpoints [6, 7, 8]. In particular, for deep CNNs having convolutional\nstructures without fully connected layers, it is unknown which kinds of func-\ntions can be approximated. This paper provides a rigorous mathematical\ntheory to answer this question and to illustrate the role of convolutions.\nNotation and Concepts\nConvolutional Filters and Matrices.\nThe deep CNNs considered in this paper have two essential ingredients:\na rectiﬁed linear unit (ReLU) deﬁned as a univariate nonlinear function σ\ngiven by\nσ(u) = (u)+ = max{u, 0},\nu ∈R\nand a sequence of convolutional ﬁlter masks w = {w(j)}j inducing sparse\nconvolutional structures. Here a ﬁlter mask w = (wk)∞\nk=−∞means a sequence\nof ﬁlter coeﬃcients. We use a ﬁxed integer ﬁlter length s ≥2 to control the\nsparsity, and assume that w(j)\nk\n̸= 0 only for 0 ≤k ≤s. The convolution of\nsuch a ﬁlter mask w with another sequence v = (v0, . . . , vD) is a sequence\nw∗v given by (w∗v)i = PD\nk=0 wi−kvk. This leads to a (D + s) × D Toeplitz\ntype convolutional matrix T which has constant diagonals:\nT =\n\n\nw0\n0\n0\n0\n·\n0\nw1\nw0\n0\n0\n·\n0\n...\n...\n...\n...\n...\n...\nws\nws−1\n· · ·\nw0\n0 · · ·\n0\n0\nws\n· · ·\nw1\nw0 · · ·\n0\n...\n...\n... ...\n...\n... ...\n...\n· · ·\n· · ·\n0\nws\n· · ·\nw0\n· · ·\n· · ·\n· · ·\n0\nws · · ·\nw1\n...\n...\n...\n...\n...\n...\n0\n· · ·\n· · ·· · ·\n· · · 0\nws\nws−1\n0\n· · ·\n· · ·\n· · ·\n· · · 0\nws\n\n\n.\nSparse matrices of this form induce deep CNNs which are essentially diﬀerent\nfrom the classical neural networks involving full connection matrices. Note\nthat the number of rows of T is s greater than that of columns. This leads us\nto take a sequence of linearly increasing widths {dj = d+js} for the network,\nwhich enables the deep CNN to represent functions of richer structures.\n2\nConvolutional Neural Networks.\nStarting with the action of T on the input data vector x ∈Rd, we can\ndeﬁne a deep CNN of depth J as a sequence of J vectors h(j)(x) of functions\non Rd given iteratively by h(0)(x) = x and\nh(j)(x) = σ\n\u0000T (j)h(j−1)(x) −b(j)\u0001\n,\nj = 1, 2, . . . , J,\nwhere T (j) =\n\u0010\nw(j)\ni−k\n\u0011\nis a dj × dj−1 convolutional matrix, σ acts on vectors\ncomponentwise, and b is a sequence of bias vectors b(j).\nExcept the last iteration, we take b(j) of the form [b1 . . . bs bs+1 bs+1 . . . bs+1 bdj−s+1 . . . bdj]T\nwith the dj −2s repeated components in the middle. The sparsity of T (j)\nand the special form of b(j) tell us that the j-th iteration of the deep CNN\ninvolves 3s + 2 free parameters. So in addition to the 2dJ + s + 1 free param-\neters for b(J), c ∈RdJ, w(J), the total number of free parameters in the deep\nCNN is (5s + 2)J + 2d −2s −1, much smaller than that in a classical fully\nconnected multi-layer neural network with full connection matrices T (j) in-\nvolving djdj−1 free parameters. It demonstrates the computational eﬃciency\nof deep CNNs.\nMathematical Theory of Deep CNNs\nThe hypothesis space of a learning algorithm is the set of all possible\nfunctions that can be represented or produced by the algorithm. For the deep\nCNN of depth J considered here, the hypothesis space is a set of functions\ndeﬁned by\nHw,b\nJ\n=\n( dJ\nX\nk=1\nckh(J)\nk (x) : c ∈RdJ\n)\n.\nThis hypothesis space and its approximation ability depend completely on\nthe sequence of convolutional ﬁlter masks w = {w(j)}J\nj=1 and the sequence\nof bias vectors b = {b(j)}J\nj=1. Observe that each function in the hypothesis\nspace Hw,b\nJ\nis a continuous piecewise linear function (linear spline) on any\ncompact subset Ωof Rd. Our ﬁrst main result veriﬁes the universality of\ndeep CNNs, asserting that any function f ∈C(Ω), the space of continuous\nfunctions on Ωwith norm ∥f∥C(Ω) = supx∈Ω|f(x)|, can be approximated by\nHw,b\nJ\nto an arbitrary accuracy when the depth J is large enough.\nTheorem A. Let 2 ≤s ≤d. For any compact subset Ωof Rd and any\nf ∈C(Ω), there exist sequences w of ﬁlter masks, b of bias vectors and\nf w,b\nJ\n∈Hw,b\nJ\nsuch that\nlim\nJ→∞∥f −f w,b\nJ\n∥C(Ω) = 0.\n3\nOur second main result presents rates of approximation by deep CNNs\nfor functions in the Sobolev space Hr(Ω) with an integer index r > 2 + d/2.\nSuch a function f is the restriction to Ωof a function F from the Sobolev\nspace Hr(Rd) on Rd meaning that F and all its partial derivatives up to order\nr are square integrable on Rd.\nTheorem B. Let 2 ≤s ≤d and Ω⊆[−1, 1]d. If J ≥2d/(s−1) and f = F|Ω\nwith F ∈Hr(Rd) and an integer index r > 2 + d/2, then there exist w, b\nand f w,b\nJ\n∈Hw,b\nJ\nsuch that\n∥f −f w,b\nJ\n∥C(Ω) ≤c ∥F∥\np\nlog J (1/J)\n1\n2+ 1\nd ,\nwhere c is an absolute constant and ∥F∥denotes the Sobolev norm of F ∈\nHr(Rd).\nAccording to Theorem B, if we take s = ⌈1 + dτ/2⌉and J = ⌈4d1−τ⌉L\nwith 0 ≤τ ≤1 and L ∈N, where ⌈u⌉denotes the smallest integer not\nsmaller than u, then we have\n∥f −f w,b\nJ\n∥C(Ω) ≤c ∥F∥\nr\n(1 −τ) log d + log L + log 5\n4d1−τL\n,\nwhile the widths of the deep CNN are bounded by 12Ld and the total number\nof free parameters by\n(5s + 2)J + 2d −2s −1 ≤(73L + 2)d.\nWe can even take L = 1 and τ = 1/2 to get a bound for the relative error\n∥f −f w,b\nJ\n∥C(Ω)\n∥F∥\n≤c\n2d−1\n4\nq\nlog(5\n√\nd)\nachieved by a deep CNN of depth ⌈4\n√\nd⌉and at most 75d free parameters,\nwhich decreases as the dimension d increases. This interesting observation is\nnew for deep CNNs, and does not exist in the literature of fully connected\nneural networks. It explains the strong approximation ability of deep CNNs.\nA key contribution in our theory of deep CNNs is that an arbitrary pre-\nassigned sequence W = (Wk)∞\n−∞supported in {0, . . . , M} can be factorized\ninto convolutions of a mask sequence {w(j)}J\nj=1. It is proved by the same\nargument as in [9] for the case with the special restriction W0 ̸= 0. Convo-\nlutions are closely related to translation-invariance in speeches and images\n[6, 10, 11], and also in some learning algorithms [12, 13].\nTheorem C. Let s ≥2 and W = (Wk)∞\n−∞be a sequence supported in\n{0, . . . , M} with M ≥0. Then there exists a ﬁnite sequence of ﬁlter masks\n4\n{w(j)}J\nj=1 supported in {0, . . . , s} with J <\nM\ns−1+1 such that the convolutional\nfactorization W = w(J)∗. . . ∗w(2)∗w(1) holds true.\nDiscussion\nThe classical shallow neural networks associated with an activation func-\ntion σ : R →R produce functions of the form\nfN(x) =\nN\nX\nk=1\nckσ(αk · x −bk)\nwith αk ∈Rd, bk, ck ∈R.\nA mathematical theory for approximation of\nfunctions by shallow neural networks was well developed three decades ago\n[14, 15, 16, 19, 17, 18] and was extended to fully connected multi-layer neural\nnetworks shortly afterwards [15, 19, 20] .\nThe ﬁrst type of results obtained in the late 1980s are about universality,\nasserting that any continuous function f on any compact subset Ωof Rd can\nbe approximated by some fN to an arbitrary accuracy when the number of\nhidden neurons N is large enough. Such results were given in [14, 15, 16] when\nσ is a sigmoidal function, meaning that σ is a continuous strictly increasing\nfunction satisfying limu→−∞σ(u) = 0 and limu→∞σ(u) = 1. A more general\nresult with a locally bounded and piecewise continuous activation function σ\nasserts [17, 18] that universality holds if and only if σ is not a polynomial.\nThe second type of results obtained in the early 1990s are about rates\nof approximation. When σ is a C∞sigmoidal function and f = F|[−1,1]d for\nsome F ∈L2(Rd) with the Fourier transform ˆF satisfying |w| ˆF(w) ∈L1(Rd),\nrates of type ∥fN −f∥L2µ([−1,1]d) = O(1/\n√\nN) were given in [16] where µ\nis an arbitrary probability measure µ. Analysis was conducted in [19] for\nshallow neural networks with more general continuous activation functions\nσ satisfying a special condition with some b ∈R that σ(k)(b) ̸= 0 for any\nnonnegative integer k and a further assumption with some integer ℓ̸= 1 that\nlimu→−∞σ(u)/|u|ℓ= 0 and limu→∞σ(u)/uℓ= 1. The rates there are of type\n∥fN −f∥C([−1,1]d) = O(N−r/d) for f ∈Cr([−1, 1]d). Note that the ReLU\nactivation function considered in this paper does not satisfy the condition\nwith σ(k)(b) ̸= 0 or the special assumption with ℓ̸= 1.\nTo achieve the\napproximation accuracy ∥fN −f∥C([−1,1]d) ≤ǫ, when r = ⌈d+1\n2\n+ 2⌉with\nd/r ≈2, the number of hidden neurons N ≥(cf,d,ℓ/ǫ)d/r and the total\nnumber of free parameters is at least (cf,d,ℓ/ǫ)d/r d, where the constant cf,d,ℓ\ndepends on the dimension d and might be very large.\nTo compare with\nour result, we take the ﬁlter length s = ⌈1 + d/2⌉and depth J = 4L with\nL ∈N. We know from Theorem B that the same approximation accuracy\n5\n∥f −f w,b\nJ\n∥C(Ω) ≤ǫ with 0 < ǫ ≤c ∥F∥can be achieved by the deep CNN\nof depth J = 4⌈1\nǫ2 log 1\nǫ2⌉having at most ⌈75\nǫ2 log 1\nǫ2⌉d free parameters, which\ndoes not depend on the dimension d. Though a logarithmic term is involved,\nthis dimension independence gives evidence for the power of deep CNNs.\nA multi-layer neural network is a sequence of function vectors h(j)(x)\nsatisfying an iterative relation\nh(j)(x) = σ\n\u0000T (j)h(j−1)(x) −b(j)\u0001\n,\nj = 1, 2, . . . , J.\nHere T (j) is a full connection matrix without special structures. So a deep\nCNN is a special multi-layer neural network with sparse convolutional ma-\ntrices. This sparsity gives diﬃculty in developing a mathematical theory for\ndeep CNNs, since the techniques in the literature of fully connected shallow\nor multi-layer neural networks do not apply. Our novelty to overcome the\ndiﬃculty is to factorize an arbitrary ﬁnitely supported sequence into convo-\nlutions of ﬁlter masks {w(j)}J\nj=1 supported in {0, 1, . . ., s}. Our method can\nbe applied to distributed learning algorithms [21, 22].\nRecently there have been quite a few papers [23, 24, 25, 26, 27, 28] on\napproximation and representation of functions by deep neural networks and\nbeneﬁt of depth, but all these results are for fully connected networks without\npre-speciﬁed structures, not for deep CNNs. In particular, it was shown in\n[27, 28] that the rate of approximaton of some function classes by multi-layer\nfully connected neural networks may be achieved by networks with sparse\nconnection matrices T (j), but the locations of the sparse connections are\nunknown. This sparsity of unknown pattern is totally diﬀerent from that of\ndeep CNNs, the latter enables computing methods like stochastic gradient\ndescent to learn values of the free parameters eﬃciently.\nDeep CNNs are often combined with pooling, a small number of fully\nconnected layers, and some other techniques for improving the practical per-\nformance of deep learning. Our purpose to analyze purely convolutional net-\nworks is to demonstrate that convolution makes full use of shift-invariance\nproperties of speeches and images for extracting data features eﬃciently.\nAlso, for processing an image, convolutions based on the 2-D lattice Z2 are\nimplemented by taking inner products of (s + 1) × (s + 1) ﬁlter matrices\nwith shifted patches of the image. Though we do not consider such deep\nlearning algorithms in this paper, some of our ideas can be used to estab-\nlish mathematical theories for more general deep neural networks involving\nconvolutions.\nMethods\nFor approximation in C(Ω) we can only consider those Sobolev spaces\nwhich can be embedded into the space of continuous functions, that is, those\n6\nspaces with the regularity index r > d\n2. To establish rates of approximation\nwe require r > d\n2 + 2 in Theorem B. In this case, the set Hr(Ω) is dense in\nC(Ω), so Theorem A follows from Theorem B by scaling.\nProof of Theorem B. Let J ≥\n2d\ns−1 and m be the integer part of (s−1)J\nd\n−1 ≥\n1. In our assumption, f = F|Ωfor some function F ∈Hr(Rd) with the\nFourier transform bF(ω) giving the norm ∥F∥=\n\r\r\r(1 + |ω|2)r/2 bF(ω)\n\r\r\r\nL2. By\nthe Schwarz inequality and the condition r > d\n2+2, vF,2 :=\nR\nRd ∥ω∥2\n1\n\f\f\f bF(ω)\n\f\f\f dω ≤\ncd,r ∥F∥where cd,r is the ﬁnite constant\n\r\r\r∥ω∥2\n1 (1 + |ω|2)−r/2\r\r\r\nL2. Then we ap-\nply a recent result from [29] on ridge approximation to F|[−1,1]d and know\nthat there exists a linear combination of ramp ridge functions of the form\nFm(x) = β0 + α0 · x + v\nm\nm\nX\nk=1\nβk (αk · x −tk)+\nwith βk ∈[−1, 1], ∥αk∥1 = 1, tk ∈[0, 1], β0 = F(0), α0 = ∇F(0) and |v| ≤\n2vF,2 such that\n∥F −Fm∥C([−1,1]d) ≤c0vF,2 max\nnp\nlog m,\n√\nd\no\nm−1\n2−1\nd\nfor some universal constant c0 > 0.\nNow we turn to the key step of constructing the ﬁlter mask sequence w.\nDeﬁne a sequence W supported in {0, . . . , (m + 1)d −1} by stacking the\nvectors α0, α1, . . . , αm (with components reversed) by\n\u0002\nW(m+1)d−1 . . . W1 W0\n\u0003\n=\n\u0002\nαT\nm . . . αT\n1 αT\n0\n\u0003\n.\nWe apply Theorem C to the sequence W with support in {0, 1, . . . , (m+1)d}\nand ﬁnd a sequence of ﬁlter masks w = {w(j)} ˆJ\nj=1 supported in {0, 1, . . . , s}\nwith ˆJ < (m+1)d\ns−1\n+ 1 such that W = w( ˆJ)∗w( ˆJ−1)∗. . . ∗w(2)∗w(1). The choice\nof m implies (m+1)d\ns−1\n≤J. So ˆJ ≤J and by taking w( ˆJ+1) = . . . = w(J) to be\nthe delta sequence, we have W = w(J)∗w(J−1)∗. . . ∗w(2)∗w(1). This tells us\n[9] that\nT (J) . . . T (1) = T W\nwhere T W is the dJ × d matrix given by [Wℓ−k]ℓ=1,...,dJ,k=1,...,d. Observe from\nthe deﬁnition of the sequence W that for k = 0, 1, . . . , m, the (k +1)d-th row\nof T W is exactly the transpose of αk. Also, since Js ≥(m + 1)d, we have\nWJs = 0 and the last row of T W is a zero row.\n7\nThen we construct b. Denote ∥w∥1 = P∞\nk=−∞|wk|, B(0) = maxx∈Ωmaxk=1,...,d |xk|\nand B(j) = ∥w(j)∥1 . . . ∥w(1)∥1B(0) for j ≥1. Then we have\n\r\r\u0000T (j) . . . T (1)x\n\u0001\nk\n\r\r\nC(Ω) ≤B(j),\n∀k = 1, . . . , dj.\nTake b(1) = −B(1)1d1 := −B(1)(1, . . . , 1)T, and\nb(j) = B(j−1)T (j)1dj−1 −B(j)1dj,\nj = 1, . . . , J −1.\nThen for j = 1, . . . , J −1, we have\nh(j)(x) = T (j) . . . T (1)x + B(j)1dj\nand b(j)\nℓ\n= B(j−1) Ps\nk=0 w(j)\nk −B(j) = b(j)\ns+1 for ℓ= s + 1, . . . , dj −s. Hence the\nbias vectors are of the required form.\nFinally, we take the bias vector b(J) by setting b(J)\nℓ\nto be\n\n\n\nB(J−1)(T (J)1dJ−1)ℓ−B(J),\nif ℓ= d, d + Js,\nB(J−1)(T (J)1dJ−1)ℓ+ tk,\nif ℓ= (k + 1)d, 1 ≤k ≤m,\nB(J−1)(T (J)1dJ−1)ℓ+ B(J),\notherwise.\nSubstituting this bias vector and the expression for h(J−1)(x) into the iterative\nrelation of the deep CNN, we see from the identity T (J) . . . T (1) = T W and\nthe deﬁnition of the sequence W that the ℓ-th component h(J)\nℓ(x) of h(J)(x)\nequals\n\n\n\n\n\n\n\nα0 · x + B(J),\nif ℓ= d,\nB(J),\nif ℓ= d + Js,\n(αk · x −tk)+ ,\nif ℓ= (k + 1)d, 1 ≤k ≤m,\n0,\notherwise.\nThus, we can take f w,b\nJ\n= Fm|Ω∈span{h(J)\nk (x)}dJ\nk=1 = Hw,b\nJ\nand know that\nthe error ∥f −f w,b\nJ\n∥C(Ω) ≤∥F −Fm∥C([−1,1]d) can be bounded as\n∥f −f w,b\nJ\n∥C(Ω) ≤c0vF,2 max\nnp\nlog m,\n√\nd\no\nm−1\n2−1\nd.\nBut 1\n2(s −1)J ≤md < (s −1)J and 2r −d −4 ≥1. By a polar coordinate\ntransformation, cd,rd1+ 1\nd ≤\nq\nd6πd/2\nΓ( d\n2 +1)\n\u0010\n1 +\n1\n√2r−d−4\n\u0011\nwhich can be bounded by\nan absolute constant c′ := maxℓ∈N 2\nq\nℓ6πℓ/2/Γ( ℓ\n2 + 1). Therefore,\n∥f −f w,b\nJ\n∥C(Ω) ≤2c0c′ ∥F∥\np\nlog JJ−1\n2−1\nd.\n8\nThis proves Theorem B by taking c = 2c0c′.\nConvolutional factorizations have been considered in our recent work [9]\nfor sequences W supported in {0, 1, . . . , S} with S ≥d under the special\nrestrictions W0 > 0 and WS ̸= 0. Theorem C gives a more general result by\nimproving the bound for J in [9] and removing the special restrictions on W0\nand WS.\nProof of Theorem C. We apply a useful concept from the literature of\nwavelets [10], the symbol ew of a sequence w ﬁnitely supported in the set of\nnonnegative integers, deﬁned as a polynomial on C by ew(z) = P∞\nk=0 wkzk.\nThe symbol of the convoluted sequence a∗b is given by f\na∗b(z) = ea(z)eb(z).\nNotice that the symbol f\nW of the sequence W supported in {0, . . . , M} is a\npolynomial of degree M with real coeﬃcients for some 0 ≤M ≤M. So we\nknow that complex roots zk = xk + iyk of f\nW with xk ̸= 0 appear in pairs\nand by (z −zk)(z −zk) = z2 −2xkz + (x2\nk + y2\nk), the polynomial f\nW(z) can\nbe completely factorized as\nf\nW(z) = WMΠK\nk=1\n\b\nz2 −2xkz +\n\u0000x2\nk + y2\nk\n\u0001\t\nΠM\nk=2K+1(z −xk),\nwhere 2K is the number of complex roots with multiplicity, and M −2K is\nthe number of real roots with multiplicity. By taking groups of up to s/2\nquadratic factors (or (s−1)/2 quadratic factors with a linear factor) and s lin-\near factors in the above factorization, we get f\nW(z) = g\nw(J)(z) . . . g\nw(2)(z)g\nw(1)(z),\na factorization of f\nW into polynomials of degree up to s, which yields a de-\nsired convolutional factorization W = w(J)∗w(J−1)∗. . . ∗w(2)∗w(1) and proves\nTheorem C.\nAcknowledgments\nThe author would like to thank Gilbert Strang and Steve Smale for their\ndetailed suggestions and encouragement. The work described in this paper\nis supported partially by the Research Grants Council of Hong Kong [Project\nNo CityU 11306617] and by National Nature Science Foundation of China\n[Grant No 11461161006].\nReferences\n[1] LeCun Y, Bottou L, Bengio Y, Haﬀner P (1998) Gradient-based learning\napplied to document recognition. Proceedings of the IEEE 86: 2278–\n2324.\n9\n[2] Hinton G, Osindero S, Teh Y (2006) A fast learning algorithm for deep\nbelief nets. Neural Computation 18: 1527–1554.\n[3] Krizhevsky A, Sutskever I, Hinton G (2012) Imagenet classiﬁcation with\ndeep convolutional neural networks NIPS: 2097–1105.\n[4] Goodfellow I, Bengio Y, Courville A (2016) Deep Learning. MIT Press.\n[5] LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521: 436.\n[6] Bruna J, Mallat S (2013) Invariant scattering convolution networks.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 35:\n1872–1886.\n[7] Mallat S (2016) Understanding deep convolutional networks. Philosoph-\nical Transactions of the Royal Society A 374: 20150203.\n[8] Mhaskar H, Poggio T (2016) Deep vs. shallow networks: An approxi-\nmation theory perspective. Analysis and Applications 14: 829–848.\n[9] Zhou\nD\n(2018)\nDeep\ndistributed\nconvolutional\nneural\nnet-\nworks:\nuniversality.\nAnalysis and Applications:\nin press.\nDOI:\n10.1142/S0219530518500124\n[10] Daubechies I (1992) Ten Lectures on Wavelets, SIAM.\n[11] Strang G (2018) Linear Algebra and Learning from Data. Book manus-\ncipt, MIT.\n[12] Smale S, Zhou D (2004) Shannon sampling and function reconstruction\nfrom point values, Bulletins of the American Mathematical Society 41:\n279–305.\n[13] Fan J, Hu T, Wu Q, Zhou D (2016) Consistency analysis of an em-\npirical minimum error entropy algorithm. Applied and Computational\nHarmonic Analysis 41: 164–189.\n[14] Cybenko G (1989) Approximations by superpositions of sigmoidal func-\ntions. Mathematics of Control, Signals, and Systems 2: 303–314.\n[15] Hornik K, Stinchcombe M, White H (1989) Multilayer feedforward net-\nworks are universal approximators. Neural networks 2: 359-366.\n[16] Barron A (1993) Universal approximation bounds for superpositions of a\nsigmoidal function. IEEE Transactions on Information Theory 39: 930-\n945.\n10\n[17] Leshno M, Lin Y, Pinkus A, Schocken S (1993) Multilayer feedforward\nnetworks with a non-polynomial activation function can approximate\nany function. Neural Networks 6: 861–867.\n[18] Pinkus A (1999) Approximation theory of the MLP model in neural\nnetworks. Acta Numerica 8: 143–195.\n[19] Mhaskar H (1993) Approximation properties of a multilayered feedfor-\nward artiﬁcial neural network. Advances in Computational Mathematics\n1: 61–80.\n[20] Chui C, Li X, Mhaskar H (1996) Limitations of the approximation ca-\npabilities of neural networks with one hidden layer. Advances in Com-\nputational Mathematics 5: 233–243.\n[21] Lin S, Guo X, Zhou D (2017) Distributed learning with regularized least\nsquares. Journal of Machine Learning Research 18: 1–31.\n[22] Guo Z, Lin S, Zhou D (2017) Learning theory of distributed spectral\nalgorithms. Inverse Problems 33: 074009 (29pp).\n[23] Telgarsky M (2016) Beneﬁts of depth in neural networks. 29th Annual\nConference on Learning Theory PMLR 49:1517–1539.\n[24] Eldan R, Shamir O (2016) The power of depth for feedforward neural\nnetworks. COLT: 907-940.\n[25] Yarotsky D (2017) Error bounds for approximations with deep ReLU\nnetworks. Neural Networks 94: 103–114.\n[26] Shaham U, Cloninger A, Coifman R (2018) Provable approximation\nproperties for deep neural networks. Applied and Computational Har-\nmonic Analysis 44: 537–557.\n[27] B¨olcskei\nH,\nGrohs\nP,\nKutyniok\nG,\nPetersen\nP\n(2018)\nOpti-\nmal approximation with sparsely connected deep neural networks.\narXiv:1705.01714v4.\n[28] Petersen\nP,\nVoigtlaender\nV\n(2018)\nOptimal\napproximation\nof\npiecewise\nsmooth\nfunctions\nusing\ndeep\nReLU\nneural\nnetworks.\narXiv:1709.05289v4.\n[29] Klusowski\nJ,\nBarron A\n(2018) Uniform approximation\nby neu-\nral networks activated by ﬁrst and second order ridge splines.\narXiv:1607.07819v2.\n11\n",
  "categories": [
    "cs.LG",
    "stat.ML",
    "68Q32"
  ],
  "published": "2018-05-28",
  "updated": "2018-07-20"
}