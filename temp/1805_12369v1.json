{
  "id": "http://arxiv.org/abs/1805.12369v1",
  "title": "Reinforced Continual Learning",
  "authors": [
    "Ju Xu",
    "Zhanxing Zhu"
  ],
  "abstract": "Most artificial intelligence models have limiting ability to solve new tasks\nfaster, without forgetting previously acquired knowledge. The recently emerging\nparadigm of continual learning aims to solve this issue, in which the model\nlearns various tasks in a sequential fashion. In this work, a novel approach\nfor continual learning is proposed, which searches for the best neural\narchitecture for each coming task via sophisticatedly designed reinforcement\nlearning strategies. We name it as Reinforced Continual Learning. Our method\nnot only has good performance on preventing catastrophic forgetting but also\nfits new tasks well. The experiments on sequential classification tasks for\nvariants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach\noutperforms existing continual learning alternatives for deep networks.",
  "text": "Reinforced Continual Learning\nJu Xu\nCenter for Data Science, Peking University\nBeijing, China\nxuju@pku.edu.cn\nZhanxing Zhu ∗\nCenter for Data Science, Peking University &\nBeijing Institute of Big Data Research (BIBDR)\nBeijing, China\nzhanxing.zhu@pku.edu.cn\nAbstract\nMost artiﬁcial intelligence models have limiting ability to solve new tasks faster,\nwithout forgetting previously acquired knowledge. The recently emerging paradigm\nof continual learning aims to solve this issue, in which the model learns various\ntasks in a sequential fashion. In this work, a novel approach for continual learning\nis proposed, which searches for the best neural architecture for each coming task\nvia sophisticatedly designed reinforcement learning strategies. We name it as\nReinforced Continual Learning. Our method not only has good performance on\npreventing catastrophic forgetting but also ﬁts new tasks well. The experiments\non sequential classiﬁcation tasks for variants of MNIST and CIFAR-100 datasets\ndemonstrate that the proposed approach outperforms existing continual learning\nalternatives for deep networks.\n1\nIntroduction\nContinual learning, or lifelong learning [13], the ability to learn consecutive tasks without forgetting\nhow to perform previously trained tasks, is an important topic for developing artiﬁcial intelligence.\nThe primary goal of continual learning is to overcome the forgetting of learned tasks and to leverage\nthe earlier knowledge for obtaining better performance or faster convergence/training speed on the\nnewly coming tasks.\nIn deep learning community, two groups of strategies have been developed to alleviate the problem of\nforgetting the previously trained tasks, distinguished by whether the network architecture changes\nduring learning.\nThe ﬁrst category of approaches maintain a ﬁxed network architecture with large capacity. When\ntraining the network for consecutive tasks, some regularization term is enforced to prevent the\nmodel parameters from deviating too much from the previous learned parameters according to their\nsigniﬁcance to old tasks [3, 17]. In [5], the authors proposed to incrementally matches the moment\nof the posterior distribution of the neural network which is trained on the ﬁrst and the second task,\nrespectively. Alternatively, an episodic memory [6] is budgeted to store the subsets of previous\ndatasets, and then trained together with the new task. Fernando et al. [2] proposed PathNet, in which\na neural network has ten or twenty modules in each layer, and three or four modules are picked\nfor one task in each layer by an evolutionary approach. However, these methods typically require\nunnecessarily large-capacity networks, particularly when the number of tasks is large, since the\nnetwork architecture is never dynamically adjusted during training.\nThe other group of methods for overcoming catastrophic forgetting dynamically expand the network\nto accommodate the new coming task while keeping the parameters of previous architecture unchang-\ning. Progressive networks [9] expand the architectures with a ﬁxed size of nodes or layers, leading to\n∗Corresponding author.\nPreprint. Work in progress.\narXiv:1805.12369v1  [cs.LG]  31 May 2018\nan extremely large network structure particularly faced with a large number of sequential tasks. The\nresultant complex architecture might be expensive to store and even unnecessary due to its high re-\ndundancy. Dynamically Expandable Network (DEN, [15] alleviated this issue slightly by introducing\ngroup sparsity regularization when adding new parameters to the original network; unfortunately,\nthere involves many hyperparameters in DEN, including various regularization and thresholding ones,\nwhich need to be tuned carefully due to the high sensitivity to the model performance.\nIn this work, in order to better facilitate knowledge transfer and avoid catastrophic forgetting, we\nprovide a novel framework to adaptively expand the network. Faced with a new task, deciding optimal\nnumber of nodes/ﬁlters to add for each layer is posed as a combinatorial optimization problem. We\nprovide a sophisticatedly designed reinforcement learning method to solve this problem. Thus, we\nname it as Reinforced Continual Learning (RCL). In RCL, a controller implemented as a recurrent\nneural network is adopted to determine the best architectural hyper-parameters of neural networks\nfor each task. We train the controller by an actor-critic strategy guided by a reward signal deriving\nfrom both validation accuracy and network complexity. This can maintain the prediction accuracy\non older tasks as much as possible while reducing the overall model complexity. To the best of our\nknowledge, the proposal is the ﬁrst attempt that employs the reinforcement learning for solving the\ncontinual learning problems.\nRCL not only differs from adding a ﬁxed number of units to the old network for solving a new task [9],\nwhich might be suboptimal and computationally expensive, but also distinguishes from [15] as well\nthat performs group sparsity regularization on the added parameters. We validate the effectiveness of\nRCL on various sequential tasks. And the results show that RCL can obtain better performance than\nexisting methods even with adding much less units.\nThe rest of this paper is organized as follows. In Section 2, we introduce the preliminary knowledge\non reinforcement learning. We propose the new method RCL in Section 3, a model to learn a\nsequence of tasks dynamically based on reinforcement learning. In Section 4, we implement various\nexperiments to demonstrate the superiority of RCL over other state-of-the-art methods. Finally, we\nconclude our paper in Section 5 and provide some directions for future research.\n2\nPreliminaries of Reinforcement learning\nReinforcement learning [11] deals with learning a policy for an agent interacting in an unknown\nenvironment. It has been applied successfully to various problems, such as games [7, 10], natural\nlanguage processing [16], neural architecture/optimizer search [18, 1] and so on. At each step,\nan agent observes the current state st of the environment, decides of an action at according to a\npolicy π(at|st), and observes a reward signal rt+1. The goal of the agent is to ﬁnd a policy that\nmaximizes the expected sum of discounted rewards Rt, Rt = P∞\nt′=t+1 γt′−t−1rt′, where γ ∈(0, 1]\nis a discount factor that determines the importance of future rewards. The value function of a policy\nπ is deﬁned as the expected return Vπ(s) = Eπ[P∞\nt=0 γtrt+1|s0 = s] and its action-value function\nas Qπ(s, a) = Eπ[P∞\nt=0 γtrt+1|s0 = s, a0 = a].\nPolicy gradient methods address the problem of ﬁnding a good policy by performing stochastic\ngradient descent to optimize a performance objective over a given family of parametrized stochastic\npolicies πθ(a|s) parameterized by θ. The policy gradient theorem [12] provides expressions for\nthe gradient of the average reward and discounted reward objectives with respect to θ. In the\ndiscounted setting, the objective is deﬁned with respect to a designated start state (or distribution) s0:\nρ(θ, s0) = Eπθ[P∞\nt=0 γtrt+1|s0]. The policy gradient theorem shows that:\n∂ρ(θ, s0)\n∂θ\n=\nX\ns\nµπθ(s|s0)\nX\na\n∂ππθ(a|s)\n∂θ\nQπθ(s, a).\n(1)\nwhere µπθ(s|s0) = P∞\nt=0 γtP(st = s|s0).\n3\nOur Proposal: Reinforced Continual Learning\nIn this section, we elaborate on the new framework for continual learning, Reinforced Continual\nLearning(RCL). RCL consists of three networks, controller, value network, and task network. The\ncontroller is implemented as a Long Short-Term Memory network (LSTM) for generating policies\n2\nht\nt-1\nt\noutput\nlayer N\nlayer 1\ninput\n……\nnumber \nof filters \naddedb\ner of\nlayer N-1\nlayer N\nlayer N+1\nnumber \nof filters \naddedb\ner of\nnumber \nof filters \naddedb\ner of\n(a)\n(b)\nFigure 1: (a) RCL adaptively expands the each layer of network when t-th task arrives. (b) The\ncontroller implemented as a RNN to determine how many ﬁlters to add for the new task.\nand determining how many ﬁlters or nodes will be added for each task. We design the value network\nas a fully-connected network, which approximates the value of the state. The task network can be any\nnetwork of interest for solving a particular task, such as image classiﬁcation or object detection. In this\npaper, we use a convolutional network (CNN) as the task network to demonstrate how RCL adaptively\nexpands this CNN to prevent forgetting, though our method can not only adapt to convolutional\nnetworks, but also to fully-connected networks.\n3.1\nThe Controller\nFigure 1(a) visually shows how RCL expands the network when a new task arrives. After the learning\nprocess of task t −1 ﬁnishes and task t arrives, we use a controller to decide how many ﬁlters or\nnodes should be added to each layer. In order to prevent semantic drift, we withhold modiﬁcation of\nnetwork weights for previous tasks and only train the newly added ﬁlters. After we have trained the\nmodel for task t, we timestamp each newly added ﬁlter by the shape of every layer to prevent the\ncaused semantic drift. During the inference time, each task only employs the parameters introduced\nin stage t, and does not consider the new ﬁlters added in the later tasks.\nSuppose the task network has m layers, when faced with a newly coming task, for each layer i, we\nspecify the the number of ﬁlters to add in the range between 0 and ni −1. A straightforward idea\nto obtain the optimal conﬁguration of added ﬁlters for m layers is to traverse all the combinatorial\ncombinations of actions. However, for an m-layer network, the time complexity of collecting the best\naction combination is O(Qm\n1 ni), which is NP-hard and unacceptable for very deep architectures\nsuch as VGG and ResNet.\nTo deal with this issue, we treat a series of actions as a ﬁxed-length string. It is possible to use a\ncontroller to generate such a string, representing how many ﬁlters should be added in each layer.\nSince there is a recurrent relationship between consecutive layers, the controller can be naturally\ndesigned as a LSTM network. At the ﬁrst step, the controller network receives an empty embedding\nas input (i.e. the state s) for the current task, which will be ﬁxed during the training. For each task t,\nwe equip the network with softmax output, pt,i ∈Rni representing the probabilities of sampling each\naction for layer i, i.e. the number of ﬁlters to be added. We design the LSTM in an autoregressive\nmanner, as Figure 1(b) shows, the probability pt,i in the previous step is fed as input into the next\nstep. This process is circulated until we obtain the actions and probabilities for all the m layers. And\nthe policy probability of the sequence of actions a1:m follows the product rule,\nπ(a1:m|s; θc) =\nm\nY\ni=1\npt,i,ai,\n(2)\nwhere θc denotes the parameters of the controller network.\n3.2\nThe Task Network\nWe deal with T tasks arriving in a sequential manner with training dataset Dt = {xi, yi}Nt\ni=1 ,\nvalidation dataset Vt = {xi, yi}Mt\ni=1, test dataset Tt = {xi, yi}Kt\ni=1 at time t. For the ﬁrst task, we train\n3\na basic task network that performs well enough via solving a standard supervised learning problem,\nmin\nW1 L1(W1; D1).\n(3)\nWe deﬁne the well-trained parameters as W a\nt for task t. When the t-th task arrives, we have already\nknown the best parameters W a\nt−1 for task t −1. Now we use the controller to decide how many ﬁlters\nshould be added to each layer, and then we obtain an expanded child network, whose parameters to\nbe learned are denoted as Wt (including W a\nt−1). The training procedure for the new task is as follows,\nkeeping W a\nt−1 ﬁxed and only back-propagating the newly added parameters of Wt\\W a\nt−1. Thus, the\noptimization formula for the new task is,\nmin\nWt\\W a\nt−1\nLt(Wt; Dt).\n(4)\nWe use stochastic gradient descent to learn the newly added ﬁlters with η as the learning rate,\nWt\\W a\nt−1 ←−Wt\\W a\nt−1 −η∇Wt\\W a\nt−1Lt.\n(5)\nThe expanded child network will be trained until the required number of epochs or convergence\nare reached. And then we test the child network on the validation dataset Vt and the corresponding\naccuracy At will be returned. The parameters of the expanded network achieving the maximal reward\n(described in Section 3.3) will be the optimal ones for task t, and we store them for later tasks.\n3.3\nReward Design\nIn order to facilitate our controller to generate better actions over time, we need design a reward\nfunction to reﬂect the performance of our actions. Considering both the validation accuracy and\ncomplexity of the expanded network, we design the reward for task t by the combination of the two\nterms,\nRt = At(St, a1:m) + αCt(St, a1:m),\n(6)\nwhere At represents the validation accuracy on Vt, the network complexity as Ct =\nm\nP\ni=1\nki, ki is\nthe numbers of ﬁlters added in layer i, and α is a parameter to balance between the prediction\nperformance and model complexity. Since Rt is non-differentiable, we use policy gradient to update\nthe controller, described in the following section.\n3.4\nTraining Procedures\nThe controller’s prediction can be viewed as a list of actions a1:m, which means the number of ﬁlters\nadded in m layers , to design an new architecture for a child network and then be trained in a new\ntask. At convergence, this child network will achieve an accuracy At on a validation dataset and the\nmodel complexity Ct, ﬁnally we can obtain the reward Rt as deﬁned in Eq. (6). We can use this\nreward Rt and reinforcement learning to train the controller.\nTo ﬁnd the optimal incremental architecture the new task t, the controller aims to maximize its\nexpected reward,\nJ(θc) = Vθc(st).\n(7)\nwhere Vθc is the true value function. In order to accelerate policy gradient training over θc, we\nuse actor–critic methods with a value network parameterized by θv to approximate the state value\nV (st; θv). The REINFORCE algorithm [14] can be used to learn θc,\n∇θcJ(θc) = E\n\"X\na1:m\nπ(a1:m|st, θc)(R(st, a1:m) −V (st, θv))∇θcπ(a1:m|st, θc)\nπ(a1:m|st, θc)\n#\n.\n(8)\nA Monte Carlo approximation for the above quantity is,\n1\nN\nN\nX\ni=1\n∇θc log π(a(i)\n1:m|st; θc)\n\u0010\nR(st, a(i)\n1:m) −V (st, θv)\n\u0011\n.\n(9)\n4\nAlgorithm 1 RCL for Continual Learning\n1: Input: A sequence of dataset D = {D1, D2, . . . , DT }\n2: Output: W a\nT\n3: for t = 1, ..., T do\n4:\nif t = 1 then\n5:\nTrain the base network using ( 3) on the ﬁrst datasest D1 and obtain W a\n1 .\n6:\nelse\n7:\nExpand the network by Algorithm 2, and obtain the trained W a\nt .\n8:\nend if\n9: end for\nAlgorithm 2 Routine for Network Expansion\n1: Input: Current dataset Dt; previous parameter W a\nt−1; the size of action space for each layer\nni, i = 1 . . . , m; number of epochs for training the controller and value network, Te.\n2: Output: Network parameter W a\nt\n3: for i = 1, . . . , Te do\n4:\nGenerate actions a1:m by controller’s policy;\n5:\nGenerate W (i)\nt\nby expanding parameters W a\nt−1 according to a1:m;\n6:\nTrain the expanded network using Eq. (5) to obtain W (i)\nt .\n7:\nEvaluate the gradients of the controller and value network by Eq. (9) and Eq.(10),\nθc = θc + ηc∇θcJ(θc),\nθv = θv −ηv∇θvLv(θv).\n8: end for\n9: Return the best network parameter conﬁguration, W a\nt = argmaxW (i)\nt Rt(W (i)\nt ).\nwhere N is the batch size. For the value network, we utilize gradient-based method to update θv, the\ngradient of which can be evaluated as follows,\nLv = 1\nN\nN\nX\ni=1\n(V (st; θv) −R(st, a(i)\n1:m))2,\n∇θvLv = 2\nN\nN\nX\ni=1\n\u0010\nV (st; θv) −R(st, a(i)\n1:m)\n\u0011 ∂V (st; θv)\n∂θv\n.\n(10)\nFinally we summarize our RCL approach for continual learning in Algorithm 1, in which the\nsubroutine for network expansion is described in Algorithm 2.\n3.5\nComparison with Other Approaches\nAs a new framework for network expansion to achieve continual learning, RCL distinguishes from\nprogressive network [9] and DEN [15] from the following aspects.\n• Compared with DEN, instead of performing selective retraining and network split, RCL\nkeeps the learned parameters for previous tasks ﬁxed and only updates the added parameters.\nThrough this training strategy, RCL can totally prevent catastrophic forgetting due to the\nfreezing parameters for corresponding tasks.\n• Progressive neural networks expand the architecture with a ﬁxed number of units or ﬁlters.\nTo obtain a satisfying model accuracy when number of sequential tasks is large, the ﬁnal\ncomplexity of progressive nets is required to be extremely high. This directly leads to high\ncomputational burden both in training and inference, even difﬁcult for the storage of the\nentire model. To handle this issue, both RCL and DEN dynamically adjust the networks to\nreach a more economic architecture.\n• While DEN achieves the expandable network by sparse regularization, RCL adaptively\nexpands the network by reinforcement learning. However, the performance of DEN is\nquite sensitive to the various hyperparameters, including regularization parameters and\n5\nthresholding coefﬁcients. RCL largely reduces the number of hyperparameters and boils\ndown to only balancing the average validation accuracy and model complexity when the\ndesigned reward function. Through different experiments in Section 4, we demonstrate that\nRCL could achieve more stable results, and better model performance could be achieved\nsimultaneously with even much less neurons than DEN.\n4\nExperiments\nWe perform a variety of experiments to access the performance of RCL in continual learning. We\nwill report the accuracy, the model complexity and the training time consumption between our RCL\nand the state-of-the-art baselines. We implemented all the experiments in Tensorfolw framework on\nGPU Tesla K80.\nDatasets\n(1) MNIST Permutations [3]. Ten variants of the MNIST data, where each task is\ntransformed by a ﬁxed permutation of pixels. In the dataset, the samples from different task are not\nindependent and identically distributed; (2) MNIST Mix. Five MNIST permutations (P1, . . . , P5)\nand ﬁve variants of the MNIST dataset (R1, . . . , R5) where each contains digits rotated by a ﬁxed\nangle between 0 and 180 degrees. These tasks are arranged in the order P1, R1, P2, . . . , P5, R5. (3)\nIncremental CIFAR-100 [8]. Different from the original CIFAR-100, each task introduces a new set\nof classes. For the total number of tasks T, each new task contains digits from a subset of 100/T\nclasses. In this dataset, the distribution of the input is similar for all tasks, but the distribution of the\noutput is different.\nFor all of the above datasets, we set the number of tasks to be learned as T = 10. For the MNIST\ndatasets, each task contains 60000 training examples and 1000 test examples from 10 different classes.\nFor the CIFAR-100 datasets, each task contains 5000 train examples and 1000 examples from 10\ndifferent classes. The model observes the tasks one by one, and once the task had been observed, the\ntask will not be observed later during the training.\nBaselines\n(1) SN, a single network trained across all tasks; (2) EWC, deep network trained with\nelastic weight consolidation [3] for regularization; (3) GEM, gradient episodic memory [6]; (4) PGN,\nprogressive neural network proposed in [9]; (5) DEN, dynamically expandable network [15].\nBase network settings\n(1) Fully connected networks for MNIST Permutations and MNIST Mix\ndatasets. We use a three-layer network with 784-312-128-10 neurons with RELU activations; (2)\nLeNet is used for Incremental CIFAR-100. LeNet has two convolutional layers and three fully-\nconnected layers, the detailed structure of LeNet can be found in [4].\n4.1\nResults\nWe evaluate each compared approach by considering average test accuracy on all the tasks, model\ncomplexity and training time. Model complexity is measured via the number of model parameters\nafter training all the tasks. We ﬁrst report the test accuracy and model complexity of baselines and\nour proposed RCL for the three datasets in Figure 2.\nComparison between ﬁxed-size and expandable networks.\nFrom Figure 2, we can easily observe\nthat the approaches with ﬁxed-size network architectures, such as IN, EWC and GEM, own low\nmodel complexity, but their prediction accuracy is much worse than those methods with expandable\nnetworks, including PGN, DEN and RCL. This shows that dynamically expanding networks can\nindeed contribute to the model performance by a large margin.\nComparison between PGN, DEN and RCL.\nRegarding to the expandable networks, RCL outper-\nforms PGN and DEN on on both test accuracy and model complexity. Particularly, RCL achieves\nsigniﬁcant reduction on the number of parameters compared with PGN and DEN, e.g. for incremental\nCifar100 data, 42% and 53% parameter reduction, respectively.\nTo further see the difference of the three methods, we vary the hyperparameters settings and train the\nnetworks accordingly, and obtain how test accuracy changes with respect to the number of parameters,\nas shown in Figure 3. We can clearly observe that RCL can achieve signiﬁcant model reduction with\n6\n0.480 \n0.816 \n0.920 \n0.960 \n0.966 \n0.966 \n0.422 \n0.672 \n0.884 \n0.963 \n0.966 \n0.966 \n0.157 \n0.321 \n0.498 \n0.564 \n0.581 \n0.599 \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nSN\nEWC\nGEM\nPGN\nDEN\nRCL\ntest accuracy\nAverage  test accuracy across all tasks\nMNIST permutations\nMNIST mix\nCIFAR100\n0\n20\n40\n60\n80\nSN\nEWC\nGEM\nPGN\nDEN\nRCL\nparameters\nx 10000\nNumber of parameters\nMNIST permutations\nMNIST mix\nCIFAR100\nFigure 2: Top: Average test accuracy for all the datasets. Bottom: The number of parameters for\ndifferent methods.\n3\n4\n5\n6\n7\nparameters\n1e5\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\ntest accuracy\nMNIST permutations\nRCL\nDEN\nPGN\n3\n4\n5\n6\n7\nparameters\n1e5\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\ntest accuracy\nMNIST mix\nRCL\nDEN\nPGN\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nparameters\n1e5\n0.50\n0.52\n0.54\n0.56\n0.58\ntest accuracy\nCIFAR100\nRCL\nDEN\nPGN\nFigure 3: Average test accuracy v.s. model complexity for RCL, DEN and PGN.\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0\ntest accuracy\nMNIST permutations\n2\n4\n6\n8\n10\n0.2\n0.4\n0.6\n0.8\n1.0\ntest accuracy\nMNIST mix\nGEM\nEWC\nSN\nPGN\nDEN\nRCL\n0\n2\n4\n6\n8\n10\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\ntest accuracy\nCIFAR100\nFigure 4: Test accuracy on the ﬁrst task as more tasks are learned.\n7\nthe same test accuracy as that of PGN and DEN, and remarkable accuracy improvement with same\nsize of networks. This demonstrates the beneﬁts of employing reinforcement learning to adaptively\ncontrol the complexity of the entire model architecture.\nEvaluating the forgetting behavior.\nFigure 4 shows the evolution of the test accuracy on the ﬁrst\ntask as more tasks are learned. RCL and PGN exhibit no forgetting while the approaches without\nexpanding the networks raise catastrophic forgetting. Moreover, DEN can not completely prevent\nforgetting since it retrains the previous parameters when learning new tasks.\nTraining time\nWe report the wall clock training time for each compared method in Table 1). Since\nRCL is based on reinforcement learning, a large number of trials are typically required that leads to\nmore training time than other methods. Improving the training efﬁciency of reinforcement learning is\nstill an open problem, and we leave it as future work.\nTable 1: Training time (in seconds) of experiments for all methods.\nMethods\nIN\nEWC\nGEM\nDEN\nPGN\nRCL\nMNIST permutations\n173\n1319\n1628\n21686\n452\n34583\nMNIST mix\n170\n1342\n1661\n19690\n451\n23626\nCIFAR100\n149\n508\n7550\n1428\n167\n3936\nBalance between test accuracy and model complexity.\nWe control the tradeoff between the\nmodel performance and complexity through the coefﬁcient α in the reward function (6). Figure 5\nshows how varying α affects the test accuracy and number of model parameters. As expected, with\nincreasing α the model complexity drops signiﬁcantly while the model performance also deteriorate\ngradually. Interestingly, when α is small, accuracy drops much slower compared with the decreasing\nof the number of parameters. This observation could help to choose a suitable α such that a medium-\nsized network can still achieve a relatively good model performance.\n10\n4\n10\n3\n10\n2\nalpha\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\ntest accuracy\nMNIST permutations\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nparameters\n1e5\naccuracy\nparameters\n10\n4\n10\n3\n10\n2\nalpha\n0.88\n0.90\n0.92\n0.94\n0.96\ntest accuracy\nMNIST mix\n3.5\n4.0\n4.5\n5.0\n5.5\nparameters\n1e5\naccuracy\nparameters\n10\n4\n10\n3\n10\n2\nalpha\n0.40\n0.45\n0.50\n0.55\n0.60\ntest accuracy\nCIFAR 100\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nparameters\n1e5\naccuracy\nparameters\nFigure 5: Experiments on the inﬂuence of the parameter α in the reward design\n5\nConclusion\nWe have proposed a novel framework for continual learning, Reinforced Continual Learning. Our\nmethod searches for the best neural architecture for coming task by reinforcement learning, which\nincreases its capacity when necessary and effectively prevents semantic drift. We implement both\nfully connected and convolutional neural networks as our task networks, and validate them on\ndifferent datasets. The experiments demonstrate that our proposal outperforms the exiting baselines\nsigniﬁcantly both on prediction accuracy and model complexity.\nAs for future works, two directions are worthy of consideration. Firstly, we will develop new strategies\nfor RCL to facilitate backward transfer, i.e. improve previous tasks’ performance by learning new\ntasks. Moreover, how to reduce the training time of RCL is particularly important for large networks\nwith more layers.\n8\nReferences\n[1] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Neural optimizer search with\nreinforcement learning. In International Conference on Machine Learning(ICML), pages\n459–468, 2017.\n[2] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu,\nAlexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super\nneural networks. arXiv preprint arXiv:1701.08734, 2017.\n[3] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catas-\ntrophic forgetting in neural networks. Proceedings of the National Academy of Sciences,\n114(13):3521–3526, 2017.\n[4] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[5] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming\ncatastrophic forgetting by incremental moment matching. In Advances in Neural Information\nProcessing Systems, pages 4655–4665, 2017.\n[6] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.\nIn NIPS, 2017.\n[7] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Pe-\ntersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan\nWierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement\nlearning. Nature, 518(7540):529–533, 2015.\n[8] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl:\nIncremental classiﬁer and representation learning. In CVPR, pages 5533–5542. IEEE Computer\nSociety, 2017.\n[9] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,\nKoray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv\npreprint arXiv:1606.04671, 2016.\n[10] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of\ngo without human knowledge. Nature, 550(7676):354, 2017.\n[11] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Cambridge:\nMIT press, 1998.\n[12] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy\ngradient methods for reinforcement learning with function approximation. In Advances in\nNeural Information Processing Systems, pages 1057–1063, 1999.\n[13] Sebastian Thrun. A lifelong learning perspective for mobile robot control. In International\nConference on Intelligent Robots and Systems, 1995.\n[14] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Machine Learning, 8:229–256, 1992.\n[15] J. Yoon and E. Yang. Lifelong learning with dynamically expandable networks. arXiv preprint\narXiv:1708.01547, 2017.\n[16] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial\nnets with policy gradient. In AAAI, pages 2852–2858, 2017.\n[17] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic\nintelligence. In International Conference on Machine Learning (ICML), 2017.\n9\n[18] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. arXiv\npreprint arXiv:1611.01578, 2016.\nA\nExperiment settings\nIn this section, we will present the experiments details of our model and baselines. When dealing\nwith dataset MNIST permutations and dataset MNIST mix, we use a three-layer network with\n784-312-128-10 neurons, and the learning rate is 0.001, the batch size is 32, the training epochs are\n15 for all models. When expanding the network, the size of search space is 30 across all layers for\nRCL,DEN and PGN. As for CIFAR-100, we use LeNet as our task network. The training epochs are\n20 and the learning rate is 0.001. The search space is 5 in convolutional layers, 25 in fully-connected\nlayers for RCL,DEN and PGN.\nOur controller is implemented as a LSTM network.\nThe LSTM network has two layers,\nand the hidden size is 100. Our value network is implemented as a fully-connected network, which\nhas only one layer. The learning rate for our controller is 0.001, for our value network is 0.005.\nThe α in our reward design is 0.0003 for MNIST permutations, 0.0002 for MNIST mix,\nand 0.001 for dataset CIFAR-100. The l1_lambda is 0.00001, l2_lambda is 0.0001, gl_lambda is\n0.001, regular_lambda is 0.5, loss_thr is 0.01, spl_thr is 0.05 in DEN for MNIST permutations and\nMNIST mix. As for CIFAR-100, the hyperparameters in DEN is the same except regular_lambda is\n5.\n10\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2018-05-31",
  "updated": "2018-05-31"
}