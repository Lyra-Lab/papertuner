{
  "id": "http://arxiv.org/abs/1806.01265v2",
  "title": "Equivalence Between Wasserstein and Value-Aware Loss for Model-based Reinforcement Learning",
  "authors": [
    "Kavosh Asadi",
    "Evan Cater",
    "Dipendra Misra",
    "Michael L. Littman"
  ],
  "abstract": "Learning a generative model is a key component of model-based reinforcement\nlearning. Though learning a good model in the tabular setting is a simple task,\nlearning a useful model in the approximate setting is challenging. In this\ncontext, an important question is the loss function used for model learning as\nvarying the loss function can have a remarkable impact on effectiveness of\nplanning. Recently Farahmand et al. (2017) proposed a value-aware model\nlearning (VAML) objective that captures the structure of value function during\nmodel learning. Using tools from Asadi et al. (2018), we show that minimizing\nthe VAML objective is in fact equivalent to minimizing the Wasserstein metric.\nThis equivalence improves our understanding of value-aware models, and also\ncreates a theoretical foundation for applications of Wasserstein in model-based\nreinforcement~learning.",
  "text": "arXiv:1806.01265v2  [cs.LG]  8 Jul 2018\nEquivalence Between Wasserstein and Value-Aware Loss for\nModel-based Reinforcement Learning\nKavosh Asadi 1 Evan Cater 1 Dipendra Misra 2 Michael L. Littman 1\nAbstract\nLearning a generative model is a key component\nof model-based reinforcement learning. Though\nlearning a good model in the tabular setting\nis a simple task, learning a useful model in\nthe approximate setting is challenging.\nIn\nthis context, an important question is the loss\nfunction used for model learning as varying\nthe\nloss\nfunction\ncan\nhave\na\nremarkable\nimpact on effectiveness of planning. Recently\nFarahmand et al. (2017) proposed a value-aware\nmodel learning (VAML) objective that captures\nthe structure of value function during model\nlearning. Using tools from Asadi et al. (2018),\nwe show that minimizing the VAML objective is\nin fact equivalent to minimizing the Wasserstein\nmetric.\nThis\nequivalence\nimproves\nour\nunderstanding\nof\nvalue-aware\nmodels,\nand\nalso\ncreates\na\ntheoretical\nfoundation\nfor\napplications of Wasserstein\nin\nmodel-based\nreinforcement learning.\n1. Introduction\nThe model-based approach to reinforcement learning\nconsists of learning an internal model of the environment\nand planning with the learned model (Sutton & Barto,\n1998). The main promise of the model-based approach is\ndata-efﬁciency: the ability to perform policy improvements\nwith\na\nrelatively\nsmall\nnumber\nof\nenvironmental\ninteractions.\nAlthough the model-based approach is well-understood in\nthe tabular case (Kaelbling et al., 1996; Sutton & Barto,\n1998), the extension to approximate setting is difﬁcult.\nModels usually have non-zero generalization error due to\n1Department of Computer Science,\nBrown University,\nProvidence, USA 2Department of Computer Science, Cornell\nTech, New York, USA. Correspondence to:\nKavosh asadi\n<kavosh@brown.edu>.\nAccepted at the FAIM workshop “Prediction and Generative\nModeling in Reinforcement Learning”, Stockholm, Sweden, 2018.\nCopyright 2018 by the author(s).\nlimited training samples.\nMoreover, the model learning\nproblem can be unrelizable, leading to an imperfect model\nwith irreducible error (Ross & Bagnell, 2012; Talvitie,\n2014).\nSometimes referred to as the compounding\nerror phenomenon, it has been shown that such small\nmodeling errors can also compound after multiple steps and\ndegrade the policy learned using the model (Talvitie, 2014;\nVenkatraman et al., 2015; Asadi et al., 2018).\nOn way of addressing this problem is by learning a model\nthat is tailored to the speciﬁc planning algorithm we intend\nto use. That is, even though the model is imperfect, it is\nuseful for the planning algorithm that is going to leverage it.\nTo this end, Farahmand et al. (2017) proposed an objective\nfunction for model-based RL that captures the structure\nof value function during model learning to ensure that the\nmodel is useful for Value Iteration. Learning a model using\nthis loss, known as value-aware model learning (VAML)\nloss, empirically improved upon a model learned using\nmaximum-likelihood objective, thus providing a promising\ndirection for learning useful models in the approximate\nsetting.\nMore\nspeciﬁcally,\nVAML\nminimizes\nthe\nmaximum\nBellman error given the learned model, MDP dynamics,\nand an arbitrary space of value functions.\nAs we will\nshow, computing the Wasserstein metric involves a similar\nmaximization problem, but over a space of Lipschitz\nfunctions.\nUnder certain assumptions, we prove that\nthe value function of an MDP is Lipschitz.\nTherefore,\nminimizing the VAML objective is in fact equivalent to\nminimizing Wasserstein.\n2. Background\n2.1. MDPs\nWe consider the Markov decision process (MDP) setting\nin which the RL problem is formulated by the tuple\n⟨S, A, R, T, γ⟩.\nHere, S denotes a state space and A\ndenotes an action set. The functions R : S × A →R\nand T : S × A →Pr(S) denote the reward and transition\ndynamics. Finally γ ∈[0, 1) is the discount rate.\nEquivalence Between Wasserstein and Value-Aware Model-based Reinforcement Learning\n2.2. Lipschitz Continuity\nWe make use of the notion of “smoothness” of a function\nas quantiﬁed below.\nDeﬁnition 1. Given two metric spaces (M1, d1) and\n(M2, d2) consisting of a space and a distance metric, a\nfunction f : M1 7→M2 is Lipschitz continuous (sometimes\nsimply Lipschitz) if the Lipschitz constant, deﬁned as\nKd1,d2(f) :=\nsup\ns1∈S,s2∈S\nd2\n\u0000f(s1), f(s2)\n\u0001\nd1(s1, s2)\n,\n(1)\nis ﬁnite.\nEquivalently, for a Lipschitz f,\n∀s1, ∀s2\nd2\n\u0000f(s1), f(s2)\n\u0001\n≤Kd1,d2(f) d1(s1, s2) .\nNote that the input and output of f can generally be scalars,\nvectors, or probability distributions. A Lipschitz function\nf is called a non-expansion when Kd1,d2(f) = 1 and a\ncontraction when Kd1,d2(f) < 1. We also deﬁne Lipschitz\ncontinuity over a subset of inputs:\nDeﬁnition 2. A function f : M1 × A 7→M2 is uniformly\nLipschitz continuous in A if\nKA\nd1,d2(f) := sup\na∈A\nsup\ns1,s2\nd2\n\u0000f(s1, a), f(s2, a)\n\u0001\nd1(s1, s2)\n,\n(2)\nis ﬁnite.\nNote that the metric d1 is still deﬁned only on M1. Below\nwe also present two useful lemmas.\nLemma 1. (Composition Lemma) Deﬁne three metric\nspaces (M1, d1), (M2, d2), and (M3, d3). Deﬁne Lipschitz\nfunctions f : M2 7→M3 and g : M1 7→M2 with constants\nKd2,d3(f) and Kd1,d2(g). Then, h : f ◦g : M1 7→M3 is\nLipschitz with constant Kd1,d3(h) ≤Kd2,d3(f)Kd1,d2(g).\nProof.\nKd1,d3(h)\n=\nsup\ns1,s2\nd3\n\u0010\nf\n\u0000g(s1)\n\u0001\n, f\n\u0000g(s2)\n\u0001\u0011\nd1(s1, s2)\n=\nsup\ns1,s2\nd2\n\u0000g(s1), g(s2)\n\u0001\nd1(s1, s2)\nd3\n\u0010\nf\n\u0000g(s1)\n\u0001\n, f\n\u0000g(s2)\n\u0001\u0011\nd2\n\u0000g(s1), g(s2)\n\u0001\n≤\nsup\ns1,s2\nd2\n\u0000g(s1), g(s2)\n\u0001\nd1(s1, s2)\nsup\ns1,s2\nd3\n\u0000f(s1), f(s2)\n\u0001\nd2(s1, s2)\n=\nKd1,d2(g)Kd2,d3(f).\nLemma 2. (Summation Lemma) Deﬁne two vector spaces\n(M1, ∥∥) and (M2, ∥∥).\nDeﬁne Lipschitz functions f :\nM1 7→M2 and g : M1 7→M2 with constants K∥∥,∥∥(f)\nand K∥∥,∥∥(g). Then, h : f + g : M1 7→M2 is Lipschitz\nwith constant K∥∥,∥∥(h) ≤K∥∥,∥∥(f) + K∥∥,∥∥(g).\nProof.\nKd1,d2(h)\n:=\nsup\ns1,s2\n∥f(s2) + g(s2) −f(s1) −g(s1)∥\n∥s2 −s1∥\n≤\nsup\ns1,s2\n∥f(s2) −f(s1)∥\n∥s2 −s1∥\n+ ∥g(s2) −g(s1)∥\n∥s2 −s1∥\n≤\nsup\ns1,s2\n∥f(s2) −f(s1)∥\n∥s2 −s1∥\n+\nsup\ns1,s2\n∥g(s2) −g(s1)∥\n∥s2 −s1∥\n=\nK∥∥,∥∥(f) + K∥∥,∥∥(g)\n2.3. Distance Between Distributions\nWe require a notion of difference between two distributions\nquantiﬁed below.\nDeﬁnition 3. Given a metric space (M, d) and the set\nP(M) of all probability measures on M, the Wasserstein\nmetric (or the 1st Kantorovic metric) between two\nprobability distributions µ1 and µ2 in P(M) is deﬁned as\nW(µ1, µ2) := inf\nj∈Λ\nZ Z\nj(s1, s2)d(s1, s2)ds2 ds1 ,\n(3)\nwhere Λ denotes the collection of all joint distributions j\non M × M with marginals µ1 and µ2 (Vaserstein, 1969).\nWasserstein is linked to Lipschitz continuity using duality:\nW(µ1, µ2) =\nsup\nf:Kd,dR(f)≤1\nZ \u0000µ1(s) −µ2(s)\n\u0001\nf(s)ds .\n(4)\nThis equivalence is known as Kantorovich-Rubinstein\nduality (Kantorovich & Rubinstein, 1958; Villani, 2008).\nSometimes referred to as “Earth Mover’s distance”,\nWasserstein has recently become popular in machine\nlearning, namely in the context of generative adversarial\nnetworks (Arjovsky et al., 2017) and value distributions\nin reinforcement learning (Bellemare et al., 2017).\nWe\nalso deﬁne Kullback Leibler divergence (simply KL)\nas an alternative measure of difference between two\ndistributions:\nKL(µ1 || µ2) :=\nZ\nµ1(s) log µ1(s)\nµ2(s)ds .\nEquivalence Between Wasserstein and Value-Aware Model-based Reinforcement Learning\n3. Value-Aware Model Learning (VAML) Loss\nThe basic idea behind VAML (Farahmand et al., 2017) is to\nlearn a model tailored to the planning algorithm that intends\nto use it. Since Bellman equations (Bellman, 1957) are in\nthe core of many RL algorithms (Sutton & Barto, 1998),\nwe assume that the planner uses the following Bellman\nequation:\nQ(s, a) = R(s, a) + γ\nZ\nT (s′|s, a)f\n\u0000Q(s′, .)\n\u0001\nds′ ,\nwhere\nf\ncan\ngenerally\nbe\nany\narbitrary\noperator\n(Littman & Szepesv´ari, 1996) such as max. We also deﬁne:\nv(s′) := f\n\u0000Q(s′, .)\n\u0001\n.\nA good model bT could then be thought of as the one that\nminimizes the error:\nl(T, bT)(s, a)\n=\nR(s, a) + γ\nZ\nT (s′|s, a)v(s′)ds′\n−\nR(s, a) −γ\nZ\nbT(s′|s, a)v(s′)ds′\n=\nγ\nZ \u0000T (s′|s, a)−bT (s′|s, a)\n\u0001\nv(s′)ds′\nNote that minimizing this objective requires access to the\nvalue function in the ﬁrst place, but we can obviate this\nneed by leveraging Holder’s inequality:\nl( bT, T )(s, a)\n=\nγ\nZ \u0000T (s′|s, a) −bT (s′|s, a)\n\u0001\nv(s′)ds′\n≤\nγ\n\r\r\rT (s′|s, a) −bT (s′|s, a)\n\r\r\r\n1 ∥v∥∞\nFurther, we can use Pinsker’s inequality to write:\n\r\r\rT (·|s, a) −bT (·|s, a)\n\r\r\r\n1 ≤\nq\n2KL\n\u0000T (·|s, a)|| bT (·|s, a)\n\u0001\n.\nThis justiﬁes the use of maximum likelihood estimation\nfor model learning, a common practice in model-based\nRL\n(Bagnell & Schneider,\n2001;\nAbbeel et al.,\n2006;\nAgostini & Celaya, 2010),\nsince maximum likelihood\nestimation is equivalent to empirical KL minimization.\nHowever, there exists a major drawback with the KL\nobjective, namely that it ignores the structure of the value\nfunction during model learning. As a simple example, if\nthe value function is constant through the state-space, any\nrandomly chosen model bT will, in fact, yield zero Bellman\nerror. However, a model learning algorithm that ignores\nthe structure of value function can potentially require many\nsamples to provide any guarantee about the performance of\nlearned policy.\nConsider the objective function l(T, bT), and notice again\nthat v itself is not known so we cannot directly optimize for\nthis objective. Farahmand et al. (2017) proposed to search\nfor a model that results in lowest error given all possible\nvalue functions belonging to a speciﬁc class:\nL(T, ˆT)(s, a)= sup\nv∈F\n\f\f\f\nZ \u0000T (s′ | s, a)−bT(s′ | s, a)\n\u0001\nv(s′)ds′\f\f\f\n2\n(5)\nNote that minimizing this objective is shown to be tractable\nif, for example, F is restricted to the class of exponential\nfunctions. Observe that the VAML objective (5) is similar\nto the dual of Wasserstein (4), but the main difference is\nthe space of value functions. In the next section we show\nthat even the space of value functions are the same under\ncertain conditions.\n4. Lipschitz Generalized Value Iteration\nWe show that solving for a class of Bellman equations\nyields a Lipschitz value function.\nOur proof is in\nthe context of GVI (Littman & Szepesv´ari, 1996), which\ndeﬁnes Value Iteration (Bellman, 1957) with arbitrary\nbackup operators. We make use of the following lemmas.\nLemma 3. Given a non-expansion f : S 7→R:\nKA\ndS,dR\n\u0000 Z\nT (s′|s, a)f(s′)ds′\u0001\n≤KA\ndS,W\n\u0000T\n\u0001\n.\nProof. Starting from the deﬁnition, we write:\nKA\ndS,dR\n\u0000 Z\nT (s′|s, a)f(s′)ds′\u0001\n= sup\na\nsup\ns1,s2\n\f\f\f\nR \u0000T (s′|s1, a) −T (s′|s2, a)\n\u0001\nf(s′)ds′\f\f\f\nd(s1, s2)\n≤sup\na\nsup\ns1,s2\n\f\f\f supg\nR \u0000T (s′|s1, a) −T (s′|s2, a)\n\u0001\ng(s′)ds′\f\f\f\nd(s1, s2)\n(where KdS,dR(g) ≤1)\n= sup\na\nsup\ns1,s2\nsupg\nR \u0000T (s′|s1, a) −T (s′|s2, a)\n\u0001\ng(s′)ds′\nd(s1, s2)\n= sup\na\nsup\ns1,s2\nW\n\u0000T (·|s1, a), T (·|s2, a)\n\u0001\nd(s1, s2)\n= KA\ndS,W (T ) .\nLemma 4. The following operators are non-expansion\n(K∥·∥∞,dR(·) = 1):\n1. max(x), mean(x)\n2. ǫ-greedy(x) := ǫ mean(x) + (1 −ǫ)max(x)\n3. mmβ(x) := log\nP\ni eβxi\nn\nβ\nEquivalence Between Wasserstein and Value-Aware Model-based Reinforcement Learning\nProof. 1 is proven by Littman & Szepesv´ari (1996).\n2\nfollows from 1: (metrics not shown for brevity)\nK(ǫ-greedy(x))\n=\nK\n\u0000ǫ mean(x) + (1 −ǫ)max(x)\n\u0001\n≤\nǫK\n\u0000mean(x)\n\u0001\n+ (1 −ǫ)K\n\u0000max(x)\n\u0001\n=\n1\nFinally, 3 is proven multiple times in the literature.\n(Asadi & Littman, 2017; Nachum et al., 2017; Neu et al.,\n2017)\nAlgorithm 1 GVI algorithm\nInput: initial bQ(s, a), δ, and choose an operator f\nrepeat\ndiff ←0\nfor each s ∈S do\nfor each a ∈A do\nQcopy ←bQ(s, a)\nbQ(s, a)←R(s, a)+γ\nR\nT (s′ | s, a)f\n\u0000 bQ(s′, ·)\n\u0001\nds′\ndiff ←max\n\b\ndiff, |Qcopy −Q(s, a)|\n\t\nend for\nend for\nuntil diff < δ\nWe now present the main result of this paper.\nTheorem. For any choice of backup operator f outlined\nin Lemma 4, GVI computes a value function with\na Lipschitz constant bounded by\nKA\ndS ,dR(R)\n1−γKdS,W (T )\nif\nγKA\ndS,W (T ) < 1.\nProof. From Algorithm 1, in the nth round of GVI updates\nwe have:\nbQn+1(s, a) ←R(s, a) + γ\nZ\nT (s′ | s, a)f\n\u0000 bQn(s′, ·)\n\u0001\nds′.\nFirst observe that:\nKA\ndS,dR( bQn+1)\n\u0000due to Summation Lemma (2)\n\u0001\n≤KA\ndS,dR(R)+γKA\ndS,dR\n\u0000Z\nT (s′ | s, a)f\n\u0000 bQn(s′, ·)\n\u0001\nds′\u0001\n\u0000due to Lemma (3)\n\u0001\n≤KA\ndS,dR(R) + γKA\ndS,W (T ) KdS,R\n\u0010\nf\n\u0000 bQn(s, ·)\n\u0001\u0011\n\u0000due to Composition Lemma (1)\n\u0001\n≤KA\ndS,dR(R) + γKA\ndS,W (T )K∥·∥∞,dR(f)KA\ndS,dR( bQn)\n\u0000due to Lemma (4), the non-expansion property of f\n\u0001\n= KA\ndS,dR(R) + γKA\ndS,W (T )KA\ndS,dR( bQn)\nEquivalently:\nKA\ndS,dR( bQn+1)\n≤\nKA\ndS,dR(R)\nn\nX\ni=0\n\u0000γKA\ndS,W (T )\n\u0001i\n+\n\u0000γKA\ndS,W (T )\n\u0001n KA\ndS,dR( bQ0) .\nBy computing the limit of both sides, we get:\nlim\nn→∞KA\ndS,dR( bQn)\n≤\nlim\nn→∞KA\ndS,dR(R)\nn\nX\ni=0\n\u0000γKA\ndS,W (T )\n\u0001i\n+\nlim\nn→∞\n\u0000γKA\ndS,W (T )\n\u0001n KA\ndS,dR( bQ0)\n=\nKA\ndS,dR(R)\n1 −γKdS,W (T ) + 0 ,\nwhere we used the fact that\nlim\nn→∞\n\u0000γKA\ndS,W (T )\n\u0001n = 0 .\nThis concludes the proof.\nNow notice that as deﬁned earlier:\nbVn(s) := f\n\u0000 bQn(s, ·)\n\u0001\n,\nso as a relevant corollary of our theorem we get:\nKdS,dR\n\u0000v(s)\n\u0001\n=\nlim\nn→∞KdS,dR(bVn)\n=\nlim\nn→∞KdS,dR\n\u0010\nf\n\u0000 bQn(s, ·)\n\u0001\u0011\n≤\nlim\nn→∞KA\ndS,dR( bQn)\n≤\nKA\ndS,dR(R)\n1 −γKdS,W (T ) .\nThat is, solving for the ﬁxed point of this general class\nof Bellman equations results in a Lipschitz state-value\nfunction.\n5. Equivalence Between VAML and\nWasserstein\nWe now show the main claim of the paper, namely\nthat minimzing for the VAML objective is the same as\nminimizing the Wasserstein metric.\nConsider again the VAML objective:\nL(T, ˆT)(s, a)= sup\nv∈F\n\f\f\f\nZ \u0000T (s′ | s, a)−bT(s′ | s, a)\n\u0001\nv(s′)ds′\f\f\f\n2\nwhere F can generally be any class of functions. From our\ntheorem, however, the space of value functions F should\nbe restricted to Lipschitz functions. Moreover, it is easy to\ndesign an MDP and a policy such that a desired Lipschitz\nvalue function is attained.\nEquivalence Between Wasserstein and Value-Aware Model-based Reinforcement Learning\nThis space LC can then be deﬁned as follows:\nLC = {f : KdS,dR(f) ≤C} ,\nwhere\nC =\nKA\ndS,dR(R)\n1 −γKdS,W (T ) .\nSo we can rewrite the VAML objective L as follows:\nL\n\u0000T, ˆT\n\u0001\n(s, a)= sup\nf∈LC\n\f\f\f\nZ\nf(s)\n\u0000T (s′ | s, a)−bT(s′ | s, a)\n\u0001\nds′\f\f\f\n2\n= sup\nf∈LC\n\f\f\f\nZ\nC f(s)\nC\n\u0000T (s′ | s, a)−bT(s′ | s, a)\n\u0001\nds′\f\f\f\n2\n= C2 sup\ng∈L1\n\f\f\f\nZ\ng(s)\n\u0000T (s′ | s, a)−bT(s′ | s, a)\n\u0001\nds′\f\f\f\n2\n.\nIt\nis\nclear\nthat\na\nfunction\ng\nthat\nmaximizes\nthe\nKantorovich-Rubinstein dual form:\nsup\ng∈L1\nZ\ng(s)\n\u0000T (s′ | s, a) −bT(s′ | s, a)\n\u0001\nds′\n:= W(T (·|s, a), bT(·|s, a)) ,\nwill also maximize:\nL\n\u0000T, ˆT\n\u0001\n(s, a) =\n\f\f\f\nZ\ng(s)\n\u0000T (s′ | s, a)−bT(s′ | s, a)\n\u0001\nds′\f\f\f\n2\n.\nThis is due to the fact that ∀g ∈L1 ⇒−g ∈L1 and\nso computing absolute value or squaring the term will not\nchange arg max in this case.\nAs a result:\nL\n\u0000T, bT\n\u0001\n(s, a) =\n\u0010\nC W\n\u0000T (·|s, a), bT(·|s, a)\n\u0001\u00112\n.\nThis highlights a nice property of Wasserstein, namely that\nminimizing this metric yields a value-aware model.\n6. Conclusion and Future Work\nWe showed that the value function of an MDP is Lipschitz.\nThis result enabled us to draw a connection between\nvalue-aware model-based reinforcement learning and the\nWassertein metric.\nWe hypothesize that the value function is Lipschitz\nin a more general sense, and so, further investigation\nof\nLipschitz\ncontinuity\nof\nvalue\nfunctions\nshould\nbe interesting on its own.\nThe second interesting\ndirection relates to design of practical model-learning\nalgorithms\nthat\ncan\nminimize\nWasserstein.\nTwo\npromising directions are the use of generative adversarial\nnetworks\n(Goodfellow et al.,\n2014;\nArjovsky et al.,\n2017) or approximations such as entropic regularization\n(Frogner et al., 2015). We leave these two directions for\nfuture work.\nReferences\nAbbeel, Pieter, Quigley, Morgan, and Ng, Andrew Y.\nUsing inaccurate models in reinforcement learning. In\nProceedings of the 23rd international conference on\nMachine learning, pp. 1–8. ACM, 2006.\nAgostini, Alejandro and Celaya, Enric.\nReinforcement\nlearning with a gaussian mixture model.\nIn Neural\nNetworks\n(IJCNN),\nThe\n2010\nInternational\nJoint\nConference on, pp. 1–8. IEEE, 2010.\nArjovsky, Martin, Chintala, Soumith, and Bottou, L´eon.\nWasserstein\ngenerative adversarial networks.\nIn\nInternational Conference on Machine Learning, pp.\n214–223, 2017.\nAsadi, Kavosh and Littman, Michael L.\nAn alternative\nsoftmax operator for reinforcement learning.\nIn\nProceedings of the 34th International Conference on\nMachine Learning, pp. 243–252, 2017.\nAsadi, Kavosh, Misra, Dipendra, and Littman, Michael L.\nLipschitz continuity in\nmodel-based reinforcement\nlearning. arXiv preprint arXiv:1804.07193, 2018.\nBagnell, J Andrew and Schneider, Jeff G.\nAutonomous\nhelicopter control using reinforcement learning policy\nsearch methods.\nIn Robotics and Automation, 2001.\nProceedings 2001 ICRA. IEEE International Conference\non, volume 2, pp. 1615–1620. IEEE, 2001.\nBellemare, Marc G, Dabney, Will, and Munos, R´emi.\nA distributional perspective on reinforcement learning.\nIn International Conference on Machine Learning, pp.\n449–458, 2017.\nBellman, Richard. A markovian decision process. Journal\nof Mathematics and Mechanics, pp. 679–684, 1957.\nFarahmand, Amir-Massoud, Barreto, Andre, and Nikovski,\nDaniel.\nValue-Aware Loss Function for Model-based\nReinforcement Learning.\nIn Proceedings of the 20th\nInternational Conference on Artiﬁcial Intelligence and\nStatistics, pp. 1486–1494, 2017.\nFrogner, Charlie, Zhang, Chiyuan, Mobahi, Hossein,\nAraya, Mauricio, and Poggio, Tomaso A. Learning with\na wasserstein loss. In Advances in Neural Information\nProcessing Systems, pp. 2053–2061, 2015.\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\nAaron, and Bengio, Yoshua. Generative adversarial nets.\nIn Advances in neural information processing systems,\npp. 2672–2680, 2014.\nEquivalence Between Wasserstein and Value-Aware Model-based Reinforcement Learning\nKaelbling, Leslie Pack, Littman, Michael L., and Moore,\nAndrew W. Reinforcement learning: A survey. J. Artif.\nIntell. Res., 4:237–285, 1996.\nKantorovich, Leonid Vasilevich and Rubinstein, G Sh.\nOn a space of completely additive functions.\nVestnik\nLeningrad. Univ, 13(7):52–59, 1958.\nLittman,\nMichael\nL.\nand\nSzepesv´ari,\nCsaba.\nA\ngeneralized reinforcement-learningmodel: Convergence\nand\napplications.\nIn\nProceedings\nof\nthe\n13th\nInternational Conference on Machine Learning, pp.\n310–318, 1996.\nNachum, Oﬁr, Norouzi, Mohammad, Xu, Kelvin, and\nSchuurmans, Dale.\nBridging the gap between value\nand policy based reinforcement learning. arXiv preprint\narXiv:1702.08892, 2017.\nNeu, Gergely, Jonsson, Anders, and G´omez, Vicenc¸.\nA\nuniﬁed view of entropy-regularized Markov decision\nprocesses. arXiv preprint arXiv:1705.07798, 2017.\nRoss, St´ephane and Bagnell, Drew.\nAgnostic system\nidentiﬁcation for model-based reinforcement learning.\nIn Proceedings of the 29th International Conference on\nMachine Learning, ICML 2012, Edinburgh, Scotland,\nUK, June 26 - July 1, 2012, 2012.\nSutton, Richard S. and Barto, Andrew G. Reinforcement\nLearning: An Introduction. The MIT Press, 1998.\nTalvitie, Erik.\nModel regularization for stable sample\nrollouts. In Proceedings of the Thirtieth Conference on\nUncertainty in Artiﬁcial Intelligence, UAI 2014, Quebec\nCity, Quebec, Canada, July 23-27, 2014, pp. 780–789,\n2014.\nVaserstein,\nLeonid Nisonovich.\nMarkov processes\nover denumerable products of spaces, describing large\nsystems of automata. Problemy Peredachi Informatsii, 5\n(3):64–72, 1969.\nVenkatraman,\nArun,\nHebert,\nMartial,\nand\nBagnell,\nJ Andrew. Improving multi-step prediction of learned\ntime series models. In Proceedings of the Twenty-Ninth\nAAAI Conference on Artiﬁcial Intelligence, January\n25-30, 2015, Austin, Texas, USA., 2015.\nVillani, C´edric. Optimal transport: old and new, volume\n338. Springer Science & Business Media, 2008.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-06-01",
  "updated": "2018-07-08"
}