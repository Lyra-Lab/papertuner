{
  "id": "http://arxiv.org/abs/1806.06798v2",
  "title": "Implicit Policy for Reinforcement Learning",
  "authors": [
    "Yunhao Tang",
    "Shipra Agrawal"
  ],
  "abstract": "We introduce Implicit Policy, a general class of expressive policies that can\nflexibly represent complex action distributions in reinforcement learning, with\nefficient algorithms to compute entropy regularized policy gradients. We\nempirically show that, despite its simplicity in implementation, entropy\nregularization combined with a rich policy class can attain desirable\nproperties displayed under maximum entropy reinforcement learning framework,\nsuch as robustness and multi-modality.",
  "text": "Implicit Policy for Reinforcement Learning\nYunhao Tang\nColumbia University\nyt2541@columbia.edu\nShipra Agrawal ∗\nColumbia University\nsa3305@columbia.edu\nAbstract\nWe introduce Implicit Policy, a general class of expressive policies that can ﬂexibly\nrepresent complex action distributions in reinforcement learning, with efﬁcient\nalgorithms to compute entropy regularized policy gradients. We empirically show\nthat, despite its simplicity in implementation, entropy regularization combined\nwith a rich policy class can attain desirable properties displayed under maximum\nentropy reinforcement learning framework, such as robustness and multi-modality.\n1\nIntroduction\nReinforcement Learning (RL) combined with deep neural networks have led to a wide range of\nsuccessful applications, including the game of Go, robotics control and video game playing [32, 30,\n24]. During the training of deep RL agent, the injection of noise into the learning procedure can\nusually prevent the agent from premature convergence to bad locally optimal solutions, for example,\nby entropy regularization [30, 23] or by explicitly optimizing a maximum entropy objective [13, 25].\nThough entropy regularization is much simpler to implement in practice, it greedily optimizes the\npolicy entropy at each time step, without accounting for future effects. On the other hand, maximum\nentropy objective considers the entropy of the distribution over entire trajectories, and is more\nconducive to theoretical analysis [2]. Recently, [13, 14] also shows that optimizing the maximum\nentropy objective can lead to desirable properties such as robustness and multi-modal policy.\nCan we preserve the simplicity of entropy regularization while attaining desirable properties under\nmaximum entropy framework? To achieve this, a necessary condition is an expressive representation\nof policy. Though various ﬂexible probabilistic models have been proposed in generative modeling\n[10, 37], such models are under-explored in policy based RL. To address such issues, we propose\nﬂexible policy classes and efﬁcient algorithms to compute entropy regularized policy gradients.\nIn Section 3, we introduce Implicit Policy, a generic policy representation from which we derive\ntwo expressive policy classes, Normalizing Flows Policy (NFP) and more generally, Non-invertible\nBlackbox Policy (NBP). NFP provides a novel architecture that embeds state information into\nNormalizing Flows; NBP assumes little about policy architecture, yet we propose algorithms to\nefﬁciently compute entropy regularized policy gradients when the policy density is not accessible.\nIn Section 4, we show that entropy regularization optimizes a lower bound of maximum entropy\nobjective. In Section 5, we show that when combined with entropy regularization, expressive policies\nachieve competitive performance on benchmarks and leads to robust and multi-modal policies.\n2\nPreliminaries\n2.1\nBackground\nWe consider the standard RL formalism consisting of an agent interacting with the environment. At\ntime step t ≥0, the agent is in state st ∈S, takes action at ∈A, receives instant reward rt ∈R\nand transitions to next state st+1 ∼p(st+1|st, at). Let π : S 7→A be a policy. The objective of\n∗This research was supported by an Amazon Research Award (2017) and AWS cloud credits.\narXiv:1806.06798v2  [cs.LG]  3 Feb 2019\nRL is to search for a policy which maximizes cumulative expected reward J(π) = Eπ\n\u0002 P∞\nt=0 rtγt\u0003\n,\nwhere γ ∈(0, 1] is a discount factor. The action value function of policy π is deﬁned as Qπ(s, a) =\nEπ\n\u0002 P∞\nt=0 rtγt|s0 = s, a0 = a\n\u0003\n. In policy based RL, a policy is explicitly parameterized as πθ with\nparameter θ, and the policy can be updated by policy gradients θ ←θ + α∇θJ(πθ), where α is the\nlearning rate. So far, there are in general two ways to compute policy gradients for either on-policy or\noff-policy updates.\nScore\nfunction\ngradient\n&\nPathwise\ngradient.\nGiven\na\nstochastic\npolicy\nat\n∼\nπθ(·|st), the score function gradient for on-policy update is computed as ∇θJ(πθ)\n=\nEπθ\n\u0002 P∞\nt=0 Qπθ(st, at)∇θ log πθ(at|st)\n\u0003\nas in [31, 30, 23, 35]. For off-policy update, it is nec-\nessary to introduce importance sampling weights to adjust the distribution difference between the\nbehavior policy and current policy. Given a deterministic policy at = πθ(st), the pathwise gradient\nfor on-policy update is computed as ∇θJ(πθ) = Eπθ\n\u0002 P∞\nt=0 ∇aQπθ(st, a)|a=πθ(st)∇θπθ(st)\n\u0003\n. In\npractice, this gradient is often computed off-policy [33, 32], where the exact derivation comes from a\nmodiﬁed off-policy objective [3].\nEntropy Regularization.\nFor on-policy update, it is common to apply entropy regularization\n[38, 26, 23, 31]. Let H[π(·|s)] be the entropy of policy π at state s. The entropy regularized update is\nθ ←θ + α{∇θJ(πθ) + βEπθ\n\u0002\n∇θ\n∞\nX\nt=0\nH[πθ(·|st)]γt)\n\u0003\n},\n(1)\nwhere β > 0 is a regularization constant. By boosting policy entropy, this update can potentially\nprevent the policy from premature convergence to bad locally optimal solutions. In Section 3, we\nwill introduce expressive policies that leverage both on-policy/off-policy updates, and algorithms to\nefﬁciently compute entropy regularized policy gradients.\nMaximum Entropy RL.\nIn maximum entropy RL formulation, the objective is to maximize the\ncumulative reward and the policy entropy JMaxEnt(πθ) = Eπθ\n\u0002 P∞\nt=0 rtγt + β P∞\nt=0 H[π(·|st)]γt\u0003\n,\nwhere β > 0 is a tradeoff constant. Note that ∇θJMaxEnt(πθ) differs from the update in (1) by an\nexchange of expectation and gradient. The intuition of JMaxEnt(πθ) is to achieve high reward while\nbeing as random as possible over trajectories. Since there is no simple low variance gradient estimate\nfor JMaxEnt(πθ), several previous works [31, 13, 25] have proposed to optimize JMaxEnt(πθ) primarily\nusing off-policy value based algorithms.\n2.2\nRelated Work\nA large number of prior works have implemented policy gradient algorithms with entropy regulariza-\ntion [30, 31, 23, 26], which boost exploration by greedily maximizing policy entropy at each time step.\nIn contrast to such greedy procedure, maximum entropy objective considers entropy over the entire\npolicy trajectories [13, 25, 29]. Though entropy regularization is simpler to implement in practice,\n[12, 13] argues in favor of maximum entropy objective by showing that trained policies can be robust\nto noise, which is desirable for real life robotics tasks; and multi-modal, a potentially desired property\nfor exploration and ﬁne-tuning for downstream tasks. However, their training procedure is fairly\ncomplex, which consists of training a soft Q function by ﬁxed point iteration and a neural sampler\nby Stein variational gradient [21]. We argue that properties as robustness and multi-modality are\nattainable through simple entropy regularized policy gradient algorithms combined with expressive\npolicy representations.\nPrior works have studied the property of maximum entropy objective [25, 39], entropy regularization\n[26] and their connections with variants of operators [2]. It is commonly believed that entropy\nregularization greedily maximizes local policy entropy and does not account for how a policy\nupdate impacts future states. In Section 4, we show that entropy regularized policy gradient update\nmaximizes a lower bound of maximum entropy objective, given constraints on the differences between\nconsecutive policy iterates. This partially justiﬁes why simple entropy regularization combined with\nexpressive policy classes can achieve competitive empirical performance in practice.\nThere is a number of prior works that discuss different policy architectures. The most common policy\nfor continuous control is unimodal Gaussian [30, 31, 23]. [14] discusses mixtures of Gaussian, which\n2\ncan represent multi-modal policies but it is necessary to specify the number of modes in advance.\n[13] also represents a policy using implicit model, but the policy is trained to sample from the soft\nQ function instead of being trained directly. Recently, we ﬁnd [11] also uses Normalizing Flows\nto represent policies, but their focus is learning an hierarchy and involves layers of pre-training.\nContrary to early works, we propose to represent ﬂexible policies using implicit models/Normalizing\nFlows and efﬁcient algorithms to train the policy end-to-end.\nImplicit models have been extensively studied in probabilistic inference and generative modeling\n[10, 17, 19, 37]. Implicit models deﬁne distributions by transforming source noise via a forward pass\nof neural networks, which in general sacriﬁce tractable probability density for more expressive repre-\nsentation. Normalizing Flows are a special case of implicit models [27, 5, 6], where transformations\nfrom source noise to output are invertible and allow for maximum likelihood inference. Borrowing\ninspirations from prior works, we introduce implicit models into policy representation and empirically\nshow that such rich policy class entails multi-modal behavior during training. In [37], GAN [10] is\nused as an optimal density estimator for likelihood free inference. In our work, we apply similar idea\nto compute entropy regularization when policy density is not available.\n3\nImplicit Policy for Reinforcement Learning\nWe assume the action space A to be a compact subset of Rm. Any sufﬁciently smooth stochastic\npolicy can be represented as a blackbox fθ(·) with parameter θ that incorporates state information s\nand independent source noise ϵ sampled from a simple distribution ρ0(·). In state s, the action a is\nsampled by a forward pass in the blackbox.\na = fθ(s, ϵ), ϵ ∼ρ0(·).\n(2)\nFor example, Gaussian policy is reduced to a = σθ(s) · ϵ + µθ(s) where ρ0 is standard Gaussian\n[30]. In general, the distribution of at is implicitly deﬁned: for any set A of A, P(a ∈A|s) =\nR\nϵ:fθ(s,ϵ)=a ρ0(ϵ)dϵ. Let πθ(·|s) be the density of this distribution2. We call such policy Implicit Policy\nas similar ideas have been previous explored in implicit generative modeling literature [10, 19, 37]. In\nthe following, we derive two expressive stochastic policy classes following this blackbox formulation,\nand propose algorithms to efﬁciently compute entropy regularized policy gradients.\n3.1\nNormalizing Flows Policy (NFP)\nWe ﬁrst construct a stochastic policy with Normalizing Flows. Normalizing Flows [27, 6] have been\napplied in variational inference and probabilistic modeling to represent complex distributions. In\ngeneral, consider transforming a source noise ϵ ∼ρ0(·) by a series of invertible nonlinear function\ngθi(·), 1 ≤i ≤K each with parameter θi, to output a target sample x,\nx = gθK ◦gθK−1 ◦... ◦gθ2 ◦gθ1(ϵ).\n(3)\nLet Σi be the Jacobian matrix of gθ(·), then the density of x is computed by chain rule,\nlog p(x) = log p(ϵ) +\nK\nX\ni=1\nlog det(Σi).\n(4)\nFor a general invertible transformation gθi(·), computing det(Σi) is expensive. We follow the\narchitecture of [5] to ensure that det(Σi) is computed in linear time. To combine state information,\nwe embed state s by another neural network Lθs(·) with parameter θs and output a state vector Lθs(s)\nwith the same dimension as ϵ. We can then insert the state vector between any two layers of (3) to\nmake the distribution conditional on state s. In our implementation, we insert the state vector after\nthe ﬁrst transformation (we detail our architecture design in Appendix C).\na = gθK ◦gθK−1 ◦... ◦gθ2 ◦(Lθs(s) + gθ1(ϵ)).\n(5)\nThough the additive form of Lθs(s) and gθ1(ϵ) may in theory limit the capacity of the model, in\npractice we ﬁnd the resulting policy still very expressive. For simplicity, we denote the above\n2In future notations, when the context is clear, we use πθ(·|s) to denote both the density of the policy as well\nas the policy itself: for example, a ∼πθ(·|s) means sampling a from the policy; log πθ(a|s) means the log\ndensity of policy at a in state s.\n3\ntransformation (5) as a = fθ(s, ϵ) with parameter θ = {θs, θi, 1 ≤i ≤K}. It is obvious that\nϵ ↔a = fθ(s, ϵ) is still invertible between a and ϵ, which is critical for computing log πθ(a|s)\naccording to (4). Such representations build complex policy distributions with explicit probability\ndensity πθ(·|s), and hence entail training using score function gradient estimators.\nSince there is no analytic form for entropy, we use samples to estimate entropy by re-parameterization,\nH\n\u0002\nπθ(·|s)\n\u0003\n= Ea∼πθ(·|s)\n\u0002\n−log πθ(a|s)\n\u0003\n= Eϵ∼ρ0(·)\n\u0002\n−log πθ(fθ(s, ϵ)|s)\n\u0003\n. The gradient of entropy\ncan be easily computed by a pathwise gradient and easily implemented using back-propagation\n∇θH\n\u0002\nπθ(·|s)\n\u0003\n= Eϵ∼ρ0(·)\n\u0002\n−∇θ log πθ(fθ(s, ϵ)|s)\n\u0003\n.\nOn-policy algorithm for NFP.\nAny on-policy policy optimization algorithms can be easily com-\nbined with NFP. Since NFP has explicit access to policy density, it allows for training using score\nfunction gradient estimators with efﬁcient entropy regularization.\n3.2\nNon-invertible Blackbox Policy (NBP)\nThe forward pass in (2) transforms the simple noise distribution ϵ ∼ρ0(·) to complex action\ndistribution at ∼πθ(·|st) through the blackbox fθ(·). However, the mapping ϵ 7→at is in general\nnon-invertible and we do not have access to the density πθ(·|st). We derive a pathwise gradient for\nsuch cases and leave all the proof in Appendix A.\nTheorem 3.1 (Stochastic Pathwise Gradient). Given an implicit stochastic policy at = fθ(st, ϵ), ϵ ∼\nρ0(·). Let πθ be the implicitly deﬁned policy. Then the pathwise policy gradient for the stochastic\npolicy is\n∇θJ(πθ) = Eπθ\n\u0002\nEϵ∼ρ0(·)[∇θfθ(s, ϵ)∇aQπθ(s, a)|a=fθ(s,ϵ)]\n\u0003\n.\n(6)\nTo compute the gradient of policy entropy for such general implicit policy, we propose to train an\nadditional classiﬁer cψ : S × A 7→R with parameter ψ along with policy πθ. The classiﬁer cψ is\ntrained to minimize the following objective given a policy πθ\nmin\nψ\nEa∼πθ(·|s)\n\u0002\n−log σ(cψ(a, s))\n\u0003\n+ Ea∼U(A)\n\u0002\n−log(1 −σ(cψ(a, s)))\n\u0003\n,\n(7)\nwhere U(A) is a uniform distribution over A and σ(·) is the sigmoid function. We have lemma A.1 in\nAppendix A.2 to guarantee that the optimal solution ψ∗of (7) provides an estimate of policy density,\ncψ∗(s, a) = log πθ(a|s)\n|A|−1 . As a result, we could evaluate the entropy by simple re-parametrization\nH\n\u0002\nπθ(·|s)\n\u0003\n= Eϵ∼ρ0(·)\n\u0002\n−log π(fθ(s, ϵ)|s)\n\u0003\n≈Eϵ∼ρ0(·)\n\u0002\n−cψ(fθ(s, ϵ), s)\n\u0003\n. Further, we can compute\ngradients of the policy entropy through the density estimate as shown by the following theorem.\nTheorem 3.2 (Unbiased Entropy Gradient). Let ψ∗be the optimal solution from (7), where the policy\nπθ(·|s) is given by implicit policy a = fθ(s, ϵ), ϵ ∼ρ0(·). The gradient of entropy ∇θH\n\u0002\nπθ(·|s)\n\u0003\ncan be computed as\n∇θH\n\u0002\nπθ(·|s)\n\u0003\n= −Eϵ∼ρ0(·)\n\u0002\n∇θcψ∗(f(θ, ϵ), s)\n\u0003\n(8)\nIt is worth noting that to compute ∇θH\n\u0002\nπθ(·|s)\n\u0003\n, simply plugging in cψ∗(a, s) to replace log πθ(a|s)\nin the entropy deﬁnition does not work in general, since the optimal solution ψ∗of (7) implicitly\ndepends on θ. However, fortunately in this case the additional term vanishes. The above theorem\nguarantees that we could apply entropy regularization even when the policy density is not accessible.\nOff-policy algorithm for NBP.\nWe develop an off-policy algorithm for NBP. The agent contains\nan implicit fθ(s, ϵ) with parameter θ, a critic Qφ(s, a) with parameter φ and a classiﬁer cψ(s, a) with\nparameter ψ. At each time step t, we sample action at = fθ(st, ϵ), ϵ ∼ρ0(·) and save experience\ntuple {st, at, rt, st+1} to a replay buffer B. During training, we sample a mini-batch of tuples\nfrom B, update critic Qφ(s, a) using TD learning, update policy fθ(s, ϵ) using pathwise gradient\n(6) and update classiﬁer cψ(s, a) by gradient descent on (7). We also maintain target networks\nfθ−(s, ϵ), Qφ−(s, a) with parameter θ−, φ−to stabilize learning [24, 32]. The pseudocode is listed\nin Appendix D.\n4\n4\nEntropy Regularization and Maximum Entropy RL\nThough policy gradient algorithms with entropy regularization are easy to implement in practice, they\nare harder to analyze due to the lack of a global objective. Now we show that entropy regularization\nmaximizes a lower bound of maximum entropy objective when consecutive policy iterates are close.\nAt each iteration of entropy regularized policy gradient algorithm, the policy parameter is updated as\nin (1). Following similar ideas in [15, 30], we now interpret such update as maximizing a linearized\nsurrogate objective in the neighborhood of the previous policy iterate πθold. The surrogate objective is\nJsurr(πθ) = J(πθ) + βEπθold\n\u0002 ∞\nX\nt=0\nH[πθ(·|st)]γt\u0003\n.\n(9)\nThe ﬁrst-order Taylor expansion of (9) centering at θold gives a linearized surrogate objective\nJsurr(πθ) ≈Jsurr(πθold) + ∇θJsurr(πθ)|θ=θold(θ −θold). Let δθ = θ −θold, the entropy regular-\nized update (1) is equivalent to solving the following optimization problem then update according to\nθ ←θold + δθ,\nmin\nδθ\n\u0002\n∇θJsurr(πθ)|θ=θθold\n\u0003T δθ\ns.t. ||δθ||2 ≤C(α, θold),\nwhere C(α, θold) is a positive constant depending on both the learning rate α and the previous iterate\nθold, and can be recovered from (1). The next theorem shows that by constraining the KL divergence\nof consecutive policy iterates, the surrogate objective (9) forms a non-trivial lower bound of maximum\nentropy objective,\nTheorem 4.1 (Lower Bound). If KL[πθ||πθold] ≤α, then\nJMaxEnt(π) ≥Jsurr(π) −βγ√αϵ\n(1 −γ)2 ,\nwhere ϵ = max\ns\n|H[π(·|s)]|.\n(10)\nBy optimizing Jsurr(πθ) at each iteration, entropy regularized policy gradient algorithms maximize a\nlower bound of JMaxEnt(πθ). This implies that though entropy regularization is a greedier procedure\nthan optimizing maximum entropy objective, it accounts for certain effects that the maximum entropy\nobjective is designed to capture. Nevertheless, the optimal solutions of both optimization procedures\nare different. Previous works [26, 13] have shown that the optimal solutions of both procedures\nare energy based policies, with energy functions being ﬁxed points of Boltzmann operator and\nMellowmax operator respectively [2]. In Appendix B, we show that Boltzmann operator interpolates\nbetween Bellman operator and Mellowmax operator, which asserts that entropy regularization is\ngreedier than optimizing JMaxEnt(πθ), yet it still maintains uncertainties in the policy updates.\nThough maximum entropy objective accounts for long term effects of policy entropy updates and is\nmore conducive to analysis [2], it is hard to implement a simple yet scalable procedure to optimize\nthe objective [13, 14, 2]. Entropy regularization, on the other hand, is simple to implement in\nboth on-policy and off-policy setting. In experiments, we will show that entropy regularized policy\ngradients combined with expressive policies achieve competitive performance in multiple aspects.\n5\nExperiments\nOur experiments aim to answer the following questions: (1) Will expressive policy be hard to train,\ndoes implicit policy provide competitive performance on benchmark tasks? (2) Are implicit policies\nrobust to noises on locomotion tasks? (3) Does implicit policy + entropy regularization entail\nmulti-modal policies as displayed under maximum entropy framework [13]?\nTo answer (1), we evaluate both NFP and NBP agent on benchmark continuous control tasks in\nMuJoCo [36] and compare with baselines. To answer (2), we compare NFP with unimodal Gaussian\npolicy on locomotion tasks with additive observational noises. To answer (3), we illustrate the\nmulti-modal capacity of both policy representations on specially designed tasks illustrated below,\nand compare with baselines. In all experiments, for NFP, we implement with standard PPO for\non-policy update to approximately enforce the KL constraint (10) as in [31]; for NBP, we implement\nthe off-policy algorithm developed in Section 3. In Appendix C and F, we detail hyper-parameter\nsettings in the experiments and provide a small ablation study.\n5\n5.1\nLocomotion Tasks\nBenchmark tasks.\nOne potential disadvantage of expressive policies compared to simple policies\n(like unimodal Gaussian) is that they pose a more serious statistical challenge due to a larger number\nof parameters. To see if implicit policy suffers from such problems, we evaluate NFP and NBP on\nMuJoCo benchmark tasks. For each task, we train for a prescribed number of time steps, then report\nthe results averaged over 5 random seeds. We compare the results with baseline algorithms, such\nas DDPG [32], SQL [13], TRPO [30] and PPO [31], where baseline TPRO and PPO use unimodal\nGaussian policies. As can be seen from Table 1, both NFP and NBP achieve competitive performances\non benchmark tasks: they outperform DDPG, SQL and TRPO on most tasks. However, baseline\nPPO tends to come on top on most tasks. Interestingly on HalfCheetah, baseline PPO gets stuck on a\nlocally optimal gait, which NFP improves upon by a large margin.\nTasks\nTimesteps\nDDPG\nSQL\nTRPO\nPPO\nNFP\nNBP\nHopper\n2.00 · 106\n≈1100\n≈1500\n≈1250\n≈2130\n≈1640\n≈1880\nHalfCheetah\n1.00 · 107\n≈6500\n≈8000\n≈1800\n≈1590\n≈4000\n≈6560\nWalker2d\n5.00 · 106\n≈1600\n≈2100\n≈800\n≈3800\n≈3000\n≈2450\nAnt\n1.00 · 107\n≈200\n≈2000\n≈0\n≈4440\n≈2500\n≈2070\nTable 1: A comparison of implicit policy optimization with baseline algorithms on MuJoCo benchmark tasks.\nFor each task, we show the average rewards achieved after training the agent for a ﬁxed number of time steps.\nThe results for NFP and NBP are averaged over 5 random seeds. The results for DDPG, SQL and TRPO are\napproximated based on the ﬁgures in [14], PPO is from OpenAI baseline implementation [4]. We highlight the\ntop two algorithms for each task in bold font. Both TRPO and PPO use unimodal Gaussian policies.\nRobustness to Noisy Observations.\nWe add independent Gaussian noise N(0, 0.12) to each com-\nponent of the observations to make the original tasks partially observable. Since PPO with unimodal\nGaussian achieves leading performance on noise-free locomotion tasks across on-policy baselines\n(A2C [23], TRPO [30]) as shown in [31] and Appendix E.1, we compare NFP only with PPO with\nunimodal Gaussian on such noisy locomotion tasks. In Figure 1, we show the learning curves of\nboth agents, where on many tasks NFP learns signiﬁcantly faster than unimodal Gaussian. Why\ncomplex policies may add to robustness? We propose that since these control tasks are known to\nbe solved by multiple separate modes of policy [22], observational noises potentially blur these\nmodes and make it harder for a unimodal Gaussian policy to learn any single mode (e.g. unimodal\nGaussian puts probability mass between two neighboring modes [18]). On the contrary, NFP can still\nnavigate a more complex reward landscape thanks to a potentially multi-modal policy distribution\nand learn effectively. We leave a more detailed study of robustness, multi-modality and complex\nreward landscape as interesting future work.\n(a) Hopper\n(b) Walker\n(c) Reacher\n(d) Swimmer\n(e) HalfCheetah\n(f) Ant\n(g) MountainCar\n(h) Pendulum\nFigure 1: Noisy Observations: learning curves on noisy locomotion tasks. For each task, the observation is\nadded a Gaussian noise N(0, 0.12) component-wise. Each curve is averaged over 4 random seeds. Red is NFP\nand blue is unimodal Gaussian, both implemented with PPO. NFP beats Gaussian on most tasks.\n6\n5.2\nMulti-modal policy\nGaussian Bandits.\nThough factorized unimodal policies sufﬁce for most benchmark tasks, below\nwe motivate the importance of a ﬂexible policy by a simple example: Gaussian bandits. Consider\na two dimensional bandit A = [−1, 1]2. The reward of action a is −aT Σ−a for a positive deﬁnite\nmatrix Σ. The optimal policy for maximum entropy objective is π∗(a) ∝exp(−aT Σ−a\nβ\n), i.e. a\nGaussian policy with covariance matrix Σ. We compare NFP with PPO with factorized Gaussian. As\nillustrated in Figure 2(a), NFP can approximate the optimal Gaussian policy pretty closely while the\nfactorized Gaussian cannot capture the high correlation between the two action components.\nNavigating 2D Multi-goal.\nWe motivate the strength of implicit policy to represent multi-modal\npolicy by Multi-goal environment [13]. The agent has 2D coordinates as states S ⊂R2 and 2D\nforces as actions A ⊂R2. A ball is randomly initialized near the origin and the goal is to push the\nball to reach one of the four goal positions plotted as red dots in Figure 2(b). While a unimodal policy\ncan only deterministically commit the agent to one of the four goals, a multi-modal policy obtained\nby NBP can stochastically commit the agent to multiple goals. On the right of Figure 2(b) we also\nshow sampled actions and contours of Q value functions at various states: NBP learns a very ﬂexible\npolicy with different number of modes in different states.\nLearning a Bimodal Reacher.\nFor a more realistic example, consider learning a bimodal policy\nfor reaching one of two targets (Figure 3(a)). The agent has the physical coordinates of the reaching\narms as states S ⊂R9 and applies torques to the joints as actions A ⊂R2. The objective is to move\nthe reacher head to be close to one of the targets. As illustrated by trajectories in Figure 2(c), while a\nunimodal Gaussian policy can only deterministically reach one target (red curves), a NFP agent can\ncapture both modes by stochastically reaching one of the two targets (blue curves).\n(a) Gaussian Bandit\n(b) 2D Multi-goal\n(c) Bimodal Reacher\nFigure 2: (a): Illustration of Gaussian bandits. The x and y axes are actions. Green dots are actions from the\noptimal policy, a Gaussian distribution with covariance structure illustrated by the contours. Red dots and blue\ndots are actions sampled from a learned factorized Gaussian and NFP. NFP captures the covariance of the optimal\npolicy while factorized Gaussian cannot. (b): Illustration of 2D multi-goal environment. Left: trajectories\ngenerated by trained NBP agent (solid blue curves). The x and y axes are coordinates of the agent (state). The\nagent is initialized randomly near the origin. The goals are red dots, and instant rewards are proportional to the\nagent’s minimum distance to one of the four goals. Right: predicted Q value contours by the critic (light blue:\nlow value, light green: high value and actions sampled from the policy (blue dots) at three selected states. The\nNFP policy has different number of modes at different states. (c): Trajectories of the reacher head by NFP (blue\ncurves) and unimodal Gaussian policies (red curves) for the bimodal reacher. Yellow dots are locations of the\ntwo targets, and the green dot is the starting location of the reacher.\nFine-tuning for downstream tasks.\nA recent paradigm for RL is to pre-train an agent to perform\na conceptually high-level task, which may accelerate ﬁne-tuning the agent to perform more speciﬁc\ntasks [13]. We consider pre-training a quadrupedal robot (Figure 3(b)) to run fast, then ﬁne-tune the\nrobot to run fast in a particular direction [13] as illustrated in Figure 3(c), where we set walls to limit\nthe directions in which to run. Wide and Narrow Hallways tasks differ by the distance of the opposing\nwalls. If an algorithm does not inject enough diversity during pre-training, it will commit the agent to\nprematurely run in a particular direction, which is bad for ﬁne-tuning. We compare the pre-training\ncapacity of DDPG [20], SQL [13] and NBP. As shown in Figure 3(d), after pre-training, NBP agent\nmanages to run in multiple directions, while DDPG agent runs in a single direction due to a determin-\nistic policy (Appendix E.2). In Table 2, we compare the cumulative rewards of agents after ﬁne-tuning\non downstream tasks with different pre-training as initializations. In both tasks, we ﬁnd NBP to out-\nperform DDPG, SQL and random initialization (no pre-training) by statistically signiﬁcant margins,\n7\npotentially because NBP agent learns a high-level running gait that is more conducive to ﬁne-tuning.\nInterestingly, in Narrow Hallway, randomly initialized agent performs better than DDPG pre-training,\nwhich is probably because running fast in Narrow Hallway requires running in a very narrow direction,\nand DDPG pre-trained agent needs to ﬁrst unlearn the overtly specialized running gait acquired from\npre-training. In Wide Hallway, randomly initialized agent easily gets stuck in a locally optimal gait\n(running between two opposing walls) while pre-training in general helps avoid such problem.\n(a) Reacher\n(b) Ant\n(c) Wide Hallway\n(d) Ant Running\nFigure 3: Illustration of locomotion tasks: (a) Bimodal Reacher. Train a reacher to reach one of two targets.\nGreen boxes are targets. (b) Ant-Running. Train a quadrupedal robot to run fast. The instant reward is the robot’s\ncenter of mass velocity; (c) Ant-Hallway. Train a quadrupedal robot to run fast under the physical constraints of\nwalls, the instant reward is the same as in (b). Narrow and Wide Hallway tasks differ by the distance between the\nopposing walls; (d) Trajectories by NBP agent in Ant-Running. The agent learns to run in multiple directions.\nTasks\nRandom init\nDDPG init\nSQL init\nNBP init\nWide Hallway\n522 ± 111\n3677 ± 472\n3624 ± 312\n4306 ± 571\nNarrow Hallway\n3023 ± 280\n2866 ± 248\n3026 ± 352\n3752 ± 408\nTable 2: A comparison of downstream ﬁne-tuning under different initializations. For each task, we show the\ncumulative rewards after pre-training for 2 · 106 steps and ﬁne-tuning for 106 steps. The rewards are shown in\nthe form (mean±std), all results are averaged over 5 seeds. Random init means the agent is trained from scratch.\nCombining multiple modes by Imitation Learning.\nWe propose another paradigm that can be of\npractical interest. In general, learning a multi-modal policy from scratch is hard for complex tasks\nsince it requires good exploration and an algorithm to learn multi-modal distributions [13], which is\nitself a hard inference problem [10]. A big advantage of policy based algorithm over value based\nalgorithm [13] is that the policy can be easily combined with imitation learning. We could decompose\na complex task into several simpler tasks, each representing a simple mode of behavior easily learned\nby a RL agent, then combine them into a single agent using imitation learning or inverse RL [1, 8, 28].\nWe illustrate with a stochastic Swimmer example (see Appendix E.3). Consider training a Swimmer\nto move fast either forward or backward. The aggregate behavior has two modes and it is easy to solve\neach single mode. We train two separate Swimmers to move forward/backward and generate expert\ntrajectories using the trained agents. We then train a NBP / NFP agent using GAN [10] / maximum\nlikelihood estimation to combine both modes. Training with the same algorithms, a unimodal policy\neither commits to only one mode or learns a policy that puts large probability mass between the two\nmodes [18, 10], which greatly deviates from the expert policy. On the contrary, expressive policies\ncan more ﬂexibly incorporate multiple modes into a single agent.\n6\nConclusion\nWe have proposed Implicit Policy, a rich class of policy that can represent complex action distributions.\nWe have derived efﬁcient algorithms to compute entropy regularized policy gradients for generic\nimplicit policies. Importantly, we have also showed that entropy regularization maximizes a lower\nbound of maximum entropy objective, which implies that in practice entropy regularization + rich\npolicy class can lead to desired properties of maximum entropy RL. We have empirically showed that\nimplicit policy achieves competitive performance on benchmark tasks, is more robust to observational\nnoise, and can ﬂexibly represent multi-modal distributions.\n8\nAcknowledgements.\nThis research was supported by an Amazon Research Award (2017) and AWS\ncloud credits. The authors would like to thank Jalaj Bhandari for helpful discussions, and Sergey\nLevine for helpful comments on early stage experiments of the paper.\nReferences\n[1] Abbeel, P. and Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In\nProceedings of the twenty-ﬁrst international conference on Machine learning, page 1. ACM.\n[2] Asadi, K. and Littman, M. L. (2017). An alternative softmax operator for reinforcement learning.\nIn International Conference on Machine Learning, pages 243–252.\n[3] Degris, T., White, M., and Sutton, R. S. (2012).\nOff-policy actor-critic.\narXiv preprint\narXiv:1205.4839.\n[4] Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor,\nS., and Wu, Y. (2017). Openai baselines. https://github.com/openai/baselines.\n[5] Dinh, L., Krueger, D., and Bengio, Y. (2014). Nice: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516.\n[6] Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2016). Density estimation using real nvp. arXiv\npreprint arXiv:1605.08803.\n[7] Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. (2016). Benchmarking deep\nreinforcement learning for continuous control. In International Conference on Machine Learning,\npages 1329–1338.\n[8] Finn, C., Christiano, P., Abbeel, P., and Levine, S. (2016). A connection between generative\nadversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint\narXiv:1611.03852.\n[9] Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos,\nR., Hassabis, D., Pietquin, O., et al. (2017). Noisy networks for exploration. arXiv preprint\narXiv:1706.10295.\n[10] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.,\nand Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing\nsystems, pages 2672–2680.\n[11] Haarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S. (2018a). Latent space policies for\nhierarchical reinforcement learning. arXiv preprint arXiv:1804.02808.\n[12] Haarnoja, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and Levine, S. (2018b). Composable\ndeep reinforcement learning for robotic manipulation. arXiv preprint arXiv:1803.06773.\n[13] Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep\nenergy-based policies. arXiv preprint arXiv:1702.08165.\n[14] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018c).\nSoft actor-critic:\nOff-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\narXiv:1801.01290.\n[15] Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning.\nIn ICML, volume 2, pages 267–274.\n[16] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\n[17] Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114.\n[18] Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and\nreview. arXiv preprint arXiv:1805.00909.\n[19] Li, Y. and Turner, R. E. (2017). Gradient estimators for implicit models. arXiv preprint\narXiv:1705.07107.\n[20] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.\n(2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.\n9\n[21] Liu, Q. and Wang, D. (2016). Stein variational gradient descent: A general purpose bayesian\ninference algorithm. In Advances In Neural Information Processing Systems, pages 2378–2386.\n[22] Mania, H., Guy, A., and Recht, B. (2018). Simple random search provides a competitive\napproach to reinforcement learning. arXiv preprint arXiv:1803.07055.\n[23] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and\nKavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In Inter-\nnational Conference on Machine Learning, pages 1928–1937.\n[24] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,\nM. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n[25] Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D. (2017). Bridging the gap between value\nand policy based reinforcement learning. In Advances in Neural Information Processing Systems,\npages 2772–2782.\n[26] O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. (2016). Pgq: Combining policy\ngradient and q-learning. arXiv preprint arXiv:1611.01626.\n[27] Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing ﬂows. arXiv\npreprint arXiv:1505.05770.\n[28] Ross, S., Gordon, G., and Bagnell, D. (2011). A reduction of imitation learning and structured\nprediction to no-regret online learning. In Proceedings of the fourteenth international conference\non artiﬁcial intelligence and statistics, pages 627–635.\n[29] Schulman, J., Chen, X., and Abbeel, P. (2017a). Equivalence between policy gradients and soft\nq-learning. arXiv preprint arXiv:1704.06440.\n[30] Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy\noptimization. In International Conference on Machine Learning, pages 1889–1897.\n[31] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017b). Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347.\n[32] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser,\nJ., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with\ndeep neural networks and tree search. nature, 529(7587):484–489.\n[33] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Determinis-\ntic policy gradient algorithms. In ICML.\n[34] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout:\nA simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958.\n[35] Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods\nfor reinforcement learning with function approximation. In Advances in neural information\nprocessing systems, pages 1057–1063.\n[36] Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control.\nIn Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages\n5026–5033. IEEE.\n[37] Tran, D., Ranganath, R., and Blei, D. M. (2017). Hierarchical implicit models and likelihood-\nfree variational inference. arXiv preprint arXiv:1702.08896.\n[38] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist\nreinforcement learning. In Reinforcement Learning, pages 5–32. Springer.\n[39] Ziebart, B. D. (2010). Modeling purposeful adaptive behavior with the principle of maximum\ncausal entropy. Carnegie Mellon University.\n10\nA\nProof of Theorems\nA.1\nStochastic Pathwise Gradient\nTheorem 3.1 (Stochastic Pathwise Gradient). Given an implicit stochastic policy at = fθ(st, ϵ), ϵ ∼\nρ0(·). Let πθ be the implicitly deﬁned policy. Then the pathwise policy gradient for the stochastic\npolicy is\n∇θJ(πθ) = Eπθ\n\u0002\nEϵ∼ρ0(·)[∇θfθ(s, ϵ)∇aQπθ(s, a)|a=fθ(s,ϵ)]\n\u0003\n.\n(6)\nProof. We follow closely the derivation of deterministic policy gradient [33]. We assume that all\nconditions are satisﬁed to exchange expectations and gradients when necessary. Let π = πθ denote\nthe implicit policy at = fθ(st, ϵ), ϵ ∼ρ0(·). Let V π, Qπ be the value function and action value\nfunction under such stochastic policy. We introduce p(s →s′, k, π) as the probability of transitioning\nfrom s to s′ in k steps under policy π. Overloading the notation a bit, p(s →s′, 1, a) is the probability\nof s →s′ in one step by taking action a (i.e., p(s →s′, 1, a) = p(s′|s, a)). We have\n∇θV π(s)\n= ∇θEa∼π(·|s)\n\u0002\nQπ(s, a)\n\u0003\n= ∇θEϵ∼ρ0(·)\n\u0002\nQπ(s, fθ(s, ϵ))\n\u0003\n= ∇θEϵ∼ρ0(·)\n\u0002\nr(s, fθ(s, ϵ)) +\nZ\nS\nγp(s′|s, fθ(s, ϵ))V π(s′)ds′\u0003\n= Eϵ∼ρ0(·)\n\u0002\n∇θr(s, fθ(s, ϵ)) + ∇θ\nZ\nS\nγp(s′|s, fθ(s, ϵ))V π(s′)ds′\u0003\n= Eϵ∼ρ0(·)\n\u0002\n∇θr(s, fθ(s, ϵ)) +\nZ\nS\nγV π(s′)∇θp(s′|s, fθ(s, ϵ))ds′ +\nZ\nS\nγp(s′|s, fθ(s, ϵ))∇θV π(s′)ds′\u0003\n= Eϵ∼ρ0(·)\n\u0002\n∇θfθ(s, ϵ)∇a[r(s, a) + γ\nZ\nS\nγp(s′|s, a)V π(s′)ds′)]|a=fθ(s,ϵ)\n+\nZ\nS\nγp(s′|s, fθ(s, ϵ))∇θV π(s′)ds′\u0003\n= Eϵ∼ρ0(·)\n\u0002\n∇θfθ(s, ϵ)∇aQπ(s, a)|a=fθ(s,ϵ)\n\u0003\n+ Eϵ∼ρ0(·)\n\u0002 Z\nS\nγp(s →s′, 1, fθ(s, ϵ))∇θV π(s′)ds′\u0003\n.\nIn the above derivation, we have used the Fubini theorem to interchange integral (expectation) and\ngradients. We can iterate the above derivation and have the following\n∇θV π(s)\n= ∇θEa∼π(·|s)\n\u0002\nQπ(s, a)\n\u0003\n= Eϵ∼ρ0(·)\n\u0002\n∇θfθ(s, ϵ)∇aQπ(s, a)|a=fθ(s,ϵ)\n\u0003\n+ Eϵ∼ρ0(·)\n\u0002\nγp(s →s′, 1, fθ(s, ϵ))∇θV π(s′)ds′\u0003\n= Eϵ∼ρ0(·)\n\u0002\n∇θfθ(s, ϵ)∇aQπ(s, a)|a=fθ(s,ϵ)\n\u0003\n+\nEϵ∼ρ0(·)\n\u0002 Z\nS\nγp(s →s′, 1, fθ(s, ϵ))Eϵ′∼ρ0(·)[∇θfθ(s′, ϵ′)∇aQπ(s′, a′)|a′=fθ(s′,ϵ′)]ds′\u0003\n+\nEϵ∼ρ0(·)\n\u0002 Z\nS\nγp(s →s′, 1, fθ(s, ϵ′))Eϵ′∼ρ0(·)\n\u0002 Z\nS\nγp(s′ →s′′, 1, fθ(s′, ϵ′))∇θV π(s′′)ds′′\u0003\nds′\u0003\n= ...\n=\nZ\nS\n∞\nX\nt=0\nγtp(s →s′, t, π)Eϵ′∼ρ0(·)\n\u0002\n∇θfθ(s′, ϵ′)∇aQπ(s′, a′)|a′=fθ(s,ϵ′)\n\u0003\nds′.\n11\nWith the above, we derive the pathwise policy gradient as follows\n∇θJ(πθ) = ∇θ\nZ\nS\np1(s)V π(s)ds\n=\nZ\nS\np1(s)∇θV π(s)ds\n=\nZ\nS\nZ\nS\n∞\nX\nt=0\nγtp1(s)p(s →s′, t, π)dsEϵ′∼ρ0(·)\n\u0002\n∇θfθ(s′, ϵ′)∇aQπ(s′, a′)|a′=fθ(s,ϵ′)\n\u0003\nds′\n=\nZ\nS\nρπ(s′)Eϵ′∼ρ0(·)\n\u0002\n∇θfθ(s′, ϵ′)∇aQπ(s′, a′)|a′=fθ(s,ϵ′)\n\u0003\nds′,\nwhere ρπ(s′) =\nR\nS\nP∞\nt=0 γtp1(s)p(s →s′, t, π)ds is the discounted state visitation probability\nunder policy π. Writing the whole integral as an expectation over states, the policy gradient is\n∇θJ(πθ) = Es∼ρπ(s)\n\u0002\nEϵ∼ρ0(·)[∇θfθ(s, ϵ)∇aQπ(s, a)|a=fθ(s,ϵ)]\n\u0003\n.\nwhich is equivalent to in (6) in theorem 3.1.\nWe can recover the result for deterministic policy gradient by using a degenerate functional form\nfθ(s, ϵ) = fθ(s), i.e. with a deterministic function to compute actions.\nA.2\nUnbiased Entropy Gradient\nLemma A.1 (Optimal Classiﬁer as Density Estimator). Assume cψ is expressive enough to represent\nany classiﬁer (for example cψ is a deep neural net). Assume A to be bounded and let U(A) be\nuniform distribution over A. Let ψ∗be the optimizer to the optimization problem in (7). Then\ncψ∗(s, a) = log πθ(a|s)\n|A|−1 and |A| is the volume of A.\nProof. Observe that (7) is a binary classiﬁcation problem with data from a ∼πθ(·|s) against\na ∼U(A). The optimal classiﬁer of the problem produces the density ratio of these two distributions.\nSee for example [10] for a detailed proof.\nTheorem 3.2 (Unbiased Entropy Gradient). Let ψ∗be the optimal solution from (7), where the policy\nπθ(·|s) is given by implicit policy a = fθ(s, ϵ), ϵ ∼ρ0(·). The gradient of entropy ∇θH\n\u0002\nπθ(·|s)\n\u0003\ncan be computed as\n∇θH\n\u0002\nπθ(·|s)\n\u0003\n= −Eϵ∼ρ0(·)\n\u0002\n∇θcψ∗(f(θ, ϵ), s)\n\u0003\n(8)\nProof. Let πθ(·|s) be the density of implicit policy a = fθ(ϵ, s), ϵ ∼ρ0(·). The entropy is computed\nas follows\nH\n\u0002\nπθ(·|s)\n\u0003\n= −Ea∼πθ(·|s)\n\u0002\nlog πθ(a|s)\n\u0003\n.\nComputing its gradient\n∇θH\n\u0002\nπθ(·|s)\n\u0003\n= −∇θEϵ∼ρ0(·)\n\u0002\nlog πθ(fθ(s, ϵ)|s)\n\u0003\n= −Ea∼πθ(·|s)\n\u0002\n∇θ log πθ(a|s)\n\u0003\n−Eϵ∼ρ0(·)\n\u0002\n∇a log πθ(a|s)|a=fθ(s,ϵ)∇θfθ(s, ϵ)\n\u0003\n= −Eϵ∼ρ0(·)\n\u0002\n∇a log πθ(a|s)|a=fθ(s,ϵ)∇θfθ(s, ϵ)\n\u0003\n= −Eϵ∼ρ0(·)\n\u0002\n∇acψ∗(f(θ, ϵ), s)∇θfθ(s, ϵ)\n\u0003\n(11)\nIn the second line we highlight the fact that the expectation depends on parameter θ both implicitly\nthrough the density πθ and through the sample fθ(s, ϵ). After decomposing the gradient using chain\nrule, we ﬁnd that the ﬁrst term vanishes, leaving the result shown in the theorem.\n12\nA.3\nLower Bound\nWe recall that given a policy π, the standard RL objective is J(π) = Eπ\n\u0002 P∞\nt=0 γtrt\n\u0003\n. In maximum\nentropy formulation, the maximum entropy objective is\nJMaxEnt(π) = Eπ\n\u0002 ∞\nX\nt=0\nγt(rt + βH[π(·|st)])\n\u0003\n,\n(12)\nwhere β > 0 is a regularization constant and H(π(·|st)) is the entropy of policy π at st. We construct\na surrogate objective based on another policy ˜π as follows\nJsurr(π, ˜π) = Eπ\n\u0002 ∞\nX\nt=0\nγtrt\n\u0003\n+ βE˜π\n\u0002 ∞\nX\nt=0\nγtH[π(·|st)]\n\u0003\n.\n(13)\nThe following proof highly mimics the proof in [30]. We have the following deﬁnition for coupling\ntwo policies\nDeﬁnition A.1 (α−coupled). Two policies π, ˜π are α−coupled if P(π(·|s) ̸= ˜π(·|s)) ≤α for any\ns ∈S.\nLemma A.2. Given π, ˜π are α−coupled, then\n|Est∼π\n\u0002\nH[π(·|st)]\n\u0003\n−Est∼˜π\n\u0002\nH[˜π(·|st)]\n\u0003\n| ≤2(1 −(1 −α)t) max\ns\n|H[π(·|s)]|.\nProof. Let nt denote the number of times that ai ̸= ˜ai for i < t, i.e. the number of times that π, ˜π\ndisagree before time t. We can decompose the expectations as follows\nEst∼π\n\u0002\nH[π(·|st)]\n\u0003\n= Est∼π|nt=0\n\u0002\nH[π(·|st)]\n\u0003\nP(nt = 0) + Est∼π|nt>0\n\u0002\nH[π(·|st)]\n\u0003\nP(nt > 0),\nEst∼˜π\n\u0002\nH[π(·|st)]\n\u0003\n= Est∼˜π|nt=0\n\u0002\nH[π(·|st)]\n\u0003\nP(nt = 0) + Est∼˜π|nt>0\n\u0002\nH[π(·|st)]\n\u0003\nP(nt > 0).\nNote that nt = 0 implies ai = ˜ai for all i < t hence\nEst∼˜π|nt=0\n\u0002\nH[π(·|st)]\n\u0003\n= Est∼π|nt=0\n\u0002\nH[π(·|st)]\n\u0003\n.\nThe deﬁnition of α−coupling implies P(nt = 0) ≥(1 −α)t, and so P(nt > 0) ≤1 −(1 −α)t.\nNow we note that\n|Est∼π|nt>0\n\u0002\nH[π(·|st)]\n\u0003\nP(nt > 0) −Est∼˜π|nt>0\n\u0002\nH[π(·|st)]\n\u0003\nP(nt > 0)| ≤2(1 −(1 −α)t) max\ns\n|H[π(·|s)]|.\nCombining previous observations, we have proved the lemma.\nNote that if we take π = πθ, ˜π = πθold, then the surrogate objective Jsurr(πθ) in (9) is equivalent to\nJsurr(π, ˜π) deﬁned in (13). With lemma A.2, we prove the following theorem.\nTheorem 4.1 (Lower Bound). If KL[πθ||πθold] ≤α, then\nJMaxEnt(π) ≥Jsurr(π) −βγ√αϵ\n(1 −γ)2 ,\nwhere ϵ = max\ns\n|H[π(·|s)]|.\n(10)\nProof. We ﬁrst show the result for general policies π and ˜π with KL\n\u0002\nπ||˜π\n\u0003\n≤α. As a result, [30]\nshows that π, ˜π are √α−coupled. Recall the maximum entropy objective JMaxEnt(π) deﬁned in (12)\nand surrogate objective in (13), take the difference of two objectives\n|JMaxEnt(π) −Jsurr(π, ˜π)| = β|\n∞\nX\nt=0\nγtEst∼π\n\u0002\nH[π(·|st)]\n\u0003\n−Est∼˜π\n\u0002\nH[˜π(·|st)]\n\u0003\n|\n≤β\n∞\nX\nt=0\nγt|Est∼π\n\u0002\nH[π(·|st)]\n\u0003\n−Est∼˜π\n\u0002\nH[˜π(·|st)]\n\u0003\n|\n≤β\n∞\nX\nt=0\nγt(1 −(1 −√α)t) max\ns\n|H[π(·|s)]|\n= (\n1\n1 −γ −\n1\n1 −γ(1 −√α))β√α max\ns\n|H[π(·|s)]|\n=\nβγ√α\n(1 −γ)2 max\ns\n|H[π(·|s)]|\n13\nNow observe that by taking π = πθ, ˜π = πθold, the above inequality implies the theorem.\nIn practice, α−coupling enforced by KL divergence is often relaxed [30, 31]. The theorem implies\nthat, by constraining the KL divergence between consecutive policy iterates, the surrogate objective\nof entropy regularization maximizes a lower bound of maximum entropy objective.\nB\nOperator view of Entropy Regularization and Maximum Entropy RL\nRecall in standard RL formulation, the agent is in state s, takes action a, receives reward r and\ntransitions to s′. Let the discount factor γ < 1. Assume that the reward r is deterministic and\nthe transitions s′ ∼p(·|s, a) are deterministic, i.e. s′ = f(s, a), it is straightforward to extend the\nfollowing to general stochastic transitions. For a given policy π, deﬁne linear Bellman operator as\nT πQ(s, a) = r + γEa′∼π(·|s′)\n\u0002\nQ(s′, a′)\n\u0003\n.\nAny policy π satisﬁes the linear Bellman equation T πQπ = Qπ. Deﬁne Bellman optimality operator\n(we will call it Bellman operator) as\nT ∗Q(s, a) = r + γ max\na′ Q(s′, a′).\nNow we deﬁne Mellowmax operator [2, 13] with parameter β > 0 as follows,\nTsQ(s, a) = r + γβ log\nZ\na′∈A\nexp(Q(s′, a′)\nβ\n)da′.\nIt can be shown that both T ∗and T are contractive operator when γ < 1. Let Q∗be the unique ﬁxed\npoint of T ∗Q = Q, then Q∗is the action value function of the optimal policy π∗= arg maxπ J(π).\nLet Q∗\ns be the unique ﬁxed point of TsQ = Q, then Q∗\ns is the soft action value function of π∗\ns =\narg maxπ JMaxEnt(π). In addition, the optimal policy π∗(·|s) = arg maxa Q∗(s, a) and π∗\ns(a|s) ∝\nexp( Q∗\ns(s,a)\nβ\n).\nDeﬁne Boltzmann operator with parameter β as follows\nTBQ(s, a) = r + γEa′∼pB(·|s′)\n\u0002\nQ(s′, a′)\n\u0003\n,\nwhere pB(a′|s′) ∝exp( Q(s′,a′)\nβ\n) is the Boltzmann distribution deﬁned by Q(s′, a′). [26] shows\nthat the stationary points of entropy regularization procedure are policies of the form π(a|s) ∝\nexp( Qπ(s,a)\nβ\n). We illustrate the connection between such stationary points and ﬁxed points of\nBoltzmann operator as follows.\nTheorem B.1 (Fixed points of Boltzmann Operators). Any ﬁxed point Q(s, a) of Boltzmann operator\nTBQ = Q, deﬁnes a stationary point for entropy regularized policy gradient algorithm by π(a|s) ∝\nexp( Q(s,a)\nβ\n); reversely, any stationary point of entropy regularized policy gradient algorithm π, has\nits action value function Qπ(s, a) as a ﬁxed point to Boltzmann operator.\nProof. Take any ﬁxed point Q of Boltzmann operator, TBQ = Q, deﬁne a policy π(a|s) ∝\nexp( Q(s,a)\nβ\n). From the deﬁnition of Boltzmann operator, we can easily check that π’s entropy\nregularized policy gradient is exactly zero, hence it is a stationary point for entropy regularized\ngradient algorithm.\nTake any policy π such that its entropy regularized gradient is zero, from [26] we know for such policy\nπ(a|s) ∝exp(Qπ(s, a)). The linear Bellman equation for such a policy T πQπ = Qπ translates\ndirectly into the Boltzmann equation TBQπ = Qπ. Hence Qπ is indeed a ﬁxed point of Boltzmann\noperator.\nThe above theorem allows us to associate the policies trained by entropy regularized policy gradient\nwith the Boltzmann operator. Unfortunately, [2] shows that unlike MellowMax operator, Boltzmann\noperator does not have unique ﬁxed point and is not a contractive operator in general, though this does\nnot necessarily prevent policy gradient algorithms from converging. We make a ﬁnal observation that\n14\nshows that Boltzmann operator TB interpolates Bellman operator T ∗and MellowMax operator Ts:\nfor any Q and ﬁxed β > 0 (see theorem B.2),\nT ∗Q ≥TBQ ≥TsQ.\n(14)\nIf we view all operators as picking out the largest value among Q(s′, a′), a′ ∈A in next state s′, then\nT ∗is the greediest and Ts is the most conservative, as it incorporates trajectory entropy as part of\nthe objective. TB is between these two operators, since it looks ahead for only one step. The ﬁrst\ninequality in (14) is trivial, now we show the second inequality.\nTheorem B.2 (Boltzmann Operator is greedier than Mellowmax Operator). For any Q ∈Rn, we\nhave TBQ ≥TsQ, and the equality is tight if and only if Qi = Qj for ∀i, j.\nProof. Recall the deﬁnition of both operators, we essentially need to show the following inequality\nEa′∼pB(·|s′)[Q(s′, a′)] ≥β log\nZ\na′∈A\nexp(Q(s′, a′)\nβ\n).\nWithout loss of generality, assume there are n actions in total and let xi = exp( Q(s′,ai)\nβ\n) for the ith\naction. The above inequality reduces to\nPn\ni=1 xi log xi\nPn\ni=1 xi\n≥log 1\nn\nn\nX\ni=1\nxi.\nWe did not ﬁnd any reference for the above inequality so we provide a proof below. Notice that\nxi > 0, ∀i. Introduce the objective J(x) =\nPn\ni=1 xi log xi\nPn\ni=1 xi\n−log 1\nn\nPn\ni=1 xi. Compute the gradient of\nJ(x),\n∂J(x)\n∂xj\n=\nPn\ni=1(log xj −log xi)xi\n(Pn\ni=1 xi)2\n, ∀j.\nThe stationary point at which ∂J(x)\n∂x\n= 0 is of the form xi = xj, ∀i, j. At such point, let xi = x0, ∀i\nfor some generic x0. Then we compute the Hessian of J(x) at such stationary point\n∂2J(x)\n∂x2\nj\n|xi=x0,∀i = n −1\nn2x2\n0\n, ∀j.\n∂2J(x)\n∂xk∂xj\n|xi=x0,∀i =\n1\nn2x2\n0\n, ∀j ̸= k.\nLet H(x0) be the Hessian at this stationary point. Let t ∈Rn be any vector and we can show\ntT H(x0)t =\nX\ni̸=j\n1\nn2x2\n0\n(ti −tj)2 ≥0,\nwhich implies that H(x0) is positive semi-deﬁnite. It is then implied that at such x0 we will achieve\nlocal minimum. Let xi = x0, ∀i we ﬁnd J(x) = 0, which implies that J(x) ≥0, ∀x. Hence the\nproof is concluded.\nC\nImplicit Policy Architecture\nC.1\nNormalizing Flows Policy Architecture\nWe design the neural network architectures following the idea of [5, 6]. Recall that Normalizing\nFlows [27] consist of layers of transformations as follows ,\nx = gθK ◦gθK−1 ◦... ◦gθ2 ◦gθ1(ϵ),\nwhere each gθi(·) is an invertible transformation. We focus on how to design each atomic trans-\nformation gθi(·). We overload the notations and let x, y be the input/output of a generic layer\ngθ(·),\ny = gθ(x).\n15\nWe design a generic transformation gθ(·) as follows. Let xI be the components of x corresponding to\nsubset indices I ⊂{1, 2...m}. Then we propose as in [6],\ny1:d = x1:d\nyd+1:m = xd+1:m ⊙exp(s(x1:d)) + t(x1:d),\n(15)\nwhere t(·), s(·) are two arbitrary functions t, s : Rd 7→Rm−d. It can be shown that such trans-\nformation entails a simple Jacobian matrix | ∂y\n∂xT | = exp(Pm−d\nj=1 [s(x1:d)]j) where [s(x1:d)]j refers\nto the jth component of s(x1:d) for 1 ≤j ≤m −d. For each layer, we can permute the input x\nbefore apply the simple transformation (15) so as to couple different components across layers. Such\ncoupling entails a complex transformation when we stack multiple layers of (15). To deﬁne a policy,\nwe need to incorporate state information. We propose to preprocess the state s ∈Rn by a neural\nnetwork Lθs(·) with parameter θs, to get a state vector Lθs(s) ∈Rm. Then combine the state vector\ninto (15) as follows,\nz1:d = x1:d\nzd+1:m = xd+1:m ⊙exp(s(x1:d)) + t(x1:d)\ny = z + Lθs(s).\n(16)\nIt is obvious that x ↔y is still bijective regardless of the form of Lθs(·) and the Jacobian matrix is\neasy to compute accordingly.\nIn our experiments, we implement s, t both as 4-layers neural networks with k = 3 or k = 6 units per\nhidden layer. We stack K = 4 transformations: we implement (16) to inject state information only\nafter the ﬁrst transformation, and the rest is conventional coupling as in (15). Lθs(s) is implemented\nas a feedforward neural network with 2 hidden layers each with 64 hidden units. Value function critic\nis implemented as a feedforward neural network with 2 hidden layers each with 64 hidden units with\nrectiﬁed-linear between hidden layers.\nC.2\nNon-Invertible Blackbox Policy Architecture\nAny implicit model architecture as in [10, 37] can represent a Non-invertible Blackbox Policy (NBP).\nOn MuJoCo control tasks, consider a task with state space S ⊂Rn and action space A ⊂Rm.\nConsider a feedforward neural network with n input units and m output units. The intermediate\nlayers have parameters θ and the output is a deterministic mapping from the input a = fθ(s). We\nchoose an architecture similar to NoisyNet [9]: introduce a distribution over θ. In our case, we choose\nfactorized Gaussian θ = µθ + σθ · ϵ. The implicit policy is generated as\na = fθ(s), θ = µθ + σθ · ϵ, ϵ ∼N(0, 1),\nwhich induces an implicit distribution over output a. In practice, we ﬁnd randomizing parameters θ\nto generate implicit policy works well and is easy to implement, we leave other approaches for future\nresearch.\nIn all experiments, we implement the network fθ(·) as a feedforward neural network with 2 hidden\nlayers each with 64 hidden units. Between layers we use rectiﬁed-linear for non-linear activation,\nlayer normalization to standardize inputs, and dropout before the last output. Both value function\ncritic and classiﬁer critic are implemented as feedforward neural networks with 2 hidden layers\neach with 64 hidden units with rectiﬁed-linear between hidden layers. Note that µθ, σθ are the\nactual parameters of the model: we initialize µθ using standard initialization method and initialize\nσθ = log(exp(ρθ) + 1) with ρθ ∈[−9.0, −1.0]. For simplicity, we set all ρθ to be the same and let\nρ = ρθ. We show below that dropout is an efﬁcient technique to represent multi-modal policy.\nDropout for multi-modal distributions.\nDropout [34] is an efﬁcient technique to regularize neural\nnetworks in supervised learning. However, in reinforcement learning where overﬁtting is not a big\nissue, the application of dropout seems limited. Under the framework of implicit policy, we want to\nhighlight that dropout serves as a natural method to parameterize multi-modal distributions. Consider\na feed-forward neural network with output y ∈Rm. Assume that the last layer is a fully-connected\nnetwork with h inputs. Let x ∈Rn be an input to the original neural network and φ(x) ∈Rh\nbe the input to the last layer (we get φ(x) by computing forward pass of x through the network\nuntil the last layer), where φ(x) can be interpreted as a representation learned by previous layers.\n16\nLet W ∈Rm×h, b ∈Rm be the weight matrix and bias vector of the last layer, then the output is\ncomputed as (we ignore the non-linear activation at the output)\nyi =\nh\nX\nj=1\nφj(x)Wij + bi, ∀1 ≤i ≤m.\n(17)\nIf dropout is applied to the last layer, let z be the Bernoulli mask i.e. zi ∼Bernoulli(p), 1 ≤i ≤h\nwhere p is the probability for dropping an input to the layer. Then\nyi =\nh\nX\nj=1\n(φj(x) · zj)Wij + bi, ∀1 ≤i ≤m\n(18)\nGiven an i, if each φj(x)Wij has a different value, their stochastic sum Ph\nj=1 φj(x) · zj)Wij in (18)\ncan take up to about 2h values. Despite some redundancy in these 2h values, in general yi in (18) has\na multi-modal distribution supported on multiple values. We have hence moved from a unimodal\ndistribution (17) to a multi-modal distribution (18) by adding a simple dropout.\nD\nAlgorithm Pseudocode\nBelow we present the pseudocode for an off-policy algorithm to train NBP. On the other hand, for\nNFP we can apply any on-policy optimization algorithms [31, 30] and we omit the pseudocode here.\nAlgorithm 1 Non-invertible Blackbox Policy (NBF) Off-policy update\n1: INPUT: target parameter update period τ; learning rate αθ, αφ, αψ; entropy regularization\nconstant β.\n2: INITIALIZE: parameters θ, φ, ψ and target network parameters θ−, φ−; replay buffer B ←{};\nstep counter counter ←0.\n3: for e = 1, 2, 3...E do\n4:\nwhile episode not terminated do\n5:\n// Control\n6:\ncounter ←counter + 1.\n7:\nIn state s, sample noise ϵ ∼ρ0(·), compute action a = fθ(s, ϵ), transition to s′ and receive\ninstant reward r.\n8:\nSave experience tuple {s, a, r, s′} to buffer B.\n9:\nSample N tuples D = {sj, aj, rj, s′\nj} from B.\n10:\n// Update Critic\n11:\nCompute TD error as in [24, 32] as follows, where a′\nj = fθ−(s′\nj, ϵj), ϵj ∼ρ0(·).\nJφ = 1\nN\nN\nX\nj=1\n(Qφ(sj, aj) −rj −γQφ−(s′\nj, a′\nj))2.\n12:\nUpdate φ ←φ −αφ∇φJφ.\n13:\n// Update classiﬁer\n14:\nSample N actions uniformly from action space a(u)\nj\n∼U(A). Compute classiﬁcation\nobjective Cψ (7) using data {sj, aj}N\nj=1 against {sj, a(u)\nj\n}N\nj=1.\n15:\nUpdate classiﬁer ψ ←ψ −αψ∇ψCψ.\n16:\n// Update policy with entropy regularization.\n17:\nCompute pathwise gradient ∇θJ(πθ) (6) with Qπ(s, a) replaced by critic Qφ(s, a) and\nstates replaced by sampled states sj.\n18:\nCompute entropy gradient ∇θH\n\u0002\nπθ(·|s)\n\u0003\nusing (8) on sampled data.\n19:\nUpdate θ ←θ + αθ(∇θJ(πθ) + β∇θH\n\u0002\nπθ(·|s)\n\u0003\n)\n20:\nif counter mod τ = 0 then\n21:\nUpdate target parameter φ−←φ, θ−←θ.\n22:\nend if\n23:\nend while\n24: end for\n17\nE\nAdditional Experiment Results\nE.1\nLocomotion tasks\nAs has been shown in previous works [31], PPO is almost the most competitive on-policy optimization\nbaseline on locomotion control tasks. We provide a table of comparison among on-policy baselines\nbelow. On each task we train for a speciﬁed number of time steps and report the average results\nover 5 random seeds. Though NFP remains a competitive algorithm, PPO with unimodal Gaussian\ngenerally achieves better performance.\nTasks\nTimesteps\nPPO\nA2C\nCEM\nTRPO\nNFP\nHopper\n106\n≈2300\n≈900\n≈500\n≈2000\n≈1880\nHalfCheetah\n106\n≈1900\n≈1000\n≈500\n≈0\n≈2200\nWalker2d\n106\n≈3500\n≈900\n≈800\n≈1000\n≈1980\nInvertedDoublePendulum\n106\n≈8000\n≈6500\n≈0\n≈0\n≈8000\nTable 1: A comparison of NFP with (on-policy) baseline algorithms on MuJoCo benchmark tasks. For each task,\nwe show the average rewards achieved after training the agent for a ﬁxed number of time steps. The results for\nNFP are averaged over 5 random seeds. The results for A2C, CEM [7] and TRPO are approximated based on\nthe ﬁgures in [31], PPO is from OpenAI baseline implementation [4]. We highlight the top two algorithms for\neach task in bold font. PPO, A2C and TRPO all use unimodal Gaussians. PPO is the most competitive.\nE.2\nMulti-modal policy: Fine-tuning for downstream tasks\nIn Figure 4, we compare trajectories generated by agents pre-trained by DDPG and NBP on the\nrunning task. Since DDPG uses a deterministic policy, starting from a ﬁxed position, the agent can\nonly run in a single direction. On the other hand, NBP agent manages to run in multiple directions.\nThis comparison partially illustrates that a NBP agent can learn the concept of general running,\ninstead of specialized running – running in a particular direction.\n(a) Trajectories by DDPG agent\n(b) Trajectories by NBP agent\nFigure 4: (a)(b): Trajectories generated by DDPG pre-trained agents and NBP pre-trained agent on Ant-Running\ntask under different random seeds. Starting from the initial position, DDPG agent can only produce a single\ndeterministic trajectory due to the deterministic policy; NBP agent produces trajectories that are more diverse,\nillustrating that NBP agent learns to run in multiple directions.\nE.3\nMulti-modal policy: Combining multiple modes by Imitation Learning.\nDidactic Example.\nWe motivate combining multiple modes of imitation learning with a simple\nexample: imitating an expert with two modes of behavior. Consider a simple MDP on an axis with\nstate space S = [−10, 10], action space A = [−1, 1]. The agent chooses which direction to move\nand transitions according to the equation st+1 = st + at. We design an expert that commits itself\nrandomly to one of the two endpoints of the state space s = −10 or s = 10 by a bimodal stochastic\npolicy. We generate 10000 trajectories from the expert and use them as training data for direct\nbehavior cloning.\nWe train a NBP agent using GAN training [10]: given the expert trajectories, train a NBP as a generator\nthat produces similar trajectories and train a separate classiﬁer to distinguish true expert/generated\ntrajectories. Unlike maximum likelihood, GAN training tends to capture modes of the expert\ntrajectories. If we train a unimodal Gaussian policy using GAN training, the agent may commit to a\nsingle mode; below we show that trained NBP policy captures both modes.\n18\n(a) Actions by Expert\n(b) Actions by NBP agent\n(c) Trajectories by Expert\n(d) Trajectories by NBP agent\nFigure 5: Imitating a bimodal expert: (a)(b) compare the actions produced by the expert and the\ntrained NBP agent at different states s. The expert policy has a bimodal policy across different\nstates and becomes increasingly unimodal when s ≈±10; the trained policy captures such bimodal\nbehavior. (c)(d) compare the trajectories of expert/trained agent. The vertical axis indicates the\nstates s and horizontal axis is the time steps in an episode, each trajectory is terminated at s = ±10.\nTrajectories of both expert and trained policy are very similar.\nStochastic Swimmer.\nThe goal is to train a Swimmer robot that moves either forward or backward.\nIt is not easy to specify a reward function that directly translates into such bimodal behavior and it\nis not easy to train a bimodal agent under such complex dynamics even if the reward is available.\nInstead, we train two Swimmers using RL objectives corresponding to two deterministic modes:\nswimming forward and swimming backward. Combining the trajectories generated by these two\nmodes provides a policy that stochastically commits to either swimming forward or backward. We\ntrain a NFP agent (with maximum likelihood behavior cloning [8]) and NBP agent (with GAN\ntraining [10]) to imitate expert policy. The trajectories generated by trained policies show that the\ntrained policies have fused these two modes of movement.\nF\nHyper-parameters and Ablation Study\nHyper-parameters.\nRefer to Appendix C for a detailed description of architectures of NFP and\nNBP and hyper-parameters used in the experiments. For NFP, critical hyper-parameters are entropy\ncoefﬁcient β, number of transformation layers K and number of hidden units per layer k for\ntransformation function s, t. For NBP, critical hyper-parameters are entropy coefﬁcient β and the\ninitialized variance parameter for factorized Gaussian ρ. In all conventional locomotion tasks, we\nset β = 0.0; for multi-modal policy tasks, we set β ∈{0.1, 0.01, 0.001}. We use Adam [16] for\noptimization with learning rate α ∈{3 · 10−5, 3 · 10−4}.\nAblation Study.\nFor NBP, the default baseline is β = 0.0, ρ = −4.0. We ﬁx other hyper-parameters\nand change only one set of hyper-parameter to observe its effect on the performance. Intuitively, large\nρ encourages and widespread distribution over parameter θ and consequently and a more uniform\ninitial distribution over actions. From Figure 7 we see that the performance is not monotonic in\nρ, β. We ﬁnd the model is relatively sensitive to hyper-parameter ρ and a general good choice is\nρ ∈{−4.0, −5.0, −6.0}.\nFor NFP, the default baseline is β = 0.0, K = 4, k = 3. We ﬁx other hyper-parameters and change\nonly one set of hyper-parameter to observe its effect on the performance. In general, we ﬁnd the\nmodel’s performance is fairly robust to hyper-parameters (see Figure 8): large K, k will increase the\ncomplexity of the policy but does not necessarily beneﬁt performance on benchmark tasks; strictly\npositive entropy coefﬁcient β > 0.0 does not make much difference on benchmark tasks, though\nfor learning multi-modal policies, adding positive entropy regularization is more likely to lead to\nmulti-modality.\n19\n(a) Swimmer\n(b) Trajectories by Expert\n(c) Trajectories by NBP agent\n(d) Trajectories by NFP agent\nFigure 6: Combining multiple modes by Imitation Learning: stochastic Swimmer. (a) Illustration\nof Swimmer; (b) Expert trajectories produced by two Swimmers moving in two opposite directions\n(forward and backward). Vertical axis is the x coordinate of the Swimmer, horizontal axis is the\ntime steps; (c) Trajectories produced by NBP agent trained using GAN under different seeds; (d)\nTrajectories produced by NFP agent trained using maximum likelihood under different seeds. Implicit\npolicy agents have incorporated two modes of behavior into a single policy, yet a unimodal Gaussian\npolicy can at most commit to one mode.\n20\n(a) Reacher: entropy\n(b) Reacher: initial Gaussian vari-\nance parameter\n(c) HalfCheetah: entropy\n(d) HalfCheetah: initial Gaussisn\nvariance parameter\nFigure 7: Ablation study: NBP\n(a) Hopper: entropy\n(b) Hopper: # of layers\n(c) Hopper: # of units\n(d) Reacher: entropy\n(e) Reacher: # of layers\n(f) Reacher: # of units\n(g) HalfCheetah: entropy\n(h) HalfCheetah: # of layers\n(i) HalfCheetah: # of units\nFigure 8: Ablation study: NFP\n21\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-06-10",
  "updated": "2019-02-03"
}