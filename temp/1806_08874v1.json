{
  "id": "http://arxiv.org/abs/1806.08874v1",
  "title": "The Foundations of Deep Learning with a Path Towards General Intelligence",
  "authors": [
    "Eray Özkural"
  ],
  "abstract": "Like any field of empirical science, AI may be approached axiomatically. We\nformulate requirements for a general-purpose, human-level AI system in terms of\npostulates. We review the methodology of deep learning, examining the explicit\nand tacit assumptions in deep learning research. Deep Learning methodology\nseeks to overcome limitations in traditional machine learning research as it\ncombines facets of model richness, generality, and practical applicability. The\nmethodology so far has produced outstanding results due to a productive synergy\nof function approximation, under plausible assumptions of irreducibility and\nthe efficiency of back-propagation family of algorithms. We examine these\nwinning traits of deep learning, and also observe the various known failure\nmodes of deep learning. We conclude by giving recommendations on how to extend\ndeep learning methodology to cover the postulates of general-purpose AI\nincluding modularity, and cognitive architecture. We also relate deep learning\nto advances in theoretical neuroscience research.",
  "text": "arXiv:1806.08874v1  [cs.AI]  22 Jun 2018\nThe Foundations of Deep Learning with a Path Towards\nGeneral Intelligence\nEray ¨Ozkural\nCelestial Intellect Cybernetics\ncelestialintellect.com\nAbstract. Like any ﬁeld of empirical science, AI may be approached axiomati-\ncally. We formulate requirements for a general-purpose, human-level AI system\nin terms of postulates. We review the methodology of deep learning, examin-\ning the explicit and tacit assumptions in deep learning research. Deep Learn-\ning methodology seeks to overcome limitations in traditional machine learning\nresearch as it combines facets of model richness, generality, and practical ap-\nplicability. The methodology so far has produced outstanding results due to a\nproductive synergy of function approximation, under plausible assumptions of ir-\nreducibility and the efﬁciency of back-propagation family of algorithms. We ex-\namine these winning traits of deep learning, and also observe the various known\nfailure modes of deep learning. We conclude by giving recommendations on how\nto extend deep learning methodology to cover the postulates of general-purpose\nAI including modularity, and cognitive architecture. We also relate deep learning\nto advances in theoretical neuroscience research.\n1\nIntroduction\nDeep learning is a rapidly developing branch of machine learning which is clustered\naround training deep neural models with many layers and rich computational structure\nwell suited to the problem domain [15,44]. Initially motivated by modelling the visual\ncortex [11,12], human-level perceptual performance was approached and eventually\nattained in a number of challenging visual perception tasks such as image recogni-\ntion with the aid of GPU acceleration [31,38,16]. The applications quickly extended\nto other computer vision tasks such as image segmentation [4], producing a variety of\nimpressive results in visual information processing such as style transfer [13], opening\nnew vistas in machine learning capabilities. The applications have been extended to do-\nmains beyond vision, such as speech recognition [18], language processing [29], and\nreinforcement learning [30] , often with striking performance, proving the versatility\nand the signiﬁcance of the approach in AI, urging us to consider whether the approach\nmay yield a general AI (called Artiﬁcial General Intelligence (AGI) in some circles),\nand if so which problems would have to be tackled to make deep learning approach\ntruly human-level AI that covers all aspects of cognition.\nWe analyze the approach from a 10,000 feet vantage point, revisiting the idea of AI\naxiomatization. Although, we are generally in agreement with Minsky that the attempt\nto make AI like physics is likely a futile pursuit, we also note the achievements of\nlater theorists who have applied Bayesian methods successfully. We make no attempt\n2\nEray ¨Ozkural\nto formalize any of our claims due to space consideration, however we discuss relevant\nresearch in cognitive sciences. Then, we apply the same foundational thinking to deep\nlearning critically probing its intellectual foundations. The axioms, or postulates, of AI,\nare examined with an eye towards whether the current progress in deep learning in some\nway satisﬁes them, and what has to be done to ﬁll the gap. The present paper may thus\nbe regarded as an analytical, critical meta-level review, rather than a comprehensive\nreview such as [44].\n2\nPostulates of General AI\nOne of the most ambitious mathematical models in AGI research is AIXI [26] which\nis a universal Reinforcement Learning (RL) model that can be applied to a very large\nvariety of AI agent models and AI tasks including game playing, machine learning\ntasks, and general problem solving. AIXI is based on an extension of Solomonoff’s\nsequence induction model which works with arbitrary loss and alphabet [24], making\nthe aforementioned induction problem fairly general. Hutter proves in his book [25]\nthat many problems can be easily transformed to this particular formulation of universal\ninduction. There are a few conditions that have to be satisﬁed for a system to be called\na universal induction system, and even then the system must be realized in a practical\nmanner so as to be widely applicable and reproduce the cognitive competencies of homo\nsapiens, or failing that, a less intelligent animal.\nThe AIXI model combines Bellman equation with universal induction, casting ac-\ntion selection as the problem of maximizing expected cumulative reward in any com-\nputable environment. Although RL is a common approach in machine learning, AIXI\nhad the novelty that it focused solely on universal RL agents. When viewed this way,\nit is obvious that AIXI is a minimalist cognitive architecture model, that exploits the\npredictive power of induction in RL setting, that does give the model the kind of ver-\nsatility noted above. Solomonoff induction presents a desirable limit of inductive infer-\nence systems, since it has the least generalization error possible; the error is dependent\nonly on the stochastic source and a good approximation can learn from very few ex-\namples [46]. AIXI model also retains a property of optimal behavior, Hutter deliberates\nthat the model deﬁnes optimal, but incomputable intelligence, and thus any RL agent\nmust approximate it. Therefore, our axiomatization must consider the conditions for\nSolomonoff’s universal induction model, and consequently AIXI, to be approximated\nwell, but we believe additional conditions are necessary for it to also satisfy generality\nin practice and within a versatile system, as follows. Completenes: The class of mod-\nels that can be acquired by the machine learning system must be Turing-complete. If\na large portion of the space of programs is unavailable to the system, it will not have\nthe full power and generalization properties of Solomonoff induction. The convergence\ntheorem in that case is voided, and the generalization performanceof Solomonoff induc-\ntion cannot be guaranteed [46]. Stochastic Models: The system requires an adequately\nwide class of stochastic models to deal with uncertainty in the real world, a system\nwith only deterministic components will be brittle. Induction is better suited to working\nwith stochastic models, one example of such an approach is Wallace’s Minimum Mes-\nsage Length (MML) model where we minimize the message length that contains both\nthe length of the statistical model encoding and data encoding length relative to model\n[51,50]. Bayesian Prediction: The system must compute the inferences with Bayes’\nThe Foundations of Deep Learning with a Path Towards General Intelligence\n3\nlaw. The inference in Solomonoff’s model is considered Bayesian. In neuroscience, the\nBayesian Brain Hypothesis has been mostly accepted, and the brain is often regarded\nas a Bayesian inference machine that extracts information from the environment in the-\noretical neuroscience. Jaynes introduced the possibility of Bayesian reasoning in the\nbrain from a statistical point of view [27]. The Bayesian approach to theoretical neu-\nroscience is examined in a relatively recent book [5]. Fahlman et. al introduced the\nstatistically motivated energy minimizing Boltzmann machine model [7]; Hinton et.\nal connected the induction principle of Minimum Description Length and Helmholtz\nfree energy introducing the autoencoder model in 1993 [22]. Bialek’s lab has greatly\ncontributed to the understanding of the Bayesian nature of the brain, a decent sum-\nmary of the approach detailing the application of the information bottleneck method\nmay be found in [1]. Friston has later rigorously applied the free energy principle and\nhas obtained even more encouraging results, he explains the Bayesian paradigm in [9].\nNote that Helmoltz free energy and the free energy principle are related, and both are\nrelated to approximate Bayesian inference. Principle of Induction: The system must\nhave a sound principle of induction that is equivalent to Solomonoff’s model of in-\nduction which uses an a priori probability model of programs that is inversely and ex-\nponentially proportional to program size. Without the proper principle of induction,\ngeneralization error will suffer greatly, as the system will be corrupted. Likewise, as\nSolomonoff induction is more completely approximated, the generalization error will\ndecrease dramatically, allowing the system to obtain one-shot learning ﬁrst predicted\nby Solomonoff, achieving a successful generalization from a sufﬁciently complex sin-\ngle example without any prior training whenever such an example is possible. Practical\nApproximation: Solomonoff induction has an exponential worst-case bound with re-\nspect to program size rendering it infeasible. This surely is not a practical result, any\napproximation must introduce algorithmic methods to obtain a feasible approximation\nof the theoretical inductive inference model. Incremental Learning: The system must\nbe capable of cumulative learning, and therefore it must have a model of memory with\nadequate practical algorithms. Solomonoff has himself described a rather elaborate ap-\nproach to transfer learning [47], however, it was not until much later that experimental\nresults were possible for universal induction since Solomonoff’s theoretical descrip-\ntion did not specify an efﬁcient algorithm. The ﬁrst such result was obtained in OOPS\nsystem [45] demonstrating signiﬁcant speedups for a universal problem solver. Mod-\nularity and Scalability: The system must be composed of parametrized modules that\nattend to different tasks, allowing complex ensemble systems to be built for scalability\nlike the neocortex in the human brain. A monolithic system is not likely to scale well,\nthe system must be able to adapt modules to distinct tasks, and then be able to re-use\nthe skills. A modular system also provides a good base for specialization according to\nmodality and cognitive task, starting from a common module description. In the human\nbrain, there are both functional regions and a complex, hierarchical modular structure in\nthe form of cortical columns, and micro-columns. Cognitive Architecture: The system\nmust have a cognitive architecture, depending on modularity that will address typical\ncognitive functions of learning, memory, perception, reasoning, planning, and language\nas well as aspects of robotics which allow it to control robotic appendages. This man-\nner of organization is modeled after the human brain, however, it seems essential for\n4\nEray ¨Ozkural\nany real-world AI system that requires these basic competencies to deliver robust per-\nformance across a sufﬁciently general set of cognitive tasks. Even if unlike the brain,\nthe system must have an architectural design, or one that is capable of introducing the\nrequired architecture.\nThese reasonable and desirable properties of a complete AI system lead naturally\nto a top-down design sometimes called an AGI Uniﬁcation Architecture among prac-\ntitioners, if built around the ﬂoor plan of a universal induction system such as AIXI.\nAn example of such an approach to designing a cognitive architecture may be seen in\n[36]. However, this is not necessarily the only kind of solution. An adequate architec-\nture could also be built around a deep learning approach; let us therefore proceed to its\npostulates.\n3\nPostulates of Deep Learning\nDeep Learning is a particular kind of Artiﬁcial Neural Network (ANN) research which\nshares some commonalities and inherits some assumptions / principles from earlier\nANN research some of which may seem implicit to outsiders. We try to recover these\ntacit or implicit assumptions for the sake of general AI readership, and also delineate the\nborders of deep learning from other ANN research in the following. No Free Lunch:\nThe well-known No Free Lunch theorem for machine learning implies that there can\nbe no general learning algorithm that will be effective for all problems. This theorem\nhas generated a strong bias towards model-based learning in ANN research where the\nresearcher tries to design a rich network model that covers all contingencies in the do-\nmain but uses insights into the problem domain and thus the experiment does not suffer\nfrom the unreasonable large search space of a model-free learning method. From im-\nage processing to language, this particular blend of speciﬁcity and generality seems to\nhave guided deep learning quite successfully and resulted in impressive outcomes. The\nspeciﬁcity determined by the ANN researcher may be likened to “innateness” in cogni-\ntive science. Note that AGI theorists have argued otherwise [6], therefore this heuristic\nprinciple remains arguable. Epistemic Non-reductionism: This is the view that loosely\ndepends on Quine’s observation that epistemic reductionism often fails in terms of ex-\nplanatory power for the real world [37], which is to say that there is a wealth of neces-\nsary complexity to account for it. When we look at a deep learning vision architecture,\nwe see that the irreducible patterns of visual information are indeed stored as they are\nuseful however not overmuch; the system does not store every pattern much like our\nbrains. Epistemic irreducibility is a guiding principle in deep learning research, and it\nis why deep learning models are large rather than small and minimalistic as in some\nANN research. Eliminative Materialism: Churchland’s philosophical observation that\nthe brain does not deal in any of the folk psychological concepts in cognitive science\nliterature, but must be understood as the activation state and trajectory of the brain [3],\nplays a fundamental intellectual role in the deep learning approach, where we shift our\nattention to brain-like representations and learning for dealing with any problem, even if\nit looks like a matter of propositional logic to us. Subsymbolic & Distributed Repre-\nsentation: Expressed in detail in the classical connectionist volume [40], this principle\nis the view that all representations in the brain have a distributed, real-valued represen-\ntation rather than discrete, symbolic representations that computer scientists prefer in\ntheir programs. Sparse Coding hypothesis has been mostly conﬁrmed in neuroscience,\nThe Foundations of Deep Learning with a Path Towards General Intelligence\n5\ntherefore we do know that the brain uses population codes that are sparse, distributed,\nand redundant. Unlike a symbolic representation, the brain networks are fault-tolerant\nand redundant, and deal with uncertainty at every level. Subsymbolic representations\nare more robust and better suited to the nature of sensory input. However, we also know\nthat “grandmother cells” exist which may correspond to predicates, which are still best\nmodeled as non-linear detectors, or ReLu units, in a neural network. Universal Ap-\nproximation: The universal approximation theorem [23] for multi-layer feed forward\nneural networks underlies the heuristic of using many hidden layers in a deep learn-\ning architecture. The theorem shows that a multi-layer neural network can approximate\narbitrary continuous real-valued functions. Therefore, the system is capable of repre-\nsenting any mapping under mild assumptions, including those with irregular features\nforming a synergy with the epistemic non-reductionism postulate. Deep Models: The\nnumber of layers in a feed forward network, or the circumference of a Recurrent Neural\nNetwork (RNN) must be greater than 3, meaning multiple hidden layers in a multi-layer\nfeed forward network, or an RNN with complex topology. Model depth avoids much\nof the criticism in Minsky and Papert’s critical book on neural networks that showed\nperceptrons cannot learn concave discriminants [34], and its later editions that extend\nthe criticism to multi-layer models. In today’s ANN applications we observe all man-\nners of intricate discrimination models were successfully learnt, however shallow net-\nworks will still not avoid Minsky’s observations. A complexity analysis also supports\nthat increasing depth can result in asymptotically smaller networks for the same func-\ntion representation [48], implying that deep models are fundamentally more efﬁcient.\nHierarchy and Locality: A distinguishing feature of deep learning is that it contains\nlocal pattern recognition networks and a hierarchy of these pattern recognition circuits\nthat afﬁxes the local and global views. Thus, a sequence of convolutional and pool-\ning layers have been a staple of image processing applications in deep learning as the\nconvolutional layer is basically a set of texture recognition patches, and downsampling\nvia max-pooling gives us a dimensionality reduction and the ability to hierarchically\ncombine pattern recognizers efﬁciently. This organization was inspired by 2d image\nprocessing in the visual cortex, however many domains can beneﬁt from the same orga-\nnizational principle since they apply to any sensory array. The principle is also valid for\ndomains that are not directly sensory arrays, but maintain a similar topological relation.\nThe principle also has great synergy with the depth principle because the network tries\nto capture perceptually salient features and avoids learning irrelevant patterns making\nit possible to increase network depth which avoids Minskyan objections even more ef-\nfectively. Gradient Descent: Perhaps the most common feature of deep learning is that\na variation of back propagation or gradient descent is used to train the model. This\nis required since any other way to train the large networks in deep learning research\nwould be infeasible. Other methods such as variational learning and MCMC tree search\nhave been applied in deep learning research, however this principle has remained fairly\nconstant as it is necessitated by other principles above, which may result in billions of\nreal valued parameters to be trained. Dataﬂow models & SIMD acceleration Since\nthe number of parameters to be trained is large, exploiting data-parallelism through\nSIMD-based accelerators such as GPU’s, and later executing data-ﬂow representations\non FPGA’s have proven to be an essential factor for deep learning research. This prop-\nerty of deep learning corresponds to the “massive parallelism” property of the brain.\n6\nEray ¨Ozkural\n4\nShortcomings and Extensions\nAlthough deep learning has generated phenomenal results, it also has some shortcom-\nings that are being worked on. The most common limitation is that a typical deep learn-\ning architecture requires on the order of 10,000 or more examples. Some of the largest\nexperiments have used millions of examples, therefore this was simply not an issue that\nwas focused on. It may well be the case that this is a fundamental shortcoming of deep\nlearning, however, researchers have tried solutions such as using stochastic gradient\nover the entire set of samples, as a usual statistical approach would necessitate, instead\nof running BP in epochs, which imitates the brain’s online learning capability. Another\ncommon problem is that most deep learning uses supervised learning, which presents a\nproblem in terms of constructing many labeled/annotated examples for every new prob-\nlem. Autoencoder [21] is an unsupervised learning model, and it has many variations\nand applications in deep learning, however, most applications still require a good deal\nof hand crafted data. A strange problem persists in deep learning systems, which makes\nthem easy to fool in ways that are not intuitive to humans, such as a simple perturba-\ntion causing a misclassiﬁcation, an intuitively unrelated artiﬁcial image recognized as\na natural image, or a specially crafted patch on an unrelated image causing a misclassi-\nﬁcation. These might either be symptoms of fundamental limitations, or they might be\nameliorated with better deep learning models. We observe that these issues look much\nlike overﬁtting, i.e., poor generalization performance.\nWhen we contrast the general AI postulates and deep learning postulates, we see\nsome interesting overlap and also some areas where deep learning requires a good deal\nof development. A deep learning system has one sort of completeness that stems from\nthe universal approximation theorem, and dataﬂow models can be augmented with ar-\nbitrary computational units such as the Neural Turing Machine model [17], and the\nlater Differentiable Neural Computer model [19] that augments neural networks with\nexternal memory. Program class extensions of this sort may be an integral part of next-\ngeneration deep learning. Recent proposals for non-Euclidian embedding of data also\nenhance generality of deep learning models [2].\nIt is possible to design deep architectures for rigorous stochastic models, which is\nan important extension to deep learning that will increase robustness.\nTypically, deep learning lacks a principle of induction, but at the same time a stochas-\ntic model of induction is implicit in deep learning as the information bottleneck analysis\nof deep learning shows [49], where we can view deep learning as a lossy compression\nscheme that forgets unnecessary information. Such theories will lead to better gen-\neralization performance. [28] applies random matrix theory to generalization in deep\nlearning, and introduces a new regularization method for improving generalization.\nProgressive deep learning architectures add layers as necessary, substantiating an\nimportant analogy to SVM’s function class iteration [41]. Much richer forms of induc-\ntion may be beneﬁcial for improving a deep learning network’s generalization power.\nThe training procedure in deep learning is efﬁcient but only locally optimal, in the fu-\nture a combination of neuro-evolution and gradient descent may outperform gradient\ndescent and approximate universal induction better. Evolution has already been applied\nto automated design of deep networks [33,35]. Neuro-evolution has been shown to be\nThe Foundations of Deep Learning with a Path Towards General Intelligence\n7\neffective in game playing [39] and other tasks that are difﬁcult for deep learning, and\ntherefore it might displace deep learning methodology altogether in the future.\nDeep learning architectures gained memory capability with the LSTM unit, and\nsimilarly designed memory cells, however, long-term memory across tasks remains\nproblematic. A good realization of algorithmic memory in deep learning is Neural Task\nProgramming (NTP) [53] which achieves an indexical algorithmic memory based on\nLSTM and the ability to hierarchically decompose skills which has been successfully\napplied to robotics tasks. Progress in the direction of NTP is likely to be a major im-\nprovement for deep learning, since without cumulative and hierarchical learning intel-\nligence is highly restricted.\nRecently, progress has been made in the matter of modularity with Hinton’s update\nof Capsule Networks, that models the cortical architecture for visual tasks [42]. Cap-\nsule Networks adds dynamic routing between visual processing modules with afﬁne\ntransformations, enhancing invariance and deﬁnes neural modules as capsules that may\nbe arranged like neurons. Capsules correspond to visual entities in the model, therefore\ncapsules that recognize a face decompose into eyes, a nose, lips, and so forth. The step\nfrom monolithic to modular deep learning is as powerful as the step from shallow to\ndeep networks, hence this line of research is a signiﬁcant extension of deep learning.\nA similar line of research is advanced by Vicarious, which propose a recursive neural\narchitecture that exploits lateral connections accounting for distinct feature sets such as\ncontour and surface, and the hierarchical representation of entities like in Capsule Net-\nworks [14]; their system can reportedly break CAPTCHA’s. Hawkins proposes a new\ncortex architecture that introduces pyramidal neurons, active dendrites, and multiple\nintegration sites, identifying cortical computations for hierarchical sequence memory,\nand it intriguingly involves dendritic computation [20]. Capsule Networks might be en-\nhanced to provide a similar dendritic model eventually, or capsule-like speciation might\nbe ported to Hawkins’s model.\nCognitive architectures built on symbolic concepts may not be readily applicable\nto deep learning, however, modeling the functional anatomy of the brain creates much\nneeded synergy with neural networks. For instance, in Deep Mind’s I2A model [52], we\nsee a direction towards capturing more brain function in the form of imagining future\nstates, while PathNet presents a modular, reﬂective learning system that can recombine\nnetwork modules by evolving paths over the network [8]. Both neural architectures\nexhibit progress towards a more complete cognitive neural architecture. Another recent\ndirection is the relational networks that model reasoning [43]. Conceivably, neural mod-\nels of fundamental cognitive functions may be developed with a similar methodology,\nand bound in a connectionist agent architecture. Likewise, the active inference agent of\n[10] with deep temporal models captures the essentials of functional anatomy based on\nhierarchical probabilistic models, and even gives us a fully unsupervised agent model\nthat is quite intriguing from a scientiﬁc perspective.\n5\nDiscussion and Future Research\nDespite recent criticism raised against deep learning [32], almost all of the postulates of\ngeneral AI we have outlined seem achievable, however, with major improvements over\nexisting systems. While it is entirely possible for a traditional symbolic-oriented system\nto achieve the same performance, the advantages of deep learning approach cannot be\n8\nEray ¨Ozkural\nneglected, and the possible extensions to deep learning discussed may also ameliorate\nthe common shortcomings we summarized. Another combination that might work is\nthe combination of the symbolic AI approach with deep learning. In some circles, re-\nsearchers pursue a mathematical AI uniﬁcation approach (like AIXI approximations),\nhowever, the merits of such an approach are yet to be proven experimentally over deep\nlearning. It seems prudent to at least try to integrate deep learning faithfully in existing\nAI architectures, or for new architectures, attempt to construct them solely on a neural\narchitecture. In the future, we expect a convergence of more powerful training meth-\nods and deep architectures, taking us to a more model-free learning system, and more\ncapable, modular neural agent architectures inspired by neuroscience.\nReferences\n1. Bialek, W., Nemenman, I., Tishby, N.: Predictability, complexity, and learning. Neural Com-\nputation 13(11), 2409–2463 (2001). https://doi.org/10.1162/089976601753195969\n2. Bronstein, M.M., Bruna, J., LeCun, Y., Szlam, A., Vandergheynst, P.: Geometric Deep Learn-\ning: Going beyond Euclidean data. IEEE Signal Processing Magazine 34, 18–42 (Jul 2017).\nhttps://doi.org/10.1109/MSP.2017.2693418\n3. Churchland, P.M.: Eliminative materialism and the propositional attitudes. Journal of Philos-\nophy 78(February), 67–90 (1981)\n4. Ciresan, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Neural networks for seg-\nmenting neuronal structures in EM stacks. In: ISBI Segmentation Challenge Competition:\nAbstracts (2012)\n5. Doya, K., Ishii, S., Pouget, A., Rao, R.P.N.: Bayesian brain: Probabilistic approaches to\nneural coding. The MIT Press (2007)\n6. Everitt, T., Lattimore, T., Hutter, M.: Free lunch for optimisation under the uni-\nversal distribution. In: Proceedings of the IEEE Congress on Evolutionary Com-\nputation,\nCEC\n2014,\nBeijing,\nChina,\nJuly\n6-11,\n2014.\npp.\n167–174\n(2014).\nhttps://doi.org/10.1109/CEC.2014.6900546\n7. Fahlman, S.E., Hinton, G.E., Sejnowski, T.J.: Massively parallel architectures for\nai: Netl, thistle, and boltzmann machines. In: Proceedings\nof the Third AAAI\nConference on Artiﬁcial Intelligence. pp. 109–113. AAAI’83, AAAI Press (1983),\nhttp://dl.acm.org/citation.cfm?id=2886844.2886868\n8. Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A.A., Pritzel, A., Wierstra,\nD.: PathNet: Evolution Channels Gradient Descent in Super Neural Networks. ArXiv e-prints\n(Jan 2017)\n9. Friston, K.: The history of the future of the bayesian brain. Neuroimage 62(248), 1230–1233\n(2012)\n10. Friston, K.J., Rosch, R., Parr, T., Price, C., Bowman, H.: Deep temporal models\nand active inference. Neuroscience and Biobehavioral Reviews 77, 388 – 402 (2017).\nhttps://doi.org/https://doi.org/10.1016/j.neubiorev.2017.04.009\n11. Fukushima, K.: Neocognitron: A self-organizing neural network for a mechanism of pattern\nrecognition unaffected by shift in position. Biological Cybernetics 36(4), 193–202 (1980)\n12. Fukushima, K.: Artiﬁcial vision by multi-layered neural networks: Neocognitron and its ad-\nvances. Neural Networks 37, 103–119 (2013)\n13. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neu-\nral networks. In: 2016 IEEE Conference on Computer Vision and Pattern Recogni-\ntion, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. pp. 2414–2423 (2016).\nhttps://doi.org/10.1109/CVPR.2016.265\n14. George, D., Lehrach, W., Kansky, K., L´azaro-Gredilla, M., Laan, C., Marthi, B., Lou,\nX., Meng, Z., Liu, Y., Wang, H., Lavin, A., Phoenix, D.S.: A generative vision model\nThe Foundations of Deep Learning with a Path Towards General Intelligence\n9\nthat trains with high data efﬁciency and breaks text-based captchas. Science (2017).\nhttps://doi.org/10.1126/science.aag2612\n15. Goodfellow,\nI.,\nBengio,\nY.,\nCourville,\nA.:\nDeep\nLearning.\nMIT\nPress\n(2016),\nhttp://www.deeplearningbook.org\n16. Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., Schmidhuber, J.: A novel\nconnectionist system for improved unconstrained handwriting recognition. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence 31(5) (2009)\n17. Graves, A., Wayne, G., Danihelka, I.: Neural Turing Machines. ArXiv e-prints (Oct 2014)\n18. Graves, A., Mohamed, A.R., Hinton, G.E.: Speech recognition with deep recurrent neural\nnetworks. In: IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). pp. 6645–6649. IEEE (2013)\n19. Graves, A., et al.: Hybrid computing using a neural network with dynamic external memory.\nNature 538(7626), 471–476 (2016). https://doi.org/10.1038/nature20101\n20. Hawkins,\nJ.,\nAhmad,\nS.:\nWhy\nneurons\nhave\nthousands\nof\nsynapses,\na\ntheory\nof sequence memory in neocortex. Frontiers in Neural Circuits 10,\n23 (2016).\nhttps://doi.org/10.3389/fncir.2016.00023\n21. Hinton, G.E., Zemel, R.S.: Autoencoders, minimum description length and Helmholtz free\nenergy. In: Cowan, J.D., Tesauro, G., Alspector, J. (eds.) Advances in Neural Information\nProcessing Systems (NIPS) 6. pp. 3–10. Morgan Kaufmann (1994)\n22. Hinton, G.E., Zemel, R.S.: Autoencoders, minimum description length and helmholtz free\nenergy. In: Proceedings of the 6th International Conference on Neural Information Process-\ning Systems. pp. 3–10. NIPS’93, Morgan Kaufmann Publishers Inc., San Francisco, CA,\nUSA (1993), http://dl.acm.org/citation.cfm?id=2987189.2987190\n23. Hornik, K.: Approximation capabilities of multilayer feedforward networks. Neural Netw.\n4(2), 251–257 (Mar 1991). https://doi.org/10.1016/0893-6080(91)90009-T\n24. Hutter, M.: Optimality of universal Bayesian prediction for general loss and alphabet. Journal\nof Machine Learning Research 4, 971–1000 (2003), (On J. Schmidhuber’s SNF grant 20-\n61847)\n25. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions Based on Algorithmic\nProbability. Springer (2005)\n26. Hutter, M.: Universal algorithmic intelligence: A mathematical top→down approach. In:\nGoertzel, B., Pennachin, C. (eds.) Artiﬁcial General Intelligence, pp. 227–290. Cognitive\nTechnologies, Springer, Berlin (2007)\n27. Jaynes, E.T.: How does the brain do plausible reasoning? In: Maximum-Entropy and\nBayesian Methods in Science and Engineering. vol. 1 (1988)\n28. Kawaguchi, K., Pack Kaelbling, L., Bengio, Y.: Generalization in Deep Learning. ArXiv\ne-prints (Oct 2017)\n29. Kim, Y., Jernite, Y., Sontag, D., Rush, A.M.: Character-aware neural language models. In:\nProceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence, February 12-17,\n2016, Phoenix, Arizona, USA. pp. 2741–2749 (2016)\n30. Koutn´ık, J., Cuccu, G., Schmidhuber, J., Gomez, F.: Evolving large-scale neural networks\nfor vision-based TORCS. In: Foundations of Digital Games. pp. 206–212. Chania, Crete,\nGR (2013)\n31. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.:\nBack-propagation applied to handwritten zip code recognition. Neural Computation 1(4),\n541–551 (1989)\n32. Marcus, G.: Deep Learning: A Critical Appraisal. ArXiv e-prints (Jan 2018)\n33. Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju, B.,\nShahrzad, H., Navruzyan, A., Duffy, N., Hodjat, B.: Evolving Deep Neural Networks. ArXiv\ne-prints (Mar 2017)\n10\nEray ¨Ozkural\n34. Minsky, M., Papert, S.: Perceptrons: An Introduction to Computational Geometry. MIT\nPress, Cambridge, MA, USA (1969)\n35. Petroski Such, F., Madhavan, V., Conti, E., Lehman, J., Stanley, K.O., Clune, J.: Deep Neu-\nroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural\nNetworks for Reinforcement Learning. ArXiv e-prints (Dec 2017)\n36. Potapov, A., Rodionov, S., Potapova, V.: Real-time ga-based probabilistic programming in\napplication to robot control. In: Artiﬁcial General Intelligence - 9th International Conference,\nAGI 2016, New York, NY, USA, July 16-19, 2016, Proceedings. pp. 95–105 (2016)\n37. Quine, W.: Two dogmas of empiricism. The Philosophical Review 60, 20–43 (1951)\n38. Ranzato, M.A., Huang, F., Boureau, Y., LeCun, Y.: Unsupervised learning of invariant feature\nhierarchies with applications to object recognition. In: Proc. Computer Vision and Pattern\nRecognition Conference (CVPR’07). pp. 1–8. IEEE Press (2007)\n39. Risi, S., Togelius, J.: Neuroevolution in games: State of the art and open chal-\nlenges.\nIEEE\nTrans.\nComput.\nIntellig.\nand\nAI\nin\nGames\n9(1),\n25–41\n(2017).\nhttps://doi.org/10.1109/TCIAIG.2015.2494596\n40. Rumelhart, D.E., McClelland, J.L., PDP Research Group, C. (eds.): Parallel Distributed Pro-\ncessing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations. MIT Press,\nCambridge, MA, USA (1986)\n41. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K.,\nPascanu, R., Hadsell, R.: Progressive neural networks. arXiv preprint arXiv:1606.04671\n(2016)\n42. Sabour,\nS.,\nFrosst,\nN.,\nHinton,\nG.E.:\nDynamic\nrout-\ning\nbetween\ncapsules.\nIn:\nNIPS.\npp.\n3859–3869\n(2017),\nhttp://papers.nips.cc/paper/6975-dynamic-routing-between-capsules\n43. Santoro, A., Raposo, D., Barrett, D.G.T., Malinowski, M., Pascanu, R., Battaglia, P., Lilli-\ncrap, T.P.: A simple neural network module for relational reasoning. CoRR abs/1706.01427\n(2017)\n44. Schmidhuber, J.: Deep learning in neural networks: An overview. Neural Networks 61, 85–\n117 (2015). https://doi.org/10.1016/j.neunet.2014.09.003\n45. Schmidhuber, J.: Optimal ordered problem solver. Machine Learning 54, 211–256 (2004)\n46. Solomonoff, R.J.: Complexity-based induction systems: Comparisons and convergence the-\norems. IEEE Trans. on Information Theory IT-24(4), 422–432 (July 1978)\n47. Solomonoff, R.J.: A system for incremental learning based on algorithmic probability. In:\nProceedings of the Sixth Israeli Conference on Artiﬁcial Intelligence. pp. 515–527. Tel Aviv,\nIsrael (December 1989)\n48. Telgarsky, M.: beneﬁts of depth in neural networks. In: Proceedings of the 29th Conference\non Learning Theory, COLT 2016, New York, USA, June 23-26, 2016. pp. 1517–1539 (2016),\nhttp://jmlr.org/proceedings/papers/v49/telgarsky16.html\n49. Tishby, N., Zaslavsky, N.: Deep learning and the information bottleneck principle. In: 2015\nIEEE Information Theory Workshop, ITW 2015, Jerusalem, Israel, April 26 - May 1, 2015.\npp. 1–5 (2015). https://doi.org/10.1109/ITW.2015.7133169\n50. Wallace, C.S., Dowe, D.L.: Minimum message length and kolmogorov complexity. The\nComputer Journal 42(4), 270–283 (1999). https://doi.org/10.1093/comjnl/42.4.270\n51. Wallace, C.S., Boulton, D.M.: A information measure for classiﬁcation. Computer Journal\n11(2), 185–194 (1968)\n52. Weber, T., et al.: Imagination-Augmented Agents for Deep Reinforcement Learning. ArXiv\ne-prints (Jul 2017)\n53. Xu, D., Nair, S., Zhu, Y., Gao, J., Garg, A., Fei-Fei, L., Savarese, S.: Neural Task Program-\nming: Learning to Generalize Across Hierarchical Tasks. ArXiv e-prints (Oct 2017)\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2018-06-22",
  "updated": "2018-06-22"
}