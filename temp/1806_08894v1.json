{
  "id": "http://arxiv.org/abs/1806.08894v1",
  "title": "Deep Reinforcement Learning: An Overview",
  "authors": [
    "Seyed Sajad Mousavi",
    "Michael Schukat",
    "Enda Howley"
  ],
  "abstract": "In recent years, a specific machine learning method called deep learning has\ngained huge attraction, as it has obtained astonishing results in broad\napplications such as pattern recognition, speech recognition, computer vision,\nand natural language processing. Recent research has also been shown that deep\nlearning techniques can be combined with reinforcement learning methods to\nlearn useful representations for the problems with high dimensional raw data\ninput. This chapter reviews the recent advances in deep reinforcement learning\nwith a focus on the most used deep architectures such as autoencoders,\nconvolutional neural networks and recurrent neural networks which have\nsuccessfully been come together with the reinforcement learning framework.",
  "text": "Deep Reinforcement Learning: An Overview\nSeyed Sajad Mousavia, Michael Schukata, Enda Howleya\naThe College of Engineering and Informatics\nNational University of Ireland, Galway, Republic of Ireland\nAbstract\nIn recent years, a speciﬁc machine learning method called deep learning has gained huge at-\ntraction, as it has obtained astonishing results in broad applications such as pattern recogni-\ntion, speech recognition, computer vision, and natural language processing. Recent research\nhas also been shown that deep learning techniques can be combined with reinforcement\nlearning methods to learn useful representations for the problems with high dimensional\nraw data input. This paper reviews the recent advances in deep reinforcement learning with\na focus on the most used deep architectures such as autoencoders, convolutional neural net-\nworks and recurrent neural networks which have successfully been come together with the\nreinforcement learning framework.\nKeywords:\nReinforcement learning, Deep Reinforcement learning, Deep leaning, Neural\nnetworks, MDPs, Observable MDPs\n1. Introduction\nReinforcement Learning (RL) [17, 39] is a branch of machine learning in which an agent\nlearns from interacting with an environment. Before an agent or robot (software or hardware)\ncan select an action, it must have a good representation of its environment [18]. Thus,\nperception is one of the key problems that must be solved before the agent can decide to\nselect an optimal action to take. Representation of the environment might be given or might\nbe acquired. In reinforcement learning tasks, usually a human expert provides features of the\nenvironment based on his knowledge of the task. However, for some real world applications\nthis work should be done automatically, since automatic feature extraction will provide\nmuch more accurate. There are several solutions to deal with this challenge, such as Mont\nCarlo Tree search [41], Hierarchical Reinforcement Learning [31] and function approximation\nEmail addresses: s.mousavi1@nuigalway.ie (Seyed Sajad Mousavi),\nmichael.schukat@nuigalway.ie (Michael Schukat), schukat,ehowley@nuigalway.ie (Enda Howley)\nPublished in Proceedings of SAI Intelligent Systems Conference (IntelliSys)\narXiv:1806.08894v1  [cs.LG]  23 Jun 2018\n[39]. Hence, we are interested in the ways that can be used to represent the state of the\nenvironment.\nDeep learning attempts to model high-level abstractions in data using deep networks of\nsupervised and/or unsupervised learning algorithms, in order to learn from multiple levels\nof abstractions. It learns hierarchical representations in deep architectures for classiﬁcation.\nIn recent years, deep learning has gained huge attraction not only in academic communi-\nties (such as pattern recognition, speech recognition, computer vision and natural language\nprocessing), but it has also been used successfully in industry products by tech giants like\nGoogle (Googles translator and Image search engine), Apple (Apple’s Siri), Microsoft (Bing\nvoice search) and other companies such as Facebook and IBM. Recent research has also\nshown that deep learning techniques can be used to learn useful representations for re-\ninforcement learning problems [28, 29, 26, 8, 30]. Combining the RL and deep learning\ntechniques enables RL agent to have a good perception of its environment.\nThe aim of this study is to outline and critically review all signiﬁcant research done\nto date in the context of combining reinforcement learning algorithms and deep learning\nmethods.\nThe research will review both supervised and unsupervised deep models that\nhave been combined with RL methods for environments which might be partially observable\nMDPs or not. This study will also present recent outstanding success stories of the combined\nRL and deep learning paradigms, which led to the introduction of a novel research route\ncalled deep reinforcement learning, to overcome the challenges in learning control policies\nfrom high-dimensional raw input data in complex RL environment.\nThe rest of this paper is organized as follows. Sections 2 and 3 give a brief review of\nreinforcement learning and deep learning (focused on three commonly used deep learning\narchitectures with reinforcement learning framework), respectively. Section 4 presents the\nstate of the art techniques that combine reinforcement learning and deep learning with a\nrough categorization of the combination of deep supervised learning models with RL and the\ncombination of deep unsupervised learning models with RL. Finally, Section 5 summarizes\nthe paper.\n2. Reinforcement Learning\nReinforcement Learning [17, 39] is a branch of machine learning in which an agent learns\nfrom interacting with an environment.An RL framework allows an agent to learn from trial\nand error. The RL agent receives a reward by acting in the environment and its goal is\nlearning to select the actions that maximize the expected cumulative reward over time. In\nother words, the agent, by observing the results of those actions that it is taking in the\n2\nenvironment, tries to learn an optimal sequence of actions to execute in order to reach its\ngoal.\nA reinforcement learning agent can be modelled as a Markov decision process (MDP). If\nthe states and action spaces are ﬁnite, then the problem is called a ﬁnite MDP. Finite MDPs\nare very important for RL problems and much of literatures have assumed the environment\nis a ﬁnite MDP in their works.\nThe way an RL agent acts in the ﬁnite MDP framework as follows: The learning agent\ninteracts with the environment by executing actions and receiving observations and rewards.\nAt each time step t, which ranges over a set of discrete time intervals, the agent select an\naction a from a set of legal actions A = 1, 2, . . . , k at state st ∈S, where S is the set\nof possible states. Action selection is based on a policy. The policy is a description of the\nbehaviour of the agent which tells the agent which actions should be selected for each possible\nstate. As a result of each action, the agent receives a scalar reward rt ∈R, and observes next\nstate st+1 ∈S at one step time later. The probability of each possible next state st+1 comes\nfrom a transition distribution which is P(st+1|st, at); st+1, st ∈S, at ∈A(st). Similarity, the\nprobability of each possible reward rt comes from a reward distribution P(rt|st, at); st ∈\nS, at ∈A(st). Hence, the expected scaler reward received, rt , by executing action a in\ncurrent state s is calculated based on EP(rt|st,at)(rt|st = s, at = a).\nThe aim of the learning agent is to learn an optimal policy π, which deﬁnes the probability\nof selecting action a in state s, so that with following the policy the sum of the discounted\nrewards over time is maximized. The expected discounted return R at time t is deﬁned as\nfollows:\nRt = E[rt + γrt+1 + γ2rt+2 + . . .] = E[\n∞\nX\nk=0\nγkrt+k],\n(1)\nWhere E[.] expectation with respect to the reward distribution and 0 < γ < 1 is called\nthe discount factor. With regard to the transition probabilities and the expected discounted\nimmediate rewards, which are the essential elements for specifying dynamics of a ﬁnite MDP,\naction-value function Qπ(s, a) is deﬁned as follows:\nQπ(s, a) = Eπ[Rt|st = s, at = a] = Eπ[\n∞\nX\nk=0\nγkrt+k|st = s, at = a],\n(2)\nThe action-value function Aπ(s, a) for an agent is the expected return achievable by\nstarting from state s, s ∈S, and performing action a, a ∈Aand then following policy π,\nwhere π is a mapping from states to actions or distributions over actions.\nWith unfolding the equation 2 it is clear that it satisﬁes a recursive property, so that the\nfollowing iterative update can be used for the estimation of action-value function:\n3\nQπ\ni+1(s, a) = Eπ[rt + γ\n∞\nX\nk=0\nγkrt+k+1|st = s, at = a]\n= Eπ[rt + γQπ\ni(st+1 = s′, at+1 = a′)|st = s, at = a],\n(3)\nFor all s, s′ ∈S and a, a′ ∈A, in Eq. 3, both states a relationship between the value of\nan action in a state and the values of its next actions which can be performed It also cites\nthe way of estimating the value based on its subsequent ones.\nReinforcement learning agent wants to ﬁnd a policy which achieves the greatest future\nreward in the course of its execution. Hence, it must learn an optimal policy π∗, a policy\nwhich is resulted to an expected value greater than or equal of following other policies for all\nstates, and as a result, an optimal state-value function Q∗(s, a). In particular, an iterative\nupdate for estimating the optimal state-value function is deﬁned as follows:\nQi+1(s, a) = Eπ[rt + γmaxa′Qi(s′, a′)|s, a],\n(4)\nWhere it is implicit that s, s′ ∈S and a, a′ ∈A. The iteration converges to the optimal\naction-value function, Q∗as i →∞and called value iteration algorithm [39].\nIn many real problems, number of states and actions are very large and use of classical\nsolution (state-action table to store the values of state-action pairs) is impractical. On way\nto deal with is use of function approximator as estimator of action-value function. The\napproximate value function is parameterized Q(s, a; θ) with parameter vector θ. Usually\ngradient-descent methods are used to learn parameters by trying to minimize the following\nloss function of mean-squared error in Q-values:\nL(θ) = Eπ[(r + γmaxa′Q(s′, a′; θ) −Q(s, a; θ))2],\n(5)\nWhere r + γmaxa′Q(s′, a′; θ) is the target value. Diﬀerentiation of the loss function with\nrespect to its parameters θ lead to the following gradient:\n∂L(θ)\n∂θ\n= E[(r + γmaxa′Q(s′, a′; θ) −Q(s, a; θ))∂Q(s, a; θ)\n∂θ\n,\n(6)\nThis is the way the gradient-based methods are applied. Typically, the gradient above\nis optimised by stochastic gradient descent method. The approximate function can be a\nlinear function or a non-linear function (for example a neural network) of the parameters\nθ. Until recently, the majority of work in reinforcement learning utilized linear function\napproximatiors because the convergence guarantees that they provide. More recently, by\n4\nalleviating the convergence problems, not only typical neural networks, for example multi-\nlayer perceptrons (MLP), have been common to use as function approximators for large\nreinforcement learning tasks, but deep neural networks such as convolutional neural networks\nand recurrent neural networks is used [34, 32, 29].\n3. Deep Learning\nObtaining a good performance of a machine learning technique is highly dependent on\nhaving good representation of input data. Hence, the pre-processing of the data (i.e., feature\nlearning) is a critical step in the process of creating the machines which are learned through\nthe observation of the data. The feature engineering process is a way to take advantage of\nthe knowledge of the domain experts in order to extract hand-crafted features and to reduce\ndimension of features of the input data. Eﬀectiveness of the shallow learning models such as\nsupport vector machines (SVMs) and logistic regression are dependent on feature learning.\nThis process is important but very time consuming and diﬃcult to do. It would be better\nto have algorithms that facilitate the problem. Deep learning techniques are one the best\nsolutions to deal with high dimension data and extract discriminative information from the\ndata. Deep learning algorithms have the capability of automating feature extraction (the\nextraction of representations) from the data. The representation are leant through the data\nare fed directly into deep nets without human knowledge (i.e., automated feature extraction).\nThis key aspect of deep learning architectures is led to progress towards those algorithms\nthat are the goal Artiﬁcial Intelligence (AI), understating the world around independent of\nexpert knowledge and interference [3].\nIn summary, deep learning attempts to model high-level abstractions in data using deep\nnetworks of supervised and/or unsupervised learning algorithms, in order to learn from\nmultiple levels of abstractions. It learns hierarchical representations in deep architectures\nfor diﬀerent tasks such as classiﬁcation.\nDeep learning models contain multiple layers of representations. Indeed, it is a stack of\nbuilding blocks such as autoencoders, Restricted Boltzmann Machines (RBMs) and convo-\nlutional layers. During training, the raw data is fed into a network consisting of multiple\nlayers. The output of the each layer which is nonlinear feature transformations, is used\nas inputs to the next layers of the deep network. The output representation of the ﬁnal\nlayer can be used for constricting classiﬁers or those applications which can have the better\neﬃciency and performance with abstract representation of the data in a hierarchical manner\nas inputs. Each layer by applying a nonlinear transformation on its input try to learn and\nextract underlying explanatory factors. Consequently, this process is led to learn a hierarchy\n5\nof abstract representations. For example, using deep leaning algorithm in image processing\napplications, the ﬁrst layer is provided by the image pixels which can lead to learn the edges\nof diﬀerent object in image. The second layer uses the representations provided by the ﬁrst\nlayer to lean complex features such as object parts (combination of edges). The third layer\ncomposes object parts (more complex features) to ﬁgure out object models. The example\nshows the hierarchical learning power of the abstracted representations by a deep learning\nalgorithm are able to recognize objects in the image. For this reason, the deep learning\napproach can be considered as a kind of representation learning algorithms [3].\nThe use of deep neural networks allow the deep learning methods to be very powerful\ntools in solving real problem. However, learning the parameters (deep nets with many hidden\nlayers are led to have millions of parameters to learn) in a deep architecture is a diﬃcult\noptimization task which imposes very high computational complexity [25]. Fortunately, with\nemergent of advanced parallel processing technologies like GPU this problem has alleviated\nsomewhat.\nIn the following sections, we will introduce a brief overview of three kind of deep neu-\nral networks which recently have successfully been had the most use in combination with\nRL. These deep architectures are included autoencoders (stacked [denoising] autoencoders),\nconvolutional neural networks (CNNs) and recurrent neural networks (RNNs).\n3.1. Autoencoder\nThe deep autoencoders are one of the most notable works in unsupervised deep feature\nlearning. They are a type of artiﬁcial neural networks which try to learn a representation\nof original data. An autoencoder, autoassociator or Diabolo network [6] is trained to learn\neﬀective encoding of input date so as to the input can be reconstructed from the encoding.\nIn fact, the target output of the network is the same of the input. It typically has three\nlayer: input layer, used for input feature vectors; hidden layer, used for representing mapped\nfeatures, and output layer which use to represent reconstructed input. Like other neural\nnetworks, the backpropagation methods are used to learn parameters, typically the stochastic\ngradient descent method. If the architecture of the autoencoder only contains one linear\nhidden layer and the mean squared error criterion, as the loss function, is used to train the\nnetwork, autoencoder will performs like principal components analysis (PCA) method and\nwill learn the ﬁrst k principle components of the data [6]. In order to get more beneﬁts of\nthe autoencoder rather than dimensionality reduction method, non-linear hidden units are\nused in the hidden layer.\nTo achieve more expressive power of autoencoders, stacked auto-encoders are constructed\nby stacking up autoencoders in tandem [4]. The output of each autoencoder is connected to\n6\nFigure 1: Architecture of a typical convolutional neural network which illustrates how the convolution and\nthe subsampling layers are connected together and are fallowed by fully connected layers to construct a\nCNN, adapted from [25].\nthe input of the next autoencoder. There is also a variant of autoencoder called demonising\nautoencoder [42]. The denoising autoencoder minimizes the reconstruction error of corrupted\nversions of the input (random noises are added to input data) and tries to recover the original\ninput data, i.e., without distortions. There are two key ideas behind of this approach. First,\nthe use of denoising will lead to a higher level representations which are more robust and\nstable when there is the noises in input data. Second, the use of the denoising task will force\nto extract those features which have more eﬀect on useful structure of the input distribution\n[43].\n3.2. Convolutional Neural Networks\nConvolutional Neural Networks (CNNs) are categorised in the class of supervised deep\nfeature learning models. Perhaps one of the ﬁrst research on CNNs is work done by LeCun\net al. [24]. They used CNNs to recognise handwritten characters. By advances in power\nof computing devises they were able to apply CNNs on other applications such as object\nrecognition and detection in image and speech recognition and time series [23].\nConvolutional networks are constructed of many layers and connections aim to learn\nhierarchical feature representation. To be invariant on some degree of translational and\ndistortional of input feature vectors, three main strategies are applied, local receptive ﬁelds,\nshared weights and spatial or temporal subsampling [25].\nFigure 1 shows a basic architecture of the CNNs. The ﬁrst two layers of the CNNs are\nconvolutional and subsampling layers. Convolutional layer performs convolution operation\nto create feature maps. It uses local receptive ﬁelds (small ﬁlter sizes) and shared weights\n(ﬁlter maps with equal size,) to bring distortion invariance attribute. The output of the con-\nvolution operation is then passed through a nonlinear activation function which is followed\n7\nby a subsampling layer. Subsampling layer performs local averaging (or max-pooling [36])\nthat reduces dimensionality of proceeding feature maps while keeping distortion invariance.\nSequence of convolutional and subsampling layers can be used in tandem in the architectures\nof CNNs of a speciﬁc task. The output of the ﬁnal subsampling layer is fed to some fully\nconnected layer for classiﬁcation or recognition tasks. An interested reader is referred to [25]\npaper for more details on CNNs.\nHowever, in order to CNNs to be applicable in real applications, especially in applications\nwith high dimensional input data such as image and speech processing, and achieve reason-\nable performance in comparison with shallow learner methods, this kind of deep networks\nmust be provided with the large amount of data. In addition, with so many parameters\nto train of the deep architecture, the high performance computing power is also required.\nUntil recently they had not been widely used because of the mentioned requirements. These\nproblems can now be dealt easily with emerging highly parallel Graphical Processing Units\n(GPU) and becoming available large data sets. For instance, in the area of image and vision\nresearch, with presenting of ImageNet database [10], a large-scale image database which\nconsists of millions labelled high resolution images in thousands of categories, and paral-\nlel GPUs, deep CNN architectures could outperform state of the art algorithms in pattern\nrecognition.\n3.3. Recurrent Neural Networks\nAnother deep supervised feature learning (as well as unsupervised) algorithm which\nis used for sequential information, where input data are depended to each other in the\nway they are coming out (data stream) or have located (words in a sentence), is recurrent\nneural networks (RNNs).\nUnlike feedforward neural networks (FNNs), recurrent neural\nnetworks (RNNs) have feedback connections which allows them to have internal states. This\nmeans that they have a memory which can keep information about previous inputs, enabling\nthem to be useful for those applications such as speech recognition which has temporal and\nsequential data [11].\nTo learn long-term dependencies in sequential data using vanilla RNN architecture gives\nrise gradient vanishing (the magnitude of the error gradients vanish exponentially during\ntraining, which makes it impossible for the RNN to learn correlation between temporally\nlong-term events) or gradient explosion (a large increase in the magnitude of the gradients\nduring training, where long-term components exponentially grow and dominate the gradients\nof short term ones) problems [5]. Other types of RNNs such as long short term memory\n(LSTMs) [16] was introduced to deal with these problems. LSTMs are able to learn very\nlong-term dependencies.\nThey could outperform alternative RNNs and Hidden Markov\n8\nModel (HMM) approaches which were state of the art in sequence learning [11].\n4. Deep Supervised and Unsupervised Learning Models for Reinforcement Learn-\ning\nThe most well-known reinforcement learning algorithm which uses neural networks (but\nno deep nets, i.e., there is only one hidden layer) is the world-class RL backgammon player\nnamed TD-Gammon, which gained a score equal to human champions by playing against\nitself [40]. TD-Gammon uses TD (lambda) algorithm [39] to train a shallow neural net to\nlearn to play the game of backgammon. However, later attempts to use the same method\nfor other games such as chess, Go and checkers were not successful.\nWith riving interest in research works on deep learning in the middle of the 2000s decade,\nthe promise to use neural networks as function approximator both for the state value func-\ntion V (s) and the action-value function Q(s, a) in visual based RL tasks came back. In the\nfollowing sections we introduce those works that have used the deep neural nets in combina-\ntion with reinforcement learning framework to improve the performance of learning control\npolicies with emphasizing on those that are fed with raw input data.\n4.1. Combination of RL Techniques with Supervised Learning Approaches of Deep Neural\nNetworks\nThe work in [34] has proposed a model free method called Neural Fitted Q learning\n(NFQ). NFQ update the weights of a multilayer perceptron by RPROP algorithm [35],a\nbatch learning method for training the neural networks which is very fast in comparison\nwith other supervised leaning methods, for regressing the value function where updating is\naccomplished oﬄine. The update is based on an entire set of transition experiences which\nhas triple form (s, a, s) where s is the current state, a is the selected action and s is the\nnext state, which is the result of taking action a. Since updating is performed oﬄine, the\ntransition experiences are collected before. Indeed, they are acquired by interacting with a\nreal or simulated environment. The proposed method has two steps (1) collecting learning\nset and (2) doing a batch update for training a multilayer perceptron by RPROP algorithm.\nSince it use a batch update the computational complexity of doing this update in each\niteration is corresponding with the size of training set [29].\nSince the most of the resent approaches that combine deep learning and RL use challeng-\ning environments introduced the Arcade Learning Environment (ALE) [2], we ﬁrst introduce\nALE. ALE provides an environment that emulate the Atari 2600 Games. Atari 2600 presents\na very challenging environment for reinforcement learning that has a high dimensional visual\n9\ninput (210×160 RGB video at 60 Hz) which is a partial observable observation. It provides\na range of interesting games that the proposed methods can be tested, where the agent use\nthe methods for playing the games.\nMore recently, researchers in DeepMind technologies have developed an approach called\nDeep Q learning Network (DQN) [29] which beneﬁts from advantages of deep learning for\nabstract representation in learning optimal policy, i.e. selecting actions in such a way that\nmaximize the expected value of the cumulative sum of rewards. It is an extension of the\nprevious work Neural Fitted Q-Learning (NFQ) [34]. DQN combine a deep convolutional\nneural network with the simplest reinforcement learning method (Q-learning) to play several\nAtari 2600 computer games only by watching the screen.\nCombing model-free reinforcement learning algorithms such as mere Q-learning algorithm\nwith neural networks causes some stability issues and makes to be diverged. There are two\nmain reasons for these issues, e.g., (1) subsequent states in RL tasks are much correlated,\n(2) the policy is changing frequently, it is because of slight changes in Q-values. DQN for\ndealing with these issues provides some solutions. For the correlated states issue, it utilizes\nthe approach introduced in [27] named experience replay. In the process of learning, DQN\nstore agent’s experience (st, at, rt, rt+1) at each time step into a date set D, where st, at\nand rt, respectively the state, selected action and received reward at time step t and st+1\nis state at the next time step. For updating Q-values, it uses stochastic minibatch updates\nwith uniformly random sampling from experience replay memory (previous transitions) at\ntraining time. This work break strong correlations between consecutive samples, and for\ninstability in the policy, the network is trained with a target Q-network to obtain consistent\nQ-learning targets by ﬁxing weight parameters used in Q-learning target and updating them\nperiodically.\nUntil recently the proposed method achieved the best real time agents. In some games\nits strategy outperformed the human player and achieved state of the art performance on\nmany Atari games with the same network architecture or hyperparameters. Several factors\nhave been involved for getting the signiﬁcant results, while the previous works had not\nconsidered [8]. First, advances in computing power, especially highly paralleled Graphical\nProcessing Units (GPU) technology which has enabled training the deep neural networks\nwith thousands of weight parameters. Second, DQN has used a large deep CNN which it\nhas been made better representation learning. Third, DQN has used experience replay for\nthe correlated states problem.\nHowever, using deep neural networks need suﬃcient data to be fed into network to\nlearn better representations and as a result getting good performance.\nHence, applying\nthis approach in real environment such as robotics is very challenging and diﬃcult since\n10\nperforming a large number of episodes to collect samples is source consuming and even not\npossible.\nThe work done by Guo et al. [13] has shown better results in comparison with DQN’s\nperformance. It uses the oﬄine Mont Carlo tree search planning to provide training data for\na convolutional neural network. Indeed, they have developed some methods which beneﬁt\nfrom deep learning nets for abstract representation and model-free RL by utilizing UCT-\nbased planning method [19] to generate input data for the CNN.\nLike DQN this work also uses ALE framework as testbed for the proposed methods. It\noutperforms DQN in several games of Atari 2600. In order to achieve these results UCT\nneeds signiﬁcant time between actions [13]. In addition, planning-based approaches are slow\nfor real time play.\nThe goal of the research in [26] is training the perception and control systems jointly\nrather than each phase is trained separately, in order to get better performance. To reach\nthis purpose, learning a policy which do both the perception and the control jointly, they\nutilized deep convolutional neural networks. The CNNs get raw images from a PR2 robots\ncamera and output the policy. The policy is a conditional distribution of a Gaussian which\ndetermines a probability distribution over actions with regard to given the observation of\nenvironment. The authors evaluated their method with comparing to other policy research\napproaches on various tasks such as hanging a coat hanger on a clothes rack, stacking Lego\nblocks on a ﬁxed base, screwing caps onto pill bottles, etc. and has shown signiﬁcant results\n(for more information see [26]).\nDeveloping an artiﬁcial agent which can play in variety of games is still a big challenge\nin AI. One type of these challenging games is board games such as the two-player game of\nGo in which the goal is to surround more territory than the opponent. In [12], the authors\nhave developed a method to play the Go game on small boards with combining the beneﬁts\nof two approaches multi-dimensional recurrent neural networks (MDRNNs) and long short-\nterm memory (LSTMs). The proposed method beneﬁt from the feature of MDRNN where\nit can use provided information of the two space dimensions of the game board. Moreover,\nby integrating the LSTM in MDRNN, the vanishing gradient problem for RNNs [15] has\nsolved as well.\nTo train the networks they used policy gradients with parameter-based\nexploration method [38], a model-free reinforcement learning for POMDPs problems which\nhas outperformed Evolution Strategies [7]. Notably, [9] as well as have used CNNs for playing\nGo game which input data was raw visual pixels. Their proposed methods have resulted\nstate of the art performance to the problem of predicting the moves made by expert Go\nplayers. However, combining CNNs and RL framework to deal with Go game might lead to\nbetter improvement.\n11\nSimilar to the past works in visual based RL domain, the research [20] receives high di-\nmensional visual inputs and learns optimal policies using end-to-end reinforcement learning.\nIt utilized a compressed recurrent neural networks which uses evolutionary algorithms for\nevolving the neural network as the action-value function approximate. It successfully used\nto two challenging tasks such as the TORCS race car driving with high-dimensional visual\ndata streams and the visual Octopus Arm task.\nThe problem of video prediction, is another domain that combing deep learning and\nRL approaches can be an optimal solution. One notable work is [32]. It introduces two\ndeep neural networks architectures which integrate convolutional neural networks, recurrent\nneural networks and RL in order to predict action-conditional frames, the next frames in\nvideo depend on preformed actions in previous time step, the work done by Guo et al. [13]\nused slow UCT to predict future frames. They have shown that using their architectures in\nsome Atari games can extract the features both spatial and temporal and generate 100-step\naction-conditional future frames without suﬀering of being diverged.\n4.2. Combination of RL Techniques with Unsupervised Learning Approaches of Deep Neural\nNetworks\nThere have been several attempts to learn representation which use the unsupervised\nlearning techniques of deep neural networks in combination with RL. In the following section,\nwe will address some unsupervised deep neural networks which are used in order to learn\ncompact low-dimensional feature space of the RL task. Solving visual-based reinforcement\nlearning tasks is usually divided into two steps. The ﬁrst is, mapping high-dimensional input\ndate into a low- dimensional representation (which here, our focus is using the unsupervised\nlearning methods of deep architectures). The second is, applying an approximation technique\nto the learned compacted feature space for approximating the Q-value function or the control\npolicies.\nThe studied research by Lange and Riedmiller [21] to handle high-dimensional visual state\nspaces problem in RL task, presented the Deep Fitted Q-iteration (DFQ) algorithm in which\nunsupervised training of deep auto-encoder networks are integrated with RL methods. DFQ\nalgorithm at the ﬁrst stage uses a deep auto-encoder to learn a low dimensional presentation\nof the input state (image) and then at the second stage, applies NFQ algorithm [34], a batch-\nmode supervised learning, to estimate the Q-value function. DFQ algorithm was successfully\napplied to some continuous grid-world tasks which have had visual input.\nDeep Fitted Q-iteration algorithm has also been successfully used to learn the control\npolicy for two control tasks, a pole balancing and a racing slot car, respectively by Mattner\net al. [28] and Lange et al. [22]. They have followed two steps, (1) raw visual input which\n12\nwere captured by a digital camera, is fed into an auto-encoder network in order to reducing\nand condensing the input state space, and (2) to estimate the value function, in former\na kernel based function approximator [33] has been applied and in latter, the ClusterRL\napproach [21] has been utilized. However, any function approximation methods can be used\nfor accomplishing this step.\n4.3. Deep RL for Partially Observable MDPs (POMDPs) Environments\nIn most real world applications the Markov assumption is not feasible, since real states\nare only partially observable and using only the current states for decision making might\nnot led to reach the optimal strategy. Unlike Markov decision processes (MDPs), POMDPs\nassume the input states of the RL agent are not complete and cannot contain all necessary\ninformation to select the optimal next action. One way which can facilitate this inconsistency\nis memorizing the history of the past observations, for this purpose [29] have stacked a history\nof the last 4 frames that the agent has recently seen in their experiments when they have\nused Atari 2600 games as the testbed.\nRecurrent networks are common solutions when the arbitrary background of the past\nevents in a system is in need. RNNs are used as function approximators where they can\nprovide condensed feature spaces of the past events which have been seen thus far. Indeed,\nthis combination, POMDP RL and DL RNNs, allows the agent to memorize important\nprevious observations [37]. One the ﬁrst use of recurrent networks with RL is the research\ndone by Bakker et al. [1] in robotics domain which enabled the RL robot with the memory\ncapability through a Long Short Term Memory (LSTM) recurrent neural network, a special\nkind of the RNNs that can learn long term dependencies of states already seen [16].\nThe proposed method by Hausknecht and Stone [14] has adjusted Deep Q-network so as\nto it can be used in those environments which the observations may be noisy and incomplete\n(e.g., POMDPs environments). For this purpose they have integrated a LSTM with a DQN.\nTo test their works they introduced the Flickering Pong POMDP which is a modiﬁed\nversion of the game of Pong (one of the Atari 2600 games). The game screen is considered\nfully obscured with probability p = 0.5 to bring the condition of the POMDP for Pong\ngame. In Flickering Pong environment, they have shown that a recurrent deep Q-network\n(DRQN) with a single frame as input, has better performance in comparison with DQN with\n4 frames and 10 frames as inputs. They have also proved that DRQN is cable to generalize its\npolicies to MDP environment, where the states are completely observable. However, it had\nnot signiﬁcant superiority in other Atari games as benchmark, when it has been compared\nto DQN with the history of frames [14].\n13\n5. Conclusions and Future Work in Deep Reinforcement Learning\nDeep learning models with great power of automatically extracting complex data rep-\nresentations from high-dimensional input data could outperform other state of the art of\ntraditional machine learning methods. A major challenge in reinforcement learning is to\nlearn optimal control policies in problems with raw visual input. Hierarchical feature ex-\ntraction and learning abstracted representations of deep architectures, not only made the\ndeep learning become a valuable tool for classiﬁcation, but it has made it to be a great\nsolution for the mentioned challenge in RL tasks as well.\nIn this paper, the focus was the role of deep neural networks as a solution for deal-\ning with high-dimensional data input issue in reinforcement learning problems. We have\npresented recent advances in combing reinforcement learning framework and deep leaning\nmodels for both deep supervised and unsupervised learning networks. In particular, the deep\narchitectures that have been most used in combination with RL such as deep convolutional\nnetworks, deep autoencoders and deep recurrent networks. In addition, appropriate deep\nnetworks for the problems with partially observable MDPs (POMDPs) environment, have\nbeen discussed.\nDespite of the signiﬁcant works done to data in combining RL and DL, research on\ndeep reinforcement learning is at its ﬁrst steps and there are still many unexplored aspects\nof this combination.\nAlso, their challenges in real application such as robotics, are yet\nunsolved and need more exploration to be done. More work is necessary on investigating\ndeep architectures both for end to end leaning, which performs a direct approach to learn\nnon-linear control policies, and deep state representation, which does dimension reduction\nto present low dimensional representations then try to approximate Q-values. Especially,\ndeveloping those mechanisms which make the end to end learning can be practical in real\nworld application, those which doing a large number of actions is impossible.\nFurthermore, an open problem that has not yet been addressed is how deep architectures\ncan help deep reinforcement learning models to transfer knowledge (transfer learning). In-\ndeed, how to use learned features by the deep networks for diﬀerent tasks, without changing\nthe network architectures.\nReferences\n[1] Bakker, B., Zhumatiy, V., Gruener, G., Schmidhuber, J., 2003. A robot that reinforcement-learns to\nidentify and memorize important previous observations. In: Intelligent Robots and Systems, 2003.(IROS\n2003). Proceedings. 2003 IEEE/RSJ International Conference on. Vol. 1. IEEE, pp. 430–435.\n[2] Bellemare, M. G., Naddaf, Y., Veness, J., Bowling, M., 2013. The arcade learning environment: An\nevaluation platform for general agents. J. Artif. Intell. Res.(JAIR) 47, 253–279.\n14\n[3] Bengio, Y., Courville, A., Vincent, P., 2013. Representation learning: A review and new perspectives.\nIEEE transactions on pattern analysis and machine intelligence 35 (8), 1798–1828.\n[4] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., 2007. Greedy layer-wise training of deep networks.\nIn: Advances in neural information processing systems. pp. 153–160.\n[5] Bengio, Y., Simard, P., Frasconi, P., 1994. Learning long-term dependencies with gradient descent is\ndiﬃcult. IEEE transactions on neural networks 5 (2), 157–166.\n[6] Bengio, Y., et al., 2009. Learning deep architectures for ai. Foundations and trends R⃝in Machine\nLearning 2 (1), 1–127.\n[7] Beyer, H.-G., Schwefel, H.-P., 2002. Evolution strategies–a comprehensive introduction. Natural com-\nputing 1 (1), 3–52.\n[8] B¨ohmer, W., Springenberg, J. T., Boedecker, J., Riedmiller, M., Obermayer, K., 2015. Autonomous\nlearning of state representations for control: An emerging ﬁeld aims to autonomously learn state rep-\nresentations for reinforcement learning agents from their real-world sensor observations. KI-K¨unstliche\nIntelligenz 29 (4), 353–362.\n[9] Clark, C., Storkey, A., 2015. Training deep convolutional neural networks to play go. In: International\nConference on Machine Learning. pp. 1766–1774.\n[10] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., 2009. Imagenet: A large-scale hierarchical\nimage database. In: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference\non. IEEE, pp. 248–255.\n[11] Deng, L., Hinton, G., Kingsbury, B., 2013. New types of deep neural network learning for speech recog-\nnition and related applications: An overview. In: Acoustics, Speech and Signal Processing (ICASSP),\n2013 IEEE International Conference on. IEEE, pp. 8599–8603.\n[12] Gr¨uttner, M., Sehnke, F., Schaul, T., Schmidhuber, J., 2010. Multi-dimensional deep memory atari-\ngo players for parameter exploring policy gradients. In: International Conference on Artiﬁcial Neural\nNetworks. Springer, pp. 114–123.\n[13] Guo, X., Singh, S., Lee, H., Lewis, R. L., Wang, X., 2014. Deep learning for real-time atari game play\nusing oﬄine monte-carlo tree search planning. In: Advances in neural information processing systems.\npp. 3338–3346.\n[14] Hausknecht, M., Stone, P., 2015. Deep recurrent q-learning for partially observable mdps. CoRR,\nabs/1507.06527.\n[15] Hochreiter, S., Bengio, Y., Frasconi, P., Schmidhuber, J., et al., 2001. Gradient ﬂow in recurrent nets:\nthe diﬃculty of learning long-term dependencies.\n[16] Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural computation 9 (8), 1735–1780.\n[17] Kaelbling, L. P., Littman, M. L., Moore, A. W., 1996. Reinforcement learning: A survey. Journal of\nartiﬁcial intelligence research 4, 237–285.\n[18] Kober, J., Bagnell, J. A., Peters, J., 2013. Reinforcement learning in robotics: A survey. The Interna-\ntional Journal of Robotics Research 32 (11), 1238–1274.\n[19] Kocsis, L., Szepesv´ari, C., 2006. Bandit based monte-carlo planning. In: European conference on\nmachine learning. Springer, pp. 282–293.\n[20] Koutn´ık, J., Cuccu, G., Schmidhuber, J., Gomez, F., 2013. Evolving large-scale neural networks for\nvision-based reinforcement learning. In: Proceedings of the 15th annual conference on Genetic and\nevolutionary computation. ACM, pp. 1061–1068.\n[21] Lange, S., Riedmiller, M., 2010. Deep auto-encoder neural networks in reinforcement learning. In:\n15\nNeural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, pp. 1–8.\n[22] Lange, S., Riedmiller, M., Voigtlander, A., 2012. Autonomous reinforcement learning on raw visual\ninput data in a real world application. In: Neural Networks (IJCNN), The 2012 International Joint\nConference on. IEEE, pp. 1–8.\n[23] LeCun, Y., Bengio, Y., et al., 1995. Convolutional networks for images, speech, and time series. The\nhandbook of brain theory and neural networks 3361 (10), 1995.\n[24] LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., Jackel, L. D., 1989.\nBackpropagation applied to handwritten zip code recognition. Neural computation 1 (4), 541–551.\n[25] LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P., 1998. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE 86 (11), 2278–2324.\n[26] Levine, S., Finn, C., Darrell, T., Abbeel, P., 2016. End-to-end training of deep visuomotor policies.\nThe Journal of Machine Learning Research 17 (1), 1334–1373.\n[27] Lin, L.-J., 1993. Reinforcement learning for robots using neural networks. Ph.D. thesis, Fujitsu Labo-\nratories Ltd.\n[28] Mattner, J., Lange, S., Riedmiller, M., 2012. Learn to swing up and balance a real pole based on\nraw visual input data. In: International Conference on Neural Information Processing. Springer, pp.\n126–133.\n[29] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M., 2013.\nPlaying atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n[30] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Ried-\nmiller, M., Fidjeland, A. K., Ostrovski, G., et al., 2015. Human-level control through deep reinforcement\nlearning. Nature 518 (7540), 529.\n[31] Mousavi, S. S., Ghazanfari, B., Mozayani, N., Jahed-Motlagh, M. R., 2014. Automatic abstraction\ncontroller in reinforcement learning agent via automata. Applied Soft Computing 25, 118–128.\n[32] Oh, J., Guo, X., Lee, H., Lewis, R. L., Singh, S., 2015. Action-conditional video prediction using deep\nnetworks in atari games. In: Advances in Neural Information Processing Systems. pp. 2863–2871.\n[33] Ormoneit, D., Sen, ´S., 2002. Kernel-based reinforcement learning. Machine learning 49 (2-3), 161–178.\n[34] Riedmiller, M., 2005. Neural ﬁtted q iteration–ﬁrst experiences with a data eﬃcient neural reinforcement\nlearning method. In: European Conference on Machine Learning. Springer, pp. 317–328.\n[35] Riedmiller, M., Braun, H., 1993. A direct adaptive method for faster backpropagation learning: The\nrprop algorithm. In: Neural Networks, 1993., IEEE International Conference on. IEEE, pp. 586–591.\n[36] Scherer, D., M¨uller, A., Behnke, S., 2010. Evaluation of pooling operations in convolutional architectures\nfor object recognition. In: International conference on artiﬁcial neural networks. Springer, pp. 92–101.\n[37] Schmidhuber, J., 2015. Deep learning in neural networks: An overview. Neural networks 61, 85–117.\n[38] Sehnke, F., Osendorfer, C., R¨uckstieß, T., Graves, A., Peters, J., Schmidhuber, J., 2010. Parameter-\nexploring policy gradients. Neural Networks 23 (4), 551–559.\n[39] Sutton, R. S., Barto, A. G., 1998. Introduction to reinforcement learning. Vol. 135. MIT press Cam-\nbridge.\n[40] Tesauro, G., 1995. Td-gammon: A self-teaching backgammon program. In: Applications of Neural\nNetworks. Springer, pp. 267–285.\n[41] Vien, N. A., Ertel, W., Dang, V.-H., Chung, T., 2013. Monte-carlo tree search for bayesian reinforcement\nlearning. Applied intelligence 39 (2), 345–353.\n[42] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A., 2008. Extracting and composing robust\n16\nfeatures with denoising autoencoders. In: Proceedings of the 25th international conference on Machine\nlearning. ACM, pp. 1096–1103.\n[43] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A., 2010. Stacked denoising autoen-\ncoders: Learning useful representations in a deep network with a local denoising criterion. Journal of\nMachine Learning Research 11 (Dec), 3371–3408.\n17\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-06-23",
  "updated": "2018-06-23"
}