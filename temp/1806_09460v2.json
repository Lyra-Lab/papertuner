{
  "id": "http://arxiv.org/abs/1806.09460v2",
  "title": "A Tour of Reinforcement Learning: The View from Continuous Control",
  "authors": [
    "Benjamin Recht"
  ],
  "abstract": "This manuscript surveys reinforcement learning from the perspective of\noptimization and control with a focus on continuous control applications. It\nsurveys the general formulation, terminology, and typical experimental\nimplementations of reinforcement learning and reviews competing solution\nparadigms. In order to compare the relative merits of various techniques, this\nsurvey presents a case study of the Linear Quadratic Regulator (LQR) with\nunknown dynamics, perhaps the simplest and best-studied problem in optimal\ncontrol. The manuscript describes how merging techniques from learning theory\nand control can provide non-asymptotic characterizations of LQR performance and\nshows that these characterizations tend to match experimental behavior. In\nturn, when revisiting more complex applications, many of the observed phenomena\nin LQR persist. In particular, theory and experiment demonstrate the role and\nimportance of models and the cost of generality in reinforcement learning\nalgorithms. This survey concludes with a discussion of some of the challenges\nin designing learning systems that safely and reliably interact with complex\nand uncertain environments and how tools from reinforcement learning and\ncontrol might be combined to approach these challenges.",
  "text": "A Tour of Reinforcement Learning:\nThe View from Continuous Control\nBenjamin Recht\nDepartment of Electrical Engineering and Computer Sciences\nUniversity of California, Berkeley\nJune 25, 2018. Last updated: November 9, 2018.\nAbstract\nThis manuscript surveys reinforcement learning from the perspective of optimization and\ncontrol with a focus on continuous control applications. It surveys the general formulation,\nterminology, and typical experimental implementations of reinforcement learning and reviews\ncompeting solution paradigms.\nIn order to compare the relative merits of various techniques, this survey presents a case study\nof the Linear Quadratic Regulator (LQR) with unknown dynamics, perhaps the simplest and\nbest-studied problem in optimal control. The manuscript describes how merging techniques from\nlearning theory and control can provide non-asymptotic characterizations of LQR performance\nand shows that these characterizations tend to match experimental behavior. In turn, when\nrevisiting more complex applications, many of the observed phenomena in LQR persist.\nIn\nparticular, theory and experiment demonstrate the role and importance of models and the cost\nof generality in reinforcement learning algorithms.\nThis survey concludes with a discussion of some of the challenges in designing learning\nsystems that safely and reliably interact with complex and uncertain environments and how\ntools from reinforcement learning and control might be combined to approach these challenges.\n1\nIntroduction\nReinforcement learning (RL) is the subﬁeld of machine learning that studies how to use past data\nto enhance the future manipulation of a dynamical system. A control engineer might be puzzled by\nsuch a deﬁnition and interject that this is precisely the scope of control theory. That the RL and\ncontrol communities remain practically disjoint has led to the co-development of vastly diﬀerent\napproaches to the same problems. However, it should be impossible for a control engineer not to\nbe impressed by the recent successes of the RL community such as solving Go [68].\nIndeed, given this dramatic recent progress in reinforcement learning, a tremendous opportunity\nlies in deploying its data-driven systems in more demanding interactive tasks including self-driving\nvehicles, distributed sensor networks, and agile robotic systems.\nFor RL to expand into such\ntechnologies, however, the methods must be both safe and reliable—the failure of such systems has\nsevere societal and economic consequences, including the loss of human life. How can we guarantee\nthat our new data-driven automated systems are robust? These types of reliability concerns are at\nthe core of control engineering, and reinforcement learning practitioners might be able to make their\nmethods robust by applying appropriate control tools for engineering systems to match prescribed\nsafety guarantees.\n1\narXiv:1806.09460v2  [math.OC]  10 Nov 2018\nThis survey aims to provide a language for the control and reinforcement learning communities\nto begin communicating, highlighting what each can learn from the other. Controls is the theory\nof designing complex actions from well-speciﬁed models, while reinforcement learning often makes\nintricate, model-free predictions from data alone. Yet both RL and control aim to design systems\nthat use richly structured perception, perform planning and control that adequately adapt to\nenvironmental changes, and exploit safeguards when surprised by a new scenario. Understanding\nhow to properly analyze, predict, and certify such systems requires insights from current machine\nlearning practice and from the applied mathematics of optimization, statistics, and control theory.\nWith a focus on problems in continuous control, I will try to disentangle the similarities and\ndiﬀerences of methods of the complementary perspectives and present a set of challenging problems\nwhose solution will require signiﬁcant input from both sets of practitioners.\nI focus ﬁrst on casting RL problems in an optimization framework, establishing the sorts of\nmethodological tools brought to bear in contemporary RL. I then lay out the main solution tech-\nniques of RL including the dichotomy between the model-free and model-based methodologies.\nNext, I try to put RL and control techniques on the same footing through a case study of the\nlinear quadratic regulator (LQR) with unknown dynamics. This baseline will illuminate the var-\nious trade-oﬀs associated with techniques from RL and control. In particular, we will see that\nthe so-called “model-free” methods popular in deep reinforcement learning are considerably less\neﬀective in both theory and practice than simple model-based schemes when applied to LQR. Per-\nhaps surprisingly, I also show cases where these observations continue to hold on more challenging\nnonlinear applications. I then argue that model-free and model-based perspectives can be uniﬁed,\ncombining their relative merits. This leads to a concluding discussion of some of the challenges at\nthe interface of control and learning that must be solved before we can build robust, safe learning\nsystems that interact with an uncertain physical environment, which will surely require tools from\nboth the machine learning and control communities.\n2\nWhat is reinforcement learning?\nReinforcement learning is the study of how to use past data to enhance the future manipulation of\na dynamical system. How does this diﬀer from ordinary machine learning? The main view of this\nsurvey is of reinforcement learning as optimal control when the dynamics are unknown. Our goal\nwill be to ﬁnd a sequence of inputs that drives a dynamical system to maximize some objective\nbeginning with minimal knowledge of how the system responds to inputs.\nIn the classic optimal control problem, we begin with a dynamical system governed by the\ndiﬀerence equation xt+1 = ft(xt, ut, et) where xt is the state of the system, ut is the control action,\nand et is a random disturbance. ft is the rule that maps the current state, control action, and\ndisturbance at time t to a new state. Assume that at every time, we receive some reward R(xt, ut) for\nour current xt and ut. The goal is to maximize this reward. In terms of mathematical optimization,\nwe aim to solve the problem\nmaximize\nEet[PN\nt=0 Rt[xt, ut]]\nsubject to\nxt+1 = ft(xt, ut, et)\n(x0 given).\n(2.1)\nThat is, we aim to maximize the expected reward over N time steps with respect to the control\nsequence ut, subject to the dynamics speciﬁed by the state-transition rule ft. The expected value is\n2\nover the disturbance, and assumes that ut is to be chosen having seen only the states x0 through xt\nand previous inputs u0 through ut−1. Rt is the reward gained at each time step and is determined\nby the state and control action. Note that xt is not really a decision variable in the optimization\nproblem: it is determined entirely by the previous state, control action, and disturbance. I will refer\nto a trajectory, τt, as a sequence of states and control actions generated by a dynamical system.\nτt = (u1, . . . , ut−1, x0, . . . , xt) .\n(2.2)\nSince the dynamics are stochastic, the optimal control problem typically allows a controller to\nobserve the state before deciding upon the next action [12]. This allows a controller to continually\nmitigate uncertainty through feedback. Hence, rather than optimizing over deterministic sequences\nof controls ut, we instead optimize over policies. A control policy (or simply “a policy”) is a function,\nπ, that takes a trajectory from a dynamical system and outputs a new control action. Note that π\ngets access only to previous states and control actions.\nTo slightly lower the notational burden, I will hereon work with the time-invariant version of\nProblem (2.1), assuming that the dynamical update rule is constant over time and that the rewards\nfor state-action pairs are also constant:\nmaximize\nEet[PN\nt=0 R(xt, ut)]\nsubject to\nxt+1 = f(xt, ut, et), ut = πt(τt)\n(x0 given).\n(2.3)\nThe policies πt are the decision variables of the problem.\nLet us now directly bring machine learning into the picture. What happens when we don’t know\nthe state-transition rule f? There are a variety of commonly occurring scenarios when we might\nlack such knowledge. We may have unknown relationships between control forces and torques in a\nmechanical system. Or we could have a considerably more complicated system such as a massive\ndata center with complex heat transfer interactions between the servers and the cooling systems.\nCan we still solve Problem (2.3) well without a precise model of the dynamics? Some lines of\nwork even assume that we don’t know the reward function R, but for the purpose of this survey, it\nmakes no diﬀerence whether R is known or unknown. The important point is that we can’t solve\nthis optimization problem using standard optimization methods unless we know the dynamics. We\nmust learn something about the dynamical system and subsequently choose the best policy based\non our knowledge.\nThe main paradigm in contemporary RL is to play the following game. We decide on a policy\nπ and horizon length L. Then we pass this policy either to a simulation engine or to a real physical\nsystem and are returned a trajectory τL and a sequence of rewards {R(xt, ut)}. We want to ﬁnd a\npolicy that maximizes the reward with the fewest total number of samples computed by the oracle,\nand we are allowed to do whatever we’d like with the previously observed trajectories and reward\ninformation when computing a new policy. If we were to run m queries with horizon length L, we\nwould pay a total cost of mL. However, we are free to vary our horizon length for each experiment.\nThis is our oracle model and is called episodic reinforcement learning (See, for example Chapter 3\nof Sutton and Barto [76], Chapter 2 of Puterman [58], or Dann and Brunskill [24]). We want the\nexpected reward to be high for our derived policy, but we also need the number of oracle queries\nto be small.\nThis oracle model is considerably more complicated than those typically considered in oracle\nmodels for optimization [53]. Each episode returns a complex feedback signal of states and rewards.\n3\nWhat is the best way to tie this information together in order to improve performance? What is the\nbest way to query and probe a system to achieve high quality control with as few interventions as\npossible? Here “best” is also not clearly deﬁned. Do we decide an algorithm is best if it crosses some\nreward threshold in the fewest number of samples? Or is it best if it achieves the highest reward\ngiven a ﬁxed budget of samples? Or maybe there’s a middle ground? This oracle provides a rich\nand complex model for interacting with a system and brings with it considerably more complexity\nthan in standard stochastic optimization settings. What’s the most eﬃcient way to use all of the\ncollected data in order to improve future performance?\n2.1\nConnections to supervised learning\nThe predominant paradigm of machine learning is supervised learning or prediction. In prediction,\nthe goal is to predict the variable y from a vector of features x such that on new data you are\npredicting y from x with high accuracy. This form of machine learning includes classiﬁcation and\nregression as special cases. Most of the time when the term machine learning is used colloquially, it\nrefers to this sort of prediction. From this perspective, niche topics like semi-supervised learning [91]\nand matrix completion [34] are prediction tasks as well.\nBy contrast, there are two special variables in reinforcement learning, u and r. The goal now\nis to analyze the features x and then subsequently choose a policy that emits u so that r is large.1\nThere are an endless number of problems where this formulation is applied [14, 39, 76] from online\ndecision making in games [20, 51, 68, 79] to engagement maximization on internet platforms [19, 72].\nA key distinguishing aspect of RL is the control action u. Unlike in prediction, the practitioner\ncan vary u, which has implications both for learning (e.g., designing experiments to learn about a\ngiven system) and for control (e.g., choosing inputs to maximize reward).\nReinforcement learning is clearly more challenging than supervised learning, but, at the same\ntime, it can be considerably more valuable. Reinforcement learning provides a useful framework to\nconceptualize interaction in machine learning, and promises to help mitigate changing distributions,\ngaming, adversarial behavior, and unexpected ampliﬁcation. There is a precarious trade-oﬀthat\nmust be carefully considered: reinforcement learning demands interventions with the promise that\nthese actions will directly lead to valuable returns, but the resulting complicated feedback loops\nare hard to study in theory, and failures can have catastrophic consequences.\n3\nStrategies for solving reinforcement learning problems\nLet us now turn to a taxonomy of the varied algorithmic frameworks for reinforcement learning,\nfocused on solving Problem (2.3) when the state-transition function is unknown.\nModel-Based\nReinforcement learning ﬁts a model to previously observed data and then uses this model in some\nfashion to approximate the solution to Problem (2.3). Model-Free Reinforcement learning eschews\nthe need for a system’s model, directly seeking a map from observations to actions.\nThe role of models in reinforcement learning remains hotly debated. Model-free methods, as\ndiscussed below, aim to solve optimal control problems only by probing the system and improving\nstrategies based on past rewards and states.\nMany researchers argue for algorithms that can\ninnately learn to control without access to the complex details required to simulate a dynamical\n1To achieve notational consistency, I am throughout adopting the control-centric notation of denoting state-action\npairs as (x, u) rather than (s, a) as is commonly used in reinforcement learning.\n4\nsystem. They argue that it is often easier to ﬁnd a policy for a task than it is to ﬁt a general\npurpose model of the system dynamics (see for example, the discussion in Chapter 3 of Volume 2\nof Bertsekas [11]). Model-free methods are primarily divided into two approaches: Policy Search\nand Approximate Dynamic Programming. Policy Search directly searches for policies by using data\nfrom previous episodes in order to improve the reward. Approximate Dynamic Programming uses\nBellman’s principle of optimality to approximate Problem (2.3) using previously observed data.\nThroughout, my aim will be to highlight the main conceptual ideas of diﬀerent approaches and\nto avoid embroiling myself in a thorough discussion of the myriad of technical details required to\nmake all of the statements crisply precise. What is important is that all of the approaches surveyed\nreduce to some sort of function ﬁtting from noisy observations of the dynamical system, though\nperformance can be drastically diﬀerent depending on how you ﬁt this function. In model-based\nreinforcement learning, we ﬁt a model of the state transitions to best match observed trajectories.\nIn approximate dynamic programming, we estimate a function that best characterizes the “cost to\ngo” for experimentally observed states. And in direct policy search, we attempt to ﬁnd a policy that\ndirectly maximizes the optimal control problem using only input-output data. The main question\nis which of these approaches makes the best use of samples and how quickly do the derived policies\nconverge to optimality.\n3.1\nModel-based reinforcement learning\nOne of the simplest and perhaps most obvious strategies to solve the core RL Problem (2.3) is to\nestimate a predictive model for the dynamical process and then to use it in a dynamic programming\nsolution to the prescribed control problem. The estimated model is called the nominal model, and\nI will refer to control design assuming the estimated model is true as nominal control. Nominal\ncontrol, commonly verbosely referred to as “control under the principle of certainty equivalence,”\nserves as a useful baseline algorithm.\nEstimation of dynamical systems is called system identiﬁcation in the control community [48].\nSystem Identiﬁcation diﬀers from conventional estimation because one needs to carefully choose\nthe right inputs to excite various degrees of freedom and because dynamical outputs are correlated\nover time with the parameters we hope to estimate, the inputs we feed to the system, and the\nstochastic disturbances. Once data are collected, however, conventional machine learning tools can\nbe used to ﬁnd the system that best agrees with the data and can be applied to analyze the number\nof samples required to yield accurate models [23, 83].\nSuppose we want to build a predictor of xt+1 from the trajectory history. A simple, classic\nstrategy is simply to inject a random probing sequence ut for control and then measure how the\nstate responds. Up to stochastic noise, we should have that\nxt+1 ≈ϕ(xt, ut) ,\n(3.1)\nwhere ϕ is some model aiming to approximate the true dynamics.\nϕ might arise from a ﬁrst-\nprinciples physical model or might be a non-parametric approximation by a neural network. The\nstate-transition function can then be ﬁt using supervised learning. For instance, a model can be ﬁt\nby solving the least squares problem\nminimizeϕ\nPN−1\nt=0 ||xt+1 −ϕ(xt, ut)||2 .\nLet ˆϕ denote the function ﬁt to the collected data to model the dynamics. Let ωt denote a\nrandom variable that we will use as a model for the noise process. With such a point estimate for\n5\nthe model, we might solve the optimal control problem\nmaximize\nEωt[PN\nt=0 R(xt, ut)]\nsubject to\nxt+1 = ˆϕ(xt, ut) + ωt, ut = πt(τt) .\nIn this case, we are solving the wrong problem to get our control policies πt. Not only is the model\nincorrect, but this formulation requires some plausible model of the noise process. But if ˆϕ and f\nare close, this approach might work well in practice.\n3.2\nApproximate Dynamic Programming\nApproximate dynamic programming approaches the RL problem by directly approximating the\noptimal control cost and then solving this approximation with techniques from dynamic program-\nming. The dynamic programming solution to Problem (2.3) is based on the principle of optimality:\nif you’ve found an optimal control policy for a time horizon of length N, π1, . . . , πN, and you want\nto know the optimal strategy starting at state x at time t, then you just have to take the optimal\npolicy starting at time t, πt, . . . , πN. Dynamic programming then lets us recursively ﬁnd a control\npolicy by starting at the ﬁnal time and recursively solving for policies at earlier times.\nDeﬁne the Q-function for (2.3) to be the mapping\nQ(x, u) = max\n(\nEet\n\" N\nX\nt=0\nR(xt, ut)\n#\n: xt+1 = f(xt, ut, et), (x0, u0) = (x, u)\n)\n(3.2)\nThe Q-function determines the value of the optimal control problem that is attained when the ﬁrst\naction is set to be u and the initial condition is x. Note that it then trivially follows that the optimal\nvalue of Problem (2.3) is maxu Q(x0, u), and the optimal policy is π(x0) = arg maxu Q(x0, u). If we\nhad access to the Q-function, we’d have everything we’d need to know to take the ﬁrst step in the\noptimal control problem. We can use dynamic programming to compute this Q-function and the\nQ-function associated with every subsequent action. That is, we deﬁne the terminal Q-function to\nbe\nQN(x, u) = R(x, u) ,\nand then deﬁne recursively\nQk(x, u) = R(x, u) + Ee\n\u0014\nmax\nu′ Qk+1(f(x, u, e), u′)\n\u0015\n.\n(3.3)\nThis is the dynamic programing algorithm in a nutshell: we can recursively deﬁne the Q-functions\nby passing backward in time, and then compute the optimal controls from any starting x0 by\napplying the policy that maximizes the right hand side of (3.3) at each time step. (3.3) is known\nas Bellman’s equation. Note that for all time, the optimal policy is uk = arg maxu Qk(xk, u) and\ndepends only on the current state.\nApproximate Dynamic Programming methods typically try to compute these action-value func-\ntions from data. They do so by assuming that the Q-function is stationary. (i.e., Qk(x, u) = Q(x, u)\nfor all k and some function Q). Such stationarity indeed arises assuming the time horizon is inﬁnite.\nConsider the limit:\nmaximize\nlimN→∞Eet[ 1\nN\nPN\nt=0 R(xt, ut)]\nsubject to\nxt+1 = f(xt, ut, et), ut = πt(τt)\n(x0 given).\n(3.4)\n6\nAnd we deﬁne the Q-function Q(x0, u0) to be the average reward accrued running from state x0 with\ninitial action u0. Unfortunately, Problem (3.4) is not directly amenable to dynamic programming\nwithout introducing further technicalities. For mathematical convenience and also to connect to\ncommon practice in RL, it’s useful to instead consider the discounted reward problem\nmaximize\n(1 −γ)Eet[P∞\nt=0 γtR(xt, ut)]\nsubject to\nxt+1 = f(xt, ut, et), ut = πt(τt)\n(x0 given).\n(3.5)\nwhere γ is a scalar in (0, 1) called the discount factor. For γ close to 1, the discounted reward\nis approximately equal to the average reward [11].\nThe discounted cost has particularly clean\noptimality conditions that make it amenable to estimation. If we deﬁne Qγ(x, u) to be the Q-\nfunction obtained from solving Problem (3.5) with initial condition x, then we have a discounted\nversion of dynamic programming, now with the same Q-functions on the left and right hand sides:\nQγ(x, u) = R(x, u) + γEe\n\u0014\nmax\nu′ Qγ(f(x, u, e), u′)\n\u0015\n.\nThe optimal policy is now for all times to let\nut = arg max\nu\nQγ(xt, u) .\n(3.6)\nThis is a remarkably simple formula which is part of what makes Q-learning methods so attractive.\nWe can try to solve for the Q-function using stochastic approximation. If we draw a sample\ntrajectory using the policy given by (3.6), then we should have (approximately and in expectation)\nQγ(xk, uk) ≈R(xk, uk) + γ max\nu′ Qγ(xk+1, u′) .\nThus, beginning with some initial guess Q(old)\nγ\nfor the Q-function, we can update\nQ(new)\nγ\n(xk, uk) = (1 −η)Q(old)\nγ\n(xk, uk) + η\n\u0012\nR(xk, uk) + γ max\nu′ Q(old)\nγ\n(xk+1, u′)\n\u0013\n(3.7)\nwhere η is a step-size or learning rate. (3.7) forms the basis of Q-learning algorithms [81, 85].\nSurveying ADP using only Q-functions is somewhat unorthodox. Most introductions to ADP\ninstead focus on value functions where\nV (x) = max\nu\nQ(x, u) .\nMethods for estimating value functions are also widely used in reinforcement learning and devel-\noped through the perspective of estimation and stochastic approximation. In particular, Temporal\nDiﬀerence algorithms are derived from the value-function-centric perspective [75, 26, 21, 13, 89].\nNote that in all cases here, though we have switched away from models, there’s no free lunch.\nWe are still estimating functions here, and we need to assume that the functions have some reason-\nable structure or we can’t learn them. Choosing a parameterization of the Q-function is a modeling\nassumption. The term “model-free” almost always means “no model of the state transition func-\ntion” when casually claimed in reinforcement learning research. However, this does not mean that\n7\nmodeling is not heavily built into the assumptions of model-free RL algorithms. Moreover, for con-\ntinuous control problems these methods appear to make an ineﬃcient use of samples. Suppose the\ninternal state of the system is of dimension d. When modeling the state-transition function, (3.1)\nprovides d equations per time step. By contrast, we are only using 1 equation per time step in\nADP. Such ineﬃciency is certainly seen in practice below. Also troubling is the fact that we had to\nintroduce the discount factor in order to get a simple Bellman equation. One can avoid discount\nfactors, but this requires considerably more sophisticated analysis. Large discount factors do in\npractice lead to brittle methods, and the discount becomes a hyperparameter that must be tuned\nto stabilize performance. We will determine below when and how these issues arise in practice in\ncontrol.\n3.3\nDirect Policy Search\nThe most ambitious form of control without models attempts to directly learn a policy function from\nepisodic experiences without ever building a model or appealing to the Bellman equation. From\nthe oracle perspective, these policy driven methods turn the problem of RL into derivative-free\noptimization.\n3.3.1\nA generic algorithm for sampling to optimize\nIn turn, let’s ﬁrst begin with a review of a general paradigm for leveraging random sampling to\nsolve optimization problems. Consider the general unconstrained optimization problem\nmaximizez∈Rd\nR(z) .\n(3.8)\nAny optimization problem like this is equivalent to an optimization over probability distributions\non z:\nmaximizep(z)\nEp[R(z)] .\nIf z⋆is the optimal solution, then we’ll get the same value if we put a δ-function around z⋆.\nMoreover, if p is a probability distribution, it is clear that the expected value of the reward function\ncan never be larger than the maximal reward achievable by a ﬁxed z. So we can either optimize\nover z or we can optimize over distributions over z.\nSince optimizing over the space of all probability densities is intractable, we must restrict the\nclass of densities over which we optimize. For example, we can consider a family parameterized by\na parameter vector ϑ: p(u; ϑ) and attempt to optimize\nmaximizeϑ\nEp(z;ϑ)[R(z)] .\n(3.9)\nIf this family of distributions contains all of the Delta functions, then the optimal value will coincide\nwith the non-random optimization problem. But if the family does not contain the Delta functions,\nthe resulting optimization problem only provides a lower bound on the optimal value no matter\nhow good of a probability distribution we ﬁnd.\nThat said, this reparameterization provides a powerful and general algorithmic framework for\noptimization.\nIn particular, we can compute the derivative of J(ϑ) := Ep(z;ϑ)[R(z)] using the\n8\nfollowing calculation (called “the log-likelihood trick”):\n∇ϑJ(ϑ) =\nZ\nR(z)∇ϑp(z; ϑ)dz\n=\nZ\nR(z)\n\u0012∇ϑp(z; ϑ)\np(z; ϑ)\n\u0013\np(z; ϑ)dz\n=\nZ\n(R(z)∇ϑ log p(z; ϑ)) p(z; ϑ)dz\n= Ep(z;ϑ) [R(z)∇ϑ log p(z; ϑ)] .\nThis derivation reveals that the gradient of J with respect to ϑ is the expected value of the function\nG(z, ϑ) = R(z)∇ϑ log p(z; ϑ)\n(3.10)\nHence, if we sample z from the distribution deﬁned by p(z; ϑ), we can compute G(z, ϑ) and will\nhave an unbiased estimate of the gradient of J. We can follow this direction and will be running\nstochastic gradient descent on J, deﬁning Algorithm 1.\nAlgorithm 1 REINFORCE\n1: Hyperparameters: step-sizes αj > 0.\n2: Initialize: ϑ0 and k = 0.\n3: while ending condition not satisﬁed do\n4:\nSample zk ∼p(z; ϑk).\n5:\nSet ϑk+1 = ϑk + αkR(zk)∇ϑ log p(zk; ϑk).\n6:\nk ←k + 1\n7: end while\nAlgorithm 1 is typically called REINFORCE [87] and its main appeal is that it is trivial to\nimplement. If you can eﬃciently sample from p(z; ϑ), you can run this algorithm on essentially\nany problem. But such generality must and does come with a signiﬁcant cost. The algorithm\noperates on stochastic gradients of the sampling distribution, but the function we cared about\noptimizing—R—is only accessed through function evaluations. Direct search methods that use the\nlog-likelihood trick are necessarily derivative free optimization methods, and, in turn, are necessarily\nless eﬀective than methods that compute actual gradients, especially when the function evaluations\nare noisy [37]. Another signiﬁcant concern is that the choice of distribution can lead to very high\nvariance in the stochastic gradients. Such high variance in turn implies that many samples need to\nbe drawn to ﬁnd a stationary point.\nThat said, the ease of implementation should not be readily discounted. Direct search methods\nare trivial to implement, and oftentimes reasonable results can be achieved with considerably less\neﬀort than custom solvers tailored to the structure of the optimization problem. There are two\nprimary ways that this sort of stochastic search arises in reinforcement learning: Policy Gradient\nand Pure Random Search.\n9\n3.3.2\nPolicy Gradient\nAs seen from Bellman’s equation, the optimal policy for Problem (2.3) is always deterministic.\nNonetheless, the main idea behind policy gradient is to use probabilistic policies.\nProbabilistic\npolicies are optimal for other optimization-based control problems such as control of partially\nobserved Markov decision processes [7, 38] or in zero-sum games. Hence, exploring their value for\nthe RL problems studied in this survey does not appear too outlandish at ﬁrst glance.\nWe ﬁx our attention on parametric, randomized policies such that ut is sampled from a distri-\nbution p(u|τt; ϑ) that is a function only of the currently observed trajectory and a parameter vector\nϑ. A probabilistic policy induces a probability distribution over trajectories:\np(τ; ϑ) =\nL−1\nY\nt=0\np(xt+1|xt, ut)p(ut|τt; ϑ) .\n(3.11)\nMoreover, we can overload notation and deﬁne the reward of a trajectory to be\nR(τ) =\nN\nX\nt=0\nRt(xt, ut)\nThen our optimization problem for reinforcement learning tidily takes the form of Problem (3.9).\nPolicy gradient thus proceeds by sampling a trajectory using the probabilistic policy with param-\neters ϑk, and then updating using REINFORCE.\nUsing the log-likelihood trick and (3.11), it is straightforward to verify that the gradient of J\nwith respect to ϑ is not an explicit function of the underlying dynamics. However, at this point\nthis should not be surprising. By shifting to distributions over policies, we push the burden of\noptimization onto the sampling procedure.\n3.3.3\nPure Random Search\nAn older and more widely applied method to solve Problem (3.8) is to directly perturb the current\ndecision variable z by random noise and then update the model based on the received reward at this\nperturbed value. That is, we apply Algorithm 1 with sampling distribution p(z; ϑ) = p0(z −ϑ) for\nsome distribution p0. The simplest examples for p0 would be the uniform distribution on a sphere\nor a normal distribution. Perhaps less surprisingly here, REINFORCE can again be run without\nany knowledge of the underlying dynamics. Note that in this case, the REINFORCE algorithm has\na simple interpretation in terms of gradient approximation. Indeed, REINFORCE is equivalent to\napproximate gradient ascent of R\nϑt+1 = ϑt + αgσ(ϑk)\nwith the gradient approximation\ngσ(ϑ) = R(ϑ + σϵ) −R(ϑ −σϵ)\n2σ\nϵ .\nThis update says to compute a ﬁnite diﬀerence approximation to the gradient along the direction\nϵ and move along the gradient. One can reduce the variance of such a ﬁnite-diﬀerence estimate by\nsampling along multiple random directions and averaging:\ng(m)\nσ\n(ϑ) = 1\nm\nm\nX\ni=1\nR(ϑ + σϵi) −R(ϑ −σϵi)\n2σ\nϵi .\n10\nThis is akin to approximating the gradient in the random subspace spanned by the ϵi\nThis particular algorithm and its generalizations go by many diﬀerent names. Probably the\nearliest proposal for this method was made by Rastrigin [60]. In an unexpected historical surprise,\nRastrigin initially developed this method to solve reinforcement learning problems! His main moti-\nvating example was an inverted pendulum. A rigorous analysis using contemporary techniques was\nprovided by Nesterov and Spokoiny [54]. Random search was also discovered by the evolutionary\nalgorithms community, where it is called a (µ, λ)-Evolution Strategy [15, 66]. Random search has\nalso been studied in the context of stochastic approximation [71] and bandits [31, 5]. Algorithms\nthat are invented independently by four diﬀerent communities probably have something good going\nfor them.\nThe random search method is considerably simpler than the policy gradient algorithm but it\nuses much less structure from the problem as well. Since RL problems tend to be nonconvex, it is\nnot clear which of these approaches is best unless we focus on speciﬁc instances. In light of this, in\nthe next section we turn to a set of instances where we may be able to glean more insights about\nthe relative merits of all of the approaches to RL covered in this section.\n3.4\nDeep reinforcement learning\nNote that in this section I have spent no time discussing deep reinforcement learning. That is\nbecause there is nothing conceptually diﬀerent other than the use of neural networks for function\napproximation. That is, if one wants to take any of the described methods and make them deep, they\nsimply need to add a neural net. In model-based RL, ϕ is parameterized as a neural net, in ADP, the\nQ-functions or Value Functions are assumed to be well-approximated by neural nets, and in policy\nsearch, the policies are set to be neural nets. The algorithmic concepts themselves don’t change.\nHowever, convergence analysis certainly will change, and algorithms like Q-learning might not\neven converge. The classic text Neuro-dynamic Programming by Bertsekas and Tsitisklis discusses\nthe adaptations needed to admit function approximation [14]. By eliminating the complicating\nvariable of function approximation, we can get better insights into the relative merits of these\nmethods, especially when focusing on a simple set of instances of optimal control, namely, the\nLinear Quadratic Regulator.\n4\nSimplifying theme: The Linear Quadratic Regulator\nWith this varied list of approaches to reinforcement learning, it is diﬃcult from afar to judge which\nmethod fares better on which problems. It is likely best to start simple and small and ﬁnd the\nsimplest non-trivial problem that can assist in distinguishing the various approaches to control.\nThough simple models are not the end of the story in analysis, it tends to be the case that if a\ncomplicated method fails to perform on a simple problem, then this indicates a ﬂaw in the method.\nI’d argue that in controls, the simplest non-trivial class of instances of optimal control is those\nwith convex quadratic rewards and linear dynamics. That is, the problem of the Linear Quadratic\nRegulator (LQR):\nminimize\nEet\nh\n1\n2\nPN\nt=0 xT\nt Qxt + uT\nt Rut + 1\n2xT\nN+1SxN+1\ni\n,\nsubject to\nxt+1 = Axt + But + et, ut = πt(τt)\n(x0 given).\n(4.1)\n11\nHere, Q, R, and S are positive semideﬁnite matrices. Do note that we have switched to minimization\nfrom maximization, as is conventional in optimal control. The state transitions are governed by a\nlinear update rule with A and B appropriately sized matrices.\nA few words are in order to defend this baseline as instructive for general problems in continuous\ncontrol and RL. Though linear dynamics are somewhat restrictive, many systems are linear over the\nrange we’d like them to operate. Indeed, enormous engineering eﬀort goes into designing systems\nso that their responses are as close to linear as possible. From an optimization perspective, linear\ndynamics are the only class where we are guaranteed that our constraint set is convex, which is\nanother appealing feature for analysis.\nWhat about cost functions?\nWhereas dynamics are typically handed to the engineer, cost\nfunctions are completely at their discretion.\nDesigning and reﬁning cost functions are part of\noptimal control design, and diﬀerent characteristics can be extracted by iteratively reﬁning cost\nfunctions to meet speciﬁcations.\nThis is no diﬀerent in machine learning where, for example,\ncombinatorial losses in classiﬁcation are replaced with smooth losses like logistic or squared loss.\nDesigning cost functions is a major challenge and tends to be an art form in engineering. But since\nwe’re designing our cost functions, we should focus our attention on costs that are easier to solve.\nQuadratic cost is particularly attractive not only because it is convex, but also for how it interacts\nwith noise. The cost of the stochastic problem is equal to that of the noiseless problem plus a\nconstant that is independent of the choice of ut. The noise will degrade the achievable cost, but it\nwill not aﬀect how control actions are chosen.\nNote that when the parameters of the dynamical system are known, the standard LQR problem\nadmits an elegant dynamic programming solution [90]. The control action is a linear function of\nthe state\nut = −Ktxt\nfor some matrix Kt that can be computed via a simple linear algebraic recursion with only knowledge\nof (A, B, Q, R).\nIn the limit as the time horizon tends to inﬁnity, the optimal control policy is static, linear state\nfeedback:\nut = −Kxt\nwhere K is a ﬁxed matrix deﬁned by\nK = (R + BT MB)−1BT MA\nand M is a solution to the Discrete Algebraic Riccati Equation\nM = Q + AT MA −(AT MB)(R + BT MB)−1(BT MA) .\n(4.2)\nThat is, for LQR on an inﬁnite time horizon, πt(xt) = −Kxt. Here, M is the unique solution of\nthe Riccati equation where all of the eigenvalues of A −BK have magnitude less than 1. Finding\nthis speciﬁc solution is relatively easy using standard linear algebraic techniques [90].\nThere are a variety of ways to derive these formulae.\nIn particular, one can use dynamic\nprogramming as in Section 3.2. In this case, one can check that the Q-function on a ﬁnite time\nhorizon satisﬁes a recursion\nQk(x, u) = xT Qx + uT Ru + (Ax + Bu)T Mk+1(Ax + Bu) + ck .\nfor some positive deﬁnite matrix Mk+1. The limit of these matrices are the solution of (4.2)\n12\nThough LQR cannot capture every interesting optimal control problem, it has many of the\nsalient features of the generic optimal control problem. Dynamic programming recursion lets us\ncompute the control actions eﬃciently and, for long time horizons, a static policy is nearly optimal.\nNow the main question to consider in the context of RL: What happens when we don’t know\nA and B? What’s the right way to interact with the dynamical system in order to quickly and\neﬃciently get it under control? Let us now dive into the diﬀerent styles of reinforcement learning\nand connect them to ideas in controls, using LQR as a guiding baseline.\n4.1\nThe sample complexity of model-based RL for LQR\nFor LQR, maximum likelihood estimation of a nominal model is a least squares problem:\nminimizeA,B\nPN−1\nt=0 ||xt+1 −Axt −But||2 .\nHow well do these model estimates work for the LQR problem? Suppose we treat the estimates as\ntrue and use them to compute a state feedback control from a Riccati equation. While we might\nexpect this to work well in practice, how can we verify the performance? As a simple case, suppose\nthat the true dynamics are slightly unstable so that A has at least one eigenvalue of magnitude\nlarger than 1. It is fully possible for the least squares estimates of such a mode is less than one,\nand, consequently, the optimal control strategy using the estimate will fail to account for the\npoorly estimated unstable eigenvalue. How can we include the knowledge that our model is just an\nestimate and not accurate with a small sample count? One possible solution is to use tools from\nrobust control to mitigate this uncertainty.\n4.1.1\nCoarse-ID Control: a new paradigm for learning to control.\nMy collaborators and I have been considering an approach to merge robust control and high-\ndimensional statistics dubbed “Coarse-ID Control.” The general framework consists of the following\nthree steps:\n1. Use supervised learning to learn a coarse model of the dynamical system to be controlled. I\nwill refer to the system estimate as the nominal system.\n2. Using either prior knowledge or statistical tools like the bootstrap, build probabilistic guar-\nantees about the distance between the nominal system and the true, unknown dynamics.\n3. Solve a robust optimization problem that optimizes control of the nominal system while\npenalizing signals with respect to the estimated uncertainty, ensuring stable, robust execution.\nAs long as the true system behavior lies in the estimated uncertainty set, we’ll be guaranteed\nto ﬁnd a performant controller. The key here is that we are using machine learning to identify not\nonly the plant to be controlled, but the uncertainty as well. Indeed, the main advances in the past\ntwo decades of estimation theory consist of providing reasonable estimates of such uncertainty sets\nwith guaranteed bounds on their errors as a function of the number of observed samples. Taking\nthese new tools and merging them with old and new ideas from robust control allow us to bound\nthe end-to-end performance of a controller in terms of the number of observations.\nThe coarse-ID procedure is well illustrated through the case study of LQR [27]. We can guaran-\ntee the accuracy of the least squares estimates for A and B using novel probabilistic analysis [70].\n13\nWith the estimate of model error in hand, one can pose a robust variant of the standard LQR\noptimal control problem that computes a robustly stabilizing controller seeking to minimize the\nworst-case performance of the system given the (high-probability) norm bounds on our modeling\nerrors.\nTo design a good control policy, we here turn to state-of-the-art tools from robust control.\nWe leverage the recently developed System Level Synthesis (SLS) framework [50, 84] to solve this\nrobust optimization problem. SLS lifts the system description into a higher dimensional space that\nenables eﬃcient search for controllers. The proposed approach provides non-asymptotic bounds\nthat guarantee ﬁnite performance on the inﬁnite time horizon, and quantitatively bound the gap\nbetween the computed solution and the true optimal controller.\nSuppose in LQR that we have a state dimension d and control dimension p. Denote the minimum\ncost achievable by the optimal controller as J⋆. Our analysis guarantees that after a observing a\ntrajectory of length T, we can design a controller that will have inﬁnite-time-horizon cost ˆJ with\nˆJ −J⋆\nJ⋆\n= ˜O\n\u0012q\nd+p\nT\n\u0013\n.\nHere, the notation ˜O(·) suppresses logarithmic factors and instance-dependent constants. In par-\nticular, we can guarantee that we stabilize the system after seeing only a ﬁnite amount of data.\nWhere Coarse-ID control diﬀers from nominal control is that it explicitly accounts for the\nuncertainty in the least squares estimate.\nBy appending this uncertainty to the original LQR\noptimization problem, we can circumvent the need to study perturbations of Riccati equations.\nMoreover, since the approach is optimization based, it can be readily applied to other optimal\ncontrol problems beyond the LQR baseline.\n4.2\nThe sample complexity of model-free RL for LQR\nSince we know that the Q-function for LQR is quadratic, we can try to estimate it by dynamic\nprogramming. Such a method was probably ﬁrst proposed by Bradtke, Barto, and Ydstie [22].\nMore recently, Tu showed that the Least-squares Temporal Diﬀerencing algorithm, also due to\nBradtke and Barto [21], could estimate the value function of LQR to low error with ˜O\n\u0012q\nd2\nT\n\u0013\nsamples [82]. This estimator can then be combined with a method to improve the estimated policy\nover time.\nNote that the bound on the eﬃciency of the estimator here is worse than the error obtained\nfor estimating the model of the dynamical system. While comparing worst-case upper bounds is\ncertainly not valid, it is suggestive that, as mentioned above, temporal diﬀerencing methods use\nonly one deﬁning equation per time step whereas model estimation uses d equations per time step.\nSo while the conventional wisdom suggests that estimating Q-functions for speciﬁc tasks should be\nsimpler than estimating models, the current methods appear to be less eﬃcient with aggregated\ndata than system identiﬁcation methods.\nWith regard to direct search methods, we can already see variance issues enter the picture even\nfor small LQR instances. Consider the most trivial example of LQR:\nR(u) = ||u||2\nLet p(u; ϑ) be a multivariate Gaussian with mean ϑ and variance σ2I. Then\nEp(u;ϑ)[R(u)] = ∥ϑ∥2 + σ2d\n14\nObviously, the best thing to do would be to set ϑ = 0. Note that the expected reward is oﬀby σ2d\nat this point, but at least this would be ﬁnding a good guess for u. Also, as a function of ϑ, the\ncost is strongly convex, and the most important thing to know is the expected norm of the gradient\nas this will control the number of iterations. Now, after sampling u from a Gaussian with mean ϑ0\nand variance σ2I and using formula (3.10), the ﬁrst gradient will be\ng = −||ω −ϑ0||2ω\nσ2\n,\nwhere ω is a normally distributed random vector with mean zero and covariance σ2I. The expected\nnorm of this stochastic gradient is on the order of\nO\n\u0000σd1.5 + σ−1d0.5∥ϑ0∥\n\u0001\n,\nwhich indicates a signiﬁcant scaling with dimension.\nSeveral works have analyzed the complexity of this method [31, 5, 37], and the upper and\nlower bounds strongly depend on the dimension of the search space.\nThe upper bounds also\ntypically depend on the largest magnitude reward B. If the function values are noisy, even for\nconvex functions, the convergence rate is O((d2B2/T)−1/3), and this assumes you get the algorithm\nparameters exactly right. For strongly convex functions, this can be reduced to O((d2B2/T)−1/2)\nfunction evaluations, but this result is also rather fragile to the choice of parameters. Finally, note\nthat just adding an constant oﬀset to the reward dramatically slows down the algorithm. If you\nstart with a reward function whose values are in [0, 1] and subtract one million from each reward,\nthis will increase the running time of the algorithm by a factor of a million, even though the ordering\nof the rewards amongst parameter values remains the same.\n5\nNumerical comparisons\nThe preceding analyses of the RL paradigms when applied to LQR are striking. A model-based\napproach combining supervised learning and robust control achieves nearly optimal performance\ngiven its sampling budget. Approximate dynamic programming appears to fare worse in terms\nof worst-case performance. And direct policy search seems to be of too high variance to work in\npractice. In this section, we implement these various methods and test them on some simple LQR\ninstances to see how these theoretical predictions reﬂect practice.\n5.1\nA Double Integrator\nAs a simple test case, consider the classic problem of a discrete-time double integrator with the\ndynamical model\nxt+1 =\n\u00141\n1\n0\n1\n\u0015\nxt +\n\u00140\n1\n\u0015\nut\n(5.1)\nSuch a system could model, say, the position (ﬁrst state) and velocity (second state) of a unit mass\nobject under force u.\nAs an instance of LQR, we can try to steer this system to reach point 0 from initial condition\nx0 = [−1, 0] without expending much force:\nQ =\n\u00141\n0\n0\n0\n\u0015\nR = r0\n(5.2)\n15\n0\n5000\n10000\n15000\n20000\n25000\n30000\nsamples\n5\n6\n7\n8\n9\n10\ncost\npolicy gradient\nrandom search\noptimal\nFigure 1: Cost for the double integrator model for various RL algorithms. The solid plots denote\nthe median performance.\nShaded regions capture the maximum and minimum performance.\nNominal control and LSPI are indistinguishable from the optimal controller in this experiment\nand hence are omitted.\nfor some scalar r0. Note that even in this simple instance there is an element of design: Changing\nthe value of r0 will change the character of the control law balancing expending energy versus speed\nor reaching the desired destination.\nTo compare the diﬀerent approaches, I ran experiments on this instance with a small amount\nof noise (et zero mean with covariance 10−4I), and training episode length L = 10. The goal was\nto design a controller that works on an arbitrarily long time horizon using the fewest number of\nsimulations of length L.\nWith one simulation (10 samples) using a white noise input with unit variance, the nominal\nestimate is correct to 3 digits of precision. And, not surprisingly, this returns a nearly optimal\ncontrol policy.\nRight out of the box, this nominal control strategy works well on this simple\nexample. Note that using a least squares estimator makes the nominal controller’s life hard here\nbecause all prior information about sparsity on the state transition matrices is discarded. In a more\nrealistic situation, the only parameter that would need to be estimated would be the (2, 1) entry in\nB which governs how much force is put out by the actuator and how much mass the system has.\nNow, let’s compare with approximate dynamic programming and policy search methods. For\npolicy search, let us restrict to policies that use a static, linear gain as would be optimal on an\ninﬁnite time horizon. Note that a static linear policy works almost as well as a time-varying policy\nfor this simple LQR problem with two state dimensions. Moreover, there are only two decision\nvariables for this simple problem. For Policy Gradient, I used the Adam algorithm to shape the\niterates [41]. I also subtracted the mean reward of previous iterates, a popular baseline subtraction\nheuristic to reduce variance (Dayan [25] attributes this heuristic to Sutton [74] and Williams [86]). I\nwas unable to get policy gradient to converge without these additional algorithmic ornamentations.\nI also compared against a simple ADP method based called Least Squares Policy Iteration proposed\nby Lagoudakis and Parr [42]. I ran each of these methods using 10 diﬀerent random seeds. Figure 1\nplots the median performance of the various methods with error bars encompassing the maximum\nand minimum over all trials. Both nominal control and LSPI are able to ﬁnd high quality controllers\nwith only ten observations. Direct policy methods, on the other hand, require many times as many\nsamples. Policy gradient, in particular requires thousands of times as many samples as simple\nnominal control.\n16\n100\n200\n300\n400\n500\n600\nsamples\n10\n1\n100\ncost\n100\n200\n300\n400\n500\n600\nsamples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nfraction stable\nNominal\nRobust LQR\nRobust LQR (bootstrap)\nFigure 2: (left) Cost for the Laplacian model for varied models. The blue curve shows the\nperformance of robust LQR when provided with the true distance between the estimate and model.\nThe green curve displays the performance when the uncertainty is learned from data. (right) the\nfraction of the time that the synthesized control strategy returns a stabilizing controller.\n5.2\nUnstable Laplacian dynamics\nAs an illustrative example of the power of LQR as a baseline, let’s now move to a considerably\nharder instance of LQR and show how it highlights issues of robustness and safety. Consider an\nidealized instance of “data center cooling,” a popularized application of reinforcement learning [32].\nDeﬁne the model to have three heat sources coupled to their own cooling devices. Each com-\nponent of the state x is the internal temperature of one each heat source, and the sources heat up\nunder a constant load. They also shed heat to their neighbors. This can be approximately modeled\nby a linear dynamical system with state-transition matrices\nA =\n\n\n1.01\n0.01\n0\n0.01\n1.01\n0.01\n0\n0.01\n1.01\n\n\nB = I .\nNote that the open loop system here is unstable: With any nonzero initial condition, the state\nvector will blow up because the limit of Ak is inﬁnite. Moreover, if a method estimates one of the\ndiagonal entries of A to be less than 1, we might guess that this mode is actually stable and put less\neﬀort into cooling that source. So it is imperative to obtain a high quality estimate of the system’s\ntrue behavior for near optimal control. Or rather, we must be able to ascertain whether or not our\ncurrent policy is safe or the consequences can be disastrous.\nLet’s try to solve the LQR problem with the settings Q = I and R = 1000I. This models a\nhigh relative cost for power consumption and hence may encourage small control inputs on modes\nthat are estimated as stable. What happens for our RL methods on this instance?\nFigure 2 compares nominal control to two versions of the robust LQR problem described in\nsection 4.1. To solve the robust LQR problem, we end up solving a small semideﬁnite programming\nproblem as described by Dean et al [27]. These semideﬁnite programs are solved on my laptop in\nwell under a second. The blue line denotes performance when we tell the robust optimization solver\nwhat the actual distance is from the nominal model to the true model. The green curve depicts what\nhappens when we estimate this diﬀerence between the models using a bootstrap simulation [29, 67].\nNote that estimating the error from data only yields slightly worse LQR performance than exactly\nknowing the true model error.\n17\n1000\n2000\n3000\n4000\n5000\nsamples\n10\n2\n10\n1\n100\ncost\n1000\n2000\n3000\n4000\n5000\nsamples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nfraction stable\nNominal\nRobust LQR\nRobust LQR (bootstrap)\nLSPI\nRandom Search\nPolicy Gradient\nFigure 3: (left) Cost for the Laplacian model for varied models over 5000 iterations. (right) The\nfraction of the time that the synthesized control strategy returns a stabilizing controller.\nNote also that the nominal controller does tend to frequently ﬁnd controllers that fail to stabilize\nthe true system. A necessary and suﬃcient condition for stabilization is for the matrix A + BK\nto have all of its eigenvalues to be less than 1. We can plot how frequently the various search\nmethods ﬁnd stabilizing control policies when looking at a ﬁnite horizon in Figure 1 (right). The\nrobust optimization really helps here to provide controllers that are guaranteed to ﬁnd a stabilizing\nsolution. On the other hand, in industrial practice nominal control does seem to work quite well.\nA great open problem is to ﬁnd reasonable assumptions under which the nominal controller is\nstabilizing.\nFigure 3 additionally compares the performance to model-free methods on this instance. Here\nwe again see that they are indeed far oﬀfrom their model-based counterparts. The x axis has\nincreased by a factor of 10, and yet even the approximate dynamic programming approach does\nnot ﬁnd a decent solution. Surprisingly, LSPI, which worked very well on the double integrator,\nnow performs worse than random search. This is likely because the LSPI subroutine requires a\nstabilizing controller for all iterations and also requires careful tuning of the discount factor. Not\nonly are model-free methods sample hungry, but they fail to be safe. And safety is much more\ncritical than sample complexity.\n6\nBeyond the Linear Quadratic Regulator\nStudying simple baselines such as LQR often provides insights into how to approach more chal-\nlenging problems. In this section, we explore some directions inspired by our analysis of LQR.\n6.1\nDerivative Free Methods for Optimal Control\nRandom search works well on simple linear problems and appears better than more complex meth-\nods like policy gradient. Does simple random search work less well on more diﬃcult problems?\nThe answer, it turns out, is yes. The deep RL community has recently been using a suite of\nbenchmarks to compare methods, maintained by OpenAI2 and based on the MuJoCo simulator [80].\nHere, the optimal control problem is to get the simulation of a legged robot to walk as far and quickly\n2https://gym.openai.com/envs/#mujoco\n18\nas possible in one direction. Some of the tasks are very simple, but some are quite diﬃcult like the\ncomplicated humanoid models with 22 degrees of freedom. The dynamics of legged robots are well\nspeciﬁed by Lagrange’s equations [52], but planning locomotion from these models is challenging\nbecause it is not clear how to best design the objective function and because the model is piecewise\nlinear. The model changes whenever part of the robot comes into contact with a solid object, and\nhence a normal force is introduced that was not previously acting upon the robot. Hence, getting\nrobots to work without having to deal with complicated nonconvex nonlinear models seems like a\nsolid and interesting challenge for the RL paradigm. Moreover, seminal work by Tedrake, Zhang,\nand Seung demonstrated that direct policy search could rapidly ﬁnd feedback control policies for\ncertain constrained legged robot designs [78].\nLevine and Koltun were among the ﬁrst to use MuJoCo as a testbed for learning-based control,\nand were able to achieve walking in complex simulators without special-purpose techniques [45].\nSince then, these techniques have become standard continuous control benchmarks for reinforcement\nlearning (see, for example [69, 47, 64, 65, 88]). Recently, Salimans and his collaborators at OpenAI\nshowed that random search worked quite well on these benchmarks [63]. In particular, they ﬁt\nneural network controllers using random search with a few algorithmic enhancements. Random\nSearch had indeed enjoyed signiﬁcant success in some corners of the robotics community, and\nothers had noted that in their applications, random search outperformed policy gradient [73]. In\nanother piece of great work, Rajeswaran et al showed that Natural Policy Gradient could learn\nlinear policies that could complete these benchmarks [59]. That is, they showed that static linear\nstate feedback, like the kind we use in LQR, was also suﬃcient to control these complex robotic\nsimulators. This of course left an open question: Can simple random search ﬁnd linear controllers\nfor these MuJoCo tasks?\nGuy, Mania, and I tested this out, coding up a rather simple version of random search with a\ncouple of small algorithmic enhancements. Many RL papers were using statistics of the states and\nwhitening the states before passing them into the neural net mapping from state to action. We\nfound that when random search performed the same whitening with linear controls, this algorithm\nwas able to get state-of-the-art results on all of the MuJoCo benchmark tasks [49].\nThere are a few of important takeaways from this study. On the one hand, the results suggest\nthat these MuJoCo demos are easy, or at least considerably easier than they were believed to be.\nBenchmarking is diﬃcult, and having only a few simulation benchmarks encourages overﬁtting to\nthese benchmarks. Indeed, it does seem like these benchmarks are more about taking advantage of\nsimulation approximations in MuJoCo than they are about learning reasonable policies for walking.\nIn terms of benchmarking, this is what makes LQR so attractive: LQR with unknown dynamics\nis a reasonable task to master as it is easy to specify new instances, and it is relatively easy to\nunderstand the limits of achievable performance.\nSecond, note that since our random search method is fast, we can evaluate its performance on\nmany random seeds. All model-free methods exhibit alarmingly high variance on these benchmarks.\nFor instance, on the humanoid task, the the model is slow to train almost a quarter of the time even\nwhen supplied with what we thought were good parameters (see Figure 4 (middle)). And, for those\nrandom seeds, we found the method returned rather peculiar gaits. Henderson et al and Islam et\nal observed this phenomenon with deep reinforcement learning methods, but our results on linear\ncontrollers suggest that such high variability will be a symptom of all model-free methods [35, 36].\nThough direct policy search methods are easy to code up, their reliability on any reasonable control\ntask remains in question.\n19\nFigure 4: (left) Sample frame of the MuJoCo humanoid. (middle) Variance of learning perfor-\nmance on 100 runs of random search on the humanoid model. Note that though high rewards are\noften achieved, it is more common to observe poor control performance from a random initializa-\ntion. (right) Using MPC and a poor model, complex actions with humanoid simulations can be\nexecuted, such as climbing into a vehicle (image from supplementary video of Erez et al. [30]).\n6.2\nReceding Horizon Control\nApproximate dynamic programming is closely related to canonical receding horizon control (RHC)\n(also known as Model Predictive Control (MPC)). In RHC an agent makes a plan based on a\nsimulation from the present until a short time into the future. The agent then executes one step\nof this plan, and then, based on what it observes after taking this action, returns to short-time\nsimulation to plan the next action. This feedback loop allows the agent to link the actual impact\nof its choice of action with what was simulated, and hence can correct for model mismatch, noise\nrealizations, and other unexpected errors.\nTo relate RHC to ADP, note that the discounted problem\nmaximize\n(1 −γ)Eet[PN−1\nt=0 γtR(xt, ut) + γNQγ(xN+1, uT+1)]\nsubject to\nxt+1 = f(xt, ut, et)\n(x0 given).\nis equivalent to Problem (3.5). Here we have just unrolled the cost beyond one step. Though this\nis trivial, it is again incredibly powerful: the longer we make the time horizon, the less we need to\nworry about the Q-function being accurate. Of course, now we need to worry about the accuracy\nof the state-transition map, f. But, especially in problems with continuous variables, it is not at all\nobvious which accuracy is more important in terms of ﬁnding algorithms with fast learning rates\nand short computation times. There is a trade-oﬀbetween learning models and learning value\nfunctions, and this is a trade-oﬀthat needs to be better understood.\nThough RHC methods appear fragile to model mismatch, the repeated feedback inside RHC\ncan correct for many modeling errors. As an example, it is worth revisiting the robotic locomotion\ntasks inside the MuJoCo framework.\nThese tasks were actually designed to test the power of\na nonlinear RHC algorithm developed by Tassa, Erez, and Todorov [77]. The receding horizon\ncontroller works to keep the robot upright even when the model is poorly speciﬁed. Moreover,\nthe RHC approach to humanoid control solved for the controller in 7x real time in 2012. In 2013,\nthe same research group published a cruder version of their controller that they used during the\nDARPA Robotics Challenge [30]. All these behaviors are generated by RHC in real-time. Though\nthe resulting walking is not of the same quality as what can be obtained from computationally\n20\nintensive long-horizon trajectory optimization, it does look considerably better than the sort of\ngaits typically obtained by popular RL methods.\nIs there a middle ground between expensive oﬄine trajectory optimization and real time RHC?\nI think the answer is yes in the same way that there is a middle ground between learning dynamical\nmodels and learning Q-functions. The performance of a RHC system can be improved by better\nmodeling of the Q-function that deﬁnes the terminal cost: The better a model you make of the\nQ-function, the shorter a time horizon you need for simulation, and the closer you get to real-time\noperation. Of course, if you had a perfect model of the Q-function, you could just solve the Bellman\nequation and you would have the optimal control policy. But by having an approximation to the\nQ-function, high performance can still be extracted in real time.\nSo what if we learn to iteratively improve the Q-function while running RHC? This idea has been\nexplored in a project by Rosolia, Carvalho, and Borrelli [61]. In their “Learning MPC” approach,\nthe terminal cost is learned by a method akin to nearest neighbors. The terminal cost of a state\nis the value obtained last time that state was tried. If a state has not yet been visited, the cost\nis inﬁnite. This formulation constrains the terminal condition to be in a state observed before. It\nenables the control system to explore new ways to decrease cost as long as it maintains the ability\nto reach a state that has already been demonstrated to be safe. This “nearest-neighbors” approach\nworks surprisingly well in practice: in RC car demonstrations, the learned controller works better\nthan a human operator after only a few laps around a ﬁxed track.\nAnother reason to like this blended RHC approach to learning to control is that one can hard\ncode in constraints on controls and states and easily incorporate models of disturbance directly into\nthe optimization problem. Some of the most challenging problems in control are how to execute\nsafely while continuing to learn more about a system’s capability, and an RHC approach provides\na direct route toward balancing safety and performance. Indeed, an interesting direction of future\nwork would be merging the robust learning of Coarse-ID Control with receding horizon control.\n7\nChallenges at the control-learning interface\nWe have set out to bridge the gap between the learning-centric views of RL and the model-centric\nviews of control. Perhaps surprisingly, we found that for continuous control problems, machine\nlearning seems best suited for model ﬁtting rather than for direct control. Moreover, perhaps less\nsurprisingly, we could seamlessly merge learned models and control action by accounting for the\nuncertainty in our model ﬁts.\nMoreover, we showed how value functions and models could be\nlearned in chorus and could provide impressive results on real embodied agents. These distinctions\nand connections are merely the beginning of what the control and machine learning communities\ncan learn from each other. Let me close by discussing three particularly exciting and important\nresearch challenges that may be best solved with input from both perspectives.\n7.1\nMerging perception and control\nOne of the grand aspirations of reinforcement learning is end-to-end control, mapping directly from\nsensors like pixels to actions. Computer vision has made major advances by adopting an “all-conv-\nnet” end-to-end approach, and many, including industrial research at NVIDIA [18], suggest the\nsame can be done for complex control tasks.\nIn general, this problem gets into very old intractability issues of nonlinear output feedback\n21\nin control [17] and partially observed Markov decision processes in reinforcement learning [57].\nNonetheless, some early results in RL have shown promise in training optimal controllers directly\nfrom pixels [47, 51]. Of course, these results have even worse sample complexity than the same\nmethods trained from states, but they are making progress.\nIn my opinion, the most promising approaches in this space follow the ideas of Guided Policy\nSearch, which bootstraps standard state feedback to provide training data for a map from sensors\ndirectly to optimal action [45, 44].\nThat is, a mapping from sensor to action can be learned\niteratively by ﬁrst ﬁnding the optimal action and then ﬁnding a map to that control setting. A\ncoupling along these lines where reliance on a precise state estimator is reduced over time could\npotentially provide a reasonably eﬃcient method for learning to control from sensors.\nHowever, these problems remain daunting. Moving from fully observed scenarios to partially\nobserved scenarios makes the control problem exponentially more diﬃcult. How to use diverse\nsensor measurements in a safe and reliable manner remains an active and increasingly important\nresearch challenge [6, 8, 10].\n7.2\nRethinking adaptive control\nThis survey has focused on “episodic” reinforcement learning and has steered clear of a much harder\nproblem: adaptive control. In the adaptive setting, we want to learn the policy online. We only get\none trajectory. The goal is, after a few steps, to have a model whose reward from here to eternity\nwill be large. This is very diﬀerent, and much harder that what people are doing in RL. In episodic\nRL, you get endless access to a simulator. In adaptive control, you get one go.\nEven for LQR, the best approach to adaptive control is not settled. Pioneering work in the\neighties used stochastic gradient-like techniques to ﬁnd adaptive controls, but the guarantees for\nthese schemes are all asymptotic [33].\nMore recently, there has been a groundswell of activity\nin trying to understand this problem from the perspective of online learning.\nBeginning with\nwork by Abbasi-Yadkori and Szepesvari [2], a variety of approaches have been devised to provide\neﬃcient schemes that yield near optimal control cost. Abbasi-Yadkori and Szepesvari’s approach\nachieves an optimal reward building on techniques that give optimal algorithms for the multiarmed\nbandit [9, 43]. But their method requires solving a hard nonconvex optimization problem as a\nsubroutine. Follow-up work has proposed methods using Thompson sampling [3, 4, 56], approximate\ndynamic programming [1], and even coarse-ID control [28], though no method has been found that\nis eﬃciently implementable and achieves optimal cost. Again, this emphasizes that even the simple\nLQR problem is not at all simple. New techniques must be developed to fully understand this\nbaseline problem, but it is clear that insights from both machine learning and control will be\nnecessary to develop eﬃcient adaptive control that can cope with an ever-evolving world.\n7.3\nHumans in the loop\nOne ﬁnal important problem, which might be the most daunting of all, is how machines should\nlearn when humans are in the loop. What can humans who are interacting with the robots do and\nhow can we model human actions? Though economists may have a diﬀerent opinion, humans are\nchallenging to model.\nOne popular approach to modeling human-robot interaction is game theoretic. Humans can\nbe modeled as solving their own optimal control problem, and then the human’s actions enter as\na disturbance in the optimal control problem [62]. In this way, modeling humans is similar to\n22\nmodeling uncertain dynamic environments. But thinking of the humans as optimizers means that\ntheir behavior is constrained. If we know the cost, then we get a complex game theoretic version\nof receding horizon control [16, 46]. But as is usually the case, humans are bad at specifying their\nobjectives, and hence what they are optimizing must be learned. This becomes a problem of inverse\noptimal control [40] or inverse reinforcement learning [55], where we have to estimate the reward\nfunctions of the human and understand the loss accrued for crudely modeling these rewards.\n7.4\nTowards Actionable Intelligence\nAs I’ve expressed before, I think that all of the daunting problems in machine learning are now\nRL problems. Whether they be autonomous transportation system or seemingly mundane social\nnetwork engagement systems, actively interacting with reality has high stakes. Indeed, as soon as\na machine learning system is unleashed in feedback with humans, that system is a reinforcement\nlearning system. The broad engineering community must take responsibility for the now ubiquitous\nmachine learning systems and understand what happens when we set them loose on the world.\nSolving these problems is going to need advances in both machine learning and control. Perhaps\nthis intersection needs a new name so that researchers can stop arguing about territory. I personally\nam fond of actionable intelligence as it sums up not only robotics but smarter, safer analytics. But\nI don’t really care what we call it: There is a large community spanning multiple disciplines that\nis invested in making progress on these problems. Hopefully this tour has set the stage for a lot\nof great research at the intersection of machine learning and control, and I’m excited to see what\nprogress the communities can make working together.\nAcknowledgements\nCountless individuals have helped to shape the contents here. First, this work was generously sup-\nported in part by two forward-looking programs at DOD: the Mathematical Data Science program\nat ONR and the Foundations and Limits of Learning program at DARPA. Second, this survey was\ndistilled from a series on my blog argmin.net. I greatly appreciated the lively debates on Twitter,\nand I hope that even those who disagree with my perspectives here ﬁnd their input incorporated\ninto this survey.\nI’d like to thank Chris Wiggins for sharing his taxonomy on machine learning, Roy Frostig for\nshaping the Section 3.3, Pavel Pravdin for consulting on how to get policy gradient methods up and\nrunning, Max Raginsky for perspectives on adaptive control and translations of Russian. I’d like to\nthank Moritz Hardt, Eric Jonas, and Ali Rahimi for helping to shape the language, rhetoric, and\nfocus of the blog series and this survey. I’d also like to thank Nevena Lazic and Gergely Neu for many\nhelpful suggestions for improving the readability and accuracy of this manuscript. Additionally, I’d\nlike to thank my other colleagues in machine learning and control for many helpful conversations\nand pointers about this material: Murat Arcak, Karl Astrom, Francesco Borrelli, John Doyle, Andy\nPackard, Anders Rantzer, Lorenzo Rosasco, Shankar Sastry, Yoram Singer, Csaba Szepesvari, Claire\nTomlin, and Stephen Wright. I’d also like to thank my colleagues in robotics, Anca Dragan, Leslie\nKaebling, Sergey Levine, Pierre-Yves Oudeyer, Olivier Sigaud, Russ Tedrake, and Emo Todorov\nfor sharing their perspectives on the sorts of RL and optimization technology works for them and\nthe challenges they face in their research.\nI’d like to thank everyone who took CS281B with me in the Spring of 2017 where I ﬁrst tried to\n23\nmake sense of the problems in learning to control. And most importantly, a big thanks everyone in\nmy research group who has been wrestling with these ideas with me for the past several years and\nfor who have done much of the research that shaped my views on this space. In particular, Ross\nBoczar, Nick Boyd, Sarah Dean, Animesh Garg, Aurelia Guy, Qingqing Huang, Kevin Jamieson,\nSanjay Krishnan, Laurent Lessard, Horia Mania, Nik Matni, Becca Roelofs, Ugo Rosolia, Ludwig\nSchmidt, Max Simchowitz, Stephen Tu, and Ashia Wilson.\nFinally, special thanks to Camon Coﬀee in Berlin for letting me haunt their shop while writing.\nReferences\n[1] Y. Abbasi-Yadkori, N. Lazic, and C. Szepesv´ari. The return of ϵ-greedy: sublinear regret for model-free\nlinear quadratic control. arXiv:1804.06021, 2018.\n[2] Y. Abbasi-Yadkori and C. Szepesv´ari. Regret Bounds for the Adaptive Control of Linear Quadratic\nSystems. In Conference on Learning Theory (COLT), 2011.\n[3] Y. Abbasi-Yadkori and C. Szepesv´ari. Bayesian Optimal Control of Smoothly Parameterized Systems:\nThe Lazy Posterior Sampling Algorithm. In Conference on Uncertainty in Artiﬁcial Intelligence, 2015.\n[4] M. Abeille and A. Lazaric. Thompson Sampling for Linear-Quadratic Control Problems. In AISTATS,\n2017.\n[5] A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point\nbandit feedback. In Conference on Learning Theory (COLT), 2010.\n[6] A. K. Akametalu, J. F. Fisac, J. H. Gillula, S. Kaynama, M. N. Zeilinger, and C. J. Tomlin. Reachability-\nbased safe learning with gaussian processes. In Proceedings of the 53rd Conference on Decision and\nControl, pages 1424–1431. IEEE, 2014.\n[7] K. J. ˚Astr¨om. Optimal control of Markov processes with incomplete state information. Journal of\nMathematical Analysis and Applications, 10(1):174–205, 1965.\n[8] A. Aswani, H. Gonzalez, S. S. Sastry, and C. Tomlin. Provably safe and robust learning-based model\npredictive control. Automatica, 49(5):1216–1226, 2013.\n[9] P. Auer, N. Cesa-Bianchi, and P. Fischer.\nFinite-time analysis of the multiarmed bandit problem.\nMachine learning, 47(2-3):235–256, 2002.\n[10] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause. Safe model-based reinforcement learning\nwith stability guarantees. In Advances in Neural Information Processing Systems (NIPS), 2017.\n[11] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientiﬁc, Nashua,\nNH, 4th edition, 2012.\n[12] D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 1. Athena Scientiﬁc, Nashua,\nNH, 4th edition, 2017.\n[13] D. P. Bertsekas and S. Ioﬀe. Temporal diﬀerences-based policy iteration and applications in neuro-\ndynamic programming. Technical Report LIDS-P-2349, MIT Laboratory for Information and Decision\nSystems Report, 1996.\n[14] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.\n[15] H.-G. Beyer and H.-P. Schwefel. Evolution Strategies - a comprehensive introduction. Natural Comput-\ning, 1(1):3–52, 2002.\n[16] W. F. Bialas.\nCooperative n-person Stackelberg games.\nIn Proceedings of the 28th Conference on\nDecision and Control’, 1989.\n24\n[17] V. D. Blondel and J. N. Tsitsiklis. A survey of computational complexity results in systems and control.\nAutomatica, 36(9):1249–1274, 2000.\n[18] M. Bojarski,\nD. Del Testa,\nD. Dworakowski,\nB. Firner,\nB. Flepp,\nP. Goyal,\nL. D. Jackel,\nM. Monfort, U. Muller, J. Zhang, et al.\nEnd to end learning for self-driving cars.\nTechni-\ncal report, NVIDIA, 2016.\narXiv:1604.07316. Press Release at https://devblogs.nvidia.com/\nexplaining-deep-learning-self-driving-car/.\n[19] L. Bottou, J. Peters, J. Qui˜nonero-Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray,\nP. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of computa-\ntional advertising. The Journal of Machine Learning Research, 14(1):3207–3260, 2013.\n[20] M. Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold’em poker is solved. Science,\n347(6218):145–149, 2015.\n[21] S. J. Bradtke and A. G. Barto. Linear least-squares algorithms for temporal diﬀerence learning. Machine\nLearning, 22(1-3):33–57, 1996.\n[22] S. J. Bradtke, B. E. Ydstie, and A. G. Barto. Adaptive linear quadratic control using policy iteration.\nIn Proceedings of the 1994 American Control Conference, 1994.\n[23] M. C. Campi and E. Weyer. Finite Sample Properties of System Identiﬁcation Methods. IEEE Trans-\nactions on Automatic Control, 47(8), 2002.\n[24] C. Dann and E. Brunskill.\nSample complexity of episodic ﬁxed-horizon reinforcement learning.\nIn\nAdvances in Neural Information Processing Systems (NIPS), 2015.\n[25] P. Dayan. Reinforcement comparison. In Connectionist Models, pages 45–51. Elsevier, 1991.\n[26] P. Dayan. The convergence of TD (λ) for general λ. Machine Learning, 8(3-4):341–362, 1992.\n[27] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the sample complexity of the linear quadratic\nregulator. Foundations of Computational Mathematics, 2018. To appear. Preprint available at arXiv:\n1710.01688.\n[28] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. Regret bounds for robust adaptive control of the\nlinear quadratic regulator. arXiv:1805.09388, 2018.\n[29] B. Efron. Bootstrap Methods: Another Look at the Jackknife. The Annals of Statistics, 7(1), 1979.\n[30] T. Erez, K. Lowrey, Y. Tassa, V. Kumar, S. Kolev, and E. Todorov. An integrated system for real-\ntime model predictive control of humanoid robots. In Proceedings of the 13th IEEE-RAS International\nConference on Humanoid Robots (Humanoids), 2013.\n[31] A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting:\ngradient descent without a gradient. Proceedings of the 16th annual ACM-SIAM symposium on Discrete\nalgorithms, pages 385–394, 2005.\n[32] J. Gao and R. Jamidar. Machine learning applications for data center optimization. Technical report,\nGoogle White Paper, 2014.\n[33] G. C. Goodwin, P. J. Ramadge, and P. E. Caines. Discrete time stochastic adaptive control. SIAM\nJournal on Control and Optimization, 19(6):829–853, 1981.\n[34] E. Hazan, S. Kale, and S. Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. In\nConference on Learning Theory (COLT), 2012.\n[35] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning\nthat matters. arXiv:1709.06560, 2017.\n[36] R. Islam, P. Henderson, M. Gomrokchi, and D. Precup. Reproducibility of benchmarked deep reinforce-\nment learning tasks for continuous control. arxiv:1708.04133, 2017.\n25\n[37] K. G. Jamieson, R. D. Nowak, and B. Recht. Query complexity of derivative-free optimization. In\nAdvances in Neural Information Processing Systems (NIPS), 2012.\n[38] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable\nstochastic domains. Artiﬁcial intelligence, 101(1-2):99–134, 1998.\n[39] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of\nartiﬁcial intelligence research, 4:237–285, 1996.\n[40] R. E. Kalman. When is a linear control system optimal? Journal of Basic Engineering, 86(1):51–60,\n1964.\n[41] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arxiv:1412.6980, 2014.\n[42] M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of machine learning research,\n4(Dec):1107–1149, 2003.\n[43] T. L. Lai and H. Robbins.\nAsymptotically eﬃcient adaptive allocation rules. Advances in Applied\nMathematics, 6(1):4–22, 1985.\n[44] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal\nof Machine Learning Research, 17(39):1–40, 2016.\n[45] S. Levine and V. Koltun. Guided policy search. In International Conference on Machine Learning\n(ICML), 2013.\n[46] N. Li, D. W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, and A. R. Girard. Game theoretic modeling\nof driver and vehicle interactions for veriﬁcation and validation of autonomous vehicle control systems.\nIEEE Transactions on control systems technology, PP(99):1–16, 2017.\n[47] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous\ncontrol with deep reinforcement learning. arxiv:1509.02971, 2015.\n[48] L. Ljung.\nSystem Identiﬁcation. Theory for the user.\nPrentice Hall, Upper Saddle River, NJ, 2nd\nedition, 1998.\n[49] H. Mania, A. Guy, and B. Recht.\nSimple random search provides a competitive approach to rein-\nforcement learning.\nIn Advances in Neural Information Processing Systems (NIPS), 2018.\narXiv:\n1803.07055.\n[50] N. Matni, Y.-S. Wang, and J. Anderson. Scalable system level synthesis for virtually localizable systems.\nIn Proceedings of the 56th Conference on Decision and Control, 2017.\n[51] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller,\nA. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature,\n518(7540):529, 2015.\n[52] R. M. Murray. A mathematical introduction to robotic manipulation. CRC press, 2017.\n[53] A. Nemirovski and D. Yudin. Problem complexity and method eﬃciency in optimization. Wiley, New\nYork, 1983.\n[54] Y. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Foundations of\nComputational Mathematics, 17(2):527–566, 2017.\n[55] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In International Conference\non Machine Learning (ICML), 2000.\n[56] Y. Ouyang, M. Gagrani, and R. Jain. Learning-based control of unknown linear systems with thompson\nsampling. arXiv:1709.04047, 2017.\n26\n[57] C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of Markov Decision Processes. Mathematics\nof Operations Research, 12(3):441–450, 1987.\n[58] M. L. Puterman.\nMarkov Decision Processes: Discrete Stochastic Dynamic Programming.\nWiley-\nInterscience, 1994.\n[59] A. Rajeswaran, K. Lowrey, E. Todorov, and S. Kakade.\nTowards generalization and simplicity in\ncontinuous control. In Advances in Neural Information Processing Systems (NIPS), 2017.\n[60] L. A. Rastrigin. About convergence of random search method in extremal control of multi-parameter\nsystems. Avtomat. i Telemekh., 24(11):1467—1473, 1963.\n[61] U. Rosolia and F. Borrelli. Learning model predictive control for iterative tasks. a data-driven control\nframework. IEEE Transactions on Automatic Control, PP(99), 2017.\n[62] D. Sadigh, S. Sastry, S. A. Seshia, and A. D. Dragan. Planning for autonomous cars that leverage eﬀects\non human actions. In Robotics: Science and Systems, 2016.\n[63] T. Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution strategies as a scalable alternative to rein-\nforcement learning. arXiv:1703.03864, 2017.\n[64] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In\nInternational Conference on Machine Learning (ICML), 2015.\n[65] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control\nusing generalized advantage estimation. arxiv:1506.02438, 2015.\n[66] H.-P. Schwefel. Evolutionsstrategie und numerische Optimierung. PhD thesis, TU Berlin, 1975.\n[67] J. Shao and D. Tu. The Jackknife and Bootstrap. Springer Series in Statistics. Springer-Verlag, New\nYork, 1995.\n[68] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural\nnetworks and tree search. Nature, 529(7587):484–489, 2016.\n[69] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient\nalgorithms. In ICML, 2014.\n[70] M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Learning without mixing. In Conference\non Learning Theory (COLT), 2018.\n[71] J. C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approx-\nimation. IEEE Transactions on Automatic Control, 37(3):332–341, 1992.\n[72] A. Strehl, J. Langford, L. Li, and S. M. Kakade. Learning from logged implicit exploration data. In\nAdvances in Neural Information Processing Systems (NIPS), 2010.\n[73] F. Stulp and O. Sigaud. Robot skill learning: From reinforcement learning to evolution strategies.\nPALADYN Journal of Behavioral Robotics, 4(1):49–61, 2013.\n[74] R. S. Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, University of Mas-\nsachusetts, Amherst, 1984.\n[75] R. S. Sutton. Learning to predict by the method of temporal diﬀerences. Machine learning, 3(1):9–44,\n1988.\n[76] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press Cambridge, 1998.\n[77] Y. Tassa, T. Erez, and E. Todorov. Synthesis and stabilization of complex behaviors through online\ntrajectory optimization. In International Conference on Intelligent Robots and Systems (IROS), 2012.\n27\n[78] R. Tedrake, T. W. Zhang, and H. S. Seung. Stochastic policy gradient reinforcement learning on a\nsimple 3D biped. In International Conference on Intelligent Robots and Systems (IROS), 2004.\n[79] G. Tesauro. TD-gammon: A self-teaching backgammon program. In Applications of Neural Networks,\npages 267–285. Springer, 1995.\n[80] E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In International\nConference on Intelligent Robots and Systems (IROS), 2012.\n[81] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Machine learning, 16(3):185–\n202, 1994.\n[82] S. L. Tu and B. Recht. Least-squares temporal diﬀerence learning for the linear quadratic regulator. In\nInternational Conference on Machine Learning (ICML), 2018.\n[83] M. Vidyasagar and R. L. Karandikar. A learning theory approach to system identiﬁcation and stochastic\nadaptive control. Journal of Process Control, 18(3), 2008.\n[84] Y.-S. Wang, N. Matni, and J. C. Doyle.\nA system level approach to controller synthesis.\narxiv:\n1610.04815, 2016.\n[85] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.\n[86] R. J. Williams. Toward a theory of reinforcement-learning connectionist systems. Technical Report\nNU-CCS-88-3, College of Computer Science, Northeastern University, 1988.\n[87] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine Learning, 8(3-4):229–256, 1992.\n[88] Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba. Scalable trust-region method for deep reinforcement\nlearning using Kronecker-factored approximation. arXiv:1708.05144, 2017.\n[89] H. Yu and D. P. Bertsekas. Convergence results for some temporal diﬀerence methods based on least\nsquares. IEEE Transactions on Automatic Control, 54(7):1515–1531, 2009.\n[90] K. Zhou, J. C. Doyle, and K. Glover. Robust and Optimal Control. Prentice Hall, New Jersey, 1995.\n[91] X. Zhu. Semi-supervised learning literature survey. Technical Report 1530, Department of Computer\nSciences, University of Wisconsin, Madison., 2005.\n28\n",
  "categories": [
    "math.OC",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-06-25",
  "updated": "2018-11-10"
}