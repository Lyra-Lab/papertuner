{
  "id": "http://arxiv.org/abs/1807.00051v3",
  "title": "Adversarial Examples in Deep Learning: Characterization and Divergence",
  "authors": [
    "Wenqi Wei",
    "Ling Liu",
    "Margaret Loper",
    "Stacey Truex",
    "Lei Yu",
    "Mehmet Emre Gursoy",
    "Yanzhao Wu"
  ],
  "abstract": "The burgeoning success of deep learning has raised the security and privacy\nconcerns as more and more tasks are accompanied with sensitive data.\nAdversarial attacks in deep learning have emerged as one of the dominating\nsecurity threat to a range of mission-critical deep learning systems and\napplications. This paper takes a holistic and principled approach to perform\nstatistical characterization of adversarial examples in deep learning. We\nprovide a general formulation of adversarial examples and elaborate on the\nbasic principle for adversarial attack algorithm design. We introduce easy and\nhard categorization of adversarial attacks to analyze the effectiveness of\nadversarial examples in terms of attack success rate, degree of change in\nadversarial perturbation, average entropy of prediction qualities, and fraction\nof adversarial examples that lead to successful attacks. We conduct extensive\nexperimental study on adversarial behavior in easy and hard attacks under deep\nlearning models with different hyperparameters and different deep learning\nframeworks. We show that the same adversarial attack behaves differently under\ndifferent hyperparameters and across different frameworks due to the different\nfeatures learned under different deep learning model training process. Our\nstatistical characterization with strong empirical evidence provides a\ntransformative enlightenment on mitigation strategies towards effective\ncountermeasures against present and future adversarial attacks.",
  "text": "arXiv:1807.00051v3  [cs.LG]  30 Dec 2018\nAdversarial Examples in Deep Learning:\nCharacterization and Divergence\nWenqi Wei1\nLing Liu1\nMargaret Loper2\nStacey Truex1\nLei Yu1\nMehmet Emre Gursoy1\nYanzhao Wu1\n1School of Computer Science, Georgia Institute of Technology\n2Georgia Tech Research Institute(GTRI)\nAtlanta, GA, USA 30332\nABSTRACT\nThe burgeoning success of deep learning has raised the security\nand privacy concerns as more and more tasks are accompanied\nwith sensitive data. Adversarial attacks in deep learning have emerged\nas one of the dominating security threat to a range of mission-\ncritical deep learning systems and applications. This paper takes\na holistic and principled approach to perform statistical charac-\nterization of adversarial examples in deep learning. We provide a\ngeneral formulation of adversarial examples and elaborate on the\nbasic principle for adversarial attack algorithm design. We intro-\nduce easy and hard categorization of adversarial attacks to analyze\nthe eﬀectiveness of adversarial examples in terms of attack suc-\ncess rate, degree of change in adversarial perturbation, average en-\ntropy of prediction qualities, and fraction of adversarial examples\nthat lead to successful attacks. We conduct extensive experimental\nstudy on adversarial behavior in easy and hard attacks under deep\nlearning models with diﬀerent hyperparameters and diﬀerent deep\nlearning frameworks. We show that the same adversarial attack be-\nhaves diﬀerently under diﬀerent hyperparameters and across dif-\nferent frameworks due to the diﬀerent features learned under dif-\nferent deep learning model training process. Our statistical char-\nacterization with strong empirical evidence provides a transforma-\ntive enlightenment on mitigation strategies towards eﬀective coun-\ntermeasures against present and future adversarial attacks.\nKEYWORDS\ndeep learning; adversarial examples; eﬀectiveness; divergence\n1\nINTRODUCTION\nDeep learning has achieved impressive success on a wide range of\ndomains like computer vision [18] and natural language process-\ning [7], outperforming other machine learning approaches. Many\nof deep learning tasks, such as face recognition [42], self-driving\ncars [16], speech recognition [14] and malware detection [8], are\nsecurity-critical [2, 34].\nRecent studies have shown that deep learning models are vul-\nnerable to adversarial input at prediction phase [41]. Adversarial\nexamples are the input artifacts that are created from natural data\nby adding adversarial distortions. The purpose of adding such ad-\nversarial noise is to covertly fool deep learning model to misclas-\nsify the input. For instance, attackers could use adversarial exam-\nples to confuse a face recognition authentication camera or a voice\nrecognition system to breach a ﬁnancial or government entity with\nmisplaced authorization [40]. Similarly, a self-driving vehicle could\ntake unexpected action if the vision camera recognizes a stop sign,\ncrafted by adversarial perturbation, as a speed limit sign, or if the\nvoice instruction receiver misinterprets a compromised stop in-\nstruction as a drive-through instruction [4].\nThe threat of adversarial examples has inspired a sizable body\nof research on various attack algorithms [1, 4, 6, 9–11, 19, 20, 26,\n29, 30, 33, 40, 41, 44]. Even with the black box access to the pre-\ndiction API of a deep learning as a service, such as those provided\nby Amazon, Google or IBM, one could launch adversarial attacks\nto the privately trained deep neural network (DNN) model. Due\nto transferability [23, 31, 32, 43], adversarial examples generated\nfrom one deep learning model can be transferred to fool other deep\nlearning models. Given that deep learning is complex, there are\nmany hidden spots that are not yet understood, such blind spots\ncan be utilized as attack surfaces for generating adversarial exam-\nples. Furthermore, adversarial attacks can happen in both train-\ning and prediction phases. Typical training phase attacks inject ad-\nversarial training data into the original training data to mis-train\nthe network model [15]. Most of existing adversarial attacks are\nschemed at prediction phase, which is our focus in this paper.\nTo develop eﬀective mitigation strategies against adversarial at-\ntacks, we articulate that the important ﬁrst step is to gain in-depth\nunderstanding of the adverse eﬀect and divergence of adversarial\nexamples on the deep learning systems. In this paper, we take a\nholistic and principled approach to characterize adversarial attacks\nas an adversarial learning of the input data and a constrained opti-\nmization problem. We dive into the general formulations of adver-\nsarial examples and establish basic principles for adversarial noise\ninjection. We characterize adversarial examples into easy and hard\nattacks based on statistical measures such as success rate, degree of\nchange and prediction entropy, and analyze diﬀerent behavior of\neasy and hard cases under diﬀerent hyperparameters, i.e., training\nepochs, sizes of feature maps and DNN frameworks. Moreover, we\nvisualize the construction of adversarial examples and character-\nize their spatial and statistical features. We present empirical evi-\ndence on the eﬀectiveness of adversarial attacks through extensive\nexperiments. Our principled and statistical characterization of ad-\nversarial noise injection, the eﬀectiveness of adversarial examples\nwith easy and hard attack cases, and the impact of DNN on adver-\nsarial examples can be considered as enlightenment on the design\nof mitigation strategies and defense mechanisms against present\nand future adversarial attacks.\n1\n2\nADVERSARIAL EXAMPLES AND ATTACKS\nWe ﬁrst review the basic concept of DNN model and the threat\nmodel. Then we provide a general formulation of adversarial exam-\nples and attacks, describe the metrics for quantifying adversarial\nobjectives and basic principle of adversarial perturbation.\n2.1\nDNN Model\nLet x ∈X be an input example in the training dataset X. A DNN\nmodel F(x) is made ofn successive layers of neurons from the input\ndata to the outputprediction, and F(x) = fn(Ωn, fn−1(Ωn−1, ...f2(Ω2,\nf1(Ω1,x)))). Each layer represents a parametric function fi,i ∈\n1 . . . n,which computes an activation function, e.g., ReLU, of weighted\nrepresentation of the input from previous layer to generate a new\nrepresentation. The parameter set Ωi,i ∈1 . . . n, consists of sev-\neral weight vectors −→\nWi = [−→\nW ki ]ki ∈1..Ki and bias vectors −→\nBi =\n[−→B ki ]ki ∈1..Ki , where Ki denotes the number of neurons in layer i.\nLet Y = {y1, , ...,ym} be the class label space, where m is the to-\ntal number of labels, and −→y = F(x) be the classiﬁcation prediction\nresult for input x, in the form of m-dimension probability vector\n−→y = {po, ...,pj...,pm}, such that pj indicates the probability gen-\nerated by DNN model toward class yj, 0 ≤pj ≤1 and Ím\nj=0 pj = 1.\nThe predicted label Cx ∈Y is the class with largest probability in\nthe prediction vector. For ease of presentation, we assume that if\nthere are multiple class labels with the same maximal probability,\ni.e., {ys |s ∈1, . . . ,m, ∀j , s ∈{1, . . . ,m}s.t.ps ≥pj }, only one\npredicted class label is chosen for a given example x, denoted by\nCx. The common output layer is a softmax layer, the prediction\nvector is also called logits, and we call the input of the softmax\nlayer the prelogits.\nThe deployment of a DNN model consists of two phases: model\ntraining phase and model-based prediction phase. In training phase,\na set of known input-label pairs (x,yL) is used to train a DNN\nmodel. yL is the known (ground truth) class label of the input x.\nThe DNN ﬁrst uses existing parameters Ωto generate classiﬁcation\nfrom input data forwardly, then computes a loss function J(−→y ,yL)\nthat measures the distance between the prediction vector and the\nground truth label. With a goal of minimizing the loss function, the\nDNN training algorithm updates the parameters Ωat each layer\nusing backpropagation with an optimizer, e.g., stochastic gradient\ndescent (SGD), in an iterative fashion. The trained DNN will be\nreﬁned through testing. Two metrics are used to measure the dif-\nference between the predicted vector and the ground truth label of\nthe test input. One is accuracy that shows the percentage of test\ninput whose predicted class Cx ∈Y is identical with its ground\ntruth label yL. The other is the loss function that computes the dis-\ntance of the predicted class vector −→y to the label yL. The trained\nDNN model produced at the end of the training phase will be used\nfor classiﬁcation prediction. At the prediction phase, the predic-\ntion API sends an input x to the trained DNN model to compute\nits prediction vector −→y and the corresponding predicted label Cx\nvia the set of ﬁxed parameters learned in the training phase.\n2.2\nThreat Model\nInsider Threat: An insider threat refers to white-box attack and\ncompromises from inside the organization that provides the DNN\nmodel based prediction service, such as poisoning attacks during\ntraining phase. Insider adversaries may know the DNN architec-\nture, the intermediate results during the DNN computation, and\nare able to fully or partially manipulate the DNN model training\nprocess, e.g., injecting adversarial samples into the training dataset,\nmanipulating the training outcome by controlling the inputs and\noutputs in some layers of DNN.\nOutsider Threat: An outsider threat refers to black-box attack\nand compromises from external to a DNN model. Such attackers\ncan only access the prediction API of the DNN as a service but\ndo not have access to the DNN training and the trained DNN pre-\ndiction model. However, attackers may have general and common\nbackground knowledge of DNN that is publicly available. We con-\nsider two types of outsider attacks: untargeted or targeted.\nUntargeted Attack is a source class misclassiﬁcation attack,\nwhich aims to misclassify the benign input by adding adversar-\nial perturbation so that the predicted class of the benign input is\nchanged to some other classes in Y without a speciﬁc target class.\nTargetedAttack is a source-target misclassiﬁcation, which aims\nto misclassify the predicted class of a benign example input x to a\ntargeted class in Y by purposely crafting the benign input example\nx via adversarial perturbation. As a result, the predicted class of\nthe input x is covertly changed from the original class (source) to\nthe speciﬁc target class intended by the attacker.\nLet xadv be the maliciously crafted sample of benign input x.\nFigure 1 illustrates the workﬂow of an adversarial example based\noutsider attack to the prediction API of a deep learning service\nprovider, consisting of 7 steps. (1) A benign input x is sent to the\nprediction API, which (2) invokes the trained model to compute the\nprediction result. (3) Upon the API returning the prediction proba-\nbility vector −→y and its predicted class labelCx, (4) the attacker may\nintercept the result and (5) launch an adversarial example based\nattack by generating adversarial example xadv (in one step or it-\neratively). (6) The adversary collects the prediction vector −→y adv\nand the predicted class Cxadv . The iteration stops if the attack is\nsuccessful. (7) The user receives the incorrect result.\n\u0004ĚǀĞƌƐĂƌŝĂů\u0003\u001cǆĂŵƉůĞ\u0003'ĞŶĞƌĂƚŽƌ\n>ŽƐƐ\u0003\nĨƵŶĐƚŝŽŶ\nWƌĞĚŝĐƚŝŽŶ\u0003\nǀĞĐƚŽƌ\u0003ݕԦ\nWƌĞĚŝĐƚĞĚ\u0003ͬ\u0003ƚĂƌŐĞƚ\u0003\nůĂďĞů\u0003ǇΎ\n\u0018ĞĞƉ >ĞĂƌŶŝŶŐ\u0003ĂƐ\u0003Ă\u0003^ĞƌǀŝĐĞ\nWƌĞĚŝĐƚŝŽŶ\u0003\u0004W/\ndƌĂŝŶŝŶŐ\u0003\u0018ĂƚĂĞƐƚ\nŝŶƉƵƚͲůĂďĞů\u0003\nƉĂŝƌ\u0003;ǆ͕\u0003Ǉ>Ϳ\n\u0011ĞŶŝŐŶ /ŶƉƵƚ\u0003ǆ\n\u0018EE\u0003ƚƌĂŝŶŝŶŐ\n&ĞĂƚƵƌĞ\u0003ŵĂƉ͕\u0003͙\nɏŝс΂tŝ͕\u0003\u0011ŝ͕\u0003͙΃\ndƌĂŝŶĞĚ\u0003\n\u0018EE\u0003ŵŽĚĞů\nϭ\nϰ\nϯ\nϮ\nϳ\nϱ\nϲ\nǆ\nǆ\nݕԦ\u0003͕\u0012ǆ\nǆĂĚǀ\nݕԦĂĚǀ͕\u0003\u0012ĂĚǀ\nݕԦ\u0003͕\u0012ǆ\n\u0018ĞĞƉ\u0003ůĞĂƌŶŝŶŐ\u0003hƐĞƌ\nϱ\nϲ\nǆĂĚǀ\nݕԦĂĚǀ͕\u0003\u0012ĂĚǀ\nݕԦ\u0003͕\u0012ǆ\nݕԦĂĚǀ͕\u0003\u0012ĂĚǀ\n͙\n͙\n͙\n͙\nFigure 1: Outsider Adversarial Attack Workﬂow\n2.3\nFormulation of Adversarial Examples\nFor adversaries, an ideal adversarial attack is to construct a per-\nturbed input xadv with minimal distance to x to fool the DNN\n2\nmodel such that the resulting prediction vector is −→y adv with the\npredicted label Cxadv , which is diﬀerent from Cx, and yet its hu-\nman user would still visually consider the adversarial input xadv\nsimilar to or the same as the benign input x and thus believe that\nthe predicted labelCadv should be identical toCx. Let ∆x = dist(x,xadv)\nmeasure the distance between the original input x and the adver-\nsarial input xadv. Example distance metrics can be L0, L2, and L∞\nnorm. For image input, L0 norm describes the number of pixels\nof x that are changed; L2 norm is the Euclidean distance between\ninput x and xadv; and L∞norm denotes the maximum change to\nany pixel of input x. ∆x can be seen as the perturbation noise in-\njected into the benign input to construct an adversarial input by\nthe adversarial attack algorithm. We below provide a general for-\nmulation of an adversarial example based attack:\n∆x = dist(x,xadv)\n(1)\ns.t.\nmin β∆x + att(xadv) (1 −β)д(−→y adv,y∗)\n(2)\nx ∈X, xadv < X\nCxadv , Cx, Cx ∈Y, Cxadv ∈Y/Cx\nHCxadv = HCx\nwhereatt(xadv) is a ﬂag of untargeted and targeted attack:att(xadv)=-\n1 when the attack is untargeted and att(xadv)=1 when the attack\nis targeted.д(−→y adv,y∗) is some objective function the attack seeks\nto optimize. Parameter 0 ≤β ≤1 controls the relative importance\nof the perturbation and the objective function. HCxadv = HCx\nmeans that human perceptional class of adversarial input needs to\nbe the same as that of the original benign input.\nFor untageted attack, the label y∗denotes the predicted class\nCx of the benign input x such that д(−→y adv,Cx) measures the dis-\ntance between the prediction vector of adversarial input xadv and\nCx. This distance is to be maximized to make the adversarial in-\nput successfully misclassiﬁed, thus achieving the goal of predict-\ning the label Cxadv of adversarial input as any label but not the\nlabel of original benign input x. Accordingly, the perturbation ∆x\nonly concerns the prediction vector of adversarial example and the\npredicted class of benign example, deﬁned as δx(−−−→\nyadv,Cx).\nFor targeted attack, the label y∗denotes the target label yT so\nthat д(−→y adv,yT ) is some objective function that measures the dis-\ntance between the prediction vector of adversarial input xadv and\nthe attack target label yT . The attack is to minimize this distance\nso that the adversarial input xadv, generated by maliciously in-\njecting perturbation to x, is classiﬁed as the target label with high\nconﬁdence. In contrast to untargeted attack, which only needs to\nlower the probability conﬁdence on the original labelCx, the target\nattack needs to enhance the probability conﬁdence on the target la-\nbel while mitigating the probability conﬁdence for all other labels\nso that the target label could stand out. This makes targeted attack\nmuch harder than untargeted ones. Thus, the adversarial noise ∆x\nis generated from perturbation function δx(−→y ,Cx,yT ).\nExamples of objective function in Equation 2 include (1) L2 loss,\n(2) cross-entropy loss, loss like (3) and their variants.\nloss(1) =\nÕ\nj ||pj −qj ||2,\npj ∈−→y adv, qj ∈−→y ∗\nloss(2) = −\nÕ\nj qj logpj,\npj ∈−→y adv, qj ∈−→y ∗\nloss(3) = max(0, maxpj −q∗),\npj ∈−→y adv, q∗= Cx oryT\nwhere −→y j is a one-hot prediction vector whose prediction class has\nthe probability of 1 and the rest is 0, i.e., pj = 1, ∀k , j, pk = 0.\nEquation2 aims to optimize the objective function and the amount\nof perturbation at the same time while the parameter β controls\nthe relative importance of these two items. When β = 1, the prob-\nlem is centered on minimizing the amount of perturbation while\nmaintaining human imperceptibility. Jacobian-based attack [33] is\nan example. When β = 0, Equation 2 aims at optimizing the objec-\ntive function without any regulation on the amount of perturba-\ntion. It may suﬀer from over-crafting and lead to violation of hu-\nman imperceptibility [40]. When 0 < β < 1, Equation 2 minimizes\nthe amount of perturbation and the objective function together to\navoid either of them dominating the optimization problem. Since\nlarge noise would increase the probability of attack being detected\nby human, the ﬁrst term of Equation 2 acts as regularization of\nthe second. In fact, the objective function could have an additional\nregularizer. Example attacks when 0 < β < 1 are the Fast Gra-\ndient Sign Method (FGSM) [11] and those optimization-based at-\ntacks [6, 10, 41].\n2.4\nAdverse Eﬀect of Adversarial Examples\nWe propose to use the following three evaluation metrics to ana-\nlyze and compare the adverse eﬀect of adversarial examples: Suc-\ncess Rate(SR), Degree of Change (DoC), and information entropy.\nSR measures the percentage of adversarial examples that result\nin successful attacks in all adversarial input data generated by us-\ning the adversarial example generator.\nSR = successful adversarial examples\nall adversarial examples\nAn adversarial example xadv is considered successful when un-\nder limited amount of perturbation noise, the following conditions\nhold: (1) Cadv , Cx; and (2) let −→y adv = {p1, . . . ,pm}, for untar-\ngeted attack, ∃k ∈1, . . . ,m, we have pk ≥pCx ; and for targeted\nattack, pyT is the maximum probability in the prediction vector\n−→y adv such that ∀j ∈1, . . . ,m, j , yT , pyT ≥pj and pyT > pCx .\nBy using SR, we are able to statistically measure how hard it is to\nmove the original class of an input into another class. The higher\nthe SR is, the more adverse eﬀect the adversarial attack can cause.\nHowever, the SR alone is not suﬃcient to characterize the adverse\neﬀect of adversarial example. This motivates us to introduce per-\nclass SR and other metrics. Per-class SR measures the percentage\nof adversarial inputs that are successfully misclassiﬁed among the\ntotal adversarial examples generated from one class.\nDoC describes the distance between adversarial example xadv\nand original benign input x, which is the objective that adversarial\nattacks aim to minimize in Equation 2. Let ∆x = ||xadv −x||p be\n3\nthe Lp distance of the two inputs, and the DoC is computed as\nDoC = 1\nN\nÕN\n1\n∆x\n||x||p),xadv ∈Xadv, |Xadv | = N\n(3)\nwhere ||x||p is the p norm of the input data x. p can be 0, 2, ∞.\nFor example, if the perturbation noise changes 30 pixels in a 28*28-\npixel image, the DoC for this perturbed image is\n30\n28∗28 = 0.038 un-\nder L0 distance. If the dataset has 100 maliciously crafted images,\ni.e., |Xadv | = N = 100, we add each of their DoC values and com-\npute their average. From the perspective of adversaries, there are\nseveral advantages in keeping DoC theoretically the smallest. First,\nit means that less eﬀort is required to launch the adversarial at-\ntack in a fast and eﬃcient manner. Second, the minimal amount of\nchange is one way to satisfy HCxadv = HCx, making the attack hu-\nman imperceptible and hard to detect. In contrast, randomly added\nperturbation can be ineﬃcient for adversaries with high risk of\nmaking more eﬀort adding noise with low SR.\nThe third metric is information entropy [39], deﬁned as the av-\nerage amount of information produced by a stochastic source of\ndata. We compute the entropy on the distribution of probabilities\nin −→y .\nΦ = −\nÕm\nj=1 pjlog2pj,\npj ∈−→y\n(4)\nThe more even the probability is distributed, the larger the entropy\nis. Generally, entropy refers to disorder or uncertainty of a distribu-\ntion. Thus the prediction vector of [0.2, 0.8, 0, 0, 0, 0, 0, 0, 0, 0] has\nlarger entropy than that of [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], because on the\nprediction of the second class, the latter is certain with a probabil-\nity of 1 whereas the former is less certain with a probability of 0.8.\nWe also use the average information entropy, which is deﬁned as\nfollows: for N input images, we compute their individual informa-\ntion entropy, accumulate the entropy of all N inputs, and divide\nthe entropy sum by N . The average entropy serves as a good alter-\nnative indicator to diﬀerentiate attacks with high and low SR.\n2.5\nBasic Principle of Perturbation Injection\nThe goal of adversarial perturbation is to make DNN model to mis-\nclassify an input by changing the objective function value based on\nits gradients on the adversarial direction. Given the trained DNN\nhas ﬁxed parameters, the update should be on the benign input\nx and be minimized so that only small amount of perturbation is\nadded to the input x to keep the human imperceptibility intact.\nSuch adversarial perturbation update can be done in either one-\nstep (t = 1) or multiple steps iteratively (t > 1):\nxt\nadv =\n\n\nx + θR( ∂h(−→y ,y∗)\n∂x\n, t = 1,\nxadvt−1 + θR( ∂h(−→y\nt−1\nadv,y∗)\n∂xadv t−1\n), t > 1,\n(5)\nFor untargeted attacks, y∗= Cx. For targeted attacks, y∗= yT . θ\ncontrols how much to update at a time. h() is a function that de-\nscribes the relation between the prediction vector −→y adv of adver-\nsarial input xadv and some attack objective class y∗. h() concerns\nthe prediction vector or loss function of the original input or the\nadversarial input in the previous iteration, which is the attack spot\nfor adversarial perturbation update. In contrast, д( ) focuses on the\nprediction vector of the current adversarial input, which indicates\nthe adversarial attack objective to be optimized. R( ) is the craft-\ning rule that is based on partial gradient of function h(), which\nimplies how the perturbation is added. R( ) also ensures that the\nperturbation is clipped. Clip refers to a value constraint so that the\nperturbation cannot go beyond the range of the feature value. For\nimage, if the perturbation increases the value of a pixel beyond 255,\nthen the pixel value is clipped to 255. The update termθR( ) has the\nsame size as the input x when t = 1 and as the previous adversarial\ninput xt−1\nadv when t > 1.\nOne-step v.s. Multi-step Adversarial Examples. One-step\nattack is fast but excessive noise may be added to the benign in-\nput unnecessarily, because it is diﬃcult to calibrate exactly how\nmuch noise is needed for successful attack. One-step attack puts\nmore weight on the objective function and less on minimizing the\namount of perturbation. The partial gradient of h(−→y adv,y∗) in\nEquation 5 only points out the direction of change and the relative\nimportance of diﬀerent places to perturb in the input data. Unlike\none-step attack which has only one update on the original benign\ninput, the multi-step attack uses a feedback-loop that iteratively\nmodiﬁes the input with more carefully injected perturbation un-\ntil the input is successfully misclassiﬁed or the maximum pertur-\nbation threshold is reached to ensure the human imperceptibility\n(HCx = HCadv ). Although iterative attack is computationally more\nexpensive, the attack is more strategic with high SR and less pertur-\nbation. The multi-step attack strikes a good balance on minimizing\nthe amount of perturbation and the objective function.\nLoss Function as the objective function. The loss function\nJ(−→y ,y∗) measures the distance between the prediction vector −→y\nand the attack destination class y∗. When ∂J(−→y ,y∗)\n∂x\n= 0, it leads\nto minimal (local minimal) value of the convex (non-convex) loss\nfunction. When ∂J(−→y ,y∗)\n∂x\n> 0 , adding values to input x will in-\ncrease the value of loss function. However, when ∂J(−→y ,y∗)\n∂x\n< 0,\nadding values to input x will decrease the value of loss function.\nThus, manipulating the input x could change the loss function.\nTypical attacks that utilize the gradient of loss function include\none step untargeted/targeted FGSM [11], untargeted/targeted Iter-\native Method [19], and the attack in [40]. To attack an image input\nby following [11], we build a noise map based on the gradient of\nloss function and a simple pixel-based crafting rule R( ), in which\nthe pixel value is set to 0 (dark) if the gradient of loss function at\nthat pixel position is below zero, to 127 if the gradient is zero, and to\n255 (light) if the gradient is above zero. We are able to perform un-\ntargeted attacks by controlling the amount of noise injected using\nθ. Figure 2 shows three adversarial examples (on the right) gener-\nated by applying the same perturbation noise (middle) to the same\noriginal input on the left under diﬀerent θ values. When θ = 0.05,\nthe attack fails. As we increase θ to 0.1, the attack successfully mis-\nclassiﬁes the input image of digit 1 to the class of digit 7, and such\nmisclassiﬁcation is imperceptible as we visually still see the per-\nturbed image on the right as digit 1 image without much change\n(HCx = HCadv ). By further increasing θ to 0.2, the attack remains\nsuccessful, but it misclassiﬁes the same input image of digit 1 to\nthe class of digit 8 instead. In Figure 3, we use a diﬀerent input\nimage of digit 1, with the same θ = 0.2 and the same algorithm to\nadd perturbation noise produced in the same way as in Figure 2.\n4\nHowever, the attack is unsuccessful and the prediction vector indi-\ncates that the predicted class for the maliciously crafted example\nis still digit 1. This experiment shows that the success of attack is\ninﬂuenced both by θ and by the input instance. Furthermore, Fig-\nure 2 shows that both entropy and loss will grow with the larger\ngradient-based noise. Larger entropy indicates more even distribu-\ntion of the probabilities in the prediction vector. Larger loss indi-\ncates that the prediction vector of adversarial input is more erro-\nneous when compared with the predicted label of the benign input.\nThis empirical evidence also indicates that the entropy of the pre-\ndiction vector and the loss is also a good metric for characterizing\nthe eﬀectiveness and divergence of attacks.\nн\u0003ɽ 灤\n/ŶƉƵƚ\u0003ǆ\n,\u0012ǆсϭ\n\u0012ǆсϭ\nݕԦс\u0003΀Ϭ͕\u0003ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ΁\n,\u0012ǆĂĚǀсϭ\nݕԦс\u0003΀Ϭ͘Ϭϴ͕\u0003Ϭ͘ϭϮ͕\u0003Ϭ͘ϭϬ͕\u0003Ϭ͘Ϭϵ͕\u0003Ϭ͘ϭϬ͕\u0003Ϭ͘Ϭϵ͕\u0003Ϭ͘Ϭϵ͕\u0003Ϭ͘ϭϭ͕\u0003Ϭ͘ϭϯ͕\u0003Ϭ͘ϭϬ΁\nƐƵĐĐĞƐƐĨƵů\u0003\nĂƚƚĂĐŬ͕\u0003\u0012ǆĂĚǀсϴ\n,\u0012ǆĂĚǀсϭ\nݕԦс\u0003΀\u0013\u0011\u0013\u0015\u000f\u0003\u0013\u0011\u001b\u0014\u000f\u0003\u0013\u0011\u0013\u0016\u000f\u0003\u0013\u0011\u0013\u0014\u000f\u0003\u0013\u0011\u0013\u0015\u000f\u0003\u0013\u0011\u0013\u0014\u000f\u0003\u0013\u0011\u0013\u0015\u000f\u0003\u0013\u0011\u0013\u0018\u000f\u0003\u0013\u0011\u0013\u0014\u000f\u0003\u0013\u0011\u0013\u0016@\n,\u0012ǆĂĚǀсϭ\nݕԦс\u0003΀\u0013\u0011\u0013\u001a\u000f\u0003\u0013\u0011\u0014\u0014\u000f\u0003\u0013\u0011\u0014\u0013\u000f\u0003\u0013\u0011\u0013\u0018\u000f\u0003\u0013\u0011\u0013\u001b\u000f\u0003\u0013\u0011\u0013\u0019\u000f\u0003\u0013\u0011\u0013\u001a\u000f\u0003\u0013\u0011\u0015\u001c\u000f\u0003\u0013\u0011\u0013\u001b\u000f\u0003\u0013\u0011\u0013\u001c@\u0003\nɽ\u0003сϬ͘ϭ\nɽ\u0003сϬ͘Ϭϱ\n\u0004ĚǀĞƌƐĂƌŝĂů\u0003\nŝŶƉƵƚ\u0003ǆĂĚǀ\nƐƵĐĐĞƐƐĨƵů\u0003\nĂƚƚĂĐŬ͕\u0003\u0012ǆĂĚǀсϳ\nƵŶƐƵĐĐĞƐƐĨƵů\u0003\nĂƚƚĂĐŬ͕\u0003\u0012ǆĂĚǀсϭ\nɽсϬ͘Ϯ\nθ=0\nθ=0.05\nθ=0.1\nθ=0.2\nentropy\n0\n1.30\n3.09\n3.32\nloss\n0\n1.23\n2.13\n2.31\nFigure 2: An illustration of loss function based attack (θ=0.05, 0.1,\n0.2), pixel values are added in light area of the perturbation, are de-\nducted in dark area, and do not change in gray area.\nн\u0003Ϭ͘Ϯ\u0003灤\n/ŶƉƵƚ\u0003ǆ\n\u0004ĚǀĞƌƐĂƌŝĂů\u0003\nŝŶƉƵƚ\u0003ǆĂĚǀ\n,\u0012ǆсϭ\n\u0012ǆсϭ\nݕԦс\u0003΀Ϭ͕\u0003ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ΁\n,\u0012ǆĂĚǀсϭ\n\u0012ǆĂĚǀсϭ\nݕԦс\u0003΀Ϭ͕\u0003Ϭ͘ϵϮ͕\u0003Ϭ͘Ϭϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͕\u0003Ϭ͘Ϭϳ͕\u0003Ϭ΁\nFigure 3: An unsuccessful loss function based attack (θ=0.2)\nOptimization of loss function with regularization is an exten-\nsion of the loss function based attacks using adversarial examples.\nCW attack [6] and RP2 attack [10] are examples of such attacks.\nThe beneﬁt of controlling the amount of change with a regularizer\nis to avoid drastic change. However, the optimization problem is of-\nten solved via techniques like Adam optimizer, which introduces\nconsiderable computation overhead.\nPrediction vector based objective function. Similarly, for\nprediction vector, adding noise by increasing values on input x\nwill make the prediction vector value on class j go up if the par-\ntial gradient of prediction vector on class j is greater than zero,\ni.e., ∂yj\n∂x\n> 0. Analogously, ∂yj\n∂x\n< 0 means increasing the value\nof input will make the prediction vector value on class j go down.\nWhen ∂yj\n∂x = 0, changing the input a little bit will not change the\nprediction vector at all. The same attack principle can be applied\nto prelogits as well, as prediction vector can be seen as normalized\nand diﬀerence-ampliﬁed prelogits. Example attacks that utilize the\ngradient of prediction vector are Jacobian-based attack [33] and\nDeepfool [29]. In fact, the gradient of loss function can be seen as\nan extension of gradient of prediction vector due to the chain rule\nsince loss function is computed from prediction vector.\nIn summary, based on the inherent relation among input data,\nthe gradient of loss function, and prediction vector, we are able to\nestablish basic attack principle for untargeted and targeted adver-\nsarial attacks. For untargeted attack, the goal is to craft the image\nin a way to lower the probability or conﬁdence of the original class\nof the input until it is no longer the largest probability in the pre-\ndiction vector. There are three ways to do so: (1) The adversaries\ncould increase the loss function J(−→y ,yCx ), pushing the prediction\n−→y away from the predicted class Cx of the benign input. When\nthe loss function is large enough, the prediction will be changed\nto some destination class other than Cx. (2) One can decrease the\nvalue of yCx in prelogits (feature vector) or logits (prediction vec-\ntor), until this value is no longer the largest among all classes, the\nDNN prediction would misclassify the adversarial input. (3) By ex-\ntending the loss function with a regularizer for the added noise,\nthe objective function is optimized by increasing the loss function\nbetween the prediction vector of xadv and yCx , while minimizing\nthe impact of perturbation.\nSimilarly, there are three methods for targeted attack: (1) The ad-\nversary may decrease the loss function J(−→y ,yT ) with perturbation,\nso that the crafting process is to perturbthe input toward the target\nclass yT . (2) The adversary may increase the value of the prelogit\nor prediction vector of yT until it becomes the largest one, so that\nthe DNN would misclassify the input into the target class. (3) The\nadversary extends loss function with optimization and decreases\nthe loss function J(−→y ,yT ) while balancing the added noise.\nTable 1 lists representative untargeted (U) and targeted (T) at-\ntacks with respect to noise origin, distance norm, human percep-\ntion (HP), and one-step or multi-step attack algorithm.\nAttack\ntype\nnoise origin\ndistance norm\nHP constrain\niteration\nFGSM\nU/T\nloss function\nL∞\nθ\none\nIterative Method\nU/T\nloss function\nL∞\nθ\nmultiple\nJacobian-based Attack\nU/T\nprelogit & prediction\nL0\nmax iteration\nmultiple\nCW [6]\nU/T\noptimization\nL0, L2, L∞\nregularizer\nmultiple\nRP2 [10]\nU/T\noptimization\nL0, L2, L∞\nregularizer\nmultiple\nTable 1: Example Adversarial Attacks and Algorithms\n3\nONE-STEP ADVERSARIAL EXAMPLES\n3.1\nOne-Step Attack Generation\nWe characterize one-step generation of adversarial examples on\ntheir attack eﬀectiveness and divergence using FGSM. All experi-\nments are conducted on MNIST dataset [21] with TensorFlow [22].\nThe adversarial update of FGSM is a special case of Equation 5 with\nthe crafting rule R( ) deﬁned by function siдn():\nxadv =x + θsiдn ( ∂J(−→y ,yCx )\n∂x\n),\nuntargeted\nxadv =x −θsiдn ( ∂J(−→y ,yT )\n∂x\n),\ntargeted\nIn FGSM, θ serves as the DoC in L∞distance and controls the\namount of injected noise. Note that DoC in L0 distance is diﬀerent\nacross instances as the gradient of loss function is determined by\n5\nthe input instance and the DNN model. The objective function h(),\nwhich is the loss function, is to be optimized to achieve the attack\ngoal. The rule of perturbation noise injection depends on siдn(),\nwhich control the direction of the perturbation according to the\nobjective of the loss function based attack. For untargeted attack,\npixel values should be decreased if ∂J(−→y ,yCx )\n∂x\n<0, and pixel values\nshould be increased if ∂J(−→y ,yCx )\n∂x\n>0. Both are controlled by siдn()\nand aim at increasing (maximizing) the loss function between the\npredicted vector and the predicted class label of the benign input\nx, which causes misclassiﬁcation on xadv.\nBy slightly changing the crafting rule, targeted FGSM attacks\ncan be established [19]. The diﬀerence is that the loss function for\ntargeted attack is deﬁned between the prediction vector of a benign\ninput x and the target class of the attack. The direction of change\nis to decrease (minimize) the loss function so that the prediction\nmoves towards the target class. We visualize the gradient of loss\nfunction for targeted FGSM attack in Figure 4. It shows the pixel\nposition whose value is to be increased when ∂J(−→y ,yCx )\n∂x\n<0 (dark\narea) and decreased when ∂J(−→y ,yCx )\n∂x\n>0 (light area). Compared to\nuntargeted attacks (recall Figure 2 and Figure 3), Figure 4 shows\nthat generating an adversarial example for successful targeted at-\ntack is much harder. When the targeted classes are set to digit 0, 4,\n5, 6, and 9 respectively, we vary θ from 0.1 to 0.4 with an interval\nof 0.05, the attacks are unsuccessful no matter how θ is set. When\nthe targeted classes are set to digit 2, 3, 7, and 8 respectively, the\nattack succeeds at diﬀerent noise level (θ). The easiest target class\nwith smallest noise level (θ = 0.1) is digit 8, followed by digit 7\n(θ = 0.15), then digit 2 and digit 3 (θ = 0.3). Larger θ beyond 0.4\nwill cause clear violation of HCx , HCadv .\nTakeaway Remarks. The eﬀectiveness of one-step attack heav-\nily relies on θ, which is the only parameter the adversary could\nﬁne-tune once the crafting rule R( ) is ﬁxed. For untargeted attack,\nlarger θ leads to larger update, and thus may result in over-crafting\nand violate the minimal amount of perturbation constraint. Also,\nlarge perturbation noise may dominate the original input, violating\nthe human imperceptibility constraint. For targeted attack, only\nthe correct amount of noise can lead to successful targeted mis-\nclassiﬁcation. Over-crafting may cause the prediction go beyond\nthe decision boundary of the target class, thus fail the attack. On\nthe other hand, smaller θ leads to smaller perturbation, and higher\nchances of constructing adversarial examples that are not suﬃ-\nciently perturbed to make attacks successful in one step.\nAs we have demonstrated in Figure 2 and Figure 3, θ is an im-\nportant control parameter for the eﬀectiveness of one step FGSM\nattack. For the same input, larger θ tends to increase SR. However,\nlarger θ successful for attacking one input may not guarantee the\nsuccess of attacking another input of the same class. Our analy-\nsis and characterization calibrate the eﬀectiveness and divergence\nof adversarial examples with three interesting ﬁndings: (1) Diﬀer-\nent input images of the same class (e.g., digit 1) may have diﬀer-\nent attack eﬀectiveness even with the same level of noise (same\nθ) under the same attack method. This reﬂects the inherent prob-\nlem of using ﬁxed crating rules for all instances. The robustness\nagainst adversarial perturbation is diﬀerent across input instances.\n(2) For any benign input, there are more than one way to gener-\nate successful adversarial examples (e.g., using diﬀerent θ values)\nusing the same attack method. (3) Diﬀerent levels of noise (vary-\ning θ) may lead to diﬀerent attack eﬀectiveness for the same input\nsince the attack is not successful when θ = 0.05. Also, two diﬀer-\nent but successful attacks to the same benign input may lead to\ninconsistent misclassiﬁcation results (recall Figure 2, Cx = 1 →\nCxadv = 7(θ = 0.1) v.s. Cx = 1 →Cxadv = 8(θ = 0.2)). Moreover,\ndiﬀerent images from the same class can be misclassiﬁed into dif-\nferent destination classes (shown in Table 2). It is critical to study\nsuch non-deterministic nature of adversarial examples for defense\nmechanism design.\n3.2\nCharacterization of One-step Attack\nWe have shown that it is hard to balance between the amount of\nnoise added and the change of loss function value in one step at-\ntack. In this section, we further characterize the eﬀectiveness and\ndivergence of one-step adversarial examples by introducing a bi-\nnary classiﬁcation in terms of easy and hard attack categories for\nall successful attacks. This allows us to gain deeper understanding\nof bothθ and the crafting rule R( ) with respect to SR, DoC, entropy,\nand the fraction of misclassiﬁed adversarial examples.\nWe observe that the hardness for changing the predicted class\nof an input into another destination class varies for diﬀerent desti-\nnation classes under untargeted or targeted attacks. We can dis-\ntinguish successful attacks with higher SR as easy and eﬃcient\nattacks and successful attacks with lower SR are hard and ineﬃ-\ncient ones. Table 2 shows a statistical characterization of easy and\nhard cases in one-step attacks with θ = 0.2. The diagonal shows\nthe percentage of adversarial examples that fail the attacks. We\nconsider two types of hardness in terms of diﬀerent SRs: attack\nhardness based on source (S) classes and destination (D) classes.\nFor the attack hardness on source classes, it is observed that the\nSR of digit 1 is as high as 0.995, and the SR of digit 2 is as low as\n0.771, indicating that 0.995 ∗1135 = 1129 images of digit 1 and\n0.771 ∗1032 = 796 images of digit 2 succeed in source misclassi-\nﬁcation attack. We consider those attacks whose source SRs are\nrelatively higher as one kind of easy cases. However, within each\nsource class, diﬀerent fractions of its N images are misclassiﬁed\ninto diﬀerent destination classes, and such distribution is highly\nskewed for some cases. For source class of digit 1, the highest frac-\ntion that are misclassiﬁed into a particular destination class, i.e.,\ndigit 8, is 0.524 (1135 ∗0.524 = 595 images), and the rest of the des-\ntination classes have signiﬁcantly smaller fractions, ranging from\n0.021 to 0.123. This indicates that the destination class of untar-\ngeted attacks is not uniformly random. We regard those successful\nattacks whose destination classes have higher fractions as another\nkind of easy cases.\nWe ﬁrst study the hardness in terms of source class. Although\nthe per-class SR for each source class (e.g., digit 1) is diﬀerent, Ta-\nble 2 shows that one-step attack is highly eﬀective and all SRs are\nrelatively high with θ=0.2. To better understand the attack hard-\nness of source classes, we gradually lower the θ value from 0.2 to\n0.1 and 0.05. Figure 5 shows the comparison of SR under diﬀerent\nθ. We highlight two observations. First, the SRs of all digits drop\nsharply as we decrease the θ. Also when θ = 0.05, all source class\n6\ndĂƌŐĞƚ\u0003ůĂďĞů\u0003Ǉ>\nϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003Ϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϰ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϱ\u0003\u0003\u0003\u0003\u0003\nϲ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϳ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϴ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϵ\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϯϬ\nz͕\u0003ɽсϬ͘ϯϬ\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nz͕\u0003ɽсϬ͘ϭϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003 z͕ ɽсϬ͘ϭϬ\u0003\u0003\u0003\u0003\u0003\u0003\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nн ɽ 灤\nс\u0003ǆĂĚǀ\nWĞƌƚƵƌďĂƚŝŽŶ\u0003ŶŽŝƐĞ\u0003ȴǆ\n/ŶƉƵƚ\u0003ǆ\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003\nŶŽŝƐĞ\n\u0004ĚǀĞƌƐĂƌŝĂů\u0003ŝŶƉƵƚ\u0003ǆĂĚǀ\nFigure 4: Visualization of Loss Function-Based Noise Injection for targeted FGSM attack\nS\\D\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSR\n# image\n0\n0.058\n0.018\n0.067\n0.004\n0.015\n0.691\n0.037\n0.027\n0.082\n0.001\n0.942\n980\n1\n0.068\n0.005\n0.123\n0.092\n0.015\n0.078\n0.006\n0.068\n0.524\n0.021\n0.995\n1135\n2\n0.010\n0.001\n0.229\n0.052\n0.049\n0.341\n0.018\n0.065\n0.208\n0.027\n0.771\n1032\n3\n0.032\n0.052\n0.192\n0.178\n0.022\n0.272\n0\n0.031\n0.093\n0.128\n0.822\n1010\n4\n0.004\n0.017\n0.110\n0.063\n0.048\n0.045\n0.014\n0.049\n0.571\n0.079\n0.952\n982\n5\n0.047\n0.006\n0.142\n0.100\n0.074\n0.135\n0.029\n0.016\n0.341\n0.110\n0.865\n892\n6\n0.056\n0.070\n0.217\n0.013\n0.116\n0.156\n0.041\n0.004\n0.307\n0.02\n0.959\n958\n7\n0.031\n0.030\n0.314\n0.047\n0.018\n0.018\n0.001\n0.117\n0.385\n0.039\n0.883\n1028\n8\n0.020\n0.033\n0.201\n0.031\n0.094\n0.380\n0.015\n0.040\n0.158\n0.028\n0.842\n974\n9\n0.005\n0.016\n0.336\n0.013\n0.046\n0.194\n0.003\n0.210\n0.168\n0.009\n0.991\n1009\nTable 2: Untargeted FGSM Attack (θ=0.2): the cell at ith row and jth\ncolumn represents the fraction of adversarial inputs misclassiﬁes\nsource class in ith row to destination class in jth column.\nattacks become hard as all SRs are smaller than 0.35. Second, Dig-\nits 1 and 9 have higher SRs consistently with all three settings of θ\ncompared to other digits. They are source class easy attacks under\nFGSM. We view this type of hardness as the vulnerability of the\nsource class.\n0.942\n0.995\n0.771\n0.822\n0.952\n0.865\n0.959\n0.883\n0.842\n0.991\n0.526\n0.713\n0.238\n0.369\n0.554\n0.329\n0.613\n0.335\n0.352\n0.656\n0.172\n0.325\n0.026\n0.063\n0.184\n0.089\n0.213\n0.11\n0.137\n0.261\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nθ =0.2\nθ =0.1\nθ =0.05\nFigure 5: SR of untargeted FGSM with Diﬀerent θ: x-axis denotes\nthe 10 classes and y-axis denotes SR.\nThe average information entropy is another statistical indicator\nto show the eﬀectiveness of adversarial examples and the distribu-\ntion of the prediction vectors. Table 3 shows the average entropy\nof source classes under untargeted FGMS attack with diﬀerent θ\nvalues. Clearly, the more vulnerable a source class is, or the more\nsuccessful the source misclassiﬁcation attack is, the higher entropy\nis, showing more even distribution of the probabilities in the pre-\ndiction vector(s). Note that without attack, the entropy is as low as\nan order of 10−7. From both Figure 5 and Table 3, we can see that\nthe most vulnerable source class is digit 1, followed by digits 9, 6,\n4, 0, whose SRs are above 0.9. The next set of digits with SR above\n0.8 is 8, 7, 5, 3, with digit 2 having the lowest SR of 0.773, which\nindicates that 234 inputs out of 1032 were failed under FGSM. We\nobserve that FGSM attacks with high SRs result in more evenly\ndistributed probabilities in prediction vector and larger entropy.\nWe next examine the second type of attack hardness with re-\nspect to destination (D) classes. Table 4 lists the top three easy and\ntop three hard attacks. For source class digit 1, the top 3 easy des-\ntination classes are target digits 8, 2, and 3 with fraction of 0.524,\n0.123 and 0.092 misclassiﬁed into digit 8 (1135*0.524=595 images),\nθ \\ S\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.05\n1.46\n1.96\n0.54\n1.47\n1.3\n1.06\n1.24\n0.93\n2.17\n2.26\n0.1\n2.72\n2.82\n1.37\n2.64\n2.54\n2.16\n2.3\n2\n3.02\n3\n0.2\n2.97\n3.27\n2.44\n3.06\n3.12\n2.89\n2.96\n2.83\n3.11\n3.12\nTable 3: Entropy of FGSM under diﬀerent θ\ndigit 2 (1135*0.123=139 images), and digit 3 (1135*0.092=104 im-\nages) respectively. This indicates that the fraction of misclassiﬁed\nadversarial examples is a good indicator for characterization of the\neﬀectiveness of attacks.\nS\nEasy 1\nEasy 2\nEasy 3\nHard 1\nHard 2\nHard 3\n0\n5/0.691\n8/0.082\n2/0.067\n9/0.001\n3/0.004\n4/0.015\n1\n8/0.524\n2/0.123\n3/0.092\n6/0.006\n4/0.015\n9/0.021\n2\n5/0.341\n8/0.208\n3/0.052\n1/0.001\n0/0.01\n6/0.016\n3\n5/0.272\n2/0.192\n9/0.128\n6/0.0\n4/0.025\n7/0.031\n4\n8/0.571\n2/0.11\n9/0.079\n0/0.004\n6/0.014\n1/0.017\n5\n8/0.341\n2/0.142\n3,9/0.11\n1/0.006\n7/0.016\n6/0.029\n6\n8/0.307\n2/0.217\n5/0.156\n7/0.004\n3/0.013\n0/0.055\n7\n8/0.385\n2/0.314\n3/0.047\n6/0.001\n4/0.018\n5/0.018\n8\n5/0.38\n2/0.201\n4/0.094\n6/0.015\n9/0.028\n0/0.033\n9\n2/0.336\n7/0.21\n5/0.194\n6/0.003\n0/0.005\n3/0.013\nTable 4: Top 3 Easy & Hard Attacks under untargeted FGSM: each\ncell indicates the destination class digit and the fraction of adver-\nsarial examples being misclassiﬁed into that destination class.\n4\nMULTI-STEP ADVERSARIAL EXAMPLES\nIn one-step generation of adversarial example, θ indicates to what\nextent an adversarial input is perturbed in one shot when the craft-\ning rule is ﬁxed. However, determining a right θ is non-trivial.\nEspecially, smaller θ may lead to low SR or failure in one step\nattack. One remedy of achieving high attack SR with small θ is\nto use a multi-step boosting method. The iteration process termi-\nnates when the misclassiﬁcation goal is reached or when the per-\nturbation violates HCx = HCadv . The multi-step approach could\nincrease SR signiﬁcantly and make hard attacks at one-step eas-\nier. Table 5 shows the SR of multi-step iterative attack under un-\ntargeted FGMS with θ = 0.005, comparing the three settings of\niterations. Since the loss function is computed at each iteration to\nﬁne-tune the attack direction, the noise injection is not simply re-\npeating the previous iteration. With a few iterations, the attacker\ncan signiﬁcantly enhance SR. Hard cases in one-step attack may\nrequire more iterations to achieve certain SR goal comparable to\none-step easy cases with high SR. Thus, the number of iterations\ncan be a good indicator of the attack hardness.\niter \\ S\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n0.172\n0.325\n0.026\n0.063\n0.184\n0.089\n0.213\n0.11\n0.137\n0.261\n3\n0.796\n0.921\n0.751\n0.789\n0.897\n0.793\n0.903\n0.843\n0.775\n0.960\n5\n0.988\n0.997\n0.924\n0.959\n0.971\n0.935\n0.982\n0.976\n0.937\n0.998\nTable 5: SR of Multi-step FGSM (θ = 0.005).\n7\nFor targeted FGSM attacks, however, multi-step attack is less ef-\nfective. The experiment conducted to attack input images of digit\n1 fails to reach the target digit 0 for all 1135 images in MNIST. This\nis consistent with one-step targeted FGSM in Figure 4, where one-\nstep targeted attack is not successful even when θ reaches 0.4. Con-\nsequently, boosting small θ iteratively may not improve SR of the\nattack when the attack under large θ is not successful. In addition\nto tuning θ, the crafting rule may also need to be reﬁned iteratively\nto boost attack SR.\n4.1\nMulti-Step Attack Generation\nTo better characterize the behavior of multi-step adversarial attack\non its adverse eﬀect and divergence, we further analyze the gen-\neration and eﬀectiveness of multi-step adversarial example using\ntargeted Jacobian-based attack [33], which possesses a prediction\nvector-based objective function with two alternative crafting rules\nbased on single pixel or a pair of pixels.\nSingle-pixel crafting rule. Given input x and its prediction\nvector −→y , the attacker ﬁrst computes the Jacobian matrix JacF =\n∂−→y\n∂x = [ ∂yj\n∂x ]j ∈1...m. Jacobian matrix on label j indicates the rela-\ntion between input features (pixels for image data) and the predic-\ntion on that label. That is, adding pixel value on one pixel would in-\ncrease the value of the prediction Y j if the Jacobian matrix on class\nj has a positive gradient on that pixel. Particularly, ∂yT\n∂x ,yT ∈−→y is\nthe Jacobian matrix for target class T. Since the prediction vector\nof the legitimate input is generated from the DNN model, the gra-\ndient value is determined by the training process and the model\nis assumed to be diﬀerentiable. After computing the Jacobian ma-\ntrix for the entire prediction vector, the adversary can compute the\nadversarial saliency map for target class S(x,T)[λ] for each pixel\nλ.\nS(x,T)[λ] =\n(\n0,\ni f\n∂yT\n∂x [λ] < 0\nor\nÍ\nj,T\n∂yj\n∂x [λ] > 0,\n∂yT\n∂x [λ]| Í\nj,T\n∂yj\n∂x [λ]|,\notherwise,\n,\n(6)\nThis equation gives four concrete gradient based crafting rules: (1)\nif adding pixel value does not move the prediction towards the\ntarget class, i.e., the gradient of prelogit or prediction for target\nclass pT for the pixel is <0, or (2) the sum of all gradients other\nthan that of the target class pj (j , T) for the pixel is >0, then the\nvalue on adversarial saliency map on that pixel is set to 0. However,\nif adding pixel value does move the prelogit and prediction towards\nthe target class, i.e., (3) the gradient of logit of the target class for\nthe pixel is >0, or (4) the sum of all gradients other than that of the\ntarget class for the pixel is <0, then the value of adversarial saliency\nmap on that pixel is set to be the gradient product of (3) and (4). The\npower of adversarial saliency map is that it optimizes the objective\nfunction by considering both the gradient towards the target class\nand the gradient of all other classes. Once the adversarial saliency\nmap is computed, the adversary could craft the image with the\npixels that have the largest adversarial saliency maps.\nPair-wise adversarial crafting rule. The above adversarial\nsaliency map considering individual pixels one at a time is too\nstrict, especially when very few pixels would meet the heuristic\nsearch criteria in Equation 6. Papernot, et al [33] introduces the\npair-wise adversarial saliency map. The heuristic searching for pairs\nof pixels is a greedy strategy that modiﬁes one pair at a time. The\nincentive is the assumption that one pixel can compensate a mi-\nnor ﬂaw of the other pixel. For a pair of pixels (λp, λq), we ﬁrst\ncompute its Jacobian matrices according to the prediction on each\nlabel. Then pair-wisely, we compute A and B:\nA =\nÕ\ni∈{p,q}\n∂yT\nλi\n,\nB =\nÕ\ni∈{p,q}\nÕ\nj,T\n∂yj\nλi\n,\nwhere A represents to what extent changing these two pixels will\nchange the prediction on the target class. B denotes the impact of\nchanging the two pixels on classes other than the target. Similar to\nadversarial saliency map, pixel pair with the largest value on −A×B\nwhen A > 0 and B < 0 is chosen to be crafted. The perturbation\nsets the pixel value to 255. A dynamic search domain is maintained\nto keep track of those pixels whose values already reach 255. The\nmulti-step perturbation process iterates until the attack is success-\nful or reaches the pre-deﬁned maximum level of noise tolerance,\nsuch as 15% of pixels. For image of 28 × 28, the adversarial exam-\nple changes up to 28*28*0.15=118 pixels. If the adversarial input\nreaches this maximum noise level but is still not predicted as the\ntarget label, the attack fails.\nFigure 6 visualizes the adversarial saliency map for targeted at-\ntacks with each of the classes other than the source digit 1 as the\ntarget in the ﬁrst iteration. Moreover, the adversarial saliency map\nof untargeted Jacbian-based attack is provided for reference. The\nconstruction is straightforward according to the general principle\nof perturbation injection. Since the value of adversarial saliency\nmap is widely ranged, it is hard to demonstrate its numerical dif-\nference. For better visualization, we set all pixels whose values are\nnon-zero in the saliency map towards the target class to 255. This\nmeans pixels in light area are potential candidates for crafting. The\nvisualization shows clearly that the saliency map perturbation is\ndiﬀerent across the attack target class, and the DoC values for suc-\ncessful targeted attacks are diﬀerent. Target attacks to digits 0 and\n6 are failed with the 15% pixel-level perturbation threshold.\ny∗\\ step\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n788\n490\n329\n268\n228\n171\n160\n119\n94\n-119\n-148\n4\n-697\n-430\n-288\n-156\n-99\n-66\n-74\n-58\n-47\n-2\n-34\n8\n-507\n-318\n-339\n-557\n-617\n-591\n-564\n-497\n-443\n-57\n-11\nTable 6: Prelogits trajectory of three representative classes (digits\n1, 4, 8) over 10 iteration steps (source digit 1, target digit 8)\nWe use a successful 10-step Jacobian-based targeted attack from\nsource class 1 to target class 8 as an example to characterize the\nmulti-step Jacobian noise injection. Table 6 shows three represen-\ntative classes over the 10-step targeted attack and Figure 7 shows\nthe prelogit trajectory for all classes in the 10-step attack. At each\nstep, an adversarial saliency map is computed and the pixel posi-\ntion with largest value is chosen to be the crafting pixel. It is a\nnested multi-step ensemble process with every step correcting the\nperturbation trajectory path a little bit. Though the perturbation\nis based on the gradient of prediction vector, the prediction vector\nremains to be [0,1,0,0,0,0,0,0,0,0] for the ﬁrst 8 steps, and changes\nto [0,0,0,0,1,0,0,0,0,0] (predicted as digit 4) at the 9th step, and to\n[0, 0, 0, 0.0067, 0, 0, 0, 0, 0.9933, 0] at the 10th step, successfully\nreaching the digit 8 goal of this targeted attack. It is observed that\nthe trajectory of prelogits is much smoother and more informative\ncompared to the drastic change in prediction vector in the iterative\n8\ndĂƌŐĞƚ\u0003ůĂďĞů\u0003ǇΎ\nϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003 Ϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϰ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϲ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϳ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϴ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϵ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ƵŶƚĂƌŐĞƚĞĚ\n\u0004ĚǀĞƌƐĂƌŝĂů\u0003\nƐĂůŝĞŶĐǇ\u0003ŵĂƉ\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nz͕\u0003\u0018Ž\u0012сϬ͘ϬϮϴ\nz͕\u0003\u0018Ž\u0012сϬ͘ϬϱϮ\u0003\u0003\u0003\u0003\u0003z͕\u0003\u0018Ž\u0012сϬ͘Ϭϳϯ\u0003\u0003\u0003 z͕\u0003\u0018Ž\u0012сϬ͘Ϭϲϵ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003\u0018Ž\u0012сϬ͘Ϭϯϯ\u0003\u0003\u0003\u0003z͕\u0003\u0018Ž\u0012сϬ͘Ϭϰϱ\u0003\u0003\u0003\u0003z͕\u0003\u0018Ž\u0012сϬ͘Ϭϯϲ\u0003\u0003\u0003z͕\u0003\u0018Ž\u0012сϬ͘Ϭϭϲ\u0003\u0003\u0003\u0003\n/ŶƉƵƚ\u0003ǆ\nFigure 6: Visualization of Adversarial Saliency Map-based Noise Injection for targeted attacks. The Adversarial Saliency Map shown is from\nthe ﬁrst iteration. The noise of digit 1 is for untargted attack.\n-3000\n-2500\n-2000\n-1500\n-1000\n-500\n0\n500\n1000\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of Steps\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-150\n-100\n-50\n0\n9\n10\nFigure 7: Prelogits over 10 steps (source digit 1, target digit 8)\nensemble learning process. According to Figure 7, the perturbation\nenhances the prediction of targeted class while the gap between\nprelogits for diﬀerent labels shrinks as the attack progresses step\nby step to the 10th iteration. The prelogit of target class 8 gradually\nbecomes the largest, succeeding in misclassifying digit 1 to digit 8\nat the 10th step.\nTakeaway Remarks. (1) The pixel-level perturbation changes\nthe prelogits, the prediction vector, and the probability of every\nsingle class in each step. This increases the hardness of targeted\nattack when the number of classes is large, as it is more diﬃcult to\nchoose the fraction of pixels as the maximum crafting cap, and very\nlikely 15% of pixel perturbation may lead to low SR or failure of the\nattack. (2) While softmax layer ampliﬁes the numerical diﬀerence\nin prelogits and normalizes them into the prediction vectors, per-\nforming prediction vector based attack is equivalent to performing\nattacks via prelogits. More interestingly, by noticing that the gap\nof prelogits converges for several successive prediction attempts,\nit may reveal the presence of a targeted attack. (3) The pixel with\nlarger adversarial saliency map indicates that adding its value will\nmove the prelogit or prediction toward the target class more. This\ncrafting rule, together with a limitation on the maximum iteration\nensures minimal amount of perturbation as well as human imper-\nceptibility. However, it also exposes some problems of this attack.\nIn addition to its computation ineﬃciency, the perturbation sim-\nply sets the chosen pixel values to 255 at each iteration, which\nmay over craft the input at times so that the noise deviates too\nmuch from the amount needed and results in unsuccessful attacks.\nAlso adding full 255 to a pixel each time may not be eﬀective or\nhuman-imperceptible for colored images.\n4.2\nEﬀectiveness of Multi-step Attack\nIn this section we characterize the eﬀectiveness and divergence of\nmulti-step targeted attack by analyzing the easy and hard cases.\nSuccess Rate (SR). We ﬁrst categorize the easy and hard cases\nin multi-step targeted attacks using the high and low SR or the\nlarge or small fraction of adversarial examples misclassiﬁed. Table\n7 shows the results. For source class digit 1 with 1135 images and\nattack target digits 0, 2, 6, and 8, the SR is 0.1%, 85.6%, 3%, and 97%\nrespectively. Clearly, the SRs for misclassifying digit 1 to target\ndigit 0 or 6 (hard cases), are much lower than the SRs for the target\ndigits 8 or 2 (easy cases). Also, the eﬀectiveness of targeted attacks\nis asymmetrical, i.e., the attack 7 →9 is much harder with low\nSR of 0.208 than the reverse attack 9 →7 with high SR of 0.944.\nIn Table 7, the last column and the last row are average SR for\neach source class and each target class respectively. Figure 8 shows\nvulnerable and robust source classes, and Figure 9 shows hard and\neasy target classes using the SR sum. Within each SR bar, diﬀerent\ncolors indicate diﬀerent contributions of each digit to build the SR\nof the attack. It is easy to see that 1, 9, 6 are the top 3 vulnerable\nsource classes while 2, 8, 3 are the top 3 easy target classes.\n0\n1\n2\n3\n4\n5\n6\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 8: Source Vulnerability\n0\n1\n2\n3\n4\n5\n6\n7\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 9: Hardness of Target\nDegree of Change (DoC). DoC is another good discriminator\nfor easy and hard cases: the higher average DoC means that the\nattack is harder. Table 8 shows the DoC for target attack on digit 1\nand other classes being the target options. Digits 2 and 3 are easy\ntargets with high SR (Table 7) and relatively low DoC. It only takes\n5% or 6.6% of change in pixels on average to misclassify an image\nof digit 1 as digit 2 or digit 3 respectively. However, for hard attack\n1 →0, the DoC is 15% and the SR is 0.1%.\nAverage Entropy. We may also use average entropy to diﬀer-\nentiate easy and hard attacks. Two situations have low entropy:\nbenign input whose order of entropy is around 10−7, and unsuc-\ncessful adversarial example. However, there is no linear correlation\nbetween high SR and larger entropy for two reasons. (1) When in-\njecting full value of 255 to a pixel at a time, the per-step perturba-\ntion may be too large, and such coarse-grained perturbation leads\nto the change of prediction vector from one-hot source class to one-\nhot target class directly, which will not increase entropy. (2) The\nresemblance of source and target images (digits) may play a role.\nFor example, digits 5 and 6, digits 7 and 9 look alike, respectively.\nAnd attacks 6 →5 and 9 →7 have higher SRs, and are more suc-\ncessful. But their entropy values are smaller than attacks 6 →3\nand 9 →8.\nTakeaway Remarks. (1) Designing a strong attack requires to\ntrade oﬀbetween larger per-step perturbation and minimal degree\nof change. An attack is considered strong if it succeeds with high\nconﬁdence or if the adversarial prediction vector is one-hot vec-\ntor. However, not all successful attacks are accompanied with high\n9\nS \\ T\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nS: avg\n0\n0.027\n0.970\n0.039\n0.205\n0.147\n0.049\n0.307\n0.352\n0.170\n0.252\n1\n0.001\n0.856\n0.838\n0.415\n0.502\n0.030\n0.686\n0.970\n0.510\n0.534\n2\n0.001\n0.006\n0.285\n0.007\n0.003\n0.009\n0.136\n0.237\n0.004\n0.076\n3\n0.001\n0.027\n0.483\n0.005\n0.136\n0.003\n0.125\n0.114\n0.110\n0.112\n4\n0.000\n0.188\n0.633\n0.155\n0.145\n0.013\n0.768\n0.386\n0.173\n0.273\n5\n0.013\n0.246\n0.077\n0.592\n0.033\n0.037\n0.217\n0.478\n0.105\n0.120\n6\n0.040\n0.176\n0.815\n0.223\n0.618\n0.382\n0.183\n0.630\n0.116\n0.354\n7\n0.003\n0.034\n0.636\n0.562\n0.027\n0.129\n0.000\n0.320\n0.208\n0.213\n8\n0.003\n0.086\n0.858\n0.575\n0.071\n0.317\n0.016\n0.107\n0.015\n0.228\n9\n0.010\n0.084\n0.613\n0.761\n0.387\n0.003\n0.000\n0.944\n0.825\n0.403\nT: avg\n0.008\n0.097\n0.660\n0.448\n0.196\n0.196\n0.017\n0.386\n0.479\n0.157\nTable 7: SR of adversarial examples in Jacobian-based attack.\nFigure 10: Top 3 easy cases per target in Jacobian-based Attack\nTarget\n0\n2\n3\n4\n5\n6\n7\n8\n9\nDoC\n0.150\n0.050\n0.066\n0.101\n0.102\n0.148\n0.066\n0.029\n0.093\nEntropy\n0.026\n0.069\n0.068\n0.03\n0.064\n0.017\n0.05\n0.067\n0.048\nTable 8: DoC and entropy of 1135 images of digit 1.\nSR. Often the successful multi-step attacks may lower the conﬁ-\ndence (the probability) of prediction to a greater extent. Moreover,\ntargeted attack is hard to develop and pays higher cost to produce\nsuccessful adversarial examples. We measure the time for perform-\ning untargeted FGSM attack and the time for performing Jacobian\nbased targeted attack using Intel @ Core i5-2300 CPU. The for-\nmer takes 0.53 second and the latter takes 3.7 seconds per instance,\nwhich is 7 times on average for the same input. (2) Even with the\nsame attack algorithm, top three easy cases may vary notably with\nrespect to SR, DoC and entropy, so do the hard cases. Figure 10\nshows the top 3 easy Jacobian based attacks. The number in the\nbracket is the SR. For images of digit 5, the top 1 easy attack is\n5 →3 with SR of 59%, and the top 3 easy attack is 5 →2 with\nSR of only 25%, though the attack is successful with only 86 per-\nturbed pixels. Also the crafted image of digit 5 still looks like digit\n5 visually, but the perturbation noise is visible too. These empirical\nevidences show some strong and complex connection between be-\nnign input, adversarial input, loss function, and prediction vector,\nwhich inspires us to investigate the eﬀectiveness of adversarial ex-\namples from another set of factors related to adversarial learning\nand DNN training in Section 6.\n5\nATTACK EFFECT OF DNN FRAMEWORKS\nIn this section, we characterize the eﬀect of diﬀerent settings of hy-\nperparameters and diﬀerent DNN frameworks on the attack eﬀec-\ntiveness of adversarial examples. We choose the number of train-\ning epochs to study the impact of overﬁtting, various sizes of fea-\nture maps to compare the eﬀectiveness under diﬀerent DNN capac-\nity and diﬀerent DNN frameworks to evaluate their impact on the\neﬀectiveness of adversarial attacks.\n5.1\nDiﬀerent Number of Training Epochs\nThe ﬁrst set of experiments reports the presence of inconsistent\neasy and hard attacks under diﬀerent training epochs.\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n10\n30\nVulnerability of Source\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1\n10\n30\nHardness of Target\nFigure 11: Impact of training epochs: higher SR, more vulnerable\nand lower SR, harder attack.\nWe study easy and hard cases using Jacobian-based targeted\nattack with a DNN model trained under three diﬀerent settings\nof epochs: 1 epoch (underﬁtting), 10 epochs (TensorFlow default)\nand 30 epochs (overﬁtting). Figure 11 compares the vulnerability\nof source class and the hardness of target class. The height of the\nbar demonstrates the sum of SRs for each of the source classes (left\nﬁgure) or target classes (right ﬁgure). We highlight two interesting\nobservations: (1) Statistically, digit 1 is the most vulnerable source\nclass for all settings of epochs and digit 8 is the most easy attack\ntarget for 1 epoch of training. For DNN with 30 epochs of training,\ndigits 1 and 6 are the most vulnerable source classes, and digits 2,\n4, and 5 are the most easy targets. Both results indicate diﬀerent\nbehavior of easy and hard attacks compared to 10-epoch results,\nwhere digits 1 and 9 are the most vulnerable source classes, and\ndigits 2, 3, and 8 are the most easy targets. (2) The reason why\neasy and hard attack cases vary under diﬀerent training epochs is\ndue to the fact that diﬀerent training accounts for diﬀerent trained\nnetwork parameters, which describe the learned features. The dif-\nferent learned feature is reﬂected on the gradient of loss function\nand prediction vectors, and subsequently impacts the eﬀectiveness\nof adversarial examples. Figure 12 visualizes the gradient of loss\nfunction for DNN training under 1 epoch or 30 epochs for FGSM\nattack and Figure 4 is for 10 epochs, where successful ones marked\nby their θ value. These empirical evidence shows visible inconsis-\ntency across diﬀerent training epochs regarding success or failure\nof attack, as well as regarding SR and DoC for successful attacks.\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϭϬ\nz͕\u0003ɽсϬ͘Ϯϱ\nE\u0003\u0003\u0003\u0003\u0003\u0003\nE\nE\nE\nE\nz͕\u0003ɽсϬ͘ϮϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003\nŶŽŝƐĞ\u0003Ăƚ\u0003ϯϬ\u0003ĞƉŽĐŚƐ\n\u0004ƚƚĂĐŬ\u0003ůĂďĞů\u0003ǇΎ\nϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϭ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003Ϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϰ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϲ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϳ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϴ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003ϵ\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϭϬ\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϭϱ\nz͕\u0003ɽсϬ͘ϭϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nE\nz͕\u0003ɽсϬ͘ϭϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003\nŶŽŝƐĞ\u0003Ăƚ\u0003ϭĞƉŽĐŚ\nFigure 12: Loss Function-Based Noise at 1 and 30 epochs\n5.2\nDiﬀerent Sizes of Feature Maps\nWe next study whether diﬀerent sizes of feature maps have dif-\nferent impacts on the features learned by the DNN model, as the\nchange of learned features will be reﬂected by the gradients of loss\nfunction and adversarial saliency maps, which will impact the be-\nhavior of easy and hard attacks. We reduce and double the original\n10\nnumber of output features to generate feature map of half and dou-\nble sizes for the ﬁrst four DNN layers in TensorFlow. Figure 13 com-\npares three sizes of feature maps: half, original and double on the\nvulnerability of source classes and the hardness of target classes.\nFor half feature map case, digit 1 is the most vulnerable source\nclass, whereas digits 2 and 3 are the easiest targets. For double fea-\nture map case, digit 1 and 9 are the most vulnerable source classes,\nwhereas digits 2 and 8 are the easiest targets. Again, the hard and\neasy attacks vary for three sizes of feature maps with more easy\ncases for normal size feature maps. Figure 14 visualizes the diﬀer-\nent features learned under half and double feature maps using the\ngradient of loss function under targeted FGSM attack, and visual-\nization for normal feature map was given in Figure 4. Similar incon-\nsistency is observed across diﬀerent sizes of feature maps, though\nthe impact of diﬀerent training epochs on the degree of inconsis-\ntency is much larger.\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nhalf\nnormal\ndouble\nVulnerability of Source\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nhalf\nnormal\ndouble\nHardness of Target\nFigure 13: Impact of varying sizes of feature maps: higher SR, more\nvulnerable and lower SR, harder attack.\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϭϱ\nz͕\u0003ɽсϬ͘ϮϬ\nE\u0003\u0003\u0003\u0003\u0003\u0003\nE\nE\nE\nE\nz͕\u0003ɽсϬ͘Ϯϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003ŶŽŝƐĞ\u0003\n;ĚŽƵďůĞ\u0003ĨĞĂƚƵƌĞ\u0003ŵĂƉƐͿ\n\u0004ƚƚĂĐŬ\u0003ůĂďĞů\u0003ǇΎ\nϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϭ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϮ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϰ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϲ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϳ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϴ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϵ\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϭϬ\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϮϬ\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nE\nz͕\u0003ɽсϬ͘ϮϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003ŶŽŝƐĞ\u0003\n;ŚĂůĨ\u0003ĨĞĂƚƵƌĞ\u0003ŵĂƉƐͿ\u0003\nFigure 14: Loss function-based noise with diﬀerent feature maps\n5.3\nDiﬀerent DNN Frameworks\nWe next evaluate the impact of diﬀerent DNN frameworks on the\neﬀectiveness and divergence of adversarial examples. Figure 15 re-\nports the comparison results for TensorFlow, Caﬀe, Theano and\nTorch. Clearly, TensorFlow and Theano are consistently more vul-\nnerable under FGSM, followed by Caﬀe and Torch. Figure 17 visual-\nizes the gradient of loss function based adversarial perturbation us-\ning DNN model trained by Caﬀe, Theano and Torch respectively. It\nis clear that diﬀerent frameworks lead to diﬀerent features learned\nby their DNN model, which contributes to their diﬀerent inﬂu-\nence on the eﬀectiveness of adversarial attacks with respect to easy\nand hard cases. Figure 17 also exposes some inherent problems in\nFGSM attack method. The crafting rule in FGSM treats all pixels\nequally, which may be ineﬃcient since the gradients reﬂected in\nthe input data for each pixel are not the same. While the positive\nand negative signs of the sign function are useful, assigning the\nmagnitude of sign function to 1 is not eﬀective in many cases. For\nexample, in Torch, the identical perturbation noise smooths the nu-\nmerical diﬀerence of the gradient of the loss function on diﬀerent\ntargets. Thus, the attack does not make full use of these gradients.\nThis may contribute to the low SR on untargeted FGSM attacks in\nTorch.\nTakeaway Remarks. First, adversarial attacks heavily rely on\nthe gradient of loss function and prediction vector produced by the\ntrained DNN model, and such gradient is determined by the pa-\nrameters in the DNN function learned during the training process.\nBoth the hyperparameters, which inﬂuence on how the training\nprocess will be conducted, and the learned parameters, which are\nthe ﬁxed components of the trained model, will impact the com-\nputation of gradient of loss function and prediction vector during\nthe generation of adversarial examples, regardless of speciﬁc at-\ntack algorithms, and subsequently impact the eﬀectiveness and di-\nvergence of adversarial attacks. Second, for the adversarial exam-\nples crafted using the same attack method, easy and hard attacks\ntend to vary under diﬀerent hyperparameters and across diﬀer-\nent DNN frameworks, indicating that the eﬀectiveness of adversar-\nial attacks is inconsistent and unpredictable across diﬀerent DNN\nframeworks. Such inconsistency also presents under diverse set-\ntings of hyperparameters used for DNN training within the same\nDNN framework.\n0.377\n0.622\n0.689\n0.733\n0.822\n0.674\n0.805\n0.549\n0.678\n0.956\n0.991\n0.978\n0.832\n0.785\n0.987\n0.996\n0.905\n0.997\n0.862\n0.994\n0.61\n0.16\n0.518\n0.372\n0.632\n0.392\n0.299\n0.603\n0.256\n0.351\n0.941\n0.987\n0.773\n0.824\n0.947\n0.859\n0.956\n0.874\n0.853\n0.984\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nCaffe\nTheano\nTorch\nTensorFlow\nFigure 15: SR of untargeted FGSM with diﬀerent frameworks\nFigure 16 shows the SR of untargeted FGSM attack on each\nsource class for only Caﬀe and Torch. Within each SR bar, diﬀer-\nent colors indicate diﬀerent contributions of destination classes to\nbuilding the attack SR. It is easy to see that the top 3 vulnerable\nsource classes in Caﬀe are very diﬀerent from that in Torch.\n6\nATTACK MITIGATION STRATEGIES\nWe have characterized the eﬀectiveness of adversarial examples\nin deep learning through general formulation, extensive empirical\nevidences, and systematic study of successful attacks and their di-\nvergence in terms of easy and hard cases. Motivated by the results\nof this study, we propose some attack mitigation strategies from\ntwo perspectives: prediction phase mitigation and model training\nphase mitigation.\nPrediction Phase Mitigation. We have shown that (1) success-\nful attacks (targeted or untargeted) often do not agree on the same\n0.549\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSuccess Rate\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.377\n0.622\n0.689\n0.733\n0.822\n0.674\n0.549\n0.678\n0.956\n0.805\nCaﬀe\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nSuccess Rate\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.610\n0.372\n0.392\n0.299\n0.603\n0.351\n0.160\n0.518\n0.632\n0.256\nTorch\nFigure 16: SR of untargeted FGSM with Caﬀe and Torch.\n11\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϭϬ\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nz͕\u0003ɽсϬ͘Ϯϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nE\nE\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003ŶŽŝƐĞ\u0003\n;dŚĞĂŶŽͿ\n\u0004ƚƚĂĐŬ\u0003ůĂďĞů\u0003ǇΎ\nϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϭ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϮ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϯ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϰ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϱ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϲ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϳ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϴ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nϵ\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003z͕\u0003ɽсϬ͘ϭϬ\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nE\nE\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nE\nz͕\u0003ɽсϬ͘ϮϬ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003ŶŽŝƐĞ\u0003\n;\u0012ĂĨĨĞͿ\u0003\n'ƌĂĚŝĞŶƚͲďĂƐĞĚ\u0003ŶŽŝƐĞ\u0003\n;dŽƌĐŚͿ\n^ƵĐĐĞƐƐĨƵů\u0003;zͬEͿ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nE\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nE\nE\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003E\nE\nE\nE\nFigure 17: Loss function-based noise with frameworks\nnoise level (θ) under the same crafting ruleR( ); (2) the sameθ value\nthat generates one successful adversarial example against a benign\ninput x of class Cx may not work eﬀectively for another benign in-\nput of the same class; and (3) the destination class of untargeted\nattacks is not uniformly random. Similarly, some target classes are\nmuch harder to attack under the same attack scheme. These ob-\nservations inspire us to propose two prediction phase mitigation\nstrategies: consensus based mitigation and time-out based mitiga-\ntion, which are independent of trained DNN model and speciﬁc\nDNN framework used for training.\nConsensus based Mitigation. For each prediction query with\nan input x, the prediction API from the DNN as a service provider\nwill generate q input queries x1, . . . ,xq such that the prediction\nresult is only accepted by a client when the majority reaches a\nconsensus. Given that all adversarial attacks do not respond to\nthe diﬀerent input data of the same class consistently, such data-\ndiversity based consensus can be an economical and eﬀective miti-\ngation strategy. There are several ways to generate suchq query in-\nputs. For images, one can leverage computer vision and computer\ngraphics techniques to generate alternatives views of the same im-\nage. Also, the consensus protocol can be decentralized to make it\nmore resilient to single point of failure [17]. Each client may accept\na prediction result upon obtaining q consensus votes from the net-\nwork.\nTime-out based Mitigation. For each type of prediction queries,\na time-out threshold is pre-set by the DNN as a service provider.\nIf such a query input x is compromised by an adversarial example\nxadv for multi-step attacks, then the time for turning-around pre-\ndiction for xadv may exceed the normal histogram statistics for\nthis type of prediction task, and thus turn on an alarm. Such time-\nout threshold can be learned over time or through training. This\nmitigation strategy can be especially useful for hard attacks, which\nrequires longer iteration rounds to be successful.\nTraining Phase Mitigation. We have shown that the eﬀective-\nness of adversarial attacks is inconsistent and unpredictable and\neasy and hard attacks tend to vary under diﬀerent hyperparame-\nters and across diﬀerent DNN frameworks. Also adversarial attacks\nheavily rely on the gradient of loss function and prediction vector\nproduced by the trained DNN model, and such gradient is deter-\nmined by the parameters in the DNN function learned during the\ntraining process. Thus, we propose three training phase mitigation\nstrategies as proactive countermeasures that can be exercised by\nthe DNN as a service provider.\nData based Ensemble Training. For each of the prediction\nclasses, an adversarial training in conjunction with data driven en-\nsemble is employed. This enables the training set to include suf-\nﬁcient representations of training data for each class, including\nthose that can strengthen the resilience of prediction queries against\nadversarial examples in the prediction phase. For instance, by study-\ning the hard cases and easy cases of each source class and each\ntarget class, we can generate training examples that make the easy\nattacks harder and make the harder attacks impossible to succeed.\nHyperparameter based ensemble training. By utilizing dif-\nferent settings of hyperparameters, such as diﬀerent number of\nepochs, we can train alternative models and use these diverse mod-\nels as a collection of candidate models in the prediction phase.\nThere are several ways to implement the consensus for hyperpa-\nrameter based ensemble. For instance, for each prediction query\nwith input x, a subset of hyperparameter-varied models will be\nselected to produce prediction results and collect consensus ac-\ncordingly. Round robin, random, weighted round robin, power of\ntwo [38] or generalized power of choice [36] can be employed to\nimplement the selection algorithms.\nDNN Framework based ensemble training. We propose to\ndeploy two types of DNN framework based ensemble training. The\nﬁrst approach is to train a DNN ensemble model for prediction us-\ning a number of diﬀerent deep learning frameworks (e.g., Tensor-\nFlow, Caﬀee, Torch) or using diﬀerent hyperparameters, such as\nfeature maps, within one framework such as TensorFlow. Recall\nSection 5, we have shown that same adversarial examples have\ninconsistent eﬀects when using diﬀerent sizes of feature maps, dif-\nferent DL frameworks from diﬀerent DNN software providers due\nto variations in neural network structures and parallel computa-\ntion libraries used in their implementations [22]. In addition to the\nensemble of ﬁnal trained DNN models, the second alternative ap-\nproach for DNN framework based ensemble training is to allow\nmultiple DNN models trained over the same training dataset to co-\nexist for serving the prediction queries.\nBoth approaches provide a number of advantages. First, diﬀer-\nent models respond to the same adversarial example very diﬀer-\nently in terms of easy and hard attacks as shown in Section 5,\nthus the prediction API can detect inconsistency, spot the attack\nattempts, and mitigate risks proactively. Second, one can also in-\ntegrate the two alternative approaches for the DNN framework\nbased ensemble training, to further strength the attack resilience\nthrough combining multi-framework or multi-conﬁguration of hy-\nperparameter based ensemble training with multi-view based pre-\ndiction ensemble. Such integrated approach can provide a larger\npool of alternative trained models for both training and prediction-\nbased consensus, which further strengthens the prediction query\nbased consensus.\nFinally, we would like to note that our proposed DNN frame-\nwork ensemble approaches are diﬀerent from the cross ML-models\nbased ensemble strategy, which provides ensemble learning model\nby integrating diﬀerent machine learning models, such as SVM, De-\ncision Tree, with DNNs, in order to train a prediction model on the\nsame training dataset [31]. The recent study of the transferability\nof adversarial attacks has shown that using the cross ML models\nbased ensemble learning may not be eﬀective under transferability\nof untargeted adversarial attacks [23, 31, 32, 43]. As pointed out\nin [23], the transferability only works under untargeted adversar-\nial attacks, and targeted adversarial attacks do not transfer. There-\nfore, our proposed two types of multi-framework based ensemble\n12\nstrategies can be viewed as a step forward towards developing a\nunifying mitigation architecture for both targeted and untargeted\nadversarial attacks.\n7\nRELATED WORK\nResearch on adversarial attacks in deep learning can be classiﬁed\ninto two broad categories: attack algorithms and defense propos-\nals [3, 5, 11–13, 27, 28, 35, 37, 46]. Given vulnerability of DNN,\nthere have been quite a few attempts in building a robust system\nagainst adversarial examples. Two classes of defense mechanisms\nhave been proposed. The ﬁrst type of defense is to detect adver-\nsarial examples so that malicious data can be removed before pre-\ndiction [5, 28]. [5] surveys ten recent proposals designed for adver-\nsarial example detection. They show that all ten detection methods\ncan be defeated by constructing new loss functions, which makes\nthe adversarial example detection of little use. Besides, the defense\nmechanism of simply distinguishing between clean and adversar-\nial data is not strong enough. It is better to also correctly classify\nthe carefully-injected adversarial examples. The second type of de-\nfense is to increase robustness by modifying the DNN model, aim-\ning to increase the cost of crafting benign samples into misleading\nones [1, 3, 11–13, 35, 37, 45, 46]. Representative defense includes\nadversarial training [11], autoencoder-based defense [13] and de-\nfensive distillation [35]. While adversarial training is computation-\nineﬃcient, the latter two mechanisms require some major modi-\nﬁcations on DNN architecture. Region-based defense [3] is a re-\ncently proposed defense mechanism. It generates a number of sam-\nples around the input data and regards the label of majority of the\nsamples as the predicted label of the input.\nFor classiﬁcation of adversarial attacks, [26] shows the impact\nof network architecture on adversarial robustness, claiming that\nnetworks with a larger capacity than needed for correctly classify-\ning natural examples could reliably withstand adversarial attacks.\nHowever, larger capacity of the network will increase computa-\ntion overhead signiﬁcantly. [25] uses local intrinsic dimensionality\nin layer-wise hidden representations of DNNs to study adversarial\nsubspaces, while [24] points out its limitation. In spite of a growing\nnumber of proposed attacks and defenses, there is a lack of statis-\ntical and principled characterization of adversarial attacks, which\nis critical for systematic instrumentation of mitigation strategies\nand defense methods.\n8\nCONCLUSION\nWe have taken a holistic approach to study the eﬀectiveness and\ndivergence of adversarial examples and attacks in deep learning\nsystems. We show that by providing a general formulation and es-\ntablishing basic principle for adversarial attack algorithm design,\nwe are able to deﬁne statistical measures and categorize successful\nattacks into easy and hard cases. These developments enhance our\nability to analyze both convergence and divergence of adversarial\nbehavior with respect to easy and hard attacks, in terms of suc-\ncess rate, degree of change, entropy and fraction of successful ad-\nversarial attacks, as well as under diﬀerent hyperparameters and\ndiﬀerent DNN frameworks. By leveraging the fact that adversar-\nial attacks exhibit multi-level inconsistency and unpredictability,\nregardless speciﬁc attack algorithms and adversarial perturbation\nmethods, we put forward both prediction phase mitigation strate-\ngies and training phase mitigation strategies against present and\nfuture adversarial attacks in deep learning.\nACKNOWLEDGMENTS\nThis research is partially support by the National Science Founda-\ntion under Grants SaTC 1564097, NSF 1547102, and an IBM Faculty\nAward.\nREFERENCES\n[1] Muhammad Ejaz Ahmed and Hyoungshick Kim. 2017. Poster: Adversarial Ex-\namples for Classiﬁers in High-Dimensional Network Data. In Proceedings of the\n2017 ACM SIGSAC Conference on Computer and Communications Security. ACM,\n2467-2469.\n[2] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug Ty-\ngar. 2006. Can machine learning be secure?. In Proceedings of the 2006 ACM Sym-\nposium on Information, computer and communications security. ACM, 16-25.\n[3] Xiaoyu Cao and Neil Zhenqiang Gong. 2017. Mitigating evasion attacks to deep\nneural networks via region-based classiﬁcation. In Proceedings of the 33rd Annual\nComputer Security Applications Conference. ACM, 278-287.\n[4] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,\nClay Shields, DavidWagner, andWenchao Zhou. 2016. Hidden Voice Commands.\nIn USENIX Security Symposium. USENIX Association, 513-530.\n[5] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily\ndetected: Bypassing ten detection methods. In Proceedings of the 10th ACM Work-\nshop on Artiﬁcial Intelligence and Security. ACM, 3-14.\n[6] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of\nneural networks. In Security and Privacy (SP), 2017 IEEE Symposium on. IEEE,\n39-57.\n[7] Ronan Collobert and Jason Weston. 2008. A uniﬁed architecture for natural lan-\nguage processing: Deep neural networks with multitask learning. In Proceedings\nof the 25th international conference on Machine learning. ACM, 160-167.\n[8] George E Dahl, JackWStokes, Li Deng, and Dong Yu. 2013. Large-scale malware\nclassiﬁcationusing random projections and neural networks. In Acoustics, Speech\nand Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE,\n3422-3426.\n[9] Gamaleldin F Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex\nKurakin, Ian Goodfellow, and Jascha Sohl-Dickstein. 2018. Adversarial Examples\nthat Fool both Human and Computer Vision. arXiv preprint arXiv:1802.08195,\n(2018).\n[10] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul\nPrakash, Amir Rahmati, and Dawn Song. 2017. Robust physical-world attacks on\nmachine learning models. arXiv preprint arXiv:1707.08945 (2017).\n[11] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and\nharnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).\n[12] Gaurav Goswami, Nalini Ratha, Akshay Agarwal, Richa Singh, and Mayank\nVatsa. 2018. Unravelling robustness of deep learning based face recognition\nagainst adversarial attacks. arXiv preprint arXiv:1803.00401 (2018).\n[13] Shixiang Gu and Luca Rigazio. 2014. Towards deep neural network architectures\nrobust to adversarial examples. arXiv preprint arXiv:1412.5068 (2014).\n[14] Geoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,\nNavdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N\nSainath, et al. 2012. Deep neural networks for acoustic modeling in speech recog-\nnition: The shared views of four researchgroups. IEEE Signal ProcessingMagazine.\n29, 6 (2012), 82-97.\n[15] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and JD\nTygar. 2011. Adversarial machine learning. In Proceedings of the 4th ACM work-\nshop on Security and artiﬁcial intelligence. ACM, 43-58.\n[16] Jiman Kim and Chanjong Park. 2017. End-to-End Ego Lane Estimation based on\nSequential Transfer Learning for Self-Driving Cars. In Computer Vision and Pat-\ntern Recognition Workshops (CVPRW), 2017 IEEE Conference on. IEEE, 1194-1202.\n[17] Ahmed Kosba, Andrew Miller, Elaine Shi, ZikaiWen, and Charalampos Pa-\npamanthou. 2016. Hawk: The blockchain model of cryptography and privacy-\npreserving smart contracts. In Security and Privacy (SP), 2016 IEEE Symposium on.\nIEEE, 839-858.\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. 2012. Imagenet classiﬁ-\ncation with deep convolutional neural networks. In Advances in neural informa-\ntion processing systems. NIPS Foundation, 1097-1105.\n[19] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples\nin the physical world. arXiv preprint arXiv:1607.02533 (2016).\n[20] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial machine\nlearning at scale. arXiv preprint arXiv:1611.01236 (2016).\n[21] Yann LeCun, Corinna Cortes, and CJ Burges. 2010. MNIST handwritten digit\ndatabase. AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist 2\n13\n(2010).\n[22] Ling Liu, YanzhaoWu,WenqiWei,Wenqi Cao, Semih Sahin, and Qi Zhang. 2018.\nBenchmarking Deep Learning Frameworks: Design Considerations, Metrics and\nBeyond. In Proceedings of the 38th International Conference on Distributed Com-\nputing Systems. IEEE.\n[23] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving\ninto transferable adversarial examples and black-box attacks. arXiv preprint\narXiv:1611.02770 (2016).\n[24] Pei-Hsuan Lu, Pin-Yu Chen, and Chia-Mu Yu. 2018. On the Limitation of Local\nIntrinsic Dimensionality for Characterizing the Subspaces of Adversarial Exam-\nples. arXiv preprint arXiv:1803.09638 (2018).\n[25] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema,\nMichael E Houle, Grant Schoenebeck, Dawn Song, and James Bailey. 2018. Char-\nacterizing Adversarial Subspaces Using Local Intrinsic Dimensionality. arXiv\npreprint arXiv:1801.02613 (2018).\n[26] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and\nAdrian Vladu. 2017. Towards deep learning models resistant to adversarial at-\ntacks. arXiv preprint arXiv:1706.06083 (2017).\n[27] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against ad-\nversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on Com-\nputer and Communications Security. ACM, 135-147.\n[28] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoﬀ. 2017.\nOn detecting adversarial perturbations. arXiv preprint arXiv:1702.04267 (2017).\n[29] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.\nDeepfool: a simple and accurate method to fool deep neural networks. In Proceed-\nings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nIEEE, 2574-2582.\n[30] Anh Nguyen, Jason Yosinski, and JeﬀClune. 2015. Deep neural networks are eas-\nily fooled: High conﬁdence predictions for unrecognizable images. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 427-436.\n[31] Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. 2016. Transferability\nin machine learning: from phenomena to black-boxattacks using adversarialsam-\nples. arXiv preprint arXiv:1605.07277 (2016).\n[32] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Ce-\nlik, and Ananthram Swami. 2017. Practical black-box attacks against machine\nlearning. In Proceedings of the 2017 ACM on Asia Conference on Computer and\nCommunications Security. ACM, 506-519.\n[33] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Ce-\nlik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial\nsettings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on.\nIEEE, 372-387.\n[34] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. 2016.\nTowards the science of security and privacy in machine learning. arXiv preprint\narXiv:1611.03814 (2016).\n[35] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.\n2016. Distillation as a defense to adversarial perturbations against deep neural\nnetworks. In Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 582-597.\n[36] Gahyun Park. 2011. A generalization of multiple choice balls-into-bins. In Pro-\nceedings of the 30th annual ACM SIGACT-SIGOPS symposium on Principles of dis-\ntributed computing. ACM, 297-298.\n[37] Adnan Siraj Rakin, Zhezhi He, Boqing Gong, and Deliang Fan. 2018. Robust Pre-\nProcessing: A Robust Defense Method Against Adversary Attack. arXiv preprint\narXiv:1802.01549 (2018).\n[38] Andrea W Richa, M Mitzenmacher, and R Sitaraman. 2001. The power of two\nrandom choices: A survey of techniques and results. Combinatorial Optimization.\n9 (2001), 255-304.\n[39] Claude Elwood Shannon. 1948. A mathematical theory of communication. Bell\nSystem Technical Journal. 27, 3 (1948), 379-423.\n[40] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016. Ac-\ncessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition.\nIn Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communi-\ncations Security. ACM, 1528-1540.\n[41] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Er-\nhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural net-\nworks. arXiv preprint arXiv:1312.6199 (2013).\n[42] Yaniv Taigman, Ming Yang, MarcAurelio Ranzato, and LiorWolf. 2014. Deepface:\nClosing the gap to human-level performance in face veriﬁcation. In Proceedings\nof the IEEE conference on computer vision and pattern recognition. IEEE, 1701-1708.\n[43] Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick\nMcDaniel. 2017. The space of transferable adversarial examples. arXiv preprint\narXiv:1704.03453 (2017), 582-597.\n[44] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan\nYuille. 2017. Adversarial examples for semantic segmentation and object detec-\ntion. In IEEE International Conference on Computer Vision. IEEE, 1378-1387.\n[45] Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. 2017. Eﬃcient\ndefenses against adversarial attacks. In Proceedings of the 10th ACM Workshop on\nArtiﬁcial Intelligence and Security. ACM, 39-49.\n[46] Jake Zhao and Kyunghyun Cho. 2018. Retrieval-Augmented Convolutional Neu-\nral Networks for Improved Robustness against Adversarial Examples. arXiv\npreprint arXiv:1802.09502 (2018).\n14\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-06-29",
  "updated": "2018-12-30"
}