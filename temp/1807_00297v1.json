{
  "id": "http://arxiv.org/abs/1807.00297v1",
  "title": "Exponential Convergence of the Deep Neural Network Approximation for Analytic Functions",
  "authors": [
    "Weinan E",
    "Qingcan Wang"
  ],
  "abstract": "We prove that for analytic functions in low dimension, the convergence rate\nof the deep neural network approximation is exponential.",
  "text": "arXiv:1807.00297v1  [cs.LG]  1 Jul 2018\nExponential Convergence of the Deep Neural Network\nApproximation for Analytic Functions\nWeinan E1,2,3 and Qingcan Wang4\n1Department of Mathematics and PACM, Princeton University\n2Center for Big Data Research, Peking University\n3Beijing Institute of Big Data Research\n4PACM, Princeton University\nDedicated to Professor Daqian Li on the occasion of his 80th birthday\nAbstract\nWe prove that for analytic functions in low dimension, the convergence rate of the deep neural network\napproximation is exponential.\n1\nIntroduction\nThe approximation properties of deep neural network models is among the most tantalizing problems in\nmachine learning. It is widely believed that deep neural network models are more accurate than shallow ones.\nYet convincing theoretical support for such a speculation is still lacking. Existing work on the superiority of\nthe deep neural network models are either for very special functions such as the examples given in Telgarsky\n(2016), or special classes of functions such as the ones having a speciﬁc compositional structure. For the latter,\nthe most notable are the results proved by Poggio et al. (2017) that the approximation error for deep neural\nnetwork models is exponentially better than the error for the shallow ones for a class of functions with speciﬁc\ncompositional structure. However, given a general function f, one cannot calculate the distance from f to\nsuch class of functions. In the more general case, Yarotsky (2017) considered Cβ-diﬀerentiable functions,\nand proved that the number of parameters needed to achieve an error tolerance of ε is O(ε−d/β log 1/ε).\nMontanelli and Du (2017) considered functions in Koborov space. Using connection with sparse grids, they\nproved that the number of parameters needed to achieve an error tolerance of ε is O(ε−1/2(log 1/ε)d).\nFor shallow networks, there has been a long history of proving the so-called universal approximation\ntheorem, going back to the 1980s (Cybenko, 1989). For networks with one hidden layer, Barron (1993)\nestablished a convergence rate of O(n−1/2) where n is the number of hidden nodes. Such universal approxi-\nmation theorems can also be proved for deep networks. Lu et al. (2017) considered networks of width d + 4\nfor functions in d dimension, and proved that these networks can approximate any integrable function with\nsuﬃcient number of layers. However, they did not give the convergence rate with respect to depth. To ﬁll\nin this gap, we give a simple proof that the same kind of convergence rate for shallow networks can also be\nproved for deep networks.\nThe main purpose of this paper is to prove that for analytic functions, deep neural network approximations\nconverges exponentially fast. The convergence rate deteriorates as a function of the dimensionality of the\nproblem. Therefore the present result is only of signiﬁcance in low dimension. However, this result does\nreveal a real superior approximation property of the deep networks for a wide class of functions.\nSpeciﬁcally this paper contains the following contributions:\n1. We construct neural networks with ﬁxed width d + 4 to approximate a large class of functions where\nthe convergence rate can be established.\n2. For analytic functions, we obtain exponential convergence rate, i.e. the depth needed only depends on\nlog 1/ε instead of ε itself.\n1\n2\nThe setup of the problem\nWe begin by deﬁning the network structure and the distance used in this paper, and proving the corresponding\nproperties for the addition and composition operations.\nWe will use the following notations:\n1. Colon notation for subscript: let {xm:n} = {xi : i = m, m + 1, . . . , n} and {xm1:n1,m2:n2} = {xi,j : i =\nm1, m1 + 1, . . . , n1, j = m2, m2 + 1, . . . , n2}.\n2. Linear combination: denote y ∈L(x1, . . . , xn) if there exist βi ∈R, i = 1, . . . , n, such that y =\nβ0 + β1x1 + · · · + βnxn.\n3. Linear combination with ReLU activation: denote ˜y ∈˜L(x1, . . . , xn) if there exists y ∈L(x1, . . . , xn)\nand ˜y = ReLU(y) = max(y, 0).\nDeﬁnition 1. Given a function f(x1, . . . , xd), if there exist variables {y1:L,1:M} such that\ny1,m ∈˜L(x1:d),\nyl+1,m ∈˜L(x1:d, yl,1:M),\nf ∈L(x1:d, y1:L,1:M),\n(1)\nwhere m = 1, . . . , M, l = 1, . . . , L−1, then f is said to be in the neural nets class FL,M\n\u0000Rd\u0001\n, and {y1:L,1:M}\nis called a set of hidden variables of f.\nA function f ∈FL,M can be regarded as a neural net with skip connections from the input layer to the\nhidden layers, and from the hidden layers to the output layer. This representation is slightly diﬀerent from\nthe one in standard fully-connected neural networks where connections only exist between adjacent layers.\nHowever, we can also easily represent such f using a standard network without skip connection.\nProposition 2. A function f ∈FL,M\n\u0000Rd\u0001\ncan be represented by a ReLU network with depth L + 1 and\nwidth M + d + 1.\nProof. Let {y1:L,1:M} be the hidden variables of f that satisﬁes (1), where\nf = α0 +\nd\nX\ni=1\nαixi +\nL\nX\nl=1\nM\nX\nm=1\nβl,myl,m.\nConsider the following variables {h1:L,1:M}:\nhl,1:M = yl,1:M,\nhl,M+1:M+d = x1:d\nfor l = 1, . . . , L, and\nh1,M+d+1 = α0 +\nd\nX\ni=1\nαixi,\nhl+1,M+d+1 = hl,M+d+1 +\nM\nX\nm=1\nβl,mhl,m\nfor l = 1, . . . , , L −1. One can see that h1,m ∈˜L(x1:d), hl+1,m ∈˜L(hl,1:M+d+1), m = 1, . . . , M + d + 1,\nl = 1, . . . , L −1, and f ∈L(hL,1:M+d+1), which is a representation of a standard neural net.\nProposition 3. (Addition and composition of neural net class FL,M)\n1.\nFL1,M + FL2,M ⊆FL1+L2,M,\n(2)\ni.e. if f1 ∈FL1,M\n\u0000Rd\u0001\nand f2 ∈FL2,M\n\u0000Rd\u0001\n, then f1 + f2 ∈FL1+L2,M.\n2.\nFL2,M ◦FL1,M+1 ⊆FL1+L2,M+1,\n(3)\ni.e if f1(x1, . . . , xd) ∈FL1,M+1\n\u0000Rd\u0001\nand f2(y, x1, . . . , xd) ∈FL2,M\n\u0000Rd+1\u0001\n, then\nf2(f1(x1, . . . , xd), x1, . . . , xd) ∈FL1+L2,M+1\n\u0000Rd\u0001\n.\n2\nProof. For the addition property, denote the hidden variables of f1 and f2 as {y(1)\n1:L1,1:M} and {y(2)\n1:L2,1:M}.\nLet\ny1:L1,1:M = y(1)\n1:L1,1:M,\nyL1+1:L1+L2,1:M = y(2)\n1:L2,1:M.\nBy deﬁnition, {y1:L1+L2,1:M} is a set of hidden variables of f1 + f2. Thus f1 + f2 ∈FL1+L2,M.\nFor the composition property, let the hidden variables of f1 and f2 as {y(1)\n1:L1,1:M+1} and {y(2)\n1:L2,1:M}. Let\ny1:L1,1:M+1 = y(1)\n1:L1,1:M+1,\nyL1+1:L1+L2,1:M = y(2)\n1:L2,1:M,\nyL1+1,M+1 = yL1+2,M+1 = · · · = yL1+L2,M+1 = f1(x1, . . . , xd).\nOne can see that {y1:L1+L2,1:M+1} is a set of hidden variables of f2(f1(x), x), thus the composition property\n(3) holds.\nDeﬁnition 4. Given a continuous function ϕ(x), x ∈[−1, 1]d and a continuous function class F\n\u0000[−1, 1]d\u0001\n,\ndeﬁne the L∞distance\ndist (ϕ, F) = inf\nf∈F\nmax\nx∈[−1,1]d |ϕ(x) −f(x)|.\n(4)\nProposition 5. (Addition and composition properties for distance function)\n1. Let ϕ1 and ϕ2 be continuous functions. Let F1 and F2 be two continuous function classes, then\ndist (α1ϕ1 + α2ϕ2, F1 + F2) ≤|α1|dist (ϕ1, F1) + |α2|dist (ϕ2, F2) ,\n(5)\nwhere α1 and α2 are two real numbers.\n2. Assume that ϕ1(x) = ϕ1(x1, . . . , xd), ϕ2(y, x) = ϕ2(y, x1, . . . , xd) satisfy ϕ1\n\u0000[−1, 1]d\u0001\n⊆[−1, 1]. Let\nF1\n\u0000[−1, 1]d\u0001\n, F2\n\u0000[−1, 1]d+1\u0001\nbe two continuous function classes, then\ndist (ϕ2(ϕ1(x), x), F2 ◦F1) ≤Lϕ2dist (ϕ1, F1) + dist (ϕ2, F2) ,\n(6)\nwhere Lϕ2 is the Lipschitz norm of ϕ2 with respect to y.\nProof. The additional property obviously holds. Now we prove the composition property. For any f1 ∈F1,\nf2 ∈F2, one has\n|ϕ2(ϕ1(x), x) −f2(f1(x), x)| ≤|ϕ2(ϕ1(x), x) −ϕ2(f1(x), x)| + |ϕ2(f1(x), x) −f2(f1(x), x)|\n≤Lϕ2∥ϕ1(x) −f1(x)∥∞+ ∥ϕ2(y, x) −f2(y, x)∥∞.\nTake f ⋆\n1 = argminf∥ϕ1(x) −f(x)∥∞and f ⋆\n2 = argminf∥ϕ2(y, x) −f(y, x)∥∞, then\n|ϕ2(ϕ1(x), x) −f ⋆\n2 (f ⋆\n1 (x), x)| ≤Lϕ2dist (ϕ1, F1) + dist (ϕ2, F2) ,\nthus the composition property (6) holds.\nNow we are ready to state the main theorem for the approximation of analytic functions.\nTheorem 6. Let f be an analytic function over (−1, 1)d. Assume that the power series f(x) = P\nk∈Nd akxk\nis absolutely convergent in [−1, 1]d. Then for any δ > 0, there exists a function ˆf that can be represented by\na deep ReLU network with depth L and width d + 4, such that\n|f(x) −ˆf(x)| < 2\nX\nk∈Nd\n|ak| · exp\n\u0010\n−dδ\n\u0010\ne−1L\n1/2d −1\n\u0011\u0011\n(7)\nfor all x ∈[−1 + δ, 1 −δ]d.\n3\n3\nProof\nThe construction of ˆf is motivated by the approximation of the square function ϕ(x) = x2 and multiplication\nfunction ϕ(x, y) = xy proposed in Yarotsky (2017), Liang and Srikant (2016). We use this as the basic\nbuilding block to construct approximations to monomials, polynomials, and analytic functions.\nLemma 7. The function ϕ(x) = x2, x ∈[−1, 1] can be approximated by deep neural nets with an exponential\nconvergence rate:\ndist\n\u0000ϕ = x2, FL,2\n\u0001\n≤2−2L.\n(8)\nProof. Consider the function\ng(y) =\n(\n2y,\n0 ≤y < 1/2,\n2(1 −y),\n1/2 ≤y ≤1,\nthen g(y) = y −4ReLU(y −1/2) in [0, 1]. Deﬁne the hidden variables {y1:L,1:2} as follows:\ny1,1 = ReLU(x),\ny1,2 = ReLU(−x),\ny2,1 = ReLU(y1,1 + y1,2),\ny2,2 = ReLU(y1,1 + y1,2 −1/2),\nyl+1,1 = ReLU(2yl,1 −4yl,2),\nyl+1,2 = ReLU(2yl,1 −4yl,2 −1/2)\nfor l = 2, 3, . . . , L −1. Using induction, one can see that |x| = y1,1 + y1,2 and gl(|x|) = g ◦g ◦· · · ◦g\n|\n{z\n}\nl\n(|x|) =\n2yl+1,1 −4yl+1,2, l = 1, . . . , L −1 for x ∈[−1, 1]. Now let\nf(x) = |x| −\nL−1\nX\nl=1\ngl(|x|)\n22l\n,\nthen f ∈FL,2, and\n\f\fx2 −f(x)\n\f\f ≤2−2L for x ∈[−1, 1].\nLemma 8. For multiplication function ϕ(x, y) = xy, we have\ndist (ϕ = xy, F3L,2) ≤3 · 2−2L.\n(9)\nProof. Notice that\nϕ = xy = 2\n\u0012x + y\n2\n\u00132\n−1\n2x2 −1\n2y2.\nApplying the addition properties (2)(5) and lemma 7, we obtain (9).\nNow we use the multiplication function as the basic block to construct monomials and polynomials.\nLemma 9. For a monomial Mp(x) of d variables with degree p, we have\ndist\n\u0000Mp, F3(p−1)L,3\n\u0001\n≤3(p −1) · 2−2L.\n(10)\nProof. Let Mp(x) = xi1xi2 · · · xip, i1, . . . , ip ∈{1, . . . , d}. Using induction, assume that the lemma holds for\nthe degree-p monomial Mp, consider a degree-(p + 1) monomial Mp+1(x) = Mp(x) · xip+1. Let ϕ(y, x) = yx,\nthen Mp+1(x) = ϕ(Mp(x), xip+1). From composition properties (3)(6) and lemma 8, we have\ndist (Mp+1, F3pL,3) ≤dist\n\u0000ϕ(Mp(x), xip+1), F3L,2 ◦F3(p−1)L,3\n\u0001\n≤Lϕdist\n\u0000Mp, F3(p−1)L,3\n\u0001\n+ dist (ϕ, F3L,2) ≤3p · 2−2L.\nNote that the Lipschitz norm Lϕ = 1 since xip+1 ∈[−1, 1].\nLemma 10. For a degree-p polynomial Pp(x) = P\n|k|≤p akxk, x ∈[−1, 1]d, k = (k1, . . . , kd) ∈Nd, we have\ndist\n\u0010\nPp, F(p+d\nd )(p−1)L,3\n\u0011\n< 3(p −1) · 2−2L X\n|k|≤p\n|ak|.\n(11)\n4\nProof. The lemma can be proved by applying the addition property (2)(5) and lemma 9.\nNote that the number of monomials of d variables with degree less or equal to p is\n\u0000p+d\nd\n\u0001\n.\nNow we are ready to prove theorem 6.\nProof. Let ε = exp\n\u0000−dδ\n\u0000e−1L\n1/2d −1\n\u0001\u0001\n, then L =\n\u0002\ne\n\u0000 1\ndδ log 1\nε + 1\n\u0001\u00032d. Without loss of generality, assume\nP\nk |ak| = 1. We will show that there exists ˆf ∈FL,3 such that ∥f −ˆf∥∞< 2ε.\nDenote\nf(x) = Pp(x) + R(x) :=\nX\n|k|≤p\nakxk +\nX\n|k|>p\nakxk.\nFor x ∈[−1 + δ, 1 −δ]d, we have |R(x)| < (1 −δ)p, thus truncation to p = 1\nδ log 1\nε will ensure |R(x)| < ε.\nFrom lemma 10, we have dist (Pp, FL,3) < 3(p −1) · 2−2L′, where\nL′ = L\n\u0012p + d\np\n\u0013−1\n(p −1)−1 > L\n\u0014(p + d)d\n(d/e)d\n\u0015−1\np−1\n= L\n\u0014\ne\n\u0012 1\ndδ log 1\nε + 1\n\u0013\u0015−d \u00121\nδ log 1\nε\n\u0013−1\n=\n\u0014\ne\n\u0012 1\ndδ log 1\nε + 1\n\u0013\u0015d \u00121\nδ log 1\nε\n\u0013−1\n≫log 1\nε + log 1\nδ\nfor d ≥2 and ε ≪1, then dist (Pp, FL,3) < 3(p −1) · 2−2L′ ≪ε. Thus there exists ˆf ∈FL,3 such that\n∥Pp −ˆf∥∞< ε, and ∥f −ˆf∥∞≤∥f −Pp∥∞+ ∥Pp −ˆf∥∞< 2ε.\nOne can also formulate theorem 6 as follows:\nCorollary 11. Assume that the analytic function f(x) = P\nk∈Nd akxk is absolutely convergent in [−1, 1]d.\nThen for any ε, δ > 0, there exists a function ˆf that can be represented by a deep ReLU network with depth\nL =\n\u0002\ne\n\u0000 1\ndδ log 1\nε + 1\n\u0001\u00032d and width d + 4, such that |f(x) −ˆf(x)| < 2ε P\nk |ak| for all x ∈[−1 + δ, 1 −δ]d.\n4\nThe convergence rate for the general case\nHere we prove that for deep neural networks, the approximation error decays like O\n\u0000(N/d)−1/2\u0001\nwhere N\nis the number of parameters in the model. The proof is quite simple but the result does not seem to be\navailable in the existing literature.\nTheorem 12. Given a function f : Rd →R with Fourier representation\nf(x) =\nZ\nRd eiω·x ˆf(ω)dω,\nand a compact domain B ⊂Rd containing 0, let\nCf,B =\nZ\nB\n|ω|B| ˆf(ω)|dω,\nwhere |ω|B = supx∈B |ω · x|. Then there exists a ReLU network fL,M with width M + d + 1 and depth L,\nsuch that\nZ\nB\n|f(x) −fL,M(x)|2dµ(x) ≤\n8C2\nf,B\nML ,\n(12)\nwhere µ is an arbitrary probability measure.\n5\nHere the number of parameters\nN = (d + 1)(M + d + 1) + (M + d + 2)(M + d + 2)(L −1) + (M + d + 2) = O\n\u0000(M + d)2L\n\u0001\n.\nTaking M = d, we will have L = O\n\u0000N/d2\u0001\nand the convergence rate becomes O\n\u0000(ML)−1/2\u0001\n= O\n\u0000(N/d)−1/2\u0001\n.\nNote that in the universal approximation theorem for shallow networks with one hidden layer , one can\nprove the same convergence rate O(n−1/2) = O((N/d)−1/2). Here n is the number of hidden nodes and\nN = (d + 2)n + 1 is the number of parameters.\nTheorem 12 is a direct consequence of the following theorem by Barron (1993) for networks with one\nhidden layer and sigmoidal type of activation function.\nHere a function σ is sigmoidal if it is bounded\nmeasurable on R with σ(+∞) = 1 and σ(−∞) = 0.\nTheorem 13. Given a function f and a domain B such that Cf,B ﬁnite, given a sigmoidal function σ, there\nexists a linear combination\nfn(x) =\nn\nX\nj=1\ncjσ(aj · x + bj) + c0,\naj ∈Rd, bj, cj ∈R,\nsuch that\nZ\nB\n|f(x) −fn(x)|2dµ(x) ≤\n4C2\nf,B\nn\n.\n(13)\nNotice that\nσ(z) = ReLU(z) −ReLU(z −1)\nis sigmoidal, so we have:\nCorollary 14. Given a function f and a set B with Cf,B ﬁnite, there exists a linear combination of n ReLU\nfunctions\nfn(x) =\nn\nX\nj=1\ncjReLU(aj · x + bj) + c0,\nsuch that\nZ\nB\n|f(x) −fn(x)|2dµ(x) ≤\n8C2\nf,B\nn\n.\nNext we convert this shallow network to a deep one.\nLemma 15. Let fn : Rd →R be a ReLU network with one hidden layer (as shown in the previous corollary).\nFor any decomposition n = m1 + · · · + mL, nk ∈N⋆, fn can also be represented by a ReLU network with L\nhidden layers, where the l-th layer has ml + d + 1 nodes.\nProof. Denote the input by x = (x1, . . . , xd). We construct a network with L hidden layers in which the l-th\nlayer has ml + d + 1 nodes {hl,1:ml+d+1}. Similar to the construction in proposition 2, let\nhL,1:d = hL−1,1:d = · · · = h1,1:d = x1:d,\nhl,d+j = ReLU (al,j · x + bl,j)\nfor j = 1, . . . , ml, l = 1, . . . , L, and\nh1,d+m1+1 = c0,\nhl+1,d+ml+1+1 = hl,d+ml+1 +\nml\nX\nj=1\ncl,jhl,d+j\nfor l = 1, . . . , L −1. Here we use the notation al,j = am1+···+ml−1+j (the same for bl,j and cl,j). One can\nsee that h1,m ∈ˆL(x1:d), hl+1,m ∈ˆL(hl,1:d+ml+1), m = 1, . . . , d + ml + 1, l = 1, . . . , L −1 and\nhl,d+ml+1 = c0 +\nm1+···+ml−1\nX\nj=1\ncjReLU(aj · x + bj).\n6\nThus\nfn = hL,d+mL+1 +\nmL\nX\nj=1\ncL,jhL,d+j ∈L(hL,1:d+mL+1)\ncan be represented by this deep network.\nNow consider a network with L layers where each layer has the same width M + d + 1. From lemma 15,\nthis network is equivalent to a one-layer network with ML hidden nodes. Apply corollary 14, we obtain the\ndesired approximation result for deep networks stated in theorem 12.\nAcknowledgement. We are grateful to Chao Ma for very helpful discussions during the early stage of\nthis work. We are also grateful to Jinchao Xu for his interest, which motivated us to write up this paper.\nThe work is supported in part by ONR grant N00014-13-1-0338 and Major Program of NNSFC under grant\n91130005.\nReferences\nAndrew R Barron.\nUniversal approximation bounds for superpositions of a sigmoidal function.\nIEEE\nTransactions on Information theory, 39(3):930–945, 1993.\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals\nand systems, 2(4):303–314, 1989.\nShiyu Liang and R Srikant.\nWhy deep neural networks for function approximation?\narXiv preprint\narXiv:1610.04161, 2016.\nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural\nnetworks: A view from the width. In Advances in Neural Information Processing Systems, pages 6232–\n6240, 2017.\nHadrien Montanelli and Qiang Du. Deep relu networks lessen the curse of dimensionality. arXiv preprint\narXiv:1712.08688, 2017.\nTomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when\ncan deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of\nAutomation and Computing, 14(5):503–519, 2017.\nMatus Telgarsky. Beneﬁts of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.\nDmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114,\n2017.\n7\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-07-01",
  "updated": "2018-07-01"
}