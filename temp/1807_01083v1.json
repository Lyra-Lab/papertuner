{
  "id": "http://arxiv.org/abs/1807.01083v1",
  "title": "A Mean-Field Optimal Control Formulation of Deep Learning",
  "authors": [
    "Weinan E",
    "Jiequn Han",
    "Qianxiao Li"
  ],
  "abstract": "Recent work linking deep neural networks and dynamical systems opened up new\navenues to analyze deep learning. In particular, it is observed that new\ninsights can be obtained by recasting deep learning as an optimal control\nproblem on difference or differential equations. However, the mathematical\naspects of such a formulation have not been systematically explored. This paper\nintroduces the mathematical formulation of the population risk minimization\nproblem in deep learning as a mean-field optimal control problem. Mirroring the\ndevelopment of classical optimal control, we state and prove optimality\nconditions of both the Hamilton-Jacobi-Bellman type and the Pontryagin type.\nThese mean-field results reflect the probabilistic nature of the learning\nproblem. In addition, by appealing to the mean-field Pontryagin's maximum\nprinciple, we establish some quantitative relationships between population and\nempirical learning problems. This serves to establish a mathematical foundation\nfor investigating the algorithmic and theoretical connections between optimal\ncontrol and deep learning.",
  "text": "arXiv:1807.01083v1  [math.OC]  3 Jul 2018\nNoname manuscript No.\n(will be inserted by the editor)\nA Mean-Field Optimal Control Formulation of Deep\nLearning\nWeinan E, Jiequn Han, Qianxiao Li\nAbstract Recent work linking deep neural networks and dynamical systems\nopened up new avenues to analyze deep learning. In particular, it is observed\nthat new insights can be obtained by recasting deep learning as an optimal\ncontrol problem on diﬀerence or diﬀerential equations. However, the mathe-\nmatical aspects of such a formulation have not been systematically explored.\nThis paper introduces the mathematical formulation of the population risk\nminimization problem in deep learning as a mean-ﬁeld optimal control prob-\nlem. Mirroring the development of classical optimal control, we state and prove\noptimality conditions of both the Hamilton-Jacobi-Bellman type and the Pon-\ntryagin type. These mean-ﬁeld results reﬂect the probabilistic nature of the\nlearning problem. In addition, by appealing to the mean-ﬁeld Pontryagin’s\nmaximum principle, we establish some quantitative relationships between pop-\nulation and empirical learning problems. This serves to establish a mathemat-\nical foundation for investigating the algorithmic and theoretical connections\nbetween optimal control and deep learning.\n1 Introduction\nDeep learning [1,2,3] has become a primary tool in many modern machine\nlearning tasks, such as image classiﬁcation and segmentation. Consequently,\nthere is a pressing need to provide a solid mathematical framework to analyze\nvarious aspects of deep neural networks. The recent line of work on linking\nWeinan E\nPrinceton University, Princeton, NJ 08544, USA,\nBeijing Institute of Big Data Research and Peking University, Beijing, China 100871\nJiequn Han\nPrinceton University, Princeton, NJ 08544, USA\nQianxiao Li\nInstitute of High Performance Computing, Agency for Science, Technology and Research.\n1 Fusionopolis Way, Connexis North, Singapore 138632\n2\nWeinan E, Jiequn Han, Qianxiao Li\ndynamical systems, optimal control and deep learning has suggested such a\ncandidate [4,5,6,7,8,9,10,11,12,13]. In this view, ResNet [14] can be regarded\nas a time-discretization of a continuous-time dynamical system. Learning (usu-\nally in the empirical risk minimization form) is then recast as an optimal con-\ntrol problem, from which novel algorithms [5,6] and network structures [7,\n8,9,10] can be designed. An attractive feature of this approach is that, the\ncompositional structure, which is widely considered the essence of deep neural\nnetworks is explicitly taken into account in the time-evolution of the dynamical\nsystems.\nWhile most prior work on the dynamical systems viewpoint of deep learning\nhave focused on algorithms and network structures, this paper aims to study\nthe fundamental mathematical aspects of the formulation. Indeed, we show\nthat the most general formulation of the population risk minimization problem\ncan be regarded as a mean-ﬁeld optimal control problem, in the sense that the\noptimal control parameters (or equivalently, the trainable weights) depend on\nthe population distribution of input-target pairs. Our task is then to analyze\nthe mathematical properties of this mean-ﬁeld control problem. Mirroring the\ndevelopment of classical optimal control, we will proceed in two parallel, but\ninter-connected ways, namely the dynamic programming formalism and the\nmaximum principle formalism.\nThe paper is organized as follows. We discuss related work in Sec. 2 and\nintroduce the basic mean-ﬁeld optimal control formulation of deep learning in\nSec. 3. In Sec. 4, following the classical dynamic programming approach [15],\nwe introduce and study the properties of a value function for the mean-ﬁeld\ncontrol problem whose state space is an appropriate Wasserstein space of prob-\nability measures. By deﬁning an appropriate notion of derivative with respect\nto probability measures, we show that the value function is related to solutions\nof an inﬁnite dimensional Hamilton-Jacobi-Bellman (HJB) partial diﬀerential\nequation. With the concept of viscosity solutions [16], we show in Sec. 5 that\nthe HJB equation admits a unique viscosity solution and completely charac-\nterize the optimal loss function and the optimal control policy of the mean-\nﬁeld control problem. This establishes a concrete link between the learning\nproblem viewed as a variational problem and the Hamilton-Jacobi-Bellman\nequation that is associated with the variational problem. It should be noted\nthe essential ideas in the proof of Sec. 4 and 5 are not new, but we present our\nsimpliﬁed treatment for this particular setting.\nNext, in Sec. 6, we develop the more local theory based on the Pontryagin’s\nmaximum principle (PMP) [17]. We state and prove a mean-ﬁeld version of the\nclassical PMP that provides necessary conditions for optimal controls. Further,\nwe study situations when the mean-ﬁeld PMP admits a unique solution, which\nthen imply that it is also suﬃcient for optimality, provided an optimal solu-\ntion exists. We will see in Sec. 7 that compared with the HJB approach, this\nfurther requires the fact that the time horizon of the learning problem is small\nenough. Finally, in Sec. 8 we study the relationship between the population\nrisk minimization problem (cast as a mean-ﬁeld control problem and charac-\nterized by a mean-ﬁeld PMP) and its empirical risk minimization counter-part\nA Mean-Field Optimal Control Formulation of Deep Learning\n3\n(cast as a classical control problem and characterized by a classical, sampled\nPMP). We prove that under appropriate conditions for every stable solution\nof the mean-ﬁeld PMP, with high probability there exist close-by solutions of\nthe sampled PMP, and the latter converge in probability to the former, with\nexplicit error estimates on both the distance between the solutions and the\ndistance between their loss function values. This provides a type of a priori\nerror estimate that has implications on the generalization ability of neural\nnetworks, which is an important and active area of machine learning research.\nNote that it is not the purpose of this paper to prove the sharpest estimates\nunder the most general conditions, thus we have taken the most convenient\nbut reasonable assumptions and the results presented could be sharpened with\nmore technical details. In each section from Sec. 4 to Sec. 8, we ﬁrst present\nthe mathematical results, and then discuss the related implications in deep\nlearning. Furthermore, in this work we shall focus our analysis on the contin-\nuous idealization of deep residual networks, but we believe that much of the\nanalysis presented also carry over to the discrete domain (i.e. discrete layers).\n2 Related work\nThe connection between back-propagation and optimal control of dynamical\nsystems is known since the earlier works on control and deep learning [18,19,\n20]. Recently, the dynamical systems approach to deep learning was proposed\nin [4] and explored in the direction of training algorithms based on the PMP\nand the method of successive approximations [5,6]. In another vein, there are\nalso studies on the continuum limit of neural networks [11,12] and on designing\nnetwork architectures for deep learning [7,8,9,10] based on dynamical systems\nand diﬀerential equations. Instead of analysis of algorithms or architectures,\nthe present paper focuses on the mathematical aspects of the control formula-\ntion itself, and develops a mean-ﬁeld theory that characterize the optimality\nconditions and value functions using both PDE (HJB) and ODE (PMP) ap-\nproaches. The over-arching goal is to develop the mathematical foundations of\nthe optimal control formulation of deep learning.\nIn the control theory literature, mean-ﬁeld optimal control is an active\narea of research. Many works on mean-ﬁeld games [21,22,23,24], the control\nof McKean-Vlasov systems [25,26,27], and the control of Cucker-Smale sys-\ntems [28,29,30] focus on deriving the limiting partial diﬀerential equations\nthat characterize the optimal control as the number of agents goes to inﬁnity.\nThis is akin to the theory of the propagation of chaos [31]. Meanwhile there are\nalso works discussing the stochastic maximum principle for stochastic diﬀer-\nential equations of mean-ﬁeld type [32,33,34]. The present paper diﬀers from\nall previous works in two aspects. First, in the context of continuous-time deep\nlearning, the problem diﬀers from these previous control formulations as the\nsource of randomness are coupled input-target pairs (the latter determines the\nterminal loss function, which can now be regarded as a random function). On\nthe other hand, a simplifying feature in our case is that the dynamics, given\n4\nWeinan E, Jiequn Han, Qianxiao Li\nthe input-target pair, are otherwise deterministic. Second, the dynamics of\neach random realization are independent of the distribution law of the popu-\nlation, and are coupled only through the shared control parameters. This is to\nbe contrasted with optimal control of McKean-Vlasov dynamics [34,26,27] or\nmean-ﬁeld games [21,22,23,24], where the population law directly enters the\ndynamical equations (and not just through the shared control). Thus, in this\nsense our dynamical equations are much simpler to analyze. Consequently,\nalthough some of our results can be deduced from more general mean-ﬁeld\nanalysis in the control literature, here we will present simpliﬁed derivations\ntailored to our setting, Note also that there are neural network structures\n(e.g. batch-normalization) that can be considered to have explicit mean-ﬁeld\ndynamics, and we defer this discussion to Sec. 9.\n3 From ResNets to mean-ﬁeld optimal control\nLet us now present the optimal control formulation of deep learning as in-\ntroduced in [4,5,6]. In the simplest form, the feed-forward propagation in a\nT -layer residual network can be represented by the diﬀerence equations\nxt+1 = xt + f(xt, θt),\nt = 0, . . . , T −1.\n(1)\nwhere x0 is the input (image, time-series, etc.) and xT is the ﬁnal output. The\nﬁnal output is then compared with some target y0 corresponding to x0 via\nsome loss function. The goal of learning is to tune the trainable parameters\nθ0, . . . , θT −1 such that xT is close to y0. The only change in the continuous-\ntime idealization of deep residual learning, which we will subsequently focus\non, is that instead of the diﬀerence equation (1), the forward dynamics is now\na diﬀerential equation. Let us now introduce this formulation more precisely.\nLet (Ω, F, P) be a ﬁxed and suﬃciently rich probability space so that all\nsubsequently required random variables can be constructed. Suppose x0 ∈\nRd and y0 ∈Rl are random variables jointly distributed according to µ0 :=\nP(x0,y0) (hereafter, for each random variable X we denote its distribution or\nlaw by PX). This represents the distribution of the input-target pairs, which\nwe assume can be embedded in Euclidean spaces. Consider a set of admissible\ncontrols or training weights Θ ⊆Rm. In typical deep learning, Θ is taken as the\nwhole space Rm, but here we consider the more general case where Θ can be\nconstrained. Fix T > 0 (network “depth”) and let f (feed-forward dynamics),\nΦ (terminal loss function) and L (regularizer) be functions\nf : Rd × Θ →Rd,\nΦ : Rd × Rl →R,\nL : Rd × Θ →R.\nWe deﬁne the state dynamics as the ordinary diﬀerential equation (ODE)\n˙xt = f(xt, θt)\n(2)\nwith initial condition equals to the random variable x0. Thus, this is a stochas-\ntic ODE, whose only source of randomness is on the initial condition. Consider\nA Mean-Field Optimal Control Formulation of Deep Learning\n5\nthe set of essentially bounded measurable controls L∞([0, T ], Θ). To improve\nclarity, we will reserve bold-faced letters for path-space quantities. For exam-\nple, θ ≡{θt : 0 ≤t ≤T }. In contrast, variables/functions taking values in\nﬁnite-dimensional Euclidean spaces are not bold-faced.\nThe population risk minimization problem in deep learning can hence be\nposed as the following mean-ﬁeld optimal control problem\ninf\nθ∈L∞([0,T ],Θ) J(θ) := Eµ0\n\"\nΦ(xT , y0) +\nZ T\n0\nL(xt, θt)dt\n#\n,\nSubject to (2).\n(3)\nThe term “mean-ﬁeld” highlights the fact that θ is shared by a whole popula-\ntion of input-target pairs, and the optimal control must depend on the law of\nthe input-target random variables. Strictly speaking, the law of x does not en-\nter the forward equations explicitly (unlike e.g., McKean-Vlasov control [34]),\nand hence our forward dynamics are not explicitly in mean-ﬁeld form. Never-\ntheless, we will use the term “mean-ﬁeld” to emphasize the dependence of the\ncontrol on the population distribution.\nIn contrast, if we were to perform empirical risk minimization, as is often\nthe case in practice (and is the case analyzed by previous work on algorithms [5,\n6]), we would ﬁrst draw i.i.d. samples {xi\n0, yi\n0}N\ni=1 ∼µ0 and pose the sampled\noptimal control problem\ninf\nθ∈L∞([0,T ],Θ) JN(θ) := 1\nN\nN\nX\ni=1\n\"\nΦ(xi\nT , yi\n0) +\nZ T\n0\nL(xi\nt, θt)dt\n#\n,\nSubject to\n˙xi\nt = f(xi\nt, θt),\ni = 1, . . . , N.\n(4)\nThus, the solutions of sampled optimal control problems are typically random\nvariables. We now focus our analysis on the mean-ﬁeld problem (3) and only\nlater in Sec. 8 relate it with the sampled problem (4).\nAdditional Notation\nThroughout this paper, we always use w to denote the concatenated (d +\nl)-dimensional variable (x, y) where x ∈Rd and y ∈Rl. Correspondingly\n¯f(w, θ) := (f(x, θ), 0) is the extended (d+l)-dimensional feed-forward function,\n¯L(w, θ) := L(x, θ) is the extended (d + l)-dimensional regularization loss, and\n¯Φ(w) := Φ(x, y) still denotes the terminal loss function. We denote by x · y\nthe inner product of two Euclidean vectors x and y with the same dimension.\nThe Euclidean norm is denoted by ∥· ∥and the absolute value is denoted by\n| · |. Gradient operators on Euclidean spaces are denoted by ∇with subscripts\nindicating the variable with which the derivative is taken with. In contrast,\nwe use D to represent the Fréchet derivative on Banach spaces. Namely, if\nx ∈U and F : U →V is a mapping between two Banach spaces (U, ∥· ∥U)\n6\nWeinan E, Jiequn Han, Qianxiao Li\nand (V, ∥· ∥V ), then DF(x) is deﬁned by the linear operator DF(x) : U →V\ns.t.\nr(x, y) := ∥F(x + y) −F(x) −DF(x)y∥V\n∥y∥U\n→0,\nas ∥y∥U →0.\n(5)\nFor a matrix A, we use the symbol A ⪯0 to mean that A is negative semi-\ndeﬁnite.\nLet the Banach space L∞([0, T ], E) be the set of essentially bounded mea-\nsurable functions from [0, T ] to E, where E is a subset of a Euclidean space with\nthe usual Lebesgue measure. The norm is ∥x∥L∞([0,T ],E) = ess supt∈[0,T ] ∥x(t)∥,\nand we shall write for brevity ∥· ∥L∞in place of ∥· ∥L∞([0,T ],E). In this paper,\nE is often either Θ or Rd, and the path-space variables we consider in this\npaper, such as the controls θ, will mostly be deﬁned in this space.\nAs this paper introduces a mean-ﬁeld optimal control approach, we also\nneed some notation for the random variables and their distributions. We\nuse the shorthand L2(Ω, Rd+l) for L2((Ω, F, P), Rd+l), the set of Rd+l-valued\nsquare integrable random variables. We equip this Hilbert space with the norm\n∥X∥L2 := (E∥X∥2)1/2 for X ∈L2(Ω, Rd+l). We denote by P2(Rd+l) the set\nof square integrable probability measures on the Euclidean space Rd+l. Note\nthat X ∈L2(Ω, Rd+l) if and only if PX ∈P2(Rd+l). The space P2(Rd+l) is\nregarded as a metric space equipped with the 2-Wasserstein distance\nW2(µ, ν) := inf\nn\u0010 Z\nRd+l×Rd+l ∥w −z∥2π(dw, dz)\n\u00111/2 \f\f\f\nπ ∈P2(Rd+l × Rd+l) with marginals µ and ν\no\n:= inf\nn\n∥X −Y ∥L2\n\f\f\f X, Y ∈L2(Ω, Rd+l) with PX = µ, PY = ν\no\n.\nFor µ ∈P2(Rd+l), we also deﬁne ∥µ∥L2 := (\nR\nRd+l ∥w∥2µ(dw))1/2.\nGiven a measurable function ψ : Rd+l →Rq that is square integrable with\nrespect to µ, we use the notation\n⟨ψ(.), µ⟩:=\nZ\nRd+l ψ(w)µ(dw).\nNow, we introduce some notation for the dynamical evolution of proba-\nbilities. Given ξ ∈L2(Ω, Rd+l) and a control process θ ∈L∞([0, T ], Θ), we\nconsider the following dynamical system for t ≤s ≤T :\nW t,ξ,θ\ns\n= ξ +\nZ s\nt\n¯f(W t,ξ,θ\ns\n, θt) ds.\nNote that W t,ξ,θ\ns\nis always square integrable given ¯f(w, θ) is Lipschitz contin-\nuous with respect to w. Let µ = Pξ ∈P2(Rd+l), we denote the law of W t,ξ,θ\ns\nfor simplicity by\nPt,µ,θ\ns\n:= PW t,ξ,θ\ns\n.\nA Mean-Field Optimal Control Formulation of Deep Learning\n7\nThis is valid since the law of W t,ξ,θ\ns\nshould only depend on the law of ξ and\nnot on the random variable itself. This notation also allow as to write down\nthe ﬂow or semi-group property of the dynamical system as\nPt,µ,θ\ns\n= P\nˆt,Pt,µ,θ\nˆt\n,θ\ns\n,\n(6)\nfor all 0 ≤t ≤ˆt ≤s ≤T, µ ∈P2(Rd+l), θ ∈L∞([0, T ], Θ).\nFinally, throughout the results and proofs, we will use K or C with sub-\nscripts as names for generic constants, whose values may change from line to\nline when there is no need for them to be distinct. In general, these constants\nmay implicitly depend on T and the ambient dimensions d, m, but for brevity\nwe omit them in the rest of the paper.\n4 Mean-ﬁeld dynamic programming principle and HJB equation\nWe begin our analysis of (3) by employing the dynamic programming prin-\nciple and the Hamilton-Jacobi-Bellman formalism. In this approach, the key\nidea is to deﬁne a value function that corresponds to the optimal loss of the\ncontrol problem (3), but under a general starting time and starting state.\nOne can then derive a partial diﬀerential equation (Hamilton-Jacobi-Bellman\nequation, or HJB equation) to be satisﬁed by such a value function, which\ncharacterizes both the optimal loss function value and the optimal control\npolicy of the original control problem. Compared to the classical optimal con-\ntrol case corresponding to empirical risk minimization in learning, here the\nvalue function’s state argument is no longer a ﬁnite-dimensional vector, but\nan inﬁnite-dimensional object corresponding to the joint distribution of the\ninput-target pair. We shall interpret it as an element of a suitable Wasserstein\nspace. The detailed mathematical deﬁnition of this value function and its basic\nproperties are discussed in Subsec. 4.1.\nIn the ﬁnite-dimensional case, the HJB equation is a classical partial dif-\nferential equation. In contrast, since the state variables we are dealing with\nare probability measures rather than Euclidean vectors, we need a concept of\nderivative with respect to a probability measure, as introduced by Lions in his\ncourse at Collège de France [35]. We give a brief introduction of this concept\nin Subsec. 4.2 and refer readers to the lecture notes [36] for more details. We\nthen present the resulting inﬁnite-dimensional HJB equation in Subsec. 4.3.\nThroughout this section and next section (Sec. 5), we assume\n(A1) f, L, Φ are bounded; f, L, Φ are Lipschitz continuous with respect to x, and\nthe Lipschitz constants of f and L are independent of θ.\n(A2) µ0 ∈P2(Rd+l).\n8\nWeinan E, Jiequn Han, Qianxiao Li\n4.1 Value function and its properties\nAdopting the viewpoint of taking probability measures µ ∈P2(Rd+l) as state\nvariables, we can deﬁne a time-dependent objective functional\nJ(t, µ, θ) := E(xt,y0)∼µ\n\"\nΦ(xT , y0) +\nZ T\nt\nL(xt, θt)dt\n#\n(subject to (2))\n= ⟨¯Φ(.), Pt,µ,θ\nT\n⟩+\nZ T\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds.\n(7)\nThe second line in the above is just a rewriting of the ﬁrst line based on the\nnotation introduced earlier. Here, we abuse the notation J in (3) for the new\nobjective functional, which now has additional arguments t, µ. Of course, J(θ)\nin (3) corresponds to J(0, µ0, θ) in (7).\nThe value function v∗(t, µ) is deﬁned as a real-valued function on [0, T ] ×\nP2(Rd+l) through\nv∗(t, µ) =\ninf\nθ∈L∞([0,T ],Θ) J(t, µ, θ).\n(8)\nIf we assume θ∗attains the inﬁmum in (3), then by deﬁnition\nJ(θ∗) = v∗(0, µ0).\nThe following proposition shows the continuity of the value function.\nProposition 1 The function (t, µ) 7→J(t, µ, θ) is Lipschitz continuous on\n[0, T ] × P2(Rd+l), uniformly with respect to θ ∈L∞([0, T ], Θ), and the value\nfunction v∗(t, µ) is Lipschitz continuous on [0, T ] × P2(Rd+l).\nProof. We ﬁrst establish some elementary estimates based on the assumptions.\nWe suppose\n⟨¯L(., θ), µ⟩≤C.\n(9)\nLet X, Y ∈L2(Ω, Rd+l) such that PX = µ, PY = ˆµ, the Lipschitz continuity\nof ¯L gives us\n|⟨¯L(., θ), µ⟩−⟨¯L(., θ), ˆµ⟩| = |E[¯L(X, θ) −¯L(Y, θ)]| ≤KL∥X −Y ∥L2.\nNote that in the proceeding inequality the left side does not depend on the\nchoice of X, Y while the right side does. Hence we can take the inﬁmum over\nall the joint choices of X, Y to get\n|⟨¯L(., θ), µ⟩−⟨¯L(., θ), ˆµ⟩|\n≤KL × inf\nn\n∥X −Y ∥L2\n\f\f\f X, Y ∈L2(Ω, Rd+l) with PX = µ, PY = ν\no\n≤KLW2(µ, ˆµ).\n(10)\nThe same argument applied to ¯Φ gives us\n|⟨¯Φ(.), µ⟩−⟨¯Φ(.), ˆµ⟩| ≤KLW2(µ, ˆµ).\n(11)\nA Mean-Field Optimal Control Formulation of Deep Learning\n9\nFor the deterministic ODE\ndwθ\nt\ndt\n= ¯f(wθ\nt , θt),\nwθ\n0 = w0,\ndeﬁne the induced ﬂow map as\nh(t, w0, θ) := wθ\nt .\nUsing Gronwall’s inequality with the boundedness and Lipschitz continuity of\n¯f, we know\n|h(t, w, θ) −h(t, ˆw, θ)| ≤KL∥w −ˆw∥,\n|h(t, w, θ) −h(ˆt, w, θ)| ≤KL|t −ˆt|.\nTherefore we use the deﬁnition of Wasserstein distance to obtain\nW2(Pt,µ,θ\ns\n, Pt,ˆµ,θ\ns\n)\n= inf\nn\n∥X −Y ∥L2\n\f\f\f X, Y ∈L2(Ω, Rd+l) with PX = Pt,µ,θ\ns\n, PY = Pt,ˆµ,θ\ns\no\n= inf\nn\n∥h(s −t, X, θ) −h(s −t, Y, θ)∥L2\n\f\f\f\nX, Y ∈L2(Ω, Rd+l) with PX = µ, PY = ˆµ\no\n≤inf\nn\nKL∥X −Y ∥L2\n\f\f\f X, Y ∈L2(Ω, Rd+l) with PX = µ, PY = ˆµ\no\n= KLW2(µ, ˆµ)\n(12)\nand similarly\nW2(Pt,µ,θ\ns\n, µ) ≤KL|s −t|.\n(13)\nThe ﬂow property (6) and estimates (12), (13) together give us\nW2(Pt,µ,θ\ns\n, Pˆt,ˆµ,θ\ns\n) = W2(P\nˆt,Pt,µ,θ\nˆt\n,θ\ns\n, Pˆt,ˆµ,θ\ns\n)\n≤KLW2(Pt,µ,θ\nˆt\n, ˆµ)\n≤KL(|t −ˆt| + W2(µ, ˆµ)).\n(14)\nNow for all 0 ≤t ≤ˆt ≤T , µ, ˆµ ∈P2(Rd+l), θ ∈L∞([0, T ], Θ), we em-\nploy (9), (10), (11), and (14) to obtain\n|J(t, µ, θ) −J(ˆt, ˆµ, θ)|\n≤\nZ ˆt\nt\n|⟨¯L(., θs), Pt,µ,θ\ns\n⟩| ds +\nZ T\nˆt\n|⟨¯L(., θs), Pt,µ,θ\ns\n⟩−⟨¯L(., θs), Pˆt,ˆµ,θ\ns\n⟩| ds\n+ |⟨¯Φ(.), Pt,µ,θ\nT\n⟩−⟨¯Φ(.), P\nˆt,ˆµ,θ\nT\n⟩|\n≤C|ˆt −t| + KL sup\nˆt≤s≤T\nW2(Pt,µ,θ\ns\n, Pˆt,ˆµ,θ\ns\n)\n≤KL(|t −ˆt| + W2(µ, ˆµ)),\n10\nWeinan E, Jiequn Han, Qianxiao Li\nwhich gives us the desired Lipschitz continuity property.\nFinally, combining the fact that\n|v∗(t, µ) −v∗(ˆt, ˆµ)| ≤supθ∈L∞([0,T ],Θ) |J(t, µ, θ) −J(ˆt, ˆµ, θ)|,\n∀t, ˆt ∈[0, T ], µ, ˆµ ∈P2(Rd+l),\nand J(t, µ, θ) is Lipschitz continuous at (t, µ) ∈[0, T ] × P2(Rd+l), uniformly\nwith respect to θ ∈L∞([0, T ], Θ), we deduce that the value function v∗(t, µ)\nis Lipschitz continuous on [0, T ] × P2(Rd+l).\nThe important observation we now make is that the value function satis-\nﬁes a recursive relation. This is known as the dynamic programming principle,\nwhich forms the basis of deriving the Hamilton-Jacobi-Bellman equation. In-\ntuitively, the dynamic programming principle states that for any optimal tra-\njectory, starting from any intermediate state in the trajectory, the remaining\ntrajectory must again be optimal, starting from that time and state. We now\nstate and prove this intuitive statement precisely.\nProposition 2 (Dynamic programming principle) For all 0 ≤t ≤ˆt ≤T ,\nµ ∈P2(Rd+l), we have\nv∗(t, µ) =\ninf\nθ∈L∞([0,T ],Θ)\nh Z ˆt\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds + v∗(ˆt, Pt,µ,θ\nˆt\n)\ni\n.\n(15)\nProof. The proof is elementary as in the context of deterministic control prob-\nlem. We provide it as follows for completeness.\n1). Given ﬁxed t, ˆt, µ and any θ1 ∈L∞([0, T ], Θ), we consider the proba-\nbility measure Pt,µ,θ1\nˆt\n. Fix ε > 0 and by deﬁnition of value function (8) we can\npick θ2 ∈L∞([0, T ], Θ) satisfying\nv∗(ˆt, Pt,µ,θ1\nˆt\n) + ε ≥⟨¯Φ(.), P\nˆt,Pt,µ,θ1\nˆt\n,θ2\nT\n⟩+\nZ T\nˆt\n⟨¯L(., θ2\ns), P\nˆt,Pt,µ,θ1\nˆt\n,θ2\ns\n⟩ds.\n(16)\nNow consider the control process ˆθ deﬁned as\nˆθs = 1{s<ˆt}θ1\ns + 1{s≥ˆt}θ2\ns.\nA Mean-Field Optimal Control Formulation of Deep Learning\n11\nThus we can use (16) and ﬂow property (6) to deduce\nv∗(t, µ)\n≤\nZ T\nt\n⟨¯L(., ˆθs), Pt,µ, ˆθ\ns\n⟩ds + ⟨¯Φ(.), Pt,µ, ˆθ\nT\n⟩\n=\nZ ˆt\nt\n⟨¯L(., ˆθs), Pt,µ, ˆθ\ns\n⟩ds +\nZ T\nˆt\n⟨¯L(., ˆθs), Pt,µ, ˆθ\ns\n⟩ds + ⟨¯Φ(.), Pt,µ, ˆθ\nT\n⟩\n=\nZ ˆt\nt\n⟨¯L(., ˆθs), Pt,µ, ˆθ\ns\n⟩ds +\nZ T\nˆt\n⟨¯L(., θ2\ns), P\nˆt,Pt,µ,θ1\nˆt\n,θ2\ns\n⟩ds + ⟨¯Φ(.), P\nˆt,Pt,µ,θ1\nˆt\n,θ2\nT\n⟩\n≤\nZ ˆt\nt\n⟨¯L(., ˆθs), Pt,µ, ˆθ\ns\n⟩ds + v∗(ˆt, Pt,µ,θ1\nˆt\n) + ε\n=\nZ ˆt\nt\n⟨¯L(., θ1\ns), Pt,µ,θ1\ns\n⟩ds + v∗(ˆt, Pt,µ,θ1\nˆt\n) + ε.\nAs θ1 and ε are both arbitrary, we have\nv∗(t, µ) ≤\ninf\nθ∈L∞([0,T ],Θ)\nh Z ˆt\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds + v∗(ˆt, Pt,µ,θ\nˆt\n)\ni\n.\n2). Fix ε > 0 again and we choose by deﬁnition θ3 ∈L∞([0, T ], Θ) such\nthat\nv∗(t, µ) + ε ≥\nZ T\nt\n⟨¯L(., θs), Pt,µ,θ3\ns\n⟩ds + ⟨¯Φ(.), Pt,µ,θ3\nT\n⟩.\nUsing the ﬂow property (6) and the deﬁnition of the value function again gives\nus the estimate\nv∗(t, µ) + ε\n≥\nZ T\nt\n⟨¯L(., θ3\ns), Pt,µ,θ3\ns\n⟩ds + ⟨¯Φ(.), Pt,µ,θ3\nT\n⟩\n=\nZ ˆt\nt\n⟨¯L(., θ3\ns), Pt,µ,θ3\ns\n⟩ds +\nZ T\nˆt\n⟨¯L(., θ3\ns), P\nˆt,Pt,µ,θ3\nˆt\n,θ3\ns\n⟩ds + ⟨¯Φ(.), P\nˆt,Pt,µ,θ3\nˆt\n,θ3\nT\n⟩\n≥\nZ ˆt\nt\n⟨¯L(., θ3\ns), Pt,µ,θ3\ns\n⟩ds + v∗(ˆt, Pt,µ,θ3\nˆt\n)\n≥\ninf\nθ∈L∞([0,T ],Θ)\nh Z ˆt\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds + v∗(ˆt, Pt,µ,θ\nˆt\n)\ni\n.\nHence we deduce\nv∗(t, µ) ≥\ninf\nθ∈L∞([0,T ],Θ)\nh Z ˆt\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds + v∗(ˆt, Pt,µ,θ\nˆt\n)\ni\n.\nCombining the inequalities in the two parts completes the proof.\n12\nWeinan E, Jiequn Han, Qianxiao Li\n4.2 Derivative and Chain Rule in Wasserstein Space\nIn classical ﬁnite-dimensional optimal control, the HJB equation can be for-\nmally derived from the dynamic programming principle by a Taylor expansion\nof the value function with respect to the state vector. However, in the current\nformulation, the state is now a probability measure. To derive the correspond-\ning HJB equation in this setting, it is essential to deﬁne a notion of deriva-\ntive of the value function with respect to a probability measure. The basic\nidea to achieve this is to take probability measures on Rd+l as laws of Rd+l-\nvalued random variables on the probability space (Ω, F, P) and then use the\ncorresponding Banach space of random variables to deﬁne derivatives. This\napproach is more extensively outlined in [36].\nConcretely, let us take any function u : P2(Rd+l) →R. We now lift it into\nits “extension” U, a function deﬁned on L2(Ω, Rd+l) by\nU(X) = u(PX),\n∀X ∈L2(Ω, Rd+l).\n(17)\nWe say u is C1(P2(Rd+l)) if the lifted function U is Fréchet diﬀerentiable with\ncontinuous derivatives. Since we can identify L2(Ω, Rd+l) with its dual space,\nif the Fréchet derivative DU(X) exists, by Riesz’ theorem one can view it as\nan element of L2(Ω, Rd+l):\nDU(X)(Y ) = E[DU(X) · Y ],\n∀Y ∈L2(Ω, Rd+l).\nThe important result one can prove is that the law of DU(X) does not depend\non X but only on the law of X. Accordingly we have the representation\nDU(X) = ∂µu(PX)(X),\nfor some function ∂µu(PX) : Rd+l →Rd+l, which is called derivative of u at\nµ = PX. Moreover, we know ∂µu(µ) is square integrable with respect to µ.\nWe next need a chain rule deﬁned on P2(Rd+l). Consider the dynamical\nsystem\nWt = ξ +\nZ t\n0\n¯f(Ws) ds,\nξ ∈L2(Ω, Rd+l),\nand u ∈C1(P2(Rd+l)). Then, for all t ∈[0, T ], we have\nu(PWt) = u(PW0) +\nZ t\n0\n⟨∂µu(PWs)(.) · ¯f(.), PWs⟩ds,\n(18)\nor equivalently its lifted version\nU(Wt) = U(W0) +\nZ t\n0\nE[DU(Ws) · ¯f(Ws)] ds.\n(19)\nA Mean-Field Optimal Control Formulation of Deep Learning\n13\n4.3 HJB equation in Wasserstein Space\nGuided by the dynamic programming principle (15) and formula (18), we are\nready to formally derive the associated HJB equation as follows. Let ˆt = t+δt\nwith δt being small. By performing a formal Taylor series expansion of (15),\nwe have\n0 =\ninf\nθ∈L∞([0,T ],Θ)\nh\nv∗(t + δt, Pt,µ,θ\nt+δt ) −v∗(t, µ) +\nZ t+δt\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds\ni\n≈\ninf\nθ∈L∞([0,T ],Θ)\nh\n∂tv(t, µ)δt +\nZ t+δt\nt\n⟨∂µv(t, µ)(.) · ¯f(., θ) + ¯L(., θs), µ⟩ds\ni\n≈δt\ninf\nθ∈L∞([0,T ],Θ)\nh\n∂tv(t, µ) + ⟨∂µv(t, µ)(.) · ¯f(., θ) + ¯L(., θs), µ⟩\ni\n.\nPassing to the limit δt →0, we obtain the following HJB equation\n\n\n\n∂v\n∂t + inf\nθ∈Θ\n\n∂µv(t, µ)(.) · ¯f(., θ) + ¯L(., θ), µ\n\u000b\n= 0,\non [0, T ) × P2(Rd+l),\nv(T, µ) = ⟨¯Φ(.), µ⟩,\non P2(Rd+l),\n(20)\nwhich the value function should satisfy. The rest of this and the next section is\nto establish the precise link between equation (20) and the value function (8).\nWe now prove a veriﬁcation result, which essentially says that if we have a\nsmooth enough solution of the HJB equation (20), then this solution must be\nthe value function. Moreover, the HJB allows us to identify the optimal control\npolicy.\nProposition 3 Let v be a function in C1,1([0, T ]×P2(Rd+l)). If v is a solution\nto (20) and there exists θ†(t, µ), which is a mapping (t, µ) 7→Θ attaining the\ninﬁmum in (20), then v(t, µ) = v∗(t, µ), and θ† is an optimal feedback control\npolicy, i.e. θ = θ∗is a solution of (3), where θ∗\nt := θ†(t, Pw∗\nt ) with Pw∗\n0 = µ0\nand dw∗\nt /dt = ¯f(w∗\nt , θ∗\nt ).\nProof. Given any control process θ, one can apply formula (18) between s = t\nand s = T with explicit t dependence and obtain\nv(T, Pt,µ,θ\nT\n) = v(t, µ) +\nZ T\nt\n∂v\n∂t (s, Pt,µ,θ\ns\n) + ⟨∂µv(s, Pt,µ,θ\ns\n)(.) · ¯f(.; θs), Pt,µ,θ\ns\n⟩ds.\nEquivalently, we have\nv(t, µ) = v(T, Pt,µ,θ\nT\n) −\nZ T\nt\n∂v\n∂t (s, Pt,µ,θ\ns\n) + ⟨∂µv(s, Pt,µ,θ\ns\n)(.) · ¯f(.; θs), Pt,µ,θ\ns\n⟩ds\n≤v(T, Pt,µ,θ\nT\n) +\nZ T\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds\n= ⟨¯Φ(.), PT,µ,θ\nt\n⟩+\nZ T\nt\n⟨¯L(., θs), Pt,µ,θ\ns\n⟩ds\n= J(t, µ, θ),\n14\nWeinan E, Jiequn Han, Qianxiao Li\nwhere the ﬁrst inequality comes from the inﬁmum condition in (20). Since the\ncontrol process is arbitrary, we have\nv(t, µ) ≤v∗(t, µ).\n(21)\nReplacing the arbitrary control process with θ∗where θ∗\nt = θ†(t, Pt,µ,θ∗\ns\n) is\ngiven by the optimal feedback control and repeating the above argument,\nnoting that the inequality becomes equality since the inﬁmum is attained, we\nhave\nv(t, µ) = J(t, µ, θ∗) ≥v∗(t, µ).\n(22)\nTherefore we obtain v(t, µ) = v∗(t, µ) and θ† deﬁnes an optimal feedback\ncontrol policy.\nProp. 3 is an important statement that links smooth solutions of the HJB\nequation with solutions of the mean-ﬁeld optimal control problem, and hence\nthe population minimization problem in deep learning. Furthermore, by taking\nthe inﬁmum in (20), it allows us to identify an optimal control policy θ† :\n[0, T ] × P2(Rd+l) →Θ. This is in general a stronger characterization of the\nsolution of the learning problem. In particular, it is of feedback, or closed-loop\nform. On the other hand, an open-loop solution can be obtained from the\nclosed-loop control policy by sequentially setting θ∗\nt = θ†(t, Pw∗\nt ), where w∗\nt is\nthe solution of the feed-forward ODE with θ = θ∗up to time t. Note that in\nusual deep learning, the open-loop type solutions are obtained during training\nand used in inference. In other words, during inference the trained weights are\nﬁxed and are not dependent on the distribution of the inputs encountered. On\nthe other hand, controls obtained from closed-loop control policies are actively\nadjusted according to the distribution encountered. In this sense, the ability\nto generate an optimal control policy in the form of state-based feedback is an\nimportant feature of the dynamic programming approach. However, we should\nnote there is a price to pay for obtaining such a feedback control: the HJB\nequation is general diﬃcult to solve numerically. We shall return to this point\nat the end of Sec. 5.\nThe limitation of Prop. 3 is that it assumes the value function v∗(t, µ) is\ncontinuously diﬀerentiable, which is often not the case. In order to formulate\na complete characterization, we would also like to deduce the statement in\nthe other direction: a solution to (3) should also solve the PDE (20) in an\nappropriate sense. In the next section, we achieve this by giving a more ﬂexible\ncharacterization of the value function as the viscosity solution of the HJB\nequation.\n5 Viscosity solution of HJB equation\n5.1 The concept of viscosity solutions\nIn general, one cannot expect to have smooth solutions to the HJB equa-\ntion (20). Therefore we need to extend the classical concept of PDE solutions\nA Mean-Field Optimal Control Formulation of Deep Learning\n15\nto a type of weak solutions. As in the analysis of classical Hamilton-Jacobi\nequations, we shall introduce a notion of viscosity solution for the HJB equa-\ntion in the Wasserstein space of probability measures. The key idea is again the\nlifting identiﬁcation between measures and random variables, working in the\nHilbert space L2(Ω, Rd+l), instead of the Wasserstein space P2(Rd+l). Then,\nwe can use the tools developed for viscosity solutions in Hilbert spaces. The\ntechniques presented below have been employed in the study of well-posedness\nfor general Hamilton-Jacobi equations in Banach spaces, see e.g. [37,38,39].\nFor convenience, we deﬁne the Hamiltonian H(ξ, P) : L2(Ω, Rd+l)×L2(Ω, Rd+l) →\nR as\nH(ξ, P) := inf\nθ∈Θ E[P · ¯f(ξ, θ) + ¯L(ξ, θ)].\n(23)\nThen the “lifted” Bellman equation of (20) with V (t, ξ) = v(t, Pξ) can be\nwritten down as follows, except that the state space is enlarged to L2(Ω, Rd+l):\n\n\n\n∂V\n∂t + H(ξ, DV (t, ξ)) = 0,\non [0, T ) × L2(Ω, Rd+l),\nV (T, ξ) = E[ ¯Φ(ξ)],\non L2(Ω, Rd+l).\n(24)\nDeﬁnition 1 We say that a bounded, uniformly continuous function u :\n[0, T ] × P2(Rd+l) →R is a viscosity (sub, super) solution to (20) if the lifted\nfunction U : [0, T ] × L2(Ω, Rd+l) →R deﬁned by\nU(t, ξ) = u(t, Pξ)\nis a viscosity (sub, super) solution to the lifted Bellman equation (24), that is:\n(i) U(T, ξ) ≤E[ ¯Φ(ξ)], and for any test function ψ ∈C1,1([0, T ] × L2(Ω, Rd+l))\nsuch that the map U−ψ has a local maximum at (t0, ξ0) ∈[0, T )×L2(Ω, Rd+l),\none has\n∂tψ(t0, ξ0) + H(ξ0, Dψ(t0, ξ0)) ≥0.\n(25)\n(ii) U(T, ξ) ≥E[ ¯Φ(ξ)], and for any test function ψ ∈C1,1([0, T ]×L2(Ω, Rd+l))\nsuch that the map U −ψ has a local minimum at (t0, ξ0) ∈[0, T )×L2(Ω, Rd+l),\none has\n∂tψ(t0, ξ0) + H(ξ0, Dψ(t0, ξ0)) ≤0.\n(26)\n5.2 Existence and uniqueness of viscosity solution\nThe main goal of introducing the concept of viscosity solutions is that in the\nviscosity sense, the HJB equation is well-posed and the value function is the\nunique solution of the HJB equation. We show this in Thm. 1 and 2.\nTheorem 1 The value function v∗(t, µ) deﬁned in (8) is a viscosity solution\nto the HJB equation (20).\nBefore proving Thm. 1, we ﬁrst introduce a useful Lemma regarding the\ncontinuity of H(ξ, P).\n16\nWeinan E, Jiequn Han, Qianxiao Li\nLemma 1 The Hamiltonian H(ξ, P) deﬁned in (23) satisﬁes the following\ncontinuity conditions:\n|H(ξ, P) −H(ξ, Q)| ≤KL∥P −Q∥L2,\n(27)\n|H(ξ, P) −H(ζ, P)| ≤KL(1 + ∥P∥L2)∥ξ −ζ∥L2.\n(28)\nProof. For simplicity we deﬁne\nˆH(ξ, P; θ) := E[P · ¯f(ξ, θ) + ¯L(ξ, θ)].\nThe boundedness of ¯f and ¯L gives us\n| ˆH(ξ, P; θ) −ˆH(ξ, Q; θ)| ≤KL∥P −Q∥L2\n(29)\n| ˆH(ξ, P; θ) −ˆH(ζ, P; θ)| ≤KL(1 + ∥P∥L2)∥ξ −ζ∥L2.\n(30)\nBy deﬁnition we know\nH(ξ, P) := inf\nθ∈Θ\nˆH(ξ, P; θ).\nLet θn satisfy\nˆH(ξ, P; θn) −H(ξ, Q) ≤1/n.\nThen\nH(ξ, P) −H(ξ, Q)\n= (H(ξ, P) −ˆH(ξ, P; θn)) + ( ˆH(ξ, P; θn) −ˆH(ξ, Q; θn)) + ( ˆH(ξ, Q; θn) −H(ξ, Q))\n≤| ˆH(ξ, P; θn) −ˆH(ξ, Q; θn)| + 1/n\n≤KL∥P −Q∥L2 + 1/n.\nTaking n →∞, we have H(ξ, P) −H(ξ, Q) ≤KL∥P −Q∥L2. A similar com-\nputation shows H(ξ, Q) −H(ξ, P) ≤KL∥P −Q∥L2, and we prove (27). (28)\ncan be proved in a similar way, based on the condition (30).\nProof of Thm. 1. We lift the value function v∗(t, µ) to [0, T ] × L2(Ω, Rd+l)\nand denote it by V ∗(t, ξ). Note that the convergence ξn →ξ in L2(Ω, Rd+l)\nimplies the convergence Pξn →Pξ in P2(Rd+l), thus Prop. 1 guarantees that\nV ∗(t, ξ) is continuous on [0, T ] × L2(Ω, Rd+l). By deﬁnition we know V ∗(t, ξ)\nis bounded and V ∗(T, ξ) = E( ¯Φ(ξ)). It remains to show the viscosity sub and\nsuper solution properties of V ∗(t, ξ). To proceed, we note that V ∗(t, ξ) also\ninherits the dynamic programming principle from v∗(t, µ) (c.f. Prop. 2), which\ncan be represented as\nV ∗(t, ξ) =\ninf\nθ∈L∞([0,T ],Θ)\nh Z ˆt\nt\nE[¯L(W t,ξ,θ\ns\n, θs)] ds + V ∗(ˆt, W t,ξ,θ\nˆt\n)\ni\n.\n(31)\n1. Subsolution property. Suppose ψ is a test function in C1,1([0, T ]×L2(Ω, Rd+l))\nand V ∗−ψ has a local maximum at (t0, ξ0) ∈[0, T )×L2(Ω, Rd+l), which means\n(V ∗−ψ)(t, ξ) ≤(V ∗−ψ)(t0, ξ0) for all (t, ξ) satisfying |t−t0|+∥ξ−ξ0∥L2 < δ.\nA Mean-Field Optimal Control Formulation of Deep Learning\n17\nLet θ0 be an arbitrary element in Θ and deﬁne a control process θ0 ∈L∞([0, T ], Θ)\nsuch that θ0\ns ≡θ0, s ∈[t0, T ]. Let h ∈(0, T −t0) be small enough such that\n|s −t0| + ∥W t0,ξ0,θ0\ns\n−ξ0∥L2 < δ for all s ∈[t0, t0 + h]. This is possible from\nan argument similar in the proof of Prop. 1. From the dynamic programming\nprinciple (31), we have\nV ∗(t0, ξ0) ≤\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θ0\ns\n, θ0\ns)] ds + V ∗(t0 + h, W t0,ξ0,θ0\nt0+h\n).\nUsing the condition of local maximality and chain rule (19), we have the\ninequality\n0 ≤V ∗(t0 + h, W t0,ξ0,θ0\nt0+h\n) −V ∗(t0, ξ0) +\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θ0\ns\n, θ0\ns)] ds\n≤ψ(t0 + h, W t0,ξ0,θ0\nt0+h\n) −ψ(t0, ξ0) +\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θ0\ns\n, θ0\ns)] ds\n=\nZ t0+h\nt0\n∂tψ(s, W t0,ξ0,θ0\ns\n) + E[Dψ(s, W t0,ξ0,θ0\ns\n) · ¯f(W t0,ξ0,θ0\ns\n, θ0\ns)] ds\n+\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θ0\ns\n, θ0\ns)] ds.\n(32)\nSince we know W t0,ξ0,θ0\ns\nis continuous in time, in the sense of L2-metric of\nL2(Ω, Rd+l), hence\n∂tψ(s, W t0,ξ0,θ0\ns\n) + E[Dψ(s, W t0,ξ0,θ0\ns\n) · ¯f(W t0,ξ0,θ0\ns\n, θ0\ns) + ¯L(W t0,ξ0,θ0\ns\n, θ0\ns)]\nis also continuous in time. Dividing the inequality (32) by h and taking the\nlimit h →0, we obtain\n0 ≤\nh\n∂tψ(s, W t0,ξ0,θ0\ns\n) + E[Dψ(s, W t0,ξ0,θ0\ns\n) · ¯f(W t0,ξ0,θ0\ns\n, θ0\ns) + ¯L(W t0,ξ0,θ0\ns\n, θ0\ns)]\ni\f\f\f\ns=t0\n= ∂tψ(t0, ξ0) + E[Dψ(t0, ξ0) · ¯f(ξ0, θ0) + ¯L(ξ0, θ0)].\nSince θ0 is arbitrary in Θ, we obtain the desired subsolution property (25).\n2. Supersolution property. Suppose ψ is a test function in C1,1([0, T ] ×\nL2(Ω, Rd+l)) and V ∗−ψ has a local minimum at (t0, ξ0) ∈[0, T )×L2(Ω, Rd+l),\nwhich means\n(V ∗−ψ)(t, ξ) ≥(V ∗−ψ)(t0, ξ0) for all (t, ξ) satisfying |t−t0|+∥ξ−ξ0∥L2 < δ1.\nGiven an arbitrary ε > 0, since Lemma 1 tells us H is continuous, there exits\nδ2 > 0 such that\n|∂tψ(t, ξ) + H(t, ξ) −∂tψ(t0, ξ0) −H(t0, ξ0)| < ε,\nfor all (t, ξ) satisfying |t −t0| + ∥ξ −ξ0∥L2 < δ2. Again as argued in the proof\nof Prop. 1, we can choose h ∈(0, T −t0) to be small enough such that |s −\nt0| + ∥W t0,ξ0,θ\ns\n−ξ0∥L2 < min{δ1, δ2} for all s ∈[t0, t0 + h], θ ∈L∞([0, T ], Θ).\n18\nWeinan E, Jiequn Han, Qianxiao Li\nFrom the dynamic programming principle (31), there exists θh such that\nV ∗(t0, ξ0) + εh ≥\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θh\ns\n, θh\ns )] ds + V ∗(t0 + h, W t0,ξ0,θh\nt0+h\n).\nAgain using the condition of local minimality, chain rule (19), and deﬁnition\nof H, we have the inequality\nεh ≥V ∗(t0 + h, W t0,ξ0,θh\nt0+h\n) −V ∗(t0, ξ0) +\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θh\ns\n, θh\ns )] ds\n≥ψ(t0 + h, W t0,ξ0,θh\nt0+h\n) −ψ(t0, ξ0) +\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θh\ns\n, θh\ns )] ds\n=\nZ t0+h\nt0\n∂tψ(s, W t0,ξ0,θh\ns\n) + E[Dψ(s, W t0,ξ0,θh\ns\n) · ¯f(W t0,ξ0,θh\ns\n, θh\ns )] ds\n+\nZ t0+h\nt0\nE[¯L(W t0,ξ0,θh\ns\n, θh\ns )] ds\n≥\nZ t0+h\nt0\n∂tψ(s, W t0,ξ0,θh\ns\n) + H(W t0,ξ0,θh\ns\n, Dψ(s, W t0,ξ0,θh\ns\n)) ds\n≥h(∂tψ(t0, ξ0) + H(t0, ξ0) −ε).\n(33)\nDividing the inequality (33) by h and taking the limit ε →0, we obtain the\ndesired supersolution property (26).\nTheorem 1 incidentally establishes the existence of viscosity solutions to\nthe HJB, which we can identify as the value function of the mean-ﬁeld control\nproblem. We show below that this solution is in fact unique.\nTheorem 2 Let u1 and u2 be two functions deﬁned on [0, T ]×P2(Rd+l) such\nthat u1 and u2 are viscosity subsolution and supersolution to (20) respectively.\nThen u1 ≤u2. Consequently, the value function v∗(t, µ) deﬁned in (8) is the\nunique viscosity solution to the HJB equation (20).\nProof. The ﬁnal assertion of the theorem follows immediately from Thm. 1. As\nbefore we consider the lifted version U1(t, ξ) = u1(t, Pξ), U2(t, ξ) = u2(t, Pξ)\non [0, T ] × L2(Ω, Rd+l). By deﬁnition we know U1 and U2 are subsolution and\nsupersolution to (24) respectively. By deﬁnition of viscosity solution, U1, U2 are\nboth bounded and uniformly continuous. We denote their moduli of continuity\nby ω1, ω2, which satisfy\n|Ui(t, ξ) −Ui(s, ζ)| ≤ωi(|t −s| + ∥ξ −ζ∥L2),\ni = 1, 2\nfor all 0 ≤t ≤s ≤T, ξ, ζ ∈L2(Ω, Rd+l), and ωi(r) →0 as r →0+. To prove\nU1 ≤U2, we assume\nδ :=\nsup\n[0,T ]×L2(Ω,Rd+l)\nU1(t, ξ) −U2(t, ξ) > 0,\n(34)\nA Mean-Field Optimal Control Formulation of Deep Learning\n19\nand proceed in ﬁve steps below to derive a contradiction.\n1). Let σ, ε ∈(0, 1) and construct the auxiliary function\nG(t, s, ξ, ζ) = U1(t, ξ)−U2(s, ζ)+σ(t+s)−ε(∥ξ∥2\n2+∥ζ∥2\n2)−1\nε2 ((t−s)2+∥ξ−ζ∥2\nL2),\n(35)\nfor t, s ∈[0, T ], ξ, ζ ∈L2(Ω, Rd+l). From Stegall Theorem [40] there exist\nηt, ηs ∈R, ηξ, ηζ ∈L2(Ω, Rd+l) such that |ηt|, |ηs|, ∥ηξ∥L2, ∥ηζ∥L2 ≤ε and the\nfunction with linear perturbation\n˜G(t, s, ξ, ζ) := G(t, s, ξ, ζ) −ηtt −ηss −E[ηξ · ξ] −E[ηζ · ζ]\n(36)\nhas a maximum over [0, T ]×[0, T ]×L2(Ω, Rd+l)×L2(Ω, Rd+l) at (t0, s0, ξ0, ζ0).\n2). Since ˜G(0, 0, 0, 0) ≤˜G(t0, s0, ξ0, ζ0) and U1, U2 are bounded, after an\narrangement of terms, we have\nε(∥ξ0∥2\nL2 + ∥ζ0∥2\nL2)\n≤C + σ(t0 + s0) −1\nε2 ((t0 −s0)2 + ∥ξ0 −ζ0∥2\nL2) −ηtt0 −ηss0\n−E[ηξ · ξ0] −E[ηζ · ζ0]\n≤C −E[ηξ · ξ0] −E[ηζ · ζ0]\n≤C +\n√\n2 ε(∥ξ0∥2\nL2 + ∥ζ0∥2\nL2)1/2.\n(37)\nHere and in the following C denotes generic positive constant, whose value\nmay change from line to line but is always independent of ε and σ. Solving\nthe quadratic inequality above, we get\n(∥ξ0∥2\nL2 + ∥ζ0∥2\nL2)1/2 ≤C(1 + ε−1/2).\n(38)\nNow arguing in the same way as (37) and further combining (37), we have\n1\nε2 ((t0 −s0)2 + ∥ξ0 −ζ0∥2\nL2) ≤C −E[ηξ · ξ0] −E[ηζ · ζ0]\n≤C +\n√\n2 ε(∥ξ0∥2\nL2 + ∥ζ0∥2\nL2)1/2\n≤C,\nor equivalently\n|t0 −s0| + ∥ξ0 −ζ0∥L2 ≤Cε.\n(39)\n3). Eq. (39) allows us to further sharpen the estimate of (t−s)2+∥ξ−ζ∥2\nL2.\nSpeciﬁcally, since ˜G(t0, t0, ξ0, ξ0) ≤˜G(t0, s0, ξ0, ζ0), we have\nE[ηs · (s0 −t0)] + E[ηζ · (ζ0 −ξ0)]\n≤U2(t0, ξ0) −U2(s0, ζ0) + σ(s0 −t0) + ε(∥ξ0∥2\nL2 −∥ζ0∥2\nL2)\n−1\nε2 ((t0 −s0)2 + ∥ξ0 −ζ0∥2\nL2).\n20\nWeinan E, Jiequn Han, Qianxiao Li\nRearranging the above inequality and using estimates (38), (39), and uni-\nform continuity of U2, we obtain\n1\nε2 ((t0 −s0)2 + ∥ξ0 −ζ0∥2\nL2)\n≤ω2(|t0 −s0| + ∥ξ0 −ζ0∥L2) + C(|t0 −s0| + ∥ξ0 −ζ0∥L2) + ε∥ξ0 + ζ0∥L2∥ξ0 −ζ0∥L2\n≤ω2(|t0 −s0| + ∥ξ0 −ζ0∥L2) + C(|t0 −s0| + ∥ξ0 −ζ0∥L2)\n≤ω2(Cε) + Cε.\nBy the property of modulus, we conclude\n|t0 −s0| + ∥ξ0 −ζ0∥L2 = o(ε).\n(40)\n4). From the deﬁnition of ˜G and δ, we can choose ε so small that\nsup\n[0,T ]×L2(Ω,Rd+l)\n˜G(t, t, ξ, ξ) ≥δ\n2.\nUsing estimate (38), (40), we can furthermore choose σ, ε small enough such\nthat\nU1(t0, ξ0) −U2(s0, ζ0)\n≥˜G(t0, s0, ξ0, ζ0) −Cσ −Cε\n≥\nsup\n[0,T ]×L2(Ω,Rd+l)\n˜G(t, t, ξ, ξ) −δ\n4\n≥δ\n4.\nNoting the terminal condition U1(T, ξ) ≤U2(T, ξ), we are ready to estimate\n|T −t0| through\nδ\n4 ≤U1(t0, ξ0) −U2(s0, ζ0)\n≤U1(t0, ξ0) −U1(T, ξ0) + U1(T, ξ0) −U2(T, ξ0)\n+ U2(T, ξ0) −U2(t0, ξ0) + U2(t0, ξ0) −U2(s0, ζ0)\n≤ω1(|T −t0|) + ω2(|T −t0|) + ω2(|t0 −s0| + ∥ξ0 −ζ0∥L2)\n= ω1(|T −t0|) + ω2(|T −t0|) + ω2(o(ε)).\nTherefore, when ε is small enough, we have\nω1(|T −t0|) + ω2(|T −t0|) ≥δ\n8,\nwhich implies\n|T −t0| ≥λ > 0,\nfor some positive constant λ, provided σ, ε are small enough. The same argu-\nment as above can also give |T −s0| ≥λ > 0.\nA Mean-Field Optimal Control Formulation of Deep Learning\n21\n5). The ﬁnite diﬀerences between t0, s0 and T ﬁnally allow us to employ the\nviscosity property. Consider the map (t, ξ) 7→˜G(t, s0, ξ, ζ0) has a maximum at\n(t0, ξ0), i.e. U1 −ψ has a maximum at (t0, ξ0) for\nψ(t, ξ) := U2(s0, ζ0) −σ(t + s0) + ε(∥ξ∥2\nL2 + ∥ζ0∥2\nL2) + 1\nε2 ((t −s0)2 + ∥ξ −ζ0∥2\nL2)\n+ ηtt + ηss0 + E[ηξ · ξ] + E[ηζ · ζ0].\nSince U1 is a viscosity subsolution, using the subsolution property (25), we\nhave\n−σ + 2(t −s0)\nε2\n+ ηt + H(ξ0, 2εξ0 + 2(ξ0 −ζ0)\nε2\n+ ηξ) ≥0.\n(41)\nIn the same way, consider the map (s, ζ) 7→−˜G(t0, s, ξ0, ζ) has a minimum at\n(s0, ζ0), i.e. U2 −ψ has a minimum at (s0, ζ0) for\nψ(t, ξ) := U1(s0, ζ0) + σ(t0 + s) −ε(∥ξ0∥2\nL2 + ∥ζ∥2\nL2) −1\nε2 ((t0 −s)2 + ∥ξ0 −ζ∥2\nL2)\n−ηtt0 −ηss −E[ηξ · ξ0] −E[ηζ · ζ].\nSince U2 is a viscosity supersolution, using the supersolution property (26),\nwe have\nσ + 2(t0 −s)\nε2\n−ηs + H(ζ0, −2εζ0 + 2(ξ0 −ζ0)\nε2\n−ηζ) ≥0.\n(42)\nComputing the diﬀerence in the two inequalities (41),(42) gives\n−2σ+ηt+ηs+H(ξ0, 2εξ0+2(ξ0 −ζ0)\nε2\n+ηξ)−H(ζ0, −2εζ0+2(ξ0 −ζ0)\nε2\n−ηζ) ≥0.\nUsing estimates (38), (40) and Lemma 1, we have\n2σ ≤ηt + ηs + H(ζ0, −2εζ0 + 2(ξ0 −ζ0)\nε2\n−ηζ) −H(ξ0, 2εξ0 + 2(ξ0 −ζ0)\nε2\n+ ηξ)\n≤2ε + |H(ζ0, −2εζ0 + 2(ξ0 −ζ0)\nε2\n−ηζ) −H(ζ0, 2εξ0 + 2(ξ0 −ζ0)\nε2\n+ ηξ)|\n+ |H(ζ0, 2εξ0 + 2(ξ0 −ζ0)\nε2\n+ ηξ) −H(ξ0, 2εξ0 + 2(ξ0 −ζ0)\nε2\n+ ηξ)|\n≤2ε + KL∥2εξ0 + 2εζ0 + ηξ + ηζ∥L2\n+ KL(1 + ∥2εξ0 + 2(ξ0 −ζ0)\nε2\n+ ηξ∥L2)∥ξ0 −ζ0∥L2\n≤o(1)\n(ε →0+).\nTherefore taking the limit gives us a contradiction 0 < σ ≤0, which completes\nthe proof.\n22\nWeinan E, Jiequn Han, Qianxiao Li\nThm. 1 and 2 establishes the well-posedness, in the viscosity sense, of the\nHJB equation and identiﬁes the value function for the mean-ﬁeld optimal con-\ntrol problem as the unique solution of the HJB equation. Moreover, it provides\nus (through solving the inﬁmum in (20) after solving for the value function)\nan optimal control policy, from which we can synthesize an optimal control as\nthe solution of our learning problem. In this sense, the HJB equation gives us\na necessary and suﬃcient condition for optimality of the learning problem (3).\nThis demonstrate an essential observation from the mean-ﬁeld optimal con-\ntrol viewpoint of deep learning: the population risk minimization problem of\ndeep learning can be viewed as a variational problem, whose solution can be\ncharacterized by a suitably deﬁned Hamilton-Jacobi-Bellman equation. This\nvery much parallels classical calculus of variations.\nIt is worth noting that the HJB equation is a global characterization of the\nvalue function, in the sense that it must in principle be solved over the entire\nspace P2(Rd+l) of input-target distributions. Of course, we would not expect\nthis to be the case in practice for any non-trivial machine learning problem.\nHowever, if we can solve it locally around some trajectories generated by the\ninitial condition µ0 ∈P2(Rd+l), then we would expect the obtained feedback\ncontrol policy to apply to nearby input-label distributions as well. This may\nbe able to give a principled way to perform transfer or one-shot learning [41,\n42,43].\nFinally, observe that if the Hamiltonian deﬁned in (23) is attained by a\nunique minimizer θ∗∈Θ given any ξ ∈L2(Ω, Rd+l) and P ∈L2(Ω, Rd+l),\nthen the uniqueness of value function immediately implies the uniqueness of\nthe open-loop optimal control, which is sometimes a desired property of the\npopulation risk minimization problem. The following example gives such an\ninstance.\nExample 1 Consider a speciﬁc type of residual networks, where f(x, θ) =\nθσ(x) and L(x, θ) ∝∥θ∥2. Here θ ∈Rd×d is a matrix and σ is a smooth\nand bounded non-linearity, e.g., tanh or sigmoid. This is similar to conventional\nresidual neural networks except that the order of the aﬃne transformation and\nthe non-linearity are swapped. In this case, the Hamiltonian deﬁned in (23)\nadmits a unique minimizer θ∗given any ξ ∈L2(Ω, Rd+l) and P ∈L2(Ω, Rd+l).\n6 Mean-ﬁeld Pontryagin’s maximum principle\nAs discussed in the earlier sections, the HJB equation provides us with a\ncomplete characterization of the optimality conditions for the population risk\nminimization problem (3). However, it has the disadvantage that it is global in\nP(Rd+l) (or its lifted version, in L2(Ω, Rd+l), and hence diﬃcult to handle in\npractice. The natural question is whether we can have a local characterization\nof optimality, and by local we mean having no need for the optimality condition\nto depend on the whole space of input-label distributions. In this section,\nwe provide such a characterization by proving a mean-ﬁeld version of the\nA Mean-Field Optimal Control Formulation of Deep Learning\n23\ncelebrated Pontryagin’s maximum principle (PMP) [44]. Although seemingly\ndisparate at ﬁrst, we will discuss in Subsec. 6.1 that the maximum principle\napproach is intimately connected with the dynamic programming approach\nintroduced earlier.\nIn classical optimal control, such a local characterization is given in the\nform of the Pontryagin’s maximum principle, where forward and backward\nHamiltonian dynamics are coupled through a maximization condition. In the\npresent formulation, a common control parameter is shared by all input-target\npair values (x0, y0) that can take under the distribution µ0. Thus, one expects\nthat a maximum principle should exist in the average sense. Let us state and\nprove such a maximum principle below. We modify the assumptions (A1),\n(A2) to\n(A1′) The function f is bounded; f, L are continuous in θ; and f, L, Φ are con-\ntinuously diﬀerentiable with respect to x.\n(A2′) The distribution µ0 has bounded support in Rd×Rl, i.e. there exists M > 0\nsuch that µ({(x, y) ∈Rd × Rl : ∥x∥+ ∥y∥≤M}) = 1.\nTheorem 3 (Mean-ﬁeld PMP) Let (A1′), (A2′) be satisﬁed and θ∗∈\nL∞([0, T ], Θ) be a solution of (3) in the sense that J(θ∗) attains the inﬁmum.\nThen, there exists absolutely continuous stochastic processes x∗, p∗such that\n˙x∗\nt = f(x∗\nt , θ∗\nt ),\nx∗\nt = x0,\n(43)\n˙p∗\nt = −∇xH(x∗\nt , p∗\nt , θ∗\nt ),\np∗\nT = −∇xΦ(x∗\nT , y0),\n(44)\nEµ0H(x∗\nt , p∗\nt , θ∗\nt ) ≥Eµ0H(x∗\nt , p∗\nt , θ),\n∀θ ∈Θ,\na.e. t ∈[0, T ],\n(45)\nwhere the Hamiltonian function H : Rd × Rd × Θ →R is given by\nH(x, p, θ) = p · f(x, θ) −L(x, θ).\n(46)\nProof. To simplify the proof we ﬁrst make a substitution by introducing a new\ncoordinate x0 satisfying the dynamics ˙x0\nt = L(xt, θt) with x0\n0 = 0. Then, it is\nclear that the PMP above can be transformed into one without running loss\nby redeﬁning\nx →(x0, x),\nf →(L, f),\nΦ(xT , y0) →Φ(xT , y0) + x0\nT .\nCheck that (A1′), (A2′) are preserved but now we can consider without loss\nof generality the case L ≡0.\nLet some τ ∈(0, T ] be a Lebesgue point of ˆf(t) := f(x∗\nt , θ∗\nt ). By assump-\ntions (A1′) and (A2′) these points are dense in [0, T ]. Now, for ǫ ∈(0, τ), deﬁne\nthe family of perturbed controls\nθτ,ǫ\nt\n=\n(\nω\nt ∈[τ −ǫ, τ],\nθ∗\nt\notherwise.\nwhere ω ∈Θ. This is a “needle” perturbation. Accordingly, deﬁne xτ,ǫ\nt\nby\nxτ,ǫ\nt\n= x0 +\nZ t\n0\nf(xτ,ǫ\ns , θτ,ǫ\ns )ds.\n24\nWeinan E, Jiequn Han, Qianxiao Li\ni.e. solution of the forward propagation equation with the perturbed control\nθτ,ǫ. It is clear that x∗\nt = xτ,ǫ\nt\nfor every t < τ −ǫ and every x0, since the\nperturbation is not present. At t = τ, we have\n1\nǫ (xτ,ǫ\nτ\n−x∗\nτ) = 1\nǫ\nZ τ\nτ−ǫ\nf(xτ,ǫ\ns , ω) −f(x∗\ns, θ∗\ns)ds.\nSince τ is Lebesgue point of F, we have\nvτ := lim\nǫ↓0\n1\nǫ (xτ,ǫ\nτ\n−x∗\nτ) = f(x∗\nτ, ω) −f(x∗\nτ, θ∗\nτ).\nHere, vτ represents the leading order perturbation on the state due to the\n“needle” perturbation introduced in the inﬁnitesimal interval [τ −ǫ, τ]. For the\nrest of the time interval (τ, T ], the dynamics remain the same since the controls\nare the same. It remains to compute how the perturbation vτ propagates.\nDeﬁne for t ≥τ, vǫ\nt := 1\nǫ(xτ,ǫ\nt\n−x∗\nt ) and vt := limǫ↓0 vǫ\nt. By Theorem 2.3.1\nof [45], we know that vt is well deﬁned for almost every t (all the Lebesgue\npoints of the map t 7→x∗(t)) and satisﬁes the following linearized equation:\n˙vt = ∇xf(x∗\nt , θ∗\nt )T vt,\nt ∈(τ, T ],\nvτ = f(x∗\nτ, ω) −f(x∗\nτ, θ∗\nτ).\n(47)\nIn particular, v(T ) represents the perturbation of the ﬁnal state introduced by\nthis control. By the optimality assumption of θ∗, we must have\nEµ0Φ(xτ,ǫ\nT , y0) ≥Eµ0Φ(x∗\nT , y0).\nAssumption (A1′) and (A2′) implies ∇xΦ is bounded so by dominated conver-\ngence theorem,\n0 ≤lim\nǫ↓0\n1\nǫ Eµ0[Φ(xτ,ǫ\nT , y0) −Φ(x∗\nT , y0)]\n= Eµ0\nd\ndǫΦ(xǫ,τ\nT , y0)\n\f\f\f\nǫ=0+\n= Eµ0∇xΦ(x∗\nT , y0) · vT .\n(48)\nNow, let us deﬁne p∗to be the solution of the adjoint of Eq (47),\n˙p∗\nt = −∇xf(x∗\ns, θ∗\ns)p∗\nt ,\np∗\nT = −∇xΦ(x∗\nT , y0).\nThen, (48) implies Eµ0p∗\nT · vT ≤0. Moreover, we have\nd\ndt(p∗\nt · vt) = ˙p∗\nt · vt + ˙vt · p∗\nt = 0\nfor all t ∈[τ, T ]. Thus, we must have Eµ0p∗\nt · vt = Eµ0p∗\nT · vT ≤0 for all\nt ∈[τ, T ] and so for t = τ (with initial condition in (47)),\nEµ0p∗\nτ · f(x∗\nτ, θ∗\nτ) ≥Eµ0p∗\nτ · f(x∗\nτ, ω).\nSince ω ∈Θ is arbitrary, this completes the proof by recalling that H(x, p, θ) =\np · f(x, θ).\nA Mean-Field Optimal Control Formulation of Deep Learning\n25\nRemark 1 In fact, one can show, under slightly stronger conditions (bounded\nﬁrst partial derivatives) that Eµ0H(x∗\nt , p∗\nt, θ∗\nt ) is constant in time, using stan-\ndard techniques (see e.g., Sec. 4.2.9 of [46]).\nLet us now discuss the mean-ﬁeld PMP. First, notice that it is a necessary\ncondition, and hence is much weaker than the HJB characterization. Also, the\nPMP refers only to the open-loop control process θ with no explicit reference\nto an optimal control policy. Now, since the PMP is a necessary condition,\nwe should discuss its relationship with classical necessary conditions in opti-\nmization. Equation (43) is simply the feed-forward ODE (2) under the optimal\nparameters θ∗. On the other hand, Eq. (44) deﬁnes the evolution of the co-\nstate p∗\ns. To draw analogy with constrained optimization, the co-state can be\nregarded as Lagrange multipliers which enforce the ODE constraint (2). How-\never, as in the proof of Thm. 3, it may be more general to interpret it as the\nevolution of an adjoint variational condition backwards in time. The Hamilto-\nnian maximization condition (45) is a unique feature of PMP-type statements,\nin that it does not characterize optimality in terms of vanishing of ﬁrst order\npartial derivatives, as is the case in usual ﬁrst order optimality conditions.\nInstead, optimal solutions must globally maximize the Hamiltonian function.\nThis feature allows greater applicability since we can also deal with the case\nwhere the dynamics are not diﬀerentiable with respect to the controls/training\nweights, or when the optimal controls/training weights lie on the boundary of\nthe set Θ. Moreover, the usual ﬁrst order optimality conditions and the cel-\nebrated back-propagation algorithm can be readily derived from the PMP,\nsee [5]. We note that compared to classical statements of the PMP [17], the\nmain diﬀerence in our result is the presence of the expectation over µ0 in the\nHamiltonian maximization condition (45). This is to be expected since the\nmean-ﬁeld optimal control must depend on the distribution of input-target\npairs.\nWe conclude the discussion by noting that the PMP above can be written\nmore compactly as follows. For each control process θ ∈L∞([0, T ], Θ), denote\nby xθ := {xθ\nt : 0 ≤t ≤T } and pθ := {pθ\nt : 0 ≤t ≤T } the solution of\nthe Hamilton’s equations (43) and (44) using this control with the random\nvariables (x0, y0) ∼µ0, i.e.\n˙xθ\nt = f(xθ\nt , θt),\nxθ\n0 = x0,\n˙pθ\nt = −∇xH(xθ\nt , pθ\nt , θt),\npθ\nT = −∇xΦ(xθ\nT , y0).\n(49)\nThen, θ∗satisﬁes the PMP if and only if\nEµ0H(xθ∗\nt , pθ∗\nt , θ∗\nt ) ≥Eµ0H(xθ∗\nt , pθ∗\nt , θ),\n∀θ ∈Θ.\n(50)\nFurthermore, observe that the mean-ﬁeld PMP derived above includes, as a\nspecial case, the necessary conditions for optimality for the sampled optimal\ncontrol problem (4). To see this, simply deﬁne the empirical measure µN\n0 :=\n1\nN\nPN\ni=1 δ(xi\n0,yi\n0) and apply the mean-ﬁeld PMP (Thm. 3) with µN\n0 in place of\n26\nWeinan E, Jiequn Han, Qianxiao Li\nµ0 to give\n1\nN\nN\nX\ni=1\nH(xθ∗,i\nt\n, pθ∗,i\nt\n, θ∗\nt ) ≥1\nN\nN\nX\ni=1\nH(xθ∗,i\nt\n, pθ∗,i\nt\n, θ),\n∀θ ∈Θ,\n(51)\nwhere each xθ,i and pθ,i are deﬁned as in (49), but with the input-target pair\n(xi\n0, yi\n0). Of course, since µN\n0 is a random measure, this is a random equation\nwhose solution are random variables.\n6.1 Connection between the HJB equation and the PMP\nWe now discuss some concrete connections between the HJB equation and the\nPMP, thus justifying our claim that the PMP can be understood as a local\nresult compared to the global characterization of the HJB equation.\nIt should be noted that the Hamiltonian deﬁned in Pontryagin’s maximum\nprinciple (46) is diﬀerent from (23) in the HJB equation, due to diﬀerent sign\nconventions in these two approaches of classical optimal control. We choose\nto keep this diﬀerence such that readers familiar with classical control theory\ncan draw an analogy easily. Nevertheless, if one replaces p, L, f in (46) by\n−P, −¯L, ¯f respectively and takes the inﬁmum over Θ instead of the maximum\ncondition in (45), one formally obtains the negative of (23).\nNow, our goal is to show that the HJB and PMP are more intimately con-\nnected than it appears in the deﬁnition of Hamiltonian. The deeper connections\noriginate from the link between Hamilton’s canonical equations (ODEs) and\nHamilton-Jacobi equations (PDEs), of which we give an informal description\nas follows.\nFirst, note that although the Hamiltonian dynamics (43) and (44) de-\nscribe the trajectory of particular random variables (completely determined\nby (x0, y0)), the optimality conditions are not dependent on the particular\nrepresentation of the probability measures by these random variables. In other\nwords, we could also formulate a maximum principle whose Hamiltonian ﬂow\nis that on measures in a Wasserstein space, from which the above PMP can\nbe seen as a “lifting”. This approach would parallel the developments in the\nprevious sections on the HJB equations. However, here we choose to establish\nand analyze the PMP in the lifted space due to the simplicity of having well-\ndeﬁned evolution equations. The corresponding evolution of measures would\nrequire more technical analysis while not being particularly more elucidating.\nInstead, we shall establish the connections by also lifting the HJB equation\ninto L2(Ω, Rd+l).\nConsider the lifted HJB equation (24) in L2(Ω, Rd+l). The key observation\nis that we can apply the method of characteristics (see e.g., Ch. 3.2 of [47]) by\ndeﬁning Pt = DV (t, ξt) and write down the characteristic evolution equations:\n( ˙ξt = DP H(ξt, Pt),\n˙Pt = −DξH(ξt, Pt).\n(52)\nA Mean-Field Optimal Control Formulation of Deep Learning\n27\nSuppose this system has a solution satisfying boundary conditions Pξ0 =\nµ0, PT = ∇w ¯Φ(ξT ), where the second condition comes from the terminal condi-\ntion of (24). To avoid technicalities, we further assume that the inﬁmum in (23)\nis attained at θ†(ξ, P), which is always an interior point of Θ. Hence (23) can\nbe explicitly written down as\nH = E[P · ¯f(ξ, θ†(ξ, P)) + ¯L(ξ, θ†(ξ, P))],\nand by ﬁrst order condition we have\nE\n\u0002\n∇θ ¯f(ξ, θ†(ξ, P))P + ∇θ ¯L(ξ, θ†(ξ, P))\n\u0003\n= 0.\nPlugging the above two equalities into (52) gives us\n( ˙ξt = ¯f(ξt, θ†(ξt, Pt)),\n˙Pt = −∇w ¯f(ξt, θ†(ξt, Pt))Pt −∇w ¯L(ξt, θ†(ξt, Pt)).\nLet θ∗\nt = θ†(ξt, Pt). Note that w = (x, y) is the concatenated variable and\nthe last l components of ¯f are zero. If we only consider the ﬁrst d components,\nthen we can deduce the d−dimensional dynamical system in L2(Ω, Rd):\n(\n˙xt = f(ξt, θ∗\nt ),\n˙pt = −∇xf(xt, θ∗\nt )pt −∇xL(xt, θ∗\nt ).\n(53)\nIf we make the transformation p →−p in Thm. 3, it is straightforward to see\nthat the deduced dynamical system by Thm. 3 satisﬁes (53) in L2(Ω, Rd) and\nthe boundary conditions are matched.\nIn summary, the Hamilton’s equations (53) in the PMP can be viewed\nas the characteristic equations for the HJB equation (24). Consequently, the\nPMP pinpoints the necessary condition a characteristic of the HJB equation\noriginating from (a random variable with law) µ0 must satisfy. This justiﬁes\nthe preceding claim that the PMP constitutes a local optimality condition as\ncompared to the HJB equation.\n7 Small-time uniqueness\nAs discussed, the PMP constitute necessary conditions for optimality. A nat-\nural question is when are the PMP solutions also suﬃcient for optimality\n(See [45], Ch. 8 for some discussions on suﬃciency). One simple case where\nit is suﬃcient, assuming an optimal solution exists, is when the PMP equa-\ntions admit a unique solution. In this section, we investigate the uniqueness\nproperties of the PMP system.\nNote that even if there exists a unique solution θ†(ν) of the Hamiltonian\nmaximization arg maxθ E(x,p)∼νH(x, p, θ) for any P(x,p), the equation (49) re-\nduces to a highly non-linear two-point boundary value problem for x∗, p∗,\nfurther coupled with their laws. Even without the coupling to laws, such two-\npoint boundary value problems are known to not have unique solutions in\n28\nWeinan E, Jiequn Han, Qianxiao Li\ngeneral (see e.g., Ch. 7 of [48]). In the following, we shall show that if T is\nsuﬃciently small and H is strongly concave, then the PMP admits a unique so-\nlution. Hereafter, we retain assumption (A2′) and replace (A1′) with a stronger\nassumption, which greatly simpliﬁes our arguments:\n(A1′′) f is bounded; f, L, Φ are twice continuously diﬀerentiable with respect to\nboth x, θ, with bounded and Lipschitz partial derivatives.\nWith an estimate of the diﬀerence in ﬂow maps due to two diﬀerent con-\ntrols, we can prove a small-time uniqueness result for the PMP.\nTheorem 4 Suppose that H(x, p, θ) is strongly concave in θ, uniformly in\nx, p ∈Rd, i.e. ∇2\nxxH(x, p, θ) + λ0I ⪯0 for some λ0 > 0. Then, for suﬃciently\nsmall T , if θ1 and θ2 are solutions of the PMP (50) then θ1 = θ2.\nNote that since we are considering the eﬀects of T , in the rest of the\nestimates in this section, the dependence of constants on T are explicitly con-\nsidered. We ﬁrst estimate the diﬀerence of ﬂow-maps driven by two diﬀerent\ncontrols.\nLemma 2 Let θ1, θ2 ∈L∞([0, T ], Θ). Then, there exists a constant T0 such\nthat for all T ∈[0, T0), we have\n∥xθ1 −xθ2∥L∞+ ∥pθ1 −pθ2∥L∞≤C(T )∥θ1 −θ2∥L∞.\nwhere C(T ) > 0 satisﬁes C(T ) →0 as T →0.\nProof. Denote δθ := θ1 −θ2, δx := xθ1 −xθ2 and δp := pθ1 −pθ2. Since\nxθ1\n0 = xθ2\n0 = x0, integrating the respective ODEs and using (A1′′) we have\n∥δxt∥≤\nZ t\n0\n∥f(xθ1\ns , θ1\ns) −f(xθ1\nt , θ2\ns)∥ds ≤KL\nZ T\n0\n∥δxs∥ds + KL\nZ T\n0\n∥δps∥ds,\nand so\n∥δx∥L∞≤KLT ∥δx∥∞+ KLT ∥δθ∥∞.\nNow, if T < T0 := 1/KL, we then have\n∥δx∥L∞≤\nKLT\n1 −KLT ∥δθ∥L∞.\n(54)\nSimilarly,\n∥δpt∥≤KL∥δxT ∥+ KL\nZ T\nt\n∥δxs∥ds + KL\nZ T\nt\n∥δps∥ds,\n∥δp∥L∞≤(KL + KLT )∥δx∥L∞+ KLT ∥δp∥L∞,\nand hence\n∥δp∥L∞≤KL(1 + T )\n1 −KLT ∥δx∥L∞.\n(55)\nCombining (54) and (55) proves the claim.\nA Mean-Field Optimal Control Formulation of Deep Learning\n29\nWith the above estimate, we can now prove Thm. 4.\nProof of Thm. 4. By uniform strong concavity, the function θ 7→Eµ0H(xθ1\nt , xθ1\nt , θ)\nis strongly concave. Thus, we have a λ0 > 0 such that\nλ0\n2 ∥θ1\nt −θ2\nt ∥2 ≤\nh\nEµ0∇H(xθ1\nt , pθ1\nt , θ2\nt ) −Eµ0∇H(xθ1\nt , pθ1\nt , θ1\nt )\ni\n· (θ1\nt −θ2\nt ).\nA similar expression holds for θ 7→Eµ0H(xθ2\nt , xθ2\nt , θ) and so combining them\nand using assumptions (A1′′) we have\nλ0∥θ1\nt −θ2\nt ∥2 ≤\nh\nEµ0∇H(xθ1\nt , pθ1\nt , θ2\nt ) −Eµ0∇H(xθ1\nt , pθ1\nt , θ1\nt )\ni\n· (θ1\nt −θ2\nt )\n+\nh\nEµ0∇H(xθ2\nt , pθ2\nt , θ1\nt ) −Eµ0∇H(xθ2\nt , pθ2\nt , θ2\nt )\ni\n· (θ1\nt −θ2\nt )\n≤Eµ0∥∇H(xθ1\nt , pθ1\nt , θ1\nt ) −∇H(xθ2\nt , pθ2\nt , θ1\nt )∥∥θ1\nt −θ2\nt ∥\n+ Eµ0∥∇H(xθ1\nt , pθ1\nt , θ2\nt ) −∇H(xθ2\nt , pθ2\nt , θ2\nt )∥∥θ1\nt −θ2\nt ∥\n≤KL∥δθ∥L∞(∥δx∥L∞+ ∥δp∥L∞).\nCombining the above and Lemma 2, we have\n∥δθ∥2\nL∞≤KL\nλ0\nC(T )∥δθ∥2\nL∞.\nBut C(T ) = o(1) and so we may take T suﬃciently small so that KLC(T ) < λ0\nto conclude that ∥δθ∥L∞= 0.\nIn the context of machine learning, since f is bounded, small T roughly\ncorresponds to the regime where the reachable set of the forward dynamics\nis small. This can be loosely interpreted as the case where the model has low\ncapacity or expressive power. We note that the number of parameters is still\ninﬁnite, since we only require θ to be essentially bounded and measurable in\ntime. Hence, Thm. 4 can be interpreted as the statement that when the model\ncapacity is low, the optimal solution is unique, albeit with possibly high loss\nfunction values. Note that the strong concavity of the Hamiltonian does not\nimply that the loss function J is strongly convex, or even convex, which is often\nan unrealistic assumption in deep learning. In fact, in the case considered in\nExample 1, we observe that H is strongly concave but the loss function J can\nbe highly non-convex due to the non-linear transformation σ. Compared with\nthe characterization using HJB (Sec. 5), we observe that the uniqueness of the\nsolutions of the PMP requires the small T condition.\n8 From mean-ﬁeld PMP to sampled PMP\nSo far, we have focused our discussion on the mean-ﬁeld control problem (3)\nand mean-ﬁeld PMP (50). However, the solution of the mean-ﬁeld PMP re-\nquires maximizing an expectation. Hence, in practice we must resort to solving\n30\nWeinan E, Jiequn Han, Qianxiao Li\na sampled version (51), which constitutes necessary conditions for the sampled\noptimal control problem (4).\nThe goal of this section is to draw some precise connections between the\nsolutions of the mean-ﬁeld PMP (50) and the sampled PMP (51). In par-\nticular, we show that under appropriate conditions, near any stable (to be\nprecisely deﬁned later) solution of the mean-ﬁeld PMP (50) we can ﬁnd with\nhigh probability a solution of the sampled PMP (51). This allows us to es-\ntablish a concrete link, via the maximum principle, between solutions of the\npopulation risk minimization problem (3) and the empirical risk minimization\nproblem (4). To proceed, the key observation is that the interior solutions\nto both the mean-ﬁeld and sampled PMPs can be written as the solutions\nto algebraic equations on Banach spaces. Indeed, in view of the compact no-\ntation (50), let us suppose that θ∗is a solution of the PMP such that the\nmaximization step attains a maximum in the interior of Θ for a.e. t ∈[0, T ].\nNote that if Θ is suﬃciently large, e.g., Θ = Rm, then this must be the case.\nWe shall hereafter assume this holds. Consequently, the PMP solution satisﬁes\n(by dominated convergence theorem)\nF (θ∗)t := Eµ0∇θH(xθ∗\nt , pθ∗\nt , θ∗\nt ) = 0,\n(56)\nfor a.e. t, where F : L∞([0, T ], Θ) →L∞([0, T ], Rm) is a Banach space map-\nping. Similarly, from (51) we know that an interior solution θN of the ﬁnite-\nsample PMP is a random variable which satisﬁes\nFN(θN)t := 1\nN\nN\nX\ni=1\n∇θH(xθN,i\nt\n, pθN,i\nt\n, θN\nt ) = 0,\n(57)\nfor a.e. t. Now, FN is a random approximation of F and EFN(θ) = F (θ)\nfor all θ. In fact, FN →F almost surely by law of large numbers. Hence, the\nanalysis of the approximation properties of the mean-ﬁeld PMP by its sampled\ncounterpart amounts to the study of the approximation of zeros of F by those\nof FN.\nIn view of this, we shall take a brief excursion to develop some theory\non random approximations of zeros of Banach space mappings at an abstract\nlevel, and then use these results to deduce properties of the PMP approxima-\ntions. The techniques employed in the next section are reminiscent of classical\nnumerical analysis results on ﬁnite diﬀerence approximation schemes [49], ex-\ncept that we work with random approximations.\n8.1 Excursion: random approximations of zeros of Banach space mappings\nLet (U, ∥· ∥U), (V, ∥· ∥V ) be Banach spaces and F : U →V be a mapping.\nWe ﬁrst deﬁne a notion of stability, which shall be a primary condition that\nensures existence of close-by zeros of approximations.\nA Mean-Field Optimal Control Formulation of Deep Learning\n31\nDeﬁnition 2 For ρ > 0 and x ∈U, deﬁne Sρ(x) := {y ∈U : ∥x −y∥U ≤ρ}.\nWe say that the mapping F is stable on Sρ(x) if there exists a constant Kρ > 0\nsuch that for all y, z ∈Sρ(x),\n∥y −z∥U ≤Kρ∥F(y) −F(z)∥V .\nNote that if F is stable on Sρ(x), then it is trivially true that it has at\nmost one solution to F = 0 on Sρ(x). If it does have a solution, say at x∗, then\nit is necessarily isolated, i.e., if DF(x∗) exists, then it is non-singular. The\nfollowing proposition establishes a stronger version of this: if DF(x) exists for\nany x ∈Sρ(x∗), then it is necessarily non-singular.\nProposition 4 Let F on Sρ(x∗) be stable. Then, for any x ∈Sρ(x∗), if DF(x)\nexists, then it is non-singular, i.e. DF(x)y = 0 implies y = 0.\nProof. Suppose for the sake of contradiction that DF(x)y = 0 and ∥y∥U ̸= 0.\nDeﬁne z(α) := x + αy with α suﬃciently small so that z(α) ∈Sρ(x∗). Then,\nα∥y∥U =∥x −z(α)∥U\n≤Kρ∥F(x) −F(z(α))∥V\n≤Kρ(α∥DF(x)y∥V + ∥F(x + αy) −F(x) −DF(x)αy∥V ).\nBut DF(x)y = 0, and so α∥y∥U ≤Kρr(x, αy)α∥y∥U, By deﬁnition of the\nFréchet derivative (5), r(x, αy) →0 as α →0. Thus if α is suﬃciently small so\nthat Kρr(x, αy) < 1, then ∥y∥U = 0 and hence we arrive at a contradiction.\nAs the previous proposition suggests, a converse statement that establishes\nstability will require DF(x) to be non-singular on some neighborhood of x∗.\nOne in fact requires more, i.e. that DF needs to be Lipschitz. Note that for\na linear operator A : U →V , we also use ∥A∥V to denote the usual induced\nnorm, ∥A∥V = sup∥y∥U≤1 ∥Ay∥V .\nProposition 5 Suppose DF(x∗) is non-singular, DF(x) exists and ∥DF(x)−\nDF(y)∥V ≤KL∥x −y∥U for all x, y ∈Sρ(x∗). Then, F is stable on Sρ0(x∗)\nfor any 0 < ρ0 ≤min(ρ, 1\n2(KL∥DF(x∗)−1∥U)−1) with stability constant\nKρ0 = 2∥DF(x∗)−1∥U.\nProof. Let ρ0 ≤ρ and take x, y ∈Sρ0(x∗). Using the mean value theorem, we\ncan write F(x) −F(y) = R(x, y)(x −y) where\nR(x, y) :=\nZ 1\n0\nDF(sx + (1 −s)y)ds.\nBut, using the Lipschitz condition we have\n∥R(x, y) −DF(x∗)∥V ≤\nZ 1\n0\n∥DF(sx + (1 −s)y) −DF(sx∗+ (1 −s)x∗)∥V ds\n≤KL\nZ 1\n0\n∥s(x −x∗) + (1 −s)(y −x∗)∥Uds\n≤ρ0KL.\n32\nWeinan E, Jiequn Han, Qianxiao Li\nWe take ρ0 suﬃciently small so that ρ0KL ≤1\n2∥DF(x∗)−1∥−1\nU . Then, by the\nBanach lemma, R(x, y) is non-singular and ∥R(x, y)−1∥U ≤2∥DF(x∗)−1∥U.\nThe result follows since (x −y) = R(x, y)−1(F(x) −F(y)).\nNow, let us now introduce a family of random mappings FN that approxi-\nmate F. Let (Ω, F, P) be a probability space and {FN(ω) : N ≥1, ω ∈Ω} be a\nfamily of mappings from U to V such that ω 7→FN(ω)(x) is F-measurable for\neach x (we equip the Banach spaces U, V with the Borel σ-algebra). We make\nthe following assumptions which will allow us to relate the random solutions\nof FN = 0 with those of F = 0 in Thm. 5.\n(B1) (Stability) There exists x∗∈U such that F(x∗) = 0 and F is stable on\nSρ(x∗) for some ρ > 0.\n(B2) (Uniform convergence in probability) For all N ≥1, DF(x) and DFN(x)\nexists for all x ∈Sρ(x∗), P-a.s. and\nP [∥F(x) −FN(x)∥V ≥s] ≤r1(N, s),\nP [∥DF(x) −DFN(x)∥V ≥s] ≤r2(N, s),\nfor some real-valued functions r1, r2 such that r1(N, s), r2(N, s) →0 as\nN →∞.\n(B3) (Uniformly Lipschitz derivative) There exists KL > 0 such that for all\nx, y ∈Sρ(x∗),\n∥DFN(x) −DFN(y)∥V ≤KL∥x −y∥U,\nP-a.s.\nTheorem 5 Let (B1)-(B3) hold. Then, there exist positive constants s0, ρ1, C\nwith ρ1 < ρ and U-valued random variables xN ∈Sρ1(x∗) satisfying\nP[∥xN −x∗∥U ≥Cs] ≤r1(N, s) + r2(N, s),\ns ∈(0, s0],\nP[FN(xN) ̸= 0] ≤r1(N, s0) + r2(N, s0).\nIn particular, xN →x∗and FN(xN) →0 in probability.\nTo establish Thm. 5, we ﬁrst prove that for large N, with high probability\nDFN(x∗) is non-singular and ∥DFN(x∗)−1∥U is uniformly bounded.\nLemma 3 Let (B1)-(B3) hold. Then, there exists a constant s0 > 0 such that\nfor each s ∈(0, s0] and N ≥1, there exists a measurable AN(s) ⊂Ωsuch that\nP[AN(s)] ≥1 −r1(N, s) −r2(N, s) and for each ω ∈AN(s),\n∥F(x∗) −FN(ω)(x∗)∥V < s.\nMoreover, DFN(ω)(x∗) is non-singular with\n∥DFN(ω)(x∗)−1∥U ≤2∥DF(x∗)−1∥U.\nIn particular, DFN(ω) is stable on Sρ0(x∗) with ρ0 ≤min(ρ, 1\n4(KL∥DF(x∗)−1∥U)\n−1)\nand stability constant Kρ0 = 4∥DF(x∗)−1∥U.\nA Mean-Field Optimal Control Formulation of Deep Learning\n33\nProof. For s > 0 set\nAN(s) := {ω ∈Ω: ∥F(x∗) −FN(ω)(x∗)∥V ) < s\nand ∥DF(x∗) −DFN(ω)(x∗)∥V < s}.\nObserve that AN(s) is measurable as DFN(ω)(x∗) is measurable and assump-\ntion (B2) implies P[AN(s)] ≥1 −r1(N, s) −r2(N, s). Now, take s suﬃciently\nsmall so that s ≤s0 =\n1\n2∥DF(x∗)−1∥−1\nU . Then, for each ω ∈AN(s), the\nBanach lemma implies DFN(ω)(x∗) is non-singular and\n∥DFN(ω)(x∗)−1∥U ≤∥DF(x∗)−1∥U\n1 −1\n2\n= 2∥DF(x∗)−1∥U.\nFinally, we use Proposition 5 to deduce stability of FN(ω).\nNow we are ready to prove Thm. 5 by constructing a uniform contraction\nmapping whose ﬁxed point is a solution of FN(x) = 0.\nProof of Thm. 5. Let s0, AN(s) and ρ0 be those deﬁned in Lemma 3. For each\nω ∈AN(s) with s ≤s0, deﬁne the mapping\nGN(ω)(x) := x −DFN(ω)(x∗)−1FN(ω)(x).\nWe now show that this is in fact a uniform contraction on Sρ1(x∗) for suﬃ-\nciently small ρ1. Let x, y ∈Sρ1(x∗). By the mean value theorem, we have\nGN(ω)(x) −GN(ω)(y)\n= DFN(ω)(x∗)−1[DFN(ω)(x∗)(x −y) −(FN(ω)(x) −FN(ω)(y))]\n= DFN(ω)(x∗)−1[DFN(ω)(x∗) −RN(ω)(x, y)](x −y),\nwhere RN(ω)(x, y) =\nR 1\n0 DFN(ω)(sx + (1 −s)y)ds. Lipschitz condition (B3)\nimplies\n∥DFN(ω)(x∗) −RN(ω)(x, y)∥V ≤ρ1KL\nand hence by Lemma 3,\n∥GN(ω)(x) −GN(ω)(y)∥U ≤α∥x −y∥U,\nwhere α = 2KLρ1∥DF(x∗)−1∥U. We now pick ρ1 < ρ0 suﬃciently small so\nthat α < 1. It remains to show that the mapping GN(ω) maps Sρ1(x∗) onto\nitself. Let x ∈Sρ1(x∗), then by noting that F(x∗) = 0,\n∥GN(ω)(x) −x∗∥U ≤∥GN(ω)(x) −GN(ω)(x∗)∥U + ∥GN(ω)(x∗) −x∗∥U\n≤αρ1 + 2∥DF(x∗)−1∥U∥FN(ω)(x∗) −F(x∗)∥V .\nUsing Lemma 3 again, we have\n∥GN(ω)(x) −x∗∥U ≤αρ1 + 2s∥DF(x∗)−1∥U.\n34\nWeinan E, Jiequn Han, Qianxiao Li\nWe now take s0 > s small enough so that 2s0∥DF(x∗)−1∥U < (1−α)ρ1. Then,\nfor all N ≥1, GN(ω) is a contraction, uniform in N, on Sρ1(x∗) and hence\nby Banach ﬁxed point theorem, there exists a unique ˜xN,s(ω) ∈Sρ1(x∗) such\nthat GN(ω)(˜xN,s(ω)) = ˜xN,s(ω), i.e. FN(ω)(˜xN,s(ω)) = 0 for all ω ∈AN(s).\nMoreover, ˜xN,s(ω) = limk→∞[GN(ω)](k)(y) for any y ∈Sρ0(x∗). Deﬁne\nxN,s(ω) = 1AN(s)(ω)˜xN(ω) + 1AN(s)c(ω)x∗.\nNow, xN,s is measurable since AN(s) is measurable and ˜xN,s is the limit\nof measurable random variables, and hence measurable. Moreover, AN(s) ⊂\n{FN(xN) = 0} and so P[FN(xN,s) = 0] ≥1 −r1(N, s) −r2(N, s). Since\nxN,s ∈Sρ1(x∗) and ρ1 < ρ0, using the stability of FN(ω) established in\nLemma 3, and the fact that FN(xN,s) = F(x∗) = 0, we have for any ω ∈AN(s)\n∥xN,s(ω) −x∗∥U ≤Kρ0∥FN(ω)(xN,s) −FN(ω)(x∗)∥V\n≤4∥DF(x∗)−1∥U∥F(x∗) −FN(ω)(x∗)∥V\n< 4s∥DF(x∗)−1∥U,\nand so P[∥xN,s(ω)−x∗∥U ≥Cs] ≤r1(N, s)+r2(N, s) with C = 4∥DF(x∗)−1∥U.\nAt this point, it appears that xN,s depends on s. However, notice that for all\ns ≤s0, AN(s) ⊂AN(s0). But, xN,s(ω) is the unique solution of FN(ω)(·) = 0\nin Sρ1(x∗) for each ω ∈AN(s) ⊂AN(s0). Therefore, xN,s(ω) = xN,s0(ω) for\nall s ≤s0. We can thus write xN := xN,s0 ≡xN,s.\nLastly, convergence in probability follows from the decay of the functions\nr1, r2 has N →∞.\n8.2 Error estimate for sampled PMP\nNow, our goal is to apply the theory developed in Sec. 8.1 to the PMP. We shall\nassume that θ∗, the solution of the mean-ﬁeld PMP, is such that F (θ∗) = 0\n(recall that this holds for Θ = Rm). Suppose further that F is stable at θ∗(see\nDef. 2). We wish to show that for suﬃciently large N, with high probability\nFN must have a solution θN close to θ∗.\nIn view of Thm. 5, we only need to check that (B2)-(B3) are satisﬁed.\nThis requires a few elementary estimates and an application of the inﬁnite-\ndimensional Hoeﬀding’s inequality [50].\nLemma 4 There exist constants KB, KL > 0 such that for all θ, φ ∈L∞([0, T ], Θ)\n∥xθ∥L∞+ ∥pθ∥L∞≤KB,\n∥xθ −xφ∥L∞+ ∥pθ −pφ∥L∞≤KL∥θ −φ∥L∞.\nA Mean-Field Optimal Control Formulation of Deep Learning\n35\nProof. We have by Gronwall’s inequality for a.e. t,\n∥xθ\nt −xφ\nt ∥=\n\r\r\r\r\nZ t\n0\nf(xθ\ns, θs) −f(xθ\ns, θs)ds\n\r\r\r\r\n≤KL\nZ t\n0\n∥xθ\ns −xφ\ns ∥ds + KL\nZ t\n0\n∥θs −φs∥ds\n≤KLT eKLT ∥θ −φ∥L∞.\nSimilarly,\n∥pθ\nt −pφ\nt ∥≤∥∇xΦ(xθ\nT , y0) −∇xΦ(xφ\nT , y0)∥\n+\n\r\r\r\r\r\nZ T\nt\n∇xH(xθ\ns , pθ\ns, θs) −∇xH(xφ\ns , pφ\ns , φs)ds\n\r\r\r\r\r\n≤KL∥xθ\nT −xφ\nT ∥+ KL\nZ T\nt\n∥xθ\ns −xφ\ns ∥ds + KL\nZ T\nt\n∥pθ\ns −pφ\ns ∥ds\n≤(KL + T )KLT e2KLT ∥θ −φ∥L∞.\nNotice that we can view xθ ≡x(θ) as a Banach space mapping from\nL∞([0, T ], Θ) to L∞([0, T ], Rd), and similarly for pθ. Below, we establish some\nelementary estimates for the derivatives of these mappings with respect to θ.\nLemma 5 There exist constants KB, KL > 0 such that for all θ, φ ∈L∞([0, T ], Θ)\n∥Dxθ∥L∞+ ∥Dpθ∥L∞≤KB,\n∥Dxθ −Dxφ∥L∞+ ∥Dpθ −Dpφ∥L∞≤KL∥θ −φ∥L∞.\nProof. Let η ∈L∞([0, T ], Rm) such that ∥η∥L∞≤1. For brevity, let us also\ndenote f θ\nt := f(xθ\nt , θt) and Hθ\nt := H(xθ\nt , pθ\nt , θt). Then, (Dxθ)η satisfy the\nlinearized ODE\nd\ndt[(Dxθ)η]t = ∇xf θ\nt [(Dxθ)η]t + ∇θf θ\nt ηt,\n[(Dxθ)η]0 = 0.\nGronwall’s inequality and (A1′′) immediately implies that ∥[(Dxθ)η]t∥≤\nKL∥η∥L∞, and so ∥Dxθ∥L∞≤K′. Next,\n∥[(Dxθ)η]t −[(Dxφ)η]t∥≤\nZ t\n0\n∥∇xf θ\ns ∥∥[(Dxθ)η]s −[(Dxφ)η]s∥ds\n+\nZ t\n0\n∥∇xf θ\ns −∇xf φ\ns ∥∥[(Dxφ)η]s∥ds\n+\nZ t\n0\n∥∇θf θ\ns −∇θf φ\ns ∥∥ηs∥ds.\n36\nWeinan E, Jiequn Han, Qianxiao Li\nBut, using Lemma 4, assumption (A1′′), we have\n∥∇xf θ\ns −∇xf φ\ns ∥≤KL∥xθ\ns −xφ\ns ∥+ KL∥θs −φs∥\n≤KL∥θ −φ∥L∞.\nA similar calculation shows ∥∇xf θ\ns −∇xf φ\ns ∥≤KL∥θ −φ∥L∞. Hence, Gron-\nwall’s inequality gives\n∥[(Dxθ)η]t −[(Dxφ)η]t∥≤KL∥η∥L∞∥θ −φ∥L∞.\nSimilarly, (Dpθ)η satisﬁes the ODE\nd\ndt[(Dpθ)η]t = −∇2\nxxHθ\nt [(Dxθ)η]t −∇2\nxpHθ\nt [(Dpθ)η]t −∇2\nxθHθ\nt ηt,\n[(Dpθ)η]T = −∇2\nxxΦ(xθ\nT , y0)[(Dxθ)η]T .\nA analogous calculation as above with (A1′′) shows that\n∥[(Dpθ)η]t −[(Dpφ)η]t∥≤KL∥η∥L∞∥θ −φ∥L∞.\nLemma 6 Let h : Rd × Rd × Θ →Rm have bounded and Lipschitz deriva-\ntives in all arguments and deﬁne the mapping θ 7→G(θ) where [G(θ)]t =\nh(xθ\nt , pθ\nt , θt). Then, G is diﬀerentiable and DG is bounded and Lipschitz µ0-\na.s., i.e.\n∥DG(θ)∥L∞≤KB,\n∥DG(θ) −DG(φ)∥L∞≤KL∥θ −φ∥L∞.\nfor some KB, KL > 0 and all θ, φ ∈L∞([0, T ], Θ).\nProof. Let η ∈L∞([0, T ], Rm) such that ∥η∥L∞≤1. By assumptions on h\nand Lemmas 4 and 5, DG exists and by the chain rule,\n[(DG(θ))η]t = ∇xhθ\nt [(Dxθ)η]t + ∇phθ\nt [(Dpθ)η]t + ∇θhθ\nt ηt,\nThus, ∥[(DG(θ))η]t∥≤KB∥η∥L∞and\n∥[(DG(θ))η]t −[(DG(φ))η]t∥≤KB∥∇xhθ\nt −∇xhφ\nt ∥\n+ KL∥[(Dxθ)η]t −[(Dxφ)η]t∥\n+ . . .\nThe other terms are split similarly and we omit them for simplicity. Using\nLipschitz assumption of the derivatives of h and Lemmas 4 and 5, we obtain\nthe result.\nApplying Lemma 6 with h = H for each sample i and summing, we see that\nDFN is bounded and Lipschitz µ0-a.s. and so (B3) is satisﬁed. It remains to\ncheck (B2). Using Lemma (6) and (A1′′), ∥FN∥L∞and ∥DFN∥L∞are almost\nsurely bounded, hence they satisfy standard concentration estimates. We have:\nA Mean-Field Optimal Control Formulation of Deep Learning\n37\nLemma 7 There exist constants KB, KL > 0 such that for all θ, φ ∈L∞([0, T ], Θ)\nP[∥F (θ) −FN(θ)∥L∞≥s] ≤2 exp\n\u0012\n−\nNs2\nK1 + K2s\n\u0013\n,\nP[∥DF (θ) −DFN(θ)∥L∞≥s] ≤2 exp\n\u0012\n−\nNs2\nK1 + K2s\n\u0013\n.\nProof. Since ∥F (θ)∥is uniformly bounded by KB, we can apply the inﬁnite-\ndimensional Hoeﬀding’s inequality ([50], Corollary 2) to obtain\nP[∥F (θ) −FN(θ)∥L∞≥s] ≤2 exp\n\u0012\n−\nNs2\n2K2\nB + (2/3)KBs\n\u0013\n.\nand similarly for DFN.\nGiven the above results, we can deduce Thm. 6 directly.\nTheorem 6 Let θ∗be a solution F = 0 (deﬁned in (56)), which is stable on\nSρ(θ∗) for some ρ > 0. Then, there exists positive constants s0, C, K1, K2 and\nρ1 < ρ and a random variable θN ∈Sρ1(θ∗) ⊂L∞([0, T ], Θ), such that\nP[∥θ −θN∥L∞≥Cs] ≤4 exp\n\u0012\n−\nNs2\nK1 + K2s\n\u0013\n,\ns ∈(0, s0],\nP[FN(θN) ̸= 0] ≤4 exp\n\u0012\n−\nNs2\n0\nK1 + K2s0\n\u0013\n.\nIn particular, θN →θ∗and FN(θN) →0 in probability.\nProof. Use Thm. 5 with estimates derived in Lemmas 6 and 7.\nThm. 6 describes the convergence of a solution of the ﬁrst order condition\nof the PMP solution in the sampled situation to the population solution of the\nPMP. Together with a condition of local strong concavity, we show further in\nCor. 1 that this stationary solution is in fact a local/global maximum of the\nsampled PMP. The claim regarding the convergence of loss function values is\nprovided in Cor. 2.\nCorollary 1 Let θ∗be a solution of the mean-ﬁled PMP such that there exists\nλ0 > 0 satisfying that for a.e. t ∈[0, T ], E∇2\nθθH(xθ∗\nt , pθ∗\nt , θ∗\nt ) + λ0I ⪯0.\nThen the random variable θN deﬁned in Thm. 6 satisﬁes, with probability at\nleast 1 −6 exp [−(Nλ2\n0)/(K1 + K2λ0)], that θN\nt\nis a strict local maximum of\nsampled Hamiltonian\n1\nN\nPN\ni=1 H(xθN,i\nt\n, pθN,i\nt\n, θ). In particular, if the ﬁnite-\nsampled Hamiltonian has a unique local maximizer, then θN is a solution of\nthe sampled PMP with the same high probability.\n38\nWeinan E, Jiequn Han, Qianxiao Li\nProof. Let\n[I(θ)]t := Eµ0∇2\nθθH(xθ\nt , pθ\nt , θt),\n[IN(θ)]t := 1\nN\nN\nX\ni=1\n∇2\nθθH(xθ,i\nt , pθ,i\nt , θt).\nGiven the assumption of negative deﬁnite Hessian matrix at θ∗\nt :\n[I(θ∗)]t + λ0I ⪯0,\nwhat we need to prove is\nP[∥IN(θN) −I(θ∗)∥L∞≥2cλ0] ≤o(1),\nN →∞,\nfor suﬃcient small c > 0. Consider the following estimate\nP[∥IN(θN) −I(θ∗)∥L∞≥2cλ0]\n≤P[∥IN(θN) −IN(θ∗)∥L∞≥cλ0 and ∥IN(θ∗) −I(θ∗)∥L∞≥cλ0]\n≤P[∥IN(θN) −IN(θ∗)∥L∞≥cλ0] + P[∥IN(θ∗) −I(θ∗)∥L∞≥cλ0].\nTo bound the ﬁrst term, we can use similar steps as in the proof of Lemma 6,\nwhich gives\ness sup\nt∈[0,T ]\n∥∇2\nθθH(xθ\nt , pθ\nt , θt) −∇2\nθθH(xφ\nt , pφ\nt , φt)∥≤KL∥θ −φ∥L∞.\nHence we have\nP[∥IN(θN) −IN(θ∗)∥L∞≥cλ0] ≤P[∥θN −θ∗∥L∞≥cλ0/KL]\n≤4 exp\n\u0012\n−\nNλ2\n0\nK1 + K2λ0\n\u0013\n.\nTo bound the second term, note that ∥IN(θ)∥is uniformly bounded, we can\napply the inﬁnite-dimensional Hoeﬀding’s inequality ([50], Corollary 2) to ob-\ntain\nP[∥IN(θ∗) −I(θ∗)∥L∞≥cλ0] ≤2 exp\n\u0012\n−\nNλ2\n0\nK′\n1 + K′\n2λ0\n\u0013\n.\nCombining two estimates together, we complete the proof.\nCorollary 2 Let θN be as deﬁned in Thm. 6. Then there exist constants\nK1, K2 such that,\nP[|J(θN) −J(θ∗)| ≥s] ≤4 exp\n\u0012\n−\nNs2\nK1 + K2s\n\u0013\n,\ns ∈(0, s0].\nA Mean-Field Optimal Control Formulation of Deep Learning\n39\nProof. Note that J(θ) = Φ(xθ\nT , y0) + R T\n0 L(xθ\nt , θt)dt. Using Lemma 4, we have\n|J(θN) −J(θ∗)| ≤KL∥xθ∗\nT −xθN\nT ∥+ KL\nZ T\n0\n∥xθ∗\nt\n−xθN\nt\n∥+ ∥θ∗\nt −θN\nt ∥dt\n≤K′\nL∥θN −θ∗∥L∞\nThus, using Thm. 6, we have\nP[|J(θN) −J(θ∗)| ≥s] ≤P[∥θN −θ∗∥L∞≥s/K′\nL]\n≤4 exp\n\u0012\n−\nNs2\nK1 + K2s\n\u0013\n.\nThm. 6 and Cor. 1 establishes a rigorous connection between solutions of\nthe mean-ﬁeld PMP and its sampled version: when a solution of the mean-\nﬁeld PMP θ∗is stable, then for large N, with high probability we can ﬁnd\nin its neighborhood a random variable θN that is a stationary solution of\nthe sampled PMP (51). If further that the maximization is non-degenerate\n(local concavity assumption in Thm. 1) and unique, then θN\nt\nmaximizes the\nsample Hamiltonian with high probability. Note that this concavity condition\nis local in the sense that it only has to be satisﬁed at the paths involving θ∗,\nwhereas the strong concavity condition required in Thm. 4 is stronger as it is\nglobal. Of course, in the case where the Hamiltonian is quadratic in θ, i.e. when\nf(x, θ) is linear in θ and the regularization L(x, θ) is quadratic in θ (this is\nstill a nonlinear network, see Example 1), then all concavity assumptions in\nthe preceding results are satisﬁed.\nThe key assumption for the results in this section is the stability condition\n(c.f. Def. 2). In general, this is diﬀerent from the assumption that H(xθ∗\nt , pθ∗\nt , θ∗\nt )\nis strongly concave point-wise in t. However, note that one can show using tri-\nangle inequality and estimates in Lemma 5 that if H is strongly concave with\nsuﬃciently large concavity parameter (λ0), then the solution must stable. Intu-\nitively, the stability assumption ensures that we can ﬁnd a small region around\nθ∗such that it is isolated from other solutions, and this then allows us to ﬁnd\na nearby solution of the sampled problem that is close to this solution. On\nthe other hand, if DF(θ∗) has a non-trivial kernel, then one cannot expect to\nconstruct a θN that is close to θ∗itself, or any speciﬁc point in the kernel.\nHowever, one may still ﬁnd θN that is close to the whole kernel.\nCor. 2 is a simple consequence of the previous results, and is eﬀectively a\nstatement about generalization error of the learning model, because it quan-\ntiﬁes the diﬀerence between loss function values when evaluated on either the\npopulation or empirical risk minimization solution. We mention an interest-\ning point of the optimal control framework alluded to earlier in the context\nof generalization. Notice that since we have only assumed that the controls\nor weights θ are measurable and essentially bounded (and thus can be very\ndiscontinuous) in time, we are always dealing with the case where the num-\nber of parameters are inﬁnite. Even in this case, we can derive non-trivial\n40\nWeinan E, Jiequn Han, Qianxiao Li\ngeneralization estimates. This is to be contrasted with classical generalization\nbounds based on measures of complexity [51], where the number of parameters\nadversely aﬀect generalization. Note that there are many recent works which\ntake on such issues from varying angles, e.g., [52,53,54].\n9 Conclusion\nIn this paper, we introduce the mathematical formulation of the population\nrisk minimization problem of continuous-time deep learning in the context of\nmean-ﬁeld optimal control. In this framework, the compositional structure of\ndeep neural networks is explicitly taken into account as the evolution of the\ndynamical system in time. To analyze this mean-ﬁeld optimal control prob-\nlem, we proceed from two parallel but interrelated perspectives, namely the\ndynamic programming approach and the maximum principle approach. In the\nformer, an inﬁnite-dimensional Hamilton-Jacobi-Bellman (HJB) equation for\nthe optimal loss function values is derived, with state variables being the joint\ndistribution of input-target pairs. The viscosity solution of the derived HJB\nequation provides us with a complete characterization of the original popula-\ntion risk minimization problem, giving both the optimal loss function value\nand a optimal feedback control policy. In the latter approach, we prove a mean-\nﬁeld Pontryagin’s maximum principle that constitutes necessary conditions for\noptimality. This can be viewed as a local characterization of optimal trajec-\ntories, and indeed we formally show that the PMP can be derived from the\nHJB equation using the method of characteristics. Using the PMP, we study\na suﬃcient condition for which the solution of the PMP is unique. Lastly, we\nprove an existence result of sampled PMP solutions near the stable solutions\nof the mean-ﬁeld PMP. We show how this result connects with generalization\nerrors of deep learning and provide a new direction for obtaining generalization\nestimates in the case of inﬁnite number of parameters and ﬁnite number of\nsample points. Overall, this work establishes a concrete mathematical frame-\nwork from which novel ways to attack the pertinent problems in practical and\ntheoretical deep learning may be further developed.\nAs a speciﬁc motivation for future work, notice that here, we have assumed\nthat the state dynamics f is independent of distribution law of xt and only\ndepends on xt itself and control θt. There are also more complex network\nstructures used in practice which are beyond this assumption. Let us take\nbatch normalization as an example [55]. A batch normalization step involves\nnormalizing inputs using some distribution ν, and then rescale (and re-center)\nthe output using trainable variables so that the matching space is recovered.\nThis has been found empirically to have a good regularization eﬀect for train-\ning, but theoretical analysis of such eﬀects are limited. In the present setting,\nwe can write a batch normalization operation as\nBNγ,β(x, ν) := γ ⊙\nx −\nR\nz dν(z)\nq\n(z −\nR\nz′ dν(z′))2dν(z) + ǫ\n+ β.\nA Mean-Field Optimal Control Formulation of Deep Learning\n41\nHere γ, β ∈Rd are trainable parameters, ⊙denotes element-wise multiplica-\ntion, and ǫ is a small constant avoiding division by zero. Suppose we insert\na Batch Normalization operation immediately after the skip connection, the\ncorresponding state dynamics f becomes\nf(x, θ) →f(BNγ,β(x, ν), θ).\nBy incorporating γ, β into the parameter vector θ and taking ν as the popula-\ntion distribution of the state, the equation of state dynamics has the following\nabstract form\n˙xt = ˜f(xt, θ, Pxt).\n(58)\nThis is a more general formulation typically considered in the mean-ﬁeld op-\ntimal control literature. The associated objective is very similar to (3) except\nthe state dynamics:\ninf\nθ∈L∞([0,T ],Θ) J(θ) := Eµ0\n\"\nΦ(xT , y0) +\nZ T\n0\nL(xt, θt)dt\n#\n,\nSubject to (58).\n(59)\nThe dynamic programming principle and the maximum principle are still ap-\nplicable in this setting. For instance, the associated HJB equation can be\nderived as\n\n\n\n∂v\n∂t + inf\nθ∈Θ\n\n∂µv(t, µ)(.) · ¯f(., θ, µ) + ¯L(., θ), µ\n\u000b\n= 0,\non [0, T ) × P2(Rd+l),\nv(T, µ) = ⟨¯Φ(.), µ⟩,\non P2(Rd+l),\nwhere ¯f(w, θ, µ) := ( ˜f(x, θ, µx), 0). Similarly, we expect the following mean\nﬁeld PMP (in the lifted space) to hold under suitable conditions:\n˙x∗\nt = ˜f(x∗\nt , θ∗\nt , Px∗\nt ),\nx∗\nt = x0,\n˙p∗\nt = −∇xH(x∗\nt , p∗\nt, θ∗\nt , Px∗\nt ),\np∗\nT = −∇xΦ(x∗\nT , y0),\nEµ0H(x∗\nt , p∗\nt, θ∗\nt , Px∗\nt ) ≥Eµ0H(x∗\nt , p∗\nt , θ, Px∗\nt ),\n∀θ ∈Θ,\na.e. t ∈[0, T ],\nwhere the Hamiltonian function H : Rd × Rd × Θ × P2(Rd) →R is given by\nH(x, p, θ, µ) = p · f(x, θ, µ) −L(x, θ).\nThus, batch normalization can be viewed as a general form of mean-ﬁeld dy-\nnamics, and can be treated in a principled way under the mean-ﬁeld optimal\ncontrol framework. We leave the study of further implications of this connec-\ntion on the theoretical understanding of batch normalization to future work.\nAcknowledgments\nThe work of W. E and J. Han are supported in part by ONR grant N00014-\n13-1-0338 and Major Program of NNSFC under grant 91130005. Q. Li is sup-\nported by the Agency for Science, Technology and Research, Singapore.\n42\nWeinan E, Jiequn Han, Qianxiao Li\nReferences\n1. Yoshua Bengio.\nLearning deep architectures for AI.\nFoundations and Trends R\n⃝in\nMachine Learning, 2(1):1–127, 2009.\n2. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.\nDeep learning.\nNature,\n521(7553):436–444, 2015.\n3. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n4. Weinan E. A proposal on machine learning via dynamical systems. Communications\nin Mathematics and Statistics, 5(1):1–11, 2017.\n5. Qianxiao Li, Long Chen, Cheng Tai, and Weinan E. Maximum principle based algo-\nrithms for deep learning. Journal of Machine Learning Research, 18:1–29, 2018.\n6. Qianxiao Li and Shuji Hao. An optimal control approach to deep learning and applica-\ntions to discrete-weight neural networks. arXiv preprint arXiv:1803.01299, 2018.\n7. Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse\nProblems, 34(1):014004, 2017.\n8. Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert.\nMulti-level\nresidual networks from dynamical systems view. In Proceedings of the International\nConference on Learning Representations, 2018.\n9. Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham.\nReversible architectures for arbitrarily deep residual neural networks. arXiv preprint\narXiv:1709.03698, 2017.\n10. Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond ﬁnite layer neural net-\nworks: Bridging deep architectures and numerical diﬀerential equations. arXiv preprint\narXiv:1710.10121, 2017.\n11. Sho Sonoda and Noboru Murata. Double continuum limit of deep neural networks. In\nICML Workshop on Principled Approaches to Deep Learning, 2017.\n12. Zhen Li and Zuoqiang Shi. Deep residual learning and PDEs on manifold. arXiv preprint\narXiv:1708.05115, 2017.\n13. Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordi-\nnary diﬀerential equations. arXiv preprint arXiv:1806.07366, 2018.\n14. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.\n15. Richard Bellman. Dynamic programming. Courier Corporation, 2013.\n16. Michael G Crandall and Pierre-Louis Lions.\nViscosity solutions of hamilton-Jacobi\nequations. Transactions of the American Mathematical Society, 277(1):1–42, 1983.\n17. Lev S Pontryagin. Mathematical theory of optimal processes. CRC Press, 1987.\n18. Arthur Earl Bryson.\nApplied optimal control: optimization, estimation and control.\nCRC Press, 1975.\n19. Michael Athans and Peter L Falb. Optimal control: an introduction to the theory and\nits applications. Courier Corporation, 2013.\n20. Yann LeCun.\nA theoretical framework for back-propagation.\nIn The Connectionist\nModels Summer School, volume 1, pages 21–28, 1988.\n21. Jean-Michel Lasry and Pierre-Louis Lions.\nMean ﬁeld games.\nJapanese Journal of\nMathematics, 2(1):229–260, 2007.\n22. Minyi Huang, Roland P Malhamé, and Peter E Caines. Large population stochastic\ndynamic games: closed-loop Mckean-Vlasov systems and the Nash certainty equivalence\nprinciple. Communications in Information & Systems, 6(3):221–252, 2006.\n23. Olivier Guéant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean ﬁeld games and ap-\nplications. In Paris-Princeton lectures on mathematical ﬁnance 2010, pages 205–266.\nSpringer, 2011.\n24. Alain Bensoussan, Jens Frehse, and Phillip Yam. Mean ﬁeld games and mean ﬁeld type\ncontrol theory, volume 101. Springer, 2013.\n25. Mathieu Lauriere and Olivier Pironneau. Dynamic programming for mean-ﬁeld type\ncontrol. Comptes Rendus Mathematique, 352(9):707–713, 2014.\n26. Huyên Pham and Xiaoli Wei. Dynamic programming for optimal control of stochastic\nMckean–Vlasov dynamics. SIAM Journal on Control and Optimization, 55(2):1069–\n1101, 2017.\nA Mean-Field Optimal Control Formulation of Deep Learning\n43\n27. Huyên Pham and Xiaoli Wei. Bellman equation and viscosity solutions for mean-ﬁeld\nstochastic control problem. ESAIM: Control, Optimisation and Calculus of Variations,\n24(1):437–461, 2018.\n28. Marco Caponigro, Massimo Fornasier, Benedetto Piccoli, and Emmanuel Trélat. Sparse\nstabilization and control of alignment models. Mathematical Models and Methods in\nApplied Sciences, 25(03):521–564, 2015.\n29. Massimo Fornasier and Francesco Solombrino.\nMean-ﬁeld optimal control.\nESAIM:\nControl, Optimisation and Calculus of Variations, 20(4):1123–1152, 2014.\n30. Mattia Bongini, Massimo Fornasier, Francesco Rossi, and Francesco Solombrino. Mean-\nﬁeld pontryagin maximum principle. Journal of Optimization Theory and Applications,\n175(1):1–38, 2017.\n31. Alain-Sol Sznitman. Topics in propagation of chaos. In Ecole d’été de probabilités de\nSaint-Flour XIX—1989, pages 165–251. Springer, 1991.\n32. Daniel Andersson and Boualem Djehiche. A maximum principle for SDEs of mean-ﬁeld\ntype. Applied Mathematics & Optimization, 63(3):341–356, 2011.\n33. Rainer Buckdahn, Boualem Djehiche, and Juan Li.\nA general stochastic maximum\nprinciple for SDEs of mean-ﬁeld type. Applied Mathematics & Optimization, 64(2):197–\n216, 2011.\n34. René Carmona and François Delarue. Forward–backward stochastic diﬀerential equa-\ntions and controlled mckean–vlasov dynamics. The Annals of Probability, 43(5):2647–\n2700, 2015.\n35. Pierre-Louis Lions. Cours au collège de france: Théorie des jeuxa champs moyens, 2012.\n36. Pierre Cardaliaguet. Notes on mean ﬁeld games. Technical report, 2010.\n37. Michael G Crandall and Pierre-Louis Lions. Hamilton-Jacobi equations in inﬁnite di-\nmensions I. Uniqueness of viscosity solutions. Journal of Functional Analysis, 62(3):379\n– 396, 1985.\n38. Michael G Crandall and Pierre-Louis Lions. Hamilton-Jacobi equations in inﬁnite di-\nmensions. II. Existence of viscosity solutions. Journal of Functional Analysis, 65(3):368\n– 405, 1986.\n39. Michael G Crandall and Pierre-Louis Lions. Hamilton-Jacobi equations in inﬁnite di-\nmensions, III. Journal of Functional Analysis, 68(2):214 – 247, 1986.\n40. Charles Stegall. Optimization of functions on certain subsets of banach spaces. Math-\nematische Annalen, 236(2):171–176, 1978.\n41. Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions\non knowledge and data engineering, 22(10):1345–1359, 2010.\n42. Xavier Glorot, Antoine Bordes, and Yoshua Bengio.\nDomain adaptation for large-\nscale sentiment classiﬁcation: A deep learning approach.\nIn Proceedings of the 28th\ninternational conference on machine learning (ICML-11), pages 513–520, 2011.\n43. Fei-Fei Li, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE\ntransactions on pattern analysis and machine intelligence, 28(4):594–611, 2006.\n44. Vladimir G Boltyanskii, Revaz V Gamkrelidze, and Lev S Pontryagin. The theory of\noptimal processes. I. The maximum principle. Technical report, TRW Space Tochnology\nLabs, Los Angeles, California, 1960.\n45. Alberto Bressan and Benedetto Piccoli.\nIntroduction to the mathematical theory of\ncontrol, volume 2. American institute of mathematical sciences Springﬁeld, 2007.\n46. Daniel Liberzon. Calculus of variations and optimal control theory: a concise introduc-\ntion. Princeton University Press, 2012.\n47. Lawrence C. Evans. Partial diﬀerential equations. Graduate studies in mathematics.\nAmerican Mathematical Society, 1998.\n48. Walter G Kelley and Allan C Peterson. The theory of diﬀerential equations: classical\nand qualitative. Springer Science & Business Media, 2010.\n49. HB Keller. Approximation methods for nonlinear problems with application to two-\npoint boundary value problems. Mathematics of Computation, 29(130):464–474, 1975.\n50. IF Pinelis and AI Sakhanenko. Remarks on inequalities for large deviation probabilities.\nTheory of Probability & Its Applications, 30(1):143–148, 1986.\n51. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical\nlearning, volume 1. Springer series in statistics New York, 2001.\n44\nWeinan E, Jiequn Han, Qianxiao Li\n52. Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.\nEx-\nploring generalization in deep learning. In Advances in Neural Information Processing\nSystems, pages 5949–5958, 2017.\n53. Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization\nbounds for deep (stochastic) neural networks with many more parameters than training\ndata. arXiv preprint arXiv:1703.11008, 2017.\n54. Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization\nbounds for deep nets via a compression approach. arXiv preprint arXiv:1802.05296,\n2018.\n55. Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift. In International conference on machine\nlearning, pages 448–456, 2015.\n",
  "categories": [
    "math.OC",
    "cs.LG"
  ],
  "published": "2018-07-03",
  "updated": "2018-07-03"
}