{
  "id": "http://arxiv.org/abs/1807.01855v1",
  "title": "Zipf's law in 50 languages: its structural pattern, linguistic interpretation, and cognitive motivation",
  "authors": [
    "Shuiyuan Yu",
    "Chunshan Xu",
    "Haitao Liu"
  ],
  "abstract": "Zipf's law has been found in many human-related fields, including language,\nwhere the frequency of a word is persistently found as a power law function of\nits frequency rank, known as Zipf's law. However, there is much dispute whether\nit is a universal law or a statistical artifact, and little is known about what\nmechanisms may have shaped it. To answer these questions, this study conducted\na large scale cross language investigation into Zipf's law. The statistical\nresults show that Zipf's laws in 50 languages all share a 3-segment structural\npattern, with each segment demonstrating distinctive linguistic properties and\nthe lower segment invariably bending downwards to deviate from theoretical\nexpectation. This finding indicates that this deviation is a fundamental and\nuniversal feature of word frequency distributions in natural languages, not the\nstatistical error of low frequency words. A computer simulation based on the\ndual-process theory yields Zipf's law with the same structural pattern,\nsuggesting that Zipf's law of natural languages are motivated by common\ncognitive mechanisms. These results show that Zipf's law in languages is\nmotivated by cognitive mechanisms like dual-processing that govern human verbal\nbehaviors.",
  "text": " 1 / 18 \n \n \nZipf‚Äôs law in 50 languages:* \nits structural pattern, linguistic interpretation, and cognitive motivation \n \nShuiyuan YU1, Chunshan Xu2, 3, Haitao LIU3, 4 \n \n1 School of Computer, Communication University of China, Beijing,100024, China \n2 School of Foreign Studies, Anhui Jianzhu University, Hefei, 230601, China. \n3 Department of Linguistics, Zhejiang University, Hangzhou, 310058, China \n4 Ningbo Institute of Technology, Zhejiang University, Ningbo, 315100, China \n \nAbstract  Zipf‚Äôs law has been found in many human-related fields, including \nlanguage, where the frequency of a word is persistently found as a power law function \nof its frequency rank, known as Zipf‚Äôs law. However, there is much dispute whether it \nis a universal law or a statistical artifact, and little is known about what mechanisms \nmay have shaped it. To answer these questions, this study conducted a large scale \ncross language investigation into Zipf‚Äôs law. The statistical results show that Zipf‚Äôs \nlaws in 50 languages all share a 3-segment structural pattern, with each segment \ndemonstrating distinctive linguistic properties and the lower segment invariably \nbending downwards to deviate from theoretical expectation. This finding indicates \nthat this deviation is a fundamental and universal feature of word frequency \ndistributions in natural languages, not the statistical error of low frequency words.  A \ncomputer simulation based on the dual-process theory yields Zipf‚Äôs law with the same \nstructural pattern, suggesting that Zipf‚Äôs law of natural languages are motivated by \ncommon cognitive mechanisms. These results show that Zipf‚Äôs law in languages is \nmotivated by cognitive mechanisms like dual-processing that govern human verbal \nbehaviors.  \n \nKeywords  Zipf‚Äôs law, language universals, 3-segment structural pattern, dual-\nprocessing mechanism \n \n                     \n* Correspondence to:  Haitao Liu,  lhtzju@yeah.net \n 2 / 18 \n \n \nIntroduction \nAbout 80 years ago, George Kingsley Zipf reported an observation that the frequency \nof a word seems to be a power law function of its frequency rank, formulated as \nf(r) ‚àùùëüùõº, where f is word frequency, r is the rank of frequency, and ùõº is the \nexponent(1, 2). This linguistic regularity was later termed as Zipf‚Äôs law. In subsequent \nyears, similar power law functions have been widely found in various domains, \ndrawing attention of scientists with different academic backgrounds‚Äîit may reflect a \nuniversal law underlying various natural, social and cognitive phenomena (3-5). To \nspecify the mechanisms motivating such power-law functions, much work has been \ndone from different perspectives (6-19). However, a universally acknowledged \nconclusion is yet to obtain, which reflects a failure to fully understand the role and the \nsignificance of these power functions in natural, social and cognitive spheres.   \nIn an effort to explore the motivations of Zipf‚Äôs laws, we conducted a case study on \nnatural languages. Though this law was first observed in language, its significance in \nverbal communication and deep-level motivations are far from being clear. We know \nneither why this law has been persistently found in samples of different languages, \nand with different sizes, nor what underlying mechanisms may have shaped it. As a \nresult, it is often proposed that this law has no linguistic significance and cognitive \nmotivations: simple and meaningless statistical processes (16), such as random typing \nmodel (6, 8, 20), can result in power-law distributions similar to Zipf‚Äôs law as well. \nHowever, a law usually describes the fundamental nature of something. The failure to \npin down deep level motivations actually suggests that Zipf‚Äôs law is not a law: it \nsheds no light on the fundamental nature of verbal communication and the cognitive \nmechanisms underlying it. To trace the possible cognitive mechanism that shapes this \nlaw, it is necessary to find a universally valid linguistic interpretation of Zipf‚Äôs law, or \nrather its universal significance in verbal communication, which means that Zipf‚Äôs \ncurve (word frequency as a function of rank on a log-log scale) should exhibit, across \ndifferent languages, similar structural patterns that have similar linguistic significance. \nHence, corpus-based investigations are needed that cover languages as diverse as \npossible. Nevertheless, previous studies were generally limited to a rather limited \nrange of languages and paid little attention to its underlying cognitive mechanism and \nsignificance in language communication.  \n 3 / 18 \n \nBased on texts from 50 languages, this quantitative study reveals that Zipf‚Äôs law \nfound in these languages all exhibit a 3-segment structural pattern, with the lower \nsegment invariably bending downward to deviate from the theoretical expectation, \nwhich has linguistic significance and can be construed as motivated by dual-process \nmodel‚Äîa cognitive mechanism common among human beings. We further \ndemonstrate that computer simulation based on this cognitive mechanism can \ngenerate Zipf‚Äôs curves that are structurally similar to those found in human languages. \nThese findings suggest that Zipf‚Äôs laws found in human-related fields are likely to \nhave deep-level motivations, especially the cognitive ones, and that big-data analysis \nbased on corpus may provide valuable means to uncover language laws and to trace \ncognitive laws hidden behind. \n \nMaterials and methods  \nData \n Language materials used in our study are taken from BNC and Leipzig Corpus. In \nthese corpora, we choose 50 languages. The sample of each language contains over \n300,000 sentences, ranging between 80,000 and 43,000,000 word tokens. These \nlanguages belong to such typological branches as Indo-European, Uralic, Altaic, \nCaucasian, Sino-Tibet, Dravidian, Afro-asiatic, etc. Of these 50 languages, 25 include \ntexts of both news and wiki.  \nTo avoid the stochastic influence of text genres on overall word frequency, we \nconduct text-based randomization upon the materials from BNC. The Leipzig Corpus \nhas already been randomized on the basis of sentences.   \nPiecewise fitting of frequency-rank curves of 50 languages to different power-law \nfunctions \nThe rank-frequency curves found in these 50 languages seem all to exhibit a 3-\nsegment structural pattern. We therefore fit these 3 segments respectively to the \nfollowing power law functions: y = a ‚àôxùëè, y = a ‚àôxùëè, and y = a ‚àô(x + c)ùëè. In other \nwords, for the fitting function of the lower segment that covers the words with low \nfrequency, the definition domain is not solely confined to the frequency rank. In \naddition, to prevent the influence of too many words with extremely low frequency, \nthe fitting of the lower segment is based on logarithmically equal size spaces, not \n 4 / 18 \n \nlinearly equal size space, and offset C is added to the fitting function to indicate the \nsteepness and the direction of the bending. \nThe influence of text genres on the 3 segments of the curves  \n25 languages are chosen in Leipzig corpus to find out the influence of the genres on \nthe exponents of fitting functions of these curves. For each language, the sample \ncovers two genres, including both wiki texts and news texts. For each genre, there are \nmore than 300, 000 sentences. The exponents derived from texts of wiki are paired \nwith the exponents derived from texts of news to conduct Wilcoxon rank sum test. If \nthe P-value of test is smaller than alpha, the null hypothesis is rejected that parameters \nof fitting function are not influenced by genres.  \nRelations between linguistic typologies and function exponents of each curve \nsegment. To study the effect of typological differences, we chose, among the 50 \nlanguages, 37 languages that fall into 7 typological branches that each contains at \nleast 3 of these 37 languages. In other words, these 37 languages are unevenly divided \ninto 7 groups to conduct a F-test, which can reveal whether typological differences \naffect exponents of fitting functions. \nThe growth rate of word frequency in upper segment, middle segment, and \nlower segment. To investigate how word frequency grows with increasing sample \nsize, the following formula is used to calculate the average growth rate of word \nfrequency: \n1\nùëÅ‚àí1 ‚àë\nùëì(ùëõ+1)/ùëì(ùëõ)\nùëê(ùëõ+1)/ùëê(ùëõ)\nùëÅ‚àí1\nùëõ=1\n, in which f(n) is the frequency of a word in language \nsample whose size is n word tokens. For each languageÔºåùëÜ1Ôπ¶ùëÜ2Ôπ¶ùëÜ3Ôπ¶‚Ä¶ ‚Ä¶ ùëÜùëõ are \nsamples with different sizes; N(S) is the number of word tokens in a sample; the \nrelations among these samples are ùëÜ1 ‚äÇùëÜ2 ‚äÇùëÜ3 ‚äÇ, ‚Ä¶ ‚Ä¶ ‚äÇùëÜùëõ and 2 C(ùëÜ1) =\nC(ùëÜ2), 2C(ùëÜ2) = C(ùëÜ3), ‚Ä¶ ‚Ä¶ 2C(ùëÜùëõ‚àí1) = ùê∂(ùëÜùëõ). \nWe also investigate the relation between the number of word types and the number of \nword tokens (sample size) through linearly fitting the logarithm of number of word \ntypes to the logarithm of the number of word tokens of each sample in a language. \nThe monomial coefficient of the fitting function is the Heaps‚Äô exponent. To examine \nthe influence of genre and typology on Heaps‚Äô exponent, a F-tests are conducted on \nthe news texts and wiki texts of 25 languages, and on the samples of 37 languages that \nfall into 7 typological branches. \n 5 / 18 \n \nComputer simulations of dual-generating mechanism model .This model includes \ntwo mechanisms to simulate the growth of word tokens, one for the 3000 high-\nfrequency words that are covered in the upper and the middle segments of the curve, \nand the other for low-frequency words and new words that are covered in the lower \nsegment of the curve. For high-frequency words, the increase of sample size will \ncause their total frequency to escalate according to a fixed probability as found in \nexistent samples. In other words, total probability of high frequency words (their \nrelative frequency) is not much influenced by the variation in sample size. The \nremainder probability (1 minus the total probability of high frequency words) is \nassigned to low frequency words and new words. For example, if the total probability \nof high frequency words is 0.8, the frequency of these words will stably increase in \naccordance with the increase of sample size, and thus the total probability will remain \nsteadily as about 0.8. At the same time, the remainder 0.2 probability will be assigned \nto low frequency words. Since the total probability is persistently 0.2, more new \nwords that may appear in a larger sample will reduce the average occurrence \nprobability (or the mean relative frequency) of each word, which shapes the \ndownward bending of the lower segment. \n \nResults \nZipf‚Äôs law reflects the power law relation between word frequency and its rank. Since \nZipf‚Äôs law is found, in our study, invariably to exhibit a 3-segment structural pattern, \nwe are mainly concerned with dynamic patterns and the limiting values of the power \nexponents of these 3 segments, that is,  the power exponent of the upper segment \n(hereafter exponent 1), the power exponent of the middle segment (hereafter exponent \n2), and the power exponent of the lower segment (hereafter exponent 3). The results \nof fitting variations of exponent 1 and 2 in samples of increasing size to power law, \nwith adjusted R2=0.8506 and 0.9177, show that, when the sample size approaches \ninfinity, increasing sample size results in zero difference of exponent 1 and 2 go to, \nand that exponent 3 has a limiting value, with the average, the maximal, and the \nminimal goodness of fitting being respectively 0.9603, 0.5277, and 0.9996. In other \nwords, with increasing sample size, exponents 1 and 2 gradually reach constants, \nexponent 3 goes to a limiting value. Since exponent 3 is always significantly smaller \n 6 / 18 \n \nthan exponents 1 and 2, the downward deviation of the lower is a universal \nphenomenon in natural languages.  \n As seen in Figure 1, for all the 50 languages, the frequency-rank curves all deviate \nfrom the theoretical expectation of Zipf‚Äôs law. For any of these 50 languages, the \nZipf‚Äôs curve can be dissected into 3 segments. Piecewise fitting of Zipf‚Äôs curves to \npower-law functions in 50 languages indicates that the minimal adjusted R2s of the 3 \nsegments are 0.9619, 0.9908, and 0.9482; the maximal adjusted R2s are 0.9976, \n0.9998, and 0.9966; the mean adjusted R2s are 0.9879, 0.9976, and 0.9831.The upper \nsegment of the curve is consistently unsmooth, roughly covering words whose \nfrequency ranks are within 200 (the 200 most frequent words); the middle segment is \nsmoother, covering the words whose frequency ranks are between 200 and 2000 (or \n3000 in some languages); the lower segment is also smooth, but slopes more steeply \n(that is, this segment bends downward), covering the rest of the words in the sample. \n  \nFigure 1Ôºé The frequency-rank curves found in 50 languages of Leipzig corpus. The \nupper segment of each curve (the segment before the 1st vertical line) is unsmooth; the \nmiddle segment of each curve (the segment between the 1st and the 2nd vertical lines) \nis smooth, with the gradient being roughly 1. The lower segment of each curve (the \nsegment after the 2nd vertical line) is also smooth, but bends downward to deviate \nfrom the expected line.  \n \n10\n0\n10\n2\n10\n4\n10\n6\n10\n8\nRank\n10\n0\n10\n2\n10\n4\n10\n6\n10\n8\nFrequency\n 7 / 18 \n \nIn addition, the 3 segments also seem to register distinctive dynamic properties. As \ncan be seen in Figure 2, the sample size has different influences on the 3 segments: 1) \nexponent 1 and exponent 2 remain stable despite the growth of sample size; 2) \nexponent 3 and the power exponent of the entire curve decrease with the growth of \nsample size, going to a limiting value when the sample size is approaching infinity. In \nother words, these 3 segments exhibit different dynamic properties. Due to limited \nspace, Figure 2 demonstrates only the statistics of English. In fact, for the exponents \nof the 3 segments in 50 languages, the Pearson's linear correlation coefficients are \n0.2543 between exponent 1 and 2, 0.0746 between exponent 1 and 3, -0.2384 between \nexponent 2 and 3. So Zipf‚Äôs curves of 50 languages all show similar patterns. And \nstatistical tests indicate no correlation between exponents of any two segments of the \ncurves.  \nApart from the sample size, linguistic factors, such as text genre and typological \ndifference, seem to also have different effects on the 3 segments. Thus, 25 languages \nwith both wiki texts and news texts are chosen to study the influence of genres on the \n3 segments. On the basis of these 25 languages, this study, first of all, investigated the \nOchiai coefficients of words that are shared by texts of both genres in each segment. \nThe mean values are respectively 0.6362, 0.524, and 0.3477 in the 3 segments. In \nother words, in the upper segment, about 36% of words are different across the two \ngenres, while for the lower segment, more than 65% of words are different. Such a \ndifference may have much to do the different topics that different text genres usually \ninvolve. Then, we use nonparametric tests to investigate the influence of genres on \nexponents 1, 2, and 3, since little is known of the distribution patterns of the \nexponents of theses languages. A Wilcoxon rank sum test is conducted to investigate \nthe influence of genres on the exponents of each segment, which indicates that the P-\nvalues of exponent 1, 2, 3 and Heaps‚Äô are respectively 0.3033, 0.0598, 0.3417, and \n0.1160. Obviously, the test only yields low p-value for exponent 2. In other words, if \nwe set significance level as 0.1, exponent 1, 3 and Heaps‚Äô are insensitive to genre \ndifferences while exponent 2 is sensitive.  To investigate the influence of typological \ndifferences on exponents 1, 2, 3 and Heaps‚Äô exponent, we conduct F-test on 40 \nlanguages that fall into 7 typological branches. The results indicate that the F-values \nand P-values of Exponent 1, 2, 3, and Heaps‚Äô exponent are  22.98 and 1.7786e-10, \n13.36 and 1.26e-07, 0.8742 and 0.5243, 0.7701 and 0.5988, which suggests \n 8 / 18 \n \nsignificant influences of typological difference on exponents 1 and 2. . Therefore, it \ncan be seen that genre differences influence exponent 2, and typological differences \ninfluence both exponents 1 and 2. For the upper segment of the curves, most words \nremain the same regardless of different text genres. For the lower segment, on the \ncontrary, most words are different across different genres. However, the exponents of \nsegment 1 and 3 are insensitive to genres, suggesting that semantics has little \ninfluence on the overall frequency distributions of words in these two segments. What \nis more, these segments seem to cover words of different categories: the upper \nsegment mainly includes the function words while the middle and the lower segment \nmainly cover content words. \n \nFigure 2Ôºé The relations between exponents 1, 2, 3 and the sample size as found in \nEnglish sample of Leipzig corpus \n \nThe different dynamic properties of the 3 segments can be seen more clearly in the \nfact that, for different segments, the increase of sample size will bring about different \ngrowth rates of word frequency. The statistics of 50 languages indicate that the \nfrequency of most words in the upper and the middle segments increase in proportion \nto the sample size, while the frequency of words in the lower segment does not. In \nother words, the upper and the middle segments of curves rise in accordance with the \ngrowth of the sample size, but the lower segment rises much slower, leading to the \ndownward bending of the curve. \n0\n0.5\n1\n1.5\n2\n2.5\nSample size\n10\n7\n-1.2\n-1\n-0.8\n-0.6\n-0.4\nExponent\nExponent 1\nExponent 2\nExponent 3\n 9 / 18 \n \nFor large scale language samples, especially those that have been shuffled, the \nprobability of words occurring in it  is expected to be stable, and as a result, their \nfrequency is expected to be in a constant proportion to sample size (the number of \nword tokens). To investigate the growth rates (dynamic properties) of word frequency \nin each segment, we calculate the growth rates of word frequency in relation to \ndoubled sample sizes. The results show that, for most words in the upper and the \nmiddle segments, the growth rates cluster around 2 when sample size doubles. That is, \nfrequencies of most words grow in proportion with the increase of sample sizes. In \ncontrary, for most words in the lower segment, the growth rates cluster around 1, \nwhich means that, in this segment, the frequencies of most words grow much slower \nthan the increase of sample sizes. Since the frequencies of most words in the lower \nsegment fail to increase proportionally with the growth of the sample size, the relative \nfrequency of words in this segment is likely to diminish in larger samples, or rather, \nthe temporal interval between two occurrences of a word is likely to widen in larger \nsamples.  \nIn short, word frequency distributions in natural languages are characterized by a \ntendency for Zipf‚Äôs curves of larger samples to bend increasingly downward. This \nphenomenon can be explained in terms of the cognitive mechanism of the dual-\nprocess model (21-25) that suggests two quite different generating mechanisms for \nhigh-frequency words and low-frequency words. To test this hypothesis, we conduct a \ncomputer simulation to find out whether a model including two different generating \nmechanisms is necessary to generate the universally observed downward bending. In \nthe simulation, new words‚Äô occurrence probability is predicted by Heaps‚Äô law and \nstatistical fitting. According to Heaps‚Äô law, there is a good power-law relation \nbetween the size of a text (the number of word tokens) and the number of word types. \nOur statistical investigations into Heaps‚Äô exponents of 50 languages suggest that the \ndistribution of Heaps‚Äô exponents is insensitive to genres and typological differences. \nThe differential form of the function of heaps‚Äô law is the ratio of increase of word \ntypes to the increase of word tokens, i.e. the occurrence rates of new words. The \nHeaps‚Äô exponent is smaller than 1, and hence, the exponent of the corresponding \ndifferential form is smaller than 0, or, decreases with the increase of sample sizes. In \nother words, new words do not grow at a steady rate, but slower and slower, with \nincreasing sample size. \n 10 / 18 \n \nPresently there are two influential models to account for the frequency distribution of \nwords, both based on a single generating process. One is Simon model (12, 26), \nwhich assigns no fixed probabilities to high-frequency words, prescribing that new \nwords appear with equal probabilities. This model has been attracting scholars \nbecause it can generate power law distributions of words and have explanatory power \noutside of linguistics. The other one is random typing model, which generates word \nfrequency distribution by randomly hitting the keys of typewriters and calculating the \nfrequencies of strings separated by spaces. This model, though mathematically simple, \nis capable of generating power law distributions. Figure 3 illustrates the simulation \nresults of these two models. As shown in Figure 3, both models produce frequency-\nrank relations that, though in agreement with power-law functions, lack the downward \nbending found in natural languages (27). In other words, the frequency distribution of \nwords in natural languages can be explained by neither of models that are respectively \nbased on only one generating process. However, the proposed model based on the \nmechanism of dual-process yields curves with downward bending similar to those \nfound in natural languages.  \n \nFigure 3. The frequency distributions generated by Simon model, random-typing \nmodel, and the model of our study. \n \nDiscussion \n10\n0\n10\n2\n10\n4\n10\n6\nRank\n10\n0\n10\n2\n10\n4\n10\n6\nFrequency\nReal data\nOur model\nSimon model\nMonkey type model\n 11 / 18 \n \nThe statistical results suggest that Zipf‚Äôs curves of natural languages seem to \nuniversally have a 3-segment structural pattern: the upper segment, the middle \nsegment, and the lower segment, as persistently found in 50 languages. Since each \nsegment has its distinctive linguistic properties, Zipf‚Äôs curve with this peculiar 3-\nsegment structural pattern is likely to reflect a linguistic law, not a mere statistical \nartifact. In fact, Zipf and many other scholars have long noticed that Zipf‚Äôs curve may \nconsist of segments with different properties, as reflected in the observed deviation \nfrom expected curves (28-29). Some other scholars have tried two-segment piecewise \nfitting in English and obtained results that are capable of linguistic interpretations (13, \n30). But their two-segment piecewise fittings are all based on one language sample, \nand hence cannot universally reveal the dynamic properties of these different \nsegments. Based on 50 different languages, our findings probably suggest universal \nlinguistic patterns that are underpinned by universal motivations such as common \nhuman cognition.  \nSince the 3 segments present different linguistic properties, it is possible to \nlinguistically account for how this pattern is shaped. The upper segment of the curve \ncovers mostly function words‚Äîthe fundamental syntactic means in sentence \norganization and comprehension. Therefore, their frequencies in one language are \nexpected to be stable regardless of text genres and sample sizes. However, syntactic \nmeans (e.g. function words, inflection, and word order) may weigh differently in \ntypologically different languages, which is a possible linguistic explanation for our \nfindings that typological differences have influence on exponents 1. Exponent 2 \nseems sensitive to text genre. Linguistically, this may have much to do with basic \ncategories or concepts (31) because the middle segment consists mainly of most \nfrequent content words. Texts of different genres may cover very different fields and \ntopics, which may rely differently on these basic categories (concepts) in semantic \npresentation. As a result, the content words denoting these concepts may account for \ndifferent proportions of words in texts of different genres, or rather, their relative \nfrequencies may vary with different text genres. In addition, typological differences \nalso affect the exponent of the middle segment, which may imply that typologically \ndifferent languages may somewhat differ in the use of basic categories.  However, in \none language, the basic categories and their roles in semantic presentation are rather \nstable in a specific field, and hence the relative frequencies of these words should be \n 12 / 18 \n \nstable, regardless of the text size. This is probably a linguistic explanation for the \nfinding that exponent 2 is sensitive to differences in both typology and genre, but \ninsensitive to differences in corpus size. Used quite infrequently, and in many cases \nonly once, the words covered in the lower segment of the curves are largely neither \nfunction words nor basic category content words. For these words, their relative \nfrequencies are not stable, with the average decreasing with larger sample sizes. What \nis noticeable is that the limiting value of exponent 3, insensitive to text genres, sample \nsizes or typological differences, is likely to be a linguistic universal shared by \ndifferent languages.   \nUniversal linguistic patterns are believed to be driven by more fundamental \nmechanisms, such as those of human cognition.  As a result, the above linguistic \npatterns as reflected by the 3-segment structural pattern may also be accountable in \nterms of common human cognitive mechanism, which may further explained why this \nlaw universally exists in various languages and in many other human related areas. .   \nHigh-level human cognitive activities are probably characterized by the dual-process \nmodel. Type-1 process features promptness, automaticity, effortlessness, freedom \nfrom conscious attention, etc.; Type-2 process features slowness, controlled attention, \nconscious effort, etc. (32). The upper and the middle segments of Zipf‚Äôs curve mainly \ncover those high-frequency words, whose frequencies rise in proportion to increase in \nsample size. This dynamic property means that there is no need to introduce any \ntemporal parameter or other constraints into formal probability models to account for \ntheir probability of occurrence. Cognitively, the freedom from extra constraints means \nlittle or no effort for activation of these high-frequency words. Such effortlessness and \npromptness characterize Type-1 process. In contrast, the lower segment mainly covers \nthose low-frequency words, whose relative frequencies are temporally unstable, \ndecreasing with the growth of sample sizes. Therefore, temporal parameter or other \nconstraints are needed to account for variable relative frequencies of low-frequency \nwords. For these words, the relation between frequency and number of word types \n(frequency spectral) is also a power law function (1, 33). This can be considered as \nthe 1/f noise, a phenomenon widely found in both natural and social worlds. The \nfrequency of low-frequency words is subject to modeling through non-stationary and \nnon-ergodic stochastic processes (34-35). Such extra constraints cognitively \n 13 / 18 \n \ncorrespond to extra mental efforts to activate or process these words. This effort-\ndemanding processing is typical of type-2 process. \nIn fact, numerous psychological studies have evinced that word frequency bears \nclosely on nearly all psychological/cognitive processes related to words, including \nintelligibility (36-37), recognizability (38), pronunciation (39), memory retention (40), \nnaming time, and semantic categorization time (41), which is called word frequency \neffect. That is, the processing of high-frequency words call for less \ncognitive/psychological cost than low-frequency words (42-43). These findings are \nconsistent with the findings of this study that high-frequency words and low-\nfrequency words have very different dynamic properties. The probable reason is that \nlanguage systems may rely heavily on the former for efficient Type-1 process, and at \nthe same time, limit the growth rate of the latter to moderately regulate Type-2 \nprocess. \nGoverned by the principle of least effort, human cognition should mold language in \nsuch ways that Type-1 process can be fully utilized. The organization and \ncomprehension of sentences depend heavily on syntactic means such as function \nwords, which may play key roles in triggering Type-1 process so as to automatize \nsyntactic organization and parsing. Cognitively, the access of function words, thanks \nto their high frequency, is largely effortless and automatic. As a result, function words \nmay contribute to automatic and quick syntactic parsing. More sentences mean more \nfunction words, and hence their relative frequencies in one language should be stable \nregardless of genres and sample sizes. However, different syntactic means (unction \nwords, inflection, and word order) may weigh differently in typologically different \nlanguages, which means the overall frequencies of function words may bear on \ntypology. \nNevertheless, sentence comprehension is more than syntactic parsing‚Äîit ultimately \naims to build semantic representations. Cognitively, the basic categories play vital and \nfundamental roles in our knowledge of world and semantic representation, frequently \nused, and cognitively easy to access. That is, these words may also be subject to \nType-1 process. In brief, their frequency should steadily grow in proportion to the \nincrease of sample sizes. However, since different genres may rely, to different \ndegrees, on basic categories, the frequencies of these words are subject to influence of \ndifferent genres.  \n 14 / 18 \n \nMost words covered in the lower segment of the curve denote those concepts less \nfundamental in the world knowledge and thus are used infrequently. It is, hence, \ninefficient to constantly store these words in mind as independent units‚Äîit would be \nmore economical to provisionally assemble a low-frequency word when needed, an \noperation calling for conscious attention and efforts. In other words, these words are \nlargely subject to Type-2 process. Despite their assumed high processing cost, these \nwords will not significantly deteriorate the general efficiency of language processing \nbecause they account for merely a quite limited proportion of word tokens in a \nlanguage sample.  \nIn brief, it is Type-1 process that shapes the upper and the middle segments of Zipf‚Äôs \ncurve, and Type-2 process, the lower segment. If human beings share similar \ncognitive mechanism, such as dual-process model, and if human languages are largely \ndriven by cognition, it may be predicted that the 3-segment structural should be found \nin various languages, which is what has been found in our work on 50 languages.  \n \nConclusions \n \nThrough the first large-scale cross-language investigation, our work has found that \nZipf‚Äôs laws in 50 languages all share a 3-segment structural pattern, with the lower \nsegment invariably bending downward to deviate from theoretical expectation, and \neach segment demonstrating distinctive linguistic properties and different biases in \nuse. Further computer simulations suggest the fundamental cognitive mechanism of \ndual-processing as a deep level motivation for the 3-segment structure. That is, Zipf‚Äôs \nlaw found in natural languages is probably the result of the general constraint of \nhuman cognition.  These findings suggest that Zipf‚Äôs laws found in human-related \nfields are likely to be human driven, having deep-level motivations such as general \ncognitive mechanisms, and that big-data analysis into languages may provide valuable \nmeans to uncover language regularities and to trace cognitive laws hidden behind \nthem. \n \nAcknowledgments  This work is partly supported by the National Social Science \nFoundation of China (Grant No. 11&ZD188). \n \n 15 / 18 \n \nReferences \n \n1. \nZipf, G. (1936). The psycho-biology of language: an introduction to dynamic \nphilology. Boston: Houghton Mifflin. \n2. \nZipf, G. (1949). Human Behavior and the Principle of Least Effort. New York: \nAddison-Wesley. \n3. \nNewman, M. (2005). Power laws, Pareto distributions and Zipf‚Äôs law. \nContemporary physics, 46(5), 323‚Äì351. \n4. \nPiantadosi, S. T. (2014). Zipf's word frequency law in natural language: A critical \nreview and future directions. Psychonomic bulletin & review, 21, 1112-1130. \n5. \nXavier Gabaix (2016). Power Laws in Economics: An Introduction. Journal of \nEconomic Perspectives, 30(1):185-206. \n6. \nMiller, G. (1957). Some effects of intermittent silence. The American Journal of \nPsychology, 311‚Äì314. \n7. \nHuberman, B.A., Adamic, L.A. Evolutionary dynamics of the World Wide Web. \nNature 1999, 399, 131 \n8. \nLi, Wentian (1992). Random Texts Exhibit Zipf's-Law-like Word Frequency \nDistribution. IEEE Transaction on Information Theory, Vol. 38(6), 1842‚Äì1845. \n9. \nParker-Rhodes, A., & Joyce, T. (1956). A theory of word-frequency distribution. \nNature, 178, 1308. \n10. Manin, D. (2009). Mandelbrot's Model for Zipf‚Äôs Law: Can Mandelbrot‚Äôs Model \nExplain Zipf‚Äôs Law for Language? Journal of Quantitative Linguistics, 16(3), \n274‚Äì285. \n11. Mason, W., & Suri, S. (2012). Conducting behavioral research on amazon‚Äôs \nmechanical turk. Behavior research methods, 44(1), 1‚Äì23. \n12. Simon, H. A. (1955). On a class of skew distribution functions. Biometrika, 425‚Äì\n440. \n13. Ferrer i Cancho, R., & Sol√©, R. (2003). Least effort and the origins of scaling in \nhuman language.  Proceedings of the National Academy of Sciences of the \nUnited States of America, 100(3), 788. \n14. Corominas-Murtra, B., & Sol√©, R. V. (2010). Universality of zipf‚Äôs law. Physical \nReview E, 82(1), 011102. \n15. Baek, S. K., Bernhardsson, S., & Minnhagen, P. (2011). Zipf‚Äôs law unzipped. \nNew Journal of Physics, 13(4), 043004. \n 16 / 18 \n \n16. Rybski D, Buldyrev SV, Havlin S, Liljeros F, Makse HA (2009). Scaling laws of \nhuman interaction activity. Proceedings of the National Academy of Sciences of \nthe United States of America, 106(31):12640-12645 \n17. Mandelbrot, B.B. On the theory of word frequencies and on related markovian \nmodels of discourse. In Structure of Language and Its Mathematical Aspects: \nProceedings of Symposia on Applied Mathematics Volume 3; Jakobson, R., Ed.; \nAmer. Math. Soc.: Providence, RI, USA, 1961; pp. 190-219. \n18. Mandelbrot, B. (1982). The Fractal Geometry of Nature. San Francisco: Freeman  \n19. Frank, S. A. (2009). The common patterns of nature. Journal of evolutionary \nbiology, 22(8), 1563‚Äì1585. \n20. Conrad, B., &Mitzenmacher, M. (2004). Power laws for monkeys typing \nrandomly: the case of unequal probabilities. IEEE Transactions on Information \nTheory 50(7), 1403‚Äì1414. \n21. Barrett, L. F.; Tugade, M. M.; Engle, R. W. (2004). Individual differences in \nworking memory capacity and dual-process theories of the mind. Psychological \nBulletin. 130: 553‚Äì573. \n22. Evans, J. (2003). In two minds: dual-process accounts of reasoning. Trends in \nCognitive Sciences, 7 (10): 454‚Äì459. \n23. Jonathan St. B. T. Evans and Keith E. Stanovich (2013). Dual-Process Theories \nof Higher Cognition: Advancing the Debate. Perspectives on Psychological \nScience, 8(3) 223‚Äì 241, \n24. Kahneman, D (2003). A perspective on judgment and choice. American \nPsychologist, 58: 697‚Äì720. \n25. Van Lancker Sidtis, D. (2004). When novel sentences spoken or heard for the \nfirst time in the history of the universe are not enough: Toward a dual-process \nmodel of language. International Journal of Language and Communication \nDisorders, 39 (1), 1 ‚Äì 44. \n26. Yule, G. U. (1944). The statistical study of literary vocabulary. Cambridge \nUniversity Press \n27. Ferrer-i-Cancho R, & Elvev√•g B (2010). Random Texts Do Not Exhibit the Real \nZipf‚Äôs Law-Like Rank Distribution. PLoS ONE, 5(3): e9411.  \n28. Mandelbrot, B. (1953). An informational theory of the statistical structure of \nlanguage. In Communication Theory. Ed. W. Jackson. London: Butterworth, 486-\n504. \n 17 / 18 \n \n29. Font-Clos, F., Boleda, G. & Corral, A. (2013). A scaling law beyond Zipf‚Äôs law \nand its relation to Heaps‚Äô law. New Journal of Physics, 15(9), 093033. \n30. Gerlach, M. & Altmann, E. G. (2013). Stochastic model for the vocabulary \ngrowth in natural languages. Physical Review X, 3, 021006. \n31. Manin, D. (2008). Zipf‚Äôs law and avoidance of excessive synonymy. Cognitive \nScience, 32(7), 1075‚Äì1098.  \n32. Stanovich, K. E., & Toplak, M. E. (2012). Defining features versus incidental \ncorrelates of Type 1 and Type 2 processing. Mind and Society, 11, 3-13. \n33. Chen, Y.-S. & Chong, P. (1992). Mathematical modeling of empirical laws in \ncomputer applications: A case study. Computers & Mathematics with \nApplications, 24(7), 77-87.  \n34. Correll, J. (2008). 1/f noise and effort on implicit measures of bias, Journal of \nPersonality and Social Psychology 94, 48‚Äì59. \n35. D.L. Gilden, T. Thornton, M.W. Mallon (1995). 1/f noise in Human Cognition, \nScience, 267: 1837‚Äì1839. \n36. Brown, H; Rubenstein, C.R (1961). Test of response bias explanation of word-\nfrequency effect. Science, 133: 280‚Äì28. \n37. Howes, D. H. (1957). On the relation between the intelligibility and frequency of \noccurrence of English words. Journal of the Acoustical Society of America, 29: \n296‚Äì305. \n38. Segui, J, Mehler, J, Frauenfelder, U, Morton, J (1982). The word frequency effect \nand lexical access. Neuropsychologia, 20: 615‚Äì627 \n39. David A Balota, James I Chumbley (1985).The locus of word-frequency effects \nin the pronunciation task: Lexical access and/or production? Journal of Memory \nand Language, Vol. 24, 89‚Äì106. \n40. Hulme, C., Roodenrys, S., Schweickert, R., Brown, G. D., Martin, S., & Stuart, G. \n(1997). Word-frequency effects on short-term memory tasks: evidence for a \nredintegration process in immediate serial recall. Journal of Experimental \nPsychology: Learning, Memory, and Cognition, 23, 1217-1232. \n41. Forster, K. I., & Chambers, S. M. (1973). Lexical access and naming time. \nJournal of Verbal Learning and Verbal Behavior, 12, 627‚Äì635. \n42. Reder, L. M., Nhouyvanisvong, A., Schunn, C. D., Ayers, M. S., Angstadt, P., & \nHiraki, K. (2000). A mechanistic account of the mirror effect for word frequency: \nA computational model of remember‚Äìknow judgments in a continuous \n 18 / 18 \n \nrecognition paradigm. Journal of Experimental Psychology: Learning, Memory, \nand Cognition, 26, 294‚Äì320. \n43. Diana, Rachel A. & Lynne M. Reder (2006). The Low-Frequency Encoding \nDisadvantage: Word Frequency Affects Processing Demands. Journal of \nExperimental Psychology: Learning, Memory, and Cognition 32, 805‚Äì815. \n \n \n(2016-10-26) \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-07-05",
  "updated": "2018-07-05"
}