{
  "id": "http://arxiv.org/abs/1807.01960v1",
  "title": "Deep Reinforcement Learning for Doom using Unsupervised Auxiliary Tasks",
  "authors": [
    "Georgios Papoudakis",
    "Kyriakos C. Chatzidimitriou",
    "Pericles A. Mitkas"
  ],
  "abstract": "Recent developments in deep reinforcement learning have enabled the creation\nof agents for solving a large variety of games given a visual input. These\nmethods have been proven successful for 2D games, like the Atari games, or for\nsimple tasks, like navigating in mazes. It is still an open question, how to\naddress more complex environments, in which the reward is sparse and the state\nspace is huge. In this paper we propose a divide and conquer deep reinforcement\nlearning solution and we test our agent in the first person shooter (FPS) game\nof Doom. Our work is based on previous works in deep reinforcement learning and\nin Doom agents. We also present how our agent is able to perform better in\nunknown environments compared to a state of the art reinforcement learning\nalgorithm.",
  "text": "Deep Reinforcement Learning for Doom using\nUnsupervised Auxiliary Tasks\nGeorgios Papoudakis\nAristotle University of Thessaloniki\nElectrical and Computer Engineering\nThessaloniki, Greece\npapoudak@auth.gr\nKyriakos C. Chatzidimitriou\nAristotle University of Thessaloniki\nElectrical and Computer Engineering\nThessaloniki, Greece\nkyrcha@issel.ee.auth.gr\nPericles A. Mitkas\nAristotle University of Thessaloniki\nElectrical and Computer Engineering\nThessaloniki, Greece\nmitkas@auth.gr\nAbstract—Recent developments in deep reinforcement learning\nhave enabled the creation of agents for solving a large variety\nof games given a visual input. These methods have been proven\nsuccessful for 2D games, like the Atari games, or for simple\ntasks, like navigating in mazes. It is still an open question, how\nto address more complex environments, in which the reward is\nsparse and the state space is huge. In this paper we propose a\ndivide and conquer deep reinforcement learning solution and we\ntest our agent in the ﬁrst person shooter (FPS) game of Doom.\nOur work is based on previous works in deep reinforcement\nlearning and in Doom agents. We also present how our agent is\nable to perform better in unknown environments compared to a\nstate of the art reinforcement learning algorithm.\nIndex\nTerms—Deep\nreinforcement\nlearning,\nunsupervised\nlearning, Doom\nI. INTRODUCTION\nRecent advances in deep learning enabled the combination\nof neural networks and reinforcement learning. The ﬁrst suc-\ncessful work in deep reinforcement learning Mnih et al. [1],\nwas a Q-learning agent in which the state action value function\nwas approximated by a deep network. In order to increase\nthe performance and the stability of the agent, an experience\nreplay buffer was proposed. This agent was able to achieve\nsuperhuman results in a large number of Atari games.\nDespite the large success of this work, it could only be\nused with off-policy learning algorithms. This drawback was\novercome by the introduction of an asynchronous framework\nMnih et al. [2] in reinforcement learning. This development\nenabled the combination of deep networks and on-policy\nalgorithms, and resulted in the removal of the experience\nbuffer. Additionally, the new agents were much more stable\nand their performance increased compared to previous works.\nAs a result, deep reinforcement learning was able to perform\ndecently in more complex environments, like 3D mazes.\nThe huge success of deep reinforcement learning resulted\nin an increased interest of the research community for various\napplications. One interesting research question is how to im-\nprove the existing algorithms in order to be able to successfully\nact in more complex environments like Doom, Starcraft, etc.\nThese environments are much more complex, with a large set\nof states and sparse rewards. This results in a lack of a standard\nprocedure in order to address such tasks, despite the fact that\nthere were many successful applications of deep reinforcement\nlearning the recent years.\nThis paper is focused on the creation of an agent using\nstate of the art deep reinforcement algorithms. Additionally,\ninspired from previous works we propose a speciﬁc solution\nfor the game of Doom based on three unsupervised auxiliary\ntasks; the value function replay, the reward prediction and the\npixel control.\nThe structure of the paper is as follows: in section II we\ndiscuss the theoretical background around deep reinforcement\nlearning, while in section III we refer to major works in\nagents for the game of Doom. In section IV we introduce our\nagent and in section V we perform an experimental evaluation.\nFinally, in section VI we conclude our work and discuss future\nresearch directions.\nFig. 1.\nScreen-shot from a Doom environment. The agent has to kill the\nMonster in order to survive.\nII. THEORETICAL BACKGROUND\nA. Reinforcement Learning and the Actor-Critic Algorithm\nA decision making problem can be deﬁned as a Markov\nDecision Process (MDP), which is described from the tuple\n(S, A, R, T), where S is the set of states, A is the set of\nactions, R : S × A × S →R is the reward function and\nT(s′|s, a) : S × A × S →[0, 1] is the transition function. We\ndeﬁne the policy function, which is the function that decides\nthe action based on the state π : S →A. In every MDP,\narXiv:1807.01960v1  [cs.LG]  5 Jul 2018\ngiven a policy π, we deﬁne the state value function as the\nexpected sum of discounted rewards from the current state\nuntil the end of the episode V π(s) = Eπ[PT\nt=0 γtrt|st = s],\nwhere γ is called discount factor and takes values between 0\nand 1. The goal of reinforcement learning is to compute the\npolicy function that maximizes the expected sum of discounted\nrewards Eπ[PT\nt=0 γtrt]. In our work, we use the actor-critic\nKonda et al. [3] algorithm in order to compute the policy of\nour agent. Using the policy gradient theorem and the deﬁnition\nof the state value function V , we can compute the parameters\nof the policy function by minimizing the error below.\nLπ = −(r + γV (s′) −V (s))log(π(a|s))\nFor the approximation of the state value function we will\nminimize the standard TD error:\nLV = 1\n2(r + γV (s′) −V (s))2\nFor the approximation of the policy and the state value\nfunction we will use the deep architecture that was proposed\nby Mnih et al. [2]. The policy and the state value networks\nshare all the layers except the output layer; the policy output\nlayer is a softmax function and the state value output layer is\nliner with one node.\nB. Asynchronous framework\nThe asynchronous framework was proposed by Mnih et\nal. [2] in order to combine on-policy algorithms with deep\nlearning. There is a global neural network which is respon-\nsible for learning the policy and the state value function.\nFurthermore, it creates a number of parallel processes and\neach process has a copy of the global network and interacts\nwith the environment independently from the other processes.\nAfter a given number of (local) steps each process updates\nasynchronously the weights of the global network. The combi-\nnation of actor-critic and the asynchronous framework resulted\nin the asynchronous advantage actor-critic (a3c) (Mnih et al.\n[2]). The a3c algorithms led not only to better results in a\nplethora of environments but also signiﬁcantly reduced the\ntraining steps.\nC. Unsupervised auxiliary tasks\nThe unsupervised auxiliary tasks were used in combination\nwith a3c algorithm (Jaderberg et al. [4]) in order to enhance\nthe learning ability of the neural network that was used for\nthe policy approximation. In order to use the auxiliary tasks,\nan experience replay buffer is included. In this work we used\nthree auxiliary tasks, which are described below:\n• Value function replay: We uniformly sample from the\nexperience replay and we update again the state value\nfunction using the temporal difference error:LV R\n=\n1\n2(r + γV (s′) −V (s))2.\n• Reward prediction: In the deep network, which is\nresponsible for the policy and the value function approx-\nimation, we add one more softmax output. This output\npredicts the sign of the reward based on the last three\nframes. As a result there are three output nodes; one for\nzero reward, one for positive reward and one for negative\nreward. In order to avoid the class imbalance problem, we\nsample from the experience replay in a way that ensures\nthat half of the samples have zero reward and the rest of\nthem have negative or positive reward.\n• Pixel change: In the deep network, a deconvolutional\nlayer is added. This layer is responsible for encourag-\ning the agent to maximize the difference between two\nconsecutive frames, due to the fact that changes in the\nsensory input many times lead to rewarding events. For\nthis purpose, we consider a pseudo-reward - the absolute\ndifference between two consecutive frames - and we\nmaximize it using the Peng et al. [6] n-step Q-learning\nalgorithm.\nIII. RELEVANT WORK\nRecently, there has been a great number of works trying\nto address the problem of visual Doom. Wu et al. [5] used\na3c and curriculum learning in order to create a Doom agent.\nLample et al. [7] used a divide and conquer method in order\nto divide the action space. Then, they trained one agent for\nﬁring at the enemy monsters, the action agent, and one agent\nfor navigating in the mazes of the game, the navigation agent.\nIn order to decide, which agent had to act at each timestep\nthey tried to detect if there is an enemy monster in the current\nframe. If an enemy exists, the action agent will act, otherwise\nthe navigation agent will act. In the next section, we will\ndiscuss our method, which is based on Lample et al. [7]\nmethod, using a different way in order to determine which\nagent is going to act at each timestep.\nIV. METHODOLOGY\nIn our work, we used a similar approach as Lample et al. [7].\nWe separated the action space and we trained an agent for ﬁr-\ning at the enemy monsters and one for navigating in the com-\nplex mazes and gathering objects. Additionally, from the set of\nthe available moves we only kept ﬁve (5) for the action agent\n(FIRE, MOVE FORWARD, TURN RIGHT, TURN LEFT,\nMOVE BACKWARD) and three (3) for the navigation agent\n(MOVE FORWARD, TURN RIGHT, TURN LEFT). This is\nthe minimum set of necessary actions for our agent. By\nreducing the number of actions, we signiﬁcantly reduce the\ncomplexity of our system and the algorithms converge faster\nto a good policy. Both agents are trained using the UNREAL\n(Jaderberg et al. [4]) algorithm, which is the combination of\nthe a3c and the auxiliary tasks.\nWe used the same architecture that was proposed by Jader-\nberg et al. [4]. The input is a RGB image with dimensions\n84 × 84 × 3. The network has two convolutional layers with\n16 8 × 8 ﬁlters and 32 4 × 4 ﬁlters respectively. After the\nconvolutional layers, a fully connected layer with 256 nodes\nis connected. All three layers use ReLU for activation function.\nFinally, the last hidden layer is a LSTM with 256 nodes and\nit is unrolled for 20 steps. The policy output is a softmax\nfunction, while the state value output is a linear function. At\nthe output of the second convolutional layer we add a fully\nconnected layer with 3 softmax output nodes. This layer is\nresponsible for the reward prediction auxiliary task. For the\npixel change task we connect a ReLU fully connected layer\nand a deconvolutional layer in the output of the LSTM.\nThe agents try to minimize the cumulative error of the\nindividual components; the policy error Lπ, the value function\nerror LV , the value function replay error LV R, the reward\nprediction error LRP and the pixel change error LP C. In\norder to minimize the error below we used a shared RMSProp\noptimizer (Mnih et al. [2]).\nL = Lπ + LV + λV RLV R + λRP LRP + λP CLP C\nWe used the Vizdoom (Kempka et al. [8]) platform for\ntraining and evaluating our agents. This platform provides a\nlarge number of environments and game settings for training\nreinforcement learning agents based on visual input. The\nplatform allows to shape the reward of the environment. This\nis necessary in order to efﬁciently train our agents. Without the\nreward shaping it would be impossible to train our agents due\nto the sparse reward of the environment. The reward shaping\nfor the action and the navigation agent can be seen in the table\nbelow:\nTABLE I\nTHE SHAPED REWARDS FOR THE ACTION AND THE NAVIGATION AGENT\nAgent\nkill\ndeath\nmissed shoot\nlost health\nobject gathered\nAction\n+1\n-1\n-0.02\n-0.06\n+0.3\nNavigation\n-1\n-0.1\n+0.5\nHaving deﬁned our action and our navigation agent, we now\nhave to decide which agent is going to act given a frame. For\nthis purpose, we will use one of the auxiliary tasks of the\nnavigation agent; the reward prediction. From the table I, we\ncan understand that the navigation agent receives a negative\nreward only when it loses health or it is killed. From this\nwe can conclude that there is an enemy close to our agent\nwhen it receives a negative reward. As a result, when there is\na prediction for negative reward from the navigation agents,\nthere is an enemy in the given frame. Otherwise, there is\na prediction for zero or positive reward. Experimentally, the\ncombined agent performs better if the action agent acts when\nthere is a prediction for positive reward. Therefore, if there is\na prediction for positive or negative reward, the action agent\nwill act, otherwise the navigation agent will act.\nV. EVALUATION\nBelow you can ﬁnd the experimental setup and the results\nboth for training and testing.\nA. Experimental setup for training the agents\nWe train both action and navigation agents using eight (8)\nparallel threads on four (4) different Doom maps. As a result\ntwo (2) actor learner threads are trained for each one of the\nfour (4) maps. In order to increase the learning capacity of\nour agents, we trained both of them in environments with\ndense rewards for 80 million global steps. Thus, the action\nagent was trained in environments that have a large number\nof enemy monsters, while the navigation agent was trained in\nenvironments that have a plethora of objects to pick up.\nWe need to formally deﬁne a set of measurements in order\nto evaluate our work. Since the reward that the agent receives\nfrom the game is based on a variety of the game’s feature, we\nconsidered the following measurements: The goal of the action\nagent is to maximize the number of kills, while minimizing the\nnumber of deaths. Similarly, the goal of navigation agent is to\nmaximize the number of objects it picks up, while minimizing\nthe number of deaths. In the Figures 2 and 3, one can ﬁnd the\nthe object-death ratio and the kill-death ratio for the navigation\nand the action agent respectively.\nFig. 2. Object-death ratio for the navigation agent during training.\nFig. 3. Kill-death ration for the action agent during training.\nB. Experimental setup for testing the agents\nAfter evaluating our agents during their training, we tested\nthe action agent and the combined agent both in known and\nunknown environments. We evaluated both agents in the four\n(4) maps that were used for training and two (2) unknown\nmap. In the known maps, we severely decreased the number of\nenemy monsters compared to those that were used for training.\nThe testing episode duration is 15 minutes in the known maps\nand in the second unknown map. In the ﬁrst unknown map\nthe episode terminates when the agent is killed. You can ﬁnd\nthe results in the table below:\nTABLE II\nRESULTS OF THE ACTION AGENT AND THE COMBINED AGENT IN A\nVARIETY OF ENVIRONMENTS\nAction agent\nCombined agent\nMap\nkills\ndeaths\nobjects\nkills\ndeaths\nobjects\nKnown 1\n13.32\n3.23\n29.51\n13.15\n4.58\n37.73\nKnown 2\n13.68\n3.32\n28.23\n12.44\n4.77\n67.21\nKnown 3\n14.7\n3.07\n62.4\n7.57\n6.53\n116.8\nKnown 4\n8.72\n1.43\n48.76\n6.45\n1.86\n36.94\nUnknown 1\n4.99\n1\n5.46\n5.89\n1\n20.19\nUnknown 2\n10.23\n1.97\n13\n12.94\n1.88\n36.4\nFrom Table II, it can be observed that the combined\nagent performs better than the action agent in the unknown\nenvironments. In the known environments the action agent\ntends to perform better than the combined agent in kills and\ndeaths statistics. This is not surprising due to the fact that\nthe action agent is trained in these environment in order to\nact independently. Finally, it is clear that the combined agent\ngathered far more objects than the action agent. This happened\nbecause the primary goal of the action agent is to ﬁre at the\nenemy monsters, while the combined agent aims both for ﬁring\nat the enemy monsters and for gathering objects.\nAfter presenting our results we evaluated their statistical\nsigniﬁcance due to the fact that Doom environments are\nstochastic with high variation. We performed t-tests between\nthe values of the combined and the action agent for all the\nevaluation metrics in all six environments. Their p-values are\ndepicted in the following table. Generally speaking, we can\nconclude that our results are statistical signiﬁcant with 95%\nconﬁdence (p-value below 0.05).\nTABLE III\nTHE P-VALUES OF THE T-TEST. ASTERISK DENOTES STATISTICAL\nSIGNIFICANCE WITH 95% CONFIDENCE.\nMap\nkills\ndeaths\nobjects\nKnown 1\n0.845\n0.038*\n0.024*\nKnown 2\n0.157\n0.049*\n1.56 × 10−11*\nKnown 3\n8.17 × 10−8*\n1.9 × 10−4*\n6.08 × 10−8*\nKnown 4\n6.38 × 10−4*\n0.22\n0.039*\nUnknown 1\n0.037*\nN/A\n1.9 × 10−29 *\nUnknown 2\n8.17 × 10−3*\n0.77\n5.7 × 10−18*\nVI. DISCUSSION AND FUTURE WORK\nIn this work, we proposed a system (agent) for learning to\nefﬁciently play the game of Doom. Our agent, uses a divide\nand conquer method based on Lample et al. [7] and a deep ar-\nchitecture for reinforcement learning based on Jaderberg et al.\n[4]. Lample et al. [7] method requires high-level informations\nfrom the environment in order to be used, like the existence or\nnot of an enemy or an object in a frame. Our method requires\nonly the reward signal.\nIn the future, we would like to evaluate how the divide\nand conquer method could be applied in other similar tasks.\nAdditionally, we consider that the unsupervised auxiliary tasks\nare an interesting method, both for enhancing the learning\ncapabilities of the agent and for learning features of the trained\nsystem. As a result, we are conﬁdent that the divide and\nconquer method combined with the auxiliary tasks could solve\na variety of problems.\nREFERENCES\n[1] Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and\nGraves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Ried-\nmiller, Martin, “Playing atari with deep reinforcement learning,” In NIPS\nDeep Learning Workshop. 2013.\n[2] Mnih, Volodymyr, Adria Puigdomenech Badia, Mehdi Mirza, Alex\nGraves, Timothy Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. “Asynchronous methods for deep reinforcement learning.”\nIn International Conference on Machine Learning, pp. 1928-1937. 2016.\n[3] Konda, Vijay R., and John N. Tsitsiklis. ”Actor-critic algorithms.” In\nAdvances in neural information processing systems, pp. 1008-1014.\n2000.\n[4] Jaderberg, Max, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom\nSchaul, Joel Z. Leibo, David Silver, and Koray Kavukcuoglu. ”Rein-\nforcement learning with unsupervised auxiliary tasks.” arXiv preprint\narXiv:1611.05397 (2016).\n[5] Wu, Yuxin, and Yuandong Tian. ”Training agent for ﬁrst-person shooter\ngame with actor-critic curriculum learning.” (2016).\n[6] Peng, Jing, and Ronald J. Williams. ”Incremental multi-step Q-learning.”\nIn Machine Learning Proceedings 1994, pp. 226-232. 1994.\n[7] Lample, Guillaume, and Devendra Singh Chaplot. ”Playing FPS Games\nwith Deep Reinforcement Learning.” In AAAI, pp. 2140-2146. 2017.\n[8] Kempka, Micha, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and\nWojciech Jakowski. ”Vizdoom: A doom-based ai research platform\nfor visual reinforcement learning.” In Computational Intelligence and\nGames (CIG), 2016 IEEE Conference on, pp. 1-8. IEEE, 2016.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-07-05",
  "updated": "2018-07-05"
}