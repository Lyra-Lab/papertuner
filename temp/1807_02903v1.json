{
  "id": "http://arxiv.org/abs/1807.02903v1",
  "title": "Predicting Concreteness and Imageability of Words Within and Across Languages via Word Embeddings",
  "authors": [
    "Nikola Ljubešić",
    "Darja Fišer",
    "Anita Peti-Stantić"
  ],
  "abstract": "The notions of concreteness and imageability, traditionally important in\npsycholinguistics, are gaining significance in semantic-oriented natural\nlanguage processing tasks. In this paper we investigate the predictability of\nthese two concepts via supervised learning, using word embeddings as\nexplanatory variables. We perform predictions both within and across languages\nby exploiting collections of cross-lingual embeddings aligned to a single\nvector space. We show that the notions of concreteness and imageability are\nhighly predictable both within and across languages, with a moderate loss of up\nto 20% in correlation when predicting across languages. We further show that\nthe cross-lingual transfer via word embeddings is more efficient than the\nsimple transfer via bilingual dictionaries.",
  "text": "Predicting Concreteness and Imageability of Words\nWithin and Across Languages via Word Embeddings\nNikola Ljubeˇsi´c\nDept. of Knowledge Technologies\nJoˇzef Stefan Institute\nJamova cesta 39, SI-1000 Ljubljana\nnikola.ljubesic@ijs.si\nDarja Fiˇser\nDept. of Translation, Faculty of Arts\nUniversity of Ljubljana\nAˇskerˇceva 2, SI-1000 Ljubljana\ndarja.fiser@ff.uni-lj.si\nAnita Peti-Stanti´c\nFaculty of Humanities and Social Sciences\nUniversity of Zagreb\nIvana Luˇci´ca 3, HR-10000 Zagreb\nanita.peti-stantic@ffzg.hr\nAbstract\nThe notions of concreteness and image-\nability,\ntraditionally important in psy-\ncholinguistics, are gaining signiﬁcance in\nsemantic-oriented natural language pro-\ncessing tasks. In this paper we investigate\nthe predictability of these two concepts via\nsupervised learning, using word embed-\ndings as explanatory variables. We per-\nform predictions both within and across\nlanguages by exploiting collections of\ncross-lingual embeddings aligned to a sin-\ngle vector space. We show that the no-\ntions of concreteness and imageability are\nhighly predictable both within and across\nlanguages, with a moderate loss of up to\n20% in correlation when predicting across\nlanguages. We further show that the cross-\nlingual transfer via word embeddings is\nmore efﬁcient than the simple transfer via\nbilingual dictionaries.\n1\nIntroduction\nConcreteness and imageability are very important\nnotions in psycholinguistic research, building on\nthe theory of the double, verbal and non-verbal,\nmodality of representation of concrete words in\nthe mental lexicon, contrasted to single verbal rep-\nresentation of abstract words (Paivio, 1975, 2010).\nAlthough often correlated with concreteness, im-\nageability is not a redundant property. While most\nabstract things are hard to visualize, some call up\nimages, e.g., torture calls up an emotional and\neven visual image. There are concrete things that\nare hard to visualize too, for example, abbey is\nharder to visualize than banana (Tsvetkov et al.,\n2014).\nBoth notions have proven to be useful in com-\nputational linguistics as well. Turney et al. (2011)\npresent a supervised model that exploits con-\ncreteness to correctly classify 79% of adjective-\nnoun pairs as having literal or non-literal mean-\ning. Tsvetkov et al. (2014) exploit both the no-\ntions of concreteness and imageability to per-\nform metaphor detection on subject-verb-object\nand adjective-noun relations, correctly classifying\n82% and 86% instances, respectively.\nThe aim of this paper is to investigate the pre-\ndictability of concreteness and imageability within\na language, as well as across languages, by ex-\nploiting cross-lingual word embeddings as our\navailable signal.\n2\nRelated Work\nWhile much work has been done on exploiting\nword embeddings in expanding sentiment lexicons\n(Tang et al., 2014; Amir et al., 2015; Hamilton\net al., 2016), there is little work on predicting other\nlexical variables, concreteness and imageability\nincluded.\nTsvetkov et al. (2014) performed metaphor de-\ntection, using, among others, concreteness and im-\nageability as their features.\nTo propagate these\nfeatures, obtained from the MRC psycholinguis-\ntic database (Wilson, 1988) to the entire lexicon,\nthey used a supervised learning algorithm on vec-\ntor space representations, where each vector ele-\nment represented a feature. Performance of these\nclassiﬁers was 0.94 for concreteness and 0.85 for\nimageability. They also applied the concreteness\nand imageability features to other languages by\nprojecting features with bilingual dictionaries.\nBroadwell et al. (2013) extended imageability\nscores to the whole lexicon by using the MRC\narXiv:1807.02903v1  [cs.CL]  9 Jul 2018\nimageability scores and hyponym and hyperonym\nlinks from WordNet.\nRothe et al. (2016) trained an orthogonal trans-\nformation to reorder word embedding dimensions\ninto one-dimensional ultradense subspaces, the\noutput thereby being a lexicon. They trained the\ntransformations for sentiment, concreteness and\nfrequency.\nFor obtaining training data for con-\ncreteness, they used the BWK database (Brysbaert\net al., 2014). They showed that concreteness and\nsentiment can be better extracted from embedding\nspaces than frequency, with a Kendall τ correla-\ntion coefﬁcient of 0.623 for concreteness. Rothe\nand Sch¨utze (2016) further exploited this method\nto perform operations over the extracted dimen-\nsions, such as given a concrete word like friend,\nﬁnd the related, but abstract word friendship.\nContributions\nIn this paper we perform a sys-\ntematic investigation of transfer of two lexical no-\ntions, concreteness and imageability, (1) to the re-\nmainder of the lexicon not covered in an annota-\ntion campaign, and (2) to other languages.\nWhile there were already successful transfers\nwithin a language based on word embeddings\n(Tsvetkov et al., 2014; Rothe and Sch¨utze, 2016),\nthe only cross-lingual transfer was based on trans-\nfer via bilingual dictionaries (Tsvetkov et al.,\n2014). In this paper we compare the effectiveness\nof cross-lingual transfer via word embeddings and\nvia bilingual dictionaries.\nA byproduct of this research is a lexical re-\nsource in 77 languages containing per-word esti-\nmates for concreteness and imageability.\n3\nData\n3.1\nLexicons\nIn our experiments we use two existing English\nand one Croatian lexicon with concreteness and\nimageability ratings.\nFor English we use the MRC database (Wilson,\n1988) (MRC onwards), consisting of 4,293 words\nwith ratings for concreteness and imageability.\nThe ratings range from 100 to 700 and were ob-\ntained by merging three different resources (Wil-\nson, 1988).\nWe also use the BWK database consisting of\n39,954 English words (Brysbaert et al., 2014)\n(BWK onwards) with concreteness ratings summa-\nrized through arithmetic mean and standard devi-\nation. The ratings were collected in a crowdsourc-\ning campaign in which each word was labeled by\n20 annotators on a 1–5 scale.\nFor Croatian we use the MEGAHR database\n(MEGA onwards), consisting of 3,000 words, with\nconcreteness and imageability ratings summarized\nthrough arithmetic mean and standard deviation.\nThe ratings were collected in an annotation cam-\npaign among university students, with each word\nobtaining 30 annotations per variable on a 1–5\nscale.\nFor performing cross-lingual transfer via a dic-\ntionary, we use data from a large popular online\nCroatian-English dictionary1 containing around\n100 thousand entries.\n3.2\nEmbeddings\nFor both in-language and cross-lingual experi-\nments we use the aligned Facebook collection of\nembeddings2, trained with fastText (Bojanowski\net al., 2016) on Wikipedia dumps, with embed-\nding spaces aligned between languages with a lin-\near transformation learned via SVD (Smith et al.,\n2017) on a bilingual dictionary of 500 out of the\n1000 most frequent English words, obtained via\nthe Google Translate API3.\nWe also experimented with another cross-\nlingual embedding collection (Conneau et al.,\n2017), obtaining similar results and backing all\nour conclusions. This is in line with recent work\non comparing cross-lingual embedding models\nwhich suggests that the actual choice of monolin-\ngual and bilingual signal is more important for the\nﬁnal model performance than the actual underly-\ning architecture (Levy et al., 2017; Ruder et al.,\n2017). Given that one of our goals is to trans-\nfer concreteness and imageability annotations to\nas many languages as possible, using cross-lingual\nword embeddings based on Wikipedia dumps and\ndictionaries obtained through a translation API is\nthe most plausible option.\n4\nExperiments\n4.1\nSetup\nWe perform two sets of experiments: one within\neach language, and another across languages.\n1http://www.taktikanova.hr/eh/\n2https://github.com/facebookresearch/\nfastText/blob/master/pretrained-vectors.\nmd\n3https://github.com/Babylonpartners/\nfastText_multilingual\nWhile in-language experiments are always\nbased on supervised learning, in cross-lingual ex-\nperiments we compare two transfer approaches:\none based on a simple dictionary transfer, and\nanother on supervised learning on the word em-\nbeddings in the source language, and perform-\ning predictions on word embeddings in the target\nlanguage, with the two embedding spaces being\naligned.\nWe perform our prediction experiments by\ntraining SVM regression models (SVR) and deep\nfeedforward neural networks (FFN) over standard-\nized (zero mean, unit variance) embeddings and\neach speciﬁc response variable. We experiment\nwith all available gold annotations as our response\nvariables, namely both the arithmetic mean and\nstandard deviation of concreteness and imageabil-\nity.\nWe tuned the hyperparameters of each of the re-\ngressors on a subset of the Croatian, MEGA dataset\nin the case of the in-language experiments, and\nanother subset of the BWK dataset for the cross-\nlingual experiments. Given that we perform the\nﬁnal experiments on the whole datasets, and that\nwe have two additional English datasets at our dis-\nposal for the in-language experiments and three\nadditional dataset pairs for the cross-lingual exper-\niments, we consider our approach to be resistant to\nthe overﬁtting of the hyperparameters going unno-\nticed.\nWhile the SVR proved to work well with the\nRBF kernel, the C hyperparameter of 1.0 and the γ\nhyperparameter of 0.003, the feedforward network\nobtained strong results with two fully-connected\nhidden layers, consisting of 128 and 32 units each\nand ReLU activation functions, with a dropout\nlayer after each of the hidden layers, and an out-\nput layer with a linear activation function. We op-\ntimized for the mean squared error loss function\nand ran 50 epochs on each of the datasets, with a\nbatch size of 32.\nWhile we used the same regressor setup for\nthe SVR system for both the in-language and\ncross-lingual experiments, for the FFN system\nthe dropout probability in the in-language exper-\niments was 0.5, while in the cross-lingual setting\nthe dropout probability was set to 0.8, obtaining\nthereby a more general model which transfers bet-\nter to the other language.\nWe perform in-language experiments via 3-fold\ncross-validation, while we train models on our\nsource language dataset and evaluate the models\non our target language dataset for cross-lingual\nexperiments. We evaluate each approach via the\nSpearman rank and Pearson linear correlation co-\nefﬁcients.\nIn the paper we report the Spear-\nman correlation coefﬁcient only as the relation-\nships across both metrics in all the experiments\nare identical. We perform our experiments with\nthe scikit-learn (Pedregosa et al., 2011) and\nkeras (Chollet et al., 2015) toolkits.\n4.2\nIn-language Experiments\nWe start our experiments in the in-language set-\nting, running cross-validation experiments over\neach of our three datasets on all available vari-\nables.\nThe results of these experiments, with\nsome basic information on the size of the datasets,\nare given in Table 1. Aside from the three lex-\nicons introduced in Section 3.1, we experiment\nwith another lexicon, BWK.3K, which is a ran-\ndomly downsampled version of the BWK lexicon\nto the size of the two remaining lexicons. We in-\ntroduce this additional resource (1) to control for\ndataset size when comparing results on our differ-\nent datasets and (2) to measure the impact of train-\ning data size by comparing the results on the two\nﬂavours of the BWK dataset.\nThe results in Table 1 show that the support vec-\ntor regressor consistently performs better than the\nfeedforward neural network at predicting almost\nall values, with relative error reduction lying be-\ntween 7% and 12%. The bold results are statis-\ntically signiﬁcantly better than the corresponding\nnon-bold ones given the approximate randomiza-\ntion test (Edgington, 1969) with p < 0.05. Our as-\nsumption is that the stronger FFN model does not\nshow a positive impact primarily due to the small\nsize of the datasets and the simplicity of the mod-\neling problem.\nWe can further observe that the arithmetic mean\nis much easier to predict than standard deviation\non both variables in all the datasets. This can be\nexplained by the fact that standard deviation on the\ntwo phenomena can partially be explained with the\nlevel of ambiguity of a speciﬁc word, and this type\nof information is at least not directly available in\ncontext-based word embeddings.\nFurthermore, imageability seems to be consis-\ntently slightly harder to predict than concreteness.\nOur initial assumption regarding this difference\nwas that imageability is a more vague notion for\ndataset\nMEGA\nBWK\nBWK.3K\nMRC\nlang\nhr\nen\nen\nen\nsize\n2,682\n22,797\n3,000\n4,061\nmethod\nSVR\nFFN\nSVR\nFFN\nSVR\nFFN\nSVR\nFFN\nC.M\n0.760\n0.742\n0.887\n0.879\n0.848\n0.834\n0.872\n0.863\nC.STD\n0.265\n0.274\n0.484\n0.461\n0.376\n0.364\n-\n-\nI.M\n0.645\n0.602\n-\n-\n-\n-\n0.803\n0.787\nI.STD\n0.439\n0.415\n-\n-\n-\n-\n-\n-\nTable 1: Results of the in-language experiments on predicting mean (.M) and standard deviation (.STD)\nof concreteness (C) and imageability (I), either using a support vector regressor (SVR) or feed-forward\nnetwork (FFN). Evaluation metric is the Spearman correlation coefﬁcient.\nhuman subjects, and therefore their responses are\nmore dispersed, adding to the complexity of the\nprediction.\nHowever, analyzing standard devia-\ntions over concreteness and imageability showed\nthat these are rather the same. We leave this open\nquestion for future research.\nWhen comparing the results on predicting mean\nconcreteness on the full BWK and the trimmed\nBWK.3K datasets, we see a signiﬁcant improve-\nment of the predictions of the on the larger dataset,\nshowing that having 10 times more data for learn-\ning can produce signiﬁcant improvements in the\nprediction quality.\n4.3\nCross-lingual Experiments\nIn cross-lingual experiments we compare our two\napproaches to cross-lingual transfer: dictionary\nlookup (DIC onwards) and supervised learning on\naligned word embedding spaces via the two meth-\nods introduced in Section 4.2, SVR and FFN.\nThe DIC method simply looks up for each word\nin the source language resource all possible trans-\nlations to the target language and directly trans-\nfers the concreteness and imageability ratings to\nthe target language words. In case of collisions\nin the target language (two source language words\nbeing translated to the same word in the target lan-\nguage), we perform averaging over the transfered\nratings. In our experiments, the arithmetic mean\nshowed to be a better averaging method than the\nmedian, we therefore report the results on that av-\neraging method.\nThe SVR and FFN methods use supervised learn-\ning in a very similar fashion to the in-language\nexperiments described in Section 4.2. We train a\nsupervised regression model on the whole source\nlanguage dataset, using word embedding dimen-\nsions as features and the variable of choice as\nour target.\nWe obtain estimates of our vari-\nable of choice in the target language by applying\nthe source-language model on the target-language\nword embeddings since the two embedding spaces\nare aligned.\nFor both approaches we compare the target-\nlanguage estimates with the gold data available\nfrom our lexicons.\nWe present the results of the cross-lingual ex-\nperiments in Table 2. Our ﬁrst observation is that,\nwhile in the in-language setting the SVR method\nhas regularly outperformed the FFN method, in the\ncross-lingual setting this is not the case any more,\nwith SVR and FFN obtaining very similar results,\nin ﬁve out of six cases in the range of no statis-\ntically signiﬁcant difference. Our explanation for\nthe loss of the positive impact in using the weaker,\nsupport vector regression model, is that with the\nnoisy alignment of the two embedding spaces the\nprediction problem became harder, now both mod-\nels performing similarly. While the strong point of\nSVR is that it performs very well on small datasets,\nthe strong point of the FFN method is that it gener-\nalizes better.\nThat higher generalization is beneﬁcial in case\nof the cross-lingual problem is observable in the\ndifference in the hyperparameter tuning results on\nthe FFN method, where in the in-language setting\nthe optimal dropout was 0.5, while in the cross-\nlingual setting it is 0.8.\nOur second observation is that all the predicted\nratings suffer in the cross-lingual setting, when\ncompared to the in-language results presented in\nTable 1, observing for the SVR method a drop of\naround 5 to 15%. While standard deviation was\nalready poorly predicted in the in-language set-\nsource\nMEGA (hr)\nBWK (en)\nMEGA (hr)\nMRC (en)\ntarget\nBWK (en)\nMEGA (hr)\nMRC (en)\nMEGA (hr)\nSVR\nFFN\nDIC\nSVR\nFFN\nDIC\nSVR\nFFN\nDIC\nSVR\nFFN\nDIC\nC.M\n0.791\n0.793\n0.728\n0.724\n0.719\n0.641\n0.797\n0.794\n0.611\n0.651\n0.644\n0.638\nC.STD\n0.178\n0.141\n0.224\n0.185\n0.145\n0.137\n-\n-\n-\n-\nI.M\n-\n-\n-\n-\n-\n-\n0.694\n0.683\n0.523\n0.548\n0.531\n0.503\nTable 2: Results of the cross-lingual experiments, either using supervised learning (SVR, FFN), or simple\ndictionary lookup (DIC). Evaluation metric is the Spearman correlation coefﬁcient. Results in bold are\nbest results per problem with no statistically signiﬁcant difference.\nting, in the cross-lingual setting it drops even fur-\nther to a non-useful level, below 0.2. This is the\nreason why we do not calculate statistical signiﬁ-\ncance of the differences in these results and do not\ninclude their estimates in our ﬁnal 77-languages-\nstrong resource. In the ﬁnal cross-lingual resource\nwe include only the mean of concreteness and\nimageability, the notions for which we have ob-\ntained strong correlation in our cross-lingual ex-\nperiments.\nFinally,\nwhen comparing the cross-lingual\ntransfer via embeddings (SVR and FFN) and via a\ndictionary (DIC), the learning-on-embeddings ap-\nproach outperforms the dictionary method in each\ninstance, with the relative loss in correlation when\nmoving from the embedding to the dictionary ap-\nproach of 5% to 25%.\n4.4\nRegressor Coefﬁcient Analysis\nOur ﬁnal analysis concerns the question of how\nmany of the embedding dimensions are crucial for\nour regressors to predict the notions of concrete-\nness and imageability. We consider two potential\nscenarios: (1) each of the notions are encoded in\none or a few of the embedding dimensions and (2)\nthe notions are encoded in many embedding di-\nmensions.\nThe analysis is performed by calculating the\ncumulative distribution of absolute and normal-\nized (sum to 1), reversely sorted coefﬁcients of the\nSVM regressor with a linear kernel. For both phe-\nnomena, concreteness and imageability, the dis-\ntributions show that the predictions are based on\na signiﬁcant number of embedding dimensions.\nNamely, while 80 most informative dimensions\ncover 50% of the coefﬁcients’ mass, half of the\ndimensions (150) cover 80% of that mass. This\nshows for the second scenario – concreteness and\nimageability are encoded in a signiﬁcant number\nof embedding dimensions – to be true.\n5\nConclusion\nIn this paper we have shown that concreteness and\nimageability ratings can be successfully transfered\nboth to non-covered portions of the lexicon and to\nother languages via (cross-lingual) word embed-\ndings.\nWith the in-language experiments we have\nshown that the arithmetic mean of both notions is\nmuch easier to predict than their standard devia-\ntion, the latter probably encoding word ambiguity,\ntype of information not directly present in word\nembeddings.\nOur experiments across languages have shown\nthat the loss in comparison to in-language ex-\nperiments on predicting the means of both con-\ncreteness and imageability are around 15%, a rea-\nsonable price to pay given the applicability of\nthe method to all of the 77 languages present\nin the word embedding collection.\nThe predic-\ntions of concreteness and imageabililty obtained in\nthe 77 languages are available at http://hdl.\nhandle.net/11356/1187.4\nComparing the two methods of transfer – dic-\ntionary vs. cross-lingual embeddings, shows regu-\nlarly better (5%–15%) results of the latter, proving\nonce more the usefulness of word embeddings, es-\npecially in the currently expanding cross-lingual\nsetup.\nFinally, while the stronger deep neural model\nshows worse results than the support vector regres-\nsor in the in-language setting, mainly because of\nthe small size of the training datasets, in the cross-\nlingual setting they both show identical perfor-\nmance due to the problem becoming harder given\nthe noise from the embedding alignment process.\n4Ongoing\ndevelopments\nare\nstored\nat\nhttps://\ngithub.com/clarinsi/megahr-crossling/.\nAcknowledgements\nThe work described in this paper has been funded\nby the Croatian National Foundation project\nHRZZ-IP-2016-06-1210, the Slovenian Research\nAgency project ARRS J7-8280, and by the Slove-\nnian research infrastructure CLARIN.SI.\nReferences\nSilvio Amir, Ram´on Astudillo, Wang Ling, Bruno\nMartins, Mario J Silva, and Isabel Trancoso. 2015.\nInesc-id: A regression model for large scale twitter\nsentiment lexicon induction. In Proceedings of the\n9th International Workshop on Semantic Evaluation\n(SemEval 2015). pages 613–618.\nPiotr Bojanowski, Edouard Grave, Armand Joulin,\nand Tomas Mikolov. 2016.\nEnriching word vec-\ntors with subword information.\narXiv preprint\narXiv:1607.04606 .\nGeorge Aaron Broadwell, Umit Boz, Ignacio Cases,\nTomek Strzalkowski, Laurie Feldman, Sarah Tay-\nlor,\nSamira Shaikh,\nTing Liu,\nKit Cho,\nand\nNick Webb. 2013.\nUsing imageability and topic\nchaining to locate metaphors in linguistic cor-\npora. In Ariel M. Greenberg, William G. Kennedy,\nand Nathan D. Bos,\neditors,\nSocial Comput-\ning, Behavioral-Cultural Modeling and Prediction.\nSpringer Berlin Heidelberg, Berlin, Heidelberg,\npages 102–110.\nMarc Brysbaert, AB Warriner, and V Kuperman.\n2014. Concreteness ratings for 40 thousand gener-\nally known english word lemmas. BEHAVIOR RE-\nSEARCH METHODS 46(3):904–911.\nFranc¸ois Chollet et al. 2015.\nKeras.\nhttps://\nkeras.io.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2017.\nWord translation without parallel data.\narXiv\npreprint arXiv:1710.04087 .\nEugene\nS.\nEdgington.\n1969.\nApproxi-\nmate\nrandomization\ntests.\nThe\nJour-\nnal\nof\nPsychology\n72(2):143–149.\nhttps://doi.org/10.1080/00223980.1969.10543491.\nWilliam L. Hamilton, Kevin Clark, Jure Leskovec, and\nDan Jurafsky. 2016. Inducing domain-speciﬁc sen-\ntiment lexicons from unlabeled corpora.\nIn Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016. pages\n595–605. http://aclweb.org/anthology/D/D16/D16-\n1057.pdf.\nOmer Levy, Anders Søgaard, and Yoav Goldberg.\n2017. A strong baseline for learning cross-lingual\nword embeddings from sentence alignments.\nIn\nProceedings of the 15th Conference of the Eu-\nropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers. Associa-\ntion for Computational Linguistics, pages 765–774.\nhttp://aclweb.org/anthology/E17-1072.\nA. Paivio. 1975. Coding Distinctions and Repetition\nEffects in Memory. Research bulletin. Department\nof Psychology, University of Western Ontario.\nAllan Paivio. 2010. Dual coding theory and the mental\nlexicon. The Mental Lexicon 5(2):205–230.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-\nsos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. 2011. Scikit-learn: Machine learning\nin Python. Journal of Machine Learning Research\n12:2825–2830.\nSascha Rothe, Sebastian Ebert, and Hinrich Sch¨utze.\n2016. Ultradense word embeddings by orthogonal\ntransformation. CoRR abs/1602.07572.\nSascha Rothe and Hinrich Sch¨utze. 2016.\nWord\nembedding calculus in meaningful ultradense sub-\nspaces.\nIn Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers). volume 2, pages 512–517.\nSebastian\nRuder,\nIvan\nVuli´c,\nand\nAnders\nSøgaard.\n2017.\nA\nsurvey\nof\ncross-lingual\nembedding\nmodels.\nCoRR\nabs/1706.04902.\nhttp://arxiv.org/abs/1706.04902.\nSamuel L. Smith, David H. P. Turban, Steven Ham-\nblin, and Nils Y. Hammerla. 2017.\nOfﬂine bilin-\ngual word vectors, orthogonal transformations and\nthe inverted softmax.\nCoRR abs/1702.03859.\nhttp://arxiv.org/abs/1702.03859.\nDuyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting\nLiu. 2014. Building large-scale twitter-speciﬁc sen-\ntiment lexicon: A representation learning approach.\nIn Proceedings of COLING 2014, the 25th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers. pages 172–182.\nYulia Tsvetkov, Leonid Boytsov, Anatole Gershman,\nEric Nyberg, and Chris Dyer. 2014. Metaphor de-\ntection with cross-lingual model transfer. In ACL.\nPeter D. Turney, Yair Neuman, Dan Assaf, and Yohai\nCohen. 2011. Literal and metaphorical sense iden-\ntiﬁcation through concrete and abstract context. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing. Association\nfor Computational Linguistics, Stroudsburg, PA,\nUSA, EMNLP ’11, pages 680–690.\nMichael Wilson. 1988. Mrc psycholinguistic database:\nMachine-usable dictionary, version 2.00.\nBehav-\nior Research Methods, Instruments, & Computers\n20(1):6–10.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-07-09",
  "updated": "2018-07-09"
}