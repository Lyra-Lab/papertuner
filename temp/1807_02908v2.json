{
  "id": "http://arxiv.org/abs/1807.02908v2",
  "title": "Partial Policy-based Reinforcement Learning for Anatomical Landmark Localization in 3D Medical Images",
  "authors": [
    "Walid Abdullah Al",
    "Il Dong Yun"
  ],
  "abstract": "Deploying the idea of long-term cumulative return, reinforcement learning has\nshown remarkable performance in various fields. We propose a formulation of the\nlandmark localization in 3D medical images as a reinforcement learning problem.\nWhereas value-based methods have been widely used to solve similar problems, we\nadopt an actor-critic based direct policy search method framed in a temporal\ndifference learning approach. Successful behavior learning is challenging in\nlarge state and/or action spaces, requiring many trials. We introduce a partial\npolicy-based reinforcement learning to enable solving the large problem of\nlocalization by learning the optimal policy on smaller partial domains.\nIndependent actors efficiently learn the corresponding partial policies, each\nutilizing their own independent critic. The proposed policy reconstruction from\nthe partial policies ensures a robust and efficient localization utilizing the\nsub-agents solving simple binary decision problems in their corresponding\npartial action spaces. The proposed reinforcement learning requires a small\nnumber of trials to learn the optimal behavior compared with the original\nbehavior learning scheme.",
  "text": "1\nPartial Policy-based Reinforcement Learning for Anatomical\nLandmark Localization in 3D Medical Images\nWalid Abdullah Al, Il Dong Yun* (IEEE Member)\nDepartment of Computer and Electronic Systems Engineering,\nHankuk University of Foreign Studies, Yongin, South Korea\nUtilizing the idea of long-term cumulative return, reinforce-\nment learning (RL) has shown remarkable performance in vari-\nous ﬁelds. We propose a formulation of the landmark localization\nin 3D medical images as a reinforcement learning problem.\nWhereas value-based methods have been widely used to solve RL-\nbased localization problems, we adopt an actor-critic based direct\npolicy search method framed in a temporal difference learning\napproach. In RL problems with large state and/or action spaces,\nlearning the optimal behavior is challenging and requires many\ntrials. To improve the learning, we introduce a partial policy-\nbased reinforcement learning to enable solving the large problem\nof localization by learning the optimal policy on smaller partial\ndomains. Independent actors efﬁciently learn the corresponding\npartial policies, each utilizing their own independent critic. The\nproposed policy reconstruction from the partial policies ensures\na robust and efﬁcient localization utilizing the sub-agents solving\nsimple binary decision problems in their corresponding partial\naction spaces. Experiments with three different localization\nproblems in 3D CT and MR images showed that the proposed\nreinforcement learning requires a signiﬁcantly smaller number\nof trials to learn the optimal behavior compared to the original\nbehavior learning scheme in RL. It also ensures a satisfactory\nperformance when trained on a fewer images.\nIndex Terms—Actor-critic, landmark localization, medical im-\nage, partial policy, reinforcement learning\nI. INTRODUCTION\nL\nANDMARK localization plays a vital role in medical\nimage analysis, facilitating the automatic process for\nregistration, classiﬁcation, and segmentation [1], [2]. Besides\nspeeding up the interpretation, it contributes to visualization\nand assessment-based applications. However, accurate land-\nmark localization in 3D medical images is a challenging\nproblem because of high inter-patient variations in terms\nof size, shape, and orientation, as well as the variations\nand artifacts caused by different parameter settings. Machine\nlearning approaches are becoming more and more common to\nsolve the localization problem under such variation. Standard\napproaches suggest classiﬁcation or regression-based model in\norder to localize the landmarks. However, all of the previous\nlearning approaches are mainly exploitative and may behave\ninconsistently for an exceptional test data. Long-term reward-\noriented reinforcement learning (RL) algorithms offer ways\nto balance between exploration and exploitation, yielding a\nnoteworthy performance in various ﬁelds of image processing\n[3], [4], [5]. With a few instances of implementation in object\n*Email: yun@hufs.ac.kr\nlocalization in terms of a bounding box, RL-enforced landmark\nlocalization is rarely found.\nValue function approximation (e.g., deep Q-Network (DQN)\n[3]) is a widely used method to solve the RL problem\nfor large state and/or action spaces, suggesting an indirect\nbehavior learning. Compared to such value-based methods,\nexplicit behavior learning by directly approximating the pol-\nicy function [6] has the advantage of a better convergence.\nHowever, direct policy search method suffers from the high\nvariance problem [7]. Actor-critic RL performs direct policy\napproximation while utilizing an additional value function\napproximator to reduce the variance, thus taking advantage\nof both the policy and value-based methods [8]. Nevertheless,\na good exploration in order to obtain the optimal policy in\na large space is challenging. Despite remarkable propositions\nand improvement to maintain the balance between exploration\nand exploitation, RL practically faces problem to successfully\nlearn a task in a large state and/or action space and requires\nmany trials [9].\nIn this paper, we formulate the landmark localization prob-\nlem as a sequential decision-making problem in RL, where an\nagent initiated at a random position inside a 3D medical image\n(i.e., volume) observes the current state and takes subsequent\nactions to move towards the target landmark. We suggest\nlearning the policy function directly using an actor-critic\napproach because of its advantage over pure policy or value-\nbased approach. To ensure a successful behavior learning\nwithin a signiﬁcantly smaller number of trials, we introduce\na partial policy-based reinforcement learning model where\nmultiple sub-agents learn assigned micro-tasks to successfully\nlearn the original task. Partial policies with respect to the\nmicro-tasks are obtained by projecting the original policy onto\nsmaller sub-action spaces, enabling a disintegration of the\ncomplex decision problem into a set of simpler problems. We\nallow independent actors to update the corresponding partial\npolicy functions each utilizing its own value function (i.e.,\ncritic). Fig. 1 shows a schematic illustration of the localization\nprocess using the partial policies for a 2D case.\nII. RELATED WORK\nPrior research on landmark localization in both medical\nimage and computer vision concentrated on the model-based\nmethods where a major focus was on the classiﬁcation-based\napproaches. For example, Shotton et al. [10] utilized a trained\nforest to perform per-pixel classiﬁcation to recognize body\narXiv:1807.02908v2  [cs.CV]  31 Dec 2018\n2\n𝜋𝑥\n𝑠𝑡\n𝑠𝑡+1\n𝑠𝑡+2\n𝑎𝑥,𝑡\nPartial policies\n𝑎𝑦,𝑡+1\n𝑎𝑥,𝑡+2\n𝑠𝑡\n𝑎𝑥,𝑡\n𝜋𝑦\n𝑠𝑡+1\n𝑎𝑦,𝑡\n𝑎𝑖,𝑡~ 𝜋𝑖𝑠𝑡, 𝑎𝑖,𝑡,\n𝑖∈{𝑥, 𝑦}\nFig. 1. Landmark localization process using the partial policies. A 2D case\nis used for illustration purposes. An agent initiated at any position at time t,\nobserves the corresponding state st and decides an action to move towards the\ntarget landmark. Instead of a single RL agent with a single policy π, multiple\nsub-agents with simpler partial policies πx and πy repeatedly contribute to\nthe successive state transitions. ai,t refers to the partial action at t, sampled\nfrom the partial policy distribution πi on the i-th sub-action space, for the\ncurrent state st.\nparts as an intermediary step required for localizing body-\njoints in depth images. Zheng et al. [11] proposed a marginal\nspace learning approach to localize aortic valve landmarks\nhierarchically, where a rough location is ﬁrst obtained from a\ndetected global object consisting of the landmarks, followed by\na reﬁnement session using local boosting tree based classiﬁer.\nA generalized Procrustes analysis (GPA) was used to ﬁnd\nthe optimal global object. However, GPA does not guarantee\nthe convergence of means [12]. Ionasec et al. [13] utilized\na similar boosting tree classiﬁer to localize the aortic valve\nlandmarks. A forest classiﬁer based method similar to [10]\nwas also proposed for hand-joint localization in X-ray images.\nNevertheless, classiﬁcation approaches suffer from dataset\nimbalance problem because of the negligible positive samples,\nresulting in a biased classiﬁer.\nRegression-based approaches are becoming more and more\npopular instead of formulating the localization task as a clas-\nsiﬁcation problem. Regression models also showed signiﬁcant\nimprovement comparing to the classiﬁcation models. These\nmodels suggest exploiting the predictions in different regions\nat a different distance from the target landmark. Criminisi et\nal. [14] proposed an efﬁcient method for anatomy localization\nusing a regression forest. Jung et al. [15] proposed the random\ntree walk (RTW) method for localizing human body joints in\ndepth images, where a regression tree is trained to estimate the\ndirection to the target joint for each position. A walker then\nstarts walking using the learned direction and the expectation\nof the stepped positions is considered as the resultant joint\nposition. An implication of RTW for localizing the aortic valve\nlandmarks can be found in our previous work [16], where\nwe performed a colonial walk initiating multiple random\nwalks from different initial positions. The successful walker\nfrom the colony was elected by the minimum walk variance\nmeasure. Some joint models combining both the regression\nand classiﬁcation approach also exist [17], [18]. Most recently,\na stratiﬁed method is introduced by Oktay et al. [19], where\nimage-patch driven local information as well as the global\ninformation in terms of organ-size, shape etc. are used for\ntraining.\nThere are a few approaches that do not fall under either\nof the above mentioned approaches. Simple connected com-\nponent analysis [20]and coronary centerline tracking [21] is\nused for coronary ostia detection after aorta segmentation.\nHowever, robustness is challenged under image noise in case\nof connected component analysis, while centerline tracking\nalgorithm has a high computational cost when operated for the\nwhole surface of aorta. Elattar et al. [22] detected coronary\nostia and aortic hinges on the aortic root surface, which is\nsegmented using connected component analysis.\nMost of the previous learning-based approaches are gen-\nerally exploitative and may face generalization problem. RL\nprovides an explorative learning scheme, which has shown re-\nmarkable improvements in various ﬁelds of image processing.\nTo the best of our knowledge, only one instance of RL-based\nlandmark localization exists in the literature, whereas a few\ninstances of bounding box-based object localization in natural\nimages can be found. Caicedo et al. [23] presented the object\nlocalization problem as a sequential decision-making problem\nin RL, where the process starts with a bounding-box covering\nthe whole image, gradually applying transformation actions\nto the bounding box to ﬁnally localize the object. A better\nintersection-over-union (IoU) between the transformed box\nand the GT bounding box yields a positive reinforcement, and\nnegative otherwise. The state is related to the image or sub-\nimage inside the bounding box. Jie et al. [24] proposed a tree-\nstructured RL with similar state and action representation. At\neach state, the agent applies two different actions i.e., scaling\nand translation, yielding two resultant states. Thus, the agent\nfollows a recursive approach to ﬁnally ﬁnd the object, repre-\nsenting a binary tree-like search. Ghesu et al. [25] present the\nonly RL implementation in anatomical landmark localization\nin medical images, where the agent makes sequential position\nupdate actions to reach the landmark. The state is deﬁned to be\nthe region-of-interest (ROI) around the corresponding position.\nRelative distance change is used as the rewarding scheme.\nAll these approaches mainly implemented Q-learning [26] to\nlearn the action-value function while learning the optimal be-\nhavior indirectly. Direct behavior learning through optimizing\nthe policy function shows better convergence comparatively,\nwhile having the problem of high variance. The actor-critic\napproach also approximates the policy function, however, uses\nan additional value function as the critic to reduce the variance\n[8].\nOur work focuses on RL formulation for landmark localiza-\ntion in 3D medical images, where the agent action follows a\ndeﬁnition similar to [25]. Unlike the previous approaches, we\ndirectly approximate the policy function following the actor-\ncritic approach, where a state-value function is used as the\npolicy evaluator or the critic. Moreover, we propose to learn\nmultiple partial policies on different sub-action spaces instead\nof a single complex policy on the original action space, in\norder to improve the slow learning problem of RL and ensure\na more robust localization.\nThe rest of the paper is organized as follows. Section III de-\n3\nscribes the formulation of landmark localization for the actor-\ncritic RL. Section IV presents the partial policy-based RL\nfor localization. Section V reports the experimental evaluation\nof the proposed approach. Finally, Section VI presents the\nconcluding remarks.\nIII. LANDMARK LOCALIZATION AS RL\nIn our reinforcement learning-based localization scheme,\nan RL agent initiated at a random position interacts with\nthe volume by taking consecutive discrete actions sampled\nfrom the learned policy distribution for the observed state\nat the current position, to ﬁnally reach the target landmark.\nDuring training, the agent tries to attain an optimal policy that\nmaximizes the long-term return formulated as a discounted\ncumulative reward. The following is the description of the\nkey elements of the Markov decision process (MDP) in the\nRL-wrapped localization scheme.\nA. State\nWe represent the state as a function of the agent-position.\nFor any position q, the corresponding state s = S(q) refers to\na stack of the axial, coronal, and sagittal sub-images observed\nthrough a squared window centered at the corresponding\nposition. Thus, we allow the agent at any position to observe\nan m × m × 3 block of surrounding voxels. Here, m is the\nwindow size. Such state is useful to provide a pseudo-3D view\nbut requires less storage in the experience replay memory. This\nalso helps traverse the state through a usual 2D CNN (of the\npolicy and value networks), treating it as a 3-channeled image.\nB. Action\nSimilar to Ghesu et al.’s approach [25], a discrete ac-\ntion space is considered where agent can take a unit\nstep along either of the axes to update its position to\na neighbouring voxel. Therefore, the agent holds three\ndegrees of freedom to move, enabling six actions i.e.,\nright, left, up, down, slice forward, slice backward. The\nﬁrst four moves are along the axial slice (X and Y axes),\nwhereas, the last two actions allow the agent to jump across\nthe slices moving along Z-axis. We represent our action space\nas follows:\nA = {x+, x−, y+, y−, z+, z−}\n(1)\nwhere x+ , x−, y+, y−, z+ and z−represent right, left, up,\ndown, slice forward and slice backward, respectively.\nUsing these simple actions yields a rather simple and\ndeterministic transition. For a given position q and action a,\nthe transitioned position q′ can easily be obtained using the\nfollowing transition function:\nq′ = T (q, a) =\n\u0000qx + Ux(a), qy + Uy(a), qz + Uz(a)\n\u0001\nUi(a) =\n\n\n\n\n\nη,\nif a = i+\n−η,\nif a = i−\n0,\notherwise\ni ∈{x, y, z}\n(2)\nHere, η is the length of a unit-step. qx, qy, and qz are the com-\nponents of q along different axes. We denote such transition as\n(q, a, q′). There could be an additional action for no transition\nwhere agent remains at its current position without moving so\nthat we can know that it has reached its destination landmark.\nHowever, adding such action would make the optimal policy\nﬁnding harder because the state-action space (that should be\nexplored) would become larger. Moreover, comparing to other\nactions, this action can render positive reward only for one\nstate in the whole volume. Thus the model will suffered\nfrom sample selection bias and highly unlikely to trigger\nthis action. Finally, we can ensure a satisfactory localization\nby using only just the aforementioned six actions. Because,\neventually it would converge the target and move back and\nforth creating an oscillation of an amplitude of 1-2 voxels.\nThe ﬁnal localized position is the centroid of the oscillation,\nand may be approximated by taking the expectation of last\nfew steps.\nC. Reward\nThe agent at any position inside a 3D volume targets at\nchoosing an action that maximizes the discounted cumulative\nreward. Therefore, we should encourage the agent to come\ncloser to the target by giving an appropriate reward. We\npropose to use a simple binary reward function, where a\npositive reward is given if an action leads the agent closer to\ntarget landmark, and a negative reward is given otherwise. The\nreward is immediate after each action. The Euclidean distance\nmeasure is undertaken to assess the closeness. Hence, for a\ntransition (q, a, q′), we can represent our reward function as\nfollows:\nR(q, a, q′) = sign(dpq −dpq′)\ndab = ||a −b||2\n(3)\nwhere p is the target landmark position. Such binary reward is\nwidely used in reinforcement learning and useful for tracking\nthe progress. Even in the case of a continuous real-valued re-\nward deﬁnition, it is recommended to perform reward clipping,\nwhere all the positive and negative outcomes are labelled as\n+1 and -1, respectively.\nD. Policy and value function\nPolicy function outputs the optimal action-probabilities for\na given state, whereas value function outputs the expected\ncumulative return for a given state and/or a given action. We\nadopt a stochastic policy to map states to actions. Previous\nRL-based localization approaches focused on implicit policy\nlearning through training a value function approximator. We\nexploit a direct and explicit policy learning that comes under\nthe category of policy-based RL, performing direct parametri-\nsation of the policy function. In the proposed approach, a non-\nlinear policy function approximator represented by a multi-\nlayer perceptron (MLP) on top of a deep-CNN is used,\nembedding the high level feature learning from the raw state\ninside the policy learning. The parametrised policy function\ncan be represented as follows:\nπθ(q, a) = P(a|S(q), θ)\n(4)\n4\nwhere θ represents the weights of the deep policy network.\nDirect policy search methods have a better convergence\nproperty while inducing a high variance problem. In actor-\ncritic RL, an additional value function serves as a policy\nevaluator or critic to tackle the high variance problem. We use\nthe state-value function approximator that tries to evaluate the\npolicy, πθ, for the current policy parameters, θ. We represent\nthe value function approximator by another MLP stacked\non top of the same CNN from the policy net, trying to\napproximate the state-value function to the true state-value\n(i.e., expected cumulative return for a state) for a given policy,\nas expressed as follows:\nVω(q) ≈V πθ(S(q))\n(5)\nwhere ω refers to the network parameters of the value approx-\nimator network. Therefore, both the policy and value function\nshare the parameters of a common CNN while having their\nown exclusive MLP parameters. Fig. 3a presents the network\narchitectures of the policy and value function in an actor-critic\napproach.\nE. Learning\nUsing the aforementioned policy and value function approx-\nimator, actor-critic learning requires updating the parameters\nof both the policy (actor) and value (critic) networks. For\na given state, actor-update aims at improving the policy to\nensure a better cumulative return than the state-value inferred\nby the critic, whereas the critic aims at updating the value\nto approximate the cumulative return for the current policy.\nWe perform the actor-critic RL in the widely used temporal\ndiffernce (TD) learning framework [27]. We used the simplest\nlinear TD(0) approach, where TD-target and TD-error are\nrespectively calculated for a transition (q, a, q′) using the\nfollowing equations:\nτ(q, a, q′) = R(q, a, q′) + γVω(q′)\nε(q, a, q′) = τ(q, a, q′) −Vω(q)\n(6)\nThus, TD-target τ refers to the discounted cumulative return\nwith a discount factor γ using the current policy, and TD-\nerror ε refers to the advantage of the current policy over the\ncritic-inferred state-value. Our goal is to approximate τ by the\nparametrised value function and update the policy towards the\nadvantage. Hence, the cost functions for updating the value\nand policy parameters is stated as follows:\nJV (ω) = E(q,a,q′)\n\u0002\u0000τ(q, a, q′) −Vω(q)\n\u00012\u0003\nJπ(θ) = E(q,a,q′)\n\u0002\n−ε(q, a, q′) log πθ(q, a)\n\u0003\n(7)\nIn pure policy-based RL, expected return is used in the cost\nfunction of the policy network. The utilization of the advantage\nfunction formulated as TD-error in (6) serves as a better cost\nfunction, enabling a low variance in the policy approximation.\nIV. PARTIAL POLICY-BASED RL\nRL agent learns the optimal behavior from episodes of\nexperience gathered by interacting with the environment. In\nproblems with large state/action space, successful task learning\nis challenging as it requires a huge number of trials, becom-\ning a major drawback of RL. Despite the various methods\nof exploration, the issue still practically remains. Moreover,\navailability of multiple actions triggering a positive feedback\nfor a state makes the optimal policy learning harder. In the case\nof 3D localization, we have six actions along X,Y and Z axes.\nExcept for a negligible portion in the state space, a number of\nalternative actions can be found triggering a positive feedback\nfor most of the states of a volume. Consequently, learning the\ndecision between a pair of actions along one axis alone, can\nensure positive feedbacks for almost the entire state space.\nTherefore, actions along other axes does not get signiﬁcance\nto inﬂuence the policy update. The situation is illustrated\nin Fig. 2, where localization in a 2D slice is presented for\nsimplicity. The agent using such biased policy is most likely\nto generate only the actions along X-axis, failing to learn the\ntask.\nFig. 2.\nOptimal policy problem in presence of alternative actions\ntriggering positive feedback. The depicted policy ensure rewards in all states\nby deciding between the actions along X (horizontal) axis only, except for\nthe states lying on the red vertical line passing through the red dotted target\nlandmark.\nFor an efﬁcient and effective learning of the optimal policy,\nwe propose a partial policy-based learning approach. Instead\nof using one actor to update a large policy, we employ multiple\nmicro-actors to learn the optimal behavior in partial sub-action\nspaces. Thus, the decomposition of the large problem into\nsmaller problems enables an efﬁcient, successful, and easier\nlearning. Consequently, the proposed learning scheme can\nachieve the optimal policy within a fewer trials, compared to\nthe conventional deep RLs. In this section, we ﬁrst describe\nthe dissection of the master policy to obtain partial policies,\nfollowed by the reconstruction of the original policy from the\npartial policies. Finally, we present the TD learning-framed\nactor-critic algorithm using the partial policies.\nA. Partial policy\nThe objective of the partial policy is to obtain multiple\nsimple policies on the projections of the actual action space,\nwhere the projected policies are able to reconstruct the policy\non the original action space. We deﬁne smaller sub-action\nspaces i.e., partial action spaces, projecting the actual action\nspace onto different Cartesian axes. Such dissection of the\nactual space also suggests multiple sub-agents corresponding\n5\nConv 5 5, 32\nPool\nConv 5 5, 32\nPool\nConv 5 5, 64\nPool\nConv 5 5, 64\nPool\nState, s\nCNN\nPolicy net\nValue net\nState, s\nCNN\nPartial policy I\nPartial value I\nPartial policy II\nPartial value II\nPartial policy III\nPartial value III\n(c) CNN block\n(a) Architecture for the original actor-critic RL\n(b) Architecture for the proposed partial policy-based RL\nFC 96\nFC 6\nSoftmax\nFC 48\nFC 1\n(d) Policy net\n(e) Value net\n(f) Partial policy unit (g) Partial value unit\n𝜋𝜃𝑠, 𝑎\n𝑉𝜔𝑠\n𝜋𝜃𝑥𝑠, 𝑎𝑥\n𝜋𝜃𝑦𝑠, 𝑎𝑦\n𝜋𝜃𝑧𝑠, 𝑎𝑧\n𝑉𝜔𝑥𝑠\n𝑉𝜔𝑦𝑠\n𝑉𝜔𝑧𝑠\nFC 32\nFC 2\nSoftmax\nFC 16\nFC 1\nFig. 3. Architectures of the original actor-critic RL and the proposed partial policy-based RL. For a state, the policy net outputs probabilities for all\npossible actions, and the value net gives a single scalar value as the long-term return. FC stands for fully-connected layer.\nto the partial action spaces, each of them trying to maximize\nthe expected cumulative reward by taking optimal actions\nsampled from the corresponding sub-action space. Thus, we\nuse multiple sub-agents learning smaller sub-tasks, instead of\none agent with a large task. The actual action space in our 3D\nlocalization problem is decomposed into the following partial\naction spaces:\nAi = {i+, i−}, i ∈{x, y, z}.\n(8)\nPartial policy refers to the policy undertaken by the sub-\nagents to map state to actions in the corresponding axial\ndomain. Thus, partial policies are projections of the original\npolicy, deﬁning the stochastic behavior for the partial action\nspaces. Therefore, we can deﬁne three partial policies with\nrespect to the partial action spaces. Three independent MLPs\nsharing a common preceding CNN are used to represent the\npartial policies. To evaluate the partial policies, we also deﬁne\nthree value function approximator networks stacked on top\nof the same CNN. Therefore, we have 6 MLPs preceded\nby a common CNN. For each sub-action space, there is a\nsub-actor and sub-critic available to update the corresponding\npartial policy and value function. The partial policy and value\nfunction approximators for our problem can be expressed as\nfollows:\nπθi(q, ai) = P(ai|S(q), θi), ai ∈Ai, i ∈{x, y, z}\nVωi(q) ≈V πθi(S(q)), i ∈{x, y, z}\n(9)\nwhere, θi and ωi are the network parameters for the i-th partial\npolicy and value approximators. Fig. 3 shows the overall\nnetwork architecture.\nIntroducing the partial policies, the learning problem be-\ncome easier and simpler in the shrunk action spaces. The goal\nof learning a partial policy is to provide the sub-agent at a state\nwith the probability of the actions in the corresponding partial\naction space. The sub-agent only needs to choose between\ntwo actions to maximize the cumulative return. Therefore, the\npartial policy learning shares the same idea with the simplest\nbinary classiﬁcation problem. Attaining the optimal partial\npolicy is easier, enabling a better convergence. The original\ndeﬁnitions of state and reward function are used without any\nalteration.\nB. Reconstruction\nPartial policy ensuring a simpler learning is not adequate\nwithout the deﬁnition of actual policy reconstruction from the\npartial policies in order to employ it appropriately. sub-agents\ncan decide the optimal partial actions from the learned partial\npolicy. Deriving the actual action am combining the partial\nactions ai, i ∈x, y, z is equivalent to sampling the action\nfrom the actual reconstructed policy. The actual action can\nbe reconstructed as follows:\nam =\nX\ni∈{x,y,z}\naiζi,\nai ∼πθi(q, ai)\n(10)\nHere, ζi is the basis vector of the i-th axis. am is the actual\naction at position q.\nOn the other hand, merging the partial policies to directly\nestimate the policy can be done by cascading the partial poli-\ncies followed by normalization as expressed in the following:\nπ(q, :) = 1\n3\n[\ni∈{x,y,z}\n{πθi(q, i+), πθi(q, i−)}\n(11)\nUsing either approach of (10) and (11) requires traversing all\nthe three partial policy networks to estimate a single optimal\n6\n2\n4\n6\n2\n4\n6\n2\n4\n6\nq0\nq1\nq2\nq3\nq4\nq5\nq6\nX\nZ\nY\nai ∈Ax ∼πθx(qi, ai)\nai ∈Ay ∼πθy(qi, ai)\nai ∈Az ∼πθz(qi, ai)\nFig. 4.\nSequential exploitation of the learned partial policies for\nlocalization. Sub-agents are periodically exploited to make a number of step\nsequences, thus walking a Manhattan-like distance.\nbehavior for a single state. Moreover, the critic evaluates a\npolicy by quantifying the next state arrived after an action\nsampled from that policy. Utilizing the above reconstruction\nsuggests a common next state obtained by a collaborative\ndecision on the partial actions. This no longer holds the as-\nsumption of independent partial policy learning, again making\nthe problem difﬁcult.\nTo ensure a greater efﬁciency, we propose a work-around\nto employ the partial policies to approximately represent\nthe original policy. We suggest a periodic and sequential\ndeployment of the partial policies, that can achieve the goal\nmaintaining the efﬁciency. Thus, we perform a step-sequence\ninstead of a single step. We deﬁne the k-th step-sequence sk\nas follows:\nsk = (ax,k, ay,k, az,k)\nax,k ∼πθx(qt, ax,k)\nay,k ∼πθy(qt+1, ay,k)\naz,k ∼πθz(qt+2, az,k)\nqt = T (qt−1, az,k−1)\nqt+1 = T (qt, ax,k)\nqt+2 = T(qt+1, ay,k)\n(12)\nwhere qt refers to the position at time step t, and k = ⌊t\n3⌋is\nthe order of step-sequence. We apply this sequence repeatedly,\nenabling a periodic selection of the partial policies. This also\nassures a balanced exploration in all sub-action spaces. The\nagent explicitly updates its position by taking a partial action\nfrom the deﬁned sequence, contributing to the independent\nlearning of the partial policies, because critic is able to give the\nfeedback on the transitioned state solely reached by exploiting\nan individual partial policy. Only the responsible policy and\nvalue parameters are updated for a transition. The periodic\napplication of the partial action sequence is depicted in Fig. 4.\nIt is apparent that the actual agent is crossing a distance similar\nto Manhattan distance, periodically exploiting the sub-agents.\nThe order of the partial actions in the unit sequence is not\nsigniﬁcant as long as they are repeated periodically in the\noverall action sequence.\nC. Actor-critic RL for partial policy\nThe periodic application of the partial actions sampled from\nthe partial policy functions establishes the foundation of partial\npolicy-based actor-critic learning. The same TD-framework is\nused maintaining the originality of the reward and transition,\nbecause those are not affected by the partial policy. Three\nindependent micro-actors are responsible for updating the\npartial policy networks, each having a corresponding critic.\nAt each step, one of the sub-agents are allowed to interact\nwith the environment using the current parameters of the\ncorresponding partial policy function, and utilize the critic\nto get directions to update the parameters. The critics also\nupdate the value based on the discounted return obtained by\nthe current policy. Periodic deployment ensures the balance\nin learning all the partial policies and values. Algorithm\n1\npresents the comprehensive actor-critic method for partial\npolicy-based reinforcement learning. While original actor-\ncritic method operates on step, the partial policy-based actor-\ncritic method operates on unit sequence consisting of three\npartial steps. For each partial steps, parameter updates occur\nin the corresponding partial policy function as well as in the\nvalue function. Though we used batch methods to update the\nnetworks from experience replay, the algorithm presented here\nuses the incremental method for easier interpretation. In batch-\nmethods, we ﬁrst gather episodes of experience and store them\nin an experience replay memory, then sample mini-batches\nfrom the memory to perform a stochastic gradient descent.\nAlgorithm 1 Actor-critic RL using partial policy\nInitialize q, θx, θy, θz, ω\nfor each step sequence do\nfor i ∈{x, y, z} do\nSample ai ∼πθi(q, ai)\nNext position, q′ = T (q, ai)\nReward, r = R(q, ai, q′)\nTD-target, τ(q, ai, q′) = r + γVωi(q′)\nTD-error, ε(q, ai, q′) = τ(q, ai, q′) −Vωi(q)\nθi ←θi + α∇θiε(q, ai, q′) log πθi(q, ai)\nωi ←ωi −α∇ωi(τ(q, ai, q′) −Vω(q))2\nq ←q′\nend for\nend for\nV. EXPERIMENT\nWe evaluate the proposed partial policy based RL method\nin three different problems in three datasets obtained from\nthree different sites. Table I summarizes the evaluation datasets\nand the corresponding problems. The ﬁrst dataset (Dataset-\nA) contains 71 contrast-enhanced coronary CT angiography\n(CCTA) volumes of 71 different patients. The corresponding\nproblem is to localize the eight landmarks of the aortic valve\n(three hinge points, three commissure points, and two coronary\nostia). Aortic valve (AV) landmark localization plays a vital\nrole in preprocedural planning of transcatheter aortic valve\nimplantation (TAVI) [11], which is an implant-based treatment\nmethod for severe aortic stenosis. Moreover, assessing the\n7\nFig. 5. Proposed partial policy based RL-localized landmarks. (left) Localized non-coronary and right coronary hinge points, the commisure point between\nthem, and right ostium of the aortic valve in a CCTA volume are shown with other landmarks being occluded. (middle) Localized LAA seed-points for different\ninitial positions in a CT volume. (right) Localized vertebra-centers in a spine MR volume.\nTABLE I\nEVALUATION DATASET AND PROBLEM DESCRIPTION.\nData\nVoxel dimension\nProblem\nDataset-A\n71 coronary CT\n0.35 mm × 0.35 mm × 0.5 mm\nAortic valve landmarks\nDataset-B\n150 cardiac CT\n0.45 mm × 0.45 mm × 0.5 mm\nLAA seed-point\nDataset-C\n20 spine MR (Public)\n0.58 mm × 0.58 mm × 1.5 mm\nVertebra centers\nTABLE II\nFOURFOLD CROSS VALIDATION TEST RESULTS FOR LOCALIZING THE AORTIC VALVE LANDMARKS IN CCTA VOLUMES (DATASET-A) AND LEFT ATRIAL\nAPPENDAGE SEED-POINT IN CT VOLUMES (DATASET-B). FOR DATASET-A, LOCALIZATION ERROR IS PRESENTED AS THE EUCLIDEAN DISTANCE FROM\nTHE GROUND TRUTH POSITION. THE RESULTS FOR THE TAVI AND NON-TAVI VOLUMES ARE PRESENTED SEPARATELY. FOR DATASET-B, THE\nPERCENTAGE OF THE LOCALIZATION FAILURE IS PRESENTED WITH RESPECT TO THE TOTAL NUMBER OF TRIALS FROM DIFFERENT RANDOM POSITION.\nLandmark/method\nPartial Policy RL\nRL\nMean ± SD\nMedian\nMean ± SD\nMedian\nNon-TAVI error (mm)\nHinge points\n1.96 ± 0.98\n1.72\n2.18 ± 1.21\n1.92\nCommissure points\n1.95 ± 0.93\n1.69\n2.16 ± 1.18\n1.87\nCoronary ostia\n1.91 ± 0.95\n1.65\n2.13 ± 1.20\n1.81\nTAVI error (mm)\nHinge points\n2.08 ± 1.24\n1.91\n2.30 ± 1.27\n2.11\nCommissure points\n2.02 ± 1.15\n1.85\n2.24 ± 1.22\n2.06\nCoronary ostia\n1.94 ± 1.01\n1.68\n2.15 ± 1.32\n1.94\nLAAO failure (%)\nLAA\n7.21 ± 5.84\n6.50\n11.62 ± 7.71\n9.22\nvalve is a clinical routine during any cardiac CT interpretation.\nHowever, it is a time consuming task because the valve\nanatomy is not easily perceived in the conventional CT views.\nAmong the 71 volumes, 31 volumes are preprocedural CT\nobtained from actual TAVI-patients. Accurate localization in\nTAVI volumes is challenging because valvular calciﬁcation can\nsigniﬁcantly affect the anatomy in unpredictable ways.\nUsing the second dataset (Dataset-B) consisting of 150 car-\ndiac CT volumes, we localized the left atrial appendage (LAA)\nseed-point, which can facilitate an automatic segmentation of\nthe appendage. LAA segmentation is helpful for physicians\nbecause it is a major site of thrombosis potentially responsi-\nble for inducing stroke-risk in non-valvular atrial ﬁbrillation\n[28]. Related prior works proposed different segmentation\napproaches, however, within a manually marked bounding\nbox (i.e., volume of interest) [29]. The prior annotation of\nsuch bounding box enclosing LAA is a major obstacle of the\napproaches to become fully automatic. Therefore, localizing\nthe aforementioned seed-point inside LAA can contribute to\nattaining an automatic segmentation method. Whereas the\ntarget points in the previous problem are speciﬁc, this problem\nsuggests localizing any point inside the appendage. There is\na large variation in appendage anatomy with an additional\nvariation for different cardiac phase. The 150 volumes are\nobtained from 30 different patients in 5 different cardiac\nphases.\nThe third dataset (Dataset-C) is a public dataset consists of\n20 MR images of spine targeted at vertebra recognition for\nspine structure analysis [30]. This dataset is available at the\nSpineWeb online repository. We implemented our proposed\nmethod to localize the centers of the vertebra. We localized\n5 lumbar vertebra (L1 L5). Among the 20 volumes, one has\nproblematic ground truth (GT) annotation and one volume cap-\ntured the head to shoulder region where the intended vertebra\n8\nTABLE III\nAORTIC VALVE LANDMARK LOCALIZATION ERROR COMPARISON WITH\nDIFFERENT APPROACHES.\nMethod\nAV Localization error (mm)\nMean ± SD\nPartial Policy RL\n1.98 ± 1.03\nActor-critic\n2.19 ± 1.23\nDQN\n2.26 ± 1.35\nRTW [16]\n2.35 ± 1.48\nInter-observer difference [22]\n2.38 ± 1.56\nTABLE IV\nFOURFOLD CROSS VALIDATION TEST RESULTS FOR LOCALIZING THE\nVERTEBRA CENTERS IN SPINE MR VOLUMES (DATASET-C) USING THE\nPARTIAL POLICY-BASED RL.\nVertebra\nLocalization error (mm)\nMean ± SD\nMedian\nL1\n2.79 ± 2.18\n2.66\nL2\n2.61 ± 2.02\n2.54\nL3\n2.58 ± 1.84\n2.52\nL4\n2.86 ± 1.81\n2.80\nL5\n3.10 ± 2.08\n3.05\nOverall\n2.79 ± 1.98\n2.71\n[30]’s result*\n2.87 ± 2.04\n2.80\n*Same dataset but different train/test split\nare not present. Therefore, we proceeded our experiment using\n18 volumes.\nFor all the datasets, necessary ground truth positions of the\ntarget landmarks were obtained from the corresponding site,\nwhich were used to process the reward signals for the RL\nagent. For all the experiments, a common set-ups for RL is\nused. We implemented both the original actor-critic RL and\nthe proposed partial policy-based RL for Dataset-A and B to\nobtain a comparative evaluation. To compare the proposed\nmethod with the widely-used DQN, we also implemented\nDQN for Dataset-A. For a fair comparison, we used identi-\ncal parameters and hyper-parameters for all the methods. A\nwindow size of m = 50 is used for the state, and the unit step\nsize is set to 2 voxels. For each epoch, the agent was allowed\nto gather around 300 episodes of experience using its current\npolicy, where each episode consists of 300 steps (or, 100 step-\nsequences in case of the partial policy). A replay memory\nof size 105 is used to store the transitions. For the partial\npolicy-based approach, three replay memories are maintained\nto keep track of the corresponding partial transitions. Sampling\nmini-batches from the experienced transitions, we perform\nstochastic gradient descend to update the policy and value\nnetwork. The learning rate for updating both the value and\npolicy was α = 10−4, and the discount factor γ was set to\n0.9. The CNN consists of 4 sets of convolutional, ReLU and\nmax-pooling layer stacks (Fig. 3). The ﬁnal layer was ﬂattened\nto obtain a non-spatial representation. 6 different MLPs were\nconnected to the ﬁnal ﬂat layer of the CNN, representing the\npartial policy and value functions. For the original RL, only\n2 MLPs are connected to represent a single set of policy and\nvalue functions. All the policy nets have a ﬁnal softmax-gating\nto generate action-probabilities. Though ϵ-greedy approach [3]\nis widely used for exploration in RL, Bayesian approach to\nallow the agent to deﬁne its own uncertainty has shown to\nperform better. Practically, the uncertainty is simulated by\nadding a dropout layer in the network [31]. We gradually\nannealed the dropout keep probability from 0.1 to 0.7 over\nthe epochs. To localize the vertebra-centers in Dataset-C using\nthe proposed method, we use the same architecture and hyper-\nparameter settings.\nA four-fold cross validation is performed on patient-basis\nto evaluate the localization performance. For the ﬁrst and\nthird experiments, the localization error is calculated in terms\nof the Euclidean distance of the localized position from\nthe corresponding ground truth. For the second experiment,\nsuch distance is not an appropriate measure because the\nassigned goal is to detect any point (seed) inside the left\natrial appendage, and the annotated ground truth were also\nnot speciﬁc. Therefore, the performance is measured using a\nbinary comparison (i.e., whether the localized point was inside\nthe appendage or not). During the test/validation session, the\nagent was not provided with any reward signal. For each\ntest case, we conducted the localization process initiating the\nagent from different random positions inside the test volume,\nand presented the average localization result. Fig. 5 depicts\nthe qualitative localization results of the proposed partial\npolicy-based RL. In Table II, we present the localization\nerror (for Dataset-A) and localization failure percentage (for\nDataset-B) of the proposed partial policy approach against the\noriginal actor-critic RL. Table III presents the average AV\nlandmarks localization error comparison for different methods.\nTable IV presents the average localization error of the vertebra\ncenters in spine MR images. The average computation time for\nlocalization is 1.2 seconds, as tested with a 3.60GHz single-\ncore CPU and a GeForce GTX TITAN Xp GPU. Computation\ntime for all the cases is same because an equal number of steps\nis performed in all cases.\nThe proposed method showed an average error of 1.98\n± 1.03 mm localizing the AV landmarks in CCTA volume,\nwhereas Elattar et al. [22]’s error for localizing the hinge\npoints and ostia in CTA was 2.65 ± 1.57 mm, and Zheng et al.\n[11]’s error for localizing all the landmarks in C-arm CT was\n2.11 ± 1.34 mm. The inter-observer difference in CTA was\n2.38 ± 1.56 mm, as reported in [22]. The proposed method\nalso showed improvement comparing to the random tree walk\nmethod in our previous work using the same dataset, where the\naverage localization error was 2.35 ± 1.48 mm [16]. On the\nother hand, RTW takes only a few milliseconds to localize a\nlandmark because of its simple feature computation. However,\nCNN-based feature computation in the currently proposed\nmethod is more useful and the current localization time of\n1.2 seconds can still be considered efﬁcient and allowable\nfor such an improved accuracy. In our previous work, we\nalso introduced a colonial walk method utilizing multiple\nwalks from multiple initial positions and choosing the ﬁnal\nwalk by minimum walk variance. Such extension can also\nbe performed with the current RL agent walk to improve the\n9\n0\n3,000\n6,000\n−50\n0\n50\n100\nEpochs\nMean reward per 100 episodes\nDQN\nAC\nPPRL\n0\n3,000\n6,000\n0\n10\n20\n30\nEpochs\nMean localization error (mm)\nDQN-train\nDQN-test\nAC-train\nAC-validation\nPPRL-train\nPPRL-validation\n20\n40\n60\n80\n100\n0\n4\n8\n12\n16\nTraining samples (%)\nAV landmark localization error (mm)\nDQN\nAC\nPPRL\nFig. 6. Learning curves of the proposed partial policy-based RL. (left and middle) Average reward and localization error over different epochs, for AV\nlandmark localization. Reward plot is smoothed out for better visualization. (right) Localization performance with respect to training data size. Training set\nsize is gradually increased while validating with a common test set. The proposed method could achieve almost the maximum accuracy with a fewer training\nexamples.\nFig. 7. Search paths of different RL agents localizing a hinge point. The\ntransitions of the PPRL, acotr-critic, and DQN agents is indicated by red,\nblue, and orange lines, respectively. Green and red dots indicate the initial\nposition and target landmark.\naccuracy even more, which we keep as our future work. As\nfor the center locations of the vertebra in spine MR volumes,\nthe proposed method showed an average error of 2.79 ± 1.98\nmm, which is as good as [30]’s result.\nThe proposed partial-policy based method exhibited note-\nworthy improvement over the original actor-critic and the\nDQN approach. The average localization error is improved\nwith a signiﬁcant reduction of error variance. Learning the par-\ntial policies facilitated an improved localization with simpler\ndecision process. At each step sequence, the sub-agents solve\nthree binary decision problems. Consequently, the proposed\nmethod exhibited a noteworthy improvement comparing to\ndirectly learning the original policy. We also performed an\nadditional experiment where the agent is trained only on the 40\nnon-TAVI volumes and attempts to localize the landmarks in\nthe TAVI volumes. Thus, we could observe the agent behavior\nin a volume with valvular calciﬁcation, without providing any\nprior knowledge about calciﬁed valves. Table V presents the\naverage localization results for the proposed method and the\nconventional actor-critic RL. Partial policy could cope with\nthe variation due to calciﬁcation signiﬁcantly better than the\nconventional RL because of simpler decision space.\nTo compare the learning process and learned trajectories, we\nTABLE V\nAORTIC VALVE LANDMARK LOCALIZATION RESULTS IN CALCIFIED TAVI\nVOLUMES BY AN AGENT TRAINED ON ONLY NON-TAVI VOLUMES.\nLocalization error (mm)\nPartial Policy RL\nRL\nMean ± SD\nMean ± SD\nHinge points\n2.52 ± 1.78\n3.38 ± 2.19\nCommissure points\n2.35 ± 1.74\n3.16 ± 2.17\nCoronary ostia\n2.26 ± 1.68\n2.97 ± 2.02\nplot the average reward and localization error over the epoch\nfrom the learning process of the proposed method and the\nconventional ones, and illustrate optimal search paths in Fig. 6.\nA remarkable improvement is observed in the learnability of\nthe proposed partial policy approach. It enabled a better and\nfaster convergence. It converges within about half the epochs\nrequired by the conventional RLs (i.e., actor-critic and DQN),\nthus improving the slow learning problem in RL. Within a\nfew-trials, the sub-agents could reach an optimal behavior, as\ndepicted in the error plots. The search paths of the sub-agents\nalso exhibit more conﬁdent transitions compared to the paths\nundertaken by the conventional agents (Fig. 7).\nPreparing training data with ground truth acquisition is a\ndifﬁcult task in medical image processing. Therefore, it is\nadvantageous to have a model that can give a satisfactory\nperformance with knowledge of a fewer training data. Apart\nfrom the standard train/test splits, we randomly sampled about\n20% of the dataset to be the test data (for Dataset-A). From\nthe rest of the dataset, we gradually sampled 20%, 40%, 60%,\n80%, and 100% to obtain ﬁve training subsets, where the last\nsubset (100% training samples) refers to the whole training\nset. The test set is validated by the models trained on those\nsubsets. For comparison, we used an identical split for the\nproposed PPRL, as well as the actor-critic and DQN approach.\nFig. 6(right) presents our observation, where the proposed\nmethod could achieve very close to the maximum accuracy\n(achieved with 100% training samples) with a notably fewer\ntraining exmaples. Even with 20% of the training data, it could\nprovide an average error of 3.8 mm, which is comparable to the\n10 mm error of DQN and actor-critic. Thus, the partial-policy\n10\nbased RL can potentially be useful in medical applications.\nVI. CONCLUSION\nAnatomical Landmark localization provides signiﬁcant prior\ninformation for different applications in medical image pro-\ncessing. Our work presented a robust localization method\nformulated as reinforcement learning. For an efﬁcient and\nsuccessful learning with actor-critic method, we introduced\na partial policy-based learning where multiple easier policies\nare learned on the sub-action spaces deﬁned as projections\nof the original action space. Employing multiple sub-agents\ninteracting with the environment, corresponding micro-actors\nand micro-critics independently update the deep partial policy\nand value networks, enabled a faster and better convergence.\nThe experiment with aortic valve landmarks localization and\nleft atrial appendage seed localization in 3D CT images, and\nvertebra localization in 3d spine MR images showed robust\nand improved performance, compared to the conventional\nactor-critic and widely used deep Q-learning approach. The\nproposed partial policy based approach required signiﬁcantly\nfewer number of trials and fewer training data to achieve the\noptimal behavior, improving the slow learning problem of RL.\nThe proposed method provides an efﬁcient and potentially use-\nful solution for localization, requiring an average localization\ntime of 1.2 seconds.\nACKNOWLEDGMENT\nThis research was supported by Basic Science Research\nProgram through the National Research Foundation of Korea\n(NRF), funded by the Ministry of Education, Science, Tech-\nnology (No. 2017R1A2B4004503).\nREFERENCES\n[1] A. Sotiras, C. Davatzikos, and N. Paragios, “Deformable medical image\nregistration: A survey,” IEEE Transactions on Medical Imaging, vol. 32,\nno. 7, pp. 1153–1190, 2013.\n[2] X. Han and Y. Zhou, “Systems and methods for segmenting medical\nimages based on anatomical landmark-based features,” Aug. 22 2017,\nUS Patent 9,740,710.\n[3] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[4] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and\nA. Farhadi, “Target-driven visual navigation in indoor scenes using\ndeep reinforcement learning,” in 2017 IEEE International Conference\non Robotics and Automation (ICRA).\nIEEE, 2017, pp. 3357–3364.\n[5] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing atari with deep reinforcement learn-\ning,” arXiv preprint arXiv:1312.5602, 2013.\n[6] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, “Policy\ngradient methods for reinforcement learning with function approxima-\ntion,” in Advances in Neural Information Processing Systems, 2000, pp.\n1057–1063.\n[7] V. R. Konda and J. N. Tsitsiklis, “Actor-critic algorithms,” in Advances\nin Neural Information Processing Systems, 2000, pp. 1008–1014.\n[8] I. Grondman, L. Busoniu, G. A. Lopes, and R. Babuska, “A survey of\nactor-critic reinforcement learning: Standard and natural policy gradi-\nents,” IEEE Transactions on Systems, Man, and Cybernetics, Part C\n(Applications and Reviews), vol. 42, no. 6, pp. 1291–1307, 2012.\n[9] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and\nP. Abbeel, “RL2: Fast reinforcement learning via slow reinforcement\nlearning,” arXiv preprint arXiv:1611.02779, 2016.\n[10] J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finocchio, A. Blake,\nM. Cook, and R. Moore, “Real-time human pose recognition in parts\nfrom single depth images,” Communications of the ACM, vol. 56, no. 1,\npp. 116–124, 2013.\n[11] Y. Zheng, M. John, R. Liao, A. Nottling, J. Boese, J. Kempfert,\nT. Walther, G. Brockmann, and D. Comaniciu, “Automatic aorta segmen-\ntation and valve landmark detection in C-arm CT for transcatheter aortic\nvalve implantation,” IEEE Transactions on Medical Imaging, vol. 31,\nno. 12, pp. 2307–2321, 2012.\n[12] A. Ross, “Procrustes analysis,” Course report, Department of Computer\nScience and Engineering, University of South Carolina, 2004.\n[13] R. I. Ionasec, I. Voigt, B. Georgescu, Y. Wang, H. Houle, F. Vega-\nHiguera, N. Navab, and D. Comaniciu, “Patient-speciﬁc modeling and\nquantiﬁcation of the aortic and mitral valves from 4-D cardiac CT and\nTEE,” IEEE Transactions on Medical Imaging, vol. 29, no. 9, pp. 1636–\n1651, 2010.\n[14] A. Criminisi, D. Robertson, E. Konukoglu, J. Shotton, S. Pathak,\nS. White, and K. Siddiqui, “Regression forests for efﬁcient anatomy\ndetection and localization in computed tomography scans,” Medical\nImage Analysis, vol. 17, no. 8, pp. 1293–1303, 2013.\n[15] H. Y. Jung, S. Lee, Y. S. Heo, and I. D. Yun, “Forest walk methods for\nlocalizing body joints from single depth image,” PLoS ONE, vol. 10,\nno. 9, p. e0138328, 2015.\n[16] W. A. Al, H. Y. Jung, I. D. Yun, Y. Jang, H.-B. Park, and H.-\nJ. Chang, “Automatic aortic valve landmark localization in coronary\nCT angiography using colonial walk,” PLoS ONE, vol. 13, no. 7, p.\ne0200317, 2018.\n[17] J. Gall, A. Yao, N. Razavi, L. Van Gool, and V. Lempitsky, “Hough\nforests for object detection, tracking, and action recognition,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 33,\nno. 11, pp. 2188–2202, 2011.\n[18] S. Schulter, C. Leistner, P. Wohlhart, P. M. Roth, and H. Bischof,\n“Accurate object detection with joint classiﬁcation-regression random\nforests,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2014, pp. 923–930.\n[19] O. Oktay, W. Bai, R. Guerrero, M. Rajchl, A. de Marvao, D. P. ORegan,\nS. A. Cook, M. P. Heinrich, B. Glocker, and D. Rueckert, “Stratiﬁed\ndecision forests for accurate anatomical landmark localization in cardiac\nimages,” IEEE Transactions on Medical Imaging, vol. 36, no. 1, pp.\n332–342, 2017.\n[20] A. Hennemuth, T. Boskamp, D. Fritz, C. K¨uhnel, S. Bock, D. Rinck,\nM. Scheuering, and H.-O. Peitgen, “One-click coronary tree segmenta-\ntion in CT angiographic images,” in International Congress Series, vol.\n1281.\nElsevier, 2005, pp. 317–321.\n[21] H. Tek, M. A. Gulsun, S. Laguitton, L. Grady, D. Lesage, and G. Funka-\nLea, “Automatic coronary tree modeling,” The Insight Journal, 2008.\n[22] M. Elattar, E. Wiegerinck, F. van Kesteren, L. Dubois, N. Planken,\nE. Vanbavel, J. Baan, and H. Marquering, “Automatic aortic root\nlandmark detection in CTA images for preprocedural planning of\ntranscatheter aortic valve implantation,” The International Journal of\nCardiovascular Imaging, pp. 1–11, 2015.\n[23] J. C. Caicedo and S. Lazebnik, “Active object localization with deep\nreinforcement learning,” in Proceedings of the IEEE International Con-\nference on Computer Vision, 2015, pp. 2488–2496.\n[24] Z. Jie, X. Liang, J. Feng, X. Jin, W. Lu, and S. Yan, “Tree-structured\nreinforcement learning for sequential object localization,” in Advances\nin Neural Information Processing Systems, 2016, pp. 127–135.\n[25] F. C. Ghesu, B. Georgescu, T. Mansi, D. Neumann, J. Hornegger, and\nD. Comaniciu, “An artiﬁcial agent for anatomical landmark detection\nin medical images,” in International Conference on Medical Image\nComputing and Computer-Assisted Intervention.\nSpringer, 2016, pp.\n229–237.\n[26] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual\nattention,” in Advances in Neural Information Processing Systems, 2014,\npp. 2204–2212.\n[27] R. S. Sutton, “Learning to predict by the methods of temporal differ-\nences,” Machine Learning, vol. 3, no. 1, pp. 9–44, 1988.\n[28] M. Zoni-Berisso, F. Lercari, T. Carazza, and S. Domenicucci, “Epidemi-\nology of atrial ﬁbrillation: European perspective,” Clinical Epidemiol-\nogy, vol. 6, p. 213, 2014.\n[29] C. Jin, J. Feng, L. Wang, J. Liu, H. Yu, J. Lu, and J. Zhou, “Left atrial\nappendage segmentation using fully convolutional neural networks and\nmodiﬁed three-dimensional conditional random ﬁelds,” IEEE Journal of\nBiomedical and Health Informatics, pp. 1–1, 2018.\n[30] Y. Cai, S. Osman, M. Sharma, M. Landis, and S. Li, “Multi-modality\nvertebra recognition in arbitrary views using 3D deformable hierarchical\n11\nmodel,” IEEE Transactions on Medical Imaging, vol. 34, no. 8, pp.\n1676–1693, 2015.\n[31] Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation:\nRepresenting model uncertainty in deep learning,” in International\nConference on Machine Learning, 2016, pp. 1050–1059.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2018-07-09",
  "updated": "2018-12-31"
}