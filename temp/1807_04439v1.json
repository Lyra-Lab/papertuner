{
  "id": "http://arxiv.org/abs/1807.04439v1",
  "title": "Will it Blend? Composing Value Functions in Reinforcement Learning",
  "authors": [
    "Benjamin van Niekerk",
    "Steven James",
    "Adam Earle",
    "Benjamin Rosman"
  ],
  "abstract": "An important property for lifelong-learning agents is the ability to combine\nexisting skills to solve unseen tasks. In general, however, it is unclear how\nto compose skills in a principled way. We provide a \"recipe\" for optimal value\nfunction composition in entropy-regularised reinforcement learning (RL) and\nthen extend this to the standard RL setting. Composition is demonstrated in a\nvideo game environment, where an agent with an existing library of policies is\nable to solve new tasks without the need for further learning.",
  "text": "Will it Blend?\nComposing Value Functions in Reinforcement Learning\nBenjamin van Niekerk * 1 Steven James * 1 Adam Earle 1 Benjamin Rosman 1 2\nAbstract\nAn important property for lifelong-learning\nagents is the ability to combine existing skills\nto solve unseen tasks. In general, however, it is\nunclear how to compose skills in a principled way.\nWe provide a “recipe” for optimal value function\ncomposition in entropy-regularised reinforcement\nlearning (RL) and then extend this to the standard\nRL setting. Composition is demonstrated in a\nvideo game environment, where an agent with an\nexisting library of policies is able to solve new\ntasks without the need for further learning.\n1. Introduction\nA major challenge in artiﬁcial intelligence is creating agents\ncapable of leveraging existing knowledge for inductive trans-\nfer. Lifelong learning, in particular, requires that an agent\nbe able to act effectively when presented with a new, un-\nseen task. A promising approach is to combine behaviours\nlearned in various separate tasks to create new skills (Taylor\n& Stone, 2009). This compositional approach allows us to\nbuild rich behaviours from relatively simple ones, resulting\nin a (good!) combinatorial explosion in the agent’s abilities\n(Saxe et al., 2017). However, in general, it is unclear how to\nproduce new optimal skills from known ones.\nOne approach to compositionality is Linearly-solvable\nMarkov Decision Processes (LMDPs) (Todorov, 2007),\nwhich structure the reward function to ensure that the Bell-\nman equation becomes linear in the exponentiated value\nfunction. Todorov (2009) proves that the optimal value\nfunctions of a set of LMDPs can be composed to produce\nthe optimal value function for a composite task. This is a\nparticularly attractive property, since solving new tasks re-\nquires no further learning. However, the LMDP framework\n1University of the Witwatersrand, Johannesburg, South Africa\n2Council for Scientiﬁc and Industrial Research, Pretoria, South\nAfrica.\nCorrespondence to:\nBenjamin van Niekerk <ben-\njamin.vanniekerk@students.wits.ac.za>.\nThe 2 nd Lifelong Learning: A Reinforcement Learning Approach\n(LLARLA) Workshop, Stockholm, Sweden, FAIM 2018. Copyright\n2018 by the author(s).\nhas so far been restricted to the tabular case with known\ndynamics, limiting its usefulness.\nRelated work has focused on entropy-regularised reinforce-\nment learning (RL) (Schulman et al., 2017; Haarnoja et al.,\n2017; Nachum et al., 2017), where rewards are augmented\nwith an entropy-based penalty term. This has been shown\nto lead to improved exploration and rich, multimodal value\nfunctions.\nPrior work (Haarnoja et al., 2018) has demonstrated that\nthese value functions can be composed to approximately\nsolve the intersection of tasks. We complement these results\nby proving optimal composition for the union of tasks in the\ntotal-reward, absorbing-state setting. Thus, any task lying in\nthe “span” of a set of basis tasks can be solved immediately,\nwithout any further learning. We provide a “recipe” for\noptimally composing value functions, and demonstrate our\nmethod in a video game. Results show that an agent is able\nto compose existing policies learned from pixel input to\ngenerate new, optimal behaviours.\n2. Background\nA Markov decision process (MDP) is deﬁned by the 4-tuple\n(S, A, ρ, r) where (i) the state space S is standard Borel;\n(ii) the action space A is ﬁnite (and therefore a compact\nmetric space when equipped with the discrete metric); (iii)\nthe transition dynamics ρ deﬁne a Markov kernel (s, a) 7→\nρ(s,a) from S ×A to S; and (iv) the reward r is a real-valued\nfunction on S × A that is bounded and measurable.\nIn RL, an agent’s goal is to maximise its utility by making a\nsequence of decisions. At each time step, the agent receives\nan observation from S and executes an action from A ac-\ncording to its policy. As a consequence of its action, the\nagent receives feedback (reward) and transitions to a new\nstate. Whereas the rewards represent only the immediate\noutcome, the utility captures the long-term consequences\nof actions. Historically, many utility functions have been\ninvestigated (Puterman, 2014), but in this paper we only\nconsider the total-reward criterion (see Section 2.1).\nWe consider the class of MDPs with an absorbing set G,\nwhich is a Borel subset of the state space. We augment the\narXiv:1807.04439v1  [cs.LG]  12 Jul 2018\nWill it Blend? Composing Value Functions in Reinforcement Learning\nstate space with a virtual state g such that ρ(s,a)({g}) = 1\nfor all (a, s) in G × A, and r = 0 after reaching g. In\nthe control literature, this class of MDPs is often called\nstochastic shortest path problems (Bertsekas & Tsitsiklis,\n1991), and naturally model domains that terminate after the\nagent achieves some goal.\nWe restrict our attention to stationary Markov policies, or\nsimply policies. A policy s 7→πs is a Markov kernel from S\nto A. Together with an initial distribution ν over S, a policy\ndeﬁnes a probability measure over trajectories. To formalise\nthis, we construct the set of n-step histories inductively by\ndeﬁning H0 = S and Hn = Hn−1 ×A×S for n in N. The\nn-step histories represent the set of all possible trajectories\nof length n in the MDP. The probability measure on Hn\ninduced by the policy π is then\nP π\nν,n = ν ⊗π ⊗ρ ⊗· · · ⊗π ⊗ρ\n|\n{z\n}\nn times\n.\nUsing the standard construction (Klenke, 1995), we can\ndeﬁne a unique probability measure P π\nν on H∞consistent\nwith the measures P π\nν,n in the sense that\nP π\nν (E × A × S × A × · · · ) = P π\nν,n(E),\nfor any n in N and any Borel set E ⊆Hn. If ν is con-\ncentrated on a single state s, we simply write P π\nν = P π\ns .\nAdditionally for any real-valued bounded measurable func-\ntion f on Hn, we deﬁne Eπ\nν[f] to be the expected value of\nf under P π\nν .\nFinally, we introduce the notion of a proper policy—a policy\nunder which the probability of reaching G after n steps\nconverges to 1 uniformly over S as n →∞. Our deﬁnition\nextends that of Bertsekas & Tsitsiklis (1995) to general\nstate spaces, and is equivalent to the deﬁnition of transient\npolicies used by James & Collins (2006):\nDeﬁnition 1. A stationary Markov policy π is said to be\nproper if\nsup\ns∈S\n∞\nX\nt=0\nP π\ns (st ̸∈G) < ∞.\nOtherwise, we say that π is improper.\n2.1. Entropy-Regularised RL\nIn the standard RL setting, the expected reward at state s un-\nder policy π is given by Ea∼π [r(s, a)]. Entropy-regularised\nRL (Ziebart, 2010; Fox et al., 2016; Haarnoja et al., 2017;\nSchulman et al., 2017; Nachum et al., 2017) augments the\nreward function with a term that penalises deviating from\nsome reference policy ¯π. That is, the expected reward is\ngiven by Ea∼π [r(s, a)] −τKL[πs||¯πs], where τ is a pos-\nitive scalar temperature parameter and KL[πs||¯πs] is the\nKullback-Leibler divergence between π and the reference\npolicy ¯π at state s. When ¯π is the uniform random policy,\nthe regularised reward is equivalent to the standard entropy\nbonus up to an additive constant (Schulman et al., 2017).\nThis results in policies that are more robust to “winner’s\ncurse” (Fox et al., 2016). Additionally, the reference pol-\nicy can be used to encode prior knowledge through expert\ndemonstration.\nBased on the above regularisation, we deﬁne the n-step\nvalue function starting from s and following policy π as:\nVπ,n(s) = Eπ\ns,n\n\"n−1\nX\nt=0\nr(st, at) −τKL[πst||¯πst]\n#\n.\nNote that since the KL-divergence term is measurable\n(Dupuis & Ellis, 2011, Lemma 1.4.3), Vπ,n is well-deﬁned.\nThe inﬁnite-horizon value function, which represents the\ntotal expected return after executing π from s, is then\nVπ(s) = lim sup\nn→∞Vπ,n(s).\nSince the reward function and KL-divergence are bounded,1\nVπ is well deﬁned. Similarly, we deﬁne the Q-function to\nbe the expected reward after taking action a in state s, and\nthereafter following policy π:\nQπ(s, a) = r(s, a) +\nZ\nS\nVπ(s′)ρ(a,s)(ds′).\n(1)\nGiven the deﬁnitions above, we say that a measurable func-\ntion V ∗is optimal if V ∗(s) = supπ Vπ(s) for all s in S.\nFurthermore, a policy π∗is optimal if Vπ∗= V ∗.\nIn the standard RL case, the optimal policy is always de-\nterministic and is deﬁned by argmaxa Q∗(s, a). On the\nother hand, entropy-regularised problems may not admit\nan optimal deterministic policy. This results from the KL-\ndivergence term, which penalises deviation from the refer-\nence policy ¯π. If ¯π is stochastic, then a deterministic policy\nmay incur more cost than a stochastic policy. To see this,\nconsider the simple two-state MDP shown in Figure 1:\ns\ng\nr(s, Right) = −1\nr(s, Left) = −1\nFigure 1. A two-state MDP with absorbing state g.\nGiven τ > 0 and a uniformly random reference policy,\nlet π be the deterministic policy that selects Right with\nprobability 1, and let πε be the stochastic policy that selects\nRight with probability 1 −ε and Left with probability\n1Under the assumptions that A is ﬁnite and ¯π is chosen so that\nπs is absolutely continuous with respect to ¯πs for any state s and\npolicy π.\nWill it Blend? Composing Value Functions in Reinforcement Learning\nε. Then, choosing ε small enough, we can guarantee that\nVπε > Vπ. Therefore for any τ > 0, the optimal policy is\nnon-deterministic.\nProof. First, the value of state s under the policy π is given\nby Vπ(s) = −1 −τ log 2. On the other hand, the expected\nnumber of steps from s to g under πε is 1/(1 −ε) so we\nhave\nVπε(s) = −1 + ε log 2ε\n1 −ε\n−log 2(1 −ε).\nNow, choose ε such that\nε < τ(log 2 −1/2)\n2 + τ\n.\n(2)\nThen, from (2) we have that:\n(i) ε < 1/2 and therefore log 2(1 −ε) < 0,\n(ii) log 2ε < 2ε, and\n(iii) log 2 −1/2 < 1 and therefore ε < τ/(2 + τ)\nUsing the above facts we get the chain of inequalities:\nVπε(s)\n(i)> −1 + ε log(2ε)\n1 −ε\n(ii)\n> −1 + 2ε\n1 −ε\n(iii)\n> −1 −τ/2 −ε(2 + τ)\n(2)> −1 −τ log 2.\nThe last inequality follows directly from (2), giving\nVπε(s) > Vπ(s).\n3. Soft Value and Policy Iteration\nIn this section, we investigate the total-reward, entropy-\nregularised criterion deﬁned above. While value and pol-\nicy iteration in entropy-regularised RL have been analysed\npreviously (Nachum et al., 2017), convergence results are\nlimited to discounted MDPs. We sketch an argument that an\noptimal proper policy exists under the total-reward criterion\nand that the soft versions of value and policy iteration (see\nAlgorithms 1 and 2) converge to optimal solutions.\nWe begin by deﬁning the Bellman operators:\n[TπVπ](s) =\nZ\nA\nQπ(s, a)πs(da) −τKL[πs||¯πs],\n(3)\n[T V ](s) = sup\nπ [TπV ](s).\n(4)\nEquations (3) and (4) are analogous to the standard Bellman\noperator and Bellman optimality operator respectively. Note\nthat since the optimal policy may not be deterministic, the\nBellman optimality operator selects the supremum over\npolicies instead of actions.\nWe also deﬁne the soft Bellman operator\n[LVπ](s) = τ log\nZ\nA\nexp (Qπ(s, a)/τ) πs(da).\n(5)\nHere L is referred to as “soft”, since it is a smooth approxi-\nmation of the max operator. The soft Bellman operator is\nconnected to the Bellman optimality operator through the\nfollowing result:\nLemma 1. Let V : S →R be a bounded measurable\nfunction. Then T V = LV and the supremum is attained\nuniquely by the Boltzmann policy B[V ] deﬁned by\ndBs[V ]\nd¯πs\n(a) =\nexp\n\u0000Q(s, a)/τ\n\u0001\nR\nA exp\n\u0000Q(s, a′)/τ\n\u0001\n¯π(da′|s).\nProof. Follows directly from Dupuis & Ellis (2011, Propo-\nsition 1.4.2).\nAnalogous to the standard RL setting, we can deﬁne value\nand policy iteration in the entropy-regularised context,\nwhere the Bellman operators are replaced with their “soft”\nequivalents:\nAlgorithm 1 Soft Value Iteration\nInput: MDP, temperature τ > 0, bounded function V\nOutput: Optimal value function V ∗\ninitialize V ∗←V\nrepeat\nreplace V ←V ∗\napply soft Bellman operator V ∗←L[V ]\nuntil convergence\nAlgorithm 2 Soft Policy Iteration\nInput: MDP, temperature τ > 0, proper policy π\nOutput: Optimal policy π∗\ninitialize π∗←π\nrepeat\nreplace π ←π∗\npolicy evaluation:\nﬁnd Vπ, the ﬁxed-point of Tπ\npolicy improvement:\ncompute the Boltzmann policy π∗←B[Vπ]\nuntil convergence\nFollowing closely along the lines of Bertsekas & Tsitsiklis\n(1991) and James & Collins (2006), but taking special care\nto account for the fact that optimal policies are not necessar-\nily deterministic, it can be shown that the above algorithms\nconverge to optimal solutions.\nWill it Blend? Composing Value Functions in Reinforcement Learning\nTheorem 1. Suppose that Assumptions 1 and 2 (James &\nCollins, 2006) hold and that the optimal value function is\nbounded above. Then:\n(i) there exists an optimal proper policy;\n(ii) the optimal value function is the unique bounded mea-\nsurable solution to the optimality equation;\n(iii) the soft policy iteration algorithm converges to the\npolicy starting from any proper policy;\n(iv) the soft value iteration algorithm converges to the opti-\nmal value function starting from any proper policy.\n4. Compositionality\nIn lifelong learning, an agent is presented with a series of\ntasks drawn from some distribution. The goal is to exploit\nknowledge gained in previous tasks to improve performance\nin the current task. We consider an environment with ﬁxed\nstate space S, action space A, deterministic transition dy-\nnamics ρ, and absorbing set G. Let D be a ﬁxed but unknown\ndistribution over (S, A, ρ, r). The agent is then presented\nwith tasks sampled from D, which differ only in their re-\nward functions. In this section, we describe a compositional\napproach for tackling this problem.\nSuppose that the reward functions drawn from D differ only\non the absorbing set G. This restriction was introduced\nby Todorov (2009), and is a strict subset of the successor\nrepresentations framework (Dayan, 1993; Barreto et al.,\n2017). Given a library of previously-solved tasks, we can\ncombine their Q-functions to solve any task lying in the\n“span” of the library without further learning:\nTheorem 2 (Optimal Composition). Let M1, . . . , Mn be\na library of tasks drawn from D. Let Q∗,k\nτ\nbe the optimal\nentropy-regularised Q-function, and rk be the reward func-\ntion for Mk. Deﬁne the vectors\nr = [r1, . . . , rn]\nand\nQ∗\nτ = [Q∗,1\nτ , . . . , Q∗,n\nτ\n].\nGiven a set of non-negative weights w, with ||w||1 = 1,\nconsider a further task drawn from D with reward function\nsatisfying r = τ log (|| exp(r/τ)||w) for all s in G, where\n|| · ||w denotes the weighted 1-norm. Then the optimal\nQ-value for this task is given by:\nQ∗\nτ = τ log (|| exp(Q∗\nτ/τ)||w) .\n(6)\nThat is, the optimal Q-functions for the library of tasks can\nbe composed to form Q∗\nτ.\nProof. Since ρ is deterministic, we can ﬁnd a measurable\nfunction f : S × A →S such that ρ(s,a) = δf(s,a). For any\nQ-function, deﬁne the desirability function\nZ(s, a) = exp (Q(s, a)/τ) ,\nand deﬁne the operator U on the space of non-negative\nbounded measurable functions by\n[UZ](s, a) = exp (r(s, a)/τ)\nZ\nA\nZ(f(s, a), a)¯πs(da′).\nWe now show that the desirability function of Q∗\nτ is a ﬁxed\npoint of U. Since V ∗\nτ is the ﬁxed point of the Bellman\noptimality operator, by combining Equation (1), Lemma 1\nand Theorem 1, we have\nV ∗\nτ (s) = τ log\nZ\nA\nexp (Q∗\nτ(f(s, a), a′)/τ) ¯πs(da′)\nand Q∗\nτ(s, a) = r(s, a) + V ∗\nτ (f(s, a)).\nThen it follows that\n[UZ∗\nτ ](s, a) = er(s,a)/τ\nZ\nexp (Q∗\nτ/τ) d(ρ(s,a) ⊗¯πs)\n= er(s,a)/τ exp (V ∗\nτ (f(s, a))/τ) = Z∗\nτ (s, a).\nHence Z∗\nτ is a ﬁxed point of U. Under the assumptions on\nthe reward function r, the optimal Q-value satisﬁes Q∗\nτ =\nτ log (|| exp(Q∗\nτ/τ)||w) on G. Therefore, restricted to G,\nZ∗\nτ is a linear combination of the desirability functions for\nthe family of tasks. Since (6) holds on G and it is clear that\nU is a linear operator, then (6) holds everywhere.\nThe following lemma links the previous result to the stan-\ndard RL setting. Recall that entropy-regularisation appends\na temperature-controlled penalty term to the reward func-\ntion. As the temperature parameter tends to 0, the reward\nprovided by the environment dominates the entropy penalty,\nand the problem reduces to the standard RL case:\nLemma 2. Let {τn}∞\nn=1 be a sequence in R such that τn ↓\n0. Let Q∗\nτn be the optimal Q-value function for MDP(τn):\nthe entropy-regularised MDP with temperature parameter\nτn. Let Q∗\n0 be the optimal Q-value for the standard MDP.\nThen Q∗\nτn ↑Q∗\n0 as n →∞.\nProof. First note that for a ﬁxed policy π, state s and action\na, we have Qπ\nτn(s, a) ↑Qπ\n0(s, a) as n →∞. This follows\ndirectly from the deﬁnition of the entropy-regularised value\nfunction, and the fact that the KL-divergence is non-negative.\nThen using Lemma 3.14 (Hinderer, 1970) to interchange\nthe limit and supremum, we have\nlim\nn→∞Q∗\nτn = lim\nn→∞sup\nπ Qπ\nτn = sup\nπ\nlim\nn→∞Qπ\nτn\n= sup\nπ Qπ\n0 = Q∗\n0.\nSince Qπ\nτn ↑Qπ\n0, we have Q∗\nτn ↑Q∗\n0 as n →∞.\nFinally, we show that composition holds in the standard RL\nsetting by taking the low-temperature limit of Theorem 2.\nWill it Blend? Composing Value Functions in Reinforcement Learning\nCorollary 1. Let {τn}∞\nn=1 be a sequence in R such that\nτn ↓0. Then max Q∗\nτn ↑Q∗\n0 as n →∞.\nProof. For a ﬁxed state s and action a and a possible re-\nordering of the vector Q∗\n0(s, a), we may suppose, without\nloss of generality, that Q∗,1\n0 (s, a) = max Q∗\n0(s, a). Then\nby Lemma 2, we can ﬁnd an N in N such that\nQ∗,1\nτn (s, a) = max Q∗\nτn(s, a) for all n ≥N.\nSince log is continuous, we have from Theorem 2 that\nlim\nn→∞Q∗\nτn = log\n\u0010\nlim\nn→∞|| exp(Q∗\nτn)||1/τn\nw\n\u0011\n,\nwhere || · ||p\nw denotes the weighted p-norm. By factoring\nexp(Q∗,1\nτn ) out of || exp(Q∗\nτn)||1/τn\nw\n, we are left with\n||1, exp(∆2), . . . , exp(∆k)||1/τn\nw\n,\nwhere ∆i = Q∗,i\nτn −Q∗,1\nτn for i = 2, . . . , k. Since Q∗,1\nτn (s, a)\nis the maximum of Q∗\nτn(s, a) for all n ≥N, the limit as\nn →∞of the above is 1. Then it follows that\nlim\nn→∞Q∗,1\nτn (s, a) = log\n\u0010\nlim\nn→∞exp(Q∗,1\nτn (s, a))\n\u0011\n= Q∗,1\n0 (s, a).\nSince s and a were arbitrary and Q∗,m\nτn\n↑Q∗,m\n0\n, we have\nthat max Q∗\nτn ↑Q∗\n0 as n →∞.\nComparing Theorem 2 to Corollary 1, we see that as the\ntemperature parameter decreases to zero, the weight vector\nhas less inﬂuence on the composed Q-function. In the limit,\nthe optimal Q-function is independent of the weights and\nis simply the maximum of the library functions. This sug-\ngests a natural trade-off between our ability to interpolate\nbetween Q-functions, and the stochasticity of the optimal\npolicy. Furthermore, Corollary 1 mirrors that of generalised\npolicy improvement (Barreto et al., 2017), which shows that\ncomputing the maximum of a set of Q-functions results in an\nimproved Q-function. In our case, the resulting Q-function\nis not merely an improvement, but is in fact optimal.\nThe composition described in this section can be viewed as\nan –OR– task composition: if objectives of two tasks are\nto achieve goals A and B respectively, then the composed\nQ-function will achieve A–OR–B optimally. Haarnoja et al.\n(2018) show that an approximate –AND– composition is\nalso possible for entropy-regularised RL. That is, if the goals\nA and B partially overlap, the composed Q-function will\nachieve A–AND–B approximately. The idea is that the opti-\nmal Q-function for the composite task can be approximated\nby the average of the library Q-functions. We include their\nresults for completeness:\nLemma 3 (Haarnoja et al., 2018). Let Q∗,1\nτ\nand Q∗,2\nτ\nbe\nthe optimal Q-functions for two tasks drawn from D with\nrewards r1 and r2. Deﬁne the averaged Q-function Qave :=\n(Q∗,1\nτ\n+ Q∗,2\nτ )/2. Then the optimal Q-function Q∗\nτ for the\ntask with reward function r = (r1 + r2)/2 satisﬁes\nQave ≥Q∗\nτ ≥Qave −C∗\nτ ,\nwhere C∗\nτ is a ﬁxed point of\nτEs′∼ρ(s,a)\nh\nD 1\n2\n\u0000π∗,1\ns ||π∗,2\ns\n\u0001\n+ max\na′ C(s′, a′)\ni\n,\nthe policy π∗,i\ns\nis the optimal Boltzmann policy for task i,\nand D 1\n2 (·||·) is the R´enyi divergence of order 1\n2.\nTheorem 3 (Haarnoja et al., 2018). Using the deﬁnitions in\nLemma 3, the value of the composed policy πave satisﬁes\nQπave ≥Q∗\nτ −F ∗\nτ ,\nwhere F ∗\nτ is a ﬁxed point of\nτEs′∼ρ(s,a)\nh\nEa′∼πave\ns′ [C∗\nτ (s′, a′) −F(s′, a′)]\ni\n.\nWe believe that the low-temperature result from Lemma 2\ncan be used to obtain similar results for the standard RL\nframework. We provide empirical evidence of this in the\nnext section, and leave a formal proof to future work.\n5. Experiments\nTo demonstrate composition, we perform a series of experi-\nments in a grid-world video game (Figure 2b). The goal of\nthe game is to collect items of different colours and shapes.\nThe agent has four actions that move it a single step in any\nof the cardinal directions, unless it collides with a wall.Each\nobject in the domain is one of two shapes (squares and cir-\ncles), and one of three colours (blue, beige and purple), for\na total of six objects (see Figure 2a).\nBeige\nBlue\nPurple\nSquare\nCircle\n(a) Items to be collected.\n(b) Layout of the grid-world.\nFigure 2.\nWe construct a number of different tasks based on the objects\nthat the agent must collect, the task’s name specifying the\nobjects to be collected. For example, Purple refers to the\ntask where an agent must collect any purple object, while\nBeigeSquare requires collecting the single beige square.\nWill it Blend? Composing Value Functions in Reinforcement Learning\nFor each task, the episode begins by randomly positioning\nthe six objects and the agent. At each timestep, the agent\nreceives a reward of −0.1. If the correct object is collected,\nthe agent receives a reward of 1 and the episode terminates.\nWe ﬁrst learn to solve a number of base tasks using (soft)\ndeep Q-learning (Mnih et al., 2015; Schulman et al., 2017),\nwhere each task is trained with a separate network. The\nresulting networks are collected into a library from which\nwe will later compose new Q-functions.\nThe input to our network is a single RGB frame of size 84×\n84, which is passed through three convolutional layers and\ntwo fully-connected layers before outputting the predicted\nQ-values for the given state. Using the results in Section 4,\nwe compose optimal Q-functions from those in the library.\n5.1. –OR– Composition\nHere we consider new tasks that can be described as the\nunion of a set of base tasks in the standard RL setting. We\ntrain an agent separately on the Purple and Blue tasks,\nadding the corresponding Q-functions to our library. We\nuse Corollary 1 to produce the optimal Q-function for the\ncomposite PurpleOrBlue task, which requires the agent\nto pick up either blue or purple objects, without any further\nlearning. Results are given in Figure 3.\nThe local maxima over blue and purple objects illustrates\nthe multimodality of the value function (Figure 3a). This\nis similar to approaches such as soft Q-learning (Haarnoja\net al., 2017), which are also able to learn multimodal poli-\ncies. However, we have observed that directly learning a\ntruly multimodal policy for the composite task can be difﬁ-\ncult. If the entropy regularisation is too high, the resulting\npolicy is extremely stochastic. Too low a temperature re-\nsults in a loss of multimodality, owing to winner’s curse. It\nis instead far easier to learn unimodal value functions for\neach of the base tasks, and then compose them to produce\noptimal multimodal value functions.\n5.2. Linear Task Combinations\nIn Theorem 2 we showed that in the entropy-regularised\nsetting, the composed Q-function is dependent on a weight\nvector w. This allows us to achieve a more general type of\ncomposition. In particular, we can immediately compute\nany optimal Q-function that lies in the “span” of the library\nQ-functions. Indeed, according to Theorem 2 the exponen-\ntiated optimal Q-function is a linear combination of the\nexponentiated library functions. Therefore, the weights can\nbe used to modulate the relative importance of the library\nfunctions—modelling the situation in which an agent has\nmultiple concurrent objectives of unequal importance.\nWe illustrate the effect of the weight vector w using soft Q-\nlearning with a temperature parameter τ = 1. We construct\na new task by composing the tasks PurpleCircle and\nBeigeSquare, and assign different weights to these tasks.\nThe different weighted value functions are given in Figure 4.\n5.3. –AND– Composition\nHere we consider tasks which can be described as the inter-\nsection of tasks in the library. In general, this form of compo-\nsition will not yield an optimal policy for the composite task\nowing to the presence of local optima in the composed value\nfunction. However, in many cases we can obtain a good\napproximation to the composite task by simply averaging\nthe Q-values for the constituent tasks. While Haarnoja et al.\n(2018) considers this type of composition in the entropy-\nregularised case, we posit that this can be extended to the\nstandard RL setting by taking the low-temperature limit.\nWe illustrate this by composing the optimal policies for the\nBlue and Square tasks, which produces a good approxi-\nmation to the optimal policy for collecting the blue square.\nResults are shown in Figure 5.\n(a)\n(b)\n(c)\nFigure 3. (a) The optimal value function for PurpleOrBlue, which is produced by composing the Purple and Blue Q-functions.\nThe multimodality of the composite value function is clearly visible. (b) Sample trajectories for the composite PurpleOrBlue task,\nwith the agent beginning at different positions. The agent selects the shortest path to any of the target objects. (c) Returns from 50k\nepisodes. The ﬁrst two box plots are the results of acting in the PurpleOrBlue task using only one of the base Q-functions, while the\nthird uses the composite Q-function.\nWill it Blend? Composing Value Functions in Reinforcement Learning\n(a) BeigeSquare: 0.0\n(b) BeigeSquare: 0.05\n(c) BeigeSquare: 0.1\n(d) BeigeSquare: 0.5\n(e) BeigeSquare: 0.9\n(f) BeigeSquare: 0.95\n(g) BeigeSquare: 1.0\nBeigeSquare\nPurpleCircle\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\nBeigeSquare Weight\nObjects Collected\n(h)\nFigure 4. (a–g) Weighted composed value function for the task BeigeSquareOrPurpleCircle. The weight assigned to the Q-\nfunction for BeigeSquare is varied from 0 to 1. (h) The number of beige squares compared to purple circles collected by the agent as\nthe weights are varied in steps of 0.05. Results for each weight were averaged over 80 runs of 100 episodes.\n5.4. Temporal\nOur ﬁnal experiment demonstrates the use of composition to\nlong-lived agents. We compose the base Q-functions for the\ntasks Blue, Beige and Purple, and use the resulting Q-\nfunction to solve the task of collecting all objects. Sample\ntrajectories are illustrated by Figure 6.\nDespite the fact that the individual tasks terminate after\ncollecting the required object, if we allow the episode to\ncontinue, the composed Q-function is able to collect all\nobjects in a greedy fashion. The above shows the power of\ncomposition—if we possess a library of skills learned from\nprevious tasks, we can compose them to solve any task in\ntheir union continually.\n6. Conclusion\nWe showed that in entropy-regularised RL, value functions\ncan be optimally composed to solve the union of tasks.\nExtending this result by taking the low-temperature limit,\nwe showed that composition is also possible in standard\nRL. However, there is a trade-off between our ability to\nsmoothly interpolate between tasks, and the stochasticity of\nthe optimal policy. We demonstrated, in a high-dimensional\nenvironment, that a library of optimal Q-functions can be\ncomposed to solve composite tasks consisting of unions, in-\ntersections or temporal sequences of simpler tasks. The pro-\nposed compositional framework is a step towards lifelong-\nlearning agents that are able to combine existing skills to\nsolve new, unseen problems.\n(a)\n(b)\n(c)\nFigure 5. (a) The approximately optimal value function of the composed policies. Local optima are clearly visible. (b) Sample trajectories\nfrom the composed policy beginning from different starting positions. The agent exhibits suboptimal, but sensible behaviour near beige\nsquares. (c) The IQR of returns from 50k episodes. The ﬁrst box plot is the return from the optimal solution to the union of tasks, the\nsecond is the result of the approximate intersection of tasks, and the third is the true optimal policy.\nWill it Blend? Composing Value Functions in Reinforcement Learning\n(a)\n(b)\n(c)\nFigure 6. (a) and (b) Sample trajectories for the task of collecting all objects. (c) Returns from 50k episodes. The ﬁrst box plot is the\nreturn of the composed Q-function, while the second is the result of DQN trained to collect all objects explicitly.\nReferences\nBarreto, A., Dabney, W., Munos, R., Hunt, J., Schaul, T.,\nvan Hasselt, H., and Silver, D. Successor features for\ntransfer in reinforcement learning. In Advances in neural\ninformation processing systems, pp. 4055–4065, 2017.\nBertsekas, D.P. and Tsitsiklis, J.N. An analysis of stochas-\ntic shortest path problems. Mathematics of Operations\nResearch, 16(3):580–595, 1991.\nBertsekas, D.P. and Tsitsiklis, J.N. Neuro-dynamic pro-\ngramming: an overview. In Proceedings of the 34th IEEE\nConference on Decision and Control, volume 1, pp. 560–\n564. IEEE, 1995.\nDayan, P. Improving generalization for temporal difference\nlearning: The successor representation. Neural Computa-\ntion, 5(4):613–624, 1993.\nDupuis, P. and Ellis, R. A weak convergence approach to\nthe theory of large deviations. 2011.\nFox, R., Pakman, A., and Tishby, N. Taming the noise in\nreinforcement learning via soft updates. In 32nd Confer-\nence on Uncertainty in Artiﬁcial Intelligence, 2016.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. Re-\ninforcement learning with deep energy-based policies.\nIn International Conference on Machine Learning, pp.\n1352–1361, 2017.\nHaarnoja, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and\nLevine, S. Composable deep reinforcement learning for\nrobotic manipulation. arXiv preprint arXiv:1803.06773,\n2018.\nHinderer, K. Foundations of non-stationary dynamic pro-\ngramming with discrete time parameter. In Lecture Notes\nin Operations Research and Mathematical Systems, vol-\nume 33. 1970.\nJames, H.W. and Collins, E.J. An analysis of transient\nMarkov decision processes. Journal of applied probabil-\nity, 43(3):603–621, 2006.\nKlenke, A. Probability Theory: A Comprehensive Course,\nvolume 158. 1995. ISBN 9781447153603.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness,\nJ., Bellemare, M.G., Graves, A., Riedmiller, M., Fidje-\nland, A.K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529, 2015.\nNachum, O., Norouzi, M., Xu, K., and Schuurmans, D.\nBridging the gap between value and policy based rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems, pp. 2772–2782, 2017.\nPuterman, M.L.\nMarkov decision processes: discrete\nstochastic dynamic programming. John Wiley & Sons,\n2014.\nSaxe, A.M., Earle, A.C., and Rosman, B.S.\nHierarchy\nthrough composition with multitask LMDPs. Proceed-\nings of the 34th International Conference on Machine\nLearning, 70:3017–3026, 2017.\nSchulman, J., Abbeel, P., and Chen, X. Equivalence between\npolicy gradients and soft Q-learning. pp. 1–15, 2017.\nTaylor, M.E. and Stone, P. Transfer learning for reinforce-\nment learning domains: a survey. Journal of Machine\nLearning Research, 10:1633–1685, 2009.\nTodorov, E. Linearly-solvable Markov decision problems.\nIn Advances in Neural Information Processing Systems,\npp. 1369–1376, 2007.\nTodorov, E. Compositionality of optimal control laws. In\nAdvances in Neural Information Processing Systems, pp.\n1856–1864, 2009.\nZiebart, B.D. Modeling purposeful adaptive behavior with\nthe principle of maximum causal entropy. Carnegie Mel-\nlon University, 2010.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-07-12",
  "updated": "2018-07-12"
}