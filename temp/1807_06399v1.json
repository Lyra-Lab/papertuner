{
  "id": "http://arxiv.org/abs/1807.06399v1",
  "title": "Are Efficient Deep Representations Learnable?",
  "authors": [
    "Maxwell Nye",
    "Andrew Saxe"
  ],
  "abstract": "Many theories of deep learning have shown that a deep network can require\ndramatically fewer resources to represent a given function compared to a\nshallow network. But a question remains: can these efficient representations be\nlearned using current deep learning techniques? In this work, we test whether\nstandard deep learning methods can in fact find the efficient representations\nposited by several theories of deep representation. Specifically, we train deep\nneural networks to learn two simple functions with known efficient solutions:\nthe parity function and the fast Fourier transform. We find that using\ngradient-based optimization, a deep network does not learn the parity function,\nunless initialized very close to a hand-coded exact solution. We also find that\na deep linear neural network does not learn the fast Fourier transform, even in\nthe best-case scenario of infinite training data, unless the weights are\ninitialized very close to the exact hand-coded solution. Our results suggest\nthat not every element of the class of compositional functions can be learned\nefficiently by a deep network, and further restrictions are necessary to\nunderstand what functions are both efficiently representable and learnable.",
  "text": "Workshop track - ICLR 2018\nARE EFFICIENT DEEP REPRESENTATIONS\nLEARNABLE?\nMaxwell Nye\nMassachusetts Institute of Technology\nmnye@mit.edu\nAndrew Saxe\nHarvard University\nasaxe@fas.harvard.edu\nABSTRACT\nMany theories of deep learning have shown that a deep network can require dra-\nmatically fewer resources to represent a given function compared to a shallow\nnetwork. But a question remains: can these efﬁcient representations be learned\nusing current deep learning techniques? In this work, we test whether standard\ndeep learning methods can in fact ﬁnd the efﬁcient representations posited by sev-\neral theories of deep representation. Speciﬁcally, we train deep neural networks\nto learn two simple functions with known efﬁcient solutions: the parity function\nand the fast Fourier transform. We ﬁnd that using gradient-based optimization, a\ndeep network does not learn the parity function, unless initialized very close to a\nhand-coded exact solution. We also ﬁnd that a deep linear neural network does not\nlearn the fast Fourier transform, even in the best-case scenario of inﬁnite training\ndata, unless the weights are initialized very close to the exact hand-coded solution.\nOur results suggest that not every element of the class of compositional functions\ncan be learned efﬁciently by a deep network, and further restrictions are necessary\nto understand what functions are both efﬁciently representable and learnable.\n1\nINTRODUCTION\nDeep learning has seen tremendous practical success. To explain this, recent theoretical work in\ndeep learning has focused on the representational efﬁciency of deep neural networks. The work of\nMontufar et al. (2014); Pascanu et al. (2013); Telgarsky (2015); Mhaskar et al. (2016); Poole et al.\n(2016); Eldan & Shamir (2016), and Poggio et al. (2016), among others, has examined the efﬁciency\nadvantage of deep networks vs. shallow networks in detail, and cite it as a key theoretical property\nunderpinning why deep networks work so well. However, while these results prove that deep repre-\nsentations can be efﬁcient, they make no claims about the learnability of these representations using\nstandard training algorithms (Liao & Poggio, 2017). A key question, then, is whether the efﬁcient\ndeep representations these theories posit can in fact be learned.\nHere we empirically test whether deep network architectures can learn efﬁcient representations of\ntwo simple functions: the parity function, and the fast Fourier transform. These functions played\nan early and important role in motivating deep learning: in an inﬂuential paper, Bengio and LeCun\nnoted that a deep logic circuit could represent the parity function with exponentially fewer terms than\na shallow circuit (Bengio & LeCun, 2007). In the same paper, Bengio and LeCun pointed out that the\nfast Fourier transform, perhaps the most celebrated numerical algorithm, is a deep representation of\nthe Fourier transform. Moreover, these functions have the sort of compositional, recursive structure\nwhich a variety of subsequent theories have shown are more efﬁciently represented in deep networks\n(Poggio et al., 2016). In particular, their deep representations require at least a polynomially smaller\namount of resources than a shallow network would (from O(2n) neurons to O(n) neurons for the\nn-bit parity function, and from O(n2) synapses to O(n log n) synapses for the n-point fast Fourier\ntransform). Remarkably, despite the long history of the parity function as a motivating example\nfor depth, whether deep networks in fact can learn it has never been directly investigated to our\nknowledge. We ﬁnd that, despite the existence of efﬁcient representations, these solutions are not\nfound in practice by gradient descent. Our explorations reveal a puzzle for current theory: deep\nlearning works well on real-world tasks, despite not being able to ﬁnd some efﬁcient compositional\ndeep representations. What, then, is distinctive about real world tasks–as opposed to compositional\nfunctions like parity or the FFT–such that deep learning typically works well in practice?\n1\narXiv:1807.06399v1  [cs.LG]  17 Jul 2018\nWorkshop track - ICLR 2018\nFigure 1: Learning the parity function. Left: Example hand-coded network for computing the parity\nfunction over n = 8 inputs using sigmoidal neurons (biases not shown). Each layer XOR’s adjacent\nbits of the layer below. Note the recursive, compositional nature of the computation. Middle: Basin\nof attraction around hand-coded solution for n = 32. Deep sigmoid networks were initialized\nwith the exact solution perturbed by Gaussian noise. The networks were then trained for 100000\nminibatches of size 1000, each containing randomly drawn inputs. The resulting ﬁnal test error\nwas low only for small initialization noise, indicating that the efﬁcient solution is a minimum, but\nis inaccessible from typical random initializations. Right: Basin of attraction with sparsity pattern\nenforced. Here the exact sparsity pattern for the parity solution was hard-coded into the weights, but\nthe value of each nonzero weight was randomly drawn. The correct sparsity pattern improves the\nsize of the basin of attraction, but training still fails from typical random initializations.\n2\nLEARNING THE PARITY FUNCTION\nIn our ﬁrst experiment, we train a deep neural network to learn the parity function. The parity\nfunction on n bits can computed with O(n) computations. We hand-select weight matrices and\nbiases, {W par\ni\n, bpar\ni\n}, which exactly implement the parity function as a tree of XOR gates using\nsigmoidal neurons (see Fig. 1). We initialize a network with parameters {W par\ni\n, bpar\ni\n} and add\nscaled Gaussian noise (Glorot & Bengio, 2010). The variance of this noise controls the distance from\na known optimal solution, and hence can be used to track the size of the basin of attraction around\nthis optimum. Mean squared error is minimized using the Adam optimizer (Kingma & Ba, 2014)\nwith learning rate 1e−4 and minibatches of 1000 randomly sampled binary vectors (other batch sizes\nyielded similar results). Our sigmoid activation function had inverse temperature parameter α = 10.\nWe tested networks with input size n = {16, 32, 64, 128} and observed similar results for all sizes.\nFigure 1 shows our results. We ﬁnd that when the weights are initialized with only a small amount\nof noise, the network error converges to zero after training, and the network is able to learn the parity\nfunction. However, when initialized with higher noise, test error remains large and the network does\nnot learn the parity function. We also perform a follow-up experiment, in which networks are trained\nand only the weights and biases which had nonzero values in the optimal solutions {W par\ni\n, bpar\ni\n}\nare allowed to vary with training. Under these conditions, the network has a much smaller search\nspace and only needs to ﬁnd the values of the sparse nonzero values. Again, we ﬁnd that when\nthe initialization noise levels are sufﬁciently high, the network does not learn the parity function.\nHence, despite having a hyperefﬁcient deep representation with compositional structure, a generic\ndeep network does not typically ﬁnd this solution for the parity function.\n3\nLEARNING THE FAST FOURIER TRANSFORM\nIn our second experiment, we train a deep linear neural network to learn the fast Fourier transform.\nHere we tried to create a best-case scenario for deep learning: whereas for the parity function we\ncould only train on a subset of all possible inputs, here we exploit the linearity of the FFT to perform\nfull gradient descent on the true loss function, corresponding to an inﬁnite amount of training data.\nFor this reason, failure to learn the FFT cannot be ascribed to insufﬁcient training examples.\nThe fast Fourier transform (FFT) computes the discrete Fourier transform (DFT), a linear operation,\nwith O(n log n) multiplications. We hand-select a set of weight matrices {W F F T\ni\n} for a linear\nneural network which exactly implements the FFT for inputs of size n. We initialize our linear\nnetwork at the hand-coded FFT solution and then add Gaussian noise. As above, the variance of this\n2\nWorkshop track - ICLR 2018\n:\nFigure 2: Learning the fast Fourier transform. Left, top: The discrete Fourier transform is a dense\none layer linear network with n2 nonzero synapses. Left, bottom: The fast Fourier transform is a\ndeep linear network with just 2 nonzero connections per neuron. For larger inputs, the FFT can be\napplied recursively. Center: Basin of attraction around the hand-coded solution for n = 32. Only\nnearby initializations converge to the correct sparse solution. Right: Scaling behavior as a function\nof input size. The scaling factor is the network L0 norm divided by n log n, which is the complexity\nof the FFT. The correct asymptotic O(n log n) scaling corresponds to a ﬂat line (as achieved by\nthe hand-coded solution, red). Networks initialized near to the efﬁcient solution (blue) successfully\nrecover the correct scaling, but networks initialized farther away fail to scale as O(n log n) (green).\nnoise can be used to track the size of the basin of attraction around this optimum. The network is\nthen trained to minimize the mean-squared error using the Adam optimizer. Without encouragement\ntowards a sparse solution, a deep linear network will learn dense solutions in general. We encouraged\nsparsity with an L1 regularization term, yielding the ﬁnal loss: Loss = E\nh\n∥y −FFT[x]∥2\n2\ni\n+\nβ P\nj ∥Wj∥2\n1 where {x, y} is a network input-output pair, β is a regularization parameter, and Wj is\nthe jth weight matrix. We take the inputs to be drawn from a white distribution (e.g., x ∼N(0, I)).\nWe exploited the linear nature of the network model to perform our training. For linear networks,\nthe gradient of the loss in expectation over all inputs can be calculated exactly by propagating a\nbatch containing the n basis vectors. We evaluated our neural network by examining both the ﬁnal\nnetwork error and the ﬁnal sparsity pattern. In order to learn the fast Fourier transform, our neural\nnetwork must have a low test error as well as a sufﬁciently sparse representation.\nFigure 2 depicts our results. There exists a basin of attraction around the hand-coded optimal net-\nwork conﬁguration W F F T , such that if the network is initialized close to W F F T , setting small\nmagnitude weights to zero actually decreases the test error, indicating convergence to the correct\nsparse solution. However, if the network is initialized outside this basin of attraction, any amount\nof weight sparsiﬁcation causes the test error to increase, indicating that the solution is not sparse.\nFigure 2 right compares the L0 norm of trained networks after optimal sparsiﬁcation as a function\nof size for networks initialized near W F F T versus networks initialized far from W F F T . Starting\nfrom random initial conditions, the learned networks do not achieve the n log n scaling of the FFT.\n4\nDISCUSSION\nIn this work, we studied functions which appear to differentiate efﬁcient deep representation from\ndeep learning. We emphasize that results on efﬁcient deep representation are of critical importance\nand of independent interest. However, our results show that optimization remains a challenge in\ndeep networks. The existence of a low approximation error deep representation does not mean that\nsuch a solution can be found, and theories based on the assumption of global empirical risk mini-\nmization may prove optimistic with respect to deep learning performance in practice. When treating\nthese problems in the same way that practitioners do, we were unable to recover the efﬁcient repre-\nsentations discussed in many theory papers. Our results point to a performance paradox: while in\ngeneral efﬁcient representations may be inaccessible by learning, deep networks work exceedingly\nwell in practice. Possible resolutions are that (a) a subset of compositional functions are also eas-\nily learnable, (b) deep networks work well for another reason such as iterative nonlinear denoising\n(Kadmon & Sompolinsky, 2016), or (c) deep networks are fundamentally reliant on special archi-\ntectures which impose the right compositional structure.\n3\nWorkshop track - ICLR 2018\nACKNOWLEDGMENTS\nA.M.S. thanks the Swartz Program in Theoretical Neuroscience at Harvard.\nREFERENCES\nYoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel\nMachines. MIT Press, 2007.\nR. Eldan and O. Shamir. The Power of Depth for Feedforward Neural Networks. In COLT, 2016.\nX. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks.\n13th International Conference on Artiﬁcial Intelligence and Statistics, 2010.\nJ.\nKadmon\nand\nH.\nSompolinsky.\nOptimal\nArchitectures\nin\na\nSolvable\nModel\nof\nDeep\nNetworks.\nIn\nNIPS,\n2016.\nURL\nhttps://papers.nips.cc/paper/\n6330-optimal-architectures-in-a-solvable-model-of-deep-networks.\npdf.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nQ. Liao and T. Poggio. Theory of Deep Learning II: Landscape of the Empirical Risk in Deep\nLearning by. CBMM Memo, (066), 2017.\nH. Mhaskar, Q. Liao, and T. Poggio. Learning Functions: When Is Deep Better Than Shallow.\nCBMM Memo, (045), 2016.\nG. Montufar, K. Cho, R. Pascanu, and Y. Bengio. On the number of linear regions of deep neural\nnetworks. In NIPS, 2014.\nRazvan Pascanu, Guido Mont´ufar, and Yoshua Bengio. On the number of inference regions of deep\nfeed forward networks with piece-wise linear activations. CoRR, abs/1312.6098, 2013. URL\nhttp://arxiv.org/abs/1312.6098.\nTomaso A. Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why\nand when can deep - but not shallow - networks avoid the curse of dimensionality: a review.\nCoRR, abs/1611.00740, 2016. URL http://arxiv.org/abs/1611.00740.\nB. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential expressivity in deep\nneural networks through transient chaos. ArXiv e-prints, June 2016.\nM. Telgarsky. Representation Beneﬁts of Deep Feedforward Networks. In ArXiv, pp. 1–5, 2015.\n4\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2018-07-17",
  "updated": "2018-07-17"
}