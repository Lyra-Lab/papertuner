{
  "id": "http://arxiv.org/abs/1807.06792v2",
  "title": "Unsupervised Online Multitask Learning of Behavioral Sentence Embeddings",
  "authors": [
    "Shao-Yen Tseng",
    "Brian Baucom",
    "Panayiotis Georgiou"
  ],
  "abstract": "Unsupervised learning has been an attractive method for easily deriving\nmeaningful data representations from vast amounts of unlabeled data. These\nrepresentations, or embeddings, often yield superior results in many tasks,\nwhether used directly or as features in subsequent training stages. However,\nthe quality of the embeddings is highly dependent on the assumed knowledge in\nthe unlabeled data and how the system extracts information without supervision.\nDomain portability is also very limited in unsupervised learning, often\nrequiring re-training on other in-domain corpora to achieve robustness. In this\nwork we present a multitask paradigm for unsupervised contextual learning of\nbehavioral interactions which addresses unsupervised domain adaption. We\nintroduce an online multitask objective into unsupervised learning and show\nthat sentence embeddings generated through this process increases performance\nof affective tasks.",
  "text": "1\nUnsupervised Online Multitask Learning of\nBehavioral Sentence Embeddings\nShao-Yen Tseng and Brian Baucom and Panayiotis Georgiou, Senior Member, IEEE\nAbstract—Unsupervised\nlearning\nhas\nbeen\nan\nattractive\nmethod for easily deriving meaningful data representations from\nvast amounts of unlabeled data. These representations, or embed-\ndings, often yield superior results in many tasks, whether used\ndirectly or as features in subsequent training stages. However, the\nquality of the embeddings is highly dependent on the assumed\nknowledge in the unlabeled data and how the system extracts\ninformation without supervision. Domain portability is also very\nlimited in unsupervised learning, often requiring re-training on\nother in-domain corpora to achieve robustness. In this work\nwe present a multitask paradigm for unsupervised contextual\nlearning of behavioral interactions which addresses unsupervised\ndomain adaption. We introduce an online multitask objective\ninto unsupervised learning and show that sentence embeddings\ngenerated through this process increases performance of affective\ntasks.\nIndex Terms—Affective computing, behavior identiﬁcation,\nemotion recognition, knowledge representation, multitask learn-\ning, sentence embeddings, unsupervised learning\nI. INTRODUCTION\nRepresentation learning has become a crucial tool for\nobtaining superior results in many machine learning tasks [1].\nIn the scope of natural language processing (NLP) a notable\nexample of transforming input into more informative non-\nlinear abstractions is word embeddings, or word2vec [2]. Word\nembeddings exploit the use of language by learning semantic\nregularities based on a context of neighboring words. This form\nof contextual learning is unsupervised, which allows learning\nfrom large-scale corpora and is a main reason for its strength\nand effectiveness in improving performance of many tasks such\nas constituency parsing [3], sentiment analysis [4], [5], natural\nlanguage inference [6], and video/image captioning [7], [8].\nLater, with the introduction of sequence-to-sequence models\n[9], embeddings were extended to encode entire sentences and\nallowed representation of higher levels of concept through\ntransformation of longer contexts. For example, [10] obtained\nsentence embeddings, which they referred to as skip-thoughts,\nby training models to generate the surrounding sentences of\nextracts from contiguous pieces of text from novels. The\nauthors showed that the embeddings were adept at representing\nthe semantic and syntactic properties of sentences through\nevaluation on various semantic related tasks. In [11] the authors\nextracted sentence embeddings from an LSTM-RNN which was\ntrained using user click-through data logged from a web search\nengine. They then showed that embeddings generated by their\nmodels were especially useful for web document retrieval tasks.\nLater, [12] extracted sentence embeddings from a conversation\nmodel and showed the richness of semantic content by applying\nan additional weakly-supervised architecture to estimate the\nbehavioral ratings of couples therapy sessions. Many other\nworks have focused on obtaining general purpose sentence\nrepresentations: sentence embeddings that are adept at multiple\nNLP tasks [13], [14], [15].\nThe beneﬁt of many of the methods in aforementioned\nworks is that the embedding transformation is learned on\nlarge amounts of unlabeled data. Since natural language is\nan extremely complex process, it is crucial to leverage large\ncorpora when learning embeddings so as to capture true\nsemantic concepts instead of regularities of the data, e.g.\ndomain-speciﬁc topics [16]. Unsupervised learning allows us\nto utilize as much data as possible to increase the breadth of\nlanguage understanding while minimizing the effort of data\nannotation.\nHowever, a common issue with unsupervised training of\nsentence embeddings is the unpredictability of the resulting\nembedding transformation. In other words the embedding\ndistribution is highly random and often contains redundant\nor irrelevant information. In addition, depending on training\nconditions such as architecture or dataset, it might fail to capture\ninformational concepts or even semantics of the input data [14].\nThis is to be expected since the amount of information increases\nsigniﬁcantly as we move from words to sentences. It has been\nalso noted that the quality of sentence embeddings is often\nhighly dependent on the training dataset [11], [12]. So much\nso that the use of embeddings trained on small domain-relevant\ndatasets could yield results better than those trained on larger\ngeneric unsupervised datasets [10].\nIn this work we propose an online multitask learning\n(MTL) framework which aims to guide unsupervised sentence\nembeddings into a space that is more discriminative in a\nﬁnal task. In our framework, transfer of domain-knowledge\nis achieved through an additional task in parallel with unsu-\npervised contextual learning. The labels for the multitask are\ngenerated online from the unlabeled data to maintain the low\nannotation effort of an unsupervised scenario. Finally we apply\nthe sentence embeddings to a ﬁnal task of annotating human\nbehaviors as evaluation and show improvement in the potency\nof unsupervised contextual learning through MTL.\nII. RELATED WORK\nMany works have focused on leveraging multitask learning\nto enhance the informational content of sentence embeddings.\nThese methods can generally be categorized into task-speciﬁc\nor general-purpose applications.\nIn task-speciﬁc implementations a multitask function is often\nadded to a primary supervised objective. For example, [17]\narXiv:1807.06792v2  [cs.CL]  1 Nov 2018\n2\njointly learned sentence embeddings with an additional pivot\nprediction task in conjunction with sentiment classiﬁcation.\n[18] predicted neighboring words as a secondary objective to\nimprove accuracy of various sequence labeling tasks.\nOn the other hand, general purpose sentence embeddings\naim to provide pre-trained features which, when transferred to\nunrelated tasks, improves overall performance. [19] achieved\nthis by combining various tasks such as machine translation,\nconstituency parsing, and image caption generation, which\nimproved the translation quality between English and German.\nRecently, [20] presented a large-scale multitask framework\nfor learning general purpose sentence embeddings by training\nwith a multitude of NLP tasks, including skip-thought training,\nmachine translation, entailment classiﬁcation, and constituent\nparsing. Similarly, universal sentence representations were also\nproposed in [14] and [15]. [14] used a single Natural Language\nInference (NLI) task as the training objective whereas [15] also\nincluded tasks such as skip-thought and response-generation.\nOur work differs in that we build on contextual learning\nand attempt to guide unsupervised learning through a related\nmultitask objective. Unlike prior works, we target unsupervised\nscenarios and instead use a simple scheme to generate multitask\nlabels online. Although unsupervised learning has historically\nrequired more data and training time, recent implementations\nof general purpose sentence embeddings have greatly scaled\nup training in both dataset size and model complexity. We\nshow that through multitask guidance unsupervised sentence\nembeddings can still excel in targeted tasks without requiring\nextensive labeled datasets or complicated models.\nIn this paper we evaluate the performance of the unsupervised\nmultitask sentence embeddings in identifying various human\nbehaviors exhibited in conversational dialogue. In order to\nassess different sentence embedding methods fairly we apply\nsimple machine learning techniques to obtain results for the\nﬁnal task rather than neural networks which would be able\nto exploit minor gains in the features. We then provide an\nanalysis of the results to give insight on the beneﬁts of our\nproposed framework.\nIII. UNSUPERVISED MULTITASK EMBEDDINGS\nA. Sequence-to-sequence sentence embeddings\nThe sequence-to-sequence model (seq2seq) [9] maps input\nsequences to output sequences using an encoder-decoder\narchitecture. Given an input sentence x = (x0, x2, ..., xT )\nand output sentence y = (y0, y2, ..., yT ′), where xt and yt\nrepresent individual words, the standard sequence model can\nbe expressed as computing the conditional probability\nP(y | x) =\nT ′\nY\nt=0\nP(yt | yi<t, s, h)\n(1)\nwhere s is the sequence of outputs st from the encoder and h is\nthe internal representation of the input given by the last hidden\nstate of the encoder. For a given dataset D = {(xn, yn)}N\nn=1,\nthe internal representation h can be expressed as\nhθ ≡f(x | D) = f(x | θ)\n(2)\nwhere f(·) is the encoder function and θ is the set of parameters\nresulting from D.\nThe internal representation hθ encodes the input x into an\ninternal representation that allows the decoder to generate the\nbest estimate of y. In cases where D contains semantically-\nrelated data pairs, hθ can be viewed as a semantic vector\nrepresentation of the input, or sentence embedding, which can\nbe useful for subsequent NLP tasks. In our case we apply\ncontextual learning and designate consecutive sentences in\ncontinuous corpora as x and y.\nWhile this model allows us to obtain semantic rich embed-\ndings through training on unsupervised data, the quality of\nthe embeddings is highly inﬂuenced by biases in the data and\nprevents the embeddings from becoming specialized in any\ntarget task [14]. Therefore we propose to enhance the quality of\nunsupervised sentence embeddings through multitask learning.\nB. Multitask embedding training\nThe addition of a multitask objective can guide embeddings\ninto a space that is more discriminative in a target application.\nWe hypothesize that this holds true even when the multitask\nlabels are generated online from unsupervised data with no\nassumption of label reliability, as long as there is some relation\nbetween the multitask and target application.\nAssuming an online system which generates multitask labels\nb for each input x we can augment the dataset to yield Daug =\n{(xn, yn, bn)}N\nn=1. We then aim to predict this new label b\nin conjunction with the original output sequence y. This is\nimplemented in our seq2seq model by adding another head,\nor multitask network, after the internal representation h, as\nshown in Figure 1. In addition to Eq. 1, the model now also\nestimates the conditional probability\nP(b | x) = g(h | Daug) = g(hθaug)\n(3)\nwhere g(·) is the multitask network and hθaug is the new internal\nrepresentation given by Daug. In this work, the multitask\nnetwork g(·) is implemented with a multilayer perceptron.\nThe training loss is then the weighted sum of losses from\nthe multiple tasks, deﬁned as\nJ = λ · L1(y, x) + (1 −λ) · L2(b, x)\n(4)\nwhere L1 and L2 are the cross entropy losses for contextual\nlearning and the additional task, respectively.\nWith most multitask setups there is an issue on how to control\nthe training ratio λ to account for different data sources. For\nexample, if there is no overlap in inputs of the multiple tasks\nthen λ can only alternate between 0 and 1 during training to\nswitch between the different tasks. However, since we propose\na multitask objective whose labels are generated from incoming\ndata we are able to freely adjust λ. It is possible to adjust\nthe multitask ratio as training progresses to put emphasis on\ndifferent tasks but we do not make any assumptions on the\noptimal weighting scheme and give equal importance to both\ntasks by setting λ to 0.5.\n3\nGRU\nC\nGRU\nGRU\nC\nGRU\nGRU\nC\nGRU\n~hb\nS\nLbxmCksNACtGk7E4I6a+xmM74=\">AB+3icbVA9SwNBE\nJ2LXzF+RWNnsxgECwl3NmoXsLGM4JlALoS9zSRZsrd37O\n4FjiP+FRsLFVv/iJ3/xs1HoYkPBh7vzezOvDARXBvX/XY\nKa+sbm1vF7dLO7t7+Qfnw6FHqWLos1jEqhVSjYJL9A03A\nluJQhqFApvh6HbqN8eoNI/lg8kS7ER0IHmfM2qs1C1Xgj\nGyPBihkm5iyHDSDUvdctWtuTOQVeItSLXuBMdg0eiWv4J\nezNIpWGCat327FudnCrDmcBJKUg1JpSN6ADblkoaoe7ks\n+Un5MwqPdKPlS1pyEz9PZHTSOsCm1nRM1QL3tT8T+vnZ\nr+dSfnMkNSjb/qJ8KYmIyTYL0uEJmRGYJZYrbXQkbUkW\nZsXlNQ/CWT14l/mXtpubde9X6BcxRhBM4hXPw4ArqcAcN8\nIFBs/wCm/Ok/PivDsf89aCs5ipwB84nz9415Ta</late\nxit>\nw\nxA3x+a5V5V/cVOLbD/vNsjcWuQ=\">AB+3icbVBNSwMxE\nE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JN\nnCstS/4sWDilf/iDd/jGD6cdDWBwOP92aSmRckgmvjOF+\n4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcC\nOgkCmgUCGgH45uZ356A0jyW9yZLoBfRoeQhZ9RYqV+t+R\nNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9au\nf/iBmaQTSMEG17r2rV5OleFMwLTspxoSysZ0CF1LJY1A9\n/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+\numJrzq5VwmqQHJFh+FqSAmJrMkyIArYEZklCmuN2VsBF\nVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt\n6iFPMRQhp7QC3rFj/gZv+H3RWsBL2dq6A/wxw+EqJZi</\nlatexit>\nw\nxA3x+a5V5V/cVOLbD/vNsjcWuQ=\">AB+3icbVBNSwMxE\nE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JN\nnCstS/4sWDilf/iDd/jGD6cdDWBwOP92aSmRckgmvjOF+\n4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcC\nOgkCmgUCGgH45uZ356A0jyW9yZLoBfRoeQhZ9RYqV+t+R\nNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9au\nf/iBmaQTSMEG17r2rV5OleFMwLTspxoSysZ0CF1LJY1A9\n/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+\numJrzq5VwmqQHJFh+FqSAmJrMkyIArYEZklCmuN2VsBF\nVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt\n6iFPMRQhp7QC3rFj/gZv+H3RWsBL2dq6A/wxw+EqJZi</\nlatexit>\nR\n0YtV2E7BnX0g0fi/mjtSY0dm1c=\">AB+3icbVBNS8NAE\nN34WetXtEcvi0XwICXxot4KXjxWMLbQhLDZTtulm03Y3R\nRCqH/FiwcVr/4Rb/4bN20O2vpg4PHezO7Mi1LOlHacb2t\ntfWNza7u2U9/d2z84tI+OH1WSQoeTXgiexFRwJkATzPNo\nZdKIHEoRtNbku/OwWpWCIedJ5CEJORYENGiTZSaDf8Kd\nDCn4AUTqrxeBZG9dBuOi1nDrxK3Io0UYVOaH/5g4RmMQh\nNOVGq75q3goJIzSiHWd3PFKSETsgI+oYKEoMKivnyM3xml\nAEeJtKU0Hiu/p4oSKxUHkemMyZ6rJa9UvzP62d6eB0UTK\nSZBkEXHw0zjnWCyTwgEmgmueGECqZ2RXTMZGEapNXGYK\n7fPIq8S5bNy3m2L6o0augEnaJz5KIr1EZ3qIM8RFGOn\ntErerOerBfr3fpYtK5Z1UwD/YH1+QOJ0pQp</latexit>~hb\nS\nLbxmCksNACtGk7E4I6a+xmM74=\">AB+3icbVA9SwNBE\nJ2LXzF+RWNnsxgECwl3NmoXsLGM4JlALoS9zSRZsrd37O\n4FjiP+FRsLFVv/iJ3/xs1HoYkPBh7vzezOvDARXBvX/XY\nKa+sbm1vF7dLO7t7+Qfnw6FHqWLos1jEqhVSjYJL9A03A\nluJQhqFApvh6HbqN8eoNI/lg8kS7ER0IHmfM2qs1C1Xgj\nGyPBihkm5iyHDSDUvdctWtuTOQVeItSLXuBMdg0eiWv4J\nezNIpWGCat327FudnCrDmcBJKUg1JpSN6ADblkoaoe7ks\n+Un5MwqPdKPlS1pyEz9PZHTSOsCm1nRM1QL3tT8T+vnZ\nr+dSfnMkNSjb/qJ8KYmIyTYL0uEJmRGYJZYrbXQkbUkW\nZsXlNQ/CWT14l/mXtpubde9X6BcxRhBM4hXPw4ArqcAcN8\nIFBs/wCm/Ok/PivDsf89aCs5ipwB84nz9415Ta</late\nxit>\nw\nxA3x+a5V5V/cVOLbD/vNsjcWuQ=\">AB+3icbVBNSwMxE\nE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JN\nnCstS/4sWDilf/iDd/jGD6cdDWBwOP92aSmRckgmvjOF+\n4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcC\nOgkCmgUCGgH45uZ356A0jyW9yZLoBfRoeQhZ9RYqV+t+R\nNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9au\nf/iBmaQTSMEG17r2rV5OleFMwLTspxoSysZ0CF1LJY1A9\n/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+\numJrzq5VwmqQHJFh+FqSAmJrMkyIArYEZklCmuN2VsBF\nVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt\n6iFPMRQhp7QC3rFj/gZv+H3RWsBL2dq6A/wxw+EqJZi</\nlatexit>\nw\nxA3x+a5V5V/cVOLbD/vNsjcWuQ=\">AB+3icbVBNSwMxE\nE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JN\nnCstS/4sWDilf/iDd/jGD6cdDWBwOP92aSmRckgmvjOF+\n4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcC\nOgkCmgUCGgH45uZ356A0jyW9yZLoBfRoeQhZ9RYqV+t+R\nNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9au\nf/iBmaQTSMEG17r2rV5OleFMwLTspxoSysZ0CF1LJY1A9\n/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+\numJrzq5VwmqQHJFh+FqSAmJrMkyIArYEZklCmuN2VsBF\nVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt\n6iFPMRQhp7QC3rFj/gZv+H3RWsBL2dq6A/wxw+EqJZi</\nlatexit>\nR\n0YtV2E7BnX0g0fi/mjtSY0dm1c=\">AB+3icbVBNS8NAE\nN34WetXtEcvi0XwICXxot4KXjxWMLbQhLDZTtulm03Y3R\nRCqH/FiwcVr/4Rb/4bN20O2vpg4PHezO7Mi1LOlHacb2t\ntfWNza7u2U9/d2z84tI+OH1WSQoeTXgiexFRwJkATzPNo\nZdKIHEoRtNbku/OwWpWCIedJ5CEJORYENGiTZSaDf8Kd\nDCn4AUTqrxeBZG9dBuOi1nDrxK3Io0UYVOaH/5g4RmMQh\nNOVGq75q3goJIzSiHWd3PFKSETsgI+oYKEoMKivnyM3xml\nAEeJtKU0Hiu/p4oSKxUHkemMyZ6rJa9UvzP62d6eB0UTK\nSZBkEXHw0zjnWCyTwgEmgmueGECqZ2RXTMZGEapNXGYK\n7fPIq8S5bNy3m2L6o0augEnaJz5KIr1EZ3qIM8RFGOn\ntErerOerBfr3fpYtK5Z1UwD/YH1+QOJ0pQp</latexit>\n~hf\nz\nXQM=\">AB+3icbVA9SwNBEJ2LXzF+na0WQyChYQ7G7UL2NiIEYwJCHsbeaSJXt7x+5eIBzxr9hYqNj6R+zs/CluP\ngpNfDweG9md+YFieDaeN6Xk1tZXVvfyG8WtrZ3dvfc/YMHaeKY3FIlaNgGoUXGLNcCOwkSikUSCwHgyuJn59iEr\nzWN6bUYLtiPYkDzmjxkodt9gaIstaA1TSwzpjzthoeOWvLI3BVkm/pyUKu7N7TcAVDvuZ6sbszRCaZigWjd9+1Y7o8\npwJnBcaKUaE8oGtIdNSyWNULez6fJjcmyVLgljZUsaMlV/T2Q0noUBbYzoqavF72J+J/XTE140c64TFKDks0+ClNBT\nEwmSZAuV8iMGFlCmeJ2V8L6VFmbF6TEPzFk5dJ7ax8Wfbv/FLlFGbIwyEcwQn4cA4VuIYq1IDBCJ7gBV6dR+fZeXP\neZ605Zz5ThD9wPn4A+m2V+Q=</latexit>\nm\nFLw=\">AB+3icbVBNS8NAEN3Ur1q/oj16WSyCBymJFxUvBS9exArGFptSNtJu3SzCbubQgn1r3jxoOLVP+LNk3/FT\nduDtj4YeLw3szvzgoQzpR3nyosLa+srhXSxubW9s79u7evYpTScGjMY9lMyAKOBPgaY5NBMJAo4NILBZe43hiA\nVi8WdHiXQjkhPsJBRo3Uscv+EGjmD0AKJ9G4P+6EpY5dcarOBHiRuDNSqdnXN98X3Yd6x/70uzFNIxCacqJUyzVvtT\nMiNaMcxiU/VZAQOiA9aBkqSASqnU2WH+NDo3RxGEtTQuOJ+nsiI5FSoygwnRHRfTXv5eJ/XivV4Vk7YyJNQg6/ShMO\ndYxzpPAXSaBaj4yhFDJzK6Y9okVJu8hDc+ZMXiXdSPa+6t26ldoymKJ9dICOkItOUQ1doTryEUj9IRe0Kv1aD1\nb9b7tLVgzWbK6A+sjx9Ewpbx</latexit>\nm\nFLw=\">AB+3icbVBNS8NAEN3Ur1q/oj16WSyCBymJFxUvBS9exArGFptSNtJu3SzCbubQgn1r3jxoOLVP+LNk3/FT\nduDtj4YeLw3szvzgoQzpR3nyosLa+srhXSxubW9s79u7evYpTScGjMY9lMyAKOBPgaY5NBMJAo4NILBZe43hiA\nVi8WdHiXQjkhPsJBRo3Uscv+EGjmD0AKJ9G4P+6EpY5dcarOBHiRuDNSqdnXN98X3Yd6x/70uzFNIxCacqJUyzVvtT\nMiNaMcxiU/VZAQOiA9aBkqSASqnU2WH+NDo3RxGEtTQuOJ+nsiI5FSoygwnRHRfTXv5eJ/XivV4Vk7YyJNQg6/ShMO\ndYxzpPAXSaBaj4yhFDJzK6Y9okVJu8hDc+ZMXiXdSPa+6t26ldoymKJ9dICOkItOUQ1doTryEUj9IRe0Kv1aD1\nb9b7tLVgzWbK6A+sjx9Ewpbx</latexit>\nh\ncxc=\">AB+3icbVBNS8NAEN34WetXtEcvi0XwICXxot4KXjxWMLbQhLZTtqlm03Y3RCqH/FiwcVr/4Rb/4bN20O2\nvpg4PHezO7MC1POlHacb2tfWNza7u2U9/d2z84tI+OH1WSQoeTXgieyFRwJkATzPNoZdKIHIoRtObku/OwWpWCI\nedJ5CEJORYBGjRBtpYDf8KdDCn4AUTqrxeDaI6gO76bScOfAqcSvSRBU6A/vLHyY0i0FoyolSfde8FREakY5zOp+pi\nAldEJG0DdUkBhUMyXn+EzowxlEhTQuO5+nuiILFSeRyazpjosVr2SvE/r5/p6DomEgzDYIuPoyjnWCyTwkEmgm\nueGECqZ2RXTMZGEapNXGYK7fPIq8S5bNy3m2L6o0augEnaJz5KIr1EZ3qIM8RFGOntErerOerBfr3fpYtK5Z1Uw\nD/YH1+QOP4pQt</latexit>~hf\nz\nXQM=\">AB+3icbVA9SwNBEJ2LXzF+na0WQyChYQ7G7UL2NiIEYwJCHsbeaSJXt7x+5eIBzxr9hYqNj6R+zs/CluP\ngpNfDweG9md+YFieDaeN6Xk1tZXVvfyG8WtrZ3dvfc/YMHaeKY3FIlaNgGoUXGLNcCOwkSikUSCwHgyuJn59iEr\nzWN6bUYLtiPYkDzmjxkodt9gaIstaA1TSwzpjzthoeOWvLI3BVkm/pyUKu7N7TcAVDvuZ6sbszRCaZigWjd9+1Y7o8\npwJnBcaKUaE8oGtIdNSyWNULez6fJjcmyVLgljZUsaMlV/T2Q0noUBbYzoqavF72J+J/XTE140c64TFKDks0+ClNBT\nEwmSZAuV8iMGFlCmeJ2V8L6VFmbF6TEPzFk5dJ7ax8Wfbv/FLlFGbIwyEcwQn4cA4VuIYq1IDBCJ7gBV6dR+fZeXP\neZ605Zz5ThD9wPn4A+m2V+Q=</latexit>\nm\nFLw=\">AB+3icbVBNS8NAEN3Ur1q/oj16WSyCBymJFxUvBS9exArGFptSNtJu3SzCbubQgn1r3jxoOLVP+LNk3/FT\nduDtj4YeLw3szvzgoQzpR3nyosLa+srhXSxubW9s79u7evYpTScGjMY9lMyAKOBPgaY5NBMJAo4NILBZe43hiA\nVi8WdHiXQjkhPsJBRo3Uscv+EGjmD0AKJ9G4P+6EpY5dcarOBHiRuDNSqdnXN98X3Yd6x/70uzFNIxCacqJUyzVvtT\nMiNaMcxiU/VZAQOiA9aBkqSASqnU2WH+NDo3RxGEtTQuOJ+nsiI5FSoygwnRHRfTXv5eJ/XivV4Vk7YyJNQg6/ShMO\ndYxzpPAXSaBaj4yhFDJzK6Y9okVJu8hDc+ZMXiXdSPa+6t26ldoymKJ9dICOkItOUQ1doTryEUj9IRe0Kv1aD1\nb9b7tLVgzWbK6A+sjx9Ewpbx</latexit>\nm\nFLw=\">AB+3icbVBNS8NAEN3Ur1q/oj16WSyCBymJFxUvBS9exArGFptSNtJu3SzCbubQgn1r3jxoOLVP+LNk3/FT\nduDtj4YeLw3szvzgoQzpR3nyosLa+srhXSxubW9s79u7evYpTScGjMY9lMyAKOBPgaY5NBMJAo4NILBZe43hiA\nVi8WdHiXQjkhPsJBRo3Uscv+EGjmD0AKJ9G4P+6EpY5dcarOBHiRuDNSqdnXN98X3Yd6x/70uzFNIxCacqJUyzVvtT\nMiNaMcxiU/VZAQOiA9aBkqSASqnU2WH+NDo3RxGEtTQuOJ+nsiI5FSoygwnRHRfTXv5eJ/XivV4Vk7YyJNQg6/ShMO\ndYxzpPAXSaBaj4yhFDJzK6Y9okVJu8hDc+ZMXiXdSPa+6t26ldoymKJ9dICOkItOUQ1doTryEUj9IRe0Kv1aD1\nb9b7tLVgzWbK6A+sjx9Ewpbx</latexit>\nh\ncxc=\">AB+3icbVBNS8NAEN34WetXtEcvi0XwICXxot4KXjxWMLbQhLZTtqlm03Y3RCqH/FiwcVr/4Rb/4bN20O2\nvpg4PHezO7MC1POlHacb2tfWNza7u2U9/d2z84tI+OH1WSQoeTXgieyFRwJkATzPNoZdKIHIoRtObku/OwWpWCI\nedJ5CEJORYBGjRBtpYDf8KdDCn4AUTqrxeDaI6gO76bScOfAqcSvSRBU6A/vLHyY0i0FoyolSfde8FREakY5zOp+pi\nAldEJG0DdUkBhUMyXn+EzowxlEhTQuO5+nuiILFSeRyazpjosVr2SvE/r5/p6DomEgzDYIuPoyjnWCyTwkEmgm\nueGECqZ2RXTMZGEapNXGYK7fPIq8S5bNy3m2L6o0augEnaJz5KIr1EZ3qIM8RFGOntErerOerBfr3fpYtK5Z1Uw\nD/YH1+QOP4pQt</latexit>\nh’0\nx0\nx1\nx2\nh0\n~hb\nm\nM74=\">AB+3icbVA9SwNBEJ2LXzF+RWNnsxgECwl3NmoXsLGM4JlALoS9zSRZsrd37O4FjiP+FRsLFVv/iJ3/xs1Ho\nYkPBh7vzezOvDARXBvX/XYKa+sbm1vF7dLO7t7+Qfnw6FHqWLos1jEqhVSjYJL9A03AluJQhqFApvh6HbqN8eoNI/\nlg8kS7ER0IHmfM2qs1C1XgjGyPBihkm5iyHDSDUvdctWtuTOQVeItSLXuBMdg0eiWv4JezNIpWGCat327FudnCrDmc\nBJKUg1JpSN6ADblkoaoe7ks+Un5MwqPdKPlS1pyEz9PZHTSOsCm1nRM1QL3tT8T+vnZr+dSfnMkNSjb/qJ8KYmIyT\nYL0uEJmRGYJZYrbXQkbUkWZsXlNQ/CWT14l/mXtpubde9X6BcxRhBM4hXPw4ArqcAcN8IFBs/wCm/Ok/PivDsf89a\nCs5ipwB84nz9415Ta</latexit>\nc\nWuQ=\">AB+3icbVBNSwMxE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JNnCstS/4sWDilf/iDd/jGD6c\ndDWBwOP92aSmRckgmvjOF+4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcCOgkCmgUCGgH45uZ356A0jy\nW9yZLoBfRoeQhZ9RYqV+t+RNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9auf/iBmaQTSMEG17r2rV5Ole\nFMwLTspxoSysZ0CF1LJY1A9/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+umJrzq5VwmqQHJFh+FqSAmJ\nrMkyIArYEZklCmuN2VsBFVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt6iFPMRQhp7QC3rFj/gZv+H\n3RWsBL2dq6A/wxw+EqJZi</latexit>\nc\nWuQ=\">AB+3icbVBNSwMxE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JNnCstS/4sWDilf/iDd/jGD6c\ndDWBwOP92aSmRckgmvjOF+4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcCOgkCmgUCGgH45uZ356A0jy\nW9yZLoBfRoeQhZ9RYqV+t+RNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9auf/iBmaQTSMEG17r2rV5Ole\nFMwLTspxoSysZ0CF1LJY1A9/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+umJrzq5VwmqQHJFh+FqSAmJ\nrMkyIArYEZklCmuN2VsBFVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt6iFPMRQhp7QC3rFj/gZv+H\n3RWsBL2dq6A/wxw+EqJZi</latexit>\nd\nm1c=\">AB+3icbVBNS8NAEN34WetXtEcvi0XwICXxot4KXjxWMLbQhLDZTtulm03Y3RCqH/FiwcVr/4Rb/4bN20O2\nvpg4PHezO7Mi1LOlHacb2tfWNza7u2U9/d2z84tI+OH1WSQoeTXgiexFRwJkATzPNoZdKIHEoRtNbku/OwWpWCI\nedJ5CEJORYENGiTZSaDf8KdDCn4AUTqrxeBZG9dBuOi1nDrxK3Io0UYVOaH/5g4RmMQhNOVGq75q3goJIzSiHWd3PFK\nSETsgI+oYKEoMKivnyM3xmlAEeJtKU0Hiu/p4oSKxUHkemMyZ6rJa9UvzP62d6eB0UTKSZBkEXHw0zjnWCyTwgEmgm\nueGECqZ2RXTMZGEapNXGYK7fPIq8S5bNy3m2L6o0augEnaJz5KIr1EZ3qIM8RFGOntErerOerBfr3fpYtK5Z1Uw\nD/YH1+QOJ0pQp</latexit>~hb\nm\nM74=\">AB+3icbVA9SwNBEJ2LXzF+RWNnsxgECwl3NmoXsLGM4JlALoS9zSRZsrd37O4FjiP+FRsLFVv/iJ3/xs1Ho\nYkPBh7vzezOvDARXBvX/XYKa+sbm1vF7dLO7t7+Qfnw6FHqWLos1jEqhVSjYJL9A03AluJQhqFApvh6HbqN8eoNI/\nlg8kS7ER0IHmfM2qs1C1XgjGyPBihkm5iyHDSDUvdctWtuTOQVeItSLXuBMdg0eiWv4JezNIpWGCat327FudnCrDmc\nBJKUg1JpSN6ADblkoaoe7ks+Un5MwqPdKPlS1pyEz9PZHTSOsCm1nRM1QL3tT8T+vnZr+dSfnMkNSjb/qJ8KYmIyT\nYL0uEJmRGYJZYrbXQkbUkWZsXlNQ/CWT14l/mXtpubde9X6BcxRhBM4hXPw4ArqcAcN8IFBs/wCm/Ok/PivDsf89a\nCs5ipwB84nz9415Ta</latexit>\nc\nWuQ=\">AB+3icbVBNSwMxE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JNnCstS/4sWDilf/iDd/jGD6c\ndDWBwOP92aSmRckgmvjOF+4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcCOgkCmgUCGgH45uZ356A0jy\nW9yZLoBfRoeQhZ9RYqV+t+RNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9auf/iBmaQTSMEG17r2rV5Ole\nFMwLTspxoSysZ0CF1LJY1A9/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+umJrzq5VwmqQHJFh+FqSAmJ\nrMkyIArYEZklCmuN2VsBFVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt6iFPMRQhp7QC3rFj/gZv+H\n3RWsBL2dq6A/wxw+EqJZi</latexit>\nc\nWuQ=\">AB+3icbVBNSwMxE2q1lqtVnv0EiyCBym7XtRbwYvHCq4tdEvJprNtaDa7JNnCstS/4sWDilf/iDd/jGD6c\ndDWBwOP92aSmRckgmvjOF+4sLG5Vdwu7ZR39yr7B9XDowcdp4qBx2IRq05ANQguwTPcCOgkCmgUCGgH45uZ356A0jy\nW9yZLoBfRoeQhZ9RYqV+t+RNguT8GJZ3EkNG0H5T71brTcOYg68RdknoT+7XvSjFr9auf/iBmaQTSMEG17r2rV5Ole\nFMwLTspxoSysZ0CF1LJY1A9/L58lNyapUBCWNlSxoyV39P5DTSOosC2xlRM9Kr3kz8z+umJrzq5VwmqQHJFh+FqSAmJ\nrMkyIArYEZklCmuN2VsBFVlBmb1ywEd/XkdeJdNK4b7p1b56jBUroGJ2gM+SiS9REt6iFPMRQhp7QC3rFj/gZv+H\n3RWsBL2dq6A/wxw+EqJZi</latexit>\nd\nm1c=\">AB+3icbVBNS8NAEN34WetXtEcvi0XwICXxot4KXjxWMLbQhLDZTtulm03Y3RCqH/FiwcVr/4Rb/4bN20O2\nvpg4PHezO7Mi1LOlHacb2tfWNza7u2U9/d2z84tI+OH1WSQoeTXgiexFRwJkATzPNoZdKIHEoRtNbku/OwWpWCI\nedJ5CEJORYENGiTZSaDf8KdDCn4AUTqrxeBZG9dBuOi1nDrxK3Io0UYVOaH/5g4RmMQhNOVGq75q3goJIzSiHWd3PFK\nSETsgI+oYKEoMKivnyM3xmlAEeJtKU0Hiu/p4oSKxUHkemMyZ6rJa9UvzP62d6eB0UTKSZBkEXHw0zjnWCyTwgEmgm\nueGECqZ2RXTMZGEapNXGYK7fPIq8S5bNy3m2L6o0augEnaJz5KIr1EZ3qIM8RFGOntErerOerBfr3fpYtK5Z1Uw\nD/YH1+QOJ0pQp</latexit>\nC\nNN\nAttn\nGRU\nyt\n…\n…\nb\nUnsupervised contextual learning\nOnline transfer learning\nFig. 1.\nBidirectional sequence-to-sequence conversation model with multitask objective. The GRU blocks represent multi-layered RNNs using GRU units, C is\nthe concatenation function, and Attn is an attention mechanism.\nC. Online multitask label generation\nTo guide the embeddings in becoming more human-\nbehaviorally relevant, we select a multitask objective that\nattempts to classify the affective state of input sentences. The\ndeﬁnition of human behavior is more complicated than these\nstates, however we hypothesize this is a suitable method of\ntransferring related domain knowledge into the unsupervised\nsentence embeddings.\nWe generate the affective labels for each input during training\nusing an online mechanism. In our online approach we apply the\nsimplest method by automatically labeling inputs using a simple\nlook-up table of affective words [21]. Speciﬁcally, we use words\ncategorized in the two top-level affective states: negative and\npositive emotion. An input sentence is assigned a Negative\nor Positive label based on the count of words corresponding\nto each affective state. Although this labeling approach is\nextremely naive with a high rate of misclassiﬁcation, we\nhypothesize the inclusion of affective knowledge in embeddings\nwill be beneﬁcial in identifying more complex behaviors or\nemotions later. Some examples of affective words in our look-\nup table are shown in Table I.\nTABLE I\nEXAMPLES OF POSITIVE AND NEGATIVE AFFECT WORDS\nAffective State\nPositive\nNegative\ncute\nlove\nugly\nhate\nrich\nnice\nhurt\nnasty\nspecial\nsweet\nwicked\ndistraught\nforgive\nhandsome\nshame\noverwhelm\nIV. BEHAVIOR IDENTIFICATION USING EMBEDDINGS\nAfter unsupervised multitask training the encoder in the\nseq2seq model is used to extract embeddings for use as features\nin behavior identiﬁcation in long pieces of text (which we refer\nto as sessions). We deﬁne sentence embeddings to be the\nconcatenation of the ﬁnal output states of both the forward\nand backward RNNs in the encoder. We also concatenated the\noutput states from all the intermediate layers of the encoder.\nThis is an extension of history-of-word embeddings [22] and\nis motivated by the intuition that intermediate layers represent\ndifferent levels of concept. By utilizing intermediate represen-\ntations of the sentence, we hypothesize that more information\nrelated to human behavior can be captured. Annotation of\nhuman behavior using sentence embeddings was then applied\nusing various unsupervised and supervised methods.\nA. Unsupervised clustering\nAs an initial step we analyzed the performance of the\nembeddings on a behavior classiﬁcation task without any\nsupervision. We applied a simple k-means clustering method\non individual sentence embeddings to obtain multiple clusters.\nWe then labeled the clusters by randomly selecting a single\nseed session and assigning the session label to the centroid\nwhich the majority of embeddings in the session were closest\nto. During evaluation, session labels were predicted based on\nthe centroid which the majority of embeddings from the session\nwere closest to.\nB. k-Nearest neighbors\nFor supervised classiﬁcation we applied a simple k-nearest\nneighbor (k-NN) approach. In k-NN, an embedding is labeled\naccording to its k-nearest neighbors in the training set. The\nﬁnal session label was then obtained by a majority vote over\nall embeddings in the session.\nC. Rating estimation using neural networks\nFinally, we applied a neural network on top of the em-\nbeddings to estimate actual behavior ratings. For this section\nwe applied the framework proposed in [12]. Sessions were\nsegmented into sentences and represented as a sequence of\nembeddings. A sliding window of size 3 was applied over the\nembeddings followed by an RNN using LSTM units. The RNN\nwas trained to predict the session rating from each window. The\nﬁnal session label was obtained by training a Support Vector\nRegressor to map from the median of the window predictions\nto the session rating. For more details the reader can refer to\n[12].\n4\nV. EXPERIMENTAL SETUP\nA. Datasets\n1) OpenSubtitles: We used separate datasets to train the\nunsupervised and supervised portions of our proposed method.\nSince our ﬁnal task is behavior annotation of human inter-\naction, we wish to use a dataset that contains conversational\nspeech when learning the unsupervised sentence embedding.\nA natural choice for a source rich in dialogue is movie\nsubtitles. To this end we used the OpenSubtitles Corpus\n[23]. This corpus was generated using data from the website\nopensubtitles.org and contains user-submitted subtitles\nof movies and TV shows.\nWe applied additional pre-processing in addition to the steps\nalready taken in [23]. Mainly, we attempted to generate a\nback-and-forth conversation by taking consecutive lines in the\nsubtitles and assigning them as utterance and replies in an\ninteraction. As there is no speaker information in the corpus\nit is hard to distinguish between dialogues and monologues\nwithout the use of advanced content analysis methods. However,\nwe assume that this difference in conversational continuity\nwill be dampened by the large amount of data available. We\nalso assume that monologues also represent some form of\ninternal dialogue which closely ties with the concepts between\nsentences.\nFinally, we applied standard text processing techniques\nto clean up the text further. These included auto-correction\nof commonly misspelled words, contraction removal, and\nreplacement of proper nouns through parts-of-speech tagging.\nThe ﬁnal unsupervised training set consists of 30 million\nsentence pairs.\n2) Couples Therapy Corpus: We applied our unsupervised\nsentence embeddings to the task of annotating behaviors in\nhuman interactions. For this we used data from the UCLA/UW\nCouple Therapy Research Project [24] which contains record-\nings of 134 real couples with marital issues interacting over\nmultiple sessions. In each session the couples each discussed\na self-selected topic for around 10 minutes. The recordings of\nthe session were then rated by multiple annotators based on\nthe Couples Interaction [25] and Social Support [26] Rating\nSystems. This rating system describes 33 behavioral codes rated\non a Likert scale of 1 to 9, where 1 indicates strong absence and\n9 indicates strong presence of the given behavior. The number\nof annotators per session ranged from 2 to 12, however the\nmajority of sessions (∼90%) had 3 to 4 annotators. Annotator\nratings were then averaged to obtain a 33 dimensional vector\nof behavior ratings per interlocutor for every session. The\nratings were binarized to produce labels for the classiﬁcation\ntask and the Likert scale values were used for behavior rating\nestimation.\nIn this work we focused on the behaviors Acceptance, Blame,\nHumor, Sadness, Negativity, and Positivity. Similar to prior\nworks ([27], [12]) we used only the top and bottom 20% of\nthe dataset in terms of averaged behavior ratings. To train our\nmodels the dataset was split into train and test sets using a\nleave-one-couple-out scheme. That is, for each fold, one couple\nwas used as the test set and the remaining as the train set. This\nresulted in 85-fold cross-validation.\n3) IEMOCAP: We also evaluated the effectiveness of our\nsentence embeddings in emotion recognition using the Interac-\ntive Emotional Dyadic Motion Capture Database (IEMOCAP)\n[28]. This dataset contains recordings from ﬁve male-female\npairs of actors performing both scripted and improvised\ndyadic interactions. Utterances from the interactions were then\nrated by multiple annotators for dimensional and categorical\nemotions. Similar to other works [29], [30], we focused on four\ncategorical labels where there was majority agreement between\nannotators: happiness, sadness, anger, and neutral, with\nexcitement considered as happiness. We used the transcripts\nfrom the dataset and removed any acoustic annotations such\nas laughter or breathing. After discarding empty sentences our\nﬁnal dataset consisted of 5,500 utterances (1103 for anger,\n1078 for sadness, 1615 for happiness, and 1704 for neutral).\nTo train the supervised layers we used leave-one-pair-out which\nresulted in a 5-fold cross-validation scheme.\nB. Model architectures and training details\n1) Sentence embeddings: The sequence-to-sequence model\nwith multitask objective, shown in Figure 1, can be described\nas three sections: the encoder, the decoder, and the multitask\nnetwork. The encoder was constructed using a multi-layered\nbidirectional RNN using GRU units. We performed a grid\nsearch using hyper-parameter settings of 2 and 3 layers, and,\n100 and 300 dimensions in each direction per layer. For the\ndecoder a unidirectional RNN using GRU units was used\ninstead of bidirectional. The number of layers in the decoder\nwere the same as the encoder while the dimension size was\ndoubled to account for the concatenation of states and outputs\nfrom both directions.\nThe multitask network was implemented using a neural\nnetwork with four hidden layers of sizes 512, 512, 256, and\n128. We used rectiﬁed linear unit (ReLU) function as activation\nfunctions in the hidden layers and 2-dimensional softmax before\nthe ﬁnal output. No other network hyper-parameters were tried\nfor the multitask network.\nThe sentence embedding models were trained with the\nOpenSubtitles dataset for 5 epochs using SGD with momentum.\nThe learning rate was set to 0.05 and momentum set to 0.9. We\nalso reduced the learning rate by a factor of 10 every epoch.\n2) Supervised behavior annotation: Similar to [12] we used\nan RNN with LSTM units to estimate behavior ratings in the\nCouples Thearpy Corpus. The RNN had a single layer with a\ndimension size of 50 in the LSTM unit. A sigmoid function\nwas applied before the output to estimate the normalized rating\nvalue. In each fold one couple was randomly selected as\nvalidation to select the best model.\n3) Supervised emotion recognition: A neural network with\nfour hidden layers was used to classify emotions using\nembeddings of sentences from the IEMOCAP dataset. The\nhidden layers were of size 256 and used ReLU as the activation\nfunction. The model was trained for 20 epochs using Adagrad\n[31] as the optimization method. No other network hyper-\nparameters were tried for the emotion recognition network. A\nsubset of the training data (∼10%) was used as validation in\nselecting the best model.\n5\nTABLE II\nBEHAVIOR IDENTIFICATION ACCURACY (%) USING MULTITASK SENTENCE EMBEDDINGS\nMethod\nEmbedding Model\nAcceptance\nBlame\nNegativity\nPositivity\nSadness\nHumor\nMean Accuracy\nk-Means\nInferSent [14]\n58.9\n63.6\n61.4\n62.1\n58.9\n60.7\n60.93\nGenSen [20]\n53.9\n66.4\n61.4\n61.4\n59.6\n58.9\n60.27\nUniversal Sentence Encoder [15]\n59.3\n65.7\n61.8\n64.3\n59.6\n59.6\n61.72\nConversation Model [12]\n61.9\n65.4\n64.6\n65.7\n57.9\n59.1\n62.43\n+ Online MTL (proposed)\n64.0\n66.4\n65.0\n62.1\n61.4\n62.1\n63.50\nk-NN\nInferSent [14]\n83.2\n81.1\n85.4\n78.6\n65.7\n57.1\n75.27\nGenSen [20]\n85.0\n85.0\n85.7\n81.1\n63.2\n56.1\n76.02\nUniversal Sentence Encoder [15]\n80.0\n82.5\n83.9\n79.6\n66.8\n60.4\n75.53\nConversation Model [12]\n79.6\n80.0\n85.7\n82.5\n64.6\n59.6\n75.53\n+ Online MTL (proposed)\n85.0\n85.4\n87.9\n86.8\n67.9\n60.0\n78.77\nTABLE III\nWEIGHTED ACCURACY OF EMOTION RECOGNITION ON IEMOCAP\nMethod\nWA (%)\nLex-eVector [32]\n57.40\nE-vector + MCNN [30]\n59.63\nmLRF [33]\n63.80\nInferSent [14] + DNN\n62.60\nGenSen [20] + DNN\n60.62\nUniversal Sentence Encoder [15] + DNN\n64.83\nConversation Model [12] + DNN\n55.82\n+ Online MTL (proposed) + DNN\n63.84\nVI. EXPERIMENTAL RESULTS\nWe compared our unsupervised multitask sentence embed-\ndings to general purpose embeddings such as InferSent [14],\nGenSen [20], and Universal Sentence Encoder [15]. Table II\nshows the results of behavior identiﬁcation using sentence\nembeddings for different behaviors in the Couple Therapy\nCorpus. The addition of the multitask objective improved the\nclassiﬁcation accuracy of unsupervised sentence embeddings\nfrom the conversation model across all behaviors except\nPositivity in unsupervised classiﬁcation with k-Means. Under\nsupervised learning using k-NN, our multitask embeddings\nimproved accuracy on all behaviors except Humor. In terms\nof mean accuracy, our multitask embeddings performed better\nthan other sentence embeddings with an absolute improvement\nover no multitasking of 1.07% and 3.24% for unsupervised and\nsupervised methods respectively. Our multitask embeddings\nalso achieved the highest mean accuracy over all the sentence\nembeddings tested.\nThe results of emotion recognition on IEMOCAP are shown\nin Table III. In addition to general purpose embeddings we also\ncompared with other works that only used transcripts ([30],\n[32], [33] ). It should be noted that there is no consensus on\ndata split and evaluation conditions in IEMOCAP, and while we\nmade every effort to be consistent with other works the results\nmay not be directly comparable. However, when comparing\namong our implementation using sentence embeddings we\nobserved that online MTL improved the weighted accuracy\n(WA) of unsupervised embeddings by an absolute value of\n8.02% which is more than 14% relative improvement. The\nhighest accuracy was obtained using embeddings from the\nUniversal Sentence Encoder, however our implementation was\na close second by less than one percent.\nFinally, we analyzed the performance of our sentence embed-\ndings on Negativity classiﬁcation in behavior identiﬁcation over\nthe progression of training across different model architectures.\nFrom the standard error plot, shown in Figure 2, we can observe\nthat the addition of the multitask learning objective collectively\nincreases performance in the ﬁnal task. This shows that online\ntransfer learning through multitask was successful at improving\nthe performance of unsupervised sentence embeddings in our\nﬁnal task.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nIteration\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\nClassification accuracy (%)\nUnsupervised\nwith MTL\nFig. 2.\nStandard error plot of classiﬁcation accuracy on Negativity across\ncheckpoints for various model conﬁgurations.\nVII. CONCLUSION\nIn this work we explored the beneﬁts of introducing\nadditional objectives to unsupervised contextual learning of sen-\ntence embeddings. We found empirical evidence that supports\nthe hypothesis that multitask learning can increase affective\nconcepts in unsupervised sentence embeddings, even when the\nmultitask labels are generated online and extremely unreliable.\nOur proposed model has the beneﬁt of not requiring additional\neffort in generating or collecting data for multitasks. This allows\nlearning from large-scale corpora in an unsupervised manner\nwhile simultaneously applying transfer learning. In contrast\nto general purpose sentence embeddings, our model learns\n6\nsentence representations using less complex models and training\neffort, while at the same time yields higher performance in our\ntarget task. We argue that when learning sentence embeddings,\nit is more beneﬁcial to apply guided unsupervised learning\ninstead of overemphasis on universality before domain transfer.\nWhile we do expect that further improvements can be\nobtained through better labels for the multitask objective,\nthat would entail additional effort in system design and label\ngeneration. In addition, we also expect that multitask labels\nthat are too domain-speciﬁc (e.g. focusing on a speciﬁc way\nor deﬁnition of affective expression) may actually hinder the\nperformance of unsupervised embeddings. However, we do not\nverify this claim and leave it to future work.\nREFERENCES\n[1] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A\nreview and new perspectives,” IEEE transactions on pattern analysis\nand machine intelligence, vol. 35, no. 8, pp. 1798–1828, 2013.\n[2] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” in In Proceedings of Workshop at\nICLR, 2013.\n[3] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic represen-\ntations from tree-structured long short-term memory networks,” arXiv\npreprint arXiv:1503.00075, 2015.\n[4] C. dos Santos and M. Gatti, “Deep convolutional neural networks for\nsentiment analysis of short texts,” in Proceedings of COLING 2014, the\n25th International Conference on Computational Linguistics: Technical\nPapers, 2014, pp. 69–78.\n[5] A. Severyn and A. Moschitti, “Twitter sentiment analysis with deep\nconvolutional neural networks,” in Proceedings of the 38th International\nACM SIGIR Conference on Research and Development in Information\nRetrieval.\nACM, 2015, pp. 959–962.\n[6] A. Parikh, O. Täckström, D. Das, and J. Uszkoreit, “A decomposable\nattention model for natural language inference,” in Proceedings of the\n2016 Conference on Empirical Methods in Natural Language Processing,\n2016, pp. 2249–2255.\n[7] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for\ngenerating image descriptions,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2015, pp. 3128–3137.\n[8] S. Venugopalan, L. A. Hendricks, R. Mooney, and K. Saenko, “Improving\nlstm-based video description with linguistic knowledge mined from text,”\nin Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, 2016, pp. 1961–1966.\n[9] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence\nlearning with neural networks,” in Advances in Neural Information\nProcessing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger, Eds.\nCurran Associates, Inc.,\n2014, pp. 3104–3112. [Online]. Available: http://papers.nips.cc/paper/\n5346-sequence-to-sequence-learning-with-neural-networks.pdf\n[10] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun,\nA. Torralba, and S. Fidler, “Skip-thought vectors,” in Advances\nin Neural Information Processing Systems 28, C. Cortes, N. D.\nLawrence,\nD.\nD.\nLee,\nM.\nSugiyama,\nand\nR.\nGarnett,\nEds.\nCurran Associates, Inc., 2015, pp. 3294–3302. [Online]. Available:\nhttp://papers.nips.cc/paper/5950-skip-thought-vectors.pdf\n[11] H.\nPalangi,\nL.\nDeng,\nY.\nShen,\nJ.\nGao,\nX.\nHe,\nJ.\nChen,\nX. Song, and R. Ward, “Deep sentence embedding using long\nshort-term memory networks: Analysis and application to information\nretrieval,”\nIEEE/ACM\nTrans.\nAudio,\nSpeech\nand\nLang.\nProc.,\nvol.\n24,\nno.\n4,\npp.\n694–707,\nApr.\n2016.\n[Online].\nAvailable:\nhttp://dl.acm.org/citation.cfm?id=2992449.2992457\n[12] S.-Y. Tseng, B. Baucom, and P. Georgiou, “Approaching human perfor-\nmance in behavior estimation in couples therapy using deep sentence\nembeddings,” in Proceedings of Interspeech, August 2017.\n[13] F. Hill, K. Cho, and A. Korhonen, “Learning distributed representations of\nsentences from unlabelled data,” in Proceedings of the 2016 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2016, pp. 1367–1377.\n[14] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes,\n“Supervised learning of universal sentence representations from natural\nlanguage inference data,” in Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, 2017, pp. 670–680.\n[15] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant,\nM. Guajardo-Cespedes, S. Yuan, C. Tar et al., “Universal sentence\nencoder,” arXiv preprint arXiv:1803.11175, 2018.\n[16] D. Klein and C. D. Manning, The unsupervised learning of natural\nlanguage structure.\nStanford University Stanford, CA, 2005.\n[17] J. Yu and J. Jiang, “Learning sentence embeddings with auxiliary tasks\nfor cross-domain sentiment classiﬁcation,” in Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing.\nAustin, Texas: Association for Computational Linguistics, November\n2016, pp. 236–246. [Online]. Available: https://aclweb.org/anthology/\nD16-1023\n[18] M. Rei, “Semi-supervised multitask learning for sequence labeling,” arXiv\npreprint arXiv:1704.07156, 2017.\n[19] M.-T. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser, “Multi-\ntask sequence to sequence learning,” in International Conference on\nLearning Representations, 2016.\n[20] S. Subramanian, A. Trischler, Y. Bengio, and C. J. Pal, “Learning general\npurpose distributed sentence representations via large scale multi-task\nlearning,” in International Conference on Learning Representations, 2018.\n[21] Y. R. Tausczik and J. W. Pennebaker, “The psychological meaning\nof words: Liwc and computerized text analysis methods,” Journal of\nlanguage and social psychology, vol. 29, no. 1, pp. 24–54, 2010.\n[22] H.-Y. Huang, C. Zhu, Y. Shen, and W. Chen, “Fusionnet: Fusing via\nfully-aware attention with application to machine comprehension,” in\nInternational Conference on Learning Representations, 2018. [Online].\nAvailable: https://openreview.net/forum?id=BJIgi_eCZ\n[23] J. Tiedemann, “News from OPUS - A collection of multilingual parallel\ncorpora with tools and interfaces,” in Recent Advances in Natural Lan-\nguage Processing, N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov,\nEds.\nBorovets, Bulgaria: John Benjamins, Amsterdam/Philadelphia,\n2009, vol. V, pp. 237–248.\n[24] A. Christensen, D. Atkins, S. Berns, J. Wheeler, D. Baucom, and\nL. Simpson, “Traditional versus integrative behavioral couple therapy\nfor signiﬁcantly and chronically distressed married couples,” Journal of\nConsulting and Clinical Psychology, vol. 72, no. 2, pp. 176–191, 2004.\n[25] C. Heavey, D. Gill, and A. Christensen, “Couples interaction rating\nsystem 2 (cirs2),” University of California, Los Angeles, vol. 7, 2002.\n[26] J. Jones and A. Christensen, “Couples interaction study: Social sup-\nport interaction rating system,” University of California, Los Angeles,\nTechnical manual, 1998.\n[27] S. N. Chakravarthula, R. Gupta, B. Baucom, and P. Georgiou, “A\nlanguage-based generative model framework for behavioral analysis\nof couples’ therapy,” in Proceedings of IEEE International Conference\non Audio, Speech and Signal Processing (ICASSP), Apr. 2015.\n[28] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.\nChang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional\ndyadic motion capture database,” Language resources and evaluation,\nvol. 42, no. 4, p. 335, 2008.\n[29] H. M. Fayek, M. Lech, and L. Cavedon, “Evaluating deep learning\narchitectures for speech emotion recognition,” Neural Networks, vol. 92,\npp. 60–68, 2017.\n[30] J. Cho, R. Pappagari, P. Kulkarni, J. Villalba, Y. Carmiel, and N. Dehak,\n“Deep neural networks for emotion recognition combining audio and\ntranscripts,” Proc. Interspeech 2018, pp. 247–251, 2018.\n[31] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for\nonline learning and stochastic optimization,” Journal of Machine Learning\nResearch, vol. 12, no. Jul, pp. 2121–2159, 2011.\n[32] Q. Jin, C. Li, S. Chen, and H. Wu, “Speech emotion recognition with\nacoustic and lexical features,” in Acoustics, Speech and Signal Processing\n(ICASSP), 2015 IEEE International Conference on.\nIEEE, 2015, pp.\n4749–4753.\n[33] K. W. Gamage, V. Sethu, and E. Ambikairajah, “Salience based lexical\nfeatures for emotion recognition,” in Acoustics, Speech and Signal\nProcessing (ICASSP), 2017 IEEE International Conference on.\nIEEE,\n2017, pp. 5830–5834.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-07-18",
  "updated": "2018-11-01"
}