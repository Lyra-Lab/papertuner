{
  "id": "http://arxiv.org/abs/1807.07987v2",
  "title": "Deep Learning",
  "authors": [
    "Nicholas G. Polson",
    "Vadim O. Sokolov"
  ],
  "abstract": "Deep learning (DL) is a high dimensional data reduction technique for\nconstructing high-dimensional predictors in input-output models. DL is a form\nof machine learning that uses hierarchical layers of latent features. In this\narticle, we review the state-of-the-art of deep learning from a modeling and\nalgorithmic perspective. We provide a list of successful areas of applications\nin Artificial Intelligence (AI), Image Processing, Robotics and Automation.\nDeep learning is predictive in its nature rather then inferential and can be\nviewed as a black-box methodology for high-dimensional function estimation.",
  "text": "Deep Learning\nNicholas G. Polson ∗\nVadim O. Sokolov †\nFirst Draft: December 2017\nThis Draft: July 2018\nAbstract\nDeep learning (DL) is a high dimensional data reduction technique for constructing\nhigh-dimensional predictors in input-output models. DL is a form of machine learning\nthat uses hierarchical layers of latent features. In this article, we review the state-of-\nthe-art of deep learning from a modeling and algorithmic perspective. We provide a\nlist of successful areas of applications in Artiﬁcial Intelligence (AI), Image Processing,\nRobotics and Automation. Deep learning is predictive in its nature rather then infer-\nential and can be viewed as a black-box methodology for high-dimensional function\nestimation.\nKey words: Deep Learning, Machine Learning, Massive Data, Data Science, Kolmogorov-\nArnold Representation, GANs, Dropout, Network Architecture.\n∗Booth School of Business, University of Chicago.\n†George Mason University.\n1\narXiv:1807.07987v2  [stat.ML]  3 Aug 2018\n1\nIntroduction\nPrediction problems are of great practical and theoretical interest. Deep learning is a form of\nmachine learning which provides a tool box for high-dimensional function estimations. It uses\nhierarchical layers of hidden features to model complex nonlinear high-dimensional input-\noutput models. As statistical predictors, DL have a number of advantages over traditional\napproaches, including\n(a) input data can include all data of possible relevance to the prediction problem at hand\n(b) nonlinearities and complex interactions among input data are accounted for seamlessly\n(c) overﬁtting is more easily avoided than traditional high dimensional procedures\n(d) there exists fast, scale computational frameworks (TensorFlow, PyTorch).\nThere are many successful applications of deep learning across many ﬁelds, including speech\nrecognition, translation, image processing, robotics, engineering, data science, healthcare\namong many others. These applications include algorithms such as\n(a) Google Neural Machine Translation [Wu et al., 2016] closes the gap with humans in\naccuracy of the translation by 55-85% (estimated by people on a 6-point scale). One\nof the keys to success of the model is the use of Google’s huge dataset.\n(b) Chat bots which predict natural language response have been available for many\nyears. Deep learning networks can signiﬁcantly improve the performance of chatbots\n[Henderson et al., 2017]. Nowadays they provide help systems for companies and home\nassistants such as Amazon’s Alexa and Google home.\n(c) Google WaveNet (developed by DeepMind [Oord et al., 2016]), generates speech from\ntext and reduces the gap between the state of the art and human-level performance by\nover 50% for both US English and Mandarin Chinese.\n(d) Google Maps were improved after deep learning was developed to analyze more then 80\nbillion Street View images and to extract names of roads and businesses [Wojna et al., 2017].\n(e) Health care diagnostics were developed using Adversarial Auto-encoder model found\nnew molecules to ﬁght cancer. Identiﬁcation and generation of new compounds was\nbased on available biochemical data [Kadurin et al., 2017].\n(f) Convolutional Neural Nets (CNNs), which are central to image processing, were de-\nveloped to detect pneumonia from chest X-rays with better accuracy then practicing\nradiologists [Rajpurkar et al., 2017]. Another CNN model is capable of identifying skin\ncancer from biopsy-labeled test images [Esteva et al., 2017].\n(g) [Shallue and Vanderburg, 2017] discovered two new planets using deep learning and\ndata from NASA’s Kepler Space Telescope.\n(h) In more traditional engineering, science applications, such as spatio-temporal and ﬁ-\nnancial analysis deep learning showed superior performance compared to traditional\nstatistical learning techniques [Polson and Sokolov, 2017b, Dixon et al., 2017, Heaton et al., 2017,\nSokolov, 2017, Feng et al., 2018b, Feng et al., 2018a]\n2\nThe rest of our article proceeds as follows. Section 2 reviews mathematical aspects of deep\nlearning and popular network architectures. Section 3 provides overview of algorithms used\nfor estimation and prediction. Finally, Section 4 discusses some theoretical results related\nto deep learning.\n2\nDeep Learning\nDeep learning is data intensive and provides predictor rules for new high-dimensional input\ndata. The fundamental problem is to ﬁnd a predictor ˆY (X) of an output Y . Deep learning\ntrains a model on data by passing learned features of data through diﬀerent “layers” of\nhidden features. That is, raw data is entered at the bottom level, and the desired output is\nproduced at the top level, the result of learning through many levels of transformed data.\nDeep learning is hierarchical in the sense that in every layer, the algorithm extracts features\ninto factors, and a deeper level’s factors become the next level’s features.\nConsider a high dimensional matrix X containing a large set of potentially relevant data.\nLet Y represent an output (or response) to a task which we aim to solve based on the in-\nformation in X. This leads to an input-output map Y = F(X) where X = (X1, . . . , Xp).\n[Breiman, 2001] summaries the diﬀerence between statistical and machine learning philoso-\nphy as follows.\n“There are two cultures in the use of statistical modeling to reach conclusions from\ndata. One assumes that the data are generated by a given stochastic data model. The\nother uses algorithmic models and treats the data mechanism as unknown.\nAlgorithmic modeling, both in theory and practice, has developed rapidly in ﬁelds\noutside statistics. It can be used both on large complex data sets and as a more accurate\nand informative alternative to data modeling on smaller data sets. If our goal as a ﬁeld\nis to use data to solve problems, then we need to move away from exclusive dependence\non data models and adopt a more diverse set of tools.”\n2.1\nNetwork Architectures\nA deep learning architecture can be described as follows. Let f1, . . . , fL be given univariate\nactivation functions for each of the L layers. Activation functions are nonlinear transforma-\ntions of weighted data. A semi-aﬃne activation rule is then deﬁned by\nf W,b\nl\n:= fl\n Nl\nX\nj=1\nWljXj + bl\n!\n= fl(WlXl + bl) ,\n3\nwhich implicitly needs the speciﬁcation of the number of hidden units Nl. Our deep predictor,\ngiven the number of layers L, then becomes the composite map\nˆY (X) := F(X) =\n\u0010\nf W1,b1\nl\n◦. . . ◦f WL,bL\nL\n\u0011\n(X) .\n(1)\nThe fact that DL forms a universal ‘basis’ which we recognise in this formulation dates to\nPoincare and Hilbert is central. From a practical perspective, given a large enough data set\nof “test cases”, we can empirically learn an optimal predictor.\nSimilar to a classic basis decomposition, the deep approach uses univariate activation func-\ntions to decompose a high dimensional X.\nLet Z(l) denote the lth layer, and so X = Z(0). The ﬁnal output Y can be numeric or\ncategorical. The explicit structure of a deep prediction rule is then\nˆY (X) = W (L)Z(L) + b(L)\nZ(1) = f (1) \u0000W (0)X + b(0)\u0001\nZ(2) = f (2) \u0000W (1)Z(1) + b(1)\u0001\n. . .\nZ(L) = f (L) \u0000W (L−1)Z(L−1) + b(L−1)\u0001\n.\nHere W (l) is a weight matrix and b(l) are threshold or activation levels. Designing a good\npredictor depends crucially on the choice of univariate activation functions f (l). The Z(l) are\nhidden features which the algorithm will extract.\nPut diﬀerently, the deep approach employs hierarchical predictors comprising of a series of\nL nonlinear transformations applied to X. Each of the L transformations is referred to as a\nlayer, where the original input is X, the output of the ﬁrst transformation is the ﬁrst layer,\nand so on, with the output ˆY as the ﬁrst layer. The layers 1 to L are called hidden layers.\nThe number of layers L represents the depth of our routine.\nFigure 1 illustrates a number of commonly used structures; for example, feed-forward archi-\ntectures, auto-encoders, convolutional, and neural Turing machines. Once you have learned\nthe dimensionality of the weight matrices which are non-zero, there’s an implied network\nstructure.\nStacked GLM.\nFrom a statistical viewpoint, deep learning models can be viewed as\nstacked Generalized Linear Models [Polson and Sokolov, 2017a]. The expectation over de-\npendent variable in GLM is computed using aﬃne transformation (linear regression model)\nfollowed by a non-linear univariate function (inverse of the link function). GLM is given by\nE(y | x) = g−1(wTx).\n4\nChoice of link function is deﬁned by the target distribution p(y | x). For example when p is\nbinomial, we choose g−1 to be sigmoid 1/(1 + exp(−wTx)).\nNeural Turing Machine\nAuto-encoder\nConvolution\nRecurrent\nLong / short term memory\nGAN\nFigure 1: Commonly used deep learning architectures. Each circle is a neuron which cal-\nculates a weighted sum of an input vector plus bias and applies a non-linear function to\nproduce an output. Yellow and red colored neurons are input-output cells correspondingly.\nPink colored neurons apply weights inputs using a kernel matrix. Green neurons are hidden\nones. Blue neurons are recurrent ones and they append its values from previous pass to the\ninput vector. Blue neuron with circle inside a neuron corresponds to a memory cell. Source:\nhttp://www.asimovinstitute.org/neural-network-zoo.\nRecently deep architectures (indicating non-zero weights) include convolutional neural net-\nworks (CNN), recurrent NN (RNN), long short-term memory (LSTM), and neural Turing\nmachines (NTM). [Pascanu et al., 2013] and [Mont´ufar and Morton, 2015] provide results\non the advantage of representing some functions compactly with deep layers. [Poggio, 2016]\nextends theoretical results on when deep learning can be exponentially better than shallow\nlearning. [Bryant, 2008] implements [Sprecher, 1972] algorithm to estimate the non-smooth\ninner link function. In practice, deep layers allow for smooth activation functions to provide\n“learned” hyper-planes which ﬁnd the underlying complex interactions and regions without\nhaving to see an exponentially large number of training samples.\nCommonly used activation functions are sigmoidal (cosh or tanh), heaviside gate func-\ntions I(x > 0), or rectiﬁed linear units (ReLU) max{·, 0}. ReLU’s especially have been\nfound [Schmidt-Hieber, 2017] to lend themselves well to rapid dimension reduction. A deep\nlearning predictor is a data reduction scheme that avoids the curse of dimensionality through\nthe use of univariate activation functions. One particular feature is that the weight matrices\n5\nWl ∈ℜNl×Nl−1 are matrix valued. This gives the predictor great ﬂexibility to uncover non-\nlinear features of the data – particularly so in ﬁnance data as the estimated hidden features\nZ(l) can be given the interpretation of portfolios of payouts. The choice of the dimension Nl\nis key, however, since if a hidden unit (aka columns of Wl) is dropped at layer l it kills all\nterms above it in the layered hierarchy.\n2.2\nAutoencoder\nAn autoencoder is a deep learning routine which trains F(X) to approximate X (i.e., X = Y )\nvia a bottleneck structure, which means we select a model F = f W1,b1\nl\n◦. . . ◦f WL,bL\nL\nwhich\naims to concentrate the information required to recreate X. Put diﬀerently, an autencoder\ncreates a more cost eﬀective representation of X.\nFor example, for a static autoencoder with two linear layers (a .k.a. traditional factor model),\nwe write\nY = W1 (W2X) .\nwhere x ∈ℜk and Whidden ∈ℜp×k and Wout ∈ℜk×p where p ≪k. The goal of an autoencoder\nis to train the weights so that y = x with loss function typically given by squared errors.\nIf W2 is estimated from the structure of the training data matrix, then we have a traditional\nfactor model, and the W1 matrix provides the factor loadings.\nWe note, that principal\ncomponent analysis (PCA) in particular falls into this category, as we have seen in (2). If\nW2 is estimated based on the pair ˆX = {Y, X} = X (which means estimation of W2 based\non the structure of the training data matrix with the speciﬁc autoencoder objective), then\nwe have a sliced inverse regression model. If W1 and W2 are simultaneously estimated based\non the training data X, then we have a two layer deep learning model.\nA dynamic one layer autoencoder for a ﬁnancial time series (Yt) can, for example, be written\nas a coupled system of the form\nYt = WxXt + WyYt−1 and\n\u0012 Xt\nYt−1\n\u0013\n= WYt .\nWe then need to learn the weight matrices Wx and Wy. Here, the state equation encodes\nand the matrix W decodes the Yt vector into its history Yt−1 and the current state Xt.\nThe auto encoder demonstrates nicely that in deep learning we do not have to model the\nvariance-covariance matrix explicitly, as our model is already directly in predictive form.\n(Given an estimated nonlinear combination of deep learners, there is an implicit variance-\ncovariance matrix, but that is not the driver of the method.)\n6\n2.3\nFactor Models\nAlmost all shallow data reduction techniques can be viewed as consisting of a low dimensional\nauxiliary variable Z and a prediction rule speciﬁed by a composition of functions\nˆY = f W1,b1\n1\n(f2(W2X + b2)\n\u0001\n= f W1,b1\n1\n(Z), where Z := f2(W2X + b2) .\nIn this formulation, we also recognise the previously introduced deep learning structure (1).\nThe problem of high dimensional data reduction in general is to ﬁnd the Z-variable and to\nestimate the layer functions (f1, f2) correctly. In the layers, we want to uncover the low-\ndimensional Z-structure in a way that does not disregard information about predicting the\noutput Y .\nPrincipal component analysis (PCA), reduced rank regression (RRR), linear discriminant\nanalysis (LDA), projection pursuit regression (PPR), and logistic regression are all shallow\nlearners. For example, PCA reduces X to f2(X) using a singular value decomposition of the\nform\nZ = f2(X) = W ⊤X + b ,\n(2)\nwhere the columns of the weight matrix W form an orthogonal basis for directions of\ngreatest variance (which is in eﬀect an eigenvector problem).\nSimilarly, for the case of\nX = (x1, . . . , xp), PPR reduces X to f2(X) by setting\nZ = f2(X) =\nN1\nX\ni=1\ngi(Wi1x1 + . . . + Wipxp) .\nAs stated before, these types of dimension reduction is independent of y and can easily discard\ninformation that is valuable for predicting the desired output.\nSliced inverse regression\n(SIR) [Li, 1991] overcomes this drawback somewhat by estimating the layer function f2\nusing data on both, Y and X, but still operates independently of f1.\nDeep learning overcomes many classic drawbacks by jointly estimating f1 and f2 based on\nthe full training data ˆX = {Yi, Xi}T\ni=1, using information on Y and X as well as their\nrelationships, and by using L > 2 layers. If we choose to use nonlinear layers, we can view\na deep learning routine as a hierarchical nonlinear factor model or, more speciﬁcally, as a\ngeneralised linear model (GLM) with recursively deﬁned nonlinear link functions.\n[Diaconis and Shahshahani, 1984] use nonlinear functions of linear combinations. The hid-\nden factors zi = wi1b1 + . . . + wipbp represents a data reduction of the output matrix X. The\nmodel selection problem is to choose N1 (how many hidden units).\n7\n2.4\nGANs: Generative Adversarial Networks\nGAN has two components Generator Neural Network and Discriminator Neural Network.\nThe Generator Network G : z →x maps random z to a sample from the target distribution\np(x) and the Discriminator Network D(x) is a binary classiﬁer with two classes: generated\nsample and true sample.\nWe train GAN iteratively by switching between generator and discriminator. This can be\nrepresented mathematically as\nmin\nθG max\nθD V (D(x | θD), G(z | θG))\nV (D, G) = Ex∼p(x) [log D(x)] + Ez∼p(z) [log(1 −D(G(z))]\nIn V (D, G), the ﬁrst term is a deviance that penalizes for misclassiﬁcation of samples, the\ngoal is to have it close to 1. The second term is entropy that the data from random input\n(p(z)) passes through the generator, which then generates a fake sample which is then passed\nthrough the discriminator to identify the fakeness (aka worst case scenario). In this term,\nthe discriminator tries to maximize it to 0 (i.e. the log probability that the generated data\nis fake is equal to 0). So overall, the discriminator is trying to maximize our function V .\nOn the other hand, the task of generator is exactly opposite, i.e. it tries to minimize the\nfunction V so that the diﬀerentiation between real and fake data is a bare minimum. This,\nin other words is a cat and mouse game between generator and discriminator!\n3\nAlgorithmic Issues\n3.1\nTraining\nLet the training dataset be denoted by ˆX = {Yi, Xi}T\ni=1. Once the activation functions, size\nand depth of the learner have been chosen, we need to solve the training problem of ﬁnding\n( ˆW,ˆb) where ˆW = ( ˆW1, . . . , ˆWL) and ˆb = (ˆb1, . . . ,ˆbL) denote the learning parameters which\nwe compute during training. A more challenging problem, is training the size and depth\nNl, L, which is known as the model selection problem. To do this, we need a training dataset\nˆX = {Yi, Xi}T\ni=1 of input-output pairs, a loss function l(Y, ˆY ) at the level of the output\nsignal. In its simplest form, we simply solve\narg minW,b\n1\nN\nT\nX\ni=1\nl(Yi, ˆY (Xi)) ,\n(3)\n8\nAn L2-norm for a traditional least squares problem becomes a suitable error measure, and\nif we then minimise the loss function l(Yi, ˆY (Xi)) = ∥Yi −ˆY (Xi)∥2\n2 , our target function (3)\nbecomes the mean-squared error (MSE).\nIt is common to add a regularisation penalty φ(W, b) to avoid over-ﬁtting and to stabilise\nour predictive rule. We combine this with the loss function with a global parameter λ that\ngauges the overall level of regularisation. We now need to solve\narg minW,b\n1\nN\nT\nX\ni=1\nl(Yi, ˆY (Xi)) + λφ(W, b) ,\n(4)\nAgain we compute a nonlinear predictor ˆY = ˆY ˆ\nW,ˆb(X) of the output Y given the input X–the\ngoal of deep learning.\nIn a traditional probabilistic model p(Y |ˆY (X)) that generates the output Y given the pre-\ndictor ˆY (X), we have the natural loss function l(Y, ˆY ) = −log p(Y |ˆY ) as the negative\nlog-likelihood. For example, when predicting the probability of default, we have a multino-\nmial logistic regression model which leads to a cross-entropy loss function. For multivariate\nnormal models, which includes many ﬁnancial time series, the L2-norm of traditional least\nsquares becomes a suitable error measure.\nThe common numerical approach for the solution of (4) is a form of stochastic gradient de-\nscent, which adapted to a deep learning setting is usually called backpropagation [Rumelhart et al., 1986].\nOne caveat of backpropagation in this context is the multi-modality of the system to be solved\nand the resulting slow convergence properties, which is the main reason why deep learning\nmethods heavily rely on the availablility of large computational power.\nTo allow for cross validation [Hastie et al., 2016] during training, we may split our training\ndata into disjoint time periods of identical length, which is particularly desirable in ﬁnancial\napplications where reliable time consistent predictors are hard to come by and have to be\ntrained and tested extensively. Cross validation also provides a tool to decide what levels of\nregularisation lead to good generalisation (i.e., predictive) rules, which is the classic variance-\nbias trade-oﬀ. A key advantage of cross validation, over traditional statistical metrics such\nas t-ratios and p-values, is that we can also use it to assess the size and depth of the hidden\nlayers, that is, solve the model selection problem of choosing Nl for 1 ≤l ≤L and L using\nthe same predictive MSE logic. This ability to seamlessly solve the model selection and\nestimation problems is one of the reasons for the current widespread use of machine learning\nmethods.\n3.1.1\nApproximate Inference\nThe recent successful approaches to develop eﬃcient Bayesian inference algorithms for deep\nlearning networks are based on the reparameterization techniques for calculating Monte\n9\nCarlo gradients while performing variational inference. Given the data D = (X, Y ), the\nvariation inference relies on approximating the posterior p(θ | D) with a variation distribution\nq(θ | D, φ), where θ = (W, b). Then q is found by minimizing the based on the Kullback-\nLeibler divergence between the approximate distribution and the posterior, namely\nKL(q || p) =\nZ\nq(θ | D, φ) log q(θ | D, φ)\np(θ | D) dθ.\nSince p(θ | D) is not necessarily tractable, we replace minimization of KL(q || p) with\nmaximization of evidence lower bound (ELBO)\nELBO(φ) =\nZ\nq(θ | D, φ) log p(Y | X, θ)p(θ)\nq(θ | D, φ)\ndθ\nThe log of the total probability (evidence) is then\nlog p(D) = ELBO(φ) + KL(q || p)\nThe sum does not depend on φ, thus minimizing KL(q || p) is the same that maximizing\nELBO(q).\nAlso, since KL(q || p) ≥0, which follows from Jensen’s inequality, we have\nlog p(D) ≥ELBO(φ). Thus, the evidence lower bound name. The resulting maximization\nproblem ELBO(φ) →maxφ is solved using stochastic gradient descent.\nTo calculate the gradient, it is convenient to write the ELBO as\nELBO(φ) =\nZ\nq(θ | D, φ) log p(Y | X, θ)dθ −\nZ\nq(θ | D, φ) log q(θ | D, φ)\np(θ)\ndθ\nThe gradient of the ﬁrst term ∇φ\nR\nq(θ | D, φ) log p(Y | X, θ)dθ = ∇φEq log p(Y | X, θ) is\nnot an expectation and thus cannot be calculated using Monte Carlo methods. The idea is\nto represent the gradient ∇φEq log p(Y | X, θ) as an expectation of some random variable, so\nthat Monte Carlo techniques can be used to calculate it. There are two standard methods to\ndo it. First, the log-derivative trick, uses the following identity ∇xf(x) = f(x)∇x log f(x) to\nobtain ∇φEq log p(Y | θ). Thus, if we select q(θ | φ) so that it is easy to compute its derivative\nand generate samples from it, the gradient can be eﬃciently calculated using Monte Carlo\nmethods. Second, we can use the reparametrization trick by representing θ as a value of a\ndeterministic function, θ = g(ϵ, x, φ), where ϵ ∼r(ϵ) does not depend on φ. The derivative\nis given by\n∇φEq log p(Y | X, θ) =\nZ\nr(ϵ)∇φ log p(Y | X, g(ϵ, x, φ))dϵ\n= Eϵ[∇g log p(Y | X, g(ϵ, x, φ))∇φg(ϵ, x, φ)].\nThe reparametrization is trivial when q(θ | D, φ) = N(θ | µ(D, φ), Σ(D, φ)), and θ =\nµ(D, φ) + ϵΣ(D, φ), ϵ ∼N(0, I). [Kingma and Welling, 2013] propose using Σ(D, φ) = I\n10\nand representing µ(D, φ) and ϵ as outputs of a neural network (multi-layer perceptron), the\nresulting approach was called variational auto-encoder. A generalized reparametrization has\nbeen proposed by [Ruiz et al., 2016] and combines both log-derivative and reparametrization\ntechniques by assuming that ϵ can depend on φ.\n3.2\nDropout\nTo avoid overﬁtting in the training process, dropout is the technique [Srivastava et al., 2014]\nof removing input dimensions in X randomly with probability p. In eﬀect, this replaces the\ninput X by D ⋆X, where ⋆denotes the element-wise product and D is a matrix of Bernoulli\nB(p) random variables. For example, setting l(Y, ˆY ) = ∥Y −ˆY ∥2\n2 (to minimise the MSE as\nexplained above) and λ = 1, marginalised over the randomness, we then have a new objective\narg minW ED∼Ber(p)∥Y −W(D ⋆X)∥2\n2 ,\nwhich is equivalent to\narg minW ∥Y −pWX∥2\n2 + p(1 −p)∥ΓW∥2\n2 ,\nwhere Γ = (diag(X⊤X))\n1\n2. We can also interpret this last expression as a Bayesian ridge\nregression with a g-prior [Wager et al., 2013]. Put simply, dropout reduces the likelihood of\nover-reliance on small sets of input data in training.\narg minW,b\n1\nNl\nT\nX\ni=1\nl(Yi, ˆYi) + λφ(W, b),\n(5)\nAnother application of dropout regularisation is the choice of the number of hidden units in\na layer (if we drop units of the hidden rather than the input layer and then establish which\nprobability p gives the best results). It is worth recalling though, as we have stated before,\nthat one of the dimension reduction properties of a network structure is that once a variable\nfrom a layer is dropped, all terms above it in the network also disappear. This is just the\nnature of a composite structure for the deep predictor in (1).\n3.3\nBatch Normalization\nDropout is mostly a technique for regularization. It introduces noise into a neural network\nto force the neural network to learn to generalize well enough to deal with noise.\nBatch normalization [Ioﬀe and Szegedy, 2015] is mostly a technique for improving optimiza-\ntion. As a side eﬀect, batch normalization happens to introduce some noise into the network,\nso it can regularize the model a little bit.\n11\nWe normalize the input layer by adjusting and scaling the activations. For example, when\nwe have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed\nup learning. If the input layer is beneﬁting from it, why not do the same thing also for\nthe values in the hidden layers, that are changing all the time, and get 10 times or more\nimprovement in the training speed.\nBatch normalization reduces the amount by what the hidden unit values shift around (co-\nvariance shift). If an algorithm learned some X to Y mapping, and if the distribution of\nX changes, then we might need to retrain the learning algorithm by trying to align the\ndistribution of X with the distribution of Y . Also, batch normalization allows each layer of\na network to learn by itself a little bit more independently of other layers. When you have\na large dataset, it’s important to optimize well, and not as important to regularize well, so\nbatch normalization is more important for large datasets. You can of course use both batch\nnormalization and dropout at the same time\nWe can use higher learning rates because batch normalization makes sure that there no\nextremely high or low activations. And by that, things that previously couldn’t get to train,\nit will start to train. It reduces overﬁtting because it has a slight regularization eﬀects.\nSimilar to dropout, it adds some noise to each hidden layers’ activations.\nTherefore, if\nwe use batch normalization, we will use less dropout, which is a good thing because we\nare not going to lose a lot of information. However, we should not depend only on batch\nnormalization for regularization; we should better use it together with dropout. How does\nbatch normalization work? To increase the stability of a neural network, batch normalization\nnormalizes the output of a previous activation layer by subtracting the batch mean and\ndividing by the batch standard deviation.\nHowever, after this shift/scale of activation outputs by some randomly initialized parameters,\nthe weights in the next layer are no longer optimal. SGD ( Stochastic gradient descent)\nundoes this normalization. Consequently, batch normalization adds two trainable parameters\nto each layer, so the normalized output is multiplied by a “standard deviation” parameter\n(gamma) and adds a “mean” parameter (beta). In other words, batch normalization lets\nSGD do the denormalization by changing only these two weights for each activation, as\nfollows:\nµB = 1\nm\nm\nX\ni=1\nxi,\nσ2\nB = 1\nm\nm\nX\ni=1\n(xi −µB)2\nˆxi = xi −µB\np\nσ2\nB + ϵ\n,\nyi = γˆxi + β = BNγ,β(xi).\n12\n4\nDeep Learning Theory\nThere are two questions for which we do not yet have a comprehensive and a satisfactory\nanswer. The ﬁrst is, how to choose a deep learning architecture for a given problem. The\nsecond is why the deep learning model does so well on out-of-sample data, i.e. generalize.\nTo choose an appropriate architecture, from practical point of view, techniques such as\nDropout or universal architectures [Kaiser et al., 2017] allow us to spend less time on choos-\ning an architecture. Also some recent Bayesian theory sheds a light on the problem\n[Polson and Rockova, 2018]. However, it is still required to go through a trial-and-error pro-\ncess and empirically evaluate large number of models, before an appropriate architecture\ncould be found. On the other hand, there are some theoretical results that shed a light on\nthe architecture choice process.\nIt was long well known that shallow networks are universal approximators and thus can\nbe used to learn any input-output relations. The ﬁrst result in this direction was obtained\nby Kolmogorov [Kolmogorov, 1957] who has shown that any multivariate function can be\nexactly represented using operations of addition and superposition on univariate functions.\nFormally, there exist continuous functions ψpq, deﬁned on [0, 1] such that each continuous\nreal function f deﬁned on [0, 1]n is represented as\ng(x1, . . . , xn) =\n2n+1\nX\nq=1\nχq\n n\nX\np=1\nψpq(xp)\n!\n,\nwhere each χq is a continuous real function. This representation is a generalization of earlier\nresults [Kolmogorov, 1956, Arnold, 1963]. In [Kolmogorov, 1956] it was shown that every\ncontinuous multivariate function can be represented in the form of a ﬁnite superposition of\ncontinuous functions of not more than three variables. Later Arnold used that result to solve\nHilbert’s thirteenth problem [Arnold, 1963].\nHowever, results of Kolmogorov and Arnold do not have practical application. Their proofs\nare not constructive and do not demonstrate how functions χ and ψ can be computed.\nFurther, [Girosi and Poggio, 1989] and references therein show that those functions are not\nsmooth, while in practice smooth functions are used. Further, while functions ψ form a\nuniversal basis and do not depend on g, the function χ does depend on the speciﬁc form of\nfunction g. More practical results were obtained by Cybenko [Cybenko, 1989] who showed\nthat a shallow network with sigmoidal activation function can arbitrarily well approximate\nany continuous function on the n-dimensional cube [0, 1]n. Generalization for a broader class\nof activation functions was derived in [Hornik, 1991].\nRecently, it was shown that deep architectures require smaller number of parameters com-\npared to shallow ones to approximate the same function and thus more computationally vi-\nable. It was observed empirically that deep learners perform better. [Montufar et al., 2014]\nprovide some geometric intuition and show that the number of open box regions generated\n13\nby deep network is much lager than those generated by a shallow one with the same number\nof parameters. [Telgarsky, 2015] and [Safran and Shamir, 2016] provided speciﬁc examples\nof classes of functions that require an exponential number of parameters as a function of\ninput dimensionality to be approximated by shallow networks.\n[Poggio et al., 2017] provides a good overview of results on complexity of shallow and deep\nnetworks required to approximate diﬀerent classes of functions. For example, for shallow\n(one-layer) network\ng(x) ≈F(x) =\nN\nX\ni=i\nakf(wT\nk x + bk)\nwith f being inﬁnitively diﬀerentiable and not a polynomial, it is required that N =\nO(ϵ−n/m). Here x ∈[0, 1]n (n-dimensional cube), g(x) is is diﬀerentiable up to order m\nand ϵ is the required accuracy of the approximation, e.g. maxx |g(x) −f(x)| < ϵ. Thus, the\nsize of neurons required is exponential in the number of input dimensions, i.e. the curse of\ndimensionality.\nMeanwhile, if function g(x) has a special structure, then deep learner avoids the curse of\ndimensionality problem.\nSpeciﬁcally, let g(x) : Rn →R be a G–function, which is deﬁned as follows. Source nodes are\ncomponents of the input x1, . . . , xn, and there is only one sink node (value of the function.\nEach node v between the source and sink is a local function of dimensionality dv << n of\nlow dimensionality and mv times diﬀerentiable. For example, in G-function f(x1, · · · , x8) =\nh3(h21(h11(x1, x2), h12(x3, x4)), h22(h13(x5, x6), h14(x7, x8))) each h is “local”, e.g.\nrequires\nonly two-dimensional input to be computed, it has two hidden nodes and each has two inputs\n(dv = 2 for every hidden node). The required complexity of deep network to represent such\na function is given by\nNs = O\n X\nv∈V\nϵ−dv/mv\n!\n.\nThus, when the target function has local structures as deﬁned for a G–function, deep neural\nnetworks allow us to avoid curse of dimensionality.\nIn [Lin et al., 2017] the authors similarly show how the speciﬁc structure of the target func-\ntion to be approximated can be exploited to avoid the curse of dimensionality. Speciﬁcally,\nproperties frequently encountered in physics, such as symmetry, locality, compositionality,\nand polynomial log-probability translate into exceptionally simple neural networks and are\nshown to lead to low complexity deep network approximators.\nThe second question of why deep learning models do not overﬁt and generalize well to out-of-\nsample data has received less attention in the literature. It was shown in [Zhang et al., 2016]\nthat regularization techniques do not explain a surprisingly good out-of-sample performance.\nUsing a series of empirical experiments it was shown that deep learning models can learn\n14\nwhite noise. A contradictory result was obtained in [Liao et al., 2018] and shows that, for\na DL model with exponential loss function and appropriately normalized, there is a linear\ndependence of test loss on training loss.\nReferences\n[Arnold, 1963] Arnold, V. I. (1963). On functions of three variables. Dokl. Akad. Nauk\nSSSR, 14(4):679–681. [ English translation: American Mathematical Society Translation,\n2(28) (1963), pp. 51-54]. 13\n[Breiman, 2001] Breiman, L. (2001). Statistical Modeling: The Two Cultures (with com-\nments and a rejoinder by the author). Statistical Science, 16(3):199–231. 3\n[Bryant, 2008] Bryant, D. W. (2008). Analysis of Kolmogorov’s superpostion theorem and its\nimplementation in applications with low and high dimensional data. University of Central\nFlorida. 5\n[Cybenko, 1989] Cybenko, G. (1989). Approximation by superpositions of a sigmoidal func-\ntion. Mathematics of control, signals and systems, 2(4):303–314. 13\n[Diaconis and Shahshahani, 1984] Diaconis, P. and Shahshahani, M. (1984). On Nonlinear\nFunctions of Linear Combinations. SIAM Journal on Scientiﬁc and Statistical Computing,\n5(1):175–191. 7\n[Dixon et al., 2017] Dixon, M. F., Polson, N. G., and Sokolov, V. O. (2017). Deep learning\nfor spatio-temporal modeling: Dynamic traﬃc ﬂows and high frequency trading. arXiv\npreprint arXiv:1705.09851. 2\n[Esteva et al., 2017] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau,\nH. M., and Thrun, S. (2017). Dermatologist-level classiﬁcation of skin cancer with deep\nneural networks. Nature, 542(7639):115–118. 2\n[Feng et al., 2018a] Feng, G., He, J., and Polson, N. G. (2018a). Deep learning for predicting\nasset returns. arXiv preprint arXiv:1804.09314. 2\n[Feng et al., 2018b] Feng, G., Polson, N. G., and Xu, J. (2018b). Deep factor alpha. arXiv\npreprint arXiv:1805.01104. 2\n[Girosi and Poggio, 1989] Girosi, F. and Poggio, T. (1989). Representation properties of\nnetworks: Kolmogorov’s theorem is irrelevant. Neural Computation, 1(4):465–469. 13\n[Hastie et al., 2016] Hastie, T., Tibshirani, R., and Friedman, J. (2016). The Elements of\nStatistical Learning: Data Mining, Inference, and Prediction, Second Edition. Springer,\nNew York, NY, 2nd edition edition. 9\n15\n[Heaton et al., 2017] Heaton, J., Polson, N., and Witte, J. H. (2017). Deep learning for\nﬁnance: deep portfolios. Applied Stochastic Models in Business and Industry, 33(1):3–12.\n2\n[Henderson et al., 2017] Henderson, M., Al-Rfou, R., Strope, B., Sung, Y.-h., Lukacs, L.,\nGuo, R., Kumar, S., Miklos, B., and Kurzweil, R. (2017).\nEﬃcient natural language\nresponse suggestion for smart reply. arXiv preprint arXiv:1705.00652. 2\n[Hornik, 1991] Hornik, K. (1991). Approximation capabilities of multilayer feedforward net-\nworks. Neural networks, 4(2):251–257. 13\n[Ioﬀe and Szegedy, 2015] Ioﬀe, S. and Szegedy, C. (2015). Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In International Conference on\nMachine Learning, pages 448–456. 11\n[Kadurin et al., 2017] Kadurin, A., Aliper, A., Kazennov, A., Mamoshina, P., Vanhaelen, Q.,\nKhrabrov, K., and Zhavoronkov, A. (2017). The cornucopia of meaningful leads: Applying\ndeep adversarial autoencoders for new molecule development in oncology. Oncotarget,\n8(7):10883. 2\n[Kaiser et al., 2017] Kaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N.,\nJones, L., and Uszkoreit, J. (2017).\nOne model to learn them all.\narXiv preprint\narXiv:1706.05137. 13\n[Kingma and Welling, 2013] Kingma, D. P. and Welling, M. (2013). Auto-encoding varia-\ntional Bayes. arXiv preprint arXiv:1312.6114. 10\n[Kolmogorov, 1956] Kolmogorov, A. N. (1956). On the representation of continuous func-\ntions of several variables by superpositions of continuous functions of a smaller number of\nvariables. Dokl. Akad. Nauk SSSR, 108:179–182. [ English translation: American Mathe-\nmatical Society Translation, 17 (1961), pp. 369-373]. 13\n[Kolmogorov, 1957] Kolmogorov, A. N. (1957). On the representation of continuous func-\ntions of many variables by superposition of continuous functions of one variable and ad-\ndition. Dokl. Akad. Nauk SSSR, 114(5):953–956. [English translation: American Mathe-\nmatical Society Translation, 28 (2) (1963), pp. 55–59]. 13\n[Li, 1991] Li, K.-C. (1991). Sliced inverse regression for dimension reduction. Journal of the\nAmerican Statistical Association, 86(414):316–327. 7\n[Liao et al., 2018] Liao, Q., Miranda, B., Hidary, J., and Poggio, T. (2018). Classical gen-\neralization bounds are surprisingly tight for deep networks. Technical report, Center for\nBrains, Minds and Machines (CBMM). 14\n[Lin et al., 2017] Lin, H. W., Tegmark, M., and Rolnick, D. (2017). Why does deep and\ncheap learning work so well? Journal of Statistical Physics, 168(6):1223–1247. 14\n16\n[Mont´ufar and Morton, 2015] Mont´ufar, G. F. and Morton, J. (2015). When Does a Mixture\nof Products Contain a Product of Mixtures?\nSIAM Journal on Discrete Mathematics,\n29(1):321–347. 5\n[Montufar et al., 2014] Montufar, G. F., Pascanu, R., Cho, K., and Bengio, Y. (2014). On\nthe number of linear regions of deep neural networks. In Advances in neural information\nprocessing systems, pages 2924–2932. 13\n[Oord et al., 2016] Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,\nA., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016). Wavenet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499. 2\n[Pascanu et al., 2013] Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2013). How to\nConstruct Deep Recurrent Neural Networks. arXiv:1312.6026 [cs, stat]. arXiv: 1312.6026.\n5\n[Poggio, 2016] Poggio, T. (2016).\nDeep Learning:\nMathematics and Neuroscience.\nA\nSponsored Supplement to Science, Brain-Inspired intelligent robotics: The intersection\nof robotics and neuroscience:9–12. 5\n[Poggio et al., 2017] Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q. (2017).\nWhy and when can deep-but not shallow-networks avoid the curse of dimensionality: a\nreview. International Journal of Automation and Computing, 14(5):503–519. 14\n[Polson and Rockova, 2018] Polson, N. and Rockova, V. (2018). Posterior concentration for\nsparse deep learning. arXiv preprint arXiv:1803.09138. 13\n[Polson and Sokolov, 2017a] Polson, N. G. and Sokolov, V. (2017a).\nDeep Learning: A\nBayesian Perspective. Bayesian Analysis, 12(4):1275–1304. 4\n[Polson and Sokolov, 2017b] Polson, N. G. and Sokolov, V. O. (2017b). Deep learning for\nshort-term traﬃc ﬂow prediction. Transportation Research Part C: Emerging Technologies,\n79:1–17. 2\n[Rajpurkar et al., 2017] Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan,\nT., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K., et al. (2017).\nChexnet:\nRadiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint\narXiv:1711.05225. 2\n[Ruiz et al., 2016] Ruiz, F. R., AUEB, M. T. R., and Blei, D. (2016).\nThe generalized\nreparameterization gradient. In Advances in Neural Information Processing Systems, pages\n460–468. 11\n[Rumelhart et al., 1986] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learn-\ning representations by back-propagating errors. nature, 323(6088):533. 9\n17\n[Safran and Shamir, 2016] Safran, I. and Shamir, O. (2016). Depth separation in relu net-\nworks for approximating smooth non-linear functions. arXiv preprint arXiv:1610.09887.\n14\n[Schmidt-Hieber, 2017] Schmidt-Hieber, J. (2017).\nNonparametric regression using deep\nneural networks with relu activation function. arXiv preprint arXiv:1708.06633. 5\n[Shallue and Vanderburg, 2017] Shallue, C. J. and Vanderburg, A. (2017). Identifying exo-\nplanets with deep learning: A ﬁve planet resonant chain around kepler-80 and an eighth\nplanet around kepler-90. arXiv preprint arXiv:1712.05044. 2\n[Sokolov, 2017] Sokolov, V. (2017). Discussion of deep learning for ﬁnance: deep portfolios.\nApplied Stochastic Models in Business and Industry, 33(1):16–18. 2\n[Sprecher, 1972] Sprecher, D. A. (1972).\nA survey of solved and unsolved problems on\nsuperpositions of functions. Journal of Approximation Theory, 6(2):123–134. 5\n[Srivastava et al., 2014] Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I., and\nSalakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from over-\nﬁtting. Journal of Machine Learning Research, 15(1):1929–1958. 11\n[Telgarsky, 2015] Telgarsky, M. (2015). Representation beneﬁts of deep feedforward net-\nworks. arXiv preprint arXiv:1509.08101. 13\n[Wager et al., 2013] Wager, S., Wang, S., and Liang, P. S. (2013).\nDropout training as\nadaptive regularization.\nIn Advances in neural information processing systems, pages\n351–359. 11\n[Wojna et al., 2017] Wojna, Z., Gorban, A., Lee, D.-S., Murphy, K., Yu, Q., Li, Y., and\nIbarz, J. (2017). Attention-based extraction of structured information from street view\nimagery. arXiv preprint arXiv:1704.03549. 2\n[Wu et al., 2016] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W.,\nKrikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016).\nGoogle’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv\npreprint arXiv:1609.08144. 2\n[Zhang et al., 2016] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.\n(2016). Understanding deep learning requires rethinking generalization. arXiv preprint\narXiv:1611.03530. 14\n18\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-07-20",
  "updated": "2018-08-03"
}