{
  "id": "http://arxiv.org/abs/1807.08169v1",
  "title": "Recent Advances in Deep Learning: An Overview",
  "authors": [
    "Matiur Rahman Minar",
    "Jibon Naher"
  ],
  "abstract": "Deep Learning is one of the newest trends in Machine Learning and Artificial\nIntelligence research. It is also one of the most popular scientific research\ntrends now-a-days. Deep learning methods have brought revolutionary advances in\ncomputer vision and machine learning. Every now and then, new and new deep\nlearning techniques are being born, outperforming state-of-the-art machine\nlearning and even existing deep learning techniques. In recent years, the world\nhas seen many major breakthroughs in this field. Since deep learning is\nevolving at a huge speed, its kind of hard to keep track of the regular\nadvances especially for new researchers. In this paper, we are going to briefly\ndiscuss about recent advances in Deep Learning for past few years.",
  "text": "arXiv:1807.08169v1  [cs.LG]  21 Jul 2018\nRecent Advances in Deep Learning: An Overview\nMatiur Rahman Minar\nminar09.bd@gmail.com\nJibon Naher\njibon.naher09@gmail.com\nDepartment of Computer Science and Engineering\nChittagong University of Engineering and Technology\nChittagong-4349, Bangladesh\nEditor:\nAbstract\nDeep Learning is one of the newest trends in Machine Learning and Artiﬁcial Intelligence\nresearch. It is also one of the most popular scientiﬁc research trends now-a-days. Deep\nlearning methods have brought revolutionary advances in computer vision and machine\nlearning.\nEvery now and then, new and new deep learning techniques are being born,\noutperforming state-of-the-art machine learning and even existing deep learning techniques.\nIn recent years, the world has seen many major breakthroughs in this ﬁeld. Since deep\nlearning is evolving at a huge speed, its kind of hard to keep track of the regular advances\nespecially for new researchers. In this paper, we are going to brieﬂy discuss about recent\nadvances in Deep Learning for past few years.\nKeywords:\nNeural Networks, Machine Learning, Deep Learning, Recent Advances,\nOverview.\n1. Introduction\nThe term ”Deep Learning” (DL) was ﬁrst introduced to Machine Learning (ML) in 1986,\nand later used for Artiﬁcial Neural Networks (ANN) in 2000 (Schmidhuber, 2015). Deep\nlearning methods are composed of multiple layers to learn features of data with multiple\nlevels of abstraction (LeCun et al., 2015). DL approaches allow computers to learn compli-\ncated concepts by building them out of simpler ones (Goodfellow et al., 2016). For Artiﬁ-\ncial Neural Networks (ANN), Deep Learning (DL) aka hierarchical learning (Deng and Yu,\n2014) is about assigning credits in many computational stages accurately, to transform the\naggregate activation of the network (Schmidhuber, 2014). To learn complicated functions,\ndeep architectures are used with multiple levels of abstractions i.e. non-linear operations;\ne.g. ANNs with many hidden layers (Bengio, 2009). To sum it accurately, Deep Learning\nis a sub-ﬁeld of Machine Learning, which uses many levels of non-linear information pro-\ncessing and abstraction, for supervised or unsupervised feature learning and representation,\nclassiﬁcation and pattern recognition (Deng and Yu, 2014).\nDeep Learning i.e. Representation Learning is class or sub-ﬁeld of Machine Learning.\nRecent deep learning methods are mostly said to be developed since 2006 (Deng, 2011).\nThis paper is an overview of most recent techniques of deep learning, mainly recommended\nfor upcoming researchers in this ﬁeld. This article includes the basic idea of DL, major\napproaches and methods, recent breakthroughs and applications.\n1\nOverview papers are found to be very beneﬁcial, especially for new researchers in a\nparticular ﬁeld. It is often hard to keep track with contemporary advances in a research\narea, provided that ﬁeld has great value in near future and related applications. Now-a-days,\nscientiﬁc research is an attractive profession since knowledge and education are more shared\nand available than ever. For a technological research trend, its only normal to assume that\nthere will be numerous advances and improvements in various ways. An overview of an\nparticular ﬁeld from couple years back, may turn out to be obsolete today.\nConsidering the popularity and expansion of Deep Learning in recent years, we present\na brief overview of Deep Learning as well as Neural Networks (NN), and its major advances\nand critical breakthroughs from past few years. We hope that this paper will help many\nnovice researchers in this ﬁeld, getting an overall picture of recent Deep Learning researches\nand techniques, and guiding them to the right way to start with. Also we hope to pay\nsome tributes by this work, to the top DL and ANN researchers of this era, Geoﬀrey Hin-\nton (Hinton), Juergen Schmidhuber (Schmidhuber), Yann LeCun (LeCun), Yoshua Bengio\n(Bengio) and many others who worked meticulously to shape the modern Artiﬁcial Intel-\nligence (AI). Its also important to follow their works to stay updated with state-of-the-art\nin DL and ML research.\nIn this paper, ﬁrstly we will provide short descriptions of the past overview papers on\ndeep learning models and approaches. Then, we will start describing the recent advances of\nthis ﬁeld. We are going to discuss Deep Learning (DL) approaches, deep architectures i.e.\nDeep Neural Networks (DNN) and Deep Generative Models (DGM), followed by important\nregularization and optimization methods. Also, there are two brief sections for open-source\nDL frameworks and signiﬁcant DL applications. Finally, we will discuss about current status\nand the future of Deep Learning in the last two sections i.e. Discussion and Conclusion.\n2. Related works\nThere were many overview papers on Deep Learning (DL) in the past years. They described\nDL methods and approaches in great ways as well as their applications and directions for\nfuture research. Here, we are going to brief some outstanding overview papers on deep\nlearning.\nYoung et al. (2017) talked about DL models and architectures, mainly used in Natural\nLanguage Processing (NLP). They showed DL applications in various NLP ﬁelds, compared\nDL models, and discussed possible future trends.\nZhang et al. (2017) discussed state-of-the-art deep learning techniques for front-end and\nback-end speech recognition systems.\nZhu et al. (2017) presented overview on state-of-the-art of DL for remote sensing. They\nalso discussed open-source DL frameworks and other technical details for deep learning.\nWang et al. (2017a) described the evolution of deep learning models in time-series man-\nner. The briefed the models graphically along with the breakthroughs in DL research. This\npaper would be a good read to know the origin of the Deep Learning in evolutionary manner.\nThey also mentioned optimization and future research of neural networks.\nGoodfellow et al. (2016) discussed deep networks and generative models in details.\nStarting from Machine Learning (ML) basics, pros and cons for deep architectures, they\nconcluded recent DL researches and applications thoroughly.\n2\nLeCun et al. (2015) published a overview of Deep Learning (DL) models with Convo-\nlutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). They described\nDL from the perspective of Representation Learning, showing how DL techniques work and\ngetting used successfully in various applications, and predicting future learning based on\nUnsupervised Learning (UL). They also pointed out the articles of major advances in DL\nin the bibliography.\nSchmidhuber (2015) did a generic and historical overview of Deep Learning along with\nCNN, RNN and Deep Reinforcement Learning (RL). He emphasized on sequence-processing\nRNNs, while pointing out the limitations of fundamental DL and NNs, and the tricks to\nimprove them.\nNielsen (2015) described the neural networks in details along with codes and examples.\nHe also discussed deep neural networks and deep learning to some extent.\nSchmidhuber (2014) covered history and evolution of neural networks based on time\nprogression, categorized with machine learning approaches, and uses of deep learning in the\nneural networks.\nDeng and Yu (2014) described deep learning classes and techniques, and applications of\nDL in several areas.\nBengio (2013) did quick overview on DL algorithms i.e. supervised and unsupervised\nnetworks, optimization and training models from the perspective of representation learning.\nHe focused on many challenges of Deep Learning e.g. scaling algorithms for larger models\nand data, reducing optimization diﬃculties, designing eﬃcient scaling methods etc. along\nwith optimistic DL researches.\nBengio et al. (2013) discussed on Representation and Feature Learning aka Deep Learn-\ning.\nThey explored various methods and models from the perspectives of applications,\ntechniques and challenges.\nDeng (2011) gave an overview of deep structured learning and its architectures from the\nperspectives of information processing and related ﬁelds.\nArel et al. (2010) provided a short overview on recent DL techniques.\nBengio (2009) discussed deep architectures i.e. neural networks and generative models\nfor AI.\nAll recent overview papers on Deep Learning (DL) discussed important things from\nseveral perspectives. It is necessary to go through them for a DL researcher. However, DL\nis a highly ﬂourishing ﬁeld right now. Many new techniques and architectures are invented,\neven after the most recently published overview paper on DL. Also, previous papers focus\nfrom diﬀerent perspectives. Our paper is mainly for the new learners and novice researchers\nwho are new to this ﬁeld. For that purpose, we will try to give a basic and clear idea of\ndeep learning to the new researchers and anyone interested in this ﬁeld.\n3. Recent Advances\nIn this section, we will discuss the main recent Deep Learning (DL) approaches derived\nfrom Machine Learning and brief evolution of Artiﬁcial Neural Networks (ANN), which is\nthe most common form used for deep learning.\n3\n3.1 Evolution of Deep Architectures\nArtiﬁcial Neural Networks (ANN) have come a long way, as well as other deep models.\nFirst generation of ANNs was composed of simple neural layers for Perceptron.\nThey\nwere limited in simples computations. Second generation used Backpropagation to update\nweights of neurons according to error rates. Then Support Vector Machine (SVM) surfaced,\nand surpassed ANNs for a while. To overcome the limitations of backpropagation, Restricted\nBoltzmann Machine was proposed, making the learning easier. Other techniques and neural\nnetworks came as well e.g.\nFeedforward Neural Networks (FNN), Convolutional Neural\nNetowrks (CNN), Recurrent Neural Networks (RNN) etc. along with Deep Belief Networks,\nAutoencoders and such (Hinton, The next generation of neural networks). From that point,\nANNs got improved and designed in various ways and for various purposes.\nSchmidhuber (2014), Bengio (2009), Deng and Yu (2014), Goodfellow et al. (2016),\nWang et al. (2017a) etc. provided detailed overview on the evolution and history of Deep\nNeural Networks (DNN) as well as Deep Learning (DL). Deep architectures are multilayer\nnon-linear repetition of simple architectures in most of the cases, which helps to obtain\nhighly complex functions out of the inputs (LeCun et al., 2015).\n4. Deep Learning Approaches\nDeep Neural Networks (DNN) gained huge success in Supervised Learning (SL). Also, Deep\nLearning (DL) models are immensely successful in Unsupervised, Hybrid and Reinforcement\nLearning as well (LeCun et al., 2015).\n4.1 Deep Supervised Learning\nSupervised learning are applied when data is labeled and the classiﬁer is used for class\nor numeric prediction. LeCun et al. (2015) provided a brief yet very good explanation of\nsupervised learning approach and how deep architectures are formed. Deng and Yu (2014)\nmentioned many deep networks for supervised and hybrid learning and explained them e.g.\nDeep Stacking Network (DSN) and its variants. Schmidhuber (2014) covered all neural\nnetworks starting from early neural networks to recently successful Convolutional Neural\nNetworks (CNN), Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM)\nand their improvements.\n4.2 Deep Unsupervised Learning\nWhen input data is not labeled, unsupervised learning approach is applied to extract fea-\ntures from data and classify or label them. LeCun et al. (2015) predicted future of deep\nlearning in unsupervised learning. Schmidhuber (2014) described neural networks for unsu-\npervised learning as well. Deng and Yu (2014) briefed deep architectures for unsupervised\nlearning and explained deep Autoencoders in detail.\n4.3 Deep Reinforcement Learning\nReinforcement learning uses reward and punishment system for the next move generated by\nthe learning model. This is mostly used for games and robots, solves usually decision making\n4\nproblems (Li, 2017). Schmidhuber (2014) described advances of deep learning in Reinforce-\nment Learning (RL) and uses of Deep Feedforward Neural Netowrk (FNN) and Recurrent\nNeural Network (RNN) for RL. Li (2017) discussed Deep Reinforcement Learning(DRL),\nits architectures e.g. Deep Q-Network (DQN), and applications in various ﬁelds.\nMnih et al. (2016) proposed a DRL framework using asynchronous gradient descent for\nDNN optimization.\nvan Hasselt et al. (2015) proposed a DRL architecture using deep neural network (DNN).\n5. Deep Neural Networks\nIn this section, we will brieﬂy discuss about the deep neural networks (DNN), and recent\nimprovements and breakthroughs of them. Neural networks work with functionalities simi-\nlar to human brain. These are composed on neurons and connections mainly. When we are\nsaying deep neural network, we can assume there should be quite a number of hidden layers,\nwhich can be used to extract features from the inputs and to compute complex functions.\nBengio (2009) explained neural networks for deep architectures e.g. Convolutional Neural\nNetworks (CNN), Auto-Encoders (AE) etc. and their variants. Deng and Yu (2014) de-\ntailed some neural network architectures e.g. AE and its variants. Goodfellow et al. (2016)\nwrote and skillfully explained about Deep Feedforward Networks, Convolutional Networks,\nRecurrent and Recursive Networks and their improvements. Schmidhuber (2014) mentioned\nfull history of neural networks from early neural networks to recent successful techniques.\n5.1 Deep Autoencoders\nAutoencoders (AE) are neural networks (NN) where outputs are the inputs.\nAE takes\nthe original input, encodes for compressed representation and then decodes to reconstruct\nthe input (Wang). In a deep AE, lower hidden layers are used for encoding and higher\nones for decoding, and error back-propagation is used for training (Deng and Yu, 2014).\nGoodfellow et al. (2016)\n5.1.1 Variational Autoencoders\nVariational Auto-Encoders (VAE) can be counted as decoders (Wang). VAEs are built upon\nstandard neural networks and can be trained with stochastic gradient descent (Doersch,\n2016)\n5.1.2 Stacked Denoising Autoencoders\nIn early Auto-Encoders (AE), encoding layer had smaller dimensions than the input layer.\nIn Stacked Denoising Auto-Encoders (SDAE), encoding layer is wider than the input layer\n(Deng and Yu, 2014).\n5.1.3 Transforming Autoencoders\nDeep Auto-Encoders (DAE) can be transformation-variant, i.e., the extracted features\nfrom multilayers of non-linear processing could be changed due to learner.\nTransform-\ning Auto-Encoders (TAE) work with both input vector and target output vector to apply\n5\ntransformation-invariant property and lead the codes towards a desired way (Deng and Yu,\n2014).\n5.2 Deep Convolutional Neural Networks\nFour basic ideas make the Convolutional Neural Networks (CNN), i.e., local connections,\nshared weights, pooling, and using many layers. First parts of a CNN are made of convolu-\ntional and pooling layers and latter parts are mainly fully connected layers. Convolutional\nlayers detect local conjunctions from features and pooling layers merge similar features into\none (LeCun et al., 2015). CNNs use convolutions instead of matrix multiplication in the\nconvolutional layers (Goodfellow et al., 2016).\nKrizhevsky et al. (2012) presented a Deep Convolutional Neural Network (CNN) archi-\ntecture, also known as AlexNet, which was a major breakthrough in Deep Learning (DL).\nThe network composed of ﬁve convolutional layers and three fully connected layers. The\narchitecture used Graphics Processing Units (GPU) for convolution operation, Rectiﬁed\nLinear Units (ReLU) as activation function and Dropout (Srivastava et al., 2014) to reduce\noverﬁtting.\nIandola et al. (2016) proposed a small CNN architecture called SqueezeNet.\nSzegedy et al. (2014) proposed a Deep CNN architecture named Inception. An improve-\nment of Inception-ResNet is proposed by Dai et al. (2017).\nRedmon et al. (2015) proposed a CNN architecture named YOLO (You Only Look\nOnce) for uniﬁed and real-time object detection.\nZeiler and Fergus (2013) proposed a method for visualizing the activities within CNN.\nGehring et al. (2017) proposed a CNN architecture for sequence-to-sequence learning.\nBansal et al. (2017) proposed PixelNet, using pixels for representations.\nGoodfellow et al. (2016) explained the basic CNN architecures and the ideas. Gu et al.\n(2015) presented a nice overview on recent advances of CNNs, multiple variants of CNN, its\narchitectures, regularization methods and functionality, and applications in various ﬁelds.\n5.2.1 Deep Max-Pooling Convolutional Neural Networks\nMax-Pooling Convolutional Neural Networks (MPCNN) operate on mainly convolutions\nand max-pooling, especially used in digital image processing. MPCNN generally consists\nof three types of layers other than the input layer. Convolutional layers take input images\nand generate maps, then apply non-linear activation function. Max-pooling layers down-\nsample images and keep the maximum value of a sub-region. And fully-connected layers\ndoes the linear multiplication (Masci et al., 2013a). In Deep MPCNN, convolutional and\nmax-pooling layers are used periodically after the input layer, followed by fully-connected\nlayers (Giusti et al., 2013).\n5.2.2 Very Deep Convolutional Neural Networks\nSimonyan and Zisserman (2014b) proposed Very Deep Convolutional Neural Network (VD-\nCNN) architecture, also known as VGG Nets. VGG Nets use very small convolution ﬁlters\nand depth to 16-19 weight layers.\nConneau et al. (2016) proposed another VDCNN architecture for text classiﬁcation\nwhich uses small convolutions and pooling.\nThey claimed this architecture is the ﬁrst\n6\nVDCNN to be used in text processing which works at the character level. This architecture\nis composed of 29 convolution layers.\n5.3 Network In Network\nLin et al. (2013) proposed Network In Network (NIN). NIN replaces convolution layers of\ntraditional Convolutional Neural Network (CNN) by micro neural networks with complex\nstructures. It uses multi-layer perceptron (MLPConv) for micro neural networks and global\naverage pooling layer instead of fully connected layers. Deep NIN architectures can be made\nfrom multi-stacking of this proposed NIN structure (Lin et al., 2013).\n5.4 Region-based Convolutional Neural Networks\nGirshick et al. (2014) proposed Region-based Convolutional Neural Network (R-CNN) which\nuses regions for recognition. R-CNN uses regions to localize and segment objects. This ar-\nchitecture consists of three modules i.e. category independent region proposals which deﬁnes\nthe set of candidate regions, large Convolutional Neural Network (CNN) for extracting fea-\ntures from the regions, and a set of class speciﬁc linear Support Vector Machines (SVM)\n(Girshick et al., 2014).\n5.4.1 Fast R-CNN\nGirshick (2015) proposed Fast Region-based Convolutional Network (Fast R-CNN). This\nmethod exploits R-CNN (Girshick et al., 2014) architecture and produces fast results. Fast\nR-CNN consists of convolutional and pooling layers, proposals of regions, and a sequence\nof fully connected layers (Girshick, 2015).\n5.4.2 Faster R-CNN\nRen et al. (2015) proposed Faster Region-based Convolutional Neural Networks (Faster R-\nCNN), which uses Region Proposal Network (RPN) for real-time object detection. RPN\nis a fully convolutional network which generates region proposals accurately and eﬃciently\n(Ren et al., 2015).\n5.4.3 Mask R-CNN\nHe et al. (2017) proposed Mask Region-based Convolutional Network (Mask R-CNN) in-\nstance object segmentation. Mask R-CNN extends Faster R-CNN (Ren et al., 2015) archi-\ntecture, and uses an extra branch for object mask (He et al., 2017).\n5.4.4 Multi-Expert R-CNN\nLee et al. (2017) proposed Multi-Expert Region-based Convolutional Neural Networks (ME\nR-CNN), which exploits Fast R-CNN (Girshick, 2015) architecture. ME R-CNN generates\nRegion of Interests (RoI) from selective and exhaustive search. Also it uses per-RoI multi-\nexpert network instead of single per-RoI network. Each expert is the same architecture of\nfully connected layers from Fast R-CNN (Lee et al., 2017).\n7\n5.5 Deep Residual Networks\nHe et al. (2015) proposed Residual Networks (ResNets) consists of 152 layers. ResNets have\nlower error and easily trained with Residual Learning. More deeper ResNets achieve more\nbetter performance (He). ResNets are considered an important advance in the ﬁeld of Deep\nLearning.\n5.5.1 Resnet in Resnet\nTarg et al. (2016) proposed Resnet in Resnet (RiR) which combines ResNets (He et al.,\n2015) and standard Convolutional Neural Networks (CNN) in a deep dual stream architec-\nture (Targ et al., 2016).\n5.5.2 ResNeXt\nXie et al. (2016) proposed ResNeXt architecture.\nResNext exploits ResNets (He et al.,\n2015) for repeating layers with split-transform-merge strategy (Xie et al., 2016).\n5.6 Capsule Networks\nSabour et al. (2017) proposed Capsule Networks (CapsNet), an architecture with two con-\nvolutional layers and one fully connected layer. CapsNet usually contains several convo-\nlution layers and on capsule layer at the end (Xi et al., 2017). CapsNet is considered as\none of the most recent breakthrough in Deep Learning (Xi et al., 2017), since this is said\nto be build upon the limitations of Convolutional Neural Networks (Hinton). It uses lay-\ners of capsules instead of layers of neurons, where a capsule is a set of neurons. Active\nlower level capsules make predictions and upon agreeing multiple predictions, a higher level\ncapsule becomes active. A routing-by-agreement mechanism is used in these capsule lay-\ners. An improvement of CapsNet is proposed with EM routing (Anonymous, 2018b) using\nExpectation-Maximization (EM) algorithm.\n5.7 Recurrent Neural Networks\nRecurrent Neural Networks (RNN) are better suited for sequential inputs like speech and\ntext and generating sequence. A Recurrent hidden unit can be considered as very deep\nfeedforward network with same weights when unfolded in time. RNNs used to be diﬃcult\nto train because of gradient vanishing and exploding problem (LeCun et al., 2015). Many\nimprovements were proposed later to solve this problem.\nGoodfellow et al. (2016) provided details of Recurrent and Recursive Neural Networks\nand architectures, its variants along with related gated and memory networks.\nKarpathy et al. (2015) used character-level language models for analyzing and visualiz-\ning predictions, representations training dynamics, and error types of RNN and its variants\ne.g. LSTMs.\nJ´ozefowicz et al. (2016) explored RNN models and limitations for language modelling.\n8\n5.7.1 RNN-EM\nPeng and Yao (2015) proposed Recurrent Neural Networks with External Memory (RNN-\nEM) to improve memory capacity of RNNs. They claimed to achieve state-of-the-art in\nlanguage understanding, better than other RNNs.\n5.7.2 GF-RNN\nChung et al. (2015) proposed Gated Feedback Recurrent Neural Networks (GF-RNN), which\nextends the standard RNN by stacking multiple recurrent layers with global gating units.\n5.7.3 CRF-RNN\nZheng et al. (2015) proposed Conditional Random Fields as Recurrent Neural Networks\n(CRF-RNN), which combines the Convolutional Neural Networks (CNNs) and Conditional\nRandom Fields (CRFs) for probabilistic graphical modelling.\n5.7.4 Quasi-RNN\nBradbury et al. (2016) proposed Quasi Recurrent Neural Networks (QRNN) for neural se-\nquence modelling, appling parallel across timesteps.\n5.8 Memory Networks\nWeston et al. (2014) proposed Memory Networks for question answering (QA). Memory\nNetworks are composed of memory, input feature map, generalization, output feature map\nand response (Weston et al., 2014) .\n5.8.1 Dynamic Memory Networks\nKumar et al. (2015) proposed Dynamic Memory Networks (DMN) for QA tasks. DMN has\nfour modules i.e. Input, Question, Episodic Memory, Output (Kumar et al., 2015).\n5.9 Augmented Neural Networks\nOlah and Carter (2016) gave nice presentation of Attentional and Augmented Recurrent\nNeural Networks i.e. Neural Turing Machines (NTM), Attentional Interfaces, Neural Pro-\ngrammer and Adaptive Computation Time. Augmented Neural Networks are usually made\nof using extra properties like logic functions along with standard Neural Network architec-\nture (Olah and Carter, 2016).\n5.9.1 Neural Turing Machines\nGraves et al. (2014) proposed Neural Turing Machine (NTM) architecture, consisting of a\nneural network controller and a memory bank. NTMs usually combine RNNs with external\nmemory bank (Olah and Carter, 2016).\n9\n5.9.2 Neural GPU\nKaiser and Sutskever (2015) proposed Neural GPU, which solves the parallel problem of\nNTM (Graves et al., 2014).\n5.9.3 Neural Random-Access Machines\nKurach et al. (2015) proposed Neural Random Access Machine, which uses an external\nvariable-size random-access memory.\n5.9.4 Neural Programmer\nNeelakantan et al. (2015) proposed Neural Programmer, an augmented neural network with\narithmetic and logic functions.\n5.9.5 Neural Programmer-Interpreters\nReed and de Freitas (2015) proposed Neural Programmer-Interpreters (NPI) which can\nlearn.\nNPI consists of recurrent core, program memory and domain-speciﬁc encoders\n(Reed and de Freitas, 2015).\n5.10 Long Short Term Memory Networks\nHochreiter and Schmidhuber (1997) proposed Long Short-Term Memory (LSTM) which\novercomes the error back-ﬂow problems of Recurrent Neural Networks (RNN). LSTM is\nbased on recurrent network along with gradient-based learning algorithm (Hochreiter and Schmidhuber,\n1997) LSTM introduced self-loops to produce paths so that gradient can ﬂow (Goodfellow et al.,\n2016).\nGreﬀet al. (2017) provided large-scale analysis of Vanilla LSTM and eight LSTM vari-\nants for three uses i.e. speech recognition, handwriting recognition, and polyphonic music\nmodeling. They claimed that eight variants of LSTM failed to perform signiﬁcant improve-\nment, while only Vanilla LSTM performs well (Greﬀet al., 2015).\nShi et al. (2016b) proposed Deep Long Short-Term Memory (DLSTM), which is a stack\nof LSTM units for feature mapping to learn representations (Shi et al., 2016b).\n5.10.1 Batch-Normalized LSTM\nCooijmans et al. (2016) proposed batch-normalized LSTM (BN-LSTM), which uses batch-\nnormalizing on hidden states of recurrent neural networks.\n5.10.2 Pixel RNN\nvan den Oord et al. (2016b) proposed Pixel Recurrent Neural Networks (PixelRNN), made\nof up to twelve two-dimensional LSTM layers.\n5.10.3 Bidirectional LSTM\nW¨ollmer et al. (2010) proposed Bidirection LSTM (BLSTM) Recurrent Networks to be used\nwith Dynamic Bayesian Network (DBN) for context-sensitive keyword detection.\n10\n5.10.4 Variational Bi-LSTM\nShabanian et al. (2017) proposed Variational Bi-LSTMs, which is a variant of Bidirectional\nLSTM architecture. Variational Bi-LSTM creates a channel of information exchange be-\ntween LSTMs using Variational Auto-Encoders (VAE), for learning better representations\n(Shabanian et al., 2017).\n5.11 Googles Neural Machine Translation\nWu et al. (2016) proposed Googles Neural Machine Translation (GNMT) System for au-\ntomated translation, which incorporates an encoder network, a decoder network and an\nattention network following the common sequence-to-sequence learning framework.\n5.12 Fader Networks\nLample et al. (2017) proposed Fader Networks, a new type of encoder-decoder architecture\nto generate realistic variations of input images by changing attribute values.\n5.13 Hyper Networks\nHa et al. (2016) proposed HyperNetworks which generates weights for other neural net-\nworks, such as static hypernetworks convolutional networks, dynamic hypernetworks for\nrecurrent networks.\nDeutsch (2018) used Hyper Networks for generating neural networks.\n5.14 Highway Networks\nSrivastava et al. (2015) proposed Highway Networks, which uses gating units to learn reg-\nulating information through. Information ﬂow across several layers are called information\nhighways (Srivastava et al., 2015).\n5.14.1 Recurrent Highway Networks\nZilly et al. (2017) proposed Recurrent Highway Networks (RHN), which extend Long Short-\nTerm Memory (LSTM) architecture. RHNs use Highway layers inside the recurrent transi-\ntion (Zilly et al., 2017).\n5.15 Highway LSTM RNN\nZhang et al. (2016c) proposed Highway Long Short-Term Memory (HLSTM) RNN, which\nextends deep LSTM networks with gated direction connections i.e.\nHighways, between\nmemory cells in adjacent layers.\n5.16 Long-Term Recurrent CNN\nDonahue et al. (2014) proposed Long-term Recurrent Convolutional Networks (LRCN),\nwhich uses CNN for inputs, then LSTM for recurrent sequence modeling and generating\npredictions.\n11\n5.17 Deep Neural SVM\nZhang et al. (2015a) proposed Deep Neural Support Vector Machines (DNSVM), which uses\nSupport Vector Machine (SVM) as the top layer for classiﬁcation in a Deep Neural Network\n(DNN)\n5.18 Convolutional Residual Memory Networks\nMoniz and Pal (2016) proposed Convolutional Residual Memory Networks, which incorpo-\nrates memory mechanism into Convolutional Neural Networks (CNN). It augments con-\nvolutional residual networks with a long short term memory mechanism (Moniz and Pal,\n2016).\n5.19 Fractal Networks\nLarsson et al. (2016) proposed Fractal Networks i.e. FractalNet, as an alternative to residual\nnets. They claimed to train ultra deep neural networks without residual learning. Fractals\nare repeated architecture generated by simple expansion rule (Larsson et al., 2016).\n5.20 WaveNet\nvan den Oord et al. (2016a) proposed WaveNet, deep neural network for generating raw\naudio. WaveNet is composed of a stack of convolutional layers, and softmax distribution\nlayer for outputs (van den Oord et al., 2016a).\nRethage et al. (2017) proposed a WaveNet model for speech denoising.\n5.21 Pointer Networks\nVinyals et al. (2017) proposed Pointer Networks (Ptr-Nets), which solves the problem of rep-\nresenting variable dictionaries by using a softmax probability distribution called ”Pointer”.\n6. Deep Generative Models\nIn this section, we will brieﬂy discuss other deep architecures which uses multiple levels\nof abstraction and representation similar to deep neural networks, also known as Deep\nGenerative Models (DGM). Bengio (2009) explained deep architectures e.g.\nBoltzmann\nMachines (BM) and Restricted Boltzmann Machines (RBM) etc. and their variants.\nGoodfellow et al. (2016) explained deep generative models in details e.g.\nRestricted\nand Unrestricted Boltzmann Machines and their variants, Deep Boltzmann Machines, Deep\nBelief Networks (DBN), Directed Generative Nets, and Generative Stochastic Networks etc.\nMaaløe et al. (2016) proposed Auxiliary Deep Generative Models where they extended\nDeep Generative Models with auxiliary variables. The auxiliary variables make variational\ndistribution with stochastic layers and skip connections (Maaløe et al., 2016).\nRezende et al. (2016) developed a class for one-shot generalization of deep generative\nmodels.\n12\n6.1 Boltzmann Machines\nBoltzmann Machines are connectionist approach for learning arbitrary probability distribu-\ntions which use maximum likelihood principle for learning (Goodfellow et al., 2016).\n6.2 Restricted Boltzmann Machines\nRestricted Boltzmann Machines (RBM) are special type of Markov random ﬁeld containing\none layer of stochastic hidden units i.e. latent variables and one layer of observable variables\n(Deng and Yu (2014), Goodfellow et al. (2016)).\nHinton and Salakhutdinov (2011) proposed a Deep Generative Model using Restricted\nBoltzmann Machines (RBM) for document processing.\n6.3 Deep Belief Networks\nDeep Belief Networks (DBN) are generative models with several layers of latent binary or\nreal variables (Goodfellow et al., 2016).\nRanzato et al. (2011) built a deep generative model using Deep Belief Network (DBN)\nfor images recognition.\n6.4 Deep Lambertian Networks\nTang et al. (2012) proposed Deep Lambertian Networks (DLN) which is a multilayer gener-\native model where latent variables are albedo, surface normals, and the light source. DLN\nis a combination of lambertian reﬂectance with Gaussian Restricted Boltzmann Machines\nand Deep Belief Networks (Tang et al., 2012).\n6.5 Generative Adversarial Networks\nGoodfellow et al. (2014) proposed Generative Adversarial Nets (GAN) for estimating gen-\nerative models with an adversarial process. GAN architecture is composed of a generative\nmodel pitted against an adversary i.e. a discriminative model to learn model or data distri-\nbution (Goodfellow et al., 2014). Some more improvements proposed for GAN by Mao et al.\n(2016), Kim et al. (2017) etc.\nSalimans et al. (2016) presented several methods for training GANs.\n6.5.1 Laplacian Generative Adversarial Networks\nDenton et al. (2015) proposed a Deep Generative Model (DGM) called Laplacian Genera-\ntive Adversarial Networks (LAPGAN) using Generative Adversarial Networks (GAN) ap-\nproach. The model also uses convolutional networks within a Laplacian pyramid framework\n(Denton et al., 2015).\n6.6 Recurrent Support Vector Machines\nShi et al. (2016a) proposed Recurrent Support Vector Machines (RSVM), which uses Re-\ncurrent Neural Network (RNN) for extracting features from input sequence and standard\nSupport Vector Machine (SVM) for sequence-level objective discrimination.\n13\n7. Training and Optimization Techniques\nIn this section, we will provide short overview on some major techniques for regularization\nand optimization of Deep Neural Networks (DNN).\n7.1 Dropout\nSrivastava et al. (2014) proposed Dropout to prevent neural networks from overﬁtting.\nDropout is a neural network model-averaging regularization method by adding noise to\nits hidden units. It drops units from the neural network along with connections randomly\nduring training. Dropout can be used with any kind of neural networks, even in graphical\nmodels like RBM (Srivastava et al., 2014). A very recent proposed improvement of dropout\nis Fraternal Dropout (Anonymous, 2018a) for Recurrent Neural Networks (RNN).\n7.2 Maxout\nGoodfellow et al. (2013) proposed Maxout, a new activation function to be used with\nDropout (Srivastava et al., 2014).\nMaxout’s output is the maximum of a set of inputs,\nwhich is beneﬁcial for Dropout’s model averaging (Goodfellow et al., 2013).\n7.3 Zoneout\nKrueger et al. (2016) proposed Zoneout, a regularization method for Recurrent Neural Net-\nworks (RNN). Zoneout uses noise randomly while training similar to Dropout (Srivastava et al.,\n2014), but preserves hidden units instead of dropping (Krueger et al., 2016).\n7.4 Deep Residual Learning\nHe et al. (2015) proposed Deep Residual Learning framework for Deep Neural Networks\n(DNN), which are called ResNets with lower training error (He).\n7.5 Batch Normalization\nIoﬀe and Szegedy (2015) proposed Batch Normalization, a method for accelerating deep\nneural network training by reducing internal covariate shift. Ioﬀe (2017) proposed Batch\nRenormalization extending the previous approach.\n7.6 Distillation\nHinton et al. (2015) proposed Distillation, from transferring knowledge from ensemble of\nhighly regularized models i.e. neural networks into compressed and smaller model.\n7.7 Layer Normalization\nBa et al. (2016) proposed Layer Normalization, for speeding-up training of deep neural net-\nworks especially for RNNs and solves the limitations of batch normalization (Ioﬀe and Szegedy,\n2015).\n14\n8. Deep Learning frameworks\nThere are a good number of open-source libraries and frameworks available for deep learning.\nMost of them are built for python programming language. Such as Theano (Bergstra et al.,\n2011), Tensorﬂow (Abadi et al., 2016), PyTorch, PyBrain (Schaul et al., 2010), Caﬀe (Jia et al.,\n2014), Blocks and Fuel (van Merri¨enboer et al., 2015), CuDNN (Chetlur et al., 2014), Honk\n(Tang and Lin, 2017), ChainerCV (Niitani et al., 2017), PyLearn2, Chainer, torch, neon etc.\nBahrampour et al. (2015) did a comparative study of several deep learning frameworks.\n9. Applications of Deep Learning\nIn this section, we will brieﬂy discuss some recent outstanding applications of Deep Learning\narchitectures.\nSince the beginning of Deep Learning (DL), DL methods are being used\nin various ﬁelds in forms of supervised, unsupervised, semi-supervised or reinforcement\nlearning. Starting from classiﬁcation and detection tasks, DL applications are spreading\nrapidly in every ﬁelds.\nSuch as -\n• image classiﬁcation and recognition (Simonyan and Zisserman (2014b), Krizhevsky et al.\n(2012), He et al. (2015))\n• video classiﬁcation (Karpathy et al., 2014)\n• sequence generation (Graves, 2013)\n• defect classiﬁcation (Masci et al., 2013b)\n• text, speech, image and video processing (LeCun et al., 2015)\n• text classiﬁcation (Conneau et al., 2016)\n• speech processing (Arel et al., 2009)\n• speech recognition and spoken language understanding (Hinton et al. (2012), Zhang et al.\n(2015b), Zhang et al. (2016c), Zhang et al. (2016c), Zhang et al. (2015a), Shi et al.\n(2016a), Mesnil et al. (2015), Peng and Yao (2015), Amodei et al. (2015))\n• text-to-speech generation (Wang et al. (2017b), Arik et al. (2017))\n• query classiﬁcation (Shi et al., 2016b)\n• sentence classiﬁcation (Kim, 2014)\n• sentence modelling (Kalchbrenner et al., 2014)\n• word processing (Mikolov et al., 2013a)\n• premise selection (Alemi et al., 2016)\n• document and sentence processing (Le and Mikolov (2014), Mikolov et al. (2013b))\n• generating image captions (Vinyals et al. (2014), Xu et al. (2015))\n15\n• photographic style transfer (Luan et al., 2017)\n• natural image manifold (Zhu et al., 2016)\n• image colorization (Zhang et al., 2016b)\n• image question answering (Yang et al., 2015)\n• generating textures and stylized images (Ulyanov et al., 2016)\n• visual and textual question answering (Xiong et al. (2016), ?DBLP:journals/corr/AntolALMBZP15))\n• visual recognition and description (Donahue et al. (2014), Razavian et al. (2014), Oquab et al.\n(2014))\n• object detection (Lee et al. (2017), Ranzato et al. (2011), Redmon et al. (2015), Liu et al.\n(2015))\n• document processing (Hinton and Salakhutdinov, 2011)\n• character motion synthesis and editing (Holden et al., 2016)\n• singing synthesis (Blaauw and Bonada, 2017)\n• person identiﬁcation (Li et al., 2014)\n• face recognition and veriﬁcation (Taigman et al., 2014)\n• action recognition in videos (Simonyan and Zisserman, 2014a)\n• human action recognition (Ji et al., 2013)\n• action recognition (Sharma et al., 2015)\n• classifying and visualizing motion capture sequences (Cho and Chen, 2013)\n• handwriting generation and prediction (Carter et al., 2016)\n• automated and machine translation (Wu et al. (2016), Cho et al. (2014), Bahdanau et al.\n(2014), Hermann et al. (2015), Luong et al. (2015))\n• named entity recognition (Lample et al., 2016)\n• mobile vision (Howard et al., 2017)\n• conversational agents (Ghazvininejad et al., 2017)\n• calling genetic variants (Poplin et al., 2016)\n• cancer detection (Cruz-Roa et al., 2013)\n• X-ray CT reconstruction (Kang et al., 2016)\n• Epileptic Seizure Prediction (Mirowski et al., 2008)\n16\n• hardware acceleration (Han et al., 2016)\n• robotics (Lenz et al., 2013)\nto name a few.\nDeng and Yu (2014) provided detailed lists of DL applications in various categories e.g.\nspeech and audio processing, information retrieval, object recognition and computer vision,\nmultimodal and multi-task learning etc.\nUsing Deep Reinforcement Learning (DRL) for mastering games has become a hot topic\nnow-a-days. Every now and then, AI bots created with DNN and DRL, are beating hu-\nman world champions and grandmasters in strategical and other games, from only hours\nof training. For example, AlphaGo and AlphaGo Zero for game of GO (Silver et al. (2017b),\nSilver et al. (2016), Dong et al. (2017)), Dota2 (Batsford (2014)), Atari (Mnih et al. (2013),Mnih et al.\n(2015), van Hasselt et al. (2015)), Chess and Shougi (Silver et al., 2017a).\n10. Discussion\nThough Deep Learning has achieved tremendous success in many areas, it still has long\nway to go. There are many rooms left for improvement. As for limitations, the list is quite\nlong as well. For example, Nguyen et al. (2014) showed that Deep Neural Networks (DNN)\ncan be easily fooled while recognizing images. There are other issues like transferability\nof features learned (Yosinski et al., 2014). Huang et al. (2017) proposed an architecture\nfor adersarial attacks on neural networks, where they think future works are needed for\ndefenses against those attacks. Zhang et al. (2016a) presented an experimental framework\nfor understanding deep learning models. They think understanding deep learning requires\nrethinking generalization.\nMarcus (2018) gave an important review on Deep Learning (DL), what it does, its limits\nand its nature. He strongly pointed out the limitations of DL methods, i.e., requiring more\ndata, having limited capacity, inability to deal with hierarchical structure, struggling with\nopen-ended inference, not being suﬃciently transparent, not being well integrated with prior\nknowledge, and inability to distinguish causation from correlation (Marcus, 2018). He also\nmentioned that DL assumes stable world, works as approximation, is diﬃcult to engineer\nand has potential risks as being an excessive hype. Marcus (2018) thinks DL needs to be\nreconceptualized and to look for possibilities in unsupervised learning, symbol manipulation\nand hybrid models, having insights from cognitive science and psychology and taking bolder\nchallenges.\n11. Conclusion\nAlthough Deep Learning (DL) has advanced the world faster than ever, there are still ways\nto go. We are still away from fully understanding of how deep learning works, how we\ncan get machines more smarter, close to or smarter than humans, or learning exactly like\nhuman. DL has been solving many problems while taking technologies to another dimension.\nHowever, there are many diﬃcult problems for humanity to deal with. For example, people\nare still dying from hunger and food crisis, cancer and other lethal diseases etc. We hope\ndeep learning and AI will be much more devoted to the betterment of humanity, to carry\n17\nout the hardest scientiﬁc researches, and last but not the least, to make the world a more\nbetter place for every single human.\nAcknowledgments\nWe would like to thank Dr. Mohammed Moshiul Hoque, Professor, Department of CSE,\nCUET, for introducing us to the amazing world of Deep Learning.\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGregory S. Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay Ghemawat,\nIan J. Goodfellow, Andrew Harp, Geoﬀrey Irving, Michael Isard, Yangqing Jia, Rafal\nJ´ozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man´e, Rajat Monga,\nSherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster, Jonathon Shlens,\nBenoit Steiner, Ilya Sutskever, Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay\nVasudevan, Fernanda B. Vi´egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Mar-\ntin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorﬂow: Large-scale machine learning on\nheterogeneous distributed systems. CoRR, abs/1603.04467, 2016.\nAlexander A. Alemi, Fran¸cois Chollet, Geoﬀrey Irving, Christian Szegedy, and Josef Urban.\nDeepmath - deep sequence models for premise selection. CoRR, abs/1606.04442, 2016.\nDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catan-\nzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse\nEngel, Linxi Fan, Christopher Fougner, Tony Han, Awni Y. Hannun, Billy Jun, Patrick\nLeGresley, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger,\nJonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang,\nZhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, and Zhenyao Zhu. Deep\nspeech 2: End-to-end speech recognition in english and mandarin. CoRR, abs/1512.02595,\n2015.\nAnonymous. Fraternal dropout. International Conference on Learning Representations,\n2018a. URL https://openreview.net/forum?id=SJyVzQ-C-.\nAnonymous. Matrix capsules with em routing. International Conference on Learning Rep-\nresentations, 2018b. URL https://openreview.net/forum?id=HJWLfGWRb.\nItamar Arel, Derek Rose, and Tom Karnowski. A deep learning architecture comprising\nhomogeneous cortical circuits for scalable spatiotemporal pattern inference. In in Proc.\nNIPS Workshop Deep Learn. Speech, pages 1–8, 2009.\nItamar Arel, Derek C. Rose, and Thomas P. Karnowski.\nResearch frontier: Deep ma-\nchine learning–a new frontier in artiﬁcial intelligence research.\nComp. Intell. Mag., 5\n(4):13–18, November 2010.\nISSN 1556-603X.\ndoi: 10.1109/MCI.2010.938364.\nURL\nhttp://dx.doi.org/10.1109/MCI.2010.938364.\n18\nSercan ¨Omer Arik, Mike Chrzanowski, Adam Coates, Greg Diamos, Andrew Gibiansky,\nYongguo Kang, Xian Li, John Miller, Jonathan Raiman, Shubho Sengupta, and Mo-\nhammad Shoeybi. Deep voice: Real-time neural text-to-speech. CoRR, abs/1702.07825,\n2017.\nLei Jimmy Ba, Ryan Kiros, and Geoﬀrey E. Hinton.\nLayer normalization.\nCoRR,\nabs/1607.06450, 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. CoRR, abs/1409.0473, 2014.\nSoheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, and Mohak Shah. Comparative\nstudy of caﬀe, neon, theano, and torch for deep learning. CoRR, abs/1511.06435, 2015.\nAayush Bansal, Xinlei Chen, Bryan C. Russell, Abhinav Gupta, and Deva Ramanan. Pixel-\nnet: Representation of the pixels, by the pixels, and for the pixels. CoRR, abs/1702.06506,\n2017.\nTom\nBatsford.\nCalculating\noptimal\njungling\nroutes\nin\ndota2\nusing\nneural\nnetworks\nand\ngenetic\nalgorithms.\nGame\nBehaviour,\n1(1),\n2014.\nURL\nhttps://computing.derby.ac.uk/ojs/index.php/gb/article/view/14.\nYoshua Bengio. URL http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html.\nMILA, University of Montreal, Quebec, Canada.\nYoshua Bengio.\nLearning deep architectures for ai.\nFound. Trends Mach. Learn.,\n2(1):1–127,\nJanuary 2009.\nISSN 1935-8237.\ndoi:\n10.1561/2200000006.\nURL\nhttp://dx.doi.org/10.1561/2200000006.\nYoshua Bengio. Deep learning of representations: Looking forward. CoRR, abs/1305.0445,\n2013.\nYoshua Bengio, Aaron Courville, and Pascal Vincent.\nRepresentation learning:\nA\nreview and new perspectives.\nIEEE Trans. Pattern Anal. Mach. Intell.,\n35(8):\n1798–1828, August 2013.\nISSN 0162-8828.\ndoi:\n10.1109/TPAMI.2013.50.\nURL\nhttp://dx.doi.org/10.1109/TPAMI.2013.50.\nJames Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guil-\nlaume Desjardins, Joseph P. Turian, David Warde-Farley, and Yoshua Bengio. Theano:\nA cpu and gpu math compiler in python. 2011.\nMerlijn Blaauw and Jordi Bonada.\nA neural parametric singing synthesizer.\nCoRR,\nabs/1704.03809, 2017.\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent\nneural networks. CoRR, abs/1611.01576, 2016.\nShan Carter, David Ha, Ian Johnson, and Chris Olah.\nExperiments in handwrit-\ning with a neural network.\nDistill,\n2016.\ndoi:\n10.23915/distill.00004.\nURL\nhttp://distill.pub/2016/handwriting.\n19\nSharan Chetlur, CliﬀWoolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan\nCatanzaro, and Evan Shelhamer. cudnn: Eﬃcient primitives for deep learning. CoRR,\nabs/1410.0759, 2014.\nKyunghyun Cho and Xi Chen. Classifying and visualizing motion capture sequences using\ndeep neural networks. CoRR, abs/1306.3874, 2013.\nKyunghyun Cho, Bart van Merrienboer, C¸aglar G¨ul¸cehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio.\nLearning phrase representations using RNN encoder-decoder for\nstatistical machine translation. CoRR, abs/1406.1078, 2014.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback\nrecurrent neural networks. In Proceedings of the 32Nd International Conference on In-\nternational Conference on Machine Learning - Volume 37, ICML’15, pages 2067–2075.\nJMLR.org, 2015. URL http://dl.acm.org/citation.cfm?id=3045118.3045338.\nAlexis Conneau, Holger Schwenk, Lo¨ıc Barrault, and Yann LeCun. Very deep convolutional\nnetworks for text classiﬁcation. CoRR, abs/1606.01781, 2016.\nTim Cooijmans, Nicolas Ballas, C´esar Laurent, and Aaron C. Courville. Recurrent batch\nnormalization. CoRR, abs/1603.09025, 2016.\nAngel Alfonso Cruz-Roa, John Edison Arevalo Ovalle, Anant Madabhushi, and Fabio Au-\ngusto Gonz´alez Osorio. A Deep Learning Architecture for Image Representation, Visual\nInterpretability and Automated Basal-Cell Carcinoma Cancer Detection, pages 403–410.\nSpringer Berlin Heidelberg, Berlin, Heidelberg, 2013. ISBN 978-3-642-40763-5. doi: 10.\n1007/978-3-642-40763-5 50. URL https://doi.org/10.1007/978-3-642-40763-5_50.\nJifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei.\nDeformable convolutional networks. CoRR, abs/1703.06211, 2017.\nLi Deng. An overview of deep-structured learning for information processing. 01 2011.\nLi Deng and Dong Yu. Deep learning: Methods and applications. Found. Trends Signal\nProcess., 7(3&#8211;4):197–387, June 2014. ISSN 1932-8346. doi: 10.1561/2000000039.\nURL http://dx.doi.org/10.1561/2000000039.\nEmily L. Denton, Soumith Chintala, Arthur Szlam, and Robert Fergus. Deep generative\nimage models using a laplacian pyramid of adversarial networks. CoRR, abs/1506.05751,\n2015.\nLior Deutsch. Generating neural networks with neural networks. CoRR, abs/1801.01952,\n2018. URL https://arxiv.org/abs/1801.01952.\nCarl Doersch.\nTutorial on variational autoencoders.\nCoRR, 1606.05908, 2016.\nURL\nhttps://arxiv.org/abs/1606.05908.\nJeﬀDonahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini\nVenugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional net-\nworks for visual recognition and description. CoRR, abs/1411.4389, 2014.\n20\nXiao Dong, Jiasong Wu, and Ling Zhou.\nDemystifying alphago zero as alphago GAN.\nCoRR, abs/1711.09091, 2017.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Con-\nvolutional sequence to sequence learning. CoRR, abs/1705.03122, 2017.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. CoRR,\nabs/1702.01932, 2017.\nRoss Girshick, JeﬀDonahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\nfor accurate object detection and semantic segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), 2014.\nRoss B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.\nAlessandro Giusti, Dan C. Ciresan, Jonathan Masci, Luca Maria Gambardella, and J¨urgen\nSchmidhuber. Fast image scanning with deep max-pooling convolutional neural networks.\nIn IEEE International Conference on Image Processing, ICIP 2013, Melbourne, Australia,\nSeptember 15-18, 2013, pages 4034–4038, 2013. doi: 10.1109/ICIP.2013.6738831. URL\nhttps://doi.org/10.1109/ICIP.2013.6738831.\nIan Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.\nMaxout networks. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of\nthe 30th International Conference on Machine Learning, volume 28 of Proceedings of\nMachine Learning Research, pages 1319–1327, Atlanta, Georgia, USA, 17–19 Jun 2013.\nPMLR. URL http://proceedings.mlr.press/v28/goodfellow13.html.\nIan\nGoodfellow,\nJean\nPouget-Abadie,\nMehdi\nMirza,\nBing\nXu,\nDavid\nWarde-\nFarley,\nSherjil\nOzair,\nAaron\nCourville,\nand\nYoshua\nBengio.\nGenerative\nad-\nversarial\nnets.\nIn Z. Ghahramani,\nM. Welling,\nC. Cortes,\nN. D. Lawrence,\nand\nK.\nQ.\nWeinberger,\neditors,\nAdvances\nin\nNeural\nInformation\nProcess-\ning\nSystems\n27,\npages\n2672–2680.\nCurran\nAssociates,\nInc.,\n2014.\nURL\nhttp://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\nhttp://www.deeplearningbook.org.\nAlex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850,\n2013.\nAlex Graves, Greg Wayne, and Ivo Danihelka.\nNeural turing machines.\nCoRR,\nabs/1410.5401, 2014.\nKlaus Greﬀ, Rupesh Kumar Srivastava, Jan Koutn´ık, Bas R. Steunebrink, and J¨urgen\nSchmidhuber. LSTM: A search space odyssey. CoRR, abs/1503.04069, 2015.\nKlaus Greﬀ, Rupesh Kumar Srivastava, Jan Koutn´ık, Bas R. Steunebrink, and J¨urgen\nSchmidhuber.\nLSTM: A search space odyssey.\nIEEE Trans. Neural Netw. Learn-\ning Syst.,\n28(10):2222–2232,\n2017.\ndoi:\n10.1109/TNNLS.2016.2582924.\nURL\nhttps://doi.org/10.1109/TNNLS.2016.2582924.\n21\nJiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai,\nTing Liu, Xingxing Wang, and Gang Wang. Recent advances in convolutional neural\nnetworks. CoRR, abs/1512.07108, 2015.\nDavid Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. CoRR, abs/1609.09106, 2016.\nSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and\nWilliam J. Dally.\nEIE: eﬃcient inference engine on compressed deep neural network.\nCoRR, abs/1602.01528, 2016.\nKaiming He. URL http://kaiminghe.com. Facebook AI Research (FAIR).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. CoRR, abs/1512.03385, 2015.\nKaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross B. Girshick. Mask R-CNN. CoRR,\nabs/1703.06870, 2017.\nKarl Moritz Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom.\nTeaching machines to read and comprehend.\nCoRR, abs/1506.03340, 2015.\nGeoﬀrey Hinton. URL http://www.cs.toronto.edu/~hinton/. University of Toronto (U\nof T), Ontario, Canada. Google Brain Team.\nGeoﬀrey Hinton and Ruslan Salakhutdinov.\nDiscovering binary codes for docu-\nments by learning deep generative models.\nTopics in Cognitive Science,\n3(1):\n74–91,\n2011.\nISSN 1756-8765.\ndoi:\n10.1111/j.1756-8765.2010.01109.x.\nURL\nhttp://dx.doi.org/10.1111/j.1756-8765.2010.01109.x.\nGeoﬀrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep\nJaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian\nKingsbury. Deep neural networks for acoustic modeling in speech recognition. Signal\nProcessing Magazine, 2012.\nGeoﬀrey Hinton, Oriol Vinyals, and Jeﬀrey Dean.\nDistilling the knowledge in a neural\nnetwork. In NIPS Deep Learning and Representation Learning Workshop, 2015. URL\nhttp://arxiv.org/abs/1503.02531.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9\n(8):1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL\nhttp://dx.doi.org/10.1162/neco.1997.9.8.1735.\nDaniel\nHolden,\nJun\nSaito,\nand\nTaku\nKomura.\nA\ndeep\nlearning\nframework\nfor character motion synthesis and editing.\nACM Trans. Graph.,\n35(4):138:1–\n138:11,\nJuly\n2016.\nISSN 0730-0301.\ndoi:\n10.1145/2897824.2925975.\nURL\nhttp://doi.acm.org/10.1145/2897824.2925975.\nAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, To-\nbias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Eﬃcient convolutional\nneural networks for mobile vision applications. CoRR, abs/1704.04861, 2017.\n22\nSandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel.\nAdversarial attacks on neural network policies. CoRR, abs/1702.02284, 2017.\nForrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally,\nand Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\n<1mb model size. CoRR, abs/1602.07360, 2016.\nSergey Ioﬀe. Batch renormalization: Towards reducing minibatch dependence in batch-\nnormalized models. CoRR, abs/1702.03275, 2017.\nSergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network train-\ning by reducing internal covariate shift. In Francis Bach and David Blei, editors, Proceed-\nings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings\nof Machine Learning Research, pages 448–456, Lille, France, 07–09 Jul 2015. PMLR. URL\nhttp://proceedings.mlr.press/v37/ioffe15.html.\nShuiwang Ji, Wei Xu, Ming Yang, and Kai Yu.\n3d convolutional neural networks\nfor human action recognition.\nIEEE Trans. Pattern Anal. Mach. Intell., 35(1):\n221–231, January 2013.\nISSN 0162-8828.\ndoi:\n10.1109/TPAMI.2012.59.\nURL\nhttp://dx.doi.org/10.1109/TPAMI.2012.59.\nYangqing Jia, Evan Shelhamer, JeﬀDonahue, Sergey Karayev, Jonathan Long, Ross B.\nGirshick, Sergio Guadarrama, and Trevor Darrell. Caﬀe: Convolutional architecture for\nfast feature embedding. CoRR, abs/1408.5093, 2014.\nRafal J´ozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. CoRR, abs/1602.02410, 2016.\nLukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. CoRR, abs/1511.08228,\n2015.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\nfor modelling sentences. CoRR, abs/1404.2188, 2014.\nEunhee Kang, Junhong Min, and Jong Chul Ye.\nWavenet: a deep convolutional neu-\nral network using directional wavelets for low-dose x-ray CT reconstruction.\nCoRR,\nabs/1610.09736, 2016.\nAndrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar,\nand Li Fei-Fei.\nLarge-scale video classiﬁcation with convolutional neural networks.\nIn Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recog-\nnition,\nCVPR ’14,\npages 1725–1732,\nWashington,\nDC, USA, 2014. IEEE Com-\nputer Society.\nISBN 978-1-4799-5118-5.\ndoi:\n10.1109/CVPR.2014.223.\nURL\nhttp://dx.doi.org/10.1109/CVPR.2014.223.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li. Visualizing and understanding recurrent\nnetworks. CoRR, abs/1506.02078, 2015.\n23\nTaeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim.\nLearn-\ning to discover cross-domain relations with generative adversarial networks.\nCoRR,\nabs/1703.05192, 2017.\nYoon Kim. Convolutional neural networks for sentence classiﬁcation. CoRR, abs/1408.5882,\n2014.\nAlex\nKrizhevsky,\nIlya\nSutskever,\nand\nGeoﬀrey\nE.\nHinton.\nImagenet\nclassiﬁ-\ncation\nwith\ndeep\nconvolutional\nneural\nnetworks.\nIn\nProceedings\nof\nthe\n25th\nInternational\nConference\non\nNeural\nInformation\nProcessing\nSystems\n-\nVolume\n1,\nNIPS’12,\npages\n1097–1105,\nUSA,\n2012.\nCurran\nAssociates\nInc.\nURL\nhttp://dl.acm.org/citation.cfm?id=2999134.2999257.\nDavid Krueger, Tegan Maharaj, J´anos Kram´ar, Mohammad Pezeshki, Nicolas Ballas,\nNan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron C. Courville,\nand Chris Pal. Zoneout: Regularizing rnns by randomly preserving hidden activations.\nCoRR, abs/1606.01305, 2016.\nAnkit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce,\nPeter Ondruska, Ishaan Gulrajani, and Richard Socher.\nAsk me anything: Dynamic\nmemory networks for natural language processing. CoRR, abs/1506.07285, 2015.\nKarol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines.\nCoRR, abs/1511.06392, 2015.\nGuillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and\nChris Dyer. Neural architectures for named entity recognition. CoRR, abs/1603.01360,\n2016.\nGuillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. Fader networks: Manipulating images by sliding attributes.\nCoRR, abs/1706.00409, 2017. URL http://arxiv.org/abs/1706.00409.\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural\nnetworks without residuals. CoRR, abs/1605.07648, 2016.\nQuoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents.\nCoRR, abs/1405.4053, 2014.\nYann LeCun.\nURL http://yann.lecun.com.\nNew York University (NYU), NY, USA.\nFacebook AI Research (FAIR).\nYann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521:436 EP –,\n05 2015. URL http://dx.doi.org/10.1038/nature14539.\nHyungtae Lee, Sungmin Eum, and Heesung Kwon. ME R-CNN: multi-expert region-based\nCNN for object detection. CoRR, abs/1704.01069, 2017.\nIan Lenz, Honglak Lee, and Ashutosh Saxena. Deep learning for detecting robotic grasps.\nCoRR, abs/1301.3592, 2013.\n24\nWei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep ﬁlter pairing neu-\nral network for person re-identiﬁcation. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), Columbus, USA, June 2014.\nYuxi Li. Deep reinforcement learning: An overview. CoRR, abs/1701.07274, 2017.\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, abs/1312.4400,\n2013.\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-\nYang Fu, and Alexander C. Berg.\nSSD: single shot multibox detector.\nCoRR,\nabs/1512.02325, 2015.\nFujun Luan, Sylvain Paris, Eli Shechtman, and Kavita Bala. Deep photo style transfer.\nCoRR, abs/1703.07511, 2017.\nMinh-Thang Luong, Hieu Pham, and Christopher D. Manning. Eﬀective approaches to\nattention-based neural machine translation. CoRR, abs/1508.04025, 2015.\nLars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary\ndeep generative models.\nIn Proceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning - Volume 48, ICML’16, pages 1445–1454.\nJMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390.3045543.\nXudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, and Zhen Wang. Multi-class\ngenerative adversarial networks with the L2 loss function. CoRR, abs/1611.04076, 2016.\nGary Marcus. Deep learning: A critical appraisal. CoRR, abs/1801.00631, 2018. URL\nhttps://arxiv.org/abs/1801.00631.\nJonathan Masci, Alessandro Giusti, Dan C. Ciresan, Gabriel Fricout, and J¨urgen Schmid-\nhuber. A fast learning algorithm for image segmentation with max-pooling convolutional\nnetworks.\nIn IEEE International Conference on Image Processing, ICIP 2013, Mel-\nbourne, Australia, September 15-18, 2013, pages 2713–2717, 2013a. doi: 10.1109/ICIP.\n2013.6738559. URL https://doi.org/10.1109/ICIP.2013.6738559.\nJonathan Masci, Ueli Meier, Gabriel Fricout, and J¨urgen Schmidhuber.\nMulti-scale\npyramidal pooling network for generic steel defect classiﬁcation.\nIn The 2013 In-\nternational Joint Conference on Neural Networks, IJCNN 2013, Dallas, TX, USA,\nAugust 4-9, 2013, pages 1–8, 2013b.\ndoi:\n10.1109/IJCNN.2013.6706920.\nURL\nhttps://doi.org/10.1109/IJCNN.2013.6706920.\nGr´egoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Z. Hakkani-\nT¨ur, Xiaodong He, Larry P. Heck, G¨okhan T¨ur, Dong Yu, and Geoﬀrey Zweig. Using\nrecurrent neural networks for slot ﬁlling in spoken language understanding. IEEE/ACM\nTrans. Audio, Speech & Language Processing, 23(3):530–539, 2015.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word\nrepresentations in vector space. CoRR, abs/1301.3781, 2013a.\n25\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. CoRR, abs/1310.4546,\n2013b.\nPiotr Mirowski, Yann LeCun, Deepak Madhavan, and Ruben Kuzniecky. Comparing svm\nand convolutional networks for epileptic seizure prediction from intracranial eeg. In Proc.\nMachine Learning and Signal Processing (MLSP’08). IEEE, 2008.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,\nDaan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning.\nCoRR, abs/1312.5602, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski,\nStig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dhar-\nshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.\nHuman-level con-\ntrol through deep reinforcement learning.\nNature, 518:529 EP –, 02 2015.\nURL\nhttp://dx.doi.org/10.1038/nature14236.\nVolodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P.\nLillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for\ndeep reinforcement learning. CoRR, abs/1602.01783, 2016.\nJoel Moniz and Christopher J. Pal.\nConvolutional residual memory networks.\nCoRR,\nabs/1606.05262, 2016.\nArvind Neelakantan, Quoc V. Le, and Ilya Sutskever. Neural programmer: Inducing latent\nprograms with gradient descent. CoRR, abs/1511.04834, 2015.\nAnh Mai Nguyen, Jason Yosinski, and JeﬀClune. Deep neural networks are easily fooled:\nHigh conﬁdence predictions for unrecognizable images. CoRR, abs/1412.1897, 2014.\nMichael A. Nielsen.\nNeural Networks and Deep Learning.\nDetermination Press, 2015.\nhttp://neuralnetworksanddeeplearning.com.\nYusuke Niitani, Toru Ogawa, Shunta Saito, and Masaki Saito. Chainercv: a library for deep\nlearning in computer vision. CoRR, abs/1708.08169, 2017.\nChris Olah and Shan Carter. Attention and augmented recurrent neural networks. Distill,\n2016. doi: 10.23915/distill.00001. URL http://distill.pub/2016/augmented-rnns.\nMaxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-\nlevel image representations using convolutional neural networks. In Proceedings of the\n2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR ’14, pages\n1717–1724, Washington, DC, USA, 2014. IEEE Computer Society. ISBN 978-1-4799-5118-\n5. doi: 10.1109/CVPR.2014.222. URL http://dx.doi.org/10.1109/CVPR.2014.222.\nBaolin Peng and Kaisheng Yao.\nRecurrent neural networks with external memory for\nlanguage understanding. volume abs/1506.00195, 2015.\n26\nRyan Poplin, Dan Newburger, Jojo Dijamco, Nam Nguyen, Dion Loy, Sam S. Gross, Cory Y.\nMcLean, and Mark A. DePristo. Creating a universal snp and small indel variant caller\nwith deep neural networks. BioRxiv, 2016. URL https://doi.org/10.1101/092890.\nM. Ranzato, J. Susskind, V. Mnih, and G. Hinton. On deep generative models with appli-\ncations to recognition. In Proceedings of the 2011 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR ’11, pages 2857–2864, Washington, DC, USA, 2011.\nIEEE Computer Society. ISBN 978-1-4577-0394-2. doi: 10.1109/CVPR.2011.5995710.\nURL http://dx.doi.org/10.1109/CVPR.2011.5995710.\nAli Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.\nCNN\nfeatures oﬀ-the-shelf: an astounding baseline for recognition.\nCoRR, abs/1403.6382,\n2014.\nJoseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look\nonce: Uniﬁed, real-time object detection. CoRR, abs/1506.02640, 2015.\nScott E. Reed and Nando de Freitas.\nNeural programmer-interpreters.\nCoRR,\nabs/1511.06279, 2015.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-\ntime object detection with region proposal networks. In Neural Information Processing\nSystems (NIPS), 2015.\nDario Rethage, Jordi Pons, and Xavier Serra. A wavenet for speech denoising. CoRR,\nabs/1706.07162, 2017.\nDanilo Rezende, Shakir, Ivo Danihelka, Karol Gregor, and Daan Wierstra.\nOne-\nshot generalization\nin deep generative\nmodels.\nIn Maria Florina Balcan\nand\nKilian Q. Weinberger,\neditors,\nProceedings of The 33rd International Conference\non Machine Learning,\nvolume 48 of Proceedings of Machine Learning Research,\npages 1521–1529,\nNew York, New York,\nUSA, 20–22 Jun 2016. PMLR.\nURL\nhttp://proceedings.mlr.press/v48/rezende16.html.\nSara\nSabour,\nNicholas\nFrosst,\nand\nGeoﬀrey\nE.\nHinton.\nDynamic\nrouting\nbetween\ncapsules.\nIn\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n30:\nAnnual\nConference\non\nNeural\nInformation\nProcessing\nSystems\n2017,\n4-\n9\nDecember\n2017,\nLong\nBeach,\nCA,\nUSA,\npages\n3859–3869,\n2017.\nURL\nhttp://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and\nXi Chen. Improved techniques for training gans. CoRR, abs/1606.03498, 2016.\nTom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas\nR¨uckstieß, and J¨urgen Schmidhuber. Pybrain. J. Mach. Learn. Res., 11:743–746, March\n2010. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=1756006.1756030.\nJ. Schmidhuber.\nDeep Learning.\nScholarpedia, 10(11):32832, 2015.\ndoi:\n10.4249/\nscholarpedia.32832. URL http://www.scholarpedia.org/article/Deep_Learning. re-\nvision #152272.\n27\nJuergen Schmidhuber. URL http://www.idsia.ch/~juergen. IDSIA, USI. Dalle Molle\nInstitute for Artiﬁcial Intelligence, Manno-Lugano, Switzerland.\nJ¨urgen Schmidhuber.\nDeep Learning in Neural Networks:\nAn Overview,\nvolume\nabs/1404.7828. 2014. URL http://arxiv.org/abs/1404.7828.\nSamira Shabanian, Devansh Arpit, Adam Trischler, and Yoshua Bengio. Variational bi-\nlstms. CoRR, 1711.05717, 2017. URL https://arxiv.org/abs/1711.05717.\nShikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Action recognition using visual\nattention. CoRR, abs/1511.04119, 2015.\nYangyang Shi, Kaisheng Yao, Hu Chen, Dong Yu, Yi-Cheng Pan, and Mei-Yuh Hwang.\nRecurrent support vector machines for slot tagging in spoken language understanding. In\nHLT-NAACL, pages 393–399. The Association for Computational Linguistics, 2016a.\nYangyang Shi, Kaisheng Yao, Le Tian, and Daxin Jiang. Deep LSTM based feature map-\nping for query classiﬁcation.\nIn HLT-NAACL, pages 1501–1511. The Association for\nComputational Linguistics, 2016b.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den\nDriessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc-\ntot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,\nTimothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis\nHassabis. Mastering the game of go with deep neural networks and tree search. Nature,\n529:484 EP –, 01 2016. URL http://dx.doi.org/10.1038/nature16961.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai,\nArthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Tim-\nothy Lillicrap, Karen Simonyan, and Demis Hassabis.\nMastering chess and shogi by\nself-play with a general reinforcement learning algorithm. CoRR, abs/1712.01815, 2017a.\nURL https://arxiv.org/abs/1712.01815.\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang,\nArthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen,\nTimothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and\nDemis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354\nEP –, 10 2017b. URL http://dx.doi.org/10.1038/nature24270.\nKaren Simonyan and Andrew Zisserman. Two-stream convolutional networks for action\nrecognition in videos. CoRR, abs/1406.2199, 2014a.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. CoRR, abs/1409.1556, 2014b.\nNitish Srivastava,\nGeoﬀrey Hinton,\nAlex Krizhevsky,\nIlya Sutskever,\nand Ruslan\nSalakhutdinov.\nDropout:\nA simple way to prevent neural networks from over-\nﬁtting.\nJournal\nof\nMachine\nLearning\nResearch,\n15:1929–1958,\n2014.\nURL\nhttp://jmlr.org/papers/v15/srivastava14a.html.\n28\nRupesh Kumar Srivastava, Klaus Greﬀ, and J¨urgen Schmidhuber.\nHighway networks.\nCoRR, abs/1505.00387, 2015. URL http://arxiv.org/abs/1505.00387.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper\nwith convolutions. CoRR, abs/1409.4842, 2014.\nYaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the\ngap to human-level performance in face veriﬁcation. In Proceedings of the 2014 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR ’14, pages 1701–1708,\nWashington, DC, USA, 2014. IEEE Computer Society. ISBN 978-1-4799-5118-5.\ndoi:\n10.1109/CVPR.2014.220. URL http://dx.doi.org/10.1109/CVPR.2014.220.\nRaphael Tang and Jimmy Lin. Honk: A pytorch reimplementation of convolutional neural\nnetworks for keyword spotting. CoRR, abs/1710.06554, 2017.\nYichuan Tang, Ruslan Salakhutdinov, and Geoﬀrey Hinton. Deep lambertian networks. 2,\n06 2012.\nSasha Targ, Diogo Almeida, and Kevin Lyman. Resnet in resnet: Generalizing residual\narchitectures. CoRR, abs/1603.08029, 2016.\nDmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor S. Lempitsky. Texture net-\nworks: Feed-forward synthesis of textures and stylized images. CoRR, abs/1603.03417,\n2016.\nA¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A\ngenerative model for raw audio. CoRR, abs/1609.03499, 2016a.\nA¨aron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural\nnetworks. CoRR, abs/1601.06759, 2016b.\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double\nq-learning. CoRR, abs/1509.06461, 2015.\nBart van Merri¨enboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David\nWarde-Farley, Jan Chorowski, and Yoshua Bengio. Blocks and fuel: Frameworks for deep\nlearning. CoRR, abs/1506.00619, 2015.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A\nneural image caption generator. CoRR, abs/1411.4555, 2014.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. CoRR, 1506.03134,\n2017. URL https://arxiv.org/abs/1506.03134.\nHaohan Wang, Bhiksha Raj, and Eric P. Xing. On the origin of deep learning. CoRR,\nabs/1702.07800, 2017a.\nShenlong Wang. URL http://www.cs.toronto.edu/~slwang/. University of Toronto (U\nof T), Ontario, Canada.\n29\nYuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep\nJaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc V. Le, Yannis\nAgiomyrgiannakis, Rob Clark, and Rif A. Saurous. Tacotron: A fully end-to-end text-\nto-speech synthesis model. CoRR, abs/1703.10135, 2017b.\nJason Weston,\nSumit Chopra,\nand Antoine Bordes.\nMemory networks.\nCoRR,\nabs/1410.3916, 2014.\nMartin W¨ollmer, Florian Eyben, Alex Graves, Bj¨orn Schuller, and Gerhard Rigoll. Bidi-\nrectional lstm networks for context-sensitive keyword detection in a cognitive virtual\nagent framework. Cognitive Computation, 2(3):180–190, Sep 2010. ISSN 1866-9964. doi:\n10.1007/s12559-010-9041-8. URL https://doi.org/10.1007/s12559-010-9041-8.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, JeﬀKlingner, Apurva\nShah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang,\nCliﬀYoung, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,\nMacduﬀHughes, and Jeﬀrey Dean. Google’s neural machine translation system: Bridging\nthe gap between human and machine translation. CoRR, abs/1609.08144, 2016.\nEdgar Xi, Selina Bing, and Yang Jin.\nCapsule network performance on complex data.\nCoRR, abs/1712.03480v1, 2017. URL https://arxiv.org/abs/1712.03480v1.\nSaining Xie, Ross B. Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated\nresidual transformations for deep neural networks. CoRR, abs/1611.05431, 2016.\nCaiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual\nand textual question answering. CoRR, abs/1603.01417, 2016.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhut-\ndinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption\ngeneration with visual attention. CoRR, abs/1502.03044, 2015.\nZichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alexander J. Smola.\nStacked\nattention networks for image question answering. CoRR, abs/1511.02274, 2015.\nJason Yosinski, JeﬀClune, Yoshua Bengio, and Hod Lipson. How transferable are features\nin deep neural networks? In Proceedings of the 27th International Conference on Neural\nInformation Processing Systems - Volume 2, NIPS’14, pages 3320–3328, Cambridge, MA,\nUSA, 2014. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969033.2969197.\nTom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. Recent trends in\ndeep learning based natural language processing. CoRR, abs/1708.02709, 2017.\nMatthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.\nCoRR, abs/1311.2901, 2013.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Under-\nstanding deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016a.\n30\nRichard Zhang, Phillip Isola, and Alexei A. Efros. Colorful image colorization.\nCoRR,\nabs/1603.08511, 2016b.\nShi-Xiong Zhang, Chaojun Liu, Kaisheng Yao, and Yifan Gong. Deep neural support vector\nmachines for speech recognition. In ICASSP, pages 4275–4279. IEEE, 2015a.\nYu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yao, Sanjeev Khudanpur, and James R.\nGlass.\nHighway long short-term memory rnns for distant speech recognition.\nCoRR,\nabs/1510.08983, 2015b.\nYu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yao, Sanjeev Khudanpur, and James R. Glass.\nHighway long short-term memory RNNS for distant speech recognition. In ICASSP, pages\n5755–5759. IEEE, 2016c.\nZixing Zhang, J¨urgen T. Geiger, Jouni Pohjalainen, Amr El-Desoky Mousa, and Bj¨orn W.\nSchuller. Deep learning for environmentally robust speech recognition: An overview of\nrecent developments. CoRR, abs/1705.10874, 2017.\nShuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong\nSu, Dalong Du, Chang Huang, and Philip H. S. Torr.\nConditional random ﬁelds as\nrecurrent neural networks. CoRR, abs/1502.03240, 2015.\nJun-Yan Zhu, Philipp Kr¨ahenb¨uhl, Eli Shechtman, and Alexei A. Efros. Generative visual\nmanipulation on the natural image manifold. CoRR, abs/1609.03552, 2016.\nXiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and\nFriedrich Fraundorfer. Deep learning in remote sensing: a review. CoRR, abs/1710.03959,\n2017.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn´ık, and J¨urgen Schmidhuber.\nRecurrent highway networks.\nIn Proceedings of the 34th International Conference on\nMachine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 4189–\n4198, 2017. URL http://proceedings.mlr.press/v70/zilly17a.html.\n31\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2018-07-21",
  "updated": "2018-07-21"
}