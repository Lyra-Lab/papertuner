{
  "id": "http://arxiv.org/abs/1807.08217v1",
  "title": "Asynchronous Advantage Actor-Critic Agent for Starcraft II",
  "authors": [
    "Basel Alghanem",
    "Keerthana P G"
  ],
  "abstract": "Deep reinforcement learning, and especially the Asynchronous Advantage\nActor-Critic algorithm, has been successfully used to achieve super-human\nperformance in a variety of video games. Starcraft II is a new challenge for\nthe reinforcement learning community with the release of pysc2 learning\nenvironment proposed by Google Deepmind and Blizzard Entertainment. Despite\nbeing a target for several AI developers, few have achieved human level\nperformance. In this project we explain the complexities of this environment\nand discuss the results from our experiments on the environment. We have\ncompared various architectures and have proved that transfer learning can be an\neffective paradigm in reinforcement learning research for complex scenarios\nrequiring skill transfer.",
  "text": "Asynchronous Advantage Actor-Critic Agent for\nStarcraft II\nBasel Alghanem\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213\nbasela@cmu.edu\nKeerthana P G\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA 15213\nkgopalak@andrew.cmu.edu\nAbstract\nDeep reinforcement learning, and especially the Asynchronous Advantage Actor-\nCritic algorithm, has been successfully used to achieve super-human performance\nin a variety of video games. Starcraft II is a new challenge for the reinforcement\nlearning community with the release of pysc2 learning environment proposed by\nGoogle Deepmind and Blizzard Entertainment. Despite being a target for several\nAI developers, few have achieved human level performance. In this project we\nexplain the complexities of this environment and discuss the results from our\nexperiments on the environment. We have compared various architectures and\nhave proved that transfer learning can be an effective paradigm in reinforcement\nlearning research for complex scenarios requiring skill transfer.\n1\nIntroduction\nResurgence in deep reinforcement learning[10] research has resulted in the development of robust\nalgorithms that can deliver human level performance in domains such as Atari [1], the game of Go\n[2], three-dimensional virtual environments [3] and simulated robotics domains [4, 5]. Many of these\nrecent successes have been stimulated by the availability of simulated domains with an appropriate\nlevel of difﬁculty. To go beyond the benchmarks, the research community is required to develop\nalgorithms in domains that are beyond the capabilities of current methods.\nSC2LE (StarCraft II Learning Environment)[6] is a new challenge for exploring reinforcement\nlearning algorithms and architectures, based on the StarCraft II video game. StarCraft is a real-time\nstrategy (RTS) game that combines fast paced micro-actions with the need for high-level planning and\nexecution. The game has been an enduring sport among humans for over 2 decades with millions of\ncasual and highly competitive professional players, therefore, defeating top human players becomes a\nmeaningful long-term objective.\n1.1\nRelated Work\nMnih, et. al. [9] ﬁrst proposed the Asynchronous Advantage Actor-Critic (A3C) algorithm, and\napplied it to Atari video games. Atari games have similar input state space to Starcraft II, in that the\ninput is screens. However, the underlying state of Starcraft II is much more complicated because it has\nmany more layers of information. Additionally, the action space of Starcraft is orders of magnitude\nlarger. These differences are discussed further in 2.1.\nThe original paper proposing Starcraft as a new deep RL challenge by Vinyals et. al. [6] provides\nimportant benchmarks for our project. They used A3C with three different network architectures to\nproduce results across several minigames as well as on one map of the full game.\narXiv:1807.08217v1  [cs.AI]  22 Jul 2018\nAndersen, et. al., [17] applied deep reinforcement learning to Tower Line Wars, a game with\ncomplexity between Atari games and Starcraft II. They were ultimately unable to outperform simple\nrule-based agents. They observed that it is difﬁcult for a deep network to develop an accurate state\nrepresentation for a game with spatial actions.\nCerticky & Churchill [15] discussed the current state of Starcraft II AI competitions. Most of the bots\ndiscussed have substantial human-designed architecture. For example, they use different approaches\nfor deciding which task to perform next or how to target opposing units. Justesen & Risi [16]\ndiscuss one such approach for macromanagement. They collected state-action pairs of expert players\nand were able to predict the next build task of the experts 21% of the time. In contrast, a deep\nreinforcement learning approach would only use a human-designed agent that would learn to make\nlarge and small decisions.\nIn a class project similar to this one, Chang [13] used A3C and a network architecture similar to\nthe baseline network proposed by Vinyals et. al. [6]. He noted that network convergence depended\nstrongly on hyperparameter selection.\nIn another class project, Barratt & Pan [14] attempted to use imitation learning, using data collected\nfrom an experienced Starcraft II player, to teach an agent advanced tactics. While their agent was\nable to achieve the basic goal of the minigame they selected, it was not able to execute the complex\nstrategies of the expert player because their number of training samples was small compared to the\ngreat size of the state and action spaces.\n1.2\nProject Goals\nWe had a few goals for this project. We sought to determine what network architectures will\nallow the agent to converge quickly and consistently. Additionally, we considered the trade-off\nbetween convergence speed and whether the agent has converged on a suboptimal policy. Finally,\nwe investigated how well we can transfer learning to different minigames. For example, what\nimprovements in learning can be achieved by training an agent in one scenario which was pre-trained\non a different scenario?\n2\nMethods\nIn this project we used the A3C algorithm to yield results on the Starcraft 2 learning environment.\n2.1\nEnvironment\nThe SC2 environment is different from prior work in RL in many ways. First, it is a multi-agent\nproblem in which several players compete for inﬂuence and resources. Second, it is an imperfect\ninformation game. The map is only partially observed via a local camera, which must be actively\nmoved in order for the player to integrate information. Furthermore, it is necessary to actively explore\nthe map in order to determine the opponent’s state. Third, the action space is vast and diverse with\nmany different unit and building types, each with unique local actions. Furthermore, the set of legal\nactions available varies as the player progresses. Fourth, it has delayed credit assignment requiring\nlong-term strategies over thousands of steps.\nWe interact with the environment using PySC2, an open source python wrapper optimised for RL\nagents. PySC2 deﬁnes an action and observation speciﬁcation to ease the interaction between Python\nreinforcement learning agents and StarCraft II. It also includes some minigames as challenges and\nvisualisation tools to understand what the agent can see and do.\nReward structure\nThe agent plays against an in-built bot and to win a game, it must: 1. Accumulate resources, 2.\nConstruct production buildings, 3. Amass an army, and 4. Eliminate all of the opponent’s buildings.\nThe built-in AI is rule-based and comes with 10 difﬁculty levels. There are two different reward\nstructures: ternary(1(win), 0(tie), -1(loss)) received at the end of a game and a Blizzard score. The\nBlizzard score increases with more mined resources, decreases when losing units/buildings, and is\nnot affected by other actions (training units, building buildings, and researching). The Blizzard score\nis far less sparse than the ternary reward signal and can act as continual feedback for the agent.\n2\nObservations\nThe main observations come as sets of feature layers which depict speciﬁc information such as unit\ntypes, hit points, visibility, etc which are rendered at N*M pixels (where N and M are conﬁgurable).\nThere are two sets of feature layers: the minimap is a coarse representation of the state of the\nentire world, and the screen is a detailed view of a subsection of the world corresponding to the\nplayer’s on-screen view. There are 13 screens and 7 minimaps. In addition, there are various\nnon-spatial observations such as the amount of gas and minerals collected, the set of actions\ncurrently available, information about selected units, build queues, and units in a transport vehicle, etc.\nActions\nIn StarCraft, not all actions are available in every game state. For example, the move command\nis only available if a unit is selected. The number arguments required to call each action are also\ndifferent. Here, an action a is represented as a composition of a function identiﬁer a0 and a sequence\nof arguments which that function identiﬁer requires: a1, a2, ..., aL. To represent the full action space,\nthere are 524 action-function identiﬁers with 13 possible types of arguments. The response rate of\nthe agent can also be conﬁgured (default at one action every 8 game frames/ 180 actions per minute\n(APM). Notably, human players take between 30 and 500 APM.\n2.2\nA3C Algorithm\nThe Asynchronous Advantage Actor-Critic (A3C) algorithm (1) was proposed by Mnih, et. al. [9].\nIt builds on top of the Actor-Critic method, where two functions are developed: one that maintains\nand updates a policy (the actor) and another that evaluates the current state (the critic). In Advantage\nActor-Critic (A2C), the critic’s evaluation uses an advantage function, rather than a value function.\nThe actor’s policy is updated according to the critic’s advantage function, and the critic’s advantage\nfunction is updated according to transitions generated by the actor’s interaction with the environment.\nAlgorithm 1 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.\n// Assume global shared parameter vectors θ and θv and global shared counter T = 0\n// Assume thread-speciﬁc parameter vector θ′ and θ′\nv\nInitialize thread step counter t ←1\nrepeat\nReset gradients: dθ ←0 and dθv ←0\nSynchronize thread-speciﬁc parameters θ′ = θ and θ′\nv = θv\ntstart = t\nGet state st\nrepeat\nPerform at according to policy π(at|st; θ′)\nReceive reward rt and new state st+1\nt ←t + 1\nT ←T + 1\nuntil terminal st or t −tstart == tmax\nR =\n\u001a0\nfor terminal st\nV (st, θ′\nv)\nfor terminal st // Bootstrap from last state\nfor i ∈t −1, . . . , tstart do\nR ←ri + γR\nAccumulate gradients wrt θ′: dθ ←dθ + ∇θ′ log π(ai|si; θ′)(R −V (si; θ′\nv))\nAccumulate gradients wrt θ′\nv: dθv + ∂(R −V (si; θ′\nv))2/∂θ′\nv\nPerform asynchronous update of θ using dθ and of θv using dθv\nuntil T > Tmax\nThe A3C algorithm has multiple agents executing in parallel. Each agent regularly learns, passes on\nthat learning to shared memory, then reloads to that shared memory. These asynchronous updates are\nan alternative to experience replay and was ﬁrst proposed by Nair, et. al. [11]. Experience replay\naggregates memory and reduces non-stationarity and decorrelated updates in reinforcement learning.\nHowever, using experience replay limits the methods to off-policy algorithms that can update from\ndata generated by an older policy. Moreover, it uses more memory and incurs more computation\nper real interaction. The parallelism of asynchronous updates decorrelates the data since at any\n3\ngiven time, the parallel agents experience a variety of states. This allows the execution of on-policy\nalgorithms like Sarsa and actor-critic. Additionally, asynchronous agents can be run on a standard\nmulticore CPU instead of using specialized hardware.\nA3C also improves the diversity of the learning experience as multiple actors-learners running in\nparallel are likely to be exploring different parts of the environment. This diversity can be maximised\nby explicitly using different exploration policies and native behaviours in each actor-learner. Using\nmultiple parallel actor learnings is reported to have resulted in a reduction in training time that is\nroughly linear in the number of parallel actor-learners[9].\n2.3\nProposed Framework & Challenges\nFigure 1: Network Architecture[6]\nThere are several challenges here. Previous work using A3C in Atari games have either used fully\nspatial features such as images or fully non-spatial features, but here, the agent requires both classes\nof information to play well. Additionally, the action space is very large and is a function of the current\nstate. Actions fall under two categories: spatial and non-spatial. Spatial actions require a spatial\nargument, which is a pixel on the game map where the action should be taken. For each spatial action,\nthere are over 7,000 underlying actions. The combination of large state space and large action space\nwas a major challenge.\nThe framework we used is shown in 1. Our software was adapted from a pre-existing library developed\nby Xiaowei Hu [12] which implemented a network designed by Vinyals, et. al. [6]. To account for\nspatial and non-spatial actions, different policy outputs are included. The non-spatial action output\nindicates which action should be taken and the spatial action output indicates where actions that are\nspatial should be taken. This means that, for the correct spatial arguments to be selected, the state\nrepresentation should be enough information for the part of the network speciﬁc to spatial actions to\nguess what the best action will be.\nWe used three variations on this one network, described in 1. The ﬁrst was the baseline FullyConv\n(hereafter referred to as \"Baseline\") agent proposed by Vinyals et. al. [6]. The second, PlusFC, used\nan additional fully connected layer before each of the three network outputs (spatial action, non-spatial\naction, and value). The third used an additional convolutional layer speciﬁc to the minimaps and the\nscreens before they’re concatenated. All three networks used the same parameters, described in 2.\nTable 2: Training Parameters Used\nParameters\nValue\nLearning Rate\n5 ∗10−4\nDiscount Factor\n0.99\nScreen resolution\n64x64\nMinimap resolution\n64x64\nNo. of asynchronous agents\n4, 8, or 16 based on machine capacity\nFrequency of agent’s action\n1 per 8 steps of sc2bot\nExploration strategy\nEpsilon greedy\n4\nTable 1: Network Architectures Used\nBaseline\nPlusFC\nPlusConv\nMinimap Input Processing\n16 ch. 5x5 conv\n32 ch. 3x3 conv\n16 ch. 5x5 conv\n32 ch. 3x3 conv\n32 ch. 5x5 conv\n48 ch. 3x3 conv\n16 ch. 3x3 conv\nScreen Input Processing\n16 ch. 5x5 conv\n32 ch. 3x3 conv\n16 ch. 5x5 conv\n32 ch. 3x3 conv\n32 ch. 5x5 conv\n48 ch. 3x3 conv\n16 ch. 3x3 conv\nFlat Inputs Processing\n256-unit FC layer\n256-unit FC layer\n256-unit FC layer\nValue Output Processing\n1 shared 256-unit\nFC layer\n1 shared 256-unit\nFC layer\n128-unit FC layer\n1 shared 256-unit\nFC layer\nNon-Spatial Output Processing\n1 shared 256-unit\nFC layer\n1 shared 256-unit\nFC layer\n128-unit FC layer\n1 shared 256-unit\nFC layer\nSpatial Output Processing\n1 ch. 1x1 conv\n1 ch. 1x1 conv\n1 ch. 1x1 conv\n3\nResults\nFirst, we have collected benchmarks to put our progress in context. For the minigame scenarios,\nwe can compare against the leaderboard at starcraftgym.com/. We can also compare against a\nrandom agent playing the same minigames to see whether the agent has begun to converge. The\nmaximum score achieved by each network architecture trained on each scenario is shown in 3\nTable 3: Maximum Scores obtained\nMap\nBaseline\nPlusFC\nPlusConv\nMoveToBeacon\n29\n-\n-\nCollectMineralShards\n33\n-\n73\nDefeatRoaches\n42\n-\n54\nFindAndDefeatZerglings\n16\n9(in 16k up-\ndates)\n-\nDefeatBanelingsAndZerglings\n113(with\ntransfer in 4k\nepisodes)\n130\n-\nBuildMarines\n0\n-\n-\nSimple64\n6750\n-\n-\n3.1\nMap By Map Discussion\n1. MoveToBeacon: Here, the agent earns rewards by moving the single unit to a beacon.\nWhenever the agent earns a reward for reaching the beacon, the beacon is teleported to a\nrandom location.\nUsing the baseline agent, we are able to perform very well and get human-level performance.\nOur performance is similar to the top performances on the leaderboard. Video of trained\nagent can be viewed here. The results for this scenario are shown in 2.\nFigure 2: MoveToBeacon Rewards\n5\n2. DefeatRoaches: Here, the objective is to defeat units labeled Roaches. Video of trained\nagent can be viewed here. We were able to achieve close to the optimal behavior, but we\nwere unable to achieve leaderboard-level results. The results for this scenario are shown in\n3.\nFigure 3: DefeatRoaches Rewards\n3. CollectMineralShards: Here, the objective is to use 2 marines to collect minerals in the\nvisible section of map. While the optimal policy is to send the marines in two different\ndirections (thereby collecting minerals twice as fast), our agent learns a successful but\nsub-optimal policy where the two marines collect minerals together. Video of trained agent\ncan be viewed here.\nFor this scenario, we tested transfer learning. We took the ﬁnal model weights from our\nDefeatRoaches network and used it as the starting point for PlusConv training. Both\nscenarios involve selecting units and moving them within the original screen. As shown\nin 4, this transfer learning resulted in substantially better results than training the Baseline\nnetwork and substantially faster convergence than training the PlusConv network from\nscratch. We were not able to achieve leaderboard-level results.\nFigure 4: Baseline (top) vs PlusConv with Transfer vs PlusConv from scratch for the CollectMineral-\nShards minigame\n4. FindAndDefeatZerglings: Here, the agent needs to balance exploration and combat to ﬁnd\nstationary Zerglings on the full-map and destroy them. After 97k episodes, our agent is\ngood at defeating Zerglings that are already found but doesn’t explore the entire map to\nﬁnd all the Zerglings. About 30-40% of the map is left unexplored. It can further be seen\nthat during the initial training, subsequent models improve on their exploration capability.\nLonger training, with forcing exploration may improve performance. Video of trained agent\nat 97k can be viewed here. Results from this scenario are shown in 5\n6\nFigure 5: Baseline and PlusFC for the FindAndDefeatZerglings minigame\n5. DefeatZerglingsAndBanelings: Agent needs to control and move Marines to destroy Zer-\nglings and Banelings which are enemy units. Video of deep-FCN trained agent, without\ntransfer learning can be viewed here. We also took the network learned from FindAndDe-\nfeatZerglings and used that as a starting point to train on DefeatZerglingsAndBanelings.\nThese results are shown in 6. An interesting observation is that FindAndDefeatZerglings\nlearns a policy of moving all marines together as a unit. While the agent at ﬁrst does well\nusing this policy on the new map, scores begin to drop as it explores newer policies where\nthe marines split up to attack and eventually learns a more optimal policy and scores improve.\nFigure 6: PlusFC from scratch vs. Baseline with transfer for the DefeatZerglingsAndBanelings\nminigame\n6. BuildMarines: Here, the objective is to build marines by collecting minerals, which are then\nused to build supply depots and barracks, further used to build Marines. Our agent was only\ntrained until 4k episodes( when it reached AWS storage maximum) up to which point the\nagent only learnt to move some SCVs to collect minerals. It never explored the other actions.\n7\nSince it never builds any marines, rewards were zero. This is also the toughest minigame.\nVideo of agent in action at 4k can be viewed here\n7. Simple64: It is the full game on a simple map where the agent is required to use all of the\nabove scenarios simultaneously. Video of trained agent playing the composite game on\nSimple64 map can be viewed here. To our best knowledge, there is no reported baseline\nperformance on Simple64 map including at StarCraftGym. Our results on this scenario are\nshown in 7.\nFigure 7: Plot of scores on the composite Simple64 map for 18k episodes using Baseline network\n4\nDiscussion\n4.1\nOur Conclusions\nFrom this work, we can conclude the following:\n1. Given the complexity of the action space, the challenging part in this problem was to\ndetermine where on the map to take the action.\n2. The models often degraded which compelled us to prune by returning to a good checkpoint\nand retraining.\n3. The more difﬁcult maps requires additional complexity in the network to be able to learn\neffectively. AS a general rule, the PlusFC and PlusConv models perform better than Baseline\nmodel.\n4. Transfer learning has been the most important breakthrough in this work. It drastically\nshortens training time and allows agents to explore new policies, not to mention that the\ntransferred policy often does really well on newer maps. In order to solve maps of higher\ndifﬁculty which are composites of easier minigames, transfer learning is surely the way to\ngo.\n4.2\nFuture Work\nIn the more complex scenarios, our agents converged on suboptimal strategies. Future work could\ninvestigate algorithms and network architectures that encourage further exploration. For example,\nMnih, et. al. [9] recommend the addition of an entropy-based term to the value output for the A3C\nnetwork.\nFuture work could investigate whether different network architectures can achieve greater perfor-\nmance. Our networks are all simple feed-forward, whereas a network including an LSTM layer\nwould have memory. The insight of memory could allow the agent to map dependencies in time\ndomain and learn better strategies.\nFinally, future work could take our methods and use them on more difﬁcult challenges. For example,\nwe could train on real Starcraft II maps instead of scenarios/minigames. We could also try transfer\nlearning from the most complex scenario we tried (BuildMarines) to the Simple64 map or other full\ngame maps.\n8\nReferences\n[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\nreinforcement learning. Nature, 518(7540):529–533, 2015.\n[2] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of Go with\ndeep neural networks and tree search. Nature, 529 (7587):484–489, 2016.\n[3] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Kuttler, Andrew\nLefrancq, Simon Green, Vıctor Valdes, Amir Sadik, et al. DeepMind Lab. arXiv preprint arXiv:1612.03801,\n2016\n[4] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye\ncoordination for robotic grasping with deep learning and large-scale data collection. The International Journal of\nRobotics Research, page 0278364917710318, 2016\n[5]Andrei A Rusu, Matej Vecerik, Thomas Rothorl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-\nreal robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286, 2016.\n[6]Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A. S., Yeo, M., ... Tsing, R. (2017).\nStarCraft II: A New Challenge for Reinforcement Learning. https://doi.org/https://deepmind.com/\ndocuments/110/sc2le.pdf\n[7]\nSteven\nBrown.\nBuild\na\nSparse\nReward\nPySC2\nAgent.\nhttps://itnext.io/\nbuild-a-sparse-reward-pysc2-agent-a44e94ba5255\n[8] Ilya Kostrikov. PyTorch implementation of Asynchronous Advantage Actor Critic (A3C). https://github.\ncom/ikostrikov/pytorch-a3c\n[9] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley,\nDavid Silver, Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. ICML 2016.\n[10] Richard S. Sutton and Andrew G. Barto. 1998. Introduction to Reinforcement Learning (1st ed.). MIT\nPress, Cambridge, MA, USA.\n[11] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria,\nVedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih,\nKoray Kavukcuoglu, and David Silver. Massively parallel methods for deep reinforcement learning. In ICML\nDeep Learning Workshop. 2015.\n[12] Xiaowei Hu. PySC2 agents. https://github.com/xhujoy/pysc2-agents\n[13] Andrew G. Chang.\nDeep RL For Starcraft II. http://cs229.stanford.edu/proj2017/\nfinal-reports/5234603.pdf\n[14] Jeffrey Barratt, Chuanbo Pan. Deep Imitation Learning for Playing Real Time Strategy Games. http:\n//cs229.stanford.edu/proj2017/final-reports/5244338.pdf.\n[15] Michal Certicky, David Churchill. The Current State of StarCraft AI Competitions and Bots. http:\n//agents.fel.cvut.cz/~certicky/files/publications/aiide17-certicky-churchill.pdf.\n[16] Niels Justesen, Sebastian Risi. Learning Macromanagement in StarCraft from Replays using Deep Learning.\nhttps://arxiv.org/pdf/1707.03743.pdf.\n[17] Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo.\nTowards a Deep Reinforcement\nLearning Approach for Tower Line Wars.\nhttps://link.springer.com/content/pdf/10.1007%\n2F978-3-319-71078-5_8.pdf\n9\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2018-07-22",
  "updated": "2018-07-22"
}