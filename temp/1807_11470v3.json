{
  "id": "http://arxiv.org/abs/1807.11470v3",
  "title": "Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis",
  "authors": [
    "Gustav Eje Henter",
    "Jaime Lorenzo-Trueba",
    "Xin Wang",
    "Junichi Yamagishi"
  ],
  "abstract": "Generating versatile and appropriate synthetic speech requires control over\nthe output expression separate from the spoken text. Important non-textual\nspeech variation is seldom annotated, in which case output control must be\nlearned in an unsupervised fashion. In this paper, we perform an in-depth study\nof methods for unsupervised learning of control in statistical speech\nsynthesis. For example, we show that popular unsupervised training heuristics\ncan be interpreted as variational inference in certain autoencoder models. We\nadditionally connect these models to VQ-VAEs, another, recently-proposed class\nof deep variational autoencoders, which we show can be derived from a very\nsimilar mathematical argument. The implications of these new probabilistic\ninterpretations are discussed. We illustrate the utility of the various\napproaches with an application to acoustic modelling for emotional speech\nsynthesis, where the unsupervised methods for learning expression control\n(without access to emotional labels) are found to give results that in many\naspects match or surpass the previous best supervised approach.",
  "text": "PREPRINT. WORK IN PROGRESS.\n1\nDeep Encoder-Decoder Models for Unsupervised\nLearning of Controllable Speech Synthesis\nGustav Eje Henter, Member, IEEE, Jaime Lorenzo-Trueba‡, Member, IEEE, Xin Wang, Student Member, IEEE,\nand Junichi Yamagishi, Senior Member, IEEE\nAbstract—Generating\nversatile\nand\nappropriate\nsynthetic\nspeech requires control over the output expression separate\nfrom the spoken text. Important non-textual speech variation is\nseldom annotated, in which case output control must be learned\nin an unsupervised fashion. In this paper, we perform an in-\ndepth study of methods for unsupervised learning of control in\nstatistical speech synthesis. For example, we show that popular\nunsupervised training heuristics can be interpreted as variational\ninference in certain autoencoder models. We additionally connect\nthese models to VQ-VAEs, another, recently-proposed class of\ndeep variational autoencoders, which we show can be derived\nfrom a very similar mathematical argument. The implications\nof these new probabilistic interpretations are discussed. We\nillustrate the utility of the various approaches with an application\nto acoustic modelling for emotional speech synthesis, where the\nunsupervised methods for learning expression control (without\naccess to emotional labels) are found to give results that in many\naspects match or surpass the previous best supervised approach.\nIndex Terms—Controllable speech synthesis, latent variable\nmodels, autoencoders, variational inference, VQ-VAE.\nI. INTRODUCTION\nT\nEXT to speech (TTS) is the task of turning a given\ntext into an audio waveform of the text message being\nspoken out loud. While speech waveforms have a very high\nbitrate (e.g., 705,600 bits per second for CD-quality audio), the\nspoken text only accounts for a handful of these bits, perhaps\n50 or 100 bits per second [1]. A major challenge of text-to-\nspeech synthesis is thus to ﬁll in the additional bits in the audio\nsignal in an appropriate and convincing manner. This is not an\neasy task, as speech features have complex interdependencies\n[2]. Furthermore, much of the excess acoustic variation in\nspeech is not completely random and incidental, but conveys\nadditional side-information of relevance to communication.\nThe acoustics may, for instance, reﬂect characteristics such as\nManuscript last revised July 30, 2018.\nThis research was carried out while all authors were with the Digital\nContent and Media Sciences Research Division at the National Institute of\nInformatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan.\nG. E. Henter is with the Department of Speech, Music and Hearing (TMH)\nat KTH Royal Institute of Technology, 100 44 Stockholm, Sweden. (e-mail:\nghe@kth.se)\nJ. Lorenzo-Trueba is with Amazon.com in Cambridge, U.K. (e-mail:\njaime@nii.ac.jp)\nX. Wang is with the Digital Content and Media Sciences Research Division\nat the National Institute of Informatics, Japan. (e-mail: wangxin@nii.ac.jp)\nJ. Yamagishi is with the Digital Content and Media Sciences Research\nDivision at the National Institute of Informatics, Japan, as well as with the\nCentre for Speech Technology Research at the University of Edinburgh, 10\nCrichton Street, Edinburgh EH8 9AB, U.K. (e-mail: jyamagis@nii.ac.jp)\n‡ Work performed prior to joining Amazon.\nspeaker identity, speaker condition, speaker mood and emo-\ntion, pragmatics (via emphasis and intonation), the acoustic\nenvironment, and properties of the communication channel\n(microphone characteristics, room acoustics). Neither of these\nare determined by the spoken text.\nIdeally, the acoustic cues and variability encountered in\nnatural speech should not only be replicated in the acoustics\nto make the synthesis more convincing, but also be adjustable\nto create ﬂexible and expressive synthesisers, and ultimately\nenhance communication between man and machine. Unfortu-\nnately, this is not the case today. Most statistical parametric\nspeech synthesis approaches are based on supervised learn-\ning, and only account for the variation that can be directly\nexplained by the annotation provided. Any deviations from\nthe conditional mean as predicted from annotated labels is\nassumed to be random and largely uncorrelated, regardless of\nany structure or information it may possess.\nAt synthesis time, recreating the lost variability by drawing\nrandom samples from ﬁtted Gaussian models has been found\nto be a poor strategy from a perceptual point of view, cf.\n[3], wherefore the predicted average speech features are used\nin synthesis instead; in fact, acoustic models must be highly\naccurate before random sampling outperforms the average\nspeech [2]. Using the model mean for synthesis makes the\nsame utterance sound exactly identical every time it is synthes-\nised (unlike when humans speak), and is still likely to give rise\nto artefacts, for instance widened formant bandwidths when\nusing spectral or cepstral acoustic feature representations.\nIn theory, salient variation beyond the text could be an-\nnotated in the database, enabling the acoustic effects of the\nadditional labels to be learned during training and controlled\nduring synthesis. However, speech annotation is laborious,\ndifﬁcult, and often subjective. This makes it costly to obtain\nsufﬁcient amounts of data where non-text variation has been\nannotated accurately. Instead, synthesis practise has focussed\non reducing the amount of (unhandled) acoustic variability\nby recording TTS databases of single talkers reading text in\na consistent neutral tone. The use of such data for building\nsynthesisers may beneﬁt segmental acoustic quality, but likely\ncontributes to the ﬂat and detached delivery that many text-\nto-speech systems suffer from. Several publications [4]–[7]\nhave meanwhile highlighted the potential beneﬁts of acoustic\nvariation (at least when annotated), for instance [7] presenting\nmulti-speaker synthesisers that are more accurate than could be\nexpected from training on any single speaker in the database\nalone and additionally allow control over properties of the\ngenerated speech, such as the speaker’s voice.\narXiv:1807.11470v3  [eess.AS]  9 Sep 2018\n2\nPREPRINT. WORK IN PROGRESS.\nThis paper considers a number of alternatives to the standard\napproach outlined above. The common theme is to investigate\nand connect methods that attempt to explicitly account for the\neffects of unannotated variation in the data. These methods are\nable to learn synthesisers with controllable output acoustics\n(beyond the effects of the input text), albeit without an a-\npriori labelling of the perceptual effects of the learned control;\nthis can be seen as an important, though not sufﬁcient, step\nto eventually enable ﬂexible speaking systems that respond\nappropriately to communicative context. Mathematically, our\nperspective is that of probabilistic modelling, speciﬁcally the\ntheory of latent variables, and a major part of the work is to\nestablish theoretical connections between practical approaches\nand principles of statistical estimation. Our main scientiﬁc\ncontributions can be summarised as follows:\n1) We use variational methods to show that several prior\nmethods for learning controllable models from data with\nunannotated variation – the training heuristic used in\n[7]–[12], as well as so-called VQ-VAEs from [13] –\ncan be interpreted as approximate maximum-likelihood\napproaches, and elucidate the approximations involved.\n2) We introduce and detail various theoretical connections\nbetween the techniques in [7], [10]–[12] and encoder-\ndecoder models, particularly VQ-VAEs.\n3) We consider ways in which prior information can be\nintegrated into the heuristic approaches (which lack an\nexplicit prior distribution).\n4) We use a large database of emotional speech to perform\nobjective and subjective empirical evaluations of the\nheuristic approaches (with and without prior informa-\ntion) against comparable VQ-VAEs and a competitive\nsupervised system on the task of acoustic modelling.\nThe unsupervised methods are found to produce equal\nor better results than the supervised approach.\nThese contributions all extend preliminary work performed in\n[14].\nThe remainder of this article is laid out as follows: Sec.\nII outlines relevant prior work while Sec. III describes math-\nematical foundations. Sec. IV then presents novel interpret-\nations of and connections between different encoder-decoder\napproaches. Sec. V recounts empirical evaluations performed\non a database of emotional speech, while Sec. VI concludes.\nII. PRIOR WORK\nIn this section, we introduce controllable speech synthesis\n(Sec. II-A) and a wide variety of previous work of relevance\nto our contributions. We especially consider unsupervised\nlearning of control (Sec. II-B) and variational autoencoders\n(Sec. II-C) and their use in speech generation (Sec. II-D). We\nalso give an introduction to prior work on emotional speech\nsynthesis (Sec. II-E), as this is the control task considered in\nour experiments.\nA. Controllable Speech Synthesis\nAll text-to-speech systems are in a sense controllable, since\nthe input text inﬂuences the output audio. (Voice conversion,\nsimilarly, represents a speech synthesiser driven by speech\nrather than text.) By controllable speech synthesis, however,\nwe refer to speech synthesisers that enable additional output\ncontrol beyond the words alone, such that the same text can\nbe made to be spoken in several, perceptually distinct ways.\nEarly, rule-based parametric speech synthesisers typically\nexposed many control knobs (“speech parameters”) relating\nto speech articulation and pronunciation; the text-to-speech\naspect was simply a set of rules for how these knobs were to\nbe moved in response to phonemes extracted from text [15],\nand the resulting parameter trajectories could be manually\nedited in order to alter pronunciation. Unit selection TTS can\nachieve control of any properties annotated in the database\nby including a term in the target cost to preferentially select\nunits with labels similar to the user-selected control input.\nHowever, success depends heavily on the database having\nadequate coverage of the desired control conﬁguration.\nWith the transition to statistical parametric speech synthesis\n(SPSS), [16], [17] it became straightforward to learn to control\nsynthesiser output, i.e., to learn a mapping from control inputs\nto acoustic outputs. This avoids having to design the signal\ngenerator to expose the desired speech properties to be con-\ntrolled or manually tuning weight factors in the target cost, and\ntypically achieves meaningful control from smaller training\ndatabases than unit-selection approaches. The decision trees\nused in early SPSS systems can relatively easily incorporate\nadditional categorical labels as phone- or frame-level inputs.\nContinuous-valued inputs can be quantised for decision-tree\nlearning, and the quantisation threshold can be learned as well\n(e.g., through C4.5 [18]). So-called multiple regression HMMs\n(MR-HMMs) [19] were developed as a more reﬁned method\nfor continuous control of synthesiser output, by endowing each\ndecision-tree node with a linear regression model that maps\ncontrol inputs to acoustics. MR-HMMs and their extensions\nhave been used for smoothly controlling properties such as\nspeaking style [20], [21] or articulation [22].\nB. Learning Control Without Annotation\nThe approaches covered in Sec. II-A all rely on control\neither being manually designed, or learned in a supervised\nmanner from annotated data. This paper, in contrast, considers\nthe more difﬁcult situation where salient speech variability\nhas not been annotated, but we nonetheless wish to learn to\naccount for and replicate such variability by adjusting some\nsynthesiser control inputs separate from the input text.\nMany approaches to this problem exist. Unlike, e.g., Jauk\n[23], where the control space is deﬁned by clustering training\nutterances based on pre-deﬁned acoustic features, we con-\ncentrate on approaches that treat the unknown values of the\nhypothesised control parameters as if they were part of the\nset of unknown model parameters, and estimate all these\nunknowns through optimisation over the training data. This\nwill learn a synthesiser that allows the control over the most\n(mathematically) salient extra-linguistic speech variation, but\nprovides no a-priori indication what perceptual aspects that\nwill be controllable (or how). One example of this approach\nis so-called cluster-adaptive training (CAT), introduced for\nautomatic speech recognition (ASR) in [24]. It can be seen as\nPREPRINT. WORK IN PROGRESS.\n3\nan extension of MR-HMMs to learning and optimising both\ndecision-tree node regression models and their inputs. CAT\nhas for instance been applied to learn expressive TTS with\ndecision trees [25]. However, the method does not include a\njoint optimisation over the regression tree structure, and the\npossible uncertainty in the determination of the control input\nvalues from the acoustics is ignored.\nWith modern synthesis techniques based on deep learning\nthere have been multiple independent proposals to improve\nmodelling by using backpropagation to jointly optimise the\nentire regression model (the unknown weights of one or more\nneural networks) together with its control inputs. The idea\nwas introduced for speaker adaptation in neural network ASR\nin [8], [9] under the name “discriminant condition codes”\n(DCC), and was independently adapted for multi-speaker\nspeech synthesis several times: ﬁrst by Luong et al. [7] and\nmore recently by Arık et al. [11] (Deep Voice 2) and Taigman\net al. [12] (VoiceLoop). In all cases, the result is that training\nand test speakers all are embedded in a low-dimensional\nspeaker space. Independent of [9], Watts et al. [10] also\nproposed a mathematically identical setup and applied it to\ntrain a TTS acoustic model on a database of expressive speech,\nspeciﬁcally children’s audiobooks from [26]. (The equivalence\nbetween [9] and [10] was ﬁrst pointed out in [14].) Watts et\nal. learned a ﬁxed input vector for each utterance in the data,\ncalling the approach “learned sentence-level control vectors”.\nAdjusting the control parameter input when synthesising from\nthe trained system was found to adjust vocal effort (pitch and\nenergy) in a nonlinear and non-uniform manner.\nSawada et al. [27] considered similar data but took a\nsomewhat different approach, wherein a unique “phrase code”\nwas assigned to each phrase in the training data through\nrandom draws from a high-dimensional Gaussian distribution;\nthis code was then used as an input to the synthesiser alongside\nthe features extracted from the text. For test sentences, the\nphrase code of the training-data phrase with the greatest\nsimilarity (as computed through by doc2vec [28]) to the text\nphrase to be spoken was used as the control parameters.\n(They also assigned “word codes” to each word in a similar\nmanner.) This overall approach is similar to the approaches\nwith learned input codes – especially [10] – in that training-\ndata segments were embedded in a ﬁxed-dimensional space\nused to control the output, but here the embeddings were\nrandom rather than learned, and codes were predicted based on\ntext rather than acoustics. Trained on children’s audiobooks the\nresulting synthesiser achieved notably successful expression\ncontrol and was one of the best-rated systems in the 2017\nBlizzard Challenge [27], [29].\nLuong et al. [7] evaluated both random and learned input\ncodes with different dimensionalities for representing speaker\nvariation, and compared them to simple one-hot vector speaker\ncodes. They found no major differences in subjective per-\nformance between the methods, though all were better than\nno adaptation. However, we note that this and other speaker-\nadaptation evaluations typically involve some degree of super-\nvision, since it generally is pre-speciﬁed which utterances that\ncame from each speaker.\nIn the last year, there have been efforts to learn unsupervised\ncontrol in the (mostly) end-to-end Tacotron [30] TTS frame-\nwork. Parallel to this paper being written, these demonstrated\nthe use of encoders and decoders for prosody transfer across\nspeakers (given similar text prompts) [31] and more general\nstyle control [32]. This extends and improves on preliminary\nwork presented by the same group in [33], which learned\nframewise rather than utterance-level control. Among other\nthings, they demonstrate that the style-token approach in [32]\nis capable of synthesis with high subjective quality even from\n95% noisy training data. They also demonstrated the use of\na separately-learned speaker veriﬁcation system as an encoder\nfor controlling and adapting speaker identity [34].\nC. Variational Autoencoders\nInterestingly, all of the above proposals for unsupervised\nlearning of controllable speech synthesis gloss over the issue\nthat the actual values of any control inputs cannot be determ-\nined to exact certainty, since they are neither annotated nor\nobserved. To properly account for the uncertainty regarding the\nunknown control inputs calls for the use of latent (or hidden)\nvariables associated with each datum. The fundamental idea is\nsimply to model the unknown quantities and their uncertainty\nas random variables. We can then use the theory of probability\nand estimation to make inferences about these unobserved\nvariables. In practice, the mathematics are very similar to\nBayesian probability, but the prior and posterior distributions\npertain to (local) control inputs, not to the (global) model\nparameters, which may still be treated in a frequentist manner.\nLatent-variables are ubiquitous in speech modelling, with\ntwo examples being the component in a mixture model and\nthe unobservable state variable in hidden Markov models\n(HMMs) [35], [36]. Training algorithms for these latent-\nvariable approaches are usually derived from the expectation-\nmaximisation (EM) framework [37]. However, the express-\niveness of these classical methods is often quite limited, and\nnew setups generally require careful, manual derivation of\nupdate equations, which often is prohibitively difﬁcult for\nmore complex and interesting models.\nA recent idea is to harness the power of deep learning to\ndescribe and train more ﬂexible latent-variable models. Using\ntechniques similar to [37], Henter et al. [14] showed that, for\nthe special case of EM-like alternate optimisation, the heuristic\nmethods [7]–[12] can be seen as “poor man’s latent variables”\nthat can learn a complex mapping from latent to observable\nvariables but ignore any uncertainty in the latent space. A more\nfull-ﬂedged example of deep learning of latent variables is so-\ncalled variational autoencoders (VAEs) [38], [39]. They use\nneural networks to parameterise both how observations depend\non continuous latent variables (control inputs) along with the\nact of inferring latent-variable distributions from observations.\nVAEs are considered autoencoders since the inference process\ncan be seen as encoding an observation into a latent variable\nvalue (or distribution) while the generation can be seen as\ndecoding the latent variable back to the observation domain.\nWe elaborate on this connection in Sec. III-C. Furthermore,\nthe two mappings can be learned tractably and jointly through\ngradient descent [40], in contrast to some mathematically\nsimilar models such as Helmholtz machines [41].\n4\nPREPRINT. WORK IN PROGRESS.\nA practical issue with VAEs is that they sometimes fail to\nlearn to make proper use of the latent variables to explain the\nobserved variation: in that case, the estimated control inputs\ndo not change appreciably over the training data (their inferred\ndistributions are highly overlapping) and exert little inﬂuence\nover model outputs, cf. [42]. Chen et al. [43], Husz´ar [44], and\nGraves et al. [45] provide lucid discussions of this problem.\nThis has been called “posterior collapse” in [13], although it\ndoes not mean that the posterior collapses to a point – just\nthat the posterior collapses to the same distribution (which is\nalso the prior) regardless of the observation made. A recent\nproposal to combat this issue is to quantise the encoder output\nthrough a vector-quantisation (VQ) step, such that the inferred\nvalue of the hidden variable for an observation is taken from a\nﬁnite codebook. The resulting construction is called VQ-VAE,\nand was introduced in [13]. While the regular VAEs objective\nfunction penalises the variational posterior diverging from\nthe prior (which can force “posterior collapse”), this penalty\nreduces to a constant for the VQ-VAE, and thus does not affect\nlearning. Although the fact that only a single codebook vector\nis used for each observation means that any uncertainty in\nthe inference step is not represented explicitly, we show in\nSec. IV-A that the mathematics still can be derived from the\nsame latent-variable principles that underpin regular VAEs.\nVQ-VAEs might use discrete latent variables, but these latents\nare nonetheless embedded in a continuous Euclidean space.\nWhile Gaussian mixture models and HMMs also consider\ndiscrete latent variables that are in some sense embedded\n(through their mean vectors) in a vector space, VQ-VAEs\nlet the latent vectors occupy a space different from that of\nthe observations. The VQ-VAE mapping from latent space\nto observation space is furthermore strongly nonlinear, which\ndifferentiates it from constructions like subspace GMMs [46].\nVariational autoencoders also resemble recently-popular\ngenerative adversarial networks (GANs) [47], in that the latter\nalso use a random latent variable to explain variation in the ob-\nservations through a highly-nonlinear mapping parameterised\nby a neural network. However, VAEs map latent variable\nvalues to output distribution parameters, whereas GANs map\nlatent samples directly to observations. Parameter estimation\nin GANs is also more challenging, since one seeks a Nash\nequilibrium of a game between two agents, rather than an\noptimum of a ﬁxed objective function as in VAEs. A taxonomy\nof different generative models such as VAEs and GANs, along\nwith connections between them, is provided in [48]. In Sec.\nIV this paper, we bring the widely-used heuristic from Sec.\nII-B (DCC/sentence-level control vectors) into the fold, by\ndescribing its connections to VAEs and latent-variable models.\nD. Variational Autoencoders in Synthesis\nVariational autoencoders have seen a number applications to\nspeech generation. For example, [42], [49]–[51] all consider\napplying VAEs to each frame in an acoustic analysis of speech,\nwith the intention of learning to encode something similar\nto phonetic identity in the absence of transcription. In [49],\n[50], this was used to identify matching data frames for non-\nparallel voice conversion. [52], [53] used VAEs to separate\nand manipulate both speaker and phone identities, though\nwithout generating or evaluating speech audio. Very recently\n[54] used VAEs to identify sentence-level latent variables in\nthe VoiceLoop [12] framework.\nVAEs have also been applied to speech waveform model-\nling, typically based on generalisations of basic VAEs to se-\nquence models such as [55]–[58]. While [56]–[58] all contain\napplications to speech data, only Chung et al. [56] considered\nspeech signal generation. Unfortunately, the perceptual quality\nof random waveforms sampled from their model is poor:\nthere is a lot of static, and no intelligible speech is produced,\nsince the models are not conditioned on an input text. Much\nbetter segmental quality has been demonstrated by generating\nsignals using WaveNet [5]. In a standard WaveNet the next-\nstep distribution only depends on the previous waveform in the\nreceptive ﬁeld and possible conditioning information, with no\nhidden state. Other successful neural networks for waveform\ngeneration include SampleRNN [59] and WaveRNN [60],\nwhich contain a deterministic (hidden) RNN state. The VQ-\nVAE paper [13] combines these breakthroughs (speciﬁcally\nWaveNet) with VAEs, using strided convolutions to down-\nsample and encode raw audio into discrete quantisation indices\nwith a WaveNet-like architecture for decoding. This approach\nwas able to reproduce high-quality versions of encoded wave-\nforms, and the quantisation indices were additionally found to\nbe closely related to phones, providing a compelling demon-\nstration of unsupervised acoustic unit discovery.\nWang [61, Ch. 7] investigated VQ-VAEs for F0 modelling\non the utterance, mora, and phone levels in Japanese TTS,\ncoupled with a linguistic linker to predict VQ-VAE codebook\nindices from linguistic features. It was found that a combined\nVQ-VAE approach on the mora and phone levels performed\nobjectively and subjectively on par with a larger deep, autore-\ngressive F0 model [62] without explicit latent variables.\nDifferent from the prior work above, but similar to the heur-\nistics [7]–[12] in Sec. II-B, this paper considers (VQ-)VAE ap-\nproaches that model and encode utterance-wide, non-phonetic\ninformation that complements the known transcription.\nThe work on speech synthesis with global style tokens\n(GSTs) in [32] has many similarities to VQ-VAEs and\nencoder-decoder based synthesis. While the global style tokens\nare initialised as random vectors (like in, e.g., [27]), only a\nlimited, ﬁxed number of style tokens is used, reminiscent of\na vector-quantiser codebook. Unlike VQ-VAEs, however, the\nstyle-token approach uses attention to obtain a set of positive\ninterpolation weights between the different tokens. This means\nthat utterances in practice can fall on a continuum in token\nspace, similar to the heuristic approaches in Sec. II-B. Another\ndifference is that the encoders in [31], [32], [34] do not have\naccess to the text features, in contrast to the heuristic and VQ-\nVAE approaches studied in this paper, which make use of both\nacoustic and text-derived features in encoding.\nE. Emotional Speech Synthesis\nThe experiments in this paper consider speech synthesis\nfrom a large corpus of acted emotional speech, described\nin [63]. The importance of emotional expression in speech\nPREPRINT. WORK IN PROGRESS.\n5\nsynthesis can be seen in, e.g., the 2016 Blizzard Challenge\n[26], where suitably accounting for the expressive nature of\nthe data was a common element of the most successful entries.\nThere have been successful demonstrations of emotional\nspeech synthesis with speech generation based on unit selec-\ntion (including hybrid speech synthesis) [64]–[66] as well as\nthrough SPSS with decision trees [67]–[71]. Most of these\nconsider a relatively limited number of discrete emotional\nclasses, from binary (e.g., neutral vs. affective as in [66]) to the\n“big six” (anger, disgust, fear, happiness, sadness and surprise,\nas considered in [64], [65], [70]); [68], which investigates\ncontinuous emotional-intensity control with MR-HMMs, is an\nexception. Applications of methods based on neural-networks\nto emotional speech synthesis are less common, though there\nare a few examples [14], [63] from the last year. This article\nbuilds on these two publications and considers the same data\nin the experiments.\nIII. MATHEMATICAL BACKGROUND\nThis section introduces the mathematical preliminaries of\nspeech synthesis as necessary for the novel insights described\nin Sec. IV. In particular, Sec. III-A outlines controllable speech\nsynthesis through latent variables, while remaining sections\ndescribe the fundamental theory of variational inference (Sec.\nIII-B) and variational autoencoders in general (Sec. III-C).\nA. Controlling Speech Synthesis Through Latent Variables\nMathematically, statistical parametric speech synthesis is\nusually formulated as a regression problem. The central statist-\nical modelling task is to map an input sequence l of text-based\n(“linguistic”) features to a sequence x of acoustic features\n(“speech parameters”) that control a waveform generator (vo-\ncoder).1 Since human speech is stochastic even for a given text\nand control input (cf. [2]), we typically want to map the input\nl to an entire distribution X(l) of acoustic feature sequences\nx. This mapping is learned from a parallel corpus of text and\nspeech using statistical methods. The linguistic features l in the\nmapping are extracted deterministically from input text by a\n(typically language-dependent) so-called front-end. While the\nfront-end traditionally has been designed rather than learned,\nthis is starting to change, with a number of frameworks [12],\n[30], [72], [73] learning to predict acoustics directly from\nsequences of characters or phones. Similarly, the waveform\ngenerator is traditionally a ﬁxed, designed component, for\nexample STRAIGHT [74] or WORLD [75], to whose control\ninterface the acoustic feature representation is tied. However,\nlearned (neural) vocoders have recently achieved impressive\nresults, e.g., [76]. Thus, while it is possible to learn both the\nfront-ends and vocoders, only the central linguistic-to-acoustic\nmapping is consistently learned from speech data.2\n1In this text, bold symbols signify vectors or matrices; the underline\ndenotes a time sequence l = (l1, . . . , lT ). Capital letters identify random\nvariables, while corresponding lowercase quantities represent speciﬁc, non-\nrandom outcomes of those variables.\n2For all the interest in waveform-level speech synthesis, it is worth noting\nthat [76] – the current state of the art in text-to-speech signal quality – still\nsolves a statistical parametric speech synthesis problem. The difference in\nspeech quality comes from matched training of a learned vocoder instead of\nsynthesising waveforms with the Grifﬁn-Lim algorithm as in [30].\nLet D =\nn\nl(n), x(n)oN\nn=1 be a dataset of N aligned\nlinguistic (input) and acoustic (output) data sequences, which\nare assumed to be independent and identically distributed\ndraws from a joint distribution of L and X. Let further\nfX|L (x | l; θ) be a parametric model describing the prob-\nability of output X given L. To estimate the unknown\nmodel parameters θ it is standard to use maximum-likelihood\nestimation\nbθML(D) = argmax\nθ\nL (θ | D)\n(1)\nL (θ | D) =\nN\nX\nn=1\nln fX|L\n\u0010\nx(n) \f\f\f l(n); θ\n\u0011\n.\n(2)\nTo achieve control over how the text message encoded\nby l is spoken, we add a second input representing control\nparameters, z. While one could envision using a sequence\nz ∈RD of control inputs that may change throughout an\nutterance, we only develop the mathematics for the case when\nthis input is constant for each data sequence, and thus can\nbe represented by a single vector z. If this control signal\nhas been annotated as z(n) for each training data sequence\nit is straightforward to train a controllable synthesiser by\nmaximising the conditional likelihood\nL (θ | D) =\nN\nX\nn=1\nln fX|L, Z\n\u0010\nx(n) \f\f\f l(n), z(n); θ\n\u0011\n.\n(3)\nChanging the control signal will then cause the output distribu-\ntion to be more similar to the examples with similar annotated\ncontrol-input values, assuming learning was successful.\nThe situation becomes more interesting if the control para-\nmeter is a latent (unobserved) variable. A general and prin-\ncipled approach is to treat the unknown control input as a\nrandom variable Z which is jointly distributed with X as in\nfX, Z|L (x, z | l; θ) = fX|L, Z (x | l, z; θ) fZ|L (z | l; θ) ,\n(4)\nwhere fZ|L is a conditional prior for Z. To perform\nmaximum-likelihood parameter estimation in the presence of\nthis latent variation one marginalises out the unknown random\nvariable, and thus maximises\nL (θ | D) =\nN\nX\nn=1\nln\nˆ\nfX, Z|L\n\u0010\nx(n), z\n\f\f\f l(n); θ\n\u0011\ndz;\n(5)\nthis is termed the marginal likelihood or the model evidence,\nbut is merely another way of writing fX|L from Eq. (2).\nTo generate speech from a latent-variable model like this,\nthere are two conceivable X-distributions to consider. One\ncould use the same marginalisation principle as in Eq. (5) and\ngenerate speech based on fX|L (i.e., after integrating out Z).\nHowever, the integral is frequently intractable, as discussed in\nthe next paragraph. Moreover, this does not allow control of\nthe output speech x. For these reasons we exclusively consider\noutput generation from the X-distribution conditioned on Z,\nfX|L, Z. By adjusting the input z-value, the same text may\nthen be spoken in (statistically) distinct ways.\n6\nPREPRINT. WORK IN PROGRESS.\nB. Variational Inference\nUnfortunately, the integral in Eq. (5) is only tractable to\nevaluate for quite basic models, which tend to be too simplistic\nto allow an acceptable description of reality. To ﬁt more\nadvanced statistical models, approximations must be made.\nSome approximation techniques rely on numerical methods\nfor estimating the value of the integral, e.g., through Monte-\nCarlo sampling. In this paper, however, we consider analytical\napproximations based on variational principles, where a para-\nmetric and tractable approximation q(z; ϕ) is used in place of\nthe intractable true posterior fZ|X, L. Instead of maximising\nthe likelihood L directly, one then maximises a lower bound\nL on it, sometimes called the evidence lower bound (ELBO).\nSpeciﬁcally, one can show [35, Sec. 10.1] that\nln fX|L (x | l; θ) = DKL\n\u0010\nq\n\f\f\f\n\f\f\f fZ|X, L\n\u0011\n+ L (θ, ϕ | x, l) ,\n(6)\nwhere\nDKL\n\u0010\nq\n\f\f\f\n\f\f\f fZ|X, L\n\u0011\n=\nˆ\nq(z; ϕ) ln\nq(z; ϕ)\nfZ|X, L (z | x, l; θ) dz\n(7)\nis the Kullback-Leibler divergence (or KLD) and\nL (θ, ϕ | x, l) =\nˆ\nq (z; ϕ) ln fX, Z|L (x, z | l; θ)\nq (z; ϕ)\ndz (8)\nis the evidence lower bound. Since the KLD between two\ndistributions satisﬁes DKL (p || q) ≥0, with equality if and\nonly if p = q, the desired bound L ≥L follows. This bound\ncan be applied to every term in Eq. (2) with a separate q-\ndistribution q(z; ϕ(n)) for each datapoint to lower-bound the\nentire training-data likelihood.\nIf q is chosen cleverly, the integral in Eq. (8) can sometimes\nbe evaluated analytically. One can then identify a parameter\nestimate bθVI and a set of per-datum q-distribution parameters\nϕ⋆(n) (producing the variational posteriors q⋆(n)) that jointly\nmaximise L. This framework provides the basis for optimising\nand using powerful statistical models through the use of\nan approximate latent posterior. The difference between the\noptimal lower bound L and the optimal (log-)likelihood L of\nthe model without the variational approximation is given by\nDKL (p || q⋆) and is referred to as the approximation gap [77].\nC. Variational Autoencoders\nThe main idea of variational autoencoders [38], [39] is\nto use neural networks to parameterise not only the output-\ndistribution dependence on latent-variable values, but also\nthe act of latent-variable inference, and then learn these\ntwo networks simultaneously. Like in variational inference in\ngeneral, we approximate the true latent posterior fZ|X, L by\na variational posterior q, but instead of optimising the set\nn\nϕ(n)o\nto identify a different posterior distribution q⋆(n) for\neach datapoint, these multiple optimisations are replaced by\na single function qZ|X, L (z | x, l; ϕ) (here a neural network)\nthat simply maps the values of x and l to (parameters of) an\nReference\noutput x\nInput\nfeatures l\nLatent distribution\nqZ|X, L (z | x, l; ϕ)\nEncoder DNN\n(weights ϕ)\nDecoder DNN\n(weights θ)\nExpected log-likelihood\nEZ∼qZ|X, L\n\u0002\nln fX|Z, L (x | Z, l; θ)\n\u0003\nFigure 1. Conditional variational autoencoder training.\napproximate posterior q.3 This function qZ|X, L, parameterised\nby the network weights ϕ, is sometimes called the inference\nnetwork, the recognition network, or the encoder and is distinct\nfrom the previously-introduced conditional output distribution\nfX|L, Z (sometimes called the decoder) that is parameterised\nby θ.\nGiven the parameterised inference qZ|X, L deﬁned above,\none can show [38], [40] that\nln fX (x; θ) −DKL\n\u0010\nqZ|X\n\f\f\f\n\f\f\f fZ|X\n\u0011\n= EZ∼qZ|X\nh\nln fX|Z (x | Z; θ)\ni\n−DKL\n\u0010\nqZ|X\n\f\f\f\n\f\f\f fZ\n\u0011\n,\n(9)\nwhere we for succinctness have suppressed the dependence\non l. (Strictly speaking, our main consideration is conditional\nVAEs, or C-VAEs, where every distribution additionally is\nconditioned on an input such as l, but this difference is not\nof importance to the exposition.) The right-hand side in the\nequation is a lower bound on the likelihood (since the KLD\non the left-hand side cannot be negative) which, it turns out,\ncan be optimised efﬁciently using stochastic gradient ascent\nfor certain choices of prior fZ|L and approximate posterior\nq. A common choice [38] is to take both distributions to\nbe Gaussian; in this article we will additionally assume that\nthe conditional output distribution fX|L, Z is an isotropic\nGaussian.\nThe act of replacing individual optimisations by the regres-\nsion problem of ﬁnding the weights ϕ in VAEs is sometimes\ncalled amortised inference, since it amortises the computa-\ntional cost of the separate optimisations (inferring q(n)) over\nthe entire training. (See [77], [78] for in-depth explanations.)\nSince the posterior parameters predicted by the learned q-\nfunction may not be optimal for each datapoint, VAEs will\nin practise usually not reach the same performance as the\ntheoretically optimal L attained using q⋆(n). The difference\nbetween the ELBO value attained by the VAE and the maximal\nELBO possible under the chosen family of approximate pos-\nteriors q is known as the amortisation gap [77], and is added\nto the approximation gap due to the use of the approximate\nvariational posterior deﬁned in Sec. III-B.\nThe “autoencoder” part of “variational autoendcoders”\ncomes from the observation that qZ|X, L (z | x, l; ϕ) essen-\ntially encodes x into a latent variable z, such that the original\nx is maximally likely to be recovered from (samples from)\nqZ|X, L, as seen in the expectation in Eq. (9). This is illustrated\nconceptually in Fig. 1. Also note that the two terms on the\nright-hand side of Eq. (9) pull in different directions during\nmaximisation: the ﬁrst term is trying to make the approximate\n3Please note that ϕ now denotes a set of neural network weights that deﬁne\na mapping from x and l to distribution parameters, rather than distribution\nparameters themselves as in Sec. III-B.\nPREPRINT. WORK IN PROGRESS.\n7\nposterior qZ|X, L resemble the true posterior as much as pos-\nsible, while the second instead prioritises q not straying too far\nfrom the given prior distribution. If our model class fX|L, Z is\nsufﬁciently powerful to describe the observations well without\ndepending on z as an input, the learned latent variables are\nlikely to stay close to the prior and exert minimal inﬂuence\non the observation distribution [44]. This is a common failure\nmode of VAEs, and is especially undesirable when learning\noutput control.\nTo reduce the risk of not learning a useful latent-variable\nrepresentation (“posterior collapse”), one can introduce a\nweight between the two terms in Eq. (9), yielding so-called\nβ-VAEs [79], which can also be annealed [80]. This is\nstraightforward to implement, but is not easy to motivate on\nprobabilistic grounds and can not generally be interpreted as\na lower bound on the marginal likelihood [81]. Alternatively,\none might reduce the capacity/ﬂexibility of the decoder model\nfX|z, l, for instance by modelling speech parameters with a\nsimple Gaussian distribution as in the experiments in Sec. V.\nVQ-VAEs were conceived as a third option for easily learning\nmeaningful and informative latent representations.\nIV. THEORETICAL INSIGHTS\nThis section presents and discusses the main theoretical\ndevelopments of this paper. In particular, Sec. IV-A describes\na new probabilistic understanding of VQ-VAEs, Sec. IV-B\nlikewise introduces a variational derivation of the heuristic\nmethods from [7]–[12] and connects these to other autoen-\ncoder models, while Sec. IV-C discusses how prior information\nmight be incorporated into the heuristic models. To the best\nof our knowledge, all of these contributions are new.\nA. A Variational Interpretation of VQ-VAEs\nVQ-VAEs were introduced in [13] as a method of training\nVAEs when Z is a discrete random variable from a codebook\nZ = (z1, . . . , zM), a ﬁnite set of vectors in RD. This\nreplaces the integrals in divergences and expectations with\nsums. Moreover, the latent prior fZ is taken to be uniform\nover Z while the variational posterior q for Z is taken to be\na point estimate zq ∈Z. The VQ-VAE encoder is realised\nas a function ze (x; ϕ) taking values on all of RD, which\nsubsequently is vector quantised using the nearest codebook\nvector to obtain zq. After adding squared-error regularisation\nterms to the ELBO to promote codebook vectors and encoded\nvalues being close together, the full VQ-VAE objective func-\ntion for a single datapoint becomes4\nLVQ (θ, ϕ, Z | x) = ln fX|Z\n\u0000x\n\f\f zq (x) ; θ\n\u0001\n−\n\r\r\rsg\n\u0000ze (x)\n\u0001\n−zq\n\r\r\r\n2\n2 −β\n\r\r\rze (x) −sg\n\u0000zq\n\u0001\r\r\r\n2\n2 .\n(10)\nHere sg(·) is the stop-gradient operator implemented in many\ndeep learning frameworks, which essentially means that the\nargument is to be treated as a constant during differentiation.\n(For simplicity, we ignore the conditioning on l in our treat-\nment of VQ-VAEs.) The straight-through estimator described\n4This formula corrects a sign inconsistency present in Eq. (3) of [13].\nin [82] is used to backpropagate the gradient through the (non-\ndifferentiable) quantisation that turns ze (x) into zq (x) in the\nlikelihood term. Since this estimator ignores the effect of the\nVQ codebook, the gradient used to update Z only depends on\nthe second term in the objective function in Eq. (10) [13].\nAs originally introduced in [13], the regularisation terms\nin Eq. (10) (e.g., the “commitment loss”) are motivated\non geometric, not probabilistic grounds. Together with the\nquantisation and the stop-gradient operators, this makes it\ndifﬁcult to assign a probabilistic interpretation to the VQ-\nVAE objective function. However, we will now show that it is\npossible to interpret the objective function as an actual ELBO\nmaximisation.\nProposition 1: For β = 1, optimising the VQ-VAE ob-\njective in Eq. (10) is equivalent to optimising the combined\nobjective\nLVQ1 (θ, ϕ, Z | x)\n= ln fX|Z\n\u0000x\n\f\f zq (x) ; θ\n\u0001\n−\n\r\rze (x; ϕ) −zq\n\r\r2\n2 ,\n(11)\nwhich lacks the stop-gradient operators.\nThis proposition is easily veriﬁed by computing and com-\nparing the partial derivatives of LVQ and LVQ1 with respect\nto θ, ϕ, and Z. In practice, the results of learning are said\n[13] not to depend substantially on the numerical value of the\nhyperparameter β. Our analysis will henceforth assume β = 1,\nalthough β = 0.25 is used for the experiments, following [13].\nNext we will show how Eq. (11) can be derived in a\nprincipled manner from a probabilistic model that includes\na statistical model of the effect of quantisation in the latent\nspace. We are not aware of any prior publications that derive\nVQ-VAEs from probabilistic principles alone.\nTo begin with, we model the distribution of encoder out-\nputs in the latent space through a Gaussian mixture model\n(GMM). More concretely, we separate encoding and quantisa-\ntion through a two-part latent variable Z = (Ze, Zq), where\nze ∈RD represents the encoder output and zq ∈Z ⊂RD is\nthe quantised version thereof. Assume that X is conditionally\nindependent of Ze given the codebook vector Zq. (This is the\nreverse of more conventional uses of mixture models in VAEs\n[83], [84], where the observation X is instead assumed to be\nconditionally independent of the mixture component identity\nZq given the mixture model sample Ze.) The joint model then\nfactorises as\nfX, Ze, Zq\n\u0000x, ze, zq; θ\n\u0001\n= fX|Zq\n\u0000x\n\f\f zq; θ\n\u0001\nfZe|Zq\n\u0000ze\n\f\f zq\n\u0001\nfZq\n\u0000zq\n\u0001\n.\n(12)\nWe further assume that the latent prior fZq over codebook\nvectors is uniform and that fZe|Zq is an isotropic Gaussian\ncentred on Zq with ﬁxed covariance matrix σ2I. Ze here\nprovides an explicit representation of the noise introduced\nby the vector quantiser. Analogous to a regular VAE, the\nremaining parameters ϕ and (here) Z deﬁne the variational\nposterior qZ. In particular, we choose a posterior of the form\nqZe, Zq|X\n\u0000ze, zq\n\f\f x; ϕ, e\n\u0001\n= qZe|Zq, X\n\u0000ze\n\f\f zq, x; ϕ\n\u0001\nqZq|X\n\u0000zq\n\f\f x; e\n\u0001\n(13)\n= f\n\u0000ze −z (x; ϕ)\n\u0001\nI\n\u0000zq = e\n\u0001\n,\n(14)\n8\nPREPRINT. WORK IN PROGRESS.\nHere, e ∈Z (to enforce quantisation), I(·) is the indicator\ndistribution (which equals one if the argument is true and\nzero otherwise), while f(·) is any ﬁxed, unimodal distribution\ncentred on the origin. To reduce confusion with the latent\noutcome ze, we have abbreviated the encoder output ze (x; ϕ)\nas z (x; ϕ). When f(·) shrinks to a point mass, meaning that\nwe ignore the uncertainty in the latent posterior, we call this\nmodel a GMM-quantised VAE, or GMMQ-VAE.\nProposition 2: Under the assumptions made in [13],\nELBO maximisation over the extended parameter set ψ =\n{θ, ϕ, Z, e ∈Z} for the GMMQ-VAE has the same form as\nparameter estimation with the VQ-VAE objective in Eq. (11).\nProof sketch: From Eq. (8), the GMMQ-VAE ELBO is\nLGMMQ (ψ | x) = −h (qZ)\n+\nX\nzq\nˆ\nqZ (z; ϕ, e) ln fX, Z (x, z; θ) dze,\n(15)\nwhere h(·) denotes the differential entropy. Since the entropy\nof qZ is independent of ψ it has no effect on ELBO max-\nimisation and can be ignored. If we then let f(·) approach a\nDirac delta function δ(·) – thus ignoring any uncertainty in the\nvariational posterior by shrinking it to a point mass – the sum\nand integral both reduce to simple evaluation, and we obtain\nbψ = argmax\nψ\nlim\nf→δ LGMMQ (ψ | x)\n(16)\n= argmax\nψ\nln fX, Ze, Zq\n\u0000x, z (x; ϕ) , e; θ\n\u0001\n(17)\n= argmax\nψ\n\u0010\nln fX|Zq (x | e; θ) + ln fZe|Zq\n\u0000z (x; ϕ)\n\f\f e\n\u0001\u0011\n,\n(18)\nusing Eq. (12) with fZq uniform. For the optimisation over\ne ∈Z in ψ, fZe|Zq is unimodal isotropic, and thus maximised\nby the e closest to z (x; ϕ). Also, for good autoencoders (i.e.,\nnear the global optimum of ψ \\e) we expect fX|Zq (x | e; θ)\nto be greatest for the e ∈Z closest to z (x; ϕ). This is es-\nsentially a less restrictive version of the VQ-VAE assumption\nfX|Zq (x | z; θ) ≈0 whenever z ̸= zq [13]. The optimisation\nover e can then solved explicitly, with the optimum being\ne⋆= zq (x; ϕ, Z)\n(19)\n= argmin\ne∈Z\n\r\rz (x; ϕ) −e\n\r\r2\n2 ,\n(20)\nthe codebook vector closest to the encoder output z (x; ϕ),\nas expected for a vector quantiser. Since fZe|Zq is Gaussian\nwith covariance matrix σ2I, its log-probability reduces to\nthe squared distance between the quantised and unquantised\nencoder output, plus a constant. We then arrive at\nn\nbθ, bϕ, b\nZ\no\n= argmax\nθ, ϕ, Z\n\u0012\nln fX|Zq\n\u0000x\n\f\f zq (x; ϕ, Z) ; θ\n\u0001\n−\n1\n2σ2\n\r\rz (x; ϕ) −zq (x; ϕ, Z)\n\r\r2\n2\n\u0013\n.\n(21)\nThis expression is of the same form as Eq. (11), as desired.\nThe variance σ2 of the isotropic Gaussian acts as a weight\nbetween the two terms in the objective function, very similar\nto the hyperparameter β in regular VQ-VAEs.\nProposition 2 shows that the entire VQ-VAE objective func-\ntion for β = 1 can be assigned a probabilistic interpretation\nas a regular VAE with a Gaussian mixture distribution in the\nlatent space, speciﬁcally a GMMQ-VAE. The key twist is\nthat X depends on the discrete GMM component zq instead\nof the continuous-valued, GMM-distributed encoder output\nze like in [83], [84]. This introduces quantisation into the\nencoder, distinguishing VQ-VAEs from the alternative of a\nsimple, unquantised VAE with a GMM prior on Z. We see that\ndifferent weights on the squared-error term (which is closely\nrelated to changing β in Eq. (10)) correspond to different\nassumptions about the magnitude of the quantisation error.\nOur derivation of Proposition 2 suggests a number of natural\ngeneralisations of GMMQ/VQ-VAEs, for example by adjust-\ning and potentially learning any combination of the component\nprior probabilities fZq and the component covariance matrices\nΣq. These extensions are however beyond the scope of the\ncurrent article, and will not be explored further here. Since\nthe GMMQ-VAEs and VQ-VAEs are so closely related, we\nwill henceforth concentrate on VQ-VAEs for simplicity.\nB. A Variational Interpretation of Heuristic Control Learning\nIn this section, we show how discriminant condition codes\n[7]–[9], [11], [12] and sentence-level control vectors [10],\nwhich we collectively will refer to as the heuristic approaches\nor poor man’s latent variables, can be connected to variational\ninference, autoencoders, and VQ-VAEs. We begin by noting\nthat the heuristic approaches are merely different names for\nthe same model-ﬁtting framework, where the likelihood max-\nimisation in Eq. (2) is replaced by a joint log-probability\noptimisation over both model parameters θ and the per-\nsequence latent variables\nn\nz(n)o\n. The resulting estimation\nproblem over the entire training data D can be written\nn\nbθDCC(D), bz(n)\nDCC(D)\no\n= argmax\n{θ,z(n)}\nN\nX\nn=1\nln fX|Z, L\n\u0010\nx(n) \f\f\f z(n), l(n); θ\n\u0011\n.\n(22)\nProposition 3: The heuristic methods based on joint op-\ntimisation of latent inputs and model parameters equivalently\nbe formulated encoder-decoder models, where the encoder for\nany θ can be written\nbz(n)\nDCC(D, θ) = argmax\nz\nln fX|Z, L\n\u0010\nx(n) \f\f\f z, l(n); θ\n\u0011\n.\n(23)\nProof sketch: Consider\nbθDCC(D)\n= argmax\nθ\nmax\n{z(n)}\nN\nX\nn=1\nln fX|Z, L\n\u0010\nx(n) \f\f\f z(n), l(n); θ\n\u0011\n(24)\n= argmax\nθ\nN\nX\nn=1\nmax\nz(n) ln fX|Z, L\n\u0010\nx(n) \f\f\f z(n), l(n); θ\n\u0011\n(25)\n= argmax\nθ\nN\nX\nn=1\nln fX|Z, L\n\u0010\nx(n) \f\f\f bz(n)\nDCC(D, θ), l(n); θ\n\u0011\n(26)\nPREPRINT. WORK IN PROGRESS.\n9\nwhere the last line follows from the observation that\nmax\nz\ng(x, z) = g(x, argmax\nz\ng(x, z)),\n(27)\nfor any function g(·, ·).\nFrom Proposition 3 we observe that the common heuristics\nfor learning controllable speech synthesis from unannotated\ndata can be seen as encoder-decoder models, where the en-\ncoder uses the same network as the decoder. This observation\nmotivates our interest in comparing these heuristics to other\nencoder-decoder approaches. (The situation is however dif-\nferent from traditional autoencoders with tied weights, where\nthe weight matrices in the decoder are transposes of those\nin the decoder.) Unlike VAEs, where encoding is performed\nvia forward propagation through a second network, encoding\nhere involves solving an optimisation problem through back-\npropagation. This is likely to be slow, but may give better\nperformance (especially on test data) since each encoded vari-\nable solves an independent posterior-probability optimisation\nproblem; there’s no amortisation gap, unlike for VAEs [78]. In\nboth VAEs and in the heuristic framework the encoder requires\nx as well as l as input, and thus cannot easily be applied in\nsituations where natural speech acoustics are unavailable.\nDifferent from the style-token encoder in [31], [32] and the\nspeaker encoder in [34], the encoder here has access to the\ntext-derived features of the spoken utterance. This is likely\nto promote encoder output that is more complementary to the\ntext (reduced redundancy), but may or may not be more trans-\nferable between different text prompts. Interestingly, while\nrecent Tacotron and VoiceLoop publications [31], [32], [85]\nhave added explicit and distinct encoding networks similar to\n(VQ-)VAEs, previous work [12], [33] by these groups used\nbackpropagation through the decoder as an implicit encoder,\nin the same way as the heuristic methods considered here.\nProposition 4: Increasing the heuristic objective function\nin Eq. (22) increases the evidence lower bound in Eq. (8).\nThe encoder output can be seen as an approximate maximum\na-posteriori estimate of the latent variable Z given X = x\nand L = l.\nProof sketch: Note that the ELBO in Eq. (8) can be written\nL (θ, ϕ | x, l) =\nˆ\nq (z; ϕ) ln fX, Z|L (x, z | l; θ)\nq (z; ϕ)\ndz\n=\nˆ\nq (z; ϕ) ln fX, Z|L (x, z | l; θ) dz −h(q),\n(28)\nwhere h(q) is the differential entropy of q (z; ϕ). Consider\nchoosing the q-distribution from a family which is paramet-\nerised by location µ only, meaning that ϕ = µ and\nq (z; µ) →q (z −µ) .\n(29)\nThis makes h(q (z; µ)) independent of µ, and we get\nbµ (x, l, θ)\n= argmax\nµ\nL (θ, µ | x, l)\n(30)\n= argmax\nµ\nˆ\nq (z; µ) ln fX, Z|L (x, z | l; θ) dz.\n(31)\nIf the shape of the q-distribution(s) is made increasingly\nnarrow (by making the variance tend to zero) so that it\napproaches a Dirac delta function δ(·) we obtain\nlim\nq→δ bµ (x, l, θ)\n= argmax\nµ\nˆ\nδ (z −µ) ln fX, Z|L (x, z | l; θ) dz\n(32)\n= argmax\nµ\nln fX, Z|L (x, µ | l; θ)\n(33)\n= argmax\nµ\nln\n\u0010\nfX|Z, L (x | µ, l; θ) · fZ|L (µ | l; θ)\n\u0011\n(34)\n= argmax\nµ\nln fX|Z, L (x | µ, l; θ) ,\n(35)\nwhere the last line assumes that fZ|L is constant. By applying\nthese approximations to each training datapoint independently\none obtains Eq. (22).\nIn summary, we have shown that the heuristic objective in\nEq. (22) can be derived from variational principles assuming:\n1) That the prior distribution fZ|L is ﬂat (constant) across\nthe range of z- and l-values considered.\n2) We use a Dirac delta function (a spike) to represent all\nlatent posterior distributions.\nBoth assumptions are directly analogous to assumptions made\nin the probabilistic derivation of VQ-VAEs in Proposition 2:\nVQ-VAEs use a uniform prior over codebook vectors and do\nnot represent any uncertainty in the (encoded) latents. This is\nanother motivation for us to compare the heuristic approach\nto the largely similar functionality offered by VQ-VAEs. The\nsecond assumption explains the nickname “poor man’s latent\nvariables”, since we see that the heuristic objective does not\nafford any representation of uncertainty in the latent space.\nIf the listed assumptions are violated, the variational approx-\nimation need not produce a maximum of the true likelihood,\nthough the agreement between the two methods is likely to\nbe greater the more accurate the two assumptions are. Unlike\nthe EM-based derivation in [14], the derivation presented\nhere establishes that any simultaneous modiﬁcation of that\nincreases Eq. (22) also increases the likelihood lower bound;\nit is not necessary to perform interleaved optimisation as in\nthe EM-algorithm [37].\nWhile L diverges to minus inﬁnity as q →δ, and thus\ndoes not provide a reasonable numeric lower bound on the\nlikelihood, it is still true that relative differences is L-are\nmeaningful and can be mapped to similar changes in the\nlower bound (consider subtracting one ELBO from another).\nA similar observation applies to the numerical value of the\nVQ-VAE objective derived in Proposition 2.\nThe domain of the optimisation over z(n) in Eq. (22)\ncan also be given a statistical interpretation. Deﬁne a binary\nprior fZ|L, which is constant and nonzero on feasible z-\nvalues, but equals zero (so that ln fZ|L = −∞) outside the\ndomain of optimisation. Unconstrained ELBO maximisation\nwith this prior will then only ﬁnd possible optimal parameters\nin the feasible set deﬁned by the constraints. Constrained\noptimisation in the latent space is thus interpretable as normal\nvariational parameter estimation under a particular prior on Z.\nTo summarise, the key similarities between VQ-VAEs and\nthe heuristic approach are:\n10\nPREPRINT. WORK IN PROGRESS.\n• Both VQ-VAEs and the heuristic approach can be viewed\nas autoencoders.\n• Both methods are closely related to variational ap-\nproaches with a ﬂat prior over the permissible z-values.\n• Neither approach represents uncertainty in the latent-\nvariable inference (the encoder output value).\nThe main differences, meanwhile, are:\n• The heuristic approach does not quantise latent vectors.\n• The heuristic approach uses a single network for both\nencoding and decoding, with an optimisation operation\ninstead of forward propagation through a separate en-\ncoder. In other words, it does not amortise inference.\nC. Using Prior Information in Control Learning\nIt is worth noting that the variational interpretation of the\nheuristic method requires that a ﬂat, noninformative prior is\nused. In Bayesian statistics, priors like fZ|L can be adjusted\nby practitioners based on side information about what z-value\nto expect for any given datapoint. With a ﬁxed prior, this\nopportunity goes away.\nThere are, however, other methods for potentially biasing\nlearning based on side information. In particular, since speech\nsynthesisers are trained by local reﬁnements of a previous\nparameter estimate and the parameter set includes explicit\nestimates of the latent encodings, the system can be initialised\nbased on an informed guess about appropriate latent-variable\nvalues. We compare this strategy against random initalisation\nin the experiments in Sec. V-D. A ﬁnding that these two\nschemes do not differ in behaviour would indicate that learning\nis robust to initialisation. The opposite ﬁnding would suggest\na more brittle learning process, but also one with room to\nstraightforwardly inject prior information into the learning.\nV. EXPERIMENTS\nFollowing the theoretical developments in the previous sec-\ntion, we now investigate the practical performance of different\nmethods for unsupervised learning of control in an example\napplication to acoustic modelling of emotional speech, using\na corpus described in Sec. V-A. The systems and baselines\nconsidered are introduced in Sec. V-B, and their training\npresented in Sec. V-C. The results of training and the associ-\nated learned latent representations are evaluated objectively\nin Sec. V-D. Sec. V-E then details the subjective listening\ntest performed, along with its analysis and resulting ﬁndings.\nWherever possible, the experiments have been designed to\nbe as similar as possible to the experiments with supervised\nspeech-synthesis control in [63], which used the same data.\nA. Data and Preprocessing\nFor the experiments in this paper, we decided to use the\nlarge database of studio-recorded, high-quality acted emotional\nspeech from [63]. (An earlier subset of this database was used\nfor the research in [14].) The database contains recordings\nof isolated utterances in Japanese, read aloud by a female\nvoice talent who is a native speaker of Japanese. Each prompt\ntext was chosen to not harbour any inherent emotion, but was\nspoken in one or more of seven different emotional styles:\nemotionally-neutral speech as well as the three pairs happy\nvs. sad, calm vs. insecure, and excited vs. angry. This means\nthat the database contains speech variation of communicative\nimportance that cannot be predicted from the text alone. 1200\nutterances (133–158 min) were recorded for each emotion,\nfor a total of 8400 utterances and nearly 17 hours of audio\n(beginning and ending silences included), all recorded at 48\nkHz. The talker was instructed to keep their expression of each\nemotion constant throughout the recordings.\nEach audio recording in the data is annotated with the\ntext prompt (in kanji and kana) as well as the prompted\nemotion. Lorenzo-Trueba et al. [63] considered a number of\ndifferent methods for encoding this emotional information for\nspeech synthesiser control, while also leveraging information\non listener perception of the different emotions. They found\nthe best-performing encoding of emotional categories to be\nbased on listener responses to emotional speech (confusion-\nmatrix columns) rather than one-hot categorical vectors. Re-\nlabelling the data based on listener perception of individual\nutterances did not improve performance. In contrast to this\nprevious work, we will treat the emotional content as a latent\nsource of variation, to be discovered and described by the\ndifferent unsupervised methods we are investigating.\nTo simplify comparison, we used the same partitioning, pre-\nprocessing, and forced alignment of the database as Lorentzo-\nTrueba et al. [63]. In particular 10% of the data were used for\nvalidation and 10% for testing, with these held-out sets only\nincorporating sentences where annotators’ perceived emotional\ncategories agreed with the prompted emotion. We also used\nthe exact same linguistic and acoustic features as those ex-\ntracted in [63]. In particular, Open JTalk [86] was used to\nextract 389 linguistic features while WORLD [75], [87] was\nused for acoustic analysis and signal synthesis. The analysis\nproduced a total of 259 acoustic features at 5 ms intervals. The\nfeatures comprised linearly interpolated log pitch estimated\nusing SWIPE [88], 60 mel-cepstrum features (MCEPs, with\nfrequency warping 0.77 to approximate the Bark scale), and 25\nband-aperiodicity coefﬁcients (BAPs) based on critical bands.\nEach of these had static, delta, and delta-delta coefﬁcients.\nThese continuous-valued features were all normalised to zero\nmean and unit variance, and subsequently complemented with\na binary voiced/unvoiced ﬂag.\nLinguistic and acoustic features were forced-aligned with\nﬁve-state left-to-right no-skip HMMs trained with HTS [89],\ngiven access to the prompted emotion as an additional\ndecision-tree feature. These HMMs were also used for duration\nprediction during synthesis, which was identical for all models;\nonly different approaches to acoustic modelling (trained with\nor without emotional labels) were compared in the experi-\nments. At synthesis time, predicted static and dynamic features\nwere reconciled through most likely parameter generation\n(MLPG) [90] and enhanced using the postﬁlter described in\n[91] with coefﬁcient 0.2.\nB. Systems\nTo investigate how supervised and unsupervised approaches\nfor learning acoustic-model control behave on data with im-\nPREPRINT. WORK IN PROGRESS.\n11\nportant non-textual variation (speciﬁcally emotion), we con-\nsidered eight different sources of speech stimuli, or systems,\nof three different kinds: stimuli based on natural speech\n(functioning as toplines), systems with only supervised learn-\ning (functioning as baselines for comparisons), and systems\ncapable of learning output control from unannotated variation.\nIn brief, the eight systems were deﬁned as follows:\n• NAT: Natural speech from the held-out test-set.\n• VOC: Natural speech from the held-out test-set, subjec-\nted to analysis synthesis as described in Sec. V-A.\n• SUP: A supervised approach to controllable speech syn-\nthesis, trained and evaluated with labels derived from the\nground-truth prompted emotion as input. Speciﬁcally, this\nsystem is equivalent to the best setup with emotional\nstrength from [63], since the approaches based on unan-\nnotated data presumably can learn to moderate emotional\nstrength as well. The only difference from [63] is that\nthe system was optimised using Adam [92] rather than\nstochastic gradient descent.\n• BOT: A bottom-line system, same as SUP but with no\ncontrol input, only linguistic features l. This system can-\nnot accommodate the differences between the different\nemotions in the database and provides a bottom line in\nterms of prediction performance.\n• VQS: A VQ-VAE with the same (‘S’) number of hidden\nnodes and layer order in the encoder as in the decoder.\n• VQR: A VQ-VAE with the same number of hidden nodes\nand but reverse (‘R’) layer order in the encoder compared\nto the decoder.\n• HZI: Poor man’s latent variables with latent-space con-\ntrol vectors initialised with all zeros (‘ZI’).\n• HSI: Poor man’s latent variables with supervised initial-\nisation (‘SI’) of latent-space control vectors. This gives\nan idea of the impact of using prior information in\ninitialisation, as discussed in Sec. IV-C.\nAll synthesisers used the same duration model and duration\npredictions as the experiments in [63]; only the acoustic\nmodels differed. They also used exact same decoder structure,\nidentical to the one used in [14], [63], [93] (among others).\nBased on the proposal in [94], it contains two 256-unit feed-\nforward layers with logistic sigmoid nonlinearities, followed\nby two 128-unit BLSTM layers and a linear output layer. The\nneural networks were implemented in CURRENNT [95].\nBased on our observation in Prop. 3 in Sec. IV-B – that\nthe heuristic methods can be interpreted as encoder-decoder\nmodels that use the same network for both encoding and\ndecoding – we made the VQ-VAE encoders in the experiments\nhave the same internal structure (hidden layers and unit counts)\nas the decoder. There is, however, some ambiguity as for how\nto order the hidden layers in the encoder: the encoder is a\nfunction zq (x, l) while the decoder is a function x\n\u0000zq, l\n\u0001\n.\nAn argument based on zq or x suggests that the order of the\nfeedforward and recurrent layers be swapped in the encoder\ncompared to the decoder, placing the recurrent layers closer\nto the input side of the encoder (as in system VQR), while\na reference to l suggests that the layer order should not be\naltered between encoder and decoder (as in system VQS).\nLatent\nvector ze(x)\nNatural\nacoustics x\nLinguistic\nfeatures l\nQuantised\nlatent zq(x)\nPredicted acoustics bx\nMean pooling\nVQ\n(a) VQS (“same”)\nLatent\nvector ze(x)\nNatural\nacoustics x\nLinguistic\nfeatures l\nQuantised\nlatent zq(x)\nPredicted acoustics bx\nMean pooling\nVQ\n(b) VQR (“reversed”)\nFigure 2. VQ-VAE schematics showing the two different encoder structures.\nThe situation is illustrated in Fig. 2. For completeness, both\ntopologies were considered in the experiments. In either case,\nthe ﬁnal per-sentence encoding vector ze was extracted from\na mean-pooling layer across all timesteps, similar to how the\nbackpropagated gradients for the latent control vectors sum\nacross frames in the heuristic approach.\nPrior to training, all networks were initialised with small\nrandom weights based on Glorot & Bengio [96]. The\nautoencoder-based approaches in this study also require that\nthe latent representations (the per-sentence control vectors or\nthe codebook) be initialised as well. We set the control-vector\ndimensionality D to 8 throughout the experiments, the same\nvalue as in [63] (based on 7 emotions plus a scalar emotional\nstrength). The latent control vector elements for HZI and HSI\nwere then initialised deterministically (either all zeros, or with\nthe same values as for as SUP, also on the validation and test\nsets). For the VQ-VAEs the codebook size was set to 1344\nand the codebook vectors were initialised with small random\nvalues as part of neural network initialisation. The size of the\ncodebook was chosen to be the same as the maximum number\nof distinct emotional-category encodings used by SUP on the\ntraining set [63], computed as 192 35-utterance mini-batches\nwith 7 emotions in each. It is good practice to use a larger\nVQ codebook than might be necessary, since some codebook\nvectors are likely to end up in regions that the encoder does not\nvisit, yielding “dead” vectors that are neither trained or used;\nwith too few vectors, the presence of local optima means that\nnot all control modes or nuances may be learned.\nIn purely objective terms, we may expect the unsupervised\napproaches to achieve a better ﬁt to the training data than the\nsupervised method, since the former can tailor their output to\neach individual utterance in the corpus. The heuristic meth-\nods are furthermore likely to give better objective prediction\naccuracy than VQ-VAEs, due to the amortisation gap and the\nVQ-VAE restriction to a discrete set of latent-space values.\n12\nPREPRINT. WORK IN PROGRESS.\n0\n20\n40\n60\nTraining epoch\n90\n95\n100\n105\n110\nMSE per frame\nBOT\nSUP\nVQS\nVQR\nHZI\nHSI\n(a) Training set\n0\n20\n40\n60\nTraining epoch\n85\n90\n95\n100\nMSE per frame\n(b) Test set\nFigure 3. Training curves for different systems. Note the different scales on the y-axes. Plus signs indicate the best epoch on the validation set.\nTable I\nOBJECTIVE RESULTS OF SYSTEM TRAINING.\nMSE per frame\nSystem\n#NN weights\nBest epoch\nTrain\nVal.\nTest\nBOT\n1.58M\n52\n93.3\n105.1\n91.1\nSUP\n1.58M\n38\n90.5\n101.3\n88.3\nVQS\n3.24M\n38\n89.7\n100.2\n86.0\nVQR\n3.18M\n38\n90.2\n100.7\n86.6\nHZI\n1.58M\n58\n88.3\n98.9\n84.6\nHSI\n1.58M\n48\n88.8\n98.9\n84.5\nSubjectively, however, SUP will be hard to beat, since it is\ntrained using supervised knowledge to explicitly control the\nperceptually most relevant variation in the data.\nC. Training\nAll mathematical approaches considered in this work are\nprobabilistic methods that operate on the principle of likeli-\nhood maximisation. For this experiment, we assume that the\nconditional output distribution X (l, z) (or X (l) for BOT)\nis an isotropic Gaussian with ﬁxed variance. Log-likelihood\nmaximisation is then mathematically equivalent to (mean)\nsquared-error (MSE) minimisation. The MSE is a common\nloss function in synthesiser training, used for instance in\nTacotron 1 and 2 [30], [76]. In our case each extracted acoustic\nfeature is normalised to unit variance prior to neural network\ntraining (see [63]), so our setup altogether corresponds to an\nassumption that the speech-feature outputs are Gaussian, un-\ncorrelated, and that each feature-vector element has a standard\ndeviation proportional to the global standard deviation of that\nfeature on the training set; the network outputs, in turn, can\nalso be interpreted probabilistically as estimated conditional\nGaussian means. It was seen in [97] that the use of such a\nglobally-constant covariance matrix did not signiﬁcantly affect\nsynthesis quality compared to the alternative of letting the\nvariance depend on linguistic context.\nEncoder and decoder parameters (including the VQ code-\nbook) were trained to minimise per-frame MSE using Adam\n[92] with default hyperparameter values. However, since each\nper-utterance control-vector input for the heuristic systems\nHZI and HSI only is updated once per epoch, these z-vectors\nmay not be a good ﬁt for the per-parameter moment estimates\nthat Adam maintains. The control vectors were therefore\ninstead updated using stochastic gradient descent (SGD) with a\nﬁxed learning rate 2·10−4, the same rate as used for the latent\nvectors in [14].5 The HZI and HSI control-vector inputs for\nvalidation and test utterances were updated similarly using the\ncorresponding synthesis network from each epoch, but without\nmodifying the network weights on these utterances (cf. [10]).\nIn an encoder-decoder view, this maximisation performed by\nSGD on training, validation, and test data is an instantiation\nof the encoder in Eq. (23).\nTraining was run until the validation-set MSE failed to\nimprove for ten consecutive epochs (or eight in the case of\nBOT), whereafter the network with the lowest validation-set\nerror was returned. In the present experiment, this scheme\nrequired at most 68 epochs for termination.\nD. Objective Evaluation\n1) Evaluation of Training: Fig. 3 presents learning curves\nfrom the synthetic systems in Sec. V-B, chronicling the evol-\nution of per-frame mean-squared error on training and test-set\ndata for each epoch of optimisation. The number of iterations\nuntil termination and ﬁnal performance numbers on all three\ndata partitions are listed in Table I, along with the number of\nneural network weights used by CURRENT for each system.\nLooking at Table I, a handful of general trends become\nevident. To begin with, validation set numbers are consistently\ninferior to both training and test set numbers; this appears to\nbe a consequence of the data partitioning in [63], and recurs\nin other systems trained on this data split. The most notable\ndifference between the methods is that all schemes with con-\ntrol achieved better MSE performance than the emotionally-\n5Paper [14] contains a typo where 0.2·10−3 is incorrectly listed as 2·10−3.\nPREPRINT. WORK IN PROGRESS.\n13\n−60\n−40\n−20\n0\n20\n40\n60\n80\n−60\n−40\n−20\n0\n20\n40\n60\n80\nNeutral\nHappy\nCalm\nExcited\nSad\nInsecure\nAngry\n(a) SUP\n−60\n−40\n−20\n0\n20\n40\n60\n−60\n−40\n−20\n0\n20\n40\n60\n(b) HZI\nFigure 4. 2D t-SNE embeddings of latent control vectors z, coloured by the prompted emotion. Scale and rotation are arbitrary.\nunaware bottom line BOT by at least 3.0 on all data partitions.\nThis is entirely expected, since only BOT is unable to adjust its\noutput based on the emotional content of the speech. The fact\nthat methods with learned control inputs slightly outdo SUP\nis not surprising either, since they had access to the natural\nground-truth acoustics for each test-set utterance as a decoder\ninput. These numbers do not imply that the resulting systems\nachieve subjectively better quality or emotional control.\nThe heuristic systems required more epochs than most other\nsystems to terminate training, but also achieved lower per-\nframe MSE than VQS and VQR by at least 1.4 on the test\nset. This difference is likely due to the amortisation gap [77],\nsince the VQ-VAEs use learned inference while the heuristic\nsystems use direct per-utterance optimisation. The use of SGD\nrather than Adam for updating the latent-variable values of\neach utterance might explain the slower convergence rate and\nlonger training seen in Fig. 3 for the heuristic systems.\nAs a side note, an earlier version of our VQ-VAE encoder\nextracted the ﬁnal state on the LSTM (in each direction)\nand mapped these to the latent space through a linear output\nlayer; such a design is perhaps more traditional in encoder-\ndecoder models, and resembles the one used in [32]. However,\nVQ-VAEs with this encoder design did not perform much\ndifferently from BOT. It seems that relevant information from\nmid-utterance acoustics did not propagate well to the end\nstates, resulting in encoder output of little predictive value.\nWithout emotional information (from label or acoustics), the\nresulting network is then essentially a version of BOT. Once\nthe choice to extract the end state of the LSTM was replaced\nby a mean pooling operation, performance improved to the\nlevels seen in Table I.6\n6As an alternative, the work in [31] chose used the ﬁnal state of a unidirec-\ntional RNN as the encoder output, but since their encoder contained several\nstrided convolutions, the training sequences were effectively downsampled\nsuch that the RNN had to run over less than ten timesteps. Similar to our mean\npooling, this allowed the encoder to better incorporate information from the\nentire utterance, but their setup is more likely to retain some order information\nof relevance to the intonation patterns they studied.\n2) Evaluation of Learned Latent Vectors: While the low\nMSE achieved by the encoder-decoder models in Table I are\nencouraging, it does not follow that the trained systems must\nhave learned to represent and control emotion speciﬁcally.\nTo investigate this, we performed objective analyses on the\nlearned latent representations. For the heuristic systems, we\nused t-distributed stochastic neighbour embedding (t-SNE)\n[98] to reduce dimensionality and visualise the latent-space\nvectors in two dimensions. The results for HZI can be seen in\nFig. 4b, and can be compared against a similar embedding of\nthe SUP control vectors in Fig. 4a. It is clear that the different\nemotions are grouped into well-deﬁned clusters with minimal\noverlap. The degree of separation can be quantiﬁed by looking\nat how frequently the nearest neighbour of an utterance vector\nin the latent space is from a different prompted emotion.\nAcross the 1680 latent vectors in the test set, this happened\n18 times for HZI and 7 times for HSI. If we measure how\nmany times at least one of the ﬁve nearest neighbours is from\na different emotion, the numbers rise to 41 for HZI and 21 for\nHSI. (For SUP, the corresponding number is 0.) All in all, this\nindicates that the heuristic approach has been highly successful\nat identifying the different base emotions in the database and\nthen separating them in the latent space.\nWhile exhibiting faster convergence, supervised initialisa-\ntion (HSI) did not seem to confer any lasting beneﬁt over the\npurely unsupervised approach HZI initialised with all zeros.\nThis suggests that latent vectors learned through standard\nheuristics are robust against differences in initialisation.\nFor the systems based on VQ-VAE we performed a clus-\ntering analysis on the 1680 quantised latent vectors zq from\nthe test set. The results are provided in Table II. We see that\nmost vectors in the codebooks were not used at all (at most\n61 vectors out of 1344 were used), so a parsimonious discrete\nrepresentation was learned despite starting from a very large\ncodebook. Of the vectors that did see use on the test set, each\nemotion only used a subset of these (ﬁrst group of numbers\nin the table). Standard measures of clustering quality like\n14\nPREPRINT. WORK IN PROGRESS.\nTable II\nANALYSIS OF QUANTISED LATENT VECTORS IN VQ-VAE SYSTEMS.\nVQ indices used\nEmotion entropy\nTotal\nPurity\nNMI\nSystem\nmin / mean / max\nmin / mean / max\nindices\n(frac)\n(bits)\nVQS\n2 / 11.7 / 33\n0.19 / 2.03 / 3.98\n61\n0.96\n0.17\nVQR\n1 /\n5.7 / 13\n0\n/ 1.24 / 2.71\n29\n0.98\n0.10\nTable III\nMEAN OPINION SCORES FOR QUALITY AND EMOTIONAL STRENGTH.\nQuality\nEmotional strength\nSystem\nPer utt.\nPer emo.\nPer utt.\nPer emo.\nNAT\n4.01\n-\n3.38\n-\nVOC\n2.94\n-\n3.18\n-\nSUP\n3.41\n-\n2.94\n-\nVQS\n3.42\n3.51\n2.92\n2.99\nVQR\n3.41\n3.50\n2.89\n2.97\nHZI\n3.43\n3.53\n2.89\n2.99\nHSI\n3.44\n3.54\n2.86\n2.98\npurity and normalised mutual information (NMI) [99, Ch. 16]\nindicate that the prompted emotions were very well separated\nby the VQ-VAE. Beyond the emotion, there is relatively little\ninformation in the encoded latent vectors, as shown by the low\nper-emotion entropies (second set of numbers in the table).\nThis suggests that the talker’s emotional expression might\nbe quite consistent across the database, precisely as intended\nduring recording, and does not leave much room for the\nencoded vectors zq to pick up additional nuances in emotional\nexpression. While VQR seems to yield smaller and more well-\ndeﬁned clusters than VQS, the differences are marginal and\nunlikely to have substantial impact on the synthesis.\nIn summary, we ﬁnd that the unsupervised methods very\nsuccessfully identiﬁed the emotional classes in held-out speech\ndata on our task, despite not having access to explicit emo-\ntional annotation. This conﬁrms that these methods are capable\nof identifying and representing salient, unannotated variation\nin the data, just like the unsupervised style tokens in [32].\nE. Subjective Evaluation\nReduced objective error does not necessarily imply a per-\nceptually better system. In fact, the true minimiser of the\nMSE objective we use is the conditional mean of X. This\nmean was estimated directly from repeated speech in [2] and\nfound to be perceptually inferior to random sampling in highly\naccurate models. In order not to be led astray by the objective\nperformance, we complemented our observations above with a\ncrowdsourced subjective listening test similar to those in [63].\n1) Listening Test Design: For the listening test, the BOT\nsystem was excluded, as it is incapable of control. Each\nof the four unsupervised systems, however, was represented\ntwice: once synthesising from control vectors derived from\nencoding the ground-truth held-out test sentences (the normal\nautoencoder approach), and once with the latent input to the\nencoder always set equal to the mean latent vector z for\nthe relevant emotion across the entire training set. While\nthe former control scheme varies the control input z from\nutterance to utterance, the latter holds z constant for each\nemotion, wherefore we refer to these schemes as per-utterance\nand per-emotion control, respectively.\nOur per-utterance control may in principle be able to\nreproduce nuances in the emotional expression of each test\nutterance, but requires access to the held-out test-set acoustics\nto do so. Per-emotion control is derived from emotional labels\non the training data (instead of using test-set acoustics), but\nany systematic variation in perceived emotional strength across\nutterances must then be attributed to the text input alone.\nTogether, the two control schemes can be used to assess the\nsystems’ abilities to replicate nuances in emotional expression\non the test set. Many other control schemes are also possible,\nbut studying them is left as future work.\nA system paired with a control scheme will be termed a\ncondition, of which we investigated a total of 11: NAT, VOC,\nSUP, and two each (for the two control schemes) for the\nunsupervised systems VQS, VQR, HZI, and HSI. Each of the\n1680 utterances in the test set (240 per emotion) can then be\nrealised in any condition, producing a stimulus waveform.\nOur subjective evaluation recruited native Japanese listeners\nthrough CrowdWorksLTD to evaluate sets of 22 randomly-\nselected stimuli through a web-based interface. The sets were\nconstrained such that all stimuli were unique and each con-\ndition appeared exactly twice in each set. No listener was\npermitted to evaluate more than 10 sets.\nEvaluators processed the stimuli in the set in sequence.\nFor each stimulus, they were asked to supply three pieces of\ninformation: i) perceived speech quality (traditional MOS scale\nof integers “1 – bad” through “5 – excellent”); ii) perceived\nemotional category (response options being the seven emotions\nin the database plus “other”); and iii) perceived emotional\nstrength (integer scale “1 – almost no emotion” through “5\n– very emotional”, or 6 for “no emotion”). Evaluators could\nlisten to each stimulus as many times as desired before\nresponding. In total, 700 response triplets were gathered for\neach emotion, from a total of 50 different listeners.\n2) Evaluation of Synthesis Quality: The ﬁrst set of columns\nin Table III shows the mean opinion scores (MOS) for speech\nquality for the different systems and control strategies invest-\nigated. To check if the differences were signiﬁcant we applied\ntwo-sided Mann-Whitney U tests comparing all condition\npairs, with Holm-Bonferroni correction [100] used to keep\nthe familywise error rate below 5%. These tests found NAT\nand VOC to be signiﬁcantly different from all other systems,\nas well as from each other. No other differences in quality\nwere found to be statistically signiﬁcant. t-tests (also with\nHolm-Bonferroni correction) gave the same conclusions. We\nthus observe that SPSS, while not achieving the same per-\nformance as natural speech, can achieve good output quality\nboth through supervised as well as unsupervised control in\nthis application. The difference between the best and the\nworst (SUP) synthesiser MOS is a mere 0.13 points on the\nﬁve-point MOS scale. While there was evidence of a minor\namortisation gap between VQ-VAEs and heuristic systems in\nterms of objective performance (i.e., MSE), this gap does\nnot appear to have affected speech quality. Given that VQ-\nVAEs have advantages of being easier to train and allow\nstraightforward latent-variable inference through amortisation,\nthis makes them an appealing practical choice.\nPREPRINT. WORK IN PROGRESS.\n15\nTable IV\nFROBENIUS DISTANCES BETWEEN EMOTIONAL CONFUSION MATRICES.\nTHE BEST UNSUPERVISED PERFORMANCE IN EACH COLUMN IS BOLDED.\nPer-utterance control\nPer-emotion control\nSystem\nvs. ID\nvs. ref\nvs. NAT\nvs. ID\nvs. ref\nvs. NAT\nNAT\n0.50\n1.04\n0.00\n-\n-\n-\nVOC\n0.68\n1.26\n0.37\n-\n-\n-\nSUP\n0.71\n1.51\n0.69\n-\n-\n-\nVQS\n0.63\n1.39\n0.46\n0.48\n1.27\n0.53\nVQR\n0.58\n1.35\n0.51\n0.65\n1.44\n0.70\nHZI\n0.60\n1.39\n0.53\n0.59\n1.37\n0.55\nHSI\n0.64\n1.42\n0.52\n0.62\n1.42\n0.63\n3) Evaluation of Output Control: Our primary interest in\nthis work is not synthesis quality but controllability. We\ntherefore assessed the synthesisers’ ability to reproduce the\nemotions in the database by studying the emotional classiﬁca-\ntions assigned by the listeners in the listening test. These clas-\nsiﬁcations can be summarised through a confusion matrix, tab-\nulating the distribution of listener classiﬁcations conditioned\non the different prompted emotions. In the ideal case when\nall emotions are perceived as intended, this matrix should be\nthe identity matrix. For completely natural speech there are\nnonetheless some confusions between emotions (as discussed\nin [63]), leading to some off-diagonal matrix structure.\nFollowing the same methodology as in [63, Sec. 8.1.1], we\ncomputed emotional classiﬁcation confusion matrices for each\nand every condition in the listening test (700 classiﬁcations\nper condition). These matrices were then compared against\nthree different reference matrices: the ideal (identity matrix,\n‘ID’) as well as two confusion matrices from natural speech,\nnamely the one tabulated in [63, Table 5] (‘ref’) as well\nas the one computed from listener classiﬁcations of natural\nspeech in the present listening test (‘NAT’). Speciﬁcally, we\ncomputed the Frobenius norm of the difference between every\nconfusion matrix and every reference matrix. Table IV presents\nthe results of this comparison. A system that well separates and\nreproduces the different emotions should have low distance to\nthe three references in the table.\nWhile\nidentifying\nstatistically\nsigniﬁcant\ndifferences\nbetween confusion matrices is not a solved problem (see,\ne.g., [101]), we note that (with one single exception) NAT\nis better than all other conditions in all metrics; this agrees\nwith our expectation that the recorded natural speech should\nperform at least as well as SPSS control schemes learned\nfrom the same data. On the other end of the spectrum, SUP\nis found to have greater distance to the reference matrices\nthan all other conditions (again with a single exception). All\nother conditions exhibit broadly comparable numbers for\neach reference. Taken together, these patterns suggest that\nunsupervised approaches are at least as good (or better) than\nsupervised learning of control in the present application,\nbut that there is little difference between VQ-VAEs and the\nheuristic methods (and between different control schemes) in\nhow reliably they reproduce the base emotions in the corpus.\nAs the controllable speech synthesisers considered in this\nwork are capable of control inputs that differentiate more than\njust the seven base emotions, there is the possibility that they\nmay learn to control other aspects of speech variability such\nas emotional nuance (cf. [14]), assuming such variation is\npresent in the training data. This might be reﬂected in the\nemotional strength ratings, whose means are tabulated in the\nlast two columns of Table III. (For this analysis, a response of\n“no emotion” was mapped to an emotional strength of zero.)\nHolm-Bonferroni corrected Mann-Whitney U tests between\nconditions (the same methodology used to analyse synthesis\nquality earlier) show that NAT and VOC perform similarly,\nand better than other conditions, which otherwise exhibit no\nsigniﬁcant differences. Thus the unsupervised approaches are\nagain competitive with the supervised system.\nNo differences are evident between per-utterance and per-\nemotion control in this evaluation. This might not be too\nsurprising, given the lack of diversity (only one or two bits of\nentropy) observed in Table II among control inputs in the same\nemotion class. Such a ﬁnding is consistent with expectations\nthat the range of nuances within each emotion is quite limited\nin our speech corpus. It is possible that exaggerating the\ndifferences between utterance control inputs, as done in [14],\nwould give more noticeable differences in expression within\neach emotion class.\nTo summarise, we have found that the unsupervised ap-\nproaches under consideration are comparable to the supervised\nsystem also in terms of perceived speech quality, emotion\nrecognition, and perceived emotional strength. Moreover, the\ndifferent unsupervised systems and control schemes appear\nessentially perceptually equivalent in our evaluation.\nVI. CONCLUSION\nThis paper has studied the theory and practice of un-\nsupervised learning of output control in statistical text to\nspeech. On the theory side, we have established novel connec-\ntions between traditional unsupervised heuristics from speech-\ntechnology, like DCC and sentence-level control vectors, and\nvariational latent-variable inference in autoencoder models.\nWe have likewise connected the heuristics to VQ-VAEs, which\nwe have shown have a similar interpretation as variational\ninference neglecting uncertainty in a Gaussian mixture model.\nIn terms of empirical insights, we have compared supervised\nand unsupervised methods for learning controllable acoustic\nmodels on a large corpus of emotional speech. The objective\nand subjective results show that the unsupervised methods\nsuccessfully learn and reproduce the emotional classes in the\nspeech data and often outperform a competitive supervised\nbaseline. This bodes well for unsupervised learning for en-\nabling output control in speech synthesis at large. Methods\nincorporating amortised inference stand out as particularly\nappealing for future applications, since they achieve similar\nperformance as the established heuristics but enable easier\ntraining and latent-variable inference.\nREFERENCES\n[1] S. Van Kuyk, W. B. Kleijn, and R. C. Hendriks, “On the information\nrate of speech communication,” in Proc. ICASSP, 2017, pp. 5625–5629.\n[2] G. E. Henter, T. Merritt, M. Shannon, C. Mayo, and S. King,\n“Measuring the perceptual effects of modelling assumptions in speech\nsynthesis using stimuli constructed from repeated natural speech,” in\nProc. Interspeech, 2014, pp. 1504–1508.\n16\nPREPRINT. WORK IN PROGRESS.\n[3] B. Uria, I. Murray, S. Renals, C. Valentini-Botinhao, and J. Bridle,\n“Modelling acoustic feature dependencies with artiﬁcial neural net-\nworks: Trajectory-RNADE,” in Proc. ICASSP, 2015, pp. 4465–4469.\n[4] Y. Fan, Y. Qian, F. K. Soong, and L. He, “Multi-speaker modeling and\nspeaker adaptation for DNN-based TTS synthesis,” in Proc. ICASSP,\n2015, pp. 4475–4479.\n[5] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “Wave-\nNet: A generative model for raw audio,” arXiv preprint 1609.03499,\n2016.\n[6] B. Li and H. Zen, “Multi-language multi-speaker acoustic modeling\nfor LSTM-RNN based statistical parametric speech synthesis,” in Proc.\nInterspeech, 2016, pp. 2468–2472.\n[7] H.-T. Luong, S. Takaki, G. E. Henter, and J. Yamagishi, “Adapting and\ncontrolling DNN-based speech synthesis using input codes,” in Proc.\nICASSP, 2017, pp. 4905–4909.\n[8] O. Abdel-Hamid and H. Jiang, “Fast speaker adaptation of hybrid\nNN/HMM model for speech recognition based on discriminative learn-\ning of speaker code,” in Proc. ICASSP, 2013, pp. 7942–7946.\n[9] S. Xue, O. Abdel-Hamid, H. Jiang, L.-R. Dai, and Q. Liu, “Fast\nadaptation of deep neural network based on discriminant codes for\nspeech recognition,” IEEE/ACM T. Audio Speech, vol. 22, no. 12, pp.\n1713–1725, 2014.\n[10] O. Watts, Z. Wu, and S. King, “Sentence-level control vectors for deep\nneural network speech synthesis,” in Proc. Interspeech, 2015, pp. 2217–\n2221.\n[11] S. ¨O. Arık, G. Diamos, A. Gibiansky, J. Miller, K. Peng, W. Ping,\nJ. Raiman, and Y. Zhou, “Deep Voice 2: Multi-speaker neural text-to-\nspeech,” in Proc. NIPS, 2017, pp. 2962–2970.\n[12] Y. Taigman, L. Wolf, A. Polyak, and E. Nachmani, “VoiceLoop: Voice\nﬁtting and synthesis via a phonological loop,” in Proc. ICLR, 2018.\n[13] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, “Neural discrete\nrepresentation learning,” in Proc. NIPS, 2017, pp. 6309–6318.\n[14] G. E. Henter, J. Lorenzo-Trueba, X. Wang, and J. Yamagishi, “Prin-\nciples for learning controllable TTS from annotated and latent vari-\nation,” in Proc. Interspeech, 2017, pp. 3956–3960.\n[15] D. H. Klatt, “Review of text-to-speech conversion for English,” The\nJournal of the Acoustical Society of America, vol. 82, no. 3, pp. 737–\n793, 1987.\n[16] H. Zen, K. Tokuda, and A. W. Black, “Statistical parametric speech\nsynthesis,” Speech Commun., vol. 51, no. 11, pp. 1039–1064, 2009.\n[17] S. King, “An introduction to statistical parametric speech synthesis,”\nSadhana, vol. 36, no. 5, pp. 837–852, 2011.\n[18] J. R. Quinlan, “Improved use of continuous attributes in C4.5,” J. Artif.\nIntel. Res., vol. 4, pp. 77–90, 1996.\n[19] K. Fujinaga, M. Nakai, H. Shimodaira, and S. Sagayama, “Multiple-\nregression hidden Markov model,” in Proc. ICASSP, 2001, pp. 513–\n516.\n[20] T. Masuko, T. Kobayashi, and K. Miyanaga, “A style control technique\nfor HMM-based speech synthesis,” in Proc. Interspeech, 2004, pp.\n1437–1439.\n[21] T. Nose, Y. Kato, and T. Kobayashi, “Style estimation of speech\nbased on multiple regression hidden semi-Markov model,” in Proc.\nInterspeech, 2007, pp. 2285–2288.\n[22] Z.-H. Ling, K. Richmond, and J. Yamagishi, “Articulatory control of\nHMM-based parametric speech synthesis using feature-space-switched\nmultiple regression,” IEEE T. Audio Speech, vol. 21, no. 1, pp. 207–\n219, 2013.\n[23] I. Jauk, “Unsupervised learning for expressive speech synthesis,” Ph.D.\ndissertation, Polytechnic University of Catalonia, Barcelona, Spain, Jun\n2017.\n[24] M. J. F. Gales, “Cluster adaptive training of hidden Markov models,”\nIEEE T. Speech Audi. P., vol. 8, no. 4, pp. 417–428, 2000.\n[25] L. Chen, M. J. F. Gales, V. Wan, J. Latorre, and M. Akamine,\n“Exploring rich expressive information from audiobook data using\ncluster adaptive training,” in Proc. Interspeech, 2012, pp. 959–962.\n[26] S. King and V. Karaiskos, “The Blizzard Challenge 2016,” in Proc.\nBlizzard Challenge Workshop, 2016.\n[27] K. Sawada, K. Hashimoto, K. Oura, and K. Tokuda, “The NITech text-\nto-speech system for the Blizzard Challenge 2017,” in Proc. Blizzard\nChallenge Workshop, 2017.\n[28] Q. V. Le and T. Mikolov, “Distributed representations of sentences and\ndocuments,” in Proc. ICML, 2014, pp. 1188–1196.\n[29] S. King, L. Wihlborg, and W. Guo, “The Blizzard Challenge 2017,” in\nProc. Blizzard Challenge Workshop, 2017.\n[30] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly,\nZ. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis,\nR. Clark, and R. A. Saurous, “Tacotron: A fully end-to-end text-to-\nspeech synthesis model,” in Proc. Interspeech, 2017, pp. 4006–4010.\n[31] R. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stanton, J. Shor,\nR. J. Weiss, R. Clark, and R. A. Saurous, “Towards end-to-end prosody\ntransfer for expressive speech synthesis with Tacotron,” arXiv preprint\narXiv:1803.09047, 2018.\n[32] Y. Wang, D. Stanton, Y. Zhang, R. Skerry-Ryan, E. Battenberg, J. Shor,\nY. Xiao, F. Ren, Y. Jia, and R. A. Saurous, “Style tokens: Unsupervised\nstyle modeling, control and transfer in end-to-end speech synthesis,”\narXiv preprint arXiv:1803.09017, 2018.\n[33] Y. Wang, R. Skerry-Ryan, Y. Xiao, D. Stanton, J. Shor, E. Battenberg,\nR. Clark, and R. A. Saurous, “Uncovering latent style factors for\nexpressive speech synthesis,” in NIPS ML4Audio Workshop, 2017.\n[34] Y. Jia, Y. Zhang, R. J. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen,\nP. Nguyen, R. Pang, I. L. Moreno, and W. Yonghui, “Transfer learning\nfrom speaker veriﬁcation to multispeaker text-to-speech synthesis,”\narXiv preprint arXiv:1806.04558, 2018.\n[35] C. M. Bishop, Pattern Recognition and Machine Learning, 1st ed. New\nYork, NY: Springer, 2006.\n[36] L. R. Rabiner, “A tutorial on hidden Markov models and selected\napplications in speech recognition,” Proc. IEEE, vol. 77, no. 2, pp.\n257–286, 1989.\n[37] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood\nfrom incomplete data via the EM algorithm,” J. Roy. Stat. Soc. B,\nvol. 39, no. 1, pp. 1–38, 1977.\n[38] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” in\nProc. ICLR, 2014.\n[39] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backpropaga-\ntion and approximate inference in deep generative models,” in Proc.\nICML, vol. 32, no. 2, 2014, pp. 1278–1286.\n[40] C. Doersch, “Tutorial on variational autoencoders,” arXiv preprint\narXiv:1606.05908, 2016.\n[41] P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel, “The Helmholtz\nmachine,” Neural Comput., vol. 7, no. 5, pp. 889–904, 1995.\n[42] M. Blaauw and J. Bonada, “Modeling and transforming speech using\nvariational autoencoders,” in Proc. Interspeech, 2016, pp. 1770–1774.\n[43] X. Chen, D. P. Kingma, T. Salimans, Y. Duan, P. Dhariwal, J. Schul-\nman, I. Sutskever, and P. Abbeel, “Variational lossy autoencoder,” arXiv\npreprint arXiv:1611.02731, 2016.\n[44] F.\nHusz´ar.\n(2017)\nIs\nmaximum\nlikelihood\nuseful\nfor\nrepresentation learning? [Online]. Available: http://www.inference.\nvc/maximum-likelihood-for-representation-learning-2/\n[45] A.\nGraves,\nJ.\nMenick,\nand\nA.\nvan\nden\nOord,\n“Associative\ncompression networks for representation learning,” arXiv preprint\narXiv:1804.02476, 2018.\n[46] D. Povey, L. Burget, M. Agarwal, P. Akyazi, K. Feng, A. Ghoshal,\nO. Glembek, N. K. Goel, M. Karaﬁ´at, A. Rastrow, R. C. Rose,\nP. Schwarz, and S. Thomas, “Subspace Gaussian mixture models for\nspeech recognition,” in Proc. ICASSP, 2010, pp. 4330–4333.\n[47] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nin Proc. NIPS, 2014, pp. 2672–2680.\n[48] I. Goodfellow, “NIPS 2016 tutorial: Generative adversarial networks,”\narXiv preprint arXiv:1701.00160, 2016.\n[49] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang, “Voice\nconversion from non-parallel corpora using variational auto-encoder,”\nin Proc. APSIPA, 2016, pp. 1–6.\n[50] ——, “Voice conversion from unaligned corpora using variational\nautoencoding Wasserstein generative adversarial networks,” in Proc.\nInterspeech, 2017, pp. 3364–3368.\n[51] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, “ACVAE-VC:\nNon-parallel many-to-many voice conversion with auxiliary classiﬁer\nvariational autoencoder,” arXiv preprint arXiv:1808.05092, 2018.\n[52] W.-N. Hsu, Y. Zhang, and J. Glass, “Learning latent representations\nfor speech generation and transformation,” in Proc. Interspeech, 2017,\np. 12731277.\n[53] ——, “Unsupervised learning of disentangled and interpretable repres-\nentations from sequential data,” in Proc. NIPS, 2017, pp. 1878–1889.\n[54] K. Akuzawa, Y. Iwasawa, and Y. Matsuo, “Expressive speech synthesis\nvia modeling expressions with variational autoencoder,” in Proc. Inter-\nspeech, 2018, to appear.\n[55] O. Fabius and J. R. van Amersfoort, “Variational recurrent auto-\nencoders,” Proc. ICLR Workshop Track, 2014.\nPREPRINT. WORK IN PROGRESS.\n17\n[56] J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, and Y. Bengio,\n“A recurrent latent variable model for sequential data,” in Proc. NIPS,\n2015, pp. 2980–2988.\n[57] M. Fraccaro, S. K. Sønderby, U. Paquet, and O. Winther, “Sequential\nneural models with stochastic layers,” in Proc. NIPS, 2016, pp. 2199–\n2207.\n[58] J. Marino, M. Cvitkovic, and Y. Yue, “A general framework for\namortizing variational ﬁltering,” in ICML 2018 Workshop Theor. Found.\nAppl. Deep Gener. Model., 2018.\n[59] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo,\nA. Courville, and Y. Bengio, “SampleRNN: An unconditional end-\nto-end neural audio generation model,” in Proc. ICLR, 2017.\n[60] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury, N. Casagrande,\nE. Lockhart, F. Stimberg, A. van den Oord, S. Dieleman, and K. Kavuk-\ncuoglu, “Efﬁcient neural audio synthesis,” in Proc. ICML, 2018, pp.\n2410–2419.\n[61] X. Wang, “Fundamental frequency modeling for neural-network-based\nstatistical parametric speech synthesis,” Ph.D. dissertation, SOKENDAI\n(The Graduate University for Advanced Studies), Tokyo, Japan, Sep\n2018.\n[62] X. Wang, S. Takaki, and J. Yamagishi, “Autoregressive neural F0\nmodel for statistical parametric speech synthesis,” IEEE/ACM T. Audio\nSpeech, vol. 26, no. 8, pp. 1406–1419, 2018.\n[63] J. Lorenzo-Trueba, G. E. Henter, S. Takaki, J. Yamagishi, Y. Morino,\nand Y. Ochiai, “Investigating different representations for modeling and\ncontrolling multiple emotions in dnn-based speech,” Speech Commun.,\n2018.\n[64] R. Barra-Chicote, J. Yamagishi, S. King, J. M. Montero, and J. Macias-\nGuarasa, “Analysis of statistical parametric and unit selection speech\nsynthesis systems applied to emotional speech,” Speech Commun.,\nvol. 52, no. 5, pp. 394–404, 2010.\n[65] D. Erro, E. Navas, I. Herndez, and I. Saratxaga, “Emotion conversion\nbased on prosodic unit selection,” IEEE T. Audio Speech, vol. 18, no. 5,\npp. 974–983, 2010.\n[66] P. Tsiakoulis, S. Raptis, S. Karabetsos, and A. Chalamandaris, “Affect-\nive word ratings for concatenative text-to-speech synthesis,” in Proc.\nPCI, 2016.\n[67] J. Yamagishi, K. Onishi, T. Masuko, and T. Kobayashi, “Acoustic\nmodeling of speaking styles and emotional expressions in HMM-based\nspeech synthesis,” IEICE T. Inf. Syst., vol. 88, no. 3, pp. 502–509, 2005.\n[68] T. Nose and T. Kobayashi, “An intuitive style control technique in\nHMM-based expressive speech synthesis using subjective style intens-\nity and multiple-regression global variance model,” Speech Commun.,\nvol. 55, no. 2, pp. 347–357, 2013.\n[69] J. Lorenzo-Trueba, R. Barra-Chicote, R. San-Segundo, J. Ferreiros,\nJ. Yamagishi, and J. M. Montero, “Emotion transplantation through\nadaptation in HMM-based speech synthesis,” Comput. Speech Lang.,\n2015.\n[70] J. P. Cabral, C. Saam, E. Vanmassenhove, S. Bradley, and F. Haider,\n“The ADAPT entry to the Blizzard Challenge 2016,” in Proc. Blizzard\nChallenge Workshop, 2016.\n[71] Q. T. Do, T. Toda, G. Neubig, S. Sakti, and S. Nakamura, “A hybrid\nsystem for continuous word-level emphasis modeling based on HMM\nstate clustering and adaptive training,” in Proc. Interspeech, 2016, pp.\n3196–3200.\n[72] J. Sotelo, S. Mehri, K. Kumar, J. a. F. Santos, K. Kastner, A. Courville,\nand Y. Bengio, “Char2Wav: End-to-end speech synthesis,” in Proc.\nICLR Workshop Track, 2017.\n[73] W. Ping, K. Peng, A. Gibiansky, S. ¨O. Arık, A. Kannan, S. Narang,\nJ. Raiman, and J. Miller, “Deep Voice 3: Scaling text-to-speech with\nconvolutional sequence learning,” in Proc. ICLR, 2018.\n[74] H. Kawahara, “STRAIGHT, exploitation of the other aspect of VO-\nCODER: Perceptually isomorphic decomposition of speech sounds,”\nAcoust. Sci. Technol., vol. 27, no. 6, pp. 349–353, 2006.\n[75] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: a vocoder-based\nhigh-quality speech synthesis system for real-time applications,” IEICE\nT. Inf. Syst., vol. 99, no. 7, pp. 1877–1884, 2016.\n[76] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen,\nY. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgian-\nnakis, and Y. Wu, “Natural TTS synthesis by conditioning WaveNet on\nmel spectrogram predictions,” in Proc. ICASSP, 2018, pp. 4799–4783.\n[77] C. Cremer, X. Li, and D. Duvenaud, “Inference suboptimality in\nvariational autoencoders,” in Proc. ICLR Workshop Track, 2018.\n[78] R. Shu, H. H. Bui, S. Zhao, M. J. Kochenderfer, and S. Ermon,\n“Amortized inference regularization,” arXiv preprint arXiv:1805.08913,\n2018.\n[79] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,\nS. Mohamed, and A. Lerchner, “beta-VAE: Learning basic visual\nconcepts with a constrained variational framework,” in Proc. ICLR,\n2016.\n[80] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and\nS. Bengio, “Generating sentences from a continuous space,” in Proc.\nCoNLL, 2016, pp. 10–21.\n[81] M. D. Hoffman, C. Riquelme, and M. J. Johnson, “The β-vaes implicit\nprior,” in Proc. NIPS 2017 Workshop Bayesian Deep Learn., vol. 2,\n2017.\n[82] Y. Bengio, N. L´eonard, and A. Courville, “Estimating or propagating\ngradients through stochastic neurons for conditional computation,”\narXiv preprint arXiv:1308.3432, 2013.\n[83] E. T. Nalisnick, L. Hertel, and P. Smyth, “Approximate inference for\ndeep latent Gaussian mixtures,” in Proc. NIPS 2016 Workshop Bayesian\nDeep Learn., vol. 1, 2016.\n[84] J. M. Tomczak and M. Welling, “VAE with a VampPrior,” arXiv\npreprint arXiv:1705.07120, 2017.\n[85] E. Nachmani, A. Polyak, Y. Taigman, and L. Wolf, “Fitting new\nspeakers based on a short untranscribed sample,” in Proc. ICML, 2018,\npp. 3683–3691.\n[86] K. Oura, S. Sako, and K. Tokuda, “Japanese text-to-speech synthesis\nsystem: Open JTalk,” in Proc. ASJ Spring, 2010, pp. 343–344.\n[87] M. Morise, “Cheaptrick, a spectral envelope estimator for high-quality\nspeech synthesis,” Speech Commun., vol. 67, pp. 1–7, 2015.\n[88] A. Camacho and J. G. Harris, “A sawtooth waveform inspired pitch\nestimator for speech and music,” The Journal of the Acoustical Society\nof America, vol. 124, no. 3, pp. 1638–1652, 2008.\n[89] H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko, A. W. Black, and\nK. Tokuda, “The HMM-based speech synthesis system (HTS) version\n2.0,” in Proc. SSW, 2007, pp. 294–299.\n[90] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Kitamura,\n“Speech parameter generation algorithms for HMM-based speech syn-\nthesis,” in Proc. ICASSP, 2000, pp. 1315–1318.\n[91] T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi, and T. Kitamura,\n“Incorporating a mixed excitation model and postﬁlter into HMM-\nbased text-to-speech synthesis,” Syst. Comput. Jpn., vol. 36, no. 12,\npp. 43–50, 2005.\n[92] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin Proc. ICLR, 2015.\n[93] X. Wang, S. Takaki, and J. Yamagishi, “An autoregressive recurrent\nmixture density network for parametric speech synthesis,” in Proc.\nICASSP, 2017, pp. 4895–4899.\n[94] Y. Fan, Y. Qian, F.-L. Xie, and F. K. Soong, “TTS synthesis with\nbidirectional LSTM based recurrent neural networks,” in Proc. Inter-\nspeech, 2014, pp. 1964–1968.\n[95] F. Weninger, J. Bergmann, and B. W. Schuller, “Introducing CUR-\nRENNT: The Munich open-source CUDA recurrent neural network\ntoolkit,” J. Mach. Learn. Res., vol. 16, no. 3, pp. 547–551, 2015.\n[96] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep\nfeedforward neural networks,” in Proc. AISTATS, 2010, pp. 249–256.\n[97] O. Watts, G. E. Henter, T. Merritt, Z. Wu, and S. King, “From HMMs\nto DNNs: where do the improvements come from?” in Proc. ICASSP,\n2016, pp. 5505–5509.\n[98] L. van der Maaten and G. Hinton, “Visualizing data using t-SNE,” J.\nMach. Learn. Res., vol. 9, no. Nov, pp. 2579–2605, 2008.\n[99] C. D. Manning, P. Raghavan, and H. Sch¨utze, Introduction to Inform-\nation Retrieval.\nCambridge University Press, 2008.\n[100] S. Holm, “A simple sequentially rejective multiple test procedure,”\nScand. J. Stat., vol. 6, no. 2, pp. 65–70, 1979.\n[101] A. Leijon, G. E. Henter, and M. Dahlquist, “Bayesian analysis of\nphoneme confusion matrices,” IEEE/ACM T. Audio Speech, vol. 24,\nno. 3, pp. 469–482, March 2016.\n",
  "categories": [
    "eess.AS",
    "cs.LG",
    "cs.SD",
    "stat.ML",
    "62F99",
    "I.2.7; G.3"
  ],
  "published": "2018-07-30",
  "updated": "2018-09-09"
}