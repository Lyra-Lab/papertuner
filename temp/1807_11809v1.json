{
  "id": "http://arxiv.org/abs/1807.11809v1",
  "title": "Deep learning in agriculture: A survey",
  "authors": [
    "Andreas Kamilaris",
    "Francesc X. Prenafeta-Boldu"
  ],
  "abstract": "Deep learning constitutes a recent, modern technique for image processing and\ndata analysis, with promising results and large potential. As deep learning has\nbeen successfully applied in various domains, it has recently entered also the\ndomain of agriculture. In this paper, we perform a survey of 40 research\nefforts that employ deep learning techniques, applied to various agricultural\nand food production challenges. We examine the particular agricultural problems\nunder study, the specific models and frameworks employed, the sources, nature\nand pre-processing of data used, and the overall performance achieved according\nto the metrics used at each work under study. Moreover, we study comparisons of\ndeep learning with other existing popular techniques, in respect to differences\nin classification or regression performance. Our findings indicate that deep\nlearning provides high accuracy, outperforming existing commonly used image\nprocessing techniques.",
  "text": "1 \n \nDeep Learning in Agriculture: A Survey \n \nAndreas Kamilaris1 and Francesc X. Prenafeta-Boldú \nInstitute for Food and Agricultural Research and Technology (IRTA)  \n \n \nAbstract: Deep learning constitutes a recent, modern technique for image processing and \ndata analysis, with promising results and large potential. As deep learning has been \nsuccessfully applied in various domains, it has recently entered also the domain of \nagriculture. In this paper, we perform a survey of 40 research efforts that employ deep \nlearning techniques, applied to various agricultural and food production challenges. We \nexamine the particular agricultural problems under study, the specific models and \nframeworks employed, the sources, nature and pre-processing of data used, and the \noverall performance achieved according to the metrics used at each work under study. \nMoreover, we study comparisons of deep learning with other existing popular techniques, \nin respect to differences in classification or regression performance. Our findings indicate \nthat deep learning provides high accuracy, outperforming existing commonly used image \nprocessing techniques. \n \nKeywords: Deep learning, Agriculture, Survey, Convolutional Neural Networks, Recurrent \nNeural Networks, Smart Farming, Food Systems. \n \n \n \n                                                 \n1 Corresponding Author. Email: andreas.kamilaris@irta.cat  \n2 \n \n1. Introduction \nSmart farming (Tyagi, 2016) is important for tackling the challenges of agricultural \nproduction in terms of productivity, environmental impact, food security and sustainability \n(Gebbers & Adamchuk, 2010). As the global population has been continuously increasing \n(Kitzes, et al., 2008), a large increase on food production must be achieved (FAO, 2009), \nmaintaining at the same time availability and high nutritional quality across the globe, \nprotecting the natural ecosystems by using sustainable farming procedures. \nTo address these challenges, the complex, multivariate and unpredictable agricultural \necosystems need to be better understood by monitoring, measuring and analyzing \ncontinuously various physical aspects and phenomena. This implies analysis of big \nagricultural data (Kamilaris, Kartakoullis, & Prenafeta-Boldú, A review on the practice of \nbig data analysis in agriculture, 2017), and the use of new information and communication \ntechnologies (ICT) (Kamilaris, Gao, Prenafeta-Boldú, & Ali, 2016), both for short-scale \ncrop/farm management as well as for larger-scale ecosystems’ observation, enhancing the \nexisting tasks of management and decision/policy making by context, situation and \nlocation awareness. Larger-scale observation is facilitated by remote sensing \n(Bastiaanssen, Molden, & Makin, 2000), performed by means of satellites, airplanes and \nunmanned aerial vehicles (UAV) (i.e. drones), providing wide-view snapshots of the \nagricultural environments. It has several advantages when applied to agriculture, being a \nwell-known, non-destructive method to collect information about earth features while data \nmay be obtained systematically over large geographical areas. \nA large subset of the volume of data collected through remote sensing involve images. \nImages constitute, in many cases, a complete picture of the agricultural environments and \ncould address a variety of challenges (Liaghat & Balasundram, 2010), (Ozdogan, Yang, \nAllez, & Cervantes, 2010). Hence, imaging analysis is an important research area in the \nagricultural domain and intelligent data analysis techniques are being used for image \n3 \n \nidentification/classification, anomaly detection etc., in various agricultural applications \n(Teke, Deveci, Haliloğlu, Gürbüz, & Sakarya, 2013), (Saxena & Armstrong, 2014), (Singh, \nGanapathysubramanian, Singh, & Sarkar, 2016). The most popular techniques and \napplications are presented in Appendix I, together with the sensing methods employed to \nacquire the images. From existing sensing methods, the most common one is satellite-\nbased, using multi-spectral and hyperspectral imaging. Synthetic aperture radar (SAR), \nthermal and near infrared (NIR) cameras are being used in a lesser but increasing extent \n(Ishimwe, Abutaleb, & Ahmed, 2014), while optical and X-ray imaging are being applied in \nfruit and packaged food grading. The most popular techniques used for analyzing images \ninclude machine learning (ML) (K-means, support vector machines (SVM), artificial neural \nnetworks (ANN) amongst others), linear polarizations, wavelet-based filtering, vegetation \nindices (NDVI) and regression analysis (Saxena & Armstrong, 2014), (Singh, \nGanapathysubramanian, Singh, & Sarkar, 2016). \nBesides the aforementioned techniques, a new one which is recently gaining momentum is \ndeep learning (DL) (LeCun, Bengio, & Hinton, 2015), (LeCun & Bengio, 1995). DL belongs \nto the machine learning computational field and is similar to ANN. However, DL is about \n“deeper” neural networks that provide a hierarchical representation of the data by means \nof various convolutions. This allows larger learning capabilities and thus higher \nperformance and precision. A brief description of DL is attempted in Section 3.  \nThe motivation for preparing this survey stems from the fact that DL in agriculture is a \nrecent, modern and promising technique with growing popularity, while advancements and \napplications of DL in other domains indicate its large potential. The fact that today there \nexists at least 40 research efforts employing DL to address various agricultural problems \nwith very good results, encouraged the authors to prepare this survey. To the authors’ \nknowledge, this is the first such survey in the agricultural domain, while a small number of \nmore general surveys do exist (Deng & Yu, 2014), (Wan, et al., 2014), (Najafabadi, et al., \n4 \n \n2015), covering related work in DL in other domains. \n2. Methodology \nThe bibliographic analysis in the domain under study involved two steps: a) collection of \nrelated work and b) detailed review and analysis of this work. In the first step, a keyword-\nbased search for conference papers or journal articles was performed from the scientific \ndatabases IEEE Xplore and ScienceDirect, and from the web scientific indexing services \nWeb of Science and Google Scholar. As search keywords, we used the following query:  \n[\"deep learning\"] AND [\"agriculture\" OR ”farming\"] \nIn this way, we filtered out papers referring to DL but not applied to the agricultural domain. \nFrom this effort, 47 papers had been initially identified. Restricting the search for papers \nwith appropriate application of the DL technique and meaningful findings2, the initial \nnumber of papers reduced to 40. \nIn the second step, the 40 papers selected from the previous step were analyzed one-by-\none, considering the following research questions:  \n1. Which was the agricultural- or food-related problem they addressed? \n2. Which was the general approach and type of DL-based models employed? \n3. Which sources and types of data had been used? \n4. Which were the classes and labels as modeled by the authors? Were there any \nvariations among them, observed by the authors? \n5. Any pre-processing of the data or data augmentation techniques used? \n6. Which has been the overall performance depending on the metric adopted? \n7. Did the authors test the performance of their models on different datasets? \n8. Did the authors compare their approach with other techniques and, if yes, which \nwas the difference in performance? \nOur main findings are presented in Section 4 and the detailed information per paper is \n                                                 \n2 A small number of papers claimed of using DL in some agricultural-related application, but they did not \nshow any results nor provided performance metrics that could indicate the success of the technique used. \n5 \n \nlisted in Appendix II. \n3. Deep Learning \nDL extends classical ML by adding more \"depth\" (complexity) into the model as well as \ntransforming the data using various functions that allow data representation in a \nhierarchical way, through several levels of abstraction (Schmidhuber, 2015), (LeCun & \nBengio, 1995). A strong advantage of DL is feature learning, i.e. the automatic feature \nextraction from raw data, with features from higher levels of the hierarchy being formed by \nthe composition of lower level features (LeCun, Bengio, & Hinton, 2015). DL can solve \nmore complex problems particularly well and fast, because of more complex models used, \nwhich allow massive parallelization (Pan & Yang, 2010). These complex models employed \nin DL can increase classification accuracy or reduce error in regression problems, \nprovided there are adequately large datasets available describing the problem. DL \nconsists of various different components (e.g. convolutions, pooling layers, fully connected \nlayers, gates, memory cells, activation functions, encode/decode schemes etc.), \ndepending on the network architecture used (i.e. Unsupervised Pre-trained Networks, \nConvolutional Neural Networks, Recurrent Neural Networks, Recursive Neural Networks). \nThe highly hierarchical structure and large learning capacity of DL models allow them to \nperform classification and predictions particularly well, being flexible and adaptable for a \nwide variety of highly complex (from a data analysis perspective) challenges (Pan & Yang, \n2010). Although DL has met popularity in numerous applications dealing with raster-based \ndata (e.g. video, images), it can be applied to any form of data, such as audio, speech, \nand natural language, or more generally to continuous or point data such as weather data \n(Sehgal, et al., 2017), soil chemistry (Song, et al., 2016) and population data (Demmers T. \nG., Cao, Parsons, Gauss, & Wathes, 2012). An example DL architecture is displayed in \nFigure 1, illustrating CaffeNet (Jia, et al., 2014), an example of a convolutional neural \nnetwork, combining convolutional and fully connected (dense) layers. \n6 \n \n \nFigure 1: CaffeNet, an example CNN architecture. Source: (Sladojevic, Arsenovic, Anderla, \nCulibrk, & Stefanovic, 2016) \n \nConvolutional Neural Networks (CNN) constitute a class of deep, feed-forward ANN, and \nthey appear in numerous of the surveyed papers as the technique used (17 papers, 42%). \nAs the figure shows, various convolutions are performed at some layers of the network, \ncreating different representations of the learning dataset, starting from more general ones \nat the first larger layers, becoming more specific at the deeper layers. The convolutional \nlayers act as feature extractors from the input images whose dimensionality is then \nreduced by the pooling layers. The convolutional layers encode multiple lower-level \nfeatures into more discriminative features, in a way that is spatially context-aware. They \nmay be understood as banks of filters that transform an input image into another, \nhighlighting specific patterns. The fully connected layers, placed in many cases near the \noutput of the model, act as classifiers exploiting the high-level features learned to classify \ninput images in predefined classes or to make numerical predictions. They take a vector \nas input and produce another vector as output. An example visualization of leaf images \nafter each processing step of the CaffeNet CNN, at a problem of identifying plant diseases, \nis depicted in Figure 2. We can observe that after each processing step, the particular \nelements of the image that reveal the indication of a disease become more evident, \nespecially at the final step (Pool5). \n7 \n \n \nFigure 2: Visualization of the output layers images after each processing step of the CaffeNet CNN \n(i.e. convolution, pooling, normalization) at a plant disease identification problem based on leaf \nimages. Source: (Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016) \n \nOne of the most important advantages of using DL in image processing is the reduced \nneed of feature engineering (FE). Previously, traditional approaches for image \nclassification tasks had been based on hand-engineered features, whose performance \naffected heavily the overall results. FE is a complex, time-consuming process which needs \nto be altered whenever the problem or the dataset changes. Thus, FE constitutes an \nexpensive effort that depends on experts’ knowledge and does not generalize well (Amara, \nBouaziz, & Algergawy, 2017). On the other hand, DL does not require FE, locating the \nimportant features itself through training.  \nA disadvantage of DL is the generally longer training time. However, testing time is \ngenerally faster than other methods ML-based methods (Chen, Lin, Zhao, Wang, & Gu, \n2014). Other disadvantages include problems that might occur when using pre-trained \nmodels on datasets that are small or significantly different, optimization issues because of \nthe models’ complexity, as well as hardware restrictions. \nIn Section 5, we discuss over advantages and disadvantages of DL as they reveal through \nthe surveyed papers. \n \n8 \n \n3.1 Available Architectures, Datasets and Tools \nThere exist various successful and popular architectures, which researchers may use to \nstart building their models instead of starting from scratch. These include AlexNet \n(Krizhevsky, Sutskever, & Hinton, 2012), CaffeNet (Jia, et al., 2014) (displayed in Figure \n1), VGG (Simonyan & Zisserman, 2014), GoogleNet (Szegedy, et al., 2015) and Inception-\nResNet (Szegedy, Ioffe, Vanhoucke, & Alemi, 2017), among others. Each architecture has \ndifferent advantages and scenarios where it is more appropriate to be used (Canziani, \nPaszke, & Culurciello, 2016). It is also worth noting that almost all of the aforementioned \nmodels come along with their weights pre-trained, which means that their network had \nbeen already trained by some dataset and has thus learned to provide accurate \nclassification for some particular problem domain (Pan & Yang, 2010). Common datasets \nused for pre-training DL architectures include ImageNet (Deng, et al., 2009) and PASCAL \nVOC (PASCAL VOC Project, 2012) (see also Appendix III). \nMoreover, there exist various tools and platforms allowing researchers to experiment with \nDL (Bahrampour, Ramakrishnan, Schott, & Shah, 2015). The most popular ones are \nTheano, TensorFlow, Keras (which is an application programmer's interface on top of \nTheano and TensorFlow), Caffe, PyTorch, TFLearn, Pylearn2 and the Deep Learning \nMatlab Toolbox. Some of these tools (i.e. Theano, Caffe) incorporate popular architectures \nsuch as the ones mentioned above (i.e. AlexNet, VGG, GoogleNet), either as libraries or \nclasses. For a more elaborate description of the DL concept and its applications, the \nreader could refer to existing bibliography (Schmidhuber, 2015), (Deng & Yu, 2014), (Wan, \net al., 2014), (Najafabadi, et al., 2015), (Canziani, Paszke, & Culurciello, 2016), \n(Bahrampour, Ramakrishnan, Schott, & Shah, 2015). \n \n4. Deep Learning Applications in Agriculture \nIn Appendix II, we list the 40 identified relevant works, indicating the agricultural-related \n9 \n \nresearch area, the particular problem they address, DL models and architectures \nimplemented, sources of data used, classes and labels of the data, data pre-processing \nand/or augmentation employed, overall performance achieved according to the metrics \nadopted, as well as comparisons with other techniques, wherever available. \n4.1 Areas of Use \nSixteen areas have been identified in total, with the popular ones being identification of \nweeds (5 papers), land cover classification (4 papers), plant recognition (4 papers), fruits \ncounting (4 papers) and crop type classification (4 papers). \nIt is remarkable that all papers, except from (Demmers T. G., et al., 2010), (Demmers T. \nG., Cao, Parsons, Gauss, & Wathes, 2012) and (Chen, Lin, Zhao, Wang, & Gu, 2014), \nwere published during or after 2015, indicating how recent and modern this technique is, in \nthe domain of agriculture. More precisely, from the remaining 37 papers, 15 papers have \nbeen published in 2017, 15 in 2016 and 7 in 2015. \nThe large majority of the papers deal with image classification and identification of areas of \ninterest, including detection of obstacles (e.g. (Steen, Christiansen, Karstoft, & Jørgensen, \n2016), (Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016)) and fruit counting (e.g. \n(Rahnemoonfar & Sheppard, 2017), (Sa, et al., 2016)). Some papers focus on predicting \nfuture parameters, such as corn yield (Kuwata & Shibasaki, 2015) soil moisture content at \nthe field (Song, et al., 2016) and weather conditions (Sehgal, et al., 2017).  \nFrom another perspective, most papers (20) target crops, while few works consider issues \nsuch as weed detection (8 papers), land cover (4 papers), research on soil (2 papers), \nlivestock agriculture (3 papers), obstacle detection (3 papers) and weather prediction (1 \npaper). \n4.2 Data Sources \nObserving the sources of data used to train the DL model at every paper, large datasets of \nimages are mainly used, containing thousands of images in some cases, either real ones \n10 \n \n(e.g. (Mohanty, Hughes, & Salathé, 2016), (Reyes, Caicedo, & Camargo, 2015), \n(Dyrmann, Karstoft, & Midtiby, 2016 )), or synthetic produced by the authors \n(Rahnemoonfar & Sheppard, 2017), (Dyrmann, Mortensen, Midtiby, & Jørgensen, 2016). \nSome datasets originate from well-known and publicly-available datasets such as \nPlantVillage, LifeCLEF, MalayaKew, UC Merced and Flavia (see Appendix III), while \nothers constitute sets of real images collected by the authors for their research needs (e.g. \n(Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016), (Bargoti & Underwood, \n2016), (Xinshao & Cheng, 2015), (Sørensen, Rasmussen, Nielsen, & Jørgensen, 2017)). \nPapers dealing with land cover, crop type classification and yield estimation, as well as \nsome papers related to weed detection employ a smaller number of images (e.g. tens of \nimages), produced by UAV (Lu, et al., 2017), (Rebetez, J., et al., 2016), (Milioto, Lottes, & \nStachniss, 2017), airborne (Chen, Lin, Zhao, Wang, & Gu, 2014), (Luus, Salmon, van den \nBergh, & Maharaj, 2015) or satellite-based remote sensing (Kussul, Lavreniuk, Skakun, & \nShelestov, 2017), (Minh, et al., 2017), (Ienco, Gaetano, Dupaquier, & Maurel, 2017), \n(Rußwurm & Körner, 2017). A particular paper investigating segmentation of root and soil \nuses images from X-ray tomography (Douarre, Schielein, Frindel, Gerth, & Rousseau, \n2016). Moreover, some papers use text data, collected either from repositories (Kuwata & \nShibasaki, 2015), (Sehgal, et al., 2017) or field sensors (Song, et al., 2016), (Demmers T. \nG., et al., 2010), (Demmers T. G., Cao, Parsons, Gauss, & Wathes, 2012). In general, the \nmore complicated the problem to be solved, the more data is required. For example, \nproblems involving large number of classes to identify (Mohanty, Hughes, & Salathé, \n2016), (Reyes, Caicedo, & Camargo, 2015), (Xinshao & Cheng, 2015) and/or small \nVariation among the classes (Luus, Salmon, van den Bergh, & Maharaj, 2015), (Rußwurm \n& Körner, 2017), (Yalcin, 2017 ), (Namin, Esmaeilzadeh, Najafi, Brown, & Borevitz, 2017), \n(Xinshao & Cheng, 2015), require large number of input images to train their models. \n \n11 \n \n4.3 Data Variation \nVariation between classes is necessary for the DL models to be able to differentiate \nfeatures and characteristics, and perform accurate classifications3. Hence, accuracy is \npositively correlated with variation among classes. Nineteen papers (47%) revealed some \naspects of poor data variation. Luus et al. (Luus, Salmon, van den Bergh, & Maharaj, \n2015) observed high relevance between some land cover classes (i.e. medium density \nand dense residential, buildings and storage tanks) while Ienko et al. (Ienco, Gaetano, \nDupaquier, & Maurel, 2017) found that tree crops, summer crops and truck farming were \nclasses highly mixed. A confusion between maize and soybeans was evident in (Kussul, \nLavreniuk, Skakun, & Shelestov, 2017) and variation was low in botanically related crops, \nsuch as meadow, fallow, triticale, wheat, and rye (Rußwurm & Körner, 2017). Moreover, \nsome particular views of the plants (i.e. flowers and leaf scans) offer different classification \naccuracy than branches, stems and photos of the entire plant. A serious issue in plant \nphenology recognition is the fact that appearances change very gradually and it is \nchallenging to distinguish images falling into the growing durations that are in the middle of \ntwo successive stages (Yalcin, 2017 ), (Namin, Esmaeilzadeh, Najafi, Brown, & Borevitz, \n2017). A similar issue appears when assessing the quality of vegetative development \n(Minh, et al., 2017). Furthermore, in the challenging problem of fruit counting, the models \nsuffer from high occlusion, depth variation, and uncontrolled illumination, including high \ncolor similarity between fruit/foliage (Chen, et al., 2017), (Bargoti & Underwood, 2016). \nFinally, identification of weeds faces issues with respect to lighting, resolution, and soil \ntype, and small variation between weeds and crops in shape, texture, color and position \n(i.e. overlapping) (Dyrmann, Karstoft, & Midtiby, 2016 ), (Xinshao & Cheng, 2015), \n(Dyrmann, Jørgensen, & Midtiby, 2017). In the large majority of the papers mentioned \nabove (except from (Minh, et al., 2017)), this low variation has affected classification \n                                                 \n3 Classification accuracy is defined in Section 4.7 and Table 1. \n12 \n \naccuracy significantly, i.e. more than 5%. \n4.4 Data Pre-Processing \nThe large majority of related work (36 papers, 90%) involved some image pre-processing \nsteps, before the image or particular characteristics/features/statistics of the image were \nfed as an input to the DL model. The most common pre-processing procedure was image \nresize (16 papers), in most cases to a smaller size, in order to adapt to the requirements of \nthe DL model. Sizes of 256x256, 128x128, 96x96 and 60x60 pixels were common. Image \nsegmentation was also a popular practice (12 papers), either to increase the size of the \ndataset (Ienco, Gaetano, Dupaquier, & Maurel, 2017), (Rebetez, J., et al., 2016), (Yalcin, \n2017 ) or to facilitate the learning process by highlighting regions of interest (Sladojevic, \nArsenovic, Anderla, Culibrk, & Stefanovic, 2016), (Mohanty, Hughes, & Salathé, 2016), \n(Grinblat, Uzal, Larese, & Granitto, 2016), (Sa, et al., 2016), (Dyrmann, Karstoft, & Midtiby, \n2016 ), (Potena, Nardi, & Pretto, 2016) or to enable easier data annotation by experts and \nvolunteers (Chen, et al., 2017), (Bargoti & Underwood, 2016). Background removal \n(Mohanty, Hughes, & Salathé, 2016), (McCool, Perez, & Upcroft, 2017), (Milioto, Lottes, & \nStachniss, 2017), foreground pixel extraction (Lee, Chan, Wilkin, & Remagnino, 2015) or \nnon-green pixels removal based on NDVI masks (Dyrmann, Karstoft, & Midtiby, 2016 ), \n(Potena, Nardi, & Pretto, 2016) were also performed to reduce the datasets’ overall noise. \nOther operations involved the creation of bounding boxes (Chen, et al., 2017), (Sa, et al., \n2016), (McCool, Perez, & Upcroft, 2017), (Milioto, Lottes, & Stachniss, 2017) to facilitate \ndetection of weeds or counting of fruits. Some datasets were converted to grayscale \n(Santoni, Sensuse, Arymurthy, & Fanany, 2015), (Amara, Bouaziz, & Algergawy, 2017) or \nto the HSV color model (Luus, Salmon, van den Bergh, & Maharaj, 2015), (Lee, Chan, \nWilkin, & Remagnino, 2015). \nFurthermore, some papers used features extracted from the images as input to their \nmodels, such as shape and statistical features (Hall, McCool, Dayoub, Sunderhauf, & \n13 \n \nUpcroft, 2015), histograms (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015), (Xinshao \n& Cheng, 2015), (Rebetez, J., et al., 2016), Principal Component Analysis (PCA) filters \n(Xinshao & Cheng, 2015), Wavelet transformations (Kuwata & Shibasaki, 2015) and Gray \nLevel Co-occurrence Matrix (GLCM) features (Santoni, Sensuse, Arymurthy, & Fanany, \n2015). Satellite or aerial images involved a combination of pre-processing steps such as \northorectification (Lu, et al., 2017), (Minh, et al., 2017) calibration and terrain correction \n(Kussul, Lavreniuk, Skakun, & Shelestov, 2017), (Minh, et al., 2017) and atmospheric \ncorrection (Rußwurm & Körner, 2017).  \n4.5 Data Augmentation \nIt is worth-mentioning that some of the related work under study (15 papers, 37%) \nemployed data augmentation techniques (Krizhevsky, Sutskever, & Hinton, 2012), to \nenlarge artificially their number of training images. This helps to improve the overall \nlearning procedure and performance, and for generalization purposes, by means of \nfeeding the model with varied data. This augmentation process is important for papers that \npossess only small datasets to train their DL models, such as (Bargoti & Underwood, \n2016), (Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016), (Sørensen, \nRasmussen, Nielsen, & Jørgensen, 2017), (Mortensen, Dyrmann, Karstoft, Jørgensen, & \nGislum, 2016), (Namin, Esmaeilzadeh, Najafi, Brown, & Borevitz, 2017) and (Chen, et al., \n2017). This process was especially important in papers where the authors trained their \nmodels using synthetic images and tested them on real ones (Rahnemoonfar & Sheppard, \n2017) and (Dyrmann, Mortensen, Midtiby, & Jørgensen, 2016). In this case, data \naugmentation allowed their models to generalize and be able to adapt to the real-world \nproblems more easily. \nTransformations are label-preserving, and included rotations (12 papers), dataset \npartitioning/cropping (3 papers), scaling (3 papers), transposing (Sørensen, Rasmussen, \nNielsen, & Jørgensen, 2017), mirroring (Dyrmann, Karstoft, & Midtiby, 2016 ), translations \n14 \n \nand perspective transform (Sladojevic, Arsenovic, Anderla, Culibrk, & Stefanovic, 2016), \nadaptations of objects’ intensity in an object detection problem (Steen, Christiansen, \nKarstoft, & Jørgensen, 2016) and a PCA augmentation technique (Bargoti & Underwood, \n2016). \nPapers involving simulated data performed additional augmentation techniques such as \nvarying the HSV channels and adding random shadows (Dyrmann, Mortensen, Midtiby, & \nJørgensen, 2016) or adding simulated roots to soil images (Douarre, Schielein, Frindel, \nGerth, & Rousseau, 2016). \n4.6 Technical Details \nFrom a technical side, almost half of the research works (17 papers, 42%) employed \npopular CNN architectures such as AlexNet, VGG16 and Inception-ResNet. From the rest, \n14 papers developed their own CNN models, 2 papers adopted a first-order Differential \nRecurrent Neural Networks (DRNN) model, 5 papers preferred to use a Long Short-Term \nMemory (LSTM) model (Gers, Schmidhuber, & Cummins, 2000), one paper used deep \nbelief networks (DBN) and one paper employed a hybrid of PCA with auto-encoders (AE). \nSome of the CNN approaches combined their model with a classifier at the output layer, \nsuch as logistic regression (Chen, Lin, Zhao, Wang, & Gu, 2014), Scalable Vector \nMachines (SVM) (Douarre, Schielein, Frindel, Gerth, & Rousseau, 2016), linear regression \n(Chen, et al., 2017), Large Margin Classifiers (LCM) (Xinshao & Cheng, 2015) and \nmacroscopic cellular automata (Song, et al., 2016). \nRegarding the frameworks used, all the works that employed some well-known CNN \narchitecture had also used a DL framework, with Caffe being the most popular (13 papers, \n32%), followed by Tensor Flow (2 papers) and deeplearning4j (1 paper). Ten research \nworks developed their own software, while some authors decided to build their own \nmodels on top of Caffe (5 papers), Keras/Theano (5 papers), Keras/TensorFlow (4 \npapers), Pylearn2 (1 paper), MatConvNet (1 paper) and Deep Learning Matlab Toolbox (1 \n15 \n \npaper). A possible reason for the wide use of Caffe is that it incorporates various CNN \nframeworks and datasets, which can be used then easily and automatically by its users. \nMost of the studies divided their dataset between training and testing/verification data \nusing a ratio of 80-20 or 90-10 respectively. In addition, various learning rates have been \nrecorded, from 0.001 (Amara, Bouaziz, & Algergawy, 2017) and 0.005 (Mohanty, Hughes, \n& Salathé, 2016) up to 0.01 (Grinblat, Uzal, Larese, & Granitto, 2016). Learning rate is \nabout how quickly a network learns. Higher values help avoid the solver being stuck in \nlocal minima, which can reduce performance significantly. A general approach used by \nmany of the evaluated papers is to start out with a high learning rate and lower it as the \ntraining goes on. We note that learning rate is very dependent on the network architecture. \nMoreover, most of the research works that incorporated popular DL architectures took \nadvantage of transfer learning (Pan & Yang, 2010), which concerns leveraging the already \nexisting knowledge of some related task or domain in order to increase the learning \nefficiency of the problem under study by fine-tuning pre-trained models. As sometimes it is \nnot possible to train a network from scratch due to having a small training data set or \nhaving a complex multi-task network, it is required that the network be at least partially \ninitialized with weights from another pre-trained model. A common transfer learning \ntechnique is the use of pre-trained CNN, which are CNN models that have been already \ntrained on some relevant dataset with possibly different number of classes. These models \nare then adapted to the particular challenge and dataset. This method was followed \n(among others) in (Lu, et al., 2017), (Douarre, Schielein, Frindel, Gerth, & Rousseau, \n2016), (Reyes, Caicedo, & Camargo, 2015), (Bargoti & Underwood, 2016), (Steen, \nChristiansen, Karstoft, & Jørgensen, 2016), (Lee, Chan, Wilkin, & Remagnino, 2015), (Sa, \net al., 2016), (Mohanty, Hughes, & Salathé, 2016), (Christiansen, Nielsen, Steen, \nJørgensen, & Karstoft, 2016), (Sørensen, Rasmussen, Nielsen, & Jørgensen, 2017), for \nthe VGG16, DenseNet, AlexNet and GoogleNet architectures. \n16 \n \n4.7 Outputs \nFinally, concerning the 31 papers that involved classification, the classes as used by the \nauthors ranged from 2 (Lu, et al., 2017), (Pound, M. P., et al., 2016), (Douarre, Schielein, \nFrindel, Gerth, & Rousseau, 2016), (Milioto, Lottes, & Stachniss, 2017) up to 1,000 \n(Reyes, Caicedo, & Camargo, 2015). A large number of classes was observed in (Luus, \nSalmon, van den Bergh, & Maharaj, 2015) (21 land-use classes), (Rebetez, J., et al., \n2016) (22 different crops plus soil), (Lee, Chan, Wilkin, & Remagnino, 2015) (44 plant \nspecies) and (Xinshao & Cheng, 2015) (91 classes of common weeds found in agricultural \nfields). In these papers, the number of outputs of the model was equal to the number of \nclasses respectively. Each output was a different probability for the input image, segment, \nblob or pixel to belong to each class, and then the model picked the highest probability as \nits predicted class. \nFrom the rest 9 papers, 2 performed predictions of fruits counted (scalar value as output) \n(Rahnemoonfar & Sheppard, 2017), (Chen, et al., 2017), 2 identified regions of fruits in the \nimage (multiple bounding boxes) (Bargoti & Underwood, 2016), (Sa, et al., 2016), 2 \npredicted animal growth (scalar value) (Demmers T. G., et al., 2010), (Demmers T. G., \nCao, Parsons, Gauss, & Wathes, 2012), one predicted weather conditions (scalar value) \n(Sehgal, et al., 2017), one crop yield index (scalar value) (Kuwata & Shibasaki, 2015) and \none paper predicted percentage of soil moisture content (scalar value) (Song, et al., 2016). \n4.8 Performance Metrics \nRegarding methods used to evaluate performance, various metrics have been employed \nby the authors, each being specific to the model used at each study. Table 1 lists these \nmetrics, together with their definition/description, and the symbol we use to refer to them in \nthis survey. In some papers where the authors referred to accuracy without specifying its \ndefinition, we assumed they referred to classification accuracy (CA, first metric listed in \nTable 1). From this point onwards, we refer to “DL performance” as its score in some \n17 \n \nperformance metric from the ones listed in Table 1. \nTable 1: Performance metrics used in related work under study. \nNo. Performance \nMetric \nSymbol \nUsed \nDescription \n1. \nClassification \nAccuracy \nCA \nThe percentage of correct predictions where the top class (the one \nhaving the highest probability), as indicated by the DL model, is the \nsame as the target label as annotated beforehand by the authors. For \nmulti-class classification problems, CA is averaged among all the \nclasses. CA is mentioned as Rank-1 identification rate in (Hall, \nMcCool, Dayoub, Sunderhauf, & Upcroft, 2015). \n2. \nPrecision \nP \nThe fraction of true positives (TP, correct predictions) from the total \namount of relevant results, i.e. the sum of TP and false positives (FP). \nFor multi-class classification problems, P is averaged among the \nclasses. P=TP/(TP+FP) \n3. \nRecall \nR \nThe fraction of TP from the total amount of TP and false negatives \n(FN). For multi-class classification problems, R gets averaged among \nall the classes. R=TP/(TP+FN) \n4. \nF1 score \nF1 \nThe harmonic mean of precision and recall. For multi-class \nclassification problems, F1 gets averaged among all the classes. It is \nmentioned as F-measure in (Minh, et al., 2017).  \nF1=2 * (TP*FP) / (TP+FP) \n5. \nLifeCLEF metric \nLC \nA score4 related to the rank of the correct species in the list of retrieved \nspecies \n6. \nQuality Measure \nQM \nObtained by multiplying sensitivity (proportion of pixels that were \ndetected correctly) and specificity (which proportion of detected pixels \nare truly correct). QM=TP2 / ((TP+FP)(TP+FN)) \n7. \nMean Square Error \nMSE \nMean of the square of the errors between predicted and observed \nvalues. \n8. \nRoot Mean Square \nError \nRMSE \nStandard deviation of the differences between predicted values and \nobserved values. A normalized RMSE (N-RMSE) has been used in \n(Sehgal, et al., 2017). \n9. \nMean Relative \nError \nMRE \nThe mean error between predicted and observed values, in \npercentage. \n10. Ratio of total fruits \ncounted \nRFC \nRatio of the predicted count of fruits by the model, with the actual \ncount. The actual count was attained by taking the average count of \nindividuals (i.e. experts or volunteers) observing the images \nindependently. \n11. L2 error \nL2 \nRoot of the squares of the sums of the differences between predicted \ncounts of fruits by the model and the actual counts. \n12. Intersection over \nUnion \nIoU \nA metric that evaluates predicted bounding boxes, by dividing the area \nof overlap between the predicted and the ground truth boxes, by the \narea of their union. An average (Dyrmann, Mortensen, Midtiby, & \nJørgensen, 2016) or frequency weighted (Mortensen, Dyrmann, \nKarstoft, Jørgensen, & Gislum, 2016) IoU can be calculated. \n13. CA-IoU, F1-IoU,  \nCA-IoU These are the same CA, F1, P and R metrics as defined above, \n                                                 \n4 LifeCLEF 2015 Challenge. http://www.imageclef.org/lifeclef/2015/plant  \n18 \n \nP-IoU or R-IoU \nF1-IoU  \nP-IoU  \nR-IoU \ncombined with IoU in order to consider true/false positives/negatives. \nUsed in problems involving bounding boxes. This is done by putting a \nminimum threshold on IoU, i.e. any value above this threshold would \nbe considered as positive by the metric (and the model involved). \nThresholds of 20% (Bargoti & Underwood, 2016), 40% (Sa, et al., \n2016) and 50% (Steen, Christiansen, Karstoft, & Jørgensen, 2016), \n(Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016), \n(Dyrmann, Jørgensen, & Midtiby, 2017) have been observed5. \n \nCA was the most popular metric used (24 papers, 60%), followed by F1 (10 papers, 25%). \nSome papers included RMSE (4 papers), IoU (3 papers), RFC (Chen, et al., 2017), \n(Rahnemoonfar & Sheppard, 2017) or others. Some works used a combination of metrics \nto evaluate their efforts. We note that some papers employing CA, F1, P or R, used IoU in \norder to consider a model’s prediction (Bargoti & Underwood, 2016), (Sa, et al., 2016), \n(Steen, Christiansen, Karstoft, & Jørgensen, 2016), (Christiansen, Nielsen, Steen, \nJørgensen, & Karstoft, 2016), (Dyrmann, Jørgensen, & Midtiby, 2017). In these cases, a \nminimum threshold was put on IoU, and any value above this threshold would be \nconsidered as positive by the model.  \nWe note that in some cases, a trade-off can exist between metrics. For example, in a \nweed detection problem (Milioto, Lottes, & Stachniss, 2017), it might be desirable to have \na high R to eliminate most weeds, but not eliminating crops is of a critical importance, \nhence a lower P might be acceptable. \n4.9 Overall Performance \nWe note that it is difficult if not impossible to compare between papers, as different metrics \nare employed for different tasks, considering different models, datasets and parameters. \nHence, the reader should consider our comments in this section with some caution. \nIn 19 out of the 24 papers that involved CA as a metric, accuracy was high (i.e. above \n90%), indicating good performance. The highest CA has been observed in the works of \n(Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015), (Pound, M. P., et al., 2016), (Chen, \n                                                 \n5 In Appendix II, where we list the values of the metrics used at each paper, we denote CA-IoU(x), F1-IoU(x), \nP-IoU(x) or R-IoU(x), where x is the threshold (in percentage), over which results are considered as positive \nby the DL model employed. \n19 \n \nLin, Zhao, Wang, & Gu, 2014), (Lee, Chan, Wilkin, & Remagnino, 2015), (Minh, et al., \n2017), (Potena, Nardi, & Pretto, 2016) and (Steen, Christiansen, Karstoft, & Jørgensen, \n2016), with values of 98% or more, constituting remarkable results. From the 10 papers \nusing F1 as metric, 5 had values higher than 0.90 with the highest F1 observed in \n(Mohanty, Hughes, & Salathé, 2016) and (Minh, et al., 2017) with values higher than 0.99. \nThe works of (Dyrmann, Karstoft, & Midtiby, 2016 ), (Rußwurm & Körner, 2017), (Ienco, \nGaetano, Dupaquier, & Maurel, 2017), (Mortensen, Dyrmann, Karstoft, Jørgensen, & \nGislum, 2016), (Rebetez, J., et al., 2016), (Christiansen, Nielsen, Steen, Jørgensen, & \nKarstoft, 2016) and (Yalcin, 2017 ) were among the ones with the lowest CA (i.e. 73-79%) \nand/or F1 scores (i.e. 0.558 - 0.746), however state of the art work in these particular \nproblems has shown lower CA (i.e. SVM, RF, Naïve- Bayes classifier). Particularly in \n(Rußwurm & Körner, 2017), the three-unit LSTM model employed provided 16.3% better \nCA than a CNN, which belongs to the family of DL. Besides, the above can be considered \nas “harder” problems, because of the use of satellite data (Ienco, Gaetano, Dupaquier, & \nMaurel, 2017), (Rußwurm & Körner, 2017) large number of classes (Dyrmann, Karstoft, & \nMidtiby, 2016 ), (Rußwurm & Körner, 2017), (Rebetez, J., et al., 2016), small training \ndatasets (Mortensen, Dyrmann, Karstoft, Jørgensen, & Gislum, 2016), (Christiansen, \nNielsen, Steen, Jørgensen, & Karstoft, 2016) or very low variation among the classes \n(Yalcin, 2017 ), (Dyrmann, Karstoft, & Midtiby, 2016 ), (Rebetez, J., et al., 2016).  \n4.10 Generalizations on Different Datasets \nIt is important to examine whether the authors had tested their implementations on the \nsame dataset (e.g. by dividing the dataset into training and testing/validation sets) or used \ndifferent datasets to test their solution. From the 40 papers, only 8 (20%) used different \ndatasets for testing than the one for training. From these, 2 approaches trained their \nmodels by using simulated data and tested on real data (Dyrmann, Mortensen, Midtiby, & \nJørgensen, 2016), (Rahnemoonfar & Sheppard, 2017) and 2 papers tested their models \n20 \n \non a dataset produced 2-4 weeks after, with a more advanced growth stage of plants and \nweeds (Milioto, Lottes, & Stachniss, 2017), (Potena, Nardi, & Pretto, 2016). Moreover, 3 \npapers used different fields for testing than the ones used for training (McCool, Perez, & \nUpcroft, 2017), with a severe degree of occlusion compared to the other training field \n(Dyrmann, Jørgensen, & Midtiby, 2017), or containing other obstacles such as people and \nanimals (Steen, Christiansen, Karstoft, & Jørgensen, 2016). Sa et al. (Sa, et al., 2016) \nused a different dataset to evaluate whether the model can generalize on different fruits. \nFrom the other 32 papers, different trees were used in training and testing in (Chen, et al., \n2017), while different rooms for pigs (Demmers T. G., Cao, Parsons, Gauss, & Wathes, \n2012) and chicken (Demmers T. G., et al., 2010) were considered. Moreover, Hall et al. \napplied condition variations in testing (i.e. translations, scaling, rotations, shading and \nocclusions) (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015) while scaling for a \ncertain range translation distance and rotation angle was performed on the testing dataset \nin (Xinshao & Cheng, 2015). The rest 27 papers did not perform any changes between the \ntraining/testing datasets, a fact that lowers the overall confidence for the results presented. \nFinally, it is interesting to observe how these generalizations affected the performance of \nthe models, at least in cases where both data from same and different datasets were used \nin testing. In (Sa, et al., 2016), F1-IoU(40) was higher for the detection of apples (0.938), \nstrawberry (0.948), avocado (0.932) and mango (0.942), than in the default case of sweet \npepper (0.838). In (Rahnemoonfar & Sheppard, 2017), RFC was 2% less in the real \nimages than in the synthetic ones. In (Potena, Nardi, & Pretto, 2016), CA was 37.6% less \nat the dataset involving plants of 4-weeks more advanced growth. According to the \nauthors, the model was trained based on plants that were in their first growth stage, thus \nwithout their complete morphological features, which were included in the testing dataset. \nMoreover, in (Milioto, Lottes, & Stachniss, 2017) P was 2% higher at the 2-weeks more \nadvanced growth dataset, with 9% lower R.  \n21 \n \nHence, in the first case there was improvement in performance (Sa, et al., 2016), and in \nthe last three cases a reduction, slight one in (Rahnemoonfar & Sheppard, 2017) and \n(Milioto, Lottes, & Stachniss, 2017) but considerable in (Potena, Nardi, & Pretto, 2016). \nFrom the other papers using different testing datasets, as mentioned above, high \npercentages of CA (94-97.3%), P-IoU (86.6%) and low values of MRE (1.8 -10%) have \nbeen reported. These show that the DL models were able to generalize well to different \ndatasets. However, without more comparisons, this is only a speculation that can be \nfigured out of the small number of observations available. \n4.11 Performance Comparison with Other Approaches \nA critical aspect of this survey is to examine how DL performs in relation to other existing \ntechniques. The 14th column of Appendix II presents whether the authors of related work \ncompared their DL-based approach with other techniques used for solving their problem \nunder study. We focus only on comparisons between techniques used for the same \ndataset at the same research paper, with the same metric. \nIn almost all cases, the DL models outperform other approaches implemented for \ncomparison purposes. CNN show 1-8% higher CA in comparison to SVM (Chen, Lin, \nZhao, Wang, & Gu, 2014), (Lee, Chan, Wilkin, & Remagnino, 2015), (Grinblat, Uzal, \nLarese, & Granitto, 2016), (Pound, M. P., et al., 2016), 41% improvement of CA when \ncompared to ANN (Lee, Chan, Wilkin, & Remagnino, 2015) and 3-8% higher CA when \ncompared to RF (Kussul, Lavreniuk, Skakun, & Shelestov, 2017), (Minh, et al., 2017), \n(McCool, Perez, & Upcroft, 2017), (Potena, Nardi, & Pretto, 2016), (Hall, McCool, Dayoub, \nSunderhauf, & Upcroft, 2015). CNN also seem to be superior than unsupervised feature \nlearning with 3-11% higher CA (Luus, Salmon, van den Bergh, & Maharaj, 2015), 2-44% \nimproved CA in relation to local shape and color features (Dyrmann, Karstoft, & Midtiby, \n2016 ), (Sørensen, Rasmussen, Nielsen, & Jørgensen, 2017), and 2% better CA (Kussul, \nLavreniuk, Skakun, & Shelestov, 2017) or 18% less RMSE (Song, et al., 2016) compared \n22 \n \nto multilayer perceptrons. CNN had also superior performance than Penalized \nDiscriminant Analysis (Grinblat, Uzal, Larese, & Granitto, 2016), SVM Regression (Kuwata \n& Shibasaki, 2015), area-based techniques (Rahnemoonfar & Sheppard, 2017), texture-\nbased regression models (Chen, et al., 2017), LMC classifiers (Xinshao & Cheng, 2015), \nGaussian Mixture Models (Santoni, Sensuse, Arymurthy, & Fanany, 2015) and Naïve-\nBayes classifiers (Yalcin, 2017 ). \nIn cases where Recurrent Neural Networks (RNN) (Mandic & Chambers, 2001) \narchitectures were employed, the LSTM model had 1% higher CA than RF and SVM in \n(Ienco, Gaetano, Dupaquier, & Maurel, 2017), 44% improved CA than SVM in (Rußwurm \n& Körner, 2017) and 7-9% better CA than RF and SVM in (Minh, et al., 2017). \nIn only one case, DL showed worse performance against another technique, and this was \nwhen a CNN was compared to an approach involving local descriptors to represent \nimages together with KNN as the classification strategy (20% worse LC) (Reyes, Caicedo, \n& Camargo, 2015). \n \n5. Discussion \nOur analysis has shown that DL offers superior performance in the vast majority of related \nwork. When comparing the performance of DL-based approaches with other techniques at \neach paper, it is of paramount importance to adhere to the same experimental conditions \n(i.e. datasets and performance metrics). From the related work under study, 28 out of the \n40 papers (70%) performed direct, valid and correct comparisons among the DL-based \napproach employed and other state-of-art techniques used to solve the particular problem \ntackled at each paper. Due to the fact that each paper involved different datasets, pre-\nprocessing techniques, metrics, models and parameters, it is difficult if not impossible to \ngeneralize and perform comparisons between papers. Thus, our comparisons have been \nstrictly limited among the techniques used at each paper. Thus, based on these \n23 \n \nconstraints, we have observed that DL has outperformed traditional approaches used such \nas SVM, RF, ANN, LMC classifiers and others. It seems that the automatic feature \nextraction performed by DL models is more effective than the feature extraction process \nthrough traditional approaches such as Scale Invariant Feature Transform (SIFT), GLCM, \nhistograms, area-based techniques (ABT), statistics-, texture-, color- and shape-based \nalgorithms, conditional random fields to model color and visual texture features, local de-\ncorrelated channel features and other manual feature extraction techniques. This is \nreinforced by the combined CNN+LSTM model employed in (Namin, Esmaeilzadeh, Najafi, \nBrown, & Borevitz, 2017), which outperformed a LSTM model which used hand crafted \nfeature descriptors as inputs by 25% higher CA. Interesting attempts to combine hand-\ncrafted features and CNN-based features were performed in (Hall, McCool, Dayoub, \nSunderhauf, & Upcroft, 2015) and (Rebetez, J., et al., 2016). \nAlthough DL has been associated with computer vision and image analysis (which is also \nthe general case in this survey), we have observed 5 related works where DL-based \nmodels have been trained based on field sensory data (Kuwata & Shibasaki, 2015), \n(Sehgal, et al., 2017) and a combination of static and dynamic environmental variables \n(Song, et al., 2016), (Demmers T. G., et al., 2010), (Demmers T. G., Cao, Parsons, Gauss, \n& Wathes, 2012). These papers indicate the potential of DL to be applied in a wide variety \nof agricultural problems, not only those involving images.  \nExamining agricultural areas where DL techniques have been applied, leaf classification, \nleaf and plant disease detection, plant recognition and fruit counting have some papers \nwhich present very good performance (i.e. CA > 95%, F1 > 0.92 or RFC > 0.9). This is \nprobably because of the availability of datasets in these domains, as well as the distinct \ncharacteristics of (sick) leaves/plants and fruits in the image. On the other hand, some \npapers in land cover classification, crop type classification, plant phenology recognition \nand weed detection showed average performance (i.e. CA < 87% or F1 < 0.8). This could \n24 \n \nbe due to leaf occlusion in weed detection, use of noise-prone satellite imagery in land \ncover problems, crops with low variation and botanical relationship or the fact that \nappearances change very gradually while plants grow in phenology recognition efforts. \nWithout underestimating the quality of any of the surveyed papers, we highlight some that \nclaim high performance (CA > 91%, F1-IoU(20) > 0.90 or RFC > 0.91), considering the \ncomplexity of the problem in terms of its definition or the large number of classes involved \n(more than 21 classes). These papers are the following: (Mohanty, Hughes, & Salathé, \n2016), (Luus, Salmon, van den Bergh, & Maharaj, 2015), (Lee, Chan, Wilkin, & \nRemagnino, 2015), (Rahnemoonfar & Sheppard, 2017), (Chen, et al., 2017), (Bargoti & \nUnderwood, 2016), (Xinshao & Cheng, 2015) and (Hall, McCool, Dayoub, Sunderhauf, & \nUpcroft, 2015). We also highlight papers that trained their models on simulated data, and \ntested them on real data, which are (Dyrmann, Mortensen, Midtiby, & Jørgensen, 2016), \n(Rahnemoonfar & Sheppard, 2017), and (Douarre, Schielein, Frindel, Gerth, & Rousseau, \n2016). These works constitute important efforts in the DL community, as they attempt to \nsolve the problem of inexistent or not large enough datasets in various problems. \nFinally, as discussed in Section 4.10, most authors used the same datasets for training \nand testing their implementation, a fact that lowers the confidence in the overall findings, \nalthough there have been indications that the models seem to generalize well, with only \nsmall reductions in performance. \n5.1 Advanced Deep Learning Applications \nAlthough the majority of papers used typical CNN architectures to perform classification \n(23 papers, 57%), some authors experimented with more advanced models in order to \nsolve more complex problems, such as crop type classification from UAV imagery (CNN + \nHistNN using RGB histograms) (Rebetez, J., et al., 2016), estimating number of tomato \nfruits (Modified Inception-ResNet CNN) (Rahnemoonfar & Sheppard, 2017) and estimating \nnumber of orange or apple fruits (CNN adapted for blob detection and counting + Linear \n25 \n \nRegression) (Chen, et al., 2017). Particularly interesting were the approaches employing \nthe Faster Region-based CNN + VGG16 model (Bargoti & Underwood, 2016), (Sa, et al., \n2016), in order not only to count fruits and vegetables, but also to locate their placement in \nthe image by means of bounding boxes. Similarly, the work in (Dyrmann, Jørgensen, & \nMidtiby, 2017) used the DetectNet CNN to detect bounding boxes of weed instances in \nimages of cereal fields. These approaches (Faster Region-based CNN, DetectNet CNN) \nconstitute a very promising research direction, since the task of identifying the bounding \nbox of fruits/vegetables/weeds in an image has numerous real-life applications and could \nsolve various agricultural problems \nMoreover, considering not only space but also time series, some authors employed RNN-\nbased models in land cover classification (one-unit LSTM model + SVM) (Ienco, Gaetano, \nDupaquier, & Maurel, 2017), crop type classification (three-unit LSTM) (Rußwurm & \nKörner, 2017), classification of different accessions of Arabidopsis thaliana based on \nsuccessive top-view images (CNN+ LSTM) (Namin, Esmaeilzadeh, Najafi, Brown, & \nBorevitz, 2017), mapping winter vegetation quality coverage (Five-unit LSTM, Gated \nRecurrent Unit) (Minh, et al., 2017), estimating the weight of pigs or chickens (DRNN) \n(Demmers T. G., et al., 2010), (Demmers T. G., Cao, Parsons, Gauss, & Wathes, 2012) \nand for predicting weather based on previous year’s conditions (LSTM) (Sehgal, et al., \n2017). RNN-based models offer higher performance, as they can capture the time \ndimension, which is impossible to be exploited by simple CNN. RNN architectures tend to \nexhibit dynamic temporal behavior, being able to record long-short temporal \ndependencies, remembering and forgetting after some time or when needed (i.e. LSTM). \nDifferences in performance between RNN and CNN are distinct in the related work under \nstudy, as shown in Table 2. This 16% improvement in CA could be attributed to the \nadditional information provided by the time series. For example, in the crop type \nclassification case (Rußwurm & Körner, 2017), the authors mention, “crops change their \n26 \n \nspectral characteristics due to environmental influences and can thus not be monitored \neffectively with classical mono-temporal approaches. Performance of temporal models \nincreases at the beginning of vegetation period”. LSTM-based approaches work well also \nfor low represented and difficult classes, as demonstrated in (Ienco, Gaetano, Dupaquier, \n& Maurel, 2017). \nTable 2: Difference in Performance between CNN and RNN. \nNo. Application in \nAgriculture \nPerforman\nce Metric \nDifference in Performance \nReference \n1. \nCrop type classification \nconsidering time series \nCA, F1 \nThree-unit LSTM: 76.2% (CA), \n0.558  (F1) \nCNN: 59.9% (CA), 0.236 (F1) \n(Rußwurm & Körner, 2017) \n2. \nClassify the phenotyping \nof Arabidopsis in four \naccessions \nCA \nCNN+ LSTM: 93% \nCNN: 76.8% \n(Namin, Esmaeilzadeh, \nNajafi, Brown, & Borevitz, \n2017) \n \nFinally, the critical aspect of fast processing of DL models in order to be easily used in \nrobots for real-time decision making (e.g. detection of weeds) was examined in (McCool, \nPerez, & Upcroft, 2017), and it is worth-mentioning. The authors have showed that a \nlightweight implementation had only a small penalty in CA (3.90%), being much faster (i.e. \nprocessing of 40.6 times more pixels per second). They proposed the idea of “teacher and \nstudent networks”, where the teacher is the more heavy approach that helps the student \n(light implementation) to learn faster and better. \n5.2 Advantages of Deep Learning \nExcept from improvements in performance of the classification/prediction problems in the \nsurveyed works (see Sections 4.9 and 4.11), the advantage of DL in terms of reduced \neffort in feature engineering was demonstrated in many of the papers. Hand-engineered \ncomponents require considerable time, an effort that takes place automatically in DL. \nBesides, sometimes manual search for good feature extractors is not an easy and obvious \ntask. For example, in the case of estimating crop yield (Kuwata & Shibasaki, 2015), \nextracting manually features that significantly affected crop growth was not possible. This \n27 \n \nwas also the case of estimating the soil moisture content (Song, et al., 2016). \nMoreover, DL models seem to generalize well. For example, in the case of fruit counting, \nthe model learned explicitly to count (Rahnemoonfar & Sheppard, 2017). In the banana \nleaf classification problem (Amara, Bouaziz, & Algergawy, 2017), the model was robust \nunder challenging conditions such as illumination, complex background, different \nresolution, size and orientation of the images. Also in the fruits counting papers (Chen, et \nal., 2017), (Rahnemoonfar & Sheppard, 2017), the models were robust to occlusion, \nvariation, illumination and scale. The same detection frameworks could be used for a \nvariety of circular fruits such as peaches, citrus, mangoes etc. As another example, a key \nfeature of the DeepAnomaly model was the ability to detect unknown objects/anomalies \nand not just a set of predefined objects, exploiting the homogeneous characteristics of an \nagricultural field to detect distant, heavy occluded and unknown objects (Christiansen, \nNielsen, Steen, Jørgensen, & Karstoft, 2016). Moreover, in the 8 papers mentioned in \nSection 4.10 where different datasets were used for testing, the performance of the model \nwas generally high, with only small reductions in performance in comparison with the \nperformance when using the same dataset for training and testing. \nAlthough DL takes longer time to train than other traditional approaches (e.g. SVM, RF), its \ntesting time efficiency is quite fast. For example, in detecting obstacles and anomaly \n(Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016), the model took much longer \nto train, but after it did, its testing time was less than the one of SVM and KNN. Besides, if \nwe take into account the time needed to manually design filters and extract features, “the \ntime used on annotating images and training the CNN becomes almost negligible” \n(Sørensen, Rasmussen, Nielsen, & Jørgensen, 2017).  \nAnother advantage of DL is the possibility to develop simulated datasets to train the \nmodel, which could be properly designed in order to solve real-world problems. For \nexample, in the issue of detecting weeds and maize in fields (Dyrmann, Mortensen, \n28 \n \nMidtiby, & Jørgensen, 2016), the authors overcame the plant foliage overlapping problem \nby simulating top-down images of overlapping plants on soil background. The trained \nnetwork was then capable of distinguish weeds from maize even in overlapping conditions. \n5.3 Disadvantages and Limitations of Deep Learning \nA considerable drawback and barrier in the use of DL is the need of large datasets, which \nwould serve as the input during the training procedure. In spite of data augmentation \ntechniques which augment some dataset with label-preserving transformations, in reality at \nleast some hundreds of images are required, depending on the complexity of the problem \nunder study (i.e. number of classes, precision required etc.). For example, the authors in \n(Mohanty, Hughes, & Salathé, 2016) and (Sa, et al., 2016) commented that a more diverse \nset of training data was needed to improve CA. A big problem with many datasets is the \nlow variation among the different classes (Yalcin, 2017 ), as discussed in Section 4.3, or \nthe existence of noise, in the form of low resolution, inaccuracy of sensory equipment \n(Song, et al., 2016), crops’ occlusions, plants overlapping and clustering, and others. \nAs data annotation is a necessary operation in the large majority of cases, some tasks are \nmore complex and there is a need for experts (who might be difficult to involve) in order to \nannotate input images. As mentioned in (Amara, Bouaziz, & Algergawy, 2017), there is a \nlimited availability of resources and expertise on banana pathology worldwide. In some \ncases, experts or volunteers are susceptible to errors during data labeling, especially when \nthis is a challenging task e.g. fruit count (Chen, et al., 2017), (Bargoti & Underwood, 2016) \nor to determine if images contain weeds or not (Sørensen, Rasmussen, Nielsen, & \nJørgensen, 2017), (Dyrmann, Jørgensen, & Midtiby, 2017). \nAnother limitation is the fact that the DL models can learn some problem particularly well, \neven generalize in some aspects as mentioned in Section 5.2, but they cannot generalize \nbeyond the “boundaries of the dataset’s expressiveness”. For example, classification of \nsingle leaves, facing up, on a homogeneous background is performed in (Mohanty, \n29 \n \nHughes, & Salathé, 2016). A real world application should be able to classify images of a \ndisease as it presents itself directly on the plant. Many diseases do not present \nthemselves on the upper side of leaves only. As another example, plant recognition in \n(Lee, Chan, Wilkin, & Remagnino, 2015) was noticeably affected by environmental factors \nsuch as wrinkled surface and insect damages. The model for counting tomatoes in \n(Rahnemoonfar & Sheppard, 2017) could count ripe and half-ripe fruits, however, “it failed \nto count green fruits because it was not trained for this purpose”. If an object size in a \ntesting image was significantly less than that of a training set, the model missed the \ndetection in (Sa, et al., 2016). Difficulty in detecting heavily occluded and distant objects \nwas observed in (Christiansen, Nielsen, Steen, Jørgensen, & Karstoft, 2016). Occlusion \nwas a serious issue also in (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015). \nA general issue in computer vision, not only in DL, is that data pre-processing is \nsometimes a necessary and time-consuming task, especially when satellite or aerial \nphotos are involved, as we saw in Section 4.4. A problem with hyperspectral data is their \nhigh dimensionality and limited training samples (Chen, Lin, Zhao, Wang, & Gu, 2014). \nMoreover, sometimes the existing datasets do not describe completely the problem they \ntarget (Song, et al., 2016). As an example, for estimating corn yield (Kuwata & Shibasaki, \n2015), it was necessary to consider also external factors other than the weather by \ninputting cultivation information such as fertilization and irrigation.  \nFinally, in the domain of agriculture, there do not exist many publicly available datasets for \nresearchers to work with, and in many cases, researchers need to develop their own sets \nof images. This could require many hours or days of work.  \n5.4 Future of Deep Learning in Agriculture \nObserving Appendix I, which lists various existing applications of computer vision in \nagriculture, we can see that only the problems of land cover classification, crop type \nestimation, crop phenology, weed detection and fruit grading have been approximated \n30 \n \nusing DL. It is interesting to see how DL would behave also in other agricultural-related \nproblems listed in Appendix I, such as seeds identification, soil and leaf nitrogen content, \nirrigation, plants’ water stress detection, water erosion assessment, pest detection, \nherbicide use, identification of contaminants, diseases or defects on food, crop hail \ndamage and greenhouse monitoring. Intuitively, since many of the aforementioned \nresearch areas employ data analysis techniques (see Appendix I) with similar concepts \nand comparable performance to DL (i.e. linear and logistic regression, SVM, KNN, K-\nmeans clustering, Wavelet-based filtering, Fourier transform) (Singh, \nGanapathysubramanian, Singh, & Sarkar, 2016), then it could be worth to examine the \napplicability of DL on these problems too. \nOther possible application areas could be the use of aerial imagery (i.e. by means of \ndrones) to monitor the effectiveness of the seeding process, to increase the quality of wine \nproduction by harvesting grapes at the right moment for best maturity levels, to monitor \nanimals and their movements to consider their overall welfare and identify possible \ndiseases, and many other scenarios where computer vision is involved.  \nIn spite of the limited availability of open datasets in agriculture, In Appendix III, we list \nsome of the most popular, free to download datasets available on the web, which could be \nused by researchers to start testing their DL architectures. These datasets could be used \nto pre-train DL models and then adapt them to more specific future agricultural challenges. \nIn addition to these datasets, remote sensing data containing multi-temporal, multi-spectral \nand multi-source images that could be used in problems related to land and crop cover \nclassification are available from satellites such as MERIS, MODIS, AVHRR, RapidEye, \nSentinel, Landsat etc. \nMore approaches adopting LSTM or other RNN models are expected in the future, \nexploiting the time dimension to perform higher performance classification or prediction. \nAn example application could be to estimate the growth of plants, trees or even animals \n31 \n \nbased on previous consecutive observations, to predict their yield, assess their water \nneeds or avoid diseases from occurring. These models could find applicability in \nenvironmental informatics too, for understanding climatic change, predicting weather \nconditions and phenomena, estimating the environmental impact of various physical or \nartificial processes (Kamilaris, Assumpcio, Blasi, Torrellas, & Prenafeta-Boldú, 2017) etc. \nRelated work under study involved up to a five-unit LSTM model (Minh, et al., 2017). We \nexpect in the future to see more layers stacked together in order to build more complex \nLSTM architectures (Ienco, Gaetano, Dupaquier, & Maurel, 2017). We also believe that \ndatasets with increasing temporal sequence length will appear, which could improve the \nperformance of LSTM (Rußwurm & Körner, 2017). \nMoreover, more complex architectures would appear, combining various DL models and \nclassifiers together, or combining hand-crafted features with automatic features extracted \nby using various techniques, fused together to improve the overall outcome, similar to \nwhat performed in (Hall, McCool, Dayoub, Sunderhauf, & Upcroft, 2015) and (Rebetez, J., \net al., 2016). Researchers are expected to test their models using more general and \nrealistic dataset, demonstrating the ability of the models to generalize to various real-world \nsituations. A combination of popular performance metrics, such as the ones mentioned in \nTable 1, are essential to be adopted by the authors for comparison purposes. It would be \ndesirable if researchers made their datasets publicly available, for use also by the general \nresearch community. \nFinally, some of the solutions discussed in the surveyed papers could have a commercial \nuse in the near future. The approaches incorporating Faster Region-based CNN and \nDetectNet CNN (Bargoti & Underwood, 2016), (Chen, et al., 2017), (Rahnemoonfar & \nSheppard, 2017) would be extremely useful for automatic robots that collect crops, remove \nweeds or for estimating the expected yields of various crops. A future application of this \ntechnique could be also in microbiology for human or animal cell counting (Chen, et al., \n32 \n \n2017). The DRNN model controlling the daily feed intake of pigs or chicken, predicting \nquite accurately the required feed intake for the whole of the growing period, would be \nuseful to farmers when deciding on a growth curve suitable for various scenarios. \nFollowing some growth patterns would have potential advantages for animal welfare in \nterms of leg health, without compromising the idea animals’ final weight and total feed \nintake requirement (Demmers T. G., et al., 2010), (Demmers T. G., Cao, Parsons, Gauss, \n& Wathes, 2012).  \n \n6. Conclusion \nIn this paper, we have performed a survey of deep learning-based research efforts applied \nin the agricultural domain. We have identified 40 relevant papers, examining the particular \narea and problem they focus on, technical details of the models employed, sources of data \nused, pre-processing tasks and data augmentation techniques adopted, and overall \nperformance according to the performance metrics employed by each paper. We have \nthen compared deep learning with other existing techniques, in terms of performance. Our \nfindings indicate that deep learning offers better performance and outperforms other \npopular image processing techniques. For future work, we plan to apply the general \nconcepts and best practices of deep learning, as described through this survey, to other \nareas of agriculture where this modern technique has not yet been adequately used. Some \nof these areas have been identified in the discussion section. \nOur aim is that this survey would motivate more researchers to experiment with deep \nlearning, applying it for solving various agricultural problems involving classification or \nprediction, related to computer vision and image analysis, or more generally to data \nanalysis. The overall benefits of deep learning are encouraging for its further use towards \nsmarter, more sustainable farming and more secure food production. \n \n33 \n \nAcknowledgments \nWe would like to thank the reviewers, whose valuable feedback, suggestions and \ncomments increased significantly the overall quality of this survey. This research has been \nsupported by the P-SPHERE project, which has received funding from the European \nUnion’s Horizon 2020 research and innovation programme under the Marie Skodowska-\nCurie grant agreement No 665919. \nReferences \nAmara, J., Bouaziz, B., & Algergawy, A. (2017). A Deep Learning-based Approach for Banana \nLeaf Diseases Classification. (págs. 79-88). Stuttgart: BTW workshop. \nBahrampour, S., Ramakrishnan, N., Schott, L., & Shah, M. (2015). Comparative study of deep \nlearning software frameworks. arXiv preprint arXiv, 1511(06435). \nBargoti, S., & Underwood, J. (2016). Deep Fruit Detection in Orchards. arXiv preprint arXiv, \n1610(03677). \nBastiaanssen, W., Molden, D., & Makin, I. (2000). Remote sensing for irrigated agriculture: \nexamples from research and possible applications. Agricultural water management, 46(2), \n137-155. \nCanziani, A., Paszke, A., & Culurciello, E. (2016). An Analysis of Deep Neural Network Models \nfor Practical Applications. arXiv preprint arXiv, 1605(07678). \nChen, S. W., Shivakumar, S. S., Dcunha, S., Das, J., Okon, E., Qu, C., & Kumar, V. (2017). \nCounting Apples and Oranges With Deep Learning: A Data-Driven Approach. IEEE \nRobotics and Automation Letters, 2(2), 781-788. \nChen, Y., Lin, Z., Zhao, X., Wang, G., & Gu, Y. (2014). Deep Learning-Based Classification of \nHyperspectral Data. IEEE Journal of Selected topics in applied earth observations and \nremote sensing, 7(6), 2094-2107. \nChi, M., Plaza, A., Benediktsson, J. A., Sun, Z., Shen, J., & Zhu, Y. (2016). Big data for remote \nsensing: challenges and opportunities. Proceedings of the IEEE, 104(11), 2207-2219. \nChristiansen, P., Nielsen, L. N., Steen, K. A., Jørgensen, R. N., & Karstoft, H. (2016). \nDeepAnomaly: Combining Background Subtraction and Deep Learning for Detecting \nObstacles and Anomalies in an Agricultural Field. Sensors , 16(11), 1904. \nDemmers, T. G., Cao, Y., Gauss, S., Lowe, J. C., Parsons, D. J., & Wathes, C. M. (2010). Neural \nPredictive Control of Broiler Chicken Growth. IFAC Proceedings Volumes, 43(6), 311-316. \nDemmers, T. G., Cao, Y., Parsons, D. J., Gauss, S., & Wathes, C. M. (2012). Simultaneous \nMonitoring and Control of Pig Growth and Ammonia Emissions. IX International Livestock \nEnvironment Symposium (ILES IX). Valencia, Spain: American Society of Agricultural and \nBiological Engineers. \nDeng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale \nhierarchical image database. (págs. 248-255). Miami, FL, USA: IEEE Conference on \nComputer Vision and Pattern Recognition (CVPR). \nDeng, L., & Yu, D. (2014). Deep learning: methods and applications. Foundations and Trends in \nSignal Processing, 7(3-4), 197-387. \nDouarre, C., Schielein, R., Frindel, C., Gerth, S., & Rousseau, D. (2016). Deep learning based root-\nsoil segmentation from X-ray tomography. bioRxiv, 071662. \nDyrmann, M., Jørgensen, R. N., & Midtiby, H. S. (2017). RoboWeedSupport - Detection of weed \nlocations in leaf occluded cereal crops using a fully convolutional neural network. 11th \n34 \n \nEuropean Conference on Precision Agriculture (ECPA). Edinburgh, Scotland. \nDyrmann, M., Karstoft, H., & Midtiby, H. S. (2016 ). Plant species classification using deep \nconvolutional neural network. Biosystems Engineering, 151, 72-80. \nDyrmann, M., Mortensen, A. K., Midtiby, H. S., & Jørgensen, R. N. (2016). Pixel-wise \nclassification of weeds and crops in images by using a fully convolutional neural network. \nInternational Conference on Agricultural Engineering. Aarhus, Denmark. \nFAO. (2009). How to Feed the World in 2050. Rome: Food and Agriculture Organization of the \nUnited Nations. \nGebbers, R., & Adamchuk, V. I. (2010). Precision agriculture and food security. Science, \n327(5967), 828-831. \nGers, F. A., Schmidhuber, J., & Cummins, F. (2000). Learning to forget: Continual prediction with \nLSTM. Neural Computation, 12(10), 2451-2471. \nGrinblat, G. L., Uzal, L. C., Larese, M. G., & Granitto, P. M. (2016). Deep learning for plant \nidentification using vein morphological patterns. Computers and Electronics in Agriculture, \n127, 418-424. \nHall, D., McCool, C., Dayoub, F., Sunderhauf, N., & Upcroft, B. (2015). Evaluation of features for \nleaf classification in challenging conditions. Winter Conference on Applications of \nComputer Vision (WACV) (págs. 797-804). Waikoloa Beach, Hawaii: IEEE. \nHashem, I., Yaqoob, I., Anuar, N., Mokhtar, S., Gani, A., & Khan, S. (2015). The rise of “big data” \non cloud computing: Review and open research issues. Information Systems, 47, 98-115. \nIenco, D., Gaetano, R., Dupaquier, C., & Maurel, P. (2017). Land Cover Classification via Multi-\ntemporal Spatial Data by Recurrent Neural Networks. arXiv preprint arXiv:1704.04055. \nIshimwe, R., Abutaleb, K., & Ahmed, F. (2014). Applications of thermal imaging in agriculture—A \nreview. Advances in Remote Sensing, 3(3), 128. \nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., . . . Darrell, T. (2014). \nCaffe: Convolutional architecture for fast feature embedding. Proceedings of the 22nd \nInternational Conference on Multimedia (págs. 675-678). Orlando, FL, USA: ACM. \nKamilaris, A., Assumpcio, A., Blasi, A. B., Torrellas, M., & Prenafeta-Boldú, F. X. (2017). \nEstimating the Environmental Impact of Agriculture by Means of Geospatial and Big Data \nAnalysis: The Case of Catalonia. From Science to Society (págs. 39-48). Luxembourg: \nSpringer. \nKamilaris, A., Gao, F., Prenafeta-Boldú, F. X., & Ali, M. I. (2016). Agri-IoT: A semantic \nframework for Internet of Things-enabled smart farming applications. 3rd World Forum on \nInternet of Things (WF-IoT) (págs. 442-447). Reston, VA, USA: IEEE. \nKamilaris, A., Kartakoullis, A., & Prenafeta-Boldú, F. X. (2017). A review on the practice of big \ndata analysis in agriculture. Computers and Electronics in Agriculture, 143(1), 23-37. \nKitzes, J., Wackernagel, M., Loh, J., Peller, A., Goldfinger, S., Cheng, D., & Tea, K. (2008). Shrink \nand share: humanity's present and future Ecological Footprint. Philosophical Transactions \nof the Royal Society of London B: Biological Sciences, 363(1491), 467-475. \nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep \nconvolutional neural networks. Advances in neural information processing systems, 1097-\n1105. \nKussul, N., Lavreniuk, M., Skakun, S., & Shelestov, A. (2017). Deep Learning Classification of \nLand Cover and Crop Types Using Remote Sensing Data. IEEE Geoscience and Remote \nSensing Letters, 14(5), 778-782. \nKuwata, K., & Shibasaki, R. (2015). Estimating crop yields with deep learning and remotely sensed \ndata. (págs. 858-861). Milan, Italy: IEEE International Geoscience and Remote Sensing \nSymposium (IGARSS). \nLeCun, Y., & Bengio, Y. (1995). Convolutional networks for images, speech, and time series. The \nhandbook of brain theory and neural networks, 3361(10). \nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444. \nLee, S. H., Chan, C. S., Wilkin, P., & Remagnino, P. (2015). Deep-plant: Plant identification with \n35 \n \nconvolutional neural networks. (págs. 452-456). Quebec city, Canada: IEEE International \nConference on Image Processing (ICIP). \nLiaghat, S., & Balasundram, S. K. (2010). A review: The role of remote sensing in precision \nagriculture. American journal of agricultural and biological sciences, 5(1), 50-55. \nLu, H., Fu, X., Liu, C., Li, L. G., He, Y. X., & Li, N. W. (2017). Cultivated land information \nextraction in UAV imagery based on deep convolutional neural network and transfer \nlearning. Journal of Mountain Science, 14(4), 731-741. \nLuus, F. P., Salmon, B. P., van den Bergh, F., & Maharaj, B. T. (2015). Multiview deep learning for \nland-use classification. IEEE Geoscience and Remote Sensing Letters, 12(12), 2448-2452. \nMandic, D. P., & Chambers, J. A. (2001). Recurrent neural networks for prediction: learning \nalgorithms, architectures and stability. New York: John Wiley. \nMcCool, C., Perez, T., & Upcroft, B. (2017). Mixtures of Lightweight Deep Convolutional Neural \nNetworks: Applied to Agricultural Robotics. IEEE Robotics and Automation Letters, 2(3), \n1344-1351. \nMilioto, A., Lottes, P., & Stachniss, C. (2017). Real-time blob-wise sugar beets vs weeds \nclassification for monitoring fields using convolutional neural networks. Proceedings of the \nInternational Conference on Unmanned Aerial Vehicles in Geomatics. Bonn, Germany. \nMinh, D. H., Ienco, D., Gaetano, R., Lalande, N., Ndikumana, E., Osman, F., & Maurel, P. (2017). \nDeep Recurrent Neural Networks for mapping winter vegetation quality coverage via multi-\ntemporal SAR Sentinel-1. arXiv preprint arXiv:1708.03694. \nMohanty, S. P., Hughes, D. P., & Salathé, M. (2016). Using deep learning for image-based plant \ndisease detection. Frontiers in plant science, 7. \nMortensen, A. K., Dyrmann, M., Karstoft, H., Jørgensen, R. N., & Gislum, R. (2016). Semantic \nsegmentation of mixed crops using deep convolutional neural network. International \nConference on Agricultural Engineering. Aarhus, Denmark. \nNajafabadi, M. M., Villanustre, F., Khoshgoftaar, T. M., Seliya, N., Wald, R., & Muharemagic, E. \n(2015). Deep learning applications and challenges in big data analytics. Journal of Big Data, \n2(1), 1. \nNamin, S. T., Esmaeilzadeh, M., Najafi, M., Brown, T. B., & Borevitz, J. O. (2017). Deep \nPhenotyping: Deep Learning For Temporal Phenotype/Genotype Classification. bioRxiv, \n134205. \nOzdogan, M., Yang, Y., Allez, G., & Cervantes, C. (2010). Remote sensing of irrigated agriculture: \nOpportunities and challenges. Remote sensing, 2(9), 2274-2304. \nPan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on knowledge and \ndata engineering , 22(10), 1345-1359. \nPASCAL VOC Project. (2012). The PASCAL Visual Object Classes. Obtenido de \nhttp://host.robots.ox.ac.uk/pascal/VOC/ \nPotena, C., Nardi, D., & Pretto, A. (2016). Fast and accurate crop and weed identification with \nsummarized train sets for precision agriculture. International Conference on Intelligent \nAutonomous Systems (págs. 105-121). Shanghai, China: Springer, Cham. \nPound, M. P., et al. (2016). Deep Machine Learning provides state-of-the-art performance in \nimage-based plant phenotyping. bioRxiv, 053033. \nRahnemoonfar, M., & Sheppard, C. (2017). Deep Count: Fruit Counting Based on Deep Simulated \nLearning. Sensors, 17(4), 905. \nRebetez, J., et al. (2016). Augmenting a convolutional neural network with local histograms—a \ncase study in crop classification from high-resolution UAV imagery. European Symposium \non Artificial Neural Networks, Computational Intelligence and Machine Learning. Bruges, \nBelgium. \nReyes, A. K., Caicedo, J. C., & Camargo, J. E. (2015). Fine-tuning Deep Convolutional Networks \nfor Plant Recognition. Toulouse: CLEF (Working Notes). \nRußwurm, M., & Körner, M. (2017). Multi-Temporal Land Cover Classification with Long Short-\nTerm Memory Neural Networks. International Archives of the Photogrammetry, Remote \n36 \n \nSensing & Spatial Information Sciences, 42. \nSa, I., Ge, Z., Dayoub, F., Upcroft, B., Perez, T., & McCool, C. (2016). Deepfruits: A fruit \ndetection system using deep neural networks. Sensors, 16(8), 1222. \nSantoni, M. M., Sensuse, D. I., Arymurthy, A. M., & Fanany, M. I. (2015). Cattle Race \nClassification Using Gray Level Co-occurrence Matrix Convolutional Neural Networks. \nProcedia Computer Science, 59, 493-502. \nSaxena, L., & Armstrong, L. (2014). A survey of image processing techniques for agriculture. \nPerth, Australia: Proceedings of Asian Federation for Information Technology in \nAgriculture, Australian Society of Information and Communication Technologies in \nAgriculture. \nSchmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural networks, 61, 85-\n117. \nSehgal, G., Gupta, B., Paneri, K., Singh, K., Sharma, G., & Shroff, G. (2017). Crop Planning using \nStochastic Visual Optimization. arXiv preprint arXiv:1710.09077. \nSimonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image \nrecognition. arXiv preprint arXiv, 1409(1556). \nSingh, A., Ganapathysubramanian, B., Singh, A. K., & Sarkar, S. (2016). Machine Learning for \nHigh-Throughput Stress Phenotyping in Plants. Trends in Plant Science, 21(2), 110-124. \nSladojevic, S., Arsenovic, M., Anderla, A., Culibrk, D., & Stefanovic, D. (2016). Deep neural \nnetworks based recognition of plant diseases by leaf image classification. Computational \nintelligence and neuroscience, 2016. \nSong, X., Zhang, G., Liu, F., Li, D., Zhao, Y., & Yang, J. (2016). Modeling spatio-temporal \ndistribution of soil moisture by deep learning-based cellular automata model. Journal of \nArid Land, 8(5), 734-748. \nSørensen, R. A., Rasmussen, J., Nielsen, J., & Jørgensen, R. (2017). Thistle detection using \nconvolutional neural networks. Montpellier, France: EFITA Congress. \nSteen, K. A., Christiansen, P., Karstoft, H., & Jørgensen, R. N. (2016). Using Deep Learning to \nChallenge Safety Standard for Highly Autonomous Machines in Agriculture. Journal of \nImaging, 2(1), 6. \nSzegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, Inception-ResNet and \nthe Impact of Residual Connections on Learning. (págs. 4278-4284). AAAI. \nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., & Rabinovich, A. (2015). \nGoing deeper with convolutions. (págs. 1-9). Boston, MA, USA: IEEE conference on \ncomputer vision and pattern recognition. \nTeke, M., Deveci, H. S., Haliloğlu, O., Gürbüz, S. Z., & Sakarya, U. (2013). A short survey of \nhyperspectral remote sensing applications in agriculture. Istanbul, Turkey: 6th International \nConference on Recent Advances in Space Technologies (RAST), IEEE. \nTyagi, A. C. (2016). Towards a Second Green Revolution. Irrigation and Drainage, 65(4), 388-\n389. \nWaga, D., & Rabah, K. (2014). Environmental conditions’ big data management and cloud \ncomputing analytics for sustainable agriculture. World Journal of Computer Application and \nTechnology, 2(3), 73-81. \nWan, J., Wang, D., Hoi, S. C., Wu, P., Zhu, J., Zhang, Y., & Li, J. (2014). Deep learning for \ncontent-based image retrieval: A comprehensive study. (págs. 157-166). Orlando, FL: \nProceedings of the 22nd ACM international conference on Multimedia, ACM. \nWeber, R. H., & Weber, R. (2010). Internet of Things (Vol. 12). New York, NY, USA: Springer. \nXinshao, W., & Cheng, C. (2015). Weed seeds classification based on PCANet deep learning \nbaseline. (págs. 408-415). IEEE Signal and Information Processing Association Annual \nSummit and Conference (APSIPA). \nYalcin, H. (2017 ). Plant phenology recognition using deep learning: Deep-Pheno. 6th International \nConference on Agro-Geoinformatics. Fairfax VA, USA. \n \n \n37 \n \nAppendix I: Applications of computer vision in agriculture and popular techniques used. \nNo. Application in \nAgriculture \nRemote sensing \nTechniques for data analysis \n1. \nSoil and \nvegetation/crop \nmapping \nHyperspectral imaging \n(satellite and airborne), \nmulti‐spectral imaging \n(satellite), synthetic \naperture radar (SAR) \nImage fusion, SVM, end-member extraction algorithm, \nco-polarized phase differences (PPD), linear \npolarizations (HH, VV, HV), distance-based \nclassification, decision trees, linear mixing models, \nlogistic regression, ANN, NDVI \n2. \nLeaf area index \nand crop canopy \nHyperspectral imaging \n(airborne), multi‐spectral \nimaging (airborne) \nLinear regression analysis, NDVI \n3. \nCrop phenology Satellite remote sensing \n(general) \nWavelet-based filtering, Fourier transforms, NDVI \n4. \nCrop height, \nestimation of \nyields, fertilizers' \neffect and \nbiomass \nLight Detection and \nRanging (LIDAR), \nhyperspectral and multi-\nspectral imaging, SAR, \nred-edge camera, \nthermal infrared \nLinear and exponential regression analysis, linear \npolarizations (VV), wavelet-based filtering, vegetation \nindices (NDVI, ICWSI), ANN \n5. \nCrop monitoring \nSatellite remote sensing, \n(hyperspectral and multi-\nspectral imaging), NIR \ncamera, SAR \nStepwise discriminate analysis (DISCRIM) feature \nextraction, linear regression analysis, co-polarized phase \ndifferences (PPD), linear polarizations (HH, VV, HV, RR \nand RL), classification and regression tree analysis \n6. \nIdentification of \nseeds and \nreorganization of \nspecies \nRemote sensing in \ngeneral, cameras and \nphoto-detectors, \nhyperspectral imaging \nPrincipal component analysis, feature extraction,  linear \nregression analysis \n7. \nSoil and leaf \nnitrogen content \nand treatment, \nsalinity detection \nHyperspectral and multi-\nspectral imaging, thermal \nimaging \nLinear and exponential regression analysis \n \n8. \nIrrigation \n \nSatellite remote sensing \n(hyperspectral and multi-\nspectral imaging), red-\nedge camera, thermal \ninfrared \nImage classification techniques (unsupervised \nclustering, density slicing with thresholds), decision \ntrees, linear regression analysis, NDVI \n9. \nPlants water \nstress detection \nand drought \nconditions \nSatellite remote sensing \n(hyperspectral and multi-\nspectral imaging, radar \nimages), thermal \nimaging, NIR camera, \nred-edge camera \nFraunhofer Line Depth (FLD) principle, linear regression \nanalysis, NDVI \n10. Water erosion \nassessment \nSatellite remote sensing \n(optical and radar \nimages), SAR, NIR \ncamera \nInterferometric SAR image processing, linear and \nexponential regression analysis, contour tracing, linear \npolarizations (HH, VV) \n38 \n \n11. \nPest detection \nand \nmanagement \nHyperspectral and multi-\nspectral imaging, \nmicrowave remote \nsensing, thermal camera \nImage processing using sample imagery, linear and \nexponential regression analysis, statistical analysis, \nCEM nonlinear signal processing, NDVI \n12. Weed detection \nRemote sensing in \ngeneral, optical cameras \nand photo-detectors, \nhyperspectral and multi-\nspectral imaging \nPixel classification based on k-means clustering and \nBayes classifier, feature extraction techniques with FFT \nand GLCM, wavelet-based classification and Gabor \nfiltering, genetic algorithms, fuzzy techniques, artificial \nneural networks, erosion and dilation segmentation, \nlogistic regression, edge detection, color detection, \nprincipal component analysis \n13. Herbicide \nRemote sensing in \ngeneral, optical cameras \nand photo-detectors \n \nFuzzy techniques, discriminant analysis \n14. Fruit grading \nOptical cameras and \nphoto-detectors, \nmonochrome images \nwith different \nilluminations \nK-means clustering, image fusion, color histogram \ntechniques, machine learning (esp. SVM), Bayesian \ndiscriminant analysis, Bayes filtering, linear discriminant \nanalysis \n15. \nPackaged food \nand food \nproducts – \nidentification of \ncontaminants, \ndiseases or \ndefects, bruise \ndetection \nX-ray imaging (or \ntransmitted light), CCD \ncameras, monochrome \nimages with different \nilluminations, thermal \ncameras, multi-spectral \nand hyperspectral NIR-\nbased imaging \n3D vision, invariance, pattern recognition and image \nmodality,  multivariate image analysis with principal \ncomponent analysis, K-mean clustering, SVM, linear \ndiscriminant analysis, classification trees, K-nearest \nneighbors, decision trees, fusion, feature extraction \ntechniques with FFT, standard Bayesian discriminant \nanalysis, feature analysis, color, shape and geometric \nfeatures using discrimination analysis, pulsed-phase \nthermography \n16. Crop hail \ndamage \nMulti-spectral imaging, \npolarimetric radar \nimagery \nLinear and exponential regression analysis, \nunsupervised image classification \n17. \nAgricultural \nexpansion and \nintensification \nSatellite remote sensing \nin general \nWavelet-based filtering \n18. Greenhouse \nmonitoring \nOptical and thermal \ncameras \nLinear and exponential regression analysis, \nunsupervised classification, NDVI, IR thermography \n39 \n \nAppendix II: Applications of deep learning in agriculture. \nNo. Agri \nArea \nProblem \nDescription Data Used \nClasses and \nLabels \nVariation \namong \nClasses \nDL \nModel \nUsed \nFW \nUsed \nData Pre-\nProcessing \nData \naugmenta\ntion \nData for \nTraining \nvs. \nTesting \nPerfor\nmance \nMetric \nUsed \nValue of \nMetric \nUsed \nComparison \nwith other \ntechnique \nRef. \n1. \nLeaf classification \nClassify \nleaves of \ndifferent \nplant species \nFlavia dataset, \nconsisting of 1,907 \nleaf images of 32 \nspecies with at \nleast 50 images \nper species and at \nmost 77 images. \n32 classes: 32 \nDifferent plant \nspecies \nN/A \nAuthor-\ndefined \nCNN + \nRF \nclassifier \nCaffe \nFeature \nextraction \nbased on \nHistograms of \nCurvature \nover Scale \n(HoCS), \nshape and \nstatistical \nfeatures, use \nof normalized \nexcessive \ngreen (NExG) \nvegetative \nindex, white \nborder \ndoubling \nimage size, \nsegmentation \nN/A \nSame. \n(condition \nvariations \napplied in \ntesting: \ntranslations, \nscaling, \nrotations, \nshading and \nocclusions) \nCA \n97.3% \n±0.6% \nFeature \nextraction \n(shape and \nstatistical \nfeatures) and \nRF classifier \n(91.2% ± \n1.6%) \n(Hall, \nMcCool, \nDayoub, \nSunderha\nuf, & \nUpcroft, \n2015) \n2. \nLeaf disease detection \n13 different \ntypes of \nplant \ndiseases out \nof healthy \nleaves \nAuthors-created \ndatabase \ncontaining 4,483 \nimages. \n15 classes: \nPlant diseases \n(13), healthy \nleaves (1) and \nbackground \nimages (1) \nN/A \nCaffeNet \nCNN \nCaffe \nCropping, \nsquare around \nthe leaves to \nhighlight \nregion of \ninterest, \nresized to \n256×256 pix, \ndupl. image \nremoval \n \nAffine \ntransform \n(translation, \nrotation), \nperspective \ntransform, \nand image \nrotations. \nSame \nCA \n96.30% \nBetter results \nthan SVM (no \nmore details) \n(Sladojevi\nc, \nArsenovic\n, Anderla, \nCulibrk, & \nStefanovi\nc, 2016) \n40 \n \n3. \nPlant disease detection \n \nIdentify 14 \ncrop species \nand 26 \ndiseases \nPlantVillage public \ndataset of 54,306 \nimages of \ndiseased and \nhealthy plant \nleaves collected \nunder controlled \nconditions. \n38 class labels \nas crop- \ndisease pairs \nN/A \nAlexNet, \nGoogleN\net CNNs \nCaffe \nResized to \n256×256 pix., \nsegmentation, \nbackground \nInformation \nremoval, fixed \ncolor casts \nN/A \nSame. Also \ntested on a \ndataset of \ndownloaded \nimages \nfrom Bing \nImage \nSearch and \nIPM Images \nF1 \n0.9935 \nSubstantial \nmargin in \nstandard \nbenchmarks \nwith \napproaches \nusing hand-\nengineered \nfeatures \n(Mohanty, \nHughes, \n& \nSalathé, \n2016) \n4. \nClassify \nbanana \nleaves’ \ndiseases \nDataset of 3,700 \nimages of banana \ndiseases obtained \nfrom the \nPlantVillage \ndataset. \n3 classes: \nhealthy, \nblack sigatoka \nand black \nspeckle \nN/A \nLeNet \nCNN \ndeeplear\nning4j \nResized to \n60x60 pix., \nconverted to \ngrayscale \nN/A \nSame \nCA, F1 \n96+% \n(CA), \n0.968 \n(F1) \nMethods \nusing hand-\ncrafted \nfeatures not \ngeneralize \nwell \n(Amara, \nBouaziz, \n& \nAlgergaw\ny, 2017) \n5. \nLand cover classification \nIdentify 13 \ndifferent \nland-cover \nclasses in \nKSC and 9 \ndifferent \nclasses in \nPavia \nA mixed \nvegetation site \nover Kennedy \nSpace Center \n(KSC), FL, USA \n(Dataset 1), and \nan urban site over \nthe city of Pavia, \nItaly (Dataset 2). \nHyperspectral \ndatasets. \n13 different \nland-cover \nclasses \n(Dataset 1), 9 \nland cover \nclasses trees \n(Dataset 2): \nSoil, meadow, \nwater, \nshadows, \ndifferent \nmaterials \nN/A \nHybrid of \nPCA, \nautoenc\noder \n(AE), \nand \nlogistic \nregressi\non \nDevelop\ned by \nthe \nauthors \nSome bands \nremoved due \nto noise \nN/A \nSame \nCA \n98.70% \n1% more \nprecise than \nRBF-SVM \n(Chen, \nLin, Zhao, \nWang, & \nGu, 2014) \n6. \nIdentify 21 \nland-use \nclasses \ncontaining a \nvariety of \nspatial \npatterns \nUC Merced land-\nuse data set. \nAerial ortho-\nimagery with a \n0.3048-m pixel \nresolution. Dataset \ncompiled from a \nselection of 100 \nimages/class. \n21 land-use \nclasses: \nAgricultural, \nairplane, \nsports, beach, \nbuildings, \nresidential, \nforest, freeway, \nharbor, parking \nlot, river etc. \nHigh relevance \nbetween \nmedium density \nand dense \nresidential, as \nwell as between \nbuildings and \nstorage tanks \nAuthor-\ndefined \nCNN + \nmultiview \nmodel \naveraging \nTheano \nFrom RGB to \nHSV (hue-\nsaturation-\nvalue) color \nmodel, \nresized to \n96×96 pix., \ncreation of \nmultiscale \nviews \nViews \nflipped \nhorizontally \nor vertically \nwith a \nprobability \nof 0.5 \nSame \nCA \n93.48% \nUnsupervised \nfeature \nlearning \n(UFL): 82-\n90%  \nSIFT: 85% \n(Luus, \nSalmon, \nvan den \nBergh, & \nMaharaj, \n2015) \n41 \n \n7. \nExtract \ninformation \nabout \ncultivated \nland \nImages from UAV \nat the areas \nPengzhou County \nand Guanghan \nCounty, Sichuan \nProvince, China. \n2 classes: \nCultivated vs. \nnon-cultivated \nThe cultivated \nland samples \nand part of \nforest land \nsamples were \neasily confused \nAuthor-\ndefined \nCNN \nN/A \nOrthorectificati\non, image \nmatching, \nlinear land \nelimination, \ncorrect \ndistortion, \nzoomed to \n40×40 pix. \nN/A \nSame \nCA \n88-91% \nN/A \n(Lu, et al., \n2017) \n8. \nLand cover \nclassification \nconsidering \ntime series \nFirst dataset \ngenerated using a \ntime series of \nPléiades VHSR \nimages at THAU \nBasin. Second \ndataset generated \nfrom an annual \ntime series of 23 \nLandsat 8 images \nacquired in 2014 \nabove Reunion \nIsland. \n11 classes \n(dataset 1),  \n9 classes \n(dataset 2). \nLand cover \nclasses such \nas trees, crops, \nforests, water, \nsoils, urban \nareas, \ngrasslands, \netc. (Image \nobject or pixel) \nTree Crops, \nSummer crops \nand Truck \nFarming were \nclasses highly \nmixed \nOne-unit \nLSTM + \nRFF, \nOne-unit \nLSTM + \nSVM \nKeras/ \nTheano \nMultiresolution \nsegmentation \ntechnique, \nfeature \nextraction, \npixel-wise \nmulti-temporal \nlinear \ninterpolation, \nvarious \nradiometric \nindices \ncalculated \nN/A \nSame \nCA, F1 \nFirst \nDataset: \n75.34% \n(CA), \n0.7463 \n(F1) \nSecond \nDataset: \n84.61% \n(CA), \n0.8441 \n(F1) \nRF and SVM \n(best of both): \nFirst Dataset: \n74.20% (CA), \n0.7158 (F1) \nSecond \nDataset: \n83.82% (CA), \n0.8274 (F1)  \n(Ienco, \nGaetano, \nDupaquie\nr, & \nMaurel, \n2017) \n9. \nCrop type classification \nClassificatio\nn of crops \nwheat, \nmaize, \nsoybeans \nsunflower \nand sugar \nbeet \n19 multi-temporal \nscenes acquired \nby Landsat-8 and \nSentinel-1A RS \nsatellites from a \ntest site in \nUkraine. \n11 classes: \nwater, forest, \ngrassland, \nbare land, \nwheat, maize, \nrapeseed, \ncereals, sugar \nbeet, \nsunflowers and \nsoybeans. \nGeneral \nconfusion \nbetween maize \nand soybeans \nAuthor-\ndefined \nCNN \nDevelop\ned by \nthe \nauthors \nCalibration, \nmulti-looking, \nspeckle \nfiltering (3×3 \nwindow with \nRefined Lee \nalgorithm), \nterrain \ncorrection, \nsegmentation, \nrestoration of \nmissing data \nN/A \nSame \nCA \n94.60% \nMultilayer \nperceptron: \n92.7%,  \nRF: 88% \n(Kussul, \nLavreniuk\n, Skakun, \n& \nShelestov\n, 2017) \n42 \n \n10. \nClassificatio\nn of crops oil \nradish, \nbarley, \nseeded \ngrass, weed \nand stump \n36 plots at Foulum \nResearch Center, \nDenmark \ncontaining oil \nradish as a catch \ncrop and amounts \nof barley, grass, \nweed and stump. \n352 patches in \ntotal. \n7 classes: oil \nradish, barley, \nweed, stump, \nsoil, equipment \nand unknown \n(pixel of the \nimage) \nCoarse features \n(radish leafs and \nsoil) were \npredicted quite \nwell. Finer \nfeatures (barley, \ngrass or stump) \nnot so much. \nAdapted \nversion \nof \nVGG16 \nCNN \nDevelop\ned by \nthe \nauthors \nResized to \n1600x1600 \npix. centered \non the sample \nareas, division \ninto 400x400 \npix. patches \nRotations 0, \n90, 180 and \n270 \ndegrees, \nflipped \ndiagonally \nand same \nset of \nrotations \nSame \nCA, IoU  \n79% \n(CA), \n0.66 (IoU) \nN/A \n(Mortens\nen, \nDyrmann, \nKarstoft, \nJørgense\nn, & \nGislum, \n2016) \n11. \nCrop type \nclassification \nconsidering \ntime series \nA raster dataset of \n26 SENTINEL 2A \nimages, acquired \nbetween 2015 \n2016 at Munich \nGermany. \nShortwave infrared \n1 and 2 bands \nwere selected. \n19 classes: \ncorn, meadow, \nasparagus, \nrape, hop, \nsummer oats, \nwinter spelt, \nfallow, wheat, \nbarley, winter \nrye, beans and \nothers \nSome classes \nrepresent \ndistinct \ncultivated crops, \nothers (such as \nmeadow, fallow, \ntriticale, wheat, \nand rye) are \nbotanically \nrelated. \nThree-\nunit \nLSTM \nTensorFl\now \nAtmospherical\nly corrected \nN/A \nSame \nCA, F1 \n76.2% \n(CA), \n0.558  \n(F1) \nCNN: 59.9% \n(CA), 0.236 \n(F1) \nSVM: 31.7 \n(CA), 84.8% \n0.317 (F1) \n(Rußwur\nm & \nKörner, \n2017) \n12. \nCrop type \nclassification \nfrom UAV \nimagery \nAerial images of \nexperimental farm \nfields issued from \na series of \nexperiments \nconducted by the \nSwiss \nConfederation’s \nAgroscope \nresearch center. \n23 classes: 22 \ndifferent crops \nplus soil (pixel \nof the image) \nLin and Simplex \nhave very \nsimilar \nhistograms \nCNN + \nHistNN \n(using \nRGB \nhistogram\ns) \nKeras \nImage \nsegmentation \nN/A \nSame \nF1 \n0.90 \n(experime\nnt 0), \n0.73 \n(experime\nnt 1) \nCNN: 0.83 \n(experiment \n0), 0.70 \n(experiment 1) \nHistNN: 0.86 \n(experiment \n0), 0.71 \n(experiment 1) \n(Rebetez, \nJ., et al., \n2016) \n13.. \nPlant recognition \n \nRecognize 7 \nviews of \ndifferent \nplants: entire \nplant, \nbranch, \nflower, fruit, \nLifeCLEF 2015 \nplant dataset, \nwhich has 91,759 \nimages distributed \nin 13,887 plant \nobservations. \nEach observation \ncaptures the \n1,000 classes: \nSpecies that \ninclude trees, \nherbs, and \nferns, among \nothers. \nImages of \nflowers and leaf \nscans offer \nhigher accuracy \nthan the rest of \nthe views \nAlexNet \nCNN \nCaffe \nN/A \nN/A \nSame \nLC \n48.60% \n20% worse \nthan local \ndescriptors to \nrepresent \nimages and \nKNN, dense \nSIFT and a \nGaussian \n(Reyes, \nCaicedo, \n& \nCamargo, \n2015) \n43 \n \nleaf, stem \nand scans \nappearance of the \nplant from various \npoints of view: \nentire plant, leaf \nbranch, fruit, stem \nscan, flower. \nMixture Model \n14. \nRoot and \nshoot feature \nidentification \nand \nlocalisation \nThe first dataset \ncontains 2,500 \nannotated images \nof whole root \nsystems. The \nsecond hand-\nannotated 1,664 \nimages of wheat \nplants, labeling \nleaf tips, leaf \nbases, ear tips, \nand ear bases. \n2 classes: \nPrediction if a \nroot tip is \npresent or not \n(first dataset)  \n5 classes: Leaf \ntips and bases, \near tips and \nbases, and \nnegative \n(second \ndataset) \nN/A \nAuthor-\ndefined \nCNN \nCaffe \nImage \ncropping at \nannotated \nlocations \n128x128 pix., \nresized to \n64x64 for use \nin the network \nN/A \nSame \nCA \n98.4% \n(first \ndataset)\n97.3% \n(second \ndataset) \nSparse coding \napproach \nusing SIFT + \nSVM: 80-90% \n(Pound, \nM. P., et \nal., 2016) \n15. \nRecognize \n44 different \nplant species \nMalayaKew (MK) \nLeaf Dataset \nwhich consists of \n44 classes, \ncollected at the \nRoyal Botanic \nGardens, Kew, \nEngland. \n44 classes: \nSpecies such \nas acutissima, \nmacranthera, \nrubra,  robur f. \npurpurascens \netc. \nN/A \nAlexNet \nCNN \nCaffe \nForeground \npixels \nextracted \nusing HSV \ncolor space,  \nimage \ncropping \nwithin leaf \narea \nRotation in \n7 different \norientations \nSame \nCA \n99.60% SVM: 95.1%, \nANN: 58% \n(Lee, \nChan, \nWilkin, & \nRemagni\nno, 2015) \n16. \nIdentify \nplants from \nleaf vein \npatterns of \nwhite, soya \nand red \nbeans \n866 leaf images \nprovided by INTA \nArgentina. Dataset \ndivided into three \nclasses: 422 \nimages \ncorrespond to \nsoybean leaves, \n272 to red bean \nleaves and 172 to \nwhite bean leaves. \n3 classes: \nLegume \nspecies white \nbean, \nred bean and \nsoybean \nAt soybean, \ninformative \nregions are in \nthe central vein. \nFor white and \nred bean, outer \nand smaller \nveins are also \nrelevant. \nAuthor-\ndefined \nCNN \nPylearn2 \nVein \nsegmentation, \ncentral patch \nextraction \nN/A \nSame \nCA \n96.90% \nPenalized \nDiscriminant \nAnalysis \n(PDA): 95.1%  \nSVM and RF \nslightly worse \n(Grinblat, \nUzal, \nLarese, & \nGranitto, \n2016) \n44 \n \n17. \nPlant phenology recognition \nClassify \nphenological \nstages of \nseveral \ntypes of \nplants purely \nbased on the \nvisual data \nDataset collected \nthrough TARBIL \nAgro-informatics \nResearch Center \nof ITU, for which \nover a thousand \nagrostations are \nplaced throughout \nTurkey. Different \nimages of various \nplants, at different \nphenological \nstages. \n9 classes: \nDifferent \ngrowth stages \nof plants, \nstarting from \nplowing to \ncropping, for \nthe plants \nwheat, barley, \nlentil, cotton, \npepper and \ncorn. (image \nsegment) \nAppearances \nchange very \ngradually and it \nis challenging to \ndistinguish \nimages falling \ninto the growing \ndurations that \nare in the middle \nof two \nsuccessive \nstages. Some \nplants from \ndifferent classes \nhave \nsimilar color and \ntexture \ndistributions \nAlexNet \nCNN \nDevelop\ned by \nthe \nauthors \nImage \nsegmentation \nImages are \ndivided into \nlarge \npatches and \nfeatures are \nextracted \nfor each \npatch. \n227x227 \npix. patches \nare carved \nfrom the \noriginal \nimages \nSame \nCA, F1 \n73.76 – \n87.14 \n(CA), \n0.7417 – \n0.8728 \n(F1) \nHand crafted \nfeature \ndescriptors \n(GLCM and \nHOG) through \na Naïve-\nBayes \nclassifier: \n68.97 – 82.41 \n(CA), 0.6931 \n– 0.8226 (F1) \n(Yalcin, \n2017 ) \n18. \nClassify the \nphenotyping \nof \nArabidopsis \nin four \naccessions \nDataset composed \nof sequences of \nimages captured \nfrom the plants in \ndifferent days \nwhile they grow, \nsuccessive top-\nview images of \ndifferent \naccessions of \nArabidopsis \nthaliana. \n4 classes: 4 \ndifferent \naccessions of \nArabidopsis: \nGenotype \nstates SF-2, \nCVI, Landsberg \n(Ler) and \nColumbia (Col) \nPlants change in \nsize rapidly \nduring their \ngrowth, the \ndecomposed \nimages from the \nplant sequences \nare not \nsufficiently \nconsistent \nCNN+ \nLSTM \nKeras/ \nTheano \nCamera \ndistortion \nremoval, color \ncorrection, \ntemporal \nmatching, \nplant \nsegmentation \nthrough the \nGrabCut \nalgorithm \nImage \nrotations by \n90, 180 and \n270 \ndegrees \naround its \ncenter \nSame \nCA \n93% \nHand crafted \nfeature \ndescriptors + \nLSTM: 68% \nCNN: 76.8% \n(Namin, \nEsmaeilz\nadeh, \nNajafi, \nBrown, & \nBorevitz, \n2017) \n19. \nSegmentation \nof root and soil \nIdentify roots \nfrom soils \nSoil images \ncoming from X-ray \ntomography. \n2 classes: Root \nor soil (pixel of \nthe image) \nSoil/root \ncontrast is \nsometimes very \nlow \nAuthor-\ndefined \nCNN \nwith \nSVM for \nclassific\nation \nMatCon\nvNet \nImage \nsegmentation \nSimulated \nroots added \nto soil \nimages \nSame \nQM \n0.23 \n(simulati\non)  \n0.57 \n(real \nroots) \nN/A \n(Douarre, \nSchielein, \nFrindel, \nGerth, & \nRoussea\nu, 2016) \n45 \n \n20. \nCrop yield estimation \nEstimate \ncorn yield of \ncounty level \nin U.S. \nCorn yields from \n2001 to 2010 in \nIllinois U.S., \ndownloaded from \nClimate Research \nUnit (CRU), plus \nMODIS Enhanced \nVegetation Index. \nCrop yield \nindex (scalar \nvalue) \nN/A \nAuthor-\ndefined \nCNN \nCaffe \nEnhanced \nVegetation \nIndex (EVI), \nhard threshold \nalgorithm, \nWavelet \ntransformation \nfor detecting \ncrop \nphenology \nN/A \nSame \nRMSE \n 6.298 \nSupport \nVector \nRegression \n(SVR): 8.204 \n(Kuwata \n& \nShibasaki\n, 2015) \n21. \nMapping \nwinter \nvegetation \nquality \ncoverage \nconsidering \ntime series \nSentinel-1 dataset \nincluding 13 \nacquisitions in \nTOPS mode from \nOctober 2016 to \nFebruary 2017, \nwith a temporal \nbaseline of 12 \ndays. Dual-\npolarization \n(VV+VH) data in \n26 images. \n5 classes: \nEstimations of \nthe quality of \nvegetative \ndevelopment \nas bare soil, \nvery low, low, \naverage, high \n“Low” class \nintersects the \ntemporal profiles \nof all the other \nclasses multiple \ntimes. A \nmisclassification \nrate exists \nbetween the \n“low” and “bare \nsoil” classes \nFive-unit \nLSTM, \nGated \nRecurre\nnt Unit \n(GRU) \nKeras/ \nTheano \nIntensity \nimage gen., \nradiometrical \ncalibration, \ntemporal \nfiltering for \nnoise \nreduction, \northorectificati\non into map \ncoordinates, \ntransformed to \nlogarithm \nscale, \nnormalized \nN/A \nSame \nCA, F1 \n99.05% \n(CA), \n0.99 (F1) \nRF and SVM \n(best of both): \n91.77% (CA), \n0.9179 (F1) \n(Minh, et \nal., 2017) \n22. \nFruit counting \nPredict \nnumber of \ntomatoes in \nthe images \n24,000 synthetic \nimages produced \nby the authors. \nEstimated \nnumber of \ntomato fruits \n(scalar value) \nN/A \nModified \nInceptio\nn-\nResNet \nCNN \nTensorFl\now \nBlurred \nsynthetic \nimages by a \nGaussian filter \nGenerated \nsynthetic \n128x128 \npix. images \nto train the \nnetwork, \ncolored \ncircles to \nsimulate \nbackground \nand tomato \nplant/crops. \n \nTrained \nentirely on \nsynthetic \ndata and \ntested on \nreal data \nRFC, \nRMSE \n91% \n(RFC) \n1.16 \n(RMSE) \non real \nimages, \n93% \n(RFC)  \n2.52 \n(RMSE) \non \nsynthetic \nimages  \n ABT: 66.16% \n(RFC), 13.56 \n(RMSE) \n(Rahnem\noonfar & \nSheppard\n, 2017) \n46 \n \n23. \nMap from \ninput images \nof apples \nand oranges \nto total fruit \ncounts \n71 1280×960 \norange images \n(day time) and 21 \n1920×1200 apple \nimages (night \ntime). \nNumber of \norange or \napple fruits \n(scalar value) \nHigh variation in \nCA. For orange, \ndataset has high \nocclusion, depth \nvariation, and \nuncontrolled \nillumination. For \napples, data set \nhas high color \nsimilarity \nbetween \nfruit/foliage \nCNN \n(blob \ndetection \nand \ncounting) \n+ Linear \nRegressi\non \nCaffe \nImage \nsegmentation \nfor easier data \nannotation by \nusers, \ncreation of \nbounding \nboxes around \nimage blobs \nTraining set \npartitioned \ninto 100 \nrandomly \ncropped \nand flipped \n320×240 \npix. sub-\nimages \nSame (but \ndifferent \ntrees used \nin training \nand testing) \nRFC, \nL2 \n \n0.968 \n(RFC), \n13.8 (L2) \nfor \noranges \n0.913 \n(RFC), \n10.5 (L2) \nfor apples  \nBest texture-\nbased \nregression \nmodel: 0.682 \n(RFC) \n(Chen, et \nal., 2017) \n24. \nFruit \ndetection \nin orchards, \nincluding \nmangoes, \nalmonds and \napples \nImages of three \nfruit varieties: \napples (726), \nalmonds (385) and \nmangoes (1,154), \ncaptured at \norchards in \nVictoria and \nQueensland, \nAustralia. \nSections of \napples, \nalmonds and \nmangoes at the \nimage \n(bounding box) \nWithin class \nvariations due to \ndistance to fruit \nillumination, fruit \nclustering, and \ncamera view-\npoint. Almonds  \nsimilar in color \nand texture to \nthe foliage \nFaster \nRegion-\nbased \nCNN \nwith \nVGG16 \nmodel \nCaffe \nImage \nsegmentation \nfor easier data \nannotation \nFlip, scale, \nflip-scale \nand the \nPCA \naugmentati\non \ntechnique \npresented \nin AlexNet \nSame \nF1-IoU \n(20) \n0.904 \n(apples) \n0.908  \n(mango) \n0.775 \n(almonds)  \nZF network: \n0.892  \n(apples) \n0.876  \n(mango) \n0.726  \n(almonds) \n(Bargoti & \nUnderwo\nod, 2016) \n25. \nDetection of \nsweet \npepper and \nrock melon \nfruits \n122 images \nobtained from two \nmodalities: color \n(RGB) and Near-\nInfrared (NIR). \nSections of \nsweet red \npeppers and \nrock melons on \nthe image \n(bounding box) \nVariations to \ncamera setup, \ntime and \nlocations of data \nacquisition. \nTime for data \ncollection is day \nand night, sites \nare different. \nVaried fruit \nripeness. \nFaster \nRegion-\nbased \nCNN \nwith \nVGG16 \nmodel \nCaffe \nEarly/late \nfusion \ntechniques for \ncombining the \nclassification \ninfo from color \nand NIR \nimagery, \nbounding box \nsegmentation, \npairwise IoU \nN/A \nSame \n(authors \ndemonstrat\ne by using a \nsmall \ndataset that \nthe model \ncan \ngeneralize) \nF1-IoU \n(40) \n0.838 \nConditional \nRandom Field \nto model color \nand visual \ntexture \nfeatures: \n0.807 \n(Sa, et \nal., 2016) \n26. \nObstacle \ndetection \nIdentify ISO \nbarrel-\nshaped \nobstacles in \n437 images from \nauthors' \nexperiments and \nrecordings, 1,925 \npositive and \nIdentify if a \nbarrel-shaped \nobject is \npresent in the \nimage \nN/A \nAlexNet \nCNN \nCaffe \nResized to \n114×114 pix., \nbounding \nboxes of the \nobject created \nVarious \nrotations at \n13 scales, \nintensity of \nthe object \nTesting in \ndifferent \nfields (row \ncrops, grass \nmowing), \nCA-IoU \n(50) \n99.9% in \nrow \ncrops \nand \n90.8% in \nN/A \n(Steen, \nChristians\nen, \nKarstoft, \n& \n47 \n \nrow crops \nand grass \nmowing \n11,550 negative \nsamples. \n(bounding box) \nadapted \ncontaining \nother \nobstacles \n(people and \nanimals) \ngrass \nmowing \nJørgense\nn, 2016) \n27. \nDetect \nobstacles \nthat are \ndistant, \nheavily \noccluded \nand \nunknown \nBackground data \nof 48 images and \ntest data of 48 \nimages from \nannotations of \nhumans, houses, \nbarrels, wells and \nmannequins. \nClassify each \npixel as either \nforeground \n(contains a \nhuman) or \nbackground \n(anomaly \ndetection) \nN/A \nAlexNet \nand \nVGG \nCNNs \nCaffe \nImage \ncropping, \nresized by a \nfactor of 0.75 \nN/A \nSame \nF1-IoU \n(50) \n0.72 \nLocal de-\ncorrelated \nchannel \nfeatures: \n0.113 \n(Christian\nsen, \nNielsen, \nSteen, \nJørgense\nn, & \nKarstoft, \n2016) \n28. \nIdentification of weeds \nClassify 91 \nweed seed \ntypes \nDataset of 3,980 \nimages containing \n91 types of weed \nseeds. \n91 classes: \nDifferent \ncommon \nweeds found in \nagricultural \nfields \nSimilarity \nbetween some \nclasses is very \nhigh (only slight \ndifferences in \nshape, texture, \nand color) \nPCANet \n+ LMC \nclassifier\ns \nDevelop\ned by \nthe \nauthors \nImage filter \nextraction \nthrough PCA \nfilters bank, \nbinarization \nand \nhistograms’ \ncounting \nN/A \nSame (also \nscaling for a \ncertain \nrange \ntranslation \ndistance \nand rotation \nangle \nCA \n90.96% \nManual \nfeature \nextraction \ntechniques + \nLMC \nclassifiers: \n64.80% \n(Xinshao \n& Cheng, \n2015) \n29. \nClassify \nweed from \ncrop species \nbased on 22 \ndifferent \nspecies in \ntotal. \nDataset of 10,413 \nimages, taken \nmainly from BBCH \n12-16 containing \n22 weed and crop \nspecies at early \ngrowth stages. \n22 classes: \nDifferent \nspecies of \nweeds and \ncrops at early \ngrowth stages \ne.g. \nchamomile, \nknotweed, \ncranesbill, \nchickweed and \nveronica \nVariations with \nrespect to \nlighting, \nresolution, and \nsoil type. Some \nspecies \n(Veronica, Field \nPancy) were \nvery similar and \ndifficult to \nclassify \nVariation \nof \nVGG16 \nTheano-\nbased \nLasagne \nlibrary \nfor \nPython \nGreen \nsegmentation \nto detect \ngreen pixels, \nnon-green \npixels \nremoval, \npadding \nadded to \nmake images \nsquare, \nresized to \n128x128 pix. \nImage \nmirroring \nand rotation \nin 90 \ndegree \nincrements \nSame \nCA \n86.2% \nLocal shape \nand color \nfeatures: \n42.5% and \n12.2% \nrespectively \n(Dyrmann\n, Karstoft, \n& Midtiby, \n2016 ) \n30. \nIdentify \nthistle in \n4,500 images from \n10, 20, 30, and \n50m of altitude \n2 classes: \nWhether the \nimage contains \nSmall variations \nin some images \ndepending on \nDenseN\net CNN \nCaffe \nImage \ncropping \nRandom flip \nboth \nhorizontally \nSame (extra \ntests for the \ncase of \nCA \n97% \nColor feature-\nbased Thistle-\nTool: 95% \n(Sørense\nn, \nRasmuss\n48 \n \nwinter wheat \nand spring \nbarley \nimages \ncaptured by a \nCanon PowerShot \nG15 camera. \nthistle in winter \nwheat or not  \n(Heatmap of \nclasses is \ngenerated at \nthe output) \nthe percentage \nof thistles they \ncontain \nand \nvertically, \nrandom \ntransposing \nwinter \nbarley) \nen, \nNielsen, \n& \nJørgense\nn, 2017) \n31. \nWeed \nsegmentatio\nn for robotic \nplatforms \nCrop/Weed Field \nImage Dataset \n(CW-FID), consists \nof 20 training and \n40 testing images. \nA dataset of 60 \ntop-down field \nimages of a \ncommon culture \n(organic carrots) \nwith the presence \nof intra-row and \nclose-to-crop \nweeds. \n2 classes: \ncarrot plants \nand weeds \n(image region) \nN/A \nAdapted \nversion of \nInception-\nv3 + \nlightweigh\nt DCNN + \nset of K \nlightweigh\nt models \nas a \nmixture \nmodel \n(MixDCN\nN) \nTensorFl\now \nImage up-\nsampling to \n299x299 pix., \nNDVI-based \nvegetation \nmasks, \nextracting \nregions based \non a sliding \nwindow on the \ncolor image \nN/A \nSame \n(different \ncarrot fields \nused for \ntesting) \nCA \n93.90% \nFeature \nextraction \n(shape and \nstatistical \nfeatures) and \nRF classifier: \n85.9% \n(McCool, \nPerez, & \nUpcroft, \n2017) \n32. \nAutomating \nweed \ndetection in \ncolor images \ndespite \nheavy leaf \nocclusion \n1,427 images from \nwinter wheat \nfields, of which \n18,541 weeds \nhave been \nannotated, \ncollected using a \ncamera mounted \non an all-terrain \nvehicle. \nDetect single \nweed instances \nin images of \ncereal fields \n(bounding box). \nA coverage \nmap is \nproduced. \n \nLarge parts of \nthe weeds \noverlap with \nwheat plants \nBased on \nDetectNet \nCNN \n(which is \nbased on \nGoogLeN\net CNN) \nDevelop\ned by \nthe \nauthors \nResized to \n1224×1024 \npix. \nN/A \nDifferent \nfield used \nfor testing. \nThis field \nhas a \nsevere \ndegree of \nocclusion \ncompared \nto the \nothers \nIoU \nP- IoU \n(50)  \nR-IoU \n(50) \n0.64  \n(IoU), \n86.6%  \n(P- IoU), \n46.3% \n(R-IoU)  \nN/A \n(Dyrmann\n, \nJørgense\nn, & \nMidtiby, \n2017) \n49 \n \n33. \nCrop/weed detection and classification \nDetecting \nsugar beet \nplants and \nweeds in the \nfield based \non image \ndata \n1,969 RGB+NIR \nimages captured \nusing a JAI \ncamera in nadir \nview placed on a \nUAV. \nIdentify if an \nimage patch \nbelongs to \nweed or sugar \nbeet (image \nregion) \nN/A \nAuthor-\ndefined \nCNN \nTensorFl\now \nSeparated \nvegetation/ \nbackground \nbased on \nNDVI, binary \nmask to \ndescribe \nvegetation, \nblob \nsegmentation, \nresized to \n64x64 pix., \nnormalized \nand centered \n64 even \nrotations \nSame (also \ngeneralized \nto a second \ndataset \nproduced 2-\nweeks after, \nat a more \nadvanced \ngrowth \nstage) \nP, R \nDataset \nA: 97% \n(P), 98% \n(R) \nDataset \nB: 99% \n(P), 89% \n(R) \nN/A \n(Milioto, \nLottes, & \nStachniss\n, 2017) \n34. \nDetecting \nand \nclassifying \nsugar beet \nplants and \nweeds \n1,600 4-channels \nRGB+NIR images \ncaptured before \n(700 images) and \nafter (900 images) \na 4-week period, \nprovided by a \nmultispectral JAI \ncamera mounted \non a BOSCH \nBonirob farm \nrobot. \nIdentifies if a \nblob belongs to \nsugar beet \ncrop, weeds or \nsoil (image \nblob) \nN/A \nAuthor-\ndefined \nCNN \nTensorFl\now \nPixel-wise \nsegmentation \nbetween \ngreen \nvegetation \nand soil based \non NDVI and \nlight CNN, \nunsupervised \ndataset \nsummariz. \nN/A \nSame (also \ngeneralized \nto a second \ndataset \nproduced 4-\nweeks after, \nat a more \nadvanced \ngrowth \nstage) \nCA \n98% \n(Dataset \nA), \n59.4% \n(Dataset \nB) \nFeature \nextraction \n(shape and \nstatistical \nfeatures) and \nRF classifier: \n95% \n(Potena, \nNardi, & \nPretto, \n2016) \n50 \n \n35. \nDetecting \nand \nclassifying \nweeds and \nmaize in \nfields \nSimulated top-\ndown images of \noverlapping plants \non soil background \nA total of 301 \nimages of soil and \n8,430 images of \nsegmented plants. \nThe plants cover \n23 different weed \nspecies and \nmaize.  \nIdentifies if an \nimage patch \nbelongs to \nweed, soil or \nmaize crop \n(image pixel) \nN/A \nAdapted \nversion \nof \nVGG16 \nCNN \nDevelop\ned by \nthe \nauthors \nImage \ncropping in \n800x800 pix. \nRandom \nscaling from \n80 to 100% \nof original \nsize, \nrandom \nrotations in \none degree \nincrements, \nvaried hue, \nsaturation \nand \nintensity, \nrandom \nshadows \nTested on \nreal images \nwhile \ntrained on \nsimulated \nones \nCA, \nIoU \n94% CA, \n0.71 IoU \n(crops), \n0.70 IoU \n(weeds) \n0.93 IoU \n(soil) \nN/A \n(Dyrmann\n, \nMortense\nn, \nMidtiby, & \nJørgense\nn, 2016) \n36. \nPrediction of soil moisture content \nPredict the \nsoil moisture \ncontent over \nan irrigated \ncorn field \nSoil data collected \nfrom an irrigated \ncorn field (an area \nof 22 sq. km) in \nthe Zhangye oasis, \nNorthwest China. \nPercentage of \nsoil moisture \ncontent (SMC) \n(scalar value) \nN/A \nDeep \nbelief \nnetwork-\nbased \nmacrosc\nopic \ncellular \nautomat\na (DBN-\nMCA) \nDevelop\ned by \nthe \nauthors \nGeospatial \ninterpolation \nfor creation of \nsoil moisture \ncontent maps, \nmultivariate \ngeostatistical \napproach for \nestimating \nthematic soil \nmaps, maps \nconverted to \nTIFF, \nresampled to \n10-m res. \nN/A \nSame \nRMSE \n6.77 \nMulti-layer \nperceptron \nMCA (MLP-\nMCA): 18% \nreduction in \nRMSE \n(Song, et \nal., 2016) \n51 \n \n37. \nAnimal research \nPractical and \naccurate \ncattle \nidentification \nfrom 5 \ndifferent \nraces \n1,300 images \ncollected by the \nauthors. \n5 classes: \nCattle races, \nBali \nOnggole or \nPasuruan, \nAceh \nMadura and \nPesisir \nN/A \n GLCM – \nCNN \nDeep \nLearning \nMatlab \nToolbox \nGLCM \nfeatures \nextraction \n(contrast, \nenergy and \nhomogeneity), \nsaliency maps \nto accelerate \nfeature \nextraction \nN/A \nSame \nCA \n93.76% \nCNN without \nextra inputs: \n89.68% \nGaussian \nMixture \nModel \n(GMM): 90% \n(Santoni, \nSensuse, \nArymurth\ny, & \nFanany, \n2015) \n38. \nPredict \ngrowth of \npigs \n160 pigs, housed \nin two climate \ncontrolled rooms, \nfour pens/room, 10 \npigs/pen. \nAmmonia, ambient \nand indoor air \ntemperature and \nhumidity, feed \ndosage and \nventilation \nmeasured at 6-\nminute intervals. \nEstimation of \nthe weight of \npigs (scalar \nvalue) \nN/A \nFirst-\norder \nDRNN \nDevelop\ned by \nthe \nauthors \nN/A \nN/A \nTested on \ndifferent \nrooms of \npigs than \nthe ones \nwhich were \nused for \ntraining \nMSE, \nMRE \n0.002 \n(MSE) on \nsame \ndataset), \n10% \n(MRE) in \nrelation to \na \ncontroller \nN/A \n(Demmer\ns T. G., \nCao, \nParsons, \nGauss, & \nWathes, \n2012) \n39. \nControl of \nthe growth of \nbroiler \nchickens \nCollecting data \nfrom 8 rooms, \neach room \nhousing 262 \nbroilers, \nmeasuring bird \nweight, feed \namount, light \nintensity and \nrelative humidity. \nEstimation of \nthe weight of \nchicken (scalar \nvalue) \nN/A \nFirst-\norder \nDRNN \nDevelop\ned by \nthe \nauthors \nN/A \nN/A \nTested on \ndifferent \nrooms of \nchicken \nthan the \nones which \nwere used \nfor training \nMSE, \nMRE \n0.02 \n(MSE), \n1.8% \n(MRE)  in \nrelation to \na \ncontroller \nN/A \n(Demmer\ns T. G., et \nal., 2010) \n52 \n \n40. \nWeather prediction \nPredict \nweather \nbased on \nprevious \nyear’s \nconditions \nSyngenta Crop \nChallenge 2016 \ndataset, containing \n6,490 sub-regions \nwith three weather \ncondition attributes \nfrom the years \n2000 to 2015. \nPredicted \nvalues of \ntemperature, \nprecipitation \nand solar \nradiation \n(scalar value) \nN/A \nLSTM \nKeras \nN/A \nN/A \nSame \nN-\nRMSE, \nMRE \n78% \n(Temperat\nure), 73% \n(Precipitati\non), 2.8% \n(Solar \nRadiation) \nN-RMSE, \n1-3% \nMRE in all  \ncategories \nN/A \n(Sehgal, \net al., \n2017) \n \n1 \n \n \n2 \n53 \n \nAppendix III: Publicly-available datasets related to agriculture. \nNo. \nOrganization/Dataset \nDescription of dataset \nSource \n1. \nImage-Net Dataset \nImages of various plants (trees, vegetables, flowers) \nhttp://image-net.org/explore?wnid=n07707451  \n2. \nImageNet Large Scale Visual \nRecognition Challenge (ILSVRC) \nImages that allow object localization and detection \nhttp://image-net.org/challenges/LSVRC/2017/#det  \n3. \nUniversity of Arcansas, Plants \nDataset \nHerbicide injury image database \nhttps://plants.uaex.edu/herbicide/  \nhttp://www.uaex.edu/yard-garden/resource-library/diseases/  \n4. \nEPFL, Plant Village Dataset \nImages of various crops and their diseases \nhttps://www.plantvillage.org/en/crops  \n5. \nLeafsnap Dataset \nLeaves from 185 tree species from the Northeastern \nUnited States. \nhttp://leafsnap.com/dataset/  \n6. \nLifeCLEF Dataset \nIdentity, geographic distribution and uses of plants \nhttp://www.imageclef.org/2014/lifeclef/plant  \n7. \nPASCAL Visual Object Classes \nDataset \nImages of various animals (birds, cats, cows, dogs, \nhorses, sheep etc.) \nhttp://host.robots.ox.ac.uk/pascal/VOC/  \n8. \nAfrica Soil Information Service \n(AFSIS) dataset \nContinent-wide digital soil maps for sub-Saharan Africa http://africasoils.net/services/data/  \n9. \nUC Merced Land Use Dataset \nA 21 class land use image dataset \nhttp://vision.ucmerced.edu/datasets/landuse.html  \n10. \nMalayaKew Dataset \nScan-like images of leaves from 44 species classes. \nhttp://web.fsktm.um.edu.my/~cschan/downloads_MKLeaf_d\nataset.html  \n11. \nCrop/Weed Field Image Dataset \nField images, vegetation segmentation masks and \ncrop/weed plant type annotations. \nhttps://github.com/cwfid/dataset  \nhttps://pdfs.semanticscholar.org/58a0/9b1351ddb447e6abd\nede7233a4794d538155.pdf  \n12. \nUniversity of Bonn \nPhotogrammetry, IGG \nSugar beets dataset for plant classification as well as \nlocalization and mapping. \nhttp://www.ipb.uni-bonn.de/data/  \n13. \nFlavia leaf dataset \nLeaf images of 32 plants. \nhttp://flavia.sourceforge.net/  \n14. \nSyngenta Crop Challenge 2017 \n2,267 of corn hybrids in 2,122 of locations between \n2008 and 2016, together with weather and soil \nconditions \nhttps://www.ideaconnection.com/syngenta-crop-\nchallenge/challenge.php  \n \n1 \n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2018-07-31",
  "updated": "2018-07-31"
}