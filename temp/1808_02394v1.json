{
  "id": "http://arxiv.org/abs/1808.02394v1",
  "title": "Application of End-to-End Deep Learning in Wireless Communications Systems",
  "authors": [
    "Woongsup Lee",
    "Ohyun Jo",
    "Minhoe Kim"
  ],
  "abstract": "Deep learning is a potential paradigm changer for the design of wireless\ncommunications systems (WCS), from conventional handcrafted schemes based on\nsophisticated mathematical models with assumptions to autonomous schemes based\non the end-to-end deep learning using a large number of data. In this article,\nwe present a basic concept of the deep learning and its application to WCS by\ninvestigating the resource allocation (RA) scheme based on a deep neural\nnetwork (DNN) where multiple goals with various constraints can be satisfied\nthrough the end-to-end deep learning. Especially, the optimality and\nfeasibility of the DNN based RA are verified through simulation. Then, we\ndiscuss the technical challenges regarding the application of deep learning in\nWCS.",
  "text": "1\nApplication of End-to-End Deep Learning in\nWireless Communications Systems\nWoongsup Lee, Ohyun Jo, and Minhoe Kim\nAbstract\nDeep learning is a potential paradigm changer for the design of wireless communications systems\n(WCS), from conventional handcrafted schemes based on sophisticated mathematical models with\nassumptions to autonomous schemes based on the end-to-end deep learning using a large number\nof data. In this article, we present a basic concept of the deep learning and its application to WCS\nby investigating the resource allocation (RA) scheme based on a deep neural network (DNN) where\nmultiple goals with various constraints can be satisÔ¨Åed through the end-to-end deep learning. Especially,\nthe optimality and feasibility of the DNN based RA are veriÔ¨Åed through simulation. Then, we discuss\nthe technical challenges regarding the application of deep learning in WCS.\nI. INTRODUCTION\nThe deep learning, which is based on the deep neural network (DNN) that emulates the\nneurons of the brain, has gained in great popularity in recent days. The current enthusiasm for\ndeep learning is mainly due to its signiÔ¨Åcant performance gains over conventional schemes [1]‚Äì\n[3]. For example, the deep learning based image classiÔ¨Åcation schemes can achieve far more\naccurate performance than handcrafted conventional schemes based on the analytic models, and\nthey have even surpassed the human-level performance. The application of deep learning is\nnot conÔ¨Åned to the simple classiÔ¨Åcation task but it also shows notable performance in more\ncomplicated tasks, such as the semantic scene understanding [2].\nThe advent of deep learning can change the research paradigm from designing scheme through\ncareful engineering based on mathematical models to end-to-end learning based scheme in which\nW. Lee is with the Department of Information and Communication Engineering, Gyeongsang National University, South\nKorea. O. Jo is with the Department of Computer Science, Chungbuk National University, South Korea. M. Kim is with the\nDepartment of Communication Systems, EURECOM, 06410 Sophia-Antipolis, France. This work has been submitted to the\nIEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\narXiv:1808.02394v1  [cs.IT]  7 Aug 2018\n2\nDNN\n- Learn optimal output directly from input data\nInput: \n- Channel gain\n- Traffic pattern\n- Mobility of user\n- etc.\nOutput: \n- Resource allocation\n- Admission control\n- Encoding/decoding\n- etc.\nAnalytical \nsystem model\nIteration is often \nneeded for \nconvergnece\nMathematical tools: \n- Game theory\n- Convex optimization\n- etc.\nMathematical \nassumptions for \ntractable analysis\n(a) Conventional approach\n(b) End-to-end deep learning based approach\nInput: \n- Channel gain\n- Traffic pattern\n- Mobility of user\n- etc.\nOutput: \n- Resource allocation\n- Admission control\n- Encoding/decoding\n- etc.\nAnalytical \nformulation\ny = f(x)\nFig. 1: Comparison of end-to-end deep learning based approach with conventional approach.\nthe proper scheme is autonomously designed by observing a large amount of data, cf. Fig. 1.\nFor example, conventional image classiÔ¨Åcation schemes rely on the handcrafted complex feature\ndetectors which are engineered by the expertise, e.g., edge detector. However, in case of deep\nlearning based image classiÔ¨Åcation, feature detectors that are far more accurate than conventional\ndetectors can be derived by a DNN structure from a large number of image data. Accordingly,\nin the era of deep learning, 1) the preparation, selection and pre-processing of data to be used\nin DNN structure, 2) the determination of proper DNN structure and 3) the interpretation of\nthe output of DNN, become more important than the development of analytic schemes from the\nmathematical system model which usually contains assumption to make analysis feasible.\nIn recent days, the deep learning has begun to be applied to the many research areas of\nwireless communications systems (WCS), especially in the classiÔ¨Åcation tasks such as trafÔ¨Åc and\nchannel estimation. The authors of [4] showed that the type of data trafÔ¨Åc can be determined\naccurately with DNN. Moreover, in [5], the channel estimation and signal detection in orthogonal\nfrequency-division multiplexing (OFDM) systems was considered based on the deep learning.\nFurthermore, the deep learning is also applied to more complicated tasks than just classiÔ¨Åcation,\ne.g., the encoder and decoder for sparse code multiple access (SCMA), were developed using a\nDNN based autoencoder in [6].\nOne interesting characteristic of deep learning is that the DNN can be considered as a universal\napproximator [7] which is capable of approximating an arbitrary function such that it can emulate\n3\nthe behavior of highly nonlinear and complicated systems. Moreover, given that the DNN can\nbe trained in an end-to-end manner which treats the operation as a black box, i.e., end-to-end\ndeep learning, the use of DNN enables the exploit of the optimal strategy without solving the\ncomplicated problems explicitly, cf. Fig. 1. In this sense, the deep learning has been applied\nfor the resource allocation (RA) of WCS where the exploitation of optimization problems was\ntaken into account previously. Unlike conventional approaches which derive the optimal strategy\nfor RA from the analytic system model with assumptions, the deep learning based approach\ncan derive the optimal strategy directly from actual channel data such that it can adapt to the\nenvironment and the performance is likely to be higher in practice. Moreover in the deep learning\nbased approach, the general solver for the optimization problem of RA can be derived such that\nthe optimal strategy can be found with low computation time even when the parameters, e.g.,\nchannel gain, change [5], [8]. The authors of [7] used a DNN to regenerate the transmit power of\nweighted minimum mean square error (WMMSE)-based scheme in order to resolve the problem\nof high computation time of the WMMSE-based scheme. Moreover, in [8], the transmit power\nwas optimized to maximize either spectral efÔ¨Åciency (SE) or energy efÔ¨Åciency (EE) where\nthe optimal strategy for transmit power control is learned through an end-to-end deep learning\nwithout needing to derive explicit mathematical formulation.\nIn this article, we focus on the application of deep learning in the WCS. To this end, we Ô¨Årst\naddress basic principles of deep learning. Then, we investigate the DNN architecture which can\nbe trained to derive general strategy for RA that can achieve diverse goals, i.e., maximization\nof SE, EE, and minimization of transmit power, while satisfying constraints on interference\nand quality-of-service (QoS). Finally, the research challenges regarding the application of deep\nlearning will be addressed before concluding the article.\nII. FUNDAMENTALS OF DEEP LEARNING\nIn this section, we Ô¨Årst describe the basic component and structure of Neural network (NN)\nand how to train it. Then, we turn our attention to deep learning and its application to WCS.\nA. Structure of Neural Network\nNN which comprises DNN, is a subcategory of machine learning which mimics the operation\nof brain. Through the extensive experiment, it was found that the brain is composed of neurons\nwhich are connected to each other. The neuron takes the outputs of other neurons as its input\n4\n‚àë \nx1\nx2\nx3\nxK\n‚àë \n‚àë \nActivation\nfunction\n‚Ä¶\n‚Ä¶\nFig. 2: Basic structure of NN.\nand activates its output when the inputs satisfy a certain condition, i.e., it can be considered as a\nbiological switch. In the NN, the connection between neurons (nodes) is modeled by the matrix\nmultiplication and the activation of neurons is modeled by the functions which look similar to a\nstep function, e.g., sigmoid and rectiÔ¨Åed linear unit (ReLU), which provide the ability to model\nnon-linear characteristic to the NN. The basic structure of NN is shown in Fig.2. Accordingly,\nthe NN can be considered as the collection of matrix computation and activation functions\nand the calculation of output of NN for one sample data, i.e., inference, usually requires low\ncomputational overhead.\nAccording to the existence of feedback loop in the network, the NN can be divided into two\ncategories.\n‚Ä¢ Feedforward neural network (FNN): In FNN, there is no feedback loop such that the\nprevious input data would not affect the current output, i.e., memoryless. The FNN is used\nwhen the data is not correlated to each other, e.g., image data. In WCS, this type of NN is\nused for the spectrum sensing and power control of users [7]‚Äì[9].\n‚Ä¢ Recurrent neural network (RNN): The connection of nodes in RNN forms a cycle such\nthat the current input can affect the output of the next input, i.e., the DNN has memory.\nAccordingly, this type of NN is used for the data which has a temporal correlation. The\ndata with temporal correlation in WCS, e.g., channel estimation, has been dealt with the\nRNN [4].\n5\nB. Overview on Training of Neural Network\nThe training of NN, i.e., determination of weights and biases of NN, is not obvious. In fact,\nthe lack of appropriate way to Ô¨Ånd weights and biases has brought the Ô¨Årst depression of research\non NN after its Ô¨Årst development in 1950s. In 1980s, an efÔ¨Åcient way to train NN, i.e., back-\npropagation algorithm, has been developed [10] which leads the second booming of the research\non NN. The back-propagation algorithm is based on the gradient descent technique such that the\nerror of the NN, e.g., difference between the target and the output, is propagated in backward\ndirection to update weights and biases according to the gradient, e.g., strengthen the parameters\nwhen the output is correct and weakening the parameters when the output is incorrect. By using\nthe back-propagation algorithm, the NN can be trained efÔ¨Åciently.\nAccording to the training methodology, the NN can be divided into three categories, same as\ngeneral machine learning algorithms.\n‚Ä¢ Supervised learning: In the supervised learning, the training data is labeled such that the\nNN can compare its output with the ground truth. This type of learning is widely used in\nthe classiÔ¨Åcation, e.g., the detection of primary user in cognitive radio (CR) systems [9].\n‚Ä¢ Unsupervised learning: In the unsupervised learning, the data is not labeled such that\nthe NN should autonomously derive the meaningful features from the input samples, e.g.,\nclustering. In [11], the encoder and decoder for SCMA system has been found using the\nautoencoder structure in unsupervised learning.\n‚Ä¢ Reinforcement learning: In the reinforcement learning, the learning of NN is conducted\nby trial-and-error. Especially, for a given input data, the proper action can be found through\nNN and reward can be observed for the selected action. Then, the NN is trained to provide\nbetter action which gives higher reward. Anti-jamming strategy for secondary users in CR\nsystems was developed based on deep reinforcement learning in [12].\nC. Deep Neural Network\nWith the back-propagation algorithm, the NN can be trained efÔ¨Åciently. Nonetheless, it was\nobserved that the NN with a large number of layers which is essential to achieve human-like\nfunctionality, is hard to be trained, mainly due to the vanishing gradient problem, and the lack\nof proper input data and computation power, and it results in the second depression of research\non NN for about 20 years.\n6\nRecently, the use of NN with a large number of layers, which is known as DNN, becomes\npossible, mainly due to the following four reasons [3].\n‚Ä¢ Availability of large dataset: Due to the development of sensors and Internet, the collection\nof a large number of data is enabled which is essential for the DNN to learn general features.\nFor instance, the development of ImageNet, which is a database of image that contains more\nthan 15,000,000 labeled images, becomes the basis for the big success of DNN in the image\nclassiÔ¨Åcation.\n‚Ä¢ Use of better activation function: The problem of vanishing gradient is mainly caused by\nthe use of inefÔ¨Åcient activation functions whose gradient is smaller than 1, e.g., sigmoid.\nHowever, in the DNN, the activation function with better gradient characteristics, e.g., ReLU,\nis used such that the error at the output can be properly propagated through the layers.\n‚Ä¢ Higher computation power: Although the inference of output for one input data requires\nsmall computational overhead, the training can take a long computation time due to the large\nnumber of training data. However, thanks to the development of parallel computation based\non graphics processing unit, this training procedure can be conducted with low computation\ntime.\n‚Ä¢ EfÔ¨Åcient initialization methodology: In the DNN, the initialization technique is utmost\nimportant to achieve high performance without falling into poor local minimum. Recently,\ngood initialization techniques such as Restricted Boltzmann Machine (RBM) and Xavier\ninitialization enable the efÔ¨Åcient training of DNN [13].\nD. Application of Deep Neural Network in Wireless Communications Systems\nThe DNN is well suited for WCS because the WSC usually deals with a large amount of data\nand has complex system model which is hard to analyze at hand, and its potential application in\nWCS is enormous. Especially, unlike conventional approach in WCS where the speciÔ¨Åc channel\nmodel is assumed such that its performance cannot be guaranteed when the assumed channel\nis different from the actual channel, the DNN based approach is able to adapt its operation\naccording to the environment without relying on the speciÔ¨Åc channel model. Moreover, same as\nimage classiÔ¨Åcation, the DNN based approach might provide better schemes than conventional\nhandcrafted schemes, especially, when the problem is complex and hard to analyze by empirical\nformulation [6], [8], [9].\n7\nIII. RESOURCE ALLOCATION BASED ON END-TO-END DEEP LEARNING\nIn this section, we investigate the DNN based RA. To this end, we Ô¨Årst describe the considered\nsystem model which comprises underlay device-to-device (D2D) communication. Then, the\nproposed DNN model for RA whose objective is either maximizing SE, EE or minimizing\ntotal transmit power, is addressed and the optimality of the DNN based RA is examined through\nsimulation.\nA. System Model and Resource Allocation\nWe consider an RA for the multi-channel cellular system with underlay D2D communication\nwhere the data transmission of N D2D user equipments (DUEs) takes place simultaneously with\ncellular transmission, and users are randomly distributed over an area D √ó D. The channel gain\nbetween i-th transmitter and j-th receiver for channel m is denoted as hm\ni,j, where the index 0\nis assigned to the cellular user equipment (CUE) and base station (BS). The considered system\nmodel is depicted in Fig. 3.\nIn the considered RA, the transmit power of DUEs allocated to each channel, which we denote\nas P m\ni\nwhere m and i are the index of channel and DUE, respectively, has to be determined to\neither 1) maximize SE, 2) maximize EE, or 3) minimize total transmit power. In the RA, we\ntake into account three constraints. First, the transmit power allocated to each channel should be\nnon-negative and the sum of transmit power for a single DUE should not exceed the maximum\ntransmit power, PT (i.e., transmit power constraint). Second, the amount of interference caused\nto cellular transmission must be less than the threshold IT (i.e., interference constraint). Third,\neach D2D transmission should satisfy the minimum QoS requirement, i.e., the SE achieved by\neach DUE has to be larger than the threshold, RT (i.e., QoS constraint).\nGiven that the considered RA can be formulated into a non-convex problem, it is hard to\nderive the optimal RA analytically, therefore, iterative methods based on Lagrangian relaxation\nhave been considered previsouly [14]. However, this approach requires a number of iterative\ncomputations, which possibly increases the computation time [7], [8], such that the real time\noperation can be hindered. However, in the DNN based RA, the generic solver is derived\nautonomously through DNN which involves only simple matrix operations, so that the proper RA\nfor any channel condition can be found with a low computational complexity without iteration.\n8\n‚Ñé0,0\nùêæ \n‚Ñé0,0\nùêæ \n‚Ñé0,1\nùêæ \n‚Ñé0,N\nùêæ \n‚ÑéùëÅ,0\nùêæ \n‚ÑéN,N\nùêæ \n‚Ä¶ \n‚Ä¶ \n‚Ñé0,0\nùêæ \n‚Ñé0,0\nùêæ \n‚Ñé0,1\nùêæ \n‚Ñé0,N\nùêæ \n‚ÑéùëÅ,0\nùêæ \n‚ÑéN,N\nùêæ \n‚Ä¶ \n‚Ä¶ \n‚Ñé0,0\n2  \n‚Ñé0,0\n2  \n‚Ñé0,1\n2  \n‚Ñé0,N\n2  \n‚ÑéùëÅ,0\n2  \n‚ÑéN,N\n2\n‚Ä¶ \n‚Ä¶\n \n‚Ñé0,0\n2  \n‚Ñé0,0\n2  \n‚Ñé0,1\n2  \n‚Ñé0,N\n2  \n‚ÑéùëÅ,0\n2  \n‚ÑéN,N\n2\n‚Ä¶ \n‚Ä¶\n \np1\nK \np2\nùêæ\npN\nùêæ \np1\nK \np2\nùêæ\npN\nùêæ \np1\n2 \np2\n2 \npN\n2  \np1\n2 \np2\n2 \npN\n2  \nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nSoftmax\nChannel gain\nReLU\nReLU\nReLU\nReLU\n‚Ä¶ \nReLU\nReLU\nReLU\nReLU\n‚Ä¶ \nSigmoid\nSigmoid\nTotal Transmit Power Network\nSoftmax\nSoftmax\np1\n1 \np2\n1 \npN\n1\np1\n1 \np2\n1 \npN\n1\n√ó \nMax SE \nMax EE\nMin PW\n‚Ñé0,0\n1  \n‚Ñé0,0\n1  \n‚Ñé0,1\n1  \n‚Ñé0,N\n1  \n‚ÑéùëÅ,0\n1  \n‚ÑéN,N\n1\n \n‚Ä¶ \n‚Ä¶ \n‚Ñé0,0\n1  \n‚Ñé0,0\n1  \n‚Ñé0,1\n1  \n‚Ñé0,N\n1  \n‚ÑéùëÅ,0\n1  \n‚ÑéN,N\n1\n \n‚Ä¶ \n‚Ä¶ \nTnet determines \nthe total transmit \npower of each \nDUE\nPnet determines \nthe proportion of \npower allocated to \neach channel.\nForward pass\nBack-propagate error\nPower Allocation Network\nData pre-processing\nCUE\nCUE\nDUE receiver\nDUE receiver\nDUE transmitter\nDUE transmitter\nBS\nBS\nLoss function\nDifferent loss \nfunction is used  \nfor different goal\nSignal\nInterference\nFig. 3: Considered system model and DNN structure for the RA.\nB. Resource Allocation based on Deep Neural Network\nIn our DNN model, the total transmit power of each DUE and the proportion of transmit\npower allocated to each channel by individual DUE are found jointly using separate DNN\nmodule, which are the total transmit power network (Tnet) and the power allocation network\n(Pnet), cf. Fig. 3. The channel gain is pre-processed for better training performance such that it\nis converted to dB and normalized to have zero mean and unit variance, and these pre-processed\nchannel gain becomes the input of two networks, Tnet and Pnet.\nThe Tnet and Pnet are composed of sub-modules which comprised a FC layer and ReLU\nwhich is used as an activation function. Given that the each output of the Tnet should be less\nthan the maximum transmit power, PT, we have implemented the sigmoid which is multiplied\nby the PT at the end of the Tnet, where the output of sigmoid is between 0 and 1. On the other\nhand, the proportion of transmit power allocated to each channel for each DUE is determined by\nthe Pnet, such that we consider N softmax modules as the last layer of Pnet where each softmax\n9\nmodule has M outputs. Given that the sum of a single softmax module is 1, it is appropriate\nto model the proportion of transmit power allocated to channels for individual DUE. Finally, by\nmultiplying the outputs of both Tnet and Pnet, the transmit power of DUEs allocated to each\nchannel can be found. It should be noted that the same DNN structure with different values of\nweights and biases can be used for three different objectives such that it is efÔ¨Åcient in practice\nin terms of reusing the same DNN structure.\nIn order to Ô¨Ånd the optimal set of weights and biases, our proposed DNN model has to\nbe trained Ô¨Årst. To this end, the channel samples for training must Ô¨Årst be collected by either\nmeasurement or simulations using well-known channel model. Hybrid use of measurement data\nand synthetic data, where the DNN is initially trained with synthetic channel data, and then,\ntrained with a few actually measured data for Ô¨Åne tuning, which is known as a transfer learning,\nis also possible.\nAfter the channel samples are prepared, the proposed DNN can be trained through a back-\npropagation algorithm. For the training, in order to achieve goal while satisfying constraints,\nthe weighted sum of objective function and the functions on constraints is taken into account.\nAccordingly, in order to train DNN model to maximize the SE, the loss function, LSE, can be\nset as LSE = ‚àíP SEi + Œª1\nP tanh\n\u0000[Ik\nCUE ‚àíIT]+\u0001\n+ Œª2\nP tanh ([SEi ‚àíRT]+), where Œª1, Œª2 are\nthe controlling parameters, SEi is the SE of DUE i, and Ik\nCUE is the interference caused to CUE\nat channel k.\nAs can be seen from the formulation, the loss function increases as the SE of DUEs decreases\nand at the same time when the interference and QoS constraints1 are not satisÔ¨Åed. Given that\nthe DNN is trained to reduce the loss, through training, the SE of DUEs will be increased and\nthe violation of interference and QoS constraint will be reduced. Note that Œª1 and Œª2 determine2\nthe penalty for the violation of constraints.\nSimilarly, the loss functions to maximize EE, which we denote as LEE can be written as\nLEE = ‚àíP EEi +Œª1\nP tanh\n\u0000[Ik\nCUE ‚àíIT]+\u0001\n+Œª2\nP tanh ([SEi ‚àíRT]+) and the loss function to\nminimize the total transmit power, which we denote as LTP, can be written as LTP = P P m\ni +\nŒª1\nP tanh\n\u0000[Ik\nCUE ‚àíIT]+\u0001\n+Œª2\nP tanh ([SEi ‚àíRT]+). Although the considered loss functions are\n1Given that the transmit power constraint is always satisÔ¨Åed in our DNN model, we do not consider this constraint in the loss\nfunction.\n2For example, when the value of Œª1 is small, the interference constraint is barely considered in the loss function such that\nthe transmit power is learned without considering the inference caused to the CUE.\n10\ndifferent from loss functions which are commonly considered in the deep learning researches,\ne.g., cross entropy, the back-propagation based learning is still possible because all the loss\nfunctions are differentiable.\nAfter the proper loss function is determined, the training of DNN can be conducted efÔ¨Åciently\nusing off-the-shelf stochastic gradient descent algorithms, e.g., Adaptive Moment Estimation\nalgorithm. After training, the appropriate transmit power allocated to each channel can be\ndetermined by feeding the current channel gain to the trained DNN.\nC. Performance Evaluation\nWe compare the performance of the proposed DNN based RA with the optimal performance\nwhich is found through an exhaustive search. For the performance evaluation, we assume that\nthe number of DUE transmission pairs and the number of channels is two such that the Ô¨Ånding\noptimal performance through exhaustive search is computationally plausible. The maximum\ntransmit power and the circuit power are set to 23 dBm, the bandwidth is set to 10 MHz, the\nnoise density is set to -173dBm/Hz, IT = -55 dBm and RT = 3 bps/Hz. Moreover, the simpliÔ¨Åed\npath loss model with path loss coefÔ¨Åcient 103.453 and path loss exponent 3.8 is considered and an\nindependent and identically distributed (i.i.d.) circularly symmetric complex Gaussian (CSCG)\nrandom variable with a mean of zero and a variance of one is used for multipath fading.\nFor the DNN model, we assume that the number of layers for Tnet and Pnet is 4 and the number\nof hidden nodes for FC layer is 100. Moreover, 40,000 channel samples are used for the training\nand the learning rate are adaptively changed over training epoch. In the performance evaluation,\nwe examine the average SE, the average EE, and the total transmit power of each DUE of the\nDNN based RA whose objectives are the maximization of SE (Max. SE), the maximization of\nEE (Max. EE) and the minimization of total transmit power (Min. PW). Moreover, the optimal\nperformance of RA for considered goals is also examined. Although we do not show graphically,\nthe computation time of the DNN based RA was measured at 1.1 milliseconds while the time\nrequired to Ô¨Ånd the optimal solution is measured at 2743 milliseconds which reveals the beneÔ¨Åt\nof the DNN based RA.\nIn Figs. 4, 5, and 6, we show the average SE, the average EE, and the total transmit power of\nDUE, respectively, as a function of the size of area, D. As can be seen from the simulation results,\nthe DNN based RA achieves the close-to-optimal performance. For example, the SE of DNN\nbased RA for the SE maximization achieves 97% of the optimal SE. The outage probability that\n11\n30\n35\n40\n45\n50\n4\n6\n8\n10\n12\n14\n16\nSize of area, D (m)\nSpectral efficiency (bps/Hz)\n \n \nDNN (Max. SE)\nDNN (Max. EE)\nDNN (Min. PW)\nOptimal SE\nFig. 4: Spectral efÔ¨Åciency vs. size of area.\n30\n35\n40\n45\n50\n30\n40\n50\n60\n70\n80\n90\nSize of area, D (m)\nEnergy efficiency (bits/Hz/Joule)\n \n \nDNN (Max. SE)\nDNN (Max. EE)\nDNN (Min. PW)\nOptimal EE\nFig. 5: Energy efÔ¨Åciency vs. size of area.\n30\n35\n40\n45\n50\n0\n50\n100\n150\n200\nSize of area, D (m)\nTotal transmit power (mW)\n \n \nDNN (Max. SE)\nDNN (Max. EE)\nDNN (Min. PW)\nOptimal transmit power\nFig. 6: Total transmit power vs. size of area.\n12\ninterference or QoS constraint is violated is measured to be less than 2.1% which is sufÔ¨Åciently\nlow. Moreover, even when the constraint is violated, the level of violation is minor. These\nsimulation results afÔ¨Årm the applicability and beneÔ¨Åts of the DNN based RA because the DNN\nbased RA has much lower computation time compared with the optimal scheme.\nIV. RESEARCH CHALLENGES\nIn the following, we discuss some research challenges for future usage of deep learning in\nWCS.\nA. Measured Channel Data as Training Sample\nIn the deep learning based approach, the DNN autonomously Ô¨Ånds the optimal strategy or\nmeaningful features directly from data instead of using handcrafted mathematical system model.\nAccordingly, in order to achieve high performance in deep learning, a large number of training\nsamples regarding WCS, e.g., channel gain data, for different scenarios has to be collected\nthrough actual measurement3. The problem regarding the preparation of training data could be\nsolved by using the DNN based on a generative model, e.g., generative adversarial network\n(GAN), which shows big success in the image synthesis, to generate realistic synthetic training\ndata.\nB. Distributed Operation and Inaccurate Channel Information\nUnlike image classiÔ¨Åcation in which all input data can be easily obtained by the single node,\nin WCS, the input data of the DNN, e.g., channel gain, is hard to be obtained by the single\nnode which executes the DNN functionality due to high signaling overhead, especially when the\nnumber of users is large. Moreover, the input data of DNN for practical WCS is likely to contain\nerror due to the inaccurate measurement and delayed channel feedback. To solve the problem\nof distributed operation, the distributed deep learning can be considered [15] and to solve the\nproblem of inaccurate channel information, denoising autoencoder can be taken into account.\n3For example, the large set of labeled image data, i.e., ImageNet dataset, enables the big success of deep learning technology\nin the image classiÔ¨Åcation.\n13\nC. Computation Complexity\nThe control of WCS, e.g., RA, has to be conducted within a very short time period, i.e., several\nmilliseconds, due to the short frame length. Moreover, the deep learning based scheme for WCS\nshould have low computational complexity because it could be operated on the mobile device\nwhich has limited computation power. Although we show that the DNN based RA has a very\nlow computation time compared with the optimal scheme, storing the trained DNN model can\nbecome overhead to the mobile device. Moreover, online learning in which each mobile device\ntrains its own model based on the collected channel samples, can be taken into account in order\nto further improve the performance, and in this case, the overhead of DNN based schemes can\nbe large. This problem can be solved by using newly developed AI accelerator chip, e.g., tensor\nprocessing unit (TPU) by Google, which is likely to be implemented in mobile device because\nmore technologies based on deep learning are now applied to mobile devices.\nV. CONCLUSIONS\nIn this article, the application of DNN for WCS through autonomous end-to-end deep learning\nas opposed to conventional handcrafted engineering based on the mathematical modeling, has\nbeen discussed. In particular, the deep learning based RA has been addressed whose optimal\nstrategy is hard to be obtained in conventional approach. It has been conÔ¨Årmed by performance\nevaluation that the DNN based RA can achieve close-to-optimal performance with low compu-\ntation time for various objectives of RA reusing the same DNN structure. We have also outlined\nthe challenges regarding the application of deep learning in the research of WCS.\nREFERENCES\n[1] T. J. O‚ÄôShea, J. Corgan, and T. C. Clancy, ‚ÄúConvolutional radio modulation recognition networks,‚Äù in Proc. of EANN,\nAberdeen, U.K., Sep. 2016.\n[2] A. Karpathy and L. Fei-Fei, ‚ÄúDeep visual-semantic alignments for generating image descriptions,‚Äù in Proc. of IEEE CVPR,\nBoston, MA, USA, Jun. 2015.\n[3] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù Nature, vol. 521, no. 7553, pp. 436‚Äì444, May 2015.\n[4] T. J. O‚ÄôShea, S. HiteÔ¨Åeld, and J. Corgan, ‚ÄúEnd-to-end radio trafÔ¨Åc sequence recognition with deep recurrent neural\nnetworks,‚Äù arXiv preprint arXiv:1610.00564, 2016.\n[5] H. Ye, G. Y. Li, and B. H. Juang, ‚ÄúPower of deep learning for channel estimation and signal detection in OFDM systems,‚Äù\nIEEE Wireless Commun. Lett., vol. 7, no. 1, pp. 114‚Äì117, Feb. 2018.\n[6] M. Kim, N. I. Kim, W. Lee, and D. H. Cho, ‚ÄúDeep learning aided SCMA,‚Äù IEEE Commun. Lett., vol. 22, no. 4, pp.\n720‚Äì723, Apr. 2018.\n14\n[7] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos, ‚ÄúLearning to optimize: Training deep neural networks\nfor wireless resource management,‚Äù arXiv preprint arXiv:1705.09412, 2017.\n[8] W. Lee, M. Kim, and D. H. Cho, ‚ÄúDeep power control: Transmit power control scheme based on convolutional neural\nnetwork,‚Äù IEEE Commun. Lett., vol. 22, no. 6, pp. 1276‚Äì1279, 2018.\n[9] W. Lee, M. Kim, and D.-H. Cho, ‚ÄúDeep sensing: Cooperative spectrum sensing based on convolutional neural networks,‚Äù\narXiv preprint arXiv:1705.08164, 2017.\n[10] D. E. Rumelhart and J. L. McClelland, Learning Internal Representations by Error Propagation.\nMIT Press, 1987, pp.\n318‚Äì362.\n[11] M. Kim, N. I. Kim, W. Lee, and D. H. Cho, ‚ÄúDeep learning aided SCMA,‚Äù IEEE Commun. Lett., vol. 22, no. 4, pp.\n720‚Äì723, Apr. 2018.\n[12] G. Han, L. Xiao, and H. V. Poor, ‚ÄúTwo-dimensional anti-jamming communication based on deep reinforcement learning,‚Äù\nin Proc. of IEEE ICASSP, New Orleans, LA, USA, Mar. 2017.\n[13] X. Glorot and Y. Bengio, ‚ÄúUnderstanding the difÔ¨Åculty of training deep feedforward neural networks,‚Äù in Proc. of AISTATS,\nSardinia, Italy, May 2010.\n[14] Y. Jiang, Q. Liu, F. Zheng, X. Gao, and X. You, ‚ÄúEnergy-efÔ¨Åcient joint resource allocation and power control for D2D\ncommunications,‚Äù IEEE Trans. Veh. Technol., vol. 65, no. 8, pp. 6119‚Äì6127, Aug. 2016.\n[15] P. de Kerret and D. Gesbert, ‚ÄúRobust decentralized joint precoding using team deep neural network,‚Äù in Proc. of ISWCS,\nLisbon, Portugal, Aug. 2018.\n",
  "categories": [
    "cs.IT",
    "cs.LG",
    "eess.SP",
    "math.IT"
  ],
  "published": "2018-08-07",
  "updated": "2018-08-07"
}