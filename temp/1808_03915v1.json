{
  "id": "http://arxiv.org/abs/1808.03915v1",
  "title": "Addressee and Response Selection for Multilingual Conversation",
  "authors": [
    "Motoki Sato",
    "Hiroki Ouch",
    "Yuta Tsuboi"
  ],
  "abstract": "Developing conversational systems that can converse in many languages is an\ninteresting challenge for natural language processing. In this paper, we\nintroduce multilingual addressee and response selection. In this task, a\nconversational system predicts an appropriate addressee and response for an\ninput message in multiple languages. A key to developing such multilingual\nresponding systems is how to utilize high-resource language data to compensate\nfor low-resource language data. We present several knowledge transfer methods\nfor conversational systems. To evaluate our methods, we create a new\nmultilingual conversation dataset. Experiments on the dataset demonstrate the\neffectiveness of our methods.",
  "text": "Addressee and Response Selection for Multilingual Conversation\nMotoki Sato1∗\nHiroki Ouchi2,3\nYuta Tsuboi1∗\n1Preferred Networks, Inc.,\n2RIKEN Center for Advanced Intelligence Project\n3Tohoku University\nsato@preferred.jp, hiroki.ouchi@riken.jp, tsuboi@preferred.jp\nAbstract\nDeveloping conversational systems that can converse in many languages is an interesting chal-\nlenge for natural language processing. In this paper, we introduce multilingual addressee and\nresponse selection. In this task, a conversational system predicts an appropriate addressee and\nresponse for an input message in multiple languages. A key to developing such multilingual re-\nsponding systems is how to utilize high-resource language data to compensate for low-resource\nlanguage data. We present several knowledge transfer methods for conversational systems. To\nevaluate our methods, we create a new multilingual conversation dataset. Experiments on the\ndataset demonstrate the effectiveness of our methods.\n1\nIntroduction\nOpen-domain conversational systems, such as chatbots, are attracting a vast amount of interest and play\ntheir functional and entertainment roles in real-world applications. Recent conversational models are\noften built in an end-to-end fashion using neural networks, which require a large amount of training data\n(Vinyals and Le, 2015; Serban et al., 2016). However, it is challenging to collect enough data to build\nsuch models for many languages. Consequently, most work has targeted high-resource languages, such\nas English and Chinese (Shang et al., 2015; Serban et al., 2016).\nIn this work, we aim to develop multilingual conversational systems that can return appropriate re-\nsponses in many languages. Speciﬁcally, we assume the two types of systems: (i) language-speciﬁc\nsystems and (ii) language-invariant systems. A language-speciﬁc system consists of multiple conver-\nsational models, each of which returns responses in a corresponding language. By contrast, a language-\ninvariant system consists of a single uniﬁed model, which returns responses in all target languages. A\nkey to building these multilingual models is how to utilize high-resource language data to compensate for\nlow-resource language data. We present several knowledge-transfer methods. To the best of our knowl-\nedge, this is the ﬁrst work focusing on low-resource language enablement of conversational systems.\nOne challenge when developing conversational systems is how to evaluate the system performance.\nFor generation-based conversational systems, which generate each word for a response one by one,\nmany studies adopt human judgments. However, it is costly and impractical to adopt this evaluation\nmethod for multilingual systems, especially for minor-language systems. Thus, as a ﬁrst step, we develop\nretrieval-based conversational systems and evaluate the ability to select appropriate responses from a set\nof candidates.\nFig. 1 shows the overview of our multilingual responding systems.\nThis paper provides: (i) formal task deﬁnitions, (ii) several knowledge-transfer methods and (iii)\na multilingual conversation dataset. First, we introduce and formalize the two task settings: single-\nlanguage adaptation for language-speciﬁc systems and multi-language adaptation for language-invariant\nsystems (Sec. 4).\n∗This work was conducted when the ﬁrst author worked at Nara Institute of Science and Technology, and portions of this\nresearch were done while the third author was at IBM Research - Tokyo.\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nLicense details: http://\ncreativecommons.org/licenses/by/4.0/.\narXiv:1808.03915v1  [cs.CL]  12 Aug 2018\nLanguage-Specific System \nModel (En)\nI have a problem \nwhen I install os\n1. see this URL\n2. Itʼs already in os\nImam problem? \nKada instalirem OS\n1. pogledajte ovaj URL\n2. Već je u os\nGiven\nSelect\nModel (Hr)\nEnglish\nCroatian\nLanguage-Invariant System \nUnified Model\n1. see this URL\n2. Itʼs already in os\n1. pogledajte ovaj URL\n2. Već je u os\nI have a problem \nwhen I install os\nImam problem? \nKada instalirem OS\nEnglish\nCroatian\nGiven\nSelect\nFigure 1: Example of multilingual response selection.\nSecond, we present several methods leveraging high-resource language data to compensate for low-\nresource language in the two settings (Sec. 5). Our basic method uses multilingual word embeddings and\ntransfers source-language knowledge to target languages (Sec. 5.1 (a)). We also design three extended\nmethods. Among them, the ﬁne-tuning method builds a model speciﬁc to a single target language (Sec.\n5.2 (b)). The joint loss training and the multi-language adversarial training methods build a uniﬁed model\ninvariant for multiple target languages (Sec. 5.2 (c) and (d)).\nThird, we create a multilingual conversation corpus and dataset1 (Sec. 6).\nFrom the Ubuntu IRC Logs2, we collect the logs in 12 languages.\nTo show benchmark results, we perform experiments on the created dataset (Sec. 7). The results\ndemonstrate that our methods allow models to effectively adapt to low-resource target languages.\nIn particular, our method using Wasserstein GAN (Arjovsky et al., 2017) achieves high-performance\nfor simultaneously dealing with multiple languages with a single uniﬁed model.\n2\nRelated Work\nShort Text Conversation\nIn short text conversation, a system predicts an appropriate response for an input message in single-turn,\ntwo-party conversation (Ritter et al., 2011). One major approach to it is the generation-based approach,\nwhich generates a response using a sequence-to-sequence model (Shang et al., 2015; Vinyals and Le,\n2015; Serban et al., 2016; Li et al., 2016; Mei et al., 2017). Another popular approach is the retrieval-\nbased approach, which retrieves candidate responses from a repository and returns the highest scoring\none using a ranking model (Wang et al., 2013; Lu and Li, 2013; Ji et al., 2014; Wang et al., 2015). Lowe\net al. (2015) proposed next utterance classiﬁcation (NUC), in which a model has to select an appropriate\nresponse from a ﬁxed set of candidates.\nEvaluation for Conversational Systems\nEvaluation methods for conversational systems are an open question (Lowe et al., 2015; Liu et al., 2016;\nLowe et al., 2017).\nWhile many of previous studies on conversational systems used human judgements, automatic evalu-\nation methods are attractive because it is much easier and less costly to use. However, according to Liu\net al. (2016), for generation-based systems, automatic evaluation metrics, such as BLEU (Papineni et al.,\n2002), correlate very weakly with human judgements.\nFor retrieval-based systems, some studies used ranking-based metrics, such as mean average precision\nand accuracy (Ji et al., 2014; Wang et al., 2015). Lowe et al. (2016) conﬁrmed the feasibility of NUC\nas a surrogate task for building conversational systems. Although there are controversial issues for these\n1Our code and dataset are publicly available at https://github.com/aonotas/multilingual_ASR\n2http://irclogs.ubuntu.com/\nevaluation methods (Lowe et al., 2015), as a practical choice, we adopt the accuracy-based metric for\nevaluating multilingual conversational systems.\nAddressee and Response Selection\nNUC focuses on two-party, multi-turn conversation. As an extension of it, Ouchi and Tsuboi (2016)\nproposed addressee and response selection (ARS) for multi-party conversation. ARS integrates the ad-\ndressee detection problem, which has been regarded as a problematic issue in multi-party conversation\n(Traum, 2003; Jovanovi´c and Akker, 2004; Bohus and Horvitz, 2011; Uthus and Aha, 2013). Mainly,\nthis problem has been tackled in spoken/multimodal dialog systems (Jovanovi´c et al., 2006; Akker and\nTraum, 2009; Nakano et al., 2013; Ravuri and Stolcke, 2014). While these systems largely rely on acous-\ntic signal or gaze information, ARS focuses on text-based conversations. Extending these studies, we\ntackle multilingual, multi-turn, and multi-party text conversation settings.\nCross-Lingual Conversation\nThe motivation of our task is similar with that of Kim et al. (2016). They tackled cross-lingual dialog\nstate tracking in English and Chinese. While they transfer knowledge from English to Chinese, we\ntransfer knowledge between a high-resource and several low-resource languages.\n3\nAddressee and Response Selection\nAddressee and response selection (ARS)3, proposed by Ouchi and Tsuboi (2016), assumes the situation\nwhere a responding agent returns a response to an addressee following a conversational context.\nFormally, given an input conversational situation x ∈X, a system predicts y ∈Y , which consists of\nan addressee a and a response r:\nGIVEN : x = (ares, C, R),\nPREDICT : y = (a, r)\nwhere ares is a responding agent, C is a context (a sequence of previous utterances) and R is a set of\ncandidate responses. To predict an addressee a, we select an agent from a set of the agents appearing\nin a context A(C). To predict a response r, we select a response from a set of candidate responses\nR = {r1, · · · , r|R|}.\nThis task evaluates accuracy on the three aspects: addressee-response pair selection (ADR-RES), ad-\ndressee selection (ADR), and response selection (RES). In ADR-RES, we regard the answer as correct if\nboth the addressee and response are correctly selected. In ADR/RES, we regard the answer as correct if\nthe addressee/response is correctly selected.\n4\nMultilingual Addressee and Response Selection\nAs an extension of monolingual ARS, we propose multilingual addressee and response selection (M-\nARS). In ARS, a system is given as input a set of candidate responses and a conversational context in\na single language. By contrast, in M-ARS, a system receives the inputs in one of multiple languages.\nIn the following, we ﬁrst explain our motivation for tackling M-ARS and then describe the formal task\ndeﬁnitions.\n4.1\nMotivative Situations\nWe assume the two multilingual conversational situations:\n• You want to build language-speciﬁc systems, each of which responds in a single language.\n• You want to build one language-invariant system, which responds in multiple languages.\nThe ﬁrst situation is that we build K models, each of which is specialized for one of K target languages.\nThe second one is that we build one uniﬁed model that can deal with all K target languages. Taking these\nsituations into account, we present the corresponding two tasks: (i) single-language adaptation and (ii)\nmulti-language adaptation.\n3Due to the space limitation, we give a brief overview of ARS. For the complete task deﬁnition, please refer to Ouchi and\nTsuboi (2016).\n4.2\nTask Overview\nThe goal of single-language adaptation is to develop and evaluate a language-speciﬁc ARS model for a\nsingle target language. For example, using English, German and Italian training data, we build a model\nspecialized for German conversation. The goal of multi-language adaptation is to develop and evaluate\na language-invariant ARS model for multiple target languages. For example, using English, German and\nItalian training data, we build a model that can respond to not only German but also Italian and English\nconversation. In the following subsections, we formalize each of these tasks.\n4.3\nFormal Task Deﬁnition\nWe assume that we have conversation data in each of a set of languages K.\nTraining\nIn the training phase, a training dataset is given for each language k ∈K:\nD(k)\ntrain = { x(k)\ni\n, y(k)\ni\n}N(k)\ni=1 , k ∈K\nDtrain =\n[\nk\nD(k)\ntrain\nwhere x(k) and y(k) are a conversational situation and the target output in language k, respectively. We\ntrain a model F : X →Y on these training samples.\nEvaluation\nIn single-language adaptation, we evaluate a trained model for a single target language t ∈K. The\ntrained model receives an input of the target language, x(t) ∼D(t)\neval, and predicts ˆy(t). As evaluation\nmetrics, we use the three accuracies (ADR-RES, ADR and RES) used in ARS (Sec. 3).\nIn multi-language adaptation, given evaluation datasets for all the languages K, i.e., S\nk D(k)\neval, the\ntrained model receives an input of each language x(k) ∼D(k)\neval and predicts ˆy(k). As evaluation metrics,\nwe use macro average over all the languages: ADR-RES =\nP\nk ADR-RES(k)\n|K|\n. ADR and RES are also\ncomputed in the same way.\n5\nMethods\nIn this section, we ﬁrstly describe a model used for addressee and response selection, and then explain\nour proposed methods to train parameters of the model.\nOur model F consists of a feature extractor fE, addressee scoring function fA and response scoring\nfunction fR. fA and fR return relevance scores (probabilities) for an addressee and response:\nfA(x, ai)\n=\nσ([ares, hc]T Wa ai)\n(1)\nfR(x, rj)\n=\nσ([ares, hc]T Wr rj)\n(2)\nwhere ares is a responding agent vector, hc is a conversational context vector, ai is an agent vector, and\nrj is a candidate response vector. All these vectors are encoded by the feature extractor fE. We use the\ndynamic model (Ouchi and Tsuboi, 2016) as fE. Fig. 2 shows the overview of the dynamic model. This\nmodel represents each agent as a hidden state vector that dynamically changes along with time steps in\nGRU (Cho et al., 2014). 4\nA model F is parameterized by θ = {θE ∪{Wa, Wr}}, where θE is parameters of fE. To train\nthese parameters, we present four methods. These methods assume that we have training sets for a set of\nlanguages K: some of them are high-resource languages S ⊆K and others are relatively low-resource\nlanguages T = ¯S.\na3,0=0\na2,0=0\na1,0=0\na3,1 \na2,1 \na1,1 \na3,2 \na2,2 \na1,2 \na3,3 \na2,3 \na1,3 \nu1\nu2\nUser 1\nUser 2\nSYSTEM\nu3\nu4\nt=1\nt=2\nt=3\nt=4\nGRU\nt\nAgent\nUtterance\n1\nUser 1\nI have …\n2\nSYSTEM\nDid you …?\n3\nUser 2\nPlease show …\n4\nUser 1\nOK. …\n5\nSYSTEM\n[            ]\nUtterance Encoder (GRU)\nMax-Pooling\nUser 1\nUser 2\nUser 1\nResponse Candidate\n1. It seems that…\n2. How about…  ?\nSYSTEM\nFigure 2: Overview of Dynamic Model.\nFigure 3: Overview of our W-GAN training method\nfor multiple target languages.\n5.1\nA Basic Method\n(a) Multilingual Embedding Replacement\nThis method trains a model F on high-resource language data D(s)\ntrain, where s ∈S, and uses the trained\nmodel for responding conversations in other languages ¯S. To realize this transfer, we use multilingual\nembeddings.\nConsider the case where the high-resource language is English (En) and low-resource language is\nGerman (De). In the training phase, we use English word embeddings to train model parameters.5 In\nthe testing phase, instead of the English embeddings, we use German embeddings:\nTrain: w = W(En)\nemb w\nTest: w = W(De)\nemb w\nwhere w is a one-hot vector. We just replace the English word embeddings W(En)\nemb with the German\nones W(De)\nemb. After looking up each word embedding w, the neural model computes the hidden states.\nOne advantage of this method is to require no target language data. As multilingual embeddings, we use\nMultiCCA6 proposed by Ammar et al. (2016). In these embeddings, semantically similar words in the\nsame language or translationally equivalent words in different languages are projected onto nearby.\nBesides multilingual embeddings, another option to build a conversational model without no conver-\nsation data in a target language is to translate high-resource language data to low-resource one and train\na conversational model on the translations. One limitation of this approach is that it is costly to prepare\nparallel corpora for building the translation model. We discuss this approach in Sec. 7.3.\n5.2\nExtended Methods\nWe present the two types of methods which use target language data for building (i) language-speciﬁc\nmodels and (ii) language-invariant models.\n5.2.1\nMethods for Language-Speciﬁc Models\n(b) Fine-Tuning with Target Language Data\nTo compensate for the lack of the low-resource language data, this method ﬁrstly trains a model Fθ on\nhigh-resource language data (pre-training phase). Then, using the pre-trained parameters θ as the initial\nvalues, this method re-trains them on low-resource language data (ﬁne-tuning phase). We can expect that\nby gaining the better initial parameters, the tuning effectively adapts the model to the target language.\n4In this example, a responding agent vector ares is a3. Note that the states of the agents that are not speaking at the time are\nupdated by zero vectors.\n5The embeddings are ﬁxed, not ﬁne-tuned, during training.\n6The pre-trained MultiCCA embeddings are provided at http://128.2.220.95/multilingual/data/\n5.2.2\nMethods for Language-Invariant Models\nIn order to build language-invariant models, it is critical to consider the two perspectives: (i) avoid-\ning catastrophic forgetting and (ii) learning language-invariant features. Catastrophic forgetting (Kirk-\npatrick et al., 2017) is the phenomenon that a model forgets knowledge of previously trained tasks (lan-\nguages) by incorporating knowledge of the current task (language). Language-invariant features are the\nfeatures that are common and unchanged in different languages. Taking these two perspectives into\naccount, we present the following two methods.\n(c) Joint Loss Training\nThis method aims to avoid catastrophic forgetting by jointly training model parameters on all the lan-\nguage data at a time. Assuming that we have a set of languages K, this method uses the joint loss\nfunction: Jjoint(θ) = P\nk J (D(k), θ) where the loss function J is the cross-entropy loss used in Ouchi\nand Tsuboi (2016).\n(d) Multi-Language Adversarial Training\nTo learn language-invariant features, we use a framework of Wasserstein-GAN (W-GAN) (Arjovsky et\nal., 2017), a recently proposed technique to improve stability for generative adversarial nets (GANs)\n(Goodfellow et al., 2014). The aim of this method is to match the distributions of feature representations\nin two languages.\nFig. 3 illustrates an example. English is the high-resource language s ∈S, and German and Croatian\nare low-resource languages t ∈T . For each language, the feature extractor fE receives an input con-\nversation x and computes the hidden features h = fE(x)7. Thus, by using fE, we obtain the hidden\nfeature h(s) and h(t) for English and the others, respectively.\nA pair of the high- and low-resource language features h(s) and h(t) is given to a critic gπ to minimize\nthe Wasserstein distance between the distributions p(h(s)) and p(h(t)):\nW( p(h(s)), p(h(t)) ) = max\nπ\nEh(s)∼p(h(s))[gπ(h(s))] −Eh(t)∼p(h(t))[gπ(h(t))]\n(3)\nwhere the maximum is taken over the set of all 1-Lipschitz functions gπ.8 By maximizing this equation,\nthe distributions of the feature representations, p(h(s)) and p(h(t)), are made as close as possible. In this\npaper, as the critic gπ, we use multi-layer perceptron.\nEq. 3 is designed for the two distributions. Thus, we generalize this W-GAN equation to deal with |S|\nhigh-resource languages and |T | low-resource languages:\nJwgan(θ) =\nX\ns∈S\nX\nt∈T\nW( p(h(k)), p(h(ℓ)) )\nThis loss function Jwgan is integrated with the joint loss: Jadv(θ) = Jjoint(θ) + λ Jwgan(θ) where λ is a\nhyper-parameter that balances these loss functions and we used λ = 0.5.\n6\nCorpus and Dataset\nOne of our goals is to provide a multilingual conversation corpus/dataset that can be used over a wide\nrange of conversation research. We follow the corpus and data creation method of Ouchi and Tsuboi\n(2016). First, we crawl the Ubuntu IRC Logs9, and preprocess the logs in many languages. Each lan-\nguage is identiﬁed by using a language detection library (Nakatani, 2010). The resulting corpus consists\nof multilingual conversations in 12 languages, shown in Tab. 1.\nThen, we create an M-ARS dataset. For each language, we set the ground-truth/false addressees and\nresponses following Ouchi and Tsuboi (2016). Note that the addressed usernames in utterances have been\n7Hidden feature representation h is the concatenation of the responding speaker vector and context vector in Eqs. 1 and 2,\ni.e. h = [ares, hc].\n8A function g is 1-Lipschitz when |g(x)−g(y)| ≤|x−y| for all x and y. To constrain the critic g to a 1-Lipschitz function,\nthe parameters of g are clipped to a ﬁxed range.\n9We use a collection of the logs during one year (2015). We plan to expand it by collecting the logs over all the years.\nCorpus\nLanguage\nDocs\nUtters\nWords\nEnglish (en)\n7355\n2.4 M\n27.0 M\nItalian (it)\n357\n165 k\n1.1 M\nCroatian (hr)\n254\n80 k\n630 k\nGerman (de)\n248\n38 k\n335 k\nPortuguese (pt)\n211\n52 k\n285 k\nSlovenian (sl)\n179\n59 k\n357 k\nPolish(pl)\n67\n8.8 k\n51 k\nDutch (nl)\n57\n7.2 k\n75 k\nSpanish (es)\n36\n7.1 k\n49 k\nSwedish (sv)\n26\n1.7 k\n6.8 k\nRussian (ru)\n5\n0.3 k\n1.5 k\nFrench (fr)\n3\n0.5 k\n3.0 k\nTable 1: Statistics of M-ARS corpus.\nDataset\nLanguage\nTrain\nDev\nTest\nEnglish (en)\n665.6 k\n45.1 k\n51.9 k\nItalian (it)\n38,511\n2,561\n3,873\nCroatian (hr)\n11,387\n512\n1,145\nGerman (de)\n5,500\n354\n569\nPortuguese (pt)\n5,951\n285\n975\nTable 2:\nStatistics of the M-ARS dataset.\n|R| = 2\n|R| = 10\nSetting\nMethod\nADR-RES\nADR\nRES\nADR-RES\nADR\nRES\nSingle Language Adaptation\nCHANCE\n3.97\n7.94\n50.00\n0.80\n7.94\n10.00\nTF-IDF\n39.51\n64.97\n60.61\n12.54\n64.97\n18.50\nTRGONLY\n47.35\n69.27\n67.35\n19.42\n69.73\n26.13\nENONLY\n38.07\n65.72\n57.65\n8.50\n62.38\n13.75\nFINETUNE\n49.58\n69.59\n69.84\n21.15\n70.33\n28.15\nJOINT\n51.55\n70.30\n71.88\n22.32\n70.36\n29.38\nWGAN\n53.17\n70.99\n73.25\n23.34\n70.20\n30.39\nTwo Language Adaptation\nCHANCE\n2.30\n4.59\n50.00\n0.46\n4.59\n10.00\nTF-IDF\n38.32\n60.29\n64.25\n13.99\n60.29\n23.84\nENONLY\n46.77\n67.62\n67.90\n19.88\n65.86\n27.83\nFINETUNE\n50.98\n68.79\n72.60\n24.30\n68.89\n32.96\nJOINT\n53.37\n69.75\n74.94\n26.60\n69.75\n35.59\nWGAN\n54.14\n70.07\n75.63\n27.23\n69.76\n36.11\nFive Language Adaptation\nTF-IDF\n39.04\n63.10\n62.06\n13.12\n63.10\n20.64\nENONLY\n41.55\n66.48\n61.75\n13.05\n63.77\n19.38\nJOINT\n50.69\n69.00\n72.18\n22.80\n69.18\n31.11\nWGAN\n52.11\n69.74\n73.34\n23.39\n69.35\n31.88\nTable 3:\nResults for Single-/Two-/Five- language adaptation. Each number represents accuracy on\naddressee-response selection (ADR-RES), addressee selection (ADR) or response selection (RES).\nremoved for addressee selection. Thus, we have to predict the addressees without seeing the addressed\nusernames. The number of candidate responses (|R|) is set to 2 or 10. The dataset is then randomly\npartitioned into a training set (90%), a development set (5%) and a test set (5%). Tab. 2 shows the\nstatistics of the top 5 largest language sections of this resulting dataset.\n7\nExperiments\n7.1\nExperimental Setup\n7.1.1\nTask Settings\nWe use the following languages: (i) English (En) as the high-resource language, and (ii) Italian (It),\nCroatian (Hr), German (De), Portuguese (Pt) as the low-resource languages. In the following, we\ndescribe the languages used in each task.\n(a) Single-Language Adaptation\nTrain: English + 1 Low-Res. Language,\nDev & Test: 1 Low-Res. Language\nFor example, in the case that the target is Italian (It), we use the En and It training sets to train a\nmodel, and evaluate the trained model on the It test set. As evaluation metrics, we use the three types\nof accuracies, ADR-RES, ARD and RES (described in Sec. 3). We report the macro average accuracies\nof all source-target language pairs (En-It, En-Hr, En-De, and En-Pt).\n(b) Multi-Language Adaptation\nTrain: English + |T | Low-Res. Languages,\nDev & Test: English + |T | Low-Res. Languages\nWe use the En, It, Hr, Pt and De training sets to train a uniﬁed model, and evaluate it on the test\nsets for all the languages (En, It, Hr, Pt, De). As evaluation metrics, we use the macro averages of\nADR-RES, ARD and RES for all the languages. For example, for two language adaptation (|T | = 1),\nwe report the macro averages over all the language pairs (En-It, En-Hr, En-De, and En-Pt). For ﬁve\nlanguage adaptation (|T | = 4), we report the macro averages over all the ﬁve languages. Note that while\nwe evaluate the performance on only the test set of the target low-resource language in single-language\nadaptation, we evaluate it on the test sets of English and the low-resource languages in multi-language\nadaptation.\n7.1.2\nComparative Methods\nWe compare several methods. Our proposed methods (Sec. 5) are orthogonal, so that we can combine a\nmethod with others. In the following, we list the methods used in the comparison.\n• TRGONLY: A dynamic model proposed by Ouchi and Tsuboi (2016) trained on only the low-\nresource target language data.\n• ENONLY: A model built by (a) multilingual embedding replacement in Sec. 5.1: training a model\non the English data and replacing the English word embeddings with the embeddings of the low-\nresource language.\n• FINETUNE: A model built by (b) ﬁne-tuning in Sec. 5.2: training a model on the high-resource\nlanguage (English), and retraining it on the low-resource language.\n• JOINT: A model built by (b) ﬁne-tuning and (c) joint loss training: building a model by FINETUNE\nas an initial model, and retraining it with the joint loss functions.\n• WGAN: A model built by (b) ﬁne-tuning and (d) multi-language adversarial training: building a\nmodel by FINETUNE as an initial model, and retraining it with W-GAN.\nBesides the neural models, we also use the TF-IDF model used in Ouchi and Tsuboi (2016). This model\nﬁrstly creates TF-IDF vectors for the context and each candidate response. Then, it computes the cosine\nsimilarity for each pair of the context vector and a response vector. Finally, it selects the candidate\nresponse with the highest similarity.\n7.1.3\nOptimization\nWe use stochastic gradient descent (SGD) with a mini-batch method. To update parameters, we use\nAdam (Kingma and Ba, 2014). We describe the details of hyper-parameter settings in Supplementary\nMaterial.\n7.2\nResults\nTab. 3 shows the results of single-language and multi-language adaptation. Note that Tab. 6, Tab. 7, and\nTab. 8 shows the detailed results for each language.\nSingle-Language Adaptation\nWGAN achieved the best scores for most of the metrics. This suggests that the W-GAN method suc-\ncessfully transfers knowledge of high-resource language to a low-resource language. Also, FINETUNE\noutperformed TRGONLY. This means that pre-training parameters on the high-resource language data\nimproves a model for a target low-resource language. Interestingly, ENONLY achieved higher scores than\nchance-level without any target language data. One possible explanation is that the multilingual embed-\ndings have good alignments to some extent between similar meaning words in different languages.\n0\n5\n10\n15\n20\n25\n30\nPreTrainOnly\nFineTune\nJoint\nWGAN\nAccuracy\n(test on German) \n   Pre-train on English\n   Pre-train on translated German\n   German Only\nFigure 4: Effects of data augmentation with NMT.\n0\n100\n200\n300\n400\n500\n600\n0\n5\n10\n15\n20\n25\n30\n2\n3\n4\n5\n≧6\nAverage Number of Samples (Test)\nMacro-average  Accuracy\nAverage Number of Agents\nADR-RES\nTrgOnly\nEnOnly\nFineTune\nJoint\nWGAN\nFigure 5: Effects of the number of agents in the context. Left\naxis: ADR-RES accuracy on test sets (drawn as lines). Right\naxis: the average number of test samples (drawn as bars).\nMulti-Language Adaptation\nIn both two- and ﬁve-language adaptation, WGAN achieved the best scores. Speciﬁcally, In ﬁve language\nadaptation, regardless of using a single, uniﬁed model, JOINT and WGAN achieved high-performance.10\nAlso, WGAN outperformed JOINT in all the metrics. This suggests that WGAN learns language-\ninvariant features more effectively.\nIn NLP tasks, Chen et al. (2016) applied W-GAN to cross-lingual sentiment classiﬁcation and suc-\ncessfully transferred the source-side knowledge to the target one. In this paper, we have extended the\nadaptation of single source-target pair to the adaptation of multiple pairs. Our experimental results show\nthat our method works well for multi-language adaptation in conversation domain.\n7.3\nData Augmentation with NMT\nAs we mentioned in Sec. 5.1 (a), as another approach to compensating for low-resource language, we use\ndata augmentation. To increase the amount of the training set of a low-resource language, we translate\nhigh-resource language (English) samples to low-resource ones by using Neural Machine Translation\n(NMT). Although some translations are noisy, we can obtain much more training samples for a low-\nresource language. One limitation of this method is that it is costly to prepare parallel corpora, which\nis often unavailable for low-resource languages. For reproducibility, we use publicly available NMT\nmodels already trained on a parallel corpus. Since OpenNMT11 provides an English-German model, we\nconduct experiments for the English-German pair.\nWe investigate the effects of translated German data for pre-training a model. Translating English\n(En) training utterances to German (De) ones by using the trained NMT model, we can obtain translated\nGerman training data (De’). We compare the two settings: (i) pre-training a model on English data and\n(ii) pre-training a model on translated German data. After pre-training, we apply the methods used in\nSec. 7. We evaluate the performance with ADR-RES accuracy on the German test set.\nFig. 4 shows the results (|R| = 10). The red dotted line is the performance of the modes trained on\nonly original German training data (De). In each method, the blue bar at left hand is a model pre-trained\non English (En), and the green bar at right hand is a model pre-trained on (De’).\nPRETRAINONLY, the left-most method in Fig.4, uses the pre-trained models. The results were almost\nthe same between models pre-trained on English or translated German data, and worse than the model\ntrained on only the original German training data (red dotted line). This suggests that only the translations\nby NMT are not sufﬁcient for building good multilingual ARS models.\nFurthermore, we re-train the pre-trained models by using the three methods, FINETUNE, JOINT and\nWGAN. In other words, each method uses a pre-trained model as the initial model and re-trains the pa-\nrameters. In all methods, the models re-trained from the De’ pre-trained model (green bars) were better\n10Since FINETUNE builds a model for each target language, it cannot analyze multiple languages with a single model. That\nis why there is no result of FINETUNE in ﬁve-language adaptation.\n11http://opennmt.net/Models/\nthan the ones from the En pre-trained ones (blue bars). This suggests that by combining the NMT-based\ndata augmentation method with the knowledge-transfer methods, the performance is boosted. Another\npoint is that WGAN consistently outperformed the other methods, which supports the utility of WGAN.\n7.4\nAnalysis of Number of Agents\nWe investigate how accuracy ﬂuctuates according to the number of agents in the context of length 15, as\nshown in Fig. 5. Overall, as the number of agents increases, the accuracies of all the methods tend to\ndecline. Among them, WGAN achieved the best results in most of the cases. This suggests that WGAN\ncan stably predicts addressees and responses in conversations with many participants.\n8\nConclusion and Future Work\nWe have introduced multilingual addressee and response selection by providing (i) formal task deﬁni-\ntions, (ii) several knowledge-transfer methods and (iii) a multilingual conversation corpus and dataset.\nExperimental results have demonstrated that our methods allow models to adapt multiple target lan-\nguages. In particular, methods for language-invariant models can simultaneously deal with multiple\nlanguages with a single model.\nSince our methods and dataset can apply to response generation, tackling the multilingual response\ngeneration tasks is an interesting line of our future work. In addition, our language-invariant systems\ncan receive conversation in a language (e.g., English ) and reply to it in another language (e.g., German).\nIt can lead to interesting ﬁndings that our system is evaluated on code-mixing situations, where two or\nmore languages are used in the same context.\nReferences\n[Akker and Traum2009] Rieks Akker and David Traum. 2009. A comparison of addressee detection methods for\nmultiparty conversations. In Workshop on the Semantics and Pragmatics of Dialogue.\n[Ammar et al.2016] Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and\nNoah A. Smith. 2016. Massively multilingual word embeddings. CoRR.\n[Arjovsky et al.2017] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. 2017. Wasserstein gan. arXiv preprint\narXiv:1701.07875.\n[Bohus and Horvitz2011] Dan Bohus and Eric Horvitz. 2011. Multiparty turn taking in situated dialog: Study,\nlessons, and directions.\nIn Proceedings of the SIGDIAL 2011 Conference, SIGDIAL ’11, pages 98–109,\nStroudsburg, PA, USA. Association for Computational Linguistics.\n[Chen et al.2016] Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. 2016. Adversar-\nial deep averaging networks for cross-lingual sentiment classiﬁcation.\n[Cho et al.2014] Kyunghyun Cho, Bart van Merrienboer, aglar G¨ulehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statis-\ntical machine translation. In EMNLP.\n[Goodfellow et al.2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Proceedings of NIPS, pages\n2672–2680.\n[Ji et al.2014] Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An information retrieval approach to short text\nconversation. arXiv preprint arXiv: 1408.6988.\n[Jovanovi´c and Akker2004] Natasa Jovanovi´c and op den Rieks Akker. 2004. Towards automatic addressee iden-\ntiﬁcation in multi-party dialogues. In Proceedings of SIGDIAL.\n[Jovanovi´c et al.2006] Natasa Jovanovi´c, op den Rieks Akker, and Anton Nijholt. 2006. Addressee identiﬁcation\nin face-to-face meetings. In Proceedings of EACL.\n[Kim et al.2016] Seokhwan Kim, Luis Fernando D’Haro, Rafael E Banchs, Jason D Williams, Matthew Henderson,\nand Koichiro Yoshino. 2016. The ﬁfth dialog state tracking challenge. In Proceeding of Spoken Language\nTechnology Workshop (SLT), pages 511–517.\n[Kingma and Ba2014] Diederik P. Kingma and Jimmy Lei Ba. 2014. Adam: A method for stochastic optimization.\narXiv preprint arXiv: 1412.6980.\n[Kirkpatrick et al.2017] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the\nUnited States of America, pages 3521–3526.\n[Li et al.2016] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A persona-based\nneural conversation model. In Proceedings of ACL.\n[Liu et al.2016] Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.\n2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for\ndialogue response generation. In Proceedings of EMNLP, pages 2122–2132.\n[Lowe et al.2015] Ryan Lowe, Nissan Pow, Iulian V. Serban, and Joelle Pineau.\n2015.\nThe ubuntu dialogue\ncorpus: A large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of SIGDIAL,\npages 285–294.\n[Lowe et al.2016] Ryan Lowe, Iulian Vlad Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016.\nOn the evaluation of dialogue systems with next utterance classiﬁcation. In Proceedings of the 17th Annual\nMeeting of the Special Interest Group on Discourse and Dialogue, pages 264–269, Los Angeles, September.\nAssociation for Computational Linguistics.\n[Lowe et al.2017] Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Ben-\ngio, and Joelle Pineau. 2017. Towards an automatic turing test: Learning to evaluate dialogue responses. In\nProceedings of ACL, pages 1116–1126.\n[Lu and Li2013] Zhengdong Lu and Hang Li. 2013. A deep architecture for matching short texts. In Proceedings\nof NIPS, pages 1367–1375.\n[Mei et al.2017] Hongyuan Mei, Mohit Bansal, and Matthew R Walter. 2017. Coherent dialogue with attention-\nbased language models. In Proceedings of AAAI.\n[Nakano et al.2013] Yukiko I. Nakano, Naoya Baba, Hung-Hsuan Huang, and Yuki Hayashi. 2013. Implementa-\ntion and evaluation of a multimodal addressee identiﬁcation mechanism for multiparty conversation systems. In\nProceedings of the 15th ACM on International Conference on Multimodal Interaction, ICMI ’13, pages 35–42,\nNew York, NY, USA. ACM.\n[Nakatani2010] Shuyo Nakatani. 2010. Language detection library for java.\n[Ouchi and Tsuboi2016] Hiroki Ouchi and Yuta Tsuboi. 2016. Addressee and response selection for multi-party\nconversation. In Proceedings of EMNLP, pages 2133–2143.\n[Papineni et al.2002] Kishore Papineni, Salim E. Roucos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method\nfor automatic evaluation of machine translation. In ACL.\n[Ravuri and Stolcke2014] Suman V Ravuri and Andreas Stolcke. 2014. Neural network models for lexical ad-\ndressee detection. In Proceedings of INTERSPEECH, pages 298–302.\n[Ritter et al.2011] Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in\nsocial media. In Proceedings of EMNL, pages 583–593.\n[Serban et al.2016] Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau.\n2016. Building end-to-end dialogue systems using generative hierarchical neural network models. In Proceed-\nings of AAAI, pages 3776–3783.\n[Shang et al.2015] Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for short-text\nconversation. In Proceedings of ACL/IJCNLP, pages 1577–1586.\n[Traum2003] David Traum. 2003. Issues in multiparty dialogues. Advances in Agent communication, pages 201–\n211.\n[Uthus and Aha2013] David C Uthus and David W Aha. 2013. Multiparticipant chat analysis: A survey. Artiﬁcial\nIntelligence, pages 106–121.\n[Vinyals and Le2015] Oriol Vinyals and V. Quoc Le. 2015. A neural conversational model. arXiv preprint arXiv:\n1506.05869.\n[Wang et al.2013] Hao Wang, Zhengdong Lu, Hang Li, and Enhong Chen. 2013. A dataset for research on short-\ntext conversations. In Proceedings of EMNLP, pages 935–945.\n[Wang et al.2015] Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. 2015. Syntax-based deep matching of\nshort texts. In Proceedings of IJCAI, pages 1354–1361.\nA\nHyper-Parameters\nHyper-parameter\nValues\nEmbedding size\n512\nGRU state size\n256\nWGAN λ\n0.50\nWGAN iterations\n5\nCritic hidden size\n512\nCritic activation function\nReLU\nBatch size\n32\nMax epoch\n30\nAdam alpha\n{0.001, 0.0005, 0.0001}\nL2 weight decay\n{0.001, 0.0005, 0.0001}\nTable 4: Hyper-parameters for our experiments.\nB\nStatics of Dataset\nTab. 5 shows the details of our dataset. “Docs” is documents, “Utters” is utterances, “W. / U.” is the\nnumber of words per utterance, “A. / D.” is the number of agents per document.\nDataset (Train / Dev / Test)\nEnglish (En)\nItalian (It)\nCroatian (Hr)\nGerman (De)\nPortuguese (Pt)\nNo. of Docs\n7355\n357\n254\n248\n211\n(6,606/ 367/ 382)\n(306/ 17/ 34)\n(216/ 12/ 26)\n(216/ 12/ 20)\n(180/ 10/ 21)\nNo. of Utters\n2.4 M\n165 k\n80 k\n38 k\n52 k\n(2.1 M / 13.2 k / 15.1 k)\n(144 k / 7 k / 14 k)\n( 71 k / 3.4 k / 6.9 k)\n( 33 k / 1.9 / 2.9 k)\n( 44 k / 2.1 / 6.1 k )\nNo. of Words\n27.0 M\n1.1 M\n630 k\n335 k\n285 k\n(23.8 M/ 1.5 M/ 1.7 M)\n( 1.0 M/ 54 k/ 100 k)\n( 553 k/ 25 k/ 52 k)\n( 294 k/ 16 k/ 24 k)\n( 243 k/ 11 k/ 30 k)\nNo. of Samples\n-\n-\n-\n-\n-\n665.6 k/ 45.1 k/ 51.9 k\n38511 / 2561 / 3873\n11387 / 512 / 1145\n5500 / 354 / 569\n5951 / 285 / 975\nAvg. W. / U.\n11.1\n7.2\n7.5\n8.5\n5.2\n(11.1/ 11.2/ 11.3)\n( 6.9/ 7.7/ 7.1)\n( 7.7/ 7.3/ 7.5)\n( 8.9/ 8.4/ 8.2)\n( 5.5/ 5.2/ 4.9)\nAvg. A. / D.\n26.8\n25.6\n12.9\n16.4\n19.0\n(26.3/ 30.68/ 32.1)\n(24.9/ 26.2/ 25.6)\n(12.7/ 13.5/ 12.7)\n(17.4/ 15.9/ 15.8)\n(19.7/ 18.6/ 18.8)\nTable 5: Statistics of the multilingual dataset.\nC\nResults for each language\nTab. 6 shows the detailed results of single-language adaptation. Tab. 7 shows the detailed results of\ntwo-language adaptation. Tab. 8 shows the detailed results of ﬁve-language adaptation.\n|R| = 2\n|R| = 10\nTarget\n#Train\nMethod\nADR-RES\nADR\nRES\nADR-RES\nADR\nRES\nIt\n38,511\nCHANCE\n2.99\n5.97\n50.00\n0.60\n5.97\n10.00\nTF-IDF\n43.89\n67.49\n64.58\n16.63\n67.49\n23.42\nTRGONLY\n63.28\n79.86\n78.36\n32.87\n80.92\n38.73\nENONLY\n44.54\n72.37\n60.11\n9.91\n66.74\n16.29\nFINETUNE\n64.81\n80.79\n79.14\n34.57\n81.44\n41.26\nJOINT\n63.44\n79.81\n78.36\n34.37\n80.30\n40.82\nWGAN\n65.17\n80.56\n79.94\n35.71\n80.76\n42.14\nHr\n11,387\nCHANCE\n5.39\n10.78\n50.00\n1.08\n10.78\n10.00\nTF-IDF\n35.63\n58.78\n61.05\n10.22\n58.78\n17.29\nTRGONLY\n40.52\n63.06\n64.10\n14.32\n63.23\n22.97\nENONLY\n34.06\n60.87\n54.24\n7.95\n60.52\n12.31\nFINETUNE\n40.00\n62.62\n63.23\n14.67\n64.72\n22.79\nJOINT\n44.37\n62.97\n69.26\n15.98\n62.97\n24.54\nWGAN\n45.07\n62.62\n70.48\n16.59\n63.76\n25.68\nDe\n5,500\nCHANCE\n4.09\n8.17\n50.00\n0.82\n8.17\n10.00\nTF-IDF\n36.38\n64.67\n55.36\n10.19\n64.67\n14.41\nTRGONLY\n43.94\n67.49\n64.15\n16.52\n66.96\n22.50\nENONLY\n36.56\n63.27\n59.75\n5.98\n57.12\n12.13\nFINETUNE\n50.44\n68.89\n72.06\n20.39\n68.19\n26.71\nJOINT\n50.79\n70.30\n70.47\n21.79\n69.24\n27.94\nWGAN\n52.90\n71.53\n72.23\n22.50\n68.89\n28.30\nPt\n5,951\nCHANCE\n3.42\n6.84\n50.00\n0.68\n6.84\n10.00\nTF-IDF\n42.15\n68.92\n61.44\n13.13\n68.92\n18.87\nTRGONLY\n41.64\n66.67\n62.77\n13.95\n67.79\n20.31\nENONLY\n37.13\n66.36\n56.51\n10.15\n65.13\n14.26\nFINETUNE\n43.08\n66.05\n64.92\n14.97\n66.97\n21.85\nJOINT\n47.59\n68.10\n69.44\n17.13\n68.92\n24.21\nWGAN\n49.54\n69.23\n70.36\n18.56\n67.38\n25.44\nAvg.\n-\nCHANCE\n3.97\n7.94\n50.00\n0.80\n7.94\n10.00\nTF-IDF\n39.51\n64.97\n60.61\n12.54\n64.97\n18.50\nTRGONLY\n47.35\n69.27\n67.35\n19.42\n69.73\n26.13\nENONLY\n38.07\n65.72\n57.65\n8.50\n62.38\n13.75\nFINETUNE\n49.58\n69.59\n69.84\n21.15\n70.33\n28.15\nJOINT\n51.55\n70.30\n71.88\n22.32\n70.36\n29.38\nWGAN\n53.17\n70.99\n73.25\n23.34\n70.20\n30.39\nTable 6:\nResults for single-language adaptation.\nEach number represents accuracy on addressee-\nresponse selection (ADR-RES), addressee selection (ADR) or response selection (RES). #Train is the\nnumber of training data.\n|R| = 2\n|R| = 10\nTarget\nMethod\nADR-RES\nADR\nRES\nADR-RES\nADR\nRES\nEn, It\nCHANCE\n1.80 ( 0.62, 2.95)\n3.61\n50.00\n0.36 ( 0.12, 0.60)\n3.61\n10.00\nTF-IDF\n40.51 (37.13, 43.89)\n61.56\n66.24\n16.04 (15.44, 16.63)\n61.56\n26.31\nENONLY\n50.01 (55.47, 44.54)\n70.95\n69.13\n20.59 (31.27, 9.91)\n68.05\n29.11\nFINETUNE\n59.13 (53.44, 64.81)\n74.71\n77.78\n31.07 (27.56, 34.57)\n74.62\n39.34\nJOINT\n59.56 (55.67, 63.44)\n74.63\n78.50\n33.04 (31.71, 34.37)\n74.77\n41.72\nWGAN\n60.20 (55.23, 65.17)\n74.94\n79.10\n33.38 (31.04, 35.71)\n75.09\n41.87\nEn, Hr\nCHANCE\n2.35 ( 0.62, 5.39)\n4.71\n50.00\n0.47 ( 0.12, 1.08)\n4.71\n10.00\nTF-IDF\n36.76 (37.13, 35.63)\n60.15\n61.63\n12.82 (15.44, 10.22)\n60.15\n21.80\nENONLY\n44.77 (55.47, 34.06)\n65.20\n66.20\n19.61 (31.27, 7.95)\n64.94\n27.12\nFINETUNE\n46.03 (52.06, 40.00)\n65.29\n69.15\n20.96 (27.24, 14.67)\n66.01\n30.28\nJOINT\n49.49 (55.66, 43.32)\n65.98\n73.14\n22.95 (31.74, 14.15)\n66.15\n32.23\nWGAN\n49.80 (54.53, 45.07)\n65.70\n73.96\n23.61 (30.62, 16.59)\n66.38\n33.52\nEn, De\nCHANCE\n3.01 ( 0.62, 4.08)\n6.01\n50.00\n0.60 ( 0.12, 0.82)\n6.01\n10.00\nTF-IDF\n36.38 (37.13, 36.38)\n57.20\n64.47\n12.83 (15.44, 10.19)\n57.20\n23.24\nENONLY\n46.02 (55.47, 36.56)\n66.40\n68.95\n18.63 (31.27, 5.98)\n63.24\n27.03\nFINETUNE\n52.22 (53.99, 50.44)\n68.79\n74.52\n24.66 (28.93, 20.39)\n68.20\n33.09\nJOINT\n53.04 (55.28, 50.79)\n69.73\n74.30\n26.09 (31.97, 20.21)\n68.98\n35.14\nWGAN\n54.05 (55.20, 52.90)\n70.35\n75.19\n27.05 (31.42, 22.67)\n69.43\n35.31\nEn, Pt\nCHANCE\n2.02 ( 0.62, 3.42)\n4.04\n50.00\n0.40 ( 0.12, 0.68)\n4.04\n10.00\nTF-IDF\n39.64 (37.13, 42.15)\n62.27\n64.67\n14.29 (15.44, 13.13)\n62.27\n24.03\nENONLY\n46.30 (55.47, 37.13)\n67.94\n67.33\n20.71 (31.27, 10.15)\n67.24\n28.09\nFINETUNE\n46.53 (49.98, 43.08)\n66.39\n68.97\n20.52 (26.06, 14.97)\n66.74\n29.13\nJOINT\n51.39 (55.18, 47.59)\n68.66\n73.81\n24.32 (31.51, 17.13)\n69.11\n33.28\nWGAN\n52.49 (55.44, 49.54)\n69.31\n74.29\n24.88 (31.19, 18.56)\n68.16\n33.75\nAvg.\nCHANCE\n2.30\n4.59\n50.00\n0.46\n4.59\n10.00\nTF-IDF\n38.32\n60.29\n64.25\n13.99\n60.29\n23.84\nENONLY\n46.77\n67.62\n67.90\n19.88\n65.86\n27.83\nFINETUNE\n50.98\n68.79\n72.60\n24.30\n68.89\n32.96\nJOINT\n53.37\n69.75\n74.94\n26.60\n69.75\n35.59\nWGAN\n54.14\n70.07\n75.63\n27.23\n69.76\n36.11\nTable 7: Results for two-language adaptation. Each number is macro average of the accuracies over all\nthe languages. Each of the parenthesized numbers in the ADR-RES column is ADR-RES accuracy for\neach language.\n|R| = 2\n|R| = 10\nTarget\nMethod\nADR-RES\nADR\nRES\nADR-RES\nADR\nRES\nEn, It\nTF-IDF\n39.04\n63.10\n62.06\n13.12\n63.10\n20.64\n(37.13, 43.89, 35.63, 36.38, 42.15)\n(15.44, 16.63, 10.22, 10.19, 13.13)\nENONLY\n41.55\n66.48\n61.75\n13.05\n63.77\n19.38\n(55.47, 44.54, 34.06, 36.56, 37.13)\n(31.27, 9.91, 7.95, 5.98, 10.15)\nHr, De, Pt\nJOINT\n50.69\n69.00\n72.18\n22.80\n69.18\n31.11\n(54.49, 61.71, 43.41, 47.80, 46.05)\n(31.26, 30.29, 14.59, 20.21, 17.64)\nWGAN\n52.11\n69.74\n73.34\n23.39\n69.35\n31.88\n(53.88, 63.18, 44.19, 52.02, 47.28)\n(30.6, 31.29, 14.67, 22.32, 18.05)\nTable 8: Results for ﬁve-language adaptation. Each number is macro average of the accuracies over all\nthe languages. Each of the parenthesized numbers in the ADR-RES column is ADR-RES accuracy for\neach language.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-08-12",
  "updated": "2018-08-12"
}