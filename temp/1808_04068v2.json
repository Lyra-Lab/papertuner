{
  "id": "http://arxiv.org/abs/1808.04068v2",
  "title": "A Transfer Learning based Feature-Weak-Relevant Method for Image Clustering",
  "authors": [
    "Bo Dong",
    "Xinnian Wang"
  ],
  "abstract": "Image clustering is to group a set of images into disjoint clusters in a way\nthat images in the same cluster are more similar to each other than to those in\nother clusters, which is an unsupervised or semi-supervised learning process.\nIt is a crucial and challenging task in machine learning and computer vision.\nThe performances of existing image clustering methods have close relations with\nfeatures used for clustering, even if unsupervised coding based methods have\nimproved the performances a lot. To reduce the effect of clustering features,\nwe propose a feature-weak-relevant method for image clustering. The proposed\nmethod converts an unsupervised clustering process into an alternative\niterative process of unsupervised learning and transfer learning. The\nclustering process firstly starts up from handcrafted features based image\nclustering to estimate an initial label for every image, and secondly use a\nproposed sampling strategy to choose images with reliable labels to feed a\ntransfer-learning model to learn representative features that can be used for\nnext round of unsupervised learning. In this manner, image clustering is\niteratively optimized. What's more, the handcrafted features are used to boot\nup the clustering process, and just have a little effect on the final\nperformance; therefore, the proposed method is feature-weak-relevant.\nExperimental results on six kinds of public available datasets show that the\nproposed method outperforms state of the art methods and depends less on the\nemployed features at the same time.",
  "text": " \nA Transfer Learning based Feature-Weak-Relevant Method for \nImage Clustering \nBo Dong, Xinnian Wang \nDalian Maritime University \nDalian, China \n{dukedong,wxn}@dlmu.edu.cn  \nAbstract. Image clustering is to group a set of images into disjoint clusters in a way that images in \nthe same cluster are more similar to each other than to those in other clusters, which is an \nunsupervised or semi-supervised learning process. It is a crucial and challenging task in machine \nlearning and computer vision. The performances of existing image clustering methods have close \nrelations with features used for clustering, even if unsupervised coding based methods have \nimproved the performances a lot. To reduce the effect of clustering features, we propose a feature-\nweak-relevant method for image clustering. The proposed method converts an unsupervised \nclustering process into an alternative iterative process of unsupervised learning and transfer \nlearning. The clustering process firstly starts up from handcrafted features based image clustering \nto estimate an initial label for every image, and secondly use a proposed sampling strategy to \nchoose images with reliable labels to feed a transfer-learning model to learn representative features \nthat can be used for next round of unsupervised learning. In this manner, image clustering is \niteratively optimized. What's more, the handcrafted features are used to boot up the clustering \nprocess, and just have a little effect on the final performance; therefore, the proposed method is \nfeature-weak-relevant. Experimental results on six kinds of public available datasets show that the \nproposed method outperforms state of the art methods and depends less on the employed features \nat the same time. \nKeywords: Image Clustering; Feature-Weak-Relevant; Transfer Learning. \n1   Introduction \nCluster analysis [1] plays an important role in data mining and machine learning, whose mission is \nto partition objects into independent groups or clusters. Image clustering aims to classify images into \ncategories based on image similarity, and it is valuable for future studies especially in machine learning. \nDesirable clustering results can ensure efficiency of further explorations. There are many classic \nclustering methods, such as K-means [18], GMM [19], Mean-shift [45], Spectral Clustering [46], \nDBSCAN [47], etc. Whatever kinds of clustering methods, the employed clustering features still have \ngreat effects on clustering results.  \nTwo types of clustering features are always used for image clustering: \n Handcrafted features \nAccording to prior knowledge of the sampled images, handcrafted features, such as color histogram \n[48], SIFT [49], HOG [34], Gabor [33], and LBP [50], can be applied for a quick image clustering. \nHowever, the clustering results can be unsatisfying when the feature space is indistinguishable for part \nof clusters. When implemented with different features, image clustering performs unstably because of \nits strong relevance to the employed features.   \n Unsupervised encoding based features \nUnsupervised Learning can be used for image encoding/decoding. The encoded codes are usually \nrobust to noise and preserves information almost matches the original data. Therefore, the codes can be \nemployed in image clustering.  Unsupervised learning based clustering is mainly classified into two \ntypes: encoding based on linear models and encoding based on nonlinear models. The former type tends \nto map original data to a more distinguishable feature space by linear optimization. These methods can \nbe well performed in data mapping, but linear models may be too simple to handle various image sets \nfor clustering.  For the latter method, deep neural networks (DNNs) based unsupervised learning \ntechniques have been widely used for data preprocessing, especially DNNs based auto-encoding \ntechniques.  Though these techniques are good at data remapping and dimension reduction, there are \n \nno cooperative links between clustering and learning. That may lead to undesirable results at the same \ntime [31]. Therefore, it is considered that mapping and clustering can be implemented jointly. The \ntypical algorithms based on the ‘joint idea’ include JNKM [30], DEC [27] and DCN [31]. Actually two \ntypes of optimization objectives are considered as the minimum reconstruction errors and clustering \nlosses. However, there exists a contradiction in the joint process: the minimum reconstruction errors \nguarantee well preserved information including interference information of images, which may lead to \nbig clustering losses. At the same time, the joint process consumes too much time on DNN optimization, \nwhich limits further improvements in this field.  \nCompared with handcrafted features and unsupervised encoding based features, features learned by \nsupervised learning are more representative and discriminative, but the process of acquiring features \nneeds a large amount of labeled images which are not provided for image clustering. An alternative \nway is to generate clustering features using a Convolutional Neural Networks (CNN) [21] pre-trained \nwith labeled images. However, this idea may not work out well when the labeled and unlabeled images \nare independently distributed in feature space. Fortunately, fine-tuning the pre-trained CNN [20], as \none type of transfer learning methods, is a better way to overcome the shortcomings of direct features \ngeneration. At the same time, this process still needs supervised information such as labels.  \nOur idea is to use images with credible labels obtained by handcrafted features based clustering for \nbooting the transfer learning process, and the features learned by transfer learning can be used for \nanother round of clustering to get more images with credible labels which are used for next round of \ntransfer learning. In this manner, more and more sampled images with credible labels are obtained, and \nimage clustering can be iteratively optimized. An illustration of the idea is shown in Figure 1. To \nimplement the idea, we propose a transfer learning based feature-weak-relevant method for image \nclustering. The proposed method aims to make image clustering depend less on handcrafted features by \ntransforming an unsupervised learning process into an alternative iterative process of unsupervised \n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nCategory A\nCategory B\nCenters\n          \n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nCategory A\nCategory B\nCenters\n \n(a)                                           (b) \n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nCategory A\nCategory B\nCenters\n          \n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n \n \nCategory A\nCategory B\nCenters\n \n(c)                                             (d) \nFig. 1.  The illustration for Transfer Learning based Feature-Weak-Relevant  clustering method(circled \nsamples are considered to be with correct labels, and the others not): (a) initial feature distribution and the \nrelated cluster center in two different clusters; (b) the learned feature distribution after 1st iteration; (c) the \nlearned feature distribution after 2nd iteration; (d) the learned feature distribution after 3rd / final iteration of \ntransfer learning based clustering \n \nlearning and transfer learning. Experimental results on six kinds of public available datasets show that \nthe proposed method outperforms state of the art methods and depends less on the employed features at \nthe same time. \n2   Background and Related Work \nAssuming there is a set of images\n\n1,2,\n,\ni\ni\nN\n\n\nX\nI\n, where \nm n\ni\n\n\nR\nI\n, the process of image clustering \nis to participate X  into K  ( K\n\nZ  ) disjoint sets C , where \n\n\n1,\n,\n=\nk\nk\nK\n\nC\nC\n . At the same \ntime,\n1\nK\nk\nk\n\nC\nX  and \nr\nq\n\n\nC\nC\n when r\nq\n\n . For \ni X\nI\n, we use \n\n\n1,2\n,\niL\nK\n\n，\n to denote \nthe label of the cluster to which \niI  belongs. In the case of unsupervised image clustering,  \niL  is \nunavailable and should be predicted by the image clustering algorithm. \nTypically, a clustering model can be described as follows: \n\n\n,\n0,1\n1\nmin\n(\n,\n)\nK\nD K\ni\ni\nN\ni\nd\n\n\n\n\ni\nM R\ns\nf Ms\n                               (1) \nHere,\nD\ni \nf\nR  denotes descriptive features of \ni\nX . \n( )\ni\ni\ng\n\nf\nI\n , where \n( )\ng  denotes a feature \nextraction function. M  is a descriptive matrix for each cluster, and each column of M  denotes one \nkind of specified cluster information, such as the centroid. \nis  is a one-hot vector for a single cluster, \nand \n( )\nd  refers to the distance measurement model.  Most of cluster algorithms are implemented \nbased on (1). \nUnsupervised encoding based\n( )\ng  is mostly studied in mapping \niI  towards \n( )\nd -friendly spaces. \nSome of the encoding methods may introduce other constraints for mapping \n\n1,2,\n,\ni\ni\nN\n\n\n\nF\nf\n, which \nmakes \n( )\ng  more complex as shown in (2): \n[ ( )]\ni\ni\nt g\n\nf\nI\n                                  (2) \nwhere ( )\nt  is the mapping function. \n Typical linear-based \n( )\nt \ns include Principal Component Analysis (PCA) [9], Canonical \nCorrelation Analysis (CCA) [10], Nonnegative Matrix Factorization (NMF) [11] and Sparse \nRepresentation (SR) [12].  Nonlinear-based ( )\nt s include Stacked Auto Encoder (SAE) [13], Sparse \nAuto Encoder [14], Denoising Auto Encoder (DAE) [15] and Contractive Auto Encoder (CAE) [16]. \nAs shown in (1) and (2), there are no mathematical connections between feature extraction and \nclustering, which makes the clustering results not satisfying enough and extracted features less \nrepresentative. However, the mapping techniques are still popular among the researches on image \nclustering. Recently, instead of proposing the handcrafted dense/sparse feature models, in order to \nembed effective mathematical connections between features and clustering, researchers [27-29] have \nbeen studying on linear/nonlinear generative model from the latent space to the data domain while \nclustering.  \nOne of the most impressive achievements is shown in [30]. Yang et al. (2017) define (1) as follows: \n\n\n\n\n\n\n1\n2\n,\n0,1\n,\n,\n1\nmin\n(\n,\n)\nK\nD K\ni\nN\ni\ni\nd\nr\nr\n\n\n\n\n\n\n\n\n\n\nX\ni\nM R\ns\nW F\nf Ms\nWF\nF\nW                (3) \nwhere \n(\n)\n\n\nR\n is a parameter for balancing data fidelity and the latent cluster structure. \n1( )\nr  and \n2( )\nr  are pre-defined regularization parameters. In this way, \n( )\ng  and ( )\nt  can be simplified into a \ntrainable and question-related linear model.  \n For further study, Yang et al.(2017) believe the data generating process is too complex to be \nprecisely approximated based on a linear transformation, and the mapped features may not be \nrepresentative enough. Therefore, in [31], they introduce Deep Neural Networks (DNN) into (3) as \nfollows: \n\n\n,\n0,1\n1\nmin\n(\n( ( ),\n)\n( ( ( )),\n))\nK\nD K\ni\nN\ni\ni\ni\ni\ni\nd g\np g\n\n\n\n\n\n\n\nM R\ns\nI\nMs\nI\nI\n                   (4) \n( ) is the least-squared loss function for measuring the reconstruction error. \n( )\ng  and \n( )\np  \nrepresent the encoding part and the decoding part respectively. It is an excellent idea to perform \n \nclustering with dimensionality reduction based on nonlinear SAE jointly for handcrafted features are no \nlonger needed. SAE based (4) can automatically map from high-dimensional data to its latent space and \nthe loss function of K-means clustering guarantees that the mapped feature is suitable for effective \nclustering.   \nHowever, for all encoding based \n( )\ng s or ( )\nt s,  the encoded F  are redundant [17] because each \niI  consists of clustering relevant data \ni\nIg  and irrelevant data \ni\nIb  as shown in (5): \ni\ni\ni\n\n\nI\nIg\nIb                                  (5) \nHence, there exists a contradiction in (4): the minimum reconstruction error can guarantee F  \nmaintain all image information including interference features have negative effects on image \nclustering. It is an efficient  way to embed\n( )\ng  into clustering process as mentioned in the latest \nresearches [27~31], and there exists improvements in feature simplification and clustering. In addition, \nSAE based clustering is working with high time and memory consumption in DNN training. \n3   Transfer Learning based Feature-weak-relevant Image Clustering \n3.1    Formulations of the prototype \nWe hope images in the same cluster to be more similar to each other than to those in other clusters, \nand the cost function can be described as (6): \n\n\n,\n0,1\n( )\n1\n1\n1\n,\n1\n1\nmin\n( (\n,\n)\n( (\n,{ (\n}\n)\n( (\n,{ (\n)\n)\n)\n}\n)\n)\n)\nK\nD K\ni\nj\ni\ni\nj\nN\ni\ng\ni\nN\nN\nin\ni\nj\nL\nL\ni\nj\nN\nN\nout\ni\nj\nL\nL\ni\nj\nd g\nd\ng\ng\nd\ng\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM\ni\nR\ns\nI\nMs\nI\nI\nI\nI\n                      (6) \nwhere \n( )\nin\nd\n represents the model for measuring image distance in the same clusters and \n( )\nout\nd\n \nrepresents the model for measuring image similarity in different clusters. \n(\n)\n\n\nR\n and \n(\n)\n\n\nR\n  \nare weight parameters. \n( )\ntg  is an ideal trainable function for generating ideal features. However, (6) \nis unsolvable for image clustering because image clustering and representative feature learning are \ncause-and-effect related. \n  To gain clustering results with high accuracy, \n( )\ng  should be the same as \n( )\ntg , which is built or \ntrained with \n( )\nin\nd\n and \n( )\nout\nd\n at first; \n  To train \n( )\ng  well, large amounts of labeled images are required at first; \n  To predict effective cluster labels, \n( )\ng  must be defined at first. \nHowever, equation (6) can be solved in an iterative form by assuming an initial feature extraction \nfunction \n0( )\ng , and the iterative form of equation (6) is : \n\n\n-1\n,\n0,1\n(\nt\n,\n)\n1\nn\nou\nmin\n(\n(\n,\n)\n(\n(\n,{\n(\n}\n(\n(\n,{\n(\n}\n)\n)\n)\n)\n)\n)\n)\nK\nD K\ni\nt\nt\nt\nj\ni\nt\nt\nt\nt\nL\nL\ni\nj\nt\nt\nN\nt\nt\nt\ni\ng\ni\ni\nt\ni\nt\nj\nL\nL\ni\nj\nt\ni\nt\nj\ni\nj\nd g\nd\ng\ng\nd\ng\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM\ni\nR\ns\nv\nv\nv\nv\nI\nM s\nI\nI\nI\nI\n                      (7) \nwhere t\n\n\n denotes the iteration counter. \ntv  denotes the image sample set that have reliable labels \nat the tth round of iteration, and \n\n\n,\n1,2\n,\ni\nj\nt\nt\nL L\nK\n\n，\n denote labels of the sample images belonging to  \ntv . \n)\n(\ntg is the feature function to be learned.  \nThe alternative iteration solution of equation (7) is as follows: \n \n \n0( )\ng  is initialized to be a handcrafted feature extracting function, such as SIFT extractor, HOG \nextractor. \n \nt\nM  and \n\n\n1\n=\n,\n,\nN\nt\nt\nt\ns\ns\ns\n are obtained by minimizing the first term of equation (7), i.e.: \n\n\n\n\n\n-1\n,\n0,\n1\n1\nmin\n,\nK\nD K\ni\nt\nt\nt\nt\nt\ni\nN\ni\nd g\n\n\n\n\n\n\nM\ni\nR\ns\nM ,s\nI\nM s\n                       (8) \nAssuming \n\n*\n*\n1,2,\n,\n=\ni\ni\nN\nL\n\nL\n is the true labels of X , and \n\n*\n*\n1,2,\n,\nˆ\nˆ\n=\nt\nt\ni\ni\nN\nL\n\nL\n is the estimated labels of \nX , we can get \n*ˆtL  the clustering optimization. \n \ntv  is determined by a sampling function \n( )\n whose goal is to choose reliable labelled samples \nwith \nt\nM  and \nts  as input, i.e.: \n\n\nt\nt\n\n\nv\nM ,s\n                                  (9) \nOnce \ntv  is determined, \ntL is determined. We define \n( )\n mathematically to make sure part of the \nlabels \n*\n*ˆt\ni\ni\nL\nL\n\n or acceptably \n*\n*ˆt\ni\ni\nL\nL\n\n to solve (8). Here, \n\n\n{0,1}N\n\nv v\n . If \n1\niv \n, we ensure \n*\n*\nˆt\ni\ni\nL\nL\n\n acceptable . If \n0\niv \n, we believe that the predicted \n*ˆt\niL  is not reliable.  \n \n)\n(\ntg is learned by minimizing the 2nd and the third term of equation (7), which is to say: \n\nn\nout\n(\nmin\n(\n(\n,{ (\n}\n(\n(\n,\n)=\n(\n)\n)\n)\n)\n)\n)\n{ (\n}\n)\nt\nt\nt\nt\nL\nL\nj\ni\nt\ni\nj\nt\nt\nt\nt\nt\ni\nt\ni\nj\nt\ni\nj\nL\nL\ng\ni\nj\ni\nj\ng\nd\ng\ng\nd\ng\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\nv\nv\nv\nv\nI\nI\nI\nI\n   (10) \nAs long as labelled images are provided, equation (10) can be solved based on CNN classification \nmodels and this process is shown as follows: \n \n*ˆ\n(\n,\n)\nt\nt\ntrain\n\nNets\nX vL\n                              (11) \nt\nt \nX (X\nX)  denotes the reliable labelled samples at the tth round of iteration. Features that are more \nvalid can be generated by computing values from any layers of the fine-tuned net, which can be \ndescribed as follows: \n(\n( ,\n)\n0\n)\nt\ntg\ngetblob\nt\n\n\n\nX\nX Nets\nF\n                      (12) \nAlternatively, we can just use the trained model for updating \n*ˆtL  instead of (8) by prediction as (13) \nshows: \n*ˆ\n( ,\n)\npredict\n\nX Nets\nL\n                             (13) \nAs features are generated by Nets  based on (12), the updated \n*ˆtL  will be more reliable. We can \ntrain models to be more effective using more images with credible \n*ˆtL .  However, it may lead to an \nover fitted CNN model when only a small amount of images is supplied for training. Commonly, we \ncannot ensure the credible labelled images are enough or not. Thus, there exists a risk generating \nunsatisfying models when solve (9) directly. In order to avoid the problems of divergence and over-\nfitting caused by distribution of training data, the transfer learning method of fine-tuning a pre-trained \nclassification model is adopted. And pre-trained structures include LeNet [22], AlexNet [23], VGG \n[24], GoogleNet [25], ResNet[26] and others. \nAmong the above existing structures, AlexNet is one of the most popular structures. Though \nAlexNet only consists of 5 convolutional layers and 3 fully connected layers, it performs well in the \nILSVRC-2012 with a top-5 test error rate of 15.3%. Assuming the pre-trained AlexNet is ANets , \nmodel (11) can be then changed as follows: \nˆ\n(\n,\n,\n)\nt\nfinetune\nv\n\nNets\nX\nANets\nL\n                      (14) \nIn conclusion, we need to initialize X  first for booting transfer learning before the feature \noptimization and then implement the optimization by fine-tuning models and increasing the number of \nimages that satisfy\n*\n*\nˆt\ni\ni\nL\nL\n\n. Therefore, we define the initialization part as the boot stage, and the initial \nfeatures as the boot features. All other steps are called the main stage, and the final features can be \nmore question-relevant. At last, model (6) can be solved by iteratively updating \n*ˆtL  based on transfer \nlearning. \n \n3.2    \n( )\n Related Definitions \nAccording to the definition of\n( )\n, \niv  can be described as follows: \n*\n*\nˆ\nˆ\n(\n,{\n}\n)\nt\ni\nj\nt\ni\ni\nj\nL\nL\nv\nc\n\n\nf\nf\n                               (15) \nwhere \n( )\nc  is a function for measuring cohesion between \niI  and \njI  with the same label \n*ˆt\niL  . As \niv  is a binary label for grouping ˆL , \n( )\nc  can use the thresholding method for binarization as follows: \n*\n*\n*\n*\nˆ\nˆ\nˆ\nˆ\n1\n(\n,{\n}\n)\nΔ\n(\n,{\n}\n)\n0\nt\nt\nt\nt\ni\nj\ni\nj\nb\ni\nj\nL\nL\ni\nj\nL\nL\nc\nc\nelse\n\n\n\n\n\n\nf\nf\nf\nf\n                    (16) \nwhere \n( )\nbc  represents inner distance evaluation models for images with same labels. According to \nclustering theories, there is a probability that images with features that densely distributed around the \nsame center belong to the same class. The closer images are to the same center, the greater the above \nprobability will be. The closer images X  with \n1\nN\n\nv\n1\n are called Cluster Center Neighboring sets \n(CCNs) and the rest with \n2\nN\n\nv\n0\n are called Cluster Center Distant sets (CCDs). \n1\n2\n,\nN N\n\n, \nand\n1\n2\nN\nN\nN\n\n\n. A simple \n( )\nbc  can be defined as (17): \n*\n*\nˆ\nˆ\n(\n,{\n}\n)\nt\nt\ni\nj\nb\ni\nj\ni\nL\nL\nc\n\n\n\ni\nf\nf\nf\nMs\n                         (17) \nThere are many alternatives for distance measuring in (17), such as Chess Board Distance, City \nBlock Distance, etc.  \nIn (16), the threshold Δ  is important for binarization. A proper threshold should be given based on \nthe clustering features before optimization, and it varies with different types of handcrafted feature.  \nTherefore, a constant Δ  can be initialized easily, but it is not effective for all features. To avoid the \nissue, we define Δ  as a relative ratio that can be also initialized simply.  Instead of referring to the \nresults of \n( )\nbc  directly, we sort the results on same labels in descending order as (18) shows. \n*\n*\n*\nˆ\nˆ\nˆ\n({ }\n)\nt\nt\nt\nii\nii\nb\nL\nL\nsort c\n\n\nL\nρ\n                            (18) \nwhere \n*\nˆt\niiL\nρ\n is a vector containing the sorted distances.  We set \n*\n*\nˆ\nˆ\n(\n,{\n}\n)=1\nj\nt\nt\ni\nb\ni\nj\nL\nL\nc\n\nf\nf\n if \n*\n*\nˆ\nˆ\n(\n,{\n}\n)=1\nj\nt\nt\ni\nb\ni\nj\nL\nL\nc\n\nf\nf\n is in the top Δ%  of \n*\nˆt\niiL\nρ\n. In this way, Δ  is considered as a belief ratio \nindicating there are Δ%  CCNs in every cluster. The relative threshold is more robust to variation of \nthe employed handcrafted clustering features. \n3.3    Optimization Procedure in Main Stage \nIn section 3.1, we convert (6) into an iterative optimization model that can be solved by iteratively \nupdating \n*ˆtL . In order to make the optimization implementable, we need to add following three \nconstraints (assuming k  is the iteration counter, k\n\nZ ): \n  CCNs and CCDs can never be the same until the optimization is done, and \n1 1\n0\nk\nk\n\nv\nv \n. \n  The number of images in CCNs must be increasing during the optimization, and \n1\n1\n1\nk\nk\n\nv\nv\n. \n  The CCNs cannot be modified after confirmation, and \n1\n1\n1\n1\nk\nk\nk\n\n\n\nv\nv\nv\n. \n3.4    Algorithm Description \nBased on constrains in section 3.3 and the implementation analysis, we can update \n*ˆtL  as \nAlgorithm 1 shows. \nmax\nk\n represents the maximum iterative count. \n(\n[0,1])\n\n is an adjusting acceleration factor. \nt\nX can be forcedly enlarged by gradually relaxing Δ when \n0\n\n( Δ\n1\n\n\n). So  the method can \nbe considered as an optional factor if fast results are required. Although the method of gradually \n \nrelaxing is regarded as a local optimized result proved from the experimental results, it still \noutperforms than the stage of the arts methods. \n \nAlgorithm 1 Iterative Updating of \n*ˆtL  \nInput: X , \nmax\nk\n, Δ , , ANets  \nOutput: \n*ˆL  \nSteps: \nS1: Initialize handcrafted features F ; \nS2: Initialize \n0\nk \n, \n0 \nv\n0  , \nt \nX\n and \ns \nX\nX ; \nS3: Compute \nkv  of \ns\nX  by (15); \nS4: Split \ns\nX  into CCN sets \nst\nX  and CCD sets \nss\nX  with \nkv ; \nS5: Update \nt\nt\nst\n\nX\nX\nX , \ns\nss\n\nX\nX ; \nS6: Fine-tune Nets  with \nt\nX  by (14); \nS7: Update \n*ˆtL  of \ns\nX  by (13); \nS8: Update features \ns\nF of \ns\nX  by (12); \nS9: Δ=Δ\n\n\n; \nS10: \n1\nk\nk\n\n; \nS11:  Repeat S3 ~S10 until \ns\nX  is  or \nmax\nk\nk\n\n. \n4   Experiments \nIn this section, various real-world data sets, different types of initial feature sets and clustering \nmethods are applied to the proposed method for effectiveness validation, respectively. In sections \n4.1~4.3, we provide detailed information of implementing relevant experiments, including baseline \nmethods, evaluation models and public image sets for clustering. In section 4.4 different comparative \nexperimental results are respectively analyzed. Then we investigate the sensitiveness of the parameter \nΔ and δ in section 4.5. Meanwhile the experiments on time consumption are analyzed in section 4.6. \nAll fine-tuning experiments are implemented with Caffe [32] and the detailed information of the \nprototxt file related to fine-tuning is available at https://github.com/DukeDong/Transfer-Learning-\nBased-Image-Clustering-.git.  \n4.1    Baseline Methods \nClustering features and methods have great effect on the performance of an image clustering \nalgorithm, so we choose the following features, methods and their combinations for evaluations.  \nTable 1. Combinations of clustering features and clustering methods \nID \nClustering Features \nClustering Method \n#1 \nRP \nK-means \n#2 \nGF \n#3 \nHOG \n#4 \nRP+PCA \n#5 \nRP \nGMM \n#6 \nGF \n#7 \nHOG \n#8 \nRP+PCA \n#9 \nDCN (RP+K-means) \n \nClustering features: \n \n Raw Pixels (RP): pixel values without processing.  \n Gabor Features (GF): features generated by Gabor transform [33]. This kind of feature shows the \ninformation in spatial and frequency domain. \n HOG Features: histogram of gradient [34]. HOG represents edges of objects in images, and it is \nrobust to geometrical and optical deformation. \n RP + PCA:  feature vectors generated by PCA. The combination is still applied in face recognition \n[35].  \nClustering methods: \n K-means [18]: the classic cluster method.  \n Gaussian Mixture Model (GMM) [19]: the cluster method based on a latent variable model. \n Deep Clustering Network (DCN) [31]: a new method that can jointly optimize dimension \nreduction and clustering. It is accomplished via learning a deep neural network to approximate any \nnonlinear function. \nThere are nine kinds of combinations between different features and methods for image clustering \nincluding one method based on DCN, which are shown in table 1.   \n4.2    Evaluation Models \nThere are mainly two kinds of models for performance evaluation of a clustering method, which are \nexternal index and internal index. The applied models with external index include Jaccard Coefficient \n(JC) [36], Fowlkes and Mallows Index (FMI) [37], Normalized Mutual Information (NMI), Clustering \nAccuracy (ACC) [38] and Adjusted Rand Index (ARI) [39].  As to internal index, we employ Davies-\nBouldin Index (DBI) [40]. Comparing DBI directly is meaningless when different features are \nemployed for clustering. Therefore, we calculate final DBI by normalizing DBI gained in the boot \nstage. In this way, the normalized DBI implies a reduction of the initial DBI.  \nBesides, one of our goals is to weaken the effect of initial features. Another evaluation factor is \nrequired to examine how much effect the proposed method can weaken the initial features. Assuming \nclustering results with different initial features for a clustering method are available, the evaluation \nfactor  can be defined as (19): \n(\n)\n(\n)\ni\ni\ni\nE\n\n\nη\nη\n                                   (19) \nwhere \ni is the index difference rate, \n9\ni R\nη\n is a vector containing the i th external index values  \ncomputed from the results of different features, and \n{1,2,3,4,5}\ni\n represents the ID of applied \nevaluation indexes respectively named  JC, FMI, NMI, ARI, and ACC. \n( )\n and \n( )\nE  compute \nstandard deviation and mean values of \niη . Small value of \ni means that the corresponding method \ndoes well in weakening the effect of initial features. \n4.3    Data Sets \nImage clustering methods need to handle many different kinds of objects. In order to evaluate the \ngeneralization of the proposed method, experiments are conducted on six typical public image sets:  \n MNIST [22]: MNIST dataset has 70,000 data samples of handwritten digit ranging from 0 to 9.  \n Caltech Categories: Caltech Categories offers 6 image classes, including 652 images for Cars, 826 \nimages for Motorcycles, 1,074 images for Airplanes, 450 images for unique faces of around 27 \npeople, 186 images for 3 species of Leaves and 550 images for assorted Scenes. \n Caltech 101 [41]: Caltech 101 contains 101 categories including certain species of animals, planes, \nchairs, medical images and so on. \n Stanford Dogs Dataset [42]: Stanford Dogs Dataset contains images of 120 breeds of dogs from \naround the world. \n \n FERET [43]: The database contains 1,564 sets of images and 14,126 images in total including image \nsets for 1,199 individuals and 365 duplicate sets. \n CASIA-WebFace [44]: CASIA-WebFace has 10,575 subjects with 49,414 images in total. \n4.4    Performance Evaluations on Six Public Available Datasets \n4.4.1    Performance on Caltech Categories \nTable 2. Clustering evaluation on Caltech Categories \nMethod \nJC \nFMI \nNMI \nARI \nACC \nDBI \n#2 \n0.39 \n0.56 \n0.52 \n0.44 \n0.62 \n1.0 \n#2+Our Method \n0.65 \n0.79 \n0.80 \n0.73 \n0.71 \n0.77 \n#3 \n0.40 \n0.58 \n0.59 \n0.48 \n0.63 \n1.0 \n#3+Our Method \n0.61 \n0.75 \n0.79 \n0.69 \n0.75 \n0.72 \n#4 \n0.19 \n0.32 \n0.24 \n0.15 \n0.43 \n1.0 \n#4+Our Method \n0.52 \n0.68 \n0.71 \n0.61 \n0.69 \n0.89 \n#7 \n0.34 \n0.50 \n0.31 \n0.37 \n0.50 \n1 \n#7+Our Method \n0.64 \n0.78 \n0.80 \n0.72 \n0.70 \n0.39 \nTable 3.  Evaluation of   on Caltech Categories \nMethod \nJC \nFMI \nNMI \nARI \nACC \n#2~#4 \n0.36 \n0.30 \n0.41 \n0.50 \n0.20 \nOur Method \n0.11 \n0.08 \n0.06 \n0.09 \n0.04 \n \nImages in Caltech Categories are more complicated than other applied data sets, and different image \nclasses require different features.  \nThough Caltech Categories only contains six categories, it requires image preprocessing to achieve \ndesirable clustering results. Image preprocessing is essential for feature selection in prior researches. \nHowever, in order to make the process more challengeable, we employ #2~#4 and #7 directly without \nany preprocessing related analysis.  Clustering with the feature RP directly is meaningless, so #1 and \n#5 are omitted. In fact, it is effective enough that GMM performs image clustering with only one type \nof handcrafted features for comparison.  For GMM, we only list out the results of #7. In the \nexperiments, Δ  is set to 0.2, and  is set to 0.1, which is applied in sections 4.4.2~4.4.6. In addition, \nthe cluster number is set to six for K-means and GMM.  \nTable 2 shows the five evaluation indexes. Table 3 shows the index difference rates with different \nfeatures.  The results show that the proposed method performs much better than other methods and \nmeanwhile it weakens the effects of initial features. \n4.4.2    Performance on Caltech 101 \nCompared with Caltech Categories, Caltech 101 dataset contains more various categories, but each \ncategory has fewer samples, sample number of which is round ninety on average. Therefore, Caltech \n101 can be used to validate the robustness to the variation of image numbers and cluster numbers. \nWe randomly choose 20 categories from Caltech 101 in this part, and repeat the random process for \nmore than 3 times to guarantee the effectiveness of our experiments. Accordingly, the cluster number is \nset to 20 for K-means and GMM. Here, the category number of 20 is larger than common maximum \ncluster number for clustering evaluation of 2, 5, 10, etc. More clusters mean more difficult clustering \nmission. However, when we conduct image clustering with bigger clustering numbers, we suffer from \n \nhigh time consumptions of K-means and GMM. Therefore, cluster number of 20 is a rough but proper \nfor clustering performance evaluation. \nTable 4. Clustering evaluation on Caltech 101 \nMethod \nJC \nFMI \nNMI \nARI \nACC \nDBI \n#2 \n0.09±0.0060 \n0.17±0.0090 \n0.22±0.020 \n0.08±0.013 \n0.26±0.018 \n1 \n#2+Our Method \n0.22±0.0090 \n0.38±0.024 \n0.47±0.012 \n0.26±0.088 \n0.45±0.092 \n1.87±0.27 \n#3 \n0.17±0.019 \n0.30±0.028 \n0.43±0.018 \n0.24±0.030 \n0.42±0.033 \n1 \n#3+Our Method \n0.32±0.12 \n0.50±0.016 \n0.69±0.011 \n0.45±0.021 \n0.60±0.026 \n1.09±0.090 \n#4 \n0.10±0.057 \n0.18±0.090 \n0.29±0.014 \n0.12±0.095 \n0.29±0.011 \n1 \n#4+Our Method \n0.29±0.080 \n0.44±0.098 \n0.61±0.082 \n0.40±0.010 \n0.56±0.090 \n1.35±0.36 \n#7 \n0.10±0.0053 \n0.19±0.0043 \n0.15±0.071 \n0.10±0.0062 \n0.26±0.021 \n1 \n#7+Our Method \n0.16±0.0082 \n0.35±0.012 \n0.31±0.022 \n0.14±0.0010 \n0.37±0.022 \n0.77±0.032 \nTable 5.  Evaluation of   on Caltech 101 \nMethod \nJC \nFMI \nNMI \nARI \nACC \n#2~#4 \n0.36 \n0.33 \n0.34 \n0.57 \n0.26 \nOur Method \n0.35 \n0.14 \n0.19 \n0.27 \n0.14 \n*  is computed with all generated indexes of all random sets. \nTable 4 shows the detailed evaluation indexes. Table 5 shows the index difference rates with \ndifferent features. \nAs the experimental results show, the proposed method not only surpasses other methods, but also \nwith the better external index values. At the same time, the proposed method weakens the effect of \ninitial features. However, the results of DBI are unsatisfying, which means the learned features is not \ndensely distributed in feature space and further studies may improve DBI and clustering results.  \n4.4.3    Performance on Stanford Dogs Dataset \nTable 6. Clustering evaluation on Stanford Dogs Dataset \nMethod \nJC \nFMI \nNMI \nARI \nACC \nDBI \n#2 \n0.04±6.0e-4 \n0.07±0.0013 \n0.02±0.0030 \n0.01±8.9e-4 \n0.09±0.0023 \n1 \n#2+Our Method \n0.08±0.014 \n0.17±0.021 \n0.27±0.024 \n0.12±0.031 \n0.30±0.014 \n1.99±0.42 \n#3 \n0.03±3.9e-4 \n0.06±7.9e-4 \n0.03±0.0022 \n0.01±9.4e-4 \n0.11±0.0029 \n1 \n#3+Our Method \n0.11±0.014 \n0.21±0.021 \n0.31±0.038 \n0.14±0.032 \n0.32±0.028 \n1.32±0.29 \n#4 \n0.03±1.6e-4 \n0.07±3.2e-4 \n0.04±0.0021 \n0.01±3.0e-4 \n0.11±0.0014 \n1 \n#4+Our Method \n0.10±0.0030 \n0.19±0.0083 \n0.26±0.019 \n0.12±0.013 \n0.27±0.022 \n1.83±0.20 \n#7 \n0.04±8.1e-4 \n0.08±0.0022 \n0.016±9.7e-4 \n0.01±2.2e-4 \n0.09±0.0021 \n1 \n#7+Our Method \n0.06±0.0021 \n0.16±0.013 \n0.16±0.031 \n0.05±0.011 \n0.25±0.011 \n1.06±0.31 \nTable 7.  Evaluation of   on Stanford Dogs Dataset \nMethod \nJC \nFMI \nNMI \nARI \nACC \n#2~#4 \n0.17 \n0.11 \n0.33 \n0.36 \n0.11 \nOur Method \n0.16 \n0.10 \n0.09 \n0.09 \n0.08 \n*  is computed with all generated indexes of all random sets. \nThe proposed method performs well on nature images. Maybe it profits a lot from AlexNet, which is \ntrained with a large number of nature images, and some of the clustered classes have been classified. \nThus, a more complicated image set should be applied to make sure that the transfer-learned features \ndo not depend on AlexNet totally. Stanford Dogs, as one of the categories, belongs to the whole \ndatasets for training AlexNet. Clustering on dog images is a process that split one of the trained \n \ncategories into different clusters. If the results turn out to be satisfying, the corresponding transfer-\nlearned features are believed to be desirable and independent from the original AlexNet. \nStanford Dogs dataset has been built using images and annotations from ImageNet for fine-grained \nimage categorization.  The annotations include class labels and bounding boxes. It helps a lot when an \nimage clustering algorithm is provided with bounding boxes, but the bounding boxes are hardly \navailable during unsupervised image clustering. In order to make the experiments more reliable, the \nbox information is omitted in this section. Compared with the Caltech datasets, Stanford Dogs Dataset \ncontains categories representing clusters in a more detailed way, and the average number of sample \nimages in each breed is approximately 172. We randomly choose 20 breeds from the database this time, \nand repeat the random process for more than 3 times to ensure the effectiveness of the experiments.  \nThe cluster number is set to 20 for K-means and GMM. \nTable 6 shows the detailed clustering evaluation indexes. Table 7 shows the index difference rates \nwith different features. Similar results to those obtained in section 4.4.2, the results of DBI evaluated \nwith the proposed method are turned out to be even larger. As the learned features can be further \nexplored, the clustering performance is highly possible to be improved. However, the detailed external \nindexes still imply that the proposed method highly improves the clustering results and weakens the \neffects of initial features. \n4.4.4    Performance on FERET \nThe FERET database is used for face recognition system evaluation. As the collection is progressed \nin a standard way, the chosen images contain just a little background information and many various \nface photos with different postures. \nDifferent from the previous datasets mentioned above, FERET is classified individually, which \nmeans FERET clustering is a process of unsupervised face recognition. Face image clustering is much \ncloser to the leaf nodes than clustering methods based on Caltech categories and dog breeds. It is a \ntraditional way to build up suitable features when the images can be clearly noticed. In order to make \nthe experiments more reliable, the box information is omitted in this section as well. In this part, we \nrandomly choose 20 individuals from FERET, and repeat the random process for more than 3 times to \nmake sure the effectiveness of the experiments. The cluster number F is directly set to 20 for K-means \nand GMM. \nTable 8. Clustering evaluation on FERET \nMethod \nJC \nFMI \nNMI \nARI \nACC \nDBI \n#2 \n0.04±6.0e-4 \n0.08±0.0020 \n0.04±0.0041 \n0.02±0.0014 \n0.26±0.014 \n1 \n#2+Our Method \n0.12±0.0085 \n0.26±0.014 \n0.38±0.028 \n0.30±0.039 \n0.53±0.053 \n1.77±0.080 \n#3 \n0.05±0.0054 \n0.10±0.0095 \n0.11±0.020 \n0.06±0.011 \n0.28±0.0082 \n1 \n#3+Our Method \n0.17±0.023 \n0.33±0.032 \n0.47±0.022 \n0.37±0.023 \n0.54±0.045 \n1.35±0.11 \n#4 \n0.15±0.013 \n0.26±0.020 \n0.36±0.019 \n0.22±0.019 \n0.42±0.015 \n1 \n#4+Our Method \n0.17±0.025 \n0.32±0.034 \n0.43±0.088 \n0.31±0.062 \n0.52±0.047 \n2.24±0.045 \n#7 \n0.12±0.0045 \n0.22±0.0093 \n0.25±0.033 \n0.18±0.0051 \n0.45±0.020 \n1 \n#7+Our Method \n0.16±0.011 \n0.30±0.025 \n0.44±0.075 \n0.32±0.033 \n0.54±0.031 \n1.53±0.78 \nTable 9.  Evaluation of   on FERET \nMethod \nJC \nFMI \nNMI \nARI \nACC \n#2~#4 \n0.76 \n0.67 \n0.99 \n1.1 \n0.27 \nOur Method \n0.19 \n0.12 \n0.11 \n0.11 \n0.02 \n*  is computed with all generated indexes of all random sets. \nTable 8 shows the detailed clustering evaluation indexes. Table 9 shows the index difference rates \nwith different features. Similar results to those reported from Sections 4.4.3 and 4.4.4, the results of the \nproposed method in DBI are larger than the corresponding ones obtained from initial methods. All the \nlearned features are with the same dimension of 4096, which is fixed and probably redundant.  \nMeanwhile, the data shows the proposed method performs better and weakens the effect of initial \nfeatures.  \n \n4.4.5    Performance on CASIA-WebFace \nImages in CASIA-WebFace are collected from the Internet, which is different from image \ncollections in FERET, therefore the CASIA-WebFace images normally carry much more background \ninformation than those in FERET.  That means it is more challenging for image clustering with \nCASIA-WebFace. \nWe randomly choose 20 individuals from CASIA-WebFace in the same way, and repeat the random \nprocess for more than 3 times. The cluster number is directly set to 20 for K-means and GMM. \nTable 10. Clustering evaluation on CASIA \nMethod \nJC \nFMI \nNMI \nARI \nACC \nDBI \n#2 \n0.08±0.0030 \n0.15±0.009 \n0.09±7.1e-4 \n0.04±0.0017 \n0.20±0.110 \n1 \n#2+Our Method \n0.14±0.0090 \n0.25±0.012 \n0.23±0.012 \n0.11±0.028 \n0.30±0.067 \n1.31±0.53 \n#3 \n0.11±0.0042 \n0.21±0.011 \n0.23±0.013 \n0.14±0.025 \n0.26±0.083 \n1 \n#3+Our Method \n0.13±0.0073 \n0.25±0.0080 \n0.30±0.021 \n0.17±0.0030 \n0.30±0.043 \n0.80±0.32 \n#4 \n0.07±0.0012 \n0.14±0.0013 \n0.10±0.0040 \n0.06±0.035 \n0.17±0.0031 \n1 \n#4+Our Method \n0.15±0.0062 \n0.26±0.0098 \n0.22±0.0090 \n0.08±0.010 \n0.24±0.089 \n1.65±0.41 \n#7 \n0.09±0.0041 \n0.17±0.0082 \n0.10±0.0011 \n0.07±0.0091 \n0.24±0.077 \n1 \n#7+Our Method \n0.12±0.0063 \n0.23±0.0030 \n0.22±0.015 \n0.07±0.0031 \n0.30±0.061 \n0.85±0.71 \nTable 11.  Evaluation of   on CASIA \nMethod \nJC \nFMI \nNMI \nARI \nACC \n#2~#4 \n0.24 \n0.23 \n0.56 \n0.66 \n0.22 \nOur Method \n0.07 \n0.02 \n0.17 \n0.38 \n0.12 \n*  is computed with all generated indexes of all random sets. \nTable 10 shows the detailed clustering evaluation indexes.  Table 11 shows the index difference \nrates with different features. Although CASIA supplies with more complicated face images, the results \nof the proposed method can be still achieved higher in external indexes and lower in internal indexes \nwith less exceptions. \n4.4.6    Performance on MNIST \nDifferent from all the other datasets applied in sections 4.4.1~4.4.5, MNIST is a dataset of \nhandwritten digit images. The images in MNIST are both simple and various. There are no related \nbackground information or background noise in each digit image. Thus MNIST is an ideal dataset for \nimage classification. Meanwhile it is considered as a pre-processed dataset for image clustering, and \nevery type of classical features can achieve good results including RP.  For all images with a size of \n28×28 pixels, the clustering based on DCN has proved to be well implementable. \nHere we set Δ  to 0.2 and  to 0.1, and the cluster number is set to 10 for K-means and GMM.  \nThe results of performance evaluation with the proposed method except DCN-based method are \ndisplayed in table 12. Table 13 shows the index difference rates with different features. \nTable 12 shows, the results generated by transfer learning based clustering are improved. Although \nthe initial features have effects on clustering, the proposed method can weaken the effects a lots. Of \ncourse, better initial features guarantee to achieve labels that are more correct and much better results. \nDCN uses a forward network with 4 layers and sets the numbers of neurons to be 500, 500, 2,000, \nand 10. The reproducible codes remain mostly unchanged from https://github.com/boyangumn/DCN. \nHowever, we failed in reproducing clustering results based on #9. Therefore, we use their results \ndirectly and simulate correct labels according to its ACC performance in raw MNIST.  Meanwhile \nwhen we change one cluster image with a size of 28×28 pixels to another one with a size of 256×256 \npixels, DCN faces the curse of dimensionality. And the configurations for the depth of layers and \nnumber for each layer are unavailable. We use K-means instead of DCN in optimization part. It is pity \nthat DCN can be comparable only in the experiments related to the image clustering based on MNIST. \n \n \nTable 12 Clustering evaluation on raw MNIST \nMethod \nJC \nFMI \nNMI \nARI \nACC \nDBI \n#1 \n0.29 \n0.45 \n0.52 \n0.39 \n0.55 \n1.0 \n#1+Our Method \n0.39 \n0.56 \n0.62 \n0.50 \n0.65 \n0.75 \n#2 \n0.32 \n0.48 \n0.55 \n0.42 \n0.58 \n1.0 \n#2+Our Method \n0.39 \n0.56 \n0.64 \n0.51 \n0.65 \n0.84 \n#3 \n019 \n0.32 \n0.36 \n0.25 \n0.46 \n1.0 \n#3+Our Method \n0.34 \n0.51 \n0.59 \n0.45 \n0.59 \n0.71 \n#4 \n0.29 \n0.45 \n0.51 \n0.39 \n0.57 \n1.0 \n#4+Our Method \n0.39 \n0.55 \n0.63 \n0.50 \n0.64 \n0.84 \n#7 \n0.14 \n0.26 \n0.19 \n0.14 \n0.35 \n1 \n#7+Our Method \n0.19 \n0.36 \n0.26 \n0.23 \n0.52 \n0.74 \nTable 13.  Evaluation of   on MNIST \nMethod \nJC \nFMI \nNMI \nARI \nACC \n#1~#4 \n0.20 \n0.17 \n0.17 \n0.21 \n0.11 \nOur Method \n0.06 \n0.05 \n0.04 \n0.06 \n0.05 \nTable 14 Comparative Results of the proposed method with DCN \nIndex \n#9 \nSimulated #9 \nSimulated #9+Our Method \nJC \n \n0.53±2.0e-04 \n0.75±0.020 \nFMI \n \n0.69±2.1e-04 \n0.86±0.010 \nNMI \n0.81 \n0.66±1.9e-03 \n0.86±0.010 \nARI \n0.75 \n0.66±2.7e-04 \n0.84±0.010 \nACC \n0.83 \n0.83 \n0.92±0.010 \nDBI \n \n1.0 \n0.42±0.010 \nAccording to DCN’s ACC index of raw MNIST, there are 83% correct labels in every cluster. Thus, \nwe randomly choose 83% images from a same class in MNIST, and the rest of unchosen images are \nlabeled as wrong labels. In this way, the initial evaluation indexes are worse than the actual results \nproduced by DCN. Moreover, we repeat the simulation for more than three times to guarantee the \neffectiveness of the experiments. \nThe actual indexes, simulated indexes and indexes gained with the proposed method are shown in \ntable 14. Although the simulated data are not perfectly reproduced, our method still improves the \ncluster performance. \n4.5    Sensitivity of Δ  and  \nΔ , theoretically represents the confidence values of clustering results.  If the value of Δ  is larger, \nthen the initial clustering is supposed to perform more desirably. The progress can possess more time \nconsumption without .  On the contrary,  may lead to local results to be unsatisfying. Thus, it is \nnecessary to examine the effect of these two parameters on the proposed method. \nWe just employ ACC as the model for performance evaluation in this part and perform the \nexperiments with sampled Caltech 101 and Stanford Dogs.  The results are shown in Figure 2. \n can fasten the optimization with a relatively small number of iterations.  Figure 3 shows counts \nof iterations with different  and fixed Δ=0.2 . \n \n0\n0.2\n0.4\n0.6\n0.8\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n \n \nCaltech 101\nStanford Dogs\nACCs\n \n  \n0\n0.2\n0.4\n0.6\n0.8\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n \n \nCaltech 101\nStanford Dogs\n \nACCs\n \n(a)                                           (b) \nFig. 2. ACCs of clustering Caltech 101 and Stanford Dogs, achieved respectively by (a) experiments with different \nΔ s and fixed \n0.1\n\n,and  (b) experiments with different  s and fixed Δ=0.2  \nAs shown in Figure 2, the configurations of Δ  and  have effects on the results. Δ  is relatively \nmore important to the result because a larger Δ  can introduce more noisy labels in training. A proper \nΔ  can ensure effectiveness of learned features. When little prior knowledge is supplied, it is a suitable \nchoice to set Δ  with values close to 1/ K , which is the ACC of a random clustering. A larger δ may \nmake the proposed method stop faster, but accordingly  is suggested to set with values close to 0.1. \nIn this case, it can hardly affect the results and decrease the time consumption as Figure 2 shows.  \n0\n0.2\n0.4\n0.6\n0.8\n0\n2\n4\n6\n8\n10\n12\n14\n \n \nCaltech 101\nStanford Dogs\n \nIterations\n \nFig. 3. Counts of iterations with different  and fixed Δ=0.2 for experiments on Caltech 101 and Stanford \nDogs. \n4.6    Time consumption \nIn this section, we list the employed hardware equipment and software in table 15. Actually many \nfactors can influence the time consumptions on implementing experiments, including the number of \nimages in different datasets, images with different resolutions, and configuration parameters. In table \n16, we show the maximum statistical time consumption on (usually goes with GMM) experiments with \nthe employed datasets and the relative rough data.  \nThe time consumptions are acceptable but not satisfying enough. Even though the number of images \nin FERET is set to 140, transfer learning by fine-tuning a pre-trained model still spends around half an \nhour. In addition, as shown in table 16, the implemented codes are combinations of matlab, python and \nC++ (Caffe), which means there are extra more time consumptions on data conversion. Maybe we will \nlater try to find out a faster way on image clustering based on transfer learning. \n \nTable 15 Configurations of the implemental experiments \nName \nConfigures \nCPU \nIntel (R) Core(TM) i7-6700 \nRAM \n32.0GB \nOperating System \nWindows 10 Professional \nGPU \nNVIDIA GeForce GTX 1080 Ti \nTable 16 Time consumptions of experimental in section 4.4 \nDataset \nTime \nConsumption \nNumber of \nImages \nMNIST \n455± 10 \nminutes \n70,000 \nCaltech \nCategories \n79 ± 5 minutes \n3,648 \nCaltech 101 \n38 ± 4 minutes  \n1,126 \nStanford Dogs \n101± 7 minutes \n3,352 \nFERET \n28± 3 minutes \n140 \nCASIA-\nWebFace \n67± 5 minutes \n1,317 \n* Number of images refers to the employed sample images in experiments, which may be smaller than the actual number of \noriginal datasets. \n4. Conclusion \nThe method of image clustering based on Transfer Learning is proposed in this paper. The proposed \nmethod can be validated by implementing a large number of experiments and employing different \ntypes of evaluation models that it can improve the original clustering results and weaken the effect of \nthe initial features on image clustering. The method is also robust to the variations of different image \ncategories. We believe that dynamic feature dimensions and faster implementations are highly \ndemanded for image clustering in the future, which may be the next stage of studies in this field. \n \nReferences \n[1] Wolfle, Deal L., 1939. Review of Cluster analysis. Psychological Bulletin. 36(9), 791-792. \n[2] Richard Ernest Bellman, 1957. Dynamic programming. Princeton University Press, New Jersey. \n[3] Shenkai Gu, Ran Cheng, Yaochu Jin, 2018. Feature selection for high-dimensional classification using a competitive swarm \noptimizer. Soft Computing. 22(3), 811-822. \n[4] Nyararai Mlambo, Wilson K., Cheruiyot, Michael W. Kimwele, 2016. A Survey and Comparative Study of Filter and \nWrapper Feature Selection Techniques. The International Journal Of Engineering And Science (IJES). 5(8), 57-67. \n[5] Anil K., Jain and Edward Angel, 1974. Image Restoration, Modelling, and Reduction of Dimensionality. IEEE Transactions \non Computers. 23(5), 470-476. \n[6] Pudil P., Novovičová J., 1998. Novel Methods for Feature Subset Selection with Respect to Problem Knowledge. The \nSpringer International Series in Engineering and Computer Science. 453,101-116. \n[7] Mehdi Hosseinzadeh Aghdam, Nasser Ghasem-Aghaee Mohammad Ehsan Basiri, 2009. Text feature selection using ant \ncolony optimization. Expert Systems with Applications. 36(3), 6843-6853. \n[8] Liu F., Liu X., 2012. Unsupervised Feature Selection for Multi-cluster Data via Smooth Distributed Score. ICIC 2012. \nCommunications in Computer and Information Science. 304, 74-79. \n[9] Svante Wold, Kim Esbensen, Paul Geladi, 1987. Principal Component Analysis. Chemometrics and Intelligent Laboratory \nSystems. 2(1-3), 37-52. \n[10] Hotelling, H., 1936. Relations between two sets of variates. Biometrika. 28(3/4), 321–377. \n[11] Michael W. Berrya, Murray Brownea, Amy N. Langville, V. Paul Pauca, Robert J. Plemmons, 2007. Algorithms and \napplications for approximate nonnegative matrix factorization. Computational Statistics & Data Analysis. 52,155-173. \n[12] David L. Donoho, Michael Elad, 2003. Optimally sparse representation in general (nonorthogonal) dictionaries via \n1  \nminimization. Proceedings of the National Academy of Sciences. 100 (5), 2197-2202. \n[13] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P., 2010. A. Stacked denoising autoencoders: Learning \nuseful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research. 11, 3371–3408. \n[14] Patel, V. M., Van Nguyen, H., and Vidal, R., 2013. Latent space sparse subspace clustering. Proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition. 225–232. \n[15] Xie J., Xu L., Chen E., 2012. Image denoising and inpainting with deep neural networks. International Conference on \nNeural Information Processing Systems. Curran Associates Inc. 1, 341-349. \n[16] Rifai S., Vincent P., Muller X., and etc., 2011. Contractive Auto-Encoders: Explicit Invariance During Feature Extraction. \nICML. 85(1), 833-840. \n[17] Lu H., Plataniotis K. N., Venetsanopoulos A. N., 2011. A survey of multilinear subspace learning for tensor data. Pattern \nRecognition.  44(7), 1540-1551. \n \n[18] Hartigan J. A., 1979. A K-Means Clustering Algorithm. Appl Stat. 28(1), 100-108. \n[19] Reynolds D. A., Quatieri T. F., Dunn R. B., 2000. Speaker Verification Using Adapted Gaussian Mixture Models. Digital \nSignal Processing. 10(1-3) ,19-41. \n[20] Pan S. J., Yang Q., 2010. A Survey on Transfer Learning. IEEE Transactions on Knowledge & Data Engineering. 22(10), \n1345-1359. \n[21] Bengio Y., Cun Y. L., Henderson D., 1994. Globally Trained Handwritten Word Recognizer using Spatial Representation, \nConvolutional Neural Networks and Hidden Markov Models. Advances in Neural Information Processing Systems. 937-944. \n[22] [dataset] Lécun Y., Bottou L., Bengio Y., and etc., 1998. Gradient-based learning applied to document recognition. \nProceedings of the IEEE., 86(11), 2278-2324. \n[23] Krizhevsky A., Sutskever I., Hinton G. E., 2012. ImageNet classification with deep convolutional neural networks. \nInternational Conference on Neural Information Processing Systems. Curran Associates Inc. 1097-1105. \n[24] Simonyan K., Zisserman A., 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. Computer \nScience.  \n[25] Szegedy C., Liu W., Jia Y., and etc., 2015. Going deeper with convolutions. IEEE Conference on Computer Vision and \nPattern Recognition. IEEE Computer Society. 1-9. \n[26] He K., Zhang X., Ren S., and etc., 2015. Deep Residual Learning for Image Recognition. IEEE Computer Society. 770-778. \n[27] Xie, J., Girshick, R., and Farhadi, A., 2015. Unsupervised deep embedding for clustering analysis.Computer Science. \n[28] Cai, D., He, X., and Han, J., 2011. Locally consistent concept factorization for document clustering. IEEE Transaction on \nKnowledge and Data Engineering. 23(6), 902–913. \n[29] Banerjee A., Merugu S., Dhillon I. S., and Ghosh, J., 2005. Clustering with bregman divergences. Journal of Machine \nLearning Research. 6, 1705–1749. \n[30] Yang B., Fu X., and Sidiropoulos N. D., 2017. Learning from hidden traits: Joint factor analysis and latent clustering. IEEE \nTransaction on Signal Processing. 256–269. \n[31] Yang B., Fu X., Sidiropoulos N. D., and etc., 2017. Towards K-means-friendly Spaces: Simultaneous Deep Learning and \nClustering. Proceedings of the 34th International Conference on Machine Learning.  \n[32] Jia Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and \nGuadarrama, Sergio and Darrell, Trevor, 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint \narXiv:1408.5093. \n[33] Movellan J. R., 2002. Tutorial on Gabor Filters. Open Source Document. \n[34] N. Dalal and B. Triggs, 2005. Histograms of Oriented Gradients for Human Detection. Proc. IEEE Conf. Computer Vision \nand Pattern Recognition. 1, 886-893. \n[35] M. Turk, A. Pentland, 1991. Face Recognition Using Eigenfaces, Proc. IEEE Conf. on Computer Vision and Pattern \nRecognition. 586-591. \n[36] Jaccard Paul, 1912. The distribution of the flora in the alpine zone. New Phytologist. 11, 37–50. \n[37] Fowlkes, E. B., Mallows, C. L., 1983. A Method for Comparing Two Hierarchical Clusterings. Journal of the American \nStatistical Association. 78 (383), 553. \n[38] Cai, D., He, X., and Han, J., 2011. Locally consistent concept factorization for document clustering. IEEE Transaction on \nKnowledge and Data Engineering, 23(6), 902–913. \n[39] Yeung, K. Y. and Ruzzo,W. L., 2001. Details of the adjusted rand index and clustering algorithms, supplement to the paper \nan empirical study on principal component analysis for clustering gene expression data. Bioinformatics. 17(9), 763–774. \n[40] Davies, David L.; Bouldin, Donald W. 1979. A Cluster Separation Measure. IEEE Transactions on Pattern Analysis and \nMachine Intelligence. PAMI. 1 (2), 224–227. \n[41] [dataset] L. Fei-Fei, R. Fergus, and P. Perona, 2004. Learning generative visual models from few training examples: an \nincremental bayesian approach tested on 101 object categories. In CVPR Workshop on Generative-Model Based Vision. \n[42] [dataset] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao and Li Fei-Fei, 2011. Novel dataset for Fine-Grained \nImage Categorization. First Workshop on Fine-Grained Visual Categorization (FGVC), IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR). \n[43] [dataset] P. J. Phillips, A. Martin, C. L. Wilson, and M. Przybocki, 2000. An Introduction to Evaluating Biometric Systems. \nIEEE Computer. 33(2), 56-63. \n[44] [dataset] Dong Yi, Zhen Lei, Shengcai Liao and Stan Z. Li, 2014. Learning Face Representation from Scratch. arXiv \npreprint arXiv:1411.7923. \n[45] Yizong Cheng, 1995. Mean shift, mode seeking, and clustering. IEEE Transactions on Pattern Analysis and Machine \nIntelligence. 17(8), 790-799. \n[46] A. Y., Jordan, M., and Weiss, Y., 2002. On spectral clustering: Analysis and an algorithm. In Advances in Neural \nInformation Processing Systems 15 (NIPS 2002). 849–856. \n[47] Ester M., Kriegel H. P., Xu X., 1996. A density-based algorithm for discovering clusters a density-based algorithm for \ndiscovering clusters in large spatial databases with noise.  International Conference on Knowledge Discovery and Data Mining. \nAAAI Press. 226-231. \n[48] Swain M. J., Ballard D. H., 1990. Indexing via color histograms. International Conference on Computer Vision.390-393. \n[49] Lowe D. G., 2004. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision. \n60(2), 91-110. \n[50] He D. C., Wang L., 1990. Texture Unit, Texture Spectrum And Texture Analysis. IEEE Transactions on Geoscience & \nRemote Sensing. 28(4), 509-512.  \n \n \n",
  "categories": [
    "cs.CV",
    "62H30"
  ],
  "published": "2018-08-13",
  "updated": "2018-08-14"
}