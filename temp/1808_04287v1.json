{
  "id": "http://arxiv.org/abs/1808.04287v1",
  "title": "Visual Sensor Network Reconfiguration with Deep Reinforcement Learning",
  "authors": [
    "Paul Jasek",
    "Bernard Abayowa"
  ],
  "abstract": "We present an approach for reconfiguration of dynamic visual sensor networks\nwith deep reinforcement learning (RL). Our RL agent uses a modified\nasynchronous advantage actor-critic framework and the recently proposed\nRelational Network module at the foundation of its network architecture. To\naddress the issue of sample inefficiency in current approaches to model-free\nreinforcement learning, we train our system in an abstract simulation\nenvironment that represents inputs from a dynamic scene. Our system is\nvalidated using inputs from a real-world scenario and preexisting object\ndetection and tracking algorithms.",
  "text": "Visual Sensor Network Reconﬁguration\nwith Deep Reinforcement Learning\nPaul Jasek\nOhio State University\nAir Force Research Laboratory\nBernard Abayowa\nAir Force Research Laboratory\nAbstract\nWe present an approach for reconﬁguration of dynamic\nvisual sensor networks with deep reinforcement learning\n(RL). Our RL agent uses a modiﬁed asynchronous advan-\ntage actor-critic framework and the recently proposed Re-\nlational Network module at the foundation of its network\narchitecture. To address the issue of sample inefﬁciency in\ncurrent approaches to model-free reinforcement learning,\nwe train our system in an abstract simulation environment\nthat represents inputs from a dynamic scene. Our system is\nvalidated using inputs from a real-world scenario and pre-\nexisting object detection and tracking algorithms.\n1. Introduction\nThe application of deep neural networks in reinforce-\nment learning (RL) has shown success in a variety of\ndomains.\nFor example, Deep Q-Networks [8] achieved\nhuman-level performance in Atari 2600 games. Other re-\ncent approaches, including trust region policy optimiza-\ntion [12], asynchronous advantage actor-critic [7], and\nproximal policy optimization [13], have shown success in\ndomains such as 3D mazes and simulated robotic motion.\nHowever, the sample inefﬁciency of these algorithms limits\nthe application of current deep RL solutions to many real-\nworld problems where access to sample operations may be\nlimited or expensive. A simulation environment generates\nsample observations quickly and cheaply, providing an RL\nagent with enough data to learn a high-performing policy.\nWe aim to apply reinforcement learning to a dynamic\nsensor-network conﬁguration problem. While, we attempt\nmaintain generality throughout our experiments, our spe-\nciﬁc motivation is to use cameras to capture high-resolution\nviews of vehicles in a scene. Directly simulating this envi-\nronment would involve a variety of difﬁcult technical chal-\nlenges and would likely be computationally expensive and\nunrealistic when compared to a real-world scenario.\nIn-\nstead, we focus on modeling an abstract scenario, where\nobjects and sensors are represented as bounding boxes. A\ndeep RL agent can learn to maximize the percentage of ob-\njects captured at high-resolution within a scene by training\nin this simulation environment. After learning an effective\npolicy, the agent can operate within a real-world environ-\nment where preexisting object detection and tracking algo-\nrithms are applied to emulate the simulation environment\nfrom training.\n2. Related work\nSeveral methods have been proposed in the literature for\nreconﬁguration of dynamic visual sensor networks of static\nand Pan-Tilt-Zoom (PTZ) cameras. These methods can be\ngrouped into resource-aware methods, target-based meth-\nods, and coverage-oriented methods [9].\nResource-aware methods seeks to ﬁnd the optimal trade-\noff between available resources in the sensor network and\ntask performance requirements.\nThe sensing parameters\nare reconﬁgured to minimize usage of resources such as\npower in energy-aware surveillance systems [4] and com-\nmunication bandwidth in distributed camera networks [3].\nResource-aware methods are often found in settings where\nthe visual sensors are static.\nIn target-based method the focus is on the optimization\nof the camera parameters to put a target of interest in view.\nCommon applications include online adjustments of the ori-\nentation and zoom parameters of a PTZ camera for single\ntarget tracking [1], and camera assignment or hand-off for\noptimal view in static camera networks.\nThe group of methods related to ours are coverage-\noriented methods in which the goal is to maintain optimal\nscene coverage with a network of PTZ cameras [2, 5, 10].\nIn these methods, the parameters of the PTZ cameras are\nadjusted to maximize the view of relevant areas in the scene\nwhile also adapting to the scene dynamics.\nExisting methods for optimal coverage with visual sen-\nsor networks make use of hand-crafted mathematical mod-\nels and shallow neural networks which do not generalize\nwell. In this work we introduce a general framework for re-\n1\narXiv:1808.04287v1  [cs.LG]  13 Aug 2018\nconﬁguring visual sensor networks to optimize coverage by\nleveraging advances in model-free RL and deep representa-\ntion learning.\n3. Background\nThis section contains relevant background information\non the asynchronous advantage actor-critic (A3C) algo-\nrithm [7] and the relational network (RN) module [11]\nwhich are the foundations of our solution. A3C is used\nas the reinforcement learning algorithm and training frame-\nwork for our agent, while RN is used as part of the deep\nneural network architecture to enable the effective applica-\ntion of A3C to our speciﬁc problem.\n3.1. Asynchronous advantage actor-critic\nConsider the standard reinforcement learning scenario in\nwhich an agent interacts with an environment E. At any\ngiven time step t, the agent receives a state st and takes an\naction at chosen from the set of possible actions A accord-\ning to its policy π, which maps states st to an action (or\ndistribution of actions) at. The goal of the agent is to maxi-\nmize the discounted return at any given state st, deﬁned by\nRt = P∞\nk=0 γkrt+k, where γ ∈(0, 1] is the reward dis-\ncount factor. The value of any particular state s following\na policy π is deﬁned by V π(s) = E[Rt|st = s]. Simi-\nlarly, the action value at any particular state is deﬁned by\nQπ(s, a) = E[Rt|st = s, a]. These two quantities are used\nto deﬁne the advantage of an action at in state st given by\nA(at, st) = Q(at, st) −V (st). The advantage function\nrepresents the expected increase in future reward if a given\naction is taken rather than following the current policy.\nA3C is an example of a model-free policy-based method\nwhich trains an agent to maximize Rt by updating the\nparameters θ of the policy π(a|s; θ). Methods stemming\nfrom the REINFORCE algorithm [15] update the parame-\nters θ by performing approximate gradient ascent on E[Rt].\nThe standard REINFORCE algorithm updates θ in the ap-\nproximate direction of ∇θE[Rt] using the unbiased esti-\nmate ∇θ log π(at|st; θ)E[Rt]. Often, a function of the state\nknown as the baseline bt(st) is subtracted from Rt to reduce\nthe variance of the estimate, while remaining unbiased. If\nbt(st) is learned estimate of V π(st), then Rt −bt can be\nseen as an estimate of the advantage A(at, st), because Rt\nestimates Qπ(at, st) and bt estimates V π(st).\nA3C uses an estimate of the advantage to scale the policy\ngradient,\nA(st, at, θ, θv) =\nk−1\nX\ni=0\nγirt+i +γkV (st+k; θv)−V (st; θv).\nHere, V (st; θv) is a learned estimate of the value func-\ntion and k varies across states, but is bounded above by\ntmax, the number of time steps performed before updat-\ning the policy parameters. To encourage exploration, an\nentropy regularization loss term, H(π(st; θ)) is added to\nthe objective function. Here, H computes the entropy of\na distribution.\nThis adds an additional hyperparameter,\nβ, which is used to scale the entropy regularization loss\nterm.\nThe resulting objective function for the policy is\nlog π(at|st; θ)A(st, at, θ, θv) + βH(π(st; θ)). To train the\npolicy function π, we apply gradient ascent to this objec-\ntive function with respect to the policy function parameters\nθ. The estimated value function V is trained via standard\nsupervised learning to approximate the same bootstrapped\nestimate of Rt that was used to compute the advantage func-\ntion given by Pk−1\ni=0 γirt+i + γkV (st+k; θv).\nThe system is designed to be trained on multiple CPU\ncores by running parallel simulation environments for a\nﬁxed amount of time steps, tmax, before accumulating gra-\ndients and updating a global network. See the original pa-\nper [7] for more details on how this is implemented.\n3.2. Relational Networks\nThe RN module is designed with the capacity to reason\nabout the pairwise relations in a set of objects. Consider a\nset of n objects O = {o1, o2, ..., on}. Here, the ith object is\nan m-dimensional vector, oi ∈Rm. Additionally, we con-\nsider a condition, c ∈Rl, represented as an l-dimensional\nvector. The RN is expressed as a composite function,\nRN(O, c) = fφ\n\nX\ni,j\ngθ(oi, oj, c)\n\n.\n(1)\nHere O and c are deﬁned as above and fφ and gθ are\nmultilayer perceptrons (MLPs) with weights φ and θ, re-\nspectively. In this formulation, the role of gθ is to compute\na relation vector corresponding to the relationship between\ntwo objects under a given condition. The role of fφ is to\nconstruct an output based on all relations by operating on\nthe sum of all relation vectors.\nObject representation vectors can be directly provided\n(as is the case for our inputs) or generated from another\nneural network module (such as a CNN) as demonstrated\nin [11]. The input size of gθ is 2m + l and the network\nmay be several layers deep. Naturally, the input size of fφ\nmust be equal to the output size of gθ and may also consist\nof multiple layers. The result is a simple, end-to-end differ-\nentiable, neural network module that can effectively reason\nabout object relations.\n4. Deep Multi-view Controller\n4.1. Problem formulation\nWe consider a master-worker setup with a single station-\nary master camera which provides an overview of a scene\n2\nof vehicles and multiple active cameras with a narrow ﬁeld\nof view. Our goal is to view a maximum number of vehi-\ncles at a speciﬁed high-resolution. These vehicles may be\nmoving or stationary and can exit or enter the scene at any\npoint in time throughout the scenario. The scenario eventu-\nally ends, but the speciﬁc time that this happens is unknown\nto the agent.\nWe created an abstract simulation environment to en-\nable the effective use of model-free reinforcement learn-\ning techniques. The developed simulation uses bounding\nboxes to represent the vehicles in a scene and the camera\nviews. This abstraction generalizes the scenario to any sen-\nsor with a rectangular view and objects with similar move-\nment patterns to vehicles. Objects within the simulation can\nrandomly switch between moving and remaining station-\nary. Moving objects randomly turn by adjusting their di-\nrection continuously and may randomly reverse directions.\nThe sensors within the simulation can select between ﬁve\npossible actions (do nothing, move up, move down, move\nleft, and move right). The agent receives a positive reward\nwhenever an object is captured at high-resolution by an ac-\ntive camera for the ﬁrst time.\nFigure 1. Simulation Environment used for training the agent. The\nlight boxes represents sensor views within the environment. The\ngreen rectangles represent objects in the scene that have been cap-\ntured at high-resolution, while the black rectangles represent ob-\njects that have yet to be captured at high-resolution.\nTo increase the observability of environment, the simu-\nlation environment marks vehicles that have already been\ncaptured at high-resolution. This can be visualized in Fig-\nure 1 where marked vehicles are represented as green. The\nsimulation environment randomizes the number of objects\nand sensors, the object and sensor view sizes, the movement\nspeeds of the sensors, and the time scale of observations by\nthe agent. The purpose of this randomization is to increase\nthe likelihood of a policy trained within the simulation envi-\nronment being able generalize to a real-world scenario. We\ndraw inspiration from recent work in which a robotic arm\ntrained in a randomized non-photo-realistic simulation en-\nvironment is able to perform the task in a real world setting\nwithout additional training [14].\nThe abstract representation was chosen, because we can\nuse recent work in computer vision to translate a real-world\nscenario into the same representation. We assume access to\nsensor-view registration and an object and tracking system.\nWhile these systems are not trivial to implement, they are\nrequired to allow the agent to avoid keeping track of each\npreviously detected object in the scene for the duration of\nthe scenario. The absence of a tracking algorithm would\nrequire the agent to implicitly track each agent in the scene.\nFurther, a system making use of the sensor network would\nlikely be able to make use of an explicit tracking system for\nadditional purposes.\n4.2. A3C with Multiple Agents\nOur agent is based on the A3C algorithm from [7]. We\nuse multiple parallel simulation environments and update a\nglobal network by accumulating gradients computed from\nsample observations in each simulation environment. We\nenable the use of multiple sensors by controlling each sen-\nsor with a different instance of the same agent. We pro-\nvide each instance of the agent with a similar input state,\nbut mark a different sensor as the controlled sensor.\nIn\neach simulation environment, we use the same agent to con-\ntrol each individual sensor, but only perform updates to the\nagent based on a chosen main agent in the simulation en-\nvironment. The main agent receives credit for the reward\nreceived by all other agents within the environment. This\nwas intended to increase cooperation between the agents in\nthe scene by eliminating incentive for the agents to compete\nto capture the same vehicles at high-resolution.\n4.3. Model Architecture\nWe base our network architecture on the A3C architec-\nture used in [7]. However, we make a signiﬁcant modiﬁca-\ntion by replacing the convolutional neural network (CNN)\nused to process the input state with a modiﬁed Relational\nNetwork (RN) module.\nThe objects used by the RN are the object representa-\ntions for each sensor and object in the scene from the last\n4 time steps. Each object is represented by a 4-dimensional\nvector representing the bounding box of the object concate-\nnated with a 1-dimensional vector representing type of ob-\nject. Bounding boxes are represented as a vector containing\nthe xy-coordinates of the object’s center normalized to fall\nbetween -1 and 1 and the width and height of the object nor-\nmalized to fall between 0 and 1. Concatenating these object\nvectors over the last 4 time steps results in 20-dimensional\nobject representation vector.\nTo optimize memory and computation time, we only\nconsider relations between objects in which at least one\nof the objects is a sensor. Additionally, we do not show\n3\nFigure 2. Graphical representation of network architecture used in our agent.\nthe agent objects which have already captured at high-\nresolution. The relations are conditioned upon the vector\nrepresentation of the sensor that is being controlled by the\nagent. This results in a 60-dimensional vector representa-\ntion for each relation.\nWe pass each relationship vector\nthrough an MLP with 3 fully-connected layers with sizes\n128, 256, and 256 respectively. This is the MLP repre-\nsented by the function gθ in Equation 1. We then perform\nan element-wise sum operation and pass the result through\na fully-connected layer with size 256 and 2% dropout. Two\nseparate fully-connected layers are applied to resulting out-\nput to produce a vector of ﬁve action probabilities and an\nestimate of value function. This is the MLP represented by\nthe function fφ in Equation 1.\nThe entire architecture can be visualized in Figure 2. The\nelement-wise sum operation in the RN module allows us to\nrepresent a dynamically sized input as a ﬁxed-size vector.\nThis vector is then used to generate the policy and value net-\nworks as done in the traditional A3C design. Arbitrary in-\nput size into the network is particularly important, because\nwe have an arbitrary number of objects and sensors within\nthe scene. In addition to feeling less natural, attempts to\nprocess our input using convolutional and recurrent neural\nnetworks resulted in signiﬁcantly worse performance and\nincreased running times.\n5. Training and Experiments\nWe trained out agent using 16 parallel simulation en-\nvironments. The number of sensors and objects in each\nscene were selected from discrete uniform distributions\nwith ranges of 1 to 5 and 1 to 50 respectively. We optimized\nthree hyperparameters: learning rate, the entropy regular-\nization constant, and the reward discount factor. We trained\n4 agents with random hyperparameters from a limited range\nand selected the highest performing agent. A graph of this\ntraining process over the course of a week is shown in Fig-\nure 3. We observed instability in training with certain hy-\nperparameters as can be seen with the yellow agent in Fig-\nure 3. The agent failed to learn a policy signiﬁcantly better\nFigure 3. Smoothed performance over the course of training 4\nagents.\nthan simply taking random actions.\nWe evaluate our algorithm’s performance by compar-\ning with random movement and with a hand-crafted ”lawn\nmower” method. We devised two random movement strate-\ngies which selected randomly and uniformly between the\npossible actions. One method included the ”do nothing”\naction in which the camera simply remains in the same po-\nsition for a time step, while the other method assigned zero\nprobability to this action resulting in a slight performance\nincrease. Under the ”lawn mower” method, each active sen-\nsor view systematically covers the entire scene by moving\nup and down in columns, moving to the side for a single\ntime step after reaching the top or bottom of the scene.\nThe performance achieved by each method are shown in\nTable 1. We can see that our agent performs signiﬁcantly\nbetter than both the random and ”lawn mower” strategies.\nNote that it certain scenarios within the simulation environ-\nment it may be impossible to capture 100% of the objects at\nhigh-resolution, because objects may move out of the scene\nbefore any sensor is able to view it. We do not place a large\nemphasis on the speciﬁc percentage of vehicles captured,\nas the performance of the agent can be easily manipulated\nby adjusting the parameters of the environment such as the\n4\nAgent\nPercentage of Objects\nCaptured at High-Resolution\nRandom with ”do nothing”\n44.75%\nRandom\n46.28%\n”Lawn mower” method\n64.44%\nOurs (stochastic)\n82.74%\nOurs (deterministic)\n84.75%\nTable 1. Percentage of objects viewed at high-resolution over 100\nepisodes for our agents and several baseline methods. The stochas-\ntic version of our algorithm samples actions from the generated\ndistribution. The deterministic version simply selects the action\nwith the largest probability in the generated distribution, resulting\nin a slight increase in performance.\nmovement speed and view size of the active cameras.\nAs an additional method of evaluating our learned pol-\nicy, we look at the contribution of each relation considered\nby the agent towards the selected action distribution. This\nwas calculated using the gradient of the KL divergence be-\ntween the uniform action distribution and the selected ac-\ntion distribution with respect to a vector that is multiplied\nentry-wise with the computed relationships in the relational\nnetwork evaluated at ⃗1. Effectively, this results in a number\nscaled proportionally to the amount that each relation con-\ntributed to the chosen action distribution. The resulting con-\ntribution computation over several time steps in a scene can\nbe visualized in Figure 4. Note that the sensor has learned\nto place high importance on the relations between itself and\nobjects that are close to it. For example, at time step 16, we\nsee that nearly all the focus of the policy is on the two ob-\njects closest to it. In this instance it chooses to captured the\nobject on the left at high-resolution ﬁrst. However, we see\nthat the relationship between object to the right of the con-\ntrolled camera contributes negatively to the agent’s choice.\nWe attempt to validate the ability of our learned policy to\ngeneralize by constructing pseudo real-world test environ-\nment. This involved a real-world video stream which was\ntreated as the sensor reading from an overview camera. We\napplied real-time object detection and tracking to the video\nstream to simulate the inaccuracies in a real-world environ-\nment. The individual sensor views were simulated simi-\nlarly to in the training simulation environment. Qualitative\nresults to this experiment can be found in Figure 5. Note\nthat the object detector does not detect all vehicles and the\ntracker occasionally loses track of its targets. This increases\nthe difﬁculty of the reconﬁguration task, while also provid-\ning a good test for the problems that may be encountered in\na real-world environment.\n6. Conclusion\nWe have shown that deep reinforcement learning can\nbe applied to the problem of visual sensor network re-\nFigure 4. Visualization of the contribution of the relation between\npairs of objects towards the chosen action under the learned pol-\nicy. Relationships that contribute strongly to the chosen action\nare shown in green, while relationships that contribute negatively\nto the chosen action are shown in red. The controlled sensor is\nshown in cyan, while other sensors are shown in blue. The objects\nin the scene that have not yet been viewed at high-resolution are\nshown in black, while the objects that have are hidden.\nconﬁguration by training within a simulated environment.\nAlthough our results suggest that applying reinforcement\nlearning to sensor network reconﬁguration is feasible, there\nis much needed in terms of future work to reach a viable\nsolution for the real world. There are several engineering\nchallenges required to run object detection and tracking al-\ngorithms and control multiple cameras with low latency.\nAdditional future work is needed to improve the col-\nlaboration between multiple sensor controllers. Traditional\nreinforcement learning algorithms tend to ignore the case\nin which multiple agents are collaborating on a task. Our\npolicy did not appear to learn many collaborative strategies\noutside of typically not overlapping sensor views. We can\n5\nFigure 5. Visualization of the learned policy operating in a non-\nsimulated environment. The white bounding boxes represent sen-\nsor views, while the vehicles in the scene that have not yet been\nviewed at high-resolution are shown in black, while the vehicles\nthat have are shown in green.\nverify this by observing that the relation between the con-\ntrolled sensor and other sensors in the scene does not seem\nto contribute to the selected action distribution as shown in\nFigure 4. Applying methods similar to recent research in\nmulti-agent reinforcement learning such as MADDPG [6]\nmay increase the performance of the sensor network by in-\ncreasing the level of collaboration between individual sen-\nsors in capturing all vehicles at high-resolution.\nReferences\n[1] A. Del Bimbo, F. Dini, G. Lisanti, and F. Pernici. Exploit-\ning distinctive visual landmark maps in pan–tilt–zoom cam-\nera networks. Computer Vision and Image Understanding,\n114(6):611–623, 2010.\n[2] A. Kansal, W. Kaiser, G. Pottie, M. Srivastava, and\nG. Sukhatme. Reconﬁguration methods for mobile sensor\nnetworks. ACM Transactions on Sensor Networks (TOSN),\n3(4):22, 2007.\n[3] D. R. Karuppiah, R. A. Grupen, Z. Zhu, and A. R. Han-\nson. Automatic resource allocation in a distributed camera\nnetwork. Machine Vision and Applications, 21(4):517–528,\n2010.\n[4] U. A. Khan and B. Rinner. A reinforcement learning frame-\nwork for dynamic power management of a portable, multi-\ncamera trafﬁc monitoring system.\nIn Green Computing\nand Communications (GreenCom), 2012 IEEE International\nConference on, pages 557–564. IEEE, 2012.\n[5] K. R. Konda and N. Conci. Optimal conﬁguration of ptz\ncamera networks based on visual quality assessment and\ncoverage maximization.\nIn Distributed Smart Cameras\n(ICDSC), 2013 Seventh International Conference on, pages\n1–8. IEEE, 2013.\n[6] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and\nI. Mordatch. Multi-agent actor-critic for mixed cooperative-\ncompetitive environments. CoRR, abs/1706.02275, 2017.\n[7] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lilli-\ncrap, T. Harley, D. Silver, and K. Kavukcuoglu.\nAsyn-\nchronous methods for deep reinforcement learning. CoRR,\nabs/1602.01783, 2016.\n[8] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-\nness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik,\nI. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,\nand D. Hassabis. Human-level control through deep rein-\nforcement learning. Nature, 518(7540):529–533, 2015.\n[9] C. Piciarelli, L. Esterle, A. Khan, B. Rinner, and G. L.\nForesti.\nDynamic reconﬁguration in camera networks: A\nshort survey. IEEE Transactions on Circuits and Systems for\nVideo Technology, 26(5):965–977, 2016.\n[10] C. Piciarelli, C. Micheloni, and G. L. Foresti. Automatic\nreconﬁguration of video sensor networks for optimal 3d cov-\nerage. In Distributed Smart Cameras (ICDSC), 2011 Fifth\nACM/IEEE International Conference on, pages 1–6. IEEE,\n2011.\n[11] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski,\nR. Pascanu, P. Battaglia, and T. P. Lillicrap.\nA simple\nneural network module for relational reasoning.\nCoRR,\nabs/1706.01427, 2017.\n[12] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abeel.\nTrust region policy optimisation. ICML, 2015.\n[13] J. Schulman, F. Wolski, and P. Dhariwal. Proximal Policy\nOptimization Algorithms Background : Policy Optimization.\nCoRR, 2017.\n[14] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and\nP. Abbeel. Domain randomization for transferring deep neu-\nral networks from simulation to the real World. ArXiv, pages\n1–8, 2017.\n[15] R. J. Willia.\nSimple Statistical Gradient-Following Algo-\nrithms for Connectionist Reinforcement Learning. Machine\nLearning, 8(3):229–256, 1992.\n6\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CV",
    "stat.ML",
    "68T05"
  ],
  "published": "2018-08-13",
  "updated": "2018-08-13"
}