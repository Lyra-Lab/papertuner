{
  "id": "http://arxiv.org/abs/1808.04459v1",
  "title": "Deep Learning Based Natural Language Processing for End to End Speech Translation",
  "authors": [
    "Sarvesh Patil"
  ],
  "abstract": "Deep Learning methods employ multiple processing layers to learn hierarchial\nrepresentations of data. They have already been deployed in a humongous number\nof applications and have produced state-of-the-art results. Recently with the\ngrowth in processing power of computers to be able to do high dimensional\ntensor calculations, Natural Language Processing (NLP) applications have been\ngiven a significant boost in terms of efficiency as well as accuracy. In this\npaper, we will take a look at various signal processing techniques and then\napplication of them to produce a speech-to-text system using Deep Recurrent\nNeural Networks.",
  "text": " \nDeep Learning Based Natural Language Processing \n \nSarvesh Patil  \nDept. Of Electronics and Telecommunication \nVivekanand Education Society's Institute of Technology \nChembur, India \n2015sarvesh.patil@ves.ac.in \n \nAbstract— Deep Learning methods employ multiple \nprocessing layers to learn hierarchial representations of \ndata. They have already been deployed in a humongous \nnumber of applications and have produced state-of-the-art \nresults. Recently with the growth in processing power of \ncomputers to be able to do high dimensional tensor \ncalculations, \nNatural \nLanguage \nProcessing \n(NLP) \napplications have been given a significant boost in terms of \nefficiency as well as accuracy. In this paper, we will take a \nlook at various signal processing techniques and then \napplication of them to produce a speech-to-text system \nusing Deep Recurrent Neural Networks.  \nKeywords—Deep Neural Network, Recurrent Neural \nNetworks, \nNatural \nLanguage \nProcessing, \nSpeech \nRecognition. \nI. INTRODUCTION  \nNatural Language Processing is a method for analysis and \nrepresentation of human language. NLP methods are heavily \ntheoretical and require a lot of in-depth knowledge of the way \nsignals behave when mathematical modulations are applied to \nthem. NLP in older times used to take up a huge amount of \ntime just to get simple language processing done. Punch Cards \nand batch processing would take up to 7 minutes to get a \nsimple task done. With the advent of highly efficient \nalgorithms and massive computational power, we can do the \nsame tasks in a matter of seconds. In fact, the Google search \nengine can scrape up to a million webpages in a matter of \nmilliseconds. NLP enables computers to perform a variety of \nnatural language related tasks at all levels, ranging from \nparsing and part-of-speech (POS) tagging, to machine \ntranslation and dialog systems, to speech recognition. \nDeep Learning algorithms are already highly scalable \nand produce highly accurate results. In fields like Computer \nVision or data analytics, they have even surpassed the \"Human \nLevel Performance\". But we are not quite there yet to make \nsuch claims about speech processing and recognition. A very \ncrucial aspect of the research going on in the NLP department \nis the focus on speech recognition and translation. We can \neasily make Hidden Markov Models connected in Recurrent \nfashion work for us in this domain, but they can only achieve \nso much and can't beat Deep Neural Networks.   \nThe main reason why we choose Deep Networks is \nthat, \"end-to-end\" tasks can be accomplished by using proper \nalgorithms and training intuitions. End-to-end Neural \nNetworks are the ones which require almost none to very less \npreprocessing. They learn the entire mapping by themselves \nprovided the network is robust enough. In speech recognition \ntasks too, such a method can be applied to achieve end-to-end \nresults which are far more accurate than the traditional \nmachine learning approach. Recent work by (Alex Graves \net al., 2014) have been the inspiration behind this paper and \nthe Deep Learning techniques behind it.   \nIn section II, we will discuss the mathematical \npreprocessing required for audio signal processing. In section \nIII, we will take a look at various architectures that RNNs can \nbe composed by and choose the one we will go ahead with. In \nsection IV, we will look at the precautionary steps required \nbefore training our network. In section V, we will provide a \nbasic abstract for the training of the network. As this work \nhasn't been already being done by me, comparative graphs and \nstatistics can't be provided. But in section VI, we will look at \nthe applications this project may help in and finally section \nVII, will summarize the paper and concluding remarks will be \ngiven.  \n \nII. FOURIER ANALYSIS OF SIGNALS  \nProbably the most important applications of Fourier \ntransforms are pertaining to signal processing. We will cover \nthis topic with an example. Suppose a dilettante plays a piano. \nWhen they play a single note on the piano, we hear a certain \ntype of sound. When another note is played, a different type of \nsound is perceived. Similarly, when a myriad of notes is being \nplayed by the pianist, a myriad of sounds is heard by the \nhuman ear. Our brains can decipher, with certain training of \ncourse, the constituent notes in the symphony being played. \nBut what about computers? A computer, when needs to be \nshown data of some kind, must be shown digitally. For this \npurpose, we use a very crucial mathematical tool, invented \nby Joseph Fourier, the Fourier transform. Fourier Analysis of a \nsignal gives us very important insights about the signal, the \nconstituent frequencies with respect to their amplitudes in the \nfrequency domain.   \nLet us consider the following example: six different \nsinusoidal signals with different frequencies are superimposed \nto form the following signal:  \n \nA signal composed of various sinusoidal waves added together \n \nIf we take the Fourier transform of this signal, and \nplot the amplitude of the output variable across various \nfrequencies, we get a graph like this: \n \n \n \nWe can now easily conclude that frequencies 50Hz, \n100Hz, 150Hz, 200Hz, 250Hz and 300Hz were combined to \nform the original signal. This process includes 3 steps.   \n1. Define the constituent signals and add them or use \nany audio clip and feed it as the input layer.  \n2. Sample the signal with sampling frequency at least \ntwice that of the maximum frequency in the input signal. \nIn this case, maximum frequency was 300Hz, so a \nsampling signal with frequency 600Hz and above can be \nused to convert the continuous signal into discrete form.   \n3. Apply fast Fourier transform on the discrete signal to \ngenerate a function with variable as 'f' which can take \nvalues from 0Hz to 500Hz.   \nAny signal that needs to be pre-processed can be \ndone so, with the aforementioned steps. Once we have a list of \nall the constituent frequencies in the input signal, for speech \nrecognition, frequencies greater than 4kHz can be neglected as \nhuman speech frequencies lie within that range.  \n \nIII. RECURRENT NEURAL NETWORKS \nRNNs have been in use for quite some time now and \nin this paper, we are sticking with RNNs because of their \ndependence on the previous computations and results. This \nmakes sequences in a language intuitive for the Neural \nNetwork. There are two most prominent RNN frameworks \nwhich provide the best sequence modelling:  \n1. Long Short-term Memories (LSTMs) (Hochreiter and \nSchmidhuber, 1997; Gers et al., 1999)  \n2. Gated Recurrent Units (GRUs) (Cho et al., 2014)  \nA simple recurrent neural network is shown in Fig. 3.1.  \n \nFig 3.1 (Tom Young et al., 2017)  \n \nEach LSTM cell works as shown in Fig 3.2. \n \nFig 3.2 LSTM cell (Alex Graves et al., 2014) \nIn GRUs, each cell 's' looks as shown in Fig 3.3. \n \nFig 3.3 GRU gate (Chung et al., 2014)  \nOutput of each LSTM unit can be calculated as:  \nit = σ (Wxixt + Whiht−1 + Wcict−1 + bi)  \nft = σ (Wxfxt + Whfht−1 + Wcfct−1 + bf)   \nct = ftct−1 + it * tanh(Wxcxt + Whcht−1 + bc)  \not = σ (Wxoxt + Whoht−1 + Wcoct + bo)  \nht = ot * tanh(ct)   \nwhere σ is the logistic sigmoid function, and i, f, o and c are \nrespectively the input gate, forget gate, output gate and cell \nactivation vectors, all of which are the same size as the hidden \nvector h. For our analysis, only LSTMs will be considered \nbecause of their high complexity, with sequences with large \nnumber of words.  \nIts unique mechanism enables it to overcome both the \nvanishing and exploding gradient problem.   \nFor providing input to the first layer for every epoch, we \nuse a Continuous Bag of Words (CBOW). By preprocessing \nthe input, we get sample values at every discrete step \nin our network. We can use these values by considering 20ms \nversions of the total information. This will yield a fixed length \ninput of 20ms chunks of data that will be trained upon by the \nRNN. An example will make the concept clear. Suppose the \nvoice input by a person is \"Welcome\" and it spans a duration \nof 700ms. Thus, we will have 30 windows of input data to our \nRNN. Suppose the network outputs its 30 characters as: \n<space>__WWWWEEEELLLL_CCCCAAAA__AAAMMM\nM__<space>. We process the output in the following steps:  \n1. Separate all the words between the spaces: \n__WWWWEEEELLLL_CCCCAAAA__AAAMMMM__\n  \n2. Replace multiple characters by single characters: \n__WEL_CA_AM__  \n3. Remove the blanks: WELCAAM \n \nNow, as humans we understand ‘welcaam’ must not be the \ncorrect spelling of ‘welcome’. To make the kernel understand \nthis, an RNN transducer is used. Generally, a transducer is a \ndevice which converts one form of energy into another. \nTalking digitally, our transducer will convert the acoustically \ngenerated output into a linguistically accurate one.  \nThus, for detecting entire transcriptions like \"She could not \nbe more right\", the network may predict \"She curd net be more \nright\". Such errors are to be removed only by increasing the \ntraining set by feeding the network with more books and news \narticles. Out of all the predicted transcriptions, we throw \nout the ones that seem least likely and keep the ones that are \nmore realistic. A possible case of training on a highly \nmorphologically complex language, this character wise \nembedding becomes exponentially better at prediction than \nCBOW. Context Specific Vectors from the paper on Character \nwise embeddings by (Xiaoqing Zheng et al., 2012) have \nproved to be much more efficient than most other embedding \ntypes. CVSs can generate the context representation of a \nword/character and learn the word/character vector carrying \nthe meaning inferred by that context.  \n \nIV. TRAINING THE NEURAL NETWORK \nA. Connectionist Temporal Classiﬁcation  \nLSTMs provide much of the necessary regularization by \nthemselves. They help prevent both, the vanishing as well as \nthe exploding gradient problems. Hence, they're more popular \neven though they may be computationally greedy than GRUs \nor CNNs. Connectionist Temporal Classiﬁcation \n(CTC) (A. Graves et al., 2006; A. graves et al., \n2012) uses a SoftMax layer to deﬁne a separate output \ndistribution Pr(k|t) at every step t along the input sequence. \nThe distribution contains K phonemes and two extra symbols, \n<blank> and <space>. For English, 44 phonemes are defined \nbut can be manipulated to accommodate the phonemes from \nany other language by changing the value of K.   \nB. Bidirectional Recurrent Neural Networks  \nBidirectional RNNs are different from conventional RNNs \nin the aspect that, they calculate the summed output over the \npreviously computed results as well as take into consideration \nthe characters that follow the current node. Fig 4.1 shows the \nblock diagram of a BRNN.  \n \nFig 4.1 BRNN (Alex Graves et al., 2014) \nC. RNN Transducers  \nCTC deﬁnes a distribution over phoneme sequences that \ndepends only on the acoustic input sequence x. It is therefore \nan acoustic-only model. A recent augmentation, known as an \nRNN transducer (A. Graves et al., 2012) combines a CTC-like \nnetwork with a separate RNN that predicts each phoneme \ngiven the previous ones, thereby yielding a jointly trained \nacoustic and language model. The fundamental idea is that, \nCTCs generate sequences of phonemes \nproducing an acoustic definition of the input and the RNN \ntransducers will produce a linguistic definition of the word \nbased on the characters. Both the outputs are multiplied to \nform a common generated text transcription, that will produce \na much more accurate result. In particular, Alex Graves et al. \nFound that the number of deletion errors during decoding is \ndrastically reduced. \nD. Decoding \nRNN transducers can be decoded to yield an n-best list of \ncandidate transcriptions. We simply sort the list by their \nprobability values instead of normalized values, as log(Pr)/|Pr| \nis useful in the case of more deletions than insertions. With \ndeletions reduced to a very small scale, only sorting by Pr will \nsuffice our needs.  \nE. Regularization \nRegularization techniques help prevent overfitting of the \ndata to the training set. Many methods have been devised to \ntackle this issue. We find that using dropouts in RNNs is really \nuseful to give a superior performance, provided it is correctly \napplied. Dropouts in recurrent sections can prove to be fatal to \nthe RNN. \n Hence, it's better to be circumspective while applying \ndropouts to RNNs.   \nAnother very useful regularization technique is addition of \nweight noise (noise induced in the network weights during \ntraining). Weight noise tends to ‘simplify’ neural networks, in \nthe sense of reducing the amount of information required to \nsend the parameters which improves generalization.  \n \nV. APPLICATIONS \nThe most promising applications of RNNs remain to \nbe Part of Speech tagging (POS), word level classification, \nsentence level classification, etc. But what this paper aims at \nis, speech recognition is very shallow if the computer doesn't \nunderstand the context in which the transcription is meant to \nbe. A very critical focus now-a-days is on sentiment analysis, \ntone analysis and sarcasm detection. Every application \nrequires a new dataset with thousands of examples of real \nhuman speech. This is one of the fundamental reasons why the \nprogress has been slow in NLP. Ultimately, end-to-end speech \ntranslation is the benchmark that we are yet to conquer, but it \nwon't take us long enough with TPUs coming into play in the \nnear future.  \nVI. CONCLUSION \nDeep \nRecurrent \nNeural \nNetworks \ncoupled \nwith \nbidirectional Long Short-Term Memory cells have achieved \nastonishingly good results on various tests around the globe. \nCTCs are very useful for acoustic modelling of sound and RNN \ntransducers coupled with CTCs can produce semantic \nrelationships with ease while keeping the syntactic relations \nalive. An obvious step forward would be to actually apply these \nconcepts into such a neural network and then step upon the \nmyriad of pedestals that we are yet to conquer.  \nReferences  \n[1] Alex Graves, Abdel-rahman Mohamed and Geoffrey \nHinton, SPEECH \nRECOGNITION \nWITH \nDEEP \nRECURRENT \nNEURAL \nNETWORKS, arXiv:1308.0850 [cs.NE]  \n[2] A.Graves,S.Fern´andez,F.Gomez,andJ.Schmidhuber, \n“Connectionist \nTemporal \nClassiﬁcation: \nLabelling \nUnsegmented Sequence Data with Recurrent Neural \nNetworks,” in ICML, Pittsburgh, USA, 2006.   \n[3] A.Graves, Supervised \nsequence \nlabelling \nwith \nrecurrent neural networks, vol. 385, Springer, 2012.    \n[4] A. Graves, “Sequence transduction with recurrent neural \nnetworks,” in ICML Representation Learning Worksop, \n2012.  \n[5] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yos\nhua Bengio, Empirical Evaluation of Gated Recurrent \nNeural \nNetworks \non \nSequence \nModeling, arXiv:1412.3555 [cs.NE]  \n[6] Kyunghyun Cho, Bart \nvan Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fe\nthi Bougares, Holger Schwenk, Yoshua Bengio, Learning \nPhrase Representations using RNN Encoder-Decoder for \nStatistical \nMachine \nTranslation, arXiv:1406.1078 [cs.CL]  \n[7] S. Hochreiter and J. Schmidhuber, “Long short-term \nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–\n1780, 1997.  \n[8] Tom Young, Devamanyu Hazarika, Soujanya Poria, Erik \nCambria, Recent Trends in Deep Learning Based Natural \nLanguage Processing, arXiv:1708.02709 [cs.CL]  \n[9] Xiaoqing Zheng, Jiangtao Feng, Yi Chen, Haoyuan Peng, \nWenqing \nZhang, Learning \nContext-Specific \nWord/Character \nEmbeddings, Shanghai \nMunicipal \nNatural Science Foundation (No. 15511104303).  \n \n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-08-09",
  "updated": "2018-08-09"
}