{
  "id": "http://arxiv.org/abs/1808.04593v1",
  "title": "Unsupervised learning of foreground object detection",
  "authors": [
    "Ioana Croitoru",
    "Simion-Vlad Bogolin",
    "Marius Leordeanu"
  ],
  "abstract": "Unsupervised learning poses one of the most difficult challenges in computer\nvision today. The task has an immense practical value with many applications in\nartificial intelligence and emerging technologies, as large quantities of\nunlabeled videos can be collected at relatively low cost. In this paper, we\naddress the unsupervised learning problem in the context of detecting the main\nforeground objects in single images. We train a student deep network to predict\nthe output of a teacher pathway that performs unsupervised object discovery in\nvideos or large image collections. Our approach is different from published\nmethods on unsupervised object discovery. We move the unsupervised learning\nphase during training time, then at test time we apply the standard\nfeed-forward processing along the student pathway. This strategy has the\nbenefit of allowing increased generalization possibilities during training,\nwhile remaining fast at testing. Our unsupervised learning algorithm can run\nover several generations of student-teacher training. Thus, a group of student\nnetworks trained in the first generation collectively create the teacher at the\nnext generation. In experiments our method achieves top results on three\ncurrent datasets for object discovery in video, unsupervised image segmentation\nand saliency detection. At test time the proposed system is fast, being one to\ntwo orders of magnitude faster than published unsupervised methods.",
  "text": "Unsupervised learning of foreground object detection\nIoana Croitoru · Simion-Vlad Bogolin · Marius Leordeanu\nAbstract Unsupervised learning poses one of the most dif-\nﬁcult challenges in computer vision today. The task has an\nimmense practical value with many applications in artiﬁcial\nintelligence and emerging technologies, as large quantities\nof unlabeled videos can be collected at relatively low cost.\nIn this paper, we address the unsupervised learning prob-\nlem in the context of detecting the main foreground objects\nin single images. We train a student deep network to pre-\ndict the output of a teacher pathway that performs unsu-\npervised object discovery in videos or large image collec-\ntions. Our approach is different from published methods on\nunsupervised object discovery. We move the unsupervised\nlearning phase during training time, then at test time we ap-\nply the standard feed-forward processing along the student\npathway. This strategy has the beneﬁt of allowing increased\ngeneralization possibilities during training, while remaining\nfast at testing. Our unsupervised learning algorithm can run\nover several generations of student-teacher training. Thus,\na group of student networks trained in the ﬁrst generation\ncollectively create the teacher at the next generation. In ex-\nperiments our method achieves top results on three current\ndatasets for object discovery in video, unsupervised image\nsegmentation and saliency detection. At test time the pro-\nIoana Croitoru1\nE-mail: ioana.croi@gmail.com\nSimion-Vlad Bogolin1\nE-mail: vladbogolin@gmail.com\nMarius Leordeanu1,2\nE-mail: marius.leordeanu@imar.ro\n1Institute of Mathematics of the Romanian Academy\n21 Calea Grivitei, Bucharest, Romania\n2University ”Politehnica” of Bucharest\n313 Splaiul Independentei, Bucharest, Romania\nThe ﬁrst two authors contributed equally to this work.\nposed system is fast, being one to two orders of magnitude\nfaster than published unsupervised methods.\nKeywords foreground object segmentation · video\ndiscovery · single image · unsupervised learning\n1 Introduction\nUnsupervised learning is one of the most difﬁcult and inter-\nesting problems in computer vision and machine learning\ntoday. Many researchers believe that learning from large\ncollections of unlabeled videos could help decode hard\nquestions regarding the nature of intelligence and learn-\ning. Moreover, as unlabeled videos are easy to collect at\nrelatively low cost, unsupervised learning could be of real\npractical value in many computer vision and robotics ap-\nplications. In this article we propose a novel approach to\nunsupervised learning that successfully tackles many of the\nchallenges associated with this task. We present a system\nthat is composed of two main pathways, one that performs\nunsupervised object discovery in videos or large image col-\nlections along the teacher branch, and the other, the student\nbranch, which learns from the teacher to detect foreground\nobjects in single images. Our approach is general in the\nsense that the student or teacher pathways do not depend\non a speciﬁc neural network architecture or implementa-\ntion. Also, our approach allows the unsupervised learning\nprocess to continue over several generations of students\nand teachers. In Algorithm 1 we present the high level de-\nscription of our method. We will use throughout the paper\nthe terms ”generation” and ”iteration” of Algorithm 1 in-\nterchangeably. A preliminary version of this work, without\npresenting the possibility of learning over several genera-\ntions and with fewer experimental results appeared at ICCV\n2017 (Croitoru et al (2017)).\nIn Figure 1 we present a graphic overview of our full sys-\ntem. In the unsupervised training stage the student network\narXiv:1808.04593v1  [cs.CV]  14 Aug 2018\n2\nIoana Croitoru et al.\n(module A) learns, frame by frame, from an unsupervised\nteacher pathway (modules B and C) to produce similar ob-\nject masks in single images. The student branch tries to imi-\ntate for each frame the output of the teacher, while having as\ninput only a single image - the current frame. The teacher on\nthe other hand has access to an entire video sequence. The\nmethod presented in Algorithm 1 follows the main steps of\nthe system as it learns from one iteration (generation) to the\nnext. The steps are discussed in more detail in Section 3.\nDuring the ﬁrst iteration of Algorithm 1, the unsuper-\nvised teacher pathway has access to information over time -\na video. In contrast, the student is deeper in structure, but it\nhas access only to a single image - the current video frame.\nThus, the information discovered by the teacher in time is\ncaptured by the student in added depth, over neural layers\nof abstraction. Several student nets with different architec-\ntures are trained at the ﬁrst iteration. In order to use as su-\npervisory signal only good quality masks, an unsupervised\nmask selection procedure is applied, as explained in Sec-\ntion 4. Once several student nets are trained, their output is\ncombined to form the teacher at the next iteration. Then, we\nrun, at the next generation, the newly formed teacher on a\nlarger set of unlabeled videos, to produce supervisory signal\nfor the next generation students. Note that while at the ﬁrst\niteration the teacher pathway is required to receive video se-\nquences as input, from the second generation on, it could\nreceive as input large image collections, as well. Due to the\nvery high computational and storage costs, required during\ntraining time, we limit our experiments to learning over two\ngenerations, but our algorithm is general and could run over\nmany iterations. We show in extensive experiments that even\ntwo generations are sufﬁcient to signiﬁcantly outperform the\ncurrent state of the art on object discovery in video and im-\nages. We also demonstrate a solid improvement from one\ngeneration to the next. Now we enumerate the main contri-\nbutions of our approach:\n1) We introduce a novel approach to unsupervised learn-\ning from videos to detect foreground objects in images. The\noverview of our system and algorithm are presented in Fig-\nure 1 and Algorithm 1. The system has two main pathways -\none that acts as a teacher and discovers objects in videos or\nlarge collections of images and the other that acts as student\nand learns from the teacher to detect the foreground objects\nin single input images. We provide a general algorithm for\nunsupervised learning over several generations of students\nand teachers. We experiment with different types of student\nnets and show how they collectively work together to form\nthe teacher at the next generation. This is done in conjunc-\ntion with a novel unsupervised soft-mask selection scheme.\nWe demonstrate experimentally that within a generation the\nstudents are more powerful than their teachers, while both\npathways improve signiﬁcantly from one generation to the\nnext.\nFig. 1 The dual student-teacher system proposed for unsupervised\nlearning to detect foreground objects in images, functioning as pre-\nsented in Algorithm 1. It has two pathways: along the teacher branch,\nan object discoverer in videos or large image collections (module B)\ndetects foreground objects. The resulting soft masks are then ﬁltered\nbased on an unsupervised data selection procedure (module C). The\nresulting ﬁnal set of pairs - input image (a video frame) and soft mask\nfor that particular frame (which acts as an unsupervised label) - are\nused to train the student pathway (module A). The whole process can\nbe repeated over several generations. At each generation several stu-\ndent CNNs are trained, then they collectively contribute to form a more\npowerful teacher, at the next iteration of the overall algorithm.\n2) At the higher level, our proposed algorithm is sufﬁ-\nciently general to accommodate different implementations\nand neural network architectures. In this paper, we also pro-\nvide a speciﬁc implementation which we describe in detail.\nWe demonstrate its performance on three recent datasets,\nnamely YouTube Objects (Prest et al (2012)), Object Dis-\ncovery in Internet Images (Rubinstein et al (2013)) and\nPascal-S (Li et al (2014)), on which we obtain state of the\nart results. To our best knowledge, it is the ﬁrst system that\nlearns to detect and segment foreground objects in images\nin unsupervised fashion, with no pre-trained features given\nor manual labeling, while requiring only a single image at\ntest time.\n2 Scientiﬁc context\nThe literature on unsupervised learning follows two main di-\nrections. 1) One is to learn powerful features in an unsuper-\nvised way and then use them for transfer learning, within a\nsupervised scheme and in combination with different classi-\nﬁers, such as SVMs or CNNs (Radenovi´c et al (2016); Misra\net al (2016); Li et al (2016)). 2) The second direction is to\ndiscover, at test time, common patterns in unlabeled data,\nusing clustering, feature matching or data mining formula-\ntions (Jain et al (1999); Cho et al (2015); Sivic et al (2005)).\nBelonging to the ﬁrst category and closely related to our\nwork, the approach in Pathak et al (2017) proposes a sys-\nUnsupervised learning of foreground object detection\n3\ntem in which a deep neural network learns to produce soft\nobject masks from an unsupervised module that uses optical\nﬂow cues in video. The deep features learned in this manner\nare then applied to several transfer learning tasks. Different\nfrom their work, we provide a more general approach that\ncould learn in an unsupervised manner over several gener-\nations. From an experimental point of view, while Pathak\net al (2017) tests their work on a supervised transfer learn-\ning task, we evaluate ours on speciﬁc unsupervised fore-\nground object detection and segmentation tasks and demon-\nstrate state of the art performance, often by a large margin.\nRecently, researchers have started to use the natural, spa-\ntial and temporal structure in images and videos as super-\nvisory signals in unsupervised learning approaches that are\nconsidered to follow a self-supervised learning paradigm\n(Raina et al (2007); Lee et al (2017); Wang and Gupta\n(2015a)). Methods that fall into this category include those\nthat learn to estimate the relative patch positions in images\n(Doersch et al (2015)), predict color channels (Larsson et al\n(2016)), solve jigsaw puzzles (Noroozi and Favaro (2016))\nand inpaint (Pathak et al (2016)). One trend is to use as\nsupervisory signal, spatial and appearance information col-\nlected from raw single images. In such single-image cases\nthe amount of information that can be learned is limited to\na single moment in time, as opposed to the case of learning\nfrom video sequences. Using unlabeled videos as input is\ncloser related to our work and includes learning to predict\nthe temporal order of frames (Lee et al (2017)), generate the\nfuture frame (Finn et al (2016); Xue et al (2016); Goroshin\net al (2015)) or learn from optical ﬂow (Wang and Gupta\n(2015b)).\nFor most of these papers, the unsupervised learning\nscheme is only an intermediate step to train features that are\neventually used on classic supervised learning tasks, such as\nobject classiﬁcation, object detection or action recognition.\nSuch pre-trained features perform better than randomly ini-\ntialized ones, as they contain valuable semantic information\nimplicit in the natural structure of the world used as su-\npervisory signal. In our work, we focus mostly on speciﬁc\nunsupervised tasks on which we perform extensive evalu-\nations, but we also show some results on transfer learning\nexperiments.\nThe second main approach to unsupervised learning\nincludes methods for image co-segmentation (Joulin et al\n(2010); Kim et al (2011); Rubinstein et al (2013); Joulin\net al (2012); Kuettel et al (2012); Vicente et al (2011);\nRubio et al (2012); Leordeanu et al (2012)) and weakly su-\npervised localization (Deselaers et al (2012); Nguyen et al\n(2009); Siva et al (2013)). Earlier methods are based on\nlocal feature matching and detection of their co-occurrence\npatterns (Stretcu and Leordeanu (2015); Sivic et al (2005);\nLeordeanu et al (2005); Parikh and Chen (2007); Liu and\nChen (2007)), while more recent ones (Joulin et al (2014);\nRochan and Wang (2014)) discover object tubes by linking\ncandidate bounding boxes between frames with or without\nreﬁning their location. Traditionally, the task of unsuper-\nvised learning from image sequences has been formulated\nas a feature matching or data clustering optimization prob-\nlem, which is computationally very expensive due to its\ncombinatorial nature.\nThere are also other papers (Lee et al (2011); Cheng\net al (2017); Dutt Jain et al (2017); Tokmakov et al (2017))\nthat tackle unsupervised learning tasks but are not fully\nunsupervised, using powerful features that are pre-trained\nin supervised fashion on large datasets, such as ImageNet\n(Russakovsky et al (2015)) or VOC2012 (Everingham et al\n(2015)). Such works take advantage of the rich source of\nsupervised information learned from other datasets, through\nfeatures trained to respond to general object properties over\ntens or hundreds of object categories.\nWith respect to the end goal, our work is more related\nto the second research direction, on unsupervised discovery\nin video. However, unlike that research, we do not discover\nobjects at test time, but during the unsupervised training pro-\ncess, when the student pathway learns to detect foreground\nobjects. Therefore, from the learning perspective, our work\nis more related to the ﬁrst research direction based on self-\nsupervised training.\n3 Overall approach\nWe propose a genuine unsupervised learning algorithm for\nforeground object detection that offers the possibility to im-\nprove over several iterations. Our method combines in com-\nplementary ways multiple modules that are well suited for\nthis task. It starts with a teacher pathway that discovers ob-\njects in unlabeled videos and produces a soft mask of the\nforeground object in each frame. The resulting soft-masks\nof lower quality are then ﬁltered out automatically. Next,\nthe remaining ones are passed to a student ConvNet, which\nlearns to predict object masks in single images. When sev-\neral student nets of different architectures are learned they\nform a new teacher for the next generation, then the whole\nprocess is repeated. At the next iteration we bring in more\nunlabeled data, we learn in an unsupervised fashion a better\ndata selection mechanism and ultimately train more pow-\nerful student networks. In Algorithm 1 we enumerate con-\ncisely the main steps of our approach.\nNow we present the main algorithm in more detail.\nAt Step 1 we start with an object discoverer in video se-\nquences. There are several available methods for video dis-\ncovery in the literature, with good performance (Borji et al\n(2012); Cheng et al (2015); Barnich and Van Droogenbroeck\n(2011)). We chose the VideoPCA algorithm introduced as\npart of the system in Stretcu and Leordeanu (2015) because\n4\nIoana Croitoru et al.\nAlgorithm 1 Unsupervised learning of foreground object\ndetection\nStep 1: perform unsupervised object discovery in unlabeled videos,\nalong the teacher pathway (module B in Figure 1).\nStep 2: automatically ﬁlter out poor soft masks produced at the pre-\nvious step (module C in Figure 1).\nStep 3: use the remaining masks as supervisory signal for training\none or more student nets, along the student pathway (module A in\nFigure 1).\nStep 4: use the ensemble of student nets to form a new teacher and\nlearn a more powerful soft-mask selector, for the next iteration (re-\nferred to as a novel student-teacher generation).\nStep 5: extend the unlabeled video dataset and return to Step 1 to\ntrain the next generation (note that from this step forward, the train-\ning dataset can also be extended with collections of unlabeled im-\nages, not just videos).\nit is very fast (50-100 fps), uses very simple features (in-\ndividual pixel colors) and it is completely unsupervised,\nwith no usage of supervised pre-trained features. It learns\nhow to separate the foreground from the background. It ex-\nploits the spatio-temporal consistency in appearance, shape,\nmovement and location of objects, common in video shots,\nalong with the contrasting properties, in size, shape, motion\nand location, between the main object and the background\nscene. Note that it would be much harder, at this ﬁrst stage,\nto discover objects in collections of unrelated images, where\nthere is no smooth variation in shape, appearance and loca-\ntion over time. Only at the second iteration of the algorithm,\nthe simpler VideoPCA is replaced with a more powerful\nensemble of student nets which is able to discover objects\nin collections of images as well.\nThe teacher branch produces soft foreground masks, one\nper each frame, which are not always of good quality. Thus,\nat Step 2, we use, during the ﬁrst iteration, a simple and ef-\nfective way to ﬁlter out poor masks. Only at the second iter-\nation we are able to learn a more powerful soft-mask selec-\ntor (see Section 4.2.1). The soft-masks that pass the ﬁltering\nphase are then used (Algorithm 1, Step 3) to train the student\npathway. As we want the student branch to learn general vi-\nsual properties of objects in images, we limit its access to a\nsingle input image.\nOur approach offers the possibility of improving per-\nformance by training a next generation of object detectors.\nIn experiments, we found that there are three key aspects,\nwhich are effective at improving generalization at the next\niteration: 1) we need to train several student nets (at module\nA), preferably of different architectures, which are stronger\nin combination than separately. Then, they become the\nteacher (module B) at the next iteration; 2) we train, also in\nan unsupervised fashion, a better soft-mask selector (mod-\nule C); 3) it is preferred to increase the unlabeled training\nset at the next iteration, for improved generalization.\nHaving access to the complete training set at the very\nﬁrst iteration could be useful, but it is not optimal. At that\nstage, the teacher is still weak and imposes a certain lim-\nitation on how much could be learned from the data, no\nmatter how large that data is. Getting access to a larger un-\nlabeled training dataset is more effective at the second it-\neration, when the teacher pathway is signiﬁcantly stronger.\nThe idea of gradually increasing the complexity in the train-\ning set is also related to curriculum learning (Bengio et al\n(2009)), when we start with simpler cases then add more\ndifﬁcult ones. Increasing the strength of the teacher path-\nway improves the quality of the supervisory signal, while\nintroducing more unlabeled data increases variety. Both act\ntogether in order to improve generalization.\n4 System architecture\nWe, now, detail the architecture and training process of our\nsystem, module by module, as seen in Figure 1. We ﬁrst\npresent the student pathway (module A in Figure 1), which\ntakes as input an individual image (e.g. current frame in the\nvideo) and learns to predict foreground soft-masks from an\nunsupervised teacher. The teacher pathway (represented by\nmodules B and C in Figure 1), is explained in detail in the\nSection 4.2.\n4.1 Student path: single-image segmentation\nThe student processing pathway (module A in Figure 1)\nconsists of a deep convolutional network. We test different\nneural network architectures, some of which are commonly\nused in the recent literature on semantic image segmenta-\ntion. We create a small pool of relatively diverse architec-\ntures, presented next.\nThe ﬁrst convolutional network architecture for seman-\ntic segmentation that we test, is based on a more traditional\nCNN design. We term it LowRes-Net (see Figure 2) due to\nits low resolution soft-mask output. It has ten layers (seven\nconvolutional, two pooling and one fully connected) and\nskip connections. Skip connections have proved to offer a\nboost in performance, as shown in the literature (Raiko et al\n(2012); Pinheiro et al (2016)). We also observed a similar\nimprovement in our experiments when using skip connec-\ntions. The LowRes-Net takes as input a 128 × 128 RGB im-\nage (along with its hue, saturation and derivatives w.r.t. x and\ny) and produces a 32×32 soft segmentation of the main ob-\njects present in the image. Because LowRes-Net has a fully\nconnected layer at the top, we reduced the output resolution\nof the soft-segmentation mask, to limit memory cost. While\nthe derviatives w.r.t x and y are in principle not needed (as\nthey could be learned by appropriate ﬁlters during training),\nin our tests explicitly providing the derivatives along with\nHSV and by using skip-connections boosted the accuracy\nby over 1%. The LowRes-Net has a total of 78M parame-\nters, most of them being in the last, fully connected layer.\nUnsupervised learning of foreground object detection\n5\nFig. 2 Different architectures for the ”student” networks, each processing a single image. They are trained to predict the unsupervised label masks\ngiven by the teacher pathway, frame by frame. The architectures vary from the more classical baseline LowRes-Net (left), with low resolution\noutput, to more recent architectures, such as the fully convolutional one (middle) and different types of U-Nets (right). For the U-Net architecture\nthe blocks denoted with double arrows can be interchanged to obtain a new architecture. We noticed that on the task of bounding box ﬁtting the\nsimpler low-resolution network performed very well, while being outperformed by the U-Nets on ﬁne object segmentation.\nThe second CNN architecture tested, termed FConv-Net,\nis fully convolutional (Long et al (2015)), as also presented\nin Figure 2. It has a higher resolution output of 128x128,\nwith input size 256x256. Its main structure is derived from\nthe basic LowRes-Net model. Different from LowRes-Net,\nit is missing the fully connected layer at the end and has\nmore parameters in the convolutional layers, for a total of\n13M parameters.\nWe also tested three different nets based on the U-Net\n(Ronneberger et al (2015)) architecture, which proved very\neffective in the semantic segmentation literature. Our U-net\nnetworks are: 1) BasicU-Net, 2) DilateU-Net - similar to\nBasicU-Net but using atrous (dilated) convolutions (Yu and\nKoltun (2015)) in the center module, and 3) DenseU-Net -\nwith dense connections in the down and up modules (J´egou\net al (2017)).\nThe BasicU-Net has 5 down modules with 2 convolu-\ntional layers each, with 32, 64, 128, 256 and 512 features\nmaps, respectively. In the center module the BasicU-Net has\ntwo convolutional layers with 1024 feature maps each. The\nup modules have 3 convolutional layers and the same num-\nber of features maps as the corresponding down modules.\nThe only difference between BasicU-Net and DilateU-Net\nis that the former has a different center module with 6 atrous\nconvolutions and 512 feature maps each. Then, DenseU-\nNet has 4 down modules with 4 corresponding up modules.\nEach down and up module has 4 convolutions with skip-\nconnections (as presented in Figure 2). The modules have\n12, 24, 48 and 64 features maps, respectively. The transition\nrepresents a convolution, having the role of reducing the out-\nput number of feature maps from each module. The BasicU-\nNet has 34M parameters, while the DilateU-Net has 18M\nparameters. DenseU-Net has only 3M parameters, but uses\nskip-connections inside the up and down blocks in order to\nmake up for the difference in the number of parameters. All\nthree U-Nets have 256x256 input and same resolution out-\nput. All networks use ReLU activation functions. Please see\nFigure 2 for more speciﬁc details regarding the architectures\nof the different models.\nGiven the current setup, the student nets do not learn\nto identify speciﬁc object classes. They will learn to softly\nsegment the main foreground objects present, regardless\nof their particular category. The main difference in their\nperformance is in their ability to produce ﬁne object seg-\nmentations. While the LowRes-Net tends to provide a good\nsupport for estimating the object’s bounding box due to\nits simpler output, the other ConvNets (especially the U-\nNets), with higher resolution, are better at ﬁnely segmenting\nobjects. Due to the different ways in which the particu-\nlar models make mistakes, they are always stronger when\n6\nIoana Croitoru et al.\nforming an ensemble. In experiments we also show that they\noutperform their teacher and are able to detect objects from\ncategories that were not seen during training.\n4.1.1 Student networks ensemble\nThe pool of student networks with different architectures\nproduce varied results that differ qualitatively. While the\nbounding boxes computed from their soft-masks have sim-\nilar accuracy, the actual soft-segmentation output looks\ndifferently. They have different strengths, while making\ndifferent kinds of mistakes. The above observation immedi-\nately suggests that they should be stronger in combination,\nso we have experimented with the idea of combining them\ninto an ensemble. We propose two types of ensembles.\nThe ﬁrst one, termed Multi-Net, outputs a soft-mask that\nis obtained by multiplying pixel-wise the soft-masks pro-\nduced by each individual student net. Thus, only positive\npixels, on which all nets agree, survive to the ﬁnal segmen-\ntation. Multi-Net offers robust masks of signiﬁcantly higher\nquality. In Section 4.2.1 we show how Multi-Net can be ef-\nfectively used to learn in an unsupervised fashion, a network\n(EvalSeg-Net) for evaluating the goodness of a speciﬁc seg-\nmentation. That network is an important part of the next gen-\neration teacher pathway and replaces module C at the next\niteration.\nThe second approach to forming an ensemble is to\nuse EvalSeg-Net in order to select the best soft-mask\nfrom the pool of masks generated by the student nets.\nWe term this ensemble system, MultiSelect-Net. Quanti-\ntatively, MultiSelect-Net and Multi-Net perform similarly,\nbut Multi-Net tends to produce fuzzier masks due to the\nadditional multiplication of the student’s soft-masks.\n4.1.2 Training the student ConvNets\nWe treat foreground object segmentation as a multidimen-\nsional regression problem, where the soft mask given by the\nunsupervised video segmentation system acts as the desired\noutput. Let I be the input RGB image (a video frame) and Y\nbe the corresponding 0-255 valued soft segmentation given\nby the unsupervised teacher for that particular frame. The\ngoal of our network is to predict a soft segmentation mask\nˆY of width W and height H (where W = H = 32 for the\nbasic architecture, W = H = 128 for fully convolutional\narchitecture and W = H = 256 for U-Net architectures),\nthat approximates as well as possible the mask Y. For each\npixel in the output image, we predict a 0-255 value, so that\nthe total difference between Y and ˆY is minimized. Thus,\ngiven a set of N training examples, let I(n) be the input\nimage (a video frame), ˆY(n) be the predicted output mask\nfor I(n), Y(n) the soft segmentation mask (corresponding\nto I(n)) and w the network parameters. Y(n) is produced\nby the video discoverer after processing the video that I(n)\nbelongs to. Then, our loss is:\nL(w) = 1\nN\nN\nX\nn=1\nW ×H\nX\np=1\n(Y(n)\np\n−ˆY(n)\np (w, I(n)))\n2\n(1)\nwhere Y(n)\np\nand ˆY(n)\np\ndenotes the p-th pixel from Y(n),\nrespectively ˆY(n).\nWe observed that in our tests, the L2 loss performed\nbetter than the cross-entropy loss, due to the fact that the\nsoft-masks used as labels have real values, not discrete\nones. Also, they are not perfect, so the idea of threshold-\ning them for training does not perform as well as directly\npredicting their real values. We train our network using\nthe Tensorﬂow (Abadi et al (2015)) framework with the\nAdam optimizer (Kingma and Ba (2014)). All models are\ntrained end-to-end using a ﬁxed learning rate of 0.001 for\n10 epochs. The training time for any given model is about\n3-5 days on a Nvidia GeForce GTX 1080 GPU, for the ﬁrst\niteration and about 2 weeks for the second iteration students.\nPost-processing. The student CNN outputs a W × H soft\nmask. In order to fairly compare our models with other\nmethods, we have two different post processing steps: 1)\nbounding box ﬁtting and 2) segmentation reﬁnement. For\nﬁtting a box around the soft mask, we ﬁrst up-sample the\nW × H output to the original size of the image, then thresh-\nold the mask (validated on a small subset), determine the\nconnected components and ﬁt a tight box around each of the\ncomponents. We perform segmentation reﬁnement (point\n2) in a single case, on the Internet Images Dataset as also\nspeciﬁed in the experiments section. For that, we use the\nOpenCV implementation of GrabCut (Rother et al (2004))\nto reﬁne our soft mask, up-sampled to the original size. In\nall other tests we use the original output of the networks.\n4.2 Teacher path: unsupervised discovery in video\nThere are several methods available for discovering objects\nand salient regions in images and videos (Borji et al (2012);\nCheng et al (2015); Hou and Zhang (2007); Jiang et al\n(2013); Cucchiara et al (2003); Barnich and Van Droogen-\nbroeck (2011)) with reasonably good performance. More\nrecent methods for foreground objects discovery such as\nPapazoglou and Ferrari (2013) are both relatively fast and\naccurate, with runtime around 4 seconds per frame. How-\never, that runtime is still long and prohibitive for training\nthe student CNN that requires millions of images. For that\nreason we used at the ﬁrst generation (Iteration 1 of Algo-\nrithm 1) for module B in Figure 1, the VideoPCA algorithm,\nwhich is a part of the whole system introduced in Stretcu\nand Leordeanu (2015). It has lower accuracy than the full\nUnsupervised learning of foreground object detection\n7\nsystem, but it is much faster, running at 50 −100 fps. At\nthis speed we can produce one million unsupervised soft\nsegmentations in a reasonable time of about 5-6 hours.\nVideoPCA. The main idea behind VideoPCA is to model\nthe background in video frames with Principal Compo-\nnent Analysis. It ﬁnds initial foreground regions as parts\nof the frames that are not reconstructed well with the PCA\nmodel. Foreground objects are smaller than the background,\nhave contrasting appearance and more complex movements.\nThey could be seen as outliers, within the larger background\nscene. That makes them less likely to be captured well by\nthe ﬁrst PCA components. Thus, for each frame, an initial\nsoft-mask is produced from an error image, which is the\ndifference between the original image and the PCA recon-\nstruction. These error images are ﬁrst smoothed with a large\nGaussian ﬁlter and then thresholded. The binary masks ob-\ntained are used to learn color models of foreground and\nbackground, based on which individual pixels are classiﬁed\nas belonging to foreground or not. The object masks ob-\ntained are further multiplied with a large centered Gaussian,\nbased on the assumption that foreground objects are often\ncloser to the image center. These are the ﬁnal masks used in\nyour system. For more technical details, the reader is invited\nto consult Stretcu and Leordeanu (2015). In this work, we\nuse the method exactly as found online1 without any param-\neter tuning.\nTeacher pathway at the next generation: At the next iter-\nation of Algorithm 1, VideoPCA (in module B) is replaced\nby the student nets trained at the previous iteration in the fol-\nlowing way. While we could use as new module B any of the\ntwo ensembles Multi-Net or MultiSelect-Net, we preferred\na simpler and more efﬁcient approach. For each unlabeled\ntraining image we ran all student nets and obtain multiple\nsoft-masks, without combining them to produce a single out-\nput per image. Therefore the new module B is the collection\nof all student nets acting in parallel. Then, their soft-masks\nare ﬁltered independently (using a given threshold) by the\nnew Module C in Figure 1, which is represented at the sec-\nond iteration by EvalSeg-Net. Note that it is possible in this\nmanner to obtain one, several or no soft segmentations for\na given training image. This approach is fast and it offers\nthe advantage of processing data in parallel over multiple\nGPUs, without having to wait for all student nets to ﬁnish\nfor every input image. As our experiments demonstrate, the\napproach is also efﬁcient, with signiﬁcantly better results at\nthe second generation.\n4.2.1 Unsupervised soft masks selection\nThe performance of the student net is inﬂuenced by the\nquality of the soft masks provided as labels by the teacher\nbranch. The cleaner the masks, the more chances the student\nhas to learn to segment well objects in images. VideoPCA\ntends to produce good results if the object present in the\nvideo stands out well against the background scene, in\nterms of motion and appearance. However, if the object is\noccluded at some point, does not move w.r.t the scene or\nhas a similar appearance to its background, the resulting\nsoft masks might be poor. In the ﬁrst generation, we used\na simple measure of masks quality to select only the good\nsoft-masks for training the student pathway, based on the\nfollowing observation: when VideoPCA masks are close\nto the ground truth, the average of their nonzero values is\nusually high. Thus, when the discoverer is conﬁdent, it is\nmore likely to be right. The average value of non-zero pixels\nin the soft mask is then used as a score indicator for each\nsegmented frame. Only masks of certain quality according\nto this indicator are selected and used for training the stu-\ndent nets. This represents module C in Figure 1 at the ﬁrst\ngeneration of Algorithm 1. While being effective at iteration\n1, the simple average value over all pixels cannot capture\nthe goodness of a segmentation at the higher level of overall\nshape. At the next iterations, we therefore explore new ways\nto improve it.\nConsequently, at the next iterations we propose an un-\nsupervised way for learning the EvalSeg-Net to estimate\nsegmentation quality. As mentioned previously, Multi-Net\nprovides masks of higher quality as it cancels errors from\nindividual student nets. Thus, we use the cosine similarity\nbetween a given individual segmentation and the ensemble\nMulti-Net mask, as a cost for ”goodness” of segmentation.\nHaving this unsupervised segmentation cost we train the\nEvalSeg-Net deep neural net to predict it. As previously\nmentioned, this net acts as an automatic mask evaluation\nprocedure, which in subsequent iterations becomes module\nC in Figure 1, replacing the simple mask average value used\nat Iteration 1. Only masks that pass a certain threshold are\nused for training the student path.\nThe architecture of EvalSeg-Net is similar to LowRes-\nNet (Figure 2), with the difference that the input channel\ncontaining image derivatives is replaced by the actual soft-\nsegmentation that requires evaluation and it does not have\nskip connections. Also, after the last fully connected layer\n(size 512) we add a last one-neuron layer to predict the seg-\nmentation quality score, which is a single real valued num-\nber.\nLet I be an input RGB image, S an input soft-mask,\nˆY = Q5\ni=1 ˆYNi be the output of our Multi-Net where ˆYNi\ndenotes the output of network Ni. We treat the segmentation\n1 https://sites.google.com/site/multipleframesmatching/\n8\nIoana Croitoru et al.\nFig. 3 Purity of soft masks vs. degree of selection. When selectivity increases, the true purity of the training frames improves. Our automatic\nselection method is not perfect: some low quality masks may have high scores, while other good ones may be ranked lower. At the ﬁrst iteration\nof Algorithm 1 we select masks obtained with VideoPCA, while at the second generation we selected masks obtained with the teacher at the\nsecond generation. The plots are computed using results from the VID dataset, where there is an annotation for each input frame. Note the\nsigniﬁcantly better quality of masks at the second iteration (red vs. blue lines, in the left plot). We have also compared the simple ”mean” based\nselection procedure used at iteration 1 (yellow line) with EvalSeg-Net used at iteration 2 (red line), on the same soft masks from iteration 2. The\nEvalSeg-Net is more powerful, which justiﬁes its use at the second iteration when it replaces the simple ”mean” based procedure.\n”goodness” evaluation task as a regression problem where\nwe want to predict the Cosine similarity between S and ˆY.\nSo, our loss for EvalSeg-Net is deﬁned as follows:\nL(w) = 1\nK\nK\nX\nk=1\n\u0012\nˆo(k)(w, I(k), S(k)) −\nS(k)· ˆY(k)\n\r\rS(k)\r\r\n\r\r\r ˆY(k)\n\r\r\r\n\u00132\n(2)\nwhere K represents the number of training examples and\nˆo(k)(w, I(k), S(k)) represents the output of EvalSeg-Net for\nimage I(k) and soft mask S(k).\nGiven a certain metric for segmentation evaluation (de-\npending on the learning iteration), we keep only the soft\nmasks above a threshold for each dataset (e.g. VID (Rus-\nsakovsky et al (2015)), YTO (Prest et al (2012)), Youtube\nBounding Boxes (Real et al (2017))). In the ﬁrst itera-\ntion this threshold was obtained by sorting the VideoPCA\nsoft-masks based on their score and keeping only the top\n10 percentile, while on the second iteration we validate a\nthreshold (= 0.8) on a small dataset and select each mask\nindependently by using this threshold on the single value\noutput of EvalSeg-Net.\nMask selection evaluation. In Figure 3 we present the de-\npendency of segmentation performance w.r.t ground truth\nobject boxes (used only for evaluation) vs. the percentile p\nof masks kept after the automatic selection, for both gen-\nerations. We notice the strong correlation between the per-\ncentage of frames kept and the quality of segmentations. It\nis also evident that the EValSeg-Net is vastly superior to the\nsimpler procedure used at iteration 1. EvaSeg-Net is able to\ncorrectly evaluate soft segmentations even in more complex\ncases (see Figure 4).\nEven though, we can expect to improve the quality of\nthe unsupervised masks by drastically pruning them (e.g.\nkeeping a smaller percentage), the fewer we are left with,\nthe less training data we get, increasing the chance to overﬁt.\nWe make up for the losses in training data by augmenting\nthe set of training masks and by also enlarging the actual\nunlabeled training set at the second generation. There is a\ntrade-off between level of selectivity and training data size:\nthe more selective we are about what masks we accept for\ntraining, the more videos we need to collect and process\nthrough the teacher pathway, to obtain the sufﬁcient training\ndata size.\nData augmentation. A drawback of the teacher at the ﬁrst\nlearning iteration (VideoPCA) is that it can only detect the\nmain object if it is close to the center of the image. The\nassumption that the foreground is close to the center is of-\nten true and indeed helps that method, which has no deep\nlearned knowledge, to produce soft masks with a relatively\nhigh precision. Not surprisingly, it often fails when the ob-\nject is not in the center, therefore its recall is relatively low.\nOur data augmentation procedure addresses this limitation\nand can be concisely described as follows: randomly crop\npatches of the input image, covering 80% of the original im-\nage and scale up the patch to the expected input size. This\nproduces slightly larger objects at locations that cover the\nUnsupervised learning of foreground object detection\n9\nFig. 4 Qualitative results of the unsupervised EvalSeg-Net used for\nmeasuring segmentation ”goodness” and ﬁltering bad masks (Module\nC, iteration 2). For each input image we present ﬁve soft-masks candi-\ndates (from ﬁrst iteration students) along with their ”goodness” scores\ngiven by EvalSeg-Net, in decreasing order of scores. Note the effec-\ntiveness of EvalSeg-Net at ranking soft segmentations.\nwhole image area, not just the center. As experiments show,\nthe student net is able to see objects at different locations in\nthe image, unlike its raw teacher (VideoPCA at iteration 1),\nwhich is strongly biased towards the image center.\nAt the second generation, the teacher branch is signif-\nicantly better at detecting objects at various locations and\nscales in the image. Therefore, while artiﬁcial data augmen-\ntation remains useful (as it is usually the case in deep learn-\ning), its importance diminishes at the second iteration of\nlearning (Algorithm 1).\n4.3 Implementation pipeline\nNow that we have presented in technical detail all major\ncomponents of our system, we concisely present the actual\nsteps taken in our experiments, in sequential order, and show\nhow they relate to our general Algorithm 1 for unsupervised\nlearning to detect foreground objects in images.\n1. Run VideoPCA on input images from VID and YouTube\nObjects datasets (Algorithm 1, Iteration 1, Step 1)\n2. Select VideoPCA masks using ﬁrst generation selection\nprocedure (Algorithm 1, Iteration 1, Step 2)\n3. Train ﬁrst generation student ConvNets on the selected\nmasks, namely LowRes-Net, FConv-Net, BasicU-Net,\nDilateU-Net and DenseU-Net (Algorithm 1, Iteration 1,\nStep 3).\n4. Create ﬁrst generation student ensemble Multi-Net by\nmultiplying the outputs of all students and train EvalSeg-\nNet to predict the similarity between a particular mask\nand the mask of Multi-Net. Create the second ensemble\nMultiSelect-Net by using EvalSeg-Net in combination\nwith the student’s masks (Algorithm 1, Iteration 1, Step\n4).\n5. Add new data from YouTube Bounding Boxes. (Algo-\nrithm 1, Iteration 1, Step 5)\n6. Return to Step 1, the teacher pathway: predict multi-\nple soft-masks per input image on the enlarged unla-\nbeled video set, using the student nets from Iteration\n1 (Module B, Iteration 2), which will be then selected\nwith EvalSeg-Net at Module C. (Algorithm 1, Iteration\n2, Step 1)\n7. Select only sufﬁciently good masks evaluated with\nEvalSeg-Net (Algorithm 1, Iteration 2, Step 2)\n8. Train the second generation students on the newly se-\nlected masks. We use the same architectures as in Itera-\ntion 1 (Algorithm 1, Iteration 2, Step 3)\n9. Create the second generation student ensembles Multi-\nNet and MultiSelect-Net. (Algorithm 1, Iteration 2, Step\n4)\nThe method presented in the introduction sections (Al-\ngorithm 1) is a general algorithm for unsupervised learning\nfrom video to detect objects in single images. It presents\na sequence of high level steps followed by different mod-\nules for an unsupervised learning system. The modules are\ncomplementary to each other and function in tandem, each\nfocusing on a speciﬁc aspect of the unsupervised learning\nprocess. Thus, we have a module for generating data, where\nsoft-masks are produced. There is a module that selects good\nquality masks. Then, we have a module for training the next\ngeneration classiﬁers. While, our concept is ﬁrst presented\nin high level terms, we also present a speciﬁc implementa-\ntion that represents the ﬁrst two iterations of the algorithm.\nWhile our implementation is costly during training, in terms\nof storage and computation time, at test time it is very fast -\n0.02 sec per student net and 0.15 sec per student ensemble.\nComputation and storage costs. During training, the com-\nputation time for passing through the teacher pathway dur-\ning the ﬁrst iteration of Algorithm 1 is about 2-3 days: it re-\nquires processing data from VID and YTO datasets, includ-\ning running the VideoPCA module. Afterwards, training the\nﬁrst iteration students, with access to 6 GPUs, takes about 5\ndays - 6 GPUs are needed for training the 5 different student\narchitectures, since training FConv-Net requires two GPUs\n10\nIoana Croitoru et al.\nin parallel. Next, training the EvalSeg-Net requires 4 addi-\ntional days on one GPU. At the second iteration, processing\nthe data through the teacher pathway takes about 3 weeks on\n6 GPUs in parallel - it is more costly due to the larger train-\ning set from which only a small percent (about 10 percent) is\nselected with EvalSeg-Net. Finally, training the second gen-\neration students takes 2 additional weeks. In conclusion, the\ntotal computation time required for training, with full access\nto 6 GPUs is about 7 weeks, when everything is optimized.\nThe total storage cost is about 4TB. At test time the student\nnets are fast, taking 0.02 sec per image, while the ensemble\nnets take around 0.15 sec per image.\n5 Experimental analysis\nIn the ﬁrst set of experiments we evaluate the impact of\nthe different components of our system. We experimen-\ntally verify that at each iteration the students perform better\nthan their teachers. Then we test the ability of the system\nto improve from one generation to the next. We also test\nthe effects of data selection and increasing training data\nsize. Then, we compare the performances of each individual\nnetwork and their combined ensembles.\nIn Section 5.2, we compare our algorithm to state of the\nart methods on object discovery in videos and images. We\nperform tests on three datasets: YouTube Objects (Prest et al\n(2012)), Object Detection in Internet images (Rubinstein\net al (2013)) and Pascal-S (Li et al (2014)). In Section 5.3\nwe verify that our unsupervised deep features are also useful\nin different transfer learning tasks.\nDatasets. Unsupervised learning requires large quantities\nof unlabeled video data. We have chosen for training data,\nvideos from three large datasets: ImageNet VID dataset\n(Russakovsky et al (2015)), YouTube Objects (Prest et al\n(2012)) and YouTube Bounding Boxes (Real et al (2017)).\nVID is one of the largest video datasets publicly available,\nbeing fully annotated with ground truth bounding boxes.\nThe dataset consists of about 4000 videos, having a total\nof about 1.2M frames. The videos contain objects that be-\nlong to 30 different classes. Each frame could have zero,\none or multiple objects annotated. The benchmark chal-\nlenge associated with this dataset focuses on the supervised\nobject detection and recognition problem, which is differ-\nent from the one that we tackle here. Our system is not\ntrained to identify different object categories, so we do not\nreport results compared to the state of the art on object class\nrecognition and detection, on this dataset.\nYouTube Objects (YTO) is a challenging video dataset\nwith objects undergoing signiﬁcant changes in appearance,\nscale and shape, going in and out of occlusion against a vary-\ning, often cluttered background. YTO is at its second ver-\nsion now and consists of about 2500 videos, having a total\nof about 700K frames. It is speciﬁcally created for unsuper-\nvised object discovery, so we perform comparisons to state\nof the art on this dataset.\nFor unsupervised training of our system we used approx-\nimately 190k frames from videos chosen from each dataset\n(120k from VID and 70k from YTO), at learning iteration 1\n- those frames which survived after the data selection mod-\nule. At the second learning iteration, besides improving the\nclassiﬁer, it is important to have access to larger quantities\nof new unlabeled data. Therefore, for training the second\ngeneration of classiﬁers we added to the unlabeled training\nset additional 1 million soft-masks, as follows: 600k frames\nfrom VID and 400k from the YouTube Bounding Boxes\ndataset - again, those frames which survived after ﬁltering\nwith the EvalSeg-Net data selection module. Before data\nselection videos were randomly chosen from each set, VID\nor YouTube Bounding Boxes, until the total of 1M was\nreached. We did not add more frames due to heavy compu-\ntation and storage limitations.\nEvaluation metrics. We use different kinds of metrics in\nour experiments, which depend on the speciﬁc task that re-\nquires either bounding box ﬁtting or ﬁne segmentation:\n– CorLoc - for evaluating the detection of bounding boxes\nthe most commonly used metric is CorLoc. It is deﬁned\nas the percentage of images correctly localized accord-\ning to the PASCAL criterion: Bp∩BGT\nBp∪BGT ≥0.5, where BP\nis the predicted bounding box and BGT is the ground\ntruth bounding box.\n– F-β = (1−β2)precision×recall\nβ2×precision+recall\nfor evaluating the segmen-\ntation score on Pascal-S dataset. We use the ofﬁcial eval-\nuation code when reporting results. As in all previous\nworks, we set β2 = 0.3.\n– P-J metric P refers to the precision per pixel, while J\nis the Jaccard similarity (the intersection over union be-\ntween the output mask the and ground truth segmenta-\ntions). We use this metric only on Object Discovery in\nInternet images. For computing the reported results we\nuse the ofﬁcial evaluation code.\n– MAE - Mean Absolute Error is deﬁned as the average\npixel-wise difference between the predicted mask and\nthe ground truth. Different from the other metrics, for\nthis metric a lower value is better.\n– mean IoU score is deﬁned as |G∩Y |\n|G∪Y | where G represents\nthe ground truth and Y the predicted mask.\n5.1 Evaluation of different system components\nStudent vs. teacher In Figure 8 we present qualitative re-\nsults on VID dataset as compared to VideoPCA. We can see\nthat the masks produced by VideoPCA are of lower qual-\nity, often having holes, non-smooth boundaries and strange\nUnsupervised learning of foreground object detection\n11\nLowRes-Net\nFConv-Net\nDenseU-Net\nBasicU-Net\nDilateU-Net\nAvg\nMulti-Net\nMultiSelect-Net\nAvg\nIteration 1\n62.1\n57.6\n54.6\n59.1\n61.8\n59.0\n65.3\n62.4\n63.9\nIteration 2\n63.5\n61.3\n59.4\n65.2\n65.8\n63.0\n67.0\n67.3\n67.2\nGain\n1.4\n:\n3.7\n:\n4.8\n:\n6.1\n:\n4.0\n:\n4.0\n:\n1.7\n:\n4.9\n:\n3.3\n:\nTable 1 Results of our networks and ensembles on YouTube Objects v1 (Prest et al (2012)) dataset (CorLoc metric) at both iterations (generations).\nWe present the average of CorLoc metric of all 10 classes from YTO dataset for each model and ensemble, as well as the average of all single\nmodels and the average of the ensembles. As it can be seen, at the second generation there is a clear increase in performance for all models. Also\nnote that at the second generation a single model is able to outperform all the methods (single or ensemble) from the ﬁrst generation.\nLowRes-Net\nFConv-Net\nDenseU-Net\nBasicU-Net\nDilateU-Net\nAvg\nMulti-Net\nMultiSelect-Net\nAvg\nIteration 1\n85.8\n79.8\n83.3\n86.8\n85.6\n84.3\n85.8\n86.7\n86.3\nIteration 2\n86.7\n85.6\n86.7\n87.1\n87.9\n86.8\n86.4\n88.2\n87.3\nGain\n0.9\n:\n5.8\n:\n3.4\n:\n0.3\n:\n2.3\n:\n2.5\n:\n0.6\n:\n1.5\n:\n1.0\n:\nTable 2 Results of our networks and ensemble on Object Discovery in Internet Images (Rubinstein et al (2013)) dataset (CorLoc metric) at both\niterations (generations). We presented the average CorLoc metric of single models and ensembles per class as well as overall. Note that at the\nsecond generation there is a clear increase in performance for all methods. On average, single models from the second iteration are superior to the\nensembles from the ﬁrst.\nLowRes-Net\nFConv-Net\nDenseU-Net\nBasicU-Net\nDilateU-Net\nAvg\nMulti-Net\nMultiSelect-Net\nAvg\nIteration 1\n64.6\n51.5\n65.2\n65.4\n65.8\n62.5\n67.8\n67.1\n67.5\nIteration 2\n66.9\n61.7\n68.4\n68.0\n67.5\n66.5\n69.1\n68.5\n68.8\nGain\n2.3\n:\n10.2\n:\n3.2\n:\n2.6\n:\n1.7\n:\n4.0\n:\n1.3\n:\n1.4\n:\n1.3\n:\nTable 3 Results of our networks and ensemble on Pascal-S (Li et al (2014)) dataset (F-β metric), for all of our methods for ﬁrst and second\ngenerations as well as their average performance. Note that in this case, since we evaluate actual segmentations and not bounding box ﬁtting, nets\nwith higher resolution output perform better (DenseU-Net, BasicU-Net and DilateU-Net). Again, ensembles outperform single models and the\nsecond iteration brings a clear gain in every case.\nshapes. In contrast, the students learn more general shape\nand appearance characteristics of objects in images, remind-\ning of the grouping principles governing the basis of visual\nperception as studied by the Gestalt psychologists (Rock and\nPalmer (1990)) and the more recent work on the concept\nof ”objectness” (Alexe et al (2010)). The object masks pro-\nduced by the students are simpler, with very few holes, have\nnicer and smoother shapes and capture well the foreground-\nbackground contrast and organization. Another interesting\nobservation is that the students are able to detect multiple\nobjects, a feature that is less commonly achieved by the\nteacher.\nIn Figure 5 we see comparative results between the av-\nerage of individual models, the ensembles formed and the\nteacher. Note that the teacher at the next generation reported\nis the MultiSelect-Net ensemble from the ﬁrst. We observe\nthat the students at both iterations outperform their respec-\ntive teachers, which is an interesting and positive outcome.\nIt suggests that we can repeat the process over several itera-\ntions and continue to improve. It is also encouraging that the\nindividual nets, which see a single image, are able to gener-\nalize and detect objects that are discovered by the teacher in\nsequences of images.\nFirst vs. next generation. As seen in Tables 1, 2 3 and Fig-\nure 7 at the second generation we obtain a clear gain over\nthe ﬁrst, on all experiments and datasets. This result proves\nthe value of our proposed algorithm that starts from a com-\npletely unsupervised object discoverer in video (VideoPCA)\nand is able to train neural nets for foreground object segmen-\ntation, while improving their accuracy over two generations.\nIt uses the students from iteration 1 as teachers at iteration\n2. At the second iteration, it also uses more unlabeled train-\ning data and it is better at automatically ﬁltering out poor\nquality segmentations.\nImpact of data selection. Data selection is important as\nseen in Figure 6. The more selective we are when we accept\nor reject soft-masks used for training, the better the end re-\nsult. Also note that being more selective means decreasing\nthe training set. There is a trade-off between selectivity and\ntraining data size.\nNeural architecture vs. data. As seen in Tables 1, 2 and 3\ndifferent network architecture yield different results, while\nensembles always outperform individual models. While the\nactual CNN architecture has a certain role in performance,\nanother equally important aspect is that of data size. The\nmore data we have the more selective we can afford to be\nand also the more we could generalize. It is important to\nincrease the data from one generation to the next in order\nto avoid simply imitating the ensemble of the previous gen-\n12\nIoana Croitoru et al.\nFig. 5 Comparison between the teacher, individual student nets and the\nensembles, across two generations (blue line - ﬁrst iteration; red line -\nsecond iteration). Individual students (for which we report average val-\nues) outperform the teacher on both iterations, while the ensembles are\neven stronger than the individual nets. For the second iteration teacher\nwe report the MultiSelect-Net version of the ensemble (since we con-\nsider this to be an upper bound). The plots are computed over results\non the YouTube Objects dataset using the CorLoc metric (percentage).\nFig. 6 Impact of data selection for both iterations. Data selection\n(module C) strongly affects the results at each iteration. Note that\nresults from iteration 2 with no selection are slightly better than the\nones from iteration 1 with selection. This happens because the unla-\nbeled training data is increased and the second generation (iteration 2)\nteacher pathway is superior, providing better quality masks for training.\nThe results represent the average over 10 classes on YouTube Objects\nusing CorLoc percentage metric.\neration. In Tables 4 and 5 we show additional tests with\nour baseline architecture, LowRes-Net, when trained with\ntraining sets of different sizes. It is obvious that adding\nnew unlabeled data has a positive effect on performance.\nThe idea of increasing the data in stages is also related to\napproaches in curriculum learning (Bengio et al (2009)),\nwhere we ﬁrst learn from easy cases then move to the more\ncomplex ones.\nAnalysis of different ConvNets. Our experiments show that\ndifferent architectures are better at different tasks. LowRes-\nNet, for example, performs well on the task of box ﬁtting\nsince that does not require a ﬁne sharp object mask. On the\nother hand, when evaluating the exact segmentation, nets\nwith higher resolution output, which are more specialized\nfor this task perform better. Overall, at the second genera-\ntion, on box ﬁtting the best single net on average is DilateU-\nNet and the top ensemble is MultiSelect-Net. However,\nwhen it comes to evaluating the actual segmentation the\nwinner is DenseU-Net for single models and Multi-Net for\nensembles. In our qualitative results we ﬁnd that DenseU-\nNet produces masks with fewer ”holes” when compared to\nDilateU-Net, after thresholding and, thus, it is better suited\nfor segmentation evaluation. When evaluating the bounding\nbox, these holes do not affect the box and the best model\nis DilateU-Net. Also, DenseU-Net tends to outputs a mask\nwith higher conﬁdence on the whole object, as opposed to\nthe BasicU-Net and DilateU-Net that output masks with\nlower conﬁdence around some regions of the object (such\nas the eyes or wheels). This could be another reason why\nDenseU-Net produces better segmentations. The model that\nstruggles most during the ﬁrst iteration is FConv-Net, with\nsigniﬁcant improvement at the second iteration when the\nunsupervised training masks are closer to the correct ones.\nAlso note that the baseline LowRes-Net is a top model on\nbox ﬁtting at the ﬁrst iteration. The quantitative differences\nbetween architectures are shown in Tables 1, 2 and 3, while\nthe qualitative differences can be seen in Figure 7.\nTraining data\nCorLoc\nTesting dataset\nLowRes-Net\nVID\n56.1\nYTO\nLowRes-Net\nVID + YTO\n62.2\nTable 4 Inﬂuence of adding more unlabeled data, evaluated on YTO\nwith the CorLoc metric. As it can be seen, adding data signiﬁcantly\nincreases the performance by about 6%.\nTraining data\nmean P\nmean J\nLowRes-Net\nVID\n87.73\n61.25\nLowRes-Net\nVID + YTO\n88.36\n62.33\nTable 5 Inﬂuence of adding more unlabeled data on the Object Dis-\ncovery in Internet images dataset - PJ metric. The performance in-\ncreases by about 1%.\nUnsupervised learning of foreground object detection\n13\nFig. 7 Visual comparison between models at each iteration (generation). We marked with a purple dot the output of our MultiSelect-Net (the top\nselected student soft mask with EvalSeg-Net). The Multi-Net represents the pixel-wise multiplication between the ﬁve models. Note the superior\nmasks at the second generation, with better shapes, fewer holes and sharper edges.\n5.2 Comparisons with state of the art\nObject discovery in video. We ﬁrst performed comparisons\nwith methods speciﬁcally designed for object discovery in\nvideo. For that, we choose the YouTube Objects dataset and\ncompare it to the best methods on this dataset in the litera-\nture (Table 6). Evaluations are conducted on both versions\nof YouTube Objects dataset, YTOv1 (Prest et al (2012))\nand YTOv2.2 (Kalogeiton et al (2016)). On YTOv1 we fol-\nlow the same experimental setup as (Jun Koh et al (2016);\nPrest et al (2012)), by running experiments only on the\ntraining videos. We have not included in Table 6 the results\nreported by Stretcu and Leordeanu (2015) because they use\na different setup, testing on all videos from YTOv1. It is\nimportant to stress out, again, the fact that while the meth-\nods presented here for comparison have access to whole\nvideo shots, ours only needs a single image at test time.\nDespite this limitation, our method outperforms the others\non 7 out of 10 classes and has the best overall average per-\nformance. Note that even our baseline LowRes-Net at the\nﬁrst iteration achieves top performance. The feed-forward\nCNN processes each image in 0.02 sec, being at least one to\ntwo orders of magnitude faster than all other methods (see\nTable 6). We also mention that in all our comparisons, while\nour system is faster at test time, it takes much longer during\nits unsupervised training phase and requires large quantities\nof unsupervised training data.\nObject discovery in images We compare our system\nagainst other methods that perform image discovery in im-\nages. We use two different datasets for this comparison:\nObject Discovery in Internet Images and Pascal-S datasets.\nWe report results using metrics that are commonly used for\nthese tasks, as presented at the beginning of the experimen-\ntal section.\nObject Discovery in Internet Images is a representative\nbenchmark for foreground object detection in single images.\nThis set contains internet images and it is annotated with\nhigh detail segmentation masks. In order to enable compar-\nison with previous methods, we use the 100 images subsets\nprovided for each of the three categories: airplane, car and\nhorse. The methods evaluated on this dataset in the litera-\nture, aim to either discover the bounding box of the main\nobject in a given image or its ﬁne segmentation mask. We\nevaluate our system on both. Note that different from other\nworks, we do not need a collection of images during test\ntime, since each image can be processed independently by\nour system. Therefore, unlike other methods, our perfor-\nmance is not affected by the structure of the image collec-\ntion or the number of classes of interest being present in the\ncollection.\nIn Table 7 we present the performance of our method\nas compared to other unsupervised object discovery meth-\nods in terms of CorLoc on the Object Discovery dataset.\nWe compare our predicted box against the tight box ﬁtted\naround the ground-truth segmentation as done in Cho et al\n14\nIoana Croitoru et al.\nFig. 8 Qualitative results on the VID dataset (Russakovsky et al (2015)). For each iteration we show results of the best individual and\nensemble models, in terms of CorLoc metric. Note the superior quality of our models compared to the VideoPCA (iteration 1 teacher).\nWe also present the ground truth bounding boxes. For more qualitative results please visit our project page https://sites.google.com/view/\nunsupervisedlearningfromvideo\nMethod\nAero\nBird\nBoat\nCar\nCat\nCow\nDog\nHorse\nMbike\nTrain\nAvg\nTime\nVersion\nPrest et al (2012)\n51.7\n17.5\n34.4\n34.7\n22.3\n17.9\n13.5\n26.7\n41.2\n25.0\n28.5\nN/A\nPapazoglou and Ferrari (2013)\n65.4\n67.3\n38.9\n65.2\n46.3\n40.2\n65.3\n48.4\n39.0\n25.0\n50.1\n4s\nv1\nJun Koh et al (2016)\n64.3\n63.2\n73.3\n68.9\n44.4\n62.5\n71.4\n52.3\n78.6\n23.1\n60.2\nN/A\nHaller and Leordeanu (2017)\n76.3\n71.4\n65.0\n58.9\n68.0\n55.9\n70.6\n33.3\n69.7\n42.4\n61.1\n0.35s\nLowRes-Netiter1\n77.0\n67.5\n77.2\n68.4\n54.5\n68.3\n72.0\n56.7\n44.1\n34.9\n62.1\n0.02s\nLowRes-Netiter2\n79.7\n67.5\n68.3\n69.6\n59.4\n75\n78.7\n48.3\n48.5\n39.5\n63.5\n0.02s\nDilateU-Netiter2\n85.1\n72.7\n76.2\n68.4\n59.4\n76.7\n77.3\n46.7\n48.5\n46.5\n65.8\n0.02s\nMultiSelect-Netiter2\n84.7\n72.7\n78.2\n69.6\n60.4\n80.0\n78.7\n51.7\n50.0\n46.5\n67.3\n0.15s\nHaller and Leordeanu (2017)\n76.3\n68.5\n54.5\n50.4\n59.8\n42.4\n53.5\n30.0\n53.5\n60.7\n54.9\n0.35s\nv2.2\nLowRes-Netiter1\n75.7\n56.0\n52.7\n57.3\n46.9\n57.0\n48.9\n44.0\n27.2\n56.2\n52.2\n0.02s\nLowRes-Netiter2\n78.1\n51.8\n49.0\n60.5\n44.8\n62.3\n52.9\n48.9\n30.6\n54.6\n53.4\n0.02s\nDilateU-Netiter2\n74.9\n50.7\n50.7\n60.9\n45.7\n60.1\n54.4\n42.9\n30.6\n57.8\n52.9\n0.02s\nBasicU-Netiter2\n82.2\n51.8\n51.5\n62.0\n50.9\n64.8\n55.5\n45.7\n35.3\n55.9\n55.6\n0.02s\nMultiSelect-Netiter2\n81.7\n51.5\n54.1\n62.5\n49.7\n68.8\n55.9\n50.4\n33.3\n57.0\n56.5\n0.15s\nTable 6 Results on Youtube Objects dataset, versions v1 (Prest et al (2012)) and v2.2 (Kalogeiton et al (2016)). We achieve state of the art results\non both versions. Please note that the baseline LowRes-Net already achieves top results on v1, while being close to the best on v2.2. We present\nresults of the top individual and ensemble models and also keep the baseline LowRes-Net at both iterations, for reference. Note that complete\nresults on this dataset v1 for all models are also presented in Table 1.\n(2015); Tang et al (2014). Our system can be considered in\nthe mixed class category: it does not depend on the struc-\nture of the image collection. It treats each image indepen-\ndently. The performance of the other algorithms degrades\nas the number of main categories increases in the collection\n(some are not even tested by their authors on the mixed-class\ncase), which is not the case with our approach.\nWe obtain state of the art results on all classes, improv-\ning by a signiﬁcant margin over the method of Cho et al\n(2015). When the method in Cho et al (2015) is allowed to\nsee a collection of images that are limited to a single major-\nity class, its performance improves and it is equal with ours\non one class. However, our method has no other information\nnecessary besides the input image, at test time.\nWe also tested our method on the task of ﬁne foreground\nobject segmentation and compared to the best performers in\nthe literature on the Object Discovery dataset in Table 8. For\nreﬁning our soft masks we apply the GrabCut method, as it\nUnsupervised learning of foreground object detection\n15\nFig. 9 Qualitative results on the Object Discovery dataset as compared to (B) Rubinstein et al (2013). For both iterations we present results of the\ntop single and ensemble models (C-F), without using GrabCut. We also present results when GrabCut is used with the top ensemble (G). Note that\nour models are able to segment objects from classes that were not present in the training set (examples on the right side). Also, note that the initial\nVideoPCA teacher cannot be applied on single images.\nMethod\nAirplane\nCar\nHorse\nAvg\nKim et al (2011)\n21.95\n0.00\n16.13\n12.69\nJoulin et al (2010)\n32.93\n66.29\n54.84\n51.35\nJoulin et al (2012)\n57.32\n64.04\n52.69\n58.02\nRubinstein et al (2013)\n74.39\n87.64\n63.44\n75.16\nTang et al (2014)\n71.95\n93.26\n64.52\n76.58\nCho et al (2015)\n82.93\n94.38\n75.27\n84.19\nCho et al (2015) mixed\n81.71\n94.38\n70.97\n82.35\nLowRes-Netiter1\n87.80\n95.51\n74.19\n85.83\nLowRes-Netiter2\n93.90\n92.13\n74.19\n86.74\nDilateU-Netiter2\n95.12\n95.51\n73.12\n87.92\nMultiSelect-Netiter2\n93.90\n95.51\n75.27\n88.22\nTable 7 Results on the Object Discovery in Internet images (Rubin-\nstein et al (2013)) dataset (CorLoc metric). The results obtained in the\nﬁrst iteration are further improved in the second one. We present the\nbest single and ensemble models, along with the baseline LowRes-Net\nat both iterations. Among the single models DilateU-Net is often the\nbest when evaluating box ﬁtting.\nis available in OpenCV. We evaluate based on the same P, J\nevaluation metric as described by Rubinstein et al (2013) -\nthe higher P and J, the better. In Figure 9 and 10 we present\nsome qualitative results for each class. As mentioned pre-\nviously, these experiments on Object Discovery in Internet\nAirplane\nCar\nHorse\nP\nJ\nP\nJ\nP\nJ\nKim et al (2011)\n80.20\n7.90\n68.85\n0.04\n75.12\n6.43\nJoulin et al (2010)\n49.25\n15.36\n58.70\n37.15\n63.84\n30.16\nJoulin et al (2012)\n47.48\n11.72\n59.20\n35.15\n64.22\n29.53\nRubinstein et al (2013)\n88.04\n55.81\n85.38\n64.42\n82.81\n51.65\nChen et al (2014)\n90.25\n40.33\n87.65\n64.86\n86.16\n33.39\nLowRes-Netiter1\n91.41\n61.37\n86.59\n70.52\n87.07\n55.09\nLowRes-Netiter2\n90.61\n60.19\n87.05\n71.52\n88.73\n55.31\nDenseU-Netiter2\n91.03\n64.46\n85.71\n72.51\n87.14\n55.44\nMulti-Netiter2\n91.13\n66.02\n87.67\n73.98\n88.83\n55.23\nTable 8 Results on the Object Discovery in Internet images (Rubin-\nstein et al (2013)) dataset using (P, J metric) on segmentation evalua-\ntion. We present results of the top single and ensemble models, along\nwith LowRes-Net at both iterations. On the task of ﬁne object seg-\nmentation the best individual model tends to be DenseU-Net as also\nmentioned in the text. Note that we applied GrabCut on these experi-\nments only as a post-processing step, since all methods reported in this\nTable also used it.\nImages are the only ones on which we apply GrabCut as a\npost-processing step, as also used by all competing methods\npresented in Table 8.\nAnother important dataset used for the evaluation of\na related task, that of salient object detection, is Pascal-S\ndataset, consisting of 850 images. As seen from Table 9\n16\nIoana Croitoru et al.\nFig. 10 Qualitative results on the Object Discovery in Internet Images Rubinstein et al (2013) dataset. For each example we show the input RGB\nimage and immediately below our segmentation result, with GrabCut post processing for obtaining a hard segmentation. Note that our method\nproduces good quality segmentation results, even in images with cluttered background.\nwe achieve top results on all three metrics against methods\nthat do not use any supervised pre-trained features. Being\na foreground object detection method, our approach is usu-\nally biased towards the main object in the image - even\nthough it can also detect multiple ones. Images in Pascal-S\nusually have more objects, so we consider our results very\nencouraging being close to approaches that use features pre-\ntrained in a supervised manner. Also note that we did not\nuse GrabCut for these experiments.\nOn single image experiments, our system was trained,\nas discussed before on other, video datasets (VID, YTO and\nYTB). It has not previously seen any of the images in Pascal-\nS or Object Discovery datasets during training.\n5.3 Transfer learning experiments\nWhile the focus of the paper is foreground object detection\nin the unsupervised learning setup, we also want to verify\nthe usefulness of our approach on transfer learning experi-\nments. We design experiments to test two aspects of our sys-\ntem - the actual unsupervised features learned and the ﬁnal\noutput foreground mask. We perform tests on YouTube Ob-\njects v1 dataset, in a relatively standard supervised classiﬁ-\ncation setup, by learning to classify individual video frames\nwith the class given by their parent video shot - for a total of\nten classes.\nWe use the frames from the YTO training videos for\ntraining and the ones from the YTO test videos for test-\nMethod\nFβ\nMAE\nmean\nIoU\npre-trained\nsupervised\nfeatures?\nWei et al (2012)\n56.2\n22.6\n41.6\nno\nLi et al (2015)\n56.8\n19.2\n42.4\nno\nZhu et al (2014)\n60.0\n19.7\n43.9\nno\nYang et al (2013)\n60.7\n21.7\n43.8\nno\nZhang et al (2015)\n60.8\n20.2\n44.3\nno\nTu et al (2016)\n60.9\n19.4\n45.3\nno\nZhang et al (2017)\n68.0\n14.1\n54.9\ninit VGG\nLowRes-Netiter1\n64.6\n19.6\n48.7\nno\nLowRes-Netiter2\n66.9\n18.3\n51.4\nno\nDenseU-Netiter2\n68.4\n17.6\n51.6\nno\nMulti-Netiter2\n69.1\n19.2\n53.0\nno\nTable 9 Results on the PASCAL-S dataset compared against other un-\nsupervised methods. For MAE score lower is better, while for Fβ and\nmean IoU higher is better. We reported max Fβ, min MAE and max\nmean IoU for every method. In bold we presented the top results when\nno supervised pre-trained features were used.\ning. We test on a frame by frame basis and report the av-\nerage multiclass classiﬁcation percentage - how often the\ncorrect class is chosen out of ten classes. This problem is dif-\nﬁcult for several reasons: 1) the training and testing frames\ncome from different videos in YTO, that vary signiﬁcantly\nin appearance and background scene 2) the object of inter-\nest is not present in every frame, which makes the classiﬁca-\ntion rely heavily on the contextual scene. 3) there are multi-\nple objects in many frames, having a cluttered background,\nUnsupervised learning of foreground object detection\n17\nwhile the object of interest goes through different changes\nin scale, viewpoint and pose.\nWe have two experimental setups for this task, one\nfocused on the pre-trained features and the other on the\nforeground masks. In the ﬁrst setup, we replace the last\nfully connected layer from our baseline model LowRes-Net\nwith a classiﬁcation part and freeze the network up to a\ngiven depth, using as pre-trained features the ones from the\nunsupervised learning task. Then, we ﬁne-tune the end part\non the given supervised classiﬁcation task. In the second\nexperimental setup we extract features from VGG network\npre-trained (Simonyan and Zisserman (2014)) on ImageNet\nfrom different subwindows of the image, one being the\nbounding box given by the unsupervised LowRes-Net. Both\ntests that are presented next in more detail, prove that our\napproach is useful on transfer learning tasks.\nUsing the unsupervised features. In this experimental\nsetup, we replace the last fully connected layer with classi-\nﬁcation part, composed of a reduction convolutional layer\nhaving four ﬁlters and a ﬁnal fully connected layer with 10\nneurons. We test various cases by freezing different parts of\nthe LowRes network and ﬁne-tune the rest on the supervised\nclassiﬁcation task. The results are presented in Figure 11.\nThey strongly suggest that the features learned in an\nunsupervised way from the middle of the network are best\nsuited for semantic classiﬁcation. The result clearly demon-\nstrates the usefulness of the unsupervised features on the su-\npervised classiﬁcation task. In all cases when these features\nare used the results are improved (”concat”, ”conv2 2”, ”init\npre-trained”) except for one case, ”conv3 3”. This happens\nbecause the pretrained features used in this case are from\nthe top level - when the ﬁnal segmentation is produced. At\nthat level the semantic information is already lost. On the\ncontrary, when features are frozen at the middle of the net-\nwork, the best results are obtained.\nUsing the detected foreground bounding box. In these\nexperiments we extract ’fc7’ VGG19 features, pre-trained\non ImageNet, by passing through VGG19 different subwin-\ndows of the image rescaled appropriately, namely the whole\nimage, the center box with height and width being half the\noriginal image size and the window cropped according to the\nbounding box produced by LowRes-Net. We concatenate\nsuch features taken from these windows in different com-\nbinations and pass them through a last fully connected layer\nwith 10 neurons, which we train on the given classiﬁcation\ntask. We then, test the different combinations as shown in\nTable 10. When using features extracted from the bounding-\nbox ﬁtted with LowRes-Net (alone or in combination with\nthe whole image), we obtain signiﬁcantly better results com-\npared to the case when windows are extracted from ﬁxed lo-\ncations only (middle box, whole image or in combination).\nFig. 11 Transfer learning with pre-trained unsupervised features: the\n”random freeze” case is when the network is randomly initialized then\nfrozen and the classiﬁcation part is trained; ”random init” is when the\nwhole network and the classiﬁcation part are randomly initialized and\nthen trained jointly, end-to-end; ”concat”, ”conv2 2” and ”conv3 3”\nrefer to cases where the network trained in the unsupervised way is\nfrozen up to and including that speciﬁed layer, and the rest, includ-\ning the classiﬁcation part, is then trained; the ”init pre-trained” case\nis when the network is initialized with the pre-trained features from\nLowRes-Net then everything ﬁne-tuned on the classiﬁcation task. The\nresults indicate that the optimal case is when unsupervised pre-trained\nfeatures from the middle part are used, which are more likely to be\nrelevant for different semantic classes than the last deep features that\nproduce the ﬁnal segmentation.\nRegion of extracted features\nMulticlass recognition rate\nWhole image\n69.1\nMiddle crop image\n64.9\nCropped image by LowRes-Net\n70.2\nWhole + middle crop\n67.2\nWhole + cropped by LowRes-Net\n72.7\nTable 10 Classiﬁcation experiments using the foreground mask. Dif-\nferent sub-windows of the image are passed through VGG19 and fea-\ntures are extracted for the given classiﬁcation task. Note that a signiﬁ-\ncant boost is obtained when features are also extracted from the bound-\ning box ﬁtting based on the soft-mask predicted by LowRes-Net.\nThese results verify that the foreground segmentation mask\ndetected with our models is, as expected, directly related to\nthe main video class and constitutes a valuable source of in-\nformation in image classiﬁcation tasks.\nOverall, the classiﬁcation experiments presented in this\nSection indicate that the features learned in an unsupervised\nmanner with our algorithm contain relevant semantic infor-\nmation about object classes and could be useful for related\nsupervised learning tasks.\n6 Short discussion on unsupervised learning\nThe ultimate goal of unsupervised learning might not be\nabout matching the performance of the supervised case but\n18\nIoana Croitoru et al.\nrather about reaching beyond the capabilities of the classi-\ncal supervised scenario. An unsupervised system should be\nable to learn and recognize different object classes, such as\nanimals, plants and man-made objects, as they evolve and\nchange over time, from the past and into the unknown future.\nIt should also be able to learn about new classes that might\nbe formed, in relation to others, maybe known ones. We see\nthis case as fundamentally different from the supervised one\nin which the classiﬁer is forced to learn from a distribution\nof samples that is ﬁxed and limited to a speciﬁc period of\ntime - that when the human labeling was performed.\nTherefore, in the supervised learning paradigm a car\nfrom the future, should not be classiﬁed as car, because it\nis not a car, according to the supervised distribution of cars\ngiven at present training time, when human annotations are\ncollected. On the other hand, a system that learns by itself\nshould be able to track how cars have been changing in time\nand recognize such objects as ”cars” - with no step by step\nhuman intervention.\nFrom a temporal perspective, unsupervised learning is\nabout continuous learning and adaptation to huge quantities\nof data that are perpetually changing. Human annotation is\nextremely limited in an ocean of data and not able to pro-\nvide the so called ”ground truth” information continuously.\nTherefore, unsupervised learning will soon become a core\npart, larger than the supervised one, in the future of artiﬁcial\nintelligence.\n7 Conclusions and future work\nIn this article, we present a novel and effective approach to\nlearning from video, in an unsupervised fashion, to detect\nforeground objects in single images. We present a relatively\ngeneral algorithm for this task, which offers the possibility\nof learning several generations of students and teachers. We\ndemonstrate in practice that the system improves its perfor-\nmance over the course of two generations. We also test the\nimpact of the different system components on performance\nand show state of the art results on three different datasets.\nTo our best knowledge, it is the ﬁrst system that learns to\ndetect and segment foreground objects in images in an unsu-\npervised fashion, with no pre-trained features given or man-\nual labeling, while requiring only a single image at test time.\nThe convolutional networks trained along the student\npathway are able to learn general ”objectness” characteris-\ntics, which include good form, closure, smooth contours, as\nwell as contrast with the background. What the simpler ini-\ntial VideoPCA teacher discovers over time, the deep, com-\nplex student is able to learn across several layers of im-\nage features at different levels of abstraction. Our results on\ntransfer learning experiments are also encouraging and show\nadditional cases in which such a system could be useful. In\nfuture work we plan to further grow our computational and\nstorage capabilities to demonstrate the power of our unsu-\npervised learning algorithm along many generations of stu-\ndent and teacher networks. We believe that our approach,\ntested here in extensive experiments, will bring a valuable\ncontribution to computer vision research.\nAcknowledgements This work was supported by UEFISCDI, under\nprojects\nPN-III-P4-ID-ERC-2016-0007,\nPN-III-P2-2.1-PED-2016-\n1842 and PN-III-P1-1.2-PCCDI-2017-0734.\nReferences\nAbadi M, Agarwal A, Barham P, Brevdo E, Chen Z,\net al (2015) Tensorﬂow: Large-scale machine learning on\nheterogeneous systems. Software available from tensor-\nﬂoworg\nAlexe B, Deselaers T, Ferrari V (2010) What is an object?\nIn: CVPR\nBarnich O, Van Droogenbroeck M (2011) Vibe: A universal\nbackground subtraction algorithm for video sequences.\nIEEE Transactions on Image processing 20(6):1709–\n1724\nBengio Y, Louradour J, Collobert R, Weston J (2009) Cur-\nriculum learning. In: Proceedings of the 26th annual inter-\nnational conference on machine learning, ACM, pp 41–48\nBorji A, Sihite D, Itti L (2012) Salient object detection: A\nbenchmark. In: ECCV\nChen X, Shrivastava A, Gupta A (2014) Enriching visual\nknowledge bases via object discovery and segmentation.\nIn: CVPR\nCheng J, Tsai YH, Wang S, Yang MH (2017) Segﬂow: Joint\nlearning for video object segmentation and optical ﬂow.\nIn: The IEEE International Conference on Computer Vi-\nsion (ICCV)\nCheng M, Mitra N, Huang X, Torr P, Hu S (2015) Global\ncontrast based salient region detection. PAMI 37(3)\nCho M, Kwak S, Schmid C, Ponce J (2015) Unsupervised\nobject discovery and localization in the wild: Part-based\nmatching with bottom-up region proposals. In: CVPR\nCroitoru I, Bogolin SV, Leordeanu M (2017) Unsupervised\nlearning from video to detect foreground objects in single\nimages. In: Computer Vision (ICCV), 2017 IEEE Inter-\nnational Conference on, IEEE, pp 4345–4353\nCucchiara R, Grana C, Piccardi M, Prati A (2003) Detecting\nmoving objects, ghosts, and shadows in video streams.\nPAMI 25(10)\nDeselaers T, Alexe B, Ferrari V (2012) Weakly supervised\nlocalization and learning with generic knowledge. IJCV\n100(3)\nDoersch C, Gupta A, Efros AA (2015) Unsupervised visual\nrepresentation learning by context prediction. In: Pro-\nceedings of the IEEE International Conference on Com-\nputer Vision, pp 1422–1430\nUnsupervised learning of foreground object detection\n19\nDutt Jain S, Xiong B, Grauman K (2017) Fusionseg: Learn-\ning to combine motion and appearance for fully automatic\nsegmentation of generic objects in videos. In: The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR)\nEveringham M, Eslami SMA, Van Gool L, Williams CKI,\nWinn J, Zisserman A (2015) The pascal visual object\nclasses challenge: A retrospective. International Journal\nof Computer Vision 111(1):98–136\nFinn C, Goodfellow I, Levine S (2016) Unsupervised learn-\ning for physical interaction through video prediction. In:\nAdvances in neural information processing systems, pp\n64–72\nGoroshin R, Mathieu MF, LeCun Y (2015) Learning to lin-\nearize under uncertainty. In: Advances in Neural Informa-\ntion Processing Systems, pp 1234–1242\nHaller E, Leordeanu M (2017) Unsupervised object segmen-\ntation in video by efﬁcient selection of highly probable\npositive features. In: The IEEE International Conference\non Computer Vision (ICCV)\nHou X, Zhang L (2007) Saliency detection: A spectral resid-\nual approach. In: CVPR\nJain AK, Murty MN, Flynn PJ (1999) Data clustering: a re-\nview. ACM computing surveys 31(3):264–323\nJ´egou S, Drozdzal M, Vazquez D, Romero A, Bengio Y\n(2017) The one hundred layers tiramisu: Fully convolu-\ntional densenets for semantic segmentation. In: Computer\nVision and Pattern Recognition Workshops (CVPRW),\n2017 IEEE Conference on, IEEE, pp 1175–1183\nJiang H, Wang J, Yuan Z, Wu Y, Zheng N, Li S (2013)\nSalient object detection: A discriminative regional feature\nintegration approach. In: CVPR\nJoulin A, Bach F, Ponce J (2010) Discriminative clustering\nfor image co-segmentation. In: CVPR\nJoulin A, Bach F, Ponce J (2012) Multi-class cosegmenta-\ntion. In: CVPR\nJoulin A, Tang K, Fei-Fei L (2014) Efﬁcient image and\nvideo co-localization with Frank-Wolfe algorithm. In:\nECCV\nJun Koh Y, Jang WD, Kim CS (2016) Pod: Discovering\nprimary objects in videos based on evolutionary reﬁne-\nment of object recurrence, background, and primary ob-\nject models. In: CVPR\nKalogeiton V, Ferrari V, Schmid C (2016) Analysing domain\nshift factors between videos and images for object detec-\ntion. PAMI 38(11)\nKim G, Xing E, Fei-Fei L, Kanade T (2011) Dis-\ntributed cosegmentation via submodular optimization on\nanisotropic diffusion. In: ICCV\nKingma D, Ba J (2014) Adam: A method for stochastic op-\ntimization. arXiv preprint arXiv:14126980\nKuettel D, Guillaumin M, Ferrari V (2012) Segmentation\npropagation in imagenet. In: ECCV\nLarsson G, Maire M, Shakhnarovich G (2016) Learning\nrepresentations for automatic colorization. In: European\nConference on Computer Vision, Springer, pp 577–593\nLee HY, Huang JB, Singh M, Yang MH (2017) Unsuper-\nvised representation learning by sorting sequences. In:\n2017 IEEE International Conference on Computer Vision\n(ICCV), IEEE, pp 667–676\nLee YJ, Kim J, Grauman K (2011) Key-segments for video\nobject segmentation. In: Computer Vision (ICCV), 2011\nIEEE International Conference on, IEEE, pp 1995–2002\nLeordeanu M, Collins R, Hebert M (2005) Unsupervised\nlearning of object features from video sequences. In:\nCVPR\nLeordeanu M, Sukthankar R, Hebert M (2012) Unsuper-\nvised learning for graph matching. Int J Comput Vis\n96:28–45\nLi D, Hung WC, Huang JB, Wang S, Ahuja N, Yang MH\n(2016) Unsupervised visual representation learning by\ngraph-based consistent constraints. In: ECCV\nLi N, Sun B, Yu J (2015) A weighted sparse coding frame-\nwork for saliency detection. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp 5216–5223\nLi Y, Hou X, Koch C, Rehg JM, Yuille AL (2014) The se-\ncrets of salient object segmentation. Georgia Institute of\nTechnology\nLiu D, Chen T (2007) A topic-motion model for unsuper-\nvised video object discovery. In: CVPR\nLong J, Shelhamer E, Darrell T (2015) Fully convolutional\nnetworks for semantic segmentation. In: Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pp 3431–3440\nMisra I, Zitnick CL, Hebert M (2016) Shufﬂe and learn: un-\nsupervised learning using temporal order veriﬁcation. In:\nECCV\nNguyen M, Torresani L, la Torre FD, Rother C (2009)\nWeakly supervised discriminative localization and clas-\nsiﬁcation: a joint learning process. In: CVPR\nNoroozi M, Favaro P (2016) Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles. In: European\nConference on Computer Vision, Springer, pp 69–84\nPapazoglou A, Ferrari V (2013) Fast object segmentation in\nunconstrained video. In: ICCV\nParikh D, Chen T (2007) Unsupervised identiﬁcation of\nmultiple objects of interest from multiple images: dis-\ncover. In: Asian Conference on Computer Vision\nPathak D, Krahenbuhl P, Donahue J, Darrell T, Efros AA\n(2016) Context encoders: Feature learning by inpainting.\nIn: Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pp 2536–2544\nPathak D, Girshick R, Dollar P, Darrell T, Hariharan B\n(2017) Learning features by watching objects move. In:\nCVPR\n20\nIoana Croitoru et al.\nPinheiro PO, Lin TY, Collobert R, Doll´ar P (2016) Learning\nto reﬁne object segments. In: ECCV\nPrest A, Leistner C, Civera J, Schmid C, Ferrari V (2012)\nLearning object class detectors from weakly annotated\nvideo. In: CVPR, IEEE, pp 3282–3289\nRadenovi´c F, Tolias G, Chum O (2016) Cnn image retrieval\nlearns from bow: Unsupervised ﬁne-tuning with hard ex-\namples. In: ECCV\nRaiko T, Valpola H, LeCun Y (2012) Deep learning made\neasier by linear transformations in perceptrons. In: AIS-\nTATS, vol 22, pp 924–932\nRaina R, Battle A, Lee H, Packer B, Ng AY (2007) Self-\ntaught learning: transfer learning from unlabeled data. In:\nProceedings of the 24th international conference on Ma-\nchine learning, ACM, pp 759–766\nReal E, Shlens J, Mazzocchi S, Pan X, Vanhoucke V (2017)\nYoutube-boundingboxes: A large high-precision human-\nannotated data set for object detection in video. In: 2017\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), IEEE, pp 7464–7473\nRochan M, Wang Y (2014) Efﬁcient object localization and\nsegmentation in weakly labeled videos. In: Advances in\nVisual Computing, Springer, pp 172–181\nRock I, Palmer S (1990) Gestalt psychology. Sci Am\n263:84–90\nRonneberger O, Fischer P, Brox T (2015) U-net: Convolu-\ntional networks for biomedical image segmentation. In:\nInternational Conference on Medical image computing\nand computer-assisted intervention, Springer, pp 234–241\nRother C, Kolmogorov V, Blake A (2004) Grabcut: Inter-\nactive foreground extraction using iterated graph cuts. In:\nACM Transactions on Graphics, vol 23, pp 309–314\nRubinstein M, Joulin A, Kopf J, Liu C (2013) Unsupervised\njoint object discovery and segmentation in internet im-\nages. In: CVPR\nRubio J, Serrat J, L´opez A (2012) Video co-segmentation.\nIn: ACCV\nRussakovsky O, et al (2015) Imagenet large scale visual\nrecognition challenge. IJCV 115(3)\nSimonyan K, Zisserman A (2014) Very deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:14091556\nSiva P, Russell C, Xiang T, Agapito L (2013) Look-\ning beyond the image: Unsupervised learning for object\nsaliency and detection. In: CVPR\nSivic J, Russell B, Efros A, Zisserman A, Freeman W\n(2005) Discovering objects and their location in images.\nIn: ICCV\nStretcu O, Leordeanu M (2015) Multiple frames matching\nfor object discovery in video. In: BMVC\nTang K, Joulin A, Li LJ, Fei-Fei L (2014) Co-localization in\nreal-world images. In: CVPR\nTokmakov P, Alahari K, Schmid C (2017) Learning motion\npatterns in videos. In: The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR)\nTu WC, He S, Yang Q, Chien SY (2016) Real-time salient\nobject detection with a minimum spanning tree. In: Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp 2334–2342\nVicente S, Rother C, Kolmogorov V (2011) Object coseg-\nmentation. In: CVPR\nWang X, Gupta A (2015a) Unsupervised learning of\nvisual\nrepresentations\nusing\nvideos.\narXiv\npreprint\narXiv:150500687\nWang X, Gupta A (2015b) Unsupervised learning of visual\nrepresentations using videos. In: The IEEE International\nConference on Computer Vision (ICCV)\nWei Y, Wen F, Zhu W, Sun J (2012) Geodesic saliency using\nbackground priors. In: European conference on computer\nvision, Springer, pp 29–42\nXue T, Wu J, Bouman K, Freeman B (2016) Visual dynam-\nics: Probabilistic future frame synthesis via cross con-\nvolutional networks. In: Advances in Neural Information\nProcessing Systems, pp 91–99\nYang C, Zhang L, Lu H, Ruan X, Yang MH (2013) Saliency\ndetection via graph-based manifold ranking. In: Com-\nputer Vision and Pattern Recognition (CVPR), 2013 IEEE\nConference on, IEEE, pp 3166–3173\nYu F, Koltun V (2015) Multi-scale context aggregation by\ndilated convolutions. arXiv preprint arXiv:151107122\nZhang D, Han J, Zhang Y (2017) Supervision by fusion: To-\nwards unsupervised learning of deep salient object detec-\ntor. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp 4048–4056\nZhang J, Sclaroff S, Lin Z, Shen X, Price B, Mech R\n(2015) Minimum barrier salient object detection at 80 fps.\nIn: Proceedings of the IEEE International Conference on\nComputer Vision, pp 1404–1412\nZhu W, Liang S, Wei Y, Sun J (2014) Saliency optimization\nfrom robust background detection. In: Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pp 2814–2821\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-08-14",
  "updated": "2018-08-14"
}