{
  "id": "http://arxiv.org/abs/1808.06670v5",
  "title": "Learning deep representations by mutual information estimation and maximization",
  "authors": [
    "R Devon Hjelm",
    "Alex Fedorov",
    "Samuel Lavoie-Marchildon",
    "Karan Grewal",
    "Phil Bachman",
    "Adam Trischler",
    "Yoshua Bengio"
  ],
  "abstract": "In this work, we perform unsupervised learning of representations by\nmaximizing mutual information between an input and the output of a deep neural\nnetwork encoder. Importantly, we show that structure matters: incorporating\nknowledge about locality of the input to the objective can greatly influence a\nrepresentation's suitability for downstream tasks. We further control\ncharacteristics of the representation by matching to a prior distribution\nadversarially. Our method, which we call Deep InfoMax (DIM), outperforms a\nnumber of popular unsupervised learning methods and competes with\nfully-supervised learning on several classification tasks. DIM opens new\navenues for unsupervised learning of representations and is an important step\ntowards flexible formulations of representation-learning objectives for\nspecific end-goals.",
  "text": "Published as a conference paper at ICLR 2019\nLEARNING DEEP REPRESENTATIONS BY MUTUAL IN-\nFORMATION ESTIMATION AND MAXIMIZATION\nR Devon Hjelm\nMSR Montreal, MILA, UdeM, IVADO\ndevon.hjelm@microsoft.com\nAlex Fedorov\nMRN, UNM\nSamuel Lavoie-Marchildon\nMILA, UdeM\nKaran Grewal\nU Toronto\nPhil Bachman\nMSR Montreal\nAdam Trischler\nMSR Montreal\nYoshua Bengio\nMILA, UdeM, IVADO, CIFAR\nABSTRACT\nThis work investigates unsupervised learning of representations by maximizing\nmutual information between an input and the output of a deep neural network en-\ncoder. Importantly, we show that structure matters: incorporating knowledge about\nlocality in the input into the objective can signiﬁcantly improve a representation’s\nsuitability for downstream tasks. We further control characteristics of the repre-\nsentation by matching to a prior distribution adversarially. Our method, which we\ncall Deep InfoMax (DIM), outperforms a number of popular unsupervised learning\nmethods and compares favorably with fully-supervised learning on several clas-\nsiﬁcation tasks in with some standard architectures. DIM opens new avenues for\nunsupervised learning of representations and is an important step towards ﬂexible\nformulations of representation learning objectives for speciﬁc end-goals.\n1\nINTRODUCTION\nOne core objective of deep learning is to discover useful representations, and the simple idea explored\nhere is to train a representation-learning function, i.e. an encoder, to maximize the mutual information\n(MI) between its inputs and outputs. MI is notoriously difﬁcult to compute, particularly in continuous\nand high-dimensional settings. Fortunately, recent advances enable effective computation of MI\nbetween high dimensional input/output pairs of deep neural networks (Belghazi et al., 2018). We\nleverage MI estimation for representation learning and show that, depending on the downstream\ntask, maximizing MI between the complete input and the encoder output (i.e., global MI) is often\ninsufﬁcient for learning useful representations. Rather, structure matters: maximizing the average\nMI between the representation and local regions of the input (e.g. patches rather than the complete\nimage) can greatly improve the representation’s quality for, e.g., classiﬁcation tasks, while global MI\nplays a stronger role in the ability to reconstruct the full input given the representation.\nUsefulness of a representation is not just a matter of information content: representational char-\nacteristics like independence also play an important role (Gretton et al., 2012; Hyv¨arinen & Oja,\n2000; Hinton, 2002; Schmidhuber, 1992; Bengio et al., 2013; Thomas et al., 2017). We combine MI\nmaximization with prior matching in a manner similar to adversarial autoencoders (AAE, Makhzani\net al., 2015) to constrain representations according to desired statistical properties. This approach is\nclosely related to the infomax optimization principle (Linsker, 1988; Bell & Sejnowski, 1995), so we\ncall our method Deep InfoMax (DIM). Our main contributions are the following:\n• We formalize Deep InfoMax (DIM), which simultaneously estimates and maximizes the\nmutual information between input data and learned high-level representations.\n• Our mutual information maximization procedure can prioritize global or local information,\nwhich we show can be used to tune the suitability of learned representations for classiﬁcation\nor reconstruction-style tasks.\n• We use adversarial learning (`a la Makhzani et al., 2015) to constrain the representation to\nhave desired statistical characteristics speciﬁc to a prior.\n1\narXiv:1808.06670v5  [stat.ML]  22 Feb 2019\nPublished as a conference paper at ICLR 2019\n• We introduce two new measures of representation quality, one based on Mutual Information\nNeural Estimation (MINE, Belghazi et al., 2018) and a neural dependency measure (NDM)\nbased on the work by Brakel & Bengio (2017), and we use these to bolster our comparison\nof DIM to different unsupervised methods.\n2\nRELATED WORK\nThere are many popular methods for learning representations. Classic methods, such as independent\ncomponent analysis (ICA, Bell & Sejnowski, 1995) and self-organizing maps (Kohonen, 1998),\ngenerally lack the representational capacity of deep neural networks. More recent approaches\ninclude deep volume-preserving maps (Dinh et al., 2014; 2016), deep clustering (Xie et al., 2016;\nChang et al., 2017), noise as targets (NAT, Bojanowski & Joulin, 2017), and self-supervised or\nco-learning (Doersch & Zisserman, 2017; Dosovitskiy et al., 2016; Sajjadi et al., 2016).\nGenerative models are also commonly used for building representations (Vincent et al., 2010; Kingma\net al., 2014; Salimans et al., 2016; Rezende et al., 2016; Donahue et al., 2016), and mutual information\n(MI) plays an important role in the quality of the representations they learn. In generative models that\nrely on reconstruction (e.g., denoising, variational, and adversarial autoencoders, Vincent et al., 2008;\nRifai et al., 2012; Kingma & Welling, 2013; Makhzani et al., 2015), the reconstruction error can be\nrelated to the MI as follows:\nIe(X, Y ) = He(X) −He(X|Y ) ≥He(X) −Re,d(X|Y ),\n(1)\nwhere X and Y denote the input and output of an encoder which is applied to inputs sampled from\nsome source distribution. Re,d(X|Y ) denotes the expected reconstruction error of X given the codes\nY . He(X) and He(X|Y ) denote the marginal and conditional entropy of X in the distribution formed\nby applying the encoder to inputs sampled from the source distribution. Thus, in typical settings,\nmodels with reconstruction-type objectives provide some guarantees on the amount of information\nencoded in their intermediate representations. Similar guarantees exist for bi-directional adversarial\nmodels (Dumoulin et al., 2016; Donahue et al., 2016), which adversarially train an encoder / decoder\nto match their respective joint distributions or to minimize the reconstruction error (Chen et al., 2016).\nMutual-information estimation\nMethods based on mutual information have a long history in\nunsupervised feature learning. The infomax principle (Linsker, 1988; Bell & Sejnowski, 1995),\nas prescribed for neural networks, advocates maximizing MI between the input and output. This\nis the basis of numerous ICA algorithms, which can be nonlinear (Hyv¨arinen & Pajunen, 1999;\nAlmeida, 2003) but are often hard to adapt for use with deep networks. Mutual Information Neural\nEstimation (MINE, Belghazi et al., 2018) learns an estimate of the MI of continuous variables, is\nstrongly consistent, and can be used to learn better implicit bi-directional generative models. Deep\nInfoMax (DIM) follows MINE in this regard, though we ﬁnd that the generator is unnecessary.\nWe also ﬁnd it unnecessary to use the exact KL-based formulation of MI. For example, a simple\nalternative based on the Jensen-Shannon divergence (JSD) is more stable and provides better results.\nWe will show that DIM can work with various MI estimators. Most signiﬁcantly, DIM can leverage\nlocal structure in the input to improve the suitability of representations for classiﬁcation.\nLeveraging known structure in the input when designing objectives based on MI maximization is\nnothing new (Becker, 1992; 1996; Wiskott & Sejnowski, 2002), and some very recent works also\nfollow this intuition. It has been shown in the case of discrete MI that data augmentations and other\ntransformations can be used to avoid degenerate solutions (Hu et al., 2017). Unsupervised clustering\nand segmentation is attainable by maximizing the MI between images associated by transforms or\nspatial proximity (Ji et al., 2018). Our work investigates the suitability of representations learned\nacross two different MI objectives that focus on local or global structure, a ﬂexibility we believe is\nnecessary for training representations intended for different applications.\nProposed independently of DIM, Contrastive Predictive Coding (CPC, Oord et al., 2018) is a MI-\nbased approach that, like DIM, maximizes MI between global and local representation pairs. CPC\nshares some motivations and computations with DIM, but there are important ways in which CPC and\nDIM differ. CPC processes local features sequentially to build partial “summary features”, which are\nused to make predictions about speciﬁc local features in the “future” of each summary feature. This\nequates to ordered autoregression over the local features, and requires training separate estimators\n2\nPublished as a conference paper at ICLR 2019\nfor each temporal offset at which one would like to predict the future. In contrast, the basic version\nof DIM uses a single summary feature that is a function of all local features, and this “global” feature\npredicts all local features simultaneously in a single step using a single estimator. Note that, when\nusing occlusions during training (see Section 4.3 for details), DIM performs both “self” predictions\nand orderless autoregression.\n3\nDEEP INFOMAX\nFigure 1:\nThe base encoder model in the\ncontext of image data. An image (in this\ncase) is encoded using a convnet until reach-\ning a feature map of M × M feature vec-\ntors corresponding to M × M input patches.\nThese vectors are summarized into a single\nfeature vector, Y . Our goal is to train this net-\nwork such that useful information about the\ninput is easily extracted from the high-level\nfeatures.\nFigure 2:\nDeep InfoMax (DIM) with a\nglobal MI(X; Y ) objective. Here, we pass\nboth the high-level feature vector, Y , and the\nlower-level M×M feature map (see Figure 1)\nthrough a discriminator to get the score. Fake\nsamples are drawn by combining the same\nfeature vector with a M × M feature map\nfrom another image.\nHere we outline the general setting of training an encoder to maximize mutual information between\nits input and output. Let X and Y be the domain and range of a continuous and (almost everywhere)\ndifferentiable parametric function, Eψ : X →Y with parameters ψ (e.g., a neural network). These\nparameters deﬁne a family of encoders, EΦ = {Eψ}ψ∈Ψ over Ψ. Assume that we are given a set of\ntraining examples on an input space, X: X := {x(i) ∈X}N\ni=1, with empirical probability distribution\nP. We deﬁne Uψ,P to be the marginal distribution induced by pushing samples from P through Eψ.\nI.e., Uψ,P is the distribution over encodings y ∈Y produced by sampling observations x ∼X and\nthen sampling y ∼Eψ(x).\nAn example encoder for image data is given in Figure 1, which will be used in the following sections,\nbut this approach can easily be adapted for temporal data. Similar to the infomax optimization\nprinciple (Linsker, 1988), we assert our encoder should be trained according to the following criteria:\n• Mutual information maximization: Find the set of parameters, ψ, such that the mutual\ninformation, I(X; Eψ(X)), is maximized. Depending on the end-goal, this maximization\ncan be done over the complete input, X, or some structured or “local” subset.\n• Statistical constraints: Depending on the end-goal for the representation, the marginal\nUψ,P should match a prior distribution, V. Roughly speaking, this can be used to encourage\nthe output of the encoder to have desired characteristics (e.g., independence).\nThe formulation of these two objectives covered below we call Deep InfoMax (DIM).\n3.1\nMUTUAL INFORMATION ESTIMATION AND MAXIMIZATION\nOur basic mutual information maximization framework is presented in Figure 2. The approach\nfollows Mutual Information Neural Estimation (MINE, Belghazi et al., 2018), which estimates mutual\ninformation by training a classiﬁer to distinguish between samples coming from the joint, J, and the\n3\nPublished as a conference paper at ICLR 2019\nFigure 3: Maximizing mutual information\nbetween local features and global features.\nFirst we encode the image to a feature map\nthat reﬂects some structural aspect of the data,\ne.g. spatial locality, and we further summarize\nthis feature map into a global feature vector\n(see Figure 1). We then concatenate this fea-\nture vector with the lower-level feature map\nat every location. A score is produced for\neach local-global pair through an additional\nfunction (see the Appendix A.2 for details).\nproduct of marginals, M, of random variables X and Y . MINE uses a lower-bound to the MI based\non the Donsker-Varadhan representation (DV, Donsker & Varadhan, 1983) of the KL-divergence,\nI(X; Y ) := DKL(J||M) ≥bI(DV )\nω\n(X; Y ) := EJ[Tω(x, y)] −log EM[eTω(x,y)],\n(2)\nwhere Tω : X × Y →R is a discriminator function modeled by a neural network with parameters ω.\nAt a high level, we optimize Eψ by simultaneously estimating and maximizing I(X, Eψ(X)),\n(ˆω, ˆψ)G = arg max\nω,ψ\nbIω(X; Eψ(X)),\n(3)\nwhere the subscript G denotes “global” for reasons that will be clear later. However, there are some\nimportant differences that distinguish our approach from MINE. First, because the encoder and\nmutual information estimator are optimizing the same objective and require similar computations, we\nshare layers between these functions, so that Eψ = fψ ◦Cψ and Tψ,ω = Dω ◦g ◦(Cψ, Eψ),1 where\ng is a function that combines the encoder output with the lower layer.\nSecond, as we are primarily interested in maximizing MI, and not concerned with its precise value,\nwe can rely on non-KL divergences which may offer favourable trade-offs. For example, one could\ndeﬁne a Jensen-Shannon MI estimator (following the formulation of Nowozin et al., 2016),\nbI(JSD)\nω,ψ (X; Eψ(X)) := EP[−sp(−Tψ,ω(x, Eψ(x)))] −EP×˜P[sp(Tψ,ω(x′, Eψ(x)))],\n(4)\nwhere x is an input sample, x′ is an input sampled from ˜P = P, and sp(z) = log(1+ez) is the softplus\nfunction. A similar estimator appeared in Brakel & Bengio (2017) in the context of minimizing the\ntotal correlation, and it amounts to the familiar binary cross-entropy. This is well-understood in terms\nof neural network optimization and we ﬁnd works better in practice (e.g., is more stable) than the\nDV-based objective (e.g., see App. A.3). Intuitively, the Jensen-Shannon-based estimator should\nbehave similarly to the DV-based estimator in Eq. 2, since both act like classiﬁers whose objectives\nmaximize the expected log-ratio of the joint over the product of marginals. We show in App. A.1 the\nrelationship between the JSD estimator and the formal deﬁnition of mutual information.\nNoise-Contrastive Estimation (NCE, Gutmann & Hyv¨arinen, 2010; 2012) was ﬁrst used as a bound\non MI in Oord et al. (and called “infoNCE”, 2018), and this loss can also be used with DIM by\nmaximizing:\nbI(infoNCE)\nω,ψ\n(X; Eψ(X)) := EP\n\"\nTψ,ω(x, Eψ(x)) −E˜P\n\"\nlog\nX\nx′\neTψ,ω(x′,Eψ(x))\n##\n.\n(5)\nFor DIM, a key difference between the DV, JSD, and infoNCE formulations is whether an expectation\nover P/˜P appears inside or outside of a log. In fact, the JSD-based objective mirrors the original\nNCE formulation in Gutmann & Hyv¨arinen (2010), which phrased unnormalized density estimation\nas binary classiﬁcation between the data distribution and a noise distribution. DIM sets the noise\ndistribution to the product of marginals over X/Y , and the data distribution to the true joint. The\ninfoNCE formulation in Eq. 5 follows a softmax-based version of NCE (Jozefowicz et al., 2016),\nsimilar to ones used in the language modeling community (Mnih & Kavukcuoglu, 2013; Mikolov et al.,\n1Here we slightly abuse the notation and use ψ for both parts of Eψ.\n4\nPublished as a conference paper at ICLR 2019\n2013), and which has strong connections to the binary cross-entropy in the context of noise-contrastive\nlearning (Ma & Collins, 2018). In practice, implementations of these estimators appear quite similar\nand can reuse most of the same code. We investigate JSD and infoNCE in our experiments, and\nﬁnd that using infoNCE often outperforms JSD on downstream tasks, though this effect diminishes\nwith more challenging data. However, as we show in the App. (A.3), infoNCE and DV require a\nlarge number of negative samples (samples from ˜P) to be competitive. We generate negative samples\nusing all combinations of global and local features at all locations of the relevant feature map, across\nall images in a batch. For a batch of size B, that gives O(B × M 2) negative samples per positive\nexample, which quickly becomes cumbersome with increasing batch size. We found that DIM with\nthe JSD loss is insensitive to the number of negative samples, and in fact outperforms infoNCE as the\nnumber of negative samples becomes smaller.\n3.2\nLOCAL MUTUAL INFORMATION MAXIMIZATION\nThe objective in Eq. 3 can be used to maximize MI between input and output, but ultimately this\nmay be undesirable depending on the task. For example, trivial pixel-level noise is useless for image\nclassiﬁcation, so a representation may not beneﬁt from encoding this information (e.g., in zero-shot\nlearning, transfer learning, etc.). In order to obtain a representation more suitable for classiﬁcation,\nwe can instead maximize the average MI between the high-level representation and local patches of\nthe image. Because the same representation is encouraged to have high MI with all the patches, this\nfavours encoding aspects of the data that are shared across patches.\nSuppose the feature vector is of limited capacity (number of units and range) and assume the encoder\ndoes not support inﬁnite output conﬁgurations. For maximizing the MI between the whole input and\nthe representation, the encoder can pick and choose what type of information in the input is passed\nthrough the encoder, such as noise speciﬁc to local patches or pixels. However, if the encoder passes\ninformation speciﬁc to only some parts of the input, this does not increase the MI with any of the\nother patches that do not contain said noise. This encourages the encoder to prefer information that is\nshared across the input, and this hypothesis is supported in our experiments below.\nOur local DIM framework is presented in Figure 3. First we encode the input to a feature map,\nCψ(x) := {C(i)\nψ }M×M\ni=1\nthat reﬂects useful structure in the data (e.g., spatial locality), indexed in this\ncase by i. Next, we summarize this local feature map into a global feature, Eψ(x) = fψ ◦Cψ(x).\nWe then deﬁne our MI estimator on global/local pairs, maximizing the average estimated MI:\n(ˆω, ˆψ)L = arg max\nω,ψ\n1\nM 2\nM 2\nX\ni=1\nbIω,ψ(C(i)\nψ (X); Eψ(X)).\n(6)\nWe found success optimizing this “local” objective with multiple easy-to-implement architectures,\nand further implementation details are provided in the App. (A.2).\n3.3\nMATCHING REPRESENTATIONS TO A PRIOR DISTRIBUTION\nAbsolute magnitude of information is only one desirable property of a representation; depending on\nthe application, good representations can be compact (Gretton et al., 2012), independent (Hyv¨arinen\n& Oja, 2000; Hinton, 2002; Dinh et al., 2014; Brakel & Bengio, 2017), disentangled (Schmidhuber,\n1992; Rifai et al., 2012; Bengio et al., 2013; Chen et al., 2018; Gonzalez-Garcia et al., 2018), or\nindependently controllable (Thomas et al., 2017). DIM imposes statistical constraints onto learned\nrepresentations by implicitly training the encoder so that the push-forward distribution, Uψ,P, matches\na prior, V. This is done (see Figure 7 in the App. A.2) by training a discriminator, Dφ : Y →R, to\nestimate the divergence, D(V||Uψ,P), then training the encoder to minimize this estimate:\n(ˆω, ˆψ)P = arg min\nψ\narg max\nφ\nbDφ(V||Uψ,P) = EV[log Dφ(y)] + EP[log(1 −Dφ(Eψ(x)))].\n(7)\nThis approach is similar to what is done in adversarial autoencoders (AAE, Makhzani et al., 2015),\nbut without a generator. It is also similar to noise as targets (Bojanowski & Joulin, 2017), but trains\nthe encoder to match the noise implicitly rather than using a priori noise samples as targets.\n5\nPublished as a conference paper at ICLR 2019\nAll three objectives – global and local MI maximization and prior matching – can be used together,\nand doing so we arrive at our complete objective for Deep InfoMax (DIM):\narg max\nω1,ω2,ψ\n\u0000αbIω1,ψ(X; Eψ(X)) + β\nM 2\nM 2\nX\ni=1\nbIω2,ψ(X(i); Eψ(X))\n\u0001\n+ arg min\nψ\narg max\nφ\nγ bDφ(V||Uψ,P),\n(8)\nwhere ω1 and ω2 are the discriminator parameters for the global and local objectives, respectively,\nand α, β, and γ are hyperparameters. We will show below that choices in these hyperparameters\naffect the learned representations in meaningful ways. As an interesting aside, we also show in the\nApp. (A.8) that this prior matching can be used alone to train a generator of image data.\n4\nEXPERIMENTS\nWe test Deep InfoMax (DIM) on four imaging datasets to evaluate its representational properties:\n• CIFAR10 and CIFAR100 (Krizhevsky & Hinton, 2009): two small-scale labeled datasets\ncomposed of 32 × 32 images with 10 and 100 classes respectively.\n• Tiny ImageNet: A reduced version of ImageNet (Krizhevsky & Hinton, 2009) images scaled\ndown to 64 × 64 with a total of 200 classes.\n• STL-10 (Coates et al., 2011): a dataset derived from ImageNet composed of 96 × 96 images\nwith a mixture of 100000 unlabeled training examples and 500 labeled examples per class.\nWe use data augmentation with this dataset, taking random 64 × 64 crops and ﬂipping\nhorizontally during unsupervised learning.\n• CelebA (Yang et al., 2015, Appendix A.5 only): An image dataset composed of faces labeled\nwith 40 binary attributes. This dataset evaluates DIM’s ability to capture information that is\nmore ﬁne-grained than the class label and coarser than individual pixels.\nFor our experiments, we compare DIM against various unsupervised methods: Variational AutoEn-\ncoders (VAE, Kingma & Welling, 2013), β-VAE (Higgins et al., 2016; Alemi et al., 2016), Adversarial\nAutoEncoders (AAE, Makhzani et al., 2015), BiGAN (a.k.a. adversarially learned inference with\na deterministic encoder: Donahue et al., 2016; Dumoulin et al., 2016), Noise As Targets (NAT,\nBojanowski & Joulin, 2017), and Contrastive Predictive Coding (CPC, Oord et al., 2018). Note\nthat we take CPC to mean ordered autoregression using summary features to predict “future” local\nfeatures, independent of the constrastive loss used to evaluate the predictions (JSD, infoNCE, or DV).\nSee the App. (A.2) for details of the neural net architectures used in the experiments.\n4.1\nHOW DO WE EVALUATE THE QUALITY OF A REPRESENTATION?\nEvaluation of representations is case-driven and relies on various proxies. Linear separability is\ncommonly used as a proxy for disentanglement and mutual information (MI) between representations\nand class labels. Unfortunately, this will not show whether the representation has high MI with\nthe class labels when the representation is not disentangled. Other works (Bojanowski & Joulin,\n2017) have looked at transfer learning classiﬁcation tasks by freezing the weights of the encoder and\ntraining a small fully-connected neural network classiﬁer using the representation as input. Others\nstill have more directly measured the MI between the labels and the representation (Rifai et al., 2012;\nChen et al., 2018), which can also reveal the representation’s degree of entanglement.\nClass labels have limited use in evaluating representations, as we are often interested in information\nencoded in the representation that is unknown to us. However, we can use mutual information neural\nestimation (MINE, Belghazi et al., 2018) to more directly measure the MI between the input and\noutput of the encoder.\nNext, we can directly measure the independence of the representation using a discriminator. Given a\nbatch of representations, we generate a factor-wise independent distribution with the same per-factor\nmarginals by randomly shufﬂing each factor along the batch dimension. A similar trick has been used\nfor learning maximally independent representations for sequential data (Brakel & Bengio, 2017). We\ncan train a discriminator to estimate the KL-divergence between the original representations (joint\n6\nPublished as a conference paper at ICLR 2019\ndistribution of the factors) and the shufﬂed representations (product of the marginals, see Figure 12).\nThe higher the KL divergence, the more dependent the factors. We call this evaluation method Neural\nDependency Measure (NDM) and show that it is sensible and empirically consistent in the App.\n(A.6).\nTo summarize, we use the following metrics for evaluating representations. For each of these, the\nencoder is held ﬁxed unless noted otherwise:\n• Linear classiﬁcation using a support vector machine (SVM). This is simultaneously a\nproxy for MI of the representation with linear separability.\n• Non-linear classiﬁcation using a single hidden layer neural network (200 units) with\ndropout. This is a proxy on MI of the representation with the labels separate from linear\nseparability as measured with the SVM above.\n• Semi-supervised learning (STL-10 here), that is, ﬁne-tuning the complete encoder by\nadding a small neural network on top of the last convolutional layer (matching architectures\nwith a standard fully-supervised classiﬁer).\n• MS-SSIM (Wang et al., 2003), using a decoder trained on the L2 reconstruction loss. This\nis a proxy for the total MI between the input and the representation and can indicate the\namount of encoded pixel-level information.\n• Mutual information neural estimate (MINE), bIρ(X, Eψ(x)), between the input, X, and\nthe output representation, Eψ(x), by training a discriminator with parameters ρ to maximize\nthe DV estimator of the KL-divergence.\n• Neural dependency measure (NDM) using a second discriminator that measures the KL\nbetween Eψ(x) and a batch-wise shufﬂed version of Eψ(x).\nFor the neural network classiﬁcation evaluation above, we performed experiments on all datasets\nexcept CelebA, while for other measures we only looked at CIFAR10. For all classiﬁcation tasks,\nwe built separate classiﬁers on the high-level vector representation (Y ), the output of the previous\nfully-connected layer (fc) and the last convolutional layer (conv). Model selection for the classiﬁers\nwas done by averaging the last 100 epochs of optimization, and the dropout rate and decaying learning\nrate schedule was set uniformly to alleviate over-ﬁtting on the test set across all models.\n4.2\nREPRESENTATION LEARNING COMPARISON ACROSS MODELS\nIn the following experiments, DIM(G) refers to DIM with a global-only objective (α = 1, β = 0, γ =\n1) and DIM(L) refers to DIM with a local-only objective (α = 0, β = 1, γ = 0.1), the latter chosen\nfrom the results of an ablation study presented in the App. (A.5). For the prior, we chose a compact\nuniform distribution on [0, 1]64, which worked better in practice than other priors, such as Gaussian,\nunit ball, or unit sphere.\nClassiﬁcation comparisons\nOur classiﬁcation results can be found in Tables 1, 2, and 3. In general,\nDIM with the local objective, DIM(L), outperformed all models presented here by a signiﬁcant margin\non all datasets, regardless of which layer the representation was drawn from, with exception to CPC.\nFor the speciﬁc settings presented (architectures, no data augmentation for datasets except for STL-\n10), DIM(L) performs as well as or outperforms a fully-supervised classiﬁer without ﬁne-tuning,\nwhich indicates that the representations are nearly as good as or better than the raw pixels given the\nmodel constraints in this setting. Note, however, that a fully supervised classiﬁer can perform much\nbetter on all of these benchmarks, especially when specialized architectures and carefully-chosen\ndata augmentations are used. Competitive or better results on CIFAR10 also exist (albeit in different\nsettings, e.g., Coates et al., 2011; Dosovitskiy et al., 2016), but to our knowledge our STL-10 results\nare state-of-the-art for unsupervised learning. The results in this setting support the hypothesis that\nour local DIM objective is suitable for extracting class information.\nOur results show that infoNCE tends to perform best, but differences between infoNCE and JSD\ndiminish with larger datasets. DV can compete with JSD with smaller datasets, but DV performs\nmuch worse with larger datasets.\nFor CPC, we were only able to achieve marginally better performance than BiGAN with the settings\nabove. However, when we adopted the strided crop architecture found in Oord et al. (2018), both\n7\nPublished as a conference paper at ICLR 2019\nTable 1: Classiﬁcation accuracy (top 1) results on CIFAR10 and CIFAR100. DIM(L) (i.e., with the\nlocal-only objective) outperforms all other unsupervised methods presented by a wide margin. In\naddition, DIM(L) approaches or even surpasses a fully-supervised classiﬁer with similar architecture.\nDIM with the global-only objective is competitive with some models across tasks, but falls short\nwhen compared to generative models and DIM(L) on CIFAR100. Fully-supervised classiﬁcation\nresults are provided for comparison.\nModel\nCIFAR10\nCIFAR100\nconv\nfc (1024)\nY (64)\nconv\nfc (1024)\nY (64)\nFully supervised\n75.39\n42.27\nVAE\n60.71\n60.54\n54.61\n37.21\n34.05\n24.22\nAE\n62.19\n55.78\n54.47\n31.50\n23.89\n27.44\nβ-VAE\n62.4\n57.89\n55.43\n32.28\n26.89\n28.96\nAAE\n59.44\n57.19\n52.81\n36.22\n33.38\n23.25\nBiGAN\n62.57\n62.74\n52.54\n37.59\n33.34\n21.49\nNAT\n56.19\n51.29\n31.16\n29.18\n24.57\n9.72\nDIM(G)\n52.2\n52.84\n43.17\n27.68\n24.35\n19.98\nDIM(L) (DV)\n72.66\n70.60\n64.71\n48.52\n44.44\n39.27\nDIM(L) (JSD)\n73.25\n73.62\n66.96\n48.13\n45.92\n39.60\nDIM(L) (infoNCE)\n75.21\n75.57\n69.13\n49.74\n47.72\n41.61\nTable 2: Classiﬁcation accuracy (top 1) results on Tiny ImageNet and STL-10. For Tiny ImageNet,\nDIM with the local objective outperforms all other models presented by a large margin, and approaches\naccuracy of a fully-supervised classiﬁer similar to the Alexnet architecture used here.\nTiny ImageNet\nSTL-10 (random crop pretraining)\nconv\nfc (4096)\nY (64)\nconv\nfc (4096)\nY (64)\nSS\nFully supervised\n36.60\n68.7\nVAE\n18.63\n16.88\n11.93\n58.27\n56.72\n46.47\n68.65\nAE\n19.07\n16.39\n11.82\n58.19\n55.57\n46.82\n70.29\nβ-VAE\n19.29\n16.77\n12.43\n57.15\n55.14\n46.87\n70.53\nAAE\n18.04\n17.27\n11.49\n59.54\n54.47\n43.89\n64.15\nBiGAN\n24.38\n20.21\n13.06\n71.53\n67.18\n58.48\n74.77\nNAT\n13.70\n11.62\n1.20\n64.32\n61.43\n48.84\n70.75\nDIM(G)\n11.32\n6.34\n4.95\n42.03\n30.82\n28.09\n51.36\nDIM(L) (DV)\n30.35\n29.51\n28.18\n69.15\n63.81\n61.92\n71.22\nDIM(L) (JSD)\n33.54\n36.88\n31.66\n72.86\n70.85\n65.93\n76.96\nDIM(L) (infoNCE)\n34.21\n38.09\n33.33\n72.57\n70.00\n67.08\n76.81\nCPC and DIM performance improved considerably. We chose a crop size of 25% of the image size\nin width and depth with a stride of 12.5% the image size (e.g., 8 × 8 crops with 4 × 4 strides for\nCIFAR10, 16 × 16 crops with 8 × 8 strides for STL-10), so that there were a total of 7 × 7 local\nfeatures. For both DIM(L) and CPC, we used infoNCE as well as the same “encode-and-dot-product”\narchitecture (tantamount to a deep bilinear model), rather than the shallow bilinear model used in\nOord et al. (2018). For CPC, we used a total of 3 such networks, where each network for CPC is\nused for a separate prediction task of local feature maps in the next 3 rows of a summary predictor\nfeature within each column.2 For simplicity, we omitted the prior term, β, from DIM. Without data\naugmentation on CIFAR10, CPC performs worse than DIM(L) with a ResNet-50 (He et al., 2016)\ntype architecture. For experiments we ran on STL-10 with data augmentation (using the same encoder\narchitecture as Table 2), CPC and DIM were competitive, with CPC performing slightly better.\nCPC makes predictions based on multiple summary features, each of which contains different amounts\nof information about the full input. We can add similar behavior to DIM by computing less global\nfeatures which condition on 3 × 3 blocks of local features sampled at random from the full 7 × 7\nsets of local features. We then maximize mutual information between these less global features and\nthe full sets of local features. We share a single MI estimator across all possible 3 × 3 blocks of\nlocal features when using this version of DIM. This represents a particular instance of the occlusion\ntechnique described in Section 4.3. The resulting model gave a signiﬁcant performance boost to\n2Note that this is slightly different from the setup used in Oord et al. (2018), which used a total of 5 such\npredictors, though we found other conﬁgurations performed similarly.\n8\nPublished as a conference paper at ICLR 2019\nTable 3: Comparisons of DIM with Contrastive Predictive Coding (CPC, Oord et al., 2018). These\nexperiments used a strided-crop architecture similar to the one used in Oord et al. (2018). For\nCIFAR10 we used a ResNet-50 encoder, and for STL-10 we used the same architecture as for Table 2.\nWe also tested a version of DIM that computes the global representation from a 3x3 block of local\nfeatures randomly selected from the full 7x7 set of local features. This is a particular instance of the\nocclusions described in Section 4.3. DIM(L) is competitive with CPC in these settings.\nModel\nCIFAR10 (no data augmentation)\nSTL10 (random crop pretraining)\nDIM(L) single global\n80.95\n76.97\nCPC\n77.45\n77.81\nDIM(L) multiple globals\n77.51\n78.21\nTable 4: Extended comparisons on CIFAR10. Linear classiﬁcation results using SVM are over ﬁve\nruns. MS-SSIM is estimated by training a separate decoder using the ﬁxed representation as input and\nminimizing the L2 loss with the original input. Mutual information estimates were done using MINE\nand the neural dependence measure (NDM) were trained using a discriminator between unshufﬂed\nand shufﬂed representations.\nModel\nProxies\nNeural Estimators\nSVM (conv)\nSVM (fc)\nSVM (Y )\nMS-SSIM\nbIρ(X, Y )\nNDM\nVAE\n53.83 ± 0.62\n42.14 ± 3.69\n39.59 ± 0.01\n0.72\n93.02\n1.62\nAAE\n55.22 ± 0.06\n43.34 ± 1.10\n37.76 ± 0.18\n0.67\n87.48\n0.03\nBiGAN\n56.40 ± 1.12\n38.42 ± 6.86\n44.90 ± 0.13\n0.46\n37.69\n24.49\nNAT\n48.62 ± 0.02\n42.63 ± 3.69\n39.59 ± 0.01\n0.29\n6.04\n0.02\nDIM(G)\n46.8 ± 2.29\n28.79 ± 7.29\n29.08 ± 0.24\n0.49\n49.63\n9.96\nDIM(L+G)\n57.55 ± 1.442\n45.56 ± 4.18\n18.63 ± 4.79\n0.53\n101.65\n22.89\nDIM(L)\n63.25 ± 0.86\n54.06 ± 3.6\n49.62 ± 0.3\n0.37\n45.09\n9.18\nDIM for STL-10. Surprisingly, this same architecture performed worse than using the fully global\nrepresentation with CIFAR10. Overall DIM only slightly outperforms CPC in this setting, which\nsuggests that the strictly ordered autoregression of CPC may be unnecessary for some tasks.\nExtended comparisons\nTables 4 shows results on linear separability, reconstruction (MS-SSIM),\nmutual information, and dependence (NDM) with the CIFAR10 dataset. We did not compare to CPC\ndue to the divergence of architectures. For linear classiﬁer results (SVC), we trained ﬁve support\nvector machines with a simple hinge loss for each model, averaging the test accuracy. For MINE,\nwe used a decaying learning rate schedule, which helped reduce variance in estimates and provided\nfaster convergence.\nMS-SSIM correlated well with the MI estimate provided by MINE, indicating that these models\nencoded pixel-wise information well. Overall, all models showed much lower dependence than\nBiGAN, indicating the marginal of the encoder output is not matching to the generator’s spherical\nGaussian input prior, though the mixed local/global version of DIM is close. For MI, reconstruction-\nbased models like VAE and AAE have high scores, and we found that combining local and global\nDIM objectives had very high scores (α = 0.5, β = 0.1 is presented here as DIM(L+G)). For more\nin-depth analyses, please see the ablation studies and the nearest-neighbor analysis in the App. (A.4,\nA.5).\n4.3\nADDING COORDINATE INFORMATION AND OCCLUSIONS\nMaximizing MI between global and local features is not the only way to leverage image structure.\nWe consider augmenting DIM by adding input occlusion when computing global features and by\nadding auxiliary tasks which maximize MI between local features and absolute or relative spatial\ncoordinates given a global feature. These additions improve classiﬁcation results (see Table 5).\nFor occlusion, we randomly occlude part of the input when computing the global features, but\ncompute local features using the full input. Maximizing MI between occluded global features and\nunoccluded local features aggressively encourages the global features to encode information which\nis shared across the entire image. For coordinate prediction, we maximize the model’s ability to\npredict the coordinates (i, j) of a local feature c(i,j) = C(i,j)\nψ\n(x) after computing the global features\n9\nPublished as a conference paper at ICLR 2019\nTable 5: Augmenting infoNCE DIM with additional structural information – adding coordinate\nprediction tasks or occluding input patches when computing the global feature vector in DIM can\nimprove the classiﬁcation accuracy, particularly with the highly-compressed global features.\nModel\nCIFAR10\nCIFAR100\nY (64)\nfc (1024)\nconv\nY (64)\nfc (1024)\nconv\nDIM\n70.65\n73.33\n77.46\n44.27\n47.96\n49.90\nDIM (coord)\n71.56\n73.89\n77.28\n45.37\n48.61\n50.27\nDIM (occlude)\n72.87\n74.45\n76.77\n44.89\n47.65\n48.87\nDIM (coord + occlude)\n73.99\n75.15\n77.27\n45.96\n48.00\n48.72\ny = Eψ(x). To accomplish this, we maximize E[log pθ((i, j)|y, c(i,j))] (i.e., minimize the cross-\nentropy). We can extend the task to maximize conditional MI given global features y between pairs\nof local features (c(i,j), c(i′,j′)) and their relative coordinates (i −i′, j −j′). This objective can be\nwritten as E[log pθ((i −i′, j −j′)|y, c(i,j), c(i′,j′))]. We use both these objectives in our results.\nAdditional implementation details can be found in the App. (A.7). Roughly speaking, our input\nocclusions and coordinate prediction tasks can be interpreted as generalizations of inpainting (Pathak\net al., 2016) and context prediction (Doersch et al., 2015) tasks which have previously been proposed\nfor self-supervised feature learning. Augmenting DIM with these tasks helps move our method\nfurther towards learning representations which encode images (or other types of inputs) not just in\nterms of compressing their low-level (e.g. pixel) content, but in terms of distributions over relations\namong higher-level features extracted from their lower-level content.\n5\nCONCLUSION\nIn this work, we introduced Deep InfoMax (DIM), a new method for learning unsupervised represen-\ntations by maximizing mutual information, allowing for representations that contain locally-consistent\ninformation across structural “locations” (e.g., patches in an image). This provides a straightforward\nand ﬂexible way to learn representations that perform well on a variety of tasks. We believe that this\nis an important direction in learning higher-level representations.\n6\nACKNOWLEDGEMENTS\nRDH received partial support from IVADO, NIH grants 2R01EB005846, P20GM103472,\nP30GM122734, and R01EB020407, and NSF grant 1539067. AF received partial support from NIH\ngrants R01EB020407, R01EB006841, P20GM103472, P30GM122734. We would also like to thank\nGeoff Gordon (MSR), Ishmael Belghazi (MILA), Marc Bellemare (Google Brain), Mikoaj Bi´nkowski\n(Imperial College London), Simon Sebbagh, and Aaron Courville (MILA) for their useful input at\nvarious points through the course of this research.\nREFERENCES\nAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information\nbottleneck. arXiv preprint arXiv:1612.00410, 2016.\nLu´ıs B Almeida. Linear and nonlinear ica based on mutual information. The Journal of Machine\nLearning Research, 4:1297–1318, 2003.\nMartin Arjovsky and L´eon Bottou. Towards principled methods for training generative adversarial\nnetworks. In International Conference on Learning Representations, 2017.\nSuzanna Becker. An information-theoretic unsupervised learning algorithm for neural networks.\nUniversity of Toronto, 1992.\nSuzanna Becker. Mutual information maximization: models of cortical self-organization. Network:\nComputation in neural systems, 7(1):7–31, 1996.\n10\nPublished as a conference paper at ICLR 2019\nIshmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and\nR Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062,\nICML’2018, 2018.\nAnthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separation\nand blind deconvolution. Neural computation, 7(6):1129–1159, 1995.\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI), 35(8):1798–1828,\n2013.\nPiotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. arXiv preprint\narXiv:1704.05310, 2017.\nPhilemon Brakel and Yoshua Bengio. Learning independent features with adversarial nets for\nnon-linear ica. arXiv preprint arXiv:1710.05050, 2017.\nJianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Deep adaptive\nimage clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 5879–5887, 2017.\nTian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement\nin variational autoencoders. arXiv preprint arXiv:1802.04942, 2018.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:\nInterpretable representation learning by information maximizing generative adversarial nets. In\nAdvances in neural information processing systems, pp. 2172–2180, 2016.\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised\nfeature learning. In Proceedings of the fourteenth international conference on artiﬁcial intelligence\nand statistics, pp. 215–223, 2011.\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516, 2014.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv\npreprint arXiv:1605.08803, 2016.\nCarl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In The IEEE\nInternational Conference on Computer Vision (ICCV), 2017.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE International Conference on Computer Vision,\n2015.\nJeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\nM.D Donsker and S.R.S Varadhan. Asymptotic evaluation of certain markov process expectations for\nlarge time, iv. Communications on Pure and Applied Mathematics, 36(2):183–212, 1983.\nAlexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox.\nDiscriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE\ntransactions on pattern analysis and machine intelligence, 38(9):1734–1747, 2016.\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro,\nand Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\nAbel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Bengio. Image-to-image translation for\ncross-domain disentanglement. arXiv preprint arXiv:1805.09730, 2018.\nArthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola. A\nkernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012.\n11\nPublished as a conference paper at ICLR 2019\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved\ntraining of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.\nMichael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings of the Thirteenth International Conference on\nArtiﬁcial Intelligence and Statistics, pp. 297–304, 2010.\nMichael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical\nmodels, with applications to natural image statistics. Journal of Machine Learning Research, 13\n(Feb):307–361, 2012.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 770–778, 2016.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. Openreview, 2016.\nGeoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural\ncomputation, 14(8):1771–1800, 2002.\nR Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, and Yoshua Bengio.\nBoundary-seeking generative adversarial networks. In International Conference on Learning\nRepresentations, 2018.\nWeihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning\ndiscrete representations via information maximizing self-augmented training. arXiv preprint\narXiv:1702.08720, 2017.\nAapo Hyv¨arinen and Erkki Oja. Independent component analysis: algorithms and applications.\nNeural networks, 13(4):411–430, 2000.\nAapo Hyv¨arinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and\nuniqueness results. Neural Networks, 12(3):429–439, 1999.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nXu Ji, Jo˜ao F Henriques, and Andrea Vedaldi. Invariant information distillation for unsupervised\nimage segmentation and clustering. arXiv preprint arXiv:1807.06653, 2018.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\nDiederik Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nDiederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised\nlearning with deep generative models. In Advances in Neural Information Processing Systems, pp.\n3581–3589, 2014.\nTeuvo Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1–6, 1998.\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.\nTechnical report, Citeseer, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Advances in neural information processing systems, pp. 1097–1105,\n2012.\nRalph Linsker. Self-organization in a perceptual network. IEEE Computer, 21(3):105–117, 1988.\ndoi: 10.1109/2.36. URL https://doi.org/10.1109/2.36.\n12\nPublished as a conference paper at ICLR 2019\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), December 2015.\nZhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional\nmodels: Consistency and statistical efﬁciency. arXiv preprint arXiv:1809.01812, 2018.\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial\nautoencoders. arXiv preprint arXiv:1511.05644, 2015.\nLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do\nactually converge? In International Conference on Machine Learning, pp. 3478–3487, 2018.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations\nof words and phrases and their compositionality. In Advances in neural information processing\nsystems, pp. 3111–3119, 2013.\nAndriy Mnih and Koray Kavukcuoglu. Learning word embeddings efﬁciently with noise-contrastive\nestimation. In Advances in neural information processing systems, pp. 2265–2273, 2013.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers\nusing variational divergence minimization. In Advances in Neural Information Processing Systems,\npp. 271–279, 2016.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759, 2016.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\nDeepak Pathak, Philipp Kr¨ahenb¨uhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context\nencoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2016.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nDanilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra. One-\nshot generalization in deep generative models. arXiv preprint arXiv:1603.05106, 2016.\nSalah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-\nencoders: Explicit invariance during feature extraction. In Proceedings of the 28th International\nConference on International Conference on Machine Learning, pp. 833–840. Omnipress, 2011.\nSalah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza. Disentangling\nfactors of variation for facial expression recognition. In European Conference on Computer Vision,\npp. 808–822. Springer, 2012.\nMehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transforma-\ntions and perturbations for deep semi-supervised learning. In Advances in Neural Information\nProcessing Systems, pp. 1163–1171, 2016.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. In Advances in Neural Information Processing Systems, pp.\n2234–2242, 2016.\nJ¨urgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,\n4(6):863–879, 1992.\nValentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean\nMeurs, Joelle Pineau, Doina Precup, and Yoshua Bengio. Independently controllable features.\narXiv preprint arXiv:1708.01289, 2017.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\ncomposing robust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning, pp. 1096–1103. ACM, 2008.\n13\nPublished as a conference paper at ICLR 2019\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. Journal of machine learning research, 11(Dec):3371–3408, 2010.\nZhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality\nassessment. In Signals, Systems and Computers, 2004. Conference Record of the Thirty-Seventh\nAsilomar Conference on, volume 2, pp. 1398–1402. Ieee, 2003.\nLaurenz Wiskott and Terrence J Sejnowski.\nSlow feature analysis: Unsupervised learning of\ninvariances. Neural computation, 14(4):715–770, 2002.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.\nIn International conference on machine learning, pp. 478–487, 2016.\nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. From facial parts responses to face\ndetection: A deep learning approach. In Proceedings of the IEEE International Conference on\nComputer Vision, pp. 3676–3684, 2015.\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-\nscale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,\n2015.\n14\nPublished as a conference paper at ICLR 2019\nA\nAPPENDIX\nA.1\nON THE JENSEN-SHANNON DIVERGENCE AND MUTUAL INFORMATION\nHere we show the relationship between the Jensen-Shannon divergence (JSD) between the joint\nand the product of marginals and the pointwise mutual information (PMI). Let p(x) and p(y) be\ntwo marginal densities, and deﬁne p(y|x) and p(x, y) = p(y|x)p(x) as the conditional and joint\ndistribution, respectively. Construct a probability mixture density, m(x, y) = 1\n2(p(x)p(y) + p(x, y)).\nIt follows that m(x) = p(x), m(y) = p(y), and m(y|x) = 1\n2(p(y) + p(y|x)).\nNote that:\nlog m(y|x) = log\n\u0000 1\n2(p(y) + p(y|x))\n\u0001\n= log 1\n2 + log p(y) + log\n\u0010\n1 + p(y|x)\np(y)\n\u0011\n.\n(9)\nDiscarding some constants:\nJSD(p(x, y)||p(x)p(y))\n∝Ex∼p(x)\nh\nEy∼p(y|x)\nh\nlog\np(y|x)p(x)\nm(y|x)m(x)\ni\n+ Ey∼p(y)\nh\nlog\np(y)p(x)\nm(y|x)m(x)\nii\n∝Ex∼p(x)\nh\nEy∼p(y|x)\nh\nlog p(y|x)\np(y) −log\n\u0010\n1 + p(y|x)\np(y)\n\u0011i\n+ Ey∼p(y)\nh\n−log\n\u0010\n1 + p(y|x)\np(y)\n\u0011ii\n∝Ex∼p(x)\nh\nEy∼p(y|x)\nh\nlog p(y|x)\np(y)\ni\n−2Ey∼m(y|x)\nh\nlog\n\u0010\n1 + p(y|x)\np(y)\n\u0011ii\n∝Ex∼p(x)\nh\nEy∼p(y|x)\nh\nlog p(y|x)\np(y) −2 m(y|x)\np(y|x) log\n\u0010\n1 + p(y|x)\np(y)\n\u0011ii\n∝Ex∼p(x)\nh\nEy∼p(y|x)\nh\nlog p(y|x)\np(y) −(1 +\np(y)\np(y|x)) log\n\u0010\n1 + p(y|x)\np(y)\n\u0011ii\n.\n(10)\nThe quantity inside the expectation of Eqn. 10 is a concave, monotonically increasing function of the\nratio p(y|x)\np(y) , which is exactly ePMI(x,y). Note this relationship does not hold for the JSD of arbitrary\ndistributions, as the the joint and product of marginals are intimately coupled.\nWe can verify our theoretical observation by plotting the JSD and KL divergences between the joint\nand the product of marginals, the latter of which is the formal deﬁnition of mutual information (MI).\nAs computing the continuous MI is difﬁcult, we assume a discrete input with uniform probability,\np(x) (e.g., these could be one-hot variables indicating one of N i.i.d. random samples), and a\nrandomly initialized N × M joint distribution, p(x, y), such that PM\nj=1 p(xi, yj) = 1 ∀i. For this\njoint distribution, we sample from a uniform distribution, then apply dropout to encourage sparsity to\nsimulate the situation when there is no bijective function between x and y, then apply a softmax. As\nthe distributions are discrete, we can compute the KL and JSD between p(x, y) and p(x)p(y).\nWe ran these experiments with matched input / output dimensions of 8, 16, 32, 64, and 128, randomly\ndrawing 1000 joint distributions, and computed the KL and JSD divergences directly. Our results\n(Figure A.1) indicate that the KL (traditional deﬁnition of mutual information) and the JSD have an\napproximately monotonic relationship. Overall, the distributions with the highest mutual information\nalso have the highest JSD.\nA.2\nEXPERIMENT AND ARCHITECTURE DETAILS\nHere we provide architectural details for our experiments. Example code for running Deep Infomax\n(DIM) can be found at https://github.com/rdevon/DIM.\nEncoder\nWe used an encoder similar to a deep convolutional GAN (DCGAN, Radford et al., 2015)\ndiscriminator for CIFAR10 and CIFAR100, and for all other datasets we used an Alexnet (Krizhevsky\net al., 2012) architecture similar to that found in Donahue et al. (2016). ReLU activations and batch\nnorm (Ioffe & Szegedy, 2015) were used on every hidden layer. For the DCGAN architecture, a\nsingle hidden layer with 1024 units was used after the ﬁnal convolutional layer, and for the Alexnet\narchitecture it was two hidden layers with 4096. For all experiments, the output of all encoders was a\n64 dimensional vector.\n15\nPublished as a conference paper at ICLR 2019\nFigure 4: Scatter plots of MI(x; y) versus\nJSD(p(x, y)||p(x)p(y))\nwith discrete inputs and a given ran-\ndomized\nand\nsparse\njoint\ndistribution,\np(x, y).\n8 × 8 indicates a square joint\ndistribution with 8 rows and 8 columns.\nOur experiments indicate a strong mono-\ntonic relationship between MI(x; y) and\nJSD(p(x, y)||p(x)p(y)) Overall, the distri-\nbutions with the highest MI(x; y) have the\nhighest JSD(p(x, y)||p(x)p(y)).\nMutual information discriminators\nFor the global mutual information objective, we ﬁrst encode\nthe input into a feature map, Cψ(x), which in this case is the output of the last convolutional layer.\nWe then encode this representation further using linear layers as detailed above to get Eψ(x). Cψ(x)\nis then ﬂattened, then concatenated with Eψ(x). We then pass this to a fully-connected network with\ntwo 512-unit hidden layers (see Table 6).\nTable 6: Global DIM network architecture\nOperation\nSize\nActivation\nInput →Linear layer\n512\nReLU\nLinear layer\n512\nReLU\nLinear layer\n1\nWe tested two different architectures for the local objective. The ﬁrst (Figure 5) concatenated the\nglobal feature vector with the feature map at every location, i.e., {[C(i)\nψ (x), Eψ(x)]}M×M\ni=1\n. A 1 × 1\nconvolutional discriminator is then used to score the (feature map, feature vector) pair,\nT (i)\nψ,ω(x, y(x)) = Dω([C(i)\nψ (x), Eψ(x)]).\n(11)\nFake samples are generated by combining global feature vectors with local feature maps coming from\ndifferent images, x′:\nT (i)\nψ,ω(x′, Eψ(x)) = Dω([C(i)\nψ (x′), Eψ(x)]).\n(12)\nThis architecture is featured in the results of Table 4, as well as the ablation and nearest-neighbor\nstudies below. We used a 1 × 1 convnet with two 512-unit hidden layers as discriminator (Table 7).\nTable 7: Local DIM concat-and-convolve network architecture\nOperation\nSize\nActivation\nInput →1 × 1 conv\n512\nReLU\n1 × 1 conv\n512\nReLU\n1 × 1 conv\n1\nThe other architecture we tested (Figure 6) is based on non-linearly embedding the global and\nlocal features in a (much) higher-dimensional space, and then computing pair-wise scores using\ndot products between their high-dimensional embeddings. This enables efﬁcient evaluation of a\nlarge number of pair-wise scores, thus allowing us to use large numbers of positive/negative samples.\nGiven a sufﬁciently high-dimensional embedding space, this approach can represent (almost) arbitrary\nclasses of pair-wise functions that are non-linear in the original, lower-dimensional features. For\nmore information, refer to Reproducing Kernel Hilbert Spaces. We pass the global feature through a\n16\nPublished as a conference paper at ICLR 2019\nFigure 5:\nConcat-and-convolve architec-\nture. The global feature vector is concate-\nnated with the lower-level feature map at ev-\nery location. A 1 × 1 convolutional discrimi-\nnator is then used to score the “real” feature\nmap / feature vector pair, while the “fake” pair\nis produced by pairing the feature vector with\na feature map from another image.\nFigure 6:\nEncode-and-dot-product archi-\ntecture.\nThe global feature vector is en-\ncoded using a fully-connected network, and\nthe lower-level feature map is encoded us-\ning 1x1 convolutions, but with the same num-\nber of output features. We then take the dot-\nproduct between the feature at each location\nof the feature map encoding and the encoded\nglobal vector for scores.\nFigure 7:\nMatching the output of the en-\ncoder to a prior. “Real” samples are drawn\nfrom a prior while “fake” samples from the en-\ncoder output are sent to a discriminator. The\ndiscriminator is trained to distinguish between\n(classify) these sets of samples. The encoder\nis trained to “fool” the discriminator.\nfully connected neural network to get the encoded global feature, Sω(Eψ(x)). In our experiments,\nwe used a single hidden layer network with a linear shortcut (See Table 8).\nTable 8: Local DIM encoder-and-dot architecture for global feature\nOperation\nSize\nActivation\nOutput\nInput →Linear\n2048\nReLU\nLinear\n2048\nOutput 1\nInput →Linear\n2048\nReLU\nOutput 2\nOutput 1 + Output 2\nWe embed each local feature in the local feature map Cψ(x) using an architecture which matches the\none for global feature embedding. We apply it via 1 × 1 convolutions. Details are in Table 9.\n17\nPublished as a conference paper at ICLR 2019\nTable 9: Local DIM encoder-and-dot architecture for local features\nOperation\nSize\nActivation\nOutput\nInput →1 × 1conv\n2048\nReLU\n1 × 1 conv\n2048\nOutput 1\nInput →1 × 1 conv\n2048\nReLU\nOutput 2\nOutput 1 + Output 2\nBlock Layer Norm\nFinally, the outputs of these two networks are combined by matrix multiplication, summing over the\nfeature dimension (2048 in the example above). As this is computed over a batch, this allows us to\nefﬁciently compute both positive and negative examples simultaneously. This architecture is featured\nin our main classiﬁcation results in Tables 1, 2, and 5.\nFor the local objective, the feature map, Cψ(x), can be taken from any level of the encoder, Eψ. For\nthe global objective, this is the last convolutional layer, and this objective was insensitive to which\nlayer we used. For the local objectives, we found that using the next-to-last layer worked best for\nCIFAR10 and CIFAR100, while for the other larger datasets it was the previous layer. This sensitivity\nis likely due to the relative size of the of the receptive ﬁelds, and further analysis is necessary to better\nunderstand this effect. Note that all feature maps used for DIM included the ﬁnal batch normalization\nand ReLU activation.\nPrior matching\nFigure 7 shows a high-level overview of the prior matching architecture. The\ndiscriminator used to match the prior in DIM was a fully-connected network with two hidden layers\nof 1000 and 200 units (Table 10).\nTable 10: Prior matching network architecture\nOperation\nSize\nActivation\nInput →Linear layer\n1000\nReLU\nLinear layer\n200\nReLU\nLinear layer\n1\nGenerative models\nFor generative models, we used a similar setup as that found in Donahue et al.\n(2016) for the generators / decoders, where we used a generator from DCGAN in all experiments.\nAll models were trained using Adam with a learning rate of 1 × 10−4 for 1000 epochs for CIFAR10\nand CIFAR100 and for 200 epochs for all other datasets.\nContrastive Predictive Coding\nFor Contrastive Predictive Coding (CPC, Oord et al., 2018), we\nused a simple a GRU-based PixelRNN (Oord et al., 2016) with the same number of hidden units as\nthe feature map depth. All experiments with CPC had the global state dimension matched with the\nsize of these recurrent hidden units.\nA.3\nSAMPLING STRATEGIES\nWe found both infoNCE and the DV-based estimators were sensitive to negative sampling strategies,\nwhile the JSD-based estimator was insensitive. JSD worked better (1−2% accuracy improvement) by\nexcluding positive samples from the product of marginals, so we exclude them in our implementation.\nIt is quite likely that this is because our batchwise sampling strategy overestimate the frequency of\npositive examples as measured across the complete dataset. infoNCE was highly sensitive to the\nnumber of negative samples for estimating the log-expectation term (see Figure 9). With high sample\nsize, infoNCE outperformed JSD on many tasks, but performance drops quickly as we reduce the\nnumber of images used for this estimation. This may become more problematic for larger datasets and\nnetworks where available memory is an issue. DV was outperformed by JSD even with the maximum\nnumber of negative samples used in these experiments, and even worse was highly unstable as the\nnumber of negative samples dropped.\n18\nPublished as a conference paper at ICLR 2019\nQuery\nDIM(G)\nDIM(L)\nFigure 8: Nearest-neighbor using the L1 distance on the encoded Tiny ImageNet images, with\nDIM(G) and DIM(L). The images on the far left are randomly-selected reference images (query)\nfrom the training set and the four images their nearest-neighbor from the test set as measured in the\nrepresentation, sorted by proximity. The nearest neighbors from DIM(L) are much more interpretable\nthan those with the purely global objective.\nFigure 9: Classiﬁcation accuracies (left: global representation, Y , right: convolutional layer) for\nCIFAR10, ﬁrst training DIM, then training a classiﬁer for 1000 epochs, keeping the encoder ﬁxed.\nAccuracies shown averaged over the last 100 epochs, averaged over 3 runs, for the infoNCE, JSD,\nand DV DIM losses. x-axis is log base-2 of the number of negative samples (0 mean one negative\nsample per positive sample). JSD is insensitive to the number of negative samples, while infoNCE\nshows a decline as the number of negative samples decreases. DV also declines, but becomes unstable\nas the number of negative samples becomes too low.\nA.4\nNEAREST-NEIGHBOR ANALYSIS\nIn order to better understand the metric structure of DIM’s representations, we did a nearest-neighbor\nanalysis, randomly choosing a sample from each class in the test set, ordering the test set in terms of\nL1 distance in the representation space (to reﬂect the uniform prior), then selecting the four with the\nlowest distance. Our results in Figure 8 show that DIM with a local-only objective, DIM(L), learns\na representation with a much more interpretable structure across the image. However, our result\npotentially highlights an issue with using only consistent information across patches, as many of the\nnearest neighbors share patterns (colors, shapes, texture) but not class.\n19\nPublished as a conference paper at ICLR 2019\n0.0\n0.5\n1.0\n1.5\n2.0\nα\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nβ\nγ = 0.1: Classiﬁer Accuracy\n35\n40\n45\n50\n55\n60\n65\n0.0\n0.5\n1.0\n1.5\n2.0\nα\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nβ\nγ = 0.1: MS-SSIM\n0.35\n0.40\n0.45\n0.50\n0.55\n0.0\n0.5\n1.0\n1.5\n2.0\nα\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nβ\nγ = 0.1: bIρ(X, Y )\n20\n40\n60\n80\n100\n0.0\n0.5\n1.0\n1.5\n2.0\nγ\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nβ\nα = 0: NDM\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n0.0\n0.5\n1.0\n1.5\n2.0\nγ\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nβ\nα = 0: Classiﬁer Accuracy\n35\n40\n45\n50\n55\n60\n65\nFigure 10: Results from the ablation studies with DIM on CIFAR10. Values calculated are points on\nthe grid, and the heatmaps were derived by bilinear interpolation. Heatmaps were thresholded at the\nminimum value (or maximum for NDM) for visual clarity. Highest (or lowest) value is marked on\nthe grid. NDM here was measured without the sigmoid function.\nFigure 11: Ablation study on CelebA over the global and local parameters, α and β. The classiﬁcation\ntask is multinomial, so provided is the average, minimum, and maximum class accuracies across\nattibutes. While the local objective is crucial, the global objective plays a stronger role here than with\nother datasets.\nA.5\nABLATION STUDIES\nTo better understand the effects of hyperparameters α, β, and γ on the representational characteristics\nof the encoder, we performed several ablation studies. These illuminate the relative importance of\nglobal verses local mutual information objectives as well as the role of the prior.\nLocal versus global mutual information maximization\nThe results of our ablation study for\nDIM on CIFAR10 are presented in Figure 10. In general, good classiﬁcation performance is highly\ndependent on the local term, β, while good reconstruction is highly dependent on the global term,\nα. However, a small amount of α helps in classiﬁcation accuracy and a small about of β improves\nreconstruction. For mutual information, we found that having a combination of α and β yielded\nhigher MINE estimates. Finally, for CelebA (Figure 11), where the classiﬁcation task is more\nﬁne-grained (is composed of potentially locally-speciﬁed labels, such as “lipstick” or “smiling”), the\nglobal objective plays a stronger role than with classiﬁcation on other datasets (e.g., CIFAR10).\n20\nPublished as a conference paper at ICLR 2019\nFigure 12: A schematic of learning the\nNeural Dependency Measure. For a\ngiven batch of inputs, we encode this\ninto a set of representations. We then\nshufﬂe each feature (dimension of the\nfeature vector) across the batch axis.\nThe original version is sent to the dis-\ncriminator and given the label “real”,\nwhile the shufﬂed version is labeled as\n“fake”. The easier this task, the more\ndependent the components of the repre-\nsentation.\n0.1\n0.5\n1.0\n1.5\n2.0\n4.0\nBeta\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nNDM\nFigure\n13:\nNeural\nDependency\nMea-\nsures (NDMs) for various β-VAE (Alemi\net al., 2016; Higgins et al., 2016) models\n(0.1, 0.5, 1.0, 1.5, 2.0, 4.0).\nError bars are\nprovided over ﬁve runs of each VAE and es-\ntimating NDM with 10 different networks.\nWe ﬁnd that there is a strong trend as we in-\ncrease the value of β and that the estimates\nare relatively consistent and informative w.r.t.\nindependence as expected.\nThe effect of the prior\nWe found including the prior term, γ, was absolutely necessary for ensuring\nlow dependence between components of the high-level representation, Eψ(x), as measured by NDM.\nIn addition, a small amount of the prior term helps improve classiﬁcation results when used with the\nlocal term, β. This may be because the additional constraints imposed on the representation help to\nencourage the local term to focus on consistent, rather than trivial, information.\nA.6\nEMPIRICAL CONSISTENCY OF NEURAL DEPENDENCY MEASURE (NDM)\nHere we evaluate the Neural Dependency Measure (NDM) over a range of β-VAE (Alemi et al., 2016;\nHiggins et al., 2016) models. β-VAE encourages disentangled representations by increasing the role\nof the KL-divergence term in the ELBO objective. We hypthesized that NDM would consistenly\nmeasure lower dependence (lower NDM) as the β values increase, and our results in Figure A.6\nconﬁrm this. As we increase β, there is a strong downward trend in the NDM, though β = 0.5 and\n21\nPublished as a conference paper at ICLR 2019\nβ = 1.0 give similar numbers. In addition, the variance over estimates and models is relatively low,\nmeaning the estimator is empirically consistent in this setting.\nA.7\nADDITIONAL DETAILS ON OCCLUSION AND COORDINATE PREDICTION EXPERIMENTS\nHere we present experimental details on the occlusion and coordinate prediction tasks.\nTraining without occlusion\nTraining with occlusion\nFigure 14: Visualizing model behaviour when computing global features with and without occlusion,\nfor NCE-based DIM. The images in each block of images come in pairs. The left image in each\npair shows the model input when computing the global feature vector. The right image shows the\nNCE loss suffered by the score between that global feature vector and the local feature vector at each\nlocation in the 8 × 8 local feature map computed from the unoccluded image. This loss is equal to\nminus the value in Equation 5. With occluded inputs, this loss tends to be highest for local features\nwith receptive ﬁelds that overlap the occluded region.\nOcclusions.\nFor the occlusion experiments, the sampling distribution for patches to occlude was\nad-hoc. Roughly, we randomly occlude the input image under the constraint that at least one 10 × 10\nblock of pixels remains visible and at least one 10 × 10 block of pixels is fully occluded. We\nchose 10 × 10 based on the receptive ﬁelds of local features in our encoder, since it guarantees\nthat occlusion leaves at least one local feature fully observed and at least one local feature fully\nunobserved. Figure 14 shows the distribution of occlusions used in our tests.\nAbsolute coordinate prediction\nFor absolute coordinate prediction, the global features y and local\nfeatures c(i,j) are sampled by 1) feeding an image from the data distribution through the feature\nencoder, and 2) sampling a random spatial location (i, j) from which to take the local features c(i,j).\nGiven y and c(i,j), we treat the coordinates i and j as independent categorical variables and measure\nthe required log probability using a sum of categorical cross-entropies. In practice, we implement the\nprediction function pθ as an MLP with two hidden layers, each with 512 units, ReLU activations,\nand batchnorm. We marginalize this objective over all local features associated with a given global\nfeature when computing gradients.\nRelative coordinate prediction\nFor relative coordinate prediction, the global features y and local\nfeatures c(i,j)/c(i′,j′) are sampled by 1) feeding an image from the data distribution through the\nfeature encoder, 2) sampling a random spatial location (i, j) from which to take source local features\nc(i,j), and 3) sampling another random location (i′, j′) from which to take target local features c(i′,j′).\nIn practice, our predictive model for this task uses the same architecture as for the task described\npreviously. For each global feature y we select one source feature c(i,j) and marginalize over all\npossible target features c(i′,j′) when computing gradients.\n22\nPublished as a conference paper at ICLR 2019\na) GAN\nb) Proxy\nc) LSGAN\nd) WGAN\ne) Our method\nFigure 15: Histograms depicting the discriminator’s unnormalized output distribution for the standard\nGAN, GAN with −log D loss, Least Squares GAN, Wasserstein GAN and our proposed method\nwhen trained with a 50:1 training ratio.\nA.8\nTRAINING A GENERATOR BY MATCHING TO A PRIOR IMPLICITLY\nWe show here and in our experiments below that we can use prior objective in DIM (Equation 7)\nto train a high-quality generator of images by training Uψ,P to map to a one-dimensional mixture\nof two Gaussians implicitly. One component of this mixture will be a target for the push-forward\ndistribution of P through the encoder while the other will be a target for the push-forward distribution\nof the generator, Qθ, through the same encoder.\nLet Gθ : Z →X be a generator function, where the input z ∈Z is drawn from a simple prior,\np(z) (such as a spherical Gaussian). Let Qθ be the generated distribution and P be the empirical\ndistribution of the training set. Like in GANs, we will pass the samples of the generator or the training\ndata through another function, Eψ, in order to get gradients to ﬁnd the parameters, θ. However, unlike\nGANs, we will not play the minimax game between the generator and this function. Rather Eψ will\nbe trained to generate a mixture of Gaussians conditioned on whether the input sample came from P\nor Qθ:\nVP = N(µP , 1),\nVQ = N(µQ, 1),\nUψ,P = P#Eψ,\nUψ,Q = Qθ#Eψ,\n(13)\nwhere N(µP , 1) and N(µQ, 1) are normal distributions with unit variances and means µP and µQ\nrespectively. In order to ﬁnd the parameters ψ, we introduce two discriminators, T P\nφP , T Q\nφQ : Y →R,\nand use the lower bounds following deﬁned by the JSD f-GAN:\n( ˆψ, ˆφP , ˆφQ) = arg min\nψ\narg max\nφP ,φQ\nLd(VP, Uψ,P, T P\nφP ) + Ld(VQ, Uψ,Q, T Q\nφQ).\n(14)\nThe generator is trained to move the ﬁrst-order moment of EUψ,Q[y] = Ep(z)[Eψ(Gθ(z))] to µP :\nˆθ = arg min(Ep(z)[Eψ(Gθ(z))] −µP )2.\n(15)\nSome intuition might help understand why this might work. As discussed in Arjovsky & Bottou\n(2017), if P and Qθ have support on a low-dimensional manifolds on X, unless they are perfectly\naligned, there exists a discriminator that will be able to perfectly distinguish between samples coming\nfrom P and Qθ, which means that Uψ,P and Uψ,Q must also be disjoint.\nHowever, to train the generator, Uψ,P and Uψ,Q need to share support on Y in order to ensure stable\nand non-zero gradients for the generator. Our own experiments by overtraining the discriminator\n(Figure 15) conﬁrm that lack of overlap between the two modes of the discriminator is symptomatic\nof poor training.\nSuppose we start with the assumption that the encoder targets, VP and VQ, should overlap. Unless P\nand Qθ are perfectly aligned (which according to Arjovsky & Bottou (2017) is almost guaranteed not\nto happen with natural images), then the discriminator can always accomplish this task by discarding\ninformation about P or Qθ. This means that, by choosing the overlap, we ﬁx the strength of the\nencoder.\n23\nPublished as a conference paper at ICLR 2019\nTable 11: Generation scores on the Tiny Imagenet dataset for non-saturating GAN with contractive\npenalty (NS-GAN-CP), Wasserstein GAN with gradient penalty (WGAN-GP) and our method. Our\nencoder was penalized using CP.\nModel\nInception score\nFID\nReal data\n31.21 ± .68\n4.03\nIE (ours)\n7.41 ± .10\n55.15\nNS-GAN-CP\n8.65 ± .08\n40.17\nWGAN-GP\n8.38 ± 0.18\n42.30\nA.9\nGENERATION EXPERIMENTS AND RESULTS\nFor the generator and encoder, we use a ResNet architecture (He et al., 2016) identical to the one\nfound in Gulrajani et al. (2017). We used the contractive penalty (found in Mescheder et al. (2018)\nbut ﬁrst introduced in contractive autoencoders (Rifai et al., 2011)) on the encoder, gradient clipping\non the discriminators, and no regularization on the generator. Batch norm (Ioffe & Szegedy, 2015)\nwas used on the generator, but not on the discriminator. We trained on 64×64 dimensional LSUN (Yu\net al., 2015), CelebA (Liu et al., 2015), and Tiny Imagenet dataset.\nA.10\nIMAGES GENERATION\na) NS-GAN-CP\nb) WGAN-GP\nc) Mapping to two Gaussians\nFigure 16: Samples of generated results used to get scores in Table 11. For every methods, the sample\nare generated after 100 epochs and the models are the same. Qualitative results from these three\nmethods show no qualitative difference.\nHere, we train a generator mapping to two Gaussian implicitly as described in Section A.8. Our\nresults (Figure 16) show highly realistic images qualitatively competitive to other methods (Gulrajani\net al., 2017; Hjelm et al., 2018). In order to quantitatively compare our method to GANs, we trained a\nnon-saturating GAN with contractive penalty (NS-GAN-CP) and WGAN-GP (Gulrajani et al., 2017)\nwith identical architectures and training procedures. Our results (Table 11) show that, while our\nmehtod did not surpass NS-GAN-CP or WGAN-GP in our experiments, they came reasonably close.\n24\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2018-08-20",
  "updated": "2019-02-22"
}