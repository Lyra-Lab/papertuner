{
  "id": "http://arxiv.org/abs/1808.08618v2",
  "title": "Deep Learning: Computational Aspects",
  "authors": [
    "Nicholas Polson",
    "Vadim Sokolov"
  ],
  "abstract": "In this article we review computational aspects of Deep Learning (DL). Deep\nlearning uses network architectures consisting of hierarchical layers of latent\nvariables to construct predictors for high-dimensional input-output models.\nTraining a deep learning architecture is computationally intensive, and\nefficient linear algebra libraries is the key for training and inference.\nStochastic gradient descent (SGD) optimization and batch sampling are used to\nlearn from massive data sets.",
  "text": "Deep Learning: Computational Aspects\nNicholas Polson∗and Vadim Sokolov†\nAbstract\nIn this article we review computational aspects of Deep Learning (DL). Deep learn-\ning uses network architectures consisting of hierarchical layers of latent variables\nto construct predictors for high-dimensional input-output models. Training a deep\nlearning architecture is computationally intensive, and efﬁcient linear algebra libraries\nis the key for training and inference. Stochastic gradient descent (SGD) optimization\nand batch sampling are used to learn from massive data sets.\n1\nIntroduction\nDeep learning (DL) is a form of machine learning that uses hierarchical layers of abstrac-\ntion to model complex structures. DL requires efﬁcient training strategies and these are\nat the heart of today’s successful applications which range from natural language pro-\ncessing to engineering and ﬁnancial analysis. While deep learning has been available for\nseveral decades there were only a few practical applications until the early 2010s when\nthe ﬁeld has changed for several reasons. The renaissance is due to a number of factors,\nin particular\n1. Hardware and software for accelerated computing (GPUs and specialized linear\nalgebra libraries)\n2. Increased size of datasets (Massive Data)\n3. Efﬁcient algorithms, such as stochastic gradient descent (SGD).\nThe goal of our article is to provide the reader with an overview of computational aspects\nunderlying the algorithms and hardware, which allow modern deep learning models to\nbe implemented at scale. Many of the leading Internet companies employ DL at scale\nHazelwood et al. (2017). The most impressive accomplishment of DL is its many suc-\ncessful applications in research and business. These applications include algorithms such\nas\n∗Booth School of Business, University of Chicago\n†Systems Engineering and Operations Research, George Mason University\n1\narXiv:1808.08618v2  [cs.LG]  28 Aug 2019\n1. Google Neural Machine Translation Wu et al. (2016) closes the gap with humans in\naccuracy of the translation by 55-85% (estimated by people on a 6-point scale). One\nof the keys to the success of the model is the use of Google’s huge dataset.\n2. Chat Bots which predict natural language response have been available for many\nyears. Deep learning networks can signiﬁcantly improve the performance of chat-\nbots Henderson et al. (2017). Nowadays they provide help systems for companies\nand home assistants such as Amazon’s Alexa and Google home.\n3. Voice Generation was taken to the next level by DL based solutions. Google WaveNet,\ndeveloped by DeepMind Oord et al. (2016), generates speech from text and reduces\nthe gap between the state-of-the art and human-level performance by over 50% for\nboth US English and Mandarin Chinese.\n4. Google Maps were improved after DL was developed to analyze more than 80 bil-\nlion Street View images and to extract names of roads and businesses Wojna et al.\n(2017).\n5. Companies like Google Calico and Google Brain Health develop DL for health care\ndiagnostics. Adversarial Auto-encoder model found new molecules to ﬁght cancer.\nIdentiﬁcation and generation of new compounds was based on available biochemi-\ncal data Kadurin et al. (2017).\n6. Convolutional Neural Nets (CNNs) have been developed to detect pneumonia from\nchest X-rays with better accuracy than practicing radiologists Rajpurkar et al. (2017).\nAnother CNN model is capable of identifying skin cancer from biopsy-labeled test\nimages Esteva et al. (2017). Shallue and Vanderburg (2017) has discovered two new\nplanets using DL and massive data from NASAs Kepler Space Telescope.\n7. In more traditional engineering, science applications, such as spatio-temporal and\nﬁnancial analysis DL showed superior performance compared to traditional statisti-\ncal learning techniques Polson and Sokolov (2017a); Dixon et al. (2017); Heaton et al.\n(2017); Sokolov (2017); Feng et al. (2018b,a)\nIn this paper we discuss computationally intensive algorithms for training deep learning\nmodels. The main advantage of DL models is the ability to learn complex relationships\nin high dimensions. However, a large number of samples is required to make useful pre-\ndictions. Ability to train DL models using large number (millions) of high-dimensional\ninputs is the key to the current success of those models. A specialized software that uti-\nlizes accelerated hardware computing architectures, such as Graphical Processing Units\n(GPUs) or Tensor Processing Units (TPU) is typically used to train DL models. We start\nin Section ?? by reviewing deep learning models. In Section 3 we review most commonly\nused optimization algorithm and highlight that the core operation of those are matrix-\nmatrix multiplications. Is Section 4 we review techniques to accelerate linear algebra\noperations (e.g. matrix-matrix multiplications). In Section 5 we review specialized pro-\ncessors that are currently used for training large scale models and the software libraries\nthat support those architectures are discussed in Section 6. Finally, we conclude with\nSection where we suggest some further readings.\n2\n2\nDeep Learning\nSimply put, DL constructs an input-output map. Let Y represent an output (or response)\nto a task which we aim to solve based on the information in a given high dimensional\ninput matrix, denoted by X. An input-output mapping is denoted by Y = F(X) where\nX = (X1, . . . , Xp) is a vector of predictors.\nPolson and Sokolov (2017b) view the theoretical roots of DL in Kolmogorov’s repre-\nsentation of a multivariate response surface as a superposition of univariate activation\nfunctions applied to an afﬁne transformation of the input variable Kolmogorov (1956,\n1957). An afﬁne transformation of a vector is a weighted sum of its elements (linear trans-\nformation) plus an offset constant (bias). Our Bayesian perspective on DL leads to new\navenues of research including faster stochastic algorithms, hyper-parameter tuning, con-\nstruction of good predictors, and model interpretation.\nOn the theoretical side, DL exploits Kolmogorov’s “universal basis”. The fact that DL\nforms a universal ‘basis’, which we recognize in this formulation, dates back to Poincare\nand Hilbert. By construction, deep learning models are very ﬂexible and gradient infor-\nmation can be efﬁciently calculated for a variety of architectures. On the empirical side,\nthe advances in DL are due to a number of factors, in particular:\n1. New activation functions, e.g. rectiﬁed linear unit (ReLU(x) = max(0, x)).\n2. Dropout as a variable selection technique and use of multiple layers\n3. Computationally efﬁcient routines to train and evaluate the models as well as ac-\ncelerated computing on graphics processing unit (GPU) and tensor processing unit\n(TPU).\n4. Computational software such as TensorFlow or PyTorch.\nSimilar to a classic basis decomposition, the deep approach uses univariate activation\nfunctions to decompose a high dimensional X. Let Z(l) denote the lth layer, and so X =\nZ(0). The ﬁnal output Y can be numeric or categorical. The explicit structure of a deep\nprediction rule is then\nZ(1) = f (1) \u0010\nW(0)X + b(0)\u0011\nZ(2) = f (2) \u0010\nW(1)Z(1) + b(1)\u0011\n(1)\n. . .\nZ(L) = f (L) \u0010\nW(L−1)Z(L−1) + b(L−1)\u0011\nˆY(X | W, b) = f L+1(W(L)Z(L) + b(L)) .\nHere W = (W(0), . . . , W(L)), are weight matrices and b = (b(0), . . . , b(L)) are threshold or\nactivation levels. Designing a good predictor depends crucially on the choice of univari-\nate activation functions f (l). The Z(l) are hidden features which the algorithm will extract.\nFor a regression problem, we use an identity function for the last layer f L+1 = I and for a\n3\nclassiﬁcation problem, we use a logistic function f L+1(x) = 1/(1 + e−x). For a more ex-\ntended overview of deep learning models, see Polson and Sokolov (2018b); LeCun et al.\n(2015); Goodfellow et al. (2016); Schmidhuber (2015). From a practical perspective, given\na large enough data set of “test cases”, we can empirically learn an optimal predictor.\nFrom a statistical point of view, Equation (1) can be viewed as a hierarchical generalized\nlinear model with a simple GLM being a speciﬁc case when L = 0. When L > 0, it is prac-\ntically impossible to interpret neither parameters of the model (W, b) nor the outputs of\nhidden layers Z(l). Thus, while adding hidden layers allows for learning more complex\nrelations in the data, it prevents us from explaining the prediction rule. Explainability of\ndeep learning models is one of the hurdles that prevents those from being used in heavily\nregulated industries, such as ﬁnance or insurance. It is possible to calculate derivative\nof the output ˆY(X) with respect to any of the inputs Xi and thus derive a measure of\nsensitivity or importance of each of the inputs. The key constraint of this approach is\nthat it is limited to be applied to one input predictor at a time. On the other hand, the\nmain advantage of deep learning model is the ability to capture interactions among the\ninputs. Thus, the sensitivity analysis is very limiting. Ability to explain predictions of\ndeep learning models is an open area of research. An extension to sensitivity based anal-\nysis of deep learning models was proposed by Shrikumar et al. (2017) who use not only\nderivatives of the model output but also of the output of each of the hidden layers and\nderive metrics that allow to explain the interactions among inputs. Sundararajan et al.\n(2017) use a modiﬁcation of the sensitivity approach called integrated gradients to extract\nexplanations of the prediction rules. Ribeiro et al. (2016) proposed to ﬁt an interpretable\nmodel locally around a speciﬁc value of the input vector. Ibrahim et al. (2019) propose a\nmethod to explain deep learning predictions for different parts of the input space (popu-\nlation of samples). Modarres et al. (2018) demonstrate empirical performance of several\ntechniques for deep learning model interoperability.\n2.1\nA Probabilistic View of DL\nProbabilistically the output Y can be viewed as a random variable being generated by\na probability model p\n\u0000Y | ˆY(X | W, b)\n\u0001\n, where ˆY(X | W, b) is a prediction by a deep\nlearning models with weights W = (W(0), . . . , W(L)), and b = (b(0), . . . , b(L)). Then, given\nparameters (W, b), the negative log-likelihood deﬁnes a loss L as\nL(Y, ˆY) = −log p\n\u0000Y | ˆY(X | W, b)\n\u0001\n.\nGiven a training sample D = {(Xi, Yi)}n\ni=1, the L2-norm,\nL(Yi, ˆY(Xi)) = 1/n\nn\n∑\ni=1\n(Yi −ˆY(Xi))2\n2\nis traditional least squares, and negative cross-entropy loss is\nL(Yi, ˆY(Xi)) = −\nn\n∑\ni=1\nYi log ˆY(Xi)\n4\nfor multi-class logistic classiﬁcation.\nThere is a bias-variance trade-off, which is controlled by adding a regularization term\nand optimizing the regularized loss\nLλ(Y, ˆY) = −log p\n\u0000Y | ˆY(X | W, b)\n\u0001 −log p(W, b | λ).\nThe regularization term is a negative log-prior distribution over parameters, namely\n−log p(W, b | λ) = λφ(W, b),\np(W, b | λ) ∝exp(−λφ(W, b)).\nDeep predictors are regularized maximum a posteriori (MAP) estimators, where\n−p(W, b|D) ∝−p\n\u0000Y | ˆY(X | W, b)\n\u0001\np(W, b | λ)\n∝exp\n\u0000−log p\n\u0000Y | ˆY(X | W, b)\n\u0001 −log p(W, b)\n\u0001\n.\nTraining requires the solution of a highly nonlinear optimization problem\narg max\nW,b\nlog p(W, b | D).\nThis problem is solved using Stochastic Gradient Descent (SGD) which iteratively up-\ndates the parameters (W, b) by taking a step in the direction negative to the gradient.\nThe key property is that ∇W,b log p(W, b | D) is computationally inexpensive to evalu-\nate using back-propagation algorithms that implemented using modern matrix compu-\ntation libraries for various hardware architectures. It makes a fast implementation on\nlarge datasets possible. TensorFlow and TPUs provide a state-of-the-art framework for a\nplethora of deep learning architectures. From a statistical perspective, one caveat is that\nthe posterior is highly multi-modal and providing good hyper-parameter (e.g. number\nof layers and neurons per layer) tuning can be expensive. This is clearly a fruitful area of\nresearch for state-of-the-art stochastic Bayesian MCMC algorithms to provide more efﬁ-\ncient algorithms. For shallow architectures, the alternating direction method of multipli-\ners (ADMM) provides an efﬁcient optimization solution. For more details on probabilistic\nand Bayesian perspective on deep learning, see Polson and Sokolov (2017b).\n3\nOptimization Algorithms\nWe now discuss two types of algorithms for training learning models. First, we discuss\nstochastic gradient descent, which is a very general algorithm that efﬁciently works for\nlarge scale datasets and has been used for many deep learning applications. Second, we\ndiscuss specialized statistical learning algorithms, which are tailored for certain types of\ntraditional statistical models.\n5\n3.1\nStochastic Gradient Descent\nStochastic gradient descent (SGD) is a default gold standard for minimizing the a func-\ntion f (W, b) (maximizing the likelihood) to ﬁnd the deep learning weights and offsets.\nSGD simply minimizes the function by taking a negative step along an estimate gk of the\ngradient ∇f (Wk, bk) at iteration k. The gradients are available via the chain rule applied\nto the superposition of semi-afﬁne functions.\nThe approximate gradient is estimated by calculating\ngk =\n1\n|Ek| ∑\ni∈Ek\n∇L(Yi, ˆY(Xi | Wk, bk)),\nwhere Ek ⊂{1, . . . , T} and |Ek| is the number of elements in Ek.\nWhen |Ek| > 1 the algorithm is called batch SGD and simply SGD otherwise. Typ-\nically, the subset E is chosen by going cyclically and picking consecutive elements of\n{1, . . . , T}, Ek+1 = [Ek mod T] + 1. The direction gk is calculated using a chain rule\n(a.k.a. back-propagation) providing an unbiased estimator of the gradient computed us-\ning the entire sample ∇f (Wk, bk). Speciﬁcally, this leads to\nE(gk) = E\n\u0010\n∇f (Wk, bk)\n\u0011\n.\nAt each iteration, SGD updates the solution\n(W, b)k+1 = (W, b)k −tkgk.\nDeep learning algorithms use a step size tk (a.k.a learning rate) that is either kept con-\nstant or a simple step size reduction strategy, such as tk = a exp(−kt) is used. The hyper\nparameters of reduction schedule are usually found empirically from numerical experi-\nments and observations of the loss function progression.\nOne caveat of SGD is that the descent in f is not guaranteed, or it can be very slow\nat every iteration. Stochastic Bayesian approaches ought to alleviate these issues. For ex-\nample, Wang et al. (2019) provide a scalable MCMC algorithm that can be used to train\nmulti-modal loss function that arise when training deep learning architectures. The vari-\nance of the gradient estimate gk can also be near zero, as the iterates converge to a solution.\nTo tackle those problems a coordinate descent (CD) and momentum-based modiﬁcations\ncan be applied. Alternative directions method of multipliers (ADMM) can also provide\na natural alternative, and leads to non-linear alternating updates, see Carreira-Perpin´an\nand Wang (2014).\nThe CD evaluates a single component Ek of the gradient ∇f at the current point and\nthen updates the Ekth component of the variable vector in the negative gradient direction.\nThe momentum-based versions of SGD, or so-called accelerated algorithms were origi-\nnally proposed by Nesterov (1983). For a more recent discussion, see Nesterov (2013).\nThe momentum term adds memory to the search process by combining new gradient in-\nformation with the previous search directions. Empirically momentum-based methods\nhave been shown to have better convergence for DL networks Sutskever et al. (2013). The\n6\ngradient only inﬂuences changes in the velocity of the update which then updates the\nvariable\nvk+1 =µvk −tkg((W, b)k)\n(W, b)k+1 =(W, b)k + vk\nThe hyper-parameter µ controls the dumping effect on the rate of update of the variables.\nThe physical analogy is the reduction in kinetic energy that allows to “slow down” the\nmovements at the minima. This parameter can also be chosen empirically using cross-\nvalidation.\nNesterov’s momentum method (a.k.a. Nesterov acceleration) calculates the gradient\nat the point predicted by the momentum. One can view this as a one-step look-ahead\nstrategy with updating scheme\nvk+1 =µvk −tkg((W, b)k + vk)\n(W, b)k+1 =(W, b)k + vk.\nAnother popular modiﬁcation Zeiler (2012), adaptively scales each of the learning param-\neter at each iteration\nck+1 =ck + g((W, b)k)2\n(W, b)k+1 =(W, b)k −tkg(W, b)k)/(\n√\nck+1 −a),\nwhere a is typically a small number, e.g. a = 10−6 that prevents dividing by zero. This\nmethod is called AdaGrad.\nPRMSprop takes the AdaGrad idea further and places more\nweight on recent values of gradient squared to scale the update direction, i.e. we have\nck+1 = dck + (1 −d)g((W, b)k)2.\nThe Adam method Kingma and Ba (2014) combines both PRMSprop and momentum meth-\nods and leads to the following update equations\nvk+1 =µvk −(1 −µ)tkg((W, b)k + vk)\nck+1 =dck + (1 −d)g((W, b)k)2\n(W, b)k+1 =(W, b)k −tkvk+1/(\n√\nck+1 −a).\nInitial guess in model weights and choice of optimization algorithms parameters plays\ncrucial role in rate of convergence of the SGD and its variants Sutskever et al. (2013).\nSecond order methods solve the optimization problem by solving a system of nonlin-\near equations ∇f (W, b) = 0 by applying the Newton’s method\n(W, b)+ = (W, b) −{∇2 f (W, b)}−1∇f (W, b).\nHere SGD simply approximates ∇2 f (W, b) by 1/t. The advantages of a second order\nmethod include much faster convergence rates and insensitivity to the conditioning of the\n7\nproblem. An ill-conditioned problem is the one that has “ﬂat directions” in which func-\ntion changes very slowly and it makes SGD rates low. In practice, second order methods\nare rarely used for deep learning applications Dean et al. (2012). The major disadvantage\nis their inability to train models using batches of data as SGD does. Second order meth-\nods require the inverse Hessian matrix, which in turn requires the entire data set to be\ncalculated. Since a typical DL model relies on large scale data sets, second order meth-\nods become memory and computationally prohibitive at even modest-sized training data\nsets.\n3.2\nAutomatic Differentiation (AD)\nTo calculate the value of the gradient vector, at each step of the optimization process, deep\nlearning libraries require calculations of derivatives. In general, there are three different\nways to calculate those derivatives. First, is numerical differentiation, when a gradient is\napproximated by a ﬁnite difference f ′(x) = ( f (x + h) −f (x))/h and requires two func-\ntion evaluations. However, the numerical differentiation is not backward stable Griewank\net al. (2012), meaning that for a small perturbation in input value x, the calculated deriva-\ntive is not the correct one. Second, is a symbolic differentiation which has been used in\nsymbolic computational frameworks such as Mathematica or Maple for decades. Sym-\nbolic differentiation uses a tree form representation of a function and applies chain rule\nto the tree to calculate the symbolic derivative of a given function. Figure 1 shows a tree\nrepresentation of of composition of afﬁne and sigmoid functions.\nPower\nPlus\n1\nPower\nⅇ\nPlus\nTimes\n-1\nb\nTimes\n-1\nw\nx\n-1\nFigure 1:\nTree form representation of composition of afﬁne and sigmoid functions:\n1\ne−b−wx+1\nThe advantage of symbolic calculations is that analytical representation of derivative\nis available for further analysis. For example, when derivative calculation is in an inter-\nmediate step of the analysis. Third way to calculate a derivate is to use automatic differ-\nentiation (AD). Similar to symbolic differentiations AD recursively applies the chain rule\nand calculates the exact value of derivative and thus avoids the problem of numerical in-\nstability. The difference between AD and symbolic differentiation is that AD provides the\n8\nvalue of derivative evaluated at a speciﬁc point rather than an analytical representation\nof the derivative.\nAD does not require analytical speciﬁcation and can be applied to a function deﬁned\nby a sequence of algebraic manipulations, logical and transient functions applied to in-\nput variables and speciﬁed in a computer code. AD can differentiate complex functions\nwhich involve IF statements and loops, and AD can be implemented using either forward\nor backward mode. Consider an example of calculating a derivative of the following func-\ntion with respect to x.\ndef sigmoid(x,b,w):\nv1 = w*x;\nv2 = v1 + b\nv3 = 1/(1+exp(-v2))\nIn the forward mode an auxiliary variable, called a dual number, will be added to\neach line of the code to track the value of the derivative associated with this line. In our\nexample, if we set x=2, w=3, b=5, we get the calculations given in Table 1.\nFunction calculations\nDerivative calculations\n1. v1 = w*x = 6\n1. dv1 = w = 3 (derivative of v1 with respect to x)\n2. v2 = v1 + b = 11\n2. dv2 = dv1 = 3 (derivative of v2 with respect to x)\n3. v3 = 1/(1+exp(-v2)) = 0.99\n3. dv3 = eps2*exp(-v2)/(1+exp(-v2))**2\n= 5e-05\n(derivative of v3 with respect to x)\nTable 1: Forward AD algorithm\nVariables dv1,dv2,dv3 in Table 1 correspond to partial (local) derivatives of each in-\ntermediate variables v1,v2,v3 with respect to x, and are called dual variables. Tracking\nfor dual variables can either be implemented using source code modiﬁcation tools that\nadd new code for calculating the dual numbers or via operator overloading.\nThe reverse AD also applies chain rule recursively but starts from the outer function,\nas shown in Table 2.\nFunction calculations\nDerivative calculations\n1. v1 = w*x = 6\n4. dv1dx =w; dv1 = dv2*dv1dx = 3*1.3e-05=5e-05\n2. v2 = v1 + b = 11\n3. dv2dv1 =1; dv2 = dv3*dv2dv1 = 1.3e-05\n3. v3 = 1/(1+exp(-v2)) = 0.99\n2. dv3dv2 = exp(-v2)/(1+exp(-v2))**2;\ndv3 = dv4*dv3dv2 = 1.3e-05\n4. v4 = v3\n1. dv4=1\nTable 2: Reverse AD algorithm\nFor DL, derivatives are calculated by applying reverse AD algorithm to a model which\nis deﬁned as a superposition of functions. A model is deﬁned either using a general pur-\npose language as it is done in PyTorch or through a sequence of function calls deﬁned by\nframework libraries (e.g. in TensorFlow). Forward AD algorithms calculate the derivative\nwith respect to a single input variable, but reverse AD produces derivatives with respect\n9\nto all intermediate variables. For models with a large number of parameters, it is much\nmore computationally feasible to perform the reverse AD.\nIn the context of neural networks the reverse AD algorithms is called back-propagation\nand was popularized in AI by Rumelhart et al. (1986). According to Schmidhuber (2015)\nthe ﬁrst version of what we call today back-propagation was published in 1970 in a\nmaster’s thesis Linnainmaa (1970) and was closely related to the work of Ostrovskii\net al. (1971). However, similar techniques rooted in Pontryagin’s maximization princi-\nple Boltyanskii et al. (1960) were discussed in the context of multi-stage control prob-\nlems Bryson (1961, 1969). Dreyfus (1962) applies back-propagation to calculate ﬁrst order\nderivative of a return function to numerically solve a variational problem. Later Drey-\nfus (1973) used back-propagation to derive an efﬁcient algorithm to solve a minimization\nproblem. The ﬁrst neural network speciﬁc version of back-propagation was proposed\nin Werbos (1974) and an efﬁcient back-propagation algo ritm was discussed in Werbos\n(1982).\nModern deep learning frameworks fully automate the process of ﬁnding derivatives\nusing AD algorithms. For example, PyTorch relies on autograd library which automat-\nically ﬁnds gradient using back-propagation algorithm. Here is a small code example\nusing autograd library in Python.\n# Thinly wrapped numpy\nimport autograd.numpy as np\n# Basically everything you need\nfrom autograd import grad\n# Define a function like normal with Python and Numpy\ndef tanh(x):\ny = np.exp(-x)\nreturn (1.0 - y) / (1.0 + y)\n# Create a function to compute the gradient\ngrad_tanh = grad(tanh)\n# Evaluate the gradient at x = 1.0\nprint(grad_tanh(1.0))\n3.3\nArchitecture Optimization\nCurrently, there is no automated way to ﬁnd a good deep learning architecture. An ar-\nchitecture is deﬁned by number of hidden layers, a number of neuron on each layer, pa-\nrameters that deﬁne weight sharing layers, such as convolution layers or recurrent layers.\nAll of those parameters that deﬁned an architecture belong to the set of hyperparameters.\nAnother group of hyperparameters specify the settings for stochastic gradient descent\nalgorithms, e.g. learning rate, momentum, etc.\nIt is not uncommon to use hand-tuning to ﬁnd a deep learning architecture, when a\nmodeler hand-picks several candidates and choses the one that performs the best on out-\nof-sample data. It is usually done iteratively and might take weeks or months. An easiest\nautomated way to ﬁnd an optimal set of hyperparameters is grid search, when space of\nhyperparameters is discretized using a grid and a model is estimated for each node of the\ngrid. This approach is used, for example, to ﬁnd an optimal penalty weight for a LASSO\nmodel. However, this approach is not feasible, when number of hyperparameters is large.\nA random search rather samples from the grid randomly. This, does not guarantee the\n10\noptimal architecture will be identiﬁed but works rather well in practice Bergstra and Ben-\ngio (2012). Figure 2 shows an example of randomly chosen grid points, while searching\nfor an optimal number of neurons on the ﬁrst hidden layer n1 and the best learning rate\nα.\n α\nn1\n10 \n20 \n30 \n0.001 \n0.005 \n0.01 \nFigure 2: Random grid search for hyperparameters\nBayesian optimization Srinivas et al. (2009); Snoek et al. (2012) for hyperparameters\nsearch is more sample efﬁcient, i.e. requires less model evaluations to ﬁnd the best candi-\ndate. Bayesian methods rely on approximating the relations between hyper-parameters\nand model performance using a Gaussian Process surrogate model Mockus (2012). Gaus-\nsian process surrogates have the attractive property that the posterior distribution over\nfunction value at any point follows a Gaussian distribution. The stochastic nature of\nthe surrogate allows to quantify uncertainty over the function values and to explore the\nhyper-parameter space using approaches that can alternate exploration (searching in-\nput regions associated with high uncertainty levels of output values) and exploitations\n(searching regions of local minimal). However, sequential nature of the search process\nprevents from distributed parallel evaluations of models and is usually less preferred\ncompared to random search when a large number of compute nodes is available. One can\nrun several instances of Bayesian search in parallel using different initial values Shah and\nGhahramani (2015). For example, Google’s default architecture search algorithm Golovin\net al. (2017) uses batched Bayesian optimization with Mat´ern kernel Gaussian process.\nHowever, this approach, empirically is less efﬁcient compared to random search. Tech-\nniques to speed up Bayesian search include early stopping Gy¨orgy and Kocsis (2011) and\nusing a fraction of the data to evaluate models Sabharwal et al. (2016).\nGenetic-like algorithms provide advantage of sample efﬁciency and of parallel com-\nputing. Recently, Jaderberg et al. (2017) proposed a population based training approach,\nthat evaluates multiple models in parallel and then generates new model candidates by\nmodifying architectures of the models that performed best thus far.\n4\nScalable Linear Algebra\nThe key computational routine required to evaluate a DL model speciﬁed by Equation\n(1) is matrix-matrix multiplication. In the context of DL models weights W and inputs\n11\nand outputs of each layer X, Z(1), . . . , Z(L), ˆY are called tensors. For example, in image\nprocessing input X is a three dimensional tensor, which is made up by three matrices that\ncorrespond to red, green and blue color channels. Thus, one of the key operations while\ntraining DL or calculating a prediction is a matrix-matrix multiplication, with matrix-\nvector, dot product or saxpy ax + y (scalar a times x plus y) being a special cases.\nNaive implementation of matrix-matrix multiplication would invoke a loop over the\nelements of the input matrices, which is inefﬁcient. We can parallelize the operations,\neven on a single processor. Concurrency arises from performing the same operations on\ndifferent pieces of data is is performed using Single Instruction Multiple Data (SIMD) in-\nstructions. SIMD performs multiple independent algebraic operations in one clock cycle.\nIt is achieved by dividing each algebraic operation into multiple simpler ones with sep-\narate hardware in the processor for each of the simple operations. The calculations are\nperformed in a pipeline fashion, a.k.a conveyor belt. For example, an addition operation\ncan have the following components\n1. Find the location of inputs\n2. Copy data to register\n3. Align the exponents; the addition .3e-1+.6e-2 becomes .3e-1+.06e-1\n4. Execute the addition of the mantissas\n5. Normalize the result.\nWhen performed one at a time as in a loop, each addition takes 5 cycles. However,\nwhen pipelined, we can do it in 1 cycle. Modern processes might have more than 20\ncomponents for addition or multiplication operations Eijkhout et al. (2014). GPU com-\nputing takes it further by using a set of threads for the same instruction (on different data\nelements), NVIDIA calls it SIMT (Single Instruction Multiple Threads).\nVectorized operations that rely on SIMD or SIMT replace naive loop implementation\nfor calculating the matrix-matrix multiplication. A vector processor comes with a reper-\ntoire of vector instructions, such as vector add, vector multiply, vector scale, dot product,\nand saxpy. These operations take place in vector registers with input and output handled\nby vector load and vector store instructions. For example, vectorization can speedup vec-\ntor dot product calculations by two orders of magnitude, as shown in code:\nN = int(1e6)\na = np.random.rand(N)\nb = np.random.rand(N)\ntic = time.time()\nc = np.dot(a,b)\ntoc = time.time()\nprint(\"Vec␣dot␣time:␣\" + str((toc-tic)*1000) + \"␣ms\")\nc = 0\ntic = time.time()\nfor i in range(N):\nc+=a[i]*b[i]\n12\ntoc = time.time()\nprint(\"Loop␣dot␣time:␣\" + str((toc-tic)*1000) + \"␣ms\")\nVec dot time:\n1.372 ms\nLoop dot time: 435.639 ms\nWhen calculations are performed on vectors of different dimensions, modern nu-\nmerical libraries, such as Python’s numpy perform those using broadcasting. It involves\n“broadcasting” the smaller array across the larger array so that they have the same shape.\nThe vectorized operation is performed on the broadcasted and the other vector. Broad-\ncasting does not make copies of data and usually leads to efﬁcient algorithm implemen-\ntations. For example, to perform b+z, where b is a scalar and z is an n-vector, the numeric\nlibrary will create a vector of length n b_broadcast = (b,b,...,b) and then will com-\npute (b,b,...,b) + z.\nFurther, matrix operations implemented by a linear algebra library take into account\nthe memory hierarchy and aim at maximizing the use of the fastest cache memory which\nis co-located with the processor on the same board Eijkhout et al. (2014). In summary, a\nmodeler should avoid loops in their model implementations and always look for ways to\nvectorize the code.\nAnother way a modern DL framework speed up calculations is by using quantiza-\ntion TensorFlow (2018), which simply replaces ﬂoating point calculations with 8-bit inte-\ngers calculations. Quantization allows to train larger models (less memory is required to\nstore the model) and faster model evaluations, since cache can be used more efﬁciently.\nIn some case you’ll have a dedicated hardware that can accelerate 8-bit calculations too.\nQuantization also allows to evaluate large scale models on embedded and mobile de-\nvices, and enables what is called edge computing, when data is analyzed locally instead\nof being shipped to a remote server. Edge computing is essential for Internet of Things\n(IoT) and robotics systems\n5\nHardware Architectures\nUsage of efﬁcient hardware architectures is an important ingredient in today’s success of\nDL models. Design and optimization of DL hardware systems is currently an active area\nof research in industry and academia. We currently see an “arms race” among large com-\npanies such as Google and Nvidia and small startups to produce the most economically\nand energy efﬁcient deep learning systems.\nGPU Computing\nIn the last 20 years, the video gaming industry drove forward huge\nadvances in Graphical Processing Unit (GPU), which is a special purpose processor for\ncalculations required for graphics processing.\nSince operations required for graphics\nheavily rely on linear algebra, and GPUs have become widely used for non-graphics pro-\ncessing, speciﬁcally for training deep learning models. GPUs rely on data parallelism,\nwhen the body of a loop is executed for all elements in a vector:\nfor i in range(10000):\na[i] = 2*b[i]\n13\nOur data is divided among multiple processing units available, and each processor exe-\ncutes the same statement a = 2*b on its local data in parallel. In graphics processing usu-\nally the same operation is independently applied to each pixel of an image, thus GPUs are\nstrongly based on data parallelism. The major drawback of GPU computing is the require-\nment to copy data from CPU to GPU memory which incurs a long latency. Throughput\ncomputing, processing large amounts of data at high rates, plays a central role in GPU\narchitectures. High throughput is enabled by a large number of threads and ability to\nswitch fast between them. Modern GPUs would typically have several thousand cores,\ncompare it to the latest Intel i9-family processors that have up to 18 cores. Further, most\nrecent GPUs from NVIDIA would include up to a thousand of so-called tensor cores, that\ncan perform multiply-accumulate operation on a small matrix in one clock cycle.\nDevelopment of GPU code requires skills and knowledge typically not available to\nmodelers. Fortunately, most deep learning modelers do not need to program GPUs di-\nrectly and use software libraries that have implementations of the most widely used op-\nerations, such as matrix-matrix multiplications.\nCurrently, Nvidia dominates the market for GPUs, with the next closest competitor\nbeing AMD. Recently, AMD announced the release of a platform called ROCm to provide\nmore support for deep learning. The status of ROCm for major deep learning libraries\nsuch as PyTorch, TensorFlow, MxNet, and CNTK is still under development.\nLet us demonstrate the speed up provided by using GPU using a code example:\ndtype = torch.FloatTensor\nN = 50000\nx = torch.randn(N,N).type(dtype)\ny = torch.randn(N,N).type(dtype)\nstart = time.time()\nz = x*y\nend = time.time()\nprint(\"CPU␣Time:\",(end - start)*1000)\nif torch.cuda.is_available():\nstart = time.time()\nx = x.cuda()\ny = y.cuda()\nend = time.time()\nprint(\"Copy␣to␣GPU␣Time:\",(end - start)*1000)\nstart = time.time()\na = x*y\nend = time.time()\nprint(\"GPU␣Time:\",(end - start)*1000)\nstart = time.time()\na = a.cpu()\nend = time.time()\nprint(\"Copy␣from␣GPU␣Time:\",(end - start)*1000)\nCPU Time:\n11.6\nCopy to GPU Time: 28.9\nGPU Time:\n0.24\nCopy from GPU Time: 33.2\n14\nThe matrix multiplication operation itself is performed 48 times faster on GPU (11.6\nms vs 0.24 ms). However, copying data from main memory to GPU memory and back\nadds another 62.1 ms (28.9 + 33.2). Thus, to efﬁciently use GPU architectures, it is neces-\nsary to minimize amount of data transferred between main and GPU memories\nIntel Xeon Phi\nRecently, in response to the dominance of GPU processors in scientiﬁc\ncomputing and machine learning, Intel has released a co-processor Intel Xeon Phi. As a\nGPU, Xeon Phi provides a large number of cores and has a considerable latency in starting\nup. The main difference is that Xeon Phi has general purpose cores, while a set of GPU\ninstructions is limited. An ordinary C code can be executed on a Xeon Phi processor.\nHowever, the ease of use of GPU libraries for linear algebra operations make those the\ndefault architecture choice.\nDL Speciﬁc Architectures\nCompanies such as Google or Facebook use deep learning\nmodels at extreme scales. Recent computational demand for training and deploying deep\nlearning models at those scales fueled development of custom hardware architectures for\ndeep learning.\nThe Intel’s Nervana NNP team is focusing on developing a co-processor with fast\nand reliable bi-directional data transfer. They use a proprietary numeric format called\nFlexpoint, to increase the throughput. Further, the power consumption is reduced by\nshrinking circuit size.\nGoogle’s Tensor Processing Units (TPU) Sato et al. (2017) has two processors, each\nhaving 128x128 matrix multiply units (MXU). Each MXU can perform multiple matrix op-\nerations in one clock cycle. Google uses TPUs for all of its online services such as Search,\nStreet View, Google Photos, and Google Translate. TPU uses Complex Instruction Set\nComputer (CISC) design style which focuses on implementing instructions for high-level\ncomplex tasks such as matrix-matrix multiplication with in one clock cycle. In contrast, a\ntypical general purpose CPU follows a Reduced Instruction Set Computer (RISC) design\nand implements a large number of small primitive instructions (load, multiply,...) and\nassumes every operation can be represented as a combination of those simple primitives.\nThere are several other established and startup companies working on developing\ncustom hardware architectures for deep learning computing. Most approaches rely on\nusage of Field-programmable gate array (FPGA) designs Brown et al. (2012). For exam-\nple, Microsoft’s Brainwave Microsoft (2017) hardware, which used FPGA is claimed to\naddress the inﬂexibility of other computing platforms by providing a design that scales\nacross range of data types. Other processor’s inﬂexibility comes from the fact that a set of\nspeciﬁc instructions is available at any given architecture.\n6\nSoftware Frameworks\nPython is by far the most commonly used language for DL. There are a number of deep\nlearning libraries available, with almost every major tech company backing a different\nlibrary. Widely used deep learning libraries include TensorFlow (Google), PyTorch and\n15\nCaffe2 (Facebook), MxNet (Amazon), CNTK (Microsoft). All of those frameworks have\nPython support. For R users Keras library (https://keras.rstudio.com) provides a high-\nlevel interface to TensorFlow, and is the most robust option at this point.\nOne of the major differences between different libraries is the use of dynamic vs. static\ngraph computations. Some libraries, such as MxNet and TensorFlow, allow for both. In\nstatic setting a model is fully speciﬁed before the training process. In dynamic graphs,\nstructure is deﬁned “in-thr-ﬂy” as code gets executed, which is the way our traditional\nprograms are executed. Static graphs provide the opportunity to pre-process the model\nand to optimize the computations and thus are preferred for large scale models used\nin production. Dynamic settings provide more ﬂexibility and is typically used during\nthe research and development phase of the model development. Furthermore, dynamic\nmodels are easier to debug and easier to learn for those who are familiar with traditional\nobject-oriented programming.\nPyTorch\nPyTorch is native Python library rather than a binding to library written in an-\nother language. Thus, it provides an intuitive and friendly interface for Python users to\nbuild and train deep learning models on CPU and GPU hardware. Pytorch is widely used\nfor research as it provides a way to build models dynamically using native Python func-\ntions. On a ﬂexibility-code simplicity scale Pytorch is an attractive option for a researcher\nwho is using Python.\nTensorFlow\nTensorFlow is an open source framework written in C++ with interfaces\navailable for many other languages such as Python. Although TensorFlow assumes a\nsteeper learning curve when compared to other DL frameworks, its performance on large\nscale problems across different hardware architectures and support for many popular\nmachine learning algorithms made it a popular choice among practitioners.\n6.1\nCompiler Based Approach\nTraditional DL systems consists of high-level interface libraries, such as PyTorch or Ten-\nsorFlow which perform computationally intensive operations by calling functions from\nlibraries optimized for a speciﬁc hardware as shown in Figure 3. Currently, hardware\nmanufactures have to develop a software stack (a set of libraries) speciﬁc to their proces-\nsors. Nvidia developed CUDA libraries, Intel has MKL library, Google developed TPU\nlibrary. The reason why Nvidia and not AMD is the GPU of choice for deep learning\nmodels is because of Nvidia’s greater level of software support for linear algebra and\nother DL speciﬁc computations.\nHowever, usage of vendor-developed libraries can be limiting.\nSome expressions\nmight require a complex combination of function calls or might be impossible to write\nusing the functions provided by the vendor library. For example, vendor library might\nnot support sparse matrices. A different approach has recently emerged that relies on\ncompiling linear algebra expressions written in special language to a code which is opti-\nmized for a given hardware architecture. This approach solves two problems, it allows to\n16\nProgramming API (PyTorch, TensorFlow…)\nTensor Compiler\nHardware-specific library for \nlinear algebra (MKL, CUDA,…)\nHardware\nA: Traditional\nB: Emerging\nFigure 3: Deep Learning System Hierarchy\nperform computations that are not implemented in hardware speciﬁc-library, and facili-\ntates support for wider a range of architectures, including mobile ones. Recent examples\ninclude tensor comprehensions by Facebook Vasilache et al. (2018), TVM from U Wash-\nington Chen et al. (2018), TACO Kjolstad et al. (2017) and Google’s XLA (Accelerated\nLinear Algebra) compiler.\nCode below demonstrates impact of Numba, which compiles functions written directly\nin Python. Numba uses annotations to compile Python code to native machine instructions.\nWhen original python code is mostly performing linear algebra operations, the resulting\nnative machine instructions will lead to performance similar to C, C++ and Fortran. Numba\ngenerates optimized machine code using the LLVM compiler infrastructure. Numba sup-\nports compilation of Python to run on either CPU or GPU hardware and is designed to\nintegrate with the Python scientiﬁc software libraries.\nfrom numba import jit, double\nimport math\nimport numpy as np\nimport time\n@jit(nopython = True)\ndef mydot(a,b,c):\nfor i in range(N):\nc+=a[i]*b[i]\nN = int(1e6); a = np.random.rand(N); b = np.random.rand(N)\nc = 0\ntic = time.time()\nmydot(a,b,c)\ntoc = time.time()\nprint(\"Numba␣dot␣time:␣\" + str((toc-tic)*1000) + \"␣ms\")\nc = 0\ntic = time.time()\nfor i in range(N):\n17\nc+=a[i]*b[i]\ntoc = time.time()\nprint(\"Loop␣dot␣time:␣\" + str((toc-tic)*1000) + \"␣ms\")\nNumba dot time:\n138.628959656 ms\nLoop dot time:\n630.362033844 ms\n7\nConcluding Remark\nThe goal of our paper is to provide an overview of computational aspects of DL. To do\nthis, we have discussed the core linear algebra, computational routines required for train-\ning, and inference using the DL models as well as the importance of hardware architec-\ntures for efﬁcient model training. A brief introduction SGD optimization and its variants,\nthat are typically used to ﬁnd parameters (weights and biases) of a deep learning model\nis also provided. For further reading, see Bottou et al. (2018).\nAlthough, DL models have been almost exclusively used for problems of image anal-\nysis and natural language processing, more traditional data sets, which arise in ﬁnance,\nscience and engineering, such as spatial Polson and Sokolov (2017a); Dixon et al. (2017)\nand temporal Polson and Sokolov (2018a) data can be efﬁciently analyzed using deep\nlearning. There are a number of areas of future research for Statisticians. In particular, un-\ncertainty quantiﬁcation and model selection such as architecture design. To algorithmic\nimprovements and Bayesian deep learning. We hope this review will make DL models\naccessible for statisticians.\nReferences\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization.\nJournal of Machine Learning Research, 13(Feb):281–305, 2012.\nV. G. Boltyanskii, R. V. Gamkrelidze, and Pontryagin.\nTheory of optimal processes i:\nMaximum principle. News of Akad. Nauk SSSR. Mathematics Series, 24:3–42, 1960.\nL´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale\nmachine learning. SIAM Review, 60(2):223–311, 2018.\nStephen D Brown, Robert J Francis, Jonathan Rose, and Zvonko G Vranesic.\nField-\nprogrammable gate arrays, volume 180. Springer Science & Business Media, 2012.\nArthur E Bryson. A gradient method for optimizing multi-stage allocation processes. In\nProc. Harvard Univ. Symposium on digital computers and their applications, volume 72, 1961.\nArthur Earl Bryson. Applied optimal control: optimization, estimation and control. Blaisdell\nPub. Co, 1969.\nMiguel A Carreira-Perpin´an and Weiran Wang. Distributed optimization of deeply nested\nsystems. In AISTATS, pages 10–19, 2014.\n18\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Haichen Shen, Eddie Q. Yan, Leyuan Wang,\nYuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: end-to-end\noptimization stack for deep learning. CoRR, abs/1802.04799, 2018. URL http://arxiv.\norg/abs/1802.04799.\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew\nSenior, Paul Tucker, Ke Yang, Quoc V. Le, and others. Large scale distributed deep\nnetworks. In Advances in Neural Information Processing Systems, pages 1223–1231, 2012.\nMatthew F Dixon, Nicholas G Polson, and Vadim O Sokolov. Deep learning for spatio-\ntemporal modeling: Dynamic trafﬁc ﬂows and high frequency trading. arXiv preprint\narXiv:1705.09851, 2017.\nStuart Dreyfus. The numerical solution of variational problems. Journal of Mathematical\nAnalysis and Applications, 5(1):30–45, 1962.\nStuart Dreyfus. The computational solution of optimal control problems with time lag.\nIEEE Transactions on Automatic Control, 18(4):383–385, 1973.\nVictor Eijkhout, Edmond Chow, and van de Geijn Robert. Introduction to High Performance\nScientiﬁc Computing. 2014.\nAndre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau,\nand Sebastian Thrun. Dermatologist-level classiﬁcation of skin cancer with deep neural\nnetworks. Nature, 542(7639):115–118, 2017.\nGuanhao Feng, Jingyu He, and Nicholas G Polson. Deep learning for predicting asset\nreturns. arXiv preprint arXiv:1804.09314, 2018a.\nGuanhao Feng, Nicholas G Polson, and Jianeng Xu. Deep factor alpha. arXiv preprint\narXiv:1805.01104, 2018b.\nDaniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and\nD Sculley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages\n1487–1495. ACM, 2017.\nIan Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, vol-\nume 1. MIT press Cambridge, 2016.\nAndreas Griewank, Kshitij Kulshreshtha, and Andrea Walther. On the numerical stability\nof algorithmic differentiation. Computing, 94(2-4):125–149, 2012.\nAndr´as Gy¨orgy and Levente Kocsis. Efﬁcient multi-start strategies for local search algo-\nrithms. Journal of Artiﬁcial Intelligence Research, 41:407–444, 2011.\nKim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhul-\ngakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee,\nJason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, and Xiaodong Wang.\nApplied machine learning at facebook: A datacenter infrastructure perspective. 2017.\n19\nJB Heaton, NG Polson, and Jan Hendrik Witte. Deep learning for ﬁnance: deep portfolios.\nApplied Stochastic Models in Business and Industry, 33(1):3–12, 2017.\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun-hsuan Sung, Laszlo Lukacs, Ruiqi\nGuo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. Efﬁcient natural language re-\nsponse suggestion for smart reply. arXiv preprint arXiv:1705.00652, 2017.\nMark Ibrahim, Melissa Louie, Ceena Modarres, and John Paisley.\nGlobal explana-\ntions of neural networks:\nMapping the landscape of predictions.\narXiv preprint\narXiv:1902.02384, 2019.\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue,\nAli Razavi, Oriol Vinyals, Tim Green, Iain Dunning, and Karen Simonyan. Population\nbased training of neural networks. arXiv preprint arXiv:1711.09846, 2017.\nArtur Kadurin, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin Van-\nhaelen, Kuzma Khrabrov, and Alex Zhavoronkov. The cornucopia of meaningful leads:\nApplying deep adversarial autoencoders for new molecule development in oncology.\nOncotarget, 8(7):10883, 2017.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\nFredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe.\nThe tensor algebra compiler. Proc. ACM Program. Lang., 1(OOPSLA):77:1–77:29, October\n2017.\nISSN 2475-1421.\ndoi: 10.1145/3133901.\nURL http://doi.acm.org/10.1145/\n3133901.\nAndrei Nikolaevich Kolmogorov. On the representation of continuous functions of sev-\neral variables by superpositions of continuous functions of a smaller number of vari-\nables. Dokl. Akad. Nauk SSSR, 108:179–182, 1956. [ English translation: American Mathe-\nmatical Society Translation, 17 (1961), pp. 369-373].\nAndrei Nikolaevich Kolmogorov. On the representation of continuous functions of many\nvariables by superposition of continuous functions of one variable and addition. Dokl.\nAkad. Nauk SSSR, 114(5):953–956, 1957.\n[English translation: American Mathematical\nSociety Translation, 28 (2) (1963), pp. 55–59].\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436,\n2015.\nSeppo Linnainmaa. The representation of the cumulative rounding error of an algorithm\nas a taylor expansion of the local rounding errors. Master’s Thesis (in Finnish), Univ.\nHelsinki, pages 6–7, 1970.\nMicrosoft.\nMicrosoft unveils Project Brainwave for real-time AI.\nhttps://www.\nmicrosoft.com/en-us/research/blog/microsoft-unveils-project-brainwave/,\n2017.\n20\nJonas Mockus. Bayesian approach to global optimization: theory and applications, volume 37.\nSpringer Science & Business Media, 2012.\nCeena Modarres, Mark Ibrahim, Melissa Louie, and John Paisley. Towards explainable\ndeep learning for credit lending: A case study. arXiv preprint arXiv:1811.06471, 2018.\nYurii Nesterov. A method of solving a convex programming problem with convergence\nrate O (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376, 1983.\nYurii Nesterov.\nIntroductory lectures on convex optimization: A basic course, volume 87.\nSpringer Science & Business Media, 2013.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A gen-\nerative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.\nGM Ostrovskii, Yu M Volin, and WW Borisov. Uber die berechnung von ableitungen.\nWissenschaftliche Zeitschrift der Technischen Hochschule f ur Chemie, Leuna-Merseburg, 13\n(4):382–384, 1971.\nMichael Polson and Vadim Sokolov. Deep learning for energy markets. arXiv preprint\narXiv:1808.05527, 2018a.\nNicholas Polson and Vadim Sokolov. Deep learning for short-term trafﬁc ﬂow prediction.\nTransportation Research Part C: Emerging Technologies, 79:1–17, 2017a.\nNicholas G. Polson and Vadim Sokolov. Deep Learning: A Bayesian Perspective. Bayesian\nAnalysis, 12(4):1275–1304, 2017b. ISSN 1936-0975, 1931-6690. doi: 10.1214/17-BA1082.\nNicholas G Polson and Vadim O Sokolov. Deep learning. arXiv preprint arXiv:1807.07987,\n2018b.\nPranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony\nDuan, Daisy Ding, Aarti Bagul, Curtis Langlotz, and Katie Shpanskaya.\nChexnet:\nRadiologist-level pneumonia detection on chest X-rays with deep learning.\narXiv\npreprint arXiv:1711.05225, 2017.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should I trust you?: Ex-\nplaining the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD inter-\nnational conference on knowledge discovery and data mining, pages 1135–1144. ACM, 2016.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations\nby back-propagating errors. nature, 323(6088):533, 1986.\nAshish Sabharwal, Horst Samulowitz, and Gerald Tesauro. Selecting near-optimal learn-\ners via incremental data allocation. In AAAI, pages 2007–2015, 2016.\nKaz Sato, Cliff Young, and David Patterson.\nAn in-depth look at googles ﬁrst tensor\nprocessing unit (tpu). Google Cloud Big Data and Machine Learning Blog, 12, 2017.\n21\nJ¨urgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks,\n61:85–117, 2015.\nAmar Shah and Zoubin Ghahramani. Parallel predictive entropy search for batch global\noptimization of expensive objective functions. In Advances in Neural Information Process-\ning Systems, pages 3330–3338, 2015.\nChristopher J Shallue and Andrew Vanderburg. Identifying exoplanets with deep learn-\ning: A ﬁve planet resonant chain around kepler-80 and an eighth planet around kepler-\n90. arXiv preprint arXiv:1712.05044, 2017.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features\nthrough propagating activation differences. In Proceedings of the 34th International Con-\nference on Machine Learning-Volume 70, pages 3145–3153, 2017.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of\nmachine learning algorithms. In Advances in neural information processing systems, pages\n2951–2959, 2012.\nVadim Sokolov. Discussion of deep learning for ﬁnance: deep portfolios. Applied Stochastic\nModels in Business and Industry, 33(1):16–18, 2017.\nNiranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger.\nGaussian\nprocess optimization in the bandit setting: No regret and experimental design. arXiv\npreprint arXiv:0912.3995, 2009.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep net-\nworks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,\npages 3319–3328. JMLR. org, 2017.\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance\nof initialization and momentum in deep learning. In International conference on machine\nlearning, pages 1139–1147, 2013.\nTensorFlow.\nFixed Point Quantization.\nhttps://www.tensorflow.org/performance/\nquantization, 2018. Accessed on 2010-09-10.\nNicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal, Zachary De-\nVito, William S Moses, Sven Verdoolaege, Andrew Adams, and Albert Cohen. Ten-\nsor comprehensions: Framework-agnostic high-performance machine learning abstrac-\ntions. arXiv preprint arXiv:1802.04730, 2018.\nYuexi Wang, Nicholas G Polson, and Vadim O Sokolov. Scalable data augmentation for\ndeep learning. arXiv preprint arXiv:1903.09668, 2019.\nPaul Werbos. Beyond regression:” new tools for prediction and analysis in the behavioral\nsciences. Ph. D. dissertation, Harvard University, 1974.\n22\nPaul J Werbos.\nApplications of advances in nonlinear sensitivity analysis.\nIn System\nmodeling and optimization, pages 762–770. Springer, 1982.\nZbigniew Wojna, Alex Gorban, Dar-Shyang Lee, Kevin Murphy, Qian Yu, Yeqing Li, and\nJulian Ibarz.\nAttention-based extraction of structured information from street view\nimagery. arXiv preprint arXiv:1704.03549, 2017.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, and Klaus Macherey. Google’s neural\nmachine translation system: Bridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144, 2016.\nMatthew D Zeiler.\nAdadelta:\nan adaptive learning rate method.\narXiv preprint\narXiv:1212.5701, 2012.\n23\n",
  "categories": [
    "cs.LG",
    "stat.CO",
    "stat.ML"
  ],
  "published": "2018-08-26",
  "updated": "2019-08-28"
}