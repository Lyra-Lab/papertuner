{
  "id": "http://arxiv.org/abs/1808.09111v1",
  "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections",
  "authors": [
    "Junxian He",
    "Graham Neubig",
    "Taylor Berg-Kirkpatrick"
  ],
  "abstract": "Unsupervised learning of syntactic structure is typically performed using\ngenerative models with discrete latent variables and multinomial parameters. In\nmost cases, these models have not leveraged continuous word representations. In\nthis work, we propose a novel generative model that jointly learns discrete\nsyntactic structure and continuous word representations in an unsupervised\nfashion by cascading an invertible neural network with a structured generative\nprior. We show that the invertibility condition allows for efficient exact\ninference and marginal likelihood computation in our model so long as the prior\nis well-behaved. In experiments we instantiate our approach with both Markov\nand tree-structured priors, evaluating on two tasks: part-of-speech (POS)\ninduction, and unsupervised dependency parsing without gold POS annotation. On\nthe Penn Treebank, our Markov-structured model surpasses state-of-the-art\nresults on POS induction. Similarly, we find that our tree-structured model\nachieves state-of-the-art performance on unsupervised dependency parsing for\nthe difficult training condition where neither gold POS annotation nor\npunctuation-based constraints are available.",
  "text": "Unsupervised Learning of Syntactic Structure\nwith Invertible Neural Projections\nJunxian He\nGraham Neubig\nTaylor Berg-Kirkpatrick\nLanguage Technologies Institute\nSchool of Computer Science\nCarnegie Mellon University\n{junxianh, gneubig, tberg}@cs.cmu.edu\nAbstract\nUnsupervised learning of syntactic structure is\ntypically performed using generative models\nwith discrete latent variables and multinomial\nparameters. In most cases, these models have\nnot leveraged continuous word representa-\ntions. In this work, we propose a novel gener-\native model that jointly learns discrete syntac-\ntic structure and continuous word representa-\ntions in an unsupervised fashion by cascading\nan invertible neural network with a structured\ngenerative prior. We show that the invertibility\ncondition allows for efﬁcient exact inference\nand marginal likelihood computation in our\nmodel so long as the prior is well-behaved. In\nexperiments we instantiate our approach with\nboth Markov and tree-structured priors, eval-\nuating on two tasks: part-of-speech (POS) in-\nduction, and unsupervised dependency parsing\nwithout gold POS annotation.\nOn the Penn\nTreebank, our Markov-structured model sur-\npasses state-of-the-art results on POS induc-\ntion. Similarly, we ﬁnd that our tree-structured\nmodel achieves state-of-the-art performance\non unsupervised dependency parsing for the\ndifﬁcult training condition where neither gold\nPOS annotation nor punctuation-based con-\nstraints are available.1\n1\nIntroduction\nData annotation is a major bottleneck for the appli-\ncation of supervised learning approaches to many\nproblems. As a result, unsupervised methods that\nlearn directly from unlabeled data are increasingly\nimportant. For tasks related to unsupervised syn-\ntactic analysis, discrete generative models have\ndominated in recent years – for example, for both\npart-of-speech (POS) induction\n(Blunsom and\nCohn, 2011; Stratos et al., 2016) and unsuper-\nvised dependency parsing (Klein and Manning,\n1Code\nis\navailable\nat\nhttps://github.com/jxhe/struct-\nlearning-with-ﬂow.\n(a) Traditional pre-trained\nskip-gram embeddings\n(b) Learned latent embedd-\nings from our approach\nFigure 1: Visualization (t-SNE) of skip-gram embeddings\n(trained on one billion words with context window size equal\nto 1) and latent embeddings learned by our approach with a\nMarkov-structured prior. Each node represents a word and is\ncolored according to the most likely gold POS tag from the\nPenn Treebank (best seen in color).\n2004; Cohen and Smith, 2009; Pate and Johnson,\n2016). While similar models have had success on\na range of unsupervised tasks, they have mostly ig-\nnored the apparent utility of continuous word rep-\nresentations evident from supervised NLP appli-\ncations (He et al., 2017; Peters et al., 2018). In\nthis work, we focus on leveraging and explicitly\nrepresenting continuous word embeddings within\nunsupervised models of syntactic structure.\nPre-trained word embeddings from massive un-\nlabeled corpora offer a compact way of inject-\ning a prior notion of word similarity into mod-\nels that would otherwise treat words as discrete,\nisolated categories. However, the speciﬁc prop-\nerties of language captured by any particular em-\nbedding scheme can be difﬁcult to control, and,\nfurther, may not be ideally suited to the task at\nhand.\nFor example, pre-trained skip-gram em-\nbeddings (Mikolov et al., 2013) with small con-\ntext window size are found to capture the syntac-\ntic properties of language well (Bansal et al., 2014;\nLin et al., 2015). However, if our goal is to sepa-\nrate syntactic categories, this embedding space is\nnot ideal – POS categories correspond to overlap-\narXiv:1808.09111v1  [cs.CL]  28 Aug 2018\nei ⇠N(µzi, ⌃zi)\nxi = fφ(ei)\nzi ⇠Syntax Model\nxi ⇠\nfφ(ei)\nPoint mass at\nNeural \nProjector\nz1\nz2\nz3\nx3\nx2\nx1\ne1\ne2\ne3\nfφ(e)\nDMV prior\nMarkov prior\nz1\nz2\nz3\nztree\nz1\nz2\nz3\nSyntax  \nModel\nFigure 2: Depiction of proposed generative model. The syntax model is composed of discrete random variables, zi. Each ei\nis a latent continuous embeddings sampled from Gaussian distribution conditioned on zi, while xi is the observed embedding,\ndeterministically derived from ei. The left portion depicts how the neural projector maps the simple Gaussian to a more\ncomplex distribution in the output space. The right portion depicts two instantiations of the syntax model in our approach: one\nis Markov-structured and the other is DMV-structured. For DMV, ztree is the latent dependency tree structure.\nping interspersed regions in the embedding space,\nevident in Figure 1(a).\nIn our approach, we propose to learn a new\nlatent embedding space as a projection of pre-\ntrained embeddings (depicted in Figure 1(b)),\nwhile jointly learning latent syntactic structure –\nfor example, POS categories or syntactic depen-\ndencies. To this end, we introduce a new gener-\native model (shown in Figure 2) that ﬁrst gener-\nates a latent syntactic representation (e.g. a de-\npendency parse) from a discrete structured prior\n(which we also call the “syntax model”), then,\nconditioned on this representation, generates a se-\nquence of latent embedding random variables cor-\nresponding to each word, and ﬁnally produces the\nobserved (pre-trained) word embeddings by pro-\njecting these latent vectors through a parameter-\nized non-linear function. The latent embeddings\ncan be jointly learned with the structured syntax\nmodel in a completely unsupervised fashion.\nBy choosing an invertible neural network as\nour non-linear projector, and then parameterizing\nour model in terms of the projection’s inverse,\nwe are able to derive tractable exact inference\nand marginal likelihood computation procedures\nso long as inference is tractable in the underlying\nsyntax model. In §3.1 we show that this derivation\ncorresponds to an alternate view of our approach\nwhereby we jointly learn a mapping of observed\nword embeddings to a new embedding space that\nis more suitable for the syntax model, but include\nan additional Jacobian regularization term to pre-\nvent information loss.\nRecent work has sought to take advantage\nof word embeddings in unsupervised generative\nmodels with alternate approaches (Lin et al., 2015;\nTran et al., 2016; Jiang et al., 2016; Han et al.,\n2017). Lin et al. (2015) build an HMM with Gaus-\nsian emissions on observed word embeddings, but\nthey do not attempt to learn new embeddings. Tran\net al. (2016), Jiang et al. (2016), and Han et al.\n(2017) extend HMM or dependency model with\nvalence (DMV) (Klein and Manning, 2004) with\nmultinomials that use word (or tag) embeddings\nin their parameterization. However, they do not\nrepresent the embeddings as latent variables.\nIn experiments, we instantiate our approach us-\ning both a Markov-structured syntax model and\na tree-structured syntax model – speciﬁcally, the\nDMV. We evaluate on two tasks: part-of-speech\n(POS) induction and unsupervised dependency\nparsing without gold POS tags. Experimental re-\nsults on the Penn Treebank (Marcus et al., 1993)\ndemonstrate that our approach improves the ba-\nsic HMM and DMV by a large margin, lead-\ning to the state-of-the-art results on POS induc-\ntion, and state-of-the-art results on unsupervised\ndependency parsing in the difﬁcult training sce-\nnario where neither gold POS annotation nor\npunctuation-based constraints are available.\n2\nModel\nAs an illustrative example, we ﬁrst present a base-\nline model for Markov syntactic structure (POS in-\nduction) that treats a sequence of pre-trained word\nembeddings as observations. Then, we propose\nour novel approach, again using Markov structure,\nthat introduces latent word embedding variables\nand a neural projector. Lastly, we extend our ap-\nproach to more general syntactic structures.\n2.1\nExample: Gaussian HMM\nWe start by describing the Gaussian hidden\nMarkov model introduced by Lin et al. (2015),\nwhich is a locally normalized model with multi-\nnomial transitions and Gaussian emissions. Given\na sentence of length ℓ, we denote the latent POS\ntags as z = {zi}ℓ\ni=1, observed (pre-trained) word\nembeddings as x = {xi}ℓ\ni=1, transition parame-\nters as θ, and Gaussian emission parameters as η.\nThe joint distribution of data and latent variables\nfactors as:\np(z, x; θ, η) =\nYℓ\ni=1 pθ(zi|zi−1)pη(xi|zi),\n(1)\nwhere pθ(zi|zi−1) is the multinomial transition\nprobability and pη(xi|zi) is the multivariate Gaus-\nsian emission probability.\nWhile the observed word embeddings do inform\nthis model with a notion of word similarity – lack-\ning in the basic multinomial HMM – the Gaussian\nemissions may not be sufﬁciently ﬂexible to sepa-\nrate some syntactic categories in the complex pre-\ntrained embedding space – for example the skip-\ngram embedding space as visualized in Figure 1(a)\nwhere different POS categories overlap. Next we\nintroduce a new approach that adds ﬂexibility to\nthe emission distribution by incorporating new la-\ntent embedding variables.\n2.2\nMarkov Structure with Neural Projector\nTo ﬂexibly model observed embeddings and yield\na new representation space that is more suitable\nfor the syntax model, we propose to cascade a neu-\nral network as a projection function, deterministi-\ncally transforming the simple space deﬁned by the\nGaussian HMM to the observed embedding space.\nWe denote the latent embedding of the ith word in\na sentence as ei ∈Rde, and the neural projection\nfunction as f, parameterized by φ. In the case of\nsequential Markov structure, our new model cor-\nresponds to the following generative process:\nFor each time step i = 1, 2, · · · , ℓ,\n• Draw the latent state zi ∼pθ(zi|zi−1)\n• Draw the latent embedding ei ∼N(µzi, Σzi)\n• Deterministically produce embedding\nxi = fφ(ei)\nThe graphical model is depicted in Figure 2. The\ndeterministic projection can also be viewed as\nsampling each observation from a point mass at\nfφ(ei). The joint distribution of our model is:\np(z, e, x; θ, η, φ)\n=\nYℓ\ni=1[pθ(zi|zi−1)pη(ei|zi)pφ(xi|ei)],\n(2)\nwhere pη(·|zi) is a conditional Gaussian distribu-\ntion, and pφ(xi|ei) is the Dirac delta function cen-\ntered at fφ(ei):\npφ(xi|ei) = δ(xi−fφ(ei)) =\n(\n∞xi = fφ(ei)\n0\notherwise\n(3)\n2.3\nGeneral Structure with Neural Projector\nOur approach can be applied to a broad family of\nstructured syntax models. We denote latent em-\nbedding variables as e = {ei}ℓ\ni=1, discrete latent\nvariables in the syntax model as z = {zk}K\nk=1\n(K ⩾ℓ), where z1, z2, . . . , zℓare conditioned to\ngenerate e1, e2, . . . , eℓ. The joint probability of\nour model factors as:\np(z, e, x; θ, η, φ) =\nYℓ\ni=1\n\u0002\npη(ei|zi)pφ(xi|ei)\n\u0003\n· psyntax(z; θ),\n(4)\nwhere psyntax(z; θ) represents the probability of\nthe syntax model, and can encode any syntactic\nstructure – though, its factorization structure will\ndetermine whether inference is tractable in our full\nmodel. As shown in Figure 2, we focus on two\nsyntax models for syntactic analysis in this paper.\nThe ﬁrst is Markov-structured, which we use for\nPOS induction, and the second is DMV-structured,\nwhich we use to learn dependency parses without\nsupervision.\nThe marginal data likelihood of our model is:\np(x) =\nX\nz\n\u0010\npsyntax(z; θ)\n·\nYℓ\ni=1\n\u0002 Z\nei\npη(ei|zi)pφ(xi|ei)dei\n|\n{z\n}\np(xi|zi)\n\u0003\u0011\n.\n(5)\nWhile the discrete variables z can be marginal-\nized out with dynamic program in many cases, it\nis generally intractable to marginalize out the la-\ntent continuous variables, ei, for an arbitrary pro-\njection f in Eq. (5), which means inference and\nlearning may be difﬁcult. In §3, we address this\nissue by constraining f to be invertible, and show\nthat this constraint enables tractable exact infer-\nence and marginal likelihood computation.\n3\nLearning & Inference\nIn this section, we introduce an invertibility con-\ndition for our neural projector to tackle the op-\ntimization challenge.\nSpeciﬁcally, we constrain\nour neural projector with two requirements: (1)\ndim(x) = dim(e) and (2) f−1\nφ\nexists.\nInvert-\nible transformations have been explored before\nin independent components analysis (Hyv¨arinen\net al., 2004), gaussianization (Chen and Gopinath,\n2001), and deep density models (Dinh et al., 2014,\n2016; Kingma and Dhariwal, 2018), for unstruc-\ntured data. Here, we generalize this style of ap-\nproach to structured learning, and augment it with\ndiscrete latent variables (zi). Under the invertibil-\nity condition, we derive a learning algorithm and\ngive another view of our approach revealed by the\nobjective function. Then, we present the architec-\nture of a neural projector we use in experiments: a\nvolume-preserving invertible neural network pro-\nposed by Dinh et al. (2014) for independent com-\nponents estimation.\n3.1\nLearning with Invertibility\nFor ease of exposition, we explain the learning\nalgorithm in terms of Markov structure without\nloss of generality. As shown in Eq. (5), the op-\ntimization challenge in our approach comes from\nthe intractability of the marginalized emission fac-\ntor p(xi|zi).\nIf we can marginalize out ei and\ncompute p(xi|zi), then the posterior and marginal\nlikelihood of our Markov-structured model can be\ncomputed with the forward-backward algorithm.\nWe can apply Eq. (3) and obtain :\np(xi|zi; η, φ) =\nZ\nei\npη(ei|zi)δ(xi −fφ(ei))dei.\nBy using the change of variable rule to the integra-\ntion, which allows the integration variable ei to be\nreplaced by x′\ni = fφ(ei), the marginal emission\nfactor can be computed in closed-form when the\ninvertibility condition is satisﬁed:\np(xi|zi; η, φ)\n=\nZ\nx′\ni\npη(f−1\nφ (x′\ni)|zi)δ(xi −x′\ni)\n\f\f\fdet\n∂f−1\nφ\n∂x′\ni\n\f\f\fdx′\ni\n= pη(f−1\nφ (xi)|zi)\n\f\f\fdet\n∂f−1\nφ\n∂xi\n\f\f\f,\n(6)\nwhere pη(·|z) is a conditional Gaussian distribu-\ntion,\n∂f−1\nφ\n∂xi is the Jacobian matrix of function f−1\nφ\nat xi, and\n\f\fdet\n∂f−1\nφ\n∂xi\n\f\f represents the absolute value\nof its determinant. This Jacobian term is nonzero\nand differentiable if and only if f−1\nφ\nexists.\nEq. (6) shows that we can directly calculate the\nmarginal emission distribution p(xi|zi). Denote\nthe marginal data likelihood of Gaussian HMM as\npHMM(x), then the log marginal data likelihood of\nour model can be directly written as:\nlog p(x) = log pHMM(f−1\nφ (x))\n+\nXℓ\ni=1 log\n\f\f\fdet\n∂f−1\nφ\n∂xi\n\f\f\f,\n(7)\nwhere f−1\nφ (x) represents the new sequence of em-\nbeddings after applying f−1\nφ\nto each xi. Eq. (7)\nshows that the training objective of our model is\nsimply the Gaussian HMM log likelihood with an\nadditional Jacobian regularization term. From this\nview, our approach can be seen as equivalent to\nreversely projecting the data through f−1\nφ\nto an-\nother manifold e that is directly modeled by the\nGaussian HMM, with a regularization term. In-\ntuitively, we optimize the reverse projection f−1\nφ\nto modify the e space, making it more appropri-\nate for the syntax model. The Jacobian regular-\nization term accounts for the volume expansion or\ncontraction behavior of the projection. Maximiz-\ning it can be thought of as preventing information\nloss. In the extreme case, the Jacobian determi-\nnant is equal to zero, which means the projection\nis non-invertible and thus information is being lost\nthrough the projection.\nSuch “information pre-\nserving” regularization is crucial during optimiza-\ntion, otherwise the trivial solution of always pro-\njecting data to the same single point to maximize\nlikelihood is viable.2\nMore generally, for an arbitrary syntax model\nthe data likelihood of our approach is:\np(x) =\nX\nz\n\u0010\npsyntax(z)\n·\nYℓ\ni=1 pη(f−1\nφ (xi)|zi)\n\f\f\fdet\n∂f−1\nφ\n∂xi\n\f\f\f\n\u0011\n.\n(8)\nIf the syntax model itself allows for tractable in-\nference and marginal likelihood computation, the\nsame dynamic program can be used to marginal-\nize out z. Therefore, our joint model inherits the\ntractability of the underlying syntax model.\n2For example, all ei could learn to be zero vectors, lead-\ning to the trivial solution of learning zero mean and zero vari-\nance Gaussian emissions achieving inﬁnite data likelihood.\nf\nxi\n· · ·\nei\n· · ·\n+\n=\n+\n=\n=\n+\n⇥\nf −1\nφ (xi)\nInverse \nProjection\ng\ng\nei\nxi\nxi,l\nxi,r\nh(1)\ni,l\nh(1)\ni,r\nh(2)\ni,r\nh(2)\ni,l\nei,l\nei,r\nFigure 3: Depiction of the architecture of the inverse pro-\njection f −1\nφ\nthat composes multiple volume-preserving cou-\npling layers, with which we parameterize our model.\nOn\nthe right, we schematically depict how the inverse projection\ntransforms the observed word embedding xi to a point ei in\na new embedding space.\n3.2\nInvertible Volume-Preserving Neural Net\nFor the projection we can use an arbitrary invert-\nible function, and given the representational power\nof neural networks they seem a natural choice.\nHowever, calculating the inverse and Jacobian of\nan arbitrary neural network can be difﬁcult, as it\nrequires that all component functions be invert-\nible and also requires storage of large Jacobian\nmatrices, which is memory intensive. To address\nthis issue, several recent papers propose specially\ndesigned invertible networks that are easily train-\nable yet still powerful (Dinh et al., 2014, 2016;\nJacobsen et al., 2018). Inspired by these works,\nwe use the invertible transformation proposed by\nDinh et al. (2014), which consists of a series of\n“coupling layers”. This architecture is specially\ndesigned to guarantee a unit Jacobian determinant\n(and thus the invertibility property).\nFrom Eq. (8) we know that only f−1\nφ\nis re-\nquired for accomplishing learning and inference;\nwe never need to explicitly construct fφ. Thus, we\ndirectly deﬁne the architecture of f−1\nφ . As shown\nin Figure 3, the nonlinear transformation from the\nobserved embedding xi to h(1)\ni\nrepresents the ﬁrst\ncoupling layer. The input in this layer is parti-\ntioned into left and right halves of dimensions, xi,l\nand xi,r, respectively. A single coupling layer is\ndeﬁned as:\nh(1)\ni,l = xi,l,\nh(1)\ni,r = xi,r + g(xi,l),\n(9)\nwhere g : Rdx/2 →Rdx/2 is the coupling func-\ntion and can be any nonlinear form. This transfor-\nmation satisﬁes dim(h(1)) = dim(x), and Dinh\net al. (2014) show that its Jacobian matrix is tri-\nangular with all ones on the main diagonal. Thus\nthe Jacobian determinant is always equal to one\n(i.e. volume-preserving) and the invertibility con-\ndition is naturally satisﬁed.\nTo be sufﬁciently expressive, we compose mul-\ntiple coupling layers as suggested in Dinh et al.\n(2014).\nSpeciﬁcally, we exchange the role of\nleft and right half vectors at each layer as shown\nin Figure 3.\nFor instance, from xi to h(1)\ni\nthe\nleft subset xi,l is unchanged, while from h(1)\ni\nto\nh(2)\ni\nthe right subset h(1)\ni,r remains the same. Also\nnote that composing multiple coupling layers does\nnot change the volume-preserving and invertibility\nproperties. Such a sequence of invertible transfor-\nmations from the data space x to e is also called\nnormalizing ﬂow (Rezende and Mohamed, 2015).\n4\nExperiments\nIn this section, we ﬁrst describe our datasets and\nexperimental setup. We then instantiate our ap-\nproach with Markov and DMV-structured syntax\nmodels, and report results on POS tagging and de-\npendency grammar induction respectively. Lastly,\nwe analyze the learned latent embeddings.\n4.1\nData\nFor both POS tagging and dependency parsing, we\nrun experiments on the Wall Street Journal (WSJ)\nportion of the Penn Treebank.3 To create the ob-\nserved data embeddings, we train skip-gram word\nembeddings (Mikolov et al., 2013) that are found\nto capture syntactic properties well when trained\nwith small context window (Bansal et al., 2014;\nLin et al., 2015). Following Lin et al. (2015), the\ndimensionality dx is set to 100, and the training\ncontext window size is set to 1 to encode more\nsyntactic information. The skip-gram embeddings\nare trained on the one billion word language mod-\neling benchmark dataset (Chelba et al., 2013) in\naddition to the WSJ corpus.\n4.2\nGeneral Experimental Setup\nFor the neural projector, we employ rectiﬁed net-\nworks as coupling function g following Dinh et al.\n(2014). We use a rectiﬁed network with an input\nlayer, one hidden layer, and linear output units,\nthe number of hidden units is set to the same as\nthe number of input units. The number of cou-\npling layers are varied as 4, 8, 16 for both tasks.\n3Preprocessing is different for the two tasks, we describe\nthe details in the following subsections.\nWe optimize marginal data likelihood directly us-\ning Adam (Kingma and Ba, 2014). For both tasks\nin the fully unsupervised setting, we do not tune\nthe hyper-parameters using supervised data.\n4.3\nUnsupervised POS tagging\nFor unsupervised POS tagging, we use a Markov-\nstructured syntax model in our approach, which\nis a popular structure for unsupervised tagging\ntasks (Lin et al., 2015; Tran et al., 2016).\nSetup.\nFollowing existing literature, we train\nand test on the entire WSJ corpus (49208 sen-\ntences, 1M tokens). We use 45 tag clusters, the\nnumber of POS tags that appear in WSJ cor-\npus. We train the discrete HMM and the Gaus-\nsian HMM (Lin et al., 2015) as baselines. For the\nGaussian HMM, mean vectors of Gaussian emis-\nsions are initialized with the empirical mean of all\nword vectors with an additive noise. We assume\ndiagonal covariance matrix for p(ei|zi) and initial-\nize it with the empirical variance of the word vec-\ntors. Following Lin et al. (2015), the covariance\nmatrix is ﬁxed during training. The multinomial\nprobabilities are initialized as θkv ∝exp(ukv),\nwhere ukv ∼U[0, 1].\nFor our approach, we\ninitialize the syntax model and Gaussian param-\neters with the pre-trained Gaussian HMM. The\nweights of layers in the rectiﬁed network are ini-\ntialized from a uniform distribution with mean\nzero and a standard deviation of\np\n1/nin, where\nnin is the input dimension.4 We evaluate the per-\nformance of POS tagging with both Many-to-One\n(M-1) accuracy (Johnson, 2007) and V-Measure\n(VM) (Rosenberg and Hirschberg, 2007). Given\na model we found that the tagging performance is\nwell-correlated with the training data likelihood,\nthus we use training data likelihood as a unsuper-\nvised criterion to select the trained model over 10\nrandom restarts after training 50 epochs. We re-\npeat this process 5 times and report the mean and\nstandard deviation of performance.\nResults.\nWe compare our approach with ba-\nsic HMM, Gaussian HMM, and several state-\nof-the-art systems, including sophisticated HMM\nvariants and clustering techniques with hand-\nengineered features. The results are presented in\nTable 1.\nThrough the introduced latent embed-\ndings and additional neural projection, our ap-\nproach improves over the Gaussian HMM by 5.4\npoints in M-1 and 5.6 points in VM. Neural HMM\n4This is the default parameter initialization in PyTorch.\nSystem\nM-1\nVM\nw/o hand-engineered features\nDiscrete HMM\n62.7\n53.8\nPYP-HMM (Blunsom and Cohn, 2011)\n77.5\n69.8\nNHMM (basic) (Tran et al., 2016)\n59.8\n54.2\nNHMM (+ Conv) (Tran et al., 2016)\n74.1\n66.1\nNHMM (+ Conv & LSTM) (Tran et al., 2016)\n79.1\n71.7\nGaussian HMM (Lin et al., 2015)\n75.4 (1.0)\n68.5 (0.5)\nOurs (4 layers)\n79.5 (0.9)\n73.0 (0.7)\nOurs (8 layers)\n80.8 (1.3)\n74.1 (0.7)\nOurs (16 layers)\n73.2 (4.3)\n70.5 (2.1)\nw/ hand-engineered features\nFeature HMM (Berg-Kirkpatrick et al., 2010)\n75.5\n–\nBrown (+ proto) (Christodoulopoulos et al., 2010)\n76.1\n68.8\nCluster (word-based) (Yatbaz et al., 2012)\n80.2\n72.1\nCluster (token-based) (Yatbaz et al., 2014)\n79.5\n69.1\nTable 1: Unsupervised POS tagging results on entire WSJ,\ncompared with other baselines and state-of-the-art systems.\nStandard deviation is given in parentheses when available.\nNN\nNNS\nNNP\nNNPS\nOthers\nNN\nNNS\nNNP\nNNPS\nOthers\n0.33\n0.00\n0.02\n0.00\n0.65\n0.13\n0.48\n0.02\n0.00\n0.37\n0.01\n0.00\n0.35\n0.06\n0.59\n0.01\n0.18\n0.50\n0.19\n0.12\n0.01\n0.00\n0.00\n0.00\n0.98\n(a) Gaussian HMM\nNN\nNNS\nNNP\nNNPS\nOthers\nNN\nNNS\nNNP\nNNPS\nOthers\n0.78\n0.00\n0.00\n0.02\n0.21\n0.03\n0.89\n0.00\n0.02\n0.06\n0.01\n0.00\n0.32\n0.30\n0.37\n0.01\n0.20\n0.01\n0.47\n0.31\n0.02\n0.00\n0.00\n0.00\n0.98\n(b) Our approach\nFigure 4: Normalized Confusion matrix for POS tagging ex-\nperiments, row label represents the gold tag.\n(NHMM) (Tran et al., 2016) is a baseline that also\nlearns word representation jointly. Both their ba-\nsic model and extended Conv version does not\noutperform the Gaussian HMM. Their best model\nincorporates another LSTM to model long dis-\ntance dependency and breaks the Markov assump-\ntion, yet our approach still achieves substantial im-\nprovement over it without considering more con-\ntext information. Moreover, our method outper-\nforms the best published result that beneﬁts from\nhand-engineered features (Yatbaz et al., 2012) by\n2.0 points on VM.\nConfusion Matrix.\nWe found that most tagging\nerrors happen in noun subcategories. Therefore,\nwe do the one-to-one mapping between gold POS\ntags and induced clusters and plot the normalized\nconfusion matrix of noun subcategories in Fig-\nure 4. The Gaussian HMM fails to identify “NN”\nand “NNS” correctly for most cases, and it often\nrecognizes “NNPS” as “NNP”. In contrast, our ap-\nproach corrects these errors well.\n4.4\nUnsupervised Dependency Parsing\nwithout gold POS tags\nFor the task of unsupervised dependency parse in-\nduction, we employ the Dependency Model with\nValence (DMV) (Klein and Manning, 2004) as the\nsyntax model in our approach. DMV is a genera-\ntive model that deﬁnes a probability distribution\nover dependency parse trees and syntactic cate-\ngories, generating tokens and dependencies in a\nhead-outward fashion. While, traditionally, DMV\nis trained using gold POS tags as observed syntac-\ntic categories, in our approach, we treat each tag\nas a latent variable, as described in §2.3.\nMost existing approaches to this task are not\nfully unsupervised since they rely on gold POS\ntags following the original experimental setup for\nDMV. This is partially because automatically pars-\ning from words is difﬁcult even when using un-\nsupervised syntactic categories (Spitkovsky et al.,\n2011a).\nHowever, inducing dependencies from\nwords alone represents a more realistic exper-\nimental condition since gold POS tags are of-\nten unavailable in practice.\nPrevious work that\nhas trained from words alone often requires ad-\nditional linguistic constraints (like sentence inter-\nnal boundaries) (Spitkovsky et al., 2011a,b, 2012,\n2013), acoustic cues (Pate and Goldwater, 2013),\nadditional training data (Pate and Johnson, 2016),\nor annotated data from related languages (Cohen\net al., 2011). Our approach is naturally designed\nto train on word embeddings directly, thus we at-\ntempt to induce dependencies without using gold\nPOS tags or other extra linguistic information.\nSetup.\nLike previous work we use sections 02-\n21 of WSJ corpus as training data and evaluate\non section 23, we remove punctuations and train\nthe models on sentences of length ⩽10, “head-\npercolation” rules (Collins, 1999) are applied to\nobtain gold dependencies for evaluation. We train\nbasic DMV, extended DMV (E-DMV) (Head-\nden III et al., 2009) and Gaussian DMV (which\ntreats POS tag as unknown latent variables and\ngenerates observed word embeddings directly\nconditioned on them following Gaussian distri-\nbution) as baselines.\nBasic DMV and E-DMV\nare trained with Viterbi EM (Spitkovsky et al.,\n2010) on unsupervised POS tags induced from\nour Markov-structured model described in §4.3.\nMultinomial parameters of the syntax model in\nboth Gaussian DMV and our model are initial-\nized with the pre-trained DMV baseline. Other\nSystem\n⩽10\nall\nw/o gold POS tags\nDMV (Klein and Manning, 2004)\n49.6\n35.8\nE-DMV (Headden III et al., 2009)\n52.1\n38.2\nUR-A E-DMV (Tu and Honavar, 2012)\n58.9\n46.1\nCS∗(Spitkovsky et al., 2013)\n72.0∗\n64.4∗\nNeural E-DMV (Jiang et al., 2016)\n55.3\n42.7\nCRFAE (Cai et al., 2017)\n37.2\n29.5\nGaussian DMV\n55.4 (1.3)\n43.1 (1.2)\nOurs (4 layers)\n58.4 (1.9)\n46.2 (2.3)\nOurs (8 layers)\n60.2 (1.3)\n47.9 (1.2)\nOurs (16 layers)\n54.1 (8.5)\n43.9 (5.7)\nw/ gold POS tags (for reference only)\nDMV (Klein and Manning, 2004)\n55.1\n39.7\nUR-A E-DMV (Tu and Honavar, 2012)\n71.4\n57.0\nMaxEnc (Le and Zuidema, 2015)\n73.2\n65.8\nNeural E-DMV (Jiang et al., 2016)\n72.5\n57.6\nCRFAE (Cai et al., 2017)\n71.7\n55.7\nL-NDMV (Big training data) (Han et al., 2017)\n77.2\n63.2\nTable 2: Directed dependency accuracy on section 23 of\nWSJ, evaluating on sentences of length ⩽10 and all lengths.\nStarred entries (∗) denote that the system beneﬁts from ad-\nditional punctuation-based constraints. Standard deviation is\ngiven in parentheses when available.\nparameters are initialized in the same way as in\nthe POS tagging experiment. The directed depen-\ndency accuracy (DDA) is used for evaluation and\nwe report accuracy on sentences of length ⩽10\nand all lengths. We train the parser until training\ndata likelihood converges, and report the mean and\nstandard deviation over 20 random restarts.\nComparison with other related work.\nOur\nmodel directly observes word embeddings and\ndoes not require gold POS tags during training.\nThus, results from related work trained on gold\ntags are not directly comparable.\nHowever, to\nmeasure how these systems might perform with-\nout gold tags, we run three recent state-of-the-\nart systems in our experimental setting:\nUR-\nA E-DMV (Tu and Honavar, 2012), Neural E-\nDMV (Jiang et al., 2016), and CRF Autoencoder\n(CRFAE) (Cai et al., 2017).5 We use unsupervised\nPOS tags (induced from our Markov-structured\nmodel) in place of gold tags.6 We also train ba-\nsic DMV on gold tags and include several state-\nof-the-art results on gold tags as reference points.\nResults.\nAs shown in Table 2, our approach\nis able to improve over the Gaussian DMV by\n4.8 points on length ⩽10 and 4.8 points on all\n5For the three systems, we use implementations from the\noriginal papers (via personal correspondence with the au-\nthors), and tune their hyperparameters on section 22 of WSJ.\n6Using words directly is not practical because these sys-\ntems often require a transition probability matrix between in-\nput symbols, which requires too much memory.\nSystem\nM-1\nVM\nOurs (4 layers)\n78.2\n71.2\nOurs (8 layers)\n72.5\n69.7\nOurs (16 layers)\n67.2\n69.2\nTable 3: Unsupervised POS tagging results of our approach\non WSJ, with random initialization of syntax model.\nlengths, which suggests the additional latent em-\nbedding layer and neural projector are helpful.\nThe proposed approach yields, to the best of our\nknowledge,7 state-of-the-art performance with-\nout gold POS annotation and without sentence-\ninternal boundary information. DMV, UR-A E-\nDMV, Neural E-DMV, and CRFAE suffer a large\ndecrease in performance when trained on unsu-\npervised tags – an effect also seen in previous\nwork (Spitkovsky et al., 2011a; Cohen et al.,\n2011).\nSince our approach induces latent POS\ntags jointly with dependency trees, it may be able\nto learn POS clusters that are more amenable to\ngrammar induction than the unsupervised tags.\nWe observe that CRFAE underperforms its gold-\ntag counterpart substantially. This may largely be\na result of the model’s reliance on prior linguistic\nrules that become unavailable when gold POS tag\ntypes are unknown. Many extensions to DMV can\nbe considered orthogonal to our approach – they\nessentially focus on improving the syntax model.\nIt is possible that incorporating these more sophis-\nticated syntax models into our approach may lead\nto further improvements.\n4.5\nSensitivity Analysis\nImpact of Initialization.\nIn the above experi-\nments we initialize the structured syntax compo-\nnents with the pre-trained Gaussian or discrete\nbaseline, which is shown as a useful technique\nto help train our deep models. We further study\nthe results with fully random initialization. In the\nPOS tagging experiment, we report the results in\nTable 3. While the performance with 4 layers is\ncomparable to the pre-trained Gaussian initializa-\ntion, deeper projections (8 or 16 layers) result in a\ndramatic drop in performance. This suggests that\nthe structured syntax model with very deep projec-\ntions is difﬁcult to train from scratch, and a simpler\nprojection might be a good compromise in the ran-\ndom initialization setting.\nDifferent from the Markov prior in POS tag-\n7We tried to be as thorough as possible in evaluation\nby running top performing systems using our more difﬁcult\ntraining setup when this was feasible – but it was not possible\nto evaluate them all.\nSystem\nM-1\nVM\nGaussian HMM\n72.0\n65.0\nOurs (4 layers)\n76.4\n69.3\nOurs (8 layers)\n76.8\n69.4\nOurs (16 layers)\n67.3\n62.0\nTable 4: Unsupervised POS tagging results on WSJ, with\nfastText vectors as the observed embeddings.\nSystem\n⩽10\nall\nGaussian DMV\n53.6\n41.3\nOurs (4 layers)\n56.9\n43.9\nOurs (8 layers)\n57.1\n42.3\nOurs (16 layers)\n52.9\n39.5\nTable 5: Directed dependency accuracy on section 23 of\nWSJ, with fastText vectors as the observed embeddings.\nging experiments, our parsing model seems to be\nquite sensitive to the initialization. For example,\ndirected accuracy of our approach on sentences of\nlength ⩽10 is below 40.0 with random initializa-\ntion. This is consistent with previous work that has\nnoted the importance of careful initialization for\nDMV-based models such as the commonly used\nharmonic initializer (Klein and Manning, 2004).\nHowever, it is not straightforward to apply the har-\nmonic initializer for DMV directly in our model\nwithout using some kind of pre-training since we\ndo not observe gold POS.\nImpact of Observed Embeddings.\nWe investi-\ngate the effect of the choice of pre-trained embed-\nding on performance while using our approach.\nTo this end, we additionally include results us-\ning fastText embeddings (Bojanowski et al., 2017)\n– which, in contrast with skip-gram embeddings,\ninclude character-level information.\nWe set the\ncontext windows size to 1 and the dimension size\nto 100 as in the skip-gram training, while keep-\ning other parameters set to their defaults. These\nresults are summarized in Table 4 and Table 5.\nWhile fastText embeddings lead to reduced perfor-\nmance with our model, our approach still yields an\nimprovement over the Gaussian baseline with the\nnew observed embeddings space.\n4.6\nQualitative Analysis of Embeddings\nWe perform qualitative analysis to understand how\nthe latent embeddings help induce syntactic struc-\ntures. First we ﬁlter out low-frequency words and\npunctuations in WSJ, and visualize the rest words\n(10k) with t-SNE (Maaten and Hinton, 2008) un-\nder different embeddings. We assign each word\nwith its most likely gold POS tags in WSJ and\ncolor them according to the gold POS tags.\nTarget\nSkip-gram\nMarkov Structure\ncome\ngo came follow\ncoming sit\nbe go do give\nfollow\nsinging\ndancing sing\ndrumming dance\ndances\ndancing drumming\nmarching playing\nrecording\ncigars\ncigarettes sodas\nchampagne cigar\nrum\nsodas bottles\ndrinks pills\ncigarettes\nnewer\nﬂashier fancier\nconventional low-end\nnew-generation\nsofter lighter\nthinner darker\nsmoother\nfanciest\npriciest up-scale\nloveliest fancier\nhigh-end\nliveliest priciest\nsmartest best-run\nfastest-growing\nTable 6: Target words and their 5 nearest neighbors, based\non skip-gram embeddings and our learned latent embeddings\nwith Markov-structured syntax model.\nagenda\nerror\nprocess\ntimetable\nplans\ndreams\npayments\n(obj)\nsmokers\nparents\nfurriers\nissuers\nfolks\naides\n(subj)\naide\nresident\nattorney\nsinger\nactress\nowner\n(subj)\nFigure 5: Visualization (t-SNE) of learned latent embed-\ndings with DMV-structured syntax model. Each node rep-\nresents a word and is colored according to the most likely\ngold POS tag in the Penn Treebank (best seen in color).\nFor our Markov-structured model, we have dis-\nplayed the embedding space in Figure 1(b), where\nthe gold POS clusters are well-formed. Further,\nwe present ﬁve example target words and their ﬁve\nnearest neighbors in terms of cosine similarity. As\nshown in Table 6, the skip-gram embedding cap-\ntures both semantic and syntactic aspects to some\ndegree, yet our embeddings are able to focus es-\npecially on the syntactic aspects of words, in an\nunsupervised fashion without using any extra mor-\nphological information.\nIn Figure 5 we depict the learned latent em-\nbeddings with the DMV-structured syntax model.\nUnlike the Markov structure, the DMV structure\nmaps a large subset of singular and plural nouns to\nthe same overlapping region. However, two clus-\nters of singular and plural nouns are actually sepa-\nrated. We inspect the two clusters and the overlap-\nping region in Figure 5, it turns out that the nouns\nin the separated clusters are words that can appear\nas subjects and, therefore, for which verb agree-\nment is important to model. In contrast, the nouns\nin the overlapping region are typically objects.\nThis demonstrates that the latent embeddings are\nfocusing on aspects of language that are speciﬁ-\ncally important for modeling dependency without\never having seen examples of dependency parses.\nSome previous work has deliberately created\nembeddings to capture different notions of sim-\nilarity (Levy and Goldberg, 2014; Cotterell and\nSch¨utze, 2015), while they use extra morphol-\nogy or dependency annotations to guide the em-\nbedding learning, our approach provides a poten-\ntial alternative to create new embeddings that are\nguided by structured syntax model, only using un-\nlabeled text corpora.\n5\nRelated Work\nOur approach is related to ﬂow-based generative\nmodels, which are ﬁrst described in NICE (Dinh\net al., 2014) and have recently received more at-\ntention (Dinh et al., 2016; Jacobsen et al., 2018;\nKingma and Dhariwal, 2018).\nThis relevant\nwork mostly adopts simple (e.g. Gaussian) and\nﬁxed priors and does not attempt to learn inter-\npretable latent structures.\nAnother related gen-\nerative model class is variational auto-encoders\n(VAEs) (Kingma and Welling, 2013) that opti-\nmize a lower bound on the marginal data likeli-\nhood, and can be extended to learn latent struc-\ntures (Miao and Blunsom, 2016; Yin et al., 2018).\nAgainst the ﬂow-based models, VAEs remove the\ninvertibility constraint but sacriﬁce the merits of\nexact inference and exact log likelihood compu-\ntation, which potentially results in optimization\nchallenges (Kingma et al., 2016). Our approach\ncan also be viewed in connection with generative\nadversarial networks (GANs) (Goodfellow et al.,\n2014) that is a likelihood-free framework to learn\nimplicit generative models. However, it is non-\ntrivial for a gradient-based method like GANs to\npropagate gradients through discrete structures.\n6\nConclusion\nIn this work, we deﬁne a novel generative ap-\nproach to leverage continuous word representa-\ntions for unsupervised learning of syntactic struc-\nture. Experiments on both POS induction and un-\nsupervised dependency parsing tasks demonstrate\nthe effectiveness of our proposed approach. Fu-\nture work might explore more sophisticated in-\nvertible projections, or recurrent projections that\njointly transform the entire input sequence.\nReferences\nMohit Bansal, Kevin Gimpel, and Karen Livescu.\n2014. Tailoring continuous word representations for\ndependency parsing. In Proceedings of ACL.\nTaylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,\nJohn DeNero, and Dan Klein. 2010. Painless un-\nsupervised learning with features. In Proceedings of\nHLT-NAACL, pages 582–590. Association for Com-\nputational Linguistics.\nPhil Blunsom and Trevor Cohn. 2011.\nA hierarchi-\ncal pitman-yor process hmm for unsupervised part\nof speech induction. In Proceedings of ACL.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion of Computational Linguistics.\nJiong Cai, Yong Jiang, and Kewei Tu. 2017. Crf au-\ntoencoder for unsupervised dependency parsing. In\nProceedings of EMNLP.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nScott Saobing Chen and Ramesh A Gopinath. 2001.\nGaussianization. In Advances in neural information\nprocessing systems.\nChristos Christodoulopoulos, Sharon Goldwater, and\nMark Steedman. 2010.\nTwo decades of unsuper-\nvised pos induction: How far have we come?\nIn\nProceedings of EMNLP.\nShay B Cohen, Dipanjan Das, and Noah A Smith.\n2011. Unsupervised structure prediction with non-\nparallel multilingual guidance.\nIn Proceedings of\nEMNLP.\nShay B Cohen and Noah A Smith. 2009. Shared logis-\ntic normal distributions for soft parameter tying in\nunsupervised grammar induction. In Proceedings of\nHLT-NAACL.\nMichael Collins. 1999. HEAD-DRIVEN STATISTICAL\nMODELS FOR NATURAL LANGUAGE PARSING.\nPh.D. thesis, University of Pennsylvania.\nRyan Cotterell and Hinrich Sch¨utze. 2015.\nMor-\nphological word-embeddings.\nIn Proceedings of\nNAACL-HLT.\nLaurent Dinh, David Krueger, and Yoshua Bengio.\n2014.\nNice: Non-linear independent components\nestimation. arXiv preprint arXiv:1410.8516.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-\ngio. 2016. Density estimation using real nvp. arXiv\npreprint arXiv:1605.08803.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Proceedings of NIPS.\nWenjuan Han, Yong Jiang, and Kewei Tu. 2017. De-\npendency grammar induction with neural lexical-\nization and big training data.\nIn Proceedings of\nEMNLP.\nLuheng He, Kenton Lee, Mike Lewis, and Luke Zettle-\nmoyer. 2017.\nDeep semantic role labeling: What\nworks and whats next. In Proceedings of ACL.\nWilliam P Headden III, Mark Johnson, and David Mc-\nClosky. 2009. Improving unsupervised dependency\nparsing with richer contexts and smoothing. In Pro-\nceedings of HLT-NAACL.\nAapo Hyv¨arinen, Juha Karhunen, and Erkki Oja. 2004.\nIndependent component analysis, volume 46. John\nWiley & Sons.\nJ¨orn-Henrik Jacobsen, Arnold Smeulders, and Edouard\nOyallon. 2018. i-revnet: Deep invertible networks.\narXiv preprint arXiv:1802.07088.\nYong Jiang, Wenjuan Han, and Kewei Tu. 2016. Unsu-\npervised neural dependency parsing. In Proceedings\nof EMNLP.\nMark Johnson. 2007.\nWhy doesnt em ﬁnd good\nhmm pos-taggers? In Proceedings of the EMNLP-\nCoNLL.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nDiederik P Kingma and Prafulla Dhariwal. 2018.\nGlow: Generative ﬂow with invertible 1x1 convo-\nlutions. arXiv preprint arXiv:1807.03039.\nDiederik P Kingma, Tim Salimans, Rafal Jozefowicz,\nXi Chen, Ilya Sutskever, and Max Welling. 2016.\nImproved variational inference with inverse autore-\ngressive ﬂow. In Advances in Neural Information\nProcessing Systems, pages 4743–4751.\nDiederik P Kingma and Max Welling. 2013.\nAuto-\nencoding\nvariational\nbayes.\narXiv\npreprint\narXiv:1312.6114.\nDan Klein and Christopher D Manning. 2004. Corpus-\nbased induction of syntactic structure: Models of de-\npendency and constituency. In Proceedings of ACL.\nPhong Le and Willem Zuidema. 2015. Unsupervised\ndependency parsing: Let’s use supervised parsers.\nIn Proceedings of NAACL-HLT.\nOmer Levy and Yoav Goldberg. 2014. Dependency-\nbased word embeddings. In Proceedings of ACL.\nChu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori\nLevin. 2015. Unsupervised pos induction with word\nembeddings. In Proceedings of the NAACL-HLT.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne.\nJournal of Machine\nLearning Research.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large annotated\ncorpus of english: The penn treebank.\nComputa-\ntional linguistics, 19(2):313–330.\nYishu Miao and Phil Blunsom. 2016. Language as a\nlatent variable: Discrete generative models for sen-\ntence compression. In Proceedings of EMNLP.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013.\nEfﬁcient estimation of word\nrepresentations in vector space.\narXiv preprint\narXiv:1301.3781.\nJohn K Pate and Sharon Goldwater. 2013.\nUnsu-\npervised dependency parsing with acoustic cues.\nTransactions of the Association for Computational\nLinguistics.\nJohn K Pate and Mark Johnson. 2016. Grammar in-\nduction from (lots of) words alone. In Proceedings\nof COLING.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of HLT-NAACL.\nDanilo Jimenez Rezende and Shakir Mohamed. 2015.\nVariational inference with normalizing ﬂows.\nIn\nProceedings of ICML.\nAndrew Rosenberg and Julia Hirschberg. 2007.\nV-\nmeasure: A conditional entropy-based external clus-\nter evaluation measure. In Proceedings of EMNLP-\nCoNLL.\nValentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,\nand Daniel Jurafsky. 2011a. Unsupervised depen-\ndency parsing without gold part-of-speech tags. In\nProceedings of EMNLP.\nValentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-\nrafsky. 2011b. Punctuation: Making a point in un-\nsupervised dependency parsing. In Proceedings of\nCoNLL.\nValentin I Spitkovsky, Hiyan Alshawi, and Daniel\nJurafsky. 2012.\nCapitalization cues improve de-\npendency grammar induction.\nIn Proceedings of\nNAACL-HLT Workshop on the Induction of Linguis-\ntic Structure.\nValentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-\nrafsky. 2013.\nBreaking out of local optima with\ncount transforms and model recombination: A study\nin grammar induction. In Proceedings of EMNLP.\nValentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,\nand Christopher D Manning. 2010. Viterbi training\nimproves unsupervised dependency parsing. In Pro-\nceedings of CoNLL.\nKarl Stratos, Michael Collins, and Daniel Hsu. 2016.\nUnsupervised part-of-speech tagging with anchor\nhidden markov models. Transactions of the Asso-\nciation for Computational Linguistics.\nKe M Tran, Yonatan Bisk, Ashish Vaswani, Daniel\nMarcu, and Kevin Knight. 2016. Unsupervised neu-\nral hidden markov models. In Proceedings of the\nWorkshop on Structured Prediction for NLP.\nKewei Tu and Vasant Honavar. 2012.\nUnambiguity\nregularization for unsupervised learning of prob-\nabilistic grammars.\nIn Proceedings of EMNLP-\nCoNLL.\nMehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.\nLearning syntactic categories using paradigmatic\nrepresentations of word context. In Proceedings of\nEMNLP-CoNLL.\nMehmet Ali Yatbaz, Enis Rıfat Sert, and Deniz Yuret.\n2014. Unsupervised instance-based part of speech\ninduction using probable substitutes.\nIn Proceed-\nings of COLING.\nPengcheng Yin, Chunting Zhou, Junxian He, and Gra-\nham Neubig. 2018. Structvae: Tree-structured latent\nvariable models for semi-supervised semantic pars-\ning. In Proceedings of ACL.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2018-08-28",
  "updated": "2018-08-28"
}