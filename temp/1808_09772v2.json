{
  "id": "http://arxiv.org/abs/1808.09772v2",
  "title": "Notes on Deep Learning for NLP",
  "authors": [
    "Antoine J. -P. Tixier"
  ],
  "abstract": "My notes on Deep Learning for NLP.",
  "text": "Notes on Deep Learning for NLP\nAntoine J.-P. Tixier\nComputer Science Department (DaSciM team)\nÉcole Polytechnique, Palaiseau, France\nantoine.tixier-1@colorado.edu\nLast updated Friday 31st August, 2018 (ﬁrst uploaded March 23, 2017)\nContents\n1\nDisclaimer\n2\n2\nCode\n2\n3\nIMDB Movie review dataset\n2\n3.1\nOverview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n3.2\nBinary classiﬁcation objective function . . . . . . . . . . . . . . . . . . . . . . . .\n2\n4\nParadigm switch\n3\n4.1\nFeature embeddings\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n4.2\nBeneﬁts of feature embeddings\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n4.3\nCombining core features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n5\nConvolutional Neural Networks (CNNs)\n4\n5.1\nLocal invariance and compositionality\n. . . . . . . . . . . . . . . . . . . . . . . .\n4\n5.2\nConvolution and pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n5.2.1\nInput\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n5.2.2\nConvolution layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n5.2.3\nPooling layer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n5.2.4\nDocument encoding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n5.2.5\nSoftmax layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n5.3\nNumber of parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.4\nVisualizing and understanding inner representations and predictions\n. . . . . . .\n7\n5.4.1\nDocument embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.4.2\nPredictive regions identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.4.3\nSaliency maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n6\nRecurrent Neural Networks (RNNs)\n9\n6.1\nRNN framework\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n6.1.1\nLanguage modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n6.2\nLSTM unit\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n6.2.1\nInner layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n6.2.2\nForgetting/learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.2.3\nVanilla RNN analogy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.3\nGated Recurrent Unit (GRU) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.4\nRNN vs LSTM vs GRU\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1\narXiv:1808.09772v2  [cs.CL]  30 Aug 2018\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n7\nAttention\n14\n7.1\nEncoder-decoder attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n7.1.1\nEncoder-decoder overview . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n7.1.2\nEncoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n7.1.3\nDecoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n7.1.4\nGlobal attention\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n7.1.5\nLocal attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n7.2\nSelf-attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.2.1\nDiﬀerence with seq2seq attention . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.2.2\nHierarchical attention\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n1\nDisclaimer\nWriting these notes is part of my learning process, so it is a work in progress. To write the\ncurrent version of this document, I curated information mainly from the original 2D CNN paper\n[16] and Stanford’s CS231n CNN course notes1, Zhang and Wallace practitioners’ guide to CNNs\nin NLP [26], the seminal papers on CNN for text classiﬁcation [13, 14], Denny Britz’ tutorial2\non RNNs, Chris Colah’s post3 on understanding the LSTM unit, and the seminal papers on the\nGRU unit [2, 4], encoder-decoder architectures [2, 24] and attention [20, 1]. Last but not least,\nYoav Golderg’s primer on neural networks for NLP [8] and Luong, Cho and Manning tutorial on\nneural machine translation4 proved very useful.\n2\nCode\nI implemented some of the models described in this document in Keras and tested them on the\nIMDB movie review dataset. The code can be found on my GitHub: https://github.com/\nTixierae/deep_learning_NLP. Again, this is a work in progress.\n3\nIMDB Movie review dataset\n3.1\nOverview\nThe task is to perform binary classiﬁcation (positive/negative) on reviews from the Internet\nMovie Database (IMDB) dataset5, which is known as sentiment analysis or opinion mining. The\ndataset contains 50K movie reviews, labeled by polarity. The data are partitioned into 50 % for\ntraining and 50% for testing. The imdb_preprocess.py script on my GitHub cleans the reviews\nand put them in a format suitable to be passed to neural networks: each review is a list of word\nindexes (integers) from a dictionary of size V where the most frequent word has index 1.\n3.2\nBinary classiﬁcation objective function\nThe objective function that our models will learn to minimize is the log loss, also known as the\ncross entropy. More precisely, in a binary classiﬁcation setting with 2 classes (say 0 and 1) the\nlog loss is deﬁned as:\nlogloss = −1\nN\nN\nX\ni=1\n\u0000yilogpi +\n\u00001 −yi\n\u0001\nlog\n\u00001 −pi\n\u0001\u0001\n(1)\n1http://cs231n.github.io/convolutional-networks/\n2http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n3http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n4https://sites.google.com/site/acl16nmt/home\n5http://ai.stanford.edu/~amaas/data/sentiment/\npage 2\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nWhere N is the number of observations, pi is the probability assigned to class 1,\n\u00001 −pi\n\u0001\nis\nthe probability assigned to class 0, and yi is the true label of the ith observation (0 or 1). You\ncan see that only the term associated with the true label of each observation contributes to the\noverall score. For a given observation, assuming that the true label is 1, and the probability\nassigned by the model to that class is 0.8 (quite good prediction), the log loss will be equal to\n−log(0.8) = 0.22. If the prediction is slightly worse, but not completely oﬀ, say with pi = 0.6,\nthe log loss will be equal to 0.51, and for 0.1, the log loss will reach 2.3. Thus, the further away\nthe model gets from the truth, the greater it gets penalized. Obviously, a perfect prediction\n(probability of 1 for the right class) gets a null score.\n4\nParadigm switch\n4.1\nFeature embeddings\nCompared to traditional machine learning models that consider core features and combinations of\nthem as unique dimensions of the feature space, deep learning models often embed core features\n(and core features only) as vectors in a low-dimensional continuous space where dimensions\nrepresent shared latent concepts [8]. The embeddings are initialized randomly or obtained from\npre-training6. They can then be updated during training just like other model parameters, or\nbe kept static.\n4.2\nBeneﬁts of feature embeddings\nThe main advantage of mapping features to dense continuous vectors is the ability to capture\nsimilarity between features, and therefore to generalize. For instance, if the model has never\nseen the word “Obama” during training, but has encountered the word “president”, by knowing\nthat the two words are related, it will be able to transfer what it has learned for “president” to\ncases where “Obama” is involved. With traditional one-hot vectors, those two features would\nbe considered orthogonal and predictive power would not be able to be shared between them7.\nAlso, going from a huge sparse space to a dense and compact space reduces computational cost\nand the amount of data required to ﬁt the model, since there are fewer parameters to learn.\n4.3\nCombining core features\nUnlike what is done in traditional ML, combinations of core features are not encoded as new\ndimensions of the feature space, but as the sum, average, or concatenation of the vectors of the\ncore features that are to be combined. Summing or averaging is an easy way to always get a\nﬁxed-size input vector regardless of the size of the training example (e.g., number of words in the\ndocument). However, both of these approaches completely ignore the ordering of the features.\nFor instance, under this setting, and using unigrams as features, the two sentences “John is\nquicker than Mary” and “Mary is quicker than John” have the exact same representation. On\nthe other hand, using concatenation allows to keep track of ordering, but padding and truncation8\nneed to be used so that the same number of vectors are concatenated for each training example.\nFor instance, regardless of its size, every document in the collection can be transformed to have\nthe same ﬁxed length s: the longer documents are truncated to their ﬁrst (or last, middle...)\ns words, and the shorter documents are padded with a special zero vector to make up for the\nmissing words [26, 14].\n6In NLP, pre-trained word vectors obtained with Word2vec or GloVe from very large corpora are often used.\nE.g., Google News word2vec vectors can be obtained from https://code.google.com/archive/p/word2vec/,\nunder the section “Pre-trained word and phrase vectors”\n7Note that one-hot vectors can be passed as input to neural networks. But then, the network implicitly learns\nfeature embeddings in its ﬁrst layer\n8https://keras.io/preprocessing/sequence/\npage 3\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n5\nConvolutional Neural Networks (CNNs)\n5.1\nLocal invariance and compositionality\nInitially inspired by studies of the cat’s visual cortex [12], CNNs were developed in computer\nvision to work on regular grids such as images [16]. They are feedforward neural networks where\neach neuron in a layer receives input from a neighborhood of the neurons in the previous layer.\nThose neighborhoods, or local receptive ﬁelds, allow CNNs to recognize more and more complex\npatterns in a hierarchical way, by combining lower-level, elementary features into higher-level\nfeatures.\nThis property is called compositionality.\nFor instance, edges can be inferred from\nraw pixels, edges can in turn be used to detect simple shapes, and ﬁnally shapes can be used to\nrecognize objects. Furthermore, the absolute positions of the features in the image do not matter.\nOnly capturing their respective positions is useful for composing higher-level patterns. So, the\nmodel should be able to detect a feature regardless of its position in the image. This property is\ncalled local invariance. Compositionality and local invariance are the two key concepts of CNNs.\nCNNs have reached very good performance in computer vision [15], but it is not diﬃcult to\nunderstand that thanks to compositionality and local invariance, they can also do very well in\nNLP. Indeed, in NLP, high-order features (n-grams) can be constructed from lower-order features\njust like in CV, and ordering is crucial locally (“not bad, quite good”, “not good, quite bad”, “do\nnot recommend”), but not at the document level. Indeed, in trying to determine the polarity of\na movie review, we don’t really care whether “not bad, quite good” is found at the start or at the\nend of the document. We just need to capture the fact that “not” precedes “bad”, and so forth.\nNote that CNNs are not able to encode long-range dependencies, and therefore, for some tasks\nlike language modeling, where long-distance dependence matters, recurrent architectures such as\nLSTMs are preferred.\n5.2\nConvolution and pooling\nThough recent work suggests that convolutional layers may directly be stacked on top of each\nother [23], the elementary construct of the CNN is a convolution layer followed by a pooling layer.\nIn what follows, we will detail how these two layers interplay, using as an example the NLP task\nof short document classiﬁcation (see Fig. 1).\n5.2.1\nInput\nWe can represent a document as a real matrix A ∈Rs×d, where s is the document length, and\nd is the dimension of the word embedding vectors. Since s must be ﬁxed at the collection level\nbut the documents are of diﬀerent sizes, we truncate the longer documents to their ﬁrst s words,\nand pad the shorter documents with a special zero vector as many times as necessary. The word\nvectors may either be initialized randomly or be pre-trained. In the latter case, they can be\nupdated during training or not (“non-static” vs. “static” approach [14]).\nThinking of A as an image is misleading, because there is only one spatial dimension. The\nembedding vectors are not actually part of the input itself, they just represent the coordinates\nof the elements of the input in a shared latent space. In computer vision, the term channels is\noften used to refer to this depth dimension (not to be mistaken with the number of hidden layers\nin the network). If we were dealing with images, we would have two spatial dimensions, plus the\ndepth. The input would be a tensor of dimensionality (width × height × n_channels), i.e., a 2D\nmatrix where each entry would be associated with a vector of length 3 or 1, respectively in the\ncase of color (RGB) and grey level images.\npage 4\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n5.2.2\nConvolution layer\nThe convolution layer is a linear operation followed by a nonlinear transformation. The linear\noperation consists in multiplying (elementwise) each instantiation of a 1D window applied over\nthe input document by a ﬁlter, represented as a matrix of parameters. The ﬁlter, just like the\nwindow, has only one spatial dimension, but it extends fully through the input depth (the d\ndimensions of the word embedding space). If h is the window size, the parameter matrix W\nassociated with the ﬁlter thus belongs to Rh×d. W is initialized randomly and learned during\ntraining.\nThe instantiations of the window over the input are called regions or receptive ﬁelds. There\nare (s−h)/stride + 1 of them, where stride corresponds to the number of words by which we slide\nthe window at each step. With a stride of 1, there are therefore s −h + 1 receptive ﬁelds. The\noutput of the convolution layer for a given ﬁlter is thus a vector o ∈Rs−h+1 whose elements are\ncomputed as:\noi = W · A[i : i + h −1, :]\n(2)\nWhere A[i : i + h −1, :] ∈Rh×d is the ith region matrix, , and · is an operator returning the\nsum of the row-wise dot product of two matrices. Note that for a given ﬁlter, the same W is\napplied to all instantiations of the window regardless of their positions in the document. In other\nwords, the parameters of the ﬁlter are shared across receptive ﬁelds. This is precisely what gives\nthe spatial invariance property to the model, because the ﬁlter is trained to recognize a pattern\nwherever it is located. It also greatly reduces the total number of parameters of the model.\nThen, a nonlinear activation function f, such as ReLU9 (max(0, x)) or tanh (e2x−1\ne2x+1), is applied\nelementwise to o, returning what is known as the feature map c ∈Rs−h+1 associated with the\nﬁlter:\nci = f(oi) + b\n(3)\nWhere b ∈R is a trainable bias.\nFor short sentence classiﬁcation, best region sizes are generally found between 1 and 10, and\nin practice, nf ﬁlters (with nf ∈[100, 600]) are applied to each region to give the model the\nability to learn diﬀerent, complementary features for each region [26]. Since each ﬁlter generates\na feature map, each region is thus embedded into an nf-dimensional space. Moreover, using\nregions of varying size around the optimal one improves performance [26]. In that case, diﬀerent\nparallel branches are created (one for each region size), and the outputs are concatenated after\npooling, as shown in Fig. 1. Performance and cost increase with nf up to a certain point, after\nwhich the model starts overﬁtting.\n5.2.3\nPooling layer\nThe exact positions of the features in the input document do not matter. What matters is only\nwhether certain features are present or absent. For instance, to classify a review as positive,\nwhether “best movie ever” appears at the beginning or at the end of the document is not im-\nportant. To inject such robustness into the model, global k-max pooling10 is employed. This\napproach extracts the k greatest values from each feature map and concatenates them, thus\nforming a ﬁnal vector whose size always remains constant during training. For short sentence\n9compared to tanh, ReLu is aﬀordable (sparsity induced by many zero values in the negative regime) and\nbetter combats the vanishing gradients problem as in the positive regime, the gradient is constant, whereas with\ntanh it becomes increasingly small\n10pooling may also be applied locally over small regions, but for short text classiﬁcation, global pooling works\nbetter [26].\npage 5\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nclassiﬁcation, [26] found that k = 1 was by far superior to higher-order strategies. They also\nreported that using the maximum was much better than using the average, which makes sense,\nsince we’re only interested in extracting the most salient feature from each feature map.\nI \nlike \nthis\nmovie\nvery \nmuch\n!\n2 feature \nmaps for \neach \nregion size \n6 entries\nconcatenated \nto form a \nsingle feature \nvector\n  Sentence matrix\n7 × 5\n3 region sizes: (2,3,4)\n2 ﬁlters for each region \nsize\ntotally 6 ﬁlters\nconvolution \nactivation function\n1-max\npooling\n2 classes\naffine layer with \nsoftmax and \ndropout\nd=5\nFigure 1: CNN architecture for (short) document classiﬁcation, taken from Zhang and Wallace (2015) [26]. s = 7,\nd = 5. 3 regions of respective sizes h =\n\b\n2, 3, 4\n\t\nare considered, with associated output vectors of resp. lengths\ns−h+1 =\n\b\n6, 5, 4\n\t\nfor each ﬁlter (produced after convolution, not shown). There are 2 ﬁlters per region size. For\nthe three region sizes, the ﬁlters are resp. associated with feature maps of lengths\n\b\n6, 5, 4\n\t\n(the output vectors\nafter elementwise application of f and addition of bias). 1-max pooling is used.\n5.2.4\nDocument encoding\nAs shown in Fig. 1, looking at things from a high level, the CNN architecture connects each\nﬁltered version of the input to a single neuron in a ﬁnal feature vector. This vector can be seen\nas an embedding, or encoding, of the input document. It is the main contribution of the model,\nthe thing we’re interested in. The rest of the architecture just depends on the task.\n5.2.5\nSoftmax layer\nSince the goal here is to classify documents, a softmax function is applied to the document\nencoding to output class probabilities. However, diﬀerent tasks would call for diﬀerent architec-\ntures: determining whether two sentences are paraphrases, for instance, would require two CNN\nencoders sharing weights, with a ﬁnal energy function and a contrastive loss (à la Siamese [3]);\nfor translation or summarization, we could use a LSTM language model decoder conditioned on\nthe CNN encoding of the input document (à la seq-to-seq [24]), etc.\nGoing back to our classiﬁcation setting, the softmax transforms a vector x ∈RK into a\nvector of positive ﬂoats that sum to one, i.e., into a probability distribution over the classes to\npage 6\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nbe predicted:\nsoftmax(xi) =\nexi\nPK\nj=1 exj\n(4)\nIn the binary classiﬁcation case, instead of having a ﬁnal output layer of two neurons with a\nsoftmax, where each neuron represents one of the two classes, we can have an output layer with\nonly one neuron and a sigmoid function (σ(x) =\n1\n1+e−x ). In that case, the neuron outputs the\nprobability of belonging to one of the two classes, and decision regarding the class to predict\nis made based on whether σ(x) is greater or smaller than 0.5 (assuming equal priors). These\ntwo approaches are equivalent. Indeed,\n1\n1+e−x =\nex\nex+e0 . So, the one-neuron sigmoid layer can\nbe viewed as a two-neuron softmax layer where one of the neurons never activates and has its\noutput always equal to zero.\n5.3\nNumber of parameters\nThe total number of trainable parameters for our CNN is the sum of the following terms:\n• word embedding matrix (only if non-static mode): (V + 1) × d, where V is the size of\nthe vocabulary. We add one row for the zero-padding vector.\n• convolution layer: h × d × nf + nf (the number of entries in each ﬁlter by the number\nof ﬁlters, plus the biases).\n• softmax layer: nf × 1 + 1 (fully connected layer with an output dimension of 1 and one\nbias).\n5.4\nVisualizing and understanding inner representations and predictions\n5.4.1\nDocument embeddings\nA fast and easy way to verify that our model is learning eﬀectively is to check whether its\ninternal document representations make sense. Recall that the feature vector which is fed to the\nsoftmax layer can be seen as an nf-dimensional encoding of the input document. By collecting\nthe intermediate output of the model at this precise level in the architecture for a subset of\ndocuments, and projecting the vectors to a low-dimensional map, we can thus visualize whether\nthere is any correlation between the embeddings and the labels. Fig.s 2 and 3 prove that indeed,\nour model is learning meaningful representations of documents.\n30\n20\n10\n0\n10\n20\n30\n30\n20\n10\n0\n10\n20\n30\n0\n1\nt-SNE visualization of CNN-based doc embeddings \n (first 1000 docs from test set)\nFigure 2:\nDoc embeddings before training.\n30\n20\n10\n0\n10\n20\n30\n30\n20\n10\n0\n10\n20\n30\n0\n1\nt-SNE visualization of CNN-based doc embeddings \n (first 1000 docs from test set)\nFigure 3:\nDoc embeddings after 2 epochs.\npage 7\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n5.4.2\nPredictive regions identiﬁcation\nThis approach is presented in section 3.6 (Tables 5 & 6) of [13]. Recall that before we lose posi-\ntional information by applying pooling, each of the nf ﬁlters of size h is associated with a vector\nof size (s−h)/stride + 1 (a feature map) whose entries represent the output of the convolution of\nthe ﬁlter with the corresponding receptive ﬁeld in the input, after application of the nonlinearity\nand addition of the bias. Therefore, each receptive ﬁeld is embedded into an nf-dimensional\nspace. Thus, after training, we can identify the regions of a given document that are the most\npredictive of its category by inspecting the intermediate output of the model corresponding to\nthe receptive ﬁeld embeddings (right before the pooling layer), and by ﬁnding the regions that\nhave the highest norms. For instance, some of the most predictive regions for negative IMDB\nreviews are: “worst movie ever”, “don’t waste your money”, “poorly written and acted”, “awful\npicture quality”. Conversely, some regions very indicative of positivity are: “amazing sound-\ntrack”, “visually beautiful”, “cool journey”, “ending quite satisfying”...\n5.4.3\nSaliency maps\nAnother way to understand how the model is issuing its predictions was described by [22] and\napplied to NLP by [17]. The idea is to rank the elements of the input document A ∈Rs×d based\non their inﬂuence on the prediction. An approximation can be given by the magnitudes of the\nﬁrst-order partial derivatives of the output of the model CNN : A 7→CNN(A) with respect to\neach row a of A:\nsaliency(a) =\n\f\f\f\f\n∂(CNN)\n∂a\n|a\n\f\f\f\f\n(5)\nThe interpretation is that we identify which words in A need to be changed the least to change the\nclass score the most. The derivatives can be obtained by performing a single back-propagation\npass (based on the prediction, not the loss like during training). Fig.s 4 and 5 show saliency map\nexamples for negative and positive reviews, respectively.\n0\n50\n100\n150\n200\n250\nmost\nvisually\nbeautiful\never\nseen\nmy\nlife\nmuch\nlearn\nhere\nhow\nplay\ncamera\ncolor\ncostumes\nset\nup\nshot\nwork\nwent\ninto\nofficial\nweb\nsites\nenglish\nfrench\nalso\ngive\nidea\nsheer\nbeauty\ncontained\n0.000\n0.005\n0.010\n0.015\n0.020\nFigure 4:\nSaliency map for document 1 of the IMDB test set (true label: positive)\npage 8\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n0\n50\n100\n150\n200\n250\nking\nalive\nflawed\ncontrived\nmess\nwhich\ncomes\noff\nself\nindulgent\nexcuse\ntransport\nbunch\nactors\ndesert\nwhere\ncan\nplay\nmajor\nconceived\nsilly\noverwrought\ndrama\nutter\ndisregard\nreal\nforces\nhuman\nnature\nmarket\nappeal\ncommon\nsense\neven\nart\nother\nwords\nflick\ndon't\nbelieve\nyour\nwatching\nonly\ncritics\n0.00000\n0.00005\n0.00010\n0.00015\n0.00020\nFigure 5:\nSaliency map for document 15 of the IMDB test set (true label: negative)\n6\nRecurrent Neural Networks (RNNs)\nWe ﬁrst present the overall RNN framework, and then two types of units widely used in practice:\nthe LSTM and the GRU. A good review of RNNs, LSTMs and their applications can be found\nin [19].\n6.1\nRNN framework\nWhile CNNs are naturally good at dealing with grids, RNNs were speciﬁcally developed to\nbe used with sequences [6]. Some examples include time series, or, in NLP, words (sequences of\ncharacters) or sentences (sequences of words). CNNs do allow to capture some order information,\nbut it is limited to local patterns, and long-range dependencies are ignored [8]. As shown in Fig.\n6, a RNN can be viewed as a chain of simple neural layers that share the same parameters.\nFigure 6:\n3 steps of an unrolled RNN (adapted from Denny Britz’ blog. Each circle represents a RNN\nunit (see equations 6 & 7).\nFrom a high level, a RNN is fed an ordered list of input vectors\n\b\nx1, ..., xT\n\t\nas well as an initial\nhidden state h0 initialized to all zeros, and returns an ordered list of hidden states\n\b\nh1, ..., hT\n\t\n, as\nwell as an ordered list of output vectors\n\b\ny1, ..., yT\n\t\n. The output vectors may serve as input for\nother RNN units, when considering deep architectures (multiple RNN layers stacked vertically,\nas shown in Fig. 7). The hidden states correspond more or less to the “short-term” memory of\nthe network. Note that each training example is a full\n\b\nx1, ..., xT\n\t\nsequence of its own, and may\nbe associated with a label depending on the task. E.g., for short document classiﬁcation, the\npage 9\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nsequences would be associated with a label, whereas for language modeling, we would just parse\nall sequences, repeatedly predicting the next words.\n. . . \n. . . \n. . . \n. . . \n. . . \n. . . \n𝑥𝑡−1\n𝑥𝑡\n𝑥𝑡+1\nℎ𝑡−1\n1\nℎ𝑡−1\n2\nℎ𝑡−1\n3\nℎ𝑡\n1\nℎ𝑡\n2\nℎ𝑡\n3\nℎ𝑡+1\n1\nℎ𝑡+1\n2\nℎ𝑡+1\n3\n𝑦𝑡−1\n𝑦𝑡\n𝑦𝑡+1\nFigure 7: 3 steps of an unrolled deep RNN. Each circle represents a RNN unit. The hidden state of each\nunit in the inner layers (1 & 2) serves as input to the corresponding unit in the layer above.\nAt any step t in the sequence, the hidden state ht is deﬁned in terms of the previous hidden state\nht−1 and the current input vector xt in the following recursive way:\nht = f(Uxt + Wht−1 + b)\n(6)\nWhere f is a nonlinearity such as tanh (applied elementwise), xt ∈Rdin, U ∈RH×din and\nW ∈RH×H are parameter matrices shared by all time steps, and ht, ht−1 and b belong to RH.\ndin can be the size of the vocabulary, if one-hot vectors are passed as input, or the dimensionality\nof the embedding space, when working with shared features. H is the dimension of the hidden\nlayer. Usually, H ∼100. The larger this layer, the greater the capacity of the memory, with an\nincrease in computational cost.\nThe output vector yt ∈Rdout transforms the current hidden state ht ∈RH in a way that\ndepends on the ﬁnal task. For classiﬁcation, it is computed as:\nyt = softmax(V ht)\n(7)\nWhere V ∈Rdout×H is a parameter matrix shared across all time steps. dout depends on the\nnumber of categories. E.g., for 3-class document classiﬁcation, dout = 3, for a word-level language\nmodel, dout = |V |.\nNote that when stacking multiple RNN layers vertically (deep RNN architecture), the hidden\nstates of the units below are directly connected to the units above, i.e., xtabove = ytbelow and\nytbelow = htbelow. The output layer (Eq. 7) lies on top of the stack.\n6.1.1\nLanguage modeling\nLanguage modeling is a special case of classiﬁcation where the model is trained to predict the next\nword or character in the sentence. At each time step t, the output vector gives the probability\ndistribution of xt over all the words/characters in the vocabulary, conditioned on the previous\nwords/characters in the sequence, that is, P[xt|xt−1, ..., x1]. At test time, the probability of a\nfull sequence {x1, ..., xT } is given by the product of all conditional probabilities as:\nP\n\u0002\n{x1, ..., xT }\n\u0003\n= P[x1] ×\nT\nY\nt=2\nP[xt|xt−1, ..., x1]\n(8)\nThe language model can also be used to generate text of arbitrary size by repeatedly sampling\ncharacters for the desired number of time steps (for character-level granularity) or until the\npage 10\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nspecial end-of-sentence token is selected11 (for word-level granularity).\nFor a character-level language model for instance, T can easily exceed 20 or 25. This greatly\nampliﬁes the adverse eﬀects of the well-known vanishing and exploding gradients problem, which\nprevents long-range dependencies from being learned12. Note that this issue can also be expe-\nrienced with feed-forward neural networks, such as the Multi-Layer Perceptron, but it just gets\nworse with RNN due to their inherent tendency to be deep.\n6.2\nLSTM unit\nIn practice, whenever people use RNNs, they use the LSTM or the GRU unit (see next sub-\nsection), as these cells are engineered in a way that allows them to escape vanishing/exploding\ngradients and keep track of information over longer time periods [11].\nAs shown in Fig. 8, the two things that change in the LSTM unit compared to the basic RNN\nunit are (1) the presence of a cell state (ct), which serves as an explicit memory, and (2) how\nhidden states are computed. With vanilla RNNs, the hidden state is computed with a single\nlayer as ht = tanh(Uxt +Wht−1 +b) (see eq. 6). With the LSTM unit however, the hidden state\nis computed by four interacting layers that give the network the ability to remember or forget\nspeciﬁc information about the preceding elements in the sequence.\nFigure 8: The LSTM unit. Adapted from Chris Colah’s blog.\n6.2.1\nInner layers\nThe four layers are:\n1. forget gate layer: ft = σ\n\u0000Ufxt + Wfht−1 + bf\n\u0001\n2. input gate layer: it = σ\n\u0000Uixt + Wiht−1 + bi\n\u0001\n3. candidate values computation layer: ˜ct = tanh\n\u0000Ucxt + Wcht−1 + bc\n\u0001\n4. output gate layer: ot = σ\n\u0000Uoxt + Woht−1 + bo\n\u0001\n11see [9] and http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n12wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-\nvanishing-gradients/\npage 11\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nThanks to the elementwise application of the sigmoid function (σ), the forget, input, and output\ngate layers (1, 2, and 4 above) generate vectors whose entries are all comprised between 0 and\n1, and either close to 0 or close to 1. When one of these layers is multiplied with another vector,\nit thus acts as a ﬁlter that only selects a certain proportion of that vector. This is precisely\nwhy those layers are called gates. The two extreme cases are when all entries are equal to 1 -the\nfull vector passes- or to 0 -nothing passes. Note that the 3 forget, input, and output gates are\ncomputed in the exact same way, only the parameters vary. The parameters are however shared\nacross all time steps.\n6.2.2\nForgetting/learning\nBy taking into account the new training example xt and the current hidden state ht−1, the\nforget gate layer ft determines how much of the previous cell state ct−1 should be forgotten\n(what fraction of the memory should be freed up), while from the same input, the input gate\nlayer it decides how much of the candidate values ˜ct should be written to the memory, or in other\nwords, how much of the new information should be learned. Combining the output of the two\nﬁlters updates the cell state:\nct = ft ◦ct−1 + it ◦˜ct\n(9)\nWhere ◦denotes elementwise multiplication (Haddamard product). This way, important in-\nformation is not overwritten by the new inputs but is able to be kept alongside them for long\nperiods of time. Finally, the activation ht is computed from the updated memory, modulated by\nthe output gate layer ot:\nht = tanh\n\u0000ct\n\u0001\n◦ot\n(10)\nThe output gate allows the unit to only activate when the in-memory information is found to be\nrelevant for the current time step. Finally, as before with the simple RNN, the output vector is\ncomputed as a function of the new hidden state:\nyt = softmax(V ht)\n(11)\n6.2.3\nVanilla RNN analogy\nIf we decide to forget everything about the previous state (all elements of ft are null), to learn\nall of the new information (all elements of it are equal to 1), and to memorize the entire cell\nstate to pass to the next time step (all elements of ot are equal to 1), we have ct = ˜ct =\ntanh\n\u0000Ucxt + Wcht−1 + bc\n\u0001\n, and thus we go back to a vanilla RNN unit, the only diﬀerence\nbeing an additional tanh, as we end up with ht = tanh\n\u0000tanh\n\u0000Ucxt + Wcht−1 + bc\n\u0001\u0001\ninstead of\nht = tanh\n\u0000Ucxt + Wcht−1 + bc\n\u0001\nlike in the classical RNN case.\n6.3\nGated Recurrent Unit (GRU)\nAs shown in Fig. 9, the GRU unit [2] is a simpliﬁed LSTM unit with only two gates (reset and\nupdate), and where there is no explicit memory ct.\n1. reset gate layer: rt = σ\n\u0000Urxt + Wrht−1 + br\n\u0001\n2. update gate layer: zt = σ\n\u0000Uzxt + Wzht−1 + bz\n\u0001\npage 12\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nFigure 9: GRU unit. Taken from Chris Colah’s blog.\nThe candidate hidden state is computed as:\n˜ht = tanh\n\u0000Uhxt + Wh(rt ◦ht−1) + bh\n\u000113\n(12)\nWhen all elements of the reset gate approach zero, information from the previous time steps\n(stored in ht−1) is discarded, and the candidate hidden state is thus only based on the current\ninput xt. The new hidden state is ﬁnally obtained in a way similar to that of the LSTM cell\nstate, by linearly interpolating between the previous hidden state and the candidate one:\nht = zt ◦ht−1 + (1 −zt) ◦˜ht\n(13)\nthe only diﬀerence is that this time, the update gate zt serves as the forget gate and determines\nthe fraction of information from the previous hidden state that should be forgotten, and the\ninput gate is coupled on the forget gate.\n6.4\nRNN vs LSTM vs GRU\nThe basic RNN unit exposes its full hidden state at every time step (see Eq. 6), so as time goes\nby, the impact of older inputs is quickly replaced by that of the more recent ones. The RNN is\ntherefore not able to remember important features for more than a few steps. Indeed, we have\nshown previously that a RNN is analogous to a LSTM where for all t, ft = ⃗0, it = ⃗1, and ot = ⃗1\n(we forget everything about the past and learn everything about the present).\nOn the other hand, thanks to the use of an explicit memory (the cell) and a gating mechanism,\nthe LSTM unit is able to control which fraction of information from the past should be kept in\nmemory (forget gate ft), which fraction of information from the current input should be written\nto memory (input gate it), and how much of the memory should be exposed to the next time\nsteps and to the units in the higher layers (output gate ot).\nThe GRU also features a gating mechanism, but has no explicit memory (no cell state). As a\nresult, the gating mechanism of the GRU is simpler, without output gate: the linear interpolation\nbetween the old and the new information is directly injected into the new hidden state without\nﬁltering (see Eq. 13). Another diﬀerence is that when computing the candidate values, the GRU,\nvia its reset gate rt, modulates the ﬂow of information coming from the previous activation ht−1\n(see Eq. 12), while in the LSTM unit, ˜ct is based on the raw ht−1. Last but not least, in the\nGRU, the balance between the old and the new information is only made by the update gate zt\n13It should be noted that the original formulation of [2] uses rt ◦(Whht−1). Here, we adopt the formulation of\n[4], Wh(rt ◦ht−1). According to [4], the two formulations perform equivalently.\npage 13\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n(see Eq. 13), whereas the LSTM unit has two independent forget and input gates.\nWhile both the LSTM and GRU units are clearly superior to the basic RNN unit [4], there is\nno evidence about which one is best [10, 4]. However, since the GRU is simpler, it is easier to\nimplement, more eﬃcient, and has less parameters so it requires less training data.\n7\nAttention\nThe attention mechanism [1] was developed in the context of encoder-decoder architectures for\nNeural Machine Translation (NMT) [2, 24], and rapidly applied to naturally related tasks such\nas image captioning (translating an image to a sentence) [25], and summarization (translating to\na more compact language) [21]. From a high-level, by allowing the decoder to shop for what it\nneeds over multiple vectors, attention relieves the encoder from the burden of having to embed\nthe input into a single ﬁxed-length vector, and thus allows to keep much more information [1].\nToday, attention is ubiquitous in deep learning models, and is not used only in encoder-\ndecoder contexts.\nNotably, attention devices have been proposed for encoders only, to solve\ntasks such as document classiﬁcation [27] or representation learning [5]. Such mechanisms are\nqualiﬁed as self or inner attention.\nIn what follows, we will start by presenting attention in the original context of encoder-\ndecoder for NMT, using the general framework introduced by [20], and then introduce self-\nattention.\n7.1\nEncoder-decoder attention\n7.1.1\nEncoder-decoder overview\nFrom a very high level, as shown in Fig. 10, the encoder embeds the input into a vector, and\nthe decoder generates some output from this vector.\nFigure 10: Overview of the encoder-decoder architecture.\nTaken from https://sites.google.com/\nsite/acl16nmt/home\nIn Neural Machine Translation (NMT), the input and the output are sequences of words, re-\nspectively x =\n\u0000x1, . . . , xTx\n\u0001\nand y =\n\u0000y1, . . . , yTy\n\u0001\n. x and y are usually referred to as the source\nand target sentences. When both the input and the output are sequences, encoder-decoder ar-\nchitectures are sometimes called sequence-to-sequence (seq2seq) [24]. Thanks to the fact that\nencoder-decoder architectures are diﬀerentiable everywhere, their parameters θ can be simulta-\nneously optimized with maximum likelihood estimation (MLE) over a parallel corpus. This way\nof training is called end-to-end.\nargmaxθ\n(\nX\n(x,y)∈corpus\nlog p(y|x; θ)\n)\n(14)\nHere, the function that we want to maximize is the log probability of a correct translation.\npage 14\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n7.1.2\nEncoder\nThe source sentence can be embedded by any model (e.g., CNN, fully connected). Usually for MT\nthough, the encoder is a deep RNN. Bahdanau et al. [1] originally used a bidirectional deep RNN.\nSuch a model is made of two deep unidirectional RNNs, with diﬀerent parameters except the\nword embedding matrix. The ﬁrst forward RNN processes the source sentence from left to right,\nwhile the second backward RNN processes it from right to left. The two sentence embeddings\nare concatenated at each time step t to obtain the inner representation of the bidirectional RNN:\nht =\n\u0002⃗ht;\n⃗\nht\n\u0003\n(15)\nThe bidirectional RNN takes into account the entire context when encoding the source words,\nnot just the preceding words. As a result, ht is biased towards a small window centered on\nword xt, while with a unidirectional RNN, ht is biased towards xt and the words immediately\npreceding it. Focusing on a small window around xt may be advantageous, but does not seem\ncrucial. Indeed, Luong et al. [20] obtained state-of-the-art results with a usual unidirectional\ndeep RNN encoder. In what follows, the hidden states of the encoder will be written ¯ht. They\nare sometimes called annotations in the literature.\n7.1.3\nDecoder\nWhile diﬀerent models can be used as the encoder, in NMT the decoder is usually a unidirec-\ntional RNN because this model is naturally adapted to the sequential nature of the generation\ntask, and is usually deep (stacking). The decoder generates each word of the target sentence one\nstep at a time.\nKey idea. Making the decoder use only the last annotation hTx produced by the encoder to\ngenerate output forces the encoder to ﬁt as much information as possible into hTx. Since hTx\nis a single ﬁxed-size vector, its capacity is limited, so some information is lost. On the other\nhand, the attention mechanism allows the decoder to consider the entire sequence\n\u0000h1, . . . , hTx\n\u0001\nof annotations produced by the encoder at each step of the generation process. As a result,\nthe encoder is able to keep much more information by distributing it among all its annotations,\nknowing that the decoder will be able to decide later on which vectors it should pay attention\nto.\nMore precisely, the target sentence y = (y1, . . . , yTy) is generated one word yt at a time based on\nthe distribution:\nP\n\u0002\nyt|{y1, ..., yt−1}, ct\n\u0003\n= softmax\n\u0000Ws˜ht\n\u0001\n(16)\nwhere ˜ht, the attentional hidden state, is computed as:\n˜ht = tanh\n\u0000Wc\n\u0002\nct; ht\n\u0003\u0001\n(17)\nht is the hidden state of the decoder (hidden state of the top layer, when the decoder is a stack-\ning RNN) and provides information about the previously generated target words {y1, ..., yt−1},\nct is the source context vector, and\n\u0002\n;\n\u0003\nis concatenation. Ws and Wc are matrices of trainable\nparameters. Biases are not shown for simplicity. As shown in Fig. 11, the context vector ct\ncan be computed in two ways: globally and locally. We describe each approach in the next two\nsubsections.\nA note on beam search.\nTrying all possible combinations of words in the vocabulary to\nﬁnd the target sentence with highest joint probability is intractable. But on the other hand,\ngenerating y in a purely greedy way, i.e., by selecting the most likely word every time, is highly\nsuboptimal. In practice, a certain number K of candidate translations are explored with beam\npage 15\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nsearch, a heuristic search algorithm [7]. Large values of K generate better target sentences, but\ndecrease decoding speed.\nFigure 11: Global (left) vs local attention (right). Adapted from [20].\n7.1.4\nGlobal attention\nHere, the context vector ct is computed as a weighted sum of the full list of annotations ¯hi of\nthe source sentence (i.e., the hidden states of the encoder). There are Tx annotations. Each one\nis a vector of size the number of units in the hidden layer of the encoder. ct has same size as any\nannotation. The size of the alignment vector αt is equal to the size Tx of the source sentence, so\nit is variable.\nct =\nTx\nX\ni=1\nαt,i¯hi\n(18)\nThe alignment vector αt is computed by applying a softmax to the output of an alignment\noperation (score()) between the current target hidden state ht and all source hidden states ¯hi’s:\nαt,i =\nexp\n\u0000score(ht, ¯hi)\n\u0001\nPTx\ni′=1 exp\n\u0000score(ht, ¯hi′)\n\u0001\n(19)\nIn other words, αt is a probability distribution over all source hidden states (its coeﬃcients\nare all between 0 and 1 and sum to 1), and indicates which words in the source sentence are\nthe most likely to help in predicting the next word. score() can in theory be any comparison\nfunction. Luong et al. [20] experimented with the dot product (score(ht, ¯hi) = h⊤\nt ¯hi), a more\ngeneral formulation with a matrix of parameters (score(ht, ¯hi) = h⊤\nt Wα¯hi), and a fully connected\nlayer. They found that dot works better for global attention while general is superior for local\nattention. A summary of global attention is provided in Fig. 12.\npage 16\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nFigure 12: Summary of the global attention mechanism [20].\n7.1.5\nLocal attention\nConsidering all words in the source sentence to generate every single target word is expensive,\nand may not be necessary. To remediate this issue, Luong et al. [20] proposed to focus only on\na small window of annotations of ﬁxed size 2D + 1:\nct =\npt+D\nX\ni=pt−D\nαt,i¯hi\n(20)\nD is prescribed by the user, and the position pt where to center the window is either set to t\n(monotonic alignment) or determined by a diﬀerentiable mechanism (predictive alignment) based\non information about the previously generated target words {y1, ..., yt−1} stored in ht:\npt = Tx · σ\n\u0000v⊤\np tanh(Wpht)\n\u0001\n(21)\nWhere Tx is the length of the source sentence, σ is the sigmoid function, and vp and Wp are\ntrainable parameters. Alignment weights are computed like in the case of global attention (Eq.\n19), with the addition of a Normal distribution term centered on pt and with standard deviation\nD/2:\nαt,i =\nexp\n\u0000score(ht, ¯hi)\n\u0001\nPpt+D\ni′=pt−D exp\n\u0000score(ht, ¯hi′)\n\u0001exp\n\u0010\n−(i −pt)2\n2(D/2)2\n\u0011\n(22)\nNote that pt ∈R∩\n\u0002\n0, Tx\n\u0003\nand i ∈N∩\n\u0002\npt −D, pt +D]. The addition of the Gaussian term makes\nthe alignment weights decay as i moves away from the center of the window pt, i.e., it gives more\nimportance to the annotations near pt. Also, unlike with global attention, the size of αt is ﬁxed\nand equal to 2D + 1, as only the annotations within the window are taken into account. Local\nattention can actually be viewed as global attention where alignment weights are multiplied by a\ntruncated Normal distribution (i.e., that returns zero outside the window). A summary of local\nattention is provided in Fig. 13.\npage 17\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nFigure 13: Summary of the local attention with predictive alignment mechanism [20].\n7.2\nSelf-attention\nWe are here in a simpler setting with a single RNN encoder taking as input a sequence\n\u0000x1, . . . , xT\n\u0001\nof length T. As usual, the RNN maps the input sequence to a sequence of annotations\n\u0000h1, . . . , hT\n\u0001\n.\nThe goal is exactly the same as with attention in the encoder-decoder context: rather than con-\nsidering the last annotation hT as a comprehensive summary of the entire sequence, which is\nprone to information loss, a new hidden representation is computed by taking into account the\nannotations at all time steps. To this purpose, the self-attention or inner attention mechanism\nemerged in the literature in 2016/2017, with, e.g., [27, 18]. In what follows we use the formulation\nof [27].\nAs shown in Eq. 23, annotation ht is ﬁrst passed to a dense layer. An alignment coeﬃcient\nαt is then derived by comparing the output ut of the dense layer with a trainable context vector\nu (initialized randomly) and normalizing with a softmax. The attentional vector s is ﬁnally\nobtained as a weighted sum of the annotations.\nut = tanh(Wht)\nαt =\nexp(score(ut, u))\nPT\nt′=1 exp(score(ut′, u))\ns =\nT\nX\nt=1\nαtht\n(23)\nscore can in theory be any alignment function. A straightforward approach is to use score(ut, u) =\nu⊤\nt u. The context vector can be interpreted as a representation of the optimal word, on average.\nWhen faced with a new example, the model uses this knowledge to decide which word it should\npay attention to. During training, through backpropagation, the model updates the context\nvector, i.e., it adjusts its internal representation of what the optimal word is.\n7.2.1\nDiﬀerence with seq2seq attention\nThe context vector in the deﬁnition of self-attention above has nothing to do with the context\nvector used in seq2seq attention! In seq2seq, the context vector ct is equal to the weighted sum\nPTx\ni=1 αt,i¯hi, and is used to compute the attentional hidden state as ˜ht = tanh\n\u0000Wc\n\u0002\nct; ht\n\u0003\u0001\n. In\npage 18\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\nself-attention however, the context vector is simply used as a replacement for the hidden state\nof the decoder when performing the alignment with score(), since there is no decoder. So, in\nself-attention, the alignment vector α indicates the similarity of each input word with respect\nto the optimal word (on average), while in seq2seq attention, α indicates the relevance of each\nsource word in generating the next element of the target sentence.\n7.2.2\nHierarchical attention\nA simple, good example of how self-attention can be useful in practice is provided by the ar-\nchitecture illustrated in Fig. 14. In this architecture, the self-attention mechanism comes into\nplay twice: at the word level, and at the sentence level. This approach makes sense for two\nreasons: ﬁrst, it matches the natural hierarchical structure of documents (words →sentences\n→document). Second, in computing the encoding of the document, it allows the model to ﬁrst\ndetermine which words are important in each sentence, and then, which sentences are impor-\ntant overall. Through being able to re-weigh the word attentional coeﬃcients by the sentence\nattentional coeﬃcients, the model captures the fact that a given instance of word may be very\nimportant when found in a given sentence, but another instance of the same word may not be\nthat important when found in another sentence.\nsentence 1\nsentence 2\nsentence N\n…\nsentence encoder\nsentence encoder\nsentence encoder\n \n \nsentence annotations\ndocument vector\n…\nself-attention\nbidirGRU\nsentence vectors\n…\nword 1  word 2  word 3  ...  word T\nbidirGRU\n  \nword \nvectors\nword \nannotations\nself-attention\nsentence vector\n…\n…\ndocument encoder\nsentence encoder\nword vectors\nFigure 14: Hierarchical attention architecture [27].\nReferences\n[1] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by\njointly learning to align and translate.\" arXiv preprint arXiv:1409.0473 (2014). 2, 14, 15\n[2] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H.,\nBengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical\nmachine translation. arXiv preprint arXiv:1406.1078. 2, 12, 13, 14\n[3] Chopra, Sumit, Raia Hadsell, and Yann LeCun. \"Learning a similarity metric discrimina-\ntively, with application to face veriﬁcation.\" Computer Vision and Pattern Recognition, 2005.\nCVPR 2005. IEEE Computer Society Conference on. Vol. 1. IEEE, 2005. 6\n[4] Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recur-\nrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555. 2, 13, 14\npage 19\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n[5] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning\nof universal sentence representations from natural language inference data. arXiv preprint\narXiv:1705.02364. 14\n[6] Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14:2, 179-211. 9\n[7] Freitag, Markus, and Yaser Al-Onaizan. \"Beam search strategies for neural machine transla-\ntion.\" arXiv preprint arXiv:1702.01806 (2017). 16\n[8] Goldberg, Y. (2015). A primer on neural network models for natural language processing.\nJournal of Artiﬁcial Intelligence Research, 57, 345-420. 2, 3, 9\n[9] Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850. 11\n[10] Greﬀ, Klaus, et al. \"LSTM: A search space odyssey.\" IEEE transactions on neural networks\nand learning systems 28.10 (2017): 2222-2232. 14\n[11] Hochreiter, S., Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8),\n1735 11\n[12] Hubel, David H., and Torsten N. Wiesel (1962). Receptive ﬁelds, binocular interaction and\nfunctional architecture in the cat’s visual cortex. The Journal of physiology 160.1:106-154. 4\n[13] Johnson, R., Zhang, T. (2015). Eﬀective Use of Word Order for Text Categorization with\nConvolutional Neural Networks. To Appear: NAACL-2015, (2011). 2, 8\n[14] Kim, Y. (2014). Convolutional Neural Networks for Sentence Classiﬁcation. Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),\n1746–1751. 2, 3, 4\n[15] Krizhevsky, Alex, Ilya Sutskever, and Geoﬀrey E. Hinton. \"Imagenet classiﬁcation with deep\nconvolutional neural networks.\" Advances in neural information processing systems. 2012. 4\n[16] LeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998). Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11), 2278-2324. 2, 4\n[17] Li, J., Chen, X., Hovy, E., and Jurafsky, D. (2015). Visualizing and understanding neural\nmodels in nlp. arXiv preprint arXiv:1506.01066. 8\n[18] Lin, Zhouhan, et al. \"A structured self-attentive sentence embedding.\" arXiv preprint\narXiv:1703.03130 (2017). 18\n[19] Lipton, Zachary C., John Berkowitz, and Charles Elkan. \"A critical review of recurrent\nneural networks for sequence learning.\" arXiv preprint arXiv:1506.00019 (2015). 9\n[20] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \"Eﬀective approaches to\nattention-based neural machine translation.\" arXiv preprint arXiv:1508.04025 (2015). 2, 14,\n15, 16, 17, 18\n[21] Rush, Alexander M., Sumit Chopra, and Jason Weston. \"A neural attention model for\nabstractive sentence summarization.\" arXiv preprint arXiv:1509.00685 (2015). 14\n[22] Simonyan, K., Vedaldi, A., and Zisserman, A. (2013). Simonyan, Karen, Andrea Vedaldi,\nand Andrew Zisserman. \"Deep inside convolutional networks: Visualising image classiﬁ-\ncation models and saliency maps.\" arXiv preprint arXiv:1312.6034 (2013). arXiv preprint\narXiv:1312.6034. 8\npage 20\nNotes on Deep Learning for NLP\nAntoine Tixier, August 2018\n[23] Springenberg, Jost Tobias, et al. \"Striving for simplicity: The all convolutional net.\" arXiv\npreprint arXiv:1412.6806 (2014). 4\n[24] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural\nnetworks.\" Advances in neural information processing systems. 2014. 2, 6, 14\n[25] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., ... & Bengio, Y. (2015,\nJune). Show, attend and tell: Neural image caption generation with visual attention. In\nInternational Conference on Machine Learning (pp. 2048-2057). 14\n[26] Zhang, Ye, and Byron Wallace. \"A sensitivity analysis of (and practitioners’ guide to) convo-\nlutional neural networks for sentence classiﬁcation.\" arXiv preprint arXiv:1510.03820 (2015).\n2, 3, 5, 6\n[27] Yang, Zichao, et al. \"Hierarchical attention networks for document classiﬁcation.\" Proceed-\nings of the 2016 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies. 2016. 14, 18, 19\npage 21\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-08-29",
  "updated": "2018-08-30"
}