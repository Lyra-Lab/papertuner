{
  "id": "http://arxiv.org/abs/1809.01694v2",
  "title": "Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction",
  "authors": [
    "Kazuma Hashimoto",
    "Yoshimasa Tsuruoka"
  ],
  "abstract": "A major obstacle in reinforcement learning-based sentence generation is the\nlarge action space whose size is equal to the vocabulary size of the\ntarget-side language. To improve the efficiency of reinforcement learning, we\npresent a novel approach for reducing the action space based on dynamic\nvocabulary prediction. Our method first predicts a fixed-size small vocabulary\nfor each input to generate its target sentence. The input-specific vocabularies\nare then used at supervised and reinforcement learning steps, and also at test\ntime. In our experiments on six machine translation and two image captioning\ndatasets, our method achieves faster reinforcement learning ($\\sim$2.7x faster)\nwith less GPU memory ($\\sim$2.3x less) than the full-vocabulary counterpart.\nThe reinforcement learning with our method consistently leads to significant\nimprovement of BLEU scores, and the scores are equal to or better than those of\nbaselines using the full vocabularies, with faster decoding time ($\\sim$3x\nfaster) on CPUs.",
  "text": "Accelerated Reinforcement Learning for Sentence Generation\nby Vocabulary Prediction\nKazuma Hashimoto∗\nSalesforce Research\nk.hashimoto@salesforce.com\nYoshimasa Tsuruoka\nThe University of Tokyo\ntsuruoka@logos.t.u-tokyo.ac.jp\nAbstract\nA major obstacle in reinforcement learning-\nbased sentence generation is the large action\nspace whose size is equal to the vocabulary\nsize of the target-side language. To improve\nthe efﬁciency of reinforcement learning, we\npresent a novel approach for reducing the ac-\ntion space based on dynamic vocabulary pre-\ndiction.\nOur method ﬁrst predicts a ﬁxed-\nsize small vocabulary for each input to gen-\nerate its target sentence.\nThe input-speciﬁc\nvocabularies are then used at supervised and\nreinforcement learning steps, and also at test\ntime. In our experiments on six machine trans-\nlation and two image captioning datasets, our\nmethod achieves faster reinforcement learning\n(∼2.7x faster) with less GPU memory (∼2.3x\nless) than the full-vocabulary counterpart. We\nalso show that our method more effectively re-\nceives rewards with fewer iterations of super-\nvised pre-training.\n1\nIntroduction\nSentence generation with neural networks plays\na key role in many language processing tasks,\nincluding machine translation (Sutskever et al.,\n2014), image captioning (Lin et al., 2014), and\nabstractive summarization (Rush et al., 2015).\nThe most common approach for learning the sen-\ntence generation models is maximizing the like-\nlihood of the model on the gold-standard target\nsentences.\nRecently, approaches based on rein-\nforcement learning have attracted increasing at-\ntention to reduce the gap between training and\ntest situations and to directly incorporate task-\nspeciﬁc and more ﬂexible evaluation metrics such\nas BLEU scores (Papineni et al., 2002) into opti-\nmization (Ranzato et al., 2016).\nWhile reinforcement learning-based sentence\ngeneration is appealing, it is often too computa-\n∗Work was done while the ﬁrst author was working at\nthe University of Tokyo.\ntionally demanding to be used with large training\ndata. In reinforcement learning for sentence gen-\neration, selecting an action corresponds to select-\ning a word in the vocabulary V . The number of\npossible actions at each time step is thus equal to\nthe vocabulary size, which often exceeds tens of\nthousands. Among such a large set of possible ac-\ntions, at most N actions are selected if the length\nof the generated sentence is N, where we can as-\nsume N ≪|V |. In other words, most of the pos-\nsible actions are not selected, and the large action\nspace slows down reinforcement learning and con-\nsumes a large amount of GPU memory.\nIn this paper, we propose to accelerate rein-\nforcement learning by reducing the large action\nspace. The reduction of action space is achieved\nby predicting a small vocabulary for each source\ninput. Our method ﬁrst constructs the small input-\nspeciﬁc vocabulary by selecting K (≤1000) rele-\nvant words, and then the small vocabulary is used\nat both training and test time.\nOur experiments on six machine translation\nand two image captioning datasets show that our\nmethod enables faster reinforcement learning with\nless GPU memory than the standard full softmax\nmethod, without degrading the accuracy of the\nsentence generation tasks. Our method also works\nfaster at test time, especially on CPUs. The imple-\nmentation of our method is available at https:\n//github.com/hassyGo/NLG-RL.\n2\nREINFORCE with Small Vocabularies\nWe ﬁrst describe a neural machine translation\nmodel and an image captioning model as examples\nof sentence generation models. Machine transla-\ntion is a text-to-text task, and image captioning is\nan image-to-text task. We then review how rein-\nforcement learning is used, and present a simple\nand efﬁcient method to accelerate the training.\narXiv:1809.01694v2  [cs.CL]  4 Apr 2019\n2.1\nSentence Generation Models\nRecurrent Neural Networks (RNNs) are widely\nused to generate sentences by outputting words\none by one (Sutskever et al., 2014).\nTo gener-\nate a sentence Y = (y1, y2, . . . , yN), where N is\nits length, given a source input X, a hidden state\nht ∈Rd is computed for each time step t (≥1) by\nusing its previous information:\nht = RNN (ht−1, e(yt−1), st−1) ,\n(1)\nwhere RNN(·) is an RNN function, e(yt−1) ∈Rd\nis a word embedding of yt−1, and st−1 ∈Rd is\na hidden state optionally used to explicitly incor-\nporate the information about the source input X\ninto the transition. We employ Long Short-Term\nMemory (LSTM) units (Hochreiter and Schmid-\nhuber, 1997) for the RNN function. The task here\nis to predict the t-th word yt by computing a tar-\nget word distribution p(y|y<t, X) ∈R|V |, where\n|V | represents the vocabulary size of the target lan-\nguage. p(y|y<t, X) is used to generate a sentence\nby either greedy/beam search or random sampling.\nTo learn the model parameters, the following\ncross entropy loss is usually employed:\nLc(Yg, X) = −\nNg\nX\nt=1\nlog p (y = yt|y<t, X) , (2)\nwhere we assume that the target sentence Yg is the\ngold sequence. Once we train the model, we can\nuse it to generate unseen sentences.\nMachine translation\nIn the context of machine\ntranslation, the source input X corresponds to a\nsource sentence (x1, x2, . . . , xM) of length M.\nEach word xi is also associated with a word em-\nbedding ˜e(xi) ∈Rd. We assume that a hidden\nstate ˜hi ∈R2d is computed for each xi by using a\nbi-directional RNN with LSTM units (Graves and\nSchmidhuber, 2005). That is, ˜hi is the concatena-\ntion of xi’s d-dimensional hidden states [−→h i; ←−h i]\ncomputed by a pair of forward and backward\nRNNs. We set the initial hidden state of the sen-\ntence generator as h0 = −→h M + ←−h 1. Following\nan attention mechanism proposed in Luong et al.\n(2015), st for predicting yt is computed as follows:\nst\n=\ntanh\n \nWs\n\"\nht;\nM\nX\ni=1\nai˜hi\n#\n+ bs\n!\n, (3)\nwhere ai = f(ht, i, ˜h) is the global-attention func-\ntion in\nLuong et al. (2015), Ws ∈Rd×3d is a\nweight matrix, and bs ∈Rd is a bias vector. st is\nthen used to compute the target word distribution:\np(y|y<t, X) = softmax(Wpst + bp),\n(4)\nwhere Wp ∈R|V |×d is a weight matrix, and bp ∈\nR|V | is a bias vector.\nImage captioning\nIn the case of image caption-\ning, the source input X corresponds to an im-\nage to be described. We assume that in our pre-\nprocessing step, each input image is fed into a\nconvolutional neural network to extract its ﬁxed-\nlength feature vector f ∈Rdf . More speciﬁcally,\nwe use the pre-computed feature vectors provided\nby Kiros et al. (2014), and the feature vectors are\nnever updated in any model training processes.\nThe input feature vector is transformed into the\ninitial hidden state h0 = tanh (Wff + bf), where\nWf ∈Rd×df is a weight matrix, and bf ∈Rd is a\nbias vector. In contrast to machine translation, we\ndo not use st−1 in Equation (1); more concretely,\nwe do not use any attention mechanisms for image\ncaptioning. Therefore, we directly use the hidden\nstate ht to compute the target word distribution:\np(y|y<t, X) = softmax(Wpht + bp),\n(5)\nwhere the weight and bias parameters are analo-\ngous to the ones in Equation (4).\nFor both of the tasks, we use the weight-tying\ntechnique (Inan et al., 2017; Press and Wolf, 2017)\nby using Wp as the word embedding matrix. That\nis, e(yt) is the yt-th row vector in Wp, and the tech-\nnique has shown to be effective in machine trans-\nlation (Hashimoto and Tsuruoka, 2017) and text\nsummarization (Paulus et al., 2018).\n2.2\nApplying Reinforcement Learning\nOne well-known limitation of using the cross en-\ntropy loss in Equation (2) is that the sentence gen-\neration models work differently at the training and\ntest time. More concretely, the models only ob-\nserve gold sequences at the training time, whereas\nthe models have to handle unseen sequences to\ngenerate sentences at the test time.\nTo bridge the gap, reinforcement learning has\nstarted gaining much attention (Ranzato et al.,\n2016; Wu et al., 2016; Rennie et al., 2017; Zhang\nand Lapata, 2017; Paulus et al., 2018; Yang et al.,\n2018). In this work, we focus on the most popular\nmethod called REINFORCE (Williams, 1992).1\nIn REINFORCE, the sentence generation model\nsets an initial state given a source input, and then\niterates an action selection and its corresponding\nstate transition. The action selection corresponds\nto randomly sampling a target word from Equa-\ntion (4) and (5), and the state transition corre-\nsponds to the RNN transition in Equation (1).\nOnce a sentence is generated, an approximated\nloss function is deﬁned as follows:\nLr(Y, X) = −\nN\nX\nt=1\nRt log p(y = yt|y<t, X), (6)\nwhere Rt is the reward at time step t, and the\nloss is approximated by the single example Y .\nRt is used to evaluate how good the t-th action\nselection is.\nUnlike maximum likelihood train-\ning, the reward function can be deﬁned by us-\ning task-speciﬁc evaluation scores like BLEU for\nmachine translation.\nIn this paper, we employ\nGLEU proposed by Wu et al. (2016), a variant\nof sentence-level BLEU. Following the implemen-\ntation in Ranzato et al. (2016), we deﬁne Rt =\nGLEU(Y, Yg)−bt, where bt is a baseline value es-\ntimating the future reward from the next time step\nto reduce the variance of the gradients. To esti-\nmate bt, we jointly train a linear regression model\nby minimizing ∥bt −GLEU(Y, Yg)∥2, and bt is\ncomputed as bt = σ(Wr ·st+br), where Wr ∈Rd\nis a weight vector, br is a bias, σ(·) is the logistic\nsigmoid function, and in the case of image cap-\ntioning, ht is used instead of st.\nOverall\nmodel\ntraining\nThe\nreinforcement\nlearning step is usually applied after pre-training\nthe models with the cross entropy loss in Equa-\ntion (2). At the REINFORCE phase, we deﬁne the\nfollowing joint loss function:\nL = λLc + (1 −λ)Lr,\n(7)\nwhere λ is a hyperparameter, and λ = 0.0 usually\nleads to unstable training (Wu et al., 2016).\n2.3\nLarge Action-Space Reduction\nThe vocabulary size |V | is usually more than ten\nthousands for datasets covering many sentences\nwith a variety of topics. However, for example,\nat most 100 unique words are selected when gen-\nerating a sentence of length 100. That is, the out-\nput length N is much smaller than the vocabulary\n1We tried self critic (Rennie et al., 2017), but did not ob-\nserve signiﬁcant improvement over REINFORCE.\nsize |V |, and this fact motivated us to reduce the\nlarge action space. Moreover, we have in practice\nfound that REINFORCE runs several times slower\nthan the supervised learning with the cross entropy\nloss.\nTo accelerate the training, we propose to con-\nstruct a small action space for each source input.\nIn other words, our method selects a small vocab-\nulary V ′ of size K for each source input in ad-\nvance to the model training. In this section, we\nassume that V ′ is given and represented with a\nsparse binary matrix MX ∈RK×|V |, where there\nare only K non-zero elements at position (i, wi)\nfor 1 ≤i ≤K. wi is a unique word index in\nV . MX is used to construct a small subset of the\nparameters in the softmax layer:\nW ′\np = MXWp,\nb′\np = MXbp,\n(8)\nand W ′\np ∈RK×d and b′\np ∈RK are used instead of\nWp and bp in Equation (4) and (5). Therefore, in\nmini-batched processes with a mini-batch size B,\nour method constructs B different sets of (W ′\np, b′\np).\nRelationship\nto\nprevious\nwork\nSampling-\nbased approximation methods have previously\nbeen studied to reduce the computational cost at\nthe large softmax layer in probabilistic language\nmodeling (Ji et al., 2016; Zoph et al., 2016),\nand such methods are also used to enable one\nto train neural machine translation models on\nCPUs (Eriguchi et al., 2016).\nThe construction\nof (W ′\np, b′\np) in our method is similar to these\nsoftmax approximation methods in that they also\nsample small vocabularies either at the word\nlevel (Ji et al., 2016), sentence level (Hashimoto\nand Tsuruoka, 2017), or mini-batch level (Zoph\net al., 2016). However, one signiﬁcant difference\nis that the approximation methods work only at\ntraining time using the cross entropy loss, and\nfull softmax computations are still required at test\ntime. The difference is crucial because a sentence\ngeneration model needs to simulate its test-time\nbehavior in reinforcement learning.\n3\nTarget Vocabulary Prediction\nThe remaining question is how to construct the\ninput-speciﬁc vocabulary V ′ for each source input\nX. This section describes our method to construct\nV ′ by using a vocabulary prediction model which\nis separated from the sentence generation models.\n3.1\nInput Representations\nIn the vocabulary prediction task, the input is the\nsource X (source sentences or images) to be de-\nscribed, and the output is V ′. We should be careful\nnot to make the prediction model computationally\nexpensive; otherwise the computational efﬁciency\nby our method would be canceled out.\nTo feed the information about X into our vocab-\nulary prediction model, we deﬁne an input vector\nv(X) ∈Rdv. For image captioning, we use the\nfeature vector f described in Section 2.1: v(X) =\nWvf + bv, where Wv ∈Rdv×df is a weight ma-\ntrix, and bv ∈Rdv is a bias vector. For machine\ntranslation, we employ a bag-of-embeddings rep-\nresentation: v(X) =\n1\nM\nPM\ni=1 ˜ev(xi), where the\ndv-dimensional word embedding ˜ev(xi) ∈Rdv is\ndifferent from ˜e(xi) used in the machine transla-\ntion model. By using the different set of the model\nparameters, we avoid the situation that our vocab-\nulary prediction model is affected during training\nthe sentence generation models.\nRelationship to previous work\nVocabulary pre-\ndiction has gained attention for training sequence-\nto-sequence\nmodels\nwith\nthe\ncross\nentropy\nloss (Weng et al., 2017; Wu et al., 2017), but\nnot for reinforcement learning. Compared to our\nmethod, previous methods jointly train a vocab-\nulary predictor by directly using source encoders\nas input to the predictor. One may expect joint\nlearning to improve both of the vocabulary pre-\ndictor and the sentence generator, but in prac-\ntice such positive effects are not clearly observed.\nWeng et al. (2017) reported that the joint learn-\ning improves the accuracy of their machine trans-\nlation models, but our preliminary experiments\ndid not indicate such accuracy gain. Such a joint\ntraining approach requires the model to continu-\nously update the vocabulary predictor during RE-\nINFORCE, because the encoder is shared. That\nis, the action space for each input changes during\nreinforcement learning, and we observed unstable\ntraining. Therefore, this work separately models\nthe vocabulary predictor and focuses on the effects\nof using the small vocabularies for REINFORCE.\nAnother note is that Jean et al. (2015) and\nL’Hostis et al. (2016) also proposed to construct\nsmall vocabularies in advance to the cross entropy-\nbased training. They suggest that the use of word\nalignment works well, but using the word align-\nment is not general enough, considering that there\nexist different types of source input. By contrast,\nour method can be straightforwardly applied to the\ntwo sentence generation tasks with the different\ninput modalities (i.e. image and text).\n3.2\nMulti-Label Classiﬁcation\nOnce the input representation v(X) is computed,\nwe further transform it by a single residual\nblock (He et al., 2016): r(X) = Res (v(X)) ∈\nRdv.2 Then r(X) is fed into a prediction layer:\no = σ (Wor(X) + bo) ,\n(9)\nwhere Wo ∈R|V |×dv is a weight matrix, and\nbo ∈R|V | is a bias vector. The i-th element oi\ncorresponds to the probability that the i-th word in\nthe target vocabulary V appears in the target sen-\ntence Y given its source X.\nWe use the training data for the sentence gener-\nations tasks to train the vocabulary predictor. For\neach X in the training data, we have its gold target\nsentence Yg. We train the vocabulary predictor as\na multi-label classiﬁcation model by the following\nloss function:\n−\n|V |\nX\ni=1\n(ti log oi + (1 −ti) log(1 −oi)) , (10)\nwhere ti is equal to 1.0 if the i-th word in V is\nincluded in Yg, and otherwise ti is 0.0. In practice,\nwe apply the label smoothing technique (Szegedy\net al., 2016) to the loss function.\nWe evaluate the accuracy of the vocabulary pre-\ndictor by using a separate development split D:\n# of correctly predicted words in D\n# of words in D\n, (11)\nwhere we select the top-K predictions in Equa-\ntion (9) for each source input X in D, and the eval-\nuation metric is a recall score. We use the top-K\nwords to construct the input-speciﬁc vocabularies\nV ′ for the sentence generation models, and we re-\nstrict that the recall is 100% for the training data.\n4\nExperimental Settings\nWe describe our experimental settings, and the de-\ntails can be found in the supplemental material.\n2We can use arbitrary types of hidden layers or even linear\nmodels like SVMs, but we found this one performed the best.\nWe describe the details of this in the supplemental material.\nDataset\nSize\n|V |\nmax(N)\nEn-De\n100,000\n24,482\n50\nEn-Ja (100K)\n100,000\n23,536\n50\nEn-Ja (2M)\n1,998,821\n70,668\n100\nEn-Ja (2M, SW)\n1,998,816\n37,905\n200\nEn-Vi\n132,406\n14,321\n100\nCh-Ja\n100,000\n23,383\n50\nMS COCO\n413,915\n14,543\n57\nFlickr8K\n30,000\n4,521\n38\nTable 1: Statistics of the training datasets.\n4.1\nDatasets\nWe used machine translation datasets of four\ndifferent\nlanguage\npairs:\nEnglish-to-German\n(En-De), English-to-Japanese (En-Ja), English-\nto-Vietnamese (En-Vi), and Chinese-to-Japanese\n(Ch-Ja).\nFor image captioning, we used two\ndatasets:\nMS COCO (Lin et al., 2014) and\nFlickr8K. Table 1 summarizes the statistics of the\ntraining datasets, where the number of training ex-\namples (“Size”), the target vocabulary size (|V |),\nand the maximum length of the target sentences\n(max(N)) are shown. For the machine translation\ndatasets, we manually set max(N) and omitted\ntraining examples which violate the constraints.\nEn-De:\nWe\nused\n100,000\ntraining\nsen-\ntence\npairs\nfrom\nnews commentary\nand\nnewstest2015\nas\nour\ndevelopment\nset,\nfollowing Eriguchi et al. (2017).\nEn-Ja:\nWe used parallel sentences in AS-\nPEC (Nakazawa et al., 2016) and constructed\nthree types of datasets:\nEn-Ja (100K), En-Ja\n(2M), and En-Ja (2M, SW). The 100K and 2M\ndatasets were constructed with the ﬁrst 100,000\nand 2,000,000 sentence pairs, respectively. To test\nour method using subword units, we further pre-\nprocessed the 2M dataset by using the Sentence-\nPiece toolkit (Kudo and Richardson, 2018) to con-\nstruct the En-Ja (2M, SW) dataset.\nEn-Vi: We used the pre-processed datasets pro-\nvided by Luong and Manning (2015). Our devel-\nopment dataset is the tst2012 dataset.\nCh-Ja: We constructed the Ch-Ja dataset by using\nthe ﬁrst 100,000 sentences from ASPEC.\nMS COCO and Flickr8K: We used the pre-\nprocessed datasets provided by Kiros et al. (2014).\nWe can also download the 4096-dimensional fea-\nture vectors f (i.e., df = 4096).\n4.2\nSettings of Vocabulary Prediction\nWe set dv = 512 for all the experiments. We used\nAdaGrad (Duchi et al., 2011) to train the vocab-\nulary predictor with a learning rate of 0.08 and a\nmini-batch size of 128. The model for each setting\nwas tuned based on recall scores (with K = 1000)\nfor the development split.\n4.3\nSettings of Sentence Generation\nWe set d = 256 with single-layer LSTMs for all\nthe experiments, except for the En-Ja (2M) and\n(2M, SW) datasets. For the larger En-Ja datasets,\nwe set d = 512 with two-layer LSTMs. We used\nstochastic gradient decent with momentum, with\na learning rate of 1.0, a momentum rate of 0.75,\nand a mini-batch size of 128. The model for each\nsetting was tuned based on BLEU scores for the\ndevelopment split. All of the models achieved the\nbest BLEU scores for all the datasets within 15 to\n20 training epochs. Each of the selected models\nwith the best BLEU scores was used for the fol-\nlowing REINFORCE step. For REINFORCE, we\nset λ = 0.005, and the learning rate was set to\n0.01. The REINFORCE steps required around 5\nepochs to signiﬁcantly improve the BLEU scores.\n4.4\nComputational Resources and\nMini-Batch Processing\nWe used a single GPU of NVIDIA GeForce\nGTX 10803 to run experiments for the En-De,\nEn-Ja (100K), En-Vi, Ch-Ja, MS COCO, and\nFlickr8K datasets. For the En-Ja (2M) and En-\nJa (2M, SW) datasets, we used a single GPU of\nNVIDIA Tesla V1004 to speedup our experi-\nments.\nMini-batch splitting\nIt should be noted that our\nsmall softmax method can be run even on the\nsingle GTX 1080 GPU for the larger translation\ndatasets, whereas the full softmax method runs out\nof the GPU memory.\nA typical strategy to ad-\ndress such out-of-memory issues is to use multi-\nple GPUs, but we have found that we need at most\neight GPUs to conduct our experiments on the full\nsoftmax method with REINFORCE.5 Moreover,\nusing the multiple GPUs does not always speedup\nthe training time. We instead employ another strat-\negy to split the mini-batch at each training itera-\ntion. First, we sort the mini-batch examples ac-\ncording to the lengths of the source (or target) text,\nand then split the mini-batch into S sets of the\ntraining examples. For example, in our case the\n3The GPU memory capacity is 11,178MiB.\n4The GPU memory capacity is 16,152MiB (AWS p3).\n5This also depends on the mini-batch size.\nCross entropy\nREINFORCE w/ cross entropy\nSmall softmax\nFull softmax\nSmall softmax\nFull softmax\nTranslation\nEn-De\n11.09±0.51\n10.84±0.37\n12.13±0.33\n11.73±0.23\nEn-Ja (100K)\n28.26±0.15\n28.05±0.40\n29.14±0.13\n29.01±0.35\nEn-Vi\n24.56±0.14\n24.53±0.18\n24.98±0.11\n24.92±0.09\nCh-Ja\n29.27±0.08\n28.97±0.15\n30.10±0.12\n29.80±0.15\nImage captioning\nMS COCO\n24.88±0.25\n24.75±0.36\n26.43±0.32\n25.74±0.13\nFlickr8K\n16.45±0.28\n16.52±0.11\n19.04±0.43\n19.17±0.24\nTable 2: BLEU scores for the development splits of the six datasets. “Small softmax” corresponds to our method.\n75\n80\n85\n90\n95\n100\n0\n1000\n2000\n3000\nRecall [%]\nK (Small vocabulary size)\nEn-De\nEn-Ja (100K)\nEn-Ja (2M)\nEn-Ja (2M, SW)\nEn-Vi\nCh-Ja\nMS COCO\nFlickr8K\nFigure 1: Recall scores of our vocabulary predictor.\nmini-batch size is 128, and if S is set to 4, each\nof the smaller sets includes 32 training examples.\nWe perform back-propagation for each set one by\none, and at each step we delete the correspond-\ning computational graphs to reduce the GPU mem-\nory consumption. Finally, the accumulated partial\nderivatives are used to update the model parame-\nters. More details can be found in our Pytorch 0.4\nimplementation.\n5\nResults of Sentence Generation Tasks\n5.1\nAccuracy of Vocabulary Prediction\nFigure 1 shows recall scores with respect to differ-\nent values of the small vocabulary size K for each\ndataset. We can see that the recall scores reach\n95% with K = 1000 for most of the datasets. One\nexception is the En-De dataset, and this is not sur-\nprising because a German vocabulary would be-\ncome sparse by many compound nouns.\nThese results show that our vocabulary pre-\ndictor works well for source inputs of different\nmodalities (text and image) and their correspond-\ning different target languages. Our method also\nworks at the subword level as well as at the stan-\ndard word level. For training the sentence gener-\nation models, we set K = 500 for the Flickr8K\ndataset and K = 1000 for the other datasets. Our\nempirical recommendation is K = 1000 if |V | is\nlarger than 10,000 and otherwise K = 500.\n5.2\nAccuracy of Sentence Generation\nThe goal of this paper is achieving efﬁcient rein-\nforcement learning for sentence generation to en-\ncourage future research, but before evaluating the\nefﬁciency of our method, we show that using the\nsmall vocabularies does not degrade the accuracy\nof the sentence generation models. Table 2 shows\nBLEU scores for the development splits of the\nfour machine translation and two image caption-\ning datasets. The BLEU scores are averaged over\nﬁve different runs with different random seeds,\nand the standard deviations are also reported.\nWe can see in Table 2 that our method (Small\nsoftmax) keeps the BLEU scores as high as those\nof “Full softmax”. For some datasets, the BLEU\nscores of our method are even better than those\nof the full softmax method. The trend is consis-\ntent in both of the cross entropy training phase and\nthe REINFORCE phase.\nThese results indicate\nthat our method works well for different machine\ntranslation and image captioning datasets. We also\nconﬁrmed that our experimental results are com-\npetitive with previously reported results when us-\ning the same training datasets; for example, our\nEn-Vi test set result on tst2013 is 27.87±0.21\n(cf. 26.9 in Luong and Manning (2015)).\nBetter generation of rare words\nThese BLEU\nscores suggest that our method for reinforcement\nlearning has the potential to outperform the full\nsoftmax baseline. However, it is still unclear what\nis the potential advantage in terms of generation\nquality. We therefore analyzed the differences be-\ntween output sentences of the small and full soft-\nmax methods, following Ott et al. (2018). Figure 2\nshows the results of the En-De translation dataset,\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n10\n20\n30\n40\n50\n60\n70\n80\n90 100\nObserved frequency [%]\nFrequency percentile in the training data\nSmall softmax (K=1000)\nFull softmax\nReference\nFigure 2: An analysis on the En-De translation results.\nThe small and full softmax results are based on the RE-\nINFORCE setting, and “Reference” corresponds to the\nEn-De reference translations of the development split.\n \n \n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n0\n5\n10\n15\n20\n25\n30\nSmall softmax\nFull softmax\nPre-training epochs\nBLEU\nFigure 3: Effects of pre-training for REINFORCE.\nand we observed the same trend for all the other\ndatasets. Each entry is computed as follows:\n# of output words in each percentile\n# of output words\n, (12)\nwhere the “10” percentile includes the top 10% of\nthe most frequent words, and the “100” percentile\nincludes the top 10% of the most infrequent words.\nWe can see that our small softmax method better\noutputs rare words, and these results suggest that\nusing input-speciﬁc vocabularies is useful in con-\ntrolling action spaces for reinforcement learning.\nEffectiveness with fewer pre-training steps\nWe followed the standard practice that the mod-\nels are pre-trained by maximum likelihood before\nstarting reinforcement learning.\nHowever, such\npre-training may have a negative effect in rein-\nforcement learning. Consider the situation where\nthe pre-training leads to zero cross-entropy loss.\nIn this case, nothing will be learned during rein-\nforcement learning because no exploratory action\ncan be performed. Although pre-training in prac-\ntice does not lead to zero cross-entropy loss, it can\nstill overﬁt the data and result in very sharp out-\n2M\n2M, SW\nCross entropy\n38.76\n39.15\nw/ beam search\n39.88\n40.35\nREINFORCE w/ cross entropy\n40.10\n40.26\nw/ beam search\n40.36\n40.38\nw/ beam search (K= 500)\n40.07\n40.07\nw/ beam search (K=2000)\n40.30\n40.50\nw/ beam search (K=3000)\n40.27\n40.41\nTable 3: BLEU scores for the development split of the\nEn-Ja (2M) and En-Ja (2M, SW) datasets.\nREINFORCE w/ cross entropy (K=1000)\n40.16\nw/ beam search\n40.50\n- Cross entropy (1.3M) w/ beam search\n39.42\n(Hashimoto and Tsuruoka, 2017)\n- Cross entropy (2M) w/ beam search\n40.29\n(Oda et al., 2017b)\n- Cross entropy (2M+1M back-trans.)\n41.42\nw/ beam search (Morishita et al., 2017)\nTable 4: BLEU scores for the En-Ja test split, where\nwe use the En-Ja (2M, SW) dataset. The 95% conﬁ-\ndence interval by bootstrap resampling (Noreen, 1989)\nis [39.61, 41.47] for our beam search result.\nput distributions, thereby hindering exploration in\nreinforcement learning. It is therefore important\nto consider a reinforcement learning setting with\nless or no pre-training (Liu et al., 2018). In Fig-\nure 3 for the En-Ja (100K) dataset, we show that\nthe small softmax method works more effectively\nwith fewer pre-training epochs. For this experi-\nment, we set λ = 0 in Equation (7) to purely focus\non REINFORCE. Using GLEU (or BLEU) scores\ngives sparse rewards, and thus the resulting BLEU\nscores are very low with fewer pre-training steps,\nbut the small softmax method has the potential to\nwork well if we can design more effective reward\nfunctions.\nResults on larger datasets\nTo see whether our\nmethod works in larger scales, Table 3 shows\nBLEU scores for the development split when us-\ning the En-Ja (2M) and En-Ja (2M, SW) datasets.6\nThese results show that our method consistently\nworks even on these larger datasets at the word\nand subword levels.\nIn this table we also re-\nport how our method works with beam search,\nand the greedy-based BLEU scores are very close\nto those of beam search after the REINFORCE\nphase. When performing a beam search, we can\noptionally use different sizes of the small vocab-\n6For the 2M dataset, the full softmax baseline achieves\nBLEU scores of 38.67 and 39.84 for the “Cross entropy” and\n“REINFORCE w/ cross entropy” settings, respectively.\nTraining time [minutes/epoch]\nGPU memory [MiB]\nCE\nREINFORCE w/ CE\nCE\nREINFORCE w/ CE\n|V |\nmax (N)\nSmall\nFull\nSmall\nFull\nSmall\nFull\nSmall\nFull\nEn-Ja (100K)\n23,536\n50\n4.6\n4.8\n10.1\n21.2\n1,781\n6,061\n2,193\n7,443\nEn-Ja (2M)\n70,668\n100\n95.7\n141.4\n231.3\n635.9\n5,033\n10,527\n6,485\n14,803\nEn-Vi\n14,321\n100\n10.5\n10.7\n23.2\n38.4\n2,149\n10,645\n2,909\n10,807\nMS COCO\n14,543\n57\n4.4\n4.2\n11.6\n22.9\n1,419\n8,587\n1,785\n10,651\nFlickr8K\n4,521\n38\n0.3\n0.3\n0.8\n0.9\n911\n2,031\n1,031\n3,197\nTable 5: Training time, and maximum memory consumption on our GPU devices for the text generation models.\nFor the full softmax baseline on the En-Ja (2M) experiments, the mini-batch splitting strategy (described in Sec-\ntion 4.4) is applied. CE: Cross-Entropy, Small: Small softmax (our proposed method), Full: Full softmax (the\nbaseline).\nulary, but we observe that our method is robust\nto the changes, whereas Wu et al. (2017) reported\nthat their dynamic vocabulary selection method is\nsensitive to such changes.\nFor reference, we report the test set results in\nTable 4. We cite BLEU scores from previously\npublished papers which reported results of single\nmodels (i.e., without ensemble). Our method with\ngreedy translation achieves a competitive score.\nIt should be noted that Morishita et al. (2017)\nachieve a better score presumably because they\nused additional in-domain one million parallel\nsentences obtained by the back-translation tech-\nnique (Sennrich et al., 2016).\n6\nEfﬁciency of the Proposed Method\nThis section discusses our main contribution: how\nefﬁcient our method is in accelerating reinforce-\nment learning for sentence generation.\n6.1\nSpeedup at Training Time\nWe have examined the training-time efﬁciency of\nour method. Table 5 shows the training time [min-\nutes/epoch] for ﬁve different datasets. We selected\nthe ﬁve datasets to show results with different vo-\ncabulary sizes and different maximum sentence\nlengths, and we observed the same trend on the\nother datasets. The vocabulary size |V | and the\nmaximum sentence length max(N) are shown for\neach training dataset.\nIn the training with the\nstandard cross entropy loss, the speedup by our\nmethod is not impressive as long as the vocabu-\nlary size |V | can be easily handled by the GPUs.\nWe set S = 2 for the cross entropy training of\nthe “Full softmax” method in the En-Ja (2M) set-\nting, to reduce the GPU memory consumption as\ndescribed in Section 4.4.\nIn the training with the REINFORCE algo-\nrithm, the speedup by our method is enhanced.\nIn particular, in the En-Ja (2M) experiments, our\nmethod gains a factor of 2.7 speedup compared\nwith the full softmax baseline (S = 3). For most\nof the experimental settings, the speedup signif-\nicantly accelerates our research and development\ncycles when working on reinforcement learning\nfor sentence generation tasks. One exception is the\nFlickr8K dataset whose original vocabulary size\n|V | is already very small, and the lengths of the\ntarget sentences are short. In the supplementary\nmaterial, we also show the test-time efﬁciency.\n6.2\nGPU Memory Consumption\nOur method is also efﬁcient in terms of GPU mem-\nory consumption at training time.\nTable 5 also\nshows the maximum GPU memory consumption\nduring the training. These results show that our\nmethod easily ﬁts in the memory of the single GTX\n1080 GPU, whereas “Full softmax” is very sen-\nsitive to the vocabulary size |V | and the sentence\nlengths. In particular, we observe about 56% re-\nduction in memory usage when using the En-Ja\n(2M) dataset. By saving the memory usage, one\ncould try using larger models, larger mini-batches,\nlarger vocabularies, and longer target sentences\nwithout relying on multiple GPUs.\nScalability of our method\nTo further show the\nmemory efﬁciency our our method, we measured\nthe GPU memory consumption with a larger mini-\nbatch size, 2048. We applied the mini-batch split-\nting strategy to both the small and full softmax\nmethods to handle such a large mini-batch size. In\nthe En-Ja (2M) experiments with REINFORCE,\nour small softmax method works with the large\nbatch-size by setting S = 6, whereas the full soft-\nmax baseline needs S = 40. Aggressively split-\nting the mini-batch (i.e. using larger values of S)\nslows down the training time, and in that sense\nour method is much more efﬁcient when we con-\nsider the larger mini-batch sizes. If we increase the\nmini-batch size to 4096, our small softmax method\nworks with S = 12.\n7\nRelated Work\nReducing the computational cost at the large soft-\nmax layer in language modeling/generation is ac-\ntively studied (Jean et al., 2015; Ji et al., 2016;\nEriguchi et al., 2016; L’Hostis et al., 2016; Zoph\net al., 2016; Wu et al., 2017). Most of the exist-\ning methods try to reduce the vocabulary size by\neither negative sampling or vocabulary prediction.\nOne exception is that Oda et al. (2017a) propose\nto predict a binary code of its corresponding tar-\nget word. Although such a sophisticated method\nis promising, we focused on the vocabulary reduc-\ntion method to apply policy-based reinforcement\nlearning in a straightforward way.\nAs reported in this paper, one simple way to de-\nﬁne a reward function for reinforcement learning\nis to use task-speciﬁc automatic evaluation met-\nrics (Ranzato et al., 2016; Wu et al., 2016; Rennie\net al., 2017; Zhang and Lapata, 2017; Paulus et al.,\n2018), but this is limited in that we can only use\ntraining data with gold target sentences. An alter-\nnative approach is to use a discriminator in gen-\nerative adversarial networks (Goodfellow et al.,\n2014), and Yang et al. (2018) showed that REIN-\nFORCE with such a discriminator improves trans-\nlation accuracy. However, Yang et al. (2018) only\nused the training data, and thus the potential of the\ngenerative adversarial networks is not fully real-\nized. One promising direction is to improve the\nuse of the generative adversarial networks for the\nsentence generation tasks by using our method,\nbecause our method can also accelerate the com-\nbination of REINFORCE and the discriminator.\n8\nConclusion\nThis paper has presented how to accelerate rein-\nforcement learning for sentence generation tasks\nby reducing large action spaces. Our method is as\naccurate as, is faster than, and uses less GPU mem-\nory than the standard full softmax counterpart, on\nsentence generation tasks of different modalities.\nIn future work, it is interesting to use our method\nin generative adversarial networks to further im-\nprove the sentence generation models.\nAcknowledgments\nWe thank anonymous reviewers for their fruit-\nful comments. This work was supported by JST\nCREST Grant Number JPMJCR1513, Japan.\nReferences\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive Subgradient Methods for Online Learning\nand Stochastic Optimization.\nJournal of Machine\nLearning Research, 12:2121–2159.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\nTsuruoka. 2016. Tree-to-Sequence Attentional Neu-\nral Machine Translation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages\n823–833.\nAkiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun\nCho. 2017.\nLearning to Parse and Translate Im-\nproves Neural Machine Translation.\nIn Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 72–78.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014.\nGenerative\nAdversarial Nets. In Advances in Neural Informa-\ntion Processing Systems 27, pages 2672–2680.\nAlex Graves and Jurgen Schmidhuber. 2005. Frame-\nwise Phoneme Classiﬁcation with Bidirectional\nLSTM and Other Neural Network Architectures.\nNeural Networks, 18(5):602–610.\nKazuma Hashimoto and Yoshimasa Tsuruoka. 2017.\nNeural Machine Translation with Source-Side La-\ntent Graph Parsing. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 125–135.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016.\nIdentity Mappings in Deep Residual\nNetworks.\nIn Proceedings of the 14th European\nConference on Computer Vision, pages 630–645.\nGeoffrey\nE.\nHinton,\nNitish\nSrivastava,\nAlex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. 2012.\nImproving neural networks by\npreventing\nco-adaptation\nof\nfeature\ndetectors.\nCoRR, abs/1207.0580.\nSepp Hochreiter and Jurgen Schmidhuber. 1997.\nLong short-term memory.\nNeural Computation,\n9(8):1735–1780.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017.\nTying Word Vectors and Word Classiﬁers:\nA Loss Framework for Language Modeling.\nIn\nProceedings of the 5th International Conference on\nLearning Representations.\nSergey Ioffe and Christian Szegedy. 2015. Batch Nor-\nmalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. In Proceedings\nof the 32nd International Conference on Machine\nLearning, pages 448–456.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015.\nOn Using Very Large\nTarget Vocabulary for Neural Machine Translation.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1–10.\nShihao Ji, S. V. N. Vishwanathan, Nadathur Satish,\nMichael J. Anderson, and Pradeep Dubey. 2016.\nBlackOut: Speeding up Recurrent Neural Network\nLanguage Models With Very Large Vocabularies. In\nProceedings of the 4th International Conference on\nLearning Representations.\nRafal\nJozefowicz,\nWojciech\nZaremba,\nand\nIlya\nSutskever. 2015. An Empirical Exploration of Re-\ncurrent Network Architectures.\nIn Proceedings\nof the 32nd International Conference on Machine\nLearning, pages 2342–2350.\nRyan Kiros, Ruslan Salakhutdinov, and Richard S\nZemel. 2014. Unifying visual-semantic embeddings\nwith multimodal neural language models.\narXiv,\ncs.LG 1411.2539.\nTaku Kudo and John Richardson. 2018.\nSentence-\nPiece: A simple and language independent subword\ntokenizer and detokenizer for Neural Text Process-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 66–71.\nGurvan L’Hostis, David Grangier, and Michael Auli.\n2016.\nVocabulary Selection Strategies for Neural\nMachine Translation. arXiv, cs.CL 1610.00072.\nTsung-Yi\nLin,\nMichael\nMaire,\nSerge\nBelongie,\nLubomir Bourdev, Ross Girshick, James Hays,\nPietro Perona, Deva Ramanan, C. Lawrence Zitnick,\nand Piotr Dollar. 2014. Microsoft COCO: Common\nObjects in Context. arXiv, cs.CV 1405.0312.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat,\nTianlin Shi, and Percy Liang. 2018. Reinforcement\nLearning on Web Interfaces using Workﬂow-Guided\nExploration. In Proceedings of the 6th International\nConference on Learning Representations.\nMinh-Thang Luong and Christopher D. Manning.\n2015.\nStanford Neural Machine Translation Sys-\ntems for Spoken Language Domain.\nIn Interna-\ntional Workshop on Spoken Language Translation.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015.\nEffective Approaches to Attention-\nbased Neural Machine Translation. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 1412–1421.\nMakoto Morishita, Jun Suzuki, and Masaaki Nagata.\n2017. NTT Neural Machine Translation Systems at\nWAT 2017. In Proceedings of the 4th Workshop on\nAsian Translation, pages 89–94.\nToshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-\nmoto, Masao Utiyama, Eiichiro Sumita, Sadao\nKurohashi, and Hitoshi Isahara. 2016.\nASPEC:\nAsian Scientiﬁc Paper Excerpt Corpus. In Proceed-\nings of the 10th Conference on International Lan-\nguage Resources and Evaluation.\nEric W. Noreen. 1989. Computer-Intensive Methods\nfor Testing Hypotheses: An Introduction.\nWiley-\nInterscience.\nYusuke Oda, Philip Arthur, Graham Neubig, Koichiro\nYoshino, and Satoshi Nakamura. 2017a. Neural Ma-\nchine Translation via Binary Code Prediction.\nIn\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 850–860.\nYusuke Oda, Katsuhito Sudoh, Satoshi Nakamura,\nMasao Utiyama, and Eiichiro Sumita. 2017b.\nA\nSimple and Strong Baseline: NAIST-NICT Neural\nMachine Translation System for WAT2017 English-\nJapanese Translation Task. In Proceedings of the 4th\nWorkshop on Asian Translation, pages 135–139.\nMyle\nOtt,\nMichael\nAuli,\nDavid\nGrangier,\nand\nMarc’Aurelio Ranzato. 2018.\nAnalyzing Uncer-\ntainty in Neural Machine Translation. In Proceed-\nings of the 35th International Conference on Ma-\nchine Learning, volume 80, pages 3956–3965.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A Method for Automatic\nEvaluation of Machine Translation. In Proceedings\nof the 40th Annual Meeting on Association for Com-\nputational Linguistics, pages 311–318.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Ben-\ngio. 2013.\nOn the difﬁculty of training recurrent\nneural networks. In Proceedings of the 30th Inter-\nnational Conference on Machine Learning, pages\n1310–1318.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018.\nA Deep Reinforced Model for Abstractive\nSummarization. In Proceedings of the 6th Interna-\ntional Conference on Learning Representations.\nOﬁr Press and Lior Wolf. 2017. Using the Output Em-\nbedding to Improve Language Models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 157–163.\nMarc?fAurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016.\nSequence Level\nTraining with Recurrent Neural Networks. In Pro-\nceedings of the 4th International Conference on\nLearning Representations.\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nSequence Training for Image Captioning. In Pro-\nceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition.\nAlexander M. Rush, Sumit Chopra, and Jason We-\nston. 2015. A Neural Attention Model for Abstrac-\ntive Sentence Summarization. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 379–389.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving Neural Machine Translation Mod-\nels with Monolingual Data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to Sequence Learning with Neural Net-\nworks. In Advances in Neural Information Process-\ning Systems 27, pages 3104–3112.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nand Jon Shlens. 2016. Rethinking the Inception Ar-\nchitecture for Computer Vision. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition.\nRongxiang Weng, Shujian Huang, Zaixiang Zheng,\nXin-Yu Dai, and Jiajun Chen. 2017.\nNeural Ma-\nchine Translation with Word Predictions.\nIn Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 136–\n145.\nRonald J. Williams. 1992. Simple Statistical Gradient-\nFollowing Algorithms for Connectionist Reinforce-\nment Learning. Machine Learning, 8:229–256.\nYonghui Wu et al. 2016.\nGoogle’s Neural Machine\nTranslation System:\nBridging the Gap between\nHuman and Machine Translation.\narXiv, cs.CL\n1609.08144.\nYu Wu, Wei Wu, Dejian Yang, Can Xu, Zhoujun Li,\nand Ming Zhou. 2017. Neural Response Generation\nwith Dynamic Vocabularies. In Proceedings of the\nThirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence.\nZhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2018.\nImproving Neural Machine Translation with Con-\nditional Sequence Generative Adversarial Nets. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1346–1355.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014.\nRecurrent Neural Network Regularization.\narXiv, cs.NE 1409.2329.\nXingxing Zhang and Mirella Lapata. 2017. Sentence\nSimpliﬁcation with Deep Reinforcement Learning.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n584–594.\nBarret Zoph, Ashish Vaswani, Jonathan May, and\nKevin Knight. 2016. Simple, Fast Noise-Contrastive\nEstimation for Large RNN Vocabularies.\nIn Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1217–1222.\nSupplementary Material\nA\nVocabulary Prediction Model\nResidual block\nIn Section 3.2, we used a resid-\nual block r(X) = Res(v(X)) ∈Rdv inspired\nby He et al. (2016) to transform the input vector\nv(X) ∈Rdv:\nr1 = BNr1(v(X)), r2 = tanh(r1),\nr3 = Wr3r2 + br3, r4 = BNr4(r3),\nr5 = tanh(r4),\nr6 = Wr6r5 + br6,\nr(X) = r6 + v(X),\n(13)\nwhere BNr1(·) and BNr4(·) correspond to batch\nnormalization (Ioffe and Szegedy, 2015), Wr3 ∈\nRdv×dv and Wr6 ∈Rdv×dv are weight matrices,\nand br3 ∈Rdv and br6 ∈Rdv are bias vectors.\nWe apply dropout (Hinton et al., 2012) to r5 with\na dropout rate of 0.4.\nLabel smoothing\nIn Section 3.2, we applied la-\nbel smoothing (Szegedy et al., 2016) to the loss\nfunction in Equation (10). More concretely, we\nmodify the gold label ti for the i-th target word as\nfollows:\nti ←(1.0 −ε)ti + εp(i),\n(14)\nwhere ε is a hyperparameter, and p(i) is a prior\nprobability that the i-th word appears in a target\nsentence. p(i) is computed for each dataset:\np(i) =\nP|T|\nj=1 tj\ni\n|T|\n,\n(15)\nwhere |T| is the size of the training dataset, and\ntj\ni is the gold label for the i-th target word in the\nj-th training example. Therefore, p(i) roughly re-\nﬂects the unigram frequency. We have empirically\nfound that the recommended value ε = 0.1 con-\nsistently improves the recall of the predictor.\nCPU\nGPU\nData size\n|V |\nModel size\nSmall softmax\nFull softmax\nSmall softmax\nFull softmax\n100K\n23,536\n1-L, 256-D\n54.4\n113.8\n71.9\n78.4\n2M\n70,668\n2-L, 512-D\n156.2\n503.2\n80.5\n105.7\n2M, SW\n37,905\n2-L, 512-D\n161.0\n369.2\n84.8\n99.2\nTable 6: Average time [milliseconds] to obtain a translation for each sentence in the En-Ja development split.\nB\nDetailed Experimental Settings\nWord segmentation\nThe sentences in the En-\nVi, MS COCO, and Flickr8K datasets were pre-\ntokenized.\nWe used the Kytea toolkit for\nJapanese and the Stanford Core NLP toolkit\nfor Chinese.\nIn the other cases, we used the\nMoses word tokenizer. We lowercased all the En-\nglish sentences. The En-Ja (2M, SW) dataset was\nobtained by the SentencePiece toolkit so that\nthe vocabulary size becomes around 32,000.\nVocabulary construction\nWe built the target\nlanguage vocabularies with words appearing at\nleast ﬁve times for the En-De dataset, seven times\nfor the En-Ja (2M) dataset, three times for the Ch-\nJa dataset, and twice for the other datasets.\nOptimization\nWe initialized all the weight and\nembedding matrices with uniform random values\nin [−0.1, +0.1], and all the bias vectors with ze-\nros, except for the LSTM forget-gate biases which\nwere initialized with ones (Jozefowicz et al.,\n2015). For all the models, we used gradient-norm\nclipping (Pascanu et al., 2013) with a clipping\nvalue of 1.0. We applied dropout to Equation (3),\n(4), and (5) with a dropout rate of 0.2, and we fur-\nther used dropout in the vertical connections of the\ntwo-layer LSTMs (Zaremba et al., 2014) for the\nEn-Ja (2M) and (2M, SW) datasets. As regulariza-\ntion, we also used weight decay with a coefﬁcient\nof 10−6. When training the vocabulary predictor\nand the sentence generation models, we checked\nthe corresponding evaluation scores at every half\nepoch, and halved the learning rate if the evalua-\ntion scores were not improved. We stabilized the\ntraining of the sentence generation models by not\ndecreasing the learning rate in the ﬁrst six epochs.\nThese training settings were tuned for the En-Ja\n(100K) dataset, but we empirically found that the\nsame settings lead to the consistent results for all\nthe datasets.\nBaseline Estimator\nWe used the Adam opti-\nmizer with a learning rate of 10−3 and the other\ndefault settings, to optimize the baseline estimator\nin Section 2.2. We have found that our results are\nnot sensitive to the training settings of the baseline\nestimator.\nBeam search\nFor the results in Table 3 and 4,\nwe tried two beam search methods in Hashimoto\nand Tsuruoka (2017) and Oda et al. (2017b), and\nselected better scores for each setting.\nIn gen-\neral, these length normalization methods lead to\nthe best BLEU scores with a beam size of 10 to\n20.\nC\nTest Time Efﬁciency\nBy the fact that our method works efﬁciently with\nreinforcement learning, we expect that our method\nalso works well at test time. Table 6 shows the av-\nerage decoding time [milliseconds] to generate a\nJapanese sentence given an English sentence for\nthe En-Ja development split. For reference, the\nvocabulary size and the model size are also shown\nfor each setting. We note that the decoding time of\nour method includes the time for constructing an\ninput-speciﬁc vocabulary for each source input.\nWe can see that our method runs faster than\n“Full softmax”; in particular, the speedup is sig-\nniﬁcant on CPUs, and the decoding time by our\nmethod is less sensitive to changing |V | than that\nof “Full softmax”. This is because our method\nhandles the full vocabulary only once for each\nsource input, whereas “Full softmax” needs to\nhandle the full vocabulary every time the model\npredicts a target word.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2018-09-05",
  "updated": "2019-04-04"
}