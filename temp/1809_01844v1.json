{
  "id": "http://arxiv.org/abs/1809.01844v1",
  "title": "Unsupervised Learning of View-invariant Action Representations",
  "authors": [
    "Junnan Li",
    "Yongkang Wong",
    "Qi Zhao",
    "Mohan S. Kankanhalli"
  ],
  "abstract": "The recent success in human action recognition with deep learning methods\nmostly adopt the supervised learning paradigm, which requires significant\namount of manually labeled data to achieve good performance. However, label\ncollection is an expensive and time-consuming process. In this work, we propose\nan unsupervised learning framework, which exploits unlabeled data to learn\nvideo representations. Different from previous works in video representation\nlearning, our unsupervised learning task is to predict 3D motion in multiple\ntarget views using video representation from a source view. By learning to\nextrapolate cross-view motions, the representation can capture view-invariant\nmotion dynamics which is discriminative for the action. In addition, we propose\na view-adversarial training method to enhance learning of view-invariant\nfeatures. We demonstrate the effectiveness of the learned representations for\naction recognition on multiple datasets.",
  "text": "Unsupervised Learning of View-invariant Action\nRepresentations\nJunnan Li\nGraduate School for Integrative Sciences and Engineering\nNational University of Singapore\nSingapore\nlijunnan@u.nus.edu\nYongkang Wong\nSmart Systems Institute\nNational University of Singapore\nSingapore\nyongkang.wong@nus.edu.sg\nQi Zhao\nDepartment of Computer Science and Engineering\nUniversity of Minnesota\nMinneapolis, USA\nqzhao@cs.umn.edu\nMohan S. Kankanhalli\nSchool of Computing\nNational University of Singapore\nSingapore\nmohan@comp.nus.edu.sg\nAbstract\nThe recent success in human action recognition with deep learning methods mostly\nadopt the supervised learning paradigm, which requires signiﬁcant amount of man-\nually labeled data to achieve good performance. However, label collection is an\nexpensive and time-consuming process. In this work, we propose an unsupervised\nlearning framework, which exploits unlabeled data to learn video representations.\nDifferent from previous works in video representation learning, our unsupervised\nlearning task is to predict 3D motion in multiple target views using video repre-\nsentation from a source view. By learning to extrapolate cross-view motions, the\nrepresentation can capture view-invariant motion dynamics which is discriminative\nfor the action. In addition, we propose a view-adversarial training method to\nenhance learning of view-invariant features. We demonstrate the effectiveness of\nthe learned representations for action recognition on multiple datasets.\n1\nIntroduction\nRecognizing human action in videos is a long-standing research problem in computer vision. Over\nthe past years, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have\nemerged as the state-of-the-art learning framework for action recognition [3, 6, 33, 49]. However, the\nsuccess of existing supervised learning methods is primarily driven by signiﬁcant amount of manually\nlabeled data, which is expensive and time-consuming to collect.\nTo tackle this problem, a stream of unsupervised methods have recently been proposed [8, 23, 31,\n32, 51], which leverage free unlabeled data for representation learning. The key idea is to design\na surrogate task that exploits the inherent structure of raw videos, and formulate a loss function\nto train the network. Some works design the surrogate task as constructing future frames [51] or\nPreprint. Work in progress.\narXiv:1809.01844v1  [cs.CV]  6 Sep 2018\nfuture motion [31], while others use the temporal order of video frames to learn representations in a\nself-supervised manner [8, 23, 32]. Although they show promising results, the learned representations\nare often view-speciﬁc, which makes them less robust to view changes.\nGenerally, human action can be observed from multiple views, where the same action appears\nquite different. Therefore, it is important to learn discriminative view-invariant features, especially\nfor action recognition from unknown or unseen views. Humans have the ability to visualize what\nan action looks like from different views, because human brains can build view-invariant action\nrepresentations immediately [18]. We hypothesize that enabling a deep network with the ability to\nextrapolate action across different views can encourage it to learn view-invariant representations. In\nthis work, we propose an unsupervised learning framework, where the task is to construct the 3D\nmotions for multiple target views using the video representation from a source view. We argue that in\norder for the network to infer cross-view motion dynamics, the learned representations should reside\nin a view-invariant discriminative space for action recognition.\nView-invariant representation learning for cross-view action recognition has been widely studied [21,\n26, 42, 43, 57, 62]. However, most of the existing methods require access to 3D human pose\ninformation during training, while others compromise discriminative power to achieve view invariance.\nWe focus on inferring motions rather than tracking body keypoints over space and time. Our method\nlearns a recurrent encoder which extracts motion dynamics insensitive to viewpoint changes. We\nrepresent motion as 3D ﬂow calculated using RGB-D data only.\nThe contributions of our work are as follows:\n• We propose an unsupervised framework to effectively learn view-invariant video representation\nthat can predict motion sequences for multiple views. The learned representation is extracted\nfrom a CNN+RNN based encoder, and decoded into multiple sequences of 3D ﬂows by CNN\ndecoders. The framework is trained by jointly minimizing several losses.\n• We propose a view-adversarial training to encourage view-invariant feature learning. Videos\nfrom different views are mapped to a shared subspace where a view classiﬁer cannot discriminate\nthem. The shared representation is enforced to contain meaningful motion information by the\nuse of ﬂow decoders.\n• We demonstrate the effectiveness of our learned representation on cross-subject and cross-view\naction recognition tasks. We experiment with various input modalities including RGB, depth and\nﬂow. Our method outperforms state-of-the-art unsupervised methods across multiple datasets.\n2\nRelated Work\n2.1\nUnsupervised Representation Learning\nWhile deep networks have shown dominant performance in various computer vision tasks, the fully\nsupervised training paradigm requires vast amount of human-labeled data. The inherent limitation\nhighlights the importance of unsupervised learning, which leverages unlabeled data to learn feature\nrepresentations. Over the past years, unsupervised learning methods have been extensively studied\nfor deep learning methods, such as Deep Boltzmann Machines [45] and auto-encoders [1, 2, 22, 53].\nUnsupervised representation learning has proven to be useful for several supervised tasks, such as\npedestrian detection, object detection and image classiﬁcation [5, 11, 34, 46, 59].\nIn the video domain, there are two lines of recent works on unsupervised representation learning.\nThe ﬁrst line of works exploit the temporal structure of videos to learn visual representation with\nsequence veriﬁcation or sequence sorting task [8, 23, 32]. The second line of works are based on\nframe reconstruction. Ranzato et al. [44] proposed a RNN model to predict missing frames or future\nframes from an input video sequence. Srivastava et al. [51] extended this framework with LSTM\nencoder-decoder model to reconstruct input sequence and predict future sequence. While the above\nrepresentation learning mostly capture semantic features, Luo et al. [31] proposed an unsupervised\nlearning framework that predicts future 3D motions from a pair of consecutive frames. Their learned\nrepresentations show promising results for supervised action recognition. However, previous works\noften learn view-speciﬁc representations which are sensitive to viewpoint changes.\n2\n2.2\nAction Recognition\nRGB Action Recognition.\nAction recognition from RGB videos is a long-standing problem. A\ndetailed survey can be found in [4]. Recent approaches have shown great progress in this ﬁeld, which\ncan be generally divided into two categories. The ﬁrst category focuses on designing handcrafted\nfeatures for video representation, where the most successful example is improved dense trajectory\nfeatures [54] combined with Fisher vector encoding [36]. The second category uses deep networks to\njointly learn feature representation and classiﬁer. Simonyan and Zisserman [49] proposed two-stream\nCNNs, which extracts spatial and motion representation from video frames and optical ﬂows. RNN\nbased architectures have also been proposed to model the temporal information [6, 33]. However, deep\nnetworks training requires large amount of human-labeled data. CNNs pre-trained with ImageNet are\ncommonly adopted as backbone [3, 6, 33, 49], to facilitate training and avoid overﬁtting.\nRGB-D Action Recognition.\nSince the ﬁrst work on action recognition using depth maps [27],\nresearchers have proposed methods for action recognition that extract features from multi-modal data,\nincluding depth, RGB, and skeleton [7, 13, 16, 28, 29, 30, 37, 42, 43, 48, 55, 57]. Recently, Wang et\nal. [58] used 3D scene ﬂow [19] calculated with RGB-D data as input for action recognition. State-of-\nthe-art methods for RGB-D action recognition report human level performance on well-established\ndatasets such as MSR-DailyActivity3D [55].However, [47] shows that there is a big performance\ngap between human and existing methods on the more challenging NTU-RGBD dataset [47], which\ncontains signiﬁcantly more subjects, viewpoints, action classes and backgrounds.\nView-invariant Feature Representation. One particularly challenging aspect of action recognition\nis to recognize actions from varied unknown and unseen views, referred to as cross-view action\nrecognition in the literature. The performance of most existing methods [6, 30, 33, 36, 37, 49, 54]\ndrop sharply as the viewpoint changes due to the inherent view dependence of the features used by\nthese methods. To tackle this problem, researchers have proposed methods to learn representations\ninvariant to viewpoint changes. Some methods create spatial-temporal representations that are\ninsensitive to view variations [25, 38, 57], while other methods ﬁnd a view independent latent space\nin which features extracted from different views are directly comparable [26, 43, 62]. For example,\nRahmani et al. [43] used a deep network to project dense trajectory features from different views into a\ncanonical view. However, most of the previous methods require access to 3D human pose information\n(e.g. mocap data [43], skeleton [57]) during training, while others are limited by their discriminative\npower. Moreover, existing methods other than skeleton based methods [28, 29, 24, 40, 61] have not\nshown effective performance on the cross-view evaluation for NTU-RGBD dataset.\n3\nMethod\nThe goal of our unsupervised learning method is to learn video representations that capture view-\ninvariant motion dynamics. We achieve this by training a model that uses the representation to predict\nsequences of motion for multiple views. The motions are represented as 3D dense scene ﬂows,\ncalculated using the primal-dual method [19] with RGB-D data. The learned representation can then\nbe used as a discriminative motion feature for action recognition. In this section, we ﬁrst present the\nunsupervised learning framework, followed by the action recognition method.\n3.1\nLearning Framework\nAn overview of the learning framework is illustrated in Figure 1. It is an end-to-end deep network\nthat consists of four components: encoder, cross-view decoder, reconstruction decoder, and view\nclassiﬁer, parameterized by {θe, θd, θr, θg} respectively. The goal is to minimize the following loss:\nL = Lxview + αLrecon + βLcls,\n(1)\nwhere α and β are weights to balance the interaction of the loss terms. Lxview is the cross-view ﬂow\nprediction loss, Lrecon is the ﬂow reconstruction loss, and Lcls is the view classiﬁcation loss applied\nin an adversarial setting to enhance view invariance. Each loss term involves the encoder and one\nother component. Next we explain each component in detail.\nEncoder.\nLet V denote all the available views of an captured action. The encoder, parameter-\nized by θe, takes as input a sequence of frames for view i ∈V , denoted as Xi = {xi\n1, xi\n2...xi\nT },\nand encodes them into a sequence of low-dimensionality feature embeddings E(Xi; θe) =\n3\n!(#$\n%)\nConv\nBiLSTM\n'$\n%\nConv\nBiLSTM\n'$()\n%\nConv\nBiLSTM\n'$*)\n%\n+$\n,\nCNN\nDeconv\n-.$\n,\n/01234\nEncoder\nCross-view Decoder\nDeconv\n-.$\n%\nReconstruction Decoder\n/53678\n!(#$*)\n%\n)\n!(#$()\n%\n)\nView Classifier\nGRL\n/69:\n;$\n……\n……\nFigure 1: The proposed unsupervised representation learning framework. For a sequence of input frames, the\nencoder generates a sequence of feature representations. At each timestep, the representation is used by the\ncross-view decoder, reconstruction decoder and view classiﬁer, where multiple loss terms are jointly minimized.\nThe encoder can learn to generate view-invariant representations that capture motion dynamics.\n{E(xi\n1), E(xi\n2)...E(xi\nT )}. Speciﬁcally, for each frame, we ﬁrst use a downsampling CNN (denoted as\n“Conv”) to extract a low-dimensionality feature of size h×w×k. Then, a bi-directional convolutional\nLSTM (denoted as “BiLSTM”) runs through the sequence of extracted Conv features. At each\ntimestep t, the BiLSTM generates two feature maps of size h × w × k, one through forward pass and\nthe other through backward pass. The two feature maps are concatenated channel-wise to form the\nencoding E(xi\nt) of size h × w × 2k. In this work, we set h = w = 7 and k = 64.\nCompared with vanilla LSTMs, convolutional LSTMs [39] replace the fully connected transforma-\ntions with spatial convolutions, which can preserve spatial information in intermediate representations.\nWe ﬁnd it to perform much better than vanilla LSTMs. Moreover, our bi-directional LSTM aggregates\ninformation from both previous frames and future frames, which helps to generate richer representa-\ntions. Compared with the LSTM encoder in [31] that only encodes 2 frames, the proposed encoder\ncan generate encodings for longer sequences, which embodies more discriminative motion dynamics\nfor the action. In this work, we set the sequence length T = 6.\nCross-view decoder. The goal of the cross-view decoder is to predict the 3D ﬂow yj\nt for view j\n(j ∈V ; j ̸= i), given the encoding E(xi\nt) for view i, at timestep t. Inferring yj\nt directly from E(xi\nt) is\ntoo difﬁcult, because the decoder has zero information about view j. Therefore, we give an additional\ninput to the decoder that contains view-speciﬁc information. This input is the depth map dj\nt for view\nj at timestep t, which serves as an anchor to inform the decoder about the spatial conﬁguration of\nview j. The decoder still requires view-invariant motion dynamics from E(xi\nt) in order to predict yj\nt.\nSpeciﬁcally, we ﬁrst use a CNN to extract a feature of size h × w × k from dj\nt. The extracted\nfeature is concatenated with E(xi\nt) channel-wise into a feature of size h × w × 3k. Then we use\nan upsampling CNN (denoted as “Deconv”) to perform spatial upsampling. Deconv consists of\nfour fractionally-strided convolutional layers [50] with batch normalization layer [17] and ReLU\nactivation in between. We observe that the batch normalization is critical to optimize the network.\nLet ˆyj\nt = D(E(xi\nt), dj\nt; θd) denote the output of the cross-view decoder for timestep t. We want to\nminimize the mean squared error between ˆyj\nt and yj\nt for t = 1, 2...T:\nLj\nxview(E(Xi), Dj, Y j) =\nT\nX\nt=1\n\r\r\ryj\nt −ˆyj\nt\n\r\r\r\n2\n2,\n(2)\nwhere Dj = {dj\n1, dj\n2...dj\nT } is the sequence of anchor depth frames, and Y j = {yj\n1, yj\n2...yj\nT } is the\nsequence of ﬂows.\nSince we want to learn a video representation that can be used to predict motions for multiple views,\nwe deploy multiple cross-view decoders with shared parameters to all views other then i. Therefore,\n4\nthe cross-view ﬂow prediction loss for Xi is:\nLxview(Xi) =\nX\nj\nLj\nxview(E(Xi), Dj, Y j)\nfor j ∈V ; j ̸= i\n(3)\nReconstruction decoder. The goal of the this decoder is to reconstruct the 3D ﬂow yi\nt given the\nencoding for the same view E(xi\nt). Learning ﬂow reconstruction helps the encoder to extract basic\nmotions, and when used together with cross-view decoders, enhances learning of view-invariant\nmotion dynamics. The architecture of the reconstruction decoder is a Deconv module similar as cross-\nview decoder, with the number of input channels in the ﬁrst layer adapted to 2k. Let ˆyi\nt = R(E(xi\nt); θr)\nbe the output of the reconstruction decoder at timestep t, the ﬂow reconstruction loss is:\nLrecon(Xi, Y i) =\nT\nX\nt=1\n\r\r\ryi\nt −ˆyi\nt\n\r\r\r\n2\n2\n(4)\nView classiﬁer. We propose a view-adversarial training that encourages the encoder to learn video\nrepresentations invariant to view changes. We draw inspiration from the domain-adversarial training\n[9, 10], which aims at learning features that are indiscriminate with respect to shift between domains.\nThe proposed view-adversarial training is achieved by adding a view classiﬁer connected to the\nencoder through a Gradient Reversal Layer (GRL). The view classiﬁer tries to predict which view\nthe encoded representation belongs to, whereas the encoder tries to confuse the view classiﬁer by\ngenerating view-invariant representations.\nMore formally, the view classiﬁer G(E(xi\nt); θg) →pt maps an encoding at timestep t to a probability\ndistribution over possible views V . Learning with GRL is adversarial in that θg is optimized to\nincrease G’s ability to discriminate encodings from different views, while GRL reverses the sign of\nthe gradient that ﬂows back to E, which results in the encoder parameters θe learning representations\nthat reduces the view classiﬁcation accuracy. Essentially, we minimize the cross-entropy loss for the\nview classiﬁcation task with respect to θg, while maximize it with respect to θe. Therefore, we deﬁne\nthe view classiﬁcation loss as the sum of the cross-entropy loss for the entire sequence:\nLcls(Xi) =\nT\nX\nt=1\n−log\n\u0000pi\nt\n\u0001\n,\n(5)\nwhere i is the ground-truth view of the input.\nThe view classiﬁer consists of two fully connected layers and a softmax layer. Since the encoding\nE(xi\nt) is a convolutional feature, it is ﬁrst ﬂattened into a vector before it goes into the view classiﬁer.\n3.2\nAction Recognition\nWe use the encoder from unsupervised learning for action recognition. Given the learned representa-\ntions for a sequence of frames E(X) = {E(xt)|t = 1, 2...T}, we apply an action classiﬁer to each\nE(xt). The action classiﬁer is a simple fully-connected layer, which takes the ﬂattened vector of\nE(xt) as input, and outputs a score st over possible action classes. The ﬁnal score of the sequence is\nthe average score for each timestep: s = 1\nT\nPT\nt=1 st.\nThe action classiﬁer is trained with cross-entropy loss. During training, we consider three scenarios:\n(a) scratch: Randomly initialize the weights of encoder and train the entire model from scratch.\n(b) ﬁne-tune: Initialize the encoder with learned weights and ﬁne-tune it for action recognition.\n(c) ﬁx: Keep the pre-trained encoder ﬁxed and only train the action classiﬁer.\nAt test time, we uniformly sample 10 sequences from each video with sequence length T = 6, and\naverage the scores across the sampled sequences to get the class score of the video.\n4\nExperiments\n4.1\nUnsupervised Representation Learning\nImplementation details. For Conv in encoder and depth CNN in cross-view decoder, we employ\nthe ResNet-18 architecture [15] up until the ﬁnal convolution layer, and add a 1×1×64 convolutional\n5\nTable 1: Cross-view ﬂow prediction error on NTU RGB+D dataset.\nMethod\nCross-subject\nCross-view\nRGB\nDepth\nFlow\nRGB\nDepth\nFlow\nproposed method w/o Lrecon & Lcls\n0.0267\n0.0244\n0.0201\n0.0265\n0.0238\n0.0199\nproposed method w/o Lcls\n0.0259\n0.0235\n0.0198\n0.0252\n0.0223\n0.0194\nproposed method\n0.0254\n0.0229\n0.0193\n0.0248\n0.0220\n0.0193\n(a) walking towards each other.\n(b) sitting down.\nFigure 2: Example of ﬂow sequences with depth input. 3D ﬂows are visualized as RGB images. Upper rows\nare ground-truth ﬂows, and lower rows are predicted ﬂows. Blue box denotes source view ﬂow reconstruction,\nwhereas red box denotes cross-view ﬂow prediction. The model can estimate raw motions for multiple views.\nlayer to reduce the feature size. The number of input channels in the ﬁrst convolutional layer is\nadapted according to input modality. Note that our CNN has not been pre-trained on ImageNet. For\nBiLSTM, we use convolutional ﬁlters of size 7 × 7 × 64 for convolution with input and hidden state.\nWe initialize all weights following the method in [14]. During training, we use a mini-batch of size 8.\nWe train the model using the Adam optimizer [20], with an initial learning rate of 1e−5 and a weight\ndecay of 5e−4. We decrease the learning rate by half every 20000 steps (mini-batches). To avoid\ndistracting the ﬂow prediction task, we activate the view adversarial training after 5000 steps. The\nweights of the loss terms are set as α = 0.5 and β = 0.05, which is determined via cross-validation.\nIn order to effectively predict the motions, we want to describe the motion as low-dimensional\nsignal. Hence, we apply spatial downsampling to the 3D ﬂows by calculating the mean of each\nnon-overlapping 8 × 8 patch. The resulting 28 × 28 × 3 ﬂow maps are multiplied by 50 to keep a\nproper scale, which become the ground-truth Y .\nDataset. We use the NTU RGB+D dataset [47] for unsupervised representation learning. The dataset\nconsists of 57K videos for 60 action classes, captured from 40 subjects in 80 camera viewpoints.\nThe 80 viewpoints can be divided into ﬁve main views based on the horizontal angle of the camera\nwith respect to the subject: front view, left side view, right side view, left side 45 degrees view and\nright side 45 degrees view. These ﬁve views form the view set used in our experiments. Each action\nsequence is simultaneously captured by three cameras from three of the ﬁve views at a time.\nEvaluation.\nThere are two sets of standard evaluation protocols for action recognition on NTU\nRGB+D dataset: cross-subject evaluation and cross-view evaluation. Following it, we conduct two\n6\nTable 2: Action recognition accuracy (%) on NTU RGB+D dataset.\nMethod\nCross-subject\nCross-view\nRGB\nDepth\nFlow\nRGB\nDepth\nFlow\nscratch\n36.6\n42.3\n70.2\n29.2\n37.7\n72.6\nﬁx\n48.9\n60.8\n77.0\n40.7\n53.9\n78.8\nﬁne-tune w/o view-adversarial\n53.4\n66.0\n80.3\n46.2\n60.1\n81.9\nﬁne-tune\n55.5\n68.1\n80.9\n49.3\n63.9\n83.4\nunsupervised learning experiments, where in each experiment we ensure that the encoder will not be\ntrained on any test samples in supervised learning setting. For cross-subject evaluation, we follow\nthe same training and testing split as in [47]. For cross-view evaluation, samples of cameras 2 and 3\nare used for training while those of camera 1 for testing. Since we need at least two cameras for our\nunsupervised task, we randomly divide the supervised training set with ratio of 8:1 for unsupervised\ntraining and test. We use the cross-view ﬂow prediction loss Lxview as the evaluation metric, which\nquantiﬁes the performance of the model to predict motions across different views. We experiment\nwith three input modalities: RGB, depth and 3D ﬂow.\nResults. Table 1 shows the quantitative results for the unsupervised ﬂow prediction task. In order\nto demonstrate the effect of different components and loss terms, we evaluate different variants\nof the proposed framework. First, we only train the encoder with cross-view decoder (denoted as\nproposed method w/o Lrecon&Lcls). Then we add the reconstruction decoder with ﬂow reconstruction\nloss (denoted as proposed method w/o Lcls). Finally we add view adversarial training with view\nclassiﬁcation loss to form the proposed method. Across all input modalities, ﬂow reconstruction and\nview adversarial training both can improve the cross-view ﬂow prediction performance. Comparing\nbetween different input modalities, ﬂow achieves the lowest Lxview. This is expected because ﬂow\ncontains more view-invariant motion information.\nFigure 2 shows qualitative examples of ﬂow prediction with depth input. For each pair of rows, the\nupper rows are ground-truth ﬂows, whereas the lower rows are ﬂows predicted by the decoders. The\nmodel shows the ability to estimate raw motions for multiple views with the encoded representations.\n4.2\nAction Recognition on NTU RGB+D\nImplementation details. We experiment with the three settings described in Section 3.2. We train\nthe model using Adam optimizer [20], with the mini-batch size as 16, learning rate as 1e−4 and\nweight decay as 5e−4. We set the learning rate of the encoder to be 1e−5 for ﬁne-tune. For scratch,\nwe decay the learning rate by half every 20000 steps. For ﬁne-tune and ﬁx, since training converges\nfaster, we half the learning rate every 10000 steps.\nResults.\nTable 2 shows the classiﬁcation accuracy for both cross-subject and cross-view action\nrecognition with three input modalities. Across all modalities, supervised learning from scratch\nhas the lowest accuracy. Using the unsupervised learned representations and training only a linear\naction classiﬁer (ﬁx) signiﬁcantly increases accuracy. Fine-tuning the encoder can further improve\nperformance. If we remove the view-adversarial training in the unsupervised framework, the accuracy\nwould decrease, especially for cross-view recognition.\nAmong the three input modalities, ﬂow input achieves the highest accuracy, which agrees with our\nunsupervised learning result. Flow is also the only input modality that has a higher accuracy for\ncross-view recognition compared with cross-subject recognition. This supports our observation that\nﬂow is more view-invariant than the other two modalities.\nComparison with state-of-the-art.\nIn Table 3 we compare our method against state-of-the-art\nmethods on NTU RGB+D dataset. The ﬁrst group of methods use depth as input, and the second group\nof methods use skeleton as input. We re-implement two unsupervised learning methods [31, 32] (in\nitalic) and report their classiﬁcation accuracy. We do not directly cite the results in [31] because [31]\nreports mAP rather than accuracy. Our re-implementation achieve similar mAP as in [31].\nUsing depth input, the proposed method outperforms all previous methods. The increase in accu-\nracy is more signiﬁcant for cross-view recognition, which shows that the learned representation is\n7\nTable 3: Comparison with state-of-the-art methods for action recognition on NTU RGB+D dataset.\nMethod\nModality\nCross-subject\nCross-view\nHOG [35]\nDepth\n32.24\n22.27\nSuper Normal Vector [60]\n31.82\n13.61\nHON4D [37]\n30.56\n7.26\nShufﬂe and Learn [32]\n46.2\n40.9\nLuo et al. [31]\n61.4\n53.2\nOurs\n68.1\n63.9\nLie Group [52]\nSkeleton\n50.08\n52.76\nFTP Dynamic Skeletons [16]\n60.23\n65.22\nHBRNN-L [7]\n59.07\n63.97\n2 Layer P-LSTM [47]\n62.93\n70.27\nST-LSTM [28]\n69.2\n77.7\nGCA-LSTM [29]\n74.4\n82.8\nEnsemble TS-LSTM [24]\n74.60\n81.25\nDepth+Skeleton [40]\n75.2\n83.1\nVA-LSTM [61]\n79.4\n87.6\nOurs\nFlow\n80.9\n83.4\nTable 4: Cross-subject action recognition accu-\nracy (%) on MSRDailyActivity3D dataset.\nMethod\nAccuracy\nActionlet Ensemble [56] (S)\n85.8\nHON4D [37] (D)\n80.0\nMST-AOG [57] (D)\n53.8\nSNV [60] (D)\n86.3\nHOPC [41] (D)\n88.8\nLuo et al. [31] (D)\n75.2\nOurs (scratch)\n42.5\nOurs (ﬁne-tune)\n82.3\nTable 5: Cross-view action recognition accuracy\n(%) on Northwestern-UCLA dataset.\nMethod\nAccuracy\nActionlet Ensemble [56] (S)\n69.9\nHankelets [25]\n45.2\nMST-AOG [57] (D)\n53.6\nHOPC [41] (D)\n71.9\nR-NKTM [43] (S)\n78.1\nLuo et al. [31] (D)\n50.7\nOurs (scratch)\n35.8\nOurs (ﬁne-tune)\n62.5\ninvariant to viewpoint changes. Using ﬂow input, our method achieves comparable performance to\nskeleton-based methods. However, skeleton is a higher level feature that is more robust to viewpoint\nchange. Moreover, the method [61] with higher cross-view accuracy uses explicit coordinate system\ntransformation to achieve view invariance.\n4.3\nTransfer Learning for Action Recognition\nIn this section, we perform transfer learning tasks, where we use the unsupervised learned representa-\ntions for action recognition on two other datasets in new domains (different subjects, environments\nand viewpoints). We perform cross-subject evaluation on MSR-DailyActivity3D Dataset, and cross-\nview evaluation on Northwestern-UCLA MultiviewAction3D Dataset. We experiment with scratch\nand ﬁne-tune settings, using depth modality as input.\nMSR-DailyActivity3D Dataset. This dataset contains 320 videos of 16 actions performed by 10\nsubjects. We follow the same experimental setting as [55], using videos of half of the subjects as\ntraining data, and videos of the rest half as test data.\nNorthwestern-UCLA MultiviewAction3D Dataset.\nThis dataset contains 1493 videos of 10\nactions performed by 10 subjects, captured by 3 cameras from 3 different views. We follow [57] and\nuse videos from the ﬁrst two views for training and videos from the third view for test.\nResults.\nTable 4 and 5 show our results in comparison with state-of-the-art methods. On both\ndatasets, training a deep model from scratch gives poor performance. Using the unsupervised\nlearned representations increases the accuracy by a large margin. Our method outperforms previous\n8\nunsupervised method [31], and achieves comparable performance with skeleton-based methods\n(marked by S) and depth-based methods (marked by D) that use carefully hand-craft features. This\ndemonstrates that the learned representations can generalize across domains.\n5\nConclusion\nIn this work, we propose an unsupervised learning framework that leverages unlabeled video data\nfrom multiple views to learn view-invariant video representations that capture motion dynamics. We\nlearn the video representations by using the representations for a source view to predict the 3D ﬂows\nfor multiple target views. We also propose a view-adversarial training to enhance view-invariance\nof the learned representations. We train our unsupervised framework on NTU RGB+D dataset, and\ndemonstrate the effectiveness of the learned representations on both cross-subject and cross-view\naction recognition tasks across multiple datasets.\nThe proposed unsupervised learning framework can be naturally extended beyond actions. For future\nwork, we intend to extend our framework for view-invariant representation learning in other tasks\nsuch as gesture recognition and person re-identiﬁcation. In addition, we can consider generative\nadversarial network (GAN) [12] for multi-view data generation.\nAcknowledgments\nThis research is supported by the National Research Foundation, Prime Minister’s Ofﬁce, Singapore\nunder its Strategic Capability Research Centres Funding Initiative.\nReferences\n[1] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In\nNIPS, pages 153–160, 2006.\n[2] Y. Bengio, E. Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop.\nIn ICML, pages 226–234, 2014.\n[3] J. Carreira and A. Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In\nCVPR, pages 4724–4733, 2017.\n[4] G. Cheng, Y. Wan, A. N. Saudagar, K. Namuduri, and B. P. Buckles. Advances in human action recognition:\nA survey. arXiv preprint arXiv:1501.05964, 2015.\n[5] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction.\nIn ICCV, pages 1422–1430, 2015.\n[6] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, T. Darrell, and K. Saenko.\nLong-term recurrent convolutional networks for visual recognition and description. In CVPR, pages\n2625–2634, 2015.\n[7] Y. Du, W. Wang, and L. Wang. Hierarchical recurrent neural network for skeleton based action recognition.\nIn CVPR, pages 1110–1118, 2015.\n[8] B. Fernando, H. Bilen, E. Gavves, and S. Gould. Self-supervised video representation learning with\nodd-one-out networks. In CVPR, pages 5729–5738, 2017.\n[9] Y. Ganin and V. S. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, pages\n1180–1189, 2015.\n[10] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky.\nDomain-adversarial training of neural networks. JMLR, 17(1):2096–2030, 2016.\n[11] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image\nrotations. In ICLR, 2018.\n[12] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and\nY. Bengio. Generative adversarial nets. In NIPS, pages 2672–2680, 2014.\n[13] A. Haque, B. Peng, Z. Luo, A. Alahi, S. Yeung, and F. Li. Towards viewpoint invariant 3d human pose\nestimation. In ECCV, pages 160–177, 2016.\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on\nimagenet classiﬁcation. In ICCV, pages 1026–1034, 2015.\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages\n770–778, 2016.\n[16] J. Hu, W. Zheng, J. Lai, and J. Zhang. Jointly learning heterogeneous features for RGB-D activity\nrecognition. In CVPR, pages 5344–5352, 2015.\n[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In ICML, pages 448–456, 2015.\n9\n[18] L. Isik, A. Tacchetti, and T. Poggio. A fast, invariant representation for human action in the visual system.\nJournal of neurophysiology, 119(2):631–640, 2017.\n[19] M. Jaimez, M. Souiai, J. G. Jiménez, and D. Cremers. A primal-dual framework for real-time dense\nRGB-D scene ﬂow. In ICRA, pages 98–104, 2015.\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Y. Kong, Z. Ding, J. Li, and Y. Fu. Deeply learned view-invariant features for cross-view action recognition.\nIEEE Trans. Image Processing, 26(6):3028–3037, 2017.\n[22] Q. V. Le. Building high-level features using large scale unsupervised learning. In ICASSP, pages 8595–\n8598, 2013.\n[23] H. Lee, J. Huang, M. Singh, and M. Yang. Unsupervised representation learning by sorting sequences. In\nICCV, pages 667–676, 2017.\n[24] I. Lee, D. Kim, S. Kang, and S. Lee. Ensemble deep learning for skeleton-based action recognition using\ntemporal sliding LSTM networks. In ICCV, pages 1012–1020, 2017.\n[25] B. Li, O. I. Camps, and M. Sznaier. Cross-view activity recognition using hankelets. In CVPR, pages\n1362–1369, 2012.\n[26] R. Li and T. Zickler. Discriminative virtual views for cross-view action recognition. In CVPR, pages\n2855–2862, 2012.\n[27] W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. In CVPR, pages 9–14, 2010.\n[28] J. Liu, A. Shahroudy, D. Xu, and G. Wang. Spatio-temporal LSTM with trust gates for 3D human action\nrecognition. In ECCV, pages 816–833, 2016.\n[29] J. Liu, G. Wang, P. Hu, L. Duan, and A. C. Kot. Global context-aware attention LSTM networks for 3d\naction recognition. In CVPR, pages 3671–3680, 2017.\n[30] C. Lu, J. Jia, and C. Tang. Range-sample depth feature for action recognition. In CVPR, pages 772–779,\n2014.\n[31] Z. Luo, B. Peng, D. Huang, A. Alahi, and L. Fei-Fei. Unsupervised learning of long-term motion dynamics\nfor videos. In CVPR, pages 7101–7110, 2017.\n[32] I. Misra, C. L. Zitnick, and M. Hebert. Shufﬂe and learn: Unsupervised learning using temporal order\nveriﬁcation. In ECCV, pages 527–544, 2016.\n[33] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici. Beyond short\nsnippets: Deep networks for video classiﬁcation. In CVPR, pages 4694–4702, 2015.\n[34] M. Noroozi, H. Pirsiavash, and P. Favaro. Representation learning by learning to count. In ICCV, pages\n5899–5907, 2017.\n[35] E. Ohn-Bar and M. M. Trivedi. Joint angles similarities and HOG2 for action recognition. In CVPR\nworkshops, pages 465–470, 2013.\n[36] D. Oneata, J. J. Verbeek, and C. Schmid. Action and event recognition with ﬁsher vectors on a compact\nfeature set. In ICCV, pages 1817–1824, 2013.\n[37] O. Oreifej and Z. Liu. HON4D: histogram of oriented 4d normals for activity recognition from depth\nsequences. In CVPR, pages 716–723, 2013.\n[38] V. Parameswaran and R. Chellappa. View invariance for human action recognition. IJCV, 66(1):83–101,\n2006.\n[39] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal video autoencoder with differentiable memory.\nIn ICLR workshops, 2016.\n[40] H. Rahmani and M. Bennamoun. Learning action recognition model from depth and skeleton videos. In\nICCV, pages 5833–5842, 2017.\n[41] H. Rahmani, A. Mahmood, D. Q. Huynh, and A. S. Mian. Histogram of oriented principal components for\ncross-view action recognition. IEEE TPAMI, 38(12):2430–2443, 2016.\n[42] H. Rahmani and A. S. Mian. 3d action recognition from novel viewpoints. In CVPR, pages 1506–1515,\n2016.\n[43] H. Rahmani, A. S. Mian, and M. Shah. Learning a deep model for human action recognition from novel\nviewpoints. IEEE TPAMI, 40(3):667–681, 2018.\n[44] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and S. Chopra. Video (language) modeling: a\nbaseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.\n[45] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In AISTATS, pages 448–455, 2009.\n[46] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-\nstage feature learning. In CVPR, pages 3626–3633, 2013.\n[47] A. Shahroudy, J. Liu, T. Ng, and G. Wang. NTU RGB+D: A large scale dataset for 3D human activity\nanalysis. In CVPR, pages 1010–1019, 2016.\n[48] A. Shahroudy, T.-T. Ng, Y. Gong, and G. Wang. Deep multimodal feature analysis for action recognition\nin RGB+D videos. IEEE TPAMI, 40(5):1045–1058, 2018.\n[49] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In\nNIPS, pages 568–576, 2014.\n10\n[50] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Riedmiller. Striving for simplicity: The all\nconvolutional net. arXiv preprint arXiv:1412.6806, 2014.\n[51] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using\nlstms. In ICML, pages 843–852, 2015.\n[52] R. Vemulapalli, F. Arrate, and R. Chellappa. Human action recognition by representing 3d skeletons as\npoints in a lie group. In CVPR, pages 588–595, 2014.\n[53] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing robust features with\ndenoising autoencoders. In ICML, pages 1096–1103, 2008.\n[54] H. Wang and C. Schmid. Action recognition with improved trajectories. In ICCV, pages 3551–3558, 2013.\n[55] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet ensemble for action recognition with depth cameras.\nIn CVPR, pages 1290–1297, 2012.\n[56] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Learning actionlet ensemble for 3d human action recognition. IEEE\nTPAMI, 36(5):914–927, 2014.\n[57] J. Wang, X. Nie, Y. Xia, Y. Wu, and S. Zhu. Cross-view action modeling, learning, and recognition. In\nCVPR, pages 2649–2656, 2014.\n[58] P. Wang, W. Li, Z. Gao, Y. Zhang, C. Tang, and P. Ogunbona. Scene ﬂow to action map: A new\nrepresentation for RGB-D based action recognition with convolutional neural networks. In CVPR, pages\n416–425, 2017.\n[59] X. Wang, K. He, and A. Gupta. Transitive invariance for self-supervised visual representation learning. In\nICCV, pages 1338–1347, 2017.\n[60] X. Yang and Y. Tian. Super normal vector for activity recognition using depth sequences. In CVPR, pages\n804–811, 2014.\n[61] P. Zhang, C. Lan, J. Xing, W. Zeng, J. Xue, and N. Zheng. View adaptive recurrent neural networks for\nhigh performance human action recognition from skeleton data. In ICCV, pages 2136–2145, 2017.\n[62] Z. Zhang, C. Wang, B. Xiao, W. Zhou, S. Liu, and C. Shi. Cross-view action recognition via a continuous\nvirtual path. In CVPR, pages 2690–2697, 2013.\n11\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-09-06",
  "updated": "2018-09-06"
}