{
  "id": "http://arxiv.org/abs/1809.02074v1",
  "title": "Emergence of Human-comparable Balancing Behaviors by Deep Reinforcement Learning",
  "authors": [
    "Chuanyu Yang",
    "Taku Komura",
    "Zhibin Li"
  ],
  "abstract": "This paper presents a hierarchical framework based on deep reinforcement\nlearning that learns a diversity of policies for humanoid balance control.\nConventional zero moment point based controllers perform limited actions during\nunder-actuation, whereas the proposed framework can perform human-like\nbalancing behaviors such as active push-off of ankles. The learning is done\nthrough the design of an explainable reward based on physical constraints. The\nsimulated results are presented and analyzed. The successful emergence of\nhuman-like behaviors through deep reinforcement learning proves the feasibility\nof using an AI-based approach for learning humanoid balancing control in a\nunified framework.",
  "text": "Emergence of Human-comparable Balancing Behaviors by Deep\nReinforcement Learning\nChuanyu Yang, Taku Komura, Zhibin Li\nAbstract— This paper presents a hierarchical framework\nbased on deep reinforcement learning that learns a diversity\nof policies for humanoid balance control. Conventional zero\nmoment point based controllers perform limited actions during\nunder-actuation, whereas the proposed framework can perform\nhuman-like balancing behaviors such as active push-off of an-\nkles. The learning is done through the design of an explainable\nreward based on physical constraints. The simulated results are\npresented and analyzed. The successful emergence of human-\nlike behaviors through deep reinforcement learning proves the\nfeasibility of using an AI-based approach for learning humanoid\nbalancing control in a uniﬁed framework.\nI. INTRODUCTION\nHumans efﬁciently make use of under-actuated control,\nsuch as toe tilting and heel rolling, for keeping balance\nwhen standing and walking. Biomechanical study of human\nwalking has discussed about the advantage of rolling around\nthe heel and toe during walking phase [1]. From a biome-\nchanical point of view, tilting the foot creates better foot-\nground clearance allowing the maximum ankle torques to be\nexploited [2], [3].\nFoot tilting give rise to a control problem as an under-\nactuated degree of freedom (DOF) is introduced. Once foot\ntilting occurs, the edge of the foot namely the heel or toe,\nbecomes the sole contact point between the foot and the\nground which the body pivots around. This new pivot point\nis an underactuated DOF as zero torque can be applied on\nthe pivoting axis. The physically feasible range of center of\npressure (COP) converges to a singular boundary line on the\nedge of the foot. The controller has no control authority over\nthe new underactuated DOF since no torque can be applied.\nMany modern humanoid robots are designed to closely\nresemble the human morphology. In theory, they possess sim-\nilar capabilities of a human, and should be able to perform\nfoot tilting behaviors comparable to humans. However, most\nrobots are shown to focus on keeping the foot ﬂat on the\nground during locomotion, which is unnatural and inefﬁcient.\nThe reason is not due to the physical capabilities, but rather\nbecause of the limitation of modern control and actuation\ntechniques. Most balance and walking control approaches\nare based on ZMP, and are developed on the assumption of\na fully actuated system where the foot is placed ﬂat on the\nground creating a large size of support polygon. Most ZMP\nbased methods will fail during underactuation phases, as they\nneed to restrict the ZMP or COP to be within the support\npolygon created by a ﬂat foot and away from the foot edges.\nChuanyu Yang, Taku Komura, and Zhibin Li are with the School of\nInformatics, The University of Edinburgh, UK.\nEmail: {chuanyu.yang, tkomura, zhibin.li}@ed.ac.uk\nCOM of individual link\nJoint\nFig. 1: Side view of valkyrie robot and the 2D humanoid\ncharacter modelled according to Valkyrie robot.\nControllers that permits the COP to lie on the narrow\nboundary of the foot have been developed to generate under-\nactuated foot tilting behaviors, demonstrating the feasibility\nof using analytic engineering approach for designing con-\ntrollers capable of dealing with underactuated phases during\nbalance recovery [3].\nRecently, machine learning approaches such as deep rein-\nforcement learning have been attracting robotics researchers,\nwhich can automatically learn the parameters for achieving\nthe given objective. Engineering based approaches require\na lot of human knowledge in designing the controllers and\nadditional effort in tuning, which is a disadvantage. Machine\nlearning approaches such as deep reinforcement learning\n(RL) have a major advantage: they require less manual tun-\ning. Certainly, reinforcement learning also requires a certain\namount of human knowledge and effort while designing the\nRL agent and the reward, but rather the main effort is in\nthe knowledge-based construction of the agent and reward,\ninstead of structuring explicit controllers. After the proper\nRL agent and reward are constructed, the agent is capable\nof learning the optimal policy by itself. Recent works done\non deep reinforcement learning have demonstrated that deep\nRL is capable of dealing with very complex and dynamic\nmotor tasks in continuous state and action spaces. Therefore,\nit is clear that deep reinforcement learning should have the\ncapability of learning a policy to deal with both ﬂat foot and\nfoot tilting situations.\nIn this paper, we propose a novel framework based on\ndeep reinforcement learning that can make use of under-\narXiv:1809.02074v1  [cs.RO]  6 Sep 2018\nactuated behavior for keeping the balance during standing.\nSince deep reinforcement learning paradigm is shown to\nallow very distinct and complex behaviours to emerge from\nsimple rewards [4], following the prior work [3], we are\nmotivated to explore an alternative to use deep reinforcement\nlearning to acquire a policy that is capable of generating\nhuman-comparable behaviors during push recovery without\nproviding any explicit knowledge of the control policies.\nThe contributions of our study are the following:\n• Provided a physical analysis on the reward design of\nhumanoid balancing;\n• Demonstrated that deep reinforcement learning is ca-\npable of learning a human like balancing strategy with\nlimited knowledge provided.\nII. RELATED WORK AND MOTIVATION\nRecent breakthroughs in reinforcement learning and deep\nlearning, have given rise to deep reinforcement learning\n(deep RL), which is a combination of reinforcement learning\nand deep neural networks. The rise of deep RL has enhanced\nthe capability of agents to perform more complex and dy-\nnamic tasks in high dimensional continuous state and action\nspaces. There are quite a few well known deep RL algorithms\ndedicated to solving problems in continuous state and action\nspaces such as Trust Region Policy Optimization (TRPO) [5],\nNormalized Advantage Function (NAF) [6], Asynchronous\nAdvantage Actor Critic (A3C) [7] and Deep Deterministic\nPolicy Gradient (DDPG) method [8].\nResearchers in the computer science and robotics com-\nmunity have published a few papers on using deep rein-\nforcement learning for humanoid motion control. Peng et\nal. successfully applied Continuous Actor Critic Learning\nAutomaton (CACLA) [9], [10] to train a bipedal character\nto learn terrain traversal skills for terrain with gaps and walls\n[11]. Later, they developed a hierarchical deep reinforcement\nlearning framework, having the low-level controller (LLC)\nto specialize on balance and limb control, while the high-\nlevel controller (HLC) focuses on navigation and trajectory\nplanning. Using their framework, the bipedal character suc-\ncessfully learned the skills in order to perform tasks such as\nsoccer ball guiding, path following and obstacle avoidance\n[12].\nKumar et al. used deep reinforcement learning to learn a\nsafe falling strategy for humanoids to minimize damage dur-\ning fall. Their algorithm is based on CACLA and the Mixture\nof Actor-Critic Experts (MACE) architecture [13]. In this\narchitecture, each joint is assigned with an independent actor-\ncritic pair. The actor with the highest corresponding critic\nvalue will be activated to generate action. This architecture\ncombines both continuous control and discrete controls.\nA lot of control methods have been proposed for humanoid\nbalancing [14], [15], [16]. However, most controllers do\nnot deal with foot tilting during humanoid locomotion as it\nrestricts the center of pressure to a single point causing the\nsystem to be underactuated creating immense difﬁculties for\nthe design of controllers. Instead, this problem is bypassed\nby restricting the foot to remain ﬂat on the ground. However,\nFig. 2: State features for the biped.\nthis ﬂat foot balancing behaviour is different from what we\nhumans do.\nFew works have been done on the topic of balance\nrecovery by active foot tilting. Li et al. have done a thorough\nanalysis on the dynamics of foot tilting and derived a foot\ntilting (ankle push-off) balancing strategy. They explained\nthe underlying mechanism and the signiﬁcance of foot tilting\n[3]. Concrete physical and mathematical proof in [3] suggest\nthat foot tilting balance strategy is more robust against\nforce perturbations than ﬂat foot balance strategy, and have\nsuccessfully designed a controller capable of underactuated\nfoot tilting and implemented it on a real robot.\nSince the physical viability of stable underactuated be-\nhaviours has been achieved in a deterministic and analytic\napproach using modern control techniques [3], our study\nhereby aim to answer whether similar human-like behavior\ncan be a natural outcome using machine learning approach,\nspeciﬁcally, deep reinforcement learning. Our task has to\nbe performed in a continuous environment, thus the deep\nreinforcement learning has to be applicable in continuous\nstate and action spaces.\nIII. PRELIMINARIES\nIn this section we brieﬂy review some of the concepts\nessential for understanding the paper.\nA. State Representation\nThe bipedal character conﬁguration used in this paper is\nshown in Fig. 1, it is roughly modelled according to Valkyrie,\nwith the notion to apply the deep RL humanoid balancing\nstrategy on the Valkyrie robot in the future.\nThe state input for reinforcement learning is crucial,\nthe input state features should contain adequate amount of\ninformation that allows the reinforcement learning algorithm\nto learn a good policy, but it should not be too redundant as\nincreasing the input dimension increases training time and in-\ntroducing irrelevant parameters can hinder the performance.\nIn our case, the state should contain enough information\nabout the kinematics and the dynamics of the humanoid.\nFig. 2 shows the selected state features, it includes pelvis\nheight (hheight), joint angle and joint velocity, the angle (φtorso,\nFriction cone\nx axis\nz axis\nFig. 3: ˙xCOM and ˙zCOM are the actual COM velocities while\nthe humanoid falls under the force of gravity. xCOM and zCOM\nis the position of the COM on the sagittal plane. φtorso and\nφtorso are orientation of the upper body. l is the length from\nthe center of the foot to the COM.\nφpelvis) and angular velocity( ˙φtorso, ˙φpelvis) of the pelvis and\ntorso, ground contact information, displacement of COM of\nlinks with reference to (w.r.t) the pelvis (red) and the linear\nvelocities of all body links (green).\nB. Capture Point\nCapture point is a concept commonly used in humanoid\nlocomotion, it is deﬁned as a point on the ground where\nthe robot can step to in order to bring itself to a complete\nstop [17]. Knowing the velocity and height of the inverted\npendulum, and the gravitational acceleration, we are able to\ncompute the capture point,\nxcapture = xCOM + ˙xCOM\nrz0\ng ,\n(1)\nJreject = m\nr g\nzc\n∆COP,\n(2)\nwhere the impulse Jreject derived by capture point is the\ntheoretical maximum of the impulse that can be rejected,\nwhere zc is the COM height, ∆COP is the relative horizontal\ndistance between the constant COP and the initial COM\nposition, m is the total mass of the inverted pendulum [3].\nThe capture point as an indication of balance is considered\nto be within the support polygon, so the maximum reacha-\nbility of the capture point is at the edge of the foot. If the\nimpulse time timpulse and the mass of the humanoid m are\nknown, we can derive other useful physical properties such\nas the maximum force Fmax the robot can withstand and the\nmaximum velocity disturbance Vmax of the COM when the\nrobot is still able to balance.\nVmax = Jreject\nm\n(3)\nFmax = Jreject\ntimpulse\n(4)\nC. Explainable Design of a Reward\nIn this section, we describe the design of our reward\nfunction for the deep reinforcement learning for maintaining\nthe balance of our humanoid model, which is based on ideas\nsuch as capture point and linear inverted pendulum model\nfrom conventional control techniques.\nThe reward has to be designed carefully in order to\nproduce desired results: Amodei et al. has mentioned the\nperformance and safety issues that can be caused by a poorly\ndesigned reward function in reinforcement learning [18]. To\ndeal with the difﬁculty of designing a functional reward\nwith human knowledge, researchers came up with methods\nsuch as inverse reinforcement learning, sometimes referred\nas imitation learning or apprenticeship learning. The idea\nis to elicit the reward function from a given sequence of\ndemonstrations [19]–[21]. In our case, we have no knowledge\non how the humanoid should react in order to produce\nbalancing behaviou that involves foot tiltingr. Therefore\ninverse reinforcement learning is not an option, as we are\nnot able to provide proper demonstration.\nBalancing can be decomposed into six objectives, turning\nthe task into a multi-objective problem: keeping the torso\nand pelvis orientation upright (denoted by rφtorso and rφpelvis\nin the reward), keeping the horizontal position of the COM\nclose to the center of the foot (denoted by rxCOM), keeping\nthe COM vertical position at a certain height (denoted by\nrzCOM) and minimizing the horizontal and vertical velocity of\nthe COM (denoted by r ˙xCOM and r ˙zCOM). The total reward is\nthe linear combination of the individual objectives as:\nr = wφtorsorφtorso + wφpelvisrφpelvis + wxCOMrxCOM\n+ wzCOMrzCOM + w ˙xCOMr ˙xCOM + w ˙zCOMr ˙zCOM,\n(5)\nwhere, the weights w[·] of each objective in the reward\nare set to 1 in default except wzCOM which is set to 5 for\ncounteracting gravity.\nThe individual reward terms r[·] are deﬁned such that\nthe individual parameters are attracted to their target values,\nwhile degrading exponentially as getting farther:\nrφtorso = exp(−αφtorso φ2\ntorso)\nrφpelvis = exp(−αφpelvis φ2\npelvis)\nrxCOM = exp(−αxCOM (xtarget\nCOM −xCOM)2)\nrzCOM = exp(−αzCOM (ztarget\nCOM −zCOM)2)\nr ˙xCOM = exp(−α ˙xCOM ( ˙xtarget\nCOM −˙xCOM)2)\nr ˙zCOM = exp(−α ˙zCOM ( ˙ztarget\nCOM −˙zCOM)2).\n(6)\nThe target for the orientation of the torso and pelvis is 0\nrad. xtarget\nCOM\nand ztarget\nCOM\nare the target horizontal and vertical\nCOM position. ˙xtarget\nCOM\nand ˙ztarget\nCOM\nare the target horizontal and\nvertical COM velocity.\nA preliminary normalization factor α[·] is introduced be-\ncause the the units and range of the values of the physical\nproperties are different. For normalization, we expect the\nindividual rewards presented in (6) are lower than ϵ =\n1×10−5 when it reaches maximum error range. By obtaining\nthe maximum error range e[·] of each physical properties,\nwe can calculate the normalization factor α[·] that meets the\nrequirement as\nαφtorso = −ln(ϵ)/e2\nφtorso\nαφpelvis = −ln(ϵ)/e2\nφpelvis\nαxCOM = −ln(ϵ)/e2\nxCOM\nαzCOM = −ln(ϵ)/e2\nzCOM\nα ˙xCOM = −ln(ϵ)/e2\n˙xCOM\nα ˙zCOM = −ln(ϵ)/e2\n˙zCOM.\n(7)\nWe now describe about how to compute the maximum\nerror range e[·] for each term. Regarding the maximum error\nrange for the torso angle eφtorso and pelvis angle eφpelvis, that\noccurs when φtorso = φpelvis = π/2, which is when the body\nis fully horizontal. Thus, eφtorso = eφpelvis = π/2 rad.\nRegarding the maximum error range for the horizontal\nand vertical COM positions exCOM, ezCOM, we can approximate\nthem by using the linear inverted pendulum model. Fig. 3\nshows the humanoid character lying on the boundary of\nthe friction cone. We consider the situation in which the\npendulum lies on the border of the friction cone as an\nextreme situation, any conﬁguration that falls outside the\nfriction cone is destined to fail and should be ignored.\nAssuming the coefﬁcient of friction µ = 1, the maximum\nangle of the friction cone θmax is π/4 rad. Under the situation\nshown in Fig. 3, the maximum error range for the horizontal\nand vertical COM positions can be computed by\nexCOM = −xCOM\n= sin(θmax)l\n(8)\nezCOM = l −zCOM\n= (1 −cos(θmax))l.\n(9)\nIn our study, we use ex = 0.768 m and ez = 0.318 m.\nRegarding the maximum error range for the horizontal\nand vertical COM velocities, e ˙xCOM, e ˙zCOM, we can compute\nthem using the extreme orientation angle θmax and the Vmax\nbased on the capture point in (3). We consider the situation\nin which the pendulum lies on the boundary of the friction\ncone, where θmax = π/4 rad, to be the extreme condition.\nGiven the orientation angle θmax, the height of the COM\nis z0 = l cos(θmax) and the horizontal displacement of the\nCOM to the COP is ∆COP = −l sin(θmax). From (2) and\n(3), we can compute Vmax =\nq\ng\nzc ∆COP, which can be used\nto calculate the target horizontal velocity ˙xtarget\nCOM . The target\nvertical velocity ˙ztarget\nCOM\nis set to 0 as we wish to minimize the\nvertical movement of the COM. As a result, e ˙xCOM, e ˙zCOM can\nbe computed as follows:\ne ˙xCOM = ˙xtarget\nCOM −˙xCOM\n= −sin(θmax)l\nr\ng\ncos(θmax)l\n−cos(θmax)\np\n2g(1 −cos(θmax))l\n(10)\ne ˙zCOM = ˙ztarget\nCOM −˙zCOM\n= −sin(θmax)\np\n2g(1 −cos(θmax))l\n(11)\nThe maximum error for horizontal and vertical COM velocity\nis thus 4.510 m/s and 1.766 m/s.\nAlgorithm 1 Deep Deterministic Policy Gradient\nInitialize critic Q(s, a|θQ) and actor network µ(s, a|θµ)\nInitialize target networks Q′ and µ′: θQ′ ←θQ,θµ′ ←θµ\nInitialize replay buffer R ←∅\nfor episode=1,M do\nInitialize random process N for action exploration\nReceive initial state observation s1\nfor t=1,T do\nSelect action at = µ(st|θµ) + Nt\nExecute action at and observe reward rt and new\nstate st+1\nStore transition (st, at, rt, st+1) in R\nSample minibatch of N transitions (si, ai, ri, si+1)\nfrom R\nif state si+1 is terminal state then\nSet yi = ri\nelse\nSet yi = ri + γQ′(si+1, µ′(si+1|θµ′)|θQ′)\nend if\nUpdate critic by minimizing loss:\nL = 1\nN\nP\ni(yi −Q(si, ai|θQ))2\nUpdate actor using sampled policy gradient:\n∇θµJ ≈\n1\nN\nP\ni ∇aQ(si, ai|θQ)|s=si,a=µ(si)∇θµµ(s|θµ)|si\nUpdate target networks:\nθQ′ ←τθQ + (1 −τ)θQ′,θµ′ ←τθµ + (1 −τ)θµ′\nend for\nend for\nApplying the calculated maximum range of the physical\nproperties back into (7), we obtain the normalization factor\nas\nαφtorso = 4.67\nαφpelvis = 4.67\nαxCOM = 19.50\nαzCOM = 113.74\nα ˙xCOM = 0.57\nα ˙zCOM = 3.69.\n(12)\nUsing the normalization factors α[·], the individual reward\ncomponents can be computed by (6) and then the total reward\nby (5).\nD. Deep deterministic policy gradient\nThe algorithm we chose to use is the Deep Deterministic\nPolicy Gradient (DDPG) algorithm [8], which is a model-\nfree, off-policy RL algorithm based on Deterministic Policy\nGradient [22] and Deep Q Networks [23]. It is able to learn\npolicies in high-dimensional, continuous state action spaces.\nDDPG is a type of actor critic reinforcement learning\nalgorithm, it uses two separate networks to parameterize the\nactor function and the critic function, respectively. The actor\nnetwork µ(s|θµ) maps the states to a deterministic action,\nand the critic network Q(s, a|θQ) maps the state action pair\nto a Q-value.\nHigh level controller\nNeural Network\nLow level controller\nPD controller\nEnvironment\n500Hz\n25Hz\nJoint torque\n Target\n Joint angle\n Joint angle\n Joint velocity\n feedback\n Reward\n during\n training\n State\n feedback\n Temporal\n difference\n update\nFig. 4: System overview.\nThe critic network is trained by minimizing the loss\nfunction:\nyt = rt + γQ′(st+1, a)|a=µ′(st+1)\nLQ(θQ) = Est,at,rt,st+1∼R\n\u0002\n(Q(st, at) −yt)2\u0003\n.\n(13)\nThe actor network is trained by applying the deterministic\npolicy gradient:\n∇θµJ = Est∼R\n\u0002\n∇aQ(s, a|θQ)|s=st,a=µ(st)∇θµ(s|θµ)|s=st\n\u0003\n.\n(14)\nE. Bounding Action Space\nThe output of the network is desired joint angles, the\nrationale behind the selecting joint angles as the choice of\naction space is discussed in section IV.\nJoints have limits which restrict their range of movement,\ntherefore we have to bound the action space within the angle\nlimit. Using squashing sigmoid activation function such as\ntanh in the output unit is a common way in bounding\nnetwork outputs. However, using tanh has its disadvantages:\nit would easily be saturated at the upper and lower bound of\nthe range and require many updates to decrease, increasing\nthe training time and hindering the performance. We use\nan approach proposed by Hausknecht et. al. called inverting\ngradients to bound the output action parameters [24],\n∇p = ∇p ·\n(\n(pmax −p)/(pmax −pmin)\nif ∇p ≥0\n(p −pmin)/(pmax −pmin)\notherwise ,\n(15)\nwhere ∇p indicates the critic gradient with reference to\naction parameter, pmax, pmin, p indicate the minimum, maxi-\nmum and current activation of the action parameter, respec-\ntively.\nFrom (15), we can see that with inverting gradients ap-\nproach, the gradients are reduced as the output parameter\napproaches near the output boundary of the desired value\nrange, and are inverted if the parameter exceeds the bound-\nary, hereby restraining the output to the desired range.\nState\ninput\nState\ninput\nAction\ninput\nActor network\nCritic network\nAction\noutput\nQ-Value\noutput\nInput layer nodes\nHidden layer nodes\nOutput layer nodes\nFig. 5: Overview of neural network structure.\nIV. HIERARCHICAL STRUCTURE OF HIGH-LEVEL\nLEARNING AND LOW-LEVEL CONTROL\nThe idea of constructing the control architecture in a\nhierarchical manner is widely adopted by many studies [12],\n[25]. In such hierarchical control systems, the Lower-level\ncontroller (LLC) and High-level controller (HLC) work at\ndifferent frequencies, where usually the HLC works at a\nlower frequency.\nThe choice of output action parameterization has been\nproven to have a signiﬁcant effect on the performance on\nreinforcement learning. Peng et al. compared the impact\nof four different actuation models that has different action\nparameterization on deep reinforcement learning: (1) direct\ntorque control; (2) muscle activation for musculotendon units\n(MTU); (3) target joint angle for proportional-derivative\ncontrollers; (4) target joint angle velocity. Their study show\nthat action parameterization including basic feedback such as\ntarget angle for PD control and muscle activation for MTU\ncan improve policy performance and learning speed since\nsuch models are able to reﬂect the embodied biomechanical\nfeatures more accurately [26].\nPD control has been proven to be a good action param-\neterization method as it is able to model the biomechanics\nof a system, and is easy to implement compared to other\ncontrol methods such as MTU. Therefore, we choose joint\nangles as the output for the HLC learnt by DDPG and apply\na PD controller as the LLC to translate the joint angle to\ntorque for motor control. The overall structure of the control\nsystem is shown in Fig 4.\nTABLE I: PD controller parameters\nJoints\nPD parameters\nWaist\nHip\nKnee\nAnkle\nKp (Nm/rad)\n720\n1080\n2580\n3160\nKd (Nms/rad)\n60\n70\n150\n300\nA. High-level Controller\nFor the high level contoller we used DDPG to learn the\ncontrol policy responsible for producing the desired motion\nsynergies, i.e desired joint angles. The network structure is\nshown in Fig. 5. Both the critic and actor network have 2\nhidden layers, each hidden layer contains 100 nodes followed\nby a rectiﬁed linear unit (ReLU) activation function. In\naddition to the state features, the critic network also takes\ninto action parameters as input, the action value skips the\nﬁrst hidden layer and is directly forwarded to the second\nhidden layer. The output of the actor network is the 4 target\njoint angles. The network inputs consists of state features\nthat are continuous, which are ﬁltered through butterworth\nﬁlters with a cutoff frequency of 10Hz, and discrete state\nfeatures that are remained untouched.\nB. Low-level Controller\nWe used PD controller as the low level controller, the input\nfor the PD controller is desired joint angles produced by the\nHLC, and the output is joint torque. The feedback for the PD\ncontroller is ﬁltered through a butterworth ﬁlter with a cutoff\nfrequency of 50Hz. The parameters of the PD controller are\nshown in Table. I.\nu = Kp(θtarget −θmeasured) −Kd ˙θmeasured\n(16)\nV. LEARNING RESULTS\nWe can calculate the maximum rejectable impulse when\nthe humanoid is in its stable balancing conﬁguration accord-\ning to capture point theory. In the stable conﬁguration of the\npolicy representation of the trained network, the horizontal\ndistance from the COM to the front tip and back tip of\nthe feet is respectively 0.189m and 0.111m. The height\nof the COM is 1.084m. According to (1), the maximum\nforward rejectable impulse is 72.8 N·s, maximum backward\nrejectable impulse is 42.6 N·s. A push force with duration\nof 0.1s is applied on the pelvis for simulating the impulse\ndisturbance, therefore magnitude of the force are 728N and\n426N respectively for the forward and backward pushes.\nPrevious work has proven that foot tilting balancing strategy\nis capable of working under boundary rejectable impulse\nconditions [3].\nFig. 6 and 7 respectively presents the data from forward\nand backward push recoveries. Fig. 8 and 9 show the snap-\nshots of maximum ankle joint angles from successful balance\nrecoveries of forward and backward pushes under various\npushes. The snapshots show that under different amount of\ndisturbances, the tilting angles of the foot and the angle of\nankle joint are different. From the simulation results, it is\nobserved that emerged behaviours are comparable to that of\n5\n5.5\n6\n6.5\n7\n7.5\n8\n1.5\n1.6\n1.7\n1.8\nangle (rad)\na\n5\n5.5\n6\n6.5\n7\n7.5\n8\n-0.2\n-0.1\n0\n0.1\nangle (rad)\nb\n5\n5.5\n6\n6.5\n7\n7.5\n8\n-2\n-1\n0\n1\nRate (rad/s)\nc\n5\n5.5\n6\n6.5\n7\n7.5\n8\n-0.4\n-0.2\n0\n0.2\nposition (m)\nd\n5\n5.5\n6\n6.5\n7\n7.5\n8\n1.1\n1.12\n1.14\nposition (m)\ne\n5\n5.5\n6\n6.5\n7\n7.5\n8\ntime (s)\n-600\n-400\n-200\n0\n200\ntorque (N\nm)\nd\nFig. 6: Simulation data of forward push recovery (72.8 N·s).\n(a) Reference and measured ankle joint angle; (b) Orientation\nof torso, pelvis and foot; (c) Angular rate of torso, pelvis and\nfoot pitch; (d) Capture point and COM; (e) COM height; (f)\nAnkle joint torque.\nhumans after sufﬁcient amount of training without any prior\nknowledge explicitly given by designers:\n• Knee lock behavior naturally emerges;\n• Heel/toe tipping behaviors naturally emerge;\n• Able to work beyond the maximum rejectable impulse\ncalculated using capture point (due to vertical motion);\n• Able to actively push-off ankle joint and create foot\ntilting stably in response to different disturbance;\n• Exploitation of maximum achievable ankle torque.\nThe amount of impulse the control system is capable\nof withstanding is slightly larger than the rejectable im-\npulse calculated from the capture point theory, since the\ncapture point uses a linear model assuming constant COM\n5\n5.5\n6\n6.5\n7\n7.5\n8\n1.4\n1.5\n1.6\n1.7\nangle (rad)\na\n5\n5.5\n6\n6.5\n7\n7.5\n8\n-0.2\n0\n0.2\nangle (rad)\nb\n5\n5.5\n6\n6.5\n7\n7.5\n8\n-1\n0\n1\n2\nRate (rad/s)\nc\n5\n5.5\n6\n6.5\n7\n7.5\n8\n-0.4\n-0.2\n0\n0.2\nposition (m)\nd\n5\n5.5\n6\n6.5\n7\n7.5\n8\n1.1\n1.105\n1.11\n1.115\nposition (m)\ne\n5\n5.5\n6\n6.5\n7\n7.5\n8\ntime (s)\n-200\n0\n200\n400\ntorque (N\nm)\nd\nFig. 7: Simulation data of backward push recovery (-42.6\nN·s). (a) Reference and measured ankle joint angle; (b)\nOrientation of torso, pelvis and foot; (c) Angular rate of\ntorso, pelvis and foot pitch; (d) Capture point and COM; (e)\nCOM height; (f) Ankle joint torque.\nheight. The network learns a balancing policy capable of\nwithstanding impulse up to 87N·s and -47.5N·s, which is\nrespectively 119.5% and 111.5% the amount of the forward\nand backward maximum rejectable impulse calculated using\ncapture point (72.8N·s and -42.6N·s). This is because while\nthe humanoid is pivoting around its toe, the horizontal\nvelocity is partially redirected upward, increasing the height\nof the COM, therefore converting part of the kinetic energy\ninto potential energy, slowing down the overall COM ve-\nlocity. Learning-based control system is less restricted than\ntraditional methods using ZMP in this aspect, because any\nstable action than improves the balance recovery will be\nreinforced such as ankle push-off, knee lock or any possible\nNo disturbance\nFig. 8: Maximum ankle angles generated by the policy and\nthe change of angle w.r.t home position (Forward push).\nNo disturbance\nFig. 9: Maximum ankle angles generated by the policy and\nthe change of angle w.r.t home position (Backward push).\nupper body movement.\nSome human-comparable features naturally emerge after\nsufﬁcient amount of training without any prior knowledge\ngiven explicitly by humans. From Fig. 8 and 9, we can see\nthe policy actively increases the ankle angle to produce ankle\npush off behaviour. It is also shown that the humanoid has\nalso learned to actuate its knee in a knee-lock conﬁguration\nthat minimizes knee torque and provides a lot of stability by\nsimply exploiting the biomechanical constraint of the knee\njoint, very similar to what humans do.\nThe balance strategy learned by reinforcement learning\nhas shown to have the ability to actively adjust the ankle\njoint angle and the tilting angle of the foot in response to\nthe amount of disturbance applied. The active change in\nmagnitude of ankle rotation ∆θ relative to home position\nincreases as the magnitude of force increases as seen in Fig.\n8 and 9.\nFig. 6(f) and 7(f) show the torque responses of all sagittal\njoints, where ankle joint in particular fully exploits the\nmaximum achievable ankle torque for balance recovery.\nThe control system responses to the disturbance by quickly\ngenerating ankle torque as large as possible, ﬁrstly larger\nthan the gravitational torque for a short period to accelerate\nfoot for tilting around the toe/heel, and then sustaining the\nmaximum achievable torque for staying at the toe/heel with\na total underactuation time about 0.8s. From the thickness,\nlength and tilting angle of the foot, plus the mass of the body,\nit can be calculated that the magnitude of the maximum\nachievable torque while tilting around the toe and heel is\nrespectively 216.14N·m and 106.86N·m. Simulation results\nfrom Fig. 6(f) and 7(f) shows that the magnitude of ankle\ntorque applied during underactuation is around 210.41N·m\nand 110.24N·m for forward and backward push, which is\nclose to the theoretical maximum achievable torque. The\nfrontal section of the foot is longer than the rear section,\nthus the maximum achievable torque during forward push is\nlarger than that of the backward push.\nVI. CONCLUSION\nPrevious studies have already demonstrated that human-\ncomparable balancing behaviours such as foot tilting be-\nhaviours can be achieved using deterministic and analytical\nengineering approaches. Our study in this paper concerns\nabout whether it is possible to produce similar or better hu-\nmanoid balance strategies that involves stable underactuated\nankle push-off behaviour comparable to humans using deep\nreinforcement learning approach.\nOur results demonstrated the feasibility and realizability of\nusing deep reinforcement learning to learn a human-like bal-\nancing behavior with limited amount of prior structure being\nimposed on the control policy. We transfer knowledge from\ncontrol engineering based methods and applied them into the\ndesign of rewards for RL. The importance of a physic based\nreward design shall be acknowledged. Otherwise it is difﬁcult\nto balance the inﬂuence among different physical quantities\nand the balance behavior is difﬁcult to be guaranteed by RL.\nThe ankle push-off behaviour learned by RL is able to work\nrobustly under circumstances where impulses are as much\nas the theoretical maximum that can be rejected. Moreover,\ndeep RL has learned an adaptive way of actively changing\nankle push-off angle in response to the applied disturbance.\nThe scope of this paper currently only covers standing\nbalance in the sagittal plane in a 2D simulation as a proof-\nof-concept using learning approach. For future work, we plan\nto perform simulation in a 3D environment, and eventually,\nto apply learning based control to the real Valkyrie robot.\nREFERENCES\n[1] P. G. Adamczyk, S. H. Collins, and A. D. Kuo, “The advantages of a\nrolling foot in human walking,” Journal of Experimental Biology, vol.\n209, no. 20, pp. 3953–3963, 2006.\n[2] Z. Li, C. Zhou, Q. Zhu, R. Xiong, N. Tsagarakis, and D. Caldwell,\n“Active control of under-actuated foot tilting for humanoid push\nrecovery,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots and Syst., 2015,\npp. 977–982.\n[3] Z. Li, C. Zhou, Q. Zhu, and R. Xiong, “Humanoid balancing be-\nhavior featured by underactuated foot motion,” IEEE Transactions on\nRobotics, vol. 33, no. 2, pp. 298–312, 2017.\n[4] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez,\nZ. Wang, A. Eslami, M. Riedmiller, et al., “Emergence of locomotion\nbehaviours in rich environments,” preprint arXiv:1707.02286, 2017.\n[5] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in Proc. Int. Conf. Machine Learning,\n2015, pp. 1889–1897.\n[6] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous deep q-\nlearning with model-based acceleration,” in Proc. Int. Conf. Machine\nLearning, 2016, pp. 2829–2838.\n[7] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in Proc. Int. Conf. Machine Learning, 2016,\npp. 1928–1937.\n[8] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforce-\nment learning,” preprint arXiv:1509.02971, 2015.\n[9] H. Van Hasselt and M. A. Wiering, “Reinforcement learning in\ncontinuous action spaces,” in IEEE Int. Symp. Approximate Dynamic\nProgramming and Reinforcement Learning, 2007, pp. 272–279.\n[10] H. Van Hasselt, “Reinforcement learning in continuous state and action\nspaces,” in Reinforcement Learning, 2012, pp. 207–251.\n[11] X. B. Peng, G. Berseth, and M. Van de Panne, “Dynamic terrain\ntraversal skills using reinforcement learning,” ACM Transactions on\nGraphics, vol. 34, no. 4, p. 80, 2015.\n[12] X. B. Peng, G. Berseth, K. Yin, and M. van de Panne, “Deeploco:\nDynamic locomotion skills using hierarchical deep reinforcement\nlearning,” ACM Transactions on Graphics, vol. 36, no. 4, 2017.\n[13] X. B. Peng, G. Berseth, and M. Van de Panne, “Terrain-adaptive loco-\nmotion skills using deep reinforcement learning,” ACM Transactions\non Graphics, vol. 35, no. 4, p. 81, 2016.\n[14] S.-H. Hyon, R. Osu, and Y. Otaka, “Integration of multi-level postural\nbalancing on humanoid robots,” in Proc. IEEE Int. Conf. Robot.\nAutom., 2009. ICRA’09., 2009, pp. 1549–1556.\n[15] B. J. Stephens and C. G. Atkeson, “Dynamic balance force control\nfor compliant humanoid robots,” in Proc. IEEE/RSJ Int. Conf. Intell.\nRobots and Syst., 2010, pp. 1248–1255.\n[16] Z. Li, B. Vanderborght, N. G. Tsagarakis, L. Colasanto, and D. G.\nCaldwell, “Stabilization for the compliant humanoid robot COMAN\nexploiting intrinsic and controlled compliance,” in Proc. IEEE Int.\nConf. Robot. Autom., 2012, pp. 2000–2006.\n[17] J. Pratt, J. Carff, S. Drakunov, and A. Goswami, “Capture point: A\nstep toward humanoid push recovery,” in Proc. IEEE-RAS Int. Conf.\nHumanoid Robots, 2006, pp. 200–207.\n[18] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and\nD. Man´e, “Concrete problems in AI safety,” arXiv:1606.06565, 2016.\n[19] A. Y. Ng, S. J. Russell, et al., “Algorithms for inverse reinforcement\nlearning.” in Proc. Int. Conf. Machine learning, 2000, pp. 663–670.\n[20] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse rein-\nforcement learning,” in Proc. Int. Conf. Machine learning, 2004, p. 1.\n[21] P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and\nD. Amodei, “Deep reinforcement learning from human preferences,”\npreprint arXiv:1706.03741, 2017.\n[22] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried-\nmiller, “Deterministic policy gradient algorithms,” in Proc. Int. Conf.\nMachine Learning, 2014, pp. 387–395.\n[23] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[24] M. Hausknecht and P. Stone, “Deep reinforcement learning in param-\neterized action space,” preprint arXiv:1511.04143, 2015.\n[25] N. Heess, G. Wayne, Y. Tassa, T. Lillicrap, M. Riedmiller, and\nD. Silver, “Learning and transfer of modulated locomotor controllers,”\npreprint arXiv:1610.05182, 2016.\n[26] X. B. Peng and M. van de Panne, “Learning locomotion skills using\ndeeprl: does the choice of action space matter?” in Proc. the ACM\nSIGGRAPH/Eurographics Symp. on Computer Animation, 2017, p. 12.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2018-09-06",
  "updated": "2018-09-06"
}