{
  "id": "http://arxiv.org/abs/1809.02874v1",
  "title": "Unsupervised Person Re-identification by Deep Learning Tracklet Association",
  "authors": [
    "Minxian Li",
    "Xiatian Zhu",
    "Shaogang Gong"
  ],
  "abstract": "Mostexistingpersonre-identification(re-id)methods relyon supervised model\nlearning on per-camera-pair manually labelled pairwise training data. This\nleads to poor scalability in practical re-id deployment due to the lack of\nexhaustive identity labelling of image positive and negative pairs for every\ncamera pair. In this work, we address this problem by proposing an unsupervised\nre-id deep learning approach capable of incrementally discovering and\nexploiting the underlying re-id discriminative information from automatically\ngenerated person tracklet data from videos in an end-to-end model optimisation.\nWe formulate a Tracklet Association Unsupervised Deep Learning (TAUDL)\nframework characterised by jointly learning per-camera (within-camera) tracklet\nassociation (labelling) and cross-camera tracklet correlation by maximising the\ndiscovery of most likely tracklet relationships across camera views. Extensive\nexperiments demonstrate the superiority of the proposed TAUDL model over the\nstate-of-the-art unsupervised and domain adaptation re- id methods using six\nperson re-id benchmarking datasets.",
  "text": "Unsupervised Person Re-identiï¬cation by\nDeep Learning Tracklet Association\nMinxian Li1,2, Xiatian Zhu3, and Shaogang Gong2\n1 Nanjing University of Science and Technology\nminxianli@njust.edu.cn\n2 Queen Mary University of London\ns.gong@qmul.ac.uk\n3 Vision Semantics Limited\neddy@visionsemantics.com\nAbstract. Most existing person re-identiï¬cation (re-id) methods rely on\nsupervised model learning on per-camera-pair manually labelled pairwise\ntraining data. This leads to poor scalability in practical re-id deployment\ndue to the lack of exhaustive identity labelling of image positive and neg-\native pairs for every camera pair. In this work, we address this problem\nby proposing an unsupervised re-id deep learning approach capable of\nincrementally discovering and exploiting the underlying re-id discrimina-\ntive information from automatically generated person tracklet data from\nvideos in an end-to-end model optimisation. We formulate a Tracklet\nAssociation Unsupervised Deep Learning (TAUDL) framework charac-\nterised by jointly learning per-camera (within-camera) tracklet associa-\ntion (labelling) and cross-camera tracklet correlation by maximising the\ndiscovery of most likely tracklet relationships across camera views. Ex-\ntensive experiments demonstrate the superiority of the proposed TAUDL\nmodel over the state-of-the-art unsupervised and domain adaptation re-\nid methods using six person re-id benchmarking datasets.\nKeywords: Person Re-Identiï¬cation; Unsupervised Learning; Tracklet;\nSurveillance Video.\n1\nIntroduction\nPerson re-identiï¬cation (re-id) aims to match the underlying identities of person\nbounding box images detected from non-overlapping camera views [15]. In recent\nyears, extensive research attention has been attracted [1, 7, 10, 11, 14, 18, 29â€“\n31, 44, 46, 53, 58] to address the re-id problem. Most existing re-id methods, in\nparticular deep learning models, adopt the supervised learning approach. These\nsupervised deep models assume the availability of a large number of manually\nlabelled cross-view identity (ID) matching image pairs for each camera pair in\norder to induce a feature representation or a distance metric function optimised\njust for that camera pair. This assumption is inherently limited for generalising a\narXiv:1809.02874v1  [cs.CV]  8 Sep 2018\n2\nM. Li, X. Zhu, and S. Gong\nre-id model to many diï¬€erent camera networks therefore cannot scale in practical\ndeployments1.\nIt is no surprise then that person re-id by unsupervised learning has become\na focus in recent research where per-camera pairwise ID labelled training data\nis not required in model learning [22, 24, 25, 32, 35, 37, 47, 49, 55, 59]. However,\nall these classical unsupervised learning models are signiï¬cantly weaker in re-id\nperformance than the supervised models. This is because the lack of cross-view\npairwise ID labelled data deprives a modelâ€™s ability to learn from strong context-\naware ID discriminative information in order to cope with signiï¬cant visual ap-\npearance change between every camera pair, as deï¬ned by a triplet veriï¬cation\nloss function. An alternative approach is to leverage jointly (1) unlabelled data\nfrom a target domain which is freely available, e.g. videos of thousands of people\ntravelling through a camera view everyday in a public scene; and (2) pairwise ID\nlabelled datasets from independent source domains [13,38,43,50,56]. The main\nidea is to ï¬rst learn a â€œview-invariantâ€ representation from ID labelled source\ndata, then adapt the model to a target domain by using only unlabelled target\ndata. This approach makes an implicit assumption that the source and target\ndomains share some common cross-view characteristics and a view-invariant rep-\nresentation can be estimated, which is not always true.\nIn this work, we consider a pure unsupervised person re-id deep learning\nproblem. That is, no ID labelled training data is assumed, neither cross-view\nnor within-view ID labelling. Although this learning objective is similar to two\ndomain transfer models [13,50], both those models do require suitable, i.e. visu-\nally similar to the target domain, person identity labelled source domain training\ndata. Speciï¬cally, we consider unsupervised re-id model learning by jointly opti-\nmising unlabelled person tracklet data within-camera view to be more discrimi-\nnative and cross-camera view to be more associative in an end-to-end manner.\nOur contributions are: We formulate a novel unsupervised person re-id deep\nlearning method using person tracklets without the need for camera pairwise\nID labelled training data, i.e. unsupervised tracklet re-id discriminative learn-\ning. Speciï¬cally, we propose a Tracklet Association Unsupervised Deep\nLearning (TAUDL) model with two key innovations: (1) Per-Camera Track-\nlet Discrimination Learning that optimises â€œlocalâ€ within-camera tracklet label\ndiscrimination for facilitating cross-camera tracklet association given per-camera\nindependently created tracklet label spaces. (2) Cross-Camera Tracklet Associ-\nation Learning that maximises â€œglobalâ€ cross-camera tracklet label association.\nThis is formulated as to maximise jointly cross-camera tracklet similarity and\nwithin-camera tracklet dissimilarity in an end-to-end deep learning framework.\nComparative experiments show the advantages of TAUDL over the state-\nof-the-art unsupervised and domain adaptation person re-id models using six\nbenchmarks including three multi-shot image based and three video based re-\nid datasets: CUHK03 [29], Market-1501 [61], DukeMTMC [41], iLIDS-VID [51],\nPRID2011 [19], and MARS [60].\n1 Exhaustive manual ID labelling of person image pairs for every camera-pair is pro-\nhibitively expensive as there are a quadratic number of camera pairs in a network.\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n3\n2\nRelated Work\nMost existing re-id models are built by supervised model learning on a separate\nset of per-camera-pair ID labelled training data [1,7â€“11,18,20,29â€“31,44,46,48,\n52,53,58,63]. Hence, their scalability and usability is poor for real-world re-id de-\nployments where no such large training sets are available for every camera pair.\nClassical unsupervised learning methods based on hand-crafted features oï¬€er\npoor re-id performance [14,22,24,25,32,35,37,47,49,55,59] when compared to\nthe supervised learning based re-id models. While a balancing trade-oï¬€between\nmodel scalability and re-id accuracy can be achieved by semi-supervised learn-\ning [33,49], these models still assume suï¬ƒciently large sized cross-view pairwise\nlabelled data for model training. More recently, there are some attempts on unsu-\npervised learning of domain adaptation models [13,38,43,50,56]. The main idea\nis to explore knowledge from pairwise labelled data in â€œrelatedâ€ source domains\nwith model adaptation on unlabelled target domain data. Whilst these domain\nadaptation models perform better than the classical unsupervised learning meth-\nods (Table 2 and Table 3), they requires implicitly similar data distributions and\nviewing conditions between the labelled source domain and the unlabelled tar-\nget domains. This restricts their scalability to arbitrarily diverse (and unknown)\ntarget domains.\nIn contrast to all these existing unsupervised learning re-id methods, the\nproposed tracklet association based method enables unsupervised re-id deep\nend-to-end learning from scratch without any assumption on either the scene\ncharacteristic similarity between source and target domains, or the complexity\nof handling identity label space (or lack of) knowledge transfer in model optimi-\nsation. Instead, our method directly learns to discover the re-id discriminative\nknowledge from unsupervised tracklet label data automatically generated and\nannotated from the video data using a common deep learning network architec-\nture. Moreover, this method does not assume any overlap of person ID classes\nacross camera views, therefore scalable to any camera networks without any\nknowledge about camera space-time topology and/or time-proï¬ling on people\ncross-view appearing patterns [36]. Compared to classical unsupervised methods\nrelying on extra hand-crafted features, our method learns tracklet based re-id\ndiscriminative features from an end-to-end deep learning process. To our best\nknowledge, this is the ï¬rst attempt at unsupervised tracklet association based\nperson re-id deep learning model without relying on any ID labelled training\ndata (either videos or images).\n3\nUnsupervised Deep Learning Tracklet Association\nTo overcome the limitation of supervised re-id model training, we propose a novel\nTracklet Association Unsupervised Deep Learning (TAUDL) approach to\nperson re-id in video (or multi-shot images in general) by uniquely exploiting per-\nson tracklet labelling obtained by an unsupervised tracklet formation (sampling)\n4\nM. Li, X. Zhu, and S. Gong\nCamera 1\nâ€¦\nâ€¦\nSSTT\nğ‘º1\nğ’š1\nğ‘º2\nğ’š2\nSparse Space-Time \nTracklet sampling\nand annotating\nSSTT\nğ‘ºğ‘¡\nTracklet images in camera ğ‘¡\nğ’šğ‘¡\nTracklet labels in camera ğ‘¡\nğ‘³ğ‘ªğ‘¬\nğ’•\nCross-Entropy loss in camera ğ‘¡\nğ‘³ğ‘ªğ‘ªğ‘»ğ‘¨Cross-Camera Tracklet Association loss\nğ‘ 1\n2\nğ‘ 1\n1\nğ‘ 5\n2\nğ‘ ğ‘›1\nğ‘ ğ‘š\n2\nâ€¦\nâ€¦\nTracklet feature space in camera 2\nTracklet feature space in camera 1\nğ‘ 3\n2\nğ‘³ğ‘ªğ‘ªğ‘»ğ‘¨\nğ‘³ğ‘ªğ‘¬\nğŸ\nğ‘³ğ‘ªğ‘¬\nğŸ\n(a)\n(b)\nCNN\nShared\nfeature\nlayer\nCamera 2\nFig. 1. An overview of Tracklet Association Unsupervised Deep Learning (TAUDL)\nre-id model: (a) Per-camera unsupervised tracklet sampling and label assignment; (b)\nJoint learning of both within-camera tracklet discrimination and cross-camera tracklet\nassociation in an end-to-end global deep learning on tracklets from all the cameras.\nmechanism2 without any ID labelling of the training data (either cross-view or\nwithin-view). The TAUDL trains a person re-id model in an end-to-end manner\nin order to beneï¬t from the inherent overall model optimisation advantages from\ndeep learning. In the following, we ï¬rst present a data sampling mechanism for\nunsupervised within-camera tracklet labelling (Sec. 3.1) and then describe our\nmodel design for cross-camera tracklet association by joint unsupervised deep\nlearning (Sec. 3.2).\n3.1\nUnsupervised Within-View Tracklet Labelling\nGiven a large quantity of video data from multiple disjoint cameras, we can\nreadily deploy existing pedestrian detection and tracking models [26,42,57,62],\nto extract person tracklets. In general, the space-time trajectory of a person in\na single-camera view from a public scene is likely to be fragmented into an arbi-\ntrary number of short tracklets due to imperfect tracking and background clutter.\nGiven a large number of person tracklets per camera, we want to annotate them\nfor deep re-id model learning in an unsupervised manner without any manual\nidentity veriï¬cation on tracklets. To this end, we need an automatic tracklet\nlabelling method to minimise the person ID duplication (i.e. multiple tracklet\n2 Although object tracklets can be generated by any independent single-camera-view\nmulti-object tracking (MOT) models widely available today, a conventional MOT\nmodel is not end-to-end optimised for cross-camera tracklet association.\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n5\nğ‘·\nğ‘¸\nTime\nğ‘ºğ’Š\nğ‘ºğ’Š+ğŸ\nTracklet #1\nTracklet #2\nTracklet #3\nTracklet #4\nTracklet #5\nTracklet #6\n(a) Temporal sampling\nTracklet #1\nTracklet #2\nTracklet #3\n(b) Spatial sampling\nFig. 2. An illustration of the Sparse Space-Time Tracklet sampling and annotating\nmethod for unsupervised tracklet labelling. Solid box: Sampled tracklets; Dashed box:\nNon-sampled tracklets; Each colour represents a distinct person ID. (a) Two time\ninstances (Si and Si+1 indicated by vertical lines) of temporal sampling are shown\nwith a time gap P greater than the common transit time Q of a camera view. (b)\nThree spatially sparse tracklets are formed at a given temporal sampling instance.\nlabels corresponding the same person ID label) rate among these labelled track-\nlets. To this end, we propose a Sparse Space-Time Tracklet (SSTT) sampling\nand label assignment method.\nOur SSTT method is built on three observations typical in surveillance\nvideos: (1) For most people, re-appearing in a camera view is rare during a short\ntime period. As such, the dominant factor for causing person tracklet duplication\n(of the same ID) in auto-generated person tracklets is trajectory fragmentation,\nand if we assign every tracklet with a distinct label. To address this problem, we\nperform sparse temporal sampling of tracklets (Fig. 2(a)) as follows: (i) At the\ni-th temporal sampling instance corresponding to a time point Si, we retrieve\nall tracklets at time Si and annotate each tracklet with a distinct label. This is\nbased on the factor that (2) people co-occurring at the same time in a single-\nview but at diï¬€erent spatial locations should have distinct ID labels. (ii) Given\na time gap P, the next ((i + 1)-th) temporal sampling and label assignment is\nrepeated, where P controls the sparsity of the temporal sampling rate. Based\non observation (3) that most people in a public scene travel through a single\ncamera view in a common time period Q < P, it is expected that at most one\ntracklet per person can be sampled at such a sparse temporal sampling rate\n(assuming no re-appearing once out of the same camera view). Consequently, we\ncan signiï¬cantly reduce the ID duplication even in highly crowded scenes with\ngreater degrees of trajectory fragmentation.\nTo further mitigate the negative eï¬€ect of inaccurate person detection and\ntracking at each temporal sampling instance, we further impose a sparse spa-\ntial sampling constraint â€“ only selecting the co-occurring tracklets distantly dis-\ntributed over the scene space (Fig. 2(b)). In doing so, the tracklet labels are more\nlikely to be of independent person identities with minimum ID duplications in\neach i-th temporal sampling instance.\n6\nM. Li, X. Zhu, and S. Gong\nBy deploying this SSTT tracklet labelling method in each camera view, we\ncan obtain an independent set of labelled tracklets {Si, yi} per-camera in a cam-\nera network, where each tracklet contains a varying number of person bounding\nboxes as S = {I1, I2, Â· Â· Â· }. Our objective is to use these SSTT labelled track-\nlets for optimising a cross-view person re-id deep learning model without any\ncross-view ID labelled pairwise training data.\n3.2\nUnsupervised Tracklet Association\nGiven per-camera independently-labelled tracklets {Si, yi} generated by SSTT,\nwe perform tracklet label re-id discriminative learning without person ID labels\nin a conventional classiï¬cation deep learning framework. To that end, we for-\nmulate a Tracklet Association Unsupervised Deep Learning (TAUDL)\nmodel. The overall design of our TAUDL architecture is shown in Fig. 1. The\nTAUDL contains two model components: (I) Per-Camera Tracklet Discrimina-\ntion Learning with the aim to optimise â€œlocalâ€ (within-camera) tracklet label\ndiscrimination for facilitating cross-camera tracklet association given indepen-\ndently created tracklet label spaces in diï¬€erent camera views. (II) Cross-Camera\nTracklet Association Learning with the aim to maximise â€œglobalâ€ (cross-camera)\ntracklet label association. The two components integrate as a whole in a single\ndeep learning network architecture, learn jointly and mutually beneï¬t each other\nin an incremental end-to-end manner.\nâ€¦\nâ€¦\nâ€¦\nğ‘°ğ‘«ğŸğ‘°ğ‘«ğŸğ‘°ğ‘«ğŸ‘ğ‘°ğ‘«ğŸ’ğ‘°ğ‘«ğŸ“ğ‘°ğ‘«ğŸ”ğ‘°ğ‘«ğŸ•ğ‘°ğ‘«ğŸ–\n(a)\nCamera 1\nCamera 2\nâ€¦\nâ€¦\nUnderlying \ntracklet\nassociation\n(b)\nCamera 1\nCamera 2\nFig. 3. Comparing (a) Fine-grained explicit instance-level cross-view ID labelled im-\nage pairs for supervised person re-id model learning and (b) Coarse-grained latent\ngroup-level cross-view tracklet (a multi-shot group) label correlation for ID label-free\n(unsupervised) person re-id learning using TAUDL.\n(I) Per-Camera Tracklet Discrimination Learning\nFor accurate cross-\ncamera tracklet association, it is important to formulate a robust image fea-\nture representation for describing the person appearance of each tracklet that\nhelps cross-view person re-id association. However, it is sub-optimal to achieve\nâ€œlocalâ€ per-camera tracklet discriminative learning using only per-camera inde-\npendent tracklet labels without â€œglobalâ€ cross-camera tracklet correlations. We\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n7\nwish to optimise jointly both local tracklet within-view discrimination and global\ntracklet cross-view association. To that end, we design a Per-Camera Tracklet\nDiscrimination (PCTD) learning algorithm. Our key idea is that, instead of re-\nlying on the conventional ï¬ne-grained explicit instance-level cross-view ID pair-\nwise supervised learning (Fig. 3(a)), we learn to maximise coarse-grained latent\ngroup-level cross-camera tracklet association by set correlation (Fig. 3(b)).\nSpeciï¬cally, we treat each individual camera view separately by optimising\nper-camera labelled tracklet discrimination as a classiï¬cation task against the\ntracklet labels per-camera (not person ID labels). Therefore, we have a total of\nT diï¬€erent tracklet classiï¬cation tasks each corresponding to a speciï¬c camera\nview. Importantly, we further formulate these T classiï¬cation tasks in a multi-\nbranch architecture design where every task shares the same feature representa-\ntion whilst enjoys an individual classiï¬cation branch (Fig. 1(b)). Conceptually,\nthis model design is in a spirit of the multi-task learning principle [2,12].\nFormally, given unsupervised training data {I, y} extracted from a camera\nview t âˆˆ{1, Â· Â· Â· , T}, where I speciï¬es a tracklet frame and y âˆˆ{1, Â· Â· Â· , Mt} the\ntracklet label (obtained as in Sec. 3.1) with a total of Mt diï¬€erent labels, we adopt\nthe softmax Cross-Entropy (CE) loss function to optimise the corresponding\nclassiï¬cation task (the t-th branch). The CE loss on a training image sample\n(I, y) is computed as:\nLce = âˆ’log\n\u0010\nexp(W âŠ¤\ny x)\nPMt\nk=1 exp(W âŠ¤\nk x)\n\u0011\n,\n(1)\nwhere x speciï¬es the feature vector of I extracted by the task-shared feature\nrepresentation component and Wy the y-th class prediction function parameters.\nGiven a mini-batch, we compute the CE loss for each such training sample w.r.t.\nthe respective tracklet label space and utilise their average to form the model\nlearning supervision as:\nLpctd =\n1\nNbs\nT\nX\nt=1\nLt\nce,\n(2)\nwhere Lt\nce denotes the CE loss summation of training samples from the t-th\ncamera among a total of T and Nbs the batch size.\nDiscussion: In PCTD, the deep learning objective loss function (Eqn. (1))\naims to optimise by supervised learning person tracklet discrimination within\neach camera view without any knowledge on cross-camera tracklet association.\nHowever, when jointly learning all the per-camera tracklet discrimination tasks\ntogether, the learned representation model is somewhat implicitly and collec-\ntively cross-view tracklet discriminative in a latent manner, due to the existence\nof cross-camera tracklet correlation. In other words, the shared feature represen-\ntation is optimised concurrently to be discriminative for tracklet discrimination\nin multiple camera views, therefore propagating model discriminative learning\nfrom per-camera to cross-camera. We will evaluate the eï¬€ect of this model design\nin our experiments (Table 4).\n8\nM. Li, X. Zhu, and S. Gong\n(II) Cross-Camera Tracklet Association Learning While the PCTD algo-\nrithm described above achieves somewhat global (all the camera views) tracklet\ndiscrimination implicitly, the learned model representation remains sub-optimal\ndue to the lack of explicitly optimising cross-camera tracklet association at the\nï¬ne-grained instance level. It is signiï¬cantly harder to impose cross-view person\nre-id discriminative model learning without camera pairwise ID labels. To ad-\ndress this problem, we introduce a Cross-Camera Tracklet Association (CCTA)\nloss function. The CCTA loss is formulated based on the idea of batch-wise in-\ncrementally aligning cross-view per tracklet feature distribution in the shared\nmulti-task learning feature space. Critically, CCTA integrates seamlessly with\nPCTD to jointly optimise model learning on discovering cross-camera tracklet\nassociation for person re-id in a single end-to-end batch-wise learning process.\nFormally, given a mini-batch including a subset of tracklets {(St\ni, yt\ni)} where\nSt\ni speciï¬es the i-th tracklet from t-th camera view with the label yt\ni where\ntracklets in a mini-batch come from all the camera views, we want to estab-\nlish for each in-batch tracklet a discriminative association with other tracklets\nfrom diï¬€erent camera views. In absence of person identity pairwise labelling as a\nlearning constraint, we propose to align similar and dissimilar tracklets in each\nmini-batch given the up-to-date shared multi-task (multi-camera) feature rep-\nresentation from optimising PCTD. More speciï¬cally, for each tracklet St\ni, we\nï¬rst retrieve K cross-view nearest tracklets N t\ni in the feature space, with the\nremaining Ëœ\nN t\ni considered as dissimilar ones. We then impose a soft discrimina-\ntive structure constraint by encouraging the model to pull N t\ni close to St\ni whilst\nto push away Ëœ\nN t\ni from St\ni. Conceptually, this is a per-tracklet cross-view data\nstructure distribution alignment. To achieve this, we formulate a CCTA deep\nlearning objective loss for each tracklet St\ni in a training mini-batch as:\nLccta = âˆ’log\nP\nzkâˆˆN t\ni exp(âˆ’1\n2Ïƒ2 âˆ¥st\ni âˆ’zk âˆ¥2)\nPT\ntâ€²=1\nPnj\nj=1 exp(âˆ’1\n2Ïƒ2 âˆ¥st\ni âˆ’stâ€²\nj âˆ¥2)\n,\n(3)\nwhere nj denotes the number of in-batch tracklets from j-th camera view, T the\ncamera view number, Ïƒ a scaling parameter, st\ni the up-to-date feature represen-\ntation of the tracklet St\ni. Given the incremental iterative deep learning nature,\nwe represent a tracklet S by the average of its in-batch framesâ€™ feature vectors\non-the-ï¬‚y. Hence, the tracklet representation is kept up-to-date without the need\nfor maintaining external per-tracklet feature representations.\nDiscussion: The proposed CCTA loss formulation is conceptually similar\nto the Histogram Loss [45] in terms of distribution alignment. However, the\nHistogram Loss is a supervised loss that requires supervised label training data,\nwhilst the CCTA is purely unsupervised and derived directly from feature similar-\nity measures. CCTA is also related to the surrogate (artiï¬cially built) class based\nunsupervised deep learning loss formulations [4,5], by not requiring groundtruth\nclass-labelled data in model training. Unlike CCTA without the need for creating\nsurrogate classes, the surrogate based models not only require additional global\ndata clustering, but also are sensitive to the clustering quality and initial feature\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n9\n(a)\n(b)\n(c)\n(a)\n(d)\n(e)\n(a)\n(d)\n(f)\nFig. 4. Example cross-view matched image/tracklet pairs from (a) CUHK03, (b)\nMarket-1501, (c) DukeMTMC, (d) PRID2011, (e) iLIDS-VID, (f) MARS.\nselection. Moreover, they do not consider the label distribution alignment across\ncameras and label spaces for which the CCTA loss is designed.\nJoint Loss Function After merging the CCTA and PCTD learning constraints,\nwe obtain the ï¬nal model objective function as:\nLtaudl = (1 âˆ’Î»)Lpctd + Î»Lccta,\n(4)\nwhere Î» is a weighting parameter estimated by cross-validation. Note that Lpctd\nis an average loss term at the tracklet individual image level whilst Lccta at the\ntracklet group (set) level, both derived from the same training batch concur-\nrently. As such, the overall TAUDL method naturally enables end-to-end deep\nmodel learning using the Stochastic Gradient Descent optimisation algorithm.\n4\nExperiments\nDatasets To evaluate the proposed TAUDL model, we tested both video (MARS\n[60], iLIDS-Video [51], PRID2011 [19]) and image (CUHK03 [29], Market-1501\n[61], DukeMTMC [41,62]) based person re-id benchmarking datasets. In previ-\nous studies, these datasets were mostly evaluated separately. We consider since\nrecent large sized image based re-id datasets were typically constructed by sam-\npling person bounding boxes from video, these image datasets share similar\ncharacteristics of those video based datasets. We adopted the standard person\nre-id setting on training/test ID split and the test protocols (Table 1).\nTracklet Label Assignment\nFor all six datasets, we cannot perform real\nSSTT tracklet sampling and label assignment due to no information available\non spatial and temporal location w.r.t. the original video data. In our experiment,\nwe instead conducted simulated SSTT to obtain the per-camera tracklet/image\nlabels. For all datasets, we assume no re-appearing subjects per camera (very\nrare in these datasets) and sparse spatial sampling. As both iLIDS-VID and\nPRID2011 provide only one tracklet per ID per camera (i.e. no fragmentation),\nit is impossible to have per-camera ID duplication. Therefore, each tracklet is\nassigned a unique label. The MARS gives multiple tracklets per ID per camera.\n10\nM. Li, X. Zhu, and S. Gong\nTable 1. Dataset statistics and evaluation setting.\nDataset\n# ID\n# Train\n# Test\n# Images\n# Tracklet\niLIDS-VID [51]\n300\n150\n150\n43,800\n600\nPRID2011 [19]\n178\n89\n89\n38,466\n354\nMARS [60]\n1,261\n625\n636\n1,191,003\n20,478\nCUHK03 [29]\n1,467\n767\n700\n14,097\n0\nMarket-1501 [61]\n1,501\n751\n750\n32,668\n0\nDukeMTMC [41]\n1,812\n702\n1,110\n36,411\n0\nBased on SSTT, at most only one tracklet can be sampled for each ID per camera\n(see Sec. 3.1). Therefore, a MARS tracklet per ID per camera was randomly\nselected and assigned a label. For all image based datasets, we assume all images\nper ID per camera were drawn from a single tracklet, same as in iLIDS-VID and\nPRID2011. The same tracklet label assignment procedure was adopted as above.\nPerformance Metrics We use the common cumulative matching characteristic\n(CMC) and mean Average Precision (mAP) metrics [61].\nImplementation Details We adopted an ImageNet pre-trained ResNet-50 [17]\nas the backbone in evaluating the proposed TAUDL method. We set the feature\ndimension of the camera-shared representation space derived on top of ResNet-\n50 to 2,048. Each camera-speciï¬c branch contains one FC classiï¬cation layer.\nPerson images are resized to 256Ã—128 for all datasets. To ensure that each batch\nhas the capacity of containing person images from all cameras, we set the batch\nsize to 384 for all datasets. For balancing the model learning speed over diï¬€erent\ncameras, we randomly selected the same number of training frame images per\ncamera when sampling each mini-batch. We adopted the Adam optimiser [23]\nwith the initial learning rate of 3.5Ã—10âˆ’4. We empirically set Î»=0.7 for Eq. (4),\nÏƒ = 2 for Eq. (3), and K = T/2 (T is the number of cameras) for cross-view\nnearest tracklets N t\ni in Eq. (3) for all the experiments.\n4.1\nComparisons to State-Of-The-Arts\nWe compared two diï¬€erent sets of state-of-the-art methods on image and video\nre-id datasets, due to the independent studies on them in the literature.\nUnsupervised Person Re-ID on Image Datasets\nTable 2 shows the un-\nsupervised re-id performance of the proposed TAUDL and 10 state-of-the-art\nmethods including 3 hand-crafted feature based methods (Dic [25], ISR [32],\nRKSL [49]) and 7 auxiliary knowledge (identity/attribute) transfer based mod-\nels (AE [27], AML [54], UsNCA [40], CAMEL [56], JSTL [53], PUL [13], TJ-\nAIDL [50]). These results show: (1) Among existing methods, the knowledge\ntransfer based method is superior, e.g. on CUHK03, Rank-1 39.4% by CAMEL\nvs. 36.5% by Dic; On Market-1501, 58.2% by TJ-AIDL vs. 50.2% by Dic. To that\nend, CAMEL beneï¬ts from learning on 7 diï¬€erent person re-id datasets of di-\nverse domains (CUHK03 [29], CUHK01 [28], PRID [19], VIPeR [16], 3DPeS [3],\ni-LIDS [39], Shinpuhkan [21]) including a total of 44,685 images and 3,791 iden-\ntities; TJ-AIDL utilises labelled Market-1501 (750 IDs and 27 attribute classes)\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n11\nTable 2. Unsupervised re-id on image datasets. 1st/2nd best results are in red/blue.\nDataset\nCUHK03 [29]\nMarket-1501 [61]\nDukeMTMC [62]\nMetric(%)\nRank-1\nmAP\nRank-1\nmAP\nRank-1\nmAP\nDic [25]\n36.5\n-\n50.2\n22.7\n-\n-\nISR [32]\n38.5\n-\n40.3\n14.3\n-\n-\nRKSL [49]\n34.8\n-\n34.0\n11.0\n-\n-\nSAE [27]\n30.5\n-\n42.4\n16.2\n-\n-\nJSTL [53]\n33.2\n-\n44.7\n18.4\n-\n-\nAML [54]\n31.4\n-\n44.7\n18.4\n-\n-\nUsNCA [40]\n29.6\n-\n45.2\n18.9\n-\n-\nCAMEL [56]\n39.4\n-\n54.5\n26.3\n-\n-\nPUL [13]\n-\n-\n44.7\n20.1\n30.4\n16.4\nTJ-AIDL [50]\n-\n-\n58.2\n26.5\n44.3\n23.0\nTAUDL\n44.7\n31.2\n63.7\n41.2\n61.7\n43.5\nGCS [6](Supervised)\n88.8\n97.2\n93.5\n81.6\n84.9\n69.5\nor DukeMTMC (702 IDs and 23 attribute classes) as source training data. (2)\nOur new model TAUDL outperforms all competitors with signiï¬cant margins.\nFor example, the Rank-1 margin by TAUDL over TJ-AIDL is 5.5% (63.7-58.2)\non Market-1501 and 17.4% (61.7-44.3) on DukeMTMC. Moreover, it is worth\npointing out that TAUDL dose not beneï¬t from any additional labelled source\ndomain training data as compared to TJ-AIDL. TAUDL is potentially more\nscalable due to no need to consider source and target domains similarities. (3)\nOur TAUDL is simpler to train with a simple end-to-end model learning, as\ncompared to the alternated deep CNN training and clustering required by PUL\nand a two-stage model training of TJ-AIDL. These results show both the perfor-\nmance advantage and model design superiority of the proposed TAUDL model\nover a wide variety of state-of-the-art re-id models.\nTable 3. Unsupervised re-id on video datasets. 1st/2nd best results are in red/blue.\nDataset\nPRID2011 [19]\niLIDS-VID [51]\nMARS [60]\nMetric(%)\nR1\nR5\nR20\nR1\nR5\nR20\nR1\nR5\nR20 mAP\nDTW [37]\n41.7 67.1\n90.1\n31.5 62.1 82.4\n-\n-\n-\n-\nGRDL [24]\n41.6 76.4\n89.9\n25.7 49.9 77.6\n19.3 33.2 46.5\n9.56\nUnKISS [22]\n58.1 81.9\n96.0\n35.9 63.3 83.4\n22.3 37.4 53.6\n10.6\nSMP [35]\n80.9 95.6 99.4\n41.7 66.3 80.7\n23.9 35.8 44.9\n10.5\nDGM+MLAPG [55]\n73.1 92.5 99.0\n37.1 61.3 82.0\n24.6 42.6 57.2\n11.8\nDGM+IDE [55]\n56.4 81.3\n96.4\n36.2 62.8 82.7 36.8 54.0 68.5 21.3\nTAUDL\n49.4 78.7\n98.9\n26.7 51.3 82.0\n43.8 59.9 72.8 29.1\nQAN [34](Supervised)\n90.3 98.2 100.0\n68.0 86.8 97.4\n73.7 84.9 91.6\n51.7\n12\nM. Li, X. Zhu, and S. Gong\nUnsupervised Person Re-ID on Video Datasets\nWe compared the pro-\nposed TAUDL with six state-of-the-art unsupervised video person re-id models.\nUnlike TAUDL, all these existing models are not end-to-end deep learning meth-\nods with either hand-crafted or separately trained deep features as model input.\nTable 3 shows that TAUDL outperforms all existing video-based person re-id\nmodels on the large scale video dataset MARS, e.g. by a Rank-1 margin of\n7.0% (43.8-36.8) over the best competitor DGM+IDE (which additionally using\nthe ID label information of one camera view for model initialisation). However,\nTAUDL is inferior than some of the existing models on the two small benchmarks\niLIDS-VID (300 training tracklets) and PRID2011 (178 training tracklets), in\ncomparison to its performance on the MARS benchmark (8,298 training track-\nlets). This shows that TAUDL does need suï¬ƒcient tracklet data from larger\nvideo datasets in order to have its performance advantage. As the tracklet data\nrequired are not manually labelled, this requirement is not a hindrance to its\nscalability to large scale data. Quite the contrary, TAUDL works the best when\nlarge scale unlabelled video data is available. A model would beneï¬t particularly\nfrom pre-training using TAUDL on large auxiliary unlabelled video data from\nsimilar camera viewing conditions.\n4.2\nComponent Analysis and Discussions\nEï¬€ectiveness of Per-Camera Tracklet Discrimination The PCTD com-\nponent was evaluated by comparing a baseline that treats all cameras together\nby concatenating per-camera tracklet label sets and deploying the Cross-Entropy\nloss to learn a uniï¬ed classiï¬cation task. We call this baseline Joint-Camera Clas-\nsiï¬cation (JCC). In this analysis, we do not consider the cross-camera tracklet as-\nsociation component for a clear evaluation. Table 4 shows that our PCTD design\nis signiï¬cantly superior over the JCC learning algorithm, e.g. achieving Rank-1\ngain of 4.0%, 34.6%, 36.3%, and 19.9% on CUHK03, Market-1501, DukeMTMC,\nand MARS respectively. This veriï¬es the modelling advantages of the proposed\nper-camera tracklet discrimination learning scheme on the unsupervised tracklet\nlabels in inducing cross-view re-id discriminative feature learning.\nTable 4. Eï¬€ect of Per-Camera Tracklet Discrimination (PCTD) learning.\nDataset\nCUHK03 [29]\nMarket-1501 [61]\nDukeMTMC [41]\nMARS [60]\nMetric(%)\nR1\nmAP\nR1\nmAP\nR1\nmAP\nR1\nmAP\nJCC\n29.8\n12.5\n17.5\n7.9\n14.9\n3.5\n18.1\n13.1\nPCTD\n33.8\n18.9\n52.1\n26.6\n51.2\n32.9\n38.0 23.9\nEï¬€ectiveness of Cross-Camera Tracklet Association The CCTA learning\ncomponent was evaluated by testing the performance drop after eliminating it.\nTable 5 shows a signiï¬cant performance beneï¬t from this model component,\ne.g. a Rank-1 boost of 10.9%, 11.6%, 10.5%, and 5.8% on CUHK03, Market-\n1501, DukeMTMC, and MARS respectively. This validates the importance of\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n13\nmodelling the correlation across cameras in discriminative optimisation and the\neï¬€ectiveness of our CCTA deep learning objective loss formulation in an end-\nto-end manner. Additionally, this also suggests the eï¬€ectiveness of the PCTD\nmodel component in facilitating the cross-view identity discrimination learning\nby providing re-id sensitive features in a joint incremental learning manner.\nTable 5. Eï¬€ect of Cross-Camera Tracklet Association (CCTA)\nDataset\nCUHK03 [29]\nMarket-1501 [61]\nDukeMTMC [62]\nMARS [60]\nCCTA\nR1\nmAP\nR1\nmAP\nR1\nmAP\nR1\nmAP\n\u0017\n33.8\n18.9\n52.1\n26.6\n51.2\n32.9\n38.0\n23.9\n\u0013\n44.7\n31.2\n63.7\n41.2\n61.7\n43.5\n43.8 29.1\nModel Robustness Analysis Finally, we performed an analysis on model ro-\nbustness against person ID duplication rates in tracklet labelling. We conducted\na controlled evaluation on MARS where multiple tracklets per ID per camera\nare available for setting simulation. Recall that the ID duplication may mainly\ncome with imperfect temporal sampling due to trajectory fragmentation and\nwhen some people stay in the same camera view for a longer time period than\nthe temporal sampling gap. To simulate such a situation, we assume a varying\npercentage (10%âˆ¼50%) of IDs per camera have two random tracklets sampled\nand annotated with diï¬€erent tracklet labels. More tracklets per ID per camera\nare likely to be sampled, which can make this analysis more complex due to the\ninterference from the number of duplicated person IDs. Table 6 shows that our\nTAUDL model is robust against the ID duplication rate, e.g. with only a Rank-1\ndrop of 3.1% given as high as 50% per-camera ID duplication rate. In reality,\nit is not too hard to minimise ID duplication rate among tracklets (Sec. 3.1),\ne.g. conducting very sparse sampling over time and space. Note, we do not care\nabout exhaustive sampling of all the tracklets from video in a given time period.\nThe model learning beneï¬ts from very sparse and diverse tracklet sampling from\na large pool of unlabelled video data.\nThe robustness of our TAUDL comes with two model components: (1) The\nmodel learning optimisation is not only subject to a single per-camera tracklet\nlabel constraint, but also concurrently to the constraints of all cameras. This\nfacilitates optimising cross-camera tracklet association globally across all cam-\neras in a common space, due to the Per-Camera Tracklet Discrimination learning\nmechanism (Eq. (2)). This provides model learning tolerance against per-camera\ntracklet label duplication errors. (2) The cross-camera tracklet association learn-\ning is designed as a feature similarity based â€œsoftâ€ objective learning constraint\n(Eq. (3)), without a direct dependence on the tracklet ID labels. Therefore, the\nID duplication rate has little eï¬€ect on this objective loss constraint.\n14\nM. Li, X. Zhu, and S. Gong\nTable 6. Model robustness analysis on varying ID duplication rates on MARS [60].\nID Duplication Rate (%)\nRank-1\nRank-5\nRank-10\nRank-20\nmAP\n0\n43.8\n59.9\n66.0\n72.8\n29.1\n10\n42.8\n59.7\n65.5\n71.6\n28.3\n20\n42.2\n58.8\n64.7\n70.6\n27.4\n30\n41.6\n57.9\n64.5\n69.7\n26.7\n50\n40.7\n57.0\n63.4\n69.6\n25.6\n5\nConclusions\nIn this work, we presented a novel Tracklet Association Unsupervised Deep Learn-\ning (TAUDL) model for unsupervised person re-identiï¬cation using unsupervised\nperson tracklet data extracted from videos, therefore eliminating the tedious and\nexhaustive manual labelling required by all supervised learning based re-id model\nlearning. This enables TAUDL to be much more scalable to real-world re-id de-\nployment at large scale video data. In contrast to most existing re-id methods\nthat either require exhaustively pairwise labelled training data for every camera\npair or assume the availability of additional labelled source domain training data\nfor target domain adaptation, the proposed TAUDL model is capable of end-to-\nend deep learning a discriminative person re-id model from scratch on totally\nunlabelled tracklet data. This is achieved by optimising jointly both the Per-\nCamera Tracklet Discrimination loss function and the Cross-Camera Tracklet\nAssociation loss function in a single end-to-end deep learning framework. To our\nknowledge, this is the ï¬rst completely unsupervised learning based re-id model\nwithout any identity labels for model learning, neither pairwise cross-view image\npair labelling nor single-view image identity class labelling. Extensive compara-\ntive evaluations were conducted on six image and video based re-id benchmarks\nto validate the advantages of the proposed TAUDL model over a wide range\nof state-of-the-art unsupervised and domain adaptation re-id methods. We also\nconducted in-depth TAUDL model component evaluation and robustness test to\ngive insights on model performance advantage and model learning stability.\nAcknowledgments\nThis work is partially supported by the China Scholarship Council, Vision Semantics\nLimited, National Natural Science Foundation of China (Project No. 61401212), the\nKey Technology Research and Development Program of Jiangsu Province (Project No.\nBE2015162), the Science and Technology Support Project of Jiangsu Province (Project\nNo. BE2014714), Royal Society Newton Advanced Fellowship Programme (NA150459),\nand Innovate UK Industrial Challenge Project on Developing and Commercialising\nIntelligent Video Analytics Solutions for Public Safety (98111-571149).\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n15\nReferences\n1. Ahmed, Jones, Marks: An improved deep learning architecture for person re-\nidentiï¬cation. In: CVPR (2015)\n2. Ando, R.K., Zhang, T.: A framework for learning predictive structures from mul-\ntiple tasks and unlabeled data. JMLR 6, 1817â€“1853 (2005)\n3. Baltieri, D., Vezzani, R., Cucchiara, R.: 3dpes: 3d people dataset for surveillance\nand forensics. In: J-HGBU (2011)\n4. Bautista, M.A., Sanakoyeu, A., Ommer, B.: Deep unsupervised similarity learning\nusing partially ordered sets. In: CVPR (2017)\n5. Bautista, M.A., Sanakoyeu, A., Tikhoncheva, E., Ommer, B.: Cliquecnn: Deep\nunsupervised exemplar learning. In: NIPS (2016)\n6. Chen, D., Xu, D., Li, H., Sebe, N., Wang, X.: Group consistent similarity learning\nvia deep crf for person re-identiï¬cation. In: CVPR (2018)\n7. Chen, W., Chen, X., Zhang, J., Huang, K.: Beyond triplet loss: a deep quadruplet\nnetwork for person re-identiï¬cation. In: CVPR (2017)\n8. Chen, Y., Zhu, X., Gong, S.: Person re-identiï¬cation by deep learning multi-scale\nrepresentations. In: ICCV Workshop (2017)\n9. Chen, Y.C., Zhu, X., Zheng, W.S., Lai, J.H.: Person re-identiï¬cation by camera\ncorrelation aware feature augmentation. IEEE TPAMI 40(2), 392â€“408 (2018)\n10. Cheng, D., Gong, Y., Zhou, S., Wang, J., Zheng, N.: Person re-identiï¬cation\nby multi-channel parts-based cnn with improved triplet loss function. In: CVPR\n(2016)\n11. Cho, Y.J., Yoon, K.J.: Improving person re-identiï¬cation via pose-aware multi-shot\nmatching. In: CVPR (2016)\n12. Evgeniou, T., Pontil, M.: Regularized multiâ€“task learning. In: SIGKDD (2004)\n13. Fan, H., Zheng, L., Yang, Y.: Unsupervised person re-identiï¬cation: Clustering and\nï¬ne-tuning. arXiv preprint arXiv:1705.10444 (2017)\n14. Farenzena, M., Bazzani, L., Perina, A., Murino, V., Cristani, M.: Person re-\nidentiï¬cation by symmetry-driven accumulation of local features. In: CVPR (2010)\n15. Gong, S., Cristani, M., Yan, S., Loy, C.C.: Person re-identiï¬cation. Springer (2014)\n16. Gray, D., Tao, H.: Viewpoint invariant pedestrian recognition with an ensemble of\nlocalized features. In: ECCV (2008)\n17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n18. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-\nidentiï¬cation. arXiv preprint arXiv:1703.07737 (2017)\n19. Hirzer, M., Beleznai, C., Roth, P.M., Bischof, H.: Person re-identiï¬cation by de-\nscriptive and discriminative classiï¬cation. In: SCIA (2011)\n20. Jiao, J., Zheng, W.S., Wu, A., Zhu, X., Gong, S.: Deep low-resolution person re-\nidentiï¬cation. In: AAAI (2018)\n21. Kawanishi, Y., Wu, Y., Mukunoki, M., Minoh, M.: Shinpuhkan2014: A multi-\ncamera pedestrian dataset for tracking people across multiple cameras. In: FCV\n(2014)\n22. Khan, F.M., Bremond, F.: Unsupervised data association for metric learning in\nthe context of multi-shot person re-identiï¬cation. In: AVSS (2016)\n23. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n24. Kodirov, E., Xiang, T., Fu, Z., Gong, S.: Person re-identiï¬cation by unsupervised\nl1 graph learning. In: ECCV (2016)\n16\nM. Li, X. Zhu, and S. Gong\n25. Kodirov, E., Xiang, T., Gong, S.: Dictionary learning with iterative laplacian reg-\nularisation for unsupervised person re-identiï¬cation. In: BMVC (2015)\n26. Leal-TaixÂ´e, L., Milan, A., Reid, I., Roth, S., Schindler, K.: Motchallenge 2015:\nTowards a benchmark for multi-target tracking. arXiv preprint arXiv:1504.01942\n(2015)\n27. Lee, H., Ekanadham, C., Ng, A.Y.: Sparse deep belief net model for visual area\nv2. In: NIPS (2008)\n28. Li, W., Zhao, R., Wang, X.: Human reidentiï¬cation with transferred metric learn-\ning. In: ACCV (2012)\n29. Li, W., Zhao, R., Xiao, T., Wang, X.: Deepreid: Deep ï¬lter pairing neural network\nfor person re-identiï¬cation. In: CVPR (2014)\n30. Li, W., Zhu, X., Gong, S.: Person re-identiï¬cation by deep joint learning of multi-\nloss classiï¬cation. In: IJCAI (2017)\n31. Li, W., Zhu, X., Gong, S.: Harmonious attention network for person re-\nidentiï¬cation. In: CVPR (2018)\n32. Lisanti, G., Masi, I., Bagdanov, A.D., Del Bimbo, A.: Person re-identiï¬cation by\niterative re-weighted sparse ranking. IEEE TPAMI 37(8), 1629â€“1642 (2015)\n33. Liu, X., Song, M., Tao, D., Zhou, X., Chen, C., Bu, J.: Semi-supervised coupled\ndictionary learning for person re-identiï¬cation. In: CVPR (2014)\n34. Liu, Y., Yan, J., Ouyang, W.: Quality aware network for set to set recognition. In:\nCVPR (2017)\n35. Liu, Z., Wang, D., Lu, H.: Stepwise metric promotion for unsupervised video person\nre-identiï¬cation. In: ICCV (2017)\n36. Loy, C., Xiang, T., Gong, S.: Time-delayed correlation analysis for multi-camera\nactivity understanding. IJCV 90(1), 106â€“129 (2010)\n37. Ma, X., Zhu, X., Gong, S., Xie, X., Hu, J., Lam, K.M., Zhong, Y.: Person re-\nidentiï¬cation by unsupervised video matching. Pattern Recognition 65, 197â€“210\n(2017)\n38. Peng, P., Xiang, T., Wang, Y., Pontil, M., Gong, S., Huang, T., Tian, Y.: Un-\nsupervised cross-dataset transfer learning for person re-identiï¬cation. In: CVPR\n(2016)\n39. Prosser, B.J., Zheng, W.S., Gong, S., Xiang, T.: Person re-identiï¬cation by support\nvector ranking. In: BMVC (2010)\n40. Qin, C., Song, S., Huang, G., Zhu, L.: Unsupervised neighborhood component\nanalysis for clustering. Neurocomputing 168, 609â€“617 (2015)\n41. Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C.: Performance measures\nand a data set for multi-target, multi-camera tracking. In: ECCV Workshop (2016)\n42. Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C.: Performance measures\nand a data set for multi-target, multi-camera tracking. In: ECCV Workshop (2016)\n43. Su, C., Zhang, S., Xing, J., Gao, W., Tian, Q.: Deep attributes driven multi-camera\nperson re-identiï¬cation. In: ECCV (2016)\n44. Subramaniam, Chatterjee, Mittal: Deep neural networks with inexact matching for\nperson re-identiï¬cation. In: NIPS (2016)\n45. Ustinova, E., Lempitsky, V.: Learning deep embeddings with histogram loss. In:\nNIPS (2016)\n46. Wang, F., Zuo, W., Lin, L., Zhang, D., Zhang, L.: Joint learning of single-image\nand cross-image representations for person re-identiï¬cation. In: CVPR (2016)\n47. Wang, H., Gong, S., Xiang, T.: Unsupervised learning of generative topic saliency\nfor person re-identiï¬cation. In: BMVC (2014)\n48. Wang, H., Zhu, X., Gong, S., Xiang, T.: Person re-identiï¬cation in identity regres-\nsion space. IJCV (2018)\nUnsupervised Person Re-ID by Deep Learning Tracklet Association\n17\n49. Wang, H., Zhu, X., Xiang, T., Gong, S.: Towards unsupervised open-set person\nre-identiï¬cation. In: ICIP (2016)\n50. Wang, J., Zhu, X., Gong, S., Li, W.: Transferable joint attribute-identity deep\nlearning for unsupervised person re-identiï¬cation. In: CVPR (2018)\n51. Wang, T., Gong, S., Zhu, X., Wang, S.: Person re-identiï¬cation by video ranking.\nIn: ECCV (2014)\n52. Wang, T., Gong, S., Zhu, X., Wang, S.: Person re-identiï¬cation by discriminative\nselection in video ranking. IEEE TPAMI 38(12), 2501â€“2514 (2016)\n53. Xiao, T., Li, H., Ouyang, W., Wang, X.: Learning deep feature representations\nwith domain guided dropout for person re-identiï¬cation. In: CVPR (2016)\n54. Ye, J., Zhao, Z., Liu, H.: Adaptive distance metric learning for clustering. In: CVPR\n(2007)\n55. Ye, M., Ma, A.J., Zheng, L., Li, J., Yuen, P.C.: Dynamic label graph matching for\nunsupervised video re-identiï¬cation. In: ICCV (2017)\n56. Yu, H.X., Wu, A., Zheng, W.S.: Cross-view asymmetric metric learning for unsu-\npervised person re-identiï¬cation. In: ICCV (2017)\n57. Zhang, S., Benenson, R., Omran, M., Hosang, J., Schiele, B.: How far are we from\nsolving pedestrian detection? In: CVPR (2016)\n58. Zhang, Y., Xiang, T., Hospedales, T.M., Lu, H.: Deep mutual learning. In: CVPR\n(2018)\n59. Zhao, R., Oyang, W., Wang, X.: Person re-identiï¬cation by saliency learning. IEEE\nTPAMI 39(2), 356â€“370 (2017)\n60. Zheng, L., Bie, Z., Sun, Y., Wang, J., Su, C., Wang, S., Tian, Q.: Mars: A video\nbenchmark for large-scale person re-identiï¬cation. In: ECCV (2016)\n61. Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person re-\nidentiï¬cation: A benchmark. In: CVPR (2015)\n62. Zheng, Z., Zheng, L., Yang, Y.: Unlabeled samples generated by gan improve the\nperson re-identiï¬cation baseline in vitro. In: ICCV (2017)\n63. Zhu, X., Wu, B., Huang, D., Zheng, W.S.: Fast openworld person re-identiï¬cation.\nIEEE TIP pp. 2286â€“2300 (2017)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-09-08",
  "updated": "2018-09-08"
}