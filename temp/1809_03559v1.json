{
  "id": "http://arxiv.org/abs/1809.03559v1",
  "title": "Deep Learning Towards Mobile Applications",
  "authors": [
    "Ji Wang",
    "Bokai Cao",
    "Philip S. Yu",
    "Lichao Sun",
    "Weidong Bao",
    "Xiaomin Zhu"
  ],
  "abstract": "Recent years have witnessed an explosive growth of mobile devices. Mobile\ndevices are permeating every aspect of our daily lives. With the increasing\nusage of mobile devices and intelligent applications, there is a soaring demand\nfor mobile applications with machine learning services. Inspired by the\ntremendous success achieved by deep learning in many machine learning tasks, it\nbecomes a natural trend to push deep learning towards mobile applications.\nHowever, there exist many challenges to realize deep learning in mobile\napplications, including the contradiction between the miniature nature of\nmobile devices and the resource requirement of deep neural networks, the\nprivacy and security concerns about individuals' data, and so on. To resolve\nthese challenges, during the past few years, great leaps have been made in this\narea. In this paper, we provide an overview of the current challenges and\nrepresentative achievements about pushing deep learning on mobile devices from\nthree aspects: training with mobile data, efficient inference on mobile\ndevices, and applications of mobile deep learning. The former two aspects cover\nthe primary tasks of deep learning. Then, we go through our two recent\napplications that apply the data collected by mobile devices to inferring mood\ndisturbance and user identification. Finally, we conclude this paper with the\ndiscussion of the future of this area.",
  "text": "Deep Learning Towards Mobile Applications\nJi Wang∗, Bokai Cao†, Philip S. Yu†‡, Lichao Sun†, Weidong Bao∗, and Xiaomin Zhu∗\n∗College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, P. R. China\n†Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA\n‡Institute for Data Science, Tsinghua University, Beijing, P. R. China\nAbstract—Recent years have witnessed an explosive growth\nof mobile devices. Mobile devices are permeating every aspect\nof our daily lives. With the increasing usage of mobile devices\nand intelligent applications, there is a soaring demand for\nmobile applications with machine learning services. Inspired\nby the tremendous success achieved by deep learning in many\nmachine learning tasks, it becomes a natural trend to push\ndeep learning towards mobile applications. However, there exist\nmany challenges to realize deep learning in mobile applications,\nincluding the contradiction between the miniature nature of\nmobile devices and the resource requirement of deep neural\nnetworks, the privacy and security concerns about individuals’\ndata, and so on. To resolve these challenges, during the past few\nyears, great leaps have been made in this area. In this paper, we\nprovide an overview of the current challenges and representative\nachievements about pushing deep learning on mobile devices from\nthree aspects: training with mobile data, efﬁcient inference on\nmobile devices, and applications of mobile deep learning. The\nformer two aspects cover the primary tasks of deep learning.\nThen, we go through our two recent applications that apply the\ndata collected by mobile devices to inferring mood disturbance\nand user identiﬁcation. Finally, we conclude this paper with the\ndiscussion of the future of this area.\nKeywords-Mobile Device; Deep Learning; Federated Learning;\nMobile Cloud; Mobile Application.\nI. INTRODUCTION\nThe past few years have witnessed an explosive growth\nof mobile devices which is expected to continue in the next\ndecades. It is predicted that mobile devices will reach 5.6\nbillion, accounting for 21% of all networked devices in 2020\n[1]. By the end of 2023, more than 90% adults in developed\ncountries will own at least one mobile device. Mobile devices,\npermeating almost every aspect of our daily lives, are redeﬁn-\ning how people live and interact with each other.\nMeanwhile, machine learning (ML) has become commonly\nused in mobile applications like object recognition, language\ntranslation, health monitoring, malware detection [2], [3], [4],\n[5], [6] etc. Due to the frequent interaction between mobile\ndevices and users in daily lives, mobile devices collect a large\namount of data concerning users’ behavior, preference and\nhabits, which makes them promising resources for machine\nlearning applications. According to the report from Delloitte\n[7], the penetration of ML-featured applications among mo-\nbile application used by adult users in developed countries\nsurpasses 60%. Machine learning will become a core element\nof future mobile devices.\nRecently, the most exciting advancement of machine learn-\ning mainly comes from the ﬁeld of deep learning (DL) whose\nunprecedented performance has beaten many records achieved\nby the traditional machine learning algorithms. Deep learning\nhas revolutionized how the world processes, models, and\ninterprets data [8]. Inspired by the outstanding performance of\ndeep learning, people naturally attempt to push deep learning\non mobile devices to provide high-quality intelligent services.\nIt is believed that deep learning will play a paramountly\nimportant role in the evolution of mobile applications [9].\nDespite the attractive prospect, the current research of blending\ndeep learning and mobile devices is just the beginning. Deep\nlearning based applications can be broken down to two primary\ntasks, i.e., training and inference. In the training phase, a\nlarge set of training data are used to adjust the trainable\nparameters in the deep neural network (DNN) automatically\nby the gradient descent algorithms [10], [11], [12]. Given a\nDNN model trained for a speciﬁc application, we are able to\napply it to the inference task such as recognizing an image it\nhas never seen. Due to the limited computation capacity and\nbattery capacity, neither of these two tasks is trivial for mobile\ndevices.\nTraining a deep neural network containing hundreds of\nmillions of parameters can easily introduce huge resource\ndemands that are far beyond the capacity of mobile devices.\nHence, rather than training a DNN model on mobile devices,\nthe current research mainly focuses on how to make full\nutilization of distributed data generated by individuals’ mobile\ndevices with the consideration of safety, conﬁdentiality, and\nprivacy. Training is not the only challenging process in pushing\ndeep learning on mobile devices. Even the inference step using\na DNN model on a mobile device can be difﬁcult [7]. The\nlarge DNN models exceed the limited on-chip memory of\nmobile devices, and thus they have to be accommodated by the\noff-chip memory, which unfortunately consumes signiﬁcantly\nmore energy [13], [14]. What is worse, the enormous dot\nproducts aggravate the burden of processing units. Running\na deep learning application can easily dominate the whole\nsystem energy consumption. To resolve these challenges, both\nthe academia and the industry have studied how to execute\nDNN efﬁciently on mobile devices, which is critical to push\ndeep learning on mobile devices.\nIn this paper, we provide an overview of the current\nchallenges and achievements about pushing deep learning on\nmobile devices. The discussion mainly focuses on how to\nutilize the data generated by individuals’ mobile devices and\nhow to execute DNNs on mobile devices with high efﬁciency.\nIn addition, we will introduce our latest works about deep\narXiv:1809.03559v1  [cs.LG]  10 Sep 2018\nlearning on mobile devices, especially the applications of using\nthe data generated by the mobile devices.\nII. UTILIZATION OF MOBILE DATA\nIn this section, we introduce the works about how to utilize\nthe data generated by individuals’ mobile devices to train a\nmobile deep learning model. Firstly, the distributed training,\nthe foundation of using distributed data to train DNNs, is de-\ntailed. Then, we introduce the latest training scheme, federated\nlearning, proposed by Google. Finally, we discuss the privacy\nconcerns and the corresponding privacy-preserving methods.\nA. Distributed Training\nMobile devices generate and record a large volume of data\nabout their users. To provide an intelligent mobile service,\nit is in high demand to create DNNs by using the data\non individuals’ mobile devices. Due to the resource-hungry\nnature of training phase, it is infeasible to train DNNs on\nresource-constrained mobile devices. Training DNNs still re-\nlies on cloud data centers and high-performance computers.\nNonetheless, the privacy risk and violation make it prohibited\nto transfer individuals’ data directly to third parties. In order\nto utilize these data without exposing them, it is needed to\npropose a distributed training scheme.\nAdjusting the trainable parameters in a DNN is the core\nprocess of training. The gradient descent algorithm [15] and its\nvariants are the most commonly used algorithms for training\nDNNs. Therefore, the main problem of distributed training is\nto design a distributed gradient descent algorithm. Shokri and\nShmatikov designed a distributed selective stochastic gradient\n(SGD) algorithm which realizes distributed and collaborative\ntraining by using data from different sources [16].\nIn the proposed distributed selective SGD, the participants\nwho contribute their own data train the DNN independently\nand concurrently. After each iteration, the participants upload\nthe gradients of some selective parameters to the global param-\neter server. The global parameter server maintains the global\nparameters that are determined by the sum of all gradients\ngathered from participants. Each participant downloads some\nglobal parameters to update its local model. The participants\ncan create a local DNN model by learning more from other\nparticipants’ data.\nFig. 1 shows the framework of distributed selective SGD.\nN participants who have local datasets participate in the dis-\ntrusted training. Each participant uses its local dataset to train\ntheir local DNN model by the standard SGD. No interaction\nwith other participants or the global parameter server is needed\nduring the local training. The gradients of selected parameters\nare then uploaded to the global parameter server to update the\nglobal parameters. The global parameter server maintains the\nparameters of DNN model based on the gradients uploaded by\nparticipants. The participants can download the most recent\nparameters from the global parameter server to update their\nlocal DNN models. This scheme realizes the distributed train-\ning for DNN models without explicitly sharing of different\nFig. 1: Framework of distributed selective SGD [16].\nparticipants’ local data, which is the foundation of utilizing\ndata from different individuals’ mobile devices.\nB. Federated Training on Mobile Devices\nIn order to train a model from the data generated and\ncaptured by mobile devices, Google introduced a new train-\ning scheme called federated training [17]. Federated training\nmakes it possible to train a shared DNN model while keeping\nall the individual’s data local. Each participant of federated\ntraining, i.e., one mobile device, downloads the shared model\nfrom the cloud, improves it by learning from local data, and\nthen summarizes the changes to the cloud to improve the\nshared model. Considering the limited bandwidth and the\nhigh energy cost of wireless communication, federated training\ntries to fully utilize processors in mobile devices to generate\nupdates with higher quality than simple gradient steps in\ndistributed selective SGD.\nSuppose there are K participants in the federated training\nwho share a common DNN model with the loss function L.\nFor participant k, its gradient gk = ∇Lk(ωt), where ωt is\nthe parameters of the current DNN model. In the traditional\ndistributed SGD such as the distributed selective SGD, the\nglobal parameter server updates the parameters as:\nωt+1 ←ωt −η\nK\nX\nk=1\nnk\nn gk,\nwhere η is the learning rate, nk is the size of local dataset, and\nn is the sum of the size of all datasets. Then, the equivalent\nform can be given as follows:\n∀k, ωk\nt+1 ←ωt −ηgk,\nωt+1 ←\nK\nP\nk=1\nnk\nn ωk\nt+1.\nIt means that the participant computes one step of gradient\ndescent on the current parameters, and the global parameter\nserver aggregates these gradients uploaded by participants to\nupdate the current parameters. Uploading gradients after one\nstep of gradient descent indicates more round of iterations\nbefore the convergence. It is natural to let the participant\nperform ωk\nt+1 ←ωt −ηgk multiple times before the uploading\nto generate gradients with higher quality. The experimental\nresults from [18] show that the improved scheme is able to use\n10-100x less communication compared to a naively distributed\nSGD.\nAlthough the federated training achieves higher efﬁciency\nthan the naively distributed SGD, training a DNN model on\nmobile devices is still a tough work which requires sophis-\nticated scheduling. Google pointed out that training happens\nonly when the mobile device is idle, plugged in, and on a free\nwireless connection to avoid possible adversarial impact on\nthe mobile device’s performance.\nC. Privacy-Preserving Training\nThe above distributed training methods enable mobile de-\nvices to learn a shared DNN model without the explicit expo-\nsion of individual’s data. Nonetheless, the gradients uploaded\nby participants may still reveal the features of local training\ndata, which makes it susceptible to powerful attacks such as a\ngenerative adversarial network based method proposed in [19].\nIn order to protect the participants’ privacy, a series of works\nhave been proposed [16], [20], [21], [22], among which the\ndifferential privacy plays an important role.\nDifferential privacy is a concept of privacy tailored to the\nprivacy-preserving data analysis. It aims at providing prov-\nable privacy guarantee for sensitive data and is increasingly\nregarded as a standard notion for rigorous privacy [23]. An\nalgorithm is differentially private when the probability of\ngenerating a particular output is not affected very much by\nwhether one data item is in the input [24]. A well-designed\ndifferentially private algorithm can provide aggregated repre-\nsentations about a set of data items without leaking informa-\ntion of any data item.\nThe property of differential privacy theory makes it a\nfoundation to design privacy-preserving training approaches.\nShokri et al. [16] presented a privacy-preserving distributed\nSGD to enable multiple data owners to collaborate on training\na DNN, in which the sparse vector technique was introduced to\nprovide differential privacy. Abadi et al. [20] designed a new\ndifferential privacy mechanism called moments accountant in\nSGD to reduce the privacy budget. Papernot et al. [21] de-\nsigned a generally applicable framework, private aggregation\nof teacher ensembles, to guarantee the privacy of sensitive\ntraining data. It trained a student model to predict an output\nchosen by noisy voting among all of the teacher models\nwhich are trained by the sensitive data locally. The individual\nteacher model and its parameters are inaccessible to control the\nprivacy budget. Based on the federated training, the researchers\nfrom Google use the differential privacy theory to control the\nparticipants’ privacy loss [22]. Several modiﬁcations to the\nnon-private federated training are made to achieve differential\nprivacy:\n• Rather than always selecting the ﬁxed participants, the\nparticipants are selected independently with probability\np;\n• The updates generated by participants are bounded by a\nspeciﬁc L2 norm;\n• An estimator for weighted aggregation is introduced so\nthat the moments accountant [20] can be applied;\n• Sufﬁcient Gaussian noise is added to the ﬁnal average\nupdate.\nThrough these modiﬁcations, the federated training becomes\na user-level differentially private training approach for DNN\nmodels. The empirical results on a realistic dataset show that it\ncan guarantee the differential privacy without losing accuracy\nof DNN models. It is feasible to use data from individuals’\nmobile devices to train a DNN model with privacy guarantees\nfor real-world applications.\nIII. EFFICIENT INFERENCE ON MOBILE DEVICES\nCompared with the training phase, the inference phase\nrequires much fewer resources, which makes it possible to\nrun the inference on mobile devices locally. Despite this\npossibility, running inference is still an energy-consuming task\nfor mobile devices. Therefore, there are two choices to run\na trained DNN model: inference on the cloud server and\ninference on the local mobile device.\nFig. 2: Illustration of inference on the cloud server.\nFig. 2 demonstrates how inference on the cloud server\nworks. The trained DNN model is deployed on the cloud server\nand publishes an interface for interacting with users’ mobile\napp over the internet. Take an image recognition application\nas an example. The mobile device sends the image to the\ncloud server over the internet. The cloud server receives the\nimage and feeds it into the DNN model for inference after\nwhich the cloud server sends the results back to the mobile\ndevice. Inference on the cloud server deploys the complex\nDNN model on the cloud server and keeps the mobile app\nsimple. All the resource-hungry tasks are accomplished by\nthe cloud server. Besides, the DNN model can be updated\nanytime without the modiﬁcation of local apps. For deep\nlearning service providers, they are free from the worry about\nthe leakage of valuable and highly tuned DNNs as the trained\nmodel are deployed on the server under control. Nonetheless,\ninference on the cloud server has two serious problems. Firstly,\nit requires the internet access to enable the interaction between\nthe mobile device and the cloud server, which sometimes is\ninfeasible in reality. Secondly, submitting individual’s data to\nthe cloud server may violate the privacy requirement. Many\nresearchers have made efforts to resolve these two problems.\nTeerapittayanon et al. [25] designed a distributed DNN archi-\ntecture across the cloud, the edge, and the mobile devices,\nwhich allowed the combination of fast and localized inference\non mobile devices and complex inference in cloud servers.\nFor the privacy concern, Osia et al. [26] designed a hybrid\ndeep learning architecture for private inference across mobile\ndevices and clouds. The Siamese network was used to protect\nagainst undesired inference attacks, which in essence provided\nk-anonymity protection. Li et al. [27] proposed a ﬂexible\nframework for private deep learning. Before revealing data to\nclouds, it was transformed by the local neural network whose\ntopological structure, including the number of layers, the depth\nof output channels, and the subset of selected channels, was\nvariable to protect against the reconstruction attacks.\nFor inference on the local mobile device, the trained DNN\nmodel is deployed on the local mobile device. There is no\ninteraction with other devices during the inference. One of the\nbeneﬁts of doing inference on a mobile device is that users can\nuse the deep learning applications even without the internet\naccess. In addition, as the data would not be transmitted\noutside the mobile device, the individuals’ privacy can be\nguaranteed. However, this scheme also suffers from a series of\ndrawbacks. Firstly, the size of the app combined with the DNN\nmodel may be extraordinarily large, which makes it difﬁcult\nto install and update this app. Secondly, doing inference of\ncomplex DNN models requires a lot of computation resources,\nwhich can quickly drain the limited energy of mobile devices.\nIn the past few years, several methods have been proposed\nto compress DNN models and prune the computation during\ninference. Han et al. [28] took three steps to compress the\nDNN model. Firstly, the network was pruned by learning only\nthe important connection. Then, they quantized the parameters\nto enforce parameter sharing. Finally, the Huffman coding was\napplied. Ding et al. [14] used the fast fourier transform based\nmultiplication to reduce the computational complexity and\nthe storage cost simultaneously. Howard et al. [29] proposed\na group of models named MobileNets which are based on\nthe streamlined architecture for vision applications on mobile\ndevices.\nWe will introduce one of our latest researches about cloud-\nbased inference with the consideration of privacy. Then, we\nwill brieﬂy discuss the researches about compressing DNN\nmodels.\nA. Private Cloud-Based Inference\nIn order to relieve the heavy burden of doing inference\non mobile devices, we propose a cloud-based solution where\nthe shallow portions of a DNN are deployed on mobile\ndevices while the complex and large parts are ofﬂoaded to\nthe cloud server [30]. A fast transformation is ﬁrst carried\nout on the input data locally. Then, the transformed data are\nrevealed to the cloud server for the time and energy consuming\ninferences. But this solution presents obvious privacy issues\nFig. 3: Framework of cloud-based solution.\nwith transmitting data from mobile devices to the cloud server.\nOnce an individual reveals its data consisting of sensitive\ninformation to cloud server for further inferences, it is almost\nimpossible for the individual to control or inﬂuence the usage\nof the data. Hence, it is a critical requirement to lessen the\nprivacy risk when implementing the cloud-based solution. To\nthis end, we designed a framework as shown in Fig. 3.\nThe cloud-based solution relies on the mobile cloud en-\nvironment and divides the DNN into the local-side part and\nthe cloud-side part. The local neural network is derived from\nthe pretrained DNN whose structure and weights are frozen.\nThe cloud neural network is ﬁne-tuned in the training phase.\nThe whole training phase and the complex inference phase\nare performed in the cloud server. Mobile devices merely\nundertake the simple and light-weight feature extraction and\nperturbation.\nIn the inference phase, the sensitive data is transformed\nby the local neural network to extract the general features\nembedded in it. For preserving privacy, the transformation is\nperturbed by both nulliﬁcation and random noise which satisfy\nthe differentially private requirements. Then, the perturbed\nrepresentations are transmitted to the cloud for further complex\ninference. Because the data to be transmitted are the abstract\nrepresentation of the raw data, the size of the data to be\ntransmitted is smaller than that of the raw data. The local initial\ntransformation can reduce communication cost compared with\ntransmitting raw data directly.\nIn the training phase, we use the public data of the same\ntype as the sensitive data to train the cloud neural network. In\norder to improve the robustness of the cloud neural network\nto the noise, we propose a noisy training method where both\nraw training data and generative training data are fed into the\nnetwork to tune the weights. The key component in noisy\ntraining is the generative model that generates sophisticated\nnoisy training samples based on the public data. In addition, it\nis worth noting that the training phase and the inference phase\ncan run in parallel once we get an initial cloud neural network.\nBeneﬁting from the transfer learning, we only need to train the\ncloud neural network for different datasets and tasks while\nkeeping the local neural network frozen. It indicates that all\nthe works in the cloud side are transparent to the end mobile\ndevices. The cloud neural network can be upgraded online\nwithout the service interruption to end users. This transparency\nproperty facilitates the deep learning service on one hand and\non the other hand protects the intellectual property of deep\nlearning service providers.\nIn this framework, the differential privacy is guaranteed\nby the nulliﬁcation and random noise added in the local\ntransformation. To alleviate the adverse impact of random\nnoise on the DNN model’s performance, we propose a novel\ntraining method called noisy training where deliberate noisy\nsamples are injected into the training data. The preliminary ex-\nperimental results show that this solution can not only preserve\nusers privacy but also improve the inference performance.\nB. Model Compression and Acceleration\nModel compression and acceleration is another efﬁcient\nmethod to reduce the DNN model’s storage and computational\ncost. By compressing the large DNN model, the resource and\nenergy requirement of inference on mobile devices can be sig-\nniﬁcantly relieved. Currently, there are three main approaches\nfor DNN model compression [31]: (1) parameter pruning and\nsharing; (2) low-rank factorization; (3) model distillation.\nThe intuition of parameter pruning and sharing is that many\nparameters in the network are not crucial to the performance of\nthe model. There are three techniques to prune the parameters,\ni.e., network quantization, weight and connection pruning, and\nstructural matrix. Network quantization compress the DNN\nby reducing the bits required to depict the parameters in the\nnetwork [32], [33], [34]. Weight and connection pruning tries\nto prune the redundant weights in the DNN model. The core\nidea of structural matrix is to describe an m × n matrix by\nusing a structured matrix with much fewer parameters than\nmn [35].\nThe low-rank factorization is used to reduce the convolution\nlayer whose kernel can be regarded as a 4D tensor. A 4D\ntensor usually has a large amount of redundancy which can\nbe removed by the low-rank factorization [36]. It should be\nnoted that the fully-connected layer can be considered as a 2D\nmatrix so the low-rank factorization can also be employed.\nThe model distillation compresses the DNNs into shallower\nones by mimicking the function of the original complex DNN.\nThis approach can be viewed as transferring knowledge from\na large teacher model into a small student model [37].\nIV. APPLICATIONS OF MOBILE DEEP LEARNING\nTwo applications using the data generated by mobile de-\nvices will be introduced in this section. The two applications\napply the data collected by mobile devices to inferring mood\ndisturbance and user identiﬁcation, respectively.\nA. Mood Disturbance Inference\nThe wide use of mobile devices presents new opportunities\nin the study and treatment of psychiatric illness including the\nability to study the manifestations of psychiatric illness in the\nsetting of patients’ daily lives in an unobtrusive manner and at\na level of detail that was not previously possible. Continuous\nreal-time monitoring in naturalistic settings and collection of\nFig. 4: The architecture of DeepMood [3].\nautomatically generated mobile device data that reﬂect illness\nactivity could facilitate early intervention and have a potential\nuse as objective outcome measures in efﬁcacy trials [38], [39],\n[40].\nThe data used in this work were collected from the BiAf-\nfect1 study. During a preliminary data collection phase, for\na period of 8 weeks, 40 individuals were provided a Galaxy\nNote 4 mobile phone which they were instructed to use as\ntheir primary phone during the study. This phone was loaded\nwith a custom keyboard that replaced the standard Android\nOS keyboard. The keyboard collected metadata consisting\nof keypress entry time and accelerometer movement and\nuploaded them to the study server. Three types of metadata\ncollected are detailed as follows:\n• Alphanumeric characters. Due to privacy reasons, we\nonly collected metadata for keypresses on alphanumeric\ncharacters, including duration of a keypress, time since\nlast keypress, and distance from last key along two axisex.\n• Special characters. We use one-hot-encoding for typing\nbehaviors other than alphanumeric characters, including\nauto-correct, backspace, space, suggestion, switching-\nkeyboard and other. They are usually sparser than al-\nphanumeric characters.\n• Accelerometer values. Accelerometer values are recored\nevery 60ms in the background during an active session\nregardless of a person’s typingspeed, thereby making\nthem much denser than alphanumeric characters.\nWe study these dynamics metadata on a session-level. A ses-\nsion is deﬁned as beginning with a keypress which occurs after\n5 or more seconds have elapsed since the last keypress and\ncontinuing until 5 or more seconds elapse between keypresses.\nThe duration of a session is typically less than one minute. In\nthis manner, each participant would contribute many samples,\none per phone usage session, which could beneﬁt data analysis\nand model training.\nIn [3], a deep architecture, named DeepMood, is proposed\nto infer mood disturbance and severity from mobile phone\ntyping dynamics. The architecture is illustrated in Fig. 4. In\nparticular, it is an end-to-end late fusion approach for modeling\nthe multi-view time series data. In the ﬁrst stage, each view of\nthe time series is separately modeled using Gated Recurrent\n1http://www.biaffect.com/\nUnit (GRU) [41] which is a simpliﬁed version of Long Short-\nTerm Memory (LSTM) [42]. A typical GRU keeps hidden\nstates over a sequence of elements and updates the hidden\nstate hk by the current input xk as well as the previous hidden\nstate hk−1 with a recurrent function as follows:\nrk = sigmoid(Wrxk + Urhk−1)\nzk = sigmoid(Wzxk + Uzhk−1)\n˜hk = tanh(Wxk + U(rk ⊙hk−1))\nhk = zk ⊙hk−1 + (1 −zk) ⊙˜hk\n(1)\nwhere ⊙is the element-wise multiplication operator, a reset\ngate rk allows the GRU to forget the previously computed state\nhk−1, and an update gate zk balances between the previous\nstate hk−1 and the candidate state ˜hk. The hidden state hk\ncan be considered as a compact representation of the input\nsequence from x1 to xk.\nThe multi-view time series data are then fused in the second\nstage by exploring interactions across the output vectors from\neach view, where three alternative approaches are developed\nfollowing the idea of Multi-view Machines [43], Factorization\nMachines [44], or in a fully connected fashion. Let us denote\nthe output vectors at the end of a sequence from the p-th view\nas h(p). In this manner, {h(p) ∈Rdh}m\np=1 can be considered\nas multi-view data where m is the number of views.\nFully connected layer. In order to generate a prediction\non the mood score, a straightforward idea is to ﬁrst con-\ncatenate features from multiple views together, i.e., h =\n[h(1); h(2); · · · ; h(m)] ∈Rd, where d is the total number\nof multi-view features, and typically d = mdh for one-\ndirectional GRU and d = 2mdh for bidirectional GRU.\nThe concatenated hidden states h are then fed into one or\nseveral fully connected neural network layers with a nonlinear\nfunction σ(·) in between [3].\nq = relu(W(1)[h; 1])\nˆy = W(2)q\n(2)\nwhere W(1) ∈Rk′×(d+1), W(2) ∈Rc×k′, k′ is the number\nof hidden units, c is the number of classes, and the constant\nsignal “1” is to model the global bias.\nFactorization Machine layer. Rather than capturing nonlin-\nearity through the transformation function, one can explicitly\nmodel feature interactions between input units [3].\nqa = Uah\nba = wT\na [h; 1]\nˆya = sum([qa ⊙qa; ba])\n(3)\nwhere Ua ∈Rk×d, wa ∈Rd+1, k is the number of factor\nunits, and a denotes the a-th class.\nMulti-view Machine layer. In contrast to modeling up to the\nsecond-order feature interactions between all input units as in\nthe Factorization Machine layer, one can further explore all\nfeature interactions up to the mth-order between inputs from\nm views [3].\nq(p)\na\n= U(p)\na [h(p); 1]\nˆya = sum([q(1)\na\n⊙· · · ⊙q(m)\na\n])\n(4)\nwhere U(p)\na\n∈Rk×(dh+1) is the factor matrix of the p-th view\nfor the a-th class. By denoting ¯h(p) = [h(p); 1], p = 1, · · · , m,\nwe can verify that Eq. (4) is equivalent to Multi-view Machines\n[43].\nFor experiments, the implementation is completed using\nKeras [45] with Tensorﬂow [46] as the backend. Features of\nalphanumeric characters, special characters and accelerome-\nter values in a phone usage session are utilized to predict\nusers’ mood score. It is shown that the late fusion based\nDeepMood methods can achieve up to 90.31% accuracy on\npredicting the depression score. It demonstrates the feasibility\nof using passive typing dynamics from mobile phone metadata\nto predict the disturbance and severity of mood states. In\naddition, it is found that the conventional shallow models like\nSupport Vector Machine and Logistic Regression are not a\ngood ﬁt to this task, or sequence prediction in general. The tree\nboosting system XGBoost2 [47] performs reasonably well as\nan ensemble method, but DeepMood still outperforms it by a\nsigniﬁcant margin 5.56% in terms of prediction accuracy.\nFig. 5: Prediction performance on each of the 20 participants\n[3].\nIn practice, it is important to understand how the model\nworks for each individual when monitoring her mood states.\nTherefore, we investigate the prediction performance on each\nof the 20 participants in our dataset. Results are shown\nin Fig. 5 where each dot represents a participant with the\nnumber of her contributed sessions in the training set and\nthe corresponding prediction accuracy. We can see that the\nproposed model can steadily produce accurate predictions\n(≥87%) of a participant’s mood states when she provides\nmore than 400 valid typing sessions in the training phase. Note\nthat the prediction we make in this work is per session which is\ntypically less than one minute. We can expect more accurate\n2https://github.com/dmlc/xgboost\nresults on the daily level by ensembling sessions occurring\nduring a day.\nThe empirical analysis suggests that mobile device metadata\ncould be used to predict the presence of mood disorders.\nThe ability to passively collect data that can be used to\ninfer the presence and severity of mood disturbances may\nenable providers to provide interventions to more patients\nearlier in their mood episodes, and it may also lead to deeper\nunderstanding of the effects of mood disturbances in the daily\nactivities of people with mood disorders.\nB. User Identiﬁcation\nUser identiﬁcation is a fundamental, but yet an open prob-\nlem in mobile computing. Traditional approaches resort to\nuser account information or browsing history. However, such\ninformation can pose security and privacy risks, and it is not\nrobust as can be easily changed, e.g., the user changes to a new\ndevice or using a different application. Monitoring biometric\ninformation including a user’s typing behaviors tends to pro-\nduce consistent results over time while being less disruptive\nto user’s experience. Furthermore, there are different kinds of\nsensors on mobile devices, meaning rich biometric information\nof users can be simultaneously collected. Thus, monitoring\nbiometric information appears to be quite promising for mobile\nuser identiﬁcation.\nIn [48], we collect information from basic keystroke and the\naccelerometer on the phone, and then propose DEEPSERVICE,\na multi-view deep learning method, to utilize this information.\nTo the best of our knowledge, this is the ﬁrst time multi-view\ndeep learning is applied to mobile user identiﬁcation.\nDEEPSERVICE is a multi-view and multi-class identiﬁcation\nframework via a deep structure. It contains three main steps\nto identify each user from several users.\n• In the ﬁrst step, sequential tapping information and ac-\ncelerometer information from volunteers who have used\nour provided mobile devices are collected. The sequential\ndata is retrieved in a real-time manner.\n• In the second step, the collected information is prepared\nas multi-view data for the problem of use identiﬁcation.\n• In the third step, the multi-view data is modeled via a\ndeep structure to perform multi-class learning.\n• In the last step, the performance of the proposed ap-\nproach is compared with the traditional machine learning\ntechniques for multi-class identiﬁcation such as support\nvector machine and random forest.\nThe feature patterns for different users are evaluated. Fig. 6\nshows the feature patterns analysis of top 5 active users in\nmulti-view.\nIn the view of Alphabet graphs, each user tends to have\nunique patterns with respect to the duration, the time since\nlast key, and the number of keystrokes in each session. For\nexample, user3 prefers to use more keystroke in every session\nwith quicker tapping speed than other users.\nIn the view of Symbol/Number graphs, we have 8 different\nfeatures. We separate these features into two groups: frequent\nkeys and infrequent keys. A frequent key is deﬁned as a key\nFig. 6: Multi-view Pattern Analysis of Top 5 Active User: Left\nis user1 and Right is user5 [48]\nTABLE I: Results of DEEPSERVICE and Baselines\n10\n26\nMethod\nAccuracy\nF1\nAccuracy\nF1\nLR\n44.25%\n45.31%\n27.44%\n30.26%\nSVM\n44.39%\n45.12%\n30.33%\n31.90%\nDecision Tree\n53.50%\n52.85%\n43.37%\n42.42%\nRandomForest\n77.05%\n76.59%\n67.87%\n66.31%\nXGBoost\n85.14%\n84.93%\n79.48%\n78.81%\nDEEPSERVICE\n87.35%\n87.69%\n82.73%\n83.25%\nthat is used more than twice per session, otherwise the key is\nan infrequent key. We show the medium number of keystroke\nper session of frequent keys such as auto correct, backspace\nand space. We also show the range and radio of infrequent\nkeys per session of the top ﬁve active users. For example,\nuser4 frequently uses auto correct, but she infrequently uses\nbackspace.\nIn view of Acceleration graphs, we show the correlation of\ndifferent directions of acceleration. From the last graph, we\nﬁnd that the top 5 active users can be well separated, which\nproves acceleration can help to identify the users well.\nOur experimental results show that DEEPSERVICE can do\nwell identiﬁcation between any two users with 98.97% f1\nscore and 99.1% accuracy in average. For example, private\nsmartphone usually would not be shared by different people.\nHowever, sometimes the private phone could be shared be-\ntween two people, such as husband and wife. DEEPSERVICE\ncan well separate any two people in this case.\nFor more general scenarios, we expand binary-identiﬁcation\nto N active class(user) identiﬁcation to ﬁgure out who is using\nthe mobile device either locally or on a web browser. Table I\nreports our results.\nIf we increase the total number of the users in our model,\nit means we want to identify more people at the same time.\nFor example, if our model is used on a home-router, it may\nneed to identify only members of a family (3 to 10 people) at\nonce. If we, instead, want to identify people working in a small\nofﬁce, we may need to identify more than 10 users. However,\nit is possible that a larger number of users would degrade the\naverage performance of user identiﬁcation. This is due to more\nvariation of shared biometric patterns that introduce ambiguity\ninto the system.\nV. CONCLUSION\nPushing deep learning towards mobile devices has become\na hot topic for both the academia and the industry. With the\nincreasing demands of intelligent services on mobile devices,\nit is supposed to see continued leaps in deep learning based\nmobile applications in the next few years. Nonetheless, the\ncurrent research about deep learning on mobile devices is just\nat the beginning. Encountered by the contradiction between\nhigh resource requirement of DNN and limited capacity of\nmobile devices, it is a challenging work to enabling deep\nlearning services on mobile devices.\nIn this paper, we introduce the representative works about\ndeep learning on mobile devices from three aspects: training\nby using mobile data, inference on mobile devices, and appli-\ncations of mobile deep learning. We point out the main difﬁ-\nculties and solutions of these problems. Besides, we introduce\nour latest researches about deep learning on mobile devices.\nThe summary of current works is provided in the paper.\nThere are many unsolved and interesting problems about\npushing deep learning towards mobile applications. Looking\ntoward the next few years, with the rapid progress of mo-\nbile machine-learning processors and the emerge of next-\ngeneration wireless communication, mobile devices will be\nendowed with much more powerful computation capacity. We\nare likely to see more training tasks not just inference tasks\nbeing performed on mobile devices and growing numbers\nof mobile applications using deep learning being published\nby app providers and deployed by mobile users. More and\nmore deep-learning applications will go out of the clouds\nand into the mobile devices, whether a smart phone or a\nembedded sensor. Nonetheless, the DNN’s great demand for\ndata would make the concerns about privacy and security of\nindividuals’ information mount, which may become a critical\nproblem hindering the development of deep learning in mobile\napplications. How to overcome the two challenges about\nefﬁciency and privacy is supposed to be the main topic in\nthis area. In addition, how to employ deep learning in diverse\nmobile applications is an important problem. Currently, deep\nlearning is mainly applied to image and voice processing on\nmobile devices. Considering the versatility of mobile devices,\nwe should attempt to empower more mobile applications by\ndeep learning to make people’s daily life more convenient and\nintelligent.\nREFERENCES\n[1] Cisco, “Cisco visual networking index: forecast and methodology, 2015-\n2020,” White Paper, 2016.\n[2] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in\nProceedings of the IEEE International Conference on Computer Vision\n(ICCV), 2017.\n[3] B. Cao, L. Zheng, C. Zhang, P. S. Yu, A. Piscitello, J. Zulueta, O. Ajilore,\nK. Ryan, and A. D. Leow, “DeepMood: Modeling mobile phone typing\ndynamics for mood detection,” in KDD.\nACM, 2017, pp. 747–755.\n[4] J. Li, D. Xiong, Z. Tu, M. Zhu, M. Zhang, and G. Zhou, “Modeling\nsource syntax for neural machine translation,” in 55th annual meeting of\nthe Association for Computational Linguistics (ACL), 2017, pp. 4594–\n4602.\n[5] L. Sun, Z. Li, Q. Yan, W. Srisa-an, and Y. Pan, “Sigpid: signiﬁcant\npermission identiﬁcation for android malware detection,” in 11th Inter-\nnational Conference on Malicious and Unwanted Software (MALWARE),\n2016, pp. 1–8.\n[6] Z. Li, L. Sun, Q. Yan, W. Srisa-an, and Z. Chen, “Droidclassiﬁer:\nefﬁcient adaptive mining of application-layer header for classifying\nandroid malware,” in International Conference on Security and Privacy\nin Communication Systems (SecureComm), 2016, pp. 597–616.\n[7] P. Lee, “Technology, media and telecommunications predictions,” Del-\nloitte Touche Tohmatsu Limited, 2017.\n[8] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge,\nMA, USA: The MIT Press, 2016.\n[9] N. D. Lane, S. Bhattacharya, A. Mathur, P. Georgiev, C. Forlivesi,\nand F. Kawsar, “Squeezing deep learning into mobile and embedded\ndevices,” IEEE Pervasive Computing, vol. 16, no. 3, pp. 82–88, 2017.\n[10] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin International Conference on Learning Representations (ICLR), 2015.\n[11] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods\nfor online learning and stochastic optimization,” Journal of Machine\nLearning Research, vol. 12, no. 6, pp. 2121–2159, 2011.\n[12] T. Tieleman and G. E. Hinto, “Lectur e6.5-rmsprop: Divide the gradient\nby a running average of its recent magnitude,” COURSERA: Neural\nNetworks for Machine Learnings, vol. 4, no. 2, 2012.\n[13] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both weights and\nconnections for efﬁcient neural networks,” in Proceedings of the 28th\nInternational Conference on Neural Information Processing Systems\n(NIPS), 2015, pp. 1135–1143.\n[14] C. Ding, S. Liao, Y. Wang, Z. Li, N. Liu, Y. Zhuo, C. Wang, X. Qian,\nY. Bai, G. Yuan, X. Ma, Y. Zhang, J. Tang, Q. Qiu, X. Lin, and B. Yuan,\n“Circnn: Accelerating and compressing deep neural networks using\nblock-circulant weight matrices,” in Proceedings of the 50th Annual\nIEEE/ACM International Symposium on Microarchitecture (MICRO).\nACM, 2017, pp. 395–408.\n[15] M. Avriel, Nonlinear Programming: Analysis and Methods.\nDover\nPublications, 2003.\n[16] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in\nProceedings of the 22nd ACM SIGSAC Conference on Computer and\nCommunications Security (CCS), 2015, pp. 1310–1321.\n[17] B. McMahan and D. Ramage, “Federated learning: Collaborative ma-\nchine learning without centralized training data,” Google Research Blog,\n2017.\n[18] H. B. McMahan, E. Moore, and D. R. D. A. y Arcas, “Federated learning\nof deep networks using model averaging,” arXiv:1602.05629, 2016.\n[19] B. Hitaj, G. Ateniese, and F. Perez-Cruz, “Deep models under the gan:\nInformation leakage from collaborative deep learning,” in Proceedings of\nACM SIGSAC Conference on Computer and Communications Security\n(CCS).\nACM, 2017, pp. 603–618.\n[20] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,\nK. Talwar, and L. Zhang, “Deep learning with differential privacy,” in\nProceedings of the 23rd ACM SIGSAC Conference on Computer and\nCommunications Security (CCS), 2016, pp. 308–318.\n[21] N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and K. Talwar,\n“Semi-supervised knowledge transfer for deep learning from private\ntraining data,” in 5th International Conference on Learning Represen-\ntations (ICLR), 2017.\n[22] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning\ndifferentially private recurrent language models,” arXiv:1710.06963,\n2017.\n[23] A. Beimel, H. Brenner, S. P. Kasiviswanathan, and K. Nissim, “Bounds\non the sample complexity for private learning and private data release,”\nMachine Learning, vol. 94, no. 3, pp. 401–437, 2014.\n[24] C. Dwork, Differential Privacy.\nBoston, MA: Springer US, 2011, pp.\n338–340.\n[25] S. Teerapittayanon, B. McDanel, and H. T. Kung, “Distributed deep\nneural networks over the cloud, the edge and end devices,” in IEEE 37th\nInternational Conference on Distributed Computing Systems (ICDCS),\n2017, pp. 328–339.\n[26] S. A. Osia, A. S. Shamsabadi, A. Taheri, H. R. Rabiee, N. D. Lane, and\nH. Haddadi, “A hybrid deep learning architecture for privacy-preserving\nmobile analytics,” arXiv:1703.02952, 2017.\n[27] M. Li, L. Lai, N. Suda, V. Chandra, and D. Z. Pan, “Privynet: A ﬂexible\nframework for privacy-preserving deep neural network training with a\nﬁne-grained privacy control,” arXiv:1709.06161, 2017.\n[28] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing\ndeep neural networks with pruning, trained quantization and huffman\ncoding,” in 4th International Conference on Learning Representations\n(ICLR), 2016.\n[29] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,\nM. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convolutional neural\nnetworks for mobile vision applications,” arXiv:1704.04861, 2017.\n[30] J. Wang, J. Zhang, W. Bao, X. Zhu, B. Cao, and P. S. Yu, “Not just pri-\nvacy: Improving performance of private deep learning in mobile cloud,”\nin Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining (KDD), 2018, pp. 2407–2416.\n[31] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A survey of model\ncompression and acceleration for deep neural networks,” IEEE Signal\nProcessing Magazine, vol. online, 2017.\n[32] Y. Gong, L. Liu, M. Yang, and L. Bourdev, “Compressing deep convo-\nlutional networks using vector quantization,” arXiv:1412.6115, 2014.\n[33] J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng, “Quantized convolutional\nneural networks for mobile devices,” in IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2016.\n[34] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep\nlearning with limited numerical precision,” in International Conference\non Learning Representations (ICLR), 2016.\n[35] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-\nF. Chang, “An exploration of parameter redundancy in deep networks\nwith circulant projections,” in Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2015.\n[36] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus,\n“Exploiting linear structure within convolutional networks for efﬁcient\nevaluation,” in Advances in Neural Information Processing Systems\n(NIPS), 2014, pp. 1269–1277.\n[37] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” in Advances in Neural Information Processing Systems\n(NIPS), 2014.\n[38] D. Ankers and S. H. Jones, “Objective assessment of circadian activity\nand sleep patterns in individuals at behavioural risk of hypomania,”\nJournal of clinical psychology, vol. 65, no. 10, pp. 1071–1086, 2009.\n[39] J. M. Bopp, D. J. Miklowitz, G. M. Goodwin, W. Stevens, J. M.\nRendell, and J. R. Geddes, “The longitudinal course of bipolar disorder\nas revealed through weekly text messaging: a feasibility study,” Bipolar\ndisorders, vol. 12, no. 3, pp. 327–334, 2010.\n[40] M. Faurholt-Jepsen, M. Vinberg, M. Frost, S. Debel, E. Margrethe Chris-\ntensen, J. E. Bardram, and L. V. Kessing, “Behavioral activities collected\nthrough smartphones and the association with illness activity in bipolar\ndisorder,” International journal of methods in psychiatric research,\nvol. 25, no. 4, pp. 309–323, 2016.\n[41] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y. Bengio, “Learning phrase representations using\nrnn encoder-decoder for statistical machine translation,” arXiv preprint\narXiv:1406.1078, 2014.\n[42] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[43] B. Cao, H. Zhou, G. Li, and P. S. Yu, “Multi-view machines,” in WSDM.\nACM, 2016, pp. 427–436.\n[44] S. Rendle, “Factorization machines with libfm,” ACM Transactions on\nIntelligent Systems and Technology, vol. 3, no. 3, p. 57, 2012.\n[45] F. Chollet, “Keras,” https://github.com/fchollet/keras, 2015.\n[46] M.\nA.\net\nal.,\n“TensorFlow:\nLarge-scale\nmachine\nlearning\non\nheterogeneous systems,” 2015, software available from tensorﬂow.org.\n[Online]. Available: http://tensorﬂow.org/\n[47] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”\nin KDD.\nACM, 2016.\n[48] L. Sun, Y. Wang, B. Cao, S. Y. Philip, W. Srisa-an, and A. D. Leow,\n“Sequential keystroke behavioral biometrics for mobile user identiﬁca-\ntion via multi-view deep learning,” in Joint European Conference on\nMachine Learning and Knowledge Discovery in Databases, 2017, pp.\n228–240.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.DC"
  ],
  "published": "2018-09-10",
  "updated": "2018-09-10"
}