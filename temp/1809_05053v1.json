{
  "id": "http://arxiv.org/abs/1809.05053v1",
  "title": "XNLI: Evaluating Cross-lingual Sentence Representations",
  "authors": [
    "Alexis Conneau",
    "Guillaume Lample",
    "Ruty Rinott",
    "Adina Williams",
    "Samuel R. Bowman",
    "Holger Schwenk",
    "Veselin Stoyanov"
  ],
  "abstract": "State-of-the-art natural language processing systems rely on supervision in\nthe form of annotated data to learn competent models. These models are\ngenerally trained on data in a single language (usually English), and cannot be\ndirectly used beyond that language. Since collecting data in every language is\nnot realistic, there has been a growing interest in cross-lingual language\nunderstanding (XLU) and low-resource cross-language transfer. In this work, we\nconstruct an evaluation set for XLU by extending the development and test sets\nof the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 15\nlanguages, including low-resource languages such as Swahili and Urdu. We hope\nthat our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence\nunderstanding by providing an informative standard evaluation task. In\naddition, we provide several baselines for multilingual sentence understanding,\nincluding two based on machine translation systems, and two that use parallel\ndata to train aligned multilingual bag-of-words and LSTM encoders. We find that\nXNLI represents a practical and challenging evaluation suite, and that directly\ntranslating the test data yields the best performance among available\nbaselines.",
  "text": "XNLI: Evaluating Cross-lingual Sentence Representations\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Holger Schwenk, Ves Stoyanov\nFacebook AI\naconneau,glample,ruty,schwenk,ves@fb.com\nAdina Williams, Samuel R. Bowman\nNew York University\nadinawilliams,bowman@nyu.edu\nAbstract\nState-of-the-art natural language processing\nsystems rely on supervision in the form of an-\nnotated data to learn competent models. These\nmodels are generally trained on data in a sin-\ngle language (usually English), and cannot be\ndirectly used beyond that language. Since col-\nlecting data in every language is not realis-\ntic, there has been a growing interest in cross-\nlingual language understanding (XLU) and\nlow-resource cross-language transfer. In this\nwork, we construct an evaluation set for XLU\nby extending the development and test sets of\nthe Multi-Genre Natural Language Inference\nCorpus (MultiNLI) to 15 languages, includ-\ning low-resource languages such as Swahili\nand Urdu. We hope that our dataset, dubbed\nXNLI, will catalyze research in cross-lingual\nsentence understanding by providing an infor-\nmative standard evaluation task. In addition,\nwe provide several baselines for multilingual\nsentence understanding, including two based\non machine translation systems, and two that\nuse parallel data to train aligned multilingual\nbag-of-words and LSTM encoders. We ﬁnd\nthat XNLI represents a practical and challeng-\ning evaluation suite, and that directly translat-\ning the test data yields the best performance\namong available baselines.\n1\nIntroduction\nContemporary natural language processing sys-\ntems typically rely on annotated data to learn how\nto perform a task (e.g., classiﬁcation, sequence\ntagging, natural language inference). Most com-\nmonly the available training data is in a single lan-\nguage (e.g., English or Chinese) and the resulting\nsystem can perform the task only in the training\nlanguage. In practice, however, systems used in\nmajor international products need to handle inputs\nin many languages. In these settings, it is nearly\nimpossible to annotate data in all languages that a\nsystem might encounter during operation.\nA scalable way to build multilingual systems\nis through cross-lingual language understanding\n(XLU), in which a system is trained primarily on\ndata in one language and evaluated on data in\nothers. While XLU shows promising results for\ntasks such as cross-lingual document classiﬁcation\n(Klementiev et al., 2012; Schwenk and Li, 2018),\nthere are very few, if any, XLU benchmarks for\nmore difﬁcult language understanding tasks like\nnatural language inference.\nLarge-scale natural\nlanguage inference (NLI), also known as recog-\nnizing textual entailment (RTE), has emerged as a\npractical test bed for work on sentence understand-\ning. In NLI, a system is tasked with reading two\nsentences and determining whether one entails the\nother, contradicts it, or neither (neutral).\nRe-\ncent crowdsourced annotation efforts have yielded\ndatasets for NLI in English (Bowman et al., 2015;\nWilliams et al., 2017) with nearly a million exam-\nples, and these have been widely used to evaluate\nneural network architectures and training strate-\ngies (Rocktäschel et al., 2016; Gong et al., 2018;\nPeters et al., 2018; Wang et al., 2018), as well as to\ntrain effective, reusable sentence representations\n(Conneau et al., 2017; Subramanian et al., 2018;\nCer et al., 2018; Conneau et al., 2018a).\nIn this work, we introduce a benchmark that\nwe call the Cross-lingual Natural Language In-\nference corpus, or XNLI, by extending these NLI\ncorpora to 15 languages. XNLI consists of 7500\nhuman-annotated development and test examples\nin NLI three-way classiﬁcation format in English,\nFrench, Spanish, German, Greek, Bulgarian, Rus-\nsian, Turkish, Arabic, Vietnamese, Thai, Chi-\nnese, Hindi, Swahili and Urdu, making a total of\n112,500 annotated pairs.\nThese languages span\nseveral language families, and with the inclusion\nof Swahili and Urdu, include two lower-resource\nlanguages as well.\nBecause of its focus on development and test\narXiv:1809.05053v1  [cs.CL]  13 Sep 2018\nLanguage Premise / Hypothesis\nGenre\nLabel\nEnglish\nYou don’t have to stay there.\nYou can leave.\nFace-To-Face\nEntailment\nFrench\nLa ﬁgure 4 montre la courbe d’offre des services de partage de travaux.\nLes services de partage de travaux ont une offre variable.\nGovernment\nEntailment\nSpanish\nY se estremeció con el recuerdo.\nEl pensamiento sobre el acontecimiento hizo su estremecimiento.\nFiction\nEntailment\nGerman\nWährend der Depression war es die ärmste Gegend, kurz vor dem Hungertod.\nDie Weltwirtschaftskrise dauerte mehr als zehn Jahre an.\nTravel\nNeutral\nSwahili\nNi silaha ya plastiki ya moja kwa moja inayopiga risasi.\nInadumu zaidi kuliko silaha ya chuma.\nTelephone\nNeutral\nRussian\nИ мы занимаемся этим уже на протяжении 85 лет.\nМы только начали этим заниматься.\nLetters\nContradiction\nChinese\n让我告诉你，美国人最终如何看待你作为独立顾问的表现。\n美国人完全不知道您是独立律师。\nSlate\nContradiction\nArabic\nNine-Eleven\nContradiction\nTable 1: Examples (premise and hypothesis) from various languages and genres from the XNLI corpus.\ndata, this corpus is designed to evaluate cross-\nlingual sentence understanding, where models\nhave to be trained in one language and tested in\ndifferent ones.\nWe evaluate several approaches to cross-lingual\nlearning of natural language inference that lever-\nage parallel data from publicly available corpora\nat training time. We show that parallel data can\nhelp align sentence encoders in multiple languages\nsuch that a classiﬁer trained with English NLI data\ncan correctly classify pairs of sentences in other\nlanguages. While outperformed by our machine\ntranslation baselines, we show that this alignment\nmechanism gives very competitive results.\nA second practical use of XNLI is the eval-\nuation of pretrained general-purpose language-\nuniversal sentence encoders.\nWe hope that this\nbenchmark will help the research community build\nmultilingual text embedding spaces. Such embed-\ndings spaces will facilitate the creation of multi-\nlingual systems that can transfer across languages\nwith little or no extra supervision.\nThe paper is organized as follows: We next sur-\nvey the related literature on cross-lingual language\nunderstanding. We then describe our data collec-\ntion methods and the resulting corpus in Section 3.\nWe describe our baselines in Section 4, and ﬁnally\npresent and discuss results in Section 5.\n2\nRelated Work\nMultilingual Word Embeddings\nMuch of the\nwork on multilinguality in language understand-\ning has been at the word level. Several approaches\nhave been proposed to learn cross-lingual word\nrepresentations, i.e., word representations where\ntranslations are close in the embedding space.\nMany of these methods require some form of\nsupervision (typically in the form of a small bi-\nlingual lexicon) to align two sets of source and\ntarget embeddings to the same space (Mikolov\net al., 2013a; Kociský et al., 2014; Faruqui and\nDyer, 2014; Ammar et al., 2016).\nMore recent\nstudies have showed that cross-lingual word em-\nbeddings can be generated with no supervision\nwhatsoever (Artetxe et al., 2017; Conneau et al.,\n2018b).\nSentence Representation Learning\nMany ap-\nproaches have been proposed to extend word em-\nbeddings to sentence or paragraph representa-\ntions (Le and Mikolov, 2014; Wieting et al., 2016;\nArora et al., 2017).\nThe most straightforward\nway to generate sentence embeddings is to con-\nsider an average or weighted average of word\nrepresentations, usually referred to as continu-\nous bag-of-words (CBOW). Although naïve, this\nmethod often provides a strong baseline. More so-\nphisticated approaches—such as the unsupervised\nSkipThought model of Kiros et al. (2015) that\nextends the skip-gram model of Mikolov et al.\n(2013b) to the sentence level—have been pro-\nposed to capture syntactic and semantic depend-\nencies inside sentence representations.\nWhile\nthese ﬁxed-size sentence embedding methods have\nbeen outperformed by their supervised counter-\nparts (Conneau et al., 2017; Subramanian et al.,\n2018), some recent developments have shown that\npretrained language models can also transfer very\nwell, either when the hidden states of the model\nare used as contextualized word vectors (Peters\net al., 2018), or when the full model is ﬁne-tuned\non transfer tasks (Radford et al., 2018; Howard\nand Ruder, 2018).\nMultilingual Sentence Representations\nThere\nhas been some effort on developing multilingual\nsentence embeddings. For example, Chandar et al.\n(2013) train bilingual autoencoders with the objec-\ntive of minimizing reconstruction error between\ntwo languages. Schwenk et al. (2017) and España-\nBonet et al. (2017) jointly train a sequence-to-\nsequence MT system on multiple languages to\nlearn a shared multilingual sentence embedding\nspace. Hermann and Blunsom (2014) propose a\ncompositional vector model involving unigrams\nand bigrams to learn document level representa-\ntions. Pham et al. (2015) directly train embedding\nrepresentations for sentences with no attempt at\ncompositionality. Zhou et al. (2016) learn bilin-\ngual document representations by minimizing the\nEuclidean distance between document representa-\ntions and their translations.\nCross-lingual\nEvaluation\nBenchmarks\nThe\nlack of evaluation benchmark has hindered the\ndevelopment of such multilingual representations.\nMost previous approaches use the Reuters cross-\nlingual document classiﬁcation corpus Klemen-\ntiev et al. (2012) for evaluation. However, the clas-\nsiﬁcation in this corpus is done at document level,\nand, as there are many ways to aggregate sen-\ntence embeddings, the comparison between differ-\nent sentence embeddings is difﬁcult. Moreover,\nthe distribution of classes in the Reuters corpus is\nhighly unbalanced, and the dataset does not pro-\nvide a development set in the target language, fur-\nther complicating experimental comparisons.\nIn addition to the Reuters corpus, Cer et al.\n(2017) propose sentence-level multilingual train-\ning and evaluation datasets for semantic textual\nsimilarity in four languages. There have also been\nefforts to build multilingual RTE datasets, either\nthrough translating English data (Mehdad et al.,\n2011), or annotating sentences from a parallel cor-\npora (Negri et al., 2011). More recently, Agi´c and\nSchluter (2018) provide a corpus, that is very com-\nplementary to our work, of human translations for\n1332 pairs of the SNLI data into Arabic, French,\nRussian, and Spanish.\nAmong all these bench-\nmarks, XNLI is the ﬁrst large-scale corpus for\nevaluating sentence-level representations on that\nmany languages.\nIn practice, cross-lingual sentence understand-\ning goes beyond translation. For instance, Mo-\nhammad et al. (2016) analyze the differences in\nhuman sentiment annotations of Arabic sentences\nand their English translations, and conclude that\nmost of them come from cultural differences. Sim-\nilarly, Smith et al. (2016) show that most of the\ndegradation in performance when applying a clas-\nsiﬁcation model trained in English to Spanish data\ntranslated to English is due to cultural differences.\nOne of the limitations of the XNLI corpus is that\nit does not capture these differences, since it was\nobtained by translation. We see the XNLI evalua-\ntion as a necessary step for multilingual NLP be-\nfore tackling the even more complex problem of\ndomain-adaptation that occurs when handling this\nthe change in style from one language to another.\n3\nThe XNLI Corpus\nBecause the test portion of the Multi-Genre NLI\ndata was kept private, the Cross-lingual NLI Cor-\npus (XNLI) is based on new English NLI data. To\ncollect the core English portion, we follow pre-\ncisely the same crowdsourcing-based procedure\nused for the existing Multi-Genre NLI corpus, and\ncollect and validate 750 new examples from each\nof the ten text sources used in that corpus for a to-\ntal of 7500 examples. With that portion in place,\nwe create the full XNLI corpus by employing pro-\nfessional translators to translate it into our ten tar-\nget languages. This section describes this process\nand the resulting corpus.\nTranslating, rather than generating new hypoth-\nesis sentences in each language separately, has\nmultiple advantages. First, it ensures that the data\ndistributions are maximally similar across lan-\nguages. As speakers of different languages may\nhave slightly different intuitions about how to ﬁll\nin the supplied prompt, this allows us to avoid\nadding this unwanted degree of freedom. Second,\nit allows us to use the same trusted pool of work-\ners as was used prior NLI crowdsourcing efforts,\nwithout the need for training a new pool of work-\ners in each language. Third, for any premise, this\nprocess allows us to have a corresponding hypoth-\nesis in any language. XNLI can thus potentially\nbe used to evaluate whether an Arabic or Urdu\npremise is entailed with a Bulgarian or French hy-\npothesis etc. This results in more than 1.5M com-\nbinations of hypothesis and premises. Note that\nwe do not consider that use case in this work.\nThis translation approach carries with it the risk\nthat the semantic relations between the two sen-\ntences in each pair might not be reliably preserved\nin translation, as Mohammad et al. (2016) ob-\nserved for sentiment. We investigate this poten-\ntial issue in our corpus and ﬁnd that, while it does\noccur, it only concerns a negligible number of sen-\ntences (see Section 3.2).\n3.1\nData Collection\nThe English Corpus\nOur collection procedure\nfor the English portion of the XNLI corpus fol-\nlows the same procedure as the MultiNLI corpus.\nWe sample 250 sentences from each of the ten\nsources that were used in that corpus, ensuring that\nnone of those selected sentences overlap with the\ndistributed corpus. Nine of the ten text sources\nare drawn from the second release of the Open\nAmerican National Corpus1: Face-To-Face, Tele-\nphone, Government, 9/11, Letters, Oxford Uni-\nversity Press (OUP), Slate, Verbatim, and Gov-\nernment. The tenth, Fiction, is drawn from the\nnovel Captain Blood (Sabatini, 1922). We refer\nthe reader to Williams et al. (2017) for more de-\ntails on each genre.\nGiven these sentences,\nwe ask the same\nMultiNLI worker pool from a crowdsourcing\nplatform to produce three hypotheses for each\npremise, one for each possible label.\nWe present premise sentences to workers using\nthe same templates as were used in MultiNLI. We\nalso follow that work in pursuing a second valida-\ntion phase of data collection in which each pair of\nsentences is relabeled by four other workers. For\neach validated sentence pair, we assign a gold la-\nbel representing a majority vote between the initial\nlabel assigned to the pair by the original annotator,\nand the four additional labels assigned by valida-\ntion annotators. We obtained a three-vote consen-\n1http://www.anc.org/\nsus for 93% of the data. In our experiments, we\nkept the 7% additional ones, but we mark these\nones with a special label ’-’.\nTranslating the Corpus\nFinally, we hire trans-\nlators to translate the resulting sentences into 15\nlanguages using the One Hour Translation plat-\nform. We translate the premises and hypotheses\nseparately, to ensure that no context is added to\nthe hypothesis that was not there originally, and\nsimply copy the labels from the English source\ntext. Some development examples are shown in\nTable 1.\n3.2\nThe Resulting Corpus\nOne main concern in studying the resulting corpus\nis to determine whether the gold label for some of\nthe sentence pairs changes as a result of informa-\ntion added or removed in the translation process.\nInvestigating the data manually, we ﬁnd an ex-\nample in the Chinese translation where an entail-\nment relation becomes a contradictory relation,\nwhile the entailment is preserved in other lan-\nguages. Speciﬁcally, the term upright which was\nused in English as entailment of standing, was\ntranslated into Chinese as sitting upright thus cre-\nating a contradiction. However, the difﬁculty of\nﬁnding such an example in the data suggests its\nrarity.\nTo quantify this observation, we recruit two\nbilingual annotators to re-annotate 100 examples\neach in both English and French following our\nstandard validation procedure. The examples are\ndrawn from two non-overlapping random subsets\nof the development data to prevent the annota-\ntors from seeing the source English text for any\ntranslated text they annotate. With no training or\nburn-in period, these annotators recover the En-\nglish consensus label 85% of the time on the orig-\ninal English data and 83% of the time on the trans-\nlated French, suggesting that the overall semantic\nrelationship between the two languages has been\npreserved. As most sentences are relatively easy\nto translate, in particular the hypotheses generated\nby the workers, there seems to be little ambiguity\nadded by the translator.\nMore broadly, we ﬁnd that the resulting corpus\nhas similar properties to the MultiNLI corpus. For\nall languages, on average, the premises are twice\nas long as the hypotheses (See Table 2). The top\nhypothesis words indicative of the class label –\nscored using the mutual information between each\nen\nfr\nes\nde\nel\nbg\nru\ntr\nar\nvi\nth\nzh\nhi\nsw\nur\nPremise\n21.7 24.1 22.1 21.1 21.0 20.9 19.6 16.8 20.7 27.6 22.1 21.8 23.2 18.7 24.1\nHypothesis\n10.7 12.4 10.9 10.8 10.6 10.4\n9.7\n8.4\n10.2 13.5 10.4 10.8 11.9\n9.0\n12.3\nTable 2: Average number of tokens per sentence in the XNLI corpus for each language.\nword and class in the corpus – are similar across\nlanguages, and overlap those of the MultiNLI cor-\npus (Gururangan et al., 2018).\nFor example, a\ntranslation of at least one of the words no, not or\nnever is among the top two cues for contradiction\nin all languages.\nAs in the original MultiNLI corpus, we ex-\npect that cues like these (‘artifacts’, in Guru-\nrangan’s terms, also observed by\nPoliak et al.,\n2018; Tsuchiya, 2018) allow a baseline system to\nachieve better-than-random accuracy with access\nonly to the premise sentences. We accept this as an\nunavoidable property of the NLI task over natural-\nistic sentence pairs, and see no reason to expect\nthat this baseline would achieve better accuracy\nthan the relatively poor 53% seen in Gururangan\net al. (2018).\nThe current version of the corpus is freely avail-\nable23 for typical machine learning uses, and may\nbe modiﬁed and redistributed. The majority of the\ncorpus sentences are released under the OANC’s\nlicense which allows all content to be freely used,\nmodiﬁed, and shared under permissive terms. The\ndata in the Fiction genre from Captain Blood are\nin the public domain in the United States (but may\nbe licensed differently elsewhere).\n4\nCross-Lingual NLI\nIn this section we present results with XLU sys-\ntems that can serve as baselines for future work.\n4.1\nTranslation-Based Approaches\nThe most straightforward techniques for XLU rely\non translation systems. There are two natural ways\nto use a translation system: TRANSLATE TRAIN,\nwhere the training data is translated into each tar-\nget language to provide data to train each clas-\nsiﬁer, and TRANSLATE TEST, where a transla-\ntion system is used at test time to translate in-\nput sentences to the training language.\nThese\ntwo methods provide strong baselines, but both\n2https://s3.amazonaws.com/xnli/XNLI-1.0.zip\n3https://s3.amazonaws.com/xnli/XNLI-MT-1.0.zip\npresent practical challenges. The former requires\ntraining and maintaining as many classiﬁers as\nthere are languages, while the latter relies on\ncomputationally-intensive translation at test time.\nBoth approaches are limited by the quality of the\ntranslation system, which itself varies with the\nquantity of available training data and the similar-\nity of the language pair involved.\n4.2\nMultilingual Sentence Encoders\nAn alternative to translation is to rely on language-\nuniversal embeddings of text and build multilin-\ngual classiﬁers on top of these representations. If\nan encoder produces an embedding of an English\nsentence close to the embedding of its translation\nin another language, then a classiﬁer learned on\ntop of English sentence embeddings will be able to\nclassify sentences from different languages with-\nout needing a translation system at inference time.\nWe evaluate two types of cross-lingual sen-\ntence encoders: (i) pretrained universal multilin-\ngual sentence embeddings based on the average\nof word embeddings (X-CBOW), (ii) bidirectional-\nLSTM (Hochreiter and Schmidhuber, 1997) sen-\ntence encoders trained on the MultiNLI training\ndata (X-BILSTM).\nThe former evaluates trans-\nfer learning while the latter evaluates NLI-speciﬁc\nencoders trained on in-domain data.\nBoth ap-\nproaches use the same alignment loss for align-\ning sentence embedding spaces from multiple lan-\nguages which is present below.\nWe consider\ntwo ways of extracting feature vectors from the\nBiLSTM: either using the initial and ﬁnal hid-\nden states (Sutskever et al., 2014), or using the\nelement-wise max over all states (Collobert and\nWeston, 2008).\nThe ﬁrst approach is commonly used as a strong\nbaseline for monolingual sentence embeddings\n(Arora et al., 2017; Conneau and Kiela, 2018;\nGouews et al., 2014).\nConcretely, we consider\nthe English fastText word embedding space as be-\ning ﬁxed, and ﬁne-tune embeddings in other lan-\nguages so that the average of the word vectors\nin a sentence is close to the average of the word\nvectors in its English translation. The second ap-\nEnglish encoder\nEnglish encoder\nA) Learning NLI English encoder and classiﬁer\nB) Aligning sentence encoders with parallel data\nC) Inference in the other language\nClassiﬁer\nEntailment\n\"You don’t have\nto stay there.\"\n\"You can leave.\"\nEnglish encoder\nSpanish encoder\nEnglish parallel\nsentence\nSpanish parallel\nsentence\nEnglish contrastive sentence vector\nSpanish contrastive sentence vector\n:\n:\nSpanish encoder\nSpanish encoder\nClassiﬁer\nContradiction\n\"Y eso te hace\nsentir fatal.\"\n\"Te hace sentir \nestupendamente.\"\nFigure 1: Illustration of language adaptation by sentence embedding alignment. A) The English\nencoder and classiﬁer in blue are learned on English (in-domain) NLI data. The encoder can also be\npretrained (transfer learning). B) The Spanish encoder in gray is trained to mimic the English encoder\nusing parallel data. C) After alignment of the encoders, the classiﬁer can make predictions for Spanish.\nproach consists in learning an English sentence en-\ncoder on the MultiNLI training data along with\nan encoder on the target language, with the ob-\njective that the representations of two translations\nare nearby in the embedding space. In both ap-\nproaches, an English encoder is ﬁxed, and we train\ntarget language encoders to match the output of\nthis encoder. This allows us to build sentence rep-\nresentations that belong to the same space. Joint\ntraining of encoders and parameter sharing are\nalso promising directions to improve and simplify\nthe alignment of sentence embedding spaces. We\nleave this for future work.\nIn all experiments, we consider encoders that\noutput a vector of ﬁxed size as a sentence repre-\nsentation. While previous work shows that perfor-\nmance on the NLI task can be improved by using\ncross-sentence attention between the premise and\nhypothesis (Rocktäschel et al., 2016; Gong et al.,\n2018), we focus on methods with ﬁxed-size sen-\ntence embeddings.\n4.2.1\nAligning Word Embeddings\nMultilingual word embeddings are an efﬁcient\nway to transfer knowledge from one language\nto another.\nFor instance, Zhang et al. (2016)\nshow that cross-lingual embeddings can be used\nto extend an English part-of-speech tagger to the\ncross-lingual setting, and Xiao and Guo (2014)\nachieve similar results in dependency parsing.\nCross-lingual embeddings also provide an ef-\nﬁcient mechanism to bootstrap neural machine\ntranslation (NMT) systems for low-resource lan-\nguage pairs, which is critical in the case of un-\nsupervised machine translation (Lample et al.,\n2018a; Artetxe et al., 2018; Lample et al., 2018b).\nIn that case, the use cross-lingual embeddings di-\nrectly helps the alignment of sentence-level en-\ncoders.\nCross-lingual embeddings can be gen-\nerated efﬁciently using a very small amount of\nsupervision. By using a small parallel dictionary\nwith n = 5000 word pairs, it is possible to learn a\nlinear mapping to minimize\nW ⋆= argmin\nW∈Od(R)\n∥WX −Y ∥F = UV T ,\nwhere d is the dimension of the embeddings, and\nX and Y are two matrices of shape (d, n) that cor-\nrespond to the aligned word embeddings that ap-\npear in the parallel dictionary, Od(R) is the group\nof orthogonal matrices of dimension d, and U and\nV are obtained from the singular value decompo-\nsition (SVD) of Y XT : UΣV T = SVD(Y XT ).\nXing et al. (2015) show that enforcing the ortho-\ngonality constraint on the linear mapping leads to\nbetter results on the word translation task.\nIn this paper, we pretrain our embeddings using\nthe common-crawl word embeddings (Grave et al.,\n2018) aligned with the MUSE library of Conneau\net al. (2018b).\n4.2.2\nUniversal Multilingual Sentence\nEmbeddings\nMost of the successful recent approaches for learn-\ning universal sentence representations have re-\nlied on English (Kiros et al., 2015; Arora et al.,\n2017; Conneau et al., 2017; Subramanian et al.,\n2018; Cer et al., 2018). While notable recent ap-\nproaches have considered building a shared sen-\ntence encoder for multiple languages using pub-\nlicly available parallel corpora (Johnson et al.,\nfr\nes\nde\nel\nbg\nru\ntr\nar\nvi\nth\nzh\nhi\nsw\nur\nXX-En BLEU\n41.2 45.8 39.3 42.1 38.7 27.1 29.9 35.2 23.6 22.6 24.6 27.3 21.3 24.4\nEn-XX BLEU\n49.3 48.5 38.8 42.4 34.2 24.9 21.9 15.8 39.9 21.4 23.2 37.5 24.6 24.1\nWord translation P@1\n73.7 73.9 65.9 61.1 61.9 60.6 55.0 51.9 35.8 25.4 48.6 48.2\n-\n-\nTable 3: BLEU scores of our translation models (XX-En) P@1 for multilingual word embeddings.\n2016; Schwenk et al., 2017; España-Bonet et al.,\n2017), the lack of a large-scale, sentence-level se-\nmantic evaluation has limited their adoption by the\ncommunity. In particular, these methods do not\ncover the scale of languages considered in XNLI,\nand are limited to high-resource languages. As a\nbaseline for the evaluation of pretrained multilin-\ngual sentence representations in the 15 languages\nof XNLI, we consider state-of-the-art common-\ncrawl embeddings with a CBOW encoder.\nOur\napproach, dubbed X-CBOW, consists in ﬁxing the\nEnglish pretrained word embeddings, and ﬁne-\ntuning the target (e.g., French) word embeddings\nso that the CBOW representations of two transla-\ntions are close in embedding space. In that case,\nwe consider our multilingual sentence embeddings\nas being pretrained and only learn a classiﬁer on\ntop of them to evaluate their quality, similar to so-\ncalled “transfer” tasks in (Kiros et al., 2015; Con-\nneau et al., 2017) but in the multilingual setting.\n4.2.3\nAligning Sentence Embeddings\nTraining for similarity of source and target sen-\ntences in an embedding space is conceptually and\ncomputationally simpler than generating a trans-\nlation in the target language from a source sen-\ntence. We propose a method for training for cross-\nlingual similarity and evaluate approaches based\non the simpler task of aligning sentence represen-\ntations. Under our objective, the embeddings of\ntwo parallel sentences need not be identical, but\nonly close enough in the embedding space that the\ndecision boundary of the English classiﬁer cap-\ntures the similarity.\nWe propose a simple alignment loss function to\nalign the embedding spaces of two different lan-\nguages. Speciﬁcally, we train an English encoder\non NLI, and train a target encoder by minimizing\nthe loss:\nLalign(x, y) = dist(x, y) −λ(dist(xc, y) + dist(x, yc))\nwhere (x, y) corresponds to the source and\ntarget sentence embeddings, (xc, yc) is a con-\ntrastive term (i.e. negative sampling), λ controls\nthe weight of the negative examples in the loss.\nFor the distance measure, we use the L2 norm\ndist(x, y) = ∥x −y∥2. A ranking loss (Weston\net al., 2011) of the form\nLrank(x, y) = max(0, α −dist(x, yc) + dist(x, y)) +\nmax(0, α −dist(xc, y) + dist(x, y))\nthat pushes the sentence embeddings of a trans-\nlation pair to be closer than the ones of negative\npairs leads to very poor results in this particular\ncase. As opposed to Lalign, Lrank does not force the\nembeddings of sentence pairs to be close enough\nso that the shared classiﬁer can understand that\nthese sentences have the same meaning.\nWe use Lalign in the cross-lingual embed-\ndings baselines X-CBOW, X-BILSTM-LAST and\nX-BILSTM-MAX.\nFor X-CBOW, the encoder is\npretrained and not ﬁne-tuned on NLI (transfer-\nlearning), while the English X-BiLSTMs are\ntrained on the MultiNLI training set (in-domain).\nFor the three methods, the English encoder and\nclassiﬁer are then ﬁxed. Each of the 14 other lan-\nguages have their own encoders with same archi-\ntecture. These encoders are trained to \"copy\" the\nEnglish encoder using the Lalign loss and the paral-\nlel data described in section 5.2. Our sentence em-\nbedding alignment approach is illustrated in Fig-\nure 1.\nWe only back-propagate through the target en-\ncoder when optimizing Lalign such that all 14 en-\ncoders live in the same English embedding space.\nIn these experiments, we initialize lookup tables\nof the LSTMs with pretrained cross-lingual em-\nbeddings discussed in Section 4.2.1.\n5\nExperiments and Results\n5.1\nTraining details\nWe use internal translation systems to translate\ndata between English and the 10 other languages.\nFor TRANSLATE TEST (see Table 4), we translate\neach test set into English, while for the TRANS-\nLATE TRAIN, we translate the English training\nen\nfr\nes\nde\nel\nbg\nru\ntr\nar\nvi\nth\nzh\nhi\nsw\nur\nMachine translation baselines (TRANSLATE TRAIN)\nBiLSTM-last\n71.0\n66.7\n67.0\n65.7\n65.3\n65.6\n65.1\n61.9\n63.9\n63.1\n61.3\n65.7\n61.3\n55.2\n55.2\nBiLSTM-max\n73.7\n68.3\n68.8\n66.5\n66.4\n67.4\n66.5\n64.5\n65.8\n66.0\n62.8\n67.0\n62.1\n58.2\n56.6\nMachine translation baselines (TRANSLATE TEST)\nBiLSTM-last\n71.0\n68.3\n68.7\n66.9\n67.3\n68.1\n66.2\n64.9\n65.8\n64.3\n63.2\n66.5\n61.8\n60.1\n58.1\nBiLSTM-max\n73.7\n70.4\n70.7\n68.7\n69.1\n70.4\n67.8\n66.3\n66.8\n66.5\n64.4\n68.3\n64.2\n61.8\n59.3\nEvaluation of XNLI multilingual sentence encoders (in-domain)\nX-BiLSTM-last\n71.0\n65.2\n67.8\n66.6\n66.3\n65.7\n63.7\n64.2\n62.7\n65.6\n62.7\n63.7\n62.8\n54.1\n56.4\nX-BiLSTM-max\n73.7\n67.7\n68.7\n67.7\n68.9\n67.9\n65.4\n64.2\n64.8\n66.4\n64.1\n65.8\n64.1\n55.7\n58.4\nEvaluation of pretrained multilingual sentence encoders (transfer learning)\nX-CBOW\n64.5\n60.3\n60.7\n61.0\n60.5\n60.4\n57.8\n58.7\n57.5\n58.8\n56.9\n58.8\n56.3\n50.4\n52.2\nTable 4: Cross-lingual natural language inference (XNLI) test accuracy for the 15 languages.\ndata of MultiNLI4. To give an idea of the trans-\nlation quality, we give BLEU scores of the auto-\nmatic translation from the foreign language into\nEnglish of the XNLI test set in Table 3. We use\nthe MOSES tokenizer for most languages, falling\nback on the default English tokenizer when neces-\nsary. We use the Stanford segmenter for Chinese\n(Chang et al., 2008), and the pythainlp package for\nThai.\nWe use pretrained 300D aligned word embed-\ndings for both X-CBOW and X-BILSTM and only\nconsider the most 500,000 frequent words in the\ndictionary, which generally covers more than 98%\nof the words found in XNLI corpora.\nWe set\nthe number of hidden units of the BiLSTMs to\n512, and use the Adam optimizer (Kingma and\nBa, 2014) with default parameters. As in (Con-\nneau et al., 2017), the classiﬁer receives a vector\n[u, v, |u −v|, u ∗v], where u and v are the em-\nbeddings of the premise and hypothesis provided\nby the shared encoder, and ∗corresponds to the\nelement-wise multiplication (see Figure 1). For\nthe alignment loss, setting λ to 0.25 worked best\nin our experiments, and we found that the trade-\noff between the importance of the positive and the\nnegative pairs was particularly important (see Ta-\nble 5). We sample negatives randomly. When ﬁt-\nting the target BiLSTM encoder to the English en-\ncoder, we ﬁne-tune the lookup table associated to\nthe target encoder, but keep the source word em-\nbeddings ﬁxed. The classiﬁer is a feed-forward\nneural network with one hidden layer of 128 hid-\nden units, regularized with dropout (Srivastava\net al., 2014) at a rate of 0.1. For X-BiLSTMs,\n4To allow replication of results, we share the MT transla-\ntions of XNLI training and test sets.\nwe perform model selection on the XNLI valida-\ntion set in each target language. For X-CBOW, we\nkeep a validation set of parallel sentences to eval-\nuate our alignment loss. The alignment loss re-\nquires a parallel dataset of sentences for each pair\nof languages, which we describe next.\n5.2\nParallel Datasets\nWe use publicly available parallel datasets to learn\nthe alignment between English and target en-\ncoders. For French, Spanish, Russian, Arabic and\nChinese, we use the United Nation corpora (Ziem-\nski et al., 2016), for German, Greek and Bulgar-\nian, the Europarl corpora (Koehn, 2005), for Turk-\nish, Vietnamese and Thai, the OpenSubtitles 2018\ncorpus (Tiedemann, 2012), and for Hindi, the IIT\nBombay corpus (Anoop et al., 2018). For all the\nabove language pairs, we were able to gather more\nthan 500,000 parallel sentences, and we set the\nmaximum number of parallel sentences to 2 mil-\nlion. For the lower-resource languages Urdu and\nSwahili, the number of parallel sentences is an or-\nder of magnitude smaller than for the other lan-\nguages we consider. For Urdu, we used the Bible\nand Quran transcriptions (Tiedemann, 2012), the\nOpenSubtitles 2016 (Pierre and Jörg, 2016) and\n2018 corpora and LDC2010T21, LDC2010T23\nLDC corpora, and obtained a total of 64k paral-\nlel sentences. For Swahili, we were only able to\ngather 42k sentences using the Global Voices cor-\npus and the Tanzil Quran transcription corpus5.\n5.3\nAnalysis\nComparing in-language performance in Table 4,\nwe observe that, when using BiLSTMs, results are\n5http://opus.nlpl.eu/\nFigure 2: Evolution along training of alignment\nlosses and X-BILSTM XNLI French (fr), Arabic\n(ar) and Urdu (ur) accuracies. Observe the cor-\nrelation between Lalign and accuracy.\nconsistently better when we take the dimension-\nwise maximum over all hidden states (BiLSTM-\nmax) compared to taking the last hidden state\n(BiLSTM-last).\nUnsuprisingly, BiLSTM results\nare better than the pretrained CBOW approach for\nall languages. As in Bowman et al. (2015), we\nalso observe the superiority of BiLSTM encoders\nover CBOW, even when ﬁne-tuning the word em-\nbeddings of the latter on the MultiNLI training\nset, thereby again conﬁrming that the NLI task\nrequires more than just word information. Both\nof these ﬁndings conﬁrm previously published re-\nsults (Conneau et al., 2017).\nTable 4 shows that translation offers a strong\nbaseline for XLU. Within translation,\nTRANS-\nLATE TEST appears to perform consistently better\nthan TRANSLATE TRAIN for all languages. The\nbest cross-lingual results in our evaluation are ob-\ntained by the TRANSLATE TEST approach for all\ncross-lingual directions.\nWithin the translation\napproaches, as expected, we observe that cross-\nlingual performance depends on the quality of\nthe translation system. In fact, translation-based\nresults are very well-correlated with the BLEU\nscores for the translation systems; XNLI perfor-\nmance for three of the four languages with the best\ntranslation systems (comparing absolute BLEU,\nTable 3) is above 70%. This performance is still\nabout three points below the English NLI perfor-\nmance of 73.7%. This slight drop in performance\nmay be related to translation error, changes in\nstyle, or artifacts introduced by the machine trans-\nlation systems that result in discrepancies between\nthe training and test data.\nFor cross-lingual performance, we observe a\nhealthy gap between the English results and the\nresults obtained on other languages. For instance,\nfor French, we obtain 67.7% accuracy when clas-\nsifying French pairs using our English classiﬁer\nand multilingual sentence encoder. When using\nour alignment process, our method is competitive\nwith the TRANSLATE TRAIN baseline, suggesting\nthat it might be possible to encode similarity be-\ntween languages directly in the embedding spaces\ngenerated by the encoders. However, these meth-\nods are still below the other machine translation\nbaseline TRANSLATE TEST, which signiﬁcantly\noutperforms the multilingual sentence encoder ap-\nproach by up to 6% (Swahili). These production\nsystems have been trained on much larger train-\ning data than the ones used for the alignment loss\n(section 5.2), which can partly explain the supe-\nriority of this method over the baseline. At infer-\nence time, the multilingual sentence encoder ap-\nproach is however much cheaper than the TRANS-\nLATE TEST baseline, and this method also does\nnot require any machine translation system. In-\nterestingly, the two points difference in accuracy\nbetween X-BiLSTM-last and X-BiLSTM-max is\nmaintained across languages, which suggests that\nhaving a stronger encoder in English also posi-\ntively impacts the transfer results on other lan-\nguages.\nfr\nru\nzh\nft = 1, λ = 0.25 [default]\n68.9\n66.4\n67.9\nft = 1, λ = 0.0 (no negatives)\n67.8\n66.2\n66.3\nft = 1, λ = 0.5\n64.5\n61.3\n63.7\nft = 0, λ = 0.25\n68.5\n66.3\n67.7\nTable 5: Validation accuracy using BiLSTM-\nmax. Default setting corresponds to λ = 0.25\n(importance of the negative terms) and uses ﬁne-\ntuning of the target lookup table (ft =1).\nFor X-BILSTM French, Urdu and Arabic en-\ncoders, we plot in Figure 2 the evolution of XNLI\ndev accuracies and the alignment losses during\ntraining.\nThe latter are computed using XNLI\nparallel dev sentences. We observe a strong cor-\nrelation between the alignment losses and XNLI\naccuracies. As the alignment on English-Arabic\ngets better for example, so does the accuracy on\nXNLI-ar. One way to understand this is to recall\nthat the English classiﬁer takes as input the vector\n[u, v, |u −v|, u ∗v] where u and v are the embed-\ndings of the premise and hypothesis. So this cor-\nrelation between Lalign and the accuracy suggests\nthat, as English and Arabic embeddings [uen, ven]\nand [uar, var] get closer for parallel sentences (in\nthe sense of the L2-norm), the English classiﬁer\ngets better at understanding Arabic embeddings\n[uar, var, |uar −var|, uar ∗var] and thus the accu-\nracy improves. We observe some over-ﬁtting for\nUrdu, which can be explained by the small num-\nber of parallel sentences (64k) available for that\nlanguage.\nIn Table 5, we report the validation accuracy\nusing BiLSTM-max on three languages with dif-\nferent training hyper-parameters. Fine-tuning the\nembeddings does not signiﬁcantly impact the re-\nsults, suggesting that the LSTM alone is ensuring\nalignment of parallel sentence embeddings. We\nalso observe that the negative term is not critical\nto the performance of the model, but can lead to\nslight improvement in Chinese (up to 1.6%).\n6\nConclusion\nA typical problem in industrial applications is\nthe lack of supervised data for languages other\nthan English, and particularly for low-resource\nlanguages.\nSince annotating data in every lan-\nguage is not a realistic approach, there has been\na growing interest in cross-lingual understanding\nand low-resource transfer in multilingual scenar-\nios. In this work, we extend the development and\ntest sets of the Multi-Genre Natural Language In-\nference Corpus to 15 languages, including low-\nresource languages such as Swahili and Urdu. Our\ndataset, dubbed XNLI, is designed to address the\nlack of standardized evaluation protocols in cross-\nlingual understanding, and will hopefully help the\ncommunity make further strides in this area. We\npresent several approaches based on cross-lingual\nsentence encoders and machine translation sys-\ntems.\nWhile machine translation baselines ob-\ntained the best results in our experiments, these ap-\nproaches rely on computationally-intensive trans-\nlation models either at training or at test time. We\nfound that cross-lingual encoder baselines provide\nan encouraging and efﬁcient alternative, and that\nfurther work is required to match the performance\nof translation based methods.\nAcknowledgments\nThis project has beneﬁted from ﬁnancial support\nto Samuel R. Bowman by Google, Tencent Hold-\nings, and Samsung Research.\nReferences\nŽeljko Agi´c and Natalie Schluter. 2018. Baselines and\ntest data for cross-lingual inference. LREC.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A Smith.\n2016.\nMassively multilingual word embeddings.\narXiv preprint arXiv:1602.01925.\nKunchukuttan\nAnoop,\nMehta\nPratik,\nand\nBhat-\ntacharyya Pushpak. 2018. The iit bombay english-\nhindi parallel corpus. In LREC.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. ICLR.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In ACL, pages 451–462.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018.\nUnsupervised neural ma-\nchine translation. In ICLR.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018.\nUniversal sentence encoder.\narXiv\npreprint arXiv:1803.11175.\nSarath Chandar, Mitesh M. Khapra, Balaraman Ravin-\ndran, Vikas Raykar, and Amrita Saha. 2013. Multi-\nlingual deep learning. In NIPS, Workshop Track.\nPi-Chuan Chang, Michel Galley, and Christopher D\nManning. 2008.\nOptimizing chinese word seg-\nmentation for machine translation performance. In\nProceedings of the third workshop on statistical\nmachine translation, pages 224–232.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In ICML,\npages 160–167. ACM.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. LREC.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017.\nSupervised\nlearning of universal sentence representations from\nnatural language inference data. In EMNLP, pages\n670–680.\nAlexis Conneau,\nGerman Kruszewski,\nGuillaume\nLample, Loïc Barrault, and Marco Baroni. 2018a.\nWhat you can cram into a single vector: Probing\nsentence embeddings for linguistic properties.\nIn\nACL.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato,\nLudovic Denoyer,\nand Hervé Jegou.\n2018b. Word translation without parallel data. In\nICLR.\nCristina España-Bonet, Ádám Csaba Varga, Alberto\nBarrón-Cedeño, and Josef van Genabith. 2017. An\nempirical analysis of nmt-derived interlingual em-\nbeddings and their use in parallel sentence identiﬁ-\ncation. IEEE Journal of Selected Topics in Signal\nProcessing, pages 1340–1348.\nManaal Faruqui and Chris Dyer. 2014. Improving vec-\ntor space word representations using monolingual\ncorrelation. In EACL.\nYichen Gong, Heng Luo, and Jian Zhang. 2018. Natu-\nral language inference over interaction space. ICLR.\nS. Gouews, Y. Bengio, and G. Corrado. 2014. Bilbowa:\nFast bilingual distributed representations without\nword alignments. In ICML.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. LREC.\nSuchin Gururangan,\nSwabha Swayamdipta,\nOmer\nLevy, Roy Schwartz, Samuel R Bowman, and\nNoah A Smith. 2018. Annotation artifacts in natural\nlanguage inference data. NAACL.\nKarl Moritz Hermann and Phil Blunsom. 2014. Multi-\nlingual models for compositional distributed seman-\ntics. In ACL, pages 58–68.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory.\nNeural computation,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Fine-tuned\nlanguage models for text classiﬁcation. In ACL.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\net al. 2016. Google’s multilingual neural machine\ntranslation system: enabling zero-shot translation.\narXiv preprint arXiv:1611.04558.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In ICLR.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nNIPS, pages 3294–3302.\nA. Klementiev, I. Titov, and B. Bhattarai. 2012. In-\nducing crosslingual distributed representations of\nwords. In COLING.\nT.\nKociský,\nK.M.\nHermann,\nand\nP.\nBlunsom.\n2014. Learning bilingual word representations by\nmarginalizing alignments. In ACL, pages 224–229.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In MT summit, vol-\nume 5, pages 79–86.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a.\nUnsupervised\nmachine translation using monolingual corpora only.\nIn ICLR.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In EMNLP.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In ICML,\npages 1188–1196.\nYashar Mehdad, Matteo Negri, and Marcello Federico.\n2011.\nUsing bilingual parallel corpora for cross-\nlingual textual entailment.\nIn ACL, pages 1336–\n1345.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In NIPS, pages 3111–3119.\nSaif M. Mohammad, Mohammad Salameh, and Svet-\nlana Kiritchenko. 2016. How translation alters sen-\ntiment. J. Artif. Int. Res., 55(1):95–130.\nMatteo Negri, Luisa Bentivogli, Yashar Mehdad,\nDanilo Giampiccolo, and Alessandro Marchetti.\n2011. Divide and conquer: Crowdsourcing the cre-\nation of cross-lingual textual entailment corpora. In\nEMNLP, pages 670–679.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL, volume 1, pages 2227–2237.\nHieu Pham, Minh-Thang Luong, and Christopher D.\nManning. 2015.\nLearning distributed representa-\ntions for multilingual text sequences. In Workshop\non Vector Space Modeling for NLP.\nLison Pierre and Tiedemann Jörg. 2016. Pierre lison\nand jörg tiedemann, 2016, opensubtitles2016: Ex-\ntracting large parallel corpora from movie and tv\nsubtitles. In LREC.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. In NAACL.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018.\nImproving language under-\nstanding by generative pre-training.\nTim Rocktäschel, Edward Grefenstette, Karl Moritz\nHermann, Tomáš Koˇcisk`y, and Phil Blunsom. 2016.\nReasoning about entailment with neural attention.\nICLR.\nRafael Sabatini. 1922. Captain Blood. Houghton Mif-\nﬂin Company.\nHolger Schwenk and Xian Li. 2018. A corpus for mul-\ntilingual document classiﬁcation in eight languages.\nIn LREC, pages 3548–3551.\nHolger Schwenk, Ke Tran, Orhan Firat, and Matthijs\nDouze. 2017. Learning joint multilingual sentence\nrepresentations with neural machine translation. In\nACL workshop, Repl4NLP.\nLaura Smith, Salvatore Giorgi, Rishi Solanki, Jo-\nhannes\nC.\nEichstaedt,\nH.\nAndrew\nSchwartz,\nMuhammad Abdul-Mageed, Anneke Buffone, and\nLyle H. Ungar. 2016. Does ’well-being’ translate on\ntwitter? In EMNLP, pages 2042–2047.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958.\nSandeep Subramanian, Adam Trischler, Yoshua Ben-\ngio, and Christopher J Pal. 2018.\nLearning gen-\neral purpose distributed sentence representations via\nlarge scale multi-task learning. In ICLR.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In NIPS, pages 3104–3112.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In LREC, Istanbul, Turkey. European\nLanguage Resources Association (ELRA).\nMasatoshi Tsuchiya. 2018.\nPerformance impact\ncaused by hidden bias of training data for recogniz-\ning textual entailment. In LREC.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nJason Weston, Samy Bengio, and Nicolas Usunier.\n2011. Wsabie: Scaling up to large vocabulary im-\nage annotation. In IJCAI.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016. Towards universal paraphrastic sen-\ntence embeddings. ICLR.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2017.\nA broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn\nNAACL.\nMin Xiao and Yuhong Guo. 2014. Distributed word\nrepresentation learning for cross-lingual dependency\nparsing. In CoNLL, pages 119–129.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. NAACL.\nYuan Zhang, David Gaddy, Regina Barzilay, and\nTommi Jaakkola. 2016.\nTen pairs to tag–\nmultilingual pos tagging via coarse mapping be-\ntween embeddings. In NAACL, pages 1307–1317.\nXinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016.\nCross-lingual sentiment classiﬁcation with bilingual\ndocument representation learning. In ACL.\nMichal Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel corpus\nv1. 0. In LREC.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2018-09-13",
  "updated": "2018-09-13"
}