{
  "id": "http://arxiv.org/abs/1809.06064v1",
  "title": "Object-sensitive Deep Reinforcement Learning",
  "authors": [
    "Yuezhang Li",
    "Katia Sycara",
    "Rahul Iyer"
  ],
  "abstract": "Deep reinforcement learning has become popular over recent years, showing\nsuperiority on different visual-input tasks such as playing Atari games and\nrobot navigation. Although objects are important image elements, few work\nconsiders enhancing deep reinforcement learning with object characteristics. In\nthis paper, we propose a novel method that can incorporate object recognition\nprocessing to deep reinforcement learning models. This approach can be adapted\nto any existing deep reinforcement learning frameworks. State-of-the-art\nresults are shown in experiments on Atari games. We also propose a new approach\ncalled \"object saliency maps\" to visually explain the actions made by deep\nreinforcement learning agents.",
  "text": "Object-sensitive Deep Reinforcement Learning\nYuezhang Li, Katia Sycara, and Rahul Iyer\nCarnegie Mellon University, Pittsburgh, PA, USA\nyuezhanl@andrew.cmu.edu, katia@cs.cmu.edu, rahuli@andrew.cmu.edu\nAbstract\nDeep reinforcement learning has become popular over recent years, showing superiority\non diﬀerent visual-input tasks such as playing Atari games and robot navigation. Although\nobjects are important image elements, few work considers enhancing deep reinforcement\nlearning with object characteristics. In this paper, we propose a novel method that can\nincorporate object recognition processing to deep reinforcement learning models.\nThis\napproach can be adapted to any existing deep reinforcement learning frameworks. State-of-\nthe-art results are shown in experiments on Atari games. We also propose a new approach\ncalled “object saliency maps” to visually explain the actions made by deep reinforcement\nlearning agents.\n1\nIntroduction\nDeep neural networks have been widely applied in reinforcement learning (RL) algorithms to\nachieve human-level control in various challenging domains. More speciﬁcally, recent work has\nfound outstanding performances of deep reinforcement learning (DRL) models on Atari 2600\ngames, by using only raw pixels to make decisions [21].\nThe literature on reinforcement learning is vast. Multiple deep RL algorithms have been\ndeveloped to incorporate both on-policy RL such as Sarsa [30], actor-critic methods [1], etc.\nand oﬀ-policy RL such as Q-learning using experience replay memory [21] [25].\nA parallel\nRL paradigm [20] has also been proposed to reduce the heavy reliance of deep RL algorithms\non specialized hardware or distributed architectures.\nHowever, while a high proportion of\nRL applications such as Atari 2600 games contain objects with diﬀerent gain or penalty (for\nexample, enemy ships and fuel vessel are two diﬀerent objects in the game “Riverraid”), most\nof previous algorithms are designed under the assumption that various game objects are treated\nequally.\nIn this paper, we propose a new Object-sensitive Deep Reinforcement Learning (O-\nDRL) model that can exploit object characteristics such as presence and positions of game\nobjects in the learning phase. This new model can be adapted to most of existing deep RL\nframeworks such as DQN [21] and A3C [20]. Our experiments show that our method outper-\nforms the state-of-the-art methods by 1% - 20% in various Atari games.\nMoreover, current deep RL models are not explainable, i.e., they cannot produce human\nunderstandable explanations. When a deep RL agent comes up with an action, people cannot\nunderstand why it picks the action. Therefore, there is a clear need for deep RL agents to\ndynamically and automatically oﬀer explanations that users can understand and act upon.\nThis capability will make autonomy more trustworthy, useful, and transparent.\nIncorporating object recognition processing is very important for providing explanations.\nFor example, in the Atari game Ms. Pacman, the player controls Pac-Man through a maze,\neating beans and avoiding monster enemies.\nThere are also ﬂashing dots known as power\npellets that provide Pac-Man with the temporary ability to eat the enemies. By identifying the\ndiﬀerent objects (Pac-Man itself, beans, power pellets, enemies, walls, etc.), the deep RL agent\ncan gain the potential to explain the actions like human beings.\narXiv:1809.06064v1  [cs.LG]  17 Sep 2018\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nIn this paper, we develop a new method called object saliency maps to automatically\nproduce object-level visual explanations that explain why an action was taken. The proposed\nmethod can be incorporated with any existing deep RL model to give human understandable\nexplanation of why the model choose a certain action.\nOur contributions are threefold: First, we propose a method to incorporate object charac-\nteristics to the learning process of deep reinforcement learning. Second, we propose a method\nto produce object-level visual explanation for deep RL models.\nThird, our experiments show\nimprovements over existing methods.\n2\nRelated Work\n2.1\nDeep Reinforcement Learning\nReinforcement learning is deﬁned as learning a policy for an agent to interact with the unknown\nenvironment. The rich representations given by deep neural network improves the eﬃciency of\nreinforcement learning (RL). A variety of works thus investigate the application of deep learning\non RL and propose a concept of deep reinforcement learning. Mnih et al. [21] proposed a deep Q-\nnetwork (DQN) that combines Q-learning with a ﬂexible deep neural network. DQN can reach\nhuman-level performance on many of Atari 2600 games but suﬀers substantial overestimation\nin some games [34].\nThus, a Double DQN (DDQN) was proposed by Hasselt et al.\n[34]\nto reduce overestimation by decoupling the target max operation into action selection and\naction evaluation.\nIn the meantime, Wang et al.\nproposed a dueling network architecture\n(DuelingDQN) [35] that decouples the state-action values into state values and action values to\nyield better approximation of the state value.\nRecent experiments of [20] show that the actor-critic (A3C) method surpasses the current\nstate-of-the-art in the Atari game domain. Comparing to Q-learning, A3C is a policy-based\nmodel that learns a network action policy. However, for some games with many objects where\ndiﬀerent objects have diﬀerent rewards, A3C does not perform very well. Therefore, Lample et\nal. [14] proposed a method that augments performance of reinforcement learning by exploiting\ngame features. Accordingly, we propose a method of incorporating object features into current\ndeep reinforcement learning models.\n2.2\nExplainable Models\nThere are some recent work on explaining the prediction result of black-box models. Erhan et\nal. [5] visualised deep models by ﬁnding an input image which maximises the neuron activity of\ninterest by carrying out an optimisation using gradient ascent in the image space. It was later\nemployed by [15] to visualise the class models, captured by a deep unsupervised auto-encoder.\nZeiler et al. [37] proposed the Deconvolutional Network (DeconvNet) architecture, which aims to\napproximately reconstruct the input of each layer from its output, to ﬁnd evidence of predicting\na class. Recently, Simonyan et al. [28] proposed saliency maps to deduce the spatial support\nof a particular class in a given image based on the derivative of class score with respect to the\ninput image. Ribeiro et al. [24] propose a method to explains the prediction of any classiﬁer\nby local exploration, and apply it on image and text classiﬁcation. All these models work at\npixel level, and cannot explain the prediction at object level.\n2\nObject-sensitive DRL\nYuezhang, Rahul and Katia\n2.3\nObject recognition\nObject recognition aims to ﬁnd and identify objects in an image or video sequence, where\nobjects may vary in sizes and scales when translated or rotated. As a challenging task in the\nﬁeld of computer vision, Object Recognition (OR) has seen many approaches implemented over\ndecades.\nSigniﬁcant progress on this problem has been observed due to the introduction of low-level\nimage features, such as Scale Invariant Feature Transformation (SIFT) [17] and Histogram of\nOriented Gradient (HOG) descriptors [4], in sophisticated machine learning frameworks such\nas polynomial SVM [22] and its combination with Gaussian ﬁlters in a dynamic programming\nframework [26].\nRecent development has also witnessed the successful application of selec-\ntive search [33] on recognizing various objects. While HOG/SIFT representation can capture\nedge or gradient structure with easily controllable degree of invariance to local geometric and\nphotometric transformations, it is generally acknowledged that progress slowed from 2010 on-\nward, with small gains obtained by building ensemble systems and employing minor variants\nof successful methods [19].\nThe burst of deep neural network over the past several years has dramatically improved\nobject recognition capabilities such as convolutional neural network (CNN) [16] and other vari-\nants [13], where the recent trend is to increase the number of layers and layer size [27], while\nusing dropout [9] to address the problem of overﬁtting. Exploration of inception layers leads\nto a 22-layer deep model in the case of the GoogLeNet model [32]. Furthermore, the Regions\nwith Convolutional Neural Networks (R-CNN) method [7] decomposes the overall recognition\nproblem into two sub-problems and achieves the current state of the art performance. Re-\ncently, R-CNN has been optimized to reduce detection time and formulated as “fast R-CNN”\n[6], together with its various extensions [11] [8] [29].\n3\nBackground\n3.1\nReinforcement Learning Background\nReinforcement learning tries to solve the sequential decision problems by learning from the\nhistory. Considering the standard RL setting where an agent interacts with an environment ε\nover discrete time steps. In the time step t, the agent receives a state st ∈S and selects an\naction at ∈A according to its policy π, where S and A denote the sets of all possible states\nand actions respectively. After the action, the agent observes a scalar reward rt and receives\nthe next state st+1.\nFor example, in the Atari games domain, the agent receives an image input (consisting of\npixels) as current state at time t, and chooses an action from the possible controls (Press the\nup/down/left/right/A/B button). After that, the agent receives a reward (how much the score\ngoes up or down) and the next image input.\nThe goal of the agent is to choose actions to maximize its rewards over time. In other words,\nthe action selection implicitly considers the future rewards. The discounted return is deﬁned as\nRt = P∞\nτ=t γτ−trτ where γ ∈[0, 1] is a discount factor that trades-oﬀthe importance of recent\nand future rewards.\nFor a stochastic policy π, the value of an action at and the value of the states are deﬁned\nas follows.\nQπ(st, at) = E[Rt|s = st, a = at, π]\n(1)\nV π(st) = Ea∼π(st)[Qπ(st, at)]\n(2)\n3\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nThe action value function (a.k.a., Q-function) can be computed recursively with dynamic\nprogramming:\nQπ(st, at) = Est+1[rt + γEat+1∼π(st+1)[Qπ(st+1, at+1)]]\n(3)\nIn value-based RL methods, the action value (a.k.a., Q-value) is commonly estimated by a\nfunction approximator, such as a deep neural network in DQN [21]. In DQN, let Q(s, a; θ) be\nthe approximator parameterized by θ. The parameter θ are learned by iteratively minimizing\na sequence of loss functions, where the ith loss function is deﬁned as:\nLi(θi) = E(rt + γmax\nat+1 Q(st+1, at+1; θi) −Q(st, at; θi))2\n(4)\nIn contrast to value-based methods, policy-based methods directly model the policy π(a|s; θ)\nand update the parameters θ. For example, standard REINFORCE algorithm [36] updates the\npolicy parameters θ in the direction ▽θ log π(at|st; θ).\nActor-critic [31] architecture is a combination of value-based and policy-based methods.\nA learned value function V π(st) is considered as the baseline (the critic), and the quantity\nAπ(at, st) = Qπ(at, st) −V π(st) is deﬁned as the advantage of the action at in state st, which\nis used to scale the policy gradient (the actor). Asynchronous deep neural network version of\nthe actor-critic method (A3C) [20] gains state of the art performance in both Atari games and\nsome other challenging domains.\nWe will enrich these models with ways to exploit object characteristics such as the presence\nand positions of objects in the training phase. Our envisioned architecture is shown in Figure. 1\nand will be described in Section. 4.\n3.2\nObject Recognition Background\nObject recognition is an essential research direction in computer vision. The common tech-\nniques include gradients, edges, linear binary patterns and Histogram of Oriented Gradients\n(HOG). Based on these techniques, a variety of models are developed, including template match-\ning, Viola-Jones algorithm and image segmentation with blob analysis [18]. Considering our goal\nis to investigate whether object features can enhance the performance of deep reinforcement\nlearning algorithms for Atari games, we use template matching to extract objects due to its\nimplementation simplicity and good performance.\nTemplate matching, as a high-level computer vision technique, is used to locate a template\nimage in a larger image. It requires two components – source image and template image [3].\nThe source image is the one we expect to ﬁnd matches to the template image while the template\nimages is the patch image that is comparable to the template image. To identify the matching\narea, we slide through the source image with a patch (up to down, left to right) and calculate\nthe patch’s similarity to the template image. In this paper, we use OpenCV [10] implement\nthe template matching. Speciﬁcally, we tried three diﬀerent similarity measures methods. In\nthe following equations, T and I denote two images while x′ and y′ are variable shift along\nx-direction and y-direction respectively. The w and h respectively represent width and height\nof the template image.\n• Square diﬀerence matching method\nSquared diﬀerence is a method that measures the pixel intensity diﬀerences between two\nimages [23]. It computes the summation of squared product of two images’ pixels subtrac-\ntion. Generally, this similarity measure directly uses the formulation of sum of squared\n4\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nerror. It is chosen when speed matters.\nR(x, y) =\nX\nx′,y′\n[T(x′, y′) −I(x + x′, y + y′)]2\n(5)\n• Correlation matching method\nThe correlation matching method decides the matching point between source image and\ntemplate image through searching the location with maximum value in the image matrices.\nIt is chosen as similarity measure due to robustness [12]. However, it is computationally\nexpensive compared with square diﬀerence matching method.\nR(x, y) =\nX\nx′,y′\n[T(x′, y′) · I(x + x′, y + y′)]2\n(6)\n• Correlation coeﬃcient matching method\nDue to the limitation on speed of correlation matching method, correlation coeﬃcient\nwas proposed by [12] that performs transformation on both T and I. The experimental\nresults show it perform very well and is suitable for practical applications.\nR(x, y) =\nX\nx′,y′\n[T(x′, y′) · I′(x + x′, y + y′)]2\n(7)\nT ′(x′, y′) = T ′(x′, y′) −1/(w · h) ·\nX\nx′′,y′′\nT(x′′, y′′)\n(8)\nI′(x + x′, y + y′) = I(x + x′, y + y′) −1/(w · h) ·\nX\nx′′,y′′\nI(x + x′′, y + y′′)\n(9)\nConsidering the eﬀect of changing intensity and template size, it is commonly applied the\nnormalized version of the three methods above.\nSince accuracy is important for our task,\nwe adopt normalized correlation coeﬃcient matching method in template matching, which\npossesses the robustness of correlation matching method and time eﬃciency.\n4\nOur Methods\n4.1\nObject-sensitive Deep Reinforcement Learning Model\nWe propose an Object-sensitive Deep Reinforcement Learning (O-DRL) model that\nutilizes object characteristics (e.g., existence of a certain object and its position, etc.) in visual\ninputs to enhance the feature representation of deep reinforcement learning agents. The learned\ndeep network would be more sensitive to the objects that appear in the input images, enabling\nthe agent diﬀerentiates between “good objects” and “bad objects”. Moreover, incorporating\nobject recognition is very important for providing visual explanations, which would be further\nintroduced in the Section. 4.2.\nThe key idea of the O-DRL model is to properly incorporate object features extracted from\nvisual inputs to deep neural networks. In this paper, we propose to use object channels as a\nway to incorporate object features.\nThe object channels are deﬁned as follows: suppose we have k objects detected in an\nimage, we add k additional channels to the original RGB channels of the original image. Each\nchannel represents a single type of object. In each channel, for the pixels belong to the detected\n5\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nFigure 1: An example of neural network architecture for Object-sensitive Deep Q-network (O-\nDQN). Here, we get the screen image as input and pass it to the object recognizer to extract\nobject channels. Then, the combined channels are given as input to the convolutional neural\nnetworks to predict Q-values.\nobject, we assign value 1 in the corresponding position, and 0 otherwise. Through this, we\nsuccessfully encode locations and categorical diﬀerence of various objects in an image. Take\nObject-sensitive Deep Q-network (O-DQN) as an example, the network architecture is\nshown in Figure 1. Here, we get the screen image as input and pass it to the object recognizer\nto extract object channels. We also use a convolutional neural network (CNN) to extract\nimage features just same as in DQN. Both object channels and the original state image are\npassed through the network to predict Q-values for each action. Note that this method can\nbe adapted to diﬀerent existing deep reinforcement learning frames, such as Object-sensitive\nDouble Q-Network (O-DDQN) and Object-sensitive Advanced Actor-critic model\n(O-A3C). In our experiments, all the object-sensitive DRL methods perform better than their\nnon-object counterparts.\n4.2\nObject Saliency Maps\nWe propose a new method to produce object saliency maps for generating visual explanation\nof decisions made by deep RL agents. Before the introduction of object saliency, we ﬁrst\nintroduce pixel saliency [28]. This technique is ﬁrst introduced to explain why a CNN classiﬁes\nan image to a certain category. In order to generate explanation of why an DRL agent choose\na certain action, we are interested in which pixels the model pays attention to when making\na decision. For each state s, the model conduct action a where a = argmaxa′∈AQ(s, a′). We\nwould like to rank the pixels of s based on their inﬂuence on Q(s, a). Since the Q-values are\napproximated by a deep neural networks, the Q-value function Q(s, a) is a highly non-linear\nfunction of s. However, given a state s0, we can approximate Q(s0, a) with a linear function in\nthe neighborhood of s0 by computing the ﬁrst-order Taylor expansion:\nQ(s, a) ≈wT s + b,\n(10)\nwhere w is the derivative of Q(s, a) with respect to the state image s at the point (state) s0\nand form the pixel saliency map:\nw = ∂Q(s, a)\n∂s\n|s0\n(11)\n6\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nAnother interpretation of computing pixel saliency is that value of the derivative indicates which\npixels need to be changed the least to aﬀect the Q-value.\nHowever, pixel-level representations are not obvious for people to understand. See Figure 2\nfor example. Figure 2a is a screen image from the game Ms.Pacman. Figure 2b is the cor-\nresponding pixel saliency map produced by an agent trained with the Double DQN(DDQN)\nmodel. The agent chooses to go right in this situation. Although we can get some intuition of\nwhich area the deep RL agent is looking at to make the decision, it is not clear what objects\nthe agent is looking at and why it chooses to move right in this situation.\nTo generate human understandable explanation, we need to focus on object level like humans\ndo. Therefore, we need to rank the objects in a state s based on their inﬂuence on Q(s, a).\nHowever, it is nontrivial to design the derivative of Q(s, a) with respect to the object area.\nHence, we apply the following approach: for each object O found in s, we mask the object with\nbackground color to form a new state so as if the object does not appear in this new state. We\ncalculate the Q-values for both states, and the diﬀerence of the Q-values actually represents\nthe inﬂuence of this object on Q(s, a).\nw = Q(s, a) −Q(so, a)\n(12)\nWe can also derive that positive w actually represents “good” object which means the object\ngives positive future reward to the agent. And negative w represents “bad” object since after\nwe remove the object, the Q-value get improved.\nFigure 2c shows an example of the object saliency map. While the pixel saliency map only\nexplain a vague region where the model is looking at, the object saliency map can reveal which\nobjects the model are looking at, and depicts the relative importance of each object. We can\nsee that the object saliency map is more clear and meaningful than the pixel saliency map.\n(a) Original State\n(b) Pixel Saliency Map\n(c) Object Saliency Map\nFigure 2: An example of original state, corresponding pixel saliency map and object saliency\nmap produced by a double DQN agent in the game “Ms. Pacman.”\nThe computational cost of computing object saliency map is proportional to the number of\nobjects we detected. If we have k objects, the computational cost is 2k forward pass calculation\nof the model, which is aﬀordable since k is generally not too large, and one forward pass is fast\nin model testing time.\n7\nObject-sensitive DRL\nYuezhang, Rahul and Katia\n5\nExperiments\n5.1\nBaseline Methods\nWe implemented deep-Q networks (DQN)[21], double deep-Q networks (DDQN) [34], dueling\ndeep-Q networks (Dueling)[35] and advanced actor-critic model (A3C)[20] described in Sec-\ntion 3.1 as baselines. We also implemented their object-sensitive counterparts by incorporating\nobject channels described in Section 4.1.\n5.2\nExperiment Settings\nWe use OpenAI gym[2] platform to perfoming our experiments. We choose 5 Atari 2600 games\nwith distinguishable objects to test our models against baselines. They are Freeway, Riverraid,\nSpaceInvaders, BankHeist and Ms.Pacman.\nWe use the same network architecture for these DRL and O-DRL methods, shown in Fig-\nure 1. The design is a little diﬀerent from the original work of DQN [21] because of better\nperformance achieved. There are four convolutional layers with 3 max pooling layers followed\nby 2 fully-connected layers. The ﬁrst convolutional layer has 32 5 ∗5 ﬁlters with stride 1, fol-\nlowed by a 2∗2 max pooling layer. The second convolutional layer has 32 5∗5 ﬁlters with stride\n1, followed by a 2 ∗2 max pooling layer. The third convolutional layer has 64 4 ∗4 ﬁlters with\nstride 1, followed by a 2 ∗2 max pooling layer. The fourth and ﬁnal convolutional layer has 64\n3 ∗3 ﬁlters with stride 1. The ﬁrst full-connected layer has 512 hidden units. The ﬁnal layer is\nthe output layer, which diﬀers in diﬀerent models. In DQN, the dimension of the output layer\nis the number of actions. In A3C, two separate output layers are produced: a policy output\nlayer with the dimension of the number of actions, a value output layer that contains only one\nunit.\nWe use 4 history frames to represent current state as described in [21]. For object repre-\nsentation, we use the last frame to extract object channels. In order to make objects distinct\nfrom each other, we do not use the reward clip strategy as described in [21]. Instead, we use\nthe normalized rewards corresponding to the maximum reward received in the game. This is\nbecause the reward clip strategy assigns +1 for all rewards that are larger than 1 and -1 for all\nrewards that are smaller then -1, which makes diﬀerent objects hard to distinguish.\n5.3\nExperiment Results\n5.3.1\nObject Recognition Results\nIn order to verify the eﬀectiveness of O-DRL methods, we need to verify the eﬀectiveness of\nobject recognition processing ﬁrst. We adopt template matching with correlation coeﬃcient\nmatching method described in Section 3.2. We manually create template images of objects\nin diﬀerent games. For evaluation, we randomly select 100 game images from each of the game\nand label the objects in these images.\nWe use precision, recall and F1 score as our evaluation metrics for object recognition. Let\nTP, FP, FN denote the number of true-positive, false-positive and false-negative predictions\nfor all the labels, Precision =\nT P\nT P +F P , Recall =\nT P\nT P +F N , F1 = 2 P recision·Recall\nP recision+Recall. The number\nof object types in each game, as well as the precision, recall and F1 scores are reported in\nTable. 1\nWe can see from the table that the precision of our object recognition is always 1, indicating\nthe eﬀectiveness of the template matching method. The F1 scores are also higher than 0.9. This\n8\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nFreeway\nRiverraid\nSpaceInvaders\nBankHeist\nMs.Pacman\n# Object types\n2\n4\n3\n3\n6\nPrecision\n1\n1\n1\n1\n1\nRecall\n0.96\n0.88\n0.94\n0.96\n0.92\nF1\n0.98\n0.93\n0.97\n0.98\n0.96\nTable 1: Number of object types, precision, recall and F1 scores for 5 Atari 2600 games.\nindicates that the extraction of object channels is accurate and can be applied to object-sensitve\nDRL models.\n5.3.2\nObject-senstive DRL Results\nWe compare diﬀerent DRL models with O-DRL models for 5 games. Each model is trained for 10\nmillion frames. The average scores over 50 plays are presented in Table. 2. By comparing DRL\nmodels with their object-sensitive counterparts, we can see that object-sensitive DRL models\nperform better then DRL models in most of the games. All the best-performing models in each\nof the game are object-sensitive. We can also observe that O-DRL models only outperform\nDRL models by a small margin (1%) in the game Freeway, but by a large margin (20%) in\nthe game Ms.Pacman. This may because that the game Ms.Pacman contain much more object\ntypes then the game Freeway, making object-sensitive models more eﬀective.\nDQN\nO-DQN\nDDQN\nO-DDQN\nDueling\nO-Dueling\nA3C\nO-A3C\nFreeway\n26.9\n27.1\n29.3\n29.6\n21.5\n21.8\n30.3\n30.7\nRiverraid\n5418\n5823\n10324\n10613\n16983\n18752\n12312\n13215\nSpaceInvaders\n1537\n1528\n2567\n2585\n5893\n5923\n23242\n24027\nBankHeist\n183\n193\n987\n978\n1056\n1098\n956\n1023\nMs.Pacman\n1503\n1593\n2029\n2453\n1614\n1827\n617\n829\nTable 2: Average scores over 50 plays of DRL and O-DRL models trained for 10 million frames\non 5 Atari 2600 games.\n5.4\nCase Study - Ms.Pacman\nWe conduct a more detailed case study of the game Ms.Pacman to show the eﬀectiveness of\nO-DRL models as well as the object saliency maps for explaining DRL models. We choose\nMs.Pacman because it has most types of objects and it is hard to solve by DRL models. In\nthis game, the player controls Pac-Man in a maze, collecting dots and avoiding ghosts. The\nactions contains left, right, down, up, leftup, leftdown, rightup, rightdown and nowhere, where\n“nowhere” represents do not press any button and continue the previous action.\nFigure. 3 shows performance of diﬀerent models during training. We can see that after\nabout 2 million training frames, all the models converge to relative stable scores. We can also\nsee that O-DRL models outperform their DRL counterparts by a non-trivial margin during the\ntraining phase.\n9\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nFigure 3: Average scores played by diﬀerent DRL and O-DRL models in the game Ms.Pacman\nduring the training process. The x-axis is the number of frames each model has been trianed\nfor, y-axis is the average scores over 50 plays.\nWe also compare the object saliency maps and decisions made by the DRL model and the\nO-DRL model. We compare the DDQN model with the O-DDQN model in this case because\nthey perform best in this game. We randomly sample 1000 states in a game play from human,\nand compare the decisions made by both models. We also produce object saliency maps for\neach model in order to see the which objects each model attend to when making decisions.\nAmong 1000 samples, 98% of the decisions given by both models are the same. Therefore,\nwe look into the remaining 2% of the samples.\nFigure. 4 shows an example of diﬀerent actions taken by both models and the object saliency\nmaps produced by both models. In this state, Pac-Man is on the right of the ghost. In order\nto run away from the ghost, the human’s choice is to go right. However, the DDQN model\nchooses to go left. We can see from Fig. 4b that the model does not focus on the ghost but\nthe bean on the left of the ghost. Therefore, it chooses to go left without noticing of the ghost.\nThe O-DDQN model successfully recognizes the ghost and choose to go right. We can also see\nthis from Fig. 4c.\nMore detailed examples for how object saliency maps can help explaining the decisions made\nby each model can be found in the Appendix. A. These prove that the object saliency maps\ncan be used to visually explain why a model choose a certain action.\n10\nObject-sensitive DRL\nYuezhang, Rahul and Katia\n(a) Current State\n(b) DDQN object saliency map (c) O-DDQN object saliency map\nFigure 4: From left to right are the current state, the object saliency map produced by the\nDDQN model and the object saliency map produced by the O-DDQN model.\nThe DDQN\nmodel chooses to go left while the O-DDQN model chooses to go right in this situation.\n6\nConclusion and Future Work\nIn this paper, we proved that by incorporating object features, we can improve the performance\nof deep reinforcement learning models by a non-trivial margin. We also proposed object saliency\nmaps for visually explaining the actions taken by deep reinforcement learning agents.\nOne interesting future direction is how to use object saliency maps to produce more human\nreadable explanations like natural language explanations, for example, to automatically produce\nnatural language explanations like “I choose to go right to avoid the ghost”.\nAnother direction is to test the ability of object features in a more realistic situation. For\nexample, how to incorporate object features to improve the performance of auto-driving cars.\n7\nAcknowledgement\nThis research was supported by awards W911NF-13-1-0416 and FA9550-15-1-0442.\nSpecial\nthanks to Tian Tian and Jing Chen for insightful discussion and writing help.\n11\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nReferences\n[1] MTRAG Barto. J. 4 supervised actor-critic reinforcement learning. Handbook of learning and\napproximate dynamic programming, 2:359, 2004.\n[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[3] Roberto Brunelli. Template Matching Techniques in Computer Vision: Theory and Practice. Wiley\nPublishing, 2009.\n[4] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In 2005\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05),\nvolume 1, pages 886–893. IEEE, 2005.\n[5] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer\nfeatures of a deep network. University of Montreal, 1341:3, 2009.\n[6] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer\nVision, pages 1440–1448, 2015.\n[7] Ross Girshick, JeﬀDonahue, Trevor Darrell, and Jitendra Malik.\nRich feature hierarchies for\naccurate object detection and semantic segmentation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 580–587, 2014.\n[8] Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, and Jitendra Malik. Learning rich features from\nrgb-d images for object detection and segmentation. In European Conference on Computer Vision,\npages 345–360. Springer, 2014.\n[9] Geoﬀrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-\nnov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint\narXiv:1207.0580, 2012.\n[10] Itseez. Open source computer vision library. https://github.com/itseez/opencv, 2015.\n[11] Andrej Karpathy, Armand Joulin, and Fei Fei F Li. Deep fragment embeddings for bidirectional\nimage sentence mapping. In Advances in neural information processing systems, pages 1889–1897,\n2014.\n[12] Simon Korman, Daniel Reichman, Gilad Tsur, and Shai Avidan. Fast-match: Fast aﬃne template\nmatching. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pages\n1940–1947. IEEE, 2013.\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In Advances in neural information processing systems, pages 1097–1105,\n2012.\n[14] Guillaume Lample and Devendra Singh Chaplot.\nPlaying fps games with deep reinforcement\nlearning. arXiv preprint arXiv:1609.05521, 2016.\n[15] Quoc V Le. Building high-level features using large scale unsupervised learning. In Acoustics,\nSpeech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8595–\n8598. IEEE, 2013.\n[16] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne\nHubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.\nNeural computation, 1(4):541–551, 1989.\n[17] David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of\ncomputer vision, 60(2):91–110, 2004.\n[18] MATLAB. version 7.10.0 (R2010a). The MathWorks Inc., Natick, Massachusetts, 2010.\n[19] Krystian Mikolajczyk, Cordelia Schmid, and Andrew Zisserman. Human detection based on a\nprobabilistic assembly of robust part detectors.\nIn European Conference on Computer Vision,\npages 69–82. Springer, 2004.\n[20] Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,\n12\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. CoRR, abs/1602.01783, 2016.\n[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529–533, February 2015.\n[22] Anuj Mohan, Constantine Papageorgiou, and Tomaso Poggio. Example-based object detection in\nimages by components. IEEE transactions on pattern analysis and machine intelligence, 23(4):349–\n361, 2001.\n[23] S´ebastien Ourselin, Radu Stefanescu, and Xavier Pennec.\nRobust Registration of Multi-modal\nImages: Towards Real-Time Clinical Applications, pages 140–147. Springer Berlin Heidelberg,\nBerlin, Heidelberg, 2002.\n[24] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the\npredictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, pages 1135–1144. ACM, 2016.\n[25] Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data eﬃcient neural reinforce-\nment learning method. In European Conference on Machine Learning, pages 317–328. Springer,\n2005.\n[26] R´emi Ronfard, Cordelia Schmid, and Bill Triggs. Learning to parse pictures of people. In European\nConference on Computer Vision, pages 700–714. Springer, 2002.\n[27] Pierre Sermanet, David Eigen, Xiang Zhang, Micha¨el Mathieu, Rob Fergus, and Yann LeCun.\nOverfeat: Integrated recognition, localization and detection using convolutional networks. arXiv\npreprint arXiv:1312.6229, 2013.\n[28] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:\nVisualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\n[29] Hyun Oh Song, Ross B Girshick, Stefanie Jegelka, Julien Mairal, Zaid Harchaoui, Trevor Darrell,\net al. On learning to localize objects with minimal supervision. In ICML, pages 1611–1619, 2014.\n[30] Richard S Sutton.\nGeneralization in reinforcement learning: Successful examples using sparse\ncoarse coding. In Advances in neural information processing systems, pages 1038–1044, 1996.\n[31] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT\npress Cambridge, 1998.\n[32] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,\nDumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–9,\n2015.\n[33] Jasper RR Uijlings, Koen EA van de Sande, Theo Gevers, and Arnold WM Smeulders. Selective\nsearch for object recognition. International journal of computer vision, 104(2):154–171, 2013.\n[34] Hado van Hasselt, Arthur Guez, and David Silver.\nDeep reinforcement learning with double\nq-learning. CoRR, abs/1509.06461, 2015.\n[35] Ziyu Wang, Nando de Freitas, and Marc Lanctot. Dueling network architectures for deep rein-\nforcement learning. arXiv preprint arXiv:1511.06581, 2015.\n[36] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3-4):229–256, 1992.\n[37] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In\nEuropean conference on computer vision, pages 818–833. Springer, 2014.\n13\nObject-sensitive DRL\nYuezhang, Rahul and Katia\nA\nExamples of object saliency maps for the Game Ms.Pacman\nFigure. 5 shows another example of diﬀerent actions taken by both models and the object\nsaliency maps produced by both models. In this state, Pac-Man is on the upside of the ghost.\nThe DDQN model chooses to go right. We can see from Fig. 5b that the model focus on\nthe beans on the right of Pac-Man. Therefore, it chooses to go right to eat the beans. The\nO-DDQN model chooses to go up. We can see from Fig. 5c that the model concentrates on\nthe beans on the upside of Pac-Man. Therefore it chooses to go up. Both of the decisions are\nreasonable becuase they both notice the ghost are on the downside of Pac-Man.\n(a) Current State\n(b) DDQN object saliency map (c) O-DDQN object saliency map\nFigure 5: From left to right are the current state, the object saliency map produced by the\nDDQN model and the object saliency map produced by the O-DDQN model.\nThe DDQN\nmodel chooses to go right while the O-DDQN model chooses to go up in this situation.\nFigure. 6 shows another example of diﬀerent actions taken by both models and the object\nsaliency maps produced by both models. In this state, Pac-Man is on the upper-right corner.\nThe DDQN model chooses to go down. We can see from Fig. 6b that the model focus on the\nghosts and beans far from Pac-man. Therefore, it chooses to go down to eat the beans in the\nleft-down side of Pac-Man. The O-DDQN model chooses to go right. We can see from Fig. 6c\nthat the model concentrates on the beans on the right of Pac-Man. Also according to the game\nsetting, if the Pac-Man goes right, it will pass the right margin and appear in the left margin\nof the screen. The model successfully captures this rule and also focuses on the beans on the\nleft margin of the screen.\n14\nObject-sensitive DRL\nYuezhang, Rahul and Katia\n(a) Current State\n(b) DDQN object saliency map (c) O-DDQN object saliency map\nFigure 6: From left to right are the current state, the object saliency map produced by the\nDDQN model and the object saliency map produced by the O-DDQN model.\nThe DDQN\nmodel chooses to go down while the O-DDQN model chooses to go right in this situation.\n15\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2018-09-17",
  "updated": "2018-09-17"
}