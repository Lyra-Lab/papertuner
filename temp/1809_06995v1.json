{
  "id": "http://arxiv.org/abs/1809.06995v1",
  "title": "Interpretable Reinforcement Learning with Ensemble Methods",
  "authors": [
    "Alexander Brown",
    "Marek Petrik"
  ],
  "abstract": "We propose to use boosted regression trees as a way to compute\nhuman-interpretable solutions to reinforcement learning problems. Boosting\ncombines several regression trees to improve their accuracy without\nsignificantly reducing their inherent interpretability. Prior work has focused\nindependently on reinforcement learning and on interpretable machine learning,\nbut there has been little progress in interpretable reinforcement learning. Our\nexperimental results show that boosted regression trees compute solutions that\nare both interpretable and match the quality of leading reinforcement learning\nmethods.",
  "text": "Interpretable Reinforcement Learning with Ensemble Methods\nAlexander Brown and Marek Petrik\nUniversity of New Hampshire, 105 Main St, Durham, NH 03824 USA\nAugust 9, 2018\nAbstract\nWe propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforce-\nment learning problems. Boosting combines several regression trees to improve their accuracy without signiﬁcantly\nreducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on inter-\npretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental\nresults show that boosted regression trees compute solutions that are both interpretable and match the quality of lead-\ning reinforcement learning methods.\n1\nIntroduction\nReinforcement learning continues to break bounds on what we even thought possible, recently with AlphaGo’s tri-\numph over leading Go player Lee Sedol and with the further successes of AlphaGoZero, which surpassed AlphaGo\nlearning only from self-play [14]. While the performance of such systems is impressive and very useful, sometimes\nit is desirable to understand and interpret the actions of a reinforcement learning system, and machine learning sys-\ntems in general. These circumstances are more common in high-pressure applications, such as healthcare, targeted\nadvertising, or ﬁnance [6].\nFor example, researchers at the University of Pittsburgh Medical Center trained a variety of machine learning\nmodels including neural networks and decision trees to predict whether pneumonia patients might develop severe\ncomplications. The neural networks performed the best on their testing data, but upon examination of the rules of the\ndecision trees, the researchers found that the trees recommended sending pneumonia patients who had asthma directly\nhome, despite the fact that asthma makes patients with pneumonia much more likely to suffer complications. Through\nfurther investigation they discovered the rule represented a trend in their data: the hospital had a policy to automatically\nsend pneumonia patients with asthma to intensive care, and because this policy was so effective, those patients almost\nnever developed complications. Without the interpretability from the decision trees, it might have been much harder\nto determine the source of the strange recommendations coming from the neural networks [3].\nInterpretability varies between machine learning methods and is difﬁcult to quantify, but decision trees are gener-\nally accepted as a good way to create an interpretable model [6]. A decision tree represents a sequence of decisions\nresulting in a prediction. Small decision trees are easy to show graphically, which helps people interpret and digest\nthem. Although small decision trees are not particularly powerful learners, several trees can be grouped together in\nan ensemble using techniques such as boosting and bagging in order to create a single more expressive model. No\ndeﬁnitive measure of interpretability has been devised, but for decision trees one could count the number of nodes to\ncompare interpretability between trees. With this measure one could also compare ensembles of trees, and as long as\nthe trees are kept small, an ensemble of a few trees could be as interpretable as a single slightly deeper tree. Thus a\nreasonable ensemble could still be considered interpretable. A more detailed description of decision trees is found in\n[9].\nOf ensemble-building algorithms, gradient boosting is of particular interest to this work. Gradient boosting itera-\ntively builds an ensemble of learners, such as decision trees, in such a way that each new learner attempts to correct\nthe mistakes of its predecessors, optimizing some differentiable loss function.\nWe propose two new ways to compute interpretable solutions to reinforcement learning problems. Firstly, we\nshow, in some benchmark environments, that policy data from a successful reinforcement learning agent (i.e. a neural\nnetwork) can be used to train an ensemble of decision trees via gradient boosting, and that such an ensemble can match\n1\narXiv:1809.06995v1  [cs.LG]  19 Sep 2018\nthe performance of the original agent in the environment. Secondly, we analyze some possible ways of constructing\nan interpretable agent from scratch by doing policy gradient descent with decision trees, similar to gradient boosting.\nThe rest of this work is organized as follows. Section 2 brieﬂy discusses prior and related work. Section 3 intro-\nduces the general reinforcement learning problem and the benchmark environments used. Section 4 describes our use\nof ensemble methods to learn a policy from a reinforcement learning actor. Section 5 explains how we combine policy\ngradient descent and gradient boosting to build interpretable reinforcement learning systems. In section 6 we present\nthe results of our experiments with those two techniques. Then, we discuss the results and propose future research\nsteps in section 7.\n2\nRelated Work\nOur work makes cursory use of neural networks and a reinforcement learning algorithm called SARSA, detailed\ndescriptions of which can be found in [9] and [17] respectively.\nA lot of reinforcement learning has historically been done with neural networks, but there has been work in using\nother tools from supervised learning in reinforcement learning algorithms. The work of Sridharan and Tesauro is\namong the ﬁrst to successfully combine regression trees with Q-learning [16]. The authors of [7] propose two new\nensemble algorithms for use in tree-base batch mode reinforcement learning. The general integration of classiﬁers\nin reinforcement learning algorithms is demonstrated in [10], with the goal of leveraging advances in support vector\nmachines to make reinforcement learning easier to apply “out-of-the-box.” An extensive analysis of classiﬁcation-\nbased reinforcement learning with policy iteration is given by [11], which explores a variant of classiﬁcation in policy\niteration that weights misclassiﬁcations by regret, or the difference between the value of the greedy action and that of\nthe action chosen.\nOur policy gradient boosting is a form of a policy gradient algorithm, which is a class of reinforcement learning\ntechniques which attempt to learn a policy directly, rather than learning any kind of value function, by performing gra-\ndient ascent on a performance measure of the current policy. The general form of this kind of algorithm is described in\n[20], which refers to the class of algorithms as REINFORCE algorithms. These algorithms can be weak by themselves,\nand can oscillate or even diverge when searching for an optimal policy, so they are often used in combination with\na value function or function approximation, forming an ”actor-critic” pair [18]. Our algorithm uses a value function\napproximation, but purely for the purpose of reducing variance, as described in [17].\nAn overview of interpretable machine learning can be found in [19]. Petrik and Luss show that computing an\noptimal interpretable policy can be viewed as a MDP and that this task is NP hard [13].\n3\nProblem Setting\nWe consider reinforcement learning tasks, which typically consist of an agent interacting with some environment in\na series of actions, observations, and rewards, formalized as a Markov Decision Process (MDP). At each time step\nthe agent chooses an action a from a set of legal actions A, which is passed to the environment, which returns a new\nstate s ∈S and reward r to the agent, where S is the set of all states. The agent chooses actions according to some\npolicy π, which is a (potentially stochastic) mapping from S to A. We conduct our experiments in two benchmark\nenvironments: cart pole and mountain car.\nThe cart pole problem [2] has been a staple reinforcement learning benchmark for many years. The idea is simple:\na cart which can move in one dimension carries an upright pole. The cart can be pushed either left or right, and the\ngoal is to balance the pole for as long as possible. It is a good benchmark because it is easy to visualize, and has\na relatively simple solution. The benchmark’s difﬁculty can also easily be scaled upwards indeﬁnitely, by stacking\nadditional poles on the initial one [12]. We deal with a version of the problem which terminates when the pole falls\ntoo far to be recovered, or when 200 timesteps have elapsed.\nThe mountain car environment, as used in [4] to test control with continuous states, involves a car whose task it is\nto drive up a hill from in a valley. The car cannot make it up the hill simply by driving forward from the middle of the\nvalley, but rather must reverse up the other side of the valley ﬁrst to gain momentum. A reward of -1 per time step is\ngiven until the car reaches the goal state, at which point the reward is 0 and the episode terminates. The episode also\nterminates when 200 timesteps have elapsed, if the car has not managed to traverse the hill to the goal.\n2\n4\nSupervised Learning on Reinforcement Data\nIn order to compute an interpretable solution to an MDP, we propose to record policy data from a well-performing\nagent in that MDP, and train an ensemble of CART [5] trees on that data using gradient boosting. This requires both a\nworking solution to the MDP and some simulation of the MDP from which data can be gathered.\nGathering the data for this approach is straightforward: both the states that the working solution encounters and the\nactions it selects are recorded for a number of trials. Then, a new ensemble of trees is trained to predict the actions from\nthe states. For some environments this may be equally straightforward, but for others it require some transformation of\nthe state or action spaces to be more conducive to tree prediction. Finally, the ensemble is evaluated in the simulation,\nand the rewards compared to those of the initial solution.\nPerforming supervised learning on reinforcement data is advantageous because it has the potential to utilize state\nof the art reinforcement learning algorithms and techniques. It is insensitive to the composition of the initial agent, as\nit merely requires a black box from which policy data can be extracted. This lends ﬂexibility to the idea, and hints at\nthe possibility of a wide range of uses.\nThe primary drawback is that it requires the MDP be at least approximately solved for any hope of reasonable\nperformance. This will be addressed in Section 5, which provides a ground-up interpretable solution. It should also be\nnoted that insights from the interpretability of the trees are not guaranteed to apply to the initial solution, as the trees\nmerely studied its behavior. This is unlikely to be a signiﬁcant drawback, however, unless it is desired to speciﬁcally\nunderstand the initial solution, in which case the interpretability of the trees is a reasonable start anyway.\n5\nGradient Boosting for Reinforcement Learning\nApplying gradient boosting to policy iteration has an intuitive appeal. Gradient boosting [8] works by training a new\ntree to compensate for the failures of the previous ensemble, in the direction of the gradient. This seems to ﬁt well into\nbatch policy gradient descent by using the update step to train a new tree according to the gradient of the policy, which\nin turn depends on the policy parameterization [20].\nWe implement a REINFORCE [20] algorithm using a policy parameterized by a softmax of numerical action\npreferences, as in [17]. REINFORCE seems to lend itself to our application; its simplicity and extensibility let us\neasily construct an algorithm with a solid backbone that uses decision trees in the inner loop. The softmax of the ith\nelement in a vecor x of length n is given by\nsoftmax(xi) =\nexp(xi)\nPn\nj=1 exp(xj),\nand the softmax of a vector is the vector that results from applying the softmax to each element. We use an approximate\nvalue function v as a baseline, in an attempt to reduce variance [17]. The general steps to the algorithm are shown in\nAlgorithm 1.\nThis algorithm continues to grow in space and computation time as it runs, by nature of endlessly adding trees. In\nthe case where it needs to train for a long time, boundless growth becomes problematic, so we also examine a version\nwhich is capped at a ﬁxed number of trees. The goal is to take some information from older trees and then discard\nthem, passing the information along to new trees which occupy the freed space. This recycling is achieved by training\nnew trees not only on the data from the most recent batch of episodes, but also on predictions from a tree about to be\nrecycled.\n6\nExperimental Results\nSupervised learning with reinforcement data was quite successful on both benchmark environments. In the cart-pole\nenvironment, the initial policy data was obtained from a neural network, the training of which is shown in Figure 1.\nAs can be seen in the plot, the neural network’s performance was less than perfect, which, although it indicates that\nthe neural network could have been tuned to better suit the environment, serves to show something important about\nthe results. The ensemble performed perfectly in the environment (see Figure 2), lasting the full 200 timesteps each\nepisode, which is noticeably better than the data on which is was trained. We believe this to be a result of overﬁtting\non the part of the neural network, and it is very promising that the ensemble was resistant enough to overﬁtting to\noutperform the neural network.\n3\nAlgorithm 1 REINFORCE with Trees\nInput: approximate state-value function v\nENSEMBLE ←empty set of trees\nwhile training ensemble do\nL ←empty list\nfor all episodes in batch do\nreset environment\nwhile timestep i in episode do\nread state si from environment\np ←empty list\nfor all action a do\npa ←\n1\n|A| where |A| is the number of actions\nfor all trees t in ENSEMBLE do\ncompute weight vector ws according to t\np ←p + ws\np ←softmax(p)\nrandomly select action ai according to action probability vector p\nreward ri ←take ai in environment\nL ←L append (si, ai, reward ri, i)\nfor all s, a, r, i in L do\ncompute discounted future reward Gi\ncompute value of s: v(s)\nreplace current r in L by Gi −v(s) (see [17])\nupdate v\ntrain tree t to predict a from s in L\nENSEMBLE ←ENSEMBLE append t\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n200\n400\n600\n800\n1000\n1200\n1400\nReward\nEpisode\nCart-Pole Neural Network Performance\nFigure 1: Neural Network training in the cart-pole en-\nvironment\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n200\n400\n600\n800\n1000\nReward\nEpisode\nCartpole Trees Performance\nFigure 2: Ensemble trained from the neural network\npolicy in the cart-pole environment\n4\n-200\n-180\n-160\n-140\n-120\n-100\n-80\n-60\n-40\n-20\n0\n0\n5000\n10000\n15000\n20000\nReward\nEpisode\nMountain Car SARSA Performance\nFigure 3: SARSA training in the mountain car envi-\nronment\n-200\n-180\n-160\n-140\n-120\n-100\n-80\n-60\n-40\n-20\n0\n0\n200\n400\n600\n800\n1000\nReward\nEpisode\nMountain Car Trees Performance\nFigure 4: Ensemble trained from the SARSA policy\nin the mountain car environment\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n2000\n4000\n6000\n8000\n10000\nReward\nEpisode\nCart-Pole Gradient Boosting Performance\nFigure 5: Policy gradient boosting in the cart-pole en-\nvironment\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n2000\n4000\n6000\n8000\n10000\nReward\nEpisode\nCart-Pole Ensemble Recycling Performance\nFigure 6: Policy gradient boosting with ensemble re-\ncycling in the cart-pole environment\nIn the mountain car environment the initial policy data was obtained from a trained SARSA agent [17]. The\nensemble did not outperform SARSA, as in the case of cart-pole, but it did match the episodic performance, which\nwas the goal. This is also promising but not truly conclusive, as both environments are quite simple.\nPolicy gradient boosting was successful in the sense that the ensembles did learn how to better behave in the\nenvironment. As can be seen in Figure 5, they learn at a decaying rate approaching the maximum reward per episode\nof 200. The rate at which they learned, however, was much lower than that of the neural network. The neural network\nbegan receiving the maximum reward around episode 400, whereas the ensembles only got close after 8,000 - 10,000\nepisodes.\nEnsemble recycling was less successful, but does appear to learn in Figure 6. Once ensembles begin to be recycled\nafter episode 4,000 the performance starts to dip, and never really recovers. At this stage it does not seem to be a viable\nalternative to the non-recycling version.\n7\nDiscussion and Further Research\nSupervised learning on policy data was very successful, to the extent it was tested. The next step for further research\nis clear: attempt it in more complicated domains. When considering these domains, in particular those with high\ndimensional action spaces, the question of whether it makes sense to try to learn how to behave with an ensemble of\nsmall trees. There has been work to suggest that some such problems may have simple solutions [15], but it may be\nchallenging for trees to learn those solutions from policy data alone.\n5\nPolicy gradient boosting was able to learn how to increase the reward in the cart pole problem, but not to the level\nwe expected. The benchmark is considered to be very easy, yet the ensemble was not able to consistently obtain the\nmaximum reward after thousands of episodes of training. While this result is disappointing, we believe there may\nstill be other feasible ways of incrementally building an interpretable solution to an MDP. Adapting such a solution\nto conserve space as we attempted with ensemble recycling also still seems to be a reasonable idea, especially in\nsituations where some accuracy can be sacriﬁced for lower space requirements.\n8\nConclusions\nIn conclusion, an ensemble of decision trees can be trained to emulate a policy in simple MDPs. The ensemble is\nresistant to overﬁtting and enables humans to better interpret the policy. Further work is necessary to determine if\nthis technique is viable in more complicated domains. An ensemble can also be trained incrementally to perform well\nin an MDP by policy gradient boosting. This has limitations in that the ensemble can grow to unwieldy sizes before\nsatisﬁable performance is achieved, and that the learning process is far slower than modern reinforcement learning\nmethods. Attempting to recycle trees in a simple way in order to limit the size of the ensemble reduces performance\nin the MDP so much that training should be halted before recycling begins for peak reward.\n9\nAcknowledgments\nWe would like to thank the Hamel Center for Undergraduate Research for facilitating the SURF program, and Mr.\nDana Hamel and Brad Larsen for their generosity.\nReferences\n[1]\nDavid Abel et al. “Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains”. In:\narXiv:1603.04119 (2016).\n[2]\nA. G. Barto, R. S. Sutton, and C. W. Anderson. “Neuronlike adaptive elements that can solve difﬁcult learning\ncontrol problems”. In: IEEE transactions on systems, man, and cybernetics SMC-13 (Sept. 1983), pp. 834–846.\n[3]\nAaron Bornstein. “Is Artiﬁcial Intelligence Permanently Inscrutable?” In: Nautilus (2016).\n[4]\nJustin A. Boyan and Andrew W. Moore. “Generalization in Reinforcement Learning: Safely Approximating the\nValue Function”. In: Advances in Neural Information Processing (1995).\n[5]\nLeo Breiman et al. Classiﬁcation and Regression Trees. 1984.\n[6]\nAmit Dhurandhar, Sechan Oh, and Marek Petrik. “Building an Interpretable Recommender via Loss-Preserving\nTransformation”. In: 2016 ICML Workshop on Human Interpretability in Machine Learning (2016).\n[7]\nDamien Ernst, Pierre Geurts, and Louis Wehenkel. “Tree-Based Batch Mode Reinforcement Learning”. In:\nJournal of Machine Learning Research 6 (2005), pp. 503–556.\n[8]\nJerome H. Friedman. “Greedy Function Approximation: A Gradient Boosting Machine”. In: The Annals of\nStatistics 29.5 (2001), pp. 1189–1232.\n[9]\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer, 2009.\n[10]\nMichail G. Lagoudakis and Ronald Parr. “Reinforcement Learning as Classiﬁcation: Leveraging Modern Clas-\nsiﬁers”. In: Proceedings of ICML (2003).\n[11]\nAlessandro Lazaric, Mohammad Ghavamzadeh, and Remi Munos. “Analysis of Classifcation-based Policy It-\neration Algorithms”. In: Journal of Machine Learning Research 16 (2016).\n[12]\nDonald Michie and R.A. Chambers. “Boxes: An Experiment in Adaptive Control”. In: Machine Intelligence 2\n(1968).\n[13]\nMarek Petrik and Ronny Luss. “Interpretable Policies for Dynamic Product Recommendations”. In: Proceed-\nings of the Thirty-Second Conference on Uncertainty in Artiﬁcial Intelligence. UAI’16. Jersey City, New Jersey,\nUSA: AUAI Press, 2016, pp. 607–616. ISBN: 978-0-9966431-1-5.\n6\n[14]\nDavid Silver et al. “Mastering the game of Go without human knowledge”. In: Nature 550 (Oct. 2017).\n[15]\nOzgur Simsek, Simon Algorta, and Amit Kothiyal. “Why Most Decisions Are Easy in Tetris—And Perhaps\nin Other Sequential Decision Problems, As Well”. In: Proceedings of The 33rd International Conference on\nMachine Learning. Ed. by Maria Florina Balcan and Kilian Q. Weinberger. Vol. 48. Proceedings of Machine\nLearning Research. New York, New York, USA: PMLR, 20–22 Jun 2016, pp. 1757–1765.\n[16]\nManu Sridharan and Gerald Tesauro. “Multi-agent Q-learning and regression trees for automated pricing deci-\nsions”. In: Seventeenth International Conference on Machine Learning (2000), pp. 927–934.\n[17]\nRichard S. Sutton and Andrew G. Barto. “Reinforcement Learning: an Introduction”. In: 2nd ed. MIT Press,\n2016. Chap. 13, pp. 265–279.\n[18]\nCsaba Szepesvari. Algorithms for Reinforcement Learning. Morgan and Claypool Publishers, 2009.\n[19]\nAlfredo Velido, Jose D. Martin-Guerro, and Paulo J.G. Lisboa. “Making machine learning models interpretable”.\nIn: Proceedings of the European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and\nMachine Learning (2012).\n[20]\nRonald J. Williams. “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learn-\ning”. In: Machine Learning 8 (1992), pp. 229–256.\n7\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-09-19",
  "updated": "2018-09-19"
}