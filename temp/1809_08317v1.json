{
  "id": "http://arxiv.org/abs/1809.08317v1",
  "title": "Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation",
  "authors": [
    "Jonas Wulff",
    "Michael J. Black"
  ],
  "abstract": "The difficulty of annotating training data is a major obstacle to using CNNs\nfor low-level tasks in video. Synthetic data often does not generalize to real\nvideos, while unsupervised methods require heuristic losses. Proxy tasks can\novercome these issues, and start by training a network for a task for which\nannotation is easier or which can be trained unsupervised. The trained network\nis then fine-tuned for the original task using small amounts of ground truth\ndata. Here, we investigate frame interpolation as a proxy task for optical\nflow. Using real movies, we train a CNN unsupervised for temporal\ninterpolation. Such a network implicitly estimates motion, but cannot handle\nuntextured regions. By fine-tuning on small amounts of ground truth flow, the\nnetwork can learn to fill in homogeneous regions and compute full optical flow\nfields. Using this unsupervised pre-training, our network outperforms similar\narchitectures that were trained supervised using synthetic optical flow.",
  "text": "Temporal Interpolation as an Unsupervised\nPretraining Task for Optical Flow Estimation\nJonas Wulﬀ1,2\nMichael J. Black1\n1 Max-Planck Institute for Intelligent Systems, T¨ubingen, Germany\n2 MIT CSAIL, Cambridge, MA, USA\nwulff@mit.edu, black@tuebingen.mpg.de\nAbstract. The diﬃculty of annotating training data is a major obstacle\nto using CNNs for low-level tasks in video. Synthetic data often does not\ngeneralize to real videos, while unsupervised methods require heuristic\nlosses. Proxy tasks can overcome these issues, and start by training a\nnetwork for a task for which annotation is easier or which can be trained\nunsupervised. The trained network is then ﬁne-tuned for the original task\nusing small amounts of ground truth data. Here, we investigate frame in-\nterpolation as a proxy task for optical ﬂow. Using real movies, we train a\nCNN unsupervised for temporal interpolation. Such a network implicitly\nestimates motion, but cannot handle untextured regions. By ﬁne-tuning\non small amounts of ground truth ﬂow, the network can learn to ﬁll in\nhomogeneous regions and compute full optical ﬂow ﬁelds. Using this un-\nsupervised pre-training, our network outperforms similar architectures\nthat were trained supervised using synthetic optical ﬂow.\n1\nIntroduction\nIn recent years, the successes of deep convolutional neural networks (CNNs) have\nled to a large number of breakthroughs in most ﬁelds of computer vision. The two\nfactors that made this possible are a widespread adoption of massively parallel\nprocessors in the form of GPUs and large amounts of available annotated training\ndata. To learn good and suﬃciently general representations of visual features,\nCNNs require tens of thousands to several hundred million [28] examples of the\nvisual content they are supposed to process, annotated with labels teaching the\nCNNs the desired output for a given visual stimulus.\nFor some tasks such as image classiﬁcation or object detection it is possible\nto generate massive amounts of data by paying human workers to manually\nannotate images. For example, writing a description of an image into a textbox\nor dragging a rectangle around an object are tasks that are easy to understand\nand relatively quick to perform. For other tasks, especially those related to video,\nobtaining ground truth is not as easy. For example, manual annotation of dense\noptical ﬂow, motion segmentation, or tracking of objects requires not just the\nannotation of a huge number of instances (in the ﬁrst two cases one would ideally\nwant one annotation per pixel), but an annotator would also have to step back\nand forth in time to ensure temporal consistency [17]. Since this makes the task\narXiv:1809.08317v1  [cs.CV]  21 Sep 2018\n2\nWulﬀ, J. and Black, M.J.\nFig. 1: Overview of our method. Using large amounts of unlabelled data we train a tem-\nporal interpolation network without explicit supervision. We then ﬁne-tune the network\nfor the task of optical ﬂow using small amounts of ground truth data, outperforming\nsimilar architectures that were trained supervised.\nboth tedious and error-prone, manually annotated data is rarely used for low-\nlevel video analysis.\nAn alternative is to use synthetic data, for example from animated movies [4],\nvideogames [26], or generated procedurally [19]. The problem here is that syn-\nthetic data always lives in a world diﬀerent from our own. Even if the low-level\nimage statistics are a good match to those of the real world, it is an open ques-\ntion whether realistic eﬀects such as physics or human behavior can be learned\nfrom synthetic data.\nA diﬀerent approach to address the issue of small data is to use proxy tasks.\nThe idea is to train a network using data for which labels are easy to acquire, or\nfor which unsupervised or self-supervised learning is possible. Training on such\ndata forces the network to learn a latent representation, and if the proxy task\nis appropriately chosen, this representation transfers to the actual task at hand.\nThe trained network, or parts thereof, can then be ﬁne-tuned to the ﬁnal task\nusing limited amounts of annotated data, making use of the structure learned\nfrom the proxy task. An example proxy task for visual reasoning about images is\ncolorization, since solving the colorization problem requires the network to learn\nabout the semantics of the world [16]. Another example is context prediction [5],\nin which the proxy task is to predict the spatial arrangement of sub-patches of\nobjects, which helps in the ﬁnal task of object detection.\nWhat, then, would be a good proxy task for video analysis? One core prob-\nlem in the analysis of video is to compute temporal correspondences between\nframes. Once correspondences are established, it is possible to reason about the\ntemporal evolution of the scene, track objects, classify actions, and reconstruct\nthe geometry of the world. Recent work by Long et al.\n[18] proposed a way\nto learn about correspondences without supervision. They ﬁrst train a network\nto interpolate between two frames. For each point in the interpolated frame,\nTemporal Interpolation for Optical Flow Pretraining\n3\nthey then backpropagate the derivatives through the network, computing which\npixels in the input images most strongly inﬂuence this point. This, in turn, es-\ntablishes correspondences between the two maxima of these derivatives in the\ninput images. Their work, however, had two main shortcomings. First, using a\ncomplete backpropagation pass to compute each correspondence is computation-\nally expensive. Second, especially in unstructured or blurry regions of the frame,\nthe derivatives are not necessarily well located, since a good (in the photometric\nsense) output pixel can be sampled from a number of wrongly corresponding\nsites in the input images; frame interpolation does not need to get the ﬂow right\nto produce a result with low photometric error. This corresponds to the classical\naperture problem in optical ﬂow, in which the ﬂow is not locally constrained, but\ncontext is needed to resolve ambiguities. Consequently, so far, the interpolation\ntask has not served as an eﬀective proxy for learning ﬂow.\nIn this work, we address these shortcomings and show that, treated properly,\nthe interpolation task can, indeed, support the learning of optical ﬂow. To this\nend, we treat training for optical ﬂow as a two-stage problem. In the ﬁrst stage,\nwe train a network to estimate the center frame from four adjacent, equally\nspaced frames. This forces the network to learn to establish correspondences in\nvisually distinct areas. Unlike previous work, which used only limited datasets\nof a few tens of thousands frames such as KITTI-RAW [8], we use a little under\none million samples from a diverse set of datasets incorporating both driving\nscenarios and several movies and TV series. This trains the network to better\ncope with eﬀects like large displacements and motion and focus blur. Thanks to\nthis varied and large body of training data, our network outperforms specialized\nframe interpolation methods despite not being tailored to this task.\nIn a second stage, we ﬁne-tune the network using a small amount of ground-\ntruth optical ﬂow data from the training sets of KITTI [8] and Sintel [4]. This\nhas three advantages. First, after ﬁne-tuning, the network outputs optical ﬂow\ndirectly, which makes it much more eﬃcient than [18]. Second, this ﬁne-tuning\nforces the network to group untextured regions and to consider the context when\nestimating the motion; as mentioned above, this can usually not be learned from\nphotometric errors alone. Third, compared to fully unsupervised optical ﬂow\nalgorithms [1,21], during training our method does not employ prior assumptions\nsuch as spatial smoothness, but is purely data-driven.\nOur resulting network is fast and yields optical ﬂow results with superior\naccuracy to the comparable networks of FlowNetS [6] and SpyNet [24] which\nwere trained using large amounts of labeled, synthetic optical ﬂow data [24].\nThis demonstrates that (a) when computing optical ﬂow, it is important to use\nreal data for training, and (b) that temporal interpolation is a suitable proxy\ntask to learn from to make learning from such data feasible.\n2\nPrevious work\nCNNs for Optical Flow. In the past years, end-to-end training has had consid-\nerable success in many tasks of computer vision, including optical ﬂow. The ﬁrst\n4\nWulﬀ, J. and Black, M.J.\npaper demonstrating end-to-end optical ﬂow was FlowNet [6], which used an ar-\nchitecture similar to ours, but trained on large amounts of synthetic ground truth\noptical ﬂow. In follow-up work [12], the authors propose a cascade of hourglass\nnetworks, each warping the images closer towards each other. Furthermore, they\nsigniﬁcantly extend their training dataset (which is still synthetic). This leads\nto high performance at the cost of a complicated training procedure.\nTaking a diﬀerent direction, SpyNet [24] combines deep learning with a spa-\ntial pyramid. Similar to classical optical ﬂow methods, each pyramid level com-\nputes the ﬂow residual for the ﬂow on the next coarser scale, and successively\nwarps the input frame using the new, reﬁned ﬂow. This allows the authors to use\na very simple network architecture, which in turns leads to high computational\neﬃciency. The training, however, is still done using the same annotated training\nset as [6]. The recently proposed PWC-Net [31] uses ideas of both, and computes\nthe ﬂow in a multiscale fashion using a cost volume on each scale, followed by a\ntrained ﬂow reﬁnement step.\nA diﬀerent approach is to not train a network for full end-to-end optical\nﬂow estimation, but to use trained networks inside a larger pipeline. Most com-\nmonly, these approaches use a network to estimate the similarity between two\npatches [11,34], eﬀectively replacing the data cost in a variational ﬂow method\nby a CNN. The resulting data costs can be combined with spatial inference, for\nexample belief-propagation [11], or a cost volume optimization [34]. A network\ntrained to denoise data can also be used as the proximal operator in a variational\nframework [20]. In the classical optical ﬂow formulation, this would correspond\nto a network that learns to regularize.\nAll these approaches, however, require large amounts of annotated data. For\nreal sequences, such training data is either hard to obtain for general scenes [14],\nor limited to speciﬁc domains such as driving scenarios [8]. Synthetic bench-\nmarks [4,19,26], on the other hand, often lack realism, and it is unclear how well\ntheir training data generalizes to the real world.\nHence, several works have investigated unsupervised training for optical ﬂow\nestimation. A common approach is to let the network estimate a ﬂow ﬁeld, warp\nthe input images towards each other using this ﬂow ﬁeld, and measure the sim-\nilarity of the images under a photometric loss. Since warping can be formulated\nas a diﬀerentiable function [13], the photometric loss can be back-propagated,\nforcing the network to learn better optical ﬂow. In [35], the authors combine\nthe photometric loss with a robust spatial loss on the estimated ﬂow, similar\nto robust regularization in variational optical ﬂow methods [29]. However, while\ntheir training is unsupervised, they do not demonstrate cross-dataset general-\nization, but train for Flying Chairs [6] and KITTI [8] using matching training\nsets and separately tuned hyper-parameters. In [25], the authors use the same\napproach, but show that a network that is pre-trained using the same dataset as\nin [6] can be ﬁne-tuned to diﬀerent output scenarios. Similarly, USCNN [1] uses\nonly a single training set. Here, the authors do not use end-to-end training, but\ninstead use a photometric loss to train a network to estimate the residual ﬂow\non diﬀerent pyramid layers, similar to [24]. The recently proposed UnFlow [21]\nTemporal Interpolation for Optical Flow Pretraining\n5\nuses a loss based on the CENSUS transform and computes the ﬂow in both\nforward and backward direction. This allows the authors to integrate occlusion\nreasoning into the loss; using an architecture based on FlowNet2, they achieve\nstate-of-the-art results on driving scenarios. All these methods require manually\nchosen heuristics as part of the loss, such as spatial smoothness or forward-\nbackward consistency-based occlusion reasoning. Therefore, they do not fully\nexploit the fact that CNNs allow us to overcome such heuristics and to purely\n“let the data speak”. In contrast, our method does not use any explicitly deﬁned\nheuristics, but uses an unsupervised interpolation task and a small number of\nground truth ﬂow ﬁelds to learn about motion.\nSeveral approaches use geometrical reasoning to self-supervise the training\nprocess. In TransFlow [2], the authors train two networks, a ﬁrst, shallow one\nestimating a global homography between two input frames, and a second, deep\nnetwork estimating the residual ﬂow after warping with this homography. Since\nthey use the homography to model the ego-motion, they focus on driving sce-\nnarios, and do not test on more generic optical ﬂow sequences. In [9], a network\nis trained to estimate depth from a single image, and the photometric error\naccording to the warping in a known stereo setup induced by the depth is pe-\nnalized. Similarly [36] trains a network to estimate depth from a single image by\nlearning to warp, but use videos instead of stereo. SfM-Net [7] learns to reduce a\nphotometric loss by estimating the 3D structure of the world, the motion of the\nobserver, and the segmentation into moving and static regions, which is enough\nto explain most of the motion of the world. However, as most other methods, it\nis only tested on the restricted automotive scenario.\nSimliar to self-supervision using geometric losses, Generative Adversarial\nNetworks [10] have been used to learn the structure of optical ﬂow ﬁelds. In [15],\nthe GAN is trained to distinguish between the warping errors caused by ground\ntruth and wrongly estimated optical ﬂow. Only the discriminator uses annotated\ndata; once trained, it provides a loss for unsupervised training of the ﬂow itself.\nFrame interpolation. Instead of warping one input frame to another, it is\nalso possible to train networks to interpolate and extrapolate images by showing\nthem unlabeled videos at training time. A hybrid network is used in [27], where\na shared contractive network is combined with two expanding networks to es-\ntimate optical ﬂow and to predict the next frame, respectively. Similar to us,\nthey hypothesize that temporal frame generation and optical ﬂow share some\ninternal representations; however, unlike us they train the optical ﬂow network\ncompletely with labeled data, and do not test on the challenging Sintel test set.\nSimilarly, [32] trains a network to anticipate the values of intermediate feature\nmaps in the future. However, they are not interested in the motion itself, but\nin the future higher-level scene properties such as objects and actions. Niklaus\net al. [23] propose a video interpolation network, where the motion is encoded\nin an estimated convolutional kernel; however, the quality of this implicit mo-\ntion is never tested. As mentioned above, the work most similar to ours is [18],\nwhere a CNN is trained to interpolate between frames and subsequently used\nto compute correspondences between images. Unlike our work, however, they\n6\nWulﬀ, J. and Black, M.J.\nFig. 2: Architecture of our network. The outputs of 3×3 convolutional layers are shown\nin blue, the output of 2×2 max-pooling operations in red, 2× transposed convolutions\nin green, and side-channel concatenation in orange. Not shown are leaky ReLUs after\neach layer except the last and batch normalization layers. The numbers indicate the\nnumber of channels of each feature map.\nFig. 3: Our network predicts the center frame (green) from four neighboring, equally\nspaced frames (red). To ensure equal spacing of the input frames, the unmarked frames\n(second from the left and right) are not taken into account.\nrequire expensive backpropagation steps to establish the correspondences; we\nshow that using a small amount of training data can teach the network to trans-\nlate between its internal motion representation and optical ﬂow, resulting in\nsigniﬁcantly improved performance.\n3\nA frame interpolation network\nThe core hypothesis of our work is that in order to properly perform temporal\ninterpolation, it is necessary to learn about the motion of the world. Temporal\ninterpolation, however, is a task that does not require explicit supervision; with\nproper selection of input and output frames, every video sequence can serve as\na supervisory signal. Therefore, we start by training a network for the task of\ninterpolation in an unsupervised manner. Given four frames (as shown in Fig. 3\nin red), the task of our network is to interpolate the center frame, shown in\ngreen in Fig. 3. Empirically, we found that using four input frames resulted in\napproximately 13% lower errors (2.04 px EPE) on the optical ﬂow estimation\ntask than using two frames (2.31 px EPE). We believe that the reasons for this\nare that with more than two frames (a) it is easier to reason about higher order\ntemporal eﬀects (ie. non-zero acceleration), and (b) it enables the network to\nreason about occlusions, which requires at least three frames [30]. We hence use\nfour input frames for both the interpolation and the optical ﬂow estimation task.\nTemporal Interpolation for Optical Flow Pretraining\n7\nWe use grayscale images as input and output, since we found the ﬁnal optical\nﬂow to have lower errors with grayscale than with color images.\nNetwork architecture. Similar to [18], we use a standard hourglass ar-\nchitecture with side-channels, as shown in Fig. 2. This simple architecture is\nnevertheless surprisingly eﬀective in many diﬀerent applications, such as opti-\ncal ﬂow computation [6] and unsupervised depth estimation [36]. Our network\nconsists of ﬁve convolutional blocks (Conv1 to Conv5 in Fig. 2), each of which\ncontains three 3 × 3 convolutional layers followed by batch normalization lay-\ners and leaky ReLUs. Between the blocks, we use max-pooling to reduce the\nresolution by a factor of two. Within each block, all layers except the last out-\nput the same number of feature maps. Conv5 is followed by a bottleneck block\nconsisting of two convolutional layers and a transposed convolution. The output\nis then successively upscaled using a series of decoder blocks (Dec5 to Dec1).\nEach consists of two convolutional layers and (except for Dec1) a transposed\nconvolution which doubles the resolution, again interleaved with leaky ReLUs\nand batch normalization layers.\nTo preserve high frequency information, we use side channels as in [6], shown\nin orange in Fig. 2. The output of the transposed convolutions are concatenated\nwith the appropriate outputs from the convolutional layers. The last convolu-\ntional layer of Dec1 directly outputs the monochrome output image and is not\nfollowed by a nonlinearity. Table 1 summarizes the number of inputs and outputs\nof each convolutional block.\nTable 1: Number of input and output channels per layer block.\nConv1\nConv2\nConv3\nConv4\nConv5\nBottleneck\nDec5\nDec4\nDec3\nDec2\nDec1\nInput\n4\n128\n128\n256\n256\n512\n1024\n1024\n512\n512\n256\nOutput\n128\n128\n256\n256\n512\n1024\n1024\n512\n512\n256\n1\nTraining data. Unsupervised training would in theory allow us to train\na network with inﬁnite amounts of data. Yet, most previous works only use\nrestricted datasets, such as KITTI-RAW. Instead, in this work we aim to com-\npile a large, varied dataset, containing both cinematic sequences from several\nmovies of diﬀerent genres as well as more restricted but very common sequences\nfrom driving scenarios. As movies, we use Victoria and Birdman. The former\nwas ﬁlmed in a true single take, and the later was ﬁlmed and edited in such a\nway that cuts are imperceptible. In addition, we use long takes from the movies\nAtonement, Children of Men, Baby Driver, and the TV series True Detective.\nShot boundaries would destroy the temporal consistency that we want our net-\nwork to learn and hence would have to be explicitly detected and removed; using\nsingle-take shots eliminates this problem.\nFor the driving scenarios, we use KITTI-RAW [22] and Malaga [3], the ﬁrst of\nwhich contains several sequences and the second of which contains one long shot\nof camera footage recorded from a driving car. For each sequence, we use around\n8\nWulﬀ, J. and Black, M.J.\n1 % of frames as validation, sampled from the beginning, center, and end of the\nsequence and sampled such that they do not overlap the actual training data.\nThe only diﬀerence is KITTI-RAW, which is already broken up into sequences.\nHere, we sample full sequences to go either to the training or validation set,\nand use 10 % for the validation set. This ensures that the validation set contains\napproximately the same amount of frames from driving and movie-like scenarios.\nTable 2 summarizes the datasets used and the amount of frames from each.\nIn total, we thus have approximately 464K training frames. However, in movie\nsequences, the camera and object motions are often small. Therefore, during both\ntraining and validation, we predict each frame twice, once from the adjacent\nframes as shown in Fig. 3, and once with doubled spacing between the frames.\nTherefore, a target frame at time t provides two training samples, one where it\nis predicted from the frames at t −3, t −1, t + 1, and t + 3, and one where it is\npredicted from t −6, t −2, t + 2, and t + 6. This ensures that we have a suﬃcient\namount of large motions in our frame interpolation training set. In total, our\ntraining and validation sets contain 928,410 and 16,966 samples, respectively.\nTraining details. As shown in Fig. 3, each training sample of our network\nconsists of the four input frames and the center frame which the network should\npredict. During training, each quadruple of frames is separately normalized by\nsubtracting the mean of the four input frames and dividing by their standard de-\nviation. We found this to work better than normalization across the full dataset.\nWe believe the reason for this is that in correspondence estimation, it is more\nimportant to consider the structure within a sample than the structure across\nsamples, the later of which is important for classiﬁcation tasks. To put it diﬀer-\nently, whether a light bar or a dark bar moves to the right does not matter for\noptical ﬂow and should produce the same output.\nAs data augmentation, we randomly crop the input images to rectangles with\nthe aspect ratio 2:1, and resize the cropped images to a resolution of 384 × 192\npixel, resulting in randomization of both scale and translation. For all input\nframes belonging to a training sample, we use the same crop. Furthermore, we\nrandomly ﬂip the images in horizontal and vertical direction, and randomly\nswitch between forward and backward temporal ordering of the input images.\nWe use a batch size of 8, train our network using ADAM and use a loss consisting\nof a structural similarity loss (SSIM) [33] and an L1 loss, weighted equally. The\ninitial learning rate is set to 1e −4, and halved after 3, 6, 8, and 10 epochs. We\ntrain our network for 12 epochs; after this point, we did not notice any further\ndecrease in our loss on the validation set.\n4\nFrom interpolation to Optical Flow\nGiven a frame interpolation network, it has been shown before [18] that motion is\nlearned by the network and can be extracted. However, this only works for regions\nwith suﬃcient texture. In unstructured areas of the image, the photometric error\nthat is used to train the frame interpolation is not informative, and even a wrong\nmotion estimation can result in virtually perfect frame reconstruction.\nTemporal Interpolation for Optical Flow Pretraining\n9\nTable 2: Training data for interpolation.\nSource\nType\nTraining frames Validation frames\nBirdman\nFull movie\n155,403\n1,543\nVictoria\nFull movie\n188,700\n1,878\nKITTI-RAW\nDriving\n39,032\n3,960\nMalaga\nDriving\n51,285\n460\nAtonement\nMovie clip\n7,062\n44\nChildren of Men Movie clip\n9,165\n65\nBaby Driver\nMovie clip\n3,888\n14\nTrue Detective Movie clip\n8,388\n57\nTotal\n464,205\n8,483\nWhat is missing for good optical ﬂow estimation, then, is the capability to\ngroup the scene and to ﬁll in the motion in unstructured regions, ie. to address\nthe aperture problem. Furthermore, the mechanism used to extract the motion\nin [18] is slow, since it requires a complete backpropagation pass for each corre-\nspondence. To eﬀectively use frame interpolation for optical ﬂow computation,\ntwo steps are thus missing: (a) to add knowledge about grouping and region\nﬁll-in to a network that can compute correspondences, and (b) to modify the\nnetwork to directly yield an optical ﬂow ﬁeld, making expensive backpropaga-\ntion steps unnecessary at test time. Luckily, both objectives can be achieved\nby ﬁne-tuning the network to directly estimate optical ﬂow, using only a very\nlimited amount of annotated ground truth data.\nFor this, we replace the last layer of the Dec1 block with a vanilla 3 ×\n3 convolutional layer with two output channels instead of one, and train this\nnetwork using available ground truth training data, consisting of the training\nsets of KITTI-2012 [8], KITTI-2015 [22] and the clean and ﬁnal passes of MPI-\nSintel [4], for a total of about 2500 frames. We use 10 % of the data as validation.\nWe again use ADAM with an initial learning rate of 10−4, halve the learning\nrate if the error on the validation set has not decreased for 20 epochs, and train\nfor a total of 200 epochs using the endpoint error as the loss. Except for the\ntemporal reversal, we use the same augmentations as described above.\nAs we will see in the next section, this simple ﬁne-tuning procedure results in\na network that computes good optical ﬂow, and even outperforms networks with\ncomparable architecture that were trained using large amounts of synthetically\ngenerated optical ﬂow.\n5\nExperiments\nIn this section, we demonstrate the eﬀectiveness of our method for interpolation\nand optical ﬂow estimation, and provide further experiments showing the impor-\ntance of pre-training and the eﬀects of reduced ground truth data for ﬁne-tuning.\n10\nWulﬀ, J. and Black, M.J.\nFig. 4: Visual interpolation results (unseen data). From top to bottom: linear blending;\ninterpolation using [23], LF variant; interpolation using our method; ground truth.\nWhile the results from [23] are sharper, they produce signiﬁcant artifacts, for example\nthe tree in the right example. Our method tends to be blurrier, but captures the gist\nof the scene better; this is reﬂected in the superior quantitative results.\n5.1\nTemporal interpolation\nTo evaluate the interpolation performance of our network, we compare with [23],\na state-of-the-art method for frame interpolation. Unlike ours, [23] is speciﬁcally\ndesigned for this task; in contrast, we use a standard hourglass network. We\ncompute temporal interpolations for 2800 frames from natural movies, a syn-\nthetic movie (Sintel), and driving scenarios. All frames were not previously seen\nin training. To compute interpolated color frames, we simply run our network\nonce for each input color channel. Table 3 shows results on both PSNR and\nSSIM. For [23], we report both the L1 and LF results; according to [23], the\nformer is better suited for numerical evaluation, while the later produces better\nvisual results. We outperform both variants in both metrics.\nTable 3: Interpolation performance in PSNR (SSIM).\nReal movie\nSynthetic movie\nDriving\nAll\n[23], L1\n33.15 (0.915)\n25.73 (0.841)\n18.26 (0.664)\n28.80 (0.854)\n[23], LF\n32.98 (0.911)\n25.44 (0.825)\n18.04 (0.631)\n28.59 (0.843)\nOurs\n34.68 (0.928)\n26.46 (0.859)\n19.76 (0.710)\n30.13 (0.8741)\nFigure 4 shows quantitative results of the interpolation for all three scenarios.\nVisually, the interpolation results are good. In particular, our method can handle\nTemporal Interpolation for Optical Flow Pretraining\n11\nlarge displacements, as can be seen in the helmet strap in Fig 4, ﬁrst column,\nand the tree in Fig. 4, third column. The results of [23] tend to be sharper;\nhowever, this comes with strong artifacts visible in the second row of Fig. 4.\nBoth the helmet strap and the tree are not reconstructed signiﬁcantly better\nthan when using simple linear blending (ﬁrst row); our method, while slightly\nblurry, localizes the objects much better.\n5.2\nOptical Flow\nResults on benchmarks. To demonstrate the eﬀectiveness of our method,\ndubbed IPFlow for Interpolation Pretrained Flow, we test the optical ﬂow per-\nformance on the two main benchmarks, KITTI [8,22] and MPI-Sintel [4]. Since\nour method uses four input frames, we double the ﬁrst and last frames to compute\nthe ﬁrst and last ﬂow ﬁeld within a sequence, respectively, thereby obtaining ﬂow\ncorresponding to all input frames. Furthermore, like FlowNet [6], we perform a\nvariational post-processing step to remove noise from our ﬂow ﬁeld. Computing\nthe ﬂow on a NVIDIA M6000 GPU takes 60 ms for the CNN; the variational\nreﬁnement takes 1.2 seconds.\nTable 4: Quantitative evaluation of our method.\nSintel\nKitti-2012\nKitti-2015\nClean\nFinal\nSupervised methods\nFlowNet2-ft [12]\n4.16\n5.74\n1.8\n11.48 %\nFlowNetS+ft+v [6]\n6.16\n7.22\n9.1\nSpyNet+ft [24]\n6.64\n8.36\n4.1\n35.07 %\nUn- and semisupervised methods\nDSTFlow [25]\n10.41\n11.28\n12.4\n39 %\nUSCNN [1]\n8.88\nSemi-GAN [15]\n6.27\n7.31\n6.8\n31.01 %\nUnFlow-CSS [21]\n9.38\n10.22\n1.7\n11.11 %\nIPFlow (ours)\n5.95\n6.59\n3.5\n29.54 %\nIPFlow-Scratch\n8.35\n8.87\nTable 4 shows the errors on the unseen test sets (average endpoint error for\nSintel and KITTI-2012, Fl-All for KITTI-2015); Figure 5 shows qualitative\nresults. While we do not quite reach the same performance as more compli-\ncated architectures such as FlowNet2 [12] or UnFlow-CSS [21]3, on all datasets\nwe outperform methods which are based on simple architectures comparable to\nours, including those that were trained with large amounts of annotated ground\n3 For UnFlow, test set results are only available for the -CSS variant, which is based\non a FlowNet2 architecture. The simpler UnFlow-C is not evaluated on the test sets.\n12\nWulﬀ, J. and Black, M.J.\nFig. 5: Visual results. From top to bottom: Input image; Ground truth ﬂow; Result of\nIPFlow; Training from scratch. The ﬂow computed using pure training from scratch is\nreasonable, but using pre-training yields signiﬁcantly better optical ﬂow maps.\ntruth data (FlowNetS+ft+v [6] and SpyNet [24]). For these simple architec-\ntures, pre-training using a slow-motion task is hence superior to pre-training\nusing synthetic, annotated optical ﬂow data. UnFlow-CSS is the only method\noutperforming ours on KITTI that does not require large amounts of annotated\nframes; yet, they use a considerably more complicated architecture and only\nachieve state-of-the-art results in driving scenarios and not on Sintel.\nPerformance without pretraining. To evaluate whether the Sintel train-\ning data might be enough to learn optical ﬂow by itself, we also tried training our\nnetwork from scratch. We test two training schedules, ﬁrst using the same learn-\ning parameters as for the ﬁne-tuning, and second the well-established s short\nschedule from [12]. As shown in Fig. 6, the network is able to learn to compute\noptical ﬂow even without pre-training, and beneﬁts from using the s short sched-\nule4. However, at convergence the error of the network without pre-training on\nunseen validation data is around 50 % higher. This is also visible in Table 4,\nwhere IPFlow-Scratch denotes the training from scratch using s short; again,\nthe errors are considerably higher. Thus, important properties of motion must\nhave been learned during the unsupervised pretraining phase.\nUsing a low number of ﬁne-tuning frames. As we just showed, using\nonly the training set and no ﬁne-tuning results in signiﬁcantly worse perfor-\nmance; pre-training from interpolation is clearly beneﬁcial. However, this now\npoints to another, related question: Once we have a pre-trained network, how\nmuch annotated training data is actually required to achieve good performance?\nIn other words, does pre-training free us from having to annotate or generate\nthousands of ground truth optical ﬂow frames, and if so, how large is this eﬀect?\nWe tested this question by repeating the ﬁnetuning procedure using only a\nsmall amount (25-200) of randomly chosen frames from the respective training\nsets. Since the scenario for using very few annotated frames points to application-\n4 For ﬁne-tuning after pretraining, s short gives higher errors than our schedule.\nTemporal Interpolation for Optical Flow Pretraining\n13\n0\n100\n200\n300\n400\n500\nEpoch\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nValidation error (EPE)\nScratch, s_short\nScratch\nInterpolation pretrained\nFig. 6: Using interpolation as pre-training, the network learns to adapt to optical ﬂow.\nFlow can also be learned without pre-training, but in this case the error is 50 % higher.\nspeciﬁc optical ﬂow (for example, ﬂow speciﬁcally for driving), we perform this\nexperiment separately for diﬀerent datasets, KITTI (containing both KITTI-\n2015 [22] and KITTI-2012 [8]), Sintel (clean) and Sintel (ﬁnal). All trained net-\nworks are tested on the same validation set for the respective dataset, and we\nrepeated the experiment three times and averaged the results.\nFigure 7 shows the results. While using only 25 frames is generally not enough\nto estimate good optical ﬂow, the performance quickly improves with the number\nof available training frames. After seeing only 100 training frames, for all datasets\nthe performance is within 0.5 px EPE of the optimal performance achievable\nwhen using the full training sets for the respective dataset. This shows that a\ninterpolation-pretrained network such as the one presented here can be quickly\nand easily tuned for computing ﬂow for a speciﬁc application, and does not\nrequire a large amount of annotated ground truth ﬂow ﬁelds.\n6\nConclusion\nIn this work, we have demonstrated that a network trained for the task of in-\nterpolation does learn about motion in the world. However, this is only true for\nimage regions containing suﬃcient texture for the photometric error to be mean-\ningful. We have shown that, using a simple ﬁne-tuning procedure, the network\ncan be taught to (a) ﬁll in untextured regions and (b) to output optical ﬂow\ndirectly, making it more eﬃcient than comparable, previous work [18]. In partic-\nular, we have shown that only a small number of annotated ground truth optical\nﬂow frames is suﬃcient to reach comparable performance to large datasets; this\nprovides the user of our algorithm with the choice of either increasing the accu-\nracy of the optical ﬂow estimation, or to require only a low number of annotated\nground truth frames. Demonstrating the importance of pre-training, we have\n14\nWulﬀ, J. and Black, M.J.\n0\n50\n100\n150\n200\n250\nNumber of frames used for training\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEPE on unseen validation\nKITTI\nSintel, clean\nSintel, final\nFig. 7: Fine-tuning with a small amount of frames. With only 100 frames, the perfor-\nmance on all validation sets gets within 0.5px EPE of the optimal performance. The\ndashed lines show the performance when using the full training set for each dataset.\nshown that the same network without the interpolation pre-training performs\nsigniﬁcantly worse; our network also outperforms all other methods with compa-\nrable architectures, regardless whether they were trained using full supervision\nor not. As a side eﬀect, we have demonstrated that, given enough and suﬃciently\nvaried training data, even a simple generic network architecture outperforms a\nspecialized architecture for frame interpolation.\nOur work suggests several directions for future work. First, it shows the\nusefulness of this simple proxy task for correspondence estimation. In the analysis\nof still images, however, we often use a proxy task that requires some semantic\nunderstanding of the scene, in the hope of arriving at internal representations\nof the image that mirror the semantic content. As video analysis moves away\nfrom the pixels and more towards higher-level understanding, ﬁnding such proxy\ntasks for video remains an open problem. Second, even when staying with the\nproblem of optical ﬂow estimation, we saw that optimized pipelines together\nwith synthetic training data still outperform our method. We believe, however,\nthat even those algorithms could beneﬁt from using a pre-training such as the\none described here; utilizing it to achieve true state-of-the-art performance on\noptical ﬂow remains for future work.\nLastly, the promise of unsupervised methods is that they scale with the\namount of data, and that showing more video to a method like ours would lead\nto better results. Testing how good an interpolation (and the subsequent ﬂow\nestimation) method can get by simply watching more and more video remains\nto be seen.\nAcknowledgements & Disclosure. JW was supported by the Max Planck\nETH Center for Learning Systems. MJB has received research funding from Intel,\nNvidia, Adobe, Facebook, and Amazon. While MJB is a part-time employee of\nAmazon, this research was performed solely at, and funded solely by, MPI.\nTemporal Interpolation for Optical Flow Pretraining\n15\nReferences\n1. Ahmadi, A., Patras, I.: Unsupervised convolutional neural networks for motion\nestimation. In: 2016 IEEE International Conference on Image Processing (ICIP).\npp. 1629–1633 (Sept 2016). https://doi.org/10.1109/ICIP.2016.7532634\n2. Alletto, S., Abati, D., Calderara, S., Cucchiara, R., Rigazio, L.: Transﬂow: Un-\nsupervised motion ﬂow by joint geometric and pixel-level estimation. Tech. rep.,\narXiv preprint arXiv:1706.00322 (2017)\n3. Blanco, J.L., Moreno, F.A., Gonzalez-Jimenez, J.: The malaga urban dataset: High-\nrate stereo and lidars in a realistic urban scenario. International Journal of Robotics\nResearch 33(2), 207–214 (2014). https://doi.org/10.1177/0278364913507326\n4. Butler, D., Wulﬀ, J., Stanley, G., Black, M.: A naturalistic open source movie for\noptical ﬂow evaluation. In: ECCV (2012)\n5. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning\nby context prediction. In: ICCV (2015)\n6. Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V., van der\nSmagt, P., Cremers, D., Brox, T.: Flownet: Learning optical ﬂow with convolutional\nnetworks. In: International Conference on Computer Vision (ICCV) (2015)\n7. Fragkiadaki, A., Seybold, B., Sukthankar, R., Vijayanarasimhan, S., Ricco, S.: Self-\nsupervised learning of structure and motion from video. In: arxiv (2017) (2017)\n8. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The KITTI\ndataset. International Journal of Robotics Research 32(11), 1231–1237 (Sep 2013).\nhttps://doi.org/10.1177/0278364913491297\n9. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth esti-\nmation with left-right consistency. In: CVPR (July 2017)\n10. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., Bengio, Y.: Generative adversarial nets. In: NIPS (2014)\n11. G¨uney, F., Geiger, A.: Deep discrete ﬂow. In: ACCV (2016)\n12. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: Flownet 2.0:\nEvolution of optical ﬂow estimation with deep networks. In: CVPR (2017)\n13. Jaderberg, M., Simonyan, K., Zisserman, A., kavukcuoglu, k.: Spatial transformer\nnetworks. In: NIPS (2015)\n14. Janai, J., G¨uney, F., Wulﬀ, J., Black, M., Geiger, A.: Slow ﬂow: Exploiting high-\nspeed cameras for accurate and diverse optical ﬂow reference data. In: CVPR\n(2017)\n15. Lai, W.S., Huang, J.B., Yang, M.H.: Semi-supervised learning for optical ﬂow with\ngenerative adversarial networks. In: NIPS (2017)\n16. Larsson, G., Maire, M., Shakhnarovich, G.: Colorization as a proxy task for visual\nunderstanding. In: CVPR (2017)\n17. Liu, C., Freeman, W.T., Adelson, E.H., Weiss, Y.: Human-assisted motion anno-\ntation. In: CVPR (2008)\n18. Long, G., Kneip, L., Alvarez, J.M., Li, H., Zhang, X., Yu, Q.: Learning Image\nMatching by Simply Watching Video (2016)\n19. Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox,\nT.: A large dataset to train convolutional networks for disparity, optical ﬂow, and\nscene ﬂow estimation. In: CVPR (2016)\n20. Meinhardt, T., Moller, M., Hazirbas, C., Cremers, D.: Learning proximal opera-\ntors: Using denoising networks for regularizing inverse imaging problems. In: ICCV\n(2017)\n16\nWulﬀ, J. and Black, M.J.\n21. Meister, S., Hur, J., Roth, S.: Unﬂow: Unsupervised learning of optical ﬂow with\na bidirectional census loss. arXiv preprint arXiv:1711.07837 (2017)\n22. Menze, M., Heipke, C., Geiger, A.: Discrete optimization for optical ﬂow. In: Ger-\nman Conference on Pattern Recognition (GCPR). vol. 9358, pp. 16–28. Springer\nInternational Publishing (2015)\n23. Niklaus, S., Mai, L., Liu, F.: Video frame interpolation via adaptive separable\nconvolution. In: ICCV (2017)\n24. Ranjan, A., Black, M.J.: Optical ﬂow estimation using a spatial pyramid network.\nTech. rep., arXiv (2016)\n25. Ren, Z., Yan, J., Ni, B., Liu, B., Yang, X., Zha, H.: Unsupervised deep learning\nfor optical ﬂow estimation. In: AAAI Conference on Artiﬁcial Intelligence (2017)\n26. Richter, S.R., Hayder, Z., Koltun, V.: Playing for benchmarks. In: ICCV (2017)\n27. Sedaghat, N., Zolfaghari, M., Brox, T.: Hybrid learning of optical ﬂow and next\nframe prediction to boost optical ﬂow in the wild. Tech. rep., arXiv:1612.03777\n(2017), https://arxiv.org/abs/1612.03777\n28. Sun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable eﬀectiveness\nof data in deep learning era. In: ICCV (2017)\n29. Sun, D., Roth, S., Black, M.: A quantitative analysis of current practices in optical\nﬂow estimation and the principles behind them. International Journal of Computer\nVision 106(2), 115–137 (2014). https://doi.org/10.1007/s11263-013-0644-x\n30. Sun, D., Sudderth, E., Black, M.J.: Layered segmentation and optical ﬂow estima-\ntion over time. In: CVPR (2012)\n31. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: Pwc-net: Cnns for optical ﬂow using\npyramid, warping, and cost volume. arXiv preprint arXiv:1709.02371 (2017)\n32. Vondrick, C., Pirsiavash, H., Torralba, A.: Anticipating visual representations from\nunlabeled video. In: CVPR (2016)\n33. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:\nfrom error visibility to structural similarity. IEEE Transactions on Image Process-\ning 13(4), 600–612 (April 2004). https://doi.org/10.1109/TIP.2003.819861\n34. Xu, J., Ranftl, R., Koltun, V.: Accurate optical ﬂow via direct cost volume pro-\ncessing. In: CVPR (2017)\n35. Yu, J.J., Harley, A.W., Derpanis, K.G.: Back to Basics: Unsupervised Learning of\nOptical Flow via Brightness Constancy and Motion Smoothness (2016)\n36. Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and\nego-motion from video. In: CVPR (2017)\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2018-09-21",
  "updated": "2018-09-21"
}