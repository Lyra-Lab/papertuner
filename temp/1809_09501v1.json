{
  "id": "http://arxiv.org/abs/1809.09501v1",
  "title": "Anderson Acceleration for Reinforcement Learning",
  "authors": [
    "Matthieu Geist",
    "Bruno Scherrer"
  ],
  "abstract": "Anderson acceleration is an old and simple method for accelerating the\ncomputation of a fixed point. However, as far as we know and quite\nsurprisingly, it has never been applied to dynamic programming or reinforcement\nlearning. In this paper, we explain briefly what Anderson acceleration is and\nhow it can be applied to value iteration, this being supported by preliminary\nexperiments showing a significant speed up of convergence, that we critically\ndiscuss. We also discuss how this idea could be applied more generally to\n(deep) reinforcement learning.",
  "text": "European Workshop on Reinforcement Learning 14 (2018)\nOctober 2018, Lille, France.\nAnderson Acceleration for Reinforcement Learning\nMatthieu Geist\nmatthieu.geist@univ-lorraine.fr\nUniversit´e de Lorraine, CNRS, LIEC, F-57000 Metz, France\n(Now at Google Brain)\nBruno Scherrer\nbruno.scherrer@inria.fr\nUniversit´e de Lorraine, CNRS, Inria, IECL, F-54000 Nancy, France\nAbstract\nAnderson (1965) acceleration is an old and simple method for accelerating the computation\nof a ﬁxed point. However, as far as we know and quite surprisingly, it has never been\napplied to dynamic programming or reinforcement learning.\nIn this paper, we explain\nbrieﬂy what Anderson acceleration is and how it can be applied to value iteration, this\nbeing supported by preliminary experiments showing a signiﬁcant speed up of convergence,\nthat we critically discuss. We also discuss how this idea could be applied more generally\nto (deep) reinforcement learning.\nKeywords:\nReinforcement learning; accelerated ﬁxed point.\n1. Introduction\nReinforcement learning (RL) (Sutton and Barto, 1998) is intrinsically linked to ﬁxed-point\ncomputation: the optimal value function is the ﬁxed point of the (nonlinear) Bellman\noptimality operator, and the value function of a given policy is the ﬁxed point of the\nrelated (linear) Bellman evaluation operator.\nMost of the time, these ﬁxed points are\ncomputed recursively, by applying repeatedly the operator of interest. Notable exceptions\nare the evaluation step of policy iteration and the least-squares temporal diﬀerences (LSTD)\nalgorithm1 (Bradtke and Barto, 1996).\nAnderson (1965) acceleration (also known as Anderson mixing, Pulay mixing, direct\ninversion on the iterative subspace or DIIS, among others2) is a method that allows speeding\nup the computation of such ﬁxed points. The classic ﬁxed-point iteration applies repeatdly\nthe operator to the last estimate. Anderson acceleration considers the m previous estimates.\nThen, it searches for the point that has minimal residual within the subspace spanned by\nthese estimates, and applies the operator to it. This approach has been successfully applied\nto ﬁelds such as electronic structure computation or computational chemistry, but it has\nnever been applied to dynamic programming or reinforcement learning, as far as we know.\nFor more about Anderson acceleration, refer to Walker and Ni (2011), for example.\n1. In the realm of deep RL, as far as we know, all ﬁxed points are computed iteratively, there is no LSTD.\n2. Anderson acceleration and variations have been rediscovered a number of times in various communities in\nthe last 50 years. Walker and Ni (2011) provide a brief overview of these methods, and a close approach\nhas been recently proposed in the machine learning community (Scieur et al., 2016).\nc⃝2018 Matthieu Geist and Bruno Scherrer.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.\narXiv:1809.09501v1  [cs.LG]  25 Sep 2018\nGeist and Scherrer\n2. Anderson Acceleration for Value Iteration\nIn this section, we brieﬂy review Markov decision processes, value iteration, and show how\nAnderson acceleration can be used to speed up convergence.\n2.1 Value Iteration\nLet ∆X be the set of probability distributions over a ﬁnite set X and Y X the set of ap-\nplications from X to the set Y . By convention, all vectors are column vectors. A Markov\nDecision Process (MDP) is a tuple {S, A, P, R, γ}, where S is the ﬁnite state space, A is\nthe ﬁnite action space, P ∈(∆S)S×A is the Markovian transition kernel (P(s′|s, a) denotes\nthe probability of transiting to s′ when action a is applied in state s), R ∈RS×A is the\nbounded reward function (R(s, a) represents the local beneﬁt of doing action a in state s)\nand γ ∈(0, 1) is the discount factor.\nA stochastic policy π ∈(∆A)S associates a distribution over actions to each state (de-\nterministic policies being a special case of this). The policy-induced reward and transition\nkernels, Rπ ∈RS and Pπ ∈(∆S)S, are deﬁned as\nRπ(s) = Eπ(.|s)[R(s, A)] and Pπ(s′|s) = Eπ(.|s)[P(s′|s, A)].\nThe quality of a policy is quantiﬁed by the associated value function vπ ∈RS:\nvπ(s) = E[\nX\nt≥0\nγtRπ(St)|S0 = s, St+1 ∼Pπ(.|St)].\nThe value vπ is the unique ﬁxed point of the Bellman operator Tπ, deﬁned as Tπv =\nRπ + γPπv for any v ∈RS.\nLet deﬁne the second Bellman operator T as, for any v ∈RS, Tv = maxπ∈(∆A)S Tπv.\nThis operator is a γ-contraction (in supremum norm), so the iteration\nvk+1 = Tvk\nconverges to its unique ﬁxed-point v∗= maxπ vπ, for any v0 ∈RS. This is the value iteration\nalgorithm.\n2.2 Accelerated Value Iteration\nAnderson acceleration is a method that aims at accelerating the computation of the ﬁxed\npoint of any operator. Here, we describe it considering the Bellman operator T, which\nprovides an accelerated value iteration algorithm.\nAssume that estimates have been computed up to iteration k, and that in addition to\nvk the m previous estimates vk−1, . . . , vk−m are known. The coeﬃcient vector αk+1 ∈Rm+1\nis deﬁned as follows:\nαk+1 = argmin\nα∈Rm+1\n\r\r\r\r\r\nm\nX\ni=0\nαi(Tvk−m+i −vk−m+i)\n\r\r\r\r\r s.t.\nm\nX\ni=0\nαi = 1.\nNotice that we don’t impose a positivity condition on the coeﬃcients. We will consider\npractically the ℓ2-norm for this problem, but it could be a diﬀerent norm (for example ℓ1 or\n2\nAnderson Acceleration for Reinforcement Learning\nℓ∞, in which case the optimization problem is a linear program). Then, the new estimate\nis given by:\nvk+1 =\nm\nX\ni=0\nαk+1\ni\nTvk−m+i.\nThe resulting Anderson accelerated value iteration is summarized in Alg. 1. Notice that the\nsolution to the optimization problem can be obtained analytically for the ℓ2-norm, using\nthe Karush-Kuhn-Tucker conditions. With the notations of Alg. 1 and writting 1 ∈Rmk+1\nthe vector with all components equal to one, it is\nαk+1 =\n(∆⊤\nk ∆k)−11\n1⊤(∆⊤\nk ∆k)−11.\n(1)\nThis can be regularized to avoid ill-conditioning.\nAlgorithm 1: Anderson Accelerated Value Iteration\ngiven: v0 and m ≥1\nCompute v1 = Tv0;\nfor k = 1, 2, 3 . . . do\nSet mk = min(m, k);\nSet ∆k = [δk−mk, . . . , δk] ∈RS×(m+1) with δi = Tvi −vi ∈RS;\nSolve minα∈Rm+1 ∥∆kα∥s.t. Pmk\ni=0 αk = 1;\nSet vk+1 = Pmk\ni=0 αiTvk−mk+i;\nThe rationale of this acceleration scheme is better understood with an aﬃne operator.\nWe consider here the Bellman evaluation operator Tπ. Given the current and the m previous\nestimates, deﬁne\n˜vα\nk+1 =\nm\nX\ni=0\nαivk−m+i with\nm\nX\ni=0\nαi = 1.\nThanks to this constraint, for an aﬃne operator (here Tπ), we have that\nTπ˜vα\nk+1 =\nm\nX\ni=0\nαiTπvk−m+i.\nThen, one searches for a vector α (satisfying the constraint) that minimizes the residual\n∥Tπ˜vα\nk+1 −˜vα\nk+1∥= ∥\nm\nX\ni=0\nαi(Tπvk−m+i −vk−m+i)∥.\nEventually, the new estimate is obtained by applying the operator to the vector ˜vα\nk+1 of\nminimal residual.\nThe same approach can be applied (heuristically) to non-aﬃne operators. The conver-\ngence of this scheme has been studied (e.g., Toth and Kelley (2015)) and it can be linked\nto quasi-Newton methods (Fang and Saad, 2009).\n3\nGeist and Scherrer\n2.3 Preliminary Experimental Results\nWe consider Garnet problems (Archibald et al., 1995; Bhatnagar et al., 2009). They are a\nclass of randomly built MDPs meant to be totally abstract while remaining representative\nof the problems that might be encountered in practice. Here, a Garnet G(|S|, |A|, b) is\nspeciﬁed by the number of states, the number of actions and the branching factor. For each\n(s, a) couple, b diﬀerent next states are chosen randomly and the associated probabilities\nare set by randomly partitioning the unit interval. The reward is null, except for 10% of\nstates where it is set to a random value, uniform in (1, 2).\nWe generate 100 random MDPs G(100, 4, 3) and set γ to 0.99. For each MDP, we apply\nvalue iteration (denoted as m = 0 in the graphics) and Anderson accelerated value iteration\nfor m ranging from 1 to 9.\nThe inital value function v0 is always the null vector.\nWe\nrun all algorithms for 250 iterations, and measure the normalised error for algorithm alg\nat iteration k, ∥v∗−valg\nk ∥1\n∥v∗∥1\n, where v∗stands for the optimal value function of the considered\nMDP.\n0\n50\n100\n150\n200\n250\n# of iterations\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnormalized error\nm = 0\nm = 1\nm = 2\nm = 3\nm = 4\nm = 5\nm = 6\nm = 7\nm = 8\nm = 9\na. Normalized error\n.\n0\n50\n100\n150\n200\n250\n# of iterations\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\n100\nnormalized error (mean)\nm = 0\nm = 1\nm = 2\nm = 3\nm = 4\nm = 5\nm = 6\nm = 7\nm = 8\nm = 9\nb. Normalized error\n(mean, log-scale).\n0\n50\n100\n150\n200\n250\n# of iterations\n10\n6\n10\n5\n10\n4\n10\n3\n10\n2\n10\n1\nnormalized error (std)\nm = 0\nm = 1\nm = 2\nm = 3\nm = 4\nm = 5\nm = 6\nm = 7\nm = 8\nm = 9\nc. Normalized error\n(std, log-scale).\nFigure 1: Results on the Garnet problems.\nFig. 1 shows the results.\nFig. 1.a shows how the normalized error evolves with the\nnumber of iterations (recall that m = 0 stands for classic value iteration). Shaded areas\ncorrespond to standard deviations and lines to means (due to randomness of the MDPs,\nthe algorithms being deterministic given the ﬁxed initial value function). Fig. 1.b and 1.c\nshow respectively the mean and the standard deviation of these errors, in a logarithmic\nscale. One can observe that Anderson acceleration consistently oﬀers a signiﬁcant speed-up\ncompared to value iteration, and that rather small values of m (m ≈5) seem to be enough.\n2.4 Nuancing the Acceleration\nWe must highlight that the optimal policy is the object of interest, the value function being\nonly a proxy to it. Regarding the value function, its level is not that important, but its rel-\native diﬀerences are. This is addressed by the relative value iteration algorithm (Puterman,\n1994, Ch. 6.6). For a given state s0, it iterates as vk+ 1\n2 = Tvk, vk+1 = vk+ 1\n2 −vk+ 1\n2 (s0)1. It\nusually converges much faster than value iteration (towards v∗−v∗(s0)1), but the greedy\npolicies resp. to each iterate’s estimated values are the same for both algorithms. This\nscheme can also be easily accelerated with Anderson’s approach.\n4\nAnderson Acceleration for Reinforcement Learning\n0\n2\n4\n6\n8\n10\n# of iterations\n10\n4\n10\n3\n10\n2\n10\n1\n100\nnormalized error (mean)\nm = 0\nm = 1\nm = 2\nm = 3\nm = 4\nm = 5\nm = 6\nm = 7\nm = 8\nm = 9\na. Error on greedy policies\n(accelerated VI).\n0\n50\n100\n150\n200\n250\n# of iterations\n10\n12\n10\n10\n10\n8\n10\n6\n10\n4\n10\n2\n100\nnormalized error (mean)\nm = 0\nm = 1\nm = 2\nm = 3\nm = 4\nm = 5\nm = 6\nm = 7\nm = 8\nm = 9\nb. Normalized error\n(accelerated relative VI).\n0\n2\n4\n6\n8\n10\n# of iterations\n10\n4\n10\n3\n10\n2\n10\n1\n100\nnormalized error (mean)\nm = 0\nm = 1\nm = 2\nm = 3\nm = 4\nm = 5\nm = 6\nm = 7\nm = 8\nm = 9\nc. Error on greedy policies\n(accelerated relative VI).\nFigure 2: Additional results.\nWe provide additional results on Fig. 2 (for the same MDPs as previously). Fig. 2.a\nshows the error of the greedy policy, that is ∥v∗−vπalg\nk ∥1/∥v∗∥1, with πalg\nk\nbeing greedy\nrespectively to valg\nk\n, for the ﬁrst 10 iterations (same data as for Fig. 1). This is what we’re\nreally interested in.\nOne can observe that value iteration provides more quickly better\nsolutions than Anderson acceleration. This is due to the fact that if the level of the value\nfunction converges slowly, its relative diﬀerences converge more quickly.\nSo, we compare relative value iteration and its accelerated counterpart in Fig. 2.b (nor-\nmalized error of the estimate, not of the greedy policy), to be compared to Fig. 1.b. There\nis still an acceleration with Anderson, at least at the beginning, but the speed-up is much\nless than in Fig. 1. We compare the error on greedy policies for the same setting in Fig. 2.c,\nand all approaches perform equally well.\n3. Anderson Acceleration for Reinforcement Learning\nSo, the advantage of Anderson acceleration applied to exact value iteration on simple Garnet\nproblems is not that clear. Yet, it could still be interesting for policy evaluation or in the\napproximate setting. We discuss brieﬂy its possible applications to (deep) RL.\n3.1 Approximate Dynamic Programming\nAnderson acceleration could be applied to approximate dynamic programming and related\nmethods. For example, the well-known DQN algorithm (Mnih et al., 2015) is nothing else\nthan a (very smart) approximate value iteration approach. A state-action value function\nQ is estimated (rather than a value function), and this function is represented as a neural\nnetwork. A target network Qk is maintained, and the Q-function is estimated by solving\nthe least-squares problem (for the memory buﬀer {(si, ai, ri, s′\ni)1≤i≤n})\n1\nn\nn\nX\ni=1\n(yi −Qθ(si, ai))2 with yi = ri + γ max\na∈A Qk(s′\ni, a).\nAnderson acceleration can be applied directly as follows. Assume that the m + 1 previous\ntarget networks Qk, . . . , Qk−m are maintained. Deﬁne for k −m ≤j ≤k\nδj = [r1 + γ max\na\nQj(s′\n1, a) −Qj(s1, a1), . . . , rn + γ max\na\nQj(s′\nn, a) −Qj(sn, an)]⊤∈Rn\n5\nGeist and Scherrer\nand ∆k = [δk−m, . . . , δk] ∈Rn×(m+1). Solve αk+1 as in Eq. (1) and deﬁne for all 1 ≤i ≤n\nyi =\nn\nX\nj=0\nαj(ri + γ max\na∈A Qk−m+j(s′\ni, a)).\nSo, Anderson acceleration would modify the targets in the regression problem, the necessary\ncoeﬃcients being obtained with a cheap least-squares (given m is small enough, as suggested\nby our preliminary experiments). Notice that the estimate αk+1 is biased, as being the\nsolution to a residual problem with sampled transitions. However, if a problem, this could\nprobably be handled with instrumental variables, giving an LSTD-like algorithm (Bradtke\nand Barto, 1996). Variations of this general scheme could also be envisionned, for example\nby computing the α vector on a subset of the memory replay or even on the current mini-\nbatch, or by considering variations of Anderson acceleration such as the one of Henderson\nand Varadhan (2018).\nThis acceleration scheme could be more generally applied to approximate modiﬁed policy\niteration, or AMPI (Scherrer et al., 2015), that generalizes both approximate policy and\nvalue iterations. Modiﬁed policy iteration is similar to policy iteration, except that instead\nof computing the ﬁxed point in the evaluation step, the Bellman evaluation operator is\napplied p times (p = 1 gives value iteration, p = ∞policy iteration), the improvement\nstep (computing the greedy policy) being the same (up to possible approximation). In the\napproximate setting, the evaluation step is usually performed by performing the regression\nof p-step returns, but it could be done by applying repeatedly the evaluation operator, this\nbeing combined with Anderson acceleration (much like DQN, but with Tπ instead of T).\n3.2 Policy Optimization\nAnother popular approach in reinforcement learning is policy optimization, or direct policy\nsearch (Deisenroth et al., 2013), that maximizes J(w) = ES∼µ[vπw(S)] (or a proxy), for a\nuser-deﬁned state distribution µ, over a class of parameterized policies. This is classically\ndone by performing a gradient ascent:\nwk+1 = wk + η∇wJ(w)|w=wk.\n(2)\nThis gradient is given by ∇wJ(w) = ES∼dµ,πw,A∼π[Qπw(S, A)∇w ln πw(A|S)]. Thus, it\ndepends on the state-action value function of the current policy.\nThis gradient can be\nestimated with rollouts, but it is quite common to estimate the Q-function itself. Related\napproaches are known as actor-critic methods (the actor being the policy, and the critic\nthe Q-function). It is quite common to estimate the critic using a SARSA-like approach,\nespecially in deep RL. In other words, the critic is estimated by applying repeatedly the\nBellman evaluation operator. Therefore, Anderson acceleration could be applied, in the\nsame spirit as what we described for DQN.\nYet, Anderson acceleration could also be used to speed up the convergence of the policy.\nConsider the gradient ascent in Eq. (2). This can be seen as a ﬁxed-point iteration to solve\nw = w+η∇wJ(w). Anderson acceleration could thus be used to speed it up. Seeing gradient\ndescent as a ﬁxed point is not new (Jung, 2017), nor is applying Anderson acceleration to\nspeed it up (Scieur et al., 2016; Xie et al., 2018). Yet, it has never been applied to policy\noptimization, as far as we know.\n6\nAnderson Acceleration for Reinforcement Learning\nReferences\nDonald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the\nACM (JACM), 12(4):547–560, 1965.\nTW Archibald, KIM McKinnon, and LC Thomas. On the generation of Markov decision\nprocesses. Journal of the Operational Research Society, pages 354–361, 1995.\nShalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural\nactor-critic algorithms. Automatica, 45(11):2471–2482, 2009.\nSteven J. Bradtke and Andrew G. Barto. Linear Least-Squares algorithms for temporal\ndiﬀerence learning. Machine Learning, 22(1-3):33–57, 1996.\nMarc Peter Deisenroth, Gerhard Neumann, Jan Peters, et al. A survey on policy search for\nrobotics. Foundations and Trends R⃝in Robotics, 2(1–2):1–142, 2013.\nHaw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceler-\nation. Numerical Linear Algebra with Applications, 16(3):197–221, 2009.\nNicholas C Henderson and Ravi Varadhan. Damped anderson acceleration with restarts\nand monotonicity control for accelerating em and em-like algorithms.\narXiv preprint\narXiv:1803.06673, 2018.\nAlexander Jung.\nA ﬁxed-point of view on gradient methods for big data. Frontiers in\nApplied Mathematics and Statistics, 3:18, 2017.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\nMartin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Program-\nming. Wiley-Interscience, 1994.\nBruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and Matthieu\nGeist. Approximate modiﬁed policy iteration and its application to the game of tetris.\nJournal of Machine Learning Research, 16:1629–1676, 2015.\nDamien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceler-\nation. In Advances In Neural Information Processing Systems, pages 712–720, 2016.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT\npress Cambridge, 1998.\nAlex Toth and CT Kelley. Convergence analysis for anderson acceleration. SIAM Journal\non Numerical Analysis, 53(2):805–819, 2015.\nHomer F Walker and Peng Ni.\nAnderson acceleration for ﬁxed-point iterations.\nSIAM\nJournal on Numerical Analysis, 49(4):1715–1735, 2011.\n7\nGeist and Scherrer\nGuangzeng Xie, Yitan Wang, Shuchang Zhou, and Zhihua Zhang. Interpolatron: Interpola-\ntion or extrapolation schemes to accelerate optimization for deep neural networks. arXiv\npreprint arXiv:1805.06753, 2018.\n8\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2018-09-25",
  "updated": "2018-09-25"
}